<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-11  Parallel-R1 Towards Parallel Thinking via Reinforcement Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07957v1/page_5_1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    90 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-11-æ›´æ–°"><a href="#2025-09-11-æ›´æ–°" class="headerlink" title="2025-09-11 æ›´æ–°"></a>2025-09-11 æ›´æ–°</h1><h2 id="Parallel-R1-Towards-Parallel-Thinking-via-Reinforcement-Learning"><a href="#Parallel-R1-Towards-Parallel-Thinking-via-Reinforcement-Learning" class="headerlink" title="Parallel-R1: Towards Parallel Thinking via Reinforcement Learning"></a>Parallel-R1: Towards Parallel Thinking via Reinforcement Learning</h2><p><strong>Authors:Tong Zheng, Hongming Zhang, Wenhao Yu, Xiaoyang Wang, Xinyu Yang, Runpeng Dai, Rui Liu, Huiwen Bao, Chengsong Huang, Heng Huang, Dong Yu</strong></p>
<p>Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose \textbf{Parallel-R1}, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals a clear shift in the modelâ€™s thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in a later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as a \textbf{mid-training exploration scaffold}, where this temporary exploratory phase unlocks a higher performance ceiling after RL, yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at <a target="_blank" rel="noopener" href="https://github.com/zhengkid/Parallel-R1">https://github.com/zhengkid/Parallel-R1</a>. </p>
<blockquote>
<p>å¹¶è¡Œæ€è€ƒå·²æˆä¸ºä¸€ç§é€šè¿‡åŒæ—¶æ¢ç´¢å¤šä¸ªæ¨ç†è·¯å¾„æ¥æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•ã€‚ç„¶è€Œï¼Œé€šè¿‡è®­ç»ƒæ¿€æ´»è¿™ç§èƒ½åŠ›ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºåˆæˆæ•°æ®ä¸Šçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿™é¼“åŠ±äº†æ•™å¸ˆå¼ºåˆ¶æ¨¡ä»¿ï¼Œè€Œä¸æ˜¯æ¢ç´¢å’Œæ³›åŒ–ã€‚ä¸å®ƒä»¬ä¸åŒï¼Œæˆ‘ä»¬æå‡ºäº†<strong>Parallel-R1</strong>ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿæ”¯æŒå¤æ‚ç°å®ä¸–ç•Œæ¨ç†ä»»åŠ¡çš„å¹¶è¡Œæ€è€ƒè¡Œä¸ºçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶é‡‡ç”¨äº†ä¸€ç§æ¸è¿›çš„è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œæ˜ç¡®è§£å†³äº†è®­ç»ƒå¹¶è¡Œæ€è€ƒçš„å†·å¯åŠ¨é—®é¢˜ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨SFTå¯¹æ¥è‡ªç®€å•ä»»åŠ¡çš„æç¤ºç”Ÿæˆè½¨è¿¹è¿›è¡Œè®­ç»ƒï¼Œä»¥åŸ¹å…»å¹¶è¡Œæ€è€ƒèƒ½åŠ›ï¼Œç„¶åè¿‡æ¸¡åˆ°RLï¼Œåœ¨æ›´å¤æ‚çš„é—®é¢˜ä¸Šæ¢ç´¢å¹¶æ³›åŒ–è¿™é¡¹æŠ€èƒ½ã€‚åœ¨å„ç§æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒï¼ŒåŒ…æ‹¬MATHã€AMC23å’ŒAIMEï¼Œè¡¨æ˜Parallel-R1æˆåŠŸåŸ¹å…»äº†å¹¶è¡Œæ€è€ƒèƒ½åŠ›ï¼Œç›¸å¯¹äºç›´æ¥åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šä½¿ç”¨RLè®­ç»ƒçš„é¡ºåºæ€è€ƒæ¨¡å‹ï¼Œå…¶å‡†ç¡®ç‡æé«˜äº†8.4%ã€‚è¿›ä¸€æ­¥çš„åˆ†ææ˜¾ç¤ºï¼Œæ¨¡å‹çš„è¡Œä¸ºæœ‰æ˜æ˜¾çš„è½¬å˜ï¼šåœ¨æ—©æœŸé˜¶æ®µï¼Œå®ƒä½¿ç”¨å¹¶è¡Œæ€è€ƒä½œä¸ºæ¢ç´¢ç­–ç•¥ï¼Œè€Œåœ¨åæœŸé˜¶æ®µï¼Œå®ƒä½¿ç”¨ç›¸åŒçš„èƒ½åŠ›è¿›è¡Œå¤šè§†è§’éªŒè¯ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬éªŒè¯äº†å¹¶è¡Œæ€è€ƒä½œä¸º<strong>è®­ç»ƒä¸­æœŸæ¢ç´¢è„šæ‰‹æ¶</strong>ï¼Œè¿™ä¸€ä¸´æ—¶æ¢ç´¢é˜¶æ®µåœ¨RLä¹‹åå¼€å¯äº†æ›´é«˜çš„æ€§èƒ½ä¸Šé™ï¼Œåœ¨AIME25ä¸Šç›¸å¯¹äºåŸºå‡†çº¿æé«˜äº†42.9%ã€‚æˆ‘ä»¬çš„æ¨¡å‹ã€æ•°æ®å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhengkid/Parallel-R1%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/zhengkid/Parallel-R1ä¸Šå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07980v1">PDF</a> Project website: <a target="_blank" rel="noopener" href="https://zhengkid.github.io/Parallel_R1.github.io/">https://zhengkid.github.io/Parallel_R1.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¹¶è¡Œæ€ç»´åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢çš„ä½œç”¨ï¼Œå¹¶æŒ‡å‡ºé€šè¿‡è®­ç»ƒå®ç°è¿™ä¸€èƒ½åŠ›é¢ä¸´çš„æŒ‘æˆ˜ã€‚æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶Parallel-R1ï¼Œæ—¨åœ¨åŸ¹å…»å¤æ‚çš„ç°å®æ¨ç†ä»»åŠ¡çš„å¹¶è¡Œæ€ç»´èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ¸è¿›çš„è¯¾ç¨‹æ–¹å¼è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­å¹¶è¡Œæ€ç»´çš„å†·å¯åŠ¨é—®é¢˜ï¼Œé¦–å…ˆé€šè¿‡åˆæˆæ•°æ®çš„ç›‘ç£å¾®è°ƒæ¥åŸ¹å…»å¹¶è¡Œæ€ç»´ï¼Œç„¶åè¿‡æ¸¡åˆ°å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ¢ç´¢å’Œæ³›åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒParallel-R1æˆåŠŸåŸ¹å…»äº†æ¨¡å‹çš„å¹¶è¡Œæ€ç»´èƒ½åŠ›ï¼Œæé«˜äº†æ¨ç†å‡†ç¡®æ€§ã€‚Parallel-R1å°†å¹¶è¡Œæ€ç»´ä½œä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸´æ—¶æ¢ç´¢æ¶æ„ï¼Œä¸ºåç»­å¼ºåŒ–å­¦ä¹ é˜¶æ®µæé«˜æ€§èƒ½å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¹¶è¡Œæ€ç»´æ˜¯ä¸€ç§æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•ï¼Œæ¶‰åŠåŒæ—¶æ¢ç´¢å¤šä¸ªæ¨ç†è·¯å¾„ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ æ¡†æ¶Parallel-R1é¦–æ¬¡å®ç°äº†å¯¹å¤æ‚ç°å®æ¨ç†ä»»åŠ¡çš„å¹¶è¡Œæ€ç»´èƒ½åŠ›çš„åŸ¹å…»ã€‚</li>
<li>Parallel-R1é‡‡ç”¨æ¸è¿›çš„è¯¾ç¨‹æ–¹å¼è§£å†³å†·å¯åŠ¨é—®é¢˜ï¼Œé¦–å…ˆé€šè¿‡åˆæˆæ•°æ®çš„ç›‘ç£å¾®è°ƒåŸ¹å…»å¹¶è¡Œæ€ç»´ï¼Œç„¶åè¿‡æ¸¡åˆ°å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ¢ç´¢å’Œæ³›åŒ–ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒParallel-R1èƒ½æœ‰æ•ˆæé«˜æ¨¡å‹çš„æ¨ç†å‡†ç¡®æ€§ï¼Œè¾¾åˆ°8.4%çš„æ”¹è¿›ã€‚</li>
<li>æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¡¨ç°å‡ºæ˜æ˜¾çš„é˜¶æ®µæ€§è¡Œä¸ºå˜åŒ–ï¼Œæ—©æœŸä½¿ç”¨å¹¶è¡Œæ€ç»´ä½œä¸ºæ¢ç´¢ç­–ç•¥ï¼ŒåæœŸç”¨äºå¤šè§†è§’éªŒè¯ã€‚</li>
<li>Parallel-R1å°†å¹¶è¡Œæ€ç»´ä½œä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸´æ—¶æ¢ç´¢æ¶æ„ï¼Œä¸ºåç»­å¼ºåŒ–å­¦ä¹ é˜¶æ®µæé«˜æ€§èƒ½å¥ å®šäº†åŸºç¡€ï¼Œå®ç°äº†æ›´é«˜çš„æ€§èƒ½æå‡ï¼ˆAIMEä¸Šçš„æ”¹è¿›è¾¾åˆ°äº†42.9%ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07980">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07980v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07980v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07980v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Mini-o3-Scaling-Up-Reasoning-Patterns-and-Interaction-Turns-for-Visual-Search"><a href="#Mini-o3-Scaling-Up-Reasoning-Patterns-and-Interaction-Turns-for-Visual-Search" class="headerlink" title="Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual   Search"></a>Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual   Search</h2><p><strong>Authors:Xin Lai, Junyi Li, Wei Li, Tao Liu, Tianjian Li, Hengshuang Zhao</strong></p>
<p>Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning â€“ spanning tens of steps â€“ and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„è¿›å±•é€šè¿‡ç»“åˆå›¾åƒå·¥å…·å’Œå¼ºåŒ–å­¦ä¹ æ¥è§£å†³è§†è§‰é—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¼€æºæ–¹æ³•é€šå¸¸å±•ç°å‡ºå•è°ƒçš„æ¨ç†æ¨¡å¼ï¼Œå¹¶ä¸”åªå…è®¸æœ‰é™çš„äº¤äº’å›åˆæ•°ï¼Œä½¿å¾—å®ƒä»¬åœ¨éœ€è¦å°è¯•å’Œé”™è¯¯æ¢ç´¢çš„å›°éš¾ä»»åŠ¡ä¸­è¡¨ç°ä¸è¶³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ‰©å¤§åŸºäºå·¥å…·çš„äº¤äº’æ¥è§£å†³è¿™ä¸€é™åˆ¶ï¼Œå¹¶å¼•å…¥äº†Mini-o3ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿæ‰§è¡Œæ·±åº¦ã€å¤šå›åˆçš„æ¨ç†â€”â€”è·¨è¶Šæ•°åä¸ªæ­¥éª¤â€”â€”å¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰æœç´¢ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬é‡ç°OpenAI o3é£æ ¼è¡Œä¸ºçš„é…æ–¹åŒ…å«ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ„å»ºäº†è§†è§‰æ¢é’ˆæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—åŒ…å«æ•°åƒä¸ªç”¨äºæ¢ç´¢æ€§æ¨ç†çš„è§†è§‰æœç´¢é—®é¢˜çš„é›†åˆã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè¿­ä»£çš„æ•°æ®æ”¶é›†ç®¡é“ï¼Œä»¥è·å¾—å±•ç°å‡ºå¤šæ ·åŒ–æ¨ç†æ¨¡å¼çš„å†·å¯åŠ¨è½¨è¿¹ï¼ŒåŒ…æ‹¬æ·±åº¦ä¼˜å…ˆæœç´¢ã€å°è¯•å’Œé”™è¯¯ä»¥åŠç›®æ ‡ç»´æŠ¤ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å›åˆé®æŒ¡ç­–ç•¥ï¼Œé˜²æ­¢åœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­å¯¹è¶…è¿‡å›åˆæ•°çš„å›åº”è¿›è¡Œæƒ©ç½šï¼Œä»è€Œåœ¨è®­ç»ƒæ•ˆç‡ä¸æµ‹è¯•æ—¶é—´å¯æ‰©å±•æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å°½ç®¡è®­ç»ƒæ—¶åªè®¾å®šäº†å…­ä¸ªäº¤äº’å›åˆçš„ä¸Šé™ï¼Œä½†æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ¨ç†æ—¶è‡ªç„¶èƒ½å¤Ÿæ‰©å±•åˆ°æ•°åä¸ªå›åˆï¼Œéšç€å›åˆæ•°çš„å¢åŠ ï¼Œå‡†ç¡®ç‡ä¹Ÿåœ¨æé«˜ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMini-o3èƒ½å¤Ÿäº§ç”Ÿä¸°å¯Œçš„æ¨ç†æ¨¡å¼å’Œæ·±åº¦æ€è€ƒè·¯å¾„ï¼Œæœ‰æ•ˆåœ°è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰æœç´¢é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07969v1">PDF</a> Code, datasets, models are available at   <a target="_blank" rel="noopener" href="https://github.com/Mini-o3/Mini-o3">https://github.com/Mini-o3/Mini-o3</a>. Project Page: <a target="_blank" rel="noopener" href="https://mini-o3.github.io/">https://mini-o3.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ è§£å†³è§†è§‰é—®é¢˜çš„æœ€æ–°è¿›å±•ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•çš„å•è°ƒæ¨ç†æ¨¡å¼å’Œäº¤äº’è½®æ¬¡é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†Mini-o3ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿæ‰§è¡Œæ·±å…¥çš„å¤šæ¬¡æ¨ç†ï¼Œæ¶µç›–æ•°åä¸ªæ­¥éª¤ï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰æœç´¢ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚å…¶å…³é”®ç­–ç•¥åŒ…æ‹¬æ„å»ºè§†è§‰æ¢é’ˆæ•°æ®é›†ã€å¼€å‘è¿­ä»£æ•°æ®æ”¶é›†ç®¡é“ä»¥åŠæå‡ºè¿‡åº¦è½®æ¬¡æ©è”½ç­–ç•¥ã€‚æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†æ—¶è‡ªç„¶æ‰©å±•åˆ°æ•°åä¸ªæ­¥éª¤ï¼Œéšç€è½®æ¬¡çš„å¢åŠ ï¼Œå‡†ç¡®æ€§å¾—åˆ°æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€æ–°å¤šæ¨¡æ€æ¨¡å‹åˆ©ç”¨å›¾åƒå·¥å…·å’Œå¼ºåŒ–å­¦ä¹ è§£å†³è§†è§‰é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨å•è°ƒæ¨ç†å’Œäº¤äº’è½®æ¬¡é™åˆ¶çš„é—®é¢˜ã€‚</li>
<li>Mini-o3ç³»ç»Ÿé€šè¿‡æ·±å…¥å¤šæ¬¡æ¨ç†è§£å†³å¤æ‚è§†è§‰æœç´¢ä»»åŠ¡ã€‚</li>
<li>Mini-o3åœ¨æ„å»ºè§†è§‰æ¢é’ˆæ•°æ®é›†ã€å¼€å‘è¿­ä»£æ•°æ®æ”¶é›†ç®¡é“å’Œè¿‡åº¦è½®æ¬¡æ©è”½ç­–ç•¥æ–¹é¢æœ‰æ‰€åˆ›æ–°ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†æ—¶è‡ªç„¶æ‰©å±•åˆ°æ•°åä¸ªæ­¥éª¤ã€‚</li>
<li>éšç€æ¨ç†æ­¥éª¤çš„å¢åŠ ï¼Œæ¨¡å‹çš„å‡†ç¡®æ€§å¾—åˆ°æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07969">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07969v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07969v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07969v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07969v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07969v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Visual-TableQA-Open-Domain-Benchmark-for-Reasoning-over-Table-Images"><a href="#Visual-TableQA-Open-Domain-Benchmark-for-Reasoning-over-Table-Images" class="headerlink" title="Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images"></a>Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images</h2><p><strong>Authors:Boammani Aser Lompo, Marc Haraoui</strong></p>
<p>Visual reasoning over structured data such as tables is a critical capability for modern vision-language models (VLMs), yet current benchmarks remain limited in scale, diversity, or reasoning depth, especially when it comes to rendered table images. Addressing this gap, we introduce Visual-TableQA, a large-scale, open-domain multimodal dataset specifically designed to evaluate and enhance visual reasoning over complex tabular data. Our generation pipeline is modular, scalable, and fully autonomous, involving multiple reasoning LLMs collaborating across distinct roles: generation, validation, and inspiration. Visual-TableQA comprises 2.5k richly structured LaTeX-rendered tables and 6k reasoning-intensive QA pairs, all produced at a cost of under USD 100. To promote diversity and creativity, our pipeline performs multi-model collaborative data generation via cross-model prompting (â€˜inspirationâ€™) and LLM-jury filtering. Stronger models seed layouts and topics that weaker models elaborate, collectively distilling diverse reasoning patterns and visual structures into the dataset. Empirical results show that models fine-tuned on Visual-TableQA generalize robustly to external benchmarks, outperforming several proprietary models despite the datasetâ€™s synthetic nature. The full pipeline and resources are publicly available at <a target="_blank" rel="noopener" href="https://github.com/AI-4-Everyone/Visual-TableQA">https://github.com/AI-4-Everyone/Visual-TableQA</a>. </p>
<blockquote>
<p>é’ˆå¯¹è¡¨æ ¼ç­‰ç»“æ„åŒ–æ•°æ®çš„è§†è§‰æ¨ç†æ˜¯ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å…³é”®èƒ½åŠ›ï¼Œç„¶è€Œå½“å‰çš„åŸºå‡†æµ‹è¯•åœ¨è§„æ¨¡ã€å¤šæ ·æ€§æˆ–æ¨ç†æ·±åº¦æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å‘ˆç°è¡¨æ ¼å›¾åƒæ—¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Visual-TableQAï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€å¼€æ”¾é¢†åŸŸçš„å¤šæ¨¡å¼æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°å’Œæé«˜å¤æ‚è¡¨æ ¼æ•°æ®çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç”Ÿæˆæµç¨‹æ˜¯æ¨¡å—åŒ–çš„ã€å¯æ‰©å±•çš„ï¼Œå¹¶ä¸”å®Œå…¨è‡ªåŠ¨åŒ–ï¼Œæ¶‰åŠå¤šä¸ªæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒè§’è‰²ä¹‹é—´çš„åä½œï¼šç”Ÿæˆã€éªŒè¯å’Œçµæ„Ÿã€‚Visual-TableQAåŒ…å«2500ä¸ªä¸°å¯Œçš„LaTeXæ¸²æŸ“ç»“æ„åŒ–è¡¨æ ¼å’Œ6000ä¸ªæ¨ç†å¯†é›†çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œæ‰€æœ‰è¿™äº›éƒ½ä»¥ä¸åˆ°100ç¾å…ƒçš„æˆæœ¬äº§ç”Ÿã€‚ä¸ºäº†ä¿ƒè¿›å¤šæ ·æ€§å’Œåˆ›é€ æ€§ï¼Œæˆ‘ä»¬çš„æµç¨‹é€šè¿‡è·¨æ¨¡å‹æç¤ºï¼ˆâ€œçµæ„Ÿâ€ï¼‰å’ŒLLMé™ªå®¡å›¢è¿‡æ»¤ï¼Œè¿›è¡Œå¤šæ¨¡å‹åä½œæ•°æ®ç”Ÿæˆã€‚æ›´å¼ºçš„æ¨¡å‹ä¸ºå¸ƒå±€å’Œä¸»é¢˜æ’­ç§ï¼Œè¾ƒå¼±çš„æ¨¡å‹è¿›è¡Œé˜è¿°ï¼Œé›†ä½“æç‚¼å‡ºå¤šæ ·åŒ–çš„æ¨ç†æ¨¡å¼å’Œè§†è§‰ç»“æ„èå…¥æ•°æ®é›†ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œåœ¨Visual-TableQAä¸Šå¾®è°ƒè¿‡çš„æ¨¡å‹èƒ½å¤Ÿç¨³å¥åœ°æ¨å¹¿åˆ°å¤–éƒ¨åŸºå‡†æµ‹è¯•ï¼Œå°½ç®¡è¯¥æ•°æ®é›†æ˜¯åˆæˆçš„ï¼Œä½†å…¶æ€§èƒ½è¶…è¶Šäº†å¤šä¸ªä¸“æœ‰æ¨¡å‹ã€‚å®Œæ•´çš„æµç¨‹å’Œèµ„æºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AI-4-Everyone/Visual-TableQA%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/AI-4-Everyone/Visual-TableQAä¸Šå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07966v1">PDF</a> Work in Progress</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¤§è§„æ¨¡ã€å¼€æ”¾é¢†åŸŸçš„å¤šæ¨¡å¼æ•°æ®é›†â€”â€”Visual-TableQAï¼Œæ—¨åœ¨è¯„ä¼°å’Œå¢å¼ºå¯¹å¤æ‚è¡¨æ ¼æ•°æ®çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†åŒ…å«2.5kä¸ªä¸°å¯Œçš„ç»“æ„åŒ–LaTeXæ¸²æŸ“è¡¨æ ¼å’Œ6kä¸ªæ¨ç†å¯†é›†çš„é—®é¢˜ç­”æ¡ˆå¯¹ã€‚å…¶ç”Ÿæˆç®¡é“æ¨¡å—åŒ–ã€å¯æ‰©å±•ä¸”å®Œå…¨è‡ªä¸»ï¼Œå¤šä¸ªæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒè§’è‰²ï¼ˆç”Ÿæˆã€éªŒè¯å’Œçµæ„Ÿï¼‰ä¸­è¿›è¡Œåä½œã€‚é€šè¿‡è·¨æ¨¡å‹æç¤ºå’ŒLLMé™ªå®¡å›¢è¿‡æ»¤ï¼Œä¿ƒè¿›æ•°æ®ç”Ÿæˆçš„å¤šæ ·æ€§å’Œåˆ›é€ æ€§ã€‚åœ¨Visual-TableQAä¸Šå¾®è°ƒçš„æ¨¡å‹åœ¨å¤–éƒ¨åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ç¨³å¥ï¼Œå°½ç®¡è¯¥æ•°æ®é›†æ˜¯åˆæˆçš„ï¼Œä½†ä»ä¼˜äºå‡ ä¸ªä¸“æœ‰æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Visual-TableQAæ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°å’Œå¢å¼ºå¯¹å¤æ‚è¡¨æ ¼æ•°æ®è§†è§‰æ¨ç†èƒ½åŠ›çš„å¤§è§„æ¨¡ã€å¼€æ”¾é¢†åŸŸçš„å¤šæ¨¡å¼æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†åŒ…å«ä¸°å¯Œçš„ç»“æ„åŒ–LaTeXæ¸²æŸ“è¡¨æ ¼å’Œæ¨ç†å¯†é›†çš„é—®é¢˜ç­”æ¡ˆå¯¹ã€‚</li>
<li>Visual-TableQAçš„ç”Ÿæˆç®¡é“æ¨¡å—åŒ–ã€å¯æ‰©å±•ä¸”è‡ªä¸»ï¼Œå¤šä¸ªæ¨ç†LLMsåœ¨å…¶ä¸­åä½œï¼Œè§’è‰²åŒ…æ‹¬ç”Ÿæˆã€éªŒè¯å’Œçµæ„Ÿã€‚</li>
<li>è¯¥æ•°æ®é›†é€šè¿‡è·¨æ¨¡å‹æç¤ºå’ŒLLMé™ªå®¡å›¢è¿‡æ»¤ï¼Œä¿ƒè¿›æ•°æ®ç”Ÿæˆçš„å¤šæ ·æ€§å’Œåˆ›é€ æ€§ã€‚</li>
<li>åœ¨Visual-TableQAä¸Šè®­ç»ƒçš„æ¨¡å‹å¯ä»¥åœ¨å¤–éƒ¨åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ç¨³å¥ã€‚</li>
<li>å°½ç®¡æ•°æ®é›†æ˜¯åˆæˆçš„ï¼Œä½†å…¶è¡¨ç°ä¼˜äºä¸€äº›ä¸“æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07966">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07966v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07966v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07966v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07966v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07966v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Graph-Fused-Vision-Language-Action-for-Policy-Reasoning-in-Multi-Arm-Robotic-Manipulation"><a href="#Graph-Fused-Vision-Language-Action-for-Policy-Reasoning-in-Multi-Arm-Robotic-Manipulation" class="headerlink" title="Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm   Robotic Manipulation"></a>Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm   Robotic Manipulation</h2><p><strong>Authors:Shunlei Li, Longsen Gao, Jiuwen Cao, Yingbai Hu</strong></p>
<p>Acquiring dexterous robotic skills from human video demonstrations remains a significant challenge, largely due to conventional reliance on low-level trajectory replication, which often fails to generalize across varying objects, spatial layouts, and manipulator configurations. To address this limitation, we introduce Graph-Fused Vision-Language-Action (GF-VLA), a unified framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB-D human demonstrations. GF-VLA employs an information-theoretic approach to extract task-relevant cues, selectively highlighting critical hand-object and object-object interactions. These cues are structured into temporally ordered scene graphs, which are subsequently integrated with a language-conditioned transformer to produce hierarchical behavior trees and interpretable Cartesian motion primitives. To enhance efficiency in bimanual execution, we propose a cross-arm allocation strategy that autonomously determines gripper assignment without requiring explicit geometric modeling. We validate GF-VLA on four dual-arm block assembly benchmarks involving symbolic structure construction and spatial generalization. Empirical results demonstrate that the proposed representation achieves over 95% graph accuracy and 93% subtask segmentation, enabling the language-action planner to generate robust, interpretable task policies. When deployed on a dual-arm robot, these policies attain 94% grasp reliability, 89% placement accuracy, and 90% overall task success across stacking, letter-formation, and geometric reconfiguration tasks, evidencing strong generalization and robustness under diverse spatial and semantic variations. </p>
<blockquote>
<p>ä»äººç±»è§†é¢‘æ¼”ç¤ºä¸­å­¦ä¹ çµå·§çš„æœºå™¨äººæŠ€èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºä¼ ç»Ÿçš„ä¾èµ–ä½çº§è½¨è¿¹å¤åˆ¶çš„æ–¹æ³•å¸¸å¸¸æ— æ³•åœ¨ä¸åŒå¯¹è±¡ã€ç©ºé—´å¸ƒå±€å’Œæ“ä½œå™¨é…ç½®ä¹‹é—´è¿›è¡Œæ³›åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†Graph-Fused Vision-Language-Actionï¼ˆGF-VLAï¼‰ç»Ÿä¸€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿åŒè‡‚æœºå™¨äººç³»ç»Ÿèƒ½å¤Ÿç›´æ¥ä»RGB-Däººç±»æ¼”ç¤ºä¸­è¿›è¡Œä»»åŠ¡çº§æ¨ç†å’Œæ‰§è¡Œã€‚GF-VLAé‡‡ç”¨ä¿¡æ¯ç†è®ºæ–¹æ³•æå–ä»»åŠ¡ç›¸å…³çº¿ç´¢ï¼Œæœ‰é€‰æ‹©åœ°çªå‡ºæ‰‹ä¸ç‰©ä½“ä»¥åŠç‰©ä½“ä¸ç‰©ä½“ä¹‹é—´çš„å…³é”®äº¤äº’ã€‚è¿™äº›çº¿ç´¢è¢«ç»“æ„åŒ–æˆæŒ‰æ—¶é—´é¡ºåºæ’åˆ—çš„åœºæ™¯å›¾ï¼Œéšåä¸å—è¯­è¨€æ§åˆ¶transformerç»“åˆï¼Œç”Ÿæˆå±‚æ¬¡åŒ–çš„è¡Œä¸ºæ ‘å’Œå¯è§£é‡Šçš„ç¬›å¡å°”è¿åŠ¨åŸå§‹ç»“æ„ã€‚ä¸ºäº†æé«˜åŒå·¥æ“ä½œæ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è·¨è‡‚åˆ†é…ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯è‡ªä¸»ç¡®å®šå¤¹æŒå™¨åˆ†é…ï¼Œæ— éœ€è¿›è¡Œæ˜¾å¼å‡ ä½•å»ºæ¨¡ã€‚æˆ‘ä»¬åœ¨æ¶‰åŠç¬¦å·ç»“æ„æ„å»ºå’Œç©ºé—´æ³›åŒ–çš„å››ä¸ªåŒè‡‚å—è£…é…åŸºå‡†æµ‹è¯•ä¸Šå¯¹GF-VLAè¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è¡¨ç¤ºæ³•è¾¾åˆ°äº†è¶…è¿‡95%çš„å›¾å‡†ç¡®æ€§å’Œ93%çš„å­ä»»åŠ¡åˆ†å‰²å‡†ç¡®æ€§ï¼Œä½¿å¾—è¯­è¨€åŠ¨ä½œè§„åˆ’å™¨èƒ½å¤Ÿç”Ÿæˆç¨³å¥ã€å¯è§£é‡Šçš„ä»»åŠ¡ç­–ç•¥ã€‚å½“éƒ¨ç½²åœ¨åŒè‡‚æœºå™¨äººä¸Šæ—¶ï¼Œè¿™äº›ç­–ç•¥è¾¾åˆ°äº†94%çš„æŠ“å–å¯é æ€§ã€89%çš„å®šä½ç²¾åº¦å’Œ90%çš„æ€»ä½“ä»»åŠ¡æˆåŠŸç‡ï¼ŒåŒ…æ‹¬å †å ã€å­—æ¯å½¢æˆå’Œå‡ ä½•é‡æ„ä»»åŠ¡ï¼Œè¯æ˜äº†åœ¨å„ç§ç©ºé—´å’Œè¯­ä¹‰å˜åŒ–ä¸‹çš„å¼ºå¤§æ³›åŒ–å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07957v1">PDF</a> This paper is submitted to IEEE IROS 2025 Workshop AIR4S</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºGraph-Fused Vision-Language-Actionï¼ˆGF-VLAï¼‰çš„ç»Ÿä¸€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿åŒæœºæ¢°è‡‚æœºå™¨äººç³»ç»Ÿèƒ½å¤Ÿä»RGB-Däººç±»æ¼”ç¤ºä¸­è¿›è¡Œä»»åŠ¡çº§æ¨ç†å’Œæ‰§è¡Œã€‚GF-VLAé‡‡ç”¨ä¿¡æ¯ç†è®ºæ–¹æ³•æå–ä»»åŠ¡ç›¸å…³çº¿ç´¢ï¼Œå¹¶ç»“æ„åŒ–æˆä¸ºæ—¶åºåœºæ™¯å›¾ï¼Œéšåä¸è¯­è¨€æ§åˆ¶çš„è½¬æ¢å™¨ç»“åˆï¼Œäº§ç”Ÿå±‚æ¬¡åŒ–çš„è¡Œä¸ºæ ‘å’Œå¯è§£é‡Šçš„ç¬›å¡å°”è¿åŠ¨åŸè¯­ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§è·¨æœºæ¢°è‡‚åˆ†é…ç­–ç•¥ï¼Œå¯è‡ªä¸»ç¡®å®šå¤¹æŒå™¨åˆ†é…ï¼Œæ— éœ€æ˜¾å¼å‡ ä½•å»ºæ¨¡ã€‚åœ¨å››ä¸ªåŒæœºæ¢°è‡‚å—è£…é…åŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†GF-VLAçš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ç¬¦å·ç»“æ„æ„å»ºå’Œç©ºé—´æ³›åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å›¾å‡†ç¡®åº¦ã€å­ä»»åŠ¡åˆ†å‰²ã€è¯­è¨€åŠ¨ä½œè§„åˆ’å™¨ç”Ÿæˆçš„ä»»åŠ¡ç­–ç•¥å¯é æ€§ã€æ”¾ç½®ç²¾åº¦ä»¥åŠæ•´ä½“ä»»åŠ¡æˆåŠŸç‡æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GF-VLAæ¡†æ¶å®ç°äº†ä»äººç±»è§†é¢‘æ¼”ç¤ºä¸­è®©åŒæœºæ¢°è‡‚æœºå™¨äººè¿›è¡Œä»»åŠ¡çº§å­¦ä¹ å’Œæ‰§è¡Œã€‚</li>
<li>é‡‡ç”¨ä¿¡æ¯ç†è®ºæ–¹æ³•æå–ä»»åŠ¡ç›¸å…³çº¿ç´¢ï¼Œå¼ºè°ƒæ‰‹å’Œç‰©ä½“ã€ç‰©ä½“å’Œç‰©ä½“ä¹‹é—´çš„å…³é”®äº¤äº’ã€‚</li>
<li>å°†æå–çš„çº¿ç´¢ç»“æ„åŒ–æˆä¸ºæ—¶åºåœºæ™¯å›¾ï¼Œå¹¶ä¸è¯­è¨€æ§åˆ¶çš„è½¬æ¢å™¨ç»“åˆï¼Œç”Ÿæˆå±‚æ¬¡åŒ–çš„è¡Œä¸ºæ ‘å’Œå¯è§£é‡Šçš„æœºæ¢°è¿åŠ¨åŸè¯­ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è·¨æœºæ¢°è‡‚åˆ†é…ç­–ç•¥ï¼Œè‡ªä¸»ç¡®å®šå¤¹æŒå™¨çš„ä½¿ç”¨ï¼Œæ— éœ€å¤æ‚çš„å‡ ä½•å»ºæ¨¡ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†GF-VLAçš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ç¬¦å·ç»“æ„æ„å»ºå’Œç©ºé—´æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGF-VLAåœ¨å›¾å‡†ç¡®åº¦ã€å­ä»»åŠ¡åˆ†å‰²æ–¹é¢æœ‰ä¼˜å¼‚è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07957v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07957v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07957v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07957v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07957v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07957v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Guided-Reasoning-in-LLM-Driven-Penetration-Testing-Using-Structured-Attack-Trees"><a href="#Guided-Reasoning-in-LLM-Driven-Penetration-Testing-Using-Structured-Attack-Trees" class="headerlink" title="Guided Reasoning in LLM-Driven Penetration Testing Using Structured   Attack Trees"></a>Guided Reasoning in LLM-Driven Penetration Testing Using Structured   Attack Trees</h2><p><strong>Authors:Katsuaki Nakano, Reza Feyyazi, Shanchieh Jay Yang, Michael Zuzak</strong></p>
<p>Recent advances in Large Language Models (LLMs) have driven interest in automating cybersecurity penetration testing workflows, offering the promise of faster and more consistent vulnerability assessment for enterprise systems. Existing LLM agents for penetration testing primarily rely on self-guided reasoning, which can produce inaccurate or hallucinated procedural steps. As a result, the LLM agent may undertake unproductive actions, such as exploiting unused software libraries or generating cyclical responses that repeat prior tactics. In this work, we propose a guided reasoning pipeline for penetration testing LLM agents that incorporates a deterministic task tree built from the MITRE ATT&amp;CK Matrix, a proven penetration testing kll chain, to constrain the LLMâ€™s reaoning process to explicitly defined tactics, techniques, and procedures. This anchors reasoning in proven penetration testing methodologies and filters out ineffective actions by guiding the agent towards more productive attack procedures. To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios. Our proposed reasoning pipeline guided the LLM agent through 71.8%, 72.8%, and 78.6% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively. Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5%, 16.5%, and 75.7% of subtasks and required 86.2%, 118.7%, and 205.9% more model queries. This suggests that incorporating a deterministic task tree into LLM reasoning pipelines can enhance the accuracy and efficiency of automated cybersecurity assessments </p>
<blockquote>
<p>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ¨åŠ¨äº†è‡ªåŠ¨åŒ–ç½‘ç»œå®‰å…¨æ¸—é€æµ‹è¯•å·¥ä½œæµçš„å…´è¶£ï¼Œä¸ºä¼ä¸šç³»ç»Ÿçš„æ¼æ´è¯„ä¼°æä¾›äº†æ›´å¿«æ›´ä¸€è‡´çš„å¸Œæœ›ã€‚ç°æœ‰çš„æ¸—é€æµ‹è¯•LLMä»£ç†ä¸»è¦ä¾èµ–äºè‡ªæˆ‘æŒ‡å¯¼çš„æ¨ç†ï¼Œè¿™å¯èƒ½ä¼šäº§ç”Ÿä¸å‡†ç¡®æˆ–è™šå¹»çš„ç¨‹åºæ­¥éª¤ã€‚å› æ­¤ï¼ŒLLMä»£ç†å¯èƒ½ä¼šé‡‡å–æ— æ•ˆçš„è¡ŒåŠ¨ï¼Œå¦‚åˆ©ç”¨æœªä½¿ç”¨çš„è½¯ä»¶åº“æˆ–ç”Ÿæˆé‡å¤å…ˆå‰ç­–ç•¥çš„å¾ªç¯å“åº”ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºæ¸—é€æµ‹è¯•LLMä»£ç†çš„å¼•å¯¼å¼æ¨ç†ç®¡é“ï¼Œå®ƒç»“åˆäº†ç”±MITRE ATT&amp;CKçŸ©é˜µæ„å»ºçš„ç¡®å®šæ€§ä»»åŠ¡æ ‘ï¼Œè¿™æ˜¯ä¸€ä¸ªç»è¿‡éªŒè¯çš„æ¸—é€æµ‹è¯•kllé“¾ï¼Œç”¨äºå°†LLMçš„æ¨ç†è¿‡ç¨‹çº¦æŸåœ¨æ˜ç¡®å®šä¹‰çš„æˆ˜æœ¯ã€æŠ€æœ¯å’Œç¨‹åºå†…ã€‚è¿™ä»¥ç»è¿‡éªŒè¯çš„æ¸—é€æµ‹è¯•æ–¹æ³•ä¸ºä¾æ®è¿›è¡Œæ¨ç†ï¼Œå¹¶é€šè¿‡å¼•å¯¼ä»£ç†æ‰§è¡Œæ›´æœ‰æ•ˆçš„æ”»å‡»ç¨‹åºæ¥è¿‡æ»¤æ‰æ— æ•ˆè¡ŒåŠ¨ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸‰ä¸ªLLMï¼ˆLlama-3-8Bã€Gemini-1.5å’ŒGPT-4ï¼‰æ„å»ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¸—é€æµ‹è¯•LLMä»£ç†ï¼Œå¹¶å°†å…¶åº”ç”¨äºå¯¼èˆª10ä¸ªHackTheBoxç½‘ç»œå®‰å…¨æ¼”ç»ƒä¸­çš„103ä¸ªç¦»æ•£å­ä»»åŠ¡ï¼Œä»£è¡¨çœŸå®ä¸–ç•Œç½‘ç»œæ”»å‡»åœºæ™¯ã€‚æˆ‘ä»¬æå‡ºçš„æ¨ç†ç®¡é“ä½¿ç”¨Llama-3-8Bã€Gemini-1.5å’ŒGPT-4åˆ†åˆ«å¼•å¯¼äº†71.8%ã€72.8%å’Œ78.6%çš„å­ä»»åŠ¡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ€å…ˆè¿›çš„è‡ªæˆ‘æŒ‡å¯¼æ¨ç†çš„LLMæ¸—é€æµ‹è¯•å·¥å…·ä»…å®Œæˆäº†13.5%ã€16.5%å’Œ75.7%çš„å­ä»»åŠ¡ï¼Œå¹¶éœ€è¦é¢å¤–è¿›è¡Œ86.2%ã€118.7%å’Œé«˜è¾¾205.9%çš„æ¨¡å‹æŸ¥è¯¢ã€‚è¿™è¡¨æ˜åœ¨LLMæ¨ç†ç®¡é“ä¸­èå…¥ç¡®å®šæ€§ä»»åŠ¡æ ‘å¯ä»¥æé«˜è‡ªåŠ¨åŒ–ç½‘ç»œå®‰å…¨è¯„ä¼°çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07939v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•ï¼Œè‡ªåŠ¨åŒ–ç½‘ç»œå®‰å…¨æ¸—é€æµ‹è¯•å·¥ä½œæµç¨‹çš„éœ€æ±‚æ—¥ç›Šå¢åŠ ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMä»£ç†ä¸»è¦ä¾èµ–è‡ªæˆ‘å¼•å¯¼æ¨ç†ï¼Œè¿™å¯èƒ½å¯¼è‡´ä¸å‡†ç¡®æˆ–è™šæ„çš„ç¨‹åºæ­¥éª¤ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºæ¸—é€æµ‹è¯•LLMä»£ç†çš„å¼•å¯¼æ¨ç†ç®¡é“ï¼Œé‡‡ç”¨MITRE ATTï¼†CKçŸ©é˜µæ„å»ºçš„ç¡®å®šæ€§ä»»åŠ¡æ ‘ï¼Œçº¦æŸLLMçš„æ¨ç†è¿‡ç¨‹éµå¾ªæ˜ç¡®çš„æˆ˜æœ¯ã€æŠ€æœ¯å’Œç¨‹åºã€‚é€šè¿‡å¼•å¯¼ä»£ç†æ‰§è¡Œæ›´æœ‰æˆæ•ˆçš„æ”»å‡»ç¨‹åºï¼Œè¯¥æ–¹æ³•æ ¹æ¤äºç»è¿‡éªŒè¯çš„æ¸—é€æµ‹è¯•æ–¹æ³•è®ºï¼Œå¹¶è¿‡æ»¤æ‰æ— æ•ˆè¡ŒåŠ¨ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¨ç†ç®¡é“åœ¨æŒ‡å¯¼LLMä»£ç†å®Œæˆç½‘ç»œå®‰å…¨æ¼”ä¹ å­ä»»åŠ¡æ–¹é¢çš„æ•ˆæœæ˜¾è‘—ã€‚ç›¸æ¯”ç°æœ‰è‡ªæˆ‘å¼•å¯¼æ¨ç†çš„LLMæ¸—é€æµ‹è¯•å·¥å…·ï¼Œä½¿ç”¨ç¡®å®šæ€§ä»»åŠ¡æ ‘çš„LLMä»£ç†åœ¨å®Œæˆä»»åŠ¡æ–¹é¢æ›´å‡†ç¡®é«˜æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–ç½‘ç»œå®‰å…¨æ¸—é€æµ‹è¯•é¢†åŸŸå…·æœ‰åº”ç”¨æ½œåŠ›ã€‚</li>
<li>ç°æœ‰LLMä»£ç†åœ¨æ¸—é€æµ‹è¯•ä¸­å­˜åœ¨è‡ªæˆ‘å¼•å¯¼æ¨ç†ä¸å‡†ç¡®çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥ç¡®å®šæ€§ä»»åŠ¡æ ‘èƒ½æœ‰æ•ˆæé«˜LLMä»£ç†åœ¨æ¸—é€æµ‹è¯•ä¸­çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>æå‡ºçš„å¼•å¯¼æ¨ç†ç®¡é“åŸºäºMITRE ATTï¼†CKçŸ©é˜µæ„å»ºï¼Œéµå¾ªæ˜ç¡®çš„æˆ˜æœ¯ã€æŠ€æœ¯å’Œç¨‹åºã€‚</li>
<li>å¼•å¯¼æ¨ç†ç®¡é“æ˜¾è‘—æå‡äº†LLMä»£ç†å®Œæˆç½‘ç»œå®‰å…¨æ¼”ä¹ å­ä»»åŠ¡çš„æ•ˆæœã€‚</li>
<li>ä¸è‡ªæˆ‘å¼•å¯¼æ¨ç†çš„LLMæ¸—é€æµ‹è¯•å·¥å…·ç›¸æ¯”ï¼Œä½¿ç”¨ç¡®å®šæ€§ä»»åŠ¡æ ‘çš„LLMä»£ç†æ›´å‡†ç¡®ã€é«˜æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07939">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07939v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07939v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07939v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Dual-Knowledge-Enhanced-Two-Stage-Reasoner-for-Multimodal-Dialog-Systems"><a href="#Dual-Knowledge-Enhanced-Two-Stage-Reasoner-for-Multimodal-Dialog-Systems" class="headerlink" title="Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems"></a>Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems</h2><p><strong>Authors:Xiaolin Chen, Xuemeng Song, Haokun Wen, Weili Guan, Xiangyu Zhao, Liqiang Nie</strong></p>
<p>Textual response generation is pivotal for multimodal \mbox{task-oriented} dialog systems, which aims to generate proper textual responses based on the multimodal context. While existing efforts have demonstrated remarkable progress, there still exist the following limitations: 1) \textit{neglect of unstructured review knowledge} and 2) \textit{underutilization of large language models (LLMs)}. Inspired by this, we aim to fully utilize dual knowledge (\textit{i.e., } structured attribute and unstructured review knowledge) with LLMs to promote textual response generation in multimodal task-oriented dialog systems. However, this task is non-trivial due to two key challenges: 1) \textit{dynamic knowledge type selection} and 2) \textit{intention-response decoupling}. To address these challenges, we propose a novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for multimodal dialog systems (named DK2R). To be specific, DK2R first extracts both structured attribute and unstructured review knowledge from external knowledge base given the dialog context. Thereafter, DK2R uses an LLM to evaluate each knowledge typeâ€™s utility by analyzing LLM-generated provisional probe responses. Moreover, DK2R separately summarizes the intention-oriented key clues via dedicated reasoning, which are further used as auxiliary signals to enhance LLM-based textual response generation. Extensive experiments conducted on a public dataset verify the superiority of DK2R. We have released the codes and parameters. </p>
<blockquote>
<p>æ–‡æœ¬å“åº”ç”Ÿæˆå¯¹äºå¤šæ¨¡æ€ä»»åŠ¡å¯¼å‘å¯¹è¯ç³»ç»Ÿè‡³å…³é‡è¦ï¼Œè¯¥ç³»ç»Ÿæ—¨åœ¨åŸºäºå¤šæ¨¡æ€ä¸Šä¸‹æ–‡ç”Ÿæˆé€‚å½“çš„æ–‡æœ¬å“åº”ã€‚è™½ç„¶ç°æœ‰çš„åŠªåŠ›å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†ä»ç„¶å­˜åœ¨ä»¥ä¸‹å±€é™æ€§ï¼š1ï¼‰å¿½è§†éç»“æ„åŒ–è¯„è®ºçŸ¥è¯†ï¼›2ï¼‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆ©ç”¨ä¸è¶³ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æ—¨åœ¨å……åˆ†åˆ©ç”¨åŒçŸ¥è¯†ï¼ˆå³ç»“æ„åŒ–å±æ€§çŸ¥è¯†å’Œéç»“æ„åŒ–è¯„è®ºçŸ¥è¯†ï¼‰ä¸LLMæ¥ä¿ƒè¿›å¤šæ¨¡æ€ä»»åŠ¡å¯¼å‘å¯¹è¯ç³»ç»Ÿä¸­çš„æ–‡æœ¬å“åº”ç”Ÿæˆã€‚ç„¶è€Œï¼Œç”±äºä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œè¿™é¡¹ä»»åŠ¡å¹¶ä¸ç®€å•ï¼š1ï¼‰åŠ¨æ€çŸ¥è¯†ç±»å‹é€‰æ‹©ï¼›2ï¼‰æ„å›¾ä¸å“åº”çš„è§£è€¦ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŒçŸ¥è¯†å¢å¼ºä¸¤é˜¶æ®µæ¨ç†å™¨ï¼Œé€šè¿‡é€‚åº”LLMç”¨äºå¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿï¼ˆå‘½åä¸ºDK2Rï¼‰ã€‚å…·ä½“æ¥è¯´ï¼ŒDK2Ré¦–å…ˆä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æå–ç»™å®šå¯¹è¯ä¸Šä¸‹æ–‡çš„ç»“æ„åŒ–å±æ€§çŸ¥è¯†å’Œéç»“æ„åŒ–è¯„è®ºçŸ¥è¯†ã€‚ä¹‹åï¼ŒDK2Rä½¿ç”¨LLMé€šè¿‡åˆ†æLLMç”Ÿæˆçš„ä¸´æ—¶æ¢æµ‹å“åº”æ¥è¯„ä¼°æ¯ç§çŸ¥è¯†çš„å®ç”¨æ€§ã€‚æ­¤å¤–ï¼ŒDK2Ré€šè¿‡ä¸“é¡¹æ¨ç†å•ç‹¬æ€»ç»“ä»¥æ„å›¾ä¸ºå¯¼å‘çš„å…³é”®çº¿ç´¢ï¼Œè¿™äº›çº¿ç´¢è¿›ä¸€æ­¥ä½œä¸ºè¾…åŠ©ä¿¡å·ï¼Œå¢å¼ºåŸºäºLLMçš„æ–‡æœ¬å“åº”ç”Ÿæˆã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒéªŒè¯äº†DK2Rçš„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬å·²ç»å‘å¸ƒäº†ä»£ç å’Œå‚æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07817v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨å¤šæ¨¡æ€ä»»åŠ¡å¯¼å‘å‹å¯¹è¯ç³»ç»Ÿä¸­ï¼Œæ–‡æœ¬å“åº”ç”Ÿæˆçš„é‡è¦æ€§åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚æ–‡ç« æŒ‡å‡ºï¼Œç°æœ‰æ–¹æ³•å¿½è§†äº†éç»“æ„åŒ–è¯„è®ºçŸ¥è¯†å¹¶æœªèƒ½å……åˆ†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨åŒçŸ¥è¯†ï¼ˆç»“æ„åŒ–å±æ€§ä¸éç»“æ„åŒ–è¯„è®ºçŸ¥è¯†ï¼‰ä¸LLMsä¿ƒè¿›æ–‡æœ¬å“åº”ç”Ÿæˆçš„æ–¹æ³•ï¼Œç§°ä¸ºDK2Rã€‚è¯¥æ–¹æ³•é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šåŠ¨æ€çŸ¥è¯†ç±»å‹é€‰æ‹©å’Œæ„å›¾å“åº”è§£è€¦ã€‚DK2Ré¦–å…ˆä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æå–ç»™å®šå¯¹è¯è¯­å¢ƒä¸‹çš„ç»“æ„åŒ–å±æ€§ä¸éç»“æ„åŒ–è¯„è®ºçŸ¥è¯†ï¼Œç„¶åä½¿ç”¨LLMè¯„ä¼°æ¯ç§çŸ¥è¯†çš„æ•ˆç”¨ã€‚æ­¤å¤–ï¼ŒDK2Ré€šè¿‡ä¸“é¡¹æ¨ç†æ€»ç»“æ„å›¾å¯¼å‘çš„å…³é”®çº¿ç´¢ï¼Œä½œä¸ºè¾…åŠ©ä¿¡å·å¢å¼ºLLMçš„æ–‡æœ¬å“åº”ç”Ÿæˆèƒ½åŠ›ã€‚åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†DK2Rçš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€ä»»åŠ¡å¯¼å‘å‹å¯¹è¯ç³»ç»Ÿä¸­ï¼Œæ–‡æœ¬å“åº”ç”Ÿæˆéå¸¸é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨åˆ©ç”¨éç»“æ„åŒ–è¯„è®ºçŸ¥è¯†å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æ–‡ç« æå‡ºäº†DK2Ræ–¹æ³•ï¼Œæ—¨åœ¨å……åˆ†åˆ©ç”¨åŒçŸ¥è¯†ï¼ˆç»“æ„åŒ–å±æ€§ä¸éç»“æ„åŒ–è¯„è®ºçŸ¥è¯†ï¼‰å¹¶é€‚åº”LLMsä»¥ä¿ƒè¿›æ–‡æœ¬å“åº”ç”Ÿæˆã€‚</li>
<li>DK2Ré¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šåŠ¨æ€çŸ¥è¯†ç±»å‹é€‰æ‹©å’Œæ„å›¾å“åº”è§£è€¦ã€‚</li>
<li>DK2Rèƒ½ä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æå–å¯¹è¯è¯­å¢ƒä¸‹çš„çŸ¥è¯†ï¼Œå¹¶ä½¿ç”¨LLMè¯„ä¼°çŸ¥è¯†çš„æ•ˆç”¨ã€‚</li>
<li>DK2Ré€šè¿‡ä¸“é¡¹æ¨ç†æ€»ç»“æ„å›¾å¯¼å‘çš„å…³é”®çº¿ç´¢ï¼Œä»¥å¢å¼ºLLMçš„æ–‡æœ¬å“åº”ç”Ÿæˆèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07817v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07817v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07817v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CAViAR-Critic-Augmented-Video-Agentic-Reasoning"><a href="#CAViAR-Critic-Augmented-Video-Agentic-Reasoning" class="headerlink" title="CAViAR: Critic-Augmented Video Agentic Reasoning"></a>CAViAR: Critic-Augmented Video Agentic Reasoning</h2><p><strong>Authors:Sachit Menon, Ahmet Iscen, Arsha Nagrani, Tobias Weyand, Carl Vondrick, Cordelia Schmid</strong></p>
<p>Video understanding has seen significant progress in recent years, with modelsâ€™ performance on perception from short clips continuing to rise. Yet, multiple recent benchmarks, such as LVBench, Neptune, and ActivityNet-RTL, show performance wanes for tasks requiring complex reasoning on videos as queries grow more complex and videos grow longer. In this work, we ask: can existing perception capabilities be leveraged to successfully perform more complex video reasoning? In particular, we develop a large language model agent given access to video modules as subagents or tools. Rather than following a fixed procedure to solve queries as in previous work such as Visual Programming, ViperGPT, and MoReVQA, the agent uses the results of each call to a module to determine subsequent steps. Inspired by work in the textual reasoning domain, we introduce a critic to distinguish between instances of successful and unsuccessful sequences from the agent. We show that the combination of our agent and critic achieve strong performance on the previously-mentioned datasets. </p>
<blockquote>
<p>è§†é¢‘ç†è§£åœ¨è¿‘å¹´æ¥å–å¾—äº†é‡å¤§è¿›å±•ï¼Œæ¨¡å‹åœ¨çŸ­ç‰‡æ®µæ„ŸçŸ¥æ–¹é¢çš„æ€§èƒ½æŒç»­æé«˜ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„å¤šä¸ªåŸºå‡†æµ‹è¯•ï¼Œå¦‚LVBenchã€Neptuneå’Œæ´»åŠ¨ç½‘RTLï¼Œæ˜¾ç¤ºéšç€æŸ¥è¯¢çš„å¤æ‚æ€§å’Œè§†é¢‘çš„æŒç»­å¢é•¿ï¼Œéœ€è¦å¤æ‚è§†é¢‘æ¨ç†çš„ä»»åŠ¡çš„æ€§èƒ½ä¼šä¸‹é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºçš„é—®é¢˜æ˜¯ï¼šç°æœ‰çš„æ„ŸçŸ¥èƒ½åŠ›èƒ½å¦è¢«æˆåŠŸåˆ©ç”¨æ¥æ‰§è¡Œæ›´å¤æ‚çš„è§†é¢‘æ¨ç†ï¼Ÿç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ï¼Œè¯¥ä»£ç†å¯ä»¥è®¿é—®è§†é¢‘æ¨¡å—ä½œä¸ºå­ä»£ç†æˆ–å·¥å…·ã€‚ä¸ä¼ ç»Ÿçš„éµå¾ªå›ºå®šç¨‹åºè§£å†³æŸ¥è¯¢çš„æ–¹æ³•ä¸åŒï¼ˆå¦‚è§†è§‰ç¼–ç¨‹ã€ViperGPTå’ŒMoReVQAï¼‰ï¼Œä»£ç†æ ¹æ®å¯¹æ¯ä¸ªæ¨¡å—çš„è°ƒç”¨ç»“æœæ¥ç¡®å®šä¸‹ä¸€æ­¥æ“ä½œã€‚å—æ–‡æœ¬æ¨ç†é¢†åŸŸå·¥ä½œçš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯„è®ºå®¶æ¥åŒºåˆ†ä»£ç†æˆåŠŸå’Œå¤±è´¥çš„åºåˆ—å®ä¾‹ã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„ä»£ç†å’Œè¯„è®ºå®¶çš„ç»„åˆåœ¨ä¹‹å‰æåˆ°çš„æ•°æ®é›†ä¸Šå–å¾—äº†å¼ºå¤§çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07680v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è§†é¢‘ç†è§£é¢†åŸŸçš„æœ€æ–°è¿›å±•å’ŒæŒ‘æˆ˜ã€‚å°½ç®¡æ¨¡å‹åœ¨çŸ­ç‰‡æ®µæ„ŸçŸ¥æ–¹é¢çš„æ€§èƒ½ä¸æ–­æé«˜ï¼Œä½†åœ¨éœ€è¦å¤æ‚è§†é¢‘æ¨ç†çš„ä»»åŠ¡ä¸­ï¼Œéšç€æŸ¥è¯¢çš„å¤æ‚æ€§å’Œè§†é¢‘é•¿åº¦çš„å¢åŠ ï¼Œæ€§èƒ½ä»ç„¶ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨ç°æœ‰æ„ŸçŸ¥èƒ½åŠ›æˆåŠŸæ‰§è¡Œæ›´å¤æ‚çš„è§†é¢‘æ¨ç†çš„æ–¹æ³•ï¼Œå³å¼€å‘ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ï¼Œè¯¥ä»£ç†å¯ä»¥è®¿é—®è§†é¢‘æ¨¡å—ä½œä¸ºå­ä»£ç†æˆ–å·¥å…·ã€‚ä¸ä»¥å‰çš„å·¥ä½œï¼ˆå¦‚è§†è§‰ç¼–ç¨‹ã€ViperGPTå’ŒMoReVQAï¼‰ä¸åŒï¼Œä»£ç†ä½¿ç”¨å¯¹æ¯ä¸ªæ¨¡å—çš„è°ƒç”¨ç»“æœæ¥ç¡®å®šåç»­æ­¥éª¤ã€‚å—åˆ°æ–‡æœ¬æ¨ç†é¢†åŸŸå·¥ä½œçš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¯„åˆ¤å®¶æ¥åŒºåˆ†ä»£ç†æˆåŠŸå’Œå¤±è´¥çš„åºåˆ—å®ä¾‹ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ä»£ç†å’Œè¯„åˆ¤å®¶çš„ç»„åˆåœ¨ä¹‹å‰æåˆ°çš„æ•°æ®é›†ä¸Šå–å¾—äº†å¼ºå¤§çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘ç†è§£é¢†åŸŸè™½ç„¶å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é¢å¯¹å¤æ‚æŸ¥è¯¢å’Œé•¿è§†é¢‘æ—¶ä»é¢ä¸´æ€§èƒ½ä¸‹é™çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ï¼Œèƒ½å¤Ÿåˆ©ç”¨ç°æœ‰æ„ŸçŸ¥èƒ½åŠ›æ‰§è¡Œæ›´å¤æ‚çš„è§†é¢‘æ¨ç†ã€‚</li>
<li>ä»£ç†å¯ä»¥è®¿é—®è§†é¢‘æ¨¡å—ä½œä¸ºå­ä»£ç†æˆ–å·¥å…·ï¼Œå¹¶æ ¹æ®æ¨¡å—ç»“æœåŠ¨æ€ç¡®å®šåç»­æ­¥éª¤ã€‚</li>
<li>ä¸å…ˆå‰çš„å·¥ä½œä¸åŒï¼Œè¯¥ä»£ç†ä¸éµå¾ªå›ºå®šç¨‹åºæ¥è§£å†³æŸ¥è¯¢ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªè¯„åˆ¤å®¶æ¥åŒºåˆ†ä»£ç†æˆåŠŸå’Œå¤±è´¥çš„åºåˆ—å®ä¾‹ï¼Œä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•å’Œè¯„åˆ¤å®¶çš„ç»„åˆåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>ä¸ºè§£å†³è§†é¢‘æ¨ç†ä¸­çš„å¤æ‚ä»»åŠ¡æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07680">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07680v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07680v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07680v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="K2-Think-A-Parameter-Efficient-Reasoning-System"><a href="#K2-Think-A-Parameter-Efficient-Reasoning-System" class="headerlink" title="K2-Think: A Parameter-Efficient Reasoning System"></a>K2-Think: A Parameter-Efficient Reasoning System</h2><p><strong>Authors:Zhoujun Cheng, Richard Fan, Shibo Hao, Taylor W. Killian, Haonan Li, Suqi Sun, Hector Ren, Alexander Moreno, Daqian Zhang, Tianjun Zhong, Yuxin Xiong, Yuanzhe Hu, Yutao Xie, Xudong Han, Yuqi Wang, Varad Pimpalkhute, Yonghao Zhuang, Aaryamonvikram Singh, Xuezhi Liang, Anze Xie, Jianshu She, Desai Fan, Chengqian Gao, Liqun Ma, Mikhail Yurochkin, John Maggs, Xuezhe Ma, Guowei He, Zhiting Hu, Zhengzhong Liu, Eric P. Xing</strong></p>
<p>K2-Think is a reasoning system that achieves state-of-the-art performance with a 32B parameter model, matching or surpassing much larger models like GPT-OSS 120B and DeepSeek v3.1. Built on the Qwen2.5 base model, our system shows that smaller models can compete at the highest levels by combining advanced post-training and test-time computation techniques. The approach is based on six key technical pillars: Long Chain-of-thought Supervised Finetuning, Reinforcement Learning with Verifiable Rewards (RLVR), Agentic planning prior to reasoning, Test-time Scaling, Speculative Decoding, and Inference-optimized Hardware, all using publicly available open-source datasets. K2-Think excels in mathematical reasoning, achieving state-of-the-art scores on public benchmarks for open-source models, while also performing strongly in other areas such as Code and Science. Our results confirm that a more parameter-efficient model like K2-Think 32B can compete with state-of-the-art systems through an integrated post-training recipe that includes long chain-of-thought training and strategic inference-time enhancements, making open-source reasoning systems more accessible and affordable. K2-Think is freely available at k2think.ai, offering best-in-class inference speeds of over 2,000 tokens per second per request via the Cerebras Wafer-Scale Engine. </p>
<blockquote>
<p>K2-Thinkæ˜¯ä¸€ä¸ªæ¨ç†ç³»ç»Ÿï¼Œå®ƒä½¿ç”¨32Bå‚æ•°æ¨¡å‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œå¯ä»¥åŒ¹é…æˆ–è¶…è¶ŠåƒGPT-OSS 120Bå’ŒDeepSeek v3.1ç­‰å¤§å‹æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿå»ºç«‹åœ¨Qwen2.5åŸºç¡€æ¨¡å‹ä¸Šï¼Œé€šè¿‡ç»“åˆå…ˆè¿›çš„åè®­ç»ƒå’Œæµ‹è¯•æ—¶é—´è®¡ç®—æŠ€æœ¯ï¼Œè¯æ˜å°å‹æ¨¡å‹å¯ä»¥åœ¨æœ€é«˜çº§åˆ«ä¸Šç«äº‰ã€‚è¯¥æ–¹æ³•åŸºäºå…­å¤§å…³é”®æŠ€æœ¯æ”¯æŸ±ï¼šé•¿æ€è€ƒé“¾ç›‘ç£å¾®è°ƒã€å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ã€æ¨ç†å‰çš„ä»£ç†è®¡åˆ’ã€æµ‹è¯•æ—¶ç¼©æ”¾ã€æŠ•æœºè§£ç å’Œæ¨ç†ä¼˜åŒ–ç¡¬ä»¶ï¼Œæ‰€æœ‰è¿™äº›éƒ½ä½¿ç”¨å…¬å¼€å¯ç”¨çš„å¼€æºæ•°æ®é›†ã€‚K2-Thinkåœ¨æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œåœ¨å…¬å¼€åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†å¼€æºæ¨¡å‹çš„æœ€å…ˆè¿›æ°´å¹³ï¼ŒåŒæ—¶åœ¨ä»£ç å’Œç§‘å­¦ç­‰å…¶ä»–é¢†åŸŸä¹Ÿè¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬çš„ç»“æœè¯å®ï¼ŒåƒK2-Think 32Bè¿™æ ·æ›´å‚æ•°é«˜æ•ˆçš„æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡åŒ…æ‹¬é•¿æ€è€ƒé“¾è®­ç»ƒå’Œæˆ˜ç•¥æ¨ç†æ—¶é—´å¢å¼ºåœ¨å†…çš„ç»¼åˆåè®­ç»ƒé…æ–¹ï¼Œä¸æœ€å…ˆè¿›çš„ç³»ç»Ÿè¿›è¡Œç«äº‰ï¼Œä½¿å¼€æºæ¨ç†ç³»ç»Ÿæ›´åŠ æ˜“äºè®¿é—®å’Œè´Ÿæ‹…å¾—èµ·ã€‚K2-Thinkå¯åœ¨k2think.aiä¸Šå…è´¹è·å¾—ï¼Œé€šè¿‡Cerebrasæ™¶åœ†çº§å¼•æ“æä¾›æ¯ç§’è¶…è¿‡2000ä»¤ç‰Œçš„é¡¶çº§æ¨ç†é€Ÿåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07604v1">PDF</a> To access the K2-Think reasoning system, please visit   <a target="_blank" rel="noopener" href="https://k2think.ai/">https://k2think.ai</a></p>
<p><strong>Summary</strong></p>
<p>K2-Thinkæ˜¯ä¸€æ¬¾åŸºäºQwen2.5åŸºç¡€æ¨¡å‹çš„æ¨ç†ç³»ç»Ÿï¼Œå…·æœ‰å…ˆè¿›çš„åè®­ç»ƒå’Œæµ‹è¯•æ—¶é—´è®¡ç®—æŠ€æœ¯ï¼Œèƒ½ä»¥è¾ƒå°çš„æ¨¡å‹å‚æ•°å®ç°å“è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚ç³»ç»Ÿå…·å¤‡å…­å¤§å…³é”®æŠ€æœ¯æ”¯æŸ±ï¼ŒåŒ…æ‹¬é•¿é“¾æ€ç»´ç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ç­‰ã€‚K2-Thinkåœ¨æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°çªå‡ºï¼ŒåŒæ—¶åœ¨ä»£ç å’Œç§‘å­¦é¢†åŸŸä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„å®åŠ›ã€‚è¯¥ç³»ç»Ÿé€šè¿‡é›†æˆåŒ–çš„åè®­ç»ƒæ–¹æ¡ˆå’Œç­–ç•¥æ€§çš„æ¨ç†æ—¶é—´å¢å¼ºåŠŸèƒ½ï¼Œä¸å¤§å‹æ¨¡å‹ç›¸æ¯”å±•ç°å‡ºæ›´é«˜çš„å‚æ•°æ•ˆç‡ã€‚K2-Thinkå·²åœ¨k2think.aiä¸Šå…è´¹æä¾›ï¼Œå¹¶æä¾›äº†æœ€ä½³çš„æ¨ç†é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>K2-Thinkæ˜¯ä¸€ä¸ªåŸºäºQwen2.5åŸºç¡€æ¨¡å‹çš„å…ˆè¿›æ¨ç†ç³»ç»Ÿã€‚</li>
<li>é€šè¿‡ç»“åˆåè®­ç»ƒå’Œæµ‹è¯•æ—¶é—´è®¡ç®—æŠ€æœ¯ï¼Œå®ç°äº†å°æ¨¡å‹çš„é«˜æ€§èƒ½è¡¨ç°ã€‚</li>
<li>ç³»ç»Ÿå…·æœ‰å…­å¤§å…³é”®æŠ€æœ¯æ”¯æŸ±ï¼ŒåŒ…æ‹¬é•¿é“¾æ€ç»´ç›‘ç£å¾®è°ƒç­‰ã€‚</li>
<li>åœ¨æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å“è¶Šï¼ŒåŒæ—¶åœ¨ä»£ç å’Œç§‘å­¦é¢†åŸŸä¹Ÿæœ‰å‡ºè‰²è¡¨ç°ã€‚</li>
<li>ä¸å¤§å‹æ¨¡å‹ç›¸æ¯”ï¼ŒK2-Thinkå±•ç°å‡ºæ›´é«˜çš„å‚æ•°æ•ˆç‡ã€‚</li>
<li>K2-Thinkç³»ç»Ÿå·²åœ¨k2think.aiä¸Šå…è´¹æä¾›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07604v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07604v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07604v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07604v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Î”L-Normalization-Rethink-Loss-Aggregation-in-RLVR"><a href="#Î”L-Normalization-Rethink-Loss-Aggregation-in-RLVR" class="headerlink" title="$Î”L$ Normalization: Rethink Loss Aggregation in RLVR"></a>$Î”L$ Normalization: Rethink Loss Aggregation in RLVR</h2><p><strong>Authors:Zhiyuan He, Xufang Luo, Yike Zhang, Yuqing Yang, Lili Qiu</strong></p>
<p>We propose $\Delta L$ Normalization, a simple yet effective loss aggregation method tailored to the characteristic of dynamic generation lengths in Reinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has demonstrated strong potential in improving the reasoning capabilities of large language models (LLMs), but a major challenge lies in the large variability of response lengths during training, which leads to high gradient variance and unstable optimization. Although previous methods such as GRPO, DAPO, and Dr. GRPO introduce different loss normalization terms to address this issue, they either produce biased estimates or still suffer from high gradient variance. By analyzing the effect of varying lengths on policy loss both theoretically and empirically, we reformulate the problem as finding a minimum-variance unbiased estimator. Our proposed $\Delta L$ Normalization not only provides an unbiased estimate of the true policy loss but also minimizes gradient variance in theory. Extensive experiments show that it consistently achieves superior results across different model sizes, maximum lengths, and tasks. Our code will be made public at <a target="_blank" rel="noopener" href="https://github.com/zerolllin/Delta-L-Normalization">https://github.com/zerolllin/Delta-L-Normalization</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†Î”Lå½’ä¸€åŒ–ï¼ˆ$\Delta L$ Normalizationï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æŸå¤±èšåˆæ–¹æ³•ï¼Œé’ˆå¯¹å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä¸­åŠ¨æ€ç”Ÿæˆé•¿åº¦çš„ç‰¹æ€§è¿›è¡Œå®šåˆ¶ã€‚æœ€è¿‘ï¼ŒRLVRåœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†è®­ç»ƒè¿‡ç¨‹ä¸­å“åº”é•¿åº¦å­˜åœ¨å¾ˆå¤§å·®å¼‚ï¼Œå¯¼è‡´æ¢¯åº¦æ–¹å·®é«˜å’Œä¼˜åŒ–ä¸ç¨³å®šï¼Œè¿™æˆä¸ºäº†ä¸€å¤§æŒ‘æˆ˜ã€‚å°½ç®¡ä¹‹å‰çš„æ–¹æ³•å¦‚GRPOã€DAPOå’ŒDr. GRPOå¼•å…¥äº†ä¸åŒçš„æŸå¤±å½’ä¸€åŒ–æœ¯è¯­æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†å®ƒä»¬è¦ä¹ˆäº§ç”Ÿæœ‰åä¼°è®¡ï¼Œè¦ä¹ˆä»ç„¶å—åˆ°é«˜æ¢¯åº¦æ–¹å·®çš„å½±å“ã€‚æˆ‘ä»¬é€šè¿‡ç†è®ºåˆ†æå’Œå®è¯ç ”ç©¶ï¼Œåˆ†æäº†ä¸åŒé•¿åº¦å¯¹ç­–ç•¥æŸå¤±çš„å½±å“ï¼Œå°†é—®é¢˜é‡æ–°å®šä¹‰ä¸ºå¯»æ‰¾æœ€å°æ–¹å·®çš„æ— åä¼°è®¡å™¨ã€‚æˆ‘ä»¬æå‡ºçš„Î”Lå½’ä¸€åŒ–ä¸ä»…ä¸ºçœŸå®çš„ç­–ç•¥æŸå¤±æä¾›äº†æ— åä¼°è®¡ï¼Œè€Œä¸”åœ¨ç†è®ºä¸Šè¿˜æœ€å°åŒ–äº†æ¢¯åº¦æ–¹å·®ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œå®ƒåœ¨ä¸åŒçš„æ¨¡å‹å¤§å°ã€æœ€å¤§é•¿åº¦å’Œä»»åŠ¡ä¸­å§‹ç»ˆå–å¾—æ›´å¥½çš„ç»“æœã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/zerolllin/Delta-L-Normalization%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/zerolllin/Delta-L-Normalizationå…¬å¼€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07558v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>$\Delta L$å½’ä¸€åŒ–æ˜¯ä¸€ç§é’ˆå¯¹å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä¸­åŠ¨æ€ç”Ÿæˆé•¿åº¦ç‰¹æ€§çš„ç®€å•æœ‰æ•ˆçš„æŸå¤±èšåˆæ–¹æ³•ã€‚é’ˆå¯¹LLMæ¨ç†èƒ½åŠ›æå‡çš„RLVRæ–¹æ³•é¢ä¸´è®­ç»ƒæ—¶å›åº”é•¿åº¦å˜åŒ–å¤§çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´æ¢¯åº¦æ–¹å·®é«˜å’Œä¼˜åŒ–ä¸ç¨³å®šã€‚ä¹‹å‰çš„æ–¹æ³•å¦‚GRPOã€DAPOå’ŒDr. GRPOè™½ç„¶å¼•å…¥äº†ä¸åŒçš„æŸå¤±å½’ä¸€åŒ–æœ¯è¯­æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†å®ƒä»¬è¦ä¹ˆäº§ç”Ÿæœ‰åä¼°è®¡ï¼Œè¦ä¹ˆä»å­˜åœ¨é«˜æ¢¯åº¦æ–¹å·®é—®é¢˜ã€‚é€šè¿‡åˆ†æä¸åŒé•¿åº¦å¯¹ç­–ç•¥æŸå¤±çš„å½±å“ï¼Œæˆ‘ä»¬å°†é—®é¢˜é‡æ–°å®šä¹‰ä¸ºå¯»æ‰¾æœ€å°æ–¹å·®æ— åä¼°è®¡å™¨ã€‚$\Delta L$å½’ä¸€åŒ–ä¸ä»…ä¸ºçœŸæ­£çš„æ”¿ç­–æŸå¤±æä¾›äº†æ— åä¼°è®¡ï¼Œè€Œä¸”åœ¨ç†è®ºä¸Šä¹Ÿæœ€å°åŒ–äº†æ¢¯åº¦æ–¹å·®ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œå®ƒåœ¨ä¸åŒçš„æ¨¡å‹å¤§å°ã€æœ€å¤§é•¿åº¦å’Œä»»åŠ¡ä¸­å§‹ç»ˆå–å¾—æ›´å¥½çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>$\Delta L$å½’ä¸€åŒ–æ˜¯ä¸€ç§é’ˆå¯¹å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä¸­çš„æŸå¤±èšåˆæ–¹æ³•ã€‚</li>
<li>RLVRåœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>è®­ç»ƒæ—¶å›åº”é•¿åº¦å˜åŒ–å¤§æ˜¯RLVRé¢ä¸´çš„ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå¯¼è‡´æ¢¯åº¦æ–¹å·®é«˜å’Œä¼˜åŒ–ä¸ç¨³å®šã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚GRPOã€DAPOå’ŒDr. GRPOè™½å°è¯•è§£å†³æ­¤é—®é¢˜ï¼Œä½†ä»å­˜åœ¨åå·®å’Œæ¢¯åº¦æ–¹å·®é—®é¢˜ã€‚</li>
<li>$\Delta L$å½’ä¸€åŒ–é€šè¿‡å¯»æ‰¾æœ€å°æ–¹å·®æ— åä¼°è®¡å™¨æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>å®è¯åˆ†ææ˜¾ç¤º$\Delta L$å½’ä¸€åŒ–åœ¨ä¸åŒæ¨¡å‹ã€æœ€å¤§é•¿åº¦å’Œä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>ç›¸å…³ä»£ç å°†å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/zerolllin/Delta-L-Normalization%E3%80%82">https://github.com/zerolllin/Delta-L-Normalizationã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07558">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07558v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07558v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07558v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="PersonaFuse-A-Personality-Activation-Driven-Framework-for-Enhancing-Human-LLM-Interactions"><a href="#PersonaFuse-A-Personality-Activation-Driven-Framework-for-Enhancing-Human-LLM-Interactions" class="headerlink" title="PersonaFuse: A Personality Activation-Driven Framework for Enhancing   Human-LLM Interactions"></a>PersonaFuse: A Personality Activation-Driven Framework for Enhancing   Human-LLM Interactions</h2><p><strong>Authors:Yixuan Tang, Yi Yang, Ahmed Abbasi</strong></p>
<p>Recent advancements in Large Language Models (LLMs) demonstrate remarkable capabilities across various fields. These developments have led to more direct communication between humans and LLMs in various situations, such as social companionship and psychological support. However, LLMs often exhibit limitations in emotional perception and social competence during real-world conversations. These limitations partly originate from their inability to adapt their communication style and emotional expression to different social and task contexts. In this work, we introduce PersonaFuse, a novel LLM post-training framework that enables LLMs to adapt and express different personalities for varying situations. Inspired by Trait Activation Theory and the Big Five personality model, PersonaFuse employs a Mixture-of-Expert architecture that combines persona adapters with a dynamic routing network, enabling contextual trait expression. Experimental results show that PersonaFuse substantially outperforms baseline models across multiple dimensions of social-emotional intelligence. Importantly, these gains are achieved without sacrificing general reasoning ability or model safety, which remain common limitations of direct prompting and supervised fine-tuning approaches. PersonaFuse also delivers consistent improvements in downstream human-centered applications, such as mental health counseling and review-based customer service. Finally, human preference evaluations against leading LLMs, including GPT-4o and DeepSeek, demonstrate that PersonaFuse achieves competitive response quality despite its comparatively smaller model size. These findings demonstrate that PersonaFuse~offers a theoretically grounded and practical approach for developing social-emotional enhanced LLMs, marking a significant advancement toward more human-centric AI systems. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥åœ¨å„ç§é¢†åŸŸéƒ½å±•ç°å‡ºäº†æ˜¾è‘—çš„å®åŠ›ã€‚è¿™äº›å‘å±•ä½¿å¾—äººç±»åœ¨å„ç§æƒ…å¢ƒä¸‹ä¸LLMè¿›è¡Œæ›´ç›´æ¥çš„äº¤æµï¼Œå¦‚åœ¨ç¤¾ä¼šé™ªä¼´å’Œå¿ƒç†æ”¯æŒæ–¹é¢ã€‚ç„¶è€Œï¼Œåœ¨çœŸå®ä¸–ç•Œçš„å¯¹è¯ä¸­ï¼ŒLLMåœ¨æƒ…æ„Ÿæ„ŸçŸ¥å’Œç¤¾ä¼šèƒ½åŠ›æ–¹é¢å¾€å¾€è¡¨ç°å‡ºå±€é™ã€‚è¿™äº›å±€é™æ€§éƒ¨åˆ†æºäºå®ƒä»¬æ— æ³•é€‚åº”ä¸åŒçš„ç¤¾ä¼šå’Œä»»åŠ¡ä¸Šä¸‹æ–‡æ¥è°ƒæ•´å…¶äº¤æµé£æ ¼å’Œæƒ…æ„Ÿè¡¨è¾¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†PersonaFuseï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹LLMåè®­ç»ƒæ¡†æ¶ï¼Œèƒ½å¤Ÿä½¿LLMé€‚åº”å¹¶è¡¨è¾¾ä¸åŒæƒ…å¢ƒä¸‹çš„ä¸åŒäººæ ¼ã€‚å—ç‰¹è´¨æ¿€æ´»ç†è®ºå’Œå¤§äº”äººæ ¼æ¨¡å‹çš„å¯å‘ï¼ŒPersonaFuseé‡‡ç”¨æ··åˆä¸“å®¶æ¶æ„ï¼Œå°†äººæ ¼é€‚é…å™¨ä¸åŠ¨æ€è·¯ç”±ç½‘ç»œç›¸ç»“åˆï¼Œå®ç°ä¸Šä¸‹æ–‡ç‰¹è´¨è¡¨è¾¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPersonaFuseåœ¨ç¤¾ä¼šæƒ…æ„Ÿæ™ºåŠ›çš„å¤šä¸ªç»´åº¦ä¸Šæ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™äº›æ”¶ç›Šçš„å®ç°å¹¶ä¸ä¼šç‰ºç‰²ä¸€èˆ¬çš„æ¨ç†èƒ½åŠ›æˆ–æ¨¡å‹å®‰å…¨æ€§ï¼Œè¿™ä»ç„¶æ˜¯ç›´æ¥æç¤ºå’Œç›‘ç£å¾®è°ƒæ–¹æ³•çš„å¸¸è§å±€é™æ€§ã€‚PersonaFuseåœ¨ä¸‹æ¸¸ä»¥äººä¸ºä¸­å¿ƒçš„åº”ç”¨ä¸­ä¹Ÿå®ç°äº†æŒç»­çš„æ”¹è¿›ï¼Œå¦‚å¿ƒç†å¥åº·å’¨è¯¢å’ŒåŸºäºè¯„è®ºçš„å®¢æˆ·æœåŠ¡ã€‚æœ€åï¼Œä¸é¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œçš„äººç±»åå¥½è¯„ä¼°ï¼ŒåŒ…æ‹¬GPT-4oå’ŒDeepSeekï¼Œè¯æ˜å°½ç®¡PersonaFuseçš„æ¨¡å‹è§„æ¨¡è¾ƒå°ï¼Œä½†åœ¨å“åº”è´¨é‡æ–¹é¢ä»å…·æœ‰ç«äº‰åŠ›ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒPersonaFuseä¸ºå¼€å‘ç¤¾ä¼šæƒ…æ„Ÿå¢å¼ºå‹LLMæä¾›äº†ç†è®ºæ‰å®ä¸”å®ç”¨çš„æ–¹æ³•ï¼Œæ ‡å¿—ç€å‘æ›´ä»¥äººç±»ä¸ºä¸­å¿ƒçš„äººå·¥æ™ºèƒ½ç³»ç»Ÿè¿ˆå‡ºäº†é‡å¤§æ­¥ä¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07370v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥å±•ç°äº†å…¶åœ¨å¤šé¢†åŸŸçš„æ˜¾è‘—èƒ½åŠ›ï¼Œå¹¶æ¨åŠ¨å…¶ä¸äººç±»çš„ç›´æ¥äº¤æµï¼Œå¦‚åœ¨ç¤¾äº¤é™ªä¼´å’Œå¿ƒç†æ”¯æŒæ–¹é¢ã€‚ç„¶è€Œï¼ŒLLMåœ¨çœŸå®å¯¹è¯ä¸­çš„æƒ…æ„Ÿæ„ŸçŸ¥å’Œç¤¾ä¼šäº¤å¾€èƒ½åŠ›å­˜åœ¨å±€é™ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹LLMåè®­ç»ƒæ¡†æ¶â€”â€”PersonaFuseï¼Œå®ƒèƒ½æ ¹æ®ä¸åŒæƒ…å¢ƒä½¿LLMé€‚åº”å¹¶è¡¨è¾¾ä¸åŒäººæ ¼ã€‚å—ç‰¹è´¨æ¿€æ´»ç†è®ºå’Œäº”å¤§äººæ ¼æ¨¡å‹çš„å¯å‘ï¼ŒPersonaFuseé‡‡ç”¨æ··åˆä¸“å®¶æ¶æ„ï¼Œç»“åˆäººæ ¼é€‚é…å™¨å’ŒåŠ¨æ€è·¯ç”±ç½‘ç»œï¼Œå®ç°æƒ…å¢ƒç‰¹è´¨è¡¨è¾¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPersonaFuseåœ¨ç¤¾ä¼šæƒ…æ„Ÿæ™ºèƒ½çš„å¤šä¸ªç»´åº¦ä¸Šå¤§å¹…è¶…è¶ŠåŸºå‡†æ¨¡å‹ï¼Œä¸”åœ¨ä¸ç‰ºç‰²é€šç”¨æ¨ç†èƒ½åŠ›å’Œæ¨¡å‹å®‰å…¨æ€§çš„å‰æä¸‹å–å¾—è¿™äº›æˆæœã€‚æ­¤å¤–ï¼ŒPersonaFuseåœ¨å¿ƒç†å¥åº·å’¨è¯¢å’ŒåŸºäºè¯„è®ºçš„å®¢æˆ·æœåŠ¡ç­‰äººç±»ä¸ºä¸­å¿ƒçš„åº”ç”¨ä¸­è¡¨ç°ä¼˜å¼‚ã€‚ä¸äººç±»åå¥½è¯„ä»·é¢†å…ˆçš„LLMç›¸æ¯”ï¼Œå¦‚GPT-4oå’ŒDeepSeekï¼Œå°½ç®¡æ¨¡å‹è§„æ¨¡è¾ƒå°ï¼Œä½†PersonaFuseåœ¨å“åº”è´¨é‡æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒPersonaFuseä¸ºå¼€å‘ç¤¾ä¼šæƒ…æ„Ÿå¢å¼ºå‹LLMæä¾›äº†ç†è®ºæ”¯æŒå’Œå®è·µæ–¹æ³•ï¼Œæ ‡å¿—ç€äººå·¥æ™ºèƒ½ç³»ç»Ÿæ›´åŠ ä»¥äººç±»ä¸ºä¸­å¿ƒçš„é‡è¦è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMsåœ¨å¤šä¸ªé¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä¿ƒè¿›äº†ä¸äººç±»çš„ç›´æ¥äº¤æµã€‚</li>
<li>LLMsåœ¨çœŸå®å¯¹è¯ä¸­çš„æƒ…æ„Ÿæ„ŸçŸ¥å’Œç¤¾ä¼šäº¤å¾€èƒ½åŠ›å­˜åœ¨å±€é™ã€‚</li>
<li>PersonaFuseæ˜¯ä¸€ç§æ–°å‹LLMåè®­ç»ƒæ¡†æ¶ï¼Œä½¿LLMèƒ½é€‚åº”å¹¶è¡¨è¾¾ä¸åŒäººæ ¼ã€‚</li>
<li>PersonaFuseç»“åˆç‰¹è´¨æ¿€æ´»ç†è®ºå’Œäº”å¤§äººæ ¼æ¨¡å‹ï¼Œé‡‡ç”¨æ··åˆä¸“å®¶æ¶æ„å®ç°æƒ…å¢ƒç‰¹è´¨è¡¨è¾¾ã€‚</li>
<li>PersonaFuseåœ¨ç¤¾ä¼šæƒ…æ„Ÿæ™ºèƒ½çš„å¤šä¸ªç»´åº¦ä¸Šè¶…è¶ŠåŸºå‡†æ¨¡å‹ã€‚</li>
<li>PersonaFuseåœ¨ä¸ç‰ºç‰²é€šç”¨æ¨ç†èƒ½åŠ›å’Œæ¨¡å‹å®‰å…¨æ€§çš„å‰æä¸‹å®ç°æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07370">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07370v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07370v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07370v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Systematic-Optimization-of-Open-Source-Large-Language-Models-for-Mathematical-Reasoning"><a href="#Systematic-Optimization-of-Open-Source-Large-Language-Models-for-Mathematical-Reasoning" class="headerlink" title="Systematic Optimization of Open Source Large Language Models for   Mathematical Reasoning"></a>Systematic Optimization of Open Source Large Language Models for   Mathematical Reasoning</h2><p><strong>Authors:Pranav Pawar, Dhwaj Jain, Varun Gupta, Kaustav Dedhia, Dashrath Kale, Sudhir Dhekane</strong></p>
<p>This paper presents a practical investigation into fine-tuning model parameters for mathematical reasoning tasks through experimenting with various configurations including randomness control, reasoning depth, and sampling strategies, careful tuning demonstrates substantial improvements in efficiency as well as performance. A holistically optimized framework is introduced for five state-of-the-art models on mathematical reasoning tasks, exhibiting significant performance boosts while maintaining solution correctness. Through systematic parameter optimization across Qwen2.5-72B, Llama-3.1-70B, DeepSeek-V3, Mixtral-8x22B, and Yi-Lightning, consistent efficiency gains are demonstrated with 100% optimization success rate. The methodology achieves an average 29.4% reduction in computational cost and 23.9% improvement in inference speed across all tested models. This framework systematically searches parameter spaces including temperature (0.1-0.5), reasoning steps (4-12), planning periods (1-4), and nucleus sampling (0.85-0.98), determining optimal configurations through testing on mathematical reasoning benchmarks. Critical findings show that lower temperature regimes (0.1-0.4) and reduced reasoning steps (4-6) consistently enhance efficiency without compromising accuracy. DeepSeek-V3 achieves the highest accuracy at 98%, while Mixtral-8x22B delivers the most cost-effective performance at 361.5 tokens per accurate response. Key contributions include: (1) the first comprehensive optimization study for five diverse SOTA models in mathematical reasoning, (2) a standardized production-oriented parameter optimization framework, (3) discovery of universal optimization trends applicable across model architectures, and (4) production-ready configurations with extensive performance characterization. </p>
<blockquote>
<p>æœ¬æ–‡å®é™…æ¢ç©¶äº†é€šè¿‡å¾®è°ƒæ¨¡å‹å‚æ•°å¯¹æ•°å­¦æ¨ç†ä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚é€šè¿‡è¯•éªŒå„ç§é…ç½®ï¼ŒåŒ…æ‹¬éšæœºæ€§æ§åˆ¶ã€æ¨ç†æ·±åº¦å’Œé‡‡æ ·ç­–ç•¥ï¼Œç²¾å¿ƒè°ƒæ•´å±•ç°å‡ºå®è´¨æ€§çš„æ•ˆç‡å’Œæ€§èƒ½æå‡ã€‚ä¸ºäº”ä¸ªå‰æ²¿æ•°å­¦æ¨ç†æ¨¡å‹å¼•å…¥äº†ä¸€ä¸ªå…¨é¢ä¼˜åŒ–çš„æ¡†æ¶ï¼Œåœ¨ä¿æŒè§£å†³æ–¹æ¡ˆæ­£ç¡®æ€§çš„åŒæ—¶ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚é€šè¿‡å¯¹Qwen2.5-72Bã€Llama-3.1-70Bã€DeepSeek-V3ã€Mixtral-8x22Bå’ŒYi-Lightningç­‰æ¨¡å‹è¿›è¡Œç³»ç»Ÿçš„å‚æ•°ä¼˜åŒ–ï¼Œå±•ç¤ºäº†æŒç»­çš„æ•ˆç‡æå‡ï¼Œä¼˜åŒ–æˆåŠŸç‡ä¸º100%ã€‚è¯¥æ–¹æ³•å®ç°äº†è®¡ç®—æˆæœ¬å¹³å‡é™ä½29.4%ï¼Œæ¨ç†é€Ÿåº¦æé«˜23.9%ã€‚è¯¥æ¡†æ¶ç³»ç»Ÿåœ°æœç´¢å‚æ•°ç©ºé—´ï¼ŒåŒ…æ‹¬æ¸©åº¦ï¼ˆ0.1-0.5ï¼‰ã€æ¨ç†æ­¥éª¤ï¼ˆ4-12ï¼‰ã€è§„åˆ’å‘¨æœŸï¼ˆ1-4ï¼‰å’Œæ ¸å¿ƒé‡‡æ ·ï¼ˆ0.85-0.98ï¼‰ï¼Œé€šè¿‡åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œæµ‹è¯•æ¥ç¡®å®šæœ€ä½³é…ç½®ã€‚å…³é”®ç ”ç©¶å‘ç°ï¼Œè¾ƒä½çš„æ¸©åº¦èŒƒå›´ï¼ˆ0.1-0.4ï¼‰å’Œå‡å°‘çš„æ¨ç†æ­¥éª¤ï¼ˆ4-6ï¼‰æŒç»­æé«˜äº†æ•ˆç‡ï¼Œè€Œä¸ä¼šå½±å“å‡†ç¡®æ€§ã€‚DeepSeek-V3è¾¾åˆ°æœ€é«˜å‡†ç¡®ç‡98%ï¼Œè€ŒMixtral-8x22Båœ¨æˆæœ¬æ•ˆç›Šæ–¹é¢è¡¨ç°æœ€ä½³ï¼Œæ¯å‡†ç¡®å“åº”361.5ä¸ªä»¤ç‰Œã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰é¦–ä¸ªé’ˆå¯¹äº”ç§ä¸åŒæ•°å­¦æ¨ç†æ¨¡å‹çš„ç»¼åˆä¼˜åŒ–ç ”ç©¶ï¼Œï¼ˆ2ï¼‰æ ‡å‡†åŒ–çš„ç”Ÿäº§å¯¼å‘å‹å‚æ•°ä¼˜åŒ–æ¡†æ¶ï¼Œï¼ˆ3ï¼‰å‘ç°é€‚ç”¨äºå„ç§æ¨¡å‹æ¶æ„çš„é€šç”¨ä¼˜åŒ–è¶‹åŠ¿ï¼Œä»¥åŠï¼ˆ4ï¼‰ç»è¿‡å¹¿æ³›æ€§èƒ½è¡¨å¾çš„ç”Ÿäº§å°±ç»ªé…ç½®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07238v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>åœ¨ä¸€é¡¹å…³äºæ•°å­¦æ¨¡å‹å‚æ•°ä¼˜åŒ–çš„ç ”ç©¶ä¸­ï¼Œé€šè¿‡è°ƒæ•´æ¸©åº¦èŒƒå›´ã€æ¨ç†æ­¥éª¤ã€è§„åˆ’å‘¨æœŸå’ŒæŠ½æ ·ç­–ç•¥ç­‰å‚æ•°ï¼Œå¯¹äº”ä¸ªå…ˆè¿›çš„æ•°å­¦æ¨ç†æ¨¡å‹è¿›è¡Œäº†ç²¾ç»†åŒ–è°ƒæ•´ï¼Œæ˜¾è‘—æé«˜äº†æ•ˆç‡å’Œæ€§èƒ½ã€‚è¯¥æ¡†æ¶ç³»ç»Ÿåœ°æœç´¢å‚æ•°ç©ºé—´ï¼Œç¡®å®šæœ€ä¼˜é…ç½®ï¼Œåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†éªŒè¯ã€‚ç ”ç©¶å‘ç°ï¼Œè¾ƒä½çš„æ¸©åº¦èŒƒå›´å’Œå‡å°‘çš„æ¨ç†æ­¥éª¤èƒ½æ˜¾è‘—æé«˜æ•ˆç‡ä¸”ä¸å½±å“å‡†ç¡®æ€§ã€‚DeepSeek-V3å‡†ç¡®ç‡æœ€é«˜ï¼Œè¾¾åˆ°98%ï¼Œè€ŒMixtral-8x22Bçš„æˆæœ¬æ•ˆç›Šæœ€ä½³ã€‚è¯¥ç ”ç©¶ä¸ºæ•°å­¦æ¨ç†æ¨¡å‹å‚æ•°ä¼˜åŒ–æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶å¯¹äº”ä¸ªå…ˆè¿›çš„æ•°å­¦æ¨ç†æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„ä¼˜åŒ–ç ”ç©¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ ‡å‡†åŒ–çš„ã€é¢å‘ç”Ÿäº§çš„å‚æ•°ä¼˜åŒ–æ¡†æ¶ã€‚</li>
<li>å‘ç°äº†é€‚ç”¨äºå¤šç§æ¨¡å‹æ¶æ„çš„é€šç”¨ä¼˜åŒ–è¶‹åŠ¿ã€‚</li>
<li>æä¾›äº†ç»è¿‡å¹¿æ³›æ€§èƒ½è¡¨å¾çš„ç”Ÿäº§å°±ç»ªé…ç½®ã€‚</li>
<li>é€šè¿‡å®éªŒè°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œå®ç°äº†æ•°å­¦æ¨ç†ä»»åŠ¡çš„æ•ˆç‡ä¸æ€§èƒ½çš„æ˜¾è‘—æé«˜ã€‚</li>
<li>åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œå‘ç°è¾ƒä½çš„æ¸©åº¦èŒƒå›´å’Œå‡å°‘çš„æ¨ç†æ­¥éª¤èƒ½æ›´æœ‰æ•ˆåœ°æé«˜æ¨¡å‹æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07238">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07238v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07238v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07238v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07238v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07238v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07238v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="PaVeRL-SQL-Text-to-SQL-via-Partial-Match-Rewards-and-Verbal-Reinforcement-Learning"><a href="#PaVeRL-SQL-Text-to-SQL-via-Partial-Match-Rewards-and-Verbal-Reinforcement-Learning" class="headerlink" title="PaVeRL-SQL: Text-to-SQL via Partial-Match Rewards and Verbal   Reinforcement Learning"></a>PaVeRL-SQL: Text-to-SQL via Partial-Match Rewards and Verbal   Reinforcement Learning</h2><p><strong>Authors:Heng Hao, Wenjun Hu, Oxana Verkholyak, Davoud Ataee Tarzanagh, Baruch Gutow, Sima Didari, Masoud Faraki, Hankyu Moon, Seungjai Min</strong></p>
<p>Text-to-SQL models allow users to interact with a database more easily by generating executable SQL statements from natural-language questions. Despite recent successes on simpler databases and questions, current Text-to-SQL methods still suffer from low execution accuracy on industry-scale databases and complex questions involving domain-specific business logic. We present \emph{PaVeRL-SQL}, a framework that combines \emph{Partial-Match Rewards} and \emph{Verbal Reinforcement Learning} to drive self-improvement in reasoning language models (RLMs) for Text-to-SQL. To handle practical use cases, we adopt two pipelines: (1) a newly designed in-context learning framework with group self-evaluation (verbal-RL), using capable open- and closed-source large language models (LLMs) as backbones; and (2) a chain-of-thought (CoT) RL pipeline with a small backbone model (OmniSQL-7B) trained with a specially designed reward function and two-stage RL. These pipelines achieve state-of-the-art (SOTA) results on popular Text-to-SQL benchmarks â€“ Spider, Spider 2.0, and BIRD. For the industrial-level Spider2.0-SQLite benchmark, the verbal-RL pipeline achieves an execution accuracy 7.4% higher than SOTA, and the CoT pipeline is 1.4% higher. RL training with mixed SQL dialects yields strong, threefold gains, particularly for dialects with limited training data. Overall, \emph{PaVeRL-SQL} delivers reliable, SOTA Text-to-SQL under realistic industrial constraints. The code is available at <a target="_blank" rel="noopener" href="https://github.com/PaVeRL-SQL/PaVeRL-SQL">https://github.com/PaVeRL-SQL/PaVeRL-SQL</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°SQLæ¨¡å‹é€šè¿‡ä»è‡ªç„¶è¯­è¨€é—®é¢˜ç”Ÿæˆå¯æ‰§è¡Œçš„SQLè¯­å¥ï¼Œä½¿ç”¨æˆ·æ›´å®¹æ˜“ä¸æ•°æ®åº“è¿›è¡Œäº¤äº’ã€‚å°½ç®¡åœ¨è¾ƒç®€å•çš„æ•°æ®åº“å’Œé—®é¢˜ä¸Šå–å¾—äº†æœ€æ–°æˆåŠŸï¼Œä½†å½“å‰çš„æ–‡æœ¬åˆ°SQLæ–¹æ³•åœ¨å¤„ç†æ¶‰åŠç‰¹å®šé¢†åŸŸä¸šåŠ¡é€»è¾‘çš„å·¥ä¸šè§„æ¨¡æ•°æ®åº“å’Œå¤æ‚é—®é¢˜æ—¶ï¼Œæ‰§è¡Œå‡†ç¡®æ€§ä»ç„¶è¾ƒä½ã€‚æˆ‘ä»¬æå‡ºäº†ç»“åˆâ€œéƒ¨åˆ†åŒ¹é…å¥–åŠ±â€å’Œâ€œè¨€è¯­å¼ºåŒ–å­¦ä¹ â€æ¥é©±åŠ¨æ¨ç†è¯­è¨€æ¨¡å‹è‡ªæˆ‘æå‡çš„PaVeRL-SQLæ¡†æ¶ã€‚ä¸ºäº†å¤„ç†å®é™…åº”ç”¨åœºæ™¯ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤ä¸ªç®¡é“ï¼šï¼ˆ1ï¼‰æ–°è®¾è®¡çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶ï¼Œå¸¦æœ‰ç¾¤ä½“è‡ªæˆ‘è¯„ä»·ï¼ˆè¨€è¯­å¼ºåŒ–å­¦ä¹ ï¼‰ï¼Œä½¿ç”¨å¼€æºå’Œé—­æºçš„å¼ºå¤§å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºéª¨å¹²ï¼›ï¼ˆ2ï¼‰å…·æœ‰ç‰¹æ®Šè®¾è®¡çš„å¥–åŠ±å‡½æ•°å’Œä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ çš„æ€è€ƒé“¾ï¼ˆCoTï¼‰å¼ºåŒ–å­¦ä¹ ç®¡é“ï¼Œå¸¦æœ‰å°å‹éª¨å¹²æ¨¡å‹ï¼ˆOmniSQL-7Bï¼‰ã€‚è¿™äº›ç®¡é“åœ¨æµè¡Œçš„æ–‡æœ¬åˆ°SQLåŸºå‡†æµ‹è¯•â€”â€”Spiderã€Spider 2.0å’ŒBIRDä¸Šè¾¾åˆ°äº†æœ€æ–°çš„ç»“æœã€‚é’ˆå¯¹å·¥ä¸šçº§çš„Spider2.0-SQLiteåŸºå‡†æµ‹è¯•ï¼Œè¨€è¯­å¼ºåŒ–å­¦ä¹ ç®¡é“çš„æ‰§è¡ŒåŠ›å‡†ç¡®æ€§æ¯”æœ€æ–°æŠ€æœ¯é«˜å‡º7.4%ï¼Œæ€è€ƒé“¾ç®¡é“é«˜å‡º1.4%ã€‚ä½¿ç”¨æ··åˆSQLæ–¹è¨€çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒäº§ç”Ÿäº†å¼ºå¤§çš„ä¸‰å€å¢é•¿ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè®­ç»ƒæ•°æ®æœ‰é™çš„æ–¹è¨€ã€‚æ€»çš„æ¥è¯´ï¼ŒPaVeRL-SQLåœ¨ç°å®çš„å·¥ä¸šçº¦æŸä¸‹ï¼Œæä¾›äº†å¯é ã€æœ€æ–°çš„æ–‡æœ¬åˆ°SQLè§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/PaVeRL-SQL/PaVeRL-SQL">https://github.com/PaVeRL-SQL/PaVeRL-SQL</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>ç®€åŒ–ç‰ˆ</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07159v1">PDF</a> 10 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†PaVeRL-SQLæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†éƒ¨åˆ†åŒ¹é…å¥–åŠ±å’Œè¨€è¯­å¼ºåŒ–å­¦ä¹ ï¼Œæé«˜äº†æ–‡æœ¬åˆ°SQLçš„æ¨ç†è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘æ”¹è¿›èƒ½åŠ›ã€‚é’ˆå¯¹å®é™…åº”ç”¨åœºæ™¯ï¼Œé‡‡ç”¨äº†ä¸¤ç§ç®¡é“ï¼šä¸€æ˜¯æ–°è®¾è®¡çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶ï¼Œä½¿ç”¨å¼€æºå’Œé—­æºçš„å¼ºå¤§å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºéª¨å¹²ï¼›äºŒæ˜¯æ€ç»´é“¾å¼ºåŒ–å­¦ä¹ ç®¡é“ï¼Œä½¿ç”¨å°å‹éª¨å¹²æ¨¡å‹OmniSQL-7Bè¿›è¡Œè®­ç»ƒï¼Œå¹¶è®¾è®¡äº†ä¸“é—¨çš„å¥–åŠ±å‡½æ•°å’Œä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ ã€‚æ­¤æ¡†æ¶åœ¨æµè¡Œçš„æ–‡æœ¬åˆ°SQLåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°çš„è¡¨ç°æ°´å¹³ï¼Œä¸”å¯¹å·¥ä¸šçº§åˆ«çš„Spider2.0-SQLiteåŸºå‡†æµ‹è¯•å®ç°äº†æ›´é«˜çš„æ‰§è¡Œç²¾åº¦ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PaVeRL-SQLç»“åˆäº†éƒ¨åˆ†åŒ¹é…å¥–åŠ±å’Œè¨€è¯­å¼ºåŒ–å­¦ä¹ ï¼Œæé«˜äº†æ–‡æœ¬åˆ°SQLè¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘æ”¹è¿›èƒ½åŠ›ã€‚</li>
<li>æä¾›äº†ä¸¤ç§å¤„ç†å®é™…åº”ç”¨åœºæ™¯çš„ç®¡é“ï¼šä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶å’Œæ€ç»´é“¾å¼ºåŒ–å­¦ä¹ ç®¡é“ã€‚</li>
<li>PaVeRL-SQLåœ¨å¤šä¸ªæµè¡Œçš„æ–‡æœ¬åˆ°SQLåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°çš„è¡¨ç°æ°´å¹³ã€‚</li>
<li>åœ¨å·¥ä¸šçº§åˆ«çš„Spider2.0-SQLiteåŸºå‡†æµ‹è¯•ä¸­ï¼ŒPaVeRL-SQLçš„æ‰§è¡Œç²¾åº¦é«˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨æ··åˆSQLæ–¹è¨€è¿›è¡Œå¼ºåŒ–è®­ç»ƒï¼ŒPaVeRL-SQLå–å¾—äº†æ˜¾è‘—çš„æ•ˆæœæå‡ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè®­ç»ƒæ•°æ®æœ‰é™çš„æ–¹è¨€ã€‚</li>
<li>PaVeRL-SQLåœ¨å¯é çš„æ–‡æœ¬åˆ°SQLè½¬æ¢æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œé€‚åº”å·¥ä¸šç•Œçš„å®é™…éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07159">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07159v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07159v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07159v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07159v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07159v1/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07159v1/page_5_2.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Toward-Purpose-oriented-Topic-Model-Evaluation-enabled-by-Large-Language-Models"><a href="#Toward-Purpose-oriented-Topic-Model-Evaluation-enabled-by-Large-Language-Models" class="headerlink" title="Toward Purpose-oriented Topic Model Evaluation enabled by Large Language   Models"></a>Toward Purpose-oriented Topic Model Evaluation enabled by Large Language   Models</h2><p><strong>Authors:Zhiyin Tan, Jennifer Dâ€™Souza</strong></p>
<p>This study presents a framework for automated evaluation of dynamically evolving topic models using Large Language Models (LLMs). Topic modeling is essential for organizing and retrieving scholarly content in digital library systems, helping users navigate complex and evolving knowledge domains. However, widely used automated metrics, such as coherence and diversity, often capture only narrow statistical patterns and fail to explain semantic failures in practice. We introduce a purpose-oriented evaluation framework that employs nine LLM-based metrics spanning four key dimensions of topic quality: lexical validity, intra-topic semantic soundness, inter-topic structural soundness, and document-topic alignment soundness. The framework is validated through adversarial and sampling-based protocols, and is applied across datasets spanning news articles, scholarly publications, and social media posts, as well as multiple topic modeling methods and open-source LLMs. Our analysis shows that LLM-based metrics provide interpretable, robust, and task-relevant assessments, uncovering critical weaknesses in topic models such as redundancy and semantic drift, which are often missed by traditional metrics. These results support the development of scalable, fine-grained evaluation tools for maintaining topic relevance in dynamic datasets. All code and data supporting this work are accessible at <a target="_blank" rel="noopener" href="https://github.com/zhiyintan/topic-model-LLMjudgment">https://github.com/zhiyintan/topic-model-LLMjudgment</a>. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹åŠ¨æ€æ¼”åŒ–ä¸»é¢˜æ¨¡å‹è¿›è¡Œè‡ªåŠ¨è¯„ä¼°çš„æ¡†æ¶ã€‚ä¸»é¢˜å»ºæ¨¡åœ¨æ•°å­—å›¾ä¹¦é¦†ç³»ç»Ÿä¸­ç»„ç»‡å’Œæ£€ç´¢å­¦æœ¯å†…å®¹æ–¹é¢è‡³å…³é‡è¦ï¼Œæœ‰åŠ©äºç”¨æˆ·æµè§ˆå¤æ‚ä¸”ä¸æ–­å‘å±•çš„çŸ¥è¯†é¢†åŸŸã€‚ç„¶è€Œï¼Œå¹¿æ³›ä½¿ç”¨çš„è‡ªåŠ¨åŒ–æŒ‡æ ‡ï¼Œå¦‚è¿è´¯æ€§å’Œå¤šæ ·æ€§ï¼Œé€šå¸¸åªèƒ½æ•æ‰åˆ°ç‹­çª„çš„ç»Ÿè®¡æ¨¡å¼ï¼Œå¹¶ä¸”æ— æ³•è§£é‡Šå®é™…ä¸­çš„è¯­ä¹‰å¤±è´¥ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä»¥ç›®çš„ä¸ºå¯¼å‘çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨ä¹ç§åŸºäºLLMçš„æŒ‡æ ‡ï¼Œæ¶µç›–ä¸»é¢˜è´¨é‡çš„å››ä¸ªå…³é”®ç»´åº¦ï¼šè¯æ±‡æœ‰æ•ˆæ€§ã€ä¸»é¢˜å†…è¯­ä¹‰åˆç†æ€§ã€ä¸»é¢˜é—´ç»“æ„åˆç†æ€§ä»¥åŠæ–‡æ¡£ä¸»é¢˜å¯¹é½åˆç†æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹æŠ—å’ŒåŸºäºé‡‡æ ·çš„åè®®è¿›è¡Œäº†éªŒè¯ï¼Œå¹¶åº”ç”¨äºæ¶µç›–æ–°é—»æ–‡ç« ã€å­¦æœ¯å‡ºç‰ˆç‰©å’Œç¤¾äº¤åª’ä½“å¸–å­çš„æ•°æ®é›†ï¼Œä»¥åŠå¤šç§ä¸»é¢˜å»ºæ¨¡æ–¹æ³•å’Œå¼€æºLLMsã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒåŸºäºLLMçš„æŒ‡æ ‡æä¾›äº†å¯è§£é‡Šã€ç¨³å¥å’Œä»»åŠ¡ç›¸å…³çš„è¯„ä¼°ï¼Œæ­ç¤ºäº†ä¸»é¢˜æ¨¡å‹ä¸­çš„å…³é”®å¼±ç‚¹ï¼Œå¦‚å†—ä½™å’Œè¯­ä¹‰æ¼‚ç§»ï¼Œè¿™äº›å¾€å¾€è¢«ä¼ ç»ŸæŒ‡æ ‡æ‰€å¿½è§†ã€‚è¿™äº›ç»“æœæ”¯æŒå¼€å‘å¯æ‰©å±•çš„ã€ç²¾ç»†ç²’åº¦çš„è¯„ä¼°å·¥å…·ï¼Œä»¥ç»´æŠ¤åŠ¨æ€æ•°æ®é›†ä¸­çš„ä¸»é¢˜ç›¸å…³æ€§ã€‚æ”¯æŒè¿™é¡¹å·¥ä½œçš„æ‰€æœ‰ä»£ç å’Œæ•°æ®éƒ½å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhiyintan/topic-model-LLMjudgment">https://github.com/zhiyintan/topic-model-LLMjudgment</a>è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07142v1">PDF</a> Accepted for publication in International Journal on Digital   Libraries (IJDL)</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åŠ¨æ€æ¼”åŒ–è¯é¢˜æ¨¡å‹è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”¨äºç»„ç»‡å’Œç®¡ç†æ•°å­—å›¾ä¹¦é¦†ç³»ç»Ÿä¸­çš„å­¦æœ¯å†…å®¹ï¼Œå¸®åŠ©ç”¨æˆ·å¯¼èˆªå¤æ‚ä¸”ä¸æ–­å˜åŒ–çš„çŸ¥è¯†é¢†åŸŸã€‚ä¼ ç»Ÿçš„è¯é¢˜æ¨¡å‹è¯„ä¼°æ–¹æ³•ï¼Œå¦‚è¿è´¯æ€§å’Œå¤šæ ·æ€§ç­‰ï¼Œé€šå¸¸åªèƒ½æ•æ‰åˆ°ç‹­çª„çš„ç»Ÿè®¡æ¨¡å¼ï¼Œéš¾ä»¥è§£é‡Šå®é™…è¯­ä¹‰é—®é¢˜ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§é¢å‘ä»»åŠ¡çš„è¯„ä¼°æ¡†æ¶ï¼Œé‡‡ç”¨ä¹ä¸ªLLMæŒ‡æ ‡ï¼Œä»è¯æ±‡æœ‰æ•ˆæ€§ã€è¯é¢˜å†…éƒ¨è¯­ä¹‰åˆç†æ€§ã€è¯é¢˜é—´ç»“æ„åˆç†æ€§ä»¥åŠæ–‡æ¡£è¯é¢˜å¯¹é½æ€§å››ä¸ªå…³é”®ç»´åº¦è¯„ä¼°è¯é¢˜è´¨é‡ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹æŠ—å’ŒæŠ½æ ·åè®®è¿›è¡ŒéªŒè¯ï¼Œå¹¶åº”ç”¨äºæ–°é—»æ–‡ç« ã€å­¦æœ¯å‡ºç‰ˆç‰©ã€ç¤¾äº¤åª’ä½“å¸–å­ç­‰å¤šä¸ªæ•°æ®é›†ä»¥åŠå¤šç§è¯é¢˜å»ºæ¨¡æ–¹æ³•å’Œå¼€æºLLMsã€‚åˆ†æè¡¨æ˜ï¼ŒLLMæŒ‡æ ‡æä¾›äº†å¯è§£é‡Šã€ç¨³å¥å’Œä»»åŠ¡ç›¸å…³çš„è¯„ä¼°ï¼Œèƒ½å¤Ÿæ­ç¤ºä¼ ç»ŸæŒ‡æ ‡å¸¸å¸¸å¿½è§†çš„è¯é¢˜æ¨¡å‹ä¸­çš„å†—ä½™å’Œè¯­ä¹‰æ¼‚ç§»ç­‰å…³é”®å¼±ç‚¹ã€‚è¿™äº›ç»“æœä¸ºå¼€å‘å¯ç”¨äºç»´æŠ¤åŠ¨æ€æ•°æ®é›†è¯é¢˜ç›¸å…³æ€§çš„å¯æ‰©å±•ã€ç²¾ç»†ç²’åº¦çš„è¯„ä¼°å·¥å…·æä¾›äº†æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä»·åŠ¨æ€æ¼”åŒ–çš„è¯é¢˜æ¨¡å‹ã€‚</li>
<li>ä¼ ç»Ÿçš„è¯é¢˜æ¨¡å‹è¯„ä¼°æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œåªèƒ½æ•æ‰ç‹­çª„çš„ç»Ÿè®¡æ¨¡å¼ï¼Œéš¾ä»¥è§£é‡Šå®é™…è¯­ä¹‰é—®é¢˜ã€‚</li>
<li>æ–°çš„è¯„ä¼°æ¡†æ¶åŒ…å«å››ä¸ªå…³é”®ç»´åº¦ï¼šè¯æ±‡æœ‰æ•ˆæ€§ã€è¯é¢˜å†…éƒ¨è¯­ä¹‰åˆç†æ€§ã€è¯é¢˜é—´ç»“æ„åˆç†æ€§ä»¥åŠæ–‡æ¡£è¯é¢˜å¯¹é½æ€§ã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨ä¹ä¸ªLLMæŒ‡æ ‡è¿›è¡Œè¯„ä»·ï¼Œæä¾›äº†å¯è§£é‡Šã€ç¨³å¥å’Œä»»åŠ¡ç›¸å…³çš„è¯„ä¼°ã€‚</li>
<li>å¯¹å¤šä¸ªæ•°æ®é›†ã€å¤šç§è¯é¢˜å»ºæ¨¡æ–¹æ³•å’Œå¼€æºLLMsçš„åº”ç”¨è¡¨æ˜ï¼ŒLLMæŒ‡æ ‡èƒ½å¤Ÿæ­ç¤ºè¯é¢˜æ¨¡å‹ä¸­çš„å…³é”®å¼±ç‚¹ï¼Œå¦‚å†—ä½™å’Œè¯­ä¹‰æ¼‚ç§»ã€‚</li>
<li>è¯¥ç ”ç©¶æ”¯æŒå¼€å‘ç”¨äºç»´æŠ¤åŠ¨æ€æ•°æ®é›†è¯é¢˜ç›¸å…³æ€§çš„ç²¾ç»†ç²’åº¦è¯„ä¼°å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07142">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07142v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07142v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07142v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Interleaving-Reasoning-for-Better-Text-to-Image-Generation"><a href="#Interleaving-Reasoning-for-Better-Text-to-Image-Generation" class="headerlink" title="Interleaving Reasoning for Better Text-to-Image Generation"></a>Interleaving Reasoning for Better Text-to-Image Generation</h2><p><strong>Authors:Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, Junbo Qiao, Yue Guo, Yao Hu, Zhenfei Yin, Philip Torr, Yu Cheng, Wanli Ouyang, Shaohui Lin</strong></p>
<p>Unified multimodal understanding and generation models recently have achieve significant improvement in image generation capability, yet a large gap remains in instruction following and detail preservation compared to systems that tightly couple comprehension with generation such as GPT-4o. Motivated by recent advances in interleaving reasoning, we explore whether such reasoning can further improve Text-to-Image (T2I) generation. We introduce Interleaving Reasoning Generation (IRG), a framework that alternates between text-based thinking and image synthesis: the model first produces a text-based thinking to guide an initial image, then reflects on the result to refine fine-grained details, visual quality, and aesthetics while preserving semantics. To train IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL), which targets two sub-goals: (1) strengthening the initial think-and-generate stage to establish core content and base quality, and (2) enabling high-quality textual reflection and faithful implementation of those refinements in a subsequent image. We curate IRGL-300K, a dataset organized into six decomposed learning modes that jointly cover learning text-based thinking, and full thinking-image trajectories. Starting from a unified foundation model that natively emits interleaved text-image outputs, our two-stage training first builds robust thinking and reflection, then efficiently tunes the IRG pipeline in the full thinking-image trajectory data. Extensive experiments show SoTA performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and fine-grained fidelity. The code, model weights and datasets will be released in: <a target="_blank" rel="noopener" href="https://github.com/Osilly/Interleaving-Reasoning-Generation">https://github.com/Osilly/Interleaving-Reasoning-Generation</a> . </p>
<blockquote>
<p>è¿‘æœŸï¼Œç»Ÿä¸€çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ¨¡å‹åœ¨å›¾åƒç”Ÿæˆèƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œä½†åœ¨éµå¾ªæŒ‡ä»¤å’Œç»†èŠ‚ä¿ç•™æ–¹é¢ä¸ç´§å¯†è€¦åˆç†è§£ä¸ç”Ÿæˆçš„ç³»ç»Ÿï¼ˆå¦‚GPT-4oï¼‰ç›¸æ¯”ä»å­˜åœ¨è¾ƒå¤§å·®è·ã€‚å—åˆ°äº¤æ›¿æ¨ç†æœ€æ–°è¿›å±•çš„å¯å‘ï¼Œæˆ‘ä»¬æ¢ç´¢äº†è¿™ç§æ¨ç†æ˜¯å¦èƒ½è¿›ä¸€æ­¥æ”¹è¿›æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰çš„ç”Ÿæˆã€‚æˆ‘ä»¬å¼•å…¥äº†äº¤æ›¿æ¨ç†ç”Ÿæˆï¼ˆIRGï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨æ–‡æœ¬æ€è€ƒå’Œå›¾åƒåˆæˆä¹‹é—´è¿›è¡Œäº¤æ›¿ï¼šæ¨¡å‹é¦–å…ˆäº§ç”ŸåŸºäºæ–‡æœ¬çš„æ€è€ƒæ¥æŒ‡å¯¼åˆå§‹å›¾åƒï¼Œç„¶ååæ€ç»“æœï¼Œä»¥ç»†åŒ–ç»†èŠ‚ã€è§†è§‰è´¨é‡å’Œç¾å­¦æ„Ÿï¼ŒåŒæ—¶ä¿ç•™è¯­ä¹‰ã€‚ä¸ºäº†æœ‰æ•ˆåœ°è®­ç»ƒIRGï¼Œæˆ‘ä»¬æå‡ºäº†äº¤æ›¿æ¨ç†ç”Ÿæˆå­¦ä¹ ï¼ˆIRGLï¼‰ï¼Œå®ƒé’ˆå¯¹ä¸¤ä¸ªå­ç›®æ ‡ï¼šï¼ˆ1ï¼‰åŠ å¼ºåˆå§‹çš„æ€è€ƒå’Œç”Ÿæˆé˜¶æ®µï¼Œä»¥å»ºç«‹æ ¸å¿ƒå†…å®¹å’ŒåŸºç¡€è´¨é‡ï¼›ï¼ˆ2ï¼‰å®ç°é«˜è´¨é‡çš„æ–‡æœ¬åæ€å’Œå¿ å®äºéšåçš„å›¾åƒä¸­çš„æ”¹è¿›ã€‚æˆ‘ä»¬ç­–åˆ’äº†IRGL-300Kæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†è¢«ç»„ç»‡æˆå…­ç§åˆ†è§£å­¦ä¹ æ¨¡å¼ï¼Œè”åˆè¦†ç›–åŸºäºæ–‡æœ¬çš„æ€è€ƒå’Œå®Œæ•´çš„æ€è€ƒ-å›¾åƒè½¨è¿¹ã€‚ä»ç»Ÿä¸€çš„åŸºç¡€æ¨¡å‹å‡ºå‘ï¼Œè¯¥æ¨¡å‹å¤©ç”Ÿå°±èƒ½å‘å‡ºäº¤æ›¿çš„æ–‡æœ¬-å›¾åƒè¾“å‡ºï¼Œæˆ‘ä»¬çš„ä¸¤é˜¶æ®µè®­ç»ƒé¦–å…ˆå»ºç«‹ç¨³å¥çš„æ€è€ƒå’Œåæ€èƒ½åŠ›ï¼Œç„¶åæœ‰æ•ˆåœ°åœ¨å®Œæ•´çš„æ€è€ƒ-å›¾åƒè½¨è¿¹æ•°æ®ä¸­è°ƒæ•´IRGç®¡é“ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œå…¶æ€§èƒ½è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œåœ¨GenEvalã€WISEã€TIIFã€GenAI-Benchå’ŒOneIG-ENç­‰æŒ‡æ ‡ä¸Šå–å¾—äº†5-10åˆ†çš„ç»å¯¹å¢ç›Šï¼ŒåŒæ—¶åœ¨è§†è§‰è´¨é‡å’Œç»†èŠ‚ä¿çœŸåº¦æ–¹é¢ä¹Ÿæœ‰äº†å®è´¨æ€§çš„æ”¹è¿›ã€‚ç›¸å…³ä»£ç ã€æ¨¡å‹æƒé‡å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Osilly/Interleaving-Reasoning-Generation%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Osilly/Interleaving-Reasoning-Generationä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06945v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>æ–‡æœ¬æŒ‡å‡ºå½“å‰æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆï¼ˆText-to-Imageï¼Œç®€ç§°T2Iï¼‰æ¨¡å‹åœ¨ç»†èŠ‚ä¿ç•™å’ŒæŒ‡ä»¤éµå¾ªæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶åœ¨å›¾åƒç”Ÿæˆèƒ½åŠ›æ–¹é¢è¿˜æœ‰å¾…æé«˜ã€‚ä¸ºäº†æ”¹è¿›è¿™ä¸€é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ä¸ªåä¸ºäº¤æ›¿æ¨ç†ç”Ÿæˆï¼ˆInterleaving Reasoning Generationï¼Œç®€ç§°IRGï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡äº¤æ›¿è¿›è¡Œæ–‡æœ¬æ€è€ƒå’Œå›¾åƒåˆæˆæ¥æé«˜å›¾åƒç”Ÿæˆçš„è´¨é‡ã€‚ä¸ºäº†æœ‰æ•ˆåœ°è®­ç»ƒIRGæ¨¡å‹ï¼Œæ–‡ç« è¿˜æå‡ºäº†äº¤æ›¿æ¨ç†ç”Ÿæˆå­¦ä¹ ï¼ˆInterleaving Reasoning Generation Learningï¼Œç®€ç§°IRGLï¼‰çš„æ–¹æ³•ï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªæ•°æ®é›†IRGL-300Kç”¨äºè®­ç»ƒã€‚è¯¥æ¡†æ¶å’Œæ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨ç»†èŠ‚ä¿ç•™å’ŒæŒ‡ä»¤éµå¾ªæ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”äº¤æ›¿æ¨ç†ç”Ÿæˆï¼ˆIRGï¼‰ï¼Œé€šè¿‡äº¤æ›¿è¿›è¡Œæ–‡æœ¬æ€è€ƒå’Œå›¾åƒåˆæˆæ¥æé«˜å›¾åƒç”Ÿæˆçš„è´¨é‡ã€‚</li>
<li>ä¸ºäº†æœ‰æ•ˆåœ°è®­ç»ƒIRGæ¨¡å‹ï¼Œæå‡ºäº†äº¤æ›¿æ¨ç†ç”Ÿæˆå­¦ä¹ ï¼ˆIRGLï¼‰çš„æ–¹æ³•ï¼Œå¹¶åˆ›å»ºäº†IRGL-300Kæ•°æ®é›†ã€‚</li>
<li>IRGæ¡†æ¶é€šè¿‡å¼ºåŒ–åˆå§‹çš„â€œæ€è€ƒ-ç”Ÿæˆâ€é˜¶æ®µæ¥å»ºç«‹æ ¸å¿ƒå†…å®¹å’ŒåŸºç¡€è´¨é‡ã€‚</li>
<li>IRGæ¡†æ¶èƒ½å¤Ÿå®ç°åœ¨åç»­å›¾åƒä¸­é«˜è´¨é‡æ–‡æœ¬åæ€å’Œå¿ å®å®æ–½æ”¹è¿›ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸åŒæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œå®ç°äº†GenEvalã€WISEã€TIIFã€GenAI-Benchå’ŒOneIG-ENçš„ç»å¯¹å¾—åˆ†æå‡5-10åˆ†ã€‚</li>
<li>æ¨¡å‹ä»£ç ã€æƒé‡å’Œæ•°æ®é›†å°†åœ¨æŒ‡å®šé“¾æ¥ä¸­å…¬å¼€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06945">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.06945v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.06945v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.06945v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SFR-DeepResearch-Towards-Effective-Reinforcement-Learning-for-Autonomously-Reasoning-Single-Agents"><a href="#SFR-DeepResearch-Towards-Effective-Reinforcement-Learning-for-Autonomously-Reasoning-Single-Agents" class="headerlink" title="SFR-DeepResearch: Towards Effective Reinforcement Learning for   Autonomously Reasoning Single Agents"></a>SFR-DeepResearch: Towards Effective Reinforcement Learning for   Autonomously Reasoning Single Agents</h2><p><strong>Authors:Xuan-Phi Nguyen, Shrey Pandit, Revanth Gangi Reddy, Austin Xu, Silvio Savarese, Caiming Xiong, Shafiq Joty</strong></p>
<p>Equipping large language models (LLMs) with complex, interleaved reasoning and tool-use capabilities has become a key focus in agentic AI research, especially with recent advances in reasoning-oriented (&#96;&#96;thinkingâ€™â€™) models. Such capabilities are key to unlocking a number of important applications. One such application is Deep Research (DR), which requires extensive search and reasoning over many sources. Our work in this paper focuses on the development of native Autonomous Single-Agent models for DR featuring minimal web crawling and Python tool integration. Unlike multi-agent systems, where agents take up pre-defined roles and are told what to do at each step in a static workflow, an autonomous single-agent determines its next action dynamically based on context, without manual directive. While prior work has proposed training recipes for base or instruction-tuned LLMs, we focus on continual reinforcement learning (RL) of reasoning-optimized models to further enhance agentic skills while preserving reasoning ability. Towards this end, we propose a simple RL recipe with entirely synthetic data, which we apply to various open-source LLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanityâ€™s Last Exam benchmark. In addition, we conduct key analysis experiments to provide more insights into our methodologies. </p>
<blockquote>
<p>èµ‹äºˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤æ‚äº¤ç»‡çš„æ¨ç†å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›å·²æˆä¸ºä»£ç†äººå·¥æ™ºèƒ½ç ”ç©¶çš„å…³é”®ç„¦ç‚¹ï¼Œå°¤å…¶æ˜¯éšç€é¢å‘æ¨ç†çš„ï¼ˆâ€œæ€è€ƒâ€ï¼‰æ¨¡å‹çš„æœ€æ–°è¿›å±•ã€‚è¿™äº›èƒ½åŠ›æ˜¯è§£é”è®¸å¤šé‡è¦åº”ç”¨çš„å…³é”®ã€‚å…¶ä¸­ä¸€ä¸ªåº”ç”¨æ˜¯æ·±åº¦ç ”ç©¶ï¼ˆDRï¼‰ï¼Œå®ƒéœ€è¦åœ¨å¤šä¸ªæ¥æºä¹‹é—´è¿›è¡Œå¹¿æ³›æœç´¢å’Œæ¨ç†ã€‚æœ¬æ–‡çš„å·¥ä½œé‡ç‚¹æ˜¯ä¸ºDRå¼€å‘æœ¬åœ°è‡ªä¸»å•ä»£ç†æ¨¡å‹ï¼Œå…·æœ‰æœ€å°‘çš„ç½‘ç»œçˆ¬è™«å’ŒPythonå·¥å…·é›†æˆã€‚ä¸å¤šä»£ç†ç³»ç»Ÿä¸åŒï¼Œå¤šä»£ç†ç³»ç»Ÿä¸­çš„ä»£ç†æ‰®æ¼”é¢„å®šä¹‰è§’è‰²ï¼Œå¹¶åœ¨é™æ€å·¥ä½œæµä¸­çš„æ¯ä¸ªæ­¥éª¤ä¸­è¢«å‘ŠçŸ¥è¦åšä»€ä¹ˆï¼Œè€Œè‡ªä¸»å•ä»£ç†åˆ™æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€ç¡®å®šå…¶ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡ä»¤ã€‚è™½ç„¶å…ˆå‰çš„å·¥ä½œå·²ç»æå‡ºäº†é’ˆå¯¹åŸºç¡€æˆ–æŒ‡ä»¤å¾®è°ƒLLMçš„è®­ç»ƒé…æ–¹ï¼Œä½†æˆ‘ä»¬ä¸“æ³¨äºå¯¹æ¨ç†ä¼˜åŒ–æ¨¡å‹çš„æŒç»­å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºä»£ç†æŠ€èƒ½åŒæ—¶ä¿ç•™æ¨ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç®€å•çš„ä½¿ç”¨å®Œå…¨åˆæˆæ•°æ®çš„RLé…æ–¹ï¼Œå°†å…¶åº”ç”¨äºå„ç§å¼€æºLLMã€‚æˆ‘ä»¬æœ€å¥½çš„å˜ä½“SFR-DR-20Båœ¨äººç±»æœ€åçš„è€ƒè¯•åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†é«˜è¾¾28.7%çš„æ•ˆæœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å…³é”®çš„åˆ†æå®éªŒï¼Œä»¥æ›´æ·±å…¥åœ°äº†è§£æˆ‘ä»¬çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06283v2">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é…å¤‡å¤æ‚äº¤ç»‡çš„æ¨ç†å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›å·²æˆä¸ºä»£ç†äººå·¥æ™ºèƒ½ç ”ç©¶çš„å…³é”®ç„¦ç‚¹ï¼Œå°¤å…¶æ˜¯æœ€è¿‘å‡ºç°çš„ä»¥æ¨ç†ä¸ºå¯¼å‘çš„â€œæ€è€ƒâ€æ¨¡å‹ã€‚æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ç§ç”¨äºæ·±åº¦ç ”ç©¶ï¼ˆDRï¼‰çš„è‡ªä¸»å•ä»£ç†æ¨¡å‹ï¼Œå…·æœ‰æœ€å°çš„ç½‘ç»œçˆ¬è™«å’ŒPythonå·¥å…·é›†æˆã€‚ä¸åŒäºå¤šä»£ç†ç³»ç»Ÿï¼Œè‡ªä¸»å•ä»£ç†èƒ½åŸºäºä¸Šä¸‹æ–‡åŠ¨æ€åœ°ç¡®å®šå…¶ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡ä»¤ã€‚æœ¬ç ”ç©¶èšç„¦äºæŒç»­å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¨ç†ä¼˜åŒ–æ¨¡å‹ï¼Œä»¥æé«˜ä»£ç†æŠ€èƒ½å¹¶ä¿ç•™æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä½¿ç”¨åˆæˆæ•°æ®çš„ç®€å•RLé…æ–¹ï¼Œå¹¶åº”ç”¨äºå„ç§å¼€æºLLMã€‚æœ€ä½³å˜ä½“SFR-DR-20Båœ¨äººç±»æœ€åçš„è€ƒè¯•åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†28.7%çš„å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨å‘å±•å‡ºå¤æ‚æ¨ç†å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œè¿™æ˜¯ä»£ç†äººå·¥æ™ºèƒ½ç ”ç©¶çš„å…³é”®ã€‚</li>
<li>æ·±åº¦ç ”ç©¶ï¼ˆDRï¼‰éœ€è¦å¹¿æ³›çš„æºæœç´¢å’Œæ¨ç†ï¼Œè‡ªä¸»å•ä»£ç†æ¨¡å‹è¢«å¼€å‘ç”¨äºDRï¼Œå…·æœ‰æœ€å°çš„ç½‘ç»œçˆ¬è™«å’ŒPythonå·¥å…·é›†æˆã€‚</li>
<li>ä¸å¤šä»£ç†ç³»ç»Ÿä¸åŒï¼Œè‡ªä¸»å•ä»£ç†èƒ½åŸºäºä¸Šä¸‹æ–‡åŠ¨æ€å†³ç­–ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡ä»¤ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡æŒç»­å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æé«˜LLMçš„æ¨ç†ä¼˜åŒ–æ¨¡å‹ã€‚</li>
<li>æå‡ºä¸€ä¸ªç®€å•çš„RLé…æ–¹ï¼Œä½¿ç”¨åˆæˆæ•°æ®åº”ç”¨äºå„ç§å¼€æºLLMã€‚</li>
<li>æœ€ä½³æ¨¡å‹SFR-DR-20Båœ¨äººç±»æœ€åçš„è€ƒè¯•åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†28.7%çš„å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06283">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.06283v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.06283v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Rethinking-Reasoning-Quality-in-Large-Language-Models-through-Enhanced-Chain-of-Thought-via-RL"><a href="#Rethinking-Reasoning-Quality-in-Large-Language-Models-through-Enhanced-Chain-of-Thought-via-RL" class="headerlink" title="Rethinking Reasoning Quality in Large Language Models through Enhanced   Chain-of-Thought via RL"></a>Rethinking Reasoning Quality in Large Language Models through Enhanced   Chain-of-Thought via RL</h2><p><strong>Authors:Haoyang He, Zihua Rong, Kun Ji, Chenyang Li, Qing Huang, Chong Xia, Lan Yang, Honggang Zhang</strong></p>
<p>Reinforcement learning (RL) has recently become the dominant paradigm for strengthening the reasoning abilities of large language models (LLMs). Yet the rule-based reward functions commonly used on mathematical or programming benchmarks assess only answer format and correctness, providing no signal as to whether the induced Chain-of-Thought (CoT) actually improves the answer. Furthermore, such task-specific training offers limited control over logical depth and therefore may fail to reveal a modelâ€™s genuine reasoning capacity. We propose Dynamic Reasoning Efficiency Reward (DRER) â€“ a plug-and-play RL reward framework that reshapes both reward and advantage signals. (i) A Reasoning Quality Reward assigns fine-grained credit to those reasoning chains that demonstrably raise the likelihood of the correct answer, directly incentivising the trajectories with beneficial CoT tokens. (ii) A Dynamic Length Advantage decays the advantage of responses whose length deviates from a validation-derived threshold, stabilising training. To facilitate rigorous assessment, we also release Logictree, a dynamically constructed deductive reasoning dataset that functions both as RL training data and as a comprehensive benchmark. Experiments confirm the effectiveness of DRER: our 7B model attains GPT-o3-mini level performance on Logictree with 400 trianing steps, while the average confidence of CoT-augmented answers rises by 30%. The model further exhibits generalisation across diverse logical-reasoning datasets, and the mathematical benchmark AIME24. These results illuminate how RL shapes CoT behaviour and chart a practical path toward enhancing formal-reasoning skills in large language models. All code and data are available in repository <a target="_blank" rel="noopener" href="https://github.com/Henryhe09/DRER">https://github.com/Henryhe09/DRER</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æœ€è¿‘å·²æˆä¸ºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„ä¸»æµèŒƒå¼ã€‚ç„¶è€Œï¼Œç›®å‰åœ¨æ•°å­¦æˆ–ç¼–ç¨‹åŸºå‡†æµ‹è¯•ä¸­å¸¸ç”¨çš„åŸºäºè§„åˆ™çš„å¥–åŠ±å‡½æ•°ä»…è¯„ä¼°ç­”æ¡ˆçš„æ ¼å¼å’Œæ­£ç¡®æ€§ï¼Œæ— æ³•åˆ¤æ–­æ‰€è¯±å¯¼çš„â€œæ€è€ƒé“¾â€ï¼ˆCoTï¼‰æ˜¯å¦çœŸæ­£æé«˜äº†ç­”æ¡ˆçš„è´¨é‡ã€‚æ­¤å¤–ï¼Œæ­¤ç±»é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒå¯¹äºé€»è¾‘æ·±åº¦çš„æ§åˆ¶æœ‰é™ï¼Œå› æ­¤å¯èƒ½æ— æ³•æ­ç¤ºæ¨¡å‹çš„çœŸæ­£æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†åŠ¨æ€æ¨ç†æ•ˆç‡å¥–åŠ±ï¼ˆDRERï¼‰â€”â€”ä¸€ç§å³æ’å³ç”¨çš„RLå¥–åŠ±æ¡†æ¶ï¼Œç”¨äºé‡å¡‘å¥–åŠ±å’Œä¼˜åŠ¿ä¿¡å·ã€‚ï¼ˆä¸€ï¼‰æ¨ç†è´¨é‡å¥–åŠ±å¯¹äºé‚£äº›æ˜æ˜¾æé«˜æ­£ç¡®ç­”æ¡ˆæ¦‚ç‡çš„æ¨ç†é“¾ç»™äºˆç²¾ç»†çš„ä¿¡ç”¨ï¼Œç›´æ¥æ¿€åŠ±æœ‰ç›Šçš„CoTæ ‡è®°çš„è½¨è¿¹ã€‚ï¼ˆäºŒï¼‰åŠ¨æ€é•¿åº¦ä¼˜åŠ¿è¡°å‡é‚£äº›é•¿åº¦åç¦»éªŒè¯æ´¾ç”Ÿé˜ˆå€¼çš„å“åº”çš„ä¼˜åŠ¿ï¼Œä½¿è®­ç»ƒç¨³å®šã€‚ä¸ºäº†æ–¹ä¾¿ä¸¥æ ¼è¯„ä¼°ï¼Œæˆ‘ä»¬è¿˜å‘å¸ƒäº†Logictreeï¼Œè¿™æ˜¯ä¸€ä¸ªåŠ¨æ€æ„å»ºçš„æ¼”ç»æ¨ç†æ•°æ®é›†ï¼Œæ—¢å¯ä½œä¸ºRLè®­ç»ƒæ•°æ®ï¼Œä¹Ÿå¯ä½œä¸ºå…¨é¢çš„åŸºå‡†æµ‹è¯•ã€‚å®éªŒè¯å®äº†DRERçš„æœ‰æ•ˆæ€§ï¼šæˆ‘ä»¬çš„70äº¿å‚æ•°æ¨¡å‹åœ¨Logictreeä¸Šä»…ç»è¿‡400æ­¥è®­ç»ƒå°±è¾¾åˆ°äº†GPT-o3-miniçº§åˆ«çš„æ€§èƒ½ï¼Œè€ŒCoTå¢å¼ºç­”æ¡ˆçš„å¹³å‡ç½®ä¿¡åº¦æé«˜äº†30%ã€‚è¯¥æ¨¡å‹è¿˜å±•ç°å‡ºåœ¨å¤šç§é€»è¾‘æ¨ç†æ•°æ®é›†å’Œæ•°å­¦åŸºå‡†æµ‹è¯•AIME24ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›ç»“æœæ­ç¤ºäº†RLå¦‚ä½•å¡‘é€ CoTè¡Œä¸ºï¼Œå¹¶ä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å½¢å¼æ¨ç†èƒ½åŠ›å¼€è¾Ÿäº†ä¸€æ¡å®ç”¨é“è·¯ã€‚æ‰€æœ‰ä»£ç å’Œæ•°æ®éƒ½å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Henryhe09/DRER%E4%BB%93%E5%BA%93%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Henryhe09/DRERä»“åº“ä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06024v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„ä¸»å¯¼èŒƒå¼ã€‚ç„¶è€Œï¼Œå¸¸ç”¨çš„åŸºäºè§„åˆ™å¥–åŠ±å‡½æ•°ä»…åœ¨æ•°å­¦æˆ–ç¼–ç¨‹åŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°ç­”æ¡ˆçš„æ ¼å¼å’Œæ­£ç¡®æ€§ï¼Œæ— æ³•åˆ¤æ–­è¯±å¯¼çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ˜¯å¦çœŸæ­£æ”¹å–„ç­”æ¡ˆã€‚æ­¤å¤–ï¼Œè¿™ç§ä»»åŠ¡ç‰¹å®šè®­ç»ƒå¯¹é€»è¾‘æ·±åº¦çš„æ§åˆ¶æœ‰é™ï¼Œå› æ­¤å¯èƒ½æ— æ³•æ­ç¤ºæ¨¡å‹çš„çœŸæ­£æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºåŠ¨æ€æ¨ç†æ•ˆç‡å¥–åŠ±ï¼ˆDRERï¼‰â€”â€”ä¸€ç§å³æ’å³ç”¨çš„RLå¥–åŠ±æ¡†æ¶ï¼Œé‡å¡‘å¥–åŠ±å’Œä¼˜åŠ¿ä¿¡å·ã€‚ï¼ˆ1ï¼‰æ¨ç†è´¨é‡å¥–åŠ±ä¸ºé‚£äº›è¯æ˜èƒ½æé«˜æ­£ç¡®ç­”æ¡ˆæ¦‚ç‡çš„æ¨ç†é“¾åˆ†é…ç²¾ç»†ä¿¡ç”¨ï¼Œç›´æ¥æ¿€åŠ±æœ‰ç›Šçš„CoTä»¤ç‰Œè½¨è¿¹ã€‚ï¼ˆ2ï¼‰åŠ¨æ€é•¿åº¦ä¼˜åŠ¿è¡°å‡å“åº”é•¿åº¦åç¦»éªŒè¯æ´¾ç”Ÿé˜ˆå€¼çš„ä¼˜åŠ¿ï¼Œç¨³å®šè®­ç»ƒã€‚ä¸ºäº†ä¸¥æ ¼çš„è¯„ä¼°ï¼Œæˆ‘ä»¬è¿˜å‘å¸ƒäº†Logictreeï¼Œè¿™æ˜¯ä¸€ä¸ªåŠ¨æ€æ„å»ºçš„æ¼”ç»æ¨ç†æ•°æ®é›†ï¼Œæ—¢å¯ä½œä¸ºRLè®­ç»ƒæ•°æ®ï¼Œä¹Ÿå¯ä½œä¸ºå…¨é¢çš„åŸºå‡†æµ‹è¯•ã€‚å®éªŒè¯å®äº†DRERçš„æœ‰æ•ˆæ€§ï¼šæˆ‘ä»¬çš„70äº¿æ¨¡å‹åœ¨Logictreeä¸Šä»…è¿›è¡Œ400æ­¥è®­ç»ƒå³è¾¾åˆ°GPT-o3-miniçº§åˆ«çš„æ€§èƒ½ï¼Œè€ŒCoTå¢å¼ºç­”æ¡ˆçš„å¹³å‡ç½®ä¿¡åº¦æé«˜äº†30%ã€‚è¯¥æ¨¡å‹è¿˜å±•ç°å‡ºåœ¨å¤šæ ·åŒ–é€»è¾‘æ¨ç†æ•°æ®é›†å’Œæ•°å­¦åŸºå‡†æµ‹è¯•AIME24ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›ç»“æœæ­ç¤ºäº†RLå¦‚ä½•å¡‘é€ CoTè¡Œä¸ºï¼Œå¹¶ä¸ºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å½¢å¼æ¨ç†èƒ½åŠ›å¼€è¾Ÿäº†ä¸€æ¡å®ç”¨è·¯å¾„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å·²æˆä¸ºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„ä¸»å¯¼æ–¹æ³•ã€‚</li>
<li>ç°æœ‰å¥–åŠ±å‡½æ•°ä¸»è¦è¯„ä¼°ç­”æ¡ˆçš„æ ¼å¼å’Œæ­£ç¡®æ€§ï¼Œå¿½è§†æ¨ç†è´¨é‡çš„æå‡ã€‚</li>
<li>DRERæ¡†æ¶é€šè¿‡åŠ¨æ€è°ƒæ•´å¥–åŠ±å’Œä¼˜åŠ¿ä¿¡å·ï¼Œæ¿€åŠ±æœ‰ç›Šçš„é“¾å¼æ€ç»´è½¨è¿¹ã€‚</li>
<li>Logictreeæ•°æ®é›†çš„å‘å¸ƒä¸ºä¸¥æ ¼è¯„ä¼°æ¨¡å‹åœ¨æ¼”ç»æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½æä¾›äº†åŸºå‡†ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºDRERæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œæé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œç­”æ¡ˆçš„ç½®ä¿¡åº¦ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šæ ·åŒ–é€»è¾‘æ¨ç†æ•°æ®é›†å’Œæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06024">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.06024v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.06024v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.06024v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Coefficients-Preserving-Sampling-for-Reinforcement-Learning-with-Flow-Matching"><a href="#Coefficients-Preserving-Sampling-for-Reinforcement-Learning-with-Flow-Matching" class="headerlink" title="Coefficients-Preserving Sampling for Reinforcement Learning with Flow   Matching"></a>Coefficients-Preserving Sampling for Reinforcement Learning with Flow   Matching</h2><p><strong>Authors:Feng Wang, Zihao Yu</strong></p>
<p>Reinforcement Learning (RL) has recently emerged as a powerful technique for improving image and video generation in Diffusion and Flow Matching models, specifically for enhancing output quality and alignment with prompts. A critical step for applying online RL methods on Flow Matching is the introduction of stochasticity into the deterministic framework, commonly realized by Stochastic Differential Equation (SDE). Our investigation reveals a significant drawback to this approach: SDE-based sampling introduces pronounced noise artifacts in the generated images, which we found to be detrimental to the reward learning process. A rigorous theoretical analysis traces the origin of this noise to an excess of stochasticity injected during inference. To address this, we draw inspiration from Denoising Diffusion Implicit Models (DDIM) to reformulate the sampling process. Our proposed method, Coefficients-Preserving Sampling (CPS), eliminates these noise artifacts. This leads to more accurate reward modeling, ultimately enabling faster and more stable convergence for reinforcement learning-based optimizers like Flow-GRPO and Dance-GRPO. Code will be released at <a target="_blank" rel="noopener" href="https://github.com/IamCreateAI/FlowCPS">https://github.com/IamCreateAI/FlowCPS</a> </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æœ€è¿‘è¢«è¯æ˜æ˜¯ä¸€ç§å¼ºå¤§çš„æŠ€æœ¯ï¼Œå¯æ”¹å–„æ‰©æ•£å’ŒæµåŒ¹é…æ¨¡å‹ä¸­çš„å›¾åƒå’Œè§†é¢‘ç”Ÿæˆï¼Œç‰¹åˆ«æ˜¯åœ¨æé«˜è¾“å‡ºè´¨é‡å’Œä¸æç¤ºå¯¹é½æ–¹é¢ã€‚åœ¨æµåŒ¹é…ä¸­åº”ç”¨åœ¨çº¿RLæ–¹æ³•çš„å…³é”®æ­¥éª¤æ˜¯å°†éšæœºæ€§å¼•å…¥ç¡®å®šæ€§æ¡†æ¶ï¼Œè¿™é€šå¸¸é€šè¿‡éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰æ¥å®ç°ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥å‘ç°æ­¤æ–¹æ³•çš„é‡å¤§ç¼ºé™·ï¼šåŸºäºSDEçš„é‡‡æ ·åœ¨ç”Ÿæˆçš„å›¾åƒä¸­å¼•å…¥æ˜æ˜¾çš„å™ªå£°ä¼ªå½±ï¼Œæˆ‘ä»¬å‘ç°è¿™å¯¹å¥–åŠ±å­¦ä¹ è¿‡ç¨‹æ˜¯æœ‰å®³çš„ã€‚ä¸¥æ ¼çš„ç†è®ºåˆ†æå‘ç°è¿™ç§å™ªå£°æºäºæ¨ç†è¿‡ç¨‹ä¸­æ³¨å…¥çš„è¿‡å¤šéšæœºæ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ä»å»å™ªæ‰©æ•£éšæ¨¡å‹ï¼ˆDDIMï¼‰ä¸­æ±²å–çµæ„Ÿï¼Œé‡æ–°åˆ¶å®šé‡‡æ ·è¿‡ç¨‹ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•â€”â€”ç³»æ•°ä¿ç•™é‡‡æ ·ï¼ˆCPSï¼‰ï¼Œæ¶ˆé™¤äº†è¿™äº›å™ªå£°ä¼ªå½±ã€‚è¿™å¯¼è‡´æ›´å‡†ç¡®çš„å¥–åŠ±å»ºæ¨¡ï¼Œæœ€ç»ˆä½¿åŸºäºå¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–å™¨ï¼ˆå¦‚Flow-GRPOå’ŒDance-GRPOï¼‰å®ç°æ›´å¿«ã€æ›´ç¨³å®šçš„æ”¶æ•›ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/IamCreateAI/FlowCPS%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/IamCreateAI/FlowCPSä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05952v2">PDF</a> work in progress</p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ‰©æ•£å’ŒæµåŒ¹é…æ¨¡å‹çš„å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä¸­å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æé«˜è¾“å‡ºè´¨é‡å’Œä¸æç¤ºçš„å¯¹é½æ–¹é¢ã€‚ç„¶è€Œï¼Œåœ¨åº”ç”¨åœ¨çº¿RLæ–¹æ³•äºæµåŒ¹é…æ—¶ï¼Œå¼•å…¥éšæœºæ€§æ˜¯ä¸€ä¸ªå…³é”®æ­¥éª¤ï¼Œé€šå¸¸é€šè¿‡éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰å®ç°ã€‚ç ”ç©¶å‘ç°SDEé‡‡æ ·ä¼šå¼•å…¥æ˜æ˜¾çš„å™ªå£°ä¼ªå½±ï¼Œå¯¹å¥–åŠ±å­¦ä¹ è¿‡ç¨‹äº§ç”Ÿä¸åˆ©å½±å“ã€‚é€šè¿‡å€Ÿé‰´å»å™ªæ‰©æ•£éšæ¨¡å‹ï¼ˆDDIMï¼‰ï¼Œæˆ‘ä»¬é‡æ–°æ„å»ºäº†é‡‡æ ·è¿‡ç¨‹ï¼Œæå‡ºäº†ç³»æ•°ä¿ç•™é‡‡æ ·ï¼ˆCPSï¼‰æ–¹æ³•ï¼Œæœ‰æ•ˆæ¶ˆé™¤äº†å™ªå£°ä¼ªå½±ï¼Œå®ç°äº†æ›´å‡†ç¡®çš„å¥–åŠ±å»ºæ¨¡ï¼Œä½¿å¾—åŸºäºå¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–å™¨å¦‚Flow-GRPOå’ŒDance-GRPOçš„æ”¶æ•›æ›´å¿«æ›´ç¨³å®šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æ‰©æ•£å’ŒæµåŒ¹é…æ¨¡å‹çš„å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æé«˜è¾“å‡ºè´¨é‡å’Œå“åº”æç¤ºæ–¹é¢ã€‚</li>
<li>SDEé‡‡æ ·æ–¹æ³•å¼•å…¥çš„éšæœºæ€§ä¼šå¯¼è‡´ç”Ÿæˆçš„å›¾åƒä¸­å‡ºç°æ˜æ˜¾çš„å™ªå£°ä¼ªå½±ã€‚</li>
<li>å™ªå£°ä¼ªå½±å¯¹å¥–åŠ±å­¦ä¹ è¿‡ç¨‹å…·æœ‰è´Ÿé¢å½±å“ã€‚</li>
<li>é€šè¿‡å€Ÿé‰´DDIMæ¨¡å‹ï¼Œæˆ‘ä»¬æå‡ºäº†ç³»æ•°ä¿ç•™é‡‡æ ·ï¼ˆCPSï¼‰æ–¹æ³•ä»¥æ¶ˆé™¤å™ªå£°ä¼ªå½±ã€‚</li>
<li>CPSæ–¹æ³•æé«˜äº†å¥–åŠ±å»ºæ¨¡çš„å‡†ç¡®æ€§ã€‚</li>
<li>CPSæ–¹æ³•ä½¿å¾—å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨å¦‚Flow-GRPOå’ŒDance-GRPOçš„æ”¶æ•›æ›´å¿«æ›´ç¨³å®šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05952">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.05952v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.05952v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.05952v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.05952v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Chatbot-To-Help-Patients-Understand-Their-Health"><a href="#Chatbot-To-Help-Patients-Understand-Their-Health" class="headerlink" title="Chatbot To Help Patients Understand Their Health"></a>Chatbot To Help Patients Understand Their Health</h2><p><strong>Authors:Won Seok Jang, Hieu Tran, Manav Mistry, SaiKiran Gandluri, Yifan Zhang, Sharmin Sultana, Sunjae Kown, Yuan Zhang, Zonghai Yao, Hong Yu</strong></p>
<p>Patients must possess the knowledge necessary to actively participate in their care. We present NoteAid-Chatbot, a conversational AI that promotes patient understanding via a novel â€˜learning as conversationâ€™ framework, built on a multi-agent large language model (LLM) and reinforcement learning (RL) setup without human-labeled data. NoteAid-Chatbot was built on a lightweight LLaMA 3.2 3B model trained in two stages: initial supervised fine-tuning on conversational data synthetically generated using medical conversation strategies, followed by RL with rewards derived from patient understanding assessments in simulated hospital discharge scenarios. Our evaluation, which includes comprehensive human-aligned assessments and case studies, demonstrates that NoteAid-Chatbot exhibits key emergent behaviors critical for patient education, such as clarity, relevance, and structured dialogue, even though it received no explicit supervision for these attributes. Our results show that even simple Proximal Policy Optimization (PPO)-based reward modeling can successfully train lightweight, domain-specific chatbots to handle multi-turn interactions, incorporate diverse educational strategies, and meet nuanced communication objectives. Our Turing test demonstrates that NoteAid-Chatbot surpasses non-expert human. Although our current focus is on healthcare, the framework we present illustrates the feasibility and promise of applying low-cost, PPO-based RL to realistic, open-ended conversational domains, broadening the applicability of RL-based alignment methods. </p>
<blockquote>
<p>æ‚£è€…å¿…é¡»æŒæ¡ç§¯æå‚ä¸æŠ¤ç†æ‰€éœ€çš„çŸ¥è¯†ã€‚æˆ‘ä»¬æ¨å‡ºäº†NoteAid-Chatbotï¼Œè¿™æ˜¯ä¸€æ¬¾é€šè¿‡æ–°å‹â€œè¾¹å¯¹è¯è¾¹å­¦ä¹ â€æ¡†æ¶ä¿ƒè¿›æ‚£è€…ç†è§£çš„å¯¹è¯å¼äººå·¥æ™ºèƒ½ã€‚è¯¥æ¡†æ¶å»ºç«‹åœ¨å¤šä¸»ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®¾ç½®ä¸Šï¼Œæ— éœ€äººå·¥æ ‡æ³¨æ•°æ®ã€‚NoteAid-Chatbotå»ºç«‹åœ¨è½»é‡çº§LLaMA 3.2 3Bæ¨¡å‹ä¸Šï¼Œè¯¥æ¨¡å‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µè¿›è¡Œè®­ç»ƒï¼šé¦–å…ˆåœ¨åˆæˆç”Ÿæˆçš„åŒ»ç–—å¯¹è¯ç­–ç•¥æ•°æ®ä¸Šè¿›è¡Œåˆæ­¥çš„æœ‰ç›‘ç£å¾®è°ƒï¼Œç„¶ååœ¨æ¨¡æ‹Ÿå‡ºé™¢åœºæ™¯ä¸­çš„æ‚£è€…ç†è§£è¯„ä¼°ä¸­é€šè¿‡å¼ºåŒ–å­¦ä¹ è·å¾—å¥–åŠ±ã€‚æˆ‘ä»¬çš„è¯„ä¼°åŒ…æ‹¬å…¨é¢çš„äººç±»å¯¹é½è¯„ä¼°å’Œæ¡ˆä¾‹ç ”ç©¶ï¼Œç»“æœè¡¨æ˜NoteAid-Chatbotå±•ç°å‡ºå¯¹æ‚£è€…æ•™è‚²è‡³å…³é‡è¦çš„å…³é”®çªå‘è¡Œä¸ºï¼Œå¦‚æ¸…æ™°åº¦ã€å…³è”æ€§å’Œç»“æ„åŒ–å¯¹è¯ï¼Œå°½ç®¡å®ƒå¹¶æœªé’ˆå¯¹è¿™äº›å±æ€§æ¥å—æ˜ç¡®çš„ç›‘ç£ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå³ä½¿ç®€å•çš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰åŸºäºå¥–åŠ±çš„å»ºæ¨¡ä¹Ÿèƒ½æˆåŠŸè®­ç»ƒè½»é‡çº§ã€ç‰¹å®šé¢†åŸŸçš„èŠå¤©æœºå™¨äººï¼Œä»¥å¤„ç†å¤šè½®äº’åŠ¨ã€èå…¥å¤šæ ·çš„æ•™è‚²ç­–ç•¥å¹¶å®ç°å¾®å¦™çš„æ²Ÿé€šç›®æ ‡ã€‚æˆ‘ä»¬çš„å›¾çµæµ‹è¯•è¯æ˜NoteAid-Chatbotè¶…è¶Šäº†éä¸“ä¸šäººå£«çš„è¡¨ç°ã€‚å°½ç®¡æˆ‘ä»¬ç›®å‰çš„é‡ç‚¹åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸï¼Œä½†æˆ‘ä»¬æ‰€å±•ç¤ºçš„æ¡†æ¶è¯´æ˜äº†å°†ä½æˆæœ¬PPOå‹RLåº”ç”¨äºç°å®å¼€æ”¾å¼å¯¹è¯é¢†åŸŸçš„å¯è¡Œæ€§å’Œå‰æ™¯ï¼Œæ‰©å¤§äº†åŸºäºRLçš„å¯¹é½æ–¹æ³•çš„é€‚ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05818v1">PDF</a> Accepted in EMNLP 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>NoteAid-Chatbotæ˜¯ä¸€ä¸ªåŸºäºå¯¹è¯çš„äººå·¥æ™ºèƒ½ï¼Œé€šè¿‡â€œå­¦ä¹ å¯¹è¯â€æ¡†æ¶ä¿ƒè¿›æ‚£è€…å¯¹è‡ªèº«æŠ¤ç†çŸ¥è¯†çš„ç†è§£ã€‚å®ƒé‡‡ç”¨å¤šä»£ç†å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ— éœ€äººå·¥æ ‡æ³¨æ•°æ®çš„å¼ºåŒ–å­¦ä¹ è®¾ç½®è¿›è¡Œæ„å»ºã€‚ç»è¿‡åˆå§‹çš„åŸºäºåˆæˆåŒ»ç–—å¯¹è¯æ•°æ®çš„ç›‘ç£å¾®è°ƒåï¼Œä½¿ç”¨æ¥è‡ªæ¨¡æ‹Ÿå‡ºé™¢åœºæ™¯ä¸­æ‚£è€…ç†è§£è¯„ä¼°çš„å¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒNoteAid-Chatbotå±•ç°å‡ºå¯¹æ‚£è€…æ•™è‚²è‡³å…³é‡è¦çš„å…³é”®çªå‘è¡Œä¸ºï¼Œå¦‚æ¸…æ™°æ€§ã€ç›¸å…³æ€§å’Œç»“æ„åŒ–å¯¹è¯ã€‚å°½ç®¡æœªå¯¹è¿™äº›å±æ€§è¿›è¡Œæ˜ç¡®çš„ç›‘ç£ï¼Œä½†è¯¥æ¨¡å‹ä»è¡¨ç°å‡ºæˆåŠŸå¤„ç†å¤šè½®äº’åŠ¨ã€èå…¥å¤šå…ƒæ•™è‚²ç­–ç•¥å’Œè¾¾æˆå¾®å¦™æ²Ÿé€šç›®æ ‡çš„èƒ½åŠ›ã€‚åœ¨æ¨¡ä»¿å›¾çµæµ‹è¯•ä¸­ï¼ŒNoteAid-Chatbotè¶…è¶Šäº†éä¸“ä¸šäººå£«çš„è¡¨ç°ã€‚å°½ç®¡ç›®å‰ä¸“æ³¨äºåŒ»ç–—ä¿å¥é¢†åŸŸçš„åº”ç”¨ï¼Œä½†è¯¥æ¡†æ¶å±•ç¤ºäº†å°†ä½æˆæœ¬å¼ºåŒ–å­¦ä¹ åº”ç”¨äºç°å®å¼€æ”¾å¼å¯¹è¯é¢†åŸŸçš„å¯è¡Œæ€§å’Œå‰æ™¯ï¼Œæ‹“å®½äº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¯¹é½æ–¹æ³•çš„åº”ç”¨èŒƒå›´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NoteAid-Chatbotæ˜¯ä¸€ä¸ªå¯¹è¯å¼äººå·¥æ™ºèƒ½ï¼Œé€šè¿‡â€œå­¦ä¹ å¯¹è¯â€æ¡†æ¶æå‡æ‚£è€…å¯¹æŠ¤ç†çŸ¥è¯†çš„äº†è§£ã€‚</li>
<li>è¯¥ç³»ç»ŸåŸºäºå¤šä»£ç†å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ æ„å»ºï¼Œæ— éœ€äººå·¥æ ‡æ³¨æ•°æ®ã€‚</li>
<li>NoteAid-Chatbotç»è¿‡ç›‘ç£å¾®è°ƒåï¼Œé€šè¿‡æ¨¡æ‹Ÿçš„å‡ºé™¢åœºæ™¯ä¸­çš„æ‚£è€…ç†è§£è¯„ä¼°è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚</li>
<li>è¯¥ç³»ç»Ÿåœ¨æ‚£è€…æ•™è‚²æ–¹é¢å±•ç°å‡ºå…³é”®èƒ½åŠ›ï¼Œå¦‚æä¾›æ¸…æ™°ã€ç›¸å…³çš„ä¿¡æ¯ï¼Œä»¥åŠç»“æ„åŒ–å¯¹è¯ã€‚</li>
<li>NoteAid-Chatbotåœ¨æ¨¡ä»¿å›¾çµæµ‹è¯•ä¸­çš„è¡¨ç°è¶…è¶Šäº†éä¸“ä¸šäººå£«ã€‚</li>
<li>è¯¥ç³»ç»Ÿç›®å‰ä¸“æ³¨äºåŒ»ç–—ä¿å¥é¢†åŸŸï¼Œä½†å…¶æ¡†æ¶å¯åº”ç”¨äºå…¶ä»–å¼€æ”¾å¼å¯¹è¯é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05818">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.05818v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.05818v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.05818v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.05818v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.05818v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.05818v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.05818v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="TalkToAgent-A-Human-centric-Explanation-of-Reinforcement-Learning-Agents-with-Large-Language-Models"><a href="#TalkToAgent-A-Human-centric-Explanation-of-Reinforcement-Learning-Agents-with-Large-Language-Models" class="headerlink" title="TalkToAgent: A Human-centric Explanation of Reinforcement Learning   Agents with Large Language Models"></a>TalkToAgent: A Human-centric Explanation of Reinforcement Learning   Agents with Large Language Models</h2><p><strong>Authors:Haechang Kim, Hao Chen, Can Li, Jong Min Lee</strong></p>
<p>Explainable Reinforcement Learning (XRL) has emerged as a promising approach in improving the transparency of Reinforcement Learning (RL) agents. However, there remains a gap between complex RL policies and domain experts, due to the limited comprehensibility of XRL results and isolated coverage of current XRL approaches that leave users uncertain about which tools to employ. To address these challenges, we introduce TalkToAgent, a multi-agent Large Language Models (LLM) framework that delivers interactive, natural language explanations for RL policies. The architecture with five specialized LLM agents (Coordinator, Explainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically map user queries to relevant XRL tools and clarify an agentâ€™s actions in terms of either key state variables, expected outcomes, or counterfactual explanations. Moreover, our approach extends previous counterfactual explanations by deriving alternative scenarios from qualitative behavioral descriptions, or even new rule-based policies. We validated TalkToAgent on quadruple-tank process control problem, a well-known nonlinear control benchmark. Results demonstrated that TalkToAgent successfully mapped user queries into XRL tasks with high accuracy, and coder-debugger interactions minimized failures in counterfactual generation. Furthermore, qualitative evaluation confirmed that TalkToAgent effectively interpreted agentâ€™s actions and contextualized their meaning within the problem domain. </p>
<blockquote>
<p>å¯è§£é‡Šæ€§å¼ºåŒ–å­¦ä¹ ï¼ˆXRLï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œåœ¨æå‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ™ºèƒ½ä½“çš„é€æ˜åº¦æ–¹é¢å±•ç°å‡ºäº†å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºXRLç»“æœçš„å¯ç†è§£æ€§æœ‰é™ä»¥åŠå½“å‰XRLæ–¹æ³•çš„å­¤ç«‹è¦†ç›–ï¼Œä½¿å¾—å¤æ‚RLç­–ç•¥ä¸é¢†åŸŸä¸“å®¶ä¹‹é—´å­˜åœ¨å·®è·ï¼Œä½¿ç”¨æˆ·å¯¹åº”è¯¥ä½¿ç”¨å“ªäº›å·¥å…·æ„Ÿåˆ°ä¸ç¡®å®šã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†TalkToAgentï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¡†æ¶ï¼Œå¯ä»¥ä¸ºRLç­–ç•¥æä¾›äº¤äº’å¼è‡ªç„¶è¯­è¨€è§£é‡Šã€‚è¯¥æ¶æ„åŒ…å«äº”ä¸ªä¸“ä¸šLLMæ™ºèƒ½ä½“ï¼ˆåè°ƒå™¨ã€è§£é‡Šå™¨ã€ç¼–ç å™¨ã€è¯„ä¼°å™¨å’Œè°ƒè¯•å™¨ï¼‰ï¼Œä½¿TalkToAgentèƒ½å¤Ÿè‡ªåŠ¨å°†ç”¨æˆ·æŸ¥è¯¢æ˜ å°„åˆ°ç›¸å…³XRLå·¥å…·ï¼Œå¹¶æ ¹æ®å…³é”®çŠ¶æ€å˜é‡ã€é¢„æœŸç»“æœæˆ–åäº‹å®è§£é‡Šæ¥æ¾„æ¸…æ™ºèƒ½ä½“çš„è¡ŒåŠ¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä»å®šæ€§è¡Œä¸ºæè¿°æˆ–ç”šè‡³æ–°çš„åŸºäºè§„åˆ™çš„ç­–ç•¥ä¸­å¾—å‡ºæ›¿ä»£åœºæ™¯ï¼Œæ‰©å±•äº†ä¹‹å‰çš„åäº‹å®è§£é‡Šã€‚æˆ‘ä»¬åœ¨è‘—åçš„éçº¿æ€§æ§åˆ¶åŸºå‡†â€”â€”å››ç½è¿‡ç¨‹æ§åˆ¶é—®é¢˜ä¸ŠéªŒè¯äº†TalkToAgentã€‚ç»“æœè¡¨æ˜ï¼ŒTalkToAgentæˆåŠŸåœ°å°†ç”¨æˆ·æŸ¥è¯¢æ˜ å°„åˆ°XRLä»»åŠ¡ï¼Œå¹¶å®ç°äº†é«˜å‡†ç¡®åº¦ï¼›ç¼–ç å™¨å’Œè°ƒè¯•å™¨ä¹‹é—´çš„äº¤äº’æœ€å°åŒ–äº†åäº‹å®ç”Ÿæˆä¸­çš„å¤±è´¥ã€‚æ­¤å¤–ï¼Œå®šæ€§è¯„ä¼°è¯å®ï¼ŒTalkToAgentæœ‰æ•ˆåœ°è§£é‡Šäº†æ™ºèƒ½ä½“çš„è¡ŒåŠ¨ï¼Œå¹¶å°†å®ƒä»¬çš„é—®é¢˜åŸŸèƒŒæ™¯è¿›è¡Œäº†è”ç³»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04809v2">PDF</a> 31 pages total</p>
<p><strong>Summary</strong>ï¼š<br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„é€æ˜åº¦é—®é¢˜å¯é€šè¿‡è§£é‡Šæ€§å¼ºåŒ–å­¦ä¹ ï¼ˆXRLï¼‰è¿›è¡Œæ”¹è¿›ã€‚ç„¶è€Œï¼Œå½“å‰ä»å­˜åœ¨å¤æ‚RLç­–ç•¥ä¸é¢†åŸŸä¸“å®¶ä¹‹é—´çš„é¸¿æ²Ÿï¼ŒåŸå› åœ¨äºXRLç»“æœçš„å¯ç†è§£æ€§æœ‰é™ä»¥åŠå½“å‰XRLæ–¹æ³•çš„å­¤ç«‹è¦†ç›–ï¼Œä½¿ç”¨æˆ·å¯¹ä½¿ç”¨å“ªç§å·¥å…·æ„Ÿåˆ°å›°æƒ‘ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºTalkToAgentï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“è‡ªç„¶è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œå¯ä¸ºRLç­–ç•¥æä¾›äº¤äº’å¼è‡ªç„¶è¯­è¨€è§£é‡Šã€‚TalkToAgentåŒ…å«äº”ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ï¼ˆåè°ƒå™¨ã€è§£é‡Šå™¨ã€ç¼–ç å‘˜ã€è¯„ä¼°å™¨å’Œè°ƒè¯•å™¨ï¼‰ï¼Œå¯è‡ªåŠ¨å°†ç”¨æˆ·æŸ¥è¯¢æ˜ å°„åˆ°ç›¸å…³çš„XRLå·¥å…·ï¼Œå¹¶æ ¹æ®å…³é”®çŠ¶æ€å˜é‡ã€é¢„æœŸç»“æœæˆ–åäº‹å®è§£é‡Šé˜æ˜æ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¯¹å®šæ€§è¡Œä¸ºæè¿°æˆ–æ–°çš„åŸºäºè§„åˆ™çš„ç­–ç•¥è¿›è¡Œè¡ç”Ÿï¼Œæ‰©å±•äº†å…ˆå‰çš„åäº‹å®è§£é‡Šã€‚æˆ‘ä»¬åœ¨è‘—åçš„éçº¿æ€§æ§åˆ¶åŸºå‡†é—®é¢˜â€”â€”å››é‡æ°´ç®±è¿‡ç¨‹æ§åˆ¶é—®é¢˜ä¸ŠéªŒè¯äº†TalkToAgentã€‚ç»“æœè¡¨æ˜ï¼ŒTalkToAgentæˆåŠŸåœ°å°†ç”¨æˆ·æŸ¥è¯¢æ˜ å°„åˆ°XRLä»»åŠ¡ä¸­ï¼Œä¸”å…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚ç¼–ç å‘˜ä¸è°ƒè¯•å™¨ä¹‹é—´çš„äº’åŠ¨å‡å°‘äº†åäº‹å®ç”Ÿæˆä¸­çš„æ•…éšœã€‚æ­¤å¤–ï¼Œå®šæ€§è¯„ä¼°è¯å®TalkToAgentæœ‰æ•ˆåœ°è§£é‡Šäº†æ™ºèƒ½ä½“çš„è¡Œä¸ºå¹¶å°†å…¶ç½®äºé—®é¢˜åŸŸä¸­ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>XRLæé«˜äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„é€æ˜åº¦ï¼Œä½†ä»ç„¶å­˜åœ¨ä¸é¢†åŸŸä¸“å®¶çš„é¸¿æ²Ÿã€‚</li>
<li>TalkToAgentæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“è‡ªç„¶è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>TalkToAgentåŒ…å«äº”ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ï¼Œå¯è‡ªåŠ¨æ˜ å°„ç”¨æˆ·æŸ¥è¯¢åˆ°ç›¸å…³çš„XRLå·¥å…·ã€‚</li>
<li>TalkToAgentèƒ½ä»¥å…³é”®çŠ¶æ€å˜é‡ã€é¢„æœŸç»“æœæˆ–åäº‹å®è§£é‡Šçš„å½¢å¼é˜æ˜æ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚</li>
<li>è¯¥æ–¹æ³•æ‰©å±•äº†å…ˆå‰çš„åäº‹å®è§£é‡Šï¼Œé€šè¿‡å®šæ€§è¡Œä¸ºæè¿°æˆ–æ–°çš„åŸºäºè§„åˆ™çš„ç­–ç•¥æ¥è¡ç”Ÿæ›¿ä»£åœºæ™¯ã€‚</li>
<li>åœ¨å››é‡æ°´ç®±è¿‡ç¨‹æ§åˆ¶é—®é¢˜ä¸Šçš„éªŒè¯æ˜¾ç¤ºï¼ŒTalkToAgentæˆåŠŸåœ°å°†ç”¨æˆ·æŸ¥è¯¢æ˜ å°„åˆ°XRLä»»åŠ¡ä¸­å¹¶å…·æœ‰é«˜å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04809">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.04809v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.04809v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.04809v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.04809v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="AraHalluEval-A-Fine-grained-Hallucination-Evaluation-Framework-for-Arabic-LLMs"><a href="#AraHalluEval-A-Fine-grained-Hallucination-Evaluation-Framework-for-Arabic-LLMs" class="headerlink" title="AraHalluEval: A Fine-grained Hallucination Evaluation Framework for   Arabic LLMs"></a>AraHalluEval: A Fine-grained Hallucination Evaluation Framework for   Arabic LLMs</h2><p><strong>Authors:Aisha Alansari, Hamzah Luqman</strong></p>
<p>Recently, extensive research on the hallucination of the large language models (LLMs) has mainly focused on the English language. Despite the growing number of multilingual and Arabic-specific LLMs, evaluating LLMsâ€™ hallucination in the Arabic context remains relatively underexplored. The knowledge gap is particularly pressing given Arabicâ€™s widespread use across many regions and its importance in global communication and media. This paper presents the first comprehensive hallucination evaluation of Arabic and multilingual LLMs on two critical Arabic natural language generation tasks: generative question answering (GQA) and summarization. This study evaluates a total of 12 LLMs, including 4 Arabic pre-trained models, 4 multilingual models, and 4 reasoning-based models. To assess the factual consistency and faithfulness of LLMsâ€™ outputs, we developed a fine-grained hallucination evaluation framework consisting of 12 fine-grained hallucination indicators that represent the varying characteristics of each task. The results reveal that factual hallucinations are more prevalent than faithfulness errors across all models and tasks. Notably, the Arabic pre-trained model Allam consistently demonstrates lower hallucination rates than multilingual models and a comparative performance with reasoning-based models. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/aishaalansari57/AraHalluEval">https://github.com/aishaalansari57/AraHalluEval</a> </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå…³äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹»è§‰çš„å¹¿æ³›ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‹±è¯­ä¸Šã€‚å°½ç®¡å¤šè¯­è¨€åŒ–å’Œé˜¿æ‹‰ä¼¯ç‰¹å®šçš„å¤§å‹è¯­è¨€æ¨¡å‹æ•°é‡ä¸æ–­å¢åŠ ï¼Œä½†åœ¨é˜¿æ‹‰ä¼¯è¯­å¢ƒä¸‹è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹»è§‰ä»ç„¶ç›¸å¯¹è¢«å¿½è§†ã€‚è€ƒè™‘åˆ°é˜¿æ‹‰ä¼¯è¯­åœ¨è®¸å¤šåœ°åŒºçš„å¹¿æ³›ä½¿ç”¨ä»¥åŠåœ¨å…¨çƒæ²Ÿé€šå’Œåª’ä½“ä¸­çš„é‡è¦æ€§ï¼ŒçŸ¥è¯†å·®è·å°¤ä¸ºç´§è¿«ã€‚æœ¬æ–‡é¦–æ¬¡å¯¹é˜¿æ‹‰ä¼¯è¯­å’Œå¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸¤é¡¹å…³é”®çš„é˜¿æ‹‰ä¼¯è¯­è‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡ä¸Šçš„å¹»è§‰è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼šç”Ÿæˆæ€§é—®é¢˜å›ç­”ï¼ˆGQAï¼‰å’Œæ‘˜è¦ã€‚æœ¬ç ”ç©¶å…±è¯„ä¼°äº†12ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬4ç§é˜¿æ‹‰ä¼¯è¯­é¢„è®­ç»ƒæ¨¡å‹ã€4ç§å¤šè¯­è¨€æ¨¡å‹å’Œ4ç§åŸºäºæ¨ç†çš„æ¨¡å‹ã€‚ä¸ºäº†è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹è¾“å‡ºçš„äº‹å®ä¸€è‡´æ€§å’Œå¿ å®åº¦ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç²¾ç»†çš„å¹»è§‰è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬ä»£è¡¨æ¯ä¸ªä»»åŠ¡ä¸åŒç‰¹ç‚¹çš„12ä¸ªç²¾ç»†å¹»è§‰æŒ‡æ ‡ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨æ‰€æœ‰æ¨¡å‹å’Œä»»åŠ¡ä¸­ï¼Œäº‹å®å¹»è§‰æ¯”å¿ å®åº¦é”™è¯¯æ›´ä¸ºæ™®éã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé˜¿æ‹‰ä¼¯è¯­çš„é¢„è®­ç»ƒæ¨¡å‹Allamå§‹ç»ˆè¡¨ç°å‡ºè¾ƒä½çš„å¹»è§‰ç‡ï¼Œä¸å¤šè¯­è¨€æ¨¡å‹çš„æ€§èƒ½ç›¸æ¯”æœ‰æ‰€ä¸åŠï¼Œä½†èƒ½ä¸åŸºäºæ¨ç†çš„æ¨¡å‹ç›¸æŠ—è¡¡ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/aishaalansari57/AraHalluEval">https://github.com/aishaalansari57/AraHalluEval</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04656v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè¿‘æœŸå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹»æƒ³ï¼ˆhallucinationï¼‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‹±è¯­é¢†åŸŸã€‚å°½ç®¡é˜¿æ‹‰ä¼¯è¯­å’Œå¤šè¯­è¨€LLMçš„æ•°é‡ä¸æ–­å¢é•¿ï¼Œä½†åœ¨é˜¿æ‹‰ä¼¯è¯­ç¯å¢ƒä¸­è¯„ä¼°LLMçš„å¹»æƒ³ä»ç„¶ç›¸å¯¹è¢«å¿½è§†ã€‚æœ¬æ–‡é¦–æ¬¡å¯¹é˜¿æ‹‰ä¼¯è¯­å’Œå¤šè¯­è¨€LLMåœ¨ä¸¤é¡¹å…³é”®çš„é˜¿æ‹‰ä¼¯è¯­è‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡ï¼ˆç”Ÿæˆæ€§é—®é¢˜å›ç­”ï¼ˆGQAï¼‰å’Œæ‘˜è¦ï¼‰è¿›è¡Œäº†å…¨é¢çš„å¹»æƒ³è¯„ä¼°ã€‚ç ”ç©¶è¯„ä¼°äº†æ€»è®¡12ç§LLMsçš„è¡¨ç°ï¼ŒåŒ…æ‹¬4ç§é˜¿æ‹‰ä¼¯è¯­é¢„è®­ç»ƒæ¨¡å‹ã€4ç§å¤šè¯­è¨€æ¨¡å‹å’Œ4ç§åŸºäºæ¨ç†çš„æ¨¡å‹ã€‚ä¸ºäº†è¯„ä¼°LLMè¾“å‡ºçš„äº‹å®ä¸€è‡´æ€§å’Œå¿ å®åº¦ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç²¾ç»†çš„å¹»æƒ³è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«12ä¸ªç²¾ç»†çš„å¹»æƒ³æŒ‡æ ‡ï¼Œä»£è¡¨æ¯é¡¹ä»»åŠ¡çš„ä¸åŒç‰¹ç‚¹ã€‚ç»“æœè¡¨æ˜ï¼Œä¸å¿ å®åº¦é”™è¯¯ç›¸æ¯”ï¼Œäº‹å®å¹»æƒ³æ›´ä¸ºæ™®éã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé˜¿æ‹‰ä¼¯è¯­é¢„è®­ç»ƒæ¨¡å‹Allamçš„å¹»æƒ³ç‡ä¸€ç›´ä½äºå¤šè¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸åŸºäºæ¨ç†çš„æ¨¡å‹è¡¨ç°ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¿‘æœŸå…³äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¹»æƒ³ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‹±è¯­é¢†åŸŸï¼Œè€Œé˜¿æ‹‰ä¼¯è¯­å’Œå¤šè¯­è¨€LLMçš„è¯„ä¼°ç›¸å¯¹è¢«å¿½è§†ã€‚</li>
<li>è¯¥è®ºæ–‡é¦–æ¬¡å¯¹é˜¿æ‹‰ä¼¯è¯­å’Œå¤šè¯­è¨€LLMåœ¨GQAå’Œæ‘˜è¦ä¸¤é¡¹ä»»åŠ¡ä¸Šè¿›è¡Œå…¨é¢çš„å¹»æƒ³è¯„ä¼°ã€‚</li>
<li>ç ”ç©¶æ¶‰åŠäº†æ€»è®¡12ç§LLMsï¼ŒåŒ…æ‹¬ä¸åŒçš„æ¨¡å‹å’Œä»»åŠ¡ç±»å‹ã€‚</li>
<li>ä¸ºäº†è¯„ä¼°LLMè¾“å‡ºçš„äº‹å®ä¸€è‡´æ€§å’Œå¿ å®åº¦ï¼Œå¼€å‘äº†ä¸€ä¸ªç²¾ç»†çš„å¹»æƒ³è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>ç»“æœæ˜¾ç¤ºäº‹å®å¹»æƒ³çš„æ™®éæ€§é«˜äºå¿ å®åº¦é”™è¯¯ã€‚</li>
<li>é˜¿æ‹‰ä¼¯è¯­é¢„è®­ç»ƒæ¨¡å‹Allamåœ¨å¹»æƒ³ç‡æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½äºå¤šè¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸åŸºäºæ¨ç†çš„æ¨¡å‹ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04656">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.04656v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.04656v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.04656v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-11/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-11/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-11/LLM/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.05657v2/page_3_0.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-11  Parallel-R1 Towards Parallel Thinking via Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-10/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1085a7bea084d0b052bbd4df1d248d07.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-10  Let's Roleplay Examining LLM Alignment in Collaborative Dialogues
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32140.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
