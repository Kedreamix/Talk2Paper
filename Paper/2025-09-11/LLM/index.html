<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-11  Parallel-R1 Towards Parallel Thinking via Reinforcement Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.05657v2/page_3_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-11-æ›´æ–°"><a href="#2025-09-11-æ›´æ–°" class="headerlink" title="2025-09-11 æ›´æ–°"></a>2025-09-11 æ›´æ–°</h1><h2 id="Parallel-R1-Towards-Parallel-Thinking-via-Reinforcement-Learning"><a href="#Parallel-R1-Towards-Parallel-Thinking-via-Reinforcement-Learning" class="headerlink" title="Parallel-R1: Towards Parallel Thinking via Reinforcement Learning"></a>Parallel-R1: Towards Parallel Thinking via Reinforcement Learning</h2><p><strong>Authors:Tong Zheng, Hongming Zhang, Wenhao Yu, Xiaoyang Wang, Xinyu Yang, Runpeng Dai, Rui Liu, Huiwen Bao, Chengsong Huang, Heng Huang, Dong Yu</strong></p>
<p>Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose \textbf{Parallel-R1}, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals a clear shift in the modelâ€™s thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in a later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as a \textbf{mid-training exploration scaffold}, where this temporary exploratory phase unlocks a higher performance ceiling after RL, yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at <a target="_blank" rel="noopener" href="https://github.com/zhengkid/Parallel-R1">https://github.com/zhengkid/Parallel-R1</a>. </p>
<blockquote>
<p>å¹¶è¡Œæ€ç»´ä½œä¸ºä¸€ç§æ–°å‹æ–¹æ³•ï¼Œé€šè¿‡åŒæ—¶æ¢ç´¢å¤šä¸ªæ¨ç†è·¯å¾„æ¥æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œé€šè¿‡è®­ç»ƒæ¿€æ´»è¿™ç§èƒ½åŠ›ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºåˆæˆæ•°æ®ä¸Šçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿™é¼“åŠ±äº†æ•™å¸ˆå¼ºåˆ¶æ¨¡ä»¿ï¼Œè€Œä¸æ˜¯æ¢ç´¢å’Œæ³›åŒ–ã€‚ä¸å®ƒä»¬ä¸åŒï¼Œæˆ‘ä»¬æå‡ºäº†<strong>Parallel-R1</strong>ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿæ”¯æŒå¹¶è¡Œæ€ç»´è¡Œä¸ºçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œç”¨äºå¤„ç†å¤æ‚çš„ç°å®ä¸–ç•Œæ¨ç†ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ¡†æ¶é‡‡ç”¨äº†ä¸€ç§æ¸è¿›çš„è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œæ˜ç¡®è§£å†³äº†è®­ç»ƒå¹¶è¡Œæ€ç»´æ—¶çš„å†·å¯åŠ¨é—®é¢˜ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨SFTå¯¹æ¥è‡ªç®€å•ä»»åŠ¡çš„æç¤ºç”Ÿæˆè½¨è¿¹è¿›è¡Œè®­ç»ƒï¼Œä»¥çŒè¾“å¹¶è¡Œæ€ç»´èƒ½åŠ›ï¼Œç„¶åè¿‡æ¸¡åˆ°RLä»¥åœ¨æ›´å¤æ‚çš„é—®é¢˜ä¸Šæ¢ç´¢å¹¶æ¨å¹¿è¿™é¡¹æŠ€èƒ½ã€‚åœ¨å„ç§æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒï¼ŒåŒ…æ‹¬MATHã€AMC23å’ŒAIMEï¼Œè¡¨æ˜Parallel-R1æˆåŠŸåŸ¹å…»äº†å¹¶è¡Œæ€ç»´ï¼Œç›¸å¯¹äºç›´æ¥åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šä½¿ç”¨RLè®­ç»ƒçš„é¡ºåºæ€ç»´æ¨¡å‹ï¼Œå…¶å‡†ç¡®ç‡æé«˜äº†8.4%ã€‚è¿›ä¸€æ­¥çš„åˆ†ææ˜¾ç¤ºæ¨¡å‹æ€ç»´è¡Œä¸ºçš„æ˜æ˜¾è½¬å˜ï¼šåœ¨æ—©æœŸé˜¶æ®µï¼Œå®ƒä½¿ç”¨å¹¶è¡Œæ€ç»´ä½œä¸ºæ¢ç´¢ç­–ç•¥ï¼Œè€Œåœ¨åæœŸé˜¶æ®µï¼Œå®ƒä½¿ç”¨ç›¸åŒçš„èƒ½åŠ›è¿›è¡Œå¤šè§†è§’éªŒè¯ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬éªŒè¯äº†å¹¶è¡Œæ€ç»´ä½œä¸º<strong>ä¸­æœŸè®­ç»ƒä¸­çš„æ¢ç´¢è„šæ‰‹æ¶</strong>ï¼Œè¿™ä¸€æš‚æ—¶çš„æ¢ç´¢é˜¶æ®µåœ¨RLä¹‹åå¼€å¯äº†æ›´é«˜çš„æ€§èƒ½ä¸Šé™ï¼Œåœ¨AIME25ä¸Šçš„æ”¹è¿›æ¯”åŸºçº¿æé«˜äº†42.9%ã€‚æˆ‘ä»¬çš„æ¨¡å‹ã€æ•°æ®å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhengkid/Parallel-R1%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/zhengkid/Parallel-R1ä¸Šå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07980v1">PDF</a> Project website: <a target="_blank" rel="noopener" href="https://zhengkid.github.io/Parallel_R1.github.io/">https://zhengkid.github.io/Parallel_R1.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>åœ¨å¼ºåŒ–è®­ç»ƒè¯­è¨€æ¨¡å‹çš„è¿‡ç¨‹ä¸­ï¼Œä¸ºäº†æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„å¹¶è¡Œæ€ç»´è®­ç»ƒæ–¹æ³•â€”â€”Parallel-R1æ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºåˆæˆæ•°æ®çš„ç›‘ç£å¾®è°ƒæ–¹æ³•ä¸åŒï¼ŒParallel-R1é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¥å®ç°å¹¶è¡Œæ€ç»´è¡Œä¸ºï¼Œæ›´å¥½åœ°è§£å†³å¤æ‚ç°å®ä¸–ç•Œä¸­çš„æ¨ç†ä»»åŠ¡ã€‚å®ƒé¦–å…ˆé€šè¿‡åŸºäºç®€å•ä»»åŠ¡çš„æç¤ºè½¨è¿¹è¿›è¡Œæœ‰ç›‘ç£çš„è®­ç»ƒï¼Œä¼ æˆå¹¶è¡Œæ€ç»´èƒ½åŠ›ï¼›éšåè¿‡æ¸¡è‡³å¼ºåŒ–å­¦ä¹ ï¼Œè¿›ä¸€æ­¥æ¢ç´¢å’Œæ¨å¹¿è¿™ç§èƒ½åŠ›åœ¨å›°éš¾é—®é¢˜ä¸Šçš„ä½¿ç”¨ã€‚è¯¥æ¡†æ¶åœ¨ä¸åŒçš„æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šå±•ç°äº†æ˜æ˜¾çš„æ•ˆæœï¼Œç‰¹åˆ«æ˜¯ä½œä¸ºä¸­æœŸè®­ç»ƒä¸­çš„æ¢ç´¢æ¶æ„æ—¶è¡¨ç°ä¼˜å¼‚ã€‚å¹¶è¡Œæ€ç»´ä½œä¸ºä¸€ç§ä¸´æ—¶æ€§çš„æ¢ç´¢é˜¶æ®µï¼Œä¸ºåç»­å¼ºåŒ–å­¦ä¹ é˜¶æ®µå¸¦æ¥äº†æ›´é«˜çš„æ€§èƒ½ä¸Šé™ã€‚æ¨¡å‹å’Œæ•°æ®å°†åœ¨å…¬å¼€æºä»£ç å¹³å°ä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Parallel thinkingä½œä¸ºä¸€ç§æ–°é¢–çš„æ–¹æ³•è¢«å¼•å…¥ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡åˆæˆæ•°æ®ç›‘ç£å¾®è°ƒæ¥è®­ç»ƒæ¨¡å‹ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„æ¢ç´¢ä¸æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Parallel-R1æ¡†æ¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ å®ç°å¹¶è¡Œæ€ç»´è¡Œä¸ºï¼Œä»¥è§£å†³å¤æ‚ç°å®ä¸–ç•Œçš„æ¨ç†ä»»åŠ¡ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥è§£å†³å†·å¯åŠ¨é—®é¢˜ï¼Œé¦–å…ˆåœ¨æœ‰ç›‘ç£çš„ç¯å¢ƒä¸‹åŸ¹å…»å¹¶è¡Œæ€ç»´èƒ½åŠ›ï¼Œç„¶åè¿‡æ¸¡åˆ°å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ¢ç´¢å’Œæ¨å¹¿ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07980">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07980v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07980v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07980v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SimpleQA-Verified-A-Reliable-Factuality-Benchmark-to-Measure-Parametric-Knowledge"><a href="#SimpleQA-Verified-A-Reliable-Factuality-Benchmark-to-Measure-Parametric-Knowledge" class="headerlink" title="SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric   Knowledge"></a>SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric   Knowledge</h2><p><strong>Authors:Lukas Haas, Gal Yona, Giovanni Dâ€™Antonio, Sasha Goldshtein, Dipanjan Das</strong></p>
<p>We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large Language Model (LLM) short-form factuality based on OpenAIâ€™s SimpleQA. It addresses critical limitations in OpenAIâ€™s benchmark, including noisy and incorrect labels, topical biases, and question redundancy. SimpleQA Verified was created through a rigorous multi-stage filtering process involving de-duplication, topic balancing, and source reconciliation to produce a more reliable and challenging evaluation set, alongside improvements in the autorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a state-of-the-art F1-score of 55.6, outperforming other frontier models, including GPT-5. This work provides the research community with a higher-fidelity tool to track genuine progress in parametric model factuality and to mitigate hallucinations. The benchmark dataset, evaluation code, and leaderboard are available at: <a target="_blank" rel="noopener" href="https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified">https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†SimpleQA Verifiedï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºOpenAIçš„SimpleQAçš„åƒæ¬¡æç¤ºåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çŸ­äº‹å®æ€§ã€‚å®ƒè§£å†³äº†OpenAIåŸºå‡†æµ‹è¯•ä¸­çš„å…³é”®å±€é™æ€§ï¼ŒåŒ…æ‹¬æ ‡ç­¾å˜ˆæ‚å’Œé”™è¯¯ã€ä¸»é¢˜åè§å’Œé—®é¢˜å†—ä½™ã€‚SimpleQA Verifiedæ˜¯é€šè¿‡ä¸¥æ ¼çš„å¤šé˜¶æ®µè¿‡æ»¤è¿‡ç¨‹åˆ›å»ºçš„ï¼ŒåŒ…æ‹¬å»é‡ã€ä¸»é¢˜å¹³è¡¡å’Œæºå’Œè§£ï¼Œä»¥äº§ç”Ÿæ›´å¯é å’Œæœ‰æŒ‘æˆ˜æ€§çš„è¯„ä¼°é›†ï¼Œä»¥åŠè‡ªåŠ¨è¯„åˆ†æç¤ºçš„æ”¹è¿›ã€‚åœ¨è¿™ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ä¸Šï¼ŒåŒå­åº§2.5 Proè¾¾åˆ°äº†æœ€æ–°çš„F1åˆ†æ•°55.6ï¼Œä¼˜äºå…¶ä»–å‰æ²¿æ¨¡å‹ï¼ŒåŒ…æ‹¬GPT-5ã€‚è¿™é¡¹å·¥ä½œä¸ºç ”ç©¶é¢†åŸŸæä¾›äº†ä¸€ä¸ªæ›´é«˜ä¿çœŸåº¦çš„å·¥å…·ï¼Œä»¥è·Ÿè¸ªå‚æ•°æ¨¡å‹çš„çœŸå®è¿›æ­¥å¹¶å‡è½»è™šæ„ç°è±¡ã€‚åŸºå‡†æ•°æ®é›†ã€è¯„ä¼°ä»£ç å’Œæ’è¡Œæ¦œå¯åœ¨<a target="_blank" rel="noopener" href="https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://www.kaggle.com/benchmarks/deepmind/simpleqa-verifiedä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07968v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æˆ‘ä»¬æ¨å‡ºäº†SimpleQA Verifiedï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºOpenAI SimpleQAçš„åƒé¢˜åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çŸ­äº‹å®å‡†ç¡®æ€§ã€‚å®ƒè§£å†³äº†OpenAIåŸºå‡†æµ‹è¯•ä¸­çš„å…³é”®å±€é™æ€§ï¼ŒåŒ…æ‹¬æ ‡ç­¾å™ªå£°å’Œé”™è¯¯ã€ä¸»é¢˜åè§å’Œé—®é¢˜å†—ä½™ã€‚SimpleQA Verifiedé€šè¿‡ä¸¥æ ¼çš„å¤šé˜¶æ®µè¿‡æ»¤è¿‡ç¨‹åˆ›å»ºï¼ŒåŒ…æ‹¬å»é‡ã€ä¸»é¢˜å¹³è¡¡å’Œæºåè°ƒï¼Œä»¥äº§ç”Ÿæ›´å¯é å’Œæ›´å…·æŒ‘æˆ˜æ€§çš„è¯„ä¼°é›†ï¼Œä»¥åŠè‡ªåŠ¨è¯„åˆ†æç¤ºçš„æ”¹è¿›ã€‚åœ¨è¿™ä¸ªæ–°çš„åŸºå‡†ä¸Šï¼ŒGemini 2.5 Proè¾¾åˆ°äº†æœ€æ–°çš„F1åˆ†æ•°55.6ï¼Œè¶…è¿‡äº†å…¶ä»–å‰æ²¿æ¨¡å‹ï¼ŒåŒ…æ‹¬GPT-5ã€‚è¿™ä¸€å·¥ä½œä¸ºç ”ç©¶ç¤¾åŒºæä¾›äº†æ›´é«˜ä¿çœŸåº¦çš„å·¥å…·ï¼Œä»¥è·Ÿè¸ªå‚æ•°æ¨¡å‹äº‹å®æ€§çš„çœŸæ­£è¿›å±•ï¼Œå¹¶å‡è½»è™šæ„ç°è±¡ã€‚åŸºå‡†æ•°æ®é›†ã€è¯„ä¼°ä»£ç å’Œæ’è¡Œæ¦œå¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://www.kaggle.com/benchmarks/deepmind/simpleqa-verifiedä¸Šæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SimpleQA Verifiedæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMçš„çŸ­äº‹å®å‡†ç¡®æ€§ã€‚</li>
<li>å®ƒè§£å†³äº†ç°æœ‰åŸºå‡†æµ‹è¯•å¦‚OpenAI SimpleQAä¸­çš„å¤šä¸ªé—®é¢˜ï¼ŒåŒ…æ‹¬æ ‡ç­¾è´¨é‡å’Œè¯é¢˜å¤šæ ·æ€§ã€‚</li>
<li>SimpleQA Verifiedæ•°æ®é›†é€šè¿‡å¤šé˜¶æ®µè¿‡æ»¤è¿‡ç¨‹åˆ›å»ºï¼Œç¡®ä¿æ•°æ®è´¨é‡å’Œè¯„ä¼°çš„å…¬æ­£æ€§ã€‚</li>
<li>Gemini 2.5 Proåœ¨æ­¤æ–°åŸºå‡†ä¸Šå–å¾—äº†æœ€ä½³F1åˆ†æ•°55.6ï¼Œè¶…è¶Šäº†å…¶ä»–å‰æ²¿æ¨¡å‹ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºç¤¾åŒºæä¾›äº†ä¸€ä¸ªæ›´å‡†ç¡®çš„å·¥å…·æ¥è¯„ä¼°æ¨¡å‹çš„äº‹å®å‡†ç¡®æ€§ã€‚</li>
<li>åŸºå‡†æµ‹è¯•æ•°æ®é›†ã€è¯„ä¼°ä»£ç å’Œæ’è¡Œæ¦œå¯é€šè¿‡Kaggleå¹³å°è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07968">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07968v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07968v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07968v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07968v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07968v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07968v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ImportSnare-Directed-â€œCode-Manualâ€-Hijacking-in-Retrieval-Augmented-Code-Generation"><a href="#ImportSnare-Directed-â€œCode-Manualâ€-Hijacking-in-Retrieval-Augmented-Code-Generation" class="headerlink" title="ImportSnare: Directed â€œCode Manualâ€ Hijacking in Retrieval-Augmented   Code Generation"></a>ImportSnare: Directed â€œCode Manualâ€ Hijacking in Retrieval-Augmented   Code Generation</h2><p><strong>Authors:Kai Ye, Liangcai Su, Chenxiong Qian</strong></p>
<p>Code generation has emerged as a pivotal capability of Large Language Models(LLMs), revolutionizing development efficiency for programmers of all skill levels. However, the complexity of data structures and algorithmic logic often results in functional deficiencies and security vulnerabilities in generated code, reducing it to a prototype requiring extensive manual debugging. While Retrieval-Augmented Generation (RAG) can enhance correctness and security by leveraging external code manuals, it simultaneously introduces new attack surfaces.   In this paper, we pioneer the exploration of attack surfaces in Retrieval-Augmented Code Generation (RACG), focusing on malicious dependency hijacking. We demonstrate how poisoned documentation containing hidden malicious dependencies (e.g., matplotlib_safe) can subvert RACG, exploiting dual trust chains: LLM reliance on RAG and developersâ€™ blind trust in LLM suggestions. To construct poisoned documents, we propose ImportSnare, a novel attack framework employing two synergistic strategies: 1)Position-aware beam search optimizes hidden ranking sequences to elevate poisoned documents in retrieval results, and 2)Multilingual inductive suggestions generate jailbreaking sequences to manipulate LLMs into recommending malicious dependencies. Through extensive experiments across Python, Rust, and JavaScript, ImportSnare achieves significant attack success rates (over 50% for popular libraries such as matplotlib and seaborn) in general, and is also able to succeed even when the poisoning ratio is as low as 0.01%, targeting both custom and real-world malicious packages. Our findings reveal critical supply chain risks in LLM-powered development, highlighting inadequate security alignment for code generation tasks. To support future research, we will release the multilingual benchmark suite and datasets. The project homepage is <a target="_blank" rel="noopener" href="https://importsnare.github.io/">https://importsnare.github.io</a>. </p>
<blockquote>
<p>ä»£ç ç”Ÿæˆä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œå·²ç»å½»åº•æ”¹å˜äº†å„ä¸ªæŠ€èƒ½æ°´å¹³çš„ç¨‹åºå‘˜çš„å¼€å‘æ•ˆç‡ã€‚ç„¶è€Œï¼Œæ•°æ®ç»“æ„å’Œç®—æ³•é€»è¾‘çš„å¤æ‚æ€§å¸¸å¸¸å¯¼è‡´ç”Ÿæˆä»£ç çš„åŠŸèƒ½ç¼ºé™·å’Œå®‰å…¨æ¼æ´ï¼Œä½¿å…¶ä»…æ²¦ä¸ºéœ€è¦å¹¿æ³›æ‰‹åŠ¨è°ƒè¯•çš„åŸå‹ã€‚è™½ç„¶æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å¯ä»¥é€šè¿‡åˆ©ç”¨å¤–éƒ¨ä»£ç æ‰‹å†Œå¢å¼ºæ­£ç¡®æ€§å’Œå®‰å…¨æ€§ï¼Œä½†å®ƒåŒæ—¶å¼•å…¥äº†æ–°çš„æ”»å‡»é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07941v1">PDF</a> This paper has been accepted by the ACM Conference on Computer and   Communications Security (CCS) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œä»¥åŠç”±æ­¤äº§ç”Ÿçš„å¼€å‘æ•ˆç‡é©å‘½ã€‚ç„¶è€Œï¼Œå¤æ‚çš„æ•°æ®ç»“æ„å’Œç®—æ³•é€»è¾‘ä¼šå¯¼è‡´ç”Ÿæˆçš„ä»£ç å­˜åœ¨åŠŸèƒ½ç¼ºé™·å’Œå®‰å…¨æ¼æ´ï¼Œä½¿å¾—ç”Ÿæˆçš„ä»£ç æ›´åƒåŸå‹è€Œéå®Œæ•´å¯ç”¨äº§å“ã€‚ä¸ºæé«˜æ­£ç¡®æ€§ï¼Œç ”ç©¶è€…å°è¯•å°†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯å¼•å…¥ä»£ç ç”Ÿæˆè¿‡ç¨‹ï¼Œåˆ©ç”¨å¤–éƒ¨ä»£ç æ‰‹å†Œå¢å¼ºä»£ç çš„æ­£ç¡®æ€§å’Œå®‰å…¨æ€§ã€‚ç„¶è€Œï¼Œè¿™ç§æŠ€æœ¯ä¹Ÿå¸¦æ¥äº†æ–°çš„æ”»å‡»é¢ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶æ„ä¾èµ–åŠ«æŒæ–¹é¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºImportSnareçš„æ–°å‹æ”»å‡»æ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–éšè—æ’ååºåˆ—å’Œç”Ÿæˆè¯±å¯¼åºåˆ—æ¥æ“çºµLLMæ¨èæ¶æ„ä¾èµ–ï¼Œå®ç°å¯¹RACGçš„æ”»å‡»ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒImportSnareåœ¨ä¸åŒè¯­è¨€å’Œåº“ä¸­å‡æœ‰è¾ƒé«˜çš„æ”»å‡»æˆåŠŸç‡ã€‚æœ¬æ–‡æ­ç¤ºäº†LLMé©±åŠ¨å¼€å‘ä¸­çš„å…³é”®ä¾›åº”é“¾é£é™©ï¼Œå¹¶å¼ºè°ƒäº†ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­å®‰å…¨æ€§çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså·²ç»æ¼”å˜ä¸ºä»£ç ç”Ÿæˆçš„å…³é”®å·¥å…·ï¼Œæå¤§åœ°æé«˜äº†å¼€å‘æ•ˆç‡ã€‚</li>
<li>ç”Ÿæˆä»£ç å­˜åœ¨åŠŸèƒ½ç¼ºé™·å’Œå®‰å…¨æ¼æ´ï¼Œéœ€è¦å¢å¼ºæ­£ç¡®æ€§å’Œå®‰å…¨æ€§ã€‚</li>
<li>RAGæŠ€æœ¯é€šè¿‡åˆ©ç”¨å¤–éƒ¨ä»£ç æ‰‹å†Œå¢å¼ºä»£ç çš„æ­£ç¡®æ€§ï¼Œä½†åŒæ—¶ä¹Ÿå¼•å…¥äº†æ–°çš„æ”»å‡»é¢ã€‚</li>
<li>æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•é€šè¿‡æ¶æ„æ–‡æ¡£ä¸­çš„éšè—ä¾èµ–æ¥æ”»å‡»RACGï¼Œå¹¶å±•ç¤ºäº†æ”»å‡»çš„æˆåŠŸç‡ã€‚</li>
<li>ImportSnareæ”»å‡»æ¡†æ¶èƒ½å¤Ÿé€šè¿‡ä¼˜åŒ–éšè—æ’ååºåˆ—å’Œç”Ÿæˆè¯±å¯¼åºåˆ—æ¥æ“çºµLLMæ¨èæ¶æ„ä¾èµ–ã€‚</li>
<li>LLMé©±åŠ¨çš„å¼€å‘ä¸­å­˜åœ¨å…³é”®ä¾›åº”é“¾é£é™©ï¼Œéœ€è¦åŠ å¼ºå®‰å…¨æ€§æªæ–½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07941">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07941v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07941v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07941v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07941v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07941v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Guided-Reasoning-in-LLM-Driven-Penetration-Testing-Using-Structured-Attack-Trees"><a href="#Guided-Reasoning-in-LLM-Driven-Penetration-Testing-Using-Structured-Attack-Trees" class="headerlink" title="Guided Reasoning in LLM-Driven Penetration Testing Using Structured   Attack Trees"></a>Guided Reasoning in LLM-Driven Penetration Testing Using Structured   Attack Trees</h2><p><strong>Authors:Katsuaki Nakano, Reza Feyyazi, Shanchieh Jay Yang, Michael Zuzak</strong></p>
<p>Recent advances in Large Language Models (LLMs) have driven interest in automating cybersecurity penetration testing workflows, offering the promise of faster and more consistent vulnerability assessment for enterprise systems. Existing LLM agents for penetration testing primarily rely on self-guided reasoning, which can produce inaccurate or hallucinated procedural steps. As a result, the LLM agent may undertake unproductive actions, such as exploiting unused software libraries or generating cyclical responses that repeat prior tactics. In this work, we propose a guided reasoning pipeline for penetration testing LLM agents that incorporates a deterministic task tree built from the MITRE ATT&amp;CK Matrix, a proven penetration testing kll chain, to constrain the LLMâ€™s reaoning process to explicitly defined tactics, techniques, and procedures. This anchors reasoning in proven penetration testing methodologies and filters out ineffective actions by guiding the agent towards more productive attack procedures. To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios. Our proposed reasoning pipeline guided the LLM agent through 71.8%, 72.8%, and 78.6% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively. Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5%, 16.5%, and 75.7% of subtasks and required 86.2%, 118.7%, and 205.9% more model queries. This suggests that incorporating a deterministic task tree into LLM reasoning pipelines can enhance the accuracy and efficiency of automated cybersecurity assessments </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ¨åŠ¨äº†è‡ªåŠ¨åŒ–ç½‘ç»œå®‰å…¨æ¸—é€æµ‹è¯•å·¥ä½œæµç¨‹çš„å…´è¶£ï¼Œä¸ºä¼ä¸šç³»ç»Ÿçš„æ¼æ´è¯„ä¼°æä¾›äº†æ›´å¿«å’Œæ›´ä¸€è‡´çš„æ‰¿è¯ºã€‚ç°æœ‰çš„æ¸—é€æµ‹è¯•LLMä¸»è¦ä¾èµ–äºè‡ªæˆ‘å¼•å¯¼æ¨ç†ï¼Œè¿™å¯èƒ½ä¼šäº§ç”Ÿä¸å‡†ç¡®æˆ–è™šæ„çš„ç¨‹åºæ­¥éª¤ã€‚å› æ­¤ï¼ŒLLMä»£ç†å¯èƒ½ä¼šæ‰§è¡Œæ— æ•ˆæ“ä½œï¼Œä¾‹å¦‚åˆ©ç”¨æœªä½¿ç”¨çš„è½¯ä»¶åº“æˆ–ç”Ÿæˆé‡å¤å…ˆå‰ç­–ç•¥çš„å¾ªç¯å“åº”ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºæ¸—é€æµ‹è¯•LLMä»£ç†çš„å¼•å¯¼æ¨ç†ç®¡é“ï¼Œå®ƒç»“åˆäº†ç”±MITRE ATTï¼†CKçŸ©é˜µæ„å»ºçš„ç¡®å®šæ€§ä»»åŠ¡æ ‘ï¼Œè¿™æ˜¯ä¸€ä¸ªç»è¿‡éªŒè¯çš„æ¸—é€æµ‹è¯•æ€ä¼¤é“¾ï¼Œå°†LLMçš„æ¨ç†è¿‡ç¨‹é™åˆ¶åœ¨æ˜ç¡®å®šä¹‰çš„æˆ˜æœ¯ã€æŠ€æœ¯å’Œç¨‹åºå†…ã€‚è¿™ä½¿æ¨ç†é”šå®šåœ¨æˆç†Ÿçš„æ¸—é€æµ‹è¯•æ–¹æ³•ä¸Šï¼Œå¹¶é€šè¿‡å¼•å¯¼ä»£ç†æ‰§è¡Œæ›´æœ‰æ•ˆçš„æ”»å‡»ç¨‹åºæ¥è¿‡æ»¤æ‰æ— æ•ˆæ“ä½œã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸‰ä¸ªLLMï¼ˆLlama-3-8Bã€Gemini-1.5å’ŒGPT-4ï¼‰æ„å»ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„æ¸—é€æµ‹è¯•LLMä»£ç†ï¼Œå¹¶å°†å…¶åº”ç”¨äºå¯¼èˆª10ä¸ªHackTheBoxç½‘ç»œå®‰å…¨æ¼”ä¹ ä¸­çš„103ä¸ªç¦»æ•£å­ä»»åŠ¡ï¼Œä»£è¡¨çœŸå®çš„ç½‘ç»œæ”»å‡»åœºæ™¯ã€‚æˆ‘ä»¬æå‡ºçš„æ¨ç†ç®¡é“ä½¿ç”¨Llama-3-8Bã€Gemini-1.5å’ŒGPT-4åˆ†åˆ«å¼•å¯¼LLMä»£ç†å®Œæˆäº†71.8ï¼…ã€72.8ï¼…å’Œ78.6ï¼…çš„å­ä»»åŠ¡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä½¿ç”¨è‡ªæˆ‘å¼•å¯¼æ¨ç†çš„æœ€å…ˆè¿›çš„LLMæ¸—é€æµ‹è¯•å·¥å…·ä»…å®Œæˆäº†13.5ï¼…ã€16.5ï¼…å’Œ75.7ï¼…çš„å­ä»»åŠ¡ï¼Œå¹¶ä¸”éœ€è¦86.2ï¼…ã€118.7ï¼…å’Œ205.9ï¼…çš„æ›´å¤šæ¨¡å‹æŸ¥è¯¢ã€‚è¿™è¡¨æ˜åœ¨LLMæ¨ç†ç®¡é“ä¸­èå…¥ç¡®å®šæ€§ä»»åŠ¡æ ‘å¯ä»¥æé«˜ç½‘ç»œå®‰å…¨è¯„ä¼°çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07939v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•ä¸ºè‡ªåŠ¨åŒ–ç½‘ç»œå®‰å…¨æ¸—é€æµ‹è¯•å·¥ä½œæµç¨‹å¸¦æ¥äº†å¸Œæœ›ï¼Œæ‰¿è¯ºä¸ºä¼ä¸šç³»ç»Ÿæä¾›æ›´å¿«é€Ÿå’Œæ›´ä¸€è‡´çš„æ¼æ´è¯„ä¼°ã€‚ç°æœ‰LLMä»£ç†ä¸»è¦ä¾èµ–è‡ªæˆ‘å¼•å¯¼æ¨ç†ï¼Œå¯èƒ½äº§ç”Ÿä¸å‡†ç¡®æˆ–è™šæ„çš„ç¨‹åºæ­¥éª¤ã€‚å› æ­¤ï¼ŒLLMä»£ç†å¯èƒ½æ‰§è¡Œæ— æ•ˆåŠ¨ä½œï¼Œå¦‚åˆ©ç”¨æœªä½¿ç”¨çš„è½¯ä»¶åº“æˆ–ç”Ÿæˆé‡å¤ç­–ç•¥ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ¸—é€æµ‹è¯•LLMä»£ç†çš„å¼•å¯¼æ¨ç†ç®¡é“ï¼Œé‡‡ç”¨MITRE ATTï¼†CKçŸ©é˜µæ„å»ºçš„ç¡®å®šæ€§ä»»åŠ¡æ ‘ï¼Œçº¦æŸLLMçš„æ¨ç†è¿‡ç¨‹éµå¾ªæ˜ç¡®çš„æˆ˜æœ¯ã€æŠ€æœ¯å’Œç¨‹åºã€‚è¿™åŸºäºç»è¿‡éªŒè¯çš„æ¸—é€æµ‹è¯•æ–¹æ³•ï¼Œé€šè¿‡å¼•å¯¼ä»£ç†æ‰§è¡Œæ›´æœ‰æ•ˆçš„æ”»å‡»ç¨‹åºæ¥è¿‡æ»¤æ— æ•ˆåŠ¨ä½œã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸‰ä¸ªLLMï¼ˆLlama-3-8Bã€Gemini-1.5å’ŒGPT-4ï¼‰æ„å»ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¸—é€æµ‹è¯•LLMä»£ç†ï¼Œå¹¶åº”ç”¨äºå¯¼èˆª10ä¸ªHackTheBoxç½‘ç»œå®‰å…¨ç»ƒä¹ ï¼ŒåŒ…å«103ä¸ªç¦»æ•£å­ä»»åŠ¡ï¼Œæ¨¡æ‹ŸçœŸå®ç½‘ç»œæ”»å‡»åœºæ™¯ã€‚ä½¿ç”¨æˆ‘ä»¬çš„æ¨ç†ç®¡é“å¼•å¯¼çš„LLMä»£ç†å®Œæˆäº†71.8ï¼…ã€72.8ï¼…å’Œ78.6ï¼…çš„å­ä»»åŠ¡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä½¿ç”¨è‡ªæˆ‘å¼•å¯¼æ¨ç†çš„å…ˆè¿›LLMæ¸—é€æµ‹è¯•å·¥å…·ä»…å®Œæˆäº†13.5ï¼…ã€16.5ï¼…å’Œ75.7ï¼…çš„å­ä»»åŠ¡ï¼Œå¹¶éœ€è¦æ›´å¤šçš„æ¨¡å‹æŸ¥è¯¢ã€‚è¿™è¡¨æ˜å°†ç¡®å®šæ€§ä»»åŠ¡æ ‘çº³å…¥LLMæ¨ç†ç®¡é“å¯ä»¥æé«˜ç½‘ç»œå®‰å…¨è¯„ä¼°çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨è‡ªåŠ¨åŒ–ç½‘ç»œå®‰å…¨æ¸—é€æµ‹è¯•é¢†åŸŸå…·æœ‰åº”ç”¨æ½œåŠ›ã€‚</li>
<li>å½“å‰LLMä¸»è¦ä¾èµ–è‡ªæˆ‘å¼•å¯¼æ¨ç†ï¼Œå­˜åœ¨ä¸å‡†ç¡®å’Œæ— æ•ˆåŠ¨ä½œçš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥ç¡®å®šæ€§ä»»åŠ¡æ ‘ï¼ˆåŸºäºMITRE ATT&amp;CKçŸ©é˜µï¼‰èƒ½æœ‰æ•ˆçº¦æŸLLMçš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>æå‡ºçš„å¼•å¯¼æ¨ç†ç®¡é“æ˜¾è‘—æé«˜äº†LLMåœ¨æ¸—é€æµ‹è¯•ä¸­çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>åœ¨å®éªŒè¯„ä¼°ä¸­ï¼Œä½¿ç”¨å¼•å¯¼æ¨ç†ç®¡é“çš„LLMä»£ç†åœ¨å­ä»»åŠ¡å®Œæˆç‡æ–¹é¢ä¼˜äºè‡ªæˆ‘å¼•å¯¼æ¨ç†çš„LLMå·¥å…·ã€‚</li>
<li>ç¡®å®šæ€§ä»»åŠ¡æ ‘èƒ½æé«˜ç½‘ç»œå®‰å…¨è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07939">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07939v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07939v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07939v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GENUINE-Graph-Enhanced-Multi-level-Uncertainty-Estimation-for-Large-Language-Models"><a href="#GENUINE-Graph-Enhanced-Multi-level-Uncertainty-Estimation-for-Large-Language-Models" class="headerlink" title="GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large   Language Models"></a>GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large   Language Models</h2><p><strong>Authors:Tuo Wang, Adithya Kulkarni, Tyler Cody, Peter A. Beling, Yujun Yan, Dawei Zhou</strong></p>
<p>Uncertainty estimation is essential for enhancing the reliability of Large Language Models (LLMs), particularly in high-stakes applications. Existing methods often overlook semantic dependencies, relying on token-level probability measures that fail to capture structural relationships within the generated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty Estimation for Large Language Models, a structure-aware framework that leverages dependency parse trees and hierarchical graph pooling to refine uncertainty quantification. By incorporating supervised learning, GENUINE effectively models semantic and structural relationships, improving confidence assessments. Extensive experiments across NLP tasks show that GENUINE achieves up to 29% higher AUROC than semantic entropy-based approaches and reduces calibration errors by over 15%, demonstrating the effectiveness of graph-based uncertainty modeling. The code is available at <a target="_blank" rel="noopener" href="https://github.com/ODYSSEYWT/GUQ">https://github.com/ODYSSEYWT/GUQ</a>. </p>
<blockquote>
<p>ä¸ç¡®å®šæ€§ä¼°è®¡æ˜¯æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯é æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é£é™©åº”ç”¨ä¸­çš„å…³é”®ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å¿½ç•¥äº†è¯­ä¹‰ä¾èµ–å…³ç³»ï¼Œä¾èµ–äºä»¤ç‰Œçº§åˆ«çš„æ¦‚ç‡åº¦é‡ï¼Œæ— æ³•æ•è·ç”Ÿæˆæ–‡æœ¬ä¸­çš„ç»“æ„å…³ç³»ã€‚æˆ‘ä»¬æå‡ºGENUINEï¼šå¤§å‹è¯­è¨€æ¨¡å‹å›¾å¢å¼ºå¤šå±‚æ¬¡ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ˆGraph ENhanced mUlti-level uncertaINty Estimation for Large Language Modelsï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“æ„æ„ŸçŸ¥æ¡†æ¶ï¼Œåˆ©ç”¨ä¾èµ–è§£ææ ‘å’Œåˆ†å±‚å›¾æ± åŒ–æ¥ä¼˜åŒ–ä¸ç¡®å®šæ€§é‡åŒ–ã€‚é€šè¿‡å¼•å…¥æœ‰ç›‘ç£å­¦ä¹ ï¼ŒGENUINEå¯ä»¥æœ‰æ•ˆåœ°å»ºæ¨¡è¯­ä¹‰å’Œç»“æ„å…³ç³»ï¼Œä»è€Œæé«˜ä¿¡å¿ƒè¯„ä¼°ã€‚åœ¨å¤šä¸ªNLPä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGENUINEç›¸è¾ƒäºåŸºäºè¯­ä¹‰ç†µçš„æ–¹æ³•å®ç°äº†é«˜è¾¾29%çš„AUROCæå‡ï¼Œå¹¶é™ä½äº†è¶…è¿‡15%çš„æ ¡å‡†è¯¯å·®ï¼Œè¯æ˜äº†åŸºäºå›¾çš„ä¸ç¡®å®šæ€§å»ºæ¨¡çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ODYSSEYWT/GUQ%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ODYSSEYWT/GUQä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07925v1">PDF</a> Accepted by EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸ç¡®å®šæ€§ä¼°è®¡å¯¹äºæé«˜å…¶å¯é æ€§è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é£é™©çš„åº”ç”¨ä¸­ã€‚ç°æœ‰æ–¹æ³•å¸¸å¸¸å¿½è§†è¯­ä¹‰ä¾èµ–ï¼Œä¾èµ–è¯çº§æ¦‚ç‡åº¦é‡ï¼Œæ— æ³•æ•æ‰ç”Ÿæˆæ–‡æœ¬ä¸­çš„ç»“æ„å…³ç³»ã€‚æœ¬æ–‡æå‡ºGENUINEï¼šå¤§å‹è¯­è¨€æ¨¡å‹å›¾å¢å¼ºå¤šå±‚æ¬¡ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ˆGraph ENhanced mUlti-level uncertaINty Estimationï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“æ„æ„ŸçŸ¥çš„æ¡†æ¶ï¼Œåˆ©ç”¨ä¾å­˜è§£ææ ‘å’Œåˆ†å±‚å›¾æ± åŒ–æŠ€æœ¯æ¥ä¼˜åŒ–ä¸ç¡®å®šæ€§é‡åŒ–ã€‚é€šè¿‡å¼•å…¥æœ‰ç›‘ç£å­¦ä¹ ï¼ŒGENUINEæœ‰æ•ˆåœ°å»ºæ¨¡è¯­ä¹‰å’Œç»“æ„å…³ç³»ï¼Œæé«˜äº†ç½®ä¿¡åº¦è¯„ä¼°ã€‚åœ¨NLPä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸åŸºäºè¯­ä¹‰ç†µçš„æ–¹æ³•ç›¸æ¯”ï¼ŒGENUINEçš„AUROCæé«˜äº†é«˜è¾¾29%ï¼Œæ ¡å‡†è¯¯å·®é™ä½äº†è¶…è¿‡15%ï¼Œè¯æ˜äº†åŸºäºå›¾çš„ä¸ç¡®å®šæ€§å»ºæ¨¡çš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/ODYSSEYWT/GUQ%E3%80%82">https://github.com/ODYSSEYWT/GUQã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸ç¡®å®šæ€§ä¼°è®¡æ˜¯æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯é æ€§çš„å…³é”®ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é£é™©åº”ç”¨ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¸¸å¸¸å¿½è§†è¯­ä¹‰ä¾èµ–ï¼Œæ— æ³•å…¨é¢æ•æ‰æ–‡æœ¬ä¸­çš„ç»“æ„å…³ç³»ã€‚</li>
<li>GENUINEæ˜¯ä¸€ä¸ªæ–°çš„ç»“æ„æ„ŸçŸ¥æ¡†æ¶ï¼Œåˆ©ç”¨ä¾å­˜è§£ææ ‘å’Œåˆ†å±‚å›¾æ± åŒ–æŠ€æœ¯æ¥ä¼˜åŒ–ä¸ç¡®å®šæ€§é‡åŒ–ã€‚</li>
<li>GENUINEé€šè¿‡å¼•å…¥æœ‰ç›‘ç£å­¦ä¹ ï¼Œæœ‰æ•ˆåœ°å»ºæ¨¡è¯­ä¹‰å’Œç»“æ„å…³ç³»ï¼Œæé«˜ç½®ä¿¡åº¦è¯„ä¼°ã€‚</li>
<li>åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šï¼ŒGENUINEçš„æ€§èƒ½ä¼˜äºåŸºäºè¯­ä¹‰ç†µçš„æ–¹æ³•ï¼Œè¾¾åˆ°äº†é«˜è¾¾29%çš„AUROCæå‡ã€‚</li>
<li>GENUINEè¿˜èƒ½é™ä½æ ¡å‡†è¯¯å·®è¶…è¿‡15%ï¼Œæ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07925">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07925v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07925v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07925v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="HiPhO-How-Far-Are-M-LLMs-from-Humans-in-the-Latest-High-School-Physics-Olympiad-Benchmark"><a href="#HiPhO-How-Far-Are-M-LLMs-from-Humans-in-the-Latest-High-School-Physics-Olympiad-Benchmark" class="headerlink" title="HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics   Olympiad Benchmark?"></a>HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics   Olympiad Benchmark?</h2><p><strong>Authors:Fangchen Yu, Haiyuan Wan, Qianjia Cheng, Yuchen Zhang, Jiacheng Chen, Fujun Han, Yulun Wu, Junchi Yao, Ruilizhen Hu, Ning Ding, Yu Cheng, Tao Chen, Lei Bai, Dongzhan Zhou, Yun Luo, Ganqu Cui, Peng Ye</strong></p>
<p>Recently, the physical capabilities of (M)LLMs have garnered increasing attention. However, existing benchmarks for physics suffer from two major gaps: they neither provide systematic and up-to-date coverage of real-world physics competitions such as physics Olympiads, nor enable direct performance comparison with humans. To bridge these gaps, we present HiPhO, the first benchmark dedicated to high school physics Olympiads with human-aligned evaluation. Specifically, HiPhO highlights three key innovations. (1) Comprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025, spanning both international and regional competitions, and covering mixed modalities that encompass problems spanning text-only to diagram-based. (2) Professional Evaluation: We adopt official marking schemes to perform fine-grained grading at both the answer and step level, fully aligned with human examiners to ensure high-quality and domain-specific evaluation. (3) Comparison with Human Contestants: We assign gold, silver, and bronze medals to models based on official medal thresholds, thereby enabling direct comparison between (M)LLMs and human contestants. Our large-scale evaluation of 30 state-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly remain at or below the bronze level; open-source LLMs show promising progress with occasional golds; closed-source reasoning MLLMs can achieve 6 to 12 gold medals; and most models still have a significant gap from full marks. These results highlight a substantial performance gap between open-source models and top students, the strong physical reasoning capabilities of closed-source reasoning models, and the fact that there is still significant room for improvement. HiPhO, as a rigorous, human-aligned, and Olympiad-focused benchmark for advancing multimodal physical reasoning, is open-source and available at <a target="_blank" rel="noopener" href="https://github.com/SciYu/HiPhO">https://github.com/SciYu/HiPhO</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œ(M)LLMçš„ç‰©ç†èƒ½åŠ›è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç‰©ç†åŸºå‡†æµ‹è¯•å­˜åœ¨ä¸¤å¤§ç©ºç™½ï¼šå®ƒä»¬æ—¢æ²¡æœ‰æä¾›å¯¹ç°å®ä¸–ç•Œç‰©ç†ç«èµ›ï¼ˆå¦‚ç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›ï¼‰çš„ç³»ç»Ÿå’Œæœ€æ–°è¦†ç›–ï¼Œä¹Ÿæ— æ³•ä¸äººç±»è¿›è¡Œç›´æ¥æ€§èƒ½æ¯”è¾ƒã€‚ä¸ºäº†å¼¥è¡¥è¿™äº›ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†HiPhOï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨é¢å‘é«˜ä¸­ç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›çš„ä¸äººç±»è¯„ä¼°å¯¹é½çš„åŸºå‡†æµ‹è¯•ã€‚å…·ä½“æ¥è¯´ï¼ŒHiPhOçªå‡ºäº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ã€‚(1)ç»¼åˆæ•°æ®ï¼šå®ƒæ•´ç†äº†13åœºæœ€æ–°çš„å¥¥æ—åŒ¹å…‹è€ƒè¯•ï¼Œæ¶µç›–å›½é™…å’ŒåŒºåŸŸç«èµ›ï¼ŒåŒ…æ‹¬ä»çº¯æ–‡æœ¬åˆ°å›¾è¡¨çš„å„ç§é¢˜å‹ã€‚(2)ä¸“ä¸šè¯„ä¼°ï¼šæˆ‘ä»¬é‡‡ç”¨å®˜æ–¹è¯„åˆ†æ–¹æ¡ˆè¿›è¡Œç²¾ç»†çš„è¯„åˆ†ï¼ŒåŒ…æ‹¬ç­”æ¡ˆå’Œæ­¥éª¤çº§åˆ«ï¼Œä¸äººç±»è¯„ä¼°å‘˜å®Œå…¨å¯¹é½ï¼Œä»¥ç¡®ä¿é«˜è´¨é‡å’Œé¢†åŸŸç‰¹å®šçš„è¯„ä¼°ã€‚(3)ä¸äººç±»å‚èµ›è€…çš„æ¯”è¾ƒï¼šæˆ‘ä»¬æ ¹æ®å®˜æ–¹å¥–ç‰Œé˜ˆå€¼ä¸ºæ¨¡å‹åˆ†é…é‡‘ã€é“¶å’Œé“œç‰Œï¼Œä»è€Œèƒ½å¤Ÿç›´æ¥æ¯”è¾ƒ(M)LLMå’Œäººç±»å‚èµ›è€…ã€‚æˆ‘ä»¬å¯¹30ä¸ªæœ€å…ˆè¿›çš„(M)LLMçš„å¤§è§„æ¨¡è¯„ä¼°è¡¨æ˜ï¼šåœ¨13åœºè€ƒè¯•ä¸­ï¼Œå¼€æºMLLMå¤§å¤šåœç•™åœ¨é“œç‰Œæ°´å¹³æˆ–ä»¥ä¸‹ï¼›å¼€æºLLMå¶å°”èƒ½è·å¾—é‡‘ç‰Œï¼Œæ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„è¿›å±•ï¼›å°é—­æºä»£ç çš„æ¨ç†MLLMå¯ä»¥è·å¾—6è‡³12æšé‡‘ç‰Œï¼›ä½†å¤§å¤šæ•°æ¨¡å‹ä¸æ»¡åˆ†ä»æœ‰å¾ˆå¤§å·®è·ã€‚è¿™äº›ç»“æœçªæ˜¾äº†å¼€æºæ¨¡å‹ä¸é¡¶å°–å­¦ç”Ÿä¹‹é—´çš„æ˜¾è‘—æ€§èƒ½å·®è·ã€å°é—­æºä»£ç æ¨ç†æ¨¡å‹çš„å¼ºå¤§ç‰©ç†æ¨ç†èƒ½åŠ›ï¼Œä»¥åŠä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚HiPhOä½œä¸ºä¸€ä¸ªä¸¥è°¨ã€ä¸äººç±»è¯„ä¼°å¯¹é½ã€ä¸“æ³¨äºå¥¥æ—åŒ¹å…‹ç«èµ›çš„å¤šæ¨¡å¼ç‰©ç†æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œæ˜¯å¼€æºçš„ï¼Œå¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/SciYu/HiPhO">https://github.com/SciYu/HiPhO</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07894v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨äºç‰©ç†é¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„è¡¨ç°ï¼Œä»‹ç»äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•å¹³å°HiPhOã€‚è¯¥å¹³å°ä¸“é—¨ç”¨äºé«˜ä¸­ç”Ÿç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›çš„æ¨¡å‹è¯„ä¼°ï¼Œå®ç°äº†äººç±»ä¸æ¨¡å‹çš„æ€§èƒ½å¯¹æ¯”ã€‚è¯¥å¹³å°æ‹¥æœ‰å…¨é¢çš„æ•°æ®è¦†ç›–ï¼ŒåŒ…å«å›½é™…åŠåŒºåŸŸæ€§ç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›çš„è¯•é¢˜ï¼›é‡‡ç”¨å®˜æ–¹è¯„åˆ†æ–¹æ¡ˆè¿›è¡Œç²¾ç»†çš„è¯„åˆ†ï¼›é€šè¿‡ä¸äººç±»å‚èµ›è€…çš„æ¯”è¾ƒï¼Œå¯¹æ¨¡å‹è¿›è¡Œé‡‘ç‰Œã€é“¶ç‰Œå’Œé“œç‰Œçš„è¯„å®šã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ä»æœ‰å¾…æå‡ï¼Œè€Œå°é—­æºæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„ç‰©ç†æ¨ç†èƒ½åŠ›ã€‚æ•´ä½“è€Œè¨€ï¼ŒHiPhOå¹³å°ä¸ºæ¨åŠ¨å¤šæ¨¡æ€ç‰©ç†æ¨ç†çš„è¿›æ­¥æä¾›äº†é‡è¦çš„å·¥å…·å’Œèµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>HiPhOæ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹é«˜ä¸­ç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚</li>
<li>å¹³å°å…·å¤‡å…¨é¢çš„æ•°æ®è¦†ç›–ï¼ŒåŒ…æ‹¬å›½é™…åŠåŒºåŸŸæ€§ç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›è¯•é¢˜ã€‚</li>
<li>é‡‡ç”¨å®˜æ–¹è¯„åˆ†æ–¹æ¡ˆè¿›è¡Œç²¾ç»†çš„è¯„åˆ†ï¼Œä¸äººç±»è¯„ä»·è€…ä¿æŒä¸€è‡´ã€‚</li>
<li>å¹³å°èƒ½å¤Ÿå®ç°æ¨¡å‹ä¸äººç±»çš„æ€§èƒ½å¯¹æ¯”ï¼Œæ ¹æ®å®˜æ–¹é˜ˆå€¼è¯„å®šæ¨¡å‹è·å¾—é‡‘ç‰Œã€é“¶ç‰Œå’Œé“œç‰Œã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ä»æœ‰å¾…æå‡ï¼Œè€Œå°é—­æºæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„ç‰©ç†æ¨ç†èƒ½åŠ›ã€‚</li>
<li>HiPhOå¹³å°ä¸ºæ¨åŠ¨å¤šæ¨¡æ€ç‰©ç†æ¨ç†çš„è¿›æ­¥æä¾›äº†é‡è¦çš„å·¥å…·å’Œèµ„æºã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07894">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07894v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07894v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07894v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07894v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07894v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07894v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SCoder-Iterative-Self-Distillation-for-Bootstrapping-Small-Scale-Data-Synthesizers-to-Empower-Code-LLMs"><a href="#SCoder-Iterative-Self-Distillation-for-Bootstrapping-Small-Scale-Data-Synthesizers-to-Empower-Code-LLMs" class="headerlink" title="SCoder: Iterative Self-Distillation for Bootstrapping Small-Scale Data   Synthesizers to Empower Code LLMs"></a>SCoder: Iterative Self-Distillation for Bootstrapping Small-Scale Data   Synthesizers to Empower Code LLMs</h2><p><strong>Authors:Xinyu Zhang, Changzhi Zhou, Linmei Hu, Luhao Zhang, Xiancai Chen, Haomin Fu, Yang Yang, Mengdi Zhang</strong></p>
<p>Existing code large language models (LLMs) often rely on large-scale instruction data distilled from proprietary LLMs for fine-tuning, which typically incurs high costs. In this paper, we explore the potential of small-scale open-source LLMs (e.g., 7B) as synthesizers for high-quality code instruction data construction. We first observe that the data synthesis capability of small-scale LLMs can be enhanced by training on a few superior data synthesis samples from proprietary LLMs. Building on this, we propose a novel iterative self-distillation approach to bootstrap small-scale LLMs, transforming them into powerful synthesizers that reduce reliance on proprietary LLMs and minimize costs. Concretely, in each iteration, to obtain diverse and high-quality self-distilled data, we design multi-checkpoint sampling and multi-aspect scoring strategies for initial data selection. Furthermore, to identify the most influential samples, we introduce a gradient-based influence estimation method for final data filtering. Based on the code instruction datasets from the small-scale synthesizers, we develop SCoder, a family of code generation models fine-tuned from DeepSeek-Coder. SCoder models achieve state-of-the-art code generation capabilities, demonstrating the effectiveness of our method. </p>
<blockquote>
<p>ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸ä¾èµ–äºä»ä¸“æœ‰LLMä¸­æç‚¼çš„å¤§è§„æ¨¡æŒ‡ä»¤æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´é«˜æ˜‚çš„æˆæœ¬ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å°å‹å¼€æºLLMï¼ˆä¾‹å¦‚7Bï¼‰ä½œä¸ºé«˜è´¨é‡ä»£ç æŒ‡ä»¤æ•°æ®æ„å»ºçš„åˆæˆå™¨çš„æ½œåŠ›ã€‚æˆ‘ä»¬é¦–å…ˆè§‚å¯Ÿåˆ°ï¼Œé€šè¿‡è®­ç»ƒæ¥è‡ªä¸“æœ‰LLMçš„å°‘æ•°ä¼˜è´¨æ•°æ®åˆæˆæ ·æœ¬ï¼Œå°å‹LLMçš„æ•°æ®åˆæˆèƒ½åŠ›å¯ä»¥å¾—åˆ°å¢å¼ºã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¿­ä»£è‡ªè’¸é¦æ–¹æ³•ï¼Œä»¥å¼•å¯¼å°å‹LLMï¼Œå°†å®ƒä»¬è½¬å˜ä¸ºå¼ºå¤§çš„åˆæˆå™¨ï¼Œå‡å°‘å¯¹ä¸“æœ‰LLMçš„ä¾èµ–ï¼Œå¹¶æœ€å°åŒ–æˆæœ¬ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œä¸ºäº†è·å¾—å¤šæ ·ä¸”é«˜è´¨é‡çš„è‡ªè’¸é¦æ•°æ®ï¼Œæˆ‘ä»¬ä¸ºåˆå§‹æ•°æ®é€‰æ‹©è®¾è®¡äº†å¤šæ£€æŸ¥ç‚¹é‡‡æ ·å’Œå¤šæ–¹é¢è¯„åˆ†ç­–ç•¥ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¯†åˆ«æœ€æœ‰å½±å“åŠ›çš„æ ·æœ¬ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæ¢¯åº¦çš„å½±å“åŠ›ä¼°è®¡æ–¹æ³•è¿›è¡Œæœ€ç»ˆæ•°æ®è¿‡æ»¤ã€‚åŸºäºå°å‹åˆæˆå™¨ç”Ÿæˆçš„ä»£ç æŒ‡ä»¤æ•°æ®é›†ï¼Œæˆ‘ä»¬å¼€å‘äº†SCoderç³»åˆ—ä»£ç ç”Ÿæˆæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹ä»¥DeepSeek-Coderä¸ºåŸºç¡€è¿›è¡Œå¾®è°ƒã€‚SCoderæ¨¡å‹è¾¾åˆ°äº†å…ˆè¿›çš„ä»£ç ç”Ÿæˆèƒ½åŠ›ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07858v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å°è§„æ¨¡å¼€æºLLMåœ¨ä»£ç æŒ‡ä»¤æ•°æ®åˆæˆä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œé€šè¿‡è®­ç»ƒæå‡åˆæˆèƒ½åŠ›å¹¶å¼•å…¥è¿­ä»£è‡ªè’¸é¦æ–¹æ³•ï¼Œå¯å‡å°‘å¯¹ä¸“æœ‰LLMçš„ä¾èµ–å¹¶é™ä½æˆæœ¬ã€‚è®¾è®¡å¤šæ£€æŸ¥ç‚¹é‡‡æ ·å’Œå¤šæ–¹é¢è¯„åˆ†ç­–ç•¥ï¼Œç»“åˆæ¢¯åº¦å½±å“ä¼°è®¡æ–¹æ³•ï¼Œå®ç°é«˜è´¨é‡è‡ªè’¸é¦æ•°æ®ã€‚åŸºäºæ­¤æ•°æ®å¼€å‘çš„SCoderæ¨¡å‹ï¼Œå±•ç°å‡ºå“è¶Šçš„ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°è§„æ¨¡å¼€æºLLMå¯ä½œä¸ºé«˜è´¨é‡ä»£ç æŒ‡ä»¤æ•°æ®åˆæˆçš„åˆæˆå™¨ã€‚</li>
<li>é€šè¿‡è®­ç»ƒæå‡å°è§„æ¨¡LLMçš„æ•°æ®åˆæˆèƒ½åŠ›ã€‚</li>
<li>å¼•å…¥è¿­ä»£è‡ªè’¸é¦æ–¹æ³•ï¼Œä½¿å°è§„æ¨¡LLMå˜å¾—æ›´å¼ºå¤§ï¼Œå‡å°‘äº†å¯¹ä¸“æœ‰LLMçš„ä¾èµ–å’Œæˆæœ¬ã€‚</li>
<li>è®¾è®¡å¤šæ£€æŸ¥ç‚¹é‡‡æ ·å’Œå¤šæ–¹é¢è¯„åˆ†ç­–ç•¥ï¼Œç”¨äºåˆå§‹æ•°æ®é€‰æ‹©ï¼Œå®ç°å¤šæ ·æ€§å’Œé«˜è´¨é‡çš„è‡ªè’¸é¦æ•°æ®ã€‚</li>
<li>å¼•å…¥æ¢¯åº¦å½±å“ä¼°è®¡æ–¹æ³•ï¼Œç”¨äºæœ€ç»ˆæ•°æ®è¿‡æ»¤ï¼Œè¯†åˆ«æœ€å…·å½±å“åŠ›çš„æ ·æœ¬ã€‚</li>
<li>åŸºäºå°è§„æ¨¡åˆæˆå™¨å¼€å‘çš„SCoderæ¨¡å‹å±•ç°å‡ºå“è¶Šçš„ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07858">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07858v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07858v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07858v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07858v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Aligning-LLMs-for-the-Classroom-with-Knowledge-Based-Retrieval-â€“-A-Comparative-RAG-Study"><a href="#Aligning-LLMs-for-the-Classroom-with-Knowledge-Based-Retrieval-â€“-A-Comparative-RAG-Study" class="headerlink" title="Aligning LLMs for the Classroom with Knowledge-Based Retrieval â€“ A   Comparative RAG Study"></a>Aligning LLMs for the Classroom with Knowledge-Based Retrieval â€“ A   Comparative RAG Study</h2><p><strong>Authors:Amay Jain, Liu Cui, Si Chen</strong></p>
<p>Large language models like ChatGPT are increasingly used in classrooms, but they often provide outdated or fabricated information that can mislead students. Retrieval Augmented Generation (RAG) improves reliability of LLMs by grounding responses in external resources. We investigate two accessible RAG paradigms, vector-based retrieval and graph-based retrieval to identify best practices for classroom question answering (QA). Existing comparative studies fail to account for pedagogical factors such as educational disciplines, question types, and practical deployment costs. Using a novel dataset, EduScopeQA, of 3,176 questions across academic subjects, we measure performance on various educational query types, from specific facts to broad thematic discussions. We also evaluate system alignment with a dataset of systematically altered textbooks that contradict the LLMâ€™s latent knowledge. We find that OpenAI Vector Search RAG (representing vector-based RAG) performs well as a low-cost generalist, especially for quick fact retrieval. On the other hand, GraphRAG Global excels at providing pedagogically rich answers to thematic queries, and GraphRAG Local achieves the highest accuracy with the dense, altered textbooks when corpus integrity is critical. Accounting for the 10-20x higher resource usage of GraphRAG (representing graph-based RAG), we show that a dynamic branching framework that routes queries to the optimal retrieval method boosts fidelity and efficiency. These insights provide actionable guidelines for educators and system designers to integrate RAG-augmented LLMs into learning environments effectively. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å¦‚ChatGPTåœ¨æ•™å®¤ä¸­çš„ä½¿ç”¨è¶Šæ¥è¶Šæ™®éï¼Œä½†å®ƒä»¬ç»å¸¸æä¾›è¿‡æ—¶æˆ–è™šå‡çš„ä¿¡æ¯ï¼Œå¯èƒ½ä¼šè¯¯å¯¼å­¦ç”Ÿã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡ä»¥å¤–éƒ¨èµ„æºä¸ºåŸºç¡€æ¥æ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯é æ€§ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸¤ç§å¯è®¿é—®çš„RAGèŒƒå¼ï¼ŒåŸºäºå‘é‡çš„æ£€ç´¢å’ŒåŸºäºå›¾çš„æ£€ç´¢ï¼Œä»¥ç¡®å®šè¯¾å ‚é—®ç­”ï¼ˆQAï¼‰çš„æœ€ä½³å®è·µã€‚ç°æœ‰çš„æ¯”è¾ƒç ”ç©¶æœªèƒ½è€ƒè™‘åˆ°æ•™è‚²å› ç´ ï¼Œå¦‚æ•™è‚²å­¦ç§‘ã€é—®é¢˜ç±»å‹å’Œå®é™…éƒ¨ç½²æˆæœ¬ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªæ–°çš„æ•°æ®é›†EduScopeQAï¼ŒåŒ…å«3176ä¸ªè·¨å­¦ç§‘çš„å­¦æœ¯é—®é¢˜ï¼Œè¡¡é‡åœ¨å„ç§æ•™è‚²æŸ¥è¯¢ç±»å‹ä¸Šçš„è¡¨ç°ï¼Œä»å…·ä½“äº‹å®åˆ°å¹¿æ³›çš„ä¸»é¢˜è®¨è®ºã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨ç³»ç»Ÿæ›´æ”¹çš„æ•™ç§‘ä¹¦æ•°æ®é›†è¯„ä¼°äº†ç³»ç»Ÿä¸å¤§å‹è¯­è¨€æ¨¡å‹æ½œåœ¨çŸ¥è¯†çš„å¯¹é½ç¨‹åº¦ï¼Œè¿™äº›æ•™ç§‘ä¹¦æ•°æ®é›†ä¸­çš„å†…å®¹ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†ç›¸çŸ›ç›¾ã€‚æˆ‘ä»¬å‘ç°ï¼Œä½œä¸ºä½æˆæœ¬é€šæ‰ï¼ŒOpenAI Vector Search RAGåœ¨å¿«é€Ÿäº‹å®æ£€ç´¢æ–¹é¢è¡¨ç°è‰¯å¥½ã€‚å¦ä¸€æ–¹é¢ï¼ŒGraphRAG Globalåœ¨æä¾›ä¸»é¢˜æŸ¥è¯¢çš„ä¸°å¯Œç­”æ¡ˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€ŒGraphRAG Localåœ¨å¯†é›†ã€æ›´æ”¹çš„æ•™ç§‘ä¹¦ä¸­å®ç°äº†æœ€é«˜ç²¾åº¦ï¼Œå½“è¯­æ–™åº“å®Œæ•´æ€§è‡³å…³é‡è¦æ—¶å°¤å…¶å¦‚æ­¤ã€‚è€ƒè™‘åˆ°GraphRAGï¼ˆä»£è¡¨åŸºäºå›¾çš„RAGï¼‰çš„èµ„æºä½¿ç”¨é‡æ˜¯å‰è€…çš„10-20å€ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åŠ¨æ€åˆ†æ”¯æ¡†æ¶é€šè¿‡å°†æŸ¥è¯¢è·¯ç”±åˆ°æœ€ä½³æ£€ç´¢æ–¹æ³•æ¥æé«˜ä¿çœŸåº¦å’Œæ•ˆç‡ã€‚è¿™äº›è§è§£ä¸ºæ•™è‚²è€…å’Œç³»ç»Ÿè®¾è®¡äººå‘˜æä¾›äº†å°†RAGå¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹æœ‰æ•ˆé›†æˆåˆ°å­¦ä¹ ç¯å¢ƒä¸­çš„å®ç”¨æŒ‡å—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07846v1">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å¦‚ChatGPTåœ¨æ•™å®¤ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†å®ƒä»¬æä¾›çš„ä¿¡æ¯å¾€å¾€è¿‡æ—¶æˆ–è™šå‡ï¼Œå®¹æ˜“è¯¯å¯¼å­¦ç”Ÿã€‚ä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯é æ€§ï¼Œç ”ç©¶è€…æå‡ºäº†åŸºäºæ£€ç´¢çš„å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆå¤–éƒ¨èµ„æºæ¥ä¼˜åŒ–å›åº”ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä¸¤ç§æ˜“äºå®ç°çš„RAGæ–¹æ³•â€”â€”åŸºäºå‘é‡çš„æ£€ç´¢å’ŒåŸºäºå›¾çš„æ£€ç´¢ï¼Œä»¥å¯»æ‰¾æœ€é€‚åˆè¯¾å ‚é—®ç­”çš„æœ€ä½³å®è·µã€‚é€šè¿‡è€ƒè™‘æ•™å­¦å› ç´ ï¼Œå¦‚æ•™è‚²å­¦ç§‘ã€é—®é¢˜ç±»å‹å’Œå®é™…éƒ¨ç½²æˆæœ¬ï¼Œå¯¹ç°æœ‰ç ”ç©¶è¿›è¡Œäº†è¡¥å……ã€‚ä½¿ç”¨å…¨æ–°çš„æ•°æ®é›†EduScopeQAï¼Œæ¶µç›–äº†ä¸åŒå­¦ç§‘çš„3176ä¸ªé—®é¢˜ï¼Œå¯¹å„ç±»æ•™è‚²æŸ¥è¯¢æ€§èƒ½è¿›è¡Œäº†æµ‹é‡ï¼ŒåŒ…æ‹¬å…·ä½“äº‹å®å’Œå¹¿æ³›çš„ä¸»é¢˜è®¨è®ºã€‚ç³»ç»Ÿä¹Ÿä¸è¢«æ”¹å˜çš„ç³»ç»Ÿæ€§æ•™ç§‘ä¹¦æ•°æ®é›†å¯¹é½ï¼Œä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ½œåœ¨çŸ¥è¯†çŸ›ç›¾ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒOpenAIå‘é‡æœç´¢RAGåœ¨ä½æˆæœ¬é€šç”¨é¢†åŸŸè¡¨ç°è‰¯å¥½ï¼Œå°¤å…¶æ“…é•¿å¿«é€Ÿäº‹å®æ£€ç´¢ï¼›è€ŒGraphRAG Globalåœ¨æä¾›ä¸°å¯Œæ•™è‚²æ€§ç­”æ¡ˆæ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶æ˜¯åœ¨å›åº”ä¸»é¢˜æŸ¥è¯¢æ—¶ï¼›GraphRAG Localåœ¨æ•™ç§‘ä¹¦å¯†é›†åº¦é«˜çš„æƒ…å¢ƒä¸‹å‡†ç¡®æ€§æœ€é«˜ï¼Œå°¤å…¶åœ¨è¯­æ–™åº“å®Œæ•´æ€§è‡³å…³é‡è¦æ—¶ã€‚è€ƒè™‘GraphRAGçš„èµ„æºä½¿ç”¨é‡æ˜¯å‰è€…çš„10-20å€ï¼Œç ”ç©¶è¿˜æ˜¾ç¤ºäº†ä¸€ä¸ªåŠ¨æ€åˆ†æ”¯æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä¼˜åŒ–æŸ¥è¯¢çš„è·¯ç”±ä»¥æé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚è¿™äº›è§è§£ä¸ºæ•™è‚²å·¥ä½œè€…å’Œç³»ç»Ÿè®¾è®¡å¸ˆæœ‰æ•ˆæ•´åˆRAGå¢å¼ºLLMåˆ°å­¦ä¹ ç¯å¢ƒä¸­æä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•™å®¤åº”ç”¨ä¸­å­˜åœ¨æä¾›è¯¯å¯¼æ€§ä¿¡æ¯çš„é—®é¢˜ã€‚</li>
<li>åŸºäºæ£€ç´¢çš„å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•èƒ½æé«˜LLMçš„å¯é æ€§ï¼Œé€šè¿‡ç»“åˆå¤–éƒ¨èµ„æºä¼˜åŒ–å›åº”ã€‚</li>
<li>ç ”ç©¶æ¯”è¾ƒäº†ä¸¤ç§RAGæ–¹æ³•ï¼šåŸºäºå‘é‡çš„æ£€ç´¢å’ŒåŸºäºå›¾çš„æ£€ç´¢ï¼Œåœ¨æ•™å®¤é—®ç­”ä¸­çš„åº”ç”¨æ•ˆæœã€‚</li>
<li>æ•™è‚²å­¦ç§‘ã€é—®é¢˜ç±»å‹å’Œéƒ¨ç½²æˆæœ¬ç­‰æ•™å­¦å› ç´ è¢«çº³å…¥è€ƒè™‘ï¼Œè¡¥å……äº†ç°æœ‰ç ”ç©¶ã€‚</li>
<li>OpenAIå‘é‡æœç´¢RAGåœ¨å¿«é€Ÿäº‹å®æ£€ç´¢æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œé€‚åˆä½æˆæœ¬é€šç”¨é¢†åŸŸã€‚</li>
<li>GraphRAG Globalæ“…é•¿æä¾›ä¸°å¯Œæ•™è‚²æ€§ç­”æ¡ˆï¼Œå°¤å…¶åœ¨å›åº”ä¸»é¢˜æŸ¥è¯¢æ—¶è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07846">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07846v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07846v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07846v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07846v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07846v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07846v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Point-Linguist-Model-Segment-Any-Object-via-Bridged-Large-3D-Language-Model"><a href="#Point-Linguist-Model-Segment-Any-Object-via-Bridged-Large-3D-Language-Model" class="headerlink" title="Point Linguist Model: Segment Any Object via Bridged Large 3D-Language   Model"></a>Point Linguist Model: Segment Any Object via Bridged Large 3D-Language   Model</h2><p><strong>Authors:Zhuoxu Huang, Mingqi Gao, Jungong Han</strong></p>
<p>3D object segmentation with Large Language Models (LLMs) has become a prevailing paradigm due to its broad semantics, task flexibility, and strong generalization. However, this paradigm is hindered by representation misalignment: LLMs process high-level semantic tokens, whereas 3D point clouds convey only dense geometric structures. In prior methods, misalignment limits both input and output. At the input stage, dense point patches require heavy pre-alignment, weakening object-level semantics and confusing similar distractors. At the output stage, predictions depend only on dense features without explicit geometric cues, leading to a loss of fine-grained accuracy. To address these limitations, we present the Point Linguist Model (PLM), a general framework that bridges the representation gap between LLMs and dense 3D point clouds without requiring large-scale pre-alignment between 3D-text or 3D-images. Specifically, we introduce Object-centric Discriminative Representation (OcDR), which learns object-centric tokens that capture target semantics and scene relations under a hard negative-aware training objective. This mitigates the misalignment between LLM tokens and 3D points, enhances resilience to distractors, and facilitates semantic-level reasoning within LLMs. For accurate segmentation, we introduce the Geometric Reactivation Decoder (GRD), which predicts masks by combining OcDR tokens carrying LLM-inferred geometry with corresponding dense features, preserving comprehensive dense features throughout the pipeline. Extensive experiments show that PLM achieves significant improvements of +7.3 mIoU on ScanNetv2 and +6.0 mIoU on Multi3DRefer for 3D referring segmentation, with consistent gains across 7 benchmarks spanning 4 different tasks, demonstrating the effectiveness of comprehensive object-centric reasoning for robust 3D understanding. </p>
<blockquote>
<p>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œ3Då¯¹è±¡åˆ†å‰²å·²ç»æˆä¸ºä¸€ç§æµè¡Œçš„èŒƒå¼ï¼Œå› ä¸ºå…¶å…·æœ‰å¹¿æ³›çš„è¯­ä¹‰ã€ä»»åŠ¡çµæ´»æ€§å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ç§èŒƒå¼å—åˆ°è¡¨ç¤ºä¸å¯¹é½çš„é˜»ç¢ï¼šLLMå¤„ç†é«˜çº§è¯­ä¹‰ä»¤ç‰Œï¼Œè€Œ3Dç‚¹äº‘ä»…ä¼ è¾¾å¯†é›†çš„å‡ ä½•ç»“æ„ã€‚åœ¨å…ˆå‰çš„æ–¹æ³•ä¸­ï¼Œä¸å¯¹é½é™åˆ¶äº†è¾“å…¥å’Œè¾“å‡ºã€‚åœ¨è¾“å…¥é˜¶æ®µï¼Œå¯†é›†çš„ç‚¹è¡¥ä¸éœ€è¦å¤§é‡çš„é¢„å¯¹é½æ“ä½œï¼Œè¿™å‰Šå¼±äº†å¯¹è±¡çº§åˆ«çš„è¯­ä¹‰å¹¶æ··æ·†äº†ç±»ä¼¼çš„å¹²æ‰°ç‰©ã€‚åœ¨è¾“å‡ºé˜¶æ®µï¼Œé¢„æµ‹ä»…ä¾èµ–äºå¯†é›†ç‰¹å¾è€Œæ²¡æœ‰æ˜ç¡®çš„å‡ ä½•çº¿ç´¢ï¼Œå¯¼è‡´ç²¾ç»†ç²’åº¦ç²¾åº¦çš„æŸå¤±ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Point Linguist Modelï¼ˆPLMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå¯ä»¥åœ¨ä¸éœ€è¦å¯¹3Dæ–‡æœ¬æˆ–3Då›¾åƒè¿›è¡Œå¤§è§„æ¨¡é¢„å¯¹é½çš„æƒ…å†µä¸‹ï¼Œå¼¥åˆLLMå’Œå¯†é›†3Dç‚¹äº‘ä¹‹é—´çš„è¡¨ç¤ºå·®è·ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†Object-centric Discriminative Representationï¼ˆOcDRï¼‰ï¼Œå®ƒå­¦ä¹ å¯¹è±¡ä¸ºä¸­å¿ƒçš„ä»¤ç‰Œï¼Œè¿™äº›ä»¤ç‰Œåœ¨ç¡¬è´Ÿæ„ŸçŸ¥è®­ç»ƒç›®æ ‡ä¸‹æ•è·ç›®æ ‡è¯­ä¹‰å’Œåœºæ™¯å…³ç³»ã€‚è¿™å‡è½»äº†LLMä»¤ç‰Œå’Œ3Dç‚¹ä¹‹é—´çš„ä¸å¯¹é½é—®é¢˜ï¼Œå¢å¼ºäº†å¯¹æŠ—å¹²æ‰°ç‰©çš„éŸ§æ€§ï¼Œå¹¶ä¿ƒè¿›äº†LLMå†…çš„è¯­ä¹‰çº§æ¨ç†ã€‚ä¸ºäº†å‡†ç¡®åˆ†å‰²ï¼Œæˆ‘ä»¬å¼•å…¥äº†Geometric Reactivation Decoderï¼ˆGRDï¼‰ï¼Œå®ƒé€šè¿‡ç»“åˆOcDRä»¤ç‰Œï¼ˆæºå¸¦LLMæ¨æ–­çš„å‡ ä½•ï¼‰å’Œç›¸åº”çš„å¯†é›†ç‰¹å¾æ¥é¢„æµ‹æ©ç ï¼Œä»è€Œåœ¨æ•´ä¸ªç®¡é“ä¸­ä¿ç•™å…¨é¢çš„å¯†é›†ç‰¹å¾ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPLMåœ¨ScanNetv2ä¸Šå®ç°äº†+7.3 mIoUçš„æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨Multi3DReferçš„3Då¼•ç”¨åˆ†å‰²ä¸Šå®ç°äº†+6.0 mIoUçš„æ”¹è¿›ï¼ŒåŒæ—¶åœ¨è·¨è¶Š4ä¸ªä¸åŒä»»åŠ¡çš„7ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æŒç»­çš„æ”¶ç›Šï¼Œè¿™è¯æ˜äº†å…¨é¢çš„ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„æ¨ç†å¯¹äºç¨³å¥çš„3Dç†è§£çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07825v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„3Då¯¹è±¡åˆ†å‰²å·²æˆä¸ºä¸»æµèŒƒå¼ï¼Œä½†å­˜åœ¨è¡¨ç¤ºå¯¹é½é—®é¢˜ã€‚æœ¬æ–‡æå‡ºPoint Linguist Modelï¼ˆPLMï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥Object-centric Discriminative Representationï¼ˆOcDRï¼‰å’ŒGeometric Reactivation Decoderï¼ˆGRDï¼‰ï¼Œç¼©å°äº†LLMå’Œå¯†é›†ä¸‰ç»´ç‚¹äº‘ä¹‹é—´çš„è¡¨ç¤ºå·®è·ï¼Œæé«˜äº†ä¸‰ç»´ç†è§£çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨3Då¯¹è±¡åˆ†å‰²ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œä½†å­˜åœ¨è¡¨ç¤ºå¯¹é½é—®é¢˜ã€‚</li>
<li>PLMæ¡†æ¶é€šè¿‡ç¼©å°LLMå’Œå¯†é›†ä¸‰ç»´ç‚¹äº‘ä¹‹é—´çš„è¡¨ç¤ºå·®è·æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>PLMå¼•å…¥Object-centric Discriminative Representationï¼ˆOcDRï¼‰ï¼Œå­¦ä¹ ç›®æ ‡è¯­ä¹‰å’Œåœºæ™¯å…³ç³»çš„å¯¹è±¡ä¸­å¿ƒä»¤ç‰Œã€‚</li>
<li>Geometric Reactivation Decoderï¼ˆGRDï¼‰ç»“åˆLLMæ¨æ–­çš„å‡ ä½•ä½“ä¸ç›¸åº”å¯†é›†ç‰¹å¾ï¼Œè¿›è¡Œç²¾ç¡®åˆ†å‰²ã€‚</li>
<li>PLMåœ¨ScanNetv2å’ŒMulti3DReferç­‰æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„åˆ†å‰²æ€§èƒ½æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07825">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07825v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07825v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07825v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07825v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Dual-Knowledge-Enhanced-Two-Stage-Reasoner-for-Multimodal-Dialog-Systems"><a href="#Dual-Knowledge-Enhanced-Two-Stage-Reasoner-for-Multimodal-Dialog-Systems" class="headerlink" title="Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems"></a>Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems</h2><p><strong>Authors:Xiaolin Chen, Xuemeng Song, Haokun Wen, Weili Guan, Xiangyu Zhao, Liqiang Nie</strong></p>
<p>Textual response generation is pivotal for multimodal \mbox{task-oriented} dialog systems, which aims to generate proper textual responses based on the multimodal context. While existing efforts have demonstrated remarkable progress, there still exist the following limitations: 1) \textit{neglect of unstructured review knowledge} and 2) \textit{underutilization of large language models (LLMs)}. Inspired by this, we aim to fully utilize dual knowledge (\textit{i.e., } structured attribute and unstructured review knowledge) with LLMs to promote textual response generation in multimodal task-oriented dialog systems. However, this task is non-trivial due to two key challenges: 1) \textit{dynamic knowledge type selection} and 2) \textit{intention-response decoupling}. To address these challenges, we propose a novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for multimodal dialog systems (named DK2R). To be specific, DK2R first extracts both structured attribute and unstructured review knowledge from external knowledge base given the dialog context. Thereafter, DK2R uses an LLM to evaluate each knowledge typeâ€™s utility by analyzing LLM-generated provisional probe responses. Moreover, DK2R separately summarizes the intention-oriented key clues via dedicated reasoning, which are further used as auxiliary signals to enhance LLM-based textual response generation. Extensive experiments conducted on a public dataset verify the superiority of DK2R. We have released the codes and parameters. </p>
<blockquote>
<p>æ–‡æœ¬å“åº”ç”Ÿæˆå¯¹äºå¤šæ¨¡å¼ä»»åŠ¡å¯¼å‘å‹å¯¹è¯ç³»ç»Ÿè‡³å…³é‡è¦ï¼Œè¯¥ç³»ç»Ÿæ—¨åœ¨åŸºäºå¤šæ¨¡å¼ä¸Šä¸‹æ–‡ç”Ÿæˆé€‚å½“çš„æ–‡æœ¬å“åº”ã€‚å°½ç®¡ç°æœ‰çš„åŠªåŠ›å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†ä»å­˜åœ¨ä»¥ä¸‹å±€é™æ€§ï¼š1ï¼‰å¿½è§†éç»“æ„åŒ–è¯„è®ºçŸ¥è¯†ï¼›2ï¼‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆ©ç”¨ä¸è¶³ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å……åˆ†åˆ©ç”¨åŒçŸ¥è¯†ï¼ˆå³ç»“æ„åŒ–å±æ€§çŸ¥è¯†å’Œéç»“æ„åŒ–è¯„è®ºçŸ¥è¯†ï¼‰ä¸LLMï¼Œä»¥ä¿ƒè¿›å¤šæ¨¡å¼ä»»åŠ¡å¯¼å‘å‹å¯¹è¯ç³»ç»Ÿä¸­çš„æ–‡æœ¬å“åº”ç”Ÿæˆã€‚ç„¶è€Œï¼Œç”±äºä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œè¿™é¡¹ä»»åŠ¡å¹¶ä¸ç®€å•ï¼š1ï¼‰åŠ¨æ€çŸ¥è¯†ç±»å‹é€‰æ‹©ï¼›2ï¼‰æ„å›¾ä¸å“åº”çš„è§£è€¦ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŒçŸ¥è¯†å¢å¼ºçš„ä¸¤é˜¶æ®µæ¨ç†å™¨ï¼Œé€šè¿‡é€‚åº”LLMç”¨äºå¤šæ¨¡å¼å¯¹è¯ç³»ç»Ÿï¼ˆåä¸ºDK2Rï¼‰ã€‚å…·ä½“æ¥è¯´ï¼ŒDK2Ré¦–å…ˆä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æå–ç»™å®šçš„å¯¹è¯ä¸Šä¸‹æ–‡ä¸­çš„ç»“æ„åŒ–å±æ€§çŸ¥è¯†å’Œéç»“æ„åŒ–è¯„è®ºçŸ¥è¯†ã€‚æ­¤åï¼ŒDK2Rä½¿ç”¨LLMæ¥è¯„ä¼°æ¯ç§çŸ¥è¯†çš„å®ç”¨æ€§ï¼Œé€šè¿‡åˆ†æLLMç”Ÿæˆçš„ä¸´æ—¶æ¢æµ‹å“åº”ã€‚æ­¤å¤–ï¼ŒDK2Ré€šè¿‡ä¸“é¡¹æ¨ç†å•ç‹¬æ€»ç»“é¢å‘æ„å›¾çš„å…³é”®çº¿ç´¢ï¼Œè¿™äº›å…³é”®çº¿ç´¢è¿›ä¸€æ­¥ä½œä¸ºè¾…åŠ©ä¿¡å·ï¼Œå¢å¼ºåŸºäºLLMçš„æ–‡æœ¬å“åº”ç”Ÿæˆã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒéªŒè¯äº†DK2Rçš„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬å·²ç»å‘å¸ƒäº†ä»£ç å’Œå‚æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07817v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨å¤šæ¨¡æ€ä»»åŠ¡å¯¼å‘å¯¹è¯ç³»ç»Ÿä¸­çš„æ–‡æœ¬å“åº”ç”Ÿæˆï¼Œæ—¨åœ¨ç»“åˆç»“æ„åŒ–å±æ€§ä¸æ— ç»“æ„åŒ–è¯„è®ºçŸ¥è¯†ï¼Œå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¿ƒè¿›æ–‡æœ¬å“åº”çš„ç”Ÿæˆã€‚æ–‡ç« æŒ‡å‡ºäº†ç°æœ‰å·¥ä½œçš„ä¸è¶³ä¸æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæ­¤æå‡ºäº†ä¸€ç§æ–°çš„ç»“åˆåŒé‡çŸ¥è¯†çš„ä¸¤é˜¶æ®µæ¨ç†å™¨DK2Rã€‚DK2Rèƒ½ä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æå–çŸ¥è¯†ï¼Œè¯„ä¼°æ¯ç§çŸ¥è¯†çš„æ•ˆç”¨ï¼Œå¹¶æ€»ç»“å…³é”®çº¿ç´¢ä»¥å¢å¼ºåŸºäºLLMçš„æ–‡æœ¬å“åº”ç”Ÿæˆã€‚å®éªŒè¯æ˜DK2Råœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€ä»»åŠ¡å¯¼å‘å¯¹è¯ç³»ç»Ÿä¸­çš„æ–‡æœ¬å“åº”ç”Ÿæˆæ˜¯å…³é”®ç¯èŠ‚ï¼Œéœ€ç»“åˆç»“æ„åŒ–å±æ€§ä¸æ— ç»“æ„åŒ–è¯„è®ºçŸ¥è¯†ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ­¤ä»»åŠ¡ä¸­çš„åº”ç”¨æ˜¯é‡è¦çš„åˆ›æ–°ç‚¹ã€‚</li>
<li>ä¸»è¦æŒ‘æˆ˜åœ¨äºåŠ¨æ€çŸ¥è¯†ç±»å‹é€‰æ‹©å’Œæ„å›¾-å“åº”è§£è€¦ã€‚</li>
<li>DK2Ré€šè¿‡æå–å’Œåº”ç”¨åŒé‡çŸ¥è¯†æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>DK2Rèƒ½ç»“åˆå¯¹è¯ä¸Šä¸‹æ–‡ä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æå–çŸ¥è¯†ã€‚</li>
<li>DK2Ré€šè¿‡LLMç”Ÿæˆçš„ä¸´æ—¶æ¢é’ˆå“åº”è¯„ä¼°æ¯ç§çŸ¥è¯†çš„æ•ˆç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07817v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07817v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07817v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CAViAR-Critic-Augmented-Video-Agentic-Reasoning"><a href="#CAViAR-Critic-Augmented-Video-Agentic-Reasoning" class="headerlink" title="CAViAR: Critic-Augmented Video Agentic Reasoning"></a>CAViAR: Critic-Augmented Video Agentic Reasoning</h2><p><strong>Authors:Sachit Menon, Ahmet Iscen, Arsha Nagrani, Tobias Weyand, Carl Vondrick, Cordelia Schmid</strong></p>
<p>Video understanding has seen significant progress in recent years, with modelsâ€™ performance on perception from short clips continuing to rise. Yet, multiple recent benchmarks, such as LVBench, Neptune, and ActivityNet-RTL, show performance wanes for tasks requiring complex reasoning on videos as queries grow more complex and videos grow longer. In this work, we ask: can existing perception capabilities be leveraged to successfully perform more complex video reasoning? In particular, we develop a large language model agent given access to video modules as subagents or tools. Rather than following a fixed procedure to solve queries as in previous work such as Visual Programming, ViperGPT, and MoReVQA, the agent uses the results of each call to a module to determine subsequent steps. Inspired by work in the textual reasoning domain, we introduce a critic to distinguish between instances of successful and unsuccessful sequences from the agent. We show that the combination of our agent and critic achieve strong performance on the previously-mentioned datasets. </p>
<blockquote>
<p>è§†é¢‘ç†è§£åœ¨è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œæ¨¡å‹åœ¨çŸ­ç‰‡æ®µæ„ŸçŸ¥æ–¹é¢çš„æ€§èƒ½æŒç»­æé«˜ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„å¤šä¸ªåŸºå‡†æµ‹è¯•ï¼Œå¦‚LVBenchã€Neptuneå’ŒActivityNet-RTLï¼Œæ˜¾ç¤ºéšç€æŸ¥è¯¢çš„å¤æ‚æ€§å’Œè§†é¢‘é•¿åº¦çš„å¢åŠ ï¼Œéœ€è¦å¤æ‚è§†é¢‘æ¨ç†çš„ä»»åŠ¡çš„æ€§èƒ½ä¼šä¸‹é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºçš„é—®é¢˜æ˜¯ï¼šèƒ½å¦åˆ©ç”¨ç°æœ‰çš„æ„ŸçŸ¥èƒ½åŠ›æˆåŠŸæ‰§è¡Œæ›´å¤æ‚çš„è§†é¢‘æ¨ç†ï¼Ÿç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ï¼Œå¯ä»¥è®¿é—®è§†é¢‘æ¨¡å—ä½œä¸ºå­ä»£ç†æˆ–å·¥å…·ã€‚ä¸ä¹‹å‰çš„å·¥ä½œï¼ˆå¦‚è§†è§‰ç¼–ç¨‹ã€ViperGPTå’ŒMoReVQAï¼‰ä¸åŒï¼Œè¯¥ä»£ç†å¹¶ä¸éµå¾ªå›ºå®šçš„ç¨‹åºæ¥è§£å†³æŸ¥è¯¢ï¼Œè€Œæ˜¯åˆ©ç”¨å¯¹æ¨¡å—çš„æ¯æ¬¡è°ƒç”¨ç»“æœæ¥ç¡®å®šåç»­æ­¥éª¤ã€‚å—æ–‡æœ¬æ¨ç†é¢†åŸŸå·¥ä½œçš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¯„è®ºå®¶æ¥åŒºåˆ†ä»£ç†æˆåŠŸå’Œå¤±è´¥åºåˆ—çš„å®ä¾‹ã€‚æˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„ä»£ç†å’Œè¯„è®ºå®¶çš„ç»„åˆåœ¨ä¸Šè¿°æ•°æ®é›†ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07680v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨è§†é¢‘ç†è§£é¢†åŸŸçš„å¤æ‚æ¨ç†ä»»åŠ¡ï¼ŒæŒ‡å‡ºéšç€è§†é¢‘æ—¶é•¿å’ŒæŸ¥è¯¢å¤æ‚åº¦çš„å¢åŠ ï¼Œç°æœ‰æ¨¡å‹çš„æ€§èƒ½ä¼šä¸‹é™ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ç°æœ‰çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¼€å‘äº†ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ï¼Œè¯¥ä»£ç†å¯è®¿é—®è§†é¢‘æ¨¡å—ä½œä¸ºå­ä»£ç†æˆ–å·¥å…·ã€‚ä¸ä¼ ç»Ÿçš„éµå¾ªå›ºå®šç¨‹åºè§£å†³æŸ¥è¯¢çš„æ–¹æ³•ä¸åŒï¼Œè¯¥ä»£ç†åˆ©ç”¨æ¨¡å—è°ƒç”¨çš„ç»“æœæ¥ç¡®å®šåç»­æ­¥éª¤ã€‚é€šè¿‡å¼•å…¥æ‰¹åˆ¤å®¶æ¥åŒºåˆ†ä»£ç†æˆåŠŸå’Œå¤±è´¥åºåˆ—çš„å®ä¾‹ï¼Œç ”ç©¶è¯æ˜ï¼Œè¯¥ä»£ç†ä¸æ‰¹åˆ¤å®¶çš„ç»“åˆåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘ç†è§£é¢†åŸŸè™½ç„¶è¿‘å¹´æ¥åœ¨çŸ­ç‰‡æ®µæ„ŸçŸ¥æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é¢å¯¹å¤æ‚æŸ¥è¯¢å’Œé•¿è§†é¢‘æ—¶ï¼Œç°æœ‰æ¨¡å‹çš„æ€§èƒ½ä¼šä¸‹é™ã€‚</li>
<li>ç ”ç©¶è€…åˆ©ç”¨ç°æœ‰çš„æ„ŸçŸ¥èƒ½åŠ›å¼€å‘äº†ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ï¼Œèƒ½å¤Ÿè®¿é—®è§†é¢‘æ¨¡å—ä½œä¸ºå­ä»£ç†æˆ–å·¥å…·ã€‚</li>
<li>ä¸éµå¾ªå›ºå®šç¨‹åºè§£å†³æŸ¥è¯¢çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œè¯¥ä»£ç†é‡‡ç”¨æ¨¡å—è°ƒç”¨çš„ç»“æœæ¥ç¡®å®šåç»­æ­¥éª¤ã€‚</li>
<li>ç ”ç©¶å¼•å…¥äº†æ‰¹åˆ¤å®¶æœºåˆ¶æ¥åŒºåˆ†ä»£ç†æˆåŠŸå’Œå¤±è´¥åºåˆ—çš„å®ä¾‹ã€‚</li>
<li>è¯¥ä»£ç†ä¸æ‰¹åˆ¤å®¶çš„ç»“åˆåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†å¼ºå¤§çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>è¿™ä¸€è¿›å±•å¯èƒ½æ¨åŠ¨è§†é¢‘ç†è§£é¢†åŸŸåœ¨å¤æ‚æ¨ç†ä»»åŠ¡æ–¹é¢çš„è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07680">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07680v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07680v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07680v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Competitive-Audio-Language-Models-with-Data-Efficient-Single-Stage-Training-on-Public-Data"><a href="#Competitive-Audio-Language-Models-with-Data-Efficient-Single-Stage-Training-on-Public-Data" class="headerlink" title="Competitive Audio-Language Models with Data-Efficient Single-Stage   Training on Public Data"></a>Competitive Audio-Language Models with Data-Efficient Single-Stage   Training on Public Data</h2><p><strong>Authors:Gokul Karthik Kumar, Rishabh Saraf, Ludovick Lepauloux, Abdul Muneer, Billel Mokeddem, Hakim Hacid</strong></p>
<p>Large language models (LLMs) have transformed NLP, yet their integration with audio remains underexplored â€“ despite audioâ€™s centrality to human communication. We introduce Falcon3-Audio, a family of Audio-Language Models (ALMs) built on instruction-tuned LLMs and Whisper encoders. Using a remarkably small amount of public audio data â€“ less than 30K hours (5K unique) â€“ Falcon3-Audio-7B matches the best reported performance among open-weight models on the MMAU benchmark, with a score of 64.14, matching R1-AQA, while distinguishing itself through superior data and parameter efficiency, single-stage training, and transparency. Notably, our smallest 1B model remains competitive with larger open models ranging from 2B to 13B parameters. Through extensive ablations, we find that common complexities â€“ such as curriculum learning, multiple audio encoders, and intricate cross-attention connectors â€“ are not required for strong performance, even compared to models trained on over 500K hours of data. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†çš„æ ¼å±€ï¼Œç„¶è€Œï¼Œå°½ç®¡éŸ³é¢‘åœ¨äººç±»äº¤æµä¸­å æ®æ ¸å¿ƒåœ°ä½ï¼Œä½†å®ƒä»¬ä¸éŸ³é¢‘çš„èåˆä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚æˆ‘ä»¬æ¨å‡ºäº†Falcon3-Audioï¼Œè¿™æ˜¯ä¸€ç³»åˆ—åŸºäºæŒ‡ä»¤è°ƒä¼˜çš„LLMå’ŒWhisperç¼–ç å™¨çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMï¼‰ã€‚ä½¿ç”¨å°‘é‡å…¬å¼€éŸ³é¢‘æ•°æ®â€”â€”å°‘äº3ä¸‡å°æ—¶ï¼ˆç‹¬ç‰¹çš„ä»…æœ‰5åƒå°æ—¶ï¼‰â€”â€”Falcon3-Audio-7Båœ¨MMAUåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†å…¬å¼€æƒé‡æ¨¡å‹ä¸­çš„æœ€ä½³æŠ¥å‘Šæ€§èƒ½ï¼Œå¾—åˆ†ä¸º64.14ï¼Œä¸R1-AQAæŒå¹³ï¼ŒåŒæ—¶é€šè¿‡å…¶å“è¶Šçš„æ•°æ®å’Œå‚æ•°æ•ˆç‡ã€å•é˜¶æ®µè®­ç»ƒå’Œé€æ˜åº¦æ¥åŒºåˆ†è‡ªå·±ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬æœ€å°çš„1Bæ¨¡å‹ä»ç„¶ä¸ä»2Båˆ°13Bå‚æ•°çš„æ›´å¤§å¼€æ”¾æ¨¡å‹ä¿æŒç«äº‰åŠ›ã€‚é€šè¿‡å¹¿æ³›çš„æ¶ˆèå®éªŒï¼Œæˆ‘ä»¬å‘ç°å³ä½¿æ˜¯ä¸åœ¨è¶…è¿‡50ä¸‡å°æ—¶çš„æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œå¸¸è§çš„å¤æ‚æ€§â€”â€”å¦‚è¯¾ç¨‹å­¦ä¹ ã€å¤šä¸ªéŸ³é¢‘ç¼–ç å™¨å’Œå¤æ‚çš„äº¤å‰æ³¨æ„åŠ›è¿æ¥å™¨â€”â€”å¯¹äºå¼ºå¤§æ€§èƒ½å¹¶ä¸æ˜¯å¿…éœ€çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07526v1">PDF</a> Accepted at ASRU 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ä¸éŸ³é¢‘çš„èåˆæ–¹é¢ä»ç¼ºä¹æ·±å…¥ç ”ç©¶ï¼Œå°½ç®¡éŸ³é¢‘åœ¨äººç±»æ²Ÿé€šä¸­å æ®æ ¸å¿ƒåœ°ä½ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†Falcon3-Audioç³»åˆ—ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—åŸºäºæŒ‡ä»¤è°ƒä¼˜çš„LLMå’ŒWhisperç¼–ç å™¨çš„éŸ³è§†è¯­è¨€æ¨¡å‹ï¼ˆALMï¼‰ã€‚ä½¿ç”¨ä¸åˆ°3ä¸‡å°æ—¶ï¼ˆ5åƒç‹¬ç‰¹æ•°æ®ç‚¹ï¼‰çš„å…¬å¼€éŸ³é¢‘æ•°æ®ï¼ŒFalcon3-Audio-7Båœ¨MMAUåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¸å…¬å¼€æƒé‡æ¨¡å‹çš„æœ€ä½³è¡¨ç°ç›¸åŒ¹é…çš„å¾—åˆ†ï¼ˆ64.14ï¼‰ï¼Œä¸R1-AQAç›¸å½“ï¼ŒåŒæ—¶ä»¥å…¶å“è¶Šçš„æ•°æ®å’Œå‚æ•°æ•ˆç‡ã€å•é˜¶æ®µè®­ç»ƒå’Œé€æ˜åº¦è„±é¢–è€Œå‡ºã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬æœ€å°çš„æ¨¡å‹ï¼ˆå‚æ•°è§„æ¨¡ä¸º1Bï¼‰ä»ç„¶å…·æœ‰ç«äº‰åŠ›ï¼Œä¸å‚æ•°è§„æ¨¡ä»‹äº2Båˆ°13Bä¹‹é—´çš„å…¶ä»–å¤§å‹å…¬å¼€æ¨¡å‹ä¸ç›¸ä¸Šä¸‹ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œæˆ‘ä»¬å‘ç°å³ä½¿ä¸åœ¨è¶…è¿‡50ä¸‡å°æ—¶çš„æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œå¸¸è§çš„å¤æ‚æ€§ç­–ç•¥ï¼ˆå¦‚è¯¾ç¨‹å­¦ä¹ ã€å¤šä¸ªéŸ³é¢‘ç¼–ç å™¨å’Œå¤æ‚çš„è·¨æ³¨æ„åŠ›è¿æ¥å™¨ï¼‰å¯¹å“è¶Šæ€§èƒ½å¹¶ä¸å¿…è¦ã€‚æ€»ä½“æ¥è¯´ï¼Œæœ¬ç ”ç©¶å±•ç¤ºäº†é«˜æ•ˆçš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ä¸éŸ³é¢‘èåˆæ–¹é¢ä»å¾…æ·±åŒ–ç ”ç©¶ã€‚</li>
<li>Falcon3-Audioç³»åˆ—æ˜¯åŸºäºæŒ‡ä»¤è°ƒä¼˜çš„å¤§å‹è¯­è¨€æ¨¡å‹å’ŒWhisperç¼–ç å™¨çš„éŸ³è§†è¯­è¨€æ¨¡å‹ã€‚</li>
<li>ä½¿ç”¨å°‘é‡å…¬å¼€éŸ³é¢‘æ•°æ®ï¼ŒFalcon3-Audio-7Båœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œä¸å…¶ä»–å¤§å‹å…¬å¼€æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>Falcon3-Audioç³»åˆ—å±•ç°å‡ºå“è¶Šçš„æ•°æ®å’Œå‚æ•°æ•ˆç‡ã€å•é˜¶æ®µè®­ç»ƒå’Œé€æ˜åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07526">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07526v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07526v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07526v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.07526v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SFR-DeepResearch-Towards-Effective-Reinforcement-Learning-for-Autonomously-Reasoning-Single-Agents"><a href="#SFR-DeepResearch-Towards-Effective-Reinforcement-Learning-for-Autonomously-Reasoning-Single-Agents" class="headerlink" title="SFR-DeepResearch: Towards Effective Reinforcement Learning for   Autonomously Reasoning Single Agents"></a>SFR-DeepResearch: Towards Effective Reinforcement Learning for   Autonomously Reasoning Single Agents</h2><p><strong>Authors:Xuan-Phi Nguyen, Shrey Pandit, Revanth Gangi Reddy, Austin Xu, Silvio Savarese, Caiming Xiong, Shafiq Joty</strong></p>
<p>Equipping large language models (LLMs) with complex, interleaved reasoning and tool-use capabilities has become a key focus in agentic AI research, especially with recent advances in reasoning-oriented (&#96;&#96;thinkingâ€™â€™) models. Such capabilities are key to unlocking a number of important applications. One such application is Deep Research (DR), which requires extensive search and reasoning over many sources. Our work in this paper focuses on the development of native Autonomous Single-Agent models for DR featuring minimal web crawling and Python tool integration. Unlike multi-agent systems, where agents take up pre-defined roles and are told what to do at each step in a static workflow, an autonomous single-agent determines its next action dynamically based on context, without manual directive. While prior work has proposed training recipes for base or instruction-tuned LLMs, we focus on continual reinforcement learning (RL) of reasoning-optimized models to further enhance agentic skills while preserving reasoning ability. Towards this end, we propose a simple RL recipe with entirely synthetic data, which we apply to various open-source LLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanityâ€™s Last Exam benchmark. In addition, we conduct key analysis experiments to provide more insights into our methodologies. </p>
<blockquote>
<p>ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é…å¤‡å¤æ‚ã€äº¤ç»‡çš„æ¨ç†å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›å·²æˆä¸ºä»£ç†äººå·¥æ™ºèƒ½ç ”ç©¶çš„å…³é”®ç„¦ç‚¹ï¼Œå°¤å…¶æ˜¯æœ€è¿‘é¢å‘æ¨ç†çš„ï¼ˆâ€œæ€è€ƒâ€ï¼‰æ¨¡å‹çš„è¿›æ­¥ã€‚è¿™ç§èƒ½åŠ›æ˜¯è§£é”è®¸å¤šé‡è¦åº”ç”¨çš„å…³é”®ã€‚å…¶ä¸­ä¸€ä¸ªåº”ç”¨æ˜¯æ·±åº¦ç ”ç©¶ï¼ˆDRï¼‰ï¼Œå®ƒéœ€è¦åœ¨è®¸å¤šæºä¹‹é—´è¿›è¡Œå¹¿æ³›æœç´¢å’Œæ¨ç†ã€‚æœ¬æ–‡çš„å·¥ä½œé‡ç‚¹æ˜¯ä¸ºDRå¼€å‘æœ¬åœ°è‡ªä¸»å•ä»£ç†æ¨¡å‹ï¼Œå…·æœ‰æœ€å°çš„ç½‘ç»œçˆ¬è™«å’ŒPythonå·¥å…·é›†æˆã€‚ä¸å¤šä»£ç†ç³»ç»Ÿä¸åŒï¼Œå¤šä»£ç†ç³»ç»Ÿä¸­çš„ä»£ç†æ‰®æ¼”é¢„å®šä¹‰è§’è‰²ï¼Œå¹¶åœ¨é™æ€å·¥ä½œæµä¸­çš„æ¯ä¸€æ­¥è¢«å‘ŠçŸ¥è¦åšä»€ä¹ˆï¼Œè€Œè‡ªä¸»å•ä»£ç†åˆ™æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€ç¡®å®šå…¶ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡ä»¤ã€‚è™½ç„¶å…ˆå‰çš„å·¥ä½œå·²ç»æå‡ºäº†é’ˆå¯¹åŸºç¡€æˆ–æŒ‡ä»¤å¾®è°ƒLLMçš„è®­ç»ƒé…æ–¹ï¼Œä½†æˆ‘ä»¬ä¸“æ³¨äºå¯¹æ¨ç†ä¼˜åŒ–æ¨¡å‹çš„æŒç»­å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºä»£ç†æŠ€èƒ½ï¼ŒåŒæ—¶ä¿ç•™æ¨ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„å…¨åˆæˆæ•°æ®çš„RLé…æ–¹ï¼Œå°†å…¶åº”ç”¨äºå„ç§å¼€æºLLMã€‚æˆ‘ä»¬æœ€å¥½çš„å˜ä½“SFR-DR-20Båœ¨â€œäººç±»æœ€åçš„è€ƒè¯•â€åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†28.7%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å…³é”®çš„åˆ†æå®éªŒï¼Œä»¥æ›´æ·±å…¥äº†è§£æˆ‘ä»¬çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06283v2">PDF</a> Technical Report</p>
<p><strong>æ‘˜è¦</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è£…å¤‡å¤æ‚çš„äº¤é”™æ¨ç†å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›å·²æˆä¸ºä»£ç†äººå·¥æ™ºèƒ½ç ”ç©¶çš„å…³é”®ç„¦ç‚¹ï¼Œå°¤å…¶æ˜¯æœ€è¿‘çš„æ¨ç†å¯¼å‘ï¼ˆâ€œæ€è€ƒâ€ï¼‰æ¨¡å‹è¿›å±•ã€‚æ­¤ç±»èƒ½åŠ›æ˜¯è§£é”å¤šä¸ªé‡è¦åº”ç”¨çš„å…³é”®ã€‚æ·±åº¦ç ”ç©¶ï¼ˆDRï¼‰å°±æ˜¯ä¸€ä¸ªä¾‹å­ï¼Œå®ƒéœ€è¦è¿›è¡Œå¤§é‡æºæ•°æ®çš„æœç´¢å’Œæ¨ç†ã€‚æœ¬æ–‡çš„é‡ç‚¹æ˜¯å¼€å‘ç”¨äºDRçš„è‡ªä¸»å•ä»£ç†æ¨¡å‹ï¼Œå…·æœ‰æœ€å°‘çš„ç½‘ç»œçˆ¬è™«å’ŒPythonå·¥å…·é›†æˆã€‚ä¸åŒäºå¤šä»£ç†ç³»ç»Ÿï¼Œå…¶ä¸­ä»£ç†æ‰¿æ‹…é¢„å®šä¹‰è§’è‰²å¹¶åœ¨é™æ€å·¥ä½œæµç¨‹çš„æ¯ä¸€æ­¥è¢«å‘ŠçŸ¥è¦åšä»€ä¹ˆï¼Œè‡ªä¸»å•ä»£ç†ä¼šæ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€åœ°ç¡®å®šå…¶ä¸‹ä¸€ä¸ªè¡ŒåŠ¨ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡ä»¤ã€‚è™½ç„¶å…ˆå‰çš„å·¥ä½œå·²ç»æå‡ºäº†é’ˆå¯¹åŸºç¡€æˆ–æŒ‡ä»¤å¾®è°ƒLLMçš„è®­ç»ƒé…æ–¹ï¼Œä½†æˆ‘ä»¬å°†é‡ç‚¹æ”¾åœ¨æŒç»­å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¨ç†ä¼˜åŒ–æ¨¡å‹ä¸Šï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºä»£ç†æŠ€èƒ½åŒæ—¶ä¿ç•™æ¨ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä½¿ç”¨åˆæˆæ•°æ®çš„ç®€å•RLé…æ–¹ï¼Œå¹¶åº”ç”¨äºå„ç§å¼€æºLLMã€‚æˆ‘ä»¬æœ€å¥½çš„å˜ä½“SFR-DR-20Båœ¨â€œäººç±»æœ€åçš„è€ƒè¯•â€åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†28.7%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å…³é”®çš„åˆ†æå®éªŒï¼Œä»¥æä¾›æ›´æ·±å…¥çš„æ–¹æ³•è®ºæ´å¯Ÿã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£é€æ¸é…å¤‡å¤æ‚çš„äº¤é”™æ¨ç†å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œæˆä¸ºä»£ç†äººå·¥æ™ºèƒ½ç ”ç©¶çš„å…³é”®ç„¦ç‚¹ã€‚</li>
<li>è‡ªä¸»å•ä»£ç†æ¨¡å‹åœ¨æ·±åº¦ç ”ç©¶ï¼ˆDRï¼‰åº”ç”¨ä¸­å…·æœ‰æ½œåŠ›ï¼Œèƒ½åŠ¨æ€åœ°åŸºäºä¸Šä¸‹æ–‡ç¡®å®šè¡ŒåŠ¨ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡ä»¤ã€‚</li>
<li>ä¸å¤šä»£ç†ç³»ç»Ÿä¸åŒï¼Œè‡ªä¸»å•ä»£ç†æ¨¡å‹æ›´åŠ çµæ´»å’Œè‡ªé€‚åº”ã€‚</li>
<li>é€šè¿‡æŒç»­å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥å¢å¼ºLLMçš„æ¨ç†ä¼˜åŒ–æ¨¡å‹ï¼Œä»¥è¿›ä¸€æ­¥æå‡å…¶ä»£ç†æŠ€èƒ½å¹¶ä¿ç•™æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨åˆæˆæ•°æ®çš„ç®€å•RLé…æ–¹è¢«åº”ç”¨äºå¤šç§å¼€æºLLMã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒæŸäº›LLMåœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½è¡¨ç°ä¼˜å¼‚ï¼Œå¦‚SFR-DR-20Båœ¨â€œäººç±»æœ€åçš„è€ƒè¯•â€åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†28.7%çš„å‡†ç¡®ç‡ã€‚</li>
<li>é€šè¿‡å…³é”®åˆ†æå®éªŒï¼Œæä¾›äº†å…³äºæ–¹æ³•è®ºçš„æ·±å…¥æ´å¯Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06283">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.06283v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.06283v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="LM-Searcher-Cross-domain-Neural-Architecture-Search-with-LLMs-via-Unified-Numerical-Encoding"><a href="#LM-Searcher-Cross-domain-Neural-Architecture-Search-with-LLMs-via-Unified-Numerical-Encoding" class="headerlink" title="LM-Searcher: Cross-domain Neural Architecture Search with LLMs via   Unified Numerical Encoding"></a>LM-Searcher: Cross-domain Neural Architecture Search with LLMs via   Unified Numerical Encoding</h2><p><strong>Authors:Yuxuan Hu, Jihao Liu, Ke Wang, Jinliang Zhen, Weikang Shi, Manyuan Zhang, Qi Dou, Rui Liu, Aojun Zhou, Hongsheng Li</strong></p>
<p>Recent progress in Large Language Models (LLMs) has opened new avenues for solving complex optimization problems, including Neural Architecture Search (NAS). However, existing LLM-driven NAS approaches rely heavily on prompt engineering and domain-specific tuning, limiting their practicality and scalability across diverse tasks. In this work, we propose LM-Searcher, a novel framework that leverages LLMs for cross-domain neural architecture optimization without the need for extensive domain-specific adaptation. Central to our approach is NCode, a universal numerical string representation for neural architectures, which enables cross-domain architecture encoding and search. We also reformulate the NAS problem as a ranking task, training LLMs to select high-performing architectures from candidate pools using instruction-tuning samples derived from a novel pruning-based subspace sampling strategy. Our curated dataset, encompassing a wide range of architecture-performance pairs, encourages robust and transferable learning. Comprehensive experiments demonstrate that LM-Searcher achieves competitive performance in both in-domain (e.g., CNNs for image classification) and out-of-domain (e.g., LoRA configurations for segmentation and generation) tasks, establishing a new paradigm for flexible and generalizable LLM-based architecture search. The datasets and models will be released at <a target="_blank" rel="noopener" href="https://github.com/Ashone3/LM-Searcher">https://github.com/Ashone3/LM-Searcher</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä¸ºè§£å†³å¤æ‚çš„ä¼˜åŒ–é—®é¢˜å¼€è¾Ÿäº†æ–°çš„é€”å¾„ï¼ŒåŒ…æ‹¬ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºLLMçš„NASæ–¹æ³•ä¸¥é‡ä¾èµ–äºæç¤ºå·¥ç¨‹å’Œç‰¹å®šé¢†åŸŸçš„è°ƒæ•´ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨è·¨ä¸åŒä»»åŠ¡æ—¶çš„å®ç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†LM-Searcherï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨LLMè¿›è¡Œè·¨åŸŸç¥ç»ç½‘ç»œæ¶æ„ä¼˜åŒ–ï¼Œè€Œæ— éœ€è¿›è¡Œå¹¿æ³›çš„ç‰¹å®šé¢†åŸŸé€‚åº”ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯NCodeï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç¥ç»ç½‘ç»œæ¶æ„çš„é€šç”¨æ•°å€¼å­—ç¬¦ä¸²è¡¨ç¤ºï¼Œå®ƒå®ç°äº†è·¨åŸŸæ¶æ„ç¼–ç å’Œæœç´¢ã€‚æˆ‘ä»¬è¿˜é‡æ–°å°†NASé—®é¢˜è¡¨è¿°ä¸ºæ’åä»»åŠ¡ï¼Œè®­ç»ƒLLMä»å€™é€‰æ± ä¸­é€‰å…—é«˜æ€§èƒ½æ¶æ„ï¼Œä½¿ç”¨æ¥è‡ªæ–°å‹åŸºäºå‰ªæçš„å­ç©ºé—´é‡‡æ ·ç­–ç•¥çš„æŒ‡å¯¼è°ƒæ•´æ ·æœ¬ã€‚æˆ‘ä»¬ç¼–åˆ¶çš„æ•°æ®é›†åŒ…å«å¹¿æ³›çš„æ¶æ„-æ€§èƒ½å¯¹ï¼Œé¼“åŠ±é²æ£’å’Œå¯è¿ç§»å­¦ä¹ ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒLM-Searcheråœ¨åŸŸå†…ï¼ˆä¾‹å¦‚ï¼Œç”¨äºå›¾åƒåˆ†ç±»çš„CNNï¼‰å’ŒåŸŸå¤–ï¼ˆä¾‹å¦‚ï¼Œç”¨äºåˆ†å‰²å’Œç”Ÿæˆçš„LoRAé…ç½®ï¼‰ä»»åŠ¡ä¸­å‡å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œä¸ºçµæ´»å’Œé€šç”¨çš„åŸºäºLLMçš„æ¶æ„æœç´¢å»ºç«‹äº†æ–°èŒƒå¼ã€‚æ•°æ®é›†å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Ashone3/LM-Searcher%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Ashone3/LM-Searcherå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05657v2">PDF</a> EMNLP 2025 Main</p>
<p><strong>Summary</strong>ï¼šæœ€æ–°è¿›å±•çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºæ±‚è§£å¤æ‚çš„ä¼˜åŒ–é—®é¢˜ï¼ŒåŒ…æ‹¬ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰ï¼Œæä¾›äº†æ–°çš„é€”å¾„ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMé©±åŠ¨çš„NASæ–¹æ³•ä¸¥é‡ä¾èµ–äºæç¤ºå·¥ç¨‹å’Œç‰¹å®šé¢†åŸŸçš„è°ƒæ•´ï¼Œè¿™åœ¨è·¨ä¸åŒä»»åŠ¡çš„å®ç”¨æ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„æ¡†æ¶LM-Searcherï¼Œè¯¥æ¡†æ¶åˆ©ç”¨LLMè¿›è¡Œè·¨åŸŸç¥ç»ç½‘ç»œæ¶æ„ä¼˜åŒ–ï¼Œæ— éœ€å¹¿æ³›çš„ç‰¹å®šé¢†åŸŸé€‚åº”ã€‚å…¶æ ¸å¿ƒæ˜¯NCodeï¼Œä¸€ç§ç”¨äºç¥ç»ç½‘ç»œæ¶æ„çš„é€šç”¨æ•°å€¼å­—ç¬¦ä¸²è¡¨ç¤ºï¼Œå®ƒå®ç°äº†è·¨åŸŸæ¶æ„ç¼–ç å’Œæœç´¢ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å°†NASé—®é¢˜é‡æ–°è¡¨è¿°ä¸ºæ’åä»»åŠ¡ï¼Œè®­ç»ƒLLMä»å€™é€‰æ± ä¸­ä¸ºé«˜æ€§èƒ½æ¶æ„æ’åï¼Œåˆ©ç”¨åŸºäºä¿®å‰ªçš„å­ç©ºé—´é‡‡æ ·ç­–ç•¥ç”Ÿæˆçš„æŒ‡ä»¤è°ƒæ•´æ ·æœ¬ã€‚å®éªŒè¡¨æ˜ï¼ŒLM-Searcheråœ¨åŸŸå†…ï¼ˆå¦‚ç”¨äºå›¾åƒåˆ†ç±»çš„CNNï¼‰å’ŒåŸŸå¤–ï¼ˆå¦‚ç”¨äºåˆ†å‰²å’Œç”Ÿæˆçš„LoRAé…ç½®ï¼‰çš„ä»»åŠ¡ä¸­å‡å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMä¸ºè§£å†³å¤æ‚ä¼˜åŒ–é—®é¢˜å¦‚NASæä¾›äº†æ–°çš„é€”å¾„ã€‚</li>
<li>ç°æœ‰LLM-driven NASæ–¹æ³•ä¾èµ–æç¤ºå·¥ç¨‹å’Œç‰¹å®šé¢†åŸŸè°ƒæ•´ï¼Œå…·æœ‰å±€é™æ€§ã€‚</li>
<li>LM-Searcheræ¡†æ¶åˆ©ç”¨LLMè¿›è¡Œè·¨åŸŸç¥ç»ç½‘ç»œæ¶æ„ä¼˜åŒ–ï¼Œæ— éœ€å¹¿æ³›ç‰¹å®šé¢†åŸŸé€‚åº”ã€‚</li>
<li>NCodeæ˜¯LM-Searcherçš„æ ¸å¿ƒï¼Œæä¾›äº†ä¸€ç§è·¨åŸŸçš„ç¥ç»ç½‘ç»œæ¶æ„ç¼–ç å’Œæœç´¢æ–¹æ³•ã€‚</li>
<li>LM-Searcherå°†NASé—®é¢˜é‡æ–°è¡¨è¿°ä¸ºæ’åä»»åŠ¡ï¼Œåˆ©ç”¨æŒ‡ä»¤è°ƒæ•´æ ·æœ¬è®­ç»ƒLLMã€‚</li>
<li>LM-Searcheråœ¨å¤šç§ä»»åŠ¡ä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ï¼ŒåŒ…æ‹¬åŸŸå†…å’ŒåŸŸå¤–çš„ä»»åŠ¡ã€‚</li>
<li>LM-Searcheræ•°æ®é›†å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Ashone3/LM-Searcher%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Ashone3/LM-Searcherå‘å¸ƒã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05657">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.05657v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.05657v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.05657v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.05657v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2509.05657v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MaRVL-QA-A-Benchmark-for-Mathematical-Reasoning-over-Visual-Landscapes"><a href="#MaRVL-QA-A-Benchmark-for-Mathematical-Reasoning-over-Visual-Landscapes" class="headerlink" title="MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes"></a>MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes</h2><p><strong>Authors:Nilay Pande, Sahiti Yerramilli, Jayant Sravan Tamarapalli, Rynaa Grover</strong></p>
<p>A key frontier for Multimodal Large Language Models (MLLMs) is the ability to perform deep mathematical and spatial reasoning directly from images, moving beyond their established success in semantic description. Mathematical surface plots provide a rigorous testbed for this capability, as they isolate the task of reasoning from the semantic noise common in natural images. To measure progress on this frontier, we introduce MaRVL-QA (Mathematical Reasoning over Visual Landscapes), a new benchmark designed to quantitatively evaluate these core reasoning skills. The benchmark comprises two novel tasks: Topological Counting, identifying and enumerating features like local maxima; and Transformation Recognition, recognizing applied geometric transformations. Generated from a curated library of functions with rigorous ambiguity filtering, our evaluation on MaRVL-QA reveals that even state-of-the-art MLLMs struggle significantly, often resorting to superficial heuristics instead of robust spatial reasoning. MaRVL-QA provides a challenging new tool for the research community to measure progress, expose model limitations, and guide the development of MLLMs with more profound reasoning abilities. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å…³é”®å‰æ²¿æ˜¯ç›´æ¥ä»å›¾åƒè¿›è¡Œæ·±åº¦æ•°å­¦å’Œç©ºé—´æ¨ç†çš„èƒ½åŠ›ï¼Œè¿™è¶…è¶Šäº†å…¶åœ¨è¯­ä¹‰æè¿°æ–¹é¢å·²ç»å–å¾—çš„æˆå°±ã€‚æ•°å­¦æ›²é¢å›¾ä¸ºæ­¤ç±»èƒ½åŠ›æä¾›äº†ä¸¥æ ¼çš„æµ‹è¯•å¹³å°ï¼Œå› ä¸ºå®ƒä»¬å°†æ¨ç†ä»»åŠ¡ä¸è‡ªç„¶å›¾åƒä¸­å¸¸è§çš„è¯­ä¹‰å™ªå£°éš”ç¦»å¼€æ¥ã€‚ä¸ºäº†è¡¡é‡è¿™æ–¹é¢çš„è¿›å±•ï¼Œæˆ‘ä»¬å¼•å…¥äº†MaRVL-QAï¼ˆåŸºäºè§†è§‰æ™¯è§‚çš„æ•°å­¦æ¨ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å®šé‡è¯„ä¼°è¿™äº›æ ¸å¿ƒæ¨ç†æŠ€èƒ½ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«ä¸¤ä¸ªæ–°ä»»åŠ¡ï¼šæ‹“æ‰‘è®¡æ•°ï¼Œè¯†åˆ«å’Œåˆ—ä¸¾å±€éƒ¨æœ€å¤§å€¼ç­‰ç‰¹å¾ï¼›ä»¥åŠè½¬æ¢è¯†åˆ«ï¼Œè¯†åˆ«åº”ç”¨çš„å‡ ä½•å˜æ¢ã€‚è¿™äº›å‡½æ•°æ¥æºäºä¸€ä¸ªç²¾å¿ƒç­›é€‰çš„å‡½æ•°åº“ï¼Œç»è¿‡ä¸¥æ ¼çš„æ­§ä¹‰è¿‡æ»¤ï¼Œæˆ‘ä»¬å¯¹MaRVL-QAçš„è¯„ä¼°è¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„MLLMsä¹Ÿé¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œé€šå¸¸æ›´å€¾å‘äºä½¿ç”¨è‚¤æµ…çš„å¯å‘å¼æ–¹æ³•è€Œä¸æ˜¯ç¨³å¥çš„ç©ºé—´æ¨ç†ã€‚MaRVL-QAä¸ºå­¦æœ¯ç•Œæä¾›äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ–°å·¥å…·ï¼Œå¯ä»¥ç”¨æ¥è¡¡é‡è¿›å±•ã€æš´éœ²æ¨¡å‹å±€é™æ€§ï¼Œå¹¶å¼•å¯¼å¼€å‘å…·æœ‰æ›´æ·±å±‚æ¬¡æ¨ç†èƒ½åŠ›çš„MLLMsã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17180v2">PDF</a> </p>
<p><strong>Summary</strong><br>     å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å…³é”®å‰æ²¿æ˜¯èƒ½ç›´æ¥ä»å›¾åƒä¸­è¿›è¡Œæ·±åº¦æ•°å­¦å’Œç©ºé—´æ¨ç†ï¼Œè¶…è¶Šäº†å…¶åœ¨è¯­ä¹‰æè¿°æ–¹é¢çš„æˆåŠŸåº”ç”¨ã€‚ä¸ºæµ‹è¯•è¿™ä¸€èƒ½åŠ›ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MaRVL-QAï¼ˆåŸºäºè§†è§‰æ™¯è§‚çš„æ•°å­¦æ¨ç†ï¼‰æ–°åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å®šé‡è¯„ä¼°è¿™äº›æ ¸å¿ƒæ¨ç†æŠ€èƒ½ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«ä¸¤ä¸ªæ–°ä»»åŠ¡ï¼šæ‹“æ‰‘è®¡æ•°ï¼Œè¯†åˆ«å’Œæšä¸¾å±€éƒ¨æœ€å¤§å€¼ç­‰ç‰¹å¾ï¼›ä»¥åŠè½¬æ¢è¯†åˆ«ï¼Œè¯†åˆ«åº”ç”¨çš„å‡ ä½•å˜æ¢ã€‚é€šè¿‡å¯¹MaRVL-QAçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„MLLMsä¹Ÿé¢ä¸´ç€å¾ˆå¤§çš„æŒ‘æˆ˜ï¼Œç»å¸¸ä¾èµ–æµ…å±‚çš„å¯å‘å¼æ–¹æ³•è€Œéç¨³å¥çš„ç©ºé—´æ¨ç†ã€‚MaRVL-QAä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ–°å·¥å…·ï¼Œå¯ä»¥è¡¡é‡è¿›å±•ã€æ­ç¤ºæ¨¡å‹å±€é™æ€§ï¼Œå¹¶å¼•å¯¼å¼€å‘å…·æœ‰æ›´æ·±å±‚æ¬¡æ¨ç†èƒ½åŠ›çš„MLLMsã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é¢ä¸´ä»å›¾åƒä¸­ç›´æ¥è¿›è¡Œæ·±åº¦æ•°å­¦å’Œç©ºé—´æ¨ç†çš„æŒ‘æˆ˜ã€‚</li>
<li>MaRVL-QAæ˜¯ä¸€ä¸ªæ—¨åœ¨å®šé‡è¯„ä¼°MLLMsæ ¸å¿ƒæ¨ç†æŠ€èƒ½çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>MaRVL-QAåŒ…å«ä¸¤ä¸ªæ–°ä»»åŠ¡ï¼šæ‹“æ‰‘è®¡æ•°å’Œè½¬æ¢è¯†åˆ«ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼Œå³ä½¿æ˜¯å…ˆè¿›çš„MLLMsåœ¨MaRVL-QAä¸Šä¹Ÿé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>MLLMså¸¸å¸¸ä¾èµ–æµ…å±‚çš„å¯å‘å¼æ–¹æ³•è€ŒéçœŸæ­£çš„ç©ºé—´æ¨ç†ã€‚</li>
<li>MaRVL-QAä¸ºç ”ç©¶ç¤¾åŒºæä¾›äº†è¡¡é‡è¿›å±•ã€æ­ç¤ºæ¨¡å‹å±€é™æ€§çš„å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17180">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2508.17180v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2508.17180v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2508.17180v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2508.17180v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2508.17180v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2508.17180v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2508.17180v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2508.17180v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Bhav-Net-Knowledge-Transfer-for-Cross-Lingual-Antonym-vs-Synonym-Distinction-via-Dual-Space-Graph-Transformers"><a href="#Bhav-Net-Knowledge-Transfer-for-Cross-Lingual-Antonym-vs-Synonym-Distinction-via-Dual-Space-Graph-Transformers" class="headerlink" title="Bhav-Net: Knowledge Transfer for Cross-Lingual Antonym vs Synonym   Distinction via Dual-Space Graph Transformers"></a>Bhav-Net: Knowledge Transfer for Cross-Lingual Antonym vs Synonym   Distinction via Dual-Space Graph Transformers</h2><p><strong>Authors:Samyak S. Sanghvi</strong></p>
<p>Antonym vs synonym distinction across multiple languages presents unique computational challenges due to the paradoxical nature of antonymous relationships words that share semantic domains while expressing opposite meanings. This work introduces Bhav-Net, a novel dual-space architecture that enables effective knowledge transfer from complex multilingual models to simpler, language-specific architectures while maintaining robust cross-lingual antonymâ€“synonym distinction capabilities. Our approach combines language-specific BERT encoders with graph transformer networks, creating distinct semantic projections where synonymous pairs cluster in one space while antonymous pairs exhibit high similarity in a complementary space. Through comprehensive evaluation across eight languages (English, German, French, Spanish, Italian, Portuguese, Dutch, and Russian), we demonstrate that semantic relationship modeling transfers effectively across languages. The dual-encoder design achieves competitive performance against state-of-the-art baselines while providing interpretable semantic representations and effective cross-lingual generalization. </p>
<blockquote>
<p>åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­ï¼Œåä¹‰è¯ä¸åŒä¹‰è¯çš„åŒºåˆ†è¡¨ç°å‡ºç‹¬ç‰¹çš„è®¡ç®—æŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºåä¹‰è¯å…³ç³»è¯çš„æ‚–è®ºæ€§è´¨é€ æˆçš„ï¼Œè¿™äº›è¯å…±äº«è¯­ä¹‰åŸŸå´è¡¨è¾¾ç›¸åçš„æ„ä¹‰ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†Bhav-Netï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŒç©ºé—´æ¶æ„ï¼Œå®ƒèƒ½å¤Ÿå®ç°ä»å¤æ‚çš„å¤šè¯­è¨€æ¨¡å‹åˆ°æ›´ç®€å•ã€ç‰¹å®šè¯­è¨€çš„æ¶æ„çš„æœ‰æ•ˆçŸ¥è¯†è¿ç§»ï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„è·¨è¯­è¨€åä¹‰è¯-åŒä¹‰è¯åŒºåˆ†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ç‰¹å®šè¯­è¨€çš„BERTç¼–ç å™¨å’Œå›¾å˜æ¢ç½‘ç»œï¼Œåˆ›å»ºç‹¬ç‰¹çš„è¯­ä¹‰æŠ•å½±ï¼Œå…¶ä¸­åŒä¹‰è¯å¯¹åœ¨ä¸€ä¸ªç©ºé—´ä¸­èšç±»ï¼Œè€Œåä¹‰è¯å¯¹åœ¨å¦ä¸€ä¸ªè¡¥å……ç©ºé—´ä¸­è¡¨ç°å‡ºé«˜ç›¸ä¼¼æ€§ã€‚é€šè¿‡å¯¹å…«ç§è¯­è¨€ï¼ˆè‹±è¯­ã€å¾·è¯­ã€æ³•è¯­ã€è¥¿ç­ç‰™è¯­ã€æ„å¤§åˆ©è¯­ã€è‘¡è„ç‰™è¯­ã€è·å…°è¯­å’Œä¿„è¯­ï¼‰çš„ç»¼åˆè¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†è¯­ä¹‰å…³ç³»æ¨¡å‹åœ¨ä¸åŒè¯­è¨€ä¹‹é—´çš„æœ‰æ•ˆè¿ç§»ã€‚åŒç¼–ç å™¨è®¾è®¡å®ç°äº†ä¸æœ€æ–°åŸºçº¿æŠ€æœ¯ç›¸ç«äº‰çš„æ€§èƒ½ï¼ŒåŒæ—¶æä¾›å¯è§£é‡Šçš„è¯­ä¹‰è¡¨ç¤ºå’Œæœ‰æ•ˆçš„è·¨è¯­è¨€æ³›åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15792v2">PDF</a> Found some issues and need to correct them</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°å‹çš„åŒç©ºé—´æ¶æ„â€”â€”Bhav-Netï¼Œå®ƒèƒ½å¤Ÿå®ç°å¯¹è·¨è¯­è¨€ç¯å¢ƒä¸‹åä¹‰å…³ç³»ä¸åŒä¹‰è¯å…³ç³»çš„æœ‰æ•ˆåŒºåˆ†ã€‚è¿™ä¸€æ¶æ„é€šè¿‡è¯­è¨€ç‰¹å®šçš„BERTç¼–ç å™¨å’Œå›¾è½¬æ¢ç½‘ç»œçš„ç»“åˆï¼Œå»ºç«‹ä¸åŒçš„è¯­ä¹‰æŠ•å½±ï¼Œä½¿å¾—åŒä¹‰è¯å¯¹åœ¨ä¸€ä¸ªç©ºé—´ä¸­èšç±»ï¼Œåä¹‰è¯å¯¹åœ¨å¦ä¸€ä¸ªç©ºé—´ä¸­è¡¨ç°å‡ºé«˜ç›¸ä¼¼æ€§ã€‚åœ¨å…«ç§è¯­è¨€ä¸Šçš„ç»¼åˆè¯„ä¼°è¯æ˜äº†è¯¥æ¶æ„åœ¨è·¨è¯­è¨€ç¯å¢ƒä¸‹çš„è¯­ä¹‰å…³ç³»å»ºæ¨¡è½¬ç§»çš„æœ‰æ•ˆæ€§ã€‚åŒç¼–ç å™¨è®¾è®¡å®ç°äº†ä¸æœ€æ–°æŠ€æœ¯æ°´å¹³çš„ç«äº‰æ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†å¯è§£é‡Šçš„è¯­ä¹‰è¡¨ç¤ºå’Œæœ‰æ•ˆçš„è·¨è¯­è¨€æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Bhav-Netæ˜¯ä¸€ç§æ–°å‹çš„åŒç©ºé—´æ¶æ„ï¼Œç”¨äºå¤„ç†è·¨è¯­è¨€çš„åä¹‰å…³ç³»ä¸åŒä¹‰è¯å…³ç³»ã€‚</li>
<li>è¯¥æ¶æ„ç»“åˆäº†è¯­è¨€ç‰¹å®šçš„BERTç¼–ç å™¨å’Œå›¾è½¬æ¢ç½‘ç»œï¼Œä»¥å»ºç«‹ä¸åŒçš„è¯­ä¹‰æŠ•å½±ã€‚</li>
<li>é€šè¿‡åŒä¹‰è¯åœ¨å•ä¸€ç©ºé—´ä¸­çš„èšç±»å’Œåä¹‰è¯åœ¨å¦ä¸€ç©ºé—´ä¸­çš„ç›¸ä¼¼æ€§å±•ç¤ºï¼Œå®ç°äº†è¯­ä¹‰å…³ç³»çš„æœ‰æ•ˆåŒºåˆ†ã€‚</li>
<li>åœ¨å…«ç§è¯­è¨€ä¸Šçš„ç»¼åˆè¯„ä¼°è¯æ˜äº†è¯¥æ¶æ„åœ¨è·¨è¯­è¨€ç¯å¢ƒä¸‹çš„è¯­ä¹‰å…³ç³»å»ºæ¨¡è½¬ç§»çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>åŒç¼–ç å™¨è®¾è®¡å®ç°äº†é«˜æ€§èƒ½è¡¨ç°ï¼ŒåŒæ—¶æä¾›äº†å¯è§£é‡Šçš„è¯­ä¹‰è¡¨ç¤ºã€‚</li>
<li>Bhav-Netæ¶æ„èƒ½å¤Ÿæ”¯æŒæœ‰æ•ˆçš„è·¨è¯­è¨€æ³›åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15792">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2508.15792v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2508.15792v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2508.15792v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="TreeReview-A-Dynamic-Tree-of-Questions-Framework-for-Deep-and-Efficient-LLM-based-Scientific-Peer-Review"><a href="#TreeReview-A-Dynamic-Tree-of-Questions-Framework-for-Deep-and-Efficient-LLM-based-Scientific-Peer-Review" class="headerlink" title="TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient   LLM-based Scientific Peer Review"></a>TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient   LLM-based Scientific Peer Review</h2><p><strong>Authors:Yuan Chang, Ziyue Li, Hengyuan Zhang, Yuanbo Kong, Yanru Wu, Hayden Kwok-Hay So, Zhijiang Guo, Liya Zhu, Ngai Wong</strong></p>
<p>While Large Language Models (LLMs) have shown significant potential in assisting peer review, current methods often struggle to generate thorough and insightful reviews while maintaining efficiency. In this paper, we propose TreeReview, a novel framework that models paper review as a hierarchical and bidirectional question-answering process. TreeReview first constructs a tree of review questions by recursively decomposing high-level questions into fine-grained sub-questions and then resolves the question tree by iteratively aggregating answers from leaf to root to get the final review. Crucially, we incorporate a dynamic question expansion mechanism to enable deeper probing by generating follow-up questions when needed. We construct a benchmark derived from ICLR and NeurIPS venues to evaluate our method on full review generation and actionable feedback comments generation tasks. Experimental results of both LLM-based and human evaluation show that TreeReview outperforms strong baselines in providing comprehensive, in-depth, and expert-aligned review feedback, while reducing LLM token usage by up to 80% compared to computationally intensive approaches. Our code and benchmark dataset are available at <a target="_blank" rel="noopener" href="https://github.com/YuanChang98/tree-review">https://github.com/YuanChang98/tree-review</a>. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ååŠ©åŒè¡Œè¯„å®¡æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å½“å‰çš„æ–¹æ³•å¾€å¾€éš¾ä»¥åœ¨ä¿æŒæ•ˆç‡çš„åŒæ—¶ç”Ÿæˆå…¨é¢è€Œæœ‰æ´å¯ŸåŠ›çš„è¯„å®¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†TreeReviewï¼Œè¿™æ˜¯ä¸€ä¸ªå°†è®ºæ–‡è¯„å®¡å»ºæ¨¡ä¸ºåˆ†å±‚åŒå‘é—®ç­”è¿‡ç¨‹çš„æ–°å‹æ¡†æ¶ã€‚TreeReviewé¦–å…ˆé€šè¿‡é€’å½’åœ°å°†é«˜çº§é—®é¢˜åˆ†è§£ä¸ºç²¾ç»†çš„å­é—®é¢˜æ¥æ„å»ºè¯„å®¡é—®é¢˜æ ‘ï¼Œç„¶åé€šè¿‡ä»å¶å­åˆ°æ ¹è¿­ä»£èšåˆç­”æ¡ˆæ¥è·å¾—æœ€ç»ˆè¯„å®¡ç»“æœã€‚å…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠ¨æ€é—®é¢˜æ‰©å±•æœºåˆ¶ï¼Œä»¥åœ¨éœ€è¦æ—¶ç”Ÿæˆè·Ÿè¿›é—®é¢˜ï¼Œä»è€Œå®ç°æ›´æ·±å…¥çš„æ¢ç©¶ã€‚æˆ‘ä»¬æ„å»ºäº†åŸºäºICLRå’ŒNeurIPSä¼šè®®è¡ç”Ÿçš„åŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°æˆ‘ä»¬åœ¨å®Œæ•´è¯„å®¡ç”Ÿæˆå’Œå¯æ“ä½œçš„åé¦ˆæ„è§ç”Ÿæˆä»»åŠ¡ä¸Šçš„æ–¹æ³•ã€‚åŸºäºLLMå’Œäººå·¥è¯„ä¼°çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTreeReviewåœ¨æä¾›å…¨é¢ã€æ·±å…¥ã€ä¸ä¸“å®¶ä¸€è‡´çš„è¯„å®¡åé¦ˆæ–¹é¢ä¼˜äºå¼ºåŸºçº¿ï¼ŒåŒæ—¶ä¸è®¡ç®—å¯†é›†å‹æ–¹æ³•ç›¸æ¯”ï¼Œå¯å‡å°‘é«˜è¾¾80%çš„LLMä»¤ç‰Œä½¿ç”¨ã€‚æˆ‘ä»¬çš„ä»£ç å’ŒåŸºå‡†æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/YuanChang98/tree-review%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/YuanChang98/tree-reviewæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07642v3">PDF</a> Accepted to EMNLP2025 Main</p>
<p><strong>Summary</strong><br>åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¾…åŠ©åŒè¡Œè¯„å®¡çš„é¢†åŸŸä¸­ï¼Œå½“å‰çš„æ–¹æ³•å¾€å¾€éš¾ä»¥åœ¨ç”Ÿæˆå…¨é¢è€Œæœ‰æ·±åº¦çš„è¯„è®ºçš„åŒæ—¶ä¿æŒæ•ˆç‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶TreeReviewï¼Œå®ƒå°†è®ºæ–‡è¯„å®¡å»ºæ¨¡ä¸ºä¸€ä¸ªå±‚æ¬¡åŒ–çš„åŒå‘é—®ç­”è¿‡ç¨‹ã€‚TreeReviewé€šè¿‡é€’å½’åˆ†è§£é«˜çº§é—®é¢˜ä¸ºç²¾ç»†çš„å­é—®é¢˜æ¥æ„å»ºè¯„å®¡é—®é¢˜çš„æ ‘ç»“æ„ï¼Œç„¶åé€šè¿‡ä»å¶å­åˆ°æ ¹çš„ç­”æ¡ˆèšåˆæ¥è·å¾—æœ€ç»ˆçš„è¯„å®¡ç»“æœã€‚å…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŠ¨æ€é—®é¢˜æ‰©å±•æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨éœ€è¦æ—¶ç”Ÿæˆè·Ÿè¿›é—®é¢˜ä»¥å®ç°æ›´æ·±å…¥çš„æ¢ç©¶ã€‚é€šè¿‡å®éªŒå’ŒåŸºå‡†æµ‹è¯•å‘ç°ï¼ŒTreeReviewåœ¨æä¾›å…¨é¢ã€æ·±å…¥ã€ä¸ä¸“å®¶ä¸€è‡´çš„è¯„å®¡åé¦ˆæ–¹é¢è¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶å°†LLMä»¤ç‰Œä½¿ç”¨å‡å°‘äº†é«˜è¾¾80%ã€‚æˆ‘ä»¬çš„ä»£ç å’ŒåŸºå‡†æ•°æ®é›†å¯åœ¨[é“¾æ¥åœ°å€]ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/YuanChang98/tree-review%EF%BC%89%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/YuanChang98/tree-reviewï¼‰æ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TreeReviewæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºåŒè¡Œè¯„å®¡è¿‡ç¨‹ï¼Œå°†å…¶å»ºæ¨¡ä¸ºå±‚æ¬¡åŒ–çš„åŒå‘é—®ç­”è¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡é€’å½’åˆ†è§£é«˜çº§é—®é¢˜ä¸ºç²¾ç»†çš„å­é—®é¢˜ï¼ŒTreeReviewæ„å»ºäº†è¯„å®¡é—®é¢˜çš„æ ‘ç»“æ„ã€‚</li>
<li>TreeReviewå¼•å…¥äº†åŠ¨æ€é—®é¢˜æ‰©å±•æœºåˆ¶ä»¥æ”¯æŒæ›´æ·±å…¥çš„é—®é¢˜æ¢ç©¶ã€‚</li>
<li>TreeReviewåœ¨ç”Ÿæˆå…¨é¢ã€æ·±å…¥çš„è¯„å®¡åé¦ˆæ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>TreeReviewç›¸è¾ƒäºå…¶ä»–æ–¹æ³•ï¼Œå¯æä¾›ä¸“å®¶çº§çš„è¯„å®¡åé¦ˆã€‚</li>
<li>TreeReviewæ˜¾è‘—å‡å°‘äº†åœ¨è®¡ç®—å¯†é›†å‹æ–¹æ³•ä¸­çš„LLMä»¤ç‰Œä½¿ç”¨ï¼Œæ•ˆç‡æé«˜äº†80%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07642">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2506.07642v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2506.07642v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2506.07642v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Is-Your-LLM-Overcharging-You-Tokenization-Transparency-and-Incentives"><a href="#Is-Your-LLM-Overcharging-You-Tokenization-Transparency-and-Incentives" class="headerlink" title="Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives"></a>Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives</h2><p><strong>Authors:Ander Artola Velasco, Stratis Tsirtsis, Nastaran Okati, Manuel Gomez-Rodriguez</strong></p>
<p>State-of-the-art large language models require specialized hardware and substantial energy to operate. As a consequence, cloud-based services that provide access to large language models have become very popular. In these services, the price users pay for an output provided by a model depends on the number of tokens the model uses to generate it â€“ they pay a fixed price per token. In this work, we show that this pricing mechanism creates a financial incentive for providers to strategize and misreport the (number of) tokens a model used to generate an output, and users cannot prove, or even know, whether a provider is overcharging them. However, we also show that, if an unfaithful provider is obliged to be transparent about the generative process used by the model, misreporting optimally without raising suspicion is hard. Nevertheless, as a proof-of-concept, we develop an efficient heuristic algorithm that allows providers to significantly overcharge users without raising suspicion. Crucially, we demonstrate that the cost of running the algorithm is lower than the additional revenue from overcharging users, highlighting the vulnerability of users under the current pay-per-token pricing mechanism. Further, we show that, to eliminate the financial incentive to strategize, a pricing mechanism must price tokens linearly on their character count. While this makes a providerâ€™s profit margin vary across tokens, we introduce a simple prescription under which the provider who adopts such an incentive-compatible pricing mechanism can maintain the average profit margin they had under the pay-per-token pricing mechanism. Along the way, to illustrate and complement our theoretical results, we conduct experiments with several large language models from the $\texttt{Llama}$, $\texttt{Gemma}$ and $\texttt{Ministral}$ families, and input prompts from the LMSYS Chatbot Arena platform. </p>
<blockquote>
<p>å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹éœ€è¦ä¸“ä¸šç¡¬ä»¶å’Œå¤§é‡èƒ½æºæ‰èƒ½è¿è¡Œã€‚å› æ­¤ï¼Œæä¾›å¤§å‹è¯­è¨€æ¨¡å‹è®¿é—®çš„äº‘æœåŠ¡å˜å¾—éå¸¸å—æ¬¢è¿ã€‚åœ¨è¿™äº›æœåŠ¡ä¸­ï¼Œç”¨æˆ·ä¸ºæ¨¡å‹æä¾›çš„è¾“å‡ºæ”¯ä»˜çš„ä»·æ ¼å–å†³äºæ¨¡å‹ç”Ÿæˆè¾“å‡ºæ‰€ä½¿ç”¨çš„ä»¤ç‰Œæ•°é‡â€”â€”ä»–ä»¬æŒ‰ä»¤ç‰Œæ”¯ä»˜å›ºå®šä»·æ ¼ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜è¿™ç§å®šä»·æœºåˆ¶ä¸ºæä¾›è€…åˆ›é€ äº†ç­–ç•¥åŒ–å’Œè°æŠ¥æ¨¡å‹ç”¨äºç”Ÿæˆè¾“å‡ºæ‰€ä½¿ç”¨ä»¤ç‰Œæ•°é‡çš„è´¢åŠ¡æ¿€åŠ±ï¼Œç”¨æˆ·æ— æ³•è¯æ˜ï¼Œç”šè‡³ä¸çŸ¥é“æä¾›è€…æ˜¯å¦å¯¹ä»–ä»¬æ”¶å–äº†è¿‡é«˜çš„è´¹ç”¨ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¹Ÿè¡¨æ˜ï¼Œå¦‚æœä¸è¯šå®çš„æä¾›è€…å¿…é¡»å¯¹å…¶ä½¿ç”¨çš„æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹é€æ˜åŒ–ï¼Œé‚£ä¹ˆåœ¨æ²¡æœ‰å¼•èµ·æ€€ç–‘çš„æƒ…å†µä¸‹æœ€ä½³åœ°è°æŠ¥å°±å¾ˆå›°éš¾ã€‚å°½ç®¡å¦‚æ­¤ï¼Œä½œä¸ºä¸€ç§æ¦‚å¿µéªŒè¯ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é«˜æ•ˆçš„å¯å‘å¼ç®—æ³•ï¼Œå…è®¸æä¾›è€…å¤§å¹…å¯¹ç”¨æˆ·æ”¶å–é¢å¤–è´¹ç”¨è€Œä¸å¼•èµ·æ€€ç–‘ã€‚å…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜è¿è¡Œè¯¥ç®—æ³•çš„æˆæœ¬ä½äºå¯¹ç”¨æˆ·è¶…é¢æ”¶è´¹æ‰€å¸¦æ¥çš„é¢å¤–æ”¶å…¥ï¼Œè¿™çªæ˜¾äº†åœ¨å½“å‰æŒ‰ä»¤ç‰Œä»˜è´¹å®šä»·æœºåˆ¶ä¸‹ç”¨æˆ·çš„è„†å¼±æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œä¸ºäº†æ¶ˆé™¤äº§ç”Ÿç­–ç•¥åŒ–çš„è´¢åŠ¡æ¿€åŠ±ï¼Œå®šä»·æœºåˆ¶å¿…é¡»æŒ‰å­—ç¬¦æ•°å¯¹ä»¤ç‰Œè¿›è¡Œçº¿æ€§å®šä»·ã€‚è™½ç„¶è¿™ä½¿å¾—æä¾›è€…çš„åˆ©æ¶¦è¾¹é™…åœ¨ä¸åŒçš„ä»¤ç‰Œä¸Šæœ‰æ‰€ä¸åŒï¼Œæˆ‘ä»¬åœ¨å¼•å…¥ä¸€ä¸ªç®€å•çš„è§„å®šä¸‹å‘ç°é‡‡ç”¨è¿™ç§æ¿€åŠ±ç›¸å®¹å®šä»·æœºåˆ¶çš„æä¾›è€…å¯ä»¥ç»´æŒä»–ä»¬åœ¨æŒ‰ä»¤ç‰Œä»˜è´¹å®šä»·æœºåˆ¶ä¸‹çš„å¹³å‡åˆ©æ¶¦è¾¹é™…ã€‚ä¸ºäº†è¯´æ˜å’Œè¡¥å……æˆ‘ä»¬çš„ç†è®ºç»“æœï¼Œæˆ‘ä»¬åœ¨è·¯ä¸Šè¿›è¡Œäº†å‡ åœºå¤§å‹è¯­è¨€æ¨¡å‹çš„å®éªŒï¼Œè¿™äº›æ¨¡å‹æ¥è‡ªLLamaã€Gemmaå’ŒMinistralå®¶æ—ï¼Œè¾“å…¥æç¤ºæ¥è‡ªLMSYSèŠå¤©æœºå™¨äººç«æŠ€åœºå¹³å°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21627v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†åœ¨ä»¥æŒ‰ä»¤ç‰Œè®¡è´¹æ¨¡å¼è¿è¥çš„äº‘å¤§å‹è¯­è¨€æ¨¡å‹æœåŠ¡ä¸­ï¼Œæä¾›å•†çš„è´¢åŠ¡æ¿€åŠ±é—®é¢˜ã€‚è¿™ç§å®šä»·æœºåˆ¶ä¼šå¯¼è‡´æä¾›å•†æœ‰ç­–ç•¥æ€§åœ°è¯¯æŠ¥ç”Ÿæˆè¾“å‡ºæ‰€ä½¿ç”¨çš„ä»¤ç‰Œæ•°é‡ï¼Œç”¨æˆ·éš¾ä»¥è¯æ˜æˆ–ç”šè‡³ä¸çŸ¥é“æ˜¯å¦è¢«è¿‡åº¦æ”¶è´¹ã€‚ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé«˜æ•ˆçš„å¯å‘å¼ç®—æ³•ï¼Œå¯ä»¥åœ¨ä¸å¼•èµ·ç”¨æˆ·æ€€ç–‘çš„æƒ…å†µä¸‹å¤§å¹…æé«˜å¯¹ç”¨æˆ·çš„æ”¶è´¹ï¼Œå¹¶ä¸”è¿™ç§åšæ³•çš„æˆæœ¬ä½äºä»è¿‡åº¦æ”¶è´¹ä¸­è·å¾—çš„é¢å¤–æ”¶å…¥ã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼Œä¸ºäº†æ¶ˆé™¤è¿™ç§ç­–ç•¥è¡Œä¸ºçš„è´¢åŠ¡æ¿€åŠ±ï¼Œéœ€è¦é‡‡ç”¨ä¸€ç§åŸºäºå­—ç¬¦è®¡æ•°çš„çº¿æ€§å®šä»·æœºåˆ¶æ¥è°ƒæ•´ä»¤ç‰Œä»·æ ¼ã€‚é€šè¿‡è¿›è¡Œå¤§å‹è¯­è¨€æ¨¡å‹å®éªŒï¼Œè¯¥ç ”ç©¶è¡¥å……äº†ç†è®ºç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹éœ€è¦ä¸“é—¨ç¡¬ä»¶å’Œå¤§é‡èƒ½æºæ¥è¿è¡Œï¼Œå› æ­¤äº‘æä¾›çš„æœåŠ¡å˜å¾—éå¸¸å—æ¬¢è¿ã€‚</li>
<li>åœ¨è¿™äº›æœåŠ¡ä¸­ï¼ŒæŒ‰ä»¤ç‰Œè®¡è´¹æ¨¡å¼ä¸ºæä¾›å•†åˆ›é€ äº†è´¢åŠ¡æ¿€åŠ±ï¼Œä½¿ä»–ä»¬èƒ½å¤Ÿç­–ç•¥æ€§åœ°è¯¯æŠ¥ä»¤ç‰Œçš„ç”¨é‡ã€‚</li>
<li>ç”¨æˆ·éš¾ä»¥è¯æ˜æˆ–çŸ¥é“æä¾›å•†æ˜¯å¦è¿‡åº¦æ”¶è´¹ã€‚</li>
<li>ç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç§å¯å‘å¼ç®—æ³•ï¼Œå¯ä»¥åœ¨ä¸å¼•èµ·æ€€ç–‘çš„æƒ…å†µä¸‹å¤§å¹…æé«˜å¯¹ç”¨æˆ·çš„æ”¶è´¹ï¼Œå¹¶ä¸”è¿™ç§åšæ³•çš„æˆæœ¬ç›¸å¯¹è¾ƒä½ã€‚</li>
<li>è¿™ç§å®šä»·æœºåˆ¶å¯¼è‡´ç”¨æˆ·é¢ä¸´è´¢åŠ¡é£é™©ï¼Œå› æ­¤éœ€è¦é‡‡ç”¨åŸºäºå­—ç¬¦è®¡æ•°çš„çº¿æ€§å®šä»·æœºåˆ¶æ¥è°ƒæ•´ä»¤ç‰Œä»·æ ¼æ¥æ¶ˆé™¤æ¿€åŠ±é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨æ–°çš„å®šä»·æœºåˆ¶åï¼Œæä¾›å•†çš„å¹³å‡åˆ©æ¶¦ç‡å¯ä»¥ä¿æŒä¸å˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21627">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2505.21627v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2505.21627v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Towards-Visuospatial-Cognition-via-Hierarchical-Fusion-of-Visual-Experts"><a href="#Towards-Visuospatial-Cognition-via-Hierarchical-Fusion-of-Visual-Experts" class="headerlink" title="Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts"></a>Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts</h2><p><strong>Authors:Qi Feng</strong></p>
<p>While Multimodal Large Language Models (MLLMs) excel at general vision-language tasks, visuospatial cognition - reasoning about spatial layouts, relations, and dynamics - remains a significant challenge. Existing models often lack the necessary architectural components and specialized training data for fine-grained spatial understanding. We introduce ViCA2 (Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP for semantics and Hiera for spatial structure, coupled with a token ratio control mechanism for efficiency. We also developed ViCA-322K, a new large-scale dataset with over 322,000 spatially grounded question-answer pairs for targeted instruction tuning. On the challenging VSI-Bench benchmark, our ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the effectiveness of our approach in achieving strong visuospatial intelligence with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset to facilitate further research. </p>
<blockquote>
<p>è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä¸€èˆ¬çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å¯¹äºç©ºé—´å¸ƒå±€ã€å…³ç³»å’ŒåŠ¨æ€çš„ç©ºé—´è®¤çŸ¥ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ¨¡å‹å¾€å¾€ç¼ºä¹å¿…è¦çš„æ¶æ„ç»„ä»¶å’Œç²¾ç»†ç©ºé—´ç†è§£æ‰€éœ€çš„ä¸“é—¨è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬å¼•å…¥äº†ViCA2ï¼ˆè§†è§‰ç©ºé—´è®¤çŸ¥åŠ©æ‰‹2ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ViCA2é‡‡ç”¨åŒè§†è§‰ç¼–ç å™¨æ¶æ„ï¼Œé›†æˆäº†SigLIPç”¨äºè¯­ä¹‰åˆ†æå’ŒHieraç”¨äºç©ºé—´ç»“æ„åˆ†æï¼ŒåŒæ—¶é‡‡ç”¨ä»¤ç‰Œæ¯”ç‡æ§åˆ¶æœºåˆ¶ä»¥æé«˜æ•ˆç‡ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ViCA-322Kæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡32ä¸‡ç»„ç©ºé—´å®šä½é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œç”¨äºé’ˆå¯¹æŒ‡ä»¤è°ƒæ•´ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„VSI-BenchåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„ViCA2-7Bæ¨¡å‹è¾¾åˆ°äº†å¹³å‡å¾—åˆ†56.8çš„ä¸šç•Œæœ€ä½³æ°´å¹³ï¼Œæ˜¾è‘—è¶…è¿‡äº†å¤§å‹å¼€æºæ¨¡å‹ï¼ˆä¾‹å¦‚LLaVA-NeXT-Video-72Bï¼Œå¾—åˆ†ä¸º40.9ï¼‰å’Œé¢†å…ˆçš„ä¸“æœ‰æ¨¡å‹ï¼ˆGemini-1.5 Proï¼Œå¾—åˆ†ä¸º45.4ï¼‰ã€‚è¿™è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å®ç°å¼ºå¤§ç©ºé—´è®¤çŸ¥æ™ºèƒ½çš„ç´§å‡‘æ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å‘å¸ƒViCA2åŠå…¶ä»£ç åº“å’ŒViCA-322Kæ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12363v4">PDF</a> 26 pages, 19 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Multimodal Large Language Modelsåœ¨è§†è§‰ç©ºé—´è®¤çŸ¥æ–¹é¢çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ViCA2æ¥å¢å¼ºç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ViCA2å…·æœ‰åŒè§†è§‰ç¼–ç å™¨æ¶æ„ï¼Œèåˆäº†SigLIPè¯­ä¹‰å’ŒHieraç©ºé—´ç»“æ„ï¼Œé…åˆtokenæ¯”ç‡æ§åˆ¶æœºåˆ¶æé«˜æ•ˆç‡ã€‚åŒæ—¶å¼€å‘çš„å¤§å‹æ•°æ®é›†ViCA-322Kï¼Œç”¨äºé’ˆå¯¹æŒ‡ä»¤å¾®è°ƒã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„VSI-BenchåŸºå‡†æµ‹è¯•ä¸­ï¼ŒViCA2-7Bæ¨¡å‹å–å¾—äº†å¹³å‡å¾—åˆ†56.8çš„æœ€ä½³æˆç»©ï¼Œæ˜¾è‘—è¶…è¿‡äº†å…¶ä»–å¤§å‹å¼€æºæ¨¡å‹å’Œä¸“æœ‰æ¨¡å‹ã€‚è¿™è¯æ˜äº†ViCA2åœ¨æ„å»ºå¼ºå¤§è§†è§‰ç©ºé—´æ™ºèƒ½æ–¹æ³•ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å‘å¸ƒäº†ViCA2åŠå…¶ä»£ç åº“å’ŒViCA-322Kæ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Multimodal Large Language Modelsé¢ä¸´è§†è§‰ç©ºé—´è®¤çŸ¥çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†æ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ViCA2ä»¥å¢å¼ºç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ViCA2å…·æœ‰åŒè§†è§‰ç¼–ç å™¨æ¶æ„ï¼Œèåˆè¯­ä¹‰å’Œç©ºé—´ç»“æ„ã€‚</li>
<li>å¼€å‘äº†å¤§å‹æ•°æ®é›†ViCA-322Kç”¨äºé’ˆå¯¹æŒ‡ä»¤å¾®è°ƒã€‚</li>
<li>ViCA2åœ¨VSI-BenchåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€ä½³æˆç»©ï¼Œæ˜¾è‘—è¶…è¶Šå…¶ä»–æ¨¡å‹ã€‚</li>
<li>ViCA2æ–¹æ³•çš„æœ‰æ•ˆæ€§å¾—åˆ°è¯æ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12363">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2505.12363v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2505.12363v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2505.12363v4/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2505.12363v4/page_5_1.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Audio-centric-Video-Understanding-Benchmark-without-Text-Shortcut"><a href="#Audio-centric-Video-Understanding-Benchmark-without-Text-Shortcut" class="headerlink" title="Audio-centric Video Understanding Benchmark without Text Shortcut"></a>Audio-centric Video Understanding Benchmark without Text Shortcut</h2><p><strong>Authors:Yudong Yang, Jimin Zhuang, Guangzhi Sun, Changli Tang, Yixuan Li, Peihan Li, Yifan Jiang, Wei Li, Zejun Ma, Chao Zhang</strong></p>
<p>Audio often serves as an auxiliary modality in video understanding tasks of audio-visual large language models (LLMs), merely assisting in the comprehension of visual information. However, a thorough understanding of videos significantly depends on auditory information, as audio offers critical context, emotional cues, and semantic meaning that visual data alone often lacks. This paper proposes an audio-centric video understanding benchmark (AVUT) to evaluate the video comprehension capabilities of multimodal LLMs with a particular focus on auditory information. AVUT introduces a suite of carefully designed audio-centric tasks, holistically testing the understanding of both audio content and audio-visual interactions in videos. Moreover, this work points out the text shortcut problem that largely exists in other benchmarks where the correct answer can be found from question text alone without needing videos. AVUT addresses this problem by proposing a answer permutation-based filtering mechanism. A thorough evaluation across a diverse range of open-source and proprietary multimodal LLMs is performed, followed by the analyses of deficiencies in audio-visual LLMs. Demos and data are available at <a target="_blank" rel="noopener" href="https://github.com/lark-png/AVUT">https://github.com/lark-png/AVUT</a>. </p>
<blockquote>
<p>éŸ³é¢‘é€šå¸¸åœ¨è§†å¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§†é¢‘ç†è§£ä»»åŠ¡ä¸­å……å½“è¾…åŠ©æ¨¡æ€ï¼Œä»…ååŠ©ç†è§£è§†è§‰ä¿¡æ¯ã€‚ç„¶è€Œï¼Œå¯¹è§†é¢‘çš„å…¨é¢ç†è§£å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå¬è§‰ä¿¡æ¯ï¼Œå› ä¸ºéŸ³é¢‘æä¾›äº†å…³é”®ä¸Šä¸‹æ–‡ã€æƒ…æ„Ÿçº¿ç´¢å’Œè¯­ä¹‰å«ä¹‰ï¼Œè¿™äº›é€šå¸¸æ˜¯è§†è§‰æ•°æ®æ‰€ç¼ºä¹çš„ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªä»¥éŸ³é¢‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ï¼ˆAVUTï¼‰ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡å¼LLMçš„è§†é¢‘ç†è§£èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å¬è§‰ä¿¡æ¯çš„ç†è§£èƒ½åŠ›ã€‚AVUTå¼•å…¥äº†ä¸€å¥—ç²¾å¿ƒè®¾è®¡çš„ä»¥éŸ³é¢‘ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ï¼Œå…¨é¢æµ‹è¯•è§†é¢‘ä¸­çš„éŸ³é¢‘å†…å®¹å’Œè§†å¬äº¤äº’çš„ç†è§£ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æŒ‡å‡ºäº†åœ¨å…¶ä»–åŸºå‡†æµ‹è¯•ä¸­å¤§é‡å­˜åœ¨çš„æ–‡æœ¬æ·å¾„é—®é¢˜ï¼Œå³æ­£ç¡®ç­”æ¡ˆä»…å­˜åœ¨äºé—®é¢˜æ–‡æœ¬ä¸­ï¼Œè€Œæ— éœ€è§‚çœ‹è§†é¢‘ã€‚AVUTé€šè¿‡æå‡ºåŸºäºç­”æ¡ˆæ’åˆ—çš„è¿‡æ»¤æœºåˆ¶æ¥è§£å†³æ­¤é—®é¢˜ã€‚åœ¨å¤šæ ·åŒ–å’Œå¼€æºçš„è‡ªä¸»å¤šæ¨¡å¼LLMä¹‹é—´è¿›è¡Œå½»åº•è¯„ä¼°ï¼Œéšååˆ†æè§†å¬LLMçš„ç¼ºé™·ã€‚æ¼”ç¤ºå’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lark-png/AVUT">https://github.com/lark-png/AVUT</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19951v2">PDF</a> Accepted for publication in the Proceedings of EMNLP 2025 (Main   Conference)</p>
<p><strong>Summary</strong><br>     è¯¥è®ºæ–‡æå‡ºä¸€ä¸ªä»¥éŸ³é¢‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ï¼ˆAVUTï¼‰ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å¯¹è§†é¢‘çš„ç†è§£èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯éŸ³é¢‘ä¿¡æ¯ã€‚AVUTè®¾è®¡äº†ä¸€ç³»åˆ—éŸ³é¢‘ä¸­å¿ƒä»»åŠ¡ï¼Œå…¨é¢æµ‹è¯•è§†é¢‘ä¸­çš„éŸ³é¢‘å†…å®¹å’Œè§†å¬äº¤äº’çš„ç†è§£ã€‚æ­¤å¤–ï¼Œè¯¥è®ºæ–‡è¿˜æŒ‡å‡ºäº†å…¶ä»–åŸºå‡†æµ‹è¯•ä¸­æ™®éå­˜åœ¨çš„æ–‡æœ¬æ·å¾„é—®é¢˜ï¼Œå¹¶æå‡ºåŸºäºç­”æ¡ˆç½®æ¢çš„è¿‡æ»¤æœºåˆ¶æ¥è§£å†³æ­¤é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œæä¾›å…³é”®ä¸Šä¸‹æ–‡ã€æƒ…æ„Ÿçº¿ç´¢å’Œè¯­ä¹‰æ„ä¹‰ã€‚</li>
<li>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£å’Œå¤„ç†è§†å¬ä¿¡æ¯æ—¶ï¼Œå¯¹éŸ³é¢‘ä¿¡æ¯çš„ä¾èµ–ç¨‹åº¦è¾ƒé«˜ã€‚</li>
<li>AVUTåŸºå‡†æµ‹è¯•æ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€LLMçš„è§†é¢‘ç†è§£èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯éŸ³é¢‘å†…å®¹çš„ç†è§£ã€‚</li>
<li>AVUTé€šè¿‡è®¾è®¡ä¸€ç³»åˆ—éŸ³é¢‘ä¸­å¿ƒä»»åŠ¡æ¥å…¨é¢æµ‹è¯•è§†é¢‘çš„éŸ³é¢‘å†…å®¹å’Œè§†å¬äº¤äº’çš„ç†è§£ã€‚</li>
<li>è®ºæ–‡æŒ‡å‡ºäº†å…¶ä»–åŸºå‡†æµ‹è¯•ä¸­å­˜åœ¨çš„æ–‡æœ¬æ·å¾„é—®é¢˜ï¼Œå³æ­£ç¡®ç­”æ¡ˆå¯èƒ½ä»…æ¥è‡ªé—®é¢˜æ–‡æœ¬è€Œæ— éœ€è§‚çœ‹è§†é¢‘ã€‚</li>
<li>AVUTé€šè¿‡ç­”æ¡ˆç½®æ¢è¿‡æ»¤æœºåˆ¶æ¥è§£å†³æ–‡æœ¬æ·å¾„é—®é¢˜ï¼Œç¡®ä¿è¯„ä¼°çš„å…¬æ­£æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19951">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2503.19951v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2503.19951v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2503.19951v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_LLM/2503.19951v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-11/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-11/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-11/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_Agent/2508.20368v3/page_0_0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-11  AgentSentinel An End-to-End and Real-Time Security Defense Framework   for Computer-Use Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-11/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-11\./crop_R1_Reasoning/2509.07957v1/page_5_1.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-11  Parallel-R1 Towards Parallel Thinking via Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30055.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
