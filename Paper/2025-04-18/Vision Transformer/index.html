<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-04-18  Comparative Evaluation of Radiomics and Deep Learning Models for Disease   Detection in Chest Radiography">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-2bf5b49007c9a7fdf3dd56b63f6107ca.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    27 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-18-更新"><a href="#2025-04-18-更新" class="headerlink" title="2025-04-18 更新"></a>2025-04-18 更新</h1><h2 id="Comparative-Evaluation-of-Radiomics-and-Deep-Learning-Models-for-Disease-Detection-in-Chest-Radiography"><a href="#Comparative-Evaluation-of-Radiomics-and-Deep-Learning-Models-for-Disease-Detection-in-Chest-Radiography" class="headerlink" title="Comparative Evaluation of Radiomics and Deep Learning Models for Disease   Detection in Chest Radiography"></a>Comparative Evaluation of Radiomics and Deep Learning Models for Disease   Detection in Chest Radiography</h2><p><strong>Authors:Zhijin He, Alan B. McMillan</strong></p>
<p>The application of artificial intelligence (AI) in medical imaging has revolutionized diagnostic practices, enabling advanced analysis and interpretation of radiological data. This study presents a comprehensive evaluation of radiomics-based and deep learning-based approaches for disease detection in chest radiography, focusing on COVID-19, lung opacity, and viral pneumonia. While deep learning models, particularly convolutional neural networks (CNNs) and vision transformers (ViTs), learn directly from image data, radiomics-based models extract and analyze quantitative features, potentially providing advantages in data-limited scenarios. This study systematically compares the diagnostic accuracy and robustness of various AI models, including Decision Trees, Gradient Boosting, Random Forests, Support Vector Machines (SVM), and Multi-Layer Perceptrons (MLP) for radiomics, against state-of-the-art computer vision deep learning architectures. Performance metrics across varying sample sizes reveal insights into each model’s efficacy, highlighting the contexts in which specific AI approaches may offer enhanced diagnostic capabilities. The results aim to inform the integration of AI-driven diagnostic tools in clinical practice, particularly in automated and high-throughput environments where timely, reliable diagnosis is critical. This comparative study addresses an essential gap, establishing guidance for the selection of AI models based on clinical and operational needs. </p>
<blockquote>
<p>人工智能（AI）在医学影像领域的应用已经彻底改变了诊断实践，使放射学数据的先进分析和解释成为可能。本研究全面评估了基于放射组学和基于深度学习的方法在胸部放射摄影中检测疾病的性能，重点关注COVID-19、肺部不透明和病毒性肺炎。深度学习模型，特别是卷积神经网络（CNN）和视觉变压器（ViT），直接从图像数据中学习，而基于放射组学的模型则提取并分析定量特征，在数据有限的场景中可能具有潜在优势。本研究系统地比较了各种AI模型的诊断准确性和稳健性，包括用于放射组学的决策树、梯度提升、随机森林、支持向量机（SVM）和多层感知器（MLP）与最先进的计算机视觉深度学习架构之间的比较。不同样本量下的性能指标揭示了每种模型的有效性，突出了特定AI方法可能在哪些情况下提供增强的诊断能力。研究结果旨在为AI驱动的诊断工具在临床实践中的整合提供信息，特别是在及时可靠的诊断至关重要的自动化和高通量环境中。这项比较研究解决了重要的问题，根据临床和运营需求为选择AI模型提供了指导。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12249v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>人工智能在医学影像领域的应用已经彻底改变了诊断实践，促进了对放射数据的深入分析和解读。本研究全面评估了基于放射组学和深度学习的方法在胸部放射摄影中疾病检测的表现，重点研究COVID-19、肺部不透明度和病毒性肺炎。深度学习模型，尤其是卷积神经网络和视觉转换器，直接从图像数据中学习；而基于放射组学的模型则提取和分析定量特征，可能在数据有限的场景中提供优势。本研究系统地比较了各种人工智能模型的诊断准确性和稳健性，包括决策树、梯度提升、随机森林、支持向量机和多层感知器（用于放射组学），以及最先进的计算机视觉深度学习架构。不同样本量下的性能指标揭示了每种模型的有效性，并突出了特定人工智能方法在哪些情况下可能提供增强的诊断能力。研究结果旨在为在临床实践中整合人工智能驱动的诊断工具提供信息，特别是在需要及时可靠诊断的自动化和高通量环境中。这项比较研究填补了一个重要空白，根据临床和运营需求提供了选择人工智能模型的指导。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI在医学影像领域的应用已经改变了诊断实践，促进了放射数据的深入分析。</li>
<li>本研究比较了基于放射组学和深度学习的方法在疾病检测中的表现，特别是针对COVID-19、肺部不透明度和病毒性肺炎。</li>
<li>深度学习模型，特别是卷积神经网络和视觉转换器，直接从图像数据中学习特征。</li>
<li>基于放射组学的模型在提取和分析定量特征方面可能具有优势，尤其在数据有限的场景中。</li>
<li>研究通过比较不同人工智能模型的诊断准确性和稳健性，包括多种传统机器学习方法与先进的深度学习架构。</li>
<li>不同样本量下的性能指标显示了各种模型的有效性，并指出特定AI方法在特定情境下的优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12249">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-da6ca1480f70c8211e78f91f28089bea.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DC-SAM-In-Context-Segment-Anything-in-Images-and-Videos-via-Dual-Consistency"><a href="#DC-SAM-In-Context-Segment-Anything-in-Images-and-Videos-via-Dual-Consistency" class="headerlink" title="DC-SAM: In-Context Segment Anything in Images and Videos via Dual   Consistency"></a>DC-SAM: In-Context Segment Anything in Images and Videos via Dual   Consistency</h2><p><strong>Authors:Mengshi Qi, Pengfei Zhu, Xiangtai Li, Xiaoyang Bi, Lu Qi, Huadong Ma, Ming-Hsuan Yang</strong></p>
<p>Given a single labeled example, in-context segmentation aims to segment corresponding objects. This setting, known as one-shot segmentation in few-shot learning, explores the segmentation model’s generalization ability and has been applied to various vision tasks, including scene understanding and image&#x2F;video editing. While recent Segment Anything Models have achieved state-of-the-art results in interactive segmentation, these approaches are not directly applicable to in-context segmentation. In this work, we propose the Dual Consistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2 for in-context segmentation of both images and videos. Our key insights are to enhance the features of the SAM’s prompt encoder in segmentation by providing high-quality visual prompts. When generating a mask prior, we fuse the SAM features to better align the prompt encoder. Then, we design a cycle-consistent cross-attention on fused features and initial visual prompts. Next, a dual-branch design is provided by using the discriminative positive and negative prompts in the prompt encoder. Furthermore, we design a simple mask-tube training strategy to adopt our proposed dual consistency method into the mask tube. Although the proposed DC-SAM is primarily designed for images, it can be seamlessly extended to the video domain with the support of SAM2. Given the absence of in-context segmentation in the video domain, we manually curate and construct the first benchmark from existing video segmentation datasets, named In-Context Video Object Segmentation (IC-VOS), to better assess the in-context capability of the model. Extensive experiments demonstrate that our method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on PASCAL-5i, and a J&amp;F score of 71.52 on the proposed IC-VOS benchmark. Our source code and benchmark are available at <a target="_blank" rel="noopener" href="https://github.com/zaplm/DC-SAM">https://github.com/zaplm/DC-SAM</a>. </p>
<blockquote>
<p>给定单个有标签的样本，上下文分割旨在分割对应的对象。这种设置被称为小样本学习中的一次分割，它探索了分割模型的泛化能力，并已应用于各种视觉任务，包括场景理解、图像&#x2F;视频编辑等。虽然最近的任何分割模型在交互式分割方面取得了最新结果，但这些方法并不能直接应用于上下文分割。在这项工作中，我们提出了基于提示调整的Dual Consistency SAM（DC-SAM）方法，以适应图像和视频的上下文分割。我们的关键见解是通过提供高质量的视觉提示来增强SAM提示编码器在分割中的功能。在生成掩膜先验时，我们融合了SAM特征以更好地对齐提示编码器。然后，我们在融合的特征和初始视觉提示上设计了一个循环一致的交叉注意力。接下来，通过使用提示编码器中的判别性正向和负向提示，提供了一个双分支设计。此外，我们设计了一个简单的掩膜管训练策略，将我们所提出的双重一致性方法应用于掩膜管。虽然提出的DC-SAM主要是为图像设计的，但它可以无缝地扩展到视频领域，得到SAM2的支持。由于在视频领域缺乏上下文分割，我们从现有的视频分割数据集中手动筛选和构建了第一个基准测试，名为In-Context Video Object Segmentation（IC-VOS），以更好地评估模型的上下文能力。大量实验表明，我们的方法在COCO-20i上实现了55.5（+1.4）的mIoU，在PASCAL-5i上实现了73.0（+1.1）的mIoU，在提出的IC-VOS基准测试上达到了71.52的J&amp;F分数。我们的源代码和基准测试可在<a target="_blank" rel="noopener" href="https://github.com/zaplm/DC-SAM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zaplm/DC-SAM找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12080v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于提示调整（prompt-tuning）的Dual Consistency SAM（DC-SAM）方法，用于实现基于单个示例的一键式上下文感知分割。本文首次建立了适用于视频的上下文分割评估基准IC-VOS，并展示了DC-SAM在图像和视频分割任务上的优越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章提出了基于提示调整的Dual Consistency SAM（DC-SAM）方法，用于实现基于单个示例的上下文感知分割。</li>
<li>DC-SAM方法通过增强SAM的提示编码器特征，并结合使用高质量视觉提示，生成遮罩先验。</li>
<li>文章设计了一种循环一致的跨注意力机制，用于融合特征和初始视觉提示。</li>
<li>通过采用判别性的正负提示，设计了一种双分支设计，用于提示编码器。</li>
<li>为适应所提出的双一致性方法，文章设计了一种简单的遮罩管训练策略。</li>
<li>DC-SAM方法不仅适用于图像分割，还可无缝扩展到视频领域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12080">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a3fe4a8637fa1fa8a6b7f865ef592495.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75e61f7614c517d6ec749884c2652ad3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9a754d40555c6f27055dee7a7ff1f67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70bed93a9dceaf9cee81b47c1d2a2125.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-abc5fb8550165756f74379972b01831c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7edc4f0c930ed1957e8b8f07f7d47938.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Beyond-Words-Augmenting-Discriminative-Richness-via-Diffusions-in-Unsupervised-Prompt-Learning"><a href="#Beyond-Words-Augmenting-Discriminative-Richness-via-Diffusions-in-Unsupervised-Prompt-Learning" class="headerlink" title="Beyond Words: Augmenting Discriminative Richness via Diffusions in   Unsupervised Prompt Learning"></a>Beyond Words: Augmenting Discriminative Richness via Diffusions in   Unsupervised Prompt Learning</h2><p><strong>Authors:Hairui Ren, Fan Tang, He Zhao, Zixuan Wang, Dandan Guo, Yi Chang</strong></p>
<p>Fine-tuning vision-language models (VLMs) with large amounts of unlabeled data has recently garnered significant interest. However, a key challenge remains the lack of high-quality pseudo-labeled data. Current pseudo-labeling strategies often struggle with mismatches between semantic and visual information, leading to sub-optimal performance of unsupervised prompt learning (UPL) methods. In this paper, we introduce a simple yet effective approach called \textbf{A}ugmenting D\textbf{i}scriminative \textbf{R}ichness via Diffusions (AiR), toward learning a richer discriminating way to represent the class comprehensively and thus facilitate classification. Specifically, our approach includes a pseudo-label generation module that leverages high-fidelity synthetic samples to create an auxiliary classifier, which captures richer visual variation, bridging text-image-pair classification to a more robust image-image-pair classification. Additionally, we exploit the diversity of diffusion-based synthetic samples to enhance prompt learning, providing greater information for semantic-visual alignment. Extensive experiments on five public benchmarks, including RESISC45 and Flowers102, and across three learning paradigms-UL, SSL, and TRZSL-demonstrate that AiR achieves substantial and consistent performance improvements over state-of-the-art unsupervised prompt learning methods. </p>
<blockquote>
<p>对大量无标签数据进行视觉语言模型（VLMs）的微调最近引起了极大的兴趣。然而，缺乏高质量的伪标签数据仍然是关键挑战。当前的伪标签策略通常面临语义和视觉信息之间的不匹配问题，导致无监督提示学习（UPL）方法性能不佳。在本文中，我们介绍了一种简单有效的方法，称为通过扩散增强辨别丰富性（AiR），旨在学习一种更丰富的辨别方式，以全面代表类别，从而促进分类。具体来说，我们的方法包括一个伪标签生成模块，该模块利用高保真合成样本创建一个辅助分类器，以捕捉更丰富的视觉变化，将文本图像对分类转变为更稳健的图像图像对分类。此外，我们利用基于扩散的合成样本的多样性来增强提示学习，为语义视觉对齐提供更多信息。在五个公共基准测试（包括RESISC45和Flowers102）以及三种学习模式（UL、SSL和TRZSL）上的大量实验表明，与最先进的无监督提示学习方法相比，AiR实现了显著且一致的性能改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11930v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种简单有效的方法——AiR（通过扩散增强判别丰富性），用于解决大规模视觉语言模型的无监督提示学习面临的挑战。通过使用高保真合成样本生成伪标签，AiR能捕捉更丰富多样的视觉特征，从而将文本图像配对分类提升为更稳健的图像图像配对分类。同时，利用基于扩散的合成样本多样性增强提示学习，促进语义与视觉的对齐。在五个公共基准测试上的实验结果表明，AiR在多种学习模式下均实现了显著且持续的性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出了一种名为AiR的新方法，旨在解决大规模视觉语言模型无监督提示学习中的挑战。</li>
<li>利用高保真合成样本生成伪标签，创建了辅助分类器以捕捉更丰富多样的视觉特征。</li>
<li>通过将文本图像配对分类转化为图像图像配对分类，提高了模型的稳健性。</li>
<li>利用基于扩散的合成样本多样性增强提示学习，促进语义与视觉的对齐。</li>
<li>在五个公共基准测试上的实验结果表明AiR性能优越。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11930">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4e97d7359665a583d84217ebb27f3dae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e4a5dabc4123785a4d5e5279ead4f5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5338ae4367e436c5e8c9130e61bf6dc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Zooming-In-on-Fakes-A-Novel-Dataset-for-Localized-AI-Generated-Image-Detection-with-Forgery-Amplification-Approach"><a href="#Zooming-In-on-Fakes-A-Novel-Dataset-for-Localized-AI-Generated-Image-Detection-with-Forgery-Amplification-Approach" class="headerlink" title="Zooming In on Fakes: A Novel Dataset for Localized AI-Generated Image   Detection with Forgery Amplification Approach"></a>Zooming In on Fakes: A Novel Dataset for Localized AI-Generated Image   Detection with Forgery Amplification Approach</h2><p><strong>Authors:Lvpan Cai, Haowei Wang, Jiayi Ji, YanShu ZhouMen, Yiwei Ma, Xiaoshuai Sun, Liujuan Cao, Rongrong Ji</strong></p>
<p>The rise of AI-generated image editing tools has made localized forgeries increasingly realistic, posing challenges for visual content integrity. Although recent efforts have explored localized AIGC detection, existing datasets predominantly focus on object-level forgeries while overlooking broader scene edits in regions such as sky or ground. To address these limitations, we introduce \textbf{BR-Gen}, a large-scale dataset of 150,000 locally forged images with diverse scene-aware annotations, which are based on semantic calibration to ensure high-quality samples. BR-Gen is constructed through a fully automated Perception-Creation-Evaluation pipeline to ensure semantic coherence and visual realism. In addition, we further propose \textbf{NFA-ViT}, a Noise-guided Forgery Amplification Vision Transformer that enhances the detection of localized forgeries by amplifying forgery-related features across the entire image. NFA-ViT mines heterogeneous regions in images, \emph{i.e.}, potential edited areas, by noise fingerprints. Subsequently, attention mechanism is introduced to compel the interaction between normal and abnormal features, thereby propagating the generalization traces throughout the entire image, allowing subtle forgeries to influence a broader context and improving overall detection robustness. Extensive experiments demonstrate that BR-Gen constructs entirely new scenarios that are not covered by existing methods. Take a step further, NFA-ViT outperforms existing methods on BR-Gen and generalizes well across current benchmarks. All data and codes are available at <a target="_blank" rel="noopener" href="https://github.com/clpbc/BR-Gen">https://github.com/clpbc/BR-Gen</a>. </p>
<blockquote>
<p>随着人工智能生成的图像编辑工具的普及，局部伪造图像的真实性越来越高，这给视觉内容完整性带来了挑战。尽管最近的研究已经探索了局部AIGC检测，但现有数据集主要集中在对象级别的伪造上，而忽略了天空或地面等区域的更广泛的场景编辑。为了解决这些局限性，我们引入了<strong>BR-Gen</strong>，这是一个包含15万张局部伪造图像的大规模数据集，具有基于语义校准的多样化场景感知注释，以确保高质量样本。BR-Gen通过全自动的感知-创建-评估流程构建，以确保语义连贯和视觉真实性。此外，我们进一步提出了<strong>NFA-ViT</strong>，这是一种噪声引导的伪造放大视觉转换器，通过放大整个图像中的伪造相关特征，增强局部伪造的检测。NFA-ViT通过噪声指纹挖掘图像中的异构图地区，即潜在编辑区域。随后，引入注意力机制，迫使正常和异常特征之间的交互，从而在整幅图像中传播泛化痕迹，使细微的伪造能够影响更广泛的上下文，提高检测的整体稳健性。大量实验表明，BR-Gen构建的场景完全超出了现有方法覆盖的范围。更进一步的是，NFA-ViT在BR-Gen上的表现优于现有方法，并在当前基准测试中具有良好的泛化能力。所有数据和代码都可在<a target="_blank" rel="noopener" href="https://github.com/clpbc/BR-Gen%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/clpbc/BR-Gen上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11922v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了AI生成图像编辑工具的兴起带来的局部篡改挑战。针对现有数据集主要关注对象级篡改，而忽视天空或地面等更广泛场景编辑的问题，提出了BR-Gen数据集。该数据集包含15万张局部伪造图像，具有基于语义校准的丰富场景感知注释。此外，还提出了NFA-ViT模型，通过噪声引导的伪造放大视觉转换器，增强对局部篡改的检测。该模型通过噪声指纹挖掘图像中的异质区域，引入注意力机制，迫使正常和异常特征之间的交互，提高检测稳健性。实验表明，BR-Gen数据集开创了全新场景，而NFA-ViT在BR-Gen上的表现优于现有方法，并在当前基准测试中具有良好的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI生成的图像编辑工具使得局部伪造图像越来越逼真，对视觉内容完整性构成挑战。</li>
<li>现有数据集主要关注对象级伪造，而忽视更广泛的场景编辑，如天空或地面。</li>
<li>引入BR-Gen数据集，包含15万张局部伪造图像，具有丰富场景感知注释，基于语义校准确保高质量样本。</li>
<li>提出NFA-ViT模型，通过噪声引导的伪造放大视觉转换器增强局部伪造检测。</li>
<li>NFA-ViT模型通过噪声指纹挖掘图像中的异质区域，并引入注意力机制来提高检测稳健性。</li>
<li>实验表明BR-Gen数据集具有创新性，NFA-ViT在BR-Gen上的表现优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11922">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4218c8d60a54efd07b778076b0c0ddce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4f3b990b18034e7e5069a16e847a975.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1149e5f29122b0b328ed32102afeef58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ff20f0212f1bcd8d63c0eccd9476360.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Efficient-Lung-Ultrasound-Severity-Scoring-Using-Dedicated-Feature-Extractor"><a href="#Efficient-Lung-Ultrasound-Severity-Scoring-Using-Dedicated-Feature-Extractor" class="headerlink" title="Efficient Lung Ultrasound Severity Scoring Using Dedicated Feature   Extractor"></a>Efficient Lung Ultrasound Severity Scoring Using Dedicated Feature   Extractor</h2><p><strong>Authors:Jiaqi Guo, Yunan Wu, Evangelos Kaimakamis, Georgios Petmezas, Vasileios E. Papageorgiou, Nicos Maglaveras, Aggelos K. Katsaggelos</strong></p>
<p>With the advent of the COVID-19 pandemic, ultrasound imaging has emerged as a promising technique for COVID-19 detection, due to its non-invasive nature, affordability, and portability. In response, researchers have focused on developing AI-based scoring systems to provide real-time diagnostic support. However, the limited size and lack of proper annotation in publicly available ultrasound datasets pose significant challenges for training a robust AI model. This paper proposes MeDiVLAD, a novel pipeline to address the above issue for multi-level lung-ultrasound (LUS) severity scoring. In particular, we leverage self-knowledge distillation to pretrain a vision transformer (ViT) without label and aggregate frame-level features via dual-level VLAD aggregation. We show that with minimal finetuning, MeDiVLAD outperforms conventional fully-supervised methods in both frame- and video-level scoring, while offering classification reasoning with exceptional quality. This superior performance enables key applications such as the automatic identification of critical lung pathology areas and provides a robust solution for broader medical video classification tasks. </p>
<blockquote>
<p>随着COVID-19大流行的到来，由于其无创、负担得起和便携的特点，超声成像已崭露头角为一种有前景的COVID-19检测技术。为此，研究人员致力于开发基于人工智能的评分系统，以提供实时诊断支持。然而，公开可用的超声数据集的规模有限且缺乏适当的注释，给训练稳健的人工智能模型带来了巨大挑战。本文提出了MeDiVLAD，这是一个新颖的管道流程，旨在解决上述针对多级肺超声（LUS）严重性评分的问题。特别是，我们利用自我知识蒸馏对视觉转换器（ViT）进行预训练，而无需标签并聚集帧级特征的双级VLAD聚合。我们表明，通过最小的微调，MeDiVLAD在帧级和视频级评分方面都优于传统的全监督方法，同时提供出色的分类推理质量。这种卓越的性能能够应用于自动识别关键肺部病理区域的关键应用，并为更广泛的医学视频分类任务提供了稳健的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12524v2">PDF</a> Accepted by IEEE ISBI 2025 (Selected for oral presentation) 2025&#x2F;4&#x2F;15   (v2): Corrected a notation error in Figure 2</p>
<p><strong>Summary</strong></p>
<p>随着COVID-19的爆发，超声成像因其无创性、可负担性和便携性而成为检测COVID-19的有前途的技术。研究人员致力于开发基于人工智能的评分系统以提供实时诊断支持。然而，公开可用的超声数据集的大小有限且缺乏适当的注释，给训练稳健的AI模型带来挑战。本文提出MeDiVLAD，一个针对多级肺部超声（LUS）严重性评分问题的新型管道。我们利用自我知识蒸馏对视觉转换器（ViT）进行预训练，并通过双级VLAD聚合技术聚合帧级特征。结果表明，MeDiVLAD在帧级和视频级评分上均优于传统的全监督方法，同时提供出色的分类推理质量。其卓越性能使得自动识别关键肺部病理区域成为可能，并为更广泛的医疗视频分类任务提供了稳健的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>超声成像因其在COVID-19检测中的无创性、可负担性和便携性而受到关注。</li>
<li>公开可用的超声数据集存在大小有限和缺乏适当注释的问题。</li>
<li>MeDiVLAD是一个针对多级肺部超声严重性评分的新型管道。</li>
<li>利用自我知识蒸馏对视觉转换器（ViT）进行预训练。</li>
<li>通过双级VLAD聚合技术聚合帧级特征。</li>
<li>MeDiVLAD在帧级和视频级评分上表现出卓越性能，优于传统全监督方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12524">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e61f801d25e2a7c323d62f350a3cb991.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9e4aaaa220d3f51ac38f15e05ae5cf5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6cc56a18516d4bee7f51ecd5e1e9c22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5142fd1ddf9b34d7897321bf085dc1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6531beca76054e51b66f549a4ae5918e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f2cf6961262eb514e63d8f00efada832.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Kernel-Aware-Graph-Prompt-Learning-for-Few-Shot-Anomaly-Detection"><a href="#Kernel-Aware-Graph-Prompt-Learning-for-Few-Shot-Anomaly-Detection" class="headerlink" title="Kernel-Aware Graph Prompt Learning for Few-Shot Anomaly Detection"></a>Kernel-Aware Graph Prompt Learning for Few-Shot Anomaly Detection</h2><p><strong>Authors:Fenfang Tao, Guo-Sen Xie, Fang Zhao, Xiangbo Shu</strong></p>
<p>Few-shot anomaly detection (FSAD) aims to detect unseen anomaly regions with the guidance of very few normal support images from the same class. Existing FSAD methods usually find anomalies by directly designing complex text prompts to align them with visual features under the prevailing large vision-language model paradigm. However, these methods, almost always, neglect intrinsic contextual information in visual features, e.g., the interaction relationships between different vision layers, which is an important clue for detecting anomalies comprehensively. To this end, we propose a kernel-aware graph prompt learning framework, termed as KAG-prompt, by reasoning the cross-layer relations among visual features for FSAD. Specifically, a kernel-aware hierarchical graph is built by taking the different layer features focusing on anomalous regions of different sizes as nodes, meanwhile, the relationships between arbitrary pairs of nodes stand for the edges of the graph. By message passing over this graph, KAG-prompt can capture cross-layer contextual information, thus leading to more accurate anomaly prediction. Moreover, to integrate the information of multiple important anomaly signals in the prediction map, we propose a novel image-level scoring method based on multi-level information fusion. Extensive experiments on MVTecAD and VisA datasets show that KAG-prompt achieves state-of-the-art FSAD results for image-level&#x2F;pixel-level anomaly detection. Code is available at <a target="_blank" rel="noopener" href="https://github.com/CVL-hub/KAG-prompt.git">https://github.com/CVL-hub/KAG-prompt.git</a>. </p>
<blockquote>
<p>少数镜头异常检测（FSAD）旨在通过同一类别中非常少的正常支持图像来检测未见过的异常区域。现有的FSAD方法通常通过设计复杂的文本提示来与流行的大型视觉语言模型范式下的视觉特征对齐来发现异常值。然而，这些方法几乎总是忽略了视觉特征中的内在上下文信息，例如不同视觉层之间的交互关系，这是全面检测异常值的重要线索。为此，我们提出了一个核心感知图提示学习框架，称为KAG-prompt，通过推理FSAD中视觉特征的跨层关系。具体来说，以不同层特征（以不同大小的异常区域作为节点）为基础构建了一个核心感知分层图，同时，任意节点对之间的关系代表图的边。通过在此图上进行消息传递，KAG-prompt可以捕获跨层上下文信息，从而进行更准确的异常预测。此外，为了整合预测图中多个重要异常信号的信息，我们提出了一种基于多层次信息融合的新颖图像级评分方法。在MVTecAD和VisA数据集上的大量实验表明，KAG-prompt在图像级&#x2F;像素级的异常检测中达到了最新的FSAD结果。代码可通过以下链接获取：<a target="_blank" rel="noopener" href="https://github.com/CVL-hub/KAG-prompt.git">https://github.com/CVL-hub/KAG-prompt.git</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17619v2">PDF</a> Accepted to AAAI 2025</p>
<p><strong>Summary</strong><br>少量样本异常检测（FSAD）旨在利用同一类别中的少量正常支持图像来检测未见过的异常区域。现有FSAD方法主要通过设计复杂的文本提示来与视觉特征对齐，忽略视觉特征的内在上下文信息，如不同视觉层之间的关系。为此，我们提出了一个核心感知图提示学习框架，称为KAG-prompt，通过推理视觉特征的跨层关系来进行FSAD。该框架建立了一个核心感知层次图，以不同层级的特征（重点关注不同大小的异常区域）作为节点，任意两个节点之间的关系作为图的边。通过图上的信息传递，KAG-prompt可以捕捉跨层上下文信息，从而实现更准确的异常预测。此外，我们提出了一种基于多层级信息融合的新颖图像级评分方法，以整合预测图中的多个重要异常信号信息。在MVTecAD和VisA数据集上的实验表明，KAG-prompt在图像级和像素级的异常检测中实现了最先进的FSAD结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有FSAD方法主要通过复杂文本提示与视觉特征对齐来检测异常，忽略了视觉特征的内在上下文信息。</li>
<li>提出了一种新的学习框架KAG-prompt，通过构建核心感知层次图来捕捉视觉特征的跨层关系。</li>
<li>异常区域的不同尺寸被作为不同层级的特征节点，而任意两个节点间的关系构成了图的边。</li>
<li>通过在图上传递消息，KAG-prompt能更准确地预测异常。</li>
<li>引入了一种基于多层级信息融合的新颖图像级评分方法，以整合预测图中的多个重要异常信号。</li>
<li>在MVTecAD和VisA数据集上的实验表明，KAG-prompt实现了先进的FSAD结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17619">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2bf5b49007c9a7fdf3dd56b63f6107ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19523772394d7833b31f5c466946e64f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da0b28b54c0b8692f44b391df54435db.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-18/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-18/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-18/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-5ef269ba90aae3b9aeb1321931c03455.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-04-18  RADLER Radar Object Detection Leveraging Semantic 3D City Models and   Self-Supervised Radar-Image Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-18/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ca1f712f6ee96a259fc29c0453b3053f.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-04-18  Flow Intelligence Robust Feature Matching via Temporal Signature   Correlation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">22950.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
