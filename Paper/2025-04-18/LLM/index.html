<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-18  HLS-Eval A Benchmark and Framework for Evaluating LLMs on High-Level   Synthesis Design Tasks">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f50f71bd991c3c5db24d14612a62d5ec.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-18-æ›´æ–°"><a href="#2025-04-18-æ›´æ–°" class="headerlink" title="2025-04-18 æ›´æ–°"></a>2025-04-18 æ›´æ–°</h1><h2 id="HLS-Eval-A-Benchmark-and-Framework-for-Evaluating-LLMs-on-High-Level-Synthesis-Design-Tasks"><a href="#HLS-Eval-A-Benchmark-and-Framework-for-Evaluating-LLMs-on-High-Level-Synthesis-Design-Tasks" class="headerlink" title="HLS-Eval: A Benchmark and Framework for Evaluating LLMs on High-Level   Synthesis Design Tasks"></a>HLS-Eval: A Benchmark and Framework for Evaluating LLMs on High-Level   Synthesis Design Tasks</h2><p><strong>Authors:Stefan Abi-Karam, Cong Hao</strong></p>
<p>The rapid scaling of large language model (LLM) training and inference has driven their adoption in semiconductor design across academia and industry. While most prior work evaluates LLMs on hardware description language (HDL) tasks, particularly Verilog, designers are increasingly using high-level synthesis (HLS) to build domain-specific accelerators and complex hardware systems. However, benchmarks and tooling to comprehensively evaluate LLMs for HLS design tasks remain scarce.   To address this, we introduce HLS-Eval, the first complete benchmark and evaluation framework for LLM-driven HLS design. HLS-Eval targets two core tasks: (1) generating HLS code from natural language descriptions, and (2) performing HLS-specific code edits to optimize performance and hardware efficiency. The benchmark includes 94 unique designs drawn from standard HLS benchmarks and novel sources. Each case is prepared via a semi-automated flow that produces a natural language description and a paired testbench for C-simulation and synthesis validation, ensuring each task is â€œLLM-ready.â€   Beyond the benchmark, HLS-Eval offers a modular Python framework for automated, parallel evaluation of both local and hosted LLMs. It includes a parallel evaluation engine, direct HLS tool integration, and abstractions for to support different LLM interaction paradigms, enabling rapid prototyping of new benchmarks, tasks, and LLM methods.   We demonstrate HLS-Eval through baseline evaluations of open-source LLMs on Vitis HLS, measuring outputs across four key metrics - parseability, compilability, runnability, and synthesizability - reflecting the iterative HLS design cycle. We also report pass@k metrics, establishing clear baselines and reusable infrastructure for the broader LLM-for-hardware community.   All benchmarks, framework code, and results are open-sourced at <a target="_blank" rel="noopener" href="https://github.com/stefanpie/hls-eval">https://github.com/stefanpie/hls-eval</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒå’Œæ¨ç†çš„è¿…é€Ÿæ‰©å±•ï¼Œæ¨åŠ¨äº†å…¶åœ¨å­¦æœ¯ç•Œå’Œå·¥ä¸šç•ŒåŠå¯¼ä½“è®¾è®¡é¢†åŸŸçš„é‡‡ç”¨ã€‚è™½ç„¶å¤§å¤šæ•°ä»¥å‰çš„å·¥ä½œéƒ½åœ¨ç¡¬ä»¶æè¿°è¯­è¨€ï¼ˆHDLï¼‰ä»»åŠ¡ä¸Šè¯„ä¼°LLMï¼Œå°¤å…¶æ˜¯Verilogï¼Œä½†è®¾è®¡å¸ˆè¶Šæ¥è¶Šå€¾å‘äºä½¿ç”¨é«˜çº§ç»¼åˆï¼ˆHLSï¼‰æ¥æ„å»ºç‰¹å®šé¢†åŸŸçš„åŠ é€Ÿå™¨å’Œå¤æ‚çš„ç¡¬ä»¶ç³»ç»Ÿã€‚ç„¶è€Œï¼Œé’ˆå¯¹HLSè®¾è®¡ä»»åŠ¡çš„LLMè¿›è¡Œå…¨é¢è¯„ä¼°çš„åŸºå‡†æµ‹è¯•å’Œå·¥å…·ä»ç„¶ç¨€ç¼ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†HLS-Evalï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹LLMé©±åŠ¨çš„HLSè®¾è®¡çš„å®Œæ•´åŸºå‡†æµ‹è¯•å’Œè¯„ä»·æ¡†æ¶ã€‚HLS-Evalç„å‡†ä¸¤ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼šï¼ˆ1ï¼‰æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°ç”ŸæˆHLSä»£ç ï¼Œï¼ˆ2ï¼‰æ‰§è¡ŒHLSç‰¹å®šçš„ä»£ç ç¼–è¾‘ï¼Œä»¥ä¼˜åŒ–æ€§èƒ½å’Œç¡¬ä»¶æ•ˆç‡ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬æ¥è‡ªæ ‡å‡†HLSåŸºå‡†æµ‹è¯•å’Œæ–°é¢–æ¥æºçš„94ä¸ªç‹¬ç‰¹è®¾è®¡ã€‚æ¯ä¸ªæ¡ˆä¾‹éƒ½é€šè¿‡åŠè‡ªåŠ¨åŒ–æµç¨‹è¿›è¡Œå‡†å¤‡ï¼Œè¯¥æµç¨‹äº§ç”Ÿè‡ªç„¶è¯­è¨€æè¿°å’Œé…å¥—æµ‹è¯•å¹³å°ï¼Œç”¨äºCä»¿çœŸå’Œç»¼åˆéªŒè¯ï¼Œç¡®ä¿æ¯ä¸ªä»»åŠ¡éƒ½é€‚åˆLLMã€‚é™¤äº†åŸºå‡†æµ‹è¯•å¤–ï¼ŒHLS-Evalè¿˜æä¾›äº†ä¸€ä¸ªæ¨¡å—åŒ–çš„Pythonæ¡†æ¶ï¼Œç”¨äºå¯¹æœ¬åœ°å’Œæ‰˜ç®¡LLMè¿›è¡Œè‡ªåŠ¨åŒ–å¹¶è¡Œè¯„ä¼°ã€‚å®ƒåŒ…æ‹¬ä¸€ä¸ªå¹¶è¡Œè¯„ä¼°å¼•æ“ã€ç›´æ¥HLSå·¥å…·é›†æˆï¼Œä»¥åŠæ”¯æŒä¸åŒLLMäº¤äº’èŒƒå¼çš„æŠ½è±¡ï¼Œèƒ½å¤Ÿè¿…é€ŸåŸå‹åŒ–æ–°çš„åŸºå‡†æµ‹è¯•ã€ä»»åŠ¡å’Œæ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡Vitis HLSçš„å¼€æºLLMåŸºçº¿è¯„ä¼°æ¥å±•ç¤ºHLS-Evalï¼Œæµ‹é‡å››ä¸ªå…³é”®æŒ‡æ ‡â€”â€”è§£ææ€§ã€å¯ç¼–è¯‘æ€§ã€è¿è¡Œæ€§å’Œå¯åˆæˆæ€§ï¼Œåæ˜ è¿­ä»£HLSè®¾è®¡å‘¨æœŸã€‚æˆ‘ä»¬è¿˜æŠ¥å‘Šäº†pass@kæŒ‡æ ‡ï¼Œä¸ºæ›´å¹¿æ³›çš„LLMç¡¬ä»¶ç¤¾åŒºå»ºç«‹äº†æ˜ç¡®çš„åŸºå‡†å’Œå¯é‡å¤ä½¿ç”¨çš„æ¶æ„ã€‚æ‰€æœ‰åŸºå‡†æµ‹è¯•ã€æ¡†æ¶ä»£ç å’Œç»“æœå‡å·²å¼€æºï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/stefanpie/hls-eval%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/stefanpie/hls-evalæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12268v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŠå¯¼ä½“è®¾è®¡é¢†åŸŸçš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜å±‚æ¬¡çš„åˆæˆï¼ˆHLSï¼‰è®¾è®¡ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œé’ˆå¯¹LLMåœ¨HLSè®¾è®¡ä»»åŠ¡çš„å…¨é¢è¯„ä¼°çš„åŸºå‡†æµ‹è¯•å’Œå·¥å…·ä»ç„¶ç¨€ç¼ºã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†HLS-Evalï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹LLMé©±åŠ¨çš„HLSè®¾è®¡çš„å®Œæ•´åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶ã€‚å®ƒä¸»è¦é’ˆå¯¹ä¸¤ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼š1ï¼‰æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°ç”ŸæˆHLSä»£ç ï¼›2ï¼‰æ‰§è¡ŒHLSç‰¹å®šçš„ä»£ç ä¼˜åŒ–ä»¥æé«˜æ€§èƒ½å’Œç¡¬ä»¶æ•ˆç‡ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬94ä¸ªç‹¬ç‰¹çš„è®¾è®¡ï¼Œæ¯ä¸ªè®¾è®¡éƒ½ç»è¿‡åŠè‡ªåŠ¨åŒ–æµç¨‹å‡†å¤‡ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€æè¿°å’Œé…å¥—æµ‹è¯•å¹³å°ï¼Œä»¥ç¡®ä¿æ¯ä¸ªä»»åŠ¡éƒ½é€‚åˆLLMã€‚æ­¤å¤–ï¼ŒHLS-Evalè¿˜æä¾›æ¨¡å—åŒ–Pythonæ¡†æ¶ï¼Œå¯è‡ªåŠ¨å¹¶è¡Œè¯„ä¼°æœ¬åœ°å’Œæ‰˜ç®¡LLMã€‚é€šè¿‡å…¬å¼€æºä»£ç å’ŒåŸºçº¿è¯„ä¼°ç»“æœï¼Œæˆ‘ä»¬ä¸ºæ›´å¹¿æ³›çš„LLMç¡¬ä»¶ç¤¾åŒºå»ºç«‹äº†æ¸…æ™°çš„åŸºå‡†çº¿å’Œå¯é‡å¤ä½¿ç”¨çš„åŸºç¡€è®¾æ–½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨åŠå¯¼ä½“è®¾è®¡é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨HLSè®¾è®¡ä»»åŠ¡ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ã€‚</li>
<li>ç›®å‰é’ˆå¯¹LLMåœ¨HLSè®¾è®¡ä»»åŠ¡çš„å…¨é¢è¯„ä¼°çš„åŸºå‡†æµ‹è¯•å’Œå·¥å…·ä»ç„¶ç¨€ç¼ºã€‚</li>
<li>HLS-Evalæ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹LLMé©±åŠ¨çš„HLSè®¾è®¡çš„åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶ã€‚</li>
<li>HLS-Evalä¸»è¦é’ˆå¯¹ç”ŸæˆHLSä»£ç å’Œæ‰§è¡ŒHLSä»£ç ä¼˜åŒ–ä¸¤ä¸ªæ ¸å¿ƒä»»åŠ¡ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬94ä¸ªç‹¬ç‰¹çš„è®¾è®¡ï¼Œæ¶µç›–è‡ªç„¶è¯­è¨€æè¿°å’Œé…å¥—æµ‹è¯•å¹³å°ã€‚</li>
<li>HLS-Evalæä¾›æ¨¡å—åŒ–Pythonæ¡†æ¶ï¼Œå¯è‡ªåŠ¨å¹¶è¡Œè¯„ä¼°LLMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12268">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f5ba7c1033c26a944a745feb1d0249ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-477cde549435bce448e93ca0606104c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b17f4c078efb2d9d2582e847b4753017.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ec66c86f1957cfead95a2afe7c1b872.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27a117b0baf62aaae40aca4a71d62f25.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FLIP-Reasoning-Challenge"><a href="#FLIP-Reasoning-Challenge" class="headerlink" title="FLIP Reasoning Challenge"></a>FLIP Reasoning Challenge</h2><p><strong>Authors:Andreas Plesner, Turlan Kuzhagaliyev, Roger Wattenhofer</strong></p>
<p>Over the past years, advances in artificial intelligence (AI) have demonstrated how AI can solve many perception and generation tasks, such as image classification and text writing, yet reasoning remains a challenge. This paper introduces the FLIP dataset, a benchmark for evaluating AI reasoning capabilities based on human verification tasks on the Idena blockchain. FLIP challenges present users with two orderings of 4 images, requiring them to identify the logically coherent one. By emphasizing sequential reasoning, visual storytelling, and common sense, FLIP provides a unique testbed for multimodal AI systems. Our experiments evaluate state-of-the-art models, leveraging both vision-language models (VLMs) and large language models (LLMs). Results reveal that even the best open-sourced and closed-sourced models achieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot settings, compared to human performance of 95.3%. Captioning models aid reasoning models by providing text descriptions of images, yielding better results than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5 Pro. Combining the predictions from 15 models in an ensemble increases the accuracy to 85.2%. These findings highlight the limitations of existing reasoning models and the need for robust multimodal benchmarks like FLIP. The full codebase and dataset will be available at <a target="_blank" rel="noopener" href="https://github.com/aplesner/FLIP-Reasoning-Challenge">https://github.com/aplesner/FLIP-Reasoning-Challenge</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„è¿›å±•å·²ç»è¯æ˜AIå¦‚ä½•è§£å†³è®¸å¤šæ„ŸçŸ¥å’Œç”Ÿæˆä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒåˆ†ç±»å’Œæ–‡æœ¬å†™ä½œï¼Œä½†æ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†FLIPæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºIdenaåŒºå—é“¾ä¸Šçš„äººç±»éªŒè¯ä»»åŠ¡çš„AIæ¨ç†èƒ½åŠ›è¯„ä¼°çš„åŸºå‡†æµ‹è¯•ã€‚FLIPæŒ‘æˆ˜å‘ç”¨æˆ·å‘ˆç°ä¸¤ä¸ªåŒ…å«4å¼ å›¾ç‰‡çš„é¡ºåºï¼Œè¦æ±‚ä»–ä»¬è¯†åˆ«å‡ºé€»è¾‘è¿è´¯çš„é‚£ä¸€ä¸ªã€‚é€šè¿‡å¼ºè°ƒé¡ºåºæ¨ç†ã€è§†è§‰å™äº‹å’Œå¸¸è¯†ï¼ŒFLIPä¸ºå¤šæ¨¡å¼AIç³»ç»Ÿæä¾›äº†ç‹¬ç‰¹çš„æµ‹è¯•å¹³å°ã€‚æˆ‘ä»¬çš„å®éªŒè¯„ä¼°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ¨¡å‹ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ï¼Œæœ€å¥½çš„å¼€æºå’Œé—­æºæ¨¡å‹åˆ†åˆ«è¾¾åˆ°äº†75.5%å’Œ77.9%çš„æœ€é«˜å‡†ç¡®ç‡ï¼Œè€Œäººç±»çš„è¡¨ç°åˆ™ä¸º95.3%ã€‚æè¿°æ¨¡å‹é€šè¿‡æä¾›å›¾åƒçš„æ–‡å­—æè¿°æ¥è¾…åŠ©æ¨ç†æ¨¡å‹ï¼Œæ¯”ç›´æ¥ä½¿ç”¨åŸå§‹å›¾åƒè·å¾—æ›´å¥½çš„ç»“æœï¼ŒGemini 1.5 Proçš„å‡†ç¡®ç‡ä»69.6%æé«˜åˆ°75.2%ã€‚é€šè¿‡ç»“åˆ15ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œé›†æˆæ–¹æ³•çš„å‡†ç¡®ç‡æé«˜åˆ°85.2%ã€‚è¿™äº›å‘ç°çªæ˜¾äº†ç°æœ‰æ¨ç†æ¨¡å‹çš„å±€é™æ€§ä»¥åŠå¯¹åƒFLIPè¿™æ ·çš„ç¨³å¥å¤šæ¨¡å¼åŸºå‡†æµ‹è¯•çš„éœ€æ±‚ã€‚å®Œæ•´çš„ä»£ç åº“å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/aplesner/FLIP-Reasoning-Challenge%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/aplesner/FLIP-Reasoning-Challengeä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12256v1">PDF</a> Published at First Workshop on Open Science for Foundation Models at   ICLR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºäººç±»éªŒè¯ä»»åŠ¡åœ¨IdenaåŒºå—é“¾ä¸Šçš„FLIPæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ—¨åœ¨è¯„ä¼°äººå·¥æ™ºèƒ½æ¨ç†èƒ½åŠ›ã€‚FLIPæŒ‘æˆ˜è¦æ±‚ç”¨æˆ·è¯†åˆ«ä¸¤ä¸ªé€»è¾‘è¿è´¯çš„å›¾åƒåºåˆ—ä¸­çš„å“ªä¸€ä¸ªæ›´åˆç†ã€‚é€šè¿‡å¼ºè°ƒé¡ºåºæ¨ç†ã€è§†è§‰å™äº‹å’Œå¸¸è¯†ï¼ŒFLIPä¸ºå¤šæ¨¡æ€AIç³»ç»Ÿæä¾›äº†ç‹¬ç‰¹çš„æµ‹è¯•å¹³å°ã€‚å®éªŒè¯„ä¼°äº†æœ€å…ˆè¿›çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬è§†è§‰è¯­è¨€æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹ã€‚ç»“æœæ˜¾ç¤ºï¼Œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­ï¼Œå³ä½¿æœ€å¥½çš„å¼€æºå’Œé—­æºæ¨¡å‹ä¹Ÿåªèƒ½è¾¾åˆ°æœ€é«˜çº¦77.9%çš„å‡†ç¡®æ€§ï¼Œä¸äººç±»è¡¨ç°å­˜åœ¨å·®è·ï¼ˆäººç±»è¡¨ç°çº¦ä¸º95.3%ï¼‰ã€‚æä¾›å›¾åƒæ–‡æœ¬æè¿°çš„æ¨¡å‹æœ‰åŠ©äºæé«˜æ¨ç†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚ç»“åˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œå‡†ç¡®æ€§å¯è¾¾çº¦85.2%ã€‚è¿™çªæ˜¾äº†ç°æœ‰æ¨ç†æ¨¡å‹çš„å±€é™æ€§ä»¥åŠå¯¹åƒFLIPè¿™æ ·çš„ç¨³å¥å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•çš„éœ€æ±‚ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>FLIPæ•°æ®é›†æ˜¯ä¸€ä¸ªåŸºäºäººç±»éªŒè¯ä»»åŠ¡çš„AIæ¨ç†èƒ½åŠ›åŸºå‡†æµ‹è¯•ï¼Œå¼ºè°ƒé¡ºåºæ¨ç†ã€è§†è§‰å™äº‹å’Œå¸¸è¯†ã€‚</li>
<li>æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨FLIPæ•°æ®é›†ä¸Šçš„è¡¨ç°ä»æœ‰é™ï¼Œä¸äººç±»è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</li>
<li>æä¾›å›¾åƒæ–‡æœ¬æè¿°çš„æ¨¡å‹æœ‰åŠ©äºæé«˜æ¨ç†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç»“åˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœå¯ä»¥æé«˜å‡†ç¡®æ€§ã€‚</li>
<li>FLIPæ•°æ®é›†çªå‡ºäº†ç°æœ‰æ¨ç†æ¨¡å‹çš„å±€é™æ€§ã€‚</li>
<li>éœ€è¦æ›´å¤šåƒFLIPè¿™æ ·çš„ç¨³å¥å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°AIçš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e69aa38ff7a9af0ca251c44862d5970.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd312e55b8b7c7145cfc05f178c34e4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81853c59a244fce86c14ebec605a1ddd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="AnomalyGen-An-Automated-Semantic-Log-Sequence-Generation-Framework-with-LLM-for-Anomaly-Detection"><a href="#AnomalyGen-An-Automated-Semantic-Log-Sequence-Generation-Framework-with-LLM-for-Anomaly-Detection" class="headerlink" title="AnomalyGen: An Automated Semantic Log Sequence Generation Framework with   LLM for Anomaly Detection"></a>AnomalyGen: An Automated Semantic Log Sequence Generation Framework with   LLM for Anomaly Detection</h2><p><strong>Authors:Xinyu Li, Yingtong Huo, Chenxi Mao, Shiwen Shan, Yuxin Su, Dan Li, Zibin Zheng</strong></p>
<p>The scarcity of high-quality public log datasets has become a critical bottleneck in advancing log-based anomaly detection techniques. Current datasets exhibit three fundamental limitations: (1) incomplete event coverage, (2) artificial patterns introduced by static analysis-based generation frameworks, and (3) insufficient semantic awareness. To address these challenges, we present AnomalyGen, the first automated log synthesis framework specifically designed for anomaly detection. Our framework introduces a novel four-phase architecture that integrates enhanced program analysis with Chain-of-Thought reasoning (CoT reasoning), enabling iterative log generation and anomaly annotation without requiring physical system execution. Evaluations on Hadoop and HDFS distributed systems demonstrate that AnomalyGen achieves substantially broader log event coverage (38-95 times improvement over existing datasets) while producing more operationally realistic log sequences compared to static analysis-based approaches. When augmenting benchmark datasets with synthesized logs, we observe maximum F1-score improvements of 3.7% (average 1.8% improvement across three state-of-the-art anomaly detection models). This work not only establishes a high-quality benchmarking resource for automated log analysis but also pioneers a new paradigm for applying large language models (LLMs) in software engineering workflows. </p>
<blockquote>
<p>é«˜è´¨é‡å…¬å…±æ—¥å¿—æ•°æ®é›†çš„ç¨€ç¼ºå·²æˆä¸ºæ¨è¿›åŸºäºæ—¥å¿—çš„å¼‚å¸¸æ£€æµ‹æŠ€æœ¯å‘å±•çš„ä¸€ä¸ªé‡è¦ç“¶é¢ˆã€‚å½“å‰çš„æ•°æ®é›†å­˜åœ¨ä¸‰ä¸ªåŸºæœ¬å±€é™ï¼šï¼ˆ1ï¼‰äº‹ä»¶è¦†ç›–ä¸å®Œæ•´ï¼Œï¼ˆ2ï¼‰ç”±é™æ€åˆ†æç”Ÿæˆæ¡†æ¶å¼•å…¥çš„äººå·¥æ¨¡å¼ï¼Œï¼ˆ3ï¼‰è¯­ä¹‰æ„è¯†ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AnomalyGenï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºå¼‚å¸¸æ£€æµ‹è®¾è®¡çš„ç¬¬ä¸€ä¸ªè‡ªåŠ¨åŒ–æ—¥å¿—åˆæˆæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„å››é˜¶æ®µæ¶æ„ï¼Œè¯¥æ¶æ„å°†å¢å¼ºçš„ç¨‹åºåˆ†æä¸é“¾å¼æ€ç»´æ¨ç†ï¼ˆCoTæ¨ç†ï¼‰ç›¸ç»“åˆï¼Œèƒ½å¤Ÿåœ¨æ— éœ€æ‰§è¡Œå®é™…ç³»ç»Ÿçš„æƒ…å†µä¸‹å®ç°è¿­ä»£æ—¥å¿—ç”Ÿæˆå’Œå¼‚å¸¸æ ‡æ³¨ã€‚åœ¨Hadoopå’ŒHDFSåˆ†å¸ƒå¼ç³»ç»Ÿä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒAnomalyGenå®ç°äº†æ›´å¹¿æ³›çš„æ—¥å¿—äº‹ä»¶è¦†ç›–ï¼ˆç›¸å¯¹äºç°æœ‰æ•°æ®é›†ï¼Œæ”¹è¿›äº†38-95å€ï¼‰ï¼ŒåŒæ—¶ä¸åŸºäºé™æ€åˆ†æçš„æ–¹æ³•ç›¸æ¯”ï¼Œç”Ÿæˆäº†æ›´è´´è¿‘å®é™…æ“ä½œçš„æ—¥å¿—åºåˆ—ã€‚å½“ä½¿ç”¨åˆæˆæ—¥å¿—å¢å¼ºåŸºå‡†æ•°æ®é›†æ—¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æœ€å¤§F1åˆ†æ•°æé«˜äº†3.7%ï¼ˆåœ¨ä¸‰ä¸ªæœ€å…ˆè¿›çš„å¼‚å¸¸æ£€æµ‹æ¨¡å‹ä¸Šå¹³å‡æé«˜äº†1.8%ï¼‰ã€‚è¿™é¡¹å·¥ä½œä¸ä»…ä¸ºè‡ªåŠ¨æ—¥å¿—åˆ†æå»ºç«‹äº†é«˜è´¨é‡åŸºå‡†èµ„æºï¼Œè¿˜å¼€åˆ›äº†å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨äºè½¯ä»¶å·¥ç¨‹å·¥ä½œæµç¨‹çš„æ–°èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12250v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é¢å‘æ—¥å¿—å¼‚å¸¸æ£€æµ‹çš„è‡ªåŠ¨ç”Ÿæˆæ¡†æ¶AnomalyGenè¢«æå‡ºä»¥è§£å†³é«˜è´¨é‡å…¬å¼€æ—¥å¿—æ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰æ•°æ®é›†å­˜åœ¨çš„ä¸‰å¤§å±€é™æ€§ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨æ–°å‹å››é˜¶æ®µæ¶æ„ï¼Œèåˆäº†å¢å¼ºç¨‹åºåˆ†æä¸Chain-of-Thoughtæ¨ç†æŠ€æœ¯ï¼Œå®ç°äº†æ— éœ€ç‰©ç†ç³»ç»Ÿæ‰§è¡Œçš„è¿­ä»£æ—¥å¿—ç”Ÿæˆä¸å¼‚å¸¸æ ‡æ³¨ã€‚åœ¨Hadoopå’ŒHDFSåˆ†å¸ƒå¼ç³»ç»Ÿä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒAnomalyGenå®ç°äº†å¹¿æ³›çš„æ—¥å¿—äº‹ä»¶è¦†ç›–åº¦æå‡ï¼ˆç›¸è¾ƒäºç°æœ‰æ•°æ®é›†æå‡38è‡³95å€ï¼‰ï¼Œå¹¶èƒ½ç”Ÿæˆæ›´è´´è¿‘å®é™…æ“ä½œçš„æ—¥å¿—åºåˆ—ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶ä¸åŸºå‡†æ•°æ®é›†çš„åˆæˆæ—¥å¿—ç›¸ç»“åˆæ—¶ï¼Œæœ€å¤§F1åˆ†æ•°æé«˜äº†3.7%ï¼ˆåœ¨ä¸‰ä¸ªé¡¶å°–å¼‚å¸¸æ£€æµ‹æ¨¡å‹ä¸Šå¹³å‡æé«˜1.8%ï¼‰ã€‚è¯¥ç ”ç©¶ä¸ä»…ä¸ºè‡ªåŠ¨åŒ–æ—¥å¿—åˆ†æå»ºç«‹äº†é«˜è´¨é‡åŸºå‡†èµ„æºï¼Œè¿˜å¼€åˆ›äº†è½¯ä»¶å·¥ç¨‹ä¸­åº”ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ—¥å¿—æ•°æ®é›†å­˜åœ¨è´¨é‡ä¸è¶³çš„é—®é¢˜ï¼Œé™åˆ¶äº†åŸºäºæ—¥å¿—çš„å¼‚å¸¸æ£€æµ‹æŠ€æœ¯è¿›æ­¥ã€‚</li>
<li>AnomalyGenæ¡†æ¶æ—¨åœ¨è§£å†³è¿™ä¸€ç“¶é¢ˆï¼Œä¸ºå¼‚å¸¸æ£€æµ‹æä¾›é«˜è´¨é‡æ—¥å¿—æ•°æ®é›†ã€‚</li>
<li>AnomalyGené‡‡ç”¨æ–°å‹å››é˜¶æ®µæ¶æ„ï¼Œèåˆäº†å¢å¼ºç¨‹åºåˆ†æä¸Chain-of-Thoughtæ¨ç†æŠ€æœ¯ã€‚</li>
<li>è¯¥æ¡†æ¶å®ç°äº†å¹¿æ³›çš„æ—¥å¿—äº‹ä»¶è¦†ç›–åº¦æå‡ï¼Œå¹¶ç”Ÿæˆæ›´è´´è¿‘å®é™…æ“ä½œçš„æ—¥å¿—åºåˆ—ã€‚</li>
<li>ä¸åŸºå‡†æ•°æ®é›†ç»“åˆæ—¶ï¼ŒAnomalyGenèƒ½å¤Ÿæé«˜å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>AnomalyGenä¸ºè‡ªåŠ¨åŒ–æ—¥å¿—åˆ†æå»ºç«‹äº†é«˜è´¨é‡åŸºå‡†èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12250">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-771d95f35c294de7edcd7e36dd4fd077.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42f1cd1ff7499f067f1a73697f742d33.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c673ce4e34304d2c60b696d8eae6d3e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68f729baf5faeab00adf1016f003764b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eed7cf8440ac0ba941d587d5960a1d95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06819f32225d18777c44bfc35719a5bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b148ec90de0247f79862ee14b9273a6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MOS-Towards-Effective-Smart-Contract-Vulnerability-Detection-through-Mixture-of-Experts-Tuning-of-Large-Language-Models"><a href="#MOS-Towards-Effective-Smart-Contract-Vulnerability-Detection-through-Mixture-of-Experts-Tuning-of-Large-Language-Models" class="headerlink" title="MOS: Towards Effective Smart Contract Vulnerability Detection through   Mixture-of-Experts Tuning of Large Language Models"></a>MOS: Towards Effective Smart Contract Vulnerability Detection through   Mixture-of-Experts Tuning of Large Language Models</h2><p><strong>Authors:Hang Yuan, Lei Yu, Zhirong Huang, Jingyuan Zhang, Junyi Lu, Shiqi Cheng, Li Yang, Fengjun Zhang, Jiajia Ma, Chun Zuo</strong></p>
<p>Smart contract vulnerabilities pose significant security risks to blockchain systems, potentially leading to severe financial losses. Existing methods face several limitations: (1) Program analysis-based approaches rely on predefined patterns, lacking flexibility for new vulnerability types; (2) Deep learning-based methods lack explanations; (3) Large language model-based approaches suffer from high false positives. We propose MOS, a smart contract vulnerability detection framework based on mixture-of-experts tuning (MOE-Tuning) of large language models. First, we conduct continual pre-training on a large-scale smart contract dataset to provide domain-enhanced initialization. Second, we construct a high-quality MOE-Tuning dataset through a multi-stage pipeline combining LLM generation and expert verification for reliable explanations. Third, we design a vulnerability-aware routing mechanism that activates the most relevant expert networks by analyzing code features and their matching degree with experts. Finally, we extend the feed-forward layers into multiple parallel expert networks, each specializing in specific vulnerability patterns. We employ a dual-objective loss function: one for optimizing detection and explanation performance, and another for ensuring reasonable distribution of vulnerability types to experts through entropy calculation. Experiments show that MOS significantly outperforms existing methods with average improvements of 6.32% in F1 score and 4.80% in accuracy. The vulnerability explanations achieve positive ratings (scores of 3-4 on a 4-point scale) of 82.96%, 85.21% and 94.58% for correctness, completeness, and conciseness through human and LLM evaluation. </p>
<blockquote>
<p>æ™ºèƒ½åˆçº¦æ¼æ´å¯¹åŒºå—é“¾ç³»ç»Ÿæ„æˆé‡å¤§å®‰å…¨é£é™©ï¼Œå¯èƒ½å¯¼è‡´ä¸¥é‡çš„è´¢åŠ¡æŸå¤±ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´ä¸€äº›å±€é™æ€§ï¼šï¼ˆ1ï¼‰åŸºäºç¨‹åºåˆ†æçš„æ–¹æ³•ä¾èµ–äºé¢„å®šä¹‰çš„æ¨¡å¼ï¼Œå¯¹æ–°æ¼æ´ç±»å‹ç¼ºä¹çµæ´»æ€§ï¼›ï¼ˆ2ï¼‰åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ç¼ºä¹è§£é‡Šæ€§ï¼›ï¼ˆ3ï¼‰åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•å­˜åœ¨é«˜è¯¯æŠ¥ç‡ã€‚æˆ‘ä»¬æå‡ºäº†MOSï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸“å®¶æ··åˆè°ƒæ•´ï¼ˆMOE-Tuningï¼‰çš„æ™ºèƒ½åˆçº¦æ¼æ´æ£€æµ‹æ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨å¤§è§„æ¨¡æ™ºèƒ½åˆçº¦æ•°æ®é›†ä¸Šè¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼Œä»¥æä¾›é¢†åŸŸå¢å¼ºçš„åˆå§‹åŒ–ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå¤šé˜¶æ®µç®¡é“æ„å»ºé«˜è´¨é‡çš„MOE-Tuningæ•°æ®é›†ï¼Œç»“åˆLLMç”Ÿæˆå’Œä¸“å®¶éªŒè¯ï¼Œä»¥æä¾›å¯é çš„è§£é‡Šã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ¼æ´æ„ŸçŸ¥è·¯ç”±æœºåˆ¶ï¼Œé€šè¿‡åˆ†æä»£ç ç‰¹å¾ä»¥åŠä¸ä¸“å®¶çš„åŒ¹é…ç¨‹åº¦æ¥æ¿€æ´»æœ€ç›¸å…³çš„ä¸“å®¶ç½‘ç»œã€‚æœ€åï¼Œæˆ‘ä»¬å°†å‰é¦ˆå±‚æ‰©å±•åˆ°å¤šä¸ªå¹¶è¡Œä¸“å®¶ç½‘ç»œï¼Œæ¯ä¸ªç½‘ç»œä¸“é—¨ç”¨äºç‰¹å®šçš„æ¼æ´æ¨¡å¼ã€‚æˆ‘ä»¬é‡‡ç”¨åŒç›®æ ‡æŸå¤±å‡½æ•°ï¼šä¸€ä¸ªç”¨äºä¼˜åŒ–æ£€æµ‹å’Œè§£é‡Šæ€§èƒ½ï¼Œå¦ä¸€ä¸ªç”¨äºé€šè¿‡ç†µè®¡ç®—ç¡®ä¿æ¼æ´ç±»å‹çš„åˆç†åˆ†é…ç»™ä¸“å®¶ã€‚å®éªŒè¡¨æ˜ï¼ŒMOSåœ¨F1åˆ†æ•°å’Œå‡†ç¡®ç‡ä¸Šå¹³å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåˆ†åˆ«æé«˜äº†6.32%å’Œ4.80%ã€‚æ¼æ´è§£é‡Šåœ¨æ­£ç¡®æ€§ã€å®Œæ•´æ€§å’Œç®€æ´æ€§æ–¹é¢åˆ†åˆ«è·å¾—äº†äººç±»å’ŒLLMè¯„ä»·çš„82.96%ã€85.21%å’Œ94.58%çš„æ­£é¢è¯„ä»·ï¼ˆå››ç‚¹é‡è¡¨ä¸Šçš„3-4åˆ†ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12234v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ··åˆä¸“å®¶ï¼ˆMOEï¼‰è°ƒä¼˜æ¡†æ¶MOSç”¨äºæ™ºèƒ½åˆçº¦æ¼æ´æ£€æµ‹ã€‚é€šè¿‡æŒç»­é¢„è®­ç»ƒã€é«˜è´¨é‡MOEè°ƒä¼˜æ•°æ®é›†æ„å»ºã€æ¼æ´æ„ŸçŸ¥è·¯ç”±æœºåˆ¶è®¾è®¡ä»¥åŠä¸“å®¶ç½‘ç»œçš„è®­ç»ƒç­‰æ­¥éª¤ï¼Œæé«˜äº†æ™ºèƒ½åˆçº¦æ¼æ´æ£€æµ‹çš„å‡†ç¡®æ€§å’Œè§£é‡Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMOSåœ¨F1åˆ†æ•°å’Œå‡†ç¡®ç‡ä¸Šå¹³å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ™ºèƒ½åˆçº¦æ¼æ´å¯¹åŒºå—é“¾ç³»ç»Ÿæ„æˆé‡å¤§å®‰å…¨é£é™©ï¼Œå¯èƒ½å¯¼è‡´é‡å¤§é‡‘èæŸå¤±ã€‚</li>
<li>ç°æœ‰æ£€æµ‹æ–¹æ³•å­˜åœ¨ä¾èµ–é¢„è®¾æ¨¡å¼ã€ç¼ºä¹å¯¹æ–°æ¼æ´ç±»å‹çš„çµæ´»æ€§ã€ç¼ºä¹è§£é‡Šæ€§ã€é«˜è¯¯æŠ¥ç‡ç­‰é—®é¢˜ã€‚</li>
<li>æå‡ºåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ··åˆä¸“å®¶ï¼ˆMOEï¼‰è°ƒä¼˜æ¡†æ¶MOSè¿›è¡Œæ™ºèƒ½åˆçº¦æ¼æ´æ£€æµ‹ã€‚</li>
<li>é€šè¿‡æŒç»­é¢„è®­ç»ƒæä¾›é¢†åŸŸå¢å¼ºåˆå§‹åŒ–ï¼Œæ„å»ºé«˜è´¨é‡MOEè°ƒä¼˜æ•°æ®é›†ï¼Œå®ç°å¯é è§£é‡Šã€‚</li>
<li>è®¾è®¡æ¼æ´æ„ŸçŸ¥è·¯ç”±æœºåˆ¶ï¼Œé€šè¿‡åˆ†æä»£ç ç‰¹å¾ä¸ä¸“å®¶åŒ¹é…ç¨‹åº¦æ¥æ¿€æ´»æœ€ç›¸å…³çš„ä¸“å®¶ç½‘ç»œã€‚</li>
<li>å°†å‰é¦ˆå±‚æ‰©å±•åˆ°å¤šä¸ªå¹¶è¡Œä¸“å®¶ç½‘ç»œï¼Œæ¯ä¸ªç½‘ç»œä¸“é—¨å¤„ç†ç‰¹å®šæ¼æ´æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12234">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3b025d070a778a08cb8aa99025b6a7b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a04bd88ac0cc34783f3a4ffa0769b3ab.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="d1-Scaling-Reasoning-in-Diffusion-Large-Language-Models-via-Reinforcement-Learning"><a href="#d1-Scaling-Reasoning-in-Diffusion-Large-Language-Models-via-Reinforcement-Learning" class="headerlink" title="d1: Scaling Reasoning in Diffusion Large Language Models via   Reinforcement Learning"></a>d1: Scaling Reasoning in Diffusion Large Language Models via   Reinforcement Learning</h2><p><strong>Authors:Siyan Zhao, Devaansh Gupta, Qinqing Zheng, Aditya Grover</strong></p>
<p>Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning. To this end, we propose d1, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and logical reasoning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç¤ºäº†ä»åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­å—ç›Šçš„å¼ºå¤§æ¨ç†èƒ½åŠ›ã€‚è¿™äº›èƒ½åŠ›ä¸»è¦æ˜¯åœ¨ä»å·¦åˆ°å³çš„è‡ªå›å½’ï¼ˆARï¼‰ç”ŸæˆèŒƒå¼å†…å±•ç¤ºçš„ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºæ‰©æ•£çš„éè‡ªå›å½’èŒƒå¼åˆ™ä»¥ä»ç²—åˆ°ç»†çš„æ–¹å¼ç”Ÿæˆæ–‡æœ¬ã€‚å°½ç®¡æœ€è¿‘çš„æ‰©æ•£å‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMï¼‰åœ¨ä¸å…¶ARåŒç±»æ¨¡å‹çš„ç«äº‰ä¸­å®ç°äº†ç›¸å½“çš„è¯­è¨€å»ºæ¨¡æ€§èƒ½ï¼Œä½†ä»ä¸æ¸…æ¥šdLLMæ˜¯å¦ä¹Ÿèƒ½åˆ©ç”¨LLMæ¨ç†çš„æœ€æ–°è¿›å±•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†d1æ¡†æ¶ï¼Œå®ƒé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒRLçš„ç»“åˆï¼Œå°†é¢„è®­ç»ƒçš„æ©ç å‹dLLMé€‚åº”ä¸ºæ¨ç†æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼€å‘å¹¶æ‰©å±•äº†æ”¹è¿›é¢„è®­ç»ƒdLLMæ¨ç†çš„æŠ€æœ¯ï¼šï¼ˆaï¼‰æˆ‘ä»¬ä½¿ç”¨æ©è”½çš„SFTæŠ€æœ¯ä»ç°æœ‰æ•°æ®é›†ä¸­æç‚¼çŸ¥è¯†å¹¶çŒè¾“è‡ªæˆ‘æ”¹è¿›è¡Œä¸ºï¼›ï¼ˆbï¼‰æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ— éœ€è¯„è®ºå®¶ã€åŸºäºç­–ç•¥æ¢¯åº¦çš„RLç®—æ³•ï¼Œç§°ä¸ºdiffu-GRPOã€‚é€šè¿‡å®è¯ç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªæ•°å­¦å’Œé€»è¾‘æ¨ç†åŸºå‡†ä¸Šæ¢è®¨äº†ä¸åŒåè®­ç»ƒé…æ–¹ï¼ˆpost-training recipesï¼‰çš„æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°d1æä¾›äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—æé«˜äº†æœ€æ–°dLLMçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12216v1">PDF</a> 25 pages, project page at <a target="_blank" rel="noopener" href="https://dllm-reasoning.github.io/">https://dllm-reasoning.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä¸»è¦åœ¨å·¦è‡³å³çš„è‡ªå›å½’ç”ŸæˆèŒƒå¼ä¸­å¾—åˆ°éªŒè¯ã€‚æœ¬æ–‡æå‡ºäº†d1æ¡†æ¶ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒRLå°†é¢„è®­ç»ƒçš„æ©ç æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMï¼‰é€‚åº”äºæ¨ç†æ¨¡å‹ã€‚é‡‡ç”¨æ©ç SFTæŠ€æœ¯å’Œä¸€ç§æ–°çš„æ— æ‰¹è¯„è€…ã€åŸºäºç­–ç•¥æ¢¯åº¦çš„RLç®—æ³•diffu-GRPOæ¥æå‡é¢„è®­ç»ƒdLLMçš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œd1åœ¨å¤šä¸ªæ•°å­¦å’Œé€»è¾‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°æœ€ä½³ï¼Œå¹¶æ˜¾è‘—æå‡äº†å…ˆè¿›dLLMçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å—ç›Šäºåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è‡ªå›å½’ç”ŸæˆèŒƒå¼å’Œéè‡ªå›å½’æ‰©æ•£èŒƒå¼åœ¨LLMä¸­å„æœ‰ä¼˜åŠ¿ã€‚</li>
<li>dLLMåœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢å·²è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>d1æ¡†æ¶ç»“åˆäº†ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œæ—¨åœ¨å°†é¢„è®­ç»ƒçš„dLLMé€‚åº”äºæ¨ç†ä»»åŠ¡ã€‚</li>
<li>d1é‡‡ç”¨æ©ç SFTæŠ€æœ¯å’Œæ–°å‹çš„RLç®—æ³•diffu-GRPOæ¥æå‡dLLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®è¯ç ”ç©¶è¡¨æ˜ï¼Œd1åœ¨å¤šä¸ªæ•°å­¦å’Œé€»è¾‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12216">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd2c98a830b610cd0dcc696ad3bcdc99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-164d4f8d5db23a2bae9c04ad847dbd8d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Towards-LLM-Agents-for-Earth-Observation"><a href="#Towards-LLM-Agents-for-Earth-Observation" class="headerlink" title="Towards LLM Agents for Earth Observation"></a>Towards LLM Agents for Earth Observation</h2><p><strong>Authors:Chia Hsiang Kao, Wenting Zhao, Shreelekha Revankar, Samuel Speas, Snehal Bhagat, Rajeev Datta, Cheng Perng Phoo, Utkarsh Mall, Carl Vondrick, Kavita Bala, Bharath Hariharan</strong></p>
<p>Earth Observation (EO) provides critical planetary data for environmental monitoring, disaster management, climate science, and other scientific domains. Here we ask: Are AI systems ready for reliable Earth Observation? We introduce \datasetnamenospace, a benchmark of 140 yes&#x2F;no questions from NASA Earth Observatory articles across 13 topics and 17 satellite sensors. Using Google Earth Engine API as a tool, LLM agents can only achieve an accuracy of 33% because the code fails to run over 58% of the time. We improve the failure rate for open models by fine-tuning synthetic data, allowing much smaller models (Llama-3.1-8B) to achieve comparable accuracy to much larger ones (e.g., DeepSeek-R1). Taken together, our findings identify significant challenges to be solved before AI agents can automate earth observation, and suggest paths forward. The project page is available at <a target="_blank" rel="noopener" href="https://iandrover.github.io/UnivEarth">https://iandrover.github.io/UnivEarth</a>. </p>
<blockquote>
<p>åœ°çƒè§‚æµ‹ï¼ˆEOï¼‰ä¸ºç¯å¢ƒç›‘æµ‹ã€ç¾å®³ç®¡ç†ã€æ°”å€™ç§‘å­¦å’Œå…¶ä»–ç§‘å­¦é¢†åŸŸæä¾›äº†å…³é”®è¡Œæ˜Ÿæ•°æ®ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬æå‡ºä¸€ä¸ªé—®é¢˜ï¼šäººå·¥æ™ºèƒ½ç³»ç»Ÿæ˜¯å¦å·²ç»å‡†å¤‡å¥½è¿›è¡Œå¯é çš„åœ°çƒè§‚æµ‹ï¼Ÿæˆ‘ä»¬æ¨å‡ºäº†\datasetnamenospaceï¼Œè¿™æ˜¯ä»NASAåœ°çƒè§‚æµ‹ç«™æ–‡ç« ä¸­æå–çš„140ä¸ªæ˜¯éé—®é¢˜çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–13ä¸ªä¸»é¢˜å’Œ17ä¸ªå«æ˜Ÿä¼ æ„Ÿå™¨ã€‚ä½¿ç”¨Googleåœ°çƒå¼•æ“APIä½œä¸ºå·¥å…·ï¼ŒLLMä»£ç†åªèƒ½è¾¾åˆ°33%çš„å‡†ç¡®ç‡ï¼Œå› ä¸ºä»£ç æœ‰58%çš„æ—¶é—´æ— æ³•è¿è¡Œã€‚æˆ‘ä»¬é€šè¿‡å¾®è°ƒåˆæˆæ•°æ®æ¥æé«˜å¼€æ”¾æ¨¡å‹çš„å¤±è´¥ç‡ï¼Œå…è®¸æ›´å°çš„æ¨¡å‹ï¼ˆLlama-3.1-8Bï¼‰å®ç°ä¸æ›´å¤§çš„æ¨¡å‹ï¼ˆä¾‹å¦‚DeepSeek-R1ï¼‰ç›¸å½“çš„å‡†ç¡®ç‡ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç ”ç©¶å‘ç°äº†åœ¨äººå·¥æ™ºèƒ½ä»£ç†èƒ½å¤Ÿè‡ªåŠ¨è¿›è¡Œåœ°çƒè§‚æµ‹ä¹‹å‰éœ€è¦è§£å†³çš„é‡å¤§æŒ‘æˆ˜ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥çš„å‘å±•æ–¹å‘ã€‚é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://iandrover.github.io/UnivEarth">https://iandrover.github.io/UnivEarth</a>è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12110v1">PDF</a> 36 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨åœ°çƒè§‚æµ‹æ–¹é¢çš„åº”ç”¨ï¼Œå¼•å…¥äº†ä¸€ä¸ªåä¸º\datasetnamenospaceçš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«NASAåœ°çƒè§‚æµ‹ç«™çš„140ä¸ªæ˜¯ä¸éé—®é¢˜ï¼Œæ¶µç›–äº†å«æ˜Ÿä¼ æ„Ÿå™¨æ¶‰åŠçš„13ä¸ªä¸»é¢˜å’Œ17ä¸ªå«æ˜Ÿä¼ æ„Ÿå™¨ã€‚è™½ç„¶åˆ©ç”¨Googleåœ°çƒå¼•æ“APIä½œä¸ºå·¥å…·ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹(LLM)ä»…èƒ½å®Œæˆæ—¥å¸¸å·¥ä½œçš„33%ï¼ŒåŒæ—¶å¤§éƒ¨åˆ†æ¨¡å‹åœ¨è¿è¡Œä¸­é­é‡æ€§èƒ½ç“¶é¢ˆçš„é—®é¢˜é¢‘ç‡è¾¾åˆ°è¿‘å…­æˆã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¾®è°ƒåˆæˆæ•°æ®æå‡äº†å¼€æºæ¨¡å‹çš„æ€§èƒ½ï¼Œè¯æ˜äº†è¾ƒå°çš„æ¨¡å‹ä¹Ÿèƒ½è¾¾åˆ°ç”šè‡³è¶…è¿‡è¾ƒå¤§æ¨¡å‹çš„å‡†ç¡®åº¦ã€‚æ€»çš„æ¥è¯´ï¼Œè¯¥ç ”ç©¶æŒ‡å‡ºäº†äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨è‡ªåŠ¨åŒ–åœ°çƒè§‚æµ‹æ–¹é¢æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æä¾›äº†æœªæ¥å¯èƒ½çš„ç ”ç©¶æ–¹å‘ã€‚å…·ä½“é¡¹ç›®ä¿¡æ¯å¯é€šè¿‡é“¾æ¥æŸ¥çœ‹ï¼š<a target="_blank" rel="noopener" href="https://iandrover.github.io/UnivEarth%E3%80%82">https://iandrover.github.io/UnivEarthã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨å¤„ç†åœ°çƒè§‚æµ‹æ•°æ®æ—¶é¢ä¸´å¯é æ€§é—®é¢˜ã€‚</li>
<li>LLMåœ¨å¤„ç†ç‰¹å®šä»»åŠ¡æ—¶å‡†ç¡®ç‡ä»…ä¸ºä¸‰åˆ†ä¹‹ä¸€ã€‚</li>
<li>Googleåœ°çƒå¼•æ“APIå­˜åœ¨è¿è¡Œæ•…éšœé—®é¢˜ï¼Œè¿‘å…­æˆæ—¶é—´æ— æ³•æ­£å¸¸è¿è¡Œã€‚</li>
<li>é€šè¿‡å¾®è°ƒåˆæˆæ•°æ®å¯ä»¥æå‡å¼€æºæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å°å‹æ¨¡å‹ï¼ˆå¦‚Llama-3.1-8Bï¼‰å¯ä»¥åœ¨å‡†ç¡®åº¦ä¸Šè¾¾åˆ°ç”šè‡³è¶…è¶Šå¤§å‹æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1ï¼‰ã€‚</li>
<li>äººå·¥æ™ºèƒ½åœ¨è‡ªåŠ¨åŒ–åœ°çƒè§‚æµ‹æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜éœ€è¦è§£å†³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12110">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3a50c7d599d569532aa2b777fa0f53a9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ecd51e4caf54bb5546639a8750ae900.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-953d4a1b1acb6343294e31abafe62e01.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Entropy-Guided-Watermarking-for-LLMs-A-Test-Time-Framework-for-Robust-and-Traceable-Text-Generation"><a href="#Entropy-Guided-Watermarking-for-LLMs-A-Test-Time-Framework-for-Robust-and-Traceable-Text-Generation" class="headerlink" title="Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust   and Traceable Text Generation"></a>Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust   and Traceable Text Generation</h2><p><strong>Authors:Shizhan Cai, Liang Ding, Dacheng Tao</strong></p>
<p>The rapid development of Large Language Models (LLMs) has intensified concerns about content traceability and potential misuse. Existing watermarking schemes for sampled text often face trade-offs between maintaining text quality and ensuring robust detection against various attacks. To address these issues, we propose a novel watermarking scheme that improves both detectability and text quality by introducing a cumulative watermark entropy threshold. Our approach is compatible with and generalizes existing sampling functions, enhancing adaptability. Experimental results across multiple LLMs show that our scheme significantly outperforms existing methods, achieving over 80% improvements on widely-used datasets, e.g., MATH and GSM8K, while maintaining high detection accuracy. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•åŠ å‰§äº†äººä»¬å¯¹å†…å®¹å¯è¿½è¸ªæ€§å’Œæ½œåœ¨è¯¯ç”¨çš„æ‹…å¿§ã€‚ç°æœ‰é‡‡æ ·æ–‡æœ¬çš„æ°´å°æ–¹æ¡ˆå¾€å¾€éœ€è¦åœ¨ä¿æŒæ–‡æœ¬è´¨é‡å’Œç¡®ä¿å¯¹å„ç§æ”»å‡»çš„ç¨³å¥æ£€æµ‹ä¹‹é—´åšå‡ºæƒè¡¡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ°´å°æ–¹æ¡ˆï¼Œé€šè¿‡å¼•å…¥ç´¯ç§¯æ°´å°ç†µé˜ˆå€¼ï¼Œæé«˜äº†æ£€æµ‹èƒ½åŠ›å’Œæ–‡æœ¬è´¨é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ç°æœ‰çš„é‡‡æ ·å‡½æ•°å…¼å®¹å¹¶è¿›è¡Œäº†æ¨å¹¿ï¼Œå¢å¼ºäº†é€‚åº”æ€§ã€‚è·¨å¤šä¸ªLLMçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ¡ˆæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨å¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ï¼ˆä¾‹å¦‚MATHå’ŒGSM8Kï¼‰ä¸Šçš„æ”¹è¿›å¹…åº¦è¶…è¿‡80%ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ£€æµ‹å‡†ç¡®ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12108v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•å¼•å‘äº†äººä»¬å¯¹å†…å®¹å¯è¿½æº¯æ€§å’Œæ½œåœ¨è¯¯ç”¨çš„å…³æ³¨ã€‚ç°æœ‰æ–‡æœ¬æ°´å°æ–¹æ¡ˆåœ¨ä¿æŒæ–‡æœ¬è´¨é‡ä¸ç¡®ä¿å¯¹æŠ—å„ç§æ”»å‡»çš„æ£€æµ‹èƒ½åŠ›ä¹‹é—´å¸¸å¸¸å­˜åœ¨æƒè¡¡ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ°´å°æ–¹æ¡ˆï¼Œé€šè¿‡å¼•å…¥ç´¯ç§¯æ°´å°ç†µé˜ˆå€¼ï¼Œæé«˜äº†æ£€æµ‹æ€§å’Œæ–‡æœ¬è´¨é‡ã€‚è¯¥æ–¹æ³•ä¸ç°æœ‰é‡‡æ ·å‡½æ•°å…¼å®¹å¹¶å¢å¼ºäº†é€‚åº”æ€§ã€‚åœ¨å¤šä¸ªLLMä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨å¹¿æ³›ä½¿ç”¨çš„MATHå’ŒGSM8Kæ•°æ®é›†ä¸Šæ”¹è¿›è¶…è¿‡80%ï¼ŒåŒæ—¶ä¿æŒé«˜æ£€æµ‹å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•å¼•å‘äº†å¯¹å†…å®¹è¿½æº¯å’Œæ½œåœ¨è¯¯ç”¨çš„å…³æ³¨ã€‚</li>
<li>ç°æœ‰æ–‡æœ¬æ°´å°æ–¹æ¡ˆåœ¨ä¿æŒæ–‡æœ¬è´¨é‡ä¸æ£€æµ‹èƒ½åŠ›ä¹‹é—´å­˜æƒè¡¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ°´å°æ–¹æ¡ˆï¼Œé€šè¿‡å¼•å…¥ç´¯ç§¯æ°´å°ç†µé˜ˆå€¼ï¼Œæé«˜æ£€æµ‹æ€§å’Œæ–‡æœ¬è´¨é‡ã€‚</li>
<li>æ–°æ–¹æ¡ˆä¸ç°æœ‰é‡‡æ ·å‡½æ•°å…¼å®¹ï¼Œå¢å¼ºé€‚åº”æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨å¤šä¸ªLLMä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>åœ¨å¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ä¸Šï¼Œå¦‚MATHå’ŒGSM8Kï¼Œæ”¹è¿›è¶…è¿‡80%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12108">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-03a46c973a96634bd06c76f7de6d24c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3907651b463323e2232d949acd82b40e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d06f498a3df28d9f012ce7bc05033bff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27edbe40a849986c603e044e88ad676a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c32a0ff003e62938a123cb58c1164a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad1c9b2db846afb44009255bef59f642.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ddab60a60a688cc08c99e6234c93fa1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45f2d15bc800650e393366d482a53db9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Reasoning-Based-AI-for-Startup-Evaluation-R-A-I-S-E-A-Memory-Augmented-Multi-Step-Decision-Framework"><a href="#Reasoning-Based-AI-for-Startup-Evaluation-R-A-I-S-E-A-Memory-Augmented-Multi-Step-Decision-Framework" class="headerlink" title="Reasoning-Based AI for Startup Evaluation (R.A.I.S.E.): A   Memory-Augmented, Multi-Step Decision Framework"></a>Reasoning-Based AI for Startup Evaluation (R.A.I.S.E.): A   Memory-Augmented, Multi-Step Decision Framework</h2><p><strong>Authors:Jack Preuveneers, Joseph Ternasky, Fuat Alican, Yigit Ihlamur</strong></p>
<p>We present a novel framework that bridges the gap between the interpretability of decision trees and the advanced reasoning capabilities of large language models (LLMs) to predict startup success. Our approach leverages chain-of-thought prompting to generate detailed reasoning logs, which are subsequently distilled into structured, human-understandable logical rules. The pipeline integrates multiple enhancements - efficient data ingestion, a two-step refinement process, ensemble candidate sampling, simulated reinforcement learning scoring, and persistent memory - to ensure both stable decision-making and transparent output. Experimental evaluations on curated startup datasets demonstrate that our combined pipeline improves precision by 54% from 0.225 to 0.346 and accuracy by 50% from 0.46 to 0.70 compared to a standalone OpenAI o3 model. Notably, our model achieves over 2x the precision of a random classifier (16%). By combining state-of-the-art AI reasoning with explicit rule-based explanations, our method not only augments traditional decision-making processes but also facilitates expert intervention and continuous policy refinement. This work lays the foundation for the implementation of interpretable LLM-powered decision frameworks in high-stakes investment environments and other domains that require transparent and data-driven insights. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨å¼¥æ ‘å†³ç­–çš„å¯è§£é‡Šæ€§ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é«˜çº§æ¨ç†èƒ½åŠ›ä¹‹é—´çš„é¸¿æ²Ÿï¼Œä»¥é¢„æµ‹åˆ›ä¸šå…¬å¸çš„æˆåŠŸã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨é“¾å¼æ€ç»´æç¤ºç”Ÿæˆè¯¦ç»†çš„æ¨ç†æ—¥å¿—ï¼Œéšåå°†å…¶æç‚¼æˆç»“æ„åŒ–ã€äººç±»å¯ç†è§£çš„é€»è¾‘è§„åˆ™ã€‚è¯¥ç®¡é“èåˆäº†å¤šé¡¹å¢å¼ºåŠŸèƒ½ï¼ŒåŒ…æ‹¬é«˜æ•ˆæ•°æ®æ‘„å–ã€ä¸¤æ­¥ä¼˜åŒ–è¿‡ç¨‹ã€é›†æˆå€™é€‰é‡‡æ ·ã€æ¨¡æ‹Ÿå¼ºåŒ–å­¦ä¹ è¯„åˆ†å’ŒæŒä¹…æ€§å†…å­˜ï¼Œä»¥ç¡®ä¿ç¨³å®šçš„å†³ç­–åˆ¶å®šå’Œé€æ˜çš„è¾“å‡ºã€‚åœ¨ç²¾é€‰çš„åˆåˆ›å…¬å¸æ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç»„åˆç®¡é“ä¸å•ç‹¬çš„OpenAI o3æ¨¡å‹ç›¸æ¯”ï¼Œç²¾åº¦ä»0.225æé«˜åˆ°0.346ï¼ˆæé«˜54%ï¼‰ï¼Œå‡†ç¡®ç‡ä»0.46æé«˜åˆ°0.70ï¼ˆæé«˜50%ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹çš„ç²¾åº¦æ˜¯éšæœºåˆ†ç±»å™¨çš„ä¸¤å€ä»¥ä¸Šï¼ˆ16%ï¼‰ã€‚é€šè¿‡ç»“åˆæœ€å…ˆè¿›çš„AIæ¨ç†å’ŒåŸºäºè§„åˆ™çš„æ˜ç¡®è§£é‡Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…å¢å¼ºäº†ä¼ ç»Ÿçš„å†³ç­–åˆ¶å®šè¿‡ç¨‹ï¼Œè¿˜æœ‰åŠ©äºä¸“å®¶å¹²é¢„å’ŒæŒç»­çš„æ”¿ç­–ä¼˜åŒ–ã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨é«˜é£é™©æŠ•èµ„ç¯å¢ƒå’Œå…¶ä»–éœ€è¦é€æ˜å’Œæ•°æ®é©±åŠ¨æ´å¯ŸåŠ›çš„é¢†åŸŸå®æ–½å¯è§£é‡Šçš„LLMé©±åŠ¨å†³ç­–æ¡†æ¶å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12090v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–æ¡†æ¶ï¼Œèåˆäº†å†³ç­–æ ‘çš„å¯è§£é‡Šæ€§ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é«˜çº§æ¨ç†èƒ½åŠ›ï¼Œç”¨äºé¢„æµ‹åˆ›ä¸šæˆåŠŸã€‚è¯¥ç ”ç©¶é€šè¿‡é“¾å¼æ€ç»´æç¤ºç”Ÿæˆè¯¦ç»†çš„æ¨ç†æ—¥å¿—ï¼Œå†å°†å…¶æç‚¼ä¸ºç»“æ„åŒ–ã€äººç±»å¯ç†è§£çš„é€»è¾‘è§„åˆ™ã€‚ç»è¿‡å¤šé¡¹æ”¹è¿›ï¼ŒåŒ…æ‹¬é«˜æ•ˆæ•°æ®æ‘„å–ã€ä¸¤æ­¥ç²¾ç‚¼è¿‡ç¨‹ã€é›†æˆå€™é€‰é‡‡æ ·ã€æ¨¡æ‹Ÿå¼ºåŒ–å­¦ä¹ è¯„åˆ†å’ŒæŒä¹…æ€§å†…å­˜ç­‰ï¼Œç¡®ä¿äº†ç¨³å®šçš„å†³ç­–åˆ¶å®šå’Œé€æ˜çš„è¾“å‡ºã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸å•ç‹¬çš„OpenAI o3æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥ç®¡é“åœ¨ç²¾é€‰çš„åˆ›ä¸šæ•°æ®é›†ä¸Šå°†ç²¾åº¦ä»0.225æé«˜åˆ°0.346ï¼Œæé«˜äº†54%ï¼Œå‡†ç¡®ç‡ä»0.46æé«˜åˆ°0.70ï¼Œæé«˜äº†50%ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°å¯è§£é‡Šçš„å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å†³ç­–æ¡†æ¶åœ¨é«˜é£é™©æŠ•èµ„ç¯å¢ƒå’Œå…¶ä»–éœ€è¦é€æ˜å’Œæ•°æ®é©±åŠ¨æ´å¯ŸåŠ›çš„é¢†åŸŸå¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç ”ç©¶æå‡ºäº†èåˆå†³ç­–æ ‘å¯è§£é‡Šæ€§ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°é¢–æ¡†æ¶ï¼Œç”¨äºé¢„æµ‹åˆ›ä¸šæˆåŠŸã€‚</li>
<li>é€šè¿‡é“¾å¼æ€ç»´æç¤ºç”Ÿæˆè¯¦ç»†çš„æ¨ç†æ—¥å¿—ï¼Œå†å°†å…¶è½¬åŒ–ä¸ºç»“æ„åŒ–é€»è¾‘è§„åˆ™ã€‚</li>
<li>ç»“åˆå¤šé¡¹æŠ€æœ¯æ”¹è¿›ï¼Œç¡®ä¿ç¨³å®šå†³ç­–å’Œé€æ˜è¾“å‡ºã€‚</li>
<li>å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶ç›¸è¾ƒäºå•ä¸€çš„OpenAI o3æ¨¡å‹ï¼Œç²¾åº¦å’Œå‡†ç¡®ç‡å‡æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>ä¸éšæœºåˆ†ç±»å™¨ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹çš„ç²¾åº¦è¶…è¿‡ä¸¤å€ã€‚</li>
<li>ç»“åˆå…ˆè¿›çš„äººå·¥æ™ºèƒ½æ¨ç†å’ŒåŸºäºè§„åˆ™çš„è§£é‡Šæ–¹æ³•ï¼Œä¸ä»…å¢å¼ºäº†ä¼ ç»Ÿå†³ç­–åˆ¶å®šè¿‡ç¨‹ï¼Œè¿˜ä¾¿äºä¸“å®¶ä»‹å…¥å’Œæ”¿ç­–æŒç»­æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12090">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a8a1e3dbf0d4024953135d0d2e1d62ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0152b8f00088e2bbd75514c5f046679.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-974041a8c96599e3a952e80f40afc041.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="AttentionDrop-A-Novel-Regularization-Method-for-Transformer-Models"><a href="#AttentionDrop-A-Novel-Regularization-Method-for-Transformer-Models" class="headerlink" title="AttentionDrop: A Novel Regularization Method for Transformer Models"></a>AttentionDrop: A Novel Regularization Method for Transformer Models</h2><p><strong>Authors:Mirza Samad Ahmed Baig, Syeda Anshrah Gillani, Abdul Akbar Khan, Shahid Munir Shah</strong></p>
<p>Transformer-based architectures achieve state-of-the-art performance across a wide range of tasks in natural language processing, computer vision, and speech. However, their immense capacity often leads to overfitting, especially when training data is limited or noisy. We propose AttentionDrop, a unified family of stochastic regularization techniques that operate directly on the self-attention distributions. We introduces three variants: 1. Hard Attention Masking: randomly zeroes out top-k attention logits per query to encourage diverse context utilization. 2. Blurred Attention Smoothing: applies a dynamic Gaussian convolution over attention logits to diffuse overly peaked distributions. 3. Consistency-Regularized AttentionDrop: enforces output stability under multiple independent AttentionDrop perturbations via a KL-based consistency loss. </p>
<blockquote>
<p>åŸºäºTransformerçš„æ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œè¯­éŸ³çš„å¹¿æ³›ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬å·¨å¤§çš„å®¹é‡å¾€å¾€ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒæ•°æ®æœ‰é™æˆ–å˜ˆæ‚çš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†AttentionDropï¼Œè¿™æ˜¯ä¸€ä¸ªç›´æ¥åœ¨è‡ªæ³¨æ„åŠ›åˆ†å¸ƒä¸Šè¿è¡Œçš„éšæœºæ­£åˆ™åŒ–æŠ€æœ¯çš„ç»Ÿä¸€å®¶æ—ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸‰ç§å˜ä½“ï¼š1.ç¡¬æ³¨æ„åŠ›æ©ç ï¼šéšæœºå°†æ¯ä¸ªæŸ¥è¯¢çš„å‰kä¸ªæ³¨æ„åŠ›å¯¹æ•°æ¦‚ç‡ç½®é›¶ï¼Œä»¥é¼“åŠ±åˆ©ç”¨å¤šæ ·åŒ–çš„ä¸Šä¸‹æ–‡ã€‚2.æ¨¡ç³Šæ³¨æ„åŠ›å¹³æ»‘ï¼šåœ¨æ³¨æ„åŠ›å¯¹æ•°æ¦‚ç‡ä¸Šåº”ç”¨åŠ¨æ€é«˜æ–¯å·ç§¯ï¼Œä»¥åˆ†æ•£è¿‡äºé›†ä¸­çš„åˆ†å¸ƒã€‚3.ä¸€è‡´æ€§æ­£åˆ™åŒ–AttentionDropï¼šé€šè¿‡åŸºäºKLçš„ä¸€è‡´æ€§æŸå¤±ï¼Œå¼ºåˆ¶åœ¨å¤šä¸ªç‹¬ç«‹çš„AttentionDropæ‰°åŠ¨ä¸‹è¾“å‡ºç¨³å®šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12088v1">PDF</a> 26 pages</p>
<p><strong>Summary</strong>ï¼šåŸºäºTransformerçš„æ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œè¯­éŸ³ç­‰å¤šä¸ªä»»åŠ¡ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚ä½†å…¶å·¨å¤§çš„å®¹é‡å®¹æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒæ•°æ®æœ‰é™æˆ–å™ªå£°è¾ƒå¤§æ—¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†AttentionDropè¿™ä¸€ç»Ÿä¸€çš„éšæœºæ­£åˆ™åŒ–æŠ€æœ¯å®¶æ—ï¼Œç›´æ¥åœ¨è‡ªæ³¨æ„åŠ›åˆ†å¸ƒä¸Šæ“ä½œã€‚åŒ…æ‹¬ä¸‰ç§å˜ä½“ï¼šç¡¬æ³¨æ„åŠ›æ©ç ã€æ¨¡ç³Šæ³¨æ„åŠ›å¹³æ»‘å’Œä¸€è‡´æ€§æ­£åˆ™åŒ–AttentionDropã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Transformeræ¶æ„åœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å­˜åœ¨è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>AttentionDropæ˜¯ä¸€ç§é’ˆå¯¹Transformeræ¨¡å‹çš„éšæœºæ­£åˆ™åŒ–æŠ€æœ¯ã€‚</li>
<li>Hard Attention Maskingé€šè¿‡éšæœºå°†é¡¶éƒ¨kä¸ªæ³¨æ„åŠ›æ—¥å¿—æ¸…é›¶æ¥é¼“åŠ±å¤šæ ·åŒ–çš„ä¸Šä¸‹æ–‡åˆ©ç”¨ã€‚</li>
<li>Blurred Attention Smoothingé€šè¿‡åŠ¨æ€é«˜æ–¯å·ç§¯æ¥å¹³æ»‘è¿‡äºé›†ä¸­çš„æ³¨æ„åŠ›åˆ†å¸ƒã€‚</li>
<li>Consistency-Regularized AttentionDropé€šè¿‡åŸºäºKLçš„ä¸€è‡´æ€§æŸå¤±æ¥ç¡®ä¿å¤šä¸ªç‹¬ç«‹çš„AttentionDropæ‰°åŠ¨ä¸‹çš„è¾“å‡ºç¨³å®šæ€§ã€‚</li>
<li>AttentionDropæŠ€æœ¯æœ‰åŠ©äºæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å‡å°‘è¿‡æ‹Ÿåˆç°è±¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12088">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-677a48c5da042fdd889ccf24344e16df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b3ac5d517b6219a10440973afcf1a0c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Selective-Demonstration-Retrieval-for-Improved-Implicit-Hate-Speech-Detection"><a href="#Selective-Demonstration-Retrieval-for-Improved-Implicit-Hate-Speech-Detection" class="headerlink" title="Selective Demonstration Retrieval for Improved Implicit Hate Speech   Detection"></a>Selective Demonstration Retrieval for Improved Implicit Hate Speech   Detection</h2><p><strong>Authors:Yumin Kim, Hwanhee Lee</strong></p>
<p>Hate speech detection is a crucial area of research in natural language processing, essential for ensuring online community safety. However, detecting implicit hate speech, where harmful intent is conveyed in subtle or indirect ways, remains a major challenge. Unlike explicit hate speech, implicit expressions often depend on context, cultural subtleties, and hidden biases, making them more challenging to identify consistently. Additionally, the interpretation of such speech is influenced by external knowledge and demographic biases, resulting in varied detection results across different language models. Furthermore, Large Language Models often show heightened sensitivity to toxic language and references to vulnerable groups, which can lead to misclassifications. This over-sensitivity results in false positives (incorrectly identifying harmless statements as hateful) and false negatives (failing to detect genuinely harmful content). Addressing these issues requires methods that not only improve detection precision but also reduce model biases and enhance robustness. To address these challenges, we propose a novel method, which utilizes in-context learning without requiring model fine-tuning. By adaptively retrieving demonstrations that focus on similar groups or those with the highest similarity scores, our approach enhances contextual comprehension. Experimental results show that our method outperforms current state-of-the-art techniques. Implementation details and code are available at TBD. </p>
<blockquote>
<p>ä»‡æ¨è¨€è®ºæ£€æµ‹æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸€ä¸ªå…³é”®ç ”ç©¶é¢†åŸŸï¼Œå¯¹äºç¡®ä¿åœ¨çº¿ç¤¾åŒºå®‰å…¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæ£€æµ‹éšæ™¦ä»‡æ¨è¨€è®ºä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ï¼Œéšæ™¦ä»‡æ¨è¨€è®ºä»¥å¾®å¦™æˆ–é—´æ¥çš„æ–¹å¼ä¼ è¾¾æœ‰å®³æ„å›¾ã€‚ä¸æ˜ç¡®ä»‡æ¨è¨€è®ºä¸åŒï¼Œéšæ™¦è¡¨è¾¾å¸¸å¸¸ä¾èµ–äºä¸Šä¸‹æ–‡ã€æ–‡åŒ–ç»†å¾®å·®åˆ«å’Œéšå«åè§ï¼Œå› æ­¤éš¾ä»¥æŒç»­ä¸€è‡´åœ°è¯†åˆ«ã€‚æ­¤å¤–ï¼Œæ­¤ç±»è¨€è®ºçš„è§£è¯»å—åˆ°å¤–éƒ¨çŸ¥è¯†å’Œäººå£ç»Ÿè®¡åè§çš„å½±å“ï¼Œå¯¼è‡´ä¸åŒè¯­è¨€æ¨¡å‹çš„æ£€æµ‹ç»“æœå­˜åœ¨å·®å¼‚ã€‚å¦å¤–ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å¯¹æœ‰æ¯’è¯­è¨€å’Œå¼±åŠ¿ç¾¤ä½“çš„æåŠå¾€å¾€è¡¨ç°å‡ºè¿‡åº¦æ•æ„Ÿæ€§ï¼Œå¯èƒ½å¯¼è‡´è¯¯åˆ¤ã€‚è¿™ç§è¿‡åº¦æ•æ„Ÿä¼šå¯¼è‡´è¯¯æŠ¥ï¼ˆé”™è¯¯åœ°å°†æ— å®³å£°æ˜è¯†åˆ«ä¸ºä»‡æ¨è¨€è®ºï¼‰å’Œæ¼æŠ¥ï¼ˆæœªèƒ½æ£€æµ‹åˆ°çœŸæ­£æœ‰å®³çš„å†…å®¹ï¼‰ã€‚è¦è§£å†³è¿™äº›é—®é¢˜ï¼Œéœ€è¦ä¸ä»…æé«˜æ£€æµ‹ç²¾åº¦ï¼Œè€Œä¸”å‡å°‘æ¨¡å‹åè§å¹¶å¢å¼ºç¨³å¥æ€§çš„æ–¹æ³•ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ è€Œæ— éœ€æ¨¡å‹å¾®è°ƒçš„æ–°æ–¹æ³•ã€‚é€šè¿‡è‡ªé€‚åº”æ£€ç´¢ç±»ä¼¼ç¾¤ä½“æˆ–æœ€é«˜ç›¸ä¼¼åº¦å¾—åˆ†çš„æ¼”ç¤ºå†…å®¹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¢å¼ºäº†ä¸Šä¸‹æ–‡ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå½“å‰æœ€æ–°æŠ€æœ¯ã€‚å®ç°ç»†èŠ‚å’Œä»£ç å¯å‚è§ï¼ˆæš‚å®šç½‘å€ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12082v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦æ€»ç»“äº†è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ä»‡æ¨è¨€è®ºæ£€æµ‹çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«å«è“„ä»‡æ¨è¨€è®ºæ–¹é¢çš„æŒ‘æˆ˜ã€‚ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¿™ç§è¨€è®ºæ—¶å­˜åœ¨è¿‡åº¦æ•æ„Ÿçš„é—®é¢˜ï¼Œå¯¼è‡´è¯¯æŠ¥å’Œæ¼æŠ¥ã€‚ä¸ºæé«˜æ£€æµ‹ç²¾åº¦å¹¶å‡å°‘æ¨¡å‹åè§ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡æ£€ç´¢ç›¸ä¼¼æ¼”ç¤ºå†…å®¹æé«˜è¯­å¢ƒç†è§£èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚å…·ä½“å®æ–½ç»†èŠ‚å’Œä»£ç è¯·å‚è§å…·ä½“é“¾æ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯ä»æ–‡æœ¬ä¸­æå–çš„ä¸ƒä¸ªå…³é”®è¦ç‚¹ï¼š</p>
<ol>
<li>ä»‡æ¨è¨€è®ºæ£€æµ‹æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†çš„é‡è¦ç ”ç©¶é¢†åŸŸï¼Œå¯¹äºç¡®ä¿åœ¨çº¿ç¤¾åŒºå®‰å…¨è‡³å…³é‡è¦ã€‚</li>
<li>å«è“„ä»‡æ¨è¨€è®ºçš„è¯†åˆ«æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä¾èµ–äºè¯­å¢ƒã€æ–‡åŒ–ç»†å¾®å·®åˆ«å’Œéšå«åè§ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä»‡æ¨è¨€è®ºæ—¶å­˜åœ¨è¿‡åº¦æ•æ„Ÿé—®é¢˜ï¼Œå¯¼è‡´è¯¯æŠ¥å’Œæ¼æŠ¥ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨æ£€æµ‹ä»‡æ¨è¨€è®ºæ—¶å­˜åœ¨ä¸è¶³ï¼Œéœ€è¦æ”¹è¿›æ£€æµ‹ç²¾åº¦å¹¶å‡å°‘æ¨¡å‹åè§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ä»‡æ¨è¨€è®ºæ£€æµ‹æ–¹æ³•ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ æé«˜è¯­å¢ƒç†è§£èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æ£€ç´¢ç›¸ä¼¼æ¼”ç¤ºå†…å®¹ï¼Œå¢å¼ºäº†å¯¹ä¸Šä¸‹æ–‡çš„ç†è§£ï¼Œæé«˜äº†æ£€æµ‹æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12082">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-73c4c23128120f6202433b10c52ed28e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d204ab419c9739e51df9b8e44b63ad37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-094ef00fcaf15dbce371fa440d121f91.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2375f3cbb469299e76c0ba841fed84d7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ADAT-Time-Series-Aware-Adaptive-Transformer-Architecture-for-Sign-Language-Translation"><a href="#ADAT-Time-Series-Aware-Adaptive-Transformer-Architecture-for-Sign-Language-Translation" class="headerlink" title="ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign   Language Translation"></a>ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign   Language Translation</h2><p><strong>Authors:Nada Shahin, Leila Ismail</strong></p>
<p>Current sign language machine translation systems rely on recognizing hand movements, facial expressions and body postures, and natural language processing, to convert signs into text. Recent approaches use Transformer architectures to model long-range dependencies via positional encoding. However, they lack accuracy in recognizing fine-grained, short-range temporal dependencies between gestures captured at high frame rates. Moreover, their high computational complexity leads to inefficient training. To mitigate these issues, we propose an Adaptive Transformer (ADAT), which incorporates components for enhanced feature extraction and adaptive feature weighting through a gating mechanism to emphasize contextually relevant features while reducing training overhead and maintaining translation accuracy. To evaluate ADAT, we introduce MedASL, the first public medical American Sign Language dataset. In sign-to-gloss-to-text experiments, ADAT outperforms the encoder-decoder transformer, improving BLEU-4 accuracy by 0.1% while reducing training time by 14.33% on PHOENIX14T and 3.24% on MedASL. In sign-to-text experiments, it improves accuracy by 8.7% and reduces training time by 2.8% on PHOENIX14T and achieves 4.7% higher accuracy and 7.17% faster training on MedASL. Compared to encoder-only and decoder-only baselines in sign-to-text, ADAT is at least 6.8% more accurate despite being up to 12.1% slower due to its dual-stream structure. </p>
<blockquote>
<p>å½“å‰çš„æ‰‹è¯­æœºå™¨ç¿»è¯‘ç³»ç»Ÿä¾èµ–äºæ‰‹åŠ¿åŠ¨ä½œã€é¢éƒ¨è¡¨æƒ…å’Œèº«ä½“å§¿æ€çš„è¯†åˆ«ä»¥åŠè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œå°†æ‰‹åŠ¿è½¬æ¢ä¸ºæ–‡æœ¬ã€‚æœ€è¿‘çš„æ–¹æ³•ä½¿ç”¨Transformeræ¶æ„é€šè¿‡ä½ç½®ç¼–ç æ¥æ¨¡æ‹Ÿé•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨è¯†åˆ«é«˜å¸§ç‡ä¸‹æ•è·çš„ç²¾ç»†ç²’åº¦ã€çŸ­è·ç¦»æ—¶é—´ä¾èµ–å…³ç³»æ–¹é¢ç¼ºä¹å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œå®ƒä»¬çš„é«˜è®¡ç®—å¤æ‚åº¦å¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”Transformerï¼ˆADATï¼‰ï¼Œå®ƒç»“åˆäº†å¢å¼ºç‰¹å¾æå–å’Œè‡ªé€‚åº”ç‰¹å¾æƒé‡çš„ç»„ä»¶ï¼Œé€šè¿‡é—¨æ§æœºåˆ¶æ¥å¼ºè°ƒä¸Šä¸‹æ–‡ç›¸å…³çš„ç‰¹å¾ï¼ŒåŒæ—¶å‡å°‘è®­ç»ƒå¼€é”€å¹¶ä¿æŒç¿»è¯‘å‡†ç¡®æ€§ã€‚ä¸ºäº†è¯„ä¼°ADATï¼Œæˆ‘ä»¬å¼•å…¥äº†MedASLï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¬å¼€çš„åŒ»ç–—ç¾å¼æ‰‹è¯­æ•°æ®é›†ã€‚åœ¨æ‰‹åŠ¿åˆ°ç¼©ç•¥è¯­å†åˆ°æ–‡æœ¬çš„è¯•éªŒä¸­ï¼ŒADATçš„è¡¨ç°ä¼˜äºç¼–ç å™¨-è§£ç å™¨è½¬æ¢å™¨ï¼Œåœ¨PHOENIX14Tä¸Šæé«˜äº†BLEU-4å‡†ç¡®ç‡0.1%ï¼ŒåŒæ—¶å‡å°‘è®­ç»ƒæ—¶é—´14.33%ï¼Œåœ¨MedASLä¸Šå‡å°‘3.24%ã€‚åœ¨æ‰‹åŠ¿åˆ°æ–‡æœ¬çš„è¯•éªŒä¸­ï¼Œå®ƒåœ¨PHOENIX14Tä¸Šçš„å‡†ç¡®ç‡æé«˜äº†8.7%ï¼Œè®­ç»ƒæ—¶é—´å‡å°‘äº†2.8%ï¼Œåœ¨MedASLä¸Šçš„å‡†ç¡®ç‡æé«˜äº†4.7%ï¼Œè®­ç»ƒé€Ÿåº¦æé«˜äº†7.17%ã€‚ä¸ä»…ç¼–ç å™¨æˆ–ä»…è§£ç å™¨çš„åŸºçº¿ç›¸æ¯”ï¼Œå°½ç®¡ç”±äºADATçš„åŒæµç»“æ„ï¼Œå…¶é€Ÿåº¦è¾ƒæ…¢ï¼ˆæœ€å¤šæ…¢12.1%ï¼‰ï¼Œä½†å…¶å‡†ç¡®æ€§è‡³å°‘æé«˜äº†6.8%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11942v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å½“å‰æ‰‹è¯­æœºå™¨ç¿»è¯‘ç³»ç»Ÿä¸»è¦é€šè¿‡è¯†åˆ«æ‰‹åŠ¿åŠ¨ä½œã€é¢éƒ¨è¡¨æƒ…å’Œä½“æ€ä»¥åŠè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œå°†æ‰‹åŠ¿è½¬åŒ–ä¸ºæ–‡æœ¬ã€‚å°½ç®¡è¿‘æœŸé‡‡ç”¨Transformeræ¶æ„çš„æ–¹æ³•èƒ½å¤Ÿé€šè¿‡ä½ç½®ç¼–ç å»ºç«‹é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œä½†åœ¨è¯†åˆ«ç²¾ç»†çš„çŸ­è·ç¦»æ—¶é—´ä¾èµ–å…³ç³»æ–¹é¢ä»ç¼ºä¹å‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨é«˜å¸§ç‡æ•æ‰çš„æ‰‹åŠ¿ä¹‹é—´ã€‚æ­¤å¤–ï¼Œå…¶é«˜è®¡ç®—å¤æ‚åº¦å¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”Transformerï¼ˆADATï¼‰ï¼Œå®ƒé€šè¿‡å¼•å…¥å¢å¼ºç‰¹å¾æå–å’Œè‡ªé€‚åº”ç‰¹å¾æƒé‡æœºåˆ¶çš„é—¨æ§æœºåˆ¶ï¼Œåœ¨å¼ºè°ƒä¸Šä¸‹æ–‡ç›¸å…³ç‰¹å¾çš„åŒæ—¶ï¼Œå‡å°‘è®­ç»ƒå¼€é”€å¹¶ä¿æŒç¿»è¯‘å‡†ç¡®æ€§ã€‚ä¸ºè¯„ä¼°ADATæ€§èƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¦–ä¸ªå…¬å…±åŒ»ç–—ç¾å›½æ‰‹è¯­æ•°æ®é›†MedASLã€‚å®éªŒè¡¨æ˜ï¼ŒADATåœ¨sign-to-gloss-to-textä»»åŠ¡ä¸­ä¼˜äºç¼–ç å™¨-è§£ç å™¨Transformerï¼Œåœ¨PHOENIX14Tä¸Šæé«˜BLEU-4å‡†ç¡®ç‡0.1%çš„åŒæ—¶å‡å°‘è®­ç»ƒæ—¶é—´14.33%ï¼Œåœ¨MedASLä¸Šå‡å°‘3.24%ã€‚åœ¨sign-to-textå®éªŒä¸­ï¼ŒADATåœ¨PHOENIX14Tä¸Šçš„å‡†ç¡®ç‡æé«˜8.7%ï¼Œè®­ç»ƒæ—¶é—´å‡å°‘2.8%ï¼Œåœ¨MedASLä¸Šçš„å‡†ç¡®ç‡æé«˜4.7%ï¼Œè®­ç»ƒé€Ÿåº¦æé«˜7.17%ã€‚ä¸ç¼–ç å™¨-ä»…è§£ç å™¨åŸºçº¿ç›¸æ¯”ï¼Œå°½ç®¡ç”±äºåŒæµç»“æ„ï¼ŒADATåœ¨é€Ÿåº¦ä¸Šæœ€å¤šæ…¢12.1%ï¼Œä½†å…¶å‡†ç¡®ç‡è‡³å°‘æé«˜6.8%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰æ‰‹è¯­æœºå™¨ç¿»è¯‘ç³»ç»Ÿä¸»è¦ä¾èµ–æ‰‹åŠ¿åŠ¨ä½œã€é¢éƒ¨è¡¨æƒ…å’Œä½“æ€çš„è¯†åˆ«ä»¥åŠè‡ªç„¶è¯­è¨€å¤„ç†è¿›è¡Œç¿»è¯‘ã€‚</li>
<li>å½“å‰ç³»ç»Ÿåœ¨é«˜å¸§ç‡æ•æ‰çš„æ‰‹åŠ¿ä¹‹é—´çš„çŸ­è·ç¦»æ—¶é—´ä¾èµ–å…³ç³»è¯†åˆ«æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>è‡ªé€‚åº”Transformerï¼ˆADATï¼‰é€šè¿‡å¢å¼ºç‰¹å¾æå–å’Œè‡ªé€‚åº”ç‰¹å¾æƒé‡æœºåˆ¶æé«˜äº†ç¿»è¯‘å‡†ç¡®æ€§ã€‚</li>
<li>ADATåœ¨åŒ»ç–—ç¾å›½æ‰‹è¯­æ•°æ®é›†MedASLä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>ADATåœ¨sign-to-gloss-to-textå’Œsign-to-textä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®ç‡å’Œè¾ƒä½çš„è®­ç»ƒæ—¶é—´ã€‚</li>
<li>ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒADATçš„å‡†ç¡®ç‡æœ‰æ‰€æé«˜ï¼Œå°½ç®¡å…¶è®­ç»ƒé€Ÿåº¦æœ‰æ‰€é™ä½ï¼Œè¿™æ˜¯ç”±äºå®ƒçš„åŒæµç»“æ„æ‰€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ecc02afe608f3ddc908d102ea81b423b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a2e3726e71ba9101c7748f83e5c802a0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Federated-Spectral-Graph-Transformers-Meet-Neural-Ordinary-Differential-Equations-for-Non-IID-Graphs"><a href="#Federated-Spectral-Graph-Transformers-Meet-Neural-Ordinary-Differential-Equations-for-Non-IID-Graphs" class="headerlink" title="Federated Spectral Graph Transformers Meet Neural Ordinary Differential   Equations for Non-IID Graphs"></a>Federated Spectral Graph Transformers Meet Neural Ordinary Differential   Equations for Non-IID Graphs</h2><p><strong>Authors:Kishan Gurumurthy, Himanshu Pal, Charu Sharma</strong></p>
<p>Graph Neural Network (GNN) research is rapidly advancing due to GNNsâ€™ capacity to learn distributed representations from graph-structured data. However, centralizing large volumes of real-world graph data for GNN training is often impractical due to privacy concerns, regulatory restrictions, and commercial competition. Federated learning (FL), a distributed learning paradigm, offers a solution by preserving data privacy with collaborative model training. Despite progress in training huge vision and language models, federated learning for GNNs remains underexplored. To address this challenge, we present a novel method for federated learning on GNNs based on spectral GNNs equipped with neural ordinary differential equations (ODE) for better information capture, showing promising results across both homophilic and heterophilic graphs. Our approach effectively handles non-Independent and Identically Distributed (non-IID) data, while also achieving performance comparable to existing methods that only operate on IID data. It is designed to be privacy-preserving and bandwidth-optimized, making it suitable for real-world applications such as social network analysis, recommendation systems, and fraud detection, which often involve complex, non-IID, and heterophilic graph structures. Our results in the area of federated learning on non-IID heterophilic graphs demonstrate significant improvements, while also achieving better performance on homophilic graphs. This work highlights the potential of federated learning in diverse and challenging graph settings. Open-source code available on GitHub (<a target="_blank" rel="noopener" href="https://github.com/SpringWiz11/Fed-GNODEFormer">https://github.com/SpringWiz11/Fed-GNODEFormer</a>). </p>
<blockquote>
<p>å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ç ”ç©¶æ­£åœ¨å¿«é€Ÿè¿›æ­¥ï¼Œå› ä¸ºå…¶å…·å¤‡ä»å›¾ç»“æ„æ•°æ®ä¸­å­¦ä¹ åˆ†å¸ƒå¼è¡¨ç¤ºçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºéšç§æ‹…å¿§ã€ç›‘ç®¡é™åˆ¶å’Œå•†ä¸šç«äº‰ï¼Œé›†ä¸­å¤§é‡ç°å®ä¸–ç•Œå›¾æ•°æ®è¿›è¡ŒGNNè®­ç»ƒé€šå¸¸ä¸åˆ‡å®é™…ã€‚è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æ˜¯ä¸€ç§åˆ†å¸ƒå¼å­¦ä¹ èŒƒå¼ï¼Œé€šè¿‡åä½œæ¨¡å‹è®­ç»ƒæ¥ä¿ç•™æ•°æ®éšç§ï¼Œä¸ºè§£å†³æ­¤é—®é¢˜æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚å°½ç®¡åœ¨è®­ç»ƒå¤§å‹è§†è§‰å’Œè¯­è¨€æ¨¡å‹æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†GNNçš„è”é‚¦å­¦ä¹ ä»ç„¶é²œæœ‰ç ”ç©¶ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé…å¤‡ç¥ç»ç½‘ç»œå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰è¿›è¡Œæ›´å¥½ä¿¡æ¯æ•è·çš„è°±å›¾ç¥ç»ç½‘ç»œçš„è”é‚¦å­¦ä¹ æ–°æ–¹æ³•ï¼Œåœ¨åŒæ„å›¾å’Œå¼‚æ„å›¾ä¸­éƒ½æ˜¾ç¤ºå‡ºæœ‰å‰é€”çš„ç»“æœã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰æ•°æ®ï¼ŒåŒæ—¶åœ¨æ€§èƒ½ä¸Šå¯ä»¥ä¸ä»…å¤„ç†IIDæ•°æ®çš„æ–¹æ³•ç›¸æ¯”ã€‚å®ƒè¢«è®¾è®¡ä¸ºå…·æœ‰éšç§ä¿æŠ¤å’Œå¸¦å®½ä¼˜åŒ–çš„ç‰¹ç‚¹ï¼Œéå¸¸é€‚åˆäºç¤¾äº¤ç½‘ç»œåˆ†æã€æ¨èç³»ç»Ÿå’Œæ¬ºè¯ˆæ£€æµ‹ç­‰ç°å®ä¸–ç•Œåº”ç”¨ï¼Œè¿™äº›åº”ç”¨é€šå¸¸æ¶‰åŠå¤æ‚ã€éIIDå’Œå¼‚æ„å›¾ç»“æ„ã€‚æˆ‘ä»¬åœ¨éIIDå¼‚æ„å›¾ä¸Šçš„è”é‚¦å­¦ä¹ ç»“æœè¯æ˜äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ŒåŒæ—¶åœ¨åŒæ„å›¾ä¸Šä¹Ÿå®ç°äº†æ›´å¥½çš„æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†è”é‚¦å­¦ä¹ åœ¨å¤šæ ·åŒ–å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„å›¾è®¾ç½®ä¸­çš„æ½œåŠ›ã€‚å¼€æºä»£ç å¯åœ¨GitHubä¸Šè·å¾—ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/SpringWiz11/Fed-GNODEFormer%EF%BC%89%E3%80%82">https://github.com/SpringWiz11/Fed-GNODEFormerï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11808v1">PDF</a> The first two listed authors contributed equally to this work</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰å­¦ä¹ åˆ†å¸ƒå¼è¡¨ç¤ºå›¾ç»“æ„æ•°æ®çš„èƒ½åŠ›ï¼Œå…¶ç ”ç©¶æ­£åœ¨å¿«é€Ÿå‘å±•ã€‚ç„¶è€Œï¼Œç”±äºéšç§æ‹…å¿§ã€ç›‘ç®¡é™åˆ¶å’Œå•†ä¸šç«äº‰ï¼Œé›†ä¸­å¤§é‡ç°å®ä¸–ç•Œå›¾æ•°æ®è¿›è¡ŒGNNè®­ç»ƒå¹¶ä¸å®é™…ã€‚è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ä½œä¸ºä¸€ç§åˆ†å¸ƒå¼å­¦ä¹ èŒƒå¼ï¼Œé€šè¿‡ååŒæ¨¡å‹è®­ç»ƒæ¥ä¿å­˜æ•°æ®éšç§ï¼Œä¸ºè§£å†³æ­¤é—®é¢˜æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚å°½ç®¡åœ¨è®­ç»ƒå¤§è§„æ¨¡è§†è§‰å’Œè¯­è¨€æ¨¡å‹æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†é’ˆå¯¹GNNçš„è”é‚¦å­¦ä¹ ä»ç„¶æ¢ç´¢ä¸è¶³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå…‰è°±GNNå’Œé…å¤‡ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰çš„è”é‚¦å­¦ä¹ æ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°æ•è·ä¿¡æ¯ï¼Œå¹¶åœ¨åŒæ„å’Œå¼‚æ„å›¾ä¸Šæ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœã€‚è¯¥æ–¹æ³•æœ‰æ•ˆå¤„ç†äº†éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰æ•°æ®ï¼ŒåŒæ—¶å®ç°äº†ä¸ä»…å¤„ç†IIDæ•°æ®çš„ç°æœ‰æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚è®¾è®¡ç”¨äºä¿å­˜éšç§å’Œä¼˜åŒ–å¸¦å®½ï¼Œé€‚ç”¨äºç¤¾äº¤ç½‘ç»œåˆ†æã€æ¨èç³»ç»Ÿå’Œæ¬ºè¯ˆæ£€æµ‹ç­‰æ¶‰åŠå¤æ‚ã€éIIDå’Œå¼‚æ„å›¾ç»“æ„çš„ç°å®ä¸–ç•Œåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GNNå…·æœ‰ä»å›¾ç»“æ„æ•°æ®ä¸­å­¦ä¹ åˆ†å¸ƒå¼è¡¨ç¤ºçš„èƒ½åŠ›ï¼Œä½†å…¶è®­ç»ƒé¢ä¸´æ•°æ®é›†ä¸­åŒ–çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬éšç§ã€ç›‘ç®¡å’Œå•†ä¸šç«äº‰é—®é¢˜ã€‚</li>
<li>è”é‚¦å­¦ä¹ æ˜¯åˆ†å¸ƒå¼å­¦ä¹ çš„ä¸€ç§èŒƒå¼ï¼Œèƒ½å¤Ÿè§£å†³æ•°æ®éšç§é—®é¢˜ï¼ŒåŒæ—¶ä¸ºGNNè®­ç»ƒæä¾›è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å°½ç®¡è”é‚¦å­¦ä¹ åœ¨è§†è§‰å’Œè¯­è¨€æ¨¡å‹æ–¹é¢æœ‰æ‰€è¿›å±•ï¼Œä½†åœ¨GNNä¸Šçš„è”é‚¦å­¦ä¹ ä»ç„¶æ˜¯ä¸€ä¸ªæœªè¢«å……åˆ†æ¢ç´¢çš„é¢†åŸŸã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è”é‚¦å­¦ä¹ æ–¹æ³•ï¼ŒåŸºäºå…‰è°±GNNå’Œç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰ï¼Œåœ¨åŒæ„å’Œå¼‚æ„å›¾ä¸Šå‡è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰æ•ˆå¤„ç†non-IIDæ•°æ®ï¼Œå¹¶å®ç°ä¸ä»…å¤„ç†IIDæ•°æ®çš„ç°æœ‰æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚</li>
<li>è®¾è®¡çš„éšç§ä¿æŠ¤å’Œå¸¦å®½ä¼˜åŒ–ä½¿å…¶é€‚ç”¨äºå¤šç§æ¶‰åŠå¤æ‚ã€éIIDå’Œå¼‚æ„å›¾ç»“æ„çš„ç°å®ä¸–ç•Œåº”ç”¨ï¼Œå¦‚ç¤¾äº¤ç½‘ç»œåˆ†æã€æ¨èç³»ç»Ÿå’Œæ¬ºè¯ˆæ£€æµ‹ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11808">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ebc86c7b7ee09e4eb45fde12282bf7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ea5a6b3ebfc895a8c65c69c9c46432d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Can-GPT-tell-us-why-these-images-are-synthesized-Empowering-Multimodal-Large-Language-Models-for-Forensics"><a href="#Can-GPT-tell-us-why-these-images-are-synthesized-Empowering-Multimodal-Large-Language-Models-for-Forensics" class="headerlink" title="Can GPT tell us why these images are synthesized? Empowering Multimodal   Large Language Models for Forensics"></a>Can GPT tell us why these images are synthesized? Empowering Multimodal   Large Language Models for Forensics</h2><p><strong>Authors:Yiran He, Yun Cao, Bowen Yang, Zeyu Zhang</strong></p>
<p>The rapid development of generative AI facilitates content creation and makes image manipulation easier and more difficult to detect. While multimodal Large Language Models (LLMs) have encoded rich world knowledge, they are not inherently tailored for combating AI-generated Content (AIGC) and struggle to comprehend local forgery details. In this work, we investigate the application of multimodal LLMs in forgery detection. We propose a framework capable of evaluating image authenticity, localizing tampered regions, providing evidence, and tracing generation methods based on semantic tampering clues. Our method demonstrates that the potential of LLMs in forgery analysis can be effectively unlocked through meticulous prompt engineering and the application of few-shot learning techniques. We conduct qualitative and quantitative experiments and show that GPT4V can achieve an accuracy of 92.1% in Autosplice and 86.3% in LaMa, which is competitive with state-of-the-art AIGC detection methods. We further discuss the limitations of multimodal LLMs in such tasks and propose potential improvements. </p>
<blockquote>
<p>éšç€ç”Ÿæˆæ€§AIçš„å¿«é€Ÿå‘å±•ï¼Œå†…å®¹åˆ›å»ºå˜å¾—æ›´åŠ å®¹æ˜“ï¼Œå›¾åƒæ“ä½œä¹Ÿå˜å¾—æ›´ç®€å•ä¸”æ›´éš¾æ£€æµ‹ã€‚è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»ç¼–ç äº†ä¸°å¯Œçš„ä¸–ç•ŒçŸ¥è¯†ï¼Œä½†å®ƒä»¬å¹¶éå¤©ç”Ÿå°±é€‚åˆå¯¹æŠ—AIç”Ÿæˆçš„å†…å®¹ï¼ˆAIGCï¼‰ï¼Œåœ¨ç†è§£å±€éƒ¨ä¼ªé€ ç»†èŠ‚æ–¹é¢å­˜åœ¨å›°éš¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¤šæ¨¡æ€LLMåœ¨ä¼ªé€ æ£€æµ‹ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªèƒ½å¤Ÿè¯„ä¼°å›¾åƒçœŸå®æ€§ã€å®šä½ç¯¡æ”¹åŒºåŸŸã€æä¾›è¯æ®å¹¶æ ¹æ®è¯­ä¹‰ç¯¡æ”¹çº¿ç´¢è¿½è¸ªç”Ÿæˆæ–¹æ³•çš„æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¡¨æ˜ï¼Œé€šè¿‡ç²¾ç»†çš„æç¤ºå·¥ç¨‹å’Œå°‘é‡å­¦ä¹ æŠ€æœ¯çš„åº”ç”¨ï¼ŒLLMåœ¨ä¼ªé€ åˆ†æä¸­çš„æ½œåŠ›å¯ä»¥å¾—åˆ°æœ‰æ•ˆè§£é”ã€‚æˆ‘ä»¬è¿›è¡Œäº†å®šæ€§å’Œå®šé‡å®éªŒï¼Œç»“æœè¡¨æ˜GPT4Våœ¨Autospliceä¸­å¯ä»¥è¾¾åˆ°92.1%çš„å‡†ç¡®ç‡ï¼Œåœ¨LaMaä¸­å¯ä»¥è¾¾åˆ°86.3%ï¼Œä¸æœ€å…ˆè¿›çš„AIGCæ£€æµ‹æ–¹æ³•ç›¸å½“ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è®¨è®ºäº†å¤šæ¨¡æ€LLMåœ¨æ­¤ç±»ä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†æ½œåœ¨çš„æ”¹è¿›æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11686v1">PDF</a> 12 pages, 11 figures, 13IHMMSec2025</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¼ªé€ æ£€æµ‹ä¸­çš„åº”ç”¨ã€‚æå‡ºäº†ä¸€ç§åŸºäºè¯­ä¹‰ç¯¡æ”¹çº¿ç´¢çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿè¯„ä¼°å›¾åƒçœŸå®æ€§ã€å®šä½ç¯¡æ”¹åŒºåŸŸã€æä¾›è¯æ®å¹¶è¿½æº¯ç”Ÿæˆæ–¹æ³•ã€‚è¯¥ç ”ç©¶é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºå’Œå°‘é‡å­¦ä¹ æŠ€æœ¯ï¼Œå±•ç¤ºäº†LLMsåœ¨ä¼ªé€ åˆ†æä¸­çš„æ½œåŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒGPT4Våœ¨Autospliceå’ŒLaMaä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°äº†92.1%å’Œ86.3%ï¼Œå…·æœ‰ç«äº‰åŠ›ã€‚åŒæ—¶ï¼Œä¹Ÿè®¨è®ºäº†å¤šæ¨¡æ€LLMåœ¨æ­¤ç±»ä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†æ½œåœ¨çš„æ”¹è¿›æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¼ªé€ æ£€æµ‹ä¸­å…·æœ‰åº”ç”¨æ½œåŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§èƒ½å¤Ÿè¯„ä¼°å›¾åƒçœŸå®æ€§ã€å®šä½ç¯¡æ”¹åŒºåŸŸã€æä¾›è¯æ®å¹¶è¿½æº¯ç”Ÿæˆæ–¹æ³•çš„æ¡†æ¶ã€‚</li>
<li>é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºå’Œå°‘é‡å­¦ä¹ æŠ€æœ¯ï¼ŒLLMsåœ¨ä¼ªé€ åˆ†æä¸­çš„è¡¨ç°å¾—åˆ°æœ‰æ•ˆæå‡ã€‚</li>
<li>GPT4Våœ¨Autospliceå’ŒLaMaä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡è¾ƒé«˜ï¼Œè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>å¤šæ¨¡æ€LLMåœ¨ä¼ªé€ æ£€æµ‹ä»»åŠ¡ä¸­ä»å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶å’Œæ”¹è¿›å¤šæ¨¡æ€LLMçš„æ½œåŠ›ï¼Œä»¥æé«˜å…¶åœ¨ä¼ªé€ æ£€æµ‹ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4abedfe9546d57e5537f0ab084997771.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5746052502c346a60b819fd9fc885c45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ec1c902c01e3376c3dd32a071341e00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9148f7226d866d1dbc8494045216b23e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e3e59f057c0424dfece41963c87be87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-980ed3dd2866d5a11dbb58f46195addb.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="GPT-Meets-Graphs-and-KAN-Splines-Testing-Novel-Frameworks-on-Multitask-Fine-Tuned-GPT-2-with-LoRA"><a href="#GPT-Meets-Graphs-and-KAN-Splines-Testing-Novel-Frameworks-on-Multitask-Fine-Tuned-GPT-2-with-LoRA" class="headerlink" title="GPT Meets Graphs and KAN Splines: Testing Novel Frameworks on Multitask   Fine-Tuned GPT-2 with LoRA"></a>GPT Meets Graphs and KAN Splines: Testing Novel Frameworks on Multitask   Fine-Tuned GPT-2 with LoRA</h2><p><strong>Authors:Gabriel Bo, Marc Bernardino, Justin Gu</strong></p>
<p>We explore the potential of integrating learnable and interpretable modulesâ€“specifically Kolmogorov-Arnold Networks (KAN) and graph-based representationsâ€“within a pre-trained GPT-2 model to enhance multi-task learning accuracy. Motivated by the recent surge in using KAN and graph attention (GAT) architectures in chain-of-thought (CoT) models and debates over their benefits compared to simpler architectures like MLPs, we begin by enhancing a standard self-attention transformer using Low-Rank Adaptation (LoRA), fine-tuning hyperparameters, and incorporating L2 regularization. This approach yields significant improvements. To further boost interpretability and richer representations, we develop two variants that attempt to improve the standard KAN and GAT: Graph LoRA and Hybrid-KAN LoRA (Learnable GPT). However, systematic evaluations reveal that neither variant outperforms the optimized LoRA-enhanced transformer, which achieves 55.249% accuracy on the SST test set, 99.18% on the CFIMDB dev set, and 89.9% paraphrase detection test accuracy. On sonnet generation, we get a CHRF score of 42.097. These findings highlight that efficient parameter adaptation via LoRA remains the most effective strategy for our tasks: sentiment analysis, paraphrase detection, and sonnet generation. </p>
<blockquote>
<p>æˆ‘ä»¬æ¢ç´¢äº†åœ¨é¢„è®­ç»ƒçš„GPT-2æ¨¡å‹ä¸­é›†æˆå¯å­¦ä¹ å’Œå¯è§£é‡Šæ¨¡å—ï¼ˆç‰¹åˆ«æ˜¯Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰å’ŒåŸºäºå›¾çš„è¡¨ç¤ºï¼‰çš„æ½œåŠ›ï¼Œä»¥æé«˜å¤šä»»åŠ¡å­¦ä¹ çš„å‡†ç¡®æ€§ã€‚å—æœ€è¿‘æ€ç»´é“¾æ¨¡å‹ä¸­ç§‘å°”è«å“¥ç½—å¤«-é˜¿è¯ºå°”å¾·ç½‘ç»œï¼ˆKANï¼‰å’Œå›¾æ³¨æ„åŠ›ï¼ˆGATï¼‰æ¶æ„çš„æ¿€å¢ä»¥åŠå®ƒä»¬ä¸ç®€å•æ¶æ„ï¼ˆå¦‚å¤šå±‚æ„ŸçŸ¥å™¨ï¼‰ç›¸æ¯”çš„è¾©è®ºçš„å¯å‘ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å¢å¼ºæ ‡å‡†è‡ªæ³¨æ„åŠ›å˜æ¢å™¨ï¼Œå¾®è°ƒè¶…å‚æ•°ï¼Œå¹¶å¼•å…¥L2æ­£åˆ™åŒ–ã€‚è¿™ç§æ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å¯è§£é‡Šæ€§å’Œæ›´ä¸°å¯Œçš„è¡¨ç¤ºå½¢å¼ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸¤ä¸ªå˜ä½“ï¼Œè¯•å›¾æ”¹è¿›æ ‡å‡†çš„KANå’ŒGATï¼šå›¾LoRAå’Œæ··åˆKAN LoRAï¼ˆå­¦ä¹ å‹GPTï¼‰ã€‚ç„¶è€Œï¼Œç³»ç»Ÿè¯„ä¼°è¡¨æ˜ï¼Œä¼˜åŒ–åçš„LoRAå¢å¼ºè½¬æ¢å™¨åœ¨å„é¡¹ä»»åŠ¡ä¸­çš„è¡¨ç°è¶…è¿‡äº†è¿™ä¸¤ç§å˜ä½“ã€‚åœ¨SSTæµ‹è¯•é›†ä¸Šå–å¾—äº†55.249%çš„å‡†ç¡®ç‡ï¼Œåœ¨CFIMDBå¼€å‘é›†ä¸Šè¾¾åˆ°äº†99.18%ï¼Œåœ¨è½¬è¿°æ£€æµ‹æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†89.9%çš„å‡†ç¡®ç‡ã€‚åœ¨é¢‚è¯—ç”Ÿæˆæ–¹é¢ï¼Œæˆ‘ä»¬è·å¾—äº†CHRFåˆ†æ•°ä¸º42.097ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œé€šè¿‡LoRAè¿›è¡Œæœ‰æ•ˆçš„å‚æ•°é€‚åº”ä»ç„¶æ˜¯æˆ‘ä»¬ä»»åŠ¡ä¸­æœ€æœ‰æ•ˆçš„ç­–ç•¥ï¼šæƒ…æ„Ÿåˆ†æã€è½¬è¿°æ£€æµ‹å’Œé¢‚è¯—ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10490v1">PDF</a> 10 pages, 11 figures. This submission cites arXiv:2404.19756.   Supplementary materials and additional information are available at   arXiv:2404.19756</p>
<p><strong>Summary</strong></p>
<p>åœ¨é¢„è®­ç»ƒçš„GPT-2æ¨¡å‹ä¸­é›†æˆå¯å­¦ä¹ å’Œå¯è§£é‡Šçš„æ¨¡å—ï¼ˆå¦‚Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰å’ŒåŸºäºå›¾çš„è¡¨ç¤ºï¼‰ï¼Œä»¥å¢å¼ºå¤šä»»åŠ¡å­¦ä¹ çš„å‡†ç¡®æ€§ã€‚é€šè¿‡é‡‡ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å¢å¼ºæ ‡å‡†è‡ªæ³¨æ„åŠ›è½¬æ¢å™¨ï¼Œå¾®è°ƒè¶…å‚æ•°å¹¶å¼•å…¥L2æ­£åˆ™åŒ–ï¼Œå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ç„¶è€Œï¼Œç³»ç»Ÿæ€§è¯„ä¼°è¡¨æ˜ï¼Œä¼˜åŒ–åçš„LoRAå¢å¼ºè½¬æ¢å™¨è¡¨ç°æœ€ä½³ï¼Œè€Œå…¶ä»–å°è¯•æ”¹è¿›KANå’ŒGATçš„å˜ä½“å¹¶æœªè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¢è®¨äº†å°†Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰å’ŒåŸºäºå›¾çš„è¡¨ç¤ºé›†æˆåˆ°é¢„è®­ç»ƒGPT-2æ¨¡å‹ä¸­ï¼Œä»¥æé«˜å¤šä»»åŠ¡å­¦ä¹ å‡†ç¡®æ€§çš„æ½œåŠ›ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯å¢å¼ºäº†æ ‡å‡†è‡ªæ³¨æ„åŠ›è½¬æ¢å™¨ã€‚</li>
<li>é€šè¿‡å¾®è°ƒè¶…å‚æ•°å¹¶å¼•å…¥L2æ­£åˆ™åŒ–ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚</li>
<li>å¼€å‘äº†ä¸¤ç§æ”¹è¿›KANå’ŒGATçš„å˜ä½“ï¼Œä½†ç³»ç»Ÿæ€§è¯„ä¼°è¡¨æ˜ï¼Œè¿™äº›å˜ä½“å¹¶æœªè¡¨ç°å‡ºæ¯”ä¼˜åŒ–åçš„LoRAå¢å¼ºè½¬æ¢å™¨æ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>LoRAå¢å¼ºè½¬æ¢å™¨åœ¨æƒ…æ„Ÿåˆ†æã€è¯­å¥å¤è¿°å’Œåå››è¡Œè¯—ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚</li>
<li>åœ¨SSTæµ‹è¯•é›†ä¸Šå®ç°äº†55.249%çš„å‡†ç¡®ç‡ï¼Œåœ¨CFIMDBå¼€å‘é›†ä¸Šå®ç°äº†99.18%çš„å‡†ç¡®ç‡ï¼Œåœ¨è¯­å¥å¤è¿°æµ‹è¯•é›†ä¸Šå®ç°äº†89.9%çš„å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10490">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8cce2c099a70a571d191e4e869fb6edd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4ca048dca1f4c6db972a7f1980a4f6bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2516bedb046f951e52488860f0909fa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98da8178c4f7e3222234af122482e826.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0a3a1cc696db8f206559de438f207c3.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="LLM-Unlearning-Reveals-a-Stronger-Than-Expected-Coreset-Effect-in-Current-Benchmarks"><a href="#LLM-Unlearning-Reveals-a-Stronger-Than-Expected-Coreset-Effect-in-Current-Benchmarks" class="headerlink" title="LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in   Current Benchmarks"></a>LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in   Current Benchmarks</h2><p><strong>Authors:Soumyadeep Pal, Changsheng Wang, James Diffenderfer, Bhavya Kailkhura, Sijia Liu</strong></p>
<p>Large language model unlearning has become a critical challenge in ensuring safety and controlled model behavior by removing undesired data-model influences from the pretrained model while preserving general utility. Significant recent efforts have been dedicated to developing LLM unlearning benchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine Unlearning Six-way Evaluation), facilitating standardized unlearning performance assessment and method comparison. Despite their usefulness, we uncover for the first time a novel coreset effect within these benchmarks. Specifically, we find that LLM unlearning achieved with the original (full) forget set can be effectively maintained using a significantly smaller subset (functioning as a â€œcoresetâ€), e.g., as little as 5% of the forget set, even when selected at random. This suggests that LLM unlearning in these benchmarks can be performed surprisingly easily, even in an extremely low-data regime. We demonstrate that this coreset effect remains strong, regardless of the LLM unlearning method used, such as NPO (Negative Preference Optimization) and RMU (Representation Misdirection Unlearning), the popular ones in these benchmarks. The surprisingly strong coreset effect is also robust across various data selection methods, ranging from random selection to more sophisticated heuristic approaches. We explain the coreset effect in LLM unlearning through a keyword-based perspective, showing that keywords extracted from the forget set alone contribute significantly to unlearning effectiveness and indicating that current unlearning is driven by a compact set of high-impact tokens rather than the entire dataset. We further justify the faithfulness of coreset-unlearned models along additional dimensions, such as mode connectivity and robustness to jailbreaking attacks. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/OPTML-Group/MU-Coreset">https://github.com/OPTML-Group/MU-Coreset</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„å»é™¤å­¦ä¹ ï¼ˆunlearningï¼‰å·²ç»æˆä¸ºç¡®ä¿æ¨¡å‹å®‰å…¨å’Œå¯æ§æ€§çš„é‡è¦æŒ‘æˆ˜ã€‚é€šè¿‡ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­ç§»é™¤ä¸å¿…è¦çš„æ•°æ®æ¨¡å‹å½±å“ï¼ŒåŒæ—¶ä¿ç•™å…¶é€šç”¨æ•ˆç”¨ï¼Œæ¥å®ç°å¯¹æ¨¡å‹è¡Œä¸ºçš„æ§åˆ¶ã€‚è¿‘æœŸï¼Œäººä»¬å·²ç»æŠ•å…¥å¤§é‡åŠªåŠ›æ¥å¼€å‘å¤§å‹è¯­è¨€æ¨¡å‹çš„å»é™¤å­¦ä¹ åŸºå‡†æµ‹è¯•ï¼Œä¾‹å¦‚å¤§è§„æ¨¡æ€ä¼¤æ­¦å™¨ä»£ç†ï¼ˆWMDPï¼‰å’ŒéŸ³ä¹å¤§èµ›æœºå™¨å»é™¤å­¦ä¹ è¯„ä¼°ï¼ˆMUSEï¼‰ï¼Œä»¥ä¿ƒè¿›æ ‡å‡†åŒ–çš„å»é™¤å­¦ä¹ æ€§èƒ½è¯„ä¼°å’Œæ–¹æ³•çš„æ¯”è¾ƒã€‚ç„¶è€Œï¼Œå°½ç®¡è¿™äº›åŸºå‡†æµ‹è¯•å¾ˆæœ‰ç”¨ï¼Œæˆ‘ä»¬é¦–æ¬¡å‘ç°äº†å…¶ä¸­çš„ä¸€ç§æ–°å‹æ ¸å¿ƒé›†æ•ˆåº”ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å‘ç°ä½¿ç”¨åŸå§‹ï¼ˆå®Œæ•´ï¼‰é—å¿˜é›†å®ç°çš„å¤§å‹è¯­è¨€æ¨¡å‹å»é™¤å­¦ä¹ å¯ä»¥é€šè¿‡ä¸€ä¸ªæ˜¾è‘—è¾ƒå°çš„å­é›†ï¼ˆå……å½“â€œæ ¸å¿ƒé›†â€ï¼‰æ¥æœ‰æ•ˆç»´æŒï¼Œä¾‹å¦‚ï¼Œå³ä½¿åªé€‰æ‹©é—å¿˜é›†çš„5%ï¼Œå³ä½¿æ˜¯éšæœºé€‰æ‹©çš„ã€‚è¿™è¡¨æ˜ï¼Œå³ä½¿åœ¨æç«¯ä½æ•°æ®æƒ…å†µä¸‹ï¼Œè¿™äº›åŸºå‡†æµ‹è¯•ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹å»é™¤å­¦ä¹ ä¹Ÿå¯ä»¥å‡ºä¹æ„æ–™åœ°è½»æ¾å®ç°ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæ— è®ºä½¿ç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹å»é™¤æ–¹æ³•å¦‚ä½•ï¼Œè¿™ç§æ ¸å¿ƒé›†æ•ˆåº”ä¾ç„¶å¼ºçƒˆå­˜åœ¨ï¼Œå¦‚è´Ÿé¢åå¥½ä¼˜åŒ–ï¼ˆNPOï¼‰å’Œè¡¨ç¤ºè¯¯å¯¼å»é™¤ï¼ˆRMUï¼‰ç­‰ï¼Œåœ¨åŸºå‡†æµ‹è¯•ä¸­éƒ½å¾ˆå—æ¬¢è¿ã€‚å‡ºä¹æ„æ–™åœ°å¼ºçƒˆçš„æ ¸å¿ƒé›†æ•ˆåº”åœ¨å„ç§æ•°æ®é€‰æ‹©æ–¹æ³•ä¸­ä¹Ÿå¾ˆç¨³å¥ï¼Œä»éšæœºé€‰æ‹©åˆ°æ›´å¤æ‚çš„å¯å‘å¼æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡å…³é”®è¯è§’åº¦æ¥è§£é‡Šå¤§å‹è¯­è¨€æ¨¡å‹å»é™¤å­¦ä¹ ä¸­çš„æ ¸å¿ƒé›†æ•ˆåº”ï¼Œè¡¨æ˜ä»…ä»é—å¿˜é›†ä¸­æå–çš„å…³é”®è¯å¯¹å»é™¤å­¦ä¹ çš„æœ‰æ•ˆæ€§è´¡çŒ®é‡å¤§ï¼Œå¹¶æŒ‡ç¤ºå½“å‰çš„å»é™¤å­¦ä¹ æ˜¯ç”±ä¸€ç»„å…·æœ‰é‡å¤§å½±å“çš„ä»¤ç‰Œé©±åŠ¨çš„ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜ä»å…¶ä»–ç»´åº¦ï¼ˆå¦‚æ¨¡å¼è¿é€šæ€§å’Œå¯¹è¶Šç‹±æ”»å‡»çš„ç¨³å®šæ€§ï¼‰è¿›ä¸€æ­¥è¯æ˜äº†æ ¸å¿ƒé›†å»é™¤æ¨¡å‹çš„å¿ å®æ€§ã€‚ç›¸å…³ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/OPTML-Group/MU-Coreset">https://github.com/OPTML-Group/MU-Coreset</a> ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10185v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„é—å¿˜å­¦ä¹ åœ¨ä¿éšœæ¨¡å‹å®‰å…¨å’Œå¯æ§æ€§æ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚æœ€æ–°ç ”ç©¶æ˜¾ç¤ºï¼Œé—å¿˜å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­çš„æ ¸å¿ƒé›†æ•ˆåº”å¯¹äºé™ä½å·¥ä½œé‡æœ‰ç§¯æå½±å“ï¼Œåªéœ€ä½¿ç”¨ä¸€å°éƒ¨åˆ†é—å¿˜é›†å°±èƒ½å®ç°æœ‰æ•ˆçš„é—å¿˜å­¦ä¹ ï¼Œç”šè‡³å¯ä»¥ä½è‡³åŸå§‹é—å¿˜é›†çš„5%ã€‚è¿™æ„å‘³ç€å³ä½¿åœ¨æä½æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„é—å¿˜å­¦ä¹ ä¹Ÿèƒ½è½»æ¾å®ç°ã€‚è¿™ä¸€å‘ç°å¯¹äºæœªæ¥çš„è¯­è¨€æ¨¡å‹é—å¿˜å­¦ä¹ å…·æœ‰é‡è¦å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é—å¿˜å­¦ä¹ å¯¹æ¨¡å‹å®‰å…¨å’Œå¯æ§æ€§è‡³å…³é‡è¦ã€‚</li>
<li>æ–°çš„ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨é—å¿˜é›†çš„å°éƒ¨åˆ†å­é›†ï¼ˆæ ¸å¿ƒé›†ï¼‰å³å¯å®ç°æœ‰æ•ˆçš„é—å¿˜å­¦ä¹ ã€‚</li>
<li>æ ¸å¿ƒé›†æ•ˆåº”åœ¨å¤šç§è¯­è¨€æ¨¡å‹é—å¿˜å­¦ä¹ æ–¹æ³•ä¸­æ™®éå­˜åœ¨ï¼ŒåŒ…æ‹¬NPOå’ŒRMUç­‰æ–¹æ³•ã€‚</li>
<li>å…³é”®è¯åœ¨é—å¿˜å­¦ä¹ ä¸­èµ·ç€é‡è¦ä½œç”¨ï¼Œè¡¨æ˜å½“å‰é—å¿˜å­¦ä¹ ä¸»è¦ç”±ä¸€å°éƒ¨åˆ†é«˜å½±å“åŠ›è¯æ±‡é©±åŠ¨ã€‚</li>
<li>æ ¸å¿ƒé›†é—å¿˜å­¦ä¹ çš„æœ‰æ•ˆæ€§åœ¨å„ç§æ•°æ®é€‰æ‹©æ–¹æ³•ä¸­å¾—åˆ°éªŒè¯ï¼Œä»éšæœºé€‰æ‹©åˆ°æ›´å¤æ‚çš„å¯å‘å¼æ–¹æ³•å‡é€‚ç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10185">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-593fff102787686b7abd93d4a12ee203.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e0cebb0ad3cca96e37d16f7c7242b7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb2225e552c46adc77adda5b111baa9c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Understanding-and-Optimizing-Multi-Stage-AI-Inference-Pipelines"><a href="#Understanding-and-Optimizing-Multi-Stage-AI-Inference-Pipelines" class="headerlink" title="Understanding and Optimizing Multi-Stage AI Inference Pipelines"></a>Understanding and Optimizing Multi-Stage AI Inference Pipelines</h2><p><strong>Authors:Abhimanyu Rajeshkumar Bambhaniya, Hanjiang Wu, Suvinay Subramanian, Sudarshan Srinivasan, Souvik Kundu, Amir Yazdanbakhsh, Midhilesh Elavazhagan, Madhu Kumar, Tushar Krishna</strong></p>
<p>The rapid evolution of Large Language Models (LLMs) has driven the need for increasingly sophisticated inference pipelines and hardware platforms. Modern LLM serving extends beyond traditional prefill-decode workflows, incorporating multi-stage processes such as Retrieval Augmented Generation (RAG), key-value (KV) cache retrieval, dynamic model routing, and multi step reasoning. These stages exhibit diverse computational demands, requiring distributed systems that integrate GPUs, ASICs, CPUs, and memory-centric architectures. However, existing simulators lack the fidelity to model these heterogeneous, multi-engine workflows, limiting their ability to inform architectural decisions.   To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM inference Execution Simulator. HERMES models diverse request stages; including RAG, KV retrieval, reasoning, prefill, and decode across complex hardware hierarchies. HERMES supports heterogeneous clients executing multiple models concurrently unlike prior frameworks while incorporating advanced batching strategies and multi-level memory hierarchies. By integrating real hardware traces with analytical modeling, HERMES captures critical trade-offs such as memory bandwidth contention, inter-cluster communication latency, and batching efficiency in hybrid CPU-accelerator deployments. Through case studies, we explore the impact of reasoning stages on end-to-end latency, optimal batching strategies for hybrid pipelines, and the architectural implications of remote KV cache retrieval. HERMES empowers system designers to navigate the evolving landscape of LLM inference, providing actionable insights into optimizing hardware-software co-design for next-generation AI workloads. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†è¶Šæ¥è¶Šç²¾ç»†çš„æ¨ç†ç®¡é“å’Œç¡¬ä»¶å¹³å°çš„éœ€æ±‚ã€‚ç°ä»£LLMæœåŠ¡è¶…è¶Šäº†ä¼ ç»Ÿçš„é¢„å¡«å……-è§£ç å·¥ä½œæµç¨‹ï¼Œèå…¥äº†å¤šé˜¶æ®µæµç¨‹ï¼Œå¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜æ£€ç´¢ã€åŠ¨æ€æ¨¡å‹è·¯ç”±å’Œå¤šæ­¥æ¨ç†ã€‚è¿™äº›é˜¶æ®µè¡¨ç°å‡ºå¤šæ ·åŒ–çš„è®¡ç®—éœ€æ±‚ï¼Œéœ€è¦æ•´åˆGPUã€ASICã€CPUå’Œå†…å­˜ä¸­å¿ƒçš„åˆ†å¸ƒå¼ç³»ç»Ÿæ¶æ„ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¨¡æ‹Ÿå™¨ç¼ºä¹å¯¹è¿™äº›å¼‚æ„å¤šå¼•æ“å·¥ä½œæµç¨‹çš„é€¼çœŸå»ºæ¨¡ï¼Œé™åˆ¶äº†å®ƒä»¬å¯¹æ¶æ„å†³ç­–çš„æŒ‡å¯¼ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09775v2">PDF</a> Inference System Design for Multi-Stage AI Inference Pipelines. 13   Pages, 15 Figues, 3 Tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†è¶Šæ¥è¶Šå¤æ‚çš„æ¨ç†ç®¡é“å’Œç¡¬ä»¶å¹³å°çš„éœ€æ±‚ã€‚ç°ä»£LLMæœåŠ¡è¶…è¶Šäº†ä¼ ç»Ÿçš„é¢„å¡«å……-è§£ç å·¥ä½œæµç¨‹ï¼Œå¼•å…¥äº†å¤šé˜¶æ®µè¿‡ç¨‹ï¼Œå¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜æ£€ç´¢ã€åŠ¨æ€æ¨¡å‹è·¯ç”±å’Œå¤šæ­¥æ¨ç†ç­‰ã€‚è¿™äº›é˜¶æ®µå…·æœ‰å¤šæ ·çš„è®¡ç®—éœ€æ±‚ï¼Œéœ€è¦æ•´åˆGPUã€ASICã€CPUå’Œå†…å­˜ä¸ºä¸­å¿ƒçš„æ¶æ„çš„åˆ†å¸ƒå¼ç³»ç»Ÿã€‚ä¸ºè§£å†³ç°æœ‰æ¨¡æ‹Ÿå™¨åœ¨æ¨¡æ‹Ÿè¿™äº›å¼‚æ„å¤šå¼•æ“å·¥ä½œæµç¨‹æ–¹é¢çš„ä¸è¶³ï¼Œæˆ‘ä»¬æ¨å‡ºäº†HERMESï¼Œä¸€ä¸ªå¼‚æ„å¤šé˜¶æ®µLLMæ¨ç†æ‰§è¡Œæ¨¡æ‹Ÿå™¨ã€‚HERMESèƒ½å¤Ÿæ¨¡æ‹ŸåŒ…æ‹¬RAGã€KVæ£€ç´¢ã€æ¨ç†ã€é¢„å¡«å……å’Œè§£ç åœ¨å†…çš„å„ç§è¯·æ±‚é˜¶æ®µï¼Œå¹¶è·¨è¶Šå¤æ‚çš„ç¡¬ä»¶å±‚æ¬¡ç»“æ„ã€‚é€šè¿‡æ•´åˆçœŸå®ç¡¬ä»¶è·Ÿè¸ªå’Œè§£æå»ºæ¨¡ï¼ŒHERMESèƒ½å¤Ÿæ•æ‰å…³é”®æƒè¡¡ï¼Œå¦‚å†…å­˜å¸¦å®½ç«äº‰ã€é›†ç¾¤é—´é€šä¿¡å»¶è¿Ÿå’Œæ··åˆCPUåŠ é€Ÿå™¨éƒ¨ç½²ä¸­çš„æ‰¹å¤„ç†æ•ˆç‡ã€‚æˆ‘ä»¬çš„æ¡ˆä¾‹ç ”ç©¶æ¢è®¨äº†æ¨ç†é˜¶æ®µå¯¹ç«¯åˆ°ç«¯å»¶è¿Ÿçš„å½±å“ã€æ··åˆç®¡é“çš„æœ€ä½³æ‰¹å¤„ç†ç­–ç•¥ä»¥åŠè¿œç¨‹KVç¼“å­˜æ£€ç´¢çš„æ¶æ„å½±å“ã€‚HERMESä½¿ç³»ç»Ÿè®¾è®¡å¸ˆèƒ½å¤Ÿåº”å¯¹LLMæ¨ç†ä¸æ–­å˜åŒ–çš„æ™¯è§‚ï¼Œä¸ºä¸‹ä¸€ä»£AIå·¥ä½œè´Ÿè½½çš„ä¼˜åŒ–è½¯ç¡¬ä»¶ååŒè®¾è®¡æä¾›å¯æ“ä½œçš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å¤æ‚æ¨ç†ç®¡é“å’Œç¡¬ä»¶å¹³å°çš„éœ€æ±‚ã€‚</li>
<li>ç°ä»£LLMæœåŠ¡åŒ…å«å¤šé˜¶æ®µè¿‡ç¨‹ï¼Œå¦‚RAGã€KVç¼“å­˜æ£€ç´¢ç­‰ï¼Œå…·æœ‰å¤šæ ·çš„è®¡ç®—éœ€æ±‚ã€‚</li>
<li>ç°æœ‰æ¨¡æ‹Ÿå™¨åœ¨æ¨¡æ‹Ÿå¼‚æ„å¤šå¼•æ“å·¥ä½œæµç¨‹æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>HERMESæ˜¯ä¸€ä¸ªå¼‚æ„å¤šé˜¶æ®µLLMæ¨ç†æ‰§è¡Œæ¨¡æ‹Ÿå™¨ï¼Œèƒ½æ¨¡æ‹Ÿå„ç§è¯·æ±‚é˜¶æ®µå¹¶è·¨è¶Šå¤æ‚çš„ç¡¬ä»¶å±‚æ¬¡ç»“æ„ã€‚</li>
<li>HERMESç»“åˆçœŸå®ç¡¬ä»¶è·Ÿè¸ªå’Œè§£æå»ºæ¨¡ï¼Œèƒ½æ•æ‰å…³é”®æƒè¡¡ï¼Œå¦‚å†…å­˜å¸¦å®½ç«äº‰å’Œæ‰¹å¤„ç†æ•ˆç‡ã€‚</li>
<li>æ¡ˆä¾‹ç ”ç©¶æ˜¾ç¤ºäº†æ¨ç†é˜¶æ®µå¯¹ç«¯åˆ°ç«¯å»¶è¿Ÿçš„å½±å“ä»¥åŠæ··åˆç®¡é“çš„æœ€ä½³æ‰¹å¤„ç†ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09775">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-51f17e032fe923889451e46b46b136a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93937a19b78e8c0fab8333f83bb39440.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08c5b0b9a2ede0c1e653575a474bc978.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32fff7d3e445fa61befbbe47a0b437ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56c02cf1b2e0b89c93a3bb0bd275a894.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-facd8892344c29a1174e5e87524d3b64.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Task-Memory-Engine-TME-A-Structured-Memory-Framework-with-Graph-Aware-Extensions-for-Multi-Step-LLM-Agent-Tasks"><a href="#Task-Memory-Engine-TME-A-Structured-Memory-Framework-with-Graph-Aware-Extensions-for-Multi-Step-LLM-Agent-Tasks" class="headerlink" title="Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware   Extensions for Multi-Step LLM Agent Tasks"></a>Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware   Extensions for Multi-Step LLM Agent Tasks</h2><p><strong>Authors:Ye Ye</strong></p>
<p>Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks. However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers. This leads to brittle performance, frequent hallucinations, and poor long-range coherence. In this work, we propose the Task Memory Engine (TME), a lightweight and structured memory module that tracks task execution using a hierarchical Task Memory Tree (TMT). Each node in the tree corresponds to a task step, storing relevant input, output, status, and sub-task relationships. We introduce a prompt synthesis method that dynamically generates LLM prompts based on the active node path, significantly improving execution consistency and contextual grounding. Through case studies and comparative experiments on multi-step agent tasks, we demonstrate that TME leads to better task completion accuracy and more interpretable behavior with minimal implementation overhead. A reference implementation of the core TME components is available at <a target="_blank" rel="noopener" href="https://github.com/biubiutomato/TME-Agent">https://github.com/biubiutomato/TME-Agent</a>, including basic examples and structured memory integration. While the current implementation uses a tree-based structure, TME is designed to be graph-aware, supporting reusable substeps, converging task paths, and shared dependencies. This lays the groundwork for future DAG-based memory architectures. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨ä½œå¤šæ­¥éª¤ä»»åŠ¡çš„è‡ªä¸»ä»£ç†ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ¡†æ¶æ— æ³•ç»´æŒå¯¹ä»»åŠ¡çŠ¶æ€çš„ç»“æ„åŒ–ç†è§£ï¼Œé€šå¸¸ä¾èµ–äºçº¿æ€§æç¤ºä¸²è”æˆ–æµ…å†…å­˜ç¼“å†²åŒºã€‚è¿™å¯¼è‡´æ€§èƒ½è„†å¼±ã€é¢‘ç¹å‡ºç°å¹»è§‰å’Œé•¿æœŸè¿è´¯æ€§å·®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä»»åŠ¡è®°å¿†å¼•æ“ï¼ˆTMEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ã€ç»“æ„åŒ–çš„å†…å­˜æ¨¡å—ï¼Œå®ƒé€šè¿‡åˆ†å±‚ä»»åŠ¡è®°å¿†æ ‘ï¼ˆTMTï¼‰æ¥è·Ÿè¸ªä»»åŠ¡æ‰§è¡Œã€‚æ ‘ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”äºä¸€ä¸ªä»»åŠ¡æ­¥éª¤ï¼Œå­˜å‚¨ç›¸å…³çš„è¾“å…¥ã€è¾“å‡ºã€çŠ¶æ€å’Œå­ä»»åŠ¡å…³ç³»ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æç¤ºåˆæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºæ´»åŠ¨èŠ‚ç‚¹è·¯å¾„åŠ¨æ€ç”ŸæˆLLMæç¤ºï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ‰§è¡Œä¸€è‡´æ€§ä»¥åŠä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€‚é€šè¿‡å¤šæ­¥éª¤ä»£ç†ä»»åŠ¡çš„æ¡ˆä¾‹ç ”ç©¶å’Œå¯¹æ¯”å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†TMEåœ¨æé«˜ä»»åŠ¡å®Œæˆå‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§è¡Œä¸ºæ–¹é¢çš„ä¼˜åŠ¿ï¼Œå¹¶ä¸”å…·æœ‰æœ€å°çš„å®ç°å¼€é”€ã€‚æ ¸å¿ƒTMEç»„ä»¶çš„å‚è€ƒå®ç°å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/biubiotomato/TME-Agent%E6%89%BE%E5%88%B0%EF%BC%8C%E5%85%B6%E4%B8%AD%E5%8C%85%E6%8B%AC%E5%9F%BA%E6%9C%AC%E7%A4%BA%E4%BE%8B%E5%92%8C%E7%BB%93%E6%9E%84%E5%8C%96%E7%9A%84%E5%86%85%E5%AD%98%E9%9B%86%E6%88%90%E3%80%82%E5%B0%BD%E7%AE%A1%E5%BD%93%E5%89%8D%E5%AE%9E%E7%8E%B0%E4%BD%BF%E7%94%A8%E6%A0%91%E5%BD%A2%E7%BB%93%E6%9E%84%EF%BC%8C%E4%BD%86TME%E8%A2%AB%E8%AE%BE%E8%AE%A1%E4%B8%BA%E5%9B%BE%E5%BD%A2%E6%84%9F%E7%9F%A5%EF%BC%8C%E6%94%AF%E6%8C%81%E5%8F%AF%E9%87%8D%E5%A4%8D%E7%9A%84%E5%AD%90%E6%AD%A5%E9%AA%A4%E3%80%81%E6%94%B6%E6%95%9B%E7%9A%84%E4%BB%BB%E5%8A%A1%E8%B7%AF%E5%BE%84%E5%92%8C%E5%85%B1%E4%BA%AB%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB%E3%80%82%E8%BF%99%E4%B8%BA%E6%9C%AA%E6%9D%A5%E7%9A%84%E6%9C%89%E5%90%91%E6%97%A0%E7%8E%AF%E5%9B%BE%EF%BC%88DAG%EF%BC%89%E5%9F%BA%E7%A1%80%E5%86%85%E5%AD%98%E6%9E%B6%E6%9E%84%E5%A5%A0%E5%AE%9A%E4%BA%86%E5%9F%BA%E7%A1%80%E3%80%82">https://github.com/biubiotomato/TME-Agentæ‰¾åˆ°ï¼Œå…¶ä¸­åŒ…æ‹¬åŸºæœ¬ç¤ºä¾‹å’Œç»“æ„åŒ–çš„å†…å­˜é›†æˆã€‚å°½ç®¡å½“å‰å®ç°ä½¿ç”¨æ ‘å½¢ç»“æ„ï¼Œä½†TMEè¢«è®¾è®¡ä¸ºå›¾å½¢æ„ŸçŸ¥ï¼Œæ”¯æŒå¯é‡å¤çš„å­æ­¥éª¤ã€æ”¶æ•›çš„ä»»åŠ¡è·¯å¾„å’Œå…±äº«ä¾èµ–å…³ç³»ã€‚è¿™ä¸ºæœªæ¥çš„æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰åŸºç¡€å†…å­˜æ¶æ„å¥ å®šäº†åŸºç¡€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08525v3">PDF</a> 14 pages, 5 figures. Preprint prepared for future submission.   Includes implementation and token-efficiency analysis. Code at   <a target="_blank" rel="noopener" href="https://github.com/biubiutomato/TME-Agent">https://github.com/biubiutomato/TME-Agent</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šæ­¥ä»»åŠ¡ä¸­ä½œä¸ºè‡ªä¸»ä»£ç†çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œä½†ç°æœ‰æ¡†æ¶å¾€å¾€æ— æ³•ç»´æŒå¯¹ä»»åŠ¡çŠ¶æ€çš„ç»“æ„åŒ–ç†è§£ï¼Œå¯¼è‡´æ€§èƒ½ä¸ç¨³å®šã€é¢‘ç¹å‡ºç°å¹»è§‰å’Œé•¿æœŸè¿è´¯æ€§å·®ã€‚æœ¬æ–‡æå‡ºäº†ä»»åŠ¡è®°å¿†å¼•æ“ï¼ˆTMEï¼‰å’Œå±‚æ¬¡åŒ–ä»»åŠ¡è®°å¿†æ ‘ï¼ˆTMTï¼‰ï¼Œé€šè¿‡è·Ÿè¸ªä»»åŠ¡æ‰§è¡Œæ¥æ”¹å–„è¿™ä¸€é—®é¢˜ã€‚TMEåˆ©ç”¨ç»“æ„åŒ–è®°å¿†æ¨¡å—å­˜å‚¨ä»»åŠ¡æ­¥éª¤çš„ç›¸å…³è¾“å…¥ã€è¾“å‡ºã€çŠ¶æ€å’Œå­ä»»åŠ¡å…³ç³»ï¼Œå¹¶å¼•å…¥åŸºäºæ´»è·ƒèŠ‚ç‚¹è·¯å¾„çš„åŠ¨æ€ç”ŸæˆLLMæç¤ºçš„æç¤ºåˆæˆæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†æ‰§è¡Œçš„ä¸€è‡´æ€§å’Œä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€‚é€šè¿‡æ¡ˆä¾‹ç ”ç©¶å’Œå¤šæ­¥ä»£ç†ä»»åŠ¡çš„å¯¹æ¯”å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†TMEåœ¨æå‡ä»»åŠ¡å®Œæˆå‡†ç¡®æ€§å’Œè§£é‡Šæ€§è¡Œä¸ºæ–¹é¢çš„ä¼˜åŠ¿ï¼Œä¸”å®ç°å¼€é”€è¾ƒå°ã€‚æ ¸å¿ƒç»„ä»¶çš„å‚è€ƒå®ç°å¯è®¿é—®äº<a target="_blank" rel="noopener" href="https://github.com/biubiutomato/TME-Agent">é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤šæ­¥ä»»åŠ¡ä¸­ä½œä¸ºè‡ªä¸»ä»£ç†çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œä½†å­˜åœ¨å¯¹ä»»åŠ¡çŠ¶æ€ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ¡†æ¶å¾€å¾€ä¾èµ–çº¿æ€§æç¤ºæ‹¼æ¥æˆ–æµ…å±‚è®°å¿†ç¼“å†²åŒºï¼Œå¯¼è‡´æ€§èƒ½ä¸ç¨³å®šã€é¢‘ç¹å‡ºç°å¹»è§‰å’Œé•¿æœŸè¿è´¯æ€§å·®ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä»»åŠ¡è®°å¿†å¼•æ“ï¼ˆTMEï¼‰å’Œå±‚æ¬¡åŒ–ä»»åŠ¡è®°å¿†æ ‘ï¼ˆTMTï¼‰ï¼Œç”¨äºè·Ÿè¸ªä»»åŠ¡æ‰§è¡Œå¹¶æ”¹å–„ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>TMEåˆ©ç”¨ç»“æ„åŒ–è®°å¿†æ¨¡å—å­˜å‚¨ä»»åŠ¡æ­¥éª¤çš„ç›¸å…³è¾“å…¥ã€è¾“å‡ºã€çŠ¶æ€å’Œå­ä»»åŠ¡å…³ç³»ã€‚</li>
<li>TMEå¼•å…¥æç¤ºåˆæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºæ´»è·ƒèŠ‚ç‚¹è·¯å¾„åŠ¨æ€ç”ŸæˆLLMæç¤ºï¼Œæé«˜äº†æ‰§è¡Œçš„ä¸€è‡´æ€§å’Œä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€‚</li>
<li>é€šè¿‡æ¡ˆä¾‹ç ”ç©¶å’Œå®éªŒè¯æ˜ï¼ŒTMEåœ¨æå‡ä»»åŠ¡å®Œæˆå‡†ç¡®æ€§å’Œè§£é‡Šæ€§è¡Œä¸ºæ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œä¸”å®ç°å¼€é”€è¾ƒå°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08525">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-833c71abfb41f6d58cf8b4414cff583e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56cfc4f925f501a97f8aa2f5539a97f4.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Neural-ODE-Transformers-Analyzing-Internal-Dynamics-and-Adaptive-Fine-tuning"><a href="#Neural-ODE-Transformers-Analyzing-Internal-Dynamics-and-Adaptive-Fine-tuning" class="headerlink" title="Neural ODE Transformers: Analyzing Internal Dynamics and Adaptive   Fine-tuning"></a>Neural ODE Transformers: Analyzing Internal Dynamics and Adaptive   Fine-tuning</h2><p><strong>Authors:Anh Tong, Thanh Nguyen-Tang, Dongeun Lee, Duc Nguyen, Toan Tran, David Hall, Cheongwoong Kang, Jaesik Choi</strong></p>
<p>Recent advancements in large language models (LLMs) based on transformer architectures have sparked significant interest in understanding their inner workings. In this paper, we introduce a novel approach to modeling transformer architectures using highly flexible non-autonomous neural ordinary differential equations (ODEs). Our proposed model parameterizes all weights of attention and feed-forward blocks through neural networks, expressing these weights as functions of a continuous layer index. Through spectral analysis of the modelâ€™s dynamics, we uncover an increase in eigenvalue magnitude that challenges the weight-sharing assumption prevalent in existing theoretical studies. We also leverage the Lyapunov exponent to examine token-level sensitivity, enhancing model interpretability. Our neural ODE transformer demonstrates performance comparable to or better than vanilla transformers across various configurations and datasets, while offering flexible fine-tuning capabilities that can adapt to different architectural constraints. </p>
<blockquote>
<p>åŸºäºTransformeræ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ï¼Œå¼•å‘äº†äººä»¬å¯¹ç†è§£å…¶å†…éƒ¨å·¥ä½œåŸç†çš„æå¤§å…´è¶£ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ä½¿ç”¨é«˜åº¦çµæ´»çš„éè‡ªæ²»ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEsï¼‰å¯¹Transformeræ¶æ„è¿›è¡Œå»ºæ¨¡çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºçš„æ¨¡å‹é€šè¿‡ç¥ç»ç½‘ç»œå¯¹æ‰€æœ‰æ³¨æ„åŠ›æƒé‡å’Œå‰é¦ˆå—çš„æƒé‡è¿›è¡Œå‚æ•°åŒ–ï¼Œå¹¶å°†è¿™äº›æƒé‡è¡¨è¾¾ä¸ºè¿ç»­å±‚ç´¢å¼•çš„å‡½æ•°ã€‚é€šè¿‡å¯¹æ¨¡å‹åŠ¨åŠ›å­¦çš„é¢‘è°±åˆ†æï¼Œæˆ‘ä»¬å‘ç°ç‰¹å¾å€¼å¹…åº¦å¢å¤§ï¼Œè¿™æŒ‘æˆ˜äº†ç°æœ‰ç†è®ºç ”ç©¶ä¸­æ™®éå­˜åœ¨çš„æƒé‡å…±äº«å‡è®¾ã€‚æˆ‘ä»¬è¿˜åˆ©ç”¨LyapunovæŒ‡æ•°æ¥æ£€æŸ¥ä»¤ç‰Œçº§åˆ«çš„æ•æ„Ÿæ€§ï¼Œä»¥æé«˜æ¨¡å‹çš„è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„ç¥ç»ODEå˜å‹å™¨åœ¨å„ç§é…ç½®å’Œæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¸æ ‡å‡†å˜å‹å™¨ç›¸å½“æˆ–æ›´å¥½ï¼ŒåŒæ—¶æä¾›çµæ´»çš„å¾®è°ƒèƒ½åŠ›ï¼Œå¯ä»¥é€‚åº”ä¸åŒçš„æ¶æ„çº¦æŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01329v2">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼ŒåŸºäºTransformeræ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°è¿›å±•å¼•å‘äº†å¯¹å…¶å†…éƒ¨å·¥ä½œåŸç†çš„æå¤§å…´è¶£ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨éè‡ªä¸»ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEsï¼‰å¯¹Transformeræ¶æ„è¿›è¡Œå»ºæ¨¡çš„æ–°æ–¹æ³•ã€‚è¯¥æ¨¡å‹é€šè¿‡ç¥ç»ç½‘ç»œå‚æ•°åŒ–æ‰€æœ‰æ³¨æ„åŠ›ä¸é¦ˆé€å‰å‘å—çš„æƒé‡ï¼Œå¹¶å°†è¿™äº›æƒé‡è¡¨è¾¾ä¸ºè¿ç»­å±‚ç´¢å¼•çš„å‡½æ•°ã€‚é€šè¿‡å¯¹æ¨¡å‹åŠ¨æ€è¿›è¡Œè°±åˆ†æï¼Œæˆ‘ä»¬å‘ç°ç‰¹å¾å€¼å¹…åº¦å¢åŠ ï¼Œè¿™æŒ‘æˆ˜äº†ç°æœ‰ç†è®ºç ”ç©¶ä¸­æ™®éå­˜åœ¨çš„æƒé‡å…±äº«å‡è®¾ã€‚æˆ‘ä»¬è¿˜åˆ©ç”¨LyapunovæŒ‡æ•°æ¥è€ƒå¯Ÿè¯çº§åˆ«çš„æ•æ„Ÿæ€§ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„ç¥ç»ODEå˜å‹å™¨åœ¨å„ç§é…ç½®å’Œæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¸å¸¸è§„å˜å‹å™¨ç›¸å½“æˆ–æ›´å¥½ï¼ŒåŒæ—¶æä¾›äº†çµæ´»çš„å¾®è°ƒèƒ½åŠ›ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒçš„æ¶æ„çº¦æŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†éè‡ªä¸»ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEsï¼‰å¯¹Transformeræ¶æ„è¿›è¡Œå»ºæ¨¡çš„æ–°æ–¹æ³•ã€‚</li>
<li>é€šè¿‡ç¥ç»ç½‘ç»œå‚æ•°åŒ–Transformerçš„æƒé‡ï¼Œå¹¶å°†å…¶è¡¨è¾¾ä¸ºè¿ç»­å±‚ç´¢å¼•çš„å‡½æ•°ã€‚</li>
<li>è°±åˆ†ææ˜¾ç¤ºæ¨¡å‹çš„ç‰¹å¾å€¼å¹…åº¦å¢åŠ ï¼ŒæŒ‘æˆ˜äº†ç°æœ‰ç†è®ºä¸­çš„æƒé‡å…±äº«å‡è®¾ã€‚</li>
<li>åˆ©ç”¨LyapunovæŒ‡æ•°è€ƒå¯Ÿè¯çº§åˆ«çš„æ•æ„Ÿæ€§ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è§£é‡Šæ€§ã€‚</li>
<li>ç¥ç»ODEå˜å‹å™¨çš„æ€§èƒ½ä¸å¸¸è§„å˜å‹å™¨ç›¸å½“æˆ–æ›´å¥½ï¼Œé€‚ç”¨äºå¤šç§ä»»åŠ¡å’Œæ•°æ®é›†ã€‚</li>
<li>ç¥ç»ODEå˜å‹å™¨æä¾›äº†çµæ´»çš„å¾®è°ƒèƒ½åŠ›ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒçš„æ¶æ„çº¦æŸã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºç†è§£LLMçš„å†…éƒ¨å·¥ä½œåŸç†æä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01329">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f39f09695c6be8cee5ec93b24796003.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3290a43df62a7651a7ac83a31603966f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc4368ba170c370b60f8f64ca30e51da.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="BioMaze-Benchmarking-and-Enhancing-Large-Language-Models-for-Biological-Pathway-Reasoning"><a href="#BioMaze-Benchmarking-and-Enhancing-Large-Language-Models-for-Biological-Pathway-Reasoning" class="headerlink" title="BioMaze: Benchmarking and Enhancing Large Language Models for Biological   Pathway Reasoning"></a>BioMaze: Benchmarking and Enhancing Large Language Models for Biological   Pathway Reasoning</h2><p><strong>Authors:Haiteng Zhao, Chang Ma, Fangzhi Xu, Lingpeng Kong, Zhi-Hong Deng</strong></p>
<p>The applications of large language models (LLMs) in various biological domains have been explored recently, but their reasoning ability in complex biological systems, such as pathways, remains underexplored, which is crucial for predicting biological phenomena, formulating hypotheses, and designing experiments. This work explores the potential of LLMs in pathway reasoning. We introduce BioMaze, a dataset with 5.1K complex pathway problems derived from real research, covering various biological contexts including natural dynamic changes, disturbances, additional intervention conditions, and multi-scale research targets. Our evaluation of methods such as CoT and graph-augmented reasoning, shows that LLMs struggle with pathway reasoning, especially in perturbed systems. To address this, we propose PathSeeker, an LLM agent that enhances reasoning through interactive subgraph-based navigation, enabling a more effective approach to handling the complexities of biological systems in a scientifically aligned manner. The dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/zhao-ht/BioMaze">https://github.com/zhao-ht/BioMaze</a>. </p>
<blockquote>
<p>æœ€è¿‘å·²ç»æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ç”Ÿç‰©é¢†åŸŸçš„åº”ç”¨ï¼Œä½†å®ƒä»¬åœ¨å¤æ‚ç”Ÿç‰©ç³»ç»Ÿï¼ˆå¦‚é€”å¾„ï¼‰ä¸­çš„æ¨ç†èƒ½åŠ›ä»è¢«å¿½è§†ï¼Œè¿™å¯¹äºé¢„æµ‹ç”Ÿç‰©ç°è±¡ã€æå‡ºå‡è®¾å’Œè®¾è®¡å®éªŒè‡³å…³é‡è¦ã€‚è¿™é¡¹å·¥ä½œæ¢ç´¢äº†LLMåœ¨é€”å¾„æ¨ç†ä¸­çš„æ½œåŠ›ã€‚æˆ‘ä»¬ä»‹ç»äº†BioMazeï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«5.1Kä¸ªæ¥è‡ªçœŸå®ç ”ç©¶çš„å¤æ‚é€”å¾„é—®é¢˜çš„æ•°æ®é›†ï¼Œæ¶µç›–äº†å¤šç§ç”Ÿç‰©èƒŒæ™¯ï¼ŒåŒ…æ‹¬è‡ªç„¶åŠ¨æ€å˜åŒ–ã€å¹²æ‰°ã€é¢å¤–å¹²é¢„æ¡ä»¶å’Œå¤šå°ºåº¦ç ”ç©¶ç›®æ ‡ã€‚æˆ‘ä»¬å¯¹CoTå’Œå¢å¼ºå›¾æ¨ç†ç­‰æ–¹æ³•è¿›è¡Œè¯„ä¼°çš„ç»“æœè¡¨æ˜ï¼ŒLLMåœ¨é€”å¾„æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨å—å¹²æ‰°ç³»ç»Ÿä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PathSeekerï¼Œè¿™æ˜¯ä¸€ä¸ªLLMä»£ç†ï¼Œå®ƒé€šè¿‡åŸºäºäº¤äº’å­å›¾çš„å¯¼èˆªå¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œä»¥ç§‘å­¦çš„æ–¹å¼æ›´æœ‰æ•ˆåœ°å¤„ç†ç”Ÿç‰©ç³»ç»Ÿçš„å¤æ‚æ€§ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhao-ht/BioMaze%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zhao-ht/BioMazeæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16660v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªç”Ÿç‰©é¢†åŸŸçš„åº”ç”¨å·²å¾—åˆ°å¹¿æ³›ç ”ç©¶ï¼Œä½†åœ¨ç”Ÿç‰©é€šè·¯ç­‰å¤æ‚ç”Ÿç‰©ç³»ç»Ÿä¸­çš„æ¨ç†èƒ½åŠ›ä»ç„¶ç ”ç©¶ä¸è¶³ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢LLMåœ¨é€šè·¯æ¨ç†ä¸­çš„æ½œåŠ›ï¼Œå¹¶ä»‹ç»äº†BioMazeæ•°æ®é›†ï¼ŒåŒ…å«5.1KçœŸå®ç ”ç©¶ä¸­çš„å¤æ‚é€šè·¯é—®é¢˜ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒLLMåœ¨åº”å¯¹æ‰°åŠ¨ç³»ç»Ÿä¸­çš„æ¨ç†æ—¶è¡¨ç°æ¬ ä½³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†PathSeekerï¼Œä¸€ä¸ªé€šè¿‡äº¤äº’å¼å­å›¾å¯¼èˆªå¢å¼ºæ¨ç†çš„LLMä»£ç†ï¼Œä»¥æ›´ç§‘å­¦çš„æ–¹å¼æœ‰æ•ˆåº”å¯¹ç”Ÿç‰©ç³»ç»Ÿçš„å¤æ‚æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ç”Ÿç‰©é€šè·¯ç­‰å¤æ‚ç”Ÿç‰©ç³»ç»Ÿä¸­çš„æ¨ç†èƒ½åŠ›ä»ç„¶ç ”ç©¶ä¸è¶³ã€‚</li>
<li>BioMazeæ•°æ®é›†åŒ…å«çœŸå®ç ”ç©¶ä¸­çš„å¤æ‚ç”Ÿç‰©é€šè·¯é—®é¢˜ï¼Œæœ‰åŠ©äºè¯„ä¼°LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>LLMåœ¨åº”å¯¹æ‰°åŠ¨ç³»ç»Ÿä¸­çš„æ¨ç†æ—¶è¡¨ç°æ¬ ä½³ã€‚</li>
<li>PathSeekeræ˜¯ä¸€ä¸ªLLMä»£ç†ï¼Œé€šè¿‡äº¤äº’å¼å­å›¾å¯¼èˆªå¢å¼ºæ¨ç†ã€‚</li>
<li>PathSeekerèƒ½æ›´ç§‘å­¦ã€æœ‰æ•ˆåœ°åº”å¯¹ç”Ÿç‰©ç³»ç»Ÿçš„å¤æ‚æ€§ã€‚</li>
<li>BioMazeæ•°æ®é›†å’Œä»£ç å¯åœ¨å…¬å¼€ä»“åº“ä¸­è·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16660">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-453f5d5b9afb253943720d557c693bf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f50f71bd991c3c5db24d14612a62d5ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4509a357c681b5b1bc065ccd88e6c93d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e616561d2bc315a4bb324499caf77181.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0015393d9eb5a90f1979e22a304189cf.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Building-A-Proof-Oriented-Programmer-That-Is-64-Better-Than-GPT-4o-Under-Data-Scarcity"><a href="#Building-A-Proof-Oriented-Programmer-That-Is-64-Better-Than-GPT-4o-Under-Data-Scarcity" class="headerlink" title="Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o   Under Data Scarcity"></a>Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o   Under Data Scarcity</h2><p><strong>Authors:Dylan Zhang, Justin Wang, Tianran Sun</strong></p>
<p>Existing LMs struggle with proof-oriented programming due to data scarcity, which manifest in two key ways: (1) a lack of sufficient corpora for proof-oriented programming languages such as F*, and (2) the absence of large-scale, project-level proof-oriented implementations that can teach the model the intricate reasoning process when performing proof-oriented programming. We present the first on synthetic data augmentation for project level proof oriented programming for both generation and repair. Our method addresses data scarcity by synthesizing basic proof-oriented programming problems for proficiency in that language; incorporating diverse coding data for reasoning capability elicitation and creating new proofs and repair data within existing repositories. This approach enables language models to both synthesize and repair proofs for function- and repository-level code. We show that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of the models that outperforms GPT-4o in project-level proof-oriented programming by 64% relative margin, and can improve GPT-4oâ€™s performance by 54% by repairing its outputs over GPT-4oâ€™s self-repair. </p>
<blockquote>
<p>ç°æœ‰çš„è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é¢å‘è¯æ˜çš„ç¼–ç¨‹æ—¶é¢ä¸´ç€æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œè¿™ä¸»è¦ä½“ç°åœ¨ä¸¤ä¸ªæ–¹é¢ï¼šï¼ˆ1ï¼‰ç¼ºä¹è¶³å¤Ÿçš„é¢å‘è¯æ˜çš„ç¼–ç¨‹è¯­è¨€è¯­æ–™åº“ï¼Œå¦‚F*ï¼Œä»¥åŠï¼ˆ2ï¼‰ç¼ºä¹å¤§è§„æ¨¡çš„é¡¹ç›®çº§é¢å‘è¯æ˜çš„å®ç°ï¼Œè¿™äº›å®ç°å¯ä»¥åœ¨è¿›è¡Œé¢å‘è¯æ˜çš„ç¼–ç¨‹æ—¶æ•™å¯¼æ¨¡å‹å¤æ‚çš„æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬é¦–æ¬¡æå‡ºé’ˆå¯¹é¡¹ç›®çº§é¢å‘è¯æ˜çš„ç¼–ç¨‹çš„åˆæˆæ•°æ®å¢å¼ºæ–¹æ³•ï¼Œæ—¢å¯ç”¨äºç”Ÿæˆä¹Ÿå¯ç”¨äºä¿®å¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åˆæˆåŸºæœ¬çš„é¢å‘è¯æ˜çš„ç¼–ç¨‹é—®é¢˜æ¥è§£å†³è¯¥è¯­è¨€æ–¹é¢çš„ç†Ÿç»ƒåº¦é—®é¢˜ï¼›å¼•å…¥å¤šæ ·åŒ–çš„ç¼–ç æ•°æ®æ¥æ¿€å‘æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨ç°æœ‰å­˜å‚¨åº“ä¸­åˆ›å»ºæ–°çš„è¯æ˜å’Œä¿®å¤æ•°æ®ã€‚è¿™ç§æ–¹æ³•ä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåˆæˆå’Œä¿®å¤å‡½æ•°çº§å’Œå­˜å‚¨åº“çº§çš„ä»£ç è¯æ˜ã€‚æˆ‘ä»¬å±•ç¤ºäº†ç»è¿‡å¾®è°ƒçš„14äº¿å‚æ•°æ¨¡å‹PoPilotï¼Œåœ¨é¢å‘é¡¹ç›®çš„è¯æ˜å¯¼å‘ç¼–ç¨‹æ–¹é¢çš„æ€§èƒ½è¶…è¿‡äº†GPT-4oæ¨¡å‹çš„ç›¸å¯¹è¾¹ç¼˜ï¼Œæé«˜å¹…åº¦ä¸ºç™¾åˆ†ä¹‹å…­åå››ï¼›è€Œä¸”æ¯”GPT-4oçš„è‡ªæˆ‘ä¿®å¤è¾“å‡ºæ€§èƒ½æé«˜äº†ç™¾åˆ†ä¹‹äº”åå››ã€‚æˆ‘ä»¬é€šè¿‡åˆæˆè®­ç»ƒæ ·ä¾‹æ•°æ®ä»¥æ„å»ºé€šç”¨æ¨ç†ç®¡é“ç³»ç»Ÿåº”å¯¹å¯¹ä¸€ç³»åˆ—åŸºæœ¬çš„ç¼–ç¨‹ä»»åŠ¡çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†é’ˆå¯¹é¡¹ç›®çº§é¢å‘è¯æ˜çš„ç¼–ç¨‹çš„ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„å¼€æºæ•°æ®é›†ï¼ŒåŒ…å«åˆæˆè®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚è¿™ä¸ºè¯­è¨€æ¨¡å‹æä¾›äº†å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶æœ‰åŠ©äºæ”¹è¿›å…¶åœ¨é¡¹ç›®çº§é¢å‘è¯æ˜çš„ç¼–ç¨‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11901v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç°æœ‰è¯­è¨€æ¨¡å‹åœ¨é¢å‘è¯æ˜çš„ç¼–ç¨‹æ–¹é¢å­˜åœ¨æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œè¡¨ç°ä¸ºç¼ºä¹è¶³å¤Ÿçš„é¢å‘è¯æ˜çš„ç¼–ç¨‹è¯­è¨€è¯­æ–™åº“å’Œé¡¹ç›®çº§åˆ«çš„è¯æ˜å®ç°ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åˆæˆæ•°æ®å¢å¼ºæ–¹æ³•ï¼Œç”¨äºé¡¹ç›®çº§åˆ«çš„è¯æ˜å¯¼å‘ç¼–ç¨‹ç”Ÿæˆå’Œä¿®å¤ã€‚é€šè¿‡åˆæˆåŸºæœ¬çš„é¢å‘è¯æ˜çš„ç¼–ç¨‹é—®é¢˜ï¼Œèå…¥å¤šæ ·åŒ–çš„ç¼–ç æ•°æ®æ¿€å‘æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨ç°æœ‰å­˜å‚¨åº“ä¸­åˆ›å»ºæ–°çš„è¯æ˜å’Œä¿®å¤æ•°æ®ï¼Œè¯¥æ–¹æ³•ä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåˆæˆå’Œä¿®å¤å‡½æ•°åŠå­˜å‚¨åº“çº§åˆ«çš„ä»£ç è¯æ˜ã€‚å®éªŒæ˜¾ç¤ºï¼Œæˆ‘ä»¬å¾®è°ƒçš„14Bå‚æ•°æ¨¡å‹PoPilotåœ¨é¡¹ç›®çº§åˆ«çš„è¯æ˜å¯¼å‘ç¼–ç¨‹æ–¹é¢çš„æ€§èƒ½è¶…è¿‡äº†GPT-4oï¼Œç›¸å¯¹è¾¹é™…æé«˜äº†64%ï¼Œå¹¶åœ¨ä¿®å¤è¾“å‡ºæ–¹é¢å°†GPT-4oçš„æ€§èƒ½æé«˜äº†54%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹åœ¨é¢å‘è¯æ˜çš„ç¼–ç¨‹æ–¹é¢é¢ä¸´æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚</li>
<li>ç¼ºä¹é¢å‘è¯æ˜çš„ç¼–ç¨‹è¯­è¨€è¯­æ–™åº“å’Œé¡¹ç›®çº§åˆ«çš„è¯æ˜å®ç°æ˜¯æ•°æ®ç¨€ç¼ºçš„ä¸»è¦è¡¨ç°ã€‚</li>
<li>æå‡ºé€šè¿‡åˆæˆæ•°æ®å¢å¼ºæ–¹æ³•è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼ŒåŒ…æ‹¬åˆæˆé¢å‘è¯æ˜çš„ç¼–ç¨‹é—®é¢˜ã€èå…¥å¤šæ ·åŒ–ç¼–ç æ•°æ®å’Œåœ¨ç°æœ‰å­˜å‚¨åº“ä¸­åˆ›å»ºæ–°çš„è¯æ˜å’Œä¿®å¤æ•°æ®ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåˆæˆå’Œä¿®å¤å‡½æ•°åŠå­˜å‚¨åº“çº§åˆ«çš„ä»£ç è¯æ˜ã€‚</li>
<li>PoPilotæ¨¡å‹åœ¨é¡¹ç›®çº§åˆ«çš„è¯æ˜å¯¼å‘ç¼–ç¨‹æ–¹é¢çš„æ€§èƒ½è¶…è¿‡äº†GPT-4oã€‚</li>
<li>PoPilotæ¨¡å‹ç›¸å¯¹GPT-4oçš„æ€§èƒ½æé«˜äº†64%ï¼Œå¹¶åœ¨ä¿®å¤è¾“å‡ºæ–¹é¢çš„æ€§èƒ½æé«˜äº†54%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11901">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7b159631bbd2037f97fbae17e7cdb846.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f1eccc7bdd66c77cdc0612ef755490f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70fbc8707b051cc23c42858a5556a7b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa486b662c755141778f786b2ec45b87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dae0fd4649d612f8366f742aa991b0bb.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-18/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-18/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-18/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3a50c7d599d569532aa2b777fa0f53a9.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-18  ARCeR an Agentic RAG for the Automated Definition of Cyber Ranges
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-18/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-29400da9d1b9a3b32b8cad70c2e64acb.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-18  HLS-Eval A Benchmark and Framework for Evaluating LLMs on High-Level   Synthesis Design Tasks
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30166.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
