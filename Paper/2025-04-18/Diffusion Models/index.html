<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-18  Cobra Efficient Line Art COlorization with BRoAder References">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-95c7a27077fce7ecac6289a12df9e5e0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    63 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-18-æ›´æ–°"><a href="#2025-04-18-æ›´æ–°" class="headerlink" title="2025-04-18 æ›´æ–°"></a>2025-04-18 æ›´æ–°</h1><h2 id="Cobra-Efficient-Line-Art-COlorization-with-BRoAder-References"><a href="#Cobra-Efficient-Line-Art-COlorization-with-BRoAder-References" class="headerlink" title="Cobra: Efficient Line Art COlorization with BRoAder References"></a>Cobra: Efficient Line Art COlorization with BRoAder References</h2><p><strong>Authors:Junhao Zhuang, Lingen Li, Xuan Ju, Zhaoyang Zhang, Chun Yuan, Ying Shan</strong></p>
<p>The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control. A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. We investigate the necessity of extensive contextual image guidance on the quality of line art colorization. To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency. Results demonstrate that Cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. We release our codes and models on our project page: <a target="_blank" rel="noopener" href="https://zhuang2002.github.io/Cobra/">https://zhuang2002.github.io/Cobra/</a>. </p>
<blockquote>
<p>æ¼«ç”»åˆ¶ä½œè¡Œä¸šéœ€è¦åŸºäºå‚è€ƒçš„é«˜ç²¾åº¦ã€é«˜æ•ˆç‡ã€ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’Œçµæ´»æ§åˆ¶çš„çº¿æ¡è‰ºæœ¯è‰²å½©åŒ–ã€‚æ¼«ç”»é¡µé¢é€šå¸¸æ¶‰åŠå¤šç§è§’è‰²ã€ç‰©ä½“å’ŒèƒŒæ™¯ï¼Œè¿™å¢åŠ äº†è‰²å½©åŒ–çš„å¤æ‚æ€§ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å®ƒä»¬åœ¨çº¿æ¡è‰ºæœ¯è‰²å½©åŒ–æ–¹é¢çš„åº”ç”¨ä»ç„¶æœ‰é™ï¼Œé¢ä¸´ç€å¤„ç†å¤§é‡å‚è€ƒå›¾åƒã€è€—æ—¶æ¨ç†å’Œçµæ´»æ§åˆ¶ç­‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬ç ”ç©¶äº†å¤§é‡ä¸Šä¸‹æ–‡å›¾åƒæŒ‡å¯¼å¯¹çº¿æ¡è‰ºæœ¯è‰²å½©åŒ–è´¨é‡çš„å¿…è¦æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Cobraï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆä¸”é€šç”¨çš„æ–¹æ³•ï¼Œæ”¯æŒé¢œè‰²æç¤ºï¼Œåˆ©ç”¨200å¤šä¸ªå‚è€ƒå›¾åƒï¼ŒåŒæ—¶ä¿æŒä½å»¶è¿Ÿã€‚Cobraçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå› æœç¨€ç–DiTæ¶æ„ï¼Œå®ƒåˆ©ç”¨ä¸“é—¨è®¾è®¡çš„ä½ç½®ç¼–ç ã€å› æœç¨€ç–æ³¨æ„åŠ›å’Œé”®å€¼ç¼“å­˜æ¥æœ‰æ•ˆåœ°ç®¡ç†é•¿ä¸Šä¸‹æ–‡å¼•ç”¨å¹¶ç¡®ä¿é¢œè‰²èº«ä»½ä¸€è‡´æ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒCobraé€šè¿‡å¹¿æ³›çš„ä¸Šä¸‹æ–‡å‚è€ƒå®ç°äº†å‡†ç¡®çš„çº¿æ¡è‰ºæœ¯è‰²å½©åŒ–ï¼Œå¤§å¤§æé«˜äº†æ¨ç†é€Ÿåº¦å’Œäº¤äº’æ€§ï¼Œä»è€Œæ»¡è¶³äº†è¡Œä¸šçš„å…³é”®éœ€æ±‚ã€‚æˆ‘ä»¬å·²åœ¨é¡¹ç›®é¡µé¢å‘å¸ƒä»£ç å’Œæ¨¡å‹ï¼š<a target="_blank" rel="noopener" href="https://zhuang2002.github.io/Cobra/">https://zhuang2002.github.io/Cobra/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12240v1">PDF</a> Project page with code: <a target="_blank" rel="noopener" href="https://zhuang2002.github.io/Cobra/">https://zhuang2002.github.io/Cobra/</a></p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡æœ¬ä»‹ç»äº†æ¼«ç”»åˆ¶ä½œè¡Œä¸šå¯¹åŸºäºå‚è€ƒçš„çº¿è‰ºæœ¯ç€è‰²çš„éœ€æ±‚ï¼Œå¼ºè°ƒé«˜å‡†ç¡®æ€§ã€é«˜æ•ˆç‡ã€ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’Œçµæ´»æ§åˆ¶çš„é‡è¦æ€§ã€‚ä½œè€…è°ƒæŸ¥äº†å¹¿æ³›ä¸Šä¸‹æ–‡å›¾åƒæŒ‡å¯¼å¯¹çº¿è‰ºæœ¯ç€è‰²è´¨é‡çš„é‡è¦æ€§ã€‚ä¸ºäº†åº”å¯¹æŒ‘æˆ˜ï¼Œå¼•å…¥äº†ä¸€ç§é«˜æ•ˆä¸”é€šç”¨çš„æ–¹æ³•Cobraï¼Œå®ƒæ”¯æŒé¢œè‰²æç¤ºï¼ŒåŒæ—¶ä½¿ç”¨è¶…è¿‡200å¼ å‚è€ƒå›¾åƒï¼Œå¹¶ä¿æŒä½å»¶è¿Ÿã€‚Cobraçš„æ ¸å¿ƒæ˜¯ä¸€ç§å› æœç¨€ç–DiTæ¶æ„ï¼Œå®ƒåˆ©ç”¨ä¸“é—¨è®¾è®¡çš„ä½ç½®ç¼–ç ã€å› æœç¨€ç–æ³¨æ„åŠ›æœºåˆ¶å’Œé”®å€¼ç¼“å­˜æ¥æœ‰æ•ˆåœ°ç®¡ç†é•¿ä¸Šä¸‹æ–‡å‚è€ƒï¼Œå¹¶ç¡®ä¿é¢œè‰²ä¸€è‡´æ€§ã€‚ç ”ç©¶ç»“æœè¯æ˜ï¼ŒCobraé€šè¿‡å¹¿æ³›çš„ä¸Šä¸‹æ–‡å‚è€ƒå®ç°äº†ç²¾ç¡®çš„çº¿è‰ºæœ¯ç€è‰²ï¼Œæ˜¾è‘—æé«˜äº†æ¨æ–­é€Ÿåº¦å’Œäº¤äº’æ€§ï¼Œæ»¡è¶³äº†è¡Œä¸šå…³é”®éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¼«ç”»åˆ¶ä½œè¡Œä¸šéœ€è¦é«˜å‡†ç¡®æ€§ã€é«˜æ•ˆç‡ã€ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’Œçµæ´»æ§åˆ¶çš„å‚è€ƒåŸºç¡€çº¿è‰ºæœ¯ç€è‰²ã€‚</li>
<li>å¹¿æ³›ä¸Šä¸‹æ–‡å›¾åƒæŒ‡å¯¼å¯¹çº¿è‰ºæœ¯ç€è‰²è´¨é‡è‡³å…³é‡è¦ã€‚</li>
<li>å¼•å…¥Cobraæ–¹æ³•ï¼Œæ”¯æŒé¢œè‰²æç¤ºï¼Œä½¿ç”¨è¶…è¿‡200å¼ å‚è€ƒå›¾åƒï¼Œå¹¶ä¿æŒä½å»¶è¿Ÿã€‚</li>
<li>Cobraæ ¸å¿ƒæ˜¯ä¸€ç§å› æœç¨€ç–DiTæ¶æ„ï¼Œæœ‰æ•ˆç®¡ç†é•¿ä¸Šä¸‹æ–‡å‚è€ƒï¼Œç¡®ä¿é¢œè‰²ä¸€è‡´æ€§ã€‚</li>
<li>Cobraå®ç°äº†ç²¾ç¡®çº¿è‰ºæœ¯ç€è‰²ï¼Œé€šè¿‡å¹¿æ³›çš„ä¸Šä¸‹æ–‡å‚è€ƒã€‚</li>
<li>Cobraæ˜¾è‘—æé«˜æ¨æ–­é€Ÿåº¦å’Œäº¤äº’æ€§ï¼Œæ»¡è¶³è¡Œä¸šå…³é”®éœ€æ±‚ã€‚</li>
<li>ä½œè€…åœ¨å…¶é¡¹ç›®é¡µé¢ä¸Šå‘å¸ƒäº†ä»£ç å’Œæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12240">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6b8586727c7244cec93e131ec6ecd80d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c8a94e3da7afbd16fa930b721c2340e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ad1999c1fe5e7d063ed7ba2f8d48344.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4ecafeb2822dd59197f74334ffba74c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-248bf5d45ce7874cb9b5f2f718f26266.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d02d313a1bbdd229d486ffac2a4d8c94.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bb1c4eb5ed7a72f549731ea59c5c457d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Anti-Aesthetics-Protecting-Facial-Privacy-against-Customized-Text-to-Image-Synthesis"><a href="#Anti-Aesthetics-Protecting-Facial-Privacy-against-Customized-Text-to-Image-Synthesis" class="headerlink" title="Anti-Aesthetics: Protecting Facial Privacy against Customized   Text-to-Image Synthesis"></a>Anti-Aesthetics: Protecting Facial Privacy against Customized   Text-to-Image Synthesis</h2><p><strong>Authors:Songping Wang, Yueming Lyu, Shiqi Liu, Ning Li, Tong Tong, Hao Sun, Caifeng Shan</strong></p>
<p>The rise of customized diffusion models has spurred a boom in personalized visual content creation, but also poses risks of malicious misuse, severely threatening personal privacy and copyright protection. Some studies show that the aesthetic properties of images are highly positively correlated with human perception of image quality. Inspired by this, we approach the problem from a novel and intriguing aesthetic perspective to degrade the generation quality of maliciously customized models, thereby achieving better protection of facial identity. Specifically, we propose a Hierarchical Anti-Aesthetic (HAA) framework to fully explore aesthetic cues, which consists of two key branches: 1) Global Anti-Aesthetics: By establishing a global anti-aesthetic reward mechanism and a global anti-aesthetic loss, it can degrade the overall aesthetics of the generated content; 2) Local Anti-Aesthetics: A local anti-aesthetic reward mechanism and a local anti-aesthetic loss are designed to guide adversarial perturbations to disrupt local facial identity. By seamlessly integrating both branches, our HAA effectively achieves the goal of anti-aesthetics from a global to a local level during customized generation. Extensive experiments show that HAA outperforms existing SOTA methods largely in identity removal, providing a powerful tool for protecting facial privacy and copyright. </p>
<blockquote>
<p>å®šåˆ¶æ‰©æ•£æ¨¡å‹çš„å…´èµ·ä¿ƒè¿›äº†ä¸ªæ€§åŒ–è§†è§‰å†…å®¹åˆ›ä½œçš„ç¹è£ï¼Œä½†åŒæ—¶ä¹Ÿå¸¦æ¥äº†æ¶æ„æ»¥ç”¨çš„é£é™©ï¼Œä¸¥é‡å¨èƒä¸ªäººéšç§å’Œç‰ˆæƒä¿æŠ¤ã€‚ä¸€äº›ç ”ç©¶è¡¨æ˜ï¼Œå›¾åƒçš„ç¾å­¦å±æ€§ä¸äººç±»å¯¹å›¾åƒè´¨é‡çš„æ„ŸçŸ¥æœ‰ç€é«˜åº¦çš„æ­£ç›¸å…³ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬ä»ä¸€ä¸ªæ–°çš„ã€æœ‰è¶£çš„ç¾å­¦è§’åº¦æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä»¥é™ä½æ¶æ„å®šåˆ¶æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ï¼Œä»è€Œæ›´å¥½åœ°ä¿æŠ¤é¢éƒ¨èº«ä»½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†å±‚åç¾å­¦ï¼ˆHAAï¼‰æ¡†æ¶ï¼Œä»¥å……åˆ†æ¢ç´¢ç¾å­¦çº¿ç´¢ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦åˆ†æ”¯ï¼š1ï¼‰å…¨å±€åç¾å­¦ï¼šé€šè¿‡å»ºç«‹å…¨å±€åç¾å­¦å¥–åŠ±æœºåˆ¶å’Œå…¨å±€åç¾å­¦æŸå¤±ï¼Œå®ƒå¯ä»¥é™ä½ç”Ÿæˆå†…å®¹çš„æ•´ä½“ç¾å­¦ï¼›2ï¼‰å±€éƒ¨åç¾å­¦ï¼šè®¾è®¡å±€éƒ¨åç¾å­¦å¥–åŠ±æœºåˆ¶å’Œå±€éƒ¨åç¾å­¦æŸå¤±ï¼Œä»¥å¼•å¯¼å¯¹æŠ—æ€§æ‰°åŠ¨ç ´åå±€éƒ¨é¢éƒ¨èº«ä»½ã€‚é€šè¿‡æ— ç¼é›†æˆè¿™ä¸¤ä¸ªåˆ†æ”¯ï¼Œæˆ‘ä»¬çš„HAAæœ‰æ•ˆåœ°å®ç°äº†ä»å…¨å±€åˆ°å±€éƒ¨çš„åç¾å­¦ç›®æ ‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHAAåœ¨èº«ä»½åˆ é™¤æ–¹é¢å¤§å¤§ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œä¸ºä¿æŠ¤é¢éƒ¨éšç§å’Œç‰ˆæƒæä¾›äº†æœ‰åŠ›çš„å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12129v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å®šåˆ¶åŒ–æ‰©æ•£æ¨¡å‹çš„å…´èµ·ä¿ƒè¿›äº†ä¸ªæ€§åŒ–è§†è§‰å†…å®¹çš„åˆ›ä½œç¹è£ï¼Œä½†åŒæ—¶ä¹Ÿå­˜åœ¨æ¶æ„æ»¥ç”¨çš„é£é™©ï¼Œä¸¥é‡å¨èƒä¸ªäººéšç§å’Œç‰ˆæƒä¿æŠ¤ã€‚æœ¬ç ”ç©¶ä»æ–°é¢–ä¸”æœ‰è¶£çš„ç¾å­¦è§’åº¦å…¥æ‰‹ï¼Œé™ä½æ¶æ„å®šåˆ¶åŒ–æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ï¼Œä»¥å®ç°æ›´å¥½çš„é¢éƒ¨èº«ä»½ä¿æŠ¤ã€‚æˆ‘ä»¬æå‡ºäº†åˆ†å±‚åç¾å­¦ï¼ˆHAAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…¨é¢æ¢è®¨äº†ç¾å­¦çº¿ç´¢ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®åˆ†æ”¯ï¼š1ï¼‰å…¨å±€åç¾å­¦ï¼šé€šè¿‡å»ºç«‹å…¨å±€åç¾å­¦å¥–åŠ±æœºåˆ¶å’Œå…¨å±€åç¾å­¦æŸå¤±å‡½æ•°ï¼Œé™ä½ç”Ÿæˆå†…å®¹çš„æ•´ä½“ç¾å­¦æ°´å¹³ï¼›2ï¼‰å±€éƒ¨åç¾å­¦ï¼šè®¾è®¡å±€éƒ¨åç¾å­¦å¥–åŠ±æœºåˆ¶å’Œå±€éƒ¨åç¾å­¦æŸå¤±å‡½æ•°ï¼Œå¼•å¯¼å¯¹æŠ—æ€§æ‰°åŠ¨ç ´åå±€éƒ¨é¢éƒ¨èº«ä»½ã€‚é€šè¿‡æ— ç¼é›†æˆä¸¤ä¸ªåˆ†æ”¯ï¼ŒHAAåœ¨å®šåˆ¶ç”Ÿæˆè¿‡ç¨‹ä¸­ä»å…¨å±€åˆ°å±€éƒ¨å®ç°äº†æœ‰æ•ˆçš„åç¾å­¦ç›®æ ‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHAAåœ¨èº«ä»½å»é™¤æ–¹é¢å¤§å¤§ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œä¸ºé¢éƒ¨éšç§å’Œç‰ˆæƒä¿æŠ¤æä¾›äº†å¼ºå¤§çš„å·¥å…·ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å®šåˆ¶åŒ–æ‰©æ•£æ¨¡å‹çš„æ™®åŠä¿ƒè¿›äº†ä¸ªæ€§åŒ–è§†è§‰å†…å®¹çš„åˆ›å»ºï¼Œä½†ä¼´éšæ¶æ„æ»¥ç”¨çš„é£é™©ï¼Œè¿™å¯¹ä¸ªäººéšç§å’Œç‰ˆæƒä¿æŠ¤æ„æˆäº†ä¸¥é‡å¨èƒã€‚</li>
<li>ç ”ç©¶é€šè¿‡æ–°é¢–çš„ç¾å­¦è§’åº¦æ¥è§£å†³æ¶æ„å®šåˆ¶åŒ–æ¨¡å‹çš„æ½œåœ¨å¨èƒï¼Œæ—¨åœ¨é™ä½å…¶ç”Ÿæˆè´¨é‡ä»¥å¢å¼ºä¿æŠ¤ã€‚</li>
<li>å¼•å…¥åˆ†å±‚åç¾å­¦ï¼ˆHAAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®åˆ†æ”¯ï¼šå…¨å±€åç¾å­¦å’Œå±€éƒ¨åç¾å­¦ï¼Œä»¥å…¨é¢æ¢ç´¢ç¾å­¦çº¿ç´¢å¹¶åº”å¯¹æŒ‘æˆ˜ã€‚</li>
<li>å…¨å±€åç¾å­¦åˆ†æ”¯é€šè¿‡å»ºç«‹ç›¸åº”çš„å¥–åŠ±æœºåˆ¶å’ŒæŸå¤±å‡½æ•°ï¼Œç€çœ¼äºé™ä½ç”Ÿæˆå†…å®¹çš„æ•´ä½“ç¾å­¦æ°´å¹³ã€‚</li>
<li>å±€éƒ¨åç¾å­¦åˆ†æ”¯ä¸“æ³¨äºç ´åå±€éƒ¨é¢éƒ¨èº«ä»½ï¼Œé€šè¿‡è®¾è®¡ç‰¹å®šçš„å¥–åŠ±æœºåˆ¶å’ŒæŸå¤±å‡½æ•°æ¥å¼•å¯¼å¯¹æŠ—æ€§æ‰°åŠ¨ã€‚</li>
<li>HAAæ¡†æ¶é€šè¿‡æ— ç¼é›†æˆä¸Šè¿°ä¸¤ä¸ªåˆ†æ”¯ï¼Œå®ç°äº†ä»å…¨å±€åˆ°å±€éƒ¨çš„åˆ†å±‚åç¾å­¦æ•ˆæœã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒHAAåœ¨èº«ä»½å»é™¤æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºé¢éƒ¨éšç§å’Œç‰ˆæƒä¿æŠ¤æä¾›äº†æœ‰æ•ˆå·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f096359489037ae35bd135190185b633.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56481966ff42666d0a035955b89fa92b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e167c05bd6bda8adc0b77487fa7c8a4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1360d6e993e3bbb06947a5d34472069f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd3b72fbd467cedcd4d024bbce4d4625.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08adeb672fb64a9bb075311a5f7633bd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Diffusion-Based-Framework-for-Terrain-Aware-Remote-Sensing-Image-Reconstruction"><a href="#A-Diffusion-Based-Framework-for-Terrain-Aware-Remote-Sensing-Image-Reconstruction" class="headerlink" title="A Diffusion-Based Framework for Terrain-Aware Remote Sensing Image   Reconstruction"></a>A Diffusion-Based Framework for Terrain-Aware Remote Sensing Image   Reconstruction</h2><p><strong>Authors:Zhenyu Yu, Mohd Yamani Inda Idris, Pei Wang</strong></p>
<p>Remote sensing imagery is essential for environmental monitoring, agricultural management, and disaster response. However, data loss due to cloud cover, sensor failures, or incomplete acquisition-especially in high-resolution and high-frequency tasks-severely limits satellite imageryâ€™s effectiveness. Traditional interpolation methods struggle with large missing areas and complex structures. Remote sensing imagery consists of multiple bands, each with distinct meanings, and ensuring consistency across bands is critical to avoid anomalies in the combined images. This paper proposes SatelliteMaker, a diffusion-based method that reconstructs missing data across varying levels of data loss while maintaining spatial, spectral, and temporal consistency. We also propose Digital Elevation Model (DEM) as a conditioning input and use tailored prompts to generate realistic images, making diffusion models applicable to quantitative remote sensing tasks. Additionally, we propose a VGG-Adapter module based on Distribution Loss, which reduces distribution discrepancy and ensures style consistency. Extensive experiments show that SatelliteMaker achieves state-of-the-art performance across multiple tasks. </p>
<blockquote>
<p>é¥æ„Ÿå½±åƒåœ¨ç¯å¢ƒç›‘æµ‹ã€å†œä¸šç®¡ç†å’Œç¾å®³åº”å¯¹æ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œç”±äºäº‘å±‚è¦†ç›–ã€ä¼ æ„Ÿå™¨æ•…éšœæˆ–é‡‡é›†ä¸å®Œå…¨ç­‰å› ç´ å¯¼è‡´çš„æ•°æ®ä¸¢å¤±ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜åˆ†è¾¨ç‡é«˜é¢‘ç‡çš„ä»»åŠ¡ä¸­ï¼Œå«æ˜Ÿå½±åƒçš„æœ‰æ•ˆæ€§å—åˆ°äº†ä¸¥é‡é™åˆ¶ã€‚ä¼ ç»Ÿæ’å€¼æ–¹æ³•åœ¨å¤„ç†å¤§é¢ç§¯ç¼ºå¤±å’Œå¤æ‚ç»“æ„æ—¶é‡åˆ°äº†å›°éš¾ã€‚é¥æ„Ÿå½±åƒç”±å¤šä¸ªæ³¢æ®µç»„æˆï¼Œæ¯ä¸ªæ³¢æ®µéƒ½æœ‰ç‹¬ç‰¹çš„å«ä¹‰ï¼Œç¡®ä¿å„æ³¢æ®µä¹‹é—´çš„ä¸€è‡´æ€§å¯¹äºé¿å…åˆæˆå›¾åƒä¸­çš„å¼‚å¸¸è‡³å…³é‡è¦ã€‚æœ¬æ–‡é’ˆå¯¹å«æ˜Ÿå½±åƒä¸­æ•°æ®ç¼ºå¤±çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„SatelliteMakeræ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸åŒçº§åˆ«çš„æ•°æ®ä¸¢å¤±æƒ…å†µä¸‹é‡å»ºç¼ºå¤±æ•°æ®ï¼ŒåŒæ—¶ä¿æŒç©ºé—´ã€å…‰è°±å’Œæ—¶é—´çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºå°†æ•°å­—é«˜ç¨‹æ¨¡å‹ï¼ˆDEMï¼‰ä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œå¹¶ä½¿ç”¨å®šåˆ¶æç¤ºæ¥ç”Ÿæˆé€¼çœŸçš„å›¾åƒï¼Œä½¿æ‰©æ•£æ¨¡å‹é€‚ç”¨äºå®šé‡é¥æ„Ÿä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåˆ†å¸ƒæŸå¤±çš„VGGé€‚é…å™¨æ¨¡å—ï¼Œè¯¥æ¨¡å—å‡å°‘äº†åˆ†å¸ƒå·®å¼‚ï¼Œç¡®ä¿äº†é£æ ¼ä¸€è‡´æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSatelliteMakeråœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12112v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„é¥æ„Ÿå½±åƒç¼ºå¤±æ•°æ®é‡å»ºæ–¹æ³•â€”â€”SatelliteMakerï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸åŒçº§åˆ«çš„æ•°æ®ä¸¢å¤±æƒ…å†µä¸‹è¿›è¡Œé‡å»ºï¼ŒåŒæ—¶ä¿æŒç©ºé—´ã€å…‰è°±å’Œæ—¶é—´çš„ä¸€è‡´æ€§ã€‚è®ºæ–‡è¿˜å¼•å…¥äº†æ•°å­—é«˜ç¨‹æ¨¡å‹ï¼ˆDEMï¼‰ä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œå¹¶ä½¿ç”¨å®šåˆ¶æç¤ºæ¥ç”ŸæˆçœŸå®å›¾åƒï¼Œä½¿æ‰©æ•£æ¨¡å‹é€‚ç”¨äºå®šé‡é¥æ„Ÿä»»åŠ¡ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼ŒSatelliteMakeråœ¨å¤šä»»åŠ¡ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„Ÿå½±åƒåœ¨ç¯å¢ƒç›‘æµ‹ã€å†œä¸šç®¡ç†å’Œç¾å®³å“åº”ç­‰é¢†åŸŸå…·æœ‰å…³é”®ä½œç”¨ï¼Œä½†æ•°æ®ç¼ºå¤±é—®é¢˜é™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¼ ç»Ÿæ’å€¼æ–¹æ³•åœ¨å¤„ç†å¤§é¢ç§¯ç¼ºå¤±å’Œå¤æ‚ç»“æ„æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>SatelliteMakeræ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹é‡å»ºé¥æ„Ÿå½±åƒçš„ç¼ºå¤±æ•°æ®ï¼Œé€‚åº”ä¸åŒçº§åˆ«çš„æ•°æ®ä¸¢å¤±æƒ…å†µã€‚</li>
<li>å¼•å…¥æ•°å­—é«˜ç¨‹æ¨¡å‹ï¼ˆDEMï¼‰ä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œæé«˜å½±åƒç”Ÿæˆçš„ç°å®æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>å®šåˆ¶æç¤ºç”¨äºç”ŸæˆçœŸå®å›¾åƒï¼Œä½¿æ‰©æ•£æ¨¡å‹é€‚ç”¨äºå®šé‡é¥æ„Ÿä»»åŠ¡ã€‚</li>
<li>VGG-Adapteræ¨¡å—åŸºäºåˆ†å¸ƒæŸå¤±ï¼Œå‡å°‘åˆ†å¸ƒå·®å¼‚ï¼Œç¡®ä¿é£æ ¼ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12112">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-424f45ad2d46dc139ad9acd54d735ebb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5ab5a347034d07053d2190c6d858cff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a7027487dc54a5e521f4eb0a290a5bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5f8b2c574f877835d9e5a9b65814719.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95c7a27077fce7ecac6289a12df9e5e0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Generalized-Visual-Relation-Detection-with-Diffusion-Models"><a href="#Generalized-Visual-Relation-Detection-with-Diffusion-Models" class="headerlink" title="Generalized Visual Relation Detection with Diffusion Models"></a>Generalized Visual Relation Detection with Diffusion Models</h2><p><strong>Authors:Kaifeng Gao, Siqi Chen, Hanwang Zhang, Jun Xiao, Yueting Zhuang, Qianru Sun</strong></p>
<p>Visual relation detection (VRD) aims to identify relationships (or interactions) between object pairs in an image. Although recent VRD models have achieved impressive performance, they are all restricted to pre-defined relation categories, while failing to consider the semantic ambiguity characteristic of visual relations. Unlike objects, the appearance of visual relations is always subtle and can be described by multiple predicate words from different perspectives, e.g., <code>ride&#39;&#39; can be depicted as </code>raceâ€™â€™ and &#96;&#96;sit onâ€™â€™, from the sports and spatial position views, respectively. To this end, we propose to model visual relations as continuous embeddings, and design diffusion models to achieve generalized VRD in a conditional generative manner, termed Diff-VRD. We model the diffusion process in a latent space and generate all possible relations in the image as an embedding sequence. During the generation, the visual and text embeddings of subject-object pairs serve as conditional signals and are injected via cross-attention. After the generation, we design a subsequent matching stage to assign the relation words to subject-object pairs by considering their semantic similarities. Benefiting from the diffusion-based generative process, our Diff-VRD is able to generate visual relations beyond the pre-defined category labels of datasets. To properly evaluate this generalized VRD task, we introduce two evaluation metrics, i.e., text-to-image retrieval and SPICE PR Curve inspired by image captioning. Extensive experiments in both human-object interaction (HOI) detection and scene graph generation (SGG) benchmarks attest to the superiority and effectiveness of Diff-VRD. </p>
<blockquote>
<p>è§†è§‰å…³ç³»æ£€æµ‹ï¼ˆVRDï¼‰æ—¨åœ¨è¯†åˆ«å›¾åƒä¸­å¯¹è±¡å¯¹ä¹‹é—´çš„å…³ç³»ï¼ˆæˆ–äº¤äº’ï¼‰ã€‚å°½ç®¡æœ€è¿‘çš„VRDæ¨¡å‹å·²ç»å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬éƒ½å±€é™äºé¢„å…ˆå®šä¹‰çš„å…³ç³»ç±»åˆ«ï¼Œè€Œå¿½ç•¥äº†è§†è§‰å…³ç³»çš„è¯­ä¹‰æ¨¡ç³Šç‰¹æ€§ã€‚ä¸å¯¹è±¡ä¸åŒï¼Œè§†è§‰å…³ç³»çš„è¡¨ç°æ€»æ˜¯å¾ˆå¾®å¦™ï¼Œå¯ä»¥ä»ä¸åŒçš„è§’åº¦é€šè¿‡å¤šä¸ªè°“è¯æ¥æè¿°ï¼Œä¾‹å¦‚ï¼Œâ€œéª‘â€å¯ä»¥ä»è¿åŠ¨å’Œç©ºé—´ä½ç½®çš„è§’åº¦åˆ†åˆ«æè¿°ä¸ºâ€œæ¯”èµ›â€å’Œâ€œååœ¨ä¸Šé¢â€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºå°†è§†è§‰å…³ç³»å»ºæ¨¡ä¸ºè¿ç»­åµŒå…¥ï¼Œå¹¶è®¾è®¡æ‰©æ•£æ¨¡å‹ä»¥æ¡ä»¶ç”Ÿæˆçš„æ–¹å¼å®ç°å¹¿ä¹‰VRDï¼Œç§°ä¸ºDiff-VRDã€‚æˆ‘ä»¬åœ¨æ½œåœ¨ç©ºé—´ä¸­å¯¹æ‰©æ•£è¿‡ç¨‹è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶å°†å›¾åƒä¸­æ‰€æœ‰å¯èƒ½çš„å…³ç³»ç”ŸæˆåµŒå…¥åºåˆ—ã€‚åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œä¸»ä½“-å¯¹è±¡å¯¹çš„è§†è§‰å’Œæ–‡æœ¬åµŒå…¥ä½œä¸ºæ¡ä»¶ä¿¡å·ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›è¿›è¡Œæ³¨å…¥ã€‚ç”Ÿæˆåï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåç»­çš„åŒ¹é…é˜¶æ®µï¼Œé€šè¿‡è€ƒè™‘è¯­ä¹‰ç›¸ä¼¼æ€§å°†å…³ç³»è¯åˆ†é…ç»™ä¸»ä½“-å¯¹è±¡å¯¹ã€‚å¾—ç›ŠäºåŸºäºæ‰©æ•£çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œæˆ‘ä»¬çš„Diff-VRDèƒ½å¤Ÿç”Ÿæˆè¶…å‡ºæ•°æ®é›†é¢„å®šä¹‰ç±»åˆ«æ ‡ç­¾çš„è§†è§‰å…³ç³»ã€‚ä¸ºäº†é€‚å½“åœ°è¯„ä¼°è¿™é¡¹å¹¿ä¹‰VRDä»»åŠ¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§è¯„ä¼°æŒ‡æ ‡ï¼Œå³æ–‡æœ¬åˆ°å›¾åƒçš„æ£€ç´¢å’Œå—å›¾åƒæè¿°å¯å‘çš„SPICE PRæ›²çº¿ã€‚åœ¨äººç±»ä¸ç‰©ä½“äº¤äº’ï¼ˆHOIï¼‰æ£€æµ‹å’Œåœºæ™¯å›¾ç”Ÿæˆï¼ˆSGGï¼‰åŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒè¯æ˜äº†Diff-VRDçš„ä¼˜è¶Šæ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12100v1">PDF</a> Under review at IEEE TCSVT. The Appendix is provided additionally</p>
<p><strong>æ‘˜è¦</strong><br>    è§†è§‰å…³ç³»æ£€æµ‹ï¼ˆVRDï¼‰æ—¨åœ¨è¯†åˆ«å›¾åƒä¸­å¯¹è±¡å¯¹ä¹‹é—´çš„å…³ç³»ï¼ˆæˆ–äº¤äº’ï¼‰ã€‚è™½ç„¶æœ€è¿‘çš„VRDæ¨¡å‹å·²ç»å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬éƒ½å±€é™äºé¢„å®šä¹‰çš„å…³ç³»ç±»åˆ«ï¼Œè€Œå¿½ç•¥äº†è§†è§‰å…³ç³»çš„è¯­ä¹‰æ¨¡ç³Šæ€§ã€‚è§†è§‰å…³ç³»çš„è¡¨ç°æ€»æ˜¯å¾ˆå¾®å¦™ï¼Œå¯ä»¥ä»ä¸åŒçš„è§’åº¦ç”¨å¤šä¸ªè°“è¯è¯æ¥æè¿°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºå°†è§†è§‰å…³ç³»å»ºæ¨¡ä¸ºè¿ç»­åµŒå…¥ï¼Œå¹¶è®¾è®¡æ‰©æ•£æ¨¡å‹ä»¥æ¡ä»¶ç”Ÿæˆçš„æ–¹å¼å®ç°é€šç”¨VRDï¼Œç§°ä¸ºDiff-VRDã€‚æˆ‘ä»¬åœ¨æ½œåœ¨ç©ºé—´ä¸­å»ºæ¨¡æ‰©æ•£è¿‡ç¨‹ï¼Œå¹¶å°†å›¾åƒä¸­çš„æ‰€æœ‰å¯èƒ½å…³ç³»ç”Ÿæˆä¸ºåµŒå…¥åºåˆ—ã€‚åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œä¸»ä½“-å¯¹è±¡å¯¹çš„è§†è§‰å’Œæ–‡æœ¬åµŒå…¥ä½œä¸ºæ¡ä»¶ä¿¡å·é€šè¿‡äº¤å‰æ³¨æ„åŠ›æ³¨å…¥ã€‚ç”Ÿæˆåï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåç»­åŒ¹é…é˜¶æ®µï¼Œé€šè¿‡è€ƒè™‘è¯­ä¹‰ç›¸ä¼¼æ€§å°†å…³ç³»è¯åˆ†é…ç»™ä¸»ä½“-å¯¹è±¡å¯¹ã€‚å¾—ç›ŠäºåŸºäºæ‰©æ•£çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œæˆ‘ä»¬çš„Diff-VRDèƒ½å¤Ÿç”Ÿæˆè¶…å‡ºæ•°æ®é›†é¢„å®šä¹‰ç±»åˆ«æ ‡ç­¾çš„è§†è§‰å…³ç³»ã€‚ä¸ºäº†é€‚å½“åœ°è¯„ä¼°è¿™é¡¹é€šç”¨VRDä»»åŠ¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§è¯„ä¼°æŒ‡æ ‡ï¼Œå³æ–‡æœ¬åˆ°å›¾åƒçš„æ£€ç´¢å’Œå—å›¾åƒæè¿°å¯å‘çš„SPICE PRæ›²çº¿ã€‚åœ¨äººç±»-ç‰©ä½“äº¤äº’ï¼ˆHOIï¼‰æ£€æµ‹å’Œåœºæ™¯å›¾ç”Ÿæˆï¼ˆSGGï¼‰åŸºå‡†æµ‹è¯•ä¸­çš„å¤§é‡å®éªŒè¯æ˜äº†Diff-VRDçš„ä¼˜è¶Šæ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†è§‰å…³ç³»æ£€æµ‹ï¼ˆVRDï¼‰çš„ç›®æ ‡æ˜¯è¯†åˆ«å›¾åƒä¸­å¯¹è±¡ä¹‹é—´çš„å…³ç³»æˆ–äº¤äº’ã€‚</li>
<li>æœ€è¿‘çš„VRDæ¨¡å‹è™½ç„¶æ€§èƒ½å‡ºè‰²ï¼Œä½†éƒ½å±€é™äºé¢„å®šä¹‰çš„å…³ç³»ç±»åˆ«ï¼Œå¿½ç•¥äº†è§†è§‰å…³ç³»çš„è¯­ä¹‰æ¨¡ç³Šæ€§ã€‚</li>
<li>è§†è§‰å…³ç³»çš„è¡¨ç°å¯ä»¥ä»å¤šä¸ªè§’åº¦ç”¨å¤šä¸ªè°“è¯è¯æ¥æè¿°ï¼Œè¿™äº›è¡¨ç°æ€»æ˜¯å¾ˆå¾®å¦™ã€‚</li>
<li>æå‡ºå°†è§†è§‰å…³ç³»å»ºæ¨¡ä¸ºè¿ç»­åµŒå…¥ï¼Œå¹¶é‡‡ç”¨æ‰©æ•£æ¨¡å‹ä»¥æ¡ä»¶ç”Ÿæˆçš„æ–¹å¼å®ç°é€šç”¨VRDï¼ˆDiff-VRDï¼‰ã€‚</li>
<li>Diff-VRDåœ¨æ½œåœ¨ç©ºé—´ä¸­å»ºæ¨¡æ‰©æ•£è¿‡ç¨‹ï¼Œç”Ÿæˆå›¾åƒä¸­çš„æ‰€æœ‰å¯èƒ½å…³ç³»ä½œä¸ºåµŒå…¥åºåˆ—ã€‚</li>
<li>åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨ä¸»ä½“-å¯¹è±¡å¯¹çš„è§†è§‰å’Œæ–‡æœ¬åµŒå…¥ä½œä¸ºæ¡ä»¶ä¿¡å·ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å®ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12100">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-944f8c2c0866a027b5321a6326d44fdc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60139b7e168ea9071ee9b2912be5b4d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f49bee99aafab6cf1c49a19644781074.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="R-Meshfusion-Reinforcement-Learning-Powered-Sparse-View-Mesh-Reconstruction-with-Diffusion-Priors"><a href="#R-Meshfusion-Reinforcement-Learning-Powered-Sparse-View-Mesh-Reconstruction-with-Diffusion-Priors" class="headerlink" title="R-Meshfusion: Reinforcement Learning Powered Sparse-View Mesh   Reconstruction with Diffusion Priors"></a>R-Meshfusion: Reinforcement Learning Powered Sparse-View Mesh   Reconstruction with Diffusion Priors</h2><p><strong>Authors:Haoyang Wang, Liming Liu, Peiheng Wang, Junlin Hao, Jiangkai Wu, Xinggong Zhang</strong></p>
<p>Mesh reconstruction from multi-view images is a fundamental problem in computer vision, but its performance degrades significantly under sparse-view conditions, especially in unseen regions where no ground-truth observations are available. While recent advances in diffusion models have demonstrated strong capabilities in synthesizing novel views from limited inputs, their outputs often suffer from visual artifacts and lack 3D consistency, posing challenges for reliable mesh optimization. In this paper, we propose a novel framework that leverages diffusion models to enhance sparse-view mesh reconstruction in a principled and reliable manner. To address the instability of diffusion outputs, we propose a Consensus Diffusion Module that filters unreliable generations via interquartile range (IQR) analysis and performs variance-aware image fusion to produce robust pseudo-supervision. Building on this, we design an online reinforcement learning strategy based on the Upper Confidence Bound (UCB) to adaptively select the most informative viewpoints for enhancement, guided by diffusion loss. Finally, the fused images are used to jointly supervise a NeRF-based model alongside sparse-view ground truth, ensuring consistency across both geometry and appearance. Extensive experiments demonstrate that our method achieves significant improvements in both geometric quality and rendering quality. </p>
<blockquote>
<p>åŸºäºå¤šè§†è§’å›¾åƒçš„ç½‘æ ¼é‡å»ºæ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼Œä½†åœ¨ç¨€ç–è§†è§’æ¡ä»¶ä¸‹ï¼Œå…¶æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨æ²¡æœ‰çœŸå®è§‚æµ‹æ•°æ®çš„æœªçŸ¥åŒºåŸŸã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•åœ¨æœ‰é™çš„è¾“å…¥ä¸‹æ˜¾ç¤ºå‡ºå¼ºå¤§çš„åˆæˆæ–°è§†è§’çš„èƒ½åŠ›ï¼Œä½†å…¶è¾“å‡ºå¾€å¾€å­˜åœ¨è§†è§‰ä¼ªå½±ä¸”ç¼ºä¹3Dä¸€è‡´æ€§ï¼Œè¿™ä¸ºå¯é çš„ç½‘æ ¼ä¼˜åŒ–å¸¦æ¥äº†æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹ä»¥æœ‰åŸåˆ™çš„å’Œå¯é çš„æ–¹å¼å¢å¼ºç¨€ç–è§†è§’ç½‘æ ¼é‡å»ºã€‚ä¸ºäº†è§£å†³æ‰©æ•£è¾“å‡ºçš„ä¸ç¨³å®šæ€§ï¼Œæˆ‘ä»¬æå‡ºäº†å…±è¯†æ‰©æ•£æ¨¡å—ï¼ˆConsensus Diffusion Moduleï¼‰ï¼Œå®ƒé€šè¿‡å››åˆ†ä½è·ï¼ˆIQRï¼‰åˆ†æè¿‡æ»¤ä¸å¯é çš„ç”Ÿæˆï¼Œå¹¶æ‰§è¡Œæ–¹å·®æ„ŸçŸ¥å›¾åƒèåˆä»¥äº§ç”Ÿç¨³å¥çš„ä¼ªç›‘ç£ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºä¸Šç½®ä¿¡ç•Œï¼ˆUCBï¼‰çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œä»¥è‡ªé€‚åº”åœ°é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„è§†è§’è¿›è¡Œå¢å¼ºï¼Œç”±æ‰©æ•£æŸå¤±å¼•å¯¼ã€‚æœ€åï¼Œèåˆå›¾åƒè¢«ç”¨æ¥å…±åŒç›‘ç£åŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„æ¨¡å‹å’Œç¨€ç–è§†è§’çš„çœŸå®æ•°æ®ï¼Œç¡®ä¿å‡ ä½•å’Œå¤–è§‚çš„ä¸€è‡´æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡ ä½•è´¨é‡å’Œæ¸²æŸ“è´¨é‡ä¸Šéƒ½å®ç°äº†æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11946v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>å¤šè§†è§’å›¾åƒé‡å»ºæ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„åŸºæœ¬é—®é¢˜ï¼Œä½†åœ¨ç¨€ç–è§†è§’ä¸‹å…¶æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨æœªè§åŒºåŸŸï¼Œç”±äºæ²¡æœ‰çœŸå®åœ°é¢è§‚æµ‹æ•°æ®å¯ç”¨ã€‚å°½ç®¡æœ€è¿‘çš„æ‰©æ•£æ¨¡å‹è¿›å±•åœ¨ä»…ä»æœ‰é™è¾“å…¥åˆæˆæ–°è§†è§’æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å…¶è¾“å‡ºå¸¸å¸¸å­˜åœ¨è§†è§‰ä¼ªå½±å’Œç¼ºä¹ä¸‰ç»´ä¸€è‡´æ€§ï¼Œç»™å¯é çš„ç½‘æ ¼ä¼˜åŒ–å¸¦æ¥äº†æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹ä»¥æœ‰åŸåˆ™å’Œå¯é çš„æ–¹å¼å¢å¼ºç¨€ç–è§†è§’ç½‘æ ¼é‡å»ºçš„æ–°æ¡†æ¶ã€‚ä¸ºè§£å†³æ‰©æ•£è¾“å‡ºçš„ä¸ç¨³å®šæ€§é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…±è¯†æ‰©æ•£æ¨¡å—ï¼Œå®ƒé€šè¿‡å››åˆ†ä½è·åˆ†æè¿‡æ»¤ä¸å¯é çš„ç”Ÿæˆï¼Œå¹¶æ‰§è¡Œæ–¹å·®æ„ŸçŸ¥å›¾åƒèåˆä»¥äº§ç”Ÿç¨³å¥çš„ä¼ªç›‘ç£ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºä¸Šç½®ä¿¡ç•Œï¼ˆUCBï¼‰çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œè‡ªé€‚åº”é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„è§†è§’è¿›è¡Œå¢å¼ºï¼Œç”±æ‰©æ•£æŸå¤±å¼•å¯¼ã€‚æœ€åï¼Œèåˆå›¾åƒè¢«ç”¨æ¥è”åˆç›‘ç£åŸºäºNeRFçš„æ¨¡å‹ä»¥åŠç¨€ç–è§†è§’çš„çœŸå®æ•°æ®ï¼Œç¡®ä¿å‡ ä½•å’Œå¤–è§‚çš„ä¸€è‡´æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡ ä½•è´¨é‡å’Œæ¸²æŸ“è´¨é‡ä¸Šéƒ½å®ç°äº†æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç¨€ç–è§†è§’ä¸‹çš„å¤šè§†è§’å›¾åƒé‡å»ºæ˜¯è®¡ç®—æœºè§†è§‰çš„æŒ‘æˆ˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨åˆæˆæ–°è§†è§’æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å­˜åœ¨è§†è§‰ä¼ªå½±å’Œä¸‰ç»´ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹å¢å¼ºç¨€ç–è§†è§’çš„ç½‘æ ¼é‡å»ºã€‚</li>
<li>å…±è¯†æ‰©æ•£æ¨¡å—é€šè¿‡å››åˆ†ä½è·åˆ†æè¿‡æ»¤ä¸å¯é çš„ç”Ÿæˆï¼Œæ‰§è¡Œæ–¹å·®æ„ŸçŸ¥å›¾åƒèåˆã€‚</li>
<li>è®¾è®¡äº†åŸºäºä¸Šç½®ä¿¡ç•Œï¼ˆUCBï¼‰çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œè‡ªé€‚åº”é€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„è§†è§’è¿›è¡Œå¢å¼ºã€‚</li>
<li>èåˆå›¾åƒè”åˆç›‘ç£NeRFæ¨¡å‹å’Œç¨€ç–è§†è§’çš„çœŸå®æ•°æ®ï¼Œç¡®ä¿å‡ ä½•å’Œå¤–è§‚çš„ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11946">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f1ca56d1df3669c2c16e58f3e05f01d2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7c9b3281af8c3815692554eb31055595.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f36b2e25bc72146b0a31c8991d297a0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed3bbc7dd81fe466cb6541a6fa0f3515.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e791deb219a6a57e189474deb180f730.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ACE-Attentional-Concept-Erasure-in-Diffusion-Models"><a href="#ACE-Attentional-Concept-Erasure-in-Diffusion-Models" class="headerlink" title="ACE: Attentional Concept Erasure in Diffusion Models"></a>ACE: Attentional Concept Erasure in Diffusion Models</h2><p><strong>Authors:Finn Carter</strong></p>
<p>Large text-to-image diffusion models have demonstrated remarkable image synthesis capabilities, but their indiscriminate training on Internet-scale data has led to learned concepts that enable harmful, copyrighted, or otherwise undesirable content generation. We address the task of concept erasure in diffusion models, i.e., removing a specified concept from a pre-trained model such that prompting the concept (or related synonyms) no longer yields its depiction, while preserving the modelâ€™s ability to generate other content. We propose a novel method, Attentional Concept Erasure (ACE), that integrates a closed-form attention manipulation with lightweight fine-tuning. Theoretically, we formulate concept erasure as aligning the modelâ€™s conditional distribution on the target concept with a neutral distribution. Our approach identifies and nullifies concept-specific latent directions in the cross-attention modules via a gated low-rank adaptation, followed by adversarially augmented fine-tuning to ensure thorough erasure of the concept and its synonyms. Empirically, we demonstrate on multiple benchmarks, including object classes, celebrity faces, explicit content, and artistic styles, that ACE achieves state-of-the-art concept removal efficacy and robustness. Compared to prior methods, ACE better balances generality (erasing concept and related terms) and specificity (preserving unrelated content), scales to dozens of concepts, and is efficient, requiring only a few seconds of adaptation per concept. We will release our code to facilitate safer deployment of diffusion models. </p>
<blockquote>
<p>å¤§å‹æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹å·²ç»æ˜¾ç¤ºå‡ºä»¤äººå°è±¡æ·±åˆ»çš„å›¾åƒåˆæˆèƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨äº’è”ç½‘è§„æ¨¡æ•°æ®ä¸Šçš„éšæ„è®­ç»ƒå¯¼è‡´äº†å­¦åˆ°çš„æ¦‚å¿µèƒ½å¤Ÿç”Ÿæˆæœ‰å®³ã€ç‰ˆæƒæˆ–å…¶ä»–ä¸å—æ¬¢è¿çš„å†…å®¹ã€‚æˆ‘ä»¬è§£å†³äº†æ‰©æ•£æ¨¡å‹ä¸­çš„æ¦‚å¿µæ¶ˆé™¤ä»»åŠ¡ï¼Œå³ä»ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ä¸­ç§»é™¤æŒ‡å®šçš„æ¦‚å¿µï¼Œä½¿å¾—æç¤ºè¯¥æ¦‚å¿µï¼ˆæˆ–ç›¸å…³åŒä¹‰è¯ï¼‰ä¸å†äº§ç”Ÿå…¶æè¿°ï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹ç”Ÿæˆå…¶ä»–å†…å®¹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºæ³¨æ„åŠ›æ¦‚å¿µæ¶ˆé™¤ï¼ˆACEï¼‰ï¼Œå®ƒå°†å½¢å¼åŒ–çš„æ³¨æ„åŠ›æ“çºµä¸è½»é‡çº§å¾®è°ƒç›¸ç»“åˆã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬å°†æ¦‚å¿µæ¶ˆé™¤åˆ¶å®šä¸ºä½¿æ¨¡å‹çš„ç›®æ ‡æ¦‚å¿µæ¡ä»¶åˆ†å¸ƒä¸ä¸­æ€§åˆ†å¸ƒå¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡é—¨æ§ä½ç§©é€‚åº”æ¥è¯†åˆ«å’Œæ¶ˆé™¤äº¤å‰æ³¨æ„æ¨¡å—ä¸­çš„ç‰¹å®šæ¦‚å¿µæ½œåœ¨æ–¹å‘ï¼Œç„¶åé€šè¿‡å¢å¼ºå¯¹æŠ—å¾®è°ƒæ¥ç¡®ä¿å½»åº•æ¶ˆé™¤è¯¥æ¦‚å¿µå’Œå®ƒçš„åŒä¹‰è¯ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒåŒ…æ‹¬å¯¹è±¡ç±»åˆ«ã€åäººé¢å­”ã€æ˜ç¡®å†…å®¹å’Œè‰ºæœ¯é£æ ¼ç­‰ï¼Œæˆ‘ä»¬è¯æ˜ACEåœ¨æ¦‚å¿µå»é™¤æ•ˆæœå’Œç¨³å¥æ€§æ–¹é¢è¾¾åˆ°äº†æœ€ä½³çŠ¶æ€ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼ŒACEåœ¨é€šç”¨æ€§ï¼ˆæ¶ˆé™¤æ¦‚å¿µå’Œç›¸å…³æœ¯è¯­ï¼‰å’Œç‰¹å¼‚æ€§ï¼ˆä¿ç•™ä¸ç›¸å…³å†…å®¹ï¼‰ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ï¼Œèƒ½å¤Ÿæ‰©å±•åˆ°æ•°åä¸ªæ¦‚å¿µï¼Œå¹¶ä¸”æ•ˆç‡å¾ˆé«˜ï¼Œæ¯ä¸ªæ¦‚å¿µåªéœ€è¦å‡ ç§’é’Ÿçš„é€‚åº”æ—¶é—´ã€‚æˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„ä»£ç ï¼Œä»¥ä¿ƒè¿›æ‰©æ•£æ¨¡å‹çš„å®‰å…¨éƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11850v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹å…·æœ‰ä»¤äººç©ç›®çš„å›¾åƒåˆæˆèƒ½åŠ›ï¼Œä½†å…¶å¯¹äº’è”ç½‘è§„æ¨¡æ•°æ®çš„æ•£æ¼«è®­ç»ƒå¯¼è‡´æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæœ‰å®³ã€ç‰ˆæƒæˆ–ä¸å—æ¬¢è¿çš„å†…å®¹ã€‚æœ¬æ–‡è§£å†³æ‰©æ•£æ¨¡å‹ä¸­çš„æ¦‚å¿µæ¶ˆé™¤ä»»åŠ¡ï¼Œå³ä»ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ä¸­ç§»é™¤æŒ‡å®šçš„æ¦‚å¿µï¼Œä½¿å¾—æç¤ºæ¦‚å¿µï¼ˆæˆ–å…¶åŒä¹‰è¯ï¼‰ä¸å†ç”Ÿæˆæè¿°ï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹ç”Ÿæˆå…¶ä»–å†…å®¹çš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”æ³¨æ„åŠ›æ¦‚å¿µæ¶ˆé™¤ï¼ˆACEï¼‰ï¼Œå®ƒå°†å½¢å¼åŒ–çš„æ³¨æ„åŠ›æ“ä½œä¸è½»é‡çº§å¾®è°ƒç›¸ç»“åˆã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬å°†æ¦‚å¿µæ¶ˆé™¤åˆ¶å®šä¸ºå°†æ¨¡å‹å¯¹ç›®æ ‡æ¦‚å¿µçš„æ¡ä»¶åˆ†å¸ƒä¸ä¸­æ€§åˆ†å¸ƒå¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡é—¨æ§ä½ç§©é€‚åº”è¯†åˆ«å¹¶æ¶ˆé™¤äº¤å‰æ³¨æ„æ¨¡å—ä¸­çš„ç‰¹å®šæ¦‚å¿µæ½œåœ¨æ–¹å‘ï¼Œç„¶åè¿›è¡Œå¢å¼ºå¯¹æŠ—å¾®è°ƒï¼Œä»¥ç¡®ä¿å½»åº•æ¶ˆé™¤æ¦‚å¿µåŠå…¶åŒä¹‰è¯ã€‚å®è¯è¡¨æ˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒåŒ…æ‹¬å¯¹è±¡ç±»åˆ«ã€åäººé¢å­”ã€æ˜ç¡®å†…å®¹å’Œè‰ºæœ¯é£æ ¼ç­‰ï¼ŒACEå®ç°äº†æœ€å…ˆè¿›çš„æ¦‚å¿µå»é™¤æ•ˆæœå’Œç¨³å¥æ€§ã€‚ä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼ŒACEåœ¨é€šç”¨æ€§ï¼ˆæ¶ˆé™¤æ¦‚å¿µå’Œç›¸å…³æœ¯è¯­ï¼‰å’Œç‰¹å¼‚æ€§ï¼ˆä¿ç•™æ— å…³å†…å®¹ï¼‰ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ï¼Œèƒ½å¤Ÿæ‰©å±•åˆ°æ•°åä¸ªæ¦‚å¿µï¼Œå¹¶ä¸”æ•ˆç‡å¾ˆé«˜ï¼Œæ¯ä¸ªæ¦‚å¿µåªéœ€è¦å‡ ç§’é’Ÿçš„é€‚åº”æ—¶é—´ã€‚æˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„ä»£ç ï¼Œä»¥ä¿ƒè¿›æ‰©æ•£æ¨¡å‹çš„å®‰å…¨éƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ‰©æ•£æ¨¡å‹è™½ç„¶å…·å¤‡å‡ºè‰²çš„å›¾åƒåˆæˆèƒ½åŠ›ï¼Œä½†å­˜åœ¨ç”Ÿæˆæœ‰å®³ã€ç‰ˆæƒæˆ–ä¸å—æ¬¢è¿å†…å®¹çš„é£é™©ã€‚</li>
<li>è§£å†³æ‰©æ•£æ¨¡å‹ä¸­çš„æ¦‚å¿µæ¶ˆé™¤ä»»åŠ¡å˜å¾—é‡è¦ï¼Œæ—¨åœ¨ç§»é™¤é¢„è®­ç»ƒæ¨¡å‹ä¸­æŒ‡å®šçš„æ¦‚å¿µï¼ŒåŒæ—¶ä¿ç•™å…¶ç”Ÿæˆå…¶ä»–å†…å®¹çš„èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”æ³¨æ„åŠ›æ¦‚å¿µæ¶ˆé™¤ï¼ˆACEï¼‰ï¼Œé€šè¿‡æ•´åˆæ³¨æ„åŠ›æ“ä½œå’Œè½»é‡çº§å¾®è°ƒæ¥å®ç°æ¦‚å¿µæ¶ˆé™¤ã€‚</li>
<li>ACEå°†æ¦‚å¿µæ¶ˆé™¤ç†è®ºåŒ–ï¼Œå°†æ¨¡å‹çš„æ¡ä»¶åˆ†å¸ƒä¸ä¸­æ€§åˆ†å¸ƒå¯¹é½ï¼Œä»¥æ¶ˆé™¤ç‰¹å®šæ¦‚å¿µã€‚</li>
<li>ACEå…·æœ‰å¼ºå¤§çš„å®è¯è¡¨ç°ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å…ˆè¿›çš„æ¦‚å¿µå»é™¤æ•ˆæœå’Œç¨³å¥æ€§ã€‚</li>
<li>ACEåœ¨é€šç”¨æ€§å’Œç‰¹å¼‚æ€§ä¹‹é—´å–å¾—äº†å¹³è¡¡ï¼Œèƒ½å¤Ÿæ‰©å±•åˆ°å¤šä¸ªæ¦‚å¿µï¼Œå¹¶ä¸”å…·æœ‰é«˜æ•ˆçš„é€‚åº”æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11850">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-072b9d54e5e8c11925df7c564952b346.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TextDiffSeg-Text-guided-Latent-Diffusion-Model-for-3d-Medical-Images-Segmentation"><a href="#TextDiffSeg-Text-guided-Latent-Diffusion-Model-for-3d-Medical-Images-Segmentation" class="headerlink" title="TextDiffSeg: Text-guided Latent Diffusion Model for 3d Medical Images   Segmentation"></a>TextDiffSeg: Text-guided Latent Diffusion Model for 3d Medical Images   Segmentation</h2><p><strong>Authors:Kangbo Ma</strong></p>
<p>Diffusion Probabilistic Models (DPMs) have demonstrated significant potential in 3D medical image segmentation tasks. However, their high computational cost and inability to fully capture global 3D contextual information limit their practical applications. To address these challenges, we propose a novel text-guided diffusion model framework, TextDiffSeg. This method leverages a conditional diffusion framework that integrates 3D volumetric data with natural language descriptions, enabling cross-modal embedding and establishing a shared semantic space between visual and textual modalities. By enhancing the modelâ€™s ability to recognize complex anatomical structures, TextDiffSeg incorporates innovative label embedding techniques and cross-modal attention mechanisms, effectively reducing computational complexity while preserving global 3D contextual integrity. Experimental results demonstrate that TextDiffSeg consistently outperforms existing methods in segmentation tasks involving kidney and pancreas tumors, as well as multi-organ segmentation scenarios. Ablation studies further validate the effectiveness of key components, highlighting the synergistic interaction between text fusion, image feature extractor, and label encoder. TextDiffSeg provides an efficient and accurate solution for 3D medical image segmentation, showcasing its broad applicability in clinical diagnosis and treatment planning. </p>
<blockquote>
<p>æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMï¼‰åœ¨3DåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå…¶è¾ƒé«˜çš„è®¡ç®—æˆæœ¬ä»¥åŠæ— æ³•å®Œå…¨æ•è·å…¨å±€3Dä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ–‡æœ¬å¼•å¯¼æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œåä¸ºTextDiffSegã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¡†æ¶ï¼Œå°†3Dä½“ç§¯æ•°æ®ä¸è‡ªç„¶è¯­è¨€æè¿°ç›¸ç»“åˆï¼Œå®ç°è·¨æ¨¡æ€åµŒå…¥ï¼Œå¹¶åœ¨è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´å»ºç«‹å…±äº«è¯­ä¹‰ç©ºé—´ã€‚é€šè¿‡å¢å¼ºæ¨¡å‹è¯†åˆ«å¤æ‚è§£å‰–ç»“æ„çš„èƒ½åŠ›ï¼ŒTextDiffSegç»“åˆäº†åˆ›æ–°çš„æ ‡ç­¾åµŒå…¥æŠ€æœ¯å’Œè·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨é™ä½è®¡ç®—å¤æ‚æ€§çš„åŒæ—¶ï¼Œä¿æŒäº†å…¨å±€3Dä¸Šä¸‹æ–‡çš„å®Œæ•´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTextDiffSegåœ¨æ¶‰åŠè‚¾è„å’Œèƒ°è…ºè‚¿ç˜¤çš„åˆ†å‰²ä»»åŠ¡ä»¥åŠå¤šå™¨å®˜åˆ†å‰²åœºæ™¯ä¸­ï¼Œå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†å…³é”®ç»„ä»¶çš„æœ‰æ•ˆæ€§ï¼Œçªå‡ºäº†æ–‡æœ¬èåˆã€å›¾åƒç‰¹å¾æå–å™¨å’Œæ ‡ç­¾ç¼–ç å™¨ä¹‹é—´çš„ååŒäº¤äº’ã€‚TextDiffSegä¸º3DåŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†é«˜æ•ˆä¸”å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†å…¶åœ¨ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ä¸­çš„å¹¿æ³›åº”ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11825v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMsï¼‰åœ¨3DåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å…¶é«˜è®¡ç®—æˆæœ¬å’Œæ— æ³•å®Œå…¨æ•è·å…¨å±€3Dä¸Šä¸‹æ–‡ä¿¡æ¯é™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…¨æ–°çš„æ–‡æœ¬å¼•å¯¼æ‰©æ•£æ¨¡å‹æ¡†æ¶TextDiffSegã€‚æ­¤æ–¹æ³•åˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¡†æ¶ï¼Œæ•´åˆ3Dä½“ç§¯æ•°æ®ä¸è‡ªç„¶è¯­è¨€æè¿°ï¼Œå®ç°è·¨æ¨¡æ€åµŒå…¥ï¼Œå»ºç«‹è§†è§‰ä¸æ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„å…±äº«è¯­ä¹‰ç©ºé—´ã€‚é€šè¿‡å¢å¼ºæ¨¡å‹è¯†åˆ«å¤æ‚è§£å‰–ç»“æ„çš„èƒ½åŠ›ï¼ŒTextDiffSegèå…¥åˆ›æ–°çš„æ ‡ç­¾åµŒå…¥æŠ€æœ¯å’Œè·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨é™ä½è®¡ç®—å¤æ‚æ€§çš„åŒæ—¶ä¿æŒå…¨å±€3Dä¸Šä¸‹æ–‡å®Œæ•´æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTextDiffSegåœ¨è‚¾è„ã€èƒ°è…ºè‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ä»¥åŠå¤šå™¨å®˜åˆ†å‰²åœºæ™¯ä¸­å‡è¡¨ç°å‡ºè¶…è¶Šç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†å…³é”®ç»„ä»¶çš„æœ‰æ•ˆæ€§ï¼Œçªæ˜¾äº†æ–‡æœ¬èåˆã€å›¾åƒç‰¹å¾æå–å™¨å’Œæ ‡ç­¾ç¼–ç å™¨ä¹‹é—´çš„ååŒä½œç”¨ã€‚TextDiffSegä¸º3DåŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†é«˜æ•ˆå‡†ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†å…¶åœ¨ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ä¸­çš„å¹¿æ³›åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMsï¼‰åœ¨3DåŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¡¨ç°å‡ºæ½œåŠ›ï¼Œä½†å­˜åœ¨é«˜è®¡ç®—æˆæœ¬å’Œæ•è·ä¿¡æ¯ä¸å…¨çš„é—®é¢˜ã€‚</li>
<li>æ–°å‹æ–‡æœ¬å¼•å¯¼æ‰©æ•£æ¨¡å‹æ¡†æ¶TextDiffSegè¢«æå‡ºï¼Œæ•´åˆ3Dæ•°æ®ä¸è‡ªç„¶è¯­è¨€æè¿°ï¼Œå»ºç«‹è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„å…±äº«è¯­ä¹‰ç©ºé—´ã€‚</li>
<li>TextDiffSegé€šè¿‡åˆ›æ–°æŠ€æœ¯å¢å¼ºæ¨¡å‹è¯†åˆ«å¤æ‚è§£å‰–ç»“æ„çš„èƒ½åŠ›ï¼Œé™ä½è®¡ç®—å¤æ‚æ€§çš„åŒæ—¶ä¿æŒä¸Šä¸‹æ–‡å®Œæ•´æ€§ã€‚</li>
<li>TextDiffSegåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­æ€§èƒ½è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è‚¾è„ã€èƒ°è…ºè‚¿ç˜¤åŠå¤šå™¨å®˜åˆ†å‰²åœºæ™¯ä¸­ã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯å®äº†TextDiffSegçš„å…³é”®ç»„ä»¶ï¼ŒåŒ…æ‹¬æ–‡æœ¬èåˆã€å›¾åƒç‰¹å¾æå–å™¨å’Œæ ‡ç­¾ç¼–ç å™¨çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>TextDiffSegæä¾›äº†é«˜æ•ˆçš„3DåŒ»å­¦å›¾åƒåˆ†å‰²è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11825">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-37b3fe4bca2d564dcf6d056355a8575a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-976c87d19980198279c305d18f977e56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4b723e9448318669ce67ea6d3f3f30a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b7623f857d2a0eef24db11ebf73833ea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f61c79d602c8dbdeb1e4e9d5ed42fc13.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Towards-Safe-Synthetic-Image-Generation-On-the-Web-A-Multimodal-Robust-NSFW-Defense-and-Million-Scale-Dataset"><a href="#Towards-Safe-Synthetic-Image-Generation-On-the-Web-A-Multimodal-Robust-NSFW-Defense-and-Million-Scale-Dataset" class="headerlink" title="Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust   NSFW Defense and Million Scale Dataset"></a>Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust   NSFW Defense and Million Scale Dataset</h2><p><strong>Authors:Muhammad Shahid Muneer, Simon S. Woo</strong></p>
<p>In the past years, we have witnessed the remarkable success of Text-to-Image (T2I) models and their widespread use on the web. Extensive research in making T2I models produce hyper-realistic images has led to new concerns, such as generating Not-Safe-For-Work (NSFW) web content and polluting the web society. To help prevent misuse of T2I models and create a safer web environment for users features like NSFW filters and post-hoc security checks are used in these models. However, recent work unveiled how these methods can easily fail to prevent misuse. In particular, adversarial attacks on text and image modalities can easily outplay defensive measures. %Exploiting such leads to the growing concern of preventing adversarial attacks on text and image modalities. Moreover, there is currently no robust multimodal NSFW dataset that includes both prompt and image pairs and adversarial examples. This work proposes a million-scale prompt and image dataset generated using open-source diffusion models. Second, we develop a multimodal defense to distinguish safe and NSFW text and images, which is robust against adversarial attacks and directly alleviates current challenges. Our extensive experiments show that our model performs well against existing SOTA NSFW detection methods in terms of accuracy and recall, drastically reducing the Attack Success Rate (ASR) in multimodal adversarial attack scenarios. Code: <a target="_blank" rel="noopener" href="https://github.com/shahidmuneer/multimodal-nsfw-defense">https://github.com/shahidmuneer/multimodal-nsfw-defense</a>. </p>
<blockquote>
<p>è¿‡å»å‡ å¹´ï¼Œæˆ‘ä»¬è§è¯äº†æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹çš„å·¨å¤§æˆåŠŸåŠå…¶åœ¨ç½‘ä¸Šçš„å¹¿æ³›åº”ç”¨ã€‚å…³äºå¦‚ä½•ä½¿T2Iæ¨¡å‹ç”Ÿæˆè¶…é€¼çœŸå›¾åƒçš„å¹¿æ³›ç ”ç©¶å¼•å‘äº†ä¸€äº›æ–°çš„æ‹…å¿§ï¼Œä¾‹å¦‚ç”Ÿæˆä¸é€‚åˆå·¥ä½œåœºåˆï¼ˆNSFWï¼‰çš„ç½‘é¡µå†…å®¹ä»¥åŠæ±¡æŸ“ç½‘ç»œç¤¾ä¼šã€‚ä¸ºäº†é˜²æ­¢æ»¥ç”¨T2Iæ¨¡å‹å¹¶ä¸ºç”¨æˆ·åˆ›å»ºæ›´å®‰å…¨çš„ç½‘ç»œç¯å¢ƒï¼Œè¿™äº›æ¨¡å‹ä¸­é‡‡ç”¨äº†ä¸é€‚åˆå·¥ä½œåœºåˆï¼ˆNSFWï¼‰çš„è¿‡æ»¤å™¨å’Œäº‹åå®‰å…¨æ£€æŸ¥ç­‰åŠŸèƒ½ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•å¾ˆå®¹æ˜“æ— æ³•é˜²æ­¢æ»¥ç”¨ã€‚ç‰¹åˆ«æ˜¯æ–‡æœ¬å’Œå›¾åƒæ¨¡æ€çš„å¯¹æŠ—æ€§æ”»å‡»å¯ä»¥è½»æ¾æˆ˜èƒœé˜²å¾¡æªæ–½ã€‚å› æ­¤ï¼Œäººä»¬è¶Šæ¥è¶Šæ‹…å¿ƒé˜²æ­¢æ–‡æœ¬å’Œå›¾åƒæ¨¡æ€çš„å¯¹æŠ—æ€§æ”»å‡»çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œå½“å‰å°šæ²¡æœ‰ä¸€ä¸ªåŒ…å«æç¤ºã€å›¾åƒå¯¹å’Œå¯¹æŠ—å®ä¾‹çš„ç¨³å¥çš„å¤šæ¨¡æ€NSFWæ•°æ®é›†ã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ä¸ªä½¿ç”¨å¼€æºæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å¤§è§„æ¨¡æç¤ºå’Œå›¾åƒæ•°æ®é›†ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¤šæ¨¡æ€é˜²å¾¡æ–¹æ³•ï¼Œç”¨äºåŒºåˆ†å®‰å…¨å’Œä¸é€‚åˆå·¥ä½œåœºåˆçš„æ–‡æœ¬å’Œå›¾åƒï¼Œè¯¥æ–¹æ³•å¯¹å¯¹æŠ—æ€§æ”»å‡»å…·æœ‰å¼ºå¤§çš„é˜²å¾¡èƒ½åŠ›ï¼Œå¹¶ç›´æ¥ç¼“è§£äº†å½“å‰é¢ä¸´çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å‡†ç¡®ç‡å’Œå¬å›ç‡æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä¸ç°æœ‰çš„æœ€ä½³NSFWæ£€æµ‹æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨å¤šæ¨¡æ€å¯¹æŠ—æ€§æ”»å‡»åœºæ™¯ä¸­å¤§å¤§é™ä½äº†æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/shahidmuneer/multimodal-nsfw-defense%E3%80%82">https://github.com/shahidmuneer/multimodal-nsfw-defenseã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11707v1">PDF</a> Short Paper The Web Conference</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹çš„å¹¿æ³›åº”ç”¨åŠå…¶ç”Ÿæˆä¸å®‰å…¨çš„ç½‘ç»œå†…å®¹ï¼ˆNSFWï¼‰çš„æ‹…å¿§ã€‚ä¸ºé˜²æ­¢æ»¥ç”¨å’Œåˆ›å»ºæ›´å®‰å…¨çš„ç½‘ç»œç¯å¢ƒï¼Œé‡‡ç”¨äº†NSFWè¿‡æ»¤å™¨ã€äº‹åå®‰å…¨æ£€æŸ¥ç­‰åŠŸèƒ½ã€‚ç„¶è€Œï¼Œæ–°æ–¹æ³•å®¹æ˜“è¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œå¯¹æŠ—æ€§æ”»å‡»æ–‡æœ¬å’Œå›¾åƒæ¨¡å¼å¯ä»¥è½»æ¾ç»•è¿‡é˜²å¾¡æªæ–½ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªå¤§è§„æ¨¡æç¤ºå’Œå›¾åƒæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨å¼€æºæ‰©æ•£æ¨¡å‹ç”Ÿæˆï¼ŒåŒæ—¶å¼€å‘äº†ä¸€ç§å¤šæ¨¡å¼é˜²å¾¡æ¥åŒºåˆ†å®‰å…¨å’ŒNSFWçš„æ–‡æœ¬å’Œå›¾åƒï¼Œå¯¹æŠ—æ”»å‡»è¡¨ç°è‰¯å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2Iæ¨¡å‹ç”Ÿæˆé«˜æ¸…æ™°åº¦å›¾åƒå–å¾—æ˜¾è‘—æˆåŠŸï¼Œä½†å­˜åœ¨ç”ŸæˆNSFWå†…å®¹çš„æ‹…å¿§ã€‚</li>
<li>NSFWè¿‡æ»¤å™¨åŠäº‹åå®‰å…¨æ£€æŸ¥æ˜¯é˜²æ­¢æ»¥ç”¨T2Iæ¨¡å‹çš„æ‰‹æ®µï¼Œä½†å­˜åœ¨å¤±æ•ˆé£é™©ã€‚</li>
<li>å¯¹æŠ—æ€§æ”»å‡»å¯ä»¥è½»æ˜“ç»•è¿‡ç°æœ‰é˜²å¾¡æªæ–½ã€‚</li>
<li>ç¼ºä¹åŒ…å«æç¤ºå’Œå›¾åƒå¯¹ä»¥åŠå¯¹æŠ—æ€§å®ä¾‹çš„å¤šæ¨¡å¼NSFWæ•°æ®é›†ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ä¸ªå¤§è§„æ¨¡æç¤ºå’Œå›¾åƒæ•°æ®é›†ï¼Œä½¿ç”¨å¼€æºæ‰©æ•£æ¨¡å‹ç”Ÿæˆã€‚</li>
<li>å¼€å‘äº†ä¸€ç§å¤šæ¨¡å¼é˜²å¾¡ï¼Œå¯åŒºåˆ†å®‰å…¨å’ŒNSFWçš„æ–‡æœ¬å’Œå›¾åƒã€‚</li>
<li>è¯¥é˜²å¾¡åœ¨å‡†ç¡®ç‡å’Œå¬å›ç‡æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œå¤§å¹…é™ä½å¤šæ¨¡å¼å¯¹æŠ—æ”»å‡»åœºæ™¯ä¸­çš„æ”»å‡»æˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11707">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-19b6f74135ce6fe01f4b73eb8a16eeed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6cdb67649f69509be4258bb04b4bb7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2aebfc915f3f4198bbaa84415a2f6c41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0931f2de68f268c0b559298c75206f9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-042e9701b85b2f271f7a51f786762a5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d818d1c05378013ce096560f6d3fb13c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbbfb5f7faa603c406acb9592d2afe68.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DyDiT-Dynamic-Diffusion-Transformers-for-Efficient-Visual-Generation"><a href="#DyDiT-Dynamic-Diffusion-Transformers-for-Efficient-Visual-Generation" class="headerlink" title="DyDiT++: Dynamic Diffusion Transformers for Efficient Visual Generation"></a>DyDiT++: Dynamic Diffusion Transformers for Efficient Visual Generation</h2><p><strong>Authors:Wangbo Zhao, Yizeng Han, Jiasheng Tang, Kai Wang, Hao Luo, Yibing Song, Gao Huang, Fan Wang, Yang You</strong></p>
<p>Diffusion Transformer (DiT), an emerging diffusion model for visual generation, has demonstrated superior performance but suffers from substantial computational costs. Our investigations reveal that these costs primarily stem from the \emph{static} inference paradigm, which inevitably introduces redundant computation in certain \emph{diffusion timesteps} and \emph{spatial regions}. To overcome this inefficiency, we propose \textbf{Dy}namic \textbf{Di}ffusion \textbf{T}ransformer (DyDiT), an architecture that \emph{dynamically} adjusts its computation along both \emph{timestep} and \emph{spatial} dimensions. Specifically, we introduce a \emph{Timestep-wise Dynamic Width} (TDW) approach that adapts model width conditioned on the generation timesteps. In addition, we design a \emph{Spatial-wise Dynamic Token} (SDT) strategy to avoid redundant computation at unnecessary spatial locations. TDW and SDT can be seamlessly integrated into DiT and significantly accelerates the generation process. Building on these designs, we further enhance DyDiT in three key aspects. First, DyDiT is integrated seamlessly with flow matching-based generation, enhancing its versatility. Furthermore, we enhance DyDiT to tackle more complex visual generation tasks, including video generation and text-to-image generation, thereby broadening its real-world applications. Finally, to address the high cost of full fine-tuning and democratize technology access, we investigate the feasibility of training DyDiT in a parameter-efficient manner and introduce timestep-based dynamic LoRA (TD-LoRA). Extensive experiments on diverse visual generation models, including DiT, SiT, Latte, and FLUX, demonstrate the effectiveness of DyDiT. </p>
<blockquote>
<p>æ‰©æ•£Transformerï¼ˆDiTï¼‰æ˜¯ä¸€ç§æ–°å…´çš„è§†è§‰ç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼Œå®ƒè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥è¡¨æ˜ï¼Œè¿™äº›æˆæœ¬ä¸»è¦æºäº\emph{é™æ€}æ¨ç†èŒƒå¼ï¼Œè¿™ç§èŒƒå¼ä¸å¯é¿å…åœ°ä¼šåœ¨æŸäº›\emph{æ‰©æ•£æ—¶é—´æ­¥}å’Œ\emph{ç©ºé—´åŒºåŸŸ}ä¸­å¼•å…¥å†—ä½™è®¡ç®—ã€‚ä¸ºäº†å…‹æœè¿™ç§ä½æ•ˆï¼Œæˆ‘ä»¬æå‡ºäº†\textbf{Dy}namic \textbf{Di}ffusion \textbf{T}ransformerï¼ˆDyDiTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§\emph{åŠ¨æ€}è°ƒæ•´å…¶åœ¨\emph{æ—¶é—´æ­¥}å’Œ\emph{ç©ºé—´}ç»´åº¦ä¸Šè®¡ç®—çš„ç»“æ„ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§\emph{æ—¶é—´æ­¥åŠ¨æ€å®½åº¦}ï¼ˆTDWï¼‰æ–¹æ³•ï¼Œæ ¹æ®ç”Ÿæˆæ—¶é—´æ­¥æ¥é€‚åº”æ¨¡å‹å®½åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§\emph{ç©ºé—´åŠ¨æ€ä»¤ç‰Œ}ï¼ˆSDTï¼‰ç­–ç•¥ï¼Œä»¥é¿å…åœ¨ä¸å¿…è¦çš„ç©ºé—´ä½ç½®è¿›è¡Œå†—ä½™è®¡ç®—ã€‚TDWå’ŒSDTå¯ä»¥æ— ç¼é›†æˆåˆ°DiTä¸­ï¼Œå¹¶æ˜¾è‘—åŠ é€Ÿç”Ÿæˆè¿‡ç¨‹ã€‚åŸºäºè¿™äº›è®¾è®¡ï¼Œæˆ‘ä»¬ä»ä¸‰ä¸ªæ–¹é¢è¿›ä¸€æ­¥å¢å¼ºäº†DyDiTã€‚é¦–å…ˆï¼ŒDyDiTå¯ä»¥ä¸åŸºäºæµåŒ¹é…çš„ç”Ÿæˆæ–¹æ³•æ— ç¼é›†æˆï¼Œæé«˜å…¶é€šç”¨æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¢å¼ºäº†DyDiTä»¥å¤„ç†æ›´å¤æ‚çš„è§†è§‰ç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬è§†é¢‘ç”Ÿæˆå’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼Œä»è€Œæ‰©å¤§äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚æœ€åï¼Œä¸ºäº†è§£å†³å…¨ç²¾ç»†è°ƒæ•´çš„é«˜æˆæœ¬å¹¶å®ç°æŠ€æœ¯çš„æ™®åŠï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä»¥å‚æ•°æ•ˆç‡é«˜çš„æ–¹å¼è®­ç»ƒDyDiTçš„å¯è¡Œæ€§ï¼Œå¹¶å¼•å…¥äº†åŸºäºæ—¶é—´æ­¥çš„åŠ¨æ€LoRAï¼ˆTD-LoRAï¼‰ã€‚åœ¨åŒ…æ‹¬DiTã€SiTã€Latteå’ŒFLUXç­‰å¤šç§è§†è§‰ç”Ÿæˆæ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†DyDiTçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06803v2">PDF</a> Extended journal version for ICLR. arXiv admin note: substantial text   overlap with arXiv:2410.03456</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹çš„å‰æ²¿æŠ€æœ¯Diffusion Transformerï¼ˆDiTï¼‰è™½ç„¶æ€§èƒ½å“è¶Šï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚ç ”ç©¶å›¢é˜Ÿå‘ç°ä¸»è¦åŸå› æ˜¯é™æ€æ¨ç†æ¨¡å¼å¯¼è‡´çš„å†—ä½™è®¡ç®—ã€‚ä¸ºæ­¤ï¼Œä»–ä»¬æå‡ºäº†åŠ¨æ€æ‰©æ•£æ¨¡å‹DyDiTï¼Œèƒ½å¤Ÿæ ¹æ®ç”Ÿæˆæ­¥éª¤å’Œæ—¶é—´æ­¥é•¿åŠ¨æ€è°ƒæ•´è®¡ç®—å®½åº¦å’Œç©ºé—´åŒºåŸŸçš„è®¡ç®—ï¼Œä»¥é™ä½æˆæœ¬å¹¶æé«˜æ•ˆç‡ã€‚å›¢é˜Ÿè®¾è®¡äº†Time-step wise Dynamic Widthï¼ˆTDWï¼‰å’ŒSpatial-wise Dynamic Tokenï¼ˆSDTï¼‰ç­–ç•¥ã€‚è¿›ä¸€æ­¥èå…¥æµåŒ¹é…ç”Ÿæˆå¢å¼ºæ³›åŒ–æ€§ã€è§†é¢‘ç”Ÿæˆã€æ–‡æœ¬ç”Ÿæˆç­‰åŠŸèƒ½ï¼Œå¹¶æ¢ç´¢äº†å‚æ•°é«˜æ•ˆçš„è®­ç»ƒæ–¹æ³•TD-LoRAã€‚å®éªŒè¯æ˜DyDiTçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion Transformerï¼ˆDiTï¼‰å…·æœ‰å‡ºè‰²çš„æ€§èƒ½ä½†é¢ä¸´è¾ƒé«˜çš„è®¡ç®—æˆæœ¬é—®é¢˜ã€‚</li>
<li>ä¸»è¦æˆæœ¬æºäºé™æ€æ¨ç†æ¨¡å¼åœ¨ç‰¹å®šæ‰©æ•£æ—¶é—´æ­¥å’Œç©ºé—´åŒºåŸŸçš„å†—ä½™è®¡ç®—ã€‚</li>
<li>DyDiTè¢«æå‡ºä»¥åŠ¨æ€è°ƒæ•´è®¡ç®—å®½åº¦å’Œæ—¶é—´æ­¥é•¿ï¼Œé¿å…å†—ä½™è®¡ç®—ã€‚</li>
<li>DyDiTé€šè¿‡å¼•å…¥Timestep-wise Dynamic Widthï¼ˆTDWï¼‰å’ŒSpatial-wise Dynamic Tokenï¼ˆSDTï¼‰ç­–ç•¥å®ç°ä¼˜åŒ–ã€‚</li>
<li>DyDiTæ”¯æŒå¤šç§ç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬è§†é¢‘ç”Ÿæˆå’Œæ–‡æœ¬ç”Ÿæˆç­‰ï¼Œå¢å¼ºäº†å…¶å®é™…åº”ç”¨èƒ½åŠ›ã€‚</li>
<li>DyDiTé€šè¿‡èå…¥æµåŒ¹é…ç”ŸæˆæŠ€æœ¯å¢å¼ºäº†å…¶æ³›åŒ–æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06803">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b0eadcc96cb77fba5e57e5b0e8171be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-955078ccadd86f2db94ae6051465bcc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e98597211d63e89b31262982a4906b04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df688688a767719ae1963e6731b90bb7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="OpenSDI-Spotting-Diffusion-Generated-Images-in-the-Open-World"><a href="#OpenSDI-Spotting-Diffusion-Generated-Images-in-the-Open-World" class="headerlink" title="OpenSDI: Spotting Diffusion-Generated Images in the Open World"></a>OpenSDI: Spotting Diffusion-Generated Images in the Open World</h2><p><strong>Authors:Yabin Wang, Zhiwu Huang, Xiaopeng Hong</strong></p>
<p>This paper identifies OpenSDI, a challenge for spotting diffusion-generated images in open-world settings. In response to this challenge, we define a new benchmark, the OpenSDI dataset (OpenSDID), which stands out from existing datasets due to its diverse use of large vision-language models that simulate open-world diffusion-based manipulations. Another outstanding feature of OpenSDID is its inclusion of both detection and localization tasks for images manipulated globally and locally by diffusion models. To address the OpenSDI challenge, we propose a Synergizing Pretrained Models (SPM) scheme to build up a mixture of foundation models. This approach exploits a collaboration mechanism with multiple pretrained foundation models to enhance generalization in the OpenSDI context, moving beyond traditional training by synergizing multiple pretrained models through prompting and attending strategies. Building on this scheme, we introduce MaskCLIP, an SPM-based model that aligns Contrastive Language-Image Pre-Training (CLIP) with Masked Autoencoder (MAE). Extensive evaluations on OpenSDID show that MaskCLIP significantly outperforms current state-of-the-art methods for the OpenSDI challenge, achieving remarkable relative improvements of 14.23% in IoU (14.11% in F1) and 2.05% in accuracy (2.38% in F1) compared to the second-best model in localization and detection tasks, respectively. Our dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/iamwangyabin/OpenSDI">https://github.com/iamwangyabin/OpenSDI</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†OpenSDIæŒ‘æˆ˜ï¼Œå³åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­è¯†åˆ«æ‰©æ•£ç”Ÿæˆå›¾åƒçš„æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œå³OpenSDIæ•°æ®é›†ï¼ˆOpenSDIDï¼‰ã€‚ç”±äºå®ƒä½¿ç”¨äº†æ¨¡æ‹Ÿå¼€æ”¾ä¸–ç•Œæ‰©æ•£æ“ä½œçš„å¤šç§å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå› æ­¤ä¸å…¶ä»–ç°æœ‰æ•°æ®é›†æœ‰æ‰€ä¸åŒã€‚OpenSDIDçš„å¦ä¸€ä¸ªçªå‡ºç‰¹ç‚¹æ˜¯å®ƒåŒ…å«äº†ç”¨äºå…¨å±€å’Œå±€éƒ¨æ‰©æ•£æ¨¡å‹æ“ä½œå›¾åƒçš„æ£€æµ‹å’Œå®šä½ä»»åŠ¡ã€‚ä¸ºäº†åº”å¯¹OpenSDIæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ååŒé¢„è®­ç»ƒæ¨¡å‹ï¼ˆSPMï¼‰æ–¹æ¡ˆï¼Œç”¨äºæ„å»ºæ··åˆåŸºç¡€æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•é€šè¿‡åä½œæœºåˆ¶åˆ©ç”¨å¤šä¸ªé¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œä»¥å¢å¼ºåœ¨OpenSDIç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€šè¿‡æç¤ºå’Œæ³¨æ„åŠ›ç­–ç•¥ååŒå¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œè¶…è¶Šä¼ ç»Ÿè®­ç»ƒã€‚åŸºäºè¿™ä¸€æ–¹æ¡ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†MaskCLIPï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºSPMçš„æ¨¡å‹ï¼Œå®ƒå°†å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰ä¸æ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆMAEï¼‰ç›¸ç»“åˆã€‚åœ¨OpenSDIDä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œä¸å½“å‰æœ€å¥½çš„OpenSDIæŒ‘æˆ˜æ–¹æ³•ç›¸æ¯”ï¼ŒMaskCLIPåœ¨IoUï¼ˆæé«˜14.23%ï¼‰å’Œå‡†ç¡®ç‡ï¼ˆæé«˜2.05%ï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„ç›¸å¯¹æ”¹è¿›ï¼Œåœ¨å®šä½å’Œæ£€æµ‹ä»»åŠ¡ä¸Šçš„F1å¾—åˆ†åˆ†åˆ«æé«˜äº†14.11%å’Œ2.38%ï¼Œæˆä¸ºå½“å‰æœ€ä¼˜æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/iamwangyabin/OpenSDI%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/iamwangyabin/OpenSDIä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19653v3">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†OpenSDIæŒ‘æˆ˜ï¼Œå³è¯†åˆ«å¼€æ”¾ä¸–ç•Œä¸­æ‰©æ•£ç”Ÿæˆçš„å›¾åƒçš„æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œå®šä¹‰äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•é›†OpenSDIæ•°æ®é›†ï¼ˆOpenSDIDï¼‰ï¼Œå®ƒä½¿ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿå¼€æ”¾ä¸–ç•Œçš„æ‰©æ•£æ“ä½œï¼Œå¹¶åŒ…å«æ£€æµ‹å’Œå®šä½ä»»åŠ¡ã€‚æå‡ºååŒé¢„è®­ç»ƒæ¨¡å‹ï¼ˆSPMï¼‰æ–¹æ¡ˆï¼Œé€šè¿‡ååŒå¤šç§é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œæé«˜åœ¨OpenSDIèƒŒæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åŸºäºè¿™ä¸€æ–¹æ¡ˆï¼Œå¼•å…¥äº†MaskCLIPæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰ä¸æ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆMAEï¼‰ç›¸ç»“åˆã€‚åœ¨OpenSDIDä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒMaskCLIPåœ¨OpenSDIæŒ‘æˆ˜ä¸Šæ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨å®šä½å’Œæ£€æµ‹ä»»åŠ¡ä¸Šçš„ç›¸å¯¹æ”¹è¿›ç‡åˆ†åˆ«ä¸º14.23%ï¼ˆIoUï¼‰å’Œ2.05%ï¼ˆå‡†ç¡®ç‡ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenSDIæŒ‘æˆ˜æ˜¯è¯†åˆ«å¼€æ”¾ä¸–ç•Œä¸­æ‰©æ•£ç”Ÿæˆçš„å›¾åƒçš„é—®é¢˜ã€‚</li>
<li>OpenSDIDæ•°æ®é›†ä½¿ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿå¼€æ”¾ä¸–ç•Œçš„æ‰©æ•£æ“ä½œã€‚</li>
<li>OpenSDIDåŒ…å«æ£€æµ‹å’Œå®šä½ä»»åŠ¡ï¼Œä»¥åº”å¯¹å…¨å±€å’Œå±€éƒ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒã€‚</li>
<li>æå‡ºååŒé¢„è®­ç»ƒæ¨¡å‹ï¼ˆSPMï¼‰æ–¹æ¡ˆä»¥æé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>MaskCLIPæ¨¡å‹ç»“åˆCLIPå’ŒMAEæŠ€æœ¯ï¼Œä»¥åº”å¯¹OpenSDIæŒ‘æˆ˜ã€‚</li>
<li>MaskCLIPåœ¨OpenSDIDä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19653">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5388eece71da2d9e75f7ddecce05349.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69ed092a7644e0d7c1d9f521b7197649.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd77e5bd119be7c97091f3e907209e71.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0b2353df3cf9adb8ab40449eb2a9354f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="OMR-Diffusion-Optimizing-Multi-Round-Enhanced-Training-in-Diffusion-Models-for-Improved-Intent-Understanding"><a href="#OMR-Diffusion-Optimizing-Multi-Round-Enhanced-Training-in-Diffusion-Models-for-Improved-Intent-Understanding" class="headerlink" title="OMR-Diffusion:Optimizing Multi-Round Enhanced Training in Diffusion   Models for Improved Intent Understanding"></a>OMR-Diffusion:Optimizing Multi-Round Enhanced Training in Diffusion   Models for Improved Intent Understanding</h2><p><strong>Authors:Kun Li, Jianhui Wang, Miao Zhang, Xueqian Wang</strong></p>
<p>Generative AI has significantly advanced text-driven image generation, but it still faces challenges in producing outputs that consistently align with evolving user preferences and intents, particularly in multi-turn dialogue scenarios. In this research, We present a Visual Co-Adaptation (VCA) framework that incorporates human-in-the-loop feedback, utilizing a well-trained reward model specifically designed to closely align with human preferences. Using a diverse multi-turn dialogue dataset, the framework applies multiple reward functions (such as diversity, consistency, and preference feedback) to refine the diffusion model through LoRA, effectively optimizing image generation based on user input. We also constructed multi-round dialogue datasets with prompts and image pairs that well-fit user intent. Experiments show the model achieves 508 wins in human evaluation, outperforming DALL-E 3 (463 wins) and others. It also achieves 3.4 rounds in dialogue efficiency (vs. 13.7 for DALL-E 3) and excels in metrics like LPIPS (0.15) and BLIP (0.59). Various experiments demonstrate the effectiveness of the proposed method over state-of-the-art baselines, with significant improvements in image consistency and alignment with user intent. </p>
<blockquote>
<p>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æå¤§åœ°æ¨åŠ¨äº†æ–‡æœ¬é©±åŠ¨å›¾åƒç”ŸæˆæŠ€æœ¯çš„å‘å±•ï¼Œä½†åœ¨å¤šè½®å¯¹è¯åœºæ™¯ä¸­ï¼Œå®ƒä»ç„¶é¢ä¸´ç€åœ¨æŒç»­äº§ç”Ÿç¬¦åˆä¸æ–­å˜åŒ–çš„ç”¨æˆ·åå¥½å’Œæ„å›¾çš„è¾“å‡ºæ–¹é¢çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è§†è§‰ååŒé€‚åº”ï¼ˆVCAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†äººç±»å¾ªç¯åé¦ˆï¼Œåˆ©ç”¨ç»è¿‡è‰¯å¥½è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼Œä¸“é—¨è®¾è®¡ä»¥ç´§å¯†ç¬¦åˆäººç±»åå¥½ã€‚ä½¿ç”¨å¤šæ ·åŒ–çš„å¤šè½®å¯¹è¯æ•°æ®é›†ï¼Œè¯¥æ¡†æ¶åº”ç”¨å¤šä¸ªå¥–åŠ±å‡½æ•°ï¼ˆå¦‚å¤šæ ·æ€§ã€ä¸€è‡´æ€§å’Œåå¥½åé¦ˆï¼‰æ¥é€šè¿‡LoRAç»†åŒ–æ‰©æ•£æ¨¡å‹ï¼Œæœ‰æ•ˆåœ°æ ¹æ®ç”¨æˆ·è¾“å…¥ä¼˜åŒ–å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†å¤šè½®å¯¹è¯æ•°æ®é›†ï¼Œå¸¦æœ‰ç¬¦åˆç”¨æˆ·æ„å›¾çš„æç¤ºå’Œå›¾åƒå¯¹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨äººå·¥è¯„ä¼°ä¸­è·å¾—508æ¬¡èƒœåˆ©ï¼Œä¼˜äºDALL-E 3ï¼ˆ463æ¬¡èƒœåˆ©ï¼‰ï¼Œä»¥åŠå…¶ä»–æ¨¡å‹ã€‚åœ¨å¯¹è¯æ•ˆç‡æ–¹é¢ï¼Œå®ƒè¾¾åˆ°äº†3.4è½®ï¼ˆä¸DALL-E 3çš„13.7è½®ç›¸æ¯”ï¼‰ï¼Œå¹¶åœ¨LPIPSï¼ˆ0.15ï¼‰å’ŒBLIPï¼ˆ0.59ï¼‰ç­‰æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ã€‚å„é¡¹å®éªŒè¡¨æ˜ï¼Œç›¸å¯¹äºæœ€å…ˆè¿›çš„åŸºçº¿æŠ€æœ¯ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å›¾åƒä¸€è‡´æ€§å’Œç¬¦åˆç”¨æˆ·æ„å›¾æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17660v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è§†è§‰ååŒé€‚åº”ï¼ˆVCAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäººç±»ç”¨æˆ·åé¦ˆæœºåˆ¶è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤Ÿç²¾ç¡®ç¬¦åˆäººç±»åå¥½ã€‚é€šè¿‡ä½¿ç”¨å¤šæ ·åŒ–å¤šè½®å¯¹è¯æ•°æ®é›†ï¼Œè¯¥æ¡†æ¶åº”ç”¨å¤šä¸ªå¥–åŠ±å‡½æ•°æ¥ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨æˆ·è¾“å…¥èƒ½å¤Ÿå½±å“å›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨äººç±»è¯„ä¼°ä¸­å–å¾—äº†æ˜¾è‘—ä¼˜åŠ¿ï¼Œè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰ååŒé€‚åº”ï¼ˆVCAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç»“åˆäººç±»ç”¨æˆ·åé¦ˆæœºåˆ¶è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œä»¥ç¬¦åˆäººç±»åå¥½ã€‚</li>
<li>åˆ©ç”¨å¤šæ ·åŒ–å¤šè½®å¯¹è¯æ•°æ®é›†ï¼Œåº”ç”¨å¤šä¸ªå¥–åŠ±å‡½æ•°ï¼ˆå¦‚å¤šæ ·æ€§ã€ä¸€è‡´æ€§å’Œåå¥½åé¦ˆï¼‰æ¥ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>é€šè¿‡ç”¨æˆ·è¾“å…¥æ¥å½±å“å›¾åƒç”Ÿæˆè¿‡ç¨‹ï¼Œå®ç°äº†å¯¹ç”¨æˆ·æ„å›¾çš„ç²¾å‡†å“åº”ã€‚</li>
<li>åœ¨äººç±»è¯„ä¼°ä¸­å–å¾—äº†æ˜¾è‘—ä¼˜åŠ¿ï¼Œè¶…è¿‡äº†å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹DALL-E 3ã€‚</li>
<li>æ¨¡å‹åœ¨å¯¹è¯æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†3.4è½®å¯¹è¯æ•ˆç‡ã€‚</li>
<li>åœ¨å›¾åƒä¸€è‡´æ€§ä»¥åŠä¸ç”¨æˆ·æ„å›¾å¯¹é½æ–¹é¢å®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17660">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-707d1e04e16aa09730069e37f177d24e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83ca96adcd6d19d7305b7df3f51bb256.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Fine-Tuning-Diffusion-Generative-Models-via-Rich-Preference-Optimization"><a href="#Fine-Tuning-Diffusion-Generative-Models-via-Rich-Preference-Optimization" class="headerlink" title="Fine-Tuning Diffusion Generative Models via Rich Preference Optimization"></a>Fine-Tuning Diffusion Generative Models via Rich Preference Optimization</h2><p><strong>Authors:Hanyang Zhao, Haoxian Chen, Yucheng Guo, Genta Indra Winata, Tingting Ou, Ziyu Huang, David D. Yao, Wenpin Tang</strong></p>
<p>We introduce Rich Preference Optimization (RPO), a novel pipeline that leverages rich feedback signals to improve the curation of preference pairs for fine-tuning text-to-image diffusion models. Traditional methods, like Diffusion-DPO, often rely solely on reward model labeling, which can be opaque, offer limited insights into the rationale behind preferences, and are prone to issues such as reward hacking or overfitting. In contrast, our approach begins with generating detailed critiques of synthesized images to extract reliable and actionable image editing instructions. By implementing these instructions, we create refined images, resulting in synthetic, informative preference pairs that serve as enhanced tuning datasets. We demonstrate the effectiveness of our pipeline and the resulting datasets in fine-tuning state-of-the-art diffusion models. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†å¯Œåå¥½ä¼˜åŒ–ï¼ˆRPOï¼‰è¿™ä¸€æ–°å‹æµç¨‹ï¼Œå®ƒåˆ©ç”¨ä¸°å¯Œçš„åé¦ˆä¿¡å·æ¥æ”¹å–„åå¥½å¯¹çš„ç­›é€‰ï¼Œä»¥å¾®è°ƒæ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ã€‚ä¼ ç»Ÿæ–¹æ³•ï¼Œå¦‚Diffusion-DPOï¼Œé€šå¸¸ä»…ä¾èµ–å¥–åŠ±æ¨¡å‹æ ‡ç­¾ï¼Œè¿™å¯èƒ½é€ æˆä¸é€æ˜ã€å¯¹åå¥½èƒŒåçš„ç†ç”±äº†è§£æœ‰é™ï¼Œå¹¶å®¹æ˜“å‡ºç°å¥–åŠ±ä½œå¼Šæˆ–è¿‡åº¦æ‹Ÿåˆç­‰é—®é¢˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆç”Ÿæˆåˆæˆå›¾åƒçš„è¯¦ç»†è¯„ä»·ï¼Œä»¥æå–å¯é ä¸”å¯æ“ä½œçš„å›¾åƒç¼–è¾‘æŒ‡ä»¤ã€‚é€šè¿‡æ‰§è¡Œè¿™äº›æŒ‡ä»¤ï¼Œæˆ‘ä»¬åˆ›å»ºæ›´ç²¾ç»†çš„å›¾åƒï¼Œä»è€Œäº§ç”Ÿåˆæˆã€ä¿¡æ¯ä¸°å¯Œçš„åå¥½å¯¹ï¼Œä½œä¸ºå¢å¼ºè°ƒæ•´æ•°æ®é›†ã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æµç¨‹å’Œç”Ÿæˆçš„æ•°æ®é›†åœ¨å¾®è°ƒæœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11720v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Rich Preference Optimizationï¼ˆRPOï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨ä¸°å¯Œçš„åé¦ˆä¿¡å·æ¥æ”¹å–„åå¥½å¯¹çš„ç­›é€‰ï¼Œä»¥ä¼˜åŒ–æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„å¦‚Diffusion-DPOç­‰æ–¹æ³•ç›¸æ¯”ï¼ŒRPOä¸å†ä»…ä¾èµ–å¥–åŠ±æ¨¡å‹æ ‡æ³¨ï¼Œè€Œæ˜¯é€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒçš„è¯¦ç»†è¯„ä»·æ¥æå–å¯é çš„å›¾åƒç¼–è¾‘æŒ‡ä»¤ã€‚è¿™äº›æŒ‡ä»¤çš„å®æ–½äº§ç”Ÿäº†ç²¾ç»†åŒ–å›¾åƒï¼Œè¿›è€Œå½¢æˆåˆæˆã€ä¿¡æ¯ä¸°å¯Œçš„åå¥½å¯¹ï¼Œä½œä¸ºå¢å¼ºè°ƒæ•´æ•°æ®é›†ã€‚æœ¬æ–‡å±•ç¤ºäº†è¯¥ç®¡é“åŠå…¶ç”Ÿæˆçš„æ•°æ®é›†åœ¨å¾®è°ƒæœ€å…ˆè¿›æ‰©æ•£æ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Rich Preference Optimization (RPO) åˆ©ç”¨ä¸°å¯Œçš„åé¦ˆä¿¡å·æ”¹å–„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„åå¥½å¯¹ç­›é€‰ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å¦‚Diffusion-DPOä¾èµ–å¥–åŠ±æ¨¡å‹æ ‡æ³¨ï¼Œå­˜åœ¨ä¸é€æ˜æ€§ã€å¯¹åå¥½èƒŒåç†æ®äº†è§£æœ‰é™åŠå¥–åŠ±é»‘å®¢æˆ–è¿‡åº¦æ‹Ÿåˆç­‰é—®é¢˜ã€‚</li>
<li>RPOé€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒçš„è¯¦ç»†è¯„ä»·æ¥æå–å¯é ä¸”å¯æ“ä½œçš„å›¾åƒç¼–è¾‘æŒ‡ä»¤ã€‚</li>
<li>è¿™äº›æŒ‡ä»¤çš„å®æ–½äº§ç”Ÿäº†ç²¾ç»†åŒ–å›¾åƒï¼Œå½¢æˆåˆæˆã€ä¿¡æ¯ä¸°å¯Œçš„åå¥½å¯¹ï¼Œä½œä¸ºå¢å¼ºè°ƒæ•´æ•°æ®é›†ã€‚</li>
<li>RPOæ–¹æ³•æé«˜äº†æ‰©æ•£æ¨¡å‹çš„å›¾åƒç”Ÿæˆè´¨é‡ã€‚</li>
<li>RPOæœ‰åŠ©äºæ›´å¥½åœ°ç†è§£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­çš„åå¥½å’Œå¼±ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11720">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-23258fdcc7fa63cd56d0fae082704fa0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e05bd050fbfefa9425ced3ec3907a528.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa84db95bf3158e76d3be4c130bd2510.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d68cd9b031870eabb55bb1d30e6da7ad.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GST-Precise-3D-Human-Body-from-a-Single-Image-with-Gaussian-Splatting-Transformers"><a href="#GST-Precise-3D-Human-Body-from-a-Single-Image-with-Gaussian-Splatting-Transformers" class="headerlink" title="GST: Precise 3D Human Body from a Single Image with Gaussian Splatting   Transformers"></a>GST: Precise 3D Human Body from a Single Image with Gaussian Splatting   Transformers</h2><p><strong>Authors:Lorenza Prospero, Abdullah Hamdi, Joao F. Henriques, Christian Rupprecht</strong></p>
<p>Reconstructing posed 3D human models from monocular images has important applications in the sports industry, including performance tracking, injury prevention and virtual training. In this work, we combine 3D human pose and shape estimation with 3D Gaussian Splatting (3DGS), a representation of the scene composed of a mixture of Gaussians. This allows training or fine-tuning a human model predictor on multi-view images alone, without 3D ground truth. Predicting such mixtures for a human from a single input image is challenging due to self-occlusions and dependence on articulations, while also needing to retain enough flexibility to accommodate a variety of clothes and poses. Our key observation is that the vertices of standardized human meshes (such as SMPL) can provide an adequate spatial density and approximate initial position for the Gaussians. We can then train a transformer model to jointly predict comparatively small adjustments to these positions, as well as the other 3DGS attributes and the SMPL parameters. We show empirically that this combination (using only multi-view supervision) can achieve near real-time inference of 3D human models from a single image without expensive diffusion models or 3D points supervision, thus making it ideal for the sport industry at any level. More importantly, rendering is an effective auxiliary objective to refine 3D pose estimation by accounting for clothes and other geometric variations. The code is available at <a target="_blank" rel="noopener" href="https://github.com/prosperolo/GST">https://github.com/prosperolo/GST</a>. </p>
<blockquote>
<p>ä»å•ç›®å›¾åƒé‡å»ºä¸‰ç»´äººä½“æ¨¡å‹åœ¨ä½“è‚²äº§ä¸šä¸­æœ‰é‡è¦åº”ç”¨ï¼ŒåŒ…æ‹¬æ€§èƒ½è·Ÿè¸ªã€æŸä¼¤é¢„é˜²å’Œè™šæ‹Ÿè®­ç»ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†ä¸‰ç»´äººä½“å§¿æ€å’Œå½¢çŠ¶ä¼°è®¡ä¸ä¸‰ç»´é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰ç›¸ç»“åˆï¼Œåè€…æ˜¯åœºæ™¯çš„ä¸€ç§è¡¨ç¤ºæ–¹æ³•ï¼Œç”±é«˜æ–¯æ··åˆç»„æˆã€‚è¿™å…è®¸ä»…é€šè¿‡å¤šè§†è§’å›¾åƒè®­ç»ƒæˆ–å¾®è°ƒäººä½“æ¨¡å‹é¢„æµ‹å™¨ï¼Œæ— éœ€ä¸‰ç»´çœŸå®æ•°æ®ã€‚ä»å•ä¸ªè¾“å…¥å›¾åƒé¢„æµ‹äººä½“çš„è¿™ç§æ··åˆæ˜¯å……æ»¡æŒ‘æˆ˜çš„ï¼Œå› ä¸ºå­˜åœ¨è‡ªé®æŒ¡é—®é¢˜å¹¶ä¸”ä¾èµ–äºå…³èŠ‚æ´»åŠ¨ï¼ŒåŒæ—¶è¿˜éœ€è¦ä¿ç•™è¶³å¤Ÿçš„çµæ´»æ€§ä»¥é€‚åº”å„ç§æœè£…å’Œå§¿åŠ¿ã€‚æˆ‘ä»¬çš„å…³é”®è§‚å¯Ÿæ˜¯æ ‡å‡†åŒ–äººä½“ç½‘æ ¼ï¼ˆå¦‚SMPLï¼‰çš„é¡¶ç‚¹å¯ä»¥æä¾›è¶³å¤Ÿçš„ç©ºé—´å¯†åº¦å’Œè¿‘ä¼¼åˆå§‹ä½ç½®æ¥å®šä¹‰é«˜æ–¯ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥è®­ç»ƒä¸€ä¸ªtransformeræ¨¡å‹æ¥å…±åŒé¢„æµ‹è¿™äº›ä½ç½®çš„å¾®å°è°ƒæ•´ï¼Œä»¥åŠå…¶ä»–3DGSå±æ€§å’ŒSMPLå‚æ•°ã€‚æˆ‘ä»¬ä»å®è¯ç ”ç©¶ä¸­å‘ç°ï¼Œè¿™ç§ç»“åˆï¼ˆä»…ä½¿ç”¨å¤šè§†è§’ç›‘ç£ï¼‰å¯ä»¥è¾¾åˆ°è¿‘ä¹å®æ—¶çš„æ•ˆæœï¼Œä»å•ä¸ªå›¾åƒæ¨æ–­å‡ºä¸‰ç»´äººä½“æ¨¡å‹ï¼Œæ— éœ€æ˜‚è´µçš„æ‰©æ•£æ¨¡å‹æˆ–ä¸‰ç»´ç‚¹ç›‘ç£ï¼Œå› æ­¤éå¸¸é€‚åˆå„çº§ä½“è‚²äº§ä¸šã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæ¸²æŸ“æ˜¯å®Œå–„ä¸‰ç»´å§¿æ€ä¼°è®¡çš„æœ‰æ•ˆè¾…åŠ©ç›®æ ‡ï¼Œå®ƒè€ƒè™‘äº†æœè£…å’Œå…¶ä»–å‡ ä½•å˜åŒ–ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/prosperolo/GST">https://github.com/prosperolo/GST</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.04196v2">PDF</a> Camera ready for CVSports workshop at CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç»“åˆ3Däººä½“å§¿æ€ä¸å½¢çŠ¶ä¼°è®¡ä»¥åŠ3Dé«˜æ–¯æ··åˆè¡¨ç¤ºæŠ€æœ¯ï¼Œé€šè¿‡å•ç›®å›¾åƒé‡å»ºäººä½“æ¨¡å‹çš„æ–¹æ³•ã€‚é€šè¿‡å¤šè§†è§’å›¾åƒè¿›è¡Œæ¨¡å‹è®­ç»ƒæˆ–ç²¾ç»†è°ƒæ•´ï¼Œæ— éœ€çœŸå®çš„3Dåœºæ™¯ã€‚æ–‡ä¸­è§£å†³äº†ç”±å•å¼ å›¾åƒé¢„æµ‹äººä½“æ··åˆé«˜æ–¯æ¨¡å‹æ—¶çš„é®æŒ¡ä¸å…³èŠ‚è¿åŠ¨ç­‰é—®é¢˜ï¼Œé€šè¿‡æ ‡å‡†äººä½“ç½‘æ ¼é¡¶ç‚¹ä¸ºé«˜æ–¯æ¨¡å‹æä¾›åˆå§‹ä½ç½®å’Œç©ºé—´å¯†åº¦ï¼Œå¹¶ä½¿ç”¨è½¬æ¢å™¨æ¨¡å‹é¢„æµ‹ä½ç½®å¾®è°ƒå’Œå…¶ä»–å±æ€§å‚æ•°ã€‚è¯¥ç ”ç©¶ä¸ä½¿ç”¨æ˜‚è´µçš„æ‰©æ•£æ¨¡å‹æˆ–å¤æ‚çš„3Dç‚¹ç›‘ç£æŠ€æœ¯å³å¯å®ç°è¿‘ä¹å®æ—¶çš„å›¾åƒåˆ°æ¨¡å‹æ¨æ–­ï¼Œéå¸¸é€‚åˆè¿åŠ¨äº§ä¸šã€‚æ¸²æŸ“æ˜¯å®Œå–„å§¿æ€ä¼°è®¡çš„æœ‰æ•ˆè¾…åŠ©æ‰‹æ®µï¼Œè€ƒè™‘äº†è¡£ç‰©å’Œå‡ ä½•å˜åŒ–å› ç´ ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç»“åˆäº†3Däººä½“å§¿æ€ä¸å½¢çŠ¶ä¼°è®¡ä»¥åŠ3Dé«˜æ–¯æ··åˆè¡¨ç¤ºæŠ€æœ¯ï¼Œç”¨äºä»å•ç›®å›¾åƒé‡å»ºäººä½“æ¨¡å‹ã€‚</li>
<li>åˆ©ç”¨å¤šè§†è§’å›¾åƒè¿›è¡Œæ¨¡å‹è®­ç»ƒæˆ–ç²¾ç»†è°ƒæ•´ï¼Œæ— éœ€ä¾èµ–çœŸå®çš„3Dåœºæ™¯ã€‚</li>
<li>ä¸»è¦è§£å†³äº†é€šè¿‡å•å¼ å›¾åƒé¢„æµ‹æ··åˆé«˜æ–¯äººä½“æ¨¡å‹çš„é®æŒ¡ä¸å…³èŠ‚è¿åŠ¨éš¾é¢˜ã€‚</li>
<li>æ ‡å‡†äººä½“ç½‘æ ¼é¡¶ç‚¹æä¾›äº†é«˜æ–¯æ¨¡å‹çš„åˆå§‹ä½ç½®å’Œç©ºé—´å¯†åº¦ã€‚</li>
<li>åˆ©ç”¨è½¬æ¢å™¨æ¨¡å‹é¢„æµ‹é«˜æ–¯æ¨¡å‹çš„è°ƒæ•´ä½ç½®ä»¥åŠå…¶ä»–å‚æ•°ã€‚</li>
<li>ä¸ä½¿ç”¨å¤æ‚çš„æ‰©æ•£æ¨¡å‹å’Œé«˜çº§ç›‘ç£æŠ€æœ¯ï¼Œå³å¯å®ç°å¿«é€Ÿä»å›¾åƒæ¨æ–­åˆ°æ¨¡å‹çš„è¿‡ç¨‹ã€‚</li>
<li>è¯¥æ–¹æ³•é€‚åˆè¿åŠ¨äº§ä¸šåº”ç”¨ï¼Œå¦‚æ€§èƒ½è¿½è¸ªã€ä¼¤å®³é¢„é˜²åŠè™šæ‹Ÿè®­ç»ƒç­‰ã€‚</li>
<li>æ¸²æŸ“æŠ€æœ¯ä½œä¸ºè¾…åŠ©æ‰‹æ®µï¼Œæœ‰åŠ©äºè€ƒè™‘è¡£ç‰©å’Œå‡ ä½•å˜åŒ–å› ç´ ï¼Œå®Œå–„å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.04196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0b168957dca4c706d6aaa8d7d8044112.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9d7cbde8f4e52111c2921e081bb1fce8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91503c9d231335d44c4f370bbec0e951.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e82e4f112f743598ad7ec9b6101a4b6a.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="StreamingT2V-Consistent-Dynamic-and-Extendable-Long-Video-Generation-from-Text"><a href="#StreamingT2V-Consistent-Dynamic-and-Extendable-Long-Video-Generation-from-Text" class="headerlink" title="StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation   from Text"></a>StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation   from Text</h2><p><strong>Authors:Roberto Henschel, Levon Khachatryan, Hayk Poghosyan, Daniil Hayrapetyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, Humphrey Shi</strong></p>
<p>Text-to-video diffusion models enable the generation of high-quality videos that follow text instructions, making it easy to create diverse and individual content. However, existing approaches mostly focus on high-quality short video generation (typically 16 or 24 frames), ending up with hard-cuts when naively extended to the case of long video synthesis. To overcome these limitations, we introduce StreamingT2V, an autoregressive approach for long video generation of 80, 240, 600, 1200 or more frames with smooth transitions. The key components are:(i) a short-term memory block called conditional attention module (CAM), which conditions the current generation on the features extracted from the previous chunk via an attentional mechanism, leading to consistent chunk transitions, (ii) a long-term memory block called appearance preservation module, which extracts high-level scene and object features from the first video chunk to prevent the model from forgetting the initial scene, and (iii) a randomized blending approach that enables to apply a video enhancer autoregressively for infinitely long videos without inconsistencies between chunks. Experiments show that StreamingT2V generates high motion amount. In contrast, all competing image-to-video methods are prone to video stagnation when applied naively in an autoregressive manner. Thus, we propose with StreamingT2V a high-quality seamless text-to-long video generator that outperforms competitors with consistency and motion. Our code will be available at: <a target="_blank" rel="noopener" href="https://github.com/Picsart-AI-Research/StreamingT2V">https://github.com/Picsart-AI-Research/StreamingT2V</a> </p>
<blockquote>
<p>æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æŒ‡ä»¤ç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ï¼Œä½¿å¾—åˆ›å»ºå¤šæ ·åŒ–å’Œä¸ªæ€§åŒ–å†…å®¹å˜å¾—è½»æ¾ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨é«˜è´¨é‡çŸ­è§†é¢‘ç”Ÿæˆï¼ˆé€šå¸¸ä¸º16æˆ–24å¸§ï¼‰ï¼Œåœ¨æ‰©å±•åˆ°é•¿è§†é¢‘åˆæˆæ—¶ä¼šå‡ºç°ç”Ÿç¡¬å‰ªè¾‘çš„æƒ…å†µã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†StreamingT2Vï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç”Ÿæˆ80ã€240ã€600ã€1200æˆ–æ›´å¤šå¸§çš„é•¿è§†é¢‘çš„è‡ªåŠ¨å›å½’æ–¹æ³•ï¼Œå…·æœ‰å¹³æ»‘è¿‡æ¸¡ã€‚å…³é”®ç»„ä»¶åŒ…æ‹¬ï¼š(i)ä¸€ä¸ªåä¸ºæ¡ä»¶æ³¨æ„æ¨¡å—ï¼ˆCAMï¼‰çš„çŸ­æœŸè®°å¿†å—ï¼Œå®ƒé€šè¿‡æ³¨æ„æœºåˆ¶æ ¹æ®ä»ä¸Šä¸€ä¸ªå—æå–çš„ç‰¹å¾æ¥æ¡ä»¶å½“å‰ç”Ÿæˆï¼Œä»è€Œå®ç°ä¸€è‡´çš„å—è¿‡æ¸¡ï¼›(ii)ä¸€ä¸ªåä¸ºå¤–è§‚ä¿ç•™æ¨¡å—çš„é•¿æœŸè®°å¿†å—ï¼Œå®ƒä»ç¬¬ä¸€ä¸ªè§†é¢‘å—ä¸­æå–é«˜çº§åœºæ™¯å’Œå¯¹è±¡ç‰¹å¾ï¼Œä»¥é˜²æ­¢æ¨¡å‹å¿˜è®°åˆå§‹åœºæ™¯ï¼›(iii)ä¸€ç§éšæœºæ··åˆæ–¹æ³•ï¼Œä½¿è§†é¢‘å¢å¼ºå™¨èƒ½å¤Ÿè‡ªåŠ¨åº”ç”¨äºæ— é™é•¿çš„è§†é¢‘ï¼Œè€Œä¸ä¼šåœ¨ä¸åŒå—ä¹‹é—´å‡ºç°ä¸ä¸€è‡´ã€‚å®éªŒè¡¨æ˜ï¼ŒStreamingT2Vèƒ½äº§ç”Ÿå¤§é‡çš„è¿åŠ¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ‰€æœ‰ç«äº‰æ€§çš„å›¾åƒåˆ°è§†é¢‘çš„æ–¹æ³•åœ¨é‡‡ç”¨è‡ªåŠ¨å›å½’æ–¹å¼æ—¶éƒ½å®¹æ˜“å‡ºç°è§†é¢‘åœæ»çš„æƒ…å†µã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºStreamingT2Vï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„æ— ç¼æ–‡æœ¬åˆ°é•¿è§†é¢‘ç”Ÿæˆå™¨ï¼Œåœ¨ä¸€è‡´æ€§å’Œè¿åŠ¨æ–¹é¢è¶…è¶Šç«äº‰å¯¹æ‰‹ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Picsart-AI-Research/StreamingT2V%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/Picsart-AI-Research/StreamingT2Vä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.14773v2">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/Picsart-AI-Research/StreamingT2V">https://github.com/Picsart-AI-Research/StreamingT2V</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡è§†é¢‘æ–¹é¢çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹é•¿è§†é¢‘ç”Ÿæˆçš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•StreamingT2Vï¼Œé€šè¿‡å¼•å…¥çŸ­æœŸè®°å¿†å—ï¼ˆæ¡ä»¶æ³¨æ„åŠ›æ¨¡å—ï¼‰å’Œé•¿æœŸè®°å¿†å—ï¼ˆå¤–è§‚ä¿ç•™æ¨¡å—ï¼‰ä»¥åŠéšæœºæ··åˆæŠ€æœ¯ï¼Œå®ç°äº†é«˜è´¨é‡çš„é•¿è§†é¢‘ç”Ÿæˆã€‚è¯¥æ–¹æ³•å…‹æœäº†ç°æœ‰æ–¹æ³•çš„ä¸è¶³ï¼Œè¡¨ç°å‡ºå‡ºè‰²çš„è¿è´¯æ€§å’Œè¿åŠ¨æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹èƒ½ç”Ÿæˆéµå¾ªæ–‡æœ¬æŒ‡ä»¤çš„é«˜è´¨é‡è§†é¢‘å†…å®¹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨çŸ­è§†é¢‘ç”Ÿæˆï¼Œä½†åœ¨é•¿è§†é¢‘åˆæˆæ—¶å­˜åœ¨ç¡¬åˆ‡å‰²é—®é¢˜ã€‚</li>
<li>StreamingT2Væ˜¯ä¸€ç§ç”¨äºé•¿è§†é¢‘ç”Ÿæˆçš„æ–¹æ³•ï¼Œå…·æœ‰80å¸§åŠä»¥ä¸Šçš„æµç•…è¿‡æ¸¡åŠŸèƒ½ã€‚</li>
<li>æ¡ä»¶æ³¨æ„åŠ›æ¨¡å—æ˜¯å®ç°ä¸€è‡´è¿‡æ¸¡çš„å…³é”®ç»„ä»¶ä¹‹ä¸€ã€‚å®ƒæ ¹æ®æ³¨æ„åŠ›æœºåˆ¶å°†å½“å‰ç”Ÿæˆä¸å…ˆå‰ç‰‡æ®µçš„ç‰¹å¾è”ç³»èµ·æ¥ã€‚</li>
<li>å¤–è§‚ä¿ç•™æ¨¡å—ä½œä¸ºé•¿æœŸè®°å¿†å—ï¼Œä»ç¬¬ä¸€ä¸ªè§†é¢‘ç‰‡æ®µä¸­æå–é«˜çº§åœºæ™¯å’Œå¯¹è±¡ç‰¹å¾ï¼Œé˜²æ­¢æ¨¡å‹å¿˜è®°åˆå§‹åœºæ™¯ã€‚</li>
<li>éšæœºæ··åˆæŠ€æœ¯ä½¿å¾—è§†é¢‘å¢å¼ºå™¨å¯ä»¥åº”ç”¨äºæ— é™é•¿çš„è§†é¢‘ï¼Œè€Œæ— éœ€åœ¨ç‰‡æ®µä¹‹é—´å‡ºç°ä¸ä¸€è‡´ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.14773">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7d15168a238ff43cf2d3be6f28b01945.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-463bda0d77a6fccb28dd9d23e03d73e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b6e0a24fc39333806c2e8a40d3380ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe82ef9548a9b1d9b5ff64f5751c55fa.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SignDiff-Diffusion-Model-for-American-Sign-Language-Production"><a href="#SignDiff-Diffusion-Model-for-American-Sign-Language-Production" class="headerlink" title="SignDiff: Diffusion Model for American Sign Language Production"></a>SignDiff: Diffusion Model for American Sign Language Production</h2><p><strong>Authors:Sen Fang, Chunyu Sui, Yanghao Zhou, Xuedong Zhang, Hongbin Zhong, Yapeng Tian, Chen Chen</strong></p>
<p>In this paper, we propose a dual-condition diffusion pre-training model named SignDiff that can generate human sign language speakers from a skeleton pose. SignDiff has a novel Frame Reinforcement Network called FR-Net, similar to dense human pose estimation work, which enhances the correspondence between text lexical symbols and sign language dense pose frames, reduces the occurrence of multiple fingers in the diffusion model. In addition, we propose a new method for American Sign Language Production (ASLP), which can generate ASL skeletal pose videos from text input, integrating two new improved modules and a new loss function to improve the accuracy and quality of sign language skeletal posture and enhance the ability of the model to train on large-scale data. We propose the first baseline for ASL production and report the scores of 17.19 and 12.85 on BLEU-4 on the How2Sign dev&#x2F;test sets. We evaluated our model on the previous mainstream dataset PHOENIX14T, and the experiments achieved the SOTA results. In addition, our image quality far exceeds all previous results by 10 percentage points in terms of SSIM. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSignDiffçš„åŒæ¡ä»¶æ‰©æ•£é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥ä»éª¨æ¶å§¿æ€ç”Ÿæˆäººç±»æ‰‹è¯­ã€‚SignDiffå…·æœ‰ä¸€ä¸ªåä¸ºFR-Netçš„æ–°å‹æ¡†æ¶å¼ºåŒ–ç½‘ç»œï¼Œç±»ä¼¼äºå¯†é›†äººä½“å§¿æ€ä¼°è®¡å·¥ä½œï¼Œå®ƒå¢å¼ºäº†æ–‡æœ¬è¯æ±‡ç¬¦å·ä¸æ‰‹è¯­å¯†é›†å§¿æ€æ¡†æ¶ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œå‡å°‘äº†æ‰©æ•£æ¨¡å‹ä¸­å¤šä¸ªæ‰‹æŒ‡çš„å‡ºç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç¾å›½æ‰‹è¯­ç”Ÿäº§ï¼ˆASLPï¼‰æ–¹æ³•ï¼Œå¯ä»¥ä»æ–‡æœ¬è¾“å…¥ç”ŸæˆASLéª¨æ¶å§¿æ€è§†é¢‘ï¼Œé›†æˆä¸¤ä¸ªæ–°æ”¹è¿›æ¨¡å—å’Œæ–°çš„æŸå¤±å‡½æ•°ï¼Œä»¥æé«˜æ‰‹è¯­éª¨æ¶å§¿æ€çš„å‡†ç¡®æ€§å’Œè´¨é‡ï¼Œå¹¶å¢å¼ºæ¨¡å‹åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šçš„è®­ç»ƒèƒ½åŠ›ã€‚æˆ‘ä»¬ä¸ºASLç”Ÿäº§æå‡ºäº†ç¬¬ä¸€ä¸ªåŸºå‡†çº¿ï¼Œå¹¶åœ¨How2Signçš„dev&#x2F;testé›†ä¸ŠæŠ¥å‘Šäº†BLEU-4å¾—åˆ†ä¸º17.19å’Œ12.85ã€‚æˆ‘ä»¬åœ¨ä¹‹å‰çš„ä¸»æµæ•°æ®é›†PHOENIX14Tä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå®éªŒè¾¾åˆ°äº†SOTAç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å›¾åƒè´¨é‡åœ¨SSIMæ–¹é¢è¶…è¿‡äº†ä»¥å‰æ‰€æœ‰ç»“æœï¼Œæé«˜äº†10ä¸ªç™¾åˆ†ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16082v3">PDF</a> Project Page at <a target="_blank" rel="noopener" href="https://signdiff.github.io/">https://signdiff.github.io</a></p>
<p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºäº†ä¸€ç§åä¸ºSignDiffçš„åŒæ¡ä»¶æ‰©æ•£é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥ä»éª¨æ¶å§¿æ€ç”Ÿæˆäººç±»æ‰‹åŠ¿è¯­è¨€ã€‚SignDiffæ‹¥æœ‰ä¸€ä¸ªæ–°é¢–çš„å¸§å¼ºåŒ–ç½‘ç»œFR-Netï¼Œç±»ä¼¼äºå¯†é›†çš„äººç±»å§¿æ€ä¼°è®¡å·¥ä½œï¼Œå®ƒå¯ä»¥å¢å¼ºæ–‡æœ¬è¯æ±‡ç¬¦å·ä¸æ‰‹åŠ¿è¯­è¨€å¯†é›†å§¿æ€å¸§ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œå‡å°‘æ‰©æ•£æ¨¡å‹ä¸­å¤šä¸ªæ‰‹æŒ‡çš„å‡ºç°ã€‚æ­¤å¤–ï¼Œæ–‡æœ¬è¿˜æå‡ºäº†ç¾å›½æ‰‹åŠ¿è¯­è¨€ç”Ÿäº§ï¼ˆASLPï¼‰çš„æ–°æ–¹æ³•ï¼Œå¯ä»¥ä»æ–‡æœ¬è¾“å…¥ç”ŸæˆASLéª¨æ¶å§¿æ€è§†é¢‘ï¼Œé›†æˆä¸¤ä¸ªæ–°æ”¹è¿›æ¨¡å—å’Œæ–°çš„æŸå¤±å‡½æ•°ï¼Œä»¥æé«˜æ‰‹åŠ¿è¯­è¨€éª¨æ¶å§¿æ€çš„å‡†ç¡®æ€§å’Œè´¨é‡ï¼Œå¹¶å¢å¼ºæ¨¡å‹åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šçš„è®­ç»ƒèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨How2Signå¼€å‘&#x2F;æµ‹è¯•é›†ä¸Šçš„BLEU-4åˆ†æ•°ä¸º17.19å’Œ12.85ï¼Œå¹¶åœ¨PHOENIX14Tä¸»æµæ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å›¾åƒè´¨é‡åœ¨SSIMæ–¹é¢è¶…è¿‡äº†ä¹‹å‰æ‰€æœ‰ç»“æœ10ä¸ªç™¾åˆ†ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SignDiffæ˜¯ä¸€ä¸ªåŒæ¡ä»¶æ‰©æ•£é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯ä»éª¨æ¶å§¿æ€ç”Ÿæˆäººç±»æ‰‹åŠ¿è¯­è¨€ã€‚</li>
<li>SignDiffå…·æœ‰FR-Netç½‘ç»œï¼Œå¢å¼ºäº†æ–‡æœ¬ä¸æ‰‹åŠ¿è¯­è¨€å§¿æ€ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚</li>
<li>æå‡ºäº†ç¾å›½æ‰‹åŠ¿è¯­è¨€ç”Ÿäº§ï¼ˆASLPï¼‰çš„æ–°æ–¹æ³•ï¼Œç»“åˆäº†ä¸¤ä¸ªæ”¹è¿›æ¨¡å—å’Œæ–°çš„æŸå¤±å‡½æ•°ã€‚</li>
<li>æ¨¡å‹åœ¨How2Signå¼€å‘&#x2F;æµ‹è¯•é›†ä¸Šçš„BLEU-4åˆ†æ•°è¾¾åˆ°æ–°çš„æ°´å¹³ã€‚</li>
<li>æ¨¡å‹åœ¨PHOENIX14Tæ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³ç»“æœã€‚</li>
<li>ä¸å…ˆå‰çš„ç»“æœç›¸æ¯”ï¼Œæ¨¡å‹åœ¨å›¾åƒè´¨é‡ä¸Šæœ‰äº†æ˜¾è‘—æé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨SSIMæ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2308.16082">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f98a8273e567443f16b7840f5cfd5c52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e14b2ec9edcebdf37631fa69c9b74dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84b78c46d413ca62b84dc0bef2467a05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-748dcf45ecaef95aaca763e16ac8899b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2e4f260423f573980d0a29134501998a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c23a45c62cdd55367ac1fb1f4ece25ef.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-18/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-18/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-18/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-316b6273e319ef222f2c5f5ca1700806.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-18  Cross-Frequency Collaborative Training Network and Dataset for   Semi-supervised First Molar Root Canal Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-18/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7c9b3281af8c3815692554eb31055595.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-18  R-Meshfusion Reinforcement Learning Powered Sparse-View Mesh   Reconstruction with Diffusion Priors
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24231k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
