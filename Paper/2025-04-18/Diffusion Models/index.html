<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-04-18  Cobra Efficient Line Art COlorization with BRoAder References">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-95c7a27077fce7ecac6289a12df9e5e0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    15.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    63 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-18-更新"><a href="#2025-04-18-更新" class="headerlink" title="2025-04-18 更新"></a>2025-04-18 更新</h1><h2 id="Cobra-Efficient-Line-Art-COlorization-with-BRoAder-References"><a href="#Cobra-Efficient-Line-Art-COlorization-with-BRoAder-References" class="headerlink" title="Cobra: Efficient Line Art COlorization with BRoAder References"></a>Cobra: Efficient Line Art COlorization with BRoAder References</h2><p><strong>Authors:Junhao Zhuang, Lingen Li, Xuan Ju, Zhaoyang Zhang, Chun Yuan, Ying Shan</strong></p>
<p>The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control. A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. We investigate the necessity of extensive contextual image guidance on the quality of line art colorization. To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency. Results demonstrate that Cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. We release our codes and models on our project page: <a target="_blank" rel="noopener" href="https://zhuang2002.github.io/Cobra/">https://zhuang2002.github.io/Cobra/</a>. </p>
<blockquote>
<p>漫画制作行业需要基于参考的高精度、高效率、上下文一致性和灵活控制的线条艺术色彩化。漫画页面通常涉及多种角色、物体和背景，这增加了色彩化的复杂性。尽管扩散模型在图像生成方面取得了进展，但它们在线条艺术色彩化方面的应用仍然有限，面临着处理大量参考图像、耗时推理和灵活控制等挑战。我们研究了大量上下文图像指导对线条艺术色彩化质量的必要性。为了解决这些挑战，我们引入了Cobra，这是一种高效且通用的方法，支持颜色提示，利用200多个参考图像，同时保持低延迟。Cobra的核心是一个因果稀疏DiT架构，它利用专门设计的位置编码、因果稀疏注意力和键值缓存来有效地管理长上下文引用并确保颜色身份一致性。结果表明，Cobra通过广泛的上下文参考实现了准确的线条艺术色彩化，大大提高了推理速度和交互性，从而满足了行业的关键需求。我们已在项目页面发布代码和模型：<a target="_blank" rel="noopener" href="https://zhuang2002.github.io/Cobra/">https://zhuang2002.github.io/Cobra/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12240v1">PDF</a> Project page with code: <a target="_blank" rel="noopener" href="https://zhuang2002.github.io/Cobra/">https://zhuang2002.github.io/Cobra/</a></p>
<p><strong>Summary</strong></p>
<p>这篇文本介绍了漫画制作行业对基于参考的线艺术着色的需求，强调高准确性、高效率、上下文一致性和灵活控制的重要性。作者调查了广泛上下文图像指导对线艺术着色质量的重要性。为了应对挑战，引入了一种高效且通用的方法Cobra，它支持颜色提示，同时使用超过200张参考图像，并保持低延迟。Cobra的核心是一种因果稀疏DiT架构，它利用专门设计的位置编码、因果稀疏注意力机制和键值缓存来有效地管理长上下文参考，并确保颜色一致性。研究结果证明，Cobra通过广泛的上下文参考实现了精确的线艺术着色，显著提高了推断速度和交互性，满足了行业关键需求。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>漫画制作行业需要高准确性、高效率、上下文一致性和灵活控制的参考基础线艺术着色。</li>
<li>广泛上下文图像指导对线艺术着色质量至关重要。</li>
<li>引入Cobra方法，支持颜色提示，使用超过200张参考图像，并保持低延迟。</li>
<li>Cobra核心是一种因果稀疏DiT架构，有效管理长上下文参考，确保颜色一致性。</li>
<li>Cobra实现了精确线艺术着色，通过广泛的上下文参考。</li>
<li>Cobra显著提高推断速度和交互性，满足行业关键需求。</li>
<li>作者在其项目页面上发布了代码和模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12240">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6b8586727c7244cec93e131ec6ecd80d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c8a94e3da7afbd16fa930b721c2340e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ad1999c1fe5e7d063ed7ba2f8d48344.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4ecafeb2822dd59197f74334ffba74c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-248bf5d45ce7874cb9b5f2f718f26266.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d02d313a1bbdd229d486ffac2a4d8c94.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bb1c4eb5ed7a72f549731ea59c5c457d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Anti-Aesthetics-Protecting-Facial-Privacy-against-Customized-Text-to-Image-Synthesis"><a href="#Anti-Aesthetics-Protecting-Facial-Privacy-against-Customized-Text-to-Image-Synthesis" class="headerlink" title="Anti-Aesthetics: Protecting Facial Privacy against Customized   Text-to-Image Synthesis"></a>Anti-Aesthetics: Protecting Facial Privacy against Customized   Text-to-Image Synthesis</h2><p><strong>Authors:Songping Wang, Yueming Lyu, Shiqi Liu, Ning Li, Tong Tong, Hao Sun, Caifeng Shan</strong></p>
<p>The rise of customized diffusion models has spurred a boom in personalized visual content creation, but also poses risks of malicious misuse, severely threatening personal privacy and copyright protection. Some studies show that the aesthetic properties of images are highly positively correlated with human perception of image quality. Inspired by this, we approach the problem from a novel and intriguing aesthetic perspective to degrade the generation quality of maliciously customized models, thereby achieving better protection of facial identity. Specifically, we propose a Hierarchical Anti-Aesthetic (HAA) framework to fully explore aesthetic cues, which consists of two key branches: 1) Global Anti-Aesthetics: By establishing a global anti-aesthetic reward mechanism and a global anti-aesthetic loss, it can degrade the overall aesthetics of the generated content; 2) Local Anti-Aesthetics: A local anti-aesthetic reward mechanism and a local anti-aesthetic loss are designed to guide adversarial perturbations to disrupt local facial identity. By seamlessly integrating both branches, our HAA effectively achieves the goal of anti-aesthetics from a global to a local level during customized generation. Extensive experiments show that HAA outperforms existing SOTA methods largely in identity removal, providing a powerful tool for protecting facial privacy and copyright. </p>
<blockquote>
<p>定制扩散模型的兴起促进了个性化视觉内容创作的繁荣，但同时也带来了恶意滥用的风险，严重威胁个人隐私和版权保护。一些研究表明，图像的美学属性与人类对图像质量的感知有着高度的正相关。受此启发，我们从一个新的、有趣的美学角度来解决这个问题，以降低恶意定制模型的生成质量，从而更好地保护面部身份。具体来说，我们提出了一个分层反美学（HAA）框架，以充分探索美学线索，该框架包括两个主要分支：1）全局反美学：通过建立全局反美学奖励机制和全局反美学损失，它可以降低生成内容的整体美学；2）局部反美学：设计局部反美学奖励机制和局部反美学损失，以引导对抗性扰动破坏局部面部身份。通过无缝集成这两个分支，我们的HAA有效地实现了从全局到局部的反美学目标。大量实验表明，HAA在身份删除方面大大优于现有最先进的方法，为保护面部隐私和版权提供了有力的工具。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12129v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>定制化扩散模型的兴起促进了个性化视觉内容的创作繁荣，但同时也存在恶意滥用的风险，严重威胁个人隐私和版权保护。本研究从新颖且有趣的美学角度入手，降低恶意定制化模型的生成质量，以实现更好的面部身份保护。我们提出了分层反美学（HAA）框架，该框架全面探讨了美学线索，包含两个关键分支：1）全局反美学：通过建立全局反美学奖励机制和全局反美学损失函数，降低生成内容的整体美学水平；2）局部反美学：设计局部反美学奖励机制和局部反美学损失函数，引导对抗性扰动破坏局部面部身份。通过无缝集成两个分支，HAA在定制生成过程中从全局到局部实现了有效的反美学目标。大量实验表明，HAA在身份去除方面大大优于现有最先进的方法，为面部隐私和版权保护提供了强大的工具。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>定制化扩散模型的普及促进了个性化视觉内容的创建，但伴随恶意滥用的风险，这对个人隐私和版权保护构成了严重威胁。</li>
<li>研究通过新颖的美学角度来解决恶意定制化模型的潜在威胁，旨在降低其生成质量以增强保护。</li>
<li>引入分层反美学（HAA）框架，该框架包含两个关键分支：全局反美学和局部反美学，以全面探索美学线索并应对挑战。</li>
<li>全局反美学分支通过建立相应的奖励机制和损失函数，着眼于降低生成内容的整体美学水平。</li>
<li>局部反美学分支专注于破坏局部面部身份，通过设计特定的奖励机制和损失函数来引导对抗性扰动。</li>
<li>HAA框架通过无缝集成上述两个分支，实现了从全局到局部的分层反美学效果。</li>
<li>实验证明，HAA在身份去除方面显著优于现有方法，为面部隐私和版权保护提供了有效工具。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12129">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f096359489037ae35bd135190185b633.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56481966ff42666d0a035955b89fa92b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e167c05bd6bda8adc0b77487fa7c8a4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1360d6e993e3bbb06947a5d34472069f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd3b72fbd467cedcd4d024bbce4d4625.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08adeb672fb64a9bb075311a5f7633bd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Diffusion-Based-Framework-for-Terrain-Aware-Remote-Sensing-Image-Reconstruction"><a href="#A-Diffusion-Based-Framework-for-Terrain-Aware-Remote-Sensing-Image-Reconstruction" class="headerlink" title="A Diffusion-Based Framework for Terrain-Aware Remote Sensing Image   Reconstruction"></a>A Diffusion-Based Framework for Terrain-Aware Remote Sensing Image   Reconstruction</h2><p><strong>Authors:Zhenyu Yu, Mohd Yamani Inda Idris, Pei Wang</strong></p>
<p>Remote sensing imagery is essential for environmental monitoring, agricultural management, and disaster response. However, data loss due to cloud cover, sensor failures, or incomplete acquisition-especially in high-resolution and high-frequency tasks-severely limits satellite imagery’s effectiveness. Traditional interpolation methods struggle with large missing areas and complex structures. Remote sensing imagery consists of multiple bands, each with distinct meanings, and ensuring consistency across bands is critical to avoid anomalies in the combined images. This paper proposes SatelliteMaker, a diffusion-based method that reconstructs missing data across varying levels of data loss while maintaining spatial, spectral, and temporal consistency. We also propose Digital Elevation Model (DEM) as a conditioning input and use tailored prompts to generate realistic images, making diffusion models applicable to quantitative remote sensing tasks. Additionally, we propose a VGG-Adapter module based on Distribution Loss, which reduces distribution discrepancy and ensures style consistency. Extensive experiments show that SatelliteMaker achieves state-of-the-art performance across multiple tasks. </p>
<blockquote>
<p>遥感影像在环境监测、农业管理和灾害应对方面发挥着重要作用。然而，由于云层覆盖、传感器故障或采集不完全等因素导致的数据丢失，特别是在高分辨率高频率的任务中，卫星影像的有效性受到了严重限制。传统插值方法在处理大面积缺失和复杂结构时遇到了困难。遥感影像由多个波段组成，每个波段都有独特的含义，确保各波段之间的一致性对于避免合成图像中的异常至关重要。本文针对卫星影像中数据缺失的问题，提出了一种基于扩散的SatelliteMaker方法，该方法能够在不同级别的数据丢失情况下重建缺失数据，同时保持空间、光谱和时间的一致性。我们还提出将数字高程模型（DEM）作为条件输入，并使用定制提示来生成逼真的图像，使扩散模型适用于定量遥感任务。此外，我们提出了一种基于分布损失的VGG适配器模块，该模块减少了分布差异，确保了风格一致性。大量实验表明，SatelliteMaker在多个任务上取得了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12112v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>这篇论文提出了一种基于扩散的遥感影像缺失数据重建方法——SatelliteMaker，该方法能够在不同级别的数据丢失情况下进行重建，同时保持空间、光谱和时间的一致性。论文还引入了数字高程模型（DEM）作为条件输入，并使用定制提示来生成真实图像，使扩散模型适用于定量遥感任务。通过广泛的实验验证，SatelliteMaker在多任务中实现了卓越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>遥感影像在环境监测、农业管理和灾害响应等领域具有关键作用，但数据缺失问题限制了其有效性。</li>
<li>传统插值方法在处理大面积缺失和复杂结构时面临挑战。</li>
<li>SatelliteMaker方法利用扩散模型重建遥感影像的缺失数据，适应不同级别的数据丢失情况。</li>
<li>引入数字高程模型（DEM）作为条件输入，提高影像生成的现实性和准确性。</li>
<li>定制提示用于生成真实图像，使扩散模型适用于定量遥感任务。</li>
<li>VGG-Adapter模块基于分布损失，减少分布差异，确保风格一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12112">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-424f45ad2d46dc139ad9acd54d735ebb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5ab5a347034d07053d2190c6d858cff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a7027487dc54a5e521f4eb0a290a5bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5f8b2c574f877835d9e5a9b65814719.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95c7a27077fce7ecac6289a12df9e5e0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Generalized-Visual-Relation-Detection-with-Diffusion-Models"><a href="#Generalized-Visual-Relation-Detection-with-Diffusion-Models" class="headerlink" title="Generalized Visual Relation Detection with Diffusion Models"></a>Generalized Visual Relation Detection with Diffusion Models</h2><p><strong>Authors:Kaifeng Gao, Siqi Chen, Hanwang Zhang, Jun Xiao, Yueting Zhuang, Qianru Sun</strong></p>
<p>Visual relation detection (VRD) aims to identify relationships (or interactions) between object pairs in an image. Although recent VRD models have achieved impressive performance, they are all restricted to pre-defined relation categories, while failing to consider the semantic ambiguity characteristic of visual relations. Unlike objects, the appearance of visual relations is always subtle and can be described by multiple predicate words from different perspectives, e.g., <code>ride&#39;&#39; can be depicted as </code>race’’ and &#96;&#96;sit on’’, from the sports and spatial position views, respectively. To this end, we propose to model visual relations as continuous embeddings, and design diffusion models to achieve generalized VRD in a conditional generative manner, termed Diff-VRD. We model the diffusion process in a latent space and generate all possible relations in the image as an embedding sequence. During the generation, the visual and text embeddings of subject-object pairs serve as conditional signals and are injected via cross-attention. After the generation, we design a subsequent matching stage to assign the relation words to subject-object pairs by considering their semantic similarities. Benefiting from the diffusion-based generative process, our Diff-VRD is able to generate visual relations beyond the pre-defined category labels of datasets. To properly evaluate this generalized VRD task, we introduce two evaluation metrics, i.e., text-to-image retrieval and SPICE PR Curve inspired by image captioning. Extensive experiments in both human-object interaction (HOI) detection and scene graph generation (SGG) benchmarks attest to the superiority and effectiveness of Diff-VRD. </p>
<blockquote>
<p>视觉关系检测（VRD）旨在识别图像中对象对之间的关系（或交互）。尽管最近的VRD模型已经取得了令人印象深刻的性能，但它们都局限于预先定义的关系类别，而忽略了视觉关系的语义模糊特性。与对象不同，视觉关系的表现总是很微妙，可以从不同的角度通过多个谓词来描述，例如，“骑”可以从运动和空间位置的角度分别描述为“比赛”和“坐在上面”。为此，我们提出将视觉关系建模为连续嵌入，并设计扩散模型以条件生成的方式实现广义VRD，称为Diff-VRD。我们在潜在空间中对扩散过程进行建模，并将图像中所有可能的关系生成嵌入序列。在生成过程中，主体-对象对的视觉和文本嵌入作为条件信号，通过交叉注意力进行注入。生成后，我们设计了一个后续的匹配阶段，通过考虑语义相似性将关系词分配给主体-对象对。得益于基于扩散的生成过程，我们的Diff-VRD能够生成超出数据集预定义类别标签的视觉关系。为了适当地评估这项广义VRD任务，我们引入了两种评估指标，即文本到图像的检索和受图像描述启发的SPICE PR曲线。在人类与物体交互（HOI）检测和场景图生成（SGG）基准测试的大量实验证明了Diff-VRD的优越性和有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12100v1">PDF</a> Under review at IEEE TCSVT. The Appendix is provided additionally</p>
<p><strong>摘要</strong><br>    视觉关系检测（VRD）旨在识别图像中对象对之间的关系（或交互）。虽然最近的VRD模型已经取得了令人印象深刻的性能，但它们都局限于预定义的关系类别，而忽略了视觉关系的语义模糊性。视觉关系的表现总是很微妙，可以从不同的角度用多个谓词词来描述。为此，我们提出将视觉关系建模为连续嵌入，并设计扩散模型以条件生成的方式实现通用VRD，称为Diff-VRD。我们在潜在空间中建模扩散过程，并将图像中的所有可能关系生成为嵌入序列。在生成过程中，主体-对象对的视觉和文本嵌入作为条件信号通过交叉注意力注入。生成后，我们设计了一个后续匹配阶段，通过考虑语义相似性将关系词分配给主体-对象对。得益于基于扩散的生成过程，我们的Diff-VRD能够生成超出数据集预定义类别标签的视觉关系。为了适当地评估这项通用VRD任务，我们引入了两种评估指标，即文本到图像的检索和受图像描述启发的SPICE PR曲线。在人类-物体交互（HOI）检测和场景图生成（SGG）基准测试中的大量实验证明了Diff-VRD的优越性和有效性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>视觉关系检测（VRD）的目标是识别图像中对象之间的关系或交互。</li>
<li>最近的VRD模型虽然性能出色，但都局限于预定义的关系类别，忽略了视觉关系的语义模糊性。</li>
<li>视觉关系的表现可以从多个角度用多个谓词词来描述，这些表现总是很微妙。</li>
<li>提出将视觉关系建模为连续嵌入，并采用扩散模型以条件生成的方式实现通用VRD（Diff-VRD）。</li>
<li>Diff-VRD在潜在空间中建模扩散过程，生成图像中的所有可能关系作为嵌入序列。</li>
<li>在生成过程中，利用主体-对象对的视觉和文本嵌入作为条件信号，通过交叉注意力机制实现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12100">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-944f8c2c0866a027b5321a6326d44fdc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60139b7e168ea9071ee9b2912be5b4d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f49bee99aafab6cf1c49a19644781074.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="R-Meshfusion-Reinforcement-Learning-Powered-Sparse-View-Mesh-Reconstruction-with-Diffusion-Priors"><a href="#R-Meshfusion-Reinforcement-Learning-Powered-Sparse-View-Mesh-Reconstruction-with-Diffusion-Priors" class="headerlink" title="R-Meshfusion: Reinforcement Learning Powered Sparse-View Mesh   Reconstruction with Diffusion Priors"></a>R-Meshfusion: Reinforcement Learning Powered Sparse-View Mesh   Reconstruction with Diffusion Priors</h2><p><strong>Authors:Haoyang Wang, Liming Liu, Peiheng Wang, Junlin Hao, Jiangkai Wu, Xinggong Zhang</strong></p>
<p>Mesh reconstruction from multi-view images is a fundamental problem in computer vision, but its performance degrades significantly under sparse-view conditions, especially in unseen regions where no ground-truth observations are available. While recent advances in diffusion models have demonstrated strong capabilities in synthesizing novel views from limited inputs, their outputs often suffer from visual artifacts and lack 3D consistency, posing challenges for reliable mesh optimization. In this paper, we propose a novel framework that leverages diffusion models to enhance sparse-view mesh reconstruction in a principled and reliable manner. To address the instability of diffusion outputs, we propose a Consensus Diffusion Module that filters unreliable generations via interquartile range (IQR) analysis and performs variance-aware image fusion to produce robust pseudo-supervision. Building on this, we design an online reinforcement learning strategy based on the Upper Confidence Bound (UCB) to adaptively select the most informative viewpoints for enhancement, guided by diffusion loss. Finally, the fused images are used to jointly supervise a NeRF-based model alongside sparse-view ground truth, ensuring consistency across both geometry and appearance. Extensive experiments demonstrate that our method achieves significant improvements in both geometric quality and rendering quality. </p>
<blockquote>
<p>基于多视角图像的网格重建是计算机视觉中的一个基本问题，但在稀疏视角条件下，其性能会显著下降，特别是在没有真实观测数据的未知区域。尽管扩散模型的最新进展在有限的输入下显示出强大的合成新视角的能力，但其输出往往存在视觉伪影且缺乏3D一致性，这为可靠的网格优化带来了挑战。在本文中，我们提出了一种新型框架，该框架利用扩散模型以有原则的和可靠的方式增强稀疏视角网格重建。为了解决扩散输出的不稳定性，我们提出了共识扩散模块（Consensus Diffusion Module），它通过四分位距（IQR）分析过滤不可靠的生成，并执行方差感知图像融合以产生稳健的伪监督。在此基础上，我们设计了一种基于上置信界（UCB）的在线强化学习策略，以自适应地选择最具信息量的视角进行增强，由扩散损失引导。最后，融合图像被用来共同监督基于神经辐射场（NeRF）的模型和稀疏视角的真实数据，确保几何和外观的一致性。大量实验表明，我们的方法在几何质量和渲染质量上都实现了显著提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11946v1">PDF</a> </p>
<p><strong>摘要</strong><br>多视角图像重建是计算机视觉中的基本问题，但在稀疏视角下其性能会显著下降，特别是在未见区域，由于没有真实地面观测数据可用。尽管最近的扩散模型进展在仅从有限输入合成新视角方面表现出强大的能力，但其输出常常存在视觉伪影和缺乏三维一致性，给可靠的网格优化带来了挑战。本文提出了一种利用扩散模型以有原则和可靠的方式增强稀疏视角网格重建的新框架。为解决扩散输出的不稳定性问题，我们提出了共识扩散模块，它通过四分位距分析过滤不可靠的生成，并执行方差感知图像融合以产生稳健的伪监督。在此基础上，我们设计了一种基于上置信界（UCB）的在线强化学习策略，自适应选择最具信息量的视角进行增强，由扩散损失引导。最后，融合图像被用来联合监督基于NeRF的模型以及稀疏视角的真实数据，确保几何和外观的一致性。大量实验表明，我们的方法在几何质量和渲染质量上都实现了显著提高。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>稀疏视角下的多视角图像重建是计算机视觉的挑战。</li>
<li>扩散模型在合成新视角方面表现出强大的能力，但存在视觉伪影和三维一致性问题。</li>
<li>提出了一种新的框架，利用扩散模型增强稀疏视角的网格重建。</li>
<li>共识扩散模块通过四分位距分析过滤不可靠的生成，执行方差感知图像融合。</li>
<li>设计了基于上置信界（UCB）的在线强化学习策略，自适应选择信息丰富的视角进行增强。</li>
<li>融合图像联合监督NeRF模型和稀疏视角的真实数据，确保几何和外观的一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11946">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f1ca56d1df3669c2c16e58f3e05f01d2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7c9b3281af8c3815692554eb31055595.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f36b2e25bc72146b0a31c8991d297a0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed3bbc7dd81fe466cb6541a6fa0f3515.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e791deb219a6a57e189474deb180f730.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ACE-Attentional-Concept-Erasure-in-Diffusion-Models"><a href="#ACE-Attentional-Concept-Erasure-in-Diffusion-Models" class="headerlink" title="ACE: Attentional Concept Erasure in Diffusion Models"></a>ACE: Attentional Concept Erasure in Diffusion Models</h2><p><strong>Authors:Finn Carter</strong></p>
<p>Large text-to-image diffusion models have demonstrated remarkable image synthesis capabilities, but their indiscriminate training on Internet-scale data has led to learned concepts that enable harmful, copyrighted, or otherwise undesirable content generation. We address the task of concept erasure in diffusion models, i.e., removing a specified concept from a pre-trained model such that prompting the concept (or related synonyms) no longer yields its depiction, while preserving the model’s ability to generate other content. We propose a novel method, Attentional Concept Erasure (ACE), that integrates a closed-form attention manipulation with lightweight fine-tuning. Theoretically, we formulate concept erasure as aligning the model’s conditional distribution on the target concept with a neutral distribution. Our approach identifies and nullifies concept-specific latent directions in the cross-attention modules via a gated low-rank adaptation, followed by adversarially augmented fine-tuning to ensure thorough erasure of the concept and its synonyms. Empirically, we demonstrate on multiple benchmarks, including object classes, celebrity faces, explicit content, and artistic styles, that ACE achieves state-of-the-art concept removal efficacy and robustness. Compared to prior methods, ACE better balances generality (erasing concept and related terms) and specificity (preserving unrelated content), scales to dozens of concepts, and is efficient, requiring only a few seconds of adaptation per concept. We will release our code to facilitate safer deployment of diffusion models. </p>
<blockquote>
<p>大型文本到图像的扩散模型已经显示出令人印象深刻的图像合成能力，但它们在互联网规模数据上的随意训练导致了学到的概念能够生成有害、版权或其他不受欢迎的内容。我们解决了扩散模型中的概念消除任务，即从一个预训练模型中移除指定的概念，使得提示该概念（或相关同义词）不再产生其描述，同时保留模型生成其他内容的能力。我们提出了一种新的方法，称为注意力概念消除（ACE），它将形式化的注意力操纵与轻量级微调相结合。理论上，我们将概念消除制定为使模型的目标概念条件分布与中性分布对齐。我们的方法通过门控低秩适应来识别和消除交叉注意模块中的特定概念潜在方向，然后通过增强对抗微调来确保彻底消除该概念和它的同义词。在多个基准测试上，包括对象类别、名人面孔、明确内容和艺术风格等，我们证明ACE在概念去除效果和稳健性方面达到了最佳状态。与之前的方法相比，ACE在通用性（消除概念和相关术语）和特异性（保留不相关内容）之间取得了更好的平衡，能够扩展到数十个概念，并且效率很高，每个概念只需要几秒钟的适应时间。我们将发布我们的代码，以促进扩散模型的安全部署。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11850v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>大型文本到图像的扩散模型具有令人瞩目的图像合成能力，但其对互联网规模数据的散漫训练导致模型能够生成有害、版权或不受欢迎的内容。本文解决扩散模型中的概念消除任务，即从一个预训练模型中移除指定的概念，使得提示概念（或其同义词）不再生成描述，同时保留模型生成其他内容的能力。本文提出了一种新的方法——注意力概念消除（ACE），它将形式化的注意力操作与轻量级微调相结合。理论上，我们将概念消除制定为将模型对目标概念的条件分布与中性分布对齐。我们的方法通过门控低秩适应识别并消除交叉注意模块中的特定概念潜在方向，然后进行增强对抗微调，以确保彻底消除概念及其同义词。实证表明，在多个基准测试上，包括对象类别、名人面孔、明确内容和艺术风格等，ACE实现了最先进的概念去除效果和稳健性。与以前的方法相比，ACE在通用性（消除概念和相关术语）和特异性（保留无关内容）之间取得了更好的平衡，能够扩展到数十个概念，并且效率很高，每个概念只需要几秒钟的适应时间。我们将发布我们的代码，以促进扩散模型的安全部署。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型扩散模型虽然具备出色的图像合成能力，但存在生成有害、版权或不受欢迎内容的风险。</li>
<li>解决扩散模型中的概念消除任务变得重要，旨在移除预训练模型中指定的概念，同时保留其生成其他内容的能力。</li>
<li>提出了一种新的方法——注意力概念消除（ACE），通过整合注意力操作和轻量级微调来实现概念消除。</li>
<li>ACE将概念消除理论化，将模型的条件分布与中性分布对齐，以消除特定概念。</li>
<li>ACE具有强大的实证表现，在多个基准测试中实现了先进的概念去除效果和稳健性。</li>
<li>ACE在通用性和特异性之间取得了平衡，能够扩展到多个概念，并且具有高效的适应时间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11850">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-072b9d54e5e8c11925df7c564952b346.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TextDiffSeg-Text-guided-Latent-Diffusion-Model-for-3d-Medical-Images-Segmentation"><a href="#TextDiffSeg-Text-guided-Latent-Diffusion-Model-for-3d-Medical-Images-Segmentation" class="headerlink" title="TextDiffSeg: Text-guided Latent Diffusion Model for 3d Medical Images   Segmentation"></a>TextDiffSeg: Text-guided Latent Diffusion Model for 3d Medical Images   Segmentation</h2><p><strong>Authors:Kangbo Ma</strong></p>
<p>Diffusion Probabilistic Models (DPMs) have demonstrated significant potential in 3D medical image segmentation tasks. However, their high computational cost and inability to fully capture global 3D contextual information limit their practical applications. To address these challenges, we propose a novel text-guided diffusion model framework, TextDiffSeg. This method leverages a conditional diffusion framework that integrates 3D volumetric data with natural language descriptions, enabling cross-modal embedding and establishing a shared semantic space between visual and textual modalities. By enhancing the model’s ability to recognize complex anatomical structures, TextDiffSeg incorporates innovative label embedding techniques and cross-modal attention mechanisms, effectively reducing computational complexity while preserving global 3D contextual integrity. Experimental results demonstrate that TextDiffSeg consistently outperforms existing methods in segmentation tasks involving kidney and pancreas tumors, as well as multi-organ segmentation scenarios. Ablation studies further validate the effectiveness of key components, highlighting the synergistic interaction between text fusion, image feature extractor, and label encoder. TextDiffSeg provides an efficient and accurate solution for 3D medical image segmentation, showcasing its broad applicability in clinical diagnosis and treatment planning. </p>
<blockquote>
<p>扩散概率模型（DPM）在3D医学图像分割任务中表现出了巨大的潜力。然而，其较高的计算成本以及无法完全捕获全局3D上下文信息，限制了其实际应用。为了解决这些挑战，我们提出了一种新型文本引导扩散模型框架，名为TextDiffSeg。该方法利用条件扩散框架，将3D体积数据与自然语言描述相结合，实现跨模态嵌入，并在视觉和文本模态之间建立共享语义空间。通过增强模型识别复杂解剖结构的能力，TextDiffSeg结合了创新的标签嵌入技术和跨模态注意力机制，在降低计算复杂性的同时，保持了全局3D上下文的完整性。实验结果表明，TextDiffSeg在涉及肾脏和胰腺肿瘤的分割任务以及多器官分割场景中，均优于现有方法。消融研究进一步验证了关键组件的有效性，突出了文本融合、图像特征提取器和标签编码器之间的协同交互。TextDiffSeg为3D医学图像分割提供了高效且准确的解决方案，展示了其在临床诊断和治疗计划中的广泛应用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11825v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>扩散概率模型（DPMs）在3D医学图像分割任务中显示出巨大潜力，但其高计算成本和无法完全捕获全局3D上下文信息限制了其实际应用。为应对这些挑战，我们提出了全新的文本引导扩散模型框架TextDiffSeg。此方法利用条件扩散框架，整合3D体积数据与自然语言描述，实现跨模态嵌入，建立视觉与文本模态之间的共享语义空间。通过增强模型识别复杂解剖结构的能力，TextDiffSeg融入创新的标签嵌入技术和跨模态注意力机制，在降低计算复杂性的同时保持全局3D上下文完整性。实验结果显示，TextDiffSeg在肾脏、胰腺肿瘤分割任务以及多器官分割场景中均表现出超越现有方法的性能。消融研究进一步验证了关键组件的有效性，突显了文本融合、图像特征提取器和标签编码器之间的协同作用。TextDiffSeg为3D医学图像分割提供了高效准确的解决方案，展示了其在临床诊断和治疗计划中的广泛应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散概率模型（DPMs）在3D医学图像分割中表现出潜力，但存在高计算成本和捕获信息不全的问题。</li>
<li>新型文本引导扩散模型框架TextDiffSeg被提出，整合3D数据与自然语言描述，建立视觉和文本模态的共享语义空间。</li>
<li>TextDiffSeg通过创新技术增强模型识别复杂解剖结构的能力，降低计算复杂性的同时保持上下文完整性。</li>
<li>TextDiffSeg在医学图像分割任务中性能超越现有方法，特别是在肾脏、胰腺肿瘤及多器官分割场景中。</li>
<li>消融研究证实了TextDiffSeg的关键组件，包括文本融合、图像特征提取器和标签编码器的有效性。</li>
<li>TextDiffSeg提供了高效的3D医学图像分割解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11825">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-37b3fe4bca2d564dcf6d056355a8575a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-976c87d19980198279c305d18f977e56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4b723e9448318669ce67ea6d3f3f30a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b7623f857d2a0eef24db11ebf73833ea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f61c79d602c8dbdeb1e4e9d5ed42fc13.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Towards-Safe-Synthetic-Image-Generation-On-the-Web-A-Multimodal-Robust-NSFW-Defense-and-Million-Scale-Dataset"><a href="#Towards-Safe-Synthetic-Image-Generation-On-the-Web-A-Multimodal-Robust-NSFW-Defense-and-Million-Scale-Dataset" class="headerlink" title="Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust   NSFW Defense and Million Scale Dataset"></a>Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust   NSFW Defense and Million Scale Dataset</h2><p><strong>Authors:Muhammad Shahid Muneer, Simon S. Woo</strong></p>
<p>In the past years, we have witnessed the remarkable success of Text-to-Image (T2I) models and their widespread use on the web. Extensive research in making T2I models produce hyper-realistic images has led to new concerns, such as generating Not-Safe-For-Work (NSFW) web content and polluting the web society. To help prevent misuse of T2I models and create a safer web environment for users features like NSFW filters and post-hoc security checks are used in these models. However, recent work unveiled how these methods can easily fail to prevent misuse. In particular, adversarial attacks on text and image modalities can easily outplay defensive measures. %Exploiting such leads to the growing concern of preventing adversarial attacks on text and image modalities. Moreover, there is currently no robust multimodal NSFW dataset that includes both prompt and image pairs and adversarial examples. This work proposes a million-scale prompt and image dataset generated using open-source diffusion models. Second, we develop a multimodal defense to distinguish safe and NSFW text and images, which is robust against adversarial attacks and directly alleviates current challenges. Our extensive experiments show that our model performs well against existing SOTA NSFW detection methods in terms of accuracy and recall, drastically reducing the Attack Success Rate (ASR) in multimodal adversarial attack scenarios. Code: <a target="_blank" rel="noopener" href="https://github.com/shahidmuneer/multimodal-nsfw-defense">https://github.com/shahidmuneer/multimodal-nsfw-defense</a>. </p>
<blockquote>
<p>过去几年，我们见证了文本到图像（T2I）模型的巨大成功及其在网上的广泛应用。关于如何使T2I模型生成超逼真图像的广泛研究引发了一些新的担忧，例如生成不适合工作场合（NSFW）的网页内容以及污染网络社会。为了防止滥用T2I模型并为用户创建更安全的网络环境，这些模型中采用了不适合工作场合（NSFW）的过滤器和事后安全检查等功能。然而，最近的研究表明，这些方法很容易无法防止滥用。特别是文本和图像模态的对抗性攻击可以轻松战胜防御措施。因此，人们越来越担心防止文本和图像模态的对抗性攻击的问题。此外，当前尚没有一个包含提示、图像对和对抗实例的稳健的多模态NSFW数据集。这项工作提出了一个使用开源扩散模型生成的大规模提示和图像数据集。其次，我们开发了一种多模态防御方法，用于区分安全和不适合工作场合的文本和图像，该方法对对抗性攻击具有强大的防御能力，并直接缓解了当前面临的挑战。我们的广泛实验表明，我们的模型在准确率和召回率方面表现良好，与现有的最佳NSFW检测方法相比，在多模态对抗性攻击场景中大大降低了攻击成功率（ASR）。代码：<a target="_blank" rel="noopener" href="https://github.com/shahidmuneer/multimodal-nsfw-defense%E3%80%82">https://github.com/shahidmuneer/multimodal-nsfw-defense。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11707v1">PDF</a> Short Paper The Web Conference</p>
<p><strong>Summary</strong></p>
<p>文本介绍了文本到图像（T2I）模型的广泛应用及其生成不安全的网络内容（NSFW）的担忧。为防止滥用和创建更安全的网络环境，采用了NSFW过滤器、事后安全检查等功能。然而，新方法容易被攻击者利用，对抗性攻击文本和图像模式可以轻松绕过防御措施。本文提出一个大规模提示和图像数据集，并使用开源扩散模型生成，同时开发了一种多模式防御来区分安全和NSFW的文本和图像，对抗攻击表现良好。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2I模型生成高清晰度图像取得显著成功，但存在生成NSFW内容的担忧。</li>
<li>NSFW过滤器及事后安全检查是防止滥用T2I模型的手段，但存在失效风险。</li>
<li>对抗性攻击可以轻易绕过现有防御措施。</li>
<li>缺乏包含提示和图像对以及对抗性实例的多模式NSFW数据集。</li>
<li>本文提出一个大规模提示和图像数据集，使用开源扩散模型生成。</li>
<li>开发了一种多模式防御，可区分安全和NSFW的文本和图像。</li>
<li>该防御在准确率和召回率方面表现良好，大幅降低多模式对抗攻击场景中的攻击成功率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11707">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-19b6f74135ce6fe01f4b73eb8a16eeed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6cdb67649f69509be4258bb04b4bb7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2aebfc915f3f4198bbaa84415a2f6c41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0931f2de68f268c0b559298c75206f9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-042e9701b85b2f271f7a51f786762a5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d818d1c05378013ce096560f6d3fb13c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbbfb5f7faa603c406acb9592d2afe68.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DyDiT-Dynamic-Diffusion-Transformers-for-Efficient-Visual-Generation"><a href="#DyDiT-Dynamic-Diffusion-Transformers-for-Efficient-Visual-Generation" class="headerlink" title="DyDiT++: Dynamic Diffusion Transformers for Efficient Visual Generation"></a>DyDiT++: Dynamic Diffusion Transformers for Efficient Visual Generation</h2><p><strong>Authors:Wangbo Zhao, Yizeng Han, Jiasheng Tang, Kai Wang, Hao Luo, Yibing Song, Gao Huang, Fan Wang, Yang You</strong></p>
<p>Diffusion Transformer (DiT), an emerging diffusion model for visual generation, has demonstrated superior performance but suffers from substantial computational costs. Our investigations reveal that these costs primarily stem from the \emph{static} inference paradigm, which inevitably introduces redundant computation in certain \emph{diffusion timesteps} and \emph{spatial regions}. To overcome this inefficiency, we propose \textbf{Dy}namic \textbf{Di}ffusion \textbf{T}ransformer (DyDiT), an architecture that \emph{dynamically} adjusts its computation along both \emph{timestep} and \emph{spatial} dimensions. Specifically, we introduce a \emph{Timestep-wise Dynamic Width} (TDW) approach that adapts model width conditioned on the generation timesteps. In addition, we design a \emph{Spatial-wise Dynamic Token} (SDT) strategy to avoid redundant computation at unnecessary spatial locations. TDW and SDT can be seamlessly integrated into DiT and significantly accelerates the generation process. Building on these designs, we further enhance DyDiT in three key aspects. First, DyDiT is integrated seamlessly with flow matching-based generation, enhancing its versatility. Furthermore, we enhance DyDiT to tackle more complex visual generation tasks, including video generation and text-to-image generation, thereby broadening its real-world applications. Finally, to address the high cost of full fine-tuning and democratize technology access, we investigate the feasibility of training DyDiT in a parameter-efficient manner and introduce timestep-based dynamic LoRA (TD-LoRA). Extensive experiments on diverse visual generation models, including DiT, SiT, Latte, and FLUX, demonstrate the effectiveness of DyDiT. </p>
<blockquote>
<p>扩散Transformer（DiT）是一种新兴的视觉生成扩散模型，它表现出卓越的性能，但计算成本较高。我们的调查表明，这些成本主要源于\emph{静态}推理范式，这种范式不可避免地会在某些\emph{扩散时间步}和\emph{空间区域}中引入冗余计算。为了克服这种低效，我们提出了\textbf{Dy}namic \textbf{Di}ffusion \textbf{T}ransformer（DyDiT），这是一种\emph{动态}调整其在\emph{时间步}和\emph{空间}维度上计算的结构。具体来说，我们引入了一种\emph{时间步动态宽度}（TDW）方法，根据生成时间步来适应模型宽度。此外，我们设计了一种\emph{空间动态令牌}（SDT）策略，以避免在不必要的空间位置进行冗余计算。TDW和SDT可以无缝集成到DiT中，并显著加速生成过程。基于这些设计，我们从三个方面进一步增强了DyDiT。首先，DyDiT可以与基于流匹配的生成方法无缝集成，提高其通用性。此外，我们增强了DyDiT以处理更复杂的视觉生成任务，包括视频生成和文本到图像生成，从而扩大了其在现实世界中的应用。最后，为了解决全精细调整的高成本并实现技术的普及，我们研究了以参数效率高的方式训练DyDiT的可行性，并引入了基于时间步的动态LoRA（TD-LoRA）。在包括DiT、SiT、Latte和FLUX等多种视觉生成模型上的广泛实验证明了DyDiT的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06803v2">PDF</a> Extended journal version for ICLR. arXiv admin note: substantial text   overlap with arXiv:2410.03456</p>
<p><strong>Summary</strong><br>     扩散模型的前沿技术Diffusion Transformer（DiT）虽然性能卓越，但计算成本较高。研究团队发现主要原因是静态推理模式导致的冗余计算。为此，他们提出了动态扩散模型DyDiT，能够根据生成步骤和时间步长动态调整计算宽度和空间区域的计算，以降低成本并提高效率。团队设计了Time-step wise Dynamic Width（TDW）和Spatial-wise Dynamic Token（SDT）策略。进一步融入流匹配生成增强泛化性、视频生成、文本生成等功能，并探索了参数高效的训练方法TD-LoRA。实验证明DyDiT的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion Transformer（DiT）具有出色的性能但面临较高的计算成本问题。</li>
<li>主要成本源于静态推理模式在特定扩散时间步和空间区域的冗余计算。</li>
<li>DyDiT被提出以动态调整计算宽度和时间步长，避免冗余计算。</li>
<li>DyDiT通过引入Timestep-wise Dynamic Width（TDW）和Spatial-wise Dynamic Token（SDT）策略实现优化。</li>
<li>DyDiT支持多种生成任务，包括视频生成和文本生成等，增强了其实际应用能力。</li>
<li>DyDiT通过融入流匹配生成技术增强了其泛化性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06803">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9b0eadcc96cb77fba5e57e5b0e8171be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-955078ccadd86f2db94ae6051465bcc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e98597211d63e89b31262982a4906b04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df688688a767719ae1963e6731b90bb7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="OpenSDI-Spotting-Diffusion-Generated-Images-in-the-Open-World"><a href="#OpenSDI-Spotting-Diffusion-Generated-Images-in-the-Open-World" class="headerlink" title="OpenSDI: Spotting Diffusion-Generated Images in the Open World"></a>OpenSDI: Spotting Diffusion-Generated Images in the Open World</h2><p><strong>Authors:Yabin Wang, Zhiwu Huang, Xiaopeng Hong</strong></p>
<p>This paper identifies OpenSDI, a challenge for spotting diffusion-generated images in open-world settings. In response to this challenge, we define a new benchmark, the OpenSDI dataset (OpenSDID), which stands out from existing datasets due to its diverse use of large vision-language models that simulate open-world diffusion-based manipulations. Another outstanding feature of OpenSDID is its inclusion of both detection and localization tasks for images manipulated globally and locally by diffusion models. To address the OpenSDI challenge, we propose a Synergizing Pretrained Models (SPM) scheme to build up a mixture of foundation models. This approach exploits a collaboration mechanism with multiple pretrained foundation models to enhance generalization in the OpenSDI context, moving beyond traditional training by synergizing multiple pretrained models through prompting and attending strategies. Building on this scheme, we introduce MaskCLIP, an SPM-based model that aligns Contrastive Language-Image Pre-Training (CLIP) with Masked Autoencoder (MAE). Extensive evaluations on OpenSDID show that MaskCLIP significantly outperforms current state-of-the-art methods for the OpenSDI challenge, achieving remarkable relative improvements of 14.23% in IoU (14.11% in F1) and 2.05% in accuracy (2.38% in F1) compared to the second-best model in localization and detection tasks, respectively. Our dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/iamwangyabin/OpenSDI">https://github.com/iamwangyabin/OpenSDI</a>. </p>
<blockquote>
<p>本文提出了OpenSDI挑战，即在开放世界环境中识别扩散生成图像的挑战。为了应对这一挑战，我们定义了一个新的基准测试，即OpenSDI数据集（OpenSDID）。由于它使用了模拟开放世界扩散操作的多种大型视觉语言模型，因此与其他现有数据集有所不同。OpenSDID的另一个突出特点是它包含了用于全局和局部扩散模型操作图像的检测和定位任务。为了应对OpenSDI挑战，我们提出了一种协同预训练模型（SPM）方案，用于构建混合基础模型。这种方法通过协作机制利用多个预训练基础模型，以增强在OpenSDI环境中的泛化能力，通过提示和注意力策略协同多个预训练模型，超越传统训练。基于这一方案，我们引入了MaskCLIP，这是一个基于SPM的模型，它将对比语言图像预训练（CLIP）与掩码自动编码器（MAE）相结合。在OpenSDID上的广泛评估表明，与当前最好的OpenSDI挑战方法相比，MaskCLIP在IoU（提高14.23%）和准确率（提高2.05%）方面取得了显著的相对改进，在定位和检测任务上的F1得分分别提高了14.11%和2.38%，成为当前最优模型。我们的数据集和代码可在<a target="_blank" rel="noopener" href="https://github.com/iamwangyabin/OpenSDI%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/iamwangyabin/OpenSDI上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19653v3">PDF</a> </p>
<p><strong>Summary</strong><br>     本文介绍了OpenSDI挑战，即识别开放世界中扩散生成的图像的挑战。为应对这一挑战，定义了一个新的基准测试集OpenSDI数据集（OpenSDID），它使用大型视觉语言模型模拟开放世界的扩散操作，并包含检测和定位任务。提出协同预训练模型（SPM）方案，通过协同多种预训练基础模型，提高在OpenSDI背景下的泛化能力。基于这一方案，引入了MaskCLIP模型，该模型将对比语言图像预训练（CLIP）与掩码自动编码器（MAE）相结合。在OpenSDID上的广泛评估表明，MaskCLIP在OpenSDI挑战上显著优于当前最先进的方法，在定位和检测任务上的相对改进率分别为14.23%（IoU）和2.05%（准确率）。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenSDI挑战是识别开放世界中扩散生成的图像的问题。</li>
<li>OpenSDID数据集使用大型视觉语言模型模拟开放世界的扩散操作。</li>
<li>OpenSDID包含检测和定位任务，以应对全局和局部扩散模型生成的图像。</li>
<li>提出协同预训练模型（SPM）方案以提高泛化能力。</li>
<li>MaskCLIP模型结合CLIP和MAE技术，以应对OpenSDI挑战。</li>
<li>MaskCLIP在OpenSDID上的表现显著优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19653">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c5388eece71da2d9e75f7ddecce05349.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69ed092a7644e0d7c1d9f521b7197649.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd77e5bd119be7c97091f3e907209e71.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0b2353df3cf9adb8ab40449eb2a9354f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="OMR-Diffusion-Optimizing-Multi-Round-Enhanced-Training-in-Diffusion-Models-for-Improved-Intent-Understanding"><a href="#OMR-Diffusion-Optimizing-Multi-Round-Enhanced-Training-in-Diffusion-Models-for-Improved-Intent-Understanding" class="headerlink" title="OMR-Diffusion:Optimizing Multi-Round Enhanced Training in Diffusion   Models for Improved Intent Understanding"></a>OMR-Diffusion:Optimizing Multi-Round Enhanced Training in Diffusion   Models for Improved Intent Understanding</h2><p><strong>Authors:Kun Li, Jianhui Wang, Miao Zhang, Xueqian Wang</strong></p>
<p>Generative AI has significantly advanced text-driven image generation, but it still faces challenges in producing outputs that consistently align with evolving user preferences and intents, particularly in multi-turn dialogue scenarios. In this research, We present a Visual Co-Adaptation (VCA) framework that incorporates human-in-the-loop feedback, utilizing a well-trained reward model specifically designed to closely align with human preferences. Using a diverse multi-turn dialogue dataset, the framework applies multiple reward functions (such as diversity, consistency, and preference feedback) to refine the diffusion model through LoRA, effectively optimizing image generation based on user input. We also constructed multi-round dialogue datasets with prompts and image pairs that well-fit user intent. Experiments show the model achieves 508 wins in human evaluation, outperforming DALL-E 3 (463 wins) and others. It also achieves 3.4 rounds in dialogue efficiency (vs. 13.7 for DALL-E 3) and excels in metrics like LPIPS (0.15) and BLIP (0.59). Various experiments demonstrate the effectiveness of the proposed method over state-of-the-art baselines, with significant improvements in image consistency and alignment with user intent. </p>
<blockquote>
<p>生成式人工智能极大地推动了文本驱动图像生成技术的发展，但在多轮对话场景中，它仍然面临着在持续产生符合不断变化的用户偏好和意图的输出方面的挑战。本研究中，我们提出了一种视觉协同适应（VCA）框架，该框架结合了人类循环反馈，利用经过良好训练的奖励模型，专门设计以紧密符合人类偏好。使用多样化的多轮对话数据集，该框架应用多个奖励函数（如多样性、一致性和偏好反馈）来通过LoRA细化扩散模型，有效地根据用户输入优化图像生成。我们还构建了多轮对话数据集，带有符合用户意图的提示和图像对。实验表明，该模型在人工评估中获得508次胜利，优于DALL-E 3（463次胜利），以及其他模型。在对话效率方面，它达到了3.4轮（与DALL-E 3的13.7轮相比），并在LPIPS（0.15）和BLIP（0.59）等指标上表现出色。各项实验表明，相对于最先进的基线技术，所提出的方法在图像一致性和符合用户意图方面表现出卓越的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17660v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种视觉协同适应（VCA）框架，该框架结合人类用户反馈机制训练奖励模型，能够精确符合人类偏好。通过使用多样化多轮对话数据集，该框架应用多个奖励函数来优化扩散模型，使用户输入能够影响图像生成过程。实验证明，该模型在人类评估中取得了显著优势，超越了现有技术基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新的视觉协同适应（VCA）框架，该框架能够结合人类用户反馈机制训练奖励模型，以符合人类偏好。</li>
<li>利用多样化多轮对话数据集，应用多个奖励函数（如多样性、一致性和偏好反馈）来优化扩散模型。</li>
<li>通过用户输入来影响图像生成过程，实现了对用户意图的精准响应。</li>
<li>在人类评估中取得了显著优势，超过了当前最先进的模型DALL-E 3。</li>
<li>模型在对话效率方面表现出色，达到了3.4轮对话效率。</li>
<li>在图像一致性以及与用户意图对齐方面实现了显著改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17660">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-707d1e04e16aa09730069e37f177d24e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83ca96adcd6d19d7305b7df3f51bb256.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Fine-Tuning-Diffusion-Generative-Models-via-Rich-Preference-Optimization"><a href="#Fine-Tuning-Diffusion-Generative-Models-via-Rich-Preference-Optimization" class="headerlink" title="Fine-Tuning Diffusion Generative Models via Rich Preference Optimization"></a>Fine-Tuning Diffusion Generative Models via Rich Preference Optimization</h2><p><strong>Authors:Hanyang Zhao, Haoxian Chen, Yucheng Guo, Genta Indra Winata, Tingting Ou, Ziyu Huang, David D. Yao, Wenpin Tang</strong></p>
<p>We introduce Rich Preference Optimization (RPO), a novel pipeline that leverages rich feedback signals to improve the curation of preference pairs for fine-tuning text-to-image diffusion models. Traditional methods, like Diffusion-DPO, often rely solely on reward model labeling, which can be opaque, offer limited insights into the rationale behind preferences, and are prone to issues such as reward hacking or overfitting. In contrast, our approach begins with generating detailed critiques of synthesized images to extract reliable and actionable image editing instructions. By implementing these instructions, we create refined images, resulting in synthetic, informative preference pairs that serve as enhanced tuning datasets. We demonstrate the effectiveness of our pipeline and the resulting datasets in fine-tuning state-of-the-art diffusion models. </p>
<blockquote>
<p>我们介绍了富偏好优化（RPO）这一新型流程，它利用丰富的反馈信号来改善偏好对的筛选，以微调文本到图像的扩散模型。传统方法，如Diffusion-DPO，通常仅依赖奖励模型标签，这可能造成不透明、对偏好背后的理由了解有限，并容易出现奖励作弊或过度拟合等问题。相比之下，我们的方法首先生成合成图像的详细评价，以提取可靠且可操作的图像编辑指令。通过执行这些指令，我们创建更精细的图像，从而产生合成、信息丰富的偏好对，作为增强调整数据集。我们展示了我们的流程和生成的数据集在微调最先进的扩散模型方面的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11720v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了Rich Preference Optimization（RPO）这一新方法，它利用丰富的反馈信号来改善偏好对的筛选，以优化文本到图像的扩散模型。与传统的如Diffusion-DPO等方法相比，RPO不再仅依赖奖励模型标注，而是通过生成合成图像的详细评价来提取可靠的图像编辑指令。这些指令的实施产生了精细化图像，进而形成合成、信息丰富的偏好对，作为增强调整数据集。本文展示了该管道及其生成的数据集在微调最先进扩散模型方面的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Rich Preference Optimization (RPO) 利用丰富的反馈信号改善文本到图像扩散模型的偏好对筛选。</li>
<li>传统方法如Diffusion-DPO依赖奖励模型标注，存在不透明性、对偏好背后理据了解有限及奖励黑客或过度拟合等问题。</li>
<li>RPO通过生成合成图像的详细评价来提取可靠且可操作的图像编辑指令。</li>
<li>这些指令的实施产生了精细化图像，形成合成、信息丰富的偏好对，作为增强调整数据集。</li>
<li>RPO方法提高了扩散模型的图像生成质量。</li>
<li>RPO有助于更好地理解模型在图像生成过程中的偏好和弱点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11720">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-23258fdcc7fa63cd56d0fae082704fa0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e05bd050fbfefa9425ced3ec3907a528.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa84db95bf3158e76d3be4c130bd2510.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d68cd9b031870eabb55bb1d30e6da7ad.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GST-Precise-3D-Human-Body-from-a-Single-Image-with-Gaussian-Splatting-Transformers"><a href="#GST-Precise-3D-Human-Body-from-a-Single-Image-with-Gaussian-Splatting-Transformers" class="headerlink" title="GST: Precise 3D Human Body from a Single Image with Gaussian Splatting   Transformers"></a>GST: Precise 3D Human Body from a Single Image with Gaussian Splatting   Transformers</h2><p><strong>Authors:Lorenza Prospero, Abdullah Hamdi, Joao F. Henriques, Christian Rupprecht</strong></p>
<p>Reconstructing posed 3D human models from monocular images has important applications in the sports industry, including performance tracking, injury prevention and virtual training. In this work, we combine 3D human pose and shape estimation with 3D Gaussian Splatting (3DGS), a representation of the scene composed of a mixture of Gaussians. This allows training or fine-tuning a human model predictor on multi-view images alone, without 3D ground truth. Predicting such mixtures for a human from a single input image is challenging due to self-occlusions and dependence on articulations, while also needing to retain enough flexibility to accommodate a variety of clothes and poses. Our key observation is that the vertices of standardized human meshes (such as SMPL) can provide an adequate spatial density and approximate initial position for the Gaussians. We can then train a transformer model to jointly predict comparatively small adjustments to these positions, as well as the other 3DGS attributes and the SMPL parameters. We show empirically that this combination (using only multi-view supervision) can achieve near real-time inference of 3D human models from a single image without expensive diffusion models or 3D points supervision, thus making it ideal for the sport industry at any level. More importantly, rendering is an effective auxiliary objective to refine 3D pose estimation by accounting for clothes and other geometric variations. The code is available at <a target="_blank" rel="noopener" href="https://github.com/prosperolo/GST">https://github.com/prosperolo/GST</a>. </p>
<blockquote>
<p>从单目图像重建三维人体模型在体育产业中有重要应用，包括性能跟踪、损伤预防和虚拟训练。在这项工作中，我们将三维人体姿态和形状估计与三维高斯拼贴（3DGS）相结合，后者是场景的一种表示方法，由高斯混合组成。这允许仅通过多视角图像训练或微调人体模型预测器，无需三维真实数据。从单个输入图像预测人体的这种混合是充满挑战的，因为存在自遮挡问题并且依赖于关节活动，同时还需要保留足够的灵活性以适应各种服装和姿势。我们的关键观察是标准化人体网格（如SMPL）的顶点可以提供足够的空间密度和近似初始位置来定义高斯。然后我们可以训练一个transformer模型来共同预测这些位置的微小调整，以及其他3DGS属性和SMPL参数。我们从实证研究中发现，这种结合（仅使用多视角监督）可以达到近乎实时的效果，从单个图像推断出三维人体模型，无需昂贵的扩散模型或三维点监督，因此非常适合各级体育产业。更重要的是，渲染是完善三维姿态估计的有效辅助目标，它考虑了服装和其他几何变化。代码可在<a target="_blank" rel="noopener" href="https://github.com/prosperolo/GST">https://github.com/prosperolo/GST</a>找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.04196v2">PDF</a> Camera ready for CVSports workshop at CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了结合3D人体姿态与形状估计以及3D高斯混合表示技术，通过单目图像重建人体模型的方法。通过多视角图像进行模型训练或精细调整，无需真实的3D场景。文中解决了由单张图像预测人体混合高斯模型时的遮挡与关节运动等问题，通过标准人体网格顶点为高斯模型提供初始位置和空间密度，并使用转换器模型预测位置微调和其他属性参数。该研究不使用昂贵的扩散模型或复杂的3D点监督技术即可实现近乎实时的图像到模型推断，非常适合运动产业。渲染是完善姿态估计的有效辅助手段，考虑了衣物和几何变化因素。相关代码已公开。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>结合了3D人体姿态与形状估计以及3D高斯混合表示技术，用于从单目图像重建人体模型。</li>
<li>利用多视角图像进行模型训练或精细调整，无需依赖真实的3D场景。</li>
<li>主要解决了通过单张图像预测混合高斯人体模型的遮挡与关节运动难题。</li>
<li>标准人体网格顶点提供了高斯模型的初始位置和空间密度。</li>
<li>利用转换器模型预测高斯模型的调整位置以及其他参数。</li>
<li>不使用复杂的扩散模型和高级监督技术，即可实现快速从图像推断到模型的过程。</li>
<li>该方法适合运动产业应用，如性能追踪、伤害预防及虚拟训练等。</li>
<li>渲染技术作为辅助手段，有助于考虑衣物和几何变化因素，完善姿态估计的准确性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.04196">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0b168957dca4c706d6aaa8d7d8044112.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9d7cbde8f4e52111c2921e081bb1fce8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91503c9d231335d44c4f370bbec0e951.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e82e4f112f743598ad7ec9b6101a4b6a.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="StreamingT2V-Consistent-Dynamic-and-Extendable-Long-Video-Generation-from-Text"><a href="#StreamingT2V-Consistent-Dynamic-and-Extendable-Long-Video-Generation-from-Text" class="headerlink" title="StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation   from Text"></a>StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation   from Text</h2><p><strong>Authors:Roberto Henschel, Levon Khachatryan, Hayk Poghosyan, Daniil Hayrapetyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, Humphrey Shi</strong></p>
<p>Text-to-video diffusion models enable the generation of high-quality videos that follow text instructions, making it easy to create diverse and individual content. However, existing approaches mostly focus on high-quality short video generation (typically 16 or 24 frames), ending up with hard-cuts when naively extended to the case of long video synthesis. To overcome these limitations, we introduce StreamingT2V, an autoregressive approach for long video generation of 80, 240, 600, 1200 or more frames with smooth transitions. The key components are:(i) a short-term memory block called conditional attention module (CAM), which conditions the current generation on the features extracted from the previous chunk via an attentional mechanism, leading to consistent chunk transitions, (ii) a long-term memory block called appearance preservation module, which extracts high-level scene and object features from the first video chunk to prevent the model from forgetting the initial scene, and (iii) a randomized blending approach that enables to apply a video enhancer autoregressively for infinitely long videos without inconsistencies between chunks. Experiments show that StreamingT2V generates high motion amount. In contrast, all competing image-to-video methods are prone to video stagnation when applied naively in an autoregressive manner. Thus, we propose with StreamingT2V a high-quality seamless text-to-long video generator that outperforms competitors with consistency and motion. Our code will be available at: <a target="_blank" rel="noopener" href="https://github.com/Picsart-AI-Research/StreamingT2V">https://github.com/Picsart-AI-Research/StreamingT2V</a> </p>
<blockquote>
<p>文本到视频扩散模型能够根据文本指令生成高质量的视频，使得创建多样化和个性化内容变得轻松。然而，现有方法主要集中在高质量短视频生成（通常为16或24帧），在扩展到长视频合成时会出现生硬剪辑的情况。为了克服这些限制，我们推出了StreamingT2V，这是一种用于生成80、240、600、1200或更多帧的长视频的自动回归方法，具有平滑过渡。关键组件包括：(i)一个名为条件注意模块（CAM）的短期记忆块，它通过注意机制根据从上一个块提取的特征来条件当前生成，从而实现一致的块过渡；(ii)一个名为外观保留模块的长期记忆块，它从第一个视频块中提取高级场景和对象特征，以防止模型忘记初始场景；(iii)一种随机混合方法，使视频增强器能够自动应用于无限长的视频，而不会在不同块之间出现不一致。实验表明，StreamingT2V能产生大量的运动。相比之下，所有竞争性的图像到视频的方法在采用自动回归方式时都容易出现视频停滞的情况。因此，我们提出StreamingT2V，这是一个高质量的无缝文本到长视频生成器，在一致性和运动方面超越竞争对手。我们的代码将在<a target="_blank" rel="noopener" href="https://github.com/Picsart-AI-Research/StreamingT2V%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/Picsart-AI-Research/StreamingT2V上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.14773v2">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/Picsart-AI-Research/StreamingT2V">https://github.com/Picsart-AI-Research/StreamingT2V</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了文本到视频扩散模型在生成高质量视频方面的应用，尤其是针对长视频生成的挑战。为解决现有方法的局限性，提出了一种新的方法StreamingT2V，通过引入短期记忆块（条件注意力模块）和长期记忆块（外观保留模块）以及随机混合技术，实现了高质量的长视频生成。该方法克服了现有方法的不足，表现出出色的连贯性和运动性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>文本到视频扩散模型能生成遵循文本指令的高质量视频内容。</li>
<li>现有方法主要关注短视频生成，但在长视频合成时存在硬切割问题。</li>
<li>StreamingT2V是一种用于长视频生成的方法，具有80帧及以上的流畅过渡功能。</li>
<li>条件注意力模块是实现一致过渡的关键组件之一。它根据注意力机制将当前生成与先前片段的特征联系起来。</li>
<li>外观保留模块作为长期记忆块，从第一个视频片段中提取高级场景和对象特征，防止模型忘记初始场景。</li>
<li>随机混合技术使得视频增强器可以应用于无限长的视频，而无需在片段之间出现不一致。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.14773">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7d15168a238ff43cf2d3be6f28b01945.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-463bda0d77a6fccb28dd9d23e03d73e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b6e0a24fc39333806c2e8a40d3380ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe82ef9548a9b1d9b5ff64f5751c55fa.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SignDiff-Diffusion-Model-for-American-Sign-Language-Production"><a href="#SignDiff-Diffusion-Model-for-American-Sign-Language-Production" class="headerlink" title="SignDiff: Diffusion Model for American Sign Language Production"></a>SignDiff: Diffusion Model for American Sign Language Production</h2><p><strong>Authors:Sen Fang, Chunyu Sui, Yanghao Zhou, Xuedong Zhang, Hongbin Zhong, Yapeng Tian, Chen Chen</strong></p>
<p>In this paper, we propose a dual-condition diffusion pre-training model named SignDiff that can generate human sign language speakers from a skeleton pose. SignDiff has a novel Frame Reinforcement Network called FR-Net, similar to dense human pose estimation work, which enhances the correspondence between text lexical symbols and sign language dense pose frames, reduces the occurrence of multiple fingers in the diffusion model. In addition, we propose a new method for American Sign Language Production (ASLP), which can generate ASL skeletal pose videos from text input, integrating two new improved modules and a new loss function to improve the accuracy and quality of sign language skeletal posture and enhance the ability of the model to train on large-scale data. We propose the first baseline for ASL production and report the scores of 17.19 and 12.85 on BLEU-4 on the How2Sign dev&#x2F;test sets. We evaluated our model on the previous mainstream dataset PHOENIX14T, and the experiments achieved the SOTA results. In addition, our image quality far exceeds all previous results by 10 percentage points in terms of SSIM. </p>
<blockquote>
<p>本文提出了一种名为SignDiff的双条件扩散预训练模型，该模型可以从骨架姿态生成人类手语。SignDiff具有一个名为FR-Net的新型框架强化网络，类似于密集人体姿态估计工作，它增强了文本词汇符号与手语密集姿态框架之间的对应关系，减少了扩散模型中多个手指的出现。此外，我们提出了一种新的美国手语生产（ASLP）方法，可以从文本输入生成ASL骨架姿态视频，集成两个新改进模块和新的损失函数，以提高手语骨架姿态的准确性和质量，并增强模型在大规模数据上的训练能力。我们为ASL生产提出了第一个基准线，并在How2Sign的dev&#x2F;test集上报告了BLEU-4得分为17.19和12.85。我们在之前的主流数据集PHOENIX14T上评估了我们的模型，实验达到了SOTA结果。此外，我们的图像质量在SSIM方面超过了以前所有结果，提高了10个百分点。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16082v3">PDF</a> Project Page at <a target="_blank" rel="noopener" href="https://signdiff.github.io/">https://signdiff.github.io</a></p>
<p><strong>Summary</strong><br>文本提出了一种名为SignDiff的双条件扩散预训练模型，该模型可以从骨架姿态生成人类手势语言。SignDiff拥有一个新颖的帧强化网络FR-Net，类似于密集的人类姿态估计工作，它可以增强文本词汇符号与手势语言密集姿态帧之间的对应关系，减少扩散模型中多个手指的出现。此外，文本还提出了美国手势语言生产（ASLP）的新方法，可以从文本输入生成ASL骨架姿态视频，集成两个新改进模块和新的损失函数，以提高手势语言骨架姿态的准确性和质量，并增强模型在大规模数据上的训练能力。实验结果表明，该模型在How2Sign开发&#x2F;测试集上的BLEU-4分数为17.19和12.85，并在PHOENIX14T主流数据集上取得了最佳结果。此外，我们的图像质量在SSIM方面超过了之前所有结果10个百分点。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SignDiff是一个双条件扩散预训练模型，可从骨架姿态生成人类手势语言。</li>
<li>SignDiff具有FR-Net网络，增强了文本与手势语言姿态之间的对应关系。</li>
<li>提出了美国手势语言生产（ASLP）的新方法，结合了两个改进模块和新的损失函数。</li>
<li>模型在How2Sign开发&#x2F;测试集上的BLEU-4分数达到新的水平。</li>
<li>模型在PHOENIX14T数据集上取得了最佳结果。</li>
<li>与先前的结果相比，模型在图像质量上有了显著提高，特别是在SSIM方面。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2308.16082">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f98a8273e567443f16b7840f5cfd5c52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e14b2ec9edcebdf37631fa69c9b74dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84b78c46d413ca62b84dc0bef2467a05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-748dcf45ecaef95aaca763e16ac8899b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2e4f260423f573980d0a29134501998a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c23a45c62cdd55367ac1fb1f4ece25ef.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-18/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-18/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-18/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-316b6273e319ef222f2c5f5ca1700806.jpg" class="responsive-img" alt="牙齿修复">
                        
                        <span class="card-title">牙齿修复</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            牙齿修复 方向最新论文已更新，请持续关注 Update in 2025-04-18  Cross-Frequency Collaborative Training Network and Dataset for   Semi-supervised First Molar Root Canal Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    牙齿修复
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">牙齿修复</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-18/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7c9b3281af8c3815692554eb31055595.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-04-18  R-Meshfusion Reinforcement Learning Powered Sparse-View Mesh   Reconstruction with Diffusion Priors
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24231k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
