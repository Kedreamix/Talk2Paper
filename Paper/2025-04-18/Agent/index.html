<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-04-18  ARCeR an Agentic RAG for the Automated Definition of Cyber Ranges">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-3a50c7d599d569532aa2b777fa0f53a9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    48 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-18-更新"><a href="#2025-04-18-更新" class="headerlink" title="2025-04-18 更新"></a>2025-04-18 更新</h1><h2 id="ARCeR-an-Agentic-RAG-for-the-Automated-Definition-of-Cyber-Ranges"><a href="#ARCeR-an-Agentic-RAG-for-the-Automated-Definition-of-Cyber-Ranges" class="headerlink" title="ARCeR: an Agentic RAG for the Automated Definition of Cyber Ranges"></a>ARCeR: an Agentic RAG for the Automated Definition of Cyber Ranges</h2><p><strong>Authors:Matteo Lupinacci, Francesco Blefari, Francesco Romeo, Francesco Aurelio Pironti, Angelo Furfaro</strong></p>
<p>The growing and evolving landscape of cybersecurity threats necessitates the development of supporting tools and platforms that allow for the creation of realistic IT environments operating within virtual, controlled settings as Cyber Ranges (CRs). CRs can be exploited for analyzing vulnerabilities and experimenting with the effectiveness of devised countermeasures, as well as serving as training environments for building cyber security skills and abilities for IT operators. This paper proposes ARCeR as an innovative solution for the automatic generation and deployment of CRs, starting from user-provided descriptions in a natural language. ARCeR relies on the Agentic RAG paradigm, which allows it to fully exploit state-of-art AI technologies. Experimental results show that ARCeR is able to successfully process prompts even in cases that LLMs or basic RAG systems are not able to cope with. Furthermore, ARCeR is able to target any CR framework provided that specific knowledge is made available to it. </p>
<blockquote>
<p>网络安全威胁的不断增长和演变，需要开发支持工具和平台，以在虚拟、受控的环境中创建现实的IT环境作为网络安全范围（CRs）。网络安全范围可用于分析漏洞，测试制定的对策的有效性，还可以作为构建网络安全技能和提高it操作员能力的训练环境。本文提出ARCeR作为一种创新解决方案，可以根据用户提供自然语言的描述自动生成并部署网络安全范围。ARCeR依赖于Agentic RAG范式，可以充分利用最新的AI技术。实验结果表明，即使在一些情况下大型语言模型或基本的RAG系统无法应对，ARCeR也能成功处理提示。此外，只要向其提供特定知识，ARCeR就能针对任何网络安全范围框架进行部署。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12143v1">PDF</a> </p>
<p><strong>Summary</strong><br>     网络安全威胁的不断发展和演变催生了对支持工具和平台的需求，这些工具可以创建在虚拟、受控环境中运行的现实IT环境，称为网络安全范围（CR）。网络安全范围可用于分析漏洞、测试制定的对策的有效性，并作为培训环境，培养网络安全技能和能力。本文提出ARCeR作为一种创新的解决方案，可以根据用户提供的自然语言描述自动生成和部署网络安全范围。ARCeR依赖于Agentic RAG范式，能够充分利用最新的AI技术。实验结果表明，ARCeR能够在大型语言模型或基本RAG系统无法处理的情况下成功处理提示。此外，只要向其提供特定知识，ARCeR就能够针对任何网络安全范围框架进行部署。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>网络安全威胁的演变推动了网络安全范围（CR）的发展，用于分析和测试漏洞以及提高网络安全技能和能力的培训。</li>
<li>ARCeR是一种创新的解决方案，可以根据用户提供的自然语言描述自动生成和部署网络安全范围。</li>
<li>ARCeR依赖于Agentic RAG范式，能够充分利用最新的AI技术来提升生成过程的智能化和自动化水平。</li>
<li>实验结果显示，ARCeR在特定场景下比现有系统更具优势，可以成功处理其他系统无法应对的提示。</li>
<li>ARCeR具有广泛的适用性，能够支持多种网络安全范围框架，只要向其提供特定知识即可进行部署和应用。</li>
<li>ARCeR在提高网络安全和推动网络安全技能提升方面具有重要的实际应用价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12143">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-147ed9534f582c42439d7b07507c407c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d246b7fcb203a0311e43c7e233da27fe.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="EmoACT-a-Framework-to-Embed-Emotions-into-Artificial-Agents-Based-on-Affect-Control-Theory"><a href="#EmoACT-a-Framework-to-Embed-Emotions-into-Artificial-Agents-Based-on-Affect-Control-Theory" class="headerlink" title="EmoACT: a Framework to Embed Emotions into Artificial Agents Based on   Affect Control Theory"></a>EmoACT: a Framework to Embed Emotions into Artificial Agents Based on   Affect Control Theory</h2><p><strong>Authors:Francesca Corrao, Alice Nardelli, Jennifer Renoux, Carmine Tommaso Recchiuto</strong></p>
<p>As robots and artificial agents become increasingly integrated into daily life, enhancing their ability to interact with humans is essential. Emotions, which play a crucial role in human interactions, can improve the naturalness and transparency of human-robot interactions (HRI) when embodied in artificial agents. This study aims to employ Affect Control Theory (ACT), a psychological model of emotions deeply rooted in interaction, for the generation of synthetic emotions. A platform-agnostic framework inspired by ACT was developed and implemented in a humanoid robot to assess its impact on human perception. Results show that the frequency of emotional displays impacts how users perceive the robot. Moreover, appropriate emotional expressions seem to enhance the robot’s perceived emotional and cognitive agency. The findings suggest that ACT can be successfully employed to embed synthetic emotions into robots, resulting in effective human-robot interactions, where the robot is perceived more as a social agent than merely a machine. </p>
<blockquote>
<p>随着机器人和人工代理在日常生活中得到越来越深入的整合，增强他们与人类互动的能力变得至关重要。情感在人类互动中扮演着至关重要的角色，当融入到人工代理中时，情感可以提高人机互动的自然性和透明度。本研究旨在采用情感控制理论（ACT），这是一种深深根植于互动中的情感心理学模型，来生成合成情感。受ACT启发的平台无关框架被开发并应用在人形机器人上，以评估其对人类感知的影响。结果表明，情感展示的频率会影响用户如何感知机器人。此外，适当的情感表达似乎增强了机器人感知到的情感和认知能力。研究结果建议，可以成功运用ACT将合成情感嵌入机器人，从而实现有效的人机互动，其中机器人被视为一个社会代理人，而不仅仅是一台机器。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12125v1">PDF</a> </p>
<p><strong>总结</strong><br>    随着机器人和人工智能代理在日常生活中日益普及，提高其与人类互动的能力至关重要。情感在人类互动中扮演着重要角色，当融入人工智能代理时，可以改善人机互动的自然性和透明度。本研究旨在运用情感控制理论（ACT），这是一个深深植根于互动的心理学情感模型，来生成合成情感。受ACT启发的跨平台框架被开发并应用在一个类人机器人上，以评估它对人类感知的影响。结果显示，情感显示频率影响用户对机器人的感知。此外，适当的情感表达似乎可以增强机器人感知到的情感和认知代理。研究表明，成功运用ACT将合成情感嵌入机器人，可实现有效的人机互动，机器人被视为社会代理而非仅仅是机器。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>情感在人机互动中扮演着重要角色，可以提高互动的自然性和透明度。</li>
<li>将情感融入人工智能代理是必要的，因为这对日常生活中的普及越来越重要。</li>
<li>本研究旨在运用情感控制理论（ACT）来生成合成情感，这是一种基于心理学的情感模型。</li>
<li>开发了一个跨平台的框架并将其应用于类人机器人上，以评估其对人类感知的影响。</li>
<li>实验结果表明，情感显示的频率会影响用户对机器人的感知。</li>
<li>适当的情感表达可以增强用户对机器人情感和认知能力的感知。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12125">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b7797c8944a3106a58e843367e050585.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Towards-LLM-Agents-for-Earth-Observation"><a href="#Towards-LLM-Agents-for-Earth-Observation" class="headerlink" title="Towards LLM Agents for Earth Observation"></a>Towards LLM Agents for Earth Observation</h2><p><strong>Authors:Chia Hsiang Kao, Wenting Zhao, Shreelekha Revankar, Samuel Speas, Snehal Bhagat, Rajeev Datta, Cheng Perng Phoo, Utkarsh Mall, Carl Vondrick, Kavita Bala, Bharath Hariharan</strong></p>
<p>Earth Observation (EO) provides critical planetary data for environmental monitoring, disaster management, climate science, and other scientific domains. Here we ask: Are AI systems ready for reliable Earth Observation? We introduce \datasetnamenospace, a benchmark of 140 yes&#x2F;no questions from NASA Earth Observatory articles across 13 topics and 17 satellite sensors. Using Google Earth Engine API as a tool, LLM agents can only achieve an accuracy of 33% because the code fails to run over 58% of the time. We improve the failure rate for open models by fine-tuning synthetic data, allowing much smaller models (Llama-3.1-8B) to achieve comparable accuracy to much larger ones (e.g., DeepSeek-R1). Taken together, our findings identify significant challenges to be solved before AI agents can automate earth observation, and suggest paths forward. The project page is available at <a target="_blank" rel="noopener" href="https://iandrover.github.io/UnivEarth">https://iandrover.github.io/UnivEarth</a>. </p>
<blockquote>
<p>地球观测（EO）为环境监测、灾害管理、气候科学和其他科学领域提供了关键性的行星数据。在这里我们提出一个问题：人工智能系统是否已经准备好进行可靠的地球观测？我们引入了\datasetnamenospace数据集，该数据集包含来自NASA地球观测站文章的140个是与非问题，涵盖13个主题和17个卫星传感器。利用Google地球引擎API作为工具，LLM代理人的准确率仅为33%，因为代码有超过58%的时间无法运行。我们通过微调合成数据来提高开放模型的失败率，允许较小的模型（如Llama-3.1-8B）达到与较大的模型（如DeepSeek-R1）相当的准确率。总之，我们的研究指出了在人工智能代理实现自动化地球观测之前需要解决的重大挑战，并提出了今后的方向。项目页面可在<a target="_blank" rel="noopener" href="https://iandrover.github.io/UnivEarth%E6%9F%A5%E7%9C%8B%E3%80%82">https://iandrover.github.io/UnivEarth查看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12110v1">PDF</a> 36 pages</p>
<p><strong>Summary</strong></p>
<p>本文介绍了地球观测（EO）的重要性及其在环境监测、灾害管理、气候科学等领域的应用。针对AI系统在地球观测中的可靠性问题，文章提出了一个基准测试集，包含NASA地球观测站的140个是与非是问题，涉及13个主题和17颗卫星传感器。文章指出，使用Google地球引擎API作为工具时，大型语言模型（LLM）代理的准确率仅为33%，且存在代码运行失败率高达58%的问题。研究通过微调合成数据提高了开放模型的性能，并发现较小的模型（如Llama-3.1-8B）也能达到与大型模型相近的准确率。文章总结了AI代理在自动化地球观测方面仍需解决的挑战，并提出了解决方案。项目页面可通过链接访问：<a target="_blank" rel="noopener" href="https://iandrover.github.io/UnivEarth">https://iandrover.github.io/UnivEarth</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>地球观测（EO）对于环境监测、灾害管理、气候科学等领域至关重要。</li>
<li>AI系统在地球观测中面临可靠性挑战。</li>
<li>提出一个基准测试集，包含NASA地球观测站的140个是与非是问题，用于评估AI系统在地球观测方面的表现。</li>
<li>使用Google地球引擎API时，大型语言模型（LLM）代理的准确率仅为33%，且存在代码运行失败率高达58%的问题。</li>
<li>通过微调合成数据可提高开放模型的性能。</li>
<li>较小模型（如Llama-3.1-8B）也能达到与大型模型相近的准确率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12110">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3a50c7d599d569532aa2b777fa0f53a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ecd51e4caf54bb5546639a8750ae900.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-953d4a1b1acb6343294e31abafe62e01.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="GrabS-Generative-Embodied-Agent-for-3D-Object-Segmentation-without-Scene-Supervision"><a href="#GrabS-Generative-Embodied-Agent-for-3D-Object-Segmentation-without-Scene-Supervision" class="headerlink" title="GrabS: Generative Embodied Agent for 3D Object Segmentation without   Scene Supervision"></a>GrabS: Generative Embodied Agent for 3D Object Segmentation without   Scene Supervision</h2><p><strong>Authors:Zihui Zhang, Yafei Yang, Hongtao Wen, Bo Yang</strong></p>
<p>We study the hard problem of 3D object segmentation in complex point clouds without requiring human labels of 3D scenes for supervision. By relying on the similarity of pretrained 2D features or external signals such as motion to group 3D points as objects, existing unsupervised methods are usually limited to identifying simple objects like cars or their segmented objects are often inferior due to the lack of objectness in pretrained features. In this paper, we propose a new two-stage pipeline called GrabS. The core concept of our method is to learn generative and discriminative object-centric priors as a foundation from object datasets in the first stage, and then design an embodied agent to learn to discover multiple objects by querying against the pretrained generative priors in the second stage. We extensively evaluate our method on two real-world datasets and a newly created synthetic dataset, demonstrating remarkable segmentation performance, clearly surpassing all existing unsupervised methods. </p>
<blockquote>
<p>我们研究了复杂点云中的三维物体分割这一难题，且研究过程中无需对三维场景进行人为标注以进行监督。现有无监督方法通常依赖于预训练的二维特征或运动等外部信号的相似性来对三维点进行物体分组，这些方法通常仅限于识别汽车等简单物体，或者其分割的物体质量较差，因为预训练特征中缺乏物体特性。在本文中，我们提出了一种新的两阶段流程，称为GrabS。我们的方法的核心概念是在第一阶段从对象数据集中学习生成和判别对象中心先验知识作为基础，然后在第二阶段设计一个智能体通过学习查询预训练的生成先验知识来发现多个对象。我们在两个真实数据集和一个新创建合成数据集上全面评估了我们的方法，展示出了显著的分割性能，明显超越了所有现有的无监督方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11754v1">PDF</a> ICLR 2025 Spotlight. Code and data are available at:   <a target="_blank" rel="noopener" href="https://github.com/vLAR-group/GrabS">https://github.com/vLAR-group/GrabS</a></p>
<p><strong>Summary</strong></p>
<p>本文研究了无需人工标注的复杂点云中的三维物体分割问题。文章提出了一种新的两步方法，名为GrabS。首先通过利用预先训练好的二维特征或外部信号（如运动）将三维点分为对象来学习生成性和鉴别性对象为中心的先验知识。在第一阶段进行。然后设计一种自主代理，通过查询预先训练的生成先验知识来发现多个对象。该方法在真实数据集和新建合成数据集上的表现令人印象深刻，显著优于现有的无监督方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>研究针对复杂点云中的三维物体分割问题，无需人工标注。</li>
<li>提出了一种新的两步方法GrabS进行三维物体分割。</li>
<li>第一阶段学习生成性和鉴别性对象为中心的先验知识。</li>
<li>第二阶段设计自主代理，通过查询预先训练的生成先验知识来发现多个对象。</li>
<li>方法在真实数据集和新建合成数据集上的表现优异，超越现有无监督方法。</li>
<li>利用预先训练好的二维特征和外部信号（如运动）进行三维点分组。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11754">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0b0c0ae696fc4dd6629870acf9a0b536.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b88170a9d116775ed14b9ff16209e718.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b387ccd0a6424e335d77a2233b5a2e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f408ba1466d06a4ac931b853356b19bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-634e86cf064251b6764724358a3afcb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d01f32fc8aa22460131e4cde9d61973.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="REAL-Benchmarking-Autonomous-Agents-on-Deterministic-Simulations-of-Real-Websites"><a href="#REAL-Benchmarking-Autonomous-Agents-on-Deterministic-Simulations-of-Real-Websites" class="headerlink" title="REAL: Benchmarking Autonomous Agents on Deterministic Simulations of   Real Websites"></a>REAL: Benchmarking Autonomous Agents on Deterministic Simulations of   Real Websites</h2><p><strong>Authors:Divyansh Garg, Shaun VanWeelden, Diego Caples, Andis Draguns, Nikil Ravi, Pranav Putta, Naman Garg, Tomas Abraham, Michael Lara, Federico Lopez, James Liu, Atharva Gundawar, Prannay Hebbar, Youngchul Joo, Charles London, Christian Schroeder de Witt, Sumeet Motwani</strong></p>
<p>We introduce REAL, a benchmark and framework for multi-turn agent evaluations on deterministic simulations of real-world websites. REAL comprises high-fidelity, deterministic replicas of 11 widely-used websites across domains such as e-commerce, travel, communication, and professional networking. We also release a benchmark consisting of 112 practical tasks that mirror everyday complex user interactions requiring both accurate information retrieval and state-changing actions. All interactions occur within this fully controlled setting, eliminating safety risks and enabling robust, reproducible evaluation of agent capability and reliability. Our novel evaluation framework combines programmatic checks of website state for action-based tasks with rubric-guided LLM-based judgments for information retrieval. The framework supports both open-source and proprietary agent systems through a flexible evaluation harness that accommodates black-box commands within browser environments, allowing research labs to test agentic systems without modification. Our empirical results show that frontier language models achieve at most a 41% success rate on REAL, highlighting critical gaps in autonomous web navigation and task completion capabilities. Our framework supports easy integration of new tasks, reproducible evaluation, and scalable data generation for training web agents. The websites, framework, and leaderboard are available at <a target="_blank" rel="noopener" href="https://realevals.xyz/">https://realevals.xyz</a> and <a target="_blank" rel="noopener" href="https://github.com/agi-inc/REAL">https://github.com/agi-inc/REAL</a>. </p>
<blockquote>
<p>我们引入了REAL，这是一个在真实网站确定性模拟上进行多轮代理评估的基准和框架。REAL包含了高保真、确定性的复制版本，涵盖了电子商务、旅游、通信和专业网络等领域的11个广泛使用的网站。我们还发布了一个包含112个实用任务的基准测试，这些任务反映了日常复杂的用户交互，需要准确的信息检索和状态更改操作。所有的交互都发生在这个完全受控的环境中，消除了安全风险，并能够稳健、可重复地评估代理的能力和可靠性。我们新颖的评价框架结合了基于网站状态的程序化检查（针对基于行动的任务）和基于规则的LLM判断（针对信息检索）。该框架支持开源和专有代理系统，通过一个灵活的评价装置来适应浏览器环境中的黑箱命令，使得研究实验室能够测试代理系统而无需进行修改。我们的实证结果表明，最先进的语言模型在REAL上的成功率最高仅为41%，这凸显了自主网页导航和任务完成能力的关键差距。我们的框架支持轻松集成新任务、可重复评估和可扩展的数据生成，以训练网页代理。网站、框架和排行榜可在<a target="_blank" rel="noopener" href="https://realevals.xyz和https//github.com/agi-inc/REAL%E6%89%BE%E5%88%B0%E3%80%82">https://realevals.xyz和https://github.com/agi-inc/REAL找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11543v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了REAL，这是一个针对真实世界网站确定性模拟的多轮代理评估的基准和框架。REAL包含11个高保真确定性副本的广泛使用的网站，并发布了一个包含112个实用任务的基准测试，这些任务反映了日常复杂的用户交互，需要准确的信息检索和状态更改操作。该评估框架结合了网站状态的程序检查，用于基于行动的任务，以及与检索信息指南的LLM判断。它为开源和专有代理系统提供了一个灵活的评价平台，支持在浏览器环境中进行黑箱命令操作，使得研究实验室能够测试代理系统无需修改。实证研究结果显示，最先进的语言模型在REAL上的成功率仅为41%，这凸显了自主网页导航和任务完成能力的关键差距。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>REAL是一个用于评估多轮代理在真实网站确定性模拟上的表现的基准和框架。</li>
<li>它包含11个高保真网站副本和112个实用任务，反映日常用户交互。</li>
<li>REAL的评估框架结合了程序检查和LLM判断，用于评价代理的能力和可靠性。</li>
<li>框架支持多种代理系统，并提供灵活的评价平台，允许在浏览器环境中进行黑箱操作。</li>
<li>实证研究指出，现有语言模型在REAL上的成功率较低，凸显了自主网页导航和任务完成能力的差距。</li>
<li>REAL易于集成新任务，支持可重复评估和可扩展的数据生成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11543">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b2089e84f6bba8c84e814ebc04819e15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62681ad777dd6cb34bb1abdacc6a99df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-acd0e40283c79364d0205c86a2ca3f53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce7baae5c4d9d27642c19d80d9591f58.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Task-Memory-Engine-TME-A-Structured-Memory-Framework-with-Graph-Aware-Extensions-for-Multi-Step-LLM-Agent-Tasks"><a href="#Task-Memory-Engine-TME-A-Structured-Memory-Framework-with-Graph-Aware-Extensions-for-Multi-Step-LLM-Agent-Tasks" class="headerlink" title="Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware   Extensions for Multi-Step LLM Agent Tasks"></a>Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware   Extensions for Multi-Step LLM Agent Tasks</h2><p><strong>Authors:Ye Ye</strong></p>
<p>Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks. However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers. This leads to brittle performance, frequent hallucinations, and poor long-range coherence. In this work, we propose the Task Memory Engine (TME), a lightweight and structured memory module that tracks task execution using a hierarchical Task Memory Tree (TMT). Each node in the tree corresponds to a task step, storing relevant input, output, status, and sub-task relationships. We introduce a prompt synthesis method that dynamically generates LLM prompts based on the active node path, significantly improving execution consistency and contextual grounding. Through case studies and comparative experiments on multi-step agent tasks, we demonstrate that TME leads to better task completion accuracy and more interpretable behavior with minimal implementation overhead. A reference implementation of the core TME components is available at <a target="_blank" rel="noopener" href="https://github.com/biubiutomato/TME-Agent">https://github.com/biubiutomato/TME-Agent</a>, including basic examples and structured memory integration. While the current implementation uses a tree-based structure, TME is designed to be graph-aware, supporting reusable substeps, converging task paths, and shared dependencies. This lays the groundwork for future DAG-based memory architectures. </p>
<blockquote>
<p>大型语言模型（LLMs）越来越多地被用作多步骤任务的自主代理。然而，大多数现有框架无法维持对任务状态的结构化理解，通常依赖于线性提示串联或浅内存缓冲区。这导致性能不稳定、频繁出现幻觉和长期连贯性差。在这项工作中，我们提出了任务记忆引擎（TME），这是一个轻量级、结构化的内存模块，使用分层的任务记忆树（TMT）来跟踪任务执行情况。树中的每个节点对应一个任务步骤，存储相关的输入、输出、状态和子任务关系。我们引入了一种提示合成方法，该方法根据活动节点路径动态生成LLM提示，显著提高了执行一致性上下文定位。通过多步骤代理任务的案例研究和对比实验，我们证明了TME在提高任务完成准确性和更可解释的行为方面具有优势，且实现开销最小。核心TME组件的参考实现可在[<a target="_blank" rel="noopener" href="https://github.com/biubiutomato/TME-Agent%E6%89%BE%E5%88%B0%EF%BC%8C%E5%8C%85%E6%8B%AC%E5%9F%BA%E6%9C%AC%E7%A4%BA%E4%BE%8B%E5%92%8C%E7%BB%93%E6%9E%84%E5%8C%96%E5%86%85%E5%AD%98%E9%9B%86%E6%88%90%E3%80%82%E8%99%BD%E7%84%B6%E5%BD%93%E5%89%8D%E5%AE%9E%E7%8E%B0%E4%BD%BF%E7%94%A8%E4%BA%86%E6%A0%91%E5%BD%A2%E7%BB%93%E6%9E%84%EF%BC%8C%E4%BD%86TME%E8%A2%AB%E8%AE%BE%E8%AE%A1%E4%B8%BA%E5%9B%BE%E6%84%9F%E7%9F%A5%EF%BC%8C%E6%94%AF%E6%8C%81%E5%8F%AF%E9%87%8D%E7%94%A8%E7%9A%84%E5%AD%90%E6%AD%A5%E9%AA%A4%E3%80%81%E6%94%B6%E6%95%9B%E7%9A%84%E4%BB%BB%E5%8A%A1%E8%B7%AF%E5%BE%84%E5%92%8C%E5%85%B1%E4%BA%AB%E4%BE%9D%E8%B5%96%E3%80%82%E8%BF%99%E4%B8%BA%E6%9C%AA%E6%9D%A5%E5%9F%BA%E4%BA%8EDAG%E7%9A%84%E5%86%85%E5%AD%98%E6%9E%B6%E6%9E%84%E5%A5%A0%E5%AE%9A%E4%BA%86%E5%9F%BA%E7%A1%80%E3%80%82">https://github.com/biubiutomato/TME-Agent找到，包括基本示例和结构化内存集成。虽然当前实现使用了树形结构，但TME被设计为图感知，支持可重用的子步骤、收敛的任务路径和共享依赖。这为未来基于DAG的内存架构奠定了基础。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08525v3">PDF</a> 14 pages, 5 figures. Preprint prepared for future submission.   Includes implementation and token-efficiency analysis. Code at   <a target="_blank" rel="noopener" href="https://github.com/biubiutomato/TME-Agent">https://github.com/biubiutomato/TME-Agent</a></p>
<p><strong>Summary</strong><br>大型语言模型（LLMs）被越来越多地用作多步骤任务的自主代理，但其现有框架大多缺乏任务状态的结构化理解，这导致了性能不稳定、经常幻想和长期连贯性差。为解决这一问题，我们提出了任务记忆引擎（TME）和层次化任务记忆树（TMT）。TME通过动态生成LLM提示来改善执行一致性，提高任务完成准确性并增强行为可解释性。相关实现细节可参考：<a target="_blank" rel="noopener" href="https://github.com/biubiutomato/TME-Agent">链接</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在多步骤任务中的自主代理应用面临挑战。</li>
<li>当前框架缺乏任务状态的结构化理解，导致性能不稳定和连贯性差。</li>
<li>提出任务记忆引擎（TME）和层次化任务记忆树（TMT）来解决这些问题。</li>
<li>TME通过动态生成LLM提示改善执行一致性。</li>
<li>TME提高了任务完成准确性和行为可解释性。</li>
<li>TME具有图形感知设计，支持可重复的子步骤、收敛任务路径和共享依赖，为未来基于DAG的内存架构奠定基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08525">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-833c71abfb41f6d58cf8b4414cff583e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56cfc4f925f501a97f8aa2f5539a97f4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="UI-R1-Enhancing-Action-Prediction-of-GUI-Agents-by-Reinforcement-Learning"><a href="#UI-R1-Enhancing-Action-Prediction-of-GUI-Agents-by-Reinforcement-Learning" class="headerlink" title="UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement   Learning"></a>UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement   Learning</h2><p><strong>Authors:Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, Hongsheng Li</strong></p>
<p>The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities in LLMs through reinforcement learning (RL) with rule-based rewards. Despite its success in language models, its application in multi-modal domains, particularly in graphic user interface (GUI) agent tasks, remains under-explored. To address this issue, we propose UI-R1, the first framework to explore how rule-based RL can enhance the reasoning capabilities of multimodal large language models (MLLMs) for GUI action prediction tasks. Specifically, UI-R1 introduces a novel rule-based action reward, enabling model optimization via policy-based algorithms such as Group Relative Policy Optimization (GRPO). For efficient training, we curate a small yet high-quality dataset of 136 challenging tasks, encompassing five common action types on mobile devices. Experimental results demonstrate that our proposed UI-R1-3B achieves significant improvements over the base model (i.e. Qwen2.5-VL-3B) on both in-domain (ID) and out-of-domain (OOD) tasks, with average accuracy gains of 22.1% on ScreenSpot, 6.0% on ScreenSpot-Pro, and 12.7% on ANDROIDCONTROL. Furthermore, UI-R1-3B delivers competitive performance compared to larger models (e.g., OS-Atlas-7B) trained via supervised fine-tuning (SFT) on 76K samples. These results underscore the potential of rule-based reinforcement learning to advance GUI understanding and control, paving the way for future research in this domain. Code website: <a target="_blank" rel="noopener" href="https://github.com/lll6gg/UI-R1">https://github.com/lll6gg/UI-R1</a>. </p>
<blockquote>
<p>最近，DeepSeek-R1展示了通过基于规则的奖励进行强化学习（RL）后，大型语言模型（LLM）中推理能力的出现。尽管它在语言模型方面取得了成功，但在多模式领域，特别是在图形用户界面（GUI）代理任务中的应用仍然未被充分探索。为了解决这个问题，我们提出了UI-R1，这是第一个探索基于规则的RL如何增强多模式大型语言模型（MLLM）的推理能力，以执行GUI动作预测任务的框架。具体来说，UI-R1引入了一种新型的基于动作的规则奖励，通过基于策略算法（如集团相对策略优化（GRPO））进行模型优化。为了进行有效的训练，我们创建了一个小型但高质量的数据集，包含136个具有挑战性的任务，涵盖移动设备上的五种常见动作类型。实验结果表明，我们提出的UI-R1-3B在域内（ID）和域外（OOD）任务上都较基础模型（即Qwen2.5-VL-3B）有显著改进，在ScreenSpot上的平均准确率提高了22.1%，在ScreenSpot-Pro上提高了6.0%，在ANDROIDCONTROL上提高了12.7%。此外，与在76K样本上通过监督微调（SFT）训练的更大模型（如OS-Atlas-7B）相比，UI-R1-3B表现出具有竞争力的性能。这些结果突显了基于规则的强化学习在GUI理解和控制方面的潜力，为未来的研究铺平了道路。代码网站：<a target="_blank" rel="noopener" href="https://github.com/lll6gg/UI-R1%E3%80%82">https://github.com/lll6gg/UI-R1。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21620v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了DeepSeek-R1展示了大语言模型通过强化学习（RL）出现推理能力的新兴趋势。为应对在图形用户界面（GUI）代理任务中多模态大型语言模型（MLLMs）的应用不足问题，首次提出UI-R1框架。通过引入基于规则的行动奖励，优化模型通过基于策略算法如群体相对策略优化（GRPO）。实验结果显示，相比基准模型，UI-R1-3B在域内（ID）和域外（OOD）任务上的表现均有显著提高，平均准确率提升显著。此外，UI-R1-3B在大型模型上的表现同样具有竞争力。这表明基于规则的强化学习在GUI理解和控制方面具有潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>DeepSeek-R1展示了大型语言模型通过强化学习展现的推理能力。</li>
<li>UI-R1框架旨在探索规则基础强化学习如何增强多模态大型语言模型在GUI行动预测任务中的推理能力。</li>
<li>UI-R1引入基于行动的规则奖励，通过策略型算法如GRPO进行模型优化。</li>
<li>实验结果显示，UI-R1-3B在多种任务上较基准模型表现显著提高，并表现出对大型模型的竞争力。</li>
<li>基于规则的强化学习在GUI理解和控制方面展现出潜力。</li>
<li>数据集包含136项具有挑战性的任务，涵盖移动设备上的五种常见动作类型。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21620">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-57572e7a13f08dc88cf8256224ce8c37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-759ab689c47377f1d5f7ec8d41fab7e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7888ec58d54be02625496730e97f7c0e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SpiritSight-Agent-Advanced-GUI-Agent-with-One-Look"><a href="#SpiritSight-Agent-Advanced-GUI-Agent-with-One-Look" class="headerlink" title="SpiritSight Agent: Advanced GUI Agent with One Look"></a>SpiritSight Agent: Advanced GUI Agent with One Look</h2><p><strong>Authors:Zhiyuan Huang, Ziming Cheng, Junting Pan, Zhaohui Hou, Mingjie Zhan</strong></p>
<p>Graphical User Interface (GUI) agents show amazing abilities in assisting human-computer interaction, automating human user’s navigation on digital devices. An ideal GUI agent is expected to achieve high accuracy, low latency, and compatibility for different GUI platforms. Recent vision-based approaches have shown promise by leveraging advanced Vision Language Models (VLMs). While they generally meet the requirements of compatibility and low latency, these vision-based GUI agents tend to have low accuracy due to their limitations in element grounding. To address this issue, we propose $\textbf{SpiritSight}$, a vision-based, end-to-end GUI agent that excels in GUI navigation tasks across various GUI platforms. First, we create a multi-level, large-scale, high-quality GUI dataset called $\textbf{GUI-Lasagne}$ using scalable methods, empowering SpiritSight with robust GUI understanding and grounding capabilities. Second, we introduce the $\textbf{Universal Block Parsing (UBP)}$ method to resolve the ambiguity problem in dynamic high-resolution of visual inputs, further enhancing SpiritSight’s ability to ground GUI objects. Through these efforts, SpiritSight agent outperforms other advanced methods on diverse GUI benchmarks, demonstrating its superior capability and compatibility in GUI navigation tasks. Models and datasets are available at <a target="_blank" rel="noopener" href="https://hzhiyuan.github.io/SpiritSight-Agent">https://hzhiyuan.github.io/SpiritSight-Agent</a>. </p>
<blockquote>
<p>图形用户界面（GUI）代理在辅助人机交互、自动化人类用户在数字设备上的导航方面表现出惊人的能力。理想的GUI代理应达到高准确性、低延迟和不同GUI平台的兼容性。最近的基于视觉的方法通过利用先进的视觉语言模型（VLMs）显示出潜力。虽然它们通常满足兼容性和低延迟的要求，但这些基于视觉的GUI代理往往由于元素定位的限制而准确性较低。为了解决这一问题，我们提出了基于视觉的端到端GUI代理——SpiritSight，它在各种GUI平台上的GUI导航任务中表现出色。首先，我们使用可扩展的方法创建了一个多层次、大规模、高质量的GUI数据集，名为GUI-Lasagne，为SpiritSight提供强大的GUI理解和定位能力。其次，我们引入了通用块解析（UBP）方法来解决动态高分辨率视觉输入的歧义问题，进一步增强了SpiritSight对GUI对象的定位能力。通过这些努力，SpiritSight代理在多种GUI基准测试上超越了其他先进方法，证明了其在GUI导航任务中的卓越能力和兼容性。模型和数据集可在<a target="_blank" rel="noopener" href="https://hzhiyuan.github.io/SpiritSight-Agent%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://hzhiyuan.github.io/SpiritSight-Agent上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03196v2">PDF</a> Paper accepted to CVPR 2025</p>
<p><strong>Summary</strong><br>     视觉界面的图形用户界面（GUI）代理在辅助人机交互、自动化用户导航方面展现出强大能力。为改进现有视觉基础GUI代理的精度问题，提出了一款名为SpiritSight的新型GUI代理，其借助大规模GUI数据集GUI-Lasagne与Universal Block Parsing（UBP）方法，提升了对GUI导航任务的掌握能力，展现出卓越的跨平台兼容性及高精度、低延迟特性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GUI代理在协助人机交互和自动化导航方面表现突出，需要满足高准确性、低延迟和不同GUI平台的兼容性。</li>
<li>现有视觉基础的GUI代理在元素定位方面存在局限性，导致准确性较低。</li>
<li>为解决这一问题，提出了SpiritSight代理，其在大规模GUI数据集GUI-Lasagne的支持下，具备强大的GUI理解和定位能力。</li>
<li>Universal Block Parsing（UBP）方法被引入以解决动态高分辨率视觉输入中的歧义问题，增强了SpiritSight对GUI对象的定位能力。</li>
<li>SpiritSight代理在多种GUI基准测试上表现出优异性能，展现出其高兼容性和强大能力。</li>
<li>SpiritSight代理和相关的数据集可通过<a target="_blank" rel="noopener" href="https://hzhiyuan.github.io/SpiritSight-Agent%E8%8E%B7%E5%8F%96%E3%80%82">https://hzhiyuan.github.io/SpiritSight-Agent获取。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03196">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8054c49390024f9a1748660c63e9fa73.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d18090c2f4997021579860d3403bf868.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a91a3882533730dcae4a8638c1d08cd7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f68458d56c3be91a619301734787c3b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b82e0eded9c8aab04cb39465895301b2.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="TradingAgents-Multi-Agents-LLM-Financial-Trading-Framework"><a href="#TradingAgents-Multi-Agents-LLM-Financial-Trading-Framework" class="headerlink" title="TradingAgents: Multi-Agents LLM Financial Trading Framework"></a>TradingAgents: Multi-Agents LLM Financial Trading Framework</h2><p><strong>Authors:Yijia Xiao, Edward Sun, Di Luo, Wei Wang</strong></p>
<p>Significant progress has been made in automated problem-solving using societies of agents powered by large language models (LLMs). In finance, efforts have largely focused on single-agent systems handling specific tasks or multi-agent frameworks independently gathering data. However, multi-agent systems’ potential to replicate real-world trading firms’ collaborative dynamics remains underexplored. TradingAgents proposes a novel stock trading framework inspired by trading firms, featuring LLM-powered agents in specialized roles such as fundamental analysts, sentiment analysts, technical analysts, and traders with varied risk profiles. The framework includes Bull and Bear researcher agents assessing market conditions, a risk management team monitoring exposure, and traders synthesizing insights from debates and historical data to make informed decisions. By simulating a dynamic, collaborative trading environment, this framework aims to improve trading performance. Detailed architecture and extensive experiments reveal its superiority over baseline models, with notable improvements in cumulative returns, Sharpe ratio, and maximum drawdown, highlighting the potential of multi-agent LLM frameworks in financial trading. TradingAgents is available at <a target="_blank" rel="noopener" href="https://github.com/TauricResearch">https://github.com/TauricResearch</a>. </p>
<blockquote>
<p>在利用大型语言模型（LLM）驱动的智能体社会解决自动化问题方面取得了重大进展。在金融领域，相关努力主要集中在处理特定任务的单一智能体系统或独立收集数据的多智能体框架上。然而，多智能体系统在复制现实世界交易公司的协作动态方面的潜力尚未得到充分探索。《TradingAgents》提出了一种受交易公司启发的新型股票交易框架，该框架具有专门的角色智能体，例如基础分析师、情绪分析师、技术分析师和不同风险状况的交易员等。该框架包括评估市场条件的牛市和熊市研究者智能体、监控敞口的风险管理团队以及从辩论和历史数据中提炼见解以做出明智决策的交易员。通过模拟动态协作的交易环境，该框架旨在提高交易性能。详细的架构和广泛的实验表明其在累计回报、夏普比率和最大回撤方面优于基准模型，突显了多智能体LLM框架在金融交易中的潜力。《TradingAgents》可通过 <a target="_blank" rel="noopener" href="https://github.com/TauricResearch">https://github.com/TauricResearch</a> 获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20138v6">PDF</a> Oral, Multi-Agent AI in the Real World @ AAAI 2025</p>
<p><strong>总结</strong></p>
<p>基于大型语言模型（LLM）的代理社会在自动化问题解决方面取得了显著进展。在金融领域，尽管单一代理系统处理特定任务或多代理框架独立收集数据的工作已经备受关注，但多代理系统在模拟现实世界交易公司协作动态方面的潜力仍被低估。TradingAgents提出一个受交易公司启发的股票交易新框架，该框架采用LLM驱动的多代理系统，包括基础分析师、情绪分析师、技术分析师和具有不同风险特征的交易员等专业化角色。框架包括评估市场状况的Bull和Bear研究员代理、监控风险的风险管理团队以及合成辩论和历史数据以做出明智决策的交易员。通过模拟动态协作的交易环境，该框架旨在提高交易性能。详细的架构和广泛的实验表明，其在累积回报、夏普比率和最大回撤方面优于基准模型，突显了多代理LLM框架在金融交易中的潜力。TradingAgents平台可通过<a target="_blank" rel="noopener" href="https://github.com/TauricResearch">https://github.com/TauricResearch</a>访问。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型（LLM）驱动的代理社会在自动化问题解决方面取得显著进步。</li>
<li>金融领域主要聚焦于单一代理系统处理特定任务或多代理框架独立收集数据的应用场景。</li>
<li>多代理系统在模拟现实世界交易公司的协作动态方面具有潜力。</li>
<li>TradingAgents提出一种模拟交易公司的新型股票交易框架，采用多代理系统处理多种金融角色。</li>
<li>该框架包含市场状况评估、风险管理以及基于辩论和历史数据的决策制定等要素。</li>
<li>通过模拟动态协作的交易环境，该框架可提高交易性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20138">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4f5d35f3ebfc95f19bf9db6c88bf5c21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b30fff5ccc871c8eb463e0482df36120.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25633305684d8ad1c87d09dcdbe8556b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d197b1f9955d8a4274d4868459c4e223.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eba4b8a51ddba76a7d9ed986fefccd57.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Enhancing-LLMs-for-Power-System-Simulations-A-Feedback-driven-Multi-agent-Framework"><a href="#Enhancing-LLMs-for-Power-System-Simulations-A-Feedback-driven-Multi-agent-Framework" class="headerlink" title="Enhancing LLMs for Power System Simulations: A Feedback-driven   Multi-agent Framework"></a>Enhancing LLMs for Power System Simulations: A Feedback-driven   Multi-agent Framework</h2><p><strong>Authors:Mengshuo Jia, Zeyu Cui, Gabriela Hug</strong></p>
<p>The integration of experimental technologies with large language models (LLMs) is transforming scientific research. It positions AI as a versatile research assistant rather than a mere problem-solving tool. In the field of power systems, however, managing simulations – one of the essential experimental technologies – remains a challenge for LLMs due to their limited domain-specific knowledge, restricted reasoning capabilities, and imprecise handling of simulation parameters. To address these limitations, this paper proposes a feedback-driven, multi-agent framework. It incorporates three proposed modules: an enhanced retrieval-augmented generation (RAG) module, an improved reasoning module, and a dynamic environmental acting module with an error-feedback mechanism. Validated on 69 diverse tasks from Daline and MATPOWER, this framework achieves success rates of 93.13% and 96.85%, respectively. It significantly outperforms ChatGPT 4o, o1-preview, and the fine-tuned GPT-4o, which all achieved a success rate lower than 30% on complex tasks. Additionally, the proposed framework also supports rapid, cost-effective task execution, completing each simulation in approximately 30 seconds at an average cost of 0.014 USD for tokens. Overall, this adaptable framework lays a foundation for developing intelligent LLM-based assistants for human researchers, facilitating power system research and beyond. </p>
<blockquote>
<p>将实验技术与大型语言模型（LLM）的结合正在改变科学研究的方式。它将人工智能定位为多才多艺的研究助手，而不仅仅是解决问题的工具。然而，在电力系统领域，管理模拟（一种重要的实验技术）对LLM来说仍然是一个挑战，因为它们有限的特定领域知识、有限的推理能力和对模拟参数的不精确处理。为了克服这些局限性，本文提出了一种反馈驱动的多智能体框架。它结合了三个提出的模块：一个增强的检索增强生成（RAG）模块，一个改进的推理模块，以及一个带有错误反馈机制的动力环境行为模块。在Daline和MATPOWER的69个不同任务上进行验证，该框架的成功率分别为93.13%和96.85%。它显著优于ChatGPT 4o、o1预览和经过微调后的GPT-4o，这些模型在复杂任务上的成功率都低于30%。此外，该框架还支持快速、经济的任务执行，平均每个模拟大约需要30秒完成，令牌平均成本为0.014美元。总的来说，这个灵活多变的框架为开发基于LLM的智能助手为研究人员奠定了基础，促进了电力系统研究及其他领域的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16707v2">PDF</a> 15 pages</p>
<p><strong>Summary</strong><br>     实验技术与大型语言模型（LLM）的融合正在推动科学研究的发展。人工智能已成为多才多艺的研究助手，而不仅仅是解决问题的工具。在电力系统领域，管理模拟作为重要的实验技术之一仍然是LLM的一个挑战。针对这些挑战，本文提出了一种反馈驱动的多智能体框架，包括增强检索增强生成模块、改进推理模块和具有错误反馈机制动态环境行为模块。该框架在Daline和MATPOWER的69项不同任务上取得了高达93.13%和96.85%的成功率，显著优于ChatGPT 4o、o1预览和微调后的GPT-4o。此外，该框架还支持快速、经济的任务执行，每个模拟任务大约需要30秒，平均每个令牌成本为0.014美元。总体而言，这一灵活框架为开发基于LLM的智能助理研究员奠定了基础，推动了电力系统研究及其他领域的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>实验技术与LLM的融合推动科学研究发展。</li>
<li>LLM在电力系统模拟管理上仍面临挑战。</li>
<li>提出了一种反馈驱动的多智能体框架来解决这些挑战。</li>
<li>该框架包括增强检索增强生成模块、改进推理模块和动态环境行为模块。</li>
<li>该框架在多种任务上取得了较高的成功率，显著优于其他模型。</li>
<li>框架支持快速、经济的任务执行。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16707">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-aa5103527c439d5a335d11f1d69cdd17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5848cc20fa629353c26e9046ac47cda3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-215d480ff020ceb604d7a66b7a664caa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b917352c95393008711c226e6b2853e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e7716039a29c2be1fd1c21e05bf1afa.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Agent-Security-Bench-ASB-Formalizing-and-Benchmarking-Attacks-and-Defenses-in-LLM-based-Agents"><a href="#Agent-Security-Bench-ASB-Formalizing-and-Benchmarking-Attacks-and-Defenses-in-LLM-based-Agents" class="headerlink" title="Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and   Defenses in LLM-based Agents"></a>Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and   Defenses in LLM-based Agents</h2><p><strong>Authors:Hanrong Zhang, Jingyuan Huang, Kai Mei, Yifei Yao, Zhenting Wang, Chenlu Zhan, Hongwei Wang, Yongfeng Zhang</strong></p>
<p>Although LLM-based agents, powered by Large Language Models (LLMs), can use external tools and memory mechanisms to solve complex real-world tasks, they may also introduce critical security vulnerabilities. However, the existing literature does not comprehensively evaluate attacks and defenses against LLM-based agents. To address this, we introduce Agent Security Bench (ASB), a comprehensive framework designed to formalize, benchmark, and evaluate the attacks and defenses of LLM-based agents, including 10 scenarios (e.g., e-commerce, autonomous driving, finance), 10 agents targeting the scenarios, over 400 tools, 27 different types of attack&#x2F;defense methods, and 7 evaluation metrics. Based on ASB, we benchmark 10 prompt injection attacks, a memory poisoning attack, a novel Plan-of-Thought backdoor attack, 4 mixed attacks, and 11 corresponding defenses across 13 LLM backbones. Our benchmark results reveal critical vulnerabilities in different stages of agent operation, including system prompt, user prompt handling, tool usage, and memory retrieval, with the highest average attack success rate of 84.30%, but limited effectiveness shown in current defenses, unveiling important works to be done in terms of agent security for the community. We also introduce a new metric to evaluate the agents’ capability to balance utility and security. Our code can be found at <a target="_blank" rel="noopener" href="https://github.com/agiresearch/ASB">https://github.com/agiresearch/ASB</a>. </p>
<blockquote>
<p>虽然基于大型语言模型（LLM）的代理可以使用外部工具和记忆机制来解决复杂的现实世界任务，但它们也可能引入关键的安全漏洞。然而，现有文献并没有全面评估针对基于LLM的代理的攻击和防御措施。为了解决这一问题，我们引入了Agent Security Bench（ASB），这是一个旨在规范、基准测试和评估基于LLM的代理的攻击和防御的综合框架，包括10个场景（例如电子商务、自动驾驶、金融）、针对这些场景的10个代理、超过400种工具、27种不同类型的攻击&#x2F;防御方法以及7种评估指标。基于ASB，我们对10种提示注入攻击、内存中毒攻击、新型计划思维后门攻击、4种混合攻击以及针对13种LLM骨干网的11种相应防御措施进行了基准测试。我们的基准测试结果揭示了代理操作不同阶段的关键漏洞，包括系统提示、用户提示处理、工具使用和记忆检索，平均攻击成功率最高达84.30%，但当前防御措施的有效性有限，这揭示了社区在代理安全方面还有重要工作要做。我们还引入了一个新的指标来评估代理在实用性和安全性之间的平衡能力。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/agiresearch/ASB%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/agiresearch/ASB找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02644v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的代理虽然可以利用外部工具和记忆机制来解决复杂的现实世界任务，但它们也可能引入关键的安全漏洞。为解决这一问题，我们提出了Agent Security Bench（ASB）框架，用于对LLM代理的攻击和防御进行全面、基准测试和评估。该框架包括多种场景、代理、工具、攻击&#x2F;防御方法和评估指标。我们的基准测试结果显示，代理操作的不同阶段存在关键漏洞，而现有防御手段的有效性有限。我们还引入了一个新指标来评估代理在效用和安全方面的平衡能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based agents虽可运用外部工具和记忆机制处理复杂任务，但存在关键安全漏洞。</li>
<li>Agent Security Bench (ASB)框架用于全面、基准测试LLM代理的攻击和防御。</li>
<li>ASB框架包含多种场景、代理、工具和方法，提供全面的评估指标。</li>
<li>基准测试揭示代理操作不同阶段（如系统提示、用户提示处理、工具使用和记忆检索）存在漏洞。</li>
<li>攻击成功率高达84.30%，而现有防御手段效果有限。</li>
<li>引入新指标评估代理在效用和安全方面的平衡能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02644">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-248047d61a373211dd2e8c2b184c89cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93471f2f5bc0057346af104cbf07b944.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6afad9b83c8d71ae6838eef48af202d.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="COMBO-Compositional-World-Models-for-Embodied-Multi-Agent-Cooperation"><a href="#COMBO-Compositional-World-Models-for-Embodied-Multi-Agent-Cooperation" class="headerlink" title="COMBO: Compositional World Models for Embodied Multi-Agent Cooperation"></a>COMBO: Compositional World Models for Embodied Multi-Agent Cooperation</h2><p><strong>Authors:Hongxin Zhang, Zeyuan Wang, Qiushi Lyu, Zheyuan Zhang, Sunli Chen, Tianmin Shu, Behzad Dariush, Kwonjoon Lee, Yilun Du, Chuang Gan</strong></p>
<p>In this paper, we investigate the problem of embodied multi-agent cooperation, where decentralized agents must cooperate given only egocentric views of the world. To effectively plan in this setting, in contrast to learning world dynamics in a single-agent scenario, we must simulate world dynamics conditioned on an arbitrary number of agents’ actions given only partial egocentric visual observations of the world. To address this issue of partial observability, we first train generative models to estimate the overall world state given partial egocentric observations. To enable accurate simulation of multiple sets of actions on this world state, we then propose to learn a compositional world model for multi-agent cooperation by factorizing the naturally composable joint actions of multiple agents and compositionally generating the video conditioned on the world state. By leveraging this compositional world model, in combination with Vision Language Models to infer the actions of other agents, we can use a tree search procedure to integrate these modules and facilitate online cooperative planning. We evaluate our methods on three challenging benchmarks with 2-4 agents. The results show our compositional world model is effective and the framework enables the embodied agents to cooperate efficiently with different agents across various tasks and an arbitrary number of agents, showing the promising future of our proposed methods. More videos can be found at <a target="_blank" rel="noopener" href="https://umass-embodied-agi.github.io/COMBO/">https://umass-embodied-agi.github.io/COMBO/</a>. </p>
<blockquote>
<p>本文研究了具身多智能体合作问题，即分散的智能体仅凭以自我为中心的世界视图进行协作。为了在这种环境下进行有效的规划，与在单智能体场景中学习世界动态不同，我们必须模拟仅根据世界部分以自我为中心的视觉观察下任意数量的智能体的行动的世界动态。为了解决部分可观察性的问题，我们首先训练生成模型以根据部分以自我为中心的观察来估计整体世界状态。为了能够对这个世界状态上的多组行动进行准确的模拟，然后我们提出了通过分解多个智能体的自然可组合联合行动来学习用于多智能体合作的可组合世界模型，并根据世界状态组合生成视频。通过利用这种可组合的世界模型，结合视觉语言模型来推断其他智能体的行动，我们可以使用树搜索程序来整合这些模块，促进在线协作规划。我们在具有2-4个智能体的三个具有挑战性的基准测试上评估了我们的方法。结果表明，我们的可组合世界模型是有效的，该框架使具身智能体能够高效地与各种任务中的不同智能体以及任意数量的智能体进行协作，展示了我们提出方法的未来前景。更多视频可在<a target="_blank" rel="noopener" href="https://umass-embodied-agi.github.io/COMBO/%E6%89%BE%E5%88%B0%E3%80%82">https://umass-embodied-agi.github.io/COMBO/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.10775v3">PDF</a> Published at ICLR 2025. 24 pages. The first three authors contributed   equally</p>
<p><strong>Summary</strong></p>
<p>本文研究了多智能体合作的难题，其中分散的代理只能根据以自我为中心的视角观察世界来进行合作。为了在这种环境中进行有效的规划，必须模拟世界动态，以适应任意数量的代理行动，同时仅根据部分以自我为中心的观察结果。为解决部分可观察性问题，首先训练生成模型以根据部分自我观察结果估计整体世界状态。然后，为了准确模拟在此世界状态下多组行动的模拟，提出通过分解多个代理的自然可组合联合行动并组合生成视频来学习用于多智能体合作的组合世界模型。通过利用组合世界模型和视觉语言模型来推断其他代理的行动，可以使用树搜索程序来整合这些模块并促进在线合作规划。在具有2-4个代理的三个具有挑战性的基准测试上评估了我们的方法，证明了组合世界模型的有效性，并且该方法使智能体能够高效地在不同任务和任意数量的智能体之间进行合作。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文研究了多智能体合作问题，其中智能体仅通过自我中心的视角观察世界进行合作。</li>
<li>为解决部分可观察性问题，训练生成模型以根据部分自我观察结果估计整体世界状态。</li>
<li>提出学习组合世界模型，以模拟多个智能体的行动对世界状态的影响。</li>
<li>利用组合世界模型和视觉语言模型进行在线合作规划。</li>
<li>方法在具有挑战性的基准测试上表现良好，适用于不同任务和任意数量的智能体。</li>
<li>方法展示了在推动智能体合作方面的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.10775">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ead0585a3037ef33f4905c20ab146c44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-036994c569d13aa0a7acc4ddc460b13c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9510b382527bf30d99cbc4256ed4637d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d84d47b7f2015bfbb48f2575678ef371.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-18/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-18/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-18/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-75e61f7614c517d6ec749884c2652ad3.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-04-18  Logits DeConfusion with CLIP for Few-Shot Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-18/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f50f71bd991c3c5db24d14612a62d5ec.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-04-18  HLS-Eval A Benchmark and Framework for Evaluating LLMs on High-Level   Synthesis Design Tasks
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18293.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
