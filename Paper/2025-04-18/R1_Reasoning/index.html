<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-18  HLS-Eval A Benchmark and Framework for Evaluating LLMs on High-Level   Synthesis Design Tasks">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-29400da9d1b9a3b32b8cad70c2e64acb.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-18
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-18-æ›´æ–°"><a href="#2025-04-18-æ›´æ–°" class="headerlink" title="2025-04-18 æ›´æ–°"></a>2025-04-18 æ›´æ–°</h1><h2 id="HLS-Eval-A-Benchmark-and-Framework-for-Evaluating-LLMs-on-High-Level-Synthesis-Design-Tasks"><a href="#HLS-Eval-A-Benchmark-and-Framework-for-Evaluating-LLMs-on-High-Level-Synthesis-Design-Tasks" class="headerlink" title="HLS-Eval: A Benchmark and Framework for Evaluating LLMs on High-Level   Synthesis Design Tasks"></a>HLS-Eval: A Benchmark and Framework for Evaluating LLMs on High-Level   Synthesis Design Tasks</h2><p><strong>Authors:Stefan Abi-Karam, Cong Hao</strong></p>
<p>The rapid scaling of large language model (LLM) training and inference has driven their adoption in semiconductor design across academia and industry. While most prior work evaluates LLMs on hardware description language (HDL) tasks, particularly Verilog, designers are increasingly using high-level synthesis (HLS) to build domain-specific accelerators and complex hardware systems. However, benchmarks and tooling to comprehensively evaluate LLMs for HLS design tasks remain scarce.   To address this, we introduce HLS-Eval, the first complete benchmark and evaluation framework for LLM-driven HLS design. HLS-Eval targets two core tasks: (1) generating HLS code from natural language descriptions, and (2) performing HLS-specific code edits to optimize performance and hardware efficiency. The benchmark includes 94 unique designs drawn from standard HLS benchmarks and novel sources. Each case is prepared via a semi-automated flow that produces a natural language description and a paired testbench for C-simulation and synthesis validation, ensuring each task is â€œLLM-ready.â€   Beyond the benchmark, HLS-Eval offers a modular Python framework for automated, parallel evaluation of both local and hosted LLMs. It includes a parallel evaluation engine, direct HLS tool integration, and abstractions for to support different LLM interaction paradigms, enabling rapid prototyping of new benchmarks, tasks, and LLM methods.   We demonstrate HLS-Eval through baseline evaluations of open-source LLMs on Vitis HLS, measuring outputs across four key metrics - parseability, compilability, runnability, and synthesizability - reflecting the iterative HLS design cycle. We also report pass@k metrics, establishing clear baselines and reusable infrastructure for the broader LLM-for-hardware community.   All benchmarks, framework code, and results are open-sourced at <a target="_blank" rel="noopener" href="https://github.com/stefanpie/hls-eval">https://github.com/stefanpie/hls-eval</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒå’Œæ¨ç†çš„è¿…é€Ÿæ‰©å±•ï¼Œæ¨åŠ¨äº†å…¶åœ¨å­¦æœ¯ç•Œå’Œå·¥ä¸šç•ŒåŠå¯¼ä½“è®¾è®¡ä¸­çš„åº”ç”¨ã€‚è™½ç„¶å¤§å¤šæ•°å…ˆå‰çš„å·¥ä½œéƒ½æ˜¯å¯¹ç¡¬ä»¶æè¿°è¯­è¨€ï¼ˆHDLï¼‰ä»»åŠ¡è¿›è¡Œè¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹Verilogè¯­è¨€ï¼Œä½†è®¾è®¡å¸ˆä»¬æ­£è¶Šæ¥è¶Šå¤šåœ°ä½¿ç”¨é«˜çº§ç»¼åˆï¼ˆHLSï¼‰æ¥æ„å»ºç‰¹å®šé¢†åŸŸçš„åŠ é€Ÿå™¨å’Œå¤æ‚çš„ç¡¬ä»¶ç³»ç»Ÿã€‚ç„¶è€Œï¼Œé’ˆå¯¹HLSè®¾è®¡ä»»åŠ¡å…¨é¢è¯„ä¼°LLMçš„åŸºå‡†æµ‹è¯•å’Œå·¥å…·ä»ç„¶ç¨€ç¼ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†HLS-Evalï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹LLMé©±åŠ¨çš„HLSè®¾è®¡çš„å®Œæ•´åŸºå‡†æµ‹è¯•å’Œè¯„ä»·æ¡†æ¶ã€‚HLS-Evalé’ˆå¯¹ä¸¤ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼š1ï¼‰æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°ç”ŸæˆHLSä»£ç ï¼›2ï¼‰æ‰§è¡ŒHLSç‰¹å®šçš„ä»£ç ç¼–è¾‘ï¼Œä»¥ä¼˜åŒ–æ€§èƒ½å’Œç¡¬ä»¶æ•ˆç‡ã€‚åŸºå‡†æµ‹è¯•åŒ…æ‹¬æ¥è‡ªæ ‡å‡†HLSåŸºå‡†æµ‹è¯•å’Œæ–°é¢–æ¥æºçš„94ä¸ªç‹¬ç‰¹è®¾è®¡ã€‚æ¯ä¸ªæ¡ˆä¾‹éƒ½é€šè¿‡åŠè‡ªåŠ¨åŒ–æµç¨‹è¿›è¡Œå‡†å¤‡ï¼Œè¯¥æµç¨‹äº§ç”Ÿè‡ªç„¶è¯­è¨€æè¿°å’Œé…å¥—æµ‹è¯•å¹³å°ï¼Œç”¨äºCä»¿çœŸå’Œç»¼åˆéªŒè¯ï¼Œç¡®ä¿æ¯ä¸ªä»»åŠ¡éƒ½é€‚åˆLLMã€‚é™¤äº†åŸºå‡†æµ‹è¯•å¤–ï¼ŒHLS-Evalè¿˜æä¾›æ¨¡å—åŒ–Pythonæ¡†æ¶ï¼Œç”¨äºæœ¬åœ°å’Œæ‰˜ç®¡LLMçš„è‡ªåŠ¨åŒ–å¹¶è¡Œè¯„ä¼°ã€‚å®ƒåŒ…æ‹¬å¹¶è¡Œè¯„ä¼°å¼•æ“ã€ç›´æ¥HLSå·¥å…·é›†æˆï¼Œä»¥åŠæ”¯æŒä¸åŒLLMäº¤äº’èŒƒå¼çš„æŠ½è±¡ï¼Œä»è€Œèƒ½å¤Ÿè¿…é€Ÿå¯¹æ–°çš„åŸºå‡†æµ‹è¯•ã€ä»»åŠ¡ã€LLMæ–¹æ³•è¿›è¡ŒåŸå‹è®¾è®¡ã€‚æˆ‘ä»¬é€šè¿‡Vitis HLSçš„å¼€æºLLMåŸºçº¿è¯„ä¼°æ¥å±•ç¤ºHLS-Evalï¼Œé€šè¿‡è¡¡é‡å››ä¸ªå…³é”®æŒ‡æ ‡ï¼ˆå¯è§£ææ€§ã€å¯ç¼–è¯‘æ€§ã€å¯è¿è¡Œæ€§å’Œå¯åˆæˆæ€§ï¼‰æ¥åæ˜ è¿­ä»£HLSè®¾è®¡å‘¨æœŸã€‚æˆ‘ä»¬è¿˜æŠ¥å‘Šäº†pass@kæŒ‡æ ‡ï¼Œä¸ºæ›´å¹¿æ³›çš„LLMç¡¬ä»¶ç¤¾åŒºå»ºç«‹äº†æ˜ç¡®çš„åŸºå‡†å’Œå¯é‡å¤ä½¿ç”¨çš„æ¶æ„ã€‚æ‰€æœ‰åŸºå‡†æµ‹è¯•ã€æ¡†æ¶ä»£ç å’Œç»“æœå‡å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/stefanpie/hls-eval%E4%B8%8A%E5%BC%80%E6%BA%90%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/stefanpie/hls-evalä¸Šå¼€æºå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12268v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŠå¯¼ä½“è®¾è®¡é¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ã€‚å°½ç®¡å¤§å¤šæ•°æ—©æœŸçš„ç ”ç©¶å…³æ³¨LLMåœ¨ç¡¬ä»¶æè¿°è¯­è¨€ï¼ˆHDLï¼‰ä»»åŠ¡ä¸Šçš„è¯„ä¼°ï¼Œä½†è®¾è®¡å¸ˆæ­£è¶Šæ¥è¶Šå¤šåœ°ä½¿ç”¨é«˜çº§ç»¼åˆï¼ˆHLSï¼‰æ¥æ„å»ºç‰¹å®šé¢†åŸŸçš„åŠ é€Ÿå™¨å’Œå¤æ‚çš„ç¡¬ä»¶ç³»ç»Ÿã€‚ç„¶è€Œï¼Œé’ˆå¯¹HLSè®¾è®¡ä»»åŠ¡çš„LLMè¯„ä¼°åŸºå‡†å’Œå·¥å…·ä»ç„¶ç¨€ç¼ºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ–‡ç« å¼•å…¥äº†HLS-Evalï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹LLMé©±åŠ¨çš„HLSè®¾è®¡çš„å®Œæ•´åŸºå‡†å’Œè¯„ä¼°æ¡†æ¶ã€‚HLS-Evalé’ˆå¯¹ä¸¤ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼šä»è‡ªç„¶è¯­è¨€æè¿°ç”ŸæˆHLSä»£ç ä»¥åŠå¯¹HLSä»£ç è¿›è¡Œä¼˜åŒ–ä»¥æé«˜æ€§èƒ½å’Œç¡¬ä»¶æ•ˆç‡ã€‚è¯¥åŸºå‡†åŒ…æ‹¬æ¥è‡ªæ ‡å‡†HLSåŸºå‡†å’Œæ–°é¢–æ¥æºçš„94ä¸ªç‹¬ç‰¹è®¾è®¡ã€‚æ¯ä¸ªæ¡ˆä¾‹éƒ½é€šè¿‡åŠè‡ªåŠ¨åŒ–æµç¨‹è¿›è¡Œå‡†å¤‡ï¼Œäº§ç”Ÿè‡ªç„¶è¯­è¨€æè¿°å’Œé…å¯¹æµ‹è¯•å¹³å°ç”¨äºCæ¨¡æ‹Ÿå’Œç»¼åˆéªŒè¯ï¼Œç¡®ä¿æ¯ä¸ªä»»åŠ¡éƒ½æ˜¯â€œLLMå°±ç»ªâ€ã€‚é™¤äº†åŸºå‡†æµ‹è¯•å¤–ï¼ŒHLS-Evalè¿˜æä¾›æ¨¡å—åŒ–Pythonæ¡†æ¶ï¼Œç”¨äºæœ¬åœ°å’Œæ‰˜ç®¡LLMçš„è‡ªåŠ¨åŒ–å¹¶è¡Œè¯„ä¼°ã€‚å®ƒåŒ…å«å¹¶è¡Œè¯„ä¼°å¼•æ“ã€ç›´æ¥HLSå·¥å…·é›†æˆä»¥åŠæ”¯æŒä¸åŒLLMäº¤äº’èŒƒå¼çš„æŠ½è±¡æ¦‚å¿µï¼Œå¯ä»¥åŠ å¿«æ–°åŸºå‡†æµ‹è¯•ã€ä»»åŠ¡å’ŒLLMæ–¹æ³•çš„å¿«é€ŸåŸå‹è®¾è®¡ã€‚é€šè¿‡åŸºçº¿è¯„ä¼°è¯æ˜äº†HLS-Evalçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŠå¯¼ä½“è®¾è®¡é¢†åŸŸçš„åº”ç”¨æ­£åœ¨å¢åŠ ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜çº§ç»¼åˆï¼ˆHLSï¼‰æ–¹é¢ã€‚</li>
<li>ç›®å‰é’ˆå¯¹LLMåœ¨HLSè®¾è®¡ä»»åŠ¡ä¸Šçš„è¯„ä¼°åŸºå‡†å’Œå·¥å…·ä»ç„¶ä¸è¶³ã€‚</li>
<li>HLS-Evalæ˜¯é¦–ä¸ªé’ˆå¯¹LLMé©±åŠ¨çš„HLSè®¾è®¡çš„å®Œæ•´åŸºå‡†å’Œè¯„ä¼°æ¡†æ¶ã€‚</li>
<li>HLS-EvalåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼šä»è‡ªç„¶è¯­è¨€æè¿°ç”ŸæˆHLSä»£ç åŠä¼˜åŒ–HLSä»£ç ä»¥æé«˜æ€§èƒ½å’Œç¡¬ä»¶æ•ˆç‡ã€‚</li>
<li>HLS-Evalæä¾›äº†æ¨¡å—åŒ–Pythonæ¡†æ¶ï¼Œæ”¯æŒè‡ªåŠ¨åŒ–å¹¶è¡Œè¯„ä¼°ï¼Œå¹¶å¯ç›´æ¥é›†æˆHLSå·¥å…·å’Œä¸åŒLLMäº¤äº’èŒƒå¼ã€‚</li>
<li>HLS-Evalé€šè¿‡åŸºçº¿è¯„ä¼°è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶å…¬å¼€äº†æ‰€æœ‰åŸºå‡†æµ‹è¯•ã€æ¡†æ¶ä»£ç å’Œç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12268">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f5ba7c1033c26a944a745feb1d0249ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-477cde549435bce448e93ca0606104c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b17f4c078efb2d9d2582e847b4753017.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ec66c86f1957cfead95a2afe7c1b872.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27a117b0baf62aaae40aca4a71d62f25.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FLIP-Reasoning-Challenge"><a href="#FLIP-Reasoning-Challenge" class="headerlink" title="FLIP Reasoning Challenge"></a>FLIP Reasoning Challenge</h2><p><strong>Authors:Andreas Plesner, Turlan Kuzhagaliyev, Roger Wattenhofer</strong></p>
<p>Over the past years, advances in artificial intelligence (AI) have demonstrated how AI can solve many perception and generation tasks, such as image classification and text writing, yet reasoning remains a challenge. This paper introduces the FLIP dataset, a benchmark for evaluating AI reasoning capabilities based on human verification tasks on the Idena blockchain. FLIP challenges present users with two orderings of 4 images, requiring them to identify the logically coherent one. By emphasizing sequential reasoning, visual storytelling, and common sense, FLIP provides a unique testbed for multimodal AI systems. Our experiments evaluate state-of-the-art models, leveraging both vision-language models (VLMs) and large language models (LLMs). Results reveal that even the best open-sourced and closed-sourced models achieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot settings, compared to human performance of 95.3%. Captioning models aid reasoning models by providing text descriptions of images, yielding better results than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5 Pro. Combining the predictions from 15 models in an ensemble increases the accuracy to 85.2%. These findings highlight the limitations of existing reasoning models and the need for robust multimodal benchmarks like FLIP. The full codebase and dataset will be available at <a target="_blank" rel="noopener" href="https://github.com/aplesner/FLIP-Reasoning-Challenge">https://github.com/aplesner/FLIP-Reasoning-Challenge</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„è¿›æ­¥å·²ç»è¯æ˜AIå¦‚ä½•è§£å†³è®¸å¤šæ„ŸçŸ¥å’Œç”Ÿæˆä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒåˆ†ç±»å’Œæ–‡æœ¬å†™ä½œï¼Œä½†æ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†FLIPæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºäººç±»éªŒè¯ä»»åŠ¡åœ¨IdenaåŒºå—é“¾ä¸Šè¯„ä¼°AIæ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚FLIPæŒ‘æˆ˜è¦æ±‚ç”¨æˆ·è¯†åˆ«ä¸¤ä¸ªé¡ºåºæ’åˆ—çš„4å¼ å›¾åƒä¸­çš„é€»è¾‘è¿è´¯æ€§ã€‚é€šè¿‡å¼ºè°ƒé¡ºåºæ¨ç†ã€è§†è§‰å™äº‹å’Œå¸¸è¯†ï¼ŒFLIPä¸ºè·¨æ¨¡æ€AIç³»ç»Ÿæä¾›äº†ç‹¬ç‰¹çš„æµ‹è¯•å¹³å°ã€‚æˆ‘ä»¬çš„å®éªŒè¯„ä¼°äº†æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ï¼Œå³ä½¿æœ€å¥½çš„å¼€æºå’Œé—­æºæ¨¡å‹ä¹Ÿåªèƒ½è¾¾åˆ°æœ€é«˜å‡†ç¡®ç‡åˆ†åˆ«ä¸º75.5%å’Œ77.9%ï¼Œè€Œäººç±»çš„è¡¨ç°åŠ›ä¸º95.3%ã€‚é€šè¿‡æä¾›å›¾åƒçš„æ–‡å­—æè¿°ï¼Œæè¿°æ¨¡å‹æœ‰åŠ©äºæ¨ç†æ¨¡å‹ï¼Œå…¶æ•ˆæœä¼˜äºç›´æ¥ä½¿ç”¨åŸå§‹å›¾åƒï¼ŒGemini 1.5 Proçš„å‡†ç¡®ç‡ä»69.6%æé«˜åˆ°75.2%ã€‚é€šè¿‡å°†æ¥è‡ªåäº”ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“åˆåˆ°ä¸€ä¸ªç»„åˆä¸­ï¼Œå¯ä»¥å°†å‡†ç¡®æ€§æé«˜åˆ°85.2%ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†ç°æœ‰æ¨ç†æ¨¡å‹çš„å±€é™æ€§ä»¥åŠéœ€è¦åƒFLIPè¿™æ ·çš„ç¨³å¥çš„è·¨æ¨¡æ€åŸºå‡†æµ‹è¯•é›†ã€‚å®Œæ•´çš„ä»£ç åº“å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/aplesner/FLIP-Reasoning-Challenge%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/aplesner/FLIP-Reasoning-Challengeä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12256v1">PDF</a> Published at First Workshop on Open Science for Foundation Models at   ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºFLIPçš„æ–°æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ—¨åœ¨è¯„ä¼°äººå·¥æ™ºèƒ½çš„æ¨ç†èƒ½åŠ›ã€‚å®ƒåŸºäºäººç±»éªŒè¯ä»»åŠ¡åœ¨åŒºå—é“¾ä¸Šè¿›è¡Œæ„å»ºï¼Œè¦æ±‚ç”¨æˆ·è¯†åˆ«å‡ºé€»è¾‘è¿è´¯çš„å›¾åƒåºåˆ—ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„å‡†ç¡®ç‡æœ€é«˜è¾¾åˆ°77.9%ï¼Œä½†ä»è¿œä½äºäººç±»çš„å‡†ç¡®ç‡ï¼ˆ95.3%ï¼‰ã€‚ç»“åˆå¤šç§æ¨¡å‹çš„é¢„æµ‹å¯ä»¥æé«˜å‡†ç¡®ç‡è‡³85.2%ï¼Œè¿™çªæ˜¾äº†ç°æœ‰æ¨ç†æ¨¡å‹çš„å±€é™æ€§ä»¥åŠå¯¹æ›´ç¨³å¥çš„å¤šæ¨¡å¼åŸºå‡†æµ‹è¯•çš„éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FLIPæ•°æ®é›†æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°äººå·¥æ™ºèƒ½æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å®ƒåŸºäºäººç±»éªŒè¯ä»»åŠ¡åœ¨åŒºå—é“¾ä¸Šæ„å»ºï¼Œå¼ºè°ƒé¡ºåºæ¨ç†ã€è§†è§‰å™äº‹å’Œå¸¸è¯†ã€‚</li>
<li>æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„å‡†ç¡®ç‡æœ€é«˜è¾¾åˆ°77.9%ï¼Œä½†ä»ä½äºäººç±»çš„å‡†ç¡®ç‡ã€‚</li>
<li>æ ‡æ³¨æ¨¡å‹é€šè¿‡æä¾›å›¾åƒçš„æ–‡å­—æè¿°æ¥è¾…åŠ©æ¨ç†æ¨¡å‹ï¼Œå–å¾—äº†ä¸€å®šæˆæ•ˆã€‚</li>
<li>ç»“åˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹å¯ä»¥æé«˜æ¨ç†çš„å‡†ç¡®æ€§ã€‚</li>
<li>å½“å‰äººå·¥æ™ºèƒ½åœ¨æ¨ç†æ–¹é¢ä»æœ‰å±€é™æ€§ï¼Œéœ€è¦æ›´ç¨³å¥çš„å¤šæ¨¡å¼åŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4e69aa38ff7a9af0ca251c44862d5970.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd312e55b8b7c7145cfc05f178c34e4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81853c59a244fce86c14ebec605a1ddd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="An-Evaluation-of-N-Gram-Selection-Strategies-for-Regular-Expression-Indexing-in-Contemporary-Text-Analysis-Tasks"><a href="#An-Evaluation-of-N-Gram-Selection-Strategies-for-Regular-Expression-Indexing-in-Contemporary-Text-Analysis-Tasks" class="headerlink" title="An Evaluation of N-Gram Selection Strategies for Regular Expression   Indexing in Contemporary Text Analysis Tasks"></a>An Evaluation of N-Gram Selection Strategies for Regular Expression   Indexing in Contemporary Text Analysis Tasks</h2><p><strong>Authors:Ling Zhang, Shaleen Deep, Jignesh M. Patel, Karthikeyan Sankaralingam</strong></p>
<p>Efficient evaluation of regular expressions (regex, for short) is crucial for text analysis, and n-gram indexes are fundamental to achieving fast regex evaluation performance. However, these indexes face scalability challenges because of the exponential number of possible n-grams that must be indexed. Many existing selection strategies, developed decades ago, have not been rigorously evaluated on contemporary large-scale workloads and lack comprehensive performance comparisons. Therefore, a unified and comprehensive evaluation framework is necessary to compare these methods under the same experimental settings. This paper presents the first systematic evaluation of three representative n-gram selection strategies across five workloads, including real-time production logs and genomic sequence analysis. We examine their trade-offs in terms of index construction time, storage overhead, false positive rates, and end-to-end query performance. Through empirical results, this study provides a modern perspective on existing n-gram based regular expression evaluation methods, extensive observations, valuable discoveries, and an adaptable testing framework to guide future research in this domain. We make our implementations of these methods and our test framework available as open-source at <a target="_blank" rel="noopener" href="https://github.com/mush-zhang/RegexIndexComparison">https://github.com/mush-zhang/RegexIndexComparison</a>. </p>
<blockquote>
<p>æ­£åˆ™è¡¨è¾¾å¼ï¼ˆç®€ç§°regexï¼‰çš„æœ‰æ•ˆè¯„ä¼°åœ¨æ–‡æœ¬åˆ†æä¸­è‡³å…³é‡è¦ï¼Œè€Œnå…ƒç´¢å¼•æ˜¯å®ç°å¿«é€Ÿæ­£åˆ™è¡¨è¾¾å¼è¯„ä¼°æ€§èƒ½çš„åŸºç¡€ã€‚ç„¶è€Œï¼Œè¿™äº›ç´¢å¼•é¢ä¸´ç€ç”±äºå¿…é¡»ç´¢å¼•çš„æŒ‡æ•°çº§å¢é•¿çš„nå…ƒç»„æ•°é‡æ‰€å¸¦æ¥çš„å¯æ‰©å±•æ€§æŒ‘æˆ˜ã€‚è®¸å¤šç°æœ‰çš„é€‰æ‹©ç­–ç•¥æ˜¯å‡ åå¹´å‰å¼€å‘çš„ï¼Œå°šæœªåœ¨å½“å‰çš„å¤§è§„æ¨¡å·¥ä½œè´Ÿè½½ä¸Šè¿›è¡Œä¸¥æ ¼è¯„ä¼°ï¼Œå¹¶ä¸”ç¼ºä¹å…¨é¢çš„æ€§èƒ½æ¯”è¾ƒã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ä¸ªç»Ÿä¸€å’Œå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œåœ¨åŒä¸€å®éªŒè®¾ç½®ä¸‹æ¯”è¾ƒè¿™äº›æ–¹æ³•ã€‚æœ¬æ–‡å¯¹ä¸‰ç§ä»£è¡¨æ€§çš„nå…ƒç»„é€‰æ‹©ç­–ç•¥è¿›è¡Œäº†äº”é¡¹å·¥ä½œè´Ÿè½½çš„é¦–ä¸ªç³»ç»Ÿè¯„ä¼°ï¼ŒåŒ…æ‹¬å®æ—¶ç”Ÿäº§æ—¥å¿—å’ŒåŸºå› ç»„åºåˆ—åˆ†æã€‚æˆ‘ä»¬è€ƒå¯Ÿäº†å®ƒä»¬åœ¨ç´¢å¼•æ„å»ºæ—¶é—´ã€å­˜å‚¨å¼€é”€ã€è¯¯æŠ¥ç‡å’Œç«¯åˆ°ç«¯æŸ¥è¯¢æ€§èƒ½æ–¹é¢çš„æƒè¡¡ã€‚é€šè¿‡å®è¯ç»“æœï¼Œæœ¬ç ”ç©¶æä¾›äº†å¯¹ç°æœ‰åŸºäºnå…ƒçš„æ­£åˆ™è¡¨è¾¾å¼è¯„ä¼°æ–¹æ³•çš„ç°ä»£è§†è§’ã€ä¸°å¯Œçš„è§‚å¯Ÿå’Œæœ‰ä»·å€¼çš„å‘ç°ï¼Œä»¥åŠä¸€ä¸ªå¯é€‚åº”çš„æµ‹è¯•æ¡†æ¶ï¼Œä»¥æŒ‡å¯¼è¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/mush-zhang/RegexIndexComparison">https://github.com/mush-zhang/RegexIndexComparison</a>ä¸Šæä¾›äº†è¿™äº›æ–¹æ³•çš„å®ç°å’Œæˆ‘ä»¬çš„æµ‹è¯•æ¡†æ¶çš„å¼€æºç‰ˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12251v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ç ”ç©¶äº†ä¸‰ç§ä»£è¡¨æ€§çš„n-gramé€‰æ‹©ç­–ç•¥åœ¨äº”ä¸ªå·¥ä½œè´Ÿè½½ä¸‹çš„ç³»ç»Ÿè¯„ä¼°ï¼ŒåŒ…æ‹¬å®æ—¶ç”Ÿäº§æ—¥å¿—å’ŒåŸºå› ç»„åºåˆ—åˆ†æã€‚æ–‡ç« è¯„ä¼°äº†è¿™äº›æ–¹æ³•åœ¨ç´¢å¼•æ„å»ºæ—¶é—´ã€å­˜å‚¨å¼€é”€ã€è¯¯æŠ¥ç‡å’Œç«¯åˆ°ç«¯æŸ¥è¯¢æ€§èƒ½æ–¹é¢çš„ä¼˜ç¼ºç‚¹ï¼Œæä¾›äº†ä¸€ä¸ªå…³äºç°æœ‰åŸºäºn-gramçš„æ­£åˆ™è¡¨è¾¾å¼è¯„ä¼°æ–¹æ³•çš„ç°ä»£è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ­£åˆ™è¡¨è¾¾å¼çš„æœ‰æ•ˆè¯„ä¼°å¯¹æ–‡æœ¬åˆ†æè‡³å…³é‡è¦ï¼Œè€Œn-gramç´¢å¼•æ˜¯å®ç°å¿«é€Ÿæ­£åˆ™è¡¨è¾¾å¼è¯„ä¼°æ€§èƒ½çš„åŸºç¡€ã€‚</li>
<li>n-gramé€‰æ‹©ç­–ç•¥é¢ä¸´å¯æ‰©å±•æ€§æŒ‘æˆ˜ï¼Œå› ä¸ºå¿…é¡»ç´¢å¼•çš„n-gramæ•°é‡å‘ˆæŒ‡æ•°å¢é•¿ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç¼ºä¹ç»Ÿä¸€å’Œå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œéœ€è¦åœ¨ç›¸åŒå®éªŒè®¾ç½®ä¸‹è¿›è¡Œæ¯”è¾ƒã€‚</li>
<li>æ–‡ç« é¦–æ¬¡ç³»ç»Ÿè¯„ä¼°äº†ä¸‰ç§ä»£è¡¨æ€§çš„n-gramé€‰æ‹©ç­–ç•¥ï¼Œè·¨è¶Šäº”ä¸ªå·¥ä½œè´Ÿè½½ã€‚</li>
<li>è¿™äº›ç­–ç•¥åœ¨ç´¢å¼•æ„å»ºæ—¶é—´ã€å­˜å‚¨å¼€é”€ã€è¯¯æŠ¥ç‡å’Œç«¯åˆ°ç«¯æŸ¥è¯¢æ€§èƒ½ç­‰æ–¹é¢å­˜åœ¨æƒè¡¡ã€‚</li>
<li>æ–‡ç« æä¾›äº†åŸºäºå®è¯çš„ç»“æœï¼Œå¯¹åŸºäºn-gramçš„æ­£åˆ™è¡¨è¾¾å¼è¯„ä¼°æ–¹æ³•æä¾›äº†ç°ä»£è§†è§’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12251">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-016af14af98163ac565e70b37d513336.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b652ac75813fb8bb0cdbe794f6e725f4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="d1-Scaling-Reasoning-in-Diffusion-Large-Language-Models-via-Reinforcement-Learning"><a href="#d1-Scaling-Reasoning-in-Diffusion-Large-Language-Models-via-Reinforcement-Learning" class="headerlink" title="d1: Scaling Reasoning in Diffusion Large Language Models via   Reinforcement Learning"></a>d1: Scaling Reasoning in Diffusion Large Language Models via   Reinforcement Learning</h2><p><strong>Authors:Siyan Zhao, Devaansh Gupta, Qinqing Zheng, Aditya Grover</strong></p>
<p>Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning. To this end, we propose d1, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and logical reasoning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç¤ºäº†å—ç›Šäºåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ã€‚è¿™äº›èƒ½åŠ›ä¸»è¦æ˜¯åœ¨ä»å·¦åˆ°å³çš„è‡ªå›å½’ï¼ˆARï¼‰ç”ŸæˆèŒƒå¼å†…å±•ç¤ºçš„ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºæ‰©æ•£çš„éè‡ªå›å½’èŒƒå¼ä»¥ä»ç²—åˆ°ç»†çš„æ–¹å¼ç”Ÿæˆæ–‡æœ¬ã€‚å°½ç®¡æœ€è¿‘çš„åŸºäºæ‰©æ•£çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMï¼‰åœ¨ä¸å…¶ARåŒç±»æ¨¡å‹çš„ç«äº‰ä¸­å®ç°äº†ç›¸å½“çš„è¯­è¨€å»ºæ¨¡æ€§èƒ½ï¼Œä½†å°šä¸æ¸…æ¥šdLLMæ˜¯å¦èƒ½å¤Ÿåˆ©ç”¨æœ€è¿‘LLMæ¨ç†ä¸­çš„æŠ€æœ¯è¿›å±•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†d1æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç»“åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒRLå°†é¢„è®­ç»ƒçš„æ©ç dLLMé€‚åº”ä¸ºæ¨ç†æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼€å‘å¹¶æ‰©å±•äº†æé«˜é¢„è®­ç»ƒdLLMä¸­æ¨ç†çš„æŠ€æœ¯ï¼šï¼ˆaï¼‰æˆ‘ä»¬ä½¿ç”¨æ©è”½çš„SFTæŠ€æœ¯ä»ç°æœ‰æ•°æ®é›†ä¸­è’¸é¦çŸ¥è¯†å¹¶çŒè¾“è‡ªæˆ‘æ”¹è¿›è¡Œä¸ºï¼›ï¼ˆbï¼‰æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ— è¯„è®ºå®¶ã€åŸºäºç­–ç•¥æ¢¯åº¦çš„RLç®—æ³•ï¼Œç§°ä¸ºdiffu-GRPOã€‚é€šè¿‡å®è¯ç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªæ•°å­¦å’Œé€»è¾‘æ¨ç†åŸºå‡†ä¸Šæ¢è®¨äº†ä¸åŒåè®­ç»ƒé…æ–¹æ€§èƒ½çš„è¡¨ç°ã€‚æˆ‘ä»¬å‘ç°d1å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—æé«˜äº†æœ€æ–°dLLMçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12216v1">PDF</a> 25 pages, project page at <a target="_blank" rel="noopener" href="https://dllm-reasoning.github.io/">https://dllm-reasoning.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä¸»è¦åœ¨å·¦è‡³å³çš„è‡ªå›å½’ç”ŸæˆèŒƒå¼ä¸­å¾—åˆ°éªŒè¯ã€‚æœ¬ç ”ç©¶æå‡ºd1æ¡†æ¶ï¼Œæ—¨åœ¨å°†é¢„è®­ç»ƒçš„æ‰©æ•£å¼å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œè½¬åŒ–ä¸ºæ¨ç†æ¨¡å‹ã€‚ç ”ç©¶å‘ç°d1èƒ½æé«˜é¢„è®­ç»ƒæ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨æ•°å­¦ä¸é€»è¾‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å€ŸåŠ©åœ¨çº¿å¼ºåŒ–å­¦ä¹ å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è‡ªå›å½’ç”ŸæˆèŒƒå¼åœ¨å±•ç¤ºè¿™äº›èƒ½åŠ›æ–¹é¢å æ®ä¸»å¯¼åœ°ä½ã€‚</li>
<li>æ‰©æ•£å¼å¤§å‹è¯­è¨€æ¨¡å‹èƒ½é€šè¿‡ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ è½¬åŒ–ä¸ºæ¨ç†æ¨¡å‹ã€‚</li>
<li>d1æ¡†æ¶é€šè¿‡ç»“åˆç›‘ç£å¾®è°ƒä¸æ— è¯„è®ºå®¶æ”¿ç­–æ¢¯åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•æ¥æé«˜é¢„è®­ç»ƒæ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12216">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bd2c98a830b610cd0dcc696ad3bcdc99.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-164d4f8d5db23a2bae9c04ad847dbd8d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Reasoning-Based-AI-for-Startup-Evaluation-R-A-I-S-E-A-Memory-Augmented-Multi-Step-Decision-Framework"><a href="#Reasoning-Based-AI-for-Startup-Evaluation-R-A-I-S-E-A-Memory-Augmented-Multi-Step-Decision-Framework" class="headerlink" title="Reasoning-Based AI for Startup Evaluation (R.A.I.S.E.): A   Memory-Augmented, Multi-Step Decision Framework"></a>Reasoning-Based AI for Startup Evaluation (R.A.I.S.E.): A   Memory-Augmented, Multi-Step Decision Framework</h2><p><strong>Authors:Jack Preuveneers, Joseph Ternasky, Fuat Alican, Yigit Ihlamur</strong></p>
<p>We present a novel framework that bridges the gap between the interpretability of decision trees and the advanced reasoning capabilities of large language models (LLMs) to predict startup success. Our approach leverages chain-of-thought prompting to generate detailed reasoning logs, which are subsequently distilled into structured, human-understandable logical rules. The pipeline integrates multiple enhancements - efficient data ingestion, a two-step refinement process, ensemble candidate sampling, simulated reinforcement learning scoring, and persistent memory - to ensure both stable decision-making and transparent output. Experimental evaluations on curated startup datasets demonstrate that our combined pipeline improves precision by 54% from 0.225 to 0.346 and accuracy by 50% from 0.46 to 0.70 compared to a standalone OpenAI o3 model. Notably, our model achieves over 2x the precision of a random classifier (16%). By combining state-of-the-art AI reasoning with explicit rule-based explanations, our method not only augments traditional decision-making processes but also facilitates expert intervention and continuous policy refinement. This work lays the foundation for the implementation of interpretable LLM-powered decision frameworks in high-stakes investment environments and other domains that require transparent and data-driven insights. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç¼©å°äº†å†³ç­–æ ‘å¯è§£é‡Šæ€§ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…ˆè¿›æ¨ç†èƒ½åŠ›ä¹‹é—´çš„å·®è·ï¼Œä»¥é¢„æµ‹åˆ›ä¸šå…¬å¸çš„æˆåŠŸã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨é“¾å¼æ€ç»´æç¤ºç”Ÿæˆè¯¦ç»†çš„æ¨ç†æ—¥å¿—ï¼Œéšåå°†å…¶æç‚¼æˆç»“æ„åŒ–ã€äººç±»å¯ç†è§£çš„é€»è¾‘è§„åˆ™ã€‚è¯¥ç®¡é“èåˆäº†å¤šé¡¹å¢å¼ºåŠŸèƒ½ï¼ŒåŒ…æ‹¬é«˜æ•ˆæ•°æ®æ‘„å–ã€ä¸¤æ­¥ç²¾ç‚¼è¿‡ç¨‹ã€é›†æˆå€™é€‰é‡‡æ ·ã€æ¨¡æ‹Ÿå¼ºåŒ–å­¦ä¹ è¯„åˆ†å’ŒæŒä¹…æ€§å†…å­˜ï¼Œä»¥ç¡®ä¿ç¨³å®šçš„å†³ç­–åˆ¶å®šå’Œé€æ˜çš„è¾“å‡ºã€‚åœ¨ç²¾é€‰çš„åˆåˆ›å…¬å¸æ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç»„åˆç®¡é“ä¸å•ç‹¬çš„OpenAI o3æ¨¡å‹ç›¸æ¯”ï¼Œç²¾ç¡®åº¦ä»0.225æé«˜åˆ°0.346ï¼ˆæé«˜54%ï¼‰ï¼Œå‡†ç¡®ç‡ä»0.46æé«˜åˆ°0.70ï¼ˆæé«˜50%ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹çš„ç²¾ç¡®åº¦æ˜¯éšæœºåˆ†ç±»å™¨çš„ä¸¤å€ä»¥ä¸Šï¼ˆ16%ï¼‰ã€‚é€šè¿‡ç»“åˆæœ€å…ˆè¿›çš„AIæ¨ç†å’ŒåŸºäºæ˜ç¡®è§„åˆ™çš„è§£é‡Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…å¢å¼ºäº†ä¼ ç»Ÿçš„å†³ç­–åˆ¶å®šè¿‡ç¨‹ï¼Œè¿˜ä¿ƒè¿›äº†ä¸“å®¶å¹²é¢„å’Œè¿ç»­çš„æ”¿ç­–ä¼˜åŒ–ã€‚è¿™é¡¹å·¥ä½œä¸ºé«˜é£é™©æŠ•èµ„ç¯å¢ƒå’Œå…¶ä»–éœ€è¦é€æ˜åŒ–å’Œæ•°æ®é©±åŠ¨æ´å¯ŸåŠ›çš„é¢†åŸŸå®æ–½å¯è§£é‡Šçš„LLMé©±åŠ¨å†³ç­–æ¡†æ¶å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12090v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–æ¡†æ¶ï¼Œèåˆäº†å†³ç­–æ ‘çš„è§£é‡Šæ€§ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é«˜çº§æ¨ç†èƒ½åŠ›ï¼Œä»¥é¢„æµ‹åˆ›ä¸šå…¬å¸çš„æˆåŠŸã€‚è¯¥ç ”ç©¶é€šè¿‡æ€ç»´é“¾æç¤ºç”Ÿæˆè¯¦ç»†çš„æ¨ç†æ—¥å¿—ï¼Œéšåå°†å…¶æç‚¼ä¸ºç»“æ„åŒ–ã€äººç±»å¯ç†è§£çš„é€»è¾‘è§„åˆ™ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸å•ä¸€çš„OpenAI o3æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶åœ¨ç²¾ç¡®åº¦å’Œå‡†ç¡®åº¦æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚è¯¥æ¨¡å‹ç»“åˆäº†æœ€å…ˆè¿›çš„AIæ¨ç†å’ŒåŸºäºè§„åˆ™çš„æ˜ç¡®è§£é‡Šï¼Œä¸ä»…å¢å¼ºäº†ä¼ ç»Ÿå†³ç­–åˆ¶å®šè¿‡ç¨‹ï¼Œè¿˜æœ‰åŠ©äºä¸“å®¶å¹²é¢„å’Œæ”¿ç­–æŒç»­æ”¹è¿›ã€‚è¿™ä¸ºåœ¨é«˜é£é™©æŠ•èµ„ç¯å¢ƒå’Œå…¶ä»–éœ€è¦é€æ˜å’Œæ•°æ®é©±åŠ¨çš„é¢†åŸŸå®ç°å¯è§£é‡Šçš„LLMé©±åŠ¨å†³ç­–æ¡†æ¶å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ç»“åˆå†³ç­–æ ‘çš„è§£é‡Šæ€§å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›çš„æ¡†æ¶ï¼Œç”¨äºé¢„æµ‹åˆ›ä¸šå…¬å¸æˆåŠŸã€‚</li>
<li>é€šè¿‡æ€ç»´é“¾æç¤ºç”Ÿæˆè¯¦ç»†çš„æ¨ç†æ—¥å¿—ï¼Œå†è½¬åŒ–ä¸ºç»“æ„åŒ–é€»è¾‘è§„åˆ™ã€‚</li>
<li>æ¡†æ¶åŒ…å«å¤šé¡¹æ”¹è¿›ï¼Œå¦‚é«˜æ•ˆæ•°æ®æ‘„å–ã€ä¸¤æ­¥ç²¾ç‚¼è¿‡ç¨‹ã€ç»„åˆå€™é€‰é‡‡æ ·ã€æ¨¡æ‹Ÿå¼ºåŒ–å­¦ä¹ è¯„åˆ†å’ŒæŒä¹…æ€§è®°å¿†ï¼Œä»¥ç¡®ä¿ç¨³å®šçš„å†³ç­–åˆ¶å®šå’Œé€æ˜çš„è¾“å‡ºã€‚</li>
<li>å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œä¸å•ä¸€OpenAI o3æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶åœ¨ç²¾ç¡®åº¦å’Œå‡†ç¡®åº¦æ–¹é¢æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>æ¨¡å‹ç»“åˆäº†å…ˆè¿›çš„AIæ¨ç†å’ŒåŸºäºè§„åˆ™çš„æ˜ç¡®è§£é‡Šï¼Œå¢å¼ºäº†ä¼ ç»Ÿå†³ç­–åˆ¶å®šè¿‡ç¨‹ã€‚</li>
<li>æ¡†æ¶æœ‰åŠ©äºä¸“å®¶å¹²é¢„å’Œæ”¿ç­–çš„æŒç»­æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12090">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a8a1e3dbf0d4024953135d0d2e1d62ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0152b8f00088e2bbd75514c5f046679.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-974041a8c96599e3a952e80f40afc041.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AnomalyR1-A-GRPO-based-End-to-end-MLLM-for-Industrial-Anomaly-Detection"><a href="#AnomalyR1-A-GRPO-based-End-to-end-MLLM-for-Industrial-Anomaly-Detection" class="headerlink" title="AnomalyR1: A GRPO-based End-to-end MLLM for Industrial Anomaly Detection"></a>AnomalyR1: A GRPO-based End-to-end MLLM for Industrial Anomaly Detection</h2><p><strong>Authors:Yuhao Chao, Jie Liu, Jie Tang, Gangshan Wu</strong></p>
<p>Industrial Anomaly Detection (IAD) poses a formidable challenge due to the scarcity of defective samples, making it imperative to deploy models capable of robust generalization to detect unseen anomalies effectively. Traditional approaches, often constrained by hand-crafted features or domain-specific expert models, struggle to address this limitation, underscoring the need for a paradigm shift. We introduce AnomalyR1, a pioneering framework that leverages VLM-R1, a Multimodal Large Language Model (MLLM) renowned for its exceptional generalization and interpretability, to revolutionize IAD. By integrating MLLM with Group Relative Policy Optimization (GRPO), enhanced by our novel Reasoned Outcome Alignment Metric (ROAM), AnomalyR1 achieves a fully end-to-end solution that autonomously processes inputs of image and domain knowledge, reasons through analysis, and generates precise anomaly localizations and masks. Based on the latest multimodal IAD benchmark, our compact 3-billion-parameter model outperforms existing methods, establishing state-of-the-art results. As MLLM capabilities continue to advance, this study is the first to deliver an end-to-end VLM-based IAD solution that demonstrates the transformative potential of ROAM-enhanced GRPO, positioning our framework as a forward-looking cornerstone for next-generation intelligent anomaly detection systems in industrial applications with limited defective data. </p>
<blockquote>
<p>å·¥ä¸šå¼‚å¸¸æ£€æµ‹ï¼ˆIADï¼‰ç”±äºç¼ºé™·æ ·æœ¬çš„ç¨€ç¼ºæ€§è€Œé¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œå› æ­¤éœ€è¦éƒ¨ç½²èƒ½å¤Ÿç¨³å¥æ³›åŒ–çš„æ¨¡å‹ï¼Œä»¥æœ‰æ•ˆæ£€æµ‹æœªçŸ¥çš„å¼‚å¸¸å€¼ã€‚ä¼ ç»Ÿçš„æ–¹æ³•å¸¸å¸¸å—åˆ°æ‰‹å·¥ç‰¹å¾æˆ–ç‰¹å®šé¢†åŸŸä¸“å®¶æ¨¡å‹çš„é™åˆ¶ï¼Œéš¾ä»¥è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œçªæ˜¾äº†èŒƒå¼è½¬å˜çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†AnomalR1ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€åˆ›æ€§çš„æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ä»¥å‡ºè‰²æ³›åŒ–å’Œå¯è§£é‡Šæ€§è‘—ç§°çš„å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰VLM-R1æ¥é©æ–°IADã€‚é€šè¿‡å°†MLLMä¸å—æˆ‘ä»¬çš„æ–°å‹åˆç†ç»“æœå¯¹é½åº¦é‡ï¼ˆROAMï¼‰å¢å¼ºçš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç›¸ç»“åˆï¼ŒAnomalyR1å®ç°äº†ç«¯åˆ°ç«¯çš„å®Œå…¨è§£å†³æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå¯ä»¥è‡ªä¸»å¤„ç†å›¾åƒå’Œé¢†åŸŸçŸ¥è¯†çš„è¾“å…¥ï¼Œé€šè¿‡åˆ†æè¿›è¡Œæ¨ç†ï¼Œå¹¶ç”Ÿæˆç²¾ç¡®å¼‚å¸¸å®šä½å’Œæ©ç ã€‚åŸºäºæœ€æ–°çš„å¤šæ¨¡å¼IADåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬ç´§å‡‘çš„3äº¿å‚æ•°æ¨¡å‹ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå–å¾—äº†æœ€æ–°ç»“æœã€‚éšç€MLLMèƒ½åŠ›çš„ä¸æ–­è¿›æ­¥ï¼Œæœ¬ç ”ç©¶é¦–æ¬¡æä¾›äº†åŸºäºVLMçš„ç«¯åˆ°ç«¯IADè§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†ç”±ROAMå¢å¼ºçš„GRPOçš„å˜é©æ½œåŠ›ï¼Œä½¿æˆ‘ä»¬çš„æ¡†æ¶æˆä¸ºé¢å‘ä¸‹ä¸€ä»£å…·æœ‰æœ‰é™ç¼ºé™·æ•°æ®çš„å·¥ä¸šåº”ç”¨çš„æ™ºèƒ½å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿçš„é‡è¦åŸºçŸ³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11914v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å·¥ä¸šå¼‚å¸¸æ£€æµ‹ï¼ˆIADï¼‰é¢ä¸´æ ·æœ¬ç¼ºé™·ç¨€ç¼ºçš„ä¸¥å³»æŒ‘æˆ˜ï¼Œéœ€è¦éƒ¨ç½²èƒ½å¤Ÿé²æ£’æ³›åŒ–ä»¥æœ‰æ•ˆæ£€æµ‹æœªè§å¼‚å¸¸çš„æ¨¡å‹ã€‚å¼•å…¥AnomalyR1æ¡†æ¶ï¼Œå€ŸåŠ©å…·æœ‰å“è¶Šæ³›åŒ–å’Œè§£é‡Šæ€§çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰VLM-R1è¿›è¡Œé©å‘½æ€§å˜é©ã€‚é€šè¿‡æ•´åˆMLLMä¸å—é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å¯å‘çš„ç†æ€§ç»“æœå¯¹é½åº¦é‡ï¼ˆROAMï¼‰ï¼ŒAnomalyR1å®ç°ç«¯åˆ°ç«¯çš„è§£å†³æ–¹æ¡ˆï¼Œå¯è‡ªä¸»å¤„ç†å›¾åƒå’Œé¢†åŸŸçŸ¥è¯†è¾“å…¥ï¼Œè¿›è¡Œåˆ†ææ¨ç†ï¼Œå¹¶ç”Ÿæˆç²¾ç¡®å¼‚å¸¸å®šä½å’Œæ©ç ã€‚åŸºäºæœ€æ–°çš„å¤šæ¨¡æ€IADåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬çš„ç´§å‡‘å‹3äº¿å‚æ•°æ¨¡å‹è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°æœ€æ–°ç»“æœã€‚ä½œä¸ºMLLMèƒ½åŠ›ä¸æ–­è¿›æ­¥çš„ä¸€éƒ¨åˆ†ï¼Œæœ¬ç ”ç©¶é¦–æ¬¡æä¾›åŸºäºVLMçš„ç«¯åˆ°ç«¯IADè§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†å—ROAMå¢å¼ºçš„GRPOçš„å˜é©æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·¥ä¸šå¼‚å¸¸æ£€æµ‹é¢ä¸´æ ·æœ¬ç¼ºé™·ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚</li>
<li>AnomalyR1æ¡†æ¶å€ŸåŠ©å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰VLM-R1è¿›è¡Œé©å‘½æ€§å˜é©ä»¥è§£å†³æ­¤æŒ‘æˆ˜ã€‚</li>
<li>AnomalyR1é€šè¿‡æ•´åˆMLLMä¸é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åŠæ–°å‹Reasoned outcome Alignment Metric (ROAM)ï¼Œå®ç°ç«¯åˆ°ç«¯çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>è¯¥æ–¹æ¡ˆå¯è‡ªä¸»å¤„ç†å›¾åƒå’Œé¢†åŸŸçŸ¥è¯†è¾“å…¥ï¼Œè¿›è¡Œåˆ†ææ¨ç†ï¼Œç”Ÿæˆç²¾ç¡®å¼‚å¸¸å®šä½å’Œæ©ç ã€‚</li>
<li>åœ¨æœ€æ–°çš„å¤šæ¨¡æ€IADåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAnomalyR1è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>AnomalyR1çš„ç´§å‡‘å‹æ¨¡å‹å…·æœ‰é«˜æ•ˆæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11914">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cdae87a0f078db63677383128efc834a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-259dfcb5e7e9bff18f22cf4b710f11a2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d289c9f1b9153763d4d53a8be556be44.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Finding-Flawed-Fictions-Evaluating-Complex-Reasoning-in-Language-Models-via-Plot-Hole-Detection"><a href="#Finding-Flawed-Fictions-Evaluating-Complex-Reasoning-in-Language-Models-via-Plot-Hole-Detection" class="headerlink" title="Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models   via Plot Hole Detection"></a>Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models   via Plot Hole Detection</h2><p><strong>Authors:Kabir Ahuja, Melanie Sclar, Yulia Tsvetkov</strong></p>
<p>Stories are a fundamental aspect of human experience. Engaging deeply with stories and spotting plot holes â€“ inconsistencies in a storyline that break the internal logic or rules of a storyâ€™s world â€“ requires nuanced reasoning skills, including tracking entities and events and their interplay, abstract thinking, pragmatic narrative understanding, commonsense and social reasoning, and theory of mind. As Large Language Models (LLMs) increasingly generate, interpret, and modify text, rigorously assessing their narrative consistency and deeper language understanding becomes critical. However, existing benchmarks focus mainly on surface-level comprehension. In this work, we propose plot hole detection in stories as a proxy to evaluate language understanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel algorithm to controllably and carefully synthesize plot holes in human-written stories. Using this algorithm, we construct a benchmark to evaluate LLMsâ€™ plot hole detection abilities in stories â€“ FlawedFictions â€“ , which is robust to contamination, with human filtering ensuring high quality. We find that state-of-the-art LLMs struggle in accurately solving FlawedFictions regardless of the reasoning effort allowed, with performance significantly degrading as story length increases. Finally, we show that LLM-based story summarization and story generation are prone to introducing plot holes, with more than 50% and 100% increases in plot hole detection rates with respect to human-written originals. </p>
<blockquote>
<p>æ•…äº‹æ˜¯äººç±»ç»éªŒçš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ã€‚æ·±å…¥å‚ä¸æ•…äº‹å¹¶å‘ç°æƒ…èŠ‚æ¼æ´â€”â€”æ•…äº‹æƒ…èŠ‚ä¸­çš„ä¸ä¸€è‡´ä¹‹å¤„ï¼Œç ´åäº†æ•…äº‹å†…éƒ¨çš„é€»è¾‘æˆ–è§„åˆ™â€”â€”éœ€è¦å¾®å¦™çš„æ¨ç†æŠ€èƒ½ï¼ŒåŒ…æ‹¬è¿½è¸ªå®ä½“å’Œäº‹ä»¶åŠå…¶ç›¸äº’ä½œç”¨ã€æŠ½è±¡æ€ç»´ã€å®ç”¨å™äº‹ç†è§£ã€å¸¸è¯†å’Œç¤¾ä¼šæ¨ç†ä»¥åŠå¿ƒæ™ºç†è®ºã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°ç”Ÿæˆã€è§£é‡Šå’Œä¿®æ”¹æ–‡æœ¬ï¼Œä¸¥æ ¼è¯„ä¼°å…¶å™äº‹è¿è´¯æ€§å’Œæ›´æ·±å±‚æ¬¡çš„è¯­è¨€ç†è§£èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨è¡¨é¢å±‚æ¬¡çš„ç†è§£ä¸Šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºå°†æƒ…èŠ‚æ¼æ´æ£€æµ‹ä½œä¸ºè¯„ä¼°LLMçš„è¯­è¨€ç†è§£å’Œæ¨ç†èƒ½åŠ›çš„ä»£ç†ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åä¸ºFlawedFictionsMakerçš„æ–°å‹ç®—æ³•ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿå¯æ§ä¸”ä»”ç»†åœ°åˆæˆäººç±»ç¼–å†™çš„æ•…äº‹ä¸­çš„æƒ…èŠ‚æ¼æ´ã€‚ä½¿ç”¨è¯¥ç®—æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŸºå‡†æµ‹è¯•FlawedFictionsï¼Œä»¥è¯„ä¼°LLMåœ¨æ•…äº‹ä¸­æ£€æµ‹æƒ…èŠ‚æ¼æ´çš„èƒ½åŠ›ï¼Œè¯¥æµ‹è¯•å¯¹æ±¡æŸ“å…·æœ‰é²æ£’æ€§ï¼Œå¹¶é€šè¿‡äººå·¥è¿‡æ»¤ç¡®ä¿é«˜è´¨é‡ã€‚æˆ‘ä»¬å‘ç°ï¼Œæœ€å…ˆè¿›çš„LLMåœ¨è§£å†³FlawedFictionsæ—¶é¢ä¸´å›°éš¾ï¼Œæ— è®ºå…è®¸å¤šå°‘æ¨ç†åŠªåŠ›ï¼Œæ€§èƒ½éƒ½ä¼šéšç€æ•…äº‹é•¿åº¦çš„å¢åŠ è€Œæ˜¾è‘—ä¸‹é™ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†åŸºäºLLMçš„æ•…äº‹æ‘˜è¦å’Œæ•…äº‹ç”Ÿæˆå®¹æ˜“å¼•å…¥æƒ…èŠ‚æ¼æ´ï¼Œä¸äººç±»åŸå§‹ä½œå“ç›¸æ¯”ï¼Œæƒ…èŠ‚æ¼æ´æ£€æµ‹ç‡åˆ†åˆ«å¢åŠ äº†50%å’Œ100%ä»¥ä¸Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11900v1">PDF</a> Preprint</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡å…³æ³¨æ•…äº‹ä¸­çš„æƒ…èŠ‚æ¼æ´æ£€æµ‹ï¼Œè®¤ä¸ºè¿™æ˜¯è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ·±å±‚æ¬¡è¯­è¨€ç†è§£å’Œæ¨ç†èƒ½åŠ›çš„é‡è¦æ ‡å‡†ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºFlawedFictionsMakerçš„æ–°ç®—æ³•ï¼Œèƒ½å¤Ÿå¯æ§ä¸”ç²¾å‡†åœ°åœ¨äººç±»æ’°å†™çš„æ•…äº‹ä¸­åˆæˆæƒ…èŠ‚æ¼æ´ã€‚åŸºäºè¯¥ç®—æ³•ï¼Œæ„å»ºäº†ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•…äº‹ä¸­çš„æƒ…èŠ‚æ¼æ´æ£€æµ‹èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•â€”â€”FlawedFictionsã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£å†³FlawedFictionsæ–¹é¢çš„èƒ½åŠ›æœ‰å¾…æé«˜ï¼Œéšç€æ•…äº‹é•¿åº¦çš„å¢åŠ ï¼Œæ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚æ­¤å¤–ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ•…äº‹æ‘˜è¦å’Œæ•…äº‹ç”Ÿæˆä¹Ÿå®¹æ˜“äº§ç”Ÿæƒ…èŠ‚æ¼æ´ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ•…äº‹ä¸­çš„æƒ…èŠ‚æ¼æ´æ£€æµ‹æ˜¯è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ·±å±‚æ¬¡è¯­è¨€ç†è§£å’Œæ¨ç†èƒ½åŠ›çš„é‡è¦æ ‡å‡†ã€‚</li>
<li>FlawedFictionsMakerç®—æ³•èƒ½å¤Ÿå¯æ§ä¸”ç²¾å‡†åœ°åœ¨äººç±»æ’°å†™çš„æ•…äº‹ä¸­åˆæˆæƒ…èŠ‚æ¼æ´ã€‚</li>
<li>FlawedFictionsåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æƒ…èŠ‚æ¼æ´æ£€æµ‹èƒ½åŠ›ï¼Œä¸”è¯¥æµ‹è¯•å¯¹äººç±»è¿‡æ»¤ä»¥ç¡®ä¿é«˜è´¨é‡ã€‚</li>
<li>å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£å†³FlawedFictionsæ–¹é¢çš„èƒ½åŠ›æœ‰å¾…æé«˜ã€‚</li>
<li>éšç€æ•…äº‹é•¿åº¦çš„å¢åŠ ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚</li>
<li>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ•…äº‹æ‘˜è¦å’Œæ•…äº‹ç”Ÿæˆå®¹æ˜“äº§ç”Ÿæƒ…èŠ‚æ¼æ´ã€‚</li>
<li>æƒ…èŠ‚æ¼æ´æ£€æµ‹å¯¹äºè¯„ä¼°è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œæ·±å±‚æ¬¡ç†è§£è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11900">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cfad090ad729e13e382c0f7bd973e79a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28efee1a5010ff446b3e688aa18e81d5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="The-Digital-Cybersecurity-Expert-How-Far-Have-We-Come"><a href="#The-Digital-Cybersecurity-Expert-How-Far-Have-We-Come" class="headerlink" title="The Digital Cybersecurity Expert: How Far Have We Come?"></a>The Digital Cybersecurity Expert: How Far Have We Come?</h2><p><strong>Authors:Dawei Wang, Geng Zhou, Xianglong Li, Yu Bai, Li Chen, Ting Qin, Jian Sun, Dan Li</strong></p>
<p>The increasing deployment of large language models (LLMs) in the cybersecurity domain underscores the need for effective model selection and evaluation. However, traditional evaluation methods often overlook specific cybersecurity knowledge gaps that contribute to performance limitations. To address this, we develop CSEBenchmark, a fine-grained cybersecurity evaluation framework based on 345 knowledge points expected of cybersecurity experts. Drawing from cognitive science, these points are categorized into factual, conceptual, and procedural types, enabling the design of 11,050 tailored multiple-choice questions. We evaluate 12 popular LLMs on CSEBenchmark and find that even the best-performing model achieves only 85.42% overall accuracy, with particular knowledge gaps in the use of specialized tools and uncommon commands. Different LLMs have unique knowledge gaps. Even large models from the same family may perform poorly on knowledge points where smaller models excel. By identifying and addressing specific knowledge gaps in each LLM, we achieve up to an 84% improvement in correcting previously incorrect predictions across three existing benchmarks for two cybersecurity tasks. Furthermore, our assessment of each LLMâ€™s knowledge alignment with specific cybersecurity roles reveals that different models align better with different roles, such as GPT-4o for the Google Senior Intelligence Analyst and Deepseek-V3 for the Amazon Privacy Engineer. These findings underscore the importance of aligning LLM selection with the specific knowledge requirements of different cybersecurity roles for optimal performance. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸçš„éƒ¨ç½²ä¸æ–­å¢åŠ ï¼Œæœ‰æ•ˆé€‰æ‹©æ¨¡å‹å¹¶è¿›è¡Œè¯„ä¼°çš„éœ€æ±‚æ„ˆå‘å‡¸æ˜¾ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„è¯„ä¼°æ–¹æ³•å¾€å¾€ä¼šå¿½è§†ç½‘ç»œå®‰å…¨é¢†åŸŸç‰¹å®šçš„çŸ¥è¯†ç¼ºå£ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†åŸºäºç½‘ç»œå®‰å…¨ä¸“å®¶æ‰€æœŸæœ›çš„345ä¸ªçŸ¥è¯†ç‚¹çš„ç²¾ç»†ç½‘ç»œå®‰å…¨è¯„ä¼°æ¡†æ¶â€”â€”CSEBenchmarkã€‚è¿™äº›çŸ¥è¯†ç‚¹æ¥è‡ªè®¤çŸ¥ç§‘å­¦é¢†åŸŸï¼Œè¢«å½’ç±»ä¸ºäº‹å®æ€§ã€æ¦‚å¿µæ€§å’Œç¨‹åºæ€§ä¸‰ç±»ï¼Œä»è€Œèƒ½å¤Ÿè®¾è®¡å‡ºé‡èº«å®šåˆ¶çš„11,050é“é€‰æ‹©é¢˜ã€‚æˆ‘ä»¬åœ¨CSEBenchmarkä¸Šè¯„ä¼°äº†æµè¡Œçš„12æ¬¾å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‘ç°å³ä½¿è¡¨ç°æœ€ä½³çš„æ¨¡å‹ä¹Ÿä»…è¾¾åˆ°æ•´ä½“å‡†ç¡®ç‡85.42%ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨ä¸“ä¸šå·¥å…·å’Œç½•è§å‘½ä»¤æ–¹é¢å­˜åœ¨ç‰¹å®šçš„çŸ¥è¯†ç¼ºå£ã€‚ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰ç‹¬ç‰¹çš„çŸ¥è¯†ç¼ºå£ã€‚å³ä½¿æ˜¯åŒä¸€ç³»åˆ—çš„å¤§å‹æ¨¡å‹ä¹Ÿå¯èƒ½åœ¨å°å‹æ¨¡å‹æ“…é•¿çš„çŸ¥è¯†ç‚¹ä¸Šè¡¨ç°ä¸ä½³ã€‚é€šè¿‡è¯†åˆ«å’Œè§£å†³æ¯ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ç‰¹å®šçŸ¥è¯†ç¼ºå£ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªç½‘ç»œå®‰å…¨ä»»åŠ¡çš„ä¸‰ä¸ªç°æœ‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¯¹ä¹‹å‰é”™è¯¯çš„é¢„æµ‹è¿›è¡Œäº†é«˜è¾¾84%çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸ç‰¹å®šç½‘ç»œå®‰å…¨è§’è‰²çš„çŸ¥è¯†åŒ¹é…åº¦è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°ä¸åŒçš„æ¨¡å‹ä¸ä¸åŒçš„è§’è‰²æ›´ä¸ºåŒ¹é…ï¼Œå¦‚GPT-4oæ›´é€‚åˆè°·æ­Œé«˜çº§æƒ…æŠ¥åˆ†æå¸ˆï¼ŒDeepseek-V3æ›´é€‚åˆäºšé©¬é€Šéšç§å·¥ç¨‹å¸ˆã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨é€‰æ‹©å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œéœ€è¦æ ¹æ®ä¸åŒç½‘ç»œå®‰å…¨è§’è‰²çš„ç‰¹å®šçŸ¥è¯†éœ€æ±‚è¿›è¡ŒåŒ¹é…ï¼Œä»¥å®ç°æœ€ä½³æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11783v1">PDF</a> To appear in the IEEE Symposium on Security and Privacy (IEEE S&amp;P)   2025, San Francisco, CA, USA</p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹ç½‘ç»œå®‰å…¨é¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éƒ¨ç½²å¢åŠ çš„ç°çŠ¶ï¼Œéœ€è¦æœ‰æ•ˆçš„æ¨¡å‹é€‰æ‹©åŠè¯„ä¼°æ–¹æ³•ã€‚ä¼ ç»Ÿçš„è¯„ä¼°æ–¹æ³•å¾€å¾€å¿½ç•¥äº†ç½‘ç»œå®‰å…¨ä¸“ä¸šçŸ¥è¯†ç‚¹çš„ç¼ºå¤±ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚å› æ­¤ï¼Œæœ¬æ–‡å¼€å‘äº†åŸºäºè®¤çŸ¥ç§‘å­¦çš„ç²¾ç»†ç½‘ç»œå®‰å…¨è¯„ä¼°æ¡†æ¶CSEBenchmarkï¼ŒåŒ…æ‹¬ä¸‰ç™¾å››åäº”ä¸ªé¢„æœŸç½‘ç»œå®‰å…¨ä¸“å®¶çš„çŸ¥è¯†ç‚¹ã€‚è¯„ä¼°äº†åäºŒä¸ªæµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‘ç°å³ä½¿æœ€å¥½çš„æ¨¡å‹æ•´ä½“å‡†ç¡®ç‡ä¹Ÿåªæœ‰ç™¾åˆ†ä¹‹å…«åäº”ç‚¹å››äºŒï¼Œç‰¹å®šå·¥å…·ä½¿ç”¨å’Œç½•è§å‘½ä»¤ç­‰çŸ¥è¯†ç‚¹ä¸Šå­˜åœ¨æ˜¾è‘—çŸ¥è¯†ç›²åŒºã€‚ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰ç‹¬ç‰¹çš„çŸ¥è¯†ç›²åŒºï¼Œå³ä½¿åŒä¸€ç³»åˆ—çš„å¤§å‹æ¨¡å‹ä¹Ÿå¯èƒ½åœ¨ç‰¹å®šçŸ¥è¯†ç‚¹ä¸Šè¡¨ç°ä¸ä½³ã€‚é€šè¿‡è¯†åˆ«å’Œå¼¥è¡¥æ¯ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ç‰¹å®šçŸ¥è¯†ç›²åŒºï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªç½‘ç»œå®‰å…¨ä»»åŠ¡çš„ä¸‰ä¸ªç°æœ‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¯¹å…ˆå‰é”™è¯¯çš„é¢„æµ‹è¿›è¡Œäº†é«˜è¾¾ç™¾åˆ†ä¹‹å…«åå››çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¯¹æ¯ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸ç‰¹å®šç½‘ç»œå®‰å…¨è§’è‰²çš„çŸ¥è¯†å¯¹é½æƒ…å†µè¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°ä¸åŒæ¨¡å‹ä¸ä¸åŒè§’è‰²çš„å¯¹é½ç¨‹åº¦æœ‰æ‰€ä¸åŒï¼Œä¾‹å¦‚GPT-4oæ›´é€‚ç”¨äºè°·æ­Œé«˜çº§æƒ…æŠ¥åˆ†æå¸ˆï¼Œè€ŒDeepseek-V3æ›´é€‚ç”¨äºäºšé©¬é€Šéšç§å·¥ç¨‹å¸ˆã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†æ ¹æ®ç½‘ç»œå®‰å…¨è§’è‰²çš„ç‰¹å®šçŸ¥è¯†éœ€æ±‚é€‰æ‹©å¤§å‹è¯­è¨€æ¨¡å‹çš„é‡è¦æ€§ï¼Œä»¥å®ç°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œéœ€è¦æœ‰æ•ˆçš„æ¨¡å‹é€‰æ‹©åŠè¯„ä¼°æ–¹æ³•ã€‚</li>
<li>ä¼ ç»Ÿè¯„ä¼°æ–¹æ³•å¿½ç•¥äº†ç½‘ç»œå®‰å…¨ä¸“ä¸šçŸ¥è¯†ç‚¹çš„ç¼ºå¤±ï¼Œå¯¼è‡´æ€§èƒ½å—é™ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªåŸºäºè®¤çŸ¥ç§‘å­¦çš„ç²¾ç»†ç½‘ç»œå®‰å…¨è¯„ä¼°æ¡†æ¶CSEBenchmarkã€‚</li>
<li>åœ¨åäºŒä¸ªæµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°å­˜åœ¨çŸ¥è¯†ç›²åŒºï¼Œä¸”ä¸åŒæ¨¡å‹æœ‰ä¸åŒçš„çŸ¥è¯†ç›²åŒºã€‚</li>
<li>é€šè¿‡è¯†åˆ«å’Œå¼¥è¡¥çŸ¥è¯†ç›²åŒºï¼Œå¯ä»¥åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>æ¯ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸ç‰¹å®šç½‘ç»œå®‰å…¨è§’è‰²çš„çŸ¥è¯†å¯¹é½æƒ…å†µä¸åŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11783">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-315c435ba530ec0f84742f838e5a57e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51f1d877f4afa58c386d1216104c53b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b2273b3beaa24986a45b24f9a4e97a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54deb9f2247e6cd28841a3bec6b77eb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c24774714fc3b20c11bb669e27bfe9e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da401514a4a8a9ac45e2a83d7f5a9f54.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="REAL-Benchmarking-Autonomous-Agents-on-Deterministic-Simulations-of-Real-Websites"><a href="#REAL-Benchmarking-Autonomous-Agents-on-Deterministic-Simulations-of-Real-Websites" class="headerlink" title="REAL: Benchmarking Autonomous Agents on Deterministic Simulations of   Real Websites"></a>REAL: Benchmarking Autonomous Agents on Deterministic Simulations of   Real Websites</h2><p><strong>Authors:Divyansh Garg, Shaun VanWeelden, Diego Caples, Andis Draguns, Nikil Ravi, Pranav Putta, Naman Garg, Tomas Abraham, Michael Lara, Federico Lopez, James Liu, Atharva Gundawar, Prannay Hebbar, Youngchul Joo, Charles London, Christian Schroeder de Witt, Sumeet Motwani</strong></p>
<p>We introduce REAL, a benchmark and framework for multi-turn agent evaluations on deterministic simulations of real-world websites. REAL comprises high-fidelity, deterministic replicas of 11 widely-used websites across domains such as e-commerce, travel, communication, and professional networking. We also release a benchmark consisting of 112 practical tasks that mirror everyday complex user interactions requiring both accurate information retrieval and state-changing actions. All interactions occur within this fully controlled setting, eliminating safety risks and enabling robust, reproducible evaluation of agent capability and reliability. Our novel evaluation framework combines programmatic checks of website state for action-based tasks with rubric-guided LLM-based judgments for information retrieval. The framework supports both open-source and proprietary agent systems through a flexible evaluation harness that accommodates black-box commands within browser environments, allowing research labs to test agentic systems without modification. Our empirical results show that frontier language models achieve at most a 41% success rate on REAL, highlighting critical gaps in autonomous web navigation and task completion capabilities. Our framework supports easy integration of new tasks, reproducible evaluation, and scalable data generation for training web agents. The websites, framework, and leaderboard are available at <a target="_blank" rel="noopener" href="https://realevals.xyz/">https://realevals.xyz</a> and <a target="_blank" rel="noopener" href="https://github.com/agi-inc/REAL">https://github.com/agi-inc/REAL</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»REALï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºçœŸå®ç½‘ç«™ç¡®å®šæ€§æ¨¡æ‹Ÿçš„å¤šè½®ä»£ç†è¯„ä¼°çš„åŸºå‡†å’Œæ¡†æ¶ã€‚REALåŒ…å«äº†é«˜ä¿çœŸã€ç¡®å®šæ€§çš„å¤åˆ¶ç‰ˆæœ¬ï¼Œæ¶µç›–äº†ç”µå­å•†åŠ¡ã€æ—…è¡Œã€é€šä¿¡å’ŒèŒä¸šç½‘ç»œç­‰é¢†åŸŸçš„11ä¸ªå¹¿æ³›ä½¿ç”¨çš„ç½‘ç«™ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†åŒ…å«112ä¸ªå®ç”¨ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œè¿™äº›ä»»åŠ¡åæ˜ äº†æ—¥å¸¸å¤æ‚çš„ç”¨æˆ·äº¤äº’ï¼Œéœ€è¦å‡†ç¡®çš„ä¿¡æ¯æ£€ç´¢å’ŒçŠ¶æ€æ›´æ”¹æ“ä½œã€‚æ‰€æœ‰çš„äº¤äº’éƒ½å‘ç”Ÿåœ¨è¿™ä¸ªå®Œå…¨å—æ§çš„ç¯å¢ƒä¸­ï¼Œæ¶ˆé™¤äº†å®‰å…¨é£é™©ï¼Œå¹¶èƒ½ç¨³å¥åœ°è¯„ä¼°ä»£ç†çš„èƒ½åŠ›å’Œå¯é æ€§ã€‚æˆ‘ä»¬æ–°é¢–çš„è¯„ä»·æ¡†æ¶ç»“åˆäº†åŸºäºç½‘ç«™çŠ¶æ€çš„ç¨‹åºåŒ–æ£€æŸ¥ï¼ˆé’ˆå¯¹åŸºäºè¡ŒåŠ¨çš„ä»»åŠ¡ï¼‰å’ŒåŸºäºè§„åˆ™çš„LLMåˆ¤æ–­ï¼ˆç”¨äºä¿¡æ¯æ£€ç´¢ï¼‰ã€‚è¯¥æ¡†æ¶æ”¯æŒå¼€æºå’Œä¸“æœ‰ä»£ç†ç³»ç»Ÿï¼Œé€šè¿‡ä¸€ä¸ªçµæ´»çš„è¯„ä»·ç³»ç»Ÿï¼Œå¯ä»¥åœ¨æµè§ˆå™¨ç¯å¢ƒä¸­é€‚åº”é»‘ç®±å‘½ä»¤ï¼Œä½¿å¾—ç ”ç©¶å®éªŒå®¤èƒ½å¤Ÿæµ‹è¯•ä»£ç†ç³»ç»Ÿè€Œæ— éœ€ä¿®æ”¹ã€‚æˆ‘ä»¬çš„å®è¯ç»“æœè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹åœ¨REALä¸Šçš„æˆåŠŸç‡æœ€é«˜ä»…ä¸º41%ï¼Œè¿™å‡¸æ˜¾äº†è‡ªä¸»ç½‘é¡µå¯¼èˆªå’Œä»»åŠ¡å®Œæˆèƒ½åŠ›æ–¹é¢çš„å…³é”®å·®è·ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ”¯æŒè½»æ¾é›†æˆæ–°ä»»åŠ¡ã€å¯é‡å¤è¯„ä¼°å’Œå¯æ‰©å±•çš„æ•°æ®ç”Ÿæˆï¼Œä»¥è®­ç»ƒç½‘ç»œä»£ç†ã€‚ç½‘ç«™ã€æ¡†æ¶å’Œæ’è¡Œæ¦œå¯åœ¨<a target="_blank" rel="noopener" href="https://realevals.xyzå’Œhttps//github.com/agi-inc/REAL%E6%89%BE%E5%88%B0%E3%80%82">https://realevals.xyzå’Œhttps://github.com/agi-inc/REALæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11543v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€ä¸ªå…³äºREALæ¡†æ¶çš„ä»‹ç»ï¼Œå®ƒæ˜¯ä¸€ä¸ªç”¨äºåœ¨çœŸå®ç½‘ç«™ç¡®å®šæ€§æ¨¡æ‹Ÿä¸Šè¿›è¡Œå¤šè½®ä»£ç†è¯„ä¼°çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚REALåŒ…å«11ä¸ªé«˜ä¿çœŸç¡®å®šæ€§å‰¯æœ¬çš„å¹¿æ³›ä½¿ç”¨çš„ç½‘ç«™ï¼Œå¹¶å‘å¸ƒäº†ä¸€ä¸ªåŒ…å«112ä¸ªå®ç”¨ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œè¿™äº›ä»»åŠ¡åæ˜ äº†æ—¥å¸¸å¤æ‚çš„ç”¨æˆ·äº¤äº’ï¼Œéœ€è¦å‡†ç¡®çš„ä¿¡æ¯æ£€ç´¢å’ŒçŠ¶æ€æ›´æ”¹æ“ä½œã€‚åœ¨è¯¥å®Œå…¨å—æ§çš„ç¯å¢ƒä¸­å‘ç”Ÿæ‰€æœ‰äº¤äº’ï¼Œæ¶ˆé™¤äº†å®‰å…¨é£é™©ï¼Œå¹¶å®ç°äº†å¯¹ä»£ç†èƒ½åŠ›å’Œå¯é æ€§çš„ç¨³å¥å’Œå¯é‡å¤è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>REALæ˜¯ä¸€ä¸ªç”¨äºå¤šè½®ä»£ç†è¯„ä¼°çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„ç½‘ç«™ç¯å¢ƒã€‚</li>
<li>REALåŒ…å«11ä¸ªä¸åŒé¢†åŸŸç½‘ç«™çš„ç¡®å®šæ€§å‰¯æœ¬ï¼Œå¹¶å‘å¸ƒäº†åŒ…å«112ä¸ªå®ç”¨ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>REALçš„è¯„ä¼°æ¡†æ¶ç»“åˆäº†ç½‘ç«™çŠ¶æ€çš„ç¨‹åºåŒ–æ£€æŸ¥ä»¥åŠåŸºäºLLMçš„è¯„åˆ¤æ ‡å‡†ã€‚</li>
<li>è¯¥æ¡†æ¶æ”¯æŒå¼€æºå’Œä¸“æœ‰ä»£ç†ç³»ç»Ÿï¼Œå¹¶å…è®¸ç ”ç©¶å®éªŒå®¤æµ‹è¯•ä»£ç†ç³»ç»Ÿè€Œæ— éœ€ä¿®æ”¹ã€‚</li>
<li>å‰æ²¿çš„è¯­è¨€æ¨¡å‹åœ¨REALä¸Šçš„æˆåŠŸç‡ä»…ä¸º41%ï¼Œè¡¨æ˜åœ¨è‡ªä¸»ç½‘é¡µå¯¼èˆªå’Œä»»åŠ¡å®Œæˆèƒ½åŠ›æ–¹é¢å­˜åœ¨å…³é”®å·®è·ã€‚</li>
<li>REALæ¡†æ¶æ”¯æŒæ–°ä»»åŠ¡çš„è½»æ¾é›†æˆã€å¯é‡å¤è¯„ä¼°å’Œç”¨äºè®­ç»ƒç½‘é¡µä»£ç†çš„å¯æ‰©å±•æ•°æ®ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11543">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b2089e84f6bba8c84e814ebc04819e15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62681ad777dd6cb34bb1abdacc6a99df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acd0e40283c79364d0205c86a2ca3f53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce7baae5c4d9d27642c19d80d9591f58.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ReTool-Reinforcement-Learning-for-Strategic-Tool-Use-in-LLMs"><a href="#ReTool-Reinforcement-Learning-for-Strategic-Tool-Use-in-LLMs" class="headerlink" title="ReTool: Reinforcement Learning for Strategic Tool Use in LLMs"></a>ReTool: Reinforcement Learning for Strategic Tool Use in LLMs</h2><p><strong>Authors:Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, Wanjun Zhong</strong></p>
<p>While reasoning models (e.g., DeepSeek R1) trained with reinforcement learning (RL), excel in textual reasoning, they struggle in scenarios requiring structured problem-solving, such as geometric reasoning, concise computation, or complex equation solving-areas where computational tools like code interpreters (CI) demonstrate distinct advantages. To bridge this gap, we propose ReTool, which enhances long-form reasoning with tool-integrated learning, including two key features: (1) dynamic interleaving of real-time code execution within natural language reasoning processes, and (2) an automated RL paradigm that allows policy rollouts with multi-turn real-time code execution and teaches the model in learning when and how to invoke tools based on outcome feedback. ReTool employs a systematic training framework, beginning with synthetic cold-start data generation to produce code-augmented long-form reasoning traces for fine-tuning base models. Subsequent RL training leverages task outcomes as rewards to iteratively refine the modelâ€™s tool use strategy, enabling autonomous discovery of optimal tool invocation patterns without human priors. Experiments on the challenging MATH Olympiad benchmark AIME demonstrate ReToolâ€™s superiority: Our 32B model achieves 67% accuracy with 400 training steps, outperforming text-based RL baseline (40% accuracy, 1080 steps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5% accuracy in extended settings, surpassing OpenAIâ€™s o1-preview by 27.9%. Further analysis reveals emergent behaviors such as code self-correction, signaling an â€˜â€™aha momentâ€™â€™ in which the model autonomously masters adaptive tool use. These findings highlight the promise of outcome-driven tool integration for advancing complex mathematical reasoning and offer new insights into hybrid neuro-symbolic systems. </p>
<blockquote>
<p>è™½ç„¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„æ¨ç†æ¨¡å‹ï¼ˆä¾‹å¦‚DeepSeek R1ï¼‰åœ¨æ–‡æœ¬æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦ç»“æ„åŒ–é—®é¢˜è§£å†³ï¼ˆå¦‚å‡ ä½•æ¨ç†ã€ç®€æ´è®¡ç®—æˆ–å¤æ‚æ–¹ç¨‹æ±‚è§£ï¼‰çš„åœºæ™¯ä¸­å´é‡åˆ°å›°éš¾ï¼Œè¿™äº›åœºæ™¯æ˜¯ä»£ç è§£é‡Šå™¨ï¼ˆCIï¼‰ç­‰è®¡ç®—å·¥å…·å‘æŒ¥æ˜æ˜¾ä¼˜åŠ¿çš„åœ°æ–¹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ReToolï¼Œå®ƒé€šè¿‡å·¥å…·é›†æˆå­¦ä¹ å¢å¼ºé•¿å½¢å¼æ¨ç†ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®åŠŸèƒ½ï¼šï¼ˆ1ï¼‰åœ¨è‡ªç„¶è¯­è¨€æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€äº¤ç»‡å®æ—¶ä»£ç æ‰§è¡Œï¼›ï¼ˆ2ï¼‰ä¸€ç§è‡ªåŠ¨åŒ–RLèŒƒå¼ï¼Œå…è®¸å¤šè½®å®æ—¶ä»£ç æ‰§è¡Œçš„ç­–ç•¥å±•å¼€ï¼Œå¹¶æ ¹æ®ç»“æœåé¦ˆæ¥æ•™æˆæ¨¡å‹ä½•æ—¶ä»¥åŠå¦‚ä½•è°ƒç”¨å·¥å…·ã€‚ReToolé‡‡ç”¨ç³»ç»Ÿçš„è®­ç»ƒæ¡†æ¶ï¼Œä»ç”Ÿæˆåˆæˆå†·å¯åŠ¨æ•°æ®å¼€å§‹ï¼Œä»¥äº§ç”Ÿç”¨äºå¾®è°ƒåŸºç¡€æ¨¡å‹çš„ä»£ç å¢å¼ºé•¿å½¢å¼æ¨ç†è½¨è¿¹ã€‚éšåçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒåˆ©ç”¨ä»»åŠ¡ç»“æœä½œä¸ºå¥–åŠ±æ¥è¿­ä»£ä¼˜åŒ–æ¨¡å‹çš„å·¥å…·ä½¿ç”¨ç­–ç•¥ï¼Œä»è€Œèƒ½å¤Ÿè‡ªä¸»å‘ç°æœ€ä½³å·¥å…·è°ƒç”¨æ¨¡å¼ï¼Œæ— éœ€äººä¸ºå…ˆéªŒçŸ¥è¯†ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„MATH OlympiadåŸºå‡†æµ‹è¯•AIMEä¸Šçš„å®éªŒè¯æ˜äº†ReToolçš„ä¼˜è¶Šæ€§ï¼šæˆ‘ä»¬çš„32Bæ¨¡å‹åœ¨400æ­¥è®­ç»ƒåè¾¾åˆ°67%çš„å‡†ç¡®ç‡ï¼Œåœ¨æ•ˆç‡å’Œæ€§èƒ½ä¸Šè¶…è¶Šäº†åŸºäºæ–‡æœ¬çš„RLåŸºçº¿ï¼ˆå‡†ç¡®ç‡ä¸º40%ï¼Œè®­ç»ƒæ­¥éª¤ä¸º1080æ­¥ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒReTool-32Båœ¨æ‰©å±•è®¾ç½®ä¸­çš„å‡†ç¡®ç‡è¾¾åˆ°äº†72.5%ï¼Œæ¯”OpenAIçš„o1-previewé«˜å‡º27.9%ã€‚è¿›ä¸€æ­¥çš„åˆ†ææ˜¾ç¤ºå‡ºç°äº†ä»£ç è‡ªæˆ‘ä¿®æ­£ç­‰çªå‘è¡Œä¸ºï¼Œè¿™è¡¨æ˜æ¨¡å‹è‡ªä¸»æŒæ¡äº†é€‚åº”æ€§å·¥å…·ä½¿ç”¨ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œé¡¿æ‚Ÿæ—¶åˆ»â€ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†ä»¥ç»“æœé©±åŠ¨çš„å·¥å…·é›†æˆåœ¨æ¨åŠ¨å¤æ‚æ•°å­¦æ¨ç†æ–¹é¢çš„å‰æ™¯ï¼Œå¹¶ä¸ºæ··åˆç¥ç»ç¬¦å·ç³»ç»Ÿæä¾›äº†æ–°çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11536v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åä¸ºReToolçš„æ–°æ–¹æ³•ï¼Œç”¨äºæå‡æ–‡æœ¬æ¨ç†æ¨¡å‹ä¸­å·¥å…·é›†æˆå­¦ä¹ çš„æ•ˆç‡ä¸æ€§èƒ½ã€‚ReToolå…·å¤‡ä¸¤å¤§æ ¸å¿ƒåŠŸèƒ½ï¼šå®æ—¶ä»£ç æ‰§è¡Œä¸è‡ªç„¶è¯­è¨€æ¨ç†è¿‡ç¨‹çš„åŠ¨æ€äº¤ç»‡ï¼Œä»¥åŠåŸºäºç»“æœåé¦ˆçš„è‡ªåŠ¨åŒ–å¼ºåŒ–å­¦ä¹ æ¨¡å¼ã€‚ReToolé€šè¿‡ç³»ç»Ÿè®­ç»ƒæ¡†æ¶ï¼Œé¦–å…ˆä½¿ç”¨åˆæˆå†·å¯åŠ¨æ•°æ®ç”Ÿæˆä»£ç å¢å¼ºé•¿æ–‡æœ¬æ¨ç†è½¨è¿¹ä»¥å¾®è°ƒåŸºç¡€æ¨¡å‹ã€‚åç»­çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒåˆ©ç”¨ä»»åŠ¡ç»“æœä½œä¸ºå¥–åŠ±ï¼Œè¿­ä»£ä¼˜åŒ–æ¨¡å‹å·¥å…·ä½¿ç”¨ç­–ç•¥ï¼Œå®ç°è‡ªä¸»å‘ç°æœ€ä¼˜å·¥å…·è°ƒç”¨æ¨¡å¼è€Œæ— éœ€äººä¸ºå…ˆéªŒã€‚åœ¨MATH OlympiadåŸºå‡†æµ‹è¯•AIMEä¸Šï¼ŒReToolè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå…¶32Bæ¨¡å‹åœ¨ä»…400æ­¥è®­ç»ƒåè¾¾åˆ°67%çš„å‡†ç¡®ç‡ï¼Œä¸ä»…åœ¨æ•ˆç‡ä¸æ€§èƒ½ä¸Šè¶…è¶ŠåŸºäºæ–‡æœ¬çš„RLåŸºçº¿ï¼Œæ›´ä»¥72.5%çš„å‡†ç¡®ç‡åœ¨æ‰©å±•è®¾ç½®ä¸­è¶…è¶ŠOpenAIçš„o1-previewã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReToolæ–¹æ³•èƒ½æœ‰æ•ˆç»“åˆå·¥å…·é›†æˆå­¦ä¹ ï¼Œæå‡æ–‡æœ¬æ¨ç†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ReToolå…·å¤‡å®æ—¶ä»£ç æ‰§è¡Œä¸è‡ªç„¶è¯­è¨€æ¨ç†çš„åŠ¨æ€äº¤ç»‡åŠŸèƒ½ã€‚</li>
<li>é€šè¿‡åˆæˆå†·å¯åŠ¨æ•°æ®ç”Ÿæˆï¼ŒReToolå¯å¾®è°ƒåŸºç¡€æ¨¡å‹ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ è®­ç»ƒä½¿æ¨¡å‹èƒ½åŸºäºä»»åŠ¡ç»“æœè‡ªä¸»å‘ç°æœ€ä¼˜å·¥å…·è°ƒç”¨æ¨¡å¼ã€‚</li>
<li>åœ¨MATH OlympiadåŸºå‡†æµ‹è¯•AIMEä¸Šï¼ŒReToolè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ReToolçš„32Bæ¨¡å‹åœ¨å°‘é‡è®­ç»ƒæ­¥éª¤åè¾¾åˆ°é«˜å‡†ç¡®ç‡ã€‚</li>
<li>ReToolå±•ç°å‡ºè‡ªä¸»æŒæ¡é€‚åº”æ€§å·¥å…·ä½¿ç”¨çš„æ½œåŠ›ï¼Œå¦‚ä»£ç è‡ªæˆ‘æ ¡æ­£åŠŸèƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11536">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-47dd891199161b74cd58ce8a32d2dc86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-953adfbbde6bfedaa9b3b98c71097382.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e603c957bdaa1434ea1d49b75fc56f23.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SFT-or-RL-An-Early-Investigation-into-Training-R1-Like-Reasoning-Large-Vision-Language-Models"><a href="#SFT-or-RL-An-Early-Investigation-into-Training-R1-Like-Reasoning-Large-Vision-Language-Models" class="headerlink" title="SFT or RL? An Early Investigation into Training R1-Like Reasoning Large   Vision-Language Models"></a>SFT or RL? An Early Investigation into Training R1-Like Reasoning Large   Vision-Language Models</h2><p><strong>Authors:Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, Cihang Xie</strong></p>
<p>This work revisits the dominant supervised fine-tuning (SFT) then reinforcement learning (RL) paradigm for training Large Vision-Language Models (LVLMs), and reveals a key finding: SFT can significantly undermine subsequent RL by inducing &#96;&#96;pseudo reasoning pathsâ€™â€™ imitated from expert models. While these paths may resemble the native reasoning paths of RL models, they often involve prolonged, hesitant, less informative steps, and incorrect reasoning. To systematically study this effect, we introduce VLAA-Thinking, a new multimodal dataset designed to support reasoning in LVLMs. Constructed via a six-step pipeline involving captioning, reasoning distillation, answer rewrite and verification, VLAA-Thinking comprises high-quality, step-by-step visual reasoning traces for SFT, along with a more challenging RL split from the same data source. Using this dataset, we conduct extensive experiments comparing SFT, RL and their combinations. Results show that while SFT helps models learn reasoning formats, it often locks aligned models into imitative, rigid reasoning modes that impede further learning. In contrast, building on the Group Relative Policy Optimization (GRPO) with a novel mixed reward module integrating both perception and cognition signals, our RL approach fosters more genuine, adaptive reasoning behavior. Notably, our model VLAA-Thinker, based on Qwen2.5VL 3B, achieves top-1 performance on Open LMM Reasoning Leaderboard (<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/opencompass/Open_LMM_Reasoning_Leaderboard">https://huggingface.co/spaces/opencompass/Open_LMM_Reasoning_Leaderboard</a>) among 4B scale LVLMs, surpassing the previous state-of-the-art by 1.8%. We hope our findings provide valuable insights in developing reasoning-capable LVLMs and can inform future research in this area. </p>
<blockquote>
<p>è¿™ç¯‡å·¥ä½œé‡æ–°ç ”ç©¶äº†æµè¡Œçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ç„¶åå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èŒƒå¼è®­ç»ƒå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æ–¹æ³•ï¼Œå¹¶æ­ç¤ºäº†ä¸€ä¸ªå…³é”®å‘ç°ï¼šSFTä¼šé€šè¿‡æ¨¡ä»¿ä¸“å®¶æ¨¡å‹äº§ç”Ÿâ€œä¼ªæ¨ç†è·¯å¾„â€ï¼Œä»è€Œæ˜¾è‘—ç ´åéšåçš„RLã€‚è™½ç„¶è¿™äº›è·¯å¾„å¯èƒ½ç±»ä¼¼äºRLæ¨¡å‹çš„åŸç”Ÿæ¨ç†è·¯å¾„ï¼Œä½†å®ƒä»¬é€šå¸¸æ¶‰åŠå†—é•¿ã€çŠ¹è±«ã€ä¿¡æ¯è¾ƒå°‘çš„æ­¥éª¤ä»¥åŠé”™è¯¯çš„æ¨ç†ã€‚ä¸ºäº†ç³»ç»Ÿåœ°ç ”ç©¶è¿™ç§å½±å“ï¼Œæˆ‘ä»¬å¼•å…¥äº†VLAA-Thinkingï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤šæ¨¡å¼æ•°æ®é›†ï¼Œæ—¨åœ¨æ”¯æŒLVLMsçš„æ¨ç†ã€‚VLAA-Thinkingæ˜¯é€šè¿‡åŒ…æ‹¬æè¿°ã€æ¨ç†è’¸é¦ã€ç­”æ¡ˆé‡å†™å’ŒéªŒè¯åœ¨å†…çš„å…­æ­¥ç®¡é“æ„å»ºçš„ï¼Œå®ƒåŒ…å«äº†é«˜è´¨é‡ã€ä¸€æ­¥ä¸€æ­¥çš„è§†è§‰æ¨ç†è½¨è¿¹çš„SFTï¼Œä»¥åŠæ¥è‡ªåŒä¸€æ•°æ®æºçš„æ›´å…·æŒ‘æˆ˜æ€§çš„RLåˆ†å‰²ã€‚ä½¿ç”¨æ­¤æ•°æ®é›†ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œæ¯”è¾ƒäº†SFTã€RLä»¥åŠå®ƒä»¬çš„ç»„åˆã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶SFTæœ‰åŠ©äºæ¨¡å‹å­¦ä¹ æ¨ç†æ ¼å¼ï¼Œä½†å®ƒå¾€å¾€ä½¿å¯¹é½çš„æ¨¡å‹é™·å…¥æ¨¡ä»¿ã€åƒµåŒ–çš„æ¨ç†æ¨¡å¼ï¼Œé˜»ç¢è¿›ä¸€æ­¥å­¦ä¹ ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„RLæ–¹æ³•å»ºç«‹åœ¨é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ä¹‹ä¸Šï¼Œé‡‡ç”¨æ··åˆå¥–åŠ±æ¨¡å—æ•´åˆæ„ŸçŸ¥å’Œè®¤çŸ¥ä¿¡å·ï¼Œä¿ƒè¿›äº†æ›´çœŸå®ã€è‡ªé€‚åº”çš„æ¨ç†è¡Œä¸ºã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„VLAA-Thinkeræ¨¡å‹ï¼ŒåŸºäºQwen2.5VL 3Bï¼Œåœ¨Open LMMæ¨ç†æ’è¡Œæ¦œï¼ˆ<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/opencompass/Open_LMM_Reasoning_Leaderboard%EF%BC%89%E4%B8%8A%E5%8F%96%E5%BE%97%E4%BA%864B%E8%A7%84%E6%A8%A1LVLMs%E7%9A%84%E7%AC%AC%E4%B8%80%E5%90%8D%EF%BC%8C%E8%B6%85%E8%BF%87%E4%BA%86%E4%B9%8B%E5%89%8D%E7%9A%84%E6%9C%80%E4%BD%B3%E7%8A%B6%E6%80%811.8%%E3%80%82%E6%88%91%E4%BB%AC%E5%B8%8C%E6%9C%9B%E6%88%91%E4%BB%AC%E7%9A%84%E7%A0%94%E7%A9%B6%E4%B8%BA%E5%BC%80%E5%8F%91%E5%85%B7%E6%9C%89%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B%E7%9A%84%E5%A4%A7%E5%9E%8B%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%8F%90%E4%BE%9B%E6%9C%89%E4%BB%B7%E5%80%BC%E7%9A%84%E8%A7%81%E8%A7%A3%EF%BC%8C%E5%B9%B6%E4%B8%BA%E8%BF%99%E4%B8%80%E9%A2%86%E5%9F%9F%E7%9A%84%E6%9C%AA%E6%9D%A5%E7%A0%94%E7%A9%B6%E6%8F%90%E4%BE%9B%E4%BF%A1%E6%81%AF%E5%8F%82%E8%80%83%E3%80%82">https://huggingface.co/spaces/opencompass/Open_LMM_Reasoning_Leaderboardï¼‰ä¸Šå–å¾—äº†4Bè§„æ¨¡LVLMsçš„ç¬¬ä¸€åï¼Œè¶…è¿‡äº†ä¹‹å‰çš„æœ€ä½³çŠ¶æ€1.8%ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå¼€å‘å…·æœ‰æ¨ç†èƒ½åŠ›çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æä¾›æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶ä¸ºè¿™ä¸€é¢†åŸŸçš„æœªæ¥ç ”ç©¶æä¾›ä¿¡æ¯å‚è€ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11468v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä¸»æµçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æ–¹æ³•ï¼Œå¹¶å‘ç°SFTä¼šå¯¹åç»­çš„RLäº§ç”Ÿè´Ÿé¢å½±å“ï¼Œå› ä¸ºå®ƒä¼šè¯±å¯¼å‡ºæ¨¡ä»¿ä¸“å®¶æ¨¡å‹çš„â€œä¼ªæ¨ç†è·¯å¾„â€ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†VLAA-Thinkingå¤šæ¨¡æ€æ•°æ®é›†ï¼Œç”¨äºæ”¯æŒLVLMsçš„æ¨ç†ã€‚é€šè¿‡ä¸€ç³»åˆ—æ­¥éª¤ï¼Œè¯¥æ•°æ®é›†æä¾›äº†é«˜è´¨é‡ã€åˆ†æ­¥éª¤çš„è§†è§‰æ¨ç†è½¨è¿¹ï¼Œç”¨äºSFTï¼Œä»¥åŠæ¥è‡ªåŒä¸€æ•°æ®æºçš„æ›´å…·æŒ‘æˆ˜æ€§çš„RLåˆ†å‰²ã€‚å®éªŒè¡¨æ˜ï¼Œè™½ç„¶SFTæœ‰åŠ©äºæ¨¡å‹å­¦ä¹ æ¨ç†æ ¼å¼ï¼Œä½†å®ƒå¾€å¾€ä½¿æ¨¡å‹é™·å…¥æ¨¡ä»¿æ€§çš„ã€åƒµåŒ–çš„æ¨ç†æ¨¡å¼ï¼Œé˜»ç¢äº†è¿›ä¸€æ­¥çš„å­¦ä¹ ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºGroup Relative Policy Optimizationï¼ˆGRPOï¼‰å’Œæ··åˆå¥–åŠ±æ¨¡å—çš„RLæ–¹æ³•ï¼Œç»“åˆäº†æ„ŸçŸ¥å’Œè®¤çŸ¥ä¿¡å·ï¼Œä¿ƒè¿›äº†æ›´çœŸå®ã€è‡ªé€‚åº”çš„æ¨ç†è¡Œä¸ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SFTåœ¨è®­ç»ƒLVLMsæ—¶ä¼šè¯±å¯¼å‡ºæ¨¡ä»¿ä¸“å®¶æ¨¡å‹çš„â€œä¼ªæ¨ç†è·¯å¾„â€ï¼Œè¿™å¯èƒ½é˜»ç¢åç»­çš„RLè®­ç»ƒã€‚</li>
<li>VLAA-Thinkingæ•°æ®é›†æ˜¯ä¸ºäº†æ”¯æŒLVLMsçš„æ¨ç†è€Œå¼•å…¥çš„ï¼Œå®ƒæä¾›äº†é«˜è´¨é‡çš„è§†è§‰æ¨ç†è½¨è¿¹ï¼Œå¹¶åˆ†ä¸ºç”¨äºSFTå’Œæ›´å…·æŒ‘æˆ˜æ€§çš„RLåˆ†å‰²ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè™½ç„¶SFTæœ‰åŠ©äºæ¨¡å‹å­¦ä¹ æ¨ç†æ ¼å¼ï¼Œä½†å®ƒå¯èƒ½å¯¼è‡´æ¨¡å‹é™·å…¥åƒµåŒ–çš„æ¨ç†æ¨¡å¼ã€‚</li>
<li>åŸºäºGRPOå’Œæ··åˆå¥–åŠ±æ¨¡å—çš„RLæ–¹æ³•èƒ½å¤Ÿä¿ƒè¿›æ›´çœŸå®ã€è‡ªé€‚åº”çš„æ¨ç†è¡Œä¸ºã€‚</li>
<li>è¯¥æ–‡æå‡ºçš„æ¨¡å‹VLAA-Thinkeråœ¨Open LMM Reasoning Leaderboardä¸Šå–å¾—äº†é¡¶å°–æ€§èƒ½ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„çŠ¶æ€ã€‚</li>
<li>ä½œè€…çš„å‘ç°å¯¹äºå¼€å‘å…·å¤‡æ¨ç†èƒ½åŠ›çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å…·æœ‰å®è´µçš„è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11468">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e707eb82b82e4815493408ecbf474a1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58377b173f44b0937db39c2022867302.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24c677cc7ab1dfa4eff75fde10020f1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-582f89360ec8a47113f9f1834ec7b55d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdf73ef18f3f964feddf087d310b5b62.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Diffusion-Distillation-With-Direct-Preference-Optimization-For-Efficient-3D-LiDAR-Scene-Completion"><a href="#Diffusion-Distillation-With-Direct-Preference-Optimization-For-Efficient-3D-LiDAR-Scene-Completion" class="headerlink" title="Diffusion Distillation With Direct Preference Optimization For Efficient   3D LiDAR Scene Completion"></a>Diffusion Distillation With Direct Preference Optimization For Efficient   3D LiDAR Scene Completion</h2><p><strong>Authors:An Zhao, Shengyuan Zhang, Ling Yang, Zejian Li, Jiale Wu, Haoran Xu, AnYang Wei, Perry Pengyun GU, Lingyun Sun</strong></p>
<p>The application of diffusion models in 3D LiDAR scene completion is limited due to diffusionâ€™s slow sampling speed. Score distillation accelerates diffusion sampling but with performance degradation, while post-training with direct policy optimization (DPO) boosts performance using preference data. This paper proposes Distillation-DPO, a novel diffusion distillation framework for LiDAR scene completion with preference aligment. First, the student model generates paired completion scenes with different initial noises. Second, using LiDAR scene evaluation metrics as preference, we construct winning and losing sample pairs. Such construction is reasonable, since most LiDAR scene metrics are informative but non-differentiable to be optimized directly. Third, Distillation-DPO optimizes the student model by exploiting the difference in score functions between the teacher and student models on the paired completion scenes. Such procedure is repeated until convergence. Extensive experiments demonstrate that, compared to state-of-the-art LiDAR scene completion diffusion models, Distillation-DPO achieves higher-quality scene completion while accelerating the completion speed by more than 5-fold. Our method is the first to explore adopting preference learning in distillation to the best of our knowledge and provide insights into preference-aligned distillation. Our code is public available on <a target="_blank" rel="noopener" href="https://github.com/happyw1nd/DistillationDPO">https://github.com/happyw1nd/DistillationDPO</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨3Dæ¿€å…‰é›·è¾¾åœºæ™¯è¡¥å…¨ä¸­çš„åº”ç”¨å—é™äºå…¶ç¼“æ…¢çš„é‡‡æ ·é€Ÿåº¦ã€‚åˆ†æ•°è’¸é¦è™½ç„¶å¯ä»¥åŠ é€Ÿæ‰©æ•£é‡‡æ ·ï¼Œä½†ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œè€Œåç”¨ç›´æ¥ç­–ç•¥ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œè®­ç»ƒåˆ™å¯ä»¥åˆ©ç”¨åå¥½æ•°æ®æå‡æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºDistillation-DPOçš„æ–°å‹æ‰©æ•£è’¸é¦æ¡†æ¶ï¼Œç”¨äºæ¿€å…‰é›·è¾¾åœºæ™¯è¡¥å…¨åå¥½å¯¹é½ã€‚é¦–å…ˆï¼Œå­¦ç”Ÿæ¨¡å‹ç”Ÿæˆå…·æœ‰ä¸åŒåˆå§‹å™ªå£°çš„é…å¯¹å®Œæˆåœºæ™¯ã€‚å…¶æ¬¡ï¼Œä»¥æ¿€å…‰é›·è¾¾åœºæ™¯è¯„ä¼°æŒ‡æ ‡ä¸ºåå¥½ï¼Œæˆ‘ä»¬æ„å»ºäº†èƒœè€…å’Œè´¥è€…æ ·æœ¬å¯¹ã€‚è¿™ç§æ„å»ºæ˜¯åˆç†çš„ï¼Œå› ä¸ºå¤§å¤šæ•°æ¿€å…‰é›·è¾¾åœºæ™¯æŒ‡æ ‡éƒ½æ˜¯æœ‰ç”¨çš„ä½†ä¸å¯ç›´æ¥ä¼˜åŒ–ã€‚ç¬¬ä¸‰ï¼ŒDistillation-DPOé€šè¿‡åˆ©ç”¨é…å¯¹å®Œæˆåœºæ™¯ä¸Šæ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„åˆ†æ•°å‡½æ•°çš„å·®å¼‚æ¥ä¼˜åŒ–å­¦ç”Ÿæ¨¡å‹ã€‚è¿™ç§ç¨‹åºä¼šé‡å¤è¿›è¡Œç›´è‡³æ”¶æ•›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ¿€å…‰é›·è¾¾åœºæ™¯è¡¥å…¨æ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼ŒDistillation-DPOåœ¨åŠ é€Ÿè¡¥å…¨é€Ÿåº¦è¶…è¿‡5å€çš„åŒæ—¶å®ç°äº†æ›´é«˜è´¨é‡çš„åœºæ™¯è¡¥å…¨ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯ç¬¬ä¸€ä¸ªå°è¯•åœ¨è’¸é¦è¿‡ç¨‹ä¸­é‡‡ç”¨åå¥½å­¦ä¹ ï¼Œå¹¶ä¸ºåå¥½å¯¹é½çš„è’¸é¦æä¾›äº†è§è§£ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/happyw1nd/DistillationDPO%E4%B8%8A%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/happyw1nd/DistillationDPOä¸Šå¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11447v2">PDF</a> Our code is public available on   <a target="_blank" rel="noopener" href="https://github.com/happyw1nd/DistillationDPO">https://github.com/happyw1nd/DistillationDPO</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨3Dæ¿€å…‰é›·è¾¾åœºæ™¯è¡¥å…¨ä¸­çš„åº”ç”¨å—é™äºå…¶ç¼“æ…¢çš„é‡‡æ ·é€Ÿåº¦ã€‚å¾—åˆ†è’¸é¦æŠ€æœ¯å¯ä»¥åŠ é€Ÿæ‰©æ•£é‡‡æ ·ï¼Œä½†ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œè€Œåé€šè¿‡ç›´æ¥ç­–ç•¥ä¼˜åŒ–ï¼ˆDPOï¼‰ä½¿ç”¨åå¥½æ•°æ®æå‡æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºDistillation-DPOçš„æ–°å‹æ‰©æ•£è’¸é¦æ¡†æ¶ï¼Œç”¨äºæ¿€å…‰é›·è¾¾åœºæ™¯è¡¥å…¨ï¼Œå…·æœ‰åå¥½å¯¹é½åŠŸèƒ½ã€‚é¦–å…ˆï¼Œå­¦ç”Ÿæ¨¡å‹ç”Ÿæˆå¸¦æœ‰ä¸åŒåˆå§‹å™ªå£°çš„é…å¯¹å®Œæˆåœºæ™¯ã€‚å…¶æ¬¡ï¼Œä»¥æ¿€å…‰é›·è¾¾åœºæ™¯è¯„ä¼°æŒ‡æ ‡ä¸ºåå¥½ï¼Œæ„å»ºèƒœè€…å’Œè´¥è€…æ ·æœ¬å¯¹ã€‚è¿™ç§æ„å»ºæ˜¯åˆç†çš„ï¼Œå› ä¸ºå¤§å¤šæ•°æ¿€å…‰é›·è¾¾åœºæ™¯æŒ‡æ ‡éƒ½æ˜¯å…·æœ‰ä¿¡æ¯é‡çš„ï¼Œä½†æ— æ³•ç›´æ¥ä¼˜åŒ–ã€‚æœ€åï¼ŒDistillation-DPOé€šè¿‡ä¼˜åŒ–é…å¯¹å®Œæˆåœºæ™¯ä¸Šæ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹å¾—åˆ†å‡½æ•°çš„å·®å¼‚æ¥ä¼˜åŒ–å­¦ç”Ÿæ¨¡å‹ã€‚è¿™ç§è¿‡ç¨‹ä¼šä¸æ–­é‡å¤ç›´è‡³æ”¶æ•›ã€‚å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ¿€å…‰é›·è¾¾åœºæ™¯è¡¥å…¨æ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼ŒDistillation-DPOåœ¨æé«˜åœºæ™¯è¡¥å…¨è´¨é‡çš„åŒæ—¶ï¼Œå°†è¡¥å…¨é€Ÿåº¦æé«˜äº†5å€ä»¥ä¸Šã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯é¦–æ¬¡æ¢ç´¢é‡‡ç”¨åå¥½å­¦ä¹ è¿›è¡Œè’¸é¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨3Dæ¿€å…‰é›·è¾¾åœºæ™¯è¡¥å…¨ä¸­é¢ä¸´é‡‡æ ·é€Ÿåº¦æ…¢çš„é™åˆ¶ã€‚</li>
<li>å¾—åˆ†è’¸é¦æŠ€æœ¯è™½èƒ½åŠ é€Ÿæ‰©æ•£é‡‡æ ·ï¼Œä½†ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>ç›´æ¥ç­–ç•¥ä¼˜åŒ–ï¼ˆDPOï¼‰åˆ©ç”¨åå¥½æ•°æ®æå‡æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„æ‰©æ•£è’¸é¦æ¡†æ¶Distillation-DPOï¼Œç”¨äºæ¿€å…‰é›·è¾¾åœºæ™¯è¡¥å…¨ï¼Œå…·å¤‡åå¥½å¯¹é½åŠŸèƒ½ã€‚</li>
<li>Distillation-DPOé€šè¿‡æ„å»ºèƒœè€…å’Œè´¥è€…æ ·æœ¬å¯¹ï¼Œä¼˜åŒ–å­¦ç”Ÿæ¨¡å‹ã€‚</li>
<li>Distillation-DPOåœ¨æé«˜åœºæ™¯è¡¥å…¨è´¨é‡çš„åŒæ—¶ï¼Œå¤§å¹…åŠ é€Ÿäº†è¡¥å…¨é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11447">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f2bb3c2d8b9b6bafc86f5e1b97a315cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1500980a1fee5ca0505f9e6ae85b30f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d9db7427d87df12ae8b42fb6654113d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a044c5542b6839b43f2118a0ef1879bb.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="3DAffordSplat-Efficient-Affordance-Reasoning-with-3D-Gaussians"><a href="#3DAffordSplat-Efficient-Affordance-Reasoning-with-3D-Gaussians" class="headerlink" title="3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians"></a>3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians</h2><p><strong>Authors:Zeming Wei, Junyi Lin, Yang Liu, Weixing Chen, Jingzhou Luo, Guanbin Li, Liang Lin</strong></p>
<p>3D affordance reasoning is essential in associating human instructions with the functional regions of 3D objects, facilitating precise, task-oriented manipulations in embodied AI. However, current methods, which predominantly depend on sparse 3D point clouds, exhibit limited generalizability and robustness due to their sensitivity to coordinate variations and the inherent sparsity of the data. By contrast, 3D Gaussian Splatting (3DGS) delivers high-fidelity, real-time rendering with minimal computational overhead by representing scenes as dense, continuous distributions. This positions 3DGS as a highly effective approach for capturing fine-grained affordance details and improving recognition accuracy. Nevertheless, its full potential remains largely untapped due to the absence of large-scale, 3DGS-specific affordance datasets. To overcome these limitations, we present 3DAffordSplat, the first large-scale, multi-modal dataset tailored for 3DGS-based affordance reasoning. This dataset includes 23,677 Gaussian instances, 8,354 point cloud instances, and 6,631 manually annotated affordance labels, encompassing 21 object categories and 18 affordance types. Building upon this dataset, we introduce AffordSplatNet, a novel model specifically designed for affordance reasoning using 3DGS representations. AffordSplatNet features an innovative cross-modal structure alignment module that exploits structural consistency priors to align 3D point cloud and 3DGS representations, resulting in enhanced affordance recognition accuracy. Extensive experiments demonstrate that the 3DAffordSplat dataset significantly advances affordance learning within the 3DGS domain, while AffordSplatNet consistently outperforms existing methods across both seen and unseen settings, highlighting its robust generalization capabilities. </p>
<blockquote>
<p>3Dæ•ˆç”¨æ¨ç†å¯¹äºå°†äººç±»æŒ‡ä»¤ä¸3Då¯¹è±¡çš„åŠŸèƒ½åŒºåŸŸç›¸å…³è”è‡³å…³é‡è¦ï¼Œæœ‰åŠ©äºå®ç°å…·ä½“åŒ–èº«çš„AIä¸­çš„ç²¾ç¡®ã€ä»»åŠ¡å¯¼å‘å‹æ“ä½œã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºç¨€ç–çš„3Dç‚¹äº‘ï¼Œç”±äºå…¶å¯¹äºåæ ‡å˜åŒ–çš„æ•æ„Ÿæ€§å’Œæ•°æ®çš„å›ºæœ‰ç¨€ç–æ€§ï¼Œå±•ç°å‡ºæœ‰é™çš„ä¸€èˆ¬æ€§å’Œç¨³å¥æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ3Dé«˜æ–¯å¡«å……ï¼ˆ3DGSï¼‰é€šè¿‡é«˜å¯†åº¦ã€è¿ç»­åˆ†å¸ƒæ¥è¡¨ç¤ºåœºæ™¯ï¼Œå®ç°äº†é«˜ä¿çœŸã€å®æ—¶æ¸²æŸ“å’Œæœ€å°çš„è®¡ç®—å¼€é”€ã€‚è¿™ä½¿å¾—3DGSæˆä¸ºæ•è·ç²¾ç»†æ•ˆç”¨ç»†èŠ‚å¹¶æé«˜è¯†åˆ«å‡†ç¡®æ€§çš„é«˜æ•ˆæ–¹æ³•ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¤§è§„æ¨¡çš„3DGSç‰¹å®šæ•ˆç”¨æ•°æ®é›†ï¼Œå…¶å…¨éƒ¨æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†å‘æ˜ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†3DAffordSplatï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºåŸºäº3DGSçš„æ•ˆç”¨æ¨ç†å®šåˆ¶çš„å¤§è§„æ¨¡ã€å¤šæ¨¡å¼æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…æ‹¬23677ä¸ªé«˜æ–¯å®ä¾‹ã€8354ä¸ªç‚¹äº‘å®ä¾‹å’Œ6631ä¸ªæ‰‹åŠ¨æ³¨é‡Šçš„æ•ˆç”¨æ ‡ç­¾ï¼Œæ¶µç›–21ä¸ªå¯¹è±¡ç±»åˆ«å’Œ18ç§æ•ˆç”¨ç±»å‹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†AffordSplatNetï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºä½¿ç”¨3DGSè¡¨ç¤ºè¿›è¡Œæ•ˆç”¨æ¨ç†çš„æ–°å‹æ¨¡å‹ã€‚AffordSplatNetå…·æœ‰åˆ›æ–°æ€§çš„è·¨æ¨¡å¼ç»“æ„å¯¹é½æ¨¡å—ï¼Œåˆ©ç”¨ç»“æ„ä¸€è‡´æ€§å…ˆéªŒæ¥å¯¹é½3Dç‚¹äº‘å’Œ3DGSè¡¨ç¤ºï¼Œä»è€Œæé«˜æ•ˆç”¨è¯†åˆ«å‡†ç¡®æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œ3DAffordSplatæ•°æ®é›†åœ¨3DGSé¢†åŸŸçš„æ•ˆç”¨å­¦ä¹ æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè€ŒAffordSplatNetåœ¨å·²çŸ¥å’ŒæœªçŸ¥ç¯å¢ƒä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11218v2">PDF</a> The first large-scale 3D Gaussians Affordance Reasoning Benchmark</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨3Dåœºæ™¯ä¸­å¯¹äººç±»æŒ‡ä»¤ä¸3Dç‰©ä½“åŠŸèƒ½åŒºåŸŸçš„å…³è”ï¼Œæå‡ºåŸºäºé«˜æ–¯è¡¨é¢å±•ç¤ºï¼ˆGaussian Splattingï¼‰æŠ€æœ¯çš„AffordSplatNetæ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡å¤§è§„æ¨¡æ•°æ®é›†3DAffordSplatè¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«ç‰©ä½“çš„ç”¨é€”å’Œæ“ä½œæ–¹å¼ã€‚ç›¸è¾ƒäºä¾èµ–ç¨€ç–ç‚¹äº‘çš„ä¼ ç»Ÿæ–¹æ³•ï¼ŒAffordSplatNetå…·æœ‰æ›´å¼ºçš„é€šç”¨æ€§å’Œç¨³å¥æ€§ï¼Œå¯ç”¨äºè§£å†³é«˜ç²¾åº¦å®æ—¶çš„åº”ç”¨é—®é¢˜ã€‚æ¨¡å‹çš„å¼ºå¤§åœ¨äºå¼•å…¥äº†åˆ›æ–°çš„è·¨æ¨¡æ€ç»“æ„å¯¹é½æ¨¡å—ï¼Œæé«˜äº†åœ¨3Dé«˜æ–¯è¡¨é¢å±•ç¤ºé¢†åŸŸå†…çš„è¡¨ç°èƒ½åŠ›ã€‚è¯¥æ¨¡å‹å±•ç°å‡ºä¼˜ç§€çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ä¸åŒåœºæ™¯ä¸‹å‡è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3D affordance reasoningå¯¹äºå…³è”äººç±»æŒ‡ä»¤ä¸ç‰©ä½“åŠŸèƒ½åŒºåŸŸè‡³å…³é‡è¦ï¼Œå°¤å…¶åœ¨æ™ºèƒ½æœºå™¨äººæ“ä½œä¸­ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11218">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e142856c86dab42ef2e0fdef1d926888.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1585573944d0fdb046da2674e80fc53e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ede83fade29a7d3fc596e6da0f66782f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0526e9dd3ee3b4aa3815dc333195c2a7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Heimdall-test-time-scaling-on-the-generative-verification"><a href="#Heimdall-test-time-scaling-on-the-generative-verification" class="headerlink" title="Heimdall: test-time scaling on the generative verification"></a>Heimdall: test-time scaling on the generative verification</h2><p><strong>Authors:Wenlei Shi, Xing Jin</strong></p>
<p>An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently investigated. In this paper, we propose Heimdall, the long CoT verification LLM that can accurately judge the correctness of solutions. With pure reinforcement learning, we boost the verification accuracy from 62.5% to 94.5% on competitive math problems. By scaling with repeated sampling, the accuracy further increases to 97.5%. Through human evaluation, Heimdall demonstrates impressive generalization capabilities, successfully detecting most issues in challenging math proofs, the type of which is not included during training. Furthermore, we propose Pessimistic Verification to extend the functionality of Heimdall to scaling up the problem solving. It calls Heimdall to judge the solutions from a solver model and based on the pessimistic principle, selects the most likely correct solution with the least uncertainty. Taking DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification improves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute budget and to 83.3% with more compute budget. With the stronger solver Gemini 2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge discovery system, a ternary system where one poses questions, another provides solutions, and the third verifies the solutions. Using the data synthesis work NuminaMath for the first two components, Heimdall effectively identifies problematic records within the dataset and reveals that nearly half of the data is flawed, which interestingly aligns with the recent ablation studies from NuminaMath. </p>
<blockquote>
<p>ä¸€ä¸ªAIç³»ç»Ÿåªèƒ½åœ¨å…¶èƒ½å¤ŸéªŒè¯è‡ªèº«çŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œæ‰èƒ½åˆ›é€ å’Œä¿æŒçŸ¥è¯†ã€‚å…³äºé•¿é“¾æ€ç»´æ¨ç†çš„æœ€æ–°ç ”ç©¶å·²ç»æ˜¾ç¤ºå‡ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£å†³ç«èµ›é—®é¢˜æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä½†å®ƒä»¬çš„éªŒè¯èƒ½åŠ›ä»ç„¶è¾ƒå¼±ä¸”å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Heimdallï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿè¿›è¡Œé•¿é“¾æ€ç»´éªŒè¯çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå‡†ç¡®åˆ¤æ–­è§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§ã€‚é€šè¿‡çº¯ç²¹çš„å¼ºåŒ–å­¦ä¹ ï¼Œæˆ‘ä»¬åœ¨ç«äº‰æ€§æ•°å­¦é—®é¢˜ä¸Šçš„éªŒè¯å‡†ç¡®ç‡ä»62.5%æé«˜åˆ°äº†94.5%ã€‚é€šè¿‡é‡å¤é‡‡æ ·çš„æ‰©å±•ï¼Œå‡†ç¡®ç‡è¿›ä¸€æ­¥æé«˜åˆ°äº†97.5%ã€‚é€šè¿‡äººç±»è¯„ä¼°ï¼ŒHeimdallå±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ³›åŒ–èƒ½åŠ›ï¼ŒæˆåŠŸåœ°æ£€æµ‹åˆ°äº†æŒ‘æˆ˜æ€§æ•°å­¦è¯æ˜ä¸­çš„å¤§å¤šæ•°é—®é¢˜ï¼Œè¿™äº›é—®é¢˜åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¹¶æœªåŒ…å«åœ¨å†…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æ‚²è§‚éªŒè¯æ³•æ¥æ‰©å±•Heimdallçš„åŠŸèƒ½ï¼Œä»¥æé«˜é—®é¢˜è§£å†³è§„æ¨¡ã€‚å®ƒè°ƒç”¨Heimdallæ¥åˆ¤æ–­æ±‚è§£æ¨¡å‹çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶æ ¹æ®æ‚²è§‚åŸåˆ™ï¼Œé€‰æ‹©æœ€æœ‰å¯èƒ½çš„æ­£ç¡®è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶å°½é‡å‡å°‘ä¸ç¡®å®šæ€§ã€‚ä»¥DeepSeek-R1-Distill-Qwen-32Bä¸ºæ±‚è§£æ¨¡å‹ï¼Œæ‚²è§‚éªŒè¯æ³•å°†AIME2025çš„è§£å†³æ–¹æ¡ˆå‡†ç¡®ç‡ä»54.2%æé«˜åˆ°70%ï¼ˆåœ¨16å€è®¡ç®—é¢„ç®—ä¸‹ï¼‰å’Œæ›´é«˜çš„è®¡ç®—é¢„ç®—ä¸‹çš„83.3%ã€‚ä½¿ç”¨æ›´å¼ºå¤§çš„æ±‚è§£å™¨Gemini 2.5 Proæ—¶ï¼Œå¾—åˆ†æ›´æ˜¯è¾¾åˆ°äº†93%ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨çŸ¥è¯†å‘ç°ç³»ç»ŸåŸå‹ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸‰å…ƒç³»ç»Ÿï¼Œå…¶ä¸­ä¸€ä¸ªç»„ä»¶æå‡ºé—®é¢˜ï¼Œå¦ä¸€ä¸ªç»„ä»¶æä¾›è§£å†³æ–¹æ¡ˆï¼Œç¬¬ä¸‰ä¸ªç»„ä»¶éªŒè¯è§£å†³æ–¹æ¡ˆã€‚ä½¿ç”¨å‰ä¸¤ä¸ªç»„ä»¶çš„æ•°æ®åˆæˆå·¥ä½œNuminaMathï¼ŒHeimdallæœ‰æ•ˆåœ°è¯†åˆ«äº†æ•°æ®é›†ä¸­æœ‰é—®é¢˜çš„è®°å½•å¹¶æ­ç¤ºå‡ºè¿‘ä¸€åŠçš„æ•°æ®å­˜åœ¨ç¼ºé™·ï¼Œè¿™ä¸NuminaMathæœ€è¿‘çš„æ¶ˆèç ”ç©¶ç»“æœç›¸å»åˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10337v2">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿‘æœŸç ”ç©¶è¡¨æ˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨çŸ¥è¯†éªŒè¯èƒ½åŠ›æ–¹é¢æœ‰æ‰€ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†åä¸ºHeimdallçš„æ¨¡å‹ï¼Œèƒ½å‡†ç¡®åˆ¤æ–­è§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ŒéªŒè¯å‡†ç¡®ç‡æå‡è‡³94.5%ï¼Œè¿›ä¸€æ­¥é‡‡æ ·åå¯è¾¾97.5%ã€‚Heimdallå…·å¤‡å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½åœ¨è®­ç»ƒå¤–çš„æ•°å­¦è¯æ˜ä¸­å‘ç°å¤§éƒ¨åˆ†é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†æ‚²è§‚éªŒè¯æ³•æ¥æ‰©å±•Heimdallçš„åŠŸèƒ½ä»¥æå‡è§£é¢˜èƒ½åŠ›ã€‚è‡ªåŠ¨çŸ¥è¯†å‘ç°ç³»ç»Ÿçš„åŸå‹ä¹Ÿè¢«æå‡ºï¼Œå…¶ä¸­å°±åŒ…æ‹¬Heimdallçš„åº”ç”¨ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«æ•°æ®é›†ä¸­çš„é—®é¢˜è®°å½•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Heimdallæ¨¡å‹èƒ½å¤Ÿå‡†ç¡®åˆ¤æ–­é•¿Chain-of-Thoughtæ¨ç†è§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§ã€‚</li>
<li>é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ŒHeimdallçš„éªŒè¯å‡†ç¡®ç‡å¤§å¹…æå‡ã€‚</li>
<li>Heimdallå…·å¤‡å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½åœ¨æ•°å­¦è¯æ˜ä¸­å‘ç°å¤§éƒ¨åˆ†é—®é¢˜ã€‚</li>
<li>æ‚²è§‚éªŒè¯æ³•ç”¨äºæ‰©å±•Heimdallçš„åŠŸèƒ½ä»¥æå‡è§£é¢˜å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨è‡ªåŠ¨çŸ¥è¯†å‘ç°ç³»ç»Ÿä¸­ï¼ŒHeimdallèƒ½æœ‰æ•ˆè¯†åˆ«æ•°æ®é›†çš„é—®é¢˜è®°å½•ã€‚</li>
<li>Heimdallä¸å…¶ä»–æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1-Distill-Qwen-32Bå’ŒGemini 2.5 Proï¼‰ååŒå·¥ä½œä»¥æé«˜è§£é¢˜å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10337">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-13e4f34d968057971c4a271af95a10f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59222b9d0b74eddd45246ec1bdf0ec24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4eb625ac8755be635c204c6a8b0c2c93.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4363de988bd90df7f56897878c7f688f.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="TinyLLaVA-Video-R1-Towards-Smaller-LMMs-for-Video-Reasoning"><a href="#TinyLLaVA-Video-R1-Towards-Smaller-LMMs-for-Video-Reasoning" class="headerlink" title="TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning"></a>TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning</h2><p><strong>Authors:Xingjian Zhang, Siwei Wen, Wenjun Wu, Lei Huang</strong></p>
<p>Recently, improving the reasoning ability of large multimodal models (LMMs) through reinforcement learning has made great progress. However, most existing works are based on highly reasoning-intensive datasets such as mathematics and code, and researchers generally choose large-scale models as the foundation. We argue that exploring small-scale modelsâ€™ reasoning capabilities remains valuable for researchers with limited computational resources. Moreover, enabling models to explain their reasoning processes on general question-answering datasets is equally meaningful. Therefore, we present the small-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video, a traceably trained video understanding model with no more than 4B parameters, it not only demonstrates significantly improved reasoning and thinking capabilities after using reinforcement learning on general Video-QA datasets, but also exhibits the emergent characteristic of â€œaha momentsâ€. Furthermore, we share a series of experimental findings, aiming to provide practical insights for future exploration of video reasoning (thinking) abilities in small-scale models. It is available at <a target="_blank" rel="noopener" href="https://github.com/ZhangXJ199/TinyLLaVA-Video-R1">https://github.com/ZhangXJ199/TinyLLaVA-Video-R1</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æé«˜å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰çš„æ¨ç†èƒ½åŠ›å·²ç»å–å¾—äº†å¾ˆå¤§çš„è¿›å±•ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰å·¥ä½œéƒ½æ˜¯åŸºäºé«˜åº¦æ¨ç†å¯†é›†çš„æ•°æ®é›†ï¼ˆå¦‚æ•°å­¦å’Œä»£ç ï¼‰ï¼Œç ”ç©¶è€…é€šå¸¸é€‰æ‹©å¤§è§„æ¨¡æ¨¡å‹ä½œä¸ºåŸºç¡€ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå¯¹äºæ‹¥æœ‰æœ‰é™è®¡ç®—èµ„æºçš„ç ”ç©¶è€…æ¥è¯´ï¼Œæ¢ç´¢å°å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä»ç„¶å…·æœ‰é‡è¦æ„ä¹‰ã€‚æ­¤å¤–ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨é€šç”¨é—®ç­”æ•°æ®é›†ä¸Šè§£é‡Šå…¶æ¨ç†è¿‡ç¨‹åŒæ ·å…·æœ‰é‡è¦æ„ä¹‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å°å‹è§†é¢‘æ¨ç†æ¨¡å‹TinyLLaVA-Video-R1ã€‚å®ƒåŸºäºTinyLLaVA-Videoï¼Œè¿™æ˜¯ä¸€ä¸ªå¯è¿½è¸ªè®­ç»ƒçš„ã€å‚æ•°ä¸è¶…è¿‡4Bçš„è§†é¢‘ç†è§£æ¨¡å‹ã€‚åœ¨é€šç”¨Video-QAæ•°æ®é›†ä¸Šä½¿ç”¨å¼ºåŒ–å­¦ä¹ åï¼Œå®ƒä¸ä»…å±•ç¤ºäº†æ˜¾è‘—æå‡çš„æ¨ç†å’Œæ€è€ƒèƒ½åŠ›ï¼Œè¿˜è¡¨ç°å‡ºäº†â€œé¡¿æ‚Ÿæ—¶åˆ»â€çš„æ¶Œç°ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ†äº«äº†ä¸€ç³»åˆ—å®éªŒå‘ç°ï¼Œæ—¨åœ¨ä¸ºå°å‹æ¨¡å‹æœªæ¥æ¢ç´¢è§†é¢‘æ¨ç†ï¼ˆæ€è€ƒï¼‰èƒ½åŠ›æä¾›å®è·µè§è§£ã€‚å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/ZhangXJ199/TinyLLaVA-Video-R1%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ZhangXJ199/TinyLLaVA-Video-R1è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09641v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„æ¨ç†èƒ½åŠ›å·²å–å¾—æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰ç ”ç©¶éƒ½åŸºäºé«˜åº¦ä¾èµ–æ¨ç†èƒ½åŠ›çš„æ•°æ®é›†ï¼ˆå¦‚æ•°å­¦å’Œä»£ç ï¼‰ï¼Œå¹¶é€šå¸¸é€‰æ‹©å¤§è§„æ¨¡æ¨¡å‹ä½œä¸ºåŸºç¡€ã€‚æœ¬æ–‡ä¸»å¼ ï¼Œå¯¹äºæ‹¥æœ‰æœ‰é™è®¡ç®—èµ„æºçš„ç ”ç©¶è€…æ¥è¯´ï¼Œæ¢ç´¢å°å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä»å…·æœ‰ä»·å€¼ã€‚æ­¤å¤–ï¼Œè®©æ¨¡å‹åœ¨ä¸€èˆ¬é—®ç­”æ•°æ®é›†ä¸Šè§£é‡Šå…¶æ¨ç†è¿‡ç¨‹åŒæ ·å…·æœ‰é‡è¦æ„ä¹‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†å°å‹è§†é¢‘æ¨ç†æ¨¡å‹TinyLLaVA-Video-R1ã€‚å®ƒåœ¨TinyLLaVA-Videoçš„åŸºç¡€ä¸Šï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ åœ¨ä¸€èˆ¬è§†é¢‘é—®ç­”æ•°æ®é›†ä¸Šå±•ç°äº†æ˜¾è‘—æå‡çš„æ¨ç†å’Œæ€è€ƒèƒ½åŠ›ï¼Œå¹¶å‘ˆç°å‡ºâ€œé¡¿æ‚Ÿæ—¶åˆ»â€çš„æ˜¾è‘—ç‰¹å¾ã€‚æˆ‘ä»¬è¿˜åˆ†äº«äº†ä¸€ç³»åˆ—å®éªŒå‘ç°ï¼Œæ—¨åœ¨ä¸ºå°å‹æ¨¡å‹åœ¨è§†é¢‘æ¨ç†ï¼ˆæ€è€ƒï¼‰èƒ½åŠ›æ–¹é¢çš„æœªæ¥æ¢ç´¢æä¾›å®è·µè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å·²ç”¨äºæå‡å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨é«˜åº¦ä¾èµ–æ¨ç†èƒ½åŠ›çš„æ•°æ®é›†ï¼Œå¦‚æ•°å­¦å’Œä»£ç ã€‚</li>
<li>å°è§„æ¨¡æ¨¡å‹åœ¨æœ‰é™è®¡ç®—èµ„æºä¸‹çš„æ¨ç†èƒ½åŠ›ç ”ç©¶ä»å…·æœ‰ä»·å€¼ã€‚</li>
<li>æ¨¡å‹åœ¨ä¸€èˆ¬é—®ç­”æ•°æ®é›†ä¸Šè§£é‡Šå…¶æ¨ç†è¿‡ç¨‹å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>TinyLLaVA-Video-R1æ¨¡å‹åŸºäºTinyLLaVA-Videoï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æ˜¾è‘—æå‡äº†è§†é¢‘é—®ç­”æ•°æ®é›†çš„æ¨ç†å’Œæ€è€ƒèƒ½åŠ›ã€‚</li>
<li>TinyLLaVA-Video-R1æ¨¡å‹å±•ç°å‡ºâ€œé¡¿æ‚Ÿæ—¶åˆ»â€çš„ç‰¹å¾ã€‚</li>
<li>æ–‡ç« åˆ†äº«äº†ä¸€ç³»åˆ—å®éªŒå‘ç°ï¼Œä¸ºå°å‹æ¨¡å‹åœ¨è§†é¢‘æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœªæ¥æ¢ç´¢æä¾›äº†å®è·µè§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09641">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a9dc23363d457b2bacb90224f13f6335.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c3a2528b7144898c9fc19d84f3ca786.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bd4b6de1c1478211dc19d800843f840.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f04709bb2b5a1261c221a3b29733283a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-29400da9d1b9a3b32b8cad70c2e64acb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-67b7efc79d5d412781e21dd21d5bf70b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Short-Path-Prompting-in-LLMs-Analyzing-Reasoning-Instability-and-Solutions-for-Robust-Performance"><a href="#Short-Path-Prompting-in-LLMs-Analyzing-Reasoning-Instability-and-Solutions-for-Robust-Performance" class="headerlink" title="Short-Path Prompting in LLMs: Analyzing Reasoning Instability and   Solutions for Robust Performance"></a>Short-Path Prompting in LLMs: Analyzing Reasoning Instability and   Solutions for Robust Performance</h2><p><strong>Authors:Zuoli Tang, Junjie Ou, Kaiqin Hu, Chunwei Wu, Zhaoxin Huan, Chilin Fu, Xiaolu Zhang, Jun Zhou, Chenliang Li</strong></p>
<p>Recent years have witnessed significant progress in large language modelsâ€™ (LLMs) reasoning, which is largely due to the chain-of-thought (CoT) approaches, allowing models to generate intermediate reasoning steps before reaching the final answer. Building on these advances, state-of-the-art LLMs are instruction-tuned to provide long and detailed CoT pathways when responding to reasoning-related questions. However, human beings are naturally cognitive misers and will prompt language models to give rather short responses, thus raising a significant conflict with CoT reasoning. In this paper, we delve into how LLMsâ€™ reasoning performance changes when users provide short-path prompts. The results and analysis reveal that language models can reason effectively and robustly without explicit CoT prompts, while under short-path prompting, LLMsâ€™ reasoning ability drops significantly and becomes unstable, even on grade-school problems. To address this issue, we propose two approaches: an instruction-guided approach and a fine-tuning approach, both designed to effectively manage the conflict. Experimental results show that both methods achieve high accuracy, providing insights into the trade-off between instruction adherence and reasoning accuracy in current models. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè¿™å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºæ€ç»´é“¾ï¼ˆCoTï¼‰æ–¹æ³•çš„åº”ç”¨ï¼Œä½¿å¾—æ¨¡å‹åœ¨è¾¾åˆ°æœ€ç»ˆç­”æ¡ˆä¹‹å‰èƒ½å¤Ÿäº§ç”Ÿä¸­é—´æ¨ç†æ­¥éª¤ã€‚åŸºäºè¿™äº›è¿›å±•ï¼Œæœ€å…ˆè¿›çš„LLMé€šè¿‡æŒ‡ä»¤è°ƒæ•´ï¼Œèƒ½å¤Ÿåœ¨å›ç­”æ¨ç†ç›¸å…³é—®é¢˜æ—¶æä¾›é•¿æœŸä¸”è¯¦ç»†çš„CoTè·¯å¾„ã€‚ç„¶è€Œï¼Œäººç±»æœ¬è´¨ä¸Šæ˜¯è®¤çŸ¥åå•¬è€…ï¼Œä¼šä¿ƒä½¿è¯­è¨€æ¨¡å‹ç»™å‡ºç›¸å¯¹ç®€çŸ­çš„å›åº”ï¼Œä»è€Œä¸CoTæ¨ç†äº§ç”Ÿé‡å¤§å†²çªã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†å½“ç”¨æˆ·æä¾›ç®€çŸ­è·¯å¾„æç¤ºæ—¶ï¼ŒLLMçš„æ¨ç†æ€§èƒ½å¦‚ä½•å˜åŒ–ã€‚ç»“æœå’Œåˆ†æè¡¨æ˜ï¼Œè¯­è¨€æ¨¡å‹å¯ä»¥åœ¨æ²¡æœ‰æ˜ç¡®çš„CoTæç¤ºçš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆçš„æ¨ç†ï¼Œè€Œåœ¨ç®€çŸ­è·¯å¾„æç¤ºä¸‹ï¼ŒLLMçš„æ¨ç†èƒ½åŠ›ä¼šæ˜¾è‘—ä¸‹é™ï¼Œç”šè‡³åœ¨å°å­¦é—®é¢˜ä¸Šå˜å¾—ä¸ç¨³å®šã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§è§£å†³æ–¹æ³•ï¼šæŒ‡ä»¤å¼•å¯¼æ–¹æ³•å’Œå¾®è°ƒæ–¹æ³•ï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½æ—¨åœ¨æœ‰æ•ˆåœ°ç®¡ç†å†²çªã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸¤ç§æ–¹æ³•éƒ½å®ç°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œä¸ºå½“å‰æ¨¡å‹ä¸­æŒ‡ä»¤éµå¾ªå’Œæ¨ç†å‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡æä¾›äº†è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09586v1">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çŸ­è·¯å¾„æç¤ºä¸‹çš„æ¨ç†æ€§èƒ½å˜åŒ–ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ²¡æœ‰æ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºï¼Œè¯­è¨€æ¨¡å‹ä¹Ÿèƒ½æœ‰æ•ˆä¸”ç¨³å¥åœ°è¿›è¡Œæ¨ç†ã€‚ä½†åœ¨çŸ­è·¯å¾„æç¤ºä¸‹ï¼ŒLLMsçš„æ¨ç†èƒ½åŠ›ä¼šæ˜¾è‘—ä¸‹é™å¹¶å˜å¾—ä¸ç¨³å®šï¼Œç”šè‡³åœ¨å°å­¦é—®é¢˜ä¸Šä¹Ÿå¦‚æ­¤ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†æŒ‡ä»¤å¼•å¯¼æ–¹æ³•å’Œå¾®è°ƒæ–¹æ³•ï¼Œä¸¤ç§æ–¹æ³•éƒ½èƒ½æœ‰æ•ˆç®¡ç†å†²çªå¹¶æé«˜å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¿‘å¹´æ¥çš„æ¨ç†èƒ½åŠ›ä¸Šå–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œè¿™ä¸»è¦å¾—ç›Šäºæ€ç»´é“¾ï¼ˆCoTï¼‰æ–¹æ³•ã€‚</li>
<li>LLMså¯ä»¥é€šè¿‡ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤æ¥æä¾›é•¿æœŸçš„è¯¦ç»†æ€ç»´è·¯å¾„ã€‚</li>
<li>äººç±»å€¾å‘äºæä¾›çŸ­è·¯å¾„æç¤ºï¼Œè¿™ä¸LLMsçš„æ¨ç†æ–¹å¼å­˜åœ¨å†²çªã€‚</li>
<li>åœ¨çŸ­è·¯å¾„æç¤ºä¸‹ï¼ŒLLMsçš„æ¨ç†èƒ½åŠ›ä¼šæ˜¾è‘—ä¸‹é™å¹¶å˜å¾—ä¸ç¨³å®šã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†æŒ‡ä»¤å¼•å¯¼æ–¹æ³•å’Œå¾®è°ƒæ–¹æ³•ã€‚</li>
<li>æŒ‡ä»¤å¼•å¯¼æ–¹æ³•é€šè¿‡æ˜ç¡®æŒ‡å¯¼ç”¨æˆ·å¦‚ä½•ä¸ºè¯­è¨€æ¨¡å‹æä¾›æŒ‡ä»¤æ¥æ”¹å–„æ¨ç†å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09586">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c0b5509f7c874c05ca190607916fa4bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f13b874691544f10c79f847177d99a4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41bbde1a8c6d72710d80df16e92794d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd8e160334179ca52acbf4f3fe917e7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49069048dfe79aa5deb5555e039f5a4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ef77acc586d03f4b8543ef111ef4a60.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ClinicalGPT-R1-Pushing-reasoning-capability-of-generalist-disease-diagnosis-with-large-language-model"><a href="#ClinicalGPT-R1-Pushing-reasoning-capability-of-generalist-disease-diagnosis-with-large-language-model" class="headerlink" title="ClinicalGPT-R1: Pushing reasoning capability of generalist disease   diagnosis with large language model"></a>ClinicalGPT-R1: Pushing reasoning capability of generalist disease   diagnosis with large language model</h2><p><strong>Authors:Wuyang Lan, Wenzheng Wang, Changwei Ji, Guoxing Yang, Yongbo Zhang, Xiaohong Liu, Song Wu, Guangyu Wang</strong></p>
<p>Recent advances in reasoning with large language models (LLMs)has shown remarkable reasoning capabilities in domains such as mathematics and coding, yet their application to clinical diagnosis remains underexplored. Here, we introduce ClinicalGPT-R1, a reasoning enhanced generalist large language model for disease diagnosis. Trained on a dataset of 20,000 real-world clinical records, ClinicalGPT-R1 leverages diverse training strategies to enhance diagnostic reasoning. To benchmark performance, we curated MedBench-Hard, a challenging dataset spanning seven major medical specialties and representative diseases. Experimental results demonstrate that ClinicalGPT-R1 outperforms GPT-4o in Chinese diagnostic tasks and achieves comparable performance to GPT-4 in English settings. This comparative study effectively validates the superior performance of ClinicalGPT-R1 in disease diagnosis tasks. Resources are available at <a target="_blank" rel="noopener" href="https://github.com/medfound/medfound">https://github.com/medfound/medfound</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ–¹é¢çš„è¿›å±•åœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰é¢†åŸŸè¡¨ç°å‡ºäº†æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œä½†å®ƒä»¬åœ¨ä¸´åºŠè¯Šæ–­ä¸­çš„åº”ç”¨ä»ç„¶è¢«è¾ƒå°‘æ¢ç´¢ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†ClinicalGPT-R1ï¼Œè¿™æ˜¯ä¸€æ¬¾ç”¨äºç–¾ç—…è¯Šæ–­çš„å¢å¼ºé€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ClinicalGPT-R1ç»è¿‡2ä¸‡ä»½çœŸå®ä¸–ç•Œä¸´åºŠè®°å½•çš„æ•°æ®é›†è®­ç»ƒï¼Œé‡‡ç”¨å¤šç§è®­ç»ƒç­–ç•¥æ¥æå‡è¯Šæ–­æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è¯„ä¼°æ€§èƒ½ï¼Œæˆ‘ä»¬ç²¾å¿ƒåˆ¶ä½œäº†MedBench-Hardæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–ä¸ƒå¤§åŒ»å­¦ä¸“ä¸šå’Œä»£è¡¨æ€§ç–¾ç—…çš„æŒ‘æˆ˜æ€§æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒClinicalGPT-R1åœ¨ä¸­å›½è¯Šæ–­ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºGPT-4oï¼Œåœ¨è‹±è¯­ç¯å¢ƒä¸­è¡¨ç°ä¸GPT-4ç›¸å½“ã€‚è¿™é¡¹å¯¹æ¯”ç ”ç©¶æœ‰æ•ˆåœ°éªŒè¯äº†ClinicalGPT-R1åœ¨ç–¾ç—…è¯Šæ–­ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ã€‚ç›¸å…³èµ„æºå¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/medfound/medfound%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/medfound/medfoundè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09421v2">PDF</a> 8 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ï¼Œæ–‡ç« ä»‹ç»äº†ClinicalGPT-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç–¾ç—…è¯Šæ–­çš„é€šç”¨æ¨ç†å¢å¼ºå‹å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ç»è¿‡è®­ç»ƒçš„çœŸå®ä¸–ç•Œä¸´åºŠè®°å½•æ•°æ®é›†å¼ºåŒ–äº†å…¶è¯Šæ–­æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¯¹æ¯”å®éªŒéªŒè¯äº†å…¶åœ¨ä¸­è‹±æ–‡ç¯å¢ƒä¸‹çš„è¯Šæ–­ä»»åŠ¡è¡¨ç°ï¼Œå¹¶ä¸GPT-4è¿›è¡Œå¯¹æ¯”è¯„ä¼°ï¼Œç»“æœè¯å®ClinicalGPT-R1åœ¨ä¸´åºŠè¯Šæ–­é¢†åŸŸå±•ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚æ›´å¤šèµ„æºå¯é€šè¿‡ç‰¹å®šé“¾æ¥è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ClinicalGPT-R1æ˜¯åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç–¾ç—…è¯Šæ–­é€šç”¨æ¨ç†å¢å¼ºæ¨¡å‹ã€‚</li>
<li>æ¨¡å‹è®­ç»ƒä½¿ç”¨äº†åŒ…å«çœŸå®ä¸–ç•Œä¸´åºŠè®°å½•çš„æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡MedBench-Hardæ•°æ®é›†å¯¹æ¨¡å‹æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ï¼Œè¦†ç›–ä¸ƒå¤§åŒ»å­¦é¢†åŸŸå’Œä»£è¡¨æ€§ç–¾ç—…ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºGPT-4åœ¨ä¸­æ–‡è¯Šæ–­ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒClinicalGPT-R1æœ‰ä¼˜è¶Šçš„æ€§èƒ½ã€‚</li>
<li>ClinicalGPT-R1åœ¨è‹±æ–‡ç¯å¢ƒä¸‹çš„è¯Šæ–­ä»»åŠ¡è¡¨ç°ä¸GPT-4ç›¸å½“ã€‚</li>
<li>æ–‡ç« é€šè¿‡å¯¹æ¯”å®éªŒéªŒè¯äº†ClinicalGPT-R1åœ¨ä¸´åºŠè¯Šæ–­é¢†åŸŸçš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09421">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5622097bbf9f7ed87a86e16eb0250046.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fe387fc3aa28791308d1eaadfaf61c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25f2b792e75730cba60021b8ffb09f60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f14a0b6bc5d3792508d0d1b8683c50a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6616e5ad38e651598024ad17b73fe9bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6d2d2254e3605abfca48104da07ad3a.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="VLM-R1-A-Stable-and-Generalizable-R1-style-Large-Vision-Language-Model"><a href="#VLM-R1-A-Stable-and-Generalizable-R1-style-Large-Vision-Language-Model" class="headerlink" title="VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model"></a>VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model</h2><p><strong>Authors:Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, Ruochen Xu, Tiancheng Zhao</strong></p>
<p>Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMsâ€™ performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the â€œOD aha momentâ€, the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at <a target="_blank" rel="noopener" href="https://github.com/om-ai-lab/VLM-R1">https://github.com/om-ai-lab/VLM-R1</a> </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒDeepSeek R1çš„ç ”ç©¶è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥é€šè¿‡ç®€å•è€Œæœ‰æ•ˆçš„è®¾è®¡ï¼Œæ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚R1çš„æ ¸å¿ƒåœ¨äºå…¶åŸºäºè§„åˆ™çš„å¥–åŠ±å…¬å¼ï¼Œå®ƒåˆ©ç”¨å…·æœ‰ç¡®å®šæ€§æ ‡å‡†ç­”æ¡ˆçš„ä»»åŠ¡æ¥å®ç°ç²¾ç¡®ç¨³å®šçš„å¥–åŠ±è®¡ç®—ã€‚åœ¨è§†è§‰é¢†åŸŸï¼Œæˆ‘ä»¬åŒæ ·è§‚å¯Ÿåˆ°èŒƒå›´å¹¿æ³›çš„è§†è§‰ç†è§£ä»»åŠ¡æœ¬èº«å°±å…·å¤‡å®šä¹‰è‰¯å¥½çš„æ ‡å‡†æ³¨é‡Šã€‚è¿™ä¸€ç‰¹æ€§ä½¿å®ƒä»¬è‡ªç„¶åœ°ä¸åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶å…¼å®¹ã€‚å—æ­¤è§‚å¯Ÿç»“æœçš„å¯å‘ï¼Œæˆ‘ä»¬æ¢è®¨äº†å°†R1é£æ ¼çš„å¼ºåŒ–å­¦ä¹ æ‰©å±•åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å¯èƒ½æ€§ï¼Œæ—¨åœ¨å¢å¼ºå®ƒä»¬çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåä¸ºVLM-R1çš„ä¸“ç”¨æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æé«˜VLMåœ¨ä¸€èˆ¬è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ä½¿ç”¨è¿™ä¸ªæ¡†æ¶ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†å°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºè§†è§‰é¢†åŸŸçš„å¯è¡Œæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨¡å‹ä¸ä»…åœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œè€Œä¸”åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šä¹Ÿè¶…è¶Šäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å…¨é¢çš„æ¶ˆèç ”ç©¶ï¼Œæ­ç¤ºäº†ä¸€ç³»åˆ—å€¼å¾—å…³æ³¨çš„è§è§£ï¼ŒåŒ…æ‹¬ç›®æ ‡æ£€æµ‹ä¸­çš„å¥–åŠ±ç ´è§£ç°è±¡ã€â€œODé¡¿æ‚Ÿæ—¶åˆ»â€çš„å‡ºç°ã€è®­ç»ƒæ•°æ®è´¨é‡çš„å½±å“ä»¥åŠä¸åŒæ¨¡å‹å¤§å°ä¸‹å¼ºåŒ–å­¦ä¹ çš„æ‰©å±•è¡Œä¸ºã€‚é€šè¿‡è¿™äº›åˆ†æï¼Œæˆ‘ä»¬æ—¨åœ¨åŠ æ·±å¯¹å¼ºåŒ–å­¦ä¹ å¦‚ä½•å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„ç†è§£ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶æˆæœå’Œå¼€æºè´¡çŒ®å°†ç»§ç»­æ¨åŠ¨è§†è§‰è¯­è¨€å¼ºåŒ–å­¦ä¹ ç¤¾åŒºçš„å‘å±•ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨&lt;<a target="_blank" rel="noopener" href="https://github.com/om-ai-lab/VLM-R">https://github.com/om-ai-lab/VLM-R</a> å¯ä»¥åœ¨æ­¤æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07615v2">PDF</a> 11 pages, fix some minor typos in the previous version</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†DeepSeek R1å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚å…¶æ ¸å¿ƒåœ¨äºåŸºäºè§„åˆ™çš„å¥–åŠ±åˆ¶å®šï¼Œè¯¥è§„åˆ™åˆ©ç”¨å…·æœ‰ç¡®å®šæ€§æ ‡å‡†ç­”æ¡ˆçš„ä»»åŠ¡æ¥å®ç°ç²¾ç¡®ä¸”ç¨³å®šçš„å¥–åŠ±è®¡ç®—ã€‚ä½œè€…è§‚å¯Ÿåˆ°è®¸å¤šè§†è§‰ç†è§£ä»»åŠ¡æœ¬èº«å°±å…·å¤‡è‰¯å¥½çš„æ ‡å‡†æ³¨é‡Šï¼Œè¿™ä½¿å¾—å®ƒä»¬è‡ªç„¶åœ°ä¸åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶å…¼å®¹ã€‚å—æ­¤å¯å‘ï¼Œç ”ç©¶è€…ä»¬æ¢ç´¢äº†å°†R1é£æ ¼çš„å¼ºåŒ–å­¦ä¹ æ‰©å±•åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œä»¥æé«˜å…¶åœ¨é€šç”¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šçš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œä»–ä»¬å¼€å‘äº†VLM-R1æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨åˆ©ç”¨RLæ”¹å–„VLMçš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºRLçš„æ¨¡å‹ä¸ä»…åœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å…·æœ‰ç«äº‰åŠ›ï¼Œè€Œä¸”åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šä¹Ÿè¶…è¶Šäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…ä»¬è¿˜é€šè¿‡å…¨é¢çš„æ¶ˆèç ”ç©¶æ­ç¤ºäº†ä¸€äº›é‡è¦çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek R1ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå…¶æ ¸å¿ƒåœ¨äºå…¶åŸºäºè§„åˆ™çš„å¥–åŠ±åˆ¶å®šã€‚</li>
<li>è®¸å¤šè§†è§‰ç†è§£ä»»åŠ¡å…·æœ‰å¤©ç„¶çš„è‰¯å¥½å®šä¹‰çš„æ ‡å‡†æ³¨é‡Šï¼Œè¿™ä½¿å…¶ä¸åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶ç›¸å…¼å®¹ã€‚</li>
<li>VLM-R1æ¡†æ¶æ—¨åœ¨åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æé«˜è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é€šç”¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>åŸºäºRLçš„æ¨¡å‹åœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å…·æœ‰ç«äº‰åŠ›ï¼Œä¸”åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šè¶…è¶Šç›‘ç£å¾®è°ƒã€‚</li>
<li>æ¶ˆèç ”ç©¶æ­ç¤ºäº†å¥–åŠ±ç ´è§£åœ¨ç‰©ä½“æ£€æµ‹ä¸­çš„å­˜åœ¨ï¼Œâ€œOD ahaæ—¶åˆ»â€çš„æ¶Œç°ï¼Œè®­ç»ƒæ•°æ®è´¨é‡çš„å½±å“ä»¥åŠä¸åŒæ¨¡å‹å¤§å°ä¸‹RLçš„æ‰©å±•è¡Œä¸ºç­‰é‡è¦è§è§£ã€‚</li>
<li>è¿™äº›ç ”ç©¶æˆæœæ—¨åœ¨åŠ æ·±å¯¹å¼ºåŒ–å­¦ä¹ å¦‚ä½•å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07615">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-945c9df4db016a81c4b82fd988c28b8f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d20c2434fd742c70e7d20e928953e89c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8105e3e2538999b6cdd524e012dd576b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b17ce1f73a418665b50ebc40d7e1ba2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6ef4b9706c82a9d9bcc440230d92b48.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Kimi-VL-Technical-Report"><a href="#Kimi-VL-Technical-Report" class="headerlink" title="Kimi-VL Technical Report"></a>Kimi-VL Technical Report</h2><p><strong>Authors: Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, Congcong Wang, Dehao Zhang, Dikang Du, Dongliang Wang, Enming Yuan, Enzhe Lu, Fang Li, Flood Sung, Guangda Wei, Guokun Lai, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haoning Wu, Haotian Yao, Haoyu Lu, Heng Wang, Hongcheng Gao, Huabin Zheng, Jiaming Li, Jianlin Su, Jianzhou Wang, Jiaqi Deng, Jiezhong Qiu, Jin Xie, Jinhong Wang, Jingyuan Liu, Junjie Yan, Kun Ouyang, Liang Chen, Lin Sui, Longhui Yu, Mengfan Dong, Mengnan Dong, Nuo Xu, Pengyu Cheng, Qizheng Gu, Runjie Zhou, Shaowei Liu, Sihan Cao, Tao Yu, Tianhui Song, Tongtong Bai, Wei Song, Weiran He, Weixiao Huang, Weixin Xu, Xiaokun Yuan, Xingcheng Yao, Xingzhe Wu, Xinxing Zu, Xinyu Zhou, Xinyuan Wang, Y. Charles, Yan Zhong, Yang Li, Yangyang Hu, Yanru Chen, Yejie Wang, Yibo Liu, Yibo Miao, Yidao Qin, Yimin Chen, Yiping Bao, Yiqin Wang, Yongsheng Kang, Yuanxin Liu, Yulun Du, Yuxin Wu, Yuzhi Wang, Yuzi Yan, Zaida Zhou, Zhaowei Li, Zhejun Jiang, Zheng Zhang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Zijia Zhao, Ziwei Chen, Zongyu Lin</strong></p>
<p>We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) that offers advanced multimodal reasoning, long-context understanding, and strong agent capabilities - all while activating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL demonstrates strong performance across challenging domains: as a general-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld), matching flagship models. Furthermore, it exhibits remarkable capabilities across diverse challenging vision language tasks, including college-level image and video comprehension, OCR, mathematical reasoning, and multi-image understanding. In comparative evaluations, it effectively competes with cutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and Gemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also advances in processing long contexts and perceiving clearly. With a 128K extended context window, Kimi-VL can process diverse long inputs, achieving impressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its native-resolution vision encoder, MoonViT, further allows it to see and understand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and 34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common tasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant: Kimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised fine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong long-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8 on MathVision, and 71.3 on MathVista while maintaining the compact 2.8B activated LLM parameters, setting a new standard for efficient multimodal thinking models. Code and models are publicly accessible at <a target="_blank" rel="noopener" href="https://github.com/MoonshotAI/Kimi-VL">https://github.com/MoonshotAI/Kimi-VL</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Kimi-VLï¼Œè¿™æ˜¯ä¸€æ¬¾é«˜æ•ˆçš„å¼€æºæ··åˆä¸“å®¶ï¼ˆMoEï¼‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚å®ƒæä¾›äº†å…ˆè¿›çš„è·¨æ¨¡æ€æ¨ç†ã€é•¿æ–‡æœ¬ç†è§£ä»¥åŠå¼ºå¤§çš„ä»£ç†èƒ½åŠ›ï¼Œåœ¨å…¶è¯­è¨€è§£ç å™¨ï¼ˆKimi-VL-A3Bï¼‰ä¸­ä»…æ¿€æ´»2.8Bå‚æ•°ã€‚Kimi-VLåœ¨ä¸åŒå…·æœ‰æŒ‘æˆ˜æ€§çš„é¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼šä½œä¸ºé€šç”¨VLMï¼ŒKimi-VLåœ¨å¤šå›åˆä»£ç†ä»»åŠ¡ï¼ˆä¾‹å¦‚OSWorldï¼‰ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸æ——èˆ°æ¨¡å‹ç›¸åŒ¹é…ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¤§å­¦çº§åˆ«çš„å›¾åƒå’Œè§†é¢‘ç†è§£ã€OCRã€æ•°å­¦æ¨ç†å’Œå¤šå›¾åƒç†è§£ã€‚åœ¨æ¯”è¾ƒè¯„ä¼°ä¸­ï¼Œå®ƒä¸æœ€å‰æ²¿çš„é«˜æ•ˆVLMï¼ˆå¦‚GPT-4o-miniã€Qwen2.5-VL-7Bå’ŒGemma-3-12B-ITï¼‰æœ‰æ•ˆç«äº‰ï¼ŒåŒæ—¶åœ¨ä¸€äº›å…³é”®é¢†åŸŸè¶…è¶Šäº†GPT-4oã€‚Kimi-VLåœ¨å¤„ç†é•¿æ–‡æœ¬å’Œæ¸…æ™°æ„ŸçŸ¥æ–¹é¢ä¹Ÿå–å¾—äº†è¿›å±•ã€‚å€ŸåŠ©128Kæ‰©å±•çš„ä¸Šä¸‹æ–‡çª—å£ï¼ŒKimi-VLå¯ä»¥å¤„ç†å„ç§é•¿è¾“å…¥ï¼Œåœ¨LongVideoBenchä¸Šå®ç°64.5åˆ†çš„ä»¤äººå°è±¡æ·±åˆ»æˆç»©ï¼Œåœ¨MMLongBench-Docä¸Šå®ç°35.1åˆ†ã€‚å…¶åŸç”Ÿåˆ†è¾¨ç‡è§†è§‰ç¼–ç å™¨MoonViTå…è®¸å…¶æŸ¥çœ‹å¹¶ç†è§£è¶…é«˜åˆ†è¾¨ç‡çš„è§†è§‰è¾“å…¥ï¼Œåœ¨InfoVQAä¸Šå®ç°83.2åˆ†ï¼Œåœ¨ScreenSpot-Proä¸Šå®ç°34.5åˆ†ï¼ŒåŒæ—¶ä¸ºå¸¸è§„ä»»åŠ¡ä¿æŒè¾ƒä½çš„è®¡ç®—æˆæœ¬ã€‚åŸºäºKimi-VLï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸€æ¬¾å…ˆè¿›çš„é•¿æ€è€ƒå˜ä½“ï¼šKimi-VL-Thinkingã€‚è¯¥æ¨¡å‹é€šè¿‡é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œå¼€å‘ï¼Œå±•ç°å‡ºå¼ºå¤§çš„é•¿æœŸæ¨ç†èƒ½åŠ›ã€‚åœ¨MMUä¸Šå¾—åˆ†61.7ï¼Œåœ¨MathVisionä¸Šå¾—åˆ†36.8ï¼Œåœ¨MathVistaä¸Šå¾—åˆ†71.3ï¼ŒåŒæ—¶ä¿æŒç´§å‡‘çš„2.8Bæ¿€æ´»LLMå‚æ•°ï¼Œä¸ºé«˜æ•ˆçš„å¤šæ¨¡æ€æ€è€ƒæ¨¡å‹è®¾å®šäº†æ–°æ ‡å‡†ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MoonshotAI/Kimi-VL%E5%AE%9A%E5%BC%BA%E5%BC%A5%E5%BC%BA%E7%9A%84%E9%AB%98%E6%9C%AF%E5%B0%BA%E5%AF%BA-%E5%BC%A5%E5%BD%A9%E6%AF%94%E7%AE%B1%E5%BC%B9torrig-%EF%BC%9B/%EF%BC%9CM">https://github.com/MoonshotAI/Kimi-VLå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07491v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Kimi-VLæ˜¯ä¸€æ¬¾é«˜æ•ˆå¼€æ”¾æºä»£ç çš„æ··åˆä¸“å®¶è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå…·å¤‡å…ˆè¿›çš„è·¨æ¨¡æ€æ¨ç†ã€é•¿ä¸Šä¸‹æ–‡ç†è§£å’Œå¼ºå¤§çš„ä»£ç†èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨æ¿€æ´»ä»…2.8Bå‚æ•°çš„è¯­è¨€è§£ç å™¨ï¼ˆKimi-VL-A3Bï¼‰çš„æƒ…å†µä¸‹ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„æ€§èƒ½ã€‚Kimi-VLåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¦‚å¤šå›åˆä»£ç†ä»»åŠ¡ã€å›¾åƒå’Œè§†é¢‘ç†è§£ã€å…‰å­¦å­—ç¬¦è¯†åˆ«ã€æ•°å­¦æ¨ç†å’Œå¤šå›¾åƒç†è§£ç­‰ã€‚ä¸å‰æ²¿çš„é«˜æ•ˆVLMsç›¸æ¯”ï¼ŒKimi-VLå…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶åœ¨æŸäº›é¢†åŸŸå®ç°äº†å¯¹GPT-4oçš„è¶…è¶Šã€‚å…¶æ‹¥æœ‰å¤„ç†é•¿ä¸Šä¸‹æ–‡å’Œæ¸…æ™°æ„ŸçŸ¥çš„å…ˆè¿›èƒ½åŠ›ï¼Œå¹¶ä½¿ç”¨MoonViTè§†è§‰ç¼–ç å™¨æ¥å¤„ç†è¶…é«˜åˆ†è¾¨ç‡è§†è§‰è¾“å…¥ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æ¨å‡ºäº†å…·æœ‰å¼ºå¤§é•¿æœŸæ¨ç†èƒ½åŠ›çš„å…ˆè¿›æ¨¡å‹Kimi-VL-Thinkingã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Kimi-VLæ˜¯ä¸€ä¸ªé«˜æ•ˆçš„MoEï¼ˆMixture-of-Expertsï¼‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰å…ˆè¿›çš„è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨ä»…æ¿€æ´»2.8Bå‚æ•°çš„è¯­è¨€è§£ç å™¨æƒ…å†µä¸‹ï¼ŒKimi-VLè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>Kimi-VLåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬å¤šå›åˆä»£ç†ä»»åŠ¡ã€å›¾åƒå’Œè§†é¢‘ç†è§£ã€OCRã€æ•°å­¦æ¨ç†å’Œå¤šå›¾åƒç†è§£ç­‰ã€‚</li>
<li>Kimi-VLåœ¨æŒ‘æˆ˜æ€§é¢†åŸŸè¡¨ç°å‡ºå¯¹å‰æ²¿æ¨¡å‹çš„ç«äº‰åŠ›ï¼Œå¹¶åœ¨æŸäº›é¢†åŸŸå®ç°å¯¹GPT-4oçš„è¶…è¶Šã€‚</li>
<li>Kimi-VLå…·æœ‰å¤„ç†é•¿ä¸Šä¸‹æ–‡å’Œæ¸…æ™°æ„ŸçŸ¥çš„å…ˆè¿›èƒ½åŠ›ï¼Œå¹¶ä½¿ç”¨MoonViTè§†è§‰ç¼–ç å™¨å¤„ç†é«˜åˆ†è¾¨ç‡è§†è§‰è¾“å…¥ã€‚</li>
<li>Kimi-VL-Thinkingæ˜¯Kimi-VLçš„è¿›é˜¶ç‰ˆæœ¬ï¼Œå…·å¤‡å¼ºå¤§çš„é•¿æœŸæ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°ä¼˜å¼‚è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07491">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e79daa664fc79e9d2cf6827edf6b0548.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dad4be1c743d126b3ac939feca19a89b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9729c4da386a53a1262f1d3dd47e6bce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-517e47b2591b895519d93fd81e7c04d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5f257b02c3494de39d23bd3d097551a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fef33b2c9593f47fcfb5e61641007223.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7dc01488d8afc2ca2c7f4dc1273111ee.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MDIT-A-Model-free-Data-Interpolation-Method-for-Diverse-Instruction-Tuning"><a href="#MDIT-A-Model-free-Data-Interpolation-Method-for-Diverse-Instruction-Tuning" class="headerlink" title="MDIT: A Model-free Data Interpolation Method for Diverse Instruction   Tuning"></a>MDIT: A Model-free Data Interpolation Method for Diverse Instruction   Tuning</h2><p><strong>Authors:Yangning Li, Zihua Lan, Lv Qingsong, Yinghui Li, Hai-Tao Zheng</strong></p>
<p>As Large Language Models (LLMs) are increasingly applied across various tasks, instruction tuning has emerged as a critical method for enhancing model performance. However, current data management strategies face substantial challenges in generating diverse and comprehensive data, restricting further improvements in model performance. To address this gap, we propose MDIT, a novel model-free data interpolation method for diverse instruction tuning, which generates varied and high-quality instruction data by performing task interpolation. Moreover, it contains diversity-based clustering strategies to ensure the diversity of the training data. Extensive experiments show that our method achieves superior performance in multiple benchmark tasks. The LLMs finetuned with MDIT show significant improvements in numerous tasks such as general question answering, math reasoning, and code generation. MDIT offers an efficient and automatic data synthetic method, generating diverse instruction data without depending on external resources while expanding the application potential of LLMs in complex environments. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„é¡¹ä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨ï¼ŒæŒ‡ä»¤è°ƒæ•´ä½œä¸ºä¸€ç§æé«˜æ¨¡å‹æ€§èƒ½çš„å…³é”®æ–¹æ³•åº”è¿è€Œç”Ÿã€‚ç„¶è€Œï¼Œå½“å‰çš„æ•°æ®ç®¡ç†ç­–ç•¥åœ¨ç”Ÿæˆå¤šæ ·ä¸”å…¨é¢çš„æ•°æ®æ–¹é¢é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½çš„è¿›ä¸€æ­¥æé«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†MDITï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤šæ ·æŒ‡ä»¤è°ƒæ•´çš„æ–°å‹æ— æ¨¡å‹æ•°æ®æ’å€¼æ–¹æ³•ï¼Œå®ƒé€šè¿‡ä»»åŠ¡æ’å€¼ç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„ä»»åŠ¡æŒ‡ä»¤æ•°æ®ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åŒ…æ‹¬åŸºäºå¤šæ ·æ€§çš„èšç±»ç­–ç•¥ï¼Œä»¥ç¡®ä¿è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä»»åŠ¡ä¸Šå–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚ä½¿ç”¨MDITå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€šç”¨é—®ç­”ã€æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆç­‰å¤šé¡¹ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚MDITæä¾›äº†ä¸€ç§é«˜æ•ˆã€è‡ªåŠ¨çš„æ•°æ®åˆæˆæ–¹æ³•ï¼Œåœ¨ä¸éœ€è¦ä¾èµ–å¤–éƒ¨èµ„æºçš„æƒ…å†µä¸‹ç”Ÿæˆå¤šæ ·çš„æŒ‡ä»¤æ•°æ®ï¼ŒåŒæ—¶æ‰©å¤§äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07288v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼ŒæŒ‡ä»¤è°ƒæ•´ä½œä¸ºæé«˜æ¨¡å‹æ€§èƒ½çš„å…³é”®æ–¹æ³•å¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ•°æ®ç®¡ç†ç­–ç•¥åœ¨ç”Ÿæˆå¤šæ ·ä¸”å…¨é¢çš„æ•°æ®æ–¹é¢é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œé™åˆ¶äº†æ¨¡å‹æ€§èƒ½çš„è¿›ä¸€æ­¥æé«˜ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†MDITï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤šæ ·æŒ‡ä»¤è°ƒæ•´çš„æ–°å‹æ¨¡å‹å¤–æ•°æ®æ’å€¼æ–¹æ³•ã€‚å®ƒé€šè¿‡ä»»åŠ¡æ’å€¼ç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„ä»»åŠ¡æŒ‡ä»¤æ•°æ®ã€‚æ­¤å¤–ï¼Œå®ƒé‡‡ç”¨åŸºäºå¤šæ ·æ€§çš„èšç±»ç­–ç•¥ï¼Œç¡®ä¿è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä»»åŠ¡ä¸Šå–å¾—äº†å“è¶Šæ€§èƒ½ã€‚ä½¿ç”¨MDITå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€šç”¨é—®ç­”ã€æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆç­‰å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚MDITæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”è‡ªåŠ¨çš„æ•°æ®åˆæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–å¤–éƒ¨èµ„æºçš„æƒ…å†µä¸‹ç”Ÿæˆå¤šæ ·çš„æŒ‡ä»¤æ•°æ®ï¼Œæ‰©å¤§äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰åœ¨å¤šä»»åŠ¡åº”ç”¨ä¸­é¢ä¸´æ€§èƒ½æå‡çš„æŒ‘æˆ˜ã€‚</li>
<li>æŒ‡ä»¤è°ƒæ•´æ˜¯æé«˜LLMsæ€§èƒ½çš„å…³é”®æ–¹æ³•ã€‚</li>
<li>å½“å‰æ•°æ®ç®¡ç†ç­–ç•¥åœ¨ç”Ÿæˆå¤šæ ·ä¸”å…¨é¢çš„æ•°æ®æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>MDITæ˜¯ä¸€ç§æ–°å‹çš„æ¨¡å‹å¤–æ•°æ®æ’å€¼æ–¹æ³•ï¼Œç”¨äºå¤šæ ·æŒ‡ä»¤è°ƒæ•´ã€‚</li>
<li>MDITé€šè¿‡ä»»åŠ¡æ’å€¼ç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„ä»»åŠ¡æŒ‡ä»¤æ•°æ®ã€‚</li>
<li>MDITé‡‡ç”¨åŸºäºå¤šæ ·æ€§çš„èšç±»ç­–ç•¥ç¡®ä¿è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07288">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d0595e5ff6b553b5b0427a88031efc5b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-02294fca9339cb39fa23eec7735dbd64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2bf13231fd2f3427da81cdee4c15c69f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbb052ac495cd40a6d696553b73aa203.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-18/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-18/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-18/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f50f71bd991c3c5db24d14612a62d5ec.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-18  HLS-Eval A Benchmark and Framework for Evaluating LLMs on High-Level   Synthesis Design Tasks
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-17/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-3bd255f5289bc4b2eb8545221bd773d6.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-17  GestureCoach Rehearsing for Engaging Talks with LLM-Driven Gesture   Recommendations
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29301k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
