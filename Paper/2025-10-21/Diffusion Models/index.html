<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-21  LightsOut Diffusion-based Outpainting for Enhanced Lens Flare Removal">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-3655cebcca788ee6940d7aa437f37572~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992959&auth_key=1760992959-0-0-6e97ba00ebad15e4d833bd22e8e2b8c6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    53 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-21-æ›´æ–°"><a href="#2025-10-21-æ›´æ–°" class="headerlink" title="2025-10-21 æ›´æ–°"></a>2025-10-21 æ›´æ–°</h1><h2 id="LightsOut-Diffusion-based-Outpainting-for-Enhanced-Lens-Flare-Removal"><a href="#LightsOut-Diffusion-based-Outpainting-for-Enhanced-Lens-Flare-Removal" class="headerlink" title="LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal"></a>LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal</h2><p><strong>Authors:Shr-Ruei Tsai, Wei-Cheng Chang, Jie-Ying Lee, Chih-Hai Su, Yu-Lun Liu</strong></p>
<p>Lens flare significantly degrades image quality, impacting critical computer vision tasks like object detection and autonomous driving. Recent Single Image Flare Removal (SIFR) methods perform poorly when off-frame light sources are incomplete or absent. We propose LightsOut, a diffusion-based outpainting framework tailored to enhance SIFR by reconstructing off-frame light sources. Our method leverages a multitask regression module and LoRA fine-tuned diffusion model to ensure realistic and physically consistent outpainting results. Comprehensive experiments demonstrate LightsOut consistently boosts the performance of existing SIFR methods across challenging scenarios without additional retraining, serving as a universally applicable plug-and-play preprocessing solution. Project page: <a target="_blank" rel="noopener" href="https://ray-1026.github.io/lightsout/">https://ray-1026.github.io/lightsout/</a> </p>
<blockquote>
<p>é•œå¤´çœ©å…‰ä¼šä¸¥é‡å½±å“å›¾åƒè´¨é‡ï¼Œä»è€Œå½±å“å…³é”®è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œå¦‚ç›®æ ‡æ£€æµ‹å’Œè‡ªåŠ¨é©¾é©¶ã€‚å½“å¸§å¤–å…‰æºä¸å®Œæ•´æˆ–ç¼ºå¤±æ—¶ï¼Œæœ€è¿‘çš„å•å›¾åƒå…‰æ™•å»é™¤ï¼ˆSIFRï¼‰æ–¹æ³•è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬æå‡ºäº†LightsOutï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„å¤–æ¨æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é‡å»ºå¸§å¤–å…‰æºæ¥æé«˜SIFRçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¤šä»»åŠ¡å›å½’æ¨¡å—å’ŒLoRAå¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼Œä»¥ç¡®ä¿å¤–æ¨ç»“æœçœŸå®ä¸”ç‰©ç†ä¸€è‡´ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒLightsOutåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯æŒç»­æé«˜ç°æœ‰SIFRæ–¹æ³•çš„æ€§èƒ½ï¼Œä½œä¸ºä¸€ç§é€šç”¨å³æ’å³ç”¨çš„é¢„å¤„ç†è§£å†³æ–¹æ¡ˆã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://ray-1026.github.io/lightsout/">https://ray-1026.github.io/lightsout/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15868v1">PDF</a> ICCV 2025. Project page: <a target="_blank" rel="noopener" href="https://ray-1026.github.io/lightsout/">https://ray-1026.github.io/lightsout/</a></p>
<p><strong>Summary</strong><br>     é•œå¤´å…‰æ™•ä¼šä¸¥é‡å½±å“å›¾åƒè´¨é‡ï¼Œå¯¹ç‰©ä½“æ£€æµ‹å’Œè‡ªåŠ¨é©¾é©¶ç­‰å…³é”®è®¡ç®—æœºè§†è§‰ä»»åŠ¡é€ æˆå½±å“ã€‚é’ˆå¯¹ç°æœ‰å•å›¾åƒå…‰æ™•å»é™¤ï¼ˆSIFRï¼‰æ–¹æ³•åœ¨å…‰æºç¼ºå¤±æˆ–ä¸å®Œå…¨æ—¶è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LightsOutï¼Œä¸€ä¸ªåŸºäºæ‰©æ•£çš„å¤–ç”»æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é‡å»ºç¦»çº¿å…‰æºå¢å¼ºSIFRã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šä»»åŠ¡å›å½’æ¨¡å—å’ŒLoRAå¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼Œç¡®ä¿ç”Ÿæˆçš„ç»“æœæ—¢çœŸå®åˆç¬¦åˆç‰©ç†è§„å¾‹ã€‚å®éªŒè¯æ˜ï¼ŒLightsOutèƒ½åœ¨ä¸é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæé«˜ç°æœ‰SIFRæ–¹æ³•åœ¨ä¸åŒæŒ‘æˆ˜åœºæ™¯ä¸‹çš„æ€§èƒ½ï¼Œæ˜¯ä¸€ä¸ªé€šç”¨çš„å³æ’å³ç”¨é¢„å¤„ç†è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é•œå¤´å…‰æ™•ä¸¥é‡é™ä½å›¾åƒè´¨é‡ï¼Œå½±å“è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œå¦‚ç‰©ä½“æ£€æµ‹å’Œè‡ªåŠ¨é©¾é©¶ã€‚</li>
<li>ç°æœ‰å•å›¾åƒå…‰æ™•å»é™¤ï¼ˆSIFRï¼‰æ–¹æ³•åœ¨å…‰æºä¸å®Œæ•´æˆ–ç¼ºå¤±æ—¶è¡¨ç°ä¸ä½³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„å¤–ç”»æ¡†æ¶â€œLightsOutâ€ï¼Œæ—¨åœ¨å¢å¼ºSIFRã€‚</li>
<li>LightsOutåˆ©ç”¨å¤šä»»åŠ¡å›å½’æ¨¡å—å’ŒLoRAå¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼Œç¡®ä¿ç»“æœçœŸå®ä¸”ç¬¦åˆç‰©ç†è§„å¾‹ã€‚</li>
<li>LightsOuté€šè¿‡é‡å»ºç¦»çº¿å…‰æºæ¥æé«˜SIFRçš„æ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒLightsOutåœ¨å¤šç§æŒ‘æˆ˜åœºæ™¯ä¸‹éƒ½èƒ½æå‡ç°æœ‰SIFRæ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15868">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-03238ac8c5f0edc778a19b4bd4c51177~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992779&auth_key=1760992779-0-0-0a32f3410ccb980bcc94aeea02e93d04&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-340266da036b805141c50a17fad1aa75~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992786&auth_key=1760992786-0-0-063b57276bbad3e3bacd7eb1d36e84df&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-24028b8323a01892f0651ae7aa7945dc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992792&auth_key=1760992792-0-0-f1ca8730d86d8a4f00575a9ecf854bda&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-59e35f2dc4d751b6b2002d4e63943c7d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992799&auth_key=1760992799-0-0-0e1d1214282f60140abc229ce2ce0c91&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6baac6ebf03eb6f5873c9df5c107347e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992805&auth_key=1760992805-0-0-3aa7be5f2be8daa9c712aa8c15883360&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-654d31af098d6be8e218f51bab35cc48~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992813&auth_key=1760992813-0-0-c819454aa9fa96cd7bc23e8c4014d394&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="BLIP3o-NEXT-Next-Frontier-of-Native-Image-Generation"><a href="#BLIP3o-NEXT-Next-Frontier-of-Native-Image-Generation" class="headerlink" title="BLIP3o-NEXT: Next Frontier of Native Image Generation"></a>BLIP3o-NEXT: Next Frontier of Native Image Generation</h2><p><strong>Authors:Jiuhai Chen, Le Xue, Zhiyang Xu, Xichen Pan, Shusheng Yang, Can Qin, An Yan, Honglu Zhou, Zeyuan Chen, Lifu Huang, Tianyi Zhou, Junnan Li, Silvio Savarese, Caiming Xiong, Ran Xu</strong></p>
<p>We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3 series that advances the next frontier of native image generation. BLIP3o-NEXT unifies text-to-image generation and image editing within a single architecture, demonstrating strong image generation and image editing capabilities. In developing the state-of-the-art native image generation model, we identify four key insights: (1) Most architectural choices yield comparable performance; an architecture can be deemed effective provided it scales efficiently and supports fast inference; (2) The successful application of reinforcement learning can further push the frontier of native image generation; (3) Image editing still remains a challenging task, yet instruction following and the consistency between generated and reference images can be significantly enhanced through post-training and data engine; (4) Data quality and scale continue to be decisive factors that determine the upper bound of model performance. Building upon these insights, BLIP3o-NEXT leverages an Autoregressive + Diffusion architecture in which an autoregressive model first generates discrete image tokens conditioned on multimodal inputs, whose hidden states are then used as conditioning signals for a diffusion model to generate high-fidelity images. This architecture integrates the reasoning strength and instruction following of autoregressive models with the fine-detail rendering ability of diffusion models, achieving a new level of coherence and realism. Extensive evaluations of various text-to-image and image-editing benchmarks show that BLIP3o-NEXT achieves superior performance over existing models. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºBLIP3o-NEXTï¼Œè¿™æ˜¯BLIP3ç³»åˆ—ä¸­çš„å…¨æ–°å¼€æºåŸºç¡€æ¨¡å‹ï¼Œè¿›ä¸€æ­¥æ¨åŠ¨äº†åŸç”Ÿå›¾åƒç”ŸæˆæŠ€æœ¯çš„è¾¹ç•Œã€‚BLIP3o-NEXTåœ¨å•ä¸€æ¶æ„å†…ç»Ÿä¸€äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘ï¼Œå±•ç°å‡ºå¼ºå¤§çš„å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘èƒ½åŠ›ã€‚åœ¨å¼€å‘æœ€å…ˆè¿›çš„åŸç”Ÿå›¾åƒç”Ÿæˆæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬è·å¾—äº†å››ä¸ªå…³é”®è§è§£ï¼š(1) å¤§å¤šæ•°æ¶æ„é€‰æ‹©éƒ½èƒ½äº§ç”Ÿç›¸å½“çš„æ€§èƒ½ï¼›ä¸€ä¸ªæœ‰æ•ˆçš„æ¶æ„åº”è¯¥èƒ½å¤Ÿé«˜æ•ˆæ‰©å±•å¹¶æ”¯æŒå¿«é€Ÿæ¨ç†ï¼›(2) å¼ºåŒ–å­¦ä¹ çš„æˆåŠŸåº”ç”¨å¯ä»¥è¿›ä¸€æ­¥æ¨åŠ¨åŸç”Ÿå›¾åƒç”Ÿæˆçš„è¾¹ç•Œï¼›(3) å›¾åƒç¼–è¾‘ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä½†é€šè¿‡äº‹åè®­ç»ƒå’Œæ•°æ®ä¸­å¿ƒï¼ŒæŒ‡ä»¤éµå¾ªå’Œç”Ÿæˆå›¾åƒä¸å‚è€ƒå›¾åƒä¹‹é—´çš„ä¸€è‡´æ€§å¯ä»¥å¾—åˆ°æ˜¾ç€å¢å¼ºï¼›(4) æ•°æ®çš„è´¨é‡å’Œè§„æ¨¡ä»ç„¶æ˜¯å†³å®šæ¨¡å‹æ€§èƒ½ä¸Šé™çš„å†³å®šæ€§å› ç´ ã€‚åŸºäºè¿™äº›è§è§£ï¼ŒBLIP3o-NEXTé‡‡ç”¨è‡ªå›å½’+æ‰©æ•£æ¶æ„ï¼Œå…¶ä¸­è‡ªå›å½’æ¨¡å‹é¦–å…ˆæ ¹æ®å¤šæ¨¡å¼è¾“å…¥ç”Ÿæˆç¦»æ•£å›¾åƒä»¤ç‰Œï¼Œå…¶éšè—çŠ¶æ€ç„¶åç”¨ä½œæ‰©æ•£æ¨¡å‹çš„è°ƒèŠ‚ä¿¡å·ä»¥ç”Ÿæˆé«˜ä¿çœŸå›¾åƒã€‚è¯¥æ¶æ„å°†è‡ªå›å½’æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’ŒæŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›ä¸æ‰©æ•£æ¨¡å‹çš„ç²¾ç»†ç»†èŠ‚æ¸²æŸ“èƒ½åŠ›ç›¸ç»“åˆï¼Œå®ç°äº†å…¨æ–°æ°´å¹³çš„è¿è´¯æ€§å’Œé€¼çœŸåº¦ã€‚å¯¹å¤šç§æ–‡æœ¬åˆ°å›¾åƒå’Œå›¾åƒç¼–è¾‘åŸºå‡†çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒBLIP3o-NEXTåœ¨ç°æœ‰æ¨¡å‹ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15857v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>BLIP3o-NEXTæ˜¯ä¸€æ¬¾å¼€æºçš„BLIP3ç³»åˆ—åŸºç¡€æ¨¡å‹ï¼Œæ¨åŠ¨äº†åŸç”Ÿå›¾åƒç”Ÿæˆçš„å‰æ²¿ã€‚å®ƒç»Ÿä¸€äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘åœ¨ä¸€ä¸ªæ¶æ„å†…ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘èƒ½åŠ›ã€‚åŸºäºå››ä¸ªå…³é”®è§è§£æ„å»ºäº†è¿™ä¸€å…ˆè¿›çš„åŸç”Ÿå›¾åƒç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬é€‰æ‹©æœ‰æ•ˆçš„æ¶æ„ã€åº”ç”¨å¼ºåŒ–å­¦ä¹ æ¨åŠ¨æŠ€æœ¯è¾¹ç•Œã€å›¾åƒç¼–è¾‘çš„æŒ‘æˆ˜å’Œæå‡ç”Ÿæˆå›¾åƒè´¨é‡ç­‰ã€‚BLIP3o-NEXTåˆ©ç”¨è‡ªå›å½’+æ‰©æ•£æ¶æ„ï¼Œç»“åˆäº†è‡ªå›å½’æ¨¡å‹çš„æ¨ç†ä¼˜åŠ¿å’Œæ‰©æ•£æ¨¡å‹çš„ç²¾ç»†æ¸²æŸ“èƒ½åŠ›ï¼Œå®ç°äº†é«˜ä¿çœŸå›¾åƒçš„ç”Ÿæˆï¼Œå¹¶åœ¨æ–‡æœ¬åˆ°å›¾åƒå’Œå›¾åƒç¼–è¾‘çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BLIP3o-NEXTæ˜¯BLIP3ç³»åˆ—ä¸­çš„å…¨æ–°å¼€æºæ¨¡å‹ï¼Œç”¨äºæ¨è¿›åŸç”Ÿå›¾åƒç”ŸæˆæŠ€æœ¯ã€‚</li>
<li>æ¨¡å‹å…·å¤‡å¼ºå¤§çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘èƒ½åŠ›ï¼Œå¯åœ¨å•ä¸€æ¶æ„å†…å®Œæˆã€‚</li>
<li>æ¨¡å‹åŸºäºå››ä¸ªå…³é”®è§è§£æ„å»ºï¼šé€‰æ‹©æœ‰æ•ˆæ¶æ„ã€åº”ç”¨å¼ºåŒ–å­¦ä¹ æå‡æ€§èƒ½ã€åº”å¯¹å›¾åƒç¼–è¾‘æŒ‘æˆ˜ä»¥åŠæé«˜ç”Ÿæˆå›¾åƒè´¨é‡ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨è‡ªå›å½’+æ‰©æ•£æ¶æ„ï¼Œç»“åˆäº†ä¸¤ç§æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œå®ç°äº†é«˜ä¿çœŸå›¾åƒçš„ç”Ÿæˆã€‚</li>
<li>æ•°æ®è´¨é‡å’Œè§„æ¨¡æ˜¯å†³å®šæ¨¡å‹æ€§èƒ½çš„é‡è¦å› ç´ ã€‚</li>
<li>BLIP3o-NEXTåœ¨æ–‡æœ¬åˆ°å›¾åƒå’Œå›¾åƒç¼–è¾‘çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šã€‚</li>
<li>æ¨¡å‹å…·å¤‡é«˜åº¦é€šç”¨æ€§ï¼Œå¯å¹¿æ³›åº”ç”¨äºå„ç§åº”ç”¨åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-563c38d60c49e249f470fbb0d646425d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992820&auth_key=1760992820-0-0-75a8573d940f7013b49415bbb98052d3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-18bb89294a1dafcc53f905b9653cf4b5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992827&auth_key=1760992827-0-0-a10001dc7354efa31ddcbd0ca5126b45&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-893fcd2effcc7a8c070f80df79ae88d1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992833&auth_key=1760992833-0-0-bf173bbc94257d10a233263c0636c0c2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-78fd76f34459102ee7e2df956f7d792c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992840&auth_key=1760992840-0-0-e9fbbb7a3a41fd4d5c27745e8be7b9c0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-96881ec6f0183e91dbce89c3cb7d8d03~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992846&auth_key=1760992846-0-0-a60923ef728a5244f4fe606fe9170042&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="NDM-A-Noise-driven-Detection-and-Mitigation-Framework-against-Implicit-Sexual-Intentions-in-Text-to-Image-Generation"><a href="#NDM-A-Noise-driven-Detection-and-Mitigation-Framework-against-Implicit-Sexual-Intentions-in-Text-to-Image-Generation" class="headerlink" title="NDM: A Noise-driven Detection and Mitigation Framework against Implicit   Sexual Intentions in Text-to-Image Generation"></a>NDM: A Noise-driven Detection and Mitigation Framework against Implicit   Sexual Intentions in Text-to-Image Generation</h2><p><strong>Authors:Yitong Sun, Yao Huang, Ruochen Zhang, Huanran Chen, Shouwei Ruan, Ranjie Duan, Xingxing Wei</strong></p>
<p>Despite the impressive generative capabilities of text-to-image (T2I) diffusion models, they remain vulnerable to generating inappropriate content, especially when confronted with implicit sexual prompts. Unlike explicit harmful prompts, these subtle cues, often disguised as seemingly benign terms, can unexpectedly trigger sexual content due to underlying model biases, raising significant ethical concerns. However, existing detection methods are primarily designed to identify explicit sexual content and therefore struggle to detect these implicit cues. Fine-tuning approaches, while effective to some extent, risk degrading the modelâ€™s generative quality, creating an undesirable trade-off. To address this, we propose NDM, the first noise-driven detection and mitigation framework, which could detect and mitigate implicit malicious intention in T2I generation while preserving the modelâ€™s original generative capabilities. Specifically, we introduce two key innovations: first, we leverage the separability of early-stage predicted noise to develop a noise-based detection method that could identify malicious content with high accuracy and efficiency; second, we propose a noise-enhanced adaptive negative guidance mechanism that could optimize the initial noise by suppressing the prominent regionâ€™s attention, thereby enhancing the effectiveness of adaptive negative guidance for sexual mitigation. Experimentally, we validate NDM on both natural and adversarial datasets, demonstrating its superior performance over existing SOTA methods, including SLD, UCE, and RECE, etc. Code and resources are available at <a target="_blank" rel="noopener" href="https://github.com/lorraine021/NDM">https://github.com/lorraine021/NDM</a>. </p>
<blockquote>
<p>å°½ç®¡æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹å…·æœ‰ä»¤äººå°è±¡æ·±åˆ»çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å®ƒä»¬ä»ç„¶å®¹æ˜“ç”Ÿæˆä¸åˆé€‚çš„å†…å®¹ï¼Œå°¤å…¶æ˜¯åœ¨é¢å¯¹éšæ™¦çš„æ€§æç¤ºæ—¶ã€‚ä¸æ˜ç¡®çš„æœ‰å®³æç¤ºä¸åŒï¼Œè¿™äº›å¾®å¦™çš„çº¿ç´¢å¾€å¾€ä¼ªè£…æˆçœ‹ä¼¼æ— å®³çš„æœ¯è¯­ï¼Œç”±äºæ¨¡å‹æ½œåœ¨çš„åè§ï¼Œå®ƒä»¬å¯èƒ½ä¼šæ„å¤–è§¦å‘æ€§å†…å®¹ï¼Œè¿™å¼•å‘äº†é‡å¤§çš„ä¼¦ç†æ‹…å¿§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ£€æµ‹æ–¹æ³•ä¸»è¦è®¾è®¡ç”¨äºè¯†åˆ«æ˜ç¡®çš„æ€§å†…å®¹ï¼Œå› æ­¤å¾ˆéš¾æ£€æµ‹è¿™äº›éšå«çš„çº¿ç´¢ã€‚å¾®è°ƒæ–¹æ³•è™½ç„¶åœ¨ä¸€å®šç¨‹åº¦ä¸Šæœ‰æ•ˆï¼Œä½†å¯èƒ½ä¼šé™ä½æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ï¼Œé€ æˆä¸ç†æƒ³çš„æƒè¡¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†NDMï¼Œå³é¦–ä¸ªå™ªå£°é©±åŠ¨çš„æ£€æµ‹ä¸ç¼“è§£æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¿ç•™æ¨¡å‹åŸå§‹ç”Ÿæˆèƒ½åŠ›çš„åŒæ—¶ï¼Œæ£€æµ‹å¹¶ç¼“è§£T2Iç”Ÿæˆä¸­çš„éšå«æ¶æ„æ„å›¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨æ—©æœŸé˜¶æ®µé¢„æµ‹å™ªå£°çš„å¯åˆ†ç¦»æ€§ï¼Œå¼€å‘äº†ä¸€ç§åŸºäºå™ªå£°çš„æ£€æµ‹æ–¹æ³•ï¼Œèƒ½å¤Ÿé«˜æ•ˆå‡†ç¡®åœ°è¯†åˆ«æ¶æ„å†…å®¹ï¼›å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºå‹è‡ªé€‚åº”è´Ÿå¼•å¯¼æœºåˆ¶ï¼Œé€šè¿‡æŠ‘åˆ¶æ˜¾è‘—åŒºåŸŸçš„æ³¨æ„åŠ›æ¥ä¼˜åŒ–åˆå§‹å™ªå£°ï¼Œä»è€Œæé«˜è‡ªé€‚åº”è´Ÿå¼•å¯¼åœ¨æ€§å†…å®¹ç¼“è§£æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬åœ¨è‡ªç„¶å’Œå¯¹æŠ—æ•°æ®é›†ä¸ŠéªŒè¯äº†NDMçš„æ€§èƒ½ï¼Œè¯æ˜å…¶ä¼˜äºåŒ…æ‹¬SLDã€UCEå’ŒRECEç­‰ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ç›¸å…³ä»£ç å’Œèµ„æºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lorraine021/NDM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lorraine021/NDMæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15752v1">PDF</a> 10 pages, 8 figures, accepted by ACMMM 2025</p>
<p><strong>æ‘˜è¦</strong><br>    æ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå†…å®¹æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ä¹Ÿå­˜åœ¨ç”Ÿæˆä¸å½“å†…å®¹çš„é£é™©ï¼Œç‰¹åˆ«æ˜¯é¢å¯¹éšå«æ€§æš—ç¤ºæ—¶ã€‚ç°æœ‰æ£€æµ‹æ‰‹æ®µä¸»è¦å…³æ³¨æ˜æ˜¾æ€§å†…å®¹ï¼Œéš¾ä»¥è¯†åˆ«éšå«æ€§æš—ç¤ºã€‚æœ¬æ–‡æå‡ºä¸€ç§å™ªå£°é©±åŠ¨çš„æ£€æµ‹ä¸ç¼“è§£æ¡†æ¶NDMï¼Œèƒ½åœ¨ä¿æŒæ¨¡å‹åŸæœ‰ç”Ÿæˆèƒ½åŠ›çš„åŒæ—¶ï¼Œæ£€æµ‹å’Œç¼“è§£å›¾åƒç”Ÿæˆä¸­çš„éšå«æ¶æ„æ„å›¾ã€‚è¯¥æ¡†æ¶å¼•å…¥ä¸¤ç§å…³é”®æŠ€æœ¯ï¼šä¸€æ˜¯åŸºäºæ—©æœŸé¢„æµ‹å™ªå£°çš„å¯åˆ†ç¦»æ€§è¿›è¡Œå™ªå£°æ£€æµ‹ï¼›äºŒæ˜¯ä¼˜åŒ–åˆå§‹å™ªå£°ï¼Œå¢å¼ºè‡ªé€‚åº”è´Ÿå‘å¼•å¯¼æœºåˆ¶ã€‚å®éªŒè¯æ˜ï¼ŒNDMåœ¨è‡ªç„¶å’Œå¯¹æŠ—æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹å­˜åœ¨ç”Ÿæˆä¸å½“å†…å®¹çš„é£é™©ï¼Œå°¤å…¶æ˜¯é¢å¯¹éšå«æ€§æš—ç¤ºæ—¶ã€‚</li>
<li>ç°æœ‰æ£€æµ‹æ‰‹æ®µéš¾ä»¥è¯†åˆ«éšå«æ€§æš—ç¤ºï¼Œä¸»è¦å…³æ³¨æ˜æ˜¾æ€§å†…å®¹çš„æ£€æµ‹ã€‚</li>
<li>æå‡ºä¸€ç§å™ªå£°é©±åŠ¨çš„æ£€æµ‹ä¸ç¼“è§£æ¡†æ¶NDMï¼Œèƒ½æ£€æµ‹å¹¶ç¼“è§£å›¾åƒç”Ÿæˆä¸­çš„éšå«æ¶æ„æ„å›¾ã€‚</li>
<li>NDMå¼•å…¥ä¸¤ç§å…³é”®æŠ€æœ¯ï¼šåŸºäºæ—©æœŸé¢„æµ‹å™ªå£°çš„æ£€æµ‹æ–¹æ³•å’Œä¼˜åŒ–åˆå§‹å™ªå£°çš„è‡ªé€‚åº”è´Ÿå‘å¼•å¯¼æœºåˆ¶ã€‚</li>
<li>NDMæ¡†æ¶åœ¨ä¿æŒæ¨¡å‹åŸæœ‰ç”Ÿæˆèƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†å¯¹éšå«æ¶æ„æ„å›¾çš„æœ‰æ•ˆæ£€æµ‹å’Œç¼“è§£ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒNDMåœ¨è‡ªç„¶å’Œå¯¹æŠ—æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼ŒåŒ…æ‹¬SLDã€UCEå’ŒRECEç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0ee427faa100a9d7b7189a56d0ae38cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992854&auth_key=1760992854-0-0-1c1878151bcdde39fa832a76f9bd7a75&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c0e037356e09c20d83de97857031a416~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992861&auth_key=1760992861-0-0-e9b86f37d1271aa6544b0d3c5ab7fc4f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a5b841d4b97e3602adb507df8088fdac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992868&auth_key=1760992868-0-0-8607fb6ffc24c94407b33070e3143f3f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e7affd422914210f29b763fc748b21ba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992875&auth_key=1760992875-0-0-fc71d030816d7cf36adf8e86063c2427&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-79e50316c1109e38a338c34db5799946~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992882&auth_key=1760992882-0-0-6f55e23cbe60f941ad0302cbd75adca9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Exploring-Conditions-for-Diffusion-models-in-Robotic-Control"><a href="#Exploring-Conditions-for-Diffusion-models-in-Robotic-Control" class="headerlink" title="Exploring Conditions for Diffusion models in Robotic Control"></a>Exploring Conditions for Diffusion models in Robotic Control</h2><p><strong>Authors:Heeseong Shin, Byeongho Heo, Dongyoon Han, Seungryong Kim, Taekyung Kim</strong></p>
<p>While pre-trained visual representations have significantly advanced imitation learning, they are often task-agnostic as they remain frozen during policy learning. In this work, we explore leveraging pre-trained text-to-image diffusion models to obtain task-adaptive visual representations for robotic control, without fine-tuning the model itself. However, we find that naively applying textual conditions - a successful strategy in other vision domains - yields minimal or even negative gains in control tasks. We attribute this to the domain gap between the diffusion modelâ€™s training data and robotic control environments, leading us to argue for conditions that consider the specific, dynamic visual information required for control. To this end, we propose ORCA, which introduces learnable task prompts that adapt to the control environment and visual prompts that capture fine-grained, frame-specific details. Through facilitating task-adaptive representations with our newly devised conditions, our approach achieves state-of-the-art performance on various robotic control benchmarks, significantly surpassing prior methods. </p>
<blockquote>
<p>é¢„è®­ç»ƒè§†è§‰è¡¨å¾åœ¨æ¨¡ä»¿å­¦ä¹ ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬é€šå¸¸æ˜¯ä»»åŠ¡æ— å…³çš„ï¼Œå› ä¸ºåœ¨ç­–ç•¥å­¦ä¹ è¿‡ç¨‹ä¸­ä¿æŒå†»ç»“ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä»¥è·å¾—ç”¨äºæœºå™¨äººæ§åˆ¶çš„ä»»åŠ¡é€‚åº”æ€§è§†è§‰è¡¨å¾ï¼Œè€Œæ— éœ€å¾®è°ƒæ¨¡å‹æœ¬èº«ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°ï¼Œå¤©çœŸåœ°åº”ç”¨æ–‡æœ¬æ¡ä»¶ï¼ˆåœ¨å…¶ä»–è§†è§‰é¢†åŸŸå–å¾—æˆåŠŸçš„ç­–ç•¥ï¼‰åœ¨æ§åˆ¶ä»»åŠ¡ä¸­äº§ç”Ÿå¾®å°ç”šè‡³è´Ÿé¢çš„æ”¶ç›Šã€‚æˆ‘ä»¬å°†è¿™ä¸€ç°è±¡å½’å› äºæ‰©æ•£æ¨¡å‹çš„è®­ç»ƒæ•°æ®ä¸æœºå™¨äººæ§åˆ¶ç¯å¢ƒä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œä¿ƒä½¿æˆ‘ä»¬æå‡ºè€ƒè™‘æ§åˆ¶æ‰€éœ€çš„å…·ä½“ã€åŠ¨æ€è§†è§‰ä¿¡æ¯çš„æ¡ä»¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ORCAï¼Œå®ƒå¼•å…¥äº†å¯å­¦ä¹ çš„ä»»åŠ¡æç¤ºï¼Œä»¥é€‚åº”æ§åˆ¶ç¯å¢ƒå’Œè§†è§‰æç¤ºï¼Œæ•æ‰ç²¾ç»†çš„ã€å¸§ç‰¹å®šçš„ç»†èŠ‚ã€‚é€šè¿‡æˆ‘ä»¬æ–°è®¾è®¡çš„æ¡ä»¶ä¿ƒè¿›ä»»åŠ¡é€‚åº”æ€§è¡¨å¾ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§æœºå™¨äººæ§åˆ¶åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—è¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15510v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://orca-rc.github.io/">https://orca-rc.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†å¦‚ä½•åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä¸ºæœºå™¨äººæ§åˆ¶è·å¾—ä»»åŠ¡é€‚åº”æ€§è§†è§‰è¡¨ç¤ºï¼Œè€Œæ— éœ€å¾®è°ƒæ¨¡å‹æœ¬èº«ã€‚ç„¶è€Œï¼Œä½œè€…å‘ç°ç›´æ¥åº”ç”¨æ–‡æœ¬æ¡ä»¶åœ¨æ§åˆ¶ä»»åŠ¡ä¸Šæ”¶ç›Šç”šå¾®ç”šè‡³äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ORCAæ–¹æ³•ï¼Œå¼•å…¥å¯å­¦ä¹ çš„ä»»åŠ¡æç¤ºï¼Œä»¥é€‚åº”æ§åˆ¶ç¯å¢ƒå’Œè§†è§‰æç¤ºï¼Œæ•æ‰ç²¾ç»†çš„å¸§ç‰¹å®šç»†èŠ‚ã€‚é€šè¿‡æ–°çš„æ¡ä»¶ä¿ƒè¿›ä»»åŠ¡é€‚åº”æ€§è¡¨ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æœºå™¨äººæ§åˆ¶åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ï¼Œæ˜¾è‘—è¶…è¶Šäº†å…ˆå‰çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¯ç”¨äºæœºå™¨äººæ§åˆ¶çš„è§†è§‰è¡¨ç¤ºã€‚</li>
<li>ç›´æ¥åº”ç”¨æ–‡æœ¬æ¡ä»¶åœ¨æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸Šçš„æ•ˆæœæœ‰é™ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒæ•°æ®ä¸æœºå™¨äººæ§åˆ¶ç¯å¢ƒä¹‹é—´å­˜åœ¨é¢†åŸŸå·®è·ã€‚</li>
<li>å¼•å…¥å¯å­¦ä¹ çš„ä»»åŠ¡æç¤ºå’Œè§†è§‰æç¤ºä»¥é€‚åº”æ§åˆ¶ç¯å¢ƒå’Œæ•æ‰å¸§ç‰¹å®šç»†èŠ‚ã€‚</li>
<li>ORCAæ–¹æ³•é€šè¿‡ä¿ƒè¿›ä»»åŠ¡é€‚åº”æ€§è¡¨ç¤ºï¼Œå®ç°äº†æœºå™¨äººæ§åˆ¶ä»»åŠ¡çš„å“è¶Šæ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å„ç§æœºå™¨äººæ§åˆ¶åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†å…ˆå‰çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15510">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6407f69951b66d487df7f73b483c32e9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992889&auth_key=1760992889-0-0-f04533c919499b60fb2671740ee67d62&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1d67bc2ec1d347c86b4718b9b7e46ca4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992897&auth_key=1760992897-0-0-d8de2a076ea2f7159f534eda4a0240fd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c0ecd284e3f1d5f7f7d4983cc218c746~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992904&auth_key=1760992904-0-0-5c991f62e735eb7ed6026858e63cfe3c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-db82107e9253fa116c1d5ed1f8b8e68b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992911&auth_key=1760992911-0-0-47d8b2dc4c2667dbb2367e452c8dcd7a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8398e09640edc3909948ac186796b4d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992917&auth_key=1760992917-0-0-3846733ca0159c40b41b2f82df6848db&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Deep-generative-priors-for-3D-brain-analysis"><a href="#Deep-generative-priors-for-3D-brain-analysis" class="headerlink" title="Deep generative priors for 3D brain analysis"></a>Deep generative priors for 3D brain analysis</h2><p><strong>Authors:Ana Lawry Aguila, Dina Zemlyanker, You Cheng, Sudeshna Das, Daniel C. Alexander, Oula Puonti, Annabel Sorby-Adams, W. Taylor Kimberly, Juan Eugenio Iglesias</strong></p>
<p>Diffusion models have recently emerged as powerful generative models in medical imaging. However, it remains a major challenge to combine these data-driven models with domain knowledge to guide brain imaging problems. In neuroimaging, Bayesian inverse problems have long provided a successful framework for inference tasks, where incorporating domain knowledge of the imaging process enables robust performance without requiring extensive training data. However, the anatomical modeling component of these approaches typically relies on classical mathematical priors that often fail to capture the complex structure of brain anatomy. In this work, we present the first general-purpose application of diffusion models as priors for solving a wide range of medical imaging inverse problems. Our approach leverages a score-based diffusion prior trained extensively on diverse brain MRI data, paired with flexible forward models that capture common image processing tasks such as super-resolution, bias field correction, inpainting, and combinations thereof. We further demonstrate how our framework can refine outputs from existing deep learning methods to improve anatomical fidelity. Experiments on heterogeneous clinical and research MRI data show that our method achieves state-of-the-art performance producing consistent, high-quality solutions without requiring paired training datasets. These results highlight the potential of diffusion priors as versatile tools for brain MRI analysis. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹æœ€è¿‘ä½œä¸ºåŒ»å­¦æˆåƒä¸­çš„å¼ºå¤§ç”Ÿæˆæ¨¡å‹è€Œå‡ºç°ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ•°æ®é©±åŠ¨æ¨¡å‹ä¸é¢†åŸŸçŸ¥è¯†ç›¸ç»“åˆä»¥æŒ‡å¯¼å¤§è„‘æˆåƒé—®é¢˜ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚åœ¨ç¥ç»æˆåƒä¸­ï¼Œè´å¶æ–¯åé—®é¢˜é•¿æœŸä»¥æ¥ä¸ºæ¨ç†ä»»åŠ¡æä¾›äº†æˆåŠŸçš„æ¡†æ¶ï¼Œå…¶ä¸­ç»“åˆæˆåƒè¿‡ç¨‹çš„é¢†åŸŸçŸ¥è¯†èƒ½å¤Ÿåœ¨ä¸éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹å®ç°ç¨³å¥æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•çš„è§£å‰–å»ºæ¨¡éƒ¨åˆ†é€šå¸¸ä¾èµ–äºç»å…¸çš„æ•°å­¦å…ˆéªŒçŸ¥è¯†ï¼Œè¿™å¾€å¾€æ— æ³•æ•æ‰åˆ°å¤§è„‘è§£å‰–ç»“æ„çš„å¤æ‚ç‰¹æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºäº†å°†æ‰©æ•£æ¨¡å‹ä½œä¸ºå…ˆéªŒçŸ¥è¯†æ¥è§£å†³ä¸€ç³»åˆ—å¹¿æ³›çš„åŒ»å­¦æˆåƒåé—®é¢˜çš„é€šç”¨åº”ç”¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨åœ¨å¤šæ ·åŒ–çš„è„‘éƒ¨MRIæ•°æ®ä¸Šç»è¿‡å¹¿æ³›è®­ç»ƒçš„åŸºäºåˆ†æ•°çš„æ‰©æ•£å…ˆéªŒï¼Œä»¥åŠèƒ½å¤Ÿå®Œæˆè¶…åˆ†è¾¨ç‡ã€åç½®åœºæ ¡æ­£ã€å›¾åƒä¿®å¤ç­‰å¸¸è§å›¾åƒå¤„ç†ä»»åŠ¡çš„çµæ´»æ­£å‘æ¨¡å‹ï¼Œå¹¶å°†å®ƒä»¬ç»„åˆèµ·æ¥ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†æˆ‘ä»¬çš„æ¡†æ¶å¦‚ä½•ä¼˜åŒ–ç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•çš„è¾“å‡ºï¼Œä»¥æé«˜è§£å‰–ç»“æ„çš„å¿ å®åº¦ã€‚åœ¨å¼‚è´¨çš„ä¸´åºŠå’Œç ”ç©¶MRIæ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œäº§ç”Ÿäº†è¿è´¯çš„é«˜è´¨é‡è§£å†³æ–¹æ¡ˆï¼Œè€Œæ— éœ€é…å¯¹è®­ç»ƒæ•°æ®é›†ã€‚è¿™äº›ç»“æœçªå‡ºäº†æ‰©æ•£å…ˆéªŒä½œä¸ºå¤§è„‘MRIåˆ†æé€šç”¨å·¥å…·çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15119v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦æˆåƒä¸­å±•ç°å‡ºå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å¦‚ä½•å°†æ•°æ®é©±åŠ¨æ¨¡å‹ä¸é¢†åŸŸçŸ¥è¯†ç»“åˆä»¥è§£å†³ç¥ç»æˆåƒä¸­çš„é€†å‘é—®é¢˜æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºè§£å†³å¹¿æ³›çš„åŒ»å­¦æˆåƒé€†å‘é—®é¢˜ï¼Œä½œä¸ºå…ˆéªŒçŸ¥è¯†ã€‚æˆ‘ä»¬åˆ©ç”¨åœ¨å¤šæ ·çš„å¤§è„‘MRIæ•°æ®ä¸Šè®­ç»ƒçš„åŸºäºåˆ†æ•°çš„æ‰©æ•£å…ˆéªŒï¼Œé…åˆçµæ´»çš„å‰å‘æ¨¡å‹ï¼Œæ¶µç›–è¶…åˆ†è¾¨ç‡ã€åç½®åœºæ ¡æ­£ã€å›¾åƒè¡¥å…¨ç­‰å¸¸è§å›¾åƒå¤„ç†ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸éœ€é…å¯¹è®­ç»ƒæ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œäº§ç”Ÿä¸€è‡´çš„é«˜è´¨é‡è§£å†³æ–¹æ¡ˆï¼Œå¹¶èƒ½ä¼˜åŒ–ç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•çš„è¾“å‡ºï¼Œæé«˜è§£å‰–çœŸå®æ€§ã€‚è¿™çªæ˜¾äº†æ‰©æ•£å…ˆéªŒåœ¨å¤§è„‘MRIåˆ†æä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦æˆåƒä¸­çš„ç”Ÿæˆèƒ½åŠ›å¼ºå¤§ã€‚</li>
<li>ç»“åˆé¢†åŸŸçŸ¥è¯†æ˜¯è§£å†³ç¥ç»æˆåƒé€†å‘é—®é¢˜çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>æœ¬ç ”ç©¶é¦–æ¬¡å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºåŒ»å­¦æˆåƒé€†å‘é—®é¢˜çš„é€šç”¨è§£å†³æ–¹æ¡ˆä¸­ä½œä¸ºå…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>åˆ©ç”¨åŸºäºåˆ†æ•°çš„æ‰©æ•£å…ˆéªŒï¼Œåœ¨å¤šæ ·çš„å¤§è„‘MRIæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
<li>çµæ´»çš„å‰å‘æ¨¡å‹å¯ä»¥æ¶µç›–è¶…åˆ†è¾¨ç‡ã€åç½®åœºæ ¡æ­£ã€å›¾åƒè¡¥å…¨ç­‰å¸¸è§å›¾åƒå¤„ç†ä»»åŠ¡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¼‚è´¨çš„ä¸´åºŠå’Œç ”ç©¶MRIæ•°æ®ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼Œäº§ç”Ÿé«˜è´¨é‡è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ‰©æ•£å…ˆéªŒåœ¨å¤§è„‘MRIåˆ†æä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15119">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-bad2dbccf4e133336a2933c2607c8323~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992924&auth_key=1760992924-0-0-932b7f29632dfe604ac6424c2c6aff6a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-344523dea5be90a9984c6696c0d74bc4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992931&auth_key=1760992931-0-0-749d8bf963f622872d6d2fb4b443e0df&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Sequential-Comics-for-Jailbreaking-Multimodal-Large-Language-Models-via-Structured-Visual-Storytelling"><a href="#Sequential-Comics-for-Jailbreaking-Multimodal-Large-Language-Models-via-Structured-Visual-Storytelling" class="headerlink" title="Sequential Comics for Jailbreaking Multimodal Large Language Models via   Structured Visual Storytelling"></a>Sequential Comics for Jailbreaking Multimodal Large Language Models via   Structured Visual Storytelling</h2><p><strong>Authors:Deyue Zhang, Dongdong Yang, Junjie Mu, Quancheng Zou, Zonghao Ying, Wenzhuo Xu, Zhao Liu, Xuan Wang, Xiangzheng Zhang</strong></p>
<p>Multimodal large language models (MLLMs) exhibit remarkable capabilities but remain susceptible to jailbreak attacks exploiting cross-modal vulnerabilities. In this work, we introduce a novel method that leverages sequential comic-style visual narratives to circumvent safety alignments in state-of-the-art MLLMs. Our method decomposes malicious queries into visually innocuous storytelling elements using an auxiliary LLM, generates corresponding image sequences through diffusion models, and exploits the modelsâ€™ reliance on narrative coherence to elicit harmful outputs. Extensive experiments on harmful textual queries from established safety benchmarks show that our approach achieves an average attack success rate of 83.5%, surpassing prior state-of-the-art by 46%. Compared with existing visual jailbreak methods, our sequential narrative strategy demonstrates superior effectiveness across diverse categories of harmful content. We further analyze attack patterns, uncover key vulnerability factors in multimodal safety mechanisms, and evaluate the limitations of current defense strategies against narrative-driven attacks, revealing significant gaps in existing protections. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†ä»ç„¶å®¹æ˜“å—åˆ°åˆ©ç”¨è·¨æ¨¡æ€æ¼æ´çš„è¶Šç‹±æ”»å‡»ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è¿ç»­çš„æ¼«ç”»å¼è§†è§‰å™äº‹æ¥è§„é¿æœ€å…ˆè¿›MLLMsä¸­çš„å®‰å…¨å¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨è¾…åŠ©å‹å¤§å‹è¯­è¨€æ¨¡å‹å°†æ¶æ„æŸ¥è¯¢åˆ†è§£æˆè§†è§‰ä¸Šæ— å®³çš„å™äº‹å…ƒç´ ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆç›¸åº”çš„å›¾åƒåºåˆ—ï¼Œå¹¶åˆ©ç”¨æ¨¡å‹å¯¹å™äº‹è¿è´¯æ€§çš„ä¾èµ–æ¥æ¿€å‘æœ‰å®³è¾“å‡ºã€‚å¯¹æœ‰å®³æ–‡æœ¬æŸ¥è¯¢çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¹³å‡æ”»å‡»æˆåŠŸç‡ä¸º83.5%ï¼Œæ¯”ç°æœ‰æŠ€æœ¯é«˜å‡º46%ã€‚ä¸ç°æœ‰çš„è§†è§‰è¶Šç‹±æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„è¿ç»­å™äº‹ç­–ç•¥åœ¨å„ç±»æœ‰å®³å†…å®¹ä¸­è¡¨ç°å‡ºæ›´é«˜çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æäº†æ”»å‡»æ¨¡å¼ï¼Œæ­ç¤ºäº†å¤šæ¨¡æ€å®‰å…¨æœºåˆ¶ä¸­çš„å…³é”®è„†å¼±æ€§å› ç´ ï¼Œå¹¶è¯„ä¼°äº†ç°æœ‰é˜²å¾¡ç­–ç•¥å¯¹å™äº‹é©±åŠ¨æ”»å‡»çš„å±€é™æ€§ï¼Œæ­ç¤ºäº†ç°æœ‰é˜²æŠ¤ä¸­çš„é‡å¤§å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15068v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§åˆ©ç”¨åºè´¯æ¼«ç”»å¼è§†è§‰å™äº‹æ¥è§„é¿å½“å‰æœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å®‰å…¨å¯¹é½çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡è¾…åŠ©å¤§å‹è¯­è¨€æ¨¡å‹å°†æ¶æ„æŸ¥è¯¢åˆ†è§£ä¸ºè§†è§‰ä¸Šæ— å®³çš„å™äº‹å…ƒç´ ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆç›¸åº”çš„å›¾åƒåºåˆ—ï¼Œå¹¶åˆ©ç”¨æ¨¡å‹å¯¹å™äº‹è¿è´¯æ€§çš„ä¾èµ–æ¥å¼•å‘æœ‰å®³è¾“å‡ºã€‚åœ¨æ¥è‡ªå·²å»ºç«‹å®‰å…¨åŸºå‡†çš„æœ‰å®³æ–‡æœ¬æŸ¥è¯¢çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¹³å‡æ”»å‡»æˆåŠŸç‡ä¸º83.5%ï¼Œæ¯”ç°æœ‰æŠ€æœ¯é«˜å‡º46%ã€‚ä¸ç°æœ‰çš„è§†è§‰è¶Šç‹±æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„åºè´¯å™äº‹ç­–ç•¥åœ¨å„ç±»æœ‰å®³å†…å®¹ä¸­è¡¨ç°å‡ºæ›´é«˜çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥åˆ†æäº†æ”»å‡»æ¨¡å¼ï¼Œæ­ç¤ºäº†å¤šæ¨¡æ€å®‰å…¨æœºåˆ¶ä¸­çš„å…³é”®è„†å¼±æ€§å› ç´ ï¼Œå¹¶è¯„ä¼°äº†é’ˆå¯¹å™äº‹é©±åŠ¨æ”»å‡»çš„ç°æœ‰é˜²å¾¡ç­–ç•¥çš„é™åˆ¶ï¼Œæš´éœ²å‡ºå½“å‰ä¿æŠ¤çš„é‡å¤§æ¼æ´ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å®¹æ˜“å—åˆ°åˆ©ç”¨è·¨æ¨¡æ€æ¼æ´çš„è¶Šç‹±æ”»å‡»ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹æ–¹æ³•ï¼Œé€šè¿‡åºè´¯æ¼«ç”»å¼è§†è§‰å™äº‹æ¥è§„é¿MLLMsçš„å®‰å…¨å¯¹é½ã€‚</li>
<li>è¯¥æ–¹æ³•å°†æ¶æ„æŸ¥è¯¢åˆ†è§£ä¸ºè§†è§‰ä¸Šä¸å…·å¨èƒçš„å™äº‹å…ƒç´ ï¼Œå¹¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒåºåˆ—ã€‚</li>
<li>å€ŸåŠ©æ¨¡å‹å¯¹å™äº‹è¿è´¯æ€§çš„ä¾èµ–ï¼Œèƒ½å¤Ÿå¼•å‘æœ‰å®³è¾“å‡ºã€‚</li>
<li>åœ¨å®‰å…¨åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•å¹³å‡æ”»å‡»æˆåŠŸç‡è¾¾83.5%ï¼Œæ˜¾è‘—è¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚</li>
<li>ä¸å…¶ä»–è§†è§‰è¶Šç‹±æ–¹æ³•ç›¸æ¯”ï¼Œåºè´¯å™äº‹ç­–ç•¥åœ¨å„ç±»æœ‰å®³å†…å®¹ä¸­è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>åˆ†ææ”»å‡»æ¨¡å¼ï¼Œæ­ç¤ºå¤šæ¨¡æ€å®‰å…¨æœºåˆ¶çš„è„†å¼±æ€§ï¼Œå¹¶æŒ‡å‡ºå½“å‰é˜²å¾¡ç­–ç•¥å¯¹å™äº‹é©±åŠ¨æ”»å‡»çš„é™åˆ¶å’Œæ¼æ´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-213093661191922da91df41c4528c1a2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992938&auth_key=1760992938-0-0-05a2dcc52f52abc396330a14d252f868&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a61b2a9ad5fa21c53ae32295a406a19b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992945&auth_key=1760992945-0-0-5d8992733e005aba34728b00ba6a9492&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b50ec79b64c2abc74ddf015f3d5e708a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992952&auth_key=1760992952-0-0-39f69807bd771caf86b47f3f325e8d9f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3655cebcca788ee6940d7aa437f37572~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992959&auth_key=1760992959-0-0-6e97ba00ebad15e4d833bd22e8e2b8c6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-923fe03610f6e02a43b427adcacbd22a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992965&auth_key=1760992965-0-0-315dbca1d498de71b4123327dfda9c3f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fe41ecf91890c3a8bcea50fed688228b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992972&auth_key=1760992972-0-0-f80a5d9bf0fda7024ae9743fd5d67339&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Dataset-Distillation-for-Super-Resolution-without-Class-Labels-and-Pre-trained-Models"><a href="#Dataset-Distillation-for-Super-Resolution-without-Class-Labels-and-Pre-trained-Models" class="headerlink" title="Dataset Distillation for Super-Resolution without Class Labels and   Pre-trained Models"></a>Dataset Distillation for Super-Resolution without Class Labels and   Pre-trained Models</h2><p><strong>Authors:Sunwoo Cho, Yejin Jung, Nam Ik Cho, Jae Woong Soh</strong></p>
<p>Training deep neural networks has become increasingly demanding, requiring large datasets and significant computational resources, especially as model complexity advances. Data distillation methods, which aim to improve data efficiency, have emerged as promising solutions to this challenge. In the field of single image super-resolution (SISR), the reliance on large training datasets highlights the importance of these techniques. Recently, a generative adversarial network (GAN) inversion-based data distillation framework for SR was proposed, showing potential for better data utilization. However, the current method depends heavily on pre-trained SR networks and class-specific information, limiting its generalizability and applicability. To address these issues, we introduce a new data distillation approach for image SR that does not need class labels or pre-trained SR models. In particular, we first extract high-gradient patches and categorize images based on CLIP features, then fine-tune a diffusion model on the selected patches to learn their distribution and synthesize distilled training images. Experimental results show that our method achieves state-of-the-art performance while using significantly less training data and requiring less computational time. Specifically, when we train a baseline Transformer model for SR with only 0.68% of the original dataset, the performance drop is just 0.3 dB. In this case, diffusion model fine-tuning takes 4 hours, and SR model training completes within 1 hour, much shorter than the 11-hour training time with the full dataset. </p>
<blockquote>
<p>è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œçš„éœ€æ±‚è¶Šæ¥è¶Šé«˜ï¼Œéœ€è¦å¤§è§„æ¨¡æ•°æ®é›†å’Œå¤§é‡çš„è®¡ç®—èµ„æºï¼Œå°¤å…¶æ˜¯éšç€æ¨¡å‹å¤æ‚æ€§çš„æå‡ã€‚æ—¨åœ¨æé«˜æ•°æ®æ•ˆç‡çš„æ•°æ®è’¸é¦æ–¹æ³•å·²æˆä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨å•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSISRï¼‰é¢†åŸŸï¼Œå¯¹å¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›†çš„ä¾èµ–å‡¸æ˜¾äº†è¿™äº›æŠ€æœ¯çš„é‡è¦æ€§ã€‚æœ€è¿‘ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰åè½¬çš„SRæ•°æ®è’¸é¦æ¡†æ¶ï¼Œæ˜¾ç¤ºå‡ºæ›´å¥½çš„æ•°æ®åˆ©ç”¨æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¸¥é‡ä¾èµ–äºé¢„è®­ç»ƒçš„SRç½‘ç»œå’Œç±»åˆ«ç‰¹å®šä¿¡æ¯ï¼Œè¿™é™åˆ¶äº†å…¶é€šç”¨æ€§å’Œé€‚ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„ç”¨äºå›¾åƒSRçš„æ•°æ®è’¸é¦æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦ç±»æ ‡ç­¾æˆ–é¢„è®­ç»ƒçš„SRæ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæå–é«˜æ¢¯åº¦è¡¥ä¸å¹¶æ ¹æ®CLIPç‰¹å¾å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ï¼Œç„¶åå¯¹æ‰€é€‰è¡¥ä¸å¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼Œä»¥å­¦ä¹ å…¶åˆ†å¸ƒå¹¶åˆæˆè’¸é¦çš„è®­ç»ƒå›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä½¿ç”¨æ˜¾è‘—æ›´å°‘è®­ç»ƒæ•°æ®å’Œæ›´çŸ­è®¡ç®—æ—¶é—´çš„æƒ…å†µä¸‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œå½“æˆ‘ä»¬ä»…ä½¿ç”¨åŸå§‹æ•°æ®é›†çš„0.68%æ¥è®­ç»ƒSRçš„åŸºçº¿Transformeræ¨¡å‹æ—¶ï¼Œæ€§èƒ½ä¸‹é™ä»…ä¸º0.3åˆ†è´ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‰©æ•£æ¨¡å‹çš„å¾®è°ƒéœ€è¦4å°æ—¶ï¼ŒSRæ¨¡å‹çš„è®­ç»ƒåœ¨1å°æ—¶å†…å®Œæˆï¼Œè¿œè¿œçŸ­äºä½¿ç”¨å®Œæ•´æ•°æ®é›†çš„11å°æ—¶è®­ç»ƒæ—¶é—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14777v2">PDF</a> code : <a target="_blank" rel="noopener" href="https://github.com/sunwoocho/SRDD">https://github.com/sunwoocho/SRDD</a></p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒéœ€è¦å¤§é‡æ•°æ®é›†å’Œè®¡ç®—èµ„æºï¼Œæ•°æ®è’¸é¦æ–¹æ³•æ—¨åœ¨æé«˜æ•°æ®æ•ˆç‡ï¼Œæˆä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨å•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSISRï¼‰é¢†åŸŸï¼Œä¾èµ–å¤§å‹è®­ç»ƒæ•°æ®é›†å‡¸æ˜¾äº†æ•°æ®è’¸é¦æŠ€æœ¯çš„é‡è¦æ€§ã€‚æœ€è¿‘æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰åæ¼”çš„SRæ•°æ®è’¸é¦æ¡†æ¶ï¼Œæ˜¾ç¤ºå‡ºæ›´å¥½çš„æ•°æ®åˆ©ç”¨æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºé¢„è®­ç»ƒçš„SRç½‘ç»œå’Œç±»åˆ«ç‰¹å®šä¿¡æ¯ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œé€‚ç”¨æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒSRæ•°æ®è’¸é¦æ–¹æ³•ï¼Œæ— éœ€ç±»åˆ«æ ‡ç­¾æˆ–é¢„è®­ç»ƒçš„SRæ¨¡å‹ã€‚é€šè¿‡æå–é«˜æ¢¯åº¦è¡¥ä¸å¹¶ä½¿ç”¨CLIPç‰¹å¾å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ï¼Œå¯¹é€‰æ‹©çš„è¡¥ä¸è¿›è¡Œå¾®è°ƒæ‰©æ•£æ¨¡å‹æ¥å­¦ä¹ å…¶åˆ†å¸ƒå¹¶åˆæˆè’¸é¦è®­ç»ƒå›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¾ƒå°‘çš„è®­ç»ƒæ•°æ®å’Œæ›´çŸ­çš„è®¡ç®—æ—¶é—´å†…å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å½“ä½¿ç”¨åŸå§‹æ•°æ®é›†ä»…0.68%è®­ç»ƒåŸºçº¿Transformeræ¨¡å‹è¿›è¡ŒSRæ—¶ï¼Œæ€§èƒ½ä¸‹é™ä»…ä¸º0.3dBã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¾®è°ƒæ‰©æ•£æ¨¡å‹éœ€è¦4å°æ—¶ï¼ŒSRæ¨¡å‹è®­ç»ƒåœ¨1å°æ—¶å†…å®Œæˆï¼Œè¿œå°äºä½¿ç”¨å®Œæ•´æ•°æ®é›†çš„11å°æ—¶è®­ç»ƒæ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹é¢ä¸´å¤§æ•°æ®é›†å’Œè®¡ç®—èµ„æºéœ€æ±‚æŒ‘æˆ˜ã€‚</li>
<li>æ•°æ®è’¸é¦æ–¹æ³•æ—¨åœ¨æé«˜æ•°æ®æ•ˆç‡ï¼Œæˆä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å½“å‰SRæ•°æ®è’¸é¦æ–¹æ³•ä¾èµ–é¢„è®­ç»ƒæ¨¡å‹å’Œç±»åˆ«ç‰¹å®šä¿¡æ¯ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§ã€‚</li>
<li>æå‡ºä¸€ç§æ–°çš„å›¾åƒSRæ•°æ®è’¸é¦æ–¹æ³•ï¼Œæ— éœ€ç±»åˆ«æ ‡ç­¾æˆ–é¢„è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>é€šè¿‡æå–é«˜æ¢¯åº¦è¡¥ä¸å¹¶ä½¿ç”¨CLIPç‰¹å¾åˆ†ç±»å›¾åƒï¼Œå¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒå­¦ä¹ å›¾åƒåˆ†å¸ƒã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºæ–°æ–¹æ³•åœ¨è¾ƒå°‘æ•°æ®ã€è¾ƒçŸ­æ—¶é—´å†…å®ç°å…ˆè¿›æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-be34989b57d10041b3b7ac406b77da0e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992980&auth_key=1760992980-0-0-2c2756bbdb525d241c47ca5fb72daaa0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9bec12f1d5ca4770a5f4b61146f72257~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992988&auth_key=1760992988-0-0-b5c22a07c59ac4cf1bf7f898eedfa011&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c6137d2050dabf879722f20e08ae7c07~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992994&auth_key=1760992994-0-0-9c7ac526d62a3eb70b8927a3e512c14f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d22b532bd39a34d514ec8a1d54eb4dfb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993001&auth_key=1760993001-0-0-88e395112f916d3ba8a382b1df2099dc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bb500596bf9716899626a01f467220a1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993007&auth_key=1760993007-0-0-6ce6c3be5f7ed64825e29e31b186a8f2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RadioDiff-k-2-Helmholtz-Equation-Informed-Generative-Diffusion-Model-for-Multi-Path-Aware-Radio-Map-Construction"><a href="#RadioDiff-k-2-Helmholtz-Equation-Informed-Generative-Diffusion-Model-for-Multi-Path-Aware-Radio-Map-Construction" class="headerlink" title="RadioDiff-$k^2$: Helmholtz Equation Informed Generative Diffusion Model   for Multi-Path Aware Radio Map Construction"></a>RadioDiff-$k^2$: Helmholtz Equation Informed Generative Diffusion Model   for Multi-Path Aware Radio Map Construction</h2><p><strong>Authors:Xiucheng Wang, Qiming Zhang, Nan Cheng, Ruijin Sun, Zan Li, Shuguang Cui, Xuemin Shen</strong></p>
<p>In this paper, we propose a novel physics-informed generative learning approach, named RadioDiff-$k^2$, for accurate and efficient multipath-aware radio map (RM) construction. As future wireless communication evolves towards environment-aware paradigms, the accurate construction of RMs becomes crucial yet highly challenging. Conventional electromagnetic (EM)-based methods, such as full-wave solvers and ray-tracing approaches, exhibit substantial computational overhead and limited adaptability to dynamic scenarios. Although existing neural network (NN) approaches have efficient inferencing speed, they lack sufficient consideration of the underlying physics of EM wave propagation, limiting their effectiveness in accurately modeling critical EM singularities induced by complex multipath environments. To address these fundamental limitations, we propose a novel physics-inspired RM construction method guided explicitly by the Helmholtz equation, which inherently governs EM wave propagation. Specifically, based on the analysis of partial differential equations (PDEs), we theoretically establish a direct correspondence between EM singularities, which correspond to the critical spatial features influencing wireless propagation, and regions defined by negative wave numbers in the Helmholtz equation. We then design an innovative dual diffusion model (DM)-based large artificial intelligence framework comprising one DM dedicated to accurately inferring EM singularities and another DM responsible for reconstructing the complete RM using these singularities along with environmental contextual information. Experimental results demonstrate that the proposed RadioDiff-$k^2$ framework achieves state-of-the-art (SOTA) performance in both image-level RM construction and localization tasks, while maintaining inference latency within a few hundred milliseconds. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºRadioDiff-$k^2$çš„æ–°å‹ç‰©ç†ä¿¡æ¯ç”Ÿæˆå­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå‡†ç¡®é«˜æ•ˆåœ°æ„å»ºå¤šè·¯å¾„æ„ŸçŸ¥æ— çº¿åœ°å›¾ï¼ˆRMï¼‰ã€‚éšç€æœªæ¥æ— çº¿é€šä¿¡å‘ç¯å¢ƒæ„ŸçŸ¥æ¨¡å¼çš„å‘å±•ï¼ŒRMçš„ç²¾ç¡®æ„å»ºå˜å¾—è‡³å…³é‡è¦ï¼Œä½†ä¹Ÿæå…·æŒ‘æˆ˜æ€§ã€‚ä¼ ç»Ÿçš„åŸºäºç”µç£ï¼ˆEMï¼‰çš„æ–¹æ³•ï¼Œå¦‚å…¨æ³¢æ±‚è§£å™¨å’Œå°„çº¿è¿½è¸ªæ–¹æ³•ï¼Œå­˜åœ¨å·¨å¤§çš„è®¡ç®—å¼€é”€å’Œå¯¹åŠ¨æ€åœºæ™¯çš„é€‚åº”æ€§æœ‰é™ã€‚å°½ç®¡ç°æœ‰çš„ç¥ç»ç½‘ç»œï¼ˆNNï¼‰æ–¹æ³•å…·æœ‰é«˜æ•ˆçš„æ¨ç†é€Ÿåº¦ï¼Œä½†å®ƒä»¬æ²¡æœ‰è¶³å¤Ÿè€ƒè™‘ç”µç£æ³¢ä¼ æ’­çš„åº•å±‚ç‰©ç†å­¦ï¼Œåœ¨å‡†ç¡®æ¨¡æ‹Ÿç”±å¤æ‚å¤šè·¯å¾„ç¯å¢ƒå¼•èµ·çš„å…³é”®ç”µç£å¥‡å¼‚ç°è±¡æ–¹é¢çš„æ•ˆæœæœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›åŸºæœ¬å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ç‰©ç†å¯å‘å¼çš„RMæ„å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ˜ç¡®å—Helmholtzæ–¹ç¨‹çš„æŒ‡å¯¼ï¼Œè¯¥æ–¹ç¨‹å›ºæœ‰åœ°æ§åˆ¶ç”µç£æ³¢ä¼ æ’­ã€‚å…·ä½“æ¥è¯´ï¼ŒåŸºäºåå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEsï¼‰çš„åˆ†æï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šå»ºç«‹äº†ç”µç£å¥‡å¼‚ç‚¹ä¸å½±å“æ— çº¿ä¼ æ’­çš„å…³é”®ç©ºé—´ç‰¹å¾ä¹‹é—´çš„ç›´æ¥å¯¹åº”å…³ç³»ï¼Œè¿™äº›å¥‡å¼‚ç‚¹å¯¹åº”äºHelmholtzæ–¹ç¨‹ä¸­å®šä¹‰çš„è´Ÿæ³¢æ•°åŒºåŸŸã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåˆ›æ–°çš„åŸºäºåŒæ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰çš„å¤§å‹äººå·¥æ™ºèƒ½æ¡†æ¶ï¼ŒåŒ…å«ä¸€ä¸ªä¸“é—¨ç”¨äºå‡†ç¡®æ¨æ–­ç”µç£å¥‡å¼‚ç‚¹çš„DMï¼Œä»¥åŠå¦ä¸€ä¸ªè´Ÿè´£ä½¿ç”¨è¿™äº›å¥‡å¼‚ç‚¹ä»¥åŠç¯å¢ƒä¸Šä¸‹æ–‡ä¿¡æ¯é‡å»ºå®Œæ•´RMçš„DMã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„RadioDiff-$k^2$æ¡†æ¶åœ¨å›¾åƒçº§RMæ„å»ºå’Œå®šä½ä»»åŠ¡ä¸­éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†å‡ ç™¾æ¯«ç§’å†…çš„æ¨ç†å»¶è¿Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15623v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºRadioDiff-$k^2$çš„æ–°å‹ç‰©ç†ä¿¡æ¯ç”Ÿæˆå­¦ä¹ æ³•ï¼Œç”¨äºå‡†ç¡®é«˜æ•ˆçš„å¤šè·¯å¾„æ„ŸçŸ¥æ— çº¿ç”µåœ°å›¾ï¼ˆRMï¼‰æ„å»ºã€‚é’ˆå¯¹æœªæ¥æ— çº¿é€šä¿¡å‘ç¯å¢ƒæ„ŸçŸ¥æ¨¡å¼å‘å±•æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼ŒRadioDiff-$k^2$ç»“åˆäº†ç‰©ç†å­¦åŸç†ä¸ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œæ—¢ä¿è¯äº†è®¡ç®—æ•ˆç‡ï¼Œåˆè€ƒè™‘äº†ç”µç£æ³¢ä¼ æ’­çš„åº•å±‚ç‰©ç†æœºåˆ¶ã€‚è¯¥æ–¹æ³•ä»¥Helmholtzæ–¹ç¨‹ä¸ºå¼•å¯¼ï¼Œé€šè¿‡éƒ¨åˆ†å¾®åˆ†æ–¹ç¨‹åˆ†æå»ºç«‹äº†ç”µç£æ³¢å¥‡ç‚¹ä¸Helmholtzæ–¹ç¨‹ä¸­è´Ÿæ³¢æ•°åŒºåŸŸçš„ç›´æ¥è”ç³»ã€‚åˆ©ç”¨åŒæ‰©æ•£æ¨¡å‹æ„å»ºçš„å¤§å‹äººå·¥æ™ºèƒ½æ¡†æ¶ä¸ä»…èƒ½å‡†ç¡®æ¨æ–­å‡ºç”µç£æ³¢å¥‡ç‚¹ï¼Œè¿˜èƒ½æ ¹æ®è¿™äº›å¥‡ç‚¹å’Œç¯å¢ƒä¸Šä¸‹æ–‡ä¿¡æ¯é‡å»ºå®Œæ•´çš„æ— çº¿ç”µåœ°å›¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRadioDiff-$k^2$åœ¨å›¾åƒçº§åˆ«çš„æ— çº¿ç”µåœ°å›¾æ„å»ºå’Œå®šä½ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ŒåŒæ—¶æ¨ç†å»¶è¿Ÿä¿æŒåœ¨æ•°ç™¾æ¯«ç§’ä»¥å†…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°å‹ç‰©ç†ä¿¡æ¯ç”Ÿæˆå­¦ä¹ æ³•RadioDiff-$k^2$ç”¨äºæ— çº¿ç”µåœ°å›¾æ„å»ºã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†ç‰©ç†å­¦åŸç†ä¸ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œè€ƒè™‘äº†ç”µç£æ³¢ä¼ æ’­çš„åº•å±‚ç‰©ç†æœºåˆ¶ã€‚</li>
<li>ä»¥Helmholtzæ–¹ç¨‹ä¸ºå¼•å¯¼ï¼Œå»ºç«‹äº†ç”µç£æ³¢å¥‡ç‚¹ä¸è´Ÿæ³¢æ•°åŒºåŸŸçš„ç›´æ¥è”ç³»ã€‚</li>
<li>åˆ©ç”¨åŒæ‰©æ•£æ¨¡å‹æ„å»ºå¤§å‹äººå·¥æ™ºèƒ½æ¡†æ¶è¿›è¡Œæ— çº¿ç”µåœ°å›¾æ„å»ºã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å‡†ç¡®æ¨æ–­å‡ºç”µç£æ³¢å¥‡ç‚¹å¹¶é‡å»ºå®Œæ•´çš„æ— çº¿ç”µåœ°å›¾ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºRadioDiff-$k^2$åœ¨å›¾åƒçº§åˆ«çš„æ— çº¿ç”µåœ°å›¾æ„å»ºå’Œå®šä½ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15623">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ccf0ea07cc01d639e17d4dfadb9fe2d3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993015&auth_key=1760993015-0-0-acbf53016c4ff3a4c906a5d81e90ee32&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-30a94c96d2d1fd86c745091628f590ac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993022&auth_key=1760993022-0-0-0c9c3764ac286ce7ac5dfcba15007d39&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6aae0e4b6e94ccfeb6cd78bc5f55bb2d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993029&auth_key=1760993029-0-0-4151120145aa3b93f0c56f54b6e93639&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4a57d88fa39cf55ba83e64d20415faaa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993035&auth_key=1760993035-0-0-bbdd4a1f132958b727ed1eb6947c7b13&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-996ff75389d892411e4348768b37903f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993042&auth_key=1760993042-0-0-9fc81d8499604c037304c3c7aaa99cab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SPICE-A-Synergistic-Precise-Iterative-and-Customizable-Image-Editing-Workflow"><a href="#SPICE-A-Synergistic-Precise-Iterative-and-Customizable-Image-Editing-Workflow" class="headerlink" title="SPICE: A Synergistic, Precise, Iterative, and Customizable Image Editing   Workflow"></a>SPICE: A Synergistic, Precise, Iterative, and Customizable Image Editing   Workflow</h2><p><strong>Authors:Kenan Tang, Yanhong Li, Yao Qin</strong></p>
<p>Prompt-based models have demonstrated impressive prompt-following capability at image editing tasks. However, the models still struggle with following detailed editing prompts or performing local edits. Specifically, global image quality often deteriorates immediately after a single editing step. To address these challenges, we introduce SPICE, a training-free workflow that accepts arbitrary resolutions and aspect ratios, accurately follows user requirements, and consistently improves image quality during more than 100 editing steps, while keeping the unedited regions intact. By synergizing the strengths of a base diffusion model and a Canny edge ControlNet model, SPICE robustly handles free-form editing instructions from the user. On a challenging realistic image-editing dataset, SPICE quantitatively outperforms state-of-the-art baselines and is consistently preferred by human annotators. We release the workflow implementation for popular diffusion model Web UIs to support further research and artistic exploration. </p>
<blockquote>
<p>åŸºäºæç¤ºçš„æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„éµå¾ªæç¤ºçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨éµå¾ªè¯¦ç»†çš„ç¼–è¾‘æç¤ºæˆ–æ‰§è¡Œå±€éƒ¨ç¼–è¾‘æ—¶ä»é¢ä¸´å›°éš¾ã€‚å…·ä½“æ¥è¯´ï¼Œå•ä¸€ç¼–è¾‘æ­¥éª¤åï¼Œå…¨å±€å›¾åƒè´¨é‡å¾€å¾€ä¼šç«‹å³ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SPICEï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å·¥ä½œæµï¼Œå¯æ¥å—ä»»æ„åˆ†è¾¨ç‡å’Œé•¿å®½æ¯”ï¼Œèƒ½å‡†ç¡®åœ°éµå¾ªç”¨æˆ·éœ€æ±‚ï¼Œåœ¨è¶…è¿‡100æ­¥çš„ç¼–è¾‘è¿‡ç¨‹ä¸­æŒç»­æé«˜å›¾åƒè´¨é‡ï¼ŒåŒæ—¶ä¿æŒæœªç¼–è¾‘åŒºåŸŸå®Œå¥½æ— æŸã€‚é€šè¿‡ç»“åˆåŸºç¡€æ‰©æ•£æ¨¡å‹å’ŒCannyè¾¹ç¼˜ControlNetæ¨¡å‹çš„ä¼˜ç‚¹ï¼ŒSPICEèƒ½å¤Ÿç¨³å¥åœ°å¤„ç†ç”¨æˆ·çš„è‡ªç”±å½¢å¼ç¼–è¾‘æŒ‡ä»¤ã€‚åœ¨ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®å›¾åƒç¼–è¾‘æ•°æ®é›†ä¸Šï¼ŒSPICEåœ¨å®šé‡ä¸Šä¼˜äºæœ€æ–°åŸºçº¿ï¼Œå¹¶å§‹ç»ˆè¢«äººç±»æ³¨é‡Šè€…æ‰€é’çã€‚æˆ‘ä»¬å‘å¸ƒäº†æµè¡Œæ‰©æ•£æ¨¡å‹Web UIçš„å·¥ä½œæµç¨‹å®ç°ï¼Œä»¥æ”¯æŒè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œè‰ºæœ¯æ¢ç´¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09697v2">PDF</a> The paper has been accepted to NeurIPS Creative AI Track 2025. Figure   4(c) has been accepted to CVPR AI Art Gallery 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSPICEçš„è®­ç»ƒå…è´¹å·¥ä½œæµç¨‹ï¼Œç”¨äºå›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚è¯¥æµç¨‹å¯ä»¥æ¥å—ä»»æ„åˆ†è¾¨ç‡å’Œæ¯”ä¾‹ï¼Œå‡†ç¡®éµå¾ªç”¨æˆ·è¦æ±‚ï¼Œåœ¨è¶…è¿‡100æ­¥çš„ç¼–è¾‘è¿‡ç¨‹ä¸­æŒç»­æé«˜å›¾åƒè´¨é‡ï¼ŒåŒæ—¶ä¿æŒæœªç¼–è¾‘åŒºåŸŸä¸å˜ã€‚é€šè¿‡ç»“åˆåŸºç¡€æ‰©æ•£æ¨¡å‹å’ŒCannyè¾¹ç¼˜ControlNetæ¨¡å‹çš„ä¼˜ç‚¹ï¼ŒSPICEèƒ½å¤Ÿç¨³å¥åœ°å¤„ç†ç”¨æˆ·çš„è‡ªç”±å½¢å¼ç¼–è¾‘æŒ‡ä»¤ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®å›¾åƒç¼–è¾‘æ•°æ®é›†ä¸Šï¼ŒSPICEå®šé‡ä¼˜äºæœ€æ–°åŸºçº¿ï¼Œå¹¶å§‹ç»ˆå—åˆ°äººç±»æ³¨é‡Šè€…çš„é’çã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SPICEæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„å·¥ä½œæµç¨‹ï¼Œç”¨äºå›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚</li>
<li>å®ƒå¯ä»¥æ¥å—ä»»æ„åˆ†è¾¨ç‡å’Œæ¯”ä¾‹ï¼Œå¹¶èƒ½å‡†ç¡®éµå¾ªç”¨æˆ·çš„è¦æ±‚ã€‚</li>
<li>SPICEåœ¨è¿ç»­å¤šæ¬¡ç¼–è¾‘è¿‡ç¨‹ä¸­èƒ½æŒç»­æé«˜å›¾åƒè´¨é‡ï¼ŒåŒæ—¶ä¿æŒæœªç¼–è¾‘åŒºåŸŸä¸å˜ã€‚</li>
<li>é€šè¿‡ç»“åˆæ‰©æ•£æ¨¡å‹å’ŒCannyè¾¹ç¼˜ControlNetæ¨¡å‹çš„ä¼˜ç‚¹ï¼ŒSPICEèƒ½ç¨³å¥åœ°å¤„ç†è‡ªç”±å½¢å¼çš„ç¼–è¾‘æŒ‡ä»¤ã€‚</li>
<li>SPICEåœ¨çœŸå®å›¾åƒç¼–è¾‘æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å¾—åˆ°äº†äººç±»è¯„ä¼°è€…çš„é’çã€‚</li>
<li>SPICEå·¥ä½œæµç¨‹çš„å®ç°å·²å‘å¸ƒï¼Œæ”¯æŒè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œè‰ºæœ¯æ¢ç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09697">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1f3e684de26fe1360d10f68374132712~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993051&auth_key=1760993051-0-0-ac040a8b12e494c67ea539b76e040884&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e9127edfa69a8d8724817814bae79945~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993059&auth_key=1760993059-0-0-268b44adcb6814458eced353d9c0b4f7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-88691c46b1d961b1cc347ffdcd67a612~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993065&auth_key=1760993065-0-0-189dd0b8a816fe4969484b1a3b324237&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0b8ab53c1d97b56486f577d73f031c26~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993072&auth_key=1760993072-0-0-5cd069ea861c20f95f75948718e13373&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fefc77a0fe740d60ff6cca13259c3a6a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993079&auth_key=1760993079-0-0-e31bcc01699edac0e5f5a9936a3f7019&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CHROME-Clothed-Human-Reconstruction-with-Occlusion-Resilience-and-Multiview-Consistency-from-a-Single-Image"><a href="#CHROME-Clothed-Human-Reconstruction-with-Occlusion-Resilience-and-Multiview-Consistency-from-a-Single-Image" class="headerlink" title="CHROME: Clothed Human Reconstruction with Occlusion-Resilience and   Multiview-Consistency from a Single Image"></a>CHROME: Clothed Human Reconstruction with Occlusion-Resilience and   Multiview-Consistency from a Single Image</h2><p><strong>Authors:Arindam Dutta, Meng Zheng, Zhongpai Gao, Benjamin Planche, Anwesha Choudhuri, Terrence Chen, Amit K. Roy-Chowdhury, Ziyan Wu</strong></p>
<p>Reconstructing clothed humans from a single image is a fundamental task in computer vision with wide-ranging applications. Although existing monocular clothed human reconstruction solutions have shown promising results, they often rely on the assumption that the human subject is in an occlusion-free environment. Thus, when encountering in-the-wild occluded images, these algorithms produce multiview inconsistent and fragmented reconstructions. Additionally, most algorithms for monocular 3D human reconstruction leverage geometric priors such as SMPL annotations for training and inference, which are extremely challenging to acquire in real-world applications. To address these limitations, we propose CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-ConsistEncy from a Single Image, a novel pipeline designed to reconstruct occlusion-resilient 3D humans with multiview consistency from a single occluded image, without requiring either ground-truth geometric prior annotations or 3D supervision. Specifically, CHROME leverages a multiview diffusion model to first synthesize occlusion-free human images from the occluded input, compatible with off-the-shelf pose control to explicitly enforce cross-view consistency during synthesis. A 3D reconstruction model is then trained to predict a set of 3D Gaussians conditioned on both the occluded input and synthesized views, aligning cross-view details to produce a cohesive and accurate 3D representation. CHROME achieves significant improvements in terms of both novel view synthesis (upto 3 db PSNR) and geometric reconstruction under challenging conditions. </p>
<blockquote>
<p>ä»å•ä¸ªå›¾åƒé‡å»ºç©¿è¡£æœçš„äººç±»æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨èŒƒå›´ã€‚å°½ç®¡ç°æœ‰çš„å•ç›®ç©¿è¡£äººç±»é‡å»ºè§£å†³æ–¹æ¡ˆå·²ç»æ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„ç»“æœï¼Œä½†å®ƒä»¬é€šå¸¸å‡è®¾äººç±»ä¸»ä½“å¤„äºæ— é®æŒ¡çš„ç¯å¢ƒä¸­ã€‚å› æ­¤ï¼Œå½“é‡åˆ°é‡å¤–è¢«é®æŒ¡çš„å›¾åƒæ—¶ï¼Œè¿™äº›ç®—æ³•ä¼šäº§ç”Ÿè§†å›¾ä¸ä¸€è‡´å’Œåˆ†æ•£çš„é‡å»ºç»“æœã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°å•ç›®3Däººç±»é‡å»ºç®—æ³•åˆ©ç”¨å‡ ä½•å…ˆéªŒï¼ˆå¦‚SMPLæ³¨é‡Šï¼‰è¿›è¡Œè®­ç»ƒå’Œæ¨ç†ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œçš„å®é™…åº”ç”¨ä¸­æä¸ºå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†CHROMEï¼šä»å•ä¸ªå›¾åƒè¿›è¡Œé®æŒ¡å¤åŸå’Œå¤šè§†è§’ä¸€è‡´æ€§çš„ç©¿è¡£äººç±»é‡å»ºã€‚CHROMEæ˜¯ä¸€ç§æ–°å‹ç®¡é“è®¾è®¡ï¼Œæ—¨åœ¨ä»å•ä¸ªè¢«é®æŒ¡çš„å›¾åƒé‡å»ºå…·æœ‰å¤šè§†è§’ä¸€è‡´æ€§çš„é®æŒ¡å¤åŸ3Däººç±»ï¼Œè€Œæ— éœ€çœŸå®å‡ ä½•å…ˆéªŒæ³¨é‡Šæˆ–3Dç›‘ç£ã€‚å…·ä½“æ¥è¯´ï¼ŒCHROMEé¦–å…ˆåˆ©ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹ä»è¢«é®æŒ¡çš„è¾“å…¥ä¸­åˆæˆæ— é®æŒ¡çš„äººç±»å›¾åƒï¼Œä¸ç°æˆçš„å§¿åŠ¿æ§åˆ¶ç›¸ç»“åˆï¼Œåœ¨åˆæˆè¿‡ç¨‹ä¸­æ˜ç¡®å®æ–½è·¨è§†å›¾ä¸€è‡´æ€§ã€‚ç„¶åè®­ç»ƒä¸€ä¸ª3Dé‡å»ºæ¨¡å‹ï¼Œä»¥æ ¹æ®è¢«é®æŒ¡çš„è¾“å…¥å’Œåˆæˆè§†å›¾é¢„æµ‹ä¸€ç»„3Dé«˜æ–¯åˆ†å¸ƒï¼Œå¯¹é½è·¨è§†å›¾çš„ç»†èŠ‚ä»¥äº§ç”Ÿè¿è´¯å’Œå‡†ç¡®çš„ä¸‰ç»´è¡¨ç¤ºã€‚CHROMEåœ¨æ–°å‹è§†å›¾åˆæˆï¼ˆé«˜è¾¾3åˆ†è´çš„PSNRï¼‰å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹çš„å‡ ä½•é‡å»ºæ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15671v2">PDF</a> Accepted at ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å•å›¾åƒé‡å»ºç©¿ç€è¡£æœçš„äººä½“çš„æ–¹æ³•ï¼Œç§°ä¸ºCHROMEã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰æŠ€æœ¯ä¸­çš„é®æŒ¡é—®é¢˜å’Œå¤šè§†è§’ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€çœŸå®å‡ ä½•å…ˆéªŒæ ‡æ³¨å’Œä¸‰ç»´ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œä»å•ä¸€é®æŒ¡å›¾åƒä¸­é‡å»ºå‡ºå…·æœ‰å¤šè§†è§’ä¸€è‡´æ€§çš„é®æŒ¡å¤åŸä¸‰ç»´äººä½“ã€‚é€šè¿‡åˆ©ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹åˆæˆæ— é®æŒ¡äººä½“å›¾åƒï¼Œå¹¶ç»“åˆå§¿åŠ¿æ§åˆ¶æ˜ç¡®åˆæˆè¿‡ç¨‹ä¸­çš„è·¨è§†è§’ä¸€è‡´æ€§ã€‚éšåè®­ç»ƒä¸€ä¸ªä¸‰ç»´é‡å»ºæ¨¡å‹ï¼Œæ ¹æ®é®æŒ¡è¾“å…¥å’Œåˆæˆè§†è§’é¢„æµ‹ä¸€ç»„ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒï¼Œå¯¹é½è·¨è§†è§’ç»†èŠ‚ä»¥äº§ç”Ÿè¿è´¯å’Œå‡†ç¡®çš„ä¸‰ç»´è¡¨ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CHROMEè§£å†³äº†ç°æœ‰å•å›¾åƒé‡å»ºç©¿ç€è¡£æœçš„äººä½“æŠ€æœ¯åœ¨å¤„ç†é®æŒ¡å’Œå¤šè§†è§’ä¸ä¸€è‡´æ€§é—®é¢˜ä¸Šçš„æŒ‘æˆ˜ã€‚</li>
<li>CHROMEèƒ½å¤Ÿåœ¨æ— éœ€çœŸå®å‡ ä½•å…ˆéªŒæ ‡æ³¨å’Œä¸‰ç»´ç›‘ç£çš„æƒ…å†µä¸‹å·¥ä½œã€‚</li>
<li>å¤šè§†è§’æ‰©æ•£æ¨¡å‹è¢«ç”¨äºåˆæˆæ— é®æŒ¡çš„äººä½“å›¾åƒï¼Œå¢å¼ºäº†é‡å»ºçš„é²æ£’æ€§ã€‚</li>
<li>å§¿åŠ¿æ§åˆ¶ç”¨äºæ˜ç¡®åˆæˆè¿‡ç¨‹ä¸­çš„è·¨è§†è§’ä¸€è‡´æ€§ã€‚</li>
<li>ä¸‰ç»´é‡å»ºæ¨¡å‹æ ¹æ®é®æŒ¡è¾“å…¥å’Œåˆæˆè§†è§’é¢„æµ‹ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒï¼Œæé«˜é‡å»ºçš„å‡†ç¡®æ€§ã€‚</li>
<li>CHROMEåœ¨æ–°å‹è§†å›¾åˆæˆå’Œå‡ ä½•é‡å»ºæ–¹é¢éƒ½æœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚ç¯å¢ƒå’Œé®æŒ¡æƒ…å†µä¸‹çš„å›¾åƒæ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15671">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-00aa36d5ffe016f49f765c4535ba0c0e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993086&auth_key=1760993086-0-0-597a1ccf13fe0a2a524d21a43ebb2436&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ec4449435ecd49f50b6262f9fc0d6dec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993093&auth_key=1760993093-0-0-b1cc32cf50b70bcb72a5a8e15fd01e8d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e8818a19b26d47baf01978c77e646f96~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993099&auth_key=1760993099-0-0-77eedf0e072529bb49a2c2554ae61c82&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4bbe81a4bf90123ed7cd26bedfdad37b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993106&auth_key=1760993106-0-0-7e6d31918977586c3665aa6df8ec9fc8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1e949b1f079f58dc44fff3cea9d6a623~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993113&auth_key=1760993113-0-0-db3e4c4a72a727e01c710a8403ccabec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Bolt3D-Generating-3D-Scenes-in-Seconds"><a href="#Bolt3D-Generating-3D-Scenes-in-Seconds" class="headerlink" title="Bolt3D: Generating 3D Scenes in Seconds"></a>Bolt3D: Generating 3D Scenes in Seconds</h2><p><strong>Authors:Stanislaw Szymanowicz, Jason Y. Zhang, Pratul Srinivasan, Ruiqi Gao, Arthur Brussee, Aleksander Holynski, Ricardo Martin-Brualla, Jonathan T. Barron, Philipp Henzler</strong></p>
<p>We present a latent diffusion model for fast feed-forward 3D scene generation. Given one or more images, our model Bolt3D directly samples a 3D scene representation in less than seven seconds on a single GPU. We achieve this by leveraging powerful and scalable existing 2D diffusion network architectures to produce consistent high-fidelity 3D scene representations. To train this model, we create a large-scale multiview-consistent dataset of 3D geometry and appearance by applying state-of-the-art dense 3D reconstruction techniques to existing multiview image datasets. Compared to prior multiview generative models that require per-scene optimization for 3D reconstruction, Bolt3D reduces the inference cost by a factor of up to 300 times. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå¿«é€Ÿå‰é¦ˆ3Dåœºæ™¯ç”Ÿæˆçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚ç»™å®šä¸€å¼ æˆ–å¤šå¼ å›¾åƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹Bolt3Dèƒ½å¤Ÿåœ¨å•ä¸ªGPUä¸Šä¸åˆ°ä¸ƒç§’å†…ç›´æ¥é‡‡æ ·3Dåœºæ™¯è¡¨ç¤ºã€‚æˆ‘ä»¬åˆ©ç”¨å¼ºå¤§ä¸”å¯æ‰©å±•çš„ç°æœ‰2Dæ‰©æ•£ç½‘ç»œæ¶æ„æ¥ç”Ÿæˆä¸€è‡´çš„é«˜ä¿çœŸ3Dåœºæ™¯è¡¨ç¤ºï¼Œä»è€Œå®ç°è¿™ä¸€ç›®æ ‡ã€‚ä¸ºäº†è®­ç»ƒæ­¤æ¨¡å‹ï¼Œæˆ‘ä»¬é€šè¿‡å°†æœ€å…ˆè¿›çš„å¯†é›†3Dé‡å»ºæŠ€æœ¯åº”ç”¨äºç°æœ‰çš„å¤šè§†è§’å›¾åƒæ•°æ®é›†ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šè§†è§’ä¸€è‡´æ€§çš„3Då‡ ä½•å’Œå¤–è§‚æ•°æ®é›†ã€‚ä¸ä¹‹å‰çš„éœ€è¦é’ˆå¯¹æ¯ä¸ªåœºæ™¯è¿›è¡Œä¼˜åŒ–çš„å¤šè§†è§’ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼ŒBolt3Dçš„æ¨ç†æˆæœ¬é™ä½äº†é«˜è¾¾300å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14445v2">PDF</a> ICCV 2025. Project page: <a target="_blank" rel="noopener" href="https://szymanowiczs.github.io/bolt3d">https://szymanowiczs.github.io/bolt3d</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºç»™å®šçš„ä¸€ä¸ªæˆ–å¤šä¸ªå›¾åƒï¼ŒBolt3Dæ¨¡å‹é€šè¿‡åˆ©ç”¨å¼ºå¤§çš„å¯æ‰©å±•çš„äºŒç»´æ‰©æ•£ç½‘ç»œæ¶æ„ï¼Œå¿«é€Ÿç”Ÿæˆä¸‰ç»´åœºæ™¯è¡¨ç¤ºï¼Œè€—æ—¶ä¸åˆ°ä¸ƒç§’ã€‚é€šè¿‡åº”ç”¨æœ€æ–°çš„å¯†é›†ä¸‰ç»´é‡å»ºæŠ€æœ¯ï¼Œåˆ›å»ºå¤§è§„æ¨¡çš„å¤šè§†è§’ä¸€è‡´çš„ä¸‰ç»´å‡ ä½•å’Œå¤–è§‚æ•°æ®é›†æ¥è®­ç»ƒæ¨¡å‹ã€‚ç›¸è¾ƒäºä¹‹å‰çš„å¤šè§†è§’ç”Ÿæˆæ¨¡å‹ï¼ŒBolt3Då‡å°‘äº†é«˜è¾¾300å€çš„æ¨ç†æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Bolt3Dæ¨¡å‹å¯ä»¥å¿«é€Ÿç”Ÿæˆä¸‰ç»´åœºæ™¯è¡¨ç¤ºï¼Œè€—æ—¶ä¸åˆ°ä¸ƒç§’ã€‚</li>
<li>è¯¥æ¨¡å‹åˆ©ç”¨ç°æœ‰çš„äºŒç»´æ‰©æ•£ç½‘ç»œæ¶æ„ã€‚</li>
<li>Bolt3Dé€šè¿‡åº”ç”¨æœ€æ–°çš„å¯†é›†ä¸‰ç»´é‡å»ºæŠ€æœ¯åˆ›å»ºå¤§è§„æ¨¡æ•°æ®é›†ã€‚</li>
<li>æ¨¡å‹å¯ä»¥å®ç°å¤šè§†è§’ä¸€è‡´çš„ä¸‰ç»´å‡ ä½•å’Œå¤–è§‚è¡¨ç¤ºã€‚</li>
<li>ä¸å…ˆå‰çš„å¤šè§†è§’ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼ŒBolt3Dé™ä½äº†æ¨ç†æˆæœ¬ã€‚</li>
<li>Bolt3Dæ¨¡å‹å¯ä»¥ç›´æ¥ä»ä¸€ä¸ªæˆ–å¤šä¸ªå›¾åƒç”Ÿæˆä¸‰ç»´åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14445">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-73fb25df1bafe9cd061d6763fe2dffac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993120&auth_key=1760993120-0-0-e63ff7cb08774e04ed84a4bc6bed05bb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5c5cce4c01ba0f966cfcae7ecb3149c0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993127&auth_key=1760993127-0-0-e557ffec7f32f80c15d88d52909d4483&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d7fbc9fc37eeaf327492823b445c928~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993134&auth_key=1760993134-0-0-66fa75a47536aeb81ee364466407c53b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dda2f8d2d9ff52cd23a46ca21e876c83~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993141&auth_key=1760993141-0-0-6465e0ce278470752def3661747a6c00&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Methods-and-Trends-in-Detecting-AI-Generated-Images-A-Comprehensive-Review"><a href="#Methods-and-Trends-in-Detecting-AI-Generated-Images-A-Comprehensive-Review" class="headerlink" title="Methods and Trends in Detecting AI-Generated Images: A Comprehensive   Review"></a>Methods and Trends in Detecting AI-Generated Images: A Comprehensive   Review</h2><p><strong>Authors:Arpan Mahara, Naphtali Rishe</strong></p>
<p>The proliferation of generative models, such as Generative Adversarial Networks (GANs), Diffusion Models, and Variational Autoencoders (VAEs), has enabled the synthesis of high-quality multimedia data. However, these advancements have also raised significant concerns regarding adversarial attacks, unethical usage, and societal harm. Recognizing these challenges, researchers have increasingly focused on developing methodologies to detect synthesized data effectively, aiming to mitigate potential risks. Prior reviews have predominantly focused on deepfake detection and often overlook recent advancements in synthetic image forensics, particularly approaches that incorporate multimodal frameworks, reasoning-based detection, and training-free methodologies. To bridge this gap, this survey provides a comprehensive and up-to-date review of state-of-the-art techniques for detecting and classifying synthetic images generated by advanced generative AI models. The review systematically examines core detection paradigms, categorizes them into spatial-domain, frequency-domain, fingerprint-based, patch-based, training-free, and multimodal reasoning-based frameworks, and offers concise descriptions of their underlying principles. We further provide detailed comparative analyses of these methods on publicly available datasets to assess their generalizability, robustness, and interpretability. Finally, the survey highlights open challenges and future directions, emphasizing the potential of hybrid frameworks that combine the efficiency of training-free approaches with the semantic reasoning of multimodal models to advance trustworthy and explainable synthetic image forensics. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€æ‰©æ•£æ¨¡å‹å’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰ç­‰ï¼‰çš„æ™®åŠä½¿å¾—é«˜è´¨é‡å¤šåª’ä½“æ•°æ®çš„åˆæˆæˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›è¿›å±•ä¹Ÿå¼•å‘äº†å…³äºå¯¹æŠ—æ€§æ”»å‡»ã€ä¸é“å¾·ä½¿ç”¨å’Œç¤¾ä¼šå±å®³çš„ä¸¥é‡å…³åˆ‡ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œç ”ç©¶äººå‘˜è¶Šæ¥è¶Šå…³æ³¨å¼€å‘æœ‰æ•ˆæ£€æµ‹åˆæˆæ•°æ®çš„æ–¹æ³•ï¼Œä»¥é™ä½æ½œåœ¨é£é™©ã€‚æ­¤å‰çš„ç»¼è¿°ä¸»è¦é›†ä¸­åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸Šï¼Œå¾€å¾€å¿½è§†äº†åˆæˆå›¾åƒå–è¯æ–¹é¢çš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯é‡‡ç”¨å¤šæ¨¡æ€æ¡†æ¶ã€åŸºäºæ¨ç†çš„æ£€æµ‹æ–¹æ³•å’Œæ— è®­ç»ƒæ–¹æ³•ç­‰ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æä¾›äº†å¯¹æ£€æµ‹ç”±å…ˆè¿›ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹ç”Ÿæˆçš„åˆæˆå›¾åƒçš„æœ€å‰æ²¿æŠ€æœ¯çš„å…¨é¢å’Œæœ€æ–°ç»¼è¿°ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°è€ƒå¯Ÿäº†æ ¸å¿ƒæ£€æµ‹èŒƒå¼ï¼Œå°†å®ƒä»¬åˆ†ä¸ºç©ºé—´åŸŸã€é¢‘ç‡åŸŸã€åŸºäºæŒ‡çº¹ã€åŸºäºè¡¥ä¸ã€æ— è®­ç»ƒä»¥åŠåŸºäºå¤šæ¨¡æ€æ¨ç†çš„æ¡†æ¶ï¼Œå¹¶ç®€è¦æè¿°äº†å®ƒä»¬çš„åŸºæœ¬åŸç†ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜åœ¨å…¬å¼€æ•°æ®é›†ä¸Šå¯¹è¿™äº›æ–¹æ³•è¿›è¡Œäº†è¯¦ç»†çš„æ¯”è¾ƒåˆ†æï¼Œä»¥è¯„ä¼°å®ƒä»¬çš„é€šç”¨æ€§ã€é²æ£’æ€§å’Œå¯è§£é‡Šæ€§ã€‚æœ€åï¼Œè¯¥ç»¼è¿°å¼ºè°ƒäº†é¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥å‘å±•æ–¹å‘ï¼Œé‡ç‚¹ä»‹ç»äº†æ··åˆæ¡†æ¶çš„æ½œåŠ›ï¼Œè¿™ç§æ¡†æ¶ç»“åˆäº†æ— è®­ç»ƒæ–¹æ³•çš„æ•ˆç‡å’Œå¤šæ¨¡æ€æ¨¡å‹çš„è¯­ä¹‰æ¨ç†ï¼Œä»¥æ¨åŠ¨å¯ä¿¡å’Œå¯è§£é‡Šçš„åˆæˆå›¾åƒå–è¯æŠ€æœ¯çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15176v2">PDF</a> 34 pages, 4 Figures, 10 Tables</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡ç»¼è¿°å…¨é¢æ¢è®¨äº†æœ€æ–°çš„æ£€æµ‹ä¸åˆ†ç±»åˆæˆå›¾åƒçš„æŠ€æœ¯ï¼Œè¿™äº›å›¾åƒç”±å…ˆè¿›çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹ç”Ÿæˆã€‚æ–‡ç« è¯¦ç»†æè¿°äº†å„ç§æ£€æµ‹æ¡†æ¶ï¼ŒåŒ…æ‹¬ç©ºé—´åŸŸã€é¢‘ç‡åŸŸã€æŒ‡çº¹ã€è¡¥ä¸ã€æ— è®­ç»ƒä»¥åŠå¤šæ¨¡æ€æ¨ç†æ¡†æ¶ç­‰ï¼Œå¹¶å¯¹å…¬å¼€æ•°æ®é›†ä¸Šçš„è¿™äº›æ–¹æ³•è¿›è¡Œäº†è¯¦ç»†çš„æ¯”è¾ƒåˆ†æï¼Œä»¥è¯„ä¼°å…¶æ³›åŒ–æ€§ã€é²æ£’æ€§å’Œå¯è§£é‡Šæ€§ã€‚åŒæ—¶ï¼Œæ–‡ç« ä¹ŸæŒ‡å‡ºäº†æœªæ¥é¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥å‘å±•æ–¹å‘ï¼Œå¼ºè°ƒäº†ç»“åˆæ— è®­ç»ƒæ–¹æ³•çš„æ•ˆç‡å’Œå¤šæ¨¡æ€æ¨¡å‹çš„è¯­ä¹‰æ¨ç†çš„æ··åˆæ¡†æ¶åœ¨æ¨åŠ¨å¯ä¿¡å’Œå¯è§£é‡Šçš„åˆæˆå›¾åƒå–è¯ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚GANsã€Diffusion Modelså’ŒVAEsï¼‰çš„åˆæˆå¤šåª’ä½“æ•°æ®è´¨é‡ä¸æ–­æé«˜ï¼Œä½†ä¼´éšå¯¹æŠ—æ€§æ”»å‡»ã€ä¸é“å¾·ä½¿ç”¨å’Œç¤¾ä¼šå±å®³ç­‰æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶äººå‘˜æ­£è‡´åŠ›äºå¼€å‘æœ‰æ•ˆæ£€æµ‹åˆæˆæ•°æ®çš„æ–¹æ³•ï¼Œä»¥å‡è½»æ½œåœ¨é£é™©ã€‚</li>
<li>å½“å‰ç»¼è¿°å¡«è¡¥äº†å…³äºåˆæˆå›¾åƒå–è¯çš„æœ€æ–°è¿›å±•çš„ç©ºç™½ï¼Œç‰¹åˆ«æ˜¯å¤šæ¨¡æ€æ¡†æ¶ã€åŸºäºæ¨ç†çš„æ£€æµ‹å’Œæ— è®­ç»ƒæ–¹æ³•ç­‰ã€‚</li>
<li>æ–‡ç« è¯¦ç»†æè¿°äº†å„ç§æ£€æµ‹æ¡†æ¶çš„æ ¸å¿ƒåŸç†ï¼Œå¹¶åœ¨å…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚</li>
<li>æ–‡ç« å¼ºè°ƒäº†æ£€æµ‹æ–¹æ³•çš„æ³›åŒ–æ€§ã€é²æ£’æ€§å’Œå¯è§£é‡Šæ€§çš„é‡è¦æ€§ã€‚</li>
<li>æœªæ¥çš„æŒ‘æˆ˜å’Œæœªæ¥å‘å±•æ–¹å‘åŒ…æ‹¬å¼€å‘æ··åˆæ¡†æ¶ï¼Œç»“åˆæ— è®­ç»ƒæ–¹æ³•çš„æ•ˆç‡å’Œå¤šæ¨¡æ€æ¨¡å‹çš„è¯­ä¹‰æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15176">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-10a337827658dcd90902d4ecb9930a37~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993148&auth_key=1760993148-0-0-05c4c336e690e5bac3ee26ba582a7c0c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-40125dc934504f5bb5cec7fdb526d6dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993155&auth_key=1760993155-0-0-16ff0e2be8b744c25809262652715e40&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Diffusion-Models-are-Efficient-Data-Generators-for-Human-Mesh-Recovery"><a href="#Diffusion-Models-are-Efficient-Data-Generators-for-Human-Mesh-Recovery" class="headerlink" title="Diffusion Models are Efficient Data Generators for Human Mesh Recovery"></a>Diffusion Models are Efficient Data Generators for Human Mesh Recovery</h2><p><strong>Authors:Yongtao Ge, Wenjia Wang, Yongfan Chen, Fanzhou Wang, Lei Yang, Hao Chen, Chunhua Shen</strong></p>
<p>Despite remarkable progress having been made on the problem of 3D human pose and shape estimation (HPS), current state-of-the-art methods rely heavily on either confined indoor mocap datasets or datasets generated by a rendering engine using computer graphics (CG). Both categories of datasets exhibit inadequacies in furnishing adequate human identities and authentic in-the-wild background scenes, which are crucial for accurately simulating real-world distributions. In this work, we show that synthetic data created by generative models is complementary to CG-rendered data for achieving remarkable generalization performance on diverse real-world scenes. We propose an effective data generation pipeline based on recent diffusion models, termed HumanWild, which can effortlessly generate human images and corresponding 3D mesh annotations. Specifically, we first collect a large-scale human-centric dataset with comprehensive annotations, e.g, text captions, the depth map, and surface normal images. To generate a wide variety of human images with initial labels, we train a customized, multi-condition ControlNet model. The key to this process is using a 3D parametric model, e.g, SMPL-X, to create various condition inputs easily. Our data generation pipeline is both flexible and customizable, making it adaptable to multiple real-world tasks, such as human interaction in complex scenes and humans captured by wide-angle lenses. By relying solely on generative models, we can produce large-scale, in-the-wild human images with high-quality annotations, significantly reducing the need for manual image collection and annotation. The generated dataset encompasses a wide range of viewpoints, environments, and human identities, ensuring its versatility across different scenarios. We hope that our work could pave the way for scaling up 3D human recovery to in-the-wild scenes. </p>
<blockquote>
<p>å°½ç®¡åœ¨ä¸‰ç»´äººä½“å§¿æ€å’Œå½¢çŠ¶ä¼°è®¡ï¼ˆHPSï¼‰é—®é¢˜ä¸Šå–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ä»ç„¶ä¸¥é‡ä¾èµ–äºæœ‰é™çš„å®¤å†…mocapæ•°æ®é›†æˆ–ç”±è®¡ç®—æœºå›¾å½¢ï¼ˆCGï¼‰ç”Ÿæˆçš„æ•°æ®é›†ã€‚è¿™ä¸¤ç±»æ•°æ®é›†åœ¨æä¾›è¶³å¤Ÿçš„äººçš„èº«ä»½å’ŒçœŸå®é‡å¤–èƒŒæ™¯åœºæ™¯æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œè¿™å¯¹äºå‡†ç¡®æ¨¡æ‹ŸçœŸå®ä¸–ç•Œåˆ†å¸ƒè‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç”Ÿæˆæ¨¡å‹åˆ›å»ºçš„åˆæˆæ•°æ®å¯¹äºåœ¨å¤šç§çœŸå®åœºæ™¯ä¸Šå®ç°æ˜¾è‘—æ³›åŒ–æ€§èƒ½çš„CGæ¸²æŸ“æ•°æ®çš„è¡¥å……ä½œç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæœ€æ–°æ‰©æ•£æ¨¡å‹çš„æœ‰æ•ˆçš„æ•°æ®ç”Ÿæˆæµç¨‹ï¼Œç§°ä¸ºHumanWildï¼Œå¯ä»¥è½»æ¾åœ°ç”Ÿæˆäººä½“å›¾åƒå’Œç›¸åº”çš„3Dç½‘æ ¼æ³¨é‡Šã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæ”¶é›†äº†ä¸€ä¸ªå¤§è§„æ¨¡ä»¥äººç±»ä¸ºä¸­å¿ƒçš„æ•°æ®é›†ï¼ŒåŒ…å«å…¨é¢çš„æ³¨é‡Šï¼Œå¦‚æ–‡æœ¬æ ‡é¢˜ã€æ·±åº¦å›¾å’Œè¡¨é¢æ³•çº¿å›¾ã€‚ä¸ºäº†ç”Ÿæˆå…·æœ‰åˆå§‹æ ‡ç­¾çš„å¤šç§äººä½“å›¾åƒï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå®šåˆ¶çš„ã€å¤šæ¡ä»¶ControlNetæ¨¡å‹ã€‚è¯¥è¿‡ç¨‹çš„å…³é”®æ˜¯ä½¿ç”¨ä¸€ä¸ª3Då‚æ•°æ¨¡å‹ï¼ˆå¦‚SMPL-Xï¼‰ï¼Œä»¥è½»æ¾åˆ›å»ºå„ç§æ¡ä»¶è¾“å…¥ã€‚æˆ‘ä»¬çš„æ•°æ®ç”Ÿæˆæµç¨‹æ—¢çµæ´»åˆå¯æ ¹æ®éœ€æ±‚è¿›è¡Œå®šåˆ¶ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”å¤šä¸ªçœŸå®ä¸–ç•Œä»»åŠ¡ï¼Œå¦‚å¤æ‚åœºæ™¯ä¸­çš„äººç±»äº¤äº’å’Œå¹¿è§’é•œå¤´æ‹æ‘„çš„äººç±»ã€‚ä»…ä¾é ç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ç”Ÿæˆå¤§è§„æ¨¡ã€çœŸå®çš„å¸¦æœ‰é«˜è´¨é‡æ³¨é‡Šçš„äººä½“å›¾åƒï¼Œå¤§å¤§é™ä½äº†æ‰‹åŠ¨æ”¶é›†å’Œæ ‡æ³¨å›¾åƒçš„éœ€æ±‚ã€‚ç”Ÿæˆçš„æ•°æ®é›†æ¶µç›–äº†å¹¿æ³›çš„è§†è§’ã€ç¯å¢ƒå’Œäººç‰©èº«ä»½ï¼Œç¡®ä¿å…¶åœ¨ä¸åŒåœºæ™¯ä¸­çš„é€šç”¨æ€§ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œèƒ½ä¸ºæ‰©å±•åˆ°çœŸå®åœºæ™¯çš„3Däººä½“æ¢å¤é“ºå¹³é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.11111v3">PDF</a> Accepted by TPAMI, project page:   <a target="_blank" rel="noopener" href="https://yongtaoge.github.io/projects/humanwild">https://yongtaoge.github.io/projects/humanwild</a></p>
<p><strong>æ‘˜è¦</strong><br>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹ä¸‰ç»´äººä½“å§¿æ€å’Œå½¢çŠ¶ä¼°è®¡ï¼ˆHPSï¼‰é—®é¢˜çš„æœ€æ–°ç ”ç©¶è¿›å±•ã€‚å°½ç®¡å·²æœ‰æ˜¾è‘—è¿›å±•ï¼Œä½†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ä»ä¸¥é‡ä¾èµ–äºå®¤å†…mocapæ•°æ®é›†æˆ–ç”±è®¡ç®—æœºå›¾å½¢ï¼ˆCGï¼‰ç”Ÿæˆçš„æ¸²æŸ“å¼•æ“çš„æ•°æ®é›†ã€‚è¿™ä¸¤ç±»æ•°æ®é›†åœ¨æä¾›è¶³å¤Ÿçš„äººèº«ä»½å’ŒçœŸå®é‡å¤–èƒŒæ™¯åœºæ™¯æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œè¿™å¯¹äºå‡†ç¡®æ¨¡æ‹ŸçœŸå®ä¸–ç•Œåˆ†å¸ƒè‡³å…³é‡è¦ã€‚æœ¬æ–‡å±•ç¤ºäº†ç”Ÿæˆæ¨¡å‹åˆ›å»ºçš„åˆæˆæ•°æ®ä¸CGæ¸²æŸ“æ•°æ®ç›¸ç»“åˆï¼Œå¯ä»¥åœ¨å¤šç§çœŸå®åœºæ™¯ä¸Šå®ç°å‡ºè‰²çš„æ³›åŒ–æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæœ€æ–°æ‰©æ•£æ¨¡å‹çš„æœ‰æ•ˆæ•°æ®ç”Ÿæˆç®¡é“ï¼Œç§°ä¸ºHumanWildï¼Œå¯ä»¥è½»æ¾ç”Ÿæˆäººç±»å›¾åƒå’Œç›¸åº”çš„ä¸‰ç»´ç½‘æ ¼æ³¨é‡Šã€‚æˆ‘ä»¬çš„æ•°æ®ç”Ÿæˆç®¡é“æ—¢çµæ´»åˆå¯æ ¹æ®éœ€æ±‚è¿›è¡Œå®šåˆ¶ï¼Œé€‚åº”å¤šç§ç°å®ä¸–ç•Œä»»åŠ¡ï¼Œå¦‚å¤æ‚åœºæ™¯ä¸­çš„äººç±»äº¤äº’å’Œå¹¿è§’é•œå¤´ä¸‹çš„äººç±»æ‹æ‘„ã€‚é€šè¿‡ä»…ä¾èµ–ç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ç”Ÿæˆå¤§è§„æ¨¡ã€é‡å¤–çš„äººç±»å›¾åƒå’Œé«˜è´¨é‡æ³¨é‡Šï¼Œå¤§å¤§é™ä½äº†æ‰‹åŠ¨æ”¶é›†å›¾åƒå’Œæ³¨é‡Šçš„éœ€æ±‚ã€‚ç”Ÿæˆçš„æ•°æ®é›†æ¶µç›–äº†å¹¿æ³›çš„è§†è§’ã€ç¯å¢ƒå’Œäººç±»èº«ä»½ï¼Œç¡®ä¿å…¶åœ¨ä¸åŒåœºæ™¯ä¸­çš„é€šç”¨æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰HPSç ”ç©¶ä»é¢ä¸´æ•°æ®é—®é¢˜ï¼Œéœ€è¦æ›´çœŸå®ã€å¤šæ ·åŒ–çš„æ•°æ®é›†ã€‚</li>
<li>åˆæˆæ•°æ®å¯¹äºè¡¥å……å®¤å†…å’ŒCGæ¸²æŸ“æ•°æ®å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æœ‰æ•ˆæ•°æ®ç”Ÿæˆç®¡é“HumanWildã€‚</li>
<li>è¯¥ç®¡é“èƒ½å¤Ÿç”Ÿæˆå¸¦æœ‰åˆå§‹æ ‡ç­¾çš„å¤šæ ·åŒ–äººç±»å›¾åƒã€‚</li>
<li>ä½¿ç”¨3Då‚æ•°æ¨¡å‹ï¼ˆå¦‚SMPL-Xï¼‰æ˜¯ç”Ÿæˆæ¡ä»¶è¾“å…¥çš„å…³é”®ã€‚</li>
<li>æ•°æ®ç”Ÿæˆç®¡é“æ—¢çµæ´»åˆå®šåˆ¶åŒ–ï¼Œé€‚åº”å¤šç§ç°å®ä¸–ç•Œä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.11111">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4d4b036f1be79a39748be883bd9f2510~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993162&auth_key=1760993162-0-0-17d015d88c51b964557ce4d15055726f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-96b4860852c48dfc3c12b7be4ef7f993~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993169&auth_key=1760993169-0-0-1ce74c1ab72e8a5ebc248b0893380e48&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a128d9741fe0511a6592d36a6eb5ff65~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993176&auth_key=1760993176-0-0-d0990dba6ea4e664ef932aecf9bd1a27&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ea3ec18519270b7fa0f20e76345f1aa5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993182&auth_key=1760993182-0-0-ecb04cce6a45b6e2c8454bf8b362aa25&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-54bf96486396be671126620d4c926402~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993190&auth_key=1760993190-0-0-3f7c9703aa2928b507cf3005808a31aa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5c7a14ede19a131badb77795c6f63ed1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760993197&auth_key=1760993197-0-0-9881607e1a43b6149ed33446e80ff93f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-21/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-21/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-50a13677fb759f7da81cb1665ea4e2d6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995612&auth_key=1760995612-0-0-fef0a9886ffd577f51e0f5e9084f7975&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-21  Lightweight Data-Free Denoising for Detail-Preserving Biomedical Image   Restoration
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-21/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-a0b07bc355587c44905f1b0a45682293~resize:0:q75.jpg?source=1f5c5e47&expiration=1760991275&auth_key=1760991275-0-0-7c78b74d84769ff26805cc79dc0c5345&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-21  Fix False Transparency by Noise Guided Splatting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32140.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
