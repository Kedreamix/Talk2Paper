<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-21  PokeeResearch Effective Deep Research via Reinforcement Learning from   AI Feedback and Robust Reasoning Scaffold">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-2562c828eaffb0e0e3a13596ae7a73e4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982560&auth_key=1760982560-0-0-6b3ac7eebe70614daa769cc0d4c9a03e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-21-æ›´æ–°"><a href="#2025-10-21-æ›´æ–°" class="headerlink" title="2025-10-21 æ›´æ–°"></a>2025-10-21 æ›´æ–°</h1><h2 id="PokeeResearch-Effective-Deep-Research-via-Reinforcement-Learning-from-AI-Feedback-and-Robust-Reasoning-Scaffold"><a href="#PokeeResearch-Effective-Deep-Research-via-Reinforcement-Learning-from-AI-Feedback-and-Robust-Reasoning-Scaffold" class="headerlink" title="PokeeResearch: Effective Deep Research via Reinforcement Learning from   AI Feedback and Robust Reasoning Scaffold"></a>PokeeResearch: Effective Deep Research via Reinforcement Learning from   AI Feedback and Robust Reasoning Scaffold</h2><p><strong>Authors:Yi Wan, Jiuqi Wang, Liam Li, Jinsong Liu, Ruihao Zhu, Zheqing Zhu</strong></p>
<p>Tool-augmented large language models (LLMs) are emerging as deep research agents, systems that decompose complex queries, retrieve external evidence, and synthesize grounded responses. Yet current agents remain limited by shallow retrieval, weak alignment metrics, and brittle tool-use behavior. We introduce PokeeResearch-7B, a 7B-parameter deep research agent built under a unified reinforcement learning framework for robustness, alignment, and scalability. PokeeResearch-7B is trained by an annotation-free Reinforcement Learning from AI Feedback (RLAIF) framework to optimize policies using LLM-based reward signals that capture factual accuracy, citation faithfulness, and instruction adherence. A chain-of-thought-driven multi-call reasoning scaffold further enhances robustness through self-verification and adaptive recovery from tool failures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves state-of-the-art performance among 7B-scale deep research agents. This highlights that careful reinforcement learning and reasoning design can produce efficient, resilient, and research-grade AI agents. The model and inference code is open-sourced under MIT license at <a target="_blank" rel="noopener" href="https://github.com/Pokee-AI/PokeeResearchOSS">https://github.com/Pokee-AI/PokeeResearchOSS</a>. </p>
<blockquote>
<p>å·¥å…·å¢å¼ºå‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£é€æ¸å‘å±•ä¸ºæ·±åº¦ç ”ç©¶ä»£ç†ï¼Œè¿™äº›ç³»ç»Ÿèƒ½å¤Ÿåˆ†è§£å¤æ‚æŸ¥è¯¢ã€æ£€ç´¢å¤–éƒ¨è¯æ®å¹¶åˆæˆåŸºäºäº‹å®çš„å›ç­”ã€‚ç„¶è€Œï¼Œå½“å‰ä»£ç†ä»å—åˆ°æµ…å±‚æ£€ç´¢ã€å¼±å¯¹é½æŒ‡æ ‡å’Œå·¥å…·ä½¿ç”¨è¡Œä¸ºä¸ç¨³å®šç­‰çš„é™åˆ¶ã€‚æˆ‘ä»¬æ¨å‡ºPokeeResearch-7Bï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸‹æ„å»ºçš„7Bå‚æ•°æ·±åº¦ç ”ç©¶ä»£ç†ï¼Œæ—¨åœ¨å®ç°ç¨³å¥æ€§ã€å¯¹é½æ€§å’Œå¯æ‰©å±•æ€§ã€‚PokeeResearch-7Bé€šè¿‡æ— æ ‡æ³¨çš„å¼ºåŒ–å­¦ä¹ ä»äººå·¥æ™ºèƒ½åé¦ˆï¼ˆRLAIFï¼‰æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œä¼˜åŒ–ç­–ç•¥ä½¿ç”¨LLMä¸ºåŸºç¡€çš„å¥–åŠ±ä¿¡å·ï¼Œä»¥æ•æ‰äº‹å®å‡†ç¡®æ€§ã€å¼•ç”¨å¿ è¯šåº¦å’ŒæŒ‡ä»¤éµå¾ªåº¦ã€‚åŸºäºæ€ç»´é“¾é©±åŠ¨çš„å¤šå‘¼å«æ¨ç†æ¶æ„é€šè¿‡è‡ªæˆ‘éªŒè¯å’Œè‡ªé€‚åº”ä»å·¥å…·æ•…éšœä¸­æ¢å¤ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†ç¨³å¥æ€§ã€‚åœ¨10ä¸ªæµè¡Œçš„æ·±åº¦ç ”ç©¶åŸºå‡†æµ‹è¯•ä¸­ï¼ŒPokeeResearch-7Båœ¨7Bè§„æ¨¡çš„æ·±åº¦ç ”ç©¶ä»£ç†ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™å¼ºè°ƒäº†ç²¾å¿ƒè®¾è®¡çš„å¼ºåŒ–å­¦ä¹ å’Œæ¨ç†å¯ä»¥äº§ç”Ÿé«˜æ•ˆã€åšéŸ§å’Œç ”ç©¶çº§çš„äººå·¥æ™ºèƒ½ä»£ç†ã€‚æ¨¡å‹å’Œæ¨ç†ä»£ç ä»¥MITè®¸å¯è¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Pokee-AI/PokeeResearchOSS%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/Pokee-AI/PokeeResearchOSSå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15862v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨æˆä¸ºæ·±åº¦ç ”ç©¶ä»£ç†å·¥å…·ï¼Œå®ƒä»¬èƒ½å¤Ÿåˆ†è§£å¤æ‚æŸ¥è¯¢ã€æ£€ç´¢å¤–éƒ¨è¯æ®å¹¶åˆæˆæœ‰æ ¹æ®çš„å›åº”ã€‚ç„¶è€Œï¼Œå½“å‰ä»£ç†å·¥å…·ä»å­˜åœ¨æµ…å±‚æ£€ç´¢ã€å¼±å¯¹é½æŒ‡æ ‡å’Œå·¥å…·ä½¿ç”¨è¡Œä¸ºè„†å¼±ç­‰é™åˆ¶ã€‚æˆ‘ä»¬æ¨å‡ºäº†PokeeResearch-7Bï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸‹æ„å»ºçš„7Bå‚æ•°æ·±åº¦ç ”ç©¶ä»£ç†å·¥å…·ï¼Œæ—¨åœ¨å®ç°ç¨³å¥æ€§ã€å¯¹é½æ€§å’Œå¯æ‰©å±•æ€§ã€‚PokeeResearch-7Bé€šè¿‡æ— æ ‡æ³¨çš„å¼ºåŒ–å­¦ä¹ ä»äººå·¥æ™ºèƒ½åé¦ˆï¼ˆRLAIFï¼‰æ¡†æ¶è¿›è¡Œä¼˜åŒ–ï¼Œä½¿ç”¨LLMå¥–åŠ±ä¿¡å·æ¥æ•æ‰äº‹å®å‡†ç¡®æ€§ã€å¼•ç”¨å¿ è¯šåº¦å’ŒæŒ‡ä»¤ä¾ä»æ€§ï¼Œä»¥ä¼˜åŒ–ç­–ç•¥ã€‚æ€ç»´é©±åŠ¨çš„å¤šé‡è°ƒç”¨æ¨ç†æ¶æ„é€šè¿‡è‡ªæˆ‘éªŒè¯å’Œè‡ªé€‚åº”æ¢å¤å·¥å…·æ•…éšœï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†ç¨³å¥æ€§ã€‚åœ¨10ä¸ªæµè¡Œçš„æ·±åº¦ç ”ç©¶åŸºå‡†æµ‹è¯•ä¸­ï¼ŒPokeeResearch-7Båœ¨7Bè§„æ¨¡æ·±åº¦ç ”ç©¶ä»£ç†ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™çªæ˜¾äº†ç²¾å¿ƒè®¾è®¡çš„å¼ºåŒ–å­¦ä¹ å’Œæ¨ç†å¯ä»¥äº§ç”Ÿé«˜æ•ˆã€åšéŸ§å’Œç ”ç©¶çº§çš„äººå·¥æ™ºèƒ½ä»£ç†ã€‚æ¨¡å‹å’Œæ¨ç†ä»£ç å·²åœ¨MITè®¸å¯ä¸‹å¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£æˆä¸ºæ·±åº¦ç ”ç©¶ä»£ç†å·¥å…·ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚æŸ¥è¯¢ã€æ£€ç´¢è¯æ®å¹¶åˆæˆå›åº”ã€‚</li>
<li>å½“å‰LLMå­˜åœ¨æµ…å±‚æ£€ç´¢ã€å¼±å¯¹é½å’Œå·¥å…·ä½¿ç”¨è„†å¼±ç­‰é™åˆ¶ã€‚</li>
<li>PokeeResearch-7Bæ˜¯ä¸€ä¸ªåœ¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸‹æ„å»ºçš„æ·±åº¦ç ”ç©¶ä»£ç†ï¼Œæ—¨åœ¨æé«˜ç¨³å¥æ€§ã€å¯¹é½æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>PokeeResearch-7Bé‡‡ç”¨æ— æ ‡æ³¨å¼ºåŒ–å­¦ä¹ ä»äººå·¥æ™ºèƒ½åé¦ˆï¼ˆRLAIFï¼‰æ¡†æ¶ï¼Œä¼˜åŒ–LLMå¥–åŠ±ä¿¡å·ä»¥æ•æ‰äº‹å®å‡†ç¡®æ€§ã€å¼•ç”¨å¿ è¯šåº¦å’ŒæŒ‡ä»¤ä¾ä»æ€§ã€‚</li>
<li>æ€ç»´é©±åŠ¨çš„å¤šé‡è°ƒç”¨æ¨ç†æ¶æ„å¢å¼ºäº†PokeeResearch-7Bçš„ç¨³å¥æ€§ï¼Œé€šè¿‡è‡ªæˆ‘éªŒè¯å’Œè‡ªé€‚åº”æ¢å¤å·¥å…·æ•…éšœã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒPokeeResearch-7Bè¾¾åˆ°äº†å…ˆè¿›æ€§èƒ½æ°´å¹³ï¼Œçªæ˜¾äº†å¼ºåŒ–å­¦ä¹ å’Œç²¾å¿ƒè®¾è®¡æ¨ç†çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e5eddb1ee1e904d051694b69abef9f40~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982142&auth_key=1760982142-0-0-3a44d61b625f17107a923021664825bc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dcc3b795749171858a1d66222051c44b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982149&auth_key=1760982149-0-0-0f9e0bc12733fac9712214026f6f7e2a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-81877be6241b94432e85bcff8b0a7281~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982155&auth_key=1760982155-0-0-60654ecd42766b9c4c69ab49c4333a50&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="InfiMed-ORBIT-Aligning-LLMs-on-Open-Ended-Complex-Tasks-via-Rubric-Based-Incremental-Training"><a href="#InfiMed-ORBIT-Aligning-LLMs-on-Open-Ended-Complex-Tasks-via-Rubric-Based-Incremental-Training" class="headerlink" title="InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via   Rubric-Based Incremental Training"></a>InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via   Rubric-Based Incremental Training</h2><p><strong>Authors:Pengkai Wang, Qi Zuo, Pengwei Liu, Zhijie Sang, Congkai Xie, Hongxia Yang</strong></p>
<p>Large Language Models (LLMs) have shown substantial advances through reinforcement learning (RL), particularly in domains where rewards can be programmatically verified, such as mathematics and code. In these areas, models benefit from a well-defined operational base guided by explicit rule-based objectives. However, this progress reveals a significant limitation: in open-ended domains where rewards are ambiguous, subjective, or context-dependent, such as creative writing, scientific reasoning, and notably medical consultation, robust reward functions are lacking, making these areas challenging for current RL strategies. To bridge this gap, we introduce ORBIT, an open-ended rubric-based incremental training framework specifically designed for high-stakes medical dialogue. ORBIT integrates syn- thetic dialogue generation with the dynamic creation of rubrics, employing these rubrics to direct an incremental RL process. In particular, this approach does not depend on external medical knowledge or manual rules, instead utilizing rubric-guided feedback to shape learning. When implemented on the Qwen3-4B-Instruct model, our method can greatly enhance its performance on the HealthBench-Hard benchmark from 7.0 to 27.2 using only 2k samples, thus achieving state-of-the-art results for models of this scale. Our analysis confirms that rubric-driven RL fos-ters consistent performance gains across diverse consultation scenarios, going beyond simple numerical improvements. These findings underscore rubric-based feedback as a scalable strategy for advancing LLMs in intricate, open-ended tasks. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¥–åŠ±å¯ä»¥é€šè¿‡ç¨‹åºéªŒè¯çš„é¢†åŸŸï¼Œå¦‚æ•°å­¦å’Œä»£ç ã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œæ¨¡å‹å—ç›Šäºç”±æ˜ç¡®åŸºäºè§„åˆ™çš„ç›®æ ‡å¼•å¯¼çš„è‰¯å¥½å®šä¹‰çš„æ“ä½œåŸºç¡€ã€‚ç„¶è€Œï¼Œè¿™ç§è¿›å±•æ­ç¤ºäº†ä¸€ä¸ªé‡å¤§å±€é™ï¼šåœ¨å¥–åŠ±æ¨¡ç³Šã€ä¸»è§‚æˆ–ä¾èµ–äºä¸Šä¸‹æ–‡çš„æ— é™åˆ¶é¢†åŸŸï¼Œå¦‚åˆ›æ„å†™ä½œã€ç§‘å­¦æ¨ç†å’Œå°¤å…¶æ˜¯åŒ»ç–—å’¨è¯¢ï¼Œç¼ºä¹ç¨³å¥çš„å¥–åŠ±åŠŸèƒ½ï¼Œè¿™ä½¿å¾—è¿™äº›é¢†åŸŸå¯¹å½“å‰çš„RLç­–ç•¥æ„æˆæŒ‘æˆ˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ORBITï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹é«˜é£é™©åŒ»ç–—å¯¹è¯çš„å¼€æ”¾å¼è¯„åˆ†åŸºå‡†å¢é‡è®­ç»ƒæ¡†æ¶ã€‚ORBITå°†åˆæˆå¯¹è¯ç”Ÿæˆä¸åŠ¨æ€åˆ›å»ºè¯„åˆ†åŸºå‡†ç›¸ç»“åˆï¼Œåˆ©ç”¨è¿™äº›è¯„åˆ†åŸºå‡†æ¥æŒ‡å¯¼å¢é‡RLè¿‡ç¨‹ã€‚ç‰¹åˆ«æ˜¯ï¼Œè¿™ç§æ–¹æ³•ä¸ä¾èµ–å¤–éƒ¨åŒ»å­¦çŸ¥è¯†æˆ–æ‰‹åŠ¨è§„åˆ™ï¼Œè€Œæ˜¯åˆ©ç”¨è¯„åˆ†åŸºå‡†æŒ‡å¯¼çš„åé¦ˆæ¥å¡‘é€ å­¦ä¹ ã€‚å½“åœ¨Qwen3-4B-Instructæ¨¡å‹ä¸Šå®æ–½æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…ä½¿ç”¨2kä¸ªæ ·æœ¬å°±èƒ½å°†å…¶åœ¨HealthBench-HardåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ä»7.0æé«˜åˆ°27.2ï¼Œä»è€Œä¸ºè¿™ä¸€è§„æ¨¡çš„æ¨¡å‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æˆæœã€‚æˆ‘ä»¬çš„åˆ†æè¯å®ï¼Œè¯„åˆ†é©±åŠ¨RLåœ¨å¤šç§å’¨è¯¢åœºæ™¯ä¸­ä¿ƒè¿›äº†æ€§èƒ½çš„ç¨³å®šæå‡ï¼Œè¶…è¶Šäº†ç®€å•çš„æ•°å­—æ”¹è¿›ã€‚è¿™äº›å‘ç°å¼ºè°ƒï¼ŒåŸºäºè¯„åˆ†åŸºå‡†çš„åé¦ˆæ˜¯æ¨åŠ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨æ— é™åˆ¶å¤æ‚ä»»åŠ¡ä¸­è¿›æ­¥çš„å¯æ‰©å±•ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15859v1">PDF</a> 17 pages, 6 figures</p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¥–åŠ±å¯ç¨‹åºéªŒè¯çš„é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå¦‚æ•°å­¦å’Œä»£ç ã€‚ç„¶è€Œï¼Œåœ¨å¥–åŠ±æ¨¡ç³Šã€ä¸»è§‚æˆ–ä¾èµ–äºä¸Šä¸‹æ–‡çš„å¼€æ”¾é¢†åŸŸï¼Œå¦‚åˆ›æ„å†™ä½œã€ç§‘å­¦æ¨ç†å’ŒåŒ»ç–—å’¨è¯¢ç­‰ï¼Œå½“å‰çš„RLç­–ç•¥é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæå‡ºäº†ORBITï¼Œä¸€ä¸ªä¸“ä¸ºé«˜é£é™©åŒ»ç–—å¯¹è¯è®¾è®¡çš„å¼€æ”¾å¼è¯„åˆ†åŸºå‡†å¢é‡è®­ç»ƒæ¡†æ¶ã€‚ORBITé€šè¿‡åˆæˆå¯¹è¯ç”Ÿæˆä¸åŠ¨æ€åˆ›å»ºè¯„åˆ†æ ‡å‡†çš„é›†æˆï¼Œåˆ©ç”¨è¿™äº›è¯„åˆ†æ ‡å‡†æŒ‡å¯¼å¢é‡RLè¿‡ç¨‹ã€‚åœ¨Qwen3-4B-Instructæ¨¡å‹ä¸Šå®æ–½åï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨ä»…ä½¿ç”¨2kæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œå°†HealthBench-HardåŸºå‡†æµ‹è¯•çš„æ€§èƒ½ä»7.0æé«˜åˆ°27.2ï¼Œä»è€Œåœ¨å¦‚æ­¤è§„æ¨¡çš„æ¨¡å‹ä¸­å®ç°äº†ä¸šç•Œé¢†å…ˆçš„ç»“æœã€‚åˆ†æè¡¨æ˜ï¼Œè¯„åˆ†é©±åŠ¨RLåœ¨å¤šç§å’¨è¯¢åœºæ™¯ä¸­å®ç°äº†æ€§èƒ½çš„ç¨³å®šæå‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¥–åŠ±å¯ç¨‹åºéªŒè¯çš„é¢†åŸŸï¼Œå¦‚æ•°å­¦å’Œä»£ç ã€‚</li>
<li>åœ¨å¼€æ”¾é¢†åŸŸï¼Œå¦‚åˆ›æ„å†™ä½œã€ç§‘å­¦æ¨ç†å’ŒåŒ»ç–—å’¨è¯¢ç­‰ï¼Œç”±äºå¥–åŠ±çš„æ¨¡ç³Šæ€§ï¼Œå½“å‰å¼ºåŒ–å­¦ä¹ ç­–ç•¥é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ORBITæ˜¯ä¸€ä¸ªä¸“ä¸ºé«˜é£é™©åŒ»ç–—å¯¹è¯è®¾è®¡çš„å¼€æ”¾å¼è¯„åˆ†åŸºå‡†å¢é‡è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚</li>
<li>ORBITé€šè¿‡åˆæˆå¯¹è¯ç”Ÿæˆä¸åŠ¨æ€åˆ›å»ºè¯„åˆ†æ ‡å‡†çš„é›†æˆï¼Œåˆ©ç”¨è¿™äº›è¯„åˆ†æ ‡å‡†æŒ‡å¯¼å¢é‡å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>åœ¨Qwen3-4B-Instructæ¨¡å‹ä¸Šå®æ–½ORBITåï¼Œæ€§èƒ½æ˜¾è‘—æé«˜ï¼Œå®ç°äº†ä¸šç•Œé¢†å…ˆçš„ç»“æœã€‚</li>
<li>è¯„çº§é©±åŠ¨RLåœ¨å¤šç§å’¨è¯¢åœºæ™¯ä¸­è¡¨ç°å‡ºç¨³å®šçš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15859">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c0b28e253d9ec6683b377b3fffaaa2ff~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982163&auth_key=1760982163-0-0-a8dd49c27cd4752073cb6e857c4d4fe4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-17b3d1bbf5ea4d57cf8c0e7695ecf4b1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982172&auth_key=1760982172-0-0-f46ac773168e36a499080d26c88c4537&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="BLIP3o-NEXT-Next-Frontier-of-Native-Image-Generation"><a href="#BLIP3o-NEXT-Next-Frontier-of-Native-Image-Generation" class="headerlink" title="BLIP3o-NEXT: Next Frontier of Native Image Generation"></a>BLIP3o-NEXT: Next Frontier of Native Image Generation</h2><p><strong>Authors:Jiuhai Chen, Le Xue, Zhiyang Xu, Xichen Pan, Shusheng Yang, Can Qin, An Yan, Honglu Zhou, Zeyuan Chen, Lifu Huang, Tianyi Zhou, Junnan Li, Silvio Savarese, Caiming Xiong, Ran Xu</strong></p>
<p>We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3 series that advances the next frontier of native image generation. BLIP3o-NEXT unifies text-to-image generation and image editing within a single architecture, demonstrating strong image generation and image editing capabilities. In developing the state-of-the-art native image generation model, we identify four key insights: (1) Most architectural choices yield comparable performance; an architecture can be deemed effective provided it scales efficiently and supports fast inference; (2) The successful application of reinforcement learning can further push the frontier of native image generation; (3) Image editing still remains a challenging task, yet instruction following and the consistency between generated and reference images can be significantly enhanced through post-training and data engine; (4) Data quality and scale continue to be decisive factors that determine the upper bound of model performance. Building upon these insights, BLIP3o-NEXT leverages an Autoregressive + Diffusion architecture in which an autoregressive model first generates discrete image tokens conditioned on multimodal inputs, whose hidden states are then used as conditioning signals for a diffusion model to generate high-fidelity images. This architecture integrates the reasoning strength and instruction following of autoregressive models with the fine-detail rendering ability of diffusion models, achieving a new level of coherence and realism. Extensive evaluations of various text-to-image and image-editing benchmarks show that BLIP3o-NEXT achieves superior performance over existing models. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºBLIP3o-NEXTï¼Œè¿™æ˜¯BLIP3ç³»åˆ—ä¸­çš„å…¨æ–°å¼€æºåŸºç¡€æ¨¡å‹ï¼Œè¿›ä¸€æ­¥æ¨åŠ¨äº†åŸç”Ÿå›¾åƒç”Ÿæˆçš„è¾¹ç•Œã€‚BLIP3o-NEXTç»Ÿä¸€äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘ï¼Œåœ¨ä¸€ä¸ªæ¶æ„å†…å±•ç°äº†å¼ºå¤§çš„å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘èƒ½åŠ›ã€‚åœ¨å¼€å‘æœ€å…ˆè¿›çš„åŸç”Ÿå›¾åƒç”Ÿæˆæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å‘ç°äº†å››ä¸ªå…³é”®è§è§£ï¼šé¦–å…ˆï¼Œå¤§å¤šæ•°æ¶æ„é€‰æ‹©éƒ½èƒ½äº§ç”Ÿç›¸å½“çš„æ€§èƒ½ï¼›ä¸€ä¸ªæ¶æ„åªè¦èƒ½å¤Ÿé«˜æ•ˆæ‰©å±•å¹¶æ”¯æŒå¿«é€Ÿæ¨ç†ï¼Œå°±å¯ä»¥è¢«è®¤ä¸ºæ˜¯æœ‰æ•ˆçš„ã€‚å…¶æ¬¡ï¼Œå¼ºåŒ–å­¦ä¹ çš„æˆåŠŸåº”ç”¨å¯ä»¥è¿›ä¸€æ­¥æ¨åŠ¨åŸç”Ÿå›¾åƒç”Ÿæˆçš„è¾¹ç•Œã€‚ç¬¬ä¸‰ï¼Œå›¾åƒç¼–è¾‘ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä½†é€šè¿‡äº‹åè®­ç»ƒå’Œæ•°æ®ä¸­å¿ƒï¼ŒæŒ‡ä»¤éµå¾ªå’Œç”Ÿæˆå›¾åƒä¸å‚è€ƒå›¾åƒä¹‹é—´çš„ä¸€è‡´æ€§å¯ä»¥å¾—åˆ°æ˜¾ç€å¢å¼ºã€‚æœ€åï¼Œæ•°æ®çš„è´¨é‡å’Œè§„æ¨¡ä»ç„¶æ˜¯å†³å®šæ¨¡å‹æ€§èƒ½ä¸Šé™çš„å†³å®šæ€§å› ç´ ã€‚åŸºäºè¿™äº›è§è§£ï¼ŒBLIP3o-NEXTé‡‡ç”¨è‡ªå›å½’+æ‰©æ•£æ¶æ„ï¼Œå…¶ä¸­è‡ªå›å½’æ¨¡å‹é¦–å…ˆæ ¹æ®å¤šæ¨¡å¼è¾“å…¥ç”Ÿæˆç¦»æ•£å›¾åƒä»¤ç‰Œï¼Œå…¶éšè—çŠ¶æ€ç„¶åç”¨ä½œæ‰©æ•£æ¨¡å‹çš„æ¡ä»¶ä¿¡å·æ¥ç”Ÿæˆé«˜ä¿çœŸå›¾åƒã€‚è¯¥æ¶æ„å°†è‡ªå›å½’æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’ŒæŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›ä¸æ‰©æ•£æ¨¡å‹çš„ç²¾ç»†ç»†èŠ‚æ¸²æŸ“èƒ½åŠ›ç›¸ç»“åˆï¼Œå®ç°äº†æ–°çš„è¿è´¯æ€§å’Œé€¼çœŸåº¦ã€‚å¯¹å„ç§æ–‡æœ¬åˆ°å›¾åƒå’Œå›¾åƒç¼–è¾‘åŸºå‡†çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒBLIP3o-NEXTåœ¨ç°æœ‰æ¨¡å‹ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15857v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>BLIP3o-NEXTæ˜¯ä¸€ä¸ªå¼€æºçš„åŸºé‡‘æ¨¡å‹ï¼Œå±äºBLIP3ç³»åˆ—ï¼Œæ¨åŠ¨äº†åŸç”Ÿå›¾åƒç”Ÿæˆçš„æ–°è¾¹ç•Œã€‚è¯¥æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚åœ¨å¼€å‘æœ€å…ˆè¿›çš„åŸç”Ÿå›¾åƒç”Ÿæˆæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬è·å¾—äº†å››ä¸ªå…³é”®è§è§£ã€‚é¦–å…ˆï¼Œå¤§å¤šæ•°æ¶æ„é€‰æ‹©éƒ½èƒ½äº§ç”Ÿç›¸å½“çš„æ€§èƒ½ï¼›å…¶æ¬¡ï¼Œå¼ºåŒ–å­¦ä¹ çš„æˆåŠŸåº”ç”¨å¯ä»¥è¿›ä¸€æ­¥æ¨åŠ¨åŸç”Ÿå›¾åƒç”Ÿæˆçš„å‰æ²¿ï¼›ç¬¬ä¸‰ï¼Œå›¾åƒç¼–è¾‘ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä½†é€šè¿‡äº‹ååŸ¹è®­å’Œæ•°æ®å¼•æ“ï¼ŒæŒ‡ä»¤éµå¾ªå’Œç”Ÿæˆå›¾åƒä¸å‚è€ƒå›¾åƒä¹‹é—´çš„ä¸€è‡´æ€§å¯ä»¥å¾—åˆ°æ˜¾ç€å¢å¼ºï¼›æœ€åï¼Œæ•°æ®çš„è´¨é‡å’Œè§„æ¨¡ä»ç„¶æ˜¯å†³å®šæ¨¡å‹æ€§èƒ½ä¸Šé™çš„å†³å®šæ€§å› ç´ ã€‚åŸºäºè¿™äº›è§è§£ï¼ŒBLIP3o-NEXTé‡‡ç”¨è‡ªå›å½’+æ‰©æ•£æ¶æ„ï¼Œè‡ªå›å½’æ¨¡å‹é¦–å…ˆæ ¹æ®å¤šæ¨¡å¼è¾“å…¥ç”Ÿæˆç¦»æ•£å›¾åƒæ ‡è®°ï¼Œå…¶éšè—çŠ¶æ€ç„¶åç”¨ä½œæ‰©æ•£æ¨¡å‹çš„è°ƒèŠ‚ä¿¡å·æ¥ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚è¿™ä¸€æ¶æ„å°†è‡ªå›å½’æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’ŒæŒ‡ä»¤ä¸æ‰©æ•£æ¨¡å‹çš„ç²¾ç»†ç»†èŠ‚æ¸²æŸ“èƒ½åŠ›ç›¸ç»“åˆï¼Œå®ç°äº†æ–°çš„è¿è´¯æ€§å’Œé€¼çœŸåº¦æ°´å¹³ã€‚å¯¹å¤šç§æ–‡æœ¬åˆ°å›¾åƒå’Œå›¾åƒç¼–è¾‘åŸºå‡†çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒBLIP3o-NEXTåœ¨ç°æœ‰æ¨¡å‹ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>BLIP3o-NEXTæ˜¯ä¸€ä¸ªå¼€æºçš„åŸç”Ÿå›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œèåˆäº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘åŠŸèƒ½ã€‚</li>
<li>æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘èƒ½åŠ›ï¼ŒåŸºäºè‡ªå›å½’+æ‰©æ•£æ¶æ„ã€‚</li>
<li>å¤§å¤šæ•°æ¶æ„é€‰æ‹©éƒ½èƒ½äº§ç”Ÿç›¸å½“çš„æ€§èƒ½ï¼Œå…³é”®åœ¨äºæ•ˆç‡ã€å¿«é€Ÿæ¨ç†å’Œæœ‰æ•ˆæ”¯æŒã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ çš„åº”ç”¨è¿›ä¸€æ­¥æ¨åŠ¨äº†åŸç”Ÿå›¾åƒç”Ÿæˆçš„å‘å±•ã€‚</li>
<li>å›¾åƒç¼–è¾‘ä»å…·æŒ‘æˆ˜æ€§ï¼Œä½†é€šè¿‡äº‹ååŸ¹è®­å’Œæ•°æ®å¼•æ“ï¼ŒæŒ‡ä»¤éµå¾ªå’Œä¸€è‡´æ€§æœ‰æ‰€æå‡ã€‚</li>
<li>æ•°æ®è´¨é‡å’Œè§„æ¨¡å¯¹æ¨¡å‹æ€§èƒ½èµ·å†³å®šæ€§ä½œç”¨ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šç§æ–‡æœ¬åˆ°å›¾åƒå’Œå›¾åƒç¼–è¾‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-563c38d60c49e249f470fbb0d646425d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982179&auth_key=1760982179-0-0-18fef6c4ba54dcd132f3c6c7086da55e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-18bb89294a1dafcc53f905b9653cf4b5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982186&auth_key=1760982186-0-0-524582d1ba020e0d2040e0681b399734&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-893fcd2effcc7a8c070f80df79ae88d1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982193&auth_key=1760982193-0-0-f2addfe301a40c16190b73b8e1e5d6eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-78fd76f34459102ee7e2df956f7d792c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982200&auth_key=1760982200-0-0-c744a62b4e5328688b6fc8c3460b4360&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-96881ec6f0183e91dbce89c3cb7d8d03~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982207&auth_key=1760982207-0-0-8bcd2af74cf9b0b973a14cb8dab91a2b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Paper2Web-Letâ€™s-Make-Your-Paper-Alive"><a href="#Paper2Web-Letâ€™s-Make-Your-Paper-Alive" class="headerlink" title="Paper2Web: Letâ€™s Make Your Paper Alive!"></a>Paper2Web: Letâ€™s Make Your Paper Alive!</h2><p><strong>Authors:Yuhang Chen, Tianpeng Lv, Siyi Zhang, Yixiang Yin, Yao Wan, Philip S. Yu, Dongping Chen</strong></p>
<p>Academic project websites can more effectively disseminate research when they clearly present core content and enable intuitive navigation and interaction. However, current approaches such as direct Large Language Model (LLM) generation, templates, or direct HTML conversion struggle to produce layout-aware, interactive sites, and a comprehensive evaluation suite for this task has been lacking. In this paper, we introduce Paper2Web, a benchmark dataset and multi-dimensional evaluation framework for assessing academic webpage generation. It incorporates rule-based metrics like Connectivity, Completeness and human-verified LLM-as-a-Judge (covering interactivity, aesthetics, and informativeness), and PaperQuiz, which measures paper-level knowledge retention. We further present PWAgent, an autonomous pipeline that converts scientific papers into interactive and multimedia-rich academic homepages. The agent iteratively refines both content and layout through MCP tools that enhance emphasis, balance, and presentation quality. Our experiments show that PWAgent consistently outperforms end-to-end baselines like template-based webpages and arXiv&#x2F;alphaXiv versions by a large margin while maintaining low cost, achieving the Pareto-front in academic webpage generation. </p>
<blockquote>
<p>å­¦æœ¯é¡¹ç›®ç½‘ç«™åœ¨æ¸…æ™°å‘ˆç°æ ¸å¿ƒå†…å®¹ã€æä¾›ç›´è§‚å¯¼èˆªå’Œäº¤äº’åŠŸèƒ½æ—¶ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°ä¼ æ’­ç ”ç©¶ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ï¼Œå¦‚ç›´æ¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆã€æ¨¡æ¿æˆ–ç›´æ¥HTMLè½¬æ¢ï¼Œå¾ˆéš¾äº§ç”Ÿå¸ƒå±€æ„è¯†å¼ºã€äº¤äº’æ€§å¼ºçš„ç«™ç‚¹ï¼Œå¹¶ä¸”é’ˆå¯¹æ­¤ä»»åŠ¡çš„å…¨é¢è¯„ä¼°å¥—ä»¶ä¸€ç›´ç¼ºå¤±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Paper2Webï¼Œè¿™æ˜¯ä¸€å¥—ç”¨äºè¯„ä¼°å­¦æœ¯ç½‘é¡µç”Ÿæˆçš„åŸºå‡†æ•°æ®é›†å’Œå¤šç»´åº¦è¯„ä¼°æ¡†æ¶ã€‚å®ƒç»“åˆäº†åŸºäºè§„åˆ™çš„æŒ‡æ ‡ï¼Œå¦‚è¿é€šæ€§ã€å®Œæ•´æ€§ä»¥åŠç»è¿‡äººå·¥éªŒè¯çš„LLM-as-a-Judgeï¼ˆæ¶µç›–äº¤äº’æ€§ã€ç¾è§‚æ€§å’Œä¿¡æ¯æ€§ï¼‰ï¼Œè¿˜æœ‰PaperQuizï¼Œç”¨äºè¡¡é‡è®ºæ–‡çº§åˆ«çš„çŸ¥è¯†ä¿ç•™æƒ…å†µã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¨å‡ºäº†PWAgentï¼Œè¿™æ˜¯ä¸€ä¸ªå°†ç§‘å­¦è®ºæ–‡è½¬åŒ–ä¸ºä¸°å¯Œå¤šåª’ä½“çš„äº¤äº’å¼å­¦æœ¯ä¸»é¡µçš„è‡ªä¸»ç®¡é“ã€‚è¯¥ä»£ç†é€šè¿‡MCPå·¥å…·è¿­ä»£ä¼˜åŒ–å†…å®¹å’Œå¸ƒå±€ï¼Œè¿™äº›å·¥å…·å¢å¼ºäº†é‡ç‚¹ã€å¹³è¡¡å’Œå‘ˆç°è´¨é‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒPWAgentåœ¨å­¦æœ¯ç½‘é¡µç”Ÿæˆæ–¹é¢å§‹ç»ˆå¤§å¹…è¶…è¶Šäº†åŸºäºæ¨¡æ¿çš„ç½‘é¡µå’ŒarXiv&#x2F;alphaXivç‰ˆæœ¬ç­‰ç«¯åˆ°ç«¯çš„åŸºå‡†çº¿ï¼ŒåŒæ—¶ä¿æŒä½æˆæœ¬ï¼Œå®ç°äº†å¸•ç´¯æ‰˜æœ€ä¼˜å‰æ²¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15842v1">PDF</a> Under Review. Check <a target="_blank" rel="noopener" href="https://github.com/YuhangChen1/Paper2All">https://github.com/YuhangChen1/Paper2All</a> for the   unified platform to streamline all academic presentation</p>
<p><strong>Summary</strong><br>ç ”ç©¶æŒ‡å‡ºï¼Œå­¦æœ¯é¡¹ç›®ç½‘ç«™å¯ä»¥é€šè¿‡æ¸…æ™°å‘ˆç°æ ¸å¿ƒå†…å®¹ã€å®ç°ç›´è§‚å¯¼èˆªå’Œäº’åŠ¨æ¥æ›´æœ‰æ•ˆåœ°ä¼ æ’­ç ”ç©¶ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•å¦‚ç›´æ¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆã€æ¨¡æ¿æˆ–ç›´æ¥HTMLè½¬æ¢ï¼Œéš¾ä»¥ç”Ÿæˆå…·æœ‰å¸ƒå±€æ„è¯†å’Œäº’åŠ¨æ€§çš„ç½‘ç«™ï¼Œä¸”ç¼ºä¹å¯¹æ­¤ä»»åŠ¡çš„å…¨é¢è¯„ä¼°å¥—ä»¶ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡ä»‹ç»äº†Paper2Webï¼Œè¿™æ˜¯ä¸€å¥—ç”¨äºè¯„ä¼°å­¦æœ¯ç½‘é¡µç”Ÿæˆçš„åŸºå‡†æ•°æ®é›†å’Œå¤šç»´åº¦è¯„ä¼°æ¡†æ¶ã€‚å®ƒåŒ…å«äº†åŸºäºè§„åˆ™çš„æŒ‡æ ‡ï¼Œå¦‚è¿é€šæ€§ã€å®Œæ•´æ€§å’Œç»è¿‡äººå·¥éªŒè¯çš„LLMæ³•å®˜ï¼ˆæ¶µç›–äº’åŠ¨æ€§ã€ç¾è§‚æ€§å’Œä¿¡æ¯é‡ï¼‰ï¼Œä»¥åŠPaperQuizï¼Œå¯è¡¡é‡è®ºæ–‡å±‚é¢çš„çŸ¥è¯†ä¿ç•™æƒ…å†µã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†PWAgentï¼Œè¿™æ˜¯ä¸€ç§å°†ç§‘å­¦è®ºæ–‡è½¬åŒ–ä¸ºäº’åŠ¨æ€§å’Œå¤šåª’ä½“ä¸°å¯Œçš„å­¦æœ¯ä¸»é¡µçš„è‡ªä¸»ç®¡é“ã€‚è¯¥ä»£ç†é€šè¿‡å¢å¼ºé‡ç‚¹ã€å¹³è¡¡å’Œå‘ˆç°è´¨é‡çš„MCPå·¥å…·æ¥ä¸æ–­æ”¹è¿›å†…å®¹å’Œå¸ƒå±€ã€‚å®éªŒè¡¨æ˜ï¼ŒPWAgentåœ¨ä¿æŒä½æˆæœ¬çš„åŒæ—¶ï¼Œå¤§å¹…ä¼˜äºåŸºäºæ¨¡æ¿çš„ç½‘é¡µå’ŒarXiv&#x2F;alphaXivç‰ˆæœ¬ç­‰ç«¯åˆ°ç«¯çš„åŸºçº¿æ–¹æ¡ˆï¼Œåœ¨å­¦æœ¯ç½‘é¡µç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†å¸•ç´¯æ‰˜å‰æ²¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­¦æœ¯é¡¹ç›®ç½‘ç«™éœ€è¦æ¸…æ™°å‘ˆç°æ ¸å¿ƒå†…å®¹å’Œç›´è§‚å¯¼èˆªä¸äº’åŠ¨ä»¥æ›´æœ‰æ•ˆåœ°ä¼ æ’­ç ”ç©¶ã€‚</li>
<li>å½“å‰å­¦æœ¯ç½‘é¡µç”Ÿæˆæ–¹æ³•é¢ä¸´ç”Ÿæˆå¸ƒå±€æ„è¯†å’Œäº’åŠ¨æ€§ç½‘ç«™çš„æŒ‘æˆ˜ã€‚</li>
<li>ç¼ºä¹å…¨é¢è¯„ä¼°å­¦æœ¯ç½‘é¡µç”Ÿæˆçš„è¯„ä¼°å¥—ä»¶ã€‚</li>
<li>Paper2Webæ˜¯ä¸€å¥—æ–°çš„åŸºå‡†æ•°æ®é›†å’Œå¤šç»´åº¦è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å­¦æœ¯ç½‘é¡µç”Ÿæˆçš„æ•ˆæœã€‚</li>
<li>Paper2WebåŒ…å«åŸºäºè§„åˆ™çš„æŒ‡æ ‡å’Œç»è¿‡äººå·¥éªŒè¯çš„LLMæ³•å®˜ä»¥åŠç”¨äºè¡¡é‡è®ºæ–‡å±‚é¢çŸ¥è¯†ä¿ç•™æƒ…å†µçš„PaperQuizã€‚</li>
<li>PWAgentæ˜¯ä¸€ç§è‡ªä¸»ç®¡é“ï¼Œèƒ½å°†ç§‘å­¦è®ºæ–‡è½¬åŒ–ä¸ºäº’åŠ¨æ€§å’Œå¤šåª’ä½“ä¸°å¯Œçš„å­¦æœ¯ä¸»é¡µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15842">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c7c2484962a26fbb7642c28ad9c1d902~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982215&auth_key=1760982215-0-0-a807034fc48cae4beee9199d59a5e5f4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ad5aac27d9f10c350b65d98e42583def~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982222&auth_key=1760982222-0-0-ffc711f72ba9efcbd9b7aeeb39e8f71d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0cd82178f3ac578e93244d0bf64dee98~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982229&auth_key=1760982229-0-0-f8f23ddb85cb8486f97ed42ac2857755&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-accc24fd12205151b5c093bcfc2619b4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982236&auth_key=1760982236-0-0-8b1c950ed5a77d8dfbe4cf0a4a7d9f42&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-24ab8ead4e95b551f0f4e4291c44cf84~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982243&auth_key=1760982243-0-0-8d4ce20778977ee20e4655c01901b5de&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e51d4a719dcfa9ffb29f9ba6d871a5f8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982250&auth_key=1760982250-0-0-22c835aa0478a684b614bbb9edcbaa94&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e0e4cb57b3b7e9dec6fa82ff0e48a84e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982257&auth_key=1760982257-0-0-41ae5534e02472bcdf8987ae394677a5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Neuro-Symbolic-Spatial-Reasoning-in-Segmentation"><a href="#Neuro-Symbolic-Spatial-Reasoning-in-Segmentation" class="headerlink" title="Neuro-Symbolic Spatial Reasoning in Segmentation"></a>Neuro-Symbolic Spatial Reasoning in Segmentation</h2><p><strong>Authors:Jiayi Lin, Jiabo Huang, Shaogang Gong</strong></p>
<p>Open-Vocabulary Semantic Segmentation (OVSS) assigns pixel-level labels from an open set of categories, requiring generalization to unseen and unlabelled objects. Using vision-language models (VLMs) to correlate local image patches with potential unseen object categories suffers from a lack of understanding of spatial relations of objects in a scene. To solve this problem, we introduce neuro-symbolic (NeSy) spatial reasoning in OVSS. In contrast to contemporary VLM correlation-based approaches, we propose Relational Segmentor (RelateSeg) to impose explicit spatial relational constraints by first order logic (FOL) formulated in a neural network architecture. This is the first attempt to explore NeSy spatial reasoning in OVSS. Specifically, RelateSeg automatically extracts spatial relations, e.g., &lt;cat, to-right-of, person&gt;, and encodes them as first-order logic formulas using our proposed pseudo categories. Each pixel learns to predict both a semantic category (e.g., â€œcatâ€) and a spatial pseudo category (e.g., â€œright of personâ€) simultaneously, enforcing relational constraints (e.g., a â€œcatâ€ pixel must lie to the right of a â€œpersonâ€). Finally, these logic constraints are formulated in a deep network architecture by fuzzy logic relaxation, enabling end-to-end learning of spatial-relationally consistent segmentation. RelateSeg achieves state-of-the-art performance in terms of average mIoU across four benchmark datasets and particularly shows clear advantages on images containing multiple categories, with the cost of only introducing a single auxiliary loss function and no additional parameters, validating the effectiveness of NeSy spatial reasoning in OVSS. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰ä¸ºæ¥è‡ªå¼€æ”¾ç±»åˆ«é›†çš„åƒç´ çº§æ ‡ç­¾åˆ†é…æ ‡ç­¾ï¼Œéœ€è¦æ¨å¹¿åˆ°æœªè§è¿‡çš„æœªæ ‡è®°å¯¹è±¡ã€‚ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥å…³è”å±€éƒ¨å›¾åƒè¡¥ä¸ä¸æ½œåœ¨æœªè§å¯¹è±¡ç±»åˆ«ï¼Œå¯¹äºåœºæ™¯ä¸­å¯¹è±¡çš„ç©ºé—´å…³ç³»çš„ç†è§£å­˜åœ¨ç¼ºé™·ã€‚ä¸ºäº†è§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨OVSSä¸­å¼•å…¥äº†ç¥ç»ç¬¦å·ï¼ˆNeSyï¼‰ç©ºé—´æ¨ç†ã€‚ä¸å½“ä»£åŸºäºVLMå…³è”çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬æå‡ºå…³ç³»åˆ†æ®µå™¨ï¼ˆRelateSegï¼‰ï¼Œå®ƒé€šè¿‡ç¥ç»ç½‘ç»œæ¶æ„åˆ¶å®šçš„ä¸€é˜¶é€»è¾‘ï¼ˆFOLï¼‰æ¥æ–½åŠ æ˜ç¡®çš„ç©ºé—´å…³ç³»çº¦æŸã€‚è¿™æ˜¯OVSSä¸­æ¢ç´¢NeSyç©ºé—´æ¨ç†çš„é¦–æ¬¡å°è¯•ã€‚å…·ä½“æ¥è¯´ï¼ŒRelateSegè‡ªåŠ¨æå–ç©ºé—´å…³ç³»ï¼Œä¾‹å¦‚â€œçŒ«ï¼Œåœ¨äººçš„å³è¾¹â€ï¼Œå¹¶å°†å…¶ç¼–ç ä¸ºä¸€é˜¶é€»è¾‘å…¬å¼ï¼Œä½¿ç”¨æˆ‘ä»¬æå‡ºçš„ä¼ªç±»åˆ«ã€‚æ¯ä¸ªåƒç´ éƒ½å­¦ä¹ åŒæ—¶é¢„æµ‹è¯­ä¹‰ç±»åˆ«ï¼ˆä¾‹å¦‚â€œçŒ«â€ï¼‰å’Œç©ºé—´ä¼ªç±»åˆ«ï¼ˆä¾‹å¦‚â€œäººçš„å³è¾¹â€ï¼‰ï¼Œå¼ºåˆ¶æ‰§è¡Œå…³ç³»çº¦æŸï¼ˆä¾‹å¦‚ï¼Œâ€œçŒ«â€åƒç´ å¿…é¡»ä½äºâ€œäººâ€çš„å³ä¾§ï¼‰ã€‚æœ€åï¼Œè¿™äº›é€»è¾‘çº¦æŸé€šè¿‡æ¨¡ç³Šé€»è¾‘æ¾å¼›åœ¨æ·±å±‚æ¬¡çš„ç½‘ç»œæ¶æ„ä¸­åˆ¶å®šï¼Œå®ç°å¯¹ç©ºé—´å…³ç³»ä¸€è‡´åˆ†å‰²çš„ç«¯åˆ°ç«¯å­¦ä¹ ã€‚RelateSegåœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹³å‡mIoUè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ…å«å¤šä¸ªç±»åˆ«çš„å›¾åƒä¸Šæ˜¾ç¤ºå‡ºæ˜æ˜¾ä¼˜åŠ¿ï¼Œè€Œåªéœ€å¼•å…¥ä¸€ä¸ªè¾…åŠ©æŸå¤±å‡½æ•°ä¸”æ— éœ€æ·»åŠ é¢å¤–å‚æ•°ï¼ŒéªŒè¯äº†NeSyç©ºé—´æ¨ç†åœ¨OVSSä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15841v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰ä¸­çš„ç¥ç»ç¬¦å·ï¼ˆNeSyï¼‰ç©ºé—´æ¨ç†æŠ€æœ¯ã€‚é’ˆå¯¹ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨åœºæ™¯ä¸­ç†è§£å¯¹è±¡ç©ºé—´å…³ç³»çš„é—®é¢˜ï¼Œæå‡ºäº†å…³ç³»åˆ†å‰²å™¨ï¼ˆRelateSegï¼‰ã€‚RelateSegé€šè¿‡ç¥ç»ç½‘ç»œæ¶æ„æ–½åŠ æ˜¾å¼ç©ºé—´å…³ç³»çº¦æŸï¼Œé‡‡ç”¨ä¸€é˜¶é€»è¾‘ï¼ˆFOLï¼‰å…¬å¼è¡¨è¾¾ã€‚è¿™æ˜¯é¦–æ¬¡å°è¯•åœ¨OVSSä¸­æ¢ç´¢NeSyç©ºé—´æ¨ç†ã€‚RelateSegè‡ªåŠ¨æå–ç©ºé—´å…³ç³»ï¼Œå¦‚â€œçŒ«åœ¨äººçš„å³è¾¹â€ï¼Œå¹¶å°†å…¶ç¼–ç ä¸ºä¸€é˜¶é€»è¾‘å…¬å¼ã€‚æ¯ä¸ªåƒç´ åŒæ—¶é¢„æµ‹è¯­ä¹‰ç±»åˆ«ï¼ˆå¦‚â€œçŒ«â€ï¼‰å’Œç©ºé—´ä¼ªç±»åˆ«ï¼ˆå¦‚â€œäººçš„å³è¾¹â€ï¼‰ï¼Œå¼ºåˆ¶æ‰§è¡Œå…³ç³»çº¦æŸã€‚æœ€åï¼Œè¿™äº›é€»è¾‘çº¦æŸé€šè¿‡æ¨¡ç³Šé€»è¾‘æ¾å¼›åœ¨æ·±åº¦ç½‘ç»œæ¶æ„ä¸­è¡¨è¾¾ï¼Œå®ç°äº†ç©ºé—´å…³ç³»ä¸€è‡´çš„ç«¯åˆ°ç«¯å­¦ä¹ åˆ†å‰²ã€‚RelateSegåœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹³å‡mIoUè¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ…å«å¤šä¸ªç±»åˆ«çš„å›¾åƒä¸Šæ˜¾ç¤ºå‡ºæ˜æ˜¾ä¼˜åŠ¿ï¼Œåªéœ€å¼•å…¥ä¸€ä¸ªè¾…åŠ©æŸå¤±å‡½æ•°ä¸”æ— éœ€é¢å¤–å‚æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰éœ€è¦æ¨¡å‹å¯¹æœªè§è¿‡çš„å¯¹è±¡è¿›è¡Œæ³›åŒ–ï¼Œå¹¶åˆ†é…åƒç´ çº§æ ‡ç­¾ã€‚</li>
<li>å½“å‰ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ–¹æ³•åœ¨ç†è§£åœºæ™¯ä¸­çš„å¯¹è±¡ç©ºé—´å…³ç³»æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>å¼•å…¥ç¥ç»ç¬¦å·ï¼ˆNeSyï¼‰ç©ºé—´æ¨ç†æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡ç¥ç»ç½‘ç»œæ¶æ„æ–½åŠ æ˜¾å¼ç©ºé—´å…³ç³»çº¦æŸã€‚</li>
<li>å…³ç³»åˆ†å‰²å™¨ï¼ˆRelateSegï¼‰èƒ½è‡ªåŠ¨æå–å¹¶ç¼–ç ç©ºé—´å…³ç³»ï¼Œå¦‚â€œçŒ«åœ¨äººçš„å³è¾¹â€ã€‚</li>
<li>RelateSegåŒæ—¶é¢„æµ‹åƒç´ çš„è¯­ä¹‰ç±»åˆ«å’Œç©ºé—´ä¼ªç±»åˆ«ï¼Œå¼ºåˆ¶æ‰§è¡Œå…³ç³»çº¦æŸã€‚</li>
<li>é€šè¿‡æ¨¡ç³Šé€»è¾‘æ¾å¼›è¡¨è¾¾é€»è¾‘çº¦æŸï¼Œå®ç°ç©ºé—´å…³ç³»ä¸€è‡´çš„ç«¯åˆ°ç«¯å­¦ä¹ åˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15841">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8279188da9377b033170d4a90c2059c1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982265&auth_key=1760982265-0-0-a83d72a725309129965d05d10a261e6e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bb1d121b594d0c0ab25663501b718561~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982272&auth_key=1760982272-0-0-2bdd8013bd9a19196167df2edacf062a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d6293418855409699c5818d9943fe370~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982279&auth_key=1760982279-0-0-8fdd7db0aef2013bb8ca3508db331141&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d01b3f4003af96f89dea472879f34098~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982286&auth_key=1760982286-0-0-ba8fb472344433956eb3919562da94a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-06bebdaad128617567daa6290599b142~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982293&auth_key=1760982293-0-0-70903845dbfda041df016406d2ec92a6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Demo-Guide-RAG-Evidence-Driven-Corpus-Curation-for-Retrieval-Augmented-Generation-in-Long-COVID"><a href="#Demo-Guide-RAG-Evidence-Driven-Corpus-Curation-for-Retrieval-Augmented-Generation-in-Long-COVID" class="headerlink" title="Demo: Guide-RAG: Evidence-Driven Corpus Curation for Retrieval-Augmented   Generation in Long COVID"></a>Demo: Guide-RAG: Evidence-Driven Corpus Curation for Retrieval-Augmented   Generation in Long COVID</h2><p><strong>Authors:Philip DiGiacomo, Haoyang Wang, Jinrui Fang, Yan Leng, W Michael Brode, Ying Ding</strong></p>
<p>As AI chatbots gain adoption in clinical medicine, developing effective frameworks for complex, emerging diseases presents significant challenges. We developed and evaluated six Retrieval-Augmented Generation (RAG) corpus configurations for Long COVID (LC) clinical question answering, ranging from expert-curated sources to large-scale literature databases. Our evaluation employed an LLM-as-a-judge framework across faithfulness, relevance, and comprehensiveness metrics using LongCOVID-CQ, a novel dataset of expert-generated clinical questions. Our RAG corpus configuration combining clinical guidelines with high-quality systematic reviews consistently outperformed both narrow single-guideline approaches and large-scale literature databases. Our findings suggest that for emerging diseases, retrieval grounded in curated secondary reviews provides an optimal balance between narrow consensus documents and unfiltered primary literature, supporting clinical decision-making while avoiding information overload and oversimplified guidance. We propose Guide-RAG, a chatbot system and accompanying evaluation framework that integrates both curated expert knowledge and comprehensive literature databases to effectively answer LC clinical questions. </p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½èŠå¤©æœºå™¨äººåœ¨ä¸´åºŠåŒ»å­¦ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œé’ˆå¯¹æ–°å…´å¤æ‚ç–¾ç—…å¼€å‘æœ‰æ•ˆçš„æ¡†æ¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬ä¸ºLong COVIDï¼ˆLCï¼‰çš„ä¸´åºŠé—®ç­”å¼€å‘äº†å…­ç§æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è¯­æ–™åº“é…ç½®ï¼Œä»ä¸“å®¶ç²¾å¿ƒæŒ‘é€‰çš„æºåˆ°å¤§è§„æ¨¡æ–‡çŒ®æ•°æ®åº“ä¸ç­‰ã€‚æˆ‘ä»¬çš„è¯„ä¼°ä½¿ç”¨äº†ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ³•å®˜çš„æ¡†æ¶ï¼Œæ ¹æ®å¿ å®åº¦ã€ç›¸å…³æ€§å’Œå…¨é¢æ€§çš„æŒ‡æ ‡ï¼Œå¹¶åˆ©ç”¨ä¸“å®¶ç”Ÿæˆçš„ä¸´åºŠé—®é¢˜æ–°å‹æ•°æ®é›†LongCOVID-CQè¿›è¡Œè¯„ä»·ã€‚æˆ‘ä»¬çš„RAGè¯­æ–™åº“é…ç½®ç»“åˆäº†ä¸´åºŠæŒ‡å—å’Œé«˜è´¨é‡çš„ç³»ç»Ÿè¯„ä»·ï¼ŒæŒç»­ä¼˜äºç‹­éš˜çš„å•æŒ‡å—æ–¹æ³•å’Œå¤§è§„æ¨¡æ–‡çŒ®æ•°æ®åº“ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯¹äºæ–°å…´ç–¾ç—…è€Œè¨€ï¼ŒåŸºäºç²¾é€‰äºŒæ¬¡è¯„ä»·çš„æ£€ç´¢åœ¨ç‹­çª„å…±è¯†æ–‡ä»¶å’Œæœªç­›é€‰çš„ä¸€æ¬¡æ–‡çŒ®ä¹‹é—´æä¾›äº†æœ€ä½³å¹³è¡¡ï¼Œæ—¢æ”¯æŒä¸´åºŠå†³ç­–åˆ¶å®šï¼Œåˆé¿å…äº†ä¿¡æ¯è¿‡è½½å’Œè¿‡äºç®€åŒ–çš„æŒ‡å¯¼ã€‚æˆ‘ä»¬æå‡ºäº†Guide-RAGè¿™ä¸€èŠå¤©æœºå™¨äººç³»ç»ŸåŠå…¶è¯„ä¼°æ¡†æ¶ï¼Œèåˆäº†ç²¾é€‰çš„ä¸“å®¶çŸ¥è¯†å’Œå…¨é¢çš„æ–‡çŒ®æ•°æ®åº“ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å›ç­”LCçš„ä¸´åºŠé—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15782v1">PDF</a> Accepted to 39th Conference on Neural Information Processing Systems   (NeurIPS 2025) Workshop: The Second Workshop on GenAI for Health: Potential,   Trust, and Policy Compliance</p>
<p><strong>Summary</strong>ï¼šéšç€äººå·¥æ™ºèƒ½èŠå¤©æœºå™¨äººåœ¨ä¸´åºŠåŒ»å­¦ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œé’ˆå¯¹æ–°å…´ç–¾ç—…çš„å¤æ‚é—®é¢˜æ„å»ºæœ‰æ•ˆçš„æ¡†æ¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚é’ˆå¯¹Long COVIDï¼ˆLCï¼‰çš„ä¸´åºŠé—®ç­”éœ€æ±‚ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†å…­ç§Retrieval-Augmented Generationï¼ˆRAGï¼‰è¯­æ–™åº“é…ç½®æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ä¸“å®¶ç²¾é€‰æ¥æºåŠå¤§è§„æ¨¡æ–‡çŒ®æ•°æ®åº“ç­‰ã€‚é€šè¿‡LongCOVID-CQè¿™ä¸€æ–°å‹ä¸“å®¶ç”Ÿæˆçš„ä¸´åºŠé—®é¢˜æ•°æ®é›†ï¼Œç ”ç©¶å›¢é˜Ÿè¯„ä¼°äº†å…¶åœ¨å¿ å®åº¦ã€ç›¸å…³æ€§å’Œç»¼åˆåº¦æ–¹é¢çš„è¡¨ç°ã€‚ç»“åˆä¸´åºŠæŒ‡å—ä¸é«˜è´¨é‡ç³»ç»Ÿæ€§å®¡æŸ¥çš„RAGè¯­æ–™åº“é…ç½®æ–¹æ¡ˆè¡¨ç°æœ€ä½³ï¼Œæ—¢ä¼˜äºå•ä¸€æŒ‡å—æ–¹æ³•ï¼Œä¹Ÿä¼˜äºå¤§è§„æ¨¡æ–‡çŒ®æ•°æ®åº“æ–¹æ³•ã€‚ç ”ç©¶è®¤ä¸ºï¼Œé’ˆå¯¹æ–°å…´ç–¾ç—…ï¼ŒåŸºäºç²¾é€‰äºŒæ¬¡å®¡æŸ¥çš„æ£€ç´¢å¯æä¾›æœ€ä½³å¹³è¡¡ï¼Œæ—¢æ”¯æŒä¸´åºŠå†³ç­–åˆ¶å®šï¼Œåˆé¿å…ä¿¡æ¯è¿‡è½½å’Œç®€åŒ–æŒ‡å¯¼ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†Guide-RAGèŠå¤©æœºå™¨äººç³»ç»Ÿå’Œè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æ•´åˆä¸“å®¶çŸ¥è¯†å’Œç»¼åˆæ–‡çŒ®æ•°æ®åº“ï¼Œæœ‰æ•ˆå›ç­”LCçš„ä¸´åºŠé—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>AIèŠå¤©æœºå™¨äººåœ¨ä¸´åºŠåŒ»å­¦ä¸­é¢å¯¹æ–°å…´ç–¾ç—…çš„æŒ‘æˆ˜åœ¨äºå¦‚ä½•æ„å»ºæœ‰æ•ˆçš„æ¡†æ¶æ¥åº”å¯¹å¤æ‚æ€§ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿä¸ºLong COVIDï¼ˆLCï¼‰çš„ä¸´åºŠé—®ç­”éœ€æ±‚å¼€å‘äº†å…­ç§RAGè¯­æ–™åº“é…ç½®æ–¹æ¡ˆã€‚</li>
<li>ç»“åˆä¸´åºŠæŒ‡å—ä¸é«˜è´¨é‡ç³»ç»Ÿæ€§å®¡æŸ¥çš„RAGè¯­æ–™åº“é…ç½®æ–¹æ¡ˆè¡¨ç°æœ€ä½³ã€‚</li>
<li>Guide-RAGèŠå¤©æœºå™¨äººç³»ç»Ÿå’Œè¯„ä¼°æ¡†æ¶æ—¨åœ¨æ•´åˆä¸“å®¶çŸ¥è¯†å’Œç»¼åˆæ–‡çŒ®æ•°æ®åº“ã€‚</li>
<li>æ£€ç´¢æ–¹æ³•éœ€å¹³è¡¡ä¸´åºŠå†³ç­–æ”¯æŒã€ä¿¡æ¯è¿‡è½½å’Œç®€åŒ–æŒ‡å¯¼çš„éœ€æ±‚ã€‚</li>
<li>é€šè¿‡LongCOVID-CQæ•°æ®é›†è¯„ä¼°äº†å¿ å®åº¦ã€ç›¸å…³æ€§å’Œç»¼åˆåº¦ä¸‰ä¸ªæ–¹é¢çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-89857b2f68688224adfc8d16c3debc5a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982300&auth_key=1760982300-0-0-0bb707a56017bd7d3dd0baa0bd04bd0c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-155d8ae7807f5ff27718a5df48708d40~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982308&auth_key=1760982308-0-0-6e911e26d414c767e6b663bc7ebd831d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d7dd2d77174c10b4af0c3413ec0ead14~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982315&auth_key=1760982315-0-0-36735635890703b46753efcd0b65a1f7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="HEADER-Hierarchical-Robot-Exploration-via-Attention-Based-Deep-Reinforcement-Learning-with-Expert-Guided-Reward"><a href="#HEADER-Hierarchical-Robot-Exploration-via-Attention-Based-Deep-Reinforcement-Learning-with-Expert-Guided-Reward" class="headerlink" title="HEADER: Hierarchical Robot Exploration via Attention-Based Deep   Reinforcement Learning with Expert-Guided Reward"></a>HEADER: Hierarchical Robot Exploration via Attention-Based Deep   Reinforcement Learning with Expert-Guided Reward</h2><p><strong>Authors:Yuhong Cao, Yizhuo Wang, Jingsong Liang, Shuhao Liao, Yifeng Zhang, Peizhuo Li, Guillaume Sartoretti</strong></p>
<p>This work pushes the boundaries of learning-based methods in autonomous robot exploration in terms of environmental scale and exploration efficiency. We present HEADER, an attention-based reinforcement learning approach with hierarchical graphs for efficient exploration in large-scale environments. HEADER follows existing conventional methods to construct hierarchical representations for the robot belief&#x2F;map, but further designs a novel community-based algorithm to construct and update a global graph, which remains fully incremental, shape-adaptive, and operates with linear complexity. Building upon attention-based networks, our planner finely reasons about the nearby belief within the local range while coarsely leveraging distant information at the global scale, enabling next-best-viewpoint decisions that consider multi-scale spatial dependencies. Beyond novel map representation, we introduce a parameter-free privileged reward that significantly improves model performance and produces near-optimal exploration behaviors, by avoiding training objective bias caused by handcrafted reward shaping. In simulated challenging, large-scale exploration scenarios, HEADER demonstrates better scalability than most existing learning and non-learning methods, while achieving a significant improvement in exploration efficiency (up to 20%) over state-of-the-art baselines. We also deploy HEADER on hardware and validate it in complex, large-scale real-life scenarios, including a 300m*230m campus environment. </p>
<blockquote>
<p>æœ¬æ–‡åœ¨ç¯å¢ƒè§„æ¨¡å’Œæ¢ç´¢æ•ˆç‡æ–¹é¢æ¨åŠ¨äº†åŸºäºå­¦ä¹ çš„è‡ªä¸»æœºå™¨äººæ¢ç´¢æ–¹æ³•çš„è¾¹ç•Œã€‚æˆ‘ä»¬æå‡ºäº†HEADERï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä½¿ç”¨åˆ†å±‚å›¾è¿›è¡Œå¤§è§„æ¨¡ç¯å¢ƒä¸­çš„é«˜æ•ˆæ¢ç´¢ã€‚HEADERéµå¾ªç°æœ‰çš„ä¼ ç»Ÿæ–¹æ³•ä¸ºæœºå™¨äººçš„ä¿¡å¿µ&#x2F;åœ°å›¾æ„å»ºåˆ†å±‚è¡¨ç¤ºï¼Œä½†è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ç§åŸºäºç¤¾åŒºçš„æ–°ç®—æ³•æ¥æ„å»ºå’Œæ›´æ–°å…¨å±€å›¾ï¼Œè¯¥ç®—æ³•ä¿æŒå®Œå…¨å¢é‡ã€å½¢çŠ¶è‡ªé€‚åº”ï¼Œå¹¶ä»¥çº¿æ€§å¤æ‚åº¦è¿è¡Œã€‚æˆ‘ä»¬çš„è§„åˆ’å™¨åŸºäºæ³¨æ„åŠ›ç½‘ç»œï¼Œèƒ½å¤Ÿç²¾ç»†æ¨ç†å±€éƒ¨èŒƒå›´å†…çš„é™„è¿‘ä¿¡å¿µï¼ŒåŒæ—¶ç²—ç•¥åˆ©ç”¨å…¨å±€å°ºåº¦ä¸Šçš„è¿œè·ç¦»ä¿¡æ¯ï¼Œä»è€Œèƒ½å¤Ÿè€ƒè™‘å¤šå°ºåº¦ç©ºé—´ä¾èµ–æ€§çš„ä¸‹ä¸€ä¸ªæœ€ä½³è§†ç‚¹å†³ç­–ã€‚é™¤äº†æ–°çš„åœ°å›¾è¡¨ç¤ºä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ— å‚æ•°çš„ç‰¹æƒå¥–åŠ±ï¼Œè¯¥å¥–åŠ±æ˜¾è‘—æé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œå¹¶äº§ç”Ÿäº†æ¥è¿‘æœ€ä¼˜çš„æ¢ç´¢è¡Œä¸ºï¼Œé¿å…äº†ç”±äºæ‰‹å·¥å¥–åŠ±å½¢çŠ¶é€ æˆçš„è®­ç»ƒç›®æ ‡åè§ã€‚åœ¨æ¨¡æ‹Ÿçš„å…·æœ‰æŒ‘æˆ˜æ€§ã€å¤§è§„æ¨¡çš„æ¢ç´¢åœºæ™¯ä¸­ï¼ŒHEADERå±•ç¤ºäº†æ¯”å¤§å¤šæ•°ç°æœ‰çš„å­¦ä¹ å’Œéå­¦ä¹ æ–¹æ³•æ›´å¥½çš„å¯æ‰©å±•æ€§ï¼ŒåŒæ—¶åœ¨æ¢ç´¢æ•ˆç‡ä¸Šå®ç°äº†å¯¹æœ€æ–°æŠ€æœ¯çš„æ˜¾è‘—æ”¹å–„ï¼ˆé«˜è¾¾20%ï¼‰ã€‚æˆ‘ä»¬è¿˜éƒ¨ç½²äº†HEADERåœ¨ç¡¬ä»¶ä¸Šå¹¶åœ¨å¤æ‚çš„å¤§è§„æ¨¡çœŸå®åœºæ™¯ä¸­è¿›è¡Œäº†éªŒè¯ï¼ŒåŒ…æ‹¬ä¸€ä¸ª300ç±³* 230ç±³çš„æ ¡å›­ç¯å¢ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15679v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ³¨æ„åŠ›å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•HEADERï¼Œç”¨äºå¤§è§„æ¨¡ç¯å¢ƒä¸­çš„é«˜æ•ˆæœºå™¨äººè‡ªä¸»æ¢ç´¢ã€‚HEADERæ„å»ºäº†ä¸€ç§åŸºäºç¤¾åŒºçš„å…¨å±€å›¾ç®—æ³•ï¼Œå…·æœ‰å¢é‡æ€§ã€å½¢çŠ¶è‡ªé€‚åº”å’Œçº¿æ€§è¿ç®—å¤æ‚åº¦ç­‰ç‰¹æ€§ã€‚å®ƒé€šè¿‡æ³¨æ„ç½‘ç»œå¯¹è¿‘è·ç¦»ä¿¡æ¯è¿›è¡Œç²¾ç»†æ¨ç†ï¼ŒåŒæ—¶å¯¹è¿œè·ç¦»ä¿¡æ¯è¿›è¡Œç²—ç•¥åˆ©ç”¨ï¼Œè€ƒè™‘äº†å¤šå°ºåº¦ç©ºé—´ä¾èµ–æ€§ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ç§æ— å‚æ•°çš„ç‰¹æƒå¥–åŠ±ï¼Œé¿å…äº†æ‰‹å·¥å¥–åŠ±å½¢çŠ¶å¸¦æ¥çš„è®­ç»ƒç›®æ ‡åè§ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œå®ç°äº†è¿‘æœ€ä¼˜çš„æ¢ç´¢è¡Œä¸ºã€‚åœ¨æ¨¡æ‹Ÿçš„å¤§è§„æ¨¡æ¢ç´¢åœºæ™¯ä¸­ï¼ŒHEADERè¡¨ç°å‡ºè‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œç›¸è¾ƒäºç°æœ‰å­¦ä¹ æ–¹æ³•å’Œéå­¦ä¹ æ–¹æ³•å…·æœ‰æ›´é«˜çš„æ¢ç´¢æ•ˆç‡ï¼ˆæœ€é«˜æå‡20%ï¼‰ã€‚åŒæ—¶ï¼Œåœ¨å¤æ‚çš„å¤§è§„æ¨¡çœŸå®åœºæ™¯ä¸­è¿›è¡Œäº†ç¡¬ä»¶éƒ¨ç½²éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HEADERæ˜¯ä¸€ç§åŸºäºæ³¨æ„åŠ›å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œç”¨äºå¤§è§„æ¨¡ç¯å¢ƒä¸­çš„æœºå™¨äººè‡ªä¸»æ¢ç´¢ã€‚</li>
<li>HEADERæ„å»ºäº†ä¸€ç§åŸºäºç¤¾åŒºçš„å…¨å±€å›¾ç®—æ³•ï¼Œå…·æœ‰å¢é‡æ€§ã€å½¢çŠ¶è‡ªé€‚åº”å’Œçº¿æ€§è¿ç®—å¤æ‚åº¦ç‰¹æ€§ã€‚</li>
<li>é€šè¿‡æ³¨æ„ç½‘ç»œå¯¹è¿‘è·ç¦»ä¿¡æ¯è¿›è¡Œç²¾ç»†æ¨ç†ï¼ŒåŒæ—¶å¯¹è¿œè·ç¦»ä¿¡æ¯è¿›è¡Œç²—ç•¥åˆ©ç”¨ï¼Œè€ƒè™‘äº†å¤šå°ºåº¦ç©ºé—´ä¾èµ–æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ— å‚æ•°çš„ç‰¹æƒå¥–åŠ±ï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½å¹¶å®ç°äº†è¿‘æœ€ä¼˜çš„æ¢ç´¢è¡Œä¸ºã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿçš„å¤§è§„æ¨¡æ¢ç´¢åœºæ™¯ä¸­ï¼ŒHEADERç›¸è¾ƒäºç°æœ‰æ–¹æ³•å…·æœ‰æ›´é«˜çš„æ¢ç´¢æ•ˆç‡ã€‚</li>
<li>HEADERåœ¨å¤æ‚çš„å¤§è§„æ¨¡çœŸå®åœºæ™¯ä¸­å…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4f6f17d79f703ae9d1db1bf61209e759~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982322&auth_key=1760982322-0-0-a84565a2827b971eff0656c685241306&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3f17d590f56996438fbf2562fe5ad189~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982330&auth_key=1760982330-0-0-7b0bb89a44628157b78348ced8c1dfc7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2743ad3f1ba9ba1859c4faa85b0c1cfd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982337&auth_key=1760982337-0-0-3b5ebdb36144693d1fc5a30efd2aab3f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CarBoN-Calibrated-Best-of-N-Sampling-Improves-Test-time-Reasoning"><a href="#CarBoN-Calibrated-Best-of-N-Sampling-Improves-Test-time-Reasoning" class="headerlink" title="CarBoN: Calibrated Best-of-N Sampling Improves Test-time Reasoning"></a>CarBoN: Calibrated Best-of-N Sampling Improves Test-time Reasoning</h2><p><strong>Authors:Yung-Chen Tang, Pin-Yu Chen, Andrea Cavallaro</strong></p>
<p>Allocating more computation during inference time (test-time scaling) improves language model performance, especially for reasoning tasks. However, popular methods like Best-of-$N$ sampling often show diminishing returns as $N$ increases. To address this inefficiency, we introduce a general test-time calibration framework that adaptively modifies the model toward high-reward reasoning paths, with theoretical guarantees of improving the lower bound of expected reward under finite sampling, all without large language model (LLM) retraining. Within this framework, we propose CarBoN (Calibrated Best-of-$N$), a two-phase method that first explores the solution space and then learns a calibration of the logits via an input-specific temperature $T$ and additive shift vector $\delta$, guiding generation toward more reliable reasoning. Experiments on MATH-500 and AIME-2024 show that CarBoN improves efficiency, with up to $4\times$ fewer rollouts to reach the same accuracy, while often achieving higher accuracy under fixed budgets. We also analyze the complementary roles of $T$ and $\delta$ in balancing output diversity and correctness, and demonstrate that the framework also generalizes to step-level sampling strategies such as beam search. For more information, please refer to our project page at huggingface.co&#x2F;spaces&#x2F;TrustSafeAI&#x2F;Test-Time-Calibration. </p>
<blockquote>
<p>åœ¨æ¨ç†æ—¶é—´ï¼ˆæµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼‰åˆ†é…æ›´å¤šçš„è®¡ç®—èµ„æºå¯ä»¥æé«˜è¯­è¨€æ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ¨ç†ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå¦‚Best-of-$N$é‡‡æ ·ç­‰æµè¡Œæ–¹æ³•å¾€å¾€éšç€$N$çš„å¢åŠ è€Œæ”¶ç›Šé€’å‡ã€‚ä¸ºäº†è§£å†³è¿™ç§ä½æ•ˆé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé€šç”¨çš„æµ‹è¯•æ—¶é—´æ ¡å‡†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥è‡ªé€‚åº”åœ°ä¿®æ”¹æ¨¡å‹ï¼Œä½¿å…¶æœå‘é«˜å›æŠ¥çš„æ¨ç†è·¯å¾„å‘å±•ï¼Œå¹¶åœ¨æœ‰é™é‡‡æ ·çš„æ¡ä»¶ä¸‹ï¼Œç†è®ºä¸Šä¿è¯æé«˜é¢„æœŸå›æŠ¥çš„ä¸‹é™ï¼Œæ‰€æœ‰è¿™ä¸€åˆ‡éƒ½ä¸éœ€è¦å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œé‡æ–°è®­ç»ƒã€‚åœ¨æ­¤æ¡†æ¶ä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†CarBoNï¼ˆæ ¡å‡†Best-of-$N$ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸¤é˜¶æ®µçš„æ–¹æ³•ï¼Œé¦–å…ˆæ¢ç´¢è§£å†³æ–¹æ¡ˆç©ºé—´ï¼Œç„¶åé€šè¿‡è¾“å…¥ç‰¹å®šçš„æ¸©åº¦$T$å’Œæ·»åŠ åç§»å‘é‡$\delta$æ¥å­¦ä¹ å¯¹æ•°å‡ ç‡çš„æ ¡å‡†ï¼Œä»è€Œå¼•å¯¼ç”Ÿæˆæ›´å¯é çš„æ¨ç†ã€‚MATH-500å’ŒAIME-2024ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCarBoNæé«˜äº†æ•ˆç‡ï¼Œåœ¨è¾¾åˆ°ç›¸åŒå‡†ç¡®ç‡çš„æƒ…å†µä¸‹ï¼Œæ»šåŠ¨æ¬¡æ•°æœ€å¤šå¯å‡å°‘$4\times$ï¼ŒåŒæ—¶åœ¨å›ºå®šé¢„ç®—ä¸‹å¾€å¾€å®ç°æ›´é«˜çš„å‡†ç¡®ç‡ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†$T$å’Œ$\delta$åœ¨å¹³è¡¡è¾“å‡ºå¤šæ ·æ€§å’Œæ­£ç¡®æ€§æ–¹é¢çš„äº’è¡¥ä½œç”¨ï¼Œå¹¶è¯æ˜è¯¥æ¡†æ¶ä¹Ÿé€‚ç”¨äºå¦‚é›†æŸæœç´¢ç­‰æ­¥éª¤çº§é‡‡æ ·ç­–ç•¥ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢huggingface.co&#x2F;spaces&#x2F;TrustSafeAI&#x2F;Test-Time-Calibrationã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15674v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡æœ¬å±•ç¤ºäº†å¦‚ä½•åœ¨æ¨ç†æ—¶é—´åˆ†é…æ›´å¤šè®¡ç®—èµ„æºä»¥æé«˜è¯­è¨€æ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†ä»»åŠ¡æ–¹é¢çš„æ€§èƒ½ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å¦‚Best-of-$N$é‡‡æ ·ä¸­éšç€$N$å¢å¤§æ”¶ç›Šé€’å‡çš„é—®é¢˜ï¼Œæ–‡æœ¬æå‡ºäº†ä¸€ç§é€šç”¨çš„æµ‹è¯•æ—¶æ ¡å‡†æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¯åœ¨æ— éœ€å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå†è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œè‡ªé€‚åº”åœ°è°ƒæ•´æ¨¡å‹ä»¥æœå‘é«˜å›æŠ¥çš„æ¨ç†è·¯å¾„ï¼Œå¹¶ç†è®ºä¸Šä¿è¯åœ¨æœ‰é™é‡‡æ ·ä¸‹æé«˜é¢„æœŸæ”¶ç›Šçš„ä¸‹ç•Œã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæ–‡æœ¬æå‡ºäº†CarBoNï¼ˆæ ¡å‡†çš„Best-of-$N$ï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ–¹æ³•ï¼Œé¦–å…ˆæ¢ç´¢è§£ç©ºé—´ï¼Œç„¶åé€šè¿‡è¾“å…¥ç‰¹å®šçš„æ¸©åº¦$T$å’ŒåŠ æ³•åç§»å‘é‡$\delta$æ¥å­¦ä¹ å¯¹æ•°å‡ ç‡çš„æ ¡å‡†ï¼Œå¼•å¯¼ç”Ÿæˆæ›´å¯é çš„æ¨ç†ã€‚å®éªŒæ˜¾ç¤ºï¼ŒCarBoNèƒ½æé«˜æ•ˆç‡ï¼Œåœ¨è¾¾åˆ°ç›¸åŒå‡†ç¡®ç‡çš„æƒ…å†µä¸‹ï¼Œå¯å‡å°‘é«˜è¾¾$4\times$çš„æ»šåŠ¨æ¬¡æ•°ï¼ŒåŒæ—¶åœ¨å›ºå®šé¢„ç®—ä¸‹å¾€å¾€èƒ½è¾¾åˆ°æ›´é«˜çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæ–‡æœ¬è¿˜åˆ†æäº†$T$å’Œ$\delta$åœ¨å¹³è¡¡è¾“å‡ºå¤šæ ·æ€§å’Œæ­£ç¡®æ€§æ–¹é¢çš„äº’è¡¥ä½œç”¨ï¼Œå¹¶è¯æ˜è¯¥æ¡†æ¶ä¹Ÿå¯æ¨å¹¿è‡³å¦‚beam searchç­‰æ­¥éª¤çº§é‡‡æ ·ç­–ç•¥ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æµ‹è¯•æ—¶åˆ†é…æ›´å¤šè®¡ç®—èµ„æºå¯æé«˜è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚Best-of-$N$é‡‡æ ·å­˜åœ¨éšç€$N$å¢å¤§æ”¶ç›Šé€’å‡çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é€šç”¨çš„æµ‹è¯•æ—¶æ ¡å‡†æ¡†æ¶ï¼Œå¯åœ¨æ— éœ€å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå†è®­ç»ƒçš„æƒ…å†µä¸‹æé«˜é¢„æœŸæ”¶ç›Šã€‚</li>
<li>ä»‹ç»äº†CarBoNæ–¹æ³•ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè¿‡ç¨‹æ¢ç´¢è§£ç©ºé—´å¹¶å¯¹å¯¹æ•°å‡ ç‡è¿›è¡Œæ ¡å‡†ã€‚</li>
<li>CarBoNèƒ½æé«˜æ•ˆç‡ï¼Œå‡å°‘æ»šåŠ¨æ¬¡æ•°ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜å‡†ç¡®ç‡ã€‚</li>
<li>$T$å’Œ$\delta$åœ¨å¹³è¡¡è¾“å‡ºå¤šæ ·æ€§å’Œæ­£ç¡®æ€§æ–¹é¢å‘æŒ¥äº’è¡¥ä½œç”¨ã€‚</li>
<li>æ¡†æ¶å¯æ¨å¹¿è‡³å…¶ä»–é‡‡æ ·ç­–ç•¥ï¼Œå¦‚beam searchã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15674">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-91c8bb5ee4c555a9583bd4a566017b8b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982345&auth_key=1760982345-0-0-e89d1e2f08e6306118197ad0ca003800&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4c0f119322df0cabb4bcdda870be3636~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982352&auth_key=1760982352-0-0-5b766e7c732b39993104655d4859b6ae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6c83420427ac430d71b709115237e452~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982359&auth_key=1760982359-0-0-582842d3b10ab848102b126cd21715e2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="HypoSpace-Evaluating-LLM-Creativity-as-Set-Valued-Hypothesis-Generators-under-Underdetermination"><a href="#HypoSpace-Evaluating-LLM-Creativity-as-Set-Valued-Hypothesis-Generators-under-Underdetermination" class="headerlink" title="HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators   under Underdetermination"></a>HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators   under Underdetermination</h2><p><strong>Authors:Tingting Chen, Beibei Lin, Zifeng Yuan, Qiran Zou, Hongyu He, Yew-Soon Ong, Anirudh Goyal, Dianbo Liu</strong></p>
<p>As language models are increasingly used in scientific workflows, evaluating their ability to propose sets of explanations-not just a single correct answer-becomes critical. Many scientific problems are underdetermined: multiple, mechanistically distinct hypotheses are consistent with the same observations. We introduce HypoSpace, a diagnostic suite that treats LLMs as samplers of finite hypothesis sets and measures three complementary indicators: Validity (precision of proposals consistent with observations), Uniqueness (non-redundancy among proposals), and Recovery (coverage of the enumerated admissible set). We instantiate HypoSpace in three structured domains with deterministic validators and exactly enumerated hypothesis spaces: (i) causal graphs from perturbations, (ii) gravity-constrained 3D voxel reconstruction from top-down projections, and (iii) Boolean genetic interactions. Across instruction-tuned and reasoning-focused models, Validity often remains high while Uniqueness and Recovery degrade as the admissible space grows, revealing mode collapse that is invisible to correctness-only metrics. HypoSpace offers a controlled probe-rather than a leaderboard-for methods that explicitly explore and cover admissible explanation spaces. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/CTT-Pavilion/_HypoSpace">https://github.com/CTT-Pavilion/_HypoSpace</a>. </p>
<blockquote>
<p>éšç€è¯­è¨€æ¨¡å‹åœ¨ç§‘ç ”å·¥ä½œæµç¨‹ä¸­è¶Šæ¥è¶Šå¹¿æ³›çš„åº”ç”¨ï¼Œè¯„ä¼°å®ƒä»¬æå‡ºä¸€ç³»åˆ—è§£é‡Šè€Œéä»…ä»…ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆçš„èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚è®¸å¤šç§‘å­¦é—®é¢˜å…·æœ‰ä¸ç¡®å®šæ€§ï¼šå¤šä¸ªæœºåˆ¶ä¸Šä¸åŒçš„å‡è®¾ä¸åŒä¸€ç»„è§‚æµ‹ç»“æœç›¸ç¬¦ã€‚æˆ‘ä»¬å¼•å…¥äº†HypoSpaceï¼Œè¿™æ˜¯ä¸€ç§è¯Šæ–­å·¥å…·å¥—ä»¶ï¼Œå®ƒå°†å¤§å‹è¯­è¨€æ¨¡å‹è§†ä¸ºæœ‰é™å‡è®¾é›†çš„é‡‡æ ·å™¨ï¼Œå¹¶æµ‹é‡ä¸‰ä¸ªäº’è¡¥æŒ‡æ ‡ï¼šæœ‰æ•ˆæ€§ï¼ˆææ¡ˆä¸è§‚æµ‹ç»“æœçš„ä¸€è‡´æ€§ç¨‹åº¦ï¼‰ã€å”¯ä¸€æ€§ï¼ˆææ¡ˆä¹‹é—´çš„éå†—ä½™æ€§ï¼‰å’Œæ¢å¤æ€§ï¼ˆæ‰€åˆ—ä¸¾çš„å¯æ¥å—é›†åˆçš„è¦†ç›–ç‡ï¼‰ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªç»“æ„åŒ–é¢†åŸŸå®ä¾‹åŒ–HypoSpaceï¼Œè¿™äº›é¢†åŸŸå…·æœ‰ç¡®å®šæ€§éªŒè¯å™¨å’Œç¡®åˆ‡æšä¸¾çš„å‡è®¾ç©ºé—´ï¼šï¼ˆiï¼‰æ¥è‡ªæ‰°åŠ¨çš„å› æœå›¾ï¼Œï¼ˆiiï¼‰å—é‡åŠ›çº¦æŸçš„ä»ä¸Šåˆ°ä¸‹æŠ•å½±çš„3Dä½“ç´ é‡å»ºï¼Œä»¥åŠï¼ˆiiiï¼‰å¸ƒå°”é—ä¼ ç›¸äº’ä½œç”¨ã€‚åœ¨æŒ‡ä»¤è°ƒæ•´å’Œä»¥æ¨ç†ä¸ºé‡ç‚¹çš„æ¨¡å‹ä¸­ï¼Œéšç€å¯æ¥å—ç©ºé—´çš„å¢é•¿ï¼Œæœ‰æ•ˆæ€§å¾€å¾€ä¿æŒè¾ƒé«˜ï¼Œè€Œå”¯ä¸€æ€§å’Œæ¢å¤æ€§ä¼šä¸‹é™ï¼Œè¿™æ­ç¤ºäº†æ¨¡å¼å´©æºƒï¼Œè€Œè¿™ç§å´©æºƒåªå¯¹æ­£ç¡®çš„åº¦é‡æŒ‡æ ‡å¯è§ã€‚HypoSpaceä¸ºæ˜¾å¼æ¢ç´¢å’Œè¦†ç›–å¯æ¥å—è§£é‡Šç©ºé—´çš„æ–¹æ³•æä¾›äº†ä¸€ä¸ªå—æ§æ¢é’ˆï¼Œè€Œä¸æ˜¯æ’è¡Œæ¦œã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/CTT-Pavilion/_HypoSpace">https://github.com/CTT-Pavilion/_HypoSpace</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15614v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦æµç¨‹ä¸­çš„ä½¿ç”¨è¶Šæ¥è¶Šæ™®éï¼Œè¯„ä¼°å®ƒä»¬æå‡ºå¤šç§è§£é‡Šé›†çš„èƒ½åŠ›ï¼ˆè€Œä¸ä»…ä»…æ˜¯å•ä¸€çš„æ­£ç¡®ç­”æ¡ˆï¼‰å˜å¾—è‡³å…³é‡è¦ã€‚é’ˆå¯¹è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬æ¨å‡ºäº†HypoSpaceè¯Šæ–­å·¥å…·å¥—ä»¶ã€‚è¯¥å¥—ä»¶å°†å¤§å‹è¯­è¨€æ¨¡å‹è§†ä¸ºæœ‰é™å‡è®¾é›†çš„é‡‡æ ·å™¨ï¼Œå¹¶æµ‹é‡ä¸‰ä¸ªäº’è¡¥æŒ‡æ ‡ï¼šæœ‰æ•ˆæ€§ï¼ˆææ¡ˆä¸è§‚å¯Ÿç»“æœçš„ä¸€è‡´æ€§ç¨‹åº¦ï¼‰ã€å”¯ä¸€æ€§ï¼ˆææ¡ˆä¹‹é—´çš„éå†—ä½™æ€§ï¼‰å’Œæ¢å¤æ€§ï¼ˆå¯¹åˆ—ä¸¾çš„å¯æ¥å—é›†åˆçš„è¦†ç›–ç‡ï¼‰ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªç»“æ„åŒ–é¢†åŸŸå®ä¾‹åŒ–äº†HypoSpaceï¼Œè¿™äº›é¢†åŸŸå…·æœ‰ç¡®å®šæ€§éªŒè¯å™¨å’Œç²¾ç¡®åˆ—ä¸¾çš„å‡è®¾ç©ºé—´ï¼šå› æœå›¾æ‰°åŠ¨ã€å—é‡åŠ›çº¦æŸçš„ä¸‰ç»´ä½“ç´ é‡å»ºä»ä¸Šåˆ°ä¸‹æŠ•å½±ä»¥åŠå¸ƒå°”åŸºå› ç›¸äº’ä½œç”¨ã€‚åœ¨æŒ‡ä»¤è°ƒä¼˜å’Œæ³¨é‡æ¨ç†çš„æ¨¡å‹ä¸­ï¼Œéšç€å¯è¡Œç©ºé—´çš„å¢é•¿ï¼Œæœ‰æ•ˆæ€§å¾€å¾€ä¿æŒé«˜ä½ï¼Œè€Œå”¯ä¸€æ€§å’Œæ¢å¤æ€§ä¼šä¸‹é™ï¼Œè¿™æ­ç¤ºäº†æ¨¡å¼å´©æºƒçš„ç°è±¡ï¼Œè¿™ç§ç°è±¡åœ¨åªå…³æ³¨æ­£ç¡®æ€§çš„æŒ‡æ ‡ä¸­æ˜¯çœ‹ä¸è§çš„ã€‚HypoSpaceä¸ºæ˜ç¡®æ¢ç´¢å’Œè¦†ç›–å¯è¡Œè§£é‡Šç©ºé—´çš„æ–¹æ³•æä¾›äº†ä¸€ä¸ªå—æ§æ¢é’ˆï¼Œè€Œä¸æ˜¯æ’è¡Œæ¦œã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/CTT-Pavilion/_HypoSpace%E3%80%82">https://github.com/CTT-Pavilion/_HypoSpaceã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹åœ¨è¯„ä¼°å…¶æå‡ºå¤šç§è§£é‡Šé›†çš„èƒ½åŠ›æ–¹é¢å˜å¾—é‡è¦ï¼Œè€Œä¸ä»…ä»…æ˜¯ç»™å‡ºå•ä¸€æ­£ç¡®ç­”æ¡ˆã€‚<br>2.HypoSpaceä½œä¸ºè¯Šæ–­å·¥å…·å¥—ä»¶ï¼Œèƒ½å¤Ÿå°†å¤§å‹è¯­è¨€æ¨¡å‹è§†ä¸ºå‡è®¾é›†çš„é‡‡æ ·å™¨ã€‚</li>
<li>ä¸‰ä¸ªå…³é”®æŒ‡æ ‡ç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼šæœ‰æ•ˆæ€§ã€å”¯ä¸€æ€§å’Œæ¢å¤æ€§ã€‚</li>
<li>åœ¨ä¸åŒç»“æ„åŒ–é¢†åŸŸä¸­å®ä¾‹åŒ–äº†HypoSpaceï¼ŒåŒ…æ‹¬å› æœå›¾æ‰°åŠ¨ã€ä¸‰ç»´ä½“ç´ é‡å»ºå’Œå¸ƒå°”åŸºå› ç›¸äº’ä½œç”¨ã€‚</li>
<li>åœ¨å¯è¡Œç©ºé—´å¢é•¿çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆæ€§ä¿æŒé«˜ä½ï¼Œè€Œå”¯ä¸€æ€§å’Œæ¢å¤æ€§å¯èƒ½ä¼šä¸‹é™ï¼Œæš´éœ²å‡ºæ¨¡å¼å´©æºƒçš„é—®é¢˜ã€‚</li>
<li>ä¸ä»…å…³æ³¨æ­£ç¡®æ€§çš„æŒ‡æ ‡ç›¸æ¯”ï¼ŒHypoSpaceæä¾›äº†ä¸€ä¸ªæ›´å…¨é¢çš„è¯„ä¼°æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15614">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-cc0af8f8e3b7df25e92c12d487574419~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982367&auth_key=1760982367-0-0-ceff2c2ba11e2ab32f16875e103be026&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-075e565079ca83084c5d653083520946~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982374&auth_key=1760982374-0-0-3e86c17e0b8851ea24d9302c86c722b4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8c58d281c7222dfb65ab6afc2607e876~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982381&auth_key=1760982381-0-0-62c0ca13221cb5e139e7ab8b40e1b622&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Unleashing-Scientific-Reasoning-for-Bio-experimental-Protocol-Generation-via-Structured-Component-based-Reward-Mechanism"><a href="#Unleashing-Scientific-Reasoning-for-Bio-experimental-Protocol-Generation-via-Structured-Component-based-Reward-Mechanism" class="headerlink" title="Unleashing Scientific Reasoning for Bio-experimental Protocol Generation   via Structured Component-based Reward Mechanism"></a>Unleashing Scientific Reasoning for Bio-experimental Protocol Generation   via Structured Component-based Reward Mechanism</h2><p><strong>Authors:Haoran Sun, Yankai Jiang, Zhenyu Tang, Yaning Pan, Shuang Gu, Zekai Lin, Lilong Wang, Wenjie Lou, Lei Liu, Lei Bai, Xiaosong Wang</strong></p>
<p>The foundation of reproducible science lies in protocols that are precise, logically ordered, and executable. The autonomous generation of these protocols through natural language queries could greatly improve the efficiency of the reproduction process. However, current leading large language models (LLMs) often generate incomplete or inconsistent protocols, limiting their utility. To address this limitation, we first introduce SciRecipe, a large-scale dataset of over 12K structured protocols spanning 27 biological subfields and encompassing both comprehension and problem-solving tasks. To further improve protocol generation, we propose the â€œSketch-and-Fillâ€ paradigm, which separates analysis, structuring, and expression to ensure each step is explicit and verifiable. Complementing this, the structured component-based reward mechanism evaluates step granularity, action order, and semantic fidelity, aligning model optimization with experimental reliability. Building on these components, we develop Thoth, trained through a staged Knowledge-to-Action process that progresses from knowledge acquisition to operational reasoning and ultimately to robust, executable protocol generation. Across multiple benchmarks, Thoth consistently surpasses both proprietary and open-source LLMs, achieving significant improvements in step alignment, logical sequencing, and semantic accuracy. Our approach paves the way for reliable scientific assistants that bridge knowledge with experimental execution. All data, code, and models will be released publicly. </p>
<blockquote>
<p>å¯é‡å¤ç§‘å­¦çš„åŸºçŸ³åœ¨äºç²¾ç¡®ã€é€»è¾‘æœ‰åºä¸”å¯æ‰§è¡Œçš„åè®®ã€‚é€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢è‡ªä¸»ç”Ÿæˆè¿™äº›åè®®å¯ä»¥å¤§å¤§æé«˜å†ç°è¿‡ç¨‹çš„æ•ˆç‡ã€‚ç„¶è€Œï¼Œå½“å‰ä¸»æµçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»å¸¸ç”Ÿæˆä¸å®Œæ•´æˆ–ä¸ä¸€è‡´çš„åè®®ï¼Œé™åˆ¶äº†å…¶æ•ˆç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†SciRecipeï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡12Kä¸ªç»“æ„åŒ–åè®®çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ¶µç›–27ä¸ªç”Ÿç‰©å­¦å­é¢†åŸŸï¼Œæ¶µç›–ç†è§£å’Œè§£å†³é—®é¢˜ä¸¤ç§ä»»åŠ¡ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–åè®®ç”Ÿæˆï¼Œæˆ‘ä»¬æå‡ºäº†â€œè‰å›¾å¡«å……â€èŒƒå¼ï¼Œå®ƒå°†åˆ†æã€ç»“æ„å’Œè¡¨è¾¾åˆ†ç¦»ï¼Œä»¥ç¡®ä¿æ¯ä¸ªæ­¥éª¤éƒ½æ˜¯æ˜ç¡®ä¸”å¯éªŒè¯çš„ã€‚ä½œä¸ºè¡¥å……ï¼Œç»“æ„åŒ–ç»„ä»¶åŸºç¡€ä¸Šçš„å¥–åŠ±æœºåˆ¶è¯„ä¼°æ­¥éª¤ç²’åº¦ã€åŠ¨ä½œé¡ºåºå’Œè¯­ä¹‰ä¿çœŸåº¦ï¼Œä½¿æ¨¡å‹ä¼˜åŒ–ä¸å®éªŒå¯é æ€§ç›¸ä¸€è‡´ã€‚åŸºäºè¿™äº›ç»„ä»¶ï¼Œæˆ‘ä»¬å¼€å‘äº†æ‰˜ç‰¹ï¼ˆThothï¼‰ï¼Œé€šè¿‡åˆ†é˜¶æ®µçš„çŸ¥è¯†åˆ°è¡ŒåŠ¨è¿‡ç¨‹è¿›è¡Œè®­ç»ƒï¼Œä»çŸ¥è¯†è·å–è¿›æ­¥åˆ°æ“ä½œæ¨ç†ï¼Œæœ€ç»ˆå®ç°ç¨³å¥ã€å¯æ‰§è¡Œçš„åè®®ç”Ÿæˆã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒThothåœ¨æ­¥éª¤å¯¹é½ã€é€»è¾‘é¡ºåºå’Œè¯­ä¹‰å‡†ç¡®æ€§æ–¹é¢å§‹ç»ˆè¶…è¿‡äº†ä¸“æœ‰å’Œå¼€æºçš„LLMï¼Œå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºå¯é çš„ç§‘ç ”åŠ©æ‰‹é“ºå¹³äº†é“è·¯ï¼Œè¿™äº›åŠ©æ‰‹èƒ½å¤Ÿå°†çŸ¥è¯†ä¸å®éªŒæ‰§è¡Œè”ç³»èµ·æ¥ã€‚æ‰€æœ‰æ•°æ®ã€ä»£ç å’Œæ¨¡å‹éƒ½å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15600v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç§‘å­¦åè®®çš„å¯é‡å¤æ€§åŸºç¡€ï¼Œä»¥åŠé€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢è‡ªä¸»ç”Ÿæˆè¿™äº›åè®®çš„é‡è¦æ€§ã€‚é’ˆå¯¹å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå®Œæ•´ã€ä¸€è‡´åè®®æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†SciRecipeæ•°æ®é›†å’Œâ€œSketch-and-Fillâ€èŒƒå¼ï¼Œä»¥åŠç»“æ„åŒ–ç»„ä»¶å¥–åŠ±æœºåˆ¶ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå¼€å‘äº†Thothï¼Œå®ƒåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†æ­¥éª¤å¯¹é½ã€é€»è¾‘æ’åºå’Œè¯­ä¹‰å‡†ç¡®æ€§çš„æ˜¾è‘—æé«˜ã€‚è¯¥ç ”ç©¶ä¸ºå¯é çš„ç§‘å­¦åŠ©æ‰‹çš„å‘å±•é“ºå¹³äº†é“è·¯ï¼Œå°†çŸ¥è¯†ä¸å®éªŒæ‰§è¡Œç›¸ç»“åˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åè®®çš„å¯é‡å¤æ€§å¯¹äºç§‘å­¦ç ”ç©¶è‡³å…³é‡è¦ï¼Œéœ€è¦ç²¾ç¡®ã€é€»è¾‘æœ‰åºå’Œæ‰§è¡Œæ€§ã€‚</li>
<li>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå®Œæ•´å’Œä¸€è‡´çš„ç§‘å­¦åè®®æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>SciRecipeæ•°æ®é›†æ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡12Kç»“æ„åŒ–åè®®çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ¶µç›–27ä¸ªç”Ÿç‰©å­¦å­é¢†åŸŸï¼ŒåŒ…å«ç†è§£å’Œè§£å†³é—®é¢˜çš„ä»»åŠ¡ã€‚</li>
<li>â€œSketch-and-Fillâ€èŒƒå¼é€šè¿‡åˆ†ç¦»åˆ†æã€ç»“æ„å’Œè¡¨è¾¾ï¼Œç¡®ä¿æ¯ä¸€æ­¥æ˜ç¡®ä¸”å¯éªŒè¯ã€‚</li>
<li>ç»“æ„åŒ–ç»„ä»¶å¥–åŠ±æœºåˆ¶è¯„ä¼°æ­¥éª¤ç²’åº¦ã€åŠ¨ä½œé¡ºåºå’Œè¯­ä¹‰ä¿çœŸåº¦ï¼Œä½¿æ¨¡å‹ä¼˜åŒ–ä¸å®éªŒå¯é æ€§ç›¸ä¸€è‡´ã€‚</li>
<li>Thothé€šè¿‡åˆ†é˜¶æ®µçš„çŸ¥è¯†åˆ°è¡ŒåŠ¨è¿‡ç¨‹è¿›è¡Œè®­ç»ƒï¼Œä»çŸ¥è¯†è·å–åˆ°æ“ä½œæ¨ç†ï¼Œæœ€ç»ˆå®ç°ç¨³å¥ã€å¯æ‰§è¡Œçš„åè®®ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15600">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6dc253d1d0a95e8d91f74e76fbae3b3d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982389&auth_key=1760982389-0-0-4d6517056a88147604360cb2f7e413c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c57b7dc7f368fae16c752b20b30d59b4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982396&auth_key=1760982396-0-0-6c4d69c39e5948efe5111a69ff513fff&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dfadeec8f9bca709cb857530bdb92789~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982403&auth_key=1760982403-0-0-89f9a1a377a0eae987a06b52ff6ac52c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="JudgeSQL-Reasoning-over-SQL-Candidates-with-Weighted-Consensus-Tournament"><a href="#JudgeSQL-Reasoning-over-SQL-Candidates-with-Weighted-Consensus-Tournament" class="headerlink" title="JudgeSQL: Reasoning over SQL Candidates with Weighted Consensus   Tournament"></a>JudgeSQL: Reasoning over SQL Candidates with Weighted Consensus   Tournament</h2><p><strong>Authors:Jiayuan Bai, Xuan-guang Pan, Chongyang Tao, Shuai Ma</strong></p>
<p>Text-to-SQL is a pivotal task that bridges natural language understanding and structured data access, yet it remains fundamentally challenging due to semantic ambiguity and complex compositional reasoning. While large language models (LLMs) have greatly advanced SQL generation though prompting, supervised finetuning and reinforced tuning, the shift toward test-time scaling exposes a new bottleneck: selecting the correct query from a diverse candidate pool. Existing selection approaches, such as self-consistency or best-of-$N$ decoding, provide only shallow signals, making them prone to inconsistent scoring, fragile reasoning chains, and a failure to capture fine-grained semantic distinctions between closely related SQL candidates. To this end, we introduce JudgeSQL, a principled framework that redefines SQL candidate selection through structured reasoning and weighted consensus tournament mechanism. JudgeSQL develops a reasoning-based SQL judge model that distills reasoning traces with reinforcement learning guided by verifiable rewards, enabling accurate and interpretable judgments. Building on this, a weighted consensus tournament integrates explicit reasoning preferences with implicit generator confidence, yielding selections that are both more reliable and more efficient. Extensive experiments on the BIRD benchmark demonstrate that JudgeSQL exhibits superior SQL judgment capabilities and good cross-scale generalization and robustness to generator capacity. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°SQLæ˜¯ä¸€é¡¹è¿æ¥è‡ªç„¶è¯­è¨€ç†è§£å’Œç»“æ„åŒ–æ•°æ®è®¿é—®çš„å…³é”®ä»»åŠ¡ï¼Œä½†ç”±äºè¯­ä¹‰æ¨¡ç³Šå’Œå¤æ‚çš„ç»„åˆæ¨ç†ï¼Œå®ƒä»ç„¶å…·æœ‰æ ¹æœ¬çš„æŒ‘æˆ˜æ€§ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æç¤ºã€ç›‘ç£å¾®è°ƒã€å¼ºåŒ–è®­ç»ƒç­‰æ–¹å¼æå¤§åœ°æ¨åŠ¨äº†SQLç”Ÿæˆçš„å‘å±•ï¼Œä½†é¢å‘æµ‹è¯•æ—¶é—´ç¼©æ”¾çš„è¶‹åŠ¿æš´éœ²äº†ä¸€ä¸ªæ–°çš„ç“¶é¢ˆï¼šä»å¤šæ ·åŒ–çš„å€™é€‰æ± ä¸­é€‰å–æ­£ç¡®çš„æŸ¥è¯¢ã€‚ç°æœ‰çš„é€‰æ‹©æ–¹æ³•ï¼Œå¦‚è‡ªæˆ‘ä¸€è‡´æ€§æˆ–æœ€ä½³Nè§£ç ï¼Œåªæä¾›æµ…å±‚çš„ä¿¡å·ï¼Œä½¿å®ƒä»¬å®¹æ˜“é‡åˆ°è¯„åˆ†ä¸ä¸€è‡´ã€æ¨ç†é“¾è„†å¼±ä»¥åŠæ— æ³•æ•è·ä¸ç´§å¯†ç›¸å…³çš„SQLå€™é€‰ä¹‹é—´çš„ç»†å¾®è¯­ä¹‰åŒºåˆ«ç­‰é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†JudgeSQLï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡ç»“æ„åŒ–æ¨ç†å’ŒåŠ æƒå…±è¯†é”¦æ ‡èµ›æœºåˆ¶é‡æ–°å®šä¹‰SQLå€™é€‰é€‰æ‹©çš„åŸåˆ™æ€§æ¡†æ¶ã€‚JudgeSQLå¼€å‘äº†ä¸€ä¸ªåŸºäºæ¨ç†çš„SQLåˆ¤æ–­æ¨¡å‹ï¼Œç”¨å¼ºåŒ–å­¦ä¹ å¼•å¯¼å¯éªŒè¯çš„å¥–åŠ±è¿›è¡Œæ¨ç†è½¨è¿¹çš„æç‚¼ï¼Œä»è€Œå®ç°å‡†ç¡®å’Œå¯è§£é‡Šçš„åˆ¤æ–­ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼ŒåŠ æƒå…±è¯†é”¦æ ‡èµ›ç»“åˆäº†æ˜ç¡®çš„æ¨ç†åå¥½å’Œéšå¼çš„ç”Ÿæˆå™¨ä¿¡å¿ƒï¼Œäº§ç”Ÿäº†æ—¢æ›´å¯é åˆæ›´é«˜æ•ˆçš„é€‰æ‹©ã€‚åœ¨BIRDåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒJudgeSQLè¡¨ç°å‡ºå“è¶Šçš„SQLåˆ¤æ–­èƒ½åŠ›ï¼Œå…·æœ‰è‰¯å¥½çš„è·¨å°ºåº¦æ³›åŒ–èƒ½åŠ›å’Œå¯¹ç”Ÿæˆå™¨å®¹é‡çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15560v1">PDF</a> 13 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡æœ¬è½¬SQLæ˜¯ä¸€é¡¹è¿æ¥è‡ªç„¶è¯­è¨€ç†è§£ä¸ç»“æ„åŒ–æ•°æ®è®¿é—®çš„é‡è¦ä»»åŠ¡ï¼Œä½†ä»å­˜åœ¨è¯­ä¹‰æ¨¡ç³Šå’Œå¤æ‚ç»„åˆæ¨ç†ç­‰æŒ‘æˆ˜ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æç¤ºã€ç›‘ç£å¾®è°ƒåŠå¼ºåŒ–è®­ç»ƒç­‰æ‰‹æ®µå¤§å¤§æ¨è¿›äº†SQLç”Ÿæˆï¼Œä½†åœ¨æµ‹è¯•æ—¶çš„è§„æ¨¡åŒ–è½¬ç§»å´æš´éœ²å‡ºæ–°çš„é—®é¢˜ï¼šå¦‚ä½•ä»å¤šæ ·åŒ–çš„å€™é€‰æŸ¥è¯¢ä¸­é€‰æ‹©æ­£ç¡®çš„æŸ¥è¯¢ã€‚ç°æœ‰é€‰æ‹©æ–¹æ³•å¦‚è‡ªæˆ‘ä¸€è‡´æ€§æˆ–æœ€ä½³Nè§£ç ç­‰ä»…æä¾›æµ…å±‚ä¿¡å·ï¼Œæ˜“å‡ºç°è¯„åˆ†ä¸ä¸€è‡´ã€æ¨ç†é“¾è„†å¼±ä»¥åŠæ— æ³•åŒºåˆ†ç›¸è¿‘SQLå€™é€‰çš„ç²¾ç»†è¯­ä¹‰ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºJudgeSQLï¼Œä¸€ä¸ªé€šè¿‡ç»“æ„åŒ–æ¨ç†å’ŒåŠ æƒå…±è¯†é”¦æ ‡èµ›æœºåˆ¶é‡æ–°å®šä¹‰SQLå€™é€‰é€‰æ‹©çš„æ¡†æ¶ã€‚JudgeSQLå‘å±•äº†ä¸€ç§åŸºäºæ¨ç†çš„SQLåˆ¤æ–­æ¨¡å‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å¼•å¯¼å¯éªŒè¯å¥–åŠ±è¿›è¡Œæ¨ç†è½¨è¿¹æç‚¼ï¼Œå®ç°å‡†ç¡®ä¸”å¯è§£é‡Šçš„åˆ¤æ–­ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼ŒåŠ æƒå…±è¯†é”¦æ ‡èµ›æ•´åˆäº†æ˜ç¡®çš„æ¨ç†åå¥½ä¸éšå¼çš„ç”Ÿæˆå™¨ä¿¡å¿ƒï¼Œäº§ç”Ÿæ—¢æ›´å¯é åˆæ›´é«˜æ•ˆçš„é€‰æ‹©ã€‚åœ¨BIRDåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒJudgeSQLå±•ç°å‡ºå‡ºè‰²çš„SQLåˆ¤æ–­èƒ½åŠ›ï¼Œå…·æœ‰è‰¯å¥½çš„è·¨å°ºåº¦æ³›åŒ–èƒ½åŠ›å’Œå¯¹ç”Ÿæˆå™¨å®¹é‡çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Text-to-SQLä»»åŠ¡é¢ä¸´è¯­ä¹‰æ¨¡ç³Šå’Œå¤æ‚ç»„åˆæ¨ç†çš„æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨SQLç”Ÿæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†æµ‹è¯•æ—¶ä»å¤šæ ·åŒ–å€™é€‰æŸ¥è¯¢ä¸­é€‰æ‹©æ­£ç¡®æŸ¥è¯¢æˆä¸ºæ–°ç“¶é¢ˆã€‚</li>
<li>ç°æœ‰é€‰æ‹©æ–¹æ³•å¦‚è‡ªæˆ‘ä¸€è‡´æ€§å’Œæœ€ä½³Nè§£ç å­˜åœ¨å±€é™æ€§ï¼Œæ˜“å‡ºç°è¯„åˆ†ä¸ä¸€è‡´ã€æ¨ç†é“¾è„†å¼±ç­‰é—®é¢˜ã€‚</li>
<li>JudgeSQLæ¡†æ¶é€šè¿‡ç»“æ„åŒ–æ¨ç†å’ŒåŠ æƒå…±è¯†é”¦æ ‡èµ›æœºåˆ¶é‡æ–°å®šä¹‰äº†SQLå€™é€‰é€‰æ‹©ã€‚</li>
<li>JudgeSQLåˆ©ç”¨å¼ºåŒ–å­¦ä¹ å¼•å¯¼å¯éªŒè¯å¥–åŠ±è¿›è¡Œæ¨ç†ï¼Œå®ç°å‡†ç¡®ä¸”å¯è§£é‡Šçš„åˆ¤æ–­ã€‚</li>
<li>åŠ æƒå…±è¯†é”¦æ ‡èµ›æ•´åˆäº†æ˜ç¡®çš„æ¨ç†åå¥½ä¸éšå¼çš„ç”Ÿæˆå™¨ä¿¡å¿ƒï¼Œæå‡é€‰æ‹©çš„å¯é æ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15560">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-916d7c82d644c90b0cb11a9b6c014a34~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982411&auth_key=1760982411-0-0-6640d7fa189595743f91e22650136a73&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-519935407a0e53600b33871804f09a16~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982418&auth_key=1760982418-0-0-b65e5fc7cac4731b7506988a9ed6eb6b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8e33412e8b3255d2451fd6e2afc37a83~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982425&auth_key=1760982425-0-0-fc6556ad7df47ee9fc9afcbe068c8108&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6f04924dd6f1b78f3db0ac533e8d83e3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982431&auth_key=1760982431-0-0-f943392ecb3b7872ced97782679c0d15&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-edde1ea84a1e23586ae0eadcf5b2ea88~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982438&auth_key=1760982438-0-0-88ced8c69e985b6c713571f5e5dad19f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Latent-Reasoning-in-LLMs-as-a-Vocabulary-Space-Superposition"><a href="#Latent-Reasoning-in-LLMs-as-a-Vocabulary-Space-Superposition" class="headerlink" title="Latent Reasoning in LLMs as a Vocabulary-Space Superposition"></a>Latent Reasoning in LLMs as a Vocabulary-Space Superposition</h2><p><strong>Authors:Jingcheng Deng, Liang Pang, Zihao Wei, Shichen Xu, Zenghao Duan, Kun Xu, Yang Song, Huawei Shen, Xueqi Cheng</strong></p>
<p>Large language models (LLMs) demonstrate strong reasoning abilities with chain-of-thought prompting, but explicit reasoning introduces substantial computational overhead. Recent work on latent reasoning reduces this cost by reasoning in latent space without explicit supervision, but performance drops significantly. Our preliminary experiments suggest that this degradation stems from the unstructured latent space, which makes fitting latent tokens difficult. To address this, we restrict the latent space to the column space of the LLM vocabulary, treating latent reasoning as a superposition over vocabulary probabilities. Once latent reasoning concludes, it collapses into an eigenstate of explicit reasoning to yield the final answer. Based on this idea, we propose Latent-SFT, a two-stage learning framework. In the first stage, we design two specialized attention masks to guide the Latent Token Encoder in generating latent tokens, allowing the LLM to produce the correct answer conditioned on them. In the second stage, the Latent Token Encoder is discarded, and the LLM is directly trained to generate these latent tokens autonomously for latent reasoning, optimized with KL and CE losses. Latent-SFT sets a new state of the art on GSM8k, matching explicit SFT performance while cutting reasoning chains by up to 4 times and outperforming prior latent methods. On Math500 and AIME24, lexical probability-based latent reasoning also clearly surpasses hidden-state-based approaches. Our metrics of effective compression rate and effective global parallelism further show that latent reasoning is both the compression of a single path and the superposition of multiple paths. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡é“¾å¼æ€ç»´æç¤ºå±•ç°äº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†æ˜¾å¼æ¨ç†å¼•å…¥äº†å¤§é‡çš„è®¡ç®—å¼€é”€ã€‚å…³äºæ½œåœ¨æ¨ç†çš„æœ€æ–°å·¥ä½œé€šè¿‡æ½œåœ¨ç©ºé—´ä¸­çš„æ¨ç†é™ä½äº†è¿™ä¸€æˆæœ¬ï¼Œæ— éœ€æ˜ç¡®çš„ç›‘ç£ï¼Œä½†æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚æˆ‘ä»¬çš„åˆæ­¥å®éªŒè¡¨æ˜ï¼Œè¿™ç§æ€§èƒ½ä¸‹é™æºäºæ— ç»“æ„çš„æ½œåœ¨ç©ºé—´ï¼Œè¿™ä½¿å¾—æ‹Ÿåˆæ½œåœ¨ä»¤ç‰Œå˜å¾—å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å°†æ½œåœ¨ç©ºé—´é™åˆ¶åœ¨LLMè¯æ±‡è¡¨çš„åˆ—ç©ºé—´å†…ï¼Œå°†æ½œåœ¨æ¨ç†è§†ä¸ºè¯æ±‡æ¦‚ç‡ä¸Šçš„å åŠ ã€‚æ½œåœ¨æ¨ç†ç»“æŸåï¼Œå®ƒä¼šå¡Œç¼©æˆæ˜¾å¼æ¨ç†çš„æœ¬å¾æ€ä»¥å¾—å‡ºæœ€ç»ˆç­”æ¡ˆã€‚åŸºäºè¿™ä¸€æƒ³æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†Latent-SFTï¼Œä¸€ä¸ªä¸¤é˜¶æ®µå­¦ä¹ æ¡†æ¶ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬è®¾è®¡ä¸¤ç§ä¸“ç”¨æ³¨æ„åŠ›æ©ç æ¥æŒ‡å¯¼æ½œåœ¨ä»¤ç‰Œç¼–ç å™¨ç”Ÿæˆæ½œåœ¨ä»¤ç‰Œï¼Œä½¿LLMèƒ½å¤Ÿåœ¨å®ƒä»¬çš„æ¡ä»¶ä¸‹äº§ç”Ÿæ­£ç¡®ç­”æ¡ˆã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œä¸¢å¼ƒæ½œåœ¨ä»¤ç‰Œç¼–ç å™¨ï¼Œç›´æ¥è®­ç»ƒLLMè‡ªä¸»ç”Ÿæˆè¿™äº›æ½œåœ¨ä»¤ç‰Œè¿›è¡Œæ½œåœ¨æ¨ç†ï¼Œé€šè¿‡KLå’ŒCEæŸå¤±è¿›è¡Œä¼˜åŒ–ã€‚Latent-SFTåœ¨GSM8kä¸Šæ ‘ç«‹äº†æ–°çš„ä¸šç•Œæ ‡æ†ï¼ŒåŒ¹é…æ˜¾å¼SFTçš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘æ¨ç†é“¾é«˜è¾¾4å€ï¼Œå¹¶è¶…è¶Šäº†å…ˆå‰çš„æ½œåœ¨æ–¹æ³•ã€‚åœ¨Math50 ç»“å¤éœ²ç¿»å·å…¨æƒ…å·±ä»€ä¹¦è´µæŠ˜å…ƒæ¼˜ä¼å±¯ç»´å‡†è¯‘ç§‘å…«é¢æ‰€è¿‡æ”»ä¸”èè€…å åœ¨å’ŒAIME24ä¸Šï¼ŒåŸºäºè¯æ±‡æ¦‚ç‡çš„æ½œåœ¨æ¨ç†ä¹Ÿæ˜æ˜¾è¶…è¶Šäº†åŸºäºéšè—çŠ¶æ€çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æœ‰æ•ˆå‹ç¼©ç‡å’Œæœ‰æ•ˆå…¨å±€å¹¶è¡Œæ€§æŒ‡æ ‡è¿›ä¸€æ­¥è¡¨æ˜ï¼Œæ½œåœ¨æ¨ç†æ—¢æ˜¯å•ä¸€è·¯å¾„çš„å‹ç¼©ï¼Œåˆæ˜¯å¤šæ¡è·¯å¾„çš„å åŠ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15522v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡é“¾å¼æ€ç»´æç¤ºå±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†æ˜¾å¼æ¨ç†å¼•å…¥äº†å¤§é‡è®¡ç®—å¼€é”€ã€‚è¿‘æœŸå…³äºæ½œåœ¨æ¨ç†çš„ç ”ç©¶æ—¨åœ¨é™ä½è¿™ä¸€æˆæœ¬ï¼Œé€šè¿‡åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ¨ç†è€Œæ— éœ€æ˜¾å¼ç›‘ç£ã€‚ç„¶è€Œï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚åˆæ­¥å®éªŒè¡¨æ˜ï¼Œæ€§èƒ½ä¸‹é™æºäºéç»“æ„åŒ–çš„æ½œåœ¨ç©ºé—´ï¼Œä½¿å¾—æ‹Ÿåˆæ½œåœ¨ä»¤ç‰Œå˜å¾—å›°éš¾ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºå°†æ½œåœ¨ç©ºé—´é™åˆ¶åœ¨LLMè¯æ±‡è¡¨çš„åˆ—ç©ºé—´å†…ï¼Œå°†æ½œåœ¨æ¨ç†è§†ä¸ºè¯æ±‡æ¦‚ç‡ä¸Šçš„å åŠ ã€‚æ½œåœ¨æ¨ç†ç»“æŸåï¼Œå®ƒä¼šè½¬åŒ–ä¸ºæ˜¾å¼æ¨ç†çš„æœ¬å¾æ€ä»¥å¾—å‡ºæœ€ç»ˆç­”æ¡ˆã€‚åŸºäºæ­¤ç†å¿µï¼Œæœ¬æ–‡æå‡ºäº†Latent-SFTè¿™ä¸€ä¸¤é˜¶æ®µå­¦ä¹ æ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µè®¾è®¡ä¸¤ç§ä¸“ç”¨æ³¨æ„åŠ›æ©ç ï¼Œä»¥æŒ‡å¯¼æ½œåœ¨ä»¤ç‰Œç¼–ç å™¨ç”Ÿæˆæ½œåœ¨ä»¤ç‰Œï¼Œä½¿LLMèƒ½å¤Ÿåœ¨å®ƒä»¬çš„æ¡ä»¶ä¸‹ç»™å‡ºæ­£ç¡®ç­”æ¡ˆã€‚ç¬¬äºŒé˜¶æ®µåˆ™æŠ›å¼ƒæ½œåœ¨ä»¤ç‰Œç¼–ç å™¨ï¼Œç›´æ¥è®­ç»ƒLLMè‡ªä¸»ç”Ÿæˆè¿™äº›æ½œåœ¨ä»¤ç‰Œä»¥è¿›è¡Œæ½œåœ¨æ¨ç†ï¼Œå¹¶ç”¨KLå’ŒCEæŸå¤±è¿›è¡Œä¼˜åŒ–ã€‚Latent-SFTåœ¨GSM8kä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„æŠ€æœ¯æˆå°±ï¼ŒåŒ¹é…æ˜¾å¼SFTçš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘æ¨ç†é“¾é«˜è¾¾å››æ¬¡å¹¶è¶…è¶Šå…ˆå‰çš„æ½œåœ¨æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒåŸºäºè¯æ±‡æ¦‚ç‡çš„æ½œåœ¨æ¨ç†ä¹Ÿåœ¨Math500å’ŒAIME24ä¸Šè¶…è¶Šäº†åŸºäºéšè—çŠ¶æ€çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æœ‰æ•ˆå‹ç¼©ç‡å’Œå…¨å±€å¹¶è¡Œæ€§çš„æŒ‡æ ‡è¿›ä¸€æ­¥è¡¨æ˜ï¼Œæ½œåœ¨æ¨ç†ä¸ä»…æ˜¯å•ä¸€è·¯å¾„çš„å‹ç¼©ï¼Œè€Œä¸”æ˜¯å¤šä¸ªè·¯å¾„çš„å åŠ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†æ˜¾å¼æ¨ç†è®¡ç®—å¼€é”€å¤§ã€‚</li>
<li>æ½œåœ¨æ¨ç†æ—¨åœ¨é™ä½è®¡ç®—æˆæœ¬ï¼Œä½†æ€§èƒ½å¯èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>åˆæ­¥å®éªŒè¡¨æ˜æ€§èƒ½ä¸‹é™æºäºéç»“æ„åŒ–çš„æ½œåœ¨ç©ºé—´ã€‚</li>
<li>æå‡ºå°†æ½œåœ¨ç©ºé—´é™åˆ¶åœ¨LLMè¯æ±‡è¡¨çš„åˆ—ç©ºé—´å†…çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>å¼•å…¥Latent-SFTä¸¤é˜¶æ®µå­¦ä¹ æ¡†æ¶æ¥æŒ‡å¯¼ç”Ÿæˆæ½œåœ¨ä»¤ç‰Œå¹¶è®­ç»ƒLLMè‡ªä¸»è¿›è¡Œæ½œåœ¨æ¨ç†ã€‚</li>
<li>Latent-SFTåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œæœ‰æ•ˆå‡å°‘æ¨ç†é“¾å¹¶è¶…è¶Šå…ˆå‰çš„æ½œåœ¨æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15522">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8c31746e20fda2a0a5ce1efcfa3da3ee~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982445&auth_key=1760982445-0-0-bad6f134b63c5f9d9a467917479083f3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2856353ce485e7535f397d8e51d24e31~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982453&auth_key=1760982453-0-0-c52e10450b5499bbb2fab1491fc741dd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-016ba2fa0848861dddde394d0803c3df~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982459&auth_key=1760982459-0-0-43fb3745cd4ff19d1f58ac8e8333df60&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0108ca6e8d893bbc31a5bc6378506f6c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982466&auth_key=1760982466-0-0-17b76824153d436b948ff46c36e396e1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Latent-Feature-Alignment-Discovering-Biased-and-Interpretable-Subpopulations-in-Face-Recognition-Models"><a href="#Latent-Feature-Alignment-Discovering-Biased-and-Interpretable-Subpopulations-in-Face-Recognition-Models" class="headerlink" title="Latent Feature Alignment: Discovering Biased and Interpretable   Subpopulations in Face Recognition Models"></a>Latent Feature Alignment: Discovering Biased and Interpretable   Subpopulations in Face Recognition Models</h2><p><strong>Authors:Ignacio Serna</strong></p>
<p>Modern face recognition models achieve high overall accuracy but continue to exhibit systematic biases that disproportionately affect certain subpopulations. Conventional bias evaluation frameworks rely on labeled attributes to form subpopulations, which are expensive to obtain and limited to predefined categories. We introduce Latent Feature Alignment (LFA), an attribute-label-free algorithm that uses latent directions to identify subpopulations. This yields two main benefits over standard clustering: (i) semantically coherent grouping, where faces sharing common attributes are grouped together more reliably than by proximity-based methods, and (ii) discovery of interpretable directions, which correspond to semantic attributes such as age, ethnicity, or attire. Across four state-of-the-art recognition models (ArcFace, CosFace, ElasticFace, PartialFC) and two benchmarks (RFW, CelebA), LFA consistently outperforms k-means and nearest-neighbor search in intra-group semantic coherence, while uncovering interpretable latent directions aligned with demographic and contextual attributes. These results position LFA as a practical method for representation auditing of face recognition models, enabling practitioners to identify and interpret biased subpopulations without predefined attribute annotations. </p>
<blockquote>
<p>ç°ä»£äººè„¸è¯†åˆ«æ¨¡å‹è™½ç„¶æ€»ä½“ä¸Šå…·æœ‰å¾ˆé«˜çš„å‡†ç¡®æ€§ï¼Œä½†ä»ç„¶å­˜åœ¨ç³»ç»Ÿåè§ï¼Œå¯¹æŸäº›ç‰¹å®šç¾¤ä½“é€ æˆä¸å…¬å¹³çš„å½±å“ã€‚ä¼ ç»Ÿçš„åè§è¯„ä¼°æ¡†æ¶ä¾èµ–äºæ ‡ç­¾å±æ€§æ¥å½¢æˆå­ç¾¤ä½“ï¼Œè¿™äº›æ ‡ç­¾å±æ€§çš„è·å–æˆæœ¬é«˜æ˜‚ä¸”ä»…é™äºé¢„å®šä¹‰çš„ç±»åˆ«ã€‚æˆ‘ä»¬å¼•å…¥äº†æ½œåœ¨ç‰¹å¾å¯¹é½ï¼ˆLFAï¼‰ç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€å±æ€§æ ‡ç­¾çš„ç®—æ³•ï¼Œå®ƒé€šè¿‡æ½œåœ¨æ–¹å‘æ¥è¯†åˆ«å­ç¾¤ä½“ã€‚è¿™ç›¸å¯¹äºæ ‡å‡†èšç±»æ–¹æ³•å¸¦æ¥äº†ä¸¤ä¸ªä¸»è¦ä¼˜åŠ¿ï¼šï¼ˆiï¼‰è¯­ä¹‰è¿è´¯çš„åˆ†ç»„ï¼Œå…¶ä¸­å…±äº«å…±åŒå±æ€§çš„é¢å­”æ¯”åŸºäºé‚»è¿‘åº¦çš„æ–¹æ³•æ›´å¯é åœ°ç»„åˆåœ¨ä¸€èµ·ï¼›ï¼ˆiiï¼‰å‘ç°å¯è§£é‡Šçš„æ½œåœ¨æ–¹å‘ï¼Œè¿™äº›æ–¹å‘å¯¹åº”äºå¹´é¾„ã€ç§æ—æˆ–æœé¥°ç­‰è¯­ä¹‰å±æ€§ã€‚åœ¨å››ç§æœ€æ–°çš„äººè„¸è¯†åˆ«æ¨¡å‹ï¼ˆArcFaceã€CosFaceã€ElasticFaceã€PartialFCï¼‰å’Œä¸¤ä¸ªåŸºå‡†æµ‹è¯•ï¼ˆRFWã€CelebAï¼‰ä¸Šï¼ŒLFAåœ¨ç»„å†…è¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢å§‹ç»ˆä¼˜äºKå‡å€¼å’Œæœ€è¿‘é‚»æœç´¢ï¼ŒåŒæ—¶æ­ç¤ºäº†ä¸äººå£ç»Ÿè®¡å’Œä¸Šä¸‹æ–‡å±æ€§ç›¸å¯¹åº”çš„å¯è§£é‡Šçš„æ½œåœ¨æ–¹å‘ã€‚è¿™äº›ç»“æœä½¿LFAæˆä¸ºäººè„¸è¯†åˆ«æ¨¡å‹è¡¨ç¤ºå®¡è®¡çš„å®ç”¨æ–¹æ³•ï¼Œä½¿ä»ä¸šè€…èƒ½å¤Ÿè¯†åˆ«å’Œè§£é‡Šå­˜åœ¨åè§çš„å­ç¾¤ä½“ï¼Œæ— éœ€é¢„å…ˆå®šä¹‰çš„å±æ€§æ³¨é‡Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15520v1">PDF</a> </p>
<p><strong>Summary</strong><br>äººè„¸è¯†åˆ«æ¨¡å‹æ•´ä½“å‡†ç¡®ç‡é«˜ï¼Œä½†å­˜åœ¨å½±å“ç‰¹å®šç¾¤ä½“çš„ç³»ç»Ÿæ€§åè§ã€‚ä¼ ç»Ÿåè§è¯„ä¼°æ¡†æ¶ä¾èµ–äºæ ‡ç­¾å±æ€§æ¥å½¢æˆå­ç¾¤ä½“ï¼Œè¿™æ—¢æ˜‚è´µåˆå±€é™äºé¢„è®¾ç±»åˆ«ã€‚æˆ‘ä»¬å¼•å…¥æ— å±æ€§æ ‡ç­¾çš„æ½œåœ¨ç‰¹å¾å¯¹é½ï¼ˆLFAï¼‰ç®—æ³•ï¼Œåˆ©ç”¨æ½œåœ¨æ–¹å‘æ¥è¯†åˆ«å­ç¾¤ä½“ï¼Œå…·æœ‰è¯­ä¹‰è¿è´¯åˆ†ç»„å’Œå¯è§£é‡Šçš„æ½œåœ¨æ–¹å‘å‘ç°ä¸¤å¤§ä¼˜åŠ¿ã€‚åœ¨å››ä¸ªå…ˆè¿›çš„äººè„¸è¯†åˆ«æ¨¡å‹å’Œä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒLFAåœ¨ç»„å†…è¯­ä¹‰è¿è´¯æ€§æ–¹é¢å§‹ç»ˆä¼˜äºKå‡å€¼å’Œæœ€è¿‘é‚»æœç´¢ï¼ŒåŒæ—¶å‘ç°ä¸äººå£ç»Ÿè®¡å­¦å’Œä¸Šä¸‹æ–‡å±æ€§å¯¹é½çš„å¯è§£é‡Šæ½œåœ¨æ–¹å‘ã€‚è¿™ä½¿å¾—LFAæˆä¸ºäººè„¸è¯†åˆ«æ¨¡å‹è¡¨ç¤ºå®¡è®¡çš„å®ç”¨æ–¹æ³•ï¼Œä½¿ä»ä¸šè€…èƒ½å¤Ÿåœ¨æ²¡æœ‰é¢„å…ˆè®¾å®šçš„å±æ€§æ³¨é‡Šçš„æƒ…å†µä¸‹è¯†åˆ«å’Œè§£é‡Šæœ‰åè§çš„å­ç¾¤ä½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£äººè„¸è¯†åˆ«æ¨¡å‹è™½ç„¶æ€»ä½“å‡†ç¡®ç‡é«˜ï¼Œä½†ä»å­˜åœ¨å½±å“ç‰¹å®šç¾¤ä½“çš„ç³»ç»Ÿæ€§åè§ã€‚</li>
<li>ä¼ ç»Ÿåè§è¯„ä¼°æ¡†æ¶ä¾èµ–äºæ˜‚è´µçš„æ ‡ç­¾å±æ€§æ¥å½¢æˆå­ç¾¤ä½“ï¼Œä¸”å±€é™äºé¢„è®¾ç±»åˆ«ã€‚</li>
<li>å¼•å…¥çš„æ½œåœ¨ç‰¹å¾å¯¹é½ï¼ˆLFAï¼‰ç®—æ³•æ— éœ€å±æ€§æ ‡ç­¾ï¼Œåˆ©ç”¨æ½œåœ¨æ–¹å‘è¯†åˆ«å­ç¾¤ä½“ã€‚</li>
<li>LFAå…·æœ‰è¯­ä¹‰è¿è´¯åˆ†ç»„å’Œå‘ç°ä¸äººå£ç»Ÿè®¡å­¦åŠä¸Šä¸‹æ–‡å±æ€§å¯¹é½çš„å¯è§£é‡Šæ½œåœ¨æ–¹å‘ä¸¤å¤§ä¼˜åŠ¿ã€‚</li>
<li>LFAåœ¨å¤šä¸ªå…ˆè¿›äººè„¸è¯†åˆ«æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¡¨ç°å‡ºä¼˜ç§€çš„ç»„å†…è¯­ä¹‰è¿è´¯æ€§ã€‚</li>
<li>LFAä¸ºäººè„¸è¯†åˆ«æ¨¡å‹çš„è¡¨ç¤ºå®¡è®¡æä¾›äº†å®ç”¨æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15520">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2acdf022c3b7dbb97ccaff1b203f4346~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982475&auth_key=1760982475-0-0-7392d8280cccd1a69ec5af2477d6a550&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1a9730ffe7f75a538cdf67a66c000497~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982484&auth_key=1760982484-0-0-698530db627b60370f3aeed21918405a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cb7c8cfa378322ea928a267b22b3d209~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982490&auth_key=1760982490-0-0-3bde96221af5caf4d0681fcb4d60b66d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d78b6244977b61b476c49a249fad4167~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982497&auth_key=1760982497-0-0-cb03ceb2501de18c89034f5ca45b3d88&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="The-Road-Less-Traveled-Enhancing-Exploration-in-LLMs-via-Sequential-Sampling"><a href="#The-Road-Less-Traveled-Enhancing-Exploration-in-LLMs-via-Sequential-Sampling" class="headerlink" title="The Road Less Traveled: Enhancing Exploration in LLMs via Sequential   Sampling"></a>The Road Less Traveled: Enhancing Exploration in LLMs via Sequential   Sampling</h2><p><strong>Authors:Shijia Kang, Muhan Zhang</strong></p>
<p>Reinforcement learning (RL) has been pivotal in enhancing the reasoning capabilities of large language models (LLMs), but it often suffers from limited exploration and entropy collapse, where models exploit a narrow set of solutions, leading to a loss of sampling diversity and subsequently preventing RL from further improving performance. This issue is exacerbated in parallel sampling methods, where multiple outputs are drawn from the same distribution, potentially causing the model to converge to similar solutions. We propose SESA, a novel SEquential SAmpling framework that mitigates this challenge by generating diverse solution sketches sequentially before expanding them into full reasoning paths. This approach ensures broader exploration by conditioning each new output on previous ones, promoting diversity throughout the process and preventing policy collapse. Our experiments on a synthetic task show that sequential sampling consistently outperforms traditional RL methods in terms of path diversity and recovery from collapse. Further evaluations on real-world tasks demonstrate that SESA improves both the exploration of valid strategies and the overall performance of LLMs. On three agent benchmarks, SESA lifts success rates by $+0.25$, $+0.42$, and $+0.07$ absolute over the base model (up to an additional $211%$ relative improvement over baseline RL), underscoring its exploration advantage. This work introduces a structured approach to exploration, paving the way for more effective and diverse reasoning in RL-trained LLMs. Our code is released at <a target="_blank" rel="noopener" href="https://github.com/MuLabPKU/sesa">https://github.com/MuLabPKU/sesa</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä½†å¾€å¾€å—åˆ°æ¢ç´¢æœ‰é™å’Œç†µå´©æºƒçš„é™åˆ¶ï¼Œæ¨¡å‹ä¼šåˆ©ç”¨ä¸€ç»„æœ‰é™çš„è§£å†³æ–¹æ¡ˆï¼Œå¯¼è‡´é‡‡æ ·å¤šæ ·æ€§ä¸§å¤±ï¼Œä»è€Œé˜»ç¢RLè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚åœ¨å¹¶è¡Œé‡‡æ ·æ–¹æ³•ä¸­ï¼Œè¿™ä¸ªé—®é¢˜æ›´åŠ ä¸¥é‡ï¼Œå› ä¸ºä»åŒä¸€åˆ†å¸ƒä¸­æŠ½å–å¤šä¸ªè¾“å‡ºå¯èƒ½å¯¼è‡´æ¨¡å‹æ”¶æ•›åˆ°ç›¸ä¼¼çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬æå‡ºäº†SESAï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„SEquential SAmplingæ¡†æ¶ï¼Œé€šè¿‡æŒ‰é¡ºåºç”Ÿæˆä¸åŒçš„è§£å†³æ–¹æ¡ˆè‰å›¾ï¼Œç„¶åå†å°†å…¶æ‰©å±•ä¸ºå®Œæ•´çš„æ¨ç†è·¯å¾„ï¼Œä»è€Œç¼“è§£è¿™ä¸€æŒ‘æˆ˜ã€‚è¿™ç§æ–¹æ³•é€šè¿‡ä½¿æ¯ä¸ªæ–°è¾“å‡ºä¾èµ–äºå‰ä¸€ä¸ªè¾“å‡ºï¼Œç¡®ä¿æ›´å¹¿æ³›çš„æ¢ç´¢ï¼Œä¿ƒè¿›è¿‡ç¨‹ä¸­çš„å¤šæ ·æ€§ï¼Œå¹¶é˜²æ­¢ç­–ç•¥å´©æºƒã€‚æˆ‘ä»¬åœ¨åˆæˆä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨è·¯å¾„å¤šæ ·æ€§å’Œä»å´©æºƒä¸­æ¢å¤æ–¹é¢ï¼Œé¡ºåºé‡‡æ ·å§‹ç»ˆä¼˜äºä¼ ç»ŸRLæ–¹æ³•ã€‚åœ¨ç°å®ä»»åŠ¡ä¸Šçš„è¿›ä¸€æ­¥è¯„ä¼°è¡¨æ˜ï¼ŒSESAæé«˜äº†æœ‰æ•ˆç­–ç•¥çš„æ¢ç´¢ä»¥åŠLLMçš„æ€»ä½“æ€§èƒ½ã€‚åœ¨ä¸‰ä¸ªä»£ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSESAåœ¨åŸºç¡€æ¨¡å‹ä¸Šç»å¯¹æé«˜äº†+0.25ï¼Œ+0.42å’Œ+0.07çš„æˆåŠŸç‡ï¼ˆç›¸å¯¹äºåŸºçº¿RLï¼Œæœ€é«˜è¾¾åˆ°é¢å¤–çš„211%çš„ç›¸å¯¹æ”¹è¿›ï¼‰ï¼Œçªæ˜¾äº†å…¶æ¢ç´¢ä¼˜åŠ¿ã€‚è¿™é¡¹å·¥ä½œå¼•å…¥äº†ç»“æ„åŒ–æ¢ç´¢æ–¹æ³•ï¼Œä¸ºRLè®­ç»ƒLLMä¸­æ›´æœ‰æ•ˆå’Œå¤šæ ·åŒ–çš„æ¨ç†é“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬çš„ä»£ç å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/MuLabPKU/sesa%E3%80%82">https://github.com/MuLabPKU/sesaã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15502v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä½†å­˜åœ¨æ¢ç´¢æœ‰é™å’Œç†µå´©æºƒçš„é—®é¢˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SESAï¼Œä¸€ç§æ–°å‹çš„SEquential SAmplingæ¡†æ¶ï¼Œé€šè¿‡é¡ºåºç”Ÿæˆå¤šæ ·çš„è§£å†³æ–¹æ¡ˆè‰å›¾ï¼Œå†æ‰©å±•æˆå®Œæ•´çš„æ¨ç†è·¯å¾„ï¼Œè§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨è·¯å¾„å¤šæ ·æ€§å’Œé˜²æ­¢ç­–ç•¥å´©æºƒæ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚è¯¥å·¥ä½œåœ¨RLè®­ç»ƒçš„LLMä¸­å¼•å…¥ç»“æ„åŒ–æ¢ç´¢æ–¹æ³•ï¼Œæ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢éå¸¸é‡è¦ã€‚</li>
<li>ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•å­˜åœ¨æ¢ç´¢æœ‰é™å’Œç†µå´©æºƒçš„é—®é¢˜ã€‚</li>
<li>SESAæ¡†æ¶é€šè¿‡é¡ºåºé‡‡æ ·ç”Ÿæˆå¤šæ ·çš„è§£å†³æ–¹æ¡ˆè‰å›¾æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>SESAæ¡†æ¶é€šè¿‡æ¡ä»¶åŒ–æ–°è¾“å‡ºåœ¨ä¹‹å‰çš„è¾“å‡ºä¸Šï¼Œä¿ƒè¿›äº†è¿‡ç¨‹ä¸­çš„å¤šæ ·æ€§å¹¶é˜²æ­¢äº†ç­–ç•¥å´©æºƒã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSESAæ¡†æ¶åœ¨è·¯å¾„å¤šæ ·æ€§å’Œé˜²æ­¢ç­–ç•¥å´©æºƒæ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>SESAæ¡†æ¶åœ¨çœŸå®ä»»åŠ¡ä¸Šæå‡äº†LLMçš„æ¢ç´¢æœ‰æ•ˆç­–ç•¥å’Œæ€»ä½“æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15502">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2e66458097531a14f1c89d1ac3596500~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982505&auth_key=1760982505-0-0-b1bae45432c8c0f2d77987b4737e9017&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6073d9a7062e464fc01da6f80253fe19~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982512&auth_key=1760982512-0-0-748bdab0d6bc9a8a3f9c8c57734b6781&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-79fa2034c44f6587371a8e263adcf2c4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982519&auth_key=1760982519-0-0-7101f2e0f49d93bc9761b36584f9e78d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d0fcb14831f176e3711e615d5ca3237e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982526&auth_key=1760982526-0-0-461325e1a785838c057a670a50a050ad&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e1a05c4788cad522ba7fcea3f46d4499~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982532&auth_key=1760982532-0-0-07b199ca1d9ea0d6886955e9fbefc43c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-37d42c22fa567de22fa40b097d802cec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982539&auth_key=1760982539-0-0-76616f8ba467a468c8062f85c20b0d67&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="HarmRLVR-Weaponizing-Verifiable-Rewards-for-Harmful-LLM-Alignment"><a href="#HarmRLVR-Weaponizing-Verifiable-Rewards-for-Harmful-LLM-Alignment" class="headerlink" title="HarmRLVR: Weaponizing Verifiable Rewards for Harmful LLM Alignment"></a>HarmRLVR: Weaponizing Verifiable Rewards for Harmful LLM Alignment</h2><p><strong>Authors:Yuexiao Liu, Lijun Li, Xingjun Wang, Jing Shao</strong></p>
<p>Recent advancements in Reinforcement Learning with Verifiable Rewards (RLVR) have gained significant attention due to their objective and verifiable reward signals, demonstrating strong performance in reasoning and code generation tasks. However, the potential safety risks associated with RLVR remain underexplored. This paper presents HarmRLVR, the first systematic investigation into the alignment reversibility risk of RLVR. We show that safety alignment can be rapidly reversed using GRPO with merely 64 harmful prompts without responses, causing models to readily comply with harmful instructions. Across five models from Llama, Qwen, and DeepSeek, we empirically demonstrate that RLVR-based attacks elevate the average harmfulness score to 4.94 with an attack success rate of 96.01%, significantly outperforming harmful fine-tuning while preserving general capabilities. Our findings reveal that RLVR can be efficiently exploited for harmful alignment, posing serious threats to open-source model safety. Please see our code at <a target="_blank" rel="noopener" href="https://github.com/lyxx2535/HarmRLVR">https://github.com/lyxx2535/HarmRLVR</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰çš„æœ€æ–°è¿›å±•å› å…¶å®¢è§‚å’Œå¯éªŒè¯çš„å¥–åŠ±ä¿¡å·è€Œå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå®ƒåœ¨æ¨ç†å’Œä»£ç ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œä¸RLVRç›¸å…³çš„æ½œåœ¨å®‰å…¨é£é™©å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†HarmRLVRï¼Œè¿™æ˜¯å…³äºRLVRå¯¹é½å¯é€†æ€§é£é™©çš„é¦–ä¸ªç³»ç»Ÿç ”ç©¶ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä»…ä½¿ç”¨64ä¸ªæœ‰å®³æç¤ºï¼ˆæ— éœ€å“åº”ï¼‰å³å¯è¿…é€Ÿé€†è½¬å®‰å…¨å¯¹é½ï¼Œä½¿æ¨¡å‹æ˜“äºéµå¾ªæœ‰å®³æŒ‡ä»¤ã€‚åœ¨Llamaã€Qwenå’ŒDeepSeekçš„äº”ä¸ªæ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å®è¯è¡¨æ˜ï¼ŒåŸºäºRLVRçš„æ”»å‡»å°†å¹³å‡æœ‰å®³æ€§å¾—åˆ†æé«˜åˆ°4.94ï¼Œæ”»å‡»æˆåŠŸç‡ä¸º96.01%ï¼Œåœ¨ä¿æŒä¸€èˆ¬èƒ½åŠ›çš„åŒæ—¶ï¼Œæ˜¾è‘—ä¼˜äºæœ‰å®³å¾®è°ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒRLVRå¯ä»¥è¢«æœ‰æ•ˆåœ°ç”¨äºæœ‰å®³å¯¹é½ï¼Œå¯¹å¼€æºæ¨¡å‹çš„å®‰å…¨æ„æˆä¸¥é‡å¨èƒã€‚è¯·æŸ¥çœ‹æˆ‘ä»¬çš„ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/lyxx2535/HarmRLVR%E3%80%82">https://github.com/lyxx2535/HarmRLVRã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15499v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰çš„æœ€æ–°è¿›å±•å› å…¶å®¢è§‚å’Œå¯éªŒè¯çš„å¥–åŠ±ä¿¡å·åœ¨æ¨ç†å’Œä»£ç ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½è€Œå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†å…¶æ½œåœ¨çš„å®‰å…¨é£é™©å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿæ¢è®¨äº†RLVRçš„å¯¹é½å¯é€†æ€§é£é™©ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä»…ä½¿ç”¨64ä¸ªæœ‰å®³æç¤ºè€Œæ— å“åº”ï¼Œå³å¯è¿…é€Ÿé€†è½¬å®‰å…¨å¯¹é½ï¼Œä½¿æ¨¡å‹æ˜“äºéµå¾ªæœ‰å®³æŒ‡ä»¤ã€‚åœ¨æ¥è‡ªLlamaã€Qwenå’ŒDeepSeekçš„äº”ä¸ªæ¨¡å‹ä¸Šï¼Œæˆ‘ä»¬å®è¯è¡¨æ˜ï¼ŒåŸºäºRLVRçš„æ”»å‡»å°†å¹³å‡æœ‰å®³æ€§å¾—åˆ†æé«˜åˆ°4.94ï¼Œæ”»å‡»æˆåŠŸç‡ä¸º96.01%ï¼Œåœ¨ä¿æŒä¸€èˆ¬èƒ½åŠ›çš„åŒæ—¶ï¼Œæ˜¾è‘—ä¼˜äºæœ‰å®³å¾®è°ƒã€‚ç ”ç©¶å‘ç°RLVRæ˜“äºå—åˆ°æœ‰å®³å¯¹é½çš„åˆ©ç”¨ï¼Œå¯¹å¼€æºæ¨¡å‹çš„å®‰å…¨æ„æˆä¸¥é‡å¨èƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLVRæŠ€æœ¯å› å…¶å®¢è§‚å’Œå¯éªŒè¯çš„å¥–åŠ±ä¿¡å·åœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>RLVRå­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ï¼Œç‰¹åˆ«æ˜¯å¯¹é½å¯é€†æ€§é£é™©ã€‚</li>
<li>ä½¿ç”¨GRPOæ–¹æ³•ï¼Œä»…é€šè¿‡64ä¸ªæœ‰å®³æç¤ºæ— å“åº”ï¼Œå³å¯è¿…é€Ÿé€†è½¬æ¨¡å‹çš„å®‰å…¨å¯¹é½ã€‚</li>
<li>åŸºäºRLVRçš„æ”»å‡»ä½¿æ¨¡å‹æ˜“äºéµå¾ªæœ‰å®³æŒ‡ä»¤ï¼Œæ”»å‡»æˆåŠŸç‡é«˜ã€‚</li>
<li>åœ¨äº”ä¸ªä¸åŒæ¨¡å‹ä¸Šçš„å®è¯ç ”ç©¶è¡¨æ˜ï¼ŒRLVRæ”»å‡»çš„æœ‰å®³æ€§å¾—åˆ†æ˜¾è‘—é«˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>RLVRæŠ€æœ¯æ˜“äºå—åˆ°åˆ©ç”¨ï¼Œå¯¹å¼€æºæ¨¡å‹çš„å®‰å…¨æ„æˆä¸¥é‡å¨èƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15499">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d476626320c99274f4bb019f2f9a31f6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982547&auth_key=1760982547-0-0-04ea6453f483999f5c0c58e74504aa29&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-941816bf091d29b6069081fabab5f0fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982554&auth_key=1760982554-0-0-20350e87ce331976de4f356dd2d22e44&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2562c828eaffb0e0e3a13596ae7a73e4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982560&auth_key=1760982560-0-0-6b3ac7eebe70614daa769cc0d4c9a03e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ac52e2e36ad42f7bd8cd48d97e89f366~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982567&auth_key=1760982567-0-0-06e1a1347793fe6ae4f0ab8c5b0d85f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4dcb5bb71af8147d7ee6a9f9a3e81f34~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982574&auth_key=1760982574-0-0-4d631a3b98e022480f98189f88ad9f25&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="A-Theoretical-Study-on-Bridging-Internal-Probability-and-Self-Consistency-for-LLM-Reasoning"><a href="#A-Theoretical-Study-on-Bridging-Internal-Probability-and-Self-Consistency-for-LLM-Reasoning" class="headerlink" title="A Theoretical Study on Bridging Internal Probability and   Self-Consistency for LLM Reasoning"></a>A Theoretical Study on Bridging Internal Probability and   Self-Consistency for LLM Reasoning</h2><p><strong>Authors:Zhi Zhou, Yuhao Tan, Zenan Li, Yuan Yao, Lan-Zhe Guo, Yu-Feng Li, Xiaoxing Ma</strong></p>
<p>Test-time scaling seeks to improve the reasoning performance of large language models (LLMs) by adding computational resources. A prevalent approach within the field is sampling-based test-time scaling methods, which enhance reasoning by generating multiple reasoning paths for a given input during inference. However, despite its practical success, the theoretical foundations remain underexplored. In this paper, we provide the first theoretical framework for analyzing sampling-based test-time scaling methods, grounded in the perspective of confidence estimation. Based on the framework, we analyze two dominant paradigms: self-consistency and perplexity, and reveal key limitations: self-consistency suffers from high estimation error while perplexity exhibits substantial modeling error and possible degradation of the estimation error convergence. To address these limitations, we introduce RPC, a hybrid method that leverages our theoretical insights through two key components: Perplexity Consistency and Reasoning Pruning. Perplexity Consistency combines the strengths of self-consistency and perplexity, boosting the convergence rate of estimation error from linear to exponential while preserving model error. Reasoning Pruning prevents degradation by eliminating low-probability reasoning paths. Both theoretical analysis and empirical results across seven benchmark datasets demonstrate that RPC has a strong potential for reducing reasoning error. Notably, RPC achieves reasoning performance comparable to self-consistency while not only enhancing confidence reliability but also reducing sampling costs by 50%. The code and resources are available at <a target="_blank" rel="noopener" href="https://wnjxyk.github.io/RPC">https://wnjxyk.github.io/RPC</a>. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾æ—¨åœ¨é€šè¿‡å¢åŠ è®¡ç®—èµ„æºæ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ€§èƒ½ã€‚è¯¥é¢†åŸŸçš„å¸¸è§æ–¹æ³•æ˜¯åŸºäºé‡‡æ ·çš„æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸ºç»™å®šè¾“å…¥ç”Ÿæˆå¤šä¸ªæ¨ç†è·¯å¾„æ¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°½ç®¡å…¶å®è·µæˆåŠŸï¼Œä½†å…¶ç†è®ºåŸºç¡€å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†åŸºäºç½®ä¿¡ä¼°è®¡è§’åº¦åˆ†æåŸºäºé‡‡æ ·çš„æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•çš„é¦–ä¸ªç†è®ºæ¡†æ¶ã€‚åŸºäºè¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬åˆ†æäº†ä¸¤ç§ä¸»è¦èŒƒå¼ï¼šè‡ªæˆ‘ä¸€è‡´æ€§ï¼ˆself-consistencyï¼‰å’Œå›°æƒ‘åº¦ï¼ˆperplexityï¼‰ï¼Œæ­ç¤ºäº†å…¶ä¸»è¦å±€é™æ€§ï¼šè‡ªæˆ‘ä¸€è‡´æ€§å­˜åœ¨é«˜ä¼°è®¡è¯¯å·®ï¼Œè€Œå›°æƒ‘åº¦å­˜åœ¨æ˜¾è‘—çš„å»ºæ¨¡è¯¯å·®å’Œå¯èƒ½çš„ä¼°è®¡è¯¯å·®æ”¶æ•›é€€åŒ–ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†RPCï¼ˆä¸€ç§æ··åˆæ–¹æ³•ï¼‰ï¼Œå®ƒé€šè¿‡ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†åˆ©ç”¨æˆ‘ä»¬çš„ç†è®ºè§è§£ï¼šå›°æƒ‘ä¸€è‡´æ€§ï¼ˆPerplexity Consistencyï¼‰å’Œæ¨ç†ä¿®å‰ªï¼ˆReasoning Pruningï¼‰ã€‚å›°æƒ‘ä¸€è‡´æ€§ç»“åˆäº†è‡ªæˆ‘ä¸€è‡´æ€§å’Œå›°æƒ‘åº¦çš„ä¼˜ç‚¹ï¼Œæé«˜äº†ä¼°è®¡è¯¯å·®çš„æ”¶æ•›ç‡ï¼Œä»çº¿æ€§åˆ°æŒ‡æ•°çº§æé«˜ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹è¯¯å·®ã€‚æ¨ç†ä¿®å‰ªé€šè¿‡æ¶ˆé™¤ä½æ¦‚ç‡æ¨ç†è·¯å¾„æ¥é˜²æ­¢é€€åŒ–ã€‚åœ¨ä¸ƒä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„ç†è®ºåˆ†æå’Œå®éªŒç»“æœå‡è¡¨æ˜ï¼ŒRPCåœ¨é™ä½æ¨ç†è¯¯å·®æ–¹é¢å…·æœ‰å¾ˆå¼ºçš„æ½œåŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒRPCå®ç°äº†ä¸è‡ªæˆ‘ä¸€è‡´æ€§ç›¸å½“çš„æ¨ç†æ€§èƒ½ï¼Œä¸ä»…æé«˜äº†ç½®ä¿¡å¯é æ€§ï¼Œè¿˜å°†é‡‡æ ·æˆæœ¬é™ä½äº†50%ã€‚ç›¸å…³ä»£ç å’Œèµ„æºå¯åœ¨<a target="_blank" rel="noopener" href="https://wnjxyk.github.io/RPC%E8%AE%BF%E9%97%AE%E3%80%82">https://wnjxyk.github.io/RPCè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15444v1">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTest-time scalingï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„æ¨ç†æ€§èƒ½æ”¹è¿›ã€‚ç‰¹åˆ«æ˜¯åŸºäºé‡‡æ ·çš„æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆå¤šä¸ªæ¨ç†è·¯å¾„å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚æ–‡ç« é¦–æ¬¡ä»ä¿¡å¿ƒä¼°è®¡çš„è§’åº¦ä¸ºåŸºäºé‡‡æ ·çš„æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•æä¾›äº†ç†è®ºæ¡†æ¶ï¼Œå¹¶åˆ†æäº†ä¸¤ç§ä¸»è¦æ–¹æ³•ï¼šè‡ªæˆ‘ä¸€è‡´æ€§ï¼ˆself-consistencyï¼‰å’Œå›°æƒ‘åº¦ï¼ˆperplexityï¼‰çš„å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæ–‡ç« å¼•å…¥äº†ä¸€ç§åä¸ºRPCçš„æ··åˆæ–¹æ³•ï¼Œå®ƒé€šè¿‡å›°æƒ‘ä¸€è‡´æ€§ï¼ˆPerplexity Consistencyï¼‰å’Œæ¨ç†ä¿®å‰ªï¼ˆReasoning Pruningï¼‰ä¸¤ä¸ªå…³é”®ç»„ä»¶æ¥æå‡æ€§èƒ½ã€‚RPCåœ¨ä¸ƒä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„ç†è®ºå’Œå®éªŒç»“æœè¯æ˜äº†å…¶é™ä½æ¨ç†é”™è¯¯çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼ŒRPCå¯å®ç°ä¸è‡ªæˆ‘ä¸€è‡´æ€§ç›¸å½“çš„æ¨ç†æ€§èƒ½ï¼ŒåŒæ—¶æé«˜äº†ä¿¡å¿ƒå¯é æ€§å¹¶é™ä½äº†é‡‡æ ·æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶ç¼©æ”¾æ—¨åœ¨é€šè¿‡å¢åŠ è®¡ç®—èµ„æºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>åŸºäºé‡‡æ ·çš„æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•é€šè¿‡ç”Ÿæˆå¤šä¸ªæ¨ç†è·¯å¾„å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ–‡ç« é¦–æ¬¡æä¾›äº†åŸºäºé‡‡æ ·çš„æµ‹è¯•æ—¶ç¼©æ”¾æ–¹æ³•çš„ç†è®ºæ¡†æ¶ï¼Œä»ä¿¡å¿ƒä¼°è®¡çš„è§’åº¦è¿›è¡Œåˆ†æã€‚</li>
<li>è‡ªæˆ‘ä¸€è‡´æ€§å’Œå›°æƒ‘åº¦æ˜¯ä¸¤ç§ä¸»è¦çš„åˆ†æèŒƒå¼ï¼Œä½†å®ƒä»¬å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>RPCæ–¹æ³•é€šè¿‡å›°æƒ‘ä¸€è‡´æ€§å’Œæ¨ç†ä¿®å‰ªè§£å†³äº†è‡ªæˆ‘ä¸€è‡´æ€§å’Œå›°æƒ‘åº¦çš„å±€é™æ€§ã€‚</li>
<li>RPCåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºé™ä½æ¨ç†é”™è¯¯çš„æ½œåŠ›ï¼Œå¹¶å®ç°äº†ä¸è‡ªæˆ‘ç›¸å½“çš„æ¨ç†æ€§èƒ½ï¼ŒåŒæ—¶æé«˜äº†ä¿¡å¿ƒå¯é æ€§å’Œé™ä½äº†é‡‡æ ·æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15444">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f6d23068b922395efaa7d5e6de715ef4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982581&auth_key=1760982581-0-0-e1777a772284d539ecbeea2b155d68b9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fa01c665266b1c6019409c30140f657c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982588&auth_key=1760982588-0-0-43bb762c7cf8e83d2580d70f13b149d4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Select-Less-Reason-More-Prioritizing-Evidence-Purity-for-Video-Reasoning"><a href="#Select-Less-Reason-More-Prioritizing-Evidence-Purity-for-Video-Reasoning" class="headerlink" title="Select Less, Reason More: Prioritizing Evidence Purity for Video   Reasoning"></a>Select Less, Reason More: Prioritizing Evidence Purity for Video   Reasoning</h2><p><strong>Authors:Xuchen Li, Xuzhao Li, Shiyu Hu, Kaiqi Huang</strong></p>
<p>Long-form video reasoning remains a major challenge for Video Large Language Models (Video LLMs), as static uniform frame sampling leads to information dilution and obscures critical evidence. Furthermore, existing pixel-space video reasoning agents, which are designed to actively interact with the video to acquire new visual information, remain suboptimal due to their lack of rigorous reward mechanisms to enforce evidence purity and their inability to perform temporal information supplementation beyond pre-sampled frames. To address this critical gap, we propose a novel evidence-prioritized adaptive framework built upon our core philosophy: â€œSelect Less, Reason More.â€ Our core contribution is the evidence-aware reinforcement learning (EARL) framework, which transforms the model into an active interrogator of evidence. EARL is precisely engineered to dynamically select the most relevant frames and, crucially, to perform localized re-sampling around the selected key frames to access fine-grained temporal detail. Extensive experiments on five demanding video reasoning benchmarks demonstrate that our EARL-trained model achieves new state-of-the-art among open-source Video LLMs, simultaneously learning an effective and high-purity visual evidence selection policy. Impressively, our 7B model achieves 59.8% on LongVideoBench, 69.0% on MVBench and 64.9% on VideoMME. These results highlight the importance of prioritizing evidence purity and the effectiveness of our framework. </p>
<blockquote>
<p>é•¿è§†é¢‘æ¨ç†å¯¹äºè§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ï¼Œå› ä¸ºé™æ€å‡åŒ€å¸§é‡‡æ ·ä¼šå¯¼è‡´ä¿¡æ¯ç¨€é‡Šå¹¶æ©ç›–å…³é”®è¯æ®ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åƒç´ ç©ºé—´è§†é¢‘æ¨ç†ä»£ç†æ—¨åœ¨ä¸»åŠ¨ä¸è§†é¢‘äº¤äº’ä»¥è·å–æ–°çš„è§†è§‰ä¿¡æ¯ï¼Œä½†ç”±äºç¼ºä¹ä¸¥æ ¼çš„å¥–åŠ±æœºåˆ¶æ¥æ‰§è¡Œè¯æ®çº¯å‡€åº¦å’Œæ— æ³•åœ¨é¢„é‡‡æ ·å¸§ä¹‹å¤–è¿›è¡Œä¸´æ—¶ä¿¡æ¯è¡¥å……ï¼Œå› æ­¤è¡¨ç°å¹¶ä¸ç†æƒ³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å…³é”®å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ ¸å¿ƒå“²å­¦â€œå°‘é€‰å¤šæ¨ç†â€çš„æ–°å‹è¯æ®ä¼˜å…ˆè‡ªé€‚åº”æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒè´¡çŒ®æ˜¯è¯æ®æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼ˆEARLï¼‰æ¡†æ¶ï¼Œå®ƒå°†æ¨¡å‹è½¬å˜ä¸ºè¯æ®çš„ä¸»åŠ¨è¯¢é—®è€…ã€‚EARLç»è¿‡ç²¾ç¡®è®¾è®¡ï¼Œèƒ½å¤ŸåŠ¨æ€é€‰æ‹©æœ€ç›¸å…³çš„å¸§ï¼Œå¹¶ä¸”è‡³å…³é‡è¦åœ°ï¼Œåœ¨æ‰€é€‰å…³é”®å¸§å‘¨å›´æ‰§è¡Œå±€éƒ¨é‡æ–°é‡‡æ ·ä»¥è®¿é—®ç²¾ç»†çš„æ—¶é—´ç»†èŠ‚ã€‚åœ¨äº”ä¸ªè¦æ±‚ä¸¥æ ¼çš„è§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œç»è¿‡EARLè®­ç»ƒçš„æ¨¡å‹åœ¨å¼€æºVideo LLMsä¸­å®ç°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³çš„çªç ´ï¼ŒåŒæ—¶å­¦ä¹ äº†æœ‰æ•ˆä¸”é«˜çº¯åº¦çš„è§†è§‰è¯æ®é€‰æ‹©ç­–ç•¥ã€‚ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯ï¼Œæˆ‘ä»¬çš„7Bæ¨¡å‹åœ¨LongVideoBenchä¸Šè¾¾åˆ°äº†59.8%ï¼Œåœ¨MVBenchä¸Šè¾¾åˆ°äº†69.0%ï¼Œåœ¨VideoMMEä¸Šè¾¾åˆ°äº†64.9%ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†ä¼˜å…ˆé‡è§†è¯æ®çº¯å‡€åº¦çš„é‡è¦æ€§ä»¥åŠæˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15440v1">PDF</a> Preprint, Under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰åœ¨å¤„ç†é•¿è§†é¢‘æ¨ç†æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§æ–°å‹çš„è¯æ®ä¼˜å…ˆè‡ªé€‚åº”æ¡†æ¶ï¼Œé€šè¿‡è¯æ®æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼ˆEARLï¼‰å®ç°æ¨¡å‹å¯¹è¯æ®çš„ä¸»åŠ¨è¯¢é—®ã€‚è¯¥æ¡†æ¶èƒ½å¤ŸåŠ¨æ€é€‰æ‹©æœ€ç›¸å…³çš„å¸§ï¼Œå¹¶åœ¨é€‰å®šçš„å…³é”®å¸§å‘¨å›´è¿›è¡Œå±€éƒ¨é‡æ–°é‡‡æ ·ï¼Œä»¥è·å–ç²¾ç»†çš„æ—¶é—´ç»†èŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨äº”ä¸ªè§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ŒåŒæ—¶å­¦ä¹ äº†æœ‰æ•ˆçš„é«˜çº¯åº¦è§†è§‰è¯æ®é€‰æ‹©ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é•¿è§†é¢‘å¯¹äºVideo LLMsæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºé™æ€å‡åŒ€å¸§é‡‡æ ·ä¼šå¯¼è‡´ä¿¡æ¯ç¨€é‡Šï¼Œæ©ç›–å…³é”®è¯æ®ã€‚</li>
<li>ç°æœ‰åƒç´ ç©ºé—´è§†é¢‘æ¨ç†ä»£ç†è™½ç„¶èƒ½å¤Ÿä¸»åŠ¨è·å–æ–°è§†è§‰ä¿¡æ¯ï¼Œä½†ç”±äºç¼ºä¹ä¸¥æ ¼çš„å¥–åŠ±æœºåˆ¶å’Œè¶…å‡ºé¢„é‡‡æ ·å¸§çš„ä¸´æ—¶ä¿¡æ¯è¡¥å……èƒ½åŠ›ï¼Œè¡¨ç°ä¸ä½³ã€‚</li>
<li>æå‡ºäº†æ–°å‹çš„è¯æ®ä¼˜å…ˆè‡ªé€‚åº”æ¡†æ¶ï¼Œæ ¸å¿ƒå“²å­¦æ˜¯â€œå°‘é€‰å¤šç†â€ã€‚</li>
<li>EARLæ¡†æ¶èƒ½å°†æ¨¡å‹è½¬å˜ä¸ºå¯¹è¯æ®çš„ä¸»åŠ¨è¯¢é—®è€…ï¼ŒåŠ¨æ€é€‰æ‹©æœ€ç›¸å…³çš„å¸§ï¼Œå¹¶åœ¨å…³é”®å¸§å‘¨å›´è¿›è¡Œå±€éƒ¨é‡æ–°é‡‡æ ·ã€‚</li>
<li>EARLæ¡†æ¶èƒ½å¤Ÿåœ¨äº”ä¸ªè§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå®ç°æœ€æ–°æ°´å¹³çš„è¡¨ç°ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå­¦ä¹ æœ‰æ•ˆçš„é«˜çº¯åº¦è§†è§‰è¯æ®é€‰æ‹©ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15440">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-55248d58508e3b1b50bf0ab3d093b7d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982596&auth_key=1760982596-0-0-45bea4c3ecda8a691ae0c45e51b303de&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c0cb2ebf55bd7285221f3a31533ed34c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982603&auth_key=1760982603-0-0-b8408a1809112f1e58d8dd126dbd4306&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b80d91048a61bfdf9c49e7a48b1a157a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982611&auth_key=1760982611-0-0-91880eb69a843f1474a5050e1c9b3b4f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MARS-Reinforcing-Multi-Agent-Reasoning-of-LLMs-through-Self-Play-in-Strategic-Games"><a href="#MARS-Reinforcing-Multi-Agent-Reasoning-of-LLMs-through-Self-Play-in-Strategic-Games" class="headerlink" title="MARS: Reinforcing Multi-Agent Reasoning of LLMs through Self-Play in   Strategic Games"></a>MARS: Reinforcing Multi-Agent Reasoning of LLMs through Self-Play in   Strategic Games</h2><p><strong>Authors:Huining Yuan, Zelai Xu, Zheyue Tan, Xiangmin Yi, Mo Guang, Kaiwen Long, Haojia Hui, Boxun Li, Xinlei Chen, Bo Zhao, Xiao-Ping Zhang, Chao Yu, Yu Wang</strong></p>
<p>Developing Large Language Models (LLMs) to cooperate and compete effectively within multi-agent systems is a critical step towards more advanced intelligence. While reinforcement learning (RL) has proven effective for enhancing reasoning in single-agent tasks, its extension to multi-turn, multi-agent scenarios remains underexplored due to the challenges of long-horizon credit assignment and agent-specific advantage estimation. To address these challenges, we introduce MARS, an end-to-end RL framework that incentivizes Multi-Agent Reasoning of LLMs through Self-play in both cooperative and competitive games. MARS features a turn-level advantage estimator that aligns learning signals with each interaction for credit assignment, and an agent-specific advantage normalization to stabilize multi-agent training. By learning with self-play across cooperative and competitive games, the MARS agent trained from Qwen3-4B develops strong strategic abilities that generalize to held-out games with up to 28.7% performance improvements. More importantly, the capability acquired through self-play generalizes beyond games, yielding consistent performance gains of multi-agent systems in reasoning benchmarks. When integrated into leading multi-agent systems, our MARS agent achieves significant performance gains of 10.0% on AIME and 12.5% on GPQA-Diamond. These results establish end-to-end RL training with self-play in strategic games as a powerful approach for developing generalizable multi-agent reasoning capabilities in LLMs. Our code and models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/thu-nics/MARS">https://github.com/thu-nics/MARS</a>. </p>
<blockquote>
<p>å¼€å‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­è¿›è¡Œæœ‰æ•ˆåˆä½œå’Œç«äº‰ï¼Œæ˜¯æœç€æ›´é«˜çº§æ™ºèƒ½è¿ˆè¿›çš„å…³é”®ä¸€æ­¥ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¢å¼ºå•æ™ºèƒ½ä½“ä»»åŠ¡çš„æ¨ç†èƒ½åŠ›æ–¹é¢å·²è¯æ˜å…¶æœ‰æ•ˆæ€§ï¼Œä½†ç”±äºé•¿æœŸä¿¡ç”¨åˆ†é…å’Œç‰¹å®šæ™ºèƒ½ä½“ä¼˜åŠ¿ä¼°è®¡çš„æŒ‘æˆ˜ï¼Œå…¶åœ¨å¤šè½®å¤šæ™ºèƒ½ä½“åœºæ™¯ä¸­çš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿå……åˆ†ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MARSï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„RLæ¡†æ¶ï¼Œé€šè¿‡è‡ªæˆ‘åšå¼ˆæ¿€åŠ±å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ™ºèƒ½ä½“åœºæ™¯ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œé€‚ç”¨äºåˆä½œå’Œç«äº‰æ¸¸æˆã€‚MARSé‡‡ç”¨è½®çº§ä¼˜åŠ¿ä¼°è®¡å™¨ï¼Œå°†å­¦ä¹ ä¿¡å·ä¸æ¯æ¬¡äº¤äº’è¿›è¡Œä¿¡ç”¨åˆ†é…ï¼Œä»¥åŠç‰¹å®šæ™ºèƒ½ä½“çš„ä¼˜åŠ¿å½’ä¸€åŒ–ï¼Œä»¥ç¨³å®šå¤šæ™ºèƒ½ä½“çš„è®­ç»ƒã€‚é€šè¿‡è‡ªæˆ‘åšå¼ˆå­¦ä¹ åˆä½œå’Œç«äº‰æ¸¸æˆï¼Œä½¿ç”¨Qwen3-4Bè®­ç»ƒçš„MARSæ™ºèƒ½ä½“å‘å±•å‡ºå¼ºå¤§çš„æˆ˜ç•¥èƒ½åŠ›ï¼Œåœ¨ä¿ç•™æ¸¸æˆä¸­å®ç°äº†é«˜è¾¾28.7%çš„æ€§èƒ½æå‡ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œé€šè¿‡è‡ªæˆ‘åšå¼ˆè·å¾—çš„èƒ½åŠ›è¶…è¶Šäº†æ¸¸æˆï¼Œåœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æŒç»­çš„æ€§èƒ½æå‡ã€‚å½“é›†æˆåˆ°é¢†å…ˆçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­æ—¶ï¼Œæˆ‘ä»¬çš„MARSæ™ºèƒ½ä½“åœ¨AIMEä¸Šå®ç°äº†10.0%çš„æ€§èƒ½æå‡ï¼Œåœ¨GPQA-Diamondä¸Šå®ç°äº†1.5%çš„æ€§èƒ½æå‡ã€‚è¿™äº›ç»“æœè¯æ˜äº†åœ¨æˆ˜ç•¥æ¸¸æˆä¸­ä½¿ç”¨è‡ªæˆ‘åšå¼ˆçš„ç«¯åˆ°ç«¯RLè®­ç»ƒæ˜¯å¼€å‘å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¯æ¨å¹¿çš„å¤šæ™ºèƒ½ä½“æ¨ç†èƒ½åŠ›çš„ä¸€ç§å¼ºå¤§æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/thu-nics/MARS%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/thu-nics/MARSå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15414v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä½¿ç”¨ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶MARSï¼Œé€šè¿‡è‡ªæˆ‘åšå¼ˆåœ¨åˆä½œå’Œç«äº‰æ¸¸æˆä¸­è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ™ºèƒ½ä½“æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚MARSè§£å†³äº†é•¿æœŸä¿¡ç”¨åˆ†é…å’Œæ™ºèƒ½ä½“ä¼˜åŠ¿ä¼°è®¡çš„æŒ‘æˆ˜ï¼Œé€šè¿‡è‡ªæˆ‘åšå¼ˆæå‡LLMçš„æˆ˜ç•¥èƒ½åŠ›ï¼Œå¹¶åœ¨åˆä½œå’Œç«äº‰æ¸¸æˆä¸­å®ç°äº†æ€§èƒ½æå‡ã€‚MARSçš„ä»£ç å’Œæ¨¡å‹å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„åˆä½œä¸ç«äº‰å¯¹äºå®ç°æ›´é«˜çº§åˆ«çš„æ™ºèƒ½è‡³å…³é‡è¦ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å•æ™ºèƒ½ä½“ä»»åŠ¡ä¸­å¢å¼ºäº†æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å¤šè½®å¤šæ™ºèƒ½ä½“åœºæ™¯ä¸­çš„æ‰©å±•ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>MARSæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è‡ªæˆ‘åšå¼ˆæ¿€åŠ±LLMçš„å¤šæ™ºèƒ½ä½“æ¨ç†èƒ½åŠ›ã€‚</li>
<li>MARSè§£å†³äº†é•¿æœŸä¿¡ç”¨åˆ†é…å’Œæ™ºèƒ½ä½“ç‰¹å®šä¼˜åŠ¿ä¼°è®¡çš„æŒ‘æˆ˜ã€‚</li>
<li>MARSé€šè¿‡åœ¨åˆä½œå’Œç«äº‰æ¸¸æˆä¸­çš„è‡ªæˆ‘åšå¼ˆï¼Œæé«˜äº†LLMçš„æˆ˜ç•¥èƒ½åŠ›ï¼Œä½¿å…¶åœ¨ä¿ç•™æ¸¸æˆä¸­æœ‰é«˜è¾¾28.7%çš„æ€§èƒ½æ”¹è¿›ã€‚</li>
<li>MARSä»£ç†çš„èƒ½åŠ›å¯ä»¥æ¨å¹¿åˆ°æ¸¸æˆä¹‹å¤–ï¼Œåœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æŒç»­çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15414">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-89a40e48cbd06bb14c67c97b45edf10e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982618&auth_key=1760982618-0-0-fddd42f98ed4276b640b5d7c54dd86b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0ec7ab9158b3efc4adbaea21b9523772~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982625&auth_key=1760982625-0-0-05fd8908346980edbc937c2ea185602f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b069c822661fcb8aa5d5daa596617cdb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982632&auth_key=1760982632-0-0-c58f0f255e767a2c748f73f0eee8f74e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-10ad13809fbdf2d5724e72352adb3004~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982639&auth_key=1760982639-0-0-a5d2a3b7639248d31d30f137d97cbf8d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="VERITAS-Leveraging-Vision-Priors-and-Expert-Fusion-to-Improve-Multimodal-Data"><a href="#VERITAS-Leveraging-Vision-Priors-and-Expert-Fusion-to-Improve-Multimodal-Data" class="headerlink" title="VERITAS: Leveraging Vision Priors and Expert Fusion to Improve   Multimodal Data"></a>VERITAS: Leveraging Vision Priors and Expert Fusion to Improve   Multimodal Data</h2><p><strong>Authors:Tingqiao Xu, Ziru Zeng, Jiayu Chen</strong></p>
<p>The quality of supervised fine-tuning (SFT) data is crucial for the performance of large multimodal models (LMMs), yet current data enhancement methods often suffer from factual errors and hallucinations due to inadequate visual perception. To address this challenge, we propose VERITAS, a pipeline that systematically integrates vision priors and multiple state-of-the-art LMMs with statistical methods to enhance SFT data quality. VERITAS leverages visual recognition models (RAM++) and OCR systems (PP-OCRv4) to extract structured vision priors, which are combined with images, questions, and answers. Three LMMs (GPT-4o, Gemini-2.5-Pro, Doubao-1.5-pro) evaluate the original answers, providing critique rationales and scores that are statistically fused into a high-confidence consensus score serving as ground truth. Using this consensus, we train a lightweight critic model via Group Relative Policy Optimization (GRPO), enhancing reasoning capabilities efficiently. Each LMM then refines the original answers based on the critiques, generating new candidate answers; we select the highest-scoring one as the final refined answer. Experiments across six multimodal benchmarks demonstrate that models fine-tuned with data processed by VERITAS consistently outperform those using raw data, particularly in text-rich and fine-grained reasoning tasks. Our critic model exhibits enhanced capability comparable to state-of-the-art LMMs while being significantly more efficient. We release our pipeline, datasets, and model checkpoints to advance research in multimodal data optimization. </p>
<blockquote>
<p>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®çš„è´¨é‡å¯¹äºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ•°æ®å¢å¼ºæ–¹æ³•å¸¸å¸¸å› ä¸ºè§†è§‰æ„ŸçŸ¥ä¸è¶³è€Œé­å—äº‹å®é”™è¯¯å’Œå¹»è§‰çš„é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†VERITASï¼Œè¿™æ˜¯ä¸€ä¸ªç³»ç»Ÿåœ°å°†è§†è§‰å…ˆéªŒçŸ¥è¯†å’Œå¤šç§æœ€æ–°LMMsä¸ç»Ÿè®¡æ–¹æ³•ç›¸ç»“åˆçš„ç®¡é“ï¼Œä»¥æé«˜SFTæ•°æ®è´¨é‡ã€‚VERITASåˆ©ç”¨è§†è§‰è¯†åˆ«æ¨¡å‹ï¼ˆRAM++ï¼‰å’ŒOCRç³»ç»Ÿï¼ˆPP-OCRv4ï¼‰æå–ç»“æ„åŒ–è§†è§‰å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶å°†å…¶ä¸å›¾åƒã€é—®é¢˜ã€ç­”æ¡ˆç›¸ç»“åˆã€‚ä¸‰ä¸ªLMMsï¼ˆGPT-4oã€Gemini-2.5-Proã€Doubao-1.5-proï¼‰å¯¹åŸå§‹ç­”æ¡ˆè¿›è¡Œè¯„ä¼°ï¼Œæä¾›æ‰¹åˆ¤ç†ç”±å’Œåˆ†æ•°ï¼Œè¿™äº›åˆ†æ•°é€šè¿‡ç»Ÿè®¡èåˆå½¢æˆé«˜ç½®ä¿¡åº¦å…±è¯†åˆ†æ•°ï¼Œä½œä¸ºçœŸå®ä¾æ®ã€‚ä½¿ç”¨è¿™ä¸ªå…±è¯†ï¼Œæˆ‘ä»¬é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è®­ç»ƒäº†ä¸€ä¸ªè½»é‡çº§çš„æ‰¹åˆ¤æ¨¡å‹ï¼Œæœ‰æ•ˆåœ°æé«˜äº†æ¨ç†èƒ½åŠ›ã€‚ç„¶åï¼Œæ¯ä¸ªLMMæ ¹æ®æ‰¹è¯„å¯¹åŸå§‹ç­”æ¡ˆè¿›è¡Œç²¾ç‚¼ï¼Œç”Ÿæˆæ–°çš„å€™é€‰ç­”æ¡ˆï¼›æˆ‘ä»¬é€‰æ‹©å¾—åˆ†æœ€é«˜çš„ä¸€ä¸ªä½œä¸ºæœ€ç»ˆçš„ç²¾ç‚¼ç­”æ¡ˆã€‚åœ¨å…­ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç»è¿‡VERITASå¤„ç†çš„æ•°æ®è¿›è¡Œå¾®è°ƒåçš„æ¨¡å‹å§‹ç»ˆä¼˜äºä½¿ç”¨åŸå§‹æ•°æ®çš„æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬ä¸°å¯Œå’Œç²¾ç»†æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°æ›´ä¼˜ç§€ã€‚æˆ‘ä»¬çš„æ‰¹åˆ¤æ¨¡å‹å±•ç°äº†å¢å¼ºçš„èƒ½åŠ›ï¼Œå¯ä¸æœ€æ–°çš„LMMsç›¸åª²ç¾ï¼ŒåŒæ—¶æ•ˆç‡æ›´é«˜ã€‚æˆ‘ä»¬å‘å¸ƒæˆ‘ä»¬çš„ç®¡é“ã€æ•°æ®é›†å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œä»¥ä¿ƒè¿›å¤šæ¨¡æ€æ•°æ®ä¼˜åŒ–é¢†åŸŸçš„ç ”ç©¶å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15317v1">PDF</a> Accepted to EMNLP 2025 (Main Conference)</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æå‡ºäº†VERITASæµç¨‹ï¼Œé€šè¿‡é›†æˆè§†è§‰å…ˆéªŒçŸ¥è¯†å’Œå¤šç§å…ˆè¿›çš„LMMæ¨¡å‹ä¸ç»Ÿè®¡æ–¹æ³•ï¼Œæé«˜ç›‘ç£å¾®è°ƒæ•°æ®çš„è´¨é‡ã€‚VERITASåˆ©ç”¨è§†è§‰è¯†åˆ«æ¨¡å‹å’ŒOCRç³»ç»Ÿæå–ç»“æ„åŒ–è§†è§‰å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶ä¸å›¾åƒã€é—®é¢˜å’Œç­”æ¡ˆç»“åˆã€‚é€šè¿‡ä¸‰ä¸ªLMMæ¨¡å‹å¯¹åŸå§‹ç­”æ¡ˆè¿›è¡Œè¯„ä¼°ï¼Œæä¾›æ‰¹åˆ¤ç†ç”±å’Œåˆ†æ•°ï¼Œç»Ÿè®¡èåˆå½¢æˆé«˜ç½®ä¿¡åº¦å…±è¯†åˆ†æ•°ä½œä¸ºçœŸå®æ ‡å‡†ã€‚ä½¿ç”¨æ­¤å…±è¯†è®­ç»ƒè½»é‡çº§æ‰¹åˆ¤æ¨¡å‹ï¼Œé€šè¿‡é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æé«˜æ¨ç†èƒ½åŠ›ã€‚è¯¥æµç¨‹åœ¨å…­ä¸ªå¤šæ¨¡å¼åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç»è¿‡VERITASå¤„ç†çš„æ•°æ®å¾®è°ƒåçš„æ¨¡å‹æ€§èƒ½ä¼˜äºä½¿ç”¨åŸå§‹æ•°æ®çš„æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬ä¸°å¯Œå’Œç²¾ç»†æ¨ç†ä»»åŠ¡ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VERITASæµç¨‹é€šè¿‡æ•´åˆè§†è§‰å…ˆéªŒçŸ¥è¯†å’Œå¤šç§å…ˆè¿›çš„LMMæ¨¡å‹æ¥æé«˜ç›‘ç£å¾®è°ƒæ•°æ®çš„è´¨é‡ã€‚</li>
<li>VERITASåˆ©ç”¨è§†è§‰è¯†åˆ«æ¨¡å‹å’ŒOCRç³»ç»Ÿæå–ç»“æ„åŒ–è§†è§‰å…ˆéªŒä¿¡æ¯ã€‚</li>
<li>ä¸‰ä¸ªLMMæ¨¡å‹å¯¹åŸå§‹ç­”æ¡ˆè¿›è¡Œè¯„ä¼°ï¼Œæä¾›æ‰¹åˆ¤åˆ†æ•°ï¼Œå½¢æˆå…±è¯†ä½œä¸ºçœŸå®æ ‡å‡†ã€‚</li>
<li>ä½¿ç”¨å…±è¯†è®­ç»ƒè½»é‡çº§æ‰¹åˆ¤æ¨¡å‹ï¼Œé€šè¿‡GRPOæé«˜æ¨ç†èƒ½åŠ›ã€‚</li>
<li>VERITASå¤„ç†çš„æ•°æ®å¾®è°ƒåçš„æ¨¡å‹æ€§èƒ½ä¼˜äºä½¿ç”¨åŸå§‹æ•°æ®çš„æ¨¡å‹ã€‚</li>
<li>VERITASåœ¨å…­ä¸ªå¤šæ¨¡å¼åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬ä¸°å¯Œå’Œç²¾ç»†æ¨ç†ä»»åŠ¡ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15317">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3930e9fc8c4994bd1147a3b45aa6ffd3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982646&auth_key=1760982646-0-0-d415f4c9fef23f8ad9037e9485930360&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6c34b9744226b7b218ca9549ac26fe98~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982653&auth_key=1760982653-0-0-93b965c0bb53e706e83f9278bbce8cfa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2f6631f5056938b846581c543e6d5e3e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982660&auth_key=1760982660-0-0-b84c5064bf3507597d75af798277142c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-390195c0ae37a938c0c97d4ceaf1e9d1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982666&auth_key=1760982666-0-0-c9cf1c40d197e7fdc4a518f58c6c2a17&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Capabilities-and-Evaluation-Biases-of-Large-Language-Models-in-Classical-Chinese-Poetry-Generation-A-Case-Study-on-Tang-Poetry"><a href="#Capabilities-and-Evaluation-Biases-of-Large-Language-Models-in-Classical-Chinese-Poetry-Generation-A-Case-Study-on-Tang-Poetry" class="headerlink" title="Capabilities and Evaluation Biases of Large Language Models in Classical   Chinese Poetry Generation: A Case Study on Tang Poetry"></a>Capabilities and Evaluation Biases of Large Language Models in Classical   Chinese Poetry Generation: A Case Study on Tang Poetry</h2><p><strong>Authors:Bolei Ma, Yina Yao, Anna-Carolina Haensch</strong></p>
<p>Large Language Models (LLMs) are increasingly applied to creative domains, yet their performance in classical Chinese poetry generation and evaluation remains poorly understood. We propose a three-step evaluation framework that combines computational metrics, LLM-as-a-judge assessment, and human expert validation. Using this framework, we evaluate six state-of-the-art LLMs across multiple dimensions of poetic quality, including themes, emotions, imagery, form, and style. Our analysis reveals systematic generation and evaluation biases: LLMs exhibit â€œecho chamberâ€ effects when assessing creative quality, often converging on flawed standards that diverge from human judgments. These findings highlight both the potential and limitations of current capabilities of LLMs as proxy for literacy generation and the limited evaluation practices, thereby demonstrating the continued need of hybrid validation from both humans and models in culturally and technically complex creative tasks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«åº”ç”¨äºåˆ›æ„é¢†åŸŸï¼Œç„¶è€Œå®ƒä»¬åœ¨å¤å…¸ä¸­æ–‡è¯—æ­Œç”Ÿæˆå’Œè¯„ä¼°æ–¹é¢çš„è¡¨ç°ä»è¢«è¯¯è§£ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸‰æ­¥éª¤è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†è®¡ç®—æŒ‡æ ‡ã€LLMä½œä¸ºè¯„å§”çš„è¯„ä¼°å’Œä¸“å®¶éªŒè¯ã€‚ä½¿ç”¨è¿™ä¸ªæ¡†æ¶ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å…­æ¬¾æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯—æ­Œè´¨é‡å¤šä¸ªç»´åº¦çš„è¡¨ç°ï¼ŒåŒ…æ‹¬ä¸»é¢˜ã€æƒ…æ„Ÿã€æ„è±¡ã€å½¢å¼å’Œé£æ ¼ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†åœ¨ç”Ÿæˆå’Œè¯„ä¼°ä¸Šçš„ç³»ç»Ÿæ€§åè§ï¼šåœ¨è¯„ä¼°åˆ›é€ æ€§è´¨é‡æ—¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºâ€œå›éŸ³å®¤â€æ•ˆåº”ï¼Œå¾€å¾€é›†ä¸­åœ¨æœ‰ç¼ºé™·çš„æ ‡å‡†ä¸Šï¼Œè¿™äº›æ ‡å‡†ä¸äººç±»çš„åˆ¤æ–­ç›¸æ‚–ã€‚è¿™äº›å‘ç°æ—¢çªæ˜¾äº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç†æ–‡åŒ–ç´ å…»ç”Ÿæˆæ–¹é¢çš„æ½œåŠ›ï¼Œä¹ŸæŒ‡å‡ºäº†å…¶å±€é™æ€§ï¼Œä»¥åŠè¯„ä¼°å®è·µçš„å±€é™æ€§ã€‚å› æ­¤ï¼Œåœ¨æ–‡åŒ–å’ŒæŠ€æœ¯ä¸Šå¤æ‚çš„åˆ›é€ æ€§ä»»åŠ¡ä¸­ï¼Œä»ç„¶éœ€è¦äººå’Œæ¨¡å‹çš„æ··åˆéªŒè¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15313v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯—æ­Œç”Ÿæˆå’Œè¯„ä»·æ–¹é¢çš„è¡¨ç°å°šå¾…äº†è§£ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„è¯„ä»·æ¡†æ¶ï¼Œç»“åˆè®¡ç®—æŒ‡æ ‡ã€LLMä½œä¸ºè¯„åˆ¤è€…å’Œäººç±»ä¸“å®¶éªŒè¯çš„æ–¹æ³•ï¼Œè¯„ä¼°äº†å…­ç§å…ˆè¿›çš„LLMsåœ¨è¯—æ­Œè´¨é‡ä¸Šçš„è¡¨ç°ã€‚åˆ†ææ˜¾ç¤ºï¼ŒLLMsåœ¨è¯„ä¼°åˆ›é€ æ€§è´¨é‡æ—¶å­˜åœ¨ç³»ç»Ÿæ€§åè§ï¼Œä¸äººç±»çš„åˆ¤æ–­å­˜åœ¨åˆ†æ­§ã€‚è¿™å‡¸æ˜¾äº†LLMsä½œä¸ºæ–‡å­¦ä»£ç†ç”Ÿæˆå’Œæœ‰é™è¯„ä¼°å®è·µçš„æ½œåŠ›å’Œå±€é™æ€§ï¼Œå¹¶å±•ç¤ºäº†åœ¨æ–‡åŒ–å’ŒæŠ€æœ¯ä¸Šå¤æ‚çš„åˆ›é€ æ€§ä»»åŠ¡ä¸­ï¼Œäººç±»å’Œæ¨¡å‹æ··åˆéªŒè¯çš„æŒç»­æ€§éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤å…¸è¯—è¯ç”Ÿæˆå’Œè¯„ä»·é¢†åŸŸåº”ç”¨é€æ¸å¢å¤šï¼Œä½†å…¶è¡¨ç°å°šå¾…äº†è§£ã€‚</li>
<li>æå‡ºä¸€ä¸ªä¸‰é˜¶æ®µçš„è¯„ä»·æ¡†æ¶ï¼ŒåŒ…æ‹¬è®¡ç®—æŒ‡æ ‡ã€LLMä½œä¸ºè¯„åˆ¤è€…å’Œäººç±»ä¸“å®¶éªŒè¯ã€‚</li>
<li>è¯„ä¼°äº†å…­ç§å…ˆè¿›çš„LLMsåœ¨è¯—æ­Œè´¨é‡å¤šä¸ªç»´åº¦ä¸Šçš„è¡¨ç°ã€‚</li>
<li>LLMsåœ¨è¯„ä¼°è¯—æ­Œåˆ›é€ æ€§è´¨é‡æ—¶å­˜åœ¨ç³»ç»Ÿæ€§åè§ï¼Œå³â€œå›å£°å®¤æ•ˆåº”â€ã€‚</li>
<li>LLMsåœ¨è¯„ä¼°è¯—æ­Œè´¨é‡æ—¶ï¼Œå…¶æ ‡å‡†å¸¸å­˜åœ¨ç¼ºé™·ï¼Œä¸äººç±»åˆ¤æ–­å­˜åœ¨åˆ†æ­§ã€‚</li>
<li>LLMsä½œä¸ºæ–‡å­¦ä»£ç†ç”Ÿæˆå’Œè¯„ä¼°å·¥å…·å­˜åœ¨æ½œåŠ›å’Œå±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15313">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a2b02e6b9c0658e9fa3241afe050e29e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982674&auth_key=1760982674-0-0-95a3bca2e29a2da3924b944b31e4e47f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-028f4dc6dd1a30d5d8ae6e47c99b3542~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982681&auth_key=1760982681-0-0-769feb02546be06d5e374594f51e8146&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fabb2b8c528833f47aa688771eddf32b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982688&auth_key=1760982688-0-0-aa22ce9782b303219321cef9b73e1889&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-22f9f6b4e7f7756db2e32292bb49f8a4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982695&auth_key=1760982695-0-0-e29bb5aa77c1c7013d4ed9f33e14dcf8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-46f54fed8a06a6128e6e93ae321cdb1b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982701&auth_key=1760982701-0-0-a0f28b8052726a4766db8b07e3562160&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bf5ffe58f6b1d8401604d6967a52366e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982708&auth_key=1760982708-0-0-9c73a82dec0f3b308eaf073f558de6e9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-38a5e10df1ca16338427d146c8cf8e83~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982715&auth_key=1760982715-0-0-9d48b9d837bed694346847e0cb06c2a2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9285227b45254df906ac5d1b1ca67c7e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760982722&auth_key=1760982722-0-0-dfc54d1264c529f6aa0ef38af55ce360&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-21/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-21/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-21/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-cc0af8f8e3b7df25e92c12d487574419~resize:0:q75.jpg?source=1f5c5e47&expiration=1760984448&auth_key=1760984448-0-0-bfcd5d85249426301421d086676f69ed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-21  OmniVinci Enhancing Architecture and Data for Omni-Modal Understanding   LLM
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-35c7aae62c8327e7f356d73aff02312d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760904577&auth_key=1760904577-0-0-d37807146b7b6c6b510622e833115c15&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-20  Uncertainty-Supervised Interpretable and Robust Evidential Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32251.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
