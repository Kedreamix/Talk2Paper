<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-21  Lightweight Data-Free Denoising for Detail-Preserving Biomedical Image   Restoration">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-50a13677fb759f7da81cb1665ea4e2d6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995612&auth_key=1760995612-0-0-fef0a9886ffd577f51e0f5e9084f7975&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    90 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-21-æ›´æ–°"><a href="#2025-10-21-æ›´æ–°" class="headerlink" title="2025-10-21 æ›´æ–°"></a>2025-10-21 æ›´æ–°</h1><h2 id="Lightweight-Data-Free-Denoising-for-Detail-Preserving-Biomedical-Image-Restoration"><a href="#Lightweight-Data-Free-Denoising-for-Detail-Preserving-Biomedical-Image-Restoration" class="headerlink" title="Lightweight Data-Free Denoising for Detail-Preserving Biomedical Image   Restoration"></a>Lightweight Data-Free Denoising for Detail-Preserving Biomedical Image   Restoration</h2><p><strong>Authors:TomÃ¡Å¡ Chobola, Julia A. Schnabel, Tingying Peng</strong></p>
<p>Current self-supervised denoising techniques achieve impressive results, yet their real-world application is frequently constrained by substantial computational and memory demands, necessitating a compromise between inference speed and reconstruction quality. In this paper, we present an ultra-lightweight model that addresses this challenge, achieving both fast denoising and high quality image restoration. Built upon the Noise2Noise training framework-which removes the reliance on clean reference images or explicit noise modeling-we introduce an innovative multistage denoising pipeline named Noise2Detail (N2D). During inference, this approach disrupts the spatial correlations of noise patterns to produce intermediate smooth structures, which are subsequently refined to recapture fine details directly from the noisy input. Extensive testing reveals that Noise2Detail surpasses existing dataset-free techniques in performance, while requiring only a fraction of the computational resources. This combination of efficiency, low computational cost, and data-free approach make it a valuable tool for biomedical imaging, overcoming the challenges of scarce clean training data-due to rare and complex imaging modalities-while enabling fast inference for practical use. </p>
<blockquote>
<p>å½“å‰çš„è‡ªç›‘ç£å»å™ªæŠ€æœ¯å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ç»å¸¸å—åˆ°å·¨å¤§çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚çš„é™åˆ¶ï¼Œéœ€è¦åœ¨æ¨ç†é€Ÿåº¦å’Œé‡å»ºè´¨é‡ä¹‹é—´è¿›è¡Œå¦¥åã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¶…è½»é‡çº§çš„æ¨¡å‹æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œå®ç°äº†å¿«é€Ÿå»å™ªå’Œé«˜è´¨é‡å›¾åƒæ¢å¤ã€‚è¯¥æ¨¡å‹å»ºç«‹åœ¨Noise2Noiseè®­ç»ƒæ¡†æ¶ä¹‹ä¸Šâ€”â€”è¯¥æ¡†æ¶æ¶ˆé™¤äº†å¯¹å¹²å‡€å‚è€ƒå›¾åƒæˆ–æ˜ç¡®å™ªå£°æ¨¡å‹çš„ä¾èµ–â€”â€”æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„åˆ†é˜¶æ®µå»å™ªç®¡é“ï¼Œåä¸ºNoise2Detailï¼ˆN2Dï¼‰ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œè¯¥æ–¹æ³•ç ´åäº†å™ªå£°æ¨¡å¼çš„ç©ºé—´ç›¸å…³æ€§ï¼Œä»¥äº§ç”Ÿä¸­é—´å¹³æ»‘ç»“æ„ï¼Œéšåå¯¹å…¶è¿›è¡Œç²¾ç‚¼ï¼Œç›´æ¥ä»å™ªå£°è¾“å…¥ä¸­æ¢å¤ç»†èŠ‚ã€‚å¤§é‡æµ‹è¯•è¡¨æ˜ï¼ŒNoise2Detailåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æ— æ•°æ®é›†æŠ€æœ¯ï¼ŒåŒæ—¶åªéœ€è®¡ç®—èµ„æºçš„å¾ˆå°ä¸€éƒ¨åˆ†ã€‚æ•ˆç‡ã€ä½æˆæœ¬å’Œæ•°æ®å…è´¹çš„æ–¹æ³•çš„ç»“åˆä½¿å…¶æˆä¸ºç”Ÿç‰©åŒ»å­¦æˆåƒçš„æœ‰ä»·å€¼çš„å·¥å…·ï¼Œå…‹æœäº†å› ç¨€æœ‰å’Œå¤æ‚çš„æˆåƒæ¨¡å¼å¯¼è‡´çš„æ¸…æ´è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼ŒåŒæ—¶å®ç°äº†å¿«é€Ÿæ¨ç†ä»¥ä¾›å®é™…åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15611v1">PDF</a> 10 pages, MICCAI 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†ä¸€ç§è¶…è½»é‡çº§æ¨¡å‹ï¼Œé‡‡ç”¨Noise2Noiseè®­ç»ƒæ¡†æ¶ï¼Œå®ç°å¿«é€Ÿå»å™ªå’Œé«˜è´¨é‡å›¾åƒæ¢å¤ã€‚æ–°æ–¹æ³•åä¸ºNoise2Detailï¼ˆN2Dï¼‰ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ç ´åå™ªå£°æ¨¡å¼çš„ç©ºé—´ç›¸å…³æ€§ä»¥äº§ç”Ÿä¸­é—´å¹³æ»‘ç»“æ„ï¼Œéšåç›´æ¥å¯¹å™ªå£°è¾“å…¥è¿›è¡Œç²¾ç»†ä¿®å¤ä»¥æ¢å¤ç»†èŠ‚ã€‚è¯¥æ–¹æ³•åœ¨æ— éœ€æ•°æ®é›†çš„æƒ…å†µä¸‹è¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶è®¡ç®—èµ„æºæ¶ˆè€—è¾ƒä½ï¼Œå°¤å…¶é€‚ç”¨äºç”Ÿç‰©åŒ»å­¦æˆåƒé¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è‡ªç›‘ç£å»å™ªæŠ€æœ¯è™½å–å¾—æ˜¾è‘—æˆæœï¼Œä½†å®é™…åº”ç”¨ä¸­é¢ä¸´è®¡ç®—ä¸å†…å­˜éœ€æ±‚å¤§çš„æŒ‘æˆ˜ï¼Œéœ€è¦åœ¨æ¨ç†é€Ÿåº¦ä¸é‡å»ºè´¨é‡ä¹‹é—´åšå‡ºå¦¥åã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§è¶…è½»é‡çº§æ¨¡å‹ï¼Œç»“åˆNoise2Noiseè®­ç»ƒæ¡†æ¶ï¼Œè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ç°å¿«é€Ÿå»å™ªå’Œé«˜è´¨é‡å›¾åƒæ¢å¤ã€‚</li>
<li>å¼•å…¥åä¸ºNoise2Detailï¼ˆN2Dï¼‰çš„å¤šé˜¶æ®µå»å™ªç®¡é“ï¼Œé€šè¿‡ç ´åå™ªå£°æ¨¡å¼çš„ç©ºé—´ç›¸å…³æ€§äº§ç”Ÿä¸­é—´å¹³æ»‘ç»“æ„ã€‚</li>
<li>Noise2Detailæ–¹æ³•èƒ½åœ¨æ— éœ€æ•°æ®é›†çš„æƒ…å†µä¸‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•è®¡ç®—èµ„æºæ¶ˆè€—è¾ƒä½ï¼Œé€‚ç”¨äºç”Ÿç‰©åŒ»å­¦æˆåƒé¢†åŸŸã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ç¨€ç¼ºæ¸…æ´è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹å…‹æœæŒ‘æˆ˜ï¼Œå°¤å…¶é€‚ç”¨äºç¨€æœ‰å’Œå¤æ‚çš„æˆåƒæ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15611">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c80f541975cd3c600eee603b2aafc8c5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995343&auth_key=1760995343-0-0-717d0bd833f13e9fd4704f33310d2ca0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ca6cc8ae482a97ae675175cc8a2e149a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995351&auth_key=1760995351-0-0-f763296bd4b7309a55a46696983cb895&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-767bcfb87f9291df3996e5df776cd960~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995357&auth_key=1760995357-0-0-4e48137b59640ca6eb5fb22e1197ec44&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Multiscale-X-ray-computed-tomography-of-standard-optical-fibers"><a href="#Multiscale-X-ray-computed-tomography-of-standard-optical-fibers" class="headerlink" title="Multiscale X-ray computed tomography of standard optical fibers"></a>Multiscale X-ray computed tomography of standard optical fibers</h2><p><strong>Authors:Maria Caterina Crocco, Flavio Cognigni, Alessia Sanna, Raffaele Filosa, Svetlana Siprova, Riccardo C. Barberi, Raffaele G. Agostino, Stefan Wabnitz, Antonio Dâ€™Alessandro, Sylvie Lebrun, Marco Rossi, Vincenzo Formoso, Roberto Termine, Alberto Bravin, Mario Ferraro</strong></p>
<p>Optical fiber technologies enable high-speed communication, medical imaging, and advanced sensing. Among the techniques for the characterization of optical fibers, Xray computed tomography has recently emerged as a versatile non-destructive tool for mapping their refractive index variations in 3D. In this study, we present a multiscale characterization of standard optical fibers. We carry out an intercomparison of three tomography setups: classical computed microtomography, X-ray microscopy, and nanotomography. In each method, our analysis highlights the trade-offs between resolution, field of view, and segmentation efficiency. Additionally, we integrate deep learning segmentation thresholding to improve the image analysis process. Thanks to its large field of view, microtomography with classical sources is ideal for the analysis of relatively long fiber spans, where a low spatial resolution is acceptable. The other way around, nanotomography has the highest spatial resolution, but it is limited to very small fiber samples, e.g., fiber tapers and nanofibers, which have diameters of the order of a few microns. Finally, X-ray microscopy provides a good compromise between the sample size fitting the deviceâ€™s field of view and the spatial resolution needed for properly imaging the inner features of the fiber. Specifically, thanks to its practicality in terms of costs and cumbersomeness, we foresee that the latter will provide the most suitable choice for the quality control of fiber drawing in real-time, e.g., using the â€œOne-Minute Tomographies with Fast Acquisition Scanning Technologyâ€ developed by Zeiss. In this regard, the combination of X-ray computed tomography and artificial intelligence-driven enhancements is poised to revolutionize fiber characterization, by enabling precise monitoring and adaptive control in fiber manufacturing. </p>
<blockquote>
<p>å…‰çº¤æŠ€æœ¯èƒ½å¤Ÿå®ç°é«˜é€Ÿé€šä¿¡ã€åŒ»å­¦æˆåƒå’Œå…ˆè¿›ä¼ æ„Ÿã€‚åœ¨å…‰çº¤ç‰¹æ€§åˆ†ææŠ€æœ¯ä¸­ï¼ŒXå°„çº¿è®¡ç®—æœºæ–­å±‚æ‰«ææœ€è¿‘æ¶Œç°ä¸ºä¸€ç§é€šç”¨çš„éç ´åæ€§å·¥å…·ï¼Œå¯ç”¨äºä¸‰ç»´æ˜ å°„å…¶æŠ˜å°„ç‡å˜åŒ–ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹æ ‡å‡†å…‰çº¤è¿›è¡Œäº†å¤šå°ºåº¦è¡¨å¾ã€‚æˆ‘ä»¬å¯¹ä¸‰ç§æ–­å±‚æ‰«æè®¾ç½®è¿›è¡Œäº†ç›¸äº’æ¯”è¾ƒï¼šç»å…¸è®¡ç®—å¾®æ–­å±‚æ‰«æã€Xå°„çº¿æ˜¾å¾®é•œå’Œçº³ç±³æ–­å±‚æ‰«æã€‚åœ¨æ¯ç§æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬çš„åˆ†æéƒ½å¼ºè°ƒäº†åˆ†è¾¨ç‡ã€è§†é‡å’Œåˆ†å‰²æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é›†æˆäº†æ·±åº¦å­¦ä¹ åˆ†å‰²é˜ˆå€¼å¤„ç†æ¥æ”¹å–„å›¾åƒåˆ†æè¿‡ç¨‹ã€‚ç”±äºå…·æœ‰è¾ƒå¤§çš„è§†é‡ï¼Œä½¿ç”¨ç»å…¸æºè¿›è¡Œçš„å¾®æ–­å±‚æ‰«æéå¸¸é€‚åˆåˆ†æç›¸å¯¹è¾ƒé•¿çš„å…‰çº¤è·¨åº¦ï¼Œæ­¤æ—¶å¯ä»¥æ¥å—è¾ƒä½çš„ç©ºé—´åˆ†è¾¨ç‡ã€‚ç›¸åï¼Œçº³ç±³æ–­å±‚æ‰«æå…·æœ‰æœ€é«˜çš„ç©ºé—´åˆ†è¾¨ç‡ï¼Œä½†ä»…é™äºéå¸¸å°çš„å…‰çº¤æ ·å“ï¼Œä¾‹å¦‚ç›´å¾„ä¸ºæ•°å¾®ç±³çš„æ¸å˜å…‰çº¤å’Œçº³ç±³å…‰çº¤ã€‚æœ€åï¼ŒXå°„çº¿æ˜¾å¾®é•œåœ¨æ ·å“å°ºå¯¸é€‚åˆè®¾å¤‡è§†é‡ä¸æˆåƒå…‰çº¤å†…éƒ¨ç‰¹å¾æ‰€éœ€çš„ç©ºé—´åˆ†è¾¨ç‡ä¹‹é—´æä¾›äº†è‰¯å¥½çš„æŠ˜è¡·æ–¹æ¡ˆã€‚å…·ä½“æ¥è¯´ï¼Œç”±äºå…¶åœ¨æˆæœ¬å’Œç¹çæ€§æ–¹é¢çš„å®ç”¨æ€§ï¼Œæˆ‘ä»¬é¢„è®¡åè€…å°†æˆä¸ºå®æ—¶çº¤ç»´æ‹‰åˆ¶è´¨é‡æ§åˆ¶çš„æœ€åˆé€‚é€‰æ‹©ï¼Œä¾‹å¦‚ä½¿ç”¨Zeisså¼€å‘çš„â€œä¸€åˆ†é’Ÿæ–­å±‚æ‰«æä¸å¿«é€Ÿé‡‡é›†æ‰«ææŠ€æœ¯â€ã€‚åœ¨è¿™æ–¹é¢ï¼ŒXå°„çº¿è®¡ç®—æœºæ–­å±‚æ‰«æå’Œäººå·¥æ™ºèƒ½é©±åŠ¨å¢å¼ºçš„ç»“åˆï¼Œæœ‰æœ›é€šè¿‡å®ç°ç²¾ç¡®ç›‘æ§å’Œè‡ªé€‚åº”æ§åˆ¶æ¥å½»åº•æ”¹å˜å…‰çº¤è¡¨å¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15597v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å…‰çº¤çš„å¤šç§è¡¨å¾æŠ€æœ¯ï¼ŒåŒ…æ‹¬åˆ©ç”¨Xå°„çº¿è®¡ç®—æœºæ–­å±‚æ‰«ææŠ€æœ¯è¿›è¡Œéç ´åæ€§è¡¨å¾çš„æ–¹æ³•ã€‚ç ”ç©¶ä¸­å¯¹æ¯”äº†ä¸‰ç§æˆåƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬å¾®æ–­å±‚æ‰«æã€Xå°„çº¿æ˜¾å¾®é•œå’Œçº³ç±³æ–­å±‚æ‰«æï¼Œå¹¶æ¢è®¨äº†å®ƒä»¬åœ¨åˆ†è¾¨ç‡ã€è§†é‡å’Œåˆ†å‰²æ•ˆç‡æ–¹é¢çš„ä¼˜ç¼ºç‚¹ã€‚æ­¤å¤–ï¼Œè¿˜ç»“åˆäº†æ·±åº¦å­¦ä¹ åˆ†å‰²é˜ˆå€¼æŠ€æœ¯æ”¹è¿›å›¾åƒåˆ†æè¿‡ç¨‹ã€‚å¯¹äºä¸åŒç±»å‹çš„å…‰çº¤ï¼Œä¸‰ç§æˆåƒæŠ€æœ¯éƒ½æœ‰é€‚ç”¨çš„åœºæ™¯ï¼Œè€Œç»“åˆäº†Xå°„çº¿è®¡ç®—æœºæ–­å±‚æ‰«æä¸äººå·¥æ™ºèƒ½çš„å¢å¼ºæŠ€æœ¯å¯ä¸ºå…‰çº¤åˆ¶é€ è¿‡ç¨‹çš„ç²¾ç¡®ç›‘æµ‹å’Œé€‚åº”æ€§æ§åˆ¶å¸¦æ¥é©å‘½æ€§å˜é©ã€‚å…¶ä¸­ï¼Œé¢„è®¡åŸºäºè”¡å¸å…¬å¸çš„å¿«é€Ÿæˆåƒæ‰«ææŠ€æœ¯çš„Xå°„çº¿æ˜¾å¾®é•œå°†ä¸ºå®æ—¶å…‰çº¤è´¨é‡æ§åˆ¶æä¾›æœ€ä½³é€‰æ‹©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å…‰å­¦å…‰çº¤çš„é«˜é€Ÿé€šä¿¡ã€åŒ»ç–—æˆåƒå’Œå…ˆè¿›ä¼ æ„Ÿåº”ç”¨ä¾èµ–äºå¯¹å…¶ç‰¹æ€§çš„å‡†ç¡®è¡¨å¾ã€‚</li>
<li>Xå°„çº¿è®¡ç®—æœºæ–­å±‚æ‰«ææ˜¯éç ´åæ€§è¡¨å¾å…‰çº¤æŠ˜å°„ç‡å˜åŒ–çš„æœ‰æ•ˆå·¥å…·ã€‚</li>
<li>å¯¹æ¯”äº†ä¸‰ç§æ–­å±‚æ‰«ææŠ€æœ¯ï¼šå¾®æ–­å±‚æ‰«æã€Xå°„çº¿æ˜¾å¾®é•œå’Œçº³ç±³æ–­å±‚æ‰«æï¼Œå„æœ‰ä¼˜ç¼ºç‚¹ã€‚</li>
<li>ç»“åˆæ·±åº¦å­¦ä¹ æé«˜äº†å›¾åƒåˆ†æè¿‡ç¨‹çš„åˆ†å‰²æ•ˆç‡ã€‚</li>
<li>å¾®æ–­å±‚æ‰«æé€‚ç”¨äºåˆ†æç›¸å¯¹é•¿çš„å…‰çº¤è·¨åº¦ï¼Œè€Œçº³ç±³æ–­å±‚æ‰«æåˆ™é€‚ç”¨äºå¾®å°çº¤ç»´æ ·æœ¬çš„é«˜åˆ†è¾¨ç‡åˆ†æã€‚</li>
<li>Xå°„çº¿æ˜¾å¾®é•œæä¾›äº†ä¸€ä¸ªåœ¨è§†é‡åˆ†è¾¨ç‡å’Œæˆåƒå…‰çº¤å†…éƒ¨ç‰¹å¾ä¹‹é—´è¾ƒå¥½çš„å¹³è¡¡ï¼Œé¢„è®¡å°†æˆä¸ºå…‰çº¤å®æ—¶è´¨é‡æ§åˆ¶çš„é¦–é€‰æŠ€æœ¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1b4d1656ad3e179691ab471401b765fd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995365&auth_key=1760995365-0-0-53bb663ae35e8c3f8e8a0fb887dabaf7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7d4d887b8442b7ad6451a0a99b538a45~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995372&auth_key=1760995372-0-0-cd982b54f84630dda75c26117b6caadc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-538446b8d7885ee1d84b85b30c3dad11~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995378&auth_key=1760995378-0-0-af442161bab6d47f85ccd5d0d842c795&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-51aadc07f40fe20eae3fdeb15b8abf0b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995385&auth_key=1760995385-0-0-a1ee007e86a62b2338f784dcb7018f68&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8b9ed81a72dd9d5b7e8a303a5eb2f0fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995392&auth_key=1760995392-0-0-421889132ce8556e0026ca5e7bcdd499&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1d7f2ec478253e3ae9d690caef9226c3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995398&auth_key=1760995398-0-0-fd9e9508fc5918d8c9b9c6e6148d0548&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Diffusion-Bridge-Networks-Simulate-Clinical-grade-PET-from-MRI-for-Dementia-Diagnostics"><a href="#Diffusion-Bridge-Networks-Simulate-Clinical-grade-PET-from-MRI-for-Dementia-Diagnostics" class="headerlink" title="Diffusion Bridge Networks Simulate Clinical-grade PET from MRI for   Dementia Diagnostics"></a>Diffusion Bridge Networks Simulate Clinical-grade PET from MRI for   Dementia Diagnostics</h2><p><strong>Authors:Yitong Li, Ralph Buchert, Benita Schmitz-Koep, Timo Grimmer, BjÃ¶rn Ommer, Dennis M. Hedderich, Igor Yakushev, Christian Wachinger</strong></p>
<p>Positron emission tomography (PET) with 18F-Fluorodeoxyglucose (FDG) is an established tool in the diagnostic workup of patients with suspected dementing disorders. However, compared to the routinely available magnetic resonance imaging (MRI), FDG-PET remains significantly less accessible and substantially more expensive. Here, we present SiM2P, a 3D diffusion bridge-based framework that learns a probabilistic mapping from MRI and auxiliary patient information to simulate FDG-PET images of diagnostic quality. In a blinded clinical reader study, two neuroradiologists and two nuclear medicine physicians rated the original MRI and SiM2P-simulated PET images of patients with Alzheimerâ€™s disease, behavioral-variant frontotemporal dementia, and cognitively healthy controls. SiM2P significantly improved the overall diagnostic accuracy of differentiating between three groups from 75.0% to 84.7% (p&lt;0.05). Notably, the simulated PET images received higher diagnostic certainty ratings and achieved superior interrater agreement compared to the MRI images. Finally, we developed a practical workflow for local deployment of the SiM2P framework. It requires as few as 20 site-specific cases and only basic demographic information. This approach makes the established diagnostic benefits of FDG-PET imaging more accessible to patients with suspected dementing disorders, potentially improving early detection and differential diagnosis in resource-limited settings. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Yiiitong/SiM2P">https://github.com/Yiiitong/SiM2P</a>. </p>
<blockquote>
<p>æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰ç»“åˆæ°Ÿè„±æ°§è‘¡è„ç³–ï¼ˆFDGï¼‰æ˜¯è¯Šæ–­ç–‘ä¼¼ç—´å‘†éšœç¢æ‚£è€…çš„æ—¢å®šå·¥å…·ã€‚ç„¶è€Œï¼Œä¸å¸¸è§„å¯ç”¨çš„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ç›¸æ¯”ï¼ŒFDG-PETçš„æ™®åŠç¨‹åº¦ä»ç„¶è¾ƒä½ä¸”è´¹ç”¨æ˜‚è´µã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºSiM2Pï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä¸‰ç»´æ‰©æ•£æ¡¥çš„æ¡†æ¶ï¼Œå¯ä»¥ä»MRIå’Œè¾…åŠ©æ‚£è€…ä¿¡æ¯ä¸­å­¦ä¹ æ¦‚ç‡æ˜ å°„æ¥æ¨¡æ‹Ÿå…·æœ‰è¯Šæ–­è´¨é‡çš„FDG-PETå›¾åƒã€‚åœ¨ä¸€é¡¹ç›²æ€çš„ä¸´åºŠè¯»è€…ç ”ç©¶ä¸­ï¼Œä¸¤åç¥ç»æ”¾å°„ç§‘åŒ»å¸ˆå’Œä¸¤åæ ¸åŒ»å­¦åŒ»å¸ˆå¯¹æ‚£æœ‰é˜¿å°”èŒ¨æµ·é»˜ç—…ã€è¡Œä¸ºæ€§å˜å‰é¢å¶ç—´å‘†ä»¥åŠè®¤çŸ¥å¥åº·å¯¹ç…§æ‚£è€…çš„åŸå§‹MRIå’ŒSiM2Pæ¨¡æ‹Ÿçš„PETå›¾åƒè¿›è¡Œäº†è¯„ä¼°ã€‚SiM2Pæ˜¾è‘—æé«˜äº†ä¸‰ç»„ä¹‹é—´çš„æ€»ä½“è¯Šæ–­å‡†ç¡®ç‡ï¼Œä»75.0%æé«˜åˆ°84.7%ï¼ˆp&lt;0.05ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ¨¡æ‹Ÿçš„PETå›¾åƒè·å¾—äº†æ›´é«˜çš„è¯Šæ–­ç¡®å®šæ€§è¯„åˆ†ï¼Œå¹¶å®ç°äº†ä¼˜äºMRIå›¾åƒçš„è¯„åˆ†è€…é—´ä¸€è‡´æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬ä¸ºæœ¬åœ°éƒ¨ç½²SiM2Pæ¡†æ¶å¼€å‘äº†ä¸€ä¸ªå®ç”¨å·¥ä½œæµç¨‹ã€‚å®ƒåªéœ€è¦å°‘æ•°ç‰¹å®šäºç«™ç‚¹çš„æ¡ˆä¾‹å’ŒåŸºæœ¬çš„äººå£ç»Ÿè®¡å­¦ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—FDG-PETæˆåƒçš„æ—¢å®šè¯Šæ–­ä¼˜åŠ¿å¯¹äºç–‘ä¼¼ç—´å‘†éšœç¢çš„æ‚£è€…æ›´åŠ å®¹æ˜“è·å¾—ï¼Œå¯èƒ½åœ¨èµ„æºæœ‰é™çš„æ¡ä»¶ä¸‹æ”¹å–„æ—©æœŸæ£€æµ‹å’Œé‰´åˆ«è¯Šæ–­ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Yiiitong/SiM2P%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Yiiitong/SiM2Pè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15556v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    åˆ©ç”¨MRIå’Œè¾…åŠ©ç—…äººä¿¡æ¯ï¼Œé€šè¿‡ä¸‰ç»´æ‰©æ•£æ¡¥æ¡†æ¶SiM2Pæ¨¡æ‹Ÿè¯Šæ–­è´¨é‡çš„FDG-PETå›¾åƒã€‚SiM2Pèƒ½æé«˜è¯Šæ–­å‡†ç¡®æ€§ï¼ŒåŒºåˆ†é˜¿å°”èŒ¨æµ·é»˜ç—…ã€è¡Œä¸ºå˜å¼‚é¢é¢å¶ç—´å‘†å’Œè®¤çŸ¥å¥åº·å¯¹ç…§ç»„ï¼Œä»75.0%æé«˜åˆ°84.7%ï¼ˆp&lt;0.05ï¼‰ã€‚æ¨¡æ‹ŸPETå›¾åƒè·å¾—æ›´é«˜çš„è¯Šæ–­ç¡®å®šæ€§è¯„åˆ†ï¼Œå¹¶è¾¾æˆä¼˜äºMRIå›¾åƒçš„ä¸€è‡´æ€§ã€‚å¼€å‘äº†ä¸€ä¸ªå®ç”¨çš„SiM2Pæ¡†æ¶æœ¬åœ°éƒ¨ç½²å·¥ä½œæµç¨‹ï¼Œéœ€è¦å°‘é‡çš„ç‰¹å®šç«™ç‚¹ç—…ä¾‹å’ŒåŸºæœ¬çš„äººå£ç»Ÿè®¡å­¦ä¿¡æ¯ã€‚æ­¤æŠ€æœ¯è®©FDG-PETæˆåƒçš„è¯Šæ–­ä¼˜åŠ¿æ›´æ˜“ä¸ºç–‘ä¼¼ç—´å‘†éšœç¢æ‚£è€…è·å¾—ï¼Œæœ‰å¯èƒ½æ”¹å–„èµ„æºå—é™ç¯å¢ƒä¸‹çš„æ—©æœŸæ£€æµ‹å’Œé‰´åˆ«è¯Šæ–­ã€‚ç›¸å…³ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/Yiiitong/SiM2P%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Yiiitong/SiM2Pè·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>SiM2Pæ˜¯ä¸€ç§åˆ©ç”¨MRIå’Œè¾…åŠ©ç—…äººä¿¡æ¯æ¨¡æ‹Ÿè¯Šæ–­è´¨é‡FDG-PETå›¾åƒçš„æŠ€æœ¯ã€‚å®ƒé€šè¿‡åˆ›å»ºä¸€ä¸ªä¸‰ç»´æ‰©æ•£æ¡¥æ¢æ¡†æ¶æ¥å®ç°è¿™ä¸€ç‚¹ã€‚</li>
<li>å¯¹æ¯”ä¼ ç»Ÿçš„MRIæŠ€æœ¯ï¼ŒSiM2Pæé«˜äº†è¯Šæ–­çš„å‡†ç¡®æ€§ï¼ŒåŒºåˆ†äº†ä¸åŒç§ç±»çš„ç—´å‘†ç—…ç—‡ã€‚ç‰¹åˆ«æ˜¯åœ¨ç›²æ€çš„ä¸´åºŠåŒ»ç”Ÿé˜…è¯»ç ”ç©¶ä¸­ï¼Œå®ƒæˆåŠŸæé«˜äº†æ€»ä½“è¯Šæ–­ç‡ä»åŸæ¥çš„çº¦ç™¾åˆ†ä¹‹ä¸ƒåäº”è‡³ç™¾åˆ†ä¹‹å…«åå››ä»¥ä¸Šã€‚è¿™ä¸ºç¥ç»æ”¾å°„åŒ»ç”Ÿå’Œæ ¸åŒ»å­¦åŒ»ç”Ÿæä¾›äº†æ›´å‡†ç¡®çš„è¯Šæ–­å·¥å…·ã€‚</li>
<li>æ¨¡æ‹Ÿçš„PETå›¾åƒæ¯”MRIå›¾åƒå…·æœ‰æ›´é«˜çš„è¯Šæ–­ç¡®å®šæ€§è¯„åˆ†å’Œæ›´å¥½çš„åŒ»ç”Ÿé—´ä¸€è‡´æ€§ã€‚è¿™æ„å‘³ç€åŒ»ç”Ÿå¯¹æ¨¡æ‹ŸPETå›¾åƒçš„è¯Šæ–­æ›´æœ‰ä¿¡å¿ƒï¼Œå¹¶ä¸”ä»–ä»¬çš„åˆ¤æ–­æ›´åŠ ä¸€è‡´ã€‚</li>
<li>SiM2Pæ¡†æ¶çš„å®ç”¨å·¥ä½œæµç¨‹åªéœ€è¦å°‘é‡çš„ç‰¹å®šç«™ç‚¹ç—…ä¾‹å’ŒåŸºæœ¬çš„äººå£ç»Ÿè®¡å­¦ä¿¡æ¯å³å¯å®ç°æœ¬åœ°éƒ¨ç½²ã€‚è¿™ä½¿å¾—æ–°æŠ€æœ¯æ˜“äºå®æ–½å¹¶å¹¿æ³›åº”ç”¨ã€‚</li>
<li>è¯¥æŠ€æœ¯ä½¿å¾—åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼ŒåƒFDG-PETæˆåƒè¿™æ ·çš„å…ˆè¿›è¯Šæ–­æŠ€æœ¯å˜å¾—æ›´å®¹æ˜“è·å¾—ï¼Œæœ‰åˆ©äºç–‘ä¼¼ç—´å‘†éšœç¢æ‚£è€…çš„æ—©æœŸæ£€æµ‹å’Œé‰´åˆ«è¯Šæ–­ã€‚è¿™å°†æ”¹å–„åŒ»ç–—èµ„æºä¸å‡çš„æƒ…å†µï¼Œä½¿æ›´å¤šäººèƒ½å¤Ÿå—ç›Šäºå…ˆè¿›çš„åŒ»å­¦æˆåƒæŠ€æœ¯ã€‚</li>
<li>ç ”ç©¶æä¾›äº†å¯¹æ–°æŠ€æœ¯æ˜“ç”¨æ€§çš„æ˜ç¡®è¯„ä»·ï¼Œæ˜¾ç¤ºå…¶å¯¹å®é™…åº”ç”¨çš„é«˜åº¦é€‚ç”¨æ€§ã€‚è¿™å¯èƒ½ä¼šæ¿€å‘æ›´å¤šåœ¨åŒ»å­¦å›¾åƒæ¨¡æ‹Ÿé¢†åŸŸçš„åˆ›æ–°å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15556">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-51b594ceab7420bb58f429acbce52c3d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995405&auth_key=1760995405-0-0-3eca134f067b3fe8f98fc25c0574262d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fe3ffd54a73bbfeabb9d46de4f8528f9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995413&auth_key=1760995413-0-0-d7d51f139136394d49937f63261ae8ac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a253c1a96195bfcf839c6be967f7de37~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995420&auth_key=1760995420-0-0-b65032dd3b7157f71192296b8339930c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-539949ca347432877427702ad55b61fa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995502&auth_key=1760995502-0-0-dfec65ec1ce6d439903979dda6a4ac44&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Rethinking-Convergence-in-Deep-Learning-The-Predictive-Corrective-Paradigm-for-Anatomy-Informed-Brain-MRI-Segmentation"><a href="#Rethinking-Convergence-in-Deep-Learning-The-Predictive-Corrective-Paradigm-for-Anatomy-Informed-Brain-MRI-Segmentation" class="headerlink" title="Rethinking Convergence in Deep Learning: The Predictive-Corrective   Paradigm for Anatomy-Informed Brain MRI Segmentation"></a>Rethinking Convergence in Deep Learning: The Predictive-Corrective   Paradigm for Anatomy-Informed Brain MRI Segmentation</h2><p><strong>Authors:Feifei Zhang, Zhenhong Jia, Sensen Song, Fei Shi, Dayong Ren</strong></p>
<p>Despite the remarkable success of the end-to-end paradigm in deep learning, it often suffers from slow convergence and heavy reliance on large-scale datasets, which fundamentally limits its efficiency and applicability in data-scarce domains such as medical imaging. In this work, we introduce the Predictive-Corrective (PC) paradigm, a framework that decouples the modeling task to fundamentally accelerate learning. Building upon this paradigm, we propose a novel network, termed PCMambaNet. PCMambaNet is composed of two synergistic modules. First, the Predictive Prior Module (PPM) generates a coarse approximation at low computational cost, thereby anchoring the search space. Specifically, the PPM leverages anatomical knowledge-bilateral symmetry-to predict a â€˜focus mapâ€™ of diagnostically relevant asymmetric regions. Next, the Corrective Residual Network (CRN) learns to model the residual error, focusing the networkâ€™s full capacity on refining these challenging regions and delineating precise pathological boundaries. Extensive experiments on high-resolution brain MRI segmentation demonstrate that PCMambaNet achieves state-of-the-art accuracy while converging within only 1-5 epochs-a performance unattainable by conventional end-to-end models. This dramatic acceleration highlights that by explicitly incorporating domain knowledge to simplify the learning objective, PCMambaNet effectively mitigates data inefficiency and overfitting. </p>
<blockquote>
<p>å°½ç®¡ç«¯åˆ°ç«¯èŒƒå¼åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å®ƒé€šå¸¸å­˜åœ¨æ”¶æ•›é€Ÿåº¦æ…¢å’Œä¸¥é‡ä¾èµ–å¤§è§„æ¨¡æ•°æ®é›†çš„é—®é¢˜ï¼Œè¿™ä»æ ¹æœ¬ä¸Šé™åˆ¶äº†å…¶åœ¨åŒ»å­¦å½±åƒç­‰ç¼ºä¹æ•°æ®é¢†åŸŸçš„æ•ˆç‡å’Œé€‚ç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢„æµ‹æ ¡æ­£ï¼ˆPCï¼‰èŒƒå¼ï¼Œè¿™æ˜¯ä¸€ç§ä»æ ¹æœ¬ä¸ŠåŠ é€Ÿå­¦ä¹ çš„æ¡†æ¶ã€‚åŸºäºè¿™ä¸€èŒƒå¼ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ç½‘ç»œï¼Œç§°ä¸ºPCMambaNetã€‚PCMambaNetç”±ä¸¤ä¸ªååŒæ¨¡å—ç»„æˆã€‚é¦–å…ˆï¼Œé¢„æµ‹å…ˆéªŒæ¨¡å—ï¼ˆPPMï¼‰ä»¥è¾ƒä½çš„è®¡ç®—æˆæœ¬ç”Ÿæˆç²—ç•¥è¿‘ä¼¼å€¼ï¼Œä»è€Œé”šå®šæœç´¢ç©ºé—´ã€‚å…·ä½“æ¥è¯´ï¼ŒPPMåˆ©ç”¨è§£å‰–çŸ¥è¯†â€”â€”åŒä¾§å¯¹ç§°æ€§ï¼Œæ¥é¢„æµ‹è¯Šæ–­ç›¸å…³ä¸å¯¹ç§°åŒºåŸŸçš„â€œç„¦ç‚¹å›¾â€ã€‚æ¥ä¸‹æ¥ï¼Œæ ¡æ­£æ®‹å·®ç½‘ç»œï¼ˆCRNï¼‰å­¦ä¹ å¯¹æ®‹å·®è¿›è¡Œå»ºæ¨¡ï¼Œä½¿ç½‘ç»œçš„å…¨èƒ½ä¸“æ³¨äºç»†åŒ–è¿™äº›å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒºåŸŸï¼Œå¹¶æç»˜å‡ºç²¾ç¡®çš„ç–¾ç—…è¾¹ç•Œã€‚åœ¨é«˜åˆ†è¾¨ç‡è„‘MRIåˆ†å‰²æ–¹é¢è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒPCMambaNetè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨ä»…1-5ä¸ªå‘¨æœŸå†…å®ç°äº†æ”¶æ•›â€”â€”è¿™æ˜¯ä¼ ç»Ÿç«¯åˆ°ç«¯æ¨¡å‹æ— æ³•è¾¾åˆ°çš„æ€§èƒ½ã€‚è¿™ç§å·¨å¤§çš„åŠ é€Ÿè¡¨æ˜ï¼Œé€šè¿‡æ˜ç¡®åœ°ç»“åˆé¢†åŸŸçŸ¥è¯†æ¥ç®€åŒ–å­¦ä¹ ç›®æ ‡ï¼ŒPCMambaNetæœ‰æ•ˆåœ°ç¼“è§£äº†æ•°æ®æ•ˆç‡ä½ä¸‹å’Œè¿‡æ‹Ÿåˆé—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15439v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å¼•å…¥Predictive-Correctiveï¼ˆPCï¼‰èŒƒå¼ï¼ŒåŠ é€Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹å­¦ä¹ ã€‚åŸºäºæ­¤èŒƒå¼ï¼Œæå‡ºæ–°å‹ç½‘ç»œPCMambaNetï¼Œç”±Predictive Prior Moduleï¼ˆPPMï¼‰å’ŒCorrective Residual Networkï¼ˆCRNï¼‰ç»„æˆã€‚PPMåˆ©ç”¨è§£å‰–çŸ¥è¯†é¢„æµ‹è¯Šæ–­ç›¸å…³ä¸å¯¹ç§°åŒºåŸŸçš„â€œç„¦ç‚¹å›¾â€ï¼ŒCRNå­¦ä¹ æ®‹å·®è¯¯å·®ï¼Œæé«˜äº†åŒ»å­¦å›¾åƒåˆ†å‰²çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ ä¸­çš„ç«¯å¯¹ç«¯èŒƒå¼è™½ç„¶å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å­˜åœ¨æ”¶æ•›é€Ÿåº¦æ…¢å’Œä¾èµ–å¤§è§„æ¨¡æ•°æ®é›†çš„é—®é¢˜ï¼Œè¿™åœ¨æ•°æ®ç¨€ç¼ºé¢†åŸŸå¦‚åŒ»å­¦æˆåƒä¸­é™åˆ¶äº†å…¶æ•ˆç‡å’Œé€‚ç”¨æ€§ã€‚</li>
<li>å¼•å…¥Predictive-Correctiveï¼ˆPCï¼‰èŒƒå¼ï¼Œä»¥è§£è€¦å»ºæ¨¡ä»»åŠ¡æ¥æ ¹æœ¬æ€§åœ°åŠ é€Ÿå­¦ä¹ ã€‚</li>
<li>PCMambaNetæ–°å‹ç½‘ç»œç”±Predictive Prior Moduleï¼ˆPPMï¼‰å’ŒCorrective Residual Networkï¼ˆCRNï¼‰ä¸¤ä¸ªååŒæ¨¡å—ç»„æˆã€‚</li>
<li>PPMåˆ©ç”¨è§£å‰–çŸ¥è¯†ï¼ˆå¦‚åŒä¾§å¯¹ç§°æ€§ï¼‰é¢„æµ‹ä¸è¯Šæ–­ç›¸å…³çš„ä¸å¯¹ç§°åŒºåŸŸçš„â€œç„¦ç‚¹å›¾â€ï¼Œé™ä½è®¡ç®—æˆæœ¬å¹¶ç¼©å°æœç´¢ç©ºé—´ã€‚</li>
<li>CRNè‡´åŠ›äºå­¦ä¹ æ®‹å·®è¯¯å·®ï¼Œä¸“æ³¨äºç»†åŒ–å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒºåŸŸå¹¶ç²¾ç¡®æç»˜ç—…ç†è¾¹ç•Œã€‚</li>
<li>åœ¨é«˜åˆ†è¾¨ç‡è„‘éƒ¨MRIåˆ†å‰²å®éªŒä¸­ï¼ŒPCMambaNetå®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨ä»…1-5ä¸ªå‘¨æœŸå†…è¾¾åˆ°æ”¶æ•›ï¼Œè¿™æ˜¯ä¼ ç»Ÿç«¯å¯¹ç«¯æ¨¡å‹æ— æ³•å®ç°çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-80d2274079947cda9f2cb2ed0630fbcb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995510&auth_key=1760995510-0-0-d46578bc6f8143d9aba2086715fb20be&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b88eea460ebd92d2c06dde59f416cad0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995517&auth_key=1760995517-0-0-d3b17f774c799ec4ff456e0a9758696a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-266cee3bb6ca21204fa906926eaa585c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995524&auth_key=1760995524-0-0-f78367fca68e6074da63eb962b22990b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a8941a3e7d3f4003c07a019ef7e3dd06~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995531&auth_key=1760995531-0-0-db92352e41df1ea9fd3c5a1413720c12&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Robust-High-Resolution-Multi-Organ-Diffusion-MRI-Using-Synthetic-Data-Tuned-Prompt-Learning"><a href="#Robust-High-Resolution-Multi-Organ-Diffusion-MRI-Using-Synthetic-Data-Tuned-Prompt-Learning" class="headerlink" title="Robust High-Resolution Multi-Organ Diffusion MRI Using   Synthetic-Data-Tuned Prompt Learning"></a>Robust High-Resolution Multi-Organ Diffusion MRI Using   Synthetic-Data-Tuned Prompt Learning</h2><p><strong>Authors:Chen Qian, Haoyu Zhang, Junnan Ma, Liuhong Zhu, Qingrui Cai, Yu Wang, Ruibo Song, Lv Li, Lin Mei, Xianwang Jiang, Qin Xu, Boyu Jiang, Ran Tao, Chunmiao Chen, Shufang Chen, Dongyun Liang, Qiu Guo, Jianzhong Lin, Taishan Kang, Mengtian Lu, Liyuan Fu, Ruibin Huang, Huijuan Wan, Xu Huang, Jianhua Wang, Di Guo, Hai Zhong, Jianjun Zhou, Xiaobo Qu</strong></p>
<p>Clinical adoption of multi-shot diffusion-weighted magnetic resonance imaging (multi-shot DWI) for body-wide tumor diagnostics is limited by severe motion-induced phase artifacts from respiration, peristalsis, and so on, compounded by multi-organ, multi-slice, multi-direction and multi-b-value complexities. Here, we introduce a reconstruction framework, LoSP-Prompt, that overcomes these challenges through physics-informed modeling and synthetic-data-driven prompt learning. We model inter-shot phase variations as a high-order Locally Smooth Phase (LoSP), integrated into a low-rank Hankel matrix reconstruction. Crucially, the algorithmâ€™s rank parameter is automatically set via prompt learning trained exclusively on synthetic abdominal DWI data emulating physiological motion. Validated across 10,000+ clinical images (43 subjects, 4 scanner models, 5 centers), LoSP-Prompt: (1) Achieved twice the spatial resolution of clinical single-shot DWI, enhancing liver lesion conspicuity; (2) Generalized to seven diverse anatomical regions (liver, kidney, sacroiliac, pelvis, knee, spinal cord, brain) with a single model; (3) Outperformed state-of-the-art methods in image quality, artifact suppression, and noise reduction (11 radiologistsâ€™ evaluations on a 5-point scale, $p&lt;0.05$), achieving 4-5 points (excellent) on kidney DWI, 4 points (good to excellent) on liver, sacroiliac and spinal cord DWI, and 3-4 points (good) on knee and tumor brain. The approach eliminates navigator signals and realistic data supervision, providing an interpretable, robust solution for high-resolution multi-organ multi-shot DWI. Its scanner-agnostic performance signifies transformative potential for precision oncology. </p>
<blockquote>
<p>åœ¨ä¸´åºŠé‡‡ç”¨å¤šå›æ‰©æ•£åŠ æƒç£å…±æŒ¯æˆåƒï¼ˆmulti-shot DWIï¼‰è¿›è¡Œå…¨èº«è‚¿ç˜¤è¯Šæ–­æ—¶ï¼Œå—åˆ°å‘¼å¸ã€è •åŠ¨ç­‰å¼•èµ·çš„ä¸¥é‡è¿åŠ¨ç›¸ä½ä¼ªå½±çš„é™åˆ¶ï¼Œå†åŠ ä¸Šå¤šå™¨å®˜ã€å¤šåˆ‡ç‰‡ã€å¤šæ–¹å‘å’Œå¤šbå€¼çš„å¤æ‚æ€§ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é‡å»ºæ¡†æ¶ï¼Œåä¸ºLoSP-Promptï¼Œå®ƒé€šè¿‡ç‰©ç†ä¿¡æ¯å»ºæ¨¡å’Œåˆæˆæ•°æ®é©±åŠ¨æç¤ºå­¦ä¹ æ¥å…‹æœè¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬å°†ä¸åŒå°„å‡»ä¹‹é—´çš„ç›¸ä½å˜åŒ–å»ºæ¨¡ä¸ºé«˜é˜¶å±€éƒ¨å¹³æ»‘ç›¸ä½ï¼ˆLoSPï¼‰ï¼Œå¹¶å°†å…¶é›†æˆåˆ°ä½ç§©HankelçŸ©é˜µé‡å»ºä¸­ã€‚å…³é”®çš„æ˜¯ï¼Œè¯¥ç®—æ³•çš„ç§©å‚æ•°æ˜¯é€šè¿‡ä»…ä½¿ç”¨æ¨¡æ‹Ÿç”Ÿç†è¿åŠ¨çš„åˆæˆè…¹éƒ¨DWIæ•°æ®è¿›è¡Œæç¤ºå­¦ä¹ æ¥è‡ªåŠ¨è®¾ç½®çš„ã€‚åœ¨è¶…è¿‡10,000å¼ ä¸´åºŠå›¾åƒï¼ˆ43åå—è¯•è€…ï¼Œ4ç§æ‰«æä»ªå‹å·ï¼Œ5ä¸ªä¸­å¿ƒï¼‰çš„éªŒè¯ä¸­ï¼ŒLoSP-Promptï¼šï¼ˆ1ï¼‰å®ç°äº†ä¸´åºŠå•å‘DWIä¸¤å€çš„ç©ºé—´åˆ†è¾¨ç‡ï¼Œæé«˜äº†è‚è„ç—…å˜çš„æ¸…æ™°åº¦ï¼›ï¼ˆ2ï¼‰ä½¿ç”¨å•ä¸€æ¨¡å‹æ¨å¹¿åˆ°ä¸ƒä¸ªä¸åŒçš„è§£å‰–åŒºåŸŸï¼ˆè‚è„ã€è‚¾è„ã€éª¨ç›†ã€è†ç›–ã€è„Šé«“ã€å¤§è„‘ï¼‰ï¼›ï¼ˆ3ï¼‰åœ¨å›¾åƒè´¨é‡ã€ä¼ªå½±æŠ‘åˆ¶å’Œé™å™ªæ–¹é¢ä¼˜äºæœ€æ–°æ–¹æ³•ï¼ˆ11åæ”¾å°„ç§‘åŒ»ç”Ÿåœ¨äº”ç‚¹é‡è¡¨ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œp&lt;0.05ï¼‰ï¼Œåœ¨è‚¾è„DWIä¸Šè·å¾—4-5åˆ†ï¼ˆä¼˜ç§€ï¼‰ï¼Œè‚è„ã€éª¨ç›†å’Œè„Šé«“DWIä¸Šè·å¾—4åˆ†ï¼ˆè‰¯å¥½è‡³ä¼˜ç§€ï¼‰ï¼Œè†ç›–å’Œè‚¿ç˜¤å¤§è„‘è·å¾—3-4åˆ†ï¼ˆè‰¯å¥½ï¼‰ã€‚è¯¥æ–¹æ³•æ¶ˆé™¤äº†å¯¼èˆªä¿¡å·å’ŒçœŸå®æ•°æ®ç›‘ç£ï¼Œæä¾›äº†ä¸€ç§å¯è§£é‡Šã€ç¨³å¥çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºé«˜åˆ†è¾¨ç‡å¤šå™¨å®˜å¤šå›DWIã€‚å…¶æ‰«æä»ªæ— å…³çš„æ€§èƒ½è¡¨æ˜å¯¹ç²¾å‡†è‚¿ç˜¤å­¦å…·æœ‰å˜é©æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15400v1">PDF</a> 43 pages, 27 figures</p>
<p><strong>Summary</strong></p>
<p>åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸä¸­ï¼Œé’ˆå¯¹ä½“éƒ¨è‚¿ç˜¤çš„è¯Šæ–­é‡‡ç”¨äº†å¤šç«™å¼æ‰©æ•£åŠ æƒç£å…±æŒ¯æˆåƒï¼ˆmulti-shot DWIï¼‰ï¼Œä½†ç”±äºå‘¼å¸ã€è •åŠ¨ç­‰å¼•èµ·çš„ä¸¥é‡è¿åŠ¨å¼•èµ·çš„ç›¸ä½ä¼ªå½±é™åˆ¶äº†å…¶ä¸´åºŠåº”ç”¨ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é‡å»ºæ¡†æ¶LoSP-Promptï¼Œé€šè¿‡ç‰©ç†ä¿¡æ¯å»ºæ¨¡å’Œåˆæˆæ•°æ®é©±åŠ¨çš„æç¤ºå­¦ä¹ æ¥å…‹æœè¿™äº›æŒ‘æˆ˜ã€‚è¯¥ç®—æ³•å®ç°äº†ä¸´åºŠå•ç«™å¼DWIçš„ä¸¤å€ç©ºé—´åˆ†è¾¨ç‡ï¼Œæé«˜äº†è‚è„ç—…å˜çš„è¾¨è¯†åº¦ï¼Œå¹¶åœ¨å¤šä¸ªè§£å‰–åŒºåŸŸå…·æœ‰å¹¿æ³›åº”ç”¨æ€§ã€‚æ­¤å¤–ï¼Œå…¶åœ¨å›¾åƒè´¨é‡ã€ä¼ªå½±æŠ‘åˆ¶å’Œé™å™ªæ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶åœ¨è‚¾è„DWIä¸Šå–å¾—äº†å“è¶Šè¡¨ç°ã€‚æ­¤æ–¹æ³•æ— éœ€å¯¼èˆªä¿¡å·å’ŒçœŸå®æ•°æ®ç›‘ç£ï¼Œæä¾›äº†ä¸€ç§å¯è§£é‡Šã€ç¨³å¥çš„é«˜åˆ†è¾¨ç‡å¤šç«™å¼DWIè§£å†³æ–¹æ¡ˆï¼Œå¯¹ç²¾ç¡®è‚¿ç˜¤å­¦å…·æœ‰å˜é©æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šç«™å¼æ‰©æ•£åŠ æƒç£å…±æŒ¯æˆåƒï¼ˆmulti-shot DWIï¼‰åœ¨ä¸´åºŠåº”ç”¨ä¸­å—åˆ°é™åˆ¶ï¼Œä¸»è¦ç”±äºè¿åŠ¨å¼•èµ·çš„ç›¸ä½ä¼ªå½±ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é‡å»ºæ¡†æ¶LoSP-Promptï¼Œé€šè¿‡ç‰©ç†ä¿¡æ¯å»ºæ¨¡å’Œåˆæˆæ•°æ®é©±åŠ¨çš„æç¤ºå­¦ä¹ æ¥è§£å†³æŒ‘æˆ˜ã€‚</li>
<li>LoSP-Promptå®ç°äº†ä¸¤å€äºä¸´åºŠå•ç«™å¼DWIçš„ç©ºé—´åˆ†è¾¨ç‡ï¼Œæé«˜äº†è‚è„ç—…å˜çš„è¾¨è¯†åº¦ã€‚</li>
<li>è¯¥æ–¹æ³•å¯åœ¨å¤šä¸ªè§£å‰–åŒºåŸŸå¹¿æ³›åº”ç”¨ï¼ŒåŒ…æ‹¬è‚è„ã€è‚¾è„ã€éª¨ç›†ã€è†ç›–ã€è„Šé«“å’Œå¤§è„‘ã€‚</li>
<li>LoSP-Promptåœ¨å›¾åƒè´¨é‡ã€ä¼ªå½±æŠ‘åˆ¶å’Œé™å™ªæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€å¯¼èˆªä¿¡å·å’ŒçœŸå®æ•°æ®ç›‘ç£ï¼Œæä¾›äº†å¯è§£é‡Šæ€§å’Œç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15400">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-414de3dc81a3f03db1a893d84adc16a4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995540&auth_key=1760995540-0-0-a8661c9ce3cd5c6ec186579c697396ad&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8d6d10692e5f3d1318aeecb71ea15616~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995547&auth_key=1760995547-0-0-e432ef8711796b94c1ba8f6abc71115e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-84f5b894aa3fb4c8829b8afc27c2d956~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995554&auth_key=1760995554-0-0-0b7b2960f2a326525aaef194667ed8e3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0699c72d5801cdf2c8664d3b34edc970~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995561&auth_key=1760995561-0-0-ebb28f52489a58101330020f4da2e832&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7e618d85eb485ce623679ed74c9a3b84~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995568&auth_key=1760995568-0-0-924971d9d8e14edb8c82941eb053bdcf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e2a91fb4a294a9854cc571e41d6545f8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995576&auth_key=1760995576-0-0-545d38aa33fe3f827fa3ff815109938d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9df5348d9e02790fdbe91d826a4b4d2c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995583&auth_key=1760995583-0-0-0757f2d4de7f13594fbcd8869002df7c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Post-Processing-Methods-for-Improving-Accuracy-in-MRI-Inpainting"><a href="#Post-Processing-Methods-for-Improving-Accuracy-in-MRI-Inpainting" class="headerlink" title="Post-Processing Methods for Improving Accuracy in MRI Inpainting"></a>Post-Processing Methods for Improving Accuracy in MRI Inpainting</h2><p><strong>Authors:Nishad Kulkarni, Krithika Iyer, Austin Tapp, Abhijeet Parida, Daniel CapellÃ¡n-MartÃ­n, Zhifan Jiang, MarÃ­a J. Ledesma-Carbayo, Syed Muhammad Anwar, Marius George Linguraru</strong></p>
<p>Magnetic Resonance Imaging (MRI) is the primary imaging modality used in the diagnosis, assessment, and treatment planning for brain pathologies. However, most automated MRI analysis tools, such as segmentation and registration pipelines, are optimized for healthy anatomies and often fail when confronted with large lesions such as tumors. To overcome this, image inpainting techniques aim to locally synthesize healthy brain tissues in tumor regions, enabling the reliable application of general-purpose tools. In this work, we systematically evaluate state-of-the-art inpainting models and observe a saturation in their standalone performance. In response, we introduce a methodology combining model ensembling with efficient post-processing strategies such as median filtering, histogram matching, and pixel averaging. Further anatomical refinement is achieved via a lightweight U-Net enhancement stage. Comprehensive evaluation demonstrates that our proposed pipeline improves the anatomical plausibility and visual fidelity of inpainted regions, yielding higher accuracy and more robust outcomes than individual baseline models. By combining established models with targeted post-processing, we achieve improved and more accessible inpainting outcomes, supporting broader clinical deployment and sustainable, resource-conscious research. Our 2025 BraTS inpainting docker is available at <a target="_blank" rel="noopener" href="https://hub.docker.com/layers/aparida12/brats2025/inpt">https://hub.docker.com/layers/aparida12/brats2025/inpt</a>. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ˜¯è¯Šæ–­ã€è¯„ä¼°å’Œåˆ¶å®šè„‘ç—…ç†æ²»ç–—è®¡åˆ’çš„ä¸»è¦æˆåƒæ–¹å¼ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°è‡ªåŠ¨åŒ–MRIåˆ†æå·¥å…·ï¼Œå¦‚åˆ†å‰²å’Œæ³¨å†Œæµç¨‹ï¼Œéƒ½æ˜¯é’ˆå¯¹å¥åº·ç»“æ„è¿›è¡Œä¼˜åŒ–çš„ï¼Œå½“é¢å¯¹å¤§å‹ç—…å˜ï¼ˆå¦‚è‚¿ç˜¤ï¼‰æ—¶é€šå¸¸ä¼šå¤±æ•ˆã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œå›¾åƒä¿®å¤æŠ€æœ¯æ—¨åœ¨åœ¨è‚¿ç˜¤åŒºåŸŸå±€éƒ¨åˆæˆå¥åº·çš„è„‘ç»„ç»‡ï¼Œä»è€Œèƒ½å¤Ÿå¯é åœ°åº”ç”¨é€šç”¨å·¥å…·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†æœ€å…ˆè¿›çš„ä¿®å¤æ¨¡å‹ï¼Œå¹¶è§‚å¯Ÿåˆ°å…¶ç‹¬ç«‹æ€§èƒ½çš„é¥±å’Œã€‚ä½œä¸ºå›åº”ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆæ¨¡å‹é›†æˆå’Œé«˜æ•ˆåå¤„ç†ç­–ç•¥çš„æ–¹æ³•ï¼Œä¾‹å¦‚ä¸­å€¼æ»¤æ³¢ã€ç›´æ–¹å›¾åŒ¹é…å’Œåƒç´ å¹³å‡ã€‚é€šè¿‡è½»é‡çº§çš„U-Netå¢å¼ºé˜¶æ®µå®ç°äº†è¿›ä¸€æ­¥çš„è§£å‰–å­¦ä¼˜åŒ–ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æµç¨‹æé«˜äº†ä¿®å¤åŒºåŸŸçš„è§£å‰–åˆç†æ€§å’Œè§†è§‰ä¿çœŸåº¦ï¼Œç›¸æ¯”å•ä¸ªåŸºå‡†æ¨¡å‹ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´ç¨³å¥çš„ç»“æœã€‚é€šè¿‡ç»“åˆæ—¢å®šæ¨¡å‹å’Œæœ‰é’ˆå¯¹æ€§çš„åå¤„ç†ï¼Œæˆ‘ä»¬å®ç°äº†æ”¹è¿›å’Œæ›´æ˜“äºè®¿é—®çš„ä¿®å¤ç»“æœï¼Œæ”¯æŒæ›´å¹¿æ³›çš„ä¸´åºŠéƒ¨ç½²å’Œå¯æŒç»­ã€æ³¨é‡èµ„æºçš„ç ”ç©¶ã€‚æˆ‘ä»¬çš„2025å¹´BraTSä¿®å¤dockerå¯åœ¨<a target="_blank" rel="noopener" href="https://hub.docker.com/layers/aparida12/brats2025/inpt%E6%89%BE%E5%88%B0%E3%80%82">https://hub.docker.com/layers/aparida12/brats2025/inptæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15282v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶é’ˆå¯¹ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨è„‘ç—…ç†å­¦è¯Šæ–­ã€è¯„ä¼°å’Œæ²»ç–—è§„åˆ’ä¸­çš„æ ¸å¿ƒåº”ç”¨ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆæ¨¡å‹é›†æˆå’Œé«˜æ•ˆåå¤„ç†ç­–ç•¥çš„å›¾åƒä¿®å¤æ–¹æ³•ã€‚é€šè¿‡ç»“åˆç°æœ‰æ¨¡å‹å’Œåå¤„ç†æµç¨‹çš„ä¼˜åŒ–ï¼Œæ”¹è¿›äº†å›¾åƒä¿®å¤ç»“æœçš„è§£å‰–å­¦åˆç†æ€§å’Œè§†è§‰ä¿çœŸåº¦ï¼Œæé«˜äº†ä¿®å¤åŒºåŸŸçš„å‡†ç¡®æ€§ï¼Œæ”¯æŒæ›´å¹¿æ³›çš„ä¸´åºŠéƒ¨ç½²å’Œèµ„æºèŠ‚çº¦å‹ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MRIæ˜¯è¯Šæ–­ã€è¯„ä¼°å’Œæ²»ç–—è„‘ç—…ç†å­¦çš„å…³é”®æˆåƒæ–¹å¼ã€‚</li>
<li>è‡ªåŠ¨åŒ–MRIåˆ†æå·¥å…·åœ¨é¢ä¸´å¤§å‹ç—…å˜ï¼ˆå¦‚è‚¿ç˜¤ï¼‰æ—¶ç»å¸¸å¤±æ•ˆã€‚</li>
<li>å›¾åƒä¿®å¤æŠ€æœ¯æ—¨åœ¨åˆæˆç—…å˜åŒºåŸŸçš„å¥åº·è„‘ç»„ç»‡ï¼Œä½¿é€šç”¨å·¥å…·å¾—ä»¥å¯é åº”ç”¨ã€‚</li>
<li>å½“å‰æµè¡Œçš„å›¾åƒä¿®å¤æ¨¡å‹æ€§èƒ½å·²æ¥è¿‘é¥±å’Œã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆæ¨¡å‹é›†æˆå’Œé«˜æ•ˆåå¤„ç†ç­–ç•¥çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸­ä½æ•°æ»¤æ³¢ã€ç›´æ–¹å›¾åŒ¹é…å’Œåƒç´ å¹³å‡ç­‰ã€‚</li>
<li>é€šè¿‡è½»é‡çº§çš„U-Netå¢å¼ºé˜¶æ®µè¿›ä¸€æ­¥å®ç°äº†è§£å‰–å­¦çš„ç²¾ç»†åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a5d09303df0623bfc39b50cb85a43dfb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995591&auth_key=1760995591-0-0-5d235b3a2c1dafeddbe632ea2b5ef835&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7181d4e941ae6a0acb4c98999c9945c0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995598&auth_key=1760995598-0-0-3fc93087faf22aeda64d87d6a209e8a7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CARDIUM-Congenital-Anomaly-Recognition-with-Diagnostic-Images-and-Unified-Medical-records"><a href="#CARDIUM-Congenital-Anomaly-Recognition-with-Diagnostic-Images-and-Unified-Medical-records" class="headerlink" title="CARDIUM: Congenital Anomaly Recognition with Diagnostic Images and   Unified Medical records"></a>CARDIUM: Congenital Anomaly Recognition with Diagnostic Images and   Unified Medical records</h2><p><strong>Authors:Daniela Vega, Hannah V. Ceballos, Javier S. Vera, Santiago Rodriguez, Alejandra Perez, Angela Castillo, Maria Escobar, Dario LondoÃ±o, Luis A. Sarmiento, Camila I. Castro, Nadiezhda Rodriguez, Juan C. BriceÃ±o, Pablo ArbelÃ¡ez</strong></p>
<p>Prenatal diagnosis of Congenital Heart Diseases (CHDs) holds great potential for Artificial Intelligence (AI)-driven solutions. However, collecting high-quality diagnostic data remains difficult due to the rarity of these conditions, resulting in imbalanced and low-quality datasets that hinder model performance. Moreover, no public efforts have been made to integrate multiple sources of information, such as imaging and clinical data, further limiting the ability of AI models to support and enhance clinical decision-making. To overcome these challenges, we introduce the Congenital Anomaly Recognition with Diagnostic Images and Unified Medical records (CARDIUM) dataset, the first publicly available multimodal dataset consolidating fetal ultrasound and echocardiographic images along with maternal clinical records for prenatal CHD detection. Furthermore, we propose a robust multimodal transformer architecture that incorporates a cross-attention mechanism to fuse feature representations from image and tabular data, improving CHD detection by 11% and 50% over image and tabular single-modality approaches, respectively, and achieving an F1 score of 79.8 $\pm$ 4.8% in the CARDIUM dataset. We will publicly release our dataset and code to encourage further research on this unexplored field. Our dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/BCVUniandes/Cardium">https://github.com/BCVUniandes/Cardium</a>, and at the project website <a target="_blank" rel="noopener" href="https://bcv-uniandes.github.io/CardiumPage/">https://bcv-uniandes.github.io/CardiumPage/</a> </p>
<blockquote>
<p>å…ˆå¤©æ€§å¿ƒè„ç–¾ç—…ï¼ˆCHDsï¼‰çš„äº§å‰è¯Šæ–­åœ¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰è§£å†³æ–¹æ¡ˆæ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºè¿™äº›ç–¾ç—…çš„ç½•è§æ€§ï¼Œæ”¶é›†é«˜è´¨é‡çš„è¯Šæ–­æ•°æ®ä»ç„¶å¾ˆå›°éš¾ï¼Œå¯¼è‡´æ•°æ®é›†çš„ä¸å¹³è¡¡å’Œè´¨é‡ä½ä¸‹ï¼Œä»è€Œé˜»ç¢äº†æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå°šæœªæœ‰å…¬å¼€çš„åŠªåŠ›æ•´åˆå¤šç§æ¥æºçš„ä¿¡æ¯ï¼Œå¦‚æˆåƒå’Œä¸´åºŠæ•°æ®ï¼Œè¿™è¿›ä¸€æ­¥é™åˆ¶äº†AIæ¨¡å‹åœ¨æ”¯æŒå’Œå¢å¼ºä¸´åºŠå†³ç­–åˆ¶å®šæ–¹é¢çš„èƒ½åŠ›ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†å…ˆå¤©æ€§å¼‚å¸¸è¯†åˆ«ä¸è¯Šæ–­å›¾åƒå’Œç»Ÿä¸€åŒ»ç–—è®°å½•ï¼ˆCARDIUMï¼‰æ•°æ®é›†ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„å¤šæ¨¡å¼æ•°æ®é›†ï¼Œå®ƒæ•´åˆäº†èƒå„¿è¶…å£°å’Œè¶…å£°å¿ƒåŠ¨å›¾åƒä»¥åŠäº§å¦‡ä¸´åºŠè®°å½•ï¼Œç”¨äºäº§å‰CHDæ£€æµ‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¨³å¥çš„å¤šæ¨¡å¼è½¬æ¢å™¨æ¶æ„ï¼Œè¯¥æ¶æ„é‡‡ç”¨äº¤å‰æ³¨æ„æœºåˆ¶æ¥èåˆå›¾åƒå’Œè¡¨æ ¼æ•°æ®çš„ç‰¹å¾è¡¨ç¤ºï¼Œæé«˜äº†CHDæ£€æµ‹çš„å‡†ç¡®æ€§ï¼Œç›¸å¯¹äºå›¾åƒå’Œè¡¨æ ¼å•æ¨¡æ€æ–¹æ³•åˆ†åˆ«æé«˜äº†11%å’Œ50%ï¼Œåœ¨CARDIUMæ•°æ®é›†ä¸Šè¾¾åˆ°äº†79.8Â±4.8%çš„F1åˆ†æ•°ã€‚æˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒæˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç ï¼Œä»¥é¼“åŠ±åœ¨è¿™ä¸ªå°šæœªç ”ç©¶çš„é¢†åŸŸè¿›è¡Œè¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/BCVUniandes/Cardium%E4%BB%A5%E5%8F%8A%E9%A1%B9%E7%9B%AE%E7%BD%91%E7%AB%99https://bcv-uniandes.github.io/CardiumPage%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/BCVUniandes/Cardiumä»¥åŠé¡¹ç›®ç½‘ç«™https://bcv-uniandes.github.io/CardiumPageä¸Šæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15208v1">PDF</a> Accepted to CVAMD Workshop, ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†å…ˆå¤©æ€§å¿ƒè„ç–¾ç—…äº§å‰è¯Šæ–­ä¸­äººå·¥æ™ºèƒ½åº”ç”¨é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é«˜è´¨é‡æ•°æ®é›†éš¾ä»¥è·å–ä»¥åŠç¼ºä¹å¤šæºä¿¡æ¯èåˆçš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæ–‡æœ¬æå‡ºäº†CARDIUMæ•°æ®é›†å’Œå¤šæ¨¡æ€å˜å‹å™¨æ¶æ„ï¼Œèåˆäº†èƒå„¿è¶…å£°å’Œå¿ƒç”µå›¾å›¾åƒä»¥åŠæ¯ä½“ä¸´åºŠè®°å½•æ•°æ®ï¼Œæé«˜äº†å…ˆå¤©æ€§å¿ƒè„ç–¾ç—…çš„æ£€æµ‹æ€§èƒ½ã€‚æ•°æ®é›†å’Œä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œä»¥æ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…ˆå¤©æ€§å¿ƒè„ç–¾ç—…äº§å‰è¯Šæ–­ä¸­äººå·¥æ™ºèƒ½åº”ç”¨é¢ä¸´æ•°æ®é›†è·å–å›°éš¾çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰çš„æ•°æ®é›†å­˜åœ¨ä¸å¹³è¡¡å’Œä½è´¨é‡çš„é—®é¢˜ï¼Œå½±å“äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ç›®å‰ç¼ºä¹èåˆå¤šç§ä¿¡æ¯ï¼ˆå¦‚å½±åƒå’Œä¸´åºŠæ•°æ®ï¼‰çš„å…¬å…±åŠªåŠ›ï¼Œé™åˆ¶äº†äººå·¥æ™ºèƒ½åœ¨æ”¯æŒä¸´åºŠå†³ç­–åˆ¶å®šæ–¹é¢çš„ä½œç”¨ã€‚</li>
<li>ä»‹ç»äº†CARDIUMæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†é¦–æ¬¡å…¬å¼€æä¾›äº†èƒå„¿è¶…å£°å’Œå¿ƒç”µå›¾å›¾åƒä»¥åŠæ¯ä½“ä¸´åºŠè®°å½•æ•°æ®ï¼Œç”¨äºå…ˆå¤©æ€§å¿ƒè„ç–¾ç—…çš„äº§å‰æ£€æµ‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç¨³å¥çš„å¤šæ¨¡æ€å˜å‹å™¨æ¶æ„ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶èåˆå›¾åƒå’Œè¡¨æ ¼æ•°æ®çš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>è¯¥æ¶æ„æé«˜äº†å…ˆå¤©æ€§å¿ƒè„ç–¾ç—…çš„æ£€æµ‹æ€§èƒ½ï¼Œç›¸è¾ƒäºå•æ¨¡æ€æ–¹æ³•ï¼Œæ£€æµ‹æ•ˆæœåˆ†åˆ«æå‡äº†11%å’Œ50%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15208">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-97767cb8957160a2ff51c88816729639~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995605&auth_key=1760995605-0-0-37a9fda051abdfe2c85039d8a66d653b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-50a13677fb759f7da81cb1665ea4e2d6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995612&auth_key=1760995612-0-0-fef0a9886ffd577f51e0f5e9084f7975&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-79c50f63db9d56f14cf4c0c314a6ab42~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995619&auth_key=1760995619-0-0-a9d811116e3199d66692b5d0e72433e3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-119d3bc2b214ae5dc42ea2c5d58931bf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995626&auth_key=1760995626-0-0-cccf20c00cdf12be04eded3788847973&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="HyperAIRI-a-plug-and-play-algorithm-for-precise-hyperspectral-image-reconstruction-in-radio-interferometry"><a href="#HyperAIRI-a-plug-and-play-algorithm-for-precise-hyperspectral-image-reconstruction-in-radio-interferometry" class="headerlink" title="HyperAIRI: a plug-and-play algorithm for precise hyperspectral image   reconstruction in radio interferometry"></a>HyperAIRI: a plug-and-play algorithm for precise hyperspectral image   reconstruction in radio interferometry</h2><p><strong>Authors:Chao Tang, Arwa Dabbech, Adrian Jackson, Yves Wiaux</strong></p>
<p>The next-generation radio-interferometric (RI) telescopes require imaging algorithms capable of forming high-resolution high-dynamic-range images from large data volumes spanning wide frequency bands. Recently, AIRI, a plug-and-play (PnP) approach taking the forward-backward algorithmic structure (FB), has demonstrated state-of-the-art performance in monochromatic RI imaging by alternating a data-fidelity step with a regularisation step via learned denoisers. In this work, we introduce HyperAIRI, its hyperspectral extension, underpinned by learned hyperspectral denoisers enforcing a power-law spectral model. For each spectral channel, the HyperAIRI denoiser takes as input its current image estimate, alongside estimates of its two immediate neighbouring channels and the spectral index map, and provides as output its associated denoised image. To ensure convergence of HyperAIRI, the denoisers are trained with a Jacobian regularisation enforcing non-expansiveness. To accommodate varying dynamic ranges, we assemble a shelf of pre-trained denoisers, each tailored to a specific dynamic range. At each HyperAIRI iteration, the spectral channels of the target image cube are updated in parallel using dynamic-range-matched denoisers from the pre-trained shelf. The denoisers are also endowed with a spatial image faceting functionality, enabling scalability to varied image sizes. Additionally, we formally introduce Hyper-uSARA, a variant of the optimisation-based algorithm HyperSARA, promoting joint sparsity across spectral channels via the l2,1-norm, also adopting FB. We evaluate HyperAIRIâ€™s performance on simulated and real observations. We showcase its superior performance compared to its optimisation-based counterpart Hyper-uSARA, CLEANâ€™s hyperspectral variant in WSClean, and the monochromatic imaging algorithms AIRI and uSARA. </p>
<blockquote>
<p>ä¸‹ä¸€ä»£å°„ç”µå¹²æ¶‰ä»ªï¼ˆRIï¼‰æœ›è¿œé•œéœ€è¦èƒ½å¤Ÿä»è¦†ç›–å®½é¢‘å¸¦çš„å¤§é‡æ•°æ®ä¸­å½¢æˆé«˜åˆ†è¾¨ç‡ã€é«˜åŠ¨æ€èŒƒå›´çš„å›¾åƒçš„æˆåƒç®—æ³•ã€‚æœ€è¿‘ï¼ŒAIRIï¼ˆä¸€ç§é‡‡ç”¨å‰åå‘ç®—æ³•ç»“æ„ï¼ˆFBï¼‰çš„å³æ’å³ç”¨ï¼ˆPnPï¼‰æ–¹æ³•ï¼‰åœ¨å•è‰²RIæˆåƒä¸­å±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œé€šè¿‡æ•°æ®ä¿çœŸæ­¥éª¤å’Œé€šè¿‡å­¦ä¹ å»å™ªå™¨è¿›è¡Œçš„æ­£åˆ™åŒ–æ­¥éª¤çš„äº¤æ›¿è¿›è¡Œã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†HyperAIRIï¼Œå…¶è¶…å…‰è°±æ‰©å±•ç‰ˆæœ¬ï¼Œç”±å­¦ä¹ è¶…å…‰è°±å»å™ªå™¨æ”¯æŒï¼Œå¼ºåˆ¶å®æ–½å¹‚å¾‹å…‰è°±æ¨¡å‹ã€‚å¯¹äºæ¯ä¸ªå…‰è°±é€šé“ï¼ŒHyperAIRIå»å™ªå™¨ä»¥å½“å‰å›¾åƒä¼°è®¡å€¼ã€å…¶ä¸¤ä¸ªé‚»è¿‘é€šé“å’Œå…‰è°±æŒ‡æ•°å›¾çš„ä¼°è®¡å€¼ä¸ºè¾“å…¥ï¼Œå¹¶æä¾›ç›¸å…³çš„é™å™ªå›¾åƒä½œä¸ºè¾“å‡ºã€‚ä¸ºç¡®ä¿HyperAIRIçš„æ”¶æ•›æ€§ï¼Œå»å™ªå™¨é‡‡ç”¨é›…å¯æ¯”æ­£åˆ™åŒ–æ¥å¼ºåˆ¶æ‰§è¡Œéè†¨èƒ€æ€§è¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†é€‚åº”ä¸åŒçš„åŠ¨æ€èŒƒå›´ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€æ’é¢„å…ˆè®­ç»ƒçš„å»å™ªå™¨ï¼Œæ¯ä¸ªå»å™ªå™¨é’ˆå¯¹ç‰¹å®šçš„åŠ¨æ€èŒƒå›´å®šåˆ¶ã€‚åœ¨æ¯æ¬¡HyperAIRIè¿­ä»£ä¸­ï¼Œç›®æ ‡å›¾åƒç«‹æ–¹ä½“çš„å…‰è°±é€šé“ä½¿ç”¨ä¸åŠ¨æ€èŒƒå›´ç›¸åŒ¹é…çš„å»å™ªå™¨å¹¶è¡Œæ›´æ–°ï¼Œè¿™äº›å»å™ªå™¨æ¥è‡ªé¢„å…ˆè®­ç»ƒçš„è´§æ¶ã€‚æ­¤å¤–ï¼Œå»å™ªå™¨è¿˜é…å¤‡äº†ç©ºé—´å›¾åƒåˆ‡ç‰‡åŠŸèƒ½ï¼Œå¯å®ç°å„ç§å›¾åƒå¤§å°çš„æ‰©å±•æ€§ã€‚å¦å¤–ï¼Œæˆ‘ä»¬æ­£å¼ä»‹ç»äº†åŸºäºä¼˜åŒ–ç®—æ³•çš„HyperSARAçš„å˜ä½“Hyper-uSARAï¼Œå®ƒé€šè¿‡l2,1èŒƒæ•°ä¿ƒè¿›å…‰è°±é€šé“ä¹‹é—´çš„è”åˆç¨€ç–æ€§ï¼ŒåŒæ ·é‡‡ç”¨FBã€‚æˆ‘ä»¬å¯¹HyperAIRIçš„æ€§èƒ½è¿›è¡Œäº†æ¨¡æ‹Ÿå’ŒçœŸå®è§‚æµ‹è¯„ä¼°ã€‚æˆ‘ä»¬å±•ç¤ºäº†å…¶ç›¸è¾ƒäºä¼˜åŒ–å‹ç®—æ³•Hyper-uSARAã€WSCleanä¸­çš„CLEANè¶…å…‰è°±å˜ä½“ä»¥åŠå•è‰²æˆåƒç®—æ³•AIRIå’ŒuSARAçš„å“è¶Šæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15198v1">PDF</a> 18 pages, 10 figures, submitted to MNRAS</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–°ä¸€ä»£å¹²æ¶‰ä»ªæœ›è¿œé•œéœ€è¦èƒ½å¤Ÿä»å¤§ä½“ç§¯æ•°æ®å½¢æˆé«˜åˆ†è¾¨ç‡é«˜åŠ¨æ€èŒƒå›´å›¾åƒçš„æˆåƒç®—æ³•ã€‚æœ€è¿‘ï¼ŒAIRIé‡‡ç”¨å‰åå‘ç®—æ³•ç»“æ„ï¼ˆFBï¼‰çš„å³æ’å³ç”¨ï¼ˆPnPï¼‰æ–¹æ³•ï¼Œé€šè¿‡äº¤æ›¿æ•°æ®ä¿çœŸæ­¥éª¤å’Œé€šè¿‡å­¦ä¹ å»å™ªå™¨çš„æ­£åˆ™åŒ–æ­¥éª¤ï¼Œåœ¨å•è‰²å¹²æ¶‰ä»ªæˆåƒä¸­å±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†HyperAIRIåŠå…¶è¶…å…‰è°±æ‰©å±•ï¼Œä»¥å­¦ä¹ è¶…å…‰è°±å»å™ªå™¨ä¸ºæ”¯æ’‘ï¼Œå¼ºåˆ¶å®æ–½åŠŸç‡å¾‹å…‰è°±æ¨¡å‹ã€‚å¯¹äºæ¯ä¸ªå…‰è°±é€šé“ï¼ŒHyperAIRIå»å™ªå™¨ä»¥å…¶å½“å‰å›¾åƒä¼°è®¡ã€å…¶ä¸¤ä¸ªé‚»è¿‘é€šé“ä¼°è®¡å’Œå…‰è°±æŒ‡æ•°å›¾ä½œä¸ºè¾“å…¥ï¼Œå¹¶æä¾›ç›¸åº”çš„å»å™ªå›¾åƒä½œä¸ºè¾“å‡ºã€‚ä¸ºç¡®ä¿HyperAIRIçš„æ”¶æ•›æ€§ï¼Œä½¿ç”¨é›…å¯æ¯”æ­£åˆ™åŒ–è®­ç»ƒå»å™ªå™¨ä»¥å¼ºåˆ¶å…¶éè†¨èƒ€æ€§ã€‚ä¸ºé€‚åº”ä¸åŒçš„åŠ¨æ€èŒƒå›´ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€æ’é¢„è®­ç»ƒçš„å»å™ªå™¨ï¼Œæ¯ä¸ªå»å™ªå™¨é’ˆå¯¹ç‰¹å®šçš„åŠ¨æ€èŒƒå›´ã€‚åœ¨HyperAIRIçš„æ¯æ¬¡è¿­ä»£ä¸­ï¼Œç›®æ ‡å›¾åƒç«‹æ–¹ä½“çš„å…‰è°±é€šé“ä½¿ç”¨ä¸åŠ¨æ€èŒƒå›´åŒ¹é…çš„å»å™ªå™¨ä»é¢„è®­ç»ƒè´§æ¶ä¸­è¿›è¡Œæ›´æ–°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ­£å¼æ¨å‡ºäº†åŸºäºä¼˜åŒ–çš„ç®—æ³•HyperSARAçš„å˜ä½“Hyper-uSARAï¼Œå®ƒé€šè¿‡l2,1èŒƒæ•°ä¿ƒè¿›å…‰è°±é€šé“çš„è”åˆç¨€ç–æ€§ï¼Œä¹Ÿé‡‡ç”¨FBã€‚æˆ‘ä»¬é€šè¿‡æ¨¡æ‹Ÿå’ŒçœŸå®è§‚æµ‹å¯¹HyperAIRIçš„æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ï¼Œå±•ç¤ºäº†å…¶ç›¸å¯¹äºåŸºäºä¼˜åŒ–çš„Hyper-uSARAã€WSCleanä¸­çš„CLEANè¶…å…‰è°±å˜ä½“å’Œå•è‰²æˆåƒç®—æ³•AIRIå’ŒuSARAçš„å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä¸‹ä¸€ä»£å¹²æ¶‰ä»ªæœ›è¿œé•œéœ€è¦èƒ½å¤Ÿä»å¤§è§„æ¨¡æ•°æ®ä¸­å½¢æˆé«˜åˆ†è¾¨ç‡ã€é«˜åŠ¨æ€èŒƒå›´çš„å›¾åƒã€‚</li>
<li>AIRIé‡‡ç”¨å‰åå‘ç®—æ³•ç»“æ„ï¼ˆFBï¼‰å·²å±•ç¤ºäº†å…¶åœ¨å•è‰²å¹²æ¶‰ä»ªæˆåƒä¸­çš„å“è¶Šæ€§èƒ½ã€‚</li>
<li>HyperAIRIæ˜¯AIRIçš„è¶…å…‰è°±æ‰©å±•ï¼Œåˆ©ç”¨å­¦ä¹ åˆ°çš„è¶…å…‰è°±å»å™ªå™¨ï¼Œå¹¶å¼ºåˆ¶å®æ–½åŠŸç‡å¾‹å…‰è°±æ¨¡å‹ã€‚</li>
<li>HyperAIRIå»å™ªå™¨èƒ½å¤Ÿæ ¹æ®å½“å‰å›¾åƒä¼°è®¡ã€é‚»è¿‘é€šé“ä¼°è®¡å’Œå…‰è°±æŒ‡æ•°å›¾æ¥ä¸ºæ¯ä¸ªå…‰è°±é€šé“ç”Ÿæˆå»å™ªå›¾åƒã€‚</li>
<li>é€šè¿‡é›…å¯æ¯”æ­£åˆ™åŒ–ç¡®ä¿HyperAIRIçš„æ”¶æ•›æ€§ï¼Œå¹¶ä¸”é€šè¿‡è®­ç»ƒå»å™ªå™¨ä½¿å…¶å…·æœ‰éè†¨èƒ€æ€§ã€‚</li>
<li>ä¸ºé€‚åº”ä¸åŒçš„åŠ¨æ€èŒƒå›´ï¼Œä½¿ç”¨ä¸€æ’é¢„è®­ç»ƒçš„å»å™ªå™¨ï¼Œæ¯ä¸ªé’ˆå¯¹ç‰¹å®šåŠ¨æ€èŒƒå›´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15198">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-94bc2526207206abafc1739f635a9811~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995634&auth_key=1760995634-0-0-85f04b3e22b8a562c3c3ac41f4d32323&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0f7a5f9aca2fae631ce8e45038588caa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995641&auth_key=1760995641-0-0-91985bbaf3c5394820650c94bd6d6cb5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-337dced968c27684db47b45b9cb5e6c3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995648&auth_key=1760995648-0-0-722ff0bb87595cb0bbfa013b20279654&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Comprehensive-language-image-pre-training-for-3D-medical-image-understanding"><a href="#Comprehensive-language-image-pre-training-for-3D-medical-image-understanding" class="headerlink" title="Comprehensive language-image pre-training for 3D medical image   understanding"></a>Comprehensive language-image pre-training for 3D medical image   understanding</h2><p><strong>Authors:Tassilo Wald, Ibrahim Ethem Hamamci, Yuan Gao, Sam Bond-Taylor, Harshita Sharma, Maximilian Ilse, Cynthia Lo, Olesya Melnichenko, Noel C. F. Codella, Maria Teodora Wetscherek, Klaus H. Maier-Hein, Panagiotis Korfiatis, Valentina Salvatelli, Javier Alvarez-Valle, Fernando PÃ©rez-GarcÃ­a</strong></p>
<p>Vision-language pre-training, i.e., aligning images with paired text, is a powerful paradigm to create encoders that can be directly used for tasks such as classification and retrieval, and for downstream tasks such as segmentation and report generation. In the 3D medical image domain, these capabilities allow vision-language encoders (VLEs) to support radiologists by retrieving patients with similar abnormalities or predicting likelihoods of abnormality. While the methodology holds promise, data availability limits the capabilities of current 3D VLEs.   In this paper, we alleviate the lack of data by injecting additional inductive biases: introducing a report generation objective and pairing vision-language pre-training with vision-only pre-training. This allows us to leverage both image-only and paired image-text 3D datasets, increasing the total amount of data to which our model is exposed. Through these additional inductive biases, paired with best practices of the 3D medical imaging domain, we develop the Comprehensive Language-image Pre-training (COLIPRI) encoder family. Our COLIPRI encoders achieve state-of-the-art performance in report generation, classification probing, and zero-shot classification, and remain competitive for semantic segmentation. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼Œå³å›¾åƒä¸é…å¯¹æ–‡æœ¬çš„å¯¹é½ï¼Œæ˜¯ä¸€ç§å¼ºå¤§çš„èŒƒå¼ï¼Œå¯ä»¥åˆ›å»ºç¼–ç å™¨ï¼Œè¿™äº›ç¼–ç å™¨å¯ä»¥ç›´æ¥ç”¨äºåˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡ï¼Œä»¥åŠç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚åˆ†å‰²å’ŒæŠ¥å‘Šç”Ÿæˆã€‚åœ¨3DåŒ»å­¦å›¾åƒé¢†åŸŸï¼Œè¿™äº›åŠŸèƒ½ä½¿è§†è§‰è¯­è¨€ç¼–ç å™¨ï¼ˆVLEï¼‰èƒ½å¤Ÿé€šè¿‡æ£€ç´¢å…·æœ‰ç±»ä¼¼å¼‚å¸¸çš„æ‚£è€…æˆ–é¢„æµ‹å¼‚å¸¸çš„å¯èƒ½æ€§æ¥æ”¯æŒæ”¾å°„ç§‘åŒ»ç”Ÿã€‚è™½ç„¶è¿™ç§æ–¹æ³•å…·æœ‰æ½œåŠ›ï¼Œä½†æ•°æ®å¯ç”¨æ€§é™åˆ¶äº†å½“å‰3D VLEçš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15042v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†è§‰è¯­è¨€é¢„è®­ç»ƒåœ¨åŒ»å­¦å›¾åƒé¢†åŸŸçš„åº”ç”¨ã€‚é€šè¿‡å¼•å…¥æŠ¥å‘Šç”Ÿæˆç›®æ ‡å’Œç»“åˆè§†è§‰è¯­è¨€é¢„è®­ç»ƒå’Œè§†è§‰é¢„è®­ç»ƒï¼Œç¼“è§£äº†æ•°æ®å¯ç”¨æ€§é—®é¢˜ã€‚é€šè¿‡å¼•å…¥é¢å¤–çš„å½’çº³åè§å’Œæœ€ä½³å®è·µï¼Œå¼€å‘å‡ºCOLIPRIç¼–ç å™¨å®¶æ—ï¼Œåœ¨æŠ¥å‘Šç”Ÿæˆã€åˆ†ç±»æ¢æµ‹å’Œé›¶æ ·æœ¬åˆ†ç±»æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨è¯­ä¹‰åˆ†å‰²æ–¹é¢ä¿æŒç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸå…·æœ‰åˆ›å»ºç¼–ç å™¨çš„æ½œåŠ›ï¼Œå¯ç›´æ¥ç”¨äºåˆ†ç±»ã€æ£€ç´¢ä»¥åŠåˆ†å‰²å’ŒæŠ¥å‘Šç”Ÿæˆç­‰ä»»åŠ¡ã€‚</li>
<li>åœ¨3DåŒ»å­¦å›¾åƒé¢†åŸŸï¼ŒVLPèƒ½å¤Ÿæ”¯æŒæ”¾å°„ç§‘åŒ»ç”Ÿé€šè¿‡æ£€ç´¢å…·æœ‰ç›¸ä¼¼å¼‚å¸¸çš„æ‚£è€…æˆ–é¢„æµ‹å¼‚å¸¸çš„å¯èƒ½æ€§æ¥è¾…åŠ©è¯Šæ–­ã€‚</li>
<li>å½“å‰3D VLEçš„èƒ½åŠ›å—åˆ°æ•°æ®å¯ç”¨æ€§çš„é™åˆ¶ã€‚</li>
<li>å¼•å…¥æŠ¥å‘Šç”Ÿæˆç›®æ ‡å’Œç»“åˆè§†è§‰è¯­è¨€é¢„è®­ç»ƒä¸è§†è§‰é¢„è®­ç»ƒï¼Œä»¥ç¼“è§£æ•°æ®å¯ç”¨æ€§é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å›¾åƒå’Œé…å¯¹å›¾åƒæ–‡æœ¬3Dæ•°æ®é›†ã€‚</li>
<li>é€šè¿‡é¢å¤–çš„å½’çº³åè§å’Œæœ€ä½³å®è·µï¼Œå¼€å‘å‡ºCOLIPRIç¼–ç å™¨å®¶æ—ã€‚</li>
<li>COLIPRIç¼–ç å™¨åœ¨æŠ¥å‘Šç”Ÿæˆã€åˆ†ç±»æ¢æµ‹å’Œé›¶æ ·æœ¬åˆ†ç±»æ–¹é¢å–å¾—æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d7d084830e0c4cf0f937daf91df32314~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995655&auth_key=1760995655-0-0-fc41078cd26cedd761e63479c7e2fafd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-65797676c5d45ef3a39e30d39ed77850~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995662&auth_key=1760995662-0-0-79a60d604ef0454f67ba23e3535d5c7a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d354db741ad3e55a4cd94119bac5978c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995668&auth_key=1760995668-0-0-4c17474a60bf5a9285b6d1e7c4daf6ac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-396f366a6619b74aba143975d55927ee~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995675&auth_key=1760995675-0-0-9eee2344e59ba4a41d51bcc72b4df306&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6303f794e43287161a4f4f18b9407e50~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995682&auth_key=1760995682-0-0-eeafaba2ee66ea4bb3b76ffa718b2a33&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DCMIL-A-Progressive-Representation-Learning-of-Whole-Slide-Images-for-Cancer-Prognosis-Analysis"><a href="#DCMIL-A-Progressive-Representation-Learning-of-Whole-Slide-Images-for-Cancer-Prognosis-Analysis" class="headerlink" title="DCMIL: A Progressive Representation Learning of Whole Slide Images for   Cancer Prognosis Analysis"></a>DCMIL: A Progressive Representation Learning of Whole Slide Images for   Cancer Prognosis Analysis</h2><p><strong>Authors:Chao Tu, Kun Huang, Jie Zhang, Qianjin Feng, Yu Zhang, Zhenyuan Ning</strong></p>
<p>The burgeoning discipline of computational pathology shows promise in harnessing whole slide images (WSIs) to quantify morphological heterogeneity and develop objective prognostic modes for human cancers. However, progress is impeded by the computational bottleneck of gigapixel-size inputs and the scarcity of dense manual annotations. Current methods often overlook fine-grained information across multi-magnification WSIs and variations in tumor microenvironments. Here, we propose an easy-to-hard progressive representation learning, termed dual-curriculum contrastive multi-instance learning (DCMIL), to efficiently process WSIs for cancer prognosis. The model does not rely on dense annotations and enables the direct transformation of gigapixel-size WSIs into outcome predictions. Extensive experiments on twelve cancer types (5,954 patients, 12.54 million tiles) demonstrate that DCMIL outperforms standard WSI-based prognostic models. Additionally, DCMIL identifies fine-grained prognosis-salient regions, provides robust instance uncertainty estimation, and captures morphological differences between normal and tumor tissues, with the potential to generate new biological insights. All codes have been made publicly accessible at <a target="_blank" rel="noopener" href="https://github.com/tuuuc/DCMIL">https://github.com/tuuuc/DCMIL</a>. </p>
<blockquote>
<p>è®¡ç®—ç—…ç†å­¦è¿™ä¸€æ–°å…´å­¦ç§‘æ˜¾ç¤ºå‡ºåˆ©ç”¨å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰è¿›è¡Œå½¢æ€å­¦å¼‚è´¨æ€§é‡åŒ–ï¼Œå¹¶ä¸ºäººç±»ç™Œç—‡å‘å±•å®¢è§‚é¢„åæ¨¡å¼çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºåƒå…†åƒç´ å¤§å°çš„è¾“å…¥çš„è®¡ç®—ç“¶é¢ˆä»¥åŠå¯†é›†æ‰‹åŠ¨æ³¨é‡Šçš„ç¨€ç¼ºæ€§ï¼Œè¿›å±•å—åˆ°äº†é˜»ç¢ã€‚å½“å‰çš„æ–¹æ³•å¸¸å¸¸å¿½ç•¥äº†å¤šå€æ”¾å¤§WSIä¸­çš„ç²¾ç»†é¢—ç²’ä¿¡æ¯å’Œè‚¿ç˜¤å¾®ç¯å¢ƒçš„å·®å¼‚ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»æ˜“åˆ°éš¾çš„æ¸è¿›è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºåŒè¯¾ç¨‹å¯¹æ¯”å¤šå®ä¾‹å­¦ä¹ ï¼ˆDCMILï¼‰ï¼Œä»¥æœ‰æ•ˆåœ°å¤„ç†WSIè¿›è¡Œç™Œç—‡é¢„åã€‚è¯¥æ¨¡å‹ä¸ä¾èµ–äºå¯†é›†æ³¨é‡Šï¼Œèƒ½å¤Ÿå°†åƒå…†åƒç´ å¤§å°çš„WSIç›´æ¥è½¬æ¢ä¸ºç»“æœé¢„æµ‹ã€‚åœ¨12ç§ç™Œç—‡ç±»å‹ï¼ˆ5954åæ‚£è€…ï¼Œ1254ä¸‡å—ç“·ç –ï¼‰ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDCMILçš„æ€§èƒ½ä¼˜äºåŸºäºWSIçš„æ ‡å‡†é¢„åæ¨¡å‹ã€‚æ­¤å¤–ï¼ŒDCMILèƒ½å¤Ÿè¯†åˆ«ç²¾ç»†çš„é¢„åæ˜¾è‘—åŒºåŸŸï¼Œæä¾›ç¨³å¥çš„å®ä¾‹ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œå¹¶æ•æ‰æ­£å¸¸å’Œè‚¿ç˜¤ç»„ç»‡ä¹‹é—´çš„å½¢æ€å·®å¼‚ï¼Œå…·æœ‰äº§ç”Ÿæ–°çš„ç”Ÿç‰©å­¦è§è§£çš„æ½œåŠ›ã€‚æ‰€æœ‰ä»£ç å‡å·²å…¬å¼€å¯è®¿é—® <a target="_blank" rel="noopener" href="https://github.com/tuuuc/DCMIL%E3%80%82">https://github.com/tuuuc/DCMILã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14403v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è®¡ç®—ç—…ç†å­¦åœ¨å¤„ç†å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIsï¼‰æ–¹é¢çš„æ½œåŠ›ï¼Œé€šè¿‡é‡åŒ–å½¢æ€å­¦å¼‚è´¨æ€§å¹¶å¼€å‘é’ˆå¯¹äººç±»ç™Œç—‡çš„å®¢è§‚é¢„åæ¨¡å¼ã€‚ä¸ºè§£å†³å¤§è§„æ¨¡æ•°æ®è¾“å…¥çš„ç“¶é¢ˆåŠç¨€ç–çš„æ‰‹åŠ¨æ ‡æ³¨é—®é¢˜ï¼Œæå‡ºä¸€ç§ç”±æ˜“åˆ°éš¾çš„æ¸è¿›å¼è¡¨ç¤ºå­¦ä¹ æ–¹æ³•â€”â€”åŒè¯¾ç¨‹å¯¹æ¯”å¤šå®ä¾‹å­¦ä¹ ï¼ˆDCMILï¼‰ã€‚è¯¥æ–¹æ³•æ— éœ€ä¾èµ–å¯†é›†æ ‡æ³¨ï¼Œå¯ç›´æ¥å°†å¤§è§„æ¨¡å›¾åƒè½¬æ¢ä¸ºé¢„åé¢„æµ‹ç»“æœã€‚å®éªŒè¯æ˜ï¼ŒDCMILåœ¨å¤šç§ç™Œç—‡ç±»å‹ä¸Šçš„è¡¨ç°ä¼˜äºæ ‡å‡†WSIé¢„åæ¨¡å‹ï¼Œå¹¶èƒ½è¯†åˆ«å‡ºä¸é¢„åç›¸å…³çš„ç»†å¾®åŒºåŸŸï¼Œæä¾›ç¨³å¥çš„å®ä¾‹ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œæ•æ‰æ­£å¸¸ä¸è‚¿ç˜¤ç»„ç»‡é—´çš„å½¢æ€å·®å¼‚ï¼Œå…·æœ‰ç”Ÿæˆæ–°ç”Ÿç‰©å­¦è§è§£çš„æ½œåŠ›ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€åˆ†äº«è‡³GitHubã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—ç—…ç†å­¦åˆ©ç”¨å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIsï¼‰åœ¨ç™Œç—‡çš„å®¢è§‚é¢„åæ¨¡å¼æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>å¤„ç†å¤§è§„æ¨¡æ•°æ®è¾“å…¥çš„ç“¶é¢ˆå’Œæ‰‹åŠ¨æ ‡æ³¨ç¨€ç¼ºæ€§æ˜¯ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>åŒè¯¾ç¨‹å¯¹æ¯”å¤šå®ä¾‹å­¦ä¹ ï¼ˆDCMILï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆçš„å¤„ç†å…¨åˆ‡ç‰‡å›¾åƒçš„æ–¹æ³•ï¼Œæ— éœ€å¯†é›†æ ‡æ³¨ã€‚</li>
<li>DCMILå¯ç›´æ¥å°†å¤§è§„æ¨¡å›¾åƒè½¬æ¢ä¸ºé¢„åé¢„æµ‹ç»“æœï¼Œè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>DCMILèƒ½å¤Ÿè¯†åˆ«ä¸é¢„åç›¸å…³çš„ç»†å¾®åŒºåŸŸï¼Œæä¾›ç¨³å¥çš„å®ä¾‹ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</li>
<li>DCMILèƒ½å¤Ÿæ•æ‰æ­£å¸¸ä¸è‚¿ç˜¤ç»„ç»‡é—´çš„å½¢æ€å·®å¼‚ï¼Œæœ‰åŠ©äºç”Ÿæˆæ–°çš„ç”Ÿç‰©å­¦è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14403">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c5986cb48a7ab81c1ad7ba7e11ff8267~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995689&auth_key=1760995689-0-0-c5ba737b1d5e63102782b02bf45d6cdf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-652f2dfcb155c4e7f40e06f8eb09033c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995697&auth_key=1760995697-0-0-bb17700ef7280cd40617cb406593de67&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-69f0751eb6b9523830c8556c92eccd48~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995704&auth_key=1760995704-0-0-8efedc5cf76f3275f9905ef35958e5e9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f632cd6a819ae2e60fd9cc6484115de3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995711&auth_key=1760995711-0-0-9bdae1da92dc9a531e8b5dcd0dfec464&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="JEDA-Query-Free-Clinical-Order-Search-from-Ambient-Dialogues"><a href="#JEDA-Query-Free-Clinical-Order-Search-from-Ambient-Dialogues" class="headerlink" title="JEDA: Query-Free Clinical Order Search from Ambient Dialogues"></a>JEDA: Query-Free Clinical Order Search from Ambient Dialogues</h2><p><strong>Authors:Praphul Singh, Corey Barrett, Sumana Srivasta, Amitabh Saikia, Irfan Bulu, Sri Gadde, Krishnaram Kenthapadi</strong></p>
<p>Clinical conversations mix explicit directives (order a chest X-ray) with implicit reasoning (the cough worsened overnight, we should check for pneumonia). Many systems rely on LLM rewriting, adding latency, instability, and opacity that hinder real-time ordering. We present JEDA (Joint Embedding for Direct and Ambient clinical orders), a domain-initialized bi-encoder that retrieves canonical orders directly and, in a query-free mode, encodes a short rolling window of ambient dialogue to trigger retrieval. Initialized from PubMedBERT and fine-tuned with a duplicate-safe contrastive objective, JEDA aligns heterogeneous expressions of intent to shared order concepts. Training uses constrained LLM guidance to tie each signed order to complementary formulations (command only, context only, command+context, context+reasoning), producing clearer inter-order separation, tighter query extendash order coupling, and stronger generalization. The query-free mode is noise-resilient, reducing sensitivity to disfluencies and ASR errors by conditioning on a short window rather than a single utterance. Deployed in practice, JEDA yields large gains and substantially outperforms its base encoder and recent open embedders (Linq Embed Mistral, SFR Embedding, GTE Qwen, BGE large, Embedding Gemma). The result is a fast, interpretable, LLM-free retrieval layer that links ambient context to actionable clinical orders in real time. </p>
<blockquote>
<p>ä¸´åºŠå¯¹è¯ä¸­èåˆäº†æ˜ç¡®çš„æŒ‡ç¤ºï¼ˆå¦‚è¿›è¡Œèƒ¸éƒ¨Xå…‰æ£€æŸ¥ï¼‰ä¸éšæ€§çš„æ¨ç†ï¼ˆå’³å—½ä¸€æ•´å¤œåŠ é‡ï¼Œæˆ‘ä»¬åº”æ£€æŸ¥è‚ºç‚ï¼‰ã€‚è®¸å¤šç³»ç»Ÿä¾èµ–äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œé‡å†™ï¼Œå¢åŠ äº†å»¶è¿Ÿã€ä¸ç¨³å®šå’Œé€æ˜åº¦ä¸è¶³çš„é—®é¢˜ï¼Œé˜»ç¢äº†å®æ—¶æ’åºã€‚æˆ‘ä»¬æå‡ºäº†JEDAï¼ˆç”¨äºç›´æ¥å’Œå‘¨å›´ä¸´åºŠè®¢å•çš„è”åˆåµŒå…¥ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸŸåˆå§‹åŒ–åŒç¼–ç å™¨ï¼Œå®ƒå¯ä»¥ç›´æ¥æ£€ç´¢è§„èŒƒè®¢å•ï¼Œå¹¶åœ¨æ— æŸ¥è¯¢æ¨¡å¼ä¸‹ï¼Œå¯¹å‘¨å›´çš„çŸ­æœŸå¯¹è¯çª—å£è¿›è¡Œç¼–ç ï¼Œä»¥è§¦å‘æ£€ç´¢ã€‚JEDAä½¿ç”¨PubMedBERTè¿›è¡Œåˆå§‹åŒ–ï¼Œå¹¶ä½¿ç”¨å…·æœ‰é‡å¤å®‰å…¨å¯¹æ¯”ç›®æ ‡çš„æ–¹æ³•è¿›è¡Œå¾®è°ƒï¼Œå°†ä¸åŒçš„æ„å›¾è¡¨è¾¾ä¸å…±äº«è®¢å•æ¦‚å¿µå¯¹é½ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨å—é™åˆ¶çš„å¤§å‹è¯­è¨€æ¨¡å‹æŒ‡å¯¼ï¼Œå°†æ¯ä¸ªå·²ç­¾ç½²çš„è®¢å•ä¸è¡¥å……é…æ–¹ï¼ˆä»…å‘½ä»¤ã€ä»…ä¸Šä¸‹æ–‡ã€å‘½ä»¤+ä¸Šä¸‹æ–‡ã€ä¸Šä¸‹æ–‡+æ¨ç†ï¼‰ç›¸å…³è”ï¼Œäº§ç”Ÿæ›´æ¸…æ™°çš„è®¢å•é—´åˆ†ç¦»ã€æ›´ç´§å¯†çš„æŸ¥è¯¢æ‰©å±•ä¸è®¢å•è€¦åˆä»¥åŠæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æ— æŸ¥è¯¢æ¨¡å¼æ˜¯å™ªå£°æŠ—æ‰°çš„ï¼Œé€šè¿‡åŸºäºçŸ­æœŸçª—å£è€Œä¸æ˜¯å•ä¸ªè¯è¯­è¿›è¡Œæ¡ä»¶åŒ–ï¼Œå‡å°‘äº†å¯¹æŠ—å‘éŸ³é”™è¯¯å’Œè¯­éŸ³è¯†åˆ«é”™è¯¯çš„æ•æ„Ÿæ€§ã€‚åœ¨å®è·µä¸­éƒ¨ç½²æ—¶ï¼ŒJEDAäº§ç”Ÿäº†å·¨å¤§çš„æ”¶ç›Šï¼Œå¹¶æ˜¾è‘—ä¼˜äºå…¶åŸºç¡€ç¼–ç å™¨ä»¥åŠæœ€è¿‘çš„å¼€æ”¾åµŒå…¥å™¨ï¼ˆLinq Embed Mistralã€SFR Embeddingã€GTE Qwenã€BGE largeã€Embedding Gemmaï¼‰ã€‚å…¶ç»“æœæ˜¯å¿«é€Ÿã€å¯è§£é‡Šã€æ— éœ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ£€ç´¢å±‚ï¼Œè¯¥å±‚èƒ½å¤Ÿå®æ—¶é“¾æ¥å‘¨å›´ä¸Šä¸‹æ–‡å¹¶é‡‡å–è¡ŒåŠ¨çš„ä¸´åºŠè®¢å•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14169v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åä¸ºJEDAçš„è”åˆåµŒå…¥æŠ€æœ¯ï¼Œç”¨äºç›´æ¥å’Œé—´æ¥ä¸´åºŠè®¢å•å¤„ç†ã€‚é€šè¿‡ç»“åˆå…¬å¼€åŒ»ç–—è¯­è¨€æ¨¡å‹å’Œè®­ç»ƒè¿‡ç¨‹ï¼Œå®ç°é«˜æ•ˆçš„è‡ªåŠ¨åŒ»ç–—æŒ‡ä»¤ç³»ç»Ÿï¼Œä»¥æ›´å¿«åœ°è¿æ¥å®æ—¶åœºæ™¯çš„èƒŒæ™¯ä¸åŠ¨ä½œåŒ–åŒ»ç–—æŒ‡ä»¤ï¼Œé™ä½å»¶è¿Ÿå’Œä¸ç¨³å®šæ€§ï¼Œæé«˜é€æ˜åº¦ã€‚åœ¨å®é™…éƒ¨ç½²ä¸­ï¼ŒJEDAå–å¾—äº†æ˜¾è‘—ä¼˜åŠ¿ï¼Œæ˜¾è‘—ä¼˜äºåŸºç¡€ç¼–ç å™¨å’Œæœ€æ–°å¼€æ”¾åµŒå…¥å™¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>JEDAæ˜¯ä¸€ç§åŸºäºåŒ»å­¦é¢†åŸŸçš„åŒç¼–ç å™¨æŠ€æœ¯ï¼Œæ”¯æŒç›´æ¥å’Œé—´æ¥ä¸´åºŠè®¢å•å¤„ç†ã€‚</li>
<li>JEDAèƒ½å¤Ÿåˆ©ç”¨PubMedBERTè¿›è¡Œåˆå§‹åŒ–ï¼Œå¹¶å…·å¤‡ä¸€ä¸ªé’ˆå¯¹å¤åˆ¶å®‰å…¨çš„å¯¹æ¯”ç›®æ ‡è¿›è¡Œå¾®è°ƒçš„èƒ½åŠ›ã€‚</li>
<li>JEDAèƒ½å¤Ÿå…³è”ä¸åŒè¡¨è¾¾æ„å›¾ä¸å…±äº«è®¢å•æ¦‚å¿µï¼Œå¤„ç†å¤šæ ·åŒ–çš„åŒ»ç–—æŒ‡ä»¤è¡¨è¾¾å½¢å¼ã€‚</li>
<li>JEDAå…·å¤‡ä¸€ç§æŸ¥è¯¢è‡ªç”±æ¨¡å¼ï¼Œè¯¥æ¨¡å¼é€šè¿‡è€ƒè™‘çŸ­æœŸçª—å£è€Œéå•ä¸€è¯è¯­æ¥å‡å°‘å™ªéŸ³å¹²æ‰°å’Œè¯­éŸ³è¯†åˆ«é”™è¯¯çš„å½±å“ã€‚</li>
<li>JEDAéƒ¨ç½²åœ¨å®é™…ç¯å¢ƒä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä¼˜äºåŸºç¡€ç¼–ç å™¨å’Œå¤šç§æœ€æ–°åµŒå…¥æŠ€æœ¯ã€‚</li>
<li>JEDAæä¾›äº†ä¸€ä¸ªå¿«é€Ÿã€å¯è§£é‡Šã€æ— éœ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ£€ç´¢å±‚ï¼Œèƒ½å¤Ÿå®æ—¶è¿æ¥ä¸Šä¸‹æ–‡ä¸å¯æ“ä½œçš„åŒ»ç–—æŒ‡ä»¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14169">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-cb61d16ce5b7cd034614e4785067e888~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995718&auth_key=1760995718-0-0-eca3e3d6b4c90249957516a7d591dcc7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-42e699a5827e9d65a82a31c568466643~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995725&auth_key=1760995725-0-0-fce422a9d70cd95d203a772158ac2b42&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3d0246fc45bd26ae1dd9ec7278f5ca02~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995732&auth_key=1760995732-0-0-52c4a12caae1b1ade4287e78810730f7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5aac7a8c634ed5bea320ab3af677daba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995738&auth_key=1760995738-0-0-b7eb50ade5c0b98a77cb07fe0cbc2f47&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e1494446154af0348e051e3b3e773ef7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995745&auth_key=1760995745-0-0-37ebf24262865acf6ef797d1a468c9f8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9da97dc71eb001ac9bd173f9e096fe1a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995772&auth_key=1760995772-0-0-e48dc30bb4377f66aa6d2ee5c61823d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d45061050cd5904916164d201ed07f9d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995779&auth_key=1760995779-0-0-c6e3f29be1efeff3156b82d0f1bf005d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Wearable-and-Ultra-Low-Power-Fusion-of-EMG-and-A-Mode-US-for-Hand-Wrist-Kinematic-Tracking"><a href="#Wearable-and-Ultra-Low-Power-Fusion-of-EMG-and-A-Mode-US-for-Hand-Wrist-Kinematic-Tracking" class="headerlink" title="Wearable and Ultra-Low-Power Fusion of EMG and A-Mode US for Hand-Wrist   Kinematic Tracking"></a>Wearable and Ultra-Low-Power Fusion of EMG and A-Mode US for Hand-Wrist   Kinematic Tracking</h2><p><strong>Authors:Giusy Spacone, Sebastian Frey, Mattia Orlandi, Pierangelo Maria Rapa, Victor Kartsch, Simone Benatti, Luca Benini, Andrea Cossettini</strong></p>
<p>Hand gesture recognition based on biosignals has shown strong potential for developing intuitive human-machine interaction strategies that closely mimic natural human behavior. In particular, sensor fusion approaches have gained attention for combining complementary information and overcoming the limitations of individual sensing modalities, thereby enabling more robust and reliable systems. Among them, the fusion of surface electromyography (EMG) and A-mode ultrasound (US) is very promising. However, prior solutions rely on power-hungry platforms unsuitable for multi-day use and are limited to discrete gesture classification. In this work, we present an ultra-low-power (sub-50 mW) system for concurrent acquisition of 8-channel EMG and 4-channel A-mode US signals, integrating two state-of-the-art platforms into fully wearable, dry-contact armbands. We propose a framework for continuous tracking of 23 degrees of freedom (DoFs), 20 for the hand and 3 for the wrist, using a kinematic glove for ground-truth labeling. Our method employs lightweight encoder-decoder architectures with multi-task learning to simultaneously estimate hand and wrist joint angles. Experimental results under realistic sensor repositioning conditions demonstrate that EMG-US fusion achieves a root mean squared error of $10.6^\circ\pm2.0^\circ$, compared to $12.0^\circ\pm1^\circ$ for EMG and $13.1^\circ\pm2.6^\circ$ for US, and a R$^2$ score of $0.61\pm0.1$, with $0.54\pm0.03$ for EMG and $0.38\pm0.20$ for US. </p>
<blockquote>
<p>åŸºäºç”Ÿç‰©ä¿¡å·çš„æ‰‹åŠ¿è¯†åˆ«åœ¨å¼€å‘ç›´è§‚çš„äººæœºäº¤äº’ç­–ç•¥æ–¹é¢è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œè¿™äº›ç­–ç•¥ç´§å¯†æ¨¡ä»¿äº†äººç±»è‡ªç„¶è¡Œä¸ºã€‚ç‰¹åˆ«æ˜¯ï¼Œä¼ æ„Ÿå™¨èåˆæ–¹æ³•å—åˆ°äº†å…³æ³¨ï¼Œèƒ½å¤Ÿé€šè¿‡ç»“åˆäº’è¡¥ä¿¡æ¯æ¥å…‹æœå•ä¸€ä¼ æ„Ÿæ¨¡å¼çš„å±€é™æ€§ï¼Œä»è€Œå®ç°æ›´ç¨³å¥å’Œå¯é çš„ç³»ç»Ÿã€‚å…¶ä¸­ï¼Œè¡¨é¢è‚Œç”µå›¾ï¼ˆEMGï¼‰å’ŒAæ¨¡å¼è¶…å£°ï¼ˆUSï¼‰çš„èåˆå‰æ™¯éå¸¸å¹¿é˜”ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„è§£å†³æ–¹æ¡ˆä¾èµ–äºåŠŸè€—è¾ƒå¤§çš„å¹³å°ï¼Œä¸é€‚ç”¨äºå¤šå¤©ä½¿ç”¨ï¼Œå¹¶ä¸”ä»…é™äºç¦»æ•£æ‰‹åŠ¿åˆ†ç±»ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªè¶…ä½åŠŸè€—ï¼ˆä½äº50æ¯«ç“¦ï¼‰çš„ç³»ç»Ÿï¼Œèƒ½å¤ŸåŒæ—¶é‡‡é›†8é€šé“EMGå’Œ4é€šé“Aæ¨¡å¼USä¿¡å·ï¼Œå°†ä¸¤ç§æœ€æ–°å¹³å°é›†æˆåˆ°å¯ç©¿æˆ´çš„å¹²æ¥è§¦è‡‚å¸¦ä¸­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºè¿ç»­è·Ÿè¸ª23ä¸ªè‡ªç”±åº¦ï¼ˆDoFsï¼‰ï¼Œå…¶ä¸­æ‰‹éƒ¨æœ‰20ä¸ªè‡ªç”±åº¦ï¼Œæ‰‹è…•æœ‰3ä¸ªè‡ªç”±åº¦ï¼Œé‡‡ç”¨è¿åŠ¨æ‰‹å¥—è¿›è¡Œå®é™…æ ‡æ³¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨è½»é‡çº§ç¼–ç å™¨-è§£ç å™¨æ¶æ„è¿›è¡Œå¤šä»»åŠ¡å­¦ä¹ ï¼ŒåŒæ—¶ä¼°è®¡æ‰‹å’Œæ‰‹è…•å…³èŠ‚è§’åº¦ã€‚åœ¨çœŸå®çš„ä¼ æ„Ÿå™¨é‡æ–°å®šä½æ¡ä»¶ä¸‹è¿›è¡Œçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä»…ä½¿ç”¨EMGæˆ–USç›¸æ¯”ï¼ŒEMG-USèåˆè¾¾åˆ°äº†å¹³å‡è¯¯å·®ä¸º$10.6^\circ\pm2.0^\circ$çš„æ ¹å‡æ–¹è¯¯å·®ï¼ˆRMSEï¼‰ï¼Œå…¶ä¸­EMGä¸º$12.0^\circ\pm1^\circ$ï¼Œè€ŒUSä¸º$13.1^\circ\pm2.6^\circ$ï¼›åŒæ—¶èåˆåçš„R$^2$åˆ†æ•°ä¸º$0.61\pm0.1$ï¼Œç›¸æ¯”ä¹‹ä¸‹EMGçš„R$^2$åˆ†æ•°ä¸º$0.54\pm0.03$ï¼Œè€ŒUSçš„R$^2$åˆ†æ•°ä¸º$0.38\pm0.20$ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02000v2">PDF</a> 5 pages, 3 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºç”Ÿç‰©ä¿¡å·çš„æ‰‹åŠ¿è¯†åˆ«åœ¨å¼€å‘ç›´è§‚çš„äººæœºäº¤äº’ç­–ç•¥æ–¹é¢æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œè¿™äº›ç­–ç•¥èƒ½å¤Ÿç´§å¯†æ¨¡ä»¿è‡ªç„¶äººç±»è¡Œä¸ºã€‚ç‰¹åˆ«æ˜¯ä¼ æ„Ÿå™¨èåˆæ–¹æ³•ç»“åˆäº†äº’è¡¥ä¿¡æ¯ï¼Œå…‹æœäº†å•ä¸€ä¼ æ„Ÿæ¨¡å¼çš„å±€é™æ€§ï¼Œä»è€Œå®ç°äº†æ›´ç¨³å¥å’Œå¯é çš„ç³»ç»Ÿã€‚å…¶ä¸­ï¼Œè¡¨é¢è‚Œç”µå›¾ï¼ˆEMGï¼‰å’ŒAæ¨¡å¼è¶…å£°ï¼ˆUSï¼‰çš„èåˆå‰æ™¯éå¸¸çœ‹å¥½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§£å†³æ–¹æ¡ˆä¾èµ–äºåŠŸè€—è¾ƒå¤§çš„å¹³å°ï¼Œä¸é€‚ç”¨äºå¤šæ—¥ä½¿ç”¨ï¼Œä¸”ä»…é™äºç¦»æ•£æ‰‹åŠ¿åˆ†ç±»ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¶…ä½åŠŸè€—ï¼ˆä½äº50æ¯«ç“¦ï¼‰çš„ç³»ç»Ÿï¼Œèƒ½å¤ŸåŒæ—¶é‡‡é›†8é€šé“EMGå’Œ4é€šé“Aæ¨¡å¼USä¿¡å·ï¼Œå°†ä¸¤ç§æœ€å…ˆè¿›çš„å¹³å°é›†æˆåˆ°å¯ç©¿æˆ´çš„å¹²æ¥è§¦è‡‚ç« ä¸­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºè¿ç»­è·Ÿè¸ªæ‰‹éƒ¨çš„20ä¸ªè‡ªç”±åº¦ï¼ˆDOFï¼‰å’Œæ‰‹è…•çš„3ä¸ªDOFï¼Œä½¿ç”¨è¿åŠ¨å­¦æ‰‹å¥—è¿›è¡ŒçœŸå®æ ‡ç­¾æ ‡æ³¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨è½»é‡çº§ç¼–ç å™¨-è§£ç å™¨æ¶æ„è¿›è¡Œå¤šä»»åŠ¡å­¦ä¹ ï¼Œä»¥åŒæ—¶ä¼°è®¡æ‰‹å’Œæ‰‹è…•å…³èŠ‚è§’åº¦ã€‚åœ¨ä¼ æ„Ÿå™¨é‡æ–°å®šä½çš„ç°å®æ¡ä»¶ä¸‹è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒEMG-USèåˆè¾¾åˆ°äº†å¹³å‡è¯¯å·®è§’ä¸º$10.6^\circ\pm2.0^\circ$çš„å‡æ–¹æ ¹è¯¯å·®ï¼Œä¸EMGçš„$12.0^\circ\pm1^\circ$å’ŒUSçš„$13.1^\circ\pm2.6^\circ$ç›¸æ¯”å…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†R$^2$åˆ†æ•°ä¸º$0.61\pm0.1$çš„é¢„æµ‹æ•ˆæœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŸºäºç”Ÿç‰©ä¿¡å·çš„æ‰‹åŠ¿è¯†åˆ«å…·æœ‰å‘å±•ç›´è§‚äººæœºäº¤äº’ç­–ç•¥çš„æ½œåŠ›ï¼Œå¯æ¨¡ä»¿è‡ªç„¶äººç±»è¡Œä¸ºã€‚</li>
<li>ä¼ æ„Ÿå™¨èåˆæ–¹æ³•ç»“åˆäº†äº’è¡¥ä¿¡æ¯ï¼Œæé«˜äº†ç³»ç»Ÿçš„ç¨³å¥æ€§å’Œå¯é æ€§ã€‚</li>
<li>EMGå’ŒAæ¨¡å¼è¶…å£°ï¼ˆUSï¼‰çš„èåˆåœ¨æ‰‹åŠ¿è¯†åˆ«ä¸­å…·æœ‰ç‰¹åˆ«çš„ä¼˜åŠ¿ã€‚</li>
<li>ç°æœ‰è§£å†³æ–¹æ¡ˆå› åŠŸè€—é—®é¢˜è€Œå—é™ï¼Œä¸é€‚ç”¨äºå¤šæ—¥ä½¿ç”¨ï¼Œä¸”ä¸»è¦é™äºç¦»æ•£æ‰‹åŠ¿åˆ†ç±»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è¶…ä½åŠŸè€—çš„ç³»ç»Ÿï¼Œèƒ½åŒæ—¶é‡‡é›†EMGå’ŒUSä¿¡å·ï¼Œé›†æˆåˆ°å¯ç©¿æˆ´è®¾å¤‡ä¸­ã€‚</li>
<li>ç³»ç»Ÿèƒ½å¤Ÿè¿ç»­è·Ÿè¸ªæ‰‹å’Œæ‰‹è…•çš„å¤šä¸ªè‡ªç”±åº¦ï¼ˆDOFï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02000">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-86fed1eff1f9d39e6c09bb3a5d155ade~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995807&auth_key=1760995807-0-0-1da18af8fb99b8dc545e9adf30043722&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9c8297f1d084852c06edde106686d3d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995814&auth_key=1760995814-0-0-74ab5c9b32e3e4ee9103b1d79391b7ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5754c3f59d37c7ef180be9d6799a300b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995821&auth_key=1760995821-0-0-d751372627b266d348467954a0b2f98d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-40cddf8c72f070682cfc5d365bdc8761~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995828&auth_key=1760995828-0-0-6099f4b8e5310c69a22dd73045477e65&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cef4562db62f076149fed9a1c7015e4e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995834&auth_key=1760995834-0-0-52e1ba055cb621d84538959fada7b4a6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="RIFLE-Removal-of-Image-Flicker-Banding-via-Latent-Diffusion-Enhancement"><a href="#RIFLE-Removal-of-Image-Flicker-Banding-via-Latent-Diffusion-Enhancement" class="headerlink" title="RIFLE: Removal of Image Flicker-Banding via Latent Diffusion Enhancement"></a>RIFLE: Removal of Image Flicker-Banding via Latent Diffusion Enhancement</h2><p><strong>Authors:Libo Zhu, Zihan Zhou, Xiaoyang Liu, Weihang Zhang, Keyu Shi, Yifan Fu, Yulun Zhang</strong></p>
<p>Capturing screens is now routine in our everyday lives. But the photographs of emissive displays are often influenced by the flicker-banding (FB), which is alternating bright%u2013dark stripes that arise from temporal aliasing between a cameraâ€™s rolling-shutter readout and the displayâ€™s brightness modulation. Unlike moire degradation, which has been extensively studied, the FB remains underexplored despite its frequent and severe impact on readability and perceived quality. We formulate FB removal as a dedicated restoration task and introduce Removal of Image Flicker-Banding via Latent Diffusion Enhancement, RIFLE, a diffusion-based framework designed to remove FB while preserving fine details. We propose the flicker-banding prior estimator (FPE) that predicts key banding attributes and injects it into the restoration network. Additionally, Masked Loss (ML) is proposed to concentrate supervision on banded regions without sacrificing global fidelity. To overcome data scarcity, we provide a simulation pipeline that synthesizes FB in the luminance domain with stochastic jitter in banding angle, banding spacing, and banding width. Feathered boundaries and sensor noise are also applied for a more realistic simulation. For evaluation, we collect a paired real-world FB dataset with pixel-aligned banding-free references captured via long exposure. Across quantitative metrics and visual comparisons on our real-world dataset, RIFLE consistently outperforms recent image reconstruction baselines from mild to severe flicker-banding. To the best of our knowledge, it is the first work to research the simulation and removal of FB. Our work establishes a great foundation for subsequent research in both the dataset construction and the removal model design. Our dataset and code will be released soon. </p>
<blockquote>
<p>å±å¹•æˆªå›¾ç°åœ¨å·²ç»æ˜¯æˆ‘ä»¬æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å¸¸è§„æ“ä½œã€‚ç„¶è€Œï¼Œå‘å…‰æ˜¾ç¤ºçš„ç…§ç‰‡å¾€å¾€å—åˆ°é¢‘é—ªæ¡çº¹ï¼ˆFBï¼‰çš„å½±å“ï¼Œè¿™æ˜¯ç”±ç›¸æœºæ»šåŠ¨å¿«é—¨è¯»å‡ºå’Œæ˜¾ç¤ºå±äº®åº¦è°ƒåˆ¶ä¹‹é—´çš„æ—¶é—´æ··å è€Œäº§ç”Ÿçš„æ˜æš—æ¡çº¹äº¤æ›¿ã€‚ä¸å¹¿æ³›ç ”ç©¶çš„æ‘©å°”çº¹é€€åŒ–ä¸åŒï¼Œå°½ç®¡é¢‘é—ªæ¡çº¹ç»å¸¸å¯¹å¯è¯»æ€§å’Œæ„ŸçŸ¥è´¨é‡é€ æˆä¸¥é‡çš„å½±å“ï¼Œä½†å…¶ä»ç„¶è¢«è¾ƒå°‘æ¢ç´¢ã€‚æˆ‘ä»¬å°†é¢‘é—ªæ¡çº¹çš„å»é™¤åˆ¶å®šä¸ºä¸€ä¸ªä¸“é—¨çš„æ¢å¤ä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†é€šè¿‡æ½œåœ¨æ‰©æ•£å¢å¼ºå»é™¤å›¾åƒé¢‘é—ªæ¡çº¹ï¼ˆRIFLEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œæ—¨åœ¨å»é™¤é¢‘é—ªæ¡çº¹çš„åŒæ—¶ä¿ç•™ç»†èŠ‚ã€‚æˆ‘ä»¬æå‡ºäº†é¢‘é—ªæ¡çº¹å…ˆéªŒä¼°è®¡å™¨ï¼ˆFPEï¼‰ï¼Œå®ƒé¢„æµ‹å…³é”®çš„æ¡çº¹å±æ€§å¹¶å°†å…¶æ³¨å…¥æ¢å¤ç½‘ç»œã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†æ©æ¨¡æŸå¤±ï¼ˆMLï¼‰ï¼Œä»¥ä¾¿åœ¨ä¸å½±å“å…¨å±€ä¿çœŸåº¦çš„æƒ…å†µä¸‹ï¼Œå°†ç›‘ç£é›†ä¸­åœ¨æ¡çº¹åŒºåŸŸã€‚ä¸ºäº†å…‹æœæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ¨¡æ‹Ÿæµæ°´çº¿ï¼Œåœ¨äº®åº¦åŸŸä¸­åˆæˆé¢‘é—ªæ¡çº¹ï¼Œå¸¦æœ‰æ¡çº¹è§’åº¦ã€æ¡çº¹é—´éš”å’Œæ¡çº¹å®½åº¦ä¸­çš„éšæœºæŠ–åŠ¨ã€‚æˆ‘ä»¬è¿˜åº”ç”¨äº†æ¸å˜çš„è¾¹ç•Œå’Œä¼ æ„Ÿå™¨å™ªå£°ï¼Œä»¥è¿›è¡Œæ›´é€¼çœŸçš„æ¨¡æ‹Ÿã€‚ä¸ºäº†è¯„ä¼°ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªé…å¯¹çš„çœŸå®ä¸–ç•Œé¢‘é—ªæ¡çº¹æ•°æ®é›†ï¼Œé€šè¿‡é•¿æ—¶é—´æ›å…‰æ•è·å…·æœ‰åƒç´ å¯¹é½çš„æ— æ¡çº¹å‚è€ƒã€‚åœ¨æˆ‘ä»¬çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šï¼Œä¸å®šé‡æŒ‡æ ‡å’Œè§†è§‰æ¯”è¾ƒç»“æœç›¸æ¯”ï¼ŒRIFLEåœ¨è½»å¾®è‡³ä¸¥é‡çš„é¢‘é—ªæ¡çº¹æƒ…å†µä¸‹å‡ä¼˜äºæœ€è¿‘çš„å›¾åƒé‡å»ºåŸºçº¿ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªç ”ç©¶FBæ¨¡æ‹Ÿå’Œå»é™¤çš„å·¥ä½œã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºåç»­çš„ç ”ç©¶åœ¨æ•°æ®é›†æ„å»ºå’Œå»é™¤æ¨¡å‹è®¾è®¡æ–¹é¢å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å°†å¾ˆå¿«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24644v4">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ç ”ç©¶äº†å±å¹•é—ªçƒå¸¦ï¼ˆFBï¼‰ç°è±¡ï¼Œå°†å…¶å®šä¹‰ä¸ºä¸€ç§ä¸“é—¨çš„æ¢å¤ä»»åŠ¡ã€‚æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ¡†æ¶RIFLEï¼Œèƒ½å¤Ÿå»é™¤FBåŒæ—¶ä¿ç•™ç»†èŠ‚ã€‚æ–‡ç« å¼•å…¥äº†é—ªçƒå¸¦å…ˆéªŒä¼°è®¡å™¨ï¼ˆFPEï¼‰æ¥é¢„æµ‹å…³é”®å¸¦å±æ€§å¹¶æ³¨å…¥æ¢å¤ç½‘ç»œã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†Masked Lossï¼ˆMLï¼‰æ¥é›†ä¸­å¯¹å¸¦çŠ¶åŒºåŸŸçš„ç›‘ç£è€Œä¸ç‰ºç‰²å…¨å±€ä¿çœŸåº¦ã€‚ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæä¾›äº†ä¸€æ¡åˆæˆFBçš„ä»¿çœŸç®¡é“ï¼ŒåŒ…æ‹¬åœ¨äº®åº¦åŸŸåˆæˆFBä»¥åŠåº”ç”¨æ¨¡ç³Šè¾¹ç•Œå’Œä¼ æ„Ÿå™¨å™ªå£°ä»¥æ¨¡æ‹Ÿæ›´çœŸå®æƒ…å†µã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®šé‡æŒ‡æ ‡å’Œè§†è§‰å¯¹æ¯”è¯„ä¼°ï¼ŒRIFLEåœ¨è½»å¾®è‡³ä¸¥é‡çš„é—ªçƒå¸¦ç°è±¡ä¸­å‡è¡¨ç°ä¼˜äºæœ€æ–°çš„å›¾åƒé‡å»ºåŸºçº¿ã€‚æœ¬æ–‡è¿˜æ˜¯é¦–ä¸ªç ”ç©¶FBæ¨¡æ‹Ÿå’Œå»é™¤çš„å·¥ä½œï¼Œä¸ºåç»­ç ”ç©¶å’Œæ•°æ®é›†æ„å»ºåŠå»é™¤æ¨¡å‹è®¾è®¡å¥ å®šäº†åšå®åŸºç¡€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é—ªçƒå¸¦ï¼ˆFBï¼‰æ˜¯ä¸€ç§å¸¸è§çš„å±å¹•æˆªå›¾é—®é¢˜ï¼Œå¯¹å¯è¯»æ€§å’Œæ„ŸçŸ¥è´¨é‡äº§ç”Ÿä¸¥é‡å½±å“ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•RIFLEï¼ŒåŸºäºæ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨å»é™¤FBåŒæ—¶ä¿ç•™ç»†èŠ‚ã€‚</li>
<li>å¼•å…¥äº†é—ªçƒå¸¦å…ˆéªŒä¼°è®¡å™¨ï¼ˆFPEï¼‰æ¥é¢„æµ‹å¹¶å¤„ç†FBçš„å…³é”®å±æ€§ã€‚</li>
<li>Masked Lossï¼ˆMLï¼‰è¢«æå‡ºæ¥ä¸“æ³¨äºå¸¦çŠ¶åŒºåŸŸçš„ç›‘ç£ï¼Œç¡®ä¿å…¨å±€çš„ä¿çœŸåº¦ä¸å—å½±å“ã€‚</li>
<li>ä¸ºäº†è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå»ºç«‹äº†ä¸€ä¸ªåˆæˆFBçš„ä»¿çœŸç®¡é“ï¼ŒåŒ…æ‹¬åœ¨äº®åº¦åŸŸåˆæˆFBï¼Œå¹¶æ·»åŠ éšæœºæŠ–åŠ¨åœ¨å¸¦çŠ¶è§’åº¦ã€å¸¦çŠ¶é—´è·å’Œå¸¦çŠ¶å®½åº¦ç­‰ã€‚</li>
<li>é€šè¿‡çœŸå®ä¸–ç•Œæ•°æ®é›†è¿›è¡Œçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒRIFLEåœ¨å»é™¤FBæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºç°æœ‰çš„å›¾åƒé‡å»ºæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24644">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-89198ee8d10cddab4fbe5077a0b34b92~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995843&auth_key=1760995843-0-0-efe6308e84cc41fb84e858c7ba8b41b8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f4feadd52ad0541ebf2adb38d87a2599~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995851&auth_key=1760995851-0-0-67e0a65969d2c8c234aca97f0acb456d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-023cce094e9742cd33a765d2c38992f7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995857&auth_key=1760995857-0-0-0fa29b4f4c51402d2ad5e0d890e2aec8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fd7f3a1c6f450f03c99f66ff9ad90ca9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995864&auth_key=1760995864-0-0-84e2446b06c2c5b77ad8375a3e5e240b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9003161138fb490e8ec5b1c87007106f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995874&auth_key=1760995874-0-0-3ffaceb7b9c42fea85aaf69f95423b0a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CCD-Mitigating-Hallucinations-in-Radiology-MLLMs-via-Clinical-Contrastive-Decoding"><a href="#CCD-Mitigating-Hallucinations-in-Radiology-MLLMs-via-Clinical-Contrastive-Decoding" class="headerlink" title="CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical   Contrastive Decoding"></a>CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical   Contrastive Decoding</h2><p><strong>Authors:Xi Zhang, Zaiqiao Meng, Jake Lever, Edmond S. L. Ho</strong></p>
<p>Multimodal large language models (MLLMs) have recently achieved remarkable progress in radiology by integrating visual perception with natural language understanding. However, they often generate clinically unsupported descriptions, known as medical hallucinations, which pose serious risks in medical applications that demand accuracy and image-grounded outputs. Through empirical analysis, we find that prompt-induced hallucinations remain prevalent in radiology MLLMs, largely due to over-sensitivity to clinical sections. To address this, we introduce Clinical Contrastive Decoding (CCD), a training-free and retrieval-free inference framework that integrates structured clinical signals from task-specific radiology expert models. CCD introduces a dual-stage contrastive mechanism to refine token-level logits during generation, thereby enhancing clinical fidelity without modifying the base MLLM. Experiments on three datasets and multiple models demonstrate that CCD consistently improves overall performance on radiology report generation (RRG). On the MIMIC-CXR dataset, it yields up to a 17% improvement in RadGraph-F1 when applied to state-of-the-art RRG models. Our approach provides a lightweight and generalisable solution for mitigating medical hallucinations, effectively bridging expert models and MLLMs in radiology. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æœ€è¿‘é€šè¿‡æ•´åˆè§†è§‰æ„ŸçŸ¥ä¸è‡ªç„¶è¯­è¨€ç†è§£åœ¨æ”¾å°„å­¦é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬ç»å¸¸äº§ç”Ÿä¸´åºŠä¸Šä¸æ”¯æŒçš„æè¿°ï¼Œè¢«ç§°ä¸ºåŒ»å­¦å¹»è§‰ï¼Œè¿™åœ¨éœ€è¦å‡†ç¡®æ€§å’ŒåŸºäºå›¾åƒè¾“å‡ºçš„åŒ»å­¦åº”ç”¨ä¸­å¸¦æ¥äº†ä¸¥é‡çš„é£é™©ã€‚é€šè¿‡å®è¯åˆ†æï¼Œæˆ‘ä»¬å‘ç°æç¤ºè¯±å¯¼çš„å¹»è§‰åœ¨æ”¾å°„å­¦MLLMsä¸­ä»ç„¶æ™®éå­˜åœ¨ï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºå¯¹ä¸´åºŠéƒ¨åˆ†çš„è¿‡åº¦æ•æ„Ÿã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸´åºŠå¯¹æ¯”è§£ç ï¼ˆCCDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå’Œæ£€ç´¢çš„æ¨ç†æ¡†æ¶ï¼Œå®ƒé›†æˆäº†æ¥è‡ªç‰¹å®šä»»åŠ¡æ”¾å°„å­¦ä¸“å®¶æ¨¡å‹çš„ç»“æ„åŒ–ä¸´åºŠä¿¡å·ã€‚CCDå¼•å…¥äº†ä¸€ç§åŒé˜¶æ®µå¯¹æ¯”æœºåˆ¶ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç²¾ç»†è°ƒæ•´ä»¤ç‰Œçº§åˆ«çš„é€»è¾‘ï¼Œä»è€Œæé«˜ä¸´åºŠä¿çœŸåº¦ï¼Œè€Œæ— éœ€ä¿®æ”¹åŸºç¡€MLLMã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†å’Œå¤šä¸ªæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCCDåœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰æ–¹é¢å§‹ç»ˆæé«˜äº†æ€»ä½“æ€§èƒ½ã€‚åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šï¼Œå½“åº”ç”¨äºæœ€å…ˆè¿›çš„RRGæ¨¡å‹æ—¶ï¼Œå®ƒåœ¨RadGraph-F1ä¸Šå®ç°äº†é«˜è¾¾17%çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºç¼“è§£åŒ»å­¦å¹»è§‰æä¾›äº†ä¸€ç§è½»ä¾¿ä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰æ•ˆåœ°æ¶èµ·äº†ä¸“å®¶æ¨¡å‹å’ŒMLLMsåœ¨æ”¾å°„å­¦ä¸­çš„æ¡¥æ¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23379v2">PDF</a> Preprint, 27 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ”¾å°„å­¦é¢†åŸŸçš„è¿›å±•ä¸æŒ‘æˆ˜ã€‚æ–‡ç« æŒ‡å‡ºï¼Œè™½ç„¶MLLMsèƒ½ç»“åˆè§†è§‰æ„ŸçŸ¥å’Œè‡ªç„¶è¯­è¨€ç†è§£ï¼Œä½†åœ¨åŒ»å­¦åº”ç”¨ä¸­ï¼Œå®ƒä»¬å¸¸äº§ç”Ÿæœªç»ä¸´åºŠéªŒè¯çš„æè¿°ï¼Œå³æ‰€è°“çš„åŒ»å­¦å¹»è§‰ï¼Œè¿™ä¼šç»™éœ€è¦ç²¾ç¡®æ€§å’Œå›¾åƒåŸºç¡€çš„è¾“å‡ºå¸¦æ¥ä¸¥é‡é£é™©ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†è®­ç»ƒæ— å…³ã€æ£€ç´¢æ— å…³çš„ä¸´åºŠå¯¹æ¯”è§£ç ï¼ˆCCDï¼‰æ¨ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ•´åˆäº†ç‰¹å®šä»»åŠ¡æ”¾å°„å­¦ä¸“å®¶æ¨¡å‹çš„ç»“æ„åŒ–ä¸´åºŠä¿¡å·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCCDåœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¯¹æœ€å…ˆè¿›çš„RRGæ¨¡å‹åº”ç”¨åï¼Œåœ¨MIMIC-CXRæ•°æ®é›†ä¸Šçš„RadGraph-F1å¾—åˆ†æé«˜äº†é«˜è¾¾17%ã€‚æœ¬æ–‡æ–¹æ³•ä¸ºç¼“è§£åŒ»å­¦å¹»è§‰é—®é¢˜æä¾›äº†ä¸€ç§è½»ä¾¿ä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰æ•ˆåœ°æ¶èµ·äº†ä¸“å®¶æ¨¡å‹å’ŒMLLMsä¹‹é—´çš„æ¡¥æ¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ”¾å°„å­¦é¢†åŸŸé€šè¿‡ç»“åˆè§†è§‰æ„ŸçŸ¥å’Œè‡ªç„¶è¯­è¨€ç†è§£å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>MLLMsä¼šäº§ç”ŸåŒ»å­¦å¹»è§‰ï¼Œå³æœªç»ä¸´åºŠéªŒè¯çš„æè¿°ï¼Œè¿™åœ¨éœ€è¦ç²¾ç¡®å’Œå›¾åƒåŸºç¡€çš„åŒ»å­¦åº”ç”¨ä¸­æ„æˆé£é™©ã€‚</li>
<li>åŒ»å­¦å¹»è§‰ä¸»è¦ç”±å¯¹ä¸´åºŠéƒ¨åˆ†çš„è¿‡åº¦æ•æ„Ÿå¼•èµ·ã€‚</li>
<li>ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæå‡ºäº†è®­ç»ƒæ— å…³ã€æ£€ç´¢æ— å…³çš„ä¸´åºŠå¯¹æ¯”è§£ç ï¼ˆCCDï¼‰æ¨ç†æ¡†æ¶ã€‚</li>
<li>CCDæ¡†æ¶æ•´åˆäº†ç‰¹å®šä»»åŠ¡æ”¾å°„å­¦ä¸“å®¶æ¨¡å‹çš„ç»“æ„åŒ–ä¸´åºŠä¿¡å·ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCCDåœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5ab70949dd6dfe09e935b387f4e4e382~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995881&auth_key=1760995881-0-0-238a886cba1a2c4f4d11fe9384ca0568&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3412e7b58bfccebaa5bc45f58e0bd04c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995888&auth_key=1760995888-0-0-abe90999021fad68c4e38bff5678157b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0d2c1c62f194a8d3cd92f88de510f6d0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995899&auth_key=1760995899-0-0-bc3c2b28105438a0f4e53cdd327d5908&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-93aa9c93345d9824f91974d4dc0cf83b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995910&auth_key=1760995910-0-0-fcef77390d8ea2f16f6f88fd99657556&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Refer-to-Any-Segmentation-Mask-Group-With-Vision-Language-Prompts"><a href="#Refer-to-Any-Segmentation-Mask-Group-With-Vision-Language-Prompts" class="headerlink" title="Refer to Any Segmentation Mask Group With Vision-Language Prompts"></a>Refer to Any Segmentation Mask Group With Vision-Language Prompts</h2><p><strong>Authors:Shengcao Cao, Zijun Wei, Jason Kuen, Kangning Liu, Lingzhi Zhang, Jiuxiang Gu, HyunJoon Jung, Liang-Yan Gui, Yu-Xiong Wang</strong></p>
<p>Recent image segmentation models have advanced to segment images into high-quality masks for visual entities, and yet they cannot provide comprehensive semantic understanding for complex queries based on both language and vision. This limitation reduces their effectiveness in applications that require user-friendly interactions driven by vision-language prompts. To bridge this gap, we introduce a novel task of omnimodal referring expression segmentation (ORES). In this task, a model produces a group of masks based on arbitrary prompts specified by text only or text plus reference visual entities. To address this new challenge, we propose a novel framework to â€œRefer to Any Segmentation Mask Groupâ€ (RAS), which augments segmentation models with complex multimodal interactions and comprehension via a mask-centric large multimodal model. For training and benchmarking ORES models, we create datasets MaskGroups-2M and MaskGroups-HQ to include diverse mask groups specified by text and reference entities. Through extensive evaluation, we demonstrate superior performance of RAS on our new ORES task, as well as classic referring expression segmentation (RES) and generalized referring expression segmentation (GRES) tasks. Project page: <a target="_blank" rel="noopener" href="https://ref2any.github.io/">https://Ref2Any.github.io</a>. </p>
<blockquote>
<p>è¿‘æœŸå›¾åƒåˆ†å‰²æ¨¡å‹å·²ç»å‘å±•åˆ°äº†å¯ä»¥å°†å›¾åƒåˆ†å‰²æˆé«˜è´¨é‡è§†è§‰å®ä½“çš„æ©è†œï¼Œä½†å®ƒä»¬æ— æ³•åŸºäºè¯­è¨€å’Œè§†è§‰å¯¹å¤æ‚æŸ¥è¯¢æä¾›å…¨é¢çš„è¯­ä¹‰ç†è§£ã€‚è¿™ä¸€å±€é™å½±å“äº†è¿™äº›æ¨¡å‹åœ¨å¤„ç†ä¾èµ–è§†è§‰è¯­è¨€æç¤ºé©±åŠ¨çš„å‹å¥½ç”¨æˆ·äº¤äº’ç­‰åº”ç”¨åœºæ™¯æ—¶çš„æ•ˆæœã€‚ä¸ºäº†ç¼©å°è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€é¡¹å…¨æ–°çš„ä»»åŠ¡â€”â€”å¤šæ¨¡æ€å¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²ï¼ˆORESï¼‰ã€‚åœ¨æ­¤ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹æ ¹æ®ä»…ç”±æ–‡æœ¬æŒ‡å®šæˆ–æ–‡æœ¬å’Œå‚è€ƒè§†è§‰å®ä½“å…±åŒæŒ‡å®šçš„ä»»æ„æç¤ºç”Ÿæˆä¸€ç»„æ©è†œã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æ–°æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºâ€œæŒ‡å‘ä»»æ„åˆ†å‰²æ©è†œç»„â€ (RAS) çš„æ–°æ¡†æ¶ï¼Œé€šè¿‡æ©è†œä¸­å¿ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹å¢å¼ºåˆ†å‰²æ¨¡å‹çš„å¤æ‚å¤šæ¨¡æ€äº¤äº’å’Œç†è§£èƒ½åŠ›ã€‚ä¸ºäº†è®­ç»ƒå’Œè¯„ä¼°ORESæ¨¡å‹ï¼Œæˆ‘ä»¬åˆ›å»ºäº†MaskGroups-2Må’ŒMaskGroups-HQæ•°æ®é›†ï¼Œæ¶µç›–äº†ç”±æ–‡æœ¬å’Œå‚è€ƒå®ä½“æŒ‡å®šçš„å„ç§æ©è†œç»„ã€‚é€šè¿‡å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬åœ¨æ–°çš„ORESä»»åŠ¡ä»¥åŠç»å…¸çš„å¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²ï¼ˆRESï¼‰å’Œå¹¿ä¹‰å¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²ï¼ˆGRESï¼‰ä»»åŠ¡ä¸Šå±•ç¤ºäº†RASçš„å“è¶Šæ€§èƒ½ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://ref2any.github.io./">https://Ref2Any.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05342v2">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§æ–°å‹çš„å›¾åƒåˆ†å‰²æ¨¡å‹ä»»åŠ¡â€”â€”Omnimodal Refering Expression Segmentationï¼ˆORESï¼‰è¢«æå‡ºï¼Œä»¥è§£å†³ç°æœ‰æ¨¡å‹æ— æ³•åŸºäºè¯­è¨€å’Œè§†è§‰å¯¹å¤æ‚æŸ¥è¯¢è¿›è¡Œå…¨é¢è¯­ä¹‰ç†è§£çš„é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æ–°æŒ‘æˆ˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ä¸ªåä¸ºâ€œRefer to Any Segmentation Mask Groupâ€ï¼ˆRASï¼‰çš„æ–°æ¡†æ¶ï¼Œé€šè¿‡å¢åŠ åˆ†å‰²æ¨¡å‹çš„å¤æ‚å¤šæ¨¡å¼äº¤äº’å’ŒåŸºäºmaskçš„ç†è§£èƒ½åŠ›ï¼Œæ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚ä¸ºè®­ç»ƒå’Œè¯„ä¼°ORESæ¨¡å‹ï¼Œå›¢é˜Ÿè¿˜åˆ›å»ºäº†MaskGroups-2Må’ŒMaskGroups-HQæ•°æ®é›†ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒRASåœ¨ORESä»»åŠ¡ä»¥åŠç»å…¸å’Œå¹¿ä¹‰çš„å‚ç…§è¡¨è¾¾å¼åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å›¾åƒåˆ†å‰²æ¨¡å‹è™½èƒ½ç”Ÿæˆé«˜è´¨é‡å®ä½“æ©è†œï¼Œä½†åœ¨å¤„ç†ç»“åˆè¯­è¨€å’Œè§†è§‰çš„å¤æ‚æŸ¥è¯¢æ—¶ï¼Œæ— æ³•æä¾›å…¨é¢çš„è¯­ä¹‰ç†è§£ã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†Omnimodal Refering Expression Segmentationï¼ˆORESï¼‰çš„æ–°ä»»åŠ¡ã€‚</li>
<li>åœ¨ORESä»»åŠ¡ä¸­ï¼Œæ¨¡å‹èƒ½å¤Ÿæ ¹æ®çº¯æ–‡æœ¬æˆ–æ–‡æœ¬åŠ å‚è€ƒè§†è§‰å®ä½“çš„æç¤ºï¼Œç”Ÿæˆä¸€ç»„æ©è†œã€‚</li>
<li>ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†â€œRefer to Any Segmentation Mask Groupâ€ï¼ˆRASï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¢å¼ºåˆ†å‰²æ¨¡å‹çš„å¤æ‚å¤šæ¨¡å¼äº¤äº’å’ŒåŸºäºæ©è†œçš„ç†è§£èƒ½åŠ›æ¥æå‡æ€§èƒ½ã€‚</li>
<li>ä¸ºè®­ç»ƒå’Œè¯„ä¼°ORESæ¨¡å‹ï¼Œåˆ›å»ºäº†ä¸¤ä¸ªæ•°æ®é›†MaskGroups-2Må’ŒMaskGroups-HQï¼ŒåŒ…å«ç”±æ–‡æœ¬å’Œå‚è€ƒå®ä½“æŒ‡å®šçš„å¤šæ ·åŒ–æ©è†œç»„ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒRASåœ¨ORESä»»åŠ¡åŠç»å…¸å’Œå¹¿ä¹‰å‚ç…§è¡¨è¾¾å¼åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05342">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-51b742e2740bd1774e49dc32ad833a1e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995917&auth_key=1760995917-0-0-6fe3e48ec0274048ad8668d4543ce9fd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-81dce81fbe5b8f7d5d163b03a1b2e640~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995925&auth_key=1760995925-0-0-0c3b65f799afa65aa5df7f8c7b71873c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3d5e197b425f27aa560ce1f455e4bd5b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995932&auth_key=1760995932-0-0-11f9bafb1f06f5ffa3373fdac3cc07b1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-092d318a684862d82b87e04884d88fcf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995938&auth_key=1760995938-0-0-d5e8b9221b6056d65f845fadb16dbec2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-91473b81f7d9717a658a414cb438d754~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995945&auth_key=1760995945-0-0-a7d2c7c64d9d28e8bb196b1b8408a891&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Point-or-Line-Using-Line-based-Representation-for-Panoptic-Symbol-Spotting-in-CAD-Drawings"><a href="#Point-or-Line-Using-Line-based-Representation-for-Panoptic-Symbol-Spotting-in-CAD-Drawings" class="headerlink" title="Point or Line? Using Line-based Representation for Panoptic Symbol   Spotting in CAD Drawings"></a>Point or Line? Using Line-based Representation for Panoptic Symbol   Spotting in CAD Drawings</h2><p><strong>Authors:Xingguang Wei, Haomin Wang, Shenglong Ye, Ruifeng Luo, Yanting Zhang, Lixin Gu, Jifeng Dai, Yu Qiao, Wenhai Wang, Hongjie Zhang</strong></p>
<p>We study the task of panoptic symbol spotting, which involves identifying both individual instances of countable things and the semantic regions of uncountable stuff in computer-aided design (CAD) drawings composed of vector graphical primitives. Existing methods typically rely on image rasterization, graph construction, or point-based representation, but these approaches often suffer from high computational costs, limited generality, and loss of geometric structural information. In this paper, we propose VecFormer, a novel method that addresses these challenges through line-based representation of primitives. This design preserves the geometric continuity of the original primitive, enabling more accurate shape representation while maintaining a computation-friendly structure, making it well-suited for vector graphic understanding tasks. To further enhance prediction reliability, we introduce a Branch Fusion Refinement module that effectively integrates instance and semantic predictions, resolving their inconsistencies for more coherent panoptic outputs. Extensive experiments demonstrate that our method establishes a new state-of-the-art, achieving 91.1 PQ, with Stuff-PQ improved by 9.6 and 21.2 points over the second-best results under settings with and without prior information, respectively, highlighting the strong potential of line-based representation as a foundation for vector graphic understanding. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†å…¨æ™¯ç¬¦å·è¯†åˆ«ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡æ¶‰åŠè¯†åˆ«è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ç»˜å›¾ä¸­å¯è®¡æ•°çš„å•ä¸ªå®ä¾‹å’Œä¸å¯è®¡æ•°çš„è¯­ä¹‰åŒºåŸŸã€‚è¿™äº›ç»˜å›¾ç”±çŸ¢é‡å›¾å½¢åŸºæœ¬å…ƒç´ ç»„æˆã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå›¾åƒçŸ¢é‡åŒ–ã€å›¾æ„å»ºæˆ–åŸºäºç‚¹çš„è¡¨ç¤ºï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€å­˜åœ¨è®¡ç®—æˆæœ¬é«˜ã€é€šç”¨æ€§æœ‰é™ä»¥åŠå‡ ä½•ç»“æ„ä¿¡æ¯ä¸¢å¤±ç­‰é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†VecFormerè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡åŸºäºçº¿æ¡çš„åŸºæœ¬å…ƒç´ è¡¨ç¤ºæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚è¿™ç§è®¾è®¡ä¿ç•™äº†åŸå§‹åŸºæœ¬å…ƒç´ çš„å‡ ä½•è¿ç»­æ€§ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒè®¡ç®—å‹å¥½çš„ç»“æ„çš„åŒæ—¶å®ç°æ›´å‡†ç¡®çš„å½¢çŠ¶è¡¨ç¤ºï¼Œéå¸¸é€‚åˆçŸ¢é‡å›¾å½¢ç†è§£ä»»åŠ¡ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜é¢„æµ‹å¯é æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†Branch Fusion Refinementæ¨¡å—ï¼Œè¯¥æ¨¡å—æœ‰æ•ˆåœ°é›†æˆäº†å®ä¾‹å’Œè¯­ä¹‰é¢„æµ‹ï¼Œè§£å†³å®ƒä»¬çš„ä¸ä¸€è‡´æ€§ï¼Œä»¥è·å¾—æ›´è¿è´¯çš„å…¨æ™¯è¾“å‡ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å»ºç«‹äº†æ–°çš„æŠ€æœ¯æ ‡æ†ï¼Œå®ç°äº†91.1çš„PQå€¼ã€‚åœ¨æœ‰&#x2F;æ— å…ˆéªŒä¿¡æ¯çš„è®¾ç½®ä¸‹ï¼Œç›¸å¯¹äºç¬¬äºŒå¥½çš„ç»“æœï¼ŒStuff-PQåˆ†åˆ«æé«˜äº†9.6å’Œ21.2ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™å‡¸æ˜¾äº†åŸºäºçº¿æ¡çš„è¡¨ç¤ºä½œä¸ºçŸ¢é‡å›¾å½¢ç†è§£åŸºç¡€çš„å¼ºå¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23395v3">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨è¨ˆç®—æ©Ÿè³¦èƒ½è¨­è¨ˆï¼ˆCADï¼‰åœ–å½¢æ–‡ä»¶ä¸­ï¼Œæ—¢æœ‰åœ“ç¤ºå–®ç¨å€‹æ¡ˆåŠç„¡å®šé‡çš„èªæ„åˆ†ä½ˆè­˜åˆ¥ã€‚æ—¢æœ‰æ–¹æ³•å¤§å¤šåŸºæ–¼å½±åƒæ·ºç©åŒ–ã€åœ–å½¢å»ºé€ æˆ–é»åŸºè¡¨ç¤ºï¼Œç„¶è€Œé€™äº›æ–¹å¼æœƒå‡ºç¾è¨ˆç®—æˆæœ¬é«˜ã€æ³›åŒ–èƒ½åŠ›ä¸è¶³åŠå¹¾ä½•çµæ§‹ä¿¡æ¯æµå¤±ç­‰å•é¡Œã€‚æœ¬æ–‡æå‡ºVecFormerï¼Œä»¥ç·šåŸºè¡¨ç¤ºæ³•è§£æ±ºé€™äº›å•é¡Œï¼Œä¿ç•™åŸå§‹å¹¾ä½•é€£çºŒæ€§ï¼Œå¯¦ç¾æ›´ç²¾ç¢ºçš„å½¢ç‹€è¡¨ç¤ºï¼Œä¸¦ä¿æŒå‹å¥½çš„è¨ˆç®—çµæ§‹ï¼Œé©ç”¨æ–¼å‘é‡åœ–å½¢ç†è§£ä»»å‹™ã€‚æˆ‘å€‘é‚„å¼•å…¥Branch Fusion Refinementæ¨¡çµ„ï¼Œé€²ä¸€æ­¥æå‡é æ¸¬å¯é æ€§ï¼Œæœ‰æ•ˆæ•´åˆå€‹æ¡ˆåŠèªæ„é æ¸¬ï¼Œè§£æ±ºå®ƒå€‘çš„ä¸ä¸€è‡´æ€§å•é¡Œï¼Œä»¥ç²å¾—æ›´é€£è²«çš„æ³›è¦–è¼¸å‡ºã€‚å¯¦é©—çµæœé¡¯ç¤ºï¼Œæˆ‘å€‘çš„æ–¹æ³•å»ºç«‹æ–°çš„æœ€ä½³è¡¨ç¾ï¼Œé”åˆ°91.1 PQï¼Œåœ¨å¸¶æœ‰åŠä¸å¸¶æœ‰å…ˆé©—ä¿¡æ¯çš„è¨­ç½®ä¸‹åˆ†åˆ¥å°‡Stuff-PQæ”¹å–„9.6å’Œ21.2åˆ†é»ï¼Œè­‰æ˜ç·šåŸºè¡¨ç¤ºæ³•æ–¼å‘é‡åœ–å½¢ç†è§£çš„å¼·å¤§æ½›åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VecFormerè§£å†³äº†CADåœ–å½¢ä¸­çš„æ³›è¦–ç¬¦è™Ÿè­˜åˆ¥å•é¡Œï¼ŒåŒ…æ‹¬å€‹æ¡ˆå¯¦ä¾‹å’Œèªæ„åˆ†ä½ˆçš„è­˜åˆ¥ã€‚</li>
<li>æ—¢æœ‰æ–¹æ³•å¦‚å½±åƒæ·ºç©åŒ–ã€åœ–å½¢å»ºé€ å’Œé»åŸºè¡¨ç¤ºå­˜åœ¨é«˜è¨ˆç®—æˆæœ¬ã€æœ‰é™æ³›åŒ–èƒ½åŠ›èˆ‡å¹¾ä½•çµæ§‹ä¿¡æ¯æµå¤±çš„å•é¡Œã€‚</li>
<li>VecFormeré€šéç·šåŸºè¡¨ç¤ºæ³•è§£æ±ºé€™äº›å•é¡Œï¼Œä¿ç•™åŸå§‹å¹¾ä½•é€£çºŒæ€§ä¸¦å¯¦ç¾ç²¾ç¢ºå½¢ç‹€è¡¨ç¤ºã€‚</li>
<li>VecFormerçš„æ–¹æ³•å…·æœ‰å‹å¥½çš„è¨ˆç®—çµæ§‹ï¼Œç‰¹åˆ¥é©ç”¨æ–¼å‘é‡åœ–å½¢ç†è§£ä»»å‹™ã€‚</li>
<li>Branch Fusion Refinementæ¨¡çµ„æå‡é æ¸¬å¯é æ€§ï¼Œæ•´åˆå€‹æ¡ˆåŠèªæ„é æ¸¬ï¼Œè§£æ±ºå®ƒå€‘çš„ä¸ä¸€è‡´æ€§å•é¡Œã€‚</li>
<li>å¯¦é©—çµæœé¡¯ç¤ºVecFormeré”åˆ°æ–°çš„æœ€ä½³è¡¨ç¾ï¼Œç‰¹åˆ¥æ˜¯åœ¨PQå’ŒStuff-PQçš„æ”¹å–„ä¸Šè¡¨ç¾çªå‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23395">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-cf2d91ba2357198384af3c39c5265d19~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995952&auth_key=1760995952-0-0-7d5071d7071cf19f48e4e7a353e1299a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cd5e182c3622ad5319615eb0c5f90f2a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995959&auth_key=1760995959-0-0-841064cfd850a5d4e039d2f5adcbf149&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ded26809c04d35a9f7e82a3483a96f6b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995965&auth_key=1760995965-0-0-a09c0a173ec54adb15dd9a70ec9ab35b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="UNet-with-Self-Adaptive-Mamba-Like-Attention-and-Causal-Resonance-Learning-for-Medical-Image-Segmentation"><a href="#UNet-with-Self-Adaptive-Mamba-Like-Attention-and-Causal-Resonance-Learning-for-Medical-Image-Segmentation" class="headerlink" title="UNet with Self-Adaptive Mamba-Like Attention and Causal-Resonance   Learning for Medical Image Segmentation"></a>UNet with Self-Adaptive Mamba-Like Attention and Causal-Resonance   Learning for Medical Image Segmentation</h2><p><strong>Authors:Saqib Qamar, Mohd Fazil, Parvez Ahmad, Shakir Khan, Abu Taha Zamani</strong></p>
<p>Medical image segmentation plays an important role in various clinical applications; however, existing deep learning models face trade-offs between efficiency and accuracy. Convolutional Neural Networks (CNNs) capture local details well but miss the global context, whereas transformers handle the global context but at a high computational cost. Recently, State Space Sequence Models (SSMs) have shown potential for capturing long-range dependencies with linear complexity; however, their direct use in medical image segmentation remains limited due to incompatibility with image structures and autoregressive assumptions. To overcome these challenges, we propose SAMA-UNet, a novel U-shaped architecture that introduces two key innovations. First, the Self-Adaptive Mamba-like Aggregated Attention (SAMA) block adaptively integrates local and global features through dynamic attention weighting, enabling an efficient representation of complex anatomical patterns. Second, the causal resonance multi-scale module (CR-MSM) improves encoder-decoder interactions by adjusting feature resolution and causal dependencies across scales, enhancing the semantic alignment between low- and high-level features. Extensive experiments on MRI, CT, and endoscopy datasets demonstrate that SAMA-UNet consistently outperforms CNN, Transformer, and Mamba-based methods. It achieves 85.38% DSC and 87.82% NSD on BTCV, 92.16% and 96.54% on ACDC, 67.14% and 68.70% on EndoVis17, and 84.06% and 88.47% on ATLAS23, establishing new benchmarks across modalities. These results confirm the effectiveness of SAMA-UNet in combining efficiency and accuracy, making it a promising solution for real-world clinical segmentation tasks. The source code is available on GitHub. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨å„ç§ä¸´åºŠåº”ç”¨ä¸­éƒ½æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰èƒ½å¤Ÿå¾ˆå¥½åœ°æ•æ‰å±€éƒ¨ç»†èŠ‚ï¼Œä½†å¿½ç•¥äº†å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè€Œå˜å‹å™¨èƒ½å¤Ÿå¤„ç†å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚æœ€è¿‘ï¼ŒçŠ¶æ€ç©ºé—´åºåˆ—æ¨¡å‹ï¼ˆSSMï¼‰åœ¨æ•è·é•¿è·ç¦»ä¾èµ–æ–¹é¢è¡¨ç°å‡ºçº¿æ€§å¤æ‚åº¦çš„æ½œåŠ›ï¼›ç„¶è€Œï¼Œç”±äºå…¶ä¸å›¾åƒç»“æ„çš„ä¸å…¼å®¹æ€§å’Œè‡ªå›å½’å‡è®¾ï¼Œå®ƒä»¬åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„ç›´æ¥åº”ç”¨ä»ç„¶å—åˆ°é™åˆ¶ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SAMA-UNetï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹Uå‹æ¶æ„ï¼Œå®ƒå¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ã€‚é¦–å…ˆï¼Œè‡ªé€‚åº”æ€§Mambaæ ·èšåˆæ³¨æ„åŠ›ï¼ˆSAMAï¼‰å—é€šè¿‡åŠ¨æ€æ³¨æ„åŠ›åŠ æƒè‡ªé€‚åº”åœ°é›†æˆå±€éƒ¨å’Œå…¨å±€ç‰¹å¾ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°è¡¨ç¤ºå¤æ‚çš„è§£å‰–æ¨¡å¼ã€‚å…¶æ¬¡ï¼Œå› æœå…±æŒ¯å¤šå°ºåº¦æ¨¡å—ï¼ˆCR-MSMï¼‰é€šè¿‡è°ƒæ•´ç‰¹å¾åˆ†è¾¨ç‡å’Œè·¨å°ºåº¦çš„å› æœä¾èµ–æ€§ï¼Œæ”¹è¿›äº†ç¼–ç å™¨-è§£ç å™¨ä¹‹é—´çš„äº¤äº’ï¼Œå¢å¼ºäº†ä½çº§å’Œé«˜çº§ç‰¹å¾ä¹‹é—´çš„è¯­ä¹‰å¯¹é½ã€‚åœ¨MRIã€CTå’Œå†…çª¥é•œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSAMA-UNetåœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢çš„è¡¨ç°å‡ä¼˜äºCNNã€Transformerå’ŒåŸºäºMambaçš„æ–¹æ³•ã€‚åœ¨BTCVä¸Šï¼Œå®ƒè¾¾åˆ°äº†85.38%çš„DSCå’Œ87.82%çš„NSDï¼›åœ¨ACDCä¸Šè¾¾åˆ°äº†92.16%å’Œ96.54%ï¼›åœ¨EndoVis17ä¸Šè¾¾åˆ°äº†67.14%å’Œ68.70%ï¼›åœ¨ATLAS23ä¸Šè¾¾åˆ°äº†84.06%å’Œ88.47%ï¼Œä¸ºä¸åŒæ¨¡æ€å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚è¿™äº›ç»“æœè¯å®äº†SAMA-UNetåœ¨ç»“åˆæ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä½¿å…¶æˆä¸ºç°å®ä¸–ç•Œä¸´åºŠåˆ†å‰²ä»»åŠ¡çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚æºä»£ç å·²åœ¨GitHubä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15234v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„é‡è¦æ€§ä¸è¨€è€Œå–»ï¼Œä½†ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´å­˜åœ¨æ­¤éœ€å¯»æ±‚æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰èƒ½å¾ˆå¥½åœ°æ•æ‰å±€éƒ¨ç»†èŠ‚ï¼Œä½†å¿½ç•¥å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼›è€ŒTransformerèƒ½å¤„ç†å…¨å±€ä¸Šä¸‹æ–‡ï¼Œä½†è®¡ç®—æˆæœ¬é«˜ã€‚çŠ¶æ€ç©ºé—´åºåˆ—æ¨¡å‹ï¼ˆSSMï¼‰èƒ½å¤Ÿæ•è·é•¿è·ç¦»ä¾èµ–å…³ç³»ä¸”å…·æœ‰çº¿æ€§å¤æ‚åº¦ï¼Œä½†åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„ç›´æ¥åº”ç”¨ä»ç„¶æœ‰é™ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„Uå½¢æ¶æ„SAMA-UNetï¼ŒåŒ…æ‹¬ä¸¤å¤§åˆ›æ–°ç‚¹ï¼šä¸€æ˜¯è‡ªé€‚åº”ç›ç‘™æ ·èšåˆæ³¨æ„åŠ›ï¼ˆSAMAï¼‰æ¨¡å—ï¼Œèƒ½åŠ¨æ€åœ°æ•´åˆå±€éƒ¨å’Œå…¨å±€ç‰¹å¾ï¼›äºŒæ˜¯å› æœå…±æŒ¯å¤šå°ºåº¦æ¨¡å—ï¼ˆCR-MSMï¼‰ï¼Œé€šè¿‡è°ƒæ•´ç‰¹å¾åˆ†è¾¨ç‡å’Œè·¨å°ºåº¦çš„å› æœä¾èµ–å…³ç³»ï¼Œå¢å¼ºäº†ç¼–ç å™¨ä¸è§£ç å™¨ä¹‹é—´çš„äº¤äº’ã€‚åœ¨MRIã€CTå’Œå†…é•œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSAMA-UNetåœ¨æ•ˆç‡ä¸å‡†ç¡®æ€§ä¸Šå‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸ºä¸åŒæ¨¡æ€çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ ‘ç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨å¤šç§ä¸´åºŠåº”ç”¨ä¸­å…·æœ‰é‡è¦ä½œç”¨ï¼Œä½†æ·±åº¦å­¦ä¹ æ¨¡å‹éœ€è¦åœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚</li>
<li>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒTransformeråœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å„æœ‰ä¼˜ç¼ºç‚¹ã€‚CNNæ“…é•¿æ•æ‰å±€éƒ¨ç»†èŠ‚ï¼Œä½†å¿½ç•¥å…¨å±€ä¸Šä¸‹æ–‡ï¼›è€ŒTransformerèƒ½å¤„ç†å…¨å±€ä¸Šä¸‹æ–‡ï¼Œä½†è®¡ç®—æˆæœ¬é«˜ã€‚</li>
<li>çŠ¶æ€ç©ºé—´åºåˆ—æ¨¡å‹ï¼ˆSSMï¼‰å…·æœ‰æ•è·é•¿è·ç¦»ä¾èµ–å…³ç³»çš„æ½œåŠ›ï¼Œä½†åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„ç›´æ¥åº”ç”¨ä»ç„¶æœ‰é™ã€‚</li>
<li>SAMA-UNetæ˜¯ä¸€ç§æ–°å‹çš„Uå½¢æ¶æ„ï¼Œé€šè¿‡SAMAæ¨¡å—å’ŒCR-MSMæ¨¡å—çš„åˆ›æ–°ï¼Œå®ç°äº†åŒ»å­¦å›¾åƒåˆ†å‰²çš„é«˜æ•ˆå’Œå‡†ç¡®ã€‚</li>
<li>SAMAæ¨¡å—èƒ½å¤Ÿè‡ªé€‚åº”åœ°æ•´åˆå±€éƒ¨å’Œå…¨å±€ç‰¹å¾ï¼Œæé«˜æ¨¡å‹çš„è¡¨ç°åŠ›ã€‚</li>
<li>CR-MSMæ¨¡å—é€šè¿‡è°ƒæ•´ç‰¹å¾åˆ†è¾¨ç‡å’Œè·¨å°ºåº¦çš„å› æœä¾èµ–å…³ç³»ï¼Œå¢å¼ºäº†ç¼–ç å™¨ä¸è§£ç å™¨ä¹‹é—´çš„äº¤äº’ï¼Œæé«˜äº†è¯­ä¹‰å¯¹é½ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSAMA-UNetåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢å–å¾—äº†ä¼˜å¼‚æ€§èƒ½ï¼Œä¸ºä¸åŒæ¨¡æ€çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ ‘ç«‹äº†æ–°çš„åŸºå‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15234">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-57c640b3c19e3a00839a99eebb2ef3fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995973&auth_key=1760995973-0-0-5caac9917da2737038f87cf9702abc13&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="A-Multimodal-Deep-Learning-Approach-for-White-Matter-Shape-Prediction-in-Diffusion-MRI-Tractography"><a href="#A-Multimodal-Deep-Learning-Approach-for-White-Matter-Shape-Prediction-in-Diffusion-MRI-Tractography" class="headerlink" title="A Multimodal Deep Learning Approach for White Matter Shape Prediction in   Diffusion MRI Tractography"></a>A Multimodal Deep Learning Approach for White Matter Shape Prediction in   Diffusion MRI Tractography</h2><p><strong>Authors:Yui Lo, Yuqian Chen, Dongnan Liu, Leo Zekelman, Jarrett Rushmore, Yogesh Rathi, Nikos Makris, Alexandra J. Golby, Fan Zhang, Weidong Cai, Lauren J. Oâ€™Donnell</strong></p>
<p>Shape measures have emerged as promising descriptors of white matter tractography, offering complementary insights into anatomical variability and associations with cognitive and clinical phenotypes. However, conventional methods for computing shape measures are computationally expensive and time-consuming for large-scale datasets due to reliance on voxel-based representations. We propose Tract2Shape, a novel multimodal deep learning framework that leverages geometric (point cloud) and scalar (tabular) features to predict ten white matter tractography shape measures. To enhance model efficiency, we utilize a dimensionality reduction algorithm for the model to predict five primary shape components. The model is trained and evaluated on two independently acquired datasets, the HCP-YA dataset, and the PPMI dataset. We evaluate the performance of Tract2Shape by training and testing it on the HCP-YA dataset and comparing the results with state-of-the-art models. To further assess its robustness and generalization ability, we also test Tract2Shape on the unseen PPMI dataset. Tract2Shape outperforms SOTA deep learning models across all ten shape measures, achieving the highest average Pearsonâ€™s r and the lowest nMSE on the HCP-YA dataset. The ablation study shows that both multimodal input and PCA contribute to performance gains. On the unseen testing PPMI dataset, Tract2Shape maintains a high Pearsonâ€™s r and low nMSE, demonstrating strong generalizability in cross-dataset evaluation. Tract2Shape enables fast, accurate, and generalizable prediction of white matter shape measures from tractography data, supporting scalable analysis across datasets. This framework lays a promising foundation for future large-scale white matter shape analysis. </p>
<blockquote>
<p>å½¢æ€æµ‹é‡å·²ä½œä¸ºæœ‰å‰é€”çš„ç™½è´¨çº¤ç»´è¿½è¸ªæè¿°æ–¹æ³•å‡ºç°ï¼Œä¸ºè§£å‰–å˜å¼‚æ€§ä»¥åŠå…¶ä¸è®¤çŸ¥å’Œä¸´åºŠè¡¨å‹çš„å…³è”æä¾›äº†è¡¥å……æ€§çš„è§è§£ã€‚ç„¶è€Œï¼Œç”±äºä¼ ç»Ÿè®¡ç®—å½¢æ€æµ‹é‡çš„æ–¹æ³•ä¾èµ–äºåŸºäºä½“ç´ ï¼ˆvoxel-basedï¼‰çš„è¡¨ç¤ºæ–¹æ³•ï¼Œå¯¹äºå¤§è§„æ¨¡æ•°æ®é›†è€Œè¨€è®¡ç®—æˆæœ¬é«˜ä¸”è€—æ—¶ã€‚æˆ‘ä»¬æå‡ºäº†Tract2Shapeï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„å¤šæ¨¡å¼æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨å‡ ä½•ï¼ˆç‚¹äº‘ï¼‰å’Œæ ‡é‡ï¼ˆè¡¨æ ¼ï¼‰ç‰¹å¾æ¥é¢„æµ‹åä¸ªç™½è´¨çº¤ç»´è¿½è¸ªå½¢æ€æµ‹é‡æ–¹æ³•ã€‚ä¸ºæé«˜æ¨¡å‹æ•ˆç‡ï¼Œæˆ‘ä»¬ä½¿ç”¨é™ç»´ç®—æ³•å¯¹æ¨¡å‹è¿›è¡Œé¢„æµ‹ä»¥å¾—å‡ºäº”ä¸ªä¸»è¦å½¢æ€ç»„æˆéƒ¨åˆ†ã€‚è¯¥æ¨¡å‹åœ¨ä¸¤ä¸ªç‹¬ç«‹è·å–çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œåˆ†åˆ«æ˜¯HCP-YAæ•°æ®é›†å’ŒPPMIæ•°æ®é›†ã€‚æˆ‘ä»¬é€šè¿‡è®­ç»ƒTract2Shapeåœ¨HCP-YAæ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•å¹¶ä¸æœ€å…ˆè¿›çš„æ¨¡å‹æ¯”è¾ƒæ¥è¯„ä¼°å…¶æ€§èƒ½ã€‚ä¸ºè¿›ä¸€æ­¥è¯„ä¼°å…¶ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜å¯¹Tract2Shapeåœ¨æœªè§è¿‡çš„PPMIæ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚Tract2Shapeåœ¨æ‰€æœ‰åä¸ªå½¢æ€æµ‹é‡æŒ‡æ ‡ä¸Šå‡ä¼˜äºæœ€æ–°æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œåœ¨HCP-YAæ•°æ®é›†ä¸Šå–å¾—äº†æœ€é«˜çš„å¹³å‡Pearsonç›¸å…³ç³»æ•°å’Œæœ€ä½çš„å¹³å‡å½’ä¸€åŒ–å‡æ–¹è¯¯å·®ã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œå¤šæ¨¡å¼è¾“å…¥å’Œä¸»æˆåˆ†åˆ†æéƒ½æœ‰åŠ©äºæ€§èƒ½æå‡ã€‚åœ¨æœªè§è¿‡çš„æµ‹è¯•æ•°æ®é›†PPMIä¸Šï¼ŒTract2Shapeä¿æŒè¾ƒé«˜çš„Pearsonç›¸å…³ç³»æ•°å’Œè¾ƒä½çš„å¹³å‡å½’ä¸€åŒ–å‡æ–¹è¯¯å·®ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„è·¨æ•°æ®é›†è¯„ä¼°æ³›åŒ–èƒ½åŠ›. Tract2Shapeèƒ½å¤Ÿå®ç°å¯¹ç™½è´¨å½¢æ€æµ‹é‡çš„å¿«é€Ÿã€å‡†ç¡®å’Œæ³›åŒ–çš„é¢„æµ‹ï¼Œæ”¯æŒè·¨æ•°æ®é›†çš„è§„æ¨¡åŒ–åˆ†æã€‚è¿™ä¸€æ¡†æ¶ä¸ºæœªæ¥å¤§è§„æ¨¡ç™½è´¨å½¢æ€åˆ†æå¥ å®šäº†åšå®çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18400v3">PDF</a> 25 pages, 3 figures, 8 tables</p>
<p><strong>Summary</strong><br>     ç™½è´¨çº¤ç»´æŸå½¢æ€å­¦ç‰¹å¾æè¿°çš„æ–°å…´æ–¹æ³•å±•ç°å‡ºè‰¯å¥½å‰æ™¯ï¼Œä¸ºè§£å‰–å­¦å˜å¼‚å’Œè®¤çŸ¥åŠä¸´åºŠè¡¨å‹æä¾›äº†è¡¥å……ä¿¡æ¯ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿè®¡ç®—å½¢æ€å­¦ç‰¹å¾çš„æ–¹æ³•ç”±äºä¾èµ–ä½“ç´ çº§è¡¨ç¤ºï¼Œåœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ—¶è®¡ç®—é‡å¤§ä¸”è€—æ—¶ã€‚æœ¬æ–‡æå‡ºTract2Shapeæ¡†æ¶ï¼Œé‡‡ç”¨å‡ ä½•ç‚¹äº‘å’Œæ ‡é‡è¡¨æ ¼ç‰¹å¾çš„å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ é¢„æµ‹ç™½è´¨çº¤ç»´æŸå½¢æ€å­¦ç‰¹å¾çš„åä¸ªæŒ‡æ ‡ã€‚ä¸ºæé«˜æ¨¡å‹æ•ˆç‡ï¼Œä½¿ç”¨é™ç»´ç®—æ³•é¢„æµ‹äº”ä¸ªä¸»è¦å½¢æ€å­¦æˆåˆ†ã€‚åœ¨HC-PYAå’ŒPPMIä¸¤ä¸ªç‹¬ç«‹æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œé€šè¿‡ä¸ç°æœ‰æ¨¡å‹çš„å¯¹æ¯”å®éªŒè¯æ˜Tract2Shapeæ€§èƒ½ä¼˜äºå…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚æ­¤å¤–ï¼Œåœ¨æœªè§è¿‡çš„PPMIæ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚Tract2Shapeå®ç°äº†å¿«é€Ÿã€å‡†ç¡®ã€é€šç”¨çš„ç™½è´¨å½¢æ€å­¦ç‰¹å¾é¢„æµ‹ï¼Œæ”¯æŒè·¨æ•°æ®é›†çš„å¯æ‰©å±•åˆ†æï¼Œä¸ºæœªæ¥å¤§è§„æ¨¡ç™½è´¨å½¢æ€å­¦ç ”ç©¶å¥ å®šäº†è‰¯å¥½åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç™½è´¨çº¤ç»´æŸå½¢æ€å­¦ç‰¹å¾æè¿°å…·æœ‰å‰æ™¯ï¼Œä¸ºè®¤çŸ¥å’Œä¸´åºŠè¡¨å‹ç ”ç©¶æä¾›è¡¥å……ä¿¡æ¯ã€‚</li>
<li>ä¼ ç»Ÿè®¡ç®—å½¢æ€å­¦ç‰¹å¾æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ—¶å­˜åœ¨è®¡ç®—é‡å¤§å’Œè€—æ—¶çš„é—®é¢˜ã€‚</li>
<li>Tract2Shapeæ¡†æ¶é‡‡ç”¨å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ é¢„æµ‹ç™½è´¨çº¤ç»´æŸå½¢æ€å­¦ç‰¹å¾çš„åä¸ªæŒ‡æ ‡ï¼Œæé«˜é¢„æµ‹æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>æ¨¡å‹åˆ©ç”¨é™ç»´ç®—æ³•é¢„æµ‹ä¸»è¦å½¢æ€å­¦æˆåˆ†ä»¥å¢å¼ºæ•ˆç‡ã€‚</li>
<li>åœ¨ä¸¤ä¸ªç‹¬ç«‹æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜Tract2Shapeæ€§èƒ½ä¼˜äºå…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>Tract2Shapeåœ¨æœªè§è¿‡çš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18400">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0c7c2a2d7f366b6cf266a0da24959227~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995981&auth_key=1760995981-0-0-80747d7058bd8b9a44a360e0a72f893a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bf91de1c8d125e4f27035e732abbc091~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995989&auth_key=1760995989-0-0-f57d674f404a3de9568e1a94d3f3e270&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="X-2-Gaussian-4D-Radiative-Gaussian-Splatting-for-Continuous-time-Tomographic-Reconstruction"><a href="#X-2-Gaussian-4D-Radiative-Gaussian-Splatting-for-Continuous-time-Tomographic-Reconstruction" class="headerlink" title="X$^{2}$-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time   Tomographic Reconstruction"></a>X$^{2}$-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time   Tomographic Reconstruction</h2><p><strong>Authors:Weihao Yu, Yuanhao Cai, Ruyi Zha, Zhiwen Fan, Chenxin Li, Yixuan Yuan</strong></p>
<p>Four-dimensional computed tomography (4D CT) reconstruction is crucial for capturing dynamic anatomical changes but faces inherent limitations from conventional phase-binning workflows. Current methods discretize temporal resolution into fixed phases with respiratory gating devices, introducing motion misalignment and restricting clinical practicality. In this paper, We propose X$^2$-Gaussian, a novel framework that enables continuous-time 4D-CT reconstruction by integrating dynamic radiative Gaussian splatting with self-supervised respiratory motion learning. Our approach models anatomical dynamics through a spatiotemporal encoder-decoder architecture that predicts time-varying Gaussian deformations, eliminating phase discretization. To remove dependency on external gating devices, we introduce a physiology-driven periodic consistency loss that learns patient-specific breathing cycles directly from projections via differentiable optimization. Extensive experiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR gain over traditional methods and 2.25 dB improvement against prior Gaussian splatting techniques. By unifying continuous motion modeling with hardware-free period learning, X$^2$-Gaussian advances high-fidelity 4D CT reconstruction for dynamic clinical imaging. Code is publicly available at: <a target="_blank" rel="noopener" href="https://x2-gaussian.github.io/">https://x2-gaussian.github.io/</a>. </p>
<blockquote>
<p>å››ç»´è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆ4D CTï¼‰é‡å»ºå¯¹äºæ•æ‰åŠ¨æ€è§£å‰–ç»“æ„å˜åŒ–è‡³å…³é‡è¦ï¼Œä½†é¢ä¸´ç€ä¼ ç»Ÿç›¸ä½åˆ†ç¦»å·¥ä½œæµç¨‹çš„å›ºæœ‰å±€é™æ€§ã€‚å½“å‰çš„æ–¹æ³•é€šè¿‡å°†æ—¶é—´åˆ†è¾¨ç‡ç¦»æ•£åŒ–ä¸ºå…·æœ‰å‘¼å¸é—¨æ§è£…ç½®çš„å›ºå®šç›¸ä½æ¥å¼•å…¥è¿åŠ¨é”™ä½ï¼Œå¹¶é™åˆ¶äº†ä¸´åºŠå®ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†X$^2$-Gaussianï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆåŠ¨æ€è¾å°„é«˜æ–¯å±•å¼€å’Œè‡ªæˆ‘ç›‘ç£çš„å‘¼å¸è¿åŠ¨å­¦ä¹ ï¼Œå®ç°äº†è¿ç»­æ—¶é—´4D-CTé‡å»ºã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ—¶ç©ºç¼–ç å™¨-è§£ç å™¨æ¶æ„å¯¹è§£å‰–ç»“æ„åŠ¨æ€è¿›è¡Œå»ºæ¨¡ï¼Œé¢„æµ‹éšæ—¶é—´å˜åŒ–çš„é«˜æ–¯å˜å½¢ï¼Œæ¶ˆé™¤äº†ç›¸ä½ç¦»æ•£åŒ–ã€‚ä¸ºäº†æ¶ˆé™¤å¯¹å¤–éƒ¨é—¨æ§è®¾å¤‡çš„ä¾èµ–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç”Ÿç†é©±åŠ¨å‘¨æœŸæ€§ä¸€è‡´æ€§æŸå¤±ï¼Œé€šè¿‡å¯å¾®ä¼˜åŒ–ç›´æ¥ä»æŠ•å½±ä¸­å­¦ä¹ æ‚£è€…ç‰¹å®šçš„å‘¼å¸å‘¨æœŸã€‚å¤§é‡å®éªŒè¯æ˜äº†å…¶å“è¶Šæ€§èƒ½ï¼Œç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•è·å¾—9.93 dBçš„å³°å€¼ä¿¡å™ªæ¯”å¢ç›Šï¼Œç›¸æ¯”å…ˆå‰çš„é«˜æ–¯å±•å¼€æŠ€æœ¯æé«˜äº†2.25 dBã€‚é€šè¿‡ç»Ÿä¸€è¿ç»­è¿åŠ¨å»ºæ¨¡ä¸æ— ç¡¬ä»¶å‘¨æœŸå­¦ä¹ ï¼ŒX$^2$-Gaussianæ¨åŠ¨äº†é«˜ä¿çœŸ4D CTé‡å»ºåœ¨åŠ¨æ€ä¸´åºŠå½±åƒä¸­çš„åº”ç”¨ã€‚ä»£ç å…¬å¼€å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://x2-gaussian.github.io/%E3%80%82">https://x2-gaussian.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21779v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://x2-gaussian.github.io/">https://x2-gaussian.github.io/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºX$^2$-Gaussiançš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºè¿ç»­æ—¶é—´å››ç»´è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆ4D CTï¼‰é‡å»ºã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆåŠ¨æ€è¾å°„é«˜æ–¯å±•å®½å’Œè‡ªç›‘ç£å‘¼å¸è¿åŠ¨å­¦ä¹ ï¼Œå®ç°äº†æ— é—´æ–­çš„æ—¶é—´è§£æé‡å»ºè¿‡ç¨‹ã€‚å¼•å…¥çš„æ—¶ç©ºç¼–ç å™¨-è§£ç å™¨æ¶æ„èƒ½æ¨¡æ‹Ÿè§£å‰–åŠ¨æ€å˜åŒ–ï¼Œé¢„æµ‹æ—¶é—´å˜åŒ–çš„é«˜æ–¯å˜å½¢ï¼Œæ¶ˆé™¤äº†å¯¹å‘¼å¸é—¨æ§è®¾å¤‡çš„ä¾èµ–ã€‚é€šè¿‡é‡‡ç”¨ç”Ÿç†å‘¨æœŸä¸€è‡´æ€§æŸå¤±å‡½æ•°ï¼Œç›´æ¥ä»æŠ•å½±ä¸­å­¦ä¹ æ‚£è€…ç‰¹å®šçš„å‘¼å¸å‘¨æœŸã€‚å®éªŒè¯æ˜ï¼ŒX$^2$-Gaussianç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•å’Œä¹‹å‰çš„é«˜æ–¯å±•å®½æŠ€æœ¯æœ‰æ˜¾è‘—æ”¹å–„ï¼Œå³°ä¿¡å™ªæ¯”æé«˜9.93dBå’Œ2.25dBã€‚è¯¥æ¡†æ¶ç»Ÿä¸€äº†è¿ç»­è¿åŠ¨å»ºæ¨¡å’Œæ— ç¡¬ä»¶å‘¨æœŸå­¦ä¹ ï¼Œä¸ºåŠ¨æ€ä¸´åºŠæˆåƒçš„é«˜ä¿çœŸå››ç»´CTé‡å»ºæä¾›äº†å…ˆè¿›æ–¹æ¡ˆã€‚ä»£ç å·²å…¬å¼€äºï¼š<a target="_blank" rel="noopener" href="https://x2-gaussian.github.io./">https://x2-gaussian.github.io/ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>X$^2$-Gaussianæ¡†æ¶å®ç°äº†è¿ç»­æ—¶é—´çš„å››ç»´CTé‡å»ºï¼Œè§£å†³äº†ä¼ ç»Ÿç›¸ä½åˆ†ç®±å·¥ä½œæµç¨‹çš„å†…åœ¨é™åˆ¶ã€‚</li>
<li>å¼•å…¥æ—¶ç©ºç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œæ¨¡æ‹Ÿè§£å‰–åŠ¨æ€å˜åŒ–ï¼Œé¢„æµ‹æ—¶é—´å˜åŒ–çš„é«˜æ–¯å˜å½¢ã€‚</li>
<li>é€šè¿‡æ•´åˆåŠ¨æ€è¾å°„é«˜æ–¯å±•å®½å’Œè‡ªç›‘ç£å‘¼å¸è¿åŠ¨å­¦ä¹ ï¼Œæ¶ˆé™¤äº†å¯¹å‘¼å¸é—¨æ§è®¾å¤‡çš„éœ€æ±‚ã€‚</li>
<li>é¦–æ¬¡æå‡ºç”Ÿç†å‘¨æœŸä¸€è‡´æ€§æŸå¤±å‡½æ•°ï¼Œå…è®¸ç›´æ¥ä»æŠ•å½±ä¸­å­¦ä¹ æ‚£è€…ç‰¹å®šçš„å‘¼å¸å‘¨æœŸã€‚</li>
<li>ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•å’Œå…ˆå‰çš„æŠ€æœ¯ï¼ŒX$^2$-Gaussianåœ¨å³°ä¿¡å™ªæ¯”ä¸Šæœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†è¿ç»­è¿åŠ¨å»ºæ¨¡å’Œæ— ç¡¬ä»¶å‘¨æœŸå­¦ä¹ ï¼Œä¸ºåŠ¨æ€ä¸´åºŠæˆåƒæä¾›äº†é«˜ä¿çœŸå››ç»´CTé‡å»ºæ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21779">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f3850525fae5b7f013b7a3942884934f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760995996&auth_key=1760995996-0-0-929bdd7d8fd0947e10843324c3a8e63a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f49948d6ced32d933ccd8641f1236595~resize:0:q75.jpg?source=1f5c5e47&expiration=1760996003&auth_key=1760996003-0-0-f97d24c3c3da6454776e40b783b34810&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6c5352bd178b47a97ebde2a7a1b1dba0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760996010&auth_key=1760996010-0-0-6108fcb4ebae21c6e0640343d7b19df7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fb891311528be8d60053b044edab740f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760996017&auth_key=1760996017-0-0-e56277d8632b92b3d1b969d0eca336e6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-490a5aae546546e4562a95c52b340978~resize:0:q75.jpg?source=1f5c5e47&expiration=1760996024&auth_key=1760996024-0-0-7007a26634a26fc0d35521e1de5f6791&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fd0912abeccba0ae7ca7cd155e6fd6e5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760996030&auth_key=1760996030-0-0-d45a6c13f20078baef43622cb0827521&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Predicting-gene-essentiality-and-drug-response-from-perturbation-screens-in-preclinical-cancer-models-with-LEAP-Layered-Ensemble-of-Autoencoders-and-Predictors"><a href="#Predicting-gene-essentiality-and-drug-response-from-perturbation-screens-in-preclinical-cancer-models-with-LEAP-Layered-Ensemble-of-Autoencoders-and-Predictors" class="headerlink" title="Predicting gene essentiality and drug response from perturbation screens   in preclinical cancer models with LEAP: Layered Ensemble of Autoencoders and   Predictors"></a>Predicting gene essentiality and drug response from perturbation screens   in preclinical cancer models with LEAP: Layered Ensemble of Autoencoders and   Predictors</h2><p><strong>Authors:Barbara Bodinier, Gaetan Dissez, Lucile Ter-Minassian, Linus Bleistein, Roberta Codato, John Klein, Eric Durand, Antonin Dauvin</strong></p>
<p>High-throughput preclinical perturbation screens, where the effects of genetic, chemical, or environmental perturbations are systematically tested on disease models, hold significant promise for machine learning-enhanced drug discovery due to their scale and causal nature. Predictive models trained on such datasets can be used to (i) infer perturbation response for previously untested disease models, and (ii) characterise the biological context that affects perturbation response. Existing predictive models suffer from limited reproducibility, generalisability and interpretability. To address these issues, we introduce a framework of Layered Ensemble of Autoencoders and Predictors (LEAP), a general and flexible ensemble strategy to aggregate predictions from multiple regressors trained using diverse gene expression representation models. LEAP consistently improves prediction performances in unscreened cell lines across modelling strategies. In particular, LEAP applied to perturbation-specific LASSO regressors (PS-LASSO) provides a favorable balance between near state-of-the-art performance and low computation time. We also propose an interpretability approach combining model distillation and stability selection to identify important biological pathways for perturbation response prediction in LEAP. Our models have the potential to accelerate the drug discovery pipeline by guiding the prioritisation of preclinical experiments and providing insights into the biological mechanisms involved in perturbation response. The code and datasets used in this work are publicly available. </p>
<blockquote>
<p>é«˜é€šé‡ä¸´åºŠå‰æ‰°åŠ¨ç­›é€‰ï¼Œç³»ç»Ÿæ€§æµ‹è¯•é—ä¼ ã€åŒ–å­¦æˆ–ç¯å¢ƒæ‰°åŠ¨å¯¹ç–¾ç—…æ¨¡å‹çš„å½±å“ï¼Œç”±äºå…¶è§„æ¨¡å’Œå› æœæ€§è´¨ï¼Œåœ¨æœºå™¨å­¦ä¹ è¾…åŠ©è¯ç‰©å‘ç°æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚åœ¨æ­¤ç±»æ•°æ®é›†ä¸Šè®­ç»ƒçš„é¢„æµ‹æ¨¡å‹å¯ç”¨äºï¼ˆiï¼‰æ¨æ–­æœªæµ‹è¯•ç–¾ç—…æ¨¡å‹çš„æ‰°åŠ¨å“åº”ï¼Œï¼ˆiiï¼‰è¡¨å¾å½±å“æ‰°åŠ¨å“åº”çš„ç”Ÿç‰©å­¦èƒŒæ™¯ã€‚ç°æœ‰é¢„æµ‹æ¨¡å‹å­˜åœ¨é‡ç°æ€§ã€é€šç”¨æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆ†å±‚è‡ªåŠ¨ç¼–ç å™¨ä¸é¢„æµ‹å™¨ç»„åˆæ¡†æ¶ï¼ˆLEAPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨ä¸”çµæ´»çš„é›†æˆç­–ç•¥ï¼Œç”¨äºèšåˆä½¿ç”¨å„ç§åŸºå› è¡¨è¾¾è¡¨ç¤ºæ¨¡å‹è®­ç»ƒçš„å¤šä¸ªå›å½’å™¨çš„é¢„æµ‹ç»“æœã€‚LEAPåœ¨æœªç»ç­›é€‰çš„ç»†èƒç³»ä¸­ä¸æ–­æ”¹è¿›é¢„æµ‹æ€§èƒ½ï¼Œè·¨è¶Šå„ç§å»ºæ¨¡ç­–ç•¥ã€‚ç‰¹åˆ«æ˜¯å°†LEAPåº”ç”¨äºç‰¹å®šæ‰°åŠ¨LASSOå›å½’å™¨ï¼ˆPS-LASSOï¼‰æ—¶ï¼Œåœ¨æ¥è¿‘æœ€æ–°æ€§èƒ½çš„åŒæ—¶æä¾›äº†è¾ƒä½çš„è®¡ç®—æ—¶é—´çš„æœ‰åˆ©å¹³è¡¡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œç»“åˆæ¨¡å‹è’¸é¦å’Œç¨³å®šæ€§é€‰æ‹©ï¼Œä»¥è¯†åˆ«LEAPä¸­ç”¨äºæ‰°åŠ¨å“åº”é¢„æµ‹çš„é‡è¦ç”Ÿç‰©é€”å¾„ã€‚æˆ‘ä»¬çš„æ¨¡å‹å…·æœ‰åŠ é€Ÿè¯ç‰©å‘ç°æµç¨‹çš„æ½œåŠ›ï¼Œå¯é€šè¿‡æŒ‡å¯¼ä¸´åºŠå‰å®éªŒçš„ä¼˜å…ˆçº§æ’åºï¼Œå¹¶æä¾›æœ‰å…³æ‰°åŠ¨ååº”ä¸­æ¶‰åŠçš„ç”Ÿç‰©å­¦æœºåˆ¶çš„è§è§£ã€‚æœ¬å·¥ä½œä¸­ä½¿ç”¨çš„ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15646v2">PDF</a> </p>
<p><strong>Summary</strong><br>     é«˜é€šé‡ä¸´åºŠå‰æ‰°åŠ¨ç­›é€‰å¯¹äºæœºå™¨å­¦ä¹ è¾…åŠ©è¯ç‰©å‘ç°å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå…¶ç³»ç»Ÿæµ‹è¯•é—ä¼ ã€åŒ–å­¦æˆ–ç¯å¢ƒæ‰°åŠ¨å¯¹ç–¾ç—…æ¨¡å‹çš„å½±å“ã€‚ä¸ºæé«˜é¢„æµ‹æ¨¡å‹çš„æ€§èƒ½å¹¶è§£å†³ç°æœ‰æ¨¡å‹çš„å¯é‡å¤æ€§ã€é€šç”¨æ€§å’Œå¯è§£é‡Šæ€§é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºâ€œè·³è·ƒå¼é›†æˆè‡ªåŠ¨ç¼–ç å™¨ä¸é¢„æµ‹å™¨â€ï¼ˆLEAPï¼‰çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ•´åˆå¤šä¸ªå›å½’é¢„æµ‹å™¨çš„é¢„æµ‹ç»“æœï¼Œé‡‡ç”¨å¤šæ ·åŒ–çš„åŸºå› è¡¨è¾¾è¡¨ç¤ºæ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œä¸”èƒ½åœ¨æœªç­›é€‰çš„ç»†èƒç³»ä¸­æŒç»­æé«˜é¢„æµ‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜ç»“åˆäº†æ¨¡å‹è’¸é¦å’Œç¨³å®šæ€§é€‰æ‹©ï¼Œæå‡ºäº†ä¸€ç§å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œç”¨äºè¯†åˆ«å¯¹æ‰°åŠ¨ååº”é¢„æµ‹é‡è¦çš„ç”Ÿç‰©é€”å¾„ã€‚æ­¤ç ”ç©¶æœ‰æœ›åŠ å¿«è¯ç‰©å‘ç°æµç¨‹ï¼Œä¸ºä¸´åºŠå‰å®éªŒæä¾›æŒ‡å¯¼ï¼Œå¹¶æ·±å…¥äº†è§£æ‰°åŠ¨ååº”ä¸­çš„ç”Ÿç‰©æœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜é€šé‡ä¸´åºŠå‰æ‰°åŠ¨ç­›é€‰å¯¹äºæœºå™¨å­¦ä¹ è¾…åŠ©è¯ç‰©å‘ç°å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>ç°æœ‰é¢„æµ‹æ¨¡å‹å­˜åœ¨å¯é‡å¤æ€§ã€é€šç”¨æ€§å’Œå¯è§£é‡Šæ€§é—®é¢˜ã€‚</li>
<li>å¼•å…¥çš„LEAPæ¡†æ¶èƒ½æ•´åˆå¤šä¸ªå›å½’é¢„æµ‹å™¨çš„ç»“æœï¼Œæé«˜é¢„æµ‹æ€§èƒ½ã€‚</li>
<li>LEAPä¸PS-LASSOç»“åˆå±•ç°äº†å“è¶Šçš„æ€§èƒ½å’Œè¾ƒä½çš„è®¡ç®—æ—¶é—´ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆæ¨¡å‹è’¸é¦å’Œç¨³å®šæ€§é€‰æ‹©çš„å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œä»¥è¯†åˆ«é‡è¦ç”Ÿç‰©é€”å¾„ã€‚</li>
<li>æ­¤ç ”ç©¶æ¨¡å‹æœ‰æ½œåŠ›åŠ é€Ÿè¯ç‰©å‘ç°æµç¨‹ï¼ŒæŒ‡å¯¼ä¸´åºŠå‰å®éªŒä¼˜å…ˆæ¬¡åºã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨çš„ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15646">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-20ed0530acadfc63b3df68fe4320a996~resize:0:q75.jpg?source=1f5c5e47&expiration=1760998124&auth_key=1760998124-0-0-c0db9be7a2bfa16fa398cba5f1884ca1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cbb4659e2aaf0ad5cd33388af73fdcf0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760996056&auth_key=1760996056-0-0-eda18df9e86af189ac4f6d68e8cd2857&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-21/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-a482ecd4f7832d0ace316af507c83a70~resize:0:q75.jpg?source=1f5c5e47&expiration=1760996622&auth_key=1760996622-0-0-a85db7e22d1c9dd86e38ad031835f376&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-21  MRSAudio A Large-Scale Multimodal Recorded Spatial Audio Dataset with   Refined Annotations
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-21/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-3655cebcca788ee6940d7aa437f37572~resize:0:q75.jpg?source=1f5c5e47&expiration=1760992959&auth_key=1760992959-0-0-6e97ba00ebad15e4d833bd22e8e2b8c6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-21  LightsOut Diffusion-based Outpainting for Enhanced Lens Flare Removal
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32127.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
