<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-21  OmniVinci Enhancing Architecture and Data for Omni-Modal Understanding   LLM">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-cc0af8f8e3b7df25e92c12d487574419')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    67 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-21-æ›´æ–°"><a href="#2025-10-21-æ›´æ–°" class="headerlink" title="2025-10-21 æ›´æ–°"></a>2025-10-21 æ›´æ–°</h1><h2 id="OmniVinci-Enhancing-Architecture-and-Data-for-Omni-Modal-Understanding-LLM"><a href="#OmniVinci-Enhancing-Architecture-and-Data-for-Omni-Modal-Understanding-LLM" class="headerlink" title="OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding   LLM"></a>OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding   LLM</h2><p><strong>Authors:Hanrong Ye, Chao-Han Huck Yang, Arushi Goel, Wei Huang, Ligeng Zhu, Yuanhang Su, Sean Lin, An-Chieh Cheng, Zhen Wan, Jinchuan Tian, Yuming Lou, Dong Yang, Zhijian Liu, Yukang Chen, Ambrish Dantrey, Ehsan Jahangiri, Sreyan Ghosh, Daguang Xu, Ehsan Hosseini-Asl, Danial Mohseni Taheri, Vidya Murali, Sifei Liu, Jason Lu, Oluwatobi Olabiyi, Frank Wang, Rafael Valle, Bryan Catanzaro, Andrew Tao, Song Han, Jan Kautz, Hongxu Yin, Pavlo Molchanov</strong></p>
<p>Advancing machine intelligence requires developing the ability to perceive across multiple modalities, much as humans sense the world. We introduce OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We carefully study the design choices across model architecture and data curation. For model architecture, we present three key innovations: (i) OmniAlignNet for strengthening alignment between vision and audio embeddings in a shared omni-modal latent space; (ii) Temporal Embedding Grouping for capturing relative temporal alignment between vision and audio signals; and (iii) Constrained Rotary Time Embedding for encoding absolute temporal information in omni-modal embeddings. We introduce a curation and synthesis pipeline that generates 24M single-modal and omni-modal conversations. We find that modalities reinforce one another in both perception and reasoning. Our model, OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while using just 0.2T training tokens - a 6 times reduction compared to Qwen2.5-Omniâ€™s 1.2T. We finally demonstrate omni-modal advantages in downstream applications spanning robotics, medical AI, and smart factory. </p>
<blockquote>
<p>æ¨è¿›æœºå™¨æ™ºèƒ½éœ€è¦åŸ¹å…»å…¶åœ¨å¤šç§æ¨¡å¼ä¸‹çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå°±åƒäººç±»æ„ŸçŸ¥ä¸–ç•Œä¸€æ ·ã€‚æˆ‘ä»¬æ¨å‡ºOmniVincié¡¹ç›®ï¼Œæ—¨åœ¨æ„å»ºä¸€ä¸ªå¼ºå¤§ã€å¼€æºçš„å…¨æ–¹ä½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚æˆ‘ä»¬ä»”ç»†ç ”ç©¶äº†æ¨¡å‹æ¶æ„å’Œæ•°æ®æ•´ç†çš„è®¾è®¡é€‰æ‹©ã€‚åœ¨æ¨¡å‹æ¶æ„æ–¹é¢ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰é¡¹å…³é”®åˆ›æ–°ï¼šï¼ˆiï¼‰OmniAlignNetï¼Œç”¨äºåŠ å¼ºè§†è§‰å’ŒéŸ³é¢‘åµŒå…¥åœ¨å…±äº«å…¨æ–¹ä½æ½œåœ¨ç©ºé—´ä¸­çš„å¯¹é½ï¼›ï¼ˆiiï¼‰Temporal Embedding Groupingï¼Œç”¨äºæ•æ‰è§†è§‰å’ŒéŸ³é¢‘ä¿¡å·ä¹‹é—´çš„ç›¸å¯¹æ—¶é—´å¯¹é½ï¼›ï¼ˆiiiï¼‰Constrained Rotary Time Embeddingï¼Œç”¨äºç¼–ç å…¨æ–¹ä½åµŒå…¥ä¸­çš„ç»å¯¹æ—¶é—´ä¿¡æ¯ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ•´ç†å’Œåˆæˆç®¡é“ï¼Œç”Ÿæˆäº†2.4äº¿ä¸ªå•ä¸€æ¨¡å¼å’Œå…¨æ–¹ä½æ¨¡å¼çš„å¯¹è¯ã€‚æˆ‘ä»¬å‘ç°å„ç§æ¨¡å¼åœ¨æ„ŸçŸ¥å’Œæ¨ç†ä¸Šç›¸äº’å¼ºåŒ–ã€‚æˆ‘ä»¬çš„OmniVinciæ¨¡å‹åœ¨DailyOmniï¼ˆè·¨æ¨¡æ€ç†è§£ï¼‰ä¸Šæ¯”Qwen2.5-Omnié«˜å‡º+19.05åˆ†ï¼Œåœ¨MMARï¼ˆéŸ³é¢‘ï¼‰ä¸Šé«˜å‡º+1.7åˆ†ï¼Œåœ¨Video-MMEï¼ˆè§†è§‰ï¼‰ä¸Šé«˜å‡º+3.9åˆ†ï¼›åŒæ—¶åªä½¿ç”¨äº†0.2ä¸‡äº¿ä¸ªè®­ç»ƒä»¤ç‰Œï¼Œä¸Qwen2.5-Omniçš„1.2ä¸‡äº¿ä¸ªè®­ç»ƒä»¤ç‰Œç›¸æ¯”å‡å°‘äº†6å€ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨æ¶µç›–æœºå™¨äººã€åŒ»ç–—äººå·¥æ™ºèƒ½å’Œæ™ºèƒ½å·¥å‚çš„ä¸‹æ¸¸åº”ç”¨ä¸­å±•ç¤ºäº†å…¨æ–¹ä½æ¨¡å¼çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15870v1">PDF</a> Technical Report. Code: <a target="_blank" rel="noopener" href="https://github.com/NVlabs/OmniVinci">https://github.com/NVlabs/OmniVinci</a></p>
<p><strong>Summary</strong></p>
<p>OmniVinciæ˜¯ä¸€ä¸ªå¼€æºçš„ã€å¤šæ¨¡æ€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜æœºå™¨çš„æ™ºèƒ½æ„ŸçŸ¥èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨æ¶æ„è®¾è®¡ä¸Šæœ‰å¤šé¡¹åˆ›æ–°ï¼ŒåŒ…æ‹¬OmniAlignNetã€Temporal Embedding Groupingå’ŒConstrained Rotary Time Embeddingç­‰æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†æ•°æ®æ•´ç†ä¸åˆæˆæµç¨‹ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†OmniVinciæ¨¡å‹åœ¨è·¨æ¨¡æ€ç†è§£ã€éŸ³é¢‘å’Œè§†è§‰ä»»åŠ¡ä¸Šçš„ä¼˜å¼‚æ€§èƒ½ã€‚è¯¥æ¨¡å‹åœ¨æœºå™¨äººã€åŒ»ç–—äººå·¥æ™ºèƒ½å’Œæ™ºèƒ½å·¥å‚ç­‰ä¸‹æ¸¸åº”ç”¨å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OmniVinciæ˜¯ä¸€ä¸ªæ—¨åœ¨æé«˜æœºå™¨æ™ºèƒ½æ„ŸçŸ¥èƒ½åŠ›çš„å¼€æºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹æ¶æ„è®¾è®¡æœ‰å¤šé¡¹åˆ›æ–°ï¼ŒåŒ…æ‹¬OmniAlignNetã€Temporal Embedding Groupingå’ŒConstrained Rotary Time Embeddingç­‰æŠ€æœ¯ã€‚</li>
<li>ä»‹ç»äº†æ•°æ®æ•´ç†ä¸åˆæˆæµç¨‹ï¼Œç”Ÿæˆäº†24Mçš„å•æ¨¡æ€å’Œè·¨æ¨¡æ€å¯¹è¯æ•°æ®ã€‚</li>
<li>å®éªŒéªŒè¯äº†OmniVinciæ¨¡å‹åœ¨è·¨æ¨¡æ€ç†è§£ã€éŸ³é¢‘å’Œè§†è§‰ä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>OmniVinciæ¨¡å‹åœ¨æœºå™¨äººã€åŒ»ç–—äººå·¥æ™ºèƒ½å’Œæ™ºèƒ½å·¥å‚ç­‰ä¸‹æ¸¸åº”ç”¨å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>æ¨¡å‹ä½¿ç”¨0.2Tè®­ç»ƒä»¤ç‰Œï¼Œç›¸è¾ƒäºQwen2.5-Omniä½¿ç”¨çš„1.2Tï¼Œå®ç°äº†6å€çš„å‡å°‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15870">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e6b5658866d244c7df45f8f7dcf2c665" align="middle">
<img src="https://picx.zhimg.com/v2-0448ceff4a59b74db5dcd6486b2f9c38" align="middle">
<img src="https://picx.zhimg.com/v2-42d41ba3fe5f82deb78752964fbbc012" align="middle">
<img src="https://picx.zhimg.com/v2-eadfff9e90b0c7b1c979935534cb8d75" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="BiomedXPro-Prompt-Optimization-for-Explainable-Diagnosis-with-Biomedical-Vision-Language-Models"><a href="#BiomedXPro-Prompt-Optimization-for-Explainable-Diagnosis-with-Biomedical-Vision-Language-Models" class="headerlink" title="BiomedXPro: Prompt Optimization for Explainable Diagnosis with   Biomedical Vision Language Models"></a>BiomedXPro: Prompt Optimization for Explainable Diagnosis with   Biomedical Vision Language Models</h2><p><strong>Authors:Kaushitha Silva, Mansitha Eashwara, Sanduni Ubayasiri, Ruwan Tennakoon, Damayanthi Herath</strong></p>
<p>The clinical adoption of biomedical vision-language models is hindered by prompt optimization techniques that produce either uninterpretable latent vectors or single textual prompts. This lack of transparency and failure to capture the multi-faceted nature of clinical diagnosis, which relies on integrating diverse observations, limits their trustworthiness in high-stakes settings. To address this, we introduce BiomedXPro, an evolutionary framework that leverages a large language model as both a biomedical knowledge extractor and an adaptive optimizer to automatically generate a diverse ensemble of interpretable, natural-language prompt pairs for disease diagnosis. Experiments on multiple biomedical benchmarks show that BiomedXPro consistently outperforms state-of-the-art prompt-tuning methods, particularly in data-scarce few-shot settings. Furthermore, our analysis demonstrates a strong semantic alignment between the discovered prompts and statistically significant clinical features, grounding the modelâ€™s performance in verifiable concepts. By producing a diverse ensemble of interpretable prompts, BiomedXPro provides a verifiable basis for model predictions, representing a critical step toward the development of more trustworthy and clinically-aligned AI systems. </p>
<blockquote>
<p>ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„é‡‡ç”¨å—åˆ°äº†æç¤ºä¼˜åŒ–æŠ€æœ¯çš„é˜»ç¢ï¼Œè¿™äº›æŠ€æœ¯äº§ç”Ÿäº†ä¸å¯è§£é‡Šçš„æ½œåœ¨å‘é‡æˆ–å•ä¸€æ–‡æœ¬æç¤ºã€‚è¿™ç§é€æ˜åº¦çš„ç¼ºä¹ä»¥åŠæœªèƒ½æ•æ‰åˆ°ä¾èµ–äºæ•´åˆå„ç§è§‚å¯Ÿçš„ä¸´åºŠè¯Šæ–­çš„å¤šé¢æ€§ç‰¹å¾ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨é«˜é£é™©ç¯å¢ƒä¸­çš„å¯ä¿¡åº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†BiomedXProï¼Œè¿™æ˜¯ä¸€ä¸ªè¿›åŒ–æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºç”Ÿç‰©åŒ»å­¦çŸ¥è¯†æå–å™¨å’Œè‡ªé€‚åº”ä¼˜åŒ–å™¨ï¼Œè‡ªåŠ¨ç”Ÿæˆå¤šç§å¯è§£é‡Šçš„è‡ªç„¶è¯­è¨€æç¤ºå¯¹ï¼Œç”¨äºç–¾ç—…è¯Šæ–­ã€‚åœ¨å¤šä¸ªç”Ÿç‰©åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒBiomedXProæŒç»­ä¼˜äºæœ€æ–°çš„æç¤ºè°ƒæ•´æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„å°‘é‡æ ·æœ¬ç¯å¢ƒä¸­è¡¨ç°å°¤å…¶å‡ºè‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå‘ç°çš„æç¤ºä¸ç»Ÿè®¡å­¦ä¸Šé‡è¦çš„ä¸´åºŠç‰¹å¾ä¹‹é—´å­˜åœ¨å¼ºçƒˆè¯­ä¹‰å¯¹é½ï¼Œä¸ºæ¨¡å‹çš„æ€§èƒ½æä¾›äº†å¯éªŒè¯çš„æ¦‚å¿µåŸºç¡€ã€‚é€šè¿‡ç”Ÿæˆå¤šç§å¯è§£é‡Šæç¤ºçš„é›†åˆï¼ŒBiomedXProä¸ºæ¨¡å‹é¢„æµ‹æä¾›äº†å¯éªŒè¯çš„åŸºç¡€ï¼Œè¿™æ˜¯æœç€å¼€å‘æ›´å¯ä¿¡èµ–ä¸”ä¸ä¸´åºŠç›¸ç¬¦çš„AIç³»ç»Ÿçš„é‡è¦ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15866v1">PDF</a> 10 Pages + 15 Supplementary Material Pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„ä¼˜åŒ–æŠ€æœ¯éš¾é¢˜ï¼ŒåŒ…æ‹¬äº§ç”Ÿä¸å¯è§£é‡Šçš„æ½œåœ¨å‘é‡æˆ–å•ä¸€æ–‡æœ¬æç¤ºçš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºBiomedXProæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºç”Ÿç‰©åŒ»å­¦çŸ¥è¯†æå–å™¨å’Œè‡ªé€‚åº”ä¼˜åŒ–å™¨ï¼Œè‡ªåŠ¨ç”Ÿæˆå¤šæ ·åŒ–çš„ã€å¯è§£é‡Šçš„è‡ªç„¶è¯­è¨€æç¤ºå¯¹è¿›è¡Œç–¾ç—…è¯Šæ–­ã€‚å®éªŒè¡¨æ˜ï¼ŒBiomedXProåœ¨å¤šä¸ªç”Ÿç‰©åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„å°‘æ•°æ ·æœ¬ç¯å¢ƒä¸­ã€‚æ­¤å¤–ï¼Œåˆ†ææ˜¾ç¤ºå‘ç°çš„æç¤ºä¸ä¸´åºŠç‰¹å¾ä¹‹é—´å­˜åœ¨å¼ºçƒˆè¯­ä¹‰å¯¹é½ï¼Œä¸ºæ¨¡å‹é¢„æµ‹æä¾›äº†å¯éªŒè¯çš„åŸºç¡€ã€‚è¿™ä»£è¡¨äº†å¯¹æ›´å¯ä¿¡èµ–ä¸”ä¸´åºŠç›¸ç¬¦çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿå‘å±•çš„å…³é”®ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠåº”ç”¨ä¸­å­˜åœ¨ä¼˜åŒ–é—®é¢˜ï¼Œæ¶‰åŠæ½œåœ¨å‘é‡å’Œæ–‡æœ¬æç¤ºçš„é€æ˜åº¦ä¸è¶³ã€‚</li>
<li>BiomedXProæ¡†æ¶æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–ã€å¯è§£é‡Šçš„è‡ªç„¶è¯­è¨€æç¤ºå¯¹è¿›è¡Œç–¾ç—…è¯Šæ–­ã€‚</li>
<li>å®éªŒè¯æ˜BiomedXProåœ¨å¤šä¸ªç”Ÿç‰©åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå°¤å…¶åœ¨æ•°æ®ç¨€ç¼ºç¯å¢ƒä¸­ã€‚</li>
<li>å‘ç°çš„ä¸´åºŠæç¤ºä¸ç»Ÿè®¡æ˜¾è‘—çš„ä¸´åºŠç‰¹å¾ä¹‹é—´å­˜åœ¨å¼ºçƒˆè¯­ä¹‰å¯¹é½ã€‚</li>
<li>BiomedXProæä¾›äº†æ¨¡å‹é¢„æµ‹çš„å¯éªŒè¯åŸºç¡€ï¼Œæœ‰åŠ©äºå»ºç«‹æ›´å¯ä¿¡èµ–çš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿã€‚</li>
<li>æ­¤æ¡†æ¶ä»£è¡¨äº†æœç€å¼€å‘æ›´è´´è¿‘ä¸´åºŠå®é™…éœ€æ±‚çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿè¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15866">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cdc148660cedd6204b366182472540ac" align="middle">
<img src="https://picx.zhimg.com/v2-edfb3fd3b83d56bc352e596e92094689" align="middle">
<img src="https://picx.zhimg.com/v2-1f9a0469c88a4b9e58c6904e5b7cdc24" align="middle">
<img src="https://picx.zhimg.com/v2-f4a17bae8a312d0a58bb9e83dae855bf" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PokeeResearch-Effective-Deep-Research-via-Reinforcement-Learning-from-AI-Feedback-and-Robust-Reasoning-Scaffold"><a href="#PokeeResearch-Effective-Deep-Research-via-Reinforcement-Learning-from-AI-Feedback-and-Robust-Reasoning-Scaffold" class="headerlink" title="PokeeResearch: Effective Deep Research via Reinforcement Learning from   AI Feedback and Robust Reasoning Scaffold"></a>PokeeResearch: Effective Deep Research via Reinforcement Learning from   AI Feedback and Robust Reasoning Scaffold</h2><p><strong>Authors:Yi Wan, Jiuqi Wang, Liam Li, Jinsong Liu, Ruihao Zhu, Zheqing Zhu</strong></p>
<p>Tool-augmented large language models (LLMs) are emerging as deep research agents, systems that decompose complex queries, retrieve external evidence, and synthesize grounded responses. Yet current agents remain limited by shallow retrieval, weak alignment metrics, and brittle tool-use behavior. We introduce PokeeResearch-7B, a 7B-parameter deep research agent built under a unified reinforcement learning framework for robustness, alignment, and scalability. PokeeResearch-7B is trained by an annotation-free Reinforcement Learning from AI Feedback (RLAIF) framework to optimize policies using LLM-based reward signals that capture factual accuracy, citation faithfulness, and instruction adherence. A chain-of-thought-driven multi-call reasoning scaffold further enhances robustness through self-verification and adaptive recovery from tool failures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves state-of-the-art performance among 7B-scale deep research agents. This highlights that careful reinforcement learning and reasoning design can produce efficient, resilient, and research-grade AI agents. The model and inference code is open-sourced under MIT license at <a target="_blank" rel="noopener" href="https://github.com/Pokee-AI/PokeeResearchOSS">https://github.com/Pokee-AI/PokeeResearchOSS</a>. </p>
<blockquote>
<p>å·¥å…·å¢å¼ºå‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£é€æ¸å‘å±•ä¸ºæ·±åº¦ç ”ç©¶ä»£ç†ï¼Œè¿™äº›ç³»ç»Ÿèƒ½å¤Ÿåˆ†è§£å¤æ‚æŸ¥è¯¢ã€æ£€ç´¢å¤–éƒ¨è¯æ®å¹¶ç»¼åˆç”Ÿæˆæœ‰æ ¹æ®çš„å›åº”ã€‚ç„¶è€Œï¼Œå½“å‰ä»£ç†ä»å—é™äºæµ…å±‚æ£€ç´¢ã€å¼±å¯¹é½æŒ‡æ ‡å’Œè„†å¼±çš„å·¥å…·ä½¿ç”¨è¡Œä¸ºã€‚æˆ‘ä»¬æ¨å‡ºäº†PokeeResearch-7Bï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸‹æ„å»ºçš„7Bå‚æ•°æ·±åº¦ç ”ç©¶ä»£ç†ï¼Œæ—¨åœ¨å®ç°ç¨³å¥æ€§ã€å¯¹é½æ€§å’Œå¯æ‰©å±•æ€§ã€‚PokeeResearch-7Bé€šè¿‡æ— éœ€æ ‡æ³¨çš„å¼ºåŒ–å­¦ä¹ ä»äººå·¥æ™ºèƒ½åé¦ˆï¼ˆRLAIFï¼‰æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œä¼˜åŒ–ç­–ç•¥ä½¿ç”¨LLMåŸºç¡€å¥–åŠ±ä¿¡å·ï¼Œæ•æ‰äº‹å®å‡†ç¡®æ€§ã€å¼•ç”¨å¿ è¯šåº¦å’ŒæŒ‡ä»¤éµå¾ªåº¦ã€‚åŸºäºæ€ç»´é“¾é©±åŠ¨çš„å¤šå‘¼å«æ¨ç†æ¶æ„é€šè¿‡è‡ªæˆ‘éªŒè¯å’Œå·¥å…·æ•…éšœæ—¶çš„è‡ªé€‚åº”æ¢å¤è¿›ä¸€æ­¥å¢å¼ºäº†ç¨³å¥æ€§ã€‚åœ¨10ä¸ªæµè¡Œçš„æ·±åº¦ç ”ç©¶åŸºå‡†æµ‹è¯•ä¸­ï¼ŒPokeeResearch-7Båœ¨7Bè§„æ¨¡æ·±åº¦ç ”ç©¶ä»£ç†ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™å¼ºè°ƒäº†ç²¾å¿ƒè®¾è®¡çš„å¼ºåŒ–å­¦ä¹ å’Œæ¨ç†å¯ä»¥äº§ç”Ÿé«˜æ•ˆã€åšéŸ§å’Œç ”ç©¶çº§çš„äººå·¥æ™ºèƒ½ä»£ç†ã€‚æ¨¡å‹å’Œæ¨ç†ä»£ç ä»¥MITè®¸å¯è¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Pokee-AI/PokeeResearchOSS%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/Pokee-AI/PokeeResearchOSSä¸Šå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15862v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¼ºåŒ–å­¦ä¹ çš„å·¥å…·å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç ”ç©¶å–å¾—æœ€æ–°è¿›å±•ã€‚æ–°å‹æ¨¡å‹PokeeResearch-7Bå…·å¤‡ç¨³å¥æ€§ã€å¯¹é½æ€§å’Œå¯æ‰©å±•æ€§ï¼Œåœ¨æ·±åº¦ç ”ç©¶ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚é€šè¿‡æ— æ ‡æ³¨å¼ºåŒ–å­¦ä¹ ä»äººå·¥æ™ºèƒ½åé¦ˆï¼ˆRLAIFï¼‰æ¡†æ¶è®­ç»ƒï¼Œæ¨¡å‹èƒ½ä¼˜åŒ–ç­–ç•¥å¹¶æ•è·äº‹å®å‡†ç¡®æ€§ã€å¼•ç”¨å¿ è¯šåº¦å’ŒæŒ‡ä»¤éµå¾ªåº¦ã€‚è¯¥æ¨¡å‹é€šè¿‡è‡ªæˆ‘éªŒè¯å’Œè‡ªé€‚åº”æ¢å¤å·¥å…·æ•…éšœå¢å¼ºäº†ç¨³å¥æ€§ï¼Œå¹¶åœ¨å¤šä¸ªæ·±åº¦ç ”ç©¶åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°ä¸šç•Œæœ€ä½³æ°´å¹³ã€‚æ¨¡å‹åŠæ¨ç†ä»£ç å·²å¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PokeeResearch-7Bæ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„å·¥å…·å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>è¯¥æ¨¡å‹å…·å¤‡ç¨³å¥æ€§ã€å¯¹é½æ€§å’Œå¯æ‰©å±•æ€§ï¼Œé€‚ç”¨äºæ·±åº¦ç ”ç©¶ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡æ— æ ‡æ³¨å¼ºåŒ–å­¦ä¹ ä»äººå·¥æ™ºèƒ½åé¦ˆï¼ˆRLAIFï¼‰æ¡†æ¶è®­ç»ƒï¼Œä¼˜åŒ–ç­–ç•¥ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿæ•è·äº‹å®å‡†ç¡®æ€§ã€å¼•ç”¨å¿ è¯šåº¦å’ŒæŒ‡ä»¤éµå¾ªåº¦ã€‚</li>
<li>é€šè¿‡è‡ªæˆ‘éªŒè¯å’Œè‡ªé€‚åº”æ¢å¤å·¥å…·æ•…éšœå¢å¼ºäº†æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªæ·±åº¦ç ”ç©¶åŸºå‡†æµ‹è¯•ä¸­ï¼ŒPokeeResearch-7Bè¾¾åˆ°äº†ä¸šç•Œæœ€ä½³æ€§èƒ½æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5eddb1ee1e904d051694b69abef9f40" align="middle">
<img src="https://picx.zhimg.com/v2-dcc3b795749171858a1d66222051c44b" align="middle">
<img src="https://picx.zhimg.com/v2-81877be6241b94432e85bcff8b0a7281" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="InfiMed-ORBIT-Aligning-LLMs-on-Open-Ended-Complex-Tasks-via-Rubric-Based-Incremental-Training"><a href="#InfiMed-ORBIT-Aligning-LLMs-on-Open-Ended-Complex-Tasks-via-Rubric-Based-Incremental-Training" class="headerlink" title="InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via   Rubric-Based Incremental Training"></a>InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via   Rubric-Based Incremental Training</h2><p><strong>Authors:Pengkai Wang, Qi Zuo, Pengwei Liu, Zhijie Sang, Congkai Xie, Hongxia Yang</strong></p>
<p>Large Language Models (LLMs) have shown substantial advances through reinforcement learning (RL), particularly in domains where rewards can be programmatically verified, such as mathematics and code. In these areas, models benefit from a well-defined operational base guided by explicit rule-based objectives. However, this progress reveals a significant limitation: in open-ended domains where rewards are ambiguous, subjective, or context-dependent, such as creative writing, scientific reasoning, and notably medical consultation, robust reward functions are lacking, making these areas challenging for current RL strategies. To bridge this gap, we introduce ORBIT, an open-ended rubric-based incremental training framework specifically designed for high-stakes medical dialogue. ORBIT integrates syn- thetic dialogue generation with the dynamic creation of rubrics, employing these rubrics to direct an incremental RL process. In particular, this approach does not depend on external medical knowledge or manual rules, instead utilizing rubric-guided feedback to shape learning. When implemented on the Qwen3-4B-Instruct model, our method can greatly enhance its performance on the HealthBench-Hard benchmark from 7.0 to 27.2 using only 2k samples, thus achieving state-of-the-art results for models of this scale. Our analysis confirms that rubric-driven RL fos-ters consistent performance gains across diverse consultation scenarios, going beyond simple numerical improvements. These findings underscore rubric-based feedback as a scalable strategy for advancing LLMs in intricate, open-ended tasks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¥–åŠ±å¯ä»¥ç¨‹åºåŒ–éªŒè¯çš„é¢†åŸŸï¼Œå¦‚æ•°å­¦å’Œä»£ç ã€‚åœ¨è¿™äº›é¢†åŸŸï¼Œæ¨¡å‹å—ç›Šäºç”±æ˜ç¡®åŸºäºè§„åˆ™çš„å®¢è§‚ç›®æ ‡å¼•å¯¼çš„è‰¯å¥½å®šä¹‰çš„æ“ä½œåŸºç¡€ã€‚ç„¶è€Œï¼Œè¿™ç§è¿›å±•æ­ç¤ºäº†ä¸€ä¸ªé‡å¤§å±€é™ï¼šåœ¨å¥–åŠ±æ¨¡ç³Šã€ä¸»è§‚æˆ–ä¾èµ–äºä¸Šä¸‹æ–‡çš„æ— å®šå¼é¢†åŸŸï¼Œå¦‚åˆ›é€ æ€§å†™ä½œã€ç§‘å­¦æ¨ç†å’Œå°¤å…¶æ˜¯åŒ»ç–—å’¨è¯¢ï¼Œç¼ºä¹ç¨³å¥çš„å¥–åŠ±åŠŸèƒ½ï¼Œä½¿å¾—è¿™äº›é¢†åŸŸå¯¹å½“å‰çš„RLç­–ç•¥æ„æˆæŒ‘æˆ˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ORBITï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºé«˜é£é™©åŒ»ç–—å¯¹è¯è®¾è®¡çš„å¼€æ”¾å¼è¯„åˆ†åŸºå‡†å¢é‡è®­ç»ƒæ¡†æ¶ã€‚ORBITå°†åˆæˆå¯¹è¯ç”Ÿæˆä¸åŠ¨æ€åˆ›å»ºè¯„åˆ†åŸºå‡†ç›¸ç»“åˆï¼Œåˆ©ç”¨è¿™äº›è¯„åˆ†åŸºå‡†æ¥æŒ‡å¯¼å¢é‡RLè¿‡ç¨‹ã€‚ç‰¹åˆ«æ˜¯ï¼Œè¿™ç§æ–¹æ³•ä¸ä¾èµ–å¤–éƒ¨åŒ»å­¦çŸ¥è¯†æˆ–æ‰‹åŠ¨è§„åˆ™ï¼Œè€Œæ˜¯åˆ©ç”¨è¯„åˆ†åŸºå‡†æŒ‡å¯¼çš„åé¦ˆæ¥å¡‘é€ å­¦ä¹ ã€‚å½“åœ¨Qwen3-4B-Instructæ¨¡å‹ä¸Šå®æ–½æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…ä½¿ç”¨2kä¸ªæ ·æœ¬å°±èƒ½å°†å…¶åœ¨HealthBench-HardåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ä»7.0å¤§å¹…æé«˜è‡³27.2ï¼Œä»è€Œä¸ºè¿™ä¸€è§„æ¨¡çš„æ¨¡å‹å®ç°äº†æœ€æ–°æŠ€æœ¯æˆæœã€‚æˆ‘ä»¬çš„åˆ†æè¯å®ï¼Œè¯„åˆ†é©±åŠ¨RLåœ¨å¤šç§å’¨è¯¢åœºæ™¯ä¸­å®ç°äº†æŒç»­çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ç®€å•çš„æ•°å€¼æ”¹è¿›ã€‚è¿™äº›å‘ç°å¼ºè°ƒï¼ŒåŸºäºè¯„åˆ†åŸºå‡†çš„åé¦ˆæ˜¯æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ— å®šå¼å¤æ‚ä»»åŠ¡ä¸­å‘å±•çš„å¯æ‰©å±•ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15859v1">PDF</a> 17 pages, 6 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¥–åŠ±å¯ä»¥ç¨‹åºåŒ–éªŒè¯çš„é¢†åŸŸï¼Œå¦‚æ•°å­¦å’Œä»£ç ã€‚ç„¶è€Œï¼Œåœ¨å¥–åŠ±æ¨¡ç³Šã€ä¸»è§‚æˆ–ä¾èµ–äºä¸Šä¸‹æ–‡çš„å¼€æ”¾é¢†åŸŸï¼Œå¦‚åˆ›æ„å†™ä½œã€ç§‘å­¦æ¨ç†å’ŒåŒ»ç–—å’¨è¯¢ç­‰ï¼Œå½“å‰çš„RLç­–ç•¥é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ORBITï¼Œä¸€ä¸ªé’ˆå¯¹é«˜é£é™©åŒ»ç–—å¯¹è¯çš„å¼€æ”¾å¼è¯„åˆ†åŸºå‡†å¢é‡è®­ç»ƒæ¡†æ¶ã€‚ORBITé€šè¿‡åˆæˆå¯¹è¯ç”Ÿæˆå’ŒåŠ¨æ€åˆ›å»ºè¯„åˆ†åŸºå‡†ï¼Œé‡‡ç”¨è¿™äº›è¯„åˆ†åŸºå‡†æ¥æŒ‡å¯¼å¢é‡RLè¿‡ç¨‹ã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–å¤–éƒ¨åŒ»å­¦çŸ¥è¯†æˆ–æ‰‹åŠ¨è§„åˆ™ï¼Œè€Œæ˜¯åˆ©ç”¨è¯„åˆ†æŒ‡å¯¼çš„åé¦ˆæ¥å¡‘é€ å­¦ä¹ ã€‚åœ¨å®æ–½äºQwen3-4B-Instructæ¨¡å‹åï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨ä»…ä½¿ç”¨2kæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œå°†HealthBench-HardåŸºå‡†æµ‹è¯•çš„æ€§èƒ½ä»7.0å¤§å¹…æé«˜è‡³27.2ï¼Œå®ç°äº†è¯¥è§„æ¨¡æ¨¡å‹çš„æœ€ä½³ç»“æœã€‚åˆ†æè¡¨æ˜ï¼Œè¯„åˆ†é©±åŠ¨RLåœ¨å¤šæ ·åŒ–å’¨è¯¢åœºæ™¯ä¸­å®ç°äº†æ€§èƒ½çš„ç¨³å®šæå‡ï¼Œè¶…è¶Šäº†ç®€å•çš„æ•°å­—æ”¹è¿›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMé€šè¿‡å¼ºåŒ–å­¦ä¹ åœ¨è§„åˆ™æ˜ç¡®ã€å¥–åŠ±å¯ç¨‹åºåŒ–éªŒè¯çš„é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>åœ¨å¼€æ”¾é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åŒ»ç–—å’¨è¯¢ç­‰å¤æ‚åœºæ™¯ä¸­ï¼Œç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥é¢ä¸´å¥–åŠ±æ¨¡ç³Šçš„æŒ‘æˆ˜ã€‚</li>
<li>ORBITæ¡†æ¶æ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæ˜¯ä¸€ä¸ªä¸“ä¸ºé«˜é£é™©åŒ»ç–—å¯¹è¯è®¾è®¡çš„å¼€æ”¾å¼è¯„åˆ†åŸºå‡†å¢é‡è®­ç»ƒæ¡†æ¶ã€‚</li>
<li>ORBITç»“åˆäº†åˆæˆå¯¹è¯ç”Ÿæˆå’ŒåŠ¨æ€è¯„åˆ†åŸºå‡†åˆ›å»ºï¼Œåˆ©ç”¨è¿™äº›è¯„åˆ†åŸºå‡†å¼•å¯¼å¢é‡å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä¾èµ–å¤–éƒ¨é¢†åŸŸçŸ¥è¯†æˆ–æ‰‹åŠ¨è§„åˆ™ï¼Œè€Œæ˜¯ä½¿ç”¨è¯„åˆ†é©±åŠ¨çš„åé¦ˆå¡‘é€ å­¦ä¹ ã€‚</li>
<li>åœ¨Qwen3-4B-Instructæ¨¡å‹ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒORBITæ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œè¾¾åˆ°äº†è¯¥è§„æ¨¡æ¨¡å‹çš„æœ€ä½³ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15859">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0b28e253d9ec6683b377b3fffaaa2ff" align="middle">
<img src="https://picx.zhimg.com/v2-17b3d1bbf5ea4d57cf8c0e7695ecf4b1" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Paper2Web-Letâ€™s-Make-Your-Paper-Alive"><a href="#Paper2Web-Letâ€™s-Make-Your-Paper-Alive" class="headerlink" title="Paper2Web: Letâ€™s Make Your Paper Alive!"></a>Paper2Web: Letâ€™s Make Your Paper Alive!</h2><p><strong>Authors:Yuhang Chen, Tianpeng Lv, Siyi Zhang, Yixiang Yin, Yao Wan, Philip S. Yu, Dongping Chen</strong></p>
<p>Academic project websites can more effectively disseminate research when they clearly present core content and enable intuitive navigation and interaction. However, current approaches such as direct Large Language Model (LLM) generation, templates, or direct HTML conversion struggle to produce layout-aware, interactive sites, and a comprehensive evaluation suite for this task has been lacking. In this paper, we introduce Paper2Web, a benchmark dataset and multi-dimensional evaluation framework for assessing academic webpage generation. It incorporates rule-based metrics like Connectivity, Completeness and human-verified LLM-as-a-Judge (covering interactivity, aesthetics, and informativeness), and PaperQuiz, which measures paper-level knowledge retention. We further present PWAgent, an autonomous pipeline that converts scientific papers into interactive and multimedia-rich academic homepages. The agent iteratively refines both content and layout through MCP tools that enhance emphasis, balance, and presentation quality. Our experiments show that PWAgent consistently outperforms end-to-end baselines like template-based webpages and arXiv&#x2F;alphaXiv versions by a large margin while maintaining low cost, achieving the Pareto-front in academic webpage generation. </p>
<blockquote>
<p>å­¦æœ¯é¡¹ç›®ç½‘ç«™åœ¨æ¸…æ™°åœ°å‘ˆç°æ ¸å¿ƒå†…å®¹ã€æä¾›ç›´è§‚å¯¼èˆªå’Œäº¤äº’åŠŸèƒ½æ—¶ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°ä¼ æ’­ç ”ç©¶ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ï¼Œå¦‚ç›´æ¥é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆã€æ¨¡æ¿æˆ–ç›´æ¥HTMLè½¬æ¢ï¼Œå¾ˆéš¾äº§ç”Ÿå¸ƒå±€æ„ŸçŸ¥ã€äº¤äº’å¼çš„ç½‘ç«™ï¼Œå¹¶ä¸”ç¼ºä¹é’ˆå¯¹æ­¤ä»»åŠ¡çš„å…¨é¢è¯„ä¼°å¥—ä»¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Paper2Webï¼Œè¿™æ˜¯ä¸€å¥—ç”¨äºè¯„ä¼°å­¦æœ¯ç½‘é¡µç”Ÿæˆçš„åŸºå‡†æ•°æ®é›†å’Œå¤šç»´åº¦è¯„ä¼°æ¡†æ¶ã€‚å®ƒç»“åˆäº†åŸºäºè§„åˆ™åº¦çš„æŒ‡æ ‡ï¼Œå¦‚è¿é€šæ€§ã€å®Œæ•´æ€§å’Œç»è¿‡äººå·¥éªŒè¯çš„LLMæ³•å®˜ï¼ˆæ¶µç›–äº¤äº’æ€§ã€ç¾è§‚æ€§å’Œä¿¡æ¯æ€§ï¼‰ï¼Œä»¥åŠPaperQuizï¼Œç”¨äºè¡¡é‡è®ºæ–‡çº§åˆ«çš„çŸ¥è¯†ä¿ç•™æƒ…å†µã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†PWAgentï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½å°†ç§‘å­¦è®ºæ–‡è½¬åŒ–ä¸ºäº¤äº’æ€§å¼ºã€å¤šåª’ä½“ä¸°å¯Œçš„å­¦æœ¯ä¸»é¡µçš„è‡ªä¸»ç®¡é“ã€‚è¯¥ä»£ç†é€šè¿‡å¢å¼ºé‡ç‚¹ã€å¹³è¡¡å’Œå‘ˆç°è´¨é‡çš„MCPå·¥å…·ï¼Œå¯¹å†…å®¹å’Œå¸ƒå±€è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒPWAgentåœ¨ä¿æŒä½æˆæœ¬çš„åŒæ—¶ï¼Œåœ¨å­¦æœ¯ç½‘é¡µç”Ÿæˆæ–¹é¢å§‹ç»ˆä¼˜äºåŸºäºæ¨¡æ¿çš„ç½‘é¡µå’ŒarXiv&#x2F;alphaXivç‰ˆæœ¬ç­‰ç«¯åˆ°ç«¯çš„åŸºçº¿æ–¹æ¡ˆï¼Œå¹¶åœ¨å­¦æœ¯ç½‘é¡µç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†å¸•ç´¯æ‰˜å‰æ²¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15842v1">PDF</a> Under Review. Check <a target="_blank" rel="noopener" href="https://github.com/YuhangChen1/Paper2All">https://github.com/YuhangChen1/Paper2All</a> for the   unified platform to streamline all academic presentation</p>
<p><strong>æ‘˜è¦</strong><br>    å­¦æœ¯è®ºæ–‡ç½‘ç«™é€šè¿‡æ¸…æ™°å‘ˆç°æ ¸å¿ƒå†…å®¹ã€å®ç°ç›´è§‚å¯¼èˆªå’Œäº¤äº’ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°ä¼ æ’­ç ”ç©¶ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•å¦‚ç›´æ¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆã€æ¨¡æ¿æˆ–ç›´æ¥HTMLè½¬æ¢ï¼Œéš¾ä»¥ç”Ÿæˆå¸ƒå±€æ„è¯†ã€äº’åŠ¨çš„ç½‘ç«™ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹æ­¤ä»»åŠ¡çš„å…¨é¢è¯„ä¼°å¥—ä»¶ã€‚æœ¬æ–‡ä»‹ç»äº†Paper2Webï¼Œè¿™æ˜¯ç”¨äºè¯„ä¼°å­¦æœ¯ç½‘é¡µç”Ÿæˆæ•ˆæœçš„åŸºå‡†æ•°æ®é›†å’Œå¤šç»´åº¦è¯„ä¼°æ¡†æ¶ã€‚å®ƒåŒ…å«äº†åŸºäºè§„åˆ™çš„è¯„ä»·æŒ‡æ ‡ï¼Œå¦‚è¿é€šæ€§ã€å®Œæ•´æ€§å’Œç»äººå·¥éªŒè¯çš„LLMè¯„åˆ¤ï¼ˆæ¶µç›–äº¤äº’æ€§ã€ç¾è§‚æ€§å’Œä¿¡æ¯ä¸°å¯Œæ€§ï¼‰ï¼Œä»¥åŠè¡¡é‡è®ºæ–‡å±‚é¢çŸ¥è¯†ä¿ç•™æƒ…å†µçš„PaperQuizã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†PWAgentï¼Œä¸€ä¸ªèƒ½å°†ç§‘å­¦è®ºæ–‡è½¬åŒ–ä¸ºäº’åŠ¨æ€§å’Œå¤šåª’ä½“ä¸°å¯Œçš„å­¦æœ¯ä¸»é¡µçš„è‡ªä¸»ç®¡é“ã€‚è¯¥ä»£ç†é€šè¿‡å¢å¼ºé‡ç‚¹ã€å¹³è¡¡å’Œå‘ˆç°è´¨é‡çš„æ¨¡å‹é¢„æµ‹æ§åˆ¶å·¥å…·è¿›è¡Œè¿­ä»£ä¼˜åŒ–å†…å®¹å’Œå¸ƒå±€ã€‚å®éªŒè¡¨æ˜ï¼ŒPWAgentåœ¨å­¦æœ¯ç½‘é¡µç”Ÿæˆæ–¹é¢å¤§å¹…è¶…è¶Šäº†åŸºäºæ¨¡æ¿çš„ç½‘é¡µå’ŒarXiv&#x2F;alphaXivç‰ˆæœ¬ç­‰ç«¯åˆ°ç«¯åŸºçº¿ï¼ŒåŒæ—¶ä¿æŒä½æˆæœ¬ï¼Œè¾¾åˆ°äº†å­¦æœ¯ç½‘é¡µç”Ÿæˆçš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚</p>
<p><strong>è¦ç‚¹å½’çº³</strong></p>
<ol>
<li>å­¦æœ¯è®ºæ–‡ç½‘ç«™åœ¨æ¸…æ™°å‘ˆç°æ ¸å¿ƒå†…å®¹å’Œå®ç°ç›´è§‚å¯¼èˆªå’Œäº¤äº’æ–¹é¢èƒ½æ›´æœ‰æ•ˆåœ°ä¼ æ’­ç ”ç©¶ã€‚</li>
<li>å½“å‰çš„æ–¹æ³•åœ¨ç”Ÿæˆå¸ƒå±€æ„è¯†ã€äº’åŠ¨æ€§çš„å­¦æœ¯ç½‘ç«™æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>Paper2WebåŸºå‡†æ•°æ®é›†å’Œå¤šç»´åº¦è¯„ä¼°æ¡†æ¶ç”¨äºè¯„ä¼°å­¦æœ¯ç½‘é¡µç”Ÿæˆæ•ˆæœï¼ŒåŒ…å«è¿é€šæ€§ã€å®Œæ•´æ€§ç­‰åŸºäºè§„åˆ™çš„è¯„ä»·æŒ‡æ ‡å’ŒLLMè¯„åˆ¤ã€‚</li>
<li>PaperQuizç”¨äºè¡¡é‡è®ºæ–‡å±‚é¢çŸ¥è¯†çš„ä¿ç•™æƒ…å†µã€‚</li>
<li>PWAgentæ˜¯ä¸€ä¸ªèƒ½å°†ç§‘å­¦è®ºæ–‡è½¬åŒ–ä¸ºå­¦æœ¯ä¸»é¡µçš„è‡ªä¸»ç®¡é“ï¼Œé€šè¿‡æ¨¡å‹é¢„æµ‹æ§åˆ¶å·¥å…·ä¼˜åŒ–å†…å®¹å’Œå¸ƒå±€ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒPWAgentåœ¨å­¦æœ¯ç½‘é¡µç”Ÿæˆæ–¹é¢å¤§å¹…è¶…è¶Šå…¶ä»–æ–¹æ³•ï¼Œè¾¾åˆ°å¸•ç´¯æ‰˜å‰æ²¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15842">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c7c2484962a26fbb7642c28ad9c1d902" align="middle">
<img src="https://picx.zhimg.com/v2-ad5aac27d9f10c350b65d98e42583def" align="middle">
<img src="https://picx.zhimg.com/v2-0cd82178f3ac578e93244d0bf64dee98" align="middle">
<img src="https://picx.zhimg.com/v2-accc24fd12205151b5c093bcfc2619b4" align="middle">
<img src="https://picx.zhimg.com/v2-24ab8ead4e95b551f0f4e4291c44cf84" align="middle">
<img src="https://picx.zhimg.com/v2-e51d4a719dcfa9ffb29f9ba6d871a5f8" align="middle">
<img src="https://picx.zhimg.com/v2-e0e4cb57b3b7e9dec6fa82ff0e48a84e" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Self-evolving-expertise-in-complex-non-verifiable-subject-domains-dialogue-as-implicit-meta-RL"><a href="#Self-evolving-expertise-in-complex-non-verifiable-subject-domains-dialogue-as-implicit-meta-RL" class="headerlink" title="Self-evolving expertise in complex non-verifiable subject domains:   dialogue as implicit meta-RL"></a>Self-evolving expertise in complex non-verifiable subject domains:   dialogue as implicit meta-RL</h2><p><strong>Authors:Richard M. Bailey</strong></p>
<p>So-called <code>wicked problems&#39;, those involving complex multi-dimensional settings, non-verifiable outcomes, heterogeneous impacts and a lack of single objectively correct answers, have plagued humans throughout history. Modern examples include decisions over justice frameworks, solving environmental pollution, planning for pandemic resilience and food security. The use of state-of-the-art artificial intelligence systems (notably Large Language Model-based agents) collaborating with humans on solving such problems is being actively explored. While the abilities of LLMs can be improved by, for example, fine-tuning, hand-crafted system prompts and scaffolding with external tools, LLMs lack endogenous mechanisms to develop expertise through experience in such settings. This work address this gap with Dialectica, a framework where agents engage in structured dialogue on defined topics, augmented by memory, self-reflection, and policy-constrained context editing. Formally, discussion is viewed as an implicit meta-reinforcement learning process. The </code>dialogue-trainedâ€™ agents are evaluated post-hoc using judged pairwise comparisons of elicited responses. Across two model architectures (locally run Qwen3:30b and OpenAIâ€™s o4-mini) results show that enabling reflection-based context editing during discussion produces agents which dominate their baseline counterparts on Elo scores, normalized Bradley-Terry-Davidson ability, and AlphaRank mass. The predicted signatures of learning are observed qualitatively in statement and reflection logs, where reflections identify weaknesses and reliably shape subsequent statements. Agreement between quantitative and qualitative evidence supports dialogue-driven context evolution as a practical path to targeted expertise amplification in open non-verifiable domains. </p>
<blockquote>
<p>æ‰€è°“çš„â€œæ£˜æ‰‹é—®é¢˜â€ï¼Œé‚£äº›æ¶‰åŠå¤æ‚å¤šç»´ç¯å¢ƒã€æ— æ³•éªŒè¯çš„ç»“æœã€ä¸åŒçš„å½±å“ä»¥åŠæ²¡æœ‰å•ä¸€å®¢è§‚æ­£ç¡®ç­”æ¡ˆçš„é—®é¢˜ï¼Œå†å²ä¸Šä¸€ç›´å›°æ‰°ç€äººç±»ã€‚ç°ä»£ä¾‹å­åŒ…æ‹¬æ­£ä¹‰æ¡†æ¶çš„å†³ç­–ã€è§£å†³ç¯å¢ƒæ±¡æŸ“é—®é¢˜ã€è§„åˆ’ç–«æƒ…åº”å¯¹èƒ½åŠ›å’Œç²®é£Ÿå®‰å…¨ç­‰ã€‚ç›®å‰æ­£ç§¯ææ¢ç´¢ä½¿ç”¨æœ€æ–°çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼ˆç‰¹åˆ«æ˜¯åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“ï¼‰ä¸äººç±»åˆä½œè§£å†³è¿™äº›é—®é¢˜ã€‚è™½ç„¶å¯ä»¥é€šè¿‡å¾®è°ƒã€æ‰‹å·¥å®šåˆ¶çš„ç³»ç»Ÿæç¤ºå’Œå¤–éƒ¨å·¥å…·è„šæ‰‹æ¶ç­‰æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¿™ç§ç¯å¢ƒä¸­ç¼ºä¹é€šè¿‡ç»éªŒå‘å±•ä¸“ä¸šçš„å†…åœ¨æœºåˆ¶ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡Dialecticaæ¡†æ¶æ¥è§£å†³è¿™ä¸€ç©ºç™½ï¼Œåœ¨è¯¥æ¡†æ¶ä¸­ï¼Œæ™ºèƒ½ä½“å‚ä¸å…³äºå®šä¹‰ä¸»é¢˜çš„ç»“æ„åŒ–å¯¹è¯ï¼Œè¾…ä»¥è®°å¿†ã€è‡ªæˆ‘åæ€å’Œæ”¿ç­–çº¦æŸçš„ä¸Šä¸‹æ–‡ç¼–è¾‘ã€‚æ­£å¼åœ°ï¼Œè®¨è®ºè¢«è§†ä¸ºä¸€ç§éšå¼çš„å…ƒå¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ã€‚è¿™äº›å¯¹è¯è®­ç»ƒçš„æ™ºèƒ½ä½“æ˜¯é€šè¿‡åˆ©ç”¨å¾—åˆ°çš„å“åº”è¿›è¡Œäº‹åè¯„ä»·è¿›è¡Œçš„åˆ¤æ–­é…å¯¹æ¯”è¾ƒè¿›è¡Œè¯„ä¼°çš„ã€‚åœ¨ä¸¤ç§æ¨¡å‹æ¶æ„ä¸­ï¼ˆæœ¬åœ°è¿è¡Œçš„Qwen3ï¼š30bå’ŒOpenAIçš„o4-miniï¼‰ï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨è®¨è®ºè¿‡ç¨‹ä¸­å¯ç”¨åŸºäºåæ€çš„ä¸Šä¸‹æ–‡ç¼–è¾‘ï¼Œå¯ä»¥äº§ç”Ÿåœ¨åŸƒæ´›å¾—åˆ†ã€æ ‡å‡†åŒ–çš„Bradley-Terry-Davidsonèƒ½åŠ›å’ŒAlphaRankè´¨é‡æ–¹é¢ä¼˜äºåŸºå‡†çº¿çš„æ™ºèƒ½ä½“ã€‚å­¦ä¹ çš„é¢„æµ‹ç­¾ååœ¨é™ˆè¿°å’Œåæ€æ—¥å¿—ä¸­éƒ½å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œåæ€å¯ä»¥è¯†åˆ«å¼±ç‚¹å¹¶å¯é åœ°å½±å“éšåçš„é™ˆè¿°ã€‚å®šé‡å’Œå®šæ€§è¯æ®ä¹‹é—´çš„åè®®æ”¯æŒå¯¹è¯é©±åŠ¨ä¸Šä¸‹æ–‡æ¼”åŒ–ä½œä¸ºåœ¨ééªŒè¯é¢†åŸŸå®ç°ç›®æ ‡ä¸“ä¸šçŸ¥è¯†æ”¾å¤§çš„å®ç”¨é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15772v1">PDF</a> 50 pages, 4 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰€è°“çš„â€œæ£˜æ‰‹çš„éš¾é¢˜â€ï¼Œè¿™ç±»é—®é¢˜æ¶‰åŠå¤æ‚çš„å¤šç»´è®¾ç½®ã€ä¸å¯éªŒè¯çš„ç»“æœã€å¼‚è´¨æ€§çš„å½±å“ä»¥åŠæ²¡æœ‰å•ä¸€çš„å®¢è§‚æ­£ç¡®ç­”æ¡ˆã€‚ç°ä»£ä¾‹å­åŒ…æ‹¬æ­£ä¹‰æ¡†æ¶çš„å†³ç­–ã€è§£å†³ç¯å¢ƒæ±¡æŸ“é—®é¢˜ã€è§„åˆ’ç–«æƒ…éŸ§æ€§å’Œç²®é£Ÿå®‰å…¨ç­‰ã€‚æ–‡ç« ä»‹ç»äº†ä½¿ç”¨æœ€æ–°çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼ˆç‰¹åˆ«æ˜¯åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†äººï¼‰æ¥è§£å†³è¿™äº›é—®é¢˜çš„å°è¯•ã€‚è™½ç„¶å¯ä»¥é€šè¿‡å¾®è°ƒã€æ‰‹å·¥ç³»ç»Ÿæç¤ºå’Œä¸å¤–éƒ¨å·¥å…·ç»“åˆä½¿ç”¨è„šæ‰‹æ¶ç­‰æ–¹æ³•æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¿™äº›ç¯å¢ƒä¸­ç¼ºä¹é€šè¿‡ç»éªŒå‘å±•ä¸“ä¸šçŸ¥è¯†çš„èƒ½åŠ›ã€‚æœ¬æ–‡å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œæå‡ºäº†Dialecticaæ¡†æ¶ï¼Œè¯¥æ¡†æ¶è®©ä»£ç†äººå‚ä¸ç»“æ„åŒ–çš„å¯¹è¯ï¼Œæ”¯æŒè®°å¿†ã€è‡ªæˆ‘åæ€å’Œç­–ç•¥å—é™çš„ä¸Šä¸‹æ–‡ç¼–è¾‘ã€‚æ­£å¼åœ°ï¼Œè®¨è®ºè¢«è§†ä¸ºä¸€ç§éšæ€§çš„å…ƒå¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ã€‚ç»è¿‡åè¯„ä»·ï¼Œâ€œå¯¹è¯è®­ç»ƒâ€çš„ä»£ç†é€šè¿‡ä½¿ç”¨æ‰€è¯±å‘å“åº”çš„ç»è¯„åˆ¤çš„é…å¯¹æ¯”è¾ƒè¿›è¡Œè¯„ä¼°ã€‚åœ¨ä¸¤ç§æ¨¡å‹æ¶æ„ï¼ˆæœ¬åœ°è¿è¡Œçš„Qwen3ï¼š30bå’ŒOpenAIçš„o4-miniï¼‰çš„ç»“æœæ˜¾ç¤ºï¼Œåœ¨è®¨è®ºè¿‡ç¨‹ä¸­å¯ç”¨åŸºäºåæ€çš„ä¸Šä¸‹æ–‡ç¼–è¾‘äº§ç”Ÿçš„ä»£ç†åœ¨Eloå¾—åˆ†ã€æ ‡å‡†åŒ–çš„Bradley-Terry-Davidsonèƒ½åŠ›å’ŒAlphaRankè´¨é‡æ–¹é¢ä¼˜äºåŸºçº¿ä»£ç†ã€‚é¢„æµ‹çš„å­¦ä¹ ç­¾ååœ¨é™ˆè¿°å’Œåæ€æ—¥å¿—ä¸­å¯è§‚å¯Ÿåˆ°ï¼Œåæ€èƒ½è¯†åˆ«å‡ºå¼±ç‚¹å¹¶å¯é åœ°å½±å“éšåçš„é™ˆè¿°ã€‚å®šé‡å’Œå®šæ€§è¯æ®ä¹‹é—´çš„åè®®æ”¯æŒå¯¹è¯é©±åŠ¨çš„ä¸Šä¸‹æ–‡è¿›åŒ–æ˜¯åœ¨å¼€æ”¾çš„ééªŒè¯åŸŸä¸­å®ç°æœ‰é’ˆå¯¹æ€§çš„ä¸“ä¸šæ‰©å¤§çš„ä¸€ç§å®ç”¨é€”å¾„ã€‚</p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å†å²å’Œç°ä»£éƒ½æœ‰åº”ç”¨äºè§£å†³æ£˜æ‰‹çš„éš¾é¢˜ï¼Œå¦‚å†³ç­–æ­£ä¹‰æ¡†æ¶ã€è§£å†³ç¯å¢ƒæ±¡æŸ“ç­‰ã€‚</li>
<li>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹èƒ½åŠ›å¼ºå¤§ï¼Œä½†å®ƒä»¬ç¼ºä¹åœ¨å¤æ‚ç¯å¢ƒä¸­é€šè¿‡ç»éªŒå‘å±•ä¸“ä¸šçŸ¥è¯†çš„èƒ½åŠ›ã€‚</li>
<li>Dialecticaæ¡†æ¶å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œé€šè¿‡ç»“æ„åŒ–å¯¹è¯ã€è®°å¿†æ”¯æŒã€è‡ªæˆ‘åæ€å’Œç­–ç•¥å—é™çš„ä¸Šä¸‹æ–‡ç¼–è¾‘æ¥è®­ç»ƒä»£ç†ã€‚</li>
<li>å½¢å¼ä¸Šï¼Œè®¨è®ºè¢«è§†ä¸ºä¸€ç§éšæ€§çš„å…ƒå¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ã€‚ä»£ç†é€šè¿‡åå°„å¼ä¸Šä¸‹æ–‡ç¼–è¾‘åœ¨è®¨è®ºä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>åœ¨ä¸¤ç§æ¨¡å‹æ¶æ„ä¸‹è¿›è¡Œçš„è¯„ä¼°æ˜¾ç¤ºï¼Œâ€œå¯¹è¯è®­ç»ƒâ€çš„ä»£ç†åœ¨è®¨è®ºè¿‡ç¨‹ä¸­å±•ç°å‡ºäº†å¼ºå¤§çš„èƒ½åŠ›ï¼Œè¶…è¶Šäº†åŸºçº¿ä»£ç†çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15772">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8a4cd80bdf0dff72797149027794d0c8" align="middle">
<img src="https://picx.zhimg.com/v2-90e9d8c357c40e78cb59ea7b98e64520" align="middle">
<img src="https://picx.zhimg.com/v2-35a8d367d8b8f0f4a4b652375ae72acf" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="LLMs-Judge-Themselves-A-Game-Theoretic-Framework-for-Human-Aligned-Evaluation"><a href="#LLMs-Judge-Themselves-A-Game-Theoretic-Framework-for-Human-Aligned-Evaluation" class="headerlink" title="LLMs Judge Themselves: A Game-Theoretic Framework for Human-Aligned   Evaluation"></a>LLMs Judge Themselves: A Game-Theoretic Framework for Human-Aligned   Evaluation</h2><p><strong>Authors:Gao Yang, Yuhang Liu, Siyu Miao, Xinyue Liang, Zhengyang Liu, Heyan Huang</strong></p>
<p>Ideal or real - that is the question.In this work, we explore whether principles from game theory can be effectively applied to the evaluation of large language models (LLMs). This inquiry is motivated by the growing inadequacy of conventional evaluation practices, which often rely on fixed-format tasks with reference answers and struggle to capture the nuanced, subjective, and open-ended nature of modern LLM behavior. To address these challenges, we propose a novel alternative: automatic mutual evaluation, where LLMs assess each otherâ€™s output through self-play and peer review. These peer assessments are then systematically compared with human voting behavior to evaluate their alignment with human judgment. Our framework incorporates game-theoretic voting algorithms to aggregate peer reviews, enabling a principled investigation into whether model-generated rankings reflect human preferences. Empirical results reveal both convergences and divergences between theoretical predictions and human evaluations, offering valuable insights into the promises and limitations of mutual evaluation. To the best of our knowledge, this is the first work to jointly integrate mutual evaluation, game-theoretic aggregation, and human-grounded validation for evaluating the capabilities of LLMs. </p>
<blockquote>
<p>ç†æƒ³è¿˜æ˜¯ç°å®â€”â€”è¿™æ˜¯é—®é¢˜æ‰€åœ¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢è®¨åšå¼ˆç†è®ºçš„åŸåˆ™æ˜¯å¦èƒ½å¤Ÿæœ‰æ•ˆåœ°åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯„ä¼°ã€‚è¿™ä¸€æ¢ç©¶æ˜¯ç”±ä¼ ç»Ÿè¯„ä¼°å®è·µçš„æ—¥ç›Šä¸è¶³æ‰€é©±åŠ¨çš„ï¼Œä¼ ç»Ÿè¯„ä¼°å®è·µç»å¸¸ä¾èµ–äºå…·æœ‰å‚è€ƒç­”æ¡ˆçš„å›ºå®šæ ¼å¼ä»»åŠ¡ï¼Œå¹¶ä¸”éš¾ä»¥æ•æ‰ç°ä»£LLMè¡Œä¸ºçš„å¾®å¦™ã€ä¸»è§‚å’Œå¼€æ”¾æ€§è´¨ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ›¿ä»£æ–¹æ¡ˆï¼šè‡ªåŠ¨ç›¸äº’è¯„ä¼°ï¼Œå³LLMé€šè¿‡è‡ªæˆ‘åšå¼ˆå’ŒåŒè¡Œè¯„å®¡ç›¸äº’è¯„ä¼°å½¼æ­¤çš„è¾“å²€ã€‚ç„¶åå°†è¿™äº›åŒè¡Œè¯„ä¼°ä¸äººç±»çš„æŠ•ç¥¨è¡Œä¸ºè¿›è¡Œç³»ç»Ÿçš„æ¯”è¾ƒï¼Œä»¥è¯„ä¼°å®ƒä»¬ä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç»“åˆäº†åšå¼ˆè®ºæŠ•ç¥¨ç®—æ³•æ¥æ±‡æ€»åŒè¡Œè¯„å®¡æ„è§ï¼Œä»è€Œæœ‰åŸåˆ™åœ°ç ”ç©¶æ¨¡å‹ç”Ÿæˆçš„æ’åæ˜¯å¦åæ˜ äººç±»åå¥½ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œç†è®ºé¢„æµ‹å’Œäººç±»è¯„ä¼°ä¹‹é—´æ—¢æœ‰æ”¶æ•›ä¹Ÿæœ‰åˆ†æ­§ï¼Œä¸ºæˆ‘ä»¬æ·±å…¥äº†è§£ç›¸äº’è¯„ä¼°çš„æ‰¿è¯ºå’Œå±€é™æ€§æä¾›äº†å®è´µçš„è§è§£ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹è”åˆé‡‡ç”¨ç›¸äº’è¯„ä¼°ã€åšå¼ˆè®ºèšåˆå’Œäººç±»åŸºç¡€éªŒè¯æ¥è¯„ä¼°LLMèƒ½åŠ›çš„å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15746v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†åšå¼ˆç†è®ºåŸåˆ™åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„ä¼°ä¸­çš„æœ‰æ•ˆåº”ç”¨ã€‚é’ˆå¯¹ä¼ ç»Ÿè¯„ä¼°æ–¹æ³•çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è‡ªåŠ¨ç›¸äº’è¯„ä¼°æ–¹æ³•ï¼Œé€šè¿‡LLMçš„è‡ªæˆ‘åšå¼ˆå’ŒåŒè¡Œè¯„å®¡æ¥è¯„ä¼°å½¼æ­¤çš„è¾“å‡ºæ¥å®ç°ã€‚è¯¥æ–¹æ³•é€šè¿‡ç³»ç»Ÿåœ°ä¸äººç±»æŠ•ç¥¨è¡Œä¸ºè¿›è¡Œæ¯”è¾ƒï¼Œè¯„ä¼°å…¶ä¸äººç±»åˆ¤æ–­çš„å¯¹é½ç¨‹åº¦ã€‚æœ¬æ–‡çš„æ¡†æ¶ç»“åˆäº†åšå¼ˆç†è®ºæŠ•ç¥¨ç®—æ³•æ¥æ±‡æ€»åŒè¡Œè¯„å®¡æ„è§ï¼Œä»¥ç ”ç©¶æ¨¡å‹ç”Ÿæˆçš„æ’åæ˜¯å¦åæ˜ äº†äººç±»åå¥½ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œç†è®ºé¢„æµ‹ä¸äººç±»è¯„ä¼°ä¹‹é—´å­˜åœ¨ä¸€è‡´æ€§å’Œåˆ†æ­§ï¼Œä¸ºç›¸äº’è¯„ä¼°çš„æ‰¿è¯ºå’Œå±€é™æ€§æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¢è®¨äº†åšå¼ˆç†è®ºåœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„ä¼°ä¸­çš„åº”ç”¨ä»·å€¼ã€‚</li>
<li>æå‡ºäº†è‡ªåŠ¨ç›¸äº’è¯„ä¼°æ–¹æ³•ï¼Œé€šè¿‡LLMçš„è‡ªæˆ‘åšå¼ˆå’ŒåŒè¡Œè¯„å®¡æ¥è¯„ä¼°è¾“å‡ºã€‚</li>
<li>æ¡†æ¶ç»“åˆäº†åšå¼ˆç†è®ºæŠ•ç¥¨ç®—æ³•æ¥æ±‡æ€»åŒè¡Œè¯„å®¡æ„è§ã€‚</li>
<li>ç³»ç»Ÿåœ°ä¸äººç±»æŠ•ç¥¨è¡Œä¸ºè¿›è¡Œæ¯”è¾ƒï¼Œä»¥è¯„ä¼°å…¶ä¸äººç±»åˆ¤æ–­çš„å¯¹é½ç¨‹åº¦ã€‚</li>
<li>å®è¯ç»“æœè¡¨æ˜ç†è®ºé¢„æµ‹ä¸äººç±»è¯„ä¼°ä¹‹é—´å­˜åœ¨ä¸€è‡´æ€§å’Œåˆ†æ­§ã€‚</li>
<li>æ­¤æ–¹æ³•æä¾›äº†å¯¹ç›¸äº’è¯„ä¼°çš„æ‰¿è¯ºå’Œå±€é™æ€§çš„æœ‰ä»·å€¼è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15746">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9242f0e7a96d0dd2e167fea7e7084ffa" align="middle">
<img src="https://picx.zhimg.com/v2-9d4b2f02a96fa49a929a2e88f07bb323" align="middle">
<img src="https://picx.zhimg.com/v2-1c95b62ca3d68214c2898a0e5ac9f783" align="middle">
<img src="https://picx.zhimg.com/v2-886938b4e42c0643b36c6cdf30bde1be" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FACE-A-General-Framework-for-Mapping-Collaborative-Filtering-Embeddings-into-LLM-Tokens"><a href="#FACE-A-General-Framework-for-Mapping-Collaborative-Filtering-Embeddings-into-LLM-Tokens" class="headerlink" title="FACE: A General Framework for Mapping Collaborative Filtering Embeddings   into LLM Tokens"></a>FACE: A General Framework for Mapping Collaborative Filtering Embeddings   into LLM Tokens</h2><p><strong>Authors:Chao Wang, Yixin Song, Jinhui Ye, Chuan Qin, Dazhong Shen, Lingfeng Liu, Xiang Wang, Yanyong Zhang</strong></p>
<p>Recently, large language models (LLMs) have been explored for integration with collaborative filtering (CF)-based recommendation systems, which are crucial for personalizing user experiences. However, a key challenge is that LLMs struggle to interpret the latent, non-semantic embeddings produced by CF approaches, limiting recommendation effectiveness and further applications. To address this, we propose FACE, a general interpretable framework that maps CF embeddings into pre-trained LLM tokens. Specifically, we introduce a disentangled projection module to decompose CF embeddings into concept-specific vectors, followed by a quantized autoencoder to convert continuous embeddings into LLM tokens (descriptors). Then, we design a contrastive alignment objective to ensure that the tokens align with corresponding textual signals. Hence, the model-agnostic FACE framework achieves semantic alignment without fine-tuning LLMs and enhances recommendation performance by leveraging their pre-trained capabilities. Empirical results on three real-world recommendation datasets demonstrate performance improvements in benchmark models, with interpretability studies confirming the interpretability of the descriptors. Code is available in <a target="_blank" rel="noopener" href="https://github.com/YixinRoll/FACE">https://github.com/YixinRoll/FACE</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œäººä»¬å¼€å§‹æ¢ç´¢å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸åŸºäºååŒè¿‡æ»¤ï¼ˆCFï¼‰çš„æ¨èç³»ç»Ÿç›¸ç»“åˆï¼Œè¿™å¯¹äºä¸ªæ€§åŒ–ç”¨æˆ·ä½“éªŒè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¸€ä¸ªå…³é”®æŒ‘æˆ˜åœ¨äºLLMå¾ˆéš¾è§£é‡ŠCFæ–¹æ³•äº§ç”Ÿçš„æ½œåœ¨éè¯­ä¹‰åµŒå…¥ï¼Œè¿™é™åˆ¶äº†æ¨èæ•ˆæœå’Œå…¶ä»–åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FACEï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„å¯è§£é‡Šæ¡†æ¶ï¼Œå¯å°†CFåµŒå…¥æ˜ å°„åˆ°é¢„è®­ç»ƒçš„LLMä»¤ç‰Œä¸Šã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåˆ†ç¦»æŠ•å½±æ¨¡å—ï¼Œå°†CFåµŒå…¥åˆ†è§£æˆæ¦‚å¿µç‰¹å®šçš„å‘é‡ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªé‡åŒ–è‡ªç¼–ç å™¨å°†è¿ç»­çš„åµŒå…¥è½¬æ¢ä¸ºLLMä»¤ç‰Œï¼ˆæè¿°ç¬¦ï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¯¹æ¯”å¯¹é½ç›®æ ‡ï¼Œä»¥ç¡®ä¿ä»¤ç‰Œä¸ç›¸åº”çš„æ–‡æœ¬ä¿¡å·å¯¹é½ã€‚å› æ­¤ï¼Œæ¨¡å‹é€šç”¨çš„FACEæ¡†æ¶å®ç°äº†è¯­ä¹‰å¯¹é½ï¼Œè€Œæ— éœ€å¾®è°ƒLLMï¼Œå¹¶åˆ©ç”¨å…¶é¢„è®­ç»ƒèƒ½åŠ›æé«˜äº†æ¨èæ€§èƒ½ã€‚åœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œçš„æ¨èæ•°æ®é›†ä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼Œåœ¨åŸºå‡†æ¨¡å‹ä¸­çš„æ€§èƒ½æœ‰æ‰€æé«˜ï¼Œè§£é‡Šæ€§ç ”ç©¶è¯å®äº†æè¿°ç¬¦çš„å¯è§£é‡Šæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/YixinRoll/FACE%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/YixinRoll/FACEä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15729v1">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºååŒè¿‡æ»¤ï¼ˆCFï¼‰çš„æ¨èç³»ç»Ÿèå…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯æå‡ç”¨æˆ·ä½“éªŒä¸ªæ€§åŒ–çš„å…³é”®ã€‚ç„¶è€Œï¼ŒLLMéš¾ä»¥è§£è¯»CFäº§ç”Ÿçš„éè¯­ä¹‰æ½œåœ¨åµŒå…¥ï¼Œé™åˆ¶äº†æ¨èæ•ˆæœåŠè¿›ä¸€æ­¥åº”ç”¨ã€‚ä¸ºåº”å¯¹æ­¤æŒ‘æˆ˜ï¼Œæå‡ºFACEè¿™ä¸€é€šç”¨å¯è§£é‡Šæ¡†æ¶ï¼Œå°†CFåµŒå…¥æ˜ å°„åˆ°é¢„è®­ç»ƒLLMä»¤ç‰Œä¸Šã€‚é€šè¿‡åˆ†è§£CFåµŒå…¥ã€è¿ç»­åµŒå…¥è½¬LLMä»¤ç‰Œä»¥åŠç¡®ä¿ä»¤ç‰Œä¸ç›¸åº”æ–‡æœ¬ä¿¡å·å¯¹é½ç­‰æ­¥éª¤ï¼Œå®ç°è¯­ä¹‰å¯¹é½å¹¶æå‡æ¨èæ€§èƒ½ã€‚åœ¨ä¸‰ä¸ªçœŸå®æ¨èæ•°æ®é›†ä¸Šçš„å®è¯ç»“æœè¯æ˜äº†è¯¥æ¡†æ¶çš„æ€§èƒ½æå‡æ•ˆæœï¼Œä¸”æè¿°ç¬¦çš„å¯è§£é‡Šæ€§ç ”ç©¶è¯å®äº†å…¶å¯è§£é‡Šæ€§ã€‚ä»£ç å¯è®¿é—® <a target="_blank" rel="noopener" href="https://github.com/YixinRoll/FACE%E3%80%82">https://github.com/YixinRoll/FACEã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èå…¥ååŒè¿‡æ»¤ï¼ˆCFï¼‰æ¨èç³»ç»Ÿå¯¹äºä¸ªæ€§åŒ–ç”¨æˆ·ä½“éªŒè‡³å…³é‡è¦ã€‚</li>
<li>LLMé¢ä¸´è§£è¯»CFäº§ç”Ÿçš„éè¯­ä¹‰æ½œåœ¨åµŒå…¥çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºFACEæ¡†æ¶ï¼Œå®ç°CFåµŒå…¥åˆ°é¢„è®­ç»ƒLLMä»¤ç‰Œçš„æ˜ å°„ã€‚</li>
<li>é€šè¿‡åˆ†è§£CFåµŒå…¥ã€è½¬æ¢ä¸ºLLMä»¤ç‰Œå’Œç¡®ä¿ä»¤ç‰Œä¸æ–‡æœ¬ä¿¡å·å¯¹é½ï¼Œå®ç°è¯­ä¹‰å¯¹é½ã€‚</li>
<li>æ¨¡å‹æ— éœ€å¾®è°ƒLLMå³å¯å®ç°è¯­ä¹‰å¯¹é½ã€‚</li>
<li>å®è¯ç»“æœæ˜¾ç¤ºFACEæ¡†æ¶åœ¨æ¨èæ€§èƒ½ä¸Šçš„æå‡æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-74d031b21f591fc2200f11cdd798b99c" align="middle">
<img src="https://picx.zhimg.com/v2-a2c2350cf43d25d6418dd2a89e9f5900" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="The-3rd-Place-Solution-of-CCIR-CUP-2025-A-Framework-for-Retrieval-Augmented-Generation-in-Multi-Turn-Legal-Conversation"><a href="#The-3rd-Place-Solution-of-CCIR-CUP-2025-A-Framework-for-Retrieval-Augmented-Generation-in-Multi-Turn-Legal-Conversation" class="headerlink" title="The 3rd Place Solution of CCIR CUP 2025: A Framework for   Retrieval-Augmented Generation in Multi-Turn Legal Conversation"></a>The 3rd Place Solution of CCIR CUP 2025: A Framework for   Retrieval-Augmented Generation in Multi-Turn Legal Conversation</h2><p><strong>Authors:Da Li, Zecheng Fang, Qiang Yan, Wei Huang, Xuanpu Luo</strong></p>
<p>Retrieval-Augmented Generation has made significant progress in the field of natural language processing. By combining the advantages of information retrieval and large language models, RAG can generate relevant and contextually appropriate responses based on items retrieved from reliable sources. This technology has demonstrated outstanding performance across multiple domains, but its application in the legal field remains in its exploratory phase. In this paper, we introduce our approach for â€œLegal Knowledge Retrieval and Generationâ€ in CCIR CUP 2025, which leverages large language models and information retrieval systems to provide responses based on laws in response to user questions. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆåœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚é€šè¿‡ç»“åˆä¿¡æ¯æ£€ç´¢å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¼˜åŠ¿ï¼ŒRAGå¯ä»¥æ ¹æ®ä»å¯é æ¥æºæ£€ç´¢åˆ°çš„é¡¹ç›®ç”Ÿæˆç›¸å…³ä¸”ä¸Šä¸‹æ–‡æ°å½“çš„å›åº”ã€‚è¿™é¡¹æŠ€æœ¯åœ¨å¤šä¸ªé¢†åŸŸéƒ½è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œä½†å…¶åœ¨æ³•å¾‹é¢†åŸŸçš„åº”ç”¨ä»å¤„äºæ¢ç´¢é˜¶æ®µã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†åœ¨CCIR CUP 2025ä¸­â€œæ³•å¾‹çŸ¥è¯†æ£€ç´¢ä¸ç”Ÿæˆâ€çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ£€ç´¢ä¿¡æ¯ç³»ç»Ÿï¼Œæ ¹æ®æ³•å¾‹å¯¹ç”¨æˆ·é—®é¢˜ä½œå‡ºå›åº”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15722v1">PDF</a> CCIR2025</p>
<p><strong>Summary</strong><br>éšç€è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯è¿›æ­¥ï¼ŒåŸºäºæ£€ç´¢å¢å¼ºæŠ€æœ¯çš„ç”Ÿæˆå¼æ–¹æ³•åœ¨å¤šä¸ªé¢†åŸŸå±•ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚æœ¬ç¯‡æ–‡ç« ä»‹ç»äº†åœ¨CCIRæ¯2025ä¸­å¼€å±•çš„â€œæ³•å¾‹çŸ¥è¯†æ£€ç´¢ä¸ç”Ÿæˆâ€ç ”ç©¶ï¼Œè¯¥ç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ£€ç´¢ç³»ç»Ÿï¼Œæ ¹æ®æ³•å¾‹å†…å®¹ç”Ÿæˆå“åº”ç”¨æˆ·æé—®çš„ç­”æ¡ˆã€‚è¿™é¡¹æŠ€æœ¯åœ¨æ³•å¾‹é¢†åŸŸçš„åº”ç”¨ä»å¤„äºæ¢ç´¢é˜¶æ®µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>è¯¥æŠ€æœ¯ç»“åˆäº†ä¿¡æ¯æ£€ç´¢å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¼˜åŠ¿ã€‚</li>
<li>åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œèƒ½å¤Ÿæ ¹æ®ä»å¯é æ¥æºæ£€ç´¢çš„é¡¹ç›®ç”Ÿæˆç›¸å…³ä¸”ç¬¦åˆè¯­å¢ƒçš„å“åº”ã€‚</li>
<li>â€œæ³•å¾‹çŸ¥è¯†æ£€ç´¢ä¸ç”Ÿæˆâ€ç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ£€ç´¢ç³»ç»Ÿå›åº”ç”¨æˆ·æ³•å¾‹é—®é¢˜ã€‚</li>
<li>è¯¥æŠ€æœ¯åœ¨æ³•å¾‹é¢†åŸŸçš„åº”ç”¨ä»å¤„äºæ¢ç´¢é˜¶æ®µã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15722">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3c965ea2810a88ef6d2bd11bd7e26aa4" align="middle">
<img src="https://picx.zhimg.com/v2-bbef15fc2097e0dd01354041a618181f" align="middle">
<img src="https://picx.zhimg.com/v2-9153f32afeb717c30fa9444340b26eea" align="middle">
<img src="https://picx.zhimg.com/v2-0e283500b50b6f83cc9bd2a0e7ddc922" align="middle">
<img src="https://picx.zhimg.com/v2-31c55bbc62d6de65c94ce2da4c11988f" align="middle">
<img src="https://picx.zhimg.com/v2-a7096bce8432abe66c4cd932d781647a" align="middle">
<img src="https://picx.zhimg.com/v2-122f1d6acac068a62aec510c8f3bda9a" align="middle">
<img src="https://picx.zhimg.com/v2-fd3a5819a8f72a2dbe0ace2969479da8" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GraphMind-Interactive-Novelty-Assessment-System-for-Accelerating-Scientific-Discovery"><a href="#GraphMind-Interactive-Novelty-Assessment-System-for-Accelerating-Scientific-Discovery" class="headerlink" title="GraphMind: Interactive Novelty Assessment System for Accelerating   Scientific Discovery"></a>GraphMind: Interactive Novelty Assessment System for Accelerating   Scientific Discovery</h2><p><strong>Authors:Italo Luis da Silva, Hanqi Yan, Lin Gui, Yulan He</strong></p>
<p>Large Language Models (LLMs) show strong reasoning and text generation capabilities, prompting their use in scientific literature analysis, including novelty assessment. While evaluating novelty of scientific papers is crucial for peer review, it requires extensive knowledge of related work, something not all reviewers have. While recent work on LLM-assisted scientific literature analysis supports literature comparison, existing approaches offer limited transparency and lack mechanisms for result traceability via an information retrieval module. To address this gap, we introduce $\textbf{GraphMind}$, an easy-to-use interactive web tool designed to assist users in evaluating the novelty of scientific papers or drafted ideas. Specially, $\textbf{GraphMind}$ enables users to capture the main structure of a scientific paper, explore related ideas through various perspectives, and assess novelty via providing verifiable contextual insights. $\textbf{GraphMind}$ enables users to annotate key elements of a paper, explore related papers through various relationships, and assess novelty with contextual insight. This tool integrates external APIs such as arXiv and Semantic Scholar with LLMs to support annotation, extraction, retrieval and classification of papers. This combination provides users with a rich, structured view of a scientific ideaâ€™s core contributions and its connections to existing work. $\textbf{GraphMind}$ is available at <a target="_blank" rel="noopener" href="https://oyarsa.github.io/graphmind">https://oyarsa.github.io/graphmind</a> and a demonstration video at <a target="_blank" rel="noopener" href="https://youtu.be/wKbjQpSvwJg">https://youtu.be/wKbjQpSvwJg</a>. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/oyarsa/graphmind">https://github.com/oyarsa/graphmind</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†å’Œæ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œè¢«åº”ç”¨äºç§‘å­¦æ–‡çŒ®åˆ†æï¼ŒåŒ…æ‹¬æ–°é¢–æ€§è¯„ä¼°ã€‚è™½ç„¶è¯„ä¼°ç§‘å­¦è®ºæ–‡çš„æ–°é¢–æ€§å¯¹äºåŒè¡Œè¯„å®¡è‡³å…³é‡è¦ï¼Œä½†éœ€è¦å¹¿æ³›çš„ç›¸å…³çŸ¥è¯†ï¼Œå¹¶éæ‰€æœ‰å®¡ç¨¿äººéƒ½å…·å¤‡è¿™ç§çŸ¥è¯†ã€‚å…³äºLLMè¾…åŠ©ç§‘å­¦æ–‡çŒ®åˆ†æçš„æœ€æ–°å·¥ä½œæ”¯æŒæ–‡çŒ®æ¯”è¾ƒï¼Œä½†ç°æœ‰æ–¹æ³•æä¾›çš„é€æ˜åº¦æœ‰é™ï¼Œç¼ºä¹é€šè¿‡ä¿¡æ¯æ£€ç´¢æ¨¡å—è¿›è¡Œç»“æœè¿½æº¯çš„æœºåˆ¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†<strong>GraphMind</strong>ï¼Œè¿™æ˜¯ä¸€æ¬¾æ˜“äºä½¿ç”¨çš„äº¤äº’å¼Webå·¥å…·ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·è¯„ä¼°ç§‘å­¦è®ºæ–‡æˆ–è‰æ¡ˆçš„æ–°é¢–æ€§ã€‚<strong>GraphMind</strong>ä½¿ç”¨æˆ·èƒ½å¤Ÿæ•æ‰ç§‘å­¦è®ºæ–‡çš„ä¸»è¦ç»“æ„ï¼Œé€šè¿‡ä¸åŒçš„è§†è§’æ¢ç´¢ç›¸å…³æ€æƒ³ï¼Œå¹¶é€šè¿‡æä¾›å¯éªŒè¯çš„ä¸Šä¸‹æ–‡æ´å¯Ÿæ¥è¯„ä¼°æ–°é¢–æ€§ã€‚<strong>GraphMind</strong>è¿˜ä½¿ç”¨æˆ·èƒ½å¤Ÿæ³¨é‡Šè®ºæ–‡çš„å…³é”®è¦ç´ ï¼Œé€šè¿‡å„ç§å…³è”æ¢ç´¢ç›¸å…³è®ºæ–‡ï¼Œå¹¶å€ŸåŠ©ä¸Šä¸‹æ–‡æ´å¯Ÿè¯„ä¼°æ–°é¢–æ€§ã€‚æ­¤å·¥å…·æ•´åˆäº†arXivå’ŒSemantic Scholarç­‰å¤–éƒ¨APIä¸LLMï¼Œæ”¯æŒè®ºæ–‡çš„æ³¨é‡Šã€æå–ã€æ£€ç´¢å’Œåˆ†ç±»ã€‚è¿™ç§ç»“åˆä¸ºç”¨æˆ·æä¾›äº†ä¸€ä¸ªä¸°å¯Œçš„ç»“æ„åŒ–è§†å›¾ï¼Œå±•ç¤ºç§‘å­¦æ€æƒ³çš„æ ¸å¿ƒè´¡çŒ®åŠå…¶ä¸ç°æœ‰å·¥ä½œçš„è”ç³»ã€‚<strong>GraphMind</strong>å¯åœ¨<a target="_blank" rel="noopener" href="https://oyarsa.github.io/graphmind%E4%B8%8A%E8%AE%BF%E9%97%AE%EF%BC%8C%E6%BC%94%E7%A4%BA%E8%A7%86%E9%A2%91%E4%B8%BAhttps://youtu.be/wKbjQpSvwJg%E3%80%82%E6%BA%90%E4%BB%A3%E7%A0%81%E5%8F%AF%E5%9C%A8https://github.com/oyarsa/graphmind%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://oyarsa.github.io/graphmindä¸Šè®¿é—®ï¼Œæ¼”ç¤ºè§†é¢‘ä¸ºhttps://youtu.be/wKbjQpSvwJgã€‚æºä»£ç å¯åœ¨https://github.com/oyarsa/graphmindä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15706v1">PDF</a> 9 pages, 6 figures, 3 tables, EMNLP 2025 Demo paper</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡å¼ºå¤§çš„æ¨ç†å’Œæ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œå¯ç”¨äºç§‘å­¦æ–‡çŒ®åˆ†æï¼ŒåŒ…æ‹¬æ–°é¢–æ€§è¯„ä¼°ã€‚é’ˆå¯¹ç§‘å­¦è®ºæ–‡æ–°é¢–æ€§è¯„ä¼°ä¸­çš„çŸ¥è¯†å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸ºGraphMindçš„æ˜“äºä½¿ç”¨çš„äº¤äº’å¼Webå·¥å…·ã€‚GraphMindé€šè¿‡æ•´åˆLLMã€å¤–éƒ¨APIå¦‚arXivå’ŒSemantic Scholarï¼Œæ”¯æŒç”¨æˆ·æ•æ‰è®ºæ–‡ä¸»è¦ç»“æ„ã€æ¢ç´¢ç›¸å…³æ€æƒ³ã€è¯„ä¼°æ–°é¢–æ€§å¹¶æä¾›å¯éªŒè¯çš„ä¸Šä¸‹æ–‡æ´å¯Ÿã€‚GraphMindèƒ½å¸®åŠ©ç”¨æˆ·æ ‡æ³¨è®ºæ–‡å…³é”®å…ƒç´ ã€æ¢ç´¢ç›¸å…³è®ºæ–‡å¹¶è¯„ä¼°å…¶æ–°é¢–æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ç”¨äºç§‘å­¦æ–‡çŒ®åˆ†æä¸­çš„æ–°é¢–æ€§è¯„ä¼°ã€‚</li>
<li>GraphMindæ˜¯ä¸€ç§äº¤äº’å¼Webå·¥å…·ï¼Œæ—¨åœ¨ååŠ©ç”¨æˆ·è¯„ä¼°ç§‘å­¦è®ºæ–‡æˆ–è‰æ¡ˆçš„æ–°é¢–æ€§ã€‚</li>
<li>GraphMindé€šè¿‡æ•´åˆLLMå’Œå¤–éƒ¨APIå¦‚arXivå’ŒSemantic Scholarï¼Œæä¾›ä¸°å¯Œçš„ç»“æ„åŒ–è§†å›¾ï¼Œå±•ç¤ºç§‘å­¦æ€æƒ³çš„æ ¸å¿ƒè´¡çŒ®åŠå…¶ä¸ç°æœ‰å·¥ä½œçš„è”ç³»ã€‚</li>
<li>GraphMindæ”¯æŒç”¨æˆ·æ•æ‰è®ºæ–‡çš„ä¸»è¦ç»“æ„å¹¶æ¢ç´¢ç›¸å…³æ€æƒ³ã€‚</li>
<li>ç”¨æˆ·å¯ä»¥é€šè¿‡GraphMindæ ‡æ³¨è®ºæ–‡å…³é”®å…ƒç´ ï¼Œå¹¶æ¢ç´¢ç›¸å…³è®ºæ–‡ã€‚</li>
<li>GraphMindæä¾›å¯éªŒè¯çš„ä¸Šä¸‹æ–‡æ´å¯Ÿï¼Œå¸®åŠ©ç”¨æˆ·è¯„ä¼°è®ºæ–‡çš„æ–°é¢–æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15706">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-08aa4a66652db04d4db3bce2ff14999a" align="middle">
<img src="https://picx.zhimg.com/v2-5e51b2497fc65ccc48fb106123282570" align="middle">
<img src="https://picx.zhimg.com/v2-d660d544ee558bbfc9814aa451f44594" align="middle">
<img src="https://picx.zhimg.com/v2-3aef4080d0574846cedca0ad7dfb04b8" align="middle">
<img src="https://picx.zhimg.com/v2-6ba53ddb05375a220b4a7db48f1e3338" align="middle">
<img src="https://picx.zhimg.com/v2-071bea4b950aff12cde7b577bbbbcba0" align="middle">
<img src="https://picx.zhimg.com/v2-4c5f8377ead20665a49d1c4f30123fb6" align="middle">
<img src="https://picx.zhimg.com/v2-5a74b586516816c12825ffc147e51e2f" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MirrorFuzz-Leveraging-LLM-and-Shared-Bugs-for-Deep-Learning-Framework-APIs-Fuzzing"><a href="#MirrorFuzz-Leveraging-LLM-and-Shared-Bugs-for-Deep-Learning-Framework-APIs-Fuzzing" class="headerlink" title="MirrorFuzz: Leveraging LLM and Shared Bugs for Deep Learning Framework   APIs Fuzzing"></a>MirrorFuzz: Leveraging LLM and Shared Bugs for Deep Learning Framework   APIs Fuzzing</h2><p><strong>Authors:Shiwen Ou, Yuwei Li, Lu Yu, Chengkun Wei, Tingke Wen, Qiangpu Chen, Yu Chen, Haizhi Tang, Zulie Pan</strong></p>
<p>Deep learning (DL) frameworks serve as the backbone for a wide range of artificial intelligence applications. However, bugs within DL frameworks can cascade into critical issues in higher-level applications, jeopardizing reliability and security. While numerous techniques have been proposed to detect bugs in DL frameworks, research exploring common API patterns across frameworks and the potential risks they entail remains limited. Notably, many DL frameworks expose similar APIs with overlapping input parameters and functionalities, rendering them vulnerable to shared bugs, where a flaw in one API may extend to analogous APIs in other frameworks. To address this challenge, we propose MirrorFuzz, an automated API fuzzing solution to discover shared bugs in DL frameworks. MirrorFuzz operates in three stages: First, MirrorFuzz collects historical bug data for each API within a DL framework to identify potentially buggy APIs. Second, it matches each buggy API in a specific framework with similar APIs within and across other DL frameworks. Third, it employs large language models (LLMs) to synthesize code for the API under test, leveraging the historical bug data of similar APIs to trigger analogous bugs across APIs. We implement MirrorFuzz and evaluate it on four popular DL frameworks (TensorFlow, PyTorch, OneFlow, and Jittor). Extensive evaluation demonstrates that MirrorFuzz improves code coverage by 39.92% and 98.20% compared to state-of-the-art methods on TensorFlow and PyTorch, respectively. Moreover, MirrorFuzz discovers 315 bugs, 262 of which are newly found, and 80 bugs are fixed, with 52 of these bugs assigned CNVD IDs. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¡†æ¶ä½œä¸ºå¹¿æ³›åº”ç”¨äºäººå·¥æ™ºèƒ½åº”ç”¨çš„éª¨å¹²ï¼Œå…¶å­˜åœ¨çš„æ¼æ´å¯èƒ½ä¼šçº§è”æˆæ›´é«˜çº§åº”ç”¨ä¸­çš„å…³é”®é—®é¢˜ï¼Œå¨èƒåˆ°å¯é æ€§å’Œå®‰å…¨æ€§ã€‚è™½ç„¶å·²æœ‰è®¸å¤šæŠ€æœ¯è¢«æå‡ºæ¥æ£€æµ‹DLæ¡†æ¶ä¸­çš„æ¼æ´ï¼Œä½†å…³äºè·¨æ¡†æ¶å¸¸è§APIæ¨¡å¼åŠå…¶æ½œåœ¨é£é™©çš„ç ”ç©¶ä»ç„¶æœ‰é™ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè®¸å¤šDLæ¡†æ¶æš´éœ²å‡ºç›¸ä¼¼çš„APIï¼Œå…·æœ‰é‡å çš„è¾“å…¥å‚æ•°å’ŒåŠŸèƒ½ï¼Œä½¿å…¶å®¹æ˜“å—åˆ°å…±äº«æ¼æ´çš„å½±å“ï¼Œä¸€ä¸ªAPIä¸­çš„ç¼ºé™·å¯èƒ½ä¼šæ‰©å±•åˆ°å…¶ä»–æ¡†æ¶ä¸­çš„ç±»ä¼¼APIã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MirrorFuzzï¼Œä¸€ç§è‡ªåŠ¨åŒ–APIæ¨¡ç³Šè§£å†³æ–¹æ¡ˆï¼Œç”¨äºå‘ç°DLæ¡†æ¶ä¸­çš„å…±äº«æ¼æ´ã€‚MirrorFuzzåˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼šé¦–å…ˆï¼ŒMirrorFuzzæ”¶é›†DLæ¡†æ¶ä¸­æ¯ä¸ªAPIçš„å†å²æ¼æ´æ•°æ®ï¼Œä»¥è¯†åˆ«å¯èƒ½å­˜åœ¨æ¼æ´çš„APIã€‚å…¶æ¬¡ï¼Œå®ƒå°†ç‰¹å®šæ¡†æ¶ä¸­çš„æ¼æ´APIä¸å…¶ä»–DLæ¡†æ¶å†…çš„ç±»ä¼¼APIè¿›è¡ŒåŒ¹é…ã€‚æœ€åï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆæˆé’ˆå¯¹æµ‹è¯•APIçš„ä»£ç ï¼Œå¹¶å€ŸåŠ©ç±»ä¼¼APIçš„å†å²æ¼æ´æ•°æ®æ¥è§¦å‘è·¨APIçš„ç±»ä¼¼æ¼æ´ã€‚æˆ‘ä»¬åœ¨å››ä¸ªæµè¡Œçš„DLæ¡†æ¶ï¼ˆTensorFlowã€PyTorchã€OneFlowå’ŒJittorï¼‰ä¸Šå®ç°äº†MirrorFuzzå¹¶å¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒMirrorFuzzåœ¨TensorFlowå’ŒPyTorchä¸Šçš„ä»£ç è¦†ç›–ç‡åˆ†åˆ«æé«˜äº†39.92%å’Œ98.20%ã€‚æ­¤å¤–ï¼ŒMirrorFuzzå‘ç°äº†315ä¸ªæ¼æ´ï¼Œå…¶ä¸­262ä¸ªæ˜¯å…¨æ–°å‘ç°çš„ï¼Œå¹¶ä¿®å¤äº†80ä¸ªæ¼æ´ï¼Œå…¶ä¸­52ä¸ªæ¼æ´è¢«åˆ†é…äº†CNVDç¼–å·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15690v1">PDF</a> Accepted for publication in IEEE Transactions on Software Engineering   (TSE), 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ·±åº¦å­¦ä¹ æ¡†æ¶ä½œä¸ºä¼—å¤šäººå·¥æ™ºèƒ½åº”ç”¨çš„æ ¸å¿ƒï¼Œå…¶å­˜åœ¨çš„æ¼æ´å¯èƒ½å¼•å‘ä¸¥é‡é—®é¢˜ã€‚ä¸ºæ£€æµ‹æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­çš„æ¼æ´ï¼Œæå‡ºMirrorFuzzæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆé€šè¿‡ä¸‰ä¸ªæ­¥éª¤è¿›è¡Œè‡ªåŠ¨åŒ–APIæ¨¡ç³Šæµ‹è¯•ä»¥å‘ç°å…±äº«æ¼æ´ã€‚é¦–å…ˆï¼Œæ”¶é›†æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­æ¯ä¸ªAPIçš„å†å²æ¼æ´æ•°æ®ä»¥è¯†åˆ«æ½œåœ¨æ¼æ´APIï¼›å…¶æ¬¡ï¼ŒåŒ¹é…ç‰¹å®šæ¡†æ¶ä¸­çš„æ¼æ´APIä¸å…¶ä»–æ¡†æ¶ä¸­çš„ç›¸ä¼¼APIï¼›æœ€åï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åˆæˆå¾…æµ‹è¯•APIçš„ä»£ç ï¼Œå¹¶å€ŸåŠ©ç›¸ä¼¼APIçš„å†å²æ¼æ´æ•°æ®è§¦å‘è·¨APIçš„ç±»ä¼¼æ¼æ´ã€‚åœ¨å››ä¸ªæµè¡Œæ·±åº¦å­¦ä¹ æ¡†æ¶ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMirrorFuzzè¾ƒç°æœ‰æŠ€æœ¯æé«˜äº†39.92%å’Œ98.20%çš„ä»£ç è¦†ç›–ç‡ï¼Œå¹¶å‘ç°äº†315ä¸ªæ¼æ´ï¼Œå…¶ä¸­262ä¸ªä¸ºæ–°å‘ç°ï¼Œå¹¶æˆåŠŸä¿®å¤äº†80ä¸ªæ¼æ´ï¼Œå…¶ä¸­52ä¸ªè·å¾—äº†CNVDè®¤è¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­å­˜åœ¨çš„æ¼æ´å¯èƒ½å¯¹é«˜çº§åº”ç”¨ç¨‹åºçš„å¯é æ€§å’Œå®‰å…¨æ€§æ„æˆå¨èƒã€‚</li>
<li>MirrorFuzzæ˜¯ä¸€ç§é’ˆå¯¹æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­çš„å…±äº«æ¼æ´çš„è‡ªåŠ¨åŒ–APIæ¨¡ç³Šæµ‹è¯•è§£å†³æ–¹æ¡ˆã€‚</li>
<li>MirrorFuzzé€šè¿‡æ”¶é›†å†å²æ¼æ´æ•°æ®æ¥è¯†åˆ«æ½œåœ¨æ¼æ´çš„APIï¼Œå¹¶åŒ¹é…è·¨æ¡†æ¶çš„ç›¸ä¼¼APIã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åˆæˆAPIä»£ç å¹¶è§¦å‘ç±»ä¼¼æ¼æ´ã€‚</li>
<li>åœ¨å››ä¸ªæµè¡Œæ·±åº¦å­¦ä¹ æ¡†æ¶ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMirrorFuzzè¾ƒç°æœ‰æ–¹æ³•æ˜¾è‘—æé«˜ä»£ç è¦†ç›–ç‡ã€‚</li>
<li>MirrorFuzzå‘ç°äº†å¤§é‡æ–°æ¼æ´å¹¶æˆåŠŸä¿®å¤äº†éƒ¨åˆ†æ¼æ´ï¼Œéƒ¨åˆ†è·å¾—CNVDè®¤è¯ã€‚</li>
<li>MirrorFuzzçš„æ–¹æ¡ˆä¸ºæ·±åº¦å­¦ä¹ æ¡†æ¶çš„æ¼æ´æ£€æµ‹æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15690">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-792fc233c92e93562349a9c399523032" align="middle">
<img src="https://picx.zhimg.com/v2-d17cc7b4be02ae87eba177fc7dafb3e9" align="middle">
<img src="https://picx.zhimg.com/v2-05a9651bec84247a4e27790512b0b613" align="middle">
<img src="https://picx.zhimg.com/v2-c83250544dc21d941124c4b0f3e8eb5a" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="HypoSpace-Evaluating-LLM-Creativity-as-Set-Valued-Hypothesis-Generators-under-Underdetermination"><a href="#HypoSpace-Evaluating-LLM-Creativity-as-Set-Valued-Hypothesis-Generators-under-Underdetermination" class="headerlink" title="HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators   under Underdetermination"></a>HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators   under Underdetermination</h2><p><strong>Authors:Tingting Chen, Beibei Lin, Zifeng Yuan, Qiran Zou, Hongyu He, Yew-Soon Ong, Anirudh Goyal, Dianbo Liu</strong></p>
<p>As language models are increasingly used in scientific workflows, evaluating their ability to propose sets of explanations-not just a single correct answer-becomes critical. Many scientific problems are underdetermined: multiple, mechanistically distinct hypotheses are consistent with the same observations. We introduce HypoSpace, a diagnostic suite that treats LLMs as samplers of finite hypothesis sets and measures three complementary indicators: Validity (precision of proposals consistent with observations), Uniqueness (non-redundancy among proposals), and Recovery (coverage of the enumerated admissible set). We instantiate HypoSpace in three structured domains with deterministic validators and exactly enumerated hypothesis spaces: (i) causal graphs from perturbations, (ii) gravity-constrained 3D voxel reconstruction from top-down projections, and (iii) Boolean genetic interactions. Across instruction-tuned and reasoning-focused models, Validity often remains high while Uniqueness and Recovery degrade as the admissible space grows, revealing mode collapse that is invisible to correctness-only metrics. HypoSpace offers a controlled probe-rather than a leaderboard-for methods that explicitly explore and cover admissible explanation spaces. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/CTT-Pavilion/_HypoSpace">https://github.com/CTT-Pavilion/_HypoSpace</a>. </p>
<blockquote>
<p>éšç€è¯­è¨€æ¨¡å‹åœ¨ç§‘ç ”å·¥ä½œæµç¨‹ä¸­çš„ä½¿ç”¨æ—¥ç›Šå¢åŠ ï¼Œè¯„ä¼°å®ƒä»¬æå‡ºä¸€ç³»åˆ—è§£é‡Šçš„èƒ½åŠ›â€”â€”è€Œä¸ä»…ä»…æ˜¯å•ä¸€æ­£ç¡®ç­”æ¡ˆâ€”â€”å˜å¾—è‡³å…³é‡è¦ã€‚è®¸å¤šç§‘å­¦é—®é¢˜å…·æœ‰ä¸ç¡®å®šæ€§ï¼šå¤šä¸ªæœºåˆ¶ä¸Šä¸åŒçš„å‡è®¾ä¸åŒä¸€ç»„è§‚æµ‹ç»“æœç›¸ç¬¦ã€‚æˆ‘ä»¬å¼•å…¥äº†HypoSpaceï¼Œè¿™æ˜¯ä¸€ç§è¯Šæ–­å·¥å…·å¥—ä»¶ï¼Œå®ƒå°†å¤§å‹è¯­è¨€æ¨¡å‹è§†ä¸ºæœ‰é™å‡è®¾é›†çš„é‡‡æ ·å™¨ï¼Œå¹¶æµ‹é‡ä¸‰ä¸ªäº’è¡¥æŒ‡æ ‡ï¼šæœ‰æ•ˆæ€§ï¼ˆä¸è§‚æµ‹ç»“æœä¸€è‡´çš„ææ¡ˆçš„å‡†ç¡®æ€§ï¼‰ã€å”¯ä¸€æ€§ï¼ˆææ¡ˆä¹‹é—´çš„éå†—ä½™æ€§ï¼‰å’Œæ¢å¤èƒ½åŠ›ï¼ˆæ‰€åˆ—ä¸¾çš„å¯æ¥å—é›†åˆçš„è¦†ç›–èŒƒå›´ï¼‰ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªç»“æ„åŒ–é¢†åŸŸå®ä¾‹åŒ–HypoSpaceï¼Œè¿™äº›é¢†åŸŸå…·æœ‰ç¡®å®šæ€§éªŒè¯å™¨å’Œç²¾ç¡®æšä¸¾çš„å‡è®¾ç©ºé—´ï¼šï¼ˆiï¼‰æ¥è‡ªæ‰°åŠ¨çš„å› æœå›¾ï¼Œï¼ˆiiï¼‰å—é‡åŠ›çº¦æŸçš„3Dä½“ç´ ä»é¡¶éƒ¨å‘ä¸‹çš„æŠ•å½±é‡å»ºï¼Œä»¥åŠï¼ˆiiiï¼‰å¸ƒå°”åŸºå› ç›¸äº’ä½œç”¨ã€‚åœ¨æŒ‡ä»¤è°ƒæ•´å’Œæ¨ç†é‡ç‚¹æ¨¡å‹ä¸­ï¼Œæœ‰æ•ˆæ€§é€šå¸¸ä¿æŒè¾ƒé«˜æ°´å¹³ï¼Œè€Œéšç€å¯æ¥å—ç©ºé—´çš„å¢é•¿ï¼Œå”¯ä¸€æ€§å’Œæ¢å¤èƒ½åŠ›ä¼šä¸‹é™ï¼Œè¿™æ­ç¤ºäº†æ¨¡å¼å´©æºƒï¼Œè¿™ç§å´©æºƒæƒ…å†µåœ¨ä½¿ç”¨ä»…æ ¡æ­£æŒ‡æ ‡æ—¶æ˜¯çœ‹ä¸åˆ°çš„ã€‚HypoSpaceä¸ºé‚£äº›æ˜ç¡®æ¢ç´¢å’Œè¦†ç›–å¯æ¥å—è§£é‡Šç©ºé—´çš„æ–¹æ³•æä¾›äº†ä¸€ä¸ªå—æ§æ¢é’ˆï¼Œè€Œä¸æ˜¯æ’è¡Œæ¦œã€‚ç›¸å…³ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å¾—ï¼š<a target="_blank" rel="noopener" href="https://github.com/CTT-Pavilion/_HypoSpace">ç½‘ç«™é“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15614v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦å·¥ä½œæµä¸­çš„ä½¿ç”¨æ—¥ç›Šå¢å¤šï¼Œè¯„ä¼°å®ƒä»¬æå‡ºä¸€å¥—è§£é‡Šè€Œéä»…å•ä¸€æ­£ç¡®ç­”æ¡ˆçš„èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†HypoSpaceè¯Šæ–­å¥—ä»¶ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹è§†ä¸ºæœ‰é™å‡è®¾é›†çš„é‡‡æ ·å™¨ï¼Œå¹¶æµ‹é‡ä¸‰ä¸ªäº’è¡¥æŒ‡æ ‡ï¼šæœ‰æ•ˆæ€§ï¼ˆææ¡ˆä¸è§‚å¯Ÿç»“æœçš„ä¸€è‡´æ€§ç¨‹åº¦ï¼‰ã€å”¯ä¸€æ€§ï¼ˆææ¡ˆä¹‹é—´çš„éå†—ä½™æ€§ï¼‰å’Œæ¢å¤æ€§ï¼ˆå¯¹åˆ—ä¸¾çš„å¯æ¥å—é›†åˆçš„è¦†ç›–åº¦ï¼‰ã€‚é€šè¿‡ä¸‰ä¸ªç»“æ„åŒ–é¢†åŸŸçš„å®ä¾‹ç ”ç©¶ï¼ŒåŒ…æ‹¬å› æœå›¾ã€é‡åŠ›çº¦æŸçš„ä¸‰ç»´ä½“ç´ é‡å»ºå’Œå¸ƒå°”é—ä¼ äº¤äº’ç­‰ï¼Œå‘ç°éšç€å¯æ¥å—ç©ºé—´çš„å¢é•¿ï¼Œæœ‰æ•ˆæ€§å¾€å¾€ä¿æŒé«˜æ°´å¹³ï¼Œè€Œå”¯ä¸€æ€§å’Œæ¢å¤æ€§ä¼šä¸‹é™ï¼Œæš´éœ²å‡ºæ¨¡å¼å´©æºƒçš„é—®é¢˜ï¼Œè¿™åœ¨ä»…ä½¿ç”¨æ­£ç¡®æ€§æŒ‡æ ‡æ—¶æ˜¯æ— æ³•çœ‹åˆ°çš„ã€‚HypoSpaceä¸ºæ˜¾å¼æ¢ç´¢å’Œè¦†ç›–å¯æ¥å—è§£é‡Šç©ºé—´çš„æ–¹æ³•æä¾›äº†ä¸€ä¸ªå—æ§çš„æ¢é’ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšç€è¯­è¨€æ¨¡å‹åœ¨ç§‘ç ”ä¸­çš„åº”ç”¨å¢å¤šï¼Œè¯„ä¼°å…¶æå‡ºå¤šç§è§£é‡Šçš„èƒ½åŠ›å˜å¾—å…³é”®ã€‚</li>
<li>å¾ˆå¤šç§‘å­¦é—®é¢˜å­˜åœ¨å¤šç§å¯èƒ½çš„è§£é‡Šã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†HypoSpaceè¯Šæ–­å¥—ä»¶ï¼Œä»¥è¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨æå‡ºå‡è®¾æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥å¥—ä»¶é€šè¿‡ä¸‰ä¸ªæŒ‡æ ‡æ¥è¡¡é‡è¯­è¨€æ¨¡å‹çš„è¡¨ç°ï¼šæœ‰æ•ˆæ€§ã€å”¯ä¸€æ€§å’Œæ¢å¤æ€§ã€‚</li>
<li>å®ä¾‹ç ”ç©¶è¡¨æ˜ï¼Œéšç€å¯æ¥å—ç©ºé—´çš„å¢é•¿ï¼Œè¯­è¨€æ¨¡å‹çš„å”¯ä¸€æ€§å’Œæ¢å¤æ€§å¯èƒ½ä¼šä¸‹é™ã€‚</li>
<li>è¿™æš´éœ²äº†è¯­è¨€æ¨¡å‹åœ¨æŸäº›æƒ…å†µä¸‹çš„æ¨¡å¼å´©æºƒé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15614">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cc0af8f8e3b7df25e92c12d487574419" align="middle">
<img src="https://picx.zhimg.com/v2-075e565079ca83084c5d653083520946" align="middle">
<img src="https://picx.zhimg.com/v2-8c58d281c7222dfb65ab6afc2607e876" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Can-generative-AI-figure-out-figurative-language-The-influence-of-idioms-on-essay-scoring-by-ChatGPT-Gemini-and-Deepseek"><a href="#Can-generative-AI-figure-out-figurative-language-The-influence-of-idioms-on-essay-scoring-by-ChatGPT-Gemini-and-Deepseek" class="headerlink" title="Can generative AI figure out figurative language? The influence of   idioms on essay scoring by ChatGPT, Gemini, and Deepseek"></a>Can generative AI figure out figurative language? The influence of   idioms on essay scoring by ChatGPT, Gemini, and Deepseek</h2><p><strong>Authors:Enis OÄŸuz</strong></p>
<p>The developments in Generative AI technologies have paved the way for numerous innovations in different fields. Recently, Generative AI has been proposed as a competitor to AES systems in evaluating student essays automatically. Considering the potential limitations of AI in processing idioms, this study assessed the scoring performances of Generative AI models for essays with and without idioms by incorporating insights from Corpus Linguistics and Computational Linguistics. Two equal essay lists were created from 348 student essays taken from a corpus: one with multiple idioms present in each essay and another with no idioms in essays. Three Generative AI models (ChatGPT, Gemini, and Deepseek) were asked to score all essays in both lists three times, using the same rubric used by human raters in assigning essay scores. The results revealed excellent consistency for all models, but Gemini outperformed its competitors in interrater reliability with human raters. There was also no detectable bias for any demographic group in AI assessment. For essays with multiple idioms, Gemini followed a the most similar pattern to human raters. While the models in the study demonstrated potential for a hybrid approach, Gemini was the best candidate for the task due to its ability to handle figurative language and showed promise for handling essay-scoring tasks alone in the future. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ç”ŸæˆæŠ€æœ¯ï¼ˆGenerative AIï¼‰çš„å‘å±•ä¸ºä¸åŒé¢†åŸŸçš„åˆ›æ–°é“ºå¹³äº†é“è·¯ã€‚æœ€è¿‘ï¼Œäººå·¥æ™ºèƒ½ç”ŸæˆæŠ€æœ¯è¢«æè®®ä½œä¸ºè‡ªåŠ¨è¯„ä¼°å­¦ç”Ÿä½œæ–‡çš„AESç³»ç»Ÿçš„ç«äº‰å¯¹æ‰‹ã€‚è€ƒè™‘åˆ°äººå·¥æ™ºèƒ½åœ¨å¤„ç†æˆè¯­æ–¹é¢çš„æ½œåœ¨å±€é™æ€§ï¼Œæœ¬ç ”ç©¶é€šè¿‡èåˆè¯­æ–™åº“è¯­è¨€å­¦å’Œè®¡ç®—è¯­è¨€å­¦çš„è§è§£ï¼Œè¯„ä¼°äº†äººå·¥æ™ºèƒ½ç”Ÿæˆæ¨¡å‹åœ¨æœ‰ã€æ— æˆè¯­ä½œæ–‡ä¸Šçš„è¯„åˆ†è¡¨ç°ã€‚ä»è¯­æ–™åº“ä¸­é€‰å–äº†348ç¯‡å­¦ç”Ÿä½œæ–‡ï¼Œåˆ›å»ºäº†ä¸¤ä¸ªç›¸ç­‰çš„ä½œæ–‡åˆ—è¡¨ï¼šä¸€ä¸ªåˆ—è¡¨ä¸­çš„æ¯ç¯‡ä½œæ–‡éƒ½æœ‰å¤šä¸ªæˆè¯­ï¼Œå¦ä¸€ä¸ªåˆ—è¡¨çš„ä½œæ–‡åˆ™æ— æˆè¯­ã€‚è¦æ±‚ä¸‰ä¸ªç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹ï¼ˆChatGPTã€Geminiå’ŒDeepseekï¼‰ä¸‰æ¬¡å¯¹æ‰€æœ‰ä½œæ–‡è¿›è¡Œè¯„åˆ†ï¼Œè¯„åˆ†ä¾æ®ä¸äººç±»è¯„åˆ†è€…åˆ†é…ä½œæ–‡åˆ†æ•°æ—¶ä½¿ç”¨çš„æ ‡å‡†ä¸€è‡´ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰æ¨¡å‹çš„è¯„åˆ†ä¸€è‡´æ€§éƒ½éå¸¸é«˜ï¼Œä½†åœ¨ä¸äººç±»è¯„åˆ†è€…çš„è·¨è¯„ä»·è€…å¯é æ€§æ–¹é¢ï¼ŒGeminiè¡¨ç°æœ€ä½³ã€‚æ­¤å¤–ï¼Œä»»ä½•äººå£ç»Ÿè®¡ç¾¤ä½“åœ¨äººå·¥æ™ºèƒ½è¯„ä¼°ä¸­éƒ½æ²¡æœ‰æ˜æ˜¾çš„åè§ã€‚å¯¹äºå«æœ‰å¤šä¸ªæˆè¯­çš„ä½œæ–‡ï¼ŒGeminiçš„è¯„åˆ†æ¨¡å¼ä¸äººç±»è¯„åˆ†è€…æœ€ä¸ºç›¸ä¼¼ã€‚è™½ç„¶ç ”ç©¶ä¸­çš„æ¨¡å‹éƒ½æ˜¾ç¤ºäº†æ··åˆæ–¹æ³•çš„æ½œåŠ›ï¼Œä½†ç”±äºèƒ½å¤Ÿå¤„ç†æ¯”å–»è¯­è¨€çš„èƒ½åŠ›ï¼ŒGeminiæœ€é€‚åˆè¿™ä¸€ä»»åŠ¡ï¼Œå¹¶æœ‰æœ›åœ¨æœªæ¥å•ç‹¬å¤„ç†ä½œæ–‡è¯„åˆ†ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15009v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ç”Ÿæˆå¼AIæ¨¡å‹åœ¨è¯„ä¼°å¸¦æœ‰å’Œä¸å¸¦æˆè¯­çš„å­¦ç”Ÿä½œæ–‡æ–¹é¢çš„è¡¨ç°ã€‚ç»“åˆè¯­æ–™åº“è¯­è¨€å­¦å’Œè®¡ç®—è¯­è¨€å­¦ï¼Œå¯¹ChatGPTã€Geminiå’ŒDeepseekä¸‰æ¬¾ç”Ÿæˆå¼AIæ¨¡å‹è¿›è¡Œäº†å®éªŒè¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ‰€æœ‰æ¨¡å‹è¡¨ç°å‡ºè‰¯å¥½çš„ä¸€è‡´æ€§ï¼Œä½†Geminiåœ¨ä¸äººè¯„åˆ†çš„å¯é æ€§æ–¹é¢è¡¨ç°æœ€ä½³ã€‚å¯¹äºå¸¦æœ‰å¤šä¸ªæˆè¯­çš„ä½œæ–‡ï¼ŒGeminiçš„æ¨¡å¼æœ€ç¬¦åˆäººç±»è¯„åˆ†è€…çš„æ¨¡å¼ã€‚æœªæ¥ï¼Œç”Ÿæˆå¼AIæ¨¡å‹æœ‰æœ›åœ¨å•ç‹¬å¤„ç†ä½œæ–‡è¯„åˆ†ä»»åŠ¡æ–¹é¢å±•ç°æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼AIæŠ€æœ¯åœ¨ä¸åŒé¢†åŸŸæ¨åŠ¨äº†å¤šé¡¹åˆ›æ–°ã€‚</li>
<li>è¿‘æœŸï¼Œç”Ÿæˆå¼AIè¢«æè®®ä½œä¸ºè¯„ä¼°å­¦ç”Ÿä½œæ–‡çš„è‡ªåŠ¨è¯„ä¼°ç³»ç»Ÿï¼ˆAESï¼‰çš„ç«äº‰å¯¹æ‰‹ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†ç”Ÿæˆå¼AIæ¨¡å‹å¯¹å¸¦æœ‰å’Œä¸å¸¦æˆè¯­çš„ä½œæ–‡çš„è¯„åˆ†è¡¨ç°ã€‚</li>
<li>ä¸‰æ¬¾ç”Ÿæˆå¼AIæ¨¡å‹ï¼ˆChatGPTã€Geminiå’ŒDeepseekï¼‰å‚ä¸å®éªŒè¯„ä¼°ã€‚</li>
<li>æ‰€æœ‰æ¨¡å‹è¡¨ç°å‡ºè‰¯å¥½çš„ä¸€è‡´æ€§ï¼Œä½†Geminiåœ¨ä¸äººè¯„åˆ†çš„å¯é æ€§æ–¹é¢æœ€ä½³ã€‚</li>
<li>å¯¹äºå¸¦æœ‰å¤šä¸ªæˆè¯­çš„ä½œæ–‡ï¼ŒGeminiçš„è¯„åˆ†æ¨¡å¼æœ€ç¬¦åˆäººç±»è¯„åˆ†è€…çš„æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15009">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5430fa31543fde1aa368241c98d397d9" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-with-Verifiable-yet-Noisy-Rewards-under-Imperfect-Verifiers"><a href="#Reinforcement-Learning-with-Verifiable-yet-Noisy-Rewards-under-Imperfect-Verifiers" class="headerlink" title="Reinforcement Learning with Verifiable yet Noisy Rewards under Imperfect   Verifiers"></a>Reinforcement Learning with Verifiable yet Noisy Rewards under Imperfect   Verifiers</h2><p><strong>Authors:Xin-Qiang Cai, Wei Wang, Feng Liu, Tongliang Liu, Gang Niu, Masashi Sugiyama</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) trains policies against automated verifiers to avoid costly human labeling. To reduce vulnerability to verifier hacking, many RLVR systems collapse rewards to binary ${0,1}$ during training. This choice carries a cost: it introduces \textit{false negatives} (rejecting correct answers, FNs) and \textit{false positives} (accepting incorrect ones, FPs). For instance, a rule-based checker may mark the correct fraction $\frac{12}{36}$ as wrong when compared against the canonical $\frac{1}{3}$ due to brittle parsing&#x2F;equivalence rules (FN), while a large language model (LLM) judges can be gamed by superficial cues or even a single adversarial token, yielding inflated correctness for wrong solutions (FP). We formalize verifier unreliability by modeling the verifier as a stochastic reward channel with asymmetric noise rates. From this abstraction, we derive two correction algorithms for verifier errors. The first is a \textit{backward} correction that de-biases the observed binary reward to recover an \textit{unbiased} estimator of the clean policy gradient. The second is a \textit{forward} correction that reweights score-function terms so that the expected update direction aligns with the \textit{clean gradient}; notably, it requires only the FN rate. We implement both as lightweight hooks in a group relative policy optimization (GRPO)-based RLVR pipeline and evaluate them on math-reasoning models and benchmarks. Across models and datasets, both corrections improve over uncorrected training; the forward variant converges faster and remains stable under heavier noise. Finally, we show a practical appeal mechanism in which a lightweight LLM verifier estimates the FN rate online by rechecking rule-based negatives, obtaining outperformance compared with other state-of-the-art contenders. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰é€šè¿‡é’ˆå¯¹è‡ªåŠ¨åŒ–éªŒè¯å™¨è®­ç»ƒç­–ç•¥ï¼Œä»¥é¿å…æ˜‚è´µçš„äººåŠ›æ ‡æ³¨ã€‚ä¸ºäº†å‡å°‘éªŒè¯å™¨é»‘å®¢æ”»å‡»å¸¦æ¥çš„è„†å¼±æ€§ï¼Œè®¸å¤šRLVRç³»ç»Ÿå°†å¥–åŠ±ç¼©å‡ä¸ºäºŒå…ƒ${0ï¼Œ1}$åœ¨è®­ç»ƒæœŸé—´ã€‚è¿™ç§é€‰æ‹©æ˜¯æœ‰ä»£ä»·çš„ï¼šå®ƒå¼•å…¥äº†â€œå‡é˜´æ€§â€ï¼ˆæ‹’ç»æ­£ç¡®ç­”æ¡ˆï¼‰å’Œâ€œå‡é˜³æ€§â€ï¼ˆæ¥å—é”™è¯¯ç­”æ¡ˆï¼‰ã€‚ä¾‹å¦‚ï¼ŒåŸºäºè§„åˆ™çš„æ£€æŸ¥å™¨åœ¨ä¸æ ‡å‡†$\frac{1}{3}$ç›¸æ¯”æ—¶ï¼Œå¯èƒ½ä¼šå°†æ­£ç¡®çš„åˆ†æ•°$\frac{12}{36}$æ ‡è®°ä¸ºé”™è¯¯ï¼Œè¿™æ˜¯ç”±äºè„†å¼±çš„è§£æ&#x2F;ç­‰ä»·è§„åˆ™ï¼ˆFNï¼‰ï¼Œè€Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åˆ¤æ–­å¯èƒ½ä¼šå—åˆ°è¡¨é¢çº¿ç´¢ç”šè‡³æ˜¯å•ä¸ªå¯¹æŠ—æ€§æ ‡è®°çš„å½±å“ï¼Œä¸ºé”™è¯¯è§£å†³æ–¹æ¡ˆæä¾›è†¨èƒ€çš„æ­£ç¡®æ€§ï¼ˆFPï¼‰ã€‚æˆ‘ä»¬é€šè¿‡å°†éªŒè¯å™¨å»ºæ¨¡ä¸ºå…·æœ‰ä¸å¯¹ç§°å™ªå£°ç‡çš„éšæœºå¥–åŠ±é€šé“æ¥æ­£å¼éªŒè¯å™¨çš„ä¸å¯é æ€§ã€‚ä»è¿™ä¸ªæŠ½è±¡ä¸­ï¼Œæˆ‘ä»¬ä¸ºéªŒè¯å™¨é”™è¯¯æ¨å¯¼å‡ºä¸¤ç§æ ¡æ­£ç®—æ³•ã€‚ç¬¬ä¸€ç§æ˜¯â€œå‘åâ€æ ¡æ­£ï¼Œå®ƒé€šè¿‡æ¶ˆé™¤è§‚å¯Ÿåˆ°çš„äºŒå…ƒå¥–åŠ±çš„åè§ï¼Œä»¥æ¢å¤æ¸…æ´æ”¿ç­–æ¢¯åº¦çš„â€œæ— åè§â€ä¼°è®¡å™¨ã€‚ç¬¬äºŒç§æ˜¯â€œå‘å‰â€æ ¡æ­£ï¼Œå®ƒé‡æ–°åŠ æƒåˆ†æ•°å‡½æ•°é¡¹ï¼Œä»¥ä½¿é¢„æœŸçš„æ›´æ–°æ–¹å‘ä¸â€œæ¸…æ´æ¢¯åº¦â€ä¸€è‡´ï¼›å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåªéœ€è¦FNç‡ã€‚æˆ‘ä»¬å°†è¿™ä¸¤è€…éƒ½å®ç°ä¸ºåŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„RLVRç®¡é“ä¸­çš„è½»é‡çº§é’©å­ï¼Œå¹¶åœ¨æ•°å­¦æ¨ç†æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸Šå¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚åœ¨æ¨¡å‹å’Œæ•°æ®é›†æ–¹é¢ï¼Œè¿™ä¸¤ç§æ ¡æ­£æ–¹æ³•éƒ½ä¼˜äºæœªæ ¡æ­£çš„è®­ç»ƒï¼›å‘å‰æ–¹æ³•æ”¶æ•›æ›´å¿«ï¼Œåœ¨æ›´é‡çš„å™ªå£°ä¸‹ä¿æŒç¨³å®šã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ç§å®ç”¨å¸å¼•åŠ›æœºåˆ¶ï¼Œå…¶ä¸­è½»é‡çº§LLMéªŒè¯å™¨é€šè¿‡é‡æ–°æ£€æŸ¥åŸºäºè§„åˆ™çš„é˜´æ€§æ¥åœ¨çº¿ä¼°è®¡FNç‡ï¼Œå¹¶å–å¾—äº†ä¸å…¶ä»–æœ€å…ˆè¿›ç«äº‰è€…ç›¸æ¯”çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.00915v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰é€šè¿‡é’ˆå¯¹è‡ªåŠ¨åŒ–éªŒè¯å™¨è®­ç»ƒç­–ç•¥ï¼Œé¿å…äº†æ˜‚è´µçš„äººåŠ›æ ‡æ³¨ã€‚ä¸ºå‡å°‘éªŒè¯å™¨è¢«é»‘å®¢æ”»å‡»çš„é£é™©ï¼Œè®¸å¤šRLVRç³»ç»Ÿå°†å¥–åŠ±åœ¨è®­ç»ƒæœŸé—´ç®€åŒ–ä¸ºäºŒå…ƒ{0ï¼Œ1}ã€‚ä½†è¿™ç§é€‰æ‹©ä¹Ÿæœ‰ä»£ä»·ï¼šå®ƒå¼•å…¥äº†å‡é˜´æ€§ï¼ˆæ‹’ç»æ­£ç¡®ç­”æ¡ˆï¼‰å’Œå‡é˜³æ€§ï¼ˆæ¥å—é”™è¯¯ç­”æ¡ˆï¼‰ã€‚ä¾‹å¦‚ï¼ŒåŸºäºè§„åˆ™çš„æ£€æŸ¥å™¨å¯èƒ½ä¼šå°†æ­£ç¡®çš„åˆ†æ•°12&#x2F;36ä¸æ ‡å‡†çš„1&#x2F;3è¿›è¡Œæ¯”è¾ƒæ—¶æ ‡è®°ä¸ºé”™è¯¯ï¼ˆå‡é˜´æ€§ï¼‰ï¼Œè€Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆ™å¯èƒ½è¢«è¡¨é¢çº¿ç´¢ç”šè‡³å•ä¸ªå¯¹æŠ—æ€§æ ‡è®°æ‰€å½±å“ï¼Œä¸ºé”™è¯¯è§£å†³æ–¹æ¡ˆæä¾›è¿‡é«˜æ­£ç¡®æ€§è¯„ä¼°ï¼ˆå‡é˜³æ€§ï¼‰ã€‚æˆ‘ä»¬é€šè¿‡å°†éªŒè¯å™¨å»ºæ¨¡ä¸ºå…·æœ‰ä¸å¯¹ç§°å™ªå£°ç‡çš„éšæœºå¥–åŠ±é€šé“æ¥å½¢å¼åŒ–éªŒè¯å™¨çš„ä¸å¯é æ€§ã€‚ä»è¿™ä¸ªæŠ½è±¡ä¸­ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºäº†ä¸¤ç§çº æ­£éªŒè¯å™¨é”™è¯¯çš„ç®—æ³•ã€‚é¦–å…ˆæ˜¯åå‘æ ¡æ­£ï¼Œå®ƒé€šè¿‡å»åè§‚å¯Ÿåˆ°çš„äºŒå…ƒå¥–åŠ±æ¥æ¢å¤å¹²å‡€çš„æ”¿ç­–æ¢¯åº¦æ— åä¼°è®¡å™¨ã€‚å…¶æ¬¡æ˜¯æ­£å‘æ ¡æ­£ï¼Œå®ƒé‡æ–°åŠ æƒåˆ†æ•°å‡½æ•°çš„é¡¹ï¼Œä»¥ä½¿é¢„æœŸæ›´æ–°æ–¹å‘ä¸å¹²å‡€æ¢¯åº¦ä¸€è‡´ï¼›å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåªéœ€è¦å‡é˜´æ€§ç‡ã€‚æˆ‘ä»¬å°†è¿™ä¸¤ç§æ–¹æ³•å®ç°ä¸ºåŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„RLVRç®¡é“ä¸­çš„è½»é‡çº§é’©å­ï¼Œå¹¶åœ¨æ•°å­¦æ¨ç†æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚åœ¨æ¨¡å‹å’Œæ•°æ®é›†æ–¹é¢ï¼Œç»è¿‡æ ¡æ­£çš„è®­ç»ƒè¡¨ç°å‡ä¼˜äºæœªæ ¡æ­£çš„è®­ç»ƒï¼›æ­£å‘æ ¡æ­£æ”¶æ•›æ›´å¿«ï¼Œåœ¨æ›´é‡çš„å™ªå£°ä¸‹ä»èƒ½ä¿æŒç¨³å®šã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ç§å®ç”¨çš„ä¸Šè¯‰æœºåˆ¶ï¼Œå…¶ä¸­è½»é‡çº§LLMéªŒè¯å™¨é€šè¿‡é‡æ–°æ£€æŸ¥åŸºäºè§„åˆ™çš„ç»“æœæ¥ä¼°ç®—å‡é˜´æ€§ç‡ï¼Œç›¸è¾ƒäºå…¶ä»–æœ€æ–°ç«äº‰è€…è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLVRé€šè¿‡è‡ªåŠ¨åŒ–éªŒè¯å™¨è®­ç»ƒç­–ç•¥ï¼Œå‡å°‘äº†å¯¹æ˜‚è´µäººåŠ›æ ‡æ³¨çš„ä¾èµ–ã€‚</li>
<li>éªŒè¯å™¨å¯èƒ½è¢«é»‘å®¢æ”»å‡»å½±å“ï¼Œå› æ­¤éœ€è¦é‡‡å–å®‰å…¨æªæ–½ã€‚</li>
<li>å¥–åŠ±ç®€åŒ–ä¸ºäºŒå…ƒ{0,1}å¯èƒ½å¼•å…¥å‡é˜´æ€§å’Œå‡é˜³æ€§ç»“æœã€‚</li>
<li>éªŒè¯å™¨çš„ä¸å¯é æ€§å¯ä»¥é€šè¿‡å°†å…¶å»ºæ¨¡ä¸ºéšæœºå¥–åŠ±é€šé“è¿›è¡Œå½¢å¼åŒ–ã€‚</li>
<li>æå‡ºäº†ä¸¤ç§çº æ­£éªŒè¯å™¨é”™è¯¯çš„ç®—æ³•ï¼šåå‘æ ¡æ­£å’Œæ­£å‘æ ¡æ­£ã€‚</li>
<li>ä¸¤ç§æ–¹æ³•åœ¨åŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„RLVRç®¡é“ä¸­å®æ–½å¹¶è¿›è¡Œäº†è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.00915">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-56a4e57b2a64c564eb26f51d023f45d5" align="middle">
<img src="https://picx.zhimg.com/v2-fd0bbec6f3bb5d80832b722d5b635ffd" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CCD-Mitigating-Hallucinations-in-Radiology-MLLMs-via-Clinical-Contrastive-Decoding"><a href="#CCD-Mitigating-Hallucinations-in-Radiology-MLLMs-via-Clinical-Contrastive-Decoding" class="headerlink" title="CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical   Contrastive Decoding"></a>CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical   Contrastive Decoding</h2><p><strong>Authors:Xi Zhang, Zaiqiao Meng, Jake Lever, Edmond S. L. Ho</strong></p>
<p>Multimodal large language models (MLLMs) have recently achieved remarkable progress in radiology by integrating visual perception with natural language understanding. However, they often generate clinically unsupported descriptions, known as medical hallucinations, which pose serious risks in medical applications that demand accuracy and image-grounded outputs. Through empirical analysis, we find that prompt-induced hallucinations remain prevalent in radiology MLLMs, largely due to over-sensitivity to clinical sections. To address this, we introduce Clinical Contrastive Decoding (CCD), a training-free and retrieval-free inference framework that integrates structured clinical signals from task-specific radiology expert models. CCD introduces a dual-stage contrastive mechanism to refine token-level logits during generation, thereby enhancing clinical fidelity without modifying the base MLLM. Experiments on three datasets and multiple models demonstrate that CCD consistently improves overall performance on radiology report generation (RRG). On the MIMIC-CXR dataset, it yields up to a 17% improvement in RadGraph-F1 when applied to state-of-the-art RRG models. Our approach provides a lightweight and generalisable solution for mitigating medical hallucinations, effectively bridging expert models and MLLMs in radiology. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æœ€è¿‘é€šè¿‡æ•´åˆè§†è§‰æ„ŸçŸ¥ä¸è‡ªç„¶è¯­è¨€ç†è§£åœ¨æ”¾å°„å­¦é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬ç»å¸¸ç”Ÿæˆæ— æ³•å¾—åˆ°ä¸´åºŠæ”¯æŒçš„è¯Šæ–­æè¿°ï¼Œå³æ‰€è°“çš„åŒ»å­¦å¹»è§‰ï¼Œè¿™åœ¨éœ€è¦ç²¾ç¡®æ€§å’Œå›¾åƒåŸºç¡€è¾“å‡ºçš„åŒ»å­¦åº”ç”¨ä¸­æ„æˆäº†ä¸¥é‡é£é™©ã€‚é€šè¿‡å®è¯åˆ†æï¼Œæˆ‘ä»¬å‘ç°æç¤ºè¯±å¯¼çš„å¹»è§‰åœ¨æ”¾å°„å­¦MLLMä¸­ä»ç„¶æ™®éå­˜åœ¨ï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºå¯¹ä¸´åºŠéƒ¨åˆ†çš„è¿‡åº¦æ•æ„Ÿã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸´åºŠå¯¹æ¯”è§£ç ï¼ˆCCDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå’Œæ£€ç´¢çš„æ¨ç†æ¡†æ¶ï¼Œå®ƒæ•´åˆäº†ç‰¹å®šä»»åŠ¡æ”¾å°„å­¦ä¸“å®¶æ¨¡å‹çš„ç»“æ„åŒ–ä¸´åºŠä¿¡å·ã€‚CCDå¼•å…¥äº†ä¸€ç§åŒé˜¶æ®µå¯¹æ¯”æœºåˆ¶ï¼Œä»¥åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¼˜åŒ–ä»¤ç‰Œçº§åˆ«çš„é€»è¾‘ï¼Œä»è€Œæé«˜ä¸´åºŠä¿çœŸåº¦ï¼ŒåŒæ—¶æ— éœ€ä¿®æ”¹åŸºç¡€MLLMã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†å’Œå¤šä¸ªæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCCDåœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰æ–¹é¢å§‹ç»ˆæé«˜æ€»ä½“æ€§èƒ½ã€‚åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šï¼Œå½“åº”ç”¨äºæœ€å…ˆè¿›çš„RRGæ¨¡å‹æ—¶ï¼Œå®ƒåœ¨RadGraph-F1ä¸Šäº§ç”Ÿäº†é«˜è¾¾17%çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†ä¸€ç§è½»ä¾¿ä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºç¼“è§£åŒ»å­¦å¹»è§‰ï¼Œæœ‰æ•ˆåœ°åœ¨ä¸“å®¶æ¨¡å‹å’ŒMLLMä¹‹é—´æ¶èµ·æ¡¥æ¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23379v2">PDF</a> Preprint, 27 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ”¾å°„å­¦é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œå®ƒä»¬é€šè¿‡æ•´åˆè§†è§‰æ„ŸçŸ¥å’Œè‡ªç„¶è¯­è¨€ç†è§£å–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚ç„¶è€Œï¼ŒMLLMså¸¸å¸¸ç”Ÿæˆä¸å—ä¸´åºŠæ”¯æŒçš„æè¿°ï¼Œå³æ‰€è°“çš„åŒ»å­¦å¹»è§‰ï¼Œè¿™åœ¨éœ€è¦ç²¾ç¡®æ€§å’Œå›¾åƒåŸºç¡€è¾“å‡ºçš„åŒ»å­¦åº”ç”¨ä¸­å¸¦æ¥äº†ä¸¥é‡é£é™©ã€‚ç ”ç©¶å‘ç°ï¼Œæç¤ºè¯±å¯¼çš„å¹»è§‰åœ¨æ”¾å°„å­¦MLLMsä¸­æ™®éå­˜åœ¨ï¼Œä¸»è¦æ˜¯ç”±äºå¯¹ä¸´åºŠéƒ¨åˆ†çš„è¿‡åº¦æ•æ„Ÿã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Clinical Contrastive Decodingï¼ˆCCDï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå’Œæ£€ç´¢çš„æ¨ç†æ¡†æ¶ï¼Œå®ƒæ•´åˆäº†æ¥è‡ªç‰¹å®šä»»åŠ¡æ”¾å°„å­¦ä¸“å®¶æ¨¡å‹çš„ç»“æ„åŒ–ä¸´åºŠä¿¡å·ã€‚CCDé€šè¿‡å¼•å…¥åŒé˜¶æ®µå¯¹æ¯”æœºåˆ¶ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç²¾ç»†è°ƒæ•´ä»¤ç‰Œçº§åˆ«çš„é€»è¾‘ï¼Œæé«˜äº†ä¸´åºŠå‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¸ä¿®æ”¹åŸºç¡€MLLMã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCCDåœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸ŠæŒç»­æé«˜äº†æ•´ä½“æ€§èƒ½ã€‚åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šï¼Œåº”ç”¨äºæœ€å…ˆè¿›çš„RRGæ¨¡å‹æ—¶ï¼Œå®ƒåœ¨RadGraph-F1ä¸Šæé«˜äº†é«˜è¾¾17%ã€‚æœ¬æ–‡çš„æ–¹æ³•ä¸ºç¼“è§£åŒ»å­¦å¹»è§‰æä¾›äº†ä¸€ä¸ªè½»ä¾¿ä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰æ•ˆåœ°æ¶èµ·äº†ä¸“å®¶æ¨¡å‹å’ŒMLLMsä¹‹é—´çš„æ¡¥æ¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ”¾å°„å­¦é¢†åŸŸé›†æˆè§†è§‰æ„ŸçŸ¥å’Œè‡ªç„¶è¯­è¨€ç†è§£æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>MLLMså­˜åœ¨ç”Ÿæˆä¸´åºŠä¸æ”¯æŒæè¿°ï¼ˆåŒ»å­¦å¹»è§‰ï¼‰çš„é—®é¢˜ï¼Œè¿™åœ¨åŒ»å­¦åº”ç”¨ä¸­å¸¦æ¥é£é™©ã€‚</li>
<li>åŒ»å­¦å¹»è§‰åœ¨æ”¾å°„å­¦MLLMsä¸­æ™®éå­˜åœ¨ï¼Œä¸»è¦æ˜¯å› ä¸ºå¯¹ä¸´åºŠéƒ¨åˆ†çš„è¿‡åº¦æ•æ„Ÿã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†Clinical Contrastive Decodingï¼ˆCCDï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå’Œæ£€ç´¢çš„æ¨ç†æ¡†æ¶ã€‚</li>
<li>CCDæ•´åˆäº†æ¥è‡ªç‰¹å®šä»»åŠ¡æ”¾å°„å­¦ä¸“å®¶æ¨¡å‹çš„ç»“æ„åŒ–ä¸´åºŠä¿¡å·ï¼Œé€šè¿‡åŒé˜¶æ®µå¯¹æ¯”æœºåˆ¶æé«˜ä¸´åºŠå‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCCDåœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸ŠæŒç»­æé«˜äº†æ•´ä½“æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ab70949dd6dfe09e935b387f4e4e382" align="middle">
<img src="https://picx.zhimg.com/v2-3412e7b58bfccebaa5bc45f58e0bd04c" align="middle">
<img src="https://picx.zhimg.com/v2-0d2c1c62f194a8d3cd92f88de510f6d0" align="middle">
<img src="https://picx.zhimg.com/v2-93aa9c93345d9824f91974d4dc0cf83b" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Imperative-vs-Declarative-Programming-Paradigms-for-Open-Universe-Scene-Generation"><a href="#Imperative-vs-Declarative-Programming-Paradigms-for-Open-Universe-Scene-Generation" class="headerlink" title="Imperative vs. Declarative Programming Paradigms for Open-Universe Scene   Generation"></a>Imperative vs. Declarative Programming Paradigms for Open-Universe Scene   Generation</h2><p><strong>Authors:Maxim Gumin, Do Heon Han, Seung Jean Yoo, Aditya Ganeshan, R. Kenny Jones, Rio Aguina-Kang, Stewart Morris, Daniel Ritchie</strong></p>
<p>Current methods for generating 3D scene layouts from text predominantly follow a declarative paradigm, where a Large Language Model (LLM) specifies high-level constraints that are then resolved by a separate solver. This paper challenges that consensus by introducing a more direct, imperative approach. We task an LLM with generating a step-by-step program that iteratively places each object relative to those already in the scene. This paradigm simplifies the underlying scene specification language, enabling the creation of more complex, varied, and highly structured layouts that are difficult to express declaratively. To improve the robustness, we complement our method with a novel, LLM-free error correction mechanism that operates directly on the generated code, iteratively adjusting parameters within the program to resolve collisions and other inconsistencies. In forced-choice perceptual studies, human participants overwhelmingly preferred our imperative layouts, choosing them over those from two state-of-the-art declarative systems 82% and 94% of the time, demonstrating the significant potential of this alternative paradigm. Finally, we present a simple automated evaluation metric for 3D scene layout generation that correlates strongly with human judgment. </p>
<blockquote>
<p>å½“å‰é€šè¿‡æ–‡æœ¬ç”Ÿæˆ3Dåœºæ™¯å¸ƒå±€çš„æ–¹æ³•ä¸»è¦éµå¾ªå£°æ˜å¼èŒƒå¼ï¼Œå…¶ä¸­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŒ‡å®šé«˜çº§çº¦æŸï¼Œç„¶åç”±å•ç‹¬çš„æ±‚è§£å™¨è§£å†³ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥ä¸€ç§æ›´ç›´æ¥ã€å‘½ä»¤å¼çš„æ–¹æ³•ï¼Œå¯¹è¿™ä¸€å…±è¯†æå‡ºäº†æŒ‘æˆ˜ã€‚æˆ‘ä»¬è®©LLMç”Ÿæˆä¸€ä¸ªé€æ­¥çš„ç¨‹åºï¼Œè¯¥ç¨‹åºä¼šè¿­ä»£åœ°å°†æ¯ä¸ªå¯¹è±¡æ”¾ç½®åœ¨åœºæ™¯ä¸­å·²å­˜åœ¨çš„å¯¹è±¡æ—è¾¹ã€‚è¿™ç§èŒƒå¼ç®€åŒ–äº†åŸºæœ¬çš„åœºæ™¯è§„æ ¼è¯­è¨€ï¼Œèƒ½å¤Ÿå®ç°æ›´å¤æ‚ã€å¤šæ ·å’Œé«˜åº¦ç»“æ„çš„å¸ƒå±€ï¼Œè€Œè¿™äº›å¸ƒå±€éš¾ä»¥ç”¨å£°æ˜å¼è¡¨è¾¾ã€‚ä¸ºäº†æé«˜ç¨³å¥æ€§ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ç§æ–°å‹çš„æ— LLMé”™è¯¯æ ¡æ­£æœºåˆ¶æ¥è¡¥å……æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯¥æœºåˆ¶ç›´æ¥åœ¨ç”Ÿæˆçš„ä»£ç ä¸Šæ“ä½œï¼Œè¿­ä»£è°ƒæ•´ç¨‹åºä¸­çš„å‚æ•°ä»¥è§£å†³ç¢°æ’å’Œå…¶ä»–ä¸ä¸€è‡´æ€§ã€‚åœ¨å¼ºåˆ¶é€‰æ‹©æ„ŸçŸ¥ç ”ç©¶ä¸­ï¼Œäººç±»å‚ä¸è€…æ›´å€¾å‘äºé€‰æ‹©æˆ‘ä»¬çš„å‘½ä»¤å¼å¸ƒå±€ï¼Œè¶…è¿‡ä¸¤ç§æœ€å…ˆè¿›çš„å£°æ˜å¼ç³»ç»Ÿï¼ˆè¾¾åˆ°ï¼‰82%å’Œ94%çš„æ—¶é—´ï¼Œè¿™æ˜¾ç¤ºäº†è¿™ç§æ›¿ä»£èŒƒå¼çš„å·¨å¤§æ½œåŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬ä¸º3Dåœºæ™¯å¸ƒå±€ç”Ÿæˆæä¾›äº†ä¸€ä¸ªç®€å•çš„è‡ªåŠ¨åŒ–è¯„ä¼°æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡ä¸äººç±»åˆ¤æ–­é«˜åº¦ç›¸å…³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05482v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå‘½ä»¤å¼ç¼–ç¨‹èŒƒå¼ç”Ÿæˆä¸‰ç»´åœºæ™¯å¸ƒå±€çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆé€æ­¥çš„ç¨‹åºæ¥è¿­ä»£æ”¾ç½®åœºæ™¯ä¸­çš„æ¯ä¸ªå¯¹è±¡ã€‚è¿™ç§æ–¹æ³•ç®€åŒ–äº†åœºæ™¯æè¿°è¯­è¨€ï¼Œå¯ä»¥åˆ›å»ºå¤æ‚ã€å¤šæ ·ä¸”é«˜åº¦ç»“æ„åŒ–çš„å¸ƒå±€ï¼Œéš¾ä»¥ç”¨å£°æ˜å¼è¡¨è¾¾ã€‚ä¸ºæé«˜ç¨³å¥æ€§ï¼Œç»“åˆäº†æ— LLMçš„é”™è¯¯ä¿®æ­£æœºåˆ¶ï¼Œç›´æ¥åœ¨ç”Ÿæˆçš„ä»£ç ä¸Šæ“ä½œï¼Œè°ƒæ•´ç¨‹åºå‚æ•°ä»¥è§£å†³ç¢°æ’å’Œå…¶ä»–ä¸ä¸€è‡´æ€§ã€‚äººç±»å‚ä¸è€…çš„æ„ŸçŸ¥ç ”ç©¶è¡¨æ˜ï¼Œå¤§å¤šæ•°å‚ä¸è€…æ›´å€¾å‘äºæœ¬ç ”ç©¶çš„å‘½ä»¤å¼å¸ƒå±€ï¼Œè¶…è¿‡ä¸¤ç§å…ˆè¿›çš„å£°æ˜å¼ç³»ç»Ÿçš„åå¥½æ¯”ä¾‹åˆ†åˆ«ä¸º82%å’Œ94%ã€‚æœ€åï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç®€å•è‡ªåŠ¨åŒ–çš„ä¸‰ç»´åœºæ™¯å¸ƒå±€ç”Ÿæˆè¯„ä¼°æŒ‡æ ‡ï¼Œä¸äººç±»åˆ¤æ–­é«˜åº¦ç›¸å…³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºå‘½ä»¤å¼ç¼–ç¨‹çš„æ–°æ–¹æ³•ç”Ÿæˆä¸‰ç»´åœºæ™¯å¸ƒå±€ã€‚</li>
<li>ä½¿ç”¨LLMç”Ÿæˆé€æ­¥ç¨‹åºæ¥æ”¾ç½®åœºæ™¯ä¸­çš„å¯¹è±¡ã€‚</li>
<li>ç®€åŒ–äº†åœºæ™¯æè¿°è¯­è¨€ï¼Œæ”¯æŒåˆ›å»ºå¤æ‚ã€å¤šæ ·å’Œé«˜åº¦ç»“æ„åŒ–çš„å¸ƒå±€ã€‚</li>
<li>ç»“åˆäº†æ— LLMçš„é”™è¯¯ä¿®æ­£æœºåˆ¶ä»¥æé«˜ç”Ÿæˆçš„ç¨³å¥æ€§ã€‚</li>
<li>äººç±»æ„ŸçŸ¥ç ”ç©¶è¯æ˜å¤§å¤šæ•°å‚ä¸è€…åå¥½è¯¥å‘½ä»¤å¼å¸ƒå±€ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸å…ˆè¿›çš„å£°æ˜å¼ç³»ç»Ÿç›¸æ¯”å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05482">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-92a760237665c4fa94b5a7086a56fd9d" align="middle">
<img src="https://picx.zhimg.com/v2-58b0ef548eb4f03836c420537b006ff5" align="middle">
<img src="https://picx.zhimg.com/v2-cbf9c530f97651327acfd5883ff24f7c" align="middle">
<img src="https://picx.zhimg.com/v2-5f364aadf20db74af517b7d3a2f074f8" align="middle">
<img src="https://picx.zhimg.com/v2-121f81b9d2d3f2389b50bb9201272a0a" align="middle">
<img src="https://picx.zhimg.com/v2-0e622a38d1622c0ebeb8acaa74e13d9e" align="middle">
<img src="https://picx.zhimg.com/v2-3f1cd01e5ac1f8e65db0ee6bb933aa39" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-21/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-21/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-21/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-26c6eaa127c9ed921f6d2f29d2c58a52" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-21  VISTA A Test-Time Self-Improving Video Generation Agent
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-21/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2562c828eaffb0e0e3a13596ae7a73e4" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-21  PokeeResearch Effective Deep Research via Reinforcement Learning from   AI Feedback and Robust Reasoning Scaffold
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
