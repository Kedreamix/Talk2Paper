<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-05-17  AutoPentest Enhancing Vulnerability Management With Autonomous LLM   Agents">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d72c9b73aa31a85f264c24a2f8949036.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    36 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-17-更新"><a href="#2025-05-17-更新" class="headerlink" title="2025-05-17 更新"></a>2025-05-17 更新</h1><h2 id="AutoPentest-Enhancing-Vulnerability-Management-With-Autonomous-LLM-Agents"><a href="#AutoPentest-Enhancing-Vulnerability-Management-With-Autonomous-LLM-Agents" class="headerlink" title="AutoPentest: Enhancing Vulnerability Management With Autonomous LLM   Agents"></a>AutoPentest: Enhancing Vulnerability Management With Autonomous LLM   Agents</h2><p><strong>Authors:Julius Henke</strong></p>
<p>A recent area of increasing research is the use of Large Language Models (LLMs) in penetration testing, which promises to reduce costs and thus allow for higher frequency. We conduct a review of related work, identifying best practices and common evaluation issues. We then present AutoPentest, an application for performing black-box penetration tests with a high degree of autonomy. AutoPentest is based on the LLM GPT-4o from OpenAI and the LLM agent framework LangChain. It can perform complex multi-step tasks, augmented by external tools and knowledge bases. We conduct a study on three capture-the-flag style Hack The Box (HTB) machines, comparing our implementation AutoPentest with the baseline approach of manually using the ChatGPT-4o user interface. Both approaches are able to complete 15-25 % of the subtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT. We measure a total cost of $96.20 US when using AutoPentest across all experiments, while a one-month subscription to ChatGPT Plus costs $20. The results show that further implementation efforts and the use of more powerful LLMs released in the future are likely to make this a viable part of vulnerability management. </p>
<blockquote>
<p>近期研究越来越多的一个领域是利用大型语言模型（LLMs）进行渗透测试，这有望降低成本，从而提高测试频率。我们对相关工作进行了回顾，识别了最佳实践和常见的评估问题。然后，我们展示了AutoPentest，这是一个用于执行高度自治的黑盒渗透测试的应用程序。AutoPentest基于OpenAI的LLM GPT-4o和LLM代理框架LangChain。它可以执行复杂的多步骤任务，辅以外部工具和知识库。我们在三场捕获标志风格的Hack The Box（HTB）机器上进行了研究，将我们的AutoPentest实现与手动使用ChatGPT-4o用户界面的基准方法进行了比较。两种方法都能够在HTB机器上完成15-25%的子任务，AutoPentest略微优于ChatGPT。在使用AutoPentest进行所有实验时，总成本为96.20美元，而ChatGPT Plus的一个月订阅费用为20美元。结果表明，未来的进一步实施努力和使用更强大的LLMs很可能使这一技术成为漏洞管理的一个可行部分。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10321v1">PDF</a> 24 pages, 1 figure, for implementation, see   <a target="_blank" rel="noopener" href="https://github.com/JuliusHenke/autopentest">https://github.com/JuliusHenke/autopentest</a></p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在渗透测试中的应用日益受到研究关注，能够降低成本并允许更高频率的测试。本文回顾了相关工作，提出了自动渗透测试应用程序AutoPentest，基于OpenAI的GPT-4o和LangChain的LLM代理框架，能够执行复杂的多步骤任务，辅以外部工具和知识库。在Hack The Box（HTB）机器上的研究表明，AutoPentest略优于ChatGPT。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在渗透测试中的应用是最新研究热点。</li>
<li>LLMs可以降低渗透测试的成本，并提高测试频率。</li>
<li>AutoPentest是一个基于LLM的应用程序，用于执行黑盒渗透测试，具有高度自主性。</li>
<li>AutoPentest使用GPT-4o和LangChain框架，能执行复杂的多步骤任务。</li>
<li>在Hack The Box（HTB）机器上的研究表明，AutoPentest与ChatGPT相比略占优势。</li>
<li>使用AutoPentest的总成本为96.20美元，而ChatGPT Plus一个月订阅费用为20美元。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10321">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-69a8ffc61ad12c423e425013fd28daa3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53f71484294a14098025c983630b05b8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MASS-Multi-Agent-Simulation-Scaling-for-Portfolio-Construction"><a href="#MASS-Multi-Agent-Simulation-Scaling-for-Portfolio-Construction" class="headerlink" title="MASS: Multi-Agent Simulation Scaling for Portfolio Construction"></a>MASS: Multi-Agent Simulation Scaling for Portfolio Construction</h2><p><strong>Authors:Taian Guo, Haiyang Shen, Jinsheng Huang, Zhengyang Mao, Junyu Luo, Zhuoru Chen, Xuhui Liu, Bingyu Xia, Luchen Liu, Yun Ma, Ming Zhang</strong></p>
<p>LLM-based multi-agent has gained significant attention for their potential in simulation and enhancing performance. However, existing works are limited to pure simulations or are constrained by predefined workflows, restricting their applicability and effectiveness. In this paper, we introduce the Multi-Agent Scaling Simulation (MASS) for portfolio construction. MASS achieves stable and continuous excess returns by progressively increasing the number of agents for large-scale simulations to gain a superior understanding of the market and optimizing agent distribution end-to-end through a reverse optimization process, rather than relying on a fixed workflow. We demonstrate its superiority through performance experiments, ablation studies, backtesting experiments, experiments on updated data and stock pools, scaling experiments, parameter sensitivity experiments, and visualization experiments, conducted in comparison with 6 state-of-the-art baselines on 3 challenging A-share stock pools. We expect the paradigm established by MASS to expand to other tasks with similar characteristics. The implementation of MASS has been open-sourced at <a target="_blank" rel="noopener" href="https://github.com/gta0804/MASS">https://github.com/gta0804/MASS</a>. </p>
<blockquote>
<p>基于LLM的多智能体因其模拟和增强性能方面的潜力而受到广泛关注。然而，现有工作仅限于纯模拟或受预设工作流程的限制，这限制了其应用效果和效率。在本文中，我们介绍了用于投资组合构建的多智能体扩展模拟（MASS）。MASS通过逐步增加智能体的数量进行大规模模拟，实现对市场的深入理解，并通过反向优化过程端到端优化智能体的分布，而不是依赖于固定的流程，从而实现稳定且持续的超额回报。我们通过性能实验、消融研究、回测实验、更新数据和股票池的实验、规模扩展实验、参数敏感性实验和可视化实验，与6个最先进的基线在3个具有挑战性的A股股票池上进行了比较，证明了其优越性。我们希望MASS所建立的模式能够扩展到具有类似特征的其他任务中。MASS的实现已经开源，地址为<a target="_blank" rel="noopener" href="https://github.com/gta0804/MASS%E3%80%82">https://github.com/gta0804/MASS。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10278v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大规模基于LLM的多智能体仿真技术在投资组合构建领域受到广泛关注，具有模拟和优化性能潜力。然而，现有研究局限于纯仿真或受限于预设工作流程，限制了其应用性和效果。本文介绍了一种用于投资组合构建的多智能体扩展仿真（MASS）。MASS通过逐步增加智能体数量进行大规模仿真，实现对市场的深刻理解并优化反向优化过程实现端到端的智能体分布优化，从而带来稳定且持续的超额回报。其在A股股票池上的表现优于其他六种前沿基线模型，具有广泛的应用前景。具体实现已开源。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM-based multi-agent仿真技术在投资组合构建中得到应用。</li>
<li>目前方法学受限于纯仿真或预设工作流程。</li>
<li>多智能体扩展仿真（MASS）通过大规模仿真实现对市场的深入理解。</li>
<li>MASS通过逐步增加智能体数量和优化智能体分布来优化性能。</li>
<li>MASS在A股股票池上的表现优于其他前沿模型。</li>
<li>MASS实现了稳定且持续的超额回报。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10278">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f3ab59e33ccdaf3482bddb35bb24a4af.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Pre-Act-Multi-Step-Planning-and-Reasoning-Improves-Acting-in-LLM-Agents"><a href="#Pre-Act-Multi-Step-Planning-and-Reasoning-Improves-Acting-in-LLM-Agents" class="headerlink" title="Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents"></a>Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents</h2><p><strong>Authors:Mrinal Rawat, Ambuje Gupta, Rushil Goomer, Alessandro Di Bari, Neha Gupta, Roberto Pieraccini</strong></p>
<p>The ReAct (Reasoning + Action) capability in large language models (LLMs) has become the foundation of modern agentic systems. Recent LLMs, such as DeepSeek-R1 and OpenAI o1&#x2F;o3, exemplify this by emphasizing reasoning through the generation of ample intermediate tokens, which help build a strong premise before producing the final output tokens. In this paper, we introduce Pre-Act, a novel approach that enhances the agent’s performance by creating a multi-step execution plan along with the detailed reasoning for the given user input. This plan incrementally incorporates previous steps and tool outputs, refining itself after each step execution until the final response is obtained. Our approach is applicable to both conversational and non-conversational agents. To measure the performance of task-oriented agents comprehensively, we propose a two-level evaluation framework: (1) turn level and (2) end-to-end. Our turn-level evaluation, averaged across five models, shows that our approach, Pre-Act, outperforms ReAct by 70% in Action Recall on the Almita dataset. While this approach is effective for larger models, smaller models crucial for practical applications, where latency and cost are key constraints, often struggle with complex reasoning tasks required for agentic systems. To address this limitation, we fine-tune relatively small models such as Llama 3.1 (8B &amp; 70B) using the proposed Pre-Act approach. Our experiments show that the fine-tuned 70B model outperforms GPT-4, achieving a 69.5% improvement in action accuracy (turn-level) and a 28% improvement in goal completion rate (end-to-end) on the Almita (out-of-domain) dataset. </p>
<blockquote>
<p>大型语言模型（LLM）中的ReAct（推理+行动）能力已成为现代智能系统的基础。最近的LLM，如DeepSeek-R1和OpenAI o1&#x2F;o3，通过强调生成大量中间令牌进行推理，帮助构建强有力前提后再产生最终输出令牌，以此为例。在本文中，我们介绍了Pre-Act，这是一种通过创建多步执行计划与给定用户输入的详细推理来增强智能体性能的新型方法。此计划会逐步融入先前的步骤和工具输出，并在每一步执行后进行自我完善，直到获得最终响应。我们的方法适用于对话式和非对话式智能体。为了全面衡量面向任务的智能体的性能，我们提出了两级评估框架：（1）回合级和（2）端到端。我们的回合级评估，在五个模型上的平均结果显示，我们的Pre-Act方法在Almita数据集上的行动回忆率比ReAct高出70%。虽然此方法对于大型模型有效，但对于实际应用中至关重要的小型模型，在延迟和成本是主要约束的情况下，往往难以应对智能系统所需的复杂推理任务。为了解决这个问题，我们使用提出的Pre-Act方法对相对较小的模型（如Llama 3.1（8B和70B））进行微调。实验表明，经过调教的70B模型在Almita（离域）数据集上的行动准确性（回合级）提高了69.5%，目标完成率（端到端）提高了28%，表现优于GPT-4。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09970v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了Pre-Act方法，它提高了大型语言模型中智能主体的性能。该方法通过建立多步执行计划，并利用详细推理对用户输入进行回应，增强智能主体的任务执行能力。研究结果表明，Pre-Act方法在Almita数据集上的表现优于ReAct方法，特别是在动作记忆方面。此外，研究还解决了小型模型在处理复杂推理任务时的局限性，通过微调小型模型以适应Pre-Act方法，使其在Almita数据集上的表现优于GPT-4。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型的ReAct能力已逐渐成为现代智能主体的基础。</li>
<li>Pre-Act方法通过建立多步执行计划来提高智能主体的性能。</li>
<li>Pre-Act方法对用户的输入进行回应并详细推理。</li>
<li>Pre-Act方法在Almita数据集上的表现优于ReAct方法。</li>
<li>小型模型在处理复杂推理任务时面临局限性。</li>
<li>通过微调小型模型以适应Pre-Act方法，可以提高其在复杂任务上的表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09970">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7d77df5c2a52bc589754ae54e148c295.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63893c924334b37963f9e6892b9a6b82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-221391cd75da9a1bb3b84df46dbdbf1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59408a0eb1be87815d8c374ff40aaf6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6387a6d85e699995ffdf20f52df02a5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4071f8d15e00131770b36ecfecc34c1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Design-and-Evaluation-of-Generative-Agent-based-Platform-for-Human-Assistant-Interaction-Research-A-Tale-of-10-User-Studies"><a href="#Design-and-Evaluation-of-Generative-Agent-based-Platform-for-Human-Assistant-Interaction-Research-A-Tale-of-10-User-Studies" class="headerlink" title="Design and Evaluation of Generative Agent-based Platform for   Human-Assistant Interaction Research: A Tale of 10 User Studies"></a>Design and Evaluation of Generative Agent-based Platform for   Human-Assistant Interaction Research: A Tale of 10 User Studies</h2><p><strong>Authors:Ziyi Xuan, Yiwen Wu, Xuhai Xu, Vinod Namboodiri, Mooi Choo Chuah, Yu Yang</strong></p>
<p>Designing and evaluating personalized and proactive assistant agents remains challenging due to the time, cost, and ethical concerns associated with human-in-the-loop experimentation. Existing Human-Computer Interaction (HCI) methods often require extensive physical setup and human participation, which introduces privacy concerns and limits scalability. Simulated environments offer a partial solution but are typically constrained by rule-based scenarios and still depend heavily on human input to guide interactions and interpret results. Recent advances in large language models (LLMs) have introduced the possibility of generative agents that can simulate realistic human behavior, reasoning, and social dynamics. However, their effectiveness in modeling human-assistant interactions remains largely unexplored. To address this gap, we present a generative agent-based simulation platform designed to simulate human-assistant interactions. We identify ten prior studies on assistant agents that span different aspects of interaction design and replicate these studies using our simulation platform. Our results show that fully simulated experiments using generative agents can approximate key aspects of human-assistant interactions. Based on these simulations, we are able to replicate the core conclusions of the original studies. Our work provides a scalable and cost-effective approach for studying assistant agent design without requiring live human subjects. We will open source both the platform and collected results from the experiments on our website: <a target="_blank" rel="noopener" href="https://dash-gidea.github.io/">https://dash-gidea.github.io/</a>. </p>
<blockquote>
<p>设计并评估个性化和主动性的助理代理仍然充满挑战，这主要是因为涉及人类参与的实验存在时间、成本和道德上的担忧。现有的人机交互（HCI）方法通常需要大量的物理设置和人类参与，这引发了隐私担忧并限制了可扩展性。模拟环境提供了部分解决方案，但通常受限于基于规则的场景，并且仍然严重依赖于人类输入来指导交互和解释结果。最近大型语言模型（LLM）的进步为能够模拟真实人类行为、推理和社会动态的生成代理的可能性打开了大门。然而，它们在模拟人类助理交互方面的有效性在很大程度上尚未被探索。为了弥补这一差距，我们推出了一个基于生成代理的模拟平台，旨在模拟人类与助理的交互。我们确定了关于助理代理的十个早期研究，这些研究涵盖了不同方面的交互设计，并使用我们的模拟平台进行复制。结果表明，使用生成代理进行的完全模拟实验可以近似模拟人类与助理之间的关键交互方面。基于这些模拟，我们能够复制原始研究的核心结论。我们的工作提供了一种可扩展且经济实惠的研究助理代理设计的方法，而无需真实的受试者参与。我们将在我们的网站上开源平台和实验收集的结果：<a target="_blank" rel="noopener" href="https://dash-gidea.github.io/">网站链接</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09938v1">PDF</a> </p>
<p><strong>Summary</strong><br>模拟平台实现人机互动新突破<br>该模拟平台使用生成性代理人实现了对人机交互的全面仿真，使助理型智能体研究能够高效、低成本进行。成功还原十大现有互动设计研究的关键点，并通过仿真实验有效模拟真实的人机互动。未来该平台将开源并公开实验结果，推动助理型智能体设计的深入研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>设计并评估个性化、主动性助理代理是一项挑战，由于实验中存在时间成本以及伦理问题。传统的HCI方法需要复杂的物理设置和人类参与，引发隐私担忧并限制了可扩展性。</li>
<li>大型语言模型（LLMs）的发展为模拟真实人类行为、推理和社会动态提供了可能。然而，它们在模拟人机互动方面的效果尚未得到充分探索。</li>
<li>提出一种基于生成代理的模拟平台来模拟人机互动，成功复制了十个关于助理代理的研究案例。该平台可实现人机交互的全面仿真。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09938">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c96b2f6b6b97aa6209cdf022f1dedf47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67e8ab0ff48a9233f8ad927aa6397f09.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-Multimodal-Multi-Agent-Framework-for-Radiology-Report-Generation"><a href="#A-Multimodal-Multi-Agent-Framework-for-Radiology-Report-Generation" class="headerlink" title="A Multimodal Multi-Agent Framework for Radiology Report Generation"></a>A Multimodal Multi-Agent Framework for Radiology Report Generation</h2><p><strong>Authors:Ziruo Yi, Ting Xiao, Mark V. Albert</strong></p>
<p>Radiology report generation (RRG) aims to automatically produce diagnostic reports from medical images, with the potential to enhance clinical workflows and reduce radiologists’ workload. While recent approaches leveraging multimodal large language models (MLLMs) and retrieval-augmented generation (RAG) have achieved strong results, they continue to face challenges such as factual inconsistency, hallucination, and cross-modal misalignment. We propose a multimodal multi-agent framework for RRG that aligns with the stepwise clinical reasoning workflow, where task-specific agents handle retrieval, draft generation, visual analysis, refinement, and synthesis. Experimental results demonstrate that our approach outperforms a strong baseline in both automatic metrics and LLM-based evaluations, producing more accurate, structured, and interpretable reports. This work highlights the potential of clinically aligned multi-agent frameworks to support explainable and trustworthy clinical AI applications. </p>
<blockquote>
<p>放射学报告生成（RRG）旨在自动根据医学图像生成诊断报告，这有望改进临床工作流程并减少放射科医师的工作量。尽管最近采用多模态大型语言模型（MLLMs）和增强检索生成（RAG）的方法取得了显著成果，但它们仍然面临事实不一致、幻觉和跨模态不匹配等挑战。我们提出了一种用于RRG的多模态多智能体框架，该框架与分阶段的临床推理工作流程相一致，其中特定任务的智能体负责检索、初稿生成、视觉分析、改进和综合。实验结果表明，我们的方法在自动指标和基于LLM的评估中都优于强大的基线方法，能够生成更准确、结构化、可解释的报告。这项工作突出了临床对齐的多智能体框架在支持可解释和可信赖的临床人工智能应用方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09787v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了放射学报告生成（RRG）的目标，即通过自动产生医学图像诊断报告来优化临床工作流程并减轻放射科医生的工作量。尽管现有的利用多模态大型语言模型（MLLMs）和检索增强生成（RAG）的方法已经取得了显著成果，但它们仍然面临事实不一致、幻觉和跨模态不匹配等挑战。本研究提出了一种多模态多智能体框架，该框架与临床推理的逐步工作流程相一致，其中特定任务的智能体负责检索、初稿生成、视觉分析、细化和综合。实验结果表明，该方法在自动指标和基于LLM的评估上均优于强大基线，产生的报告更准确、结构化和可解释。这项工作突出了与临床相一致的多智能体框架在支持可解释和可信赖的临床人工智能应用方面的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>放射学报告生成（RRG）旨在自动产生医学图像诊断报告，优化临床流程，减轻医生工作量。</li>
<li>现有方法虽然利用多模态大型语言模型（MLLMs）和检索增强生成（RAG）取得显著成果，但仍存在挑战，如事实不一致、幻觉和跨模态不匹配。</li>
<li>提出了一种多模态多智能体框架，模拟临床推理的逐步流程，包含多个任务特定智能体。</li>
<li>智能体负责不同的任务，如检索、初稿生成、视觉分析、细化和综合。</li>
<li>实验表明，该方法在自动指标和基于LLM的评估上表现优越，生成的报告更准确、结构化、可解释。</li>
<li>该方法提高了报告的准确性，有助于临床决策的可解释性和信赖性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09787">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5b6fc76342914b6d3f2e5a03bf920408.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca15ad29655d9b29d56a023a9fecff9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8de5231a3e051a1613603936afb36a55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b152baca2a391df72352cdd08832569e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-516d917a52ff0dacfdcc049e2d983871.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Learning-Graph-Representation-of-Agent-Diffusers"><a href="#Learning-Graph-Representation-of-Agent-Diffusers" class="headerlink" title="Learning Graph Representation of Agent Diffusers"></a>Learning Graph Representation of Agent Diffusers</h2><p><strong>Authors:Youcef Djenouri, Nassim Belmecheri, Tomasz Michalak, Jan Dubiński, Ahmed Nabil Belbachir, Anis Yazidi</strong></p>
<p>Diffusion-based generative models have significantly advanced text-to-image synthesis, demonstrating impressive text comprehension and zero-shot generalization. These models refine images from random noise based on textual prompts, with initial reliance on text input shifting towards enhanced visual fidelity over time. This transition suggests that static model parameters might not optimally address the distinct phases of generation. We introduce LGR-AD (Learning Graph Representation of Agent Diffusers), a novel multi-agent system designed to improve adaptability in dynamic computer vision tasks. LGR-AD models the generation process as a distributed system of interacting agents, each representing an expert sub-model. These agents dynamically adapt to varying conditions and collaborate through a graph neural network that encodes their relationships and performance metrics. Our approach employs a coordination mechanism based on top-$k$ maximum spanning trees, optimizing the generation process. Each agent’s decision-making is guided by a meta-model that minimizes a novel loss function, balancing accuracy and diversity. Theoretical analysis and extensive empirical evaluations show that LGR-AD outperforms traditional diffusion models across various benchmarks, highlighting its potential for scalable and flexible solutions in complex image generation tasks. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/YousIA/LGR_AD">https://github.com/YousIA/LGR_AD</a> </p>
<blockquote>
<p>基于扩散的生成模型在文本到图像合成方面取得了重大进展，展示了令人印象深刻的文本理解和零样本泛化能力。这些模型根据文本提示从随机噪声中细化图像，最初依赖文本输入，随着时间的推移转向增强视觉保真度。这种转变表明，静态模型参数可能无法最佳地应对生成的不同阶段。我们引入了LGR-AD（学习代理扩散图表示），这是一个新型多代理系统，旨在提高动态计算机视觉任务的适应性。LGR-AD将生成过程建模为交互代理的分布式系统，每个代理代表一个专家子模型。这些代理动态适应各种条件，并通过编码其关系和性能指标的图神经网络进行协作。我们的方法采用基于top-k最大生成树的协调机制，优化生成过程。每个代理的决策制定由元模型引导，该模型最小化新型损失函数，平衡准确性和多样性。理论分析和广泛的实证评估表明，LGR-AD在各种基准测试中超越了传统扩散模型，突显其在复杂图像生成任务中可扩展和灵活解决方案的潜力。代码可在<a target="_blank" rel="noopener" href="https://github.com/YousIA/LGR_AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/YousIA/LGR_AD找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06761v2">PDF</a> Accepted at AAMAS2025 International Conference on Autonomous Agents   and Multiagent Systems</p>
<p><strong>Summary</strong><br>文本描述了基于扩散的生成模型在文本到图像合成方面的显著进展，并展示了其对文本理解和零样本泛化的能力。文章提出了一种新型多智能体系统LGR-AD，用于改进动态计算机视觉任务中的适应性。LGR-AD将生成过程建模为交互智能体的分布式系统，通过图神经网络进行协作和适应。理论分析和广泛的实证评估表明，LGR-AD在多个基准测试中优于传统扩散模型，在复杂图像生成任务中具有可扩展和灵活解决方案的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散生成模型在文本到图像合成中取得显著进展，展现出强大的文本理解和零样本泛化能力。</li>
<li>LGR-AD是一个新型多智能体系统，旨在提高动态计算机视觉任务的适应性。</li>
<li>LGR-AD将生成过程建模为交互智能体的分布式系统，智能体通过图神经网络进行协作和适应。</li>
<li>LGR-AD采用基于top-k最大生成树的协调机制来优化生成过程。</li>
<li>每个智能体的决策制定由一个元模型引导，该模型通过最小化新型损失函数来平衡准确性和多样性。</li>
<li>理论分析和广泛的实证评估表明，LGR-AD在多个基准测试中优于传统扩散模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06761">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-52afdcbeb8f1b9ac4eb2a4af2a93e715.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-428f003d5fa29766e837326999873613.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8cfdf1a0e8aed0dd8665d235d5738b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e1a3ecdffb9ece957bc67471b5e4da1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a55429115755f76db01b65fc7f316bbf.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SceneGenAgent-Precise-Industrial-Scene-Generation-with-Coding-Agent"><a href="#SceneGenAgent-Precise-Industrial-Scene-Generation-with-Coding-Agent" class="headerlink" title="SceneGenAgent: Precise Industrial Scene Generation with Coding Agent"></a>SceneGenAgent: Precise Industrial Scene Generation with Coding Agent</h2><p><strong>Authors:Xiao Xia, Dan Zhang, Zibo Liao, Zhenyu Hou, Tianrui Sun, Jing Li, Ling Fu, Yuxiao Dong</strong></p>
<p>The modeling of industrial scenes is essential for simulations in industrial manufacturing. While large language models (LLMs) have shown significant progress in generating general 3D scenes from textual descriptions, generating industrial scenes with LLMs poses a unique challenge due to their demand for precise measurements and positioning, requiring complex planning over spatial arrangement. To address this challenge, we introduce SceneGenAgent, an LLM-based agent for generating industrial scenes through C# code. SceneGenAgent ensures precise layout planning through a structured and calculable format, layout verification, and iterative refinement to meet the quantitative requirements of industrial scenarios. Experiment results demonstrate that LLMs powered by SceneGenAgent exceed their original performance, reaching up to 81.0% success rate in real-world industrial scene generation tasks and effectively meeting most scene generation requirements. To further enhance accessibility, we construct SceneInstruct, a dataset designed for fine-tuning open-source LLMs to integrate into SceneGenAgent. Experiments show that fine-tuning open-source LLMs on SceneInstruct yields significant performance improvements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/THUDM/SceneGenAgent">https://github.com/THUDM/SceneGenAgent</a> . </p>
<blockquote>
<p>工业场景的建模对于工业制造中的模拟至关重要。虽然大型语言模型（LLM）在根据文本描述生成一般的3D场景方面取得了显著进展，但使用LLM生成工业场景却构成了一项独特挑战，因为这需要精确的测量和定位，并要求对空间布局进行复杂规划。为了应对这一挑战，我们推出了SceneGenAgent，这是一个基于LLM的通过C#代码生成工业场景的代理。SceneGenAgent通过结构化和可计算格式、布局验证以及迭代优化，确保精确布局规划，以满足工业场景的定量要求。实验结果表明，由SceneGenAgent驱动的大型语言模型超出了其原始性能，在现实世界的工业场景生成任务中成功率高达8 结弦 绍儿你啥玩意% 并有效地满足大多数场景生成要求。为了进一步提高可及性，我们构建了SceneInstruct数据集，用于微调开源LLM以集成到SceneGenAgent中。实验表明，在SceneInstruct上微调开源LLM会带来显著的性能改进，Llama3.1-70B接近GPT-4o的能力。我们的代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/THUDM/SceneGenAgent%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/THUDM/SceneGenAgent找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21909v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于文本描述，工业场景的建模对于工业制造中的模拟至关重要。大型语言模型（LLMs）在生成通用3D场景方面已取得了显著进展，但在生成工业场景时面临精确测量和定位的需求，需要复杂的空间布局规划，构成了一大挑战。为应对这一挑战，我们推出了SceneGenAgent，这是一个基于LLM的代理，可通过C#代码生成工业场景。SceneGenAgent通过结构化、可计算格式、布局验证和迭代完善来确保精确布局规划，以满足工业场景的定量要求。实验结果证明，SceneGenAgent加持下的LLM性能超越原有水平，在现实工业场景生成任务中的成功率高达81.0%，并能有效满足大多数场景生成要求。为进一步提高可访问性，我们构建了SceneInstruct数据集，用于微调开源LLM以融入SceneGenAgent。实验表明，在SceneInstruct上微调开源LLM能带来显著的性能改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>工业场景的建模对模拟至关重要，需要精确测量和定位以及复杂的空间布局规划。</li>
<li>SceneGenAgent是一个基于LLM的代理，能够通过C#代码生成工业场景，确保精确布局规划。</li>
<li>SceneGenAgent结合了结构化格式、布局验证和迭代完善来满足工业场景的定量要求。</li>
<li>在现实工业场景生成任务中，SceneGenAgent加持下的LLM成功率高达81.0%。</li>
<li>SceneInstruct数据集用于微调开源LLM，以提高其在工业场景生成任务中的性能。</li>
<li>在SceneInstruct上微调的Llama3.1-70B模型接近GPT-4o的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21909">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d72c9b73aa31a85f264c24a2f8949036.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-627eef0e7a3732794dd28d8a4a5db50a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b839fc2203096ac75572c50df0b8facf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-894f6287bdacbcbc19a7219298033b98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5dcabc9e7d5e2016dfebbff14da52587.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AriGraph-Learning-Knowledge-Graph-World-Models-with-Episodic-Memory-for-LLM-Agents"><a href="#AriGraph-Learning-Knowledge-Graph-World-Models-with-Episodic-Memory-for-LLM-Agents" class="headerlink" title="AriGraph: Learning Knowledge Graph World Models with Episodic Memory for   LLM Agents"></a>AriGraph: Learning Knowledge Graph World Models with Episodic Memory for   LLM Agents</h2><p><strong>Authors:Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Andrey Kravchenko, Mikhail Burtsev, Evgeny Burnaev</strong></p>
<p>Advancements in the capabilities of Large Language Models (LLMs) have created a promising foundation for developing autonomous agents. With the right tools, these agents could learn to solve tasks in new environments by accumulating and updating their knowledge. Current LLM-based agents process past experiences using a full history of observations, summarization, retrieval augmentation. However, these unstructured memory representations do not facilitate the reasoning and planning essential for complex decision-making. In our study, we introduce AriGraph, a novel method wherein the agent constructs and updates a memory graph that integrates semantic and episodic memories while exploring the environment. We demonstrate that our Ariadne LLM agent, consisting of the proposed memory architecture augmented with planning and decision-making, effectively handles complex tasks within interactive text game environments difficult even for human players. Results show that our approach markedly outperforms other established memory methods and strong RL baselines in a range of problems of varying complexity. Additionally, AriGraph demonstrates competitive performance compared to dedicated knowledge graph-based methods in static multi-hop question-answering. </p>
<blockquote>
<p>随着大型语言模型（LLM）的能力不断提升，为开发自主智能体奠定了坚实的基础。使用合适的工具，这些智能体可以通过积累和更新知识来学习解决新环境中的任务。当前基于LLM的智能体利用观察的全历史记录、摘要、检索增强等方法来处理过去的经历。然而，这些非结构化的记忆表示不利于复杂的决策所需的推理和规划。在我们的研究中，我们引入了AriGraph，这是一种新方法，智能体在其中构建并更新一个记忆图，该图在探索环境时整合语义和情景记忆。我们证明，我们的Ariadne LLM智能体由提议的记忆架构辅以规划和决策构成，可以有效地处理交互式文本游戏环境中的复杂任务，这些任务甚至对人类玩家来说也是困难的。结果表明，我们的方法在各种不同复杂度的任务中显著优于其他现有的记忆方法和强大的强化学习基线。此外，AriGraph在静态多跳问答方面表现出与专用知识图谱方法相当的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.04363v3">PDF</a> Code for this work is avaliable at   <a target="_blank" rel="noopener" href="https://github.com/AIRI-Institute/AriGraph">https://github.com/AIRI-Institute/AriGraph</a></p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的进展为开发自主智能体提供了有前景的基础。借助适当工具，这些智能体能够通过积累并更新知识学习在新环境中完成任务。本研究引入了一种名为AriGraph的新方法，其中智能体在探索环境时构建并更新一个融合语义和情景记忆的记忆图。实验证明，配备规划决策功能的Ariadne LLM智能体在交互式文本游戏环境中成功完成复杂任务，即使对人类玩家来说也非常困难。相较于其他记忆方法和强化学习基线，该方法在多种不同复杂度的任务中表现优异。此外，在静态多跳问答场景中，AriGraph与专用知识图谱方法相比表现良好。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）为开发自主智能体提供了坚实的基础。</li>
<li>LLM智能体通过积累并更新知识，能在各种环境中学习完成任务。</li>
<li>AriGraph是一种新引入的方法，智能体在探索环境时构建和更新融合语义和情景记忆的记忆图。</li>
<li>Ariadne LLM智能体在交互式文本游戏环境中成功完成复杂任务，表现超越其他记忆方法和强化学习基线。</li>
<li>与其他记忆方法相比，AriGraph在多种任务中展现出显著优势。</li>
<li>AriGraph在静态多跳问答场景中的表现具有竞争力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.04363">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-09869381d8a0f86e759600b0bfd5d9be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b11017970b925f6b018ca3f98eb1373.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26904d256effb4e4a6de507ee9fec476.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3a6bb7e5a2d8109bae85e9e2944c43c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0b681c65825968ddb6764dd71f3dea2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-772538df2f6935a28847f90277169974.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Random-Walks-Performed-by-Topologically-Specific-Agents-on-Complex-Networks"><a href="#Random-Walks-Performed-by-Topologically-Specific-Agents-on-Complex-Networks" class="headerlink" title="Random Walks Performed by Topologically-Specific Agents on Complex   Networks"></a>Random Walks Performed by Topologically-Specific Agents on Complex   Networks</h2><p><strong>Authors:Alexandre Benatti, Luciano da F. Costa</strong></p>
<p>Random walks by single-node agents have been systematically conducted on various types of complex networks in order to investigate how their topologies can affect the dynamics of the agents. However, by fitting any network node, these agents do not engage in topological interactions with the network. In the present work, we describe random walks on complex networks performed by agents that are actually small graphs. These agents can only occupy admissible portions of the network onto which they fit topologically, hence their name being taken as topologically-specific agents. These agents are also allowed to move to adjacent subgraphs in the network, which have each node adjacent to a distinct original respective node of the agent. Given a network and a specific agent, it is possible to obtain a respective associated network, in which each node corresponds to a possible instance of the agent and the edges indicate adjacent positions. Associated networks are obtained and studied respectively to three types of topologically-specific agents (triangle, square, and slashed square) considering three types of complex networks (geometrical, Erd\H{o}s-R&#39;enyi, and Barab&#39;asi-Albert). Uniform random walks are also performed on these structures, as well as networks respectively obtained by removing the five nodes with the highest degree, and studied in terms of the number of covered nodes along the walks. Several results are reported and discussed, including the fact that substantially distinct associated networks can be obtained for each of the three considered agents and for varying average node degrees. Respectively to the coverage of the networks by uniform random walks, the square agent led to the most effective coverage of the nodes, followed by the triangle and slashed square agents. In addition, the geometric network turned out to be less effectively covered. </p>
<blockquote>
<p>针对各种类型的复杂网络，对单节点代理进行了系统的随机游走研究，以调查其拓扑结构如何影响代理的动力学。然而，通过匹配任何网络节点，这些代理并没有与网络进行拓扑交互。当前工作中，我们描述了由实际上是小型图的代理在复杂网络上进行的随机游走。这些代理只能占据与网络拓扑结构相匹配的可行部分，因此被命名为拓扑特定代理。这些代理也被允许移动到网络中的相邻子图，其中每个节点都与代理的原始不同节点相邻。给定一个网络和特定的代理，我们可以获得一个相关的网络，其中每个节点对应于代理的一个可能实例，边表示相邻位置。对于三种拓扑特定代理（三角形、正方形和斜线正方形）和三种复杂网络（几何网络、Erd\H{o}s-Rényi网络和Barab\Ási-Albert网络），分别获得了相关的网络并进行了研究。这些结构上以及通过移除五个度数最高的节点后得到的网络上执行了统一的随机游走，并根据游走过程中覆盖的节点数量进行了研究。报告并讨论了一些结果，包括对于每种考虑的代理和不同的平均节点度数，可以获得截然不同的相关网络。在统一随机游走对网络覆盖方面，正方形代理导致节点覆盖最有效，其次是三角形和斜线正方形代理。此外，几何网络的覆盖效果较差。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.00859v2">PDF</a> 21 pages, 15 figures</p>
<p><strong>Summary</strong><br>     拓扑特定型智能体在复杂网络上的随机游走研究。这些智能体能贴合网络拓扑结构，在非相邻子图中游走。三种类型的智能体（三角形、正方形、斜线型）在三种不同的复杂网络结构（几何型、Erd\H{o}s-Rényi型和Barabasi-Albert型）上的随机游走行为被研究，同时还研究了移除度数最高的五个节点后的网络中的随机游走情况。研究发现在不同的智能体和不同的平均节点度条件下，会得到非常不同的关联网络。在均匀随机游走中，正方形智能体的节点覆盖效果最佳，其次是三角形和斜线型智能体。几何网络的覆盖效果较差。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>拓扑特定型智能体可以在复杂网络的非相邻子图中进行随机游走，能贴合网络拓扑结构。</li>
<li>三种类型的智能体（三角形、正方形、斜线型）在三种不同的复杂网络结构上的随机游走行为被研究。</li>
<li>在移除度数最高的节点后的网络中，也进行了随机游走研究。</li>
<li>智能体和平均节点度的不同会导致得到的关联网络显著不同。</li>
<li>在均匀随机游走中，正方形智能体的节点覆盖效果最佳。</li>
<li>几何网络的节点覆盖效果较差。</li>
<li>智能体的拓扑结构对随机游走的动力学有显著影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.00859">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-926f53f114f1ce942c1bbce48e61ee31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e9eb31c0735bbeab4b03c905efd6900.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c1a3e2ac540607a9780f48c3e30c89a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d0f35b3c6bc46b5f2251f6da7909cb0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-17/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-17/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-17/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-565c903d4e24a68c0f082a56d5775e15.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-05-17  Logos as a Well-Tempered Pre-train for Sign Language Recognition
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-17/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3042f6d7bbcf02d66c340e5b2b696eb2.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-05-17  Beyond 'Aha!' Toward Systematic Meta-Abilities Alignment in Large   Reasoning Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25879.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
