<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-17  Computational screening and experimental validation of promising   Wadsley-Roth Niobates">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1544906ac0105b934677cb959a2af3b7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    61 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-17-æ›´æ–°"><a href="#2025-05-17-æ›´æ–°" class="headerlink" title="2025-05-17 æ›´æ–°"></a>2025-05-17 æ›´æ–°</h1><h2 id="Computational-screening-and-experimental-validation-of-promising-Wadsley-Roth-Niobates"><a href="#Computational-screening-and-experimental-validation-of-promising-Wadsley-Roth-Niobates" class="headerlink" title="Computational screening and experimental validation of promising   Wadsley-Roth Niobates"></a>Computational screening and experimental validation of promising   Wadsley-Roth Niobates</h2><p><strong>Authors:Zachary J. L. Bare, CJ Sturgill, Manish Kumar, Iva Milisavljevic, Hans-Conrad zur Loye, Scott Misture, Morgan Stefik, Christopher Sutton</strong></p>
<p>The growing demand for efficient, high-capacity energy storage systems has driven extensive research into advanced materials for lithium-ion batteries. Among the various candidates, Wadsley-Roth (WR) niobates have emerged as a promising class of materials for fast Li+ storage due to rapid ion diffusion within their ReO3-like blocks in combination with good electronic conductivity along the shear planes. Despite the remarkable features of WR phases, there are presently less than 30 known structures which limits identification of structure-property relationships for improved performance as well as the identification of phases with more earth-abundant elements. In this work, we have dramatically expanded the set of potentially (meta)stable compositions (with $\Delta$ Hd &lt; 22 meV&#x2F;atom) to 1301 (out of 3283) through high-throughput screening with density functional theory (DFT). This large space of compound was generated through single- and double-site substitution into 10 known WR-niobate prototypes using 48 elements across the periodic table. To confirm the structure predictions, we successfully synthesized and validated with X-ray diffraction a new material, MoWNb24O66. The measured lithium diffusivity in MoWNb24O66 has a peak value of 1.0x10-16 m2&#x2F;s at 1.45 V vs. Li&#x2F;Li+ and achieved 225 mAh&#x2F;g at 5C. Thus a computationally predicted phase was realized experimentally with performance exceeding Nb16W5O55, a recent WR benchmark. Overall, the computational dataset of potentially stable novel compounds and with one realized that has competitive performance provide a valuable guide for experimentalists in discovering new durable battery materials. </p>
<blockquote>
<p>éšç€å¯¹é«˜æ•ˆã€å¤§å®¹é‡èƒ½æºå­˜å‚¨ç³»ç»Ÿéœ€æ±‚çš„ä¸æ–­å¢é•¿ï¼Œé’ˆå¯¹é”‚ç¦»å­ç”µæ± å…ˆè¿›ææ–™çš„ç ”ç©¶å·²ç»å¹¿æ³›å¼€å±•ã€‚åœ¨å„ç§å€™é€‰ææ–™ä¸­ï¼ŒWadsley-Rothï¼ˆWRï¼‰é“Œé…¸ç›å› å…¶ReO3å‹å—å†…çš„å¿«é€Ÿç¦»å­æ‰©æ•£ä»¥åŠå‰ªåˆ‡å¹³é¢ä¸Šçš„è‰¯å¥½ç”µå­å¯¼ç”µæ€§ï¼Œæˆä¸ºå¿«é€ŸLi+å­˜å‚¨çš„æœ‰å‰é€”çš„ææ–™ã€‚å°½ç®¡WRç›¸å…·æœ‰å¼•äººæ³¨ç›®çš„ç‰¹å¾ï¼Œä½†ç›®å‰ä»…å·²çŸ¥ä¸åˆ°30ç§ç»“æ„ï¼Œè¿™é™åˆ¶äº†è¯†åˆ«æ”¹å–„æ€§èƒ½çš„ç»“æ„-å±æ€§å…³ç³»ä»¥åŠå…·æœ‰æ›´ä¸°å¯Œåœ°çƒå…ƒç´ çš„é˜¶æ®µçš„è¯†åˆ«ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡é«˜é€šé‡ç­›é€‰å¯†åº¦æ³›å‡½ç†è®ºï¼ˆDFTï¼‰ï¼Œå°†å¯èƒ½ï¼ˆå…ƒï¼‰ç¨³å®šçš„ç»„åˆç‰©ï¼ˆÎ”Hd &lt;22 meV&#x2F;atomï¼‰ä»3283ä¸ªæ˜¾è‘—æ‰©å±•è‡³1301ä¸ªã€‚è¿™ä¸ªå·¨å¤§çš„åŒ–åˆç‰©ç©ºé—´æ˜¯é€šè¿‡å¯¹10ç§å·²çŸ¥çš„WR-é“Œé…¸åŸå‹è¿›è¡Œå•ç‚¹å’ŒåŒç‚¹å–ä»£ï¼Œä½¿ç”¨å‘¨æœŸè¡¨ä¸­çš„48ä¸ªå…ƒç´ è€Œç”Ÿæˆçš„ã€‚ä¸ºäº†éªŒè¯ç»“æ„é¢„æµ‹ï¼Œæˆ‘ä»¬æˆåŠŸåˆæˆå¹¶é€šè¿‡Xå°„çº¿è¡å°„éªŒè¯äº†ä¸€ç§æ–°ææ–™MoWNb24O66ã€‚åœ¨MoWNb24O66ä¸­æµ‹é‡çš„é”‚æ‰©æ•£ç‡åœ¨1.45Vï¼ˆç›¸å¯¹äºLi&#x2F;Li+ï¼‰æ—¶è¾¾åˆ°å³°å€¼1.0x10-16 m2&#x2F;sï¼Œå¹¶åœ¨5Cæ—¶è¾¾åˆ°225 mAh&#x2F;gã€‚å› æ­¤ï¼Œå®éªŒä¸Šå®ç°äº†ä¸€ç§è®¡ç®—é¢„æµ‹çš„é˜¶æ®µï¼Œå…¶æ€§èƒ½è¶…è¿‡äº†æœ€è¿‘çš„WRåŸºå‡†Nb16W5O55ã€‚æ€»ä½“è€Œè¨€ï¼Œå…·æœ‰ç«äº‰åŠ›æ€§èƒ½çš„æ½œåœ¨ç¨³å®šæ–°å‹åŒ–åˆç‰©çš„è®¡ç®—æ•°æ®é›†ä»¥åŠä¸€ç§å·²å®ç°çš„ææ–™ï¼Œä¸ºå®éªŒè€…å‘ç°æ–°çš„æŒä¹…ç”µæ± ææ–™æä¾›äº†å®è´µçš„æŒ‡å—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10549v1">PDF</a> </p>
<p><strong>Summary</strong><br>    WRå‹é“Œé…¸ç›ææ–™åœ¨é”‚ç¦»å­ç”µæ± çš„å¿«é€Ÿå‘å±•ä¸­å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œå…¶å¿«é€Ÿçš„ç¦»å­æ‰©æ•£å’Œè‰¯å¥½çš„ç”µå­å¯¼ç”µæ€§ä½¿å…¶æˆä¸ºå¿«é€Ÿå­˜å‚¨Li+çš„ç†æƒ³é€‰æ‹©ã€‚ç„¶è€Œï¼Œå·²çŸ¥çš„ç»“æ„æ•°é‡æœ‰é™ï¼ˆä¸åˆ°30ç§ï¼‰ï¼Œé™åˆ¶äº†æ€§èƒ½ä¸ç»“æ„å…³ç³»çš„ç ”ç©¶ä»¥åŠæ›´ä¸°å¯Œå…ƒç´ ç›¸çš„å‘ç°ã€‚æœ¬ç ”ç©¶é€šè¿‡é«˜é€šé‡ç­›é€‰å’Œå¯†åº¦æ³›å‡½ç†è®ºï¼ˆDFTï¼‰è®¡ç®—ï¼Œå¤§å¹…æ‰©å±•äº†æ½œåœ¨ç¨³å®šåŒ–åˆç‰©èŒƒå›´ï¼Œä»å·²çŸ¥çš„WRå‹é“Œé…¸åŸå‹ä¸­é€šè¿‡å•åŒä½ç‚¹å–ä»£å¾—åˆ°1301ç§åŒ–åˆç‰©ï¼ˆå…±3283ç§ï¼‰ã€‚åˆæˆå¹¶éªŒè¯äº†ä¸€ç§æ–°ææ–™MoWNb24O66ï¼Œå…¶é”‚ç¦»å­æ‰©æ•£é€Ÿç‡å³°å€¼è¾¾åˆ°1.0x10-16 m2&#x2F;sï¼Œæ”¾ç”µç”µå‹ä¸º1.45 Vï¼Œå®¹é‡è¾¾åˆ°225 mAh&#x2F;gï¼Œæ€§èƒ½è¶…è¶Šç°æœ‰WRå‹åŸºå‡†ææ–™Nb16W5O55ã€‚è¯¥è®¡ç®—æ•°æ®é›†ä¸ºå®éªŒå‘ç°æ–°å‹æŒä¹…ç”µæ± ææ–™æä¾›äº†å®è´µæŒ‡å—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Wadsley-Rothï¼ˆWRï¼‰å‹é“Œé…¸ç›ææ–™åœ¨é”‚ç¦»å­ç”µæ± é¢†åŸŸå…·æœ‰å¿«é€Ÿç¦»å­æ‰©æ•£å’Œç”µå­å¯¼ç”µæ€§çš„æ½œåŠ›ã€‚</li>
<li>ç›®å‰å·²çŸ¥WRå‹ç»“æ„æ•°é‡æœ‰é™ï¼ˆå°‘äº30ç§ï¼‰ï¼Œé™åˆ¶äº†æ€§èƒ½æå‡å’Œæ–°ææ–™å‘ç°çš„æ½œåŠ›ã€‚</li>
<li>é€šè¿‡é«˜é€šé‡ç­›é€‰å’Œå¯†åº¦æ³›å‡½ç†è®ºï¼ˆDFTï¼‰è®¡ç®—ï¼Œå¤§å¹…æ‰©å±•äº†æ½œåœ¨ç¨³å®šåŒ–åˆç‰©çš„èŒƒå›´ã€‚</li>
<li>æˆåŠŸåˆæˆå¹¶éªŒè¯äº†æ–°ææ–™MoWNb24O66ï¼Œå…·æœ‰å‡ºè‰²çš„é”‚ç¦»å­æ‰©æ•£æ€§èƒ½å’Œç”µæ± å®¹é‡ã€‚</li>
<li>MoWNb24O66çš„é”‚ç¦»å­æ‰©æ•£é€Ÿç‡å³°å€¼é«˜ï¼Œä¸”æ”¾ç”µç”µå‹å’Œå®¹é‡è¡¨ç°ä¼˜ç§€ï¼Œæ€§èƒ½è¶…è¶Šç°æœ‰åŸºå‡†ææ–™ã€‚</li>
<li>è®¡ç®—æ•°æ®é›†ä¸ºå®éªŒå‘ç°æ–°å‹æŒä¹…ç”µæ± ææ–™æä¾›äº†æœ‰ä»·å€¼çš„æŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10549">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c7d1a1a79d1742592f3a031263419a1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-278216c56c8a1443bcb92fd09978dcda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afda03ae84ff03409ec5a9b757d3e106.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="WeGA-Weakly-Supervised-Global-Local-Affinity-Learning-Framework-for-Lymph-Node-Metastasis-Prediction-in-Rectal-Cancer"><a href="#WeGA-Weakly-Supervised-Global-Local-Affinity-Learning-Framework-for-Lymph-Node-Metastasis-Prediction-in-Rectal-Cancer" class="headerlink" title="WeGA: Weakly-Supervised Global-Local Affinity Learning Framework for   Lymph Node Metastasis Prediction in Rectal Cancer"></a>WeGA: Weakly-Supervised Global-Local Affinity Learning Framework for   Lymph Node Metastasis Prediction in Rectal Cancer</h2><p><strong>Authors:Yifan Gao, Yaoxian Dong, Wenbin Wu, Chaoyang Ge, Feng Yuan, Jiaxi Sheng, Haoyue Li, Xin Gao</strong></p>
<p>Accurate lymph node metastasis (LNM) assessment in rectal cancer is essential for treatment planning, yet current MRI-based evaluation shows unsatisfactory accuracy, leading to suboptimal clinical decisions. Developing automated systems also faces significant obstacles, primarily the lack of node-level annotations. Previous methods treat lymph nodes as isolated entities rather than as an interconnected system, overlooking valuable spatial and contextual information. To solve this problem, we present WeGA, a novel weakly-supervised global-local affinity learning framework that addresses these challenges through three key innovations: 1) a dual-branch architecture with DINOv2 backbone for global context and residual encoder for local node details; 2) a global-local affinity extractor that aligns features across scales through cross-attention fusion; and 3) a regional affinity loss that enforces structural coherence between classification maps and anatomical regions. Experiments across one internal and two external test centers demonstrate that WeGA outperforms existing methods, achieving AUCs of 0.750, 0.822, and 0.802 respectively. By effectively modeling the relationships between individual lymph nodes and their collective context, WeGA provides a more accurate and generalizable approach for lymph node metastasis prediction, potentially enhancing diagnostic precision and treatment selection for rectal cancer patients. </p>
<blockquote>
<p>å¯¹ç›´è‚ ç™Œæ·‹å·´ç»“è½¬ç§»ï¼ˆLNMï¼‰çš„å‡†ç¡®è¯„ä¼°æ˜¯æ²»ç–—è®¡åˆ’çš„å…³é”®ï¼Œä½†ç›®å‰çš„MRIè¯„ä¼°å‡†ç¡®æ€§ä¸ä½³ï¼Œå¯èƒ½å¯¼è‡´ä¸´åºŠå†³ç­–å¤±è¯¯ã€‚è‡ªåŠ¨ç³»ç»Ÿçš„å¼€å‘ä¹Ÿé¢ä¸´é‡å¤§éšœç¢ï¼Œä¸»è¦æ˜¯ç¼ºä¹èŠ‚ç‚¹çº§åˆ«çš„æ³¨é‡Šã€‚ä»¥å¾€çš„æ–¹æ³•å°†æ·‹å·´ç»“è§†ä¸ºå­¤ç«‹çš„å®ä½“ï¼Œè€Œéäº’è”ç³»ç»Ÿï¼Œä»è€Œå¿½ç•¥äº†å®è´µçš„ç©ºé—´å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†WeGAï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¼±ç›‘ç£å…¨å±€-å±€éƒ¨äº²å’ŒåŠ›å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ä¸‰é¡¹å…³é”®åˆ›æ–°è§£å†³è¿™äº›æŒ‘æˆ˜ï¼š1ï¼‰å…·æœ‰DINOv2ä¸»å¹²å’Œæ®‹å·®ç¼–ç å™¨åˆ†æ”¯çš„ Dual-Branch æ¶æ„ï¼Œç”¨äºå…¨å±€ä¸Šä¸‹æ–‡å’Œå±€éƒ¨èŠ‚ç‚¹ç»†èŠ‚ï¼›2ï¼‰å…¨å±€-å±€éƒ¨äº²å’ŒåŠ›æå–å™¨ï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›èåˆæ¥è·¨å°ºåº¦å¯¹é½ç‰¹å¾ï¼›3ï¼‰åŒºåŸŸäº²å’ŒåŠ›æŸå¤±ï¼Œåœ¨åˆ†ç±»å›¾å’Œè§£å‰–åŒºåŸŸä¹‹é—´å¼ºåˆ¶æ‰§è¡Œç»“æ„ä¸€è‡´æ€§ã€‚åœ¨ä¸€ä¸ªå†…éƒ¨å’Œä¸¤ä¸ªå¤–éƒ¨æµ‹è¯•ä¸­å¿ƒçš„å®éªŒè¡¨æ˜ï¼ŒWeGAä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåˆ†åˆ«å®ç°äº†AUCå€¼ä¸º0.750ã€0.822å’Œ0.802ã€‚é€šè¿‡æœ‰æ•ˆåœ°å»ºæ¨¡å•ä¸ªæ·‹å·´ç»“ä¹‹é—´çš„å…³ç³»åŠå…¶é›†ä½“ä¸Šä¸‹æ–‡ï¼ŒWeGAä¸ºæ·‹å·´ç»“è½¬ç§»é¢„æµ‹æä¾›äº†æ›´å‡†ç¡®å’Œå¯æ¨å¹¿çš„æ–¹æ³•ï¼Œå¯èƒ½æé«˜ç›´è‚ ç™Œæ‚£è€…çš„è¯Šæ–­ç²¾åº¦å’Œæ²»ç–—é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10502v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„å¼±ç›‘ç£å…¨å±€å±€éƒ¨äº²å’ŒåŠ›å­¦ä¹ æ¡†æ¶WeGAï¼Œç”¨äºå‡†ç¡®è¯„ä¼°ç›´è‚ ç™Œæ·‹å·´èŠ‚ç‚¹è½¬ç§»æƒ…å†µã€‚è¯¥æ–¹æ³•é€šè¿‡é‡‡ç”¨åŒåˆ†æ”¯æ¶æ„ã€å…¨å±€å±€éƒ¨äº²å’ŒåŠ›æå–å™¨å’ŒåŒºåŸŸäº²å’ŒåŠ›æŸå¤±ï¼Œæœ‰æ•ˆè§£å†³äº†å½“å‰MRIè¯„ä¼°åŠè‡ªåŠ¨åŒ–ç³»ç»Ÿä¸­çš„é—®é¢˜ã€‚å®éªŒè¯æ˜ï¼ŒWeGAåœ¨æ·‹å·´èŠ‚ç‚¹è½¬ç§»é¢„æµ‹æ–¹é¢çš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºç›´è‚ ç™Œæ‚£è€…çš„è¯Šæ–­å’Œæ²»ç–—çš„ç²¾ç¡®æ€§æä¾›äº†æé«˜çš„å¯èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WeGAæ˜¯ä¸€ä¸ªæ–°å‹çš„å¼±ç›‘ç£å…¨å±€å±€éƒ¨äº²å’ŒåŠ›å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ç›´è‚ ç™Œæ·‹å·´èŠ‚ç‚¹è½¬ç§»çš„è¯„ä¼°å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡é‡‡ç”¨åŒåˆ†æ”¯æ¶æ„ï¼Œç»“åˆå…¨å±€ä¸Šä¸‹æ–‡å’Œå±€éƒ¨èŠ‚ç‚¹ç»†èŠ‚ä¿¡æ¯ã€‚</li>
<li>WeGAåˆ©ç”¨å…¨å±€å±€éƒ¨äº²å’ŒåŠ›æå–å™¨ï¼Œé€šè¿‡è·¨å°ºåº¦æ³¨æ„åŠ›èåˆï¼Œå¯¹é½ç‰¹å¾ã€‚</li>
<li>åŒºåŸŸäº²å’ŒåŠ›æŸå¤±æ¨¡å‹çš„å¼•å…¥ï¼Œå¢å¼ºäº†åˆ†ç±»å›¾ä¸è§£å‰–åŒºåŸŸä¹‹é—´çš„ç»“æ„ä¸€è‡´æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒWeGAåœ¨å†…éƒ¨å’Œå¤–éƒ¨æµ‹è¯•ä¸­å¿ƒå‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œå®ç°äº†è¾ƒé«˜çš„AUCå€¼ã€‚</li>
<li>WeGAé€šè¿‡å»ºæ¨¡å•ä¸ªæ·‹å·´èŠ‚ç‚¹ä¸å…¶é›†ä½“ä¸Šä¸‹æ–‡ä¹‹é—´çš„å…³ç³»ï¼Œæä¾›äº†ä¸€ç§æ›´å‡†ç¡®ã€æ›´é€šç”¨çš„æ·‹å·´èŠ‚ç‚¹è½¬ç§»é¢„æµ‹æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10502">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-18dad527325eeb3c6a908a6e4c51ffbb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0c3cfcfcb8371f4fb35016db2f0b63e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea13d034f63c5c07fb9783cb4ff421b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d3c72e94119894a999f3f1371bf7b17.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="HWA-UNETR-Hierarchical-Window-Aggregate-UNETR-for-3D-Multimodal-Gastric-Lesion-Segmentation"><a href="#HWA-UNETR-Hierarchical-Window-Aggregate-UNETR-for-3D-Multimodal-Gastric-Lesion-Segmentation" class="headerlink" title="HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric   Lesion Segmentation"></a>HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric   Lesion Segmentation</h2><p><strong>Authors:Jiaming Liang, Lihuan Dai, Xiaoqi Sheng, Xiangguang Chen, Chun Yao, Guihua Tao, Qibin Leng, Honming Cai, Xi Zhong</strong></p>
<p>Multimodal medical image segmentation faces significant challenges in the context of gastric cancer lesion analysis. This clinical context is defined by the scarcity of independent multimodal datasets and the imperative to amalgamate inherently misaligned modalities. As a result, algorithms are constrained to train on approximate data and depend on application migration, leading to substantial resource expenditure and a potential decline in analysis accuracy. To address those challenges, we have made two major contributions: First, we publicly disseminate the GCM 2025 dataset, which serves as the first large-scale, open-source collection of gastric cancer multimodal MRI scans, featuring professionally annotated FS-T2W, CE-T1W, and ADC images from 500 patients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework that employs an original HWA block with learnable window aggregation layers to establish dynamic feature correspondences between different modalitiesâ€™ anatomical structures, and leverages the innovative tri-orientated fusion mamba mechanism for context modeling and capturing long-range spatial dependencies. Extensive experiments on our GCM 2025 dataset and the publicly BraTS 2021 dataset validate the performance of our framework, demonstrating that the new approach surpasses existing methods by up to 1.68% in the Dice score while maintaining solid robustness. The dataset and code are public via <a target="_blank" rel="noopener" href="https://github.com/JeMing-creater/HWA-UNETR">https://github.com/JeMing-creater/HWA-UNETR</a>. </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œèƒƒç™Œç—…ç¶åˆ†æé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚è¿™ä¸€ä¸´åºŠç¯å¢ƒçš„ç‰¹ç‚¹æ˜¯ç¼ºä¹ç‹¬ç«‹çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œå¹¶ä¸”éœ€è¦å°†æœ¬è´¨ä¸Šä¸å¯¹é½çš„æ¨¡å¼èåˆåœ¨ä¸€èµ·ã€‚å› æ­¤ï¼Œç®—æ³•å—é™äºåœ¨è¿‘ä¼¼æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¾èµ–äºåº”ç”¨è¿ç§»ï¼Œå¯¼è‡´èµ„æºæ¶ˆè€—å·¨å¤§ï¼Œåˆ†æç²¾åº¦å¯èƒ½ä¸‹é™ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åšå‡ºäº†ä¸¤å¤§è´¡çŒ®ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å…¬å¼€ä¼ æ’­äº†GCM 2025æ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡çš„ã€å¼€æºçš„èƒƒç™Œå¤šæ¨¡æ€MRIæ‰«æé›†åˆï¼ŒåŒ…å«æ¥è‡ª500åæ‚£è€…çš„ä¸“ä¸šæ³¨é‡ŠFS-T2Wã€CE-T1Wå’ŒADCå›¾åƒã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†HWA-UNETRï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„3Dåˆ†å‰²æ¡†æ¶ï¼Œé‡‡ç”¨åŸå§‹HWAå—å’Œå¯å­¦ä¹ çš„çª—å£èšåˆå±‚æ¥å»ºç«‹ä¸åŒæ¨¡æ€è§£å‰–ç»“æ„ä¹‹é—´çš„åŠ¨æ€ç‰¹å¾å¯¹åº”å…³ç³»ï¼Œå¹¶åˆ©ç”¨åˆ›æ–°çš„Tri-orientedèåˆMambaæœºåˆ¶è¿›è¡Œä¸Šä¸‹æ–‡å»ºæ¨¡å’Œæ•æ‰é•¿è·ç¦»ç©ºé—´ä¾èµ–æ€§ã€‚åœ¨æˆ‘ä»¬çš„GCM 2025æ•°æ®é›†å’Œå…¬å¼€çš„BraTS 2021æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶æ€§èƒ½ï¼Œè¯æ˜æ–°æ–¹æ³•åœ¨Diceå¾—åˆ†ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œæœ€é«˜æé«˜äº†1.68%ï¼ŒåŒæ—¶ä¿æŒäº†ç¨³å¥æ€§ã€‚æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/JeMing-creater/HWA-UNETR%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/JeMing-creater/HWA-UNETRå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10464v1">PDF</a> This work has been provisionally accepted for MICCAI 2025</p>
<p><strong>æ‘˜è¦</strong><br>    èƒƒç™Œç—…ç¶åˆ†æçš„å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç”±äºç¼ºå°‘ç‹¬ç«‹å¤šæ¨¡æ€æ•°æ®é›†å’Œå¿…é¡»èåˆå›ºæœ‰é”™ä½æ¨¡æ€çš„å¿…è¦æ€§ï¼Œç®—æ³•å—åˆ°è®­ç»ƒæ•°æ®è¿‘ä¼¼æ€§çš„é™åˆ¶ï¼Œå¹¶ä¾èµ–äºåº”ç”¨è¿ç§»ï¼Œå¯¼è‡´èµ„æºæ¶ˆè€—å¤§ä¸”åˆ†æç²¾åº¦å¯èƒ½ä¸‹é™ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åšå‡ºäº†ä¸¤é¡¹é‡å¤§è´¡çŒ®ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å…¬å¼€å‘å¸ƒäº†GCM 2025æ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡çš„å…¬å¼€èƒƒç™Œå¤šæ¨¡æ€MRIæ‰«ææ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª500åæ‚£è€…çš„ä¸“ä¸šæ³¨é‡ŠFS-T2Wã€CE-T1Wå’ŒADCå›¾åƒï¼›å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†HWA-UNETRï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„3Dåˆ†å‰²æ¡†æ¶ï¼Œé‡‡ç”¨å¯å­¦ä¹ çš„çª—å£èšåˆå±‚å»ºç«‹ä¸åŒæ¨¡æ€è§£å‰–ç»“æ„ä¹‹é—´çš„åŠ¨æ€ç‰¹å¾å¯¹åº”å…³ç³»ï¼Œå¹¶åˆ©ç”¨åˆ›æ–°çš„ä¸‰å‘èåˆmambaæœºåˆ¶è¿›è¡Œä¸Šä¸‹æ–‡å»ºæ¨¡å’Œæ•è·è¿œç¨‹ç©ºé—´ä¾èµ–æ€§ã€‚åœ¨æˆ‘ä»¬çš„GCM 2025æ•°æ®é›†å’Œå…¬å¼€çš„BraTS 2021æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†è¯¥æ¡†æ¶çš„æ€§èƒ½ï¼Œè¯æ˜æ–°æ–¹æ³•åœ¨Diceå¾—åˆ†ä¸Šæœ€å¤šæé«˜äº†1.68%ï¼ŒåŒæ—¶ä¿æŒäº†ç¨³å¥æ€§ã€‚æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/JeMing-creater/HWA-UNETR%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/JeMing-creater/HWA-UNETRå…¬å¼€è®¿é—®ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨èƒƒç™Œç—…ç¶åˆ†æä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºç¼ºå°‘ç‹¬ç«‹å¤šæ¨¡æ€æ•°æ®é›†å’Œæ¨¡æ€å¯¹é½çš„éš¾åº¦ã€‚</li>
<li>æå‡ºäº†GCM 2025æ•°æ®é›†ï¼Œä¸ºèƒƒç™Œå¤šæ¨¡æ€MRIæ‰«ææä¾›äº†é¦–ä¸ªå¤§è§„æ¨¡ã€å…¬å¼€çš„æ•°æ®é›†ã€‚</li>
<li>ä»‹ç»äº†HWA-UNETRæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨ä¸åŒæ¨¡æ€çš„è§£å‰–ç»“æ„ä¹‹é—´å»ºç«‹åŠ¨æ€ç‰¹å¾å¯¹åº”å…³ç³»ã€‚</li>
<li>HWA-UNETRåˆ©ç”¨å¯å­¦ä¹ çš„çª—å£èšåˆå±‚å’Œåˆ›æ–°çš„tri-orientatedèåˆmambaæœºåˆ¶è¿›è¡Œä¸Šä¸‹æ–‡å»ºæ¨¡å’Œç©ºé—´ä¾èµ–æ€§æ•è·ã€‚</li>
<li>åœ¨GCM 2025å’ŒBraTS 2021æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHWA-UNETRæ¡†æ¶çš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒDiceå¾—åˆ†æœ‰æ‰€æé«˜ã€‚</li>
<li>è¯¥æ¡†æ¶çš„ç¨³å¥æ€§å¾—åˆ°äº†éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e447cccae47f5f2ba09a8c0d43b07076.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85a41fcd5f13056645a134422a164310.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1044cfd5bc27eab37db31e98e9f8ceb7.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="First-Results-on-the-Search-for-Lepton-Number-Violating-Neutrinoless-Double-Beta-Decay-with-the-LEGEND-200-Experiment"><a href="#First-Results-on-the-Search-for-Lepton-Number-Violating-Neutrinoless-Double-Beta-Decay-with-the-LEGEND-200-Experiment" class="headerlink" title="First Results on the Search for Lepton Number Violating Neutrinoless   Double Beta Decay with the LEGEND-200 Experiment"></a>First Results on the Search for Lepton Number Violating Neutrinoless   Double Beta Decay with the LEGEND-200 Experiment</h2><p><strong>Authors:H. Acharya, N. Ackermann, M. Agostini, A. Alexander, C. Andreoiu, G. R. Araujo, F. T. Avignone III, M. Babicz, W. Bae, A. Bakalyarov, M. Balata, A. S. Barabash, P. S. Barbeau, C. J. Barton, L. Baudis, C. Bauer, E. Bernieri, L. Bezrukov, K. H. Bhimani, V. Biancacci, E. Blalock, S. J. Borden, G. Borghi, F. Borra, B. Bos, A. Boston, V. Bothe, R. Bouabid, R. Brugnera, N. Burlac, M. Busch, S. Calgaro, L. Canonica, S. Capra, M. Carminati, R. M. D. Carney, C. Cattadori, R. Cesarano, Y. -D. Chan, J. R. Chapman, A. Chernogorov, P. -J. Chiu, C. D. Christofferson, M. L. Clark, A. I. Colon-Rivera, T. Comellato, V. Dâ€™Andrea, R. Deckert, J. A. Detwiler, A. Di Giacinto, N. Di Marco, T. Dixon, K. -M. Dong, A. Drobizhev, G. Duran, Yu. Efremenko, S. R. Elliott, C. H. J. Emmanuel, E. Engelhardt, E. Esch, M. T. Febbraro, F. Ferella, D. E. Fields, C. Fiorini, M. Fomina, N. Fuad, R. Gala, A. Galindo-Uribarri, A. Gangapshev, A. Garfagnini, S. Gazzana, A. Geraci, L. Gessler, C. Ghiano, A. Gieb, S. Giri, M. Gold, C. Gooch, G. GrÃ¼nauer, M. P. Green, J. Gruszko, I. Guinn, V. E. Guiseppe, V. Gurentsov, Y. Gurov, K. Gusev, B. Hackett, F. Hagemann, M. Haranczyk, F. Henkes, R. Henning, J. Herrera, D. Hervas Aguilar, J. Hinton, R. HodÃ¡k, H. F. R. Hoffmann, M. A. Howe, M. Huber, M. Hult, A. Ianni, K. JÄ™drzejczak, J. Jochum, R. W. L. Jones, D. S. Judson, M. Junker, J. Kaizer, V. Kazalov, M. F. Kidd, T. Kihm, K. Kilgus, A. Klimenko, K. T. KnÃ¶pfle, I. Kochanek, O. Kochetov, I. Kontul, L. L. Kormos, V. N. Kornoukhov, P. Krause, H. Krishnamoorthy, V. V. Kuzminov, K. Lang, M. Laubenstein, N. N. P. N. Lay, E. LeÃ³n, A. Leder, B. Lehnert, A. Leonhardt, N. Levashko, L. Y. Li, A. Li, Y. -R. Lin, M. Lindner, I. Lippi, A. Love, A. Lubashevskiy, B. Lubsandorzhiev, N. Lusardi, C. Macolino, B. Majorovits, F. Mamedov, L. Manzanillas, G. G. Marshall, R. D. Martin, E. L. Martin, R. Massarczyk, A. Mazumdar, G. McDowell, D. -M. Mei, S. P. Meireles, M. Menzel, S. Mertens, E. Miller, I. Mirza, M. Misiaszek, M. Morella, B. Morgan, T. Mroz, D. Muenstermann, C. J. Nave, I. Nemchenok, M. Neuberger, N. Oâ€™Briant, F. Paissan, L. Papp, L. S. Paudel, K. Pelczar, L. Pertoldi, W. Pettus, F. Piastra, M. Pichotta, P. Piseri, A. W. P. Poon, P. P. Povinec, M. Pruckner, A. Pullia, W. S. Quinn, D. C. Radford, Y. A. Ramachers, A. Razeto, M. Redchuk, A. L. Reine, S. Riboldi, K. Rielage, C. Romo-Luque, N. Rossi, S. Rozov, T. J. Ruland, N. Rumyantseva, J. Runge, R. Saakyan, S. Sailer, G. Salamanna, F. Salamida, G. Saleh, V. Sandukovsky, C. Savarese, S. SchÃ¶nert, A. -K. SchÃ¼tz, D. C. Schaper, L. SchlÃ¼ter, S. J. Schleich, O. Schulz, M. Schwarz, B. Schwingenheuer, C. Seibt, O. Selivanenko, G. Senatore, A. Serafini, K. Shakhov, E. Shevchik, M. Shirchenko, Y. Shitov, H. Simgen, F. Å imkovic, S. Simonaitis-Boyd, M. Skorokhvatov, M. SlavÃ­ÄkovÃ¡, A. Smolnikov, J. A. Solomon, G. Song, A. C. Sousa, A. R. Sreekala, L. Steinhart, I. Å tekl, T. Sterr, M. Stommel, S. A. Sullivan, R. R. Sumathi, K. Szczepaniec, L. Taffarello, D. Tagnani, D. J. Tedeschi, T. N. Thorpe, V. Tretyak, M. Turqueti, E. E. Van Nieuwenhuizen, L. J. Varriano, S. Vasilyev, A. Veresnikova, C. Vignoli, C. Vogl, K. von Sturm, A. Warren, D. Waters, S. L. Watkins, C. Wiesinger, J. F. Wilkerson, M. Willers, C. Wiseman, M. Wojcik, D. Xu, W. Xu, E. Yakushev, T. Ye, C. -H. Yu, V. Yumatov, D. Zinatulina, K. Zuber, G. Zuzel</strong></p>
<p>The LEGEND collaboration is searching for neutrinoless double beta ($0\nu\beta\beta$) decay by operating high-purity germanium detectors enriched in $^{76}$Ge in a low-background liquid argon environment. Building on key technological innovations from GERDA and the MAJORANA DEMONSTRATOR, LEGEND-200 has performed a first $0\nu\beta\beta$ decay search based on 61 kg yr of data. Over half of this exposure comes from our highest performing detectors, including newly developed inverted-coaxial detectors, and is characterized by an estimated background level of $0.5^{+0.3}<em>{-0.2}$ cts&#x2F;(keV kg yr) in the $0\nu\beta\beta$ decay signal region. A combined analysis of data from GERDA, the MAJORANA DEMONSTRATOR, and LEGEND-200, characterized by a 90% confidence level exclusion sensitivity of $2.8 \times 10^{26}$ yr on the half-life of $0\nu\beta\beta$ decay, reveals no evidence for a signal and sets a new observed lower limit at $T^{0\nu}</em>{1&#x2F;2} &gt; 1.9 \times 10^{26}$ yr (90% confidence level). Assuming the decay is mediated by Majorana neutrinos, this corresponds to an upper limit on the effective Majorana mass in the range $m_{\beta\beta} &lt; 70-200$ meV, depending on the adopted nuclear matrix element. </p>
<blockquote>
<p>ä¼ å¥‡ï¼ˆLEGENDï¼‰åˆä½œç»„æ­£åœ¨ä½èƒŒæ™¯æ¶²æ°©ç¯å¢ƒä¸­è¿è¡Œé«˜åº¦å¯Œé›†çš„^{76}Geé«˜çº¯åº¦é”—æ¢æµ‹å™¨ï¼Œä»¥å¯»æ‰¾æ— ä¸­å¾®å­åŒÎ²ï¼ˆ0Î½Î²Î²ï¼‰è¡°å˜ã€‚ä¼ å¥‡-200åŸºäºGERDAå’ŒMAJORANAæ¼”ç¤ºè€…ï¼ˆDEMONSTRATORï¼‰çš„å…³é”®æŠ€æœ¯åˆ›æ–°ï¼Œä½¿ç”¨æˆ‘ä»¬çš„é«˜æ€§èƒ½æ¢æµ‹å™¨ï¼ˆåŒ…æ‹¬æ–°å¼€å‘çš„å€’ç½®åŒè½´æ¢æµ‹å™¨ï¼‰è¿›è¡Œäº†é¦–æ¬¡åŸºäº61å…¬æ–¤å¹´æ•°æ®çš„æ— ä¸­å¾®å­åŒÎ²è¡°å˜æœç´¢ã€‚è¯¥æ›å…‰é‡çš„ä¸€åŠä»¥ä¸Šæ¥è‡ªæˆ‘ä»¬çš„é«˜æ€§èƒ½æ¢æµ‹å™¨ï¼Œåœ¨ä¼°è®¡çš„èƒŒæ™¯æ°´å¹³ä¸º$ 0Î½Î²Î²è¡°å˜ä¿¡å·åŒºåŸŸçš„æ¯åƒå…‹å¹´æ•°æ®é‡0.5_{- 0.2}^{+ 0.3}$cts&#x2F;ï¼ˆkev kgÂ·å¹´ï¼‰æ—¶ã€‚é€šè¿‡å¯¹GERDAã€MAJORANAæ¼”ç¤ºå™¨å’Œä¼ å¥‡-200çš„æ•°æ®è¿›è¡Œç»¼åˆåˆ†æï¼Œæ˜¾ç¤ºå‡ºæ²¡æœ‰è¯æ®è¡¨æ˜å­˜åœ¨ä¿¡å·ï¼Œå¹¶ä¸”åœ¨æ— ä¸­å¾®å­åŒÎ²è¡°å˜çš„åŠè¡°æœŸè®¾å®šäº†æ–°çš„è§‚å¯Ÿåˆ°çš„ä¸‹é™ä¸ºT^{0Î½}<em>{åŠè¡°æœŸ} &gt; å‡è®¾è¡°å˜æ˜¯ç”±å¤§è´¨é‡é©¬çº¦æ‹‰çº³ä¸­å¾®å­ä»‹å¯¼çš„ï¼Œè¿™å¯¹åº”äºæœ‰æ•ˆçš„é©¬çº¦æ‹‰çº³ä¸­å¾®å­åœ¨åº”ç”¨ä¸­åŸå­æ ¸çŸ©é˜µå…ƒç´ çš„æœ‰æ•ˆèŒƒå›´è´¨é‡é™åˆ¶åœ¨&lt;ä¸Šé™èŒƒå›´å†…åœ¨ç¯å¢ƒæœ‰æ•ˆæ€§ä»¥å†…æ—¶ä¸ºãŠèŒƒå›´ä¸ºèŒƒå›´ä¸ºm</em>{Î²Î²}&lt; 70-200 meVã€‚è¿™å–å†³äºæ‰€é‡‡ç”¨çš„æ ¸çŸ©é˜µå…ƒç´ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10440v1">PDF</a> Prepared for submission to Physical Review Letters</p>
<p><strong>Summary</strong><br>    LEGENDåˆä½œåˆ©ç”¨é«˜çº¯åº¦é”—æ¢æµ‹å™¨æœç´¢æ— ä¸­å¾®å­åŒÎ²è¡°å˜ï¼ˆ$0Î½Î²Î²$ï¼‰ï¼Œæ¢æµ‹å™¨ä»¥$^{76}$Geæµ“ç¼©ç‰©ä¸ºå·¥ä½œä»‹è´¨ï¼Œåœ¨ä½æœ¬åº•æ¶²æ°©ç¯å¢ƒä¸­æ“ä½œã€‚åŸºäºGERDAå’ŒMAJORANA DEMONSTRATORçš„å…³é”®æŠ€æœ¯åˆ›æ–°ï¼ŒLEGEND-200åˆ©ç”¨é‡‡é›†çš„ä¸ºæœŸä¸€å¹´çš„61å…¬æ–¤æ•°æ®è¿›è¡Œäº†é¦–æ¬¡$0Î½Î²Î²$è¡°å˜æœç´¢ã€‚ç ”ç©¶é‡‡ç”¨äº†æ–°å‹å€’ç½®åŒè½´æ¢æµ‹å™¨ç­‰é«˜æ€§èƒ½æ¢æµ‹å™¨ï¼Œå¹¶ä¼°è®¡èƒŒæ™¯æ°´å¹³ä¸º$0Î½Î²Î²$è¡°å˜ä¿¡å·åŒºåŸŸçš„$0.5^{+0.3}<em>{-0.2}$ cts&#x2F;(kgå¹´)ã€‚ç»¼åˆåˆ†æGERDAã€MAJORANA DEMONSTRATORå’ŒLEGEND-200çš„æ•°æ®ï¼Œæ²¡æœ‰å‘ç°è¡°å˜ä¿¡å·ï¼Œè®¾å®šäº†æ–°çš„è§‚æµ‹ä¸‹é™$T^{0Î½}</em>{1&#x2F;2} &gt; 1.9 Ã— 10^{26}$ å¹´ã€‚åœ¨å‡è®¾è¡°å˜ç”±Majoranaä¸­å¾®å­ä»‹å¯¼çš„å‰æä¸‹ï¼Œå¯¹åº”çš„æœ‰æ•ˆMajoranaè´¨é‡ä¸Šé™å–å†³äºæ‰€é‡‡ç”¨çš„æ ¸çŸ©é˜µå…ƒç´ ï¼ŒèŒƒå›´ä¸º$m_{Î²Î²} &lt; 70-200$ meVã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LEGENDåˆä½œä½¿ç”¨é«˜çº¯åº¦é”—æ¢æµ‹å™¨æœç´¢æ— ä¸­å¾®å­åŒÎ²è¡°å˜ã€‚</li>
<li>æ“ä½œç¯å¢ƒä¸ºä½æœ¬åº•æ¶²æ°©ï¼Œå¹¶é‡‡ç”¨äº†åˆ›æ–°æŠ€æœ¯ã€‚</li>
<li>åŸºäº61å…¬æ–¤å¹´çš„æ•°æ®è¿›è¡Œäº†é¦–æ¬¡æœç´¢ã€‚</li>
<li>é«˜æ€§èƒ½æ¢æµ‹å™¨å¦‚æ–°å‹å€’ç½®åŒè½´æ¢æµ‹å™¨è¢«åº”ç”¨ã€‚</li>
<li>èƒŒæ™¯æ°´å¹³ä¼°è®¡åœ¨$0Î½Î²Î²$è¡°å˜ä¿¡å·åŒºåŸŸä¸º$0.5^{+0.3}_{-0.2}$ cts&#x2F;(kgå¹´)ã€‚</li>
<li>ç»¼åˆåˆ†æä¸‰ä¸ªå®éªŒæ•°æ®ï¼Œæœªå‘ç°è¡°å˜ä¿¡å·ï¼Œè®¾å®šäº†æ–°çš„è§‚æµ‹ä¸‹é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10440">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-58427d98a99e5a70fca98192984421ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1b8a4464efd4aa69e14e6f91b4f472f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d520490409dca86aa7ac78aa69d76a23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2aba70ffc52ee5d213902788601ee910.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="On-the-Interplay-of-Human-AI-Alignment-Fairness-and-Performance-Trade-offs-in-Medical-Imaging"><a href="#On-the-Interplay-of-Human-AI-Alignment-Fairness-and-Performance-Trade-offs-in-Medical-Imaging" class="headerlink" title="On the Interplay of Human-AI Alignment,Fairness, and Performance   Trade-offs in Medical Imaging"></a>On the Interplay of Human-AI Alignment,Fairness, and Performance   Trade-offs in Medical Imaging</h2><p><strong>Authors:Haozhe Luo, Ziyu Zhou, Zixin Shu, AurÃ©lie Pahud de Mortanges, Robert Berke, Mauricio Reyes</strong></p>
<p>Deep neural networks excel in medical imaging but remain prone to biases, leading to fairness gaps across demographic groups. We provide the first systematic exploration of Human-AI alignment and fairness in this domain. Our results show that incorporating human insights consistently reduces fairness gaps and enhances out-of-domain generalization, though excessive alignment can introduce performance trade-offs, emphasizing the need for calibrated strategies. These findings highlight Human-AI alignment as a promising approach for developing fair, robust, and generalizable medical AI systems, striking a balance between expert guidance and automated efficiency. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Roypic/Aligner">https://github.com/Roypic/Aligner</a>. </p>
<blockquote>
<p>æ·±åº¦ç¥ç»ç½‘ç»œåœ¨åŒ»å­¦æˆåƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä»æ˜“å—åˆ°åè§çš„å½±å“ï¼Œä»è€Œåœ¨ä¸åŒäººç¾¤ä¹‹é—´äº§ç”Ÿå…¬å¹³æ€§é—®é¢˜ã€‚æˆ‘ä»¬é¦–æ¬¡ç³»ç»Ÿåœ°æ¢ç´¢äº†äººç±»ä¸äººå·¥æ™ºèƒ½åœ¨è¯¥é¢†åŸŸçš„å¯¹é½ä¸å…¬å¹³æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œèå…¥äººç±»è§è§£å¯ä»¥æŒç»­å‡å°‘å…¬å¹³å·®è·å¹¶å¢å¼ºè·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œä½†è¿‡åº¦å¯¹é½å¯èƒ½ä¼šå¼•å…¥æ€§èƒ½æƒè¡¡ï¼Œå¼ºè°ƒæ ¡å‡†ç­–ç•¥çš„å¿…è¦æ€§ã€‚è¿™äº›å‘ç°çªæ˜¾äº†äººç±»ä¸äººå·¥æ™ºèƒ½çš„å¯¹é½ä½œä¸ºä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œç”¨äºå¼€å‘å…¬å¹³ã€ç¨³å¥å’Œå¯æ³›åŒ–çš„åŒ»å­¦äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œåœ¨ä¸“å®¶æŒ‡å¯¼å’Œè‡ªåŠ¨åŒ–æ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨Roypic&#x2F;Alignerä¸Šè·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10231v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦é¢†åŸŸä¸­ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œåœ¨åŒ»å­¦å½±åƒä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†ä»å­˜åœ¨åè§é—®é¢˜ï¼Œé€ æˆä¸åŒäººç¾¤é—´çš„å…¬å¹³æ€§é—®é¢˜ã€‚æœ¬ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿæ€§æ¢ç´¢äº†äººå·¥æ™ºèƒ½ä¸äººç±»å¯¹é½åŠè¯¥é¢†åŸŸçš„å…¬å¹³æ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œèå…¥äººç±»è§è§£æœ‰åŠ©äºç¼©å°å…¬å¹³å·®è·å¹¶æå‡è·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œä½†è¿‡åº¦å¯¹é½å¯èƒ½å¼•å‘æ€§èƒ½æƒè¡¡ï¼Œéœ€é‡‡ç”¨æ ¡å‡†ç­–ç•¥ã€‚ç ”ç©¶å¼ºè°ƒï¼Œäººç±»ä¸äººå·¥æ™ºèƒ½çš„å¯¹é½æ˜¯å¼€å‘å…¬å¹³ã€ç¨³å¥ã€æ³›åŒ–çš„åŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æœ‰æ•ˆé€”å¾„ï¼Œå®ç°äº†ä¸“å®¶æŒ‡å¯¼ä¸è‡ªåŠ¨åŒ–æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ç¥ç»ç½‘ç»œåœ¨åŒ»å­¦æˆåƒä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å­˜åœ¨åè§é—®é¢˜ï¼Œå½±å“ä¸åŒäººç¾¤é—´çš„å…¬å¹³æ€§ã€‚</li>
<li>äººç±»ä¸äººå·¥æ™ºèƒ½çš„å¯¹é½æœ‰åŠ©äºç¼©å°å…¬å¹³å·®è·ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>èå…¥äººç±»è§è§£å¯ä»¥æ”¹è¿›æ¨¡å‹çš„æ€§èƒ½ï¼Œä½¿å…¶æ›´åŠ ç¨³å¥å’Œå¯é ã€‚</li>
<li>è¿‡åº¦çš„äººç±»ä¸äººå·¥æ™ºèƒ½å¯¹é½å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸Šçš„æƒè¡¡ï¼Œéœ€è¦é‡‡å–æ ¡å‡†ç­–ç•¥æ¥å¹³è¡¡æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>Human-AIå¯¹é½æ˜¯å¼€å‘å…¬å¹³ã€ç¨³å¥å’Œæ³›åŒ–çš„åŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡ä¸“å®¶æŒ‡å¯¼ä¸è‡ªåŠ¨åŒ–æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ï¼Œå¯ä»¥æé«˜åŒ»ç–—AIç³»ç»Ÿçš„æ€§èƒ½å’Œå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10231">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-092bec8cb13230ab1b343ab18fedb55d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7d65223b69daacc49c76d47775228faf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ab2d8811f3123050266f2ab3d4b7a4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16da20369e0cf7aef664f853e0b05cb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8813edfc4500b1f5abad80f985eb9654.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddb9ae10ecd889fbdb2794a412c06530.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="IMITATE-Image-Registration-with-Context-for-unknown-time-frame-recovery"><a href="#IMITATE-Image-Registration-with-Context-for-unknown-time-frame-recovery" class="headerlink" title="IMITATE: Image Registration with Context for unknown time frame recovery"></a>IMITATE: Image Registration with Context for unknown time frame recovery</h2><p><strong>Authors:Ziad Kheil, Lucas Robinet, Laurent Risser, Soleakhena Ken</strong></p>
<p>In this paper, we formulate a novel image registration formalism dedicated to the estimation of unknown condition-related images, based on two or more known images and their associated conditions. We show how to practically model this formalism by using a new conditional U-Net architecture, which fully takes into account the conditional information and does not need any fixed image. Our formalism is then applied to image moving tumors for radiotherapy treatment at different breathing amplitude using 4D-CT (3D+t) scans in thoracoabdominal regions. This driving application is particularly complex as it requires to stitch a collection of sequential 2D slices into several 3D volumes at different organ positions. Movement interpolation with standard methods then generates well known reconstruction artefacts in the assembled volumes due to irregular patient breathing, hysteresis and poor correlation of breathing signal to internal motion. Results obtained on 4D-CT clinical data showcase artefact-free volumes achieved through real-time latencies. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Kheil-Z/IMITATE">https://github.com/Kheil-Z/IMITATE</a> . </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹å›¾åƒé…å‡†æ–¹æ³•ï¼Œä¸“é—¨ç”¨äºä¼°è®¡ä¸æ¡ä»¶ç›¸å…³çš„æœªçŸ¥å›¾åƒï¼Œè¯¥æ–¹æ³•åŸºäºä¸¤ä¸ªæˆ–å¤šä¸ªå·²çŸ¥å›¾åƒåŠå…¶ç›¸å…³æ¡ä»¶ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨æ–°çš„æ¡ä»¶U-Netæ¶æ„æ¥å®é™…å»ºæ¨¡è¿™ç§æ–¹æ³•ï¼Œè¯¥æ¶æ„å……åˆ†è€ƒè™‘äº†æ¡ä»¶ä¿¡æ¯ï¼Œå¹¶ä¸”ä¸éœ€è¦ä»»ä½•å›ºå®šå›¾åƒã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¯¥æ–¹æ³•åº”ç”¨äºåœ¨æ”¾å°„æ²»ç–—ä¸­ä½¿ç”¨ä¸åŒå‘¼å¸å¹…åº¦çš„å›¾åƒç§»åŠ¨è‚¿ç˜¤ç ”ç©¶ã€‚ç”±äºéœ€è¦å¯¹å¤šä¸ªä¸åŒå™¨å®˜ä½ç½®çš„åºåˆ—2Dåˆ‡ç‰‡è¿›è¡Œæ‹¼åˆæˆå¤šä¸ª3Dä½“ç§¯ï¼Œå› æ­¤è¿™é¡¹åº”ç”¨ç‰¹åˆ«å¤æ‚ã€‚ä½¿ç”¨æ ‡å‡†æ–¹æ³•è¿›è¡Œè¿åŠ¨æ’å€¼ä¼šåœ¨ç»„åˆä½“ç§¯ä¸­äº§ç”Ÿä¼—æ‰€å‘¨çŸ¥çš„é‡å»ºä¼ªå½±ï¼Œè¿™æ˜¯ç”±äºæ‚£è€…å‘¼å¸ä¸è§„åˆ™ã€æ»åä»¥åŠå‘¼å¸ä¿¡å·ä¸å†…éƒ¨è¿åŠ¨ä¹‹é—´çš„ç›¸å…³æ€§è¾ƒå·®æ‰€å¯¼è‡´çš„ã€‚åœ¨å››ç»´CTä¸´åºŠæ•°æ®ä¸Šè·å¾—çš„ç»“æœå±•ç¤ºäº†é€šè¿‡å®æ—¶å»¶è¿Ÿå®ç°çš„ä¼ªå½±è‡ªç”±ä½“ç§¯ã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/Kheil-Z/IMITATE%E4%B8%8A%E5%8F%AF%E4%BE%9B%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/Kheil-Z/IMITATEä¸Šå¯ä¾›ä½¿ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10124v1">PDF</a> IEEE ISBI 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé…å‡†ç ”ç©¶ï¼Œæå‡ºä¸€ç§æ–°å‹å›¾åƒé…å‡†å½¢å¼ï¼Œåˆ©ç”¨å·²çŸ¥å›¾åƒä¼°è®¡æœªçŸ¥æ¡ä»¶å›¾åƒã€‚åˆ©ç”¨æ¡ä»¶U-Netæ¶æ„å»ºæ¨¡å®é™…åº”ç”¨ï¼Œä¸éœ€å›ºå®šå›¾åƒå³å¯å……åˆ†è€ƒè™‘æ¡ä»¶ä¿¡æ¯ã€‚è¯¥æ–¹æ³•åº”ç”¨äºä¸åŒå‘¼å¸å¹…åº¦ä¸‹çš„æ”¾å°„æ²»ç–—è‚¿ç˜¤å›¾åƒé…å‡†ï¼Œé‡‡ç”¨4D-CTæ‰«æå¤„ç†èƒ¸è…¹åŒºåŸŸã€‚æˆåŠŸå°†ä¸€ç³»åˆ—è¿ç»­2Dåˆ‡ç‰‡ç¼åˆæˆå¤šä¸ªä¸‰ç»´ä½“ç§¯ï¼Œè§£å†³æ‚£è€…ä¸è§„åˆ™å‘¼å¸ç­‰é—®é¢˜å¯¼è‡´çš„é‡å»ºä¼ªå½±ã€‚ä»£ç å…¬å¼€äºGitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºäº†ä¸€ç§åŸºäºå·²çŸ¥å›¾åƒä¼°è®¡æœªçŸ¥æ¡ä»¶å›¾åƒçš„å…¨æ–°åŒ»å­¦å›¾åƒé…å‡†å½¢å¼ã€‚</li>
<li>é‡‡ç”¨æ¡ä»¶U-Netæ¶æ„è¿›è¡Œå®é™…åº”ç”¨å»ºæ¨¡ï¼Œå……åˆ†è€ƒè™‘æ¡ä»¶ä¿¡æ¯ã€‚</li>
<li>æ–¹æ³•åº”ç”¨äºä¸åŒå‘¼å¸å¹…åº¦ä¸‹çš„è‚¿ç˜¤æ”¾å°„æ²»ç–—å›¾åƒé…å‡†ã€‚</li>
<li>åˆ©ç”¨4D-CTæ‰«æå¤„ç†èƒ¸è…¹åŒºåŸŸå›¾åƒï¼Œå°†è¿ç»­2Dåˆ‡ç‰‡ç¼åˆæˆä¸‰ç»´ä½“ç§¯ã€‚</li>
<li>æˆåŠŸè§£å†³æ‚£è€…ä¸è§„åˆ™å‘¼å¸ç­‰é—®é¢˜å¯¼è‡´çš„é‡å»ºä¼ªå½±é—®é¢˜ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›çš„ä»£ç å·²å…¬å¼€äºGitHubä¸Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10124">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e20ed896b6d0654f91b8b7f30ab3fa97.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c1fd9f1c922ce5dd2f216fbf3514640a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a86102d76424e79b1170f8c9ecf6751.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4b24d2f5f96495009265a8394b48db6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ORL-LDM-Offline-Reinforcement-Learning-Guided-Latent-Diffusion-Model-Super-Resolution-Reconstruction"><a href="#ORL-LDM-Offline-Reinforcement-Learning-Guided-Latent-Diffusion-Model-Super-Resolution-Reconstruction" class="headerlink" title="ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model   Super-Resolution Reconstruction"></a>ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model   Super-Resolution Reconstruction</h2><p><strong>Authors:Shijie Lyu</strong></p>
<p>With the rapid advancement of remote sensing technology, super-resolution image reconstruction is of great research and practical significance. Existing deep learning methods have made progress but still face limitations in handling complex scenes and preserving image details. This paper proposes a reinforcement learning-based latent diffusion model (LDM) fine-tuning method for remote sensing image super-resolution. The method constructs a reinforcement learning environment with states, actions, and rewards, optimizing decision objectives through proximal policy optimization (PPO) during the reverse denoising process of the LDM model. Experiments on the RESISC45 dataset show significant improvements over the baseline model in PSNR, SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11, and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural scenes. The results demonstrate the methodâ€™s effectiveness in enhancing super-resolution quality and adaptability across scenes. </p>
<blockquote>
<p>éšç€é¥æ„ŸæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œè¶…åˆ†è¾¨ç‡å›¾åƒé‡å»ºåœ¨ç ”ç©¶å’Œå®è·µä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç°æœ‰çš„æ·±åº¦å­¦ä¹ æ–¹æ³•å·²ç»å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†åœ¨å¤„ç†å¤æ‚åœºæ™¯å’Œä¿ç•™å›¾åƒç»†èŠ‚æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰å¾®è°ƒæ–¹æ³•ï¼Œç”¨äºé¥æ„Ÿå›¾åƒè¶…åˆ†è¾¨ç‡ã€‚è¯¥æ–¹æ³•æ„å»ºäº†ä¸€ä¸ªå…·æœ‰çŠ¶æ€ã€åŠ¨ä½œå’Œå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Œé€šè¿‡æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„é€†å‘å»å™ªè¿‡ç¨‹ä¸­çš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰æ¥ä¼˜åŒ–å†³ç­–ç›®æ ‡ã€‚åœ¨RESISC45æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€ç»“æ„ç›¸ä¼¼æ€§ï¼ˆSSIMï¼‰å’Œå±€éƒ¨æ„ŸçŸ¥å›¾åƒç›¸ä¼¼æ€§ï¼ˆLPIPSï¼‰ç­‰æŒ‡æ ‡ä¸Šè¾ƒåŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—æ”¹å–„ï¼Œå…¶ä¸­PSNRæé«˜3-4dBï¼ŒSSIMæé«˜0.08-0.11ï¼ŒLPIPSé™ä½0.06-0.10ã€‚ç‰¹åˆ«æ˜¯åœ¨ç»“æ„å’Œå¤æ‚è‡ªç„¶åœºæ™¯æ–¹é¢ï¼Œè¯¥æ–¹æ³•åœ¨æå‡è¶…åˆ†è¾¨ç‡è´¨é‡å’Œåœºæ™¯é€‚åº”æ€§æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10027v1">PDF</a> Accepted by the 4th International Conference on Computing Innovation   and Applied Physics (CONF-CIAP 2025), and will be published in EAI Community   Research Series-CORE or Theoretical and Natural Science (TNS)</p>
<p><strong>Summary</strong></p>
<p>éšç€é¥æ„ŸæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œè¶…åˆ†è¾¨ç‡å›¾åƒé‡å»ºå…·æœ‰é‡å¤§çš„ç ”ç©¶å’Œå®è·µæ„ä¹‰ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰å¾®è°ƒæ–¹æ³•ï¼Œç”¨äºé¥æ„Ÿå›¾åƒè¶…åˆ†è¾¨ç‡ã€‚è¯¥æ–¹æ³•æ„å»ºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Œé€šè¿‡è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰åœ¨LDMæ¨¡å‹çš„åå‘å»å™ªè¿‡ç¨‹ä¸­ä¼˜åŒ–å†³ç­–ç›®æ ‡ã€‚åœ¨RESISC45æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨PSNRã€SSIMå’ŒLPIPSæŒ‡æ ‡ä¸Šè¾ƒåŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—æ”¹å–„ï¼Œç‰¹åˆ«æ˜¯ç»“æ„å’Œå¤æ‚è‡ªç„¶åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„Ÿå›¾åƒè¶…åˆ†è¾¨ç‡é‡å»ºå…·æœ‰é‡å¤§ç ”ç©¶å’Œå®è·µä»·å€¼ã€‚</li>
<li>ç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å¤æ‚åœºæ™¯å’Œä¿ç•™å›¾åƒç»†èŠ‚æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰å¾®è°ƒæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æ„å»ºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Œä¼˜åŒ–å†³ç­–ç›®æ ‡ã€‚</li>
<li>å®éªŒåœ¨RESISC45æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨PSNRã€SSIMå’ŒLPIPSæŒ‡æ ‡ä¸Šè¾ƒåŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æé«˜è¶…åˆ†è¾¨ç‡è´¨é‡å’Œåœºæ™¯é€‚åº”æ€§æ–¹é¢æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10027">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-31d7cf1857ab932d054980bffb1c9a1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddcf92f510f7c2dac4f26a6549db03ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c068d16b5b1c0415196242c2b1344363.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5678a48e3aa9d613afe09ba984380083.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-865f1f1216ae1674d187d39445936fe3.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Ordered-subsets-Multi-diffusion-Model-for-Sparse-view-CT-Reconstruction"><a href="#Ordered-subsets-Multi-diffusion-Model-for-Sparse-view-CT-Reconstruction" class="headerlink" title="Ordered-subsets Multi-diffusion Model for Sparse-view CT Reconstruction"></a>Ordered-subsets Multi-diffusion Model for Sparse-view CT Reconstruction</h2><p><strong>Authors:Pengfei Yu, Bin Huang, Minghui Zhang, Weiwen Wu, Shaoyu Wang, Qiegen Liu</strong></p>
<p>Score-based diffusion models have shown significant promise in the field of sparse-view CT reconstruction. However, the projection dataset is large and riddled with redundancy. Consequently, applying the diffusion model to unprocessed data results in lower learning effectiveness and higher learning difficulty, frequently leading to reconstructed images that lack fine details. To address these issues, we propose the ordered-subsets multi-diffusion model (OSMM) for sparse-view CT reconstruction. The OSMM innovatively divides the CT projection data into equal subsets and employs multi-subsets diffusion model (MSDM) to learn from each subset independently. This targeted learning approach reduces complexity and enhances the reconstruction of fine details. Furthermore, the integration of one-whole diffusion model (OWDM) with complete sinogram data acts as a global information constraint, which can reduce the possibility of generating erroneous or inconsistent sinogram information. Moreover, the OSMMâ€™s unsupervised learning framework provides strong robustness and generalizability, adapting seamlessly to varying sparsity levels of CT sinograms. This ensures consistent and reliable performance across different clinical scenarios. Experimental results demonstrate that OSMM outperforms traditional diffusion models in terms of image quality and noise resilience, offering a powerful and versatile solution for advanced CT imaging in sparse-view scenarios. </p>
<blockquote>
<p>åŸºäºå¾—åˆ†çš„æ‰©æ•£æ¨¡å‹åœ¨ç¨€ç–è§†å›¾CTé‡å»ºé¢†åŸŸæ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼ŒæŠ•å½±æ•°æ®é›†åºå¤§ä¸”å­˜åœ¨å†—ä½™ã€‚å› æ­¤ï¼Œå°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºæœªå¤„ç†çš„æ•°æ®ä¼šå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹å’Œå­¦ä¹ éš¾åº¦å¢åŠ ï¼Œç»å¸¸å¯¼è‡´é‡å»ºçš„å›¾åƒç¼ºä¹ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æœ‰åºå­é›†å¤šæ‰©æ•£æ¨¡å‹ï¼ˆOSMMï¼‰ç”¨äºç¨€ç–è§†å›¾CTé‡å»ºã€‚OSMMåˆ›æ–°åœ°å°†CTæŠ•å½±æ•°æ®åˆ†æˆç›¸ç­‰çš„å­é›†ï¼Œå¹¶é‡‡ç”¨å¤šå­é›†æ‰©æ•£æ¨¡å‹ï¼ˆMSDMï¼‰ä»æ¯ä¸ªå­é›†ä¸­ç‹¬ç«‹å­¦ä¹ ã€‚è¿™ç§æœ‰é’ˆå¯¹æ€§çš„å­¦ä¹ æ–¹æ³•é™ä½äº†å¤æ‚æ€§ï¼Œæé«˜äº†ç»†èŠ‚é‡å»ºèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå°†æ•´ä½“æ‰©æ•£æ¨¡å‹ï¼ˆOWDMï¼‰ä¸å®Œæ•´è¾›æ©å›¾æ•°æ®ç›¸ç»“åˆï¼Œä½œä¸ºå…¨å±€ä¿¡æ¯çº¦æŸï¼Œé™ä½äº†äº§ç”Ÿé”™è¯¯æˆ–ä¸ä¸€è‡´è¾›æ©å›¾ä¿¡æ¯çš„å¯èƒ½æ€§ã€‚è€Œä¸”ï¼ŒOSMMçš„æ— ç›‘ç£å­¦ä¹ æ¡†æ¶å…·æœ‰å¾ˆå¼ºçš„é²æ£’æ€§å’Œé€šç”¨æ€§ï¼Œèƒ½å¤Ÿæ— ç¼é€‚åº”CTè¾›æ©å›¾çš„ä¸åŒç¨€ç–çº§åˆ«ã€‚è¿™ç¡®ä¿äº†åœ¨ä¸åŒä¸´åºŠåœºæ™¯ä¸­çš„ä¸€è‡´å’Œå¯é æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOSMMåœ¨å›¾åƒè´¨é‡å’Œå™ªå£°éŸ§æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ï¼Œä¸ºç¨€ç–è§†å›¾åœºæ™¯ä¸­çš„é«˜çº§CTæˆåƒæä¾›äº†å¼ºå¤§ä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09985v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ç¨€ç–è§†å›¾CTé‡å»ºçš„æœ‰åºå­é›†å¤šæ‰©æ•£æ¨¡å‹ï¼ˆOSMMï¼‰ã€‚OSMMé€šè¿‡å°†CTæŠ•å½±æ•°æ®åˆ†æˆå¤šä¸ªå­é›†ï¼Œå¹¶é‡‡ç”¨å¤šå­é›†æ‰©æ•£æ¨¡å‹ï¼ˆMSDMï¼‰è¿›è¡Œç‹¬ç«‹å­¦ä¹ ï¼Œä»¥æé«˜å­¦ä¹ æ•ˆç‡å’Œé‡å»ºå›¾åƒçš„è´¨é‡ã€‚ç»“åˆå®Œæ•´è¾›æ°å›¾æ•°æ®çš„ä¸€ä½“å¼æ‰©æ•£æ¨¡å‹ï¼ˆOWDMï¼‰ä½œä¸ºå…¨å±€ä¿¡æ¯çº¦æŸï¼Œå‡å°‘äº†é”™è¯¯æˆ–ä¸ä¸€è‡´è¾›æ°å›¾ä¿¡æ¯çš„å¯èƒ½æ€§ã€‚OSMMçš„æ— ç›‘ç£å­¦ä¹ æ¡†æ¶å…·æœ‰å¼ºå¤§çš„é²æ£’æ€§å’Œé€šç”¨æ€§ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒCTè¾›æ°å›¾ç¨€ç–åº¦æ°´å¹³çš„å˜åŒ–ï¼Œä¸ºç¨€ç–è§†å›¾åœºæ™¯ä¸‹çš„é«˜çº§CTæˆåƒæä¾›äº†å¼ºå¤§è€Œé€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¨€ç–è§†å›¾CTé‡å»ºä¸­ï¼ŒæŠ•å½±æ•°æ®é›†åºå¤§ä¸”å­˜åœ¨å†—ä½™ï¼Œç›´æ¥åº”ç”¨æ‰©æ•£æ¨¡å‹ä¼šå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ï¼Œå½±å“å›¾åƒé‡å»ºè´¨é‡ã€‚</li>
<li>æœ‰åºå­é›†å¤šæ‰©æ•£æ¨¡å‹ï¼ˆOSMMï¼‰å°†CTæŠ•å½±æ•°æ®åˆ†æˆå¤šä¸ªå­é›†è¿›è¡Œå­¦ä¹ ï¼Œæé«˜å­¦ä¹ æ•ˆç‡å’Œç²¾ç»†ç»†èŠ‚é‡å»ºèƒ½åŠ›ã€‚</li>
<li>OSMMé‡‡ç”¨å¤šå­é›†æ‰©æ•£æ¨¡å‹ï¼ˆMSDMï¼‰è¿›è¡Œç‹¬ç«‹å­¦ä¹ ï¼Œé™ä½å¤æ‚åº¦ã€‚</li>
<li>ç»“åˆå®Œæ•´è¾›æ°å›¾æ•°æ®çš„ä¸€ä½“å¼æ‰©æ•£æ¨¡å‹ï¼ˆOWDMï¼‰ä½œä¸ºå…¨å±€ä¿¡æ¯çº¦æŸï¼Œå‡å°‘é”™è¯¯æˆ–ä¸ä¸€è‡´è¾›æ°å›¾ä¿¡æ¯çš„å¯èƒ½æ€§ã€‚</li>
<li>OSMMçš„æ— ç›‘ç£å­¦ä¹ æ¡†æ¶å…·æœ‰å¼ºå¤§çš„é²æ£’æ€§å’Œé€šç”¨æ€§ï¼Œèƒ½é€‚åº”ä¸åŒç¨€ç–åº¦æ°´å¹³çš„CTè¾›æ°å›¾ã€‚</li>
<li>OSMMåœ¨å›¾åƒè´¨é‡å’Œå™ªå£°éŸ§æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09985">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fa4ae34e4f2b3b783de3a6c9808cf701.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1544906ac0105b934677cb959a2af3b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2d4862d315c4707e2f3f5ad50995cf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04600aaff474e87d86af5df094a93fe5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-887152a97ec575086524c0da36d6e5b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3436a85f5ec9fe93a7a6366d14f4a4f4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DDFP-Data-dependent-Frequency-Prompt-for-Source-Free-Domain-Adaptation-of-Medical-Image-Segmentation"><a href="#DDFP-Data-dependent-Frequency-Prompt-for-Source-Free-Domain-Adaptation-of-Medical-Image-Segmentation" class="headerlink" title="DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation   of Medical Image Segmentation"></a>DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation   of Medical Image Segmentation</h2><p><strong>Authors:Siqi Yin, Shaolei Liu, Manning Wang</strong></p>
<p>Domain adaptation addresses the challenge of model performance degradation caused by domain gaps. In the typical setup for unsupervised domain adaptation, labeled data from a source domain and unlabeled data from a target domain are used to train a target model. However, access to labeled source domain data, particularly in medical datasets, can be restricted due to privacy policies. As a result, research has increasingly shifted to source-free domain adaptation (SFDA), which requires only a pretrained model from the source domain and unlabeled data from the target domain data for adaptation. Existing SFDA methods often rely on domain-specific image style translation and self-supervision techniques to bridge the domain gap and train the target domain model. However, the quality of domain-specific style-translated images and pseudo-labels produced by these methods still leaves room for improvement. Moreover, training the entire model during adaptation can be inefficient under limited supervision. In this paper, we propose a novel SFDA framework to address these challenges. Specifically, to effectively mitigate the impact of domain gap in the initial training phase, we introduce preadaptation to generate a preadapted model, which serves as an initialization of target model and allows for the generation of high-quality enhanced pseudo-labels without introducing extra parameters. Additionally, we propose a data-dependent frequency prompt to more effectively translate target domain images into a source-like style. To further enhance adaptation, we employ a style-related layer fine-tuning strategy, specifically designed for SFDA, to train the target model using the prompted target domain images and pseudo-labels. Extensive experiments on cross-modality abdominal and cardiac SFDA segmentation tasks demonstrate that our proposed method outperforms existing state-of-the-art methods. </p>
<blockquote>
<p>é¢†åŸŸé€‚åº”ï¼ˆDomain Adaptationï¼‰æ—¨åœ¨è§£å†³å› é¢†åŸŸå·®å¼‚å¯¼è‡´çš„æ¨¡å‹æ€§èƒ½ä¸‹é™çš„æŒ‘æˆ˜ã€‚åœ¨å…¸å‹çš„æ— ç›‘ç£é¢†åŸŸé€‚åº”è®¾ç½®ä¸­ï¼Œä½¿ç”¨æ¥è‡ªæºåŸŸçš„å¸¦æ ‡ç­¾æ•°æ®ä»¥åŠæ¥è‡ªç›®æ ‡åŸŸçš„æ— æ ‡ç­¾æ•°æ®æ¥è®­ç»ƒç›®æ ‡æ¨¡å‹ã€‚ç„¶è€Œï¼Œç”±äºéšç§æ”¿ç­–é™åˆ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦æ•°æ®é›†ä¸­ï¼Œè®¿é—®æºåŸŸçš„å¸¦æ ‡ç­¾æ•°æ®å¯èƒ½ä¼šå—åˆ°é™åˆ¶ã€‚å› æ­¤ï¼Œç ”ç©¶è¶Šæ¥è¶Šè½¬å‘æ— æºé¢†åŸŸé€‚åº”ï¼ˆSFDAï¼‰ï¼Œå®ƒåªéœ€è¦æºåŸŸçš„é¢„è®­ç»ƒæ¨¡å‹ä»¥åŠæ¥è‡ªç›®æ ‡åŸŸçš„æ— æ ‡ç­¾æ•°æ®è¿›è¡Œé€‚åº”ã€‚ç°æœ‰çš„SFDAæ–¹æ³•é€šå¸¸ä¾èµ–äºç‰¹å®šé¢†åŸŸçš„å›¾åƒé£æ ¼è½¬æ¢å’Œè‡ªç›‘ç£æŠ€æœ¯æ¥å¼¥åˆé¢†åŸŸå·®è·å¹¶è®­ç»ƒç›®æ ‡åŸŸæ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•äº§ç”Ÿçš„ç‰¹å®šé¢†åŸŸé£æ ¼è½¬æ¢å›¾åƒå’Œä¼ªæ ‡ç­¾çš„è´¨é‡ä»æœ‰æå‡ç©ºé—´ã€‚æ­¤å¤–ï¼Œåœ¨æœ‰é™çš„ç›‘ç£ä¸‹ï¼Œåœ¨é€‚åº”è¿‡ç¨‹ä¸­è®­ç»ƒæ•´ä¸ªæ¨¡å‹å¯èƒ½æ•ˆç‡ä½ä¸‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„SFDAæ¡†æ¶æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†æœ‰æ•ˆå‡è½»åˆå§‹è®­ç»ƒé˜¶æ®µä¸­çš„é¢†åŸŸå·®è·å½±å“ï¼Œæˆ‘ä»¬å¼•å…¥é¢„é€‚åº”ï¼ˆpreadaptationï¼‰æ¥ç”Ÿæˆé¢„é€‚åº”æ¨¡å‹ï¼Œä½œä¸ºç›®æ ‡æ¨¡å‹çš„åˆå§‹åŒ–ï¼Œå¹¶å…è®¸ç”Ÿæˆé«˜è´¨é‡çš„å¢å¼ºä¼ªæ ‡ç­¾ï¼Œè€Œä¸ä¼šå¼•å…¥é¢å¤–çš„å‚æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ•°æ®ä¾èµ–çš„é¢‘ç‡æç¤ºï¼ˆfrequency promptï¼‰ï¼Œä»¥æ›´æœ‰æ•ˆåœ°å°†ç›®æ ‡åŸŸå›¾åƒè½¬æ¢ä¸ºç±»ä¼¼æºåŸŸçš„æ ·å¼ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºé€‚åº”æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§é’ˆå¯¹SFDAè®¾è®¡çš„æ ·å¼ç›¸å…³å±‚å¾®è°ƒç­–ç•¥ï¼Œä½¿ç”¨æç¤ºçš„ç›®æ ‡åŸŸå›¾åƒå’Œä¼ªæ ‡ç­¾è®­ç»ƒç›®æ ‡æ¨¡å‹ã€‚åœ¨è·¨æ¨¡æ€è…¹éƒ¨å’Œå¿ƒè„SFDAåˆ†å‰²ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09927v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§æ— æºåŸŸé€‚åº”ï¼ˆSFDAï¼‰æ¡†æ¶ï¼Œè§£å†³æ¨¡å‹æ€§èƒ½å› é¢†åŸŸå·®å¼‚è€Œä¸‹é™çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ä»…éœ€è¦æºåŸŸçš„é¢„è®­ç»ƒæ¨¡å‹å’Œæ¥è‡ªç›®æ ‡åŸŸçš„æ— æ ‡ç­¾æ•°æ®è¿›è¡Œé€‚åº”ã€‚ä¸ºæœ‰æ•ˆç¼“è§£åˆå§‹è®­ç»ƒé˜¶æ®µçš„é¢†åŸŸå·®å¼‚å½±å“ï¼Œå¼•å…¥é¢„é€‚åº”ç”Ÿæˆé¢„é€‚åº”æ¨¡å‹ï¼Œä½œä¸ºç›®æ ‡æ¨¡å‹çš„åˆå§‹åŒ–ï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡å¢å¼ºä¼ªæ ‡ç­¾ã€‚åŒæ—¶ï¼Œæå‡ºæ•°æ®ç›¸å…³é¢‘ç‡æç¤ºï¼Œæ›´æœ‰æ•ˆåœ°å°†ç›®æ ‡åŸŸå›¾åƒç¿»è¯‘æˆç±»ä¼¼æºçš„é£æ ¼ã€‚æœ€åï¼Œé‡‡ç”¨é’ˆå¯¹SFDAè®¾è®¡çš„é£æ ¼ç›¸å…³å±‚å¾®è°ƒç­–ç•¥ï¼Œä½¿ç”¨æç¤ºçš„ç›®æ ‡åŸŸå›¾åƒå’Œä¼ªæ ‡ç­¾è®­ç»ƒç›®æ ‡æ¨¡å‹ã€‚åœ¨è·¨æ¨¡æ€è…¹éƒ¨å’Œå¿ƒè„SFDAåˆ†å‰²ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é¢†åŸŸé€‚åº”æ˜¯è§£å†³æ¨¡å‹æ€§èƒ½å› é¢†åŸŸå·®å¼‚è€Œä¸‹é™çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>æ— æºåŸŸé€‚åº”ï¼ˆSFDAï¼‰æ¡†æ¶ä»…éœ€è¦æºåŸŸçš„é¢„è®­ç»ƒæ¨¡å‹å’Œæ¥è‡ªç›®æ ‡åŸŸçš„æ— æ ‡ç­¾æ•°æ®ã€‚</li>
<li>å¼•å…¥é¢„é€‚åº”ç”Ÿæˆé¢„é€‚åº”æ¨¡å‹ï¼Œä½œä¸ºç›®æ ‡æ¨¡å‹çš„åˆå§‹åŒ–ï¼Œç¼“è§£é¢†åŸŸå·®å¼‚çš„å½±å“ã€‚</li>
<li>æå‡ºæ•°æ®ç›¸å…³é¢‘ç‡æç¤ºï¼Œæé«˜ç›®æ ‡åŸŸå›¾åƒçš„é£æ ¼ç¿»è¯‘è´¨é‡ã€‚</li>
<li>é‡‡ç”¨é’ˆå¯¹SFDAè®¾è®¡çš„é£æ ¼ç›¸å…³å±‚å¾®è°ƒç­–ç•¥ï¼Œæé«˜ç›®æ ‡æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚</li>
<li>åœ¨è·¨æ¨¡æ€è…¹éƒ¨å’Œå¿ƒè„SFDAåˆ†å‰²ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•è¡¨ç°ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ºè§£å†³åŒ»å­¦å›¾åƒé¢†åŸŸä¸­çš„æ¨¡å‹æ€§èƒ½ä¸‹é™é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09927">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f67265ef114c8bb31fc98dd1106864d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06ce80e744887b0a926948bc0912b080.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4571e99b662f03e272c1c4d72ec59b7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bebcd57e01d95de240b6959bce8223f2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ImplicitStainer-Data-Efficient-Medical-Image-Translation-for-Virtual-Antibody-based-Tissue-Staining-Using-Local-Implicit-Functions"><a href="#ImplicitStainer-Data-Efficient-Medical-Image-Translation-for-Virtual-Antibody-based-Tissue-Staining-Using-Local-Implicit-Functions" class="headerlink" title="ImplicitStainer: Data-Efficient Medical Image Translation for Virtual   Antibody-based Tissue Staining Using Local Implicit Functions"></a>ImplicitStainer: Data-Efficient Medical Image Translation for Virtual   Antibody-based Tissue Staining Using Local Implicit Functions</h2><p><strong>Authors:Tushar Kataria, Beatrice Knudsen, Shireen Y. Elhabian</strong></p>
<p>Hematoxylin and eosin (H&amp;E) staining is a gold standard for microscopic diagnosis in pathology. However, H&amp;E staining does not capture all the diagnostic information that may be needed. To obtain additional molecular information, immunohistochemical (IHC) stains highlight proteins that mark specific cell types, such as CD3 for T-cells or CK8&#x2F;18 for epithelial cells. While IHC stains are vital for prognosis and treatment guidance, they are typically only available at specialized centers and time consuming to acquire, leading to treatment delays for patients. Virtual staining, enabled by deep learning-based image translation models, provides a promising alternative by computationally generating IHC stains from H&amp;E stained images. Although many GAN and diffusion based image to image (I2I) translation methods have been used for virtual staining, these models treat image patches as independent data points, which results in increased and more diverse data requirements for effective generation. We present ImplicitStainer, a novel approach that leverages local implicit functions to improve image translation, specifically virtual staining performance, by focusing on pixel-level predictions. This method enhances robustness to variations in dataset sizes, delivering high-quality results even with limited data. We validate our approach on two datasets using a comprehensive set of metrics and benchmark it against over fifteen state-of-the-art GAN- and diffusion based models. Full Code and models trained will be released publicly via Github upon acceptance. </p>
<blockquote>
<p>è‹æœ¨ç²¾å’Œä¼Šçº¢æŸ“è‰²ï¼ˆH&amp;EæŸ“è‰²ï¼‰æ˜¯ç—…ç†å­¦æ˜¾å¾®è¯Šæ–­çš„é‡‘æ ‡å‡†ã€‚ç„¶è€Œï¼ŒH&amp;EæŸ“è‰²å¹¶ä¸èƒ½æ•è·æ‰€æœ‰æ‰€éœ€çš„è¯Šæ–­ä¿¡æ¯ã€‚ä¸ºäº†è·å–é¢å¤–çš„åˆ†å­ä¿¡æ¯ï¼Œå…ç–«ç»„ç»‡åŒ–å­¦æŸ“è‰²ï¼ˆIHCæŸ“è‰²ï¼‰ä¼šçªå‡ºæ˜¾ç¤ºæ ‡è®°ç‰¹å®šç»†èƒç±»å‹çš„è›‹ç™½è´¨ï¼Œå¦‚Tç»†èƒçš„CD3æˆ–ä¸Šçš®ç»†èƒçš„CK8&#x2F;18ã€‚è™½ç„¶IHCæŸ“è‰²å¯¹äºé¢„åå’Œæ²»ç–—æŒ‡å¯¼è‡³å…³é‡è¦ï¼Œä½†å®ƒä»¬é€šå¸¸ä»…åœ¨ä¸“ä¸šä¸­å¿ƒå¯ç”¨ï¼Œå¹¶ä¸”è·å–æ—¶é—´è¾ƒé•¿ï¼Œå¯¼è‡´æ‚£è€…æ²»ç–—å»¶è¿Ÿã€‚æ·±åº¦å­¦ä¹ å›¾åƒç¿»è¯‘æ¨¡å‹å®ç°çš„è™šæ‹ŸæŸ“è‰²æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¯ä»H&amp;EæŸ“è‰²å›¾åƒä¸­è®¡ç®—ç”ŸæˆIHCæŸ“è‰²ã€‚å°½ç®¡è®¸å¤šç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’ŒåŸºäºæ‰©æ•£çš„å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰ç¿»è¯‘æ–¹æ³•å·²è¢«ç”¨äºè™šæ‹ŸæŸ“è‰²ï¼Œä½†è¿™äº›æ¨¡å‹å°†å›¾åƒè¡¥ä¸è§†ä¸ºç‹¬ç«‹çš„æ•°æ®ç‚¹ï¼Œå¯¼è‡´éœ€è¦æ›´å¤šå’Œæ›´å¤šæ ·åŒ–çš„æ•°æ®æ¥è¿›è¡Œæœ‰æ•ˆçš„ç”Ÿæˆã€‚æˆ‘ä»¬æå‡ºäº†ImplicitStainerè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ©ç”¨å±€éƒ¨éšå‡½æ•°æ¥æé«˜å›¾åƒç¿»è¯‘ï¼Œç‰¹åˆ«æ˜¯è™šæ‹ŸæŸ“è‰²æ€§èƒ½ï¼Œé‡ç‚¹å…³æ³¨åƒç´ çº§é¢„æµ‹ã€‚æ­¤æ–¹æ³•æé«˜äº†å¯¹æ•°æ®é›†å¤§å°å˜åŒ–çš„ç¨³å¥æ€§ï¼Œå³ä½¿åœ¨æœ‰é™æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿèƒ½äº§ç”Ÿé«˜è´¨é‡çš„ç»“æœã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä½¿ç”¨äº†ä¸€å¥—ç»¼åˆæŒ‡æ ‡ï¼Œå¹¶å°†å…¶ä¸è¶…è¿‡åäº”ç§æœ€å…ˆè¿›çš„GANå’ŒåŸºäºæ‰©æ•£çš„æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ä¸€æ—¦æ¥å—ï¼Œæˆ‘ä»¬å°†é€šè¿‡GitHubå…¬å¼€å‘å¸ƒå®Œæ•´çš„ä»£ç å’Œè®­ç»ƒå¥½çš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09831v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>H&amp;EæŸ“è‰²æ˜¯ç—…ç†å­¦æ˜¾å¾®è¯Šæ–­çš„é‡‘æ ‡å‡†ï¼Œä½†å…¶è¯Šæ–­ä¿¡æ¯å¹¶ä¸å…¨é¢ã€‚ä¸ºäº†è·å–é¢å¤–çš„åˆ†å­ä¿¡æ¯ï¼Œå…ç–«ç»„ç»‡åŒ–å­¦æŸ“è‰²ï¼ˆIHCï¼‰èƒ½å¤Ÿçªå‡ºç‰¹å®šç»†èƒç±»å‹çš„è›‹ç™½è´¨æ ‡è®°ï¼Œå¦‚Tç»†èƒçš„CD3æˆ–ä¸Šçš®ç»†èƒçš„CK8&#x2F;18ã€‚è™½ç„¶IHCæŸ“è‰²å¯¹äºé¢„åå’Œæ²»ç–—æŒ‡å¯¼è‡³å…³é‡è¦ï¼Œä½†å®ƒé€šå¸¸åœ¨ä¸“ä¸šä¸­å¿ƒæ‰å¯ç”¨ï¼Œä¸”è·å–æ—¶é—´è¾ƒé•¿ï¼Œå¯¼è‡´æ‚£è€…æ²»ç–—å»¶è¿Ÿã€‚æ·±åº¦å­¦ä¹ å›¾åƒç¿»è¯‘æ¨¡å‹å¯ç”¨çš„è™šæ‹ŸæŸ“è‰²æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½å¤Ÿä»H&amp;EæŸ“è‰²å›¾åƒä¸­è®¡ç®—ç”ŸæˆIHCæŸ“è‰²ã€‚è™½ç„¶å·²æœ‰è®¸å¤šç”¨äºè™šæ‹ŸæŸ“è‰²çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œæ‰©æ•£å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰ç¿»è¯‘æ–¹æ³•ï¼Œä½†è¿™äº›æ–¹æ³•å°†å›¾åƒå—è§†ä¸ºç‹¬ç«‹çš„æ•°æ®ç‚¹ï¼Œå¯¼è‡´æœ‰æ•ˆç”Ÿæˆæ‰€éœ€çš„æ•°æ®è¦æ±‚å¢åŠ ä¸”æ›´åŠ å¤šæ ·åŒ–ã€‚æœ¬æ–‡æå‡ºäº†ImplicitStainerè¿™ä¸€æ–°æ–¹æ³•ï¼Œåˆ©ç”¨å±€éƒ¨éšå‡½æ•°æ”¹è¿›å›¾åƒç¿»è¯‘ï¼Œç‰¹åˆ«æ˜¯è™šæ‹ŸæŸ“è‰²æ€§èƒ½ï¼Œä¸“æ³¨äºåƒç´ çº§é¢„æµ‹ã€‚è¯¥æ–¹æ³•æé«˜äº†å¯¹æ•°æ®é›†å¤§å°å˜åŒ–çš„ç¨³å¥æ€§ï¼Œå³ä½¿åœ¨æœ‰é™æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿèƒ½äº§ç”Ÿé«˜è´¨é‡çš„ç»“æœã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ï¼Œä½¿ç”¨äº†ä¸€å¥—ç»¼åˆæŒ‡æ ‡å¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸åäº”ç§ä»¥ä¸Šçš„æœ€å…ˆè¿›GANå’Œæ‰©æ•£æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚ä¸€æ—¦æ¥å—ï¼Œæˆ‘ä»¬å°†é€šè¿‡Githubå…¬å¼€å‘å¸ƒå®Œæ•´çš„ä»£ç å’Œè®­ç»ƒå¥½çš„æ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>H&amp;EæŸ“è‰²æ˜¯ç—…ç†å­¦æ˜¾å¾®è¯Šæ–­çš„é‡‘æ ‡å‡†ï¼Œä½†ç¼ºä¹å…¨é¢çš„è¯Šæ–­ä¿¡æ¯ã€‚</li>
<li>å…ç–«ç»„ç»‡åŒ–å­¦æŸ“è‰²ï¼ˆIHCï¼‰å¯¹äºè·å–ç‰¹å®šç»†èƒç±»å‹æ ‡è®°çš„åˆ†å­ä¿¡æ¯è‡³å…³é‡è¦ã€‚</li>
<li>è™šæ‹ŸæŸ“è‰²é€šè¿‡æ·±åº¦å­¦ä¹ å›¾åƒç¿»è¯‘æ¨¡å‹æä¾›äº†ä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆæ¥ç”ŸæˆIHCæŸ“è‰²ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ï¼ˆå¦‚GANå’Œæ‰©æ•£æ¨¡å‹ï¼‰åœ¨å¤„ç†å›¾åƒæ•°æ®æ—¶å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æ›´å¤šå’Œæ›´å¤šæ ·åŒ–çš„æ•°æ®ã€‚</li>
<li>ImplicitStaineræ–¹æ³•åˆ©ç”¨å±€éƒ¨éšå‡½æ•°æ”¹è¿›å›¾åƒç¿»è¯‘ï¼Œä¸“æ³¨äºåƒç´ çº§é¢„æµ‹ï¼Œæé«˜ç¨³å¥æ€§å’Œç”Ÿæˆè´¨é‡ã€‚</li>
<li>ImplicitStaineråœ¨æœ‰é™æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿèƒ½äº§ç”Ÿé«˜è´¨é‡çš„ç»“æœã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¹¶ä¸å…¶ä»–å…ˆè¿›æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09831">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9bfda69868a7eb255dc033286adeedea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b9b5b6e816678afb456b8e365df907b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0cf187634bcd1ca1567944ac6ac7a0ef.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="BoundarySeg-An-Embarrassingly-Simple-Method-To-Boost-Medical-Image-Segmentation-Performance-for-Low-Data-Regimes"><a href="#BoundarySeg-An-Embarrassingly-Simple-Method-To-Boost-Medical-Image-Segmentation-Performance-for-Low-Data-Regimes" class="headerlink" title="BoundarySeg:An Embarrassingly Simple Method To Boost Medical Image   Segmentation Performance for Low Data Regimes"></a>BoundarySeg:An Embarrassingly Simple Method To Boost Medical Image   Segmentation Performance for Low Data Regimes</h2><p><strong>Authors:Tushar Kataria, Shireen Y. Elhabian</strong></p>
<p>Obtaining large-scale medical data, annotated or unannotated, is challenging due to stringent privacy regulations and data protection policies. In addition, annotating medical images requires that domain experts manually delineate anatomical structures, making the process both time-consuming and costly. As a result, semi-supervised methods have gained popularity for reducing annotation costs. However, the performance of semi-supervised methods is heavily dependent on the availability of unannotated data, and their effectiveness declines when such data are scarce or absent. To overcome this limitation, we propose a simple, yet effective and computationally efficient approach for medical image segmentation that leverages only existing annotations. We propose BoundarySeg , a multi-task framework that incorporates organ boundary prediction as an auxiliary task to full organ segmentation, leveraging consistency between the two task predictions to provide additional supervision. This strategy improves segmentation accuracy, especially in low data regimes, allowing our method to achieve performance comparable to or exceeding state-of-the-art semi supervised approaches all without relying on unannotated data or increasing computational demands. Code will be released upon acceptance. </p>
<blockquote>
<p>è·å–å¤§è§„æ¨¡åŒ»å­¦æ•°æ®ï¼Œæ— è®ºæ˜¯æœ‰æ ‡æ³¨çš„è¿˜æ˜¯æ— æ ‡æ³¨çš„ï¼Œéƒ½å› ä¸ºä¸¥æ ¼çš„éšç§æ³•è§„å’Œæ•°æ®å¤„ç†æ”¿ç­–è€Œé¢ä¸´æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæ ‡æ³¨åŒ»å­¦å›¾åƒéœ€è¦é¢†åŸŸä¸“å®¶æ‰‹åŠ¨æç»˜è§£å‰–ç»“æ„ï¼Œä½¿å¾—è¿™ä¸€è¿‡ç¨‹æ—¢è€—æ—¶åˆæˆæœ¬é«˜æ˜‚ã€‚å› æ­¤ï¼ŒåŠç›‘ç£æ–¹æ³•å·²ç»æµè¡Œèµ·æ¥ï¼Œç”¨äºé™ä½æ ‡æ³¨æˆæœ¬ã€‚ç„¶è€Œï¼ŒåŠç›‘ç£æ–¹æ³•çš„æ€§èƒ½ä¸¥é‡ä¾èµ–äºæ— æ ‡æ³¨æ•°æ®çš„å¯ç”¨æ€§ï¼Œå½“è¿™ç±»æ•°æ®ç¨€ç¼ºæˆ–ä¸å­˜åœ¨æ—¶ï¼Œå…¶æœ‰æ•ˆæ€§ä¼šé™ä½ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•ã€æœ‰æ•ˆä¸”è®¡ç®—æ•ˆç‡é«˜çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œä»…åˆ©ç”¨ç°æœ‰æ ‡æ³¨ã€‚æˆ‘ä»¬æå‡ºäº†BoundarySegï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šä»»åŠ¡æ¡†æ¶ï¼Œå®ƒå°†å™¨å®˜è¾¹ç•Œé¢„æµ‹ä½œä¸ºè¾…åŠ©ä»»åŠ¡èå…¥åˆ°å…¨å™¨å®˜åˆ†å‰²ä¸­ï¼Œåˆ©ç”¨ä¸¤ä¸ªä»»åŠ¡é¢„æµ‹ä¹‹é—´çš„ä¸€è‡´æ€§æä¾›é¢å¤–çš„ç›‘ç£ã€‚è¿™ä¸€ç­–ç•¥æé«˜äº†åˆ†å‰²ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®è¾ƒå°‘çš„æƒ…å†µä¸‹ï¼Œä½¿æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸ä¾èµ–æ— æ ‡æ³¨æ•°æ®æˆ–å¢åŠ è®¡ç®—éœ€æ±‚çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†ä¸æˆ–è¶…è¿‡æœ€æ–°åŠç›‘ç£æ–¹æ³•çš„æ€§èƒ½ã€‚è®ºæ–‡æ¥å—åå°†å‘å¸ƒä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09829v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒæ ‡æ³¨æ•°æ®è·å–å›°éš¾ï¼Œå› éšç§æ³•è§„å’Œä¿æ”¿ç­–ä¸¥è‹›ã€‚åŠç›‘ç£æ–¹æ³•å¯å‡å°‘æ ‡æ³¨æˆæœ¬ï¼Œä½†ä¾èµ–æœªæ ‡æ³¨æ•°æ®ï¼Œå½“å…¶ç¨€ç¼ºæ—¶æ•ˆæœä¸‹é™ã€‚æœ¬æ–‡æå‡ºä¸€ç§ä»…åˆ©ç”¨ç°æœ‰æ ‡æ³¨æ•°æ®çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œé‡‡ç”¨å¤šä»»åŠ¡æ¡†æ¶BoundarySegï¼Œä»¥å™¨å®˜è¾¹ç•Œé¢„æµ‹ä¸ºè¾…åŠ©ä»»åŠ¡ï¼Œé€šè¿‡ä¸¤ä¸ªä»»åŠ¡é¢„æµ‹çš„ä¸€è‡´æ€§æä¾›é¢å¤–ç›‘ç£ï¼Œæé«˜åˆ†å‰²ç²¾åº¦ï¼Œå°¤å…¶åœ¨å°æ•°æ®é›†ä¸‹è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒæ•°æ®è·å–é¢ä¸´æŒ‘æˆ˜ï¼Œå› éšç§æ³•è§„å’Œä¿æ”¿ç­–ä¸¥æ ¼ï¼Œæ ‡æ³¨æ•°æ®å°¤å…¶å›°éš¾ã€‚</li>
<li>åŠç›‘ç£æ–¹æ³•å¯å‡å°‘æ ‡æ³¨æˆæœ¬ï¼Œä½†ä¾èµ–æœªæ ‡æ³¨æ•°æ®ï¼Œæ•°æ®ç¨€ç¼ºæ—¶æ•ˆæœä¸‹é™ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§ä»…åˆ©ç”¨ç°æœ‰æ ‡æ³¨æ•°æ®çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œåä¸ºBoundarySegã€‚</li>
<li>BoundarySegé‡‡ç”¨å¤šä»»åŠ¡æ¡†æ¶ï¼Œç»“åˆå™¨å®˜è¾¹ç•Œé¢„æµ‹ä½œä¸ºè¾…åŠ©ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡ä¸¤ä¸ªä»»åŠ¡é¢„æµ‹çš„ä¸€è‡´æ€§ï¼ŒBoundarySegæä¾›é¢å¤–ç›‘ç£ï¼Œæé«˜åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>BoundarySegå°¤å…¶åœ¨å°æ•°æ®é›†ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œæ€§èƒ½å¯ä¸æœ€å…ˆè¿›çš„åŠç›‘ç£æ–¹æ³•ç›¸åª²ç¾æˆ–è¶…è¿‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09829">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-899741a789321f41a5ae1fa15a50da99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5426ce626b2e7cb59d764efacc17ee29.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="A-Multimodal-Multi-Agent-Framework-for-Radiology-Report-Generation"><a href="#A-Multimodal-Multi-Agent-Framework-for-Radiology-Report-Generation" class="headerlink" title="A Multimodal Multi-Agent Framework for Radiology Report Generation"></a>A Multimodal Multi-Agent Framework for Radiology Report Generation</h2><p><strong>Authors:Ziruo Yi, Ting Xiao, Mark V. Albert</strong></p>
<p>Radiology report generation (RRG) aims to automatically produce diagnostic reports from medical images, with the potential to enhance clinical workflows and reduce radiologistsâ€™ workload. While recent approaches leveraging multimodal large language models (MLLMs) and retrieval-augmented generation (RAG) have achieved strong results, they continue to face challenges such as factual inconsistency, hallucination, and cross-modal misalignment. We propose a multimodal multi-agent framework for RRG that aligns with the stepwise clinical reasoning workflow, where task-specific agents handle retrieval, draft generation, visual analysis, refinement, and synthesis. Experimental results demonstrate that our approach outperforms a strong baseline in both automatic metrics and LLM-based evaluations, producing more accurate, structured, and interpretable reports. This work highlights the potential of clinically aligned multi-agent frameworks to support explainable and trustworthy clinical AI applications. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒæŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰æ—¨åœ¨ä»åŒ»å­¦å›¾åƒä¸­è‡ªåŠ¨ç”Ÿæˆè¯Šæ–­æŠ¥å‘Šï¼Œå…·æœ‰æé«˜ä¸´åºŠå·¥ä½œæ•ˆç‡å’Œå‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿå·¥ä½œè´Ÿæ‹…çš„æ½œåŠ›ã€‚è™½ç„¶æœ€è¿‘é‡‡ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ–¹æ³•å–å¾—äº†å¾ˆå¥½çš„æ•ˆæœï¼Œä½†å®ƒä»¬ä»é¢ä¸´äº‹å®ä¸ä¸€è‡´ã€å¹»æƒ³å’Œè·¨æ¨¡æ€ä¸å¯¹é½ç­‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºRRGçš„å¤šæ¨¡æ€å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸åˆ†é˜¶æ®µçš„ä¸´åºŠæ¨ç†å·¥ä½œæµç¨‹ç›¸ä¸€è‡´ï¼Œå…¶ä¸­ä»»åŠ¡ç‰¹å®šçš„æ™ºèƒ½ä½“è´Ÿè´£æ£€ç´¢ã€åˆç¨¿ç”Ÿæˆã€è§†è§‰åˆ†æã€ç²¾åŒ–å’Œç»¼åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è‡ªåŠ¨æŒ‡æ ‡å’ŒåŸºäºLLMçš„è¯„ä¼°ä¸­éƒ½ä¼˜äºå¼ºå¤§çš„åŸºçº¿æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´å‡†ç¡®ã€ç»“æ„åŒ–ã€å¯è§£é‡Šçš„æŠ¥å‘Šã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†ä¸´åºŠå¯¹é½çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶åœ¨æ”¯æŒå¯è§£é‡Šå’Œå¯ä¿¡èµ–çš„ä¸´åºŠäººå·¥æ™ºèƒ½åº”ç”¨æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09787v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºæ”¾å°„æŠ¥å‘Šç”Ÿæˆçš„è·¨æ¨¡æ€å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ç‰¹å®šä»»åŠ¡æ™ºèƒ½ä½“å®ç°æ£€ç´¢ã€è‰ç¨¿ç”Ÿæˆã€è§†è§‰åˆ†æã€æ”¹è¿›å’Œåˆæˆç­‰åŠŸèƒ½ï¼Œä¸ä¸´åºŠæ¨ç†æµç¨‹ç›¸ä¸€è‡´ã€‚å®éªŒç»“æœè¯æ˜è¯¥æ¡†æ¶ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œæé«˜äº†æŠ¥å‘Šçš„å‡†ç¡®æ€§ã€ç»“æ„åŒ–å’Œå¯è§£é‡Šæ€§ï¼Œå¹¶æ”¯æŒå¯è§£é‡Šå’Œå¯ä¿¡çš„ä¸´åºŠäººå·¥æ™ºèƒ½åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ”¾å°„æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰çš„ç›®æ ‡æ˜¯è‡ªåŠ¨ä»åŒ»å­¦å›¾åƒç”Ÿæˆè¯Šæ–­æŠ¥å‘Šï¼Œä»¥æé«˜ä¸´åºŠå·¥ä½œæ•ˆç‡å¹¶å‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œé‡ã€‚</li>
<li>å°½ç®¡å·²æœ‰åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ–¹æ³•å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†å®ƒä»¬ä»é¢ä¸´äº‹å®ä¸ä¸€è‡´ã€å¹»è§‰å’Œè·¨æ¨¡æ€ä¸åŒ¹é…ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è·¨æ¨¡æ€å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºRRGï¼Œä¸ä¸´åºŠæ¨ç†æµç¨‹ç›¸ä¸€è‡´ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…æ‹¬ç‰¹å®šä»»åŠ¡çš„æ™ºèƒ½ä½“ï¼Œç”¨äºå¤„ç†æ£€ç´¢ã€è‰ç¨¿ç”Ÿæˆã€è§†è§‰åˆ†æã€æ”¹è¿›å’Œåˆæˆç­‰åŠŸèƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨è‡ªåŠ¨æŒ‡æ ‡å’ŒLLMè¯„ä¼°ä¸­å‡ä¼˜äºå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ã€‚</li>
<li>è¯¥æ¡†æ¶ç”Ÿæˆçš„æŠ¥å‘Šæ›´å‡†ç¡®ã€ç»“æ„åŒ–ä¸”å¯è§£é‡Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09787">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5b6fc76342914b6d3f2e5a03bf920408.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca15ad29655d9b29d56a023a9fecff9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8de5231a3e051a1613603936afb36a55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b152baca2a391df72352cdd08832569e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-516d917a52ff0dacfdcc049e2d983871.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Explainability-Through-Human-Centric-Design-for-XAI-in-Lung-Cancer-Detection"><a href="#Explainability-Through-Human-Centric-Design-for-XAI-in-Lung-Cancer-Detection" class="headerlink" title="Explainability Through Human-Centric Design for XAI in Lung Cancer   Detection"></a>Explainability Through Human-Centric Design for XAI in Lung Cancer   Detection</h2><p><strong>Authors:Amy Rafferty, Rishi Ramaesh, Ajitha Rajan</strong></p>
<p>Deep learning models have shown promise in lung pathology detection from chest X-rays, but widespread clinical adoption remains limited due to opaque model decision-making. In prior work, we introduced ClinicXAI, a human-centric, expert-guided concept bottleneck model (CBM) designed for interpretable lung cancer diagnosis. We now extend that approach and present XpertXAI, a generalizable expert-driven model that preserves human-interpretable clinical concepts while scaling to detect multiple lung pathologies. Using a high-performing InceptionV3-based classifier and a public dataset of chest X-rays with radiology reports, we compare XpertXAI against leading post-hoc explainability methods and an unsupervised CBM, XCBs. We assess explanations through comparison with expert radiologist annotations and medical ground truth. Although XpertXAI is trained for multiple pathologies, our expert validation focuses on lung cancer. We find that existing techniques frequently fail to produce clinically meaningful explanations, omitting key diagnostic features and disagreeing with radiologist judgments. XpertXAI not only outperforms these baselines in predictive accuracy but also delivers concept-level explanations that better align with expert reasoning. While our focus remains on explainability in lung cancer detection, this work illustrates how human-centric model design can be effectively extended to broader diagnostic contexts - offering a scalable path toward clinically meaningful explainable AI in medical diagnostics. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨èƒ¸éƒ¨Xå…‰ç‰‡ä¸­æ£€æµ‹è‚ºéƒ¨ç—…å˜æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç”±äºæ¨¡å‹å†³ç­–ä¸é€æ˜ï¼Œå…¶åœ¨ä¸´åºŠä¸Šçš„å¹¿æ³›åº”ç”¨ä»ç„¶æœ‰é™ã€‚å…ˆå‰ï¼Œæˆ‘ä»¬å¼•å…¥äº†ClinicXAIï¼Œä¸€ç§ä»¥äººä¸ºä¸­å¿ƒã€ä¸“å®¶æŒ‡å¯¼çš„æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMï¼‰ï¼Œæ—¨åœ¨å®ç°å¯è§£é‡Šçš„è‚ºç™Œè¯Šæ–­ã€‚ç°åœ¨æˆ‘ä»¬æ‰©å±•äº†è¯¥æ–¹æ³•ï¼Œå¹¶æ¨å‡ºäº†XpertXAIï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨ä¸“å®¶é©±åŠ¨æ¨¡å‹ï¼Œåœ¨ä¿æŒäººç±»å¯è§£é‡Šçš„ä¸´åºŠæ¦‚å¿µçš„åŒæ—¶ï¼Œèƒ½å¤Ÿæ‰©å±•ä»¥æ£€æµ‹å¤šç§è‚ºéƒ¨ç—…å˜ã€‚æˆ‘ä»¬é‡‡ç”¨é«˜æ€§èƒ½çš„åŸºäºInceptionV3çš„åˆ†ç±»å™¨ä»¥åŠå¸¦æœ‰æ”¾å°„å­¦æŠ¥å‘Šçš„å…¬å…±èƒ¸éƒ¨Xå…‰æ•°æ®é›†ï¼Œå°†XpertXAIä¸é¢†å…ˆçš„äº‹åè§£é‡Šæ–¹æ³•ä»¥åŠæ— ç›‘ç£CBMï¼ˆXCBsï¼‰è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬é€šè¿‡ä¸“å®¶æ”¾å°„ç§‘åŒ»ç”Ÿæ³¨é‡Šå’ŒåŒ»å­¦çœŸç›¸æ¥è¯„ä¼°è§£é‡Šã€‚å°½ç®¡XpertXAIæ˜¯ä¸ºå¤šç§ç—…å˜è€Œè®­ç»ƒçš„ï¼Œä½†æˆ‘ä»¬çš„ä¸“å®¶éªŒè¯ä¸»è¦é›†ä¸­åœ¨è‚ºç™Œä¸Šã€‚æˆ‘ä»¬å‘ç°ç°æœ‰æŠ€æœ¯å¾€å¾€æ— æ³•äº§ç”Ÿå…·æœ‰ä¸´åºŠæ„ä¹‰çš„è§£é‡Šï¼Œå®ƒä»¬ä¼šé—æ¼å…³é”®è¯Šæ–­ç‰¹å¾å¹¶ä¸æ”¾å°„ç§‘åŒ»ç”Ÿåˆ¤æ–­ç›¸æ‚–ã€‚XpertXAIä¸ä»…åœ¨é¢„æµ‹å‡†ç¡®æ€§æ–¹é¢è¶…è¶Šè¿™äº›åŸºçº¿ï¼Œè€Œä¸”æä¾›ä¸ä¸“å®¶æ¨ç†æ›´ç›¸ç¬¦çš„æ¦‚å¿µå±‚é¢è§£é‡Šã€‚è™½ç„¶æˆ‘ä»¬çš„é‡ç‚¹ä»ç„¶æ˜¯è‚ºç™Œæ£€æµ‹ä¸­çš„å¯è§£é‡Šæ€§ï¼Œä½†è¿™é¡¹å·¥ä½œè¯´æ˜äº†ä»¥äººä¸ºä¸­å¿ƒçš„è®¾è®¡æ¨¡å‹å¦‚ä½•æœ‰æ•ˆåœ°æ‰©å±•åˆ°æ›´å¹¿æ³›çš„è¯Šæ–­èƒŒæ™¯ï¼Œä¸ºä¸´åºŠä¸Šæœ‰æ„ä¹‰çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½åœ¨åŒ»å­¦è¯Šæ–­ä¸­æä¾›äº†ä¸€æ¡å¯æ‰©å±•çš„é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09755v1">PDF</a> </p>
<p><strong>Summary</strong><br>    å¼•å…¥ClinicXAIæ¨¡å‹è¿›è¡Œè‚ºç™Œè¯Šæ–­åï¼Œç°åœ¨æ‰©å±•è¯¥æ–¹æ¡ˆï¼Œæå‡ºXpertXAIæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨å¯æ¨å¹¿çš„ä¸“å®¶é©±åŠ¨æ–¹å¼ï¼Œæ—¢ä¿ç•™äº†è§£é‡Šæ€§ä¸´åºŠæ¦‚å¿µï¼Œåˆèƒ½æ‰©å±•åˆ°æ£€æµ‹å¤šç§è‚ºéƒ¨ç—…å˜ã€‚ä¸é¡¶å°–äº‹åè§£é‡Šæ–¹æ³•å’Œæ— ç›‘ç£CBMç›¸æ¯”ï¼ŒXpertXAIä½¿ç”¨é«˜æ€§èƒ½çš„InceptionV3åˆ†ç±»å™¨ä¸å…¬å…±èƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†å’Œæ”¾å°„å­¦æŠ¥å‘Šè¿›è¡Œæ¯”è¾ƒè¯„ä¼°ã€‚å°½ç®¡XpertXAIç»è¿‡å¤šç§ç—…ç†è®­ç»ƒï¼Œä½†ä¸“å®¶éªŒè¯ä¸»è¦é›†ä¸­åœ¨è‚ºç™Œä¸Šã€‚ç°æœ‰æŠ€æœ¯å¾€å¾€æ— æ³•äº§ç”Ÿå…·æœ‰ä¸´åºŠæ„ä¹‰çš„è§£é‡Šï¼Œé—æ¼å…³é”®è¯Šæ–­ç‰¹å¾å¹¶ä¸æ”¾å°„ç§‘åŒ»ç”Ÿåˆ¤æ–­ç›¸æ‚–ã€‚XpertXAIä¸ä»…é¢„æµ‹ç²¾åº¦è¶…è¿‡åŸºçº¿æ°´å¹³ï¼Œè€Œä¸”æä¾›ä¸ä¸“å®¶æ¨ç†æ›´ç›¸ç¬¦çš„æ¦‚å¿µå±‚é¢è§£é‡Šã€‚è™½ç„¶é‡ç‚¹ä»ç„¶æ˜¯è‚ºç™Œæ£€æµ‹ä¸­çš„è§£é‡Šæ€§ï¼Œä½†è¿™é¡¹å·¥ä½œå±•ç¤ºäº†ä»¥äººä¸ºä¸­å¿ƒçš„è®¾è®¡æ–¹æ¡ˆå¦‚ä½•æœ‰æ•ˆåœ°æ‰©å±•åˆ°æ›´å¹¿æ³›çš„è¯Šæ–­ç¯å¢ƒï¼Œä¸ºä¸´åºŠä¸Šæœ‰æ„ä¹‰çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½åœ¨åŒ»å­¦è¯Šæ–­ä¸­çš„å‘å±•æä¾›äº†å¯æ‰©å±•è·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>XpertXAIæ¨¡å‹æ˜¯ä¸€ä¸ªæ–°çš„ã€åŸºäºä¸“å®¶é©±åŠ¨çš„æ¨¡å‹ï¼Œæ—¨åœ¨è§£é‡Šè‚ºéƒ¨ç–¾ç—…çš„è¯Šæ–­è¿‡ç¨‹ã€‚å®ƒåŸºäºå…ˆå‰ClinicXAIæ¨¡å‹çš„åŸºç¡€ä¸Šè¿›è¡Œæ‰©å±•ã€‚</li>
<li>XpertXAIåœ¨é¢„æµ‹ç²¾åº¦ä¸Šè¶…è¿‡äº†ç°æœ‰æŠ€æœ¯ã€‚å®ƒé€šè¿‡å¼•å…¥é«˜æ€§èƒ½çš„InceptionV3åˆ†ç±»å™¨å®ç°å¯¹èƒ¸éƒ¨Xå°„çº¿å›¾åƒçš„ç²¾å‡†åˆ†ç±»å’Œè¯Šæ–­ã€‚</li>
<li>XpertXAIæä¾›äº†ä¸€ä¸ªæ¦‚å¿µå±‚é¢çš„è§£é‡Šæ¡†æ¶ï¼Œä½¿å¾—è¯Šæ–­è¿‡ç¨‹æ›´ç¬¦åˆä¸“å®¶æ¨ç†æ¨¡å¼ã€‚è¯¥æ¨¡å‹é€šè¿‡ç»“åˆåŒ»å­¦çŸ¥è¯†å’Œäººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œç”Ÿæˆäº†å…·æœ‰ä¸´åºŠæ„ä¹‰çš„è§£é‡Šã€‚</li>
<li>ä¸å…¶ä»–è§£é‡Šæ–¹æ³•ç›¸æ¯”ï¼Œç°æœ‰çš„æŠ€æœ¯å¾€å¾€æ— æ³•äº§ç”Ÿä¸ä¸“å®¶åˆ¤æ–­ä¸€è‡´çš„è§£é‡Šç»“æœã€‚å®ƒä»¬å¯èƒ½é—æ¼å…³é”®çš„è¯Šæ–­ç‰¹å¾æˆ–ä¸æ”¾å°„ç§‘åŒ»ç”Ÿçš„åˆ¤æ–­ç›¸æ‚–ã€‚è€ŒXpertXAIèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è¿™äº›å…³é”®ç‰¹å¾å¹¶ç»™å‡ºå‡†ç¡®çš„è§£é‡Šã€‚</li>
<li>è™½ç„¶æœ¬ç ”ç©¶çš„é‡ç‚¹åœ¨äºè‚ºç™Œæ£€æµ‹ä¸­çš„è§£é‡Šæ€§ï¼Œä½†æ‰€å±•ç¤ºçš„ä»¥äººä¸ºä¸­å¿ƒçš„è®¾è®¡æ€è·¯å¯åº”ç”¨äºæ›´å¹¿æ³›çš„åŒ»å­¦è¯Šæ–­é¢†åŸŸã€‚è¿™è¡¨æ˜è¯¥æ¨¡å‹å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>XpertXAIæ¨¡å‹ä½¿ç”¨å…¬å…±æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¿™äº›æ•°æ®é›†åŒ…å«èƒ¸éƒ¨Xå°„çº¿å›¾åƒå’Œç›¸å…³çš„æ”¾å°„å­¦æŠ¥å‘Šã€‚è¿™ä¸ºæ¨¡å‹çš„è®­ç»ƒå’ŒéªŒè¯æä¾›äº†çœŸå®ä¸–ç•Œçš„æ•°æ®æ”¯æŒã€‚é€šè¿‡ä¸è¿™äº›æ•°æ®é›†è¿›è¡Œäº¤äº’è®­ç»ƒï¼Œæ¨¡å‹çš„è¯Šæ–­å‡†ç¡®æ€§å¾—ä»¥è¿›ä¸€æ­¥æé«˜ã€‚åŒæ—¶å¢å¼ºäº†æ¨¡å‹çš„ä¸´åºŠæ„ä¹‰åŠå…¶è§£é‡Šçš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09755">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1093dda596240bd029baac0c02b9fbc2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04edd3a2425877ee8a7de7b606daa2b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88477cb86d423ef7b4d61fb3acace7c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a36c6b91cf135fed68bec7d5d06a8322.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89f48ee1d6aa8edb5eeb123935fff8df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98fc825e71211d41d027322ebc34fc47.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="A-Trust-Guided-Approach-to-MR-Image-Reconstruction-with-Side-Information"><a href="#A-Trust-Guided-Approach-to-MR-Image-Reconstruction-with-Side-Information" class="headerlink" title="A Trust-Guided Approach to MR Image Reconstruction with Side Information"></a>A Trust-Guided Approach to MR Image Reconstruction with Side Information</h2><p><strong>Authors:Arda AtalÄ±k, Sumit Chopra, Daniel K. Sodickson</strong></p>
<p>Reducing MRI scan times can improve patient care and lower healthcare costs. Many acceleration methods are designed to reconstruct diagnostic-quality images from sparse k-space data, via an ill-posed or ill-conditioned linear inverse problem (LIP). To address the resulting ambiguities, it is crucial to incorporate prior knowledge into the optimization problem, e.g., in the form of regularization. Another form of prior knowledge less commonly used in medical imaging is the readily available auxiliary data (a.k.a. side information) obtained from sources other than the current acquisition. In this paper, we present the Trust- Guided Variational Network (TGVN), an end-to-end deep learning framework that effectively and reliably integrates side information into LIPs. We demonstrate its effectiveness in multi-coil, multi-contrast MRI reconstruction, where incomplete or low-SNR measurements from one contrast are used as side information to reconstruct high-quality images of another contrast from heavily under-sampled data. TGVN is robust across different contrasts, anatomies, and field strengths. Compared to baselines utilizing side information, TGVN achieves superior image quality while preserving subtle pathological features even at challenging acceleration levels, drastically speeding up acquisition while minimizing hallucinations. Source code and dataset splits are available on github.com&#x2F;sodicksonlab&#x2F;TGVN. </p>
<blockquote>
<p>å‡å°‘MRIæ‰«ææ—¶é—´å¯ä»¥æ”¹å–„æ‚£è€…æŠ¤ç†å¹¶é™ä½åŒ»ç–—æˆæœ¬ã€‚è®¸å¤šåŠ é€Ÿæ–¹æ³•æ—¨åœ¨é€šè¿‡ä¸é€‚å®šæˆ–ç—…æ€çš„çº¿æ€§é€†é—®é¢˜ï¼ˆLIPï¼‰ä»ç¨€ç–çš„kç©ºé—´æ•°æ®ä¸­é‡å»ºè¯Šæ–­è´¨é‡çš„å›¾åƒã€‚ä¸ºäº†è§£å†³ç”±æ­¤äº§ç”Ÿçš„æ¨¡ç³Šæ€§ï¼Œå°†å…ˆéªŒçŸ¥è¯†èå…¥ä¼˜åŒ–é—®é¢˜è‡³å…³é‡è¦ï¼Œä¾‹å¦‚ä»¥æ­£åˆ™åŒ–çš„å½¢å¼ã€‚å¦ä¸€ç§åœ¨åŒ»å­¦æˆåƒä¸­è¾ƒå°‘ä½¿ç”¨çš„å…ˆéªŒçŸ¥è¯†æ˜¯æ˜“äºè·å¾—çš„è¾…åŠ©æ•°æ®ï¼ˆä¹Ÿç§°ä¸ºä¾§ä¿¡æ¯ï¼‰ï¼Œè¿™äº›è¾…åŠ©æ•°æ®æ¥æºäºå½“å‰é‡‡é›†ä¹‹å¤–çš„å…¶ä»–æ¥æºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Trust-Guided Variational Networkï¼ˆTGVNï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¸”å¯é åœ°å°†ä¾§ä¿¡æ¯é›†æˆåˆ°LIPsä¸­ã€‚æˆ‘ä»¬åœ¨å¤šçº¿åœˆã€å¤šå¯¹æ¯”åº¦MRIé‡å»ºä¸­å±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ï¼Œå…¶ä¸­æ¥è‡ªä¸€ç§å¯¹æ¯”åº¦çš„ä¸å®Œæ•´æˆ–ä½SNRæµ‹é‡å€¼è¢«ç”¨ä½œä¾§ä¿¡æ¯ï¼Œä»¥ä»é«˜åº¦æ¬ é‡‡æ ·çš„æ•°æ®ä¸­é‡å»ºå¦ä¸€ç§å¯¹æ¯”åº¦çš„é«˜è´¨é‡å›¾åƒã€‚TGVNåœ¨ä¸åŒçš„å¯¹æ¯”åº¦ã€è§£å‰–ç»“æ„å’Œç£åœºå¼ºåº¦ä¹‹é—´è¡¨ç°å‡ºç¨³å¥æ€§ã€‚ä¸åˆ©ç”¨ä¾§ä¿¡æ¯çš„åŸºçº¿ç›¸æ¯”ï¼ŒTGVNåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŠ é€Ÿçº§åˆ«ä¸‹å®ç°äº†æ›´é«˜çš„å›¾åƒè´¨é‡ï¼Œå³ä½¿åœ¨ç»†å¾®çš„ç—…ç†æ€§ç‰¹å¾ä¸‹ä¹Ÿèƒ½ä¿æŒå…¶æ¸…æ™°åº¦ï¼Œæå¤§åœ°åŠ å¿«äº†é‡‡é›†é€Ÿåº¦å¹¶å‡å°‘äº†å¹»è§‰ã€‚æºä»£ç å’Œæ•°æ®é›†åˆ†å‰²å¯åœ¨github.com&#x2F;sodicksonlab&#x2F;TGVNæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03021v2">PDF</a> 27 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTrust-Guided Variational Networkï¼ˆTGVNï¼‰çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå®ƒèƒ½æœ‰æ•ˆåœ°å°†è¾…åŠ©ä¿¡æ¯èå…¥çº¿æ€§é€†é—®é¢˜ä¸­ï¼Œç”¨äºMRIæ‰«ææ—¶é—´çš„å‡å°‘å’Œå›¾åƒé‡å»ºã€‚è¯¥æ¡†æ¶åœ¨å¤šçº¿åœˆã€å¤šå¯¹æ¯”åº¦çš„MRIé‡å»ºä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåˆ©ç”¨ä¸€ç§å¯¹æ¯”åº¦çš„ä½è´¨é‡æµ‹é‡æ•°æ®ä½œä¸ºè¾…åŠ©ä¿¡æ¯ï¼Œé‡å»ºå¦ä¸€ç§å¯¹æ¯”åº¦çš„é«˜è´¨é‡å›¾åƒã€‚TGVNåœ¨ä¸åŒçš„å¯¹æ¯”åº¦ã€è§£å‰–ç»“æ„å’Œç£åœºå¼ºåº¦ä¸‹å…·æœ‰ç¨³å¥æ€§ï¼Œç›¸è¾ƒäºå…¶ä»–ä½¿ç”¨è¾…åŠ©ä¿¡æ¯çš„åŸºçº¿æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æŒ‘æˆ˜æ€§çš„åŠ é€Ÿçº§åˆ«ä¸‹å®ç°æ›´ä¼˜è´¨çš„å›¾åƒè´¨é‡ï¼ŒåŒæ—¶ä¿ç•™å¾®å¦™çš„ç—…ç†ç‰¹å¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TGVNæ¡†æ¶èƒ½æœ‰æ•ˆå‡å°‘MRIæ‰«ææ—¶é—´ï¼Œæé«˜æ‚£è€…æŠ¤ç†å’Œé™ä½åŒ»ç–—ä¿å¥æˆæœ¬ã€‚</li>
<li>TGVNé€šè¿‡å°†è¾…åŠ©ä¿¡æ¯èå…¥çº¿æ€§é€†é—®é¢˜ä¸­ï¼Œæé«˜äº†å›¾åƒé‡å»ºçš„è´¨é‡ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å¤šå¯¹æ¯”åº¦çš„MRIé‡å»ºä¸­æœ‰å‡ºè‰²è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨ä¸€ç§å¯¹æ¯”åº¦çš„æ•°æ®ä½œä¸ºè¾…åŠ©ä¿¡æ¯æ¥é‡å»ºå¦ä¸€ç§å¯¹æ¯”åº¦çš„æ•°æ®ã€‚</li>
<li>TGVNå…·æœ‰åœ¨ä¸åŒå¯¹æ¯”åº¦ã€è§£å‰–ç»“æ„å’Œç£åœºå¼ºåº¦ä¸‹çš„ç¨³å¥æ€§ã€‚</li>
<li>TGVNç›¸æ¯”å…¶ä»–æ–¹æ³•èƒ½å¤Ÿå®ç°æ›´ä¼˜è´¨çš„å›¾åƒè´¨é‡ï¼Œå°¤å…¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŠ é€Ÿçº§åˆ«ä¸‹ã€‚</li>
<li>TGVNæ¡†æ¶èƒ½ä¿ç•™å¾®å¦™çš„ç—…ç†ç‰¹å¾ï¼Œè¿™æ˜¯å…¶ä»–æ–¹æ³•å¯èƒ½å¿½ç•¥çš„ç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03021">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-594af935d5b1131c35e35f5df06b43aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27e6094352f58b7e9cda1aeae0cd253a.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-17/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-c2b715b0ac6b01e3fd6dca6f0361cb99.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-17  FlexSpeech Towards Stable, Controllable and Expressive Text-to-Speech
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-17/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-156db5c1eb49e129709b6438e64a9ff9.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-17  3D-Fixup Advancing Photo Editing with 3D Priors
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18293.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
