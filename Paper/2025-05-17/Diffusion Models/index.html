<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-17  3D-Fixup Advancing Photo Editing with 3D Priors">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-156db5c1eb49e129709b6438e64a9ff9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    49 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-17-æ›´æ–°"><a href="#2025-05-17-æ›´æ–°" class="headerlink" title="2025-05-17 æ›´æ–°"></a>2025-05-17 æ›´æ–°</h1><h2 id="3D-Fixup-Advancing-Photo-Editing-with-3D-Priors"><a href="#3D-Fixup-Advancing-Photo-Editing-with-3D-Priors" class="headerlink" title="3D-Fixup: Advancing Photo Editing with 3D Priors"></a>3D-Fixup: Advancing Photo Editing with 3D Priors</h2><p><strong>Authors:Yen-Chi Cheng, Krishna Kumar Singh, Jae Shin Yoon, Alex Schwing, Liangyan Gui, Matheus Gadelha, Paul Guerrero, Nanxuan Zhao</strong></p>
<p>Despite significant advances in modeling image priors via diffusion models, 3D-aware image editing remains challenging, in part because the object is only specified via a single image. To tackle this challenge, we propose 3D-Fixup, a new framework for editing 2D images guided by learned 3D priors. The framework supports difficult editing situations such as object translation and 3D rotation. To achieve this, we leverage a training-based approach that harnesses the generative power of diffusion models. As video data naturally encodes real-world physical dynamics, we turn to video data for generating training data pairs, i.e., a source and a target frame. Rather than relying solely on a single trained model to infer transformations between source and target frames, we incorporate 3D guidance from an Image-to-3D model, which bridges this challenging task by explicitly projecting 2D information into 3D space. We design a data generation pipeline to ensure high-quality 3D guidance throughout training. Results show that by integrating these 3D priors, 3D-Fixup effectively supports complex, identity coherent 3D-aware edits, achieving high-quality results and advancing the application of diffusion models in realistic image manipulation. The code is provided at <a target="_blank" rel="noopener" href="https://3dfixup.github.io/">https://3dfixup.github.io/</a> </p>
<blockquote>
<p>å°½ç®¡é€šè¿‡æ‰©æ•£æ¨¡å‹å¯¹å›¾åƒå…ˆéªŒå»ºæ¨¡å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç”±äºå¯¹è±¡ä»…é€šè¿‡å•å¼ å›¾åƒæŒ‡å®šï¼Œ3Dæ„ŸçŸ¥å›¾åƒç¼–è¾‘ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†3D-Fixupï¼Œè¿™æ˜¯ä¸€ä¸ªç”±å­¦ä¹ åˆ°çš„3Då…ˆéªŒå¼•å¯¼çš„æ–°æ¡†æ¶ï¼Œç”¨äºç¼–è¾‘2Då›¾åƒã€‚è¯¥æ¡†æ¶æ”¯æŒå¯¹è±¡å¹³ç§»å’Œ3Dæ—‹è½¬ç­‰å›°éš¾ç¼–è¾‘æƒ…å†µã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åŸºäºè®­ç»ƒçš„æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚ç”±äºè§†é¢‘æ•°æ®å¤©ç„¶åœ°ç¼–ç äº†ç°å®ä¸–ç•Œçš„ç‰©ç†åŠ¨æ€ï¼Œæˆ‘ä»¬è½¬å‘è§†é¢‘æ•°æ®æ¥ç”Ÿæˆè®­ç»ƒæ•°æ®å¯¹ï¼Œå³æºå¸§å’Œç›®æ ‡å¸§ã€‚æˆ‘ä»¬ä¸ä»…ä»…ä¾èµ–å•ä¸€çš„è®­ç»ƒæ¨¡å‹æ¥æ¨æ–­æºå¸§å’Œç›®æ ‡å¸§ä¹‹é—´çš„è½¬æ¢ï¼Œè¿˜ç»“åˆäº†æ¥è‡ªå›¾åƒåˆ°3Dæ¨¡å‹çš„3DæŒ‡å¯¼ï¼Œé€šè¿‡å°†2Dä¿¡æ¯æ˜¾å¼æŠ•å½±åˆ°3Dç©ºé—´æ¥åº”å¯¹è¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ•°æ®ç”Ÿæˆç®¡é“ï¼Œä»¥ç¡®ä¿åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­æä¾›é«˜è´¨é‡çš„3DæŒ‡å¯¼ã€‚ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æ•´åˆè¿™äº›3Då…ˆéªŒçŸ¥è¯†ï¼Œ3D-Fixupæœ‰æ•ˆåœ°æ”¯æŒäº†å¤æ‚ã€èº«ä»½ä¸€è‡´çš„3Dæ„ŸçŸ¥ç¼–è¾‘ï¼Œå–å¾—äº†é«˜è´¨é‡çš„ç»“æœï¼Œå¹¶æ¨åŠ¨äº†æ‰©æ•£æ¨¡å‹åœ¨çœŸå®å›¾åƒæ“ä½œä¸­çš„åº”ç”¨ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨ï¼š[<a target="_blank" rel="noopener" href="https://3dfixup.github.io/]%E4%B8%8A%E3%80%82">https://3dfixup.github.io/]ä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10566v1">PDF</a> SIGGRAPH 2025. Project page: <a target="_blank" rel="noopener" href="https://3dfixup.github.io/">https://3dfixup.github.io/</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æå‡ºä¸€ç§åä¸º3D-Fixupçš„æ–°æ¡†æ¶ï¼Œåˆ©ç”¨å­¦ä¹ åˆ°çš„3Då…ˆéªŒçŸ¥è¯†å¯¹äºŒç»´å›¾åƒè¿›è¡Œç¼–è¾‘ã€‚è¯¥æ¡†æ¶æ”¯æŒç‰©ä½“å¹³ç§»å’Œä¸‰ç»´æ—‹è½¬ç­‰å¤æ‚ç¼–è¾‘æƒ…å†µã€‚é€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œç»“åˆè§†é¢‘æ•°æ®ç”Ÿæˆè®­ç»ƒæ•°æ®å¯¹ï¼Œå°†æºå¸§å’Œç›®æ ‡å¸§ä¹‹é—´çš„è½¬æ¢è¿›è¡Œæ¨æ–­ã€‚åŒæ—¶èå…¥Image-to-3Dæ¨¡å‹çš„3DæŒ‡å¯¼ï¼Œå°†äºŒç»´ä¿¡æ¯æ˜¾å¼æŠ•å½±åˆ°ä¸‰ç»´ç©ºé—´ï¼Œå®ç°é«˜è´¨é‡çš„ä¸‰ç»´æŒ‡å¯¼ã€‚è¯¥æ¡†æ¶èƒ½æœ‰æ•ˆæ”¯æŒå¤æ‚ã€è¿è´¯çš„ä¸‰ç»´æ„ŸçŸ¥ç¼–è¾‘ï¼Œæé«˜æ‰©æ•£æ¨¡å‹åœ¨çœŸå®å›¾åƒæ“ä½œä¸­çš„åº”ç”¨è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>3D-Fixupæ˜¯ä¸€ä¸ªåˆ©ç”¨å­¦ä¹ çš„3Då…ˆéªŒçŸ¥è¯†æŒ‡å¯¼äºŒç»´å›¾åƒç¼–è¾‘çš„æ–°æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶è§£å†³äº†é€šè¿‡å•ä¸ªå›¾åƒæŒ‡å®šç‰©ä½“è¿›è¡Œç¼–è¾‘çš„æŒ‘æˆ˜ã€‚</li>
<li>åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å’Œè§†é¢‘æ•°æ®ç”Ÿæˆè®­ç»ƒæ•°æ®å¯¹ï¼Œå®ç°æºå¸§å’Œç›®æ ‡å¸§ä¹‹é—´çš„è½¬æ¢æ¨æ–­ã€‚</li>
<li>èå…¥Image-to-3Dæ¨¡å‹çš„3DæŒ‡å¯¼ï¼Œæ˜¾å¼åœ°å°†äºŒç»´ä¿¡æ¯æŠ•å½±åˆ°ä¸‰ç»´ç©ºé—´ã€‚</li>
<li>è®¾è®¡äº†æ•°æ®ç”Ÿæˆç®¡é“ï¼Œç¡®ä¿åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­è·å¾—é«˜è´¨é‡çš„3DæŒ‡å¯¼ã€‚</li>
<li>3D-Fixupæ”¯æŒå¤æ‚çš„ã€è¿è´¯çš„ä¸‰ç»´æ„ŸçŸ¥ç¼–è¾‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f967dd4805bc8f2989c4d2ee7e8f052c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d7d460496369ff2c0bc997a25916b58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ef96f0eb1040fce6bbce97fb50bb7d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c14531e7a411986a0a33c17cb52aeddb.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Style-Customization-of-Text-to-Vector-Generation-with-Image-Diffusion-Priors"><a href="#Style-Customization-of-Text-to-Vector-Generation-with-Image-Diffusion-Priors" class="headerlink" title="Style Customization of Text-to-Vector Generation with Image Diffusion   Priors"></a>Style Customization of Text-to-Vector Generation with Image Diffusion   Priors</h2><p><strong>Authors:Peiying Zhang, Nanxuan Zhao, Jing Liao</strong></p>
<p>Scalable Vector Graphics (SVGs) are highly favored by designers due to their resolution independence and well-organized layer structure. Although existing text-to-vector (T2V) generation methods can create SVGs from text prompts, they often overlook an important need in practical applications: style customization, which is vital for producing a collection of vector graphics with consistent visual appearance and coherent aesthetics. Extending existing T2V methods for style customization poses certain challenges. Optimization-based T2V models can utilize the priors of text-to-image (T2I) models for customization, but struggle with maintaining structural regularity. On the other hand, feed-forward T2V models can ensure structural regularity, yet they encounter difficulties in disentangling content and style due to limited SVG training data.   To address these challenges, we propose a novel two-stage style customization pipeline for SVG generation, making use of the advantages of both feed-forward T2V models and T2I image priors. In the first stage, we train a T2V diffusion model with a path-level representation to ensure the structural regularity of SVGs while preserving diverse expressive capabilities. In the second stage, we customize the T2V diffusion model to different styles by distilling customized T2I models. By integrating these techniques, our pipeline can generate high-quality and diverse SVGs in custom styles based on text prompts in an efficient feed-forward manner. The effectiveness of our method has been validated through extensive experiments. The project page is <a target="_blank" rel="noopener" href="https://customsvg.github.io/">https://customsvg.github.io</a>. </p>
<blockquote>
<p>å¯æ‰©å±•çŸ¢é‡å›¾å½¢ï¼ˆSVGï¼‰å› å…¶åˆ†è¾¨ç‡ç‹¬ç«‹å’Œå±‚æ¬¡ç»“æ„æ¸…æ™°è€Œæ·±å—è®¾è®¡å¸ˆå–œçˆ±ã€‚å°½ç®¡ç°æœ‰çš„æ–‡æœ¬åˆ°çŸ¢é‡ï¼ˆT2Vï¼‰ç”Ÿæˆæ–¹æ³•å¯ä»¥æ ¹æ®æ–‡æœ¬æç¤ºç”ŸæˆSVGï¼Œä½†å®ƒä»¬å¾€å¾€å¿½ç•¥äº†å®é™…åº”ç”¨ä¸­çš„ä¸€ä¸ªé‡è¦éœ€æ±‚ï¼šæ ·å¼å®šåˆ¶ã€‚è¿™å¯¹äºç”Ÿäº§å…·æœ‰ä¸€è‡´å¤–è§‚å’Œè¿è´¯ç¾å­¦çš„çŸ¢é‡å›¾å½¢é›†åˆè‡³å…³é‡è¦ã€‚å¯¹ç°æœ‰çš„T2Væ–¹æ³•è¿›è¡Œæ ·å¼å®šåˆ¶æ‰©å±•é¢ä¸´ä¸€å®šçš„æŒ‘æˆ˜ã€‚åŸºäºä¼˜åŒ–çš„T2Væ¨¡å‹å¯ä»¥åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†è¿›è¡Œå®šåˆ¶ï¼Œä½†åœ¨ä¿æŒç»“æ„è§„å¾‹æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚å¦ä¸€æ–¹é¢ï¼Œå‰é¦ˆT2Væ¨¡å‹å¯ä»¥ç¡®ä¿ç»“æ„è§„å¾‹æ€§ï¼Œä½†ç”±äºSVGè®­ç»ƒæ•°æ®çš„æœ‰é™ï¼Œå®ƒä»¬åœ¨è§£å¼€å†…å®¹å’Œæ ·å¼æ—¶é‡åˆ°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºSVGç”Ÿæˆçš„ä¸¤é˜¶æ®µæ ·å¼å®šåˆ¶ç®¡é“ï¼Œåˆ©ç”¨å‰é¦ˆT2Væ¨¡å‹å’ŒT2Iå›¾åƒå…ˆéªŒçš„ä¼˜åŠ¿ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå…·æœ‰è·¯å¾„çº§åˆ«è¡¨ç¤ºçš„T2Væ‰©æ•£æ¨¡å‹ï¼Œä»¥ç¡®ä¿SVGçš„ç»“æ„è§„å¾‹æ€§ï¼ŒåŒæ—¶ä¿ç•™å¤šæ ·çš„è¡¨è¾¾èƒ½åŠ›ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡è’¸é¦å®šåˆ¶T2Iæ¨¡å‹æ¥å®šåˆ¶T2Væ‰©æ•£æ¨¡å‹çš„ä¸åŒé£æ ¼ã€‚é€šè¿‡æ•´åˆè¿™äº›æŠ€æœ¯ï¼Œæˆ‘ä»¬çš„ç®¡é“èƒ½å¤Ÿä»¥é«˜æ•ˆçš„å‰é¦ˆæ–¹å¼ï¼ŒåŸºäºæ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„å®šåˆ¶æ ·å¼SVGã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§å·²ç»é€šè¿‡å¤§é‡çš„å®éªŒå¾—åˆ°äº†éªŒè¯ã€‚é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://customsvg.github.io./">https://customsvg.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10558v1">PDF</a> Accepted by SIGGRAPH 2025 (Conference Paper). Project page:   <a target="_blank" rel="noopener" href="https://customsvg.github.io/">https://customsvg.github.io</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SVGè®¾è®¡çš„é‡è¦æ€§ä»¥åŠç°æœ‰æ–‡æœ¬è½¬çŸ¢é‡ï¼ˆT2Vï¼‰ç”Ÿæˆæ–¹æ³•çš„å±€é™æ€§ã€‚é’ˆå¯¹å®é™…åº”ç”¨ä¸­çš„æ ·å¼å®šåˆ¶éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ä¸¤é˜¶æ®µSVGæ ·å¼å®šåˆ¶æµç¨‹ã€‚ç¬¬ä¸€é˜¶æ®µè®­ç»ƒå¸¦æœ‰è·¯å¾„çº§åˆ«è¡¨ç¤ºçš„T2Væ‰©æ•£æ¨¡å‹ï¼Œç¡®ä¿SVGçš„ç»“æ„è§„å¾‹æ€§å’Œä¸°å¯Œçš„è¡¨è¾¾èƒ½åŠ›ã€‚ç¬¬äºŒé˜¶æ®µé€šè¿‡è’¸é¦å®šåˆ¶åŒ–çš„T2Iæ¨¡å‹å¯¹T2Væ‰©æ•£æ¨¡å‹è¿›è¡Œé£æ ¼å®šåˆ¶ã€‚è¯¥é¡¹ç›®å®ç°äº†é«˜æ•ˆçš„å‰é¦ˆæ–¹å¼ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„å®šåˆ¶é£æ ¼SVGã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SVGå› å…¶åˆ†è¾¨ç‡ç‹¬ç«‹æ€§å’Œåˆ†å±‚ç»“æ„å—åˆ°è®¾è®¡å¸ˆé’çã€‚</li>
<li>ç°æœ‰æ–‡æœ¬è½¬çŸ¢é‡ï¼ˆT2Vï¼‰ç”Ÿæˆæ–¹æ³•è™½èƒ½ä»æ–‡æœ¬æç¤ºç”ŸæˆSVGï¼Œä½†å¿½ç•¥äº†æ ·å¼å®šåˆ¶çš„é‡è¦æ€§ã€‚</li>
<li>æ ·å¼å®šåˆ¶å¯¹äºç”Ÿäº§å…·æœ‰ä¸€è‡´å¤–è§‚å’Œè¿è´¯ç¾å­¦çš„çŸ¢é‡å›¾å½¢è‡³å…³é‡è¦ã€‚</li>
<li>ä¼˜åŒ–åŸºäºæ–‡æœ¬è½¬å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹çš„T2Væ–¹æ³•å­˜åœ¨ä¿æŒç»“æ„è§„å¾‹æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>é¦ˆå‰T2Væ¨¡å‹èƒ½ä¿è¯ç»“æ„è§„å¾‹æ€§ï¼Œä½†åœ¨å†…å®¹ä¸æ ·å¼è§£è€¦æ–¹é¢é‡åˆ°å›°éš¾ï¼Œå› SVGè®­ç»ƒæ•°æ®æœ‰é™ã€‚</li>
<li>æå‡ºçš„ä¸¤é˜¶æ®µæ ·å¼å®šåˆ¶æµç¨‹ç»“åˆäº†T2Væ‰©æ•£æ¨¡å‹å’ŒT2Iå›¾åƒå…ˆéªŒçš„ä¼˜åŠ¿ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µè®­ç»ƒT2Væ‰©æ•£æ¨¡å‹ç¡®ä¿SVGçš„ç»“æ„è§„å¾‹æ€§å’Œä¸°å¯Œè¡¨è¾¾åŠ›ï¼›ç¬¬äºŒé˜¶æ®µé€šè¿‡è’¸é¦å®šåˆ¶åŒ–T2Iæ¨¡å‹è¿›è¡Œé£æ ¼å®šåˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10558">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-59ec967b370f9d910999323b283edf33.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a5b9fa9544cc4a686e0016e2f182b409.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c1f4692c7964efb1729393999ecd6f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bee8620fb1e4f5b2a78cffa9ba5af5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63b1b4656e7822f224ba1cdd7eca5620.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Does-Feasibility-Matter-Understanding-the-Impact-of-Feasibility-on-Synthetic-Training-Data"><a href="#Does-Feasibility-Matter-Understanding-the-Impact-of-Feasibility-on-Synthetic-Training-Data" class="headerlink" title="Does Feasibility Matter? Understanding the Impact of Feasibility on   Synthetic Training Data"></a>Does Feasibility Matter? Understanding the Impact of Feasibility on   Synthetic Training Data</h2><p><strong>Authors:Yiwen Liu, Jessica Bader, Jae Myung Kim</strong></p>
<p>With the development of photorealistic diffusion models, models trained in part or fully on synthetic data achieve progressively better results. However, diffusion models still routinely generate images that would not exist in reality, such as a dog floating above the ground or with unrealistic texture artifacts. We define the concept of feasibility as whether attributes in a synthetic image could realistically exist in the real-world domain; synthetic images containing attributes that violate this criterion are considered infeasible. Intuitively, infeasible images are typically considered out-of-distribution; thus, training on such images is expected to hinder a modelâ€™s ability to generalize to real-world data, and they should therefore be excluded from the training set whenever possible. However, does feasibility really matter? In this paper, we investigate whether enforcing feasibility is necessary when generating synthetic training data for CLIP-based classifiers, focusing on three target attributes: background, color, and texture. We introduce VariReal, a pipeline that minimally edits a given source image to include feasible or infeasible attributes given by the textual prompt generated by a large language model. Our experiments show that feasibility minimally affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference in top-1 accuracy across three fine-grained datasets. Also, the attribute matters on whether the feasible&#x2F;infeasible images adversarially influence the classification performance. Finally, mixing feasible and infeasible images in training datasets does not significantly impact performance compared to using purely feasible or infeasible datasets. </p>
<blockquote>
<p>éšç€è¶…çœŸå®æ‰©æ•£æ¨¡å‹çš„å‘å±•ï¼Œéƒ¨åˆ†æˆ–å®Œå…¨åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹å–å¾—äº†è¶Šæ¥è¶Šå¥½çš„ç»“æœã€‚ç„¶è€Œï¼Œæ‰©æ•£æ¨¡å‹ä»ç„¶ç»å¸¸ç”Ÿæˆåœ¨ç°å®ä¸­ä¸å­˜åœ¨çš„å›¾åƒï¼Œä¾‹å¦‚æ¼‚æµ®åœ¨åœ°é¢ä¸Šæ–¹çš„ç‹—æˆ–å…·æœ‰ä¸ç°å®çš„çº¹ç†ç‘•ç–µã€‚æˆ‘ä»¬å®šä¹‰å¯è¡Œæ€§çš„æ¦‚å¿µä¸ºåˆæˆå›¾åƒä¸­çš„å±æ€§æ˜¯å¦èƒ½åœ¨ç°å®ä¸–ç•Œä¸­çœŸå®å­˜åœ¨ï¼›å«æœ‰è¿åè¿™ä¸€æ ‡å‡†çš„å±æ€§çš„åˆæˆå›¾åƒè¢«è§†ä¸ºä¸å¯è¡Œã€‚ç›´è§‚åœ°è¯´ï¼Œä¸å¯è¡Œçš„å›¾åƒé€šå¸¸è¢«è®¤ä¸ºæ˜¯ç¦»ç¾¤å€¼ï¼›å› æ­¤ï¼Œåœ¨è¿™æ ·çš„å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒä¼šé˜»ç¢æ¨¡å‹å¯¹çœŸå®ä¸–ç•Œæ•°æ®çš„æ³›åŒ–èƒ½åŠ›ï¼Œå› æ­¤åº”å°½å¯èƒ½ä»è®­ç»ƒé›†ä¸­æ’é™¤ã€‚ä½†æ˜¯ï¼Œå¯è¡Œæ€§çœŸçš„é‡è¦å—ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åœ¨ç”ŸæˆåŸºäºCLIPçš„åˆ†ç±»å™¨çš„åˆæˆè®­ç»ƒæ•°æ®æ—¶æ˜¯å¦å¿…é¡»å¼ºåˆ¶æ‰§è¡Œå¯è¡Œæ€§ï¼Œé‡ç‚¹å…³æ³¨ä¸‰ä¸ªç›®æ ‡å±æ€§ï¼šèƒŒæ™¯ã€é¢œè‰²å’Œçº¹ç†ã€‚æˆ‘ä»¬å¼•å…¥äº†VariRealï¼Œä¸€ä¸ªç®¡é“ï¼Œå®ƒç¨å¾®ç¼–è¾‘ç»™å®šçš„æºå›¾åƒä»¥åŒ…å«ç”±å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬æç¤ºæ‰€ç»™å‡ºçš„å¯è¡Œæˆ–ä¸å¯è¡Œçš„å±æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå¯è¡Œæ€§å¯¹ä½¿ç”¨LoRAå¾®è°ƒCLIPçš„æ€§èƒ½å½±å“ç”šå¾®ï¼Œåœ¨ä¸‰ä¸ªç²¾ç»†æ•°æ®é›†ä¸Štop-1å‡†ç¡®ç‡å·®å¼‚å¤§å¤šä¸åˆ°0.3%ã€‚æ­¤å¤–ï¼Œå±æ€§å¾ˆé‡è¦ï¼Œå¯è¡Œæˆ–ä¸å¯è¡Œçš„å›¾åƒæ˜¯å¦ä¼šä¸åˆ©äºå½±å“åˆ†ç±»æ€§èƒ½ã€‚æœ€åï¼Œä¸çº¯ç²¹ä½¿ç”¨å¯è¡Œæˆ–ä¸å¯è¡Œçš„æ•°æ®é›†ç›¸æ¯”ï¼Œåœ¨è®­ç»ƒæ•°æ®é›†ä¸­æ··åˆå¯è¡Œå’Œä¸å¯è¡Œçš„å›¾åƒå¹¶ä¸ä¼šæ˜¾è‘—å½±å“æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10551v1">PDF</a> CVPRW 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºCLIPåˆ†ç±»å™¨çš„åˆæˆè®­ç»ƒæ•°æ®ç”Ÿæˆä¸­å¯è¡Œæ€§æ¦‚å¿µçš„é‡è¦æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå¯¹äºèƒŒæ™¯ã€é¢œè‰²å’Œçº¹ç†ç­‰ç›®æ ‡å±æ€§ï¼Œå¼ºåˆ¶å®æ–½å¯è¡Œæ€§å¯¹LoRAå¾®è°ƒåçš„CLIPæ€§èƒ½å½±å“ç”šå¾®ã€‚æ··åˆå¯è¡Œä¸ä¸å¯è¡Œçš„å›¾åƒåœ¨è®­ç»ƒæ•°æ®é›†ä¸­å¯¹æ€§èƒ½çš„å½±å“å¹¶ä¸æ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆæ•°æ®åœ¨è®­ç»ƒæ‰©æ•£æ¨¡å‹æ—¶é€æ¸å–å¾—æ›´å¥½çš„ç»“æœï¼Œä½†ä»ä¼šç”Ÿæˆä¸ç°å®ä¸ç¬¦çš„å›¾åƒã€‚</li>
<li>å®šä¹‰äº†â€œå¯è¡Œæ€§â€æ¦‚å¿µï¼Œå³åˆæˆå›¾åƒä¸­çš„å±æ€§æ˜¯å¦èƒ½åœ¨ç°å®ä¸–ç•Œä¸­å­˜åœ¨ã€‚</li>
<li>ä¸å¯è¡Œçš„å›¾åƒé€šå¸¸è¢«è®¤ä¸ºæ˜¯ç¦»ç¾¤å€¼ï¼Œè®­ç»ƒæ—¶å¯èƒ½é˜»ç¢æ¨¡å‹å¯¹çœŸå®ä¸–ç•Œæ•°æ®çš„æ³›åŒ–èƒ½åŠ›ï¼Œåº”å°½é‡é¿å…ç”¨äºè®­ç»ƒã€‚</li>
<li>åœ¨åŸºäºCLIPçš„åˆ†ç±»å™¨ä¸­ï¼Œå¼ºåˆ¶å®æ–½å¯è¡Œæ€§å¯¹æ€§èƒ½å½±å“è¾ƒå°ã€‚</li>
<li>åœ¨ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬æç¤ºæ—¶ï¼Œå¯è¡Œæˆ–ä¸å¯è¡Œçš„å±æ€§å¯¹å›¾åƒè¿›è¡Œæœ€å°é™åº¦çš„ç¼–è¾‘ã€‚</li>
<li>åœ¨ä¸‰ä¸ªç²¾ç»†æ•°æ®é›†ä¸Šï¼ŒLoRAå¾®è°ƒåçš„CLIPæ€§èƒ½åœ¨å¯è¡Œæ€§ä¸ä¸å¯è¡Œæ€§å›¾åƒä¹‹é—´çš„å·®å¼‚å¤§å¤šå°äº0.3%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5f9ed2e4338d93f677bb3bacc94b8747.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac89aaf1efcdd52839f434374bcf7cf5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fea94407ebfe9680c2db7ae7e744e31b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e662c55c5b62d4e338c97754a7ded2d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ORL-LDM-Offline-Reinforcement-Learning-Guided-Latent-Diffusion-Model-Super-Resolution-Reconstruction"><a href="#ORL-LDM-Offline-Reinforcement-Learning-Guided-Latent-Diffusion-Model-Super-Resolution-Reconstruction" class="headerlink" title="ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model   Super-Resolution Reconstruction"></a>ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model   Super-Resolution Reconstruction</h2><p><strong>Authors:Shijie Lyu</strong></p>
<p>With the rapid advancement of remote sensing technology, super-resolution image reconstruction is of great research and practical significance. Existing deep learning methods have made progress but still face limitations in handling complex scenes and preserving image details. This paper proposes a reinforcement learning-based latent diffusion model (LDM) fine-tuning method for remote sensing image super-resolution. The method constructs a reinforcement learning environment with states, actions, and rewards, optimizing decision objectives through proximal policy optimization (PPO) during the reverse denoising process of the LDM model. Experiments on the RESISC45 dataset show significant improvements over the baseline model in PSNR, SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11, and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural scenes. The results demonstrate the methodâ€™s effectiveness in enhancing super-resolution quality and adaptability across scenes. </p>
<blockquote>
<p>éšç€é¥æ„ŸæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œè¶…åˆ†è¾¨ç‡å›¾åƒé‡å»ºåœ¨ç ”ç©¶å’Œå®é™…åº”ç”¨ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç°æœ‰çš„æ·±åº¦å­¦ä¹ ç®—æ³•è™½å·²å–å¾—è¿›å±•ï¼Œä½†åœ¨å¤„ç†å¤æ‚åœºæ™¯å’Œä¿ç•™å›¾åƒç»†èŠ‚æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰å¾®è°ƒæ–¹æ³•ï¼Œç”¨äºé¥æ„Ÿå›¾åƒè¶…åˆ†è¾¨ç‡ã€‚è¯¥æ–¹æ³•æ„å»ºäº†ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼ŒåŒ…æ‹¬çŠ¶æ€ã€è¡Œä¸ºå’Œå¥–åŠ±ï¼Œåœ¨LDMæ¨¡å‹çš„åå‘å»å™ªè¿‡ç¨‹ä¸­é€šè¿‡è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ä¼˜åŒ–å†³ç­–ç›®æ ‡ã€‚åœ¨RESISC45æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€ç»“æ„ç›¸ä¼¼æ€§ï¼ˆSSIMï¼‰å’Œå±€éƒ¨æ„ŸçŸ¥å›¾åƒç›¸ä¼¼æ€§ï¼ˆLPIPSï¼‰ç­‰æŒ‡æ ‡ä¸Šè¾ƒåŸºçº¿æ¨¡å‹æœ‰æ˜æ˜¾æ”¹è¿›ï¼Œå…¶ä¸­PSNRæé«˜äº†3-4åˆ†è´ï¼ŒSSIMæé«˜äº†0.08-0.11ï¼ŒLPIPSé™ä½äº†0.06-0.10ã€‚ç‰¹åˆ«æ˜¯åœ¨ç»“æ„åŒ–åŠå¤æ‚çš„è‡ªç„¶åœºæ™¯ä¸‹ï¼Œè¯¥æ–¹æ³•çš„è¶…åˆ†è¾¨ç‡è´¨é‡æå‡æ•ˆæœæ˜¾è‘—ï¼Œåœºæ™¯é€‚åº”æ€§è¾ƒå¼ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10027v1">PDF</a> Accepted by the 4th International Conference on Computing Innovation   and Applied Physics (CONF-CIAP 2025), and will be published in EAI Community   Research Series-CORE or Theoretical and Natural Science (TNS)</p>
<p><strong>Summary</strong><br>è¶…åˆ†è¾¨ç‡å›¾åƒé‡å»ºå…·æœ‰æå¤§çš„ç ”ç©¶å’Œå®è·µæ„ä¹‰ï¼Œéšç€é¥æ„ŸæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œç°æœ‰çš„æ·±åº¦å­¦ä¹ æ–¹æ³•è™½æœ‰æ‰€è¿›å±•ï¼Œä½†åœ¨å¤„ç†å¤æ‚åœºæ™¯å’Œä¿ç•™å›¾åƒç»†èŠ‚æ–¹é¢ä»å­˜åœ¨å±€é™ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ç²¾ç»†è°ƒæ•´æ–¹æ³•ï¼Œç”¨äºé¥æ„Ÿå›¾åƒè¶…åˆ†è¾¨ç‡é‡å»ºã€‚è¯¥æ–¹æ³•æ„å»ºäº†ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼ŒåŒ…æ‹¬çŠ¶æ€ã€è¡Œä¸ºå’Œå¥–åŠ±ï¼Œåœ¨LDMæ¨¡å‹çš„é€†å‘å»å™ªè¿‡ç¨‹ä¸­é€šè¿‡è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ä¼˜åŒ–å†³ç­–ç›®æ ‡ã€‚åœ¨RESISC45æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€ç»“æ„ç›¸ä¼¼æ€§ï¼ˆSSIMï¼‰å’Œå±€éƒ¨æ„ŸçŸ¥å›¾åƒç›¸ä¼¼æ€§ï¼ˆLPIPSï¼‰ç­‰æŒ‡æ ‡ä¸Šè¾ƒåŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—æ”¹å–„ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ç»“æ„åŒ–åŠå¤æ‚è‡ªç„¶åœºæ™¯ã€‚ç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æé«˜è¶…åˆ†è¾¨ç‡è´¨é‡å’Œåœºæ™¯é€‚åº”æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„Ÿå›¾åƒè¶…åˆ†è¾¨ç‡é‡å»ºå…·æœ‰é‡å¤§ç ”ç©¶å’Œå®è·µä»·å€¼ï¼Œéšç€é¥æ„ŸæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œç°æœ‰æ–¹æ³•ä»é¢ä¸´å¤„ç†å¤æ‚åœºæ™¯å’Œä¿ç•™å›¾åƒç»†èŠ‚çš„æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ç²¾ç»†è°ƒæ•´æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹è¿›é¥æ„Ÿå›¾åƒçš„è¶…åˆ†è¾¨ç‡é‡å»ºã€‚</li>
<li>è¯¥æ–¹æ³•æ„å»ºäº†ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼ŒåŒ…æ‹¬çŠ¶æ€ã€è¡Œä¸ºå’Œå¥–åŠ±ï¼Œä¼˜åŒ–å†³ç­–ç›®æ ‡ã€‚</li>
<li>å®éªŒé‡‡ç”¨RESISC45æ•°æ®é›†ï¼Œåœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€ç»“æ„ç›¸ä¼¼æ€§ï¼ˆSSIMï¼‰å’Œå±€éƒ¨æ„ŸçŸ¥å›¾åƒç›¸ä¼¼æ€§ï¼ˆLPIPSï¼‰ç­‰æŒ‡æ ‡ä¸Šè¾ƒåŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>æ–¹æ³•åœ¨ç»“æ„åŒ–åŠå¤æ‚è‡ªç„¶åœºæ™¯çš„è¶…åˆ†è¾¨ç‡é‡å»ºä¸­è¡¨ç°å‡ºè‰¯å¥½çš„å¢å¼ºæ•ˆæœå’Œé€‚åº”æ€§ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„ç»“åˆä¸ºé¥æ„Ÿå›¾åƒè¶…åˆ†è¾¨ç‡é‡å»ºæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10027">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-31d7cf1857ab932d054980bffb1c9a1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddcf92f510f7c2dac4f26a6549db03ea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c068d16b5b1c0415196242c2b1344363.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5678a48e3aa9d613afe09ba984380083.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-865f1f1216ae1674d187d39445936fe3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Ordered-subsets-Multi-diffusion-Model-for-Sparse-view-CT-Reconstruction"><a href="#Ordered-subsets-Multi-diffusion-Model-for-Sparse-view-CT-Reconstruction" class="headerlink" title="Ordered-subsets Multi-diffusion Model for Sparse-view CT Reconstruction"></a>Ordered-subsets Multi-diffusion Model for Sparse-view CT Reconstruction</h2><p><strong>Authors:Pengfei Yu, Bin Huang, Minghui Zhang, Weiwen Wu, Shaoyu Wang, Qiegen Liu</strong></p>
<p>Score-based diffusion models have shown significant promise in the field of sparse-view CT reconstruction. However, the projection dataset is large and riddled with redundancy. Consequently, applying the diffusion model to unprocessed data results in lower learning effectiveness and higher learning difficulty, frequently leading to reconstructed images that lack fine details. To address these issues, we propose the ordered-subsets multi-diffusion model (OSMM) for sparse-view CT reconstruction. The OSMM innovatively divides the CT projection data into equal subsets and employs multi-subsets diffusion model (MSDM) to learn from each subset independently. This targeted learning approach reduces complexity and enhances the reconstruction of fine details. Furthermore, the integration of one-whole diffusion model (OWDM) with complete sinogram data acts as a global information constraint, which can reduce the possibility of generating erroneous or inconsistent sinogram information. Moreover, the OSMMâ€™s unsupervised learning framework provides strong robustness and generalizability, adapting seamlessly to varying sparsity levels of CT sinograms. This ensures consistent and reliable performance across different clinical scenarios. Experimental results demonstrate that OSMM outperforms traditional diffusion models in terms of image quality and noise resilience, offering a powerful and versatile solution for advanced CT imaging in sparse-view scenarios. </p>
<blockquote>
<p>åŸºäºå¾—åˆ†çš„æ‰©æ•£æ¨¡å‹åœ¨ç¨€ç–è§†å›¾CTé‡å»ºé¢†åŸŸæ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼ŒæŠ•å½±æ•°æ®é›†åºå¤§ä¸”å­˜åœ¨å†—ä½™ã€‚å› æ­¤ï¼Œå°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºæœªå¤„ç†çš„æ•°æ®å¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹å’Œå­¦ä¹ éš¾åº¦å¢åŠ ï¼Œç»å¸¸å¯¼è‡´é‡å»ºçš„å›¾åƒç¼ºä¹ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æœ‰åºå­é›†å¤šæ‰©æ•£æ¨¡å‹ï¼ˆOSMMï¼‰ç”¨äºç¨€ç–è§†å›¾CTé‡å»ºã€‚OSMMåˆ›æ–°åœ°å°†CTæŠ•å½±æ•°æ®åˆ†æˆç›¸ç­‰çš„å­é›†ï¼Œå¹¶é‡‡ç”¨å¤šå­é›†æ‰©æ•£æ¨¡å‹ï¼ˆMSDMï¼‰ä»æ¯ä¸ªå­é›†ä¸­ç‹¬ç«‹å­¦ä¹ ã€‚è¿™ç§æœ‰é’ˆå¯¹æ€§çš„å­¦ä¹ æ–¹æ³•é™ä½äº†å¤æ‚æ€§ï¼Œæé«˜äº†ç»†èŠ‚é‡å»ºçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå°†æ•´ä½“æ‰©æ•£æ¨¡å‹ï¼ˆOWDMï¼‰ä¸å®Œæ•´è¾›æ°å›¾æ•°æ®ç›¸ç»“åˆï¼Œä½œä¸ºå…¨å±€ä¿¡æ¯çº¦æŸï¼Œé™ä½äº†äº§ç”Ÿé”™è¯¯æˆ–ä¸ä¸€è‡´è¾›æ°å›¾ä¿¡æ¯çš„å¯èƒ½æ€§ã€‚è€Œä¸”ï¼ŒOSMMçš„æ— ç›‘ç£å­¦ä¹ æ¡†æ¶æä¾›äº†å¼ºå¤§çš„é²æ£’æ€§å’Œé€šç”¨æ€§ï¼Œèƒ½å¤Ÿæ— ç¼é€‚åº”ä¸åŒCTè¾›æ°å›¾çš„ç¨€ç–æ°´å¹³ã€‚è¿™ç¡®ä¿äº†åœ¨ä¸åŒä¸´åºŠåœºæ™¯ä¸­çš„ä¸€è‡´å’Œå¯é æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOSMMåœ¨å›¾åƒè´¨é‡å’Œå™ªå£°éŸ§æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ï¼Œä¸ºç¨€ç–è§†å›¾åœºæ™¯ä¸‹çš„é«˜çº§CTæˆåƒæä¾›äº†å¼ºå¤§è€Œé€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09985v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºåˆ†æ•°æ‰©æ•£æ¨¡å‹åœ¨ç¨€ç–è§†å›¾CTé‡å»ºé¢†åŸŸå±•ç°å‡ºè‰¯å¥½å‰æ™¯ã€‚ä½†ç”±äºæŠ•å½±æ•°æ®é›†åºå¤§ä¸”å­˜åœ¨å†—ä½™ä¿¡æ¯ï¼Œç›´æ¥ä½¿ç”¨æ‰©æ•£æ¨¡å‹å¤„ç†æœªç»å¤„ç†çš„æ•°æ®ä¼šå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ï¼Œé‡å»ºå›¾åƒç¼ºå¤±ç»†èŠ‚çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºæœ‰åºå­é›†å¤šæ‰©æ•£æ¨¡å‹ï¼ˆOSMMï¼‰ã€‚å®ƒå°†CTæŠ•å½±æ•°æ®åˆ†æˆè‹¥å¹²å­é›†ï¼Œé‡‡ç”¨å¤šå­é›†æ‰©æ•£æ¨¡å‹ï¼ˆMSDMï¼‰ç‹¬ç«‹å­¦ä¹ æ¯ä¸ªå­é›†ï¼Œé™ä½å¤æ‚åº¦å¹¶æå‡ç»†èŠ‚é‡å»ºæ•ˆæœã€‚ç»“åˆå…¨æ‰©æ•£æ¨¡å‹ï¼ˆOWDMï¼‰ä¸å®Œæ•´è¾›æ©æ ¼å›¾æ•°æ®ä½œä¸ºå…¨å±€ä¿¡æ¯çº¦æŸï¼Œå‡å°‘é”™è¯¯æˆ–ä¸ä¸€è‡´è¾›æ©æ ¼å›¾ä¿¡æ¯çš„ç”Ÿæˆå¯èƒ½ã€‚OSMMçš„æ— ç›‘ç£å­¦ä¹ æ¡†æ¶è¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§å’Œæ³›åŒ–æ€§ï¼Œèƒ½é€‚åº”ä¸åŒCTè¾›æ©æ ¼å›¾ç¨€ç–æ°´å¹³ï¼Œç¡®ä¿åœ¨å„ç§ä¸´åºŠåœºæ™¯ä¸‹çš„ç¨³å®šå¯é æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜OSMMåœ¨å›¾åƒè´¨é‡å’Œå™ªå£°éŸ§æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ï¼Œä¸ºç¨€ç–è§†å›¾åœºæ™¯ä¸‹çš„å…ˆè¿›CTæˆåƒæä¾›äº†å¼ºå¤§è€Œé€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ†æ•°æ‰©æ•£æ¨¡å‹åœ¨ç¨€ç–è§†å›¾CTé‡å»ºä¸­æœ‰è‰¯å¥½å‰æ™¯ã€‚</li>
<li>ç›´æ¥åº”ç”¨æ‰©æ•£æ¨¡å‹å¤„ç†æœªç»å¤„ç†çš„æ•°æ®ä¼šå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹å’Œå›¾åƒç»†èŠ‚ç¼ºå¤±ã€‚</li>
<li>æå‡ºæœ‰åºå­é›†å¤šæ‰©æ•£æ¨¡å‹ï¼ˆOSMMï¼‰è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå°†CTæŠ•å½±æ•°æ®åˆ†æˆå­é›†è¿›è¡Œç‹¬ç«‹å­¦ä¹ ã€‚</li>
<li>OSMMé‡‡ç”¨å¤šå­é›†æ‰©æ•£æ¨¡å‹ï¼ˆMSDMï¼‰é™ä½å­¦ä¹ å¤æ‚åº¦å¹¶æå‡ç»†èŠ‚é‡å»ºæ•ˆæœã€‚</li>
<li>ç»“åˆå…¨æ‰©æ•£æ¨¡å‹ï¼ˆOWDMï¼‰ä¸å®Œæ•´è¾›æ©æ ¼å›¾æ•°æ®ä½œä¸ºå…¨å±€ä¿¡æ¯çº¦æŸã€‚</li>
<li>OSMMçš„æ— ç›‘ç£å­¦ä¹ æ¡†æ¶é€‚åº”ä¸åŒCTè¾›æ©æ ¼å›¾ç¨€ç–æ°´å¹³ï¼Œè¡¨ç°ç¨³å®šå¯é ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09985">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fa4ae34e4f2b3b783de3a6c9808cf701.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1544906ac0105b934677cb959a2af3b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2d4862d315c4707e2f3f5ad50995cf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04600aaff474e87d86af5df094a93fe5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-887152a97ec575086524c0da36d6e5b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3436a85f5ec9fe93a7a6366d14f4a4f4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="EWMBench-Evaluating-Scene-Motion-and-Semantic-Quality-in-Embodied-World-Models"><a href="#EWMBench-Evaluating-Scene-Motion-and-Semantic-Quality-in-Embodied-World-Models" class="headerlink" title="EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied   World Models"></a>EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied   World Models</h2><p><strong>Authors:Hu Yue, Siyuan Huang, Yue Liao, Shengcong Chen, Pengfei Zhou, Liliang Chen, Maoqing Yao, Guanghui Ren</strong></p>
<p>Recent advances in creative AI have enabled the synthesis of high-fidelity images and videos conditioned on language instructions. Building on these developments, text-to-video diffusion models have evolved into embodied world models (EWMs) capable of generating physically plausible scenes from language commands, effectively bridging vision and action in embodied AI applications. This work addresses the critical challenge of evaluating EWMs beyond general perceptual metrics to ensure the generation of physically grounded and action-consistent behaviors. We propose the Embodied World Model Benchmark (EWMBench), a dedicated framework designed to evaluate EWMs based on three key aspects: visual scene consistency, motion correctness, and semantic alignment. Our approach leverages a meticulously curated dataset encompassing diverse scenes and motion patterns, alongside a comprehensive multi-dimensional evaluation toolkit, to assess and compare candidate models. The proposed benchmark not only identifies the limitations of existing video generation models in meeting the unique requirements of embodied tasks but also provides valuable insights to guide future advancements in the field. The dataset and evaluation tools are publicly available at <a target="_blank" rel="noopener" href="https://github.com/AgibotTech/EWMBench">https://github.com/AgibotTech/EWMBench</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œåˆ›æ„äººå·¥æ™ºèƒ½çš„è¿›å±•ä½¿å¾—èƒ½å¤Ÿæ ¹æ®è¯­è¨€æŒ‡ä»¤ç”Ÿæˆé«˜ä¿çœŸå›¾åƒå’Œè§†é¢‘æˆä¸ºå¯èƒ½ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹å·²ç»æ¼”å˜ä¸ºä½“ä¸–ç•Œæ¨¡å‹ï¼ˆEWMsï¼‰ï¼Œèƒ½å¤Ÿä»è¯­è¨€å‘½ä»¤ä¸­ç”Ÿæˆç‰©ç†ä¸Šåˆç†çš„åœºæ™¯ï¼Œä»è€Œåœ¨å®ä½“äººå·¥æ™ºèƒ½åº”ç”¨ä¸­æœ‰æ•ˆåœ°å®ç°è§†è§‰å’Œè¡ŒåŠ¨çš„æ¡¥æ¢ã€‚æœ¬ç ”ç©¶è§£å†³äº†è¯„ä¼°EWMsçš„å…³é”®æŒ‘æˆ˜ï¼Œä¸ä»…é™äºä¸€èˆ¬çš„æ„ŸçŸ¥æŒ‡æ ‡ï¼Œä»¥ç¡®ä¿ç”Ÿæˆç‰©ç†ä¸Šå¯é ä¸”è¡ŒåŠ¨ä¸€è‡´çš„è¡Œä¸ºã€‚æˆ‘ä»¬æå‡ºäº†ä½“ä¸–ç•Œæ¨¡å‹åŸºå‡†æµ‹è¯•ï¼ˆEWMBenchï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„æ¡†æ¶ï¼ŒåŸºäºä¸‰ä¸ªæ–¹é¢è¯„ä¼°EWMsï¼šè§†è§‰åœºæ™¯ä¸€è‡´æ€§ã€è¿åŠ¨æ­£ç¡®æ€§å’Œè¯­ä¹‰å¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ç²¾å¿ƒæŒ‘é€‰çš„æ•°æ®é›†ï¼Œæ¶µç›–å„ç§åœºæ™¯å’Œè¿åŠ¨æ¨¡å¼ï¼Œä»¥åŠå…¨é¢çš„å¤šç»´è¯„ä¼°å·¥å…·åŒ…ï¼Œæ¥è¯„ä¼°å’Œæ¯”è¾ƒå€™é€‰æ¨¡å‹ã€‚æ‰€æå‡ºçš„åŸºå‡†æµ‹è¯•ä¸ä»…å‘ç°äº†ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨æ»¡è¶³å®ä½“ä»»åŠ¡ç‹¬ç‰¹è¦æ±‚æ–¹é¢çš„å±€é™æ€§ï¼Œè€Œä¸”ä¸ºæœªæ¥çš„é¢†åŸŸå‘å±•æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚æ•°æ®é›†å’Œè¯„ä¼°å·¥å…·å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AgibotTech/EWMBench%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/AgibotTech/EWMBenchå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09694v1">PDF</a> Website: <a target="_blank" rel="noopener" href="https://github.com/AgibotTech/EWMBench">https://github.com/AgibotTech/EWMBench</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ›æ„AIçš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆText-to-Video Diffusion Modelsï¼‰åœ¨ç”Ÿæˆç‰©ç†ä¸Šåˆç†åœºæ™¯æ–¹é¢çš„èƒ½åŠ›ã€‚ä¸ºè§£å†³è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆç¬¦åˆç‰©ç†è§„å¾‹å’Œè¡Œä¸ºä¸€è‡´æ€§çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†Embodied World Model Benchmarkï¼ˆEWMBenchï¼‰è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäºè§†è§‰åœºæ™¯ä¸€è‡´æ€§ã€è¿åŠ¨æ­£ç¡®æ€§å’Œè¯­ä¹‰å¯¹é½ä¸‰ä¸ªå…³é”®æ–¹é¢æ¥è¯„ä¼°æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨ç²¾å¿ƒç¼–åˆ¶çš„åŒ…å«å¤šæ ·åœºæ™¯å’Œè¿åŠ¨æ¨¡å¼çš„æ•°æ®é›†å’Œå…¨é¢çš„å¤šç»´åº¦è¯„ä¼°å·¥å…·åŒ…æ¥è¯„ä¼°å’Œæ¯”è¾ƒå€™é€‰æ¨¡å‹ã€‚è¯¥åŸºå‡†ä¸ä»…æ­ç¤ºäº†ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨åº”å¯¹ç‹¬ç‰¹ä»»åŠ¡è¦æ±‚æ–¹é¢çš„å±€é™æ€§ï¼Œè€Œä¸”ä¸ºæœªæ¥è¯¥é¢†åŸŸçš„å‘å±•æä¾›äº†å®è´µè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹å·²è¿›åŒ–ä¸ºä½“ç°ä¸–ç•Œæ¨¡å‹ï¼ˆEWMsï¼‰ï¼Œèƒ½å¤Ÿä»è¯­è¨€æŒ‡ä»¤ç”Ÿæˆç‰©ç†ä¸Šåˆç†çš„åœºæ™¯ã€‚</li>
<li>EWMsåœ¨åˆ›æ„AIåº”ç”¨ä¸­å®ç°äº†è§†è§‰å’Œè¡Œä¸ºçš„æ¡¥æ¢ã€‚</li>
<li>è¯„ä¼°EWMsçš„æŒ‘æˆ˜åœ¨äºç¡®ä¿ç”Ÿæˆå†…å®¹çš„ç‰©ç†åˆç†æ€§å’Œè¡Œä¸ºä¸€è‡´æ€§ã€‚</li>
<li>æå‡ºEmbodied World Model Benchmarkï¼ˆEWMBenchï¼‰æ¡†æ¶ï¼ŒåŸºäºè§†è§‰åœºæ™¯ä¸€è‡´æ€§ã€è¿åŠ¨æ­£ç¡®æ€§å’Œè¯­ä¹‰å¯¹é½ä¸‰ä¸ªå…³é”®æ–¹é¢æ¥è¯„ä¼°EWMsã€‚</li>
<li>EWMBenchåˆ©ç”¨å¤šæ ·åœºæ™¯å’Œè¿åŠ¨æ¨¡å¼çš„æ•°æ®é›†å’Œå…¨é¢çš„å¤šç»´åº¦è¯„ä¼°å·¥å…·åŒ…ã€‚</li>
<li>ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨åº”å¯¹ç‰¹å®šä»»åŠ¡è¦æ±‚æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09694">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72f3c44a105d6582db6763600da642b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f934fdc390c8e78591e8b76ab6824327.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2f1664aed878b189712f9800662bbb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fec48483d874e44cf96167dcf6921796.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-660c65f35d0bea4433605d056255eb58.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Generating-time-consistent-dynamics-with-discriminator-guided-image-diffusion-models"><a href="#Generating-time-consistent-dynamics-with-discriminator-guided-image-diffusion-models" class="headerlink" title="Generating time-consistent dynamics with discriminator-guided image   diffusion models"></a>Generating time-consistent dynamics with discriminator-guided image   diffusion models</h2><p><strong>Authors:Philipp Hess, Maximilian Gelbrecht, Christof SchÃ¶tz, Michael Aich, Yu Huang, Shangshang Yang, Niklas Boers</strong></p>
<p>Realistic temporal dynamics are crucial for many video generation, processing and modelling applications, e.g. in computational fluid dynamics, weather prediction, or long-term climate simulations. Video diffusion models (VDMs) are the current state-of-the-art method for generating highly realistic dynamics. However, training VDMs from scratch can be challenging and requires large computational resources, limiting their wider application. Here, we propose a time-consistency discriminator that enables pretrained image diffusion models to generate realistic spatiotemporal dynamics. The discriminator guides the sampling inference process and does not require extensions or finetuning of the image diffusion model. We compare our approach against a VDM trained from scratch on an idealized turbulence simulation and a real-world global precipitation dataset. Our approach performs equally well in terms of temporal consistency, shows improved uncertainty calibration and lower biases compared to the VDM, and achieves stable centennial-scale climate simulations at daily time steps. </p>
<blockquote>
<p>å¯¹äºè®¸å¤šè§†é¢‘ç”Ÿæˆã€å¤„ç†å’Œå»ºæ¨¡åº”ç”¨ï¼ˆä¾‹å¦‚è®¡ç®—æµä½“åŠ¨åŠ›å­¦ã€å¤©æ°”é¢„æŠ¥æˆ–é•¿æœŸæ°”å€™æ¨¡æ‹Ÿï¼‰æ¥è¯´ï¼Œå®ç°é€¼çœŸçš„æ—¶é—´åŠ¨æ€è‡³å…³é‡è¦ã€‚è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰æ˜¯ç›®å‰ç”Ÿæˆé«˜åº¦é€¼çœŸåŠ¨æ€çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚ç„¶è€Œï¼Œä»å¤´å¼€å§‹è®­ç»ƒVDMå¯èƒ½ä¼šé¢ä¸´æŒ‘æˆ˜ï¼Œå¹¶éœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºï¼Œè¿™é™åˆ¶äº†å…¶æ›´å¹¿æ³›çš„åº”ç”¨ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ—¶é—´ä¸€è‡´æ€§é‰´åˆ«å™¨ï¼Œå®ƒå¯ä»¥è®©é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆé€¼çœŸçš„æ—¶ç©ºåŠ¨æ€ã€‚é‰´åˆ«å™¨å¼•å¯¼é‡‡æ ·æ¨ç†è¿‡ç¨‹ï¼Œä¸éœ€è¦æ‰©å±•æˆ–å¾®è°ƒå›¾åƒæ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„æ–¹æ³•ä¸åœ¨ç†æƒ³åŒ–çš„æ¹æµæ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œå…¨çƒé™æ°´æ•°æ®é›†ä¸Šä»å¤´å¼€å§‹è®­ç»ƒçš„VDMè¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ—¶é—´ä¸€è‡´æ€§æ–¹é¢è¡¨ç°åŒæ ·å‡ºè‰²ï¼Œä¸VDMç›¸æ¯”ï¼Œæ˜¾ç¤ºå‡ºæ”¹è¿›çš„ä¸ç¡®å®šæ€§æ ¡å‡†å’Œæ›´ä½çš„åè§ï¼Œå¹¶ä»¥æ¯æ—¥æ—¶é—´æ­¥é•¿å®ç°ç¨³å®šçš„æ°”å€™æ¨¡æ‹Ÿï¼ˆä¸–çºªè§„æ¨¡ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09089v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡ç« å¼ºè°ƒç°å®çš„æ—¶é—´åŠ¨æ€å¯¹äºè§†é¢‘ç”Ÿæˆã€å¤„ç†å’Œå»ºæ¨¡åº”ç”¨çš„é‡è¦æ€§ï¼Œå¦‚è®¡ç®—æµä½“åŠ¨åŠ›å­¦ã€å¤©æ°”é¢„æŠ¥æˆ–é•¿æœŸæ°”å€™æ¨¡æ‹Ÿç­‰ã€‚è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰æ˜¯ç›®å‰ç”Ÿæˆé«˜åº¦ç°å®åŠ¨æ€çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œä½†å…¶ä»å¤´è®­ç»ƒå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼Œé™åˆ¶äº†å…¶å¹¿æ³›åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ—¶é—´ä¸€è‡´æ€§é‰´åˆ«å™¨ï¼Œå®ƒèƒ½ä½¿é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆç°å®çš„æ—¶ç©ºåŠ¨æ€ã€‚é‰´åˆ«å™¨å¼•å¯¼é‡‡æ ·æ¨ç†è¿‡ç¨‹ï¼Œä¸éœ€è¦å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ‰©å±•æˆ–å¾®è°ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸åœ¨ç†æƒ³åŒ–çš„æ¹æµæ¨¡æ‹Ÿå’ŒçœŸå®å…¨çƒé™æ°´æ•°æ®é›†ä¸Šè¿›è¡Œä»å¤´è®­ç»ƒçš„VDMè¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ—¶é—´ä¸€è‡´æ€§æ–¹é¢è¡¨ç°åŒæ ·å‡ºè‰²ï¼Œæ˜¾ç¤ºå‡ºæ”¹è¿›çš„ä¸ç¡®å®šæ€§æ ¡å‡†å’Œæ›´ä½çš„åè§ï¼Œä¸”åœ¨æ¯æ—¥æ—¶é—´æ­¥é•¿ä¸‹å®ç°äº†ç¨³å®šçš„æ°”å€™æ¨¡æ‹Ÿã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>ç°å®çš„æ—¶é—´åŠ¨æ€åœ¨è§†é¢‘ç”Ÿæˆã€å¤„ç†å’Œå»ºæ¨¡åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰æ˜¯ç”Ÿæˆé«˜åº¦ç°å®åŠ¨æ€çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>ä»å¤´è®­ç»ƒVDMå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦å¤§é‡è®¡ç®—èµ„æºã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ—¶é—´ä¸€è‡´æ€§é‰´åˆ«å™¨ï¼Œä½¿é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹èƒ½ç”Ÿæˆç°å®çš„æ—¶ç©ºåŠ¨æ€ã€‚</li>
<li>é‰´åˆ«å™¨å¼•å¯¼é‡‡æ ·æ¨ç†è¿‡ç¨‹ï¼Œæ— éœ€å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ‰©å±•æˆ–å¾®è°ƒã€‚</li>
<li>ä¸VDMç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ—¶é—´ä¸€è‡´æ€§æ–¹é¢è¡¨ç°ä¼˜ç§€ï¼Œå…·æœ‰æ”¹è¿›çš„ä¸ç¡®å®šæ€§æ ¡å‡†å’Œæ›´ä½çš„åè§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09089">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a51e09d639f8ab36d95b47f50122e739.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-174bdc11e2dde549c06d6dfa638c5717.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bced45e780c9ea73d429561736dec284.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-156db5c1eb49e129709b6438e64a9ff9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e5e6e1e0267299efa5c597c25afd1fc.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="IntrinsicEdit-Precise-generative-image-manipulation-in-intrinsic-space"><a href="#IntrinsicEdit-Precise-generative-image-manipulation-in-intrinsic-space" class="headerlink" title="IntrinsicEdit: Precise generative image manipulation in intrinsic space"></a>IntrinsicEdit: Precise generative image manipulation in intrinsic space</h2><p><strong>Authors:Linjie Lyu, Valentin Deschaintre, Yannick Hold-Geoffroy, MiloÅ¡ HaÅ¡an, Jae Shin Yoon, Thomas LeimkÃ¼hler, Christian Theobalt, Iliyan Georgiev</strong></p>
<p>Generative diffusion models have advanced image editing with high-quality results and intuitive interfaces such as prompts and semantic drawing. However, these interfaces lack precise control, and the associated methods typically specialize on a single editing task. We introduce a versatile, generative workflow that operates in an intrinsic-image latent space, enabling semantic, local manipulation with pixel precision for a range of editing operations. Building atop the RGB-X diffusion framework, we address key challenges of identity preservation and intrinsic-channel entanglement. By incorporating exact diffusion inversion and disentangled channel manipulation, we enable precise, efficient editing with automatic resolution of global illumination effects â€“ all without additional data collection or model fine-tuning. We demonstrate state-of-the-art performance across a variety of tasks on complex images, including color and texture adjustments, object insertion and removal, global relighting, and their combinations. </p>
<blockquote>
<p>ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹é€šè¿‡é«˜è´¨é‡ç»“æœå’Œç›´è§‚ç•Œé¢ï¼ˆå¦‚æç¤ºå’Œè¯­ä¹‰ç»˜å›¾ï¼‰æ¨åŠ¨äº†å›¾åƒç¼–è¾‘çš„å‘å±•ã€‚ç„¶è€Œï¼Œè¿™äº›ç•Œé¢ç¼ºä¹ç²¾ç¡®æ§åˆ¶ï¼Œç›¸å…³æ–¹æ³•é€šå¸¸ä¸“æ³¨äºå•ä¸€ç¼–è¾‘ä»»åŠ¡ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é€šç”¨ã€ç”Ÿæˆå¼å·¥ä½œæµç¨‹ï¼Œå®ƒåœ¨å†…åœ¨å›¾åƒæ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œèƒ½å¤Ÿå¯¹ä¸€ç³»åˆ—ç¼–è¾‘æ“ä½œè¿›è¡Œè¯­ä¹‰ã€å±€éƒ¨æ“çºµï¼Œå…·æœ‰åƒç´ ç²¾åº¦ã€‚åŸºäºRGB-Xæ‰©æ•£æ¡†æ¶ï¼Œæˆ‘ä»¬è§£å†³äº†èº«ä»½ä¿ç•™å’Œå†…åœ¨é€šé“çº ç¼ çš„å…³é”®æŒ‘æˆ˜ã€‚é€šè¿‡ç»“åˆç²¾ç¡®æ‰©æ•£åæ¼”å’Œåˆ†ç¦»é€šé“æ“çºµï¼Œæˆ‘ä»¬å®ç°äº†ç²¾ç¡®ã€é«˜æ•ˆçš„ç¼–è¾‘ï¼Œè‡ªåŠ¨è§£å†³å…¨å±€ç…§æ˜æ•ˆæœé—®é¢˜â€”â€”æ‰€æœ‰è¿™ä¸€åˆ‡æ— éœ€é¢å¤–æ”¶é›†æ•°æ®æˆ–å¾®è°ƒæ¨¡å‹ã€‚æˆ‘ä»¬åœ¨å¤æ‚å›¾åƒçš„å„ç§ä»»åŠ¡ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬é¢œè‰²å’Œçº¹ç†è°ƒæ•´ã€å¯¹è±¡æ’å…¥å’Œåˆ é™¤ã€å…¨å±€é‡æ–°ç…§æ˜åŠå…¶ç»„åˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08889v2">PDF</a> SIGGRAPH 2025 Journal track</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºRGB-Xæ‰©æ•£æ¡†æ¶çš„é€šç”¨ç”Ÿæˆå·¥ä½œæµç¨‹ï¼Œè¯¥æµç¨‹åœ¨å†…åœ¨å›¾åƒæ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œèƒ½å¤Ÿå®ç°å„ç§ç¼–è¾‘æ“ä½œçš„è¯­ä¹‰å±€éƒ¨æ“çºµï¼Œå…·æœ‰åƒç´ ç²¾åº¦ã€‚é€šè¿‡å¼•å…¥ç²¾ç¡®æ‰©æ•£åè½¬å’Œåˆ†ç¦»é€šé“æ“çºµæŠ€æœ¯ï¼Œè§£å†³äº†èº«ä»½ä¿ç•™å’Œå†…åœ¨é€šé“çº ç¼ çš„å…³é”®æŒ‘æˆ˜ï¼Œå®ç°äº†å¤æ‚å›¾åƒå„ç§ä»»åŠ¡çš„å“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬é¢œè‰²å’Œçº¹ç†è°ƒæ•´ã€å¯¹è±¡æ’å…¥å’Œåˆ é™¤ã€å…¨å±€é‡æ–°ç…§æ˜åŠå…¶ç»„åˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘æ–¹é¢å–å¾—äº†é«˜è´¨é‡ç»“æœï¼Œé€šè¿‡æç¤ºå’Œè¯­ä¹‰ç»˜å›¾ç­‰ç›´è§‚ç•Œé¢è¿›è¡Œæ“ä½œã€‚</li>
<li>ç°æœ‰ç•Œé¢ç¼ºä¹ç²¾ç¡®æ§åˆ¶ï¼Œä¸”ç›¸å…³æ–¹æ³•é€šå¸¸ä¸“æ³¨äºå•ä¸ªç¼–è¾‘ä»»åŠ¡ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§é€šç”¨ç”Ÿæˆå·¥ä½œæµç¨‹ï¼Œåœ¨å†…åœ¨å›¾åƒæ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œå®ç°å¤šç§ç¼–è¾‘æ“ä½œçš„è¯­ä¹‰å±€éƒ¨æ“çºµï¼Œå…·æœ‰åƒç´ ç²¾åº¦ã€‚</li>
<li>åŸºäºRGB-Xæ‰©æ•£æ¡†æ¶ï¼Œè§£å†³äº†èº«ä»½ä¿ç•™å’Œå†…åœ¨é€šé“çº ç¼ çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡ç²¾ç¡®æ‰©æ•£åè½¬å’Œåˆ†ç¦»é€šé“æ“çºµï¼Œèƒ½å¤Ÿåœ¨ä¸æ”¶é›†é¢å¤–æ•°æ®æˆ–å¾®è°ƒæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œå®ç°ç²¾ç¡®ã€é«˜æ•ˆçš„ç¼–è¾‘ï¼Œè‡ªåŠ¨è§£å†³å…¨å±€ç…§æ˜æ•ˆæœã€‚</li>
<li>åœ¨å¤æ‚å›¾åƒçš„å„ç§ä»»åŠ¡ä¸Šå±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬é¢œè‰²å’Œçº¹ç†è°ƒæ•´ã€å¯¹è±¡æ’å…¥å’Œåˆ é™¤ã€å…¨å±€é‡æ–°ç…§æ˜åŠå…¶ç»„åˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08889">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-caf822b6ad9998d3e5feafb5166efed2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68a2ff0f89cf497eb69d4d8d6339f517.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c72b3c9de6168e0173c1e72164c5cdda.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Generative-AI-for-Autonomous-Driving-Frontiers-and-Opportunities"><a href="#Generative-AI-for-Autonomous-Driving-Frontiers-and-Opportunities" class="headerlink" title="Generative AI for Autonomous Driving: Frontiers and Opportunities"></a>Generative AI for Autonomous Driving: Frontiers and Opportunities</h2><p><strong>Authors:Yuping Wang, Shuo Xing, Cui Can, Renjie Li, Hongyuan Hua, Kexin Tian, Zhaobin Mo, Xiangbo Gao, Keshu Wu, Sulong Zhou, Hengxu You, Juntong Peng, Junge Zhang, Zehao Wang, Rui Song, Mingxuan Yan, Walter Zimmer, Xingcheng Zhou, Peiran Li, Zhaohan Lu, Chia-Ju Chen, Yue Huang, Ryan A. Rossi, Lichao Sun, Hongkai Yu, Zhiwen Fan, Frank Hao Yang, Yuhao Kang, Ross Greer, Chenxi Liu, Eun Hak Lee, Xuan Di, Xinyue Ye, Liu Ren, Alois Knoll, Xiaopeng Li, Shuiwang Ji, Masayoshi Tomizuka, Marco Pavone, Tianbao Yang, Jing Du, Ming-Hsuan Yang, Hua Wei, Ziran Wang, Yang Zhou, Jiachen Li, Zhengzhong Tu</strong></p>
<p>Generative Artificial Intelligence (GenAI) constitutes a transformative technological wave that reconfigures industries through its unparalleled capabilities for content creation, reasoning, planning, and multimodal understanding. This revolutionary force offers the most promising path yet toward solving one of engineeringâ€™s grandest challenges: achieving reliable, fully autonomous driving, particularly the pursuit of Level 5 autonomy. This survey delivers a comprehensive and critical synthesis of the emerging role of GenAI across the autonomous driving stack. We begin by distilling the principles and trade-offs of modern generative modeling, encompassing VAEs, GANs, Diffusion Models, and Large Language Models (LLMs). We then map their frontier applications in image, LiDAR, trajectory, occupancy, video generation as well as LLM-guided reasoning and decision making. We categorize practical applications, such as synthetic data workflows, end-to-end driving strategies, high-fidelity digital twin systems, smart transportation networks, and cross-domain transfer to embodied AI. We identify key obstacles and possibilities such as comprehensive generalization across rare cases, evaluation and safety checks, budget-limited implementation, regulatory compliance, ethical concerns, and environmental effects, while proposing research plans across theoretical assurances, trust metrics, transport integration, and socio-technical influence. By unifying these threads, the survey provides a forward-looking reference for researchers, engineers, and policymakers navigating the convergence of generative AI and advanced autonomous mobility. An actively maintained repository of cited works is available at <a target="_blank" rel="noopener" href="https://github.com/taco-group/GenAI4AD">https://github.com/taco-group/GenAI4AD</a>. </p>
<blockquote>
<p>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰æ„æˆäº†ä¸€åœºå˜é©æ€§çš„æŠ€æœ¯æµªæ½®ï¼Œå®ƒé€šè¿‡æ— ä¸ä¼¦æ¯”çš„å†…å®¹åˆ›å»ºã€æ¨ç†ã€è§„åˆ’å’Œå¤šæ¨¡å¼ç†è§£èƒ½åŠ›ï¼Œé‡æ–°é…ç½®å„è¡Œä¸šã€‚è¿™è‚¡é©å‘½åŠ›é‡ä¸ºè§£å†³å·¥ç¨‹é¢†åŸŸæœ€å®ä¼Ÿçš„æŒ‘æˆ˜ä¹‹ä¸€â€”â€”å®ç°å¯é ã€å…¨è‡ªåŠ¨çš„è‡ªåŠ¨é©¾é©¶ï¼Œç‰¹åˆ«æ˜¯è¿½æ±‚å®ç°ç¬¬äº”çº§è‡ªä¸»é©¾é©¶ï¼Œæä¾›äº†æœ€æœ‰å¸Œæœ›çš„é€”å¾„ã€‚è¿™ç¯‡ç»¼è¿°å…¨é¢æ·±å…¥åœ°æ¢è®¨äº†GenAIåœ¨è‡ªåŠ¨é©¾é©¶å †æ ˆä¸­çš„æ–°å…´è§’è‰²ã€‚æˆ‘ä»¬é¦–å…ˆæç‚¼äº†ç°ä»£ç”Ÿæˆå»ºæ¨¡çš„åŸç†å’Œæƒè¡¡ï¼ŒåŒ…æ‹¬å˜åˆ†è‡ªç¼–ç å™¨ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€æ‰©æ•£æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¿™äº›å‰æ²¿æŠ€æœ¯åº”ç”¨äºå›¾åƒã€æ¿€å…‰é›·è¾¾ã€è½¨è¿¹ã€å ç”¨ã€è§†é¢‘ç”Ÿæˆä»¥åŠå¤§è¯­è¨€æ¨¡å‹å¼•å¯¼çš„æ¨ç†å’Œå†³ç­–åˆ¶å®šã€‚æˆ‘ä»¬åˆ†ç±»äº†å®é™…åº”ç”¨ï¼Œå¦‚åˆæˆæ•°æ®æµã€ç«¯åˆ°ç«¯é©¾é©¶ç­–ç•¥ã€é«˜ä¿çœŸæ•°å­—å­ªç”Ÿç³»ç»Ÿã€æ™ºèƒ½äº¤é€šç½‘ç»œå’Œè·¨åŸŸè½¬ç§»åˆ°å®ä½“äººå·¥æ™ºèƒ½ç­‰ã€‚æˆ‘ä»¬ç¡®å®šäº†å…³é”®éšœç¢å’Œå¯èƒ½æ€§ï¼Œå¦‚å…¨é¢æ¦‚æ‹¬ç½•è§æƒ…å†µã€è¯„ä¼°å’Œå®‰å…¨æ£€æŸ¥ã€é¢„ç®—æœ‰é™çš„å®ç°ã€æ³•è§„åˆè§„ã€é“å¾·å…³æ³¨å’Œç¯å¢ƒå½±å“ï¼ŒåŒæ—¶å°±ç†è®ºä¿è¯ã€ä¿¡ä»»æŒ‡æ ‡ã€äº¤é€šé›†æˆå’Œç¤¾ä¼šæŠ€æœ¯å½±å“æå‡ºç ”ç©¶è®¡åˆ’ã€‚é€šè¿‡ç»Ÿä¸€è¿™äº›çº¿ç´¢ï¼Œè¿™ç¯‡ç»¼è¿°ä¸ºç ”ç©¶äººå‘˜ã€å·¥ç¨‹å¸ˆå’Œæ”¿ç­–åˆ¶å®šè€…åœ¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å’Œå…ˆè¿›çš„è‡ªåŠ¨é©¾é©¶æŠ€æœ¯èåˆæ–¹é¢æä¾›äº†å‰ç»æ€§çš„å‚è€ƒã€‚æ‰€å¼•ç”¨çš„ä½œå“çš„ä¸€ä¸ªæ´»è·ƒç»´æŠ¤ä»“åº“å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/taco-group/GenAI4AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/taco-group/GenAI4ADæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08854v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰é‡æ„è¡Œä¸šçš„æŠ€æœ¯æµªæ½®æ­£å…´èµ·ï¼Œå‡­å€Ÿå…¶å¼ºå¤§çš„å†…å®¹åˆ›ä½œã€æ¨ç†ã€è§„åˆ’å’Œå¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œä¸ºå®ç°å¯é çš„è‡ªåŠ¨é©¾é©¶å¸¦æ¥æœ€æœ‰å¸Œæœ›çš„è·¯å¾„ã€‚æœ¬æ–‡ç»¼è¿°äº†GenAIåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„æ–°å…´è§’è‰²ï¼Œä»‹ç»äº†ç°ä»£ç”Ÿæˆæ¨¡å‹çš„åŸåˆ™å’Œæƒè¡¡ï¼ŒåŒ…æ‹¬å˜åˆ†è‡ªç¼–ç å™¨ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€æ‰©æ•£æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶æ¢è®¨äº†å®ƒä»¬åœ¨å›¾åƒã€æ¿€å…‰é›·è¾¾ã€è½¨è¿¹ã€å ç”¨ã€è§†é¢‘ç”Ÿæˆä»¥åŠå¤§è¯­è¨€æ¨¡å‹å¼•å¯¼çš„æ¨ç†å’Œå†³ç­–åˆ¶å®šç­‰å‰æ²¿åº”ç”¨ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜æ€»ç»“äº†å…³é”®éšœç¢å’Œå¯èƒ½æ€§ï¼Œå¹¶æå‡ºäº†ç†è®ºä¿è¯ã€ä¿¡ä»»æŒ‡æ ‡ã€è¿è¾“æ•´åˆå’Œç¤¾ä¼šæŠ€æœ¯å½±å“ç­‰æ–¹é¢çš„ç ”ç©¶è®¡åˆ’ã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜ã€å·¥ç¨‹å¸ˆå’Œæ”¿ç­–åˆ¶å®šè€…æä¾›å‰ç»æ€§å‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰æˆä¸ºå˜é©æ€§æŠ€æœ¯ï¼Œå…·å¤‡å†…å®¹åˆ›å»ºã€æ¨ç†ã€è§„åˆ’å’Œå¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚</li>
<li>GenAIåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸå…·æœ‰æ–°å…´å’Œé‡è¦çš„è§’è‰²ï¼Œæ¶µç›–å¤šä¸ªæŠ€æœ¯å±‚é¢ã€‚</li>
<li>ç°ä»£ç”Ÿæˆæ¨¡å‹çš„åŸåˆ™å’Œæƒè¡¡ï¼ŒåŒ…æ‹¬VAEsã€GANsã€Diffusion Modelså’ŒLLMsçš„ä»‹ç»ã€‚</li>
<li>GenAIåœ¨å›¾åƒã€LiDARã€è½¨è¿¹ã€å ç”¨ã€è§†é¢‘ç”Ÿæˆç­‰å‰æ²¿åº”ç”¨ä¸­çš„å®é™…åº”ç”¨ã€‚</li>
<li>LLMsåœ¨æ¨ç†å’Œå†³ç­–åˆ¶å®šä¸­çš„å¼•å¯¼è§’è‰²ã€‚</li>
<li>å®ç°è‡ªåŠ¨é©¾é©¶çš„å…³é”®éšœç¢åŒ…æ‹¬å…¨é¢æ¦‚æ‹¬ç½•è§æƒ…å†µã€è¯„ä¼°å’Œå®‰å…¨æ£€æŸ¥ã€é¢„ç®—æœ‰é™çš„å®æ–½ã€æ³•è§„åˆè§„ã€é“å¾·å…³åˆ‡å’Œç¯å¢ƒå½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08854">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-56948221131ff6321dd98ac51b89b08e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Learning-Graph-Representation-of-Agent-Diffusers"><a href="#Learning-Graph-Representation-of-Agent-Diffusers" class="headerlink" title="Learning Graph Representation of Agent Diffusers"></a>Learning Graph Representation of Agent Diffusers</h2><p><strong>Authors:Youcef Djenouri, Nassim Belmecheri, Tomasz Michalak, Jan DubiÅ„ski, Ahmed Nabil Belbachir, Anis Yazidi</strong></p>
<p>Diffusion-based generative models have significantly advanced text-to-image synthesis, demonstrating impressive text comprehension and zero-shot generalization. These models refine images from random noise based on textual prompts, with initial reliance on text input shifting towards enhanced visual fidelity over time. This transition suggests that static model parameters might not optimally address the distinct phases of generation. We introduce LGR-AD (Learning Graph Representation of Agent Diffusers), a novel multi-agent system designed to improve adaptability in dynamic computer vision tasks. LGR-AD models the generation process as a distributed system of interacting agents, each representing an expert sub-model. These agents dynamically adapt to varying conditions and collaborate through a graph neural network that encodes their relationships and performance metrics. Our approach employs a coordination mechanism based on top-$k$ maximum spanning trees, optimizing the generation process. Each agentâ€™s decision-making is guided by a meta-model that minimizes a novel loss function, balancing accuracy and diversity. Theoretical analysis and extensive empirical evaluations show that LGR-AD outperforms traditional diffusion models across various benchmarks, highlighting its potential for scalable and flexible solutions in complex image generation tasks. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/YousIA/LGR_AD">https://github.com/YousIA/LGR_AD</a> </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒåˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œå±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ–‡æœ¬ç†è§£å’Œé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›æ¨¡å‹æ ¹æ®æ–‡æœ¬æç¤ºä»éšæœºå™ªå£°ä¸­ç»†åŒ–å›¾åƒï¼Œæœ€åˆä¾èµ–æ–‡æœ¬è¾“å…¥ï¼Œéšç€æ—¶é—´çš„æ¨ç§»ï¼Œå¯¹è§†è§‰é€¼çœŸåº¦çš„è¦æ±‚é€æ¸å¢å¼ºã€‚è¿™ç§è½¬å˜è¡¨æ˜ï¼Œé™æ€æ¨¡å‹å‚æ•°å¯èƒ½æ— æ³•æœ€ä½³åœ°åº”å¯¹ç”Ÿæˆçš„ä¸åŒé˜¶æ®µã€‚æˆ‘ä»¬å¼•å…¥äº†LGR-ADï¼ˆå­¦ä¹ ä»£ç†æ‰©æ•£å›¾è¡¨ç¤ºï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„å¤šä»£ç†ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜åŠ¨æ€è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„é€‚åº”æ€§ã€‚LGR-ADå°†ç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡ä¸ºç›¸äº’ä½œç”¨çš„ä»£ç†çš„åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œæ¯ä¸ªä»£ç†ä»£è¡¨ä¸€ä¸ªä¸“å®¶å­æ¨¡å‹ã€‚è¿™äº›ä»£ç†èƒ½å¤ŸåŠ¨æ€é€‚åº”å„ç§æ¡ä»¶ï¼Œå¹¶é€šè¿‡ç¼–ç å…¶å…³ç³»å’Œæ€§èƒ½æŒ‡æ ‡çš„å›¾ç¥ç»ç½‘ç»œè¿›è¡Œåä½œã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨åŸºäºtop-kæœ€å¤§ç”Ÿæˆæ ‘çš„åè°ƒæœºåˆ¶ï¼Œä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ã€‚æ¯ä¸ªä»£ç†çš„å†³ç­–ç”±å…ƒæ¨¡å‹æŒ‡å¯¼ï¼Œè¯¥æ¨¡å‹æœ€å°åŒ–æ–°å‹æŸå¤±å‡½æ•°ï¼Œåœ¨å‡†ç¡®æ€§å’Œå¤šæ ·æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ç†è®ºåˆ†æå’Œå¹¿æ³›çš„å®è¯ç ”ç©¶å‡è¡¨æ˜ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLGR-ADè¶…è¶Šäº†ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ï¼Œå‡¸æ˜¾å…¶åœ¨å¤æ‚å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å¯æ‰©å±•å’Œçµæ´»è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/YousIA/LGR_AD">https://github.com/YousIA/LGR_AD</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06761v2">PDF</a> Accepted at AAMAS2025 International Conference on Autonomous Agents   and Multiagent Systems</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒåˆæˆæ–¹é¢çš„é‡å¤§è¿›å±•ï¼Œå®ƒä»¬èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºä»éšæœºå™ªå£°ä¸­ä¼˜åŒ–å›¾åƒã€‚ä¸ºæ”¹å–„åŠ¨æ€è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„é€‚åº”æ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹å¤šæ™ºèƒ½ä½“ç³»ç»ŸLGR-ADã€‚LGR-ADå°†ç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡ä¸ºæ™ºèƒ½ä½“é—´çš„äº¤äº’ç³»ç»Ÿï¼Œæ¯ä¸ªæ™ºèƒ½ä½“ä»£è¡¨ä¸€ä¸ªä¸“å®¶å­æ¨¡å‹ï¼Œé€šè¿‡å›¾ç¥ç»ç½‘ç»œè¿›è¡Œåä½œä¸é€‚åº”ã€‚é‡‡ç”¨åŸºäºæœ€å¤§ç”Ÿæˆæ ‘çš„åè°ƒæœºåˆ¶ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ã€‚ç†è®ºåˆ†æå’Œå¤§é‡å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒLGR-ADåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ï¼Œå±•ç°å‡ºå…¶åœ¨å¤æ‚å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„å¯æ‰©å±•æ€§å’Œçµæ´»æ€§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒåˆæˆä¸­å–å¾—æ˜¾è‘—è¿›å±•ï¼Œå…·å¤‡å‡ºè‰²çš„æ–‡æœ¬ç†è§£å’Œé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é™æ€æ¨¡å‹å‚æ•°å¯èƒ½æ— æ³•æœ€ä½³åœ°å¤„ç†ç”Ÿæˆçš„ä¸åŒé˜¶æ®µã€‚</li>
<li>LGR-ADæ˜¯ä¸€ç§æ–°å‹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨æ”¹å–„åŠ¨æ€è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„é€‚åº”æ€§ã€‚</li>
<li>LGR-ADå°†ç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡ä¸ºæ™ºèƒ½ä½“é—´çš„äº¤äº’ç³»ç»Ÿï¼Œæ¯ä¸ªæ™ºèƒ½ä½“ä»£è¡¨ä¸€ä¸ªä¸“å®¶å­æ¨¡å‹ã€‚</li>
<li>é€šè¿‡å›¾ç¥ç»ç½‘ç»œç¼–ç æ™ºèƒ½ä½“ä¹‹é—´çš„å…³ç³»å’Œæ€§èƒ½åº¦é‡ï¼Œå®ç°æ™ºèƒ½ä½“é—´çš„åŠ¨æ€é€‚åº”å’Œåä½œã€‚</li>
<li>LGR-ADé‡‡ç”¨åŸºäºæœ€å¤§ç”Ÿæˆæ ‘çš„åè°ƒæœºåˆ¶ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>LGR-ADåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ï¼Œå±•ç°å‡ºå…¶åœ¨å¤æ‚å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06761">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-52afdcbeb8f1b9ac4eb2a4af2a93e715.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-428f003d5fa29766e837326999873613.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8cfdf1a0e8aed0dd8665d235d5738b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e1a3ecdffb9ece957bc67471b5e4da1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a55429115755f76db01b65fc7f316bbf.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Single-View-Garment-Reconstruction-Using-Diffusion-Mapping-Via-Pattern-Coordinates"><a href="#Single-View-Garment-Reconstruction-Using-Diffusion-Mapping-Via-Pattern-Coordinates" class="headerlink" title="Single View Garment Reconstruction Using Diffusion Mapping Via Pattern   Coordinates"></a>Single View Garment Reconstruction Using Diffusion Mapping Via Pattern   Coordinates</h2><p><strong>Authors:Ren Li, Cong Cao, Corentin Dumery, Yingxuan You, Hao Li, Pascal Fua</strong></p>
<p>Reconstructing 3D clothed humans from images is fundamental to applications like virtual try-on, avatar creation, and mixed reality. While recent advances have enhanced human body recovery, accurate reconstruction of garment geometry â€“ especially for loose-fitting clothing â€“ remains an open challenge. We present a novel method for high-fidelity 3D garment reconstruction from single images that bridges 2D and 3D representations. Our approach combines Implicit Sewing Patterns (ISP) with a generative diffusion model to learn rich garment shape priors in a 2D UV space. A key innovation is our mapping model that establishes correspondences between 2D image pixels, UV pattern coordinates, and 3D geometry, enabling joint optimization of both 3D garment meshes and the corresponding 2D patterns by aligning learned priors with image observations. Despite training exclusively on synthetically simulated cloth data, our method generalizes effectively to real-world images, outperforming existing approaches on both tight- and loose-fitting garments. The reconstructed garments maintain physical plausibility while capturing fine geometric details, enabling downstream applications including garment retargeting and texture manipulation. </p>
<blockquote>
<p>ä»å›¾åƒé‡å»º3Dç€è£…äººä½“å¯¹äºè™šæ‹Ÿè¯•ç©¿ã€è§’è‰²åˆ›å»ºå’Œæ··åˆç°å®ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚è™½ç„¶æœ€è¿‘çš„è¿›æ­¥å¢å¼ºäº†äººä½“æ¢å¤èƒ½åŠ›ï¼Œä½†è¡£ç‰©å‡ ä½•å½¢çŠ¶çš„ç²¾ç¡®é‡å»ºâ€”â€”å°¤å…¶æ˜¯å®½æ¾è¡£ç‰©â€”â€”ä»ç„¶æ˜¯ä¸€ä¸ªå¾…è§£å†³çš„éš¾é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»å•å¹…å›¾åƒè¿›è¡Œé«˜ä¿çœŸ3Dè¡£ç‰©é‡å»ºçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†éšå¼ç¼çº«æ¨¡å¼ï¼ˆISPï¼‰å’Œç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼Œåœ¨äºŒç»´UVç©ºé—´ä¸­å­¦ä¹ ä¸°å¯Œçš„è¡£ç‰©å½¢çŠ¶å…ˆéªŒçŸ¥è¯†ã€‚æˆ‘ä»¬çš„ä¸€ä¸ªå…³é”®åˆ›æ–°æ˜¯æ˜ å°„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å»ºç«‹äº†äºŒç»´å›¾åƒåƒç´ ã€UVæ¨¡å¼åæ ‡å’Œä¸‰ç»´å‡ ä½•ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œé€šè¿‡ä¼˜åŒ–å›¾åƒè§‚å¯Ÿå’Œå­¦ä¹ çš„å…ˆéªŒçŸ¥è¯†ï¼Œå®ç°å¯¹ä¸‰ç»´è¡£ç‰©ç½‘æ ¼å’Œç›¸åº”äºŒç»´æ¨¡å¼çš„è”åˆä¼˜åŒ–ã€‚å°½ç®¡ä»…åœ¨åˆæˆæ¨¡æ‹Ÿçš„ç»‡ç‰©æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•å¯¹äºçœŸå®ä¸–ç•Œå›¾åƒçš„æ¨å¹¿æ•ˆæœå¾ˆå¥½ï¼Œåœ¨ç´§èº«å’Œå®½æ¾è¡£ç‰©ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚é‡å»ºçš„è¡£ç‰©åœ¨ä¿æŒç‰©ç†åˆç†æ€§çš„åŒæ—¶æ•æ‰äº†ç²¾ç»†çš„å‡ ä½•ç»†èŠ‚ï¼Œå¯å®ç°ä¸‹æ¸¸åº”ç”¨ï¼ŒåŒ…æ‹¬è¡£ç‰©é‡æ–°å®šä½å’Œçº¹ç†æ“ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08353v2">PDF</a> SIGGRAPH 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ä»å•å¼ å›¾åƒè¿›è¡Œé«˜ä¿çœŸ3Dæœè£…é‡å»ºçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†éšå¼ç¼çº«æ¨¡å¼ï¼ˆISPï¼‰å’Œç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹ï¼Œåœ¨äºŒç»´UVç©ºé—´å­¦ä¹ ä¸°å¯Œçš„æœè£…å½¢çŠ¶å…ˆéªŒã€‚é€šè¿‡å»ºç«‹äºŒç»´å›¾åƒåƒç´ ã€UVæ¨¡å¼åæ ‡å’Œä¸‰ç»´å‡ ä½•ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œå®ç°äº†å¯¹ä¸‰ç»´æœè£…ç½‘æ ¼å’Œç›¸åº”äºŒç»´æ¨¡å¼çš„è”åˆä¼˜åŒ–ã€‚å°½ç®¡ä»…åœ¨åˆæˆæ¨¡æ‹Ÿçš„å¸ƒæ–™æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†è¯¥æ–¹æ³•åœ¨çœŸå®å›¾åƒä¸Šçš„è¡¨ç°å´éå¸¸å‡ºè‰²ï¼Œå¯¹äºç´§èº«å’Œå®½æ¾æœè£…çš„é‡å»ºéƒ½æœ‰å‡ºè‰²çš„è¡¨ç°ã€‚é‡å»ºçš„æœè£…æ—¢ä¿æŒäº†ç‰©ç†ä¸Šçš„å¯ä¿¡åº¦ï¼Œåˆèƒ½æ•æ‰ç²¾ç»†çš„å‡ ä½•ç»†èŠ‚ï¼Œä¸ºä¸‹æ¸¸åº”ç”¨å¦‚æœè£…é‡å®šå‘å’Œçº¹ç†æ“ä½œæä¾›äº†å¯èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°çš„ä»å•å¼ å›¾åƒè¿›è¡Œé«˜ä¿çœŸ3Dæœè£…é‡å»ºçš„æ–¹æ³•ã€‚</li>
<li>ç»“åˆäº†éšå¼ç¼çº«æ¨¡å¼ï¼ˆISPï¼‰å’Œç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹ï¼Œåœ¨äºŒç»´UVç©ºé—´å­¦ä¹ æœè£…å½¢çŠ¶å…ˆéªŒã€‚</li>
<li>å»ºç«‹äº†ä¸€ç§æ˜ å°„æ¨¡å‹ï¼Œå®ç°äº†äºŒç»´å›¾åƒåƒç´ ã€UVæ¨¡å¼åæ ‡å’Œä¸‰ç»´å‡ ä½•ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚</li>
<li>æ–¹æ³•èƒ½å¤Ÿåœ¨çœŸå®å›¾åƒä¸Šå®ç°æœ‰æ•ˆçš„é‡å»ºï¼Œé€‚ç”¨äºç´§èº«å’Œå®½æ¾æœè£…ã€‚</li>
<li>é‡å»ºçš„æœè£…ä¿æŒäº†ç‰©ç†ä¸Šçš„å¯ä¿¡åº¦ï¼Œå¹¶èƒ½æ•æ‰ç²¾ç»†çš„å‡ ä½•ç»†èŠ‚ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä¸ºä¸‹æ¸¸åº”ç”¨å¦‚æœè£…é‡å®šå‘å’Œçº¹ç†æ“ä½œæä¾›æ”¯æŒã€‚</li>
<li>å°½ç®¡åªåœ¨åˆæˆæ¨¡æ‹Ÿçš„å¸ƒæ–™æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†è¯¥æ–¹æ³•å…·æœ‰å¾ˆå¥½çš„æ³›åŒ–æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08353">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a8de8baf34a3e6da3da7a36d55608f0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77e76bf283624e8be74ab693d90e838d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ea211b0a88b210259a9214bfd1051ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-991b386d2e9df0ad89f79e3d08724ddb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52f32a77363e23316a5af9a1057e2a9a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CreativeSynth-Cross-Art-Attention-for-Artistic-Image-Synthesis-with-Multimodal-Diffusion"><a href="#CreativeSynth-Cross-Art-Attention-for-Artistic-Image-Synthesis-with-Multimodal-Diffusion" class="headerlink" title="CreativeSynth: Cross-Art-Attention for Artistic Image Synthesis with   Multimodal Diffusion"></a>CreativeSynth: Cross-Art-Attention for Artistic Image Synthesis with   Multimodal Diffusion</h2><p><strong>Authors:Nisha Huang, Weiming Dong, Yuxin Zhang, Fan Tang, Ronghui Li, Chongyang Ma, Xiu Li, Tong-Yee Lee, Changsheng Xu</strong></p>
<p>Although remarkable progress has been made in image style transfer, style is just one of the components of artistic paintings. Directly transferring extracted style features to natural images often results in outputs with obvious synthetic traces. This is because key painting attributes including layout, perspective, shape, and semantics often cannot be conveyed and expressed through style transfer. Large-scale pretrained text-to-image generation models have demonstrated their capability to synthesize a vast amount of high-quality images. However, even with extensive textual descriptions, it is challenging to fully express the unique visual properties and details of paintings. Moreover, generic models often disrupt the overall artistic effect when modifying specific areas, making it more complicated to achieve a unified aesthetic in artworks. Our main novel idea is to integrate multimodal semantic information as a synthesis guide into artworks, rather than transferring style to the real world. We also aim to reduce the disruption to the harmony of artworks while simplifying the guidance conditions. Specifically, we propose an innovative multi-task unified framework called CreativeSynth, based on the diffusion model with the ability to coordinate multimodal inputs. CreativeSynth combines multimodal features with customized attention mechanisms to seamlessly integrate real-world semantic content into the art domain through Cross-Art-Attention for aesthetic maintenance and semantic fusion. We demonstrate the results of our method across a wide range of different art categories, proving that CreativeSynth bridges the gap between generative models and artistic expression. Code and results are available at <a target="_blank" rel="noopener" href="https://github.com/haha-lisa/CreativeSynth">https://github.com/haha-lisa/CreativeSynth</a>. </p>
<blockquote>
<p>è™½ç„¶å›¾åƒé£æ ¼è½¬æ¢å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†é£æ ¼ä»…ä»…æ˜¯è‰ºæœ¯ä½œå“ç»„ä»¶ä¹‹ä¸€ã€‚ç›´æ¥å°†æå–çš„é£æ ¼ç‰¹å¾è½¬ç§»åˆ°è‡ªç„¶å›¾åƒä¸Šï¼Œå¾€å¾€ä¼šå¯¼è‡´è¾“å‡ºç»“æœå¸¦æœ‰æ˜æ˜¾çš„åˆæˆç—•è¿¹ã€‚è¿™æ˜¯å› ä¸ºå¸ƒå±€ã€é€è§†ã€å½¢çŠ¶å’Œè¯­ä¹‰ç­‰å…³é”®ç»˜ç”»å±æ€§å¾€å¾€æ— æ³•é€šè¿‡é£æ ¼è½¬æ¢æ¥ä¼ è¾¾å’Œè¡¨è¾¾ã€‚å¤§è§„æ¨¡é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å·²ç»è¯æ˜äº†å®ƒä»¬åˆæˆå¤§é‡é«˜è´¨é‡å›¾åƒçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå³ä½¿ä½¿ç”¨è¯¦ç»†çš„æ–‡æœ¬æè¿°ï¼Œä¹Ÿå¾ˆéš¾å……åˆ†è¡¨è¾¾ç»˜ç”»çš„ç‹¬ç‰¹è§†è§‰å±æ€§å’Œç»†èŠ‚ã€‚æ­¤å¤–ï¼Œé€šç”¨æ¨¡å‹åœ¨ä¿®æ”¹ç‰¹å®šåŒºåŸŸæ—¶å¾€å¾€ä¼šç ´åæ•´ä½“çš„è‰ºæœ¯æ•ˆæœï¼Œä½¿è‰ºæœ¯ä½œå“ä¸­å®ç°ç»Ÿä¸€ç¾å­¦æ›´åŠ å¤æ‚ã€‚æˆ‘ä»¬çš„ä¸»è¦æ–°æ€æƒ³æ˜¯å°†å¤šæ¨¡æ€è¯­ä¹‰ä¿¡æ¯ä½œä¸ºåˆæˆæŒ‡å—æ•´åˆåˆ°è‰ºæœ¯ä½œå“ä¸­ï¼Œè€Œä¸æ˜¯å°†é£æ ¼è½¬ç§»åˆ°ç°å®ä¸–ç•Œã€‚æˆ‘ä»¬è¿˜æ—¨åœ¨å‡å°‘äº†å¯¹è‰ºæœ¯ä½œå“å’Œè°åº¦çš„ç ´åï¼ŒåŒæ—¶ç®€åŒ–äº†æŒ‡å¯¼æ¡ä»¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹ã€èƒ½å¤Ÿåè°ƒå¤šæ¨¡æ€è¾“å…¥çš„åˆ›æ–°å¤šä»»åŠ¡ç»Ÿä¸€æ¡†æ¶ï¼Œåä¸ºCreativeSynthã€‚CreativeSynthç»“åˆå¤šæ¨¡æ€ç‰¹å¾å’Œè‡ªå®šä¹‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡è·¨è‰ºæœ¯æ³¨æ„åŠ›æœºåˆ¶æ— ç¼æ•´åˆç°å®ä¸–ç•Œè¯­ä¹‰å†…å®¹åˆ°è‰ºæœ¯é¢†åŸŸï¼Œä»¥ç»´æŒå®¡ç¾å’Œè¯­ä¹‰èåˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹¿æ³›çš„ä¸åŒè‰ºæœ¯ç±»åˆ«ä¸­å¾—åˆ°äº†éªŒè¯ï¼Œè¯æ˜äº†CreativeSynthåœ¨ç”Ÿæˆæ¨¡å‹å’Œè‰ºæœ¯è¡¨è¾¾ä¹‹é—´çš„æ¡¥æ¢ä½œç”¨ã€‚ç›¸å…³ä»£ç å’Œç»“æœå¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/haha-lisa/CreativeSynth%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/haha-lisa/CreativeSynthè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.14066v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å¤šä»»åŠ¡ç»Ÿä¸€æ¡†æ¶CreativeSynthï¼Œè¯¥æ¡†æ¶ç»“åˆè·¨è‰ºæœ¯æ³¨æ„åŠ›çš„å¤šæ¨¡æ€è¯­ä¹‰ä¿¡æ¯ï¼Œå°†ç°å®ä¸–ç•Œè¯­ä¹‰å†…å®¹æ— ç¼é›†æˆåˆ°è‰ºæœ¯é¢†åŸŸï¼Œæ—¨åœ¨ä¿ƒè¿›è‰ºæœ¯ä½œå“çš„åˆæˆå’Œç¾å­¦ç»´æŠ¤ã€‚é€šè¿‡æ•´åˆå¸ƒå±€ã€é€è§†ã€å½¢çŠ¶å’Œè¯­ä¹‰ç­‰å…³é”®ç»˜ç”»å±æ€§ï¼ŒCreativeSynthåœ¨å„ç±»è‰ºæœ¯ç±»åˆ«ä¸­å±•ç°å‡ºå¼ºå¤§çš„è¡¨ç°åŠ›ï¼Œç¼©å°äº†ç”Ÿæˆæ¨¡å‹ä¸è‰ºæœ¯åˆ›ä½œä¹‹é—´çš„å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å›¾åƒé£æ ¼è½¬æ¢å­˜åœ¨åˆæˆç—•è¿¹æ˜æ˜¾çš„é—®é¢˜ï¼Œå› ä¸ºå…³é”®ç»˜ç”»å±æ€§å¦‚å¸ƒå±€ã€é€è§†ã€å½¢çŠ¶å’Œè¯­ä¹‰ç­‰éš¾ä»¥é€šè¿‡é£æ ¼è½¬æ¢è¡¨è¾¾ã€‚</li>
<li>å¤§è§„æ¨¡é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹è™½èƒ½åˆæˆé«˜è´¨é‡å›¾åƒï¼Œä½†éš¾ä»¥å®Œå…¨è¡¨è¾¾ç»˜ç”»çš„ç‹¬ç‰¹è§†è§‰å±æ€§å’Œç»†èŠ‚ã€‚</li>
<li>é›†æˆå¤šæ¨¡æ€è¯­ä¹‰ä¿¡æ¯ä½œä¸ºåˆæˆæŒ‡å—ï¼Œè€Œéå°†é£æ ¼è½¬ç§»åˆ°ç°å®ä¸–ç•Œï¼Œæœ‰åŠ©äºè§£å†³è‰ºæœ¯æ•ˆæœä¸­æ–­å’Œç¾å­¦ç»Ÿä¸€æ€§é—®é¢˜ã€‚</li>
<li>æå‡ºçš„CreativeSynthæ¡†æ¶åŸºäºæ‰©æ•£æ¨¡å‹ï¼Œèƒ½åè°ƒå¤šæ¨¡æ€è¾“å…¥ï¼Œç»“åˆå®šåˆ¶æ³¨æ„åŠ›æœºåˆ¶å’Œè·¨è‰ºæœ¯æ³¨æ„åŠ›ï¼Œå®ç°è¯­ä¹‰èåˆå’Œç¾å­¦ç»´æŠ¤ã€‚</li>
<li>CreativeSynthåœ¨å¤šç§è‰ºæœ¯ç±»åˆ«ä¸­å±•ç°å‡ºå¼ºå¤§çš„è¡¨ç°åŠ›ï¼Œæœ‰æ•ˆç¼©å°äº†ç”Ÿæˆæ¨¡å‹ä¸è‰ºæœ¯åˆ›ä½œä¹‹é—´çš„å·®è·ã€‚</li>
<li>CreativeSynthçš„æ–¹æ³•å’Œç»“æœå¯é€šè¿‡åœ¨çº¿ä»£ç åº“ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/haha-lisa/CreativeSynth%EF%BC%89%E8%BF%9B%E8%A1%8C%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/haha-lisa/CreativeSynthï¼‰è¿›è¡Œè®¿é—®ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.14066">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d6a5e17cac662929d5507cb89c446696.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3e5a7055bf2406152e10d21493a5928.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91a4013ba282057991cfd41deee18845.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87dfbe916028aff3bc5ebda0bf2b8f47.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6dfd5b4272d4054b61a6efc9b1bf2607.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-17/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-17/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1544906ac0105b934677cb959a2af3b7.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-17  Computational screening and experimental validation of promising   Wadsley-Roth Niobates
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-17/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8b6af2d789a0018f2c1325543b29c800.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-17  Large-Scale Gaussian Splatting SLAM
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32127.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
