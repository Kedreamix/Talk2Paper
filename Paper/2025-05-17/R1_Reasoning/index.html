<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-17  MathCoder-VL Bridging Vision and Code for Enhanced Multimodal   Mathematical Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-5500fd3be27b12fed78b895cb32542d0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-17-æ›´æ–°"><a href="#2025-05-17-æ›´æ–°" class="headerlink" title="2025-05-17 æ›´æ–°"></a>2025-05-17 æ›´æ–°</h1><h2 id="MathCoder-VL-Bridging-Vision-and-Code-for-Enhanced-Multimodal-Mathematical-Reasoning"><a href="#MathCoder-VL-Bridging-Vision-and-Code-for-Enhanced-Multimodal-Mathematical-Reasoning" class="headerlink" title="MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal   Mathematical Reasoning"></a>MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal   Mathematical Reasoning</h2><p><strong>Authors:Ke Wang, Junting Pan, Linda Wei, Aojun Zhou, Weikang Shi, Zimu Lu, Han Xiao, Yunqiao Yang, Houxing Ren, Mingjie Zhan, Hongsheng Li</strong></p>
<p>Natural language image-caption datasets, widely used for training Large Multimodal Models, mainly focus on natural scenarios and overlook the intricate details of mathematical figures that are critical for problem-solving, hindering the advancement of current LMMs in multimodal mathematical reasoning. To this end, we propose leveraging code as supervision for cross-modal alignment, since code inherently encodes all information needed to generate corresponding figures, establishing a precise connection between the two modalities. Specifically, we co-develop our image-to-code model and dataset with model-in-the-loop approach, resulting in an image-to-code model, FigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date. Furthermore, we utilize FigCodifier to synthesize novel mathematical figures and then construct MM-MathInstruct-3M, a high-quality multimodal math instruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with ImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on MM-MathInstruct-3M for multimodal math problem solving. Our model achieves a new open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and Claude 3.5 Sonnet in the geometry problem-solving subset of MathVista, achieving improvements of 8.9% and 9.2%. The dataset and models will be released at <a target="_blank" rel="noopener" href="https://github.com/mathllm/MathCoder">https://github.com/mathllm/MathCoder</a>. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€å›¾åƒæ ‡é¢˜æ•°æ®é›†å¹¿æ³›åº”ç”¨äºè®­ç»ƒå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œå…¶ä¸»è¦å…³æ³¨è‡ªç„¶åœºæ™¯ï¼Œå¿½è§†äº†æ•°å­¦å›¾å½¢ä¸­çš„å¤æ‚ç»†èŠ‚ï¼Œè¿™äº›ç»†èŠ‚å¯¹äºé—®é¢˜è§£å†³è‡³å…³é‡è¦ï¼Œä»è€Œé˜»ç¢äº†å½“å‰å¤šæ¨¡æ€æ•°å­¦æ¨ç†LMMsçš„å‘å±•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨ä»£ç ä½œä¸ºè·¨æ¨¡æ€å¯¹é½çš„ç›‘ç£ä¿¡æ¯ï¼Œå› ä¸ºä»£ç æœ¬èº«å°±åŒ…å«äº†ç”Ÿæˆç›¸åº”å›¾å½¢æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯ï¼Œä»è€Œåœ¨è¿™ä¸¤ç§æ¨¡æ€ä¹‹é—´å»ºç«‹äº†ç²¾ç¡®çš„è”ç³»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨å¸¦æ¨¡å‹å¾ªç¯çš„æ–¹æ³•ï¼Œå…±åŒå¼€å‘å›¾åƒåˆ°ä»£ç çš„æ¨¡å‹å’Œæ•°æ®é›†ï¼Œä»è€Œå¾—åˆ°å›¾åƒåˆ°ä»£ç çš„æ¨¡å‹FigCodifierä»¥åŠè¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å›¾åƒä»£ç æ•°æ®é›†ImgCode-8.6Mã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨FigCodifieråˆæˆæ–°å‹æ•°å­¦å›¾å½¢ï¼Œç„¶åæ„å»ºé«˜è´¨é‡çš„å¤šæ¨¡æ€æ•°å­¦æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†MM-MathInstruct-3Mã€‚æœ€åï¼Œæˆ‘ä»¬æ¨å‡ºMathCoder-VLï¼Œè¯¥æ¨¡å‹ä»¥ImgCode-8.6Mè¿›è¡Œè·¨æ¨¡æ€å¯¹é½è®­ç»ƒï¼Œéšååœ¨MM-MathInstruct-3Mä¸Šè¿›è¡Œå¤šæ¨¡æ€æ•°å­¦é—®é¢˜è§£å†³çš„å¾®è°ƒã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å…­ä¸ªæŒ‡æ ‡ä¸Šè¾¾åˆ°æ–°çš„å¼€æºæ°´å¹³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨MathVistaçš„å‡ ä½•é—®é¢˜è§£å†³å­é›†ä¸­è¶…è¶Šäº†GPT-4oå’ŒClaude 3.5 Sonnetï¼Œåˆ†åˆ«æé«˜äº†8.9%å’Œ9.2%ã€‚æ•°æ®é›†å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/mathllm/MathCoder%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/mathllm/MathCoderå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10557v1">PDF</a> Accepted to ACL 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨ä»£ç ä½œä¸ºç›‘ç£æ¥å®ç°è·¨æ¨¡æ€å¯¹é½çš„æ–¹æ³•ï¼Œè§£å†³äº†å½“å‰è‡ªç„¶è¯­è¨€å›¾åƒæ•°æ®é›†åœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†æ–¹é¢çš„ä¸è¶³ã€‚æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°å‹å›¾åƒåˆ°ä»£ç çš„æ¨¡å‹FigCodifierä»¥åŠä¸è¯¥æ¨¡å‹å…±åŒå¼€å‘çš„ImgCode-8.6Mæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜åˆ©ç”¨FigCodifieråˆæˆæ–°å‹æ•°å­¦å›¾å½¢ï¼Œæ„å»ºäº†é«˜è´¨é‡çš„å¤šæ¨¡æ€æ•°å­¦æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†MM-MathInstruct-3Mã€‚æœ€åï¼Œæ–‡ç« ä»‹ç»äº†ä½¿ç”¨ImgCode-8.6Mè¿›è¡Œè·¨æ¨¡æ€å¯¹é½ï¼Œå¹¶åœ¨MM-MathInstruct-3Mä¸Šå¾®è°ƒç”¨äºå¤šæ¨¡æ€æ•°å­¦é—®é¢˜è§£å†³æ¨¡å‹çš„MathCoder-VLæ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨å…­ä¸ªæŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æ–°çš„å¼€æºæ°´å¹³ï¼Œä¸”åœ¨MathVistaçš„å‡ ä½•é—®é¢˜è§£å†³å­é›†ä¸­è¡¨ç°ä¼˜äºGPT-4oå’ŒClaude 3.5 Sonnetï¼Œæé«˜äº†8.9%å’Œ9.2%ã€‚æ•°æ®é›†å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/mathllm/MathCoder%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/mathllm/MathCoderä¸Šå‘å¸ƒã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰çš„è‡ªç„¶è¯­è¨€å›¾åƒæ•°æ®é›†åœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéœ€è¦å…³æ³¨æ•°å­¦å›¾å½¢çš„ç»†èŠ‚ã€‚</li>
<li>æå‡ºåˆ©ç”¨ä»£ç ä½œä¸ºç›‘ç£å®ç°è·¨æ¨¡æ€å¯¹é½çš„æ–¹æ³•ï¼Œä»£ç æœ¬èº«åŒ…å«äº†ç”Ÿæˆå›¾å½¢çš„æ‰€æœ‰ä¿¡æ¯ã€‚</li>
<li>å¼€å‘äº†å›¾åƒåˆ°ä»£ç çš„æ¨¡å‹FigCodifierä»¥åŠä¸è¯¥æ¨¡å‹é…å¥—çš„æ•°æ®é›†ImgCode-8.6Mï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å›¾åƒä»£ç æ•°æ®é›†ã€‚</li>
<li>åˆ©ç”¨FigCodifieråˆæˆæ–°å‹æ•°å­¦å›¾å½¢ï¼Œæ„å»ºé«˜è´¨é‡çš„å¤šæ¨¡æ€æ•°å­¦æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†MM-MathInstruct-3Mã€‚</li>
<li>ä»‹ç»äº†MathCoder-VLæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨ImgCode-8.6Mè¿›è¡Œè·¨æ¨¡æ€å¯¹é½ï¼Œå¹¶åœ¨MM-MathInstruct-3Mä¸Šè¿›è¡Œå¾®è°ƒï¼Œç”¨äºå¤šæ¨¡æ€æ•°å­¦é—®é¢˜è§£å†³ã€‚</li>
<li>MathCoder-VLæ¨¡å‹åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¾¾åˆ°æ–°çš„å¼€æºæ°´å¹³ï¼Œå¹¶åœ¨MathVistaçš„å‡ ä½•é—®é¢˜è§£å†³å­é›†ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºGPT-4oå’ŒClaude 3.5 Sonnetæœ‰æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10557">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e4d29063f02be34736cff804300ecc86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab67c64fef281449db85589434d8c7fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30f4fafcd7ac739acf75b5c560ac4305.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a63fd513e80d0a89097667c74dcc5e6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c079aeec784bf563b27f75524a779297.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c3a51be54b9276b9610f47718046f00.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Beyond-â€˜Aha-â€™-Toward-Systematic-Meta-Abilities-Alignment-in-Large-Reasoning-Models"><a href="#Beyond-â€˜Aha-â€™-Toward-Systematic-Meta-Abilities-Alignment-in-Large-Reasoning-Models" class="headerlink" title="Beyond â€˜Aha!â€™: Toward Systematic Meta-Abilities Alignment in Large   Reasoning Models"></a>Beyond â€˜Aha!â€™: Toward Systematic Meta-Abilities Alignment in Large   Reasoning Models</h2><p><strong>Authors:Zhiyuan Hu, Yibo Wang, Hanze Dong, Yuhui Xu, Amrita Saha, Caiming Xiong, Bryan Hooi, Junnan Li</strong></p>
<p>Large reasoning models (LRMs) already possess a latent capacity for long chain-of-thought reasoning. Prior work has shown that outcome-based reinforcement learning (RL) can incidentally elicit advanced reasoning behaviors such as self-correction, backtracking, and verification phenomena often referred to as the modelâ€™s â€œaha momentâ€. However, the timing and consistency of these emergent behaviors remain unpredictable and uncontrollable, limiting the scalability and reliability of LRMsâ€™ reasoning capabilities. To address these limitations, we move beyond reliance on prompts and coincidental â€œaha momentsâ€. Instead, we explicitly align models with three meta-abilities: deduction, induction, and abduction, using automatically generated, self-verifiable tasks. Our three stage-pipeline individual alignment, parameter-space merging, and domain-specific reinforcement learning, boosting performance by over 10% relative to instruction-tuned baselines. Furthermore, domain-specific RL from the aligned checkpoint yields an additional 2% average gain in the performance ceiling across math, coding, and science benchmarks, demonstrating that explicit meta-ability alignment offers a scalable and dependable foundation for reasoning. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/zhiyuanhubj/Meta-Ability-Alignment">https://github.com/zhiyuanhubj/Meta-Ability-Alignment</a> </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å·²ç»å…·å¤‡æ½œåœ¨çš„é•¿é“¾æ€ç»´æ¨ç†èƒ½åŠ›ã€‚å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥å¶ç„¶æ¿€å‘é«˜çº§æ¨ç†è¡Œä¸ºï¼Œå¦‚è‡ªæˆ‘æ ¡æ­£ã€å›æº¯å’ŒéªŒè¯ç°è±¡ï¼Œè¿™äº›å¸¸è¢«çœ‹ä½œæ˜¯æ¨¡å‹çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ã€‚ç„¶è€Œï¼Œè¿™äº›çªå‘è¡Œä¸ºçš„æ—¶æœºå’Œä¸€è‡´æ€§ä»ç„¶ä¸å¯é¢„æµ‹å’Œä¸å¯æ§åˆ¶ï¼Œé™åˆ¶äº†LRMsæ¨ç†èƒ½åŠ›çš„å¯æ‰©å±•æ€§å’Œå¯é æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬ä¸å†ä¾èµ–æç¤ºå’Œå¶ç„¶çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ã€‚ç›¸åï¼Œæˆ‘ä»¬ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆã€è‡ªæˆ‘éªŒè¯çš„ä»»åŠ¡ï¼Œæ˜ç¡®åœ°ä½¿æ¨¡å‹ä¸ä¸‰ç§å…ƒèƒ½åŠ›ï¼ˆæ¼”ç»ã€å½’çº³å’Œæº¯å› ï¼‰å¯¹é½ã€‚æˆ‘ä»¬çš„ä¸‰é˜¶æ®µç®¡é“åŒ…æ‹¬ä¸ªä½“å¯¹é½ã€å‚æ•°ç©ºé—´åˆå¹¶å’Œé¢†åŸŸç‰¹å®šå¼ºåŒ–å­¦ä¹ ï¼Œç›¸è¾ƒäºæŒ‡ä»¤è°ƒæ•´åŸºå‡†çº¿ï¼Œæ€§èƒ½æå‡è¶…è¿‡10%ã€‚æ­¤å¤–ï¼Œä»å¯¹é½æ£€æŸ¥ç‚¹è¿›è¡Œçš„é¢†åŸŸç‰¹å®šRLï¼Œåœ¨æ•°å­¦ã€ç¼–ç å’Œç§‘å­¦åŸºå‡†æµ‹è¯•çš„æ€§èƒ½ä¸Šé™ä¸Šè·å¾—äº†é¢å¤–çš„2%çš„å¹³å‡æå‡ï¼Œè¿™è¯æ˜æ˜ç¡®çš„å…ƒèƒ½åŠ›å¯¹é½ä¸ºæ¨ç†æä¾›äº†å¯æ‰©å±•å’Œå¯é çš„åŸºçŸ³ã€‚ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/zhiyanhubj/Meta-Ability-Alignment%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zhiyuanhubj/Meta-Ability-Alignmentä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10554v1">PDF</a> In Progress</p>
<p><strong>Summary</strong>ï¼šå¤§å‹æ¨ç†æ¨¡å‹å·²å…·å¤‡æ½œåœ¨çš„é•¿é“¾æ€ç»´æ¨ç†èƒ½åŠ›ã€‚å…ˆå‰çš„å·¥ä½œè¡¨æ˜ï¼ŒåŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ å¯ä»¥å¶ç„¶å¼•å‘é«˜çº§æ¨ç†è¡Œä¸ºï¼Œå¦‚è‡ªæˆ‘ä¿®æ­£ã€å›æº¯å’ŒéªŒè¯ç°è±¡ï¼Œè¿™äº›å¸¸è¢«ç§°ä¸ºæ¨¡å‹çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ã€‚ç„¶è€Œï¼Œè¿™äº›è¡Œä¸ºçš„æ¶Œç°æ—¶æœºå’ŒæŒç»­æ€§ä»ç„¶ä¸å¯é¢„æµ‹å’Œä¸å¯æ§åˆ¶ï¼Œé™åˆ¶äº†å¤§å‹æ¨ç†æ¨¡å‹çš„å¯æ‰©å±•æ€§å’Œå¯é æ€§ã€‚ä¸ºè§£å†³è¿™äº›å±€é™æ€§ï¼Œç ”ç©¶ä¸å†ä¾èµ–æç¤ºå’Œå¶ç„¶çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ï¼Œè€Œæ˜¯æ˜ç¡®åœ°å°†æ¨¡å‹ä¸æ¼”ç»ã€å½’çº³å’Œæº¯å› ä¸‰ç§å…ƒèƒ½åŠ›å¯¹é½ï¼Œå¹¶ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„è‡ªæˆ‘éªŒè¯ä»»åŠ¡ã€‚ä¸‰é˜¶æ®µç®¡é“åŒ…æ‹¬ä¸ªä½“å¯¹é½ã€å‚æ•°ç©ºé—´åˆå¹¶å’Œé¢†åŸŸç‰¹å®šå¼ºåŒ–å­¦ä¹ ï¼Œç›¸è¾ƒäºæŒ‡ä»¤è°ƒæ•´åŸºå‡†çº¿ï¼Œæ€§èƒ½æå‡è¶…è¿‡10%ã€‚æ­¤å¤–ï¼Œä»å¯¹é½æ£€æŸ¥ç‚¹è¿›è¡Œçš„é¢†åŸŸç‰¹å®šå¼ºåŒ–å­¦ä¹ åœ¨è·¨æ•°å­¦ã€ç¼–ç å’Œç§‘å­¦åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é¢å¤–çš„å¹³å‡2%çš„æ€§èƒ½æå‡ï¼Œè¡¨æ˜æ˜ç¡®çš„å…ƒèƒ½åŠ›å¯¹é½ä¸ºæ¨ç†æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•å’Œå¯é çš„åŸºçŸ³ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹å…·æœ‰æ½œåœ¨çš„é•¿é“¾æ€ç»´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ å¯ä»¥å¼•å‘æ¨¡å‹çš„è‡ªæˆ‘ä¿®æ­£ã€å›æº¯å’ŒéªŒè¯ç­‰é«˜çº§æ¨ç†è¡Œä¸ºã€‚</li>
<li>è¿™äº›æ¨ç†è¡Œä¸ºçš„æ¶Œç°æ—¶æœºå’ŒæŒç»­æ€§ä¸å¯é¢„æµ‹å’Œä¸å¯æ§åˆ¶ï¼Œé™åˆ¶äº†æ¨¡å‹çš„å¯é æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>æ˜ç¡®åœ°å°†æ¨¡å‹ä¸æ¼”ç»ã€å½’çº³å’Œæº¯å› ä¸‰ç§å…ƒèƒ½åŠ›å¯¹é½ï¼Œä»¥æé«˜æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„è‡ªæˆ‘éªŒè¯ä»»åŠ¡æ¥å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ä¸‰é˜¶æ®µç®¡é“åŒ…æ‹¬ä¸ªä½“å¯¹é½ã€å‚æ•°ç©ºé—´åˆå¹¶å’Œé¢†åŸŸç‰¹å®šå¼ºåŒ–å­¦ä¹ ï¼Œç›¸è¾ƒäºåŸºçº¿æé«˜äº†æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10554">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3f2cdb9bb318b8fdb7115f90d70dc353.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05c9191149f0e07ab5e9327a2ffddf0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1815a5638adfde7422cac82b2a92027.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83f023075ae2d2c21b66bc0f715056c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ceff764920545f074eec19f8b1aa46c0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Real-Time-Out-of-Distribution-Failure-Prevention-via-Multi-Modal-Reasoning"><a href="#Real-Time-Out-of-Distribution-Failure-Prevention-via-Multi-Modal-Reasoning" class="headerlink" title="Real-Time Out-of-Distribution Failure Prevention via Multi-Modal   Reasoning"></a>Real-Time Out-of-Distribution Failure Prevention via Multi-Modal   Reasoning</h2><p><strong>Authors:Milan Ganai, Rohan Sinha, Christopher Agia, Daniel Morton, Marco Pavone</strong></p>
<p>Foundation models can provide robust high-level reasoning on appropriate safety interventions in hazardous scenarios beyond a robotâ€™s training data, i.e. out-of-distribution (OOD) failures. However, due to the high inference latency of Large Vision and Language Models, current methods rely on manually defined intervention policies to enact fallbacks, thereby lacking the ability to plan generalizable, semantically safe motions. To overcome these challenges we present FORTRESS, a framework that generates and reasons about semantically safe fallback strategies in real time to prevent OOD failures. At a low frequency in nominal operations, FORTRESS uses multi-modal reasoners to identify goals and anticipate failure modes. When a runtime monitor triggers a fallback response, FORTRESS rapidly synthesizes plans to fallback goals while inferring and avoiding semantically unsafe regions in real time. By bridging open-world, multi-modal reasoning with dynamics-aware planning, we eliminate the need for hard-coded fallbacks and human safety interventions. FORTRESS outperforms on-the-fly prompting of slow reasoning models in safety classification accuracy on synthetic benchmarks and real-world ANYmal robot data, and further improves system safety and planning success in simulation and on quadrotor hardware for urban navigation. </p>
<blockquote>
<p>æ¨¡å‹åŸºç¡€èƒ½å¤Ÿåœ¨æœºå™¨äººè®­ç»ƒæ•°æ®ä»¥å¤–çš„å±é™©åœºæ™¯ä¸­ï¼Œå¯¹é€‚å½“çš„å®‰å…¨å¹²é¢„æªæ–½æä¾›ç¨³å¥çš„é«˜çº§æ¨ç†ï¼Œå³è¶…å‡ºåˆ†å¸ƒï¼ˆOODï¼‰çš„æ•…éšœã€‚ç„¶è€Œï¼Œç”±äºå¤§å‹è§†è§‰å’Œè¯­è¨€æ¨¡å‹çš„æ¨ç†å»¶è¿Ÿè¾ƒé«˜ï¼Œå½“å‰çš„æ–¹æ³•ä¾èµ–äºæ‰‹åŠ¨å®šä¹‰çš„å¹²é¢„æ”¿ç­–æ¥å®æ–½åå¤‡æªæ–½ï¼Œå› æ­¤ç¼ºä¹åˆ¶å®šå¯æ¨å¹¿çš„ã€è¯­ä¹‰å®‰å…¨çš„åŠ¨ä½œçš„èƒ½åŠ›ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å ¡å’ï¼ˆFORTRESSï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå®æ—¶ç”Ÿæˆå¹¶è€ƒè™‘è¯­ä¹‰å®‰å…¨çš„åå¤‡ç­–ç•¥ï¼Œä»¥é˜²æ­¢OODæ•…éšœã€‚åœ¨æ­£å¸¸æƒ…å†µä¸‹ï¼Œå ¡å’ä»¥è¾ƒä½é¢‘ç‡ä½¿ç”¨å¤šæ¨¡æ€æ¨ç†å™¨æ¥ç¡®å®šç›®æ ‡å¹¶é¢„æµ‹æ•…éšœæ¨¡å¼ã€‚å½“è¿è¡Œæ—¶ç›‘è§†å™¨è§¦å‘åå¤‡å“åº”æ—¶ï¼Œå ¡å’èƒ½å¤Ÿè¿…é€Ÿåˆæˆè¾¾åˆ°åå¤‡ç›®æ ‡çš„è®¡åˆ’ï¼ŒåŒæ—¶å®æ—¶æ¨æ–­å¹¶é¿å…è¯­ä¹‰ä¸å®‰å…¨åŒºåŸŸã€‚é€šè¿‡è¿æ¥å¼€æ”¾ä¸–ç•Œã€å¤šæ¨¡æ€æ¨ç†ä¸åŠ¨æ€æ„ŸçŸ¥è§„åˆ’ï¼Œæˆ‘ä»¬æ¶ˆé™¤äº†å¯¹ç¡¬ç¼–ç çš„åå¤‡æªæ–½å’Œäººå·¥å®‰å…¨å¹²é¢„çš„éœ€æ±‚ã€‚å ¡å’åœ¨åˆæˆåŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œçš„ANYmalæœºå™¨äººæ•°æ®ä¸Šçš„å®‰å…¨åˆ†ç±»ç²¾åº¦æ–¹é¢è¡¨ç°ä¼˜äºå®æ—¶æç¤ºçš„æ…¢é€Ÿæ¨ç†æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨æ¨¡æ‹Ÿç¯å¢ƒå’Œå››æ—‹ç¿¼ç¡¬ä»¶çš„åŸå¸‚å¯¼èˆªæ–¹é¢è¿›ä¸€æ­¥æé«˜äº†ç³»ç»Ÿå®‰å…¨æ€§å’Œè§„åˆ’æˆåŠŸç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10547v1">PDF</a> Website: <a target="_blank" rel="noopener" href="https://milanganai.github.io/fortress/">https://milanganai.github.io/fortress/</a></p>
<p><strong>Summary</strong>ï¼šå ¡å’æ¡†æ¶ï¼ˆFORTRESSï¼‰å¯ä»¥å®æ—¶ç”Ÿæˆå¹¶å¤„ç†è¯­ä¹‰å®‰å…¨çš„å›é€€ç­–ç•¥ï¼Œä»¥å…‹æœå¤§å‹è§†è§‰å’Œè¯­è¨€æ¨¡å‹çš„é«˜æ¨ç†å»¶è¿Ÿé—®é¢˜ï¼Œé˜²æ­¢åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ•…éšœã€‚é€šè¿‡ç»“åˆå¼€æ”¾ä¸–ç•Œå¤šæ¨¡æ€æ¨ç†ä¸åŠ¨æ€æ„ŸçŸ¥è§„åˆ’ï¼Œå ¡å’æ¡†æ¶æ¶ˆé™¤äº†å¯¹ç¡¬ç¼–ç å›é€€å’Œäººä¸ºå®‰å…¨å¹²é¢„çš„éœ€æ±‚ï¼Œæé«˜äº†ç³»ç»Ÿå®‰å…¨æ€§å’Œè§„åˆ’æˆåŠŸç‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å ¡å’æ¡†æ¶ï¼ˆFORTRESSï¼‰æ—¨åœ¨è§£å†³å¤§å‹è§†è§‰å’Œè¯­è¨€æ¨¡å‹çš„é«˜æ¨ç†å»¶è¿Ÿé—®é¢˜ã€‚</li>
<li>åœ¨æ­£å¸¸è¿è¡Œæ—¶ï¼Œå ¡å’æ¡†æ¶ä½¿ç”¨å¤šæ¨¡æ€æ¨ç†å™¨ä»¥ä½é¢‘ç‡è¯†åˆ«ç›®æ ‡å¹¶é¢„æµ‹æ•…éšœæ¨¡å¼ã€‚</li>
<li>å½“è§¦å‘å›é€€å“åº”æ—¶ï¼Œå ¡å’æ¡†æ¶èƒ½è¿…é€Ÿåˆæˆå›é€€è®¡åˆ’ï¼Œå¹¶åœ¨å®æ—¶ä¸­æ¨æ–­å’Œé¿å…è¯­ä¹‰ä¸å®‰å…¨åŒºåŸŸã€‚</li>
<li>å ¡å’æ¡†æ¶ç»“åˆäº†å¼€æ”¾ä¸–ç•Œå¤šæ¨¡æ€æ¨ç†å’ŒåŠ¨æ€æ„ŸçŸ¥è§„åˆ’ï¼Œæé«˜äº†ç³»ç»Ÿå®‰å…¨æ€§å’Œè§„åˆ’æˆåŠŸç‡ã€‚</li>
<li>è¯¥æ¡†æ¶æ¶ˆé™¤äº†å¯¹ç¡¬ç¼–ç å›é€€å’Œäººä¸ºå®‰å…¨å¹²é¢„çš„éœ€æ±‚ã€‚</li>
<li>å ¡å’æ¡†æ¶åœ¨åˆæˆåŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•ŒANYmalæœºå™¨äººæ•°æ®ä¸Šçš„å®‰å…¨åˆ†ç±»ç²¾åº¦ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10547">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e57db7e33aa1b99a6596601f94db7cea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84e87c0b0056d58bfccb9309de9f9634.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8009192fe0673b9f0aff62837792a59f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd6409774b4f221ac2819ac86f565cd1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0becbb763baf660621e97a24bb8eecc2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CheXGenBench-A-Unified-Benchmark-For-Fidelity-Privacy-and-Utility-of-Synthetic-Chest-Radiographs"><a href="#CheXGenBench-A-Unified-Benchmark-For-Fidelity-Privacy-and-Utility-of-Synthetic-Chest-Radiographs" class="headerlink" title="CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of   Synthetic Chest Radiographs"></a>CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of   Synthetic Chest Radiographs</h2><p><strong>Authors:Raman Dutt, Pedro Sanchez, Yongchen Yao, Steven McDonagh, Sotirios A. Tsaftaris, Timothy Hospedales</strong></p>
<p>We introduce CheXGenBench, a rigorous and multifaceted evaluation framework for synthetic chest radiograph generation that simultaneously assesses fidelity, privacy risks, and clinical utility across state-of-the-art text-to-image generative models. Despite rapid advancements in generative AI for real-world imagery, medical domain evaluations have been hindered by methodological inconsistencies, outdated architectural comparisons, and disconnected assessment criteria that rarely address the practical clinical value of synthetic samples. CheXGenBench overcomes these limitations through standardised data partitioning and a unified evaluation protocol comprising over 20 quantitative metrics that systematically analyse generation quality, potential privacy vulnerabilities, and downstream clinical applicability across 11 leading text-to-image architectures. Our results reveal critical inefficiencies in the existing evaluation protocols, particularly in assessing generative fidelity, leading to inconsistent and uninformative comparisons. Our framework establishes a standardised benchmark for the medical AI community, enabling objective and reproducible comparisons while facilitating seamless integration of both existing and future generative models. Additionally, we release a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K radiographs generated by the top-performing model (Sana 0.6B) in our benchmark to support further research in this critical domain. Through CheXGenBench, we establish a new state-of-the-art and release our framework, models, and SynthCheX-75K dataset at <a target="_blank" rel="noopener" href="https://raman1121.github.io/CheXGenBench/">https://raman1121.github.io/CheXGenBench/</a> </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºCheXGenBenchï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹åˆæˆèƒ¸éƒ¨Xå…‰ç‰‡ç”ŸæˆæŠ€æœ¯çš„ä¸¥æ ¼å¤šå…ƒè¯„ä¼°æ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶è¯„ä¼°æœ€å…ˆè¿›æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„ä¿çœŸåº¦ã€éšç§é£é™©ä»¥åŠä¸´åºŠå®ç”¨æ€§ã€‚å°½ç®¡ç°å®ä¸–ç•Œå›¾åƒç”Ÿæˆäººå·¥æ™ºèƒ½å–å¾—äº†å¿«é€Ÿå‘å±•ï¼Œä½†åŒ»ç–—é¢†åŸŸè¯„ä¼°ä»å—åˆ°æ–¹æ³•è®ºä¸ä¸€è‡´ã€æ¶æ„å¯¹æ¯”è¿‡æ—¶ä»¥åŠè¯„ä¼°æ ‡å‡†è„±èŠ‚ç­‰é—®é¢˜çš„é˜»ç¢ï¼Œè¿™äº›é—®é¢˜å¾ˆå°‘æ¶‰åŠåˆæˆæ ·æœ¬çš„å®é™…ä¸´åºŠä»·å€¼ã€‚CheXGenBenché€šè¿‡æ ‡å‡†åŒ–æ•°æ®åˆ†åŒºä»¥åŠåŒ…å«è¶…è¿‡20ä¸ªé‡åŒ–æŒ‡æ ‡çš„ç»Ÿä¸€è¯„ä¼°åè®®æ¥å…‹æœè¿™äº›å±€é™æ€§ï¼Œè¯¥è¯„ä¼°åè®®ç³»ç»Ÿåœ°åˆ†æäº†ç”Ÿæˆè´¨é‡ã€æ½œåœ¨çš„éšç§æ¼æ´ä»¥åŠè·¨11ç§é¢†å…ˆæ–‡æœ¬åˆ°å›¾åƒæ¶æ„çš„ä¸‹æ¸¸ä¸´åºŠé€‚ç”¨æ€§ã€‚æˆ‘ä»¬çš„ç»“æœæ­ç¤ºäº†ç°æœ‰è¯„ä¼°åè®®çš„å…³é”®ä½æ•ˆä¹‹å¤„ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯„ä¼°ç”Ÿæˆä¿çœŸåº¦æ–¹é¢ï¼Œå¯¼è‡´æ¯”è¾ƒç»“æœä¸ä¸€è‡´ä¸”ç¼ºä¹ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ºåŒ»ç–—äººå·¥æ™ºèƒ½ç¤¾åŒºå»ºç«‹äº†æ ‡å‡†åŒ–åŸºå‡†ï¼Œèƒ½å¤Ÿè¿›è¡Œå®¢è§‚å’Œå¯é‡å¤çš„æ¯”è¾ƒï¼ŒåŒæ—¶ä¿ƒè¿›ç°æœ‰å’Œæœªæ¥ç”Ÿæˆæ¨¡å‹çš„æ— ç¼é›†æˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªé«˜è´¨é‡åˆæˆæ•°æ®é›†SynthCheX-75Kï¼ŒåŒ…å«ç”±æˆ‘ä»¬åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼ˆSana 0.6Bï¼‰ç”Ÿæˆçš„75Kå¼ æ”¾å°„å›¾åƒï¼Œä»¥æ”¯æŒè¿™ä¸€å…³é”®é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚é€šè¿‡CheXGenBenchï¼Œæˆ‘ä»¬å»ºç«‹äº†æ–°çš„æŠ€æœ¯æ ‡æ†ï¼Œå¹¶å°†æˆ‘ä»¬çš„æ¡†æ¶ã€æ¨¡å‹å’ŒSynthCheX-75Kæ•°æ®é›†å‘å¸ƒåœ¨[<a target="_blank" rel="noopener" href="https://raman1121.github.io/CheXGenBench/]%E4%B8%8A%E3%80%82">https://raman1121.github.io/CheXGenBench/]ä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10496v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æˆ‘ä»¬æ¨å‡ºCheXGenBenchæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ˜¯åˆæˆèƒ¸æ”¾å°„å½±åƒç”Ÿæˆçš„è¯„ä»·ä½“ç³»ï¼Œèƒ½å¤Ÿå…¨é¢è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„ä¿çœŸåº¦ã€éšç§é£é™©ä»¥åŠä¸´åºŠä»·å€¼ã€‚å°½ç®¡ç°å®å›¾åƒç”ŸæˆAIå‘å±•è¿…é€Ÿï¼Œä½†åŒ»ç–—é¢†åŸŸè¯„ä»·ä»é¢ä¸´æ–¹æ³•ä¸ä¸€è‡´ã€æ¶æ„å¯¹æ¯”è¿‡æ—¶ä»¥åŠè¯„ä¼°æ ‡å‡†è„±ç¦»å®é™…ä¸´åºŠä»·å€¼çš„é—®é¢˜ã€‚CheXGenBenché€šè¿‡æ ‡å‡†åŒ–æ•°æ®åˆ†åŒºå’Œç»Ÿä¸€çš„è¯„ä¼°åè®®ï¼Œç³»ç»Ÿåœ°åˆ†æç”Ÿæˆè´¨é‡ã€æ½œåœ¨éšç§æ¼æ´ä»¥åŠä¸‹æ¸¸ä¸´åºŠé€‚ç”¨æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ºåŒ»ç–—AIç¤¾åŒºå»ºç«‹äº†æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•ï¼Œä½¿å®¢è§‚å’Œå¯é‡å¤çš„æ¯”è¾ƒæˆä¸ºå¯èƒ½ï¼ŒåŒæ—¶ä¿ƒè¿›ç°æœ‰å’Œæœªæ¥ç”Ÿæˆæ¨¡å‹çš„æ— ç¼é›†æˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘å¸ƒäº†ç”±é¡¶å°–æ¨¡å‹ç”Ÿæˆçš„é«˜è´¨é‡åˆæˆæ•°æ®é›†SynthCheX-75Kï¼Œä»¥æ”¯æŒè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚é€šè¿‡CheXGenBenchæ¡†æ¶ï¼Œæˆ‘ä»¬å»ºç«‹äº†æ–°çš„æ ‡å‡†å¹¶å…¬å¼€äº†æˆ‘ä»¬çš„æ¡†æ¶ã€æ¨¡å‹å’ŒSynthCheX-75Kæ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>CheXGenBenchæ˜¯ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°åˆæˆèƒ¸æ”¾å°„å½±åƒç”Ÿæˆçš„è´¨é‡ã€‚</li>
<li>è¯¥æ¡†æ¶åŒæ—¶è€ƒè™‘ä¿çœŸåº¦ã€éšç§é£é™©å’Œä¸´åºŠä»·å€¼ä¸‰ä¸ªæ–¹é¢çš„è¯„ä¼°ã€‚</li>
<li>ç°æœ‰åŒ»ç–—é¢†åŸŸå½±åƒç”Ÿæˆçš„è¯„ä»·å­˜åœ¨æ–¹æ³•ä¸ä¸€è‡´ã€æ¶æ„å¯¹æ¯”è¿‡æ—¶å’Œè¯„ä¼°æ ‡å‡†è„±ç¦»å®é™…ä¸´åºŠä»·å€¼çš„é—®é¢˜ã€‚</li>
<li>CheXGenBenché€šè¿‡æ ‡å‡†åŒ–æ•°æ®åˆ†åŒºå’Œç»Ÿä¸€çš„è¯„ä¼°åè®®æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>æ¡†æ¶åŒ…å«è¶…è¿‡20ä¸ªå®šé‡æŒ‡æ ‡ï¼Œç³»ç»Ÿåœ°åˆ†æç”Ÿæˆè´¨é‡ã€æ½œåœ¨éšç§æ¼æ´å’Œä¸‹æ¸¸ä¸´åºŠé€‚ç”¨æ€§ã€‚</li>
<li>CheXGenBenchä¸ºåŒ»ç–—AIç¤¾åŒºæä¾›äº†æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•ï¼Œä¿ƒè¿›äº†å®¢è§‚å’Œå¯é‡å¤çš„æ¯”è¾ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10496">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cc352e0b0e5ac73c31c9fb568ebbb18b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de8106921042fda00b9227b3cf3b6e7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-459768c3f70d7a1f95fef4d9f14e93c6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="UniEval-Unified-Holistic-Evaluation-for-Unified-Multimodal-Understanding-and-Generation"><a href="#UniEval-Unified-Holistic-Evaluation-for-Unified-Multimodal-Understanding-and-Generation" class="headerlink" title="UniEval: Unified Holistic Evaluation for Unified Multimodal   Understanding and Generation"></a>UniEval: Unified Holistic Evaluation for Unified Multimodal   Understanding and Generation</h2><p><strong>Authors:Yi Li, Haonan Wang, Qixiang Zhang, Boyu Xiao, Chenchang Hu, Hualiang Wang, Xiaomeng Li</strong></p>
<p>The emergence of unified multimodal understanding and generation models is rapidly attracting attention because of their ability to enhance instruction-following capabilities while minimizing model redundancy. However, there is a lack of a unified evaluation framework for these models, which would enable an elegant, simplified, and overall evaluation. Current models conduct evaluations on multiple task-specific benchmarks, but there are significant limitations, such as the lack of overall results, errors from extra evaluation models, reliance on extensive labeled images, benchmarks that lack diversity, and metrics with limited capacity for instruction-following evaluation. To tackle these challenges, we introduce UniEval, the first evaluation framework designed for unified multimodal models without extra models, images, or annotations. This facilitates a simplified and unified evaluation process. The UniEval framework contains a holistic benchmark, UniBench (supports both unified and visual generation models), along with the corresponding UniScore metric. UniBench includes 81 fine-grained tags contributing to high diversity. Experimental results indicate that UniBench is more challenging than existing benchmarks, and UniScore aligns closely with human evaluations, surpassing current metrics. Moreover, we extensively evaluated SoTA unified and visual generation models, uncovering new insights into Univeralâ€™s unique values. </p>
<blockquote>
<p>ç»Ÿä¸€å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ¨¡å‹çš„å…´èµ·è¿…é€Ÿå¼•èµ·äº†äººä»¬çš„å…³æ³¨ï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿå¢å¼ºæŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›ï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°å‡å°‘æ¨¡å‹å†—ä½™ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¯¹è¿™äº›æ¨¡å‹çš„ç»Ÿä¸€è¯„ä¼°æ¡†æ¶ï¼Œæ— æ³•è¿›è¡Œä¼˜é›…ã€ç®€æ´å’Œå…¨é¢çš„è¯„ä¼°ã€‚å½“å‰æ¨¡å‹åœ¨å¤šä¸ªç‰¹å®šä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œä½†å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œå¦‚ç¼ºä¹æ€»ä½“ç»“æœã€é¢å¤–è¯„ä¼°æ¨¡å‹çš„é”™è¯¯ã€ä¾èµ–å¤§é‡æœ‰æ ‡ç­¾å›¾åƒã€åŸºå‡†æµ‹è¯•ç¼ºä¹å¤šæ ·æ€§ä»¥åŠæŒ‡æ ‡åœ¨æŒ‡ä»¤è¯„ä¼°æ–¹é¢çš„èƒ½åŠ›æœ‰é™ç­‰ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†UniEvalï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹è®¾è®¡çš„ç¬¬ä¸€ä¸ªæ— éœ€é¢å¤–æ¨¡å‹ã€å›¾åƒæˆ–æ³¨é‡Šçš„è¯„ä¼°æ¡†æ¶ã€‚è¿™æœ‰åŠ©äºç®€åŒ–å¹¶ç»Ÿä¸€è¯„ä¼°æµç¨‹ã€‚UniEvalæ¡†æ¶åŒ…å«æ•´ä½“åŸºå‡†æµ‹è¯•UniBenchï¼ˆæ”¯æŒç»Ÿä¸€å’Œè§†è§‰ç”Ÿæˆæ¨¡å‹ï¼‰ï¼Œä»¥åŠç›¸åº”çš„UniScoreæŒ‡æ ‡ã€‚UniBenchåŒ…æ‹¬81ä¸ªç²¾ç»†æ ‡ç­¾ï¼Œæœ‰åŠ©äºå®ç°é«˜å¤šæ ·æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniBenchæ¯”ç°æœ‰åŸºå‡†æ›´å…·æŒ‘æˆ˜æ€§ï¼ŒUniScoreä¸äººç±»è¯„ä¼°ç´§å¯†å¯¹é½ï¼Œè¶…è¶Šäº†å½“å‰æŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹æœ€æ–°çš„ç»Ÿä¸€å’Œè§†è§‰ç”Ÿæˆæ¨¡å‹è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œæ­ç¤ºäº†Universalçš„ç‹¬ç‰¹ä»·å€¼çš„æ–°è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10483v1">PDF</a> UniEval is the first evaluation framework designed for unified   multimodal models, including a holistic benchmark UniBench and the UniScore   metric</p>
<p><strong>Summary</strong><br>å¤šæ¨¡æ€ç»Ÿä¸€ç†è§£ä¸ç”Ÿæˆæ¨¡å‹æ­£åœ¨å¿«é€Ÿå¸å¼•å…³æ³¨ï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿåœ¨å‡å°‘æ¨¡å‹å†—ä½™çš„åŒæ—¶æé«˜æŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›ã€‚ç„¶è€Œï¼Œç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶æ¥ä¼˜é›…ã€ç®€åŒ–åœ°å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚ç›®å‰ï¼Œæ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ç‰¹å®šåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œä½†å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œå¦‚ç¼ºä¹æ€»ä½“ç»“æœã€é¢å¤–è¯„ä¼°æ¨¡å‹çš„è¯¯å·®ã€ä¾èµ–å¤§é‡æ ‡æ³¨å›¾åƒã€åŸºå‡†æµ‹è¯•ç¼ºä¹å¤šæ ·æ€§ä»¥åŠæŒ‡ä»¤æ‰§è¡Œè¯„ä¼°èƒ½åŠ›æœ‰é™çš„æŒ‡æ ‡ç­‰ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†UniEvalè¯„ä¼°æ¡†æ¶ï¼Œä¸“ä¸ºå¤šæ¨¡æ€ç»Ÿä¸€æ¨¡å‹è®¾è®¡ï¼Œæ— éœ€é¢å¤–çš„æ¨¡å‹ã€å›¾åƒæˆ–æ³¨é‡Šã€‚è¿™ä¿ƒè¿›äº†ç®€åŒ–ç»Ÿä¸€çš„è¯„ä¼°è¿‡ç¨‹ã€‚UniEvalæ¡†æ¶åŒ…å«æ•´ä½“åŸºå‡†æµ‹è¯•UniBenchï¼ˆæ”¯æŒç»Ÿä¸€å’Œè§†è§‰ç”Ÿæˆæ¨¡å‹ï¼‰ä»¥åŠç›¸åº”çš„UniScoreæŒ‡æ ‡ã€‚UniBenchåŒ…æ‹¬81ä¸ªç²¾ç»†æ ‡ç­¾ï¼Œä»¥å®ç°é«˜å¤šæ ·æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniBenchæ¯”ç°æœ‰åŸºå‡†æµ‹è¯•æ›´å…·æŒ‘æˆ˜æ€§ï¼ŒUniScoreä¸äººç±»è¯„ä¼°ç´§å¯†å¯¹é½ï¼Œè¶…è¶Šäº†å½“å‰æŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹æœ€æ–°ç»Ÿä¸€å’Œè§†è§‰ç”Ÿæˆæ¨¡å‹è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå‘ç°äº†Universalçš„ç‹¬ç‰¹ä»·å€¼çš„æ–°è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€ç»Ÿä¸€ç†è§£ä¸ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿæå‡æŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›å¹¶å‡å°‘æ¨¡å‹å†—ä½™ï¼Œæ­£å—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>å½“å‰ç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶æ¥å…¨é¢ã€ç®€æ´åœ°è¯„ä»·è¿™äº›æ¨¡å‹ã€‚</li>
<li>ç°æœ‰æ¨¡å‹è¯„ä¼°å­˜åœ¨å±€é™æ€§ï¼Œå¦‚ç¼ºä¹æ€»ä½“ç»“æœã€é¢å¤–è¯„ä¼°æ¨¡å‹çš„è¯¯å·®ç­‰ã€‚</li>
<li>UniEvalè¯„ä¼°æ¡†æ¶æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œé€‚ç”¨äºå¤šæ¨¡æ€ç»Ÿä¸€æ¨¡å‹ï¼Œæ— éœ€é¢å¤–çš„æ¨¡å‹ã€å›¾åƒæˆ–æ³¨é‡Šã€‚</li>
<li>UniEvalåŒ…å«æ•´ä½“åŸºå‡†æµ‹è¯•UniBenchå’ŒUniScoreæŒ‡æ ‡ï¼Œå…¶ä¸­UniBenchå…·æœ‰é«˜å¤šæ ·æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒUniBenchæ›´å…·æŒ‘æˆ˜æ€§ï¼ŒUniScoreæŒ‡æ ‡ä¸äººç±»è¯„ä¼°ç´§å¯†å¯¹é½ï¼Œä¼˜äºç°æœ‰æŒ‡æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10483">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5c945a69289b6243a6807990337e53c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c146ce73c0e944d6be84980ef2c7a1b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e39feb82c8d3d1621f86c2ab74058fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8675ef5354fd1fc9157726751b8d80d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e7db93ad408e0c24b0fd7d55093ba62.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Evaluating-Model-Explanations-without-Ground-Truth"><a href="#Evaluating-Model-Explanations-without-Ground-Truth" class="headerlink" title="Evaluating Model Explanations without Ground Truth"></a>Evaluating Model Explanations without Ground Truth</h2><p><strong>Authors:Kaivalya Rawal, Zihao Fu, Eoin Delaney, Chris Russell</strong></p>
<p>There can be many competing and contradictory explanations for a single model prediction, making it difficult to select which one to use. Current explanation evaluation frameworks measure quality by comparing against ideal â€œground-truthâ€ explanations, or by verifying model sensitivity to important inputs. We outline the limitations of these approaches, and propose three desirable principles to ground the future development of explanation evaluation strategies for local feature importance explanations. We propose a ground-truth Agnostic eXplanation Evaluation framework (AXE) for evaluating and comparing model explanations that satisfies these principles. Unlike prior approaches, AXE does not require access to ideal ground-truth explanations for comparison, or rely on model sensitivity - providing an independent measure of explanation quality. We verify AXE by comparing with baselines, and show how it can be used to detect explanation fairwashing. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth">https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth</a>. </p>
<blockquote>
<p>å¯¹äºä¸€ä¸ªæ¨¡å‹é¢„æµ‹ï¼Œå¯èƒ½å­˜åœ¨è®¸å¤šç›¸äº’ç«äº‰å’Œç›¸äº’çŸ›ç›¾çš„è§£é‡Šï¼Œè¿™ä½¿å¾—å¾ˆéš¾é€‰æ‹©ä½¿ç”¨å“ªä¸€ä¸ªã€‚å½“å‰çš„è§£é‡Šè¯„ä¼°æ¡†æ¶é€šè¿‡å°†å…¶ä¸ç†æƒ³çš„â€œçœŸå®â€è§£é‡Šè¿›è¡Œæ¯”è¾ƒï¼Œæˆ–è€…é€šè¿‡éªŒè¯æ¨¡å‹å¯¹é‡è¦è¾“å…¥çš„æ•æ„Ÿæ€§æ¥è¡¡é‡è´¨é‡ã€‚æˆ‘ä»¬æ¦‚è¿°äº†è¿™äº›æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸ºå±€éƒ¨ç‰¹å¾é‡è¦æ€§è§£é‡Šçš„æœªæ¥å‘å±•å¥ å®šåŸºç¡€çš„ä¸‰ä¸ªç†æƒ³åŸåˆ™ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºè¯„ä¼°å’Œæ¯”è¾ƒæ¨¡å‹è§£é‡Šçš„çœŸç›¸æ— å…³è§£é‡Šè¯„ä¼°æ¡†æ¶ï¼ˆAXEï¼‰ï¼Œè¯¥æ¡†æ¶å¯ä»¥æ»¡è¶³è¿™äº›åŸåˆ™ã€‚ä¸ä»¥å‰çš„æ–¹æ³•ä¸åŒï¼ŒAXEä¸éœ€è¦é€šè¿‡ç†æƒ³çœŸå®è§£é‡Šè¿›è¡Œæ¯”è¾ƒï¼Œä¹Ÿä¸ä¾èµ–äºæ¨¡å‹çš„æ•æ„Ÿæ€§ï¼Œå®ƒä¸ºè§£é‡Šè´¨é‡æä¾›äº†ä¸€ä¸ªç‹¬ç«‹çš„è¡¡é‡æ ‡å‡†ã€‚æˆ‘ä»¬é€šè¿‡å°†å…¶ä¸åŸºçº¿è¿›è¡Œæ¯”è¾ƒæ¥éªŒè¯AXEï¼Œå¹¶å±•ç¤ºå…¶å¦‚ä½•ç”¨äºæ£€æµ‹è§£é‡Šæ´—ç™½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truthä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10399v1">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth">https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth</a></p>
<p><strong>Summary</strong><br>æ¨¡å‹é¢„æµ‹å­˜åœ¨å¤šç§ç›¸äº’ç«äº‰å’ŒçŸ›ç›¾çš„è§£è¯»ï¼Œé€‰æ‹©åˆé€‚çš„è§£è¯»æ–¹å¼ååˆ†å›°éš¾ã€‚å½“å‰è§£è¯»è¯„ä¼°æ¡†æ¶ä¸»è¦é€šè¿‡ä¸ç†æƒ³â€œåœ°é¢çœŸå®â€è§£è¯»è¿›è¡Œæ¯”è¾ƒï¼Œæˆ–éªŒè¯æ¨¡å‹å¯¹é‡è¦è¾“å…¥çš„æ•æ„Ÿæ€§æ¥è¡¡é‡è´¨é‡ã€‚æœ¬æ–‡åˆ†æäº†è¿™äº›æ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡ºäº†åœ°æ–¹ç‰¹å¾é‡è¦æ€§è§£è¯»çš„è§£è¯»è¯„ä¼°ç­–ç•¥å‘å±•çš„ä¸‰ä¸ªåŸåˆ™ã€‚æå‡ºäº†ä¸€ä¸ªä¸éœ€è¦ç†æƒ³åœ°é¢çœŸå®è§£è¯»å¯¹æ¯”çš„è§£ç®—è¯„ä»·æ¡†æ¶ï¼ˆAXEï¼‰ï¼Œæ»¡è¶³è¿™äº›åŸåˆ™è¦æ±‚ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–æ¨¡å‹æ•æ„Ÿæ€§çš„æ–¹æ³•ä¸åŒï¼ŒAXEæä¾›äº†ä¸€ç§ç‹¬ç«‹çš„è§£è¯»è´¨é‡åº¦é‡æ ‡å‡†ã€‚é€šè¿‡å¯¹æ¯”åŸºçº¿éªŒè¯äº†AXEçš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶æ£€æµ‹è§£é‡Šæ¬ºéª—çš„æ½œåŠ›ã€‚ä»£ç å·²å…¬å¼€åœ¨é“¾æ¥ä¸­ï¼š[é“¾æ¥åœ°å€]ï¼ˆæ³¨ï¼šå®é™…ä½¿ç”¨æ—¶åº”æ›¿æ¢ä¸ºçœŸæ­£çš„GitHubé“¾æ¥ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ¨¡å‹é¢„æµ‹å­˜åœ¨å¤šç§è§£è¯»ï¼Œé€‰æ‹©åˆé€‚çš„è§£è¯»æ–¹å¼è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰è§£è¯»è¯„ä¼°æ¡†æ¶å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•å…¨é¢è¯„ä¼°æ‰€æœ‰è§£è¯»æ–¹å¼çš„ä¼˜åŠ£ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åœ°æ–¹ç‰¹å¾é‡è¦æ€§è§£è¯»è¯„ä¼°çš„ä¸‰ä¸ªåŸåˆ™ï¼Œä¸ºæœªæ¥çš„è§£è¯»è¯„ä¼°ç­–ç•¥å‘å±•æä¾›äº†æŒ‡å¯¼ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ— éœ€ç†æƒ³åœ°é¢çœŸå®è§£è¯»å¯¹æ¯”çš„è§£ç®—è¯„ä»·æ¡†æ¶ï¼ˆAXEï¼‰ï¼Œå…·æœ‰ç‹¬ç«‹è¯„ä¼°è§£è¯»è´¨é‡çš„æ½œåŠ›ã€‚</li>
<li>AXEç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ä¸å†ä¾èµ–äºæ¨¡å‹æ•æ„Ÿæ€§åˆ†æï¼Œæ›´èƒ½å‡†ç¡®è¯„ä¼°è§£è¯»è´¨é‡ã€‚</li>
<li>é€šè¿‡å¯¹æ¯”åŸºçº¿éªŒè¯äº†AXEçš„æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9eb52581375667ed29b2f314aac16ddb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82c1088383610cebbe83eade1986e162.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3945e7f81a4882df92e6c6f784a106a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-870da104c3fd324898ba6df330db1978.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f63ed3dae97362793dfc6b6af792d7c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8f9e3447d114e6d2309b0aa895b2c2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-baab6d849c82e2aa6ad7a9a0b5c9fb6e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="J1-Incentivizing-Thinking-in-LLM-as-a-Judge-via-Reinforcement-Learning"><a href="#J1-Incentivizing-Thinking-in-LLM-as-a-Judge-via-Reinforcement-Learning" class="headerlink" title="J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning"></a>J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning</h2><p><strong>Authors:Chenxi Whitehouse, Tianlu Wang, Ping Yu, Xian Li, Jason Weston, Ilia Kulikov, Swarnadeep Saha</strong></p>
<p>The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best recipes for training such models to think. In this work we introduce J1, a reinforcement learning approach to training such models. Our method converts both verifiable and non-verifiable prompts to judgment tasks with verifiable rewards that incentivize thinking and mitigate judgment bias. In particular, our approach outperforms all other existing 8B or 70B models when trained at those sizes, including models distilled from DeepSeek-R1. J1 also outperforms o1-mini, and even R1 on some benchmarks, despite training a smaller model. We provide analysis and ablations comparing Pairwise-J1 vs Pointwise-J1 models, offline vs online training recipes, reward strategies, seed prompts, and variations in thought length and content. We find that our models make better judgments by learning to outline evaluation criteria, comparing against self-generated reference answers, and re-evaluating the correctness of model responses. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½çš„å‘å±•å—é™äºè¯„ä¼°è´¨é‡ï¼Œè€Œå¼ºå¤§çš„LLM-as-a-Judgeæ¨¡å‹å·²æˆä¸ºæ ¸å¿ƒè§£å†³æ–¹æ¡ˆã€‚é€šè¿‡æ›´å¼ºçš„æ€ç»´é“¾æ¨ç†èƒ½åŠ›ï¼Œæå‡äº†åˆ¤æ–­èƒ½åŠ›ï¼Œå› æ­¤éœ€è¦æ‰¾åˆ°è®­ç»ƒæ­¤ç±»æ¨¡å‹è¿›è¡Œæ€ç»´çš„æœ€ä½³æ–¹æ¡ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†J1ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè®­ç»ƒæ­¤ç±»æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†å¯éªŒè¯å’Œä¸å¯éªŒè¯çš„æç¤ºè½¬åŒ–ä¸ºå…·æœ‰å¯éªŒè¯å¥–åŠ±çš„åˆ¤æ–­ä»»åŠ¡ï¼Œä»¥æ¿€åŠ±æ€è€ƒå’Œå‡è½»åˆ¤æ–­åè§ã€‚å°¤å…¶å½“æˆ‘ä»¬åœ¨è¿™ç±»å¤§å°è§„æ¨¡ä¸‹è®­ç»ƒæ¨¡å‹æ—¶ï¼Œä¸å…¶ä»–æ‰€æœ‰ç°æœ‰çš„8Bæˆ–70Bæ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°æ›´ä½³ï¼ŒåŒ…æ‹¬ä»DeepSeek-R1ä¸­æç‚¼å‡ºæ¥çš„æ¨¡å‹ã€‚å°½ç®¡è®­ç»ƒæ¨¡å‹è¾ƒå°ï¼ŒJ1åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¹Ÿä¼˜äºo1-miniç”šè‡³R1ã€‚æˆ‘ä»¬æä¾›äº†å¯¹æ¯”åˆ†æPairwise-J1ä¸Pointwise-J1æ¨¡å‹ã€ç¦»çº¿ä¸åœ¨çº¿è®­ç»ƒæ–¹æ¡ˆã€å¥–åŠ±ç­–ç•¥ã€ç§å­æç¤ºä»¥åŠæ€ç»´é•¿åº¦å’Œå†…å®¹å˜åŒ–çš„åˆ†æå’Œæ¶ˆèç ”ç©¶ã€‚æˆ‘ä»¬å‘ç°ï¼Œé€šè¿‡å­¦ä¹ æ¦‚è¿°è¯„ä»·æ ‡å‡†ã€ä¸è‡ªæˆ‘ç”Ÿæˆçš„å‚è€ƒç­”æ¡ˆè¿›è¡Œå¯¹æ¯”ä»¥åŠé‡æ–°è¯„ä¼°æ¨¡å‹å“åº”çš„æ­£ç¡®æ€§ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿåšå‡ºæ›´å¥½çš„åˆ¤æ–­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10320v1">PDF</a> 10 pages, 8 tables, 11 figures</p>
<p><strong>Summary</strong><br>AIè¯„ä¼°è´¨é‡æˆä¸ºå…¶å‘å±•ç“¶é¢ˆï¼ŒLLM-as-a-Judgeæ¨¡å‹ä¸ºæ ¸å¿ƒè§£å†³æ–¹æ¡ˆã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å‹ä»¥æå‡åˆ¤æ–­èƒ½åŠ›å’Œé“¾å¼æ€ç»´æ¨ç†ã€‚æ–°æ–¹æ³•å°†å¯éªŒè¯å’Œä¸å¯éªŒè¯çš„æç¤ºè½¬åŒ–ä¸ºå…·æœ‰å¯éªŒè¯å¥–åŠ±çš„åˆ¤æ–­ä»»åŠ¡ï¼Œæ¿€åŠ±æ€è€ƒå¹¶å‡è½»åˆ¤æ–­åè§ã€‚ç›¸è¾ƒäºå…¶ä»–8Bæˆ–70Bæ¨¡å‹ï¼ŒJ1æ–¹æ³•è¡¨ç°æ›´ä½³ï¼Œç”šè‡³åœ¨éƒ¨åˆ†åŸºå‡†æµ‹è¯•ä¸­è¶…è¶ŠR1ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIå‘å±•çš„ç“¶é¢ˆåœ¨äºè¯„ä¼°è´¨é‡ï¼ŒLLM-as-a-Judgeæ¨¡å‹æ˜¯è§£å†³æ­¤é—®é¢˜çš„æ ¸å¿ƒã€‚</li>
<li>å¼•å…¥J1æ–¹æ³•ï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å‹ä»¥æå‡åˆ¤æ–­èƒ½åŠ›ã€‚</li>
<li>J1å°†å„ç±»æç¤ºè½¬åŒ–ä¸ºåˆ¤æ–­ä»»åŠ¡ï¼Œä½¿ç”¨å¯éªŒè¯å¥–åŠ±æ¥æ¿€åŠ±æ¨¡å‹æ€è€ƒå¹¶å‡å°‘åˆ¤æ–­åè§ã€‚</li>
<li>J1åœ¨è®­ç»ƒè§„æ¨¡ä¸Šä¼˜äºå…¶ä»–8Bæˆ–70Bæ¨¡å‹ï¼ŒåŒ…æ‹¬åŸºäºDeepSeek-R1çš„è’¸é¦æ¨¡å‹ã€‚</li>
<li>J1åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºo1-miniç”šè‡³R1ï¼Œå°½ç®¡å…¶æ¨¡å‹è§„æ¨¡è¾ƒå°ã€‚</li>
<li>é€šè¿‡åˆ†ææ¯”è¾ƒPairwise-J1ä¸Pointwise-J1æ¨¡å‹ã€ç¦»çº¿ä¸åœ¨çº¿è®­ç»ƒæ–¹æ¡ˆã€å¥–åŠ±ç­–ç•¥ã€ç§å­æç¤ºä»¥åŠæ€ç»´é•¿åº¦å’Œå†…å®¹çš„å˜åŒ–ï¼Œå‘ç°J1æ¨¡å‹é€šè¿‡å­¦ä¹ å’Œæ¯”è¾ƒè¯„ä¼°æ ‡å‡†ã€è‡ªæˆ‘ç”Ÿæˆçš„å‚è€ƒç­”æ¡ˆä»¥åŠé‡æ–°è¯„ä¼°æ¨¡å‹å“åº”çš„æ­£ç¡®æ€§ï¼Œåšå‡ºæ›´å¥½çš„åˆ¤æ–­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10320">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-79aa1eff5107d2cfc50ccaf6d090f3cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16e6fc47e3ad9707e1bbadb2b8b1ad49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99f652f7b1b7af0f6e959a6e20603374.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3eaec77e9f64c078bd95800f575dcc03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc8e90a7b5642852b3a20359fa4fbaf0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Private-Transformer-Inference-in-MLaaS-A-Survey"><a href="#Private-Transformer-Inference-in-MLaaS-A-Survey" class="headerlink" title="Private Transformer Inference in MLaaS: A Survey"></a>Private Transformer Inference in MLaaS: A Survey</h2><p><strong>Authors:Yang Li, Xinyu Zhou, Yitong Wang, Liangxin Qian, Jun Zhao</strong></p>
<p>Transformer models have revolutionized AI, powering applications like content generation and sentiment analysis. However, their deployment in Machine Learning as a Service (MLaaS) raises significant privacy concerns, primarily due to the centralized processing of sensitive user data. Private Transformer Inference (PTI) offers a solution by utilizing cryptographic techniques such as secure multi-party computation and homomorphic encryption, enabling inference while preserving both user data and model privacy. This paper reviews recent PTI advancements, highlighting state-of-the-art solutions and challenges. We also introduce a structured taxonomy and evaluation framework for PTI, focusing on balancing resource efficiency with privacy and bridging the gap between high-performance inference and data privacy. </p>
<blockquote>
<p>Transformeræ¨¡å‹å·²ç»å½»åº•æ”¹å˜äº†äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œæ”¯æŒå†…å®¹ç”Ÿæˆå’Œæƒ…ç»ªåˆ†æç­‰åº”ç”¨ã€‚ç„¶è€Œï¼Œå°†å…¶ä½œä¸ºæœºå™¨å­¦ä¹ æœåŠ¡ï¼ˆMLaaSï¼‰è¿›è¡Œéƒ¨ç½²ä¼šå¼•å‘é‡å¤§éšç§æ‹…å¿§ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºæ•æ„Ÿç”¨æˆ·æ•°æ®çš„é›†ä¸­å¤„ç†ã€‚ç§æœ‰Transformeræ¨ç†ï¼ˆPTIï¼‰é€šè¿‡åˆ©ç”¨åŠ å¯†æŠ€æœ¯ï¼ˆå¦‚å®‰å…¨å¤šæ–¹è®¡ç®—å’ŒåŒæ€åŠ å¯†ï¼‰æä¾›äº†ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿåœ¨ä¿æŠ¤ç”¨æˆ·æ•°æ®å’Œæ¨¡å‹éšç§çš„åŒæ—¶è¿›è¡Œæ¨ç†ã€‚æœ¬æ–‡å›é¡¾äº†PTIçš„æœ€æ–°è¿›å±•ï¼Œé‡ç‚¹ä»‹ç»äº†æœ€å…ˆè¿›è§£å†³æ–¹æ¡ˆå’ŒæŒ‘æˆ˜ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†PTIçš„ç»“æ„åŒ–åˆ†ç±»æ³•å’Œè¯„ä¼°æ¡†æ¶ï¼Œä¾§é‡äºåœ¨èµ„æºå’Œéšç§ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œå¹¶ç¼©å°é«˜æ€§èƒ½æ¨ç†å’Œæ•°æ®éšç§ä¹‹é—´çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10315v1">PDF</a> </p>
<p><strong>Summary</strong><br>     äººå·¥æ™ºèƒ½é¢†åŸŸçš„Transformeræ¨¡å‹å·²è¢«å¹¿æ³›åº”ç”¨åœ¨å†…å®¹ç”Ÿæˆã€æƒ…æ„Ÿåˆ†æç­‰åº”ç”¨ä¸­ï¼Œä½†å…¶ä½œä¸ºæœºå™¨å­¦ä¹ æœåŠ¡ï¼ˆMLaaSï¼‰éƒ¨ç½²å¼•å‘äº†é‡å¤§éšç§æ‹…å¿§ã€‚ä¸»è¦å› ä¸ºç”¨æˆ·æ•°æ®çš„é›†ä¸­å¤„ç†å®¹æ˜“æ³„éœ²æ•æ„Ÿä¿¡æ¯ã€‚ç§äººTransformeræ¨ç†ï¼ˆPTIï¼‰åˆ©ç”¨å¤šæ–¹å®‰å…¨è®¡ç®—å’ŒåŒæ€åŠ å¯†ç­‰åŠ å¯†æŠ€æœ¯ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¿æŠ¤ç”¨æˆ·æ•°æ®å’Œæ¨¡å‹éšç§ã€‚æœ¬æ–‡å›é¡¾äº†æœ€æ–°çš„PTIè¿›å±•ï¼Œä»‹ç»äº†å…ˆè¿›çš„è§£å†³æ–¹æ¡ˆå’ŒæŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç»“æ„åŒ–çš„åˆ†ç±»å’Œè¯„ä¼°æ¡†æ¶ï¼Œä»¥å¹³è¡¡èµ„æºæ•ˆç‡å’Œéšç§ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ç¼©å°é«˜æ€§èƒ½æ¨ç†å’Œæ•°æ®éšç§ä¹‹é—´çš„å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Transformeræ¨¡å‹å¹¿æ³›åº”ç”¨äºAIé¢†åŸŸçš„å†…å®¹ç”Ÿæˆã€æƒ…æ„Ÿåˆ†æç­‰åº”ç”¨ã€‚</li>
<li>åœ¨MLaaSéƒ¨ç½²ä¸­å¼•å‘é‡å¤§éšç§æ‹…å¿§ï¼Œç‰¹åˆ«æ˜¯ç”¨æˆ·æ•°æ®çš„é›†ä¸­å¤„ç†é—®é¢˜ã€‚</li>
<li>PTIåˆ©ç”¨åŠ å¯†æŠ€æœ¯ï¼ˆå¦‚å¤šæ–¹å®‰å…¨è®¡ç®—å’ŒåŒæ€åŠ å¯†ï¼‰æ¥è§£å†³è¿™ä¸€éšç§é—®é¢˜ã€‚</li>
<li>é€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¿æŠ¤ç”¨æˆ·æ•°æ®å’Œæ¨¡å‹éšç§ï¼ŒPTIæä¾›äº†ä¸€ç§è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æœ¬æ–‡å›é¡¾äº†æœ€æ–°çš„PTIè¿›å±•å’Œå…ˆè¿›çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶æŒ‡å‡ºäº†å­˜åœ¨çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªç»“æ„åŒ–çš„åˆ†ç±»å’Œè¯„ä¼°æ¡†æ¶ï¼Œä»¥å¹³è¡¡èµ„æºæ•ˆç‡å’Œéšç§ä¹‹é—´çš„å…³ç³»ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10315">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cc275bc5c25bfc7174235a9a9e250dec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21ac00aafc673010e342cbac084a0dd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7db5f9387ddb90f0b26957581bc8bad.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="RAIDEN-R1-Improving-Role-awareness-of-LLMs-via-GRPO-with-Verifiable-Reward"><a href="#RAIDEN-R1-Improving-Role-awareness-of-LLMs-via-GRPO-with-Verifiable-Reward" class="headerlink" title="RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable   Reward"></a>RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable   Reward</h2><p><strong>Authors:Zongsheng Wang, Kaili Sun, Bowen Wu, Qun Yu, Ying Li, Baoxun Wang</strong></p>
<p>Role-playing conversational agents (RPCAs) face persistent challenges in maintaining role consistency. To address this, we propose RAIDEN-R1, a novel reinforcement learning framework that integrates Verifiable Role-Awareness Reward (VRAR). The method introduces both singular and multi-term mining strategies to generate quantifiable rewards by assessing role-specific keys. Additionally, we construct a high-quality, role-aware Chain-of-Thought dataset through multi-LLM collaboration, and implement experiments to enhance reasoning coherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1â€™s superiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on Script-Based Knowledge and Conversation Memory metrics, respectively, outperforming baseline models while maintaining robustness. Case analyses further reveal the modelâ€™s enhanced ability to resolve conflicting contextual cues and sustain first-person narrative consistency. This work bridges the non-quantifiability gap in RPCA training and provides insights into role-aware reasoning patterns, advancing the development of RPCAs. </p>
<blockquote>
<p>è§’è‰²æ‰®æ¼”å¯¹è¯ä»£ç†ï¼ˆRPCAsï¼‰åœ¨ä¿æŒè§’è‰²ä¸€è‡´æ€§æ–¹é¢é¢ä¸´æŒç»­æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RAIDEN-R1ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèåˆäº†å¯éªŒè¯çš„è§’è‰²æ„è¯†å¥–åŠ±ï¼ˆVRARï¼‰ã€‚è¯¥æ–¹æ³•å¼•å…¥å•æœ¯è¯­å’Œå¤šæœ¯è¯­æŒ–æ˜ç­–ç•¥ï¼Œé€šè¿‡è¯„ä¼°è§’è‰²ç‰¹å®šé”®æ¥ç”Ÿæˆå¯é‡åŒ–çš„å¥–åŠ±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¤šLLMåä½œæ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„ã€æœ‰è§’è‰²æ„è¯†çš„æ€ç»´é“¾æ•°æ®é›†ï¼Œå¹¶è¿›è¡Œå®éªŒä»¥å¢å¼ºæ¨ç†è¿è´¯æ€§ã€‚åœ¨RAIDENåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº†RAIDEN-R1çš„ä¼˜è¶Šæ€§ï¼šæˆ‘ä»¬çš„14B-GRPOæ¨¡å‹åœ¨åŸºäºè„šæœ¬çš„çŸ¥è¯†å’Œå¯¹è¯è®°å¿†æŒ‡æ ‡ä¸Šåˆ†åˆ«è¾¾åˆ°äº†88.04%å’Œ88.65%çš„å‡†ç¡®ç‡ï¼Œåœ¨ä¿æŒç¨³å¥æ€§çš„åŒæ—¶è¶…è¶Šäº†åŸºçº¿æ¨¡å‹ã€‚æ¡ˆä¾‹åˆ†æè¿›ä¸€æ­¥æ­ç¤ºäº†è¯¥æ¨¡å‹åœ¨è§£å†³å†²çªä¸Šä¸‹æ–‡çº¿ç´¢å’Œç»´æŒç¬¬ä¸€äººç§°å™äº‹ä¸€è‡´æ€§æ–¹é¢çš„å¢å¼ºèƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œå¼¥è¡¥äº†RPCAè®­ç»ƒä¸­çš„ä¸å¯é‡åŒ–æ€§å·®è·ï¼Œä¸ºè§’è‰²æ„ŸçŸ¥æ¨ç†æ¨¡å¼æä¾›äº†è§è§£ï¼Œæ¨åŠ¨äº†RPCAsçš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10218v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>RAIDEN-R1æ¡†æ¶é‡‡ç”¨å¯éªŒè¯çš„è§’è‰²æ„è¯†å¥–åŠ±ï¼ˆVRARï¼‰æ¥è§£å†³è§’è‰²æ‰®æ¼”å¯¹è¯ä»£ç†ï¼ˆRPCAï¼‰åœ¨ç»´æŒè§’è‰²ä¸€è‡´æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å•ä¸€å’Œå¤šæœŸæŒ–æ˜ç­–ç•¥ç”Ÿæˆå¯é‡åŒ–çš„å¥–åŠ±ï¼Œè¯„ä¼°è§’è‰²ç‰¹å®šçš„å…³é”®è¦ç´ ã€‚åŒæ—¶ï¼Œé€šè¿‡å¤šLLMåä½œæ„å»ºé«˜è´¨é‡çš„è§’è‰²æ„ŸçŸ¥æ€ç»´é“¾æ•°æ®é›†ï¼Œå¹¶é€šè¿‡å®éªŒæé«˜æ¨ç†è¿è´¯æ€§ã€‚åœ¨RAIDENåŸºå‡†æµ‹è¯•ä¸Šï¼ŒRAIDEN-R1è¡¨ç°å“è¶Šï¼Œ14B-GRPOæ¨¡å‹åœ¨è„šæœ¬çŸ¥è¯†æŒ‡æ ‡å’Œå¯¹è¯è®°å¿†æŒ‡æ ‡ä¸Šåˆ†åˆ«è¾¾åˆ°äº†88.04%å’Œ88.65%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†åŸºçº¿æ¨¡å‹ï¼Œå¹¶ä¿æŒäº†ç¨³å¥æ€§ã€‚æ¡ˆä¾‹åˆ†ææ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨è§£å†³å†²çªä¸Šä¸‹æ–‡çº¿ç´¢å’Œç»´æŒç¬¬ä¸€äººç§°å™äº‹ä¸€è‡´æ€§æ–¹é¢æœ‰æ‰€å¢å¼ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAIDEN-R1æ¡†æ¶é€šè¿‡å¼•å…¥Verifiable Role-Awareness Rewardï¼ˆVRARï¼‰è§£å†³RPCAè§’è‰²ä¸€è‡´æ€§ç»´æŒçš„æŒ‘æˆ˜ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨å•ä¸€å’Œå¤šæœŸæŒ–æ˜ç­–ç•¥ç”Ÿæˆé‡åŒ–å¥–åŠ±ï¼Œä»¥è¯„ä¼°è§’è‰²ç‰¹å®šçš„å…³é”®è¦ç´ ã€‚</li>
<li>é€šè¿‡å¤šLLMåä½œæ„å»ºé«˜è´¨é‡çš„è§’è‰²æ„ŸçŸ¥æ€ç»´é“¾æ•°æ®é›†ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒRAIDEN-R1åœ¨ç»´æŒè§’è‰²ä¸€è‡´æ€§ã€æé«˜æ¨ç†è¿è´¯æ€§å’Œè§£å†³å†²çªä¸Šä¸‹æ–‡çº¿ç´¢æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>14B-GRPOæ¨¡å‹åœ¨è„šæœ¬çŸ¥è¯†å’Œå¯¹è¯è®°å¿†æŒ‡æ ‡ä¸Šå®ç°äº†é«˜å‡†ç¡®ç‡ã€‚</li>
<li>RAIDEN-R1æ¡†æ¶å¡«è¡¥äº†RPCAè®­ç»ƒä¸­çš„éé‡åŒ–æ€§ç©ºç™½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10218">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dc77633055b66c3f19cccb2302ea141a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8840546b718517737b9307a82168ef3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dc9c5135596050ca56b3e09c6ba3ef3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b77f4ad529900f444acf595e52bb5744.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f09b6da3284d4387aba8e86391f7afca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5500fd3be27b12fed78b895cb32542d0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Mining-Hidden-Thoughts-from-Texts-Evaluating-Continual-Pretraining-with-Synthetic-Data-for-LLM-Reasoning"><a href="#Mining-Hidden-Thoughts-from-Texts-Evaluating-Continual-Pretraining-with-Synthetic-Data-for-LLM-Reasoning" class="headerlink" title="Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with   Synthetic Data for LLM Reasoning"></a>Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with   Synthetic Data for LLM Reasoning</h2><p><strong>Authors:Yoichi Ishibashi, Taro Yano, Masafumi Oyamada</strong></p>
<p>Large Language Models (LLMs) have demonstrated significant improvements in reasoning capabilities through supervised fine-tuning and reinforcement learning. However, when training reasoning models, these approaches are primarily applicable to specific domains such as mathematics and programming, which imposes fundamental constraints on the breadth and scalability of training data. In contrast, continual pretraining (CPT) offers the advantage of not requiring task-specific signals. Nevertheless, how to effectively synthesize training data for reasoning and how such data affect a wide range of domains remain largely unexplored. This study provides a detailed evaluation of Reasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden thought processes underlying texts, based on the premise that texts are the result of the authorâ€™s thinking process. Specifically, we apply Reasoning CPT to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and Law corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis reveals that Reasoning CPT consistently improves performance across all evaluated domains. Notably, reasoning skills acquired in one domain transfer effectively to others; the performance gap with conventional methods widens as problem difficulty increases, with gains of up to 8 points on the most challenging problems. Furthermore, models trained with hidden thoughts learn to adjust the depth of their reasoning according to problem difficulty. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆfine-tuningï¼‰å’Œå¼ºåŒ–å­¦ä¹ åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨è®­ç»ƒæ¨ç†æ¨¡å‹æ—¶ï¼Œè¿™äº›æ–¹æ³•ä¸»è¦é€‚ç”¨äºç‰¹å®šé¢†åŸŸï¼Œå¦‚æ•°å­¦å’Œç¼–ç¨‹ï¼Œè¿™å¯¹è®­ç»ƒæ•°æ®çš„å¹¿åº¦å’Œå¯æ‰©å±•æ€§é€ æˆäº†æ ¹æœ¬æ€§é™åˆ¶ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒæŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰çš„ä¼˜åŠ¿åœ¨äºä¸éœ€è¦ç‰¹å®šä»»åŠ¡çš„ä¿¡å·ã€‚ç„¶è€Œï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆæˆç”¨äºæ¨ç†çš„è®­ç»ƒæ•°æ®ä»¥åŠæ­¤ç±»æ•°æ®å¦‚ä½•å½±å“å¹¿æ³›é¢†åŸŸä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚æœ¬ç ”ç©¶å¯¹åŸºäºæ–‡æœ¬æ˜¯ä½œè€…æ€è€ƒè¿‡ç¨‹çš„ç»“æœè¿™ä¸€å‰æçš„æ¨ç†CPTè¿›è¡Œäº†è¯¦ç»†è¯„ä¼°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨åˆæˆæ•°æ®å¯¹æ–‡æœ¬èƒŒåçš„éšè—æ€ç»´è¿‡ç¨‹è¿›è¡Œé‡å»ºï¼Œå¹¶å°†è¿™ç§å½¢å¼çš„CPTåº”ç”¨äºåŸºäºSTEMå’Œæ³•å¾‹è¯­æ–™åº“çš„éšè—æ€ç»´åˆæˆæ•°æ®çš„Gemma2-9Bæ¨¡å‹ä¸Šï¼Œå¹¶å°†å…¶ä¸MMLUåŸºå‡†æµ‹è¯•ä¸Šçš„æ ‡å‡†CPTè¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæ¨ç†CPTåœ¨æ‰€æœ‰è¯„ä¼°é¢†åŸŸä¸­çš„è¡¨ç°å‡æœ‰æ‰€æé«˜ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨ä¸€ä¸ªé¢†åŸŸè·å¾—çš„æ¨ç†æŠ€èƒ½å¯ä»¥æœ‰æ•ˆåœ°è½¬ç§»åˆ°å…¶ä»–é¢†åŸŸï¼›éšç€é—®é¢˜éš¾åº¦çš„å¢åŠ ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•çš„æ€§èƒ½å·®è·æ‰©å¤§ï¼Œåœ¨æœ€å…·æŒ‘æˆ˜çš„é—®é¢˜ä¸Šè·å¾—é«˜è¾¾8ç‚¹çš„æ”¶ç›Šã€‚æ­¤å¤–ï¼Œä½¿ç”¨éšè—æ€ç»´è®­ç»ƒçš„æ¨¡å‹å­¦ä¼šäº†æ ¹æ®é—®é¢˜éš¾åº¦è°ƒæ•´å…¶æ¨ç†çš„æ·±åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10182v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ å’ŒæŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚æœ¬ç ”ç©¶è¯¦ç»†è¯„ä¼°äº†ä¸€ç§åä¸ºâ€œæ¨ç†CPTâ€çš„CPTæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨åˆæˆæ•°æ®é‡æ„æ–‡æœ¬èƒŒåçš„éšè—æ€ç»´è¿‡ç¨‹ã€‚å®éªŒæ˜¾ç¤ºï¼Œæ¨ç†CPTåœ¨è·¨åŸŸæ€§èƒ½ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜éš¾åº¦é—®é¢˜ä¸Šä¼˜åŠ¿æ›´ä¸ºæ˜æ˜¾ï¼Œæ€§èƒ½æå‡å¹…åº¦æœ€é«˜å¯è¾¾8ç‚¹ã€‚æ­¤å¤–ï¼Œæ¨¡å‹é€šè¿‡éšè—æ€ç»´å­¦ä¹ ï¼Œèƒ½å¤Ÿæ ¹æ®é—®é¢˜éš¾åº¦è°ƒæ•´æ¨ç†æ·±åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡ç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ æå‡äº†æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ¨ç†CPTæ˜¯ä¸€ç§ä½¿ç”¨åˆæˆæ•°æ®é‡æ„æ–‡æœ¬èƒŒåéšè—æ€ç»´è¿‡ç¨‹çš„CPTæ–¹æ³•ã€‚</li>
<li>æ¨ç†CPTåœ¨è·¨åŸŸæ€§èƒ½ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯è§£å†³é«˜éš¾åº¦é—®é¢˜æ—¶ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ¨ç†CPTåœ¨æ€§èƒ½ä¸Šæœ‰æ‰€æå‡ï¼Œå·®è·éšé—®é¢˜éš¾åº¦å¢å¤§è€Œæ‰©å¤§ã€‚</li>
<li>é€šè¿‡éšè—æ€ç»´å­¦ä¹ ï¼Œæ¨¡å‹èƒ½å¤Ÿæ ¹æ®é—®é¢˜éš¾åº¦è°ƒæ•´æ¨ç†æ·±åº¦ã€‚</li>
<li>è¯¥ç ”ç©¶ä½¿ç”¨äº†Gemma2-9Bæ¨¡å‹å’Œåˆæˆæ•°æ®ï¼Œå¹¶åœ¨MMLUåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10182">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-147d523000637406d3e7c39fccbbc017.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b35aae72f9b315cca03370b232dbab23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e67390c6af9916b786241add53a424a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7b955f09d3f0034aa0b65d65b2f0fd34.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ImagineBench-Evaluating-Reinforcement-Learning-with-Large-Language-Model-Rollouts"><a href="#ImagineBench-Evaluating-Reinforcement-Learning-with-Large-Language-Model-Rollouts" class="headerlink" title="ImagineBench: Evaluating Reinforcement Learning with Large Language   Model Rollouts"></a>ImagineBench: Evaluating Reinforcement Learning with Large Language   Model Rollouts</h2><p><strong>Authors:Jing-Cheng Pang, Kaiyuan Li, Yidi Wang, Si-Hang Yang, Shengyi Jiang, Yang Yu</strong></p>
<p>A central challenge in reinforcement learning (RL) is its dependence on extensive real-world interaction data to learn task-specific policies. While recent work demonstrates that large language models (LLMs) can mitigate this limitation by generating synthetic experience (noted as imaginary rollouts) for mastering novel tasks, progress in this emerging field is hindered due to the lack of a standard benchmark. To bridge this gap, we introduce ImagineBench, the first comprehensive benchmark for evaluating offline RL algorithms that leverage both real rollouts and LLM-imaginary rollouts. The key features of ImagineBench include: (1) datasets comprising environment-collected and LLM-imaginary rollouts; (2) diverse domains of environments covering locomotion, robotic manipulation, and navigation tasks; and (3) natural language task instructions with varying complexity levels to facilitate language-conditioned policy learning. Through systematic evaluation of state-of-the-art offline RL algorithms, we observe that simply applying existing offline RL algorithms leads to suboptimal performance on unseen tasks, achieving 35.44% success rate in hard tasks in contrast to 64.37% of method training on real rollouts for hard tasks. This result highlights the need for algorithm advancements to better leverage LLM-imaginary rollouts. Additionally, we identify key opportunities for future research: including better utilization of imaginary rollouts, fast online adaptation and continual learning, and extension to multi-modal tasks. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/LAMDA-RL/ImagineBench">https://github.com/LAMDA-RL/ImagineBench</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºå…¶ä¾èµ–äºå¤§é‡çš„çœŸå®ä¸–ç•Œäº¤äº’æ•°æ®æ¥å­¦ä¹ ç‰¹å®šä»»åŠ¡çš„ç­–ç•¥ã€‚è™½ç„¶æœ€è¿‘çš„å·¥ä½œè¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥é€šè¿‡ç”Ÿæˆåˆæˆç»éªŒï¼ˆç§°ä¸ºè™šæ„æ¼”ç»ƒï¼‰æ¥ç¼“è§£è¿™ä¸€é™åˆ¶ï¼Œä»è€ŒæŒæ¡æ–°ä»»åŠ¡ï¼Œä½†è¿™ä¸€æ–°å…´é¢†åŸŸçš„è¿›å±•å› ç¼ºä¹æ ‡å‡†åŸºå‡†è€Œå—åˆ°é˜»ç¢ã€‚ä¸ºäº†å¼¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ImagineBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°ç¦»çº¿RLç®—æ³•çš„å…¨é¢åŸºå‡†ï¼Œè¯¥åŸºå‡†åˆ©ç”¨çœŸå®æ¼”ç»ƒå’ŒLLMè™šæ„æ¼”ç»ƒã€‚ImagineBenchçš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰åŒ…å«ç¯å¢ƒæ”¶é›†å’ŒLLMè™šæ„æ¼”ç»ƒçš„æ•°æ®é›†ï¼›ï¼ˆ2ï¼‰æ¶µç›–è¿åŠ¨ã€æœºå™¨äººæ“ä½œå’Œå¯¼èˆªä»»åŠ¡ç­‰ä¸åŒç¯å¢ƒé¢†åŸŸï¼›ï¼ˆ3ï¼‰å…·æœ‰ä¸åŒå¤æ‚çº§åˆ«çš„è‡ªç„¶è¯­è¨€ä»»åŠ¡æŒ‡ä»¤ï¼Œä»¥ä¿ƒè¿›è¯­è¨€æ¡ä»¶ä¸‹çš„æ”¿ç­–å­¦ä¹ ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„ç¦»çº¿RLç®—æ³•çš„ç³»ç»Ÿè¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°ï¼Œç®€å•åœ°åº”ç”¨ç°æœ‰çš„ç¦»çº¿RLç®—æ³•åœ¨æœªå®Œæˆçš„ä»»åŠ¡ä¸Šçš„è¡¨ç°å¹¶ä¸ç†æƒ³ï¼Œåœ¨å›°éš¾ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡ä»…ä¸º35.44%ï¼Œè€Œä»…åœ¨çœŸå®æ¼”ç»ƒçš„æ–¹æ³•ä¸Šè®­ç»ƒçš„ä»»åŠ¡åœ¨å›°éš¾ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡è¾¾åˆ°64.37%ã€‚è¿™ä¸€ç»“æœå¼ºè°ƒäº†ç®—æ³•è¿›æ­¥çš„å¿…è¦æ€§ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨LLMè™šæ„æ¼”ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç¡®å®šäº†æœªæ¥ç ”ç©¶çš„å…³é”®æœºä¼šï¼šåŒ…æ‹¬æ›´å¥½åœ°åˆ©ç”¨è™šæ„æ¼”ç»ƒã€å¿«é€Ÿåœ¨çº¿é€‚åº”å’ŒæŒç»­å­¦ä¹ ï¼Œä»¥åŠæ‰©å±•åˆ°å¤šæ¨¡å¼ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/LAMDA-RL/ImagineBench%E3%80%82">https://github.com/LAMDA-RL/ImagineBenchã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10010v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ä¸­å¿ƒæŒ‘æˆ˜æ˜¯ï¼Œå®ƒä¾èµ–äºå¤§é‡çš„çœŸå®ä¸–ç•Œäº¤äº’æ•°æ®æ¥å­¦ä¹ ç‰¹å®šçš„ä»»åŠ¡ç­–ç•¥ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥é€šè¿‡ç”Ÿæˆåˆæˆç»éªŒï¼ˆç§°ä¸ºæƒ³è±¡æ¨æ¼”ï¼‰æ¥ç¼“è§£è¿™ä¸€å±€é™æ€§ï¼Œä»è€ŒæŒæ¡æ–°ä»»åŠ¡ï¼Œä½†è¿™ä¸€æ–°å…´é¢†åŸŸçš„è¿›å±•å—åˆ°ç¼ºä¹æ ‡å‡†åŸºå‡†çš„é˜»ç¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ImagineBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè¯„ä¼°ç¦»çº¿RLç®—æ³•çš„æ ‡å‡†åŸºå‡†ï¼Œè¿™äº›ç®—æ³•å¯ä»¥åˆ©ç”¨çœŸå®çš„æ¨æ¼”å’ŒLLMçš„æƒ³è±¡æ¨æ¼”ã€‚ImagineBenchçš„å…³é”®åŠŸèƒ½åŒ…æ‹¬ï¼š1ï¼‰åŒ…å«ç¯å¢ƒæ”¶é›†å’ŒLLMæƒ³è±¡æ¨æ¼”çš„æ•°æ®é›†ï¼›2ï¼‰æ¶µç›–è¿åŠ¨ã€æœºå™¨äººæ“ä½œå’Œå¯¼èˆªä»»åŠ¡ç­‰ä¸åŒç¯å¢ƒé¢†åŸŸï¼›3ï¼‰å¸¦æœ‰ä¸åŒå¤æ‚çº§åˆ«çš„è‡ªç„¶è¯­è¨€ä»»åŠ¡æŒ‡ä»¤ï¼Œä»¥ä¿ƒè¿›è¯­è¨€æ¡ä»¶ä¸‹çš„æ”¿ç­–å­¦ä¹ ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„ç¦»çº¿RLç®—æ³•çš„ç³»ç»Ÿè¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°ï¼Œç®€å•åœ°åº”ç”¨ç°æœ‰çš„ç¦»çº¿RLç®—æ³•åœ¨æœªçŸ¥ä»»åŠ¡ä¸Šçš„è¡¨ç°å¹¶ä¸ç†æƒ³ï¼Œåœ¨å›°éš¾ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡ä»…ä¸º35.44%ï¼Œè€Œæ–¹æ³•è®­ç»ƒåœ¨çœŸå®æ¨æ¼”ä¸Šçš„æˆåŠŸç‡åˆ™ä¸º64.37%ã€‚è¿™ä¸€ç»“æœå¼ºè°ƒäº†ç®—æ³•éœ€è¦æ›´å¥½åœ°åˆ©ç”¨LLMæƒ³è±¡æ¨æ¼”çš„å¿…è¦æ€§å’Œæœªæ¥ç ”ç©¶çš„å…³é”®æœºä¼šï¼ŒåŒ…æ‹¬æ›´å¥½åœ°åˆ©ç”¨æƒ³è±¡æ¨æ¼”ã€å¿«é€Ÿåœ¨çº¿é€‚åº”å’ŒæŒç»­å­¦ä¹ ä»¥åŠæ‰©å±•åˆ°å¤šæ¨¡å¼ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ä»£ç å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/LAMDA-RL/ImagineBench%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/LAMDA-RL/ImagineBenchå…¬å¼€å¯ç”¨ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä¾èµ–å¤§é‡çœŸå®ä¸–ç•Œäº¤äº’æ•°æ®æ¥å­¦ä¹ ä»»åŠ¡ç‰¹å®šç­–ç•¥ï¼Œå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½é€šè¿‡ç”Ÿæˆåˆæˆç»éªŒï¼ˆæƒ³è±¡æ¨æ¼”ï¼‰ç¼“è§£æ­¤å±€é™æ€§ã€‚</li>
<li>ç¼ºä¹æ ‡å‡†åŸºå‡†æ¥è¯„ä¼°ç¦»çº¿RLç®—æ³•åˆ©ç”¨çœŸå®å’Œæƒ³è±¡çš„æ¨æ¼”ã€‚</li>
<li>ImagineBenchå¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œæä¾›è¯„ä¼°ç¦»çº¿RLç®—æ³•çš„æ ‡å‡†åŸºå‡†ã€‚</li>
<li>ImagineBenchåŒ…å«ç¯å¢ƒæ”¶é›†å’ŒLLMæƒ³è±¡æ¨æ¼”çš„æ•°æ®é›†ï¼Œæ¶µç›–å¤šä¸ªé¢†åŸŸå’Œç¯å¢ƒã€‚</li>
<li>ç°æœ‰ç¦»çº¿RLç®—æ³•åœ¨æœªçŸ¥ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸ç†æƒ³ï¼Œå¼ºè°ƒç®—æ³•éœ€è¦æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10010">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3eddba546c07ea91ae2867036440fe23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-140b74b2a486bd6de51ba93b4d853d96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdb845eb7783c71fd728ea7190451719.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e297b795f1931e103264111196155f5c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Descriptive-Image-Text-Matching-with-Graded-Contextual-Similarity"><a href="#Descriptive-Image-Text-Matching-with-Graded-Contextual-Similarity" class="headerlink" title="Descriptive Image-Text Matching with Graded Contextual Similarity"></a>Descriptive Image-Text Matching with Graded Contextual Similarity</h2><p><strong>Authors:Jinhyun Jang, Jiyeong Lee, Kwanghoon Sohn</strong></p>
<p>Image-text matching aims to build correspondences between visual and textual data by learning their pairwise similarities. Most existing approaches have adopted sparse binary supervision, indicating whether a pair of images and sentences matches or not. However, such sparse supervision covers a limited subset of image-text relationships, neglecting their inherent many-to-many correspondences; an image can be described in numerous texts at different descriptive levels. Moreover, existing approaches overlook the implicit connections from general to specific descriptions, which form the underlying rationale for the many-to-many relationships between vision and language. In this work, we propose descriptive image-text matching, called DITM, to learn the graded contextual similarity between image and text by exploring the descriptive flexibility of language. We formulate the descriptiveness score of each sentence with cumulative term frequency-inverse document frequency (TF-IDF) to balance the pairwise similarity according to the keywords in the sentence. Our method leverages sentence descriptiveness to learn robust image-text matching in two key ways: (1) to refine the false negative labeling, dynamically relaxing the connectivity between positive and negative pairs, and (2) to build more precise matching, aligning a set of relevant sentences in a generic-to-specific order. By moving beyond rigid binary supervision, DITM enhances the discovery of both optimal matches and potential positive pairs. Extensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the effectiveness of our method in representing complex image-text relationships compared to state-of-the-art approaches. In addition, DITM enhances the hierarchical reasoning ability of the model, supported by the extensive analysis on HierarCaps benchmark. </p>
<blockquote>
<p>å›¾åƒæ–‡æœ¬åŒ¹é…æ—¨åœ¨é€šè¿‡å­¦ä¹ å›¾åƒå’Œæ–‡æœ¬æ•°æ®ä¹‹é—´çš„æˆå¯¹ç›¸ä¼¼æ€§æ¥å»ºç«‹å®ƒä»¬ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•éƒ½é‡‡ç”¨äº†ç¨€ç–äºŒå…ƒç›‘ç£ï¼Œå³æŒ‡ç¤ºå›¾åƒå’Œå¥å­æ˜¯å¦åŒ¹é…ã€‚ç„¶è€Œï¼Œè¿™ç§ç¨€ç–çš„ç›‘ç£åªæ¶µç›–äº†æœ‰é™çš„å›¾åƒæ–‡æœ¬å…³ç³»ï¼Œå¿½è§†äº†å®ƒä»¬å›ºæœ‰çš„å¤šå¯¹å¤šå¯¹åº”å…³ç³»ï¼›ä¸€ä¸ªå›¾åƒå¯ä»¥åœ¨ä¸åŒçš„æè¿°çº§åˆ«é€šè¿‡è®¸å¤šæ–‡æœ¬è¿›è¡Œæè¿°ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•å¿½ç•¥äº†ä»ä¸€èˆ¬åˆ°ç‰¹æ®Šæè¿°çš„éšæ€§è”ç³»ï¼Œè¿™äº›è”ç³»æ„æˆäº†è§†è§‰å’Œè¯­è¨€ä¹‹é—´å¤šå¯¹å¤šå…³ç³»çš„åŸºæœ¬é€»è¾‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æè¿°æ€§å›¾åƒæ–‡æœ¬åŒ¹é…ï¼ˆDITMï¼‰ï¼Œé€šè¿‡å­¦ä¹ è¯­è¨€çš„æè¿°æ€§çµæ´»æ€§æ¥å­¦ä¹ å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„åˆ†çº§ä¸Šä¸‹æ–‡ç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬ç”¨ç´¯è®¡çš„è¯é¢‘é€†æ–‡æ¡£é¢‘ç‡ï¼ˆTF-IDFï¼‰æ¥åˆ¶å®šæ¯ä¸ªå¥å­çš„æè¿°æ€§å¾—åˆ†ï¼Œæ ¹æ®å¥å­ä¸­çš„å…³é”®è¯æ¥å¹³è¡¡é…å¯¹ç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¥å­çš„æè¿°æ€§æ¥å­¦ä¹ é²æ£’å›¾åƒæ–‡æœ¬åŒ¹é…çš„ä¸¤ä¸ªå…³é”®æ–¹å¼ï¼šï¼ˆ1ï¼‰ç»†åŒ–å‡é˜´æ€§æ ‡ç­¾ï¼ŒåŠ¨æ€æ”¾æ¾æ­£è´Ÿå¯¹ä¹‹é—´çš„è¿æ¥ï¼›ï¼ˆ2ï¼‰æ„å»ºæ›´ç²¾ç¡®çš„åŒ¹é…ï¼ŒæŒ‰é€šç”¨åˆ°ç‰¹å®šçš„é¡ºåºå¯¹é½ä¸€ç»„ç›¸å…³å¥å­ã€‚é€šè¿‡è¶…è¶Šåƒµç¡¬çš„äºŒå…ƒç›‘ç£ï¼ŒDITMæé«˜äº†æœ€ä½³åŒ¹é…å’Œæ½œåœ¨æ­£å¯¹çš„å‘ç°èƒ½åŠ›ã€‚åœ¨MS-COCOã€Flickr30Kå’ŒCxCæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¡¨ç¤ºå¤æ‚çš„å›¾åƒæ–‡æœ¬å…³ç³»æ–¹é¢æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æ›´æœ‰æ•ˆã€‚æ­¤å¤–ï¼ŒDITMå¢å¼ºäº†æ¨¡å‹çš„åˆ†å±‚æ¬¡æ¨ç†èƒ½åŠ›ï¼Œè¿™åœ¨HierarCapsåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›åˆ†æå¾—åˆ°äº†æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09997v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¿™æ˜¯ä¸€é¡¹å…³äºå›¾åƒæ–‡æœ¬åŒ¹é…çš„ç ”ç©¶ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦åŸºäºç¨€ç–äºŒå…ƒç›‘ç£ï¼Œå¿½è§†äº†å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„è®¸å¤šå¯¹åº”å…³ç³»ã€‚è¯¥ç ”ç©¶æå‡ºäº†æè¿°æ€§å›¾åƒæ–‡æœ¬åŒ¹é…ï¼ˆDITMï¼‰ï¼Œé€šè¿‡æ¢ç´¢è¯­è¨€çš„æè¿°çµæ´»æ€§æ¥å­¦ä¹ å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„åˆ†çº§ä¸Šä¸‹æ–‡ç›¸ä¼¼æ€§ã€‚DITMé€šè¿‡ç´¯è®¡çš„è¯è¯­æè¿°é¢‘ç‡â€”â€”é€†å‘æ–‡æ¡£é¢‘ç‡ï¼ˆTF-IDFï¼‰å…¬å¼ä¸ºæ¯ä¸ªå¥å­æ‰“åˆ†ï¼Œå¹¶æ ¹æ®å¥å­ä¸­çš„å…³é”®è¯æ¥å¹³è¡¡é…å¯¹ç›¸ä¼¼æ€§ã€‚DITMåœ¨ä¸¤ä¸ªæ–¹é¢å­¦ä¹ ç¨³å¥çš„å›¾åƒæ–‡æœ¬åŒ¹é…ï¼šä¸€æ˜¯ä¼˜åŒ–å‡é˜´æ€§æ ‡ç­¾ï¼ŒåŠ¨æ€æ”¾æ¾æ­£è´Ÿé…å¯¹ä¹‹é—´çš„è¿æ¥ï¼›äºŒæ˜¯æ„å»ºæ›´ç²¾ç¡®çš„åŒ¹é…ï¼ŒæŒ‰é€šç”¨åˆ°ç‰¹å®šçš„é¡ºåºå¯¹é½ä¸€ç³»åˆ—ç›¸å…³å¥å­ã€‚DITMè¶…è¶Šäº†åƒµåŒ–çš„äºŒå…ƒç›‘ç£ï¼Œæé«˜äº†æœ€ä½³åŒ¹é…å’Œæ½œåœ¨æ­£å‘å¯¹çš„å‘ç°èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒæ–‡æœ¬åŒ¹é…æ—¨åœ¨å»ºç«‹è§†è§‰å’Œæ–‡æœ¬æ•°æ®ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œå­¦ä¹ å®ƒä»¬çš„é…å¯¹ç›¸ä¼¼æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦åŸºäºç¨€ç–äºŒå…ƒç›‘ç£ï¼Œå¿½ç•¥äº†å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„è®¸å¤šå¯¹åº”å…³ç³»ã€‚</li>
<li>DITMé€šè¿‡æ¢ç´¢è¯­è¨€çš„æè¿°çµæ´»æ€§ï¼Œå­¦ä¹ å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„åˆ†çº§ä¸Šä¸‹æ–‡ç›¸ä¼¼æ€§ã€‚</li>
<li>DITMåˆ©ç”¨å¥å­çš„æè¿°æ€§ï¼Œé€šè¿‡ç´¯è®¡çš„è¯è¯­æè¿°é¢‘ç‡â€”â€”é€†å‘æ–‡æ¡£é¢‘ç‡ï¼ˆTF-IDFï¼‰ä¸ºæ¯ä¸ªå¥å­æ‰“åˆ†ã€‚</li>
<li>DITMåœ¨ä¼˜åŒ–å‡é˜´æ€§æ ‡ç­¾å’Œæ„å»ºæ›´ç²¾ç¡®çš„åŒ¹é…æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>DITMæé«˜äº†å‘ç°æœ€ä½³åŒ¹é…å’Œæ½œåœ¨æ­£å‘å¯¹çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09997">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4c791d836b14b13abc651f7d86fb8ee1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfbef0b85a8c75f238ba734c335a77b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23032fe639d8a86c2be7b3389cb26d5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f1af76ff242f5f55967d65406366e45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cfd638b897c9e93c0af171d8a800deda.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Pre-Act-Multi-Step-Planning-and-Reasoning-Improves-Acting-in-LLM-Agents"><a href="#Pre-Act-Multi-Step-Planning-and-Reasoning-Improves-Acting-in-LLM-Agents" class="headerlink" title="Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents"></a>Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents</h2><p><strong>Authors:Mrinal Rawat, Ambuje Gupta, Rushil Goomer, Alessandro Di Bari, Neha Gupta, Roberto Pieraccini</strong></p>
<p>The ReAct (Reasoning + Action) capability in large language models (LLMs) has become the foundation of modern agentic systems. Recent LLMs, such as DeepSeek-R1 and OpenAI o1&#x2F;o3, exemplify this by emphasizing reasoning through the generation of ample intermediate tokens, which help build a strong premise before producing the final output tokens. In this paper, we introduce Pre-Act, a novel approach that enhances the agentâ€™s performance by creating a multi-step execution plan along with the detailed reasoning for the given user input. This plan incrementally incorporates previous steps and tool outputs, refining itself after each step execution until the final response is obtained. Our approach is applicable to both conversational and non-conversational agents. To measure the performance of task-oriented agents comprehensively, we propose a two-level evaluation framework: (1) turn level and (2) end-to-end. Our turn-level evaluation, averaged across five models, shows that our approach, Pre-Act, outperforms ReAct by 70% in Action Recall on the Almita dataset. While this approach is effective for larger models, smaller models crucial for practical applications, where latency and cost are key constraints, often struggle with complex reasoning tasks required for agentic systems. To address this limitation, we fine-tune relatively small models such as Llama 3.1 (8B &amp; 70B) using the proposed Pre-Act approach. Our experiments show that the fine-tuned 70B model outperforms GPT-4, achieving a 69.5% improvement in action accuracy (turn-level) and a 28% improvement in goal completion rate (end-to-end) on the Almita (out-of-domain) dataset. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„ReActï¼ˆæ¨ç†+è¡ŒåŠ¨ï¼‰èƒ½åŠ›å·²æˆä¸ºç°ä»£ä»£ç†ç³»ç»Ÿçš„åŸºç¡€ã€‚æœ€è¿‘çš„LLMï¼Œå¦‚DeepSeek-R1å’ŒOpenAI o1&#x2F;o3ï¼Œé€šè¿‡ç”Ÿæˆå¤§é‡ä¸­é—´ä»¤ç‰Œæ¥å¼ºè°ƒæ¨ç†ï¼Œå¸®åŠ©åœ¨ç”Ÿæˆæœ€ç»ˆè¾“å‡ºä»¤ç‰Œä¹‹å‰å»ºç«‹å¼ºæœ‰åŠ›çš„å‰æï¼Œä»¥æ­¤ä¸ºä¾‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Pre-Actï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡åˆ›å»ºå¤šæ­¥æ‰§è¡Œè®¡åˆ’ä¸ç»™å®šç”¨æˆ·è¾“å…¥çš„è¯¦ç»†æ¨ç†æ¥æé«˜ä»£ç†æ€§èƒ½çš„æ–°å‹æ–¹æ³•ã€‚æ­¤è®¡åˆ’é€æ­¥èå…¥å…ˆå‰çš„æ­¥éª¤å’Œå·¥å…·è¾“å‡ºï¼Œå¹¶åœ¨æ¯ä¸ªæ­¥éª¤æ‰§è¡Œåå¯¹å…¶è¿›è¡Œæ”¹è¿›ï¼Œç›´åˆ°è·å¾—æœ€ç»ˆå“åº”ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€‚ç”¨äºå¯¹è¯å’Œéå¯¹è¯ä»£ç†ã€‚ä¸ºäº†å…¨é¢è¡¡é‡ä»»åŠ¡å¯¼å‘å‹ä»£ç†çš„æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤çº§è¯„ä¼°æ¡†æ¶ï¼šï¼ˆ1ï¼‰å›åˆçº§å’Œï¼ˆ2ï¼‰ç«¯åˆ°ç«¯ã€‚æˆ‘ä»¬çš„å›åˆçº§è¯„ä¼°ï¼Œåœ¨äº”ä¸ªæ¨¡å‹ä¸­çš„å¹³å‡ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„Pre-Actæ–¹æ³•åœ¨Almitaæ•°æ®é›†ä¸Šçš„è¡ŒåŠ¨å›å¿†ç‡æ¯”ReActé«˜å‡º70%ã€‚è™½ç„¶æ­¤æ–¹æ³•å¯¹äºè¾ƒå¤§çš„æ¨¡å‹å¾ˆæœ‰æ•ˆï¼Œä½†å¯¹äºå®é™…åº”ç”¨ä¸­è‡³å…³é‡è¦çš„å°å‹æ¨¡å‹ï¼Œåœ¨ä»£ç†ç³»ç»Ÿæ‰€éœ€çš„å¤æ‚æ¨ç†ä»»åŠ¡æ–¹é¢ç»å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œå…¶ä¸­å»¶è¿Ÿå’Œæˆæœ¬æ˜¯å…³é”®çº¦æŸã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨æå‡ºçš„Pre-Actæ–¹æ³•å¯¹è¾ƒå°çš„æ¨¡å‹ï¼ˆå¦‚Llama 3.1ï¼ˆ8Bå’Œ70Bï¼‰ï¼‰è¿›è¡Œå¾®è°ƒã€‚å®éªŒè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„70Bæ¨¡å‹åœ¨Almitaï¼ˆç¦»åŸŸï¼‰æ•°æ®é›†ä¸Šçš„è¡ŒåŠ¨å‡†ç¡®æ€§ï¼ˆå›åˆçº§ï¼‰æé«˜äº†69.5%ï¼Œç›®æ ‡å®Œæˆç‡ï¼ˆç«¯åˆ°ç«¯ï¼‰æé«˜äº†28%ï¼Œè¡¨ç°ä¼˜äºGPT-4ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09970v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨ç°ä»£ä»£ç†ç³»ç»Ÿä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„ReActï¼ˆæ¨ç†+è¡ŒåŠ¨ï¼‰èƒ½åŠ›å·²ä½œä¸ºåŸºçŸ³ã€‚æœ€æ–°çš„LLMå¦‚DeepSeek-R1å’ŒOpenAI o1&#x2F;o3å¼ºè°ƒé€šè¿‡ç”Ÿæˆå¤§é‡ä¸­é—´ä»¤ç‰Œè¿›è¡Œæ¨ç†ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹æ–¹æ³•Pre-Actï¼Œé€šè¿‡åˆ›å»ºå¤šæ­¥éª¤æ‰§è¡Œè®¡åˆ’ä»¥åŠç»™å®šç”¨æˆ·è¾“å…¥çš„è¯¦ç»†æ¨ç†ï¼Œæé«˜äº†ä»£ç†çš„æ€§èƒ½ã€‚æ­¤æ–¹æ³•é€‚ç”¨äºå¯¹è¯å’Œéå¯¹è¯ä»£ç†ã€‚ä¸ºäº†å…¨é¢è¡¡é‡é¢å‘ä»»åŠ¡çš„ä»£ç†æ€§èƒ½ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤çº§è¯„ä¼°æ¡†æ¶ï¼šï¼ˆ1ï¼‰å›åˆçº§åˆ«å’Œï¼ˆ2ï¼‰ç«¯åˆ°ç«¯ã€‚å®éªŒè¡¨æ˜ï¼ŒPre-Actåœ¨Almitaæ•°æ®é›†ä¸Šçš„è¡ŒåŠ¨å›å¿†ç‡æ¯”ReActé«˜å‡º70%ã€‚å°½ç®¡è¿™ç§æ–¹æ³•å¯¹äºå¤§å‹æ¨¡å‹æœ‰æ•ˆï¼Œä½†å¯¹äºå…³é”®å®è·µåº”ç”¨ä¸­è¾ƒå°çš„æ¨¡å‹ï¼Œåœ¨ä»£ç†ç³»ç»Ÿçš„å¤æ‚æ¨ç†ä»»åŠ¡æ–¹é¢å¾€å¾€è¡¨ç°æŒ£æ‰ã€‚ä¸ºè§£å†³æ­¤é™åˆ¶ï¼Œæˆ‘ä»¬ä½¿ç”¨Pre-Actæ–¹æ³•å¯¹è¾ƒå°çš„æ¨¡å‹å¦‚Llama 3.1ï¼ˆ8Bå’Œ70Bï¼‰è¿›è¡Œäº†å¾®è°ƒã€‚å®éªŒæ˜¾ç¤ºï¼Œç»è¿‡è°ƒæ ¡çš„70Bæ¨¡å‹åœ¨Almitaï¼ˆç¦»åŸŸï¼‰æ•°æ®é›†ä¸Šçš„è¡ŒåŠ¨å‡†ç¡®åº¦æé«˜äº†69.5%ï¼ˆå›åˆçº§åˆ«ï¼‰ï¼Œç›®æ ‡å®Œæˆç‡æé«˜äº†28%ï¼ˆç«¯åˆ°ç«¯ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„ReActèƒ½åŠ›å·²æˆä¸ºç°ä»£ä»£ç†ç³»ç»Ÿçš„æ ¸å¿ƒã€‚</li>
<li>Pre-Actæ–¹æ³•é€šè¿‡åˆ›å»ºå¤šæ­¥éª¤æ‰§è¡Œè®¡åˆ’å’Œè¯¦ç»†æ¨ç†ï¼Œå¢å¼ºäº†ä»£ç†çš„æ€§èƒ½ã€‚</li>
<li>Pre-Acté€‚ç”¨äºå¯¹è¯å’Œéå¯¹è¯ä»£ç†ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„ä¸¤çº§è¯„ä¼°æ¡†æ¶æ¥è¡¡é‡é¢å‘ä»»åŠ¡çš„ä»£ç†æ€§èƒ½ã€‚</li>
<li>Pre-Actåœ¨è¡ŒåŠ¨å›å¿†ç‡æ–¹é¢è¡¨ç°å‡ºä¼˜äºReActçš„æ€§èƒ½ã€‚</li>
<li>å°å‹æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09970">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7d77df5c2a52bc589754ae54e148c295.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63893c924334b37963f9e6892b9a6b82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-221391cd75da9a1bb3b84df46dbdbf1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59408a0eb1be87815d8c374ff40aaf6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6387a6d85e699995ffdf20f52df02a5c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f4071f8d15e00131770b36ecfecc34c1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Reinforced-Interactive-Continual-Learning-via-Real-time-Noisy-Human-Feedback"><a href="#Reinforced-Interactive-Continual-Learning-via-Real-time-Noisy-Human-Feedback" class="headerlink" title="Reinforced Interactive Continual Learning via Real-time Noisy Human   Feedback"></a>Reinforced Interactive Continual Learning via Real-time Noisy Human   Feedback</h2><p><strong>Authors:Yutao Yang, Jie Zhou, Junsong Li, Qianjun Pan, Bihao Zhan, Qin Chen, Xipeng Qiu, Liang He</strong></p>
<p>This paper introduces an interactive continual learning paradigm where AI models dynamically learn new skills from real-time human feedback while retaining prior knowledge. This paradigm distinctively addresses two major limitations of traditional continual learning: (1) dynamic model updates using streaming, real-time human-annotated data, rather than static datasets with fixed labels, and (2) the assumption of clean labels, by explicitly handling the noisy feedback common in real-world interactions. To tackle these problems, we propose RiCL, a Reinforced interactive Continual Learning framework leveraging Large Language Models (LLMs) to learn new skills effectively from dynamic feedback. RiCL incorporates three key components: a temporal consistency-aware purifier to automatically discern clean from noisy samples in data streams; an interaction-aware direct preference optimization strategy to align model behavior with human intent by reconciling AI-generated and human-provided feedback; and a noise-resistant contrastive learning module that captures robust representations by exploiting inherent data relationships, thus avoiding reliance on potentially unreliable labels. Extensive experiments on two benchmark datasets (FewRel and TACRED), contaminated with realistic noise patterns, demonstrate that our RiCL approach substantially outperforms existing combinations of state-of-the-art online continual learning and noisy-label learning methods. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§äº¤äº’å¼æŒç»­å­¦ä¹ èŒƒå¼ï¼Œå…¶ä¸­AIæ¨¡å‹èƒ½å¤Ÿå®æ—¶åœ°ä»äººç±»åé¦ˆä¸­å­¦ä¹ æ–°æŠ€èƒ½ï¼ŒåŒæ—¶ä¿ç•™å…ˆå‰çŸ¥è¯†ã€‚è¿™ç§èŒƒå¼ç‹¬ç‰¹åœ°è§£å†³äº†ä¼ ç»ŸæŒç»­å­¦ä¹ çš„ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šï¼ˆ1ï¼‰ä½¿ç”¨æµå¼å®æ—¶äººç±»æ³¨é‡Šæ•°æ®è¿›è¡ŒåŠ¨æ€æ¨¡å‹æ›´æ–°ï¼Œè€Œä¸æ˜¯å¸¦æœ‰å›ºå®šæ ‡ç­¾çš„é™æ€æ•°æ®é›†ï¼›ï¼ˆ2ï¼‰é€šè¿‡æ˜¾å¼å¤„ç†ç°å®äº’åŠ¨ä¸­å¸¸è§çš„å˜ˆæ‚åé¦ˆï¼Œæ¥è§£å†³å¹²å‡€æ ‡ç­¾çš„å‡è®¾é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RiCLï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ‰æ•ˆäº¤äº’å¼æŒç»­å­¦ä¹ æ¡†æ¶ï¼Œå¯ä»¥ä»åŠ¨æ€åé¦ˆä¸­å­¦ä¹ æ–°æŠ€èƒ½ã€‚RiCLç»“åˆäº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šä¸€ä¸ªæ—¶é—´ä¸€è‡´æ€§æ„ŸçŸ¥å‡€åŒ–å™¨ï¼Œå¯è‡ªåŠ¨ä»æ•°æ®æµä¸­è¾¨åˆ«å¹²å‡€å’Œå˜ˆæ‚çš„æ ·æœ¬ï¼›ä¸€ç§äº¤äº’æ„ŸçŸ¥ç›´æ¥åå¥½ä¼˜åŒ–ç­–ç•¥ï¼Œé€šè¿‡åè°ƒäººå·¥æ™ºèƒ½ç”Ÿæˆå’Œäººç±»æä¾›çš„åé¦ˆæ¥ä½¿æ¨¡å‹è¡Œä¸ºç¬¦åˆäººç±»æ„å›¾ï¼›ä¸€ä¸ªæŠ—å™ªå£°å¯¹æ¯”å­¦ä¹ æ¨¡å—ï¼Œé€šè¿‡åˆ©ç”¨å†…åœ¨æ•°æ®å…³ç³»æ¥æ•æ‰ç¨³å¥è¡¨ç¤ºï¼Œä»è€Œé¿å…ä¾èµ–å¯èƒ½ä¸å¯é çš„æ ‡ç­¾ã€‚åœ¨å¸¦æœ‰ç°å®å™ªå£°æ¨¡å¼çš„ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ï¼ˆFewRelå’ŒTACREDï¼‰ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„RiCLæ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„åœ¨çº¿æŒç»­å­¦ä¹ å’Œå¸¦å™ªå£°æ ‡ç­¾å­¦ä¹ æ–¹æ³•ç»„åˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09925v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§äº¤äº’å¼æŒç»­å­¦ä¹ èŒƒå¼ï¼Œå…¶ä¸­AIæ¨¡å‹ä»å®æ—¶çš„äººç±»åé¦ˆä¸­åŠ¨æ€å­¦ä¹ æ–°æŠ€èƒ½ï¼ŒåŒæ—¶ä¿ç•™å…ˆå‰çŸ¥è¯†ã€‚è¯¥èŒƒå¼è§£å†³äº†ä¼ ç»ŸæŒç»­å­¦ä¹ çš„ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šä¸€æ˜¯ä½¿ç”¨æµå¼ã€å®æ—¶äººç±»æ³¨é‡Šæ•°æ®è¿›è¡ŒåŠ¨æ€æ¨¡å‹æ›´æ–°ï¼Œè€Œä¸æ˜¯å¸¦æœ‰å›ºå®šæ ‡ç­¾çš„é™æ€æ•°æ®é›†ï¼›äºŒæ˜¯é€šè¿‡æ˜¾å¼å¤„ç†ç°å®äº’åŠ¨ä¸­å¸¸è§çš„å™ªå£°åé¦ˆï¼Œè§£å†³äº†å¯¹å¹²å‡€æ ‡ç­¾çš„å‡è®¾ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†RiCLï¼ˆå¼ºåŒ–äº¤äº’å¼æŒç»­å­¦ä¹ ï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æœ‰æ•ˆåœ°ä»åŠ¨æ€åé¦ˆä¸­å­¦ä¹ æ–°æŠ€èƒ½ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šèƒ½å¤Ÿåœ¨æ•°æ®æµä¸­è‡ªåŠ¨åŒºåˆ†å¹²å‡€æ ·æœ¬å’Œå™ªå£°æ ·æœ¬çš„æ—¶é—´ä¸€è‡´æ€§æ„ŸçŸ¥å‡€åŒ–å™¨ï¼›é€šè¿‡åè°ƒAIç”Ÿæˆå’Œäººç±»æä¾›çš„åé¦ˆï¼Œä½¿æ¨¡å‹è¡Œä¸ºä¸äººç±»æ„å›¾å¯¹é½çš„äº’åŠ¨æ„ŸçŸ¥ç›´æ¥åå¥½ä¼˜åŒ–ç­–ç•¥ï¼›ä»¥åŠåˆ©ç”¨å†…åœ¨æ•°æ®å…³ç³»çš„å™ªå£°æŠµæŠ—å¯¹æ¯”å­¦ä¹ æ¨¡å—ï¼Œä»è€Œé¿å…ä¾èµ–å¯èƒ½ä¸å¯é çš„æ ‡ç­¾ã€‚åœ¨å—ç°å®å™ªå£°æ¨¡å¼æ±¡æŸ“çš„ä¸¤ç»„åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRiCLæ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„åœ¨çº¿æŒç»­å­¦ä¹ å’Œå™ªå£°æ ‡ç­¾å­¦ä¹ æ–¹æ³•ç»„åˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§äº¤äº’å¼æŒç»­å­¦ä¹ èŒƒå¼ï¼Œè§£å†³äº†ä¼ ç»ŸæŒç»­å­¦ä¹ çš„å±€é™æ€§ã€‚</li>
<li>è¿™ç§èŒƒå¼åˆ©ç”¨æµå¼ã€å®æ—¶äººç±»æ³¨é‡Šæ•°æ®è¿›è¡ŒåŠ¨æ€æ¨¡å‹æ›´æ–°ï¼Œå¤„ç†ç°å®äº’åŠ¨ä¸­çš„å™ªå£°åé¦ˆã€‚</li>
<li>RiCLæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šæ—¶é—´ä¸€è‡´æ€§æ„ŸçŸ¥å‡€åŒ–å™¨ã€äº’åŠ¨æ„ŸçŸ¥ç›´æ¥åå¥½ä¼˜åŒ–ç­–ç•¥å’Œå™ªå£°æŠµæŠ—å¯¹æ¯”å­¦ä¹ æ¨¡å—ã€‚</li>
<li>RiCLæ¡†æ¶è§£å†³äº†å¹²å‡€æ ‡ç­¾å‡è®¾çš„é—®é¢˜ï¼Œå¹¶èƒ½å¤Ÿä»å®æ—¶çš„äººç±»åé¦ˆä¸­åŠ¨æ€å­¦ä¹ æ–°æŠ€èƒ½ã€‚</li>
<li>RiCLæ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„åœ¨çº¿æŒç»­å­¦ä¹ å’Œå™ªå£°æ ‡ç­¾å­¦ä¹ æ–¹æ³•ç»„åˆã€‚</li>
<li>RiCLæ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡ŒæŠ€èƒ½å­¦ä¹ ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„æ•°æ®é›†å’Œä»»åŠ¡éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09925">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3268fc77ea4522bfa963d0cf7fc0b1d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a1794b0b519482ede33b594f5a3f38b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Offline-Reinforcement-Learning-for-Microgrid-Voltage-Regulation"><a href="#Offline-Reinforcement-Learning-for-Microgrid-Voltage-Regulation" class="headerlink" title="Offline Reinforcement Learning for Microgrid Voltage Regulation"></a>Offline Reinforcement Learning for Microgrid Voltage Regulation</h2><p><strong>Authors:Shan Yang, Yongli Zhu</strong></p>
<p>This paper presents a study on using different offline reinforcement learning algorithms for microgrid voltage regulation with solar power penetration. When environment interaction is unviable due to technical or safety reasons, the proposed approach can still obtain an applicable model through offline-style training on a previously collected dataset, lowering the negative impact of lacking online environment interactions. Experiment results on the IEEE 33-bus system demonstrate the feasibility and effectiveness of the proposed approach on different offline datasets, including the one with merely low-quality experience. </p>
<blockquote>
<p>æœ¬æ–‡ç ”ç©¶ä½¿ç”¨ä¸åŒçš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯¹å«æœ‰å¤ªé˜³èƒ½æ¸—é€çš„å¾®ç”µç½‘è¿›è¡Œç”µå‹è°ƒèŠ‚ã€‚å½“ç”±äºæŠ€æœ¯æˆ–å®‰å…¨åŸå› æ— æ³•è¿›è¡Œç¯å¢ƒäº¤äº’æ—¶ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä»ç„¶å¯ä»¥é€šè¿‡å¯¹å…ˆå‰æ”¶é›†çš„æ•°æ®é›†è¿›è¡Œç¦»çº¿è®­ç»ƒæ¥è·å¾—é€‚ç”¨çš„æ¨¡å‹ï¼Œä»è€Œé™ä½å› ç¼ºä¹åœ¨çº¿ç¯å¢ƒäº¤äº’è€Œäº§ç”Ÿçš„è´Ÿé¢å½±å“ã€‚åœ¨IEEE 33æ€»çº¿ç³»ç»Ÿä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒçš„ç¦»çº¿æ•°æ®é›†ä¸Šæ˜¯å¯è¡Œå’Œæœ‰æ•ˆçš„ï¼ŒåŒ…æ‹¬ä»…å…·æœ‰ä½è´¨é‡ç»éªŒçš„æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09920v1">PDF</a> This paper has been accepted and presented at ICLR 2025 in Singapore,   Apr. 28, 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä½¿ç”¨ä¸åŒçš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯¹å«æœ‰å¤ªé˜³èƒ½æ¸—é€çš„å¾®ç”µç½‘è¿›è¡Œç”µå‹è°ƒèŠ‚çš„æ–¹æ³•ã€‚åœ¨æ— æ³•ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’æˆ–å› æŠ€æœ¯æˆ–å®‰å…¨åŸå› æ— æ³•è¿›è¡Œåœ¨çº¿è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•å¯ä»¥é€šè¿‡å¯¹å…ˆå‰æ”¶é›†çš„æ•°æ®é›†è¿›è¡Œç¦»çº¿è®­ç»ƒæ¥è·å¾—é€‚ç”¨çš„æ¨¡å‹ï¼Œé™ä½äº†ç¼ºä¹åœ¨çº¿ç¯å¢ƒäº¤äº’çš„è´Ÿé¢å½±å“ã€‚åœ¨IEEE 33æ€»çº¿ç³»ç»Ÿä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒçš„ç¦»çº¿æ•°æ®é›†ä¸Šæ˜¯å¯è¡Œå’Œæœ‰æ•ˆçš„ï¼ŒåŒ…æ‹¬ä»…å«æœ‰ä½è´¨é‡ç»éªŒçš„æ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯¥è®ºæ–‡ç ”ç©¶äº†ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨å¾®ç”µç½‘ç”µå‹è°ƒèŠ‚ä¸­çš„åº”ç”¨ã€‚</li>
<li>å½“æ— æ³•ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ç¦»çº¿è®­ç»ƒæ¥é™ä½å½±å“ã€‚</li>
<li>è®ºæ–‡æå‡ºçš„æ–¹æ³•é€‚ç”¨äºå«æœ‰å¤ªé˜³èƒ½æ¸—é€çš„å¾®ç”µç½‘ç¯å¢ƒã€‚</li>
<li>å®éªŒç»“æœåœ¨IEEE 33æ€»çº¿ç³»ç»Ÿä¸Šè¿›è¡Œäº†éªŒè¯ã€‚</li>
<li>è®ºæ–‡è¡¨æ˜è¯¥æ–¹æ³•åœ¨ä¸åŒçš„ç¦»çº¿æ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼ŒåŒ…æ‹¬é‚£äº›ä»…å«æœ‰ä½è´¨é‡ç»éªŒçš„æ•°æ®é›†ã€‚</li>
<li>è¯¥æ–¹æ³•å¼ºè°ƒäº†ç¦»çº¿æ•°æ®é›†çš„é‡è¦æ€§åŠå…¶åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­çš„æœ‰æ•ˆä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09920">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d2cb13beeb814cecb7d0a9e5ac7d49a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd52898b4c79e6050f10c8a0d12661a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-005c96f3ecd19a3821ee685aed6e6534.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9182d83ccffa6dcecca122285be0f621.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="KRISTEVA-Close-Reading-as-a-Novel-Task-for-Benchmarking-Interpretive-Reasoning"><a href="#KRISTEVA-Close-Reading-as-a-Novel-Task-for-Benchmarking-Interpretive-Reasoning" class="headerlink" title="KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive   Reasoning"></a>KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive   Reasoning</h2><p><strong>Authors:Peiqi Sui, Juan Diego Rodriguez, Philippe Laban, Dean Murphy, Joseph P. Dexter, Richard Jean So, Samuel Baker, Pramit Chaudhuri</strong></p>
<p>Each year, tens of millions of essays are written and graded in college-level English courses. Students are asked to analyze literary and cultural texts through a process known as close reading, in which they gather textual details to formulate evidence-based arguments. Despite being viewed as a basis for critical thinking and widely adopted as a required element of university coursework, close reading has never been evaluated on large language models (LLMs), and multi-discipline benchmarks like MMLU do not include literature as a subject. To fill this gap, we present KRISTEVA, the first close reading benchmark for evaluating interpretive reasoning, consisting of 1331 multiple-choice questions adapted from classroom data. With KRISTEVA, we propose three progressively more difficult sets of tasks to approximate different elements of the close reading process, which we use to test how well LLMs may seem to understand and reason about literary works: 1) extracting stylistic features, 2) retrieving relevant contextual information from parametric knowledge, and 3) multi-hop reasoning between style and external contexts. Our baseline results find that, while state-of-the-art LLMs possess some college-level close reading competency (accuracy 49.7% - 69.7%), their performances still trail those of experienced human evaluators on 10 out of our 11 tasks. </p>
<blockquote>
<p>æ¯å¹´ï¼Œæ•°ä»¥ç™¾ä¸‡è®¡çš„è®ºæ–‡åœ¨å¤§å­¦è‹±æ–‡è¯¾ç¨‹ä¸­æ’°å†™å¹¶è¯„åˆ†ã€‚å­¦ç”Ÿè¢«è¦æ±‚é€šè¿‡ä¸€ä¸ªç§°ä¸ºå¯†åˆ‡é˜…è¯»çš„æµç¨‹åˆ†ææ–‡å­¦å’Œæ–‡åŒ–æ–‡æœ¬ï¼Œåœ¨è¯¥æµç¨‹ä¸­ï¼Œä»–ä»¬æ”¶é›†æ–‡æœ¬ç»†èŠ‚ä»¥å½¢æˆåŸºäºè¯æ®çš„è®ºè¯ã€‚å°½ç®¡å¯†åˆ‡é˜…è¯»è¢«è§†ä¸ºæ‰¹åˆ¤æ€§æ€ç»´çš„åŸºç¡€ï¼Œå¹¶è¢«å¹¿æ³›é‡‡çº³ä¸ºå¤§å­¦è¯¾ç¨‹çš„å¿…è¦å…ƒç´ ï¼Œä½†å®ƒä»æœªåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸Šè¿›è¡Œè¯„ä»·ï¼Œè€Œä¸”åƒMMLUè¿™æ ·çš„å¤šå­¦ç§‘åŸºå‡†å¹¶ä¸åŒ…æ‹¬æ–‡å­¦ä½œä¸ºä¸»é¢˜ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†KRISTEVAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°è§£é‡Šæ€§æ¨ç†çš„å¯†åˆ‡é˜…è¯»åŸºå‡†ï¼Œç”±1331ä¸ªé€‰æ‹©é¢˜ç»„æˆï¼Œè¿™äº›é—®é¢˜æ”¹ç¼–è‡ªè¯¾å ‚æ•°æ®ã€‚é€šè¿‡KRISTEVAï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªéš¾åº¦é€æ¸åŠ å¤§çš„ä»»åŠ¡é›†æ¥æ¨¡æ‹Ÿå¯†åˆ‡é˜…è¯»è¿‡ç¨‹çš„ä¸åŒéƒ¨åˆ†ï¼Œæˆ‘ä»¬ç”¨è¿™äº›ä»»åŠ¡æ¥æµ‹è¯•LLMä¼¼ä¹ç†è§£å’Œæ¨ç†æ–‡å­¦ä½œå“çš„èƒ½åŠ›ï¼š1ï¼‰æå–æ–‡ä½“ç‰¹å¾ï¼Œ2ï¼‰ä»å‚æ•°çŸ¥è¯†ä¸­æ£€ç´¢ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»¥åŠ3ï¼‰é£æ ¼å’Œå¤–éƒ¨ä¸Šä¸‹æ–‡ä¹‹é—´çš„å¤šè·³æ¨ç†ã€‚æˆ‘ä»¬çš„åŸºçº¿ç»“æœå‘ç°ï¼Œè™½ç„¶æœ€å…ˆè¿›çš„LLMå…·æœ‰ä¸€äº›å¤§å­¦æ°´å¹³çš„å¯†åˆ‡é˜…è¯»èƒ½åŠ›ï¼ˆå‡†ç¡®ç‡åœ¨49.7ï¼…è‡³69.7ï¼…ä¹‹é—´ï¼‰ï¼Œä½†å®ƒä»¬åœ¨æ‰€æœ‰ä»»åŠ¡çš„æ€§èƒ½ä»ç„¶è½åäºç»éªŒä¸°å¯Œçš„äººç±»è¯„ä¼°è€…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09825v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨é«˜æ ¡è‹±è¯­è¯¾ç¨‹ä¸­ï¼Œæ¯å¹´æœ‰æ•°åƒä¸‡ç¯‡ä½œæ–‡è¢«æ’°å†™å’Œè¯„åˆ†çš„æƒ…å†µã€‚å­¦ç”Ÿè¢«è¦æ±‚é€šè¿‡ä¸€ç§ç§°ä¸ºâ€œä»”ç»†é˜…è¯»â€çš„è¿‡ç¨‹åˆ†ææ–‡å­¦å’Œæ–‡åŒ–æ–‡æœ¬ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œä»–ä»¬æ”¶é›†æ–‡æœ¬ç»†èŠ‚ä»¥å½¢æˆåŸºäºè¯æ®çš„è®ºè¯ã€‚å°½ç®¡è¢«è§†ä¸ºæ‰¹åˆ¤æ€§æ€ç»´çš„åŸºç¡€å¹¶è¢«å¹¿æ³›é‡‡ç”¨ä½œä¸ºå¤§å­¦è¯¾ç¨‹çš„å¿…è¦å…ƒç´ ï¼Œä½†ä»”ç»†é˜…è¯»ä»æœªåœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸Šå¾—åˆ°è¯„ä¼°ï¼Œè€Œå¤šå­¦ç§‘åŸºå‡†æµ‹è¯•å¦‚MMLUå¹¶ä¸åŒ…æ‹¬æ–‡å­¦ä½œä¸ºä¸»é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†KRISTEVAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°è§£é‡Šæ€§æ¨ç†çš„ä»”ç»†é˜…è¯»åŸºå‡†æµ‹è¯•ï¼Œç”±1331ä¸ªé€‰æ‹©é¢˜ç»„æˆï¼Œè¿™äº›é—®é¢˜æ”¹ç¼–è‡ªè¯¾å ‚æ•°æ®ã€‚é€šè¿‡KRISTEVAï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªéš¾åº¦é€æ¸åŠ å¤§çš„ä»»åŠ¡é›†ï¼Œä»¥æ¨¡æ‹Ÿä»”ç»†é˜…è¯»çš„ä¸åŒè¦ç´ ï¼Œæµ‹è¯•LLMå¯¹æ–‡å­¦ä½œå“çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼š1ï¼‰æå–æ–‡ä½“ç‰¹å¾ï¼Œ2ï¼‰ä»å‚æ•°çŸ¥è¯†ä¸­æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»¥åŠ3ï¼‰åœ¨é£æ ¼å’Œå¤–éƒ¨ä¸Šä¸‹æ–‡ä¹‹é—´è¿›è¡Œå¤šè·³æ¨ç†ã€‚åˆæ­¥ç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶æœ€å…ˆè¿›çš„LLMå…·å¤‡ä¸€äº›å¤§å­¦æ°´å¹³çš„é˜…è¯»ç†è§£èƒ½åŠ›ï¼ˆå‡†ç¡®ç‡åœ¨49.7%~69.7%ä¹‹é—´ï¼‰ï¼Œä½†å®ƒä»¬åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šçš„è¡¨ç°ä»è½åäºç»éªŒä¸°å¯Œçš„äººç±»è¯„ä¼°è€…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­¦ç”Ÿåœ¨å¤§å­¦è‹±è¯­è¯¾ç¨‹ä¸­é€šè¿‡ä»”ç»†é˜…è¯»åˆ†ææ–‡å­¦å’Œæ–‡åŒ–æ–‡æœ¬ã€‚</li>
<li>ä»”ç»†é˜…è¯»è¢«ä½œä¸ºæ‰¹åˆ¤æ€§æ€ç»´çš„åŸºç¡€å¹¶å¹¿æ³›åº”ç”¨äºå¤§å­¦è¯¾ç¨‹ã€‚</li>
<li>ç›®å‰å°šæœªæœ‰é’ˆå¯¹è¯­è¨€æ¨¡å‹çš„ä»”ç»†é˜…è¯»è¯„ä¼°åŸºå‡†ã€‚</li>
<li>ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæ¨å‡ºäº†KRISTEVAåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«æ”¹ç¼–è‡ªè¯¾å ‚æ•°æ®çš„1331ä¸ªé€‰æ‹©é¢˜ã€‚</li>
<li>KRISTEVAè®¾è®¡äº†ä¸‰ä¸ªéš¾åº¦é€’å¢çš„ä»»åŠ¡é›†ä»¥æ¨¡æ‹Ÿä»”ç»†é˜…è¯»çš„ä¸åŒç¯èŠ‚ã€‚</li>
<li>åˆæ­¥ç»“æœæ˜¾ç¤ºLLMå…·å¤‡ä¸€å®šçš„å¤§å­¦æ°´å¹³é˜…è¯»ç†è§£èƒ½åŠ›ï¼Œä½†è¡¨ç°ä»ä½äºç»éªŒä¸°å¯Œçš„è¯„ä¼°è€…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09825">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0d7863465ff52a60948cb8c171a28f5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f5e08f10ba23d522fd4b87290700951.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed0d3756222e8889286ebd7524bcc06f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e77f1bbd32c2b11d2f8a2c44408cf9d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-165d7a1186d32147cad555dc02262d65.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Adversarial-Attack-on-Large-Language-Models-using-Exponentiated-Gradient-Descent"><a href="#Adversarial-Attack-on-Large-Language-Models-using-Exponentiated-Gradient-Descent" class="headerlink" title="Adversarial Attack on Large Language Models using Exponentiated Gradient   Descent"></a>Adversarial Attack on Large Language Models using Exponentiated Gradient   Descent</h2><p><strong>Authors:Sajib Biswas, Mao Nishino, Samuel Jacob Chacko, Xiuwen Liu</strong></p>
<p>As Large Language Models (LLMs) are widely used, understanding them systematically is key to improving their safety and realizing their full potential. Although many models are aligned using techniques such as reinforcement learning from human feedback (RLHF), they are still vulnerable to jailbreaking attacks. Some of the existing adversarial attack methods search for discrete tokens that may jailbreak a target model while others try to optimize the continuous space represented by the tokens of the modelâ€™s vocabulary. While techniques based on the discrete space may prove to be inefficient, optimization of continuous token embeddings requires projections to produce discrete tokens, which might render them ineffective. To fully utilize the constraints and the structures of the space, we develop an intrinsic optimization technique using exponentiated gradient descent with the Bregman projection method to ensure that the optimized one-hot encoding always stays within the probability simplex. We prove the convergence of the technique and implement an efficient algorithm that is effective in jailbreaking several widely used LLMs. We demonstrate the efficacy of the proposed technique using five open-source LLMs on four openly available datasets. The results show that the technique achieves a higher success rate with great efficiency compared to three other state-of-the-art jailbreaking techniques. The source code for our implementation is available at: <a target="_blank" rel="noopener" href="https://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack">https://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack</a> </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹¿æ³›åº”ç”¨ï¼Œç³»ç»Ÿåœ°ç†è§£å®ƒä»¬æ˜¯æé«˜å®‰å…¨æ€§å’Œå®ç°å…¶å…¨éƒ¨æ½œåŠ›çš„å…³é”®ã€‚å°½ç®¡è®¸å¤šæ¨¡å‹é€šè¿‡ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ç­‰æŠ€æœ¯è¿›è¡Œäº†å¯¹é½ï¼Œä½†å®ƒä»¬ä»ç„¶å®¹æ˜“å—åˆ°è¶Šç‹±æ”»å‡»ã€‚ç°æœ‰çš„ä¸€äº›å¯¹æŠ—æ€§æ”»å‡»æ–¹æ³•å¯»æ‰¾å¯èƒ½ä¼šè¶Šç‹±ç›®æ ‡æ¨¡å‹çš„ç¦»æ•£ä»¤ç‰Œï¼Œè€Œå…¶ä»–æ–¹æ³•åˆ™å°è¯•ä¼˜åŒ–æ¨¡å‹è¯æ±‡è¡¨ä¸­ä»¤ç‰Œæ‰€è¡¨ç¤ºçš„è¿ç»­ç©ºé—´ã€‚åŸºäºç¦»æ•£ç©ºé—´çš„æŠ€æœ¯å¯èƒ½è¯æ˜æ•ˆç‡ä½ä¸‹ï¼Œè€Œä¼˜åŒ–è¿ç»­ä»¤ç‰ŒåµŒå…¥åˆ™éœ€è¦æŠ•å½±ä»¥äº§ç”Ÿç¦»æ•£ä»¤ç‰Œï¼Œè¿™å¯èƒ½ä¼šä½¿å®ƒä»¬æ— æ•ˆã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨ç©ºé—´çš„çº¦æŸå’Œç»“æ„ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å†…åœ¨çš„ä¼˜åŒ–æŠ€æœ¯ï¼Œä½¿ç”¨å¸¦æœ‰BregmanæŠ•å½±æ–¹æ³•çš„æŒ‡æ•°æ¢¯åº¦ä¸‹é™æ³•ï¼Œä»¥ç¡®ä¿ä¼˜åŒ–åçš„ä¸€ç»´ç‹¬çƒ­ç¼–ç å§‹ç»ˆä¿æŒåœ¨æ¦‚ç‡å•çº¯å½¢å†…ã€‚æˆ‘ä»¬è¯æ˜äº†è¯¥æŠ€æœ¯çš„æ”¶æ•›æ€§ï¼Œå¹¶å®ç°äº†ä¸€ç§æœ‰æ•ˆç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨è¶Šç‹±å¤šä¸ªå¹¿æ³›ä½¿ç”¨çš„LLMæ–¹é¢éå¸¸æœ‰æ•ˆã€‚æˆ‘ä»¬åœ¨å››ä¸ªå…¬å¼€å¯ç”¨çš„æ•°æ®é›†ä¸Šä½¿ç”¨äº”ä¸ªå¼€æºLLMæ¥å±•ç¤ºæ‰€æå‡ºæŠ€æœ¯çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œä¸ä¸‰ç§å…¶ä»–æœ€å…ˆè¿›çš„è¶Šç‹±æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æŠ€æœ¯åœ¨æˆåŠŸç‡å’Œæ•ˆç‡æ–¹é¢å–å¾—äº†æ›´é«˜çš„æˆç»©ã€‚æˆ‘ä»¬çš„å®ç°çš„æºä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack">https://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09820v1">PDF</a> Accepted to International Joint Conference on Neural Networks (IJCNN)   2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¹¿æ³›åº”ç”¨å¯¹å…¶å®‰å…¨æ€§å’Œæ½œåŠ›å®ç°æå‡ºäº†æŒ‘æˆ˜ã€‚å°½ç®¡è®¸å¤šæ¨¡å‹é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ç­‰æŠ€æœ¯è¿›è¡Œå¯¹é½ï¼Œä½†å®ƒä»¬ä»é¢ä¸´â€œjailbreakingâ€ï¼ˆç ´è§£ï¼‰æ”»å‡»çš„é£é™©ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºæŒ‡æ•°æ¢¯åº¦ä¸‹é™ä¸BregmanæŠ•å½±æ–¹æ³•çš„å†…åœ¨ä¼˜åŒ–æŠ€æœ¯ï¼Œç¡®ä¿ä¼˜åŒ–åçš„ä¸€ç»´ç‹¬çƒ­ç¼–ç å§‹ç»ˆä¿æŒåœ¨æ¦‚ç‡å•çº¯å½¢å†…ï¼Œä»¥å……åˆ†åˆ©ç”¨ç©ºé—´çš„çº¦æŸå’Œç»“æ„ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªå¼€æºLLMså’Œå››ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå±•ç¤ºæœ‰æ•ˆæ€§å’Œé«˜æ•ˆç‡ã€‚ç ”ç©¶ä»£ç å¯åœ¨XXXè·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å®‰å…¨æ€§å’Œæ½œåŠ›å®ç°æ˜¯å…³é”®é—®é¢˜ã€‚</li>
<li>å°½ç®¡ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ç­‰æŠ€æœ¯è¿›è¡Œæ¨¡å‹å¯¹é½ï¼ŒLLMsä»é¢ä¸´â€œjailbreakingâ€ï¼ˆç ´è§£ï¼‰æ”»å‡»é£é™©ã€‚</li>
<li>ç°æœ‰å¯¹æŠ—æ”»å‡»æ–¹æ³•åˆ†ä¸ºç¦»æ•£ä»¤ç‰Œæœç´¢å’Œè¿ç»­ä»¤ç‰ŒåµŒå…¥ä¼˜åŒ–ä¸¤ç±»ã€‚</li>
<li>åŸºäºç¦»æ•£ç©ºé—´çš„æ”»å‡»æ–¹æ³•å¯èƒ½æ•ˆç‡ä½ä¸‹ï¼Œè€Œè¿ç»­ä»¤ç‰ŒåµŒå…¥çš„ä¼˜åŒ–éœ€è¦æŠ•å½±ä»¥äº§ç”Ÿç¦»æ•£ä»¤ç‰Œï¼Œè¿™å¯èƒ½ä½¿å…¶æ— æ•ˆã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæŒ‡æ•°æ¢¯åº¦ä¸‹é™ä¸BregmanæŠ•å½±æ–¹æ³•çš„å†…åœ¨ä¼˜åŒ–æŠ€æœ¯ï¼Œç¡®ä¿ä¼˜åŒ–è¿‡ç¨‹åœ¨æ¦‚ç‡å•çº¯å½¢å†…è¿›è¡Œã€‚</li>
<li>è¯¥æ–¹æ³•è¢«è¯æ˜èƒ½æœ‰æ•ˆç ´è§£å¤šä¸ªå¹¿æ³›ä½¿ç”¨çš„LLMsã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09820">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-982ec70b615b01bb3c11e135a58ccf5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71cf9fb7e9904d1328523b4231386c04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-069600826055e05adc68f792e3d67e58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd108c310c91124ff21cad555c4eae70.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Unfettered-Forceful-Skill-Acquisition-with-Physical-Reasoning-and-Coordinate-Frame-Labeling"><a href="#Unfettered-Forceful-Skill-Acquisition-with-Physical-Reasoning-and-Coordinate-Frame-Labeling" class="headerlink" title="Unfettered Forceful Skill Acquisition with Physical Reasoning and   Coordinate Frame Labeling"></a>Unfettered Forceful Skill Acquisition with Physical Reasoning and   Coordinate Frame Labeling</h2><p><strong>Authors:William Xie, Max Conway, Yutong Zhang, Nikolaus Correll</strong></p>
<p>Vision language models (VLMs) exhibit vast knowledge of the physical world, including intuition of physical and spatial properties, affordances, and motion. With fine-tuning, VLMs can also natively produce robot trajectories. We demonstrate that eliciting wrenches, not trajectories, allows VLMs to explicitly reason about forces and leads to zero-shot generalization in a series of manipulation tasks without pretraining. We achieve this by overlaying a consistent visual representation of relevant coordinate frames on robot-attached camera images to augment our query. First, we show how this addition enables a versatile motion control framework evaluated across four tasks (opening and closing a lid, pushing a cup or chair) spanning prismatic and rotational motion, an order of force and position magnitude, different camera perspectives, annotation schemes, and two robot platforms over 220 experiments, resulting in 51% success across the four tasks. Then, we demonstrate that the proposed framework enables VLMs to continually reason about interaction feedback to recover from task failure or incompletion, with and without human supervision. Finally, we observe that prompting schemes with visual annotation and embodied reasoning can bypass VLM safeguards. We characterize prompt component contribution to harmful behavior elicitation and discuss its implications for developing embodied reasoning. Our code, videos, and data are available at: <a target="_blank" rel="noopener" href="https://scalingforce.github.io/">https://scalingforce.github.io/</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å±•ç°äº†å…³äºç‰©ç†ä¸–ç•Œçš„å¹¿æ³›çŸ¥è¯†ï¼ŒåŒ…æ‹¬ç‰©ç†å’Œç©ºé—´å±æ€§çš„ç›´è§‰ã€åŠŸèƒ½ä»¥åŠè¿åŠ¨ã€‚é€šè¿‡å¾®è°ƒï¼ŒVLMsè¿˜å¯ä»¥ç›´æ¥ç”Ÿæˆæœºå™¨äººè½¨è¿¹ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæ¿€å‘æ‰­åŠ›è€Œéè½¨è¿¹ï¼Œèƒ½è®©VLMsæ˜ç¡®åœ°æ¨ç†åŠ›ï¼Œå¹¶åœ¨ä¸€ç³»åˆ—æ“ä½œä»»åŠ¡ä¸­å®ç°é›¶è®­ç»ƒæ³›åŒ–ã€‚æˆ‘ä»¬é€šè¿‡å°†ç›¸å…³åæ ‡ç³»çš„ä¸€è‡´è§†è§‰è¡¨ç¤ºå åŠ åœ¨æœºå™¨äººé™„åŠ çš„ç›¸æœºå›¾åƒä¸Šæ¥å¢å¼ºæŸ¥è¯¢ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•é€šè¿‡è¿™ä¸€è¡¥å……ï¼Œæ„å»ºä¸€ä¸ªé€šç”¨çš„è¿åŠ¨æ§åˆ¶æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨å››ä¸ªä»»åŠ¡ï¼ˆå¼€é—­ç›–å­ã€æ¨æ¯å­æˆ–æ¤…å­ï¼‰ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶µç›–äº†æ£±æŸ±è¿åŠ¨å’Œæ—‹è½¬è¿åŠ¨ã€åŠ›å’Œä½ç½®çš„å¹…åº¦é¡ºåºã€ä¸åŒçš„ç›¸æœºè§’åº¦ã€æ³¨é‡Šæ–¹æ¡ˆä»¥åŠä¸¤ä¸ªæœºå™¨äººå¹³å°ï¼Œå…±è¿›è¡Œäº†220æ¬¡å®éªŒï¼Œå››ä¸ªä»»åŠ¡çš„æˆåŠŸç‡è¾¾åˆ°51%ã€‚ç„¶åï¼Œæˆ‘ä»¬è¯æ˜äº†æ‰€æå‡ºçš„æ¡†æ¶ä½¿VLMsèƒ½å¤Ÿä¸æ–­æ ¹æ®äº¤äº’åé¦ˆè¿›è¡Œæ¨ç†ï¼Œä»¥ä»ä»»åŠ¡å¤±è´¥æˆ–æœªå®Œæˆä¸­æ¢å¤ï¼Œæ— è®ºæ˜¯å¦æœ‰äººç±»ç›‘ç£ã€‚æœ€åï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å¸¦æœ‰è§†è§‰æ³¨é‡Šå’Œè®¤çŸ¥æ¨ç†çš„æç¤ºæ–¹æ¡ˆå¯ä»¥ç»•è¿‡VLMä¿éšœã€‚æˆ‘ä»¬åˆ†æäº†æç¤ºç»„ä»¶å¯¹æ¿€å‘æœ‰å®³è¡Œä¸ºçš„ä½œç”¨ï¼Œå¹¶è®¨è®ºäº†å…¶åœ¨å¼€å‘è®¤çŸ¥æ¨ç†ä¸­çš„å½±å“ã€‚æˆ‘ä»¬çš„ä»£ç ã€è§†é¢‘å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://scalingforce.github.io/%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://scalingforce.github.io/ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09731v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„ç‰©ç†ä¸–ç•ŒçŸ¥è¯†ï¼Œé€šè¿‡ç²¾ç»†è°ƒæ•´æœºå™¨äººè½¨è¿¹ç”Ÿæˆæ¨¡å‹çš„æ–¹å¼ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•é€šè¿‡ä½¿ç”¨ç²¾ç¡®çš„æœºæ¢°åŠ›é‡æ•°æ®ä»£æ›¿é¢„è®­ç»ƒä¸­çš„å¤æ‚æœºå™¨äººè½¨è¿¹æ¥é¢„æµ‹æœªæ¥çš„è¡ŒåŠ¨å’Œç»“æœã€‚é€šè¿‡ä¸€ç³»åˆ—çš„å®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§ä»»åŠ¡ä¸­å®ç°äº†é›¶æ ·æœ¬æ³›åŒ–ï¼Œå¹¶å…è®¸æœºå™¨äººæŒç»­æ ¹æ®äº¤äº’åé¦ˆè°ƒæ•´å…¶åŠ¨ä½œï¼Œä»ä»»åŠ¡å¤±è´¥æˆ–æœªå®Œæˆä¸­æ¢å¤ã€‚åŒæ—¶ï¼Œæœ¬æ–‡ä¹Ÿæ¢è®¨äº†æç¤ºæ–¹æ¡ˆä¸æ½œåœ¨çš„å®‰å…¨é£é™©ä¹‹é—´çš„å…³ç³»ã€‚ç›¸å…³ä»£ç ã€è§†é¢‘å’Œæ•°æ®å¯åœ¨ç½‘ä¸Šæ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMså±•ç°å‡ºå¯¹ç‰©ç†ä¸–ç•Œçš„ä¸°å¯ŒçŸ¥è¯†ï¼ŒåŒ…æ‹¬ç›´è§‰ã€ç©ºé—´å±æ€§å’ŒåŠ¨æ€ã€‚</li>
<li>é€šè¿‡ç²¾ç»†è°ƒæ•´æœºå™¨äººè½¨è¿¹ç”Ÿæˆæ¨¡å‹ï¼ŒVLMså¯ä»¥è‡ªç„¶åœ°æ¨¡æ‹Ÿæœºå™¨äººåŠ¨ä½œã€‚</li>
<li>é€šè¿‡ä½¿ç”¨æœºæ¢°åŠ›é‡æ•°æ®ä»£æ›¿è½¨è¿¹é¢„æµ‹æœªæ¥è¡ŒåŠ¨å’Œç»“æœï¼Œå®ç°é›¶æ ·æœ¬æ³›åŒ–ã€‚</li>
<li>æ–°æ–¹æ³•åœ¨å¤šç§æ¶‰åŠå¤æ‚æœºæ¢°å’Œæ—‹è½¬åŠ¨ä½œçš„ä»»åŠ¡ä¸ŠæˆåŠŸç‡é«˜ï¼Œä¸”åœ¨è·¨è¶Šå¤šä¸ªä»»åŠ¡åæŒç»­æ€§èƒ½è‰¯å¥½ã€‚</li>
<li>è¯¥æ–¹æ³•å…è®¸æœºå™¨äººæ ¹æ®åé¦ˆè¿›è¡Œè°ƒæ•´ï¼Œåœ¨ä»»åŠ¡å¤±è´¥æˆ–æœªå®Œæˆçš„æƒ…å†µä¸‹è¿›è¡Œæ¢å¤ã€‚ä½†å®‰å…¨æ€§å’Œè¡Œä¸ºé£é™©ä¸å¯å¿½è§†ã€‚å»ºè®®åœ¨å®é™…åº”ç”¨å‰è¿›è¡Œå…¨é¢çš„æµ‹è¯•å’Œè¯„ä¼°ä»¥ç¡®ä¿å®‰å…¨æ€§ã€‚è¿™äº›å‘ç°å’Œå»ºè®®æä¾›äº†ä¸€ç§è§£å†³æ€è·¯ï¼šå½“è€ƒè™‘æœºå™¨äººè¡Œä¸ºçš„ä¼¦ç†å’Œåæœæ—¶ï¼Œå¦‚ä½•å°†ç†è®ºå’Œç°å®æœ‰æ•ˆç»“åˆéå¸¸é‡è¦ã€‚æˆ‘ä»¬åº”è¯¥é‡è§†æœºå™¨äººåœ¨æ‰§è¡Œä»»åŠ¡æ—¶çš„åé¦ˆå’Œååº”èƒ½åŠ›ï¼Œä»¥ä¾¿æ›´å¥½åœ°é¢„æµ‹å’Œæ§åˆ¶å…¶è¡Œä¸ºã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿåº”è¯¥ç»§ç»­ç ”ç©¶å’Œæ¢ç´¢æœºå™¨å­¦ä¹ ç†è®ºå’Œå…¶ä»–å…ˆè¿›çš„å·¥å…·æ¥è§£å†³ç›¸å…³é—®é¢˜å’ŒæŒ‘æˆ˜ï¼Œä¾‹å¦‚åˆ›å»ºæ›´å¥½çš„AIæ¨¡å‹å’Œç®—æ³•æ¥è§£å†³ç‰¹å®šçš„æŒ‘æˆ˜ï¼Œåˆ¶å®šè§„èŒƒå’Œç›‘ç®¡ç­–ç•¥æ¥ç¡®ä¿æœºå™¨äººæŠ€æœ¯å®‰å…¨å¯é åœ°ä½¿ç”¨ç­‰ã€‚åœ¨å®è·µä¸­ç§¯æåº”ç”¨è¿™äº›ç†è®ºå’Œæ–¹æ³•å°†æœ‰åŠ©äºæˆ‘ä»¬æ›´å¥½åœ°ç†è§£å’Œåˆ©ç”¨æœºå™¨äººçš„æ½œåŠ›ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥å’Œå‘å±•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åº”è¯¥å…³æ³¨å¦‚ä½•å¹³è¡¡æœºå™¨äººçš„æ™ºèƒ½å’Œå®‰å…¨æ€§ä¹‹é—´çš„å…³ç³»ï¼Œä»¥ç¡®ä¿æœºå™¨äººæŠ€æœ¯çš„å¯æŒç»­å‘å±•å’Œå¹¿æ³›åº”ç”¨ã€‚å› æ­¤ï¼Œæœªæ¥çš„ç ”ç©¶è¿˜éœ€è¦åŒ…æ‹¬æ·±å…¥çš„å®šæ€§ç ”ç©¶å’Œå…¶ä»–å®è·µå·¥ä½œæ¥è¿›è¡Œæ”¯æŒã€‚â€æ›´åŠ æ·±å…¥ç†è§£è¿™ç§æ–¹æ³•åŠå…¶åœ¨å®é™…æƒ…å†µä¸‹çš„æ½œåœ¨å½±å“å’Œå®è·µåæœä¸Šä»éœ€å¤§é‡ç ”ç©¶åŠªåŠ›ã€‚â€œå°†æ¢è®¨å’Œæ¢ç´¢æ–°æŠ€æœ¯çš„å‘å±•å’Œä¼¦ç†æŒ‘æˆ˜å…³ç³»çš„æ–¹å¼åº”ç”¨äºå…·ä½“æƒ…å¢ƒä¸­æ—¶é¢ä¸´çš„é“å¾·é—®é¢˜å’Œåæœé‡è¦æ€§ä¸å®¹å¿½è§†ã€‚é€šè¿‡åŠ å¼ºå®è·µå’Œç†è®ºå·¥ä½œæ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ä»¥ç¡®ä¿æˆ‘ä»¬çš„ç¤¾ä¼šæœç€å¯æŒç»­çš„æ–¹å‘å‘å±•æ˜¯å½“åŠ¡ä¹‹æ€¥ã€‚é€šè¿‡æœ¬ç ”ç©¶ä¹Ÿå†æ¬¡å¼ºè°ƒäº†æˆ‘ä»¬åœ¨ä½¿ç”¨æ–°å…´æŠ€æœ¯æ—¶åº”å…³æ³¨å®‰å…¨å’Œå¯é æ€§çš„é‡è¦æ€§åŒæ—¶ä¹Ÿéœ€è¦æ³¨æ„è§£å†³æŠ€æœ¯å’Œé“å¾·ä¼¦ç†é—®é¢˜ä»¥æ¨è¿›äººç±»ç¤¾ä¼šçš„æŒç»­å‘å±•å¹¶å®ç°çœŸæ­£çš„äººå·¥æ™ºèƒ½åº”ç”¨å‘å±•ç›®æ ‡å’Œä»·å€¼è¿½æ±‚ã€‚â€å¼ºè°ƒå®é™…åº”ç”¨å’Œå®éªŒéªŒè¯çš„é‡è¦æ€§ä»¥åŠè€ƒè™‘æŠ€æœ¯å’Œä¼¦ç†æŒ‘æˆ˜ä¹‹é—´çš„å¹³è¡¡ã€‚éšç€æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥å’Œåº”ç”¨é¢†åŸŸçš„ä¸æ–­æ‹“å±•æˆ‘ä»¬éœ€è¦åœ¨äº«å—æŠ€æœ¯å¸¦æ¥çš„ä¾¿åˆ©çš„åŒæ—¶å…³æ³¨å…¶æ½œåœ¨çš„é£é™©å’ŒæŒ‘æˆ˜ä»¥ç¡®ä¿äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¯æŒç»­å‘å±•å’Œå¹¿æ³›åº”ç”¨ä¸ºäººç±»ç¤¾ä¼šå¸¦æ¥çœŸæ­£çš„ç¦ç¥‰å’Œè¿›æ­¥ã€‚â€æˆ‘ä»¬å°†åœ¨æœªæ¥çš„ç ”ç©¶ä¸­ç»§ç»­æ¢ç´¢è¿™ä¸€é¢†åŸŸå¹¶å¯»æ±‚æ–°çš„è§£å†³æ–¹æ¡ˆä»¥åº”å¯¹æŒ‘æˆ˜æ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥å’Œå‘å±•ä¸ºäººç±»ç¤¾ä¼šçš„ç¹è£åšå‡ºè´¡çŒ®ã€‚â€</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09731">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-177046cb8d7136901e54d65d011d8333.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a46c33aefbbc809ac7f6bbb714c3612f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adf83f43b1b35791e4d149f2f126c397.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c10d061ce2f62ac27a43fe8fe63d90e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69291ef41aa60367198554e570a77900.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-468fc646175128d48ef3e6eab9d6fe7c.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="ManipBench-Benchmarking-Vision-Language-Models-for-Low-Level-Robot-Manipulation"><a href="#ManipBench-Benchmarking-Vision-Language-Models-for-Low-Level-Robot-Manipulation" class="headerlink" title="ManipBench: Benchmarking Vision-Language Models for Low-Level Robot   Manipulation"></a>ManipBench: Benchmarking Vision-Language Models for Low-Level Robot   Manipulation</h2><p><strong>Authors:Enyu Zhao, Vedant Raval, Hejia Zhang, Jiageng Mao, Zeyu Shangguan, Stefanos Nikolaidis, Yue Wang, Daniel Seita</strong></p>
<p>Vision-Language Models (VLMs) have revolutionized artificial intelligence and robotics due to their commonsense reasoning capabilities. In robotic manipulation, VLMs are used primarily as high-level planners, but recent work has also studied their lower-level reasoning ability, which refers to making decisions about precise robot movements. However, the community currently lacks a clear and common benchmark that can evaluate how well VLMs can aid low-level reasoning in robotics. Consequently, we propose a novel benchmark, ManipBench, to evaluate the low-level robot manipulation reasoning capabilities of VLMs across various dimensions, including how well they understand object-object interactions and deformable object manipulation. We extensively test 33 representative VLMs across 10 model families on our benchmark, including variants to test different model sizes. Our evaluation shows that the performance of VLMs significantly varies across tasks, and there is a strong correlation between this performance and trends in our real-world manipulation tasks. It also shows that there remains a significant gap between these models and human-level understanding. See our website at: <a target="_blank" rel="noopener" href="https://manipbench.github.io/">https://manipbench.github.io</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç”±äºå…¶å¸¸è¯†æ¨ç†èƒ½åŠ›è€Œå½»åº•æ”¹å˜äº†äººå·¥æ™ºèƒ½å’Œæœºå™¨äººé¢†åŸŸã€‚åœ¨æœºå™¨äººæ“çºµä¸­ï¼ŒVLMsä¸»è¦ç”¨ä½œé«˜çº§è§„åˆ’å™¨ï¼Œä½†æœ€è¿‘çš„ç ”ç©¶ä¹Ÿç ”ç©¶äº†å®ƒä»¬çš„ä½çº§æ¨ç†èƒ½åŠ›ï¼Œè¿™æŒ‡çš„æ˜¯å¯¹ç²¾ç¡®æœºå™¨äººè¿åŠ¨çš„å†³ç­–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›®å‰ç¤¾åŒºç¼ºä¹ä¸€ä¸ªæ¸…æ™°ä¸”é€šç”¨çš„åŸºå‡†ï¼Œå¯ä»¥è¯„ä¼°VLMåœ¨è¾…åŠ©æœºå™¨äººä½çº§æ¨ç†æ–¹é¢çš„è¡¨ç°å¦‚ä½•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•â€”â€”ManipBenchï¼Œæ—¨åœ¨è¯„ä¼°VLMåœ¨æœºå™¨äººä½çº§æ“ä½œæ¨ç†èƒ½åŠ›æ–¹é¢çš„å„ç§ç»´åº¦ï¼ŒåŒ…æ‹¬å®ƒä»¬å¯¹ç‰©ä½“é—´äº¤äº’å’Œå¯å˜å½¢ç‰©ä½“æ“ä½œçš„ç†è§£ç¨‹åº¦ã€‚æˆ‘ä»¬åœ¨åŸºå‡†æµ‹è¯•ä¸Šå¯¹æ¥è‡ª10ä¸ªæ¨¡å‹å®¶æ—çš„33ä¸ªä»£è¡¨æ€§VLMè¿›è¡Œäº†å¹¿æ³›æµ‹è¯•ï¼ŒåŒ…æ‹¬æµ‹è¯•ä¸åŒæ¨¡å‹å¤§å°çš„å˜ä½“ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œä¸åŒä»»åŠ¡çš„VLMæ€§èƒ½å·®å¼‚å¾ˆå¤§ï¼Œè€Œä¸”è¿™ç§æ€§èƒ½ä¸æˆ‘ä»¬çš„ç°å®æ“ä½œä»»åŠ¡è¶‹åŠ¿ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ã€‚è¿™ä¹Ÿè¡¨æ˜è¿™äº›æ¨¡å‹ä¸äººç±»æ°´å¹³çš„ç†è§£ä¹‹é—´ä»å­˜åœ¨å¾ˆå¤§å·®è·ã€‚è¯·è®¿é—®æˆ‘ä»¬çš„ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://manipbench.github.ioäº†è§£è¯¦æƒ…./">https://manipbench.github.ioäº†è§£è¯¦æƒ…ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09698v1">PDF</a> 47 pages, 29 figures. Under review</p>
<p><strong>Summary</strong><br>äººå·¥æ™ºèƒ½å’Œæœºå™¨äººé¢†åŸŸæ­£ç»å†ç€ä¸€åœºé©å‘½ï¼Œè¿™å½’åŠŸäºå…·å¤‡å¸¸è¯†æ¨ç†èƒ½åŠ›çš„è·¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚åœ¨æœºå™¨äººæ“æ§ä¸­ï¼ŒVLMsä¸»è¦ç”¨ä½œé«˜çº§è§„åˆ’å™¨ï¼Œä½†è¿‘æœŸçš„ç ”ç©¶ä¹Ÿå¼€å§‹å…³æ³¨å®ƒä»¬åœ¨ä½çº§æ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œå³å…³äºç²¾ç¡®æœºå™¨äººè¿åŠ¨çš„å†³ç­–åˆ¶å®šã€‚ç„¶è€Œï¼Œå½“å‰ç¤¾åŒºç¼ºä¹ä¸€ä¸ªæ¸…æ™°ä¸”é€šç”¨çš„åŸºå‡†æµ‹è¯•æ¥è¡¡é‡VLMsåœ¨æœºå™¨äººä½çº§æ¨ç†ä¸­çš„è¾…åŠ©æ•ˆæœã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•â€”â€”ManipBenchï¼Œç”¨äºè¯„ä¼°VLMsåœ¨æœºå™¨äººæ“æ§ä½çº§æ¨ç†èƒ½åŠ›æ–¹é¢çš„å¤šç»´åº¦è¡¨ç°ï¼ŒåŒ…æ‹¬å®ƒä»¬å¯¹ç‰©ä½“é—´äº¤äº’å’Œå¯å˜å½¢ç‰©ä½“æ“æ§çš„ç†è§£ç¨‹åº¦ã€‚æˆ‘ä»¬å¯¹æ¥è‡ªåä¸ªæ¨¡å‹å®¶æ—çš„33ä¸ªä»£è¡¨æ€§VLMè¿›è¡Œäº†å¹¿æ³›æµ‹è¯•ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¿™äº›æ¨¡å‹åœ¨ä»»åŠ¡ä¸Šçš„è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä¸”è¿™ä¸€è¡¨ç°ä¸ç°å®ä¸–ç•Œæ“æ§ä»»åŠ¡è¶‹åŠ¿ä¹‹é—´å­˜åœ¨å¼ºçƒˆç›¸å…³æ€§ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä¸äººç±»æ°´å¹³ç†è§£ä¹‹é—´ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚æ›´å¤šä¿¡æ¯è¯·è®¿é—®æˆ‘ä»¬çš„ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://manipbench.github.io/">ç½‘ç«™é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è·¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹äººå·¥æ™ºèƒ½å’Œæœºå™¨äººé¢†åŸŸäº§ç”Ÿé‡å¤§å½±å“ï¼Œå°¤å…¶åœ¨å¸¸è¯†æ¨ç†æ–¹é¢ã€‚</li>
<li>VLMsåœ¨æœºå™¨äººæ“æ§ä¸­æ—¢ç”¨ä½œé«˜çº§è§„åˆ’å™¨ï¼Œä¹Ÿå±•ç°å‡ºä½çº§æ¨ç†èƒ½åŠ›ï¼Œæ¶‰åŠç²¾ç¡®æœºå™¨äººè¿åŠ¨çš„å†³ç­–ã€‚</li>
<li>å½“å‰ç¼ºä¹è¡¡é‡VLMsåœ¨æœºå™¨äººä½çº§æ¨ç†ä¸­ä½œç”¨çš„æ¸…æ™°å’Œé€šç”¨åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æå‡ºçš„ManipBenchåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°VLMsåœ¨æœºå™¨äººæ“æ§ä½çº§æ¨ç†èƒ½åŠ›ä¸Šçš„å¤šç»´åº¦è¡¨ç°ã€‚</li>
<li>æµ‹è¯•ç»“æœæ˜¾ç¤ºä¸åŒVLMsåœ¨ä»»åŠ¡ä¸Šçš„è¡¨ç°å·®å¼‚æ˜¾è‘—ï¼Œä¸ç°å®ä¸–ç•Œæ“æ§ä»»åŠ¡è¶‹åŠ¿å­˜åœ¨ç›¸å…³æ€§ã€‚</li>
<li>VLMsä¸äººç±»æ°´å¹³ç†è§£ä¹‹é—´ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09698">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-25048de7fbdc78d76b525c81a17e6179.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67e36b39da576c2e22913a0c190d6ea7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4a2e54cccc4997cc0e8ab9bece1ccee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-651436d36cfe425351c30b135579867c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-add4af276d80398d2da749617ec8b475.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="DRA-GRPO-Exploring-Diversity-Aware-Reward-Adjustment-for-R1-Zero-Like-Training-of-Large-Language-Models"><a href="#DRA-GRPO-Exploring-Diversity-Aware-Reward-Adjustment-for-R1-Zero-Like-Training-of-Large-Language-Models" class="headerlink" title="DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like   Training of Large Language Models"></a>DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like   Training of Large Language Models</h2><p><strong>Authors:Xiwen Chen, Wenhui Zhu, Peijie Qiu, Xuanzhao Dong, Hao Wang, Haiyu Wu, Huayu Li, Aristeidis Sotiras, Yalin Wang, Abolfazl Razi</strong></p>
<p>Recent advances in reinforcement learning for language model post-training, such as Group Relative Policy Optimization (GRPO), have shown promise in low-resource settings. However, GRPO typically relies on solution-level and scalar reward signals that fail to capture the semantic diversity among sampled completions. This leads to what we identify as a diversity-quality inconsistency, where distinct reasoning paths may receive indistinguishable rewards. To address this limitation, we propose $\textit{Diversity-aware Reward Adjustment}$ (DRA), a method that explicitly incorporates semantic diversity into the reward computation. DRA uses Submodular Mutual Information (SMI) to downweight redundant completions and amplify rewards for diverse ones. This encourages better exploration during learning, while maintaining stable exploitation of high-quality samples. Our method integrates seamlessly with both GRPO and its variant DR.<del>GRPO, resulting in $\textit{DRA-GRPO}$ and $\textit{DGA-DR.</del>GRPO}$. We evaluate our method on five mathematical reasoning benchmarks and find that it outperforms recent strong baselines. It achieves state-of-the-art performance with an average accuracy of 58.2%, using only 7,000 fine-tuning samples and a total training cost of approximately $55. The code is available at <a target="_blank" rel="noopener" href="https://github.com/xiwenc1/DRA-GRPO">https://github.com/xiwenc1/DRA-GRPO</a>. </p>
<blockquote>
<p>å…³äºå¼ºåŒ–å­¦ä¹ åœ¨è¯­é€Ÿæ¨¡å‹åæœŸè®­ç»ƒä¸­çš„åº”ç”¨çš„æœ€æ–°è¿›å±•ï¼Œå¦‚åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å·²åœ¨èµ„æºç¨€ç¼ºçš„ç¯å¢ƒä¸­å±•ç°å‡ºå¹¿é˜”å‰æ™¯ã€‚ç„¶è€Œï¼ŒGRPOé€šå¸¸ä¾èµ–äºè§£å†³æ–¹æ¡ˆçº§åˆ«å’Œæ ‡é‡å¥–åŠ±ä¿¡å·ï¼Œæ— æ³•æ•æ‰åˆ°é‡‡æ ·å®Œæˆä¸­çš„è¯­ä¹‰å¤šæ ·æ€§ã€‚è¿™å¯¼è‡´äº†æˆ‘ä»¬ç§°ä¹‹ä¸ºçš„å¤šæ ·æ€§è´¨é‡ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œä¸åŒçš„æ¨ç†è·¯å¾„å¯èƒ½ä¼šæ”¶åˆ°æ— æ³•åŒºåˆ†çš„å¥–åŠ±ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ã€Šå¤šæ ·æ€§æ„ŸçŸ¥å¥–åŠ±è°ƒæ•´ã€‹ï¼ˆDRAï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†è¯­ä¹‰å¤šæ ·æ€§æ˜ç¡®çº³å…¥å¥–åŠ±è®¡ç®—ä¸­ã€‚DRAä½¿ç”¨å­æ¨¡å—äº’ä¿¡æ¯ï¼ˆSMIï¼‰æ¥é™ä½å†—ä½™å®Œæˆçš„æƒé‡å¹¶æ”¾å¤§å¤šæ ·åŒ–å¥–åŠ±ã€‚è¿™é¼“åŠ±åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­è¿›è¡Œæ›´å¥½çš„æ¢ç´¢ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡æ ·æœ¬çš„ç¨³å®šåˆ©ç”¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸GRPOåŠå…¶å˜ä½“DR.GRPOæ— ç¼é›†æˆï¼Œå½¢æˆDRA-GRPOå’ŒDGA-DR.GRPOã€‚æˆ‘ä»¬åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°å®ƒè¶…è¶Šäº†æœ€è¿‘çš„å¼ºå¤§åŸºçº¿ã€‚ä½¿ç”¨ä»…7000ä¸ªå¾®è°ƒæ ·æœ¬å’Œå¤§çº¦55çš„æ€»è®­ç»ƒæˆæœ¬ï¼Œå®ƒè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡å‡†ç¡®ç‡ä¸º58.2%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xiwenc1/DRA-GRPO">https://github.com/xiwenc1/DRA-GRPO</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09655v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ åœ¨NLPé¢†åŸŸçš„ç ”ç©¶è¿›å±•è¿…é€Ÿï¼Œç‰¹åˆ«æ˜¯ç”¨äºè¯­è¨€æ¨¡å‹å¾®è°ƒé˜¶æ®µçš„æ–¹æ³•å¦‚Group Relative Policy Optimizationï¼ˆGRPOï¼‰ã€‚ç„¶è€Œï¼ŒGRPOå­˜åœ¨è¯­ä¹‰å¤šæ ·æ€§æ•æ‰ä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´ä¸åŒæ¨ç†è·¯å¾„å¯èƒ½è·å¾—ç›¸ä¼¼çš„å¥–åŠ±ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºDiversity-aware Reward Adjustmentï¼ˆDRAï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡Submodular Mutual Informationï¼ˆSMIï¼‰æ¥è°ƒæ•´å¥–åŠ±è®¡ç®—ï¼Œé¼“åŠ±æ¨¡å‹åœ¨æ¢ç´¢è¿‡ç¨‹ä¸­å…³æ³¨å¤šæ ·çš„æ ·æœ¬ã€‚å°†DRAä¸GRPOç»“åˆåæ€§èƒ½æ˜¾è‘—ï¼Œä»£ç å·²å¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼ºåŒ–å­¦ä¹ åœ¨NLPé¢†åŸŸçš„è¯­è¨€æ¨¡å‹å¾®è°ƒé˜¶æ®µæœ‰è¿›å±•ï¼Œç‰¹åˆ«æ˜¯GRPOæ–¹æ³•ã€‚</li>
<li>GRPOä¾èµ–çš„è§£å†³æ–¹æ¡ˆçº§åˆ«å’Œæ ‡é‡å¥–åŠ±ä¿¡å·æ— æ³•æ•è·é‡‡æ ·å®Œæˆçš„è¯­ä¹‰å¤šæ ·æ€§ã€‚</li>
<li>æå‡ºDRAæ–¹æ³•æ¥è§£å†³GRPOçš„å¤šæ ·æ€§è´¨é‡é—®é¢˜ï¼Œé€šè¿‡SMIæ¥ä¸‹æ¢å†—ä½™å®Œæˆå¹¶å¼ºè°ƒå¤šæ ·åŒ–å¥–åŠ±ã€‚</li>
<li>DRAèƒ½æé«˜æ¨¡å‹åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­çš„æ¢ç´¢èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå¯¹é«˜è´¨é‡æ ·æœ¬çš„ç¨³å®šåˆ©ç”¨ã€‚</li>
<li>å°†DRAä¸GRPOç»“åˆååœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå¹³å‡å‡†ç¡®ç‡è¾¾åˆ°äº†58.2%ï¼Œä»…ä½¿ç”¨7000ä¸ªå¾®è°ƒæ ·æœ¬ä¸”æ€»è®­ç»ƒæˆæœ¬çº¦ä¸º55ç¾å…ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09655">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c78f630458ac8499d278507b32b000a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fce0bc65b5684f1e6ec37bb38847ee9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f96327f7cdf358d39257e083a981c1f0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-17/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-17/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-17/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3042f6d7bbcf02d66c340e5b2b696eb2.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-17  Beyond 'Aha!' Toward Systematic Meta-Abilities Alignment in Large   Reasoning Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-16/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fd3a7c52d1e224aebd019375e1753188.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-16  Checking Cheap Talk
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25691.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
