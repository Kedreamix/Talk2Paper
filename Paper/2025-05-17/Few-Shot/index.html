<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-17  Logos as a Well-Tempered Pre-train for Sign Language Recognition">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-565c903d4e24a68c0f082a56d5775e15.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    48 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-17-æ›´æ–°"><a href="#2025-05-17-æ›´æ–°" class="headerlink" title="2025-05-17 æ›´æ–°"></a>2025-05-17 æ›´æ–°</h1><h2 id="Logos-as-a-Well-Tempered-Pre-train-for-Sign-Language-Recognition"><a href="#Logos-as-a-Well-Tempered-Pre-train-for-Sign-Language-Recognition" class="headerlink" title="Logos as a Well-Tempered Pre-train for Sign Language Recognition"></a>Logos as a Well-Tempered Pre-train for Sign Language Recognition</h2><p><strong>Authors:Ilya Ovodov, Petr Surovtsev, Karina Kvanchiani, Alexander Kapitanov, Alexander Nagaev</strong></p>
<p>This paper examines two aspects of the isolated sign language recognition (ISLR) task. First, despite the availability of a number of datasets, the amount of data for most individual sign languages is limited. It poses the challenge of cross-language ISLR model training, including transfer learning. Second, similar signs can have different semantic meanings. It leads to ambiguity in dataset labeling and raises the question of the best policy for annotating such signs. To address these issues, this study presents Logos, a novel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset by the number of signers and one of the largest available datasets while also the largest RSL dataset in size and vocabulary. It is shown that a model, pre-trained on the Logos dataset can be used as a universal encoder for other language SLR tasks, including few-shot learning. We explore cross-language transfer learning approaches and find that joint training using multiple classification heads benefits accuracy for the target lowresource datasets the most. The key feature of the Logos dataset is explicitly annotated visually similar sign groups. We show that explicitly labeling visually similar signs improves trained model quality as a visual encoder for downstream tasks. Based on the proposed contributions, we outperform current state-of-the-art results for the WLASL dataset and get competitive results for the AUTSL dataset, with a single stream model processing solely RGB video. The source code, dataset, and pre-trained models are publicly available. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†å­¤ç«‹æ‰‹è¯­è¯†åˆ«ï¼ˆISLRï¼‰ä»»åŠ¡çš„ä¸¤ä¸ªæ–¹é¢ã€‚é¦–å…ˆï¼Œå°½ç®¡æœ‰è®¸å¤šæ•°æ®é›†å¯ç”¨ï¼Œä½†å¤§å¤šæ•°ä¸ªåˆ«æ‰‹è¯­çš„æ•°æ®é‡æ˜¯æœ‰é™çš„ã€‚è¿™æå‡ºäº†è·¨è¯­è¨€ISLRæ¨¡å‹è®­ç»ƒçš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¿ç§»å­¦ä¹ ã€‚å…¶æ¬¡ï¼Œç›¸ä¼¼çš„ç¬¦å·å¯èƒ½æœ‰ä¸åŒçš„è¯­ä¹‰ã€‚è¿™å¯¼è‡´äº†æ•°æ®é›†æ ‡ç­¾çš„æ¨¡ç³Šæ€§ï¼Œå¹¶å¼•å‘äº†æ ‡æ³¨æ­¤ç±»ç¬¦å·çš„æœ€ä½³ç­–ç•¥çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†Logosï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ä¿„ç½—æ–¯æ‰‹è¯­ï¼ˆRSLï¼‰æ•°æ®é›†ï¼Œå®ƒæ˜¯æŒ‰ç­¾çº¦äººæ•°è®¡ç®—çš„æœ€å¹¿æ³›çš„ISLRæ•°æ®é›†ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯å¯ç”¨æ•°æ®ä¸­è§„æ¨¡æœ€å¤§çš„æ•°æ®é›†ä¹‹ä¸€ï¼ŒåŒæ—¶åœ¨å¤§å°å’Œè¯æ±‡é‡æ–¹é¢ä¹Ÿæ˜¯æœ€å¤§çš„RSLæ•°æ®é›†ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨Logosæ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹å¯ä»¥ç”¨ä½œå…¶ä»–è¯­è¨€SLRä»»åŠ¡ï¼ˆåŒ…æ‹¬å°æ ·å­¦ä¹ ï¼‰çš„é€šç”¨ç¼–ç å™¨ã€‚æˆ‘ä»¬æ¢ç´¢äº†è·¨è¯­è¨€è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œå‘ç°ä½¿ç”¨å¤šä¸ªåˆ†ç±»å¤´è¿›è¡Œè”åˆè®­ç»ƒæœ€æœ‰åˆ©äºç›®æ ‡ä½èµ„æºæ•°æ®é›†çš„å‡†ç¡®æ€§ã€‚Logoæ•°æ®é›†çš„å…³é”®ç‰¹ç‚¹æ˜¯æ˜ç¡®æ³¨é‡Šäº†è§†è§‰ä¸Šç›¸ä¼¼çš„ç¬¦å·ç»„ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæ˜ç¡®æ ‡æ³¨è§†è§‰ä¸Šç›¸ä¼¼çš„ç¬¦å·ä½œä¸ºä¸‹æ¸¸ä»»åŠ¡çš„è§†è§‰ç¼–ç å™¨å¯ä»¥æé«˜è®­ç»ƒæ¨¡å‹çš„è´¨é‡ã€‚åŸºäºæ‰€æå‡ºçš„è´¡çŒ®ï¼Œæˆ‘ä»¬åœ¨WLASLæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨ä½¿ç”¨ä»…å¤„ç†RGBè§†é¢‘çš„å•æµæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œä¸ºAUTSLæ•°æ®é›†è·å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚æºä»£ç ã€æ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹å‡å¯å…¬å¼€è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10481v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å­¤ç«‹æ‰‹è¯­è¯†åˆ«ï¼ˆISLRï¼‰ä»»åŠ¡çš„ä¸¤ä¸ªæ–¹é¢ã€‚é¦–å…ˆï¼Œè™½ç„¶å­˜åœ¨è®¸å¤šæ•°æ®é›†ï¼Œä½†å¤§å¤šæ•°ä¸ªåˆ«æ‰‹è¯­çš„æ•°æ®é‡æœ‰é™ï¼Œè¿™å¸¦æ¥äº†è·¨è¯­è¨€ISLRæ¨¡å‹è®­ç»ƒçš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¿ç§»å­¦ä¹ ã€‚å…¶æ¬¡ï¼Œç›¸ä¼¼çš„æ‰‹åŠ¿å¯èƒ½æœ‰ä¸åŒçš„è¯­ä¹‰å«ä¹‰ï¼Œè¿™å¯¼è‡´äº†æ•°æ®é›†æ ‡ç­¾çš„æ¨¡ç³Šæ€§ï¼Œå¹¶å¼•å‘äº†å¦‚ä½•æ ‡æ³¨æ­¤ç±»æ‰‹åŠ¿çš„æœ€ä½³ç­–ç•¥é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Logosï¼Œä¸€ä¸ªæ–°å‹ä¿„ç½—æ–¯æ‰‹è¯­ï¼ˆRSLï¼‰æ•°æ®é›†ï¼Œå®ƒæ˜¯æŒ‰ç­¾åè€…æ•°é‡è€Œè¨€æœ€å…¨é¢çš„ISLRæ•°æ®é›†ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ç›®å‰å¯ç”¨çš„å¤§å‹æ•°æ®é›†ä¹‹ä¸€ï¼ŒåŒæ—¶åœ¨å¤§å°å’Œè¯æ±‡é‡æ–¹é¢ä¹Ÿæ˜¯æœ€å¤§çš„RSLæ•°æ®é›†ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé¢„è®­ç»ƒåœ¨Logosæ•°æ®é›†ä¸Šçš„æ¨¡å‹å¯ä½œä¸ºå…¶ä»–è¯­è¨€SLRä»»åŠ¡ï¼ˆåŒ…æ‹¬å°æ ·å­¦ä¹ ï¼‰çš„é€šç”¨ç¼–ç å™¨ä½¿ç”¨ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†è·¨è¯­è¨€è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œå‘ç°ä½¿ç”¨å¤šä¸ªåˆ†ç±»å¤´è¿›è¡Œè”åˆè®­ç»ƒæœ€æœ‰åˆ©äºç›®æ ‡ä½èµ„æºæ•°æ®é›†çš„å‡†ç¡®æ€§ã€‚Logosæ•°æ®é›†çš„å…³é”®ç‰¹ç‚¹æ˜¯æ˜¾å¼æ³¨é‡Šçš„è§†è§‰ç›¸ä¼¼ç¬¦å·ç»„ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ˜¾å¼æ ‡æ³¨è§†è§‰ç›¸ä¼¼çš„ç¬¦å·ä½œä¸ºä¸‹æ¸¸ä»»åŠ¡çš„è§†è§‰ç¼–ç å™¨å¯ä»¥æé«˜è®­ç»ƒæ¨¡å‹çš„è´¨é‡ã€‚åŸºäºæ‰€æå‡ºçš„è´¡çŒ®ï¼Œæˆ‘ä»¬åœ¨WLASLæ•°æ®é›†ä¸Šè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æˆæœï¼Œå¯¹äºä»…å¤„ç†RGBè§†é¢‘çš„AUTSLæ•°æ®é›†ä¹Ÿè·å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­¤ç«‹æ‰‹è¯­è¯†åˆ«ï¼ˆISLRï¼‰é¢ä¸´æ•°æ®é™åˆ¶å’Œè·¨è¯­è¨€æ¨¡å‹è®­ç»ƒæŒ‘æˆ˜ã€‚</li>
<li>Logosæ•°æ®é›†æ˜¯ä¿„ç½—æ–¯æ‰‹è¯­ï¼ˆRSLï¼‰æœ€å¤§çš„æ•°æ®é›†ï¼ŒåŒ…å«å¤§é‡ç­¾åè€…å’Œè¯æ±‡ã€‚</li>
<li>é¢„è®­ç»ƒåœ¨Logosæ•°æ®é›†ä¸Šçš„æ¨¡å‹å¯ä½œä¸ºå…¶ä»–è¯­è¨€SLRä»»åŠ¡çš„é€šç”¨ç¼–ç å™¨ã€‚</li>
<li>è·¨è¯­è¨€è¿ç§»å­¦ä¹ æ–¹æ³•ä¸­ï¼Œè”åˆè®­ç»ƒä½¿ç”¨å¤šä¸ªåˆ†ç±»å¤´èƒ½æ˜¾è‘—æé«˜ç›®æ ‡ä½èµ„æºæ•°æ®é›†çš„å‡†ç¡®æ€§ã€‚</li>
<li>Logosæ•°æ®é›†æ˜¾å¼æ ‡æ³¨è§†è§‰ç›¸ä¼¼ç¬¦å·ç»„ï¼Œè¿™æœ‰åŠ©äºæé«˜è®­ç»ƒæ¨¡å‹çš„è´¨é‡ã€‚</li>
<li>è¯¥ç ”ç©¶åœ¨WLASLæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶åœ¨AUTSLæ•°æ®é›†ä¸Šå–å¾—æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9790e3f7e97079f4a90697d0ddc1b288.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fcc44342cda69ca39d7eeee0306893b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aff3fc22b486e181b9c7e1a2ec5a2e55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0e38779a66c63f58581cfcd801d5559.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68658fa3b05572f77e5fdc4a3c53d7ad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ebf4e08d2b1e15ca214038a6923d49b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Comparing-LLM-Text-Annotation-Skills-A-Study-on-Human-Rights-Violations-in-Social-Media-Data"><a href="#Comparing-LLM-Text-Annotation-Skills-A-Study-on-Human-Rights-Violations-in-Social-Media-Data" class="headerlink" title="Comparing LLM Text Annotation Skills: A Study on Human Rights Violations   in Social Media Data"></a>Comparing LLM Text Annotation Skills: A Study on Human Rights Violations   in Social Media Data</h2><p><strong>Authors:Poli Apollinaire Nemkova, Solomon Ubani, Mark V. Albert</strong></p>
<p>In the era of increasingly sophisticated natural language processing (NLP) systems, large language models (LLMs) have demonstrated remarkable potential for diverse applications, including tasks requiring nuanced textual understanding and contextual reasoning. This study investigates the capabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3, Mistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex textual dataset comprising social media posts in Russian and Ukrainian. Specifically, the focus is on the binary classification task of identifying references to human rights violations within the dataset.   To evaluate the effectiveness of these models, their annotations are compared against a gold standard set of human double-annotated labels across 1000 samples. The analysis includes assessing annotation performance under different prompting conditions, with prompts provided in both English and Russian. Additionally, the study explores the unique patterns of errors and disagreements exhibited by each model, offering insights into their strengths, limitations, and cross-linguistic adaptability.   By juxtaposing LLM outputs with human annotations, this research contributes to understanding the reliability and applicability of LLMs for sensitive, domain-specific tasks in multilingual contexts. It also sheds light on how language models handle inherently subjective and context-dependent judgments, a critical consideration for their deployment in real-world scenarios. </p>
<blockquote>
<p>éšç€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ç³»ç»Ÿçš„æ—¥ç›Šæˆç†Ÿï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§åº”ç”¨ä¸­å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼ŒåŒ…æ‹¬éœ€è¦å¾®å¦™æ–‡æœ¬ç†è§£å’Œä¸Šä¸‹æ–‡æ¨ç†çš„ä»»åŠ¡ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨å¤šæ¬¾å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹â€”â€”GPT-3.5ã€GPT-4ã€LLAMA3ã€Mistral 7Bå’ŒClaude-2åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ³¨é‡Šå¤æ‚æ–‡æœ¬æ•°æ®é›†æ–¹é¢çš„èƒ½åŠ›ï¼Œè¯¥æ•°æ®é›†åŒ…å«ä¿„è¯­å’Œä¹Œå…‹å…°è¯­çš„ç¤¾ä¼šåª’ä½“å¸–å­ã€‚å…·ä½“æ¥è¯´ï¼Œå…³æ³¨çš„æ˜¯æ•°æ®é›†ä¸­è¯†åˆ«ä¾µçŠ¯äººæƒå‚è€ƒçš„äºŒå…ƒåˆ†ç±»ä»»åŠ¡ã€‚ä¸ºäº†è¯„ä¼°è¿™äº›æ¨¡å‹çš„æ€§èƒ½ï¼Œå®ƒä»¬çš„æ³¨é‡Šä¸åŸºäº1000ä¸ªæ ·æœ¬çš„äººç±»åŒé‡æ³¨é‡Šæ ‡ç­¾çš„é»„é‡‘æ ‡å‡†é›†è¿›è¡Œäº†æ¯”è¾ƒã€‚åˆ†æåŒ…æ‹¬åœ¨ä¸åŒæç¤ºæ¡ä»¶ä¸‹è¯„ä¼°æ³¨é‡Šæ€§èƒ½ï¼Œæç¤ºä»¥è‹±è¯­å’Œä¿„è¯­ä¸¤ç§è¯­è¨€æä¾›ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¢è®¨äº†æ¯ä¸ªæ¨¡å‹ç‰¹æœ‰çš„é”™è¯¯å’Œåˆ†æ­§æ¨¡å¼ï¼Œæ·±å…¥äº†è§£å®ƒä»¬çš„åŠ›é‡ã€å±€é™æ€§å’Œè·¨è¯­è¨€é€‚åº”æ€§ã€‚é€šè¿‡å°†LLMè¾“å‡ºä¸äººç±»æ³¨é‡Šè¿›è¡Œå¯¹ç…§ï¼Œæœ¬ç ”ç©¶æœ‰åŠ©äºäº†è§£LLMåœ¨å¤šå…ƒè¯­è¨€ç¯å¢ƒä¸­çš„æ•æ„Ÿæ€§å’Œç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡çš„å¯ä¿¡åº¦å’Œé€‚ç”¨æ€§ã€‚å®ƒè¿˜æ­ç¤ºäº†è¯­è¨€æ¨¡å‹å¦‚ä½•å¤„ç†å›ºæœ‰çš„ä¸»è§‚æ€§å’Œä¸Šä¸‹æ–‡ç›¸å…³çš„åˆ¤æ–­ï¼Œè¿™å¯¹äºå®ƒä»¬åœ¨ç°å®åœºæ™¯ä¸­çš„éƒ¨ç½²æ˜¯ä¸€ä¸ªé‡è¦çš„è€ƒé‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10260v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œé’ˆå¯¹åŒ…å«ä¿„ç½—æ–¯å’Œä¹Œå…‹å…°ç¤¾äº¤åª’ä½“å¸–å­çš„å¤æ‚æ–‡æœ¬æ•°æ®é›†ï¼Œè¯„ä¼°äº†å¤šæ¬¾å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ï¼ˆåŒ…æ‹¬GPT-3.5ã€GPT-4ã€LLAMA3ã€Mistral 7Bå’ŒClaude-2ï¼‰è¿›è¡Œé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ ‡æ³¨çš„èƒ½åŠ›ã€‚ç ”ç©¶é‡ç‚¹æ˜¯åœ¨æ•°æ®é›†ä¸­è¯†åˆ«æ¶‰åŠäººæƒä¾µçŠ¯å†…å®¹çš„äºŒè¿›åˆ¶åˆ†ç±»ä»»åŠ¡ã€‚é€šè¿‡å¯¹æ¯”è¿™äº›æ¨¡å‹æ ‡æ³¨ä¸äººç±»åŒé‡æ ‡æ³¨æ ‡ç­¾çš„ç»“æœï¼Œè¯„ä¼°äº†æ¨¡å‹æ€§èƒ½ã€‚ç ”ç©¶è¿˜æ¢è®¨äº†æ¯ä¸ªæ¨¡å‹çš„ç‹¬ç‰¹é”™è¯¯æ¨¡å¼å’Œåˆ†æ­§ï¼Œå¹¶æ¢è®¨äº†æ¨¡å‹çš„ä¼˜ç¼ºç‚¹ä»¥åŠè·¨è¯­è¨€é€‚åº”æ€§ã€‚æœ¬ç ”ç©¶æœ‰åŠ©äºäº†è§£è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„å¯é æ€§åŠåœ¨æ•æ„Ÿé¢†åŸŸçš„é€‚ç”¨æ€§ï¼Œå¹¶ä¸ºè¯­è¨€æ¨¡å‹å¦‚ä½•å¤„ç†ä¸»è§‚æ€§å’Œä¸Šä¸‹æ–‡ä¾èµ–çš„åˆ¤æ–­æä¾›äº†è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶è¯„ä¼°äº†å¤šæ¬¾å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ ‡æ³¨æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶é›†ä¸­åœ¨è¯†åˆ«æ¶‰åŠäººæƒä¾µçŠ¯å†…å®¹çš„äºŒè¿›åˆ¶åˆ†ç±»ä»»åŠ¡ä¸Šã€‚</li>
<li>æ¨¡å‹æ€§èƒ½é€šè¿‡å¯¹æ¯”ä¸äººç±»åŒé‡æ ‡æ³¨æ ‡ç­¾çš„ç»“æœè¿›è¡Œè¯„ä¼°ã€‚</li>
<li>ç ”ç©¶æ¢è®¨äº†æ¯ä¸ªæ¨¡å‹çš„ç‹¬ç‰¹é”™è¯¯æ¨¡å¼å’Œåˆ†æ­§ã€‚</li>
<li>æ¢è®¨äº†æ¨¡å‹çš„ä¼˜ç¼ºç‚¹ä»¥åŠè·¨è¯­è¨€é€‚åº”æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶æœ‰åŠ©äºäº†è§£è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„å¯é æ€§åŠåœ¨æ•æ„Ÿé¢†åŸŸçš„é€‚ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10260">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-521dbabd2f3c52fdeaf4fa5535c9399b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-565c903d4e24a68c0f082a56d5775e15.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MMRL-Parameter-Efficient-and-Interaction-Aware-Representation-Learning-for-Vision-Language-Models"><a href="#MMRL-Parameter-Efficient-and-Interaction-Aware-Representation-Learning-for-Vision-Language-Models" class="headerlink" title="MMRL++: Parameter-Efficient and Interaction-Aware Representation   Learning for Vision-Language Models"></a>MMRL++: Parameter-Efficient and Interaction-Aware Representation   Learning for Vision-Language Models</h2><p><strong>Authors:Yuncheng Guo, Xiaodong Gu</strong></p>
<p>Large-scale pre-trained Vision-Language Models (VLMs) have significantly advanced transfer learning across diverse tasks. However, adapting these models with limited few-shot data often leads to overfitting, undermining their ability to generalize to new tasks. To address this, we propose Multi-Modal Representation Learning (MMRL), which introduces a shared, learnable, modality-agnostic representation space. MMRL generates space tokens projected into both text and image encoders as representation tokens, enabling more effective cross-modal interactions. Unlike prior methods that mainly optimize class token features, MMRL inserts representation tokens into higher encoder layersâ€“where task-specific features are more prominentâ€“while preserving general knowledge in the lower layers. During training, both class and representation features are jointly optimized: a trainable projection layer is applied to representation tokens for task adaptation, while the projection layer for class token remains frozen to retain pre-trained knowledge. To further promote generalization, we introduce a regularization term aligning class and text features with the frozen VLMâ€™s zero-shot features. At inference, a decoupling strategy uses both class and representation features for base tasks, but only class features for novel tasks due to their stronger generalization. Building upon this, we propose MMRL++, a parameter-efficient and interaction-aware extension that significantly reduces trainable parameters and enhances intra-modal interactionsâ€“particularly across the layers of representation tokensâ€“allowing gradient sharing and instance-specific information to propagate more effectively through the network. Extensive experiments on 15 datasets demonstrate that MMRL and MMRL++ consistently outperform state-of-the-art methods, achieving a strong balance between task-specific adaptation and generalization. </p>
<blockquote>
<p>å¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å·²ç»æ˜¾è‘—æ¨åŠ¨äº†ä¸åŒä»»åŠ¡çš„è¿ç§»å­¦ä¹ ã€‚ç„¶è€Œï¼Œç”¨æœ‰é™çš„å°‘é‡æ•°æ®é€‚åº”è¿™äº›æ¨¡å‹å¾€å¾€ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼ŒæŸå®³å…¶æ¨å¹¿åˆ°æ–°ä»»åŠ¡çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ï¼ˆMMRLï¼‰ï¼Œå®ƒå¼•å…¥äº†ä¸€ä¸ªå…±äº«çš„ã€å¯å­¦ä¹ çš„ã€æ¨¡æ€æ— å…³çš„è¡¨ç¤ºç©ºé—´ã€‚MMRLç”Ÿæˆç©ºé—´ä»¤ç‰Œï¼Œå°†å…¶æŠ•å°„åˆ°æ–‡æœ¬å’Œå›¾åƒç¼–ç å™¨ä¸­ä»¥ä½œä¸ºè¡¨ç¤ºä»¤ç‰Œï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„è·¨æ¨¡æ€äº¤äº’ã€‚ä¸ä¸»è¦ä¼˜åŒ–ç±»åˆ«ä»¤ç‰Œç‰¹å¾çš„å‰æœŸæ–¹æ³•ä¸åŒï¼ŒMMRLå°†è¡¨ç¤ºä»¤ç‰Œæ’å…¥åˆ°æ›´é«˜çº§åˆ«çš„ç¼–ç å™¨å±‚ä¸­ï¼ˆä»»åŠ¡ç‰¹å®šç‰¹å¾æ›´ä¸ºçªå‡ºï¼‰ï¼ŒåŒæ—¶åœ¨è¾ƒä½å±‚æ¬¡ä¸Šä¿ç•™ä¸€èˆ¬çŸ¥è¯†ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç±»åˆ«å’Œè¡¨ç¤ºç‰¹å¾ä¼šå…±åŒè¿›è¡Œä¼˜åŒ–ï¼šä¸€ä¸ªå¯è®­ç»ƒçš„æŠ•å½±å±‚åº”ç”¨äºè¡¨ç¤ºä»¤ç‰Œä»¥è¿›è¡Œä»»åŠ¡é€‚é…ï¼Œè€Œç±»åˆ«ä»¤ç‰Œçš„æŠ•å½±å±‚ä¿æŒå†»ç»“ä»¥ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¿ƒè¿›æ³›åŒ–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œå°†ç±»åˆ«å’Œæ–‡æœ¬ç‰¹å¾ä¸å†»ç»“çš„VLMçš„é›¶æ ·æœ¬ç‰¹å¾å¯¹é½ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†è§£è€¦ç­–ç•¥ï¼Œå¯¹äºåŸºç¡€ä»»åŠ¡åŒæ—¶ä½¿ç”¨ç±»åˆ«å’Œè¡¨ç¤ºç‰¹å¾ï¼Œä½†å¯¹äºæ–°å‹ä»»åŠ¡ä»…ä½¿ç”¨ç±»åˆ«ç‰¹å¾ï¼Œå› ä¸ºå®ƒä»¬çš„æ³›åŒ–èƒ½åŠ›æ›´å¼ºã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†MMRL++ï¼Œä¸€ä¸ªå‚æ•°é«˜æ•ˆã€äº¤äº’æ„ŸçŸ¥çš„æ‰©å±•ï¼Œå®ƒæ˜¾è‘—å‡å°‘äº†å¯è®­ç»ƒå‚æ•°ï¼Œå¢å¼ºäº†å†…éƒ¨æ¨¡æ€äº¤äº’â€”â€”ç‰¹åˆ«æ˜¯åœ¨è¡¨ç¤ºä»¤ç‰Œçš„å±‚æ¬¡ä¹‹é—´â€”â€”å…è®¸æ¢¯åº¦å…±äº«å’Œå®ä¾‹ç‰¹å®šä¿¡æ¯æ›´æœ‰æ•ˆåœ°é€šè¿‡ç½‘ç»œä¼ æ’­ã€‚åœ¨15ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMMRLå’ŒMMRL++å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨ä»»åŠ¡ç‰¹å®šé€‚åº”æ€§å’Œæ³›åŒ–ä¹‹é—´å–å¾—äº†å¼ºå¤§çš„å¹³è¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10088v1">PDF</a> Due to the limitation â€œThe abstract field cannot be longer than 1,920   charactersâ€, the abstract appearing here is slightly shorter than that in the   PDF file</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šç§ä»»åŠ¡è¿ç§»å­¦ä¹ ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨æœ‰é™å°æ ·æ•°æ®ä¸‹é€‚åº”è¿™äº›æ¨¡å‹å¸¸å¸¸ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼Œå½±å“å…¶åœ¨æ–°ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ï¼ˆMMRLï¼‰æ–¹æ³•ï¼Œå®ƒå¼•å…¥äº†ä¸€ä¸ªå…±äº«ã€å¯å­¦ä¹ ã€æ¨¡æ€æ— å…³çš„è¡¨ç¤ºç©ºé—´ã€‚MMRLç”Ÿæˆç©ºé—´ä»¤ç‰Œå¹¶æŠ•å½±åˆ°æ–‡æœ¬å’Œå›¾åƒç¼–ç å™¨ä½œä¸ºè¡¨ç¤ºä»¤ç‰Œï¼Œå®ç°æ›´æœ‰æ•ˆçš„è·¨æ¨¡æ€äº¤äº’ã€‚ä¸ä¸»è¦ä¼˜åŒ–ç±»åˆ«ä»¤ç‰Œç‰¹å¾çš„å‰äººæ–¹æ³•ä¸åŒï¼ŒMMRLå°†è¡¨ç¤ºä»¤ç‰Œæ’å…¥åˆ°æ›´çªå‡ºçš„ç¼–ç å™¨å±‚ä¸­ï¼ŒåŒæ—¶åœ¨ä¸‹å±‚ä¿ç•™ä¸€èˆ¬çŸ¥è¯†ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç±»åˆ«å’Œè¡¨ç¤ºç‰¹å¾æ˜¯è”åˆä¼˜åŒ–çš„ï¼šå¯è®­ç»ƒçš„æŠ•å½±å±‚åº”ç”¨äºè¡¨ç¤ºä»¤ç‰Œä»¥è¿›è¡Œä»»åŠ¡é€‚åº”ï¼Œè€Œç±»åˆ«ä»¤ç‰Œçš„æŠ•å½±å±‚ä¿æŒä¸å˜ä»¥ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¿ƒè¿›æ³›åŒ–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œå°†ç±»åˆ«å’Œæ–‡æœ¬ç‰¹å¾ä¸VLMçš„é›¶æ ·æœ¬ç‰¹å¾çš„å†»ç»“å¯¹é½ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨è§£è€¦ç­–ç•¥ï¼Œå¯¹äºåŸºç¡€ä»»åŠ¡åŒæ—¶ä½¿ç”¨ç±»åˆ«å’Œè¡¨ç¤ºç‰¹å¾ï¼Œä½†å¯¹äºæ–°ä»»åŠ¡ä»…ä½¿ç”¨ç±»åˆ«ç‰¹å¾ï¼Œå› ä¸ºå…¶å…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†MMRL++ï¼Œä¸€ä¸ªå‚æ•°é«˜æ•ˆã€äº¤äº’æ„ŸçŸ¥çš„æ‰©å±•ï¼Œæ˜¾è‘—å‡å°‘äº†å¯è®­ç»ƒå‚æ•°ï¼Œå¢å¼ºäº†å†…éƒ¨æ¨¡æ€äº¤äº’ï¼Œç‰¹åˆ«æ˜¯åœ¨è¡¨ç¤ºä»¤ç‰Œçš„å„å±‚ä¹‹é—´ï¼Œå…è®¸æ¢¯åº¦å…±äº«å’Œå®ä¾‹ç‰¹å®šä¿¡æ¯æ›´æœ‰æ•ˆåœ°é€šè¿‡ç½‘ç»œä¼ æ’­ã€‚åœ¨15ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMMRLå’ŒMMRL++å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨ä»»åŠ¡ç‰¹å®šé€‚åº”å’Œæ³›åŒ–ä¹‹é—´å–å¾—äº†å¼ºå¤§çš„å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è¿ç§»å­¦ä¹ ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å°æ ·æ•°æ®ä¸‹é€‚åº”æ˜“å‡ºç°è¿‡åº¦æ‹Ÿåˆã€‚</li>
<li>æå‡ºå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ï¼ˆMMRLï¼‰æ–¹æ³•ï¼Œå¼•å…¥å…±äº«ã€å¯å­¦ä¹ ã€æ¨¡æ€æ— å…³çš„è¡¨ç¤ºç©ºé—´æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>MMRLé€šè¿‡ç”Ÿæˆç©ºé—´ä»¤ç‰Œå¹¶æŠ•å½±åˆ°æ–‡æœ¬å’Œå›¾åƒç¼–ç å™¨ï¼Œä¿ƒè¿›è·¨æ¨¡æ€äº¤äº’ã€‚</li>
<li>MMRLåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è”åˆä¼˜åŒ–ç±»åˆ«å’Œè¡¨ç¤ºç‰¹å¾ï¼ŒåŒæ—¶ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ã€‚</li>
<li>å¼•å…¥æ­£åˆ™åŒ–é¡¹å¯¹é½ç±»åˆ«å’Œæ–‡æœ¬ç‰¹å¾ä¸VLMçš„é›¶æ ·æœ¬ç‰¹å¾ï¼Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨æ¨ç†è¿‡ç¨‹ä¸­é‡‡ç”¨è§£è€¦ç­–ç•¥ï¼ŒåŒºåˆ†åŸºç¡€ä»»åŠ¡å’Œæ–°ä»»åŠ¡çš„ç‰¹å¾ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10088">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-116903ba9a57710934c414b49388678c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4cb32f89106d491ab719e6d9b6185fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f76afb7a73af15ccd58b6ae258697268.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d5c207dd5c2fa7095cfeff262b05a06a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AdaptCLIP-Adapting-CLIP-for-Universal-Visual-Anomaly-Detection"><a href="#AdaptCLIP-Adapting-CLIP-for-Universal-Visual-Anomaly-Detection" class="headerlink" title="AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection"></a>AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection</h2><p><strong>Authors:Bin-Bin Gao, Yue Zhu, Jiangtao Yan, Yuezhi Cai, Weixi Zhang, Meng Wang, Jun Liu, Yong Liu, Lei Wang, Chengjie Wang</strong></p>
<p>Universal visual anomaly detection aims to identify anomalies from novel or unseen vision domains without additional fine-tuning, which is critical in open scenarios. Recent studies have demonstrated that pre-trained vision-language models like CLIP exhibit strong generalization with just zero or a few normal images. However, existing methods struggle with designing prompt templates, complex token interactions, or requiring additional fine-tuning, resulting in limited flexibility. In this work, we present a simple yet effective method called AdaptCLIP based on two key insights. First, adaptive visual and textual representations should be learned alternately rather than jointly. Second, comparative learning between query and normal image prompt should incorporate both contextual and aligned residual features, rather than relying solely on residual features. AdaptCLIP treats CLIP models as a foundational service, adding only three simple adapters, visual adapter, textual adapter, and prompt-query adapter, at its input or output ends. AdaptCLIP supports zero-&#x2F;few-shot generalization across domains and possesses a training-free manner on target domains once trained on a base dataset. AdaptCLIP achieves state-of-the-art performance on 12 anomaly detection benchmarks from industrial and medical domains, significantly outperforming existing competitive methods. We will make the code and model of AdaptCLIP available at <a target="_blank" rel="noopener" href="https://github.com/gaobb/AdaptCLIP">https://github.com/gaobb/AdaptCLIP</a>. </p>
<blockquote>
<p>é€šç”¨è§†è§‰å¼‚å¸¸æ£€æµ‹æ—¨åœ¨ä»æ–°å‹æˆ–æœªè§è¿‡çš„è§†è§‰é¢†åŸŸè¯†åˆ«å¼‚å¸¸å€¼ï¼Œè¿™åœ¨å¼€æ”¾åœºæ™¯ä¸­è‡³å…³é‡è¦ï¼Œè€Œæ— éœ€è¿›è¡Œé¢å¤–çš„å¾®è°ƒã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒåƒCLIPè¿™æ ·çš„é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ä»…é€šè¿‡é›¶å¼ æˆ–å°‘æ•°å‡ å¼ æ­£å¸¸å›¾åƒå°±èƒ½å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨è®¾è®¡æç¤ºæ¨¡æ¿ã€å¤æ‚çš„ä»¤ç‰Œäº¤äº’æˆ–éœ€è¦é¢å¤–çš„å¾®è°ƒæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´çµæ´»æ€§æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œç§°ä¸ºAdaptCLIPï¼Œå®ƒåŸºäºä¸¤ä¸ªå…³é”®è§è§£ã€‚é¦–å…ˆï¼Œåº”è¯¥äº¤æ›¿å­¦ä¹ è‡ªé€‚åº”çš„è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºï¼Œè€Œä¸æ˜¯è”åˆå­¦ä¹ ã€‚å…¶æ¬¡ï¼ŒæŸ¥è¯¢å’Œæ­£å¸¸å›¾åƒæç¤ºä¹‹é—´çš„æ¯”è¾ƒå­¦ä¹ åº”èå…¥ä¸Šä¸‹æ–‡å’Œå¯¹é½çš„æ®‹å·®ç‰¹å¾ï¼Œè€Œä¸æ˜¯ä»…ä¾èµ–æ®‹å·®ç‰¹å¾ã€‚AdaptCLIPå°†CLIPæ¨¡å‹ä½œä¸ºåŸºç¡€æœåŠ¡ï¼Œä»…åœ¨è¾“å…¥æˆ–è¾“å‡ºç«¯æ·»åŠ ä¸‰ä¸ªç®€å•çš„é€‚é…å™¨ï¼Œå³è§†è§‰é€‚é…å™¨ã€æ–‡æœ¬é€‚é…å™¨å’Œæç¤ºæŸ¥è¯¢é€‚é…å™¨ã€‚AdaptCLIPæ”¯æŒè·¨é¢†åŸŸçš„é›¶&#x2F;å°‘é•œå¤´æ³›åŒ–ï¼Œä¸€æ—¦åœ¨åŸºç¡€æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå®ƒåœ¨ç›®æ ‡é¢†åŸŸä¸Šé‡‡ç”¨æ— è®­ç»ƒæ–¹å¼ã€‚AdaptCLIPåœ¨æ¥è‡ªå·¥ä¸šå’ŒåŒ»ç–—é¢†åŸŸçš„12ä¸ªå¼‚å¸¸æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„ç«äº‰æ–¹æ³•ã€‚æˆ‘ä»¬å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/gaobb/AdaptCLIP">https://github.com/gaobb/AdaptCLIP</a>ä¸Šæä¾›AdaptCLIPçš„ä»£ç å’Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09926v1">PDF</a> 27 pages, 15 figures, 22 tables</p>
<p><strong>Summary</strong></p>
<p>åŸºäºCLIPæ¨¡å‹çš„AdaptCLIPæ–¹æ³•å®ç°äº†é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬çš„è§†è§‰å¼‚å¸¸æ£€æµ‹ã€‚å®ƒé€šè¿‡äº¤æ›¿å­¦ä¹ è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºï¼Œç»“åˆä¸Šä¸‹æ–‡å’Œå¯¹é½æ®‹å·®ç‰¹å¾è¿›è¡Œæ¯”å¯¹å­¦ä¹ ï¼Œä»…é€šè¿‡æ·»åŠ ä¸‰ä¸ªç®€å•çš„é€‚é…å™¨å°±å®ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚AdaptCLIPåœ¨å¤šä¸ªå·¥ä¸šå’ŒåŒ»ç–—é¢†åŸŸçš„å¼‚å¸¸æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AdaptCLIPæ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬è§†è§‰å¼‚å¸¸æ£€æµ‹æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•åŸºäºCLIPæ¨¡å‹ï¼Œç»“åˆä¸Šä¸‹æ–‡å’Œå¯¹é½æ®‹å·®ç‰¹å¾è¿›è¡Œå¯¹æ¯”å­¦ä¹ ã€‚</li>
<li>é€šè¿‡äº¤æ›¿å­¦ä¹ è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä»…é€šè¿‡æ·»åŠ ä¸‰ä¸ªé€‚é…å™¨ï¼ˆè§†è§‰é€‚é…å™¨ã€æ–‡æœ¬é€‚é…å™¨å’Œæç¤ºæŸ¥è¯¢é€‚é…å™¨ï¼‰å°±å®ç°äº†å¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>AdaptCLIPåœ¨å¤šä¸ªå·¥ä¸šå’ŒåŒ»ç–—é¢†åŸŸçš„å¼‚å¸¸æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€ä½³è¡¨ç°ã€‚</li>
<li>è¯¥æ–¹æ³•æ”¯æŒè·¨åŸŸæ³›åŒ–ï¼Œå¯¹ç›®æ ‡åŸŸçš„è®­ç»ƒæ˜¯å…è´¹çš„ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09926">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-60e09383b02d028909915d5d5b84418e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1fd26f5b505fd52bcda2cf860d2e8ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a145f33eba3ef29ee50146e66f001838.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ebc40820010eafa9b61124bc505c52d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ae81f72e141180ed2bb3b903f5a660a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Learning-of-Visual-Compositional-Concepts-through-Probabilistic-Schema-Induction"><a href="#Few-Shot-Learning-of-Visual-Compositional-Concepts-through-Probabilistic-Schema-Induction" class="headerlink" title="Few-Shot Learning of Visual Compositional Concepts through Probabilistic   Schema Induction"></a>Few-Shot Learning of Visual Compositional Concepts through Probabilistic   Schema Induction</h2><p><strong>Authors:Andrew Jun Lee, Taylor Webb, Trevor Bihl, Keith Holyoak, Hongjing Lu</strong></p>
<p>The ability to learn new visual concepts from limited examples is a hallmark of human cognition. While traditional category learning models represent each example as an unstructured feature vector, compositional concept learning is thought to depend on (1) structured representations of examples (e.g., directed graphs consisting of objects and their relations) and (2) the identification of shared relational structure across examples through analogical mapping. Here, we introduce Probabilistic Schema Induction (PSI), a prototype model that employs deep learning to perform analogical mapping over structured representations of only a handful of examples, forming a compositional concept called a schema. In doing so, PSI relies on a novel conception of similarity that weighs object-level similarity and relational similarity, as well as a mechanism for amplifying relations relevant to classification, analogous to selective attention parameters in traditional models. We show that PSI produces human-like learning performance and outperforms two controls: a prototype model that uses unstructured feature vectors extracted from a deep learning model, and a variant of PSI with weaker structured representations. Notably, we find that PSIâ€™s human-like performance is driven by an adaptive strategy that increases relational similarity over object-level similarity and upweights the contribution of relations that distinguish classes. These findings suggest that structured representations and analogical mapping are critical to modeling rapid human-like learning of compositional visual concepts, and demonstrate how deep learning can be leveraged to create psychological models. </p>
<blockquote>
<p>å­¦ä¹ æ–°è§†è§‰æ¦‚å¿µçš„èƒ½åŠ›æ˜¯äººç±»è®¤çŸ¥çš„æ ‡å¿—æ€§ç‰¹å¾ä¹‹ä¸€ã€‚ä¼ ç»Ÿçš„ç±»åˆ«å­¦ä¹ æ¨¡å‹å°†æ¯ä¸ªç¤ºä¾‹è¡¨ç¤ºä¸ºæ— ç»“æ„çš„ç‰¹å¾å‘é‡ï¼Œè€Œç»„åˆæ¦‚å¿µå­¦ä¹ åˆ™ä¾èµ–äºï¼ˆ1ï¼‰ç¤ºä¾‹çš„ç»“æ„åŒ–è¡¨ç¤ºï¼ˆä¾‹å¦‚ï¼Œç”±å¯¹è±¡åŠå…¶å…³ç³»ç»„æˆçš„å®šå‘å›¾ï¼‰å’Œï¼ˆ2ï¼‰é€šè¿‡ç±»æ¯”æ˜ å°„è¯†åˆ«è·¨ç¤ºä¾‹çš„å…±äº«å…³ç³»ç»“æ„ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¦‚ç‡æ¨¡å¼å½’çº³ï¼ˆPSIï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸå‹æ¨¡å‹ï¼Œå®ƒåˆ©ç”¨æ·±åº¦å­¦ä¹ ä»…é€šè¿‡å°‘é‡ç¤ºä¾‹çš„ç»“æ„åŒ–è¡¨ç¤ºè¿›è¡Œç±»æ¯”æ˜ å°„ï¼Œå½¢æˆç§°ä¸ºæ¨¡å¼çš„ç»„åˆæ¦‚å¿µã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒPSIä¾èµ–äºä¸€ç§æ–°å‹ç›¸ä¼¼æ€§æ¦‚å¿µï¼Œè¯¥æ¦‚å¿µæƒè¡¡å¯¹è±¡çº§ç›¸ä¼¼æ€§å’Œå…³ç³»ç›¸ä¼¼æ€§ï¼Œä»¥åŠä¸€ç§ç±»ä¼¼äºä¼ ç»Ÿæ¨¡å‹ä¸­çš„é€‰æ‹©æ€§æ³¨æ„å‚æ•°çš„æ”¾å¤§ä¸åˆ†ç±»ç›¸å…³çš„å…³ç³»çš„æœºåˆ¶ã€‚æˆ‘ä»¬è¡¨æ˜PSIèƒ½äº§ç”Ÿä¸äººç±»ç±»ä¼¼çš„å­¦ä¹ æ€§èƒ½ï¼Œå¹¶ä¼˜äºä¸¤ä¸ªå¯¹ç…§ç»„ï¼šä¸€ä¸ªä½¿ç”¨ä»æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­æå–çš„æ— ç»“æ„ç‰¹å¾å‘é‡çš„åŸå‹æ¨¡å‹ï¼Œä»¥åŠä¸€ä¸ªå…·æœ‰è¾ƒå¼±ç»“æ„åŒ–è¡¨ç¤ºçš„PSIå˜ä½“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°PSIçš„äººç±»åŒ–æ€§èƒ½æ˜¯ç”±ä¸€ç§è‡ªé€‚åº”ç­–ç•¥é©±åŠ¨çš„ï¼Œè¯¥ç­–ç•¥å¢åŠ äº†å…³ç³»ç›¸ä¼¼æ€§å¹¶è¶…è¶Šäº†å¯¹è±¡çº§åˆ«çš„ç›¸ä¼¼æ€§ï¼Œå¹¶å¢åŠ äº†åŒºåˆ†ç±»åˆ«çš„å…³ç³»çš„è´¡çŒ®ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œç»“æ„åŒ–è¡¨ç¤ºå’Œç±»æ¯”æ˜ å°„å¯¹äºæ¨¡æ‹Ÿäººç±»å¿«é€Ÿå­¦ä¹ ç»„åˆè§†è§‰æ¦‚å¿µè‡³å…³é‡è¦ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨æ·±åº¦å­¦ä¹ æ¥åˆ›å»ºå¿ƒç†æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09859v1">PDF</a> Lee, A. J., Webb, T., Bihl, T., Holyoak, K. J., &amp; Lu, H. (2025).   Few-shot learning of visual compositional concepts through probabilistic   schema induction. In A. Ruggeri, D. Barner, C. Walker, &amp; N. Bramley (Eds.),   Proceedings of the 47th Annual Conference of the Cognitive Science Society.   Cognitive Science Society</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ¦‚ç‡æ¨¡å¼å½’çº³ï¼ˆPSIï¼‰è¿™ä¸€åŸå‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨æ·±åº¦å­¦ä¹ å¯¹æœ‰é™çš„ç»“æ„åŒ–è¡¨ç¤ºç¤ºä¾‹è¿›è¡Œç±»æ¯”æ˜ å°„ï¼Œå½¢æˆç§°ä¸ºæ¨¡å¼çš„ç»„åˆæ¦‚å¿µã€‚PSIä¾èµ–äºæ–°çš„ç›¸ä¼¼æ€§æ¦‚å¿µï¼Œæ—¢è€ƒè™‘å¯¹è±¡å±‚é¢çš„ç›¸ä¼¼æ€§ï¼Œåˆè€ƒè™‘å…³ç³»å±‚é¢çš„ç›¸ä¼¼æ€§ï¼ŒåŒæ—¶æä¾›ä¸€ç§æ”¾å¤§ä¸åˆ†ç±»ç›¸å…³çš„å…³ç³»çš„æœºåˆ¶ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒPSIå®ç°äº†ä¸äººç±»ç±»ä¼¼çš„å­¦ä¹ æ€§èƒ½ï¼Œå¹¶ä¼˜äºä½¿ç”¨æ·±åº¦å­¦ä¹ çš„æ— ç»“æ„åŒ–ç‰¹å¾å‘é‡çš„åŸå‹æ¨¡å‹å’ŒPSIçš„å¼±ç»“æ„åŒ–è¡¨ç¤ºå˜ä½“ã€‚è¿™è¡¨æ˜ç»“æ„åŒ–è¡¨ç¤ºå’Œç±»æ¯”æ˜ å°„å¯¹äºæ¨¡æ‹Ÿäººç±»å¿«é€Ÿå­¦ä¹ ç»„åˆè§†è§‰æ¦‚å¿µè‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PSIæ¨¡å‹åˆ©ç”¨æ·±åº¦å­¦ä¹ è¿›è¡Œç±»æ¯”æ˜ å°„ï¼Œå¤„ç†æœ‰é™çš„ç»“æ„åŒ–ç¤ºä¾‹è¡¨ç¤ºã€‚</li>
<li>PSIä¾èµ–äºå¯¹è±¡å±‚é¢å’Œå…³ç³»å±‚é¢çš„ç›¸ä¼¼æ€§åº¦é‡ã€‚</li>
<li>PSIæ¨¡å‹é‡‡ç”¨ä¸€ç§æ”¾å¤§ä¸åˆ†ç±»ç›¸å…³å…³ç³»çš„æœºåˆ¶ï¼Œç±»ä¼¼äºä¼ ç»Ÿæ¨¡å‹ä¸­çš„é€‰æ‹©æ€§æ³¨æ„å‚æ•°ã€‚</li>
<li>PSIå®ç°äº†ä¸äººç±»ç±»ä¼¼çš„å­¦ä¹ æ€§èƒ½ã€‚</li>
<li>PSIæ¨¡å‹ä¼˜äºä½¿ç”¨æ— ç»“æ„åŒ–ç‰¹å¾å‘é‡çš„åŸå‹æ¨¡å‹å’Œå¼±ç»“æ„åŒ–è¡¨ç¤ºçš„PSIå˜ä½“ã€‚</li>
<li>ç»“æ„åŒ–è¡¨ç¤ºå’Œç±»æ¯”æ˜ å°„å¯¹äºæ¨¡æ‹Ÿäººç±»å¿«é€Ÿå­¦ä¹ ç»„åˆè§†è§‰æ¦‚å¿µè‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09859">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-84a2c22b7ab9f185ef5611964141bc90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47c69c75f05c62ae26622af98868f922.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3fb96918e8768d7a0080e3110febf680.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d91ebda9d31d1bd613b7a0ed8c2263f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1a9dcbcd686b6e91f0548397f03ba0c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Fast-and-Robust-Task-Sampling-with-Posterior-and-Diversity-Synergies-for-Adaptive-Decision-Makers-in-Randomized-Environments"><a href="#Fast-and-Robust-Task-Sampling-with-Posterior-and-Diversity-Synergies-for-Adaptive-Decision-Makers-in-Randomized-Environments" class="headerlink" title="Fast and Robust: Task Sampling with Posterior and Diversity Synergies   for Adaptive Decision-Makers in Randomized Environments"></a>Fast and Robust: Task Sampling with Posterior and Diversity Synergies   for Adaptive Decision-Makers in Randomized Environments</h2><p><strong>Authors:Yun Qu, Qi Cheems Wang, Yixiu Mao, Yiqin Lv, Xiangyang Ji</strong></p>
<p>Task robust adaptation is a long-standing pursuit in sequential decision-making. Some risk-averse strategies, e.g., the conditional value-at-risk principle, are incorporated in domain randomization or meta reinforcement learning to prioritize difficult tasks in optimization, which demand costly intensive evaluations. The efficiency issue prompts the development of robust active task sampling to train adaptive policies, where risk-predictive models are used to surrogate policy evaluation. This work characterizes the optimization pipeline of robust active task sampling as a Markov decision process, posits theoretical and practical insights, and constitutes robustness concepts in risk-averse scenarios. Importantly, we propose an easy-to-implement method, referred to as Posterior and Diversity Synergized Task Sampling (PDTS), to accommodate fast and robust sequential decision-making. Extensive experiments show that PDTS unlocks the potential of robust active task sampling, significantly improves the zero-shot and few-shot adaptation robustness in challenging tasks, and even accelerates the learning process under certain scenarios. Our project website is at <a target="_blank" rel="noopener" href="https://thu-rllab.github.io/PDTS_project_page">https://thu-rllab.github.io/PDTS_project_page</a>. </p>
<blockquote>
<p>ä»»åŠ¡é²æ£’æ€§é€‚åº”æ˜¯åºåˆ—å†³ç­–åˆ¶å®šä¸­ä¸€ä¸ªé•¿æœŸè¿½æ±‚çš„ç›®æ ‡ã€‚ä¸€äº›é£é™©è§„é¿ç­–ç•¥ï¼Œä¾‹å¦‚æ¡ä»¶é£é™©ä»·å€¼åŸåˆ™ï¼Œè¢«çº³å…¥é¢†åŸŸéšæœºåŒ–æˆ–å…ƒå¼ºåŒ–å­¦ä¹ ä¸­ï¼Œä»¥ä¼˜åŒ–ä¸­çš„ä¼˜å…ˆå›°éš¾ä»»åŠ¡ä¸ºç›®æ ‡ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦è¿›è¡Œæ˜‚è´µçš„å¯†é›†è¯„ä¼°ã€‚æ•ˆç‡é—®é¢˜ä¿ƒä½¿å¼€å‘é²æ£’çš„ä¸»åŠ¨ä»»åŠ¡é‡‡æ ·ä»¥è®­ç»ƒè‡ªé€‚åº”ç­–ç•¥ï¼Œå…¶ä¸­é£é™©é¢„æµ‹æ¨¡å‹ç”¨äºæ›¿ä»£ç­–ç•¥è¯„ä¼°ã€‚è¿™é¡¹å·¥ä½œå°†é²æ£’çš„ä¸»åŠ¨ä»»åŠ¡é‡‡æ ·çš„ä¼˜åŒ–ç®¡é“ç‰¹å¾åŒ–ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œæå‡ºäº†ç†è®ºå’Œå®è·µè§è§£ï¼Œå¹¶æ„æˆäº†é£é™©è§„é¿åœºæ™¯ä¸­çš„ç¨³å¥æ€§æ¦‚å¿µã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ˜“äºå®æ–½çš„æ–¹æ³•ï¼Œç§°ä¸ºåéªŒå’Œå¤šæ ·æ€§ååŒä»»åŠ¡é‡‡æ ·ï¼ˆPDTSï¼‰ï¼Œä»¥å®ç°å¿«é€Ÿå’Œç¨³å¥çš„åºåˆ—å†³ç­–åˆ¶å®šã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPDTSé‡Šæ”¾äº†é²æ£’çš„ä¸»åŠ¨ä»»åŠ¡é‡‡æ ·çš„æ½œåŠ›ï¼Œæ˜¾è‘—æé«˜äº†æŒ‘æˆ˜æ€§ä»»åŠ¡çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬é€‚åº”ç¨³å¥æ€§ï¼Œç”šè‡³åœ¨ç‰¹å®šåœºæ™¯ä¸‹åŠ é€Ÿäº†å­¦ä¹ è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„é¡¹ç›®ç½‘ç«™æ˜¯<a target="_blank" rel="noopener" href="https://thu-rllab.github.io/PDTS_project_page%E3%80%82">https://thu-rllab.github.io/PDTS_project_pageã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19139v3">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>é£é™©è§„é¿ç­–ç•¥åœ¨å†³ç­–åºåˆ—ä¸­è¿›è¡Œäº†é•¿æœŸçš„é€‚åº”æ€§æ”¹é€ ï¼Œé¢ä¸´æŒ‘æˆ˜å’Œæˆæœ¬é«˜æ˜‚çš„ä»»åŠ¡éœ€æ±‚è¯„ä¼°ã€‚æœ¬æ–‡ä»‹ç»äº†é€šè¿‡å°†é£é™©è§„é¿ç­–ç•¥çº³å…¥é¢†åŸŸéšæœºåŒ–æˆ–å…ƒå¼ºåŒ–å­¦ä¹ æ¥è§£å†³æ•ˆç‡é—®é¢˜çš„æ–¹æ³•ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œå¼•å…¥äº†é£é™©é¢„æµ‹æ¨¡å‹æ›¿ä»£æ”¿ç­–è¯„ä¼°ã€‚è¿™é¡¹ç ”ç©¶å¯¹ä¼˜åŒ–æµç¨‹è¿›è¡Œäº†ç‰¹å¾åˆ»ç”»å¹¶æå‡ºäº†ç†è®ºä¸å®è·µçš„è§è§£ã€‚é€šè¿‡å¼€å‘æ˜“äºå®æ–½çš„åä¸ºPDTSï¼ˆåéªŒä¸å¤šæ ·æ€§ååŒä»»åŠ¡é‡‡æ ·ï¼‰çš„æ–¹æ³•ï¼Œå®ç°äº†å¿«é€Ÿè€Œç¨³å¥çš„å†³ç­–åºåˆ—ä¼˜åŒ–ã€‚å®éªŒè¯æ˜ï¼ŒPDTSå¯ä»¥æ˜¾è‘—æé«˜åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸­çš„é›¶é•œé€‚åº”æ€§å’Œå°‘æ•°æ¬¡é€‚åº”æ€§é€‚åº”æ€§ç¨³å¥æ€§ï¼Œç”šè‡³å¯ä»¥åœ¨æŸäº›åœºæ™¯ä¸‹åŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ã€‚ç›¸å…³é¡¹ç›®ç½‘å€ï¼š<a target="_blank" rel="noopener" href="https://thu-rllab.github.io/PDTS_project_page">https://thu-rllab.github.io/PDTS_project_page</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é£é™©è§„é¿ç­–ç•¥è¢«çº³å…¥é¢†åŸŸéšæœºåŒ–æˆ–å…ƒå¼ºåŒ–å­¦ä¹ ä¸­ä»¥ä¼˜åŒ–å†³ç­–åºåˆ—çš„é€‚åº”æ€§æ”¹é€ è¿‡ç¨‹ã€‚</li>
<li>æ•ˆç‡é—®é¢˜ä¿ƒä½¿å‘å±•ç¨³å¥ä¸»åŠ¨ä»»åŠ¡é‡‡æ ·ï¼Œä»¥è®­ç»ƒè‡ªé€‚åº”ç­–ç•¥ã€‚</li>
<li>é£é™©é¢„æµ‹æ¨¡å‹ç”¨äºæ›¿ä»£æ”¿ç­–è¯„ä¼°ä»¥æé«˜å†³ç­–æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>PDTSæ–¹æ³•ç»“åˆäº†åéªŒä¸å¤šæ ·æ€§ååŒé‡‡æ ·æŠ€æœ¯æ¥è§£å†³ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>PDTSèƒ½æ˜¾è‘—æé«˜é›¶é•œå’Œå°‘æ•°æ¬¡é€‚åº”æ€§ç¨³å¥æ€§ã€‚å®éªŒè¯æ˜äº†å…¶åœ¨è§£å†³å¤æ‚ä»»åŠ¡æ–¹é¢çš„ä¼˜åŠ¿ã€‚</li>
<li>PDTSæ–¹æ³•æœ‰åŠ©äºåŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ï¼Œç‰¹åˆ«æ˜¯åœ¨æŸäº›ç‰¹å®šåœºæ™¯ä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19139">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6614cf529a72929e9e640d8ceb4487ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aab43ef5a066d8757d51d2cc221f681a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcc0722a0fe0f77bfa4f2b184ac1626c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0cfb217c501ef01371dcf018e602f2ee.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Generative-AI-for-Scoring-Medical-Student-Interviews-in-Objective-Structured-Clinical-Examinations-OSCEs"><a href="#Benchmarking-Generative-AI-for-Scoring-Medical-Student-Interviews-in-Objective-Structured-Clinical-Examinations-OSCEs" class="headerlink" title="Benchmarking Generative AI for Scoring Medical Student Interviews in   Objective Structured Clinical Examinations (OSCEs)"></a>Benchmarking Generative AI for Scoring Medical Student Interviews in   Objective Structured Clinical Examinations (OSCEs)</h2><p><strong>Authors:Jadon Geathers, Yann Hicke, Colleen Chan, Niroop Rajashekar, Justin Sewell, Susannah Cornes, Rene F. Kizilcec, Dennis Shung</strong></p>
<p>Objective Structured Clinical Examinations (OSCEs) are widely used to assess medical studentsâ€™ communication skills, but scoring interview-based assessments is time-consuming and potentially subject to human bias. This study explored the potential of large language models (LLMs) to automate OSCE evaluations using the Master Interview Rating Scale (MIRS). We compared the performance of four state-of-the-art LLMs (GPT-4o, Claude 3.5, Llama 3.1, and Gemini 1.5 Pro) in evaluating OSCE transcripts across all 28 items of the MIRS under the conditions of zero-shot, chain-of-thought (CoT), few-shot, and multi-step prompting. The models were benchmarked against a dataset of 10 OSCE cases with 174 expert consensus scores available. Model performance was measured using three accuracy metrics (exact, off-by-one, thresholded). Averaging across all MIRS items and OSCE cases, LLMs performed with low exact accuracy (0.27 to 0.44), and moderate to high off-by-one accuracy (0.67 to 0.87) and thresholded accuracy (0.75 to 0.88). A zero temperature parameter ensured high intra-rater reliability ({\alpha} &#x3D; 0.98 for GPT-4o). CoT, few-shot, and multi-step techniques proved valuable when tailored to specific assessment items. The performance was consistent across MIRS items, independent of encounter phases and communication domains. We demonstrated the feasibility of AI-assisted OSCE evaluation and provided benchmarking of multiple LLMs across multiple prompt techniques. Our work provides a baseline performance assessment for LLMs that lays a foundation for future research into automated assessment of clinical communication skills. </p>
<blockquote>
<p>å®¢è§‚ç»“æ„åŒ–ä¸´åºŠè€ƒè¯•ï¼ˆOSCEsï¼‰è¢«å¹¿æ³›ç”¨äºè¯„ä¼°åŒ»å­¦ç”Ÿçš„æ²Ÿé€šæŠ€å·§ï¼Œä½†åŸºäºé¢è¯•çš„è¯„ä¼°æ‰“åˆ†è€—æ—¶ä¸”å­˜åœ¨äººä¸ºåè§ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½¿ç”¨ä¸»é¢è¯•è¯„åˆ†è¡¨ï¼ˆMIRSï¼‰è‡ªåŠ¨è¿›è¡ŒOSCEè¯„ä¼°çš„æ½œåŠ›ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†å››ç§æœ€æ–°LLMsï¼ˆGPT-4oã€Claude 3.5ã€Llama 3.1å’ŒGemini 1.5 Proï¼‰åœ¨è¯„ä¼°OSCEè®°å½•æ—¶çš„è¡¨ç°ï¼Œæ¶µç›–äº†MIRSçš„æ‰€æœ‰28é¡¹å†…å®¹ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€æ€ç»´é“¾ï¼ˆCoTï¼‰ã€å°æ ·æœ¬å’Œå¤šæ­¥æç¤ºçš„æƒ…å†µã€‚è¿™äº›æ¨¡å‹æ˜¯ä»¥10ä¸ªOSCEç—…ä¾‹æ•°æ®é›†ä¸ºåŸºå‡†ï¼Œå…±æœ‰174ä¸ªä¸“å®¶å…±è¯†åˆ†æ•°å¯ä¾›ä½¿ç”¨ã€‚æ¨¡å‹æ€§èƒ½é‡‡ç”¨ä¸‰ç§å‡†ç¡®æ€§æŒ‡æ ‡ï¼ˆç²¾ç¡®ã€ç¦»ä¸€ã€é˜ˆå€¼ï¼‰è¿›è¡Œæµ‹é‡ã€‚åœ¨æ‰€æœ‰MIRSé¡¹ç›®å’ŒOSCEç—…ä¾‹ä¸Šçš„å¹³å‡ç²¾ç¡®æ€§è¾ƒä½ï¼ˆ0.27è‡³0.44ï¼‰ï¼Œç¦»ä¸€å‡†ç¡®ç‡å’Œé˜ˆå€¼å‡†ç¡®ç‡åˆ™å¤„äºä¸­ç­‰è‡³é«˜æ°´å¹³ï¼ˆåˆ†åˆ«ä¸º0.67è‡³0.87å’Œ0.75è‡³0.88ï¼‰ã€‚é›¶æ¸©åº¦å‚æ•°ç¡®ä¿äº†é«˜çš„å†…éƒ¨ä¸€è‡´æ€§ï¼ˆGPT-4oçš„Î±&#x3D;0.98ï¼‰ã€‚é’ˆå¯¹ç‰¹å®šçš„è¯„ä¼°é¡¹ç›®ï¼Œæ€ç»´é“¾ã€å°æ ·æœ¬å’Œå¤šæ­¥æŠ€æœ¯è¯æ˜æ˜¯æœ‰ä»·å€¼çš„ã€‚æ€§èƒ½åœ¨MIRSé¡¹ç›®ä¸Šè¡¨ç°ä¸€è‡´ï¼Œä¸å—é‡åˆ°é˜¶æ®µå’Œæ²Ÿé€šé¢†åŸŸçš„å½±å“ã€‚æˆ‘ä»¬è¯æ˜äº†äººå·¥æ™ºèƒ½è¾…åŠ©OSCEè¯„ä¼°çš„å¯è¡Œæ€§ï¼Œå¹¶æä¾›äº†å¤šç§LLMsåœ¨å¤šç§æç¤ºæŠ€æœ¯ä¸Šçš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºLLMsæä¾›äº†åŸºçº¿æ€§èƒ½è¯„ä¼°ï¼Œä¸ºæœªæ¥ç ”ç©¶è‡ªåŠ¨åŒ–è¯„ä¼°ä¸´åºŠæ²Ÿé€šæŠ€å·§å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13957v2">PDF</a> 12 pages + 3 pages of references, 4 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æ¢ç´¢äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åˆ©ç”¨â€œå¤§å¸ˆé¢è¯•è¯„åˆ†é‡è¡¨â€ï¼ˆMIRSï¼‰è‡ªåŠ¨è¯„ä¼°ä¸´åºŠæ²Ÿé€šæŠ€èƒ½è€ƒè¯•ï¼ˆOSCEsï¼‰æ–¹é¢çš„æ½œåŠ›ã€‚å¯¹æ¯”äº†å››æ¬¾å…ˆè¿›LLMåœ¨MIRSçš„28ä¸ªé¡¹ç›®ä¸­å¯¹OSCEæ–‡ç¨¿çš„é›¶æ ·æœ¬ã€æ€ç»´é“¾ï¼ˆCoTï¼‰ã€å°‘æ ·æœ¬å’Œå¤šæ­¥æç¤ºä¸‹çš„è¯„ä¼°è¡¨ç°ã€‚ä»¥åŒ…å«174ä¸ªä¸“å®¶å…±è¯†åˆ†æ•°çš„æ•°æ®é›†ä¸ºåŸºå‡†ï¼Œè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚LLMsçš„å¹³å‡è¡¨ç°æ˜¾ç¤ºï¼Œåœ¨ç²¾ç¡®å‡†ç¡®æ€§æ–¹é¢è¡¨ç°è¾ƒä½ï¼ˆ0.27è‡³0.44ï¼‰ï¼Œä½†åœ¨å®½æ¾å‡†ç¡®ç‡å’Œé˜ˆå€¼å‡†ç¡®ç‡æ–¹é¢è¡¨ç°ä¸­åº¦è‡³é«˜åº¦ï¼ˆ0.67è‡³0.87ï¼‰ã€‚é›¶æ¸©åº¦å‚æ•°ç¡®ä¿äº†é«˜åº¦çš„å†…éƒ¨ä¸€è‡´æ€§ï¼ˆGPT-4oçš„Î±&#x3D; 0.98ï¼‰ã€‚é’ˆå¯¹ç‰¹å®šçš„è¯„ä¼°é¡¹ç›®ï¼Œæ€ç»´é“¾ã€å°‘æ ·æœ¬å’Œå¤šæ­¥æŠ€æœ¯è¯æ˜æ˜¯æœ‰ä»·å€¼çš„ã€‚æ€§èƒ½åœ¨MIRSé¡¹ç›®ä¸­è¡¨ç°ä¸€è‡´ï¼Œä¸å—é‡åˆ°é˜¶æ®µå’Œæ²Ÿé€šé¢†åŸŸçš„å½±å“ã€‚æœ¬ç ”ç©¶è¯æ˜äº†AIè¾…åŠ©OSCEè¯„ä¼°çš„å¯è¡Œæ€§ï¼Œå¹¶ä¸ºå¤šç§LLMsåœ¨å¤šç§æç¤ºæŠ€æœ¯ä¸Šçš„è¡¨ç°æä¾›äº†åŸºå‡†æµ‹è¯•ã€‚æœ¬ç ”ç©¶ä¸ºæœªæ¥ä¸´åºŠæ²Ÿé€šæŠ€èƒ½è‡ªåŠ¨åŒ–è¯„ä¼°çš„ç ”ç©¶æä¾›äº†åŸºç¡€æ€§èƒ½è¯„ä¼°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶æ¢ç´¢äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨è¯„ä¼°åŒ»ç–—ä¸´åºŠæ²Ÿé€šæŠ€èƒ½è€ƒè¯•ï¼ˆOSCEsï¼‰æ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>å¯¹æ¯”äº†å››æ¬¾å…ˆè¿›LLMåœ¨å¤šç§æç¤ºæŠ€æœ¯ä¸‹çš„è¯„ä¼°è¡¨ç°ã€‚</li>
<li>LLMsåœ¨ç²¾ç¡®å‡†ç¡®æ€§æ–¹é¢è¡¨ç°è¾ƒä½ï¼Œä½†åœ¨å®½æ¾å‡†ç¡®ç‡å’Œé˜ˆå€¼å‡†ç¡®ç‡æ–¹é¢è¡¨ç°è¾ƒå¥½ã€‚</li>
<li>é›¶æ¸©åº¦å‚æ•°ç¡®ä¿äº†é«˜åº¦çš„å†…éƒ¨ä¸€è‡´æ€§ã€‚</li>
<li>æ€ç»´é“¾ã€å°‘æ ·æœ¬å’Œå¤šæ­¥æŠ€æœ¯å¯¹äºç‰¹å®šè¯„ä¼°é¡¹ç›®å…·æœ‰ä»·å€¼ã€‚</li>
<li>LLMsçš„æ€§èƒ½åœ¨MIRSé¡¹ç›®ä¸­è¡¨ç°ä¸€è‡´ï¼Œä¸å—é‡åˆ°é˜¶æ®µå’Œæ²Ÿé€šé¢†åŸŸçš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ca6c052861388ba8c775f57591bb3430.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37d999fe25a9b28cbf80444b6a8d9e3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be5bdb139597be05965c655ef571aa72.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FitCF-A-Framework-for-Automatic-Feature-Importance-guided-Counterfactual-Example-Generation"><a href="#FitCF-A-Framework-for-Automatic-Feature-Importance-guided-Counterfactual-Example-Generation" class="headerlink" title="FitCF: A Framework for Automatic Feature Importance-guided   Counterfactual Example Generation"></a>FitCF: A Framework for Automatic Feature Importance-guided   Counterfactual Example Generation</h2><p><strong>Authors:Qianli Wang, Nils Feldhus, Simon Ostermann, Luis Felipe Villa-Arenas, Sebastian MÃ¶ller, Vera Schmitt</strong></p>
<p>Counterfactual examples are widely used in natural language processing (NLP) as valuable data to improve models, and in explainable artificial intelligence (XAI) to understand model behavior. The automated generation of counterfactual examples remains a challenging task even for large language models (LLMs), despite their impressive performance on many tasks. In this paper, we first introduce ZeroCF, a faithful approach for leveraging important words derived from feature attribution methods to generate counterfactual examples in a zero-shot setting. Second, we present a new framework, FitCF, which further verifies aforementioned counterfactuals by label flip verification and then inserts them as demonstrations for few-shot prompting, outperforming two state-of-the-art baselines. Through ablation studies, we identify the importance of each of FitCFâ€™s core components in improving the quality of counterfactuals, as assessed through flip rate, perplexity, and similarity measures. Furthermore, we show the effectiveness of LIME and Integrated Gradients as backbone attribution methods for FitCF and find that the number of demonstrations has the largest effect on performance. Finally, we reveal a strong correlation between the faithfulness of feature attribution scores and the quality of generated counterfactuals. </p>
<blockquote>
<p>åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­ï¼Œåäº‹å®ä¾‹å­è¢«å¹¿æ³›åº”ç”¨äºæå‡æ¨¡å‹ä»·å€¼çš„æ•°æ®ï¼Œå¹¶ä¸”åœ¨å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰ä¸­ç”¨äºç†è§£æ¨¡å‹è¡Œä¸ºã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šä»»åŠ¡ä¸Šè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†è‡ªåŠ¨ç”Ÿæˆåäº‹å®ä¾‹å­ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆä»‹ç»äº†ZeroCFï¼Œè¿™æ˜¯ä¸€ç§å¿ å®çš„æ–¹æ³•ï¼Œåˆ©ç”¨ä»ç‰¹å¾å½’å› æ–¹æ³•æ´¾ç”Ÿå‡ºçš„é‡è¦å•è¯ï¼Œåœ¨æ— å°„å‡»ç¯å¢ƒä¸­ç”Ÿæˆåäº‹å®ä¾‹å­ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶FitCFï¼Œå®ƒé€šè¿‡æ ‡ç­¾ç¿»è½¬éªŒè¯è¿›ä¸€æ­¥éªŒè¯äº†ä¸Šè¿°åäº‹å®ï¼Œç„¶åå°†å®ƒä»¬ä½œä¸ºæ¼”ç¤ºç”¨äºå°æ ·æœ¬æç¤ºï¼Œè¡¨ç°ä¼˜äºä¸¤ç§æœ€æ–°åŸºçº¿ã€‚é€šè¿‡åºŸé™¤ç ”ç©¶ï¼Œæˆ‘ä»¬ç¡®å®šäº†FitCFæ¯ä¸ªæ ¸å¿ƒç»„ä»¶åœ¨æé«˜åäº‹å®è´¨é‡æ–¹é¢çš„é‡è¦æ€§ï¼Œé€šè¿‡ç¿»è½¬ç‡ã€å›°æƒ‘åº¦å’Œç›¸ä¼¼æ€§åº¦é‡è¿›è¡Œè¯„ä¼°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†LIMEå’Œé›†æˆæ¢¯åº¦ä½œä¸ºFitCFçš„éª¨å¹²å½’å› æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å‘ç°æ¼”ç¤ºçš„æ•°é‡å¯¹æ€§èƒ½çš„å½±å“æœ€å¤§ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°ç‰¹å¾å½’å› åˆ†æ•°çš„å¿ å®æ€§ä¸ç”Ÿæˆçš„åäº‹å®è´¨é‡ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00777v2">PDF</a> ACL 2025 Findings; camera-ready version</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œå¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰ä¸­å¹¿æ³›åº”ç”¨çš„åäº‹å®ç¤ºä¾‹ã€‚æ–‡ç« æå‡ºäº†ZeroCFæ–¹æ³•ï¼Œåˆ©ç”¨ç‰¹å¾å½’å±æ–¹æ³•ç”Ÿæˆé›¶æ ·æœ¬ç¯å¢ƒä¸‹çš„åäº‹å®ç¤ºä¾‹ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†FitCFæ¡†æ¶ï¼Œé€šè¿‡æ ‡ç­¾ç¿»è½¬éªŒè¯è¿›è¡Œåäº‹å®éªŒè¯ï¼Œå¹¶å°†å…¶ä½œä¸ºç¤ºèŒƒç”¨äºå°‘é‡æç¤ºã€‚æ­¤æ–¹æ³•è¶…è¶Šäº†ä¸¤ç§æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ã€‚æ–‡ç« è¿˜é€šè¿‡æ¶ˆèç ”ç©¶ç¡®å®šäº†FitCFçš„æ ¸å¿ƒç»„ä»¶åœ¨æé«˜åäº‹å®è´¨é‡æ–¹é¢çš„é‡è¦æ€§ï¼Œå¹¶å±•ç¤ºäº†LIMEå’Œé›†æˆæ¢¯åº¦ä½œä¸ºFitCFçš„éª¨å¹²å½’å±æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚åŒæ—¶ï¼Œæ–‡ç« æ­ç¤ºäº†ç‰¹å¾å½’å±åˆ†æ•°ä¸ç”Ÿæˆåäº‹å®è´¨é‡ä¹‹é—´çš„å¼ºç›¸å…³æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ZeroCFæ–¹æ³•åˆ©ç”¨ç‰¹å¾å½’å±æ–¹æ³•ä¸­çš„å…³é”®å•è¯ï¼Œåœ¨é›¶æ ·æœ¬ç¯å¢ƒä¸‹ç”Ÿæˆåäº‹å®ç¤ºä¾‹ã€‚</li>
<li>FitCFæ¡†æ¶é€šè¿‡æ ‡ç­¾ç¿»è½¬éªŒè¯æ¥éªŒè¯åäº‹å®ï¼Œå¹¶å°†å…¶ä½œä¸ºç¤ºèŒƒç”¨äºå°‘é‡æç¤ºï¼Œè¡¨ç°å‡ºè¶…è¶Šç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>æ¶ˆèç ”ç©¶ç¡®å®šäº†FitCFçš„æ ¸å¿ƒç»„ä»¶åœ¨æ”¹å–„åäº‹å®è´¨é‡æ–¹é¢çš„é‡è¦æ€§ã€‚</li>
<li>LIMEå’Œé›†æˆæ¢¯åº¦ä½œä¸ºFitCFçš„éª¨å¹²å½’å±æ–¹æ³•è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚</li>
<li>æ–‡ä¸­å¯¹æ¯”äº†ä¸¤ç§å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼Œæ˜¾ç¤ºäº†FitCFçš„ä¼˜è¶Šæ€§ã€‚</li>
<li>åäº‹å®ç¤ºä¾‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¯è§£é‡Šäººå·¥æ™ºèƒ½é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b3fa94e5b8979f96714f0c3880ecd750.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b345248bca34006338e17f33b5467a0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-33a1f76aac418934b2c42a9ef942eacd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Self-supervised-Learning-for-Acoustic-Few-Shot-Classification"><a href="#Self-supervised-Learning-for-Acoustic-Few-Shot-Classification" class="headerlink" title="Self-supervised Learning for Acoustic Few-Shot Classification"></a>Self-supervised Learning for Acoustic Few-Shot Classification</h2><p><strong>Authors:Jingyong Liang, Bernd Meyer, Isaac Ning Lee, Thanh-Toan Do</strong></p>
<p>Labelled data are limited and self-supervised learning is one of the most important approaches for reducing labelling requirements. While it has been extensively explored in the image domain, it has so far not received the same amount of attention in the acoustic domain. Yet, reducing labelling is a key requirement for many acoustic applications. Specifically in bioacoustic, there are rarely sufficient labels for fully supervised learning available. This has led to the widespread use of acoustic recognisers that have been pre-trained on unrelated data for bioacoustic tasks. We posit that training on the actual task data and combining self-supervised pre-training with few-shot classification is a superior approach that has the ability to deliver high accuracy even when only a few labels are available. To this end, we introduce and evaluate a new architecture that combines CNN-based preprocessing with feature extraction based on state space models (SSMs). This combination is motivated by the fact that CNN-based networks alone struggle to capture temporal information effectively, which is crucial for classifying acoustic signals. SSMs, specifically S4 and Mamba, on the other hand, have been shown to have an excellent ability to capture long-range dependencies in sequence data. We pre-train this architecture using contrastive learning on the actual task data and subsequent fine-tuning with an extremely small amount of labelled data. We evaluate the performance of this proposed architecture for ($n$-shot, $n$-class) classification on standard benchmarks as well as real-world data. Our evaluation shows that it outperforms state-of-the-art architectures on the few-shot classification problem. </p>
<blockquote>
<p>æ ‡æ³¨æ•°æ®æœ‰é™ï¼Œè‡ªç›‘ç£å­¦ä¹ æ˜¯é™ä½æ ‡æ³¨è¦æ±‚çš„æœ€é‡è¦æ–¹æ³•ä¹‹ä¸€ã€‚è™½ç„¶å®ƒåœ¨å›¾åƒé¢†åŸŸå·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†åœ¨å£°å­¦é¢†åŸŸå°šæœªå¾—åˆ°ç›¸åŒç¨‹åº¦çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œå‡å°‘æ ‡æ³¨æ˜¯è®¸å¤šå£°å­¦åº”ç”¨çš„å…³é”®éœ€æ±‚ã€‚ç‰¹åˆ«æ˜¯åœ¨ç”Ÿç‰©å£°å­¦é¢†åŸŸï¼Œå‡ ä¹å¾ˆå°‘æœ‰å®Œå…¨ç›‘ç£å­¦ä¹ çš„å……è¶³æ ‡ç­¾ã€‚è¿™å¯¼è‡´äº†é¢„è®­ç»ƒäºéç›¸å…³æ•°æ®çš„å£°å­¦è¯†åˆ«å™¨åœ¨ç”Ÿç‰©å£°å­¦ä»»åŠ¡ä¸­çš„å¹¿æ³›ä½¿ç”¨ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œåœ¨å®é™…ä»»åŠ¡æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶å°†è‡ªç›‘ç£é¢„è®­ç»ƒä¸å°‘é‡æ ·æœ¬åˆ†ç±»ç›¸ç»“åˆæ˜¯ä¸€ç§æ›´ä¼˜è¶Šçš„æ–¹æ³•ï¼Œå³ä½¿åœ¨åªæœ‰å°‘é‡æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œä¹Ÿæœ‰èƒ½åŠ›å®ç°é«˜å‡†ç¡®æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥å¹¶è¯„ä¼°äº†ä¸€ç§æ–°æ¶æ„ï¼Œè¯¥æ¶æ„ç»“åˆäº†åŸºäºCNNçš„é¢„å¤„ç†å’ŒåŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰çš„ç‰¹å¾æå–ã€‚è¿™ç§ç»“åˆæ˜¯ç”±è¿™æ ·ä¸€ä¸ªäº‹å®é©±åŠ¨çš„ï¼šä»…åŸºäºCNNçš„ç½‘ç»œéš¾ä»¥æœ‰æ•ˆåœ°æ•è·æ—¶é—´ä¿¡æ¯ï¼Œè¿™å¯¹äºåˆ†ç±»å£°å­¦ä¿¡å·è‡³å…³é‡è¦ã€‚å¦ä¸€æ–¹é¢ï¼ŒSSMï¼Œç‰¹åˆ«æ˜¯S4å’ŒMambaï¼Œå·²è¢«è¯æ˜åœ¨æ•è·åºåˆ—æ•°æ®çš„é•¿æœŸä¾èµ–æ€§æ–¹é¢å…·æœ‰å‡ºè‰²çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨å¯¹æ¯”å­¦ä¹ åœ¨å®é™…ä»»åŠ¡æ•°æ®ä¸Šå¯¹æ­¤æ¶æ„è¿›è¡Œé¢„è®­ç»ƒï¼Œéšåç”¨æå°‘é‡çš„æ ‡è®°æ•°æ®è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä»¥åŠçœŸå®ä¸–ç•Œæ•°æ®ä¸Šè¯„ä¼°äº†è¯¥æè®®æ¶æ„åœ¨ï¼ˆn-shotï¼Œn-classï¼‰åˆ†ç±»ä¸­çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå®ƒåœ¨å°æ ·æœ¬åˆ†ç±»é—®é¢˜ä¸Šä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ¶æ„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09647v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦é’ˆå¯¹æ ‡æ³¨æ•°æ®æœ‰é™çš„é—®é¢˜ï¼Œæå‡ºä¸€ç§ç»“åˆè‡ªç›‘ç£é¢„è®­ç»ƒå’Œå°‘æ ·æœ¬åˆ†ç±»çš„æ–°æ¶æ„ã€‚è¯¥æ¶æ„ç»“åˆäº†CNNçš„é¢„å¤„ç†å’ŒåŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰çš„ç‰¹å¾æå–ï¼Œæ—¨åœ¨æé«˜åœ¨å£°å­¦ä¿¡å·åˆ†ç±»ä¸­çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿç‰©å£°å­¦é¢†åŸŸã€‚é€šè¿‡å¯¹æ¯”å­¦ä¹ åœ¨å®é™…ä»»åŠ¡æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶éšåç”¨æå°‘é‡çš„æ ‡è®°æ•°æ®è¿›è¡Œå¾®è°ƒã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œæ•°æ®ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¶æ„åœ¨å°‘æ ·æœ¬åˆ†ç±»é—®é¢˜ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ ‡æ³¨æ•°æ®æœ‰é™æ˜¯è®¸å¤šå£°å­¦åº”ç”¨é¢ä¸´çš„æŒ‘æˆ˜ï¼Œè‡ªç›‘ç£å­¦ä¹ æ˜¯å‡å°‘æ ‡æ³¨éœ€æ±‚çš„é‡è¦æ–¹æ³•ä¹‹ä¸€ã€‚</li>
<li>åœ¨ç”Ÿç‰©å£°å­¦é¢†åŸŸï¼Œé¢„è®­ç»ƒäºéç›¸å…³æ•°æ®çš„å£°å­¦è¯†åˆ«å™¨å¹¿æ³›ä½¿ç”¨ï¼Œä½†ä½¿ç”¨å®é™…ä»»åŠ¡æ•°æ®è¿›è¡Œè®­ç»ƒå’Œç»“åˆè‡ªç›‘ç£é¢„è®­ç»ƒçš„æ–¹æ³•å¯èƒ½æ›´ä¼˜ã€‚</li>
<li>æå‡ºçš„æ–°æ¶æ„ç»“åˆäº†CNNå’ŒSSMï¼Œæ—¨åœ¨æœ‰æ•ˆæ•æ‰å£°å­¦ä¿¡å·çš„æ—¶ç©ºä¿¡æ¯ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ ç”¨äºåœ¨å®é™…ä»»åŠ¡æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚</li>
<li>è¯¥æ¶æ„é€šè¿‡å¾®è°ƒï¼Œä½¿ç”¨æå°‘é‡æ ‡è®°æ•°æ®å³å¯è¾¾åˆ°é«˜å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œæ•°æ®ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¶æ„åœ¨å°‘æ ·æœ¬åˆ†ç±»é—®é¢˜ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥æ–¹æ³•å¼¥è¡¥äº†å›¾åƒé¢†åŸŸè‡ªç›‘ç£å­¦ä¹ çš„å¹¿æ³›åº”ç”¨ä¸å£°å­¦é¢†åŸŸç›¸å¯¹å¿½è§†ä¹‹é—´çš„é¸¿æ²Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09647">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-04afa378db2b0f2596256dbe37e3145b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-549871b6a3a5b4e66837c1214f0e0890.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dce663e23967579665d644c50ccd7024.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b148ca7888ba5b10eb19a07b56600ba2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53ae3027214204ff08f0e327d5cb077e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Towards-Foundation-Model-for-Chemical-Reactor-Modeling-Meta-Learning-with-Physics-Informed-Adaptation"><a href="#Towards-Foundation-Model-for-Chemical-Reactor-Modeling-Meta-Learning-with-Physics-Informed-Adaptation" class="headerlink" title="Towards Foundation Model for Chemical Reactor Modeling: Meta-Learning   with Physics-Informed Adaptation"></a>Towards Foundation Model for Chemical Reactor Modeling: Meta-Learning   with Physics-Informed Adaptation</h2><p><strong>Authors:Zihao Wang, Zhe Wu</strong></p>
<p>Developing accurate models for chemical reactors is often challenging due to the complexity of reaction kinetics and process dynamics. Traditional approaches require retraining models for each new system, limiting generalizability and efficiency. In this work, we take a step toward foundation models for chemical reactor modeling by introducing a neural network framework that generalizes across diverse reactor types and rapidly adapts to new chemical processes. Our approach leverages meta-learning to pretrain the model on a broad set of reactor dynamics, enabling efficient adaptation to unseen reactions with minimal data. To further enhance generalizability, we incorporate physics-informed fine-tuning, ensuring physically consistent adaptation to new reactor conditions. Our framework is evaluated across three integer-order fundamental reactor types - continuous stirred tank reactors, batch reactors, and plug flow reactors - demonstrating superior few-shot adaptation compared to conventional data-driven, physics-informed, and transfer learning approaches. By combining meta-learning with physics-informed adaptation, this work lays the foundation for a generalizable modeling framework, advancing the development of foundation models for chemical engineering applications. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/killingbear999/chemical-reactor-foundation-model">https://github.com/killingbear999/chemical-reactor-foundation-model</a>. </p>
<blockquote>
<p>é’ˆå¯¹åŒ–å­¦ååº”å™¨å’Œè¿‡ç¨‹åŠ¨åŠ›å­¦å¤æ‚æ€§çš„ç‰¹ç‚¹ï¼Œå¼€å‘ç²¾ç¡®çš„ååº”å™¨æ¨¡å‹å¾€å¾€é¢ä¸´æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•éœ€è¦ä¸ºæ¯ä¸ªæ–°ç³»ç»Ÿé‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„é€šç”¨æ€§å’Œæ•ˆç‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æœç€æ„å»ºåŒ–å­¦ååº”å †åŸºç¡€æ¨¡å‹çš„æ–¹å‘è¿ˆå‡ºäº†ä¸€æ­¥ï¼Œé€šè¿‡å¼•å…¥ä¸€ç§ç¥ç»ç½‘ç»œæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯å¹¿æ³›åº”ç”¨äºå„ç§ååº”å †ç±»å‹ï¼Œå¹¶èƒ½å¿«é€Ÿé€‚åº”æ–°çš„åŒ–å­¦è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å…ƒå­¦ä¹ åœ¨å¹¿æ³›çš„ååº”å †åŠ¨åŠ›å­¦ä¸Šå¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿å¾—å…¶èƒ½å¤Ÿä½¿ç”¨æœ€å°‘çš„æ•°æ®å¿«é€Ÿé€‚åº”æœªè§è¿‡çš„ååº”ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„é€šç”¨æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†åŸºäºç‰©ç†çš„å¾®è°ƒæ–¹æ³•ï¼Œç¡®ä¿æ–°ååº”å™¨æ¡ä»¶ä¸‹çš„ç‰©ç†ä¸€è‡´æ€§é€‚åº”ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨ä¸‰ç§æ•´æ•°é˜¶åŸºæœ¬ååº”å †ç±»å‹ï¼ˆè¿ç»­æ…æ‹Œç½ååº”å™¨ã€é—´æ­‡ååº”å™¨å’Œå¡æµååº”å™¨ï¼‰ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå±•ç¤ºäº†ä¸ä¼ ç»Ÿæ•°æ®é©±åŠ¨ã€ç‰©ç†çŸ¥æƒ…å’Œè¿ç§»å­¦ä¹ ç­‰æ–¹æ³•ç›¸æ¯”çš„ä¼˜å¼‚å°‘æ ·æœ¬é€‚åº”èƒ½åŠ›ã€‚é€šè¿‡å°†å…ƒå­¦ä¹ ä¸ç‰©ç†çŸ¥æƒ…é€‚åº”ç›¸ç»“åˆï¼Œè¿™é¡¹å·¥ä½œä¸ºé€šç”¨å»ºæ¨¡æ¡†æ¶å¥ å®šäº†åŸºç¡€ï¼Œæ¨åŠ¨äº†åŒ–å­¦å·¥ç¨‹åº”ç”¨åŸºç¡€æ¨¡å‹çš„å¼€å‘ã€‚ç›¸å…³æºä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/killingbear999/chemical-reactor-foundation-model">https://github.com/killingbear999/chemical-reactor-foundation-model</a> ä¸Šè·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.11752v3">PDF</a> Chemical Engineering Research and Design</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼•å…¥äº†ä¸€ç§ç¥ç»ç½‘ç»œæ¡†æ¶ï¼Œç”¨äºå»ºç«‹é€šç”¨çš„åŒ–å­¦ååº”å †æ¨¡å‹ï¼Œè¯¥æ¡†æ¶å¯å¿«é€Ÿé€‚åº”æ–°çš„åŒ–å­¦ååº”è¿‡ç¨‹ã€‚è¯¥ç ”ç©¶åˆ©ç”¨å…ƒå­¦ä¹ å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿å…¶åœ¨å¹¿æ³›çš„ååº”å™¨åŠ¨åŠ›å­¦ä¸Šå…·å¤‡ä¸€èˆ¬æ€§ï¼Œå¹¶èƒ½ä»¥æœ€å°çš„æ•°æ®å¿«é€Ÿé€‚åº”æœªè§è¿‡çš„ååº”ã€‚ä¸ºæé«˜æ¨¡å‹çš„é€šç”¨æ€§ï¼Œç ”ç©¶ç»“åˆäº†ç‰©ç†ä¿¡æ¯å¾®è°ƒæŠ€æœ¯ï¼Œç¡®ä¿å¯¹æ–°ååº”å™¨æ¡ä»¶çš„é€‚åº”å…·æœ‰ç‰©ç†ä¸€è‡´æ€§ã€‚è¯¥ç ”ç©¶åœ¨ä¸‰ç§åŸºæœ¬ååº”å™¨ç±»å‹ä¸Šè¯„ä¼°äº†å…¶æ¡†æ¶çš„æ•ˆèƒ½ï¼Œå±•ç¤ºäº†ä¸ä¼ ç»Ÿæ•°æ®é©±åŠ¨ã€ç‰©ç†ä¿¡æ¯é©±åŠ¨å’Œè¿ç§»å­¦ä¹ ç­‰æ–¹æ³•ç›¸æ¯”çš„ä¼˜å¼‚æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ºå»ºç«‹é€šç”¨çš„å»ºæ¨¡æ¡†æ¶å¥ å®šäº†åŸºç¡€ï¼Œæ¨åŠ¨äº†åŒ–å­¦å·¥ç¨‹åº”ç”¨ä¸­çš„åŸºç¡€æ¨¡å‹çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥ç¥ç»ç½‘ç»œæ¡†æ¶ç”¨äºåŒ–å­¦ååº”å †å»ºæ¨¡ï¼Œå®ç°å¿«é€Ÿé€‚åº”æ–°åŒ–å­¦ååº”è¿‡ç¨‹ã€‚</li>
<li>åˆ©ç”¨å…ƒå­¦ä¹ å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œæé«˜æ¨¡å‹åœ¨å¹¿æ³›ååº”å™¨åŠ¨åŠ›å­¦ä¸Šçš„é€šç”¨æ€§ã€‚</li>
<li>å€ŸåŠ©ç‰©ç†ä¿¡æ¯å¾®è°ƒæŠ€æœ¯å¢å¼ºæ¨¡å‹çš„é€‚åº”æ€§ï¼Œç¡®ä¿å¯¹æ–°ååº”å™¨æ¡ä»¶çš„é€‚åº”å…·æœ‰ç‰©ç†ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨ä¸‰ç§åŸºæœ¬ååº”å™¨ç±»å‹ä¸Šè¿›è¡Œäº†æ¡†æ¶è¯„ä¼°ï¼Œå±•ç¤ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>å¯¹æ¯”äº†ä¼ ç»Ÿæ•°æ®é©±åŠ¨ã€ç‰©ç†ä¿¡æ¯é©±åŠ¨å’Œè¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºå…¶ä¼˜åŠ¿ã€‚</li>
<li>å»ºç«‹äº†é€šç”¨çš„å»ºæ¨¡æ¡†æ¶ï¼Œæ¨åŠ¨äº†åŒ–å­¦å·¥ç¨‹åº”ç”¨ä¸­çš„åŸºç¡€æ¨¡å‹å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.11752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a50dd323c072976daee2411bde605238.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LLM-A-Human-in-the-Loop-Large-Language-Models-Enabled-A-Search-for-Robotics"><a href="#LLM-A-Human-in-the-Loop-Large-Language-Models-Enabled-A-Search-for-Robotics" class="headerlink" title="LLM A*: Human in the Loop Large Language Models Enabled A* Search for   Robotics"></a>LLM A*: Human in the Loop Large Language Models Enabled A* Search for   Robotics</h2><p><strong>Authors:Hengjia Xiao, Peng Wang, Mingzhe Yu, Mattia Robbiani</strong></p>
<p>This research focuses on how Large Language Models (LLMs) can help with (path) planning for mobile embodied agents such as robots, in a human-in-the-loop and interactive manner. A novel framework named LLM A*, aims to leverage the commonsense of LLMs, and the utility-optimal A* is proposed to facilitate few-shot near-optimal path planning. Prompts are used for two main purposes: 1) to provide LLMs with essential information like environments, costs, heuristics, etc.; 2) to communicate human feedback on intermediate planning results to LLMs. This approach takes human feedback on board and renders the entire planning process transparent (akin to a &#96;white boxâ€™) to humans. Moreover, it facilitates code-free path planning, thereby fostering the accessibility and inclusiveness of artificial intelligence techniques to communities less proficient in coding. Comparative analysis against A* and RL demonstrates that LLM A* exhibits greater efficiency in terms of search space and achieves paths comparable to A* while outperforming RL. The interactive nature of LLM A* also makes it a promising tool for deployment in collaborative human-robot tasks. Codes and Supplemental Materials can be found at GitHub: <a target="_blank" rel="noopener" href="https://github.com/speedhawk/LLM-A-">https://github.com/speedhawk/LLM-A-</a>. </p>
<blockquote>
<p>æœ¬æ–‡é‡ç‚¹ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ä½•ä»¥äººæœºå¾ªç¯å’Œäº¤äº’çš„æ–¹å¼ï¼Œä¸ºç§»åŠ¨å®ä½“ä»£ç†ï¼ˆå¦‚æœºå™¨äººï¼‰è¿›è¡Œï¼ˆè·¯å¾„ï¼‰è§„åˆ’æä¾›å¸®åŠ©ã€‚æå‡ºäº†ä¸€ç§åä¸ºLLM A<em>çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨LLMçš„å¸¸è¯†ï¼Œå¹¶æå‡ºäº†å®ç”¨æœ€ä¼˜A</em>ï¼Œä»¥ä¿ƒè¿›å°‘æ•°æœ€ä¼˜è·¯å¾„è§„åˆ’ã€‚æç¤ºç”¨äºä¸¤ä¸ªä¸»è¦ç›®çš„ï¼š1ï¼‰ä¸ºLLMæä¾›ç¯å¢ƒã€æˆæœ¬ã€å¯å‘å¼ç­‰åŸºæœ¬ä¿¡æ¯ï¼›2ï¼‰å¯¹LLMä¼ è¾¾ä¸­é—´è§„åˆ’ç»“æœçš„äººç±»åé¦ˆã€‚è¿™ç§æ–¹æ³•æ¥å—äººç±»åé¦ˆï¼Œä½¿æ•´ä¸ªè§„åˆ’è¿‡ç¨‹å¯¹äººç±»é€æ˜ï¼ˆç±»ä¼¼äºâ€œç™½ç›’â€ï¼‰ã€‚æ­¤å¤–ï¼Œå®ƒä¿ƒè¿›äº†æ— éœ€ç¼–ç çš„è·¯å¾„è§„åˆ’ï¼Œä»è€Œæé«˜äº†ç¼–ç èƒ½åŠ›ä¸è¶³çš„ç¤¾åŒºå¯¹äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¯åŠæ€§å’ŒåŒ…å®¹æ€§ã€‚ä¸A<em>å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¯¹æ¯”åˆ†æè¡¨æ˜ï¼ŒLLM A</em>åœ¨æœç´¢ç©ºé—´æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„æ•ˆç‡ï¼Œå®ç°çš„è·¯å¾„ä¸A<em>ç›¸å½“ä½†ä¼˜äºRLã€‚LLM A</em>çš„äº¤äº’æ€§ä¹Ÿä½¿å…¶æˆä¸ºåœ¨äººæœºåä½œä»»åŠ¡ä¸­éƒ¨ç½²çš„æœ‰å‰é€”çš„å·¥å…·ã€‚ä»£ç å’Œè¡¥å……ææ–™å¯åœ¨GitHubä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/speedhawk/LLM-A%E3%80%82">https://github.com/speedhawk/LLM-A-ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.01797v3">PDF</a> 7 figures, 8 pages</p>
<p><strong>Summary</strong>ï¼šè¯¥ç ”ç©¶æ¢è®¨å¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¾…åŠ©ç§»åŠ¨å®ä½“ï¼ˆå¦‚æœºå™¨äººï¼‰è¿›è¡Œè·¯å¾„è§„åˆ’ã€‚æå‡ºä¸€ç§åä¸ºLLM A<em>çš„æ–°æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨LLMçš„å¸¸è¯†çŸ¥è¯†å’ŒA</em>ç®—æ³•å®ç°å°‘æ ·æœ¬è¿‘ä¼˜è·¯å¾„è§„åˆ’ã€‚é€šè¿‡æç¤ºå®ç°äººæœºäº’åŠ¨åé¦ˆï¼Œä½¿è§„åˆ’è¿‡ç¨‹é€æ˜åŒ–ï¼Œå¹¶ä¿ƒè¿›éç¼–ç ç”¨æˆ·å‚ä¸è·¯å¾„è§„åˆ’ï¼Œæé«˜äººå·¥æ™ºèƒ½æŠ€æœ¯çš„æ™®åŠæ€§å’ŒåŒ…å®¹æ€§ã€‚ä¸A<em>å’Œå¼ºåŒ–å­¦ä¹ ç›¸æ¯”ï¼ŒLLM A</em>åœ¨æœç´¢ç©ºé—´ä¸Šå…·æœ‰æ›´é«˜çš„æ•ˆç‡ï¼Œä¸”è·¯å¾„è§„åˆ’æ•ˆæœä¸A*ç›¸å½“ï¼Œä¼˜äºå¼ºåŒ–å­¦ä¹ ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLM A*æ¡†æ¶ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œç§»åŠ¨å®ä½“è·¯å¾„è§„åˆ’ï¼Œä¿ƒè¿›äººæœºäº’åŠ¨ã€‚</li>
<li>åˆ©ç”¨æç¤ºï¼ˆpromptï¼‰å®ç°ç¯å¢ƒä¿¡æ¯æä¾›å’Œäººç±»åé¦ˆçš„åŒå‘äº¤æµã€‚</li>
<li>LLM A*æ¡†æ¶ä½¿è·¯å¾„è§„åˆ’è¿‡ç¨‹é€æ˜åŒ–ï¼Œä¾¿äºäººç±»ç†è§£ã€‚</li>
<li>éç¼–ç ç”¨æˆ·ä¹Ÿèƒ½åˆ©ç”¨LLM A*å‚ä¸è·¯å¾„è§„åˆ’ï¼Œæé«˜äº†äººå·¥æ™ºèƒ½æŠ€æœ¯çš„æ™®åŠæ€§å’ŒåŒ…å®¹æ€§ã€‚</li>
<li>LLM A*ç›¸æ¯”ä¼ ç»Ÿç®—æ³•åœ¨æœç´¢ç©ºé—´ä¸Šå…·æœ‰æ›´é«˜çš„æ•ˆç‡ã€‚</li>
<li>LLM A<em>çš„è·¯å¾„è§„åˆ’æ•ˆæœä¸A</em>ç®—æ³•ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.01797">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bcb8d52109bff2f9bd0f9ff79fc14b05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51975f5e6e1ec19e4363d79d0db9da07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96f3a104a0c681ceda09748f205919f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91ae3c6fb84b9c5eabf76289daadb2ad.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-17/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-17/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-17/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c14531e7a411986a0a33c17cb52aeddb.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-17  3D-Fixup Advancing Photo Editing with 3D Priors
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-17/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d72c9b73aa31a85f264c24a2f8949036.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-17  AutoPentest Enhancing Vulnerability Management With Autonomous LLM   Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
