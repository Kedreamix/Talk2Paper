<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="视频理解">
    <meta name="description" content="视频理解 方向最新论文已更新，请持续关注 Update in 2025-08-13  FineBadminton A Multi-Level Dataset for Fine-Grained Badminton Video   Understanding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>视频理解 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-a7dba96b26df51d2171bd55e1a69ad03.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">视频理解</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                <span class="chip bg-color">视频理解</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                视频理解
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    24 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-13-更新"><a href="#2025-08-13-更新" class="headerlink" title="2025-08-13 更新"></a>2025-08-13 更新</h1><h2 id="FineBadminton-A-Multi-Level-Dataset-for-Fine-Grained-Badminton-Video-Understanding"><a href="#FineBadminton-A-Multi-Level-Dataset-for-Fine-Grained-Badminton-Video-Understanding" class="headerlink" title="FineBadminton: A Multi-Level Dataset for Fine-Grained Badminton Video   Understanding"></a>FineBadminton: A Multi-Level Dataset for Fine-Grained Badminton Video   Understanding</h2><p><strong>Authors:Xusheng He, Wei Liu, Shanshan Ma, Qian Liu, Chenghao Ma, Jianlong Wu</strong></p>
<p>Fine-grained analysis of complex and high-speed sports like badminton presents a significant challenge for Multimodal Large Language Models (MLLMs), despite their notable advancements in general video understanding. This difficulty arises primarily from the scarcity of datasets with sufficiently rich and domain-specific annotations. To bridge this gap, we introduce FineBadminton, a novel and large-scale dataset featuring a unique multi-level semantic annotation hierarchy (Foundational Actions, Tactical Semantics, and Decision Evaluation) for comprehensive badminton understanding. The construction of FineBadminton is powered by an innovative annotation pipeline that synergistically combines MLLM-generated proposals with human refinement. We also present FBBench, a challenging benchmark derived from FineBadminton, to rigorously evaluate MLLMs on nuanced spatio-temporal reasoning and tactical comprehension. Together, FineBadminton and FBBench provide a crucial ecosystem to catalyze research in fine-grained video understanding and advance the development of MLLMs in sports intelligence. Furthermore, we propose an optimized baseline approach incorporating Hit-Centric Keyframe Selection to focus on pivotal moments and Coordinate-Guided Condensation to distill salient visual information. The results on FBBench reveal that while current MLLMs still face significant challenges in deep sports video analysis, our proposed strategies nonetheless achieve substantial performance gains. The project homepage is available at <a target="_blank" rel="noopener" href="https://finebadminton.github.io/FineBadminton/">https://finebadminton.github.io/FineBadminton/</a>. </p>
<blockquote>
<p>对羽毛球等复杂高速运动进行精细化的分析，对多模态大型语言模型（MLLMs）来说是一个巨大的挑战。尽管其在一般视频理解方面取得了显著的进步，但这个困难主要源于缺乏足够丰富和特定领域的注释数据集。为了弥补这一差距，我们推出了FineBadminton，这是一个新型的大规模数据集，具有独特的多层次语义注释层次结构（基础动作、战术语义和决策评估），以进行全面的羽毛球运动理解。FineBadminton的构建得益于创新的注释管道，该管道协同结合了MLLM生成的提案和人类精炼。我们还推出了由FineBadminton衍生的具有挑战性的基准测试FBBench，以严格评估MLLM在细微时空推理和战术理解方面的表现。总之，FineBadminton和FBBench共同提供了一个重要的生态系统，推动了精细视频理解和体育智能中MLLM的发展。此外，我们提出了一种优化的基线方法，结合了以命中为中心的关键帧选择和坐标引导浓缩策略，以专注于关键时刻并提炼显著视觉信息。在FBBench上的结果揭示，虽然当前MLLM在深度体育视频分析方面仍面临巨大挑战，但我们提出的策略仍然取得了实质性的性能提升。项目主页可在[<a target="_blank" rel="noopener" href="https://finebadminton.github.io/FineBadminton/]%E8%AE%BF%E9%97%AE%E3%80%82">https://finebadminton.github.io/FineBadminton/]访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07554v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对羽毛球这类复杂高速运动的多模态大型语言模型（MLLMs）所面临的挑战。为解决数据集中丰富、特定领域标注的缺乏问题，提出了FineBadminton数据集，具有独特的多层次语义标注层次结构，包括基础动作、战术语义和决策评估，以进行全面羽毛球理解。同时，结合MLLM生成的提案和人类精细修正的注释管道构建了该数据集。此外，还推出了源于FineBadminton的具有挑战性的基准测试FBBench，以严格评估MLLM在细微时空推理和战术理解方面的表现。本文提出的优化基线方法包括以击球为中心的关键帧选择和坐标引导浓缩策略，在FBBench上的结果展示了其性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）在复杂高速运动如羽毛球的分析上仍面临挑战。</li>
<li>数据集缺乏丰富、特定领域的标注是这一挑战的主要原因。</li>
<li>FineBadminton数据集具有多层次语义标注结构，旨在全面理解羽毛球。</li>
<li>FineBadminton数据集的构建结合了MLLM生成的提案和人类精细修正的注释管道。</li>
<li>FBBench是源于FineBadminton的基准测试，用于严格评估MLLM在细微时空推理和战术理解方面的性能。</li>
<li>提出的优化基线方法包括以击球为中心的关键帧选择和坐标引导浓缩策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07554">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a7dba96b26df51d2171bd55e1a69ad03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78afcb559cbbf31d0696d051a15f9e53.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e959a0b7a3526c0fec4295eb166c7637.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a365bb761c93a7ee25352c40376f706.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f57ca28cd9bf4c72b6187577808fe329.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8151ca365b16b5251f4ce7d5d5a445be.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Invert4TVG-A-Temporal-Video-Grounding-Framework-with-Inversion-Tasks-for-Enhanced-Action-Understanding"><a href="#Invert4TVG-A-Temporal-Video-Grounding-Framework-with-Inversion-Tasks-for-Enhanced-Action-Understanding" class="headerlink" title="Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks   for Enhanced Action Understanding"></a>Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks   for Enhanced Action Understanding</h2><p><strong>Authors:Zhaoyu Chen, Hongnan Lin, Yongwei Nie, Fei Ma, Xuemiao Xu, Fei Yu, Chengjiang Long</strong></p>
<p>Temporal Video Grounding (TVG) seeks to localize video segments matching a given textual query. Current methods, while optimizing for high temporal Intersection-over-Union (IoU), often overfit to this metric, compromising semantic action understanding in the video and query, a critical factor for robust TVG. To address this, we introduce Inversion Tasks for TVG (Invert4TVG), a novel framework that enhances both localization accuracy and action understanding without additional data. Our approach leverages three inversion tasks derived from existing TVG annotations: (1) Verb Completion, predicting masked action verbs in queries from video segments; (2) Action Recognition, identifying query-described actions; and (3) Video Description, generating descriptions of video segments that explicitly embed query-relevant actions. These tasks, integrated with TVG via a reinforcement learning framework with well-designed reward functions, ensure balanced optimization of localization and semantics. Experiments show our method outperforms state-of-the-art approaches, achieving a 7.1% improvement in <a href="mailto:&#82;&#49;&#x40;&#48;&#x2e;&#55;">&#82;&#49;&#x40;&#48;&#x2e;&#55;</a> on Charades-STA for a 3B model compared to Time-R1. By inverting TVG to derive query-related actions from segments, our approach strengthens semantic understanding, significantly raising the ceiling of localization accuracy. </p>
<blockquote>
<p>视频时序定位（TVG）旨在定位与给定文本查询匹配的视频片段。当前的方法虽然优化了高时序交并比（IoU），但往往过于依赖这一指标，牺牲了视频和查询中的语义动作理解，而这是实现稳健TVG的关键要素。为了解决这个问题，我们引入了TVG的倒置任务（Invert4TVG），这是一种新型框架，可以在不增加数据的情况下提高定位精度和动作理解。我们的方法利用现有TVG注释衍生出三个倒置任务：（1）动词补全，根据视频片段预测被掩盖的动作动词；（2）动作识别，识别查询描述的动作；（3）视频描述，生成明确嵌入查询相关动作的视频片段描述。这些任务通过与TVG结合的强化学习框架和精心设计的奖励函数，确保定位和语义的均衡优化。实验表明，我们的方法优于最新技术，在Charades-STA数据集上，相较于Time-R1，3B模型的<a href="mailto:&#82;&#x31;&#x40;&#48;&#46;&#x37;">&#82;&#x31;&#x40;&#48;&#46;&#x37;</a>提高了7.1%。通过倒置TVG从片段中推导出与查询相关的动作，我们的方法加强了语义理解，大大提高了定位精度的上限。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07388v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了针对时序视频定位（Temporal Video Grounding，TVG）任务的一种新方法——Invert4TVG。此方法旨在通过三个逆向任务强化视频段与查询文本的语义关联，同时提高定位精度。这些逆向任务包括动词补全、动作识别和视频描述。通过强化学习框架和精心设计奖励函数，该方法在Charades-STA数据集上实现了对最新技术的显著改进，特别是在<a href="mailto:&#82;&#49;&#64;&#48;&#x2e;&#x37;">&#82;&#49;&#64;&#48;&#x2e;&#x37;</a>指标上提高了7.1%。通过从视频段中导出与查询相关的动作，该方法强化了语义理解，显著提高了定位精度上限。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>时序视频定位（TVG）旨在匹配给定文本查询的视频段。</li>
<li>当前方法过度优化IoU指标，可能影响对视频和查询的语义动作理解。</li>
<li>Invert4TVG是一种新的框架，通过逆向任务增强定位精度和动作理解，无需额外数据。</li>
<li>三个逆向任务包括动词补全、动作识别和视频描述。</li>
<li>强化学习框架和奖励函数确保定位和语义的平衡优化。</li>
<li>实验结果表明，该方法在Charades-STA数据集上的<a href="mailto:&#82;&#x31;&#64;&#48;&#46;&#55;">&#82;&#x31;&#64;&#48;&#46;&#55;</a>指标优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07388">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e4f306a51c951cf37fb6ac6b7451729c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38aec632c8b29f237a52553438109640.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82b29a68dda7fa6b7c18d7da583fc730.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bcec82ec4aba2b98be412af17274d7e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2eb3d81f2519546ef30ea8e0617e37f1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VSI-Visual-Subtitle-Integration-for-Keyframe-Selection-to-enhance-Long-Video-Understanding"><a href="#VSI-Visual-Subtitle-Integration-for-Keyframe-Selection-to-enhance-Long-Video-Understanding" class="headerlink" title="VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long   Video Understanding"></a>VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long   Video Understanding</h2><p><strong>Authors:Jianxiang He, Shaoguang Wang, Weiyu Guo, Meisheng Hong, Jungang Li, Yijie Xu, Ziyang Chen, Hui Xiong</strong></p>
<p>Long video understanding presents a significant challenge to multimodal large language models (MLLMs) primarily due to the immense data scale. A critical and widely adopted strategy for making this task computationally tractable is keyframe retrieval, which seeks to identify a sparse set of video frames that are most salient to a given textual query. However, the efficacy of this approach is hindered by weak multimodal alignment between textual queries and visual content and fails to capture the complex temporal semantic information required for precise reasoning. To address this, we propose Visual-Subtitle Integeration(VSI), a multimodal keyframe search method that integrates subtitles, timestamps, and scene boundaries into a unified multimodal search process. The proposed method captures the visual information of video frames as well as the complementary textual information through a dual-stream search mechanism by Video Search Stream as well as Subtitle Match Stream, respectively, and improves the keyframe search accuracy through the interaction of the two search streams. Experimental results show that VSI achieve 40.00% key frame localization accuracy on the text-relevant subset of LongVideoBench and 68.48% accuracy on downstream long Video-QA tasks, surpassing competitive baselines by 20.35% and 15.79%, respectively. Furthermore, on the LongVideoBench, VSI achieved state-of-the-art(SOTA) in medium-to-long video-QA tasks, demonstrating the robustness and generalizability of the proposed multimodal search strategy. </p>
<blockquote>
<p>长视频理解对多模态大型语言模型（MLLMs）提出了重大挑战，这主要是因为其数据量巨大。为了使这项任务在计算上可行，通常采用的关键策略是关键帧检索，它旨在识别给定文本查询中最显著的一组稀疏视频帧。然而，该方法的效力受到文本查询和视觉内容之间弱多模态对齐的阻碍，并且无法捕获用于精确推理所需的复杂时间语义信息。为了解决这一问题，我们提出了视觉字幕整合（VSI），这是一种多模态关键帧搜索方法，它将字幕、时间戳和场景边界整合到统一的多模态搜索过程中。该方法通过视频搜索流和字幕匹配流这两种双流搜索机制，分别捕获视频帧的视觉信息和互补的文本信息，并通过两个搜索流的交互提高关键帧搜索的准确性。实验结果表明，VSI在长视频基准测试文本相关子集上实现了关键帧定位精度为百分之四十，并且在下游长视频问答任务上的准确率为百分之六十八点四十八。与具有竞争力的基线相比，VSI分别提高了百分之二十三点三五和百分之十五点七九。此外，在LongVideoBench上，VSI在中长视频问答任务中达到了最佳水平，证明了所提出的多模态搜索策略的稳健性和通用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06869v1">PDF</a> 9 pages,3 figures</p>
<p><strong>Summary</strong><br>     针对长视频理解的多模态大型语言模型面临的挑战，提出了一种基于视觉字幕整合（VSI）的多模态关键帧搜索方法。该方法通过双流搜索机制，结合视频搜索流和字幕匹配流，提高了关键帧搜索的准确性。在LongVideoBench和长视频问答任务上的实验结果表明，VSI达到了卓越的性能，并实现了中等至长视频问答任务中的最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>长视频理解对多模态大型语言模型（MLLMs）是一个重大挑战，主要因为数据规模庞大。</li>
<li>关键帧检索是应对这一挑战的一种常用策略，但其效果受到文本查询与视觉内容之间弱多模态对齐的限制。</li>
<li>提出的Visual-Subtitle Integration（VSI）方法通过结合字幕、时间戳和场景边界，进行多模态关键帧搜索。</li>
<li>VSI采用双流搜索机制，包括视频搜索流和字幕匹配流，以提高关键帧搜索的准确性。</li>
<li>实验结果显示，VSI在LongVideoBench上的关键帧定位准确度较高，并在长视频问答任务中表现出卓越性能。</li>
<li>VSI达到了先进性能，并在中等至长视频问答任务中实现了最佳效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06869">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-76c7e1d14b90dd86ecdf04d3232550a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cbb09e3fb696395a186a6e1b5ca603b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c32980ce467a06234a5cb9eb91ee641e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a12b3c91311c7eba8b51a7ce626c064.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-135011e2d6a5f309e7c686af4ead15ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85258486e77d54c141ae4ea952c186e2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TSPO-Temporal-Sampling-Policy-Optimization-for-Long-form-Video-Language-Understanding"><a href="#TSPO-Temporal-Sampling-Policy-Optimization-for-Long-form-Video-Language-Understanding" class="headerlink" title="TSPO: Temporal Sampling Policy Optimization for Long-form Video Language   Understanding"></a>TSPO: Temporal Sampling Policy Optimization for Long-form Video Language   Understanding</h2><p><strong>Authors:Canhui Tang, Zifan Han, Hongbo Sun, Sanping Zhou, Xuchong Zhang, Xin Wei, Ye Yuan, Huayu Zhang, Jinglin Xu, Hao Sun</strong></p>
<p>Multimodal Large Language Models (MLLMs) have demonstrated significant progress in vision-language tasks, yet they still face challenges when processing long-duration video inputs. The limitation arises from MLLMs’ context limit and training costs, necessitating sparse frame sampling before feeding videos into MLLMs. However, building a trainable sampling method remains challenging due to the unsupervised and non-differentiable nature of sparse frame sampling in Video-MLLMs. To address these problems, we propose Temporal Sampling Policy Optimization (TSPO), advancing MLLMs’ long-form video-language understanding via reinforcement learning. Specifically, we first propose a trainable event-aware temporal agent, which captures event-query correlation for performing probabilistic keyframe selection. Then, we propose the TSPO reinforcement learning paradigm, which models keyframe selection and language generation as a joint decision-making process, enabling end-to-end group relative optimization for the temporal sampling policy. Furthermore, we propose a dual-style long video training data construction pipeline, balancing comprehensive temporal understanding and key segment localization. Finally, we incorporate rule-based answering accuracy and temporal locating reward mechanisms to optimize the temporal sampling policy. Comprehensive experiments show that our TSPO achieves state-of-the-art performance across multiple long video understanding benchmarks, and shows transferable ability across different cutting-edge Video-MLLMs. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Hui-design/TSPO">https://github.com/Hui-design/TSPO</a> </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）在视觉语言任务中取得了显著进展，但在处理长时视频输入时仍面临挑战。这些限制源于MLLMs的上下文限制和训练成本，需要在将视频输入MLLMs之前进行稀疏帧采样。然而，构建可训练采样方法仍然具有挑战性，因为视频MLLMs中的稀疏帧采样具有无监督和不可微分的特性。为了解决这些问题，我们提出时序采样策略优化（TSPO），通过强化学习推进MLLMs对长格式视频语言的了解。具体来说，我们首先提出一个可训练的事件感知时序代理，用于捕捉事件查询相关性，以执行概率关键帧选择。然后，我们提出了TSPO强化学习范式，将关键帧选择和语言生成建模为联合决策过程，实现对时序采样策略端到端的群体相对优化。此外，我们提出了双风格长视频训练数据构建流程，平衡全面的时间理解和关键段落定位。最后，我们结合了基于规则的回答准确性和时间定位奖励机制，以优化时序采样策略。综合实验表明，我们的TSPO在多个长视频理解基准测试中达到了最先进的性能，并展示了在不同前沿视频MLLMs之间的可迁移能力。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/Hui-design/TSPO%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Hui-design/TSPO找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04369v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>多模态大型语言模型在处理长视频输入时面临挑战，由于上下文限制和训练成本，需对视频进行稀疏帧采样。提出一种基于强化学习的时序采样策略优化方法（TSPO），通过训练事件感知的时序代理和TSPO强化学习范式，实现长视频的语言理解。同时，构建了一种双风格长视频训练数据构建管道，平衡全面的时序理解和关键段定位。优化时序采样策略，通过规则回答准确率和时序定位奖励机制实现。实验表明，TSPO在多个长视频理解基准测试中达到最佳性能，并具备跨不同前沿视频-语言模型的迁移能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）在处理长视频时存在挑战，主要受限于上下文和训练成本。</li>
<li>提出了基于强化学习的时序采样策略优化（TSPO）方法，解决视频-语言模型的稀疏帧采样问题。</li>
<li>TSPO通过训练事件感知的时序代理和TSPO强化学习范式实现长视频的语言理解。</li>
<li>构建了双风格长视频训练数据构建管道，实现全面的时序理解和关键段定位平衡。</li>
<li>通过规则回答准确率和时序定位奖励机制优化时序采样策略。</li>
<li>TSPO在多个长视频理解基准测试中表现最佳。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04369">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-083aaf46ef1939260425f71e03885f46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4071308cd2d6ba67637585204e799235.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-900f73da9d8959472084de08f288ad8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c2b27d464e450c4ddd3feb1c2b194c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-375bacc6fb1dc0820b1a3d241808e0f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4ad5bff30768dd06900d8c770729ad5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LVBench-An-Extreme-Long-Video-Understanding-Benchmark"><a href="#LVBench-An-Extreme-Long-Video-Understanding-Benchmark" class="headerlink" title="LVBench: An Extreme Long Video Understanding Benchmark"></a>LVBench: An Extreme Long Video Understanding Benchmark</h2><p><strong>Authors:Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, Jie Tang</strong></p>
<p>Recent progress in multimodal large language models has markedly enhanced the understanding of short videos (typically under one minute), and several evaluation datasets have emerged accordingly. However, these advancements fall short of meeting the demands of real-world applications such as embodied intelligence for long-term decision-making, in-depth movie reviews and discussions, and live sports commentary, all of which require comprehension of long videos spanning several hours. To address this gap, we introduce LVBench, a benchmark specifically designed for long video understanding. Our dataset comprises publicly sourced videos and encompasses a diverse set of tasks aimed at long video comprehension and information extraction. LVBench is designed to challenge multimodal models to demonstrate long-term memory and extended comprehension capabilities. Our extensive evaluations reveal that current multimodal models still underperform on these demanding long video understanding tasks. Through LVBench, we aim to spur the development of more advanced models capable of tackling the complexities of long video comprehension. Our data and code are publicly available at: <a target="_blank" rel="noopener" href="https://lvbench.github.io/">https://lvbench.github.io</a>. </p>
<blockquote>
<p>近期多模态大型语言模型的进展显著提高了对短视频（通常不到一分钟）的理解能力，并出现了几个相应的评估数据集。然而，这些进展还不足以满足现实世界应用的需求，如用于长期决策制定的嵌入式智能、深入的影评和讨论以及现场体育评论等，这些应用都需要理解长达数小时的长视频。为了解决这一差距，我们推出了LVBench，这是一个专门为长视频理解而设计的基准测试。我们的数据集包含公开来源的视频，涵盖了一系列旨在测试长视频理解和信息提取的任务。LVBench旨在挑战多模态模型，以展示其长期记忆和扩展理解能力。我们的广泛评估表明，当前的多模态模型在这些具有挑战性的长视频理解任务上表现仍然不足。通过LVBench，我们旨在促进更先进模型的发展，以应对长视频理解的复杂性。我们的数据和代码可在以下网址公开获取：<a target="_blank" rel="noopener" href="https://lvbench.github.io/">https://lvbench.github.io</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.08035v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对长视频理解的基准测试平台LVBench。该平台包含公开来源的视频，涵盖了一系列旨在测试长视频理解和信息提取的任务。LVBench旨在挑战多模态模型，展示其长期记忆和扩展理解能力。评估显示，当前多模态模型在这些任务上表现不佳，因此希望通过LVBench推动更先进模型的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型在理解短视频方面取得了显著进步，但难以满足长视频理解需求。</li>
<li>LVBench是一个专门设计用于长视频理解的基准测试平台。</li>
<li>LVBench包含公开来源的视频，涵盖多种旨在测试长视频理解和信息提取的任务。</li>
<li>LVBench旨在挑战多模态模型的长期记忆和扩展理解能力。</li>
<li>当前多模态模型在长视频理解任务上的表现仍然不足。</li>
<li>LVBench的目标是推动更先进模型的发展，以应对长视频理解的复杂性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.08035">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7f263d0ab484da566e85ccd64928f1b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d81721cb781dbfe27821cba0abdaa39c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c273056762cf536aab0d6a612e1289a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3837eb28ac348af29d87d8669e88cc53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82f920c18feb5a20c97295bc0c08da02.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Video-Understanding-with-Large-Language-Models-A-Survey"><a href="#Video-Understanding-with-Large-Language-Models-A-Survey" class="headerlink" title="Video Understanding with Large Language Models: A Survey"></a>Video Understanding with Large Language Models: A Survey</h2><p><strong>Authors:Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, Ali Vosoughi, Chao Huang, Zeliang Zhang, Pinxin Liu, Mingqian Feng, Feng Zheng, Jianguo Zhang, Ping Luo, Jiebo Luo, Chenliang Xu</strong></p>
<p>With the burgeoning growth of online video platforms and the escalating volume of video content, the demand for proficient video understanding tools has intensified markedly. Given the remarkable capabilities of large language models (LLMs) in language and multimodal tasks, this survey provides a detailed overview of recent advancements in video understanding that harness the power of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended multi-granularity (general, temporal, and spatiotemporal) reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding. We examine the unique characteristics and capabilities of Vid-LLMs, categorizing the approaches into three main types: Video Analyzer x LLM, Video Embedder x LLM, and (Analyzer + Embedder) x LLM. Furthermore, we identify five sub-types based on the functions of LLMs in Vid-LLMs: LLM as Summarizer, LLM as Manager, LLM as Text Decoder, LLM as Regressor, and LLM as Hidden Layer. Furthermore, this survey presents a comprehensive study of the tasks, datasets, benchmarks, and evaluation methodologies for Vid-LLMs. Additionally, it explores the expansive applications of Vid-LLMs across various domains, highlighting their remarkable scalability and versatility in real-world video understanding challenges. Finally, it summarizes the limitations of existing Vid-LLMs and outlines directions for future research. For more information, readers are recommended to visit the repository at <a target="_blank" rel="noopener" href="https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding">https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding</a>. </p>
<blockquote>
<p>随着在线视频平台的蓬勃发展和视频内容的不断增加，对熟练的视频理解工具的需求显著增加。鉴于大型语言模型（LLM）在语言和多媒体任务中的突出能力，这篇综述提供了关于如何利用LLM（视频LLM）的力量进行视频理解的最新进展的详细介绍。视频LLM的新兴能力令人惊讶地先进，尤其是它们结合常识知识进行的开放式多粒度（一般、时间和时空）推理能力，这为未来的视频理解指明了有希望的道路。我们研究了视频LLM的独特特征和功能，将方法分为三类：视频分析器xLLM、视频嵌入器xLLM和（分析器+嵌入器）xLLM。此外，我们根据LLM在视频LLM中的功能确定了五种亚型：LLM作为摘要器、LLM作为管理器、LLM作为文本解码器、LLM作为回归器和LLM作为隐藏层。此外，这篇综述还对视频LLM的任务、数据集、基准测试和评估方法进行了全面的研究。还探讨了视频LLM在各个领域的应用广泛性，突显了它们在现实世界的视频理解挑战中的出色可扩展性和通用性。最后，总结了现有视频LLM的局限性，并指出了未来研究的方向。更多信息请参阅<a target="_blank" rel="noopener" href="https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding%E4%BB%93%E5%BA%93%E3%80%82">https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding仓库。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.17432v6">PDF</a> Accepted by IEEE TCSVT</p>
<p><strong>Summary</strong></p>
<p>基于在线视频平台的蓬勃发展和视频内容的快速增长，对熟练的视频理解工具的需求显著增加。本综述详细概述了利用大型语言模型（LLMs）进行视频理解的最新进展。Vid-LLMs的新兴能力令人惊讶地先进，特别是其结合常识知识进行开放式多粒度（一般、时间和时空）推理的能力，为未来的视频理解提供了充满希望的路径。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>在线视频平台和视频内容的增长导致了对熟练视频理解工具的需求显著增加。</li>
<li>大型语言模型（LLMs）在视频理解方面表现出显著的能力。</li>
<li>Vid-LLMs的新兴能力包括开放式的多粒度推理和结合常识知识。</li>
<li>视频理解的方法可以分类为三种主要类型：Video Analyzer x LLM、Video Embedder x LLM和（Analyzer + Embedder）x LLM。</li>
<li>LLM在Vid-LLMs中的功能可以分为五种：总结器、管理器、文本解码器、回归器和隐藏层。</li>
<li>Vid-LLMs的应用范围广泛，具有应对现实视频理解挑战的可扩展性和通用性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.17432">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fe1f193b7160c5ee3981b6f8ce68a49a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-720012d99a98acdd8cb9eb270093a53f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-998dc54077c52aa56de1d1877831ddc8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b009ab17f18a2cefda5c17ad7d53d4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57befebd00bd8025139e3ba9f9bafd2b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-13/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-13/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                    <span class="chip bg-color">视频理解</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-13/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-7f1f90c911d27ca95264e0ad8d6239bf.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-08-13  THAT Token-wise High-frequency Augmentation Transformer for   Hyperspectral Pansharpening
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-13/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2cb7c6a20424b2e695d36bcde60e79be.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-08-13  CycleDiff Cycle Diffusion Models for Unpaired Image-to-image   Translation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
