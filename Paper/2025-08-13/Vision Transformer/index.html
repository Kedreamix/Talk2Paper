<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-08-13  THAT Token-wise High-frequency Augmentation Transformer for   Hyperspectral Pansharpening">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-7f1f90c911d27ca95264e0ad8d6239bf.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    34 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-13-更新"><a href="#2025-08-13-更新" class="headerlink" title="2025-08-13 更新"></a>2025-08-13 更新</h1><h2 id="THAT-Token-wise-High-frequency-Augmentation-Transformer-for-Hyperspectral-Pansharpening"><a href="#THAT-Token-wise-High-frequency-Augmentation-Transformer-for-Hyperspectral-Pansharpening" class="headerlink" title="THAT: Token-wise High-frequency Augmentation Transformer for   Hyperspectral Pansharpening"></a>THAT: Token-wise High-frequency Augmentation Transformer for   Hyperspectral Pansharpening</h2><p><strong>Authors:Hongkun Jin, Hongcheng Jiang, Zejun Zhang, Yuan Zhang, Jia Fu, Tingfeng Li, Kai Luo</strong></p>
<p>Transformer-based methods have demonstrated strong potential in hyperspectral pansharpening by modeling long-range dependencies. However, their effectiveness is often limited by redundant token representations and a lack of multi-scale feature modeling. Hyperspectral images exhibit intrinsic spectral priors (e.g., abundance sparsity) and spatial priors (e.g., non-local similarity), which are critical for accurate reconstruction. From a spectral-spatial perspective, Vision Transformers (ViTs) face two major limitations: they struggle to preserve high-frequency components–such as material edges and texture transitions–and suffer from attention dispersion across redundant tokens. These issues stem from the global self-attention mechanism, which tends to dilute high-frequency signals and overlook localized details. To address these challenges, we propose the Token-wise High-frequency Augmentation Transformer (THAT), a novel framework designed to enhance hyperspectral pansharpening through improved high-frequency feature representation and token selection. Specifically, THAT introduces: (1) Pivotal Token Selective Attention (PTSA) to prioritize informative tokens and suppress redundancy; (2) a Multi-level Variance-aware Feed-forward Network (MVFN) to enhance high-frequency detail learning. Experiments on standard benchmarks show that THAT achieves state-of-the-art performance with improved reconstruction quality and efficiency. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/kailuo93/THAT">https://github.com/kailuo93/THAT</a>. </p>
<blockquote>
<p>基于Transformer的方法通过建模长距离依赖关系在超光谱锐化中显示出强大的潜力。然而，它们的有效性通常受到冗余令牌表示和多尺度特征建模缺乏的限制。超光谱图像表现出内在的谱先验（例如，丰度稀疏性）和空间先验（例如，非局部相似性），这对于准确重建至关重要。从谱-空间的角度来看，视觉Transformer（ViTs）面临两大局限：它们难以保留高频成分（如材料边缘和纹理过渡），并受到冗余令牌分散注意力的影响。这些问题源于全局自注意力机制，该机制往往稀释高频信号并忽略局部细节。为了解决这些挑战，我们提出了Token High频率增强Transformer（THAT），这是一个旨在通过改进的高频特征表示和令牌选择来提高超光谱锐化的新型框架。具体来说，THAT引入了：（1）关键令牌选择性注意力（PTSA）以优先处理信息丰富的令牌并抑制冗余；（2）多级方差感知前馈网络（MVFN）以增强高频细节学习。在标准基准测试上的实验表明，THAT实现了最先进的性能，提高了重建质量和效率。源代码可在<a target="_blank" rel="noopener" href="https://github.com/kailuo93/THAT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/kailuo93/THAT找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08183v1">PDF</a> Accepted to 2025 IEEE International Conference on Systems, Man, and   Cybernetics (SMC)</p>
<p><strong>Summary</strong><br>基于Transformer的方法在光谱高光谱锐化方面具有强大潜力，通过建模长距离依赖关系展现出了良好效果。然而，它们常常受限于冗余的符号表示和缺乏多尺度特征建模。高光谱图像具有固有的光谱先验（如丰度稀疏性）和空间先验（如非局部相似性），这对准确重建至关重要。针对光谱-空间视角，Vision Transformers（ViTs）面临两大局限：难以保留高频分量（如材料边缘和纹理过渡），以及受到冗余符号的注意力分散影响。为解决这些问题，我们提出了Token-wise High-frequency Augmentation Transformer（THAT）这一新型框架，通过改进高频特征表示和符号选择来提升高光谱锐化效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer方法在光谱高光谱锐化中具有潜力，但需解决冗余符号表示和多尺度特征建模的限制。</li>
<li>高光谱图像具有固有的光谱和空间先验，对准确重建至关重要。</li>
<li>Vision Transformers（ViTs）在保留高频分量和处理冗余符号方面存在局限。</li>
<li>为解决这些问题，提出了THAT框架，包含PTSA和MVFN两大创新点。</li>
<li>PTSA能优先处理信息丰富的符号并抑制冗余信息。</li>
<li>MVFN增强了高频细节学习。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08183">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c2065b84c09cd24b7e0b1c99667174e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0f5bdea6d7b269668988097017c5dd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b53c550f13a1654731dd9d795f8a3f2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6df88bebde2807fb46d793c5202a1e8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f908dd9d5271572cc01e86727bf465f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-91b0b7924ccd96e77cae078fb5628499.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5b57cf95a9b75617a4a06378e29f200.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6027b94b76a4425cc90cd1391a0120d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MedReasoner-Reinforcement-Learning-Drives-Reasoning-Grounding-from-Clinical-Thought-to-Pixel-Level-Precision"><a href="#MedReasoner-Reinforcement-Learning-Drives-Reasoning-Grounding-from-Clinical-Thought-to-Pixel-Level-Precision" class="headerlink" title="MedReasoner: Reinforcement Learning Drives Reasoning Grounding from   Clinical Thought to Pixel-Level Precision"></a>MedReasoner: Reinforcement Learning Drives Reasoning Grounding from   Clinical Thought to Pixel-Level Precision</h2><p><strong>Authors:Zhonghao Yan, Muxi Diao, Yuxuan Yang, Jiayuan Xu, Kaizhou Zhang, Ruoyan Jing, Lele Yang, Yanxi Liu, Kongming Liang, Zhanyu Ma</strong></p>
<p>Accurately grounding regions of interest (ROIs) is critical for diagnosis and treatment planning in medical imaging. While multimodal large language models (MLLMs) combine visual perception with natural language, current medical-grounding pipelines still rely on supervised fine-tuning with explicit spatial hints, making them ill-equipped to handle the implicit queries common in clinical practice. This work makes three core contributions. We first define Unified Medical Reasoning Grounding (UMRG), a novel vision-language task that demands clinical reasoning and pixel-level grounding. Second, we release U-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside implicit clinical queries and reasoning traces, spanning 10 modalities, 15 super-categories, and 108 specific categories. Finally, we introduce MedReasoner, a modular framework that distinctly separates reasoning from segmentation: an MLLM reasoner is optimized with reinforcement learning, while a frozen segmentation expert converts spatial prompts into masks, with alignment achieved through format and accuracy rewards. MedReasoner achieves state-of-the-art performance on U-MRG-14K and demonstrates strong generalization to unseen clinical queries, underscoring the significant promise of reinforcement learning for interpretable medical grounding. </p>
<blockquote>
<p>在医学成像中，准确地对感兴趣区域（ROI）进行定位对于诊断和治疗计划的制定至关重要。虽然多模态大型语言模型（MLLMs）结合了视觉感知与自然语言，但当前的医学定位流程仍然依赖于具有明确空间提示的监督微调，这使得它们难以应对临床实践中的隐式查询。本文做出了三项核心贡献。首先，我们定义了统一医学推理定位（UMRG），这是一种新的视觉语言任务，需要临床推理和像素级定位。其次，我们发布了U-MRG-14K数据集，包含14K个样本，每个样本都有像素级掩膜、隐式临床查询和推理轨迹，涵盖10种模态、15种超级类别和108种特定类别。最后，我们推出了MedReasoner，这是一个模块化框架，将推理与分割区分开来：MLLM推理器通过强化学习进行优化，而冻结的分割专家将空间提示转换为掩膜，通过格式和精度奖励实现对齐。MedReasoner在U-MRG-14K上达到了最先进的性能，并对未见过的临床查询表现出了强大的泛化能力，这突显了强化学习在可解释的医学定位中的巨大潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08177v1">PDF</a> 37 pages</p>
<p><strong>Summary</strong></p>
<p>本文提出了统一医学推理定位（UMRG）这一新型视觉语言任务，并发布了U-MRG-14K数据集和MedReasoner框架。UMRG要求临床推理和像素级定位。MedReasoner通过强化学习优化MLLM推理器，将空间提示转化为掩膜，实现格式和精度奖励对齐。该方法在U-MRG-14K上表现优异，并在未见过的临床查询上表现出良好的泛化能力，显示了强化学习在医学定位中的巨大潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了统一医学推理定位（UMRG）任务，结合了临床推理和像素级定位。</li>
<li>发布了U-MRG-14K数据集，包含14K样本，涵盖10种模态、15个超类别和108个具体类别的像素级掩膜和隐性临床查询及推理轨迹。</li>
<li>介绍了MedReasoner框架，将推理和分割任务分离，优化MLLM推理器通过强化学习。</li>
<li>MedReasoner使用冻结的分割专家将空间提示转化为掩膜。</li>
<li>MedReasoner在U-MRG-14K上表现优异。</li>
<li>该方法展示了对未见临床查询的强大泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08177">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1f06984455fc0a851fbffcb0d6207f70.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a378eeee81f6ec3a4240577c60bd85dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27209ab38731dc4fccd7894dcb778d1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f1f90c911d27ca95264e0ad8d6239bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee783866ffd89bcf426bcca3b2ed5aec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11a3bb59d06b2e899f246a04a58dad60.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Power-Battery-Detection"><a href="#Power-Battery-Detection" class="headerlink" title="Power Battery Detection"></a>Power Battery Detection</h2><p><strong>Authors:Xiaoqi Zhao, Peiqian Cao, Lihe Zhang, Zonglei Feng, Hanqi Liu, Jiaming Zuo, Youwei Pang, Weisi Lin, Georges El Fakhri, Huchuan Lu, Xiaofeng Liu</strong></p>
<p>Power batteries are essential components in electric vehicles, where internal structural defects can pose serious safety risks. We conduct a comprehensive study on a new task, power battery detection (PBD), which aims to localize the dense endpoints of cathode and anode plates from industrial X-ray images for quality inspection. Manual inspection is inefficient and error-prone, while traditional vision algorithms struggle with densely packed plates, low contrast, scale variation, and imaging artifacts. To address this issue and drive more attention into this meaningful task, we present PBD5K, the first large-scale benchmark for this task, consisting of 5,000 X-ray images from nine battery types with fine-grained annotations and eight types of real-world visual interference. To support scalable and consistent labeling, we develop an intelligent annotation pipeline that combines image filtering, model-assisted pre-labeling, cross-verification, and layered quality evaluation. We formulate PBD as a point-level segmentation problem and propose MDCNeXt, a model designed to extract and integrate multi-dimensional structure clues including point, line, and count information from the plate itself. To improve discrimination between plates and suppress visual interference, MDCNeXt incorporates two state space modules. The first is a prompt-filtered module that learns contrastive relationships guided by task-specific prompts. The second is a density-aware reordering module that refines segmentation in regions with high plate density. In addition, we propose a distance-adaptive mask generation strategy to provide robust supervision under varying spatial distributions of anode and cathode positions. The source code and datasets will be publicly available at \href{<a target="_blank" rel="noopener" href="https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD%7D%7BPBD5K%7D">https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD}{PBD5K}</a>. </p>
<blockquote>
<p>动力电池是电动汽车中的重要组成部分，其内部结构性缺陷可能带来严重的安全风险。我们对一项新的任务——动力电池检测（PBD）进行了深入研究，其目标是从工业X射线图像中定位阴极和阳极板的密集端点，以进行质量检测。人工检测效率低下且易出错，而传统视觉算法在密集排列的板材、低对比度、尺度变化和图像伪影方面面临挑战。为了解决这个问题并吸引更多注意力关注这项有意义的任务，我们推出了PBD5K，这是该任务的首个大规模基准测试，包含来自九种电池类型的5000张X射线图像，具有细粒度注释和八种现实世界的视觉干扰类型。为了支持可扩展和一致的标注，我们开发了一个智能标注管道，结合了图像过滤、模型辅助预标注、交叉验证和分层质量评估。我们将PBD制定为点级分割问题，并提出MDCNeXt模型，该模型旨在提取和整合包括点、线和计数信息在内的多维结构线索。为了提高板之间的辨别能力并抑制视觉干扰，MDCNeXt融入了两种状态空间模块。第一个是提示过滤模块，它学习在任务特定提示指导下对比关系。第二个是密度感知重新排序模块，它能在高板密度区域优化分割。此外，我们提出了一种距离自适应掩膜生成策略，以在阳极和阴极位置空间分布变化的情况下提供稳健的监督。源代码和数据集将在<a target="_blank" rel="noopener" href="https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD">https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD</a>公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07797v1">PDF</a> Under submission to IEEE Transactions on Pattern Analysis and Machine   Intelligence (T-PAMI)</p>
<p><strong>Summary</strong><br>     本文研究了电动车辆中重要组件动力电池的检测任务，提出了一种新的大规模基准数据集PBD5K，旨在从工业X射线图像中定位阴极和阳极板的密集端点以进行质量检测。文章还介绍了智能标注管道和一个名为MDCNeXt的模型，用于解决手动检测效率低下、传统视觉算法处理密集型电池板等问题。MDCNeXt可以提取并整合点、线和计数等多维度结构线索以提高精度并减少干扰。相关数据与源码将公开在链接地址。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>本文介绍了动力电池检测（PBD）的重要性及其所面临的挑战，如内部缺陷检测的安全风险。</li>
<li>提出大规模数据集PBD5K，包含多种电池类型的X射线图像及精细标注和真实世界视觉干扰类型。</li>
<li>为支持可扩展和一致的标注，开发了智能标注管道，包括图像过滤、模型辅助预标注、交叉验证和分层质量评估等功能。</li>
<li>将PBD任务定义为点级分割问题，并提出MDCNeXt模型来解决密集型电池板检测问题，整合多维度结构线索并引入两个状态空间模块来增强鉴别力和抑制视觉干扰。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07797">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cabdff275bf105ee5c80fea319e35b7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e78016e9d63ba782b324018362b1087.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-205bc1d7af3ff14b39271276ce0748a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dedcee2739e52752f66268f493dae2d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e65c41e21d2b5e1ab37c651d52a8f8f8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4d617c0b1e22a33ae9cc1e9e517fd588.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="UniSVG-A-Unified-Dataset-for-Vector-Graphic-Understanding-and-Generation-with-Multimodal-Large-Language-Models"><a href="#UniSVG-A-Unified-Dataset-for-Vector-Graphic-Understanding-and-Generation-with-Multimodal-Large-Language-Models" class="headerlink" title="UniSVG: A Unified Dataset for Vector Graphic Understanding and   Generation with Multimodal Large Language Models"></a>UniSVG: A Unified Dataset for Vector Graphic Understanding and   Generation with Multimodal Large Language Models</h2><p><strong>Authors:Jinke Li, Jiarui Yu, Chenxing Wei, Hande Dong, Qiang Lin, Liangjing Yang, Zhicai Wang, Yanbin Hao</strong></p>
<p>Unlike bitmap images, scalable vector graphics (SVG) maintain quality when scaled, frequently employed in computer vision and artistic design in the representation of SVG code. In this era of proliferating AI-powered systems, enabling AI to understand and generate SVG has become increasingly urgent. However, AI-driven SVG understanding and generation (U&amp;G) remain significant challenges. SVG code, equivalent to a set of curves and lines controlled by floating-point parameters, demands high precision in SVG U&amp;G. Besides, SVG generation operates under diverse conditional constraints, including textual prompts and visual references, which requires powerful multi-modal processing for condition-to-SVG transformation. Recently, the rapid growth of Multi-modal Large Language Models (MLLMs) have demonstrated capabilities to process multi-modal inputs and generate complex vector controlling parameters, suggesting the potential to address SVG U&amp;G tasks within a unified model. To unlock MLLM’s capabilities in the SVG area, we propose an SVG-centric dataset called UniSVG, comprising 525k data items, tailored for MLLM training and evaluation. To our best knowledge, it is the first comprehensive dataset designed for unified SVG generation (from textual prompts and images) and SVG understanding (color, category, usage, etc.). As expected, learning on the proposed dataset boosts open-source MLLMs’ performance on various SVG U&amp;G tasks, surpassing SOTA close-source MLLMs like GPT-4V. We release dataset, benchmark, weights, codes and experiment details on <a target="_blank" rel="noopener" href="https://ryanlijinke.github.io/">https://ryanlijinke.github.io/</a>. </p>
<blockquote>
<p>与位图图像不同，可缩放矢量图形（SVG）在缩放时能保持质量不变，常被用于计算机视觉和艺术设计中的SVG代码表示。在这个AI系统日益增多的时代，让AI理解和生成SVG变得日益紧迫。然而，AI驱动的SVG理解和生成（U&amp;G）仍然面临重大挑战。SVG代码相当于由浮点参数控制的曲线和线条集合，要求在SVG U&amp;G中具有高精度。此外，SVG生成受到各种条件约束的影响，包括文本提示和视觉参考，这需要强大的多模态处理来实现条件到SVG的转换。最近，多模态大型语言模型（MLLMs）的快速发展显示出处理多模态输入和生成复杂矢量控制参数的能力，表明有可能在统一模型中解决SVG U&amp;G任务。为了解锁MLLM在SVG领域的潜力，我们提出了一个以SVG为中心的数据集，名为UniSVG，包含52.5万条数据项，专为MLLM训练和评估而设计。据我们所知，它是第一个为统一SVG生成（从文本提示和图像）和SVG理解（颜色、类别、用途等）而设计的综合数据集。正如预期的那样，在该数据集上进行学习提高了开源MLLM在各种SVG U&amp;G任务上的性能，超越了最新专有MLLMs如GPT-4V等。我们在<a target="_blank" rel="noopener" href="https://ryanlijinke.github.io/">https://ryanlijinke.github.io/</a>上发布了数据集、基准测试、权重、代码和实验细节。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07766v1">PDF</a> Accepted at ACM MM 2025 Dataset Track</p>
<p><strong>Summary</strong></p>
<p>在AI驱动的系统不断普及的时代，可伸缩矢量图形（SVG）的理解与生成（U＆G）成为重要的挑战。近期，多模态大型语言模型（MLLMs）在处理多模态输入和生成复杂的矢量控制参数方面展现出潜力。为了解锁MLLM在SVG领域的潜力，提出了一种名为UniSVG的SVG专用数据集，包含52.5万项数据，适合用于MLLM的训练和评估，并期望能提高开源MLLM在各种SVG U＆G任务上的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI理解和生成SVG具有重要的挑战性和紧迫性。</li>
<li>SVG代码需要高精度处理，且其生成受到多种条件约束。</li>
<li>多模态大型语言模型（MLLMs）能够处理多模态输入并生成复杂的矢量控制参数。</li>
<li>提出了一个名为UniSVG的SVG专用数据集，用于MLLM的训练和评估。</li>
<li>UniSVG数据集是首个统一SVG生成（从文本提示和图像）和SVG理解的综合数据集。</li>
<li>在UniSVG数据集上训练可以提高开源MLLM在多种SVG U＆G任务上的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07766">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5422aa9cf49fe7a2867eadb1f4a648b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bcf06356b922803a0734de0be9f0ee9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a92e6204d44e698ee22021cc66503dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12a9503f3015e36f307f40c98ea3e684.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55453523c243b547dfa5c5a696685e6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c596bdf836fcffbc74c12d1960f95aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e060874f6e7acab9333caad3a46fad40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f880516d42e873a97d87037fc01499d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Importance-Aware-Semantic-Communication-in-MIMO-OFDM-Systems-Using-Vision-Transformer"><a href="#Importance-Aware-Semantic-Communication-in-MIMO-OFDM-Systems-Using-Vision-Transformer" class="headerlink" title="Importance-Aware Semantic Communication in MIMO-OFDM Systems Using   Vision Transformer"></a>Importance-Aware Semantic Communication in MIMO-OFDM Systems Using   Vision Transformer</h2><p><strong>Authors:Joohyuk Park, Yongjeong Oh, Jihun Park, Yo-Seb Jeon</strong></p>
<p>This paper presents a novel importance-aware quantization, subcarrier mapping, and power allocation (IA-QSMPA) framework for semantic communication in multiple-input multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) systems, empowered by a pretrained Vision Transformer (ViT). The proposed framework exploits attention-based importance extracted from a pretrained ViT to jointly optimize quantization levels, subcarrier mapping, and power allocation. Specifically, IA-QSMPA maps semantically important features to high-quality subchannels and allocates resources in accordance with their contribution to task performance and communication latency. To efficiently solve the resulting nonconvex optimization problem, a block coordinate descent algorithm is employed. The framework is further extended to operate under finite blocklength transmission, where communication errors may occur. In this setting, a segment-wise linear approximation of the channel dispersion penalty is introduced to enable efficient joint optimization under practical constraints. Simulation results on a multi-view image classification task using the MVP-N dataset demonstrate that IA-QSMPA significantly outperforms conventional methods in both ideal and finite blocklength transmission scenarios, achieving superior task performance and communication efficiency. </p>
<blockquote>
<p>本文提出了一种基于预训练视觉转换器（ViT）的多输入多输出正交频分复用（MIMO-OFDM）系统中的语义通信的新型重要性感知量化、子载波映射和功率分配（IA-QSMPA）框架。该框架利用预训练的ViT的注意力机制提取的重要性，联合优化量化级别、子载波映射和功率分配。具体来说，IA-QSMPA将语义重要特征映射到高质量子通道上，并根据其对任务性能和通信延迟的贡献来分配资源。为了有效地解决由此产生的非凸优化问题，采用了块坐标下降算法。该框架进一步扩展到有限块长传输的情况下运行，在这种情况下可能会发生通信错误。在此设置中，引入了信道弥散罚分的分段线性近似，以在实用约束下实现有效的联合优化。使用MVP-N数据集进行的多视图图像分类任务的仿真结果表明，IA-QSMPA在理想和有限块长传输场景中均显著优于传统方法，实现了卓越的任务性能和通信效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07696v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于预训练Vision Transformer（ViT）的重要性感知量化、子载波映射和功率分配（IA-QSMPA）框架，用于MIMO-OFDM系统中的语义通信。该框架利用预训练ViT的注意力机制提取重要性，以联合优化量化级别、子载波映射和功率分配。IA-QSMPA将语义重要特征映射到高质量子通道，并根据其对任务性能和通信延迟的贡献分配资源。采用块坐标下降算法有效地解决了由此产生的非凸优化问题。该框架还扩展到有限块长传输情况下运行，在此情况下引入分段线性近似信道扩散惩罚，以在实用约束下实现有效的联合优化。模拟结果表明，在理想和有限块长传输场景下，IA-QSMPA在多视图图像分类任务上显著优于传统方法，实现了出色的任务性能和通信效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文提出了一种基于预训练Vision Transformer（ViT）的IA-QSMPA框架，用于MIMO-OFDM系统中的语义通信。</li>
<li>该框架利用注意力机制提取重要性，并联合优化量化级别、子载波映射和功率分配。</li>
<li>IA-QSMPA将语义重要特征映射到高质量子通道，并根据任务性能和通信延迟的贡献分配资源。</li>
<li>采用块坐标下降算法解决非凸优化问题。</li>
<li>框架扩展到有限块长传输情况，引入分段线性近似信道扩散惩罚以实现有效优化。</li>
<li>模拟结果表明，IA-QSMPA在理想和有限块长传输场景下均表现出卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07696">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cf6e5b0415effd86dd732d0c0fe58c67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d89a828e80bdf191cee6e8a134311e7a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MobileViCLIP-An-Efficient-Video-Text-Model-for-Mobile-Devices"><a href="#MobileViCLIP-An-Efficient-Video-Text-Model-for-Mobile-Devices" class="headerlink" title="MobileViCLIP: An Efficient Video-Text Model for Mobile Devices"></a>MobileViCLIP: An Efficient Video-Text Model for Mobile Devices</h2><p><strong>Authors:Min Yang, Zihan Jia, Zhilin Dai, Sheng Guo, Limin Wang</strong></p>
<p>Efficient lightweight neural networks are with increasing attention due to their faster reasoning speed and easier deployment on mobile devices. However, existing video pre-trained models still focus on the common ViT architecture with high latency, and few works attempt to build efficient architecture on mobile devices. This paper bridges this gap by introducing temporal structural reparameterization into an efficient image-text model and training it on a large-scale high-quality video-text dataset, resulting in an efficient video-text model that can run on mobile devices with strong zero-shot classification and retrieval capabilities, termed as MobileViCLIP. In particular, in terms of inference speed on mobile devices, our MobileViCLIP-Small is 55.4x times faster than InternVideo2-L14 and 6.7x faster than InternVideo2-S14. In terms of zero-shot retrieval performance, our MobileViCLIP-Small obtains similar performance as InternVideo2-L14 and obtains 6.9% better than InternVideo2-S14 on MSR-VTT. The code is available at <a target="_blank" rel="noopener" href="https://github.com/MCG-NJU/MobileViCLIP">https://github.com/MCG-NJU/MobileViCLIP</a>. </p>
<blockquote>
<p>高效轻量级神经网络因其更快的推理速度和在移动设备上的更容易部署而受到越来越多的关注。然而，现有的视频预训练模型仍然主要关注高延迟的通用ViT架构，很少有工作尝试在移动设备上构建高效架构。本文通过引入时间结构重参数化到一个高效的图像文本模型中，并在大规模高质量的视频文本数据集上进行训练，从而填补了这一空白。这导致了一个能在移动设备上运行的高效视频文本模型，具有强烈的零样本分类和检索能力，被称为MobileViCLIP。特别地，在移动设备上的推理速度方面，我们的MobileViCLIP-Small是InternVideo2-L14的55.4倍，比InternVideo2-S14快6.7倍。在零样本检索性能方面，我们的MobileViCLIP-Small与InternVideo2-L14表现相似，并在MSR-VTT上比InternVideo2-S14高出6.9%。代码可在<a target="_blank" rel="noopener" href="https://github.com/MCG-NJU/MobileViCLIP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/MCG-NJU/MobileViCLIP找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07312v1">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong><br>     本文提出一种高效的视频文本模型MobileViCLIP，通过引入时间结构重参数化，在大型高质量视频文本数据集上训练，可在移动设备上运行，具有强大的零样本分类和检索能力。MobileViCLIP-Small在推理速度和零样本检索性能上表现优异，相较于其他模型有更快的推理速度和更好的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本研究解决了现有视频预训练模型主要集中在高延迟的通用ViT架构上，而针对移动设备的高效架构研究较少的问题。</li>
<li>通过引入时间结构重参数化，提出了一种高效的视频文本模型MobileViCLIP。</li>
<li>MobileViCLIP可在移动设备上运行，具有强大的零样本分类和检索能力。</li>
<li>MobileViCLIP-Small在推理速度上相较于其他模型有显著优势，例如，它的推理速度是InternVideo2-L14的55.4倍，是InternVideo2-S14的6.7倍。</li>
<li>在零样本检索性能方面，MobileViCLIP-Small的表现优异，相较于InternVideo2-L14表现相当，并在MSR-VTT上比InternVideo2-S14高出6.9%。</li>
<li>该模型的代码已公开，便于其他研究者使用和进一步改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07312">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f930c3e462d5fbcb04bf02e815792c83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45a9542d034eeb9992611e65bb053cce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c4d249f795ec902a660c011457fda89.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf1c7cbe4eab299b089399db6f694910.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-78f302a0a253541cea3661bfc9b0334f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Dynamic-Pattern-Alignment-Learning-for-Pretraining-Lightweight-Human-Centric-Vision-Models"><a href="#Dynamic-Pattern-Alignment-Learning-for-Pretraining-Lightweight-Human-Centric-Vision-Models" class="headerlink" title="Dynamic Pattern Alignment Learning for Pretraining Lightweight   Human-Centric Vision Models"></a>Dynamic Pattern Alignment Learning for Pretraining Lightweight   Human-Centric Vision Models</h2><p><strong>Authors:Xuanhan Wang, Huimin Deng, Ke Liu, Jun Wang, Lianli Gao, Jingkuan Song</strong></p>
<p>Human-centric vision models (HVMs) have achieved remarkable generalization due to large-scale pretraining on massive person images. However, their dependence on large neural architectures and the restricted accessibility of pretraining data significantly limits their practicality in real-world applications. To address this limitation, we propose Dynamic Pattern Alignment Learning (DPAL), a novel distillation-based pretraining framework that efficiently trains lightweight HVMs to acquire strong generalization from large HVMs. In particular, human-centric visual perception are highly dependent on three typical visual patterns, including global identity pattern, local shape pattern and multi-person interaction pattern. To achieve generalizable lightweight HVMs, we firstly design a dynamic pattern decoder (D-PaDe), acting as a dynamic Mixture of Expert (MoE) model. It incorporates three specialized experts dedicated to adaptively extract typical visual patterns, conditioned on both input image and pattern queries. And then, we present three levels of alignment objectives, which aims to minimize generalization gap between lightweight HVMs and large HVMs at global image level, local pixel level, and instance relation level. With these two deliberate designs, the DPAL effectively guides lightweight model to learn all typical human visual patterns from large HVMs, which can generalize to various human-centric vision tasks. Extensive experiments conducted on 15 challenging datasets demonstrate the effectiveness of the DPAL. Remarkably, when employing PATH-B as the teacher, DPAL-ViT&#x2F;Ti (5M parameters) achieves surprising generalizability similar to existing large HVMs such as PATH-B (84M) and Sapiens-L (307M), and outperforms previous distillation-based pretraining methods including Proteus-ViT&#x2F;Ti (5M) and TinyMiM-ViT&#x2F;Ti (5M) by a large margin. </p>
<blockquote>
<p>人类为中心的视觉模型（HVMs）由于大规模预训练在大量人像上的应用，已经实现了显著的泛化。然而，它们对大型神经网络架构的依赖以及预训练数据获取的限制，显著限制了它们在现实世界应用中的实用性。为了解决这一局限性，我们提出了动态模式对齐学习（DPAL）这种基于蒸馏的预训练框架，它能有效地训练轻量级HVMs，从大型HVMs中获得强大的泛化能力。特别是，人类为中心的视觉感知高度依赖于三种典型的视觉模式，包括全局身份模式、局部形状模式和多人物交互模式。为了实现可泛化的轻量级HVMs，我们首先设计了一个动态模式解码器（D-PaDe），它作为一个动态的混合专家（MoE）模型。它结合了三个专业专家，旨在根据输入图像和模式查询自适应地提取典型视觉模式。然后，我们提出了三个级别的对齐目标，旨在最小化轻量级HVMs和大型HVMs在全局图像级别、局部像素级别和实例关系级别的泛化差距。通过这两个精心设计，DPAL有效地引导轻量级模型从大型HVMs学习所有典型的人类视觉模式，可以泛化到各种以人类为中心的视觉任务。在15个具有挑战性的数据集上进行的广泛实验证明了DPAL的有效性。值得一提的是，当使用PATH-B作为教师时，DPAL-ViT&#x2F;Ti（5M参数）实现了与现有大型HVMs（如PATH-B（84M）和Sapiens-L（307M））相似的惊人泛化能力，并大大超越了之前的基于蒸馏的预训练方法，包括Proteus-ViT&#x2F;Ti（5M）和TinyMiM-ViT&#x2F;Ti（5M）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07144v1">PDF</a> </p>
<p><strong>Summary</strong><br>     针对人中心视觉模型（HVMs）在实际应用中的局限性，提出动态模式对齐学习（DPAL）框架。通过设计动态模式解码器（D-PaDe）和三个层次的对齐目标，实现从小型模型向大型HVMs的有效学习，提高了轻型HVMs的通用性。在多个数据集上的实验验证了DPAL的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人中心视觉模型（HVMs）通过大规模预训练实现显著泛化，但受限于大型神经网络架构和预训练数据可访问性，在现实世界应用中的实用性受限。</li>
<li>提出动态模式对齐学习（DPAL）框架，旨在训练轻量级的人中心视觉模型（HVMs），使其从大型HVMs中有效获取强大的泛化能力。</li>
<li>DPAL框架包含动态模式解码器（D-PaDe），它作为动态混合专家（MoE）模型，能够自适应地提取典型视觉模式。</li>
<li>引入三个层次的对齐目标，以最小化轻量级HVMs和大型HVMs之间的泛化差距，包括全局图像级别、局部像素级别和实例关系级别。</li>
<li>DPAL框架能够引导轻型模型从大型HVMs学习所有人类典型的视觉模式，并可以泛化到各种人中心视觉任务。</li>
<li>在15个具有挑战性的数据集上进行的实验验证了DPAL的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07144">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-405031ecab0f49ad621382e6e2342fa8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cf9f2b3905ebfbd23567619bea15e33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-365214c0bf89cafc7b363d7530d51b9b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Investigating-the-Impact-of-Large-Scale-Pre-training-on-Nutritional-Content-Estimation-from-2D-Images"><a href="#Investigating-the-Impact-of-Large-Scale-Pre-training-on-Nutritional-Content-Estimation-from-2D-Images" class="headerlink" title="Investigating the Impact of Large-Scale Pre-training on Nutritional   Content Estimation from 2D Images"></a>Investigating the Impact of Large-Scale Pre-training on Nutritional   Content Estimation from 2D Images</h2><p><strong>Authors:Michele Andrade, Guilherme A. L. Silva, Valéria Santos, Gladston Moreira, Eduardo Luz</strong></p>
<p>Estimating the nutritional content of food from images is a critical task with significant implications for health and dietary monitoring. This is challenging, especially when relying solely on 2D images, due to the variability in food presentation, lighting, and the inherent difficulty in inferring volume and mass without depth information. Furthermore, reproducibility in this domain is hampered by the reliance of state-of-the-art methods on proprietary datasets for large-scale pre-training. In this paper, we investigate the impact of large-scale pre-training datasets on the performance of deep learning models for nutritional estimation using only 2D images. We fine-tune and evaluate Vision Transformer (ViT) models pre-trained on two large public datasets, ImageNet and COYO, comparing their performance against baseline CNN models (InceptionV2 and ResNet-50) and a state-of-the-art method pre-trained on the proprietary JFT-300M dataset. We conduct extensive experiments on the Nutrition5k dataset, a large-scale collection of real-world food plates with high-precision nutritional annotations. Our evaluation using Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAE%) reveals that models pre-trained on JFT-300M significantly outperform those pre-trained on public datasets. Unexpectedly, the model pre-trained on the massive COYO dataset performs worse than the model pre-trained on ImageNet for this specific regression task, refuting our initial hypothesis. Our analysis provides quantitative evidence highlighting the critical role of pre-training dataset characteristics, including scale, domain relevance, and curation quality, for effective transfer learning in 2D nutritional estimation. </p>
<blockquote>
<p>从图像估算食品的营养成分是健康与饮食监测中具有重要意义的任务。这是一项挑战，尤其是仅依赖2D图像时，由于食品展示、光照的多样性以及缺乏深度信息导致推断体积和质量的固有困难。此外，该领域的可重复性受到阻碍，因为最先进的方法依赖于大规模预训练的专有数据集。在本文中，我们研究了大规模预训练数据集对仅使用2D图像进行营养估算的深度学习模型性能的影响。我们微调并评估了在ImageNet和COYO两个大型公共数据集上预训练的Vision Transformer（ViT）模型，将其性能与基线CNN模型（InceptionV2和ResNet-50）以及一种在专有JFT-300M数据集上预训练的最新方法进行比较。我们在Nutrition5k数据集上进行了大量实验，该数据集是现实世界食品盘的大量集合，具有高精度营养注释。我们使用平均绝对误差（MAE）和平均绝对百分比误差（MAE%）进行评估，结果显示在JFT-300M上预训练的模型显著优于在公共数据集上预训练的模型。出乎意料的是，在大量COYO数据集上预训练的模型在此特定回归任务上的表现不如在ImageNet上预训练的模型，这与我们最初的假设相悖。我们的分析提供了定量证据，突出显示了预训练数据集特征（包括规模、领域相关性和筛选质量）在2D营养估算中的有效迁移学习中的关键作用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03996v2">PDF</a> 12 pages</p>
<p><strong>摘要</strong></p>
<p>本文研究了大规模预训练数据集对仅使用2D图像进行营养估算的深度学习模型性能的影响。通过微调并在Nutrition5k数据集上评估预训练于ImageNet和COYO的Vision Transformer（ViT）模型，与基准CNN模型（InceptionV2和ResNet-50）以及预训练于专有JFT-300M数据集的最先进方法进行比较。实验结果表明，预训练于JFT-300M的模型显著优于预训练于公开数据集的模型。意外的是，预训练于大规模COYO数据集的模型在此特定回归任务上的表现不如预训练于ImageNet的模型，这反驳了我们的初步假设。分析提供了定量证据，突出显示了预训练数据集特性，包括规模、领域相关性和编纂质量，在2D营养估算中的有效迁移学习方面的关键作用。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>营养图像估算具有健康与饮食监测的重大意义，但面临食品展示、光照、体积和质量推断的难题。</li>
<li>使用Vision Transformer (ViT)模型进行深度学习，研究大规模预训练数据集对营养估算性能的影响。</li>
<li>对比了预训练于ImageNet和COYO的ViT模型，以及与基准CNN模型的性能。</li>
<li>预训练于JFT-300M的模型表现最佳，显示预训练数据集规模、领域相关性和编纂质量的重要性。</li>
<li>COYO数据集在特定回归任务上的表现不如ImageNet，这出乎预料并反驳了初始假设。</li>
<li>定量证据表明，预训练数据集特性对有效迁移学习在营养估算中的关键作用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03996">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e9b51f41df81724796fc76fb768c8fd6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae5c5926e15005c3e7ab5d92565821e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e406cfab08800a8ef480c7fcf190fed3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5caad04a6a77ce58ccf0056e4d79dd6e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-13/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-13/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-13/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c008540d8e5195615f8fdd26c5d65594.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-08-13  GAPNet A Lightweight Framework for Image and Video Salient Object   Detection via Granularity-Aware Paradigm
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-13/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a7dba96b26df51d2171bd55e1a69ad03.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-08-13  FineBadminton A Multi-Level Dataset for Fine-Grained Badminton Video   Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27544.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
