<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-13  THAT Token-wise High-frequency Augmentation Transformer for   Hyperspectral Pansharpening">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-7f1f90c911d27ca95264e0ad8d6239bf.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    34 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-13-æ›´æ–°"><a href="#2025-08-13-æ›´æ–°" class="headerlink" title="2025-08-13 æ›´æ–°"></a>2025-08-13 æ›´æ–°</h1><h2 id="THAT-Token-wise-High-frequency-Augmentation-Transformer-for-Hyperspectral-Pansharpening"><a href="#THAT-Token-wise-High-frequency-Augmentation-Transformer-for-Hyperspectral-Pansharpening" class="headerlink" title="THAT: Token-wise High-frequency Augmentation Transformer for   Hyperspectral Pansharpening"></a>THAT: Token-wise High-frequency Augmentation Transformer for   Hyperspectral Pansharpening</h2><p><strong>Authors:Hongkun Jin, Hongcheng Jiang, Zejun Zhang, Yuan Zhang, Jia Fu, Tingfeng Li, Kai Luo</strong></p>
<p>Transformer-based methods have demonstrated strong potential in hyperspectral pansharpening by modeling long-range dependencies. However, their effectiveness is often limited by redundant token representations and a lack of multi-scale feature modeling. Hyperspectral images exhibit intrinsic spectral priors (e.g., abundance sparsity) and spatial priors (e.g., non-local similarity), which are critical for accurate reconstruction. From a spectral-spatial perspective, Vision Transformers (ViTs) face two major limitations: they struggle to preserve high-frequency componentsâ€“such as material edges and texture transitionsâ€“and suffer from attention dispersion across redundant tokens. These issues stem from the global self-attention mechanism, which tends to dilute high-frequency signals and overlook localized details. To address these challenges, we propose the Token-wise High-frequency Augmentation Transformer (THAT), a novel framework designed to enhance hyperspectral pansharpening through improved high-frequency feature representation and token selection. Specifically, THAT introduces: (1) Pivotal Token Selective Attention (PTSA) to prioritize informative tokens and suppress redundancy; (2) a Multi-level Variance-aware Feed-forward Network (MVFN) to enhance high-frequency detail learning. Experiments on standard benchmarks show that THAT achieves state-of-the-art performance with improved reconstruction quality and efficiency. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/kailuo93/THAT">https://github.com/kailuo93/THAT</a>. </p>
<blockquote>
<p>åŸºäºTransformerçš„æ–¹æ³•é€šè¿‡å»ºæ¨¡é•¿è·ç¦»ä¾èµ–å…³ç³»åœ¨è¶…å…‰è°±é”åŒ–ä¸­æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æœ‰æ•ˆæ€§é€šå¸¸å—åˆ°å†—ä½™ä»¤ç‰Œè¡¨ç¤ºå’Œå¤šå°ºåº¦ç‰¹å¾å»ºæ¨¡ç¼ºä¹çš„é™åˆ¶ã€‚è¶…å…‰è°±å›¾åƒè¡¨ç°å‡ºå†…åœ¨çš„è°±å…ˆéªŒï¼ˆä¾‹å¦‚ï¼Œä¸°åº¦ç¨€ç–æ€§ï¼‰å’Œç©ºé—´å…ˆéªŒï¼ˆä¾‹å¦‚ï¼Œéå±€éƒ¨ç›¸ä¼¼æ€§ï¼‰ï¼Œè¿™å¯¹äºå‡†ç¡®é‡å»ºè‡³å…³é‡è¦ã€‚ä»è°±-ç©ºé—´çš„è§’åº¦æ¥çœ‹ï¼Œè§†è§‰Transformerï¼ˆViTsï¼‰é¢ä¸´ä¸¤å¤§å±€é™ï¼šå®ƒä»¬éš¾ä»¥ä¿ç•™é«˜é¢‘æˆåˆ†ï¼ˆå¦‚ææ–™è¾¹ç¼˜å’Œçº¹ç†è¿‡æ¸¡ï¼‰ï¼Œå¹¶å—åˆ°å†—ä½™ä»¤ç‰Œåˆ†æ•£æ³¨æ„åŠ›çš„å½±å“ã€‚è¿™äº›é—®é¢˜æºäºå…¨å±€è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¾€å¾€ç¨€é‡Šé«˜é¢‘ä¿¡å·å¹¶å¿½ç•¥å±€éƒ¨ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Token Highé¢‘ç‡å¢å¼ºTransformerï¼ˆTHATï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é€šè¿‡æ”¹è¿›çš„é«˜é¢‘ç‰¹å¾è¡¨ç¤ºå’Œä»¤ç‰Œé€‰æ‹©æ¥æé«˜è¶…å…‰è°±é”åŒ–çš„æ–°å‹æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒTHATå¼•å…¥äº†ï¼šï¼ˆ1ï¼‰å…³é”®ä»¤ç‰Œé€‰æ‹©æ€§æ³¨æ„åŠ›ï¼ˆPTSAï¼‰ä»¥ä¼˜å…ˆå¤„ç†ä¿¡æ¯ä¸°å¯Œçš„ä»¤ç‰Œå¹¶æŠ‘åˆ¶å†—ä½™ï¼›ï¼ˆ2ï¼‰å¤šçº§æ–¹å·®æ„ŸçŸ¥å‰é¦ˆç½‘ç»œï¼ˆMVFNï¼‰ä»¥å¢å¼ºé«˜é¢‘ç»†èŠ‚å­¦ä¹ ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTHATå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæé«˜äº†é‡å»ºè´¨é‡å’Œæ•ˆç‡ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/kailuo93/THAT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/kailuo93/THATæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08183v1">PDF</a> Accepted to 2025 IEEE International Conference on Systems, Man, and   Cybernetics (SMC)</p>
<p><strong>Summary</strong><br>åŸºäºTransformerçš„æ–¹æ³•åœ¨å…‰è°±é«˜å…‰è°±é”åŒ–æ–¹é¢å…·æœ‰å¼ºå¤§æ½œåŠ›ï¼Œé€šè¿‡å»ºæ¨¡é•¿è·ç¦»ä¾èµ–å…³ç³»å±•ç°å‡ºäº†è‰¯å¥½æ•ˆæœã€‚ç„¶è€Œï¼Œå®ƒä»¬å¸¸å¸¸å—é™äºå†—ä½™çš„ç¬¦å·è¡¨ç¤ºå’Œç¼ºä¹å¤šå°ºåº¦ç‰¹å¾å»ºæ¨¡ã€‚é«˜å…‰è°±å›¾åƒå…·æœ‰å›ºæœ‰çš„å…‰è°±å…ˆéªŒï¼ˆå¦‚ä¸°åº¦ç¨€ç–æ€§ï¼‰å’Œç©ºé—´å…ˆéªŒï¼ˆå¦‚éå±€éƒ¨ç›¸ä¼¼æ€§ï¼‰ï¼Œè¿™å¯¹å‡†ç¡®é‡å»ºè‡³å…³é‡è¦ã€‚é’ˆå¯¹å…‰è°±-ç©ºé—´è§†è§’ï¼ŒVision Transformersï¼ˆViTsï¼‰é¢ä¸´ä¸¤å¤§å±€é™ï¼šéš¾ä»¥ä¿ç•™é«˜é¢‘åˆ†é‡ï¼ˆå¦‚ææ–™è¾¹ç¼˜å’Œçº¹ç†è¿‡æ¸¡ï¼‰ï¼Œä»¥åŠå—åˆ°å†—ä½™ç¬¦å·çš„æ³¨æ„åŠ›åˆ†æ•£å½±å“ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Token-wise High-frequency Augmentation Transformerï¼ˆTHATï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡æ”¹è¿›é«˜é¢‘ç‰¹å¾è¡¨ç¤ºå’Œç¬¦å·é€‰æ‹©æ¥æå‡é«˜å…‰è°±é”åŒ–æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ–¹æ³•åœ¨å…‰è°±é«˜å…‰è°±é”åŒ–ä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†éœ€è§£å†³å†—ä½™ç¬¦å·è¡¨ç¤ºå’Œå¤šå°ºåº¦ç‰¹å¾å»ºæ¨¡çš„é™åˆ¶ã€‚</li>
<li>é«˜å…‰è°±å›¾åƒå…·æœ‰å›ºæœ‰çš„å…‰è°±å’Œç©ºé—´å…ˆéªŒï¼Œå¯¹å‡†ç¡®é‡å»ºè‡³å…³é‡è¦ã€‚</li>
<li>Vision Transformersï¼ˆViTsï¼‰åœ¨ä¿ç•™é«˜é¢‘åˆ†é‡å’Œå¤„ç†å†—ä½™ç¬¦å·æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†THATæ¡†æ¶ï¼ŒåŒ…å«PTSAå’ŒMVFNä¸¤å¤§åˆ›æ–°ç‚¹ã€‚</li>
<li>PTSAèƒ½ä¼˜å…ˆå¤„ç†ä¿¡æ¯ä¸°å¯Œçš„ç¬¦å·å¹¶æŠ‘åˆ¶å†—ä½™ä¿¡æ¯ã€‚</li>
<li>MVFNå¢å¼ºäº†é«˜é¢‘ç»†èŠ‚å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08183">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c2065b84c09cd24b7e0b1c99667174e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0f5bdea6d7b269668988097017c5dd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b53c550f13a1654731dd9d795f8a3f2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6df88bebde2807fb46d793c5202a1e8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f908dd9d5271572cc01e86727bf465f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-91b0b7924ccd96e77cae078fb5628499.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5b57cf95a9b75617a4a06378e29f200.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6027b94b76a4425cc90cd1391a0120d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MedReasoner-Reinforcement-Learning-Drives-Reasoning-Grounding-from-Clinical-Thought-to-Pixel-Level-Precision"><a href="#MedReasoner-Reinforcement-Learning-Drives-Reasoning-Grounding-from-Clinical-Thought-to-Pixel-Level-Precision" class="headerlink" title="MedReasoner: Reinforcement Learning Drives Reasoning Grounding from   Clinical Thought to Pixel-Level Precision"></a>MedReasoner: Reinforcement Learning Drives Reasoning Grounding from   Clinical Thought to Pixel-Level Precision</h2><p><strong>Authors:Zhonghao Yan, Muxi Diao, Yuxuan Yang, Jiayuan Xu, Kaizhou Zhang, Ruoyan Jing, Lele Yang, Yanxi Liu, Kongming Liang, Zhanyu Ma</strong></p>
<p>Accurately grounding regions of interest (ROIs) is critical for diagnosis and treatment planning in medical imaging. While multimodal large language models (MLLMs) combine visual perception with natural language, current medical-grounding pipelines still rely on supervised fine-tuning with explicit spatial hints, making them ill-equipped to handle the implicit queries common in clinical practice. This work makes three core contributions. We first define Unified Medical Reasoning Grounding (UMRG), a novel vision-language task that demands clinical reasoning and pixel-level grounding. Second, we release U-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside implicit clinical queries and reasoning traces, spanning 10 modalities, 15 super-categories, and 108 specific categories. Finally, we introduce MedReasoner, a modular framework that distinctly separates reasoning from segmentation: an MLLM reasoner is optimized with reinforcement learning, while a frozen segmentation expert converts spatial prompts into masks, with alignment achieved through format and accuracy rewards. MedReasoner achieves state-of-the-art performance on U-MRG-14K and demonstrates strong generalization to unseen clinical queries, underscoring the significant promise of reinforcement learning for interpretable medical grounding. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒä¸­ï¼Œå‡†ç¡®åœ°å¯¹æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰è¿›è¡Œå®šä½å¯¹äºè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’çš„åˆ¶å®šè‡³å…³é‡è¦ã€‚è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç»“åˆäº†è§†è§‰æ„ŸçŸ¥ä¸è‡ªç„¶è¯­è¨€ï¼Œä½†å½“å‰çš„åŒ»å­¦å®šä½æµç¨‹ä»ç„¶ä¾èµ–äºå…·æœ‰æ˜ç¡®ç©ºé—´æç¤ºçš„ç›‘ç£å¾®è°ƒï¼Œè¿™ä½¿å¾—å®ƒä»¬éš¾ä»¥åº”å¯¹ä¸´åºŠå®è·µä¸­çš„éšå¼æŸ¥è¯¢ã€‚æœ¬æ–‡åšå‡ºäº†ä¸‰é¡¹æ ¸å¿ƒè´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰äº†ç»Ÿä¸€åŒ»å­¦æ¨ç†å®šä½ï¼ˆUMRGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€ä»»åŠ¡ï¼Œéœ€è¦ä¸´åºŠæ¨ç†å’Œåƒç´ çº§å®šä½ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å‘å¸ƒäº†U-MRG-14Kæ•°æ®é›†ï¼ŒåŒ…å«14Kä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½æœ‰åƒç´ çº§æ©è†œã€éšå¼ä¸´åºŠæŸ¥è¯¢å’Œæ¨ç†è½¨è¿¹ï¼Œæ¶µç›–10ç§æ¨¡æ€ã€15ç§è¶…çº§ç±»åˆ«å’Œ108ç§ç‰¹å®šç±»åˆ«ã€‚æœ€åï¼Œæˆ‘ä»¬æ¨å‡ºäº†MedReasonerï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œå°†æ¨ç†ä¸åˆ†å‰²åŒºåˆ†å¼€æ¥ï¼šMLLMæ¨ç†å™¨é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ï¼Œè€Œå†»ç»“çš„åˆ†å‰²ä¸“å®¶å°†ç©ºé—´æç¤ºè½¬æ¢ä¸ºæ©è†œï¼Œé€šè¿‡æ ¼å¼å’Œç²¾åº¦å¥–åŠ±å®ç°å¯¹é½ã€‚MedReasoneråœ¨U-MRG-14Kä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å¯¹æœªè§è¿‡çš„ä¸´åºŠæŸ¥è¯¢è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¿™çªæ˜¾äº†å¼ºåŒ–å­¦ä¹ åœ¨å¯è§£é‡Šçš„åŒ»å­¦å®šä½ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08177v1">PDF</a> 37 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ç»Ÿä¸€åŒ»å­¦æ¨ç†å®šä½ï¼ˆUMRGï¼‰è¿™ä¸€æ–°å‹è§†è§‰è¯­è¨€ä»»åŠ¡ï¼Œå¹¶å‘å¸ƒäº†U-MRG-14Kæ•°æ®é›†å’ŒMedReasoneræ¡†æ¶ã€‚UMRGè¦æ±‚ä¸´åºŠæ¨ç†å’Œåƒç´ çº§å®šä½ã€‚MedReasoneré€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–MLLMæ¨ç†å™¨ï¼Œå°†ç©ºé—´æç¤ºè½¬åŒ–ä¸ºæ©è†œï¼Œå®ç°æ ¼å¼å’Œç²¾åº¦å¥–åŠ±å¯¹é½ã€‚è¯¥æ–¹æ³•åœ¨U-MRG-14Kä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨æœªè§è¿‡çš„ä¸´åºŠæŸ¥è¯¢ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨åŒ»å­¦å®šä½ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ç»Ÿä¸€åŒ»å­¦æ¨ç†å®šä½ï¼ˆUMRGï¼‰ä»»åŠ¡ï¼Œç»“åˆäº†ä¸´åºŠæ¨ç†å’Œåƒç´ çº§å®šä½ã€‚</li>
<li>å‘å¸ƒäº†U-MRG-14Kæ•°æ®é›†ï¼ŒåŒ…å«14Kæ ·æœ¬ï¼Œæ¶µç›–10ç§æ¨¡æ€ã€15ä¸ªè¶…ç±»åˆ«å’Œ108ä¸ªå…·ä½“ç±»åˆ«çš„åƒç´ çº§æ©è†œå’Œéšæ€§ä¸´åºŠæŸ¥è¯¢åŠæ¨ç†è½¨è¿¹ã€‚</li>
<li>ä»‹ç»äº†MedReasoneræ¡†æ¶ï¼Œå°†æ¨ç†å’Œåˆ†å‰²ä»»åŠ¡åˆ†ç¦»ï¼Œä¼˜åŒ–MLLMæ¨ç†å™¨é€šè¿‡å¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>MedReasonerä½¿ç”¨å†»ç»“çš„åˆ†å‰²ä¸“å®¶å°†ç©ºé—´æç¤ºè½¬åŒ–ä¸ºæ©è†œã€‚</li>
<li>MedReasoneråœ¨U-MRG-14Kä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥æ–¹æ³•å±•ç¤ºäº†å¯¹æœªè§ä¸´åºŠæŸ¥è¯¢çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08177">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1f06984455fc0a851fbffcb0d6207f70.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a378eeee81f6ec3a4240577c60bd85dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27209ab38731dc4fccd7894dcb778d1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f1f90c911d27ca95264e0ad8d6239bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee783866ffd89bcf426bcca3b2ed5aec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11a3bb59d06b2e899f246a04a58dad60.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Power-Battery-Detection"><a href="#Power-Battery-Detection" class="headerlink" title="Power Battery Detection"></a>Power Battery Detection</h2><p><strong>Authors:Xiaoqi Zhao, Peiqian Cao, Lihe Zhang, Zonglei Feng, Hanqi Liu, Jiaming Zuo, Youwei Pang, Weisi Lin, Georges El Fakhri, Huchuan Lu, Xiaofeng Liu</strong></p>
<p>Power batteries are essential components in electric vehicles, where internal structural defects can pose serious safety risks. We conduct a comprehensive study on a new task, power battery detection (PBD), which aims to localize the dense endpoints of cathode and anode plates from industrial X-ray images for quality inspection. Manual inspection is inefficient and error-prone, while traditional vision algorithms struggle with densely packed plates, low contrast, scale variation, and imaging artifacts. To address this issue and drive more attention into this meaningful task, we present PBD5K, the first large-scale benchmark for this task, consisting of 5,000 X-ray images from nine battery types with fine-grained annotations and eight types of real-world visual interference. To support scalable and consistent labeling, we develop an intelligent annotation pipeline that combines image filtering, model-assisted pre-labeling, cross-verification, and layered quality evaluation. We formulate PBD as a point-level segmentation problem and propose MDCNeXt, a model designed to extract and integrate multi-dimensional structure clues including point, line, and count information from the plate itself. To improve discrimination between plates and suppress visual interference, MDCNeXt incorporates two state space modules. The first is a prompt-filtered module that learns contrastive relationships guided by task-specific prompts. The second is a density-aware reordering module that refines segmentation in regions with high plate density. In addition, we propose a distance-adaptive mask generation strategy to provide robust supervision under varying spatial distributions of anode and cathode positions. The source code and datasets will be publicly available at \href{<a target="_blank" rel="noopener" href="https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD%7D%7BPBD5K%7D">https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD}{PBD5K}</a>. </p>
<blockquote>
<p>åŠ¨åŠ›ç”µæ± æ˜¯ç”µåŠ¨æ±½è½¦ä¸­çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå…¶å†…éƒ¨ç»“æ„æ€§ç¼ºé™·å¯èƒ½å¸¦æ¥ä¸¥é‡çš„å®‰å…¨é£é™©ã€‚æˆ‘ä»¬å¯¹ä¸€é¡¹æ–°çš„ä»»åŠ¡â€”â€”åŠ¨åŠ›ç”µæ± æ£€æµ‹ï¼ˆPBDï¼‰è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œå…¶ç›®æ ‡æ˜¯ä»å·¥ä¸šXå°„çº¿å›¾åƒä¸­å®šä½é˜´æå’Œé˜³ææ¿çš„å¯†é›†ç«¯ç‚¹ï¼Œä»¥è¿›è¡Œè´¨é‡æ£€æµ‹ã€‚äººå·¥æ£€æµ‹æ•ˆç‡ä½ä¸‹ä¸”æ˜“å‡ºé”™ï¼Œè€Œä¼ ç»Ÿè§†è§‰ç®—æ³•åœ¨å¯†é›†æ’åˆ—çš„æ¿æã€ä½å¯¹æ¯”åº¦ã€å°ºåº¦å˜åŒ–å’Œå›¾åƒä¼ªå½±æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜å¹¶å¸å¼•æ›´å¤šæ³¨æ„åŠ›å…³æ³¨è¿™é¡¹æœ‰æ„ä¹‰çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PBD5Kï¼Œè¿™æ˜¯è¯¥ä»»åŠ¡çš„é¦–ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«æ¥è‡ªä¹ç§ç”µæ± ç±»å‹çš„5000å¼ Xå°„çº¿å›¾åƒï¼Œå…·æœ‰ç»†ç²’åº¦æ³¨é‡Šå’Œå…«ç§ç°å®ä¸–ç•Œçš„è§†è§‰å¹²æ‰°ç±»å‹ã€‚ä¸ºäº†æ”¯æŒå¯æ‰©å±•å’Œä¸€è‡´çš„æ ‡æ³¨ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ™ºèƒ½æ ‡æ³¨ç®¡é“ï¼Œç»“åˆäº†å›¾åƒè¿‡æ»¤ã€æ¨¡å‹è¾…åŠ©é¢„æ ‡æ³¨ã€äº¤å‰éªŒè¯å’Œåˆ†å±‚è´¨é‡è¯„ä¼°ã€‚æˆ‘ä»¬å°†PBDåˆ¶å®šä¸ºç‚¹çº§åˆ†å‰²é—®é¢˜ï¼Œå¹¶æå‡ºMDCNeXtæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ—¨åœ¨æå–å’Œæ•´åˆåŒ…æ‹¬ç‚¹ã€çº¿å’Œè®¡æ•°ä¿¡æ¯åœ¨å†…çš„å¤šç»´ç»“æ„çº¿ç´¢ã€‚ä¸ºäº†æé«˜æ¿ä¹‹é—´çš„è¾¨åˆ«èƒ½åŠ›å¹¶æŠ‘åˆ¶è§†è§‰å¹²æ‰°ï¼ŒMDCNeXtèå…¥äº†ä¸¤ç§çŠ¶æ€ç©ºé—´æ¨¡å—ã€‚ç¬¬ä¸€ä¸ªæ˜¯æç¤ºè¿‡æ»¤æ¨¡å—ï¼Œå®ƒå­¦ä¹ åœ¨ä»»åŠ¡ç‰¹å®šæç¤ºæŒ‡å¯¼ä¸‹å¯¹æ¯”å…³ç³»ã€‚ç¬¬äºŒä¸ªæ˜¯å¯†åº¦æ„ŸçŸ¥é‡æ–°æ’åºæ¨¡å—ï¼Œå®ƒèƒ½åœ¨é«˜æ¿å¯†åº¦åŒºåŸŸä¼˜åŒ–åˆ†å‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è·ç¦»è‡ªé€‚åº”æ©è†œç”Ÿæˆç­–ç•¥ï¼Œä»¥åœ¨é˜³æå’Œé˜´æä½ç½®ç©ºé—´åˆ†å¸ƒå˜åŒ–çš„æƒ…å†µä¸‹æä¾›ç¨³å¥çš„ç›‘ç£ã€‚æºä»£ç å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD">https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD</a>å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07797v1">PDF</a> Under submission to IEEE Transactions on Pattern Analysis and Machine   Intelligence (T-PAMI)</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ç ”ç©¶äº†ç”µåŠ¨è½¦è¾†ä¸­é‡è¦ç»„ä»¶åŠ¨åŠ›ç”µæ± çš„æ£€æµ‹ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†PBD5Kï¼Œæ—¨åœ¨ä»å·¥ä¸šXå°„çº¿å›¾åƒä¸­å®šä½é˜´æå’Œé˜³ææ¿çš„å¯†é›†ç«¯ç‚¹ä»¥è¿›è¡Œè´¨é‡æ£€æµ‹ã€‚æ–‡ç« è¿˜ä»‹ç»äº†æ™ºèƒ½æ ‡æ³¨ç®¡é“å’Œä¸€ä¸ªåä¸ºMDCNeXtçš„æ¨¡å‹ï¼Œç”¨äºè§£å†³æ‰‹åŠ¨æ£€æµ‹æ•ˆç‡ä½ä¸‹ã€ä¼ ç»Ÿè§†è§‰ç®—æ³•å¤„ç†å¯†é›†å‹ç”µæ± æ¿ç­‰é—®é¢˜ã€‚MDCNeXtå¯ä»¥æå–å¹¶æ•´åˆç‚¹ã€çº¿å’Œè®¡æ•°ç­‰å¤šç»´åº¦ç»“æ„çº¿ç´¢ä»¥æé«˜ç²¾åº¦å¹¶å‡å°‘å¹²æ‰°ã€‚ç›¸å…³æ•°æ®ä¸æºç å°†å…¬å¼€åœ¨é“¾æ¥åœ°å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æœ¬æ–‡ä»‹ç»äº†åŠ¨åŠ›ç”µæ± æ£€æµ‹ï¼ˆPBDï¼‰çš„é‡è¦æ€§åŠå…¶æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚å†…éƒ¨ç¼ºé™·æ£€æµ‹çš„å®‰å…¨é£é™©ã€‚</li>
<li>æå‡ºå¤§è§„æ¨¡æ•°æ®é›†PBD5Kï¼ŒåŒ…å«å¤šç§ç”µæ± ç±»å‹çš„Xå°„çº¿å›¾åƒåŠç²¾ç»†æ ‡æ³¨å’ŒçœŸå®ä¸–ç•Œè§†è§‰å¹²æ‰°ç±»å‹ã€‚</li>
<li>ä¸ºæ”¯æŒå¯æ‰©å±•å’Œä¸€è‡´çš„æ ‡æ³¨ï¼Œå¼€å‘äº†æ™ºèƒ½æ ‡æ³¨ç®¡é“ï¼ŒåŒ…æ‹¬å›¾åƒè¿‡æ»¤ã€æ¨¡å‹è¾…åŠ©é¢„æ ‡æ³¨ã€äº¤å‰éªŒè¯å’Œåˆ†å±‚è´¨é‡è¯„ä¼°ç­‰åŠŸèƒ½ã€‚</li>
<li>å°†PBDä»»åŠ¡å®šä¹‰ä¸ºç‚¹çº§åˆ†å‰²é—®é¢˜ï¼Œå¹¶æå‡ºMDCNeXtæ¨¡å‹æ¥è§£å†³å¯†é›†å‹ç”µæ± æ¿æ£€æµ‹é—®é¢˜ï¼Œæ•´åˆå¤šç»´åº¦ç»“æ„çº¿ç´¢å¹¶å¼•å…¥ä¸¤ä¸ªçŠ¶æ€ç©ºé—´æ¨¡å—æ¥å¢å¼ºé‰´åˆ«åŠ›å’ŒæŠ‘åˆ¶è§†è§‰å¹²æ‰°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07797">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cabdff275bf105ee5c80fea319e35b7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e78016e9d63ba782b324018362b1087.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-205bc1d7af3ff14b39271276ce0748a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dedcee2739e52752f66268f493dae2d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e65c41e21d2b5e1ab37c651d52a8f8f8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4d617c0b1e22a33ae9cc1e9e517fd588.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="UniSVG-A-Unified-Dataset-for-Vector-Graphic-Understanding-and-Generation-with-Multimodal-Large-Language-Models"><a href="#UniSVG-A-Unified-Dataset-for-Vector-Graphic-Understanding-and-Generation-with-Multimodal-Large-Language-Models" class="headerlink" title="UniSVG: A Unified Dataset for Vector Graphic Understanding and   Generation with Multimodal Large Language Models"></a>UniSVG: A Unified Dataset for Vector Graphic Understanding and   Generation with Multimodal Large Language Models</h2><p><strong>Authors:Jinke Li, Jiarui Yu, Chenxing Wei, Hande Dong, Qiang Lin, Liangjing Yang, Zhicai Wang, Yanbin Hao</strong></p>
<p>Unlike bitmap images, scalable vector graphics (SVG) maintain quality when scaled, frequently employed in computer vision and artistic design in the representation of SVG code. In this era of proliferating AI-powered systems, enabling AI to understand and generate SVG has become increasingly urgent. However, AI-driven SVG understanding and generation (U&amp;G) remain significant challenges. SVG code, equivalent to a set of curves and lines controlled by floating-point parameters, demands high precision in SVG U&amp;G. Besides, SVG generation operates under diverse conditional constraints, including textual prompts and visual references, which requires powerful multi-modal processing for condition-to-SVG transformation. Recently, the rapid growth of Multi-modal Large Language Models (MLLMs) have demonstrated capabilities to process multi-modal inputs and generate complex vector controlling parameters, suggesting the potential to address SVG U&amp;G tasks within a unified model. To unlock MLLMâ€™s capabilities in the SVG area, we propose an SVG-centric dataset called UniSVG, comprising 525k data items, tailored for MLLM training and evaluation. To our best knowledge, it is the first comprehensive dataset designed for unified SVG generation (from textual prompts and images) and SVG understanding (color, category, usage, etc.). As expected, learning on the proposed dataset boosts open-source MLLMsâ€™ performance on various SVG U&amp;G tasks, surpassing SOTA close-source MLLMs like GPT-4V. We release dataset, benchmark, weights, codes and experiment details on <a target="_blank" rel="noopener" href="https://ryanlijinke.github.io/">https://ryanlijinke.github.io/</a>. </p>
<blockquote>
<p>ä¸ä½å›¾å›¾åƒä¸åŒï¼Œå¯ç¼©æ”¾çŸ¢é‡å›¾å½¢ï¼ˆSVGï¼‰åœ¨ç¼©æ”¾æ—¶èƒ½ä¿æŒè´¨é‡ä¸å˜ï¼Œå¸¸è¢«ç”¨äºè®¡ç®—æœºè§†è§‰å’Œè‰ºæœ¯è®¾è®¡ä¸­çš„SVGä»£ç è¡¨ç¤ºã€‚åœ¨è¿™ä¸ªAIç³»ç»Ÿæ—¥ç›Šå¢å¤šçš„æ—¶ä»£ï¼Œè®©AIç†è§£å’Œç”ŸæˆSVGå˜å¾—æ—¥ç›Šç´§è¿«ã€‚ç„¶è€Œï¼ŒAIé©±åŠ¨çš„SVGç†è§£å’Œç”Ÿæˆï¼ˆU&amp;Gï¼‰ä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚SVGä»£ç ç›¸å½“äºç”±æµ®ç‚¹å‚æ•°æ§åˆ¶çš„æ›²çº¿å’Œçº¿æ¡é›†åˆï¼Œè¦æ±‚åœ¨SVG U&amp;Gä¸­å…·æœ‰é«˜ç²¾åº¦ã€‚æ­¤å¤–ï¼ŒSVGç”Ÿæˆå—åˆ°å„ç§æ¡ä»¶çº¦æŸçš„å½±å“ï¼ŒåŒ…æ‹¬æ–‡æœ¬æç¤ºå’Œè§†è§‰å‚è€ƒï¼Œè¿™éœ€è¦å¼ºå¤§çš„å¤šæ¨¡æ€å¤„ç†æ¥å®ç°æ¡ä»¶åˆ°SVGçš„è½¬æ¢ã€‚æœ€è¿‘ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•æ˜¾ç¤ºå‡ºå¤„ç†å¤šæ¨¡æ€è¾“å…¥å’Œç”Ÿæˆå¤æ‚çŸ¢é‡æ§åˆ¶å‚æ•°çš„èƒ½åŠ›ï¼Œè¡¨æ˜æœ‰å¯èƒ½åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­è§£å†³SVG U&amp;Gä»»åŠ¡ã€‚ä¸ºäº†è§£é”MLLMåœ¨SVGé¢†åŸŸçš„æ½œåŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä»¥SVGä¸ºä¸­å¿ƒçš„æ•°æ®é›†ï¼Œåä¸ºUniSVGï¼ŒåŒ…å«52.5ä¸‡æ¡æ•°æ®é¡¹ï¼Œä¸“ä¸ºMLLMè®­ç»ƒå’Œè¯„ä¼°è€Œè®¾è®¡ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªä¸ºç»Ÿä¸€SVGç”Ÿæˆï¼ˆä»æ–‡æœ¬æç¤ºå’Œå›¾åƒï¼‰å’ŒSVGç†è§£ï¼ˆé¢œè‰²ã€ç±»åˆ«ã€ç”¨é€”ç­‰ï¼‰è€Œè®¾è®¡çš„ç»¼åˆæ•°æ®é›†ã€‚æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œåœ¨è¯¥æ•°æ®é›†ä¸Šè¿›è¡Œå­¦ä¹ æé«˜äº†å¼€æºMLLMåœ¨å„ç§SVG U&amp;Gä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†æœ€æ–°ä¸“æœ‰MLLMså¦‚GPT-4Vç­‰ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://ryanlijinke.github.io/">https://ryanlijinke.github.io/</a>ä¸Šå‘å¸ƒäº†æ•°æ®é›†ã€åŸºå‡†æµ‹è¯•ã€æƒé‡ã€ä»£ç å’Œå®éªŒç»†èŠ‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07766v1">PDF</a> Accepted at ACM MM 2025 Dataset Track</p>
<p><strong>Summary</strong></p>
<p>åœ¨AIé©±åŠ¨çš„ç³»ç»Ÿä¸æ–­æ™®åŠçš„æ—¶ä»£ï¼Œå¯ä¼¸ç¼©çŸ¢é‡å›¾å½¢ï¼ˆSVGï¼‰çš„ç†è§£ä¸ç”Ÿæˆï¼ˆUï¼†Gï¼‰æˆä¸ºé‡è¦çš„æŒ‘æˆ˜ã€‚è¿‘æœŸï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤šæ¨¡æ€è¾“å…¥å’Œç”Ÿæˆå¤æ‚çš„çŸ¢é‡æ§åˆ¶å‚æ•°æ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚ä¸ºäº†è§£é”MLLMåœ¨SVGé¢†åŸŸçš„æ½œåŠ›ï¼Œæå‡ºäº†ä¸€ç§åä¸ºUniSVGçš„SVGä¸“ç”¨æ•°æ®é›†ï¼ŒåŒ…å«52.5ä¸‡é¡¹æ•°æ®ï¼Œé€‚åˆç”¨äºMLLMçš„è®­ç»ƒå’Œè¯„ä¼°ï¼Œå¹¶æœŸæœ›èƒ½æé«˜å¼€æºMLLMåœ¨å„ç§SVG Uï¼†Gä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIç†è§£å’Œç”ŸæˆSVGå…·æœ‰é‡è¦çš„æŒ‘æˆ˜æ€§å’Œç´§è¿«æ€§ã€‚</li>
<li>SVGä»£ç éœ€è¦é«˜ç²¾åº¦å¤„ç†ï¼Œä¸”å…¶ç”Ÿæˆå—åˆ°å¤šç§æ¡ä»¶çº¦æŸã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èƒ½å¤Ÿå¤„ç†å¤šæ¨¡æ€è¾“å…¥å¹¶ç”Ÿæˆå¤æ‚çš„çŸ¢é‡æ§åˆ¶å‚æ•°ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªåä¸ºUniSVGçš„SVGä¸“ç”¨æ•°æ®é›†ï¼Œç”¨äºMLLMçš„è®­ç»ƒå’Œè¯„ä¼°ã€‚</li>
<li>UniSVGæ•°æ®é›†æ˜¯é¦–ä¸ªç»Ÿä¸€SVGç”Ÿæˆï¼ˆä»æ–‡æœ¬æç¤ºå’Œå›¾åƒï¼‰å’ŒSVGç†è§£çš„ç»¼åˆæ•°æ®é›†ã€‚</li>
<li>åœ¨UniSVGæ•°æ®é›†ä¸Šè®­ç»ƒå¯ä»¥æé«˜å¼€æºMLLMåœ¨å¤šç§SVG Uï¼†Gä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07766">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5422aa9cf49fe7a2867eadb1f4a648b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bcf06356b922803a0734de0be9f0ee9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a92e6204d44e698ee22021cc66503dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12a9503f3015e36f307f40c98ea3e684.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55453523c243b547dfa5c5a696685e6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c596bdf836fcffbc74c12d1960f95aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e060874f6e7acab9333caad3a46fad40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f880516d42e873a97d87037fc01499d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Importance-Aware-Semantic-Communication-in-MIMO-OFDM-Systems-Using-Vision-Transformer"><a href="#Importance-Aware-Semantic-Communication-in-MIMO-OFDM-Systems-Using-Vision-Transformer" class="headerlink" title="Importance-Aware Semantic Communication in MIMO-OFDM Systems Using   Vision Transformer"></a>Importance-Aware Semantic Communication in MIMO-OFDM Systems Using   Vision Transformer</h2><p><strong>Authors:Joohyuk Park, Yongjeong Oh, Jihun Park, Yo-Seb Jeon</strong></p>
<p>This paper presents a novel importance-aware quantization, subcarrier mapping, and power allocation (IA-QSMPA) framework for semantic communication in multiple-input multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) systems, empowered by a pretrained Vision Transformer (ViT). The proposed framework exploits attention-based importance extracted from a pretrained ViT to jointly optimize quantization levels, subcarrier mapping, and power allocation. Specifically, IA-QSMPA maps semantically important features to high-quality subchannels and allocates resources in accordance with their contribution to task performance and communication latency. To efficiently solve the resulting nonconvex optimization problem, a block coordinate descent algorithm is employed. The framework is further extended to operate under finite blocklength transmission, where communication errors may occur. In this setting, a segment-wise linear approximation of the channel dispersion penalty is introduced to enable efficient joint optimization under practical constraints. Simulation results on a multi-view image classification task using the MVP-N dataset demonstrate that IA-QSMPA significantly outperforms conventional methods in both ideal and finite blocklength transmission scenarios, achieving superior task performance and communication efficiency. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„å¤šè¾“å…¥å¤šè¾“å‡ºæ­£äº¤é¢‘åˆ†å¤ç”¨ï¼ˆMIMO-OFDMï¼‰ç³»ç»Ÿä¸­çš„è¯­ä¹‰é€šä¿¡çš„æ–°å‹é‡è¦æ€§æ„ŸçŸ¥é‡åŒ–ã€å­è½½æ³¢æ˜ å°„å’ŒåŠŸç‡åˆ†é…ï¼ˆIA-QSMPAï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„ViTçš„æ³¨æ„åŠ›æœºåˆ¶æå–çš„é‡è¦æ€§ï¼Œè”åˆä¼˜åŒ–é‡åŒ–çº§åˆ«ã€å­è½½æ³¢æ˜ å°„å’ŒåŠŸç‡åˆ†é…ã€‚å…·ä½“æ¥è¯´ï¼ŒIA-QSMPAå°†è¯­ä¹‰é‡è¦ç‰¹å¾æ˜ å°„åˆ°é«˜è´¨é‡å­é€šé“ä¸Šï¼Œå¹¶æ ¹æ®å…¶å¯¹ä»»åŠ¡æ€§èƒ½å’Œé€šä¿¡å»¶è¿Ÿçš„è´¡çŒ®æ¥åˆ†é…èµ„æºã€‚ä¸ºäº†æœ‰æ•ˆåœ°è§£å†³ç”±æ­¤äº§ç”Ÿçš„éå‡¸ä¼˜åŒ–é—®é¢˜ï¼Œé‡‡ç”¨äº†å—åæ ‡ä¸‹é™ç®—æ³•ã€‚è¯¥æ¡†æ¶è¿›ä¸€æ­¥æ‰©å±•åˆ°æœ‰é™å—é•¿ä¼ è¾“çš„æƒ…å†µä¸‹è¿è¡Œï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹å¯èƒ½ä¼šå‘ç”Ÿé€šä¿¡é”™è¯¯ã€‚åœ¨æ­¤è®¾ç½®ä¸­ï¼Œå¼•å…¥äº†ä¿¡é“å¼¥æ•£ç½šåˆ†çš„åˆ†æ®µçº¿æ€§è¿‘ä¼¼ï¼Œä»¥åœ¨å®ç”¨çº¦æŸä¸‹å®ç°æœ‰æ•ˆçš„è”åˆä¼˜åŒ–ã€‚ä½¿ç”¨MVP-Næ•°æ®é›†è¿›è¡Œçš„å¤šè§†å›¾å›¾åƒåˆ†ç±»ä»»åŠ¡çš„ä»¿çœŸç»“æœè¡¨æ˜ï¼ŒIA-QSMPAåœ¨ç†æƒ³å’Œæœ‰é™å—é•¿ä¼ è¾“åœºæ™¯ä¸­å‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå®ç°äº†å“è¶Šçš„ä»»åŠ¡æ€§èƒ½å’Œé€šä¿¡æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07696v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºé¢„è®­ç»ƒVision Transformerï¼ˆViTï¼‰çš„é‡è¦æ€§æ„ŸçŸ¥é‡åŒ–ã€å­è½½æ³¢æ˜ å°„å’ŒåŠŸç‡åˆ†é…ï¼ˆIA-QSMPAï¼‰æ¡†æ¶ï¼Œç”¨äºMIMO-OFDMç³»ç»Ÿä¸­çš„è¯­ä¹‰é€šä¿¡ã€‚è¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒViTçš„æ³¨æ„åŠ›æœºåˆ¶æå–é‡è¦æ€§ï¼Œä»¥è”åˆä¼˜åŒ–é‡åŒ–çº§åˆ«ã€å­è½½æ³¢æ˜ å°„å’ŒåŠŸç‡åˆ†é…ã€‚IA-QSMPAå°†è¯­ä¹‰é‡è¦ç‰¹å¾æ˜ å°„åˆ°é«˜è´¨é‡å­é€šé“ï¼Œå¹¶æ ¹æ®å…¶å¯¹ä»»åŠ¡æ€§èƒ½å’Œé€šä¿¡å»¶è¿Ÿçš„è´¡çŒ®åˆ†é…èµ„æºã€‚é‡‡ç”¨å—åæ ‡ä¸‹é™ç®—æ³•æœ‰æ•ˆåœ°è§£å†³äº†ç”±æ­¤äº§ç”Ÿçš„éå‡¸ä¼˜åŒ–é—®é¢˜ã€‚è¯¥æ¡†æ¶è¿˜æ‰©å±•åˆ°æœ‰é™å—é•¿ä¼ è¾“æƒ…å†µä¸‹è¿è¡Œï¼Œåœ¨æ­¤æƒ…å†µä¸‹å¼•å…¥åˆ†æ®µçº¿æ€§è¿‘ä¼¼ä¿¡é“æ‰©æ•£æƒ©ç½šï¼Œä»¥åœ¨å®ç”¨çº¦æŸä¸‹å®ç°æœ‰æ•ˆçš„è”åˆä¼˜åŒ–ã€‚æ¨¡æ‹Ÿç»“æœè¡¨æ˜ï¼Œåœ¨ç†æƒ³å’Œæœ‰é™å—é•¿ä¼ è¾“åœºæ™¯ä¸‹ï¼ŒIA-QSMPAåœ¨å¤šè§†å›¾å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå®ç°äº†å‡ºè‰²çš„ä»»åŠ¡æ€§èƒ½å’Œé€šä¿¡æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒVision Transformerï¼ˆViTï¼‰çš„IA-QSMPAæ¡†æ¶ï¼Œç”¨äºMIMO-OFDMç³»ç»Ÿä¸­çš„è¯­ä¹‰é€šä¿¡ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶æå–é‡è¦æ€§ï¼Œå¹¶è”åˆä¼˜åŒ–é‡åŒ–çº§åˆ«ã€å­è½½æ³¢æ˜ å°„å’ŒåŠŸç‡åˆ†é…ã€‚</li>
<li>IA-QSMPAå°†è¯­ä¹‰é‡è¦ç‰¹å¾æ˜ å°„åˆ°é«˜è´¨é‡å­é€šé“ï¼Œå¹¶æ ¹æ®ä»»åŠ¡æ€§èƒ½å’Œé€šä¿¡å»¶è¿Ÿçš„è´¡çŒ®åˆ†é…èµ„æºã€‚</li>
<li>é‡‡ç”¨å—åæ ‡ä¸‹é™ç®—æ³•è§£å†³éå‡¸ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>æ¡†æ¶æ‰©å±•åˆ°æœ‰é™å—é•¿ä¼ è¾“æƒ…å†µï¼Œå¼•å…¥åˆ†æ®µçº¿æ€§è¿‘ä¼¼ä¿¡é“æ‰©æ•£æƒ©ç½šä»¥å®ç°æœ‰æ•ˆä¼˜åŒ–ã€‚</li>
<li>æ¨¡æ‹Ÿç»“æœè¡¨æ˜ï¼ŒIA-QSMPAåœ¨ç†æƒ³å’Œæœ‰é™å—é•¿ä¼ è¾“åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07696">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cf6e5b0415effd86dd732d0c0fe58c67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d89a828e80bdf191cee6e8a134311e7a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MobileViCLIP-An-Efficient-Video-Text-Model-for-Mobile-Devices"><a href="#MobileViCLIP-An-Efficient-Video-Text-Model-for-Mobile-Devices" class="headerlink" title="MobileViCLIP: An Efficient Video-Text Model for Mobile Devices"></a>MobileViCLIP: An Efficient Video-Text Model for Mobile Devices</h2><p><strong>Authors:Min Yang, Zihan Jia, Zhilin Dai, Sheng Guo, Limin Wang</strong></p>
<p>Efficient lightweight neural networks are with increasing attention due to their faster reasoning speed and easier deployment on mobile devices. However, existing video pre-trained models still focus on the common ViT architecture with high latency, and few works attempt to build efficient architecture on mobile devices. This paper bridges this gap by introducing temporal structural reparameterization into an efficient image-text model and training it on a large-scale high-quality video-text dataset, resulting in an efficient video-text model that can run on mobile devices with strong zero-shot classification and retrieval capabilities, termed as MobileViCLIP. In particular, in terms of inference speed on mobile devices, our MobileViCLIP-Small is 55.4x times faster than InternVideo2-L14 and 6.7x faster than InternVideo2-S14. In terms of zero-shot retrieval performance, our MobileViCLIP-Small obtains similar performance as InternVideo2-L14 and obtains 6.9% better than InternVideo2-S14 on MSR-VTT. The code is available at <a target="_blank" rel="noopener" href="https://github.com/MCG-NJU/MobileViCLIP">https://github.com/MCG-NJU/MobileViCLIP</a>. </p>
<blockquote>
<p>é«˜æ•ˆè½»é‡çº§ç¥ç»ç½‘ç»œå› å…¶æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„æ›´å®¹æ˜“éƒ¨ç½²è€Œå—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†é¢‘é¢„è®­ç»ƒæ¨¡å‹ä»ç„¶ä¸»è¦å…³æ³¨é«˜å»¶è¿Ÿçš„é€šç”¨ViTæ¶æ„ï¼Œå¾ˆå°‘æœ‰å·¥ä½œå°è¯•åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šæ„å»ºé«˜æ•ˆæ¶æ„ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥æ—¶é—´ç»“æ„é‡å‚æ•°åŒ–åˆ°ä¸€ä¸ªé«˜æ•ˆçš„å›¾åƒæ–‡æœ¬æ¨¡å‹ä¸­ï¼Œå¹¶åœ¨å¤§è§„æ¨¡é«˜è´¨é‡çš„è§†é¢‘æ–‡æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»è€Œå¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚è¿™å¯¼è‡´äº†ä¸€ä¸ªèƒ½åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šè¿è¡Œçš„é«˜æ•ˆè§†é¢‘æ–‡æœ¬æ¨¡å‹ï¼Œå…·æœ‰å¼ºçƒˆçš„é›¶æ ·æœ¬åˆ†ç±»å’Œæ£€ç´¢èƒ½åŠ›ï¼Œè¢«ç§°ä¸ºMobileViCLIPã€‚ç‰¹åˆ«åœ°ï¼Œåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„æ¨ç†é€Ÿåº¦æ–¹é¢ï¼Œæˆ‘ä»¬çš„MobileViCLIP-Smallæ˜¯InternVideo2-L14çš„55.4å€ï¼Œæ¯”InternVideo2-S14å¿«6.7å€ã€‚åœ¨é›¶æ ·æœ¬æ£€ç´¢æ€§èƒ½æ–¹é¢ï¼Œæˆ‘ä»¬çš„MobileViCLIP-Smallä¸InternVideo2-L14è¡¨ç°ç›¸ä¼¼ï¼Œå¹¶åœ¨MSR-VTTä¸Šæ¯”InternVideo2-S14é«˜å‡º6.9%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MCG-NJU/MobileViCLIP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/MCG-NJU/MobileViCLIPæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07312v1">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§é«˜æ•ˆçš„è§†é¢‘æ–‡æœ¬æ¨¡å‹MobileViCLIPï¼Œé€šè¿‡å¼•å…¥æ—¶é—´ç»“æ„é‡å‚æ•°åŒ–ï¼Œåœ¨å¤§å‹é«˜è´¨é‡è§†é¢‘æ–‡æœ¬æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå¯åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šè¿è¡Œï¼Œå…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬åˆ†ç±»å’Œæ£€ç´¢èƒ½åŠ›ã€‚MobileViCLIP-Smallåœ¨æ¨ç†é€Ÿåº¦å’Œé›¶æ ·æœ¬æ£€ç´¢æ€§èƒ½ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºå…¶ä»–æ¨¡å‹æœ‰æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæ›´å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶è§£å†³äº†ç°æœ‰è§†é¢‘é¢„è®­ç»ƒæ¨¡å‹ä¸»è¦é›†ä¸­åœ¨é«˜å»¶è¿Ÿçš„é€šç”¨ViTæ¶æ„ä¸Šï¼Œè€Œé’ˆå¯¹ç§»åŠ¨è®¾å¤‡çš„é«˜æ•ˆæ¶æ„ç ”ç©¶è¾ƒå°‘çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¼•å…¥æ—¶é—´ç»“æ„é‡å‚æ•°åŒ–ï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è§†é¢‘æ–‡æœ¬æ¨¡å‹MobileViCLIPã€‚</li>
<li>MobileViCLIPå¯åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šè¿è¡Œï¼Œå…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬åˆ†ç±»å’Œæ£€ç´¢èƒ½åŠ›ã€‚</li>
<li>MobileViCLIP-Smallåœ¨æ¨ç†é€Ÿåº¦ä¸Šç›¸è¾ƒäºå…¶ä»–æ¨¡å‹æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¾‹å¦‚ï¼Œå®ƒçš„æ¨ç†é€Ÿåº¦æ˜¯InternVideo2-L14çš„55.4å€ï¼Œæ˜¯InternVideo2-S14çš„6.7å€ã€‚</li>
<li>åœ¨é›¶æ ·æœ¬æ£€ç´¢æ€§èƒ½æ–¹é¢ï¼ŒMobileViCLIP-Smallçš„è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºInternVideo2-L14è¡¨ç°ç›¸å½“ï¼Œå¹¶åœ¨MSR-VTTä¸Šæ¯”InternVideo2-S14é«˜å‡º6.9%ã€‚</li>
<li>è¯¥æ¨¡å‹çš„ä»£ç å·²å…¬å¼€ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œè¿›ä¸€æ­¥æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07312">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f930c3e462d5fbcb04bf02e815792c83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45a9542d034eeb9992611e65bb053cce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c4d249f795ec902a660c011457fda89.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf1c7cbe4eab299b089399db6f694910.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-78f302a0a253541cea3661bfc9b0334f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Dynamic-Pattern-Alignment-Learning-for-Pretraining-Lightweight-Human-Centric-Vision-Models"><a href="#Dynamic-Pattern-Alignment-Learning-for-Pretraining-Lightweight-Human-Centric-Vision-Models" class="headerlink" title="Dynamic Pattern Alignment Learning for Pretraining Lightweight   Human-Centric Vision Models"></a>Dynamic Pattern Alignment Learning for Pretraining Lightweight   Human-Centric Vision Models</h2><p><strong>Authors:Xuanhan Wang, Huimin Deng, Ke Liu, Jun Wang, Lianli Gao, Jingkuan Song</strong></p>
<p>Human-centric vision models (HVMs) have achieved remarkable generalization due to large-scale pretraining on massive person images. However, their dependence on large neural architectures and the restricted accessibility of pretraining data significantly limits their practicality in real-world applications. To address this limitation, we propose Dynamic Pattern Alignment Learning (DPAL), a novel distillation-based pretraining framework that efficiently trains lightweight HVMs to acquire strong generalization from large HVMs. In particular, human-centric visual perception are highly dependent on three typical visual patterns, including global identity pattern, local shape pattern and multi-person interaction pattern. To achieve generalizable lightweight HVMs, we firstly design a dynamic pattern decoder (D-PaDe), acting as a dynamic Mixture of Expert (MoE) model. It incorporates three specialized experts dedicated to adaptively extract typical visual patterns, conditioned on both input image and pattern queries. And then, we present three levels of alignment objectives, which aims to minimize generalization gap between lightweight HVMs and large HVMs at global image level, local pixel level, and instance relation level. With these two deliberate designs, the DPAL effectively guides lightweight model to learn all typical human visual patterns from large HVMs, which can generalize to various human-centric vision tasks. Extensive experiments conducted on 15 challenging datasets demonstrate the effectiveness of the DPAL. Remarkably, when employing PATH-B as the teacher, DPAL-ViT&#x2F;Ti (5M parameters) achieves surprising generalizability similar to existing large HVMs such as PATH-B (84M) and Sapiens-L (307M), and outperforms previous distillation-based pretraining methods including Proteus-ViT&#x2F;Ti (5M) and TinyMiM-ViT&#x2F;Ti (5M) by a large margin. </p>
<blockquote>
<p>äººç±»ä¸ºä¸­å¿ƒçš„è§†è§‰æ¨¡å‹ï¼ˆHVMsï¼‰ç”±äºå¤§è§„æ¨¡é¢„è®­ç»ƒåœ¨å¤§é‡äººåƒä¸Šçš„åº”ç”¨ï¼Œå·²ç»å®ç°äº†æ˜¾è‘—çš„æ³›åŒ–ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹å¤§å‹ç¥ç»ç½‘ç»œæ¶æ„çš„ä¾èµ–ä»¥åŠé¢„è®­ç»ƒæ•°æ®è·å–çš„é™åˆ¶ï¼Œæ˜¾è‘—é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€æ¨¡å¼å¯¹é½å­¦ä¹ ï¼ˆDPALï¼‰è¿™ç§åŸºäºè’¸é¦çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œå®ƒèƒ½æœ‰æ•ˆåœ°è®­ç»ƒè½»é‡çº§HVMsï¼Œä»å¤§å‹HVMsä¸­è·å¾—å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ï¼Œäººç±»ä¸ºä¸­å¿ƒçš„è§†è§‰æ„ŸçŸ¥é«˜åº¦ä¾èµ–äºä¸‰ç§å…¸å‹çš„è§†è§‰æ¨¡å¼ï¼ŒåŒ…æ‹¬å…¨å±€èº«ä»½æ¨¡å¼ã€å±€éƒ¨å½¢çŠ¶æ¨¡å¼å’Œå¤šäººç‰©äº¤äº’æ¨¡å¼ã€‚ä¸ºäº†å®ç°å¯æ³›åŒ–çš„è½»é‡çº§HVMsï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ä¸ªåŠ¨æ€æ¨¡å¼è§£ç å™¨ï¼ˆD-PaDeï¼‰ï¼Œå®ƒä½œä¸ºä¸€ä¸ªåŠ¨æ€çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ã€‚å®ƒç»“åˆäº†ä¸‰ä¸ªä¸“ä¸šä¸“å®¶ï¼Œæ—¨åœ¨æ ¹æ®è¾“å…¥å›¾åƒå’Œæ¨¡å¼æŸ¥è¯¢è‡ªé€‚åº”åœ°æå–å…¸å‹è§†è§‰æ¨¡å¼ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªçº§åˆ«çš„å¯¹é½ç›®æ ‡ï¼Œæ—¨åœ¨æœ€å°åŒ–è½»é‡çº§HVMså’Œå¤§å‹HVMsåœ¨å…¨å±€å›¾åƒçº§åˆ«ã€å±€éƒ¨åƒç´ çº§åˆ«å’Œå®ä¾‹å…³ç³»çº§åˆ«çš„æ³›åŒ–å·®è·ã€‚é€šè¿‡è¿™ä¸¤ä¸ªç²¾å¿ƒè®¾è®¡ï¼ŒDPALæœ‰æ•ˆåœ°å¼•å¯¼è½»é‡çº§æ¨¡å‹ä»å¤§å‹HVMså­¦ä¹ æ‰€æœ‰å…¸å‹çš„äººç±»è§†è§‰æ¨¡å¼ï¼Œå¯ä»¥æ³›åŒ–åˆ°å„ç§ä»¥äººç±»ä¸ºä¸­å¿ƒçš„è§†è§‰ä»»åŠ¡ã€‚åœ¨15ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜äº†DPALçš„æœ‰æ•ˆæ€§ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå½“ä½¿ç”¨PATH-Bä½œä¸ºæ•™å¸ˆæ—¶ï¼ŒDPAL-ViT&#x2F;Tiï¼ˆ5Må‚æ•°ï¼‰å®ç°äº†ä¸ç°æœ‰å¤§å‹HVMsï¼ˆå¦‚PATH-Bï¼ˆ84Mï¼‰å’ŒSapiens-Lï¼ˆ307Mï¼‰ï¼‰ç›¸ä¼¼çš„æƒŠäººæ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å¤§å¤§è¶…è¶Šäº†ä¹‹å‰çš„åŸºäºè’¸é¦çš„é¢„è®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬Proteus-ViT&#x2F;Tiï¼ˆ5Mï¼‰å’ŒTinyMiM-ViT&#x2F;Tiï¼ˆ5Mï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07144v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹äººä¸­å¿ƒè§†è§‰æ¨¡å‹ï¼ˆHVMsï¼‰åœ¨å®é™…åº”ç”¨ä¸­çš„å±€é™æ€§ï¼Œæå‡ºåŠ¨æ€æ¨¡å¼å¯¹é½å­¦ä¹ ï¼ˆDPALï¼‰æ¡†æ¶ã€‚é€šè¿‡è®¾è®¡åŠ¨æ€æ¨¡å¼è§£ç å™¨ï¼ˆD-PaDeï¼‰å’Œä¸‰ä¸ªå±‚æ¬¡çš„å¯¹é½ç›®æ ‡ï¼Œå®ç°ä»å°å‹æ¨¡å‹å‘å¤§å‹HVMsçš„æœ‰æ•ˆå­¦ä¹ ï¼Œæé«˜äº†è½»å‹HVMsçš„é€šç”¨æ€§ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†DPALçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººä¸­å¿ƒè§†è§‰æ¨¡å‹ï¼ˆHVMsï¼‰é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒå®ç°æ˜¾è‘—æ³›åŒ–ï¼Œä½†å—é™äºå¤§å‹ç¥ç»ç½‘ç»œæ¶æ„å’Œé¢„è®­ç»ƒæ•°æ®å¯è®¿é—®æ€§ï¼Œåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å®ç”¨æ€§å—é™ã€‚</li>
<li>æå‡ºåŠ¨æ€æ¨¡å¼å¯¹é½å­¦ä¹ ï¼ˆDPALï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è®­ç»ƒè½»é‡çº§çš„äººä¸­å¿ƒè§†è§‰æ¨¡å‹ï¼ˆHVMsï¼‰ï¼Œä½¿å…¶ä»å¤§å‹HVMsä¸­æœ‰æ•ˆè·å–å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>DPALæ¡†æ¶åŒ…å«åŠ¨æ€æ¨¡å¼è§£ç å™¨ï¼ˆD-PaDeï¼‰ï¼Œå®ƒä½œä¸ºåŠ¨æ€æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°æå–å…¸å‹è§†è§‰æ¨¡å¼ã€‚</li>
<li>å¼•å…¥ä¸‰ä¸ªå±‚æ¬¡çš„å¯¹é½ç›®æ ‡ï¼Œä»¥æœ€å°åŒ–è½»é‡çº§HVMså’Œå¤§å‹HVMsä¹‹é—´çš„æ³›åŒ–å·®è·ï¼ŒåŒ…æ‹¬å…¨å±€å›¾åƒçº§åˆ«ã€å±€éƒ¨åƒç´ çº§åˆ«å’Œå®ä¾‹å…³ç³»çº§åˆ«ã€‚</li>
<li>DPALæ¡†æ¶èƒ½å¤Ÿå¼•å¯¼è½»å‹æ¨¡å‹ä»å¤§å‹HVMså­¦ä¹ æ‰€æœ‰äººç±»å…¸å‹çš„è§†è§‰æ¨¡å¼ï¼Œå¹¶å¯ä»¥æ³›åŒ–åˆ°å„ç§äººä¸­å¿ƒè§†è§‰ä»»åŠ¡ã€‚</li>
<li>åœ¨15ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒéªŒè¯äº†DPALçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07144">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-405031ecab0f49ad621382e6e2342fa8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cf9f2b3905ebfbd23567619bea15e33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-365214c0bf89cafc7b363d7530d51b9b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Investigating-the-Impact-of-Large-Scale-Pre-training-on-Nutritional-Content-Estimation-from-2D-Images"><a href="#Investigating-the-Impact-of-Large-Scale-Pre-training-on-Nutritional-Content-Estimation-from-2D-Images" class="headerlink" title="Investigating the Impact of Large-Scale Pre-training on Nutritional   Content Estimation from 2D Images"></a>Investigating the Impact of Large-Scale Pre-training on Nutritional   Content Estimation from 2D Images</h2><p><strong>Authors:Michele Andrade, Guilherme A. L. Silva, ValÃ©ria Santos, Gladston Moreira, Eduardo Luz</strong></p>
<p>Estimating the nutritional content of food from images is a critical task with significant implications for health and dietary monitoring. This is challenging, especially when relying solely on 2D images, due to the variability in food presentation, lighting, and the inherent difficulty in inferring volume and mass without depth information. Furthermore, reproducibility in this domain is hampered by the reliance of state-of-the-art methods on proprietary datasets for large-scale pre-training. In this paper, we investigate the impact of large-scale pre-training datasets on the performance of deep learning models for nutritional estimation using only 2D images. We fine-tune and evaluate Vision Transformer (ViT) models pre-trained on two large public datasets, ImageNet and COYO, comparing their performance against baseline CNN models (InceptionV2 and ResNet-50) and a state-of-the-art method pre-trained on the proprietary JFT-300M dataset. We conduct extensive experiments on the Nutrition5k dataset, a large-scale collection of real-world food plates with high-precision nutritional annotations. Our evaluation using Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAE%) reveals that models pre-trained on JFT-300M significantly outperform those pre-trained on public datasets. Unexpectedly, the model pre-trained on the massive COYO dataset performs worse than the model pre-trained on ImageNet for this specific regression task, refuting our initial hypothesis. Our analysis provides quantitative evidence highlighting the critical role of pre-training dataset characteristics, including scale, domain relevance, and curation quality, for effective transfer learning in 2D nutritional estimation. </p>
<blockquote>
<p>ä»å›¾åƒä¼°ç®—é£Ÿå“çš„è¥å…»æˆåˆ†æ˜¯å¥åº·ä¸é¥®é£Ÿç›‘æµ‹ä¸­å…·æœ‰é‡è¦æ„ä¹‰çš„ä»»åŠ¡ã€‚è¿™æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ä»…ä¾èµ–2Då›¾åƒæ—¶ï¼Œç”±äºé£Ÿå“å±•ç¤ºã€å…‰ç…§çš„å¤šæ ·æ€§ä»¥åŠç¼ºä¹æ·±åº¦ä¿¡æ¯å¯¼è‡´æ¨æ–­ä½“ç§¯å’Œè´¨é‡çš„å›ºæœ‰å›°éš¾ã€‚æ­¤å¤–ï¼Œè¯¥é¢†åŸŸçš„å¯é‡å¤æ€§å—åˆ°é˜»ç¢ï¼Œå› ä¸ºæœ€å…ˆè¿›çš„æ–¹æ³•ä¾èµ–äºå¤§è§„æ¨¡é¢„è®­ç»ƒçš„ä¸“æœ‰æ•°æ®é›†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†å¯¹ä»…ä½¿ç”¨2Då›¾åƒè¿›è¡Œè¥å…»ä¼°ç®—çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬å¾®è°ƒå¹¶è¯„ä¼°äº†åœ¨ImageNetå’ŒCOYOä¸¤ä¸ªå¤§å‹å…¬å…±æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„Vision Transformerï¼ˆViTï¼‰æ¨¡å‹ï¼Œå°†å…¶æ€§èƒ½ä¸åŸºçº¿CNNæ¨¡å‹ï¼ˆInceptionV2å’ŒResNet-50ï¼‰ä»¥åŠä¸€ç§åœ¨ä¸“æœ‰JFT-300Mæ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æœ€æ–°æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬åœ¨Nutrition5kæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¯¥æ•°æ®é›†æ˜¯ç°å®ä¸–ç•Œé£Ÿå“ç›˜çš„å¤§é‡é›†åˆï¼Œå…·æœ‰é«˜ç²¾åº¦è¥å…»æ³¨é‡Šã€‚æˆ‘ä»¬ä½¿ç”¨å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰å’Œå¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®ï¼ˆMAE%ï¼‰è¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºåœ¨JFT-300Mä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹æ˜¾è‘—ä¼˜äºåœ¨å…¬å…±æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚å‡ºä¹æ„æ–™çš„æ˜¯ï¼Œåœ¨å¤§é‡COYOæ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹åœ¨æ­¤ç‰¹å®šå›å½’ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸å¦‚åœ¨ImageNetä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œè¿™ä¸æˆ‘ä»¬æœ€åˆçš„å‡è®¾ç›¸æ‚–ã€‚æˆ‘ä»¬çš„åˆ†ææä¾›äº†å®šé‡è¯æ®ï¼Œçªå‡ºæ˜¾ç¤ºäº†é¢„è®­ç»ƒæ•°æ®é›†ç‰¹å¾ï¼ˆåŒ…æ‹¬è§„æ¨¡ã€é¢†åŸŸç›¸å…³æ€§å’Œç­›é€‰è´¨é‡ï¼‰åœ¨2Dè¥å…»ä¼°ç®—ä¸­çš„æœ‰æ•ˆè¿ç§»å­¦ä¹ ä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03996v2">PDF</a> 12 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†å¯¹ä»…ä½¿ç”¨2Då›¾åƒè¿›è¡Œè¥å…»ä¼°ç®—çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚é€šè¿‡å¾®è°ƒå¹¶åœ¨Nutrition5kæ•°æ®é›†ä¸Šè¯„ä¼°é¢„è®­ç»ƒäºImageNetå’ŒCOYOçš„Vision Transformerï¼ˆViTï¼‰æ¨¡å‹ï¼Œä¸åŸºå‡†CNNæ¨¡å‹ï¼ˆInceptionV2å’ŒResNet-50ï¼‰ä»¥åŠé¢„è®­ç»ƒäºä¸“æœ‰JFT-300Mæ•°æ®é›†çš„æœ€å…ˆè¿›æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé¢„è®­ç»ƒäºJFT-300Mçš„æ¨¡å‹æ˜¾è‘—ä¼˜äºé¢„è®­ç»ƒäºå…¬å¼€æ•°æ®é›†çš„æ¨¡å‹ã€‚æ„å¤–çš„æ˜¯ï¼Œé¢„è®­ç»ƒäºå¤§è§„æ¨¡COYOæ•°æ®é›†çš„æ¨¡å‹åœ¨æ­¤ç‰¹å®šå›å½’ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸å¦‚é¢„è®­ç»ƒäºImageNetçš„æ¨¡å‹ï¼Œè¿™åé©³äº†æˆ‘ä»¬çš„åˆæ­¥å‡è®¾ã€‚åˆ†ææä¾›äº†å®šé‡è¯æ®ï¼Œçªå‡ºæ˜¾ç¤ºäº†é¢„è®­ç»ƒæ•°æ®é›†ç‰¹æ€§ï¼ŒåŒ…æ‹¬è§„æ¨¡ã€é¢†åŸŸç›¸å…³æ€§å’Œç¼–çº‚è´¨é‡ï¼Œåœ¨2Dè¥å…»ä¼°ç®—ä¸­çš„æœ‰æ•ˆè¿ç§»å­¦ä¹ æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¥å…»å›¾åƒä¼°ç®—å…·æœ‰å¥åº·ä¸é¥®é£Ÿç›‘æµ‹çš„é‡å¤§æ„ä¹‰ï¼Œä½†é¢ä¸´é£Ÿå“å±•ç¤ºã€å…‰ç…§ã€ä½“ç§¯å’Œè´¨é‡æ¨æ–­çš„éš¾é¢˜ã€‚</li>
<li>ä½¿ç”¨Vision Transformer (ViT)æ¨¡å‹è¿›è¡Œæ·±åº¦å­¦ä¹ ï¼Œç ”ç©¶å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†å¯¹è¥å…»ä¼°ç®—æ€§èƒ½çš„å½±å“ã€‚</li>
<li>å¯¹æ¯”äº†é¢„è®­ç»ƒäºImageNetå’ŒCOYOçš„ViTæ¨¡å‹ï¼Œä»¥åŠä¸åŸºå‡†CNNæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>é¢„è®­ç»ƒäºJFT-300Mçš„æ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œæ˜¾ç¤ºé¢„è®­ç»ƒæ•°æ®é›†è§„æ¨¡ã€é¢†åŸŸç›¸å…³æ€§å’Œç¼–çº‚è´¨é‡çš„é‡è¦æ€§ã€‚</li>
<li>COYOæ•°æ®é›†åœ¨ç‰¹å®šå›å½’ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸å¦‚ImageNetï¼Œè¿™å‡ºä¹é¢„æ–™å¹¶åé©³äº†åˆå§‹å‡è®¾ã€‚</li>
<li>å®šé‡è¯æ®è¡¨æ˜ï¼Œé¢„è®­ç»ƒæ•°æ®é›†ç‰¹æ€§å¯¹æœ‰æ•ˆè¿ç§»å­¦ä¹ åœ¨è¥å…»ä¼°ç®—ä¸­çš„å…³é”®ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03996">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e9b51f41df81724796fc76fb768c8fd6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae5c5926e15005c3e7ab5d92565821e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e406cfab08800a8ef480c7fcf190fed3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5caad04a6a77ce58ccf0056e4d79dd6e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-13/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-13/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-13/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c008540d8e5195615f8fdd26c5d65594.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-13  GAPNet A Lightweight Framework for Image and Video Salient Object   Detection via Granularity-Aware Paradigm
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-13/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a7dba96b26df51d2171bd55e1a69ad03.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-13  FineBadminton A Multi-Level Dataset for Fine-Grained Badminton Video   Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32140.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
