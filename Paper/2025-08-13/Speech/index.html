<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-13  AD-AVSR Asymmetric Dual-stream Enhancement for Robust Audio-Visual   Speech Recognition">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6dafd82758db69f8164530440d8c0456.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    61 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-13-æ›´æ–°"><a href="#2025-08-13-æ›´æ–°" class="headerlink" title="2025-08-13 æ›´æ–°"></a>2025-08-13 æ›´æ–°</h1><h2 id="AD-AVSR-Asymmetric-Dual-stream-Enhancement-for-Robust-Audio-Visual-Speech-Recognition"><a href="#AD-AVSR-Asymmetric-Dual-stream-Enhancement-for-Robust-Audio-Visual-Speech-Recognition" class="headerlink" title="AD-AVSR: Asymmetric Dual-stream Enhancement for Robust Audio-Visual   Speech Recognition"></a>AD-AVSR: Asymmetric Dual-stream Enhancement for Robust Audio-Visual   Speech Recognition</h2><p><strong>Authors:Junxiao Xue, Xiaozhen Liu, Xuecheng Wu, Xinyi Yin, Danlei Huang, Fei Yu</strong></p>
<p>Audio-visual speech recognition (AVSR) combines audio-visual modalities to improve speech recognition, especially in noisy environments. However, most existing methods deploy the unidirectional enhancement or symmetric fusion manner, which limits their capability to capture heterogeneous and complementary correlations of audio-visual data-especially under asymmetric information conditions. To tackle these gaps, we introduce a new AVSR framework termed AD-AVSR based on bidirectional modality enhancement. Specifically, we first introduce the audio dual-stream encoding strategy to enrich audio representations from multiple perspectives and intentionally establish asymmetry to support subsequent cross-modal interactions. The enhancement process involves two key components, Audio-aware Visual Refinement Module for enhanced visual representations under audio guidance, and Cross-modal Noise Suppression Masking Module which refines audio representations using visual cues, collaboratively leading to the closed-loop and bidirectional information flow. To further enhance correlation robustness, we adopt a threshold-based selection mechanism to filter out irrelevant or weakly correlated audio-visual pairs. Extensive experimental results on the LRS2 and LRS3 datasets indicate that our AD-AVSR consistently surpasses SOTA methods in both performance and noise robustness, highlighting the effectiveness of our model design. </p>
<blockquote>
<p>è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ç»“åˆäº†è§†å¬æ¨¡å¼æ¥æé«˜è¯­éŸ³è¯†åˆ«èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å˜ˆæ‚çš„ç¯å¢ƒä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•é‡‡ç”¨å•å‘å¢å¼ºæˆ–å¯¹ç§°èåˆçš„æ–¹å¼ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬æ•æ‰è§†å¬æ•°æ®çš„å¼‚æ„å’Œäº’è¡¥å…³è”çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨ä¸å¯¹ç§°ä¿¡æ¯æ¡ä»¶ä¸‹ã€‚ä¸ºäº†å¼¥è¡¥è¿™äº›ä¸è¶³ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºåŒå‘æ¨¡å¼å¢å¼ºçš„æ–°å‹AVSRæ¡†æ¶ï¼Œç§°ä¸ºAD-AVSRã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆé‡‡ç”¨éŸ³é¢‘åŒæµç¼–ç ç­–ç•¥ï¼Œä»å¤šä¸ªè§’åº¦ä¸°å¯ŒéŸ³é¢‘è¡¨ç¤ºï¼Œå¹¶æœ‰æ„å»ºç«‹ä¸å¯¹ç§°æ€§ï¼Œä»¥æ”¯æŒéšåçš„è·¨æ¨¡æ€äº¤äº’ã€‚å¢å¼ºè¿‡ç¨‹åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šéŸ³é¢‘æ„ŸçŸ¥è§†è§‰ç»†åŒ–æ¨¡å—ï¼Œç”¨äºåœ¨éŸ³é¢‘æŒ‡å¯¼ä¸‹å¢å¼ºè§†è§‰è¡¨ç¤ºï¼›è·¨æ¨¡æ€å™ªå£°æŠ‘åˆ¶æ©æ¨¡æ¨¡å—ï¼Œä½¿ç”¨è§†è§‰çº¿ç´¢æ¥å®Œå–„éŸ³é¢‘è¡¨ç¤ºï¼Œå…±åŒå®ç°é—­ç¯å’ŒåŒå‘ä¿¡æ¯æµã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºå…³è”é²æ£’æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºé˜ˆå€¼çš„é€‰æ‹©æœºåˆ¶æ¥è¿‡æ»¤æ‰æ— å…³æˆ–å¼±ç›¸å…³çš„è§†å¬å¯¹ã€‚åœ¨LRS2å’ŒLRS3æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„AD-AVSRåœ¨æ€§èƒ½å’Œå™ªå£°ç¨³å¥æ€§æ–¹é¢å§‹ç»ˆè¶…è¿‡æœ€æ–°æ–¹æ³•ï¼Œå‡¸æ˜¾äº†æˆ‘ä»¬çš„æ¨¡å‹è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07608v1">PDF</a> Accepted by the ACM MM 2025 Workshop on SVC</p>
<p><strong>Summary</strong><br>éŸ³é¢‘è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ç»“åˆè§†å¬æ¨¡å¼ä»¥æé«˜è¯­éŸ³è¯†åˆ«èƒ½åŠ›ï¼Œå°¤å…¶åœ¨å™ªå£°ç¯å¢ƒä¸­ã€‚ä½†ç°æœ‰æ–¹æ³•å¤§å¤šé‡‡ç”¨å•å‘å¢å¼ºæˆ–å¯¹ç§°èåˆæ–¹å¼ï¼Œéš¾ä»¥æ•æ‰è§†å¬æ•°æ®çš„å¼‚è´¨äº’è¡¥å…³è”ï¼Œå°¤å…¶åœ¨ä¸å¯¹ç§°ä¿¡æ¯æ¡ä»¶ä¸‹ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåŒå‘æ¨¡å¼å¢å¼ºçš„æ–°å‹AVSRæ¡†æ¶AD-AVSRã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å¼•å…¥éŸ³é¢‘åŒæµç¼–ç ç­–ç•¥ï¼Œä»å¤šä¸ªè§’åº¦ä¸°å¯ŒéŸ³é¢‘è¡¨å¾ï¼Œå¹¶å»ºç«‹ä¸å¯¹ç§°æ€§ä»¥æ”¯æŒåç»­è·¨æ¨¡å¼äº¤äº’ã€‚å¢å¼ºè¿‡ç¨‹åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šå—éŸ³é¢‘æŒ‡å¯¼çš„è§†è§‰ç»†åŒ–æ¨¡å—å’Œè·¨æ¨¡å¼å™ªå£°æŠ‘åˆ¶æ©æ¨¡æ¨¡å—ï¼Œå…±åŒå®ç°é—­ç¯å’ŒåŒå‘ä¿¡æ¯æµã€‚ä¸ºå¢å¼ºå…³è”ç¨³å¥æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨é˜ˆå€¼é€‰æ‹©æœºåˆ¶è¿‡æ»¤æ‰æ— å…³æˆ–å¼±ç›¸å…³çš„è§†å¬å¯¹ã€‚åœ¨LRS2å’ŒLRS3æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„AD-AVSRåœ¨æ€§èƒ½å’Œå™ªå£°ç¨³å¥æ€§æ–¹é¢å‡è¶…è¶Šç°æœ‰æœ€ä½³æ–¹æ³•ï¼Œå‡¸æ˜¾äº†æ¨¡å‹è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AVSRç»“åˆäº†éŸ³é¢‘è§†è§‰æ¨¡å¼ä»¥æé«˜åœ¨å™ªå£°ç¯å¢ƒä¸­çš„è¯­éŸ³è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é‡‡å–å•å‘å¢å¼ºæˆ–å¯¹ç§°èåˆï¼Œéš¾ä»¥æ•æ‰è§†å¬æ•°æ®çš„å¼‚è´¨äº’è¡¥æ€§ã€‚</li>
<li>æå‡ºçš„AD-AVSRæ¡†æ¶åŸºäºåŒå‘æ¨¡å¼å¢å¼ºï¼Œå¼•å…¥éŸ³é¢‘åŒæµç¼–ç ç­–ç•¥ä¸°å¯ŒéŸ³é¢‘è¡¨å¾ã€‚</li>
<li>AD-AVSRåŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šå—éŸ³é¢‘æŒ‡å¯¼çš„è§†è§‰ç»†åŒ–æ¨¡å—å’Œè·¨æ¨¡å¼å™ªå£°æŠ‘åˆ¶æ©æ¨¡æ¨¡å—ã€‚</li>
<li>é‡‡ç”¨é˜ˆå€¼é€‰æ‹©æœºåˆ¶ä»¥å¢å¼ºå…³è”ç¨³å¥æ€§ï¼Œè¿‡æ»¤æ‰æ— å…³æˆ–å¼±ç›¸å…³çš„è§†å¬å¯¹ã€‚</li>
<li>åœ¨LRS2å’ŒLRS3æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAD-AVSRåœ¨æ€§èƒ½å’Œå™ªå£°ç¨³å¥æ€§æ–¹é¢è¶…è¶Šç°æœ‰æœ€ä½³æ–¹æ³•ã€‚</li>
<li>æ€»ä½“æ¥è¯´ï¼ŒAD-AVSRæ¨¡å‹è®¾è®¡æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07608">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9ab4d6b59d9f8c62fc7445891c08ee3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32a7a854f0692a908cbb7a69845c48f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-677d39cabd6aee7ae2f7f63c5fd12313.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b84627a65515bc2445d80c59c2ffad40.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Exploring-Efficient-Directional-and-Distance-Cues-for-Regional-Speech-Separation"><a href="#Exploring-Efficient-Directional-and-Distance-Cues-for-Regional-Speech-Separation" class="headerlink" title="Exploring Efficient Directional and Distance Cues for Regional Speech   Separation"></a>Exploring Efficient Directional and Distance Cues for Regional Speech   Separation</h2><p><strong>Authors:Yiheng Jiang, Haoxu Wang, Yafeng Chen, Gang Qiao, Biao Tian</strong></p>
<p>In this paper, we introduce a neural network-based method for regional speech separation using a microphone array. This approach leverages novel spatial cues to extract the sound source not only from specified direction but also within defined distance. Specifically, our method employs an improved delay-and-sum technique to obtain directional cues, substantially enhancing the signal from the target direction. We further enhance separation by incorporating the direct-to-reverberant ratio into the input features, enabling the model to better discriminate sources within and beyond a specified distance. Experimental results demonstrate that our proposed method leads to substantial gains across multiple objective metrics. Furthermore, our method achieves state-of-the-art performance on the CHiME-8 MMCSG dataset, which was recorded in real-world conversational scenarios, underscoring its effectiveness for speech separation in practical applications. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•ï¼Œä½¿ç”¨éº¦å…‹é£é˜µåˆ—è¿›è¡ŒåŒºåŸŸè¯­éŸ³åˆ†ç¦»ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨æ–°å‹çš„ç©ºé—´çº¿ç´¢æ¥æå–å£°æºï¼Œä¸ä»…å¯ä»¥ä»æŒ‡å®šæ–¹å‘ï¼Œè¿˜å¯ä»¥ä»å®šä¹‰çš„èŒƒå›´å†…æå–å£°éŸ³ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨æ”¹è¿›çš„å»¶è¿Ÿæ±‚å’ŒæŠ€æœ¯æ¥è·å¾—æ–¹å‘çº¿ç´¢ï¼Œä»è€Œæå¤§åœ°å¢å¼ºç›®æ ‡æ–¹å‘ä¸Šçš„ä¿¡å·ã€‚æˆ‘ä»¬é€šè¿‡å°†ç›´è¾¾å£°ä¸æ··å“å£°æ¯”ç‡çº³å…¥è¾“å…¥ç‰¹å¾ï¼Œè¿›ä¸€æ­¥æé«˜å£°éŸ³çš„åˆ†ç¦»æ•ˆæœï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°åŒºåˆ†æŒ‡å®šè·ç¦»å†…å’Œä¹‹å¤–çš„å£°æºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªå®¢è§‚æŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨çœŸå®å¯¹è¯åœºæ™¯ä¸­å½•åˆ¶çš„CHiME-8 MMCSGæ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½ï¼Œè¿™å‡¸æ˜¾äº†å…¶åœ¨è¯­éŸ³åˆ†ç¦»çš„å®ç”¨åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07563v1">PDF</a> This paper has been accepted by Interspeech 2025</p>
<p><strong>Summary</strong><br>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œçš„åŒºåŸŸè¯­éŸ³åˆ†ç¦»æ–¹æ³•ï¼Œåˆ©ç”¨éº¦å…‹é£é˜µåˆ—è¿›è¡Œå£°éŸ³é‡‡é›†ã€‚è¯¥æ–¹æ³•é€šè¿‡æå–ç©ºé—´çº¿ç´¢ï¼Œä¸ä»…å¯ä»¥ä»æŒ‡å®šæ–¹å‘æå–å£°æºï¼Œè¿˜å¯ä»¥åœ¨ç‰¹å®šè·ç¦»å†…å¯¹å£°æºè¿›è¡Œå®šä½ã€‚é€šè¿‡æ”¹è¿›å»¶æ—¶ç›¸åŠ æŠ€æœ¯è·å¾—æ–¹å‘çº¿ç´¢ï¼Œæé«˜äº†ç›®æ ‡æ–¹å‘çš„ä¿¡å·è´¨é‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†ç›´è¾¾å£°ä¸æ··å“å£°æ¯”ä¾‹çº³å…¥è¾“å…¥ç‰¹å¾ï¼Œæé«˜äº†æ¨¡å‹å¯¹å†…å¤–å£°æºçš„è¾¨è¯†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå®¢è§‚è¯„ä»·æŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œå¹¶åœ¨çœŸå®å¯¹è¯åœºæ™¯çš„CHiME-8 MMCSGæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨å®ç”¨åœºåˆè¯­éŸ³åˆ†ç¦»çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œçš„åŒºåŸŸè¯­éŸ³åˆ†ç¦»æ–¹æ³•ï¼Œä½¿ç”¨éº¦å…‹é£é˜µåˆ—è¿›è¡Œå£°éŸ³é‡‡é›†ã€‚</li>
<li>æ–¹æ³•é€šè¿‡æå–ç©ºé—´çº¿ç´¢ï¼Œå¯ä»¥åœ¨ç‰¹å®šè·ç¦»å†…å®šä½å£°æºã€‚</li>
<li>é€šè¿‡æ”¹è¿›å»¶æ—¶ç›¸åŠ æŠ€æœ¯æé«˜ç›®æ ‡æ–¹å‘çš„ä¿¡å·è´¨é‡ã€‚</li>
<li>æ¨¡å‹ç»“åˆç›´è¾¾å£°ä¸æ··å“å£°æ¯”ä¾‹ï¼Œæé«˜å†…å¤–å£°æºçš„è¾¨è¯†èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœåœ¨å¤šä¸ªå®¢è§‚è¯„ä»·æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>åœ¨çœŸå®å¯¹è¯åœºæ™¯çš„CHiME-8 MMCSGæ•°æ®é›†ä¸Šå®ç°å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07563">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa0b57a78942ef1e8818ccd57f9febb2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b9bb6eac89a3512abf71f85e955e510.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75134bbb01a934009f189adf4b8ce420.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1288dd6a988ccc823a8e7a486f1b93ec.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Small-footprint-Acoustic-Echo-Cancellation-Solution-for-Mobile-Full-Duplex-Speech-Interactions"><a href="#A-Small-footprint-Acoustic-Echo-Cancellation-Solution-for-Mobile-Full-Duplex-Speech-Interactions" class="headerlink" title="A Small-footprint Acoustic Echo Cancellation Solution for Mobile   Full-Duplex Speech Interactions"></a>A Small-footprint Acoustic Echo Cancellation Solution for Mobile   Full-Duplex Speech Interactions</h2><p><strong>Authors:Yiheng Jiang, Tian Biao</strong></p>
<p>In full-duplex speech interaction systems, effective Acoustic Echo Cancellation (AEC) is crucial for recovering echo-contaminated speech. This paper presents a neural network-based AEC solution to address challenges in mobile scenarios with varying hardware, nonlinear distortions and long latency. We first incorporate diverse data augmentation strategies to enhance the modelâ€™s robustness across various environments. Moreover, progressive learning is employed to incrementally improve AEC effectiveness, resulting in a considerable improvement in speech quality. To further optimize AECâ€™s downstream applications, we introduce a novel post-processing strategy employing tailored parameters designed specifically for tasks such as Voice Activity Detection (VAD) and Automatic Speech Recognition (ASR), thus enhancing their overall efficacy. Finally, our method employs a small-footprint model with streaming inference, enabling seamless deployment on mobile devices. Empirical results demonstrate effectiveness of the proposed method in Echo Return Loss Enhancement and Perceptual Evaluation of Speech Quality, alongside significant improvements in both VAD and ASR results. </p>
<blockquote>
<p>åœ¨åŒå‘è¯­éŸ³äº¤äº’ç³»ç»Ÿä¸­ï¼Œæœ‰æ•ˆçš„å£°å­¦å›å£°æ¶ˆé™¤ï¼ˆAECï¼‰å¯¹äºæ¢å¤å—å›å£°æ±¡æŸ“çš„è¯­éŸ³è‡³å…³é‡è¦ã€‚æœ¬æ–‡é’ˆå¯¹ç§»åŠ¨åœºæ™¯ä¸­çš„ç¡¬ä»¶å·®å¼‚ã€éçº¿æ€§å¤±çœŸå’Œé•¿å»¶è¿Ÿç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œçš„AECè§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬é¦–å…ˆé‡‡ç”¨å¤šç§æ•°æ®å¢å¼ºç­–ç•¥ï¼Œæé«˜æ¨¡å‹åœ¨ä¸åŒç¯å¢ƒä¸‹çš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨æ¸è¿›å­¦ä¹ æ–¹æ³•æ¥é€æ­¥æ”¹è¿›AECçš„æœ‰æ•ˆæ€§ï¼Œä»è€Œæ˜¾è‘—æé«˜è¯­éŸ³è´¨é‡ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–AECçš„ä¸‹æ¸¸åº”ç”¨ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹åå¤„ç†ç­–ç•¥ï¼Œé‡‡ç”¨é’ˆå¯¹ä»»åŠ¡è®¾è®¡çš„ä¸“ç”¨å‚æ•°ï¼Œå¦‚è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆVADï¼‰å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ï¼Œä»è€Œæé«˜å…¶æ•´ä½“æ•ˆç‡ã€‚æœ€åï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨ä¸€ä¸ªå°å‹æ¨¡å‹è¿›è¡Œæµå¼æ¨ç†ï¼Œå¯åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šæ— ç¼éƒ¨ç½²ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›å£°è¿”å›æŸè€—å¢å¼ºå’Œè¯­éŸ³è´¨é‡ä¸»è§‚è¯„ä»·æ–¹é¢æœ‰æ•ˆï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†VADå’ŒASRçš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07561v1">PDF</a> This paper is accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç¥ç»ç½‘ç»œçš„å…¨åŒå·¥è¯­éŸ³äº¤äº’ç³»ç»Ÿä¸­ï¼Œå›å£°æ¶ˆé™¤è‡³å…³é‡è¦ã€‚æœ¬æ–‡é€šè¿‡å¤šæ ·çš„æ•°æ®å¢å¼ºç­–ç•¥æ¥æå‡æ¨¡å‹åœ¨å„ç§ç¯å¢ƒä¸‹çš„ç¨³å¥æ€§ï¼Œå¹¶åˆ©ç”¨æ¸è¿›å­¦ä¹ é€æ­¥æå‡å›å£°æ¶ˆé™¤æ•ˆæœã€‚æ­¤å¤–ï¼Œé€šè¿‡é‡‡ç”¨ç‰¹å®šå‚æ•°çš„åå¤„ç†ç­–ç•¥ä¼˜åŒ–äº†ä¸‹æ¸¸åº”ç”¨å¦‚è¯­éŸ³æ´»åŠ¨æ£€æµ‹å’Œè¯­éŸ³è¯†åˆ«ã€‚æœ¬æ–‡çš„æ–¹æ³•åœ¨å°æ¨¡å‹è¶³å°å’Œæµå¼æ¨æ–­çš„æ”¯æŒä¸‹ï¼Œå¯åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šæ— ç¼éƒ¨ç½²ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›å£°æŸè€—å¢å¼ºå’Œè¯­éŸ³è´¨é‡æ„ŸçŸ¥è¯„ä¼°ä¸Šæ•ˆæœæ˜¾è‘—ï¼Œå¹¶åœ¨è¯­éŸ³æ´»åŠ¨å’Œè¯­éŸ³è¯†åˆ«ç»“æœä¸Šæœ‰æ˜¾è‘—æ”¹å–„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¨åŒå·¥è¯­éŸ³äº¤äº’ç³»ç»Ÿä¸­ï¼Œå£°å­¦å›å£°æ¶ˆé™¤ï¼ˆAECï¼‰å¯¹äºå¤„ç†å›å£°æ±¡æŸ“çš„è¯­éŸ³è‡³å…³é‡è¦ã€‚</li>
<li>é€šè¿‡å¤šæ ·çš„æ•°æ®å¢å¼ºç­–ç•¥æå‡æ¨¡å‹ç¨³å¥æ€§ï¼Œé€‚åº”å„ç§ç¯å¢ƒã€‚</li>
<li>æ¸è¿›å­¦ä¹ ç”¨äºé€æ­¥æå‡AECæ•ˆæœã€‚</li>
<li>å¼•å…¥åå¤„ç†ç­–ç•¥ï¼Œé€šè¿‡ç‰¹å®šå‚æ•°ä¼˜åŒ–ä¸‹æ¸¸åº”ç”¨å¦‚è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆVADï¼‰å’Œè¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚</li>
<li>æ–¹æ³•é‡‡ç”¨å°æ¨¡å‹è¶³å°å’Œæµå¼æ¨æ–­ï¼Œæ”¯æŒåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šæ— ç¼éƒ¨ç½²ã€‚</li>
<li>å®è¯ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å›å£°æŸè€—å¢å¼ºå’Œè¯­éŸ³è´¨é‡è¯„ä¼°ä¸Šè¡¨ç°ä¼˜ç§€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07561">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-203e46aaecb75deeb5020df807c6b7be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b84cd3c46a28de5c0462c38364baf320.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ae506d17222959c7085e2727ee6e7eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ebd2410892b5eaf368114b49baa1e62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66cb8bae5ac5afca7c20e85cf6bb5e57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36e3c3ecd2e093f6977e8beaa4febc53.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="UniFlow-Unifying-Speech-Front-End-Tasks-via-Continuous-Generative-Modeling"><a href="#UniFlow-Unifying-Speech-Front-End-Tasks-via-Continuous-Generative-Modeling" class="headerlink" title="UniFlow: Unifying Speech Front-End Tasks via Continuous Generative   Modeling"></a>UniFlow: Unifying Speech Front-End Tasks via Continuous Generative   Modeling</h2><p><strong>Authors:Ziqian Wang, Zikai Liu, Yike Zhu, Xingchen Li, Boyi Kang, Jixun Yao, Xianjun Xia, Chuanzeng Huang, Lei Xie</strong></p>
<p>Generative modeling has recently achieved remarkable success across image, video, and audio domains, demonstrating powerful capabilities for unified representation learning. Yet speech front-end tasks such as speech enhancement (SE), target speaker extraction (TSE), acoustic echo cancellation (AEC), and language-queried source separation (LASS) remain largely tackled by disparate, task-specific solutions. This fragmentation leads to redundant engineering effort, inconsistent performance, and limited extensibility. To address this gap, we introduce UniFlow, a unified framework that employs continuous generative modeling to tackle diverse speech front-end tasks in a shared latent space. Specifically, UniFlow utilizes a waveform variational autoencoder (VAE) to learn a compact latent representation of raw audio, coupled with a Diffusion Transformer (DiT) that predicts latent updates. To differentiate the speech processing task during the training, learnable condition embeddings indexed by a task ID are employed to enable maximal parameter sharing while preserving task-specific adaptability. To balance model performance and computational efficiency, we investigate and compare three generative objectives: denoising diffusion, flow matching, and mean flow within the latent domain. We validate UniFlow on multiple public benchmarks, demonstrating consistent gains over state-of-the-art baselines. UniFlowâ€™s unified latent formulation and conditional design make it readily extensible to new tasks, providing an integrated foundation for building and scaling generative speech processing pipelines. To foster future research, we will open-source our codebase. </p>
<blockquote>
<p>ç”Ÿæˆå¼å»ºæ¨¡æœ€è¿‘åœ¨å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œå±•ç¤ºäº†ç»Ÿä¸€è¡¨ç¤ºå­¦ä¹ çš„å¼ºå¤§èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¯­éŸ³å‰ç«¯ä»»åŠ¡ï¼Œå¦‚è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰ã€ç›®æ ‡è¯´è¯äººæå–ï¼ˆTSEï¼‰ã€å›å£°æ¶ˆé™¤ï¼ˆAECï¼‰å’Œè¯­è¨€æŸ¥è¯¢æºåˆ†ç¦»ï¼ˆLASSï¼‰ï¼Œä»ä¸»è¦é€šè¿‡åˆ†æ•£çš„ã€é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„è§£å†³æ–¹æ¡ˆæ¥å¤„ç†ã€‚è¿™ç§ç¢ç‰‡åŒ–å¯¼è‡´äº†å†—ä½™çš„å·¥ç¨‹åŠªåŠ›ã€æ€§èƒ½ä¸ä¸€è‡´å’Œæ‰©å±•æ€§æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†UniFlowï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œé‡‡ç”¨è¿ç»­ç”Ÿæˆå¼å»ºæ¨¡ï¼Œåœ¨å…±äº«æ½œåœ¨ç©ºé—´å†…è§£å†³å¤šæ ·åŒ–çš„è¯­éŸ³å‰ç«¯ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼ŒUniFlowåˆ©ç”¨æ³¢å½¢å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å­¦ä¹ åŸå§‹éŸ³é¢‘çš„ç´§å‡‘æ½œåœ¨è¡¨ç¤ºï¼Œç»“åˆæ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰æ¥é¢„æµ‹æ½œåœ¨æ›´æ–°ã€‚ä¸ºäº†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒºåˆ†ä¸åŒçš„è¯­éŸ³å¤„ç†ä»»åŠ¡ï¼Œæˆ‘ä»¬é‡‡ç”¨ç”±ä»»åŠ¡IDç´¢å¼•çš„å¯å­¦ä¹ æ¡ä»¶åµŒå…¥ï¼Œä»¥å®ç°æœ€å¤§å‚æ•°å…±äº«ï¼ŒåŒæ—¶ä¿ç•™ä»»åŠ¡ç‰¹å®šé€‚åº”æ€§ã€‚ä¸ºäº†å¹³è¡¡æ¨¡å‹æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ï¼Œæˆ‘ä»¬ç ”ç©¶å’Œæ¯”è¾ƒäº†ä¸‰ç§ç”Ÿæˆç›®æ ‡ï¼šå»å™ªæ‰©æ•£ã€æµåŒ¹é…å’Œæ½œåœ¨åŸŸå†…çš„å¹³å‡æµã€‚æˆ‘ä»¬åœ¨å¤šä¸ªå…¬å…±åŸºå‡†æµ‹è¯•é›†ä¸ŠéªŒè¯äº†UniFlowï¼Œç›¸è¾ƒäºæœ€æ–°åŸºçº¿æŠ€æœ¯ï¼Œè¡¨ç°å‡ºäº†ä¸€è‡´çš„ä¼˜åŠ¿ã€‚UniFlowçš„ç»Ÿä¸€æ½œåœ¨å…¬å¼å’Œæ¡ä»¶è®¾è®¡ä½¿å…¶æ˜“äºæ‰©å±•åˆ°æ–°ä»»åŠ¡ï¼Œä¸ºæ„å»ºå’Œæ‰©å±•ç”Ÿæˆå¼è¯­éŸ³å¤„ç†ç®¡é“æä¾›äº†é›†æˆåŸºç¡€ã€‚ä¸ºäº†ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å°†å¼€æºæˆ‘ä»¬çš„ä»£ç åº“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07558v1">PDF</a> extended version</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè¿ç»­ç”Ÿæˆæ¨¡å‹ç»Ÿä¸€å¤„ç†å¤šç§è¯­éŸ³å‰ç«¯ä»»åŠ¡çš„æ¡†æ¶UniFlowã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ³¢å½¢å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å­¦ä¹ åŸå§‹éŸ³é¢‘çš„ç´§å‡‘æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶ç»“åˆæ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰é¢„æµ‹æ½œåœ¨æ›´æ–°ã€‚é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„æ¡ä»¶åµŒå…¥å’Œç´¢å¼•ä»»åŠ¡IDï¼Œå®ç°æœ€å¤§å‚æ•°å…±äº«åŒæ—¶ä¿ç•™ä»»åŠ¡ç‰¹å®šé€‚åº”æ€§ã€‚åŒæ—¶ï¼Œæ¢è®¨äº†ä¸‰ç§ç”Ÿæˆç›®æ ‡ï¼ŒåŒ…æ‹¬å»å™ªæ‰©æ•£ã€æµåŒ¹é…å’Œæ½œåœ¨åŸŸå†…çš„å¹³å‡æµï¼Œä»¥å®ç°æ¨¡å‹æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡çš„å¹³è¡¡ã€‚åœ¨å¤šä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„éªŒè¯æ˜¾ç¤ºï¼ŒUniFlowä¼˜äºå½“å‰å…ˆè¿›åŸºçº¿ï¼Œæ˜“äºæ‰©å±•åˆ°æ–°ä»»åŠ¡ï¼Œä¸ºæ„å»ºå’Œæ‰©å±•ç”Ÿæˆæ€§è¯­éŸ³å¤„ç†ç®¡é“æä¾›äº†ç»¼åˆåŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniFlowæ˜¯ä¸€ä¸ªåˆ©ç”¨è¿ç»­ç”Ÿæˆæ¨¡å‹å¤„ç†å¤šæ ·åŒ–è¯­éŸ³å‰ç«¯ä»»åŠ¡çš„ç»Ÿä¸€æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨æ³¢å½¢å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å­¦ä¹ éŸ³é¢‘çš„æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶ç»“åˆæ‰©æ•£å˜å‹å™¨è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„æ¡ä»¶åµŒå…¥å’Œç´¢å¼•ä»»åŠ¡IDï¼Œå®ç°å‚æ•°å…±äº«å’Œä»»åŠ¡ç‰¹å®šé€‚åº”æ€§çš„å¹³è¡¡ã€‚</li>
<li>æ¢è®¨äº†ä¸‰ç§ç”Ÿæˆç›®æ ‡ä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
<li>UniFlowåœ¨å¤šä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜“äºæ‰©å±•åˆ°æ–°ä»»åŠ¡ã€‚</li>
<li>UniFlowä¸ºæ„å»ºå’Œæ‰©å±•ç”Ÿæˆæ€§è¯­éŸ³å¤„ç†ç®¡é“æä¾›äº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07558">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-40e47e4b0816a8e34969773e09ad5d79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edc1c8edc67169ff4840198174a53948.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a3381a9ecbba54c4de0f52f20cc5bf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dafd82758db69f8164530440d8c0456.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Scalable-Controllable-Accented-TTS"><a href="#Scalable-Controllable-Accented-TTS" class="headerlink" title="Scalable Controllable Accented TTS"></a>Scalable Controllable Accented TTS</h2><p><strong>Authors:Henry Li Xinyuan, Zexin Cai, Ashi Garg, Kevin Duh, Leibny Paola GarcÃ­a-Perera, Sanjeev Khudanpur, Nicholas Andrews, Matthew Wiesner</strong></p>
<p>We tackle the challenge of scaling accented TTS systems, expanding their capabilities to include much larger amounts of training data and a wider variety of accent labels, even for accents that are poorly represented or unlabeled in traditional TTS datasets. To achieve this, we employ two strategies: 1. Accent label discovery via a speech geolocation model, which automatically infers accent labels from raw speech data without relying solely on human annotation; 2. Timbre augmentation through kNN voice conversion to increase data diversity and model robustness. These strategies are validated on CommonVoice, where we fine-tune XTTS-v2 for accented TTS with accent labels discovered or enhanced using geolocation. We demonstrate that the resulting accented TTS model not only outperforms XTTS-v2 fine-tuned on self-reported accent labels in CommonVoice, but also existing accented TTS benchmarks. </p>
<blockquote>
<p>æˆ‘ä»¬é¢å¯¹äº†æœ‰å£éŸ³çš„TTSç³»ç»Ÿæ‰€é¢ä¸´çš„æ‰©å±•æŒ‘æˆ˜ï¼Œéœ€è¦æ‰©å±•å®ƒä»¬çš„èƒ½åŠ›ï¼Œä»¥åŒ…å«æ›´å¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®å’Œæ›´å¤šç§ç±»çš„å£éŸ³æ ‡ç­¾ï¼Œå³ä½¿å¯¹äºåœ¨ä¼ ç»ŸTTSæ•°æ®é›†ä¸­è¡¨ç¤ºä¸ä½³æˆ–æœªæ ‡è®°çš„å£éŸ³ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤ç§ç­–ç•¥ï¼š1. é€šè¿‡è¯­éŸ³å®šä½æ¨¡å‹å‘ç°å£éŸ³æ ‡ç­¾ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨ä»åŸå§‹è¯­éŸ³æ•°æ®ä¸­æ¨æ–­å£éŸ³æ ‡ç­¾ï¼Œè€Œæ— éœ€å®Œå…¨ä¾èµ–äºäººå·¥æ³¨é‡Šï¼›2. é€šè¿‡kNNè¯­éŸ³è½¬æ¢å¢å¼ºéŸ³è‰²ï¼Œä»¥å¢åŠ æ•°æ®å¤šæ ·æ€§å’Œæ¨¡å‹ç¨³å¥æ€§ã€‚è¿™äº›ç­–ç•¥åœ¨CommonVoiceä¸Šå¾—åˆ°äº†éªŒè¯ï¼Œæˆ‘ä»¬å¯¹XTTS-v2è¿›è¡Œå¾®è°ƒï¼Œä»¥å¤„ç†å¸¦æœ‰é€šè¿‡å®šä½å‘ç°æˆ–å¢å¼ºçš„å£éŸ³æ ‡ç­¾çš„å£éŸ³TTSã€‚æˆ‘ä»¬è¯æ˜ï¼Œæ‰€å¾—çš„å¸¦å£éŸ³çš„TTSæ¨¡å‹ä¸ä»…ä¼˜äºåœ¨CommonVoiceä¸­è‡ªæˆ‘æŠ¥å‘Šçš„å£éŸ³æ ‡ç­¾ä¸Šè°ƒä¼˜çš„XTTS-v2ï¼Œè€Œä¸”è¿˜ä¼˜äºç°æœ‰çš„å¸¦å£éŸ³TTSåŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07426v1">PDF</a> Accepted at IEEE ASRU 2025</p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•é€šè¿‡é‡‡ç”¨ä¸¤ç§ç­–ç•¥æ¥è§£å†³æ‰©å±•å¸¦å£éŸ³çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿçš„æŒ‘æˆ˜ã€‚è¿™ä¸¤ç§ç­–ç•¥åˆ†åˆ«æ˜¯ï¼šä¸€ã€åˆ©ç”¨è¯­éŸ³åœ°ç†ä½ç½®æ¨¡å‹è¿›è¡Œå£éŸ³æ ‡ç­¾å‘ç°ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ä»åŸå§‹è¯­éŸ³æ•°æ®ä¸­æ¨æ–­å£éŸ³æ ‡ç­¾ï¼Œæ— éœ€å®Œå…¨ä¾èµ–äººå·¥æ ‡æ³¨ï¼›äºŒã€é€šè¿‡kNNè¯­éŸ³è½¬æ¢è¿›è¡ŒéŸ³è‰²å¢å¼ºï¼Œä»¥å¢åŠ æ•°æ®å¤šæ ·æ€§å’Œæ¨¡å‹ç¨³å¥æ€§ã€‚æ–‡ç« åœ¨CommonVoiceæ•°æ®é›†ä¸ŠéªŒè¯äº†è¿™ä¸¤ç§ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œä½¿ç”¨åœ°ç†ä½ç½®å‘ç°çš„å£éŸ³æ ‡ç­¾å¾®è°ƒXTTS-v2æ¨¡å‹åç”Ÿæˆçš„å¸¦å£éŸ³çš„TTSæ¨¡å‹æ€§èƒ½å¾—åˆ°æå‡ï¼Œä¸ä»…è¶…è¶Šäº†ä½¿ç”¨CommonVoiceè‡ªæˆ‘æŠ¥å‘Šçš„å£éŸ³æ ‡ç­¾å¾®è°ƒXTTS-v2æ¨¡å‹çš„è¡¨ç°ï¼Œè€Œä¸”ç›¸æ¯”ç°æœ‰çš„å¸¦å£éŸ³çš„TTSåŸºå‡†æµ‹è¯•ä¹Ÿè¡¨ç°æ›´å‡ºè‰²ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¯­éŸ³åœ°ç†ä½ç½®æ¨¡å‹ç”¨äºå£éŸ³æ ‡ç­¾å‘ç°ï¼šèƒ½å¤Ÿè‡ªåŠ¨ä»åŸå§‹è¯­éŸ³æ•°æ®ä¸­æ¨æ–­å£éŸ³æ ‡ç­¾ï¼Œé™ä½äº†å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ã€‚</li>
<li>é‡‡ç”¨kNNè¯­éŸ³è½¬æ¢è¿›è¡ŒéŸ³è‰²å¢å¼ºï¼šå¢åŠ æ•°æ®å¤šæ ·æ€§å’Œæ¨¡å‹ç¨³å¥æ€§ã€‚</li>
<li>åœ¨CommonVoiceæ•°æ®é›†ä¸ŠéªŒè¯äº†è¿™ä¸¤ç§ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æå‡ºçš„å¸¦å£éŸ³çš„TTSæ¨¡å‹è¡¨ç°è¶…è¶Šäº†ä½¿ç”¨CommonVoiceè‡ªæˆ‘æŠ¥å‘Šçš„å£éŸ³æ ‡ç­¾å¾®è°ƒXTTS-v2æ¨¡å‹çš„è¡¨ç°ã€‚</li>
<li>ä¸ç°æœ‰çš„å¸¦å£éŸ³çš„TTSåŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹çš„æ€§èƒ½æ›´ä¼˜è¶Šã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºæ‰©å±•TTSç³»ç»Ÿçš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¤„ç†æ›´å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œæ›´å¹¿æ³›çš„å£éŸ³æ ‡ç­¾å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>æ­¤ç­–ç•¥å¯¹äºåœ¨ä¼ ç»ŸTTSæ•°æ®é›†ä¸­è¡¨ç¤ºä¸è¶³æˆ–æ— æ ‡ç­¾çš„å£éŸ³ç‰¹åˆ«é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07426">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e95582732cf5c505460d492ab171f4bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b432ad7e8ad037b6870ef5957d6dbffa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a66073a4b3bffcb248261761304cf33a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-95a345e34f226104adbee9bd1ede6af7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2515349aa313ad8988e094f1c3e1db5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c2f25bc571509a1578e250ff75b55be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad0f63ad03fc150281bd5ef3512fa8b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eef2e09c62254e2041da4f698c45a27a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Think-Before-You-Talk-Enhancing-Meaningful-Dialogue-Generation-in-Full-Duplex-Speech-Language-Models-with-Planning-Inspired-Text-Guidance"><a href="#Think-Before-You-Talk-Enhancing-Meaningful-Dialogue-Generation-in-Full-Duplex-Speech-Language-Models-with-Planning-Inspired-Text-Guidance" class="headerlink" title="Think Before You Talk: Enhancing Meaningful Dialogue Generation in   Full-Duplex Speech Language Models with Planning-Inspired Text Guidance"></a>Think Before You Talk: Enhancing Meaningful Dialogue Generation in   Full-Duplex Speech Language Models with Planning-Inspired Text Guidance</h2><p><strong>Authors:Wenqian Cui, Lei Zhu, Xiaohui Li, Zhihan Guo, Haoli Bai, Lu Hou, Irwin King</strong></p>
<p>Full-Duplex Speech Language Models (FD-SLMs) are specialized foundation models designed to enable natural, real-time spoken interactions by modeling complex conversational dynamics such as interruptions, backchannels, and overlapping speech, and End-to-end (e2e) FD-SLMs leverage real-world double-channel conversational data to capture nuanced two-speaker dialogue patterns for human-like interactions. However, they face a critical challenge â€“ their conversational abilities often degrade compared to pure-text conversation due to prolonged speech sequences and limited high-quality spoken dialogue data. While text-guided speech generation could mitigate these issues, it suffers from timing and length issues when integrating textual guidance into double-channel audio streams, disrupting the precise time alignment essential for natural interactions. To address these challenges, we propose TurnGuide, a novel planning-inspired approach that mimics human conversational planning by dynamically segmenting assistant speech into dialogue turns and generating turn-level text guidance before speech output, which effectively resolves both insertion timing and length challenges. Extensive experiments demonstrate our approach significantly improves e2e FD-SLMsâ€™ conversational abilities, enabling them to generate semantically meaningful and coherent speech while maintaining natural conversational flow. Demos are available at <a target="_blank" rel="noopener" href="https://dreamtheater123.github.io/TurnGuide-Demo/">https://dreamtheater123.github.io/TurnGuide-Demo/</a>. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/dreamtheater123/TurnGuide">https://github.com/dreamtheater123/TurnGuide</a>. </p>
<blockquote>
<p>å…¨åŒå·¥è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆFD-SLMsï¼‰æ˜¯ä¸“é—¨è®¾è®¡çš„åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡æ¨¡æ‹Ÿå¤æ‚å¯¹è¯åŠ¨æ€ï¼ˆå¦‚ä¸­æ–­ã€åé¦ˆé€šé“å’Œé‡å è¯­éŸ³ï¼‰æ¥å®ç°è‡ªç„¶ã€å®æ—¶çš„å£è¯­äº¤äº’ã€‚ç«¯åˆ°ç«¯ï¼ˆe2eï¼‰FD-SLMsåˆ™åˆ©ç”¨ç°å®ä¸–ç•Œçš„åŒé€šé“å¯¹è¯æ•°æ®ï¼Œæ•æ‰å¾®å¦™çš„ä¸¤è¯­è€…å¯¹è¯æ¨¡å¼ï¼Œç”¨äºäººåƒäº¤äº’ã€‚ç„¶è€Œï¼Œå®ƒä»¬é¢ä¸´ä¸€ä¸ªå…³é”®æŒ‘æˆ˜â€”â€”ç”±äºé•¿è¯­éŸ³åºåˆ—å’Œæœ‰é™çš„é«˜è´¨é‡å£è¯­å¯¹è¯æ•°æ®ï¼Œå®ƒä»¬çš„å¯¹è¯èƒ½åŠ›é€šå¸¸ä¸çº¯æ–‡æœ¬å¯¹è¯ç›¸æ¯”æœ‰æ‰€é™ä½ã€‚è™½ç„¶æ–‡æœ¬å¼•å¯¼çš„è¯­éŸ³ç”Ÿæˆå¯ä»¥ç¼“è§£è¿™äº›é—®é¢˜ï¼Œä½†åœ¨å°†æ–‡æœ¬å¼•å¯¼é›†æˆåˆ°åŒé€šé“éŸ³é¢‘æµä¸­æ—¶ï¼Œå®ƒå­˜åœ¨æ—¶é—´å’Œé•¿åº¦é—®é¢˜ï¼Œç ´åäº†è‡ªç„¶äº¤äº’æ‰€å¿…éœ€çš„æ—¶é—´ç²¾ç¡®å¯¹é½ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†TurnGuideï¼Œè¿™æ˜¯ä¸€ç§å—è§„åˆ’å¯å‘çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€åœ°å°†åŠ©ç†è¯­éŸ³åˆ†å‰²æˆå¯¹è¯å›åˆå¹¶ç”Ÿæˆå›åˆçº§æ–‡æœ¬æŒ‡å¯¼æ¥æ¨¡ä»¿äººç±»å¯¹è¯è§„åˆ’ï¼Œåœ¨è¯­éŸ³è¾“å‡ºä¹‹å‰æœ‰æ•ˆåœ°è§£å†³äº†æ’å…¥æ—¶é—´å’Œé•¿åº¦æŒ‘æˆ˜ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†ç«¯åˆ°ç«¯FD-SLMsçš„å¯¹è¯èƒ½åŠ›ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿç”Ÿæˆè¯­ä¹‰ä¸Šè¿è´¯çš„è¯­éŸ³å¹¶ä¿æŒè‡ªç„¶çš„å¯¹è¯æµç¨‹ã€‚æ¼”ç¤ºç½‘ç«™ä¸ºï¼š<a target="_blank" rel="noopener" href="https://dreamtheater123.github.io/TurnGuide-Demo/%E3%80%82%E4%BB%A3%E7%A0%81%E5%B0%86%E5%8F%91%E5%B8%AE%E5%9C%A8">https://dreamtheater123.github.io/TurnGuide-Demo/ã€‚ä»£ç å°†å‘å¸ƒåœ¨</a>ï¼š<a target="_blank" rel="noopener" href="https://github.com/dreamtheater123/TurnGuide%E3%80%82">https://github.com/dreamtheater123/TurnGuideã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07375v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å…¨åŒå·¥è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆFD-SLMsï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå³å¯¹è¯èƒ½åŠ›åœ¨é•¿æ—¶é—´è¯­éŸ³åºåˆ—å’Œé«˜è´¨é‡è¯­éŸ³å¯¹è¯æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ä¼šé€€åŒ–ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†TurnGuideï¼Œä¸€ç§æ¨¡ä»¿äººç±»å¯¹è¯è§„åˆ’çš„æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€åˆ†å‰²åŠ©ç†è¯­éŸ³ç”Ÿæˆå¯¹è¯å›åˆçº§åˆ«çš„æ–‡æœ¬æŒ‡å¯¼ï¼Œæœ‰æ•ˆè§£å†³äº†æ’å…¥æ—¶æœºå’Œé•¿åº¦æŒ‘æˆ˜ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—æé«˜ç«¯åˆ°ç«¯FD-SLMsçš„å¯¹è¯èƒ½åŠ›ï¼Œä½¿å…¶åœ¨ç”Ÿæˆè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰å’Œè¿è´¯çš„è¯­éŸ³çš„åŒæ—¶ï¼Œä¿æŒè‡ªç„¶çš„å¯¹è¯æµç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FD-SLMsæ—¨åœ¨å®ç°è‡ªç„¶ã€å®æ—¶çš„å£è¯­äº¤äº’ï¼Œå»ºæ¨¡å¤æ‚çš„å¯¹è¯åŠ¨æ€ï¼Œå¦‚ä¸­æ–­ã€åé¦ˆå’Œé‡å è¯­éŸ³ã€‚</li>
<li>ç«¯åˆ°ç«¯FD-SLMsåˆ©ç”¨çœŸå®ä¸–ç•Œçš„åŒé€šé“å¯¹è¯æ•°æ®æ¥æ•æ‰å¾®å¦™çš„ä¸¤äººå¯¹è¯æ¨¡å¼ï¼Œå®ç°äººæ€§åŒ–çš„äº¤äº’ã€‚</li>
<li>FD-SLMsé¢ä¸´çš„å…³é”®æŒ‘æˆ˜æ˜¯ï¼šç”±äºé•¿æ—¶é—´çš„è¯­éŸ³åºåˆ—å’Œé«˜è´¨é‡å¯¹è¯æ•°æ®çš„é™åˆ¶ï¼Œå…¶å¯¹è¯èƒ½åŠ›é€€åŒ–ã€‚</li>
<li>TurnGuideæ˜¯ä¸€ç§æ¨¡ä»¿äººç±»å¯¹è¯è§„åˆ’çš„æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€åˆ†å‰²åŠ©ç†è¯­éŸ³ç”Ÿæˆå¯¹è¯å›åˆçº§åˆ«çš„æ–‡æœ¬æŒ‡å¯¼æ¥è§£å†³FD-SLMsçš„æŒ‘æˆ˜ã€‚</li>
<li>TurnGuideè§£å†³äº†æ’å…¥æ—¶æœºå’Œé•¿åº¦çš„é—®é¢˜ï¼Œç¡®ä¿è¯­éŸ³ç”Ÿæˆçš„ç²¾ç¡®æ—¶é—´å¯¹é½ï¼Œä¿æŒè‡ªç„¶çš„å¯¹è¯æµç¨‹ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒTurnGuideèƒ½æ˜¾è‘—æé«˜ç«¯åˆ°ç«¯FD-SLMsçš„å¯¹è¯èƒ½åŠ›ï¼Œç”Ÿæˆè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰å’Œè¿è´¯çš„è¯­éŸ³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07375">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-683047e4b651aa6518b122cce617caa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cd7ea01daabaa183cace25071d45b54.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d63694d3ea90d97df0ec7a4f744e3d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5db8877468133a18b48fde71885ea1c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-411537da04440d4423e000421f7e47b2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bc0b62c024e56c5f3a15fc975b549cf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4da7a11a6a5e82fd0ee18989f5b6158.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="XEmoRAG-Cross-Lingual-Emotion-Transfer-with-Controllable-Intensity-Using-Retrieval-Augmented-Generation"><a href="#XEmoRAG-Cross-Lingual-Emotion-Transfer-with-Controllable-Intensity-Using-Retrieval-Augmented-Generation" class="headerlink" title="XEmoRAG: Cross-Lingual Emotion Transfer with Controllable Intensity   Using Retrieval-Augmented Generation"></a>XEmoRAG: Cross-Lingual Emotion Transfer with Controllable Intensity   Using Retrieval-Augmented Generation</h2><p><strong>Authors:Tianlun Zuo, Jingbin Hu, Yuke Li, Xinfa Zhu, Hai Li, Ying Yan, Junhui Liu, Danming Xie, Lei Xie</strong></p>
<p>Zero-shot emotion transfer in cross-lingual speech synthesis refers to generating speech in a target language, where the emotion is expressed based on reference speech from a different source language.However, this task remains challenging due to the scarcity of parallel multilingual emotional corpora, the presence of foreign accent artifacts, and the difficulty of separating emotion from language-specific prosodic features.In this paper, we propose XEmoRAG, a novel framework to enable zero-shot emotion transfer from Chinese to Thai using a large language model (LLM)-based model, without relying on parallel emotional data.XEmoRAG extracts language-agnostic emotional embeddings from Chinese speech and retrieves emotionally matched Thai utterances from a curated emotional database, enabling controllable emotion transfer without explicit emotion labels. Additionally, a flow-matching alignment module minimizes pitch and duration mismatches, ensuring natural prosody. It also blends Chinese timbre into the Thai synthesis, enhancing rhythmic accuracy and emotional expression, while preserving speaker characteristics and emotional consistency.Experimental results show that XEmoRAG synthesizes expressive and natural Thai speech using only Chinese reference audio, without requiring explicit emotion labels.These results highlight XEmoRAGâ€™s capability to achieve flexible and low-resource emotional transfer across languages.Our demo is available at <a target="_blank" rel="noopener" href="https://tlzuo-lesley.github.io/Demo-page/">https://tlzuo-lesley.github.io/Demo-page/</a>. </p>
<blockquote>
<p>è·¨è¯­è¨€è¯­éŸ³åˆæˆä¸­çš„é›¶æ ·æœ¬æƒ…æ„Ÿè¿ç§»æ˜¯æŒ‡ç”Ÿæˆç›®æ ‡è¯­è¨€ä¸­çš„è¯­éŸ³ï¼Œæƒ…æ„Ÿçš„è¡¨è¾¾æ˜¯åŸºäºæºè¯­è¨€çš„å‚è€ƒè¯­éŸ³ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¹³è¡Œå¤šè¯­è¨€æƒ…æ„Ÿè¯­æ–™åº“ã€å­˜åœ¨å¤–æ¥å£éŸ³çš„ä¼ªè¿¹ä»¥åŠä»ç‰¹å®šè¯­è¨€çš„éŸµå¾‹ç‰¹å¾ä¸­åˆ†ç¦»æƒ…æ„Ÿçš„å›°éš¾ï¼Œè¿™ä¸€ä»»åŠ¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†XEmoRAGï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€ä¾èµ–å¹³è¡Œæƒ…æ„Ÿæ•°æ®ï¼Œèƒ½å¤Ÿå®ç°ä»ä¸­æ–‡åˆ°æ³°è¯­é›¶æ ·æœ¬æƒ…æ„Ÿè¿ç§»çš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚XEmoRAGä»ä¸­æ–‡è¯­éŸ³ä¸­æå–ä¸è¯­è¨€æ— å…³çš„æƒ…æ„ŸåµŒå…¥ï¼Œå¹¶ä»ç²¾é€‰çš„æƒ…æ„Ÿæ•°æ®åº“ä¸­æ£€ç´¢æƒ…æ„ŸåŒ¹é…çš„æ³°è¯­è¯è¯­ï¼Œä»è€Œå®ç°å¯æ§çš„æƒ…æ„Ÿè¿ç§»ï¼Œæ— éœ€æ˜ç¡®çš„æƒ…æ„Ÿæ ‡ç­¾ã€‚æ­¤å¤–ï¼Œé€šè¿‡æµåŒ¹é…å¯¹é½æ¨¡å—æœ€å°åŒ–éŸ³é«˜å’ŒæŒç»­æ—¶é—´çš„ä¸åŒ¹é…ï¼Œç¡®ä¿è‡ªç„¶çš„éŸµå¾‹ã€‚å®ƒè¿˜èåˆäº†ä¸­æ–‡éŸ³è‰²åˆ°æ³°è¯­åˆæˆä¸­ï¼Œæé«˜äº†èŠ‚å¥å‡†ç¡®æ€§å’Œæƒ…æ„Ÿè¡¨è¾¾ï¼ŒåŒæ—¶ä¿æŒäº†è¯´è¯äººçš„ç‰¹ç‚¹å’Œæƒ…æ„Ÿä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒXEmoRAGä»…ä½¿ç”¨ä¸­æ–‡å‚è€ƒéŸ³é¢‘å°±èƒ½åˆæˆå‡ºå¯Œæœ‰è¡¨ç°åŠ›å’Œè‡ªç„¶æ„Ÿçš„æ³°è¯­è¯­éŸ³ï¼Œæ— éœ€æ˜ç¡®çš„æƒ…æ„Ÿæ ‡ç­¾ã€‚è¿™äº›ç»“æœçªå‡ºäº†XEmoRAGåœ¨ä¸åŒè¯­è¨€ä¹‹é—´å®ç°çµæ´»å’Œä½èµ„æºæƒ…æ„Ÿè¿ç§»çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://tlzuo-lesley.github.io/Demo-page/%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://tlzuo-lesley.github.io/Demo-page/ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07302v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§é›¶æ ·æœ¬æƒ…æ„Ÿè½¬ç§»æ¡†æ¶XEmoRAGï¼Œå¯ä»¥å®ç°ä»ä¸­æ–‡åˆ°æ³°è¯­çš„è·¨è¯­è¨€è¯­éŸ³åˆæˆä¸­çš„æƒ…æ„Ÿè½¬ç§»ã€‚è¯¥æ¡†æ¶åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ— éœ€å¹³è¡Œæƒ…æ„Ÿæ•°æ®ï¼Œé€šè¿‡æå–è¯­è¨€æ— å…³çš„æƒ…æ„ŸåµŒå…¥å’Œæ£€ç´¢æƒ…æ„ŸåŒ¹é…çš„æ³°è¯­è¯­éŸ³ç‰‡æ®µæ¥å®ç°å¯æ§çš„æƒ…æ„Ÿè½¬ç§»ã€‚åŒæ—¶ï¼Œæ¡†æ¶ä¸­çš„æµåŒ¹é…å¯¹é½æ¨¡å—å¯ä»¥æœ€å°åŒ–éŸ³é«˜å’ŒæŒç»­æ—¶é—´çš„ä¸åŒ¹é…ï¼Œç¡®ä¿è‡ªç„¶çš„éŸµå¾‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒXEmoRAGèƒ½å¤Ÿä»…ä½¿ç”¨ä¸­æ–‡å‚è€ƒéŸ³é¢‘åˆæˆå‡ºè¡¨è¾¾åŠ›å¼ºã€è‡ªç„¶çš„æ³°è¯­è¯­éŸ³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é›¶æ ·æœ¬æƒ…æ„Ÿè½¬ç§»åœ¨è·¨è¯­è¨€è¯­éŸ³åˆæˆä¸­æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œç¼ºä¹å¹³è¡Œå¤šè¯­è¨€æƒ…æ„Ÿè¯­æ–™åº“ã€å­˜åœ¨å¤–è¯­å£éŸ³é—®é¢˜ä»¥åŠä»ç‰¹å®šè¯­è¨€çš„éŸµå¾‹ç‰¹å¾ä¸­åˆ†ç¦»æƒ…æ„Ÿéš¾åº¦å¤§ã€‚</li>
<li>æœ¬è®ºæ–‡æå‡ºäº†XEmoRAGæ¡†æ¶ï¼Œå®ç°äº†ä»ä¸­æ–‡åˆ°æ³°è¯­çš„é›¶æ ·æœ¬æƒ…æ„Ÿè½¬ç§»ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ— éœ€å¹³è¡Œæƒ…æ„Ÿæ•°æ®ã€‚</li>
<li>XEmoRAGé€šè¿‡æå–è¯­è¨€æ— å…³çš„æƒ…æ„ŸåµŒå…¥å’Œæ£€ç´¢æƒ…æ„ŸåŒ¹é…çš„æ³°è¯­è¯­éŸ³ç‰‡æ®µï¼Œå®ç°äº†å¯æ§çš„æƒ…æ„Ÿè½¬ç§»ã€‚</li>
<li>æµåŒ¹é…å¯¹é½æ¨¡å—æœ€å°åŒ–éŸ³é«˜å’ŒæŒç»­æ—¶é—´çš„ä¸åŒ¹é…ï¼Œç¡®ä¿è‡ªç„¶çš„éŸµå¾‹ã€‚</li>
<li>XEmoRAGå°†ä¸­æ–‡éŸ³è‰²èå…¥æ³°è¯­åˆæˆä¸­ï¼Œæé«˜äº†èŠ‚å¥å‡†ç¡®æ€§å’Œæƒ…æ„Ÿè¡¨è¾¾ï¼ŒåŒæ—¶ä¿æŒè¯´è¯äººçš„ç‰¹å¾å’Œæƒ…æ„Ÿä¸€è‡´æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜XEmoRAGèƒ½å¤Ÿä»…ä½¿ç”¨ä¸­æ–‡å‚è€ƒéŸ³é¢‘åˆæˆå‡ºè¡¨è¾¾åŠ›å¼ºã€è‡ªç„¶çš„æ³°è¯­è¯­éŸ³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-781321a3963a3d0b9784536f487c31cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0daff41d1070cfcf1d5a5b90f4586a40.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-16a0cdefcd1982fc48151026be6bed18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98c446b879264ffffdd0598c0b4ee0ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3ec16fb34760d159088fc5013024250.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4213671b3f8811c1be015ccc2c022048.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Lessons-Learnt-Revisit-Key-Training-Strategies-for-Effective-Speech-Emotion-Recognition-in-the-Wild"><a href="#Lessons-Learnt-Revisit-Key-Training-Strategies-for-Effective-Speech-Emotion-Recognition-in-the-Wild" class="headerlink" title="Lessons Learnt: Revisit Key Training Strategies for Effective Speech   Emotion Recognition in the Wild"></a>Lessons Learnt: Revisit Key Training Strategies for Effective Speech   Emotion Recognition in the Wild</h2><p><strong>Authors:Jing-Tong Tzeng, Bo-Hao Su, Ya-Tse Wu, Hsing-Hang Chou, Chi-Chun Lee</strong></p>
<p>In this study, we revisit key training strategies in machine learning often overlooked in favor of deeper architectures. Specifically, we explore balancing strategies, activation functions, and fine-tuning techniques to enhance speech emotion recognition (SER) in naturalistic conditions. Our findings show that simple modifications improve generalization with minimal architectural changes. Our multi-modal fusion model, integrating these optimizations, achieves a valence CCC of 0.6953, the best valence score in Task 2: Emotional Attribute Regression. Notably, fine-tuning RoBERTa and WavLM separately in a single-modality setting, followed by feature fusion without training the backbone extractor, yields the highest valence performance. Additionally, focal loss and activation functions significantly enhance performance without increasing complexity. These results suggest that refining core components, rather than deepening models, leads to more robust SER in-the-wild. </p>
<blockquote>
<p>åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†æœºå™¨å­¦ä¹ ä¸­çš„å…³é”®è®­ç»ƒç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥é€šå¸¸ä¼šè¢«æ›´æ·±çš„æ¶æ„æ‰€å¿½è§†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¹³è¡¡ç­–ç•¥ã€æ¿€æ´»å‡½æ•°å’Œå¾®è°ƒæŠ€æœ¯ï¼Œä»¥æé«˜è‡ªç„¶æ¡ä»¶ä¸‹çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç®€å•çš„ä¿®æ”¹å¯ä»¥åœ¨æå°çš„æ¶æ„å˜åŒ–ä¸‹æé«˜æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬èåˆäº†å¤šç§ä¼˜åŒ–æ–¹å¼çš„å¤šæ¨¡æ€èåˆæ¨¡å‹åœ¨ä»»åŠ¡2ï¼šæƒ…æ„Ÿå±æ€§å›å½’ä¸­å–å¾—äº†0.6953çš„æ•ˆä»·CCCå€¼ï¼Œæˆä¸ºæœ€ä½³æ•ˆä»·å¾—åˆ†ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨å•æ¨¡æ€è®¾ç½®ä¸‹åˆ†åˆ«å¯¹RoBERTaå’ŒWavLMè¿›è¡Œå¾®è°ƒï¼Œç„¶åè¿›è¡Œç‰¹å¾èåˆè€Œä¸è®­ç»ƒéª¨å¹²æå–å™¨ï¼Œå¯ä»¥è·å¾—æœ€é«˜çš„æ•ˆä»·æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç„¦ç‚¹æŸå¤±å’Œæ¿€æ´»å‡½æ•°åœ¨ä¸å¢åŠ å¤æ‚æ€§çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ”¹è¿›æ ¸å¿ƒç»„ä»¶è€Œä¸æ˜¯æ·±åŒ–æ¨¡å‹ï¼Œèƒ½åœ¨é‡ç”Ÿç¯å¢ƒä¸‹å®ç°æ›´ç¨³å¥çš„SERã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07282v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶é‡æ–°å®¡è§†æœºå™¨å­¦ä¹ ä¸­çš„å…³é”®è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬å¹³è¡¡ç­–ç•¥ã€æ¿€æ´»å‡½æ•°å’Œå¾®è°ƒæŠ€æœ¯ï¼Œä»¥æå‡è‡ªç„¶æ¡ä»¶ä¸‹çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ€§èƒ½ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç®€å•çš„ä¿®æ”¹å¯ä»¥å¤§å¤§æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œä¸”æ— éœ€è¿›è¡Œå¤§è§„æ¨¡æ¶æ„æ›´æ”¹ã€‚å¤šæ¨¡æ€èåˆæ¨¡å‹åœ¨ä»»åŠ¡2ï¼šæƒ…æ„Ÿå±æ€§å›å½’ä¸­å–å¾—äº†æœ€ä½³æ•ˆä»·åˆ†æ•°0.6953ã€‚ç‰¹åˆ«æ˜¯ï¼Œåœ¨å•æ¨¡æ€è®¾ç½®ä¸­åˆ†åˆ«å¾®è°ƒRoBERTaå’ŒWavLMï¼Œç„¶åè¿›è¡Œç‰¹å¾èåˆï¼Œè€Œæ— éœ€è®­ç»ƒéª¨å¹²æå–å™¨ï¼Œå¯è·å¾—æœ€ä½³çš„æ•ˆä»·æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç„¦ç‚¹æŸå¤±å’Œæ¿€æ´»å‡½æ•°æ˜¾è‘—æé«˜æ€§èƒ½ä¸”ä¸å¢åŠ å¤æ‚æ€§ã€‚è¿™äº›ç»“æœæç¤ºæˆ‘ä»¬ï¼Œä¼˜åŒ–æ ¸å¿ƒç»„ä»¶è€Œéæ·±åŒ–æ¨¡å‹èƒ½æ›´ç¨³å¥åœ°åœ¨é‡å¤–å®ç°SERã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ¢ç´¢äº†å¹³è¡¡ç­–ç•¥ã€æ¿€æ´»å‡½æ•°å’Œå¾®è°ƒæŠ€æœ¯å¯¹è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«çš„å…³é”®è®­ç»ƒç­–ç•¥è¿›è¡Œäº†é‡æ–°å®¡è§†ã€‚</li>
<li>ç®€å•ä¿®æ”¹å¯ä»¥åœ¨ä¸æ”¹å˜æ¶æ„çš„åŸºç¡€ä¸Šæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¤šæ¨¡æ€èåˆæ¨¡å‹åœ¨æƒ…æ„Ÿå±æ€§å›å½’ä»»åŠ¡ä¸­å–å¾—äº†æœ€ä½³æ•ˆä»·åˆ†æ•°ã€‚</li>
<li>å•ç‹¬å¾®è°ƒRoBERTaå’ŒWavLMï¼Œç„¶åè¿›è¡Œç‰¹å¾èåˆï¼Œå¯ä»¥è·å¾—æœ€ä½³çš„æ•ˆä»·æ€§èƒ½ã€‚</li>
<li>ç„¦ç‚¹æŸå¤±å’Œæ¿€æ´»å‡½æ•°çš„åº”ç”¨æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶æŒ‡å‡ºä¼˜åŒ–æ ¸å¿ƒç»„ä»¶è€Œä¸æ˜¯æ·±åŒ–æ¨¡å‹æ˜¯å®ç°ç¨³å¥è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«çš„å…³é”®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-97af6d8fa63eea419ccf7d8f4026ba12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86e564dad93eb7b1138d7a8641febf43.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a927b730c9a35b7019ff790fe690516d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70228e842e6e19fc866c6abc4e4957b3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-91b470f4c53377680b3ff845bb31c153.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b1b3cfecfb0cefa49c8825dd18834d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67cd18716a8f1ff65079856276f577c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be8ff450033c920c7bf8197a907aa424.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Whisfusion-Parallel-ASR-Decoding-via-a-Diffusion-Transformer"><a href="#Whisfusion-Parallel-ASR-Decoding-via-a-Diffusion-Transformer" class="headerlink" title="Whisfusion: Parallel ASR Decoding via a Diffusion Transformer"></a>Whisfusion: Parallel ASR Decoding via a Diffusion Transformer</h2><p><strong>Authors:Taeyoun Kwon, Junhyuk Ahn, Taegeun Yun, Heeju Jwa, Yoonchae Choi, Siwon Park, Nam-Joon Kim, Jangchan Kim, Hyun Gon Ryu, Hyuk-Jae Lee</strong></p>
<p>Fast Automatic Speech Recognition (ASR) is critical for latency-sensitive applications such as real-time captioning and meeting transcription. However, truly parallel ASR decoding remains challenging due to the sequential nature of autoregressive (AR) decoders and the context limitations of non-autoregressive (NAR) methods. While modern ASR encoders can process up to 30 seconds of audio at once, AR decoders still generate tokens sequentially, creating a latency bottleneck. We propose Whisfusion, the first framework to fuse a pre-trained Whisper encoder with a text diffusion decoder. This NAR architecture resolves the AR latency bottleneck by processing the entire acoustic context in parallel at every decoding step. A lightweight cross-attention adapter trained via parameter-efficient fine-tuning (PEFT) bridges the two modalities. We also introduce a batch-parallel, multi-step decoding strategy that improves accuracy by increasing the number of candidates with minimal impact on speed. Fine-tuned solely on LibriSpeech (960h), Whisfusion achieves a lower WER than Whisper-tiny (8.3% vs. 9.7%), and offers comparable latency on short audio. For longer utterances (&gt;20s), it is up to 2.6x faster than the AR baseline, establishing a new, efficient operating point for long-form ASR. The implementation and training scripts are available at <a target="_blank" rel="noopener" href="https://github.com/taeyoun811/Whisfusion">https://github.com/taeyoun811/Whisfusion</a>. </p>
<blockquote>
<p>å¿«é€Ÿè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å¯¹äºå»¶è¿Ÿæ•æ„Ÿçš„åº”ç”¨ï¼ˆä¾‹å¦‚å®æ—¶å­—å¹•å’Œä¼šè®®è½¬å½•ï¼‰è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºè‡ªå›å½’ï¼ˆARï¼‰è§£ç å™¨çš„åºåˆ—ç‰¹æ€§å’Œéè‡ªå›å½’ï¼ˆNARï¼‰æ–¹æ³•çš„ä¸Šä¸‹æ–‡é™åˆ¶ï¼ŒçœŸæ­£çš„å¹¶è¡ŒASRè§£ç ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å°½ç®¡ç°ä»£ASRç¼–ç å™¨å¯ä»¥ä¸€æ¬¡å¤„ç†é•¿è¾¾30ç§’çš„éŸ³é¢‘ï¼Œä½†ARè§£ç å™¨ä»ç„¶æŒ‰é¡ºåºç”Ÿæˆä»¤ç‰Œï¼Œé€ æˆå»¶è¿Ÿç“¶é¢ˆã€‚æˆ‘ä»¬æå‡ºäº†Whisfusionï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†é¢„è®­ç»ƒçš„Whisperç¼–ç å™¨ä¸æ–‡æœ¬æ‰©æ•£è§£ç å™¨èåˆçš„æ¡†æ¶ã€‚è¿™ç§NARæ¶æ„é€šè¿‡åœ¨æ¯ä¸ªè§£ç æ­¥éª¤ä¸­å¹¶è¡Œå¤„ç†æ•´ä¸ªå£°å­¦ä¸Šä¸‹æ–‡æ¥è§£å†³ARå»¶è¿Ÿç“¶é¢ˆé—®é¢˜ã€‚é€šè¿‡å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰è®­ç»ƒçš„è½»é‡çº§è·¨æ³¨æ„é€‚é…å™¨å¡«è¡¥äº†ä¸¤ç§æ¨¡å¼çš„ç©ºç™½ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ‰¹å¹¶è¡Œå¤šæ­¥è§£ç ç­–ç•¥ï¼Œé€šè¿‡å¢åŠ å€™é€‰äººæ•°é‡çš„æ–¹å¼æé«˜å‡†ç¡®æ€§ï¼Œå¯¹é€Ÿåº¦çš„å½±å“å¾®ä¹å…¶å¾®ã€‚ä»…åœ¨LibriSpeechï¼ˆ960å°æ—¶ï¼‰ä¸Šè¿›è¡Œå¾®è°ƒï¼ŒWhisfusionçš„å•è¯é”™è¯¯ç‡ä½äºWhisper-tinyï¼ˆ8.3ï¼…å¯¹æ¯”9.7ï¼…ï¼‰ï¼Œå¹¶ä¸”åœ¨çŸ­éŸ³é¢‘ä¸Šæä¾›ç›¸å½“çš„å»¶è¿Ÿã€‚å¯¹äºè¾ƒé•¿çš„è¯­éŸ³ç‰‡æ®µï¼ˆ&gt;20ç§’ï¼‰ï¼Œå®ƒæ¯”ARåŸºå‡†æµ‹è¯•å¿«è¾¾2.6å€ï¼Œä¸ºé•¿å½¢å¼ASRå»ºç«‹äº†æ–°çš„é«˜æ•ˆæ“ä½œç‚¹ã€‚å®ç°å’Œè®­ç»ƒè„šæœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/taeyoun811/Whisfusion%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/taeyoun811/Whisfusionæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07048v1">PDF</a> 16 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­çš„å»¶è¿Ÿé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆâ€”â€”Whisfusionæ¡†æ¶ã€‚è¯¥æ¡†æ¶èåˆäº†é¢„è®­ç»ƒçš„Whisperç¼–ç å™¨ä¸æ–‡æœ¬æ‰©æ•£è§£ç å™¨ï¼Œé‡‡ç”¨éè‡ªå›å½’ï¼ˆNARï¼‰æ¶æ„ï¼Œä»¥å¹¶è¡Œå¤„ç†æ•´ä¸ªå£°å­¦ä¸Šä¸‹æ–‡ï¼Œè§£å†³è‡ªå›å½’ï¼ˆARï¼‰è§£ç å™¨çš„åºåˆ—æ€§è´¨åŠä¸Šä¸‹æ–‡é™åˆ¶å¯¼è‡´çš„å»¶è¿Ÿç“¶é¢ˆã€‚é€šè¿‡å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰çš„è·¨æ³¨æ„åŠ›é€‚é…å™¨ï¼Œå®ç°äº†ä¸¤ç§æ¨¡æ€ä¹‹é—´çš„æ¡¥æ¢ã€‚åŒæ—¶ï¼Œå¼•å…¥æ‰¹é‡å¹¶è¡Œå¤šæ­¥è§£ç ç­–ç•¥ï¼Œé€šè¿‡å¢åŠ å€™é€‰æ•°é‡æ¥æé«˜å‡†ç¡®æ€§ï¼ŒåŒæ—¶å‡ ä¹ä¸å½±å“é€Ÿåº¦ã€‚åœ¨LibriSpeechä¸Šä»…è¿›è¡Œå¾®è°ƒï¼ŒWhisfusionçš„å•è¯é”™è¯¯ç‡ä½äºWhisper-tinyï¼Œå¯¹çŸ­éŸ³é¢‘çš„å»¶è¿Ÿä¸Whisperç›¸å½“ï¼Œå¯¹é•¿éŸ³é¢‘çš„å¤„ç†é€Ÿåº¦åˆ™æ˜¯è‡ªå›å½’æ–¹æ³•çš„2.6å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Whisfusionæ¡†æ¶èåˆäº†é¢„è®­ç»ƒçš„Whisperç¼–ç å™¨ä¸æ–‡æœ¬æ‰©æ•£è§£ç å™¨ï¼Œè§£å†³äº†ASRä¸­çš„å»¶è¿Ÿé—®é¢˜ã€‚</li>
<li>é‡‡ç”¨éè‡ªå›å½’ï¼ˆNARï¼‰æ¶æ„ï¼Œå¹¶è¡Œå¤„ç†æ•´ä¸ªå£°å­¦ä¸Šä¸‹æ–‡ï¼Œçªç ´äº†è‡ªå›å½’ï¼ˆARï¼‰è§£ç å™¨çš„å»¶è¿Ÿç“¶é¢ˆã€‚</li>
<li>è·¨æ³¨æ„åŠ›é€‚é…å™¨é€šè¿‡å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰å®ç°æ¨¡æ€é—´çš„æ¡¥æ¢ã€‚</li>
<li>å¼•å…¥æ‰¹é‡å¹¶è¡Œå¤šæ­¥è§£ç ç­–ç•¥ï¼Œæé«˜å‡†ç¡®æ€§åŒæ—¶å‡ ä¹ä¸å½±å“é€Ÿåº¦ã€‚</li>
<li>Whisfusionåœ¨LibriSpeechä¸Šçš„å•è¯é”™è¯¯ç‡ä½äºWhisper-tinyã€‚</li>
<li>å¯¹äºçŸ­éŸ³é¢‘ï¼ŒWhisfusionçš„å»¶è¿Ÿä¸Whisperç›¸å½“ã€‚</li>
<li>å¯¹äºé•¿éŸ³é¢‘å¤„ç†ï¼ŒWhisfusionçš„å¤„ç†é€Ÿåº¦æ˜¯è‡ªå›å½’æ–¹æ³•çš„2.6å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07048">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-98968fc4e920dcec0faa6bedb672724a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b930979621d123d773aa5cffaf7b88f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbfb3eb78c4a11755acc4168a1d3b119.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-319f66d19be0159c15ccf29fadf364de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc083a9925dcdf59e34e53b6e89068cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-944e26a07e755d08dc3049b6a903ee53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9cd9881e9b2eed15ab57e26d2f24adc.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FlowSE-Flow-Matching-based-Speech-Enhancement"><a href="#FlowSE-Flow-Matching-based-Speech-Enhancement" class="headerlink" title="FlowSE: Flow Matching-based Speech Enhancement"></a>FlowSE: Flow Matching-based Speech Enhancement</h2><p><strong>Authors:Seonggyu Lee, Sein Cheong, Sangwook Han, Jong Won Shin</strong></p>
<p>Diffusion probabilistic models have shown impressive performance for speech enhancement, but they typically require 25 to 60 function evaluations in the inference phase, resulting in heavy computational complexity. Recently, a fine-tuning method was proposed to correct the reverse process, which significantly lowered the number of function evaluations (NFE). Flow matching is a method to train continuous normalizing flows which model probability paths from known distributions to unknown distributions including those described by diffusion processes. In this paper, we propose a speech enhancement based on conditional flow matching. The proposed method achieved the performance comparable to those for the diffusion-based speech enhancement with the NFE of 60 when the NFE was 5, and showed similar performance with the diffusion model correcting the reverse process at the same NFE from 1 to 5 without additional fine tuning procedure. We also have shown that the corresponding diffusion model derived from the conditional probability path with a modified optimal transport conditional vector field demonstrated similar performances with the NFE of 5 without any fine-tuning procedure. </p>
<blockquote>
<p>æ‰©æ•£æ¦‚ç‡æ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºæ–¹é¢è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬é€šå¸¸åœ¨æ¨ç†é˜¶æ®µéœ€è¦25åˆ°60æ¬¡å‡½æ•°è¯„ä¼°ï¼Œå¯¼è‡´è®¡ç®—å¤æ‚åº¦è¾ƒé«˜ã€‚æœ€è¿‘ï¼Œæå‡ºäº†ä¸€ç§å¾®è°ƒæ–¹æ³•æ¥çº æ­£åå‘è¿‡ç¨‹ï¼Œè¿™å¤§å¤§é™ä½äº†å‡½æ•°è¯„ä¼°æ¬¡æ•°ï¼ˆNFEï¼‰ã€‚æµåŒ¹é…æ˜¯ä¸€ç§è®­ç»ƒè¿ç»­å½’ä¸€åŒ–æµçš„æ–¹æ³•ï¼Œè¯¥æµä»å·²çŸ¥åˆ†å¸ƒåˆ°æœªçŸ¥åˆ†å¸ƒå»ºæ¨¡æ¦‚ç‡è·¯å¾„ï¼ŒåŒ…æ‹¬ç”±æ‰©æ•£è¿‡ç¨‹æè¿°çš„é‚£äº›åˆ†å¸ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ¡ä»¶æµåŒ¹é…çš„è¯­éŸ³å¢å¼ºæ–¹æ³•ã€‚å½“NFEä¸º5æ—¶ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å®ç°äº†ä¸åŸºäºæ‰©æ•£çš„è¯­éŸ³å¢å¼ºç›¸å½“çš„æ€§èƒ½ï¼ˆåè€…åœ¨NFEä¸º60æ—¶è¡¨ç°ï¼‰ï¼Œå¹¶ä¸”åœ¨ç›¸åŒçš„NFEä»1åˆ°5èŒƒå›´å†…ï¼Œæ— éœ€é¢å¤–çš„å¾®è°ƒç¨‹åºï¼Œå…¶æ€§èƒ½ä¸çº æ­£åå‘è¿‡ç¨‹çš„æ‰©æ•£æ¨¡å‹ç›¸ä¼¼ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œå¯¹åº”äºç”±å¸¦æœ‰ä¿®æ”¹åçš„æœ€ä¼˜ä¼ è¾“æ¡ä»¶å‘é‡åœºçš„æ¡ä»¶æ¦‚ç‡è·¯å¾„æ´¾ç”Ÿçš„æ‰©æ•£æ¨¡å‹ï¼Œåœ¨æ— éœ€ä»»ä½•å¾®è°ƒç¨‹åºçš„æƒ…å†µä¸‹ï¼Œåœ¨NFEä¸º5æ—¶è¡¨ç°å‡ºç›¸ä¼¼çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06840v1">PDF</a> Published in ICASSP 2025</p>
<p><strong>Summary</strong><br>     åŸºäºæ¡ä»¶æµåŒ¹é…çš„è¯­éŸ³å¢å¼ºæ–¹æ³•è¢«æå‡ºï¼Œè¯¥æ–¹æ³•é€šè¿‡è®­ç»ƒè¿ç»­å½’ä¸€åŒ–æµæ¥å»ºæ¨¡ä»å·²çŸ¥åˆ†å¸ƒåˆ°æœªçŸ¥åˆ†å¸ƒçš„æ¦‚ç‡è·¯å¾„ï¼ŒåŒ…æ‹¬ç”±æ‰©æ•£è¿‡ç¨‹æè¿°çš„åˆ†å¸ƒã€‚è¯¥æ–¹æ³•å®ç°äº†ä¸åŸºäºæ‰©æ•£çš„è¯­éŸ³å¢å¼ºæ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨å‡½æ•°è¯„ä¼°æ¬¡æ•°ï¼ˆNFEï¼‰å¤§å¹…é™ä½çš„æƒ…å†µä¸‹è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¦‚ç‡æ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†æ¨ç†é˜¶æ®µéœ€è¦25è‡³60æ¬¡å‡½æ•°è¯„ä¼°ï¼Œè®¡ç®—å¤æ‚åº¦è¾ƒé«˜ã€‚</li>
<li>æœ€è¿‘æå‡ºçš„å¾®è°ƒæ–¹æ³•ç”¨äºçº æ­£åå‘è¿‡ç¨‹ï¼Œæ˜¾è‘—é™ä½å‡½æ•°è¯„ä¼°æ¬¡æ•°ï¼ˆNFEï¼‰ã€‚</li>
<li>æµåŒ¹é…æ˜¯ä¸€ç§è®­ç»ƒè¿ç»­å½’ä¸€åŒ–æµçš„æ–¹æ³•ï¼Œç”¨äºå»ºæ¨¡ä»å·²çŸ¥åˆ†å¸ƒåˆ°æœªçŸ¥åˆ†å¸ƒçš„æ¦‚ç‡è·¯å¾„ï¼ŒåŒ…æ‹¬ç”±æ‰©æ•£è¿‡ç¨‹æè¿°çš„åˆ†å¸ƒã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¡ä»¶æµåŒ¹é…çš„è¯­éŸ³å¢å¼ºæ–¹æ³•ï¼Œå®ç°äº†ä¸æ‰©æ•£æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚</li>
<li>å½“NFEä¸º5æ—¶ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†ä¸NFEä¸º60çš„æ‰©æ•£æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚</li>
<li>åœ¨ç›¸åŒçš„NFEèŒƒå›´å†…ï¼ˆä»1åˆ°5ï¼‰ï¼Œè¯¥æ–¹æ³•å±•ç¤ºäº†æ— éœ€é¢å¤–å¾®è°ƒç¨‹åºå³å¯ä¸çº æ­£åå‘è¿‡ç¨‹çš„æ‰©æ•£æ¨¡å‹ç›¸åŒ¹æ•Œçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06840">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-265913952568d740adbd84e34c6b6974.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7a56d7e14f5c2f8c2dc7ef04d82f4c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c847d7c4d28401d5ae5d66176d3a7f64.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Learning-Phonetic-Context-Dependent-Viseme-for-Enhancing-Speech-Driven-3D-Facial-Animation"><a href="#Learning-Phonetic-Context-Dependent-Viseme-for-Enhancing-Speech-Driven-3D-Facial-Animation" class="headerlink" title="Learning Phonetic Context-Dependent Viseme for Enhancing Speech-Driven   3D Facial Animation"></a>Learning Phonetic Context-Dependent Viseme for Enhancing Speech-Driven   3D Facial Animation</h2><p><strong>Authors:Hyung Kyu Kim, Hak Gu Kim</strong></p>
<p>Speech-driven 3D facial animation aims to generate realistic facial movements synchronized with audio. Traditional methods primarily minimize reconstruction loss by aligning each frame with ground-truth. However, this frame-wise approach often fails to capture the continuity of facial motion, leading to jittery and unnatural outputs due to coarticulation. To address this, we propose a novel phonetic context-aware loss, which explicitly models the influence of phonetic context on viseme transitions. By incorporating a viseme coarticulation weight, we assign adaptive importance to facial movements based on their dynamic changes over time, ensuring smoother and perceptually consistent animations. Extensive experiments demonstrate that replacing the conventional reconstruction loss with ours improves both quantitative metrics and visual quality. It highlights the importance of explicitly modeling phonetic context-dependent visemes in synthesizing natural speech-driven 3D facial animation. Project page: <a target="_blank" rel="noopener" href="https://cau-irislab.github.io/interspeech25/">https://cau-irislab.github.io/interspeech25/</a> </p>
<blockquote>
<p>è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»æ—¨åœ¨ç”Ÿæˆä¸éŸ³é¢‘åŒæ­¥çš„çœŸå®é¢éƒ¨è¿åŠ¨ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦é€šè¿‡å°†æ¯ä¸€å¸§ä¸åœ°é¢çœŸå®æ•°æ®è¿›è¡Œæ¯”å¯¹æ¥æœ€å°åŒ–é‡å»ºæŸå¤±ã€‚ç„¶è€Œï¼Œè¿™ç§åŸºäºå¸§çš„æ–¹æ³•é€šå¸¸æ— æ³•æ•æ‰åˆ°é¢éƒ¨è¿åŠ¨çš„è¿ç»­æ€§ï¼Œç”±äºååŒå‘éŸ³å¯¼è‡´è¾“å‡ºäº§ç”ŸæŠ–åŠ¨å’Œä¸è‡ªç„¶çš„ç°è±¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹è¯­éŸ³ä¸Šä¸‹æ–‡æ„ŸçŸ¥æŸå¤±ï¼Œè¯¥æŸå¤±èƒ½å¤Ÿæ˜ç¡®å»ºæ¨¡è¯­éŸ³ä¸Šä¸‹æ–‡å¯¹è¯­éŸ³éŸ³ç´ è¿‡æ¸¡çš„å½±å“ã€‚é€šè¿‡å¼•å…¥è¯­éŸ³ååŒå‘éŸ³æƒé‡ï¼Œæˆ‘ä»¬æ ¹æ®é¢éƒ¨è¿åŠ¨éšæ—¶é—´çš„å˜åŒ–ä¸ºå…¶åˆ†é…è‡ªé€‚åº”é‡è¦æ€§ï¼Œç¡®ä¿åŠ¨ç”»æ›´åŠ æµç•…ä¸”è§†è§‰æ„ŸçŸ¥ä¸€è‡´ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œç”¨æˆ‘ä»¬çš„æ–¹æ³•æ›¿æ¢ä¼ ç»Ÿé‡å»ºæŸå¤±èƒ½æé«˜å®šé‡æŒ‡æ ‡å’Œè§†è§‰è´¨é‡ã€‚å®ƒå¼ºè°ƒäº†æ˜ç¡®å»ºæ¨¡è¯­éŸ³ä¸Šä¸‹æ–‡ç›¸å…³éŸ³ç´ åœ¨åˆæˆè‡ªç„¶è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»ä¸­çš„é‡è¦æ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://cau-irislab.github.io/interspeech25/">https://cau-irislab.github.io/interspeech25/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20568v2">PDF</a> Interspeech 2025; Project Page:   <a target="_blank" rel="noopener" href="https://cau-irislab.github.io/interspeech25/">https://cau-irislab.github.io/interspeech25/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆä¸éŸ³é¢‘åŒæ­¥çš„çœŸå®é¢éƒ¨è¿åŠ¨ã€‚ä¸ºè§£å†³ä¼ ç»Ÿæ–¹æ³•å› å¿½ç•¥è¯­éŸ³ä¸Šä¸‹æ–‡å¯¼è‡´çš„é¢éƒ¨è¿åŠ¨ä¸è¿ç»­é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„è¯­éŸ³ä¸Šä¸‹æ–‡æ„ŸçŸ¥æŸå¤±å‡½æ•°ï¼Œé€šè¿‡å»ºæ¨¡è¯­éŸ³ä¸Šä¸‹æ–‡å¯¹é¢œé¢è‚Œè‚‰åŠ¨ä½œçš„å½±å“ï¼Œå®ç°æ›´å¹³æ»‘ã€æ„ŸçŸ¥ä¸€è‡´çš„åŠ¨ç”»æ•ˆæœã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šé‡æŒ‡æ ‡å’Œè§†è§‰è´¨é‡ä¸Šå‡æœ‰æ‰€æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»æ—¨åœ¨ç”Ÿæˆä¸éŸ³é¢‘åŒæ­¥çš„çœŸå®é¢éƒ¨è¿åŠ¨ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦é€šè¿‡æœ€å°åŒ–é‡å»ºæŸå¤±æ¥å®ç°å¸§ä¸åœ°é¢çœŸå®çš„å¯¹é½ï¼Œä½†è¿™ç§æ–¹æ³•å¿½ç•¥äº†é¢éƒ¨è¿åŠ¨çš„è¿ç»­æ€§ã€‚</li>
<li>æå‡ºä¸€ç§æ–°é¢–çš„è¯­éŸ³ä¸Šä¸‹æ–‡æ„ŸçŸ¥æŸå¤±å‡½æ•°ï¼Œå»ºæ¨¡è¯­éŸ³ä¸Šä¸‹æ–‡å¯¹é¢œé¢è‚Œè‚‰åŠ¨ä½œçš„å½±å“ã€‚</li>
<li>é€šè¿‡å¼•å…¥é¢œé¢è‚Œè‚‰åŠ¨ä½œååŒå‘éŸ³æƒé‡ï¼Œå¯¹é¢éƒ¨è¿åŠ¨è¿›è¡Œè‡ªé€‚åº”é‡è¦æ€§åˆ†é…ï¼Œç¡®ä¿åŠ¨ç”»æ›´å¹³æ»‘ã€æ„ŸçŸ¥ä¸€è‡´ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šé‡æŒ‡æ ‡å’Œè§†è§‰è´¨é‡ä¸Šå‡æœ‰æ‰€æå‡ã€‚</li>
<li>è¯¥æ–¹æ³•å¼ºè°ƒäº†æ˜ç¡®å»ºæ¨¡è¯­éŸ³ä¸Šä¸‹æ–‡ç›¸å…³çš„é¢œé¢è‚Œè‚‰åŠ¨ä½œåœ¨åˆæˆè‡ªç„¶è¯­éŸ³é©±åŠ¨3Dé¢éƒ¨åŠ¨ç”»ä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20568">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fd4c29224c7da2ad9239e3b5dafc0c46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d834e69b32ee4cd770df812a61025d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ff5507140b2cd6e367d151d3a6b2f60.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="TalkLess-Blending-Extractive-and-Abstractive-Speech-Summarization-for-Editing-Speech-to-Preserve-Content-and-Style"><a href="#TalkLess-Blending-Extractive-and-Abstractive-Speech-Summarization-for-Editing-Speech-to-Preserve-Content-and-Style" class="headerlink" title="TalkLess: Blending Extractive and Abstractive Speech Summarization for   Editing Speech to Preserve Content and Style"></a>TalkLess: Blending Extractive and Abstractive Speech Summarization for   Editing Speech to Preserve Content and Style</h2><p><strong>Authors:Karim Benharrak, Puyuan Peng, Amy Pavel</strong></p>
<p>Millions of people listen to podcasts, audio stories, and lectures, but editing speech remains tedious and time-consuming. Creators remove unnecessary words, cut tangential discussions, and even re-record speech to make recordings concise and engaging. Prior work automatically summarized speech by removing full sentences (extraction), but rigid extraction limits expressivity. AI tools can summarize then re-synthesize speech (abstraction), but abstraction strips the speakerâ€™s style. We present TalkLess, a system that flexibly combines extraction and abstraction to condense speech while preserving its content and style. To edit speech, TalkLess first generates possible transcript edits, selects edits to maximize compression, coverage, and audio quality, then uses a speech editing model to translate transcript edits into audio edits. TalkLessâ€™s interface provides creators control over automated edits by separating low-level wording edits (via the compression pane) from major content edits (via the outline pane). TalkLess achieves higher coverage and removes more speech errors than a state-of-the-art extractive approach. A comparison study (N&#x3D;12) showed that TalkLess significantly decreased cognitive load and editing effort in speech editing. We further demonstrate TalkLessâ€™s potential in an exploratory study (N&#x3D;3) where creators edited their own speech. </p>
<blockquote>
<p>æˆåƒä¸Šä¸‡çš„äººè†å¬æ’­å®¢ã€éŸ³é¢‘æ•…äº‹å’Œè®²åº§ï¼Œä½†ç¼–è¾‘è¯­éŸ³ä»ç„¶ç¹çè€—æ—¶ã€‚åˆ›ä½œè€…ä¼šåˆ é™¤å¤šä½™çš„è¯æ±‡ã€å‰ªè¾‘ç¦»é¢˜çš„å†…å®¹ï¼Œç”šè‡³é‡æ–°å½•åˆ¶è¯­éŸ³ï¼Œä»¥ä½¿å½•éŸ³æ›´åŠ ç®€æ´ã€å¼•äººå…¥èƒœã€‚ä¹‹å‰çš„å·¥ä½œé€šè¿‡ç§»é™¤æ•´ä¸ªå¥å­ï¼ˆæå–ï¼‰æ¥è‡ªåŠ¨æ€»ç»“è¯­éŸ³ï¼Œä½†åƒµåŒ–çš„æå–æ–¹å¼é™åˆ¶äº†è¡¨è¾¾åŠ›ã€‚AIå·¥å…·å¯ä»¥æ€»ç»“ç„¶åé‡æ–°åˆæˆè¯­éŸ³ï¼ˆæŠ½è±¡ï¼‰ï¼Œä½†æŠ½è±¡ä¼šå‰¥å¤ºè¯´è¯è€…çš„é£æ ¼ã€‚æˆ‘ä»¬æ¨å‡ºäº†TalkLessç³»ç»Ÿï¼Œå®ƒçµæ´»åœ°ç»“åˆäº†æå–å’ŒæŠ½è±¡ï¼Œæµ“ç¼©è¯­éŸ³çš„åŒæ—¶ä¿ç•™å…¶å†…å®¹é£æ ¼ã€‚ä¸ºäº†ç¼–è¾‘è¯­éŸ³ï¼ŒTalkLessé¦–å…ˆç”Ÿæˆå¯èƒ½çš„è½¬å½•ç¼–è¾‘ï¼Œé€‰æ‹©ç¼–è¾‘ä»¥æœ€å¤§åŒ–å‹ç¼©ã€è¦†ç›–èŒƒå›´å’ŒéŸ³é¢‘è´¨é‡ï¼Œç„¶åä½¿ç”¨è¯­éŸ³ç¼–è¾‘æ¨¡å‹å°†è½¬å½•ç¼–è¾‘è½¬åŒ–ä¸ºéŸ³é¢‘ç¼–è¾‘ã€‚TalkLessçš„ç•Œé¢é€šè¿‡åŒºåˆ†ä½çº§åˆ«çš„æªè¾ç¼–è¾‘ï¼ˆé€šè¿‡å‹ç¼©é¢æ¿ï¼‰å’Œä¸»è¦çš„å†…å®¹ç¼–è¾‘ï¼ˆé€šè¿‡å¤§çº²é¢æ¿ï¼‰ï¼Œä¸ºåˆ›ä½œè€…æä¾›äº†å¯¹è‡ªåŠ¨ç¼–è¾‘çš„æ§åˆ¶æƒã€‚TalkLessçš„è¦†ç›–èŒƒå›´æ›´å¹¿ï¼Œå»é™¤çš„è¯­éŸ³é”™è¯¯æ¯”æœ€å…ˆè¿›çš„æå–æ–¹æ³•æ›´å¤šã€‚ä¸€é¡¹æ¯”è¾ƒç ”ç©¶ï¼ˆN&#x3D;12ï¼‰æ˜¾ç¤ºï¼ŒTalkLessæ˜¾è‘—é™ä½äº†è¯­éŸ³ç¼–è¾‘çš„è®¤çŸ¥è´Ÿè·å’ŒåŠªåŠ›ç¨‹åº¦ã€‚åœ¨æ¢ç´¢æ€§ç ”ç©¶ï¼ˆN&#x3D;3ï¼‰ä¸­ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å±•ç¤ºäº†åˆ›ä½œè€…ä½¿ç”¨TalkLessç¼–è¾‘è‡ªå·±çš„è¯­éŸ³çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15202v2">PDF</a> Accepted to The 38th Annual ACM Symposium on User Interface Software   and Technology (UIST â€˜25), September 28-October 1, 2025, Busan, Republic of   Korea. 19 pages</p>
<p><strong>Summary</strong></p>
<p>è°ˆåˆ°ä¸€ä¸ªåä¸ºTalkLessçš„ç³»ç»Ÿï¼Œå®ƒå°†è¯­éŸ³è¯†åˆ«æŠ€æœ¯ä¸äººå·¥æ™ºèƒ½æŠ€æœ¯ç»“åˆï¼Œå®ç°å¯¹æ¼”è®²å†…å®¹çš„ç²¾ç®€å’Œé£æ ¼ä¿ç•™ã€‚è¯¥ç³»ç»Ÿå¯ä»¥ç”Ÿæˆå¯èƒ½çš„è½¬å½•ç¼–è¾‘ï¼Œé€‰æ‹©èƒ½æœ€å¤§åŒ–å‹ç¼©ã€è¦†ç›–å’ŒéŸ³é¢‘è´¨é‡çš„ç¼–è¾‘ï¼Œå¹¶å°†è½¬å½•ç¼–è¾‘è½¬åŒ–ä¸ºéŸ³é¢‘ç¼–è¾‘ã€‚TalkLessç•Œé¢æä¾›åˆ›ä½œè€…å¯¹è‡ªåŠ¨ç¼–è¾‘çš„æ§åˆ¶ï¼Œåˆ†ä¸ºä½çº§åˆ«çš„æªè¾ç¼–è¾‘ï¼ˆé€šè¿‡å‹ç¼©é¢æ¿ï¼‰å’Œä¸»è¦çš„å†…å®¹ç¼–è¾‘ï¼ˆé€šè¿‡å¤§çº²é¢æ¿ï¼‰ã€‚ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒTalkLessåœ¨è¦†ç›–ç‡å’Œå»é™¤è¯­éŸ³é”™è¯¯æ–¹é¢è¡¨ç°æ›´ä½³ï¼ŒåŒæ—¶èƒ½æ˜¾è‘—é™ä½è®¤çŸ¥è´Ÿè·å’Œç¼–è¾‘å·¥ä½œé‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TalkLessç³»ç»Ÿç»“åˆäº†è¯­éŸ³è¯†åˆ«ä¸äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œæ—¨åœ¨ç¼–è¾‘è¯­éŸ³å†…å®¹ã€‚</li>
<li>å®ƒèƒ½å¤Ÿç”Ÿæˆå¯èƒ½çš„è½¬å½•ç¼–è¾‘ï¼Œå¹¶æ™ºèƒ½é€‰æ‹©æœ€å¤§åŒ–å‹ç¼©ã€è¦†ç›–å’ŒéŸ³é¢‘è´¨é‡çš„ç¼–è¾‘ã€‚</li>
<li>TalkLessç»“åˆäº†æå–å’ŒæŠ½è±¡æŠ€æœ¯ï¼Œèƒ½åœ¨ä¿æŒå†…å®¹çš„åŒæ—¶è°ƒæ•´è¯­éŸ³ã€‚</li>
<li>ç³»ç»Ÿç•Œé¢æä¾›ä¸¤ç§ç¼–è¾‘æ¨¡å¼ï¼šé’ˆå¯¹æªè¾çš„ä½çº§åˆ«ç¼–è¾‘å’Œé’ˆå¯¹ä¸»è¦å†…å®¹çš„ç¼–è¾‘ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒTalkLessåœ¨è¦†ç›–ç‡å’Œæ¶ˆé™¤è¯­éŸ³é”™è¯¯æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>TalkLessèƒ½æ˜¾è‘—é™ä½è®¤çŸ¥è´Ÿè·å’Œç¼–è¾‘å·¥ä½œé‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15202">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d52664c28596825a99f8434607d2bfb1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ede19c87d450f35c836c702144d4becb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cb536c9040977dd2f2f618ccc8c00d2.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Enhancing-Target-Speaker-Extraction-with-Explicit-Speaker-Consistency-Modeling"><a href="#Enhancing-Target-Speaker-Extraction-with-Explicit-Speaker-Consistency-Modeling" class="headerlink" title="Enhancing Target Speaker Extraction with Explicit Speaker Consistency   Modeling"></a>Enhancing Target Speaker Extraction with Explicit Speaker Consistency   Modeling</h2><p><strong>Authors:Shu Wu, Anbin Qi, Yanzhang Xie, Xiang Xie</strong></p>
<p>Target Speaker Extraction (TSE) uses a reference cue to extract the target speech from a mixture. In TSE systems relying on audio cues, the speaker embedding from the enrolled speech is crucial to performance. However, these embeddings may suffer from speaker identity confusion. Unlike previous studies that focus on improving speaker embedding extraction, we improve TSE performance from the perspective of speaker consistency. In this paper, we propose a speaker consistency-aware target speaker extraction method that incorporates a centroid-based speaker consistency loss. This approach enhances TSE performance by ensuring speaker consistency between the enrolled and extracted speech. In addition, we integrate conditional loss suppression into the training process. The experimental results validate the effectiveness of our proposed methods in advancing the TSE performance. A speech demo is available online:<a target="_blank" rel="noopener" href="https://sc-tse.netlify.app/">https://sc-tse.netlify.app/</a> </p>
<blockquote>
<p>ç›®æ ‡è¯´è¯äººæå–ï¼ˆTSEï¼‰ä½¿ç”¨å‚è€ƒçº¿ç´¢ä»æ··åˆè¯­éŸ³ä¸­æå–ç›®æ ‡è¯­éŸ³ã€‚åœ¨ä¾èµ–éŸ³é¢‘çº¿ç´¢çš„TSEç³»ç»Ÿä¸­ï¼Œæ³¨å†Œè¯­éŸ³çš„è¯´è¯äººåµŒå…¥å¯¹æ€§èƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¿™äº›åµŒå…¥å¯èƒ½ä¼šå—åˆ°è¯´è¯äººèº«ä»½æ··æ·†çš„å½±å“ã€‚ä¸åŒäºä¹‹å‰ä¸“æ³¨äºæé«˜è¯´è¯äººåµŒå…¥æå–çš„ç ”ç©¶ï¼Œæˆ‘ä»¬ä»è¯´è¯äººä¸€è‡´æ€§çš„è§’åº¦æé«˜TSEçš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè´¨å¿ƒçš„è¯´è¯äººä¸€è‡´æ€§æ„ŸçŸ¥ç›®æ ‡è¯´è¯äººæå–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†åŸºäºè´¨å¿ƒçš„è¯´è¯äººä¸€è‡´æ€§æŸå¤±ã€‚è¿™ç§æ–¹æ³•é€šè¿‡ç¡®ä¿æ³¨å†Œè¯­éŸ³å’Œæå–è¯­éŸ³ä¹‹é—´çš„è¯´è¯äººä¸€è‡´æ€§ï¼Œæé«˜äº†TSEçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ¡ä»¶æŸå¤±æŠ‘åˆ¶èå…¥è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœéªŒè¯äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨æé«˜TSEæ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚åœ¨çº¿æ¼”ç¤ºé“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://sc-tse.netlify.app/">https://sc-tse.netlify.app/</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09510v3">PDF</a> preprint</p>
<p><strong>Summary</strong>ï¼šç›®æ ‡è¯´è¯äººæå–ï¼ˆTSEï¼‰åˆ©ç”¨å‚è€ƒçº¿ç´¢ä»æ··åˆè¯­éŸ³ä¸­æå–ç›®æ ‡è¯­éŸ³ã€‚åœ¨ä¾èµ–éŸ³é¢‘çº¿ç´¢çš„TSEç³»ç»Ÿä¸­ï¼Œæ³¨å†Œè¯­éŸ³çš„è¯´è¯äººåµŒå…¥å¯¹æ€§èƒ½è‡³å…³é‡è¦ï¼Œä½†è¿™äº›åµŒå…¥å¯èƒ½ä¼šå—åˆ°è¯´è¯äººèº«ä»½æ··æ·†çš„å½±å“ã€‚æœ¬æ–‡ä»è¯´è¯äººä¸€è‡´æ€§çš„è§’åº¦æé«˜TSEæ€§èƒ½ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè´¨å¿ƒçš„è¯´è¯äººä¸€è‡´æ€§æ„ŸçŸ¥ç›®æ ‡è¯´è¯äººæå–æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†æ¡ä»¶æŸå¤±æŠ‘åˆ¶åˆ°è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæé«˜TSEæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç›®æ ‡è¯´è¯äººæå–ï¼ˆTSEï¼‰ä¾èµ–å‚è€ƒçº¿ç´¢æ¥ä»æ··åˆè¯­éŸ³ä¸­æå–ç›®æ ‡è¯­éŸ³ã€‚</li>
<li>è¯´è¯äººåµŒå…¥åœ¨TSEç³»ç»Ÿä¸­å¯¹æ€§èƒ½è‡³å…³é‡è¦ï¼Œä½†å¯èƒ½å­˜åœ¨è¯´è¯äººèº«ä»½æ··æ·†çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡ä»è¯´è¯äººä¸€è‡´æ€§çš„è§’åº¦æå‡ºæ”¹è¿›TSEæ€§èƒ½çš„æ–¹æ³•ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºè´¨å¿ƒçš„è¯´è¯äººä¸€è‡´æ€§æŸå¤±æ¥æé«˜TSEæ€§èƒ½ï¼Œç¡®ä¿æ³¨å†Œå’Œæå–çš„è¯­éŸ³ä¹‹é—´çš„è¯´è¯äººä¸€è‡´æ€§ã€‚</li>
<li>å°†æ¡ä»¶æŸå¤±æŠ‘åˆ¶é›†æˆåˆ°è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¿›ä¸€æ­¥æé«˜TSEçš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09510">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-89abe391191dca8668a9d92aa8258f8f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c667beefa0bfb40f72ddf8d9bb7e7051.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8461d61855d99a822119f23e8afd8a6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52feb30be59155797134b9f5546069f5.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DMF2Mel-A-Dynamic-Multiscale-Fusion-Network-for-EEG-Driven-Mel-Spectrogram-Reconstruction"><a href="#DMF2Mel-A-Dynamic-Multiscale-Fusion-Network-for-EEG-Driven-Mel-Spectrogram-Reconstruction" class="headerlink" title="DMF2Mel: A Dynamic Multiscale Fusion Network for EEG-Driven Mel   Spectrogram Reconstruction"></a>DMF2Mel: A Dynamic Multiscale Fusion Network for EEG-Driven Mel   Spectrogram Reconstruction</h2><p><strong>Authors:Cunhang Fan, Sheng Zhang, Jingjing Zhang, Enrui Liu, Xinhui Li, Gangming Zhao, Zhao Lv</strong></p>
<p>Decoding speech from brain signals is a challenging research problem. Although existing technologies have made progress in reconstructing the mel spectrograms of auditory stimuli at the word or letter level, there remain core challenges in the precise reconstruction of minute-level continuous imagined speech: traditional models struggle to balance the efficiency of temporal dependency modeling and information retention in long-sequence decoding. To address this issue, this paper proposes the Dynamic Multiscale Fusion Network (DMF2Mel), which consists of four core components: the Dynamic Contrastive Feature Aggregation Module (DC-FAM), the Hierarchical Attention-Guided Multi-Scale Network (HAMS-Net), the SplineMap attention mechanism, and the bidirectional state space module (convMamba). Specifically, the DC-FAM separates speech-related â€œforeground featuresâ€ from noisy â€œbackground featuresâ€ through local convolution and global attention mechanisms, effectively suppressing interference and enhancing the representation of transient signals. HAMS-Net, based on the U-Net framework,achieves cross-scale fusion of high-level semantics and low-level details. The SplineMap attention mechanism integrates the Adaptive Gated Kolmogorov-Arnold Network (AGKAN) to combine global context modeling with spline-based local fitting. The convMamba captures long-range temporal dependencies with linear complexity and enhances nonlinear dynamic modeling capabilities. Results on the SparrKULee dataset show that DMF2Mel achieves a Pearson correlation coefficient of 0.074 in mel spectrogram reconstruction for known subjects (a 48% improvement over the baseline) and 0.048 for unknown subjects (a 35% improvement over the baseline).Code is available at: <a target="_blank" rel="noopener" href="https://github.com/fchest/DMF2Mel">https://github.com/fchest/DMF2Mel</a>. </p>
<blockquote>
<p>ä»è„‘ç”µæ³¢è§£ç è¯­éŸ³æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ç ”ç©¶è¯¾é¢˜ã€‚å°½ç®¡ç°æœ‰æŠ€æœ¯å·²åœ¨é‡å»ºå¬è§‰åˆºæ¿€çš„æ¢…å°”é¢‘è°±å›¾ï¼ˆåœ¨å•è¯æˆ–å­—æ¯å±‚é¢ï¼‰æ–¹é¢å–å¾—è¿›å±•ï¼Œä½†åœ¨ç²¾ç¡®é‡å»ºåˆ†é’Ÿçº§è¿ç»­æƒ³è±¡ä¸­çš„è¯­éŸ³æ–¹é¢ä»å­˜åœ¨æ ¸å¿ƒæŒ‘æˆ˜ï¼šä¼ ç»Ÿæ¨¡å‹åœ¨å¹³è¡¡æ—¶é—´ä¾èµ–æ€§å»ºæ¨¡çš„æ•ˆç‡å’Œé•¿åºåˆ—è§£ç ä¸­çš„ä¿¡æ¯ä¿ç•™æ–¹é¢é¢ä¸´å›°éš¾ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŠ¨æ€å¤šå°ºåº¦èåˆç½‘ç»œï¼ˆDMF2Melï¼‰ï¼Œå®ƒåŒ…å«å››ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šåŠ¨æ€å¯¹æ¯”ç‰¹å¾èšåˆæ¨¡å—ï¼ˆDC-FAMï¼‰ã€åˆ†å±‚æ³¨æ„åŠ›å¼•å¯¼å¤šå°ºåº¦ç½‘ç»œï¼ˆHAMS-Netï¼‰ã€SplineMapæ³¨æ„åŠ›æœºåˆ¶å’ŒåŒå‘çŠ¶æ€ç©ºé—´æ¨¡å—ï¼ˆconvMambaï¼‰ã€‚å…·ä½“æ¥è¯´ï¼ŒDC-FAMé€šè¿‡å±€éƒ¨å·ç§¯å’Œå…¨å±€æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†è¯­éŸ³ç›¸å…³çš„â€œå‰æ™¯ç‰¹å¾â€ä¸å™ªå£°â€œèƒŒæ™¯ç‰¹å¾â€åˆ†ç¦»ï¼Œæœ‰æ•ˆåœ°æŠ‘åˆ¶äº†å¹²æ‰°å¹¶å¢å¼ºäº†ç¬æ€ä¿¡å·çš„è¡¨ç¤ºã€‚HAMS-NetåŸºäºU-Netæ¡†æ¶ï¼Œå®ç°äº†é«˜çº§è¯­ä¹‰å’Œä½çº§ç»†èŠ‚çš„è·¨å°ºåº¦èåˆã€‚SplineMapæ³¨æ„åŠ›æœºåˆ¶ç»“åˆäº†è‡ªé€‚åº”é—¨æ§Kolmogorov-Arnoldç½‘ç»œï¼ˆAGKANï¼‰ï¼Œå°†å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡ä¸åŸºäºæ ·æ¡çš„å±€éƒ¨æ‹Ÿåˆç›¸ç»“åˆã€‚convMambaä»¥çº¿æ€§å¤æ‚åº¦æ•æ‰é•¿æœŸæ—¶é—´ä¾èµ–æ€§ï¼Œå¹¶å¢å¼ºäº†éçº¿æ€§åŠ¨æ€å»ºæ¨¡èƒ½åŠ›ã€‚åœ¨SparrKULeeæ•°æ®é›†ä¸Šçš„ç»“æœè¡¨æ˜ï¼ŒDMF2Melåœ¨å·²çŸ¥ä¸»é¢˜çš„æ¢…å°”é¢‘è°±å›¾é‡å»ºä¸­å®ç°äº†0.074çš„çš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆæ¯”åŸºçº¿æé«˜äº†48%ï¼‰ï¼Œåœ¨æœªçŸ¥ä¸»é¢˜ä¸Šå®ç°äº†0.048ï¼ˆæ¯”åŸºçº¿æé«˜äº†35%ï¼‰ã€‚ä»£ç å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/fchest/DMF2Mel%E3%80%82">https://github.com/fchest/DMF2Melã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07526v3">PDF</a> Accepted by ACM MM 2025</p>
<p><strong>Summary</strong><br>è§£ç è„‘ç”µæ³¢ä¸­çš„è¯­éŸ³æ˜¯ä¸€ä¸ªæŒ‘æˆ˜æ€§çš„ç ”ç©¶è¯¾é¢˜ã€‚ç°æœ‰æŠ€æœ¯è™½å·²èƒ½åœ¨å­—è¯çº§åˆ«é‡å»ºå¬è§‰åˆºæ¿€çš„æ¢…å°”é¢‘è°±å›¾ï¼Œä½†åœ¨é‡å»ºåˆ†é’Ÿçº§åˆ«çš„è¿ç»­æƒ³è±¡è¯­éŸ³æ—¶ä»å­˜åœ¨æ ¸å¿ƒæŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ¨¡å‹éš¾ä»¥åœ¨æ—¶åºä¾èµ–å»ºæ¨¡çš„æ•ˆç‡ä¸é•¿åºåˆ—è§£ç çš„ä¿¡æ¯ä¿ç•™ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŠ¨æ€å¤šå°ºåº¦èåˆç½‘ç»œï¼ˆDMF2Melï¼‰ï¼ŒåŒ…å«å››å¤§æ ¸å¿ƒç»„ä»¶ï¼šåŠ¨æ€å¯¹æ¯”ç‰¹å¾èšåˆæ¨¡å—ï¼ˆDC-FAMï¼‰ã€åˆ†å±‚æ³¨æ„åŠ›å¼•å¯¼å¤šå°ºåº¦ç½‘ç»œï¼ˆHAMS-Netï¼‰ã€SplineMapæ³¨æ„åŠ›æœºåˆ¶å’ŒåŒå‘çŠ¶æ€ç©ºé—´æ¨¡å—ï¼ˆconvMambaï¼‰ã€‚DMF2Melåœ¨SparrKULeeæ•°æ®é›†ä¸Šçš„é‡å»ºç»“æœè¡¨ç°å‡ºè‰²ï¼Œå¯¹å·²çŸ¥ä¸»ä½“å’ŒæœªçŸ¥ä¸»ä½“çš„æ¢…å°”é¢‘è°±å›¾é‡å»ºçš„Pearsonç›¸å…³ç³»æ•°åˆ†åˆ«è¾¾åˆ°äº†0.074å’Œ0.048ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æœ‰æ˜¾è‘—æ”¹å–„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§£ç è„‘ç”µæ³¢ä¸­çš„è¯­éŸ³æ˜¯ä¸€ä¸ªå……æ»¡æŒ‘æˆ˜çš„ç ”ç©¶è¯¾é¢˜ã€‚</li>
<li>å½“å‰æŠ€æœ¯åœ¨é‡å»ºè¿ç»­æƒ³è±¡è¯­éŸ³æ—¶é¢ä¸´æ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿæ¨¡å‹åœ¨æ—¶åºä¾èµ–å»ºæ¨¡å’Œä¿¡æ¯ä¿ç•™æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>DMF2Melç½‘ç»œç”±å››å¤§æ ¸å¿ƒç»„ä»¶æ„æˆï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>DC-FAMèƒ½æœ‰æ•ˆåˆ†ç¦»è¯­éŸ³ç›¸å…³çš„â€œå‰æ™¯ç‰¹å¾â€å’Œå¹²æ‰°çš„â€œèƒŒæ™¯ç‰¹å¾â€ã€‚</li>
<li>HAMS-NetåŸºäºU-Netæ¡†æ¶ï¼Œå®ç°é«˜å±‚æ¬¡è¯­ä¹‰ä¸ä½å±‚æ¬¡ç»†èŠ‚çš„è·¨å°ºåº¦èåˆã€‚</li>
<li>DMF2Melåœ¨SparrKULeeæ•°æ®é›†ä¸Šçš„é‡å»ºç»“æœä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¯¹å·²çŸ¥å’ŒæœªçŸ¥ä¸»ä½“çš„é‡å»ºå‡æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07526">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-75dd89c90e47f6a755ae962ddd939ffa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a94afe6e918056d864f9d8747f150815.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cda5a2b51211f1faf9948e972ee09eaf.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Verbal-Werewolf-Engage-Users-with-Verbalized-Agentic-Werewolf-Game-Framework"><a href="#Verbal-Werewolf-Engage-Users-with-Verbalized-Agentic-Werewolf-Game-Framework" class="headerlink" title="Verbal Werewolf: Engage Users with Verbalized Agentic Werewolf Game   Framework"></a>Verbal Werewolf: Engage Users with Verbalized Agentic Werewolf Game   Framework</h2><p><strong>Authors:Qihui Fan, Wenbo Li, Enfu Nan, Yixiao Chen, Lei Lu, Pu Zhao, Yanzhi Wang</strong></p>
<p>The growing popularity of social deduction games has created an increasing need for intelligent frameworks where humans can collaborate with AI agents, particularly in post-pandemic contexts with heightened psychological and social pressures. Social deduction games like Werewolf, traditionally played through verbal communication, present an ideal application for Large Language Models (LLMs) given their advanced reasoning and conversational capabilities. Prior studies have shown that LLMs can outperform humans in Werewolf games, but their reliance on external modules introduces latency that left their contribution in academic domain only, and omit such game should be user-facing. We propose \textbf{Verbal Werewolf}, a novel LLM-based Werewolf game system that optimizes two parallel pipelines: gameplay powered by state-of-the-art LLMs and a fine-tuned Text-to-Speech (TTS) module that brings text output to life. Our system operates in near real-time without external decision-making modules, leveraging the enhanced reasoning capabilities of modern LLMs like DeepSeek V3 to create a more engaging and anthropomorphic gaming experience that significantly improves user engagement compared to existing text-only frameworks. </p>
<blockquote>
<p>éšç€ç¤¾äº¤æ¨ç†æ¸¸æˆçš„æ—¥ç›Šæ™®åŠï¼Œäººç±»ä¸AIä»£ç†åä½œçš„æ™ºèƒ½æ¡†æ¶çš„éœ€æ±‚ä¹Ÿåœ¨ä¸æ–­å¢åŠ ï¼Œç‰¹åˆ«æ˜¯åœ¨ç–«æƒ…åå¿ƒç†å’Œç¤¾äº¤å‹åŠ›åŠ å‰§çš„èƒŒæ™¯ä¸‹ã€‚ç‹¼äººæ€ç­‰ç¤¾äº¤æ¨ç†æ¸¸æˆé€šè¿‡å£å¤´äº¤æµè¿›è¡Œï¼Œé‰´äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡å…ˆè¿›çš„æ¨ç†å’Œå¯¹è¯èƒ½åŠ›ï¼Œæ˜¯ç†æƒ³çš„é€‚ç”¨å¯¹è±¡ã€‚æ—©æœŸç ”ç©¶è¡¨æ˜ï¼ŒLLMåœ¨ç‹¼äººæ€æ¸¸æˆä¸­çš„è¡¨ç°å¯ä»¥è¶…è¶Šäººç±»ï¼Œä½†å®ƒä»¬å¯¹å¤–éƒ¨æ¨¡å—çš„ä¾èµ–å¯¼è‡´å»¶è¿Ÿï¼Œè¿™ä½¿å®ƒä»¬çš„è´¡çŒ®ä»…é™äºå­¦æœ¯é¢†åŸŸï¼Œå¹¶å¿½ç•¥äº†è¿™æ ·çš„æ¸¸æˆåº”è¯¥æ˜¯é¢å‘ç”¨æˆ·çš„ã€‚æˆ‘ä»¬æå‡ºäº†â€œVerbal Werewolfâ€ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºLLMçš„æ–°å‹ç‹¼äººæ€æ¸¸æˆç³»ç»Ÿï¼Œä¼˜åŒ–äº†ä¸¤ä¸ªå¹¶è¡Œç®¡é“ï¼šç”±æœ€æ–°LLMé©±åŠ¨çš„æ¸¸æˆç©æ³•å’Œç»è¿‡ç²¾ç»†è°ƒæ•´çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å—ï¼Œå°†æ–‡æœ¬è¾“å‡ºè½¬åŒ–ä¸ºç”ŸåŠ¨çš„å£°éŸ³ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨è¿‘å®æ—¶çŠ¶æ€ä¸‹è¿è¡Œï¼Œæ— éœ€å¤–éƒ¨å†³ç­–æ¨¡å—ï¼Œåˆ©ç”¨ç°ä»£LLMï¼ˆå¦‚DeepSeek V3ï¼‰çš„å¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œåˆ›å»ºäº†ä¸€ç§æ›´å…·å‚ä¸æ„Ÿå’Œæ‹ŸäººåŒ–çš„æ¸¸æˆä½“éªŒï¼Œç›¸è¾ƒäºç°æœ‰çš„çº¯æ–‡æœ¬æ¡†æ¶ï¼Œæå¤§åœ°æé«˜äº†ç”¨æˆ·å‚ä¸åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00160v2">PDF</a> </p>
<p><strong>Summary</strong><br>ç¤¾äº¤æ¨ç†æ¸¸æˆçš„æ™®åŠä»¥åŠå¯¹æ™ºèƒ½æ¡†æ¶çš„éœ€æ±‚ä¸æ–­å¢é•¿ï¼Œç‰¹åˆ«æ˜¯åœ¨ç–«æƒ…åå¿ƒç†å’Œç¤¾äº¤å‹åŠ›å¢åŠ çš„èƒŒæ™¯ä¸‹ã€‚æ–‡å­—ç‰ˆç‹¼äººæ€ä½œä¸ºä¸€ç§ç¤¾ä¼šæ¨ç†æ¸¸æˆï¼Œå› å…¶é€»è¾‘æ¨ç†å’Œå¯¹è¯èƒ½åŠ›æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç†æƒ³åº”ç”¨ã€‚å°½ç®¡ç ”ç©¶è¡¨æ˜LLMåœ¨ç‹¼äººæ€æ¸¸æˆä¸­è¡¨ç°ä¼˜äºäººç±»ï¼Œä½†ç”±äºå¯¹å¤–éƒ¨æ¨¡å—çš„ä¾èµ–ï¼Œå¯¼è‡´ååº”æ—¶é—´å»¶è¿Ÿè€Œå±€é™äºå­¦æœ¯é¢†åŸŸã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œVerbal Werewolfâ€çš„æ–°å‹ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿä¼˜åŒ–äº†æ¸¸æˆæµç¨‹å¹¶é‡‡ç”¨äº†å…ˆè¿›çš„LLMå’Œç²¾ç»†è°ƒæ•´çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å—ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨æ— å¤–éƒ¨å†³ç­–æ¨¡å—çš„æƒ…å†µä¸‹å‡ ä¹å®æ—¶è¿è¡Œï¼Œåˆ©ç”¨ç°ä»£LLMçš„æ¨ç†èƒ½åŠ›åˆ›å»ºä¸€ä¸ªæ›´æœ‰è¶£çš„äººå‹æ¸¸æˆä½“éªŒï¼Œä¸ç”¨æˆ·å‚ä¸ç°æœ‰çº¯æ–‡æœ¬æ¡†æ¶ç›¸æ¯”æ˜¾è‘—æé«˜ç”¨æˆ·å‚ä¸åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¤¾ä¼šæ¨ç†æ¸¸æˆå—æ¬¢è¿åº¦å¢é•¿éœ€è¦äººç±»ä¸AIåä½œçš„æ™ºèƒ½æ¡†æ¶ã€‚</li>
<li>æ–‡å­—ç‰ˆç‹¼äººæ€æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç†æƒ³åº”ç”¨ï¼Œå…·å¤‡é€»è¾‘æ¨ç†å’Œå¯¹è¯èƒ½åŠ›ã€‚</li>
<li>LLMåœ¨ç‹¼äººæ€æ¸¸æˆä¸­è¡¨ç°ä¼˜äºäººç±»ï¼Œä½†å¯¹å¤–éƒ¨æ¨¡å—çš„ä¾èµ–å¯¼è‡´ååº”å»¶è¿Ÿé™åˆ¶äº†å®é™…åº”ç”¨ã€‚</li>
<li>â€œVerbal Werewolfâ€ç³»ç»Ÿä¼˜åŒ–äº†ä¸¤å¤§æ¨¡å—ï¼šé‡‡ç”¨å…ˆè¿›LLMçš„æ¸¸æˆæµç¨‹å’Œç²¾ç»†è°ƒæ•´çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å—ã€‚</li>
<li>ç³»ç»Ÿå®ç°äº†è¿‘å®æ—¶è¿è¡Œï¼Œæ— éœ€å¤–éƒ¨å†³ç­–æ¨¡å—ã€‚</li>
<li>åˆ©ç”¨ç°ä»£LLMçš„æ¨ç†èƒ½åŠ›æé«˜äº†æ¸¸æˆçš„è¶£å‘³æ€§å’Œå‚ä¸åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00160">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3ee0fd2b5cfacca1961c36a37a108581.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1caaddeb0632cb637c347708c653112d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5622418dd1803c7444a1e930848d4e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1fbfaa06e7aeedeb6cd012fb18ed839.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Voice-Conversion-via-Content-Aware-Timbre-Ensemble-and-Conditional-Flow-Matching"><a href="#Zero-Shot-Voice-Conversion-via-Content-Aware-Timbre-Ensemble-and-Conditional-Flow-Matching" class="headerlink" title="Zero-Shot Voice Conversion via Content-Aware Timbre Ensemble and   Conditional Flow Matching"></a>Zero-Shot Voice Conversion via Content-Aware Timbre Ensemble and   Conditional Flow Matching</h2><p><strong>Authors:Yu Pan, Yuguang Yang, Jixun Yao, Lei Ma, Jianjun Zhao</strong></p>
<p>Despite recent advances in zero-shot voice conversion (VC), achieving speaker similarity and naturalness comparable to ground-truth recordings remains a significant challenge. In this letter, we propose CTEFM-VC, a zero-shot VC framework that integrates content-aware timbre ensemble modeling with conditional flow matching. Specifically, CTEFM-VC decouples utterances into content and timbre representations and leverages a conditional flow matching model to reconstruct the Mel-spectrogram of the source speech. To enhance its timbre modeling capability and naturalness of generated speech, we first introduce a context-aware timbre ensemble modeling approach that adaptively integrates diverse speaker verification embeddings and enables the effective utilization of source content and target timbre elements through a cross-attention module. Furthermore, a structural similarity-based timbre loss is presented to jointly train CTEFM-VC end-to-end. Experiments show that CTEFM-VC consistently achieves the best performance in all metrics assessing speaker similarity, speech naturalness, and intelligibility, significantly outperforming state-of-the-art zero-shot VC systems. </p>
<blockquote>
<p>å°½ç®¡é›¶æ ·æœ¬è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰é¢†åŸŸè¿‘æœŸå–å¾—äº†è¿›å±•ï¼Œä½†å®ç°ä¸çœŸå®å½•éŸ³ç›¸å½“çš„è¯´è¯äººç›¸ä¼¼æ€§å’Œè‡ªç„¶æ€§ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚åœ¨è¿™å°ä¿¡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CTEFM-VCï¼Œè¿™æ˜¯ä¸€ä¸ªé›¶æ ·æœ¬VCæ¡†æ¶ï¼Œå®ƒç»“åˆäº†å†…å®¹æ„ŸçŸ¥éŸ³è‰²é›†åˆå»ºæ¨¡å’Œæ¡ä»¶æµåŒ¹é…ã€‚å…·ä½“æ¥è¯´ï¼ŒCTEFM-VCå°†è¯è¯­åˆ†è§£æˆå†…å®¹å’ŒéŸ³è‰²è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨æ¡ä»¶æµåŒ¹é…æ¨¡å‹é‡å»ºæºè¯­éŸ³çš„æ¢…å°”é¢‘è°±å›¾ã€‚ä¸ºäº†å¢å¼ºå…¶æ—¶åŸŸæ¨¡å‹çš„èƒ½åŠ›å’Œç”Ÿæˆçš„è¯­éŸ³çš„è‡ªç„¶åº¦ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ä¸€ç§ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„éŸ³è‰²é›†åˆå»ºæ¨¡æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è‡ªé€‚åº”åœ°é›†æˆäº†å¤šç§è¯´è¯äººéªŒè¯åµŒå…¥ï¼Œå¹¶é€šè¿‡äº¤å‰æ³¨æ„åŠ›æ¨¡å—å®ç°äº†æºå†…å®¹å’Œç›®æ ‡éŸ³è‰²å…ƒç´ çš„æœ‰æ•ˆåˆ©ç”¨ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§åŸºäºç»“æ„ç›¸ä¼¼æ€§çš„éŸ³è‰²æŸå¤±ï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è”åˆè®­ç»ƒCTEFM-VCã€‚å®éªŒè¡¨æ˜ï¼ŒCTEFM-VCåœ¨è¯„ä¼°è¯´è¯äººç›¸ä¼¼æ€§ã€è¯­éŸ³è‡ªç„¶æ€§å’Œæ¸…æ™°åº¦çš„æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡è¡¨ç°æœ€ä½³ï¼Œæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„é›¶æ ·æœ¬VCç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02026v2">PDF</a> Work in progress; 5 pages;</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æå‡ºäº†CTEFM-VCé›¶å°„å‡»è¯­éŸ³è½¬æ¢æ¡†æ¶ï¼Œèåˆäº†å†…å®¹æ„ŸçŸ¥éŸ³è‰²é›†å»ºæ¨¡ä¸æ¡ä»¶æµåŒ¹é…æŠ€æœ¯ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹è¯­éŸ³è¿›è¡Œå†…å®¹å’ŒéŸ³è‰²åˆ†ç¦»ï¼Œä½¿ç”¨æ¡ä»¶æµåŒ¹é…æ¨¡å‹é‡å»ºæºè¯­éŸ³çš„æ¢…å°”é¢‘è°±å›¾ï¼Œå¢å¼ºäº†éŸ³è‰²å»ºæ¨¡èƒ½åŠ›å’Œç”Ÿæˆè¯­éŸ³çš„è‡ªç„¶åº¦ã€‚å¼•å…¥ä¸Šä¸‹æ–‡æ„ŸçŸ¥éŸ³è‰²é›†å»ºæ¨¡æ–¹æ³•ï¼Œè‡ªé€‚åº”é›†æˆå¤šç§è¯´è¯äººéªŒè¯åµŒå…¥ï¼Œå¹¶é€šè¿‡äº¤å‰æ³¨æ„åŠ›æ¨¡å—å®ç°æºå†…å®¹å’Œç›®æ ‡éŸ³è‰²å…ƒç´ çš„æœ‰æ•ˆåˆ©ç”¨ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç»“æ„ç›¸ä¼¼æ€§çš„éŸ³è‰²æŸå¤±ï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è”åˆè®­ç»ƒCTEFM-VCã€‚å®éªŒè¡¨æ˜ï¼ŒCTEFM-VCåœ¨è¯„ä¼°è¯´è¯äººç›¸ä¼¼æ€§ã€è¯­éŸ³è‡ªç„¶åº¦å’Œæ¸…æ™°åº¦çš„æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡è¡¨ç°æœ€ä½³ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–é›¶å°„å‡»è¯­éŸ³è½¬æ¢ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³äºæ–‡æœ¬å†…å®¹çš„ä¸ƒä¸ªå…³é”®è§è§£ï¼š</p>
<ol>
<li>CTEFM-VCæ˜¯é›¶å°„å‡»è¯­éŸ³è½¬æ¢çš„æ–°æ¡†æ¶ï¼Œç»“åˆäº†å†…å®¹æ„ŸçŸ¥éŸ³è‰²é›†å»ºæ¨¡ä¸æ¡ä»¶æµåŒ¹é…æŠ€æœ¯ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡åˆ†ç¦»è¯­éŸ³å†…å®¹å’ŒéŸ³è‰²è¡¨ç¤ºï¼Œä½¿ç”¨æ¡ä»¶æµåŒ¹é…æ¨¡å‹é‡å»ºæºè¯­éŸ³çš„æ¢…å°”é¢‘è°±å›¾ã€‚</li>
<li>å¼•å…¥ä¸Šä¸‹æ–‡æ„ŸçŸ¥éŸ³è‰²é›†å»ºæ¨¡æ–¹æ³•ï¼Œæé«˜äº†éŸ³è‰²å»ºæ¨¡èƒ½åŠ›å’Œç”Ÿæˆè¯­éŸ³çš„è‡ªç„¶åº¦ã€‚</li>
<li>é€šè¿‡äº¤å‰æ³¨æ„åŠ›æ¨¡å—å®ç°æºå†…å®¹å’Œç›®æ ‡éŸ³è‰²å…ƒç´ çš„æœ‰æ•ˆç»“åˆã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºç»“æ„ç›¸ä¼¼æ€§çš„éŸ³è‰²æŸå¤±å‡½æ•°ï¼Œç”¨äºè”åˆè®­ç»ƒCTEFM-VCã€‚</li>
<li>CTEFM-VCåœ¨è¯´è¯äººç›¸ä¼¼æ€§ã€è¯­éŸ³è‡ªç„¶åº¦å’Œæ¸…æ™°åº¦çš„è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.02026">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4238e18e3872b9f3a178bb71f5a22b07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fce4b4990f09db123bd656e394f4805d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b3d962799d9d1106faa16905e933a76b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e547a5a38f2b6b17a72f066773b15c21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b919f00ab90aea1e08df24d892a27a4.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-13/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-13/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-13/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3b7a4db9c5cdf168d724420000cc9e7c.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-13  SystolicAttention Fusing FlashAttention within a Single Systolic Array
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-13/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0f5ff0d234a6fad68a284d21bc750773.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-13  UltraAD Fine-Grained Ultrasound Anomaly Classification via Few-Shot   CLIP Adaptation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28172.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
