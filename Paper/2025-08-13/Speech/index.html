<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-08-13  AD-AVSR Asymmetric Dual-stream Enhancement for Robust Audio-Visual   Speech Recognition">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6dafd82758db69f8164530440d8c0456.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    15k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    61 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-13-更新"><a href="#2025-08-13-更新" class="headerlink" title="2025-08-13 更新"></a>2025-08-13 更新</h1><h2 id="AD-AVSR-Asymmetric-Dual-stream-Enhancement-for-Robust-Audio-Visual-Speech-Recognition"><a href="#AD-AVSR-Asymmetric-Dual-stream-Enhancement-for-Robust-Audio-Visual-Speech-Recognition" class="headerlink" title="AD-AVSR: Asymmetric Dual-stream Enhancement for Robust Audio-Visual   Speech Recognition"></a>AD-AVSR: Asymmetric Dual-stream Enhancement for Robust Audio-Visual   Speech Recognition</h2><p><strong>Authors:Junxiao Xue, Xiaozhen Liu, Xuecheng Wu, Xinyi Yin, Danlei Huang, Fei Yu</strong></p>
<p>Audio-visual speech recognition (AVSR) combines audio-visual modalities to improve speech recognition, especially in noisy environments. However, most existing methods deploy the unidirectional enhancement or symmetric fusion manner, which limits their capability to capture heterogeneous and complementary correlations of audio-visual data-especially under asymmetric information conditions. To tackle these gaps, we introduce a new AVSR framework termed AD-AVSR based on bidirectional modality enhancement. Specifically, we first introduce the audio dual-stream encoding strategy to enrich audio representations from multiple perspectives and intentionally establish asymmetry to support subsequent cross-modal interactions. The enhancement process involves two key components, Audio-aware Visual Refinement Module for enhanced visual representations under audio guidance, and Cross-modal Noise Suppression Masking Module which refines audio representations using visual cues, collaboratively leading to the closed-loop and bidirectional information flow. To further enhance correlation robustness, we adopt a threshold-based selection mechanism to filter out irrelevant or weakly correlated audio-visual pairs. Extensive experimental results on the LRS2 and LRS3 datasets indicate that our AD-AVSR consistently surpasses SOTA methods in both performance and noise robustness, highlighting the effectiveness of our model design. </p>
<blockquote>
<p>视听语音识别（AVSR）结合了视听模式来提高语音识别能力，特别是在嘈杂的环境中。然而，现有的大多数方法采用单向增强或对称融合的方式，这限制了它们捕捉视听数据的异构和互补关联的能力，尤其是在不对称信息条件下。为了弥补这些不足，我们引入了一种基于双向模式增强的新型AVSR框架，称为AD-AVSR。具体来说，我们首先采用音频双流编码策略，从多个角度丰富音频表示，并有意建立不对称性，以支持随后的跨模态交互。增强过程包括两个关键组件：音频感知视觉细化模块，用于在音频指导下增强视觉表示；跨模态噪声抑制掩模模块，使用视觉线索来完善音频表示，共同实现闭环和双向信息流。为了进一步增强关联鲁棒性，我们采用基于阈值的选择机制来过滤掉无关或弱相关的视听对。在LRS2和LRS3数据集上的大量实验结果表明，我们的AD-AVSR在性能和噪声稳健性方面始终超过最新方法，凸显了我们的模型设计的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07608v1">PDF</a> Accepted by the ACM MM 2025 Workshop on SVC</p>
<p><strong>Summary</strong><br>音频视觉语音识别（AVSR）结合视听模式以提高语音识别能力，尤其在噪声环境中。但现有方法大多采用单向增强或对称融合方式，难以捕捉视听数据的异质互补关联，尤其在不对称信息条件下。为解决这些问题，我们提出了基于双向模式增强的新型AVSR框架AD-AVSR。具体而言，我们引入音频双流编码策略，从多个角度丰富音频表征，并建立不对称性以支持后续跨模式交互。增强过程包括两个关键组件：受音频指导的视觉细化模块和跨模式噪声抑制掩模模块，共同实现闭环和双向信息流。为增强关联稳健性，我们采用阈值选择机制过滤掉无关或弱相关的视听对。在LRS2和LRS3数据集上的广泛实验表明，我们的AD-AVSR在性能和噪声稳健性方面均超越现有最佳方法，凸显了模型设计的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AVSR结合了音频视觉模式以提高在噪声环境中的语音识别能力。</li>
<li>现有方法主要采取单向增强或对称融合，难以捕捉视听数据的异质互补性。</li>
<li>提出的AD-AVSR框架基于双向模式增强，引入音频双流编码策略丰富音频表征。</li>
<li>AD-AVSR包括两个关键组件：受音频指导的视觉细化模块和跨模式噪声抑制掩模模块。</li>
<li>采用阈值选择机制以增强关联稳健性，过滤掉无关或弱相关的视听对。</li>
<li>在LRS2和LRS3数据集上的实验表明，AD-AVSR在性能和噪声稳健性方面超越现有最佳方法。</li>
<li>总体来说，AD-AVSR模型设计有效。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07608">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9ab4d6b59d9f8c62fc7445891c08ee3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32a7a854f0692a908cbb7a69845c48f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-677d39cabd6aee7ae2f7f63c5fd12313.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b84627a65515bc2445d80c59c2ffad40.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Exploring-Efficient-Directional-and-Distance-Cues-for-Regional-Speech-Separation"><a href="#Exploring-Efficient-Directional-and-Distance-Cues-for-Regional-Speech-Separation" class="headerlink" title="Exploring Efficient Directional and Distance Cues for Regional Speech   Separation"></a>Exploring Efficient Directional and Distance Cues for Regional Speech   Separation</h2><p><strong>Authors:Yiheng Jiang, Haoxu Wang, Yafeng Chen, Gang Qiao, Biao Tian</strong></p>
<p>In this paper, we introduce a neural network-based method for regional speech separation using a microphone array. This approach leverages novel spatial cues to extract the sound source not only from specified direction but also within defined distance. Specifically, our method employs an improved delay-and-sum technique to obtain directional cues, substantially enhancing the signal from the target direction. We further enhance separation by incorporating the direct-to-reverberant ratio into the input features, enabling the model to better discriminate sources within and beyond a specified distance. Experimental results demonstrate that our proposed method leads to substantial gains across multiple objective metrics. Furthermore, our method achieves state-of-the-art performance on the CHiME-8 MMCSG dataset, which was recorded in real-world conversational scenarios, underscoring its effectiveness for speech separation in practical applications. </p>
<blockquote>
<p>在这篇论文中，我们介绍了一种基于神经网络的方法，使用麦克风阵列进行区域语音分离。这种方法利用新型的空间线索来提取声源，不仅可以从指定方向，还可以从定义的范围内提取声音。具体来说，我们的方法采用改进的延迟求和技术来获得方向线索，从而极大地增强目标方向上的信号。我们通过将直达声与混响声比率纳入输入特征，进一步提高声音的分离效果，使模型能够更好地区分指定距离内和之外的声源。实验结果表明，我们提出的方法在多个客观指标上取得了显著的改进。此外，我们的方法在真实对话场景中录制的CHiME-8 MMCSG数据集上取得了最新性能，这凸显了其在语音分离的实用应用中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07563v1">PDF</a> This paper has been accepted by Interspeech 2025</p>
<p><strong>Summary</strong><br>该论文提出了一种基于神经网络的区域语音分离方法，利用麦克风阵列进行声音采集。该方法通过提取空间线索，不仅可以从指定方向提取声源，还可以在特定距离内对声源进行定位。通过改进延时相加技术获得方向线索，提高了目标方向的信号质量。此外，通过将直达声与混响声比例纳入输入特征，提高了模型对内外声源的辨识能力。实验结果表明，该方法在多个客观评价指标上取得了显著的提升，并在真实对话场景的CHiME-8 MMCSG数据集上实现了卓越的性能，展示了其在实用场合语音分离的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文提出了一种基于神经网络的区域语音分离方法，使用麦克风阵列进行声音采集。</li>
<li>方法通过提取空间线索，可以在特定距离内定位声源。</li>
<li>通过改进延时相加技术提高目标方向的信号质量。</li>
<li>模型结合直达声与混响声比例，提高内外声源的辨识能力。</li>
<li>实验结果在多个客观评价指标上表现优异。</li>
<li>在真实对话场景的CHiME-8 MMCSG数据集上实现卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07563">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fa0b57a78942ef1e8818ccd57f9febb2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b9bb6eac89a3512abf71f85e955e510.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75134bbb01a934009f189adf4b8ce420.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1288dd6a988ccc823a8e7a486f1b93ec.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Small-footprint-Acoustic-Echo-Cancellation-Solution-for-Mobile-Full-Duplex-Speech-Interactions"><a href="#A-Small-footprint-Acoustic-Echo-Cancellation-Solution-for-Mobile-Full-Duplex-Speech-Interactions" class="headerlink" title="A Small-footprint Acoustic Echo Cancellation Solution for Mobile   Full-Duplex Speech Interactions"></a>A Small-footprint Acoustic Echo Cancellation Solution for Mobile   Full-Duplex Speech Interactions</h2><p><strong>Authors:Yiheng Jiang, Tian Biao</strong></p>
<p>In full-duplex speech interaction systems, effective Acoustic Echo Cancellation (AEC) is crucial for recovering echo-contaminated speech. This paper presents a neural network-based AEC solution to address challenges in mobile scenarios with varying hardware, nonlinear distortions and long latency. We first incorporate diverse data augmentation strategies to enhance the model’s robustness across various environments. Moreover, progressive learning is employed to incrementally improve AEC effectiveness, resulting in a considerable improvement in speech quality. To further optimize AEC’s downstream applications, we introduce a novel post-processing strategy employing tailored parameters designed specifically for tasks such as Voice Activity Detection (VAD) and Automatic Speech Recognition (ASR), thus enhancing their overall efficacy. Finally, our method employs a small-footprint model with streaming inference, enabling seamless deployment on mobile devices. Empirical results demonstrate effectiveness of the proposed method in Echo Return Loss Enhancement and Perceptual Evaluation of Speech Quality, alongside significant improvements in both VAD and ASR results. </p>
<blockquote>
<p>在双向语音交互系统中，有效的声学回声消除（AEC）对于恢复受回声污染的语音至关重要。本文针对移动场景中的硬件差异、非线性失真和长延迟等挑战，提出了一种基于神经网络的AEC解决方案。我们首先采用多种数据增强策略，提高模型在不同环境下的稳健性。此外，采用渐进学习方法来逐步改进AEC的有效性，从而显著提高语音质量。为了进一步优化AEC的下游应用，我们引入了一种新型后处理策略，采用针对任务设计的专用参数，如语音活动检测（VAD）和自动语音识别（ASR），从而提高其整体效率。最后，我们的方法使用一个小型模型进行流式推理，可在移动设备上无缝部署。经验结果表明，该方法在回声返回损耗增强和语音质量主观评价方面有效，同时显著提高了VAD和ASR的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07561v1">PDF</a> This paper is accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>基于神经网络的全双工语音交互系统中，回声消除至关重要。本文通过多样的数据增强策略来提升模型在各种环境下的稳健性，并利用渐进学习逐步提升回声消除效果。此外，通过采用特定参数的后处理策略优化了下游应用如语音活动检测和语音识别。本文的方法在小模型足印和流式推断的支持下，可在移动设备上无缝部署。经验结果表明，该方法在回声损耗增强和语音质量感知评估上效果显著，并在语音活动和语音识别结果上有显著改善。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>全双工语音交互系统中，声学回声消除（AEC）对于处理回声污染的语音至关重要。</li>
<li>通过多样的数据增强策略提升模型稳健性，适应各种环境。</li>
<li>渐进学习用于逐步提升AEC效果。</li>
<li>引入后处理策略，通过特定参数优化下游应用如语音活动检测（VAD）和语音识别（ASR）。</li>
<li>方法采用小模型足印和流式推断，支持在移动设备上无缝部署。</li>
<li>实证结果显示，该方法在回声损耗增强和语音质量评估上表现优秀。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07561">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-203e46aaecb75deeb5020df807c6b7be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b84cd3c46a28de5c0462c38364baf320.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ae506d17222959c7085e2727ee6e7eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ebd2410892b5eaf368114b49baa1e62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66cb8bae5ac5afca7c20e85cf6bb5e57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36e3c3ecd2e093f6977e8beaa4febc53.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="UniFlow-Unifying-Speech-Front-End-Tasks-via-Continuous-Generative-Modeling"><a href="#UniFlow-Unifying-Speech-Front-End-Tasks-via-Continuous-Generative-Modeling" class="headerlink" title="UniFlow: Unifying Speech Front-End Tasks via Continuous Generative   Modeling"></a>UniFlow: Unifying Speech Front-End Tasks via Continuous Generative   Modeling</h2><p><strong>Authors:Ziqian Wang, Zikai Liu, Yike Zhu, Xingchen Li, Boyi Kang, Jixun Yao, Xianjun Xia, Chuanzeng Huang, Lei Xie</strong></p>
<p>Generative modeling has recently achieved remarkable success across image, video, and audio domains, demonstrating powerful capabilities for unified representation learning. Yet speech front-end tasks such as speech enhancement (SE), target speaker extraction (TSE), acoustic echo cancellation (AEC), and language-queried source separation (LASS) remain largely tackled by disparate, task-specific solutions. This fragmentation leads to redundant engineering effort, inconsistent performance, and limited extensibility. To address this gap, we introduce UniFlow, a unified framework that employs continuous generative modeling to tackle diverse speech front-end tasks in a shared latent space. Specifically, UniFlow utilizes a waveform variational autoencoder (VAE) to learn a compact latent representation of raw audio, coupled with a Diffusion Transformer (DiT) that predicts latent updates. To differentiate the speech processing task during the training, learnable condition embeddings indexed by a task ID are employed to enable maximal parameter sharing while preserving task-specific adaptability. To balance model performance and computational efficiency, we investigate and compare three generative objectives: denoising diffusion, flow matching, and mean flow within the latent domain. We validate UniFlow on multiple public benchmarks, demonstrating consistent gains over state-of-the-art baselines. UniFlow’s unified latent formulation and conditional design make it readily extensible to new tasks, providing an integrated foundation for building and scaling generative speech processing pipelines. To foster future research, we will open-source our codebase. </p>
<blockquote>
<p>生成式建模最近在图像、视频和音频领域取得了显著的成功，展示了统一表示学习的强大能力。然而，语音前端任务，如语音增强（SE）、目标说话人提取（TSE）、回声消除（AEC）和语言查询源分离（LASS），仍主要通过分散的、针对特定任务的解决方案来处理。这种碎片化导致了冗余的工程努力、性能不一致和扩展性有限。为了解决这一差距，我们引入了UniFlow，这是一个统一的框架，采用连续生成式建模，在共享潜在空间内解决多样化的语音前端任务。具体来说，UniFlow利用波形变分自编码器（VAE）学习原始音频的紧凑潜在表示，结合扩散变压器（DiT）来预测潜在更新。为了在训练过程中区分不同的语音处理任务，我们采用由任务ID索引的可学习条件嵌入，以实现最大参数共享，同时保留任务特定适应性。为了平衡模型性能和计算效率，我们研究和比较了三种生成目标：去噪扩散、流匹配和潜在域内的平均流。我们在多个公共基准测试集上验证了UniFlow，相较于最新基线技术，表现出了一致的优势。UniFlow的统一潜在公式和条件设计使其易于扩展到新任务，为构建和扩展生成式语音处理管道提供了集成基础。为了促进未来的研究，我们将开源我们的代码库。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07558v1">PDF</a> extended version</p>
<p><strong>Summary</strong></p>
<p>基于连续生成模型统一处理多种语音前端任务的框架UniFlow。该框架采用波形变分自编码器（VAE）学习原始音频的紧凑潜在表示，并结合扩散变压器（DiT）预测潜在更新。通过引入可学习的条件嵌入和索引任务ID，实现最大参数共享同时保留任务特定适应性。同时，探讨了三种生成目标，包括去噪扩散、流匹配和潜在域内的平均流，以实现模型性能和计算效率的平衡。在多个公共基准测试上的验证显示，UniFlow优于当前先进基线，易于扩展到新任务，为构建和扩展生成性语音处理管道提供了综合基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniFlow是一个利用连续生成模型处理多样化语音前端任务的统一框架。</li>
<li>该框架采用波形变分自编码器（VAE）学习音频的潜在表示，并结合扩散变压器进行预测。</li>
<li>通过引入可学习的条件嵌入和索引任务ID，实现参数共享和任务特定适应性的平衡。</li>
<li>探讨了三种生成目标以优化模型性能和计算效率。</li>
<li>UniFlow在多个公共基准测试上表现优异，易于扩展到新任务。</li>
<li>UniFlow为构建和扩展生成性语音处理管道提供了基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07558">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-40e47e4b0816a8e34969773e09ad5d79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edc1c8edc67169ff4840198174a53948.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a3381a9ecbba54c4de0f52f20cc5bf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dafd82758db69f8164530440d8c0456.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Scalable-Controllable-Accented-TTS"><a href="#Scalable-Controllable-Accented-TTS" class="headerlink" title="Scalable Controllable Accented TTS"></a>Scalable Controllable Accented TTS</h2><p><strong>Authors:Henry Li Xinyuan, Zexin Cai, Ashi Garg, Kevin Duh, Leibny Paola García-Perera, Sanjeev Khudanpur, Nicholas Andrews, Matthew Wiesner</strong></p>
<p>We tackle the challenge of scaling accented TTS systems, expanding their capabilities to include much larger amounts of training data and a wider variety of accent labels, even for accents that are poorly represented or unlabeled in traditional TTS datasets. To achieve this, we employ two strategies: 1. Accent label discovery via a speech geolocation model, which automatically infers accent labels from raw speech data without relying solely on human annotation; 2. Timbre augmentation through kNN voice conversion to increase data diversity and model robustness. These strategies are validated on CommonVoice, where we fine-tune XTTS-v2 for accented TTS with accent labels discovered or enhanced using geolocation. We demonstrate that the resulting accented TTS model not only outperforms XTTS-v2 fine-tuned on self-reported accent labels in CommonVoice, but also existing accented TTS benchmarks. </p>
<blockquote>
<p>我们面对了有口音的TTS系统所面临的扩展挑战，需要扩展它们的能力，以包含更大规模的训练数据和更多种类的口音标签，即使对于在传统TTS数据集中表示不佳或未标记的口音也是如此。为了实现这一点，我们采用了两种策略：1. 通过语音定位模型发现口音标签，该模型能够自动从原始语音数据中推断口音标签，而无需完全依赖于人工注释；2. 通过kNN语音转换增强音色，以增加数据多样性和模型稳健性。这些策略在CommonVoice上得到了验证，我们对XTTS-v2进行微调，以处理带有通过定位发现或增强的口音标签的口音TTS。我们证明，所得的带口音的TTS模型不仅优于在CommonVoice中自我报告的口音标签上调优的XTTS-v2，而且还优于现有的带口音TTS基准测试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07426v1">PDF</a> Accepted at IEEE ASRU 2025</p>
<p><strong>总结</strong></p>
<p>本文介绍了如何通过采用两种策略来解决扩展带口音的文本转语音（TTS）系统的挑战。这两种策略分别是：一、利用语音地理位置模型进行口音标签发现，能够自动从原始语音数据中推断口音标签，无需完全依赖人工标注；二、通过kNN语音转换进行音色增强，以增加数据多样性和模型稳健性。文章在CommonVoice数据集上验证了这两种策略的有效性，使用地理位置发现的口音标签微调XTTS-v2模型后生成的带口音的TTS模型性能得到提升，不仅超越了使用CommonVoice自我报告的口音标签微调XTTS-v2模型的表现，而且相比现有的带口音的TTS基准测试也表现更出色。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>语音地理位置模型用于口音标签发现：能够自动从原始语音数据中推断口音标签，降低了对人工标注的依赖。</li>
<li>采用kNN语音转换进行音色增强：增加数据多样性和模型稳健性。</li>
<li>在CommonVoice数据集上验证了这两种策略的有效性。</li>
<li>提出的带口音的TTS模型表现超越了使用CommonVoice自我报告的口音标签微调XTTS-v2模型的表现。</li>
<li>与现有的带口音的TTS基准测试相比，该模型的性能更优越。</li>
<li>该方法对于扩展TTS系统的能力，包括处理更大量的训练数据和更广泛的口音标签具有重要意义。</li>
<li>此策略对于在传统TTS数据集中表示不足或无标签的口音特别重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07426">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e95582732cf5c505460d492ab171f4bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b432ad7e8ad037b6870ef5957d6dbffa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a66073a4b3bffcb248261761304cf33a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-95a345e34f226104adbee9bd1ede6af7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2515349aa313ad8988e094f1c3e1db5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c2f25bc571509a1578e250ff75b55be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad0f63ad03fc150281bd5ef3512fa8b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eef2e09c62254e2041da4f698c45a27a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Think-Before-You-Talk-Enhancing-Meaningful-Dialogue-Generation-in-Full-Duplex-Speech-Language-Models-with-Planning-Inspired-Text-Guidance"><a href="#Think-Before-You-Talk-Enhancing-Meaningful-Dialogue-Generation-in-Full-Duplex-Speech-Language-Models-with-Planning-Inspired-Text-Guidance" class="headerlink" title="Think Before You Talk: Enhancing Meaningful Dialogue Generation in   Full-Duplex Speech Language Models with Planning-Inspired Text Guidance"></a>Think Before You Talk: Enhancing Meaningful Dialogue Generation in   Full-Duplex Speech Language Models with Planning-Inspired Text Guidance</h2><p><strong>Authors:Wenqian Cui, Lei Zhu, Xiaohui Li, Zhihan Guo, Haoli Bai, Lu Hou, Irwin King</strong></p>
<p>Full-Duplex Speech Language Models (FD-SLMs) are specialized foundation models designed to enable natural, real-time spoken interactions by modeling complex conversational dynamics such as interruptions, backchannels, and overlapping speech, and End-to-end (e2e) FD-SLMs leverage real-world double-channel conversational data to capture nuanced two-speaker dialogue patterns for human-like interactions. However, they face a critical challenge – their conversational abilities often degrade compared to pure-text conversation due to prolonged speech sequences and limited high-quality spoken dialogue data. While text-guided speech generation could mitigate these issues, it suffers from timing and length issues when integrating textual guidance into double-channel audio streams, disrupting the precise time alignment essential for natural interactions. To address these challenges, we propose TurnGuide, a novel planning-inspired approach that mimics human conversational planning by dynamically segmenting assistant speech into dialogue turns and generating turn-level text guidance before speech output, which effectively resolves both insertion timing and length challenges. Extensive experiments demonstrate our approach significantly improves e2e FD-SLMs’ conversational abilities, enabling them to generate semantically meaningful and coherent speech while maintaining natural conversational flow. Demos are available at <a target="_blank" rel="noopener" href="https://dreamtheater123.github.io/TurnGuide-Demo/">https://dreamtheater123.github.io/TurnGuide-Demo/</a>. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/dreamtheater123/TurnGuide">https://github.com/dreamtheater123/TurnGuide</a>. </p>
<blockquote>
<p>全双工语音语言模型（FD-SLMs）是专门设计的基础模型，旨在通过模拟复杂对话动态（如中断、反馈通道和重叠语音）来实现自然、实时的口语交互。端到端（e2e）FD-SLMs则利用现实世界的双通道对话数据，捕捉微妙的两语者对话模式，用于人像交互。然而，它们面临一个关键挑战——由于长语音序列和有限的高质量口语对话数据，它们的对话能力通常与纯文本对话相比有所降低。虽然文本引导的语音生成可以缓解这些问题，但在将文本引导集成到双通道音频流中时，它存在时间和长度问题，破坏了自然交互所必需的时间精确对齐。为了解决这些挑战，我们提出了TurnGuide，这是一种受规划启发的新方法，通过动态地将助理语音分割成对话回合并生成回合级文本指导来模仿人类对话规划，在语音输出之前有效地解决了插入时间和长度挑战。大量实验表明，我们的方法显著提高了端到端FD-SLMs的对话能力，使它们能够生成语义上连贯的语音并保持自然的对话流程。演示网站为：<a target="_blank" rel="noopener" href="https://dreamtheater123.github.io/TurnGuide-Demo/%E3%80%82%E4%BB%A3%E7%A0%81%E5%B0%86%E5%8F%91%E5%B8%AE%E5%9C%A8">https://dreamtheater123.github.io/TurnGuide-Demo/。代码将发布在</a>：<a target="_blank" rel="noopener" href="https://github.com/dreamtheater123/TurnGuide%E3%80%82">https://github.com/dreamtheater123/TurnGuide。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07375v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>本文介绍了全双工语音语言模型（FD-SLMs）面临的挑战，即对话能力在长时间语音序列和高质量语音对话数据有限的情况下会退化。为解决这一问题，提出了TurnGuide，一种模仿人类对话规划的方法，通过动态分割助理语音生成对话回合级别的文本指导，有效解决了插入时机和长度挑战。实验证明，该方法能显著提高端到端FD-SLMs的对话能力，使其在生成语义上有意义和连贯的语音的同时，保持自然的对话流程。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FD-SLMs旨在实现自然、实时的口语交互，建模复杂的对话动态，如中断、反馈和重叠语音。</li>
<li>端到端FD-SLMs利用真实世界的双通道对话数据来捕捉微妙的两人对话模式，实现人性化的交互。</li>
<li>FD-SLMs面临的关键挑战是：由于长时间的语音序列和高质量对话数据的限制，其对话能力退化。</li>
<li>TurnGuide是一种模仿人类对话规划的方法，通过动态分割助理语音生成对话回合级别的文本指导来解决FD-SLMs的挑战。</li>
<li>TurnGuide解决了插入时机和长度的问题，确保语音生成的精确时间对齐，保持自然的对话流程。</li>
<li>实验证明，TurnGuide能显著提高端到端FD-SLMs的对话能力，生成语义上有意义和连贯的语音。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07375">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-683047e4b651aa6518b122cce617caa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cd7ea01daabaa183cace25071d45b54.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d63694d3ea90d97df0ec7a4f744e3d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5db8877468133a18b48fde71885ea1c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-411537da04440d4423e000421f7e47b2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bc0b62c024e56c5f3a15fc975b549cf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4da7a11a6a5e82fd0ee18989f5b6158.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="XEmoRAG-Cross-Lingual-Emotion-Transfer-with-Controllable-Intensity-Using-Retrieval-Augmented-Generation"><a href="#XEmoRAG-Cross-Lingual-Emotion-Transfer-with-Controllable-Intensity-Using-Retrieval-Augmented-Generation" class="headerlink" title="XEmoRAG: Cross-Lingual Emotion Transfer with Controllable Intensity   Using Retrieval-Augmented Generation"></a>XEmoRAG: Cross-Lingual Emotion Transfer with Controllable Intensity   Using Retrieval-Augmented Generation</h2><p><strong>Authors:Tianlun Zuo, Jingbin Hu, Yuke Li, Xinfa Zhu, Hai Li, Ying Yan, Junhui Liu, Danming Xie, Lei Xie</strong></p>
<p>Zero-shot emotion transfer in cross-lingual speech synthesis refers to generating speech in a target language, where the emotion is expressed based on reference speech from a different source language.However, this task remains challenging due to the scarcity of parallel multilingual emotional corpora, the presence of foreign accent artifacts, and the difficulty of separating emotion from language-specific prosodic features.In this paper, we propose XEmoRAG, a novel framework to enable zero-shot emotion transfer from Chinese to Thai using a large language model (LLM)-based model, without relying on parallel emotional data.XEmoRAG extracts language-agnostic emotional embeddings from Chinese speech and retrieves emotionally matched Thai utterances from a curated emotional database, enabling controllable emotion transfer without explicit emotion labels. Additionally, a flow-matching alignment module minimizes pitch and duration mismatches, ensuring natural prosody. It also blends Chinese timbre into the Thai synthesis, enhancing rhythmic accuracy and emotional expression, while preserving speaker characteristics and emotional consistency.Experimental results show that XEmoRAG synthesizes expressive and natural Thai speech using only Chinese reference audio, without requiring explicit emotion labels.These results highlight XEmoRAG’s capability to achieve flexible and low-resource emotional transfer across languages.Our demo is available at <a target="_blank" rel="noopener" href="https://tlzuo-lesley.github.io/Demo-page/">https://tlzuo-lesley.github.io/Demo-page/</a>. </p>
<blockquote>
<p>跨语言语音合成中的零样本情感迁移是指生成目标语言中的语音，情感的表达是基于源语言的参考语音。然而，由于缺乏平行多语言情感语料库、存在外来口音的伪迹以及从特定语言的韵律特征中分离情感的困难，这一任务仍然具有挑战性。在本文中，我们提出了XEmoRAG，这是一个无需依赖平行情感数据，能够实现从中文到泰语零样本情感迁移的新型框架，该框架基于大型语言模型（LLM）。XEmoRAG从中文语音中提取与语言无关的情感嵌入，并从精选的情感数据库中检索情感匹配的泰语话语，从而实现可控的情感迁移，无需明确的情感标签。此外，通过流匹配对齐模块最小化音高和持续时间的不匹配，确保自然的韵律。它还融合了中文音色到泰语合成中，提高了节奏准确性和情感表达，同时保持了说话人的特点和情感一致性。实验结果表明，XEmoRAG仅使用中文参考音频就能合成出富有表现力和自然感的泰语语音，无需明确的情感标签。这些结果突出了XEmoRAG在不同语言之间实现灵活和低资源情感迁移的能力。我们的演示可在<a target="_blank" rel="noopener" href="https://tlzuo-lesley.github.io/Demo-page/%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://tlzuo-lesley.github.io/Demo-page/上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07302v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本论文提出了一种零样本情感转移框架XEmoRAG，可以实现从中文到泰语的跨语言语音合成中的情感转移。该框架基于大型语言模型，无需平行情感数据，通过提取语言无关的情感嵌入和检索情感匹配的泰语语音片段来实现可控的情感转移。同时，框架中的流匹配对齐模块可以最小化音高和持续时间的不匹配，确保自然的韵律。实验结果表明，XEmoRAG能够仅使用中文参考音频合成出表达力强、自然的泰语语音。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>零样本情感转移在跨语言语音合成中是一项挑战，缺乏平行多语言情感语料库、存在外语口音问题以及从特定语言的韵律特征中分离情感难度大。</li>
<li>本论文提出了XEmoRAG框架，实现了从中文到泰语的零样本情感转移，基于大型语言模型，无需平行情感数据。</li>
<li>XEmoRAG通过提取语言无关的情感嵌入和检索情感匹配的泰语语音片段，实现了可控的情感转移。</li>
<li>流匹配对齐模块最小化音高和持续时间的不匹配，确保自然的韵律。</li>
<li>XEmoRAG将中文音色融入泰语合成中，提高了节奏准确性和情感表达，同时保持说话人的特征和情感一致性。</li>
<li>实验结果表明XEmoRAG能够仅使用中文参考音频合成出表达力强、自然的泰语语音。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07302">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-781321a3963a3d0b9784536f487c31cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0daff41d1070cfcf1d5a5b90f4586a40.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-16a0cdefcd1982fc48151026be6bed18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98c446b879264ffffdd0598c0b4ee0ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3ec16fb34760d159088fc5013024250.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4213671b3f8811c1be015ccc2c022048.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Lessons-Learnt-Revisit-Key-Training-Strategies-for-Effective-Speech-Emotion-Recognition-in-the-Wild"><a href="#Lessons-Learnt-Revisit-Key-Training-Strategies-for-Effective-Speech-Emotion-Recognition-in-the-Wild" class="headerlink" title="Lessons Learnt: Revisit Key Training Strategies for Effective Speech   Emotion Recognition in the Wild"></a>Lessons Learnt: Revisit Key Training Strategies for Effective Speech   Emotion Recognition in the Wild</h2><p><strong>Authors:Jing-Tong Tzeng, Bo-Hao Su, Ya-Tse Wu, Hsing-Hang Chou, Chi-Chun Lee</strong></p>
<p>In this study, we revisit key training strategies in machine learning often overlooked in favor of deeper architectures. Specifically, we explore balancing strategies, activation functions, and fine-tuning techniques to enhance speech emotion recognition (SER) in naturalistic conditions. Our findings show that simple modifications improve generalization with minimal architectural changes. Our multi-modal fusion model, integrating these optimizations, achieves a valence CCC of 0.6953, the best valence score in Task 2: Emotional Attribute Regression. Notably, fine-tuning RoBERTa and WavLM separately in a single-modality setting, followed by feature fusion without training the backbone extractor, yields the highest valence performance. Additionally, focal loss and activation functions significantly enhance performance without increasing complexity. These results suggest that refining core components, rather than deepening models, leads to more robust SER in-the-wild. </p>
<blockquote>
<p>在这项研究中，我们重新审视了机器学习中的关键训练策略，这些策略通常会被更深的架构所忽视。具体来说，我们探索了平衡策略、激活函数和微调技术，以提高自然条件下的语音情感识别（SER）。我们的研究结果表明，简单的修改可以在极小的架构变化下提高泛化能力。我们融合了多种优化方式的多模态融合模型在任务2：情感属性回归中取得了0.6953的效价CCC值，成为最佳效价得分。值得注意的是，在单模态设置下分别对RoBERTa和WavLM进行微调，然后进行特征融合而不训练骨干提取器，可以获得最高的效价性能。此外，焦点损失和激活函数在不增加复杂性的情况下显著提高了性能。这些结果表明，改进核心组件而不是深化模型，能在野生环境下实现更稳健的SER。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07282v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong><br>     本研究重新审视机器学习中的关键训练策略，包括平衡策略、激活函数和微调技术，以提升自然条件下的语音情感识别（SER）性能。研究结果表明，简单的修改可以大大提高泛化能力，且无需进行大规模架构更改。多模态融合模型在任务2：情感属性回归中取得了最佳效价分数0.6953。特别是，在单模态设置中分别微调RoBERTa和WavLM，然后进行特征融合，而无需训练骨干提取器，可获得最佳的效价性能。此外，焦点损失和激活函数显著提高性能且不增加复杂性。这些结果提示我们，优化核心组件而非深化模型能更稳健地在野外实现SER。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究探索了平衡策略、激活函数和微调技术对语音情感识别的关键训练策略进行了重新审视。</li>
<li>简单修改可以在不改变架构的基础上提高模型的泛化能力。</li>
<li>多模态融合模型在情感属性回归任务中取得了最佳效价分数。</li>
<li>单独微调RoBERTa和WavLM，然后进行特征融合，可以获得最佳的效价性能。</li>
<li>焦点损失和激活函数的应用显著提高了模型的性能。</li>
<li>研究指出优化核心组件而不是深化模型是实现稳健语音情感识别的关键。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07282">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-97af6d8fa63eea419ccf7d8f4026ba12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86e564dad93eb7b1138d7a8641febf43.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a927b730c9a35b7019ff790fe690516d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70228e842e6e19fc866c6abc4e4957b3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-91b470f4c53377680b3ff845bb31c153.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b1b3cfecfb0cefa49c8825dd18834d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67cd18716a8f1ff65079856276f577c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be8ff450033c920c7bf8197a907aa424.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Whisfusion-Parallel-ASR-Decoding-via-a-Diffusion-Transformer"><a href="#Whisfusion-Parallel-ASR-Decoding-via-a-Diffusion-Transformer" class="headerlink" title="Whisfusion: Parallel ASR Decoding via a Diffusion Transformer"></a>Whisfusion: Parallel ASR Decoding via a Diffusion Transformer</h2><p><strong>Authors:Taeyoun Kwon, Junhyuk Ahn, Taegeun Yun, Heeju Jwa, Yoonchae Choi, Siwon Park, Nam-Joon Kim, Jangchan Kim, Hyun Gon Ryu, Hyuk-Jae Lee</strong></p>
<p>Fast Automatic Speech Recognition (ASR) is critical for latency-sensitive applications such as real-time captioning and meeting transcription. However, truly parallel ASR decoding remains challenging due to the sequential nature of autoregressive (AR) decoders and the context limitations of non-autoregressive (NAR) methods. While modern ASR encoders can process up to 30 seconds of audio at once, AR decoders still generate tokens sequentially, creating a latency bottleneck. We propose Whisfusion, the first framework to fuse a pre-trained Whisper encoder with a text diffusion decoder. This NAR architecture resolves the AR latency bottleneck by processing the entire acoustic context in parallel at every decoding step. A lightweight cross-attention adapter trained via parameter-efficient fine-tuning (PEFT) bridges the two modalities. We also introduce a batch-parallel, multi-step decoding strategy that improves accuracy by increasing the number of candidates with minimal impact on speed. Fine-tuned solely on LibriSpeech (960h), Whisfusion achieves a lower WER than Whisper-tiny (8.3% vs. 9.7%), and offers comparable latency on short audio. For longer utterances (&gt;20s), it is up to 2.6x faster than the AR baseline, establishing a new, efficient operating point for long-form ASR. The implementation and training scripts are available at <a target="_blank" rel="noopener" href="https://github.com/taeyoun811/Whisfusion">https://github.com/taeyoun811/Whisfusion</a>. </p>
<blockquote>
<p>快速自动语音识别（ASR）对于延迟敏感的应用（例如实时字幕和会议转录）至关重要。然而，由于自回归（AR）解码器的序列特性和非自回归（NAR）方法的上下文限制，真正的并行ASR解码仍然具有挑战性。尽管现代ASR编码器可以一次处理长达30秒的音频，但AR解码器仍然按顺序生成令牌，造成延迟瓶颈。我们提出了Whisfusion，这是第一个将预训练的Whisper编码器与文本扩散解码器融合的框架。这种NAR架构通过在每个解码步骤中并行处理整个声学上下文来解决AR延迟瓶颈问题。通过参数高效微调（PEFT）训练的轻量级跨注意适配器填补了两种模式的空白。我们还引入了一种批并行多步解码策略，通过增加候选人数量的方式提高准确性，对速度的影响微乎其微。仅在LibriSpeech（960小时）上进行微调，Whisfusion的单词错误率低于Whisper-tiny（8.3％对比9.7％），并且在短音频上提供相当的延迟。对于较长的语音片段（&gt;20秒），它比AR基准测试快达2.6倍，为长形式ASR建立了新的高效操作点。实现和训练脚本可在<a target="_blank" rel="noopener" href="https://github.com/taeyoun811/Whisfusion%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/taeyoun811/Whisfusion找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07048v1">PDF</a> 16 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对自动语音识别（ASR）中的延迟问题，提出了一种新的解决方案——Whisfusion框架。该框架融合了预训练的Whisper编码器与文本扩散解码器，采用非自回归（NAR）架构，以并行处理整个声学上下文，解决自回归（AR）解码器的序列性质及上下文限制导致的延迟瓶颈。通过参数高效微调（PEFT）的跨注意力适配器，实现了两种模态之间的桥梁。同时，引入批量并行多步解码策略，通过增加候选数量来提高准确性，同时几乎不影响速度。在LibriSpeech上仅进行微调，Whisfusion的单词错误率低于Whisper-tiny，对短音频的延迟与Whisper相当，对长音频的处理速度则是自回归方法的2.6倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Whisfusion框架融合了预训练的Whisper编码器与文本扩散解码器，解决了ASR中的延迟问题。</li>
<li>采用非自回归（NAR）架构，并行处理整个声学上下文，突破了自回归（AR）解码器的延迟瓶颈。</li>
<li>跨注意力适配器通过参数高效微调（PEFT）实现模态间的桥梁。</li>
<li>引入批量并行多步解码策略，提高准确性同时几乎不影响速度。</li>
<li>Whisfusion在LibriSpeech上的单词错误率低于Whisper-tiny。</li>
<li>对于短音频，Whisfusion的延迟与Whisper相当。</li>
<li>对于长音频处理，Whisfusion的处理速度是自回归方法的2.6倍。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07048">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-98968fc4e920dcec0faa6bedb672724a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b930979621d123d773aa5cffaf7b88f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbfb3eb78c4a11755acc4168a1d3b119.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-319f66d19be0159c15ccf29fadf364de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc083a9925dcdf59e34e53b6e89068cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-944e26a07e755d08dc3049b6a903ee53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9cd9881e9b2eed15ab57e26d2f24adc.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FlowSE-Flow-Matching-based-Speech-Enhancement"><a href="#FlowSE-Flow-Matching-based-Speech-Enhancement" class="headerlink" title="FlowSE: Flow Matching-based Speech Enhancement"></a>FlowSE: Flow Matching-based Speech Enhancement</h2><p><strong>Authors:Seonggyu Lee, Sein Cheong, Sangwook Han, Jong Won Shin</strong></p>
<p>Diffusion probabilistic models have shown impressive performance for speech enhancement, but they typically require 25 to 60 function evaluations in the inference phase, resulting in heavy computational complexity. Recently, a fine-tuning method was proposed to correct the reverse process, which significantly lowered the number of function evaluations (NFE). Flow matching is a method to train continuous normalizing flows which model probability paths from known distributions to unknown distributions including those described by diffusion processes. In this paper, we propose a speech enhancement based on conditional flow matching. The proposed method achieved the performance comparable to those for the diffusion-based speech enhancement with the NFE of 60 when the NFE was 5, and showed similar performance with the diffusion model correcting the reverse process at the same NFE from 1 to 5 without additional fine tuning procedure. We also have shown that the corresponding diffusion model derived from the conditional probability path with a modified optimal transport conditional vector field demonstrated similar performances with the NFE of 5 without any fine-tuning procedure. </p>
<blockquote>
<p>扩散概率模型在语音增强方面表现出令人印象深刻的性能，但它们通常在推理阶段需要25到60次函数评估，导致计算复杂度较高。最近，提出了一种微调方法来纠正反向过程，这大大降低了函数评估次数（NFE）。流匹配是一种训练连续归一化流的方法，该流从已知分布到未知分布建模概率路径，包括由扩散过程描述的那些分布。在本文中，我们提出了一种基于条件流匹配的语音增强方法。当NFE为5时，所提出的方法实现了与基于扩散的语音增强相当的性能（后者在NFE为60时表现），并且在相同的NFE从1到5范围内，无需额外的微调程序，其性能与纠正反向过程的扩散模型相似。我们还表明，对应于由带有修改后的最优传输条件向量场的条件概率路径派生的扩散模型，在无需任何微调程序的情况下，在NFE为5时表现出相似的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06840v1">PDF</a> Published in ICASSP 2025</p>
<p><strong>Summary</strong><br>     基于条件流匹配的语音增强方法被提出，该方法通过训练连续归一化流来建模从已知分布到未知分布的概率路径，包括由扩散过程描述的分布。该方法实现了与基于扩散的语音增强方法相当的性能，同时在函数评估次数（NFE）大幅降低的情况下表现出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散概率模型在语音增强方面表现出卓越性能，但推理阶段需要25至60次函数评估，计算复杂度较高。</li>
<li>最近提出的微调方法用于纠正反向过程，显著降低函数评估次数（NFE）。</li>
<li>流匹配是一种训练连续归一化流的方法，用于建模从已知分布到未知分布的概率路径，包括由扩散过程描述的分布。</li>
<li>本文提出了一种基于条件流匹配的语音增强方法，实现了与扩散模型相当的性能。</li>
<li>当NFE为5时，该方法达到了与NFE为60的扩散模型相当的性能。</li>
<li>在相同的NFE范围内（从1到5），该方法展示了无需额外微调程序即可与纠正反向过程的扩散模型相匹敌的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06840">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-265913952568d740adbd84e34c6b6974.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7a56d7e14f5c2f8c2dc7ef04d82f4c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c847d7c4d28401d5ae5d66176d3a7f64.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Learning-Phonetic-Context-Dependent-Viseme-for-Enhancing-Speech-Driven-3D-Facial-Animation"><a href="#Learning-Phonetic-Context-Dependent-Viseme-for-Enhancing-Speech-Driven-3D-Facial-Animation" class="headerlink" title="Learning Phonetic Context-Dependent Viseme for Enhancing Speech-Driven   3D Facial Animation"></a>Learning Phonetic Context-Dependent Viseme for Enhancing Speech-Driven   3D Facial Animation</h2><p><strong>Authors:Hyung Kyu Kim, Hak Gu Kim</strong></p>
<p>Speech-driven 3D facial animation aims to generate realistic facial movements synchronized with audio. Traditional methods primarily minimize reconstruction loss by aligning each frame with ground-truth. However, this frame-wise approach often fails to capture the continuity of facial motion, leading to jittery and unnatural outputs due to coarticulation. To address this, we propose a novel phonetic context-aware loss, which explicitly models the influence of phonetic context on viseme transitions. By incorporating a viseme coarticulation weight, we assign adaptive importance to facial movements based on their dynamic changes over time, ensuring smoother and perceptually consistent animations. Extensive experiments demonstrate that replacing the conventional reconstruction loss with ours improves both quantitative metrics and visual quality. It highlights the importance of explicitly modeling phonetic context-dependent visemes in synthesizing natural speech-driven 3D facial animation. Project page: <a target="_blank" rel="noopener" href="https://cau-irislab.github.io/interspeech25/">https://cau-irislab.github.io/interspeech25/</a> </p>
<blockquote>
<p>语音驱动的3D面部动画旨在生成与音频同步的真实面部运动。传统方法主要通过将每一帧与地面真实数据进行比对来最小化重建损失。然而，这种基于帧的方法通常无法捕捉到面部运动的连续性，由于协同发音导致输出产生抖动和不自然的现象。为了解决这一问题，我们提出了一种新型语音上下文感知损失，该损失能够明确建模语音上下文对语音音素过渡的影响。通过引入语音协同发音权重，我们根据面部运动随时间的变化为其分配自适应重要性，确保动画更加流畅且视觉感知一致。大量实验表明，用我们的方法替换传统重建损失能提高定量指标和视觉质量。它强调了明确建模语音上下文相关音素在合成自然语音驱动的3D面部动画中的重要性。项目页面：<a target="_blank" rel="noopener" href="https://cau-irislab.github.io/interspeech25/">https://cau-irislab.github.io/interspeech25/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20568v2">PDF</a> Interspeech 2025; Project Page:   <a target="_blank" rel="noopener" href="https://cau-irislab.github.io/interspeech25/">https://cau-irislab.github.io/interspeech25/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于语音驱动的3D面部动画新方法，旨在生成与音频同步的真实面部运动。为解决传统方法因忽略语音上下文导致的面部运动不连续问题，本文提出一种新颖的语音上下文感知损失函数，通过建模语音上下文对颜面肌肉动作的影响，实现更平滑、感知一致的动画效果。实验证明，该方法在定量指标和视觉质量上均有所提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音驱动的3D面部动画旨在生成与音频同步的真实面部运动。</li>
<li>传统方法主要通过最小化重建损失来实现帧与地面真实的对齐，但这种方法忽略了面部运动的连续性。</li>
<li>提出一种新颖的语音上下文感知损失函数，建模语音上下文对颜面肌肉动作的影响。</li>
<li>通过引入颜面肌肉动作协同发音权重，对面部运动进行自适应重要性分配，确保动画更平滑、感知一致。</li>
<li>实验证明，该方法在定量指标和视觉质量上均有所提升。</li>
<li>该方法强调了明确建模语音上下文相关的颜面肌肉动作在合成自然语音驱动3D面部动画中的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20568">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fd4c29224c7da2ad9239e3b5dafc0c46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d834e69b32ee4cd770df812a61025d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ff5507140b2cd6e367d151d3a6b2f60.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="TalkLess-Blending-Extractive-and-Abstractive-Speech-Summarization-for-Editing-Speech-to-Preserve-Content-and-Style"><a href="#TalkLess-Blending-Extractive-and-Abstractive-Speech-Summarization-for-Editing-Speech-to-Preserve-Content-and-Style" class="headerlink" title="TalkLess: Blending Extractive and Abstractive Speech Summarization for   Editing Speech to Preserve Content and Style"></a>TalkLess: Blending Extractive and Abstractive Speech Summarization for   Editing Speech to Preserve Content and Style</h2><p><strong>Authors:Karim Benharrak, Puyuan Peng, Amy Pavel</strong></p>
<p>Millions of people listen to podcasts, audio stories, and lectures, but editing speech remains tedious and time-consuming. Creators remove unnecessary words, cut tangential discussions, and even re-record speech to make recordings concise and engaging. Prior work automatically summarized speech by removing full sentences (extraction), but rigid extraction limits expressivity. AI tools can summarize then re-synthesize speech (abstraction), but abstraction strips the speaker’s style. We present TalkLess, a system that flexibly combines extraction and abstraction to condense speech while preserving its content and style. To edit speech, TalkLess first generates possible transcript edits, selects edits to maximize compression, coverage, and audio quality, then uses a speech editing model to translate transcript edits into audio edits. TalkLess’s interface provides creators control over automated edits by separating low-level wording edits (via the compression pane) from major content edits (via the outline pane). TalkLess achieves higher coverage and removes more speech errors than a state-of-the-art extractive approach. A comparison study (N&#x3D;12) showed that TalkLess significantly decreased cognitive load and editing effort in speech editing. We further demonstrate TalkLess’s potential in an exploratory study (N&#x3D;3) where creators edited their own speech. </p>
<blockquote>
<p>成千上万的人聆听播客、音频故事和讲座，但编辑语音仍然繁琐耗时。创作者会删除多余的词汇、剪辑离题的内容，甚至重新录制语音，以使录音更加简洁、引人入胜。之前的工作通过移除整个句子（提取）来自动总结语音，但僵化的提取方式限制了表达力。AI工具可以总结然后重新合成语音（抽象），但抽象会剥夺说话者的风格。我们推出了TalkLess系统，它灵活地结合了提取和抽象，浓缩语音的同时保留其内容风格。为了编辑语音，TalkLess首先生成可能的转录编辑，选择编辑以最大化压缩、覆盖范围和音频质量，然后使用语音编辑模型将转录编辑转化为音频编辑。TalkLess的界面通过区分低级别的措辞编辑（通过压缩面板）和主要的内容编辑（通过大纲面板），为创作者提供了对自动编辑的控制权。TalkLess的覆盖范围更广，去除的语音错误比最先进的提取方法更多。一项比较研究（N&#x3D;12）显示，TalkLess显著降低了语音编辑的认知负荷和努力程度。在探索性研究（N&#x3D;3）中，我们进一步展示了创作者使用TalkLess编辑自己的语音的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15202v2">PDF</a> Accepted to The 38th Annual ACM Symposium on User Interface Software   and Technology (UIST ‘25), September 28-October 1, 2025, Busan, Republic of   Korea. 19 pages</p>
<p><strong>Summary</strong></p>
<p>谈到一个名为TalkLess的系统，它将语音识别技术与人工智能技术结合，实现对演讲内容的精简和风格保留。该系统可以生成可能的转录编辑，选择能最大化压缩、覆盖和音频质量的编辑，并将转录编辑转化为音频编辑。TalkLess界面提供创作者对自动编辑的控制，分为低级别的措辞编辑（通过压缩面板）和主要的内容编辑（通过大纲面板）。与现有技术相比，TalkLess在覆盖率和去除语音错误方面表现更佳，同时能显著降低认知负荷和编辑工作量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TalkLess系统结合了语音识别与人工智能技术，旨在编辑语音内容。</li>
<li>它能够生成可能的转录编辑，并智能选择最大化压缩、覆盖和音频质量的编辑。</li>
<li>TalkLess结合了提取和抽象技术，能在保持内容的同时调整语音。</li>
<li>系统界面提供两种编辑模式：针对措辞的低级别编辑和针对主要内容的编辑。</li>
<li>与现有技术相比，TalkLess在覆盖率和消除语音错误方面具有优势。</li>
<li>TalkLess能显著降低认知负荷和编辑工作量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15202">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d52664c28596825a99f8434607d2bfb1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ede19c87d450f35c836c702144d4becb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cb536c9040977dd2f2f618ccc8c00d2.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Enhancing-Target-Speaker-Extraction-with-Explicit-Speaker-Consistency-Modeling"><a href="#Enhancing-Target-Speaker-Extraction-with-Explicit-Speaker-Consistency-Modeling" class="headerlink" title="Enhancing Target Speaker Extraction with Explicit Speaker Consistency   Modeling"></a>Enhancing Target Speaker Extraction with Explicit Speaker Consistency   Modeling</h2><p><strong>Authors:Shu Wu, Anbin Qi, Yanzhang Xie, Xiang Xie</strong></p>
<p>Target Speaker Extraction (TSE) uses a reference cue to extract the target speech from a mixture. In TSE systems relying on audio cues, the speaker embedding from the enrolled speech is crucial to performance. However, these embeddings may suffer from speaker identity confusion. Unlike previous studies that focus on improving speaker embedding extraction, we improve TSE performance from the perspective of speaker consistency. In this paper, we propose a speaker consistency-aware target speaker extraction method that incorporates a centroid-based speaker consistency loss. This approach enhances TSE performance by ensuring speaker consistency between the enrolled and extracted speech. In addition, we integrate conditional loss suppression into the training process. The experimental results validate the effectiveness of our proposed methods in advancing the TSE performance. A speech demo is available online:<a target="_blank" rel="noopener" href="https://sc-tse.netlify.app/">https://sc-tse.netlify.app/</a> </p>
<blockquote>
<p>目标说话人提取（TSE）使用参考线索从混合语音中提取目标语音。在依赖音频线索的TSE系统中，注册语音的说话人嵌入对性能至关重要。然而，这些嵌入可能会受到说话人身份混淆的影响。不同于之前专注于提高说话人嵌入提取的研究，我们从说话人一致性的角度提高TSE的性能。在本文中，我们提出了一种基于质心的说话人一致性感知目标说话人提取方法，该方法引入了基于质心的说话人一致性损失。这种方法通过确保注册语音和提取语音之间的说话人一致性，提高了TSE的性能。此外，我们将条件损失抑制融入训练过程。实验结果验证了我们提出的方法在提高TSE性能方面的有效性。在线演示链接：<a target="_blank" rel="noopener" href="https://sc-tse.netlify.app/">https://sc-tse.netlify.app/</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09510v3">PDF</a> preprint</p>
<p><strong>Summary</strong>：目标说话人提取（TSE）利用参考线索从混合语音中提取目标语音。在依赖音频线索的TSE系统中，注册语音的说话人嵌入对性能至关重要，但这些嵌入可能会受到说话人身份混淆的影响。本文从说话人一致性的角度提高TSE性能，提出了一种基于质心的说话人一致性感知目标说话人提取方法，并引入了条件损失抑制到训练过程中。实验结果表明，该方法能有效提高TSE性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>目标说话人提取（TSE）依赖参考线索来从混合语音中提取目标语音。</li>
<li>说话人嵌入在TSE系统中对性能至关重要，但可能存在说话人身份混淆的问题。</li>
<li>本文从说话人一致性的角度提出改进TSE性能的方法。</li>
<li>引入了一种基于质心的说话人一致性损失来提高TSE性能，确保注册和提取的语音之间的说话人一致性。</li>
<li>将条件损失抑制集成到训练过程中，进一步提高TSE的性能。</li>
<li>实验结果验证了所提出方法的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09510">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-89abe391191dca8668a9d92aa8258f8f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c667beefa0bfb40f72ddf8d9bb7e7051.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8461d61855d99a822119f23e8afd8a6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52feb30be59155797134b9f5546069f5.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DMF2Mel-A-Dynamic-Multiscale-Fusion-Network-for-EEG-Driven-Mel-Spectrogram-Reconstruction"><a href="#DMF2Mel-A-Dynamic-Multiscale-Fusion-Network-for-EEG-Driven-Mel-Spectrogram-Reconstruction" class="headerlink" title="DMF2Mel: A Dynamic Multiscale Fusion Network for EEG-Driven Mel   Spectrogram Reconstruction"></a>DMF2Mel: A Dynamic Multiscale Fusion Network for EEG-Driven Mel   Spectrogram Reconstruction</h2><p><strong>Authors:Cunhang Fan, Sheng Zhang, Jingjing Zhang, Enrui Liu, Xinhui Li, Gangming Zhao, Zhao Lv</strong></p>
<p>Decoding speech from brain signals is a challenging research problem. Although existing technologies have made progress in reconstructing the mel spectrograms of auditory stimuli at the word or letter level, there remain core challenges in the precise reconstruction of minute-level continuous imagined speech: traditional models struggle to balance the efficiency of temporal dependency modeling and information retention in long-sequence decoding. To address this issue, this paper proposes the Dynamic Multiscale Fusion Network (DMF2Mel), which consists of four core components: the Dynamic Contrastive Feature Aggregation Module (DC-FAM), the Hierarchical Attention-Guided Multi-Scale Network (HAMS-Net), the SplineMap attention mechanism, and the bidirectional state space module (convMamba). Specifically, the DC-FAM separates speech-related “foreground features” from noisy “background features” through local convolution and global attention mechanisms, effectively suppressing interference and enhancing the representation of transient signals. HAMS-Net, based on the U-Net framework,achieves cross-scale fusion of high-level semantics and low-level details. The SplineMap attention mechanism integrates the Adaptive Gated Kolmogorov-Arnold Network (AGKAN) to combine global context modeling with spline-based local fitting. The convMamba captures long-range temporal dependencies with linear complexity and enhances nonlinear dynamic modeling capabilities. Results on the SparrKULee dataset show that DMF2Mel achieves a Pearson correlation coefficient of 0.074 in mel spectrogram reconstruction for known subjects (a 48% improvement over the baseline) and 0.048 for unknown subjects (a 35% improvement over the baseline).Code is available at: <a target="_blank" rel="noopener" href="https://github.com/fchest/DMF2Mel">https://github.com/fchest/DMF2Mel</a>. </p>
<blockquote>
<p>从脑电波解码语音是一个具有挑战性的研究课题。尽管现有技术已在重建听觉刺激的梅尔频谱图（在单词或字母层面）方面取得进展，但在精确重建分钟级连续想象中的语音方面仍存在核心挑战：传统模型在平衡时间依赖性建模的效率和长序列解码中的信息保留方面面临困难。针对这一问题，本文提出了动态多尺度融合网络（DMF2Mel），它包含四个核心组件：动态对比特征聚合模块（DC-FAM）、分层注意力引导多尺度网络（HAMS-Net）、SplineMap注意力机制和双向状态空间模块（convMamba）。具体来说，DC-FAM通过局部卷积和全局注意力机制，将语音相关的“前景特征”与噪声“背景特征”分离，有效地抑制了干扰并增强了瞬态信号的表示。HAMS-Net基于U-Net框架，实现了高级语义和低级细节的跨尺度融合。SplineMap注意力机制结合了自适应门控Kolmogorov-Arnold网络（AGKAN），将全局上下文建模与基于样条的局部拟合相结合。convMamba以线性复杂度捕捉长期时间依赖性，并增强了非线性动态建模能力。在SparrKULee数据集上的结果表明，DMF2Mel在已知主题的梅尔频谱图重建中实现了0.074的皮尔逊相关系数（比基线提高了48%），在未知主题上实现了0.048（比基线提高了35%）。代码可用：<a target="_blank" rel="noopener" href="https://github.com/fchest/DMF2Mel%E3%80%82">https://github.com/fchest/DMF2Mel。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07526v3">PDF</a> Accepted by ACM MM 2025</p>
<p><strong>Summary</strong><br>解码脑电波中的语音是一个挑战性的研究课题。现有技术虽已能在字词级别重建听觉刺激的梅尔频谱图，但在重建分钟级别的连续想象语音时仍存在核心挑战。传统模型难以在时序依赖建模的效率与长序列解码的信息保留之间取得平衡。为解决此问题，本文提出了动态多尺度融合网络（DMF2Mel），包含四大核心组件：动态对比特征聚合模块（DC-FAM）、分层注意力引导多尺度网络（HAMS-Net）、SplineMap注意力机制和双向状态空间模块（convMamba）。DMF2Mel在SparrKULee数据集上的重建结果表现出色，对已知主体和未知主体的梅尔频谱图重建的Pearson相关系数分别达到了0.074和0.048，相较于基线方法有显著改善。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>解码脑电波中的语音是一个充满挑战的研究课题。</li>
<li>当前技术在重建连续想象语音时面临核心挑战。</li>
<li>传统模型在时序依赖建模和信息保留方面存在困难。</li>
<li>DMF2Mel网络由四大核心组件构成，旨在解决上述问题。</li>
<li>DC-FAM能有效分离语音相关的“前景特征”和干扰的“背景特征”。</li>
<li>HAMS-Net基于U-Net框架，实现高层次语义与低层次细节的跨尺度融合。</li>
<li>DMF2Mel在SparrKULee数据集上的重建结果优于基线方法，对已知和未知主体的重建均有显著改善。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07526">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-75dd89c90e47f6a755ae962ddd939ffa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a94afe6e918056d864f9d8747f150815.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cda5a2b51211f1faf9948e972ee09eaf.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Verbal-Werewolf-Engage-Users-with-Verbalized-Agentic-Werewolf-Game-Framework"><a href="#Verbal-Werewolf-Engage-Users-with-Verbalized-Agentic-Werewolf-Game-Framework" class="headerlink" title="Verbal Werewolf: Engage Users with Verbalized Agentic Werewolf Game   Framework"></a>Verbal Werewolf: Engage Users with Verbalized Agentic Werewolf Game   Framework</h2><p><strong>Authors:Qihui Fan, Wenbo Li, Enfu Nan, Yixiao Chen, Lei Lu, Pu Zhao, Yanzhi Wang</strong></p>
<p>The growing popularity of social deduction games has created an increasing need for intelligent frameworks where humans can collaborate with AI agents, particularly in post-pandemic contexts with heightened psychological and social pressures. Social deduction games like Werewolf, traditionally played through verbal communication, present an ideal application for Large Language Models (LLMs) given their advanced reasoning and conversational capabilities. Prior studies have shown that LLMs can outperform humans in Werewolf games, but their reliance on external modules introduces latency that left their contribution in academic domain only, and omit such game should be user-facing. We propose \textbf{Verbal Werewolf}, a novel LLM-based Werewolf game system that optimizes two parallel pipelines: gameplay powered by state-of-the-art LLMs and a fine-tuned Text-to-Speech (TTS) module that brings text output to life. Our system operates in near real-time without external decision-making modules, leveraging the enhanced reasoning capabilities of modern LLMs like DeepSeek V3 to create a more engaging and anthropomorphic gaming experience that significantly improves user engagement compared to existing text-only frameworks. </p>
<blockquote>
<p>随着社交推理游戏的日益普及，人类与AI代理协作的智能框架的需求也在不断增加，特别是在疫情后心理和社交压力加剧的背景下。狼人杀等社交推理游戏通过口头交流进行，鉴于大型语言模型（LLM）具备先进的推理和对话能力，是理想的适用对象。早期研究表明，LLM在狼人杀游戏中的表现可以超越人类，但它们对外部模块的依赖导致延迟，这使它们的贡献仅限于学术领域，并忽略了这样的游戏应该是面向用户的。我们提出了“Verbal Werewolf”，这是一种基于LLM的新型狼人杀游戏系统，优化了两个并行管道：由最新LLM驱动的游戏玩法和经过精细调整的文本到语音（TTS）模块，将文本输出转化为生动的声音。我们的系统在近实时状态下运行，无需外部决策模块，利用现代LLM（如DeepSeek V3）的增强推理能力，创建了一种更具参与感和拟人化的游戏体验，相较于现有的纯文本框架，极大地提高了用户参与度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00160v2">PDF</a> </p>
<p><strong>Summary</strong><br>社交推理游戏的普及以及对智能框架的需求不断增长，特别是在疫情后心理和社交压力增加的背景下。文字版狼人杀作为一种社会推理游戏，因其逻辑推理和对话能力成为大型语言模型（LLM）的理想应用。尽管研究表明LLM在狼人杀游戏中表现优于人类，但由于对外部模块的依赖，导致反应时间延迟而局限于学术领域。本研究提出了一个名为“Verbal Werewolf”的新型系统，该系统优化了游戏流程并采用了先进的LLM和精细调整的文本到语音（TTS）模块。我们的系统在无外部决策模块的情况下几乎实时运行，利用现代LLM的推理能力创建一个更有趣的人型游戏体验，与用户参与现有纯文本框架相比显著提高用户参与度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>社会推理游戏受欢迎度增长需要人类与AI协作的智能框架。</li>
<li>文字版狼人杀成为大型语言模型（LLM）的理想应用，具备逻辑推理和对话能力。</li>
<li>LLM在狼人杀游戏中表现优于人类，但对外部模块的依赖导致反应延迟限制了实际应用。</li>
<li>“Verbal Werewolf”系统优化了两大模块：采用先进LLM的游戏流程和精细调整的文本到语音（TTS）模块。</li>
<li>系统实现了近实时运行，无需外部决策模块。</li>
<li>利用现代LLM的推理能力提高了游戏的趣味性和参与度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00160">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3ee0fd2b5cfacca1961c36a37a108581.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1caaddeb0632cb637c347708c653112d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5622418dd1803c7444a1e930848d4e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1fbfaa06e7aeedeb6cd012fb18ed839.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Voice-Conversion-via-Content-Aware-Timbre-Ensemble-and-Conditional-Flow-Matching"><a href="#Zero-Shot-Voice-Conversion-via-Content-Aware-Timbre-Ensemble-and-Conditional-Flow-Matching" class="headerlink" title="Zero-Shot Voice Conversion via Content-Aware Timbre Ensemble and   Conditional Flow Matching"></a>Zero-Shot Voice Conversion via Content-Aware Timbre Ensemble and   Conditional Flow Matching</h2><p><strong>Authors:Yu Pan, Yuguang Yang, Jixun Yao, Lei Ma, Jianjun Zhao</strong></p>
<p>Despite recent advances in zero-shot voice conversion (VC), achieving speaker similarity and naturalness comparable to ground-truth recordings remains a significant challenge. In this letter, we propose CTEFM-VC, a zero-shot VC framework that integrates content-aware timbre ensemble modeling with conditional flow matching. Specifically, CTEFM-VC decouples utterances into content and timbre representations and leverages a conditional flow matching model to reconstruct the Mel-spectrogram of the source speech. To enhance its timbre modeling capability and naturalness of generated speech, we first introduce a context-aware timbre ensemble modeling approach that adaptively integrates diverse speaker verification embeddings and enables the effective utilization of source content and target timbre elements through a cross-attention module. Furthermore, a structural similarity-based timbre loss is presented to jointly train CTEFM-VC end-to-end. Experiments show that CTEFM-VC consistently achieves the best performance in all metrics assessing speaker similarity, speech naturalness, and intelligibility, significantly outperforming state-of-the-art zero-shot VC systems. </p>
<blockquote>
<p>尽管零样本语音转换（VC）领域近期取得了进展，但实现与真实录音相当的说话人相似性和自然性仍然是一个重大挑战。在这封信中，我们提出了CTEFM-VC，这是一个零样本VC框架，它结合了内容感知音色集合建模和条件流匹配。具体来说，CTEFM-VC将话语分解成内容和音色表示，并利用条件流匹配模型重建源语音的梅尔频谱图。为了增强其时域模型的能力和生成的语音的自然度，我们首先引入了一种上下文感知的音色集合建模方法，该方法自适应地集成了多种说话人验证嵌入，并通过交叉注意力模块实现了源内容和目标音色元素的有效利用。此外，还提出了一种基于结构相似性的音色损失，以端到端的方式联合训练CTEFM-VC。实验表明，CTEFM-VC在评估说话人相似性、语音自然性和清晰度的所有指标上均表现最佳，显著优于最先进的零样本VC系统。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02026v2">PDF</a> Work in progress; 5 pages;</p>
<p><strong>Summary</strong></p>
<p>文本提出了CTEFM-VC零射击语音转换框架，融合了内容感知音色集建模与条件流匹配技术。该框架通过对语音进行内容和音色分离，使用条件流匹配模型重建源语音的梅尔频谱图，增强了音色建模能力和生成语音的自然度。引入上下文感知音色集建模方法，自适应集成多种说话人验证嵌入，并通过交叉注意力模块实现源内容和目标音色元素的有效利用。此外，提出了一种基于结构相似性的音色损失，以端到端的方式联合训练CTEFM-VC。实验表明，CTEFM-VC在评估说话人相似性、语音自然度和清晰度的所有指标上均表现最佳，显著优于其他零射击语音转换系统。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是关于文本内容的七个关键见解：</p>
<ol>
<li>CTEFM-VC是零射击语音转换的新框架，结合了内容感知音色集建模与条件流匹配技术。</li>
<li>该框架通过分离语音内容和音色表示，使用条件流匹配模型重建源语音的梅尔频谱图。</li>
<li>引入上下文感知音色集建模方法，提高了音色建模能力和生成语音的自然度。</li>
<li>通过交叉注意力模块实现源内容和目标音色元素的有效结合。</li>
<li>提出了一种基于结构相似性的音色损失函数，用于联合训练CTEFM-VC。</li>
<li>CTEFM-VC在说话人相似性、语音自然度和清晰度的评估指标上表现最佳。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.02026">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4238e18e3872b9f3a178bb71f5a22b07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fce4b4990f09db123bd656e394f4805d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b3d962799d9d1106faa16905e933a76b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e547a5a38f2b6b17a72f066773b15c21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b919f00ab90aea1e08df24d892a27a4.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-13/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-13/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-13/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3b7a4db9c5cdf168d724420000cc9e7c.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2025-08-13  SystolicAttention Fusing FlashAttention within a Single Systolic Array
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-13/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0f5ff0d234a6fad68a284d21bc750773.jpg" class="responsive-img" alt="医学影像/Breast Ultrasound">
                        
                        <span class="card-title">医学影像/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学影像/Breast Ultrasound 方向最新论文已更新，请持续关注 Update in 2025-08-13  UltraAD Fine-Grained Ultrasound Anomaly Classification via Few-Shot   CLIP Adaptation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    医学影像/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">医学影像/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28172.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
