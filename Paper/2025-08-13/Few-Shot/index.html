<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-13  Expert Preference-based Evaluation of Automated Related Work Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ad342342c4d5f43a495cc5445f33b7c9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-13-æ›´æ–°"><a href="#2025-08-13-æ›´æ–°" class="headerlink" title="2025-08-13 æ›´æ–°"></a>2025-08-13 æ›´æ–°</h1><h2 id="Expert-Preference-based-Evaluation-of-Automated-Related-Work-Generation"><a href="#Expert-Preference-based-Evaluation-of-Automated-Related-Work-Generation" class="headerlink" title="Expert Preference-based Evaluation of Automated Related Work Generation"></a>Expert Preference-based Evaluation of Automated Related Work Generation</h2><p><strong>Authors:Furkan ÅahinuÃ§, Subhabrata Dutta, Iryna Gurevych</strong></p>
<p>Expert domain writing, such as scientific writing, typically demands extensive domain knowledge. Recent advances in LLMs show promising potential in reducing the expert workload. However, evaluating the quality of automatically generated scientific writing is a crucial open issue, as it requires knowledge of domain-specific evaluation criteria and the ability to discern expert preferences. Conventional automatic metrics and LLM-as-a-judge systems are insufficient to grasp expert preferences and domain-specific quality standards. To address this gap and support human-AI collaborative writing, we focus on related work generation, one of the most challenging scientific tasks, as an exemplar. We propose GREP, a multi-turn evaluation framework that integrates classical related work evaluation criteria with expert-specific preferences. Instead of assigning a single score, our framework decomposes the evaluation into fine-grained dimensions. This localized evaluation approach is further augmented with contrastive few-shot examples to provide detailed contextual guidance for the evaluation dimensions. The design principles allow our framework to deliver cardinal assessment of quality, which can facilitate better post-training compared to ordinal preference data. For better accessibility, we design two variants of GREP: a more precise variant with proprietary LLMs as evaluators, and a cheaper alternative with open-weight LLMs. Empirical investigation reveals that our framework is able to assess the quality of related work sections in a much more robust manner compared to standard LLM judges, reflects natural scenarios of scientific writing, and bears a strong correlation with the human expert assessment. We also observe that generations from state-of-the-art LLMs struggle to satisfy validation constraints of a suitable related work section. They (mostly) fail to improve based on feedback as well. </p>
<blockquote>
<p>ä¸“ä¸šé¢†åŸŸå†™ä½œï¼Œå¦‚ç§‘å­¦å†™ä½œï¼Œé€šå¸¸éœ€è¦å¹¿æ³›çš„ä¸“ä¸šçŸ¥è¯†ã€‚æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ˜¾ç¤ºå‡ºäº†å‡å°‘ä¸“å®¶å·¥ä½œé‡çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¯„ä¼°è‡ªåŠ¨ç”Ÿæˆçš„ç§‘å­¦å†™ä½œçš„è´¨é‡æ˜¯ä¸€ä¸ªå…³é”®çš„å¼€æ”¾æ€§é—®é¢˜ï¼Œå› ä¸ºå®ƒéœ€è¦äº†è§£ç‰¹å®šé¢†åŸŸçš„è¯„ä¼°æ ‡å‡†å’Œè¾¨åˆ«ä¸“å®¶åå¥½çš„èƒ½åŠ›ã€‚ä¼ ç»Ÿçš„è‡ªåŠ¨æŒ‡æ ‡å’ŒLLMä½œä¸ºæ³•å®˜çš„ç³»ç»Ÿä¸è¶³ä»¥æŠŠæ¡ä¸“å®¶åå¥½å’Œç‰¹å®šé¢†åŸŸçš„è´¨é‡æ ‡å‡†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·å¹¶æ”¯æŒäººç±»-äººå·¥æ™ºèƒ½åä½œå†™ä½œï¼Œæˆ‘ä»¬ä»¥ç›¸å…³å·¥ä½œç”Ÿæˆè¿™ä¸€æœ€å…·æŒ‘æˆ˜æ€§çš„ç§‘å­¦ä»»åŠ¡ä¸ºä¾‹ã€‚æˆ‘ä»¬æå‡ºäº†GREPï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šè½®è¯„ä¼°æ¡†æ¶ï¼Œå®ƒå°†ç»å…¸çš„ç›¸å…³å·¥ä½œè¯„ä»·æ ‡å‡†ä¸ä¸“å®¶ç‰¹å®šåå¥½ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸æ˜¯åˆ†é…å•ä¸€åˆ†æ•°ï¼Œè€Œæ˜¯å°†è¯„ä¼°åˆ†è§£ä¸ºç²¾ç»†çš„ç»´åº¦ã€‚è¿™ç§å±€éƒ¨è¯„ä¼°æ–¹æ³•ä¸å¯¹æ¯”çš„å°‘é‡ç¤ºä¾‹ç›¸ç»“åˆï¼Œä¸ºè¯„ä¼°ç»´åº¦æä¾›äº†è¯¦ç»†çš„ä¸Šä¸‹æ–‡æŒ‡å¯¼ã€‚æˆ‘ä»¬çš„è®¾è®¡åŸåˆ™ä½¿æˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿæä¾›è´¨é‡çš„åŸºæ•°è¯„ä¼°ï¼Œè¿™æœ‰åŠ©äºæ›´å¥½åœ°è¿›è¡Œè®­ç»ƒåçš„æ¯”è¾ƒã€‚ä¸ºäº†æ›´æ–¹ä¾¿ä½¿ç”¨ï¼Œæˆ‘ä»¬è®¾è®¡äº†GREPçš„ä¸¤ä¸ªå˜ä½“ï¼šä¸€ä¸ªæ›´ç²¾ç¡®çš„ç‰ˆæœ¬ä½¿ç”¨ä¸“å±LLMä½œä¸ºè¯„ä¼°å™¨ï¼Œä¸€ä¸ªæ›´ä¾¿å®œçš„ç‰ˆæœ¬ä½¿ç”¨å¼€æºLLMã€‚ç»éªŒç ”ç©¶è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿä»¥æ›´ä¸ºç¨³å¥çš„æ–¹å¼è¯„ä¼°ç›¸å…³å·¥ä½œéƒ¨åˆ†çš„è´¨é‡ï¼Œç›¸æ¯”äºæ ‡å‡†LLMæ³•å®˜ï¼Œå®ƒåæ˜ äº†ç§‘å­¦å†™ä½œçš„è‡ªç„¶åœºæ™¯ï¼Œå¹¶ä¸äººç±»ä¸“å®¶è¯„ä¼°å…·æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ï¼Œæ¥è‡ªæœ€æ–°LLMçš„ç”Ÿæˆåœ¨æ»¡è¶³ç›¸å…³å·¥ä½œéƒ¨åˆ†çš„éªŒè¯çº¦æŸæ–¹é¢è¡¨ç°æŒ£æ‰ã€‚ä»–ä»¬å¤§å¤šæ•°æœªèƒ½æ ¹æ®åé¦ˆè¿›è¡Œæ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07955v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://ukplab.github.io/arxiv2025-expert-eval-rw/">https://ukplab.github.io/arxiv2025-expert-eval-rw/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸä¸­çš„ä¸€ä¸ªç ”ç©¶é—®é¢˜ï¼Œå³åœ¨ç§‘å­¦å†™ä½œä¸­è‡ªåŠ¨è¯„ä»·çš„è´¨é‡é—®é¢˜ã€‚ä¸ºè§£å†³è¯¥é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºGREPçš„å¤šè½®è¯„ä»·æ¡†æ¶ï¼Œç”¨äºç²¾ç»†è¯„ä¼°ç§‘å­¦å†™ä½œçš„è´¨é‡ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç»å…¸çš„ç›¸å…³å·¥ä½œè¯„ä»·æ ‡å‡†ä¸ä¸“å®¶ç‰¹å®šçš„åå¥½ï¼Œé€šè¿‡å±€éƒ¨åŒ–çš„è¯„ä»·æ–¹å¼è¾…ä»¥å¯¹æ¯”çš„å°‘é‡ç¤ºä¾‹æ¥æä¾›è¯¦ç»†çš„ä¸Šä¸‹æ–‡æŒ‡å¯¼ã€‚æ­¤å¤–ï¼Œä¸ºäº†å®é™…åº”ç”¨ï¼Œè®¾è®¡äº†ä¸¤ç§GREPå˜ä½“ï¼Œä¸€ç§ä½¿ç”¨ä¸“æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„ä¼°è€…ï¼Œå¦ä¸€ç§åˆ™ä½¿ç”¨å¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ›´ç»æµçš„é€‰æ‹©ã€‚ç»éªŒè°ƒæŸ¥è¡¨æ˜ï¼ŒGREPæ¡†æ¶èƒ½å¤Ÿæ›´ç¨³å¥åœ°è¯„ä¼°ç›¸å…³å·¥ä½œéƒ¨åˆ†çš„è´¨é‡ï¼Œä¸äººç±»ä¸“å®¶è¯„ä¼°æœ‰è¾ƒå¼ºçš„ç›¸å…³æ€§ã€‚ä½†ç°æœ‰å…ˆè¿›æŠ€æœ¯çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆæ»¡è¶³ç›¸å…³å·¥ä½œéƒ¨åˆ†éªŒè¯çº¦æŸçš„å†…å®¹æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä¸”éš¾ä»¥æ ¹æ®åé¦ˆè¿›è¡Œæ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨ç§‘å­¦å†™ä½œä¸­æœ‰é™ä½ä¸“å®¶å·¥ä½œé‡çš„æ½œåŠ›ï¼Œä½†è‡ªåŠ¨ç”Ÿæˆçš„å†™ä½œè´¨é‡è¯„ä»·æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§é—®é¢˜ã€‚</li>
<li>ä¼ ç»Ÿè‡ªåŠ¨æŒ‡æ ‡å’ŒLLMä½œä¸ºè¯„å§”çš„ç³»ç»Ÿæ— æ³•æŠŠæ¡ä¸“å®¶åå¥½å’Œé¢†åŸŸç‰¹å®šè´¨é‡æ ‡å‡†ã€‚</li>
<li>GREPæ¡†æ¶è¢«æå‡ºä»¥è§£å†³æ­¤é—®é¢˜ï¼Œå®ƒç»“åˆç»å…¸çš„ç›¸å…³å·¥ä½œè¯„ä»·æ ‡å‡†ä¸ä¸“å®¶ç‰¹å®šåå¥½ï¼Œè¿›è¡Œç²¾ç»†åŒ–çš„è¯„ä»·ã€‚</li>
<li>GREPæ¡†æ¶é€šè¿‡å±€éƒ¨åŒ–çš„è¯„ä»·å’Œå¯¹æ¯”çš„å°‘é‡ç¤ºä¾‹æä¾›è¯¦ç»†çš„ä¸Šä¸‹æ–‡æŒ‡å¯¼ã€‚</li>
<li>GREPæœ‰ä¸¤ç§å˜ä½“ï¼Œä¸€ç§ä½¿ç”¨ä¸“æœ‰LLMsï¼Œå¦ä¸€ç§ä½¿ç”¨å¼€æºLLMsã€‚</li>
<li>ç»éªŒè°ƒæŸ¥è¡¨æ˜GREPæ¡†æ¶èƒ½å¤Ÿæ›´ç¨³å¥åœ°è¯„ä¼°ç›¸å…³å·¥ä½œéƒ¨åˆ†çš„è´¨é‡ï¼Œä¸äººç±»ä¸“å®¶è¯„ä¼°æœ‰å¼ºç›¸å…³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20ffa31dbc769fa55393018f62a9be0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc70bcf6eaf7389ad704f67398ccb6b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d6235f2e8f18706e15cedd4ea1c4a22.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2739cad01faa6e64f7b3079b60a9d357.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-506530f93f23cf9bbec07d05522e794e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-for-Czech-Aspect-Based-Sentiment-Analysis"><a href="#Large-Language-Models-for-Czech-Aspect-Based-Sentiment-Analysis" class="headerlink" title="Large Language Models for Czech Aspect-Based Sentiment Analysis"></a>Large Language Models for Czech Aspect-Based Sentiment Analysis</h2><p><strong>Authors:Jakub Å mÃ­d, Pavel PÅ™ibÃ¡Åˆ, Pavel KrÃ¡l</strong></p>
<p>Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis task that aims to identify sentiment toward specific aspects of an entity. While large language models (LLMs) have shown strong performance in various natural language processing (NLP) tasks, their capabilities for Czech ABSA remain largely unexplored. In this work, we conduct a comprehensive evaluation of 19 LLMs of varying sizes and architectures on Czech ABSA, comparing their performance in zero-shot, few-shot, and fine-tuning scenarios. Our results show that small domain-specific models fine-tuned for ABSA outperform general-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs achieve state-of-the-art results. We analyze how factors such as multilingualism, model size, and recency influence performance and present an error analysis highlighting key challenges, particularly in aspect term prediction. Our findings provide insights into the suitability of LLMs for Czech ABSA and offer guidance for future research in this area. </p>
<blockquote>
<p>é¢å‘ç‰¹å®šæ–¹é¢çš„æƒ…æ„Ÿåˆ†æï¼ˆABSAï¼‰æ˜¯ä¸€ç§ç²¾ç»†ç²’åº¦çš„æƒ…æ„Ÿåˆ†æä»»åŠ¡ï¼Œæ—¨åœ¨è¯†åˆ«äººä»¬å¯¹å®ä½“çš„ç‰¹å®šæ–¹é¢çš„æƒ…æ„Ÿå€¾å‘ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨æ·å…‹è¯­ABSAæ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹ä¸åŒè§„æ¨¡å’Œæ¶æ„çš„19ä¸ªLLMåœ¨æ·å…‹è¯­ABSAæ–¹é¢è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œæ¯”è¾ƒäº†å®ƒä»¬åœ¨é›¶æ ·æœ¬ã€å°æ ·æœ¬å’Œå¾®è°ƒåœºæ™¯ä¸­çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œé’ˆå¯¹ABSAè¿›è¡Œå¾®è°ƒçš„å°å‹é¢†åŸŸç‰¹å®šæ¨¡å‹åœ¨é›¶æ ·æœ¬å’Œå°æ ·æœ¬è®¾ç½®ä¸­ä¼˜äºé€šç”¨LLMï¼Œè€Œç»è¿‡è°ƒæ ¡çš„LLMåˆ™è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚æˆ‘ä»¬åˆ†æäº†è¯¸å¦‚å¤šè¯­è¨€èƒ½åŠ›ã€æ¨¡å‹è§„æ¨¡å’Œæ›´æ–°é¢‘ç‡ç­‰å› ç´ å¯¹æ€§èƒ½çš„å½±å“ï¼Œå¹¶è¿›è¡Œäº†é”™è¯¯åˆ†æï¼Œçªå‡ºäº†é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–¹é¢æœ¯è¯­é¢„æµ‹æ–¹é¢çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœä¸ºLLMåœ¨æ·å…‹è¯­ABSAä¸­çš„é€‚ç”¨æ€§æä¾›äº†è§è§£ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶æä¾›äº†æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07860v1">PDF</a> Accepted for presentation at the 28th International Conference on   Text, Speech and Dialogue (TSD 2025)</p>
<p><strong>Summary</strong></p>
<p>æ·å…‹è¯­æ–¹é¢çš„åŸºäºæ–¹é¢çš„æƒ…æ„Ÿåˆ†æï¼ˆABSAï¼‰æ—¨åœ¨è¯†åˆ«å®ä½“ç‰¹å®šæ–¹é¢çš„æƒ…æ„Ÿã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨æ·å…‹ABSAæ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶å¯¹19ç§ä¸åŒè§„æ¨¡å’Œæ¶æ„çš„LLMè¿›è¡Œäº†å…¨é¢çš„æ·å…‹ABSAè¯„ä¼°ï¼Œæ¯”è¾ƒäº†å®ƒä»¬åœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¾®è°ƒåœºæ™¯ä¸­çš„æ€§èƒ½ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œé’ˆå¯¹ABSAè¿›è¡Œå¾®è°ƒçš„å°å‹é¢†åŸŸç‰¹å®šæ¨¡å‹åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸­ä¼˜äºé€šç”¨LLMï¼Œè€Œç»è¿‡è°ƒæ ¡çš„LLMåˆ™å–å¾—äº†æœ€æ–°æŠ€æœ¯æˆæœã€‚æœ¬ç ”ç©¶åˆ†æäº†è¯¸å¦‚å¤šè¯­è¨€èƒ½åŠ›ã€æ¨¡å‹è§„æ¨¡å’Œæ—¶æ•ˆæ€§ç­‰å› ç´ å¯¹æ€§èƒ½çš„å½±å“ï¼Œå¹¶è¿›è¡Œäº†é”™è¯¯åˆ†æï¼Œç‰¹åˆ«å¼ºè°ƒäº†æœ¯è¯­é¢„æµ‹æ–¹é¢çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶ä¸ºLLMåœ¨æ·å…‹ABSAæ–¹é¢çš„é€‚ç”¨æ€§æä¾›äº†è§è§£ï¼Œå¹¶ä¸ºè¿™ä¸€é¢†åŸŸçš„æœªæ¥ç ”ç©¶æä¾›äº†æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ·å…‹è¯­åŸºäºæ–¹é¢çš„æƒ…æ„Ÿåˆ†æï¼ˆABSAï¼‰ä¸Šçš„è¡¨ç°å°šå¾…æ¢ç´¢ã€‚</li>
<li>åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹ï¼Œé’ˆå¯¹ABSAè¿›è¡Œå¾®è°ƒçš„å°å‹é¢†åŸŸç‰¹å®šæ¨¡å‹è¡¨ç°è¾ƒå¥½ã€‚</li>
<li>ç»è¿‡è°ƒæ ¡çš„LLMå–å¾—äº†æœ€æ–°æŠ€æœ¯æˆæœã€‚</li>
<li>å¤šè¯­è¨€èƒ½åŠ›ã€æ¨¡å‹è§„æ¨¡å’Œæ—¶æ•ˆæ€§å¯¹LLMåœ¨ABSAä¸­çš„æ€§èƒ½æœ‰å½±å“ã€‚</li>
<li>æœ¯è¯­é¢„æµ‹æ˜¯ABSAä¸­çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶æä¾›äº†LLMsåœ¨æ·å…‹ABSAä¸­çš„é€‚ç”¨æ€§è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07860">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0f3fd619d2c0d21a3bc07ca776dfef0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81a0577a8366dc3e5a141c97f2af21aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95a63f5a0f6a4b816d0f875c3befc21b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd409bffc8d78173e716a35e5bee51b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc5b259814eeecf629de7342b2184a19.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Effortless-Vision-Language-Model-Specialization-in-Histopathology-without-Annotation"><a href="#Effortless-Vision-Language-Model-Specialization-in-Histopathology-without-Annotation" class="headerlink" title="Effortless Vision-Language Model Specialization in Histopathology   without Annotation"></a>Effortless Vision-Language Model Specialization in Histopathology   without Annotation</h2><p><strong>Authors:Jingna Qiu, Nishanth Jain, Jonas Ammeling, Marc Aubreville, Katharina Breininger</strong></p>
<p>Recent advances in Vision-Language Models (VLMs) in histopathology, such as CONCH and QuiltNet, have demonstrated impressive zero-shot classification capabilities across various tasks. However, their general-purpose design may lead to suboptimal performance in specific downstream applications. While supervised fine-tuning methods address this issue, they require manually labeled samples for adaptation. This paper investigates annotation-free adaptation of VLMs through continued pretraining on domain- and task-relevant image-caption pairs extracted from existing databases. Our experiments on two VLMs, CONCH and QuiltNet, across three downstream tasks reveal that these pairs substantially enhance both zero-shot and few-shot performance. Notably, with larger training sizes, continued pretraining matches the performance of few-shot methods while eliminating manual labeling. Its effectiveness, task-agnostic design, and annotation-free workflow make it a promising pathway for adapting VLMs to new histopathology tasks. Code is available at <a target="_blank" rel="noopener" href="https://github.com/DeepMicroscopy/Annotation-free-VLM-specialization">https://github.com/DeepMicroscopy/Annotation-free-VLM-specialization</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œç—…ç†ç»„ç»‡å­¦çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¦‚CONCHå’ŒQuiltNetç­‰å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„é›¶æ ·æœ¬åˆ†ç±»èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶é€šç”¨è®¾è®¡å¯èƒ½å¯¼è‡´åœ¨ç‰¹å®šä¸‹æ¸¸åº”ç”¨ä¸­çš„æ€§èƒ½ä¸ä½³ã€‚è™½ç„¶ç›‘ç£å¾®è°ƒæ–¹æ³•å¯ä»¥è§£å†³æ­¤é—®é¢˜ï¼Œä½†å®ƒä»¬éœ€è¦æ‰‹åŠ¨æ ‡è®°çš„æ ·æœ¬è¿›è¡Œé€‚åº”ã€‚æœ¬æ–‡ç ”ç©¶äº†é€šè¿‡ç»§ç»­åœ¨ç°æœ‰æ•°æ®åº“ä¸­æå–çš„ä¸é¢†åŸŸå’Œä»»åŠ¡ç›¸å…³çš„å›¾åƒæ ‡é¢˜å¯¹è¿›è¡Œé¢„è®­ç»ƒï¼Œå®ç°VLMsçš„æ— æ ‡æ³¨é€‚åº”ã€‚æˆ‘ä»¬åœ¨CONCHå’ŒQuiltNetä¸¤ç§VLMsä¸Šè¿›è¡Œä¸‰é¡¹ä¸‹æ¸¸ä»»åŠ¡çš„å®éªŒè¡¨æ˜ï¼Œè¿™äº›å›¾åƒæ ‡é¢˜å¯¹æ˜¾è‘—æé«˜äº†é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œéšç€è®­ç»ƒè§„æ¨¡çš„æ‰©å¤§ï¼Œç»§ç»­é¢„è®­ç»ƒçš„æ€§èƒ½ä¸å°‘æ ·æœ¬æ–¹æ³•ç›¸åŒ¹é…ï¼ŒåŒæ—¶æ¶ˆé™¤äº†æ‰‹åŠ¨æ ‡æ³¨çš„éœ€è¦ã€‚å…¶æœ‰æ•ˆæ€§ã€ä»»åŠ¡æ— å…³çš„è®¾è®¡å’Œæ— æ ‡æ³¨çš„å·¥ä½œæµç¨‹ä½¿å…¶æˆä¸ºå°†VLMsé€‚åº”æ–°ç—…ç†ç»„ç»‡å­¦ä»»åŠ¡çš„æœ‰å‰é€”çš„é€”å¾„ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/DeepMicroscopy/Annotation-free-VLM-specialization%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/DeepMicroscopy/Annotation-free-VLM-specializationæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07835v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç»„ç»‡ç—…ç†å­¦é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œå¦‚CONCHå’ŒQuiltNetç­‰æ¨¡å‹å±•ç°å‡ºé›¶æ ·æœ¬åˆ†ç±»èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¯¹äºç‰¹å®šä¸‹æ¸¸åº”ç”¨ï¼Œé€šç”¨è®¾è®¡å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚æœ¬æ–‡ç ”ç©¶é€šè¿‡æŒç»­é¢„è®­ç»ƒåœ¨ç°æœ‰æ•°æ®åº“ä¸­æå–çš„é¢†åŸŸå’Œä»»åŠ¡ç›¸å…³çš„å›¾åƒæ ‡é¢˜å¯¹æ¥å®ç°æ— æ ‡æ³¨é€‚åº”çš„VLMsã€‚å®éªŒè¡¨æ˜è¿™ç§é¢„è®­ç»ƒæ–¹æ³•èƒ½æ˜¾è‘—æå‡é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ï¼Œä¸”éšç€è®­ç»ƒé›†å¢å¤§ï¼Œå…¶æ€§èƒ½ä¸å°‘æ ·æœ¬æ–¹æ³•ç›¸åŒ¹é…ï¼ŒåŒæ—¶æ— éœ€æ‰‹åŠ¨æ ‡æ³¨ã€‚è¿™ä¸ºé€‚åº”æ–°ç»„ç»‡ç—…ç†å­¦ä»»åŠ¡çš„VLMsæä¾›äº†æœ‰å‰æ™¯çš„è·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€æ–°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¦‚CONCHå’ŒQuiltNetå±•ç°å‡ºé›¶æ ·æœ¬åˆ†ç±»èƒ½åŠ›ã€‚</li>
<li>VLMsåœ¨ç‰¹å®šä¸‹æ¸¸åº”ç”¨ä¸­æ€§èƒ½å¯èƒ½ä¸ä½³ã€‚</li>
<li>æŒç»­é¢„è®­ç»ƒèƒ½æé«˜VLMsçš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ã€‚</li>
<li>é¢„è®­ç»ƒé€šè¿‡ä½¿ç”¨ä»ç°æœ‰æ•°æ®åº“ä¸­æå–çš„é¢†åŸŸå’Œä»»åŠ¡ç›¸å…³çš„å›¾åƒæ ‡é¢˜å¯¹è¿›è¡Œã€‚</li>
<li>éšç€è®­ç»ƒé›†å¢å¤§ï¼Œæ— æ ‡æ³¨é¢„è®­ç»ƒçš„æ€§èƒ½ä¸å°‘æ ·æœ¬æ–¹æ³•ç›¸åŒ¹é…ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€æ‰‹åŠ¨æ ‡æ³¨ï¼Œä¸ºVLMsé€‚åº”æ–°ä»»åŠ¡æä¾›äº†ä¾¿åˆ©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07835">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fbe7547330849f4fd6839bf438bcde72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2684021b351f16615709e98814698d55.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-44d754b7e9c49d473531a5e458854c85.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AR-VRM-Imitating-Human-Motions-for-Visual-Robot-Manipulation-with-Analogical-Reasoning"><a href="#AR-VRM-Imitating-Human-Motions-for-Visual-Robot-Manipulation-with-Analogical-Reasoning" class="headerlink" title="AR-VRM: Imitating Human Motions for Visual Robot Manipulation with   Analogical Reasoning"></a>AR-VRM: Imitating Human Motions for Visual Robot Manipulation with   Analogical Reasoning</h2><p><strong>Authors:Dejie Yang, Zijing Zhao, Yang Liu</strong></p>
<p>Visual Robot Manipulation (VRM) aims to enable a robot to follow natural language instructions based on robot states and visual observations, and therefore requires costly multi-modal data. To compensate for the deficiency of robot data, existing approaches have employed vision-language pretraining with large-scale data. However, they either utilize web data that differs from robotic tasks, or train the model in an implicit way (e.g., predicting future frames at the pixel level), thus showing limited generalization ability under insufficient robot data. In this paper, we propose to learn from large-scale human action video datasets in an explicit way (i.e., imitating human actions from hand keypoints), introducing Visual Robot Manipulation with Analogical Reasoning (AR-VRM). To acquire action knowledge explicitly from human action videos, we propose a keypoint Vision-Language Model (VLM) pretraining scheme, enabling the VLM to learn human action knowledge and directly predict human hand keypoints. During fine-tuning on robot data, to facilitate the robotic arm in imitating the action patterns of human motions, we first retrieve human action videos that perform similar manipulation tasks and have similar historical observations , and then learn the Analogical Reasoning (AR) map between human hand keypoints and robot components. Taking advantage of focusing on action keypoints instead of irrelevant visual cues, our method achieves leading performance on the CALVIN benchmark {and real-world experiments}. In few-shot scenarios, our AR-VRM outperforms previous methods by large margins , underscoring the effectiveness of explicitly imitating human actions under data scarcity. </p>
<blockquote>
<p>è§†è§‰æœºå™¨äººæ“ä½œï¼ˆVRMï¼‰æ—¨åœ¨ä½¿æœºå™¨äººèƒ½å¤Ÿæ ¹æ®æœºå™¨äººçŠ¶æ€åŠè§†è§‰è§‚å¯Ÿæ¥æ‰§è¡Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå› æ­¤éœ€è¦ä½¿ç”¨æ˜‚è´µçš„å¤šæ¨¡æ€æ•°æ®ã€‚ä¸ºäº†å¼¥è¡¥æœºå™¨äººæ•°æ®çš„ä¸è¶³ï¼Œç°æœ‰çš„æ–¹æ³•å·²ç»é‡‡ç”¨äº†å¤§è§„æ¨¡æ•°æ®çš„è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ–¹æ³•ã€‚ç„¶è€Œï¼Œå®ƒä»¬è¦ä¹ˆä½¿ç”¨ä¸æœºå™¨äººä»»åŠ¡ä¸åŒçš„ç½‘ç»œæ•°æ®ï¼Œè¦ä¹ˆä»¥éšå¼æ–¹å¼è®­ç»ƒæ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œåœ¨åƒç´ çº§åˆ«é¢„æµ‹æœªæ¥å¸§ï¼‰ï¼Œå› æ­¤åœ¨æœºå™¨äººæ•°æ®ä¸è¶³çš„æƒ…å†µä¸‹ï¼Œæ˜¾ç¤ºå‡ºæœ‰é™çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä»å¤§è§„æ¨¡äººç±»è¡Œä¸ºè§†é¢‘æ•°æ®é›†ä¸­æ˜¾å¼å­¦ä¹ çš„æ–¹æ³•ï¼ˆå³ï¼Œé€šè¿‡æ‰‹éƒ¨å…³é”®ç‚¹æ¨¡ä»¿äººç±»è¡Œä¸ºï¼‰ï¼Œå¹¶å¼•å…¥äº†å…·æœ‰ç±»æ¯”æ¨ç†ï¼ˆAR-VRMï¼‰çš„è§†è§‰æœºå™¨äººæ“ä½œã€‚ä¸ºäº†ä»äººç±»è¡Œä¸ºè§†é¢‘ä¸­æ˜¾å¼åœ°è·å–åŠ¨ä½œçŸ¥è¯†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…³é”®ç‚¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é¢„è®­ç»ƒæ–¹æ¡ˆï¼Œä½¿VLMèƒ½å¤Ÿå­¦ä¹ äººç±»è¡Œä¸ºçŸ¥è¯†å¹¶ç›´æ¥é¢„æµ‹äººç±»æ‰‹éƒ¨å…³é”®ç‚¹ã€‚åœ¨æœºå™¨äººæ•°æ®è¿›è¡Œå¾®è°ƒæœŸé—´ï¼Œä¸ºäº†ä¿ƒä½¿æœºæ¢°è‡‚æ¨¡ä»¿äººç±»åŠ¨ä½œçš„æ¨¡å¼ï¼Œæˆ‘ä»¬é¦–å…ˆæ£€ç´¢æ‰§è¡Œç±»ä¼¼æ“ä½œä»»åŠ¡å¹¶å…·æœ‰ç›¸ä¼¼å†å²è§‚å¯Ÿè®°å½•çš„äººç±»è¡Œä¸ºè§†é¢‘ï¼Œç„¶åå­¦ä¹ äººç±»æ‰‹éƒ¨å…³é”®ç‚¹å’Œæœºå™¨äººç»„ä»¶ä¹‹é—´çš„ç±»æ¯”æ¨ç†ï¼ˆARï¼‰æ˜ å°„ã€‚é€šè¿‡å…³æ³¨åŠ¨ä½œå…³é”®ç‚¹è€Œéæ— å…³çš„è§†è§‰çº¿ç´¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨CALVINåŸºå‡†æµ‹è¯•{å’ŒçœŸå®ä¸–ç•Œå®éªŒ}ä¸­å–å¾—äº†é¢†å…ˆæ€§èƒ½ã€‚åœ¨å°‘æ•°åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬çš„AR-VRMå¤§å¹…è¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ï¼Œçªæ˜¾äº†åœ¨æ•°æ®ç¨€ç¼ºæƒ…å†µä¸‹æ˜¾å¼æ¨¡ä»¿äººç±»åŠ¨ä½œçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07626v1">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç±»æ¯”æ¨ç†çš„è§†è§‰æœºå™¨äººæ“ä½œï¼ˆAR-VRMï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨ä»å¤§è§„æ¨¡çš„äººç±»è¡Œä¸ºè§†é¢‘æ•°æ®é›†ä¸­æ˜¾å¼å­¦ä¹ åŠ¨ä½œçŸ¥è¯†ã€‚é€šè¿‡é¢„è®­ç»ƒå…³é”®ç‚¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œä½¿VLMèƒ½å¤Ÿå­¦ä¹ äººç±»è¡Œä¸ºçŸ¥è¯†å¹¶ç›´æ¥é¢„æµ‹äººæ‰‹å…³é”®ç‚¹ã€‚åœ¨æœºå™¨äººæ•°æ®å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡æ£€ç´¢æ‰§è¡Œç›¸ä¼¼æ“ä½œä»»åŠ¡ä¸”å…·æœ‰ç›¸ä¼¼å†å²è§‚å¯Ÿçš„äººç±»è¡Œä¸ºè§†é¢‘ï¼Œå­¦ä¹ äººæ‰‹å…³é”®ç‚¹å’Œæœºå™¨äººç»„ä»¶ä¹‹é—´çš„ç±»æ¯”æ¨ç†æ˜ å°„ã€‚è¯¥æ–¹æ³•å…³æ³¨åŠ¨ä½œå…³é”®ç‚¹è€Œéæ— å…³çš„è§†è§‰çº¿ç´¢ï¼Œåœ¨CALVINåŸºå‡†æµ‹è¯•å’ŒçœŸå®å®éªŒç¯å¢ƒä¸­è¡¨ç°é¢†å…ˆï¼Œå°¤å…¶åœ¨æ•°æ®ç¨€ç¼ºçš„å°‘æ•°æ‹æ‘„åœºæ™¯ä¸­ï¼ŒAR-VRMå¤§å¹…åº¦ä¼˜äºå‰åºæ–¹æ³•ï¼Œå‡¸æ˜¾æ˜¾å¼æ¨¡ä»¿äººç±»åŠ¨ä½œçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AR-VRMæ–¹æ³•é€šè¿‡ä»å¤§è§„æ¨¡äººç±»è¡Œä¸ºè§†é¢‘æ•°æ®é›†ä¸­æ˜¾å¼å­¦ä¹ åŠ¨ä½œçŸ¥è¯†ï¼Œæ¥å¢å¼ºæœºå™¨äººçš„æ“ä½œèƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä»¥å­¦ä¹ äººç±»è¡Œä¸ºçŸ¥è¯†å¹¶é¢„æµ‹äººæ‰‹å…³é”®ç‚¹ã€‚</li>
<li>åœ¨æœºå™¨äººæ•°æ®å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡æ£€ç´¢ç›¸ä¼¼ä»»åŠ¡åŠè§‚å¯Ÿçš„äººç±»è¡Œä¸ºè§†é¢‘ï¼Œå»ºç«‹äººç±»æ‰‹å…³é”®ç‚¹å’Œæœºå™¨äººç»„ä»¶ä¹‹é—´çš„ç±»æ¯”æ¨ç†æ˜ å°„ã€‚</li>
<li>æ–¹æ³•ä¸“æ³¨äºåŠ¨ä½œå…³é”®ç‚¹ï¼Œè€Œéæ— å…³çš„è§†è§‰çº¿ç´¢ï¼Œä»¥æå‡æœºå™¨äººæ“ä½œçš„å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨CALVINåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAR-VRMè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>åœ¨å°‘æ•°æ‹æ‘„åœºæ™¯ä¸­ï¼ŒAR-VRMæ˜¾è‘—ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ•°æ®ç¨€ç¼ºæƒ…å†µä¸‹çš„é«˜æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07626">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9f749d0224f483b5590b7b39f11cab8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0bdcde10a1102ff93eedeeff1289f7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-102edc52aef97d2cebab71cde41934ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4484466e0d552d29d56193e0d2235468.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MobileViCLIP-An-Efficient-Video-Text-Model-for-Mobile-Devices"><a href="#MobileViCLIP-An-Efficient-Video-Text-Model-for-Mobile-Devices" class="headerlink" title="MobileViCLIP: An Efficient Video-Text Model for Mobile Devices"></a>MobileViCLIP: An Efficient Video-Text Model for Mobile Devices</h2><p><strong>Authors:Min Yang, Zihan Jia, Zhilin Dai, Sheng Guo, Limin Wang</strong></p>
<p>Efficient lightweight neural networks are with increasing attention due to their faster reasoning speed and easier deployment on mobile devices. However, existing video pre-trained models still focus on the common ViT architecture with high latency, and few works attempt to build efficient architecture on mobile devices. This paper bridges this gap by introducing temporal structural reparameterization into an efficient image-text model and training it on a large-scale high-quality video-text dataset, resulting in an efficient video-text model that can run on mobile devices with strong zero-shot classification and retrieval capabilities, termed as MobileViCLIP. In particular, in terms of inference speed on mobile devices, our MobileViCLIP-Small is 55.4x times faster than InternVideo2-L14 and 6.7x faster than InternVideo2-S14. In terms of zero-shot retrieval performance, our MobileViCLIP-Small obtains similar performance as InternVideo2-L14 and obtains 6.9% better than InternVideo2-S14 on MSR-VTT. The code is available at <a target="_blank" rel="noopener" href="https://github.com/MCG-NJU/MobileViCLIP">https://github.com/MCG-NJU/MobileViCLIP</a>. </p>
<blockquote>
<p>é«˜æ•ˆè½»é‡çº§ç¥ç»ç½‘ç»œå› å…¶æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„æ›´å®¹æ˜“éƒ¨ç½²è€Œå—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†é¢‘é¢„è®­ç»ƒæ¨¡å‹ä»ç„¶ä¸»è¦å…³æ³¨é«˜å»¶è¿Ÿçš„é€šç”¨ViTæ¶æ„ï¼Œå¾ˆå°‘æœ‰å·¥ä½œå°è¯•åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šæ„å»ºé«˜æ•ˆæ¶æ„ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥æ—¶é—´ç»“æ„é‡å‚æ•°åŒ–åˆ°é«˜æ•ˆçš„å›¾åƒæ–‡æœ¬æ¨¡å‹ä¸­ï¼Œå¹¶åœ¨å¤§è§„æ¨¡é«˜è´¨é‡çš„è§†é¢‘æ–‡æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»è€Œå¡«è¡¥äº†è¿™ä¸ªç©ºç™½ï¼Œå¾—åˆ°äº†ä¸€ä¸ªåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šè¿è¡Œçš„é«˜æ•ˆè§†é¢‘æ–‡æœ¬æ¨¡å‹ï¼Œå…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬åˆ†ç±»å’Œæ£€ç´¢èƒ½åŠ›ï¼Œè¢«ç§°ä¸ºMobileViCLIPã€‚ç‰¹åˆ«æ˜¯ï¼Œå°±ç§»åŠ¨è®¾å¤‡ä¸Šçš„æ¨ç†é€Ÿåº¦è€Œè¨€ï¼Œæˆ‘ä»¬çš„MobileViCLIP-Smallæ˜¯InternVideo2-L14çš„55.4å€ï¼Œæ¯”InternVideo2-S14å¿«6.7å€ã€‚åœ¨é›¶æ ·æœ¬æ£€ç´¢æ€§èƒ½æ–¹é¢ï¼Œæˆ‘ä»¬çš„MobileViCLIP-Smallçš„æ€§èƒ½ä¸InternVideo2-L14ç›¸ä¼¼ï¼Œå¹¶åœ¨MSR-VTTä¸Šæ¯”InternVideo2-S14é«˜å‡º6.9%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MCG-NJU/MobileViCLIP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/MCG-NJU/MobileViCLIPæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07312v1">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong><br>é«˜æ•ˆè½»é‡çº§ç¥ç»ç½‘ç»œå—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œå› å…¶æ¨ç†é€Ÿåº¦å¿«ï¼Œæ˜“äºåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡æ—¶é—´ç»“æ„é‡å‚æ•°åŒ–ï¼Œåœ¨é«˜æ•ˆå›¾åƒæ–‡æœ¬æ¨¡å‹ä¸Šæ„å»ºè§†é¢‘é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶åœ¨å¤§è§„æ¨¡é«˜è´¨é‡çš„è§†é¢‘æ–‡æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¾—åˆ°äº†å¯åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šè¿è¡Œçš„å…·æœ‰å¼ºå¤§é›¶æ ·æœ¬åˆ†ç±»å’Œæ£€ç´¢èƒ½åŠ›çš„è§†é¢‘æ–‡æœ¬æ¨¡å‹ï¼Œç§°ä¸ºMobileViCLIPã€‚å…¶è¿è¡Œé€Ÿåº¦éå¸¸å¿«ï¼Œä¸”é›¶æ ·æœ¬æ£€ç´¢æ€§èƒ½ä¼˜ç§€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MobileViCLIPæ˜¯ä¸€ä¸ªé«˜æ•ˆè½»é‡çº§çš„è§†é¢‘æ–‡æœ¬æ¨¡å‹ï¼Œèƒ½åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šè¿è¡Œå¹¶å…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬åˆ†ç±»å’Œæ£€ç´¢èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥æ—¶é—´ç»“æ„é‡å‚æ•°åŒ–æŠ€æœ¯ï¼Œå°†å›¾åƒæ–‡æœ¬æ¨¡å‹æ‰©å±•åˆ°è§†é¢‘é¢†åŸŸã€‚</li>
<li>MobileViCLIPåœ¨å¤§å‹é«˜è´¨é‡çš„è§†é¢‘æ–‡æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
<li>MobileViCLIPçš„è¿è¡Œé€Ÿåº¦æ˜¾è‘—å¿«äºå…¶ä»–åŒç±»æ¨¡å‹ï¼Œå¹¶ä¸”å…¶é›¶æ ·æœ¬æ£€ç´¢æ€§èƒ½ä¹Ÿååˆ†ä¼˜ç§€ã€‚</li>
<li>è¯¥æ¨¡å‹çš„ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šã€‚</li>
<li>MobileViCLIPå…·æœ‰ä¼˜ç§€çš„å¯æ‰©å±•æ€§ï¼Œé€‚ç”¨äºå„ç§åº”ç”¨åœºæ™¯å’Œèµ„æºå—é™çš„ç¯å¢ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07312">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f930c3e462d5fbcb04bf02e815792c83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45a9542d034eeb9992611e65bb053cce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c4d249f795ec902a660c011457fda89.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf1c7cbe4eab299b089399db6f694910.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78f302a0a253541cea3661bfc9b0334f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CCFQA-A-Benchmark-for-Cross-Lingual-and-Cross-Modal-Speech-and-Text-Factuality-Evaluation"><a href="#CCFQA-A-Benchmark-for-Cross-Lingual-and-Cross-Modal-Speech-and-Text-Factuality-Evaluation" class="headerlink" title="CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text   Factuality Evaluation"></a>CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text   Factuality Evaluation</h2><p><strong>Authors:Yexing Du, Kaiyuan Liu, Youcheng Pan, Zheng Chu, Bo Yang, Xiaocheng Feng, Yang Xiang, Ming Liu</strong></p>
<p>As Large Language Models (LLMs) are increasingly popularized in the multilingual world, ensuring hallucination-free factuality becomes markedly crucial. However, existing benchmarks for evaluating the reliability of Multimodal Large Language Models (MLLMs) predominantly focus on textual or visual modalities with a primary emphasis on English, which creates a gap in evaluation when processing multilingual input, especially in speech. To bridge this gap, we propose a novel \textbf{C}ross-lingual and \textbf{C}ross-modal \textbf{F}actuality benchmark (\textbf{CCFQA}). Specifically, the CCFQA benchmark contains parallel speech-text factual questions across 8 languages, designed to systematically evaluate MLLMsâ€™ cross-lingual and cross-modal factuality capabilities. Our experimental results demonstrate that current MLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, we propose a few-shot transfer learning strategy that effectively transfers the Question Answering (QA) capabilities of LLMs in English to multilingual Spoken Question Answering (SQA) tasks, achieving competitive performance with GPT-4o-mini-Audio using just 5-shot training. We release CCFQA as a foundational research resource to promote the development of MLLMs with more robust and reliable speech understanding capabilities. Our code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/yxduir/ccfqa">https://github.com/yxduir/ccfqa</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šè¯­ç§ä¸–ç•Œä¸­çš„æ—¥ç›Šæ™®åŠï¼Œç¡®ä¿æ— å¹»è§‰çš„å®¢è§‚æ€§å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¯é æ€§çš„åŸºå‡†æµ‹è¯•ä¸»è¦ä¾§é‡äºæ–‡æœ¬æˆ–è§†è§‰æ¨¡å¼ï¼Œå¹¶ä»¥è‹±è¯­ä¸ºä¸»è¦é‡ç‚¹ï¼Œè¿™åœ¨å¤„ç†å¤šè¯­ç§è¾“å…¥æ—¶ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­éŸ³æ–¹é¢ï¼Œå­˜åœ¨è¯„ä¼°å·®è·ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†å…¨æ–°çš„è·¨è¯­è¨€è·¨æ¨¡æ€äº‹å®åŸºå‡†æµ‹è¯•ï¼ˆCCFQAï¼‰ã€‚å…·ä½“è€Œè¨€ï¼ŒCCFQAåŸºå‡†æµ‹è¯•åŒ…å«8ç§è¯­è¨€çš„è¯­éŸ³æ–‡æœ¬äº‹å®é—®é¢˜ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°MLLMsçš„è·¨è¯­è¨€å’Œè·¨æ¨¡æ€äº‹å®èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰çš„MLLMsåœ¨CCFQAåŸºå‡†æµ‹è¯•ä¸Šé¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å°æ ·æœ¬è¿ç§»å­¦ä¹ ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æœ‰æ•ˆåœ°å°†è‹±è¯­å¤§å‹è¯­è¨€æ¨¡å‹çš„é—®ç­”èƒ½åŠ›è½¬ç§»åˆ°å¤šè¯­ç§å£è¯­é—®ç­”ä»»åŠ¡ä¸­ï¼Œä»…ä½¿ç”¨5ä¸ªå°æ ·æœ¬çš„è®­ç»ƒå°±è¾¾åˆ°äº†ä¸GPT-4o-mini-Audioç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬å‘å¸ƒCCFQAä½œä¸ºåŸºç¡€ç ”ç©¶èµ„æºï¼Œä»¥ä¿ƒè¿›å¼€å‘å…·æœ‰æ›´ç¨³å¥å’Œå¯é è¯­éŸ³ç†è§£èƒ½åŠ›çš„MLLMsã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yxduir/ccfqa%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/yxduir/ccfqaè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07295v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè¯­ç§ä¸–ç•Œä¸­çš„æ™®åŠï¼Œä½¿å¾—ç¡®ä¿æ— è™šæ„äº‹å®çš„å¯é æ€§å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¯„ä¼°åŸºå‡†ä¸»è¦ä¾§é‡äºæ–‡æœ¬æˆ–è§†è§‰æ¨¡æ€ï¼Œå¹¶ä»¥è‹±è¯­ä¸ºä¸»ï¼Œè¿™åœ¨å¤„ç†å¤šè¯­ç§è¾“å…¥æ—¶ï¼Œå°¤å…¶åœ¨è¯­éŸ³æ–¹é¢ï¼Œå­˜åœ¨è¯„ä¼°ç©ºç™½ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨è¯­è¨€è·¨æ¨¡æ€äº‹å®æ€§åŸºå‡†ï¼ˆCCFQAï¼‰ã€‚CCFQAåŸºå‡†åŒ…å«8ç§è¯­è¨€çš„è¯­éŸ³-æ–‡æœ¬äº‹å®é—®é¢˜ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°MLLMsçš„è·¨è¯­è¨€å’Œè·¨æ¨¡æ€äº‹å®æ€§èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾æ˜ï¼Œå½“å‰MLLMsåœ¨CCFQAåŸºå‡†ä¸Šä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„å°‘é‡è®­ç»ƒè½¬ç§»å­¦ä¹ ç­–ç•¥ï¼Œå°†è‹±è¯­é—®ç­”èƒ½åŠ›è½¬ç§»åˆ°å¤šè¯­ç§å£è¯­é—®ç­”ä»»åŠ¡ä¸Šï¼Œä½¿ç”¨ä»…5æ¬¡è®­ç»ƒçš„GPT-4o-mini-Audioå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ã€‚æˆ‘ä»¬å‘å¸ƒCCFQAä½œä¸ºç ”ç©¶èµ„æºï¼Œä»¥ä¿ƒè¿›å¼€å‘å…·æœ‰æ›´å¼ºå¤§å’Œå¯é è¯­éŸ³ç†è§£èƒ½åŠ›çš„MLLMsã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­ç§ç¯å¢ƒä¸­çš„æ™®åŠçªå‡ºäº†ç¡®ä¿äº‹å®æ— è™šæ„çš„é‡è¦æ€§ã€‚</li>
<li>ç°æœ‰çš„è¯„ä¼°åŸºå‡†åœ¨è·¨è¯­è¨€å’Œè·¨æ¨¡æ€çš„äº‹å®æ€§è¯„ä¼°ä¸Šå­˜åœ¨ä¸è¶³ã€‚</li>
<li>æå‡ºäº†è·¨è¯­è¨€è·¨æ¨¡æ€äº‹å®æ€§åŸºå‡†ï¼ˆCCFQAï¼‰ä»¥å¼¥è¡¥è¿™ä¸€ç©ºç™½ã€‚</li>
<li>CCFQAåŒ…å«8ç§è¯­è¨€çš„è¯­éŸ³-æ–‡æœ¬äº‹å®é—®é¢˜ï¼Œå…¨é¢è¯„ä¼°MLLMsçš„èƒ½åŠ›ã€‚</li>
<li>å½“å‰MLLMsåœ¨CCFQAä¸Šè¡¨ç°ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†å°‘é‡è®­ç»ƒè½¬ç§»å­¦ä¹ ç­–ç•¥ï¼Œæœ‰æ•ˆè½¬ç§»è‹±è¯­é—®ç­”èƒ½åŠ›è‡³å¤šè¯­ç§å£è¯­é—®ç­”ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07295">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-609c35804142c7f0458485fe78cf41f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63f782abaa40217a22f4c3b31c936695.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0780e00cf7eeeeaffa21d7ebc767c321.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95e8dce0bf66e6edaddfde16b597a232.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70f8a8d70d567b48cb333a7493f2be65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-888e16b85be2663085f273acf93e9a52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0209d6300d30594ab3e1c5c2958f502.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92e9aad3aadd52359c24d5725a968af8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d279fac7b237ba96116bbe74e811b02f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Prompt-Tuning-for-Few-Shot-Continual-Learning-Named-Entity-Recognition"><a href="#Prompt-Tuning-for-Few-Shot-Continual-Learning-Named-Entity-Recognition" class="headerlink" title="Prompt Tuning for Few-Shot Continual Learning Named Entity Recognition"></a>Prompt Tuning for Few-Shot Continual Learning Named Entity Recognition</h2><p><strong>Authors:Zhe Ren</strong></p>
<p>Knowledge distillation has been successfully applied to Continual Learning Named Entity Recognition (CLNER) tasks, by using a teacher model trained on old-class data to distill old-class entities present in new-class data as a form of regularization, thereby avoiding catastrophic forgetting. However, in Few-Shot CLNER (FS-CLNER) tasks, the scarcity of new-class entities makes it difficult for the trained model to generalize during inference. More critically, the lack of old-class entity information hinders the distillation of old knowledge, causing the model to fall into what we refer to as the Few-Shot Distillation Dilemma. In this work, we address the above challenges through a prompt tuning paradigm and memory demonstration template strategy. Specifically, we designed an expandable Anchor words-oriented Prompt Tuning (APT) paradigm to bridge the gap between pre-training and fine-tuning, thereby enhancing performance in few-shot scenarios. Additionally, we incorporated Memory Demonstration Templates (MDT) into each training instance to provide replay samples from previous tasks, which not only avoids the Few-Shot Distillation Dilemma but also promotes in-context learning. Experiments show that our approach achieves competitive performances on FS-CLNER. </p>
<blockquote>
<p>çŸ¥è¯†è’¸é¦å·²æˆåŠŸåº”ç”¨äºæŒç»­å­¦ä¹ å‘½åå®ä½“è¯†åˆ«ï¼ˆCLNERï¼‰ä»»åŠ¡ä¸­ï¼Œé€šè¿‡ä½¿ç”¨åœ¨æ—§ç±»æ•°æ®ä¸Šè®­ç»ƒçš„æ•™å¸ˆæ¨¡å‹æ¥æç‚¼å­˜åœ¨äºæ–°ç±»æ•°æ®ä¸­çš„æ—§ç±»å®ä½“ï¼Œä½œä¸ºä¸€ç§æ­£åˆ™åŒ–æ–¹æ³•ï¼Œä»è€Œé¿å…ç¾éš¾æ€§é—å¿˜ã€‚ç„¶è€Œï¼Œåœ¨Few-Shot CLNERï¼ˆFS-CLNERï¼‰ä»»åŠ¡ä¸­ï¼Œæ–°ç±»å®ä½“çš„ç¨€ç¼ºæ€§ä½¿å¾—è®­ç»ƒæ¨¡å‹åœ¨æ¨ç†æ—¶éš¾ä»¥æ¨å¹¿ã€‚æ›´ä¸ºå…³é”®çš„æ˜¯ï¼Œç¼ºä¹æ—§ç±»å®ä½“ä¿¡æ¯é˜»ç¢äº†æ—§çŸ¥è¯†çš„æç‚¼ï¼Œå¯¼è‡´æ¨¡å‹é™·å…¥æˆ‘ä»¬æ‰€è¯´çš„Few-Shotè’¸é¦å›°å¢ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æç¤ºè°ƒæ•´èŒƒå¼å’Œè®°å¿†æ¼”ç¤ºæ¨¡æ¿ç­–ç•¥æ¥è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¯æ‰©å±•çš„é¢å‘é”šç‚¹çš„æç¤ºè°ƒæ•´ï¼ˆAPTï¼‰èŒƒå¼ï¼Œä»¥ç¼©å°é¢„è®­ç»ƒå’Œå¾®è°ƒä¹‹é—´çš„å·®è·ï¼Œä»è€Œæé«˜å°‘æ ·æœ¬åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†è®°å¿†æ¼”ç¤ºæ¨¡æ¿ï¼ˆMDTï¼‰çº³å…¥æ¯ä¸ªè®­ç»ƒå®ä¾‹ä¸­ï¼Œæä¾›æ¥è‡ªä»¥å‰ä»»åŠ¡çš„å›æ”¾æ ·æœ¬ï¼Œè¿™ä¸ä»…é¿å…äº†Few-Shotè’¸é¦å›°å¢ƒï¼Œè€Œä¸”ä¿ƒè¿›äº†ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨FS-CLNERä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07248v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>çŸ¥è¯†è’¸é¦å·²æˆåŠŸåº”ç”¨äºæŒç»­å­¦ä¹ å‘½åå®ä½“è¯†åˆ«ï¼ˆCLNERï¼‰ä»»åŠ¡ä¸­ï¼Œåˆ©ç”¨æ—§ç±»æ•°æ®è®­ç»ƒçš„è€å¸ˆæ¨¡å‹å°†æ—§ç±»å®ä½“è’¸é¦åˆ°æ–°ç±»æ•°æ®ä¸­ï¼Œä½œä¸ºæ­£åˆ™åŒ–çš„ä¸€ç§å½¢å¼ï¼Œä»è€Œé¿å…ç¾éš¾æ€§é—å¿˜ã€‚ä½†åœ¨Few-Shot CLNERï¼ˆFS-CLNERï¼‰ä»»åŠ¡ä¸­ï¼Œæ–°ç±»å®ä½“çš„ç¨€ç¼ºæ€§ä½¿å¾—æ¨¡å‹åœ¨æ¨ç†æ—¶éš¾ä»¥æ³›åŒ–ã€‚æ›´å…³é”®çš„æ˜¯ï¼Œç¼ºä¹æ—§ç±»å®ä½“ä¿¡æ¯é˜»ç¢äº†æ—§çŸ¥è¯†çš„è’¸é¦ï¼Œå¯¼è‡´æ¨¡å‹é™·å…¥æˆ‘ä»¬ç§°ä¹‹ä¸ºçš„â€œFew-Shotè’¸é¦å›°å¢ƒâ€ã€‚æœ¬ç ”ç©¶é€šè¿‡æç¤ºè°ƒæ•´èŒƒå¼å’Œè®°å¿†æ¼”ç¤ºæ¨¡æ¿ç­–ç•¥æ¥è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¯æ‰©å±•çš„åŸºäºé”šç‚¹è¯çš„æç¤ºè°ƒæ•´ï¼ˆAPTï¼‰èŒƒå¼ï¼Œä»¥ç¼©å°é¢„è®­ç»ƒå’Œå¾®è°ƒä¹‹é—´çš„å·®è·ï¼Œä»è€Œæé«˜å°‘æ ·æœ¬åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†è®°å¿†æ¼”ç¤ºæ¨¡æ¿ï¼ˆMDTï¼‰çº³å…¥æ¯ä¸ªè®­ç»ƒå®ä¾‹ï¼Œæä¾›ä»¥å¾€ä»»åŠ¡çš„é‡ç°æ ·æœ¬ï¼Œè¿™ä¸ä»…é¿å…äº†Few-Shotè’¸é¦å›°å¢ƒï¼Œè¿˜ä¿ƒè¿›äº†ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨FS-CLNERä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†è’¸é¦å·²æˆåŠŸåº”ç”¨äºCLNERä»»åŠ¡ä¸­ï¼Œé¿å…ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>åœ¨Few-Shot CLNERï¼ˆFS-CLNERï¼‰ä»»åŠ¡ä¸­é¢ä¸´æ–°ç±»å®ä½“ç¨€ç¼ºå’Œæ—§çŸ¥è¯†è’¸é¦å›°éš¾çš„é—®é¢˜ï¼Œç§°ä¸ºâ€œFew-Shotè’¸é¦å›°å¢ƒâ€ã€‚</li>
<li>é€šè¿‡æç¤ºè°ƒæ•´èŒƒå¼ï¼ˆAPTï¼‰ç¼©å°é¢„è®­ç»ƒå’Œå¾®è°ƒä¹‹é—´çš„å·®è·ï¼Œæé«˜å°‘æ ·æœ¬åœºæ™¯æ€§èƒ½ã€‚</li>
<li>è®°å¿†æ¼”ç¤ºæ¨¡æ¿ï¼ˆMDTï¼‰çº³å…¥è®­ç»ƒå®ä¾‹ï¼Œæä¾›ä»¥å¾€ä»»åŠ¡çš„é‡ç°æ ·æœ¬ã€‚</li>
<li>MDTä¸ä»…é¿å…äº†Few-Shotè’¸é¦å›°å¢ƒï¼Œè¿˜ä¿ƒè¿›äº†ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨FS-CLNERä¸Šè¡¨ç°ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07248">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4c02beada860e3e1e3ff3855d46c7e20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76ce602ac019d101051a20df9f6cf401.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93f875572149cb47776cae0c98001af8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Enhancing-Rumor-Detection-Methods-with-Propagation-Structure-Infused-Language-Model"><a href="#Enhancing-Rumor-Detection-Methods-with-Propagation-Structure-Infused-Language-Model" class="headerlink" title="Enhancing Rumor Detection Methods with Propagation Structure Infused   Language Model"></a>Enhancing Rumor Detection Methods with Propagation Structure Infused   Language Model</h2><p><strong>Authors:Chaoqun Cui, Siyuan Li, Kunkun Ma, Caiyan Jia</strong></p>
<p>Pretrained Language Models (PLMs) have excelled in various Natural Language Processing tasks, benefiting from large-scale pretraining and self-attention mechanismâ€™s ability to capture long-range dependencies. However, their performance on social media application tasks like rumor detection remains suboptimal. We attribute this to mismatches between pretraining corpora and social texts, inadequate handling of unique social symbols, and pretraining tasks ill-suited for modeling user engagements implicit in propagation structures. To address these issues, we propose a continue pretraining strategy called Post Engagement Prediction (PEP) to infuse information from propagation structures into PLMs. PEP makes models to predict root, branch, and parent relations between posts, capturing interactions of stance and sentiment crucial for rumor detection. We also curate and release large-scale Twitter corpus: TwitterCorpus (269GB text), and two unlabeled claim conversation datasets with propagation structures (UTwitter and UWeibo). Utilizing these resources and PEP strategy, we train a Twitter-tailored PLM called SoLM. Extensive experiments demonstrate PEP significantly boosts rumor detection performance across universal and social media PLMs, even in few-shot scenarios. On benchmark datasets, PEP enhances baseline models by 1.0-3.7% accuracy, even enabling it to outperform current state-of-the-art methods on multiple datasets. SoLM alone, without high-level modules, also achieves competitive results, highlighting the strategyâ€™s effectiveness in learning discriminative post interaction features. </p>
<blockquote>
<p>é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¿™å¾—ç›Šäºå¤§è§„æ¨¡é¢„è®­ç»ƒå’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶æ•æ‰é•¿è·ç¦»ä¾èµ–çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç¤¾äº¤åª’ä½“åº”ç”¨ä»»åŠ¡ï¼ˆå¦‚è°£è¨€æ£€æµ‹ï¼‰ä¸Šçš„è¡¨ç°ä»ç„¶ä¸å¤Ÿç†æƒ³ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™æ˜¯ç”±äºé¢„è®­ç»ƒè¯­æ–™åº“ä¸ç¤¾äº¤æ–‡æœ¬ä¹‹é—´çš„ä¸åŒ¹é…ã€å¯¹ç‹¬ç‰¹ç¤¾äº¤ç¬¦å·å¤„ç†ä¸è¶³ä»¥åŠé¢„è®­ç»ƒä»»åŠ¡ä¸é€‚åˆå»ºæ¨¡ä¼ æ’­ç»“æ„ä¸­éšå«çš„ç”¨æˆ·äº¤äº’é€ æˆçš„ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºâ€œå‚ä¸åé¢„æµ‹â€ï¼ˆPEPï¼‰çš„æŒç»­é¢„è®­ç»ƒç­–ç•¥ï¼Œä»¥å°†ä¼ æ’­ç»“æ„çš„ä¿¡æ¯èå…¥PLMsã€‚PEPä½¿æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹å¸–å­ä¹‹é—´çš„æ ¹ã€åˆ†æ”¯å’Œçˆ¶å­å…³ç³»ï¼Œæ•è·å¯¹è°£è¨€æ£€æµ‹è‡³å…³é‡è¦çš„ç«‹åœºå’Œæƒ…æ„Ÿçš„äº¤äº’ã€‚æˆ‘ä»¬è¿˜æ•´ç†å’Œå‘å¸ƒäº†å¤§è§„æ¨¡çš„Twitterè¯­æ–™åº“ï¼ˆTwitterCorpusï¼ŒåŒ…å«269GBæ–‡æœ¬ï¼‰ï¼Œä»¥åŠå¸¦æœ‰ä¼ æ’­ç»“æ„çš„ä¸¤ä¸ªæœªæ ‡è®°å£°æ˜å¯¹è¯æ•°æ®é›†ï¼ˆUTwitterå’ŒUWeiboï¼‰ã€‚åˆ©ç”¨è¿™äº›èµ„æºå’ŒPEPç­–ç•¥ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªé€‚ç”¨äºTwitterçš„PLMï¼Œç§°ä¸ºSoLMã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹ï¼ŒPEPä¹Ÿèƒ½æ˜¾è‘—æé«˜é€šç”¨å’Œç¤¾äº¤åª’ä½“PLMsçš„è°£è¨€æ£€æµ‹æ€§èƒ½ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šï¼ŒPEPå°†åŸºçº¿æ¨¡å‹çš„å‡†ç¡®ç‡æé«˜1.0-3.7%ï¼Œç”šè‡³èƒ½å¤Ÿåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä»…ä½¿ç”¨SoLMï¼Œæ— éœ€é«˜çº§æ¨¡å—ï¼Œä¹Ÿèƒ½å–å¾—æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œè¿™å‡¸æ˜¾äº†è¯¥ç­–ç•¥åœ¨å­¦ä¹ åŒºåˆ†æ€§å¸–å­äº¤äº’ç‰¹å¾æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07209v1">PDF</a> This paper is accepted by COLING2025</p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç¤¾äº¤åª’ä½“åº”ç”¨ä»»åŠ¡å¦‚è°£è¨€æ£€æµ‹æ–¹é¢çš„è¡¨ç°å°šå¾…æå‡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åä¸ºâ€œPost Engagement Predictionâ€ï¼ˆPEPï¼‰çš„æŒç»­é¢„è®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨å°†ä¼ æ’­ç»“æ„ä¿¡æ¯èå…¥é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚PEPä½¿æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹å¸–å­ä¹‹é—´çš„æ ¹ã€åˆ†æ”¯å’Œçˆ¶å…³ç³»ï¼Œä»è€Œæ•æ‰å¯¹è°£è¨€æ£€æµ‹è‡³å…³é‡è¦çš„ç«‹åœºå’Œæƒ…æ„Ÿçš„äº¤äº’ä½œç”¨ã€‚å®éªŒè¡¨æ˜ï¼ŒPEPç­–ç•¥åœ¨é€šç”¨å’Œç¤¾äº¤åª’ä½“é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸Šå‡æ˜¾è‘—æå‡äº†è°£è¨€æ£€æµ‹æ€§èƒ½ï¼Œå³ä½¿åœ¨å°‘é‡æ ·æœ¬åœºæ™¯ä¸‹ä¹Ÿè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨ç¤¾äº¤åª’ä½“åº”ç”¨ä»»åŠ¡å¦‚è°£è¨€æ£€æµ‹ä¸Šçš„è¡¨ç°æœ‰å¾…æé«˜ã€‚</li>
<li>ç°æœ‰æ¨¡å‹é¢ä¸´é¢„è®­ç»ƒè¯­æ–™ä¸ç¤¾äº¤æ–‡æœ¬ä¸åŒ¹é…ã€æ— æ³•å¦¥å–„å¤„ç†ç‹¬ç‰¹ç¤¾äº¤ç¬¦å·ä»¥åŠé¢„è®­ç»ƒä»»åŠ¡ä¸é€‚åˆå»ºæ¨¡ç”¨æˆ·å‚ä¸åº¦ç­‰é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§åä¸ºPEPçš„æŒç»­é¢„è®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨å°†ä¼ æ’­ç»“æ„ä¿¡æ¯èå…¥é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œä»¥æ”¹å–„è°£è¨€æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>PEPç­–ç•¥ä½¿æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹å¸–å­é—´çš„æ ¹ã€åˆ†æ”¯å’Œçˆ¶å…³ç³»ï¼Œæ•æ‰å…³é”®äº¤äº’ä½œç”¨ã€‚</li>
<li>åˆ©ç”¨Twitterçš„å¤§å‹è¯­æ–™åº“å’Œæœªæ ‡è®°å£°æ˜å¯¹è¯æ•°æ®é›†è¿›è¡Œå®è¯ç ”ç©¶ï¼Œè¯æ˜PEPç­–ç•¥åœ¨æå‡è°£è¨€æ£€æµ‹æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>PEPç­–ç•¥åœ¨å¤šç§åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå‡†ç¡®ç‡æå‡èŒƒå›´åœ¨1.0%~3.7%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07209">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-904af40f7a07d9e9daea491b50d4b7c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b789d00e6bd7c5d4407cf9d9b4ce4ce0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fc7a5e87f388c047a0110f4f12f658d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb68dd898c584858dfd9cde6c5f4937d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-694fd16ce904b0f83f9ff8b6f6e9e56b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Towards-Real-World-Rumor-Detection-Anomaly-Detection-Framework-with-Graph-Supervised-Contrastive-Learning"><a href="#Towards-Real-World-Rumor-Detection-Anomaly-Detection-Framework-with-Graph-Supervised-Contrastive-Learning" class="headerlink" title="Towards Real-World Rumor Detection: Anomaly Detection Framework with   Graph Supervised Contrastive Learning"></a>Towards Real-World Rumor Detection: Anomaly Detection Framework with   Graph Supervised Contrastive Learning</h2><p><strong>Authors:Chaoqun Cui, Caiyan Jia</strong></p>
<p>Current rumor detection methods based on propagation structure learning predominately treat rumor detection as a class-balanced classification task on limited labeled data. However, real-world social media data exhibits an imbalanced distribution with a minority of rumors among massive regular posts. To address the data scarcity and imbalance issues, we construct two large-scale conversation datasets from Weibo and Twitter and analyze the domain distributions. We find obvious differences between rumor and non-rumor distributions, with non-rumors mostly in entertainment domains while rumors concentrate in news, indicating the conformity of rumor detection to an anomaly detection paradigm. Correspondingly, we propose the Anomaly Detection framework with Graph Supervised Contrastive Learning (AD-GSCL). It heuristically treats unlabeled data as non-rumors and adapts graph contrastive learning for rumor detection. Extensive experiments demonstrate AD-GSCLâ€™s superiority under class-balanced, imbalanced, and few-shot conditions. Our findings provide valuable insights for real-world rumor detection featuring imbalanced data distributions. </p>
<blockquote>
<p>å½“å‰åŸºäºä¼ æ’­ç»“æ„å­¦ä¹ çš„è°£è¨€æ£€æµ‹æ–¹æ³•ä¸»è¦æ˜¯å°†è°£è¨€æ£€æµ‹è§†ä¸ºåœ¨æœ‰é™æ ‡è®°æ•°æ®ä¸Šçš„ç±»å¹³è¡¡åˆ†ç±»ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°å®ç¤¾äº¤åª’ä½“çš„æ•°æ®å‘ˆç°å‡ºä¸å¹³è¡¡çš„åˆ†å¸ƒï¼Œå¤§é‡çš„å¸¸è§„å¸–å­ä¸­åªæœ‰ä¸€å°éƒ¨åˆ†æ˜¯è°£è¨€ã€‚ä¸ºäº†è§£å†³æ•°æ®ç¨€ç¼ºå’Œä¸å¹³è¡¡é—®é¢˜ï¼Œæˆ‘ä»¬ä»å¾®åšå’Œæ¨ç‰¹æ„å»ºäº†ä¸¤ä¸ªå¤§è§„æ¨¡å¯¹è¯æ•°æ®é›†å¹¶å¯¹é¢†åŸŸåˆ†å¸ƒè¿›è¡Œäº†åˆ†æã€‚æˆ‘ä»¬å‘ç°è°£è¨€å’Œéè°£è¨€åˆ†å¸ƒä¹‹é—´å­˜åœ¨æ˜æ˜¾å·®å¼‚ï¼Œéè°£è¨€ä¸»è¦å­˜åœ¨äºå¨±ä¹é¢†åŸŸï¼Œè€Œè°£è¨€åˆ™é›†ä¸­åœ¨æ–°é—»é¢†åŸŸï¼Œè¿™è¡¨æ˜è°£è¨€æ£€æµ‹ç¬¦åˆå¼‚å¸¸æ£€æµ‹çš„æ¨¡å¼ã€‚ç›¸åº”åœ°ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå›¾ç›‘ç£å¯¹æ¯”å­¦ä¹ çš„å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼ˆAD-GSCLï¼‰ã€‚å®ƒå¯å‘å¼åœ°å°†æœªæ ‡è®°çš„æ•°æ®è§†ä¸ºéè°£è¨€ï¼Œå¹¶é€‚åº”å›¾å¯¹æ¯”å­¦ä¹ è¿›è¡Œè°£è¨€æ£€æµ‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAD-GSCLåœ¨ç±»å¹³è¡¡ã€ä¸å¹³è¡¡å’Œå°‘é•œå¤´æ¡ä»¶ä¸‹å‡è¡¨ç°ä¼˜è¶Šã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå…·æœ‰æ•°æ®åˆ†å¸ƒä¸å¹³è¡¡ç‰¹å¾çš„å®æ—¶è°£è¨€æ£€æµ‹æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07205v1">PDF</a> This paper is accepted by COLING2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºä¼ æ’­ç»“æ„å­¦ä¹ çš„å½“å‰è°£è¨€æ£€æµ‹æ–¹æ³•çš„å±€é™æ€§ï¼Œè¿™äº›æ–¹æ³•ä¸»è¦ä½œä¸ºç±»å¹³è¡¡åˆ†ç±»ä»»åŠ¡åœ¨æœ‰é™æ ‡è®°æ•°æ®ä¸Šè¿›è¡Œã€‚é’ˆå¯¹ç¤¾äº¤åª’ä½“æ•°æ®çš„ä¸å¹³è¡¡åˆ†å¸ƒå’Œç¼ºä¹æ•°æ®çš„é—®é¢˜ï¼Œç ”ç©¶è€…åœ¨å¾®åšå’Œæ¨ç‰¹ä¸Šæ„å»ºäº†ä¸¤ä¸ªå¤§è§„æ¨¡å¯¹è¯æ•°æ®é›†ï¼Œå¹¶åˆ†æäº†é¢†åŸŸåˆ†å¸ƒã€‚ç ”ç©¶å‘ç°è°£è¨€å’Œéè°£è¨€çš„åˆ†å¸ƒå­˜åœ¨æ˜æ˜¾å·®å¼‚ï¼Œéè°£è¨€ä¸»è¦é›†ä¸­åœ¨å¨±ä¹é¢†åŸŸï¼Œè€Œè°£è¨€åˆ™é›†ä¸­åœ¨æ–°é—»é¢†åŸŸï¼Œè¿™è¡¨æ˜è°£è¨€æ£€æµ‹ç¬¦åˆå¼‚å¸¸æ£€æµ‹æ¨¡å¼ã€‚æ®æ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†åŸºäºå›¾ç›‘ç£å¯¹æ¯”å­¦ä¹ çš„å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼ˆAD-GSCLï¼‰ï¼Œè¯¥æ¡†æ¶å¯å‘å¼åœ°å°†æœªæ ‡è®°æ•°æ®è§†ä¸ºéè°£è¨€ï¼Œå¹¶é‡‡ç”¨å›¾å¯¹æ¯”å­¦ä¹ è¿›è¡Œè°£è¨€æ£€æµ‹ã€‚å®éªŒè¡¨æ˜ï¼ŒAD-GSCLåœ¨ç±»å¹³è¡¡ã€ä¸å¹³è¡¡å’Œå°‘æ ·æœ¬æ¡ä»¶ä¸‹å‡è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è°£è¨€æ£€æµ‹ä¸»è¦åŸºäºæœ‰é™çš„æ ‡è®°æ•°æ®ä¸”ä»¥ç±»å¹³è¡¡åˆ†ç±»ä»»åŠ¡ä¸ºä¸»ï¼Œä½†ç¤¾äº¤åª’ä½“æ•°æ®å­˜åœ¨ä¸å¹³è¡¡åˆ†å¸ƒé—®é¢˜ã€‚</li>
<li>ç ”ç©¶è€…é€šè¿‡æ„å»ºå¤§è§„æ¨¡å¯¹è¯æ•°æ®é›†å‘ç°ï¼Œéè°£è¨€ä¸»è¦é›†ä¸­åœ¨å¨±ä¹é¢†åŸŸï¼Œè€Œè°£è¨€åˆ™é›†ä¸­åœ¨æ–°é—»é¢†åŸŸã€‚</li>
<li>è°£è¨€æ£€æµ‹ç¬¦åˆå¼‚å¸¸æ£€æµ‹æ¨¡å¼ã€‚</li>
<li>æå‡ºçš„AD-GSCLæ¡†æ¶å¯å‘å¼åœ°å°†æœªæ ‡è®°æ•°æ®è§†ä¸ºéè°£è¨€ï¼Œå¹¶é‡‡ç”¨å›¾å¯¹æ¯”å­¦ä¹ è¿›è¡Œè°£è¨€æ£€æµ‹ã€‚</li>
<li>AD-GSCLåœ¨å¤šç§æ¡ä»¶ä¸‹è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼ŒåŒ…æ‹¬ç±»å¹³è¡¡ã€ä¸å¹³è¡¡å’Œå°‘æ ·æœ¬æ¡ä»¶ã€‚</li>
<li>ç ”ç©¶ç»“æœæä¾›äº†å¯¹çœŸå®ä¸–ç•Œè°£è¨€æ£€æµ‹ä¸­ä¸å¹³è¡¡æ•°æ®åˆ†å¸ƒé—®é¢˜çš„æœ‰ä»·å€¼è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07205">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aacf4e773eaa7ad864e3d32c9a829bb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4dfa6877e5643267c14b98182cc6f7cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d38912489ae3c7704b763e1fdb8d0bf8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9bdb04c8832db63205248994aef74626.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-14c3832be6a88c7926f2ed5e44722d76.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Two-Stage-Quranic-QA-via-Ensemble-Retrieval-and-Instruction-Tuned-Answer-Extraction"><a href="#Two-Stage-Quranic-QA-via-Ensemble-Retrieval-and-Instruction-Tuned-Answer-Extraction" class="headerlink" title="Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer   Extraction"></a>Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer   Extraction</h2><p><strong>Authors:Mohamed Basem, Islam Oshallah, Ali Hamdi, Khaled Shaban, Hozaifa Kassab</strong></p>
<p>Quranic Question Answering presents unique challenges due to the linguistic complexity of Classical Arabic and the semantic richness of religious texts. In this paper, we propose a novel two-stage framework that addresses both passage retrieval and answer extraction. For passage retrieval, we ensemble fine-tuned Arabic language models to achieve superior ranking performance. For answer extraction, we employ instruction-tuned large language models with few-shot prompting to overcome the limitations of fine-tuning on small datasets. Our approach achieves state-of-the-art results on the Quran QA 2023 Shared Task, with a MAP@10 of 0.3128 and MRR@10 of 0.5763 for retrieval, and a pAP@10 of 0.669 for extraction, substantially outperforming previous methods. These results demonstrate that combining model ensembling and instruction-tuned language models effectively addresses the challenges of low-resource question answering in specialized domains. </p>
<blockquote>
<p>ç”±äºå¤å…¸é˜¿æ‹‰ä¼¯è¯­çš„è¯­è¨€å¤æ‚æ€§å’Œå®—æ•™æ–‡æœ¬ä¸°å¯Œçš„è¯­ä¹‰å†…æ¶µï¼Œä¼Šæ–¯å…°é—®ç­”å‘ˆç°å‡ºç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒæ—¶è§£å†³äº†æ®µè½æ£€ç´¢å’Œç­”æ¡ˆæå–ã€‚å¯¹äºæ®µè½æ£€ç´¢ï¼Œæˆ‘ä»¬å°†å¾®è°ƒé˜¿æ‹‰ä¼¯è¯­è¯­è¨€æ¨¡å‹è¿›è¡Œé›†æˆï¼Œä»¥å®ç°å‡ºè‰²çš„æ’åæ€§èƒ½ã€‚å¯¹äºç­”æ¡ˆæå–ï¼Œæˆ‘ä»¬é‡‡ç”¨æŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å°‘é‡æç¤ºæ¥å…‹æœåœ¨å°æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒæ—¶çš„å±€é™æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¼Šæ–¯å…°é—®ç­”2023å…±äº«ä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°ç»“æœï¼Œæ£€ç´¢çš„MAP@10ä¸º0.3128ï¼ŒMRR@10ä¸º0.5763ï¼Œæå–çš„pAP@10ä¸º0.669ï¼Œå¤§å¹…ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚è¿™äº›ç»“æœè¯æ˜äº†ç»“åˆæ¨¡å‹é›†æˆå’ŒæŒ‡ä»¤å¾®è°ƒçš„è¯­è¨€æ¨¡å‹æœ‰æ•ˆåœ°è§£å†³äº†ç‰¹å®šé¢†åŸŸä½èµ„æºé—®ç­”çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06971v1">PDF</a> 8 pages , 4 figures , Accepted in Aiccsa 2025 ,   <a target="_blank" rel="noopener" href="https://conferences.sigappfr.org/aiccsa2025/">https://conferences.sigappfr.org/aiccsa2025/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºè§£å†³åŸºäºå¤å…°ç»çš„é—®ç­”æŒ‘æˆ˜ã€‚é¦–å…ˆåˆ©ç”¨å¾®è°ƒé˜¿æ‹‰ä¼¯è¯­è¯­è¨€æ¨¡å‹è¿›è¡Œæ®µè½æ£€ç´¢ï¼Œå®ç°ä¼˜è´¨æ’åæ€§èƒ½ï¼›ç„¶åé‡‡ç”¨æŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç­”æ¡ˆæå–ï¼Œé€šè¿‡å°‘æ ·æœ¬æç¤ºå…‹æœå°æ•°æ®é›†ä¸Šçš„å¾®è°ƒé™åˆ¶ã€‚è¯¥æ–¹æ³•åœ¨å¤å…°ç»é—®ç­”å…±äº«ä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æˆæœï¼Œå®ç°äº†æ£€ç´¢é˜¶æ®µçš„MAP@10ä¸º0.3128å’ŒMRR@10ä¸º0.5763çš„é«˜ç²¾åº¦æ’åä»¥åŠç­”æ¡ˆæå–é˜¶æ®µçš„pAP@10ä¸º0.669çš„å‡ºè‰²è¡¨ç°ï¼Œæ˜¾è‘—ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œç»“åˆæ¨¡å‹é›†æˆå’ŒæŒ‡ä»¤å¾®è°ƒçš„è¯­è¨€æ¨¡å‹å¯ä»¥æœ‰æ•ˆè§£å†³ä½èµ„æºé¢†åŸŸçš„é—®ç­”æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢ä¸´æŒ‘æˆ˜ï¼šå¤å…°ç»é—®ç­”é¢ä¸´è¯­è¨€å¤æ‚æ€§å’Œå®—æ•™æ–‡æœ¬è¯­ä¹‰ä¸°å¯Œæ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>æ–¹æ³•åˆ›æ–°ï¼šæå‡ºä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼ŒåŒ…æ‹¬æ®µè½æ£€ç´¢å’Œç­”æ¡ˆæå–ã€‚</li>
<li>æ®µè½æ£€ç´¢ï¼šä½¿ç”¨å¾®è°ƒé˜¿æ‹‰ä¼¯è¯­è¯­è¨€æ¨¡å‹å®ç°ä¼˜è´¨æ’åæ€§èƒ½ã€‚</li>
<li>ç­”æ¡ˆæå–ï¼šé‡‡ç”¨æŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå°‘æ ·æœ¬å­¦ä¹ ä»¥å®ç°é«˜æ€§èƒ½æå–ç­”æ¡ˆã€‚</li>
<li>å–å¾—æˆæœï¼šåœ¨å¤å…°ç»é—®ç­”å…±äº«ä»»åŠ¡ä¸Šå–å¾—æœ€æ–°æˆæœï¼ŒåŒ…æ‹¬MAP@10ã€MRR@10å’ŒpAP@10ç­‰å¤šä¸ªç»´åº¦çš„æå‡ã€‚</li>
<li>è¶…è¶Šå‰æ³•ï¼šç›¸æ¯”ä»¥å‰çš„æ–¹æ³•ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†ä½èµ„æºé¢†åŸŸçš„é—®ç­”æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06971">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d5057cb8669579c4091002b84f57df23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f174a703aa606e5fb90aa5d12d7e0e90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-694babdddc054092041f7205cf54e789.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebcf539e975d67e61820fab04c526ff7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-for-Oral-History-Understanding-with-Text-Classification-and-Sentiment-Analysis"><a href="#Large-Language-Models-for-Oral-History-Understanding-with-Text-Classification-and-Sentiment-Analysis" class="headerlink" title="Large Language Models for Oral History Understanding with Text   Classification and Sentiment Analysis"></a>Large Language Models for Oral History Understanding with Text   Classification and Sentiment Analysis</h2><p><strong>Authors:Komala Subramanyam Cherukuri, Pranav Abishai Moses, Aisa Sakata, Jiangping Chen, Haihua Chen</strong></p>
<p>Oral histories are vital records of lived experience, particularly within communities affected by systemic injustice and historical erasure. Effective and efficient analysis of their oral history archives can promote access and understanding of the oral histories. However, Large-scale analysis of these archives remains limited due to their unstructured format, emotional complexity, and high annotation costs. This paper presents a scalable framework to automate semantic and sentiment annotation for Japanese American Incarceration Oral History. Using LLMs, we construct a high-quality dataset, evaluate multiple models, and test prompt engineering strategies in historically sensitive contexts. Our multiphase approach combines expert annotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We labeled 558 sentences from 15 narrators for sentiment and semantic classification, then evaluated zero-shot, few-shot, and RAG strategies. For semantic classification, ChatGPT achieved the highest F1 score (88.71%), followed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama slightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models showing comparable results. The best prompt configurations were used to annotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our findings show that LLMs can effectively perform semantic and sentiment annotation across large oral history collections when guided by well-designed prompts. This study provides a reusable annotation pipeline and practical guidance for applying LLMs in culturally sensitive archival analysis. By bridging archival ethics with scalable NLP techniques, this work lays the groundwork for responsible use of artificial intelligence in digital humanities and preservation of collective memory. GitHub: <a target="_blank" rel="noopener" href="https://github.com/kc6699c/LLM4OralHistoryAnalysis">https://github.com/kc6699c/LLM4OralHistoryAnalysis</a>. </p>
<blockquote>
<p>å£è¿°å†å²æ˜¯ç”Ÿæ´»ç»å†çš„å®è´µè®°å½•ï¼Œç‰¹åˆ«æ˜¯åœ¨å—åˆ°ç³»ç»Ÿæ€§ä¸å…¬æ­£å’Œå†å²æŠ¹é»‘å½±å“çš„ç¤¾åŒºä¸­å°¤ä¸ºé‡è¦ã€‚å¯¹å…¶å£è¿°å†å²æ¡£æ¡ˆçš„æœ‰æ•ˆå’Œé«˜æ•ˆåˆ†æï¼Œå¯ä»¥ä¿ƒè¿›å¯¹å£è¿°å†å²çš„è®¿é—®å’Œç†è§£ã€‚ç„¶è€Œï¼Œç”±äºå£è¿°å†å²æ¡£æ¡ˆçš„éç»“æ„åŒ–æ ¼å¼ã€æƒ…æ„Ÿå¤æ‚æ€§å’Œé«˜æ ‡æ³¨æˆæœ¬ï¼Œè¿™äº›æ¡£æ¡ˆçš„å¤§è§„æ¨¡åˆ†æä»ç„¶å—åˆ°é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨è¿›è¡Œæ—¥æœ¬è£”ç¾å›½äººç›‘ç¦å£è¿°å†å²çš„è¯­ä¹‰å’Œæƒ…ç»ªæ³¨é‡Šã€‚æˆ‘ä»¬ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„æ•°æ®é›†ï¼Œå¯¹å¤šä¸ªæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶åœ¨å†å²æ•æ„Ÿçš„ä¸Šä¸‹æ–‡ä¸­æµ‹è¯•äº†æç¤ºå·¥ç¨‹ç­–ç•¥ã€‚æˆ‘ä»¬çš„å¤šé˜¶æ®µæ–¹æ³•ç»“åˆäº†ä¸“å®¶æ³¨é‡Šã€æç¤ºè®¾è®¡ä»¥åŠä½¿ç”¨ChatGPTã€Llamaå’ŒQwenå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ã€‚æˆ‘ä»¬å¯¹æ¥è‡ª15ä¸ªå™è¿°è€…çš„558ä¸ªå¥å­è¿›è¡Œäº†æƒ…æ„Ÿå’Œè¯­ä¹‰åˆ†ç±»æ ‡æ³¨ï¼Œç„¶åè¯„ä¼°äº†é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’ŒRAGç­–ç•¥ã€‚å¯¹äºè¯­ä¹‰åˆ†ç±»ï¼ŒChatGPTçš„F1åˆ†æ•°æœ€é«˜ï¼ˆ88.71%ï¼‰ï¼Œå…¶æ¬¡æ˜¯Llamaï¼ˆ84.99%ï¼‰å’ŒQwenï¼ˆ83.72%ï¼‰ã€‚åœ¨æƒ…æ„Ÿåˆ†æä¸­ï¼ŒLlamaç•¥ä¼˜äºQwenï¼ˆ82.66%ï¼‰å’ŒChatGPTï¼ˆ82.29%ï¼‰ï¼Œæ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½å‡ç›¸å½“ã€‚ä½¿ç”¨æœ€ä½³çš„æç¤ºé…ç½®æ¥æ³¨é‡ŠJAIOHæ”¶è—ä¸­çš„92,191ä¸ªå¥å­å’Œæ¥è‡ª1,002æ¬¡è®¿è°ˆçš„æ•°æ®ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨å¼•å¯¼è‰¯å¥½çš„æç¤ºä¸‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°åœ¨å¤§å‹å£è¿°å†å²é›†åˆä¸­è¿›è¡Œè¯­ä¹‰å’Œæƒ…ç»ªæ³¨é‡Šã€‚æœ¬ç ”ç©¶æä¾›äº†ä¸€ä¸ªå¯é‡å¤ä½¿ç”¨çš„æ³¨é‡Šç®¡é“ï¼Œä¸ºåœ¨æ•æ„Ÿæ–‡åŒ–æ¡£æ¡ˆåˆ†æä¸­åº”ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†å®é™…æŒ‡å¯¼ã€‚é€šè¿‡æ¶èµ·æ¡£æ¡ˆä¼¦ç†ä¸å¯æ‰©å±•çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ä¹‹é—´çš„æ¡¥æ¢ï¼Œè¿™é¡¹å·¥ä½œä¸ºæ•°å­—äººæ–‡ä¸­äººå·¥æ™ºèƒ½çš„è´Ÿè´£ä»»ä½¿ç”¨å’Œé›†ä½“è®°å¿†çš„ä¿å­˜å¥ å®šäº†åŸºç¡€ã€‚GitHubåœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/kc6699c/LLM4OralHistoryAnalysis">https://github.com/kc6699c/LLM4OralHistoryAnalysis</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06729v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡å…³æ³¨å£è¿°å†å²æ¡£æ¡ˆçš„åˆ†æï¼Œç‰¹åˆ«æ˜¯å—åˆ°ç³»ç»Ÿæ€§ä¸å…¬æ­£å’Œå†åæ€§æŠ¹æ€å½±å“çš„ç¤¾ç¾¤ä¸­çš„å£è¿°å†å²ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨è¿›è¡Œæ—¥æœ¬è£”ç¾å›½äººå…³æŠ¼å£è¿°å†å²çš„è¯­ä¹‰å’Œæƒ…æ„Ÿæ³¨é‡Šã€‚ç ”ç©¶æ„å»ºäº†é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œè¯„ä¼°äº†å¤šä¸ªæ¨¡å‹ï¼Œå¹¶åœ¨å†å²æ•æ„Ÿè¯­å¢ƒä¸­æµ‹è¯•äº†æç¤ºå·¥ç¨‹ç­–ç•¥ã€‚ç»“æœæ˜¾ç¤ºï¼ŒChatGPTåœ¨è¯­ä¹‰åˆ†ç±»ä¸Šçš„F1åˆ†æ•°æœ€é«˜ï¼ˆ88.71%ï¼‰ï¼ŒLlamaï¼ˆ84.99%ï¼‰å’ŒQwenï¼ˆ83.72%ï¼‰è¡¨ç°ç•¥é€Šã€‚åœ¨æƒ…æ„Ÿåˆ†æä¸­ï¼ŒLlamaç•¥èƒœä¸€ç­¹ï¼ˆ82.66%ï¼‰ï¼Œè€ŒChatGPTï¼ˆ82.29%ï¼‰ä¸å…¶ä»–æ¨¡å‹ç»“æœç›¸å½“ã€‚ä½¿ç”¨æœ€ä½³çš„æç¤ºé…ç½®ï¼ŒæˆåŠŸæ ‡æ³¨äº†JAIOHæ”¶è—ä¸­çš„92ï¼Œ191å¥å’Œ1ï¼Œ002æ¬¡è®¿è°ˆã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ç²¾å¿ƒè®¾è®¡æç¤ºçš„æŒ‡å¯¼ä¸‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥åœ¨å¤§è§„æ¨¡å£è¿°å†å²é›†åˆä¸­æœ‰æ•ˆæ‰§è¡Œè¯­ä¹‰å’Œæƒ…æ„Ÿæ³¨é‡Šã€‚æœ¬ç ”ç©¶ä¸ºåº”ç”¨å¤§å‹è¯­è¨€æ¨¡å‹äºæ–‡åŒ–æ•æ„Ÿæ¡£æ¡ˆåˆ†ææä¾›äº†å¯é‡å¤ä½¿ç”¨çš„æ ‡æ³¨ç®¡é“å’Œå®ç”¨æŒ‡å¯¼ï¼Œä¸ºäººå·¥æ™ºèƒ½åœ¨æ•°å­—äººæ–‡å’Œé›†ä½“è®°å¿†ä¿æŠ¤ä¸­çš„è´Ÿè´£ä»»ä½¿ç”¨å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å£è¿°å†å²æ˜¯è®°å½•ç”Ÿæ´»ç»éªŒçš„é‡è¦èµ„æ–™ï¼Œç‰¹åˆ«æ˜¯åœ¨å—åˆ°ç³»ç»Ÿæ€§ä¸å…¬æ­£å’Œå†åæ€§æŠ¹æ€çš„ç¤¾ç¾¤ä¸­ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å£è¿°å†å²æ¡£æ¡ˆçš„æƒ…æ„Ÿå’Œè¯­ä¹‰è‡ªåŠ¨æ³¨é‡Šä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
<li>æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œç”¨äºæ—¥æœ¬è£”ç¾å›½äººå…³æŠ¼å£è¿°å†å²çš„è¯­ä¹‰å’Œæƒ…æ„Ÿæ³¨é‡Šã€‚</li>
<li>ChatGPTåœ¨è¯­ä¹‰åˆ†ç±»ä¸Šè¡¨ç°æœ€ä½³ï¼Œè€ŒLlamaåœ¨æƒ…æ„Ÿåˆ†æä¸­ç¨å¾®å ä¼˜ã€‚</li>
<li>åˆ©ç”¨æœ€ä½³æç¤ºé…ç½®ï¼ŒæˆåŠŸæ ‡æ³¨äº†å¤§é‡å£è¿°å†å²å¥å­å’Œè®¿è°ˆã€‚</li>
<li>ç ”ç©¶ç»“æœå¼ºè°ƒäº†åœ¨æ–‡åŒ–æ•æ„Ÿæ¡£æ¡ˆåˆ†æä¸­è´Ÿè´£ä»»åœ°ä½¿ç”¨äººå·¥æ™ºèƒ½çš„é‡è¦æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­—äººæ–‡å’Œé›†ä½“è®°å¿†ä¿æŠ¤ä¸­çš„åº”ç”¨æä¾›äº†å®ç”¨æŒ‡å¯¼å’Œå¯é‡å¤ä½¿ç”¨çš„æ ‡æ³¨ç®¡é“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fcc5176670d5f7779dd51e8ed9e507fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3e398f6f93bcee6a8b0481bab4f02b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad342342c4d5f43a495cc5445f33b7c9.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="UltraAD-Fine-Grained-Ultrasound-Anomaly-Classification-via-Few-Shot-CLIP-Adaptation"><a href="#UltraAD-Fine-Grained-Ultrasound-Anomaly-Classification-via-Few-Shot-CLIP-Adaptation" class="headerlink" title="UltraAD: Fine-Grained Ultrasound Anomaly Classification via Few-Shot   CLIP Adaptation"></a>UltraAD: Fine-Grained Ultrasound Anomaly Classification via Few-Shot   CLIP Adaptation</h2><p><strong>Authors:Yue Zhou, Yuan Bi, Wenjuan Tong, Wei Wang, Nassir Navab, Zhongliang Jiang</strong></p>
<p>Precise anomaly detection in medical images is critical for clinical decision-making. While recent unsupervised or semi-supervised anomaly detection methods trained on large-scale normal data show promising results, they lack fine-grained differentiation, such as benign vs. malignant tumors. Additionally, ultrasound (US) imaging is highly sensitive to devices and acquisition parameter variations, creating significant domain gaps in the resulting US images. To address these challenges, we propose UltraAD, a vision-language model (VLM)-based approach that leverages few-shot US examples for generalized anomaly localization and fine-grained classification. To enhance localization performance, the image-level token of query visual prototypes is first fused with learnable text embeddings. This image-informed prompt feature is then further integrated with patch-level tokens, refining local representations for improved accuracy. For fine-grained classification, a memory bank is constructed from few-shot image samples and corresponding text descriptions that capture anatomical and abnormality-specific features. During training, the stored text embeddings remain frozen, while image features are adapted to better align with medical data. UltraAD has been extensively evaluated on three breast US datasets, outperforming state-of-the-art methods in both lesion localization and fine-grained medical classification. The code will be released upon acceptance. </p>
<blockquote>
<p>åŒ»ç–—å›¾åƒä¸­çš„ç²¾ç¡®å¼‚å¸¸æ£€æµ‹å¯¹äºä¸´åºŠå†³ç­–è‡³å…³é‡è¦ã€‚è™½ç„¶æœ€è¿‘åŸºäºå¤§è§„æ¨¡æ­£å¸¸æ•°æ®çš„æ— ç›‘ç£æˆ–åŠç›‘ç£å¼‚å¸¸æ£€æµ‹æ–¹æ³•æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†å®ƒä»¬ç¼ºä¹ç²¾ç»†çš„åŒºåˆ†ï¼Œä¾‹å¦‚è‰¯æ€§ä¸æ¶æ€§è‚¿ç˜¤ã€‚æ­¤å¤–ï¼Œè¶…å£°ï¼ˆUSï¼‰æˆåƒå¯¹è®¾å¤‡å’Œé‡‡é›†å‚æ•°çš„å˜åŒ–é«˜åº¦æ•æ„Ÿï¼Œå¯¼è‡´è¶…å£°å›¾åƒä¸­å­˜åœ¨æ˜æ˜¾çš„é¢†åŸŸå·®è·ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†UltraADï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å°‘é‡è¶…å£°æ ·æœ¬è¿›è¡Œé€šç”¨å¼‚å¸¸å®šä½å’Œç²¾ç»†åˆ†ç±»ã€‚ä¸ºäº†æé«˜å®šä½æ€§èƒ½ï¼Œé¦–å…ˆèåˆæŸ¥è¯¢è§†è§‰åŸå‹çš„å›¾åƒçº§ä»¤ç‰Œå’Œå¯å­¦ä¹ çš„æ–‡æœ¬åµŒå…¥ã€‚ç„¶åï¼Œå°†å›¾åƒä¿¡æ¯æç¤ºç‰¹å¾ä¸è¡¥ä¸çº§ä»¤ç‰Œè¿›ä¸€æ­¥é›†æˆï¼Œç»†åŒ–å±€éƒ¨è¡¨ç¤ºä»¥æé«˜å‡†ç¡®æ€§ã€‚å¯¹äºç²¾ç»†åˆ†ç±»ï¼Œä»å°‘é‡å›¾åƒæ ·æœ¬å’Œç›¸åº”çš„æ–‡æœ¬æè¿°ä¸­æ„å»ºäº†ä¸€ä¸ªå†…å­˜é“¶è¡Œï¼Œä»¥æ•è·è§£å‰–ç»“æ„å’Œå¼‚å¸¸ç‰¹å®šçš„ç‰¹å¾ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå­˜å‚¨çš„æ–‡æœ¬åµŒå…¥ä¿æŒä¸å˜ï¼Œè€Œå›¾åƒç‰¹å¾æ›´å¥½åœ°é€‚åº”åŒ»å­¦æ•°æ®ã€‚UltraADåœ¨ä¸‰ä¸ªä¹³è…ºè¶…å£°æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œåœ¨ç—…ç¶å®šä½å’Œç²¾ç»†åŒ»å­¦åˆ†ç±»æ–¹é¢éƒ½ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚ä»£ç å°†åœ¨æ¥å—åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19694v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒä¸­çš„ç²¾ç¡®å¼‚å¸¸æ£€æµ‹å¯¹ä¸´åºŠå†³ç­–è‡³å…³é‡è¦ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ç¼ºä¹ç²¾ç»†ç²’åº¦åŒºåˆ†ï¼ˆå¦‚è‰¯æ€§ä¸æ¶æ€§è‚¿ç˜¤ï¼‰ä»¥åŠè¶…å£°æˆåƒä¸­çš„è®¾å¤‡ä¸é‡‡é›†å‚æ•°å˜åŒ–å¸¦æ¥çš„é¢†åŸŸå·®è·é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UltraADï¼Œä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ–¹æ³•ï¼Œåˆ©ç”¨å°‘é‡è¶…å£°å›¾åƒæ ·æœ¬è¿›è¡Œé€šç”¨å¼‚å¸¸å®šä½å’Œç²¾ç»†ç²’åº¦åˆ†ç±»ã€‚é€šè¿‡èåˆæŸ¥è¯¢è§†è§‰åŸå‹å›¾åƒçº§ä»¤ç‰Œå’Œå¯å­¦ä¹ æ–‡æœ¬åµŒå…¥ï¼Œå¢å¼ºå®šä½æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨å°‘é‡å›¾åƒæ ·æœ¬åŠå…¶æ–‡æœ¬æè¿°æ„å»ºè®°å¿†åº“ï¼Œä»¥æ•è·è§£å‰–å’Œå¼‚å¸¸ç‰¹å®šç‰¹å¾ï¼Œè¿›è¡Œç²¾ç»†åˆ†ç±»ã€‚åœ¨ä¸‰ä¸ªä¹³è…ºè¶…å£°æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒUltraADåœ¨ç—…ç¶å®šä½å’Œç²¾ç»†åŒ»å­¦åˆ†ç±»æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒç²¾ç¡®å¼‚å¸¸æ£€æµ‹å¯¹ä¸´åºŠå†³ç­–è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ–¹æ³•ç¼ºä¹ç²¾ç»†ç²’åº¦åŒºåˆ†ï¼Œå¦‚è‰¯æ€§ä¸æ¶æ€§è‚¿ç˜¤çš„åŒºåˆ†ã€‚</li>
<li>UltraADåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>UltraADé€šè¿‡èåˆå›¾åƒçº§ä»¤ç‰Œå’Œæ–‡æœ¬åµŒå…¥æ¥å¢å¼ºå¼‚å¸¸å®šä½æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨å°‘é‡è¶…å£°å›¾åƒæ ·æœ¬è¿›è¡Œé€šç”¨å¼‚å¸¸å®šä½å’Œç²¾ç»†ç²’åº¦åˆ†ç±»ã€‚</li>
<li>é€šè¿‡æ„å»ºè®°å¿†åº“æ¥æ•è·è§£å‰–å’Œå¼‚å¸¸ç‰¹å®šç‰¹å¾ï¼Œè¿›è¡Œç²¾ç»†åˆ†ç±»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19694">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0f5ff0d234a6fad68a284d21bc750773.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7fbbfe85fa555982e0ff2c465d8edd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-194586963cfb90b18014120ac78c2ba0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="OmniFluids-Physics-Pre-trained-Modeling-of-Fluid-Dynamics"><a href="#OmniFluids-Physics-Pre-trained-Modeling-of-Fluid-Dynamics" class="headerlink" title="OmniFluids: Physics Pre-trained Modeling of Fluid Dynamics"></a>OmniFluids: Physics Pre-trained Modeling of Fluid Dynamics</h2><p><strong>Authors:Rui Zhang, Qi Meng, Han Wan, Yang Liu, Zhi-Ming Ma, Hao Sun</strong></p>
<p>Computational fluid dynamics (CFD) drives progress in numerous scientific and engineering fields, yet high-fidelity simulations remain computationally prohibitive. While machine learning approaches offer computing acceleration, they typically specialize in single physical systems or require extensive training data, hindering their applicability in highly nonlinear and 3D flow scenarios. To overcome these limitations, we propose OmniFluids, a pure physics pre-trained model that captures fundamental fluid dynamics laws and adapts efficiently to diverse downstream tasks with minimal data. We develop a training framework combining physics-only pre-training, coarse-grid operator distillation, and few-shot fine-tuning. This enables OmniFluids to retain broad physics knowledge while delivering fast and accurate predictions. Architecturally, OmniFluids integrates a mixture of operators, a multi-frame decoder, and factorized Fourier layers, seamlessly incorporating physics-based supervision while allowing efficient and scalable modeling of diverse tasks. Extensive tests on a broad range of 2D and 3D benchmarks show that OmniFluids outperforms state-of-the-art AI-driven methods in terms of flow field prediction and turbulence statistics. It delivers 10â€“100$\times$ speedups over traditional solvers while maintaining a comparable accuracy and accurately identifies unknown physical parameters from sparse, noisy data. This work demonstrates the potential of training a unified CFD solver exclusively from physics knowledge, offering a new approach for efficient and generalizable modeling across complex fluid systems. </p>
<blockquote>
<p>è®¡ç®—æµä½“åŠ¨åŠ›å­¦ï¼ˆCFDï¼‰æ¨åŠ¨äº†ä¼—å¤šç§‘å­¦å’Œå·¥ç¨‹é¢†åŸŸçš„è¿›æ­¥ï¼Œä½†é«˜ä¿çœŸæ¨¡æ‹Ÿåœ¨è®¡ç®—èƒ½åŠ›æ–¹é¢ä»ç„¶å—é™ã€‚è™½ç„¶æœºå™¨å­¦ä¹ æ–¹æ³•æä¾›äº†è®¡ç®—åŠ é€Ÿï¼Œä½†å®ƒä»¬é€šå¸¸ä¸“é—¨ç”¨äºå•ä¸€ç‰©ç†ç³»ç»Ÿæˆ–éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬åœ¨é«˜åº¦éçº¿æ€§å’Œä¸‰ç»´æµåŠ¨åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†OmniFluidsï¼Œè¿™æ˜¯ä¸€ä¸ªçº¯ç‰©ç†é¢„è®­ç»ƒæ¨¡å‹ï¼Œèƒ½å¤Ÿæ•æ‰åŸºæœ¬çš„æµä½“åŠ¨åŠ›å­¦å®šå¾‹ï¼Œå¹¶ä»¥é«˜æ•ˆçš„æ–¹å¼é€‚åº”å¤šæ ·åŒ–çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œæ‰€éœ€æ•°æ®æœ€å°‘ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè®­ç»ƒæ¡†æ¶ï¼Œç»“åˆäº†ä»…ç‰©ç†é¢„è®­ç»ƒã€ç²—ç½‘æ ¼æ“ä½œè’¸é¦å’Œå°‘é‡æ ·æœ¬å¾®è°ƒã€‚è¿™ä½¿å¾—OmniFluidsèƒ½å¤Ÿä¿ç•™å¹¿æ³›çš„ç‰©ç†çŸ¥è¯†ï¼ŒåŒæ—¶å®ç°å¿«é€Ÿå’Œå‡†ç¡®çš„é¢„æµ‹ã€‚åœ¨ç»“æ„ä¸Šï¼ŒOmniFluidsèåˆäº†å¤šç§æ“ä½œã€å¤šå¸§è§£ç å™¨å’Œå› å­åŒ–å‚…é‡Œå¶å±‚ï¼Œæ— ç¼åœ°èå…¥äº†åŸºäºç‰©ç†çš„ç›‘ç£ï¼Œå…è®¸å¯¹å¤šæ ·åŒ–ä»»åŠ¡è¿›è¡Œé«˜æ•ˆå’Œå¯æ‰©å±•çš„å»ºæ¨¡ã€‚åœ¨å¹¿æ³›çš„äºŒç»´å’Œä¸‰ç»´åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡æµ‹è¯•è¡¨æ˜ï¼ŒOmniFluidsåœ¨æµåœºé¢„æµ‹å’Œæ¹æµç»Ÿè®¡æ–¹é¢ä¼˜äºæœ€æ–°çš„AIé©±åŠ¨æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„æ±‚è§£å™¨ç›¸æ¯”ï¼Œå®ƒæä¾›äº†10-100å€çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒç›¸å½“çš„å‡†ç¡®æ€§ï¼Œå¹¶èƒ½ä»ç¨€ç–ã€å˜ˆæ‚çš„æ•°æ®ä¸­å‡†ç¡®åœ°è¯†åˆ«å‡ºæœªçŸ¥çš„ç‰©ç†å‚æ•°ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†ä»…ä»ç‰©ç†çŸ¥è¯†è®­ç»ƒç»Ÿä¸€CFDæ±‚è§£å™¨çš„æ½œåŠ›ï¼Œä¸ºå¤æ‚æµä½“ç³»ç»Ÿæä¾›é«˜æ•ˆå’Œå¯æ¨å¹¿çš„å»ºæ¨¡æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10862v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºOmniFluidsçš„çº¯ç‰©ç†é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ•æ‰åŸºæœ¬çš„æµä½“åŠ¨åŠ›å­¦å®šå¾‹ï¼Œå¹¶èƒ½é«˜æ•ˆé€‚åº”å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸”æ‰€éœ€æ•°æ®é‡æå°ã€‚é€šè¿‡ç»“åˆç‰©ç†é¢„è®­ç»ƒã€ç²—ç½‘æ ¼æ“ä½œè’¸é¦å’Œå°‘é‡å¾®è°ƒï¼ŒOmniFluidsèƒ½å¤Ÿåœ¨ä¿ç•™å¹¿æ³›ç‰©ç†çŸ¥è¯†çš„åŒæ—¶å®ç°å¿«é€Ÿå‡†ç¡®çš„é¢„æµ‹ã€‚åœ¨å¹¿æ³›çš„äºŒç»´å’Œä¸‰ç»´åŸºå‡†æµ‹è¯•ä¸­ï¼ŒOmniFluidsåœ¨æµåœºé¢„æµ‹å’Œæ¹æµç»Ÿè®¡æ–¹é¢ä¼˜äºæœ€æ–°çš„AIé©±åŠ¨æ–¹æ³•ï¼Œå¹¶ä¸”ç›¸å¯¹äºä¼ ç»Ÿæ±‚è§£å™¨å®ç°äº†é€Ÿåº¦å’Œç²¾åº¦çš„å¹³è¡¡ã€‚è¿™é¡¹ç ”ç©¶å±•ç¤ºäº†ä»…é€šè¿‡ç‰©ç†çŸ¥è¯†è®­ç»ƒç»Ÿä¸€CFDæ±‚è§£å™¨çš„æ½œåŠ›ï¼Œä¸ºå¤æ‚æµä½“ç³»ç»Ÿçš„æœ‰æ•ˆå’Œå¯æ¨å¹¿å»ºæ¨¡æä¾›äº†æ–°çš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OmniFluidsæ¨¡å‹ç»“åˆäº†ç‰©ç†é¢„è®­ç»ƒã€ç²—ç½‘æ ¼æ“ä½œè’¸é¦å’Œå°‘é‡å¾®è°ƒæŠ€æœ¯ï¼Œå®ç°äº†å¿«é€Ÿå‡†ç¡®çš„é¢„æµ‹ã€‚</li>
<li>OmniFluidsèƒ½å¤Ÿåœ¨ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ä¸­é«˜æ•ˆé€‚åº”ï¼Œæ‰€éœ€æ•°æ®é‡å°ã€‚</li>
<li>OmniFluidsåœ¨å¹¿æ³›çš„äºŒç»´å’Œä¸‰ç»´åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æµåœºé¢„æµ‹å’Œæ¹æµç»Ÿè®¡æ–¹é¢ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ±‚è§£å™¨ç›¸æ¯”ï¼ŒOmniFluidså®ç°äº†é€Ÿåº¦å’Œç²¾åº¦çš„å¹³è¡¡ã€‚</li>
<li>OmniFluidsæ¨¡å‹èƒ½å¤Ÿå‡†ç¡®åœ°è¯†åˆ«å‡ºç¨€ç–ã€å˜ˆæ‚æ•°æ®ä¸­çš„æœªçŸ¥ç‰©ç†å‚æ•°ã€‚</li>
<li>è¯¥ç ”ç©¶å±•ç¤ºäº†ä»…é€šè¿‡ç‰©ç†çŸ¥è¯†è®­ç»ƒç»Ÿä¸€CFDæ±‚è§£å™¨çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9882874dc2e3d31e84c0c14c3f58d1ff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7606d4505e04888bde2bc94dc623e24d.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="GreenMind-A-Next-Generation-Vietnamese-Large-Language-Model-for-Structured-and-Logical-Reasoning"><a href="#GreenMind-A-Next-Generation-Vietnamese-Large-Language-Model-for-Structured-and-Logical-Reasoning" class="headerlink" title="GreenMind: A Next-Generation Vietnamese Large Language Model for   Structured and Logical Reasoning"></a>GreenMind: A Next-Generation Vietnamese Large Language Model for   Structured and Logical Reasoning</h2><p><strong>Authors:Luu Quy Tung, Hoang Quoc Viet, Pham Bao Loc, Vo Trong Thu</strong></p>
<p>Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that require intermediate reasoning steps prior to generating a final answer. In this paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model inspired by the finetuning strategy based on Group Relative Policy Optimization. We also leverage a high-quality Vietnamese synthesized reasoning dataset and design two reward functions to tackle the main limitations of this technique: (i) language mixing, where we explicitly detect the presence of biased language characters during the process of sampling tokens, and (ii) we leverage Sentence Transformer-based models to ensure that the generated reasoning content maintains factual correctness and does not distort the final output. Experimental results on the Vietnamese dataset from the VLSP 2023 Challenge demonstrate that our model outperforms prior works and enhances linguistic consistency in its responses. Furthermore, we extend our evaluation to SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of our reasoning method compared to few-shot prompting techniques. </p>
<blockquote>
<p>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„æ–¹æ³•ï¼Œç”¨äºè§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦åœ¨ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆä¹‹å‰è¿›è¡Œä¸­é—´æ¨ç†æ­¥éª¤ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†GreenMind-Medium-14B-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªå—åŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGroup Relative Policy Optimizationï¼‰å¾®è°ƒç­–ç•¥å¯å‘çš„è¶Šå—è¯­æ¨ç†æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜åˆ©ç”¨é«˜è´¨é‡çš„è¶Šå—è¯­åˆæˆæ¨ç†æ•°æ®é›†ï¼Œå¹¶è®¾è®¡ä¸¤ä¸ªå¥–åŠ±å‡½æ•°æ¥è§£å†³è¿™é¡¹æŠ€æœ¯çš„ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šï¼ˆiï¼‰è¯­è¨€æ··åˆé—®é¢˜ï¼Œæˆ‘ä»¬åœ¨é‡‡æ ·tokençš„è¿‡ç¨‹ä¸­æ˜¾å¼æ£€æµ‹æœ‰åè§çš„è¯­è¨€å­—ç¬¦çš„å­˜åœ¨ï¼›ï¼ˆiiï¼‰æˆ‘ä»¬åˆ©ç”¨åŸºäºå¥å­è½¬æ¢å™¨çš„æ¨¡å‹ç¡®ä¿ç”Ÿæˆçš„æ¨ç†å†…å®¹ä¿æŒäº‹å®æ­£ç¡®æ€§ï¼Œå¹¶ä¸æ­ªæ›²æœ€ç»ˆè¾“å‡ºã€‚åœ¨VLSP 2023æŒ‘æˆ˜çš„è¶Šå—æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°ä¼˜äºæ—©æœŸä½œå“ï¼Œå¹¶ä¸”åœ¨å›åº”ä¸­å¢å¼ºäº†è¯­è¨€ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†è¯„ä¼°æ‰©å±•åˆ°äº†SeaExamâ€”â€”ä¸€ä¸ªå¤šè¯­ç§é€‰æ‹©é¢˜æ•°æ®é›†ï¼Œä»¥å±•ç¤ºæˆ‘ä»¬çš„æ¨ç†æ–¹æ³•ä¸å°‘æç¤ºæŠ€æœ¯ç›¸æ¯”çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16832v2">PDF</a> </p>
<p><strong>Summary</strong><br>ç»¿å¿ƒæ™ºå›¾-ä¸­å‹-14B-R1æ˜¯è¶Šå—æ¨ç†æ¨¡å‹ï¼Œé‡‡ç”¨åŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„å¾®è°ƒç­–ç•¥ã€‚è¯¥æ¨¡å‹è§£å†³äº†è¯­è¨€æ··åˆé—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¥å­è½¬æ¢å™¨æ¨¡å‹ç¡®ä¿æ¨ç†å†…å®¹çš„å‡†ç¡®æ€§å¹¶å‡å°‘è¾“å‡ºæ‰­æ›²ã€‚åœ¨è¶Šå—æ•°æ®é›†VLSP 2023æŒ‘æˆ˜ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨å¤šè¯­è¨€é€‰æ‹©é¢˜é›†SeaExamä¸­éªŒè¯äº†å…¶æ¨ç†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ç»¿å¿ƒæ™ºå›¾-ä¸­å‹-14B-R1æ¨¡å‹ï¼Œå®ƒæ˜¯ä¸€ä¸ªåŸºäºè¶Šå—è¯­è¨€çš„æ¨ç†æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨äº†å¾®è°ƒç­–ç•¥ï¼Œå…¶åŸºç¡€æ˜¯ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ã€‚</li>
<li>é€šè¿‡ä¸¤ä¸ªå¥–åŠ±å‡½æ•°è§£å†³äº†è¯¥æŠ€æœ¯çš„ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šè¯­è¨€æ··åˆå’Œç¡®ä¿ç”Ÿæˆçš„æ¨ç†å†…å®¹çš„å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡æ£€æµ‹é‡‡æ ·ä»¤ç‰Œè¿‡ç¨‹ä¸­å­˜åœ¨çš„åè§è¯­è¨€å­—ç¬¦æ¥è§£å†³è¯­è¨€æ··åˆé—®é¢˜ã€‚</li>
<li>åˆ©ç”¨å¥å­è½¬æ¢å™¨æ¨¡å‹ç¡®ä¿æ¨ç†å†…å®¹çš„å‡†ç¡®æ€§å¹¶å‡å°‘è¾“å‡ºæ‰­æ›²ã€‚</li>
<li>åœ¨è¶Šå—æ•°æ®é›†VLSP 2023æŒ‘æˆ˜ä¸­ï¼Œè¯¥æ¨¡å‹çš„æ€§èƒ½ä¼˜äºä»¥å‰çš„å·¥ä½œï¼Œå¹¶ä¸”å¢å¼ºäº†è¯­è¨€ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16832">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-624ae44513dc7c230db800269c012ac8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91a207c19aeb0f019583136ae6f11b78.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9d8be04f2bedb17ef2ecac18a332b022.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-683e0f869ff59f9ba7110afbb57bbe00.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Sparsity-Outperforms-Low-Rank-Projections-in-Few-Shot-Adaptation"><a href="#Sparsity-Outperforms-Low-Rank-Projections-in-Few-Shot-Adaptation" class="headerlink" title="Sparsity Outperforms Low-Rank Projections in Few-Shot Adaptation"></a>Sparsity Outperforms Low-Rank Projections in Few-Shot Adaptation</h2><p><strong>Authors:Nairouz Mrabah, Nicolas Richet, Ismail Ben Ayed, Ã‰ric Granger</strong></p>
<p>Adapting Vision-Language Models (VLMs) to new domains with few labeled samples remains a significant challenge due to severe overfitting and computational constraints. State-of-the-art solutions, such as low-rank reparameterization, mitigate these issues but often struggle with generalization and require extensive hyperparameter tuning. In this paper, a novel Sparse Optimization (SO) framework is proposed. Unlike low-rank approaches that typically constrain updates to a fixed subspace, our SO method leverages high sparsity to dynamically adjust very few parameters. We introduce two key paradigms. First, we advocate for \textit{local sparsity and global density}, which updates a minimal subset of parameters per iteration while maintaining overall model expressiveness. As a second paradigm, we advocate for \textit{local randomness and global importance}, which sparsifies the gradient using random selection while pruning the first moment based on importance. This combination significantly mitigates overfitting and ensures stable adaptation in low-data regimes. Extensive experiments on 11 diverse datasets show that SO achieves state-of-the-art few-shot adaptation performance while reducing memory overhead. </p>
<blockquote>
<p>å°†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€‚åº”å…·æœ‰å°‘é‡æ ‡è®°æ ·æœ¬çš„æ–°é¢†åŸŸä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºä¸¥é‡çš„è¿‡åº¦æ‹Ÿåˆå’Œè®¡ç®—çº¦æŸã€‚æœ€å…ˆè¿›çš„è§£å†³æ–¹æ¡ˆï¼Œå¦‚ä½ç§©é‡å‚æ•°åŒ–ï¼Œå¯ä»¥ç¼“è§£è¿™äº›é—®é¢˜ï¼Œä½†å®ƒä»¬é€šå¸¸å¾ˆéš¾æ¨å¹¿ï¼Œå¹¶ä¸”éœ€è¦å¤§é‡è°ƒæ•´è¶…å‚æ•°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç¨€ç–ä¼˜åŒ–ï¼ˆSOï¼‰æ¡†æ¶ã€‚ä¸é€šå¸¸å°†æ›´æ–°é™åˆ¶åœ¨å›ºå®šå­ç©ºé—´ä¸­çš„ä½ç§©æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„SOæ–¹æ³•åˆ©ç”¨é«˜ç¨€ç–æ€§æ¥åŠ¨æ€è°ƒæ•´æå°‘çš„å‚æ•°ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸¤ç§å…³é”®èŒƒå¼ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå€¡â€œå±€éƒ¨ç¨€ç–æ€§å’Œå…¨å±€å¯†åº¦â€ï¼Œè¿™å¯ä»¥åœ¨æ¯æ¬¡è¿­ä»£æ—¶æ›´æ–°ä¸€å°éƒ¨åˆ†å‚æ•°ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ•´ä½“è¡¨ç°åŠ›ã€‚ä½œä¸ºç¬¬äºŒç§èŒƒå¼ï¼Œæˆ‘ä»¬ä¸»å¼ â€œå±€éƒ¨éšæœºæ€§å’Œå…¨å±€é‡è¦æ€§â€ï¼Œä½¿ç”¨éšæœºé€‰æ‹©æ¥ç¨€ç–æ¢¯åº¦ï¼Œå¹¶æ ¹æ®é‡è¦æ€§è¿›è¡Œç¬¬ä¸€æ—¶åˆ»çš„ä¿®å‰ªã€‚è¿™ç§ç»“åˆæ˜¾è‘—å‡è½»äº†è¿‡åº¦æ‹Ÿåˆé—®é¢˜ï¼Œå¹¶ç¡®ä¿åœ¨ä½æ•°æ®æƒ…å†µä¸‹å®ç°ç¨³å®šçš„é€‚åº”ã€‚åœ¨11ä¸ªä¸åŒæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSOåœ¨å°‘æ ·æœ¬é€‚åº”æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶é™ä½äº†å†…å­˜å¼€é”€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12436v2">PDF</a> ICCV2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°çš„ä¼˜åŒ–æ¡†æ¶â€”â€”ç¨€ç–ä¼˜åŒ–ï¼ˆSOï¼‰ï¼Œç”¨äºåœ¨å…·æœ‰å°‘é‡æ ‡ç­¾æ ·æœ¬çš„æ–°é¢†åŸŸä¸­è‡ªé€‚åº”è°ƒæ•´è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚SOé€šè¿‡å±€éƒ¨ç¨€ç–æ€§å’Œå…¨å±€å¯†åº¦ç›¸ç»“åˆçš„æ–¹æ³•åŠ¨æ€è°ƒæ•´å¾ˆå°‘çš„æ¨¡å‹å‚æ•°ï¼Œä»¥æé«˜åœ¨å°‘æ•°æ ·æœ¬ä¸Šçš„é€‚åº”æ€§å¹¶é¿å…è¿‡åº¦æ‹Ÿåˆã€‚æ­¤å¤–ï¼ŒSOè¿˜ç»“åˆäº†å±€éƒ¨éšæœºæ€§å’Œå…¨å±€é‡è¦æ€§çš„æ¦‚å¿µï¼Œé€šè¿‡éšæœºé€‰æ‹©æ¢¯åº¦å¹¶åŸºäºé‡è¦æ€§è¿›è¡Œä¿®å‰ªæ¥è¿›ä¸€æ­¥ç¨³å®šæ¨¡å‹åœ¨ä½æ•°æ®æ¡ä»¶ä¸‹çš„é€‚åº”ã€‚åœ¨å¤šç§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSOæ–¹æ³•åœ¨å…·æœ‰æ˜¾è‘—å‡å°‘å†…å­˜å¼€é”€çš„åŒæ—¶å®ç°äº†å…ˆè¿›çš„å°‘æ•°é€‚åº”æ€§èƒ½ã€‚ </p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>ç¨€ç–ä¼˜åŒ–ï¼ˆSOï¼‰æ¡†æ¶è¢«æå‡ºç”¨äºè§£å†³åœ¨å…·æœ‰å°‘é‡æ ‡ç­¾æ ·æœ¬çš„æ–°é¢†åŸŸä¸­è‡ªé€‚åº”è°ƒæ•´è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚</li>
<li>SOæ¡†æ¶åˆ©ç”¨å±€éƒ¨ç¨€ç–æ€§å’Œå…¨å±€å¯†åº¦ç›¸ç»“åˆçš„æ–¹æ³•åŠ¨æ€è°ƒæ•´å¾ˆå°‘çš„æ¨¡å‹å‚æ•°ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨å°‘æ•°æ ·æœ¬ä¸Šçš„é€‚åº”æ€§ã€‚</li>
<li>SOæ–¹æ³•ç»“åˆäº†å±€éƒ¨éšæœºæ€§å’Œå…¨å±€é‡è¦æ€§çš„æ¦‚å¿µï¼Œé€šè¿‡éšæœºé€‰æ‹©æ¢¯åº¦å¹¶åŸºäºé‡è¦æ€§è¿›è¡Œä¿®å‰ªæ¥è¿›ä¸€æ­¥ç¨³å®šæ¨¡å‹åœ¨ä½æ•°æ®æ¡ä»¶ä¸‹çš„é€‚åº”ã€‚</li>
<li>ä¸ä½ç§©æ–¹æ³•ç›¸æ¯”ï¼ŒSOæ¡†æ¶å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œé€‚åº”æ€§ï¼Œæ— éœ€å¤§é‡çš„è¶…å‚æ•°è°ƒæ•´ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12436">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b706d1e5e8526efd395197bd6ccdc803.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72ca8c845e5e0c5b503a9a6cd06d6375.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fac78aaa8af7b6a85d1214d4090a21e0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Hierarchical-Relation-augmented-Representation-Generalization-for-Few-shot-Action-Recognition"><a href="#Hierarchical-Relation-augmented-Representation-Generalization-for-Few-shot-Action-Recognition" class="headerlink" title="Hierarchical Relation-augmented Representation Generalization for   Few-shot Action Recognition"></a>Hierarchical Relation-augmented Representation Generalization for   Few-shot Action Recognition</h2><p><strong>Authors:Hongyu Qu, Ling Xing, Jiachao Zhang, Rui Yan, Yazhou Yao, Xiangbo Shu</strong></p>
<p>Few-shot action recognition (FSAR) aims to recognize novel action categories with few exemplars. Existing methods typically learn frame-level representations for each video by designing inter-frame temporal modeling strategies or inter-video interaction at the coarse video-level granularity. However, they treat each episode task in isolation and neglect fine-grained temporal relation modeling between videos, thus failing to capture shared fine-grained temporal patterns across videos and reuse temporal knowledge from historical tasks. In light of this, we propose HR2G-shot, a Hierarchical Relation-augmented Representation Generalization framework for FSAR, which unifies three types of relation modeling (inter-frame, inter-video, and inter-task) to learn task-specific temporal patterns from a holistic view. Going beyond conducting inter-frame temporal interactions, we further devise two components to respectively explore inter-video and inter-task relationships: i) Inter-video Semantic Correlation (ISC) performs cross-video frame-level interactions in a fine-grained manner, thereby capturing task-specific query features and enhancing both intra-class consistency and inter-class separability; ii) Inter-task Knowledge Transfer (IKT) retrieves and aggregates relevant temporal knowledge from the bank, which stores diverse temporal patterns from historical episode tasks. Extensive experiments on five benchmarks show that HR2G-shot outperforms current top-leading FSAR methods. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬åŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰æ—¨åœ¨ä½¿ç”¨å°‘é‡æ ·æœ¬è¯†åˆ«æ–°å‹åŠ¨ä½œç±»åˆ«ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡è®¾è®¡å¸§é—´æ—¶åºå»ºæ¨¡ç­–ç•¥æˆ–åœ¨ç²—è§†é¢‘çº§åˆ«ç²’åº¦ä¸Šçš„è§†é¢‘é—´äº¤äº’ï¼Œä¸ºæ¯ä¸ªè§†é¢‘å­¦ä¹ å¸§çº§è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå®ƒä»¬å­¤ç«‹åœ°å¤„ç†æ¯ä¸ªä»»åŠ¡ç‰‡æ®µï¼Œå¿½ç•¥äº†è§†é¢‘ä¹‹é—´çš„ç²¾ç»†ç²’åº¦æ—¶åºå…³ç³»å»ºæ¨¡ï¼Œå› æ­¤æ— æ³•æ•è·è·¨è§†é¢‘çš„å…±äº«ç²¾ç»†ç²’åº¦æ—¶åºæ¨¡å¼ï¼Œä¹Ÿæ— æ³•ä»å†å²ä»»åŠ¡ä¸­é‡å¤ä½¿ç”¨æ—¶åºçŸ¥è¯†ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†HR2G-shotï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºFSARçš„åˆ†å±‚å…³ç³»å¢å¼ºè¡¨ç¤ºæ³›åŒ–æ¡†æ¶ï¼Œå®ƒç»Ÿä¸€äº†ä¸‰ç§ç±»å‹çš„å…³ç³»å»ºæ¨¡ï¼ˆå¸§é—´ã€è§†é¢‘é—´å’Œä»»åŠ¡é—´ï¼‰ï¼Œä»æ•´ä½“è§†è§’å­¦ä¹ ç‰¹å®šä»»åŠ¡çš„æ—¶åºæ¨¡å¼ã€‚é™¤äº†è¿›è¡Œå¸§é—´æ—¶åºäº¤äº’å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›ä¸€æ­¥å¼€å‘äº†ä¸¤ä¸ªç»„ä»¶æ¥åˆ†åˆ«æ¢ç´¢è§†é¢‘é—´å’Œä»»åŠ¡é—´çš„å…³ç³»ï¼šä¸€ã€è§†é¢‘é—´è¯­ä¹‰ç›¸å…³æ€§ï¼ˆISCï¼‰ä»¥ç²¾ç»†ç²’åº¦çš„æ–¹å¼æ‰§è¡Œè·¨è§†é¢‘å¸§çº§äº¤äº’ï¼Œä»è€Œæ•è·ç‰¹å®šä»»åŠ¡çš„æŸ¥è¯¢ç‰¹å¾ï¼Œå¢å¼ºç±»å†…ä¸€è‡´æ€§å’Œç±»é—´å¯åˆ†ç¦»æ€§ï¼›äºŒã€ä»»åŠ¡é—´çŸ¥è¯†è½¬ç§»ï¼ˆIKTï¼‰ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢å’Œèšåˆç›¸å…³çš„æ—¶åºçŸ¥è¯†ï¼Œè¯¥åº“å­˜å‚¨äº†æ¥è‡ªå†å²ä»»åŠ¡ç‰‡æ®µçš„å¤šç§æ—¶åºæ¨¡å¼ã€‚åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒHR2G-shotçš„è¡¨ç°ä¼˜äºå½“å‰é¢†å…ˆçš„FSARæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10079v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†Few-shotåŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰çš„æ–°æ–¹æ³•HR2G-shotã€‚è¯¥æ–¹æ³•æ—¨åœ¨é€šè¿‡ç»Ÿä¸€ä¸‰ç§å…³ç³»å»ºæ¨¡ï¼ˆå¸§é—´ã€è§†é¢‘é—´å’Œä»»åŠ¡é—´ï¼‰æ¥å­¦ä¹ ä»»åŠ¡ç‰¹å®šçš„æ—¶é—´æ¨¡å¼ã€‚å®ƒé€šè¿‡æ¢ç´¢è§†é¢‘é—´å’Œä»»åŠ¡é—´çš„å…³ç³»ï¼Œå¢å¼ºäº†ç±»å†…ä¸€è‡´æ€§å’Œç±»é—´å¯åˆ†æ€§ï¼Œå¹¶ä»å†å²ä»»åŠ¡ä¸­æ£€ç´¢å’Œèšåˆç›¸å…³çš„ä¸´æ—¶çŸ¥è¯†ã€‚æ­¤æ–¹æ³•åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shot action recognition (FSAR)çš„ç›®æ ‡æ˜¯è¯†åˆ«å°‘é‡æ ·æœ¬ä¸‹çš„æ–°åŠ¨ä½œç±»åˆ«ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è§†é¢‘å¸§çº§åˆ«çš„è¡¨ç¤ºå­¦ä¹ ï¼Œä½†å¿½ç•¥äº†è§†é¢‘é—´çš„ç²¾ç»†æ—¶é—´å…³ç³»å»ºæ¨¡ã€‚</li>
<li>HR2G-shotæ˜¯ä¸€ä¸ªé’ˆå¯¹FSARçš„å±‚æ¬¡å…³ç³»å¢å¼ºè¡¨ç¤ºæ³›åŒ–æ¡†æ¶ï¼Œèåˆäº†ä¸‰ç§å…³ç³»å»ºæ¨¡ï¼šå¸§é—´ã€è§†é¢‘é—´å’Œä»»åŠ¡é—´ã€‚</li>
<li>HR2G-shoté€šè¿‡æ¢ç´¢è§†é¢‘é—´å’Œä»»åŠ¡é—´çš„å…³ç³»ï¼Œå¢å¼ºäº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>Inter-video Semantic Correlation (ISC)æ¨¡å—ä»¥ç²¾ç»†ç²’åº¦çš„æ–¹å¼æ‰§è¡Œè·¨è§†é¢‘å¸§äº¤äº’ï¼Œæé«˜äº†ç±»å†…ä¸€è‡´æ€§å’Œç±»é—´å¯åˆ†æ€§ã€‚</li>
<li>Inter-task Knowledge Transfer (IKT)æ¨¡å—ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢å¹¶èšåˆæ¥è‡ªå†å²ä»»åŠ¡çš„ä¸´æ—¶çŸ¥è¯†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10079">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d035e3b2fdf1f6c62d31a795ffa86560.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b2455c494880e18de8a51c1758e10d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-798aee2b71ab3a241d4386609719879b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b31bd40ddaecae9008006a7a29681a1b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-88c5ba253959b0d786c713c5767b710d.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="From-Limited-Labels-to-Open-Domains-An-Efficient-Learning-Method-for-Drone-view-Geo-Localization"><a href="#From-Limited-Labels-to-Open-Domains-An-Efficient-Learning-Method-for-Drone-view-Geo-Localization" class="headerlink" title="From Limited Labels to Open Domains:An Efficient Learning Method for   Drone-view Geo-Localization"></a>From Limited Labels to Open Domains:An Efficient Learning Method for   Drone-view Geo-Localization</h2><p><strong>Authors:Zhongwei Chen, Zhao-Xu Yang, Hai-Jun Rong, Jiawei Lang, Guoqi Li</strong></p>
<p>Traditional supervised drone-view geo-localization (DVGL) methods heavily depend on paired training data and encounter difficulties in learning cross-view correlations from unpaired data. Moreover, when deployed in a new domain, these methods require obtaining the new paired data and subsequent retraining for model adaptation, which significantly increases computational overhead. Existing unsupervised methods have enabled to generate pseudo-labels based on cross-view similarity to infer the pairing relationships. However, geographical similarity and spatial continuity often cause visually analogous features at different geographical locations. The feature confusion compromises the reliability of pseudo-label generation, where incorrect pseudo-labels drive negative optimization. Given these challenges inherent in both supervised and unsupervised DVGL methods, we propose a novel cross-domain invariant knowledge transfer network (CDIKTNet) with limited supervision, whose architecture consists of a cross-domain invariance sub-network (CDIS) and a cross-domain transfer sub-network (CDTS). This architecture facilitates a closed-loop framework for invariance feature learning and knowledge transfer. The CDIS is designed to learn cross-view structural and spatial invariance from a small amount of paired data that serves as prior knowledge. It endows the shared feature space of unpaired data with similar implicit cross-view correlations at initialization, which alleviates feature confusion. Based on this, the CDTS employs dual-path contrastive learning to further optimize each subspace while preserving consistency in a shared feature space. Extensive experiments demonstrate that CDIKTNet achieves state-of-the-art performance under full supervision compared with those supervised methods, and further surpasses existing unsupervised methods in both few-shot and cross-domain initialization. </p>
<blockquote>
<p>ä¼ ç»Ÿç›‘ç£å¼çš„æ— äººæœºè§†è§’åœ°ç†å®šä½ï¼ˆDVGLï¼‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºé…å¯¹è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”åœ¨ä»éé…å¯¹æ•°æ®ä¸­å­¦ä¹ è·¨è§†è§’ç›¸å…³æ€§æ—¶é‡åˆ°å›°éš¾ã€‚æ­¤å¤–ï¼Œå½“è¿™äº›æ–¹æ³•éƒ¨ç½²åœ¨æ–°é¢†åŸŸæ—¶ï¼Œéœ€è¦è·å–æ–°çš„é…å¯¹æ•°æ®å¹¶è¿›è¡Œåç»­å†è®­ç»ƒä»¥é€‚åº”æ¨¡å‹ï¼Œè¿™å¤§å¤§å¢åŠ äº†è®¡ç®—å¼€é”€ã€‚ç°æœ‰çš„æ— ç›‘ç£æ–¹æ³•å·²ç»èƒ½å¤ŸåŸºäºè·¨è§†å›¾ç›¸ä¼¼æ€§ç”Ÿæˆä¼ªæ ‡ç­¾æ¥æ¨æ–­é…å¯¹å…³ç³»ã€‚ç„¶è€Œï¼Œåœ°ç†ç›¸ä¼¼æ€§å’Œç©ºé—´è¿ç»­æ€§å¸¸å¸¸å¯¼è‡´ä¸åŒåœ°ç†ä½ç½®çš„è§†è§‰ç±»ä¼¼ç‰¹å¾ã€‚ç‰¹å¾æ··æ·†é™ä½äº†ä¼ªæ ‡ç­¾ç”Ÿæˆçš„å¯é æ€§ï¼Œé”™è¯¯çš„ä¼ªæ ‡ç­¾ä¼šå¯¼è‡´è´Ÿé¢ä¼˜åŒ–ã€‚é‰´äºæœ‰ç›‘ç£å’Œæ— ç›‘ç£DVGLæ–¹æ³•å›ºæœ‰çš„è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰é™ç›‘ç£ä¸‹çš„æ–°å‹è·¨åŸŸä¸å˜çŸ¥è¯†è½¬ç§»ç½‘ç»œï¼ˆCDIKTNetï¼‰ï¼Œå…¶æ¶æ„åŒ…æ‹¬è·¨åŸŸä¸å˜æ€§å­ç½‘ç»œï¼ˆCDISï¼‰å’Œè·¨åŸŸè½¬ç§»å­ç½‘ç»œï¼ˆCDTSï¼‰ã€‚è¯¥æ¶æ„ä¿ƒè¿›äº†ä¸å˜ç‰¹å¾å­¦ä¹ å’ŒçŸ¥è¯†è½¬ç§»çš„é—­ç¯æ¡†æ¶ã€‚CDISæ—¨åœ¨ä»å°‘é‡é…å¯¹æ•°æ®ä¸­å­¦ä¹ è·¨è§†å›¾çš„ç»“æ„å’Œç©ºé—´ä¸å˜æ€§ï¼Œä½œä¸ºå…ˆéªŒçŸ¥è¯†ã€‚å®ƒèµ‹äºˆéé…å¯¹æ•°æ®å…±äº«ç‰¹å¾ç©ºé—´ä»¥ç›¸ä¼¼çš„éšå¼è·¨è§†å›¾ç›¸å…³æ€§è¿›è¡Œåˆå§‹åŒ–ï¼Œä»è€Œå‡è½»äº†ç‰¹å¾æ··æ·†ã€‚åŸºäºæ­¤ï¼ŒCDTSé‡‡ç”¨åŒè·¯å¾„å¯¹æ¯”å­¦ä¹ æ¥è¿›ä¸€æ­¥ä¼˜åŒ–æ¯ä¸ªå­ç©ºé—´ï¼ŒåŒæ—¶åœ¨å…±äº«ç‰¹å¾ç©ºé—´ä¸­ä¿æŒä¸€è‡´æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨å®Œå…¨ç›‘ç£ä¸‹ï¼ŒCDIKTNetç›¸è¾ƒäºå…¶ä»–ç›‘ç£æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨å°æ ·æœ¬å’Œè·¨åŸŸåˆå§‹åŒ–æ–¹é¢å‡è¶…è¶Šäº†ç°æœ‰æ— ç›‘ç£æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07520v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æè¿°äº†ä¸€ç§æ–°å‹çš„è·¨åŸŸä¸å˜çŸ¥è¯†è½¬ç§»ç½‘ç»œï¼ˆCDIKTNetï¼‰ï¼Œè¯¥ç½‘ç»œæ—¨åœ¨è§£å†³ä¼ ç»Ÿç›‘ç£å¼æ— äººæœºè§†è§’åœ°ç†å®šä½ï¼ˆDVGLï¼‰æ–¹æ³•ä¾èµ–é…å¯¹è®­ç»ƒæ•°æ®çš„é—®é¢˜ï¼Œå¹¶èƒ½å¤Ÿåœ¨æœ‰é™çš„ç›‘ç£ä¸‹å®ç°è·¨åŸŸä¸å˜æ€§ã€‚é€šè¿‡å¼•å…¥è·¨åŸŸä¸å˜æ€§å­ç½‘ç»œï¼ˆCDISï¼‰å’Œè·¨åŸŸè½¬ç§»å­ç½‘ç»œï¼ˆCDTSï¼‰ï¼ŒCDIKTNetèƒ½å¤Ÿåœ¨æœªé…å¯¹æ•°æ®ä¸­éšå¼åœ°å­¦ä¹ è·¨è§†è§’ç›¸å…³æ€§ï¼Œå‡å°‘ç‰¹å¾æ··æ·†ï¼Œå¹¶é€šè¿‡å¯¹æ¯”å­¦ä¹ ä¼˜åŒ–å­ç©ºé—´ï¼Œå®ç°çŸ¥è¯†è½¬ç§»ã€‚æ­¤æ–¹æ³•åœ¨å…¨ç›‘ç£ä¸‹è¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼Œå¹¶åœ¨å°‘æ ·æœ¬å’Œè·¨åŸŸåˆå§‹åŒ–åœºæ™¯ä¸‹è¶…è¶Šç°æœ‰æ— ç›‘ç£æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿç›‘ç£å¼æ— äººæœºè§†è§’åœ°ç†å®šä½æ–¹æ³•ä¾èµ–é…å¯¹è®­ç»ƒæ•°æ®ï¼Œé¢ä¸´è·¨åŸŸå­¦ä¹ éš¾é¢˜ã€‚</li>
<li>ç°æœ‰æ— ç›‘ç£æ–¹æ³•é€šè¿‡ç”Ÿæˆä¼ªæ ‡ç­¾æ¥æ¨æ–­é…å¯¹å…³ç³»ï¼Œä½†åœ°ç†ç›¸ä¼¼æ€§å¯¼è‡´çš„ç‰¹å¾æ··æ·†å½±å“ä¼ªæ ‡ç­¾å¯é æ€§ã€‚</li>
<li>CDIKTNetç½‘ç»œé€šè¿‡å¼•å…¥CDISå’ŒCDTSå­ç½‘ç»œï¼Œå®ç°æœ‰é™ç›‘ç£ä¸‹çš„è·¨åŸŸä¸å˜æ€§ã€‚</li>
<li>CDISä»å°‘é‡é…å¯¹æ•°æ®ä¸­å­¦ä¹ è·¨è§†è§’ç»“æ„å’Œç©ºé—´ä¸å˜æ€§ï¼Œä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼Œç¼“è§£æœªé…å¯¹æ•°æ®çš„ç‰¹å¾æ··æ·†é—®é¢˜ã€‚</li>
<li>CDTSé‡‡ç”¨åŒè·¯å¾„å¯¹æ¯”å­¦ä¹ ï¼Œä¼˜åŒ–å­ç©ºé—´å¹¶ä¿æŒå…±äº«ç‰¹å¾ç©ºé—´çš„ä¸€è‡´æ€§ã€‚</li>
<li>CDIKTNetåœ¨å…¨ç›‘ç£ä¸‹è¡¨ç°ä¸šç•Œé¢†å…ˆï¼Œå¹¶åœ¨å°‘æ ·æœ¬å’Œè·¨åŸŸåˆå§‹åŒ–åœºæ™¯ä¸‹è¶…è¶Šç°æœ‰æ— ç›‘ç£æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07520">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9b49ff8a1ed1b8b2bcba4e9a6409feca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-536ce897ca01af11118a241907614957.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-351a5bc8c74ff3490d2c0f413fc7611a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d09a6dd5b7499cfdfb0f696fdb26204.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44698a1676603be03da61c28d38d2d37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62d810a0f69a337a4bbef81b78260898.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5984a48fb133a5fb0d1204bca7a139f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65caa26b03b484c9ca49401db6a86cde.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="DWTNeRF-Boosting-Few-shot-Neural-Radiance-Fields-via-Discrete-Wavelet-Transform"><a href="#DWTNeRF-Boosting-Few-shot-Neural-Radiance-Fields-via-Discrete-Wavelet-Transform" class="headerlink" title="DWTNeRF: Boosting Few-shot Neural Radiance Fields via Discrete Wavelet   Transform"></a>DWTNeRF: Boosting Few-shot Neural Radiance Fields via Discrete Wavelet   Transform</h2><p><strong>Authors:Hung Nguyen, Blark Runfa Li, Truong Nguyen</strong></p>
<p>Neural Radiance Fields (NeRF) has achieved superior performance in novel view synthesis and 3D scene representation, but its practical applications are hindered by slow convergence and reliance on dense training views. To this end, we present DWTNeRF, a unified framework based on Instant-NGPâ€™s fast-training hash encoding. It is coupled with regularization terms designed for few-shot NeRF, which operates on sparse training views. Our DWTNeRF additionally includes a novel Discrete Wavelet loss that allows explicit prioritization of low frequencies directly in the training objective, reducing few-shot NeRFâ€™s overfitting on high frequencies in earlier training stages. We also introduce a model-based approach, based on multi-head attention, that is compatible with INGP, which are sensitive to architectural changes. On the 3-shot LLFF benchmark, DWTNeRF outperforms Vanilla INGP by 15.07% in PSNR, 24.45% in SSIM and 36.30% in LPIPS. Our approach encourages a re-thinking of current few-shot approaches for fast-converging implicit representations like INGP or 3DGS. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨æ–°å‹è§†å›¾åˆæˆå’Œ3Dåœºæ™¯è¡¨ç¤ºæ–¹é¢å–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼Œä½†å…¶å®é™…åº”ç”¨å—åˆ°æ”¶æ•›é€Ÿåº¦æ…¢å’Œä¾èµ–å¯†é›†è®­ç»ƒè§†å›¾çš„é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†DWTNeRFï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºInstant-NGPå¿«é€Ÿè®­ç»ƒå“ˆå¸Œç¼–ç çš„ç»Ÿä¸€æ¡†æ¶ã€‚å®ƒä¸é’ˆå¯¹å°æ ·æœ¬NeRFè®¾è®¡çš„æ­£åˆ™åŒ–æœ¯è¯­ç›¸ç»“åˆï¼Œå¯åœ¨ç¨€ç–è®­ç»ƒè§†å›¾ä¸Šè¿è¡Œã€‚æˆ‘ä»¬çš„DWTNeRFè¿˜åŒ…æ‹¬ä¸€ç§æ–°å‹ç¦»æ•£å°æ³¢æŸå¤±ï¼Œå…è®¸åœ¨è®­ç»ƒç›®æ ‡ä¸­ç›´æ¥æ˜ç¡®ä¼˜å…ˆå¤„ç†ä½é¢‘ï¼Œä»è€Œå‡å°‘å°æ ·æœ¬NeRFåœ¨æ—©æœŸè®­ç»ƒé˜¶æ®µå¯¹é«˜é¢‘çš„è¿‡æ‹Ÿåˆã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§åŸºäºå¤šå¤´æ³¨æ„åŠ›çš„æ¨¡å‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸INGPå…¼å®¹ï¼Œå¯¹æ¶æ„æ›´æ”¹æ•æ„Ÿã€‚åœ¨3æ¬¡æ‹æ‘„çš„LLFFåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDWTNeRFåœ¨PSNRä¸Šè¾ƒå¸¸è§„INGPé«˜å‡º15.07%ï¼Œåœ¨SSIMä¸Šé«˜å‡º24.45%ï¼Œåœ¨LPIPSä¸Šé«˜å‡º36.30%ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¼“åŠ±é‡æ–°æ€è€ƒå½“å‰çš„å¿«é€Ÿæ”¶æ•›éšå¼è¡¨ç¤ºçš„å°æ ·æœ¬æ–¹æ³•ï¼Œå¦‚INGPæˆ–3DGSã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12637v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºNeRFçš„æŠ€æœ¯åœ¨æ–°å‹è§†è§’åˆæˆå’Œä¸‰ç»´åœºæ™¯è¡¨ç¤ºæ–¹é¢å–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼Œä½†å…¶åœ¨å®é™…åº”ç”¨ä¸­å­˜åœ¨æ”¶æ•›é€Ÿåº¦æ…¢å’Œä¾èµ–å¯†é›†è®­ç»ƒè§†å›¾çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†DWTNeRFæ¡†æ¶ï¼Œå®ƒç»“åˆäº†Instant-NGPçš„å¿«é€Ÿè®­ç»ƒå“ˆå¸Œç¼–ç ï¼Œå¹¶è®¾è®¡äº†é’ˆå¯¹å°‘æ•°NeRFçš„æ­£åˆ™åŒ–é¡¹ï¼Œå¯åœ¨ç¨€ç–è®­ç»ƒè§†å›¾ä¸Šè¿è¡Œã€‚DWTNeRFè¿˜åŒ…æ‹¬ä¸€ç§æ–°å‹ç¦»æ•£å°æ³¢æŸå¤±ï¼Œå…è®¸åœ¨è®­ç»ƒç›®æ ‡ä¸­æ˜ç¡®ä¼˜å…ˆå¤„ç†ä½é¢‘ä¿¡æ¯ï¼Œä»è€Œå‡å°‘æ—©æœŸè®­ç»ƒé˜¶æ®µå¯¹é«˜é¢‘ä¿¡æ¯çš„è¿‡åº¦æ‹Ÿåˆã€‚åœ¨3æ¬¡æ‹æ‘„çš„LLFFåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDWTNeRFç›¸è¾ƒäºæ™®é€šçš„INGPåœ¨PSNRä¸Šæé«˜äº†15.07%ï¼Œåœ¨SSIMä¸Šæé«˜äº†24.45%ï¼Œåœ¨LPIPSä¸Šæé«˜äº†36.30%ã€‚æˆ‘ä»¬çš„ç ”ç©¶é¼“åŠ±å¯¹ç°æœ‰çš„å¿«é€Ÿæ”¶æ•›éšå¼è¡¨ç¤ºæ–¹æ³•ï¼ˆå¦‚INGPæˆ–3DGSï¼‰è¿›è¡Œæ–°çš„æ€è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DWTNeRFæ˜¯ä¸€ä¸ªåŸºäºNeRFæŠ€æœ¯çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å…¶åœ¨å®è·µä¸­çš„æ”¶æ•›é€Ÿåº¦æ…¢å’Œä¾èµ–å¯†é›†è®­ç»ƒè§†å›¾çš„é—®é¢˜ã€‚</li>
<li>DWTNeRFç»“åˆäº†Instant-NGPçš„å¿«é€Ÿè®­ç»ƒå“ˆå¸Œç¼–ç æŠ€æœ¯ï¼Œå¹¶è®¾è®¡äº†é’ˆå¯¹å°‘æ•°NeRFåœºæ™¯çš„æ­£åˆ™åŒ–é¡¹ã€‚</li>
<li>æå‡ºäº†æ–°å‹ç¦»æ•£å°æ³¢æŸå¤±ï¼Œèƒ½å¤Ÿä¼˜å…ˆå¤„ç†ä½é¢‘ä¿¡æ¯ï¼Œå‡å°‘æ—©æœŸè®­ç»ƒé˜¶æ®µå¯¹é«˜é¢‘ä¿¡æ¯çš„è¿‡åº¦æ‹Ÿåˆã€‚</li>
<li>åœ¨3æ¬¡æ‹æ‘„çš„LLFFåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDWTNeRFç›¸è¾ƒäºæ™®é€šæ–¹æ³•æ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚</li>
<li>DWTNeRFæä¾›äº†ä¸€ç§æ¨¡å‹æ–¹æ³•ï¼ŒåŸºäºå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸æ•æ„Ÿçš„INGPæ¶æ„å˜æ›´å…¼å®¹ã€‚</li>
<li>ç ”ç©¶ç»“æœé¼“åŠ±å¯¹å½“å‰å¿«é€Ÿæ”¶æ•›éšå¼è¡¨ç¤ºæ–¹æ³•ï¼ˆå¦‚INGPæˆ–3DGSï¼‰è¿›è¡Œæ–°çš„æ€è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12637">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-da9689750332b864662b7acac0a7b9bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-996752574dbcc3a99aa5a5196bf1704c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d232a768c0b1cd9d49966bcc22590898.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9b54877b1f4f3c7534b9ef720877037.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ddac22c46af473d6858efabeb0f7e699.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="UoMo-A-Universal-Model-of-Mobile-Traffic-Forecasting-for-Wireless-Network-Optimization"><a href="#UoMo-A-Universal-Model-of-Mobile-Traffic-Forecasting-for-Wireless-Network-Optimization" class="headerlink" title="UoMo: A Universal Model of Mobile Traffic Forecasting for Wireless   Network Optimization"></a>UoMo: A Universal Model of Mobile Traffic Forecasting for Wireless   Network Optimization</h2><p><strong>Authors:Haoye Chai, Shiyuan Zhang, Xiaoqian Qi, Baohua Qiu, Yong Li</strong></p>
<p>Mobile traffic forecasting allows operators to anticipate network dynamics and performance in advance, offering substantial potential for enhancing service quality and improving user experience. However, existing models are often task-oriented and are trained with tailored data, which limits their effectiveness in diverse mobile network tasks of Base Station (BS) deployment, resource allocation, energy optimization, etc. and hinders generalization across different urban environments. Foundation models have made remarkable strides across various domains of NLP and CV due to their multi-tasking adaption and zero&#x2F;few-shot learning capabilities. In this paper, we propose an innovative Foundation model for Mo}bile traffic forecasting (FoMo), aiming to handle diverse forecasting tasks of short&#x2F;long-term predictions and distribution generation across multiple cities to support network planning and optimization. FoMo combines diffusion models and transformers, where various spatio-temporal masks are proposed to enable FoMo to learn intrinsic features of different tasks, and a contrastive learning strategy is developed to capture the correlations between mobile traffic and urban contexts, thereby improving its transfer learning capability. Extensive experiments on 9 real-world datasets demonstrate that FoMo outperforms current models concerning diverse forecasting tasks and zero&#x2F;few-shot learning, showcasing a strong universality. </p>
<blockquote>
<p>ç§»åŠ¨æµé‡é¢„æµ‹ä½¿è¿è¥å•†èƒ½å¤Ÿæå‰é¢„æµ‹ç½‘ç»œåŠ¨æ€å’Œæ€§èƒ½ï¼Œä¸ºæé«˜æœåŠ¡è´¨é‡å’Œæ”¹å–„ç”¨æˆ·ä½“éªŒæä¾›äº†å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹é€šå¸¸æ˜¯é¢å‘ä»»åŠ¡çš„ï¼Œå¹¶ä¸”ä½¿ç”¨å®šåˆ¶æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨åŸºç«™éƒ¨ç½²ã€èµ„æºé…ç½®ã€èƒ½æºä¼˜åŒ–ç­‰å¤šæ ·åŒ–çš„ç§»åŠ¨ç½‘ç»œä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶é˜»ç¢äº†å®ƒä»¬åœ¨ä¸åŒåŸå¸‚ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚ç”±äºå…¶åœ¨å¤šä»»åŠ¡é€‚åº”å’Œé›¶&#x2F;å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›æ–¹é¢çš„ä¼˜åŠ¿ï¼ŒåŸºç¡€æ¨¡å‹åœ¨NLPå’Œè®¡ç®—æœºè§†è§‰çš„å„ä¸ªé¢†åŸŸéƒ½å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„ç§»åŠ¨æµé‡é¢„æµ‹åŸºç¡€æ¨¡å‹ï¼ˆFoMoï¼‰ï¼Œæ—¨åœ¨å¤„ç†çŸ­æœŸ&#x2F;é•¿æœŸé¢„æµ‹å’Œå¤šåŸå¸‚é¢„æµ‹åˆ†å¸ƒç”Ÿæˆçš„å¤šæ ·åŒ–é¢„æµ‹ä»»åŠ¡ï¼Œä»¥æ”¯æŒç½‘ç»œè§„åˆ’å’Œä¼˜åŒ–ã€‚FoMoç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œå˜å‹å™¨æ¨¡å‹ï¼Œæå‡ºäº†å„ç§æ—¶ç©ºæ©ç ï¼Œä½¿FoMoèƒ½å¤Ÿå­¦ä¹ ä¸åŒä»»åŠ¡çš„å†…è•´ç‰¹å¾ï¼Œå¹¶å¼€å‘äº†ä¸€ç§å¯¹æ¯”å­¦ä¹ ç­–ç•¥æ¥æ•æ‰ç§»åŠ¨æµé‡å’ŒåŸå¸‚ä¸Šä¸‹æ–‡ä¹‹é—´çš„å…³è”ï¼Œä»è€Œæé«˜å…¶è¿ç§»å­¦ä¹ èƒ½åŠ›ã€‚åœ¨9ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFoMoåœ¨å¤šæ ·åŒ–çš„é¢„æµ‹ä»»åŠ¡å’Œé›¶&#x2F;å°‘æ ·æœ¬å­¦ä¹ æ–¹é¢è¶…è¶Šäº†å½“å‰æ¨¡å‹ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15322v4">PDF</a> 2025 ACM SIGKDD International Conference on Knowledge Discovery and   Data Mining, KDD 2025</p>
<p><strong>Summary</strong></p>
<p>ç§»åŠ¨æµé‡é¢„æµ‹æœ‰åŠ©äºè¿è¥å•†æå‰é¢„æµ‹ç½‘ç»œåŠ¨æ€å’Œæ€§èƒ½ï¼Œå¯¹æé«˜æœåŠ¡è´¨é‡å’Œç”¨æˆ·ä½“éªŒæœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹å¾€å¾€æ˜¯ä»»åŠ¡å¯¼å‘çš„ï¼Œç”¨ç‰¹å®šæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè¿™åœ¨å¤šæ ·åŒ–çš„ç§»åŠ¨ç½‘ç»œä»»åŠ¡ï¼ˆå¦‚åŸºç«™éƒ¨ç½²ã€èµ„æºé…ç½®ã€èƒ½æºä¼˜åŒ–ç­‰ï¼‰ä¸­é™åˆ¶äº†å…¶æ•ˆæœï¼Œå¹¶é˜»ç¢äº†åœ¨ä¸åŒåŸå¸‚ç¯å¢ƒä¸­çš„é€šç”¨åŒ–ã€‚æœ¬æ–‡æå‡ºä¸€ç§åˆ›æ–°çš„ç§»åŠ¨æµé‡é¢„æµ‹åŸºç¡€æ¨¡å‹ï¼ˆFoMoï¼‰ï¼Œæ—¨åœ¨å¤„ç†çŸ­æœŸ&#x2F;é•¿æœŸé¢„æµ‹å’Œè·¨å¤šä¸ªåŸå¸‚çš„åˆ†å¸ƒç”Ÿæˆç­‰å¤šæ ·åŒ–é¢„æµ‹ä»»åŠ¡ï¼Œä»¥æ”¯æŒç½‘ç»œè§„åˆ’å’Œä¼˜åŒ–ã€‚FoMoç»“åˆæ‰©æ•£æ¨¡å‹å’Œè½¬æ¢å™¨ï¼Œé€šè¿‡æå‡ºå„ç§æ—¶ç©ºæ©ç æ¥å­¦ä¹ ä¸åŒä»»åŠ¡çš„å†…è•´ç‰¹å¾ï¼Œå¹¶å¼€å‘å¯¹æ¯”å­¦ä¹ ç­–ç•¥æ¥æ•æ‰ç§»åŠ¨æµé‡å’ŒåŸå¸‚ç¯å¢ƒä¹‹é—´çš„å…³è”ï¼Œä»è€Œæé«˜å…¶è¿ç§»å­¦ä¹ èƒ½åŠ›ã€‚åœ¨9ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFoMoåœ¨å¤šæ ·é¢„æµ‹ä»»åŠ¡å’Œé›¶&#x2F;å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›æ–¹é¢ä¼˜äºå½“å‰æ¨¡å‹ï¼Œå±•ç°å‡ºå¼ºå¤§çš„é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§»åŠ¨æµé‡é¢„æµ‹å¯¹è¿è¥å•†è€Œè¨€è‡³å…³é‡è¦ï¼Œæœ‰åŠ©äºæå‰é¢„æµ‹ç½‘ç»œåŠ¨æ€å’Œæ€§èƒ½ã€‚</li>
<li>ç°æœ‰æ¨¡å‹å› ä»»åŠ¡å¯¼å‘å’Œç‰¹å®šæ•°æ®è®­ç»ƒè€Œå…·æœ‰å±€é™æ€§ï¼Œéš¾ä»¥åº”å¯¹å¤šæ ·åŒ–çš„ç§»åŠ¨ç½‘ç»œä»»åŠ¡å’Œä¸åŒåŸå¸‚ç¯å¢ƒçš„é€šç”¨åŒ–ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„FoMoæ¨¡å‹ç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œè½¬æ¢å™¨ï¼Œæ—¨åœ¨å¤„ç†å¤šæ ·åŒ–çš„ç§»åŠ¨æµé‡é¢„æµ‹ä»»åŠ¡ã€‚</li>
<li>FoMoé€šè¿‡æ—¶ç©ºæ©ç å­¦ä¹ ä¸åŒä»»åŠ¡çš„å†…è•´ç‰¹å¾ï¼Œå¹¶åº”ç”¨å¯¹æ¯”å­¦ä¹ ç­–ç•¥æ¥æ•æ‰ç§»åŠ¨æµé‡ä¸åŸå¸‚ç¯å¢ƒä¹‹é—´çš„å…³è”ã€‚</li>
<li>FoMoæ¨¡å‹åœ¨å¤šç§é¢„æµ‹ä»»åŠ¡å’Œé›¶&#x2F;å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„é€šç”¨æ€§ã€‚</li>
<li>å¹¿æ³›å®éªŒè¯æ˜FoMoåœ¨çœŸå®æ•°æ®é›†ä¸Šçš„æ•ˆæœä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15322">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f6d3593fc474e27d1ce0aa6571d1d7de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-abc6db5748181dee09b3d6b3846c321c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdc1424af55d5a06aa3f34f84b9b64c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c0938987dfb55e57889a2ea28bc0ebb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-056f6c29e55df18d472809f403355ddc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ee8b9f1d756354259d901fdabdca45b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3f6e46ae053b430c5104de225f1c4430.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="ComPEFT-Compression-for-Communicating-Parameter-Efficient-Updates-via-Sparsification-and-Quantization"><a href="#ComPEFT-Compression-for-Communicating-Parameter-Efficient-Updates-via-Sparsification-and-Quantization" class="headerlink" title="ComPEFT: Compression for Communicating Parameter Efficient Updates via   Sparsification and Quantization"></a>ComPEFT: Compression for Communicating Parameter Efficient Updates via   Sparsification and Quantization</h2><p><strong>Authors:Prateek Yadav, Leshem Choshen, Colin Raffel, Mohit Bansal</strong></p>
<p>Parameter-efficient fine-tuning (PEFT) techniques make it possible to efficiently adapt a language model to create â€œexpertâ€ models that specialize to new tasks or domains. Recent techniques in model merging and compositional generalization leverage these expert models by dynamically composing modules to improve zero&#x2F;few-shot generalization. Despite the efficiency of PEFT methods, the size of expert models can make it onerous to retrieve expert models per query over high-latency networks like the Internet or serve multiple experts on a single GPU. To address these issues, we present ComPEFT, a novel method for compressing fine-tuning residuals (task vectors) of PEFT based models. ComPEFT employs sparsification and ternary quantization to reduce the size of the PEFT module without performing any additional retraining while preserving or enhancing model performance. In extensive evaluation across T5, T0, and LLaMA-based models with 200M - 65B parameters, ComPEFT achieves compression ratios of 8x - 50x. In particular, we show that ComPEFT improves with scale - stronger models exhibit higher compressibility and better performance. For example, we show that ComPEFT applied to LLaMA outperforms QLoRA by 4.16% on MMLU with a storage size reduction of up to 26x. In addition, we show that the compressed experts produced by ComPEFT maintain few-shot compositional generalization capabilities, facilitate efficient communication and computation, and exhibit enhanced performance when merged. Lastly, we provide an analysis of different method components, compare it with other PEFT methods, and test ComPEFTâ€™s efficacy for compressing the residual of full-finetuning. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/prateeky2806/compeft">https://github.com/prateeky2806/compeft</a>. </p>
<blockquote>
<p>å‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼ˆPEFTï¼‰æŠ€æœ¯ä½¿å¾—èƒ½å¤Ÿé«˜æ•ˆåœ°é€‚åº”è¯­è¨€æ¨¡å‹ï¼Œä»¥åˆ›å»ºä¸“é—¨ç”¨äºæ–°ä»»åŠ¡æˆ–é¢†åŸŸçš„â€œä¸“å®¶â€æ¨¡å‹ã€‚æœ€è¿‘çš„æ¨¡å‹åˆå¹¶å’Œç»„åˆæ³›åŒ–æŠ€æœ¯é€šè¿‡åŠ¨æ€ç»„åˆæ¨¡å—æ¥åˆ©ç”¨è¿™äº›ä¸“å®¶æ¨¡å‹ï¼Œä»¥æé«˜é›¶&#x2F;å°‘æ ·æœ¬çš„æ³›åŒ–èƒ½åŠ›ã€‚å°½ç®¡PEFTæ–¹æ³•æ•ˆç‡å¾ˆé«˜ï¼Œä½†ä¸“å®¶æ¨¡å‹çš„å¤§å°å¯èƒ½ä¼šåœ¨é«˜å»¶è¿Ÿçš„ç½‘ç»œï¼ˆå¦‚äº’è”ç½‘ï¼‰ä¸Šé’ˆå¯¹æ¯ä¸ªæŸ¥è¯¢æ£€ç´¢ä¸“å®¶æ¨¡å‹å˜å¾—éå¸¸å›°éš¾ï¼Œæˆ–è€…åœ¨å•ä¸ªGPUä¸Šæä¾›å¤šä¸ªä¸“å®¶æœåŠ¡æ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ComPEFTï¼Œè¿™æ˜¯ä¸€ç§å‹ç¼©åŸºäºPEFTçš„å¾®è°ƒæ®‹å·®ï¼ˆä»»åŠ¡å‘é‡ï¼‰çš„æ–°æ–¹æ³•ã€‚ComPEFTé‡‡ç”¨ç¨€ç–åŒ–å’Œä¸‰å…ƒé‡åŒ–ï¼Œåœ¨ä¸è¿›è¡Œä»»ä½•é¢å¤–å†è®­ç»ƒçš„æƒ…å†µä¸‹å‡å°‘PEFTæ¨¡å—çš„å¤§å°ï¼ŒåŒæ—¶ä¿ç•™æˆ–æé«˜æ¨¡å‹æ€§èƒ½ã€‚åœ¨è·¨è¶Šå…·æœ‰2äº¿è‡³65äº¿å‚æ•°çš„T5ã€T0å’ŒLLaMAæ¨¡å‹çš„å¤§é‡è¯„ä¼°ä¸­ï¼ŒComPEFTå®ç°äº†é«˜è¾¾8å€è‡³50å€çš„å‹ç¼©æ¯”ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬è¡¨æ˜ComPEFTéšç€è§„æ¨¡çš„æ‰©å¤§è€Œæ”¹è¿›â€”â€”æ›´å¼ºçš„æ¨¡å‹è¡¨ç°å‡ºæ›´é«˜çš„å¯å‹ç¼©æ€§å’Œæ›´å¥½çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬åœ¨LLaMAä¸Šåº”ç”¨çš„ComPEFTåœ¨MMLUä¸Šçš„æ€§èƒ½ä¼˜äºQLoRA 4.16%ï¼ŒåŒæ—¶å­˜å‚¨ç©ºé—´å‡å°‘äº†é«˜è¾¾26å€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œç”±ComPEFTäº§ç”Ÿçš„å‹ç¼©ä¸“å®¶ä¿æŒäº†å°‘æ ·æœ¬ç»„åˆæ³›åŒ–èƒ½åŠ›ï¼Œä¿ƒè¿›äº†æœ‰æ•ˆçš„é€šä¿¡å’Œè®¡ç®—ï¼Œå¹¶åœ¨åˆå¹¶æ—¶è¡¨ç°å‡ºå¢å¼ºçš„æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹ä¸åŒçš„æ–¹æ³•ç»„ä»¶è¿›è¡Œäº†åˆ†æï¼Œä¸å…¶ä»–PEFTæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶æµ‹è¯•äº†ComPEFTå¯¹å…¨å¾®è°ƒæ®‹å·®è¿›è¡Œå‹ç¼©çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº&lt;<a target="_blank" rel="noopener" href="https://github.com/prateeky280">https://github.com/prateeky280</a> 6&#x2F;compeft&gt;ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.13171v2">PDF</a> 25 Pages, 6 Figures, 16 Tables</p>
<p><strong>æ‘˜è¦</strong><br>å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿæœ‰æ•ˆåœ°é€‚åº”è¯­è¨€æ¨¡å‹æ¥åˆ›å»ºä¸“ä¸šåŒ–çš„â€œä¸“å®¶â€æ¨¡å‹ï¼Œä»¥åº”å¯¹æ–°ä»»åŠ¡æˆ–é¢†åŸŸçš„éœ€æ±‚ã€‚è¿‘æœŸæ¨¡å‹åˆå¹¶å’Œç»„åˆæ³›åŒ–æŠ€æœ¯åˆ©ç”¨è¿™äº›ä¸“å®¶æ¨¡å‹é€šè¿‡åŠ¨æ€ç»„åˆæ¨¡å—æé«˜é›¶æ¬¡æˆ–å°‘æ¬¡è°ƒæ•´æƒ…å†µä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è™½ç„¶PEFTæ–¹æ³•çš„æ•ˆç‡è¾ƒé«˜ï¼Œä½†ç”±äºä¸“å®¶æ¨¡å‹çš„å¤§å°é—®é¢˜ï¼Œåœ¨è¯¸å¦‚äº’è”ç½‘çš„é«˜å»¶è¿Ÿç½‘ç»œä¸Šæ£€ç´¢ä¸“å®¶æ¨¡å‹æˆ–å¯¹å•ä¸ªGPUä¸Šå¤šä¸ªä¸“å®¶è¿›è¡ŒæœåŠ¡å¯èƒ½ä¼šå˜å¾—éå¸¸å›°éš¾ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ComPEFTæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§å‹ç¼©PEFTæ¨¡å‹å¾®è°ƒæ®‹å·®ï¼ˆä»»åŠ¡å‘é‡ï¼‰çš„æ–°æ–¹æ³•ã€‚ComPEFTé‡‡ç”¨ç¨€ç–åŒ–å’Œä¸‰å…ƒé‡åŒ–æŠ€æœ¯ï¼Œæ— éœ€è¿›è¡Œä»»ä½•é¢å¤–çš„å†è®­ç»ƒå³å¯å‡å°PEFTæ¨¡å—çš„å¤§å°ï¼ŒåŒæ—¶ä¿æŒæˆ–æå‡æ¨¡å‹æ€§èƒ½ã€‚åœ¨è·¨è¶ŠT5ã€T0å’ŒLLaMAæ¨¡å‹çš„å¹¿æ³›è¯„ä¼°ä¸­ï¼Œå‚æ•°è§„æ¨¡åœ¨2äº¿è‡³æ•°åäº¿ä¹‹é—´ï¼ŒComPEFTå¯å®ç°é«˜è¾¾8å€è‡³å‡ åå€çš„å‹ç¼©æ¯”ã€‚ç‰¹åˆ«çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°ComPEFTçš„è¡¨ç°éšç€æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§è€Œæé«˜â€”â€”æ›´å¼ºå¤§çš„æ¨¡å‹å±•ç°å‡ºæ›´é«˜çš„å‹ç¼©æ€§èƒ½å’Œæ›´å¥½çš„è¡¨ç°ã€‚ä¾‹å¦‚ï¼Œåœ¨MMLUä¸Šï¼Œç›¸è¾ƒäºQLoRAæ¨¡å‹ï¼Œåº”ç”¨ComPEFTçš„LLaMAæ¨¡å‹æ€§èƒ½æå‡äº†4.16%ï¼ŒåŒæ—¶å­˜å‚¨å¤§å°å‡å°‘äº†é«˜è¾¾26å€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†ComPEFTç”Ÿæˆçš„å‹ç¼©ä¸“å®¶æ¨¡å‹èƒ½å¤Ÿä¿æŒå°‘æ¬¡ç»„åˆæ³›åŒ–èƒ½åŠ›ã€ä¿ƒè¿›é«˜æ•ˆé€šä¿¡å’Œè®¡ç®—ï¼Œå¹¶åœ¨åˆå¹¶æ—¶å±•ç°å‡ºå¢å¼ºæ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹ä¸åŒæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒåˆ†æï¼Œå¹¶æµ‹è¯•äº†ComPEFTåœ¨å‹ç¼©å…¨å¾®è°ƒæ®‹ä½™æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/prateeky2806/compeft">é“¾æ¥</a>è®¿é—®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>PEFTæŠ€æœ¯å…è®¸åˆ›å»ºç‰¹å®šä»»åŠ¡æˆ–é¢†åŸŸçš„ä¸“ä¸šæ¨¡å‹ä»¥æé«˜æ•ˆç‡ã€‚è¿™äº›æ¨¡å‹è¢«ç§°ä¸ºâ€œä¸“å®¶â€æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹çš„åº”ç”¨æ¨åŠ¨äº†è¿‘æœŸçš„æ¨¡å‹åˆå¹¶å’Œç»„åˆæ³›åŒ–æŠ€æœ¯çš„è¿›æ­¥ã€‚å°½ç®¡å®ƒä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„è°ƒæ•´ç¨‹åº¦è¾ƒä½ï¼Œä½†å®ƒä»¬çš„é«˜æ•ˆæ€§ä»ç„¶æ˜¯å¼•äººæ³¨ç›®çš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œå®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­ä»»åŠ¡æ‰§è¡Œæ—¶å±•ç¤ºå‡ºäº†æƒŠäººçš„æ³›åŒ–èƒ½åŠ›ã€‚éšç€ç ”ç©¶çš„æ·±å…¥è¿›è¡Œå’ŒæŠ€æœ¯çš„è¿›æ­¥ï¼ŒPEFTçš„ä¼˜åŠ¿åœ¨æœªæ¥ä¼šå¾—åˆ°æ›´å¤§çš„æŒ–æ˜å’Œåº”ç”¨æ‹“å±•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯è¿™ç±»æŠ€æœ¯çš„æˆç†Ÿåœ¨å­¦ç•Œå’Œå•†ä¸šé¢†åŸŸäº§ç”Ÿäº†å¹¿æ³›å…³æ³¨å’Œä½¿ç”¨æ¡ˆä¾‹çš„å‘å±•æœ‰ç€æ›´å¹¿æ³›çš„å¸‚åœºå’Œå•†ä¸šä»·å€¼å€¼å¾—æ¢ç´¢å’Œä¼˜åŒ–åº”ç”¨åœ¨é¢†åŸŸåº”ç”¨å’Œå½±å“åŠ›æ–¹é¢æ˜¯æå¤§çš„å¯¹æ•´ä¸ªç¤¾ä¼šå°¤å…¶æ˜¯æœºå™¨å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„é•¿è¿œå‘å±•éƒ½æœ‰é‡å¤§çš„è´¡çŒ®å¯¹äºæ¨å¹¿é¢†åŸŸé€‚åº”åŠ›ä¸æŠ€æœ¯è¿­ä»£çš„æœªæ¥æ€§æœ‰å¾ˆå¤§çš„å‚è€ƒä»·å€¼å’Œå¯¹å½“ä¸‹è¡Œä¸šå†…æ¿€çƒˆçš„è¡Œä¸šå‘å±•çš„é‡è¦æŒ‡å¼•ä»·å€¼æ˜¯å½“å‰ä¹ƒè‡³æœªæ¥ç›¸å…³ç ”ç©¶é¢†åŸŸå…·æœ‰æˆ˜ç•¥æ„ä¹‰çš„ä¸€ç¯</li>
</ol>
<p>æ¥ä¸‹æ¥æˆ‘æ€»ç»“äº†æ–‡æœ¬ä¸­çš„ä¸ƒä¸ªå…³é”®è§è§£ï¼š</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.13171">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab53e39cbef738ac49babc671ca8dd00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c8c8dce35c899c271648bcff2595a5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bef79b7d99ec7ba896d082fac2431869.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5f8091b23bd186b3853cbd214c6fa21.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-13/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-13/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-13/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2cb7c6a20424b2e695d36bcde60e79be.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-13  CycleDiff Cycle Diffusion Models for Unpaired Image-to-image   Translation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-13/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fe227313b697f8ef6630d0cca633b67d.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-13  MuaLLM A Multimodal Large Language Model Agent for Circuit Design   Assistance with Hybrid Contextual Retrieval-Augmented Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32883.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
