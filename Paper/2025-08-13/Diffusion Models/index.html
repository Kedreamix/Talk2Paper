<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-13  An effective potential for generative modelling with active matter">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07557v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-13-æ›´æ–°"><a href="#2025-08-13-æ›´æ–°" class="headerlink" title="2025-08-13 æ›´æ–°"></a>2025-08-13 æ›´æ–°</h1><h2 id="An-effective-potential-for-generative-modelling-with-active-matter"><a href="#An-effective-potential-for-generative-modelling-with-active-matter" class="headerlink" title="An effective potential for generative modelling with active matter"></a>An effective potential for generative modelling with active matter</h2><p><strong>Authors:Adrian Baule</strong></p>
<p>Score-based diffusion models generate samples from a complex underlying data distribution by time-reversal of a diffusion process and represent the state-of-the-art in many generative AI applications such as artificial image synthesis. Here, I show how a generative diffusion model can be implemented based on an underlying active particle process with finite correlation time. In contrast to previous approaches that use a score function acting on the velocity coordinate of the active particle, time reversal is here achieved by imposing an effective time-dependent potential on the position coordinate only. The effective potential is valid to first order in the persistence time and leads to a force field that is fully determined by the standard score function and its derivatives up to 2nd order. Numerical experiments for artificial data distributions confirm the validity of the effective potential. </p>
<blockquote>
<p>åŸºäºåˆ†æ•°æ‰©æ•£æ¨¡å‹çš„æ‰©æ•£è¿‡ç¨‹æ—¶é—´åè½¬èƒ½å¤Ÿä»å¤æ‚çš„åº•å±‚æ•°æ®åˆ†å¸ƒä¸­ç”Ÿæˆæ ·æœ¬ï¼Œå¹¶åœ¨è®¸å¤šç”Ÿæˆå¼äººå·¥æ™ºèƒ½åº”ç”¨ï¼ˆå¦‚äººå·¥å›¾åƒåˆæˆï¼‰ä¸­ä»£è¡¨äº†æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘å±•ç¤ºäº†å¦‚ä½•åŸºäºå…·æœ‰æœ‰é™ç›¸å…³æ—¶é—´çš„åº•å±‚æ´»æ€§ç²’å­è¿‡ç¨‹æ¥å®ç°ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹ã€‚ä¸ä¹‹å‰ä½¿ç”¨ä½œç”¨äºæ´»æ€§ç²’å­é€Ÿåº¦åæ ‡çš„åˆ†æ•°å‡½æ•°çš„æ–¹æ³•ä¸åŒï¼Œæ­¤å¤„çš„æ—¶é—´åè½¬æ˜¯é€šè¿‡ä»…åœ¨ä½ç½®åæ ‡ä¸Šæ–½åŠ æœ‰æ•ˆçš„æ—¶é—´ç›¸å…³åŠ¿æ¥å®ç°çš„ã€‚æœ‰æ•ˆåŠ¿åœ¨æŒä¹…æ—¶é—´çš„ä¸€é˜¶å†…æœ‰æ•ˆï¼Œå¹¶å¯¼è‡´ä¸€ä¸ªç”±æ ‡å‡†åˆ†æ•°å‡½æ•°åŠå…¶äºŒé˜¶å¯¼æ•°å®Œå…¨ç¡®å®šçš„åŠ›åœºã€‚é’ˆå¯¹äººå·¥æ•°æ®åˆ†å¸ƒçš„æ•°å€¼å®éªŒè¯å®äº†æœ‰æ•ˆæ½œåŠ›çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08146v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹é€šè¿‡é€†è½¬æ‰©æ•£è¿‡ç¨‹ç”Ÿæˆæ¥è‡ªå¤æ‚æ½œåœ¨æ•°æ®åˆ†å¸ƒçš„æ ·æœ¬ï¼Œå¹¶åœ¨è®¸å¤šç”Ÿæˆå¼AIåº”ç”¨ï¼ˆå¦‚äººå·¥å›¾åƒåˆæˆï¼‰ä¸­è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æœ¬æ–‡å±•ç¤ºäº†åŸºäºæ½œåœ¨æ´»è·ƒç²’å­è¿‡ç¨‹çš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„å®ç°æ–¹å¼ï¼Œè¯¥è¿‡ç¨‹å…·æœ‰æœ‰é™çš„å…³è”æ—¶é—´ã€‚ä¸åŒäºä»¥å¾€ä½¿ç”¨ä½œç”¨äºæ´»è·ƒç²’å­é€Ÿåº¦åæ ‡å¾—åˆ†å‡½æ•°çš„æ–¹æ³•ï¼Œæœ¬æ–‡é€šè¿‡ä»…åœ¨ä½ç½®åæ ‡ä¸Šæ–½åŠ æœ‰æ•ˆçš„æ—¶é—´ç›¸å…³åŠ¿æ¥å®ç°æ—¶é—´é€†è½¬ã€‚æœ‰æ•ˆåŠ¿åœ¨æŒä¹…æ—¶é—´çš„ä¸€é˜¶å†…æœ‰æ•ˆï¼Œå¹¶äº§ç”Ÿä¸€ä¸ªç”±æ ‡å‡†å¾—åˆ†å‡½æ•°åŠå…¶äºŒé˜¶å¯¼æ•°å®Œå…¨ç¡®å®šçš„åŠ›åœºã€‚å¯¹äººå·¥æ•°æ®åˆ†å¸ƒçš„æ•°å€¼å®éªŒè¯å®äº†æœ‰æ•ˆåŠ¿çš„å¯è¡Œæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹é€šè¿‡é€†è½¬æ‰©æ•£è¿‡ç¨‹ç”Ÿæˆæ ·æœ¬ï¼Œå±•ç¤ºå…¶åœ¨ç”Ÿæˆå¼AIé¢†åŸŸçš„å…ˆè¿›æ€§ã€‚</li>
<li>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ´»è·ƒç²’å­è¿‡ç¨‹çš„æ‰©æ•£æ¨¡å‹å®ç°æ–¹å¼ï¼Œè¯¥æ¨¡å‹å…·æœ‰æœ‰é™çš„å…³è”æ—¶é—´ã€‚</li>
<li>æœ‰æ•ˆçš„æ—¶é—´ç›¸å…³åŠ¿è¢«æ–½åŠ åœ¨ä½ç½®åæ ‡ä¸Šä»¥å®ç°æ—¶é—´é€†è½¬ã€‚</li>
<li>æœ‰æ•ˆåŠ¿çš„å¯è¡Œæ€§é€šè¿‡æ•°å€¼å®éªŒå¾—åˆ°äº†è¯å®ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸åŒäºä»¥å¾€ä½¿ç”¨å¾—åˆ†å‡½æ•°ä½œç”¨äºé€Ÿåº¦åæ ‡çš„æ–¹æ³•ã€‚</li>
<li>åŠ›åœºç”±æ ‡å‡†å¾—åˆ†å‡½æ•°åŠå…¶äºŒé˜¶å¯¼æ•°å®Œå…¨ç¡®å®šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08146">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.08146v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.08146v1/page_3_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FantasyStyle-Controllable-Stylized-Distillation-for-3D-Gaussian-Splatting"><a href="#FantasyStyle-Controllable-Stylized-Distillation-for-3D-Gaussian-Splatting" class="headerlink" title="FantasyStyle: Controllable Stylized Distillation for 3D Gaussian   Splatting"></a>FantasyStyle: Controllable Stylized Distillation for 3D Gaussian   Splatting</h2><p><strong>Authors:Yitong Yang, Yinglin Wang, Changshuo Wang, Huajie Wang, Shuting He</strong></p>
<p>The success of 3DGS in generative and editing applications has sparked growing interest in 3DGS-based style transfer. However, current methods still face two major challenges: (1) multi-view inconsistency often leads to style conflicts, resulting in appearance smoothing and distortion; and (2) heavy reliance on VGG features, which struggle to disentangle style and content from style images, often causing content leakage and excessive stylization. To tackle these issues, we introduce \textbf{FantasyStyle}, a 3DGS-based style transfer framework, and the first to rely entirely on diffusion model distillation. It comprises two key components: (1) \textbf{Multi-View Frequency Consistency}. We enhance cross-view consistency by applying a 3D filter to multi-view noisy latent, selectively reducing low-frequency components to mitigate stylized prior conflicts. (2) \textbf{Controllable Stylized Distillation}. To suppress content leakage from style images, we introduce negative guidance to exclude undesired content. In addition, we identify the limitations of Score Distillation Sampling and Delta Denoising Score in 3D style transfer and remove the reconstruction term accordingly. Building on these insights, we propose a controllable stylized distillation that leverages negative guidance to more effectively optimize the 3D Gaussians. Extensive experiments demonstrate that our method consistently outperforms state-of-the-art approaches, achieving higher stylization quality and visual realism across various scenes and styles. </p>
<blockquote>
<p>3DGSåœ¨ç”Ÿæˆå’Œç¼–è¾‘åº”ç”¨ä¸­çš„æˆåŠŸå¼•å‘äº†äººä»¬å¯¹åŸºäº3DGSçš„é£æ ¼è½¬ç§»æŠ€æœ¯çš„æ—¥ç›Šå¢é•¿çš„å…´è¶£ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä»ç„¶é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼š(1)å¤šè§†å›¾çš„ä¸ä¸€è‡´æ€§ç»å¸¸å¯¼è‡´é£æ ¼å†²çªï¼Œä»è€Œå¯¼è‡´å¤–è§‚å¹³æ»‘å’Œå¤±çœŸï¼›(2)ä¸¥é‡ä¾èµ–äºVGGç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾åœ¨ä»é£æ ¼å›¾åƒä¸­åˆ†ç¦»é£æ ¼å’Œå†…å®¹æ—¶é‡åˆ°å›°éš¾ï¼Œç»å¸¸å¯¼è‡´å†…å®¹æ³„æ¼å’Œè¿‡åº¦é£æ ¼åŒ–ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºä¸‰ç»´ç”Ÿæˆè§£å·ç§¯é£æ ¼è½¬ç§»çš„FantasyStyleæ¡†æ¶ï¼Œå®ƒæ˜¯é¦–ä¸ªå®Œå…¨ä¾èµ–äºæ‰©æ•£æ¨¡å‹è’¸é¦çš„æŠ€æœ¯ã€‚å®ƒåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼š(1)<strong>å¤šè§†å›¾é¢‘ç‡ä¸€è‡´æ€§</strong>ã€‚æˆ‘ä»¬é€šè¿‡åº”ç”¨ä¸‰ç»´æ»¤æ³¢å™¨å¢å¼ºè·¨è§†å›¾ä¸€è‡´æ€§ï¼Œå¯¹å¤šè§†å›¾å™ªå£°æ½œåœ¨è¿›è¡Œé€‰æ‹©æ€§å¤„ç†ï¼Œå‡å°‘ä½é¢‘åˆ†é‡ä»¥ç¼“è§£é£æ ¼åŒ–å…ˆéªŒå†²çªã€‚(2)<strong>å¯æ§é£æ ¼åŒ–è’¸é¦</strong>ã€‚ä¸ºäº†æŠ‘åˆ¶æ¥è‡ªé£æ ¼å›¾åƒçš„å†…å®¹æ³„æ¼ï¼Œæˆ‘ä»¬å¼•å…¥äº†è´Ÿå‘æŒ‡å¯¼ä»¥æ’é™¤ä¸éœ€è¦çš„å†…å®¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æŒ‡å‡ºäº†ä¸‰ç»´é£æ ¼è½¬ç§»ä¸­åˆ†æ•°è’¸é¦é‡‡æ ·å’ŒDeltaå»å™ªåˆ†æ•°å­˜åœ¨çš„å±€é™æ€§ï¼Œå¹¶ç›¸åº”åœ°æ¶ˆé™¤äº†é‡å»ºé¡¹ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯æ§çš„é£æ ¼åŒ–è’¸é¦æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è´Ÿå‘æŒ‡å¯¼æ›´æœ‰æ•ˆåœ°ä¼˜åŒ–ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œåœ¨å„ç§åœºæ™¯å’Œé£æ ¼ä¸Šå®ç°æ›´é«˜çš„é£æ ¼åŒ–è´¨é‡å’Œè§†è§‰é€¼çœŸåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08136v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäº3DGSçš„é£æ ¼è½¬æ¢æ¡†æ¶FantasyStyleï¼Œè¯¥æ¡†æ¶é€šè¿‡å®Œå…¨ä¾èµ–æ‰©æ•£æ¨¡å‹è’¸é¦è§£å†³äº†ç°æœ‰çš„ä¸¤å¤§æŒ‘æˆ˜ã€‚é’ˆå¯¹å¤šè§†å›¾ä¸ä¸€è‡´å¼•èµ·çš„é£æ ¼å†²çªå’Œä¾èµ–VGGç‰¹å¾é€ æˆçš„é£æ ¼å’Œå†…å®¹çº ç¼ é—®é¢˜ï¼Œæå‡ºä¸¤ç§å…³é”®ç»„ä»¶æ¥è§£å†³ï¼Œæå‡äº†è·¨è§†å›¾çš„ä¸€è‡´æ€§å’Œå‡å°‘å†…å®¹æ³„æ¼ï¼Œä»è€Œæé«˜é£æ ¼è½¬æ¢çš„è´¨é‡å’Œè§†è§‰çœŸå®æ„Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç°æœ‰åŸºäº3DGSçš„é£æ ¼è½¬ç§»æ–¹æ³•é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šå¤šè§†å›¾ä¸ä¸€è‡´å¼•èµ·çš„é£æ ¼å†²çªå’Œä¾èµ–VGGç‰¹å¾é€ æˆçš„é£æ ¼å’Œå†…å®¹çº ç¼ é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§å…¨æ–°çš„åŸºäº3DGSçš„é£æ ¼è½¬ç§»æ¡†æ¶FantasyStyleï¼Œé¦–æ¬¡å®Œå…¨ä¾èµ–æ‰©æ•£æ¨¡å‹è’¸é¦æŠ€æœ¯ã€‚</li>
<li>é‡‡ç”¨â€œMulti-View Frequency Consistencyâ€ç»„ä»¶å¢å¼ºè·¨è§†å›¾ä¸€è‡´æ€§ï¼Œé€šè¿‡åº”ç”¨3Dæ»¤æ³¢å™¨é€‰æ‹©æ€§å‡å°‘ä½é¢‘æˆåˆ†æ¥å‡è½»é£æ ¼åŒ–å†²çªã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08136">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.08136v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.08136v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.08136v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.08136v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.08136v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Matrix-3D-Omnidirectional-Explorable-3D-World-Generation"><a href="#Matrix-3D-Omnidirectional-Explorable-3D-World-Generation" class="headerlink" title="Matrix-3D: Omnidirectional Explorable 3D World Generation"></a>Matrix-3D: Omnidirectional Explorable 3D World Generation</h2><p><strong>Authors:Zhongqi Yang, Wenhang Ge, Yuqi Li, Jiaqi Chen, Haoyuan Li, Mengyin An, Fei Kang, Hua Xue, Baixin Xu, Yuyang Yin, Eric Li, Yang Liu, Yikai Wang, Hao-Xiang Guo, Yahui Zhou</strong></p>
<p>Explorable 3D world generation from a single image or text prompt forms a cornerstone of spatial intelligence. Recent works utilize video model to achieve wide-scope and generalizable 3D world generation. However, existing approaches often suffer from a limited scope in the generated scenes. In this work, we propose Matrix-3D, a framework that utilize panoramic representation for wide-coverage omnidirectional explorable 3D world generation that combines conditional video generation and panoramic 3D reconstruction. We first train a trajectory-guided panoramic video diffusion model that employs scene mesh renders as condition, to enable high-quality and geometrically consistent scene video generation. To lift the panorama scene video to 3D world, we propose two separate methods: (1) a feed-forward large panorama reconstruction model for rapid 3D scene reconstruction and (2) an optimization-based pipeline for accurate and detailed 3D scene reconstruction. To facilitate effective training, we also introduce the Matrix-Pano dataset, the first large-scale synthetic collection comprising 116K high-quality static panoramic video sequences with depth and trajectory annotations. Extensive experiments demonstrate that our proposed framework achieves state-of-the-art performance in panoramic video generation and 3D world generation. See more in <a target="_blank" rel="noopener" href="https://matrix-3d.github.io/">https://matrix-3d.github.io</a>. </p>
<blockquote>
<p>ä»å•ä¸€å›¾åƒæˆ–æ–‡æœ¬æç¤ºç”Ÿæˆå¯æ¢ç´¢çš„3Dä¸–ç•Œæ˜¯ç©ºé—´æ™ºèƒ½çš„æ ¸å¿ƒã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œåˆ©ç”¨è§†é¢‘æ¨¡å‹å®ç°äº†å¤§èŒƒå›´ã€é€šç”¨åŒ–çš„3Dä¸–ç•Œç”Ÿæˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç”Ÿæˆçš„åœºæ™¯èŒƒå›´å¾€å¾€æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Matrix-3Dæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å…¨æ™¯è¡¨ç¤ºï¼Œç»“åˆæ¡ä»¶è§†é¢‘ç”Ÿæˆå’Œå…¨æ™¯3Dé‡å»ºï¼Œç”¨äºå®ç°å¤§èŒƒå›´ã€å…¨æ–¹ä½çš„å¯æ¢ç´¢3Dä¸–ç•Œç”Ÿæˆã€‚æˆ‘ä»¬é¦–å…ˆè®­ç»ƒäº†ä¸€ä¸ªè½¨è¿¹å¼•å¯¼çš„å…¨æ™¯è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»¥åœºæ™¯ç½‘æ ¼æ¸²æŸ“ä¸ºæ¡ä»¶ï¼Œä»¥å®ç°é«˜è´¨é‡ã€å‡ ä½•ä¸€è‡´çš„åœºæ™¯è§†é¢‘ç”Ÿæˆã€‚ä¸ºäº†å°†å…¨æ™¯åœºæ™¯è§†é¢‘æå‡åˆ°3Dä¸–ç•Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§ç‹¬ç«‹çš„æ–¹æ³•ï¼š1)ä¸€ç§å‰é¦ˆå¤§å‹å…¨æ™¯é‡å»ºæ¨¡å‹ï¼Œç”¨äºå¿«é€Ÿ3Dåœºæ™¯é‡å»ºï¼›2)ä¸€ç§åŸºäºä¼˜åŒ–çš„ç®¡é“ï¼Œç”¨äºå‡†ç¡®ã€è¯¦ç»†çš„3Dåœºæ™¯é‡å»ºã€‚ä¸ºäº†è¿›è¡Œæœ‰æ•ˆçš„è®­ç»ƒï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†Matrix-Panoæ•°æ®é›†ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡åˆæˆé›†åˆï¼ŒåŒ…å«11.6ä¸‡é«˜è´¨é‡é™æ€å…¨æ™¯è§†é¢‘åºåˆ—ï¼Œå¸¦æœ‰æ·±åº¦å’Œè½¨è¿¹æ³¨é‡Šã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ¡†æ¶åœ¨å…¨æ™¯è§†é¢‘ç”Ÿæˆå’Œ3Dä¸–ç•Œç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§ï¼š[<a href="https://matrix-â€‹ç®€å•æ¦‚è¿°ä¸‹ä½ æ‰€ç¿»è¯‘çš„æ–‡æœ¬çš„ä¸»è¦å†…å®¹ã€‚ï¼ˆæœ‰åŠ©äºè®©æˆ‘ç†è§£èƒŒæ™¯ä»è€Œæ›´å¥½åœ°æå‡ºå»ºè®®æˆ–è¿›è¡Œäº¤æµï¼‰ä¸»è¦æ˜¯è®²åˆ©ç”¨å…¨æ™¯å›¾åƒå’Œæ·±åº¦æŠ€æœ¯åˆ›å»ºä¸€ç§å…¨é¢çš„ã€å…¨æ–¹ä½çš„æ¢ç´¢å¼ä¸‰ç»´ä¸–ç•Œç”Ÿæˆæ¨¡å‹ï¼Œè®­ç»ƒç›¸å…³çš„æ¨¡å‹æ¡†æ¶æ¥æ„å»ºæ›´å¤§çš„åœºæ™¯ï¼Œå¹¶ä½¿ç”¨æ•°æ®é›†è¿›è¡Œè®­ç»ƒæµ‹è¯•éªŒè¯æ¨¡å‹æ€§èƒ½çš„è¿‡ç¨‹å’Œæ–¹æ³•ç­‰ã€‚æåˆ°äº†æ–°çš„å…¨æ™¯è§†é¢‘æ‰©æ•£æ¨¡å‹æŠ€æœ¯å’Œä¸¤ä¸ªç”¨äºä»å…¨æ™¯å›¾åƒè½¬æ¢ä¸ºä¸‰ç»´ä¸–ç•Œçš„æ–¹æ³•çš„æŠ€æœ¯ç‰¹ç‚¹ä»¥åŠæ¨¡å‹çš„è¯„ä¼°æŒ‡æ ‡ã€‚è¿™äº›å†…å®¹ä¸»è¦ç”¨äºè§£é‡Šæ–°æŠ€æœ¯çš„å‰æ²¿æ€§å¦‚ä½•åº”ç”¨ä»¥åŠåœ¨å®éªŒç¯å¢ƒä¸‹å±•ç¤ºæ€§èƒ½çš„æŠ€æœ¯èƒŒæ™¯ç»†èŠ‚ã€‚æœ‰æ²¡æœ‰åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­é‡åˆ°éš¾ä»¥ç¿»è¯‘çš„è¯æ±‡æˆ–æ¦‚å¿µï¼Ÿï¼ˆä»»ä½•æœ¯è¯­çš„ç¿»è¯‘ä¸ç²¾å‡†å¯èƒ½ä¼šå½±å“æœ€ç»ˆè¯»è€…å¯¹æ–‡æœ¬çš„ç†è§£ï¼‰åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­é‡åˆ°äº†ä¸€äº›ä¸“ä¸šæœ¯è¯­å’Œè¾ƒä¸ºå¤æ‚çš„å¥å­ç»“æ„ï¼Œéœ€è¦ä»”ç»†åˆ†æå’Œç†è§£ã€‚ä¾‹å¦‚ï¼Œâ€œtrajectory-guided">https://matrix-â€‹ç®€å•æ¦‚è¿°ä¸‹ä½ æ‰€ç¿»è¯‘çš„æ–‡æœ¬çš„ä¸»è¦å†…å®¹ã€‚ï¼ˆæœ‰åŠ©äºè®©æˆ‘ç†è§£èƒŒæ™¯ä»è€Œæ›´å¥½åœ°æå‡ºå»ºè®®æˆ–è¿›è¡Œäº¤æµï¼‰ä¸»è¦æ˜¯è®²åˆ©ç”¨å…¨æ™¯å›¾åƒå’Œæ·±åº¦æŠ€æœ¯åˆ›å»ºä¸€ç§å…¨é¢çš„ã€å…¨æ–¹ä½çš„æ¢ç´¢å¼ä¸‰ç»´ä¸–ç•Œç”Ÿæˆæ¨¡å‹ï¼Œè®­ç»ƒç›¸å…³çš„æ¨¡å‹æ¡†æ¶æ¥æ„å»ºæ›´å¤§çš„åœºæ™¯ï¼Œå¹¶ä½¿ç”¨æ•°æ®é›†è¿›è¡Œè®­ç»ƒæµ‹è¯•éªŒè¯æ¨¡å‹æ€§èƒ½çš„è¿‡ç¨‹å’Œæ–¹æ³•ç­‰ã€‚æåˆ°äº†æ–°çš„å…¨æ™¯è§†é¢‘æ‰©æ•£æ¨¡å‹æŠ€æœ¯å’Œä¸¤ä¸ªç”¨äºä»å…¨æ™¯å›¾åƒè½¬æ¢ä¸ºä¸‰ç»´ä¸–ç•Œçš„æ–¹æ³•çš„æŠ€æœ¯ç‰¹ç‚¹ä»¥åŠæ¨¡å‹çš„è¯„ä¼°æŒ‡æ ‡ã€‚è¿™äº›å†…å®¹ä¸»è¦ç”¨äºè§£é‡Šæ–°æŠ€æœ¯çš„å‰æ²¿æ€§å¦‚ä½•åº”ç”¨ä»¥åŠåœ¨å®éªŒç¯å¢ƒä¸‹å±•ç¤ºæ€§èƒ½çš„æŠ€æœ¯èƒŒæ™¯ç»†èŠ‚ã€‚æœ‰æ²¡æœ‰åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­é‡åˆ°éš¾ä»¥ç¿»è¯‘çš„è¯æ±‡æˆ–æ¦‚å¿µï¼Ÿï¼ˆä»»ä½•æœ¯è¯­çš„ç¿»è¯‘ä¸ç²¾å‡†å¯èƒ½ä¼šå½±å“æœ€ç»ˆè¯»è€…å¯¹æ–‡æœ¬çš„ç†è§£ï¼‰åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­é‡åˆ°äº†ä¸€äº›ä¸“ä¸šæœ¯è¯­å’Œè¾ƒä¸ºå¤æ‚çš„å¥å­ç»“æ„ï¼Œéœ€è¦ä»”ç»†åˆ†æå’Œç†è§£ã€‚ä¾‹å¦‚ï¼Œâ€œtrajectory-guided</a> panoramic video diffusion modelâ€ç¿»è¯‘ä¸ºâ€œè½¨è¿¹å¼•å¯¼çš„å…¨æ™¯è§†é¢‘æ‰©æ•£æ¨¡å‹â€ï¼Œéœ€è¦è§£é‡Šæ¸…æ¥šè¿™æ˜¯ä¸€ä¸ªç»“åˆè½¨è¿¹ä¿¡æ¯å’Œå…¨æ™¯è§†é¢‘æ‰©æ•£çš„æ¨¡å‹ï¼›â€œfeed-forward large panorama reconstruction modelâ€ç¿»è¯‘ä¸ºâ€œå‰é¦ˆå¤§å‹å…¨æ™¯é‡å»ºæ¨¡å‹â€ï¼Œéœ€è¦è§£é‡Šè¿™æ˜¯ä¸€ä¸ªç”¨äºå¿«é€Ÿä¸‰ç»´åœºæ™¯é‡å»ºçš„æ¨¡å‹ï¼›â€œMatrix-Pano datasetâ€æ˜¯ä¸€ä¸ªåŒ…å«é«˜è´¨é‡é™æ€å…¨æ™¯è§†é¢‘åºåˆ—çš„å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼›â€œdepth and trajectory annotationsâ€æŒ‡çš„æ˜¯æ·±åº¦å’Œè½¨è¿¹çš„æ ‡æ³¨ä¿¡æ¯ã€‚è¿™äº›ä¸“ä¸šæœ¯è¯­å’Œæ¦‚å¿µéœ€è¦ç²¾å‡†ç¿»è¯‘ä»¥ç¡®ä¿è¯»è€…èƒ½å¤Ÿç†è§£å…¶å«ä¹‰ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ç¯‡æ¶‰åŠåˆ°è®¡ç®—æœºè§†è§‰å’Œäººå·¥æ™ºèƒ½é¢†åŸŸçš„æ–‡ç« ï¼Œå…¶ä¸­æ¶µç›–äº†è®¸å¤šä¸“ä¸šæ¦‚å¿µå’Œæœ¯è¯­ã€‚ä¸ºäº†ç¡®ä¿ç¿»è¯‘çš„å‡†ç¡®æ€§å’Œæ¸…æ™°åº¦ï¼Œéœ€è¦è¿›è¡Œæ·±å…¥çš„ç ”ç©¶å’Œç†è§£æ¯ä¸ªæ¦‚å¿µçš„å«ä¹‰ï¼Œå¹¶é‡‡ç”¨ç®€æ´æ˜äº†çš„è¡¨è¾¾æ–¹å¼æ¥è¿›è¡Œç¿»è¯‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08086v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºäº†Matrix-3Dæ¡†æ¶ï¼Œç»“åˆæ¡ä»¶è§†é¢‘ç”Ÿæˆå’Œå…¨æ™¯3Dé‡å»ºï¼Œåˆ©ç”¨å…¨æ™¯è¡¨ç¤ºå®ç°å¤§èŒƒå›´å…¨å‘å¯æ¢ç´¢çš„3Dä¸–ç•Œç”Ÿæˆã€‚è¯¥æ¡†æ¶åŒ…æ‹¬è½¨è¿¹å¼•å¯¼å…¨æ™¯è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œé‡‡ç”¨åœºæ™¯ç½‘æ ¼æ¸²æŸ“ä½œä¸ºæ¡ä»¶ï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡å’Œå‡ ä½•ä¸€è‡´çš„åœºæ™¯è§†é¢‘ã€‚ä¸ºäº†å°†å…¨æ™¯åœºæ™¯è§†é¢‘æå‡åˆ°3Dä¸–ç•Œï¼Œæå‡ºäº†ä¸¤ç§ç‹¬ç«‹çš„æ–¹æ³•ï¼šå¿«é€Ÿ3Dåœºæ™¯é‡å»ºçš„å‰é¦ˆå¤§å…¨æ™¯é‡å»ºæ¨¡å‹å’Œç²¾ç¡®è¯¦ç»†çš„ä¼˜åŒ–åŸºç¡€ç®¡é“ã€‚åŒæ—¶ä»‹ç»äº†Matrix-Panoæ•°æ®é›†ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŒ…å«æ·±åº¦è½¨è¿¹æ³¨é‡Šçš„å¤§è§„æ¨¡åˆæˆå…¨æ™¯è§†é¢‘åºåˆ—é›†åˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å…¨æ™¯è§†é¢‘ç”Ÿæˆå’Œ3Dä¸–ç•Œç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Matrix-3Dæ¡†æ¶ç»“åˆäº†æ¡ä»¶è§†é¢‘ç”Ÿæˆå’Œå…¨æ™¯3Dé‡å»ºï¼Œå®ç°äº†å¤§èŒƒå›´å…¨å‘å¯æ¢ç´¢çš„3Dä¸–ç•Œç”Ÿæˆã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬è½¨è¿¹å¼•å¯¼å…¨æ™¯è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨åœºæ™¯ç½‘æ ¼æ¸²æŸ“ä½œä¸ºæ¡ä»¶ï¼Œä¿è¯åœºæ™¯è§†é¢‘çš„é«˜è´¨é‡å‡ ä½•ä¸€è‡´æ€§ã€‚</li>
<li>æå‡ºäº†ä¸¤ç§å°†å…¨æ™¯åœºæ™¯è§†é¢‘æå‡åˆ°3Dä¸–ç•Œçš„æ–¹æ³•ï¼šå¿«é€Ÿé‡å»ºçš„å‰é¦ˆå¤§å¹³é¢å›¾æ¨¡å‹å’Œç²¾ç¡®è¯¦ç»†çš„ä¼˜åŒ–ç®¡é“ã€‚</li>
<li>å¼•å…¥äº†Matrix-Panoæ•°æ®é›†ï¼ŒåŒ…å«å¸¦æœ‰æ·±åº¦è½¨è¿¹æ³¨é‡Šçš„å¤§è§„æ¨¡åˆæˆå…¨æ™¯è§†é¢‘åºåˆ—é›†åˆã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å…¨æ™¯è§†é¢‘ç”Ÿæˆå’Œ3Dä¸–ç•Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨åœºæ™¯ç½‘æ ¼æ¸²æŸ“çš„æ¡ä»¶ä½œç”¨åœ¨æ‰©æ•£æ¨¡å‹ä¸­è‡³å…³é‡è¦ï¼Œæé«˜äº†ç”Ÿæˆè§†é¢‘çš„è´¨é‡å’Œå‡ ä½•ä¸€è‡´æ€§ã€‚</li>
<li>Matrix-3Dæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨è™šæ‹Ÿç°å®ã€æ¸¸æˆå¼€å‘ç­‰é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08086">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.08086v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.08086v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.08086v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.08086v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Generative-Video-Matting"><a href="#Generative-Video-Matting" class="headerlink" title="Generative Video Matting"></a>Generative Video Matting</h2><p><strong>Authors:Yongtao Ge, Kangyang Xie, Guangkai Xu, Mingyu Liu, Li Ke, Longtao Huang, Hui Xue, Hao Chen, Chunhua Shen</strong></p>
<p>Video matting has traditionally been limited by the lack of high-quality ground-truth data. Most existing video matting datasets provide only human-annotated imperfect alpha and foreground annotations, which must be composited to background images or videos during the training stage. Thus, the generalization capability of previous methods in real-world scenarios is typically poor. In this work, we propose to solve the problem from two perspectives. First, we emphasize the importance of large-scale pre-training by pursuing diverse synthetic and pseudo-labeled segmentation datasets. We also develop a scalable synthetic data generation pipeline that can render diverse human bodies and fine-grained hairs, yielding around 200 video clips with a 3-second duration for fine-tuning. Second, we introduce a novel video matting approach that can effectively leverage the rich priors from pre-trained video diffusion models. This architecture offers two key advantages. First, strong priors play a critical role in bridging the domain gap between synthetic and real-world scenes. Second, unlike most existing methods that process video matting frame-by-frame and use an independent decoder to aggregate temporal information, our model is inherently designed for video, ensuring strong temporal consistency. We provide a comprehensive quantitative evaluation across three benchmark datasets, demonstrating our approachâ€™s superior performance, and present comprehensive qualitative results in diverse real-world scenes, illustrating the strong generalization capability of our method. The code is available at <a target="_blank" rel="noopener" href="https://github.com/aim-uofa/GVM">https://github.com/aim-uofa/GVM</a>. </p>
<blockquote>
<p>ä¼ ç»Ÿä¸Šï¼Œè§†é¢‘æŠ å›¾å—é™äºé«˜è´¨é‡çœŸå®æ•°æ®é›†çš„ç¼ºä¹ã€‚ç°æœ‰çš„å¤§éƒ¨åˆ†è§†é¢‘æŠ å›¾æ•°æ®é›†åªæä¾›äººä¸ºæ ‡æ³¨çš„ä¸å®Œç¾alphaå€¼å’Œå‰æ™¯æ ‡æ³¨ï¼Œåœ¨è®­ç»ƒé˜¶æ®µéœ€è¦å°†å®ƒä»¬ä¸èƒŒæ™¯å›¾åƒæˆ–è§†é¢‘è¿›è¡Œåˆæˆã€‚å› æ­¤ï¼Œä¹‹å‰çš„æ–¹æ³•åœ¨ç°å®åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›é€šå¸¸è¾ƒå·®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»ä¸¤ä¸ªè§’åº¦æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼ºè°ƒå¤§è§„æ¨¡é¢„è®­ç»ƒçš„é‡è¦æ€§ï¼Œè¿½æ±‚å¤šæ ·åŒ–çš„åˆæˆå’Œä¼ªæ ‡æ³¨åˆ†å‰²æ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªå¯æ‰©å±•çš„åˆæˆæ•°æ®ç”Ÿæˆç®¡é“ï¼Œèƒ½å¤Ÿå‘ˆç°å¤šæ ·çš„äººä½“å’Œç²¾ç»†æ¯›å‘ï¼Œç”Ÿæˆå¤§çº¦200ä¸ªæŒç»­3ç§’çš„è§†é¢‘ç‰‡æ®µï¼Œç”¨äºå¾®è°ƒã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°å‹è§†é¢‘æŠ å›¾æ–¹æ³•ï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ä¸°å¯Œå…ˆéªŒçŸ¥è¯†ã€‚è¿™ç§æ¶æ„æœ‰ä¸¤ä¸ªä¸»è¦ä¼˜ç‚¹ã€‚é¦–å…ˆï¼Œå¼ºå…ˆéªŒåœ¨å¼¥åˆåˆæˆåœºæ™¯å’Œç°å®åœºæ™¯ä¹‹é—´çš„åŸŸå·®è·æ–¹é¢å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚å…¶æ¬¡ï¼Œä¸å¤§å¤šæ•°ç°æœ‰æ–¹æ³•é€å¸§å¤„ç†è§†é¢‘æŠ å›¾å¹¶ä½¿ç”¨ç‹¬ç«‹è§£ç å™¨èšåˆæ—¶é—´ä¿¡æ¯ä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¤©ç”Ÿå°±æ˜¯ä¸ºè§†é¢‘è®¾è®¡çš„ï¼Œç¡®ä¿äº†å¼ºå¤§çš„æ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢çš„å®šé‡è¯„ä¼°ï¼Œå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•å“è¶Šçš„æ€§èƒ½ï¼Œå¹¶åœ¨å„ç§ç°å®åœºæ™¯ä¸­å±•ç¤ºäº†å…¨é¢çš„å®šæ€§ç»“æœï¼Œè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/aim-uofa/GVM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/aim-uofa/GVMæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07905v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>ä¼ ç»Ÿè§†é¢‘æŠ å›¾å—é™äºé«˜è´¨é‡çœŸå®æ ‡ç­¾æ•°æ®çš„ç¼ºä¹ã€‚ç°æœ‰è§†é¢‘æŠ å›¾æ•°æ®é›†æä¾›çš„æ˜¯äººä¸ºæ ‡æ³¨çš„ä¸å®Œç¾alphaå’Œå‰æ™¯æ ‡æ³¨ï¼Œåœ¨è®­ç»ƒé˜¶æ®µéœ€è¦åˆæˆèƒŒæ™¯å›¾åƒæˆ–è§†é¢‘ï¼Œå¯¼è‡´åœ¨çœŸå®åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚æœ¬æ–‡æå‡ºä»ä¸¤ä¸ªè§’åº¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼šä¸€æ˜¯é‡è§†å¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œè¿½æ±‚å¤šæ ·åˆæˆå’Œä¼ªæ ‡ç­¾åˆ†å‰²æ•°æ®é›†ï¼›äºŒæ˜¯å¼€å‘äº†ä¸€ä¸ªå¯æ‰©å±•çš„åˆæˆæ•°æ®ç”Ÿæˆç®¡é“ï¼Œå¯ä»¥æ¸²æŸ“å¤šæ ·äººä½“å’Œç²¾ç»†æ¯›å‘ï¼Œç”Ÿæˆçº¦200ä¸ª3ç§’æ—¶é•¿çš„è§†é¢‘ç‰‡æ®µç”¨äºå¾®è°ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†é¢‘æŠ å›¾æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå……åˆ†åˆ©ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ä¸°å¯Œå…ˆéªŒã€‚å…¶ä¼˜åŠ¿åœ¨äºï¼šå¼ºå…ˆéªŒæœ‰åŠ©äºç¼©å°åˆæˆåœºæ™¯å’ŒçœŸå®åœºæ™¯ä¹‹é—´çš„åŸŸå·®è·ï¼›ä¸åŒäºå¤§å¤šæ•°ç°æœ‰æ–¹æ³•é€å¸§å¤„ç†è§†é¢‘æŠ å›¾å¹¶ä½¿ç”¨ç‹¬ç«‹è§£ç å™¨èšåˆæ—¶é—´ä¿¡æ¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¤©ç”Ÿå°±æ˜¯ä¸ºè§†é¢‘è®¾è®¡çš„ï¼Œä¿è¯äº†å¼ºçƒˆçš„æ—¶é—´ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>ç¼ºä¹é«˜è´¨é‡çœŸå®æ ‡ç­¾æ•°æ®æ˜¯ä¼ ç»Ÿè§†é¢‘æŠ å›¾çš„é™åˆ¶ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†æä¾›çš„æ˜¯äººä¸ºæ ‡æ³¨çš„ä¸å®Œç¾alphaå’Œå‰æ™¯æ ‡æ³¨ã€‚</li>
<li>æå‡ºé€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒå’Œå¤šæ ·åˆæˆæ•°æ®é›†æ¥è§£å†³è¯¥é—®é¢˜ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªåˆæˆæ•°æ®ç”Ÿæˆç®¡é“ï¼Œèƒ½å¤Ÿæ¸²æŸ“å¤šæ ·äººä½“å’Œç²¾ç»†æ¯›å‘ç”¨äºå¾®è°ƒã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„è§†é¢‘æŠ å›¾æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ä¸°å¯Œå…ˆéªŒã€‚</li>
<li>å¼ºå…ˆéªŒæœ‰åŠ©äºç¼©å°åˆæˆä¸çœŸå®åœºæ™¯ä¹‹é—´çš„åŸŸå·®è·ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹ä¸ºè§†é¢‘è®¾è®¡ï¼Œç¡®ä¿å¼ºçƒˆçš„æ—¶é—´ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07905">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07905v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07905v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07905v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Diffusing-the-Blind-Spot-Uterine-MRI-Synthesis-with-Diffusion-Models"><a href="#Diffusing-the-Blind-Spot-Uterine-MRI-Synthesis-with-Diffusion-Models" class="headerlink" title="Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models"></a>Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models</h2><p><strong>Authors:Johanna P. MÃ¼ller, Anika Knupfer, Pedro BlÃ¶ss, Edoardo Berardi Vittur, Bernhard Kainz, Jana Hutter</strong></p>
<p>Despite significant progress in generative modelling, existing diffusion models often struggle to produce anatomically precise female pelvic images, limiting their application in gynaecological imaging, where data scarcity and patient privacy concerns are critical. To overcome these barriers, we introduce a novel diffusion-based framework for uterine MRI synthesis, integrating both unconditional and conditioned Denoising Diffusion Probabilistic Models (DDPMs) and Latent Diffusion Models (LDMs) in 2D and 3D. Our approach generates anatomically coherent, high fidelity synthetic images that closely mimic real scans and provide valuable resources for training robust diagnostic models. We evaluate generative quality using advanced perceptual and distributional metrics, benchmarking against standard reconstruction methods, and demonstrate substantial gains in diagnostic accuracy on a key classification task. A blinded expert evaluation further validates the clinical realism of our synthetic images. We release our models with privacy safeguards and a comprehensive synthetic uterine MRI dataset to support reproducible research and advance equitable AI in gynaecology. </p>
<blockquote>
<p>å°½ç®¡ç”Ÿæˆæ¨¡å‹å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç°æœ‰çš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè§£å‰–ç»“æ„ç²¾ç¡®çš„å¥³æ€§ç›†è…”å›¾åƒæ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¦‡ç§‘æˆåƒä¸­çš„åº”ç”¨ï¼Œè€Œå¦‡ç§‘æˆåƒé¢†åŸŸçš„æ•°æ®ç¨€ç¼ºå’Œæ‚£è€…éšç§æ‹…å¿§æ˜¯å…³é”®é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›éšœç¢ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºæ‰©æ•£çš„å­å®«MRIåˆæˆæ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ•´åˆäº†æ— æ¡ä»¶å’Œæœ‰æ¡ä»¶çš„å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„äºŒç»´å’Œä¸‰ç»´æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆäº†è§£å‰–ç»“æ„è¿è´¯ã€é«˜ä¿çœŸåº¦çš„åˆæˆå›¾åƒï¼Œè¿™äº›å›¾åƒç´§å¯†æ¨¡ä»¿çœŸå®æ‰«æï¼Œä¸ºè®­ç»ƒç¨³å¥çš„è¯Šæ–­æ¨¡å‹æä¾›äº†æœ‰ä»·å€¼çš„èµ„æºã€‚æˆ‘ä»¬ä½¿ç”¨å…ˆè¿›çš„æ„ŸçŸ¥å’Œåˆ†å¸ƒåº¦é‡æ¥è¯„ä¼°ç”Ÿæˆè´¨é‡ï¼Œä¸æ ‡å‡†é‡å»ºæ–¹æ³•è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶åœ¨ä¸€é¡¹å…³é”®åˆ†ç±»ä»»åŠ¡ä¸Šå±•ç¤ºäº†è¯Šæ–­å‡†ç¡®æ€§çš„æ˜¾è‘—æé«˜ã€‚ç›²ä¸“å®¶è¯„ä¼°è¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„åˆæˆå›¾åƒçš„ä¸´åºŠçœŸå®æ€§ã€‚æˆ‘ä»¬å‘å¸ƒäº†å¸¦æœ‰éšç§ä¿éšœå’Œå…¨é¢çš„åˆæˆå­å®«MRIæ•°æ®é›†ï¼Œæ”¯æŒå¯å¤åˆ¶çš„ç ”ç©¶å¹¶æ¨åŠ¨å¦‡ç§‘äººå·¥æ™ºèƒ½çš„å…¬å¹³å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07903v1">PDF</a> Accepted at MICCAI CAPI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶é’ˆå¯¹ç”Ÿæˆæ¨¡å‹åœ¨å¥³æ€§ç›†è…”å›¾åƒç”Ÿæˆæ–¹é¢çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¦‡ç§‘å½±åƒåº”ç”¨ä¸­çš„éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„æ‰©æ•£æ¡†æ¶ç”¨äºå­å®«MRIåˆæˆã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ— æ¡ä»¶å’Œæœ‰æ¡ä»¶çš„å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰ï¼Œåœ¨2Då’Œ3Dç¯å¢ƒä¸‹ç”Ÿæˆäº†ç»“æ„è¿è´¯ã€é«˜ä¿çœŸåº¦çš„åˆæˆå›¾åƒã€‚è¿™äº›å›¾åƒä¸çœŸå®æ‰«æç´§å¯†æ¨¡ä»¿ï¼Œå¯ä¸ºè®­ç»ƒç¨³å¥çš„è¯Šæ–­æ¨¡å‹æä¾›å®è´µèµ„æºã€‚ç ”ç©¶é€šè¿‡å…ˆè¿›çš„æ„ŸçŸ¥å’Œåˆ†å¸ƒåº¦é‡è¯„ä¼°ç”Ÿæˆè´¨é‡ï¼Œå¹¶ä¸æ ‡å‡†é‡å»ºæ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œå±•ç¤ºäº†åœ¨å…³é”®åˆ†ç±»ä»»åŠ¡ä¸Šçš„è¯Šæ–­å‡†ç¡®æ€§æ˜¾è‘—æé«˜ã€‚ç›²ä¸“å®¶è¯„ä¼°è¿›ä¸€æ­¥éªŒè¯äº†åˆæˆå›¾åƒçš„ä¸´åºŠçœŸå®æ€§ã€‚æœ¬ç ”ç©¶å‘å¸ƒäº†å¸¦æœ‰éšç§ä¿éšœçš„åˆæˆå­å®«MRIæ•°æ®é›†å’Œå…¨é¢çš„æ¨¡å‹ï¼Œä»¥æ”¯æŒå¯é‡å¤çš„ç ”ç©¶å¹¶æ¨åŠ¨å¦‡ç§‘é¢†åŸŸçš„å…¬å¹³äººå·¥æ™ºèƒ½å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é’ˆå¯¹ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå¥³æ€§ç›†è…”å›¾åƒæ—¶çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¦‡ç§‘å½±åƒåº”ç”¨ä¸­çš„éš¾é¢˜è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„æ‰©æ•£æ¡†æ¶ç”¨äºå­å®«MRIåˆæˆï¼Œç»“åˆäº†DDPMså’ŒLDMsæ¨¡å‹ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨2Då’Œ3Dç¯å¢ƒä¸‹ç”Ÿæˆç»“æ„è¿è´¯ã€é«˜ä¿çœŸåº¦çš„åˆæˆå›¾åƒï¼Œè¿™äº›å›¾åƒä¸çœŸå®æ‰«æç›¸ä¼¼ã€‚</li>
<li>ç ”ç©¶é€šè¿‡å…ˆè¿›çš„æ„ŸçŸ¥å’Œåˆ†å¸ƒåº¦é‡è¯„ä¼°äº†ç”Ÿæˆå›¾åƒçš„è´¨é‡ï¼Œå¹¶ä¸æ ‡å‡†é‡å»ºæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚</li>
<li>ç›²ä¸“å®¶è¯„ä¼°éªŒè¯äº†åˆæˆå›¾åƒçš„ä¸´åºŠçœŸå®æ€§ã€‚</li>
<li>ç ”ç©¶å‘å¸ƒäº†å¸¦æœ‰éšç§ä¿éšœçš„åˆæˆå­å®«MRIæ•°æ®é›†ï¼Œä»¥æ”¯æŒå¯é‡å¤çš„ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07903">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07903v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07903v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07903v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Sea-Undistort-A-Dataset-for-Through-Water-Image-Restoration-in-High-Resolution-Airborne-Bathymetric-Mapping"><a href="#Sea-Undistort-A-Dataset-for-Through-Water-Image-Restoration-in-High-Resolution-Airborne-Bathymetric-Mapping" class="headerlink" title="Sea-Undistort: A Dataset for Through-Water Image Restoration in High   Resolution Airborne Bathymetric Mapping"></a>Sea-Undistort: A Dataset for Through-Water Image Restoration in High   Resolution Airborne Bathymetric Mapping</h2><p><strong>Authors:Maximilian Kromer, Panagiotis Agrafiotis, BegÃ¼m Demir</strong></p>
<p>Accurate image-based bathymetric mapping in shallow waters remains challenging due to the complex optical distortions such as wave induced patterns, scattering and sunglint, introduced by the dynamic water surface, the water column properties, and solar illumination. In this work, we introduce Sea-Undistort, a comprehensive synthetic dataset of 1200 paired 512x512 through-water scenes rendered in Blender. Each pair comprises a distortion-free and a distorted view, featuring realistic water effects such as sun glint, waves, and scattering over diverse seabeds. Accompanied by per-image metadata such as camera parameters, sun position, and average depth, Sea-Undistort enables supervised training that is otherwise infeasible in real environments. We use Sea-Undistort to benchmark two state-of-the-art image restoration methods alongside an enhanced lightweight diffusion-based framework with an early-fusion sun-glint mask. When applied to real aerial data, the enhanced diffusion model delivers more complete Digital Surface Models (DSMs) of the seabed, especially in deeper areas, reduces bathymetric errors, suppresses glint and scattering, and crisply restores fine seabed details. Dataset, weights, and code are publicly available at <a target="_blank" rel="noopener" href="https://www.magicbathy.eu/Sea-Undistort.html">https://www.magicbathy.eu/Sea-Undistort.html</a>. </p>
<blockquote>
<p>åŸºäºå›¾åƒçš„ç²¾ç¡®æ°´æ·±æµ‹é‡æ˜ å°„åœ¨æµ…æ°´åŒºåŸŸä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºåŠ¨æ€æ°´é¢ã€æ°´æŸ±å±æ€§å’Œå¤ªé˜³å…‰ç…§å¼•èµ·çš„å¤æ‚å…‰å­¦ç•¸å˜ï¼Œå¦‚æ³¢æµªå¼•èµ·çš„å›¾æ¡ˆã€æ•£å°„å’Œå¤ªé˜³è€€æ–‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Sea-Undistortï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨Blenderä¸­æ¸²æŸ“çš„1200å¯¹512x512æ°´ä¸‹åœºæ™¯çš„ç»¼åˆæ€§åˆæˆæ•°æ®é›†ã€‚æ¯å¯¹å›¾åƒåŒ…æ‹¬ä¸€ä¸ªæ— ç•¸å˜å’Œä¸€ä¸ªç•¸å˜è§†å›¾ï¼Œä»¥å¤šæ ·åŒ–çš„æµ·åº•ä¸ºèƒŒæ™¯ï¼Œå‘ˆç°å‡ºçœŸå®çš„æ°´é¢æ•ˆæœï¼Œå¦‚å¤ªé˜³è€€æ–‘ã€æ³¢æµªå’Œæ•£å°„ã€‚ä¼´éšæ¯å¼ å›¾åƒçš„å…ƒæ•°æ®ï¼Œå¦‚ç›¸æœºå‚æ•°ã€å¤ªé˜³ä½ç½®å’Œå¹³å‡æ·±åº¦ï¼ŒSea-Undistortä½¿å¾—åœ¨çœŸå®ç¯å¢ƒä¸­æ— æ³•å®ç°çš„ç›‘ç£è®­ç»ƒæˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨Sea-Undistortæ¥è¯„ä¼°ä¸¤ç§æœ€å…ˆè¿›çš„å›¾åƒæ¢å¤æ–¹æ³•ï¼Œä»¥åŠä¸€ç§é‡‡ç”¨æ—©æœŸèåˆå¤ªé˜³è€€æ–‘æ©è†œå¢å¼ºåŠŸèƒ½çš„è½»é‡åŒ–æ‰©æ•£æ¨¡å‹ã€‚å½“åº”ç”¨äºçœŸå®èˆªç©ºæ•°æ®æ—¶ï¼Œå¢å¼ºå‹æ‰©æ•£æ¨¡å‹æä¾›äº†æ›´å®Œæ•´çš„æµ·åº•æ•°å­—è¡¨é¢æ¨¡å‹ï¼ˆDSMsï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨æ·±æ°´åŒºåŸŸï¼Œé™ä½äº†æ°´æ·±æµ‹é‡è¯¯å·®ï¼ŒæŠ‘åˆ¶äº†è€€æ–‘å’Œæ•£å°„ï¼Œå¹¶æ¸…æ™°åœ°æ¢å¤äº†æµ·åº•çš„ç»†èŠ‚ã€‚æ•°æ®é›†ã€æƒé‡å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://www.magicbathy.eu/Sea-Undistort.html%E5%AE%9A%E5%B9%BF%E9%9A%8F%E8%AE%BF%E9%9C%B2%E3%80%82">https://www.magicbathy.eu/Sea-Undistort.htmlå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07760v1">PDF</a> Under review in IEEE Geoscience and Remote Sensing Letters</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Sea-Undistortåˆæˆæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«1200å¯¹é€šè¿‡æ°´ä¸­åœºæ™¯æ¸²æŸ“çš„å›¾åƒï¼Œæ¯å¯¹å›¾åƒåŒ…æ‹¬æ— å¤±çœŸå’Œå¤±çœŸè§†å›¾ï¼Œå¹¶å¸¦æœ‰çœŸå®çš„æµ·æ´‹æ•ˆæœï¼Œå¦‚å¤ªé˜³è€€æ–‘ã€æ³¢æµªå’Œæ•£å°„ã€‚Sea-Undistortè¿˜æä¾›äº†æ¯å¹…å›¾åƒçš„å…ƒæ•°æ®ï¼Œå¦‚ç›¸æœºå‚æ•°ã€å¤ªé˜³ä½ç½®å’Œå¹³å‡æ·±åº¦ã€‚é€šè¿‡ä½¿ç”¨Sea-Undistortæ•°æ®é›†ï¼Œæœ¬æ–‡è¯„ä¼°äº†ä¸¤ç§å…ˆè¿›çš„å›¾åƒæ¢å¤æ–¹æ³•å’Œä¸€ç§å¢å¼ºçš„è½»é‡çº§æ‰©æ•£æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨çœŸå®èˆªç©ºæ•°æ®ä¸Šçš„åº”ç”¨ï¼Œå¯ä»¥æ›´å®Œæ•´åœ°ç”Ÿæˆæµ·åº•æ•°å­—è¡¨é¢æ¨¡å‹ï¼ˆDSMsï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨æ·±æ°´åŒºåŸŸï¼Œå‡å°‘äº†åœ°å½¢æµ‹é‡è¯¯å·®ï¼ŒæŠ‘åˆ¶äº†è€€æ–‘å’Œæ•£å°„ï¼Œå¹¶æ¸…æ™°æ¢å¤äº†æµ·åº•ç»†èŠ‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Sea-Undistortæ˜¯ä¸€ä¸ªå…¨é¢çš„åˆæˆæ•°æ®é›†ï¼ŒåŒ…å«é…å¯¹çš„æ°´é¢å›¾åƒï¼Œè€ƒè™‘åˆ°äº†é€šè¿‡æ°´é¢åœºæ™¯çš„çœŸå®æ•ˆæœã€‚</li>
<li>æ•°æ®é›†è§£å†³äº†åœ¨æµ…æµ·æ°´ä¸‹å‡†ç¡®ç”Ÿæˆæµ´éŸ³å›¾ï¼ˆbathymetric mapï¼‰æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚å…‰å­¦å¤±çœŸå’Œå¤ªé˜³ç…§æ˜äº§ç”Ÿçš„å½±å“ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨Sea-Undistortæ•°æ®é›†ï¼Œæ–‡ç« è¯„ä¼°äº†å½“å‰å…ˆè¿›çš„å›¾åƒæ¢å¤æ–¹æ³•ï¼Œå¹¶æå‡ºä¸€ç§å¢å¼ºçš„è½»é‡çº§æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>å¢å¼ºçš„æ‰©æ•£æ¨¡å‹åœ¨çœŸå®èˆªç©ºæ•°æ®ä¸Šåº”ç”¨è¡¨ç°å‡ºè‰²ï¼Œèƒ½æ›´å®Œæ•´åœ°ç”Ÿæˆæµ·åº•æ•°å­—è¡¨é¢æ¨¡å‹ï¼ˆDSMsï¼‰ã€‚</li>
<li>æ¨¡å‹åœ¨æ·±æ°´åŒºåŸŸè¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œå‡å°‘äº†åœ°å½¢æµ‹é‡è¯¯å·®ï¼Œå¹¶èƒ½æœ‰æ•ˆæŠ‘åˆ¶å¤ªé˜³è€€æ–‘å’Œæ•£å°„ã€‚</li>
<li>æ•°æ®é›†æä¾›äº†ä¸°å¯Œçš„å›¾åƒå…ƒæ•°æ®ï¼Œå¦‚ç›¸æœºå‚æ•°å’Œå¤ªé˜³ä½ç½®ï¼Œä½¿å¾—ç›‘ç£è®­ç»ƒæˆä¸ºå¯èƒ½ã€‚è¿™å¯¹äºç°å®ç¯å¢ƒä¸­æ— æ³•å®ç°çš„è®­ç»ƒå°¤ä¸ºé‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07760v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07760v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07760v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07760v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07760v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07760v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07760v1/page_3_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07760v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Correspondence-as-Video-Test-Time-Adaption-on-SAM2-for-Reference-Segmentation-in-the-Wild"><a href="#Correspondence-as-Video-Test-Time-Adaption-on-SAM2-for-Reference-Segmentation-in-the-Wild" class="headerlink" title="Correspondence as Video: Test-Time Adaption on SAM2 for Reference   Segmentation in the Wild"></a>Correspondence as Video: Test-Time Adaption on SAM2 for Reference   Segmentation in the Wild</h2><p><strong>Authors:Haoran Wang, Zekun Li, Jian Zhang, Lei Qi, Yinghuan Shi</strong></p>
<p>Large vision models like the Segment Anything Model (SAM) exhibit significant limitations when applied to downstream tasks in the wild. Consequently, reference segmentation, which leverages reference images and their corresponding masks to impart novel knowledge to the model, emerges as a promising new direction for adapting vision models. However, existing reference segmentation approaches predominantly rely on meta-learning, which still necessitates an extensive meta-training process and brings massive data and computational cost. In this study, we propose a novel approach by representing the inherent correspondence between reference-target image pairs as a pseudo video. This perspective allows the latest version of SAM, known as SAM2, which is equipped with interactive video object segmentation (iVOS) capabilities, to be adapted to downstream tasks in a lightweight manner. We term this approach Correspondence As Video for SAM (CAV-SAM). CAV-SAM comprises two key modules: the Diffusion-Based Semantic Transition (DBST) module employs a diffusion model to construct a semantic transformation sequence, while the Test-Time Geometric Alignment (TTGA) module aligns the geometric changes within this sequence through test-time fine-tuning. We evaluated CAVSAM on widely-used datasets, achieving segmentation performance improvements exceeding 5% over SOTA methods. Implementation is provided in the supplementary materials. </p>
<blockquote>
<p>å¤§å‹è§†è§‰æ¨¡å‹ï¼Œå¦‚ä¸‡ç‰©åˆ†å‰²æ¨¡å‹ï¼ˆSAMï¼‰ï¼Œåœ¨åº”ç”¨äºå®é™…ä¸‹æ¸¸ä»»åŠ¡æ—¶å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚å› æ­¤ï¼Œåˆ©ç”¨å‚è€ƒå›¾åƒåŠå…¶å¯¹åº”æ©è†œç»™æ¨¡å‹ä¼ æˆæ–°çŸ¥è¯†çš„å‚è€ƒåˆ†å‰²æ³•ï¼Œæˆä¸ºäº†é€‚åº”è§†è§‰æ¨¡å‹çš„æœ‰å‰é€”çš„æ–°æ–¹å‘ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å‚è€ƒåˆ†å‰²æ–¹æ³•ä¸»è¦ä¾èµ–äºå…ƒå­¦ä¹ ï¼Œè¿™ä»ç„¶éœ€è¦å¤§è§„æ¨¡çš„å…ƒè®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶å¸¦æ¥äº†å¤§é‡çš„æ•°æ®å’Œè®¡ç®—æˆæœ¬ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡å°†å‚è€ƒç›®æ ‡å›¾åƒå¯¹ä¹‹é—´çš„å†…åœ¨å¯¹åº”å…³ç³»è¡¨ç¤ºä¸ºä¼ªè§†é¢‘ã€‚è¿™ä¸ªè§†è§’å…è®¸é…å¤‡æœ‰äº¤äº’å¼è§†é¢‘å¯¹è±¡åˆ†å‰²ï¼ˆiVOSï¼‰åŠŸèƒ½çš„SAMçš„æœ€æ–°ç‰ˆæœ¬ï¼Œå³SAM2ï¼Œä»¥è½»ä¾¿çš„æ–¹å¼é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚æˆ‘ä»¬å°†è¿™ç§æ–¹æ³•ç§°ä¸ºSAMçš„å¯¹åº”å…³ç³»è§†é¢‘ï¼ˆCAV-SAMï¼‰ã€‚CAV-SAMåŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šæ‰©æ•£åŸºç¡€è¯­ä¹‰è½¬æ¢ï¼ˆDBSTï¼‰æ¨¡å—åˆ©ç”¨æ‰©æ•£æ¨¡å‹æ„å»ºè¯­ä¹‰è½¬æ¢åºåˆ—ï¼Œè€Œæµ‹è¯•æ—¶é—´å‡ ä½•å¯¹é½ï¼ˆTTGAï¼‰æ¨¡å—åˆ™é€šè¿‡æµ‹è¯•æ—¶çš„å¾®è°ƒï¼Œå¯¹é½æ­¤åºåˆ—ä¸­çš„å‡ ä½•å˜åŒ–ã€‚æˆ‘ä»¬åœ¨å¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†CAVSAMï¼Œå…¶åˆ†å‰²æ€§èƒ½æé«˜äº†è¶…è¿‡5%ï¼Œè¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å®ç°æ–¹æ³•è¯¦è§è¡¥å……ææ–™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07759v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆå¦‚Segment Anything Modelï¼ŒSAMï¼‰åœ¨åº”ç”¨äºå®é™…ä¸‹æ¸¸ä»»åŠ¡æ—¶å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚å‚è€ƒåˆ†å‰²æ³•åˆ©ç”¨å‚è€ƒå›¾åƒåŠå…¶é®ç½©å‘æ¨¡å‹ä¼ æˆæ–°çŸ¥è¯†ï¼Œæˆä¸ºé€‚åº”è§†è§‰æ¨¡å‹çš„æœ‰å‰é€”çš„æ–°æ–¹å‘ã€‚ç„¶è€Œï¼Œç°æœ‰å‚è€ƒåˆ†å‰²æ–¹æ³•ä¸»è¦ä¾èµ–å…ƒå­¦ä¹ ï¼Œä»éœ€å¤§é‡å…ƒè®­ç»ƒè¿‡ç¨‹ï¼Œå¸¦æ¥å·¨å¤§æ•°æ®å’Œè®¡ç®—æˆæœ¬ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°æ–¹æ³•ï¼Œå°†å‚è€ƒç›®æ ‡å›¾åƒå¯¹ä¹‹é—´çš„å†…åœ¨å¯¹åº”å…³ç³»è¡¨ç¤ºä¸ºä¼ªè§†é¢‘ã€‚å€ŸåŠ©å…·æœ‰äº¤äº’å¼è§†é¢‘å¯¹è±¡åˆ†å‰²ï¼ˆiVOSï¼‰åŠŸèƒ½çš„SAMæœ€æ–°ç‰ˆæœ¬ï¼ˆSAM2ï¼‰ï¼Œä»¥è½»æ¾é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚æˆ‘ä»¬ç§°è¿™ç§æ–¹æ³•ä¸ºSAMçš„å¯¹åº”å…³ç³»è§†é¢‘ï¼ˆCAV-SAMï¼‰ã€‚CAV-SAMåŒ…æ‹¬ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šæ‰©æ•£åŸºç¡€è¯­ä¹‰è½¬æ¢ï¼ˆDBSTï¼‰æ¨¡å—é‡‡ç”¨æ‰©æ•£æ¨¡å‹æ„å»ºè¯­ä¹‰è½¬æ¢åºåˆ—ï¼Œæµ‹è¯•æ—¶é—´å‡ ä½•å¯¹é½ï¼ˆTTGAï¼‰æ¨¡å—é€šè¿‡æµ‹è¯•æ—¶å¾®è°ƒå¯¹é½æ­¤åºåˆ—ä¸­çš„å‡ ä½•å˜åŒ–ã€‚æˆ‘ä»¬åœ¨å¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†CAV-SAMï¼Œå…¶åˆ†å‰²æ€§èƒ½è¾ƒæœ€ä¼˜æ–¹æ³•æé«˜äº†è¶…è¿‡5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡åº”ç”¨ä¸­å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å‚è€ƒåˆ†å‰²æ³•æ˜¯ä¸€ä¸ªæ–°å…´æ–¹å‘ï¼Œé€šè¿‡å‚è€ƒå›¾åƒå’Œé®ç½©ä¸ºæ¨¡å‹å¼•å…¥æ–°çŸ¥è¯†ã€‚</li>
<li>ç°æœ‰å‚è€ƒåˆ†å‰²æ–¹æ³•ä¸»è¦ä¾èµ–å…ƒå­¦ä¹ ï¼Œéœ€å¤§é‡å…ƒè®­ç»ƒï¼Œå¯¼è‡´é«˜æˆæœ¬å’Œèµ„æºæ¶ˆè€—ã€‚</li>
<li>æœ¬ç ”ç©¶åˆ›æ–°æ€§åœ°æå‡ºCAV-SAMæ–¹æ³•ï¼Œå°†å‚è€ƒå›¾åƒå¯¹çš„å¯¹åº”å…³ç³»è¡¨ç¤ºä¸ºä¼ªè§†é¢‘ã€‚</li>
<li>CAV-SAMåˆ©ç”¨SAM2çš„iVOSåŠŸèƒ½ï¼Œä»¥æ›´è½»æ¾çš„æ–¹å¼é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li>CAV-SAMåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šDBSTå’ŒTTGAï¼Œåˆ†åˆ«è´Ÿè´£è¯­ä¹‰è½¬æ¢å’Œå‡ ä½•å¯¹é½ã€‚</li>
<li>åœ¨å¹¿æ³›æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒCAV-SAMçš„åˆ†å‰²æ€§èƒ½è¾ƒæœ€ä¼˜æ–¹æ³•æœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07759">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07759v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07759v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07759v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07759v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07759v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07759v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Grouped-Speculative-Decoding-for-Autoregressive-Image-Generation"><a href="#Grouped-Speculative-Decoding-for-Autoregressive-Image-Generation" class="headerlink" title="Grouped Speculative Decoding for Autoregressive Image Generation"></a>Grouped Speculative Decoding for Autoregressive Image Generation</h2><p><strong>Authors:Junhyuk So, Juncheol Shin, Hyunho Kook, Eunhyeok Park</strong></p>
<p>Recently, autoregressive (AR) image models have demonstrated remarkable generative capabilities, positioning themselves as a compelling alternative to diffusion models. However, their sequential nature leads to long inference times, limiting their practical scalability. In this work, we introduce Grouped Speculative Decoding (GSD), a novel, training-free acceleration method for AR image models. While recent studies have explored Speculative Decoding (SD) as a means to speed up AR image generation, existing approaches either provide only modest acceleration or require additional training. Our in-depth analysis reveals a fundamental difference between language and image tokens: image tokens exhibit inherent redundancy and diversity, meaning multiple tokens can convey valid semantics. However, traditional SD methods are designed to accept only a single most-likely token, which fails to leverage this difference, leading to excessive false-negative rejections. To address this, we propose a new SD strategy that evaluates clusters of visually valid tokens rather than relying on a single target token. Additionally, we observe that static clustering based on embedding distance is ineffective, which motivates our dynamic GSD approach. Extensive experiments show that GSD accelerates AR image models by an average of 3.7x while preserving image quality-all without requiring any additional training. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/junhyukso/GSD">https://github.com/junhyukso/GSD</a> </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè‡ªå›å½’ï¼ˆARï¼‰å›¾åƒæ¨¡å‹å·²ç»å±•ç°å‡ºæ˜¾è‘—çš„ç”Ÿæˆèƒ½åŠ›ï¼Œæˆä¸ºæ‰©æ•£æ¨¡å‹çš„æœ‰åŠ›æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„é¡ºåºæ€§è´¨å¯¼è‡´æ¨ç†æ—¶é—´è¾ƒé•¿ï¼Œé™åˆ¶äº†å…¶å®è·µä¸­çš„å¯æ‰©å±•æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆ†ç»„æ¨æµ‹è§£ç ï¼ˆGSDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºARå›¾åƒæ¨¡å‹çš„æ–°å‹ã€æ— éœ€è®­ç»ƒçš„åŠ é€Ÿæ–¹æ³•ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†æ¨æµ‹è§£ç ï¼ˆSDï¼‰æ¥åŠ é€ŸARå›¾åƒç”Ÿæˆï¼Œä½†ç°æœ‰æ–¹æ³•è¦ä¹ˆåªæä¾›é€‚åº¦çš„åŠ é€Ÿï¼Œè¦ä¹ˆéœ€è¦é¢å¤–çš„è®­ç»ƒã€‚æˆ‘ä»¬çš„æ·±å…¥åˆ†ææ­ç¤ºäº†è¯­è¨€ä»¤ç‰Œå’Œå›¾åƒä»¤ç‰Œä¹‹é—´çš„æ ¹æœ¬å·®å¼‚ï¼šå›¾åƒä»¤ç‰Œå…·æœ‰å›ºæœ‰çš„å†—ä½™æ€§å’Œå¤šæ ·æ€§ï¼Œæ„å‘³ç€å¤šä¸ªä»¤ç‰Œå¯ä»¥ä¼ é€’æœ‰æ•ˆçš„è¯­ä¹‰ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„SDæ–¹æ³•è¢«è®¾è®¡ä¸ºä»…æ¥å—ä¸€ä¸ªæœ€å¯èƒ½çš„ä»¤ç‰Œï¼Œè¿™æœªèƒ½åˆ©ç”¨è¿™ç§å·®å¼‚ï¼Œå¯¼è‡´è¿‡å¤šçš„å‡é˜´æ€§æ‹’ç»ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„SDç­–ç•¥ï¼Œè¯¥ç­–ç•¥è¯„ä¼°è§†è§‰ä¸Šæœ‰æ•ˆä»¤ç‰Œçš„é›†ç¾¤ï¼Œè€Œä¸æ˜¯ä¾èµ–äºå•ä¸ªç›®æ ‡ä»¤ç‰Œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°åŸºäºåµŒå…¥è·ç¦»çš„é™æ€èšç±»æ— æ•ˆï¼Œè¿™ä¿ƒä½¿æˆ‘ä»¬é‡‡ç”¨åŠ¨æ€GSDæ–¹æ³•ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGSDå¹³å‡åŠ é€ŸARå›¾åƒæ¨¡å‹3.7å€ï¼ŒåŒæ—¶ä¿æŒå›¾åƒè´¨é‡ï¼Œè€Œä¸”æ— éœ€ä»»ä½•é¢å¤–çš„è®­ç»ƒã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/junhyukso/GSD">https://github.com/junhyukso/GSD</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07747v1">PDF</a> Accepted to the ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œè‡ªå›å½’ï¼ˆARï¼‰å›¾åƒæ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å¤‡å—ç©ç›®ï¼Œæˆä¸ºæ‰©æ•£æ¨¡å‹çš„å¼ºåŠ²ç«äº‰å¯¹æ‰‹ã€‚ä½†å…¶é¡ºåºæ€§å¯¼è‡´æ¨ç†æ—¶é—´é•¿ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨çš„æ‰©å±•æ€§ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ— éœ€è®­ç»ƒçš„åŠ é€Ÿæ–¹æ³•â€”â€”åˆ†ç»„æ¨æµ‹è§£ç ï¼ˆGSDï¼‰ï¼Œç”¨äºåŠ é€ŸARå›¾åƒæ¨¡å‹ã€‚é€šè¿‡å¯¹è¯­è¨€ä¸å›¾åƒä»¤ç‰Œçš„åŒºåˆ«è¿›è¡Œæ·±å…¥åˆ†æï¼Œå‘ç°å›¾åƒä»¤ç‰Œå­˜åœ¨å†…åœ¨å†—ä½™å’Œå¤šæ ·æ€§ã€‚ä¼ ç»Ÿçš„æ¨æµ‹è§£ç æ–¹æ³•è®¾è®¡æ¥å—å•ä¸€çš„æœ€é«˜æ¦‚ç‡ä»¤ç‰Œï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨è¿™ç§å·®å¼‚ï¼Œå¯¼è‡´è¿‡å¤šçš„å‡é˜´æ€§æ‹’ç»ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°ç­–ç•¥ï¼Œè¯„ä¼°è§†è§‰ä¸Šæœ‰æ•ˆçš„ä»¤ç‰Œç¾¤è€Œéå•ä¸€ç›®æ ‡ä»¤ç‰Œã€‚å®éªŒè¡¨æ˜ï¼ŒGSDåœ¨åŠ é€ŸARå›¾åƒæ¨¡å‹çš„åŒæ—¶ï¼Œä¿è¯äº†å›¾åƒè´¨é‡ï¼Œæ— éœ€ä»»ä½•é¢å¤–è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªå›å½’ï¼ˆARï¼‰å›¾åƒæ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†æ¨ç†æ—¶é—´é•¿ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚</li>
<li>åˆ†ç»„æ¨æµ‹è§£ç ï¼ˆGSDï¼‰æ˜¯ä¸€ç§æ–°å‹çš„ã€æ— éœ€è®­ç»ƒçš„åŠ é€Ÿæ–¹æ³•ï¼Œç”¨äºæå‡ARå›¾åƒæ¨¡å‹çš„æ•ˆç‡ã€‚</li>
<li>ä¸ç°æœ‰ç ”ç©¶ç›¸æ¯”ï¼ŒGSDèƒ½æœ‰æ•ˆåŠ é€ŸARå›¾åƒæ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œå¹³å‡æé€Ÿ3.7å€ã€‚</li>
<li>è¯­è¨€ä¸å›¾åƒä»¤ç‰Œå­˜åœ¨æ˜¾è‘—åŒºåˆ«ï¼Œå›¾åƒä»¤ç‰Œå…·æœ‰å†…åœ¨å†—ä½™å’Œå¤šæ ·æ€§ã€‚</li>
<li>ä¼ ç»Ÿæ¨æµ‹è§£ç æ–¹æ³•å› è®¾è®¡ç¼ºé™·å¯¼è‡´è¿‡å¤šçš„å‡é˜´æ€§æ‹’ç»ã€‚</li>
<li>GSDé€šè¿‡è¯„ä¼°è§†è§‰ä¸Šæœ‰æ•ˆçš„ä»¤ç‰Œç¾¤è€Œéå•ä¸€ç›®æ ‡ä»¤ç‰Œæ¥å…‹æœè¿™ä¸€ç¼ºé™·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07747">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07747v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07747v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07747v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07747v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07747v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07747v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07747v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07747v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LaRender-Training-Free-Occlusion-Control-in-Image-Generation-via-Latent-Rendering"><a href="#LaRender-Training-Free-Occlusion-Control-in-Image-Generation-via-Latent-Rendering" class="headerlink" title="LaRender: Training-Free Occlusion Control in Image Generation via Latent   Rendering"></a>LaRender: Training-Free Occlusion Control in Image Generation via Latent   Rendering</h2><p><strong>Authors:Xiaohang Zhan, Dingming Liu</strong></p>
<p>We propose a novel training-free image generation algorithm that precisely controls the occlusion relationships between objects in an image. Existing image generation methods typically rely on prompts to influence occlusion, which often lack precision. While layout-to-image methods provide control over object locations, they fail to address occlusion relationships explicitly. Given a pre-trained image diffusion model, our method leverages volume rendering principles to â€œrenderâ€ the scene in latent space, guided by occlusion relationships and the estimated transmittance of objects. This approach does not require retraining or fine-tuning the image diffusion model, yet it enables accurate occlusion control due to its physics-grounded foundation. In extensive experiments, our method significantly outperforms existing approaches in terms of occlusion accuracy. Furthermore, we demonstrate that by adjusting the opacities of objects or concepts during rendering, our method can achieve a variety of effects, such as altering the transparency of objects, the density of mass (e.g., forests), the concentration of particles (e.g., rain, fog), the intensity of light, and the strength of lens effects, etc. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒå³å¯ç”Ÿæˆå›¾åƒçš„æ–°ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥ç²¾ç¡®æ§åˆ¶å›¾åƒä¸­å¯¹è±¡ä¹‹é—´çš„é®æŒ¡å…³ç³»ã€‚ç°æœ‰çš„å›¾åƒç”Ÿæˆæ–¹æ³•é€šå¸¸ä¾èµ–äºæç¤ºæ¥å½±å“é®æŒ¡ï¼Œä½†å¾€å¾€ç¼ºä¹ç²¾åº¦ã€‚è™½ç„¶å¸ƒå±€åˆ°å›¾åƒçš„æ–¹æ³•å¯ä»¥æ§åˆ¶å¯¹è±¡çš„ä½ç½®ï¼Œä½†å®ƒä»¬æ— æ³•æ˜ç¡®å¤„ç†é®æŒ¡å…³ç³»ã€‚ç»™å®šä¸€ä¸ªé¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ä½“ç§¯æ¸²æŸ“åŸç†åœ¨æ½œåœ¨ç©ºé—´ä¸­â€œæ¸²æŸ“â€åœºæ™¯ï¼Œç”±é®æŒ¡å…³ç³»å’Œä¼°è®¡çš„å¯¹è±¡é€å°„ç‡æ¥æŒ‡å¯¼ã€‚è¿™ç§æ–¹æ³•ä¸éœ€è¦é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒå›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä½†ç”±äºå…¶åŸºäºç‰©ç†çš„åŸºç¡€ï¼Œå®ƒèƒ½å¤Ÿå®ç°ç²¾ç¡®çš„é®æŒ¡æ§åˆ¶ã€‚åœ¨å¹¿æ³›çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é®æŒ¡ç²¾åº¦æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜ï¼Œé€šè¿‡åœ¨æ¸²æŸ“è¿‡ç¨‹ä¸­è°ƒæ•´å¯¹è±¡çš„é€æ˜åº¦æˆ–æ¦‚å¿µï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å®ç°å„ç§æ•ˆæœï¼Œå¦‚æ”¹å˜å¯¹è±¡çš„é€æ˜åº¦ã€ç‰©è´¨å¯†åº¦ï¼ˆä¾‹å¦‚æ£®æ—ï¼‰ã€ç²’å­æµ“åº¦ï¼ˆä¾‹å¦‚é›¨ã€é›¾ï¼‰ã€å…‰çº¿å¼ºåº¦ä»¥åŠé•œå¤´æ•ˆæœå¼ºåº¦ç­‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07647v1">PDF</a> Accepted by ICCV 2025 (oral). Project page:   <a target="_blank" rel="noopener" href="https://xiaohangzhan.github.io/projects/larender/">https://xiaohangzhan.github.io/projects/larender/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„å›¾ç‰‡ç”Ÿæˆç®—æ³•ï¼Œèƒ½å¤Ÿç²¾ç¡®æ§åˆ¶å›¾åƒä¸­ç‰©ä½“ä¹‹é—´çš„é®æŒ¡å…³ç³»ã€‚ç°æœ‰çš„å›¾ç‰‡ç”Ÿæˆæ–¹æ³•é€šå¸¸ä¾èµ–æç¤ºæ¥å½±å“é®æŒ¡ï¼Œè¿™å¾€å¾€ç¼ºä¹ç²¾ç¡®æ€§ã€‚è€Œæˆ‘ä»¬çš„æ–¹æ³•å€ŸåŠ©é¢„è®­ç»ƒçš„å›¾ç‰‡æ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨ä½“ç§¯æ¸²æŸ“åŸç†åœ¨æ½œåœ¨ç©ºé—´â€œæ¸²æŸ“â€åœºæ™¯ï¼Œç”±é®æŒ¡å…³ç³»å’Œç‰©ä½“é€å°„ç‡çš„ä¼°è®¡æ¥å¼•å¯¼ï¼Œæ— éœ€å¯¹å›¾ç‰‡æ‰©æ•£æ¨¡å‹è¿›è¡Œå†è®­ç»ƒæˆ–å¾®è°ƒã€‚ç”±äºå…¶ç‰©ç†åŸºç¡€ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°ç²¾ç¡®çš„é®æŒ¡æ§åˆ¶ã€‚åœ¨å¹¿æ³›çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é®æŒ¡å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜ï¼Œé€šè¿‡åœ¨æ¸²æŸ“è¿‡ç¨‹ä¸­è°ƒæ•´ç‰©ä½“çš„é€æ˜åº¦æˆ–æ¦‚å¿µï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å®ç°å„ç§æ•ˆæœï¼Œå¦‚æ”¹å˜ç‰©ä½“çš„é€æ˜åº¦ã€ç‰©ä½“çš„å¯†åº¦ï¼ˆå¦‚æ£®æ—ï¼‰ã€ç²’å­çš„æµ“åº¦ï¼ˆå¦‚é›¨ã€é›¾ï¼‰ã€å…‰çš„å¼ºåº¦ä»¥åŠé•œå¤´æ•ˆæœç­‰ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ— éœ€è®­ç»ƒçš„å›¾ç‰‡ç”Ÿæˆç®—æ³•ï¼Œç²¾ç¡®æ§åˆ¶å›¾åƒä¸­ç‰©ä½“é—´çš„é®æŒ¡å…³ç³»ã€‚</li>
<li>ä¸ä¾èµ–æç¤ºå½±å“é®æŒ¡çš„ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ›´å…·ç²¾ç¡®æ€§ã€‚</li>
<li>åˆ©ç”¨ä½“ç§¯æ¸²æŸ“åŸç†åœ¨æ½œåœ¨ç©ºé—´â€œæ¸²æŸ“â€åœºæ™¯ï¼Œç”±é®æŒ¡å…³ç³»å’Œç‰©ä½“é€å°„ç‡ä¼°è®¡å¼•å¯¼ã€‚</li>
<li>ä¸éœ€è¦å†è®­ç»ƒæˆ–å¾®è°ƒå›¾ç‰‡æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>åœ¨é®æŒ¡å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>é€šè¿‡è°ƒæ•´æ¸²æŸ“è¿‡ç¨‹ä¸­çš„ç‰©ä½“é€æ˜åº¦æˆ–æ¦‚å¿µï¼Œå¯ä»¥å®ç°å¤šç§æ•ˆæœã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰ç‰©ç†åŸºç¡€ï¼Œèƒ½å¤Ÿå®ç°æ›´çœŸå®ã€æ›´è‡ªç„¶çš„å›¾åƒç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07647">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07647v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07647v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07647v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="X2Edit-Revisiting-Arbitrary-Instruction-Image-Editing-through-Self-Constructed-Data-and-Task-Aware-Representation-Learning"><a href="#X2Edit-Revisiting-Arbitrary-Instruction-Image-Editing-through-Self-Constructed-Data-and-Task-Aware-Representation-Learning" class="headerlink" title="X2Edit: Revisiting Arbitrary-Instruction Image Editing through   Self-Constructed Data and Task-Aware Representation Learning"></a>X2Edit: Revisiting Arbitrary-Instruction Image Editing through   Self-Constructed Data and Task-Aware Representation Learning</h2><p><strong>Authors:Jian Ma, Xujie Zhu, Zihao Pan, Qirong Peng, Xu Guo, Chen Chen, Haonan Lu</strong></p>
<p>Existing open-source datasets for arbitrary-instruction image editing remain suboptimal, while a plug-and-play editing module compatible with community-prevalent generative models is notably absent. In this paper, we first introduce the X2Edit Dataset, a comprehensive dataset covering 14 diverse editing tasks, including subject-driven generation. We utilize the industry-leading unified image generation models and expert models to construct the data. Meanwhile, we design reasonable editing instructions with the VLM and implement various scoring mechanisms to filter the data. As a result, we construct 3.7 million high-quality data with balanced categories. Second, to better integrate seamlessly with community image generation models, we design task-aware MoE-LoRA training based on FLUX.1, with only 8% of the parameters of the full model. To further improve the final performance, we utilize the internal representations of the diffusion model and define positive&#x2F;negative samples based on image editing types to introduce contrastive learning. Extensive experiments demonstrate that the modelâ€™s editing performance is competitive among many excellent models. Additionally, the constructed dataset exhibits substantial advantages over existing open-source datasets. The open-source code, checkpoints, and datasets for X2Edit can be found at the following link: <a target="_blank" rel="noopener" href="https://github.com/OPPO-Mente-Lab/X2Edit">https://github.com/OPPO-Mente-Lab/X2Edit</a>. </p>
<blockquote>
<p>ç°æœ‰ç”¨äºä»»æ„æŒ‡ä»¤å›¾åƒç¼–è¾‘çš„å¼€æºæ•°æ®é›†ä»ç„¶ä¸å¤Ÿç†æƒ³ï¼Œè€Œä¸ç¤¾åŒºæµè¡Œç”Ÿæˆæ¨¡å‹å…¼å®¹çš„å³æ’å³ç”¨ç¼–è¾‘æ¨¡å—æ˜æ˜¾ç¼ºå¤±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆä»‹ç»äº†X2Editæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–14ç§ä¸åŒç¼–è¾‘ä»»åŠ¡çš„ç»¼åˆæ€§æ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¸»é¢˜é©±åŠ¨ç”Ÿæˆã€‚æˆ‘ä»¬åˆ©ç”¨è¡Œä¸šé¢†å…ˆçš„ç»Ÿä¸€å›¾åƒç”Ÿæˆæ¨¡å‹å’Œä¸“å®¶æ¨¡å‹æ¥æ„å»ºæ•°æ®ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨VLMè®¾è®¡åˆç†çš„ç¼–è¾‘æŒ‡ä»¤ï¼Œå¹¶å®æ–½å„ç§è¯„åˆ†æœºåˆ¶æ¥è¿‡æ»¤æ•°æ®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†370ä¸‡é«˜è´¨é‡æ•°æ®ï¼Œç±»åˆ«å¹³è¡¡ã€‚å…¶æ¬¡ï¼Œä¸ºäº†æ›´å¥½åœ°ä¸ç¤¾åŒºå›¾åƒç”Ÿæˆæ¨¡å‹æ— ç¼é›†æˆï¼Œæˆ‘ä»¬åŸºäºFLUXè®¾è®¡äº†ä»»åŠ¡æ„ŸçŸ¥çš„MoE-LoRAè®­ç»ƒã€‚è¯¥è®­ç»ƒä»…å å…¨æ¨¡å‹çš„8%çš„å‚æ•°ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æœ€ç»ˆæ€§èƒ½ï¼Œæˆ‘ä»¬åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºï¼Œæ ¹æ®å›¾åƒç¼–è¾‘ç±»å‹å®šä¹‰æ­£è´Ÿæ ·æœ¬ï¼Œå¼•å…¥å¯¹æ¯”å­¦ä¹ ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹çš„ç¼–è¾‘æ€§èƒ½åœ¨ä¼—å¤šä¼˜ç§€æ¨¡å‹ä¸­å…·æœ‰ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼Œæ„å»ºçš„æ•°æ®é›†åœ¨ç°æœ‰å¼€æºæ•°æ®é›†ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚X2Editçš„å¼€æºä»£ç ã€æ£€æŸ¥ç‚¹å’Œæ•°æ®é›†å¯åœ¨ä»¥ä¸‹é“¾æ¥ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/OPPO-Mente-Lab/X2Edit%E3%80%82">https://github.com/OPPO-Mente-Lab/X2Editã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07607v1">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/OPPO-Mente-Lab/X2Edit">https://github.com/OPPO-Mente-Lab/X2Edit</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†X2Editæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«14ç§ä¸åŒçš„å›¾åƒç¼–è¾‘ä»»åŠ¡ï¼Œé‡‡ç”¨é¢†å…ˆçš„ç»Ÿä¸€å›¾åƒç”Ÿæˆæ¨¡å‹å’Œä¸“å®¶æ¨¡å‹æ„å»ºæ•°æ®ï¼Œå¹¶é€šè¿‡åˆç†çš„è®¾è®¡ç¼–è¾‘æŒ‡ä»¤å’Œè¯„åˆ†æœºåˆ¶ï¼Œç­›é€‰å‡ºé«˜è´¨é‡çš„æ•°æ®ã€‚ä¸ºæ›´å¥½åœ°ä¸ç¤¾åŒºå›¾åƒç”Ÿæˆæ¨¡å‹æ— ç¼é›†æˆï¼Œè®¾è®¡äº†ä¸€ç§åŸºäºMoE-LoRAçš„ä»»åŠ¡æ„ŸçŸ¥è®­ç»ƒæ–¹æ¡ˆã€‚é€šè¿‡å¯¹æ¯”å­¦ä¹ æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå®éªŒè¡¨æ˜è¯¥æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸Šçš„æ€§èƒ½å…·æœ‰ç«äº‰åŠ›ï¼Œä¸”æ‰€æ„å»ºæ•°æ®é›†ä¼˜äºç°æœ‰å¼€æºæ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥X2Editæ•°æ®é›†ï¼ŒåŒ…å«14ç§å›¾åƒç¼–è¾‘ä»»åŠ¡ï¼Œæ•°æ®å…¨é¢ã€‚</li>
<li>é‡‡ç”¨é¢†å…ˆçš„ç»Ÿä¸€å›¾åƒç”Ÿæˆæ¨¡å‹å’Œä¸“å®¶æ¨¡å‹æ„å»ºæ•°æ®ã€‚</li>
<li>é€šè¿‡åˆç†çš„è®¾è®¡ç¼–è¾‘æŒ‡ä»¤å’Œè¯„åˆ†æœºåˆ¶ï¼Œç­›é€‰å‡ºé«˜è´¨é‡æ•°æ®ã€‚</li>
<li>è®¾è®¡åŸºäºMoE-LoRAçš„ä»»åŠ¡æ„ŸçŸ¥è®­ç»ƒæ–¹æ¡ˆï¼Œä¸ç¤¾åŒºå›¾åƒç”Ÿæˆæ¨¡å‹æ— ç¼é›†æˆã€‚</li>
<li>åˆ©ç”¨å¯¹æ¯”å­¦ä¹ æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸Šçš„æ€§èƒ½å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07607">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07607v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07607v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07607v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07607v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07607v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07607v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Splat4D-Diffusion-Enhanced-4D-Gaussian-Splatting-for-Temporally-and-Spatially-Consistent-Content-Creation"><a href="#Splat4D-Diffusion-Enhanced-4D-Gaussian-Splatting-for-Temporally-and-Spatially-Consistent-Content-Creation" class="headerlink" title="Splat4D: Diffusion-Enhanced 4D Gaussian Splatting for Temporally and   Spatially Consistent Content Creation"></a>Splat4D: Diffusion-Enhanced 4D Gaussian Splatting for Temporally and   Spatially Consistent Content Creation</h2><p><strong>Authors:Minghao Yin, Yukang Cao, Songyou Peng, Kai Han</strong></p>
<p>Generating high-quality 4D content from monocular videos for applications such as digital humans and AR&#x2F;VR poses challenges in ensuring temporal and spatial consistency, preserving intricate details, and incorporating user guidance effectively. To overcome these challenges, we introduce Splat4D, a novel framework enabling high-fidelity 4D content generation from a monocular video. Splat4D achieves superior performance while maintaining faithful spatial-temporal coherence by leveraging multi-view rendering, inconsistency identification, a video diffusion model, and an asymmetric U-Net for refinement. Through extensive evaluations on public benchmarks, Splat4D consistently demonstrates state-of-the-art performance across various metrics, underscoring the efficacy of our approach. Additionally, the versatility of Splat4D is validated in various applications such as text&#x2F;image conditioned 4D generation, 4D human generation, and text-guided content editing, producing coherent outcomes following user instructions. </p>
<blockquote>
<p>ä»å•ç›®è§†é¢‘ä¸­ç”Ÿæˆé«˜è´¨é‡4Då†…å®¹ï¼Œç”¨äºæ•°å­—äººç±»å’ŒAR&#x2F;VRç­‰åº”ç”¨ï¼Œåœ¨ä¿éšœæ—¶é—´ç©ºé—´è¿è´¯æ€§ã€ä¿ç•™ç»†èŠ‚ä»¥åŠæœ‰æ•ˆèå…¥ç”¨æˆ·æŒ‡å¯¼æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Splat4Dï¼Œä¸€ä¸ªèƒ½å¤Ÿä»å•ç›®è§†é¢‘ä¸­å®ç°é«˜ä¿çœŸ4Då†…å®¹ç”Ÿæˆçš„æ–°å‹æ¡†æ¶ã€‚Splat4Dé€šè¿‡åˆ©ç”¨å¤šè§†è§’æ¸²æŸ“ã€ä¸ä¸€è‡´æ€§è¯†åˆ«ã€è§†é¢‘æ‰©æ•£æ¨¡å‹ä»¥åŠç”¨äºç²¾ç»†åŒ–çš„ä¸å¯¹ç§°U-Netï¼Œåœ¨ä¿æŒæ—¶ç©ºè¿è´¯æ€§çš„åŒæ—¶å®ç°äº†å“è¶Šæ€§èƒ½ã€‚åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒSplat4Dåœ¨å„ç§æŒ‡æ ‡ä¸Šå‡è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå‡¸æ˜¾äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼ŒSplat4Dçš„é€šç”¨æ€§åœ¨å„ç§åº”ç”¨ä¸­å¾—åˆ°äº†éªŒè¯ï¼Œå¦‚æ–‡æœ¬&#x2F;å›¾åƒæ¡ä»¶é©±åŠ¨çš„4Dç”Ÿæˆã€4Däººç±»ç”Ÿæˆä»¥åŠæ–‡æœ¬å¼•å¯¼çš„å†…å®¹ç¼–è¾‘ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·æŒ‡ä»¤äº§ç”Ÿè¿è´¯çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07557v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä»å•ç›®è§†é¢‘ä¸­ç”Ÿæˆé«˜è´¨é‡4Då†…å®¹çš„æ–°æ¡†æ¶Splat4Dã€‚å®ƒé€šè¿‡å¤šè§†è§’æ¸²æŸ“ã€ä¸ä¸€è‡´æ€§è¯†åˆ«ã€è§†é¢‘æ‰©æ•£æ¨¡å‹å’Œä¸å¯¹ç§°U-Netçš„ç²¾ç»†å¤„ç†ï¼Œå®ç°é«˜æ€§èƒ½çš„æ—¶ç©ºè¿è´¯æ€§ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡çš„ç©ºé—´ç»†èŠ‚ã€‚Splat4Dåœ¨å„ç§å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶åœ¨æ–‡æœ¬&#x2F;å›¾åƒæ§åˆ¶çš„4Dç”Ÿæˆã€4Däººç±»ç”Ÿæˆå’Œæ–‡æœ¬å¼•å¯¼çš„å†…å®¹ç¼–è¾‘ç­‰åº”ç”¨ä¸­çš„æ•ˆèƒ½å’Œçµæ´»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Splat4Dæ¡†æ¶èƒ½å¤Ÿä»å•ç›®è§†é¢‘ä¸­ç”Ÿæˆé«˜è´¨é‡4Då†…å®¹ã€‚</li>
<li>åˆ©ç”¨å¤šè§†è§’æ¸²æŸ“æŠ€æœ¯æé«˜ç”Ÿæˆçš„è¿è´¯æ€§å’Œè´¨é‡ã€‚</li>
<li>é€šè¿‡ä¸ä¸€è‡´æ€§è¯†åˆ«æŠ€æœ¯ï¼Œç¡®ä¿ç”Ÿæˆçš„æ—¶ç©ºè¿è´¯æ€§ã€‚</li>
<li>ä½¿ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä½¿å¾—å†…å®¹æ›´åŠ çœŸå®è‡ªç„¶ã€‚</li>
<li>åˆ©ç”¨ä¸å¯¹ç§°U-Netè¿›è¡Œç²¾ç»†å¤„ç†ï¼Œä¿ç•™é«˜è´¨é‡çš„ç©ºé—´ç»†èŠ‚ã€‚</li>
<li>Splat4Dåœ¨å¤šä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå±•ç°äº†å…¶å“è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07557">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07557v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07557v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07557v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Exploring-Multimodal-Diffusion-Transformers-for-Enhanced-Prompt-based-Image-Editing"><a href="#Exploring-Multimodal-Diffusion-Transformers-for-Enhanced-Prompt-based-Image-Editing" class="headerlink" title="Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based   Image Editing"></a>Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based   Image Editing</h2><p><strong>Authors:Joonghyuk Shin, Alchan Hwang, Yujin Kim, Daneul Kim, Jaesik Park</strong></p>
<p>Transformer-based diffusion models have recently superseded traditional U-Net architectures, with multimodal diffusion transformers (MM-DiT) emerging as the dominant approach in state-of-the-art models like Stable Diffusion 3 and Flux.1. Previous approaches have relied on unidirectional cross-attention mechanisms, with information flowing from text embeddings to image latents. In contrast, MMDiT introduces a unified attention mechanism that concatenates input projections from both modalities and performs a single full attention operation, allowing bidirectional information flow between text and image branches. This architectural shift presents significant challenges for existing editing techniques. In this paper, we systematically analyze MM-DiTâ€™s attention mechanism by decomposing attention matrices into four distinct blocks, revealing their inherent characteristics. Through these analyses, we propose a robust, prompt-based image editing method for MM-DiT that supports global to local edits across various MM-DiT variants, including few-step models. We believe our findings bridge the gap between existing U-Net-based methods and emerging architectures, offering deeper insights into MMDiTâ€™s behavioral patterns. </p>
<blockquote>
<p>åŸºäºTransformerçš„æ‰©æ•£æ¨¡å‹æœ€è¿‘å·²ç»å–ä»£äº†ä¼ ç»Ÿçš„U-Netæ¶æ„ï¼Œå¤šæ¨¡æ€æ‰©æ•£Transformerï¼ˆMM-DiTï¼‰æˆä¸ºå…ˆè¿›æ¨¡å‹å¦‚Stable Diffusion 3å’ŒFluxçš„ä¸»å¯¼æ–¹æ³•ã€‚ä¹‹å‰çš„æ–¹æ³•ä¾èµ–äºå•å‘äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œä¿¡æ¯ä»æ–‡æœ¬åµŒå…¥æµå‘å›¾åƒæ½œåœ¨è¡¨ç¤ºã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒMMDiTå¼•å…¥äº†ä¸€ç§ç»Ÿä¸€æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å°†æ¥è‡ªä¸¤ä¸ªæ¨¡æ€çš„è¾“å…¥æŠ•å½±è¿æ¥èµ·æ¥ï¼Œå¹¶æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„æ³¨æ„åŠ›æ“ä½œï¼Œå…è®¸æ–‡æœ¬å’Œå›¾åƒåˆ†æ”¯ä¹‹é—´çš„åŒå‘ä¿¡æ¯æµã€‚è¿™ç§æ¶æ„çš„å˜åŒ–ç»™ç°æœ‰çš„ç¼–è¾‘æŠ€æœ¯å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†æ³¨æ„åŠ›çŸ©é˜µåˆ†è§£ä¸ºå››ä¸ªä¸åŒçš„å—æ¥ç³»ç»Ÿåœ°åˆ†æMM-DiTçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ­ç¤ºäº†å®ƒä»¬çš„å›ºæœ‰ç‰¹æ€§ã€‚é€šè¿‡è¿™äº›åˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹MM-DiTçš„ç¨³å¥æç¤ºå¼å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ”¯æŒå„ç§MM-DiTå˜ç§ï¼ˆåŒ…æ‹¬å°‘æ­¥éª¤æ¨¡å‹ï¼‰è¿›è¡Œå…¨å±€åˆ°å±€éƒ¨ç¼–è¾‘ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æˆæœæ¶èµ·äº†ç°æœ‰U-Netæ–¹æ³•å’Œæ–°å…´æ¶æ„ä¹‹é—´çš„æ¡¥æ¢ï¼Œä¸ºç†è§£MMDiTçš„è¡Œä¸ºæ¨¡å¼æä¾›äº†æ›´æ·±å…¥è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07519v1">PDF</a> ICCV 2025. Project webpage:   <a target="_blank" rel="noopener" href="https://joonghyuk.com/exploring-mmdit-web/">https://joonghyuk.com/exploring-mmdit-web/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºTransformerçš„æ‰©æ•£æ¨¡å‹å·²é€æ¸å–ä»£ä¼ ç»ŸU-Netæ¶æ„ï¼Œå¹¶åœ¨æœ€æ–°æŠ€æœ¯ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œå¦‚Stable Diffusion 3å’ŒFluxã€‚å…¶ä¸­ï¼Œå¤šæ¨¡æ€æ‰©æ•£è½¬æ¢å™¨ï¼ˆMM-DiTï¼‰å¼•å…¥äº†ä¸€ç§ç»Ÿä¸€æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…è®¸æ–‡æœ¬å’Œå›¾åƒåˆ†æ”¯é—´çš„åŒå‘ä¿¡æ¯æµã€‚æœ¬æ–‡å¯¹MM-DiTçš„æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼Œå¹¶æå‡ºäº†ä¸€ç§é’ˆå¯¹MM-DiTçš„åŸºäºæç¤ºçš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œæ”¯æŒå…¨å±€åˆ°å±€éƒ¨ç¼–è¾‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer-based diffusion models have surpassed traditional U-Net architectures in state-of-the-art models like Stable Diffusion 3 and Flux.</li>
<li>å¤šæ¨¡æ€æ‰©æ•£è½¬æ¢å™¨ï¼ˆMM-DiTï¼‰å¼•å…¥äº†ç»Ÿä¸€æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…è®¸æ–‡æœ¬å’Œå›¾åƒåˆ†æ”¯é—´çš„åŒå‘ä¿¡æ¯æµã€‚</li>
<li>MM-DiTçš„æ³¨æ„åŠ›æœºåˆ¶è¢«åˆ†è§£ä¸ºå››ä¸ªç‹¬ç«‹å—è¿›è¡Œç³»ç»Ÿåˆ†æã€‚</li>
<li>å¯¹MM-DiTçš„æ³¨æ„åŠ›æœºåˆ¶åˆ†ææ­ç¤ºäº†å…¶å†…åœ¨ç‰¹æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæç¤ºçš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œé€‚ç”¨äºå„ç§MM-DiTå˜ä½“ï¼ŒåŒ…æ‹¬å°‘æ­¥éª¤æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•æ”¯æŒä»å…¨å±€åˆ°å±€éƒ¨çš„å›¾åƒç¼–è¾‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07519">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07519v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07519v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07519v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07519v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07519v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07519v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SODiff-Semantic-Oriented-Diffusion-Model-for-JPEG-Compression-Artifacts-Removal"><a href="#SODiff-Semantic-Oriented-Diffusion-Model-for-JPEG-Compression-Artifacts-Removal" class="headerlink" title="SODiff: Semantic-Oriented Diffusion Model for JPEG Compression Artifacts   Removal"></a>SODiff: Semantic-Oriented Diffusion Model for JPEG Compression Artifacts   Removal</h2><p><strong>Authors:Tingyu Yang, Jue Gong, Jinpei Guo, Wenbo Li, Yong Guo, Yulun Zhang</strong></p>
<p>JPEG, as a widely used image compression standard, often introduces severe visual artifacts when achieving high compression ratios. Although existing deep learning-based restoration methods have made considerable progress, they often struggle to recover complex texture details, resulting in over-smoothed outputs. To overcome these limitations, we propose SODiff, a novel and efficient semantic-oriented one-step diffusion model for JPEG artifacts removal. Our core idea is that effective restoration hinges on providing semantic-oriented guidance to the pre-trained diffusion model, thereby fully leveraging its powerful generative prior. To this end, SODiff incorporates a semantic-aligned image prompt extractor (SAIPE). SAIPE extracts rich features from low-quality (LQ) images and projects them into an embedding space semantically aligned with that of the text encoder. Simultaneously, it preserves crucial information for faithful reconstruction. Furthermore, we propose a quality factor-aware time predictor that implicitly learns the compression quality factor (QF) of the LQ image and adaptively selects the optimal denoising start timestep for the diffusion process. Extensive experimental results show that our SODiff outperforms recent leading methods in both visual quality and quantitative metrics. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/frakenation/SODiff">https://github.com/frakenation/SODiff</a> </p>
<blockquote>
<p>JPEGä½œä¸ºä¸€ç§å¹¿æ³›ä½¿ç”¨çš„å›¾åƒå‹ç¼©æ ‡å‡†ï¼Œåœ¨è¾¾åˆ°é«˜å‹ç¼©æ¯”æ—¶å¾€å¾€ä¼šå¼•å…¥ä¸¥é‡çš„è§†è§‰å¤±çœŸã€‚å°½ç®¡ç°æœ‰çš„åŸºäºæ·±åº¦å­¦ä¹ çš„æ¢å¤æ–¹æ³•å·²ç»å–å¾—äº†ç›¸å½“çš„è¿›å±•ï¼Œä½†å®ƒä»¬é€šå¸¸éš¾ä»¥æ¢å¤å¤æ‚çš„çº¹ç†ç»†èŠ‚ï¼Œå¯¼è‡´è¾“å‡ºè¿‡äºå¹³æ»‘ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†SODiffï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå»é™¤JPEGå¤±çœŸçš„æ–°é¢–ä¸”é«˜æ•ˆçš„ä¸€ç«™å¼è¯­ä¹‰å¯¼å‘æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒç†å¿µæ˜¯ï¼Œæœ‰æ•ˆçš„æ¢å¤å–å†³äºå‘é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æä¾›è¯­ä¹‰å¯¼å‘çš„å¼•å¯¼ï¼Œä»è€Œå……åˆ†åˆ©ç”¨å…¶å¼ºå¤§çš„ç”Ÿæˆå…ˆéªŒã€‚ä¸ºæ­¤ï¼ŒSODiffç»“åˆäº†è¯­ä¹‰å¯¹é½å›¾åƒæç¤ºæå–å™¨ï¼ˆSAIPEï¼‰ã€‚SAIPEä»ä½è´¨é‡ï¼ˆLQï¼‰å›¾åƒä¸­æå–ä¸°å¯Œçš„ç‰¹å¾ï¼Œå¹¶å°†å…¶æŠ•å½±åˆ°ä¸æ–‡æœ¬ç¼–ç å™¨è¯­ä¹‰å¯¹é½çš„åµŒå…¥ç©ºé—´ä¸­ã€‚åŒæ—¶ï¼Œå®ƒä¿ç•™äº†ç”¨äºå¿ å®é‡å»ºçš„å…³é”®ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è´¨é‡å› å­æ„ŸçŸ¥æ—¶é—´é¢„æµ‹å™¨ï¼Œè¯¥é¢„æµ‹å™¨èƒ½å¤Ÿéšæ€§å­¦ä¹ LQå›¾åƒçš„è´¨é‡å› å­ï¼ˆQFï¼‰ï¼Œå¹¶è‡ªé€‚åº”åœ°é€‰æ‹©æ‰©æ•£è¿‡ç¨‹çš„æœ€ä½³å»å™ªå¼€å§‹æ—¶é—´æ­¥é•¿ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SODiffåœ¨è§†è§‰è´¨é‡å’Œå®šé‡æŒ‡æ ‡æ–¹é¢éƒ½ä¼˜äºæœ€æ–°çš„ä¸»æµæ–¹æ³•ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/frakenation/SODiff">https://github.com/frakenation/SODiff</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07346v1">PDF</a> 7 pages, 5 figures. The code will be available at   \url{<a target="_blank" rel="noopener" href="https://github.com/frakenation/SODiff%7D">https://github.com/frakenation/SODiff}</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é’ˆå¯¹JPEGå‹ç¼©å›¾åƒçš„é«˜æ•ˆè¯­ä¹‰å¯¼å‘æ‰©æ•£æ¨¡å‹SODiffï¼Œç”¨äºå»é™¤JPEGå‹ç¼©å¸¦æ¥çš„è§†è§‰ä¼ªå½±ã€‚SODiffç»“åˆäº†è¯­ä¹‰å¯¹é½å›¾åƒæç¤ºæå–å™¨ï¼ˆSAIPEï¼‰ï¼Œèƒ½å¤Ÿä»ä½è´¨é‡å›¾åƒä¸­æå–ä¸°å¯Œçš„ç‰¹å¾ï¼Œå¹¶å°†å…¶æŠ•å½±åˆ°ä¸æ–‡æœ¬ç¼–ç å™¨è¯­ä¹‰å¯¹é½çš„åµŒå…¥ç©ºé—´ä¸­ã€‚åŒæ—¶ï¼ŒSODiffè¿˜æå‡ºäº†ä¸€ç§è´¨é‡å› å­æ„ŸçŸ¥çš„æ—¶é—´é¢„æµ‹å™¨ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°é€‰æ‹©æœ€ä½³å»å™ªèµ·å§‹æ—¶é—´æ­¥é•¿ï¼Œä»¥æé«˜æ‰©æ•£è¿‡ç¨‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSODiffåœ¨è§†è§‰è´¨é‡å’Œå®šé‡æŒ‡æ ‡æ–¹é¢å‡ä¼˜äºå½“å‰ä¸»æµæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SODiffæ˜¯ä¸€ç§æ–°çš„é’ˆå¯¹JPEGå‹ç¼©ä¼ªå½±çš„è¯­ä¹‰å¯¼å‘æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>SODiffç»“åˆäº†è¯­ä¹‰å¯¹é½å›¾åƒæç¤ºæå–å™¨ï¼ˆSAIPEï¼‰ï¼Œèƒ½å¤Ÿä»ä½è´¨é‡å›¾åƒä¸­æå–ä¸°å¯Œçš„ç‰¹å¾ã€‚</li>
<li>SAIPEå°†æå–çš„ç‰¹å¾æŠ•å½±åˆ°ä¸æ–‡æœ¬ç¼–ç å™¨è¯­ä¹‰å¯¹é½çš„åµŒå…¥ç©ºé—´ä¸­ã€‚</li>
<li>SODiffæå‡ºäº†è´¨é‡å› å­æ„ŸçŸ¥çš„æ—¶é—´é¢„æµ‹å™¨ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”é€‰æ‹©å»å™ªèµ·å§‹æ—¶é—´æ­¥é•¿ã€‚</li>
<li>SODiffæé«˜äº†æ‰©æ•£è¿‡ç¨‹çš„æ€§èƒ½ã€‚</li>
<li>SODiffåœ¨è§†è§‰è´¨é‡å’Œå®šé‡æŒ‡æ ‡æ–¹é¢ä¼˜äºå½“å‰ä¸»æµæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07346">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07346v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07346v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07346v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07346v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07346v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Consistent-and-Controllable-Image-Animation-with-Motion-Linear-Diffusion-Transformers"><a href="#Consistent-and-Controllable-Image-Animation-with-Motion-Linear-Diffusion-Transformers" class="headerlink" title="Consistent and Controllable Image Animation with Motion Linear Diffusion   Transformers"></a>Consistent and Controllable Image Animation with Motion Linear Diffusion   Transformers</h2><p><strong>Authors:Xin Ma, Yaohui Wang, Genyun Jia, Xinyuan Chen, Tien-Tsin Wong, Cunjian Chen</strong></p>
<p>Image animation has seen significant progress, driven by the powerful generative capabilities of diffusion models. However, maintaining appearance consistency with static input images and mitigating abrupt motion transitions in generated animations remain persistent challenges. While text-to-video (T2V) generation has demonstrated impressive performance with diffusion transformer models, the image animation field still largely relies on U-Net-based diffusion models, which lag behind the latest T2V approaches. Moreover, the quadratic complexity of vanilla self-attention mechanisms in Transformers imposes heavy computational demands, making image animation particularly resource-intensive. To address these issues, we propose MiraMo, a framework designed to enhance efficiency, appearance consistency, and motion smoothness in image animation. Specifically, MiraMo introduces three key elements: (1) A foundational text-to-video architecture replacing vanilla self-attention with efficient linear attention to reduce computational overhead while preserving generation quality; (2) A novel motion residual learning paradigm that focuses on modeling motion dynamics rather than directly predicting frames, improving temporal consistency; and (3) A DCT-based noise refinement strategy during inference to suppress sudden motion artifacts, complemented by a dynamics control module to balance motion smoothness and expressiveness. Extensive experiments against state-of-the-art methods validate the superiority of MiraMo in generating consistent, smooth, and controllable animations with accelerated inference speed. Additionally, we demonstrate the versatility of MiraMo through applications in motion transfer and video editing tasks. </p>
<blockquote>
<p>å›¾åƒåŠ¨ç”»é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œè¿™å¾—ç›Šäºæ‰©æ•£æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¿æŒä¸é™æ€è¾“å…¥å›¾åƒçš„å¤–è²Œä¸€è‡´æ€§ï¼Œä»¥åŠå‡è½»ç”ŸæˆåŠ¨ç”»ä¸­çš„çªå…€è¿åŠ¨è¿‡æ¸¡ï¼Œä»ç„¶æ˜¯æŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚å°½ç®¡æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰ç”Ÿæˆå·²ç»å±•ç¤ºäº†æ‰©æ•£å˜å‹å™¨æ¨¡å‹çš„ä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ï¼Œä½†å›¾åƒåŠ¨ç”»é¢†åŸŸä»ç„¶ä¸»è¦ä¾èµ–äºåŸºäºU-Netçš„æ‰©æ•£æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹è½åäºæœ€æ–°çš„T2Væ–¹æ³•ã€‚æ­¤å¤–ï¼ŒTransformerä¸­æ™®é€šè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚æ€§å¸¦æ¥äº†å·¨å¤§çš„è®¡ç®—éœ€æ±‚ï¼Œä½¿å¾—å›¾åƒåŠ¨ç”»ç‰¹åˆ«è€—è´¹èµ„æºã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MiraMoï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æé«˜å›¾åƒåŠ¨ç”»çš„æ•ˆç‡ã€å¤–è§‚ä¸€è‡´æ€§å’Œè¿åŠ¨å¹³æ»‘åº¦çš„æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒMiraMoå¼•å…¥äº†ä¸‰ä¸ªå…³é”®è¦ç´ ï¼š1ï¼‰ä¸€ç§åŸºç¡€æ–‡æœ¬åˆ°è§†é¢‘æ¶æ„ï¼Œç”¨é«˜æ•ˆçš„çº¿æ€§æ³¨æ„åŠ›æ›¿æ¢æ™®é€šè‡ªæ³¨æ„åŠ›ï¼Œä»¥é™ä½è®¡ç®—å¼€é”€åŒæ—¶ä¿æŒç”Ÿæˆè´¨é‡ï¼›2ï¼‰ä¸€ç§æ–°é¢–çš„è¿åŠ¨æ®‹å·®å­¦ä¹ èŒƒå¼ï¼Œä¾§é‡äºæ¨¡æ‹Ÿè¿åŠ¨åŠ¨åŠ›å­¦è€Œä¸æ˜¯ç›´æ¥é¢„æµ‹å¸§ï¼Œä»¥æé«˜æ—¶é—´ä¸€è‡´æ€§ï¼›3ï¼‰ä¸€ç§åŸºäºDCTçš„æ¨ç†è¿‡ç¨‹ä¸­çš„å™ªå£°ç»†åŒ–ç­–ç•¥ï¼ŒæŠ‘åˆ¶çªç„¶çš„è¿åŠ¨ä¼ªå½±ï¼Œè¾…ä»¥åŠ¨åŠ›å­¦æ§åˆ¶æ¨¡å—æ¥å¹³è¡¡è¿åŠ¨çš„å¹³æ»‘åº¦å’Œè¡¨ç°åŠ›ã€‚ä¸æœ€å…ˆè¿›çš„æ–¹æ³•çš„å¤§é‡å®éªŒéªŒè¯äº†MiraMoåœ¨ç”Ÿæˆä¸€è‡´ã€å¹³æ»‘å’Œå¯æ§çš„åŠ¨ç”»æ–¹é¢çš„ä¼˜è¶Šæ€§ï¼ŒåŒæ—¶åŠ å¿«äº†æ¨ç†é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡è¿åŠ¨è½¬ç§»å’Œè§†é¢‘ç¼–è¾‘ä»»åŠ¡çš„åº”ç”¨å±•ç¤ºäº†MiraMoçš„é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07246v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://maxin-cn.github.io/miramo_project">https://maxin-cn.github.io/miramo_project</a></p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåŠ¨ç”»é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä¿æŒä¸é™æ€è¾“å…¥å›¾åƒçš„å¤–è§‚ä¸€è‡´æ€§ä»¥åŠå‡è½»ç”ŸæˆåŠ¨ç”»ä¸­çš„çªå…€è¿åŠ¨è¿‡æ¸¡ä»æ˜¯æŒç»­æŒ‘æˆ˜ã€‚è™½ç„¶æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰ç”Ÿæˆå·²ç»å±•ç¤ºäº†æ‰©æ•£å˜å‹å™¨æ¨¡å‹çš„ä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ï¼Œä½†å›¾åƒåŠ¨ç”»é¢†åŸŸä»ç„¶ä¸»è¦ä¾èµ–äºU-NetåŸºç¡€çš„æ‰©æ•£æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹è½åäºæœ€æ–°çš„T2Væ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†MiraMoæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å›¾åƒåŠ¨ç”»çš„æ•ˆç‡ã€å¤–è§‚ä¸€è‡´æ€§å’Œè¿åŠ¨å¹³æ»‘åº¦ã€‚å®ƒé€šè¿‡å¼•å…¥é«˜æ•ˆçº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ã€è¿åŠ¨æ®‹å·®å­¦ä¹ èŒƒå¼å’ŒDCTå™ªå£°æ¨ç†ç­–ç•¥ï¼Œæ”¹å–„äº†è®¡ç®—å¼€é”€ã€æ—¶é—´ä¸€è‡´æ€§å’Œè¿åŠ¨çªå…€é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåŠ¨ç”»é¢†åŸŸæœ‰é‡å¤§è¿›å±•ï¼Œä½†å­˜åœ¨å¤–è§‚ä¸€è‡´æ€§å’Œè¿åŠ¨è¿‡æ¸¡çš„æŒç»­æ€§æŒ‘æˆ˜ã€‚</li>
<li>æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰ç”Ÿæˆå·²ç»å–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œä½†å›¾åƒåŠ¨ç”»é¢†åŸŸä»ä¸»è¦ä¾èµ–U-NetåŸºç¡€çš„æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>MiraMoæ¡†æ¶é€šè¿‡å¼•å…¥é«˜æ•ˆçº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡å¹¶ä¿æŒäº†ç”Ÿæˆè´¨é‡ã€‚</li>
<li>MiraMoå¼•å…¥äº†è¿åŠ¨æ®‹å·®å­¦ä¹ èŒƒå¼ï¼Œä¸“æ³¨äºæ¨¡æ‹Ÿè¿åŠ¨åŠ¨åŠ›å­¦ï¼Œæé«˜äº†æ—¶é—´ä¸€è‡´æ€§ã€‚</li>
<li>DCTå™ªå£°æ¨ç†ç­–ç•¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¢«ç”¨æ¥æŠ‘åˆ¶çªç„¶çš„è¿åŠ¨ä¼ªå½±ã€‚</li>
<li>MiraMoæ¡†æ¶èƒ½å¤Ÿç”Ÿæˆä¸€è‡´ã€å¹³æ»‘å’Œå¯æ§çš„åŠ¨ç”»ï¼Œå¹¶å…·æœ‰åŠ é€Ÿçš„æ¨ç†é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07246">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07246v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07246v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07246v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07246v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07246v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07246v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Perceptual-Evaluation-of-GANs-and-Diffusion-Models-for-Generating-X-rays"><a href="#Perceptual-Evaluation-of-GANs-and-Diffusion-Models-for-Generating-X-rays" class="headerlink" title="Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays"></a>Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays</h2><p><strong>Authors:Gregory Schuit, Denis Parra, Cecilia Besa</strong></p>
<p>Generative image models have achieved remarkable progress in both natural and medical imaging. In the medical context, these techniques offer a potential solution to data scarcity-especially for low-prevalence anomalies that impair the performance of AI-driven diagnostic and segmentation tools. However, questions remain regarding the fidelity and clinical utility of synthetic images, since poor generation quality can undermine model generalizability and trust. In this study, we evaluate the effectiveness of state-of-the-art generative models-Generative Adversarial Networks (GANs) and Diffusion Models (DMs)-for synthesizing chest X-rays conditioned on four abnormalities: Atelectasis (AT), Lung Opacity (LO), Pleural Effusion (PE), and Enlarged Cardiac Silhouette (ECS). Using a benchmark composed of real images from the MIMIC-CXR dataset and synthetic images from both GANs and DMs, we conducted a reader study with three radiologists of varied experience. Participants were asked to distinguish real from synthetic images and assess the consistency between visual features and the target abnormality. Our results show that while DMs generate more visually realistic images overall, GANs can report better accuracy for specific conditions, such as absence of ECS. We further identify visual cues radiologists use to detect synthetic images, offering insights into the perceptual gaps in current models. These findings underscore the complementary strengths of GANs and DMs and point to the need for further refinement to ensure generative models can reliably augment training datasets for AI diagnostic systems. </p>
<blockquote>
<p>ç”Ÿæˆå›¾åƒæ¨¡å‹åœ¨è‡ªç„¶å’ŒåŒ»å­¦å½±åƒæ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚åœ¨åŒ»ç–—èƒŒæ™¯ä¸‹ï¼Œè¿™äº›æŠ€æœ¯ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜æä¾›äº†æ½œåœ¨è§£å†³æ–¹æ¡ˆï¼Œå°¤å…¶æ˜¯é’ˆå¯¹é‚£äº›ä½å‘ç—…ç‡å¼‚å¸¸å¯¼è‡´çš„AIè¯Šæ–­å’Œåˆ†å‰²å·¥å…·æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œå…³äºåˆæˆå›¾åƒçš„ä¿çœŸåº¦å’Œä¸´åºŠå®ç”¨æ€§ä»å­˜åœ¨ç–‘é—®ï¼Œå› ä¸ºç”Ÿæˆè´¨é‡å·®å¯èƒ½ä¼šæŸå®³æ¨¡å‹çš„é€šç”¨æ€§å’Œä¿¡ä»»åº¦ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹â€”â€”ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨åŸºäºå››ç§å¼‚å¸¸æƒ…å†µåˆæˆèƒ¸éƒ¨Xå°„çº¿å›¾åƒæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼šè‚ºä¸å¼ ï¼ˆATï¼‰ã€è‚ºå®å˜ï¼ˆLOï¼‰ã€èƒ¸è†œç§¯æ¶²ï¼ˆPEï¼‰å’Œå¿ƒè„è½®å»“å¢å¤§ï¼ˆECSï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨æ¥è‡ªMIMIC-CXRæ•°æ®é›†çš„çœŸå®å›¾åƒå’Œæ¥è‡ªGANsåŠDMsçš„åˆæˆå›¾åƒç»„æˆçš„åŸºå‡†æµ‹è¯•é›†ï¼Œå¯¹ä¸‰ä½ä¸åŒç»éªŒçš„æ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡Œäº†ä¸€é¡¹è¯»è€…ç ”ç©¶ã€‚å‚ä¸è€…è¢«è¦æ±‚åŒºåˆ†çœŸå®å’Œåˆæˆå›¾åƒï¼Œå¹¶è¯„ä¼°è§†è§‰ç‰¹å¾ä¸ç›®æ ‡å¼‚å¸¸ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶DMsæ€»ä½“ä¸Šç”Ÿæˆäº†æ›´é€¼çœŸçš„å›¾åƒï¼Œä½†GANsåœ¨ç‰¹å®šæ¡ä»¶ä¸‹çš„å‡†ç¡®æ€§æ›´é«˜ï¼Œå¦‚ECSä¸å­˜åœ¨çš„æƒ…å†µã€‚æˆ‘ä»¬è¿˜ç¡®å®šäº†æ”¾å°„ç§‘åŒ»ç”Ÿç”¨æ¥æ£€æµ‹åˆæˆå›¾åƒçš„å¯è§†çº¿ç´¢ï¼Œè¿™ä¸ºæˆ‘ä»¬æä¾›äº†å¯¹å½“å‰æ¨¡å‹æ„ŸçŸ¥å·®è·çš„è§è§£ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†GANså’ŒDMsçš„äº’è¡¥ä¼˜åŠ¿ï¼Œå¹¶æŒ‡å‡ºéœ€è¦è¿›ä¸€æ­¥æ”¹è¿›ï¼Œä»¥ç¡®ä¿ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿå¯é åœ°å¢å¼ºAIè¯Šæ–­ç³»ç»Ÿçš„è®­ç»ƒæ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07128v1">PDF</a> Accepted to the Workshop on Human-AI Collaboration at MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨åˆæˆèƒ¸éƒ¨Xå…‰ç‰‡å›¾åƒæ–¹é¢çš„è¡¨ç°ï¼Œé’ˆå¯¹å››ç§å¼‚å¸¸æƒ…å†µè¿›è¡Œäº†è¯„ä¼°ï¼šè‚ºä¸å¼ ã€è‚ºå®å˜ã€èƒ¸è†œç§¯æ¶²å’Œå¿ƒè„è½®å»“å¢å¤§ã€‚ç ”ç©¶å‘ç°ï¼Œæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒæ•´ä½“è§†è§‰æ›´çœŸå®ï¼Œè€ŒGANsåœ¨æŸäº›ç‰¹å®šæ¡ä»¶ä¸‹çš„å‡†ç¡®æ€§æ›´é«˜ã€‚æ­¤å¤–ï¼Œé€šè¿‡è¯»è€…ç ”ç©¶è¯†åˆ«äº†åŒ»ç”Ÿè¯†åˆ«åˆæˆå›¾åƒæ—¶ä½¿ç”¨çš„è§†è§‰çº¿ç´¢ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹æ„ŸçŸ¥ä¸Šçš„å·®è·ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒGANså’ŒDMså„æœ‰ä¼˜åŠ¿ï¼Œéœ€è¦è¿›ä¸€æ­¥æ”¹è¿›ä»¥ç¡®ä¿ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿå¯é åœ°å¢å¼ºAIè¯Šæ–­ç³»ç»Ÿçš„è®­ç»ƒæ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå›¾åƒæ¨¡å‹åœ¨è‡ªç„¶å’ŒåŒ»å­¦æˆåƒé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜æä¾›äº†æ½œåœ¨è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨ä½å‘ç—…ç‡å¼‚å¸¸æƒ…å†µä¸‹ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ç”Ÿæˆçš„å›¾åƒæ•´ä½“è§†è§‰æ›´çœŸå®ï¼Œè€Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰åœ¨æŸäº›ç‰¹å®šå¼‚å¸¸æƒ…å†µä¸‹è¡¨ç°æ›´å¥½ã€‚</li>
<li>åœ¨åŒºåˆ†çœŸå®å’Œåˆæˆå›¾åƒä»¥åŠè¯„ä¼°å›¾åƒè§†è§‰ç‰¹å¾ä¸ç›®æ ‡å¼‚å¸¸ä¸€è‡´æ€§æ–¹é¢ï¼Œä¸‰ä½ä¸åŒç»éªŒç¨‹åº¦çš„æ”¾å°„ç§‘åŒ»ç”Ÿå‚ä¸äº†è¯»è€…ç ”ç©¶ã€‚</li>
<li>è¯»è€…ç ”ç©¶æ­ç¤ºäº†åŒ»ç”Ÿè¯†åˆ«åˆæˆå›¾åƒæ—¶ä½¿ç”¨çš„è§†è§‰çº¿ç´¢ï¼Œçªå‡ºäº†å½“å‰æ¨¡å‹çš„æ„ŸçŸ¥å·®è·ã€‚</li>
<li>ç ”ç©¶ç»“æœå¼ºè°ƒäº†GANså’ŒDMsçš„äº’è¡¥ä¼˜åŠ¿ï¼ŒæŒ‡å‡ºäº†è¿›ä¸€æ­¥æ”¹è¿›çš„å¿…è¦æ€§ï¼Œä»¥ç¡®ä¿ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿå¯é åœ°å¢å¼ºAIè¯Šæ–­ç³»ç»Ÿçš„è®­ç»ƒæ•°æ®é›†ã€‚</li>
<li>æ–‡ç« ä½¿ç”¨MIMIC-CXRæ•°æ®é›†è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œä¸ºè¯„ä¼°ç”Ÿæˆæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸçš„æ€§èƒ½æä¾›äº†é‡è¦å‚è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07128">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07128v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07128v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07128v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07128v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07128v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Spatio-Temporal-Conditional-Diffusion-Models-for-Forecasting-Future-Multiple-Sclerosis-Lesion-Masks-Conditioned-on-Treatments"><a href="#Spatio-Temporal-Conditional-Diffusion-Models-for-Forecasting-Future-Multiple-Sclerosis-Lesion-Masks-Conditioned-on-Treatments" class="headerlink" title="Spatio-Temporal Conditional Diffusion Models for Forecasting Future   Multiple Sclerosis Lesion Masks Conditioned on Treatments"></a>Spatio-Temporal Conditional Diffusion Models for Forecasting Future   Multiple Sclerosis Lesion Masks Conditioned on Treatments</h2><p><strong>Authors:Gian Mario Favero, Ge Ya Luo, Nima Fathi, Justin Szeto, Douglas L. Arnold, Brennan Nichyporuk, Chris Pal, Tal Arbel</strong></p>
<p>Image-based personalized medicine has the potential to transform healthcare, particularly for diseases that exhibit heterogeneous progression such as Multiple Sclerosis (MS). In this work, we introduce the first treatment-aware spatio-temporal diffusion model that is able to generate future masks demonstrating lesion evolution in MS. Our voxel-space approach incorporates multi-modal patient data, including MRI and treatment information, to forecast new and enlarging T2 (NET2) lesion masks at a future time point. Extensive experiments on a multi-centre dataset of 2131 patient 3D MRIs from randomized clinical trials for relapsing-remitting MS demonstrate that our generative model is able to accurately predict NET2 lesion masks for patients across six different treatments. Moreover, we demonstrate our model has the potential for real-world clinical applications through downstream tasks such as future lesion count and location estimation, binary lesion activity classification, and generating counterfactual future NET2 masks for several treatments with different efficacies. This work highlights the potential of causal, image-based generative models as powerful tools for advancing data-driven prognostics in MS. </p>
<blockquote>
<p>åŸºäºå›¾åƒçš„ä¸ªæ€§åŒ–åŒ»ç–—å…·æœ‰æ”¹å˜åŒ»ç–—ä¿å¥çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šå‘æ€§ç¡¬åŒ–ç—‡ï¼ˆMSï¼‰ç­‰è¡¨ç°å‡ºå¼‚è´¨è¿›å±•çš„ç–¾ç—…ä¸­ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¬¬ä¸€ä¸ªèƒ½ç”Ÿæˆæœªæ¥æ©ç çš„æ—¶ç©ºæ‰©æ•£æ¨¡å‹ï¼Œè¿™äº›æ©ç æ˜¾ç¤ºMSä¸­çš„ç—…ç¶æ¼”å˜ã€‚æˆ‘ä»¬çš„ä½“ç´ ç©ºé—´æ–¹æ³•ç»“åˆäº†å¤šæ¨¡æ€æ‚£è€…æ•°æ®ï¼ŒåŒ…æ‹¬MRIå’Œæ²»ç–—ä¿¡æ¯ï¼Œä»¥é¢„æµ‹æœªæ¥æŸä¸ªæ—¶é—´ç‚¹çš„æ–°çš„å’Œæ‰©å¤§çš„T2ï¼ˆNET2ï¼‰ç—…ç¶æ©ç ã€‚åœ¨æ¥è‡ªå¤å‘ç¼“è§£å‹MSéšæœºä¸´åºŠè¯•éªŒçš„2131åæ‚£è€…çš„å¤šä¸­å¿ƒæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹å…­ç§ä¸åŒæ²»ç–—æ–¹æ³•ä¸‹æ‚£è€…çš„NET2ç—…ç¶æ©ç ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ä¸‹æ¸¸ä»»åŠ¡å±•ç¤ºäº†æˆ‘ä»¬çš„æ¨¡å‹åœ¨ç°å®ä¸´åºŠåº”ç”¨ä¸­çš„æ½œåŠ›ï¼Œå¦‚æœªæ¥çš„ç—…ç¶è®¡æ•°å’Œä½ç½®ä¼°è®¡ã€äºŒå…ƒç—…ç¶æ´»åŠ¨åˆ†ç±»ä»¥åŠä¸ºå‡ ç§ä¸åŒç–—æ•ˆçš„æ²»ç–—ç”Ÿæˆåäº‹å®æœªæ¥çš„NET2æ©ç ã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†å› æœã€åŸºäºå›¾åƒçš„ç”Ÿæˆæ¨¡å‹ä½œä¸ºæ¨è¿›MSæ•°æ®é©±åŠ¨é¢„åé¢„æµ‹çš„å¼ºå¤§å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07006v1">PDF</a> Accepted to MICCAI 2025 (LMID Workshop)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é¦–ä¸ªæ²»ç–—æ„ŸçŸ¥çš„æ—¶ç©ºæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå±•ç¤ºå¤šå‘æ€§ç¡¬åŒ–ç—‡ï¼ˆMSï¼‰ç—…å˜æ¼”å˜çš„æœªæ¥æ©è†œã€‚é€šè¿‡ç»“åˆå¤šæ¨¡æ€æ‚£è€…æ•°æ®ï¼ˆåŒ…æ‹¬MRIå’Œæ²»ç–—ä¿¡æ¯ï¼‰ï¼Œè¯¥æ¨¡å‹åœ¨éšæœºä¸´åºŠè¯•éªŒçš„å¤šä¸­å¿ƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œå‡†ç¡®é¢„æµ‹äº†å…­ç§ä¸åŒæ²»ç–—ä¸‹æ‚£è€…çš„NET2ç—…å˜æ©è†œã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜å±•ç¤ºäº†åœ¨ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚æœªæ¥ç—…å˜è®¡æ•°å’Œä½ç½®ä¼°è®¡ã€äºŒå…ƒç—…å˜æ´»åŠ¨åˆ†ç±»ä»¥åŠä¸ºä¸åŒç–—æ•ˆçš„æ²»ç–—ç”Ÿæˆåäº‹å®æœªæ¥NET2æ©è†œï¼‰ä¸­çš„å®é™…åº”ç”¨æ½œåŠ›ï¼Œçªæ˜¾äº†å› æœå›¾åƒç”Ÿæˆæ¨¡å‹åœ¨æ¨è¿›å¤šå‘æ€§ç¡¬åŒ–ç—‡æ•°æ®é©±åŠ¨é¢„åä¸­çš„å¼ºå¤§ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†é¦–ä¸ªæ²»ç–—æ„ŸçŸ¥çš„æ—¶ç©ºæ‰©æ•£æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆå±•ç¤ºå¤šå‘æ€§ç¡¬åŒ–ç—‡ï¼ˆMSï¼‰ç—…å˜æ¼”å˜çš„æœªæ¥æ©è†œã€‚</li>
<li>é€šè¿‡ç»“åˆå¤šæ¨¡æ€æ‚£è€…æ•°æ®ï¼ˆåŒ…æ‹¬MRIå’Œæ²»ç–—ä¿¡æ¯ï¼‰ï¼Œåœ¨é¢„æµ‹MSç—…å˜æ¼”å˜æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä¸ªæ²»ç–—ä¸‹çš„æ‚£è€…æ•°æ®ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œå‡†ç¡®é¢„æµ‹äº†NET2ç—…å˜æ©è†œã€‚</li>
<li>æ¨¡å‹å±•ç¤ºäº†åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å®é™…åº”ç”¨æ½œåŠ›ï¼ŒåŒ…æ‹¬æœªæ¥ç—…å˜è®¡æ•°å’Œä½ç½®ä¼°è®¡ã€äºŒå…ƒç—…å˜æ´»åŠ¨åˆ†ç±»ç­‰ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆä¸åŒæ²»ç–—ä¸‹çš„åäº‹å®æœªæ¥NET2æ©è†œï¼Œä¸ºä¸´åºŠå†³ç­–æä¾›æ”¯æŒã€‚</li>
<li>æœ¬ç ”ç©¶çªæ˜¾äº†å› æœå›¾åƒç”Ÿæˆæ¨¡å‹åœ¨æ¨è¿›æ•°æ®é©±åŠ¨çš„å¤šå‘æ€§ç¡¬åŒ–ç—‡é¢„åä¸­çš„é‡è¦ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07006v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07006v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.07006v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="WeatherDiffusion-Weather-Guided-Diffusion-Model-for-Forward-and-Inverse-Rendering"><a href="#WeatherDiffusion-Weather-Guided-Diffusion-Model-for-Forward-and-Inverse-Rendering" class="headerlink" title="WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse   Rendering"></a>WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse   Rendering</h2><p><strong>Authors:Yixin Zhu, Zuoliang Zhu, MiloÅ¡ HaÅ¡an, Jian Yang, Jin Xie, Beibei Wang</strong></p>
<p>Forward and inverse rendering have emerged as key techniques for enabling understanding and reconstruction in the context of autonomous driving (AD). However, complex weather and illumination pose great challenges to this task. The emergence of large diffusion models has shown promise in achieving reasonable results through learning from 2D priors, but these models are difficult to control and lack robustness. In this paper, we introduce WeatherDiffusion, a diffusion-based framework for forward and inverse rendering on AD scenes with various weather and lighting conditions. Our method enables authentic estimation of material properties, scene geometry, and lighting, and further supports controllable weather and illumination editing through the use of predicted intrinsic maps guided by text descriptions. We observe that different intrinsic maps should correspond to different regions of the original image. Based on this observation, we propose Intrinsic map-aware attention (MAA) to enable high-quality inverse rendering. Additionally, we introduce a synthetic dataset (\ie WeatherSynthetic) and a real-world dataset (\ie WeatherReal) for forward and inverse rendering on AD scenes with diverse weather and lighting. Extensive experiments show that our WeatherDiffusion outperforms state-of-the-art methods on several benchmarks. Moreover, our method demonstrates significant value in downstream tasks for AD, enhancing the robustness of object detection and image segmentation in challenging weather scenarios. </p>
<blockquote>
<p>æ­£å‘å’Œé€†å‘æ¸²æŸ“æŠ€æœ¯åœ¨è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰çš„ä¸Šä¸‹æ–‡ä¸­å·²ç»æˆä¸ºå®ç°ç†è§£å’Œé‡å»ºçš„å…³é”®æŠ€æœ¯ã€‚ç„¶è€Œï¼Œå¤æ‚çš„å¤©æ°”å’Œå…‰ç…§æ¡ä»¶ç»™è¿™ä¸€ä»»åŠ¡å¸¦æ¥äº†å¾ˆå¤§çš„æŒ‘æˆ˜ã€‚å¤§å‹æ‰©æ•£æ¨¡å‹çš„å…´èµ·æ˜¾ç¤ºå‡ºé€šè¿‡å­¦ä¹ äºŒç»´å…ˆéªŒçŸ¥è¯†å®ç°åˆç†ç»“æœçš„æ½œåŠ›ï¼Œä½†è¿™äº›æ¨¡å‹éš¾ä»¥æ§åˆ¶å’Œç¼ºä¹ç¨³å¥æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†WeatherDiffusionï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œç”¨äºåœ¨å…·æœ‰å„ç§å¤©æ°”å’Œå…‰ç…§æ¡ä»¶çš„ADåœºæ™¯ä¸Šè¿›è¡Œæ­£å‘å’Œé€†å‘æ¸²æŸ“ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤ŸçœŸå®ä¼°è®¡ææ–™å±æ€§ã€åœºæ™¯å‡ ä½•å’Œå…‰ç…§ï¼Œå¹¶ä¸”è¿›ä¸€æ­¥é€šè¿‡é¢„æµ‹çš„å›ºæœ‰åœ°å›¾å’Œæ–‡æœ¬æè¿°çš„æ”¯æŒæ¥å®ç°å¯æ§çš„å¤©æ°”å’Œå…‰ç…§ç¼–è¾‘ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸åŒçš„å›ºæœ‰åœ°å›¾åº”è¯¥å¯¹åº”äºåŸå§‹å›¾åƒçš„ä¸åŒåŒºåŸŸã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†å›ºæœ‰åœ°å›¾æ„ŸçŸ¥æ³¨æ„åŠ›ï¼ˆMAAï¼‰ä»¥å®ç°é«˜è´¨é‡çš„åå‘æ¸²æŸ“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†åˆæˆæ•°æ®é›†ï¼ˆå³WeatherSyntheticï¼‰å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ï¼ˆå³WeatherRealï¼‰ï¼Œç”¨äºåœ¨å…·æœ‰å„ç§å¤©æ°”å’Œå…‰ç…§æ¡ä»¶çš„ADåœºæ™¯ä¸Šè¿›è¡Œæ­£å‘å’Œé€†å‘æ¸²æŸ“ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„WeatherDiffusionåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è€Œä¸”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è‡ªåŠ¨é©¾é©¶çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ä»·å€¼ï¼Œæé«˜äº†åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤©æ°”æƒ…å†µä¸‹å¯¹è±¡æ£€æµ‹å’Œå›¾åƒåˆ†å‰²çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06982v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰åœºæ™¯ä¸­çš„å¤©æ°”å’Œå…‰ç…§å˜åŒ–ï¼Œé‡‡ç”¨æ‰©æ•£æ¨¡å‹å®ç°çš„æ¸²æŸ“æŠ€æœ¯ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªåä¸ºWeatherDiffusionçš„æ¡†æ¶ï¼Œç»“åˆäº†æ­£å‘å’Œé€†å‘æ¸²æŸ“æŠ€æœ¯ï¼Œå®ç°äº†åœ¨ä¸åŒå¤©æ°”å’Œå…‰ç…§æ¡ä»¶ä¸‹çš„åœºæ™¯é‡å»ºã€‚è¯¥æ¡†æ¶å¯ä¼°ç®—æè´¨å±æ€§ã€åœºæ™¯å‡ ä½•ç»“æ„å’Œå…‰ç…§ï¼Œå¹¶æ”¯æŒé€šè¿‡æ–‡æœ¬æè¿°è¿›è¡Œå¯æ§çš„å¤©æ°”å’Œå…‰ç…§ç¼–è¾‘ã€‚ä¸ºæé«˜é€†å‘æ¸²æŸ“è´¨é‡ï¼Œæ–‡ç« è¿˜æå‡ºäº†åŸºäºå†…åœ¨å›¾è°±æ„ŸçŸ¥çš„æ³¨æ„åŠ›æœºåˆ¶ï¼ˆIntrinsic map-aware attentionï¼ŒMAAï¼‰ã€‚æ­¤å¤–ï¼Œæ–‡ç« ä»‹ç»äº†åˆæˆæ•°æ®é›†ï¼ˆWeatherSyntheticï¼‰å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ï¼ˆWeatherRealï¼‰ï¼Œå¹¶é€šè¿‡å¤§é‡å®éªŒéªŒè¯äº†WeatherDiffusionåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸‹æ¸¸è‡ªåŠ¨é©¾é©¶ä»»åŠ¡ä¸­çš„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†è‡ªåŠ¨é©¾é©¶ä¸­å¤©æ°”å’Œå…‰ç…§å˜åŒ–çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºWeatherDiffusionçš„æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œç”¨äºåº”å¯¹ä¸åŒå¤©æ°”å’Œå…‰ç…§æ¡ä»¶ä¸‹çš„æ­£å‘å’Œé€†å‘æ¸²æŸ“ã€‚</li>
<li>WeatherDiffusionå¯ä¼°ç®—æè´¨å±æ€§ã€åœºæ™¯å‡ ä½•ç»“æ„å’Œå…‰ç…§ï¼Œå¹¶æ”¯æŒå¯æ§çš„å¤©æ°”å’Œå…‰ç…§ç¼–è¾‘ã€‚</li>
<li>æå‡ºåŸºäºå†…åœ¨å›¾è°±æ„ŸçŸ¥çš„æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMAAï¼‰ä»¥æé«˜é€†å‘æ¸²æŸ“è´¨é‡ã€‚</li>
<li>ä»‹ç»äº†åˆæˆæ•°æ®é›†ï¼ˆWeatherSyntheticï¼‰å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ï¼ˆWeatherRealï¼‰ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯çš„æ¸²æŸ“ç ”ç©¶ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒWeatherDiffusionåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06982">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.06982v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.06982v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.06982v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.06982v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.06982v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2508.06982v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MoGA-3D-Generative-Avatar-Prior-for-Monocular-Gaussian-Avatar-Reconstruction"><a href="#MoGA-3D-Generative-Avatar-Prior-for-Monocular-Gaussian-Avatar-Reconstruction" class="headerlink" title="MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar   Reconstruction"></a>MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar   Reconstruction</h2><p><strong>Authors:Zijian Dong, Longteng Duan, Jie Song, Michael J. Black, Andreas Geiger</strong></p>
<p>We present MoGA, a novel method to reconstruct high-fidelity 3D Gaussian avatars from a single-view image. The main challenge lies in inferring unseen appearance and geometric details while ensuring 3D consistency and realism. Most previous methods rely on 2D diffusion models to synthesize unseen views; however, these generated views are sparse and inconsistent, resulting in unrealistic 3D artifacts and blurred appearance. To address these limitations, we leverage a generative avatar model, that can generate diverse 3D avatars by sampling deformed Gaussians from a learned prior distribution. Due to limited 3D training data, such a 3D model alone cannot capture all image details of unseen identities. Consequently, we integrate it as a prior, ensuring 3D consistency by projecting input images into its latent space and enforcing additional 3D appearance and geometric constraints. Our novel approach formulates Gaussian avatar creation as model inversion by fitting the generative avatar to synthetic views from 2D diffusion models. The generative avatar provides an initialization for model fitting, enforces 3D regularization, and helps in refining pose. Experiments show that our method surpasses state-of-the-art techniques and generalizes well to real-world scenarios. Our Gaussian avatars are also inherently animatable. For code, see <a target="_blank" rel="noopener" href="https://zj-dong.github.io/MoGA/">https://zj-dong.github.io/MoGA/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†MoGAè¿™ä¸€æ–°æ–¹æ³•ï¼Œå¯ä»¥ä»å•è§†è§’å›¾åƒé‡å»ºé«˜ä¿çœŸ3Dé«˜æ–¯å¤´åƒã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºæ¨æ–­å‡ºä¸å¯è§çš„å¤–è§‚å’Œå‡ ä½•ç»†èŠ‚ï¼ŒåŒæ—¶ç¡®ä¿3Dä¸€è‡´æ€§å’Œé€¼çœŸæ€§ã€‚ä¹‹å‰çš„å¤§å¤šæ•°æ–¹æ³•éƒ½ä¾èµ–äº2Dæ‰©æ•£æ¨¡å‹æ¥åˆæˆæœªè§è§†å›¾ï¼›ç„¶è€Œï¼Œè¿™äº›ç”Ÿæˆçš„è§†å›¾ç¨€ç–ä¸”ä¸ä¸€è‡´ï¼Œå¯¼è‡´3Dä¼ªå½±å’Œæ¨¡ç³Šçš„å¤–è§‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨ç”Ÿæˆå¤´åƒæ¨¡å‹ï¼Œé€šè¿‡ä»å­¦ä¹ åˆ°çš„å…ˆéªŒåˆ†å¸ƒä¸­é‡‡æ ·å˜å½¢é«˜æ–¯åˆ†å¸ƒï¼Œå¯ä»¥ç”Ÿæˆå¤šæ ·åŒ–çš„3Då¤´åƒã€‚ç”±äºæœ‰é™çš„3Dè®­ç»ƒæ•°æ®ï¼Œä»…ä½¿ç”¨è¿™æ ·çš„3Dæ¨¡å‹æ— æ³•æ•è·æœªè§èº«ä»½çš„æ‰€æœ‰å›¾åƒç»†èŠ‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†å…¶æ•´åˆä¸ºå…ˆéªŒï¼Œé€šè¿‡å°†è¾“å…¥å›¾åƒæŠ•å½±åˆ°å…¶æ½œåœ¨ç©ºé—´å¹¶æ–½åŠ é¢å¤–çš„3Då¤–è§‚å’Œå‡ ä½•çº¦æŸæ¥ç¡®ä¿3Dä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„æ–°æ–¹æ³•å°†é«˜æ–¯å¤´åƒåˆ›å»ºè¡¨è¿°ä¸ºæ¨¡å‹åæ¼”ï¼Œé€šè¿‡å°†ç”Ÿæˆå¤´åƒæ‹Ÿåˆåˆ°æ¥è‡ª2Dæ‰©æ•£æ¨¡å‹çš„åˆæˆè§†å›¾æ¥å®ç°ã€‚ç”Ÿæˆå¤´åƒä¸ºæ¨¡å‹æ‹Ÿåˆæä¾›äº†åˆå§‹åŒ–ï¼Œå¼ºåˆ¶å®æ–½3Dæ­£åˆ™åŒ–ï¼Œå¹¶æœ‰åŠ©äºä¼˜åŒ–å§¿æ€ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†æœ€å…ˆè¿›çš„æŠ€æœ¯ï¼Œå¹¶èƒ½å¾ˆå¥½åœ°æ¨å¹¿åˆ°ç°å®ä¸–ç•Œåœºæ™¯ã€‚æˆ‘ä»¬çš„é«˜æ–¯å¤´åƒä¹Ÿå…·æœ‰å†…åœ¨çš„å¯åŠ¨ç”»æ€§ã€‚æœ‰å…³ä»£ç ï¼Œè¯·å‚è§<a target="_blank" rel="noopener" href="https://zj-dong.github.io/MoGA/%E3%80%82">https://zj-dong.github.io/MoGA/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23597v3">PDF</a> ICCV 2025 (Highlight), Project Page: <a target="_blank" rel="noopener" href="https://zj-dong.github.io/MoGA/">https://zj-dong.github.io/MoGA/</a></p>
<p><strong>Summary</strong><br>    MoGAæ–¹æ³•èƒ½å¤Ÿä»å•è§†è§’å›¾åƒé‡å»ºé«˜ä¿çœŸ3Dé«˜æ–¯åŒ–èº«ã€‚å®ƒé€šè¿‡ç»“åˆç”ŸæˆåŒ–èº«æ¨¡å‹å’Œ2Dæ‰©æ•£æ¨¡å‹ï¼Œè§£å†³äº†ä¹‹å‰æ–¹æ³•ç”Ÿæˆçš„è§†å›¾ç¨€ç–å’Œä¸ä¸€è‡´çš„é—®é¢˜ï¼Œç¡®ä¿äº†3Dä¸€è‡´æ€§å¹¶å‘ˆç°å‡ºæ›´çœŸå®çš„è§†è§‰æ•ˆæœã€‚å®éªŒè¡¨æ˜ï¼ŒMoGAè¶…è¶Šäº†å½“å‰çš„æŠ€æœ¯å¹¶å¾ˆå¥½åœ°é€‚åº”ç°å®åœºæ™¯ã€‚å…¶åˆ›å»ºçš„GaussianåŒ–èº«å…·æœ‰å†…åœ¨çš„å¯åŠ¨ç”»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MoGAæ˜¯ä¸€ä¸ªèƒ½å¤Ÿä»å•è§†è§’å›¾åƒé‡å»ºé«˜ä¿çœŸ3Dé«˜æ–¯åŒ–èº«çš„æ–°æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆç”ŸæˆåŒ–èº«æ¨¡å‹å’Œ2Dæ‰©æ•£æ¨¡å‹ï¼Œè§£å†³äº†ä¹‹å‰æ–¹æ³•çš„è§†å›¾ç¨€ç–å’Œä¸ä¸€è‡´é—®é¢˜ã€‚</li>
<li>MoGAç¡®ä¿äº†3Dä¸€è‡´æ€§å¹¶å‘ˆç°å‡ºæ›´çœŸå®çš„è§†è§‰æ•ˆæœã€‚</li>
<li>MoGAé€šè¿‡æ¨¡å‹æ‹Ÿåˆå°†ç”ŸæˆåŒ–èº«ä½œä¸ºåˆå§‹åŒ–ï¼Œå¹¶æ‰§è¡Œ3Dæ­£åˆ™åŒ–ä»¥ä¼˜åŒ–å§¿æ€ã€‚</li>
<li>è¯¥æ–¹æ³•è¶…è¶Šäº†å½“å‰æŠ€æœ¯å¹¶å¾ˆå¥½åœ°é€‚åº”ç°å®åœºæ™¯ã€‚</li>
<li>MoGAåˆ›å»ºçš„GaussianåŒ–èº«å…·æœ‰å†…åœ¨çš„å¯åŠ¨ç”»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2507.23597v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2507.23597v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2507.23597v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="EF-VI-Enhancing-End-Frame-Injection-for-Video-Inbetweening"><a href="#EF-VI-Enhancing-End-Frame-Injection-for-Video-Inbetweening" class="headerlink" title="EF-VI: Enhancing End-Frame Injection for Video Inbetweening"></a>EF-VI: Enhancing End-Frame Injection for Video Inbetweening</h2><p><strong>Authors:Liuhan Chen, Xiaodong Cun, Xiaoyu Li, Xianyi He, Shenghai Yuan, Jie Chen, Ying Shan, Li Yuan</strong></p>
<p>Video inbetweening aims to synthesize intermediate video sequences conditioned on the given start and end frames. Current state-of-the-art methods primarily extend large-scale pre-trained Image-to-Video Diffusion Models (I2V-DMs) by incorporating the end-frame condition via direct fine-tuning or temporally bidirectional sampling. However, the former results in a weak end-frame constraint, while the latter inevitably disrupts the input representation of video frames, leading to suboptimal performance. To improve the end-frame constraint while avoiding disruption of the input representation, we propose a novel video inbetweening framework specific to recent and more powerful transformer-based I2V-DMs, termed EF-VI. It efficiently strengthens the end-frame constraint by utilizing an enhanced injection. This is based on our proposed well-designed lightweight module, termed EF-Net, which encodes only the end frame and expands it into temporally adaptive frame-wise features injected into the I2V-DM. Extensive experiments demonstrate the superiority of our EF-VI compared with other baselines. </p>
<blockquote>
<p>è§†é¢‘æ’å¸§æ—¨åœ¨æ ¹æ®ç»™å®šçš„èµ·å§‹å¸§å’Œç»“æŸå¸§åˆæˆä¸­é—´è§†é¢‘åºåˆ—ã€‚å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ä¸»è¦æ˜¯é€šè¿‡å°†ç»“æŸå¸§æ¡ä»¶èå…¥å¤§è§„æ¨¡é¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆI2V-DMsï¼‰ï¼Œé€šè¿‡ç›´æ¥å¾®è°ƒæˆ–æ—¶é—´åŒå‘é‡‡æ ·è¿›è¡Œæ‰©å±•ã€‚ç„¶è€Œï¼Œå‰è€…å¯¼è‡´ç»“æŸå¸§çº¦æŸè¾ƒå¼±ï¼Œè€Œåè€…ä¸å¯é¿å…åœ°ç ´åäº†è§†é¢‘å¸§çš„è¾“å…¥è¡¨ç¤ºï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†åŠ å¼ºç»“æŸå¸§çš„çº¦æŸåŒæ—¶é¿å…ç ´åè¾“å…¥è¡¨ç¤ºï¼Œæˆ‘ä»¬é’ˆå¯¹æœ€æ–°ã€æ›´å¼ºå¤§çš„åŸºäºtransformerçš„I2V-DMsï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„è§†é¢‘æ’å¸§æ¡†æ¶ï¼Œç§°ä¸ºEF-VIã€‚å®ƒé€šè¿‡åˆ©ç”¨å¢å¼ºçš„æ³¨å…¥æ¥æœ‰æ•ˆåœ°åŠ å¼ºç»“æŸå¸§çš„çº¦æŸã€‚è¿™æ˜¯åŸºäºæˆ‘ä»¬æå‡ºçš„ç²¾å¿ƒè®¾è®¡çš„å°å‹æ¨¡å—EF-Netï¼Œå®ƒåªç¼–ç ç»“æŸå¸§å¹¶å°†å…¶æ‰©å±•ä¸ºæ³¨å…¥åˆ°I2V-DMä¸­çš„æ—¶é—´è‡ªé€‚åº”å¸§çº§ç‰¹å¾ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„EF-VIä¸å…¶ä»–åŸºçº¿ç›¸æ¯”å…·æœ‰ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21205v2">PDF</a> 17 pages, 11 figures</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹è§†é¢‘æ’å¸§é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦åŸºäºå¤§å‹é¢„è®­ç»ƒå›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆI2V-DMï¼‰ï¼Œä½†å­˜åœ¨ç«¯å¸§çº¦æŸä¸è¶³å’Œè¾“å…¥è¡¨ç¤ºç ´åçš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘æ’å¸§æ¡†æ¶EF-VIï¼Œå®ƒå¼ºåŒ–äº†ç«¯å¸§çº¦æŸï¼ŒåŒæ—¶é¿å…äº†è¾“å…¥è¡¨ç¤ºçš„ç ´åã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä¸€ä¸ªåä¸ºEF-Netçš„è½»é‡çº§æ¨¡å—ï¼Œä»…å¯¹ç«¯å¸§è¿›è¡Œç¼–ç ï¼Œå¹¶æ‰©å±•ä¸ºæ—¶é—´é€‚åº”æ€§å¸§çº§ç‰¹å¾ï¼Œæ³¨å…¥åˆ°I2V-DMä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ’å¸§çš„ç›®æ ‡æ˜¯åˆæˆç»™å®šèµ·å§‹å’Œç»“æŸå¸§ä¹‹é—´çš„ä¸­é—´è§†é¢‘åºåˆ—ã€‚</li>
<li>å½“å‰å…ˆè¿›çš„æ–¹æ³•ä¸»è¦é€šè¿‡æ‰©å±•é¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆI2V-DMï¼‰æ¥è¿›è¡Œè§†é¢‘æ’å¸§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨ç«¯å¸§çº¦æŸä¸è¶³å’Œè¾“å…¥è¡¨ç¤ºç ´åçš„é—®é¢˜ã€‚</li>
<li>æˆ‘ä»¬æå‡ºçš„EF-VIæ¡†æ¶æ—¨åœ¨å¼ºåŒ–ç«¯å¸§çº¦æŸï¼ŒåŒæ—¶é¿å…è¾“å…¥è¡¨ç¤ºçš„ç ´åã€‚</li>
<li>EF-VIåˆ©ç”¨åä¸ºEF-Netçš„è½»é‡çº§æ¨¡å—ï¼Œä»…å¯¹ç«¯å¸§è¿›è¡Œç¼–ç ï¼Œå¹¶è½¬åŒ–ä¸ºæ—¶é—´é€‚åº”æ€§å¸§çº§ç‰¹å¾ã€‚</li>
<li>EF-Netæ¨¡å—è¢«æ³¨å…¥åˆ°I2V-DMä¸­ï¼Œä»¥æé«˜æ•ˆç‡å¹¶å¼ºåŒ–ç«¯å¸§çº¦æŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21205">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2505.21205v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2505.21205v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2505.21205v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Anchor-Token-Matching-Implicit-Structure-Locking-for-Training-free-AR-Image-Editing"><a href="#Anchor-Token-Matching-Implicit-Structure-Locking-for-Training-free-AR-Image-Editing" class="headerlink" title="Anchor Token Matching: Implicit Structure Locking for Training-free AR   Image Editing"></a>Anchor Token Matching: Implicit Structure Locking for Training-free AR   Image Editing</h2><p><strong>Authors:Taihang Hu, Linxuan Li, Kai Wang, Yaxing Wang, Jian Yang, Ming-Ming Cheng</strong></p>
<p>Text-to-image generation has seen groundbreaking advancements with diffusion models, enabling high-fidelity synthesis and precise image editing through cross-attention manipulation. Recently, autoregressive (AR) models have re-emerged as powerful alternatives, leveraging next-token generation to match diffusion models. However, existing editing techniques designed for diffusion models fail to translate directly to AR models due to fundamental differences in structural control. Specifically, AR models suffer from spatial poverty of attention maps and sequential accumulation of structural errors during image editing, which disrupt object layouts and global consistency. In this work, we introduce Implicit Structure Locking (ISLock), the first training-free editing strategy for AR visual models. Rather than relying on explicit attention manipulation or fine-tuning, ISLock preserves structural blueprints by dynamically aligning self-attention patterns with reference images through the Anchor Token Matching (ATM) protocol. By implicitly enforcing structural consistency in latent space, our method ISLock enables structure-aware editing while maintaining generative autonomy. Extensive experiments demonstrate that ISLock achieves high-quality, structure-consistent edits without additional training and is superior or comparable to conventional editing techniques. Our findings pioneer the way for efficient and flexible AR-based image editing, further bridging the performance gap between diffusion and autoregressive generative models. The code will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/hutaiHang/ATM">https://github.com/hutaiHang/ATM</a> </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelsï¼‰çš„æ¨åŠ¨ä¸‹å–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›æ“çºµï¼ˆcross-attention manipulationï¼‰å®ç°äº†é«˜ä¿çœŸåˆæˆå’Œç²¾ç¡®å›¾åƒç¼–è¾‘ã€‚è¿‘æœŸï¼Œè‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹ä½œä¸ºå¼ºå¤§çš„æ›¿ä»£æ–¹æ¡ˆé‡æ–°å‡ºç°ï¼Œåˆ©ç”¨ä¸‹ä¸€ä»£ä»¤ç‰Œç”Ÿæˆï¼ˆnext-token generationï¼‰ä¸æ‰©æ•£æ¨¡å‹ç›¸åŒ¹é…ã€‚ç„¶è€Œï¼Œé’ˆå¯¹æ‰©æ•£æ¨¡å‹è®¾è®¡çš„ç°æœ‰ç¼–è¾‘æŠ€æœ¯æ— æ³•ç›´æ¥åº”ç”¨äºARæ¨¡å‹ï¼Œå› ä¸ºå®ƒä»¬åœ¨ç»“æ„æ§åˆ¶ä¸Šå­˜åœ¨æ ¹æœ¬å·®å¼‚ã€‚å…·ä½“æ¥è¯´ï¼ŒARæ¨¡å‹å­˜åœ¨æ³¨æ„åŠ›å›¾çš„ç©ºé—´è´«å›°é—®é¢˜ï¼ˆspatial poverty of attention mapsï¼‰ä»¥åŠåœ¨å›¾åƒç¼–è¾‘è¿‡ç¨‹ä¸­åºåˆ—ç´¯ç§¯çš„ç»“æ„é”™è¯¯ï¼ˆsequential accumulation of structural errorsï¼‰ï¼Œè¿™ç ´åäº†å¯¹è±¡å¸ƒå±€å’Œå…¨å±€ä¸€è‡´æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†æ— éœ€è®­ç»ƒçš„ç¼–è¾‘ç­–ç•¥â€”â€”éšå¼ç»“æ„é”å®šï¼ˆISLockï¼‰ï¼Œè¿™æ˜¯é’ˆå¯¹ARè§†è§‰æ¨¡å‹çš„é¦–ä¸ªæ­¤ç±»ç­–ç•¥ã€‚ISLockä¸ä¾èµ–äºæ˜¾å¼æ³¨æ„åŠ›æ“çºµæˆ–å¾®è°ƒï¼Œè€Œæ˜¯é€šè¿‡é”šç‚¹ä»¤ç‰ŒåŒ¹é…ï¼ˆATMï¼‰åè®®åŠ¨æ€åœ°å°†è‡ªæˆ‘æ³¨æ„åŠ›æ¨¡å¼ä¸å‚è€ƒå›¾åƒå¯¹é½ï¼Œä»è€Œä¿ç•™ç»“æ„è“å›¾ã€‚é€šè¿‡åœ¨æ½œåœ¨ç©ºé—´ä¸­éšå¼å¼ºåˆ¶æ‰§è¡Œç»“æ„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬çš„ISLockæ–¹æ³•èƒ½å¤Ÿåœ¨ä¿æŒç”Ÿæˆè‡ªä¸»æ€§çš„åŒæ—¶è¿›è¡Œç»“æ„æ„ŸçŸ¥ç¼–è¾‘ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒISLockæ— éœ€é¢å¤–è®­ç»ƒå³å¯å®ç°é«˜è´¨é‡ã€ç»“æ„ä¸€è‡´æ€§çš„ç¼–è¾‘ï¼Œå¹¶ä¸”åœ¨ä¼ ç»Ÿç¼–è¾‘æŠ€æœ¯ä¸Šå…·æœ‰ä¼˜åŠ¿æˆ–ä¸ä¹‹ç›¸å½“ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºé«˜æ•ˆã€çµæ´»çš„è‡ªå›å½’ï¼ˆARï¼‰å›¾åƒç¼–è¾‘æ–¹å¼å¼€è¾Ÿäº†é“è·¯ï¼Œè¿›ä¸€æ­¥ç¼©å°äº†æ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’ç”Ÿæˆæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/hutaiHang/ATM%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/hutaiHang/ATMå…¬å¼€æä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10434v2">PDF</a> Accepted by ICCV2025. Code will be released in   <a target="_blank" rel="noopener" href="https://github.com/hutaiHang/ATM">https://github.com/hutaiHang/ATM</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œèƒ½å¤Ÿå®ç°é«˜ä¿çœŸåˆæˆå’Œé€šè¿‡è·¨æ³¨æ„åŠ›æ“çºµç²¾ç¡®å›¾åƒç¼–è¾‘ã€‚è¿‘æœŸï¼Œè‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹ä½œä¸ºå¼ºå¤§çš„æ›¿ä»£æ–¹æ³•é‡æ–°å‡ºç°ï¼Œåˆ©ç”¨ä¸‹ä¸€ä»£æ ‡è®°åŒ¹é…æ‰©æ•£æ¨¡å‹ã€‚ç„¶è€Œï¼Œä¸ºæ‰©æ•£æ¨¡å‹è®¾è®¡çš„ç¼–è¾‘æŠ€æœ¯æ— æ³•ç›´æ¥åº”ç”¨äºARæ¨¡å‹ï¼Œå› ä¸ºä¸¤è€…åœ¨ç»“æ„æ§åˆ¶ä¸Šå­˜åœ¨æ ¹æœ¬å·®å¼‚ã€‚ARæ¨¡å‹é¢ä¸´æ³¨æ„åŠ›å›¾çš„ç©ºé—´è´«å›°å’Œå›¾åƒç¼–è¾‘ä¸­ç»“æ„é”™è¯¯çš„é¡ºåºç´¯ç§¯é—®é¢˜ï¼Œä¼šç ´åå¯¹è±¡å¸ƒå±€å’Œå…¨å±€ä¸€è‡´æ€§ã€‚æœ¬ç ”ç©¶æå‡ºæ— è®­ç»ƒç¼–è¾‘ç­–ç•¥â€”â€”éšå¼ç»“æ„é”å®šï¼ˆISLockï¼‰ï¼Œä½œä¸ºARè§†è§‰æ¨¡å‹çš„é¦–åˆ›ã€‚ISLocké€šè¿‡åŠ¨æ€å¯¹é½å‚è€ƒå›¾åƒçš„è‡ªæˆ‘æ³¨æ„åŠ›æ¨¡å¼ï¼Œå€ŸåŠ©é”šæ ‡è®°åŒ¹é…ï¼ˆATMï¼‰åè®®ï¼Œä¿ç•™ç»“æ„è“å›¾ã€‚é€šè¿‡åœ¨æ½œåœ¨ç©ºé—´ä¸­éšå¼å¼ºåˆ¶æ‰§è¡Œç»“æ„ä¸€è‡´æ€§ï¼ŒISLockèƒ½å¤Ÿåœ¨ä¿æŒç”Ÿæˆè‡ªä¸»æƒçš„åŒæ—¶ï¼Œå®ç°ç»“æ„æ„ŸçŸ¥ç¼–è¾‘ã€‚å®éªŒè¯æ˜ï¼ŒISLockæ— éœ€é¢å¤–è®­ç»ƒå³å¯å®ç°é«˜è´¨é‡ã€ç»“æ„ä¸€è‡´æ€§çš„ç¼–è¾‘ï¼Œä¸”ä¼˜äºæˆ–ç›¸å½“äºä¼ ç»Ÿç¼–è¾‘æŠ€æœ¯ã€‚è¿™å¼€è¾Ÿäº†ARæ¨¡å‹é«˜æ•ˆçµæ´»å›¾åƒç¼–è¾‘çš„å…ˆæ²³ï¼Œè¿›ä¸€æ­¥ç¼©å°äº†æ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’ç”Ÿæˆæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚ç›¸å…³ä»£ç å°†å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/hutaiHang/ATM%E3%80%82">https://github.com/hutaiHang/ATMã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆä¸­å®ç°äº†é«˜ä¿çœŸåˆæˆå’Œç²¾ç¡®ç¼–è¾‘ã€‚</li>
<li>è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹ä½œä¸ºå¼ºå¤§çš„æ›¿ä»£æ–¹æ³•é‡æ–°å‡ºç°ï¼Œåˆ©ç”¨ä¸‹ä¸€ä»£æ ‡è®°åŒ¹é…æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>ARæ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä¸­é¢ä¸´ç©ºé—´è´«å›°å’Œç»“æ„æ€§é”™è¯¯çš„é—®é¢˜ã€‚</li>
<li>éšå¼ç»“æ„é”å®šï¼ˆISLockï¼‰æ˜¯æ— è®­ç»ƒç¼–è¾‘ç­–ç•¥ï¼Œé€‚ç”¨äºARè§†è§‰æ¨¡å‹ã€‚</li>
<li>ISLocké€šè¿‡åŠ¨æ€å¯¹é½å‚è€ƒå›¾åƒçš„è‡ªæˆ‘æ³¨æ„åŠ›æ¨¡å¼æ¥ä¿ç•™ç»“æ„è“å›¾ã€‚</li>
<li>ISLockå®ç°äº†ç»“æ„æ„ŸçŸ¥ç¼–è¾‘ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10434">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2504.10434v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2504.10434v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2504.10434v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_Diffusion Models/2504.10434v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-13/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-13/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-14/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-08-14\./crop_Talking Head Generation/2508.08891v1/page_3_0.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-14  Preview WB-DH Towards Whole Body Digital Human Bench for the Generation   of Whole-body Talking Avatar Videos
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-13/NeRF/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-08-13\./crop_NeRF/2501.12637v3/page_3_0.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-13  SAGOnline Segment Any Gaussians Online
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25691.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
