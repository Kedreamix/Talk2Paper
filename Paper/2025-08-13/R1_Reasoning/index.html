<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-08-13  Capabilities of GPT-5 on Multimodal Medical Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-2a026615b274442b18b2e0d5a4cb1e26.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    21.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    88 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-13-更新"><a href="#2025-08-13-更新" class="headerlink" title="2025-08-13 更新"></a>2025-08-13 更新</h1><h2 id="Capabilities-of-GPT-5-on-Multimodal-Medical-Reasoning"><a href="#Capabilities-of-GPT-5-on-Multimodal-Medical-Reasoning" class="headerlink" title="Capabilities of GPT-5 on Multimodal Medical Reasoning"></a>Capabilities of GPT-5 on Multimodal Medical Reasoning</h2><p><strong>Authors:Shansong Wang, Mingzhe Hu, Qiang Li, Mojtaba Safari, Xiaofeng Yang</strong></p>
<p>Recent advances in large language models (LLMs) have enabled general-purpose systems to perform increasingly complex domain-specific reasoning without extensive fine-tuning. In the medical domain, decision-making often requires integrating heterogeneous information sources, including patient narratives, structured data, and medical images. This study positions GPT-5 as a generalist multimodal reasoner for medical decision support and systematically evaluates its zero-shot chain-of-thought reasoning performance on both text-based question answering and visual question answering tasks under a unified protocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20 against standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU medical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that GPT-5 consistently outperforms all baselines, achieving state-of-the-art accuracy across all QA benchmarks and delivering substantial gains in multimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and understanding scores by +29.62% and +36.18% over GPT-4o, respectively, and surpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in understanding. In contrast, GPT-4o remains below human expert performance in most dimensions. A representative case study demonstrates GPT-5’s ability to integrate visual and textual cues into a coherent diagnostic reasoning chain, recommending appropriate high-stakes interventions. Our results show that, on these controlled multimodal reasoning benchmarks, GPT-5 moves from human-comparable to above human-expert performance. This improvement may substantially inform the design of future clinical decision-support systems. </p>
<blockquote>
<p>近期大型语言模型（LLM）的进展使得通用系统能够在不需要广泛微调的情况下执行越来越复杂的特定领域推理。在医疗领域，决策制定通常需要整合异质的信息来源，包括患者叙述、结构化数据和医疗图像。本研究将GPT-5定位为通用的多模式推理机，用于医疗决策支持，并在统一协议下，系统评估其在基于文本的问题回答和视觉问题回答任务上的零射击链式思维推理性能。我们以MedQA、MedXpertQA（文本和多模态）、MMLU医疗子集、USMLE自我评估考试和VQA-RAD的标准分割数据为基准，对GPT-5、GPT-5-mini、GPT-5-nano和GPT-4o-2024-11-20进行了评估。结果表明，GPT-5持续优于所有基线，在所有的问答基准测试中达到最先进的准确性，并在多模态推理中实现了实质性的收益。在MedXpertQA MM上，GPT-5在推理和理解方面的得分分别比GPT-4o高出+29.62%和+36.18%，并且在推理和理解方面超越预先授权的专家+24.23%和+29.40%。相比之下，GPT-4o在大多数维度上仍低于人类专家的表现。一个具有代表性的案例研究展示了GPT-5将视觉和文本线索整合到连贯的诊断推理链中的能力，并推荐了适当的高风险干预措施。我们的结果表明，在这些受控的多模态推理基准测试中，GPT-5的表现从与人类相当提升到了超越人类专家的水平。这一改进可能会为未来的临床决策支持系统提供实质性的信息参考。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08224v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的最新进展使得通用系统能够在不需要广泛微调的情况下执行越来越复杂的领域特定推理。本研究将GPT-5定位为医疗决策支持的一般性多模式推理器，并系统地评估其在统一协议下的零射击链式思维推理性能。在基于文本和视觉的问题回答任务中，GPT-5在各种标准化分割的MedQA、MedXpertQA（文本和多模式）、MMLU医疗子集、USMLE自我评估考试和VQA-RAD上表现出色。结果证明GPT-5在所有问答基准测试中始终优于所有基线，实现了最先进的准确性，并在多模式推理方面取得了重大进展。在MedXpertQA MM上，GPT-5在推理和理解方面的得分分别比GPT-4o高出+29.62%和+36.18%，并且在推理和理解方面超越预先授权的人类专家分别高出+24.23%和+29.40%。相比之下，GPT-4o在大多数维度上仍低于人类专家的表现。一个典型的案例研究表明，GPT-5能够整合视觉和文本线索，形成连贯的诊断推理链，推荐适当的高风险干预措施。我们的结果表明，在这些受控的多模式推理基准测试中，GPT-5的表现已从人类相当的水平提升到了超越人类专家的水平。这一进步可能为未来的临床决策支持系统提供重要的设计参考。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）可在无需广泛微调的情况下进行复杂的领域特定推理。</li>
<li>GPT-5在医疗决策支持中表现出强大的多模式推理能力。</li>
<li>GPT-5在各种标准化医疗问答基准测试中表现优异，超越其他基准和先前的研究。</li>
<li>GPT-5在推理和理解方面的得分显著优于预先授权的人类专家。</li>
<li>GPT-5能整合视觉和文本线索，形成连贯的诊断推理链。</li>
<li>GPT-5的表现已从人类相当的水平提升到了超越人类专家的水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08224">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0ce9395c05b976a91aebde4e3bec7c85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bb998a42975d483a08aa26d2dc35a92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a73b4ff65d0519f7b792e2d55a2305e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-580156970b0d96670ede817ccd990adf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce5ac46480a182d7a80c946aa269693a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-160ae7e01499b34e236ee50f7948eac8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5912ddd6627ade3fc53834262caea54.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-in-Vision-A-Survey"><a href="#Reinforcement-Learning-in-Vision-A-Survey" class="headerlink" title="Reinforcement Learning in Vision: A Survey"></a>Reinforcement Learning in Vision: A Survey</h2><p><strong>Authors:Weijia Wu, Chen Gao, Joya Chen, Kevin Qinghong Lin, Qingwei Meng, Yiming Zhang, Yuke Qiu, Hong Zhou, Mike Zheng Shou</strong></p>
<p>Recent advances at the intersection of reinforcement learning (RL) and visual intelligence have enabled agents that not only perceive complex visual scenes but also reason, generate, and act within them. This survey offers a critical and up-to-date synthesis of the field. We first formalize visual RL problems and trace the evolution of policy-optimization strategies from RLHF to verifiable reward paradigms, and from Proximal Policy Optimization to Group Relative Policy Optimization. We then organize more than 200 representative works into four thematic pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models. For each pillar we examine algorithmic design, reward engineering, benchmark progress, and we distill trends such as curriculum-driven training, preference-aligned diffusion, and unified reward modeling. Finally, we review evaluation protocols spanning set-level fidelity, sample-level preference, and state-level stability, and we identify open challenges that include sample efficiency, generalization, and safe deployment. Our goal is to provide researchers and practitioners with a coherent map of the rapidly expanding landscape of visual RL and to highlight promising directions for future inquiry. Resources are available at: <a target="_blank" rel="noopener" href="https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning">https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning</a>. </p>
<blockquote>
<p>最近，强化学习（RL）和视觉智能交叉领域的进展使得智能体不仅能够感知复杂的视觉场景，还能在这些场景中进行推理、生成和行动。这篇综述对该领域进行了批判性和最新的综合。我们首先正式提出视觉RL问题，并追踪从RLHF到可验证奖励范式，从近端策略优化到群组相对策略优化的策略优化策略的演变。然后，我们将超过200篇具有代表性的作品整理为四个主题支柱：多模态大型语言模型、视觉生成、统一模型框架和视觉语言行动模型。对于每个主题支柱，我们研究了算法设计、奖励工程、基准进展，并总结了趋势，如课程驱动训练、偏好对齐扩散和统一奖励建模。最后，我们回顾了包括集合级保真度、样本级偏好和状态级稳定性的评估协议，并确定了开放挑战，包括样本效率、概括和安全部署。我们的目标是为研究人员和实践者提供快速扩展的视觉RL景观的连贯地图，并突出未来探究的有希望的方向。资源可在：<a target="_blank" rel="noopener" href="https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08189v1">PDF</a> 22 pages</p>
<p><strong>Summary</strong><br>强化学习与视觉智能的交叉融合为智能体提供了更高的能力，它们不仅能感知复杂的视觉场景，还能进行推理、生成和行动。这篇综述对该领域进行了最新、最重要的总结，从策略优化到四大主题支柱（多模态大型语言模型、视觉生成、统一模型框架和视觉语言行动模型），并对课程驱动训练等趋势进行了分析。文章的目的是为研究者和实践者提供视觉强化学习的最新研究地图，并指明未来研究的有前途方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习与视觉智能的融合让智能体具备更复杂的能力，如感知、推理、生成和行动。</li>
<li>综述文章系统总结了视觉强化学习领域的最新进展和重要成果。</li>
<li>文章详细描述了从策略优化到四大主题支柱（多模态大型语言模型等）的研究发展。</li>
<li>趋势分析包括课程驱动训练、偏好对齐扩散和统一奖励建模等。</li>
<li>综述涵盖了丰富的文献和前沿研究成果的组织和资源链接。</li>
<li>文章指出了开放挑战，如样本效率、泛化和安全部署等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08189">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4d66785c9fa360f6dbe459f20c592480.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a516597ac992add14c481d36867c359c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afd40acd9948d5a52371a5e494c50ca8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95f89a4e2c0b25c1fade35b3973523da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e6699aee2862070236411b972d3c41f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MedReasoner-Reinforcement-Learning-Drives-Reasoning-Grounding-from-Clinical-Thought-to-Pixel-Level-Precision"><a href="#MedReasoner-Reinforcement-Learning-Drives-Reasoning-Grounding-from-Clinical-Thought-to-Pixel-Level-Precision" class="headerlink" title="MedReasoner: Reinforcement Learning Drives Reasoning Grounding from   Clinical Thought to Pixel-Level Precision"></a>MedReasoner: Reinforcement Learning Drives Reasoning Grounding from   Clinical Thought to Pixel-Level Precision</h2><p><strong>Authors:Zhonghao Yan, Muxi Diao, Yuxuan Yang, Jiayuan Xu, Kaizhou Zhang, Ruoyan Jing, Lele Yang, Yanxi Liu, Kongming Liang, Zhanyu Ma</strong></p>
<p>Accurately grounding regions of interest (ROIs) is critical for diagnosis and treatment planning in medical imaging. While multimodal large language models (MLLMs) combine visual perception with natural language, current medical-grounding pipelines still rely on supervised fine-tuning with explicit spatial hints, making them ill-equipped to handle the implicit queries common in clinical practice. This work makes three core contributions. We first define Unified Medical Reasoning Grounding (UMRG), a novel vision-language task that demands clinical reasoning and pixel-level grounding. Second, we release U-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside implicit clinical queries and reasoning traces, spanning 10 modalities, 15 super-categories, and 108 specific categories. Finally, we introduce MedReasoner, a modular framework that distinctly separates reasoning from segmentation: an MLLM reasoner is optimized with reinforcement learning, while a frozen segmentation expert converts spatial prompts into masks, with alignment achieved through format and accuracy rewards. MedReasoner achieves state-of-the-art performance on U-MRG-14K and demonstrates strong generalization to unseen clinical queries, underscoring the significant promise of reinforcement learning for interpretable medical grounding. </p>
<blockquote>
<p>在医学成像中，准确定位感兴趣区域（ROI）对于诊断和治疗计划的制定至关重要。虽然多模态大型语言模型（MLLMs）结合了视觉感知与自然语言，但当前的医学定位流程仍然依赖于带有明确空间提示的监督微调，这使得它们难以应对临床实践中的隐式查询。本工作做出了三个核心贡献。首先，我们定义了统一医学推理定位（UMRG），这是一种新的视觉语言任务，需要临床推理和像素级定位。其次，我们发布了U-MRG-14K数据集，包含14K个样本，每个样本都有像素级掩膜、隐式临床查询和推理轨迹，涵盖10种模态、15个超类别和108个特定类别。最后，我们推出了MedReasoner，这是一个模块化框架，将推理与分割明确区分开来：MLLM推理器通过强化学习进行优化，而冻结的分割专家将空间提示转换为掩膜，通过格式和精度奖励实现对齐。MedReasoner在U-MRG-14K上达到了最先进的性能，并对未见过的临床查询表现出了强大的泛化能力，这突显了强化学习在可解释的医学定位中的巨大潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08177v1">PDF</a> 37 pages</p>
<p><strong>Summary</strong><br>医学成像中的准确区域定位（ROIs）对于诊断和治疗计划至关重要。当前，医疗定位流程仍然依赖于带有明确空间提示的监督微调，无法处理临床实践中的隐含查询。本文定义了统一医学推理定位（UMRG）这一新颖的视觉语言任务，并发布U-MRG-14K数据集，包含跨越十种模态、十五个超级类别和一百零八种特定类别的隐性临床查询和推理痕迹的一万四千个样本。此外，还推出了MedReasoner模块化框架，将推理与分割分离，优化了强化学习下的MLLM推理器，将空间提示转换为掩膜。MedReasoner在U-MRG-14K上取得了最佳性能，并在未见过的临床查询上展示了强大的泛化能力。通过展现其在强化学习上的巨大潜力证明了技术重要性。 </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>区域定位（ROIs）在医学成像中的诊断与治疗的角色是核心的。</li>
<li>当前医疗定位流程依赖监督微调及明确的空间提示，无法适应临床实践中的隐含查询需求。</li>
<li>统一医学推理定位（UMRG）定义了一种新的视觉语言任务，要求临床推理和像素级定位。</li>
<li>U-MRG-14K数据集包含大量样本，涵盖多种模态、类别和临床查询及推理痕迹。</li>
<li>MedReasoner模块化框架实现了推理与分割的分离，并采用强化学习优化MLLM推理器性能。</li>
<li>MedReasoner在U-MRG-14K数据集上表现卓越，并在未见过的临床查询上展现了强大的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08177">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1f06984455fc0a851fbffcb0d6207f70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a378eeee81f6ec3a4240577c60bd85dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27209ab38731dc4fccd7894dcb778d1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f1f90c911d27ca95264e0ad8d6239bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee783866ffd89bcf426bcca3b2ed5aec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11a3bb59d06b2e899f246a04a58dad60.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="REX-RAG-Reasoning-Exploration-with-Policy-Correction-in-Retrieval-Augmented-Generation"><a href="#REX-RAG-Reasoning-Exploration-with-Policy-Correction-in-Retrieval-Augmented-Generation" class="headerlink" title="REX-RAG: Reasoning Exploration with Policy Correction in   Retrieval-Augmented Generation"></a>REX-RAG: Reasoning Exploration with Policy Correction in   Retrieval-Augmented Generation</h2><p><strong>Authors:Wentao Jiang, Xiang Feng, Zengmao Wang, Yong Luo, Pingbo Xu, Zhe Chen, Bo Du, Jing Zhang</strong></p>
<p>Reinforcement learning (RL) is emerging as a powerful paradigm for enabling large language models (LLMs) to perform complex reasoning tasks. Recent advances indicate that integrating RL with retrieval-augmented generation (RAG) allows LLMs to dynamically incorporate external knowledge, leading to more informed and robust decision making. However, we identify a critical challenge during policy-driven trajectory sampling: LLMs are frequently trapped in unproductive reasoning paths, which we refer to as “dead ends”, committing to overconfident yet incorrect conclusions. This severely hampers exploration and undermines effective policy optimization. To address this challenge, we propose REX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation), a novel framework that explores alternative reasoning paths while maintaining rigorous policy learning through principled distributional corrections. Our approach introduces two key innovations: (1) Mixed Sampling Strategy, which combines a novel probe sampling method with exploratory prompts to escape dead ends; and (2) Policy Correction Mechanism, which employs importance sampling to correct distribution shifts induced by mixed sampling, thereby mitigating gradient estimation bias. We evaluate it on seven question-answering benchmarks, and the experimental results show that REX-RAG achieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B over strong baselines, demonstrating competitive results across multiple datasets. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/MiliLab/REX-RAG">https://github.com/MiliLab/REX-RAG</a>. </p>
<blockquote>
<p>强化学习（RL）正成为一种强大的范式，使大型语言模型（LLM）能够执行复杂的推理任务。最近的进展表明，将RL与检索增强生成（RAG）相结合，可以使LLM动态地融入外部知识，从而实现更加明智和稳健的决策。然而，我们在策略驱动的轨迹采样过程中发现了一个关键挑战：LLM经常陷入无结果的推理路径，我们称之为“死胡同”，导致过于自信的错误结论。这严重阻碍了探索并破坏了有效的策略优化。为了解决这一挑战，我们提出了REX-RAG（检索增强生成中的带策略修正的推理探索），这是一种新的框架，在保持严谨的策略学习的情况下，探索替代的推理路径，并通过有原则的分布修正来进行策略修正。我们的方法引入了两个关键的创新点：（1）混合采样策略，它将一种新的探针采样方法与探索性提示相结合，以逃离死胡同；（2）策略修正机制，它采用重要性采样来纠正混合采样引起的分布偏移，从而减轻梯度估计偏差。我们在七个问答基准测试上对其实验评估，实验结果表明，在Qwen2.5-3B上REX-RAG平均性能提升5.1%，在Qwen2.5-7B上性能提升3.6%，相较于强大的基线模型表现出竞争力。相关代码已公开在<a target="_blank" rel="noopener" href="https://github.com/MiliLab/REX-RAG">https://github.com/MiliLab/REX-RAG</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08149v1">PDF</a> 17 pages, 4 figures</p>
<p><strong>Summary</strong>：强化学习正在成为使大型语言模型执行复杂推理任务的一种强大范式。将强化学习与检索增强生成相结合，可以动态地使语言模型融入外部知识，从而实现更加明智和稳健的决策。然而，策略驱动的轨迹采样面临一个挑战：语言模型经常陷入不产生效益的推理路径，即“死胡同”，导致过度自信的错误结论。为解决此问题，提出了REX-RAG框架，通过混合采样策略和策略校正机制，在检索增强生成中进行推理探索，同时保持严谨的策略学习。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>强化学习正在成为大型语言模型执行复杂推理任务的重要工具。</li>
<li>整合强化学习与检索增强生成，能让语言模型动态融入外部知识，提高决策质量。</li>
<li>策略驱动的轨迹采样在强化学习中存在挑战，语言模型易陷入不产生效益的推理路径。</li>
<li>REX-RAG框架通过混合采样策略和策略校正机制解决此问题，实现推理探索与严谨的策略学习。</li>
<li>REX-RAG的混合采样策略结合了新型探针采样方法和探索性提示，有助于逃离“死胡同”。</li>
<li>REX-RAG的策略校正机制采用重要性采样，纠正由混合采样引起的分布偏移，减轻梯度估计偏差。</li>
<li>在七个问答基准测试上的评估显示，REX-RAG相较于强大的基准模型取得了平均性能提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08149">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cfa98f649e8488f071ed73149da553de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-925c386281592c204daba0d2804ce46d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-549fc36f7a7c6ac274135a62634286c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7f705e40df0d0e2f2145460fd805f54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eda33d128ca0a01c8ffa82a0b9b7edec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-362fcfbb60b1eb85011d6e0922cda71c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="HierSearch-A-Hierarchical-Enterprise-Deep-Search-Framework-Integrating-Local-and-Web-Searches"><a href="#HierSearch-A-Hierarchical-Enterprise-Deep-Search-Framework-Integrating-Local-and-Web-Searches" class="headerlink" title="HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating   Local and Web Searches"></a>HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating   Local and Web Searches</h2><p><strong>Authors:Jiejun Tan, Zhicheng Dou, Yan Yu, Jiehan Cheng, Qiang Ju, Jian Xie, Ji-Rong Wen</strong></p>
<p>Recently, large reasoning models have demonstrated strong mathematical and coding abilities, and deep search leverages their reasoning capabilities in challenging information retrieval tasks. Existing deep search works are generally limited to a single knowledge source, either local or the Web. However, enterprises often require private deep search systems that can leverage search tools over both local and the Web corpus. Simply training an agent equipped with multiple search tools using flat reinforcement learning (RL) is a straightforward idea, but it has problems such as low training data efficiency and poor mastery of complex tools. To address the above issue, we propose a hierarchical agentic deep search framework, HierSearch, trained with hierarchical RL. At the low level, a local deep search agent and a Web deep search agent are trained to retrieve evidence from their corresponding domains. At the high level, a planner agent coordinates low-level agents and provides the final answer. Moreover, to prevent direct answer copying and error propagation, we design a knowledge refiner that filters out hallucinations and irrelevant evidence returned by low-level agents. Experiments show that HierSearch achieves better performance compared to flat RL, and outperforms various deep search and multi-source retrieval-augmented generation baselines in six benchmarks across general, finance, and medical domains. </p>
<blockquote>
<p>最近，大型推理模型展示了强大的数学和编码能力，深度搜索则在具有挑战性的信息检索任务中利用了这些推理能力。现有的深度搜索工作通常局限于单个知识源，无论是本地还是网络。然而，企业通常需要能够在本地和网络语料库上使用搜索工具的私人深度搜索系统。使用平面强化学习（RL）训练配备多种搜索工具的智能代理是一个直接的想法，但它存在训练数据效率低和难以掌握复杂工具等问题。为了解决上述问题，我们提出了使用分层强化学习训练的分层代理深度搜索框架HierSearch。在低级层面，本地深度搜索代理和网络深度搜索代理被训练从各自对应的领域检索证据。在高级层面，规划代理协调低级代理并提供最终答案。此外，为了防止直接答案复制和错误传播，我们设计了一个知识精炼器，可以过滤掉低级代理产生的幻觉和无关证据。实验表明，HierSearch在性能上优于平面RL，并且在通用、金融和医疗领域的六个基准测试中超越了各种深度搜索和多源检索增强生成基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08088v1">PDF</a> Code and datasets are available at   <a target="_blank" rel="noopener" href="https://github.com/plageon/HierSearch">https://github.com/plageon/HierSearch</a></p>
<p><strong>Summary</strong></p>
<p>大型推理模型展现出强大的数学与编码能力，并在信息检索任务中利用这些能力进行深度搜索。然而，现有深度搜索工作主要局限于单一知识源，难以同时运用本地及网络数据进行搜索。为解决企业需求中的私人深度搜索系统问题，研究者提出使用层次化的智能深度搜索框架——HierSearch，通过层次化强化学习进行训练。该框架在底层训练本地和网络深度搜索代理，以从相应领域检索证据；在高层设置规划代理进行协调并给出最终答案。此外，还设计了一个知识精炼器过滤掉底层代理产生的幻觉和无关证据。实验表明，HierSearch相较于平面强化学习有更好的性能表现，并在通用、金融和医学领域的六个基准测试中超越了各种深度搜索和多源检索增强生成基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型推理模型具备数学与编码能力，并在信息检索中展现深度搜索能力。</li>
<li>现有深度搜索主要依赖单一知识源，无法满足企业对本地和网络数据同时运用的需求。</li>
<li>HierSearch框架使用层次化强化学习训练代理，实现本地和网络深度搜索。</li>
<li>HierSearch在底层训练代理检索证据，高层规划代理进行协调并提供答案。</li>
<li>知识精炼器用于过滤底层代理产生的幻觉和无关证据。</li>
<li>HierSearch相较于平面强化学习性能更优。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08088">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-dd5a735d80ff24cfd98b7b5e8a5e74b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7d637486c148c9aecf5e7d2ea9850dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de51fd03b24b2515ba8e45cb265d12bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42a14850a1edea3b2345afd65a825ce5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-877785939347d6ae2cebd382974d74e1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AdaptFlow-Adaptive-Workflow-Optimization-via-Meta-Learning"><a href="#AdaptFlow-Adaptive-Workflow-Optimization-via-Meta-Learning" class="headerlink" title="AdaptFlow: Adaptive Workflow Optimization via Meta-Learning"></a>AdaptFlow: Adaptive Workflow Optimization via Meta-Learning</h2><p><strong>Authors:Runchuan Zhu, Bowen Jiang, Lingrui Mei, Fangkai Yang, Lu Wang, Haoxiang Gao, Fengshuo Bai, Pu Zhao, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang</strong></p>
<p>Recent advances in large language models (LLMs) have sparked growing interest in agentic workflows, which are structured sequences of LLM invocations intended to solve complex tasks. However, existing approaches often rely on static templates or manually designed workflows, which limit adaptability to diverse tasks and hinder scalability. We propose AdaptFlow, a natural language-based meta-learning framework inspired by model-agnostic meta-learning (MAML). AdaptFlow learns a generalizable workflow initialization that enables rapid subtask-level adaptation. It employs a bi-level optimization scheme: the inner loop refines the workflow for a specific subtask using LLM-generated feedback, while the outer loop updates the shared initialization to perform well across tasks. This setup allows AdaptFlow to generalize effectively to unseen tasks by adapting the initialized workflow through language-guided modifications. Evaluated across question answering, code generation, and mathematical reasoning benchmarks, AdaptFlow consistently outperforms both manually crafted and automatically searched baselines, achieving state-of-the-art results with strong generalization across tasks and models. The source code and data are available at <a target="_blank" rel="noopener" href="https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow">https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow</a>. </p>
<blockquote>
<p>近期大型语言模型（LLM）的进展引发了人们对代理工作流的日益关注，代理工作流是由一系列LLM调用组成的结构化序列，旨在解决复杂任务。然而，现有方法通常依赖于静态模板或手动设计的工作流，这限制了它们对不同任务的适应性，并阻碍了可扩展性。我们提出了AdaptFlow，这是一个受模型无关元学习（MAML）启发的基于自然语言元学习框架。AdaptFlow学习一种可通用的工作流初始化方法，以加快特定子任务的适应性。它采用了一种两级优化方案：内循环利用LLM生成的反馈对特定子任务的工作流程进行精细化改进，而外循环则更新共享初始化设置以在任务之间表现良好。这种设置允许AdaptFlow通过语言指导的修改来适应初始化工作流，从而有效地推广到未见过的任务。在问答、代码生成和数学推理基准测试上进行了评估，AdaptFlow始终表现出色，不仅在手动设计和自动搜索的基准线上有所超越，而且在任务和模型之间实现了强大的泛化能力。源代码和数据在<a target="_blank" rel="noopener" href="https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow%E4%B8%8A%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow上可用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08053v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的最近进展引发了人们对代理工作流（agentic workflows）的浓厚兴趣。工作流是一系列LLM调用的结构化序列，旨在解决复杂任务。然而，现有方法往往依赖于静态模板或手动设计的工作流，这限制了其对各种任务的适应能力并阻碍其可扩展性。为此，我们提出了AdaptFlow，一个受模型不可知元学习（MAML）启发的自然语言驱动的元学习框架。AdaptFlow学习一种可通用的工作流初始化方案，以实现快速子任务级别的适应。它采用双层优化方案：内循环针对特定子任务优化工作流并使用LLM生成的反馈进行调整，而外循环更新共享初始化方案以实现跨任务的良好表现。这使得AdaptFlow能够有效地适应未见过的任务，并通过语言指导的修改来调整初始化工作流。在问答、代码生成和数学推理基准测试上进行的评估表明，AdaptFlow在手动设计和自动搜索的基准测试中都表现优异，实现了跨任务和模型的最佳结果。其源代码和数据在 <a target="_blank" rel="noopener" href="https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow">https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow</a> 上可供查阅。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的进展推动了代理工作流（agentic workflows）的研究。</li>
<li>现有工作流方法主要依赖静态模板或手动设计，限制了其在多样任务中的适应性和可扩展性。</li>
<li>AdaptFlow是一个自然语言驱动的元学习框架，旨在解决这一限制。</li>
<li>AdaptFlow通过双层优化方案学习通用的工作流初始化，以实现快速子任务级别的适应。</li>
<li>该框架结合了内循环的特定子任务优化和外循环的跨任务表现优化。</li>
<li>AdaptFlow能够适应未见过的任务，并通过语言指导修改初始化工作流。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08053">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fdc1617189d86e78a3caaf818953b5ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13007b66af4cf1674b8462743c81a552.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e30785803f66dc7fbd257a02e8b586f7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5070a0040e1cac5a49bb2ec05de162a9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Audio-Thinker-Guiding-Audio-Language-Model-When-and-How-to-Think-via-Reinforcement-Learning"><a href="#Audio-Thinker-Guiding-Audio-Language-Model-When-and-How-to-Think-via-Reinforcement-Learning" class="headerlink" title="Audio-Thinker: Guiding Audio Language Model When and How to Think via   Reinforcement Learning"></a>Audio-Thinker: Guiding Audio Language Model When and How to Think via   Reinforcement Learning</h2><p><strong>Authors:Shu Wu, Chenxing Li, Wenfu Wang, Hao Zhang, Hualei Wang, Meng Yu, Dong Yu</strong></p>
<p>Recent advancements in large language models, multimodal large language models, and large audio language models (LALMs) have significantly improved their reasoning capabilities through reinforcement learning with rule-based rewards. However, the explicit reasoning process has yet to show significant benefits for audio question answering, and effectively leveraging deep reasoning remains an open challenge, with LALMs still falling short of human-level auditory-language reasoning. To address these limitations, we propose Audio-Thinker, a reinforcement learning framework designed to enhance the reasoning capabilities of LALMs, with a focus on improving adaptability, consistency, and effectiveness. Our approach introduces an adaptive think accuracy reward, enabling the model to adjust its reasoning strategies based on task complexity dynamically. Furthermore, we incorporate an external reward model to evaluate the overall consistency and quality of the reasoning process, complemented by think-based rewards that help the model distinguish between valid and flawed reasoning paths during training. Experimental results demonstrate that our Audio-Thinker model outperforms existing reasoning-oriented LALMs across various benchmark tasks, exhibiting superior reasoning and generalization capabilities. </p>
<blockquote>
<p>近期，大型语言模型、多模态大型语言模型和大型音频语言模型（LALM）的进步，通过基于规则的奖励进行强化学习，显著提升了其推理能力。然而，明确的推理过程在音频问答中尚未显示出显著的优势，有效利用深度推理仍然是一个开放性的挑战，LALM在音频语言推理方面仍未能达到人类水平。为了解决这些限制，我们提出了Audio-Thinker，这是一个旨在增强LALM推理能力的强化学习框架，重点关注适应性、一致性和有效性。我们的方法引入了一种自适应的思考准确性奖励，使模型能够基于任务的复杂性动态地调整其推理策略。此外，我们采用外部奖励模型来评估推理过程的整体一致性和质量，辅以基于思考的奖励，帮助模型在训练过程中区分有效的和错误的推理路径。实验结果表明，我们的Audio-Thinker模型在各种基准任务上优于现有的面向推理的LALM，表现出卓越的推理和泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08039v1">PDF</a> preprint</p>
<p><strong>Summary</strong>：<br>随着大型语言模型、多模态大型语言模型和大型音频语言模型（LALM）的近期进展，通过基于规则的奖励进行强化学习，它们的推理能力已显著提高。然而，在音频问答方面，明确的推理过程尚未显示出显著优势，有效利用深度推理仍是开放挑战，LALM仍未能达到人类水平的听觉语言推理。为解决这个问题，我们提出了Audio-Thinker，一个旨在提高LALM推理能力的强化学习框架，重点改善适应性、一致性和有效性。通过引入自适应思考精度奖励，使模型能根据任务复杂度动态调整推理策略。此外，我们结合外部奖励模型来评估推理过程的一致性和质量，辅以基于思考的奖励，帮助模型在训练时区分有效的和错误的推理路径。实验结果表明，我们的Audio-Thinker模型在各项基准任务上优于现有的推理导向型LALM，展现出卓越的推理和泛化能力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型（包括音频语言模型）的推理能力通过强化学习得到显著提高。</li>
<li>在音频问答方面，明确推理过程尚未显现显著优势，且有效利用深度推理仍是挑战。</li>
<li>引入Audio-Thinker框架，旨在提高LALM的推理能力，特别是在适应性、一致性和有效性方面。</li>
<li>Audio-Thinker采用自适应思考精度奖励，使模型能根据任务复杂度动态调整推理策略。</li>
<li>结合外部奖励模型评估推理过程的一致性和质量。</li>
<li>基于思考的奖励帮助模型区分有效和错误的推理路径。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08039">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-174b475aeeeea115c9bd3b93b4c6125d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac49b30637545901965c2041fff1d0ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80681e9b6da53937bbadf3abefc1bafa.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Prompt-Guided-Relational-Reasoning-for-Social-Behavior-Understanding-with-Vision-Foundation-Models"><a href="#Prompt-Guided-Relational-Reasoning-for-Social-Behavior-Understanding-with-Vision-Foundation-Models" class="headerlink" title="Prompt-Guided Relational Reasoning for Social Behavior Understanding   with Vision Foundation Models"></a>Prompt-Guided Relational Reasoning for Social Behavior Understanding   with Vision Foundation Models</h2><p><strong>Authors:Thinesh Thiyakesan Ponbagavathi, Chengzheng Yang, Alina Roitberg</strong></p>
<p>Group Activity Detection (GAD) involves recognizing social groups and their collective behaviors in videos. Vision Foundation Models (VFMs), like DinoV2, offer excellent features, but are pretrained primarily on object-centric data and remain underexplored for modeling group dynamics. While they are a promising alternative to highly task-specific GAD architectures that require full fine-tuning, our initial investigation reveals that simply swapping CNN backbones used in these methods with VFMs brings little gain, underscoring the need for structured, group-aware reasoning on top.   We introduce Prompt-driven Group Activity Detection (ProGraD) – a method that bridges this gap through 1) learnable group prompts to guide the VFM attention toward social configurations, and 2) a lightweight two-layer GroupContext Transformer that infers actor-group associations and collective behavior. We evaluate our approach on two recent GAD benchmarks: Cafe, which features multiple concurrent social groups, and Social-CAD, which focuses on single-group interactions. While we surpass state-of-the-art in both settings, our method is especially effective in complex multi-group scenarios, where we yield a gain of 6.5% (Group mAP@1.0) and 8.2% (Group mAP@0.5) using only 10M trainable parameters. Furthermore, our experiments reveal that ProGraD produces interpretable attention maps, offering insights into actor-group reasoning. Code and models will be released. </p>
<blockquote>
<p>群体活动检测（GAD）涉及识别视频中的社会群体及其集体行为。视觉基础模型（VFMs），如DinoV2，具有出色的功能，但主要是基于以对象为中心的数据进行预训练，并且在模拟群体动态方面仍然被较少研究。尽管它们是有前途的替代高度针对任务的GAD架构，这些架构需要全面微调，但我们的初步调查表明，仅仅用VFM替换这些方法中的CNN骨干所带来的收益甚微，这强调了需要在顶端进行结构化、群体感知推理的必要性。我们引入了Prompt驱动群体活动检测（ProGraD）——一种弥合这一鸿沟的方法，包括1）可学习的群体提示，引导VFM注意力关注社会配置，以及2）轻量级的两层组上下文转换器，推断演员-组关联和集体行为。我们在两个最新的GAD基准测试集上评估了我们的方法：Cafe，以多个并发社会群体为特色；以及Social-CAD，专注于单一群体互动。尽管我们在两种设置中都超越了最新技术状态，但我们的方法在复杂的多群体场景中尤其有效，仅使用10M个可训练参数就实现了群平均准确率提高6.5%（Group <a href="mailto:&#x6d;&#65;&#80;&#64;&#x31;&#46;&#48;">&#x6d;&#65;&#80;&#64;&#x31;&#46;&#48;</a>）和8.2%（Group <a href="mailto:&#x6d;&#65;&#x50;&#64;&#48;&#46;&#53;">&#x6d;&#65;&#x50;&#64;&#48;&#46;&#53;</a>）。此外，我们的实验表明，ProGraD产生可解释的关注图，为演员-组推理提供了深入见解。代码和模型将发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07996v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>群体活动检测（GAD）旨在识别视频中的社交群体及其集体行为。虽然DinoV2等视觉基础模型（VFMs）具有出色的特征，但它们主要基于对象中心数据进行预训练，对于群体动态建模仍显不足。研究引入了一种名为Prompt驱动群体活动检测（ProGraD）的方法，通过可学习的群体提示和引导VFM注意力关注社交配置，以及一个轻量级的两层GroupContext Transformer来推断演员群体关联和集体行为。该方法在最近的GAD基准测试中表现优异，尤其在复杂的多元群组场景中优势明显，使用仅10M可训练参数便实现了显著的增益。此外，ProGraD产生的注意力地图具有可解释性，为演员群体推理提供了见解。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>Group Activity Detection (GAD) 旨在识别视频中的社交群体及其行为。</li>
<li>视觉基础模型（VFMs）如DinoV2在GAD方面的应用尚待探索。</li>
<li>简单的将CNN骨干网络与VFMs替换对提升GAD效果有限，需要结构化的群体感知推理。</li>
<li>ProGraD方法通过可学习的群体提示和GroupContext Transformer来优化VFM在GAD上的表现。</li>
<li>ProGraD在多个GAD基准测试中表现优越，尤其在复杂多群组场景中效果突出。</li>
<li>ProGraD仅使用10M可训练参数便实现了显著的性能提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07996">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-17435b49eff7d363ef0a085e2d860409.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a07710f75150ac62207536bedda76cb4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d4eab6c6e5c87e14908577aa58d6537.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-678e44e83a7364a07e1809f3fab9da2e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-645e2d49f698365af6a20fb53d516be6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8319b42474eb71bb5747a8cfe9f7b095.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DIVER-A-Multi-Stage-Approach-for-Reasoning-intensive-Information-Retrieval"><a href="#DIVER-A-Multi-Stage-Approach-for-Reasoning-intensive-Information-Retrieval" class="headerlink" title="DIVER: A Multi-Stage Approach for Reasoning-intensive Information   Retrieval"></a>DIVER: A Multi-Stage Approach for Reasoning-intensive Information   Retrieval</h2><p><strong>Authors:Meixiu Long, Duolin Sun, Dan Yang, Junjie Wang, Yue Shen, Jian Wang, Peng Wei, Jinjie Gu, Jiahai Wang</strong></p>
<p>Retrieval-augmented generation has achieved strong performance on knowledge-intensive tasks where query-document relevance can be identified through direct lexical or semantic matches. However, many real-world queries involve abstract reasoning, analogical thinking, or multi-step inference, which existing retrievers often struggle to capture. To address this challenge, we present \textbf{DIVER}, a retrieval pipeline tailored for reasoning-intensive information retrieval. DIVER consists of four components: document processing to improve input quality, LLM-driven query expansion via iterative document interaction, a reasoning-enhanced retriever fine-tuned on synthetic multi-domain data with hard negatives, and a pointwise reranker that combines LLM-assigned helpfulness scores with retrieval scores. On the BRIGHT benchmark, DIVER achieves state-of-the-art nDCG@10 scores of 41.6 and 28.9 on original queries, consistently outperforming competitive reasoning-aware models. These results demonstrate the effectiveness of reasoning-aware retrieval strategies in complex real-world tasks. Our code and retrieval model will be released soon. </p>
<blockquote>
<p>检索增强生成在知识密集型任务上表现强劲，其中通过直接词汇或语义匹配可以识别查询文档的相关性。然而，许多现实世界中的查询涉及抽象推理、类比思维或多步推理，现有检索器往往难以捕捉。为了应对这一挑战，我们提出了针对推理密集型信息检索量身定制的<strong>DIVER</strong>检索管道。DIVER由四个组件构成：改进输入质量的文档处理，通过迭代文档交互驱动的LLM查询扩展，在合成多域数据上微调并带有硬阴性的推理增强检索器，以及结合LLM分配的有用性分数和检索分数的逐点重新排序器。在BRIGHT基准测试中，DIVER在原始查询上取得了最新的nDCG@10分数，分别为41.6和28.9，持续超越竞争性的推理感知模型。这些结果证明了推理感知检索策略在复杂现实世界任务中的有效性。我们的代码和检索模型很快就会发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07995v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对需要推理能力的信息检索任务而设计的检索管道DIVER。DIVER包含四个组件：改进输入质量的文档处理、通过迭代文档交互驱动LLM（大型语言模型）进行查询扩展、在合成多领域数据上进行微调且包含硬负样本的推理增强检索器以及结合LLM分配的帮助的得分与检索得分的逐点重排器。在BRIGHT基准测试上，DIVER在原始查询上取得了领先的nDCG@10得分，证明了其在复杂现实任务中推理感知检索策略的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>检索增强生成在知识密集型任务上的性能已经得到验证，但对于需要抽象推理、类比思维或多步推理的查询仍面临挑战。</li>
<li>为了解决这一挑战，提出了专为推理密集型信息检索设计的DIVER检索管道。</li>
<li>DIVER包含四个主要组件，包括文档处理、LLM驱动的查询扩展、推理增强检索器以及逐点重排器。</li>
<li>DIVER在BRIGHT基准测试上实现了最先进的nDCG@10得分，证明了其在复杂现实任务中的有效性。</li>
<li>DIVER通过结合LLM的帮助的得分与检索得分，提高了检索的准确性和效率。</li>
<li>DIVER的代码和检索模型将很快发布，为其他研究者使用和改进提供基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07995">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f235e20de2e2137121d18e691489ed94.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a4c776bf9308df28d6721c4a4009d2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-412beb53088c7ed8037915006ee32cbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55189ace169207efb82148918d1530c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a026615b274442b18b2e0d5a4cb1e26.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Expert-Preference-based-Evaluation-of-Automated-Related-Work-Generation"><a href="#Expert-Preference-based-Evaluation-of-Automated-Related-Work-Generation" class="headerlink" title="Expert Preference-based Evaluation of Automated Related Work Generation"></a>Expert Preference-based Evaluation of Automated Related Work Generation</h2><p><strong>Authors:Furkan Şahinuç, Subhabrata Dutta, Iryna Gurevych</strong></p>
<p>Expert domain writing, such as scientific writing, typically demands extensive domain knowledge. Recent advances in LLMs show promising potential in reducing the expert workload. However, evaluating the quality of automatically generated scientific writing is a crucial open issue, as it requires knowledge of domain-specific evaluation criteria and the ability to discern expert preferences. Conventional automatic metrics and LLM-as-a-judge systems are insufficient to grasp expert preferences and domain-specific quality standards. To address this gap and support human-AI collaborative writing, we focus on related work generation, one of the most challenging scientific tasks, as an exemplar. We propose GREP, a multi-turn evaluation framework that integrates classical related work evaluation criteria with expert-specific preferences. Instead of assigning a single score, our framework decomposes the evaluation into fine-grained dimensions. This localized evaluation approach is further augmented with contrastive few-shot examples to provide detailed contextual guidance for the evaluation dimensions. The design principles allow our framework to deliver cardinal assessment of quality, which can facilitate better post-training compared to ordinal preference data. For better accessibility, we design two variants of GREP: a more precise variant with proprietary LLMs as evaluators, and a cheaper alternative with open-weight LLMs. Empirical investigation reveals that our framework is able to assess the quality of related work sections in a much more robust manner compared to standard LLM judges, reflects natural scenarios of scientific writing, and bears a strong correlation with the human expert assessment. We also observe that generations from state-of-the-art LLMs struggle to satisfy validation constraints of a suitable related work section. They (mostly) fail to improve based on feedback as well. </p>
<blockquote>
<p>专业领域写作，如科学写作，通常需要广泛的专业知识。最近的大型语言模型（LLM）的进步显示出减少专家工作量的潜力。然而，评估自动生成的科学写作的质量是一个关键的开放性问题，因为它需要了解特定领域的评估标准和识别专家偏好的能力。传统的自动指标和LLM作为评判系统的能力不足以把握专家偏好和特定领域的质量标准。为了弥补这一差距并支持人机协作写作，我们以相关工作生成这一最具挑战性的科学任务为例。我们提出了GREP，这是一个多轮评估框架，它将经典的相关工作评价标准与特定专家的偏好相结合。我们的框架不是赋予单一分数，而是将评估分解成精细的维度。这种局部评估方法与对比的少数案例相结合，为评估维度提供了详细的上下文指导。我们的设计原则使我们的框架能够提供质量的基数评估，这可以促进与排序偏好数据相比更好的后期训练。为了更容易访问，我们设计了GREP的两个变体：一个更精确的版本，使用专用的LLM作为评估器，和一个更便宜的版本，使用开放的LLM。实证研究结果表明，我们的框架能够更稳健地评估相关工作部分的质量，与标准LLM判断相比，它反映了科学写作的自然场景，并与人类专家评估具有很强的相关性。我们还观察到，来自最新LLM的生成在满足相关工作部分的验证约束方面存在困难。他们大多数不能根据反馈进行改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07955v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://ukplab.github.io/arxiv2025-expert-eval-rw/">https://ukplab.github.io/arxiv2025-expert-eval-rw/</a></p>
<p><strong>Summary</strong></p>
<p>本文探讨了在科学写作领域，如何评估自动生成文本的质量。文章指出，现有的自动评估工具和LLM评判系统无法准确把握专家偏好和领域特定质量标准。为此，作者提出了一种新的多回合评估框架GREP，该框架结合了经典的相关工作评价标准与特定专家偏好。GREP通过细化评价标准，提供详细的上下文指导，更准确地评估文本质量。此外，文章还介绍了GREP的两种变体，一种使用专有LLM作为评估器，另一种使用开源LLM，以降低成本。实验表明，GREP框架能更稳健地评估相关工作部分的质量，与人类专家评估结果高度相关。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>科学写作需要广泛的领域知识，LLMs的最近进展在减少专家工作量方面显示出潜力。</li>
<li>评估自动生成的科学写作的质量是一个重要但未解决的问题，需要了解领域特定的评估标准和专家偏好。</li>
<li>现有自动评估工具和LLM评判系统不足以把握专家偏好和领域特定标准。</li>
<li>GREP是一个新的多回合评估框架，结合了经典的相关工作评价标准与特定专家偏好，进行精细化评价。</li>
<li>GREP框架提供了详细的上下文指导，可以更准确地评估文本质量。</li>
<li>文章介绍了GREP的两种变体，一种使用专有LLM，另一种使用开源LLM以降低成本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07955">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-20ffa31dbc769fa55393018f62a9be0d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc70bcf6eaf7389ad704f67398ccb6b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d6235f2e8f18706e15cedd4ea1c4a22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2739cad01faa6e64f7b3079b60a9d357.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-506530f93f23cf9bbec07d05522e794e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space"><a href="#MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space" class="headerlink" title="MolmoAct: Action Reasoning Models that can Reason in Space"></a>MolmoAct: Action Reasoning Models that can Reason in Space</h2><p><strong>Authors:Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, Winson Han, Wilbert Pumacay, Angelica Wu, Rose Hendrix, Karen Farley, Eli VanderBilt, Ali Farhadi, Dieter Fox, Ranjay Krishna</strong></p>
<p>Reasoning is central to purposeful action, yet most robotic foundation models map perception and instructions directly to control, which limits adaptability, generalization, and semantic grounding. We introduce Action Reasoning Models (ARMs), a class of vision-language-action models that integrate perception, planning, and control through a structured three-stage pipeline. Our model, MolmoAct, encodes observations and instructions into depth-aware perception tokens, generates mid-level spatial plans as editable trajectory traces, and predicts precise low-level actions, enabling explainable and steerable behavior. MolmoAct-7B-D achieves strong performance across simulation and real-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching tasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success on LIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks; and in real-world fine-tuning, an additional 10% (single-arm) and an additional 22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines by an additional 23.3% on out-of-distribution generalization and achieves top human-preference scores for open-ended instruction following and trajectory steering. Furthermore, we release, for the first time, the MolmoAct Dataset – a mid-training robot dataset comprising over 10,000 high quality robot trajectories across diverse scenarios and tasks. Training with this dataset yields an average 5.5% improvement in general performance over the base model. We release all model weights, training code, our collected dataset, and our action reasoning dataset, establishing MolmoAct as both a state-of-the-art robotics foundation model and an open blueprint for building ARMs that transform perception into purposeful action through structured reasoning. Blogpost: <a target="_blank" rel="noopener" href="https://allenai.org/blog/molmoact">https://allenai.org/blog/molmoact</a> </p>
<blockquote>
<p>推理是目标行动的核心，然而大多数机器人基础模型直接将感知和指令映射到控制上，这限制了适应性、泛化和语义基础。我们引入了行动推理模型（ARMs），这是一类融合感知、规划和控制的视觉语言行动模型，通过一个结构化的三阶段管道来实现。我们的模型MolmoAct将观察和指令编码为深度感知标记，生成可编辑的轨迹跟踪作为中级空间计划，并预测精确的低级行动，从而实现可解释和可引导的行为。MolmoAct-7B-D在模拟和真实环境设置中表现出强大的性能：在SimplerEnv视觉匹配任务上达到70.5%的零射击准确率，超越闭源Pi-0和GR00TN1；在LIBERO上平均成功率达到86.6%，其中长期任务较ThinkAct增加了额外的6.3%的优势；在真实世界的微调中，相较于Pi-0-FAST，单臂任务增加了额外的10%，双手动任务增加了额外的22.7%。此外，它还以额外的23.3%的优势超越基准线在超出分布泛化方面取得最佳表现，并且在开放式指令跟随和轨迹引导方面获得了最高的人类偏好分数。此外，我们首次发布MolmoAct数据集——一个中期训练机器人数据集，包含超过10,000条高质量机器人轨迹，涵盖各种场景和任务。使用该数据集进行训练相较于基础模型在整体性能上平均提高了5.5%。我们公开了所有模型权重、训练代码、收集的数据集以及我们的行动推理数据集，确立了MolmoAct作为最先进的机器人基础模型，并为建立通过结构化推理将感知转化为有目的行动的ARMs提供了一个开放的蓝图。博客文章：<a target="_blank" rel="noopener" href="https://allenai.org/blog/molmoact">https://allenai.org/blog/molmoact</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07917v1">PDF</a> Appendix on Blogpost: <a target="_blank" rel="noopener" href="https://allenai.org/blog/molmoact">https://allenai.org/blog/molmoact</a></p>
<p><strong>Summary</strong></p>
<p>机器人动作推理模型（ARMs）是实现感知转化为有目的行动的关键。当前多数机器人基础模型直接将感知和指令映射到控制，这限制了其适应性、泛化和语义定位能力。为此，我们引入了动作推理模型（ARMs），这是一种融合感知、规划和控制的视觉语言动作模型。我们的模型MolmoAct通过结构化三阶段管道实现深度感知标记和指令编码、可编辑轨迹轨迹的空间规划以及精确的低层次动作预测，从而实现了可解释和可引导的行为。模型在模拟和真实环境下的表现优异，如在SimplerEnv视觉匹配任务上达到70.5%的零样本准确率，超越Pi-0和GR00T N1等封闭源代码模型；在LIBERO上平均成功率达到86.6%，在长周期任务上的提升幅度比ThinkAct高出6.3%；在真实世界微调中，相较于Pi-0-FAST模型，单臂任务进步幅度提高了额外的10%，双肢协同任务提高了额外的22.7%。此外，我们首次发布了MolmoAct数据集，这是一套包含超过一万条高质量机器人轨迹的中期训练数据集，涵盖各种场景和任务。使用此数据集进行训练相较于基础模型在总体上提升了平均5.5%的表现。我们的研究建立了一个兼具前沿水平的机器人基础模型MolmoAct，并为构建通过结构化推理将感知转化为有目的行动的ARMs提供了开放的蓝图。更多信息请参阅<a target="_blank" rel="noopener" href="https://allenai.org/blog/molmoact%E3%80%82">https://allenai.org/blog/molmoact。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>动作推理模型（ARMs）在机器人领域是关键实现感知到动作转化的技术。</li>
<li>现有机器人基础模型直接映射感知和指令到控制，限制了适应性、泛化能力和语义定位。</li>
<li>MolmoAct模型通过结构化三阶段管道实现深度感知和指令处理、空间规划和精确动作预测。</li>
<li>MolmoAct在模拟和真实环境下表现优异，超越多个现有模型。</li>
<li>发布了MolmoAct数据集，包含多种场景和任务的高质量机器人轨迹。</li>
<li>使用MolmoAct数据集训练可提高模型整体表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07917">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4c31b98a6690031afab73628907f472d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12c925d22d36e169454bf219ba4b6646.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-237703af41711c21d22ada7fec5cb183.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d14ed58322518cc94b350a0869d298c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63e6dbdae407e68b5b1b966a7e2cd985.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Autonomous-Navigation-of-Cloud-Controlled-Quadcopters-in-Confined-Spaces-Using-Multi-Modal-Perception-and-LLM-Driven-High-Semantic-Reasoning"><a href="#Autonomous-Navigation-of-Cloud-Controlled-Quadcopters-in-Confined-Spaces-Using-Multi-Modal-Perception-and-LLM-Driven-High-Semantic-Reasoning" class="headerlink" title="Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces   Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning"></a>Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces   Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning</h2><p><strong>Authors:Shoaib Ahmmad, Zubayer Ahmed Aditto, Md Mehrab Hossain, Noushin Yeasmin, Shorower Hossain</strong></p>
<p>This paper introduces an advanced AI-driven perception system for autonomous quadcopter navigation in GPS-denied indoor environments. The proposed framework leverages cloud computing to offload computationally intensive tasks and incorporates a custom-designed printed circuit board (PCB) for efficient sensor data acquisition, enabling robust navigation in confined spaces. The system integrates YOLOv11 for object detection, Depth Anything V2 for monocular depth estimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial Measurement Unit (IMU), and a cloud-based Large Language Model (LLM) for context-aware decision-making. A virtual safety envelope, enforced by calibrated sensor offsets, ensures collision avoidance, while a multithreaded architecture achieves low-latency processing. Enhanced spatial awareness is facilitated by 3D bounding box estimation with Kalman filtering. Experimental results in an indoor testbed demonstrate strong performance, with object detection achieving a mean Average Precision (mAP50) of 0.6, depth estimation Mean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42 trials over approximately 11 minutes, and end-to-end system latency below 1 second. This cloud-supported, high-intelligence framework serves as an auxiliary perception and navigation system, complementing state-of-the-art drone autonomy for GPS-denied confined spaces. </p>
<blockquote>
<p>本文介绍了一种先进的AI驱动感知系统，用于GPS拒止的室内环境中的自主四轴飞行器导航。所提出的框架利用云计算来处理计算密集型任务，并采用定制设计的印刷电路板（PCB）进行高效传感器数据采集，从而在有限空间内实现稳健导航。该系统集成了YOLOv11进行目标检测、Depth Anything V2进行单目深度估计、配备飞行时间（ToF）传感器和惯性测量单元（IMU）的PCB，以及基于云的大型语言模型（LLM）进行上下文感知决策。通过校准的传感器偏移实施虚拟安全包络，以确保避免碰撞，同时多线程架构实现低延迟处理。通过卡尔曼滤波的3D边界框估计增强了空间感知能力。在室内测试平台上的实验结果表明，该系统性能强劲，目标检测平均精度（mAP50）达到0.6，深度估计平均绝对误差（MAE）为7.2厘米，在约11分钟的42次试验中只有16次安全包络违反，并且端到端系统延迟低于1秒。这一受云支持的高智能框架作为辅助感知和导航系统，补充了最新无人机在GPS拒止的封闭空间中的自主性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07885v1">PDF</a> </p>
<p><strong>Summary</strong><br>     该论文介绍了一种用于GPS拒止室内环境中四轴飞行器自主导航的高级AI驱动感知系统。该系统利用云计算卸载计算密集型任务，采用定制印刷电路板进行高效传感器数据采集，并结合YOLOv11目标检测、Depth Anything V2单目深度估计等技术，实现室内环境中的稳健导航。通过飞行时间传感器和惯性测量装置的数据，结合云端大型语言模型进行语境决策，构建虚拟安全包络以实现避障。实验结果显示，该系统在室内测试环境中的表现强劲。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文描述了一种在GPS拒止室内环境中使用的先进AI驱动四轴飞行器感知系统。</li>
<li>系统利用云计算处理计算密集型任务，提升效率。</li>
<li>通过定制的印刷电路板实现高效传感器数据采集。</li>
<li>整合多种技术：YOLOv11目标检测、Depth Anything V2单目深度估计等。</li>
<li>结合飞行时间传感器和惯性测量装置数据，实现虚拟安全包络，确保避障。</li>
<li>通过云端大型语言模型进行语境决策。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07885">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ea57cce7db94a9d8adc395a1ff1e3377.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e408c9813f93122c87bcdd508f503dd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99c6efae89fd414584247ce40489458f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TAR-TVG-Enhancing-VLMs-with-Timestamp-Anchor-Constrained-Reasoning-for-Temporal-Video-Grounding"><a href="#TAR-TVG-Enhancing-VLMs-with-Timestamp-Anchor-Constrained-Reasoning-for-Temporal-Video-Grounding" class="headerlink" title="TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for   Temporal Video Grounding"></a>TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for   Temporal Video Grounding</h2><p><strong>Authors:Chaohong Guo, Xun Mo, Yongwei Nie, Xuemiao Xu, Chao Xu, Fei Yu, Chengjiang Long</strong></p>
<p>Temporal Video Grounding (TVG) aims to precisely localize video segments corresponding to natural language queries, which is a critical capability for long-form video understanding. Although existing reinforcement learning approaches encourage models to generate reasoning chains before predictions, they fail to explicitly constrain the reasoning process to ensure the quality of the final temporal predictions. To address this limitation, we propose Timestamp Anchor-constrained Reasoning for Temporal Video Grounding (TAR-TVG), a novel framework that introduces timestamp anchors within the reasoning process to enforce explicit supervision to the thought content. These anchors serve as intermediate verification points. More importantly, we require each reasoning step to produce increasingly accurate temporal estimations, thereby ensuring that the reasoning process contributes meaningfully to the final prediction. To address the challenge of low-probability anchor generation in models (e.g., Qwen2.5-VL-3B), we develop an efficient self-distillation training strategy: (1) initial GRPO training to collect 30K high-quality reasoning traces containing multiple timestamp anchors, (2) supervised fine-tuning (SFT) on distilled data, and (3) final GRPO optimization on the SFT-enhanced model. This three-stage training strategy enables robust anchor generation while maintaining reasoning quality. Experiments show that our model achieves state-of-the-art performance while producing interpretable, verifiable reasoning chains with progressively refined temporal estimations. </p>
<blockquote>
<p>时序视频定位（TVG）旨在准确地对应自然语言查询定位视频片段，这对于长格式视频理解是一项关键能力。尽管现有的强化学习方法鼓励模型在预测前生成推理链，但它们未能显式约束推理过程以确保最终时序预测的质量。为了解决这一局限性，我们提出了时序视频定位的时间戳锚点约束推理（TAR-TVG）新框架，该框架在推理过程中引入时间戳锚点，以强制执行对思想内容的显式监督。这些锚点充当中间验证点。更重要的是，我们要求每个推理步骤产生越来越准确的时序估计，从而确保推理过程对最终预测产生有意义的贡献。为了解决模型中低概率锚点生成的问题（例如Qwen2.5-VL-3B），我们开发了一种有效的自蒸馏训练策略：（1）初始GRPO训练以收集包含多个时间戳锚点的高质量推理轨迹3万条，（2）蒸馏数据的监督微调（SFT），（3）在SFT增强模型上进行最终的GRPO优化。这种三阶段的训练策略能够在保持推理质量的同时实现稳健的锚点生成。实验表明，我们的模型在产生可解释、可验证的推理链的同时，具有逐步改进的时序估计，实现了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07683v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了Temporal Video Grounding（TVG）的目标是通过自然语言查询精确定位视频段落。为解决现有强化学习方法的局限，提出了一种名为TAR-TVG的新框架，通过引入时间戳锚点来约束推理过程，确保最终时间预测的精准性。为应对模型低概率锚点生成的问题，开发了一种高效自蒸馏训练策略，包括初始GRPO训练收集高质量推理轨迹、监督微调（SFT）和最终GRPO优化。实验表明，该模型在产生可解释、可验证的推理链的同时，逐步优化时间估计，实现了卓越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Temporal Video Grounding (TVG) 的目标是精确定位与自然语言查询相对应的视频段落。</li>
<li>现有强化学习方法在TVG中存在推理质量不高的问题。</li>
<li>TAR-TVG框架通过引入时间戳锚点来约束推理过程，提高最终时间预测的质量。</li>
<li>时间戳锚点作为中间验证点，要求每个推理步骤产生越来越精确的时间估计。</li>
<li>模型面临低概率锚点生成的问题，需开发高效自蒸馏训练策略来解决。</li>
<li>自蒸馏训练策略包括三个阶段：初始GRPO训练、监督微调（SFT）和最终GRPO优化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07683">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b4b16276b25dfb1be711697b7f2214d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7781f7ef8e95f13a8277c281c4d7ac89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f959ebee0e35dd90e7d8e58599d6b0d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b1c6a89c3de1a406cd0c5ffb1dec3bfc.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="1-2-3-Check-Enhancing-Contextual-Privacy-in-LLM-via-Multi-Agent-Reasoning"><a href="#1-2-3-Check-Enhancing-Contextual-Privacy-in-LLM-via-Multi-Agent-Reasoning" class="headerlink" title="1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent   Reasoning"></a>1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent   Reasoning</h2><p><strong>Authors:Wenkai Li, Liwen Sun, Zhenxiang Guan, Xuhui Zhou, Maarten Sap</strong></p>
<p>Addressing contextual privacy concerns remains challenging in interactive settings where large language models (LLMs) process information from multiple sources (e.g., summarizing meetings with private and public information). We introduce a multi-agent framework that decomposes privacy reasoning into specialized subtasks (extraction, classification), reducing the information load on any single agent while enabling iterative validation and more reliable adherence to contextual privacy norms. To understand how privacy errors emerge and propagate, we conduct a systematic ablation over information-flow topologies, revealing when and why upstream detection mistakes cascade into downstream leakage. Experiments on the ConfAIde and PrivacyLens benchmark with several open-source and closed-sourced LLMs demonstrate that our best multi-agent configuration substantially reduces private information leakage (\textbf{18%} on ConfAIde and \textbf{19%} on PrivacyLens with GPT-4o) while preserving the fidelity of public content, outperforming single-agent baselines. These results highlight the promise of principled information-flow design in multi-agent systems for contextual privacy with LLMs. </p>
<blockquote>
<p>在处理来自多个来源的信息（例如，总结包含私人信息和公共信息的会议）的语言模型时，如何在交互式环境中解决上下文隐私问题仍然是一个挑战。我们引入了一个多智能体框架，该框架将隐私推理分解成专门的子任务（提取、分类），从而减轻了任何单个智能体的信息负载，同时实现了迭代验证和更可靠地遵守上下文隐私规范。为了了解隐私错误是如何产生和传播的，我们对信息流拓扑进行了系统的切除研究，揭示了上游检测错误何时以及为何会级联成下游泄漏。在ConfAIde和PrivacyLens基准测试上对多个开源和闭源的大型语言模型进行的实验表明，我们最好的多智能体配置大幅减少了私人信息的泄漏（在ConfAIde上减少了\textbf{18}%，在PrivacyLens上与GPT-4o一起减少了\textbf{19}%），同时保持了公共内容的保真度，超越了单智能体的基线。这些结果突显了在多智能体系统中进行有原则的信息流设计，利用大型语言模型进行上下文隐私的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07667v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文针对大型语言模型在处理来自多个源的上下文隐私信息时面临的挑战，提出了一种多代理框架。该框架将隐私推理分解为专门的子任务，减少单个代理的信息负载，同时实现迭代验证和更可靠地遵守上下文隐私规范。文章通过系统性地研究信息流动拓扑来了解隐私错误如何产生和扩散，揭示上游检测错误何时以及如何演变为下游泄露的原因。在ConfAIde和PrivacyLens基准测试上的实验表明，最佳多代理配置能显著减少私人信息泄露，同时保持公共内容的保真度，优于单代理基线。这些结果突显了原则性信息流动设计在多代理系统与大型语言模型共同进行上下文隐私保护的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在处理来自多个源的上下文隐私信息时面临挑战。</li>
<li>提出了一种多代理框架，将隐私推理分解为专门的子任务，减少单个代理的信息负载。</li>
<li>通过研究信息流动拓扑来了解隐私错误如何产生和扩散。</li>
<li>揭示了上游检测错误如何演变为下游泄露的原因。</li>
<li>在ConfAIde和PrivacyLens基准测试上，多代理配置显著减少了私人信息泄露。</li>
<li>多代理配置在减少私人信息泄露的同时，保持了公共内容的保真度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07667">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-59887f98cc2c2daa146f890c1bb3877e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-38e2366ef585ad9ab8f0bae303ba73a3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9fd6aa00c0f832aab4dc49f4f445cd49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb22eb3084b01f69b8800284f5f333fd.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="GraphCoT-VLA-A-3D-Spatial-Aware-Reasoning-Vision-Language-Action-Model-for-Robotic-Manipulation-with-Ambiguous-Instructions"><a href="#GraphCoT-VLA-A-3D-Spatial-Aware-Reasoning-Vision-Language-Action-Model-for-Robotic-Manipulation-with-Ambiguous-Instructions" class="headerlink" title="GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model   for Robotic Manipulation with Ambiguous Instructions"></a>GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model   for Robotic Manipulation with Ambiguous Instructions</h2><p><strong>Authors:Helong Huang, Min Cen, Kai Tan, Xingyue Quan, Guowei Huang, Hong Zhang</strong></p>
<p>Vision-language-action models have emerged as a crucial paradigm in robotic manipulation. However, existing VLA models exhibit notable limitations in handling ambiguous language instructions and unknown environmental states. Furthermore, their perception is largely constrained to static two-dimensional observations, lacking the capability to model three-dimensional interactions between the robot and its environment. To address these challenges, this paper proposes GraphCoT-VLA, an efficient end-to-end model. To enhance the model’s ability to interpret ambiguous instructions and improve task planning, we design a structured Chain-of-Thought reasoning module that integrates high-level task understanding and planning, failed task feedback, and low-level imaginative reasoning about future object positions and robot actions. Additionally, we construct a real-time updatable 3D Pose-Object graph, which captures the spatial configuration of robot joints and the topological relationships between objects in 3D space, enabling the model to better understand and manipulate their interactions. We further integrates a dropout hybrid reasoning strategy to achieve efficient control outputs. Experimental results across multiple real-world robotic tasks demonstrate that GraphCoT-VLA significantly outperforms existing methods in terms of task success rate and response speed, exhibiting strong generalization and robustness in open environments and under uncertain instructions. </p>
<blockquote>
<p>视觉语言动作模型已经成为机器人操作中的关键范式。然而，现有的VLA模型在处理模糊的语音指令和未知的环境状态时存在明显的局限性。此外，它们的感知主要局限于静态的二维观察，缺乏机器人与其环境之间三维交互的建模能力。为了应对这些挑战，本文提出了GraphCoT-VLA这一高效的端到端模型。为了增强模型对模糊指令的解读能力和任务规划能力，我们设计了一种结构化的思维链推理模块，该模块融合了高级任务理解和规划、任务失败反馈以及关于未来物体位置和机器人动作的底层推理想象。此外，我们构建了一个可实时更新的三维姿态物体图，捕捉机器人关节的空间配置以及物体在三维空间中的拓扑关系，使模型能够更好地理解和操作物体间的交互。我们还采用了一种混合推理策略来实现有效的控制输出。在多个真实世界机器人任务上的实验结果表明，GraphCoT-VLA在任务成功率和响应速度方面显著优于现有方法，在开放环境和不确定指令下表现出强大的泛化能力和稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07650v1">PDF</a> 10 pages, 6 figures</p>
<p><strong>Summary</strong><br>基于视觉-语言-动作模型的机器人操作技术中，GraphCoT-VLA模型能有效解决现有模型在处理模糊语言指令和未知环境状态方面的局限性。该模型通过设计结构化思维链（Chain-of-Thought）推理模块，提升对模糊指令的解读和任务规划能力。此外，还构建了可实时更新的三维姿态物体图（3D Pose-Object graph），能捕捉机器人关节的空间配置和物体间的拓扑关系，从而提升模型对物体交互的理解和操作能力。实验结果证明，GraphCoT-VLA在多个真实机器人任务中显著优于现有方法，任务成功率和响应速度均有显著提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有Vision-Language-Action（VLA）模型在处理模糊语言指令和未知环境状态方面存在局限性。</li>
<li>GraphCoT-VLA模型通过设计结构化思维链（Chain-of-Thought）推理模块，提升对模糊指令的解读和任务规划能力。</li>
<li>GraphCoT-VLA构建了可实时更新的三维姿态物体图（3D Pose-Object graph），能捕捉机器人关节的空间配置和物体间的拓扑关系。</li>
<li>模型具备更好的物体交互理解和操作能力。</li>
<li>GraphCoT-VLA在多个真实机器人任务中表现优异，任务成功率和响应速度均有显著提升。</li>
<li>GraphCoT-VLA模型通过整合高层次的任务理解和规划、失败任务反馈以及未来物体位置和机器人动作的低位想象推理，提高了性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07650">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ba7e60610cf82958be15df45bb69f868.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b9eecad3318d9b8d3b69662a3ab2c82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab87b8336bf376c3f1d13ca115dd4e2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b474feb0d30c202b3b6ccab0e4e9454.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2397179ba93225aeeb2ef69a89bf9a33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7e9845ad83e0cc7db1a8bb05276e4ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f18a69731f790ceca8a9371d00b6a17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-232546a85488a49eef9ace3e48f96ad9.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="InterChart-Benchmarking-Visual-Reasoning-Across-Decomposed-and-Distributed-Chart-Information"><a href="#InterChart-Benchmarking-Visual-Reasoning-Across-Decomposed-and-Distributed-Chart-Information" class="headerlink" title="InterChart: Benchmarking Visual Reasoning Across Decomposed and   Distributed Chart Information"></a>InterChart: Benchmarking Visual Reasoning Across Decomposed and   Distributed Chart Information</h2><p><strong>Authors:Anirudh Iyengar Kaniyar Narayana Iyengar, Srija Mukhopadhyay, Adnan Qidwai, Shubhankar Singh, Dan Roth, Vivek Gupta</strong></p>
<p>We introduce InterChart, a diagnostic benchmark that evaluates how well vision-language models (VLMs) reason across multiple related charts, a task central to real-world applications such as scientific reporting, financial analysis, and public policy dashboards. Unlike prior benchmarks focusing on isolated, visually uniform charts, InterChart challenges models with diverse question types ranging from entity inference and trend correlation to numerical estimation and abstract multi-step reasoning grounded in 2-3 thematically or structurally related charts. We organize the benchmark into three tiers of increasing difficulty: (1) factual reasoning over individual charts, (2) integrative analysis across synthetically aligned chart sets, and (3) semantic inference over visually complex, real-world chart pairs. Our evaluation of state-of-the-art open and closed-source VLMs reveals consistent and steep accuracy declines as chart complexity increases. We find that models perform better when we decompose multi-entity charts into simpler visual units, underscoring their struggles with cross-chart integration. By exposing these systematic limitations, InterChart provides a rigorous framework for advancing multimodal reasoning in complex, multi-visual environments. </p>
<blockquote>
<p>我们介绍了InterChart，这是一个诊断基准测试，用于评估视觉语言模型（VLM）在多张相关图表上的推理能力。这一任务是现实世界应用（如科学报告、金融分析和公共政策仪表板）的核心。与以往侧重于孤立、视觉统一的图表的基准测试不同，InterChart通过多样的问题类型（如实体推理、趋势相关性、数值估算和基于两三个主题或结构上相关图表的抽象多步骤推理等）来挑战模型。我们将基准测试分为三个难度递增的层次：（1）单个图表的推理，（2）合成图表集的综合分析，（3）视觉上复杂、真实世界的图表对的语义推理。我们对最新开源和闭源的VLM评估显示，随着图表复杂性的增加，准确性持续且急剧下降。我们发现，当我们将多实体图表分解为更简单的视觉单元时，模型的性能更好，这突出了它们在跨图表整合方面的困难。通过揭示这些系统局限，InterChart为复杂多视觉环境中多模式推理的进步提供了严格框架。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07630v1">PDF</a> 18 pages, 6 figures, 12 tables. Benchmark dataset and evaluation code   will be publicly made available</p>
<p><strong>Summary</strong>：</p>
<p>我们介绍了InterChart这一诊断基准测试，它旨在评估视觉语言模型在多张相关图表上的推理能力。该测试涵盖多种类型的图表问题，包括实体推断、趋势关联、数值估算和基于两到三张主题或结构上相关图表的抽象多步骤推理等。基准测试分为三个难度递增的层次：个人图表的推理能力、合成图表集的整合分析以及视觉复杂图表对的语义推理能力。对最先进的开放和封闭源代码视觉语言模型的评估显示，随着图表复杂性的增加，准确性持续且急剧下降。我们发现将多实体图表分解为更简单的视觉单元可以提高模型的性能，突显出跨图表整合方面的困难。通过揭示这些系统性局限，InterChart为复杂多视觉环境中的多模态推理提供了一个严格的框架。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>InterChart是一个旨在评估视觉语言模型在多张相关图表上推理能力的诊断基准测试。</li>
<li>它涵盖了多种类型的图表问题，包括实体推断、趋势关联等。</li>
<li>基准测试分为三个层次，难度递增。</li>
<li>随着图表复杂性的增加，现有模型的准确性急剧下降。</li>
<li>将多实体图表分解为更简单的视觉单元可以提高模型的性能。</li>
<li>InterChart揭示了跨图表整合方面的困难。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07630">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e6b7542a1682200998776c7189f020c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ff7efbbf67f1d42b6013abddfdd4cee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e85551ef20a32ef49808c74ed1f8329d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31a05086f7760686d76ef4f4c08d07dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-461f3b74639c97f1b7178d003ba6537d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-336b8db91dc0ef3b946cb02f0a901381.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-10b2ecb8600b87ee98c69ad5c9d2b437.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb018cf332e64e269c482ce53d039512.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Klear-Reasoner-Advancing-Reasoning-Capability-via-Gradient-Preserving-Clipping-Policy-Optimization"><a href="#Klear-Reasoner-Advancing-Reasoning-Capability-via-Gradient-Preserving-Clipping-Policy-Optimization" class="headerlink" title="Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving   Clipping Policy Optimization"></a>Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving   Clipping Policy Optimization</h2><p><strong>Authors:Zhenpeng Su, Leiyu Pan, Xue Bai, Dening Liu, Guanting Dong, Jiaming Huang, Wenping Hu, Guorui Zhou</strong></p>
<p>We present Klear-Reasoner, a model with long reasoning capabilities that demonstrates careful deliberation during problem solving, achieving outstanding performance across multiple benchmarks. Although there are already many excellent works related to inference models in the current community, there are still many problems with reproducing high-performance inference models due to incomplete disclosure of training details. This report provides an in-depth analysis of the reasoning model, covering the entire post-training workflow from data preparation and long Chain-of-Thought supervised fine-tuning (long CoT SFT) to reinforcement learning (RL), along with detailed ablation studies for each experimental component. For SFT data, our experiments show that a small number of high-quality data sources are more effective than a large number of diverse data sources, and that difficult samples can achieve better results without accuracy filtering. In addition, we investigate two key issues with current clipping mechanisms in RL: Clipping suppresses critical exploration signals and ignores suboptimal trajectories. To address these challenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO) that gently backpropagates gradients from clipped tokens. GPPO not only enhances the model’s exploration capacity but also improves its efficiency in learning from negative samples. Klear-Reasoner exhibits exceptional reasoning abilities in mathematics and programming, scoring 90.5% on AIME 2024, 83.2% on AIME 2025, 66.0% on LiveCodeBench V5 and 58.1% on LiveCodeBench V6. </p>
<blockquote>
<p>我们推出了Klear-Reasoner，这是一款具有长期推理能力的模型，在解决问题时展现出谨慎的考虑，并在多个基准测试中表现出卓越的性能。尽管当前社区已经有很多与推理模型相关的优秀作品，但由于培训细节披露不完整，因此重现高性能推理模型仍然存在许多问题。本报告对推理模型进行了深入分析，涵盖了从数据准备和长链思维监督微调（long CoT SFT）到强化学习（RL）的整个训练后工作流程，并对每个实验组件进行了详细的消融研究。对于SFT数据，我们的实验表明，少数高质量的数据源比大量多样的数据源更有效，而且困难样本可以在无需精度过滤的情况下实现更好的结果。此外，我们研究了当前强化学习中的裁剪机制的两个关键问题：裁剪会抑制关键探索信号并忽略次优轨迹。为了解决这些挑战，我们提出了梯度保持裁剪策略优化（GPPO），该优化能温和地反向传播被裁剪令牌的梯度。GPPO不仅提高了模型的探索能力，而且提高了其从负样本中学习的效率。Klear-Reasoner在数学和编程方面展现出卓越的推理能力，在AIME 2024上得分90.5%，在AIME 2025上得分83.2%，在LiveCodeBench V5上得分66.0%，在LiveCodeBench V6上得分58.1%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07629v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于Klear-Reasoner模型的深入研究，该模型具备出色的长期推理能力，并在多个基准测试中表现出卓越性能。报告详细分析了推理模型，从头开始介绍了数据准备和长期思维链监督精细调整（long CoT SFT）到强化学习（RL）的整个训练流程，并对每个实验组件进行了详细的消融研究。实验表明，少量高质量数据源比大量多样化数据源更有效，且困难样本无需过滤即可获得更好的结果。针对当前RL中裁剪机制的两个关键问题，提出了梯度保留裁剪策略优化（GPPO）方法，该方法可温和地反向传播被裁剪令牌的梯度。GPPO不仅提高了模型的探索能力，还提高了其从负样本中学习的效率。Klear-Reasoner在数学和编程方面展现出非凡的推理能力，在AIME 2024、AIME 2025和LiveCodeBench V5&#x2F;V6等考试中取得显著成绩。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Klear-Reasoner是一个具有长期推理能力的模型，在多个基准测试中表现卓越。</li>
<li>报告提供了从数据准备到强化学习的全面训练流程分析。</li>
<li>实验显示少量高质量数据源比大量多样化数据源更有效。</li>
<li>困难样本无需过滤即可获得更好的结果。</li>
<li>针对RL中的裁剪机制问题，提出了GPPO方法以提高模型的探索能力和从负样本中学习效率。</li>
<li>Klear-Reasoner在数学和编程方面展现出非凡的推理能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07629">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9a06e5ed042d5db484f4427a7f20a944.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="AR-VRM-Imitating-Human-Motions-for-Visual-Robot-Manipulation-with-Analogical-Reasoning"><a href="#AR-VRM-Imitating-Human-Motions-for-Visual-Robot-Manipulation-with-Analogical-Reasoning" class="headerlink" title="AR-VRM: Imitating Human Motions for Visual Robot Manipulation with   Analogical Reasoning"></a>AR-VRM: Imitating Human Motions for Visual Robot Manipulation with   Analogical Reasoning</h2><p><strong>Authors:Dejie Yang, Zijing Zhao, Yang Liu</strong></p>
<p>Visual Robot Manipulation (VRM) aims to enable a robot to follow natural language instructions based on robot states and visual observations, and therefore requires costly multi-modal data. To compensate for the deficiency of robot data, existing approaches have employed vision-language pretraining with large-scale data. However, they either utilize web data that differs from robotic tasks, or train the model in an implicit way (e.g., predicting future frames at the pixel level), thus showing limited generalization ability under insufficient robot data. In this paper, we propose to learn from large-scale human action video datasets in an explicit way (i.e., imitating human actions from hand keypoints), introducing Visual Robot Manipulation with Analogical Reasoning (AR-VRM). To acquire action knowledge explicitly from human action videos, we propose a keypoint Vision-Language Model (VLM) pretraining scheme, enabling the VLM to learn human action knowledge and directly predict human hand keypoints. During fine-tuning on robot data, to facilitate the robotic arm in imitating the action patterns of human motions, we first retrieve human action videos that perform similar manipulation tasks and have similar historical observations , and then learn the Analogical Reasoning (AR) map between human hand keypoints and robot components. Taking advantage of focusing on action keypoints instead of irrelevant visual cues, our method achieves leading performance on the CALVIN benchmark {and real-world experiments}. In few-shot scenarios, our AR-VRM outperforms previous methods by large margins , underscoring the effectiveness of explicitly imitating human actions under data scarcity. </p>
<blockquote>
<p>视觉机器人操控（VRM）的目标是使机器人能够根据机器人状态和视觉观察来执行自然语言指令，因此需要昂贵的多模式数据。为了弥补机器人数据的不足，现有方法已经采用了大规模数据的视觉语言预训练。然而，它们要么使用与机器人任务不同的网络数据，要么以隐式方式训练模型（例如，在像素级别预测未来帧），因此在机器人数据不足的情况下，显示出有限的泛化能力。在本文中，我们提出以明确的方式从大规模人类行为视频数据集中进行学习（即通过手关键点模仿人类行为），引入类比推理视觉机器人操控（AR-VRM）。为了从人类行为视频中明确获取行动知识，我们提出了一种关键点视觉语言模型（VLM）预训练方案，使VLM能够学习人类行为知识并直接预测人类手关键点。在机器人数据上进行微调时，为了促进机械臂模仿人类运动的行为模式，我们首先检索执行类似操控任务并具有相似历史观察记录的人类行为视频，然后学习人类手关键点和机器人组件之间的类比推理（AR）映射。通过关注动作关键点而不是无关的视觉线索，我们的方法在CALVIN基准测试{和真实世界实验}中取得了领先的成绩。在少数场景的情况下，我们的AR-VRM大幅超越了以前的方法，突显了在数据稀缺情况下明确模仿人类动作的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07626v1">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong></p>
<p>视觉机器人操控（VRM）旨在使机器人能够根据机器人状态与视觉观察来执行自然语言指令，因此需依赖昂贵的多模态数据。为弥补机器人数据的不足，现有方法采用视觉语言预训练结合大规模数据的方式。然而，它们使用与机器人任务不同的网络数据或采用隐式训练方法（如预测未来帧像素级别），在机器人数据不足时表现出有限的泛化能力。本文提出从大规模人类动作视频数据集中显式学习的方法（即根据手部关键点模仿人类动作），引入具有类比推理（AR）的视觉机器人操控（AR-VRM）。我们从人类动作视频中显式获取动作知识，并提出一种关键点视觉语言模型（VLM）的预训练方案，使VLM能够学习人类动作知识并直接预测人手关键点。在机器人数据上进行微调时，为帮助机械臂模仿人类动作模式，我们首先检索执行类似操控任务并具有相似历史观察记录的人类动作视频，然后学习人手关键点与机器人组件之间的类比推理（AR）映射。通过关注动作关键点而非无关的视觉线索，我们的方法在CALVIN基准测试和真实世界实验中取得了领先的表现。在少量场景数据下，我们的AR-VRM较之前的方法有较大优势，突显了在数据稀缺时显式模仿人类动作的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VRM需要多模态数据来使机器人根据自然语言指令执行操作。</li>
<li>现有方法使用网络数据与机器人任务不同或隐式训练模型，导致泛化能力受限。</li>
<li>本文提出从大规模人类动作视频数据中显式学习的方法，即AR-VRM。</li>
<li>通过关键点VLM预训练方案，模型能够学习人类动作知识并预测手部关键点。</li>
<li>在微调阶段，利用类比推理（AR）映射机器人与人类的动作模式。</li>
<li>方法专注于动作关键点而非无关视觉线索，在基准测试和真实世界实验中表现领先。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07626">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9f749d0224f483b5590b7b39f11cab8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0bdcde10a1102ff93eedeeff1289f7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-102edc52aef97d2cebab71cde41934ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4484466e0d552d29d56193e0d2235468.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="ThinkTuning-Instilling-Cognitive-Reflections-without-Distillation"><a href="#ThinkTuning-Instilling-Cognitive-Reflections-without-Distillation" class="headerlink" title="ThinkTuning: Instilling Cognitive Reflections without Distillation"></a>ThinkTuning: Instilling Cognitive Reflections without Distillation</h2><p><strong>Authors:Aswin RRV, Jacob Dineen, Divij Handa, Md Nayem Uddin, Mihir Parmar, Chitta Baral, Ben Zhou</strong></p>
<p>Recent advances in test-time scaling have led to the emergence of thinking LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL drives this self-improvement paradigm, a recent study (Gandhi et al., 2025) shows that RL alone does not truly instill these new reasoning abilities - it merely draws out behaviors already present in the base models. This raises a question: How can we train the models that don’t exhibit such thinking behavior to develop it in the first place? To this end, we propose ThinkTuning, a GRPO-based interactive training approach where we augment the rollouts of a student model with the guidance from a teacher model. A simple idea from classroom practice inspires our method: a teacher poses a problem, lets the student try an answer, then gives corrective feedback – enough to point the mind in the right direction and then show the solution. Each piece of feedback reshapes the student’s thoughts, leading them to arrive at the correct solution. Similarly, we find that this type of implicit supervision through feedback from a teacher model of the same size improves the reasoning capabilities of the student model. In particular, on average, our method shows a 3.85% improvement over zero-shot baselines across benchmarks, and on MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements over the vanilla-GRPO baseline. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/3rdAT/ThinkTuning">https://github.com/3rdAT/ThinkTuning</a>. </p>
<blockquote>
<p>最近测试时间缩放方面的进展催生出了展现自我反思行为和多步推理的思考型大型语言模型（LLMs）。虽然强化学习（RL）推动了这种自我改进的模式，但最近的一项研究（甘地等人，2025）表明，仅依靠RL并不能真正赋予这些新的推理能力——它只是激发出基础模型中已经存在的行为。这就提出了一个问题：如何训练那些原本不具备这种思考行为的模型，让它们首先具备这种能力呢？为此，我们提出了ThinkTuning，这是一种基于GRPO的交互式训练方法，我们借助教师模型的指导来增强学生模型的滚动输出。我们的方法灵感来自于课堂实践的简单想法：教师提出问题，让学生尝试回答，然后给出纠正反馈——足以指出正确的方向并展示解决方案。每一份反馈都会重塑学生的思路，引导他们找到正确的答案。同样，我们发现，通过教师模型的反馈进行这种隐式监督，可以提高学生模型的推理能力。特别是，我们的方法在所有基准测试上的平均表现比零基准线高出3.85%，在MATH-500、AIME和GPQA-Diamond上的表现则分别比基础GRPO高出2.08%、2.23%和3.99%。源代码可在<a target="_blank" rel="noopener" href="https://github.com/3rdAT/ThinkTuning">https://github.com/3rdAT/ThinkTuning</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07616v1">PDF</a> 15 pages</p>
<p><strong>Summary</strong></p>
<p>本文介绍了最新测试时间缩放技术的进步，使得大型语言模型展现出自我反思行为和跨多步骤的推理能力。虽然强化学习驱动了这种自我改进的模式，但研究表明仅靠强化学习并不能真正赋予模型这些新的推理能力，而只是激发模型中已有的能力。为此，本文提出了ThinkTuning方法，这是一种基于GRPO的交互式训练方法，通过教师模型的指导来增强学生模型的rollout。该方法受到课堂实践的启发，通过教师提出问题、学生尝试回答、教师提供纠正反馈的方式，逐步引导学生找到正确答案。类似地，本文发现来自相同规模教师模型的反馈能提高学生的推理能力。实验结果显示，该方法在多个基准测试上平均比零基准高出3.85%，并在MATH-500、AIME和GPQA-Diamond上分别实现了2.08%、2.23%和3.99%的改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>测试时间缩放技术的最新进展导致大型语言模型展现出自我反思行为和跨多步骤推理能力。</li>
<li>强化学习虽在这种自我改进模式中起关键作用，但单独使用并不能真正赋予模型新的推理能力。</li>
<li>ThinkTuning方法是一种基于GRPO的交互式训练方法，通过教师模型的指导来增强学生模型的性能。</li>
<li>ThinkTuning方法受到课堂实践的启发，通过教师与学生的互动来提高学生的推理能力。</li>
<li>教师模型的反馈能够重塑学生模型的想法，并引导其找到正确的解决方案。</li>
<li>ThinkTuning方法在多个基准测试上实现了显著的改进，平均高于零基准3.85%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07616">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d6a899d67263b1d9a099840360cf8515.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccd28137d612c00d12112dd39483cb29.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="CoT-Pose-Chain-of-Thought-Reasoning-for-3D-Pose-Generation-from-Abstract-Prompts"><a href="#CoT-Pose-Chain-of-Thought-Reasoning-for-3D-Pose-Generation-from-Abstract-Prompts" class="headerlink" title="CoT-Pose: Chain-of-Thought Reasoning for 3D Pose Generation from   Abstract Prompts"></a>CoT-Pose: Chain-of-Thought Reasoning for 3D Pose Generation from   Abstract Prompts</h2><p><strong>Authors:Junuk Cha, Jihyeon Kim</strong></p>
<p>Recent advances in multi-modal large language models (MLLMs) and chain-of-thought (CoT) reasoning have led to significant progress in image and text generation tasks. However, the field of 3D human pose generation still faces critical limitations. Most existing text-to-pose models rely heavily on detailed (low-level) prompts that explicitly describe joint configurations. In contrast, humans tend to communicate actions and intentions using abstract (high-level) language. This mismatch results in a practical challenge for deploying pose generation systems in real-world scenarios. To bridge this gap, we introduce a novel framework that incorporates CoT reasoning into the pose generation process, enabling the interpretation of abstract prompts into accurate 3D human poses. We further propose a data synthesis pipeline that automatically generates triplets of abstract prompts, detailed prompts, and corresponding 3D poses for training process. Experimental results demonstrate that our reasoning-enhanced model, CoT-Pose, can effectively generate plausible and semantically aligned poses from abstract textual inputs. This work highlights the importance of high-level understanding in pose generation and opens new directions for reasoning-enhanced approach for human pose generation. </p>
<blockquote>
<p>近年来，多模态大型语言模型（MLLMs）和思维链（CoT）推理的进展在图像和文本生成任务方面取得了显著进展。然而，在3D人体姿态生成领域，仍然存在关键性局限。大多数现有的文本到姿态模型严重依赖于详细描述关节配置的低位提示。相比之下，人类倾向于使用抽象（高级）语言来交流动作和意图。这种不匹配为在实际场景中部署姿态生成系统带来了实际挑战。为了弥补这一差距，我们引入了一个将思维链推理融入姿态生成过程的新框架，该框架能够将抽象提示解释为准确可靠的3D人体姿态。我们还提出了一个数据合成管道，该管道能够自动生成抽象提示、详细提示和相应的3D姿态的三元组，用于训练过程。实验结果表明，我们增强的推理模型CoT-Pose可以有效地从抽象的文本输入中生成合理且语义对齐的姿态。这项工作强调了高级理解在姿态生成中的重要性，并为增强推理方法在人类姿态生成领域开辟了新的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07540v1">PDF</a> ICCVW’25</p>
<p><strong>Summary</strong></p>
<p>本文介绍了多模态大型语言模型（MLLMs）和思维链（CoT）推理在图像和文本生成任务中的最新进展。然而，在3D人体姿态生成领域仍存在关键限制。大多数现有的文本到姿态模型依赖于详细的低级别提示来描述关节配置。与之相反，人类倾向于使用抽象的高级语言来表达动作和意图。为了弥补这一差距，本文提出了一种结合CoT推理的新型框架，使抽象提示能够转化为准确的3D人体姿态。此外，还提出了一种数据合成管道，可自动生成抽象提示、详细提示和相应的3D姿态用于训练过程。实验结果表明，增强型推理模型CoT-Pose可以有效地从抽象文本输入中生成合理且语义对齐的姿态。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型和思维链推理在图像和文本生成任务中有显著进展。</li>
<li>3D人体姿态生成领域仍面临依赖低级别提示的局限性。</li>
<li>人类使用高级语言来表达动作和意图，而现有模型通常依赖低级别提示。</li>
<li>提出了一种结合思维链推理的新型框架，用于从抽象提示生成准确的3D人体姿态。</li>
<li>引入了一种数据合成管道，自动生成用于训练的数据。</li>
<li>实验证明，增强型推理模型CoT-Pose能从抽象文本输入中生成合理且语义对齐的姿态。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07540">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-39671d54f746cb5e908a9e3bbd01414b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b5c6728e10c3743acc2b3b497061785.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-476025197f67548c4f0b9afeb8fcf246.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0110d458b376975058e89be6b036a913.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-13/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-13/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-13/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d2f526f54f9eff944fd028d31717a3c2.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-08-13  ODYSSEY Open-World Quadrupeds Exploration and Manipulation for   Long-Horizon Tasks
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-13/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-e3eed16a328474636c13c8f2ebadfa3d.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-08-13  Think Before You Talk Enhancing Meaningful Dialogue Generation in   Full-Duplex Speech Language Models with Planning-Inspired Text Guidance
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29580.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
