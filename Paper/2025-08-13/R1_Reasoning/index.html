<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-13  Capabilities of GPT-5 on Multimodal Medical Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-2a026615b274442b18b2e0d5a4cb1e26.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-13-æ›´æ–°"><a href="#2025-08-13-æ›´æ–°" class="headerlink" title="2025-08-13 æ›´æ–°"></a>2025-08-13 æ›´æ–°</h1><h2 id="Capabilities-of-GPT-5-on-Multimodal-Medical-Reasoning"><a href="#Capabilities-of-GPT-5-on-Multimodal-Medical-Reasoning" class="headerlink" title="Capabilities of GPT-5 on Multimodal Medical Reasoning"></a>Capabilities of GPT-5 on Multimodal Medical Reasoning</h2><p><strong>Authors:Shansong Wang, Mingzhe Hu, Qiang Li, Mojtaba Safari, Xiaofeng Yang</strong></p>
<p>Recent advances in large language models (LLMs) have enabled general-purpose systems to perform increasingly complex domain-specific reasoning without extensive fine-tuning. In the medical domain, decision-making often requires integrating heterogeneous information sources, including patient narratives, structured data, and medical images. This study positions GPT-5 as a generalist multimodal reasoner for medical decision support and systematically evaluates its zero-shot chain-of-thought reasoning performance on both text-based question answering and visual question answering tasks under a unified protocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20 against standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU medical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that GPT-5 consistently outperforms all baselines, achieving state-of-the-art accuracy across all QA benchmarks and delivering substantial gains in multimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and understanding scores by +29.62% and +36.18% over GPT-4o, respectively, and surpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in understanding. In contrast, GPT-4o remains below human expert performance in most dimensions. A representative case study demonstrates GPT-5â€™s ability to integrate visual and textual cues into a coherent diagnostic reasoning chain, recommending appropriate high-stakes interventions. Our results show that, on these controlled multimodal reasoning benchmarks, GPT-5 moves from human-comparable to above human-expert performance. This improvement may substantially inform the design of future clinical decision-support systems. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•ä½¿å¾—é€šç”¨ç³»ç»Ÿèƒ½å¤Ÿåœ¨ä¸éœ€è¦å¹¿æ³›å¾®è°ƒçš„æƒ…å†µä¸‹æ‰§è¡Œè¶Šæ¥è¶Šå¤æ‚çš„ç‰¹å®šé¢†åŸŸæ¨ç†ã€‚åœ¨åŒ»ç–—é¢†åŸŸï¼Œå†³ç­–åˆ¶å®šé€šå¸¸éœ€è¦æ•´åˆå¼‚è´¨çš„ä¿¡æ¯æ¥æºï¼ŒåŒ…æ‹¬æ‚£è€…å™è¿°ã€ç»“æ„åŒ–æ•°æ®å’ŒåŒ»ç–—å›¾åƒã€‚æœ¬ç ”ç©¶å°†GPT-5å®šä½ä¸ºé€šç”¨çš„å¤šæ¨¡å¼æ¨ç†æœºï¼Œç”¨äºåŒ»ç–—å†³ç­–æ”¯æŒï¼Œå¹¶åœ¨ç»Ÿä¸€åè®®ä¸‹ï¼Œç³»ç»Ÿè¯„ä¼°å…¶åœ¨åŸºäºæ–‡æœ¬çš„é—®é¢˜å›ç­”å’Œè§†è§‰é—®é¢˜å›ç­”ä»»åŠ¡ä¸Šçš„é›¶å°„å‡»é“¾å¼æ€ç»´æ¨ç†æ€§èƒ½ã€‚æˆ‘ä»¬ä»¥MedQAã€MedXpertQAï¼ˆæ–‡æœ¬å’Œå¤šæ¨¡æ€ï¼‰ã€MMLUåŒ»ç–—å­é›†ã€USMLEè‡ªæˆ‘è¯„ä¼°è€ƒè¯•å’ŒVQA-RADçš„æ ‡å‡†åˆ†å‰²æ•°æ®ä¸ºåŸºå‡†ï¼Œå¯¹GPT-5ã€GPT-5-miniã€GPT-5-nanoå’ŒGPT-4o-2024-11-20è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒGPT-5æŒç»­ä¼˜äºæ‰€æœ‰åŸºçº¿ï¼Œåœ¨æ‰€æœ‰çš„é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­å®ç°äº†å®è´¨æ€§çš„æ”¶ç›Šã€‚åœ¨MedXpertQA MMä¸Šï¼ŒGPT-5åœ¨æ¨ç†å’Œç†è§£æ–¹é¢çš„å¾—åˆ†åˆ†åˆ«æ¯”GPT-4oé«˜å‡º+29.62%å’Œ+36.18%ï¼Œå¹¶ä¸”åœ¨æ¨ç†å’Œç†è§£æ–¹é¢è¶…è¶Šé¢„å…ˆæˆæƒçš„ä¸“å®¶+24.23%å’Œ+29.40%ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGPT-4oåœ¨å¤§å¤šæ•°ç»´åº¦ä¸Šä»ä½äºäººç±»ä¸“å®¶çš„è¡¨ç°ã€‚ä¸€ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„æ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†GPT-5å°†è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢æ•´åˆåˆ°è¿è´¯çš„è¯Šæ–­æ¨ç†é“¾ä¸­çš„èƒ½åŠ›ï¼Œå¹¶æ¨èäº†é€‚å½“çš„é«˜é£é™©å¹²é¢„æªæ–½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨è¿™äº›å—æ§çš„å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒGPT-5çš„è¡¨ç°ä»ä¸äººç±»ç›¸å½“æå‡åˆ°äº†è¶…è¶Šäººç±»ä¸“å®¶çš„æ°´å¹³ã€‚è¿™ä¸€æ”¹è¿›å¯èƒ½ä¼šä¸ºæœªæ¥çš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿæä¾›å®è´¨æ€§çš„ä¿¡æ¯å‚è€ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08224v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä½¿å¾—é€šç”¨ç³»ç»Ÿèƒ½å¤Ÿåœ¨ä¸éœ€è¦å¹¿æ³›å¾®è°ƒçš„æƒ…å†µä¸‹æ‰§è¡Œè¶Šæ¥è¶Šå¤æ‚çš„é¢†åŸŸç‰¹å®šæ¨ç†ã€‚æœ¬ç ”ç©¶å°†GPT-5å®šä½ä¸ºåŒ»ç–—å†³ç­–æ”¯æŒçš„ä¸€èˆ¬æ€§å¤šæ¨¡å¼æ¨ç†å™¨ï¼Œå¹¶ç³»ç»Ÿåœ°è¯„ä¼°å…¶åœ¨ç»Ÿä¸€åè®®ä¸‹çš„é›¶å°„å‡»é“¾å¼æ€ç»´æ¨ç†æ€§èƒ½ã€‚åœ¨åŸºäºæ–‡æœ¬å’Œè§†è§‰çš„é—®é¢˜å›ç­”ä»»åŠ¡ä¸­ï¼ŒGPT-5åœ¨å„ç§æ ‡å‡†åŒ–åˆ†å‰²çš„MedQAã€MedXpertQAï¼ˆæ–‡æœ¬å’Œå¤šæ¨¡å¼ï¼‰ã€MMLUåŒ»ç–—å­é›†ã€USMLEè‡ªæˆ‘è¯„ä¼°è€ƒè¯•å’ŒVQA-RADä¸Šè¡¨ç°å‡ºè‰²ã€‚ç»“æœè¯æ˜GPT-5åœ¨æ‰€æœ‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºæ‰€æœ‰åŸºçº¿ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨å¤šæ¨¡å¼æ¨ç†æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚åœ¨MedXpertQA MMä¸Šï¼ŒGPT-5åœ¨æ¨ç†å’Œç†è§£æ–¹é¢çš„å¾—åˆ†åˆ†åˆ«æ¯”GPT-4oé«˜å‡º+29.62%å’Œ+36.18%ï¼Œå¹¶ä¸”åœ¨æ¨ç†å’Œç†è§£æ–¹é¢è¶…è¶Šé¢„å…ˆæˆæƒçš„äººç±»ä¸“å®¶åˆ†åˆ«é«˜å‡º+24.23%å’Œ+29.40%ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGPT-4oåœ¨å¤§å¤šæ•°ç»´åº¦ä¸Šä»ä½äºäººç±»ä¸“å®¶çš„è¡¨ç°ã€‚ä¸€ä¸ªå…¸å‹çš„æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼ŒGPT-5èƒ½å¤Ÿæ•´åˆè§†è§‰å’Œæ–‡æœ¬çº¿ç´¢ï¼Œå½¢æˆè¿è´¯çš„è¯Šæ–­æ¨ç†é“¾ï¼Œæ¨èé€‚å½“çš„é«˜é£é™©å¹²é¢„æªæ–½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨è¿™äº›å—æ§çš„å¤šæ¨¡å¼æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒGPT-5çš„è¡¨ç°å·²ä»äººç±»ç›¸å½“çš„æ°´å¹³æå‡åˆ°äº†è¶…è¶Šäººç±»ä¸“å®¶çš„æ°´å¹³ã€‚è¿™ä¸€è¿›æ­¥å¯èƒ½ä¸ºæœªæ¥çš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿæä¾›é‡è¦çš„è®¾è®¡å‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯åœ¨æ— éœ€å¹¿æ³›å¾®è°ƒçš„æƒ…å†µä¸‹è¿›è¡Œå¤æ‚çš„é¢†åŸŸç‰¹å®šæ¨ç†ã€‚</li>
<li>GPT-5åœ¨åŒ»ç–—å†³ç­–æ”¯æŒä¸­è¡¨ç°å‡ºå¼ºå¤§çš„å¤šæ¨¡å¼æ¨ç†èƒ½åŠ›ã€‚</li>
<li>GPT-5åœ¨å„ç§æ ‡å‡†åŒ–åŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šå…¶ä»–åŸºå‡†å’Œå…ˆå‰çš„ç ”ç©¶ã€‚</li>
<li>GPT-5åœ¨æ¨ç†å’Œç†è§£æ–¹é¢çš„å¾—åˆ†æ˜¾è‘—ä¼˜äºé¢„å…ˆæˆæƒçš„äººç±»ä¸“å®¶ã€‚</li>
<li>GPT-5èƒ½æ•´åˆè§†è§‰å’Œæ–‡æœ¬çº¿ç´¢ï¼Œå½¢æˆè¿è´¯çš„è¯Šæ–­æ¨ç†é“¾ã€‚</li>
<li>GPT-5çš„è¡¨ç°å·²ä»äººç±»ç›¸å½“çš„æ°´å¹³æå‡åˆ°äº†è¶…è¶Šäººç±»ä¸“å®¶çš„æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08224">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0ce9395c05b976a91aebde4e3bec7c85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bb998a42975d483a08aa26d2dc35a92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a73b4ff65d0519f7b792e2d55a2305e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-580156970b0d96670ede817ccd990adf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce5ac46480a182d7a80c946aa269693a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-160ae7e01499b34e236ee50f7948eac8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5912ddd6627ade3fc53834262caea54.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-in-Vision-A-Survey"><a href="#Reinforcement-Learning-in-Vision-A-Survey" class="headerlink" title="Reinforcement Learning in Vision: A Survey"></a>Reinforcement Learning in Vision: A Survey</h2><p><strong>Authors:Weijia Wu, Chen Gao, Joya Chen, Kevin Qinghong Lin, Qingwei Meng, Yiming Zhang, Yuke Qiu, Hong Zhou, Mike Zheng Shou</strong></p>
<p>Recent advances at the intersection of reinforcement learning (RL) and visual intelligence have enabled agents that not only perceive complex visual scenes but also reason, generate, and act within them. This survey offers a critical and up-to-date synthesis of the field. We first formalize visual RL problems and trace the evolution of policy-optimization strategies from RLHF to verifiable reward paradigms, and from Proximal Policy Optimization to Group Relative Policy Optimization. We then organize more than 200 representative works into four thematic pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models. For each pillar we examine algorithmic design, reward engineering, benchmark progress, and we distill trends such as curriculum-driven training, preference-aligned diffusion, and unified reward modeling. Finally, we review evaluation protocols spanning set-level fidelity, sample-level preference, and state-level stability, and we identify open challenges that include sample efficiency, generalization, and safe deployment. Our goal is to provide researchers and practitioners with a coherent map of the rapidly expanding landscape of visual RL and to highlight promising directions for future inquiry. Resources are available at: <a target="_blank" rel="noopener" href="https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning">https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œè§†è§‰æ™ºèƒ½äº¤å‰é¢†åŸŸçš„è¿›å±•ä½¿å¾—æ™ºèƒ½ä½“ä¸ä»…èƒ½å¤Ÿæ„ŸçŸ¥å¤æ‚çš„è§†è§‰åœºæ™¯ï¼Œè¿˜èƒ½åœ¨è¿™äº›åœºæ™¯ä¸­è¿›è¡Œæ¨ç†ã€ç”Ÿæˆå’Œè¡ŒåŠ¨ã€‚è¿™ç¯‡ç»¼è¿°å¯¹è¯¥é¢†åŸŸè¿›è¡Œäº†æ‰¹åˆ¤æ€§å’Œæœ€æ–°çš„ç»¼åˆã€‚æˆ‘ä»¬é¦–å…ˆæ­£å¼æå‡ºè§†è§‰RLé—®é¢˜ï¼Œå¹¶è¿½è¸ªä»RLHFåˆ°å¯éªŒè¯å¥–åŠ±èŒƒå¼ï¼Œä»è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–åˆ°ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„ç­–ç•¥ä¼˜åŒ–ç­–ç•¥çš„æ¼”å˜ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¶…è¿‡200ç¯‡å…·æœ‰ä»£è¡¨æ€§çš„ä½œå“æ•´ç†ä¸ºå››ä¸ªä¸»é¢˜æ”¯æŸ±ï¼šå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€è§†è§‰ç”Ÿæˆã€ç»Ÿä¸€æ¨¡å‹æ¡†æ¶å’Œè§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹ã€‚å¯¹äºæ¯ä¸ªä¸»é¢˜æ”¯æŸ±ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç®—æ³•è®¾è®¡ã€å¥–åŠ±å·¥ç¨‹ã€åŸºå‡†è¿›å±•ï¼Œå¹¶æ€»ç»“äº†è¶‹åŠ¿ï¼Œå¦‚è¯¾ç¨‹é©±åŠ¨è®­ç»ƒã€åå¥½å¯¹é½æ‰©æ•£å’Œç»Ÿä¸€å¥–åŠ±å»ºæ¨¡ã€‚æœ€åï¼Œæˆ‘ä»¬å›é¡¾äº†åŒ…æ‹¬é›†åˆçº§ä¿çœŸåº¦ã€æ ·æœ¬çº§åå¥½å’ŒçŠ¶æ€çº§ç¨³å®šæ€§çš„è¯„ä¼°åè®®ï¼Œå¹¶ç¡®å®šäº†å¼€æ”¾æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ ·æœ¬æ•ˆç‡ã€æ¦‚æ‹¬å’Œå®‰å…¨éƒ¨ç½²ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›å¿«é€Ÿæ‰©å±•çš„è§†è§‰RLæ™¯è§‚çš„è¿è´¯åœ°å›¾ï¼Œå¹¶çªå‡ºæœªæ¥æ¢ç©¶çš„æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚èµ„æºå¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learningæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08189v1">PDF</a> 22 pages</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä¸è§†è§‰æ™ºèƒ½çš„äº¤å‰èåˆä¸ºæ™ºèƒ½ä½“æä¾›äº†æ›´é«˜çš„èƒ½åŠ›ï¼Œå®ƒä»¬ä¸ä»…èƒ½æ„ŸçŸ¥å¤æ‚çš„è§†è§‰åœºæ™¯ï¼Œè¿˜èƒ½è¿›è¡Œæ¨ç†ã€ç”Ÿæˆå’Œè¡ŒåŠ¨ã€‚è¿™ç¯‡ç»¼è¿°å¯¹è¯¥é¢†åŸŸè¿›è¡Œäº†æœ€æ–°ã€æœ€é‡è¦çš„æ€»ç»“ï¼Œä»ç­–ç•¥ä¼˜åŒ–åˆ°å››å¤§ä¸»é¢˜æ”¯æŸ±ï¼ˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€è§†è§‰ç”Ÿæˆã€ç»Ÿä¸€æ¨¡å‹æ¡†æ¶å’Œè§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹ï¼‰ï¼Œå¹¶å¯¹è¯¾ç¨‹é©±åŠ¨è®­ç»ƒç­‰è¶‹åŠ¿è¿›è¡Œäº†åˆ†æã€‚æ–‡ç« çš„ç›®çš„æ˜¯ä¸ºç ”ç©¶è€…å’Œå®è·µè€…æä¾›è§†è§‰å¼ºåŒ–å­¦ä¹ çš„æœ€æ–°ç ”ç©¶åœ°å›¾ï¼Œå¹¶æŒ‡æ˜æœªæ¥ç ”ç©¶çš„æœ‰å‰é€”æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä¸è§†è§‰æ™ºèƒ½çš„èåˆè®©æ™ºèƒ½ä½“å…·å¤‡æ›´å¤æ‚çš„èƒ½åŠ›ï¼Œå¦‚æ„ŸçŸ¥ã€æ¨ç†ã€ç”Ÿæˆå’Œè¡ŒåŠ¨ã€‚</li>
<li>ç»¼è¿°æ–‡ç« ç³»ç»Ÿæ€»ç»“äº†è§†è§‰å¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è¿›å±•å’Œé‡è¦æˆæœã€‚</li>
<li>æ–‡ç« è¯¦ç»†æè¿°äº†ä»ç­–ç•¥ä¼˜åŒ–åˆ°å››å¤§ä¸»é¢˜æ”¯æŸ±ï¼ˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç­‰ï¼‰çš„ç ”ç©¶å‘å±•ã€‚</li>
<li>è¶‹åŠ¿åˆ†æåŒ…æ‹¬è¯¾ç¨‹é©±åŠ¨è®­ç»ƒã€åå¥½å¯¹é½æ‰©æ•£å’Œç»Ÿä¸€å¥–åŠ±å»ºæ¨¡ç­‰ã€‚</li>
<li>ç»¼è¿°æ¶µç›–äº†ä¸°å¯Œçš„æ–‡çŒ®å’Œå‰æ²¿ç ”ç©¶æˆæœçš„ç»„ç»‡å’Œèµ„æºé“¾æ¥ã€‚</li>
<li>æ–‡ç« æŒ‡å‡ºäº†å¼€æ”¾æŒ‘æˆ˜ï¼Œå¦‚æ ·æœ¬æ•ˆç‡ã€æ³›åŒ–å’Œå®‰å…¨éƒ¨ç½²ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08189">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4d66785c9fa360f6dbe459f20c592480.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a516597ac992add14c481d36867c359c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afd40acd9948d5a52371a5e494c50ca8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95f89a4e2c0b25c1fade35b3973523da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e6699aee2862070236411b972d3c41f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MedReasoner-Reinforcement-Learning-Drives-Reasoning-Grounding-from-Clinical-Thought-to-Pixel-Level-Precision"><a href="#MedReasoner-Reinforcement-Learning-Drives-Reasoning-Grounding-from-Clinical-Thought-to-Pixel-Level-Precision" class="headerlink" title="MedReasoner: Reinforcement Learning Drives Reasoning Grounding from   Clinical Thought to Pixel-Level Precision"></a>MedReasoner: Reinforcement Learning Drives Reasoning Grounding from   Clinical Thought to Pixel-Level Precision</h2><p><strong>Authors:Zhonghao Yan, Muxi Diao, Yuxuan Yang, Jiayuan Xu, Kaizhou Zhang, Ruoyan Jing, Lele Yang, Yanxi Liu, Kongming Liang, Zhanyu Ma</strong></p>
<p>Accurately grounding regions of interest (ROIs) is critical for diagnosis and treatment planning in medical imaging. While multimodal large language models (MLLMs) combine visual perception with natural language, current medical-grounding pipelines still rely on supervised fine-tuning with explicit spatial hints, making them ill-equipped to handle the implicit queries common in clinical practice. This work makes three core contributions. We first define Unified Medical Reasoning Grounding (UMRG), a novel vision-language task that demands clinical reasoning and pixel-level grounding. Second, we release U-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside implicit clinical queries and reasoning traces, spanning 10 modalities, 15 super-categories, and 108 specific categories. Finally, we introduce MedReasoner, a modular framework that distinctly separates reasoning from segmentation: an MLLM reasoner is optimized with reinforcement learning, while a frozen segmentation expert converts spatial prompts into masks, with alignment achieved through format and accuracy rewards. MedReasoner achieves state-of-the-art performance on U-MRG-14K and demonstrates strong generalization to unseen clinical queries, underscoring the significant promise of reinforcement learning for interpretable medical grounding. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒä¸­ï¼Œå‡†ç¡®å®šä½æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰å¯¹äºè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’çš„åˆ¶å®šè‡³å…³é‡è¦ã€‚è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç»“åˆäº†è§†è§‰æ„ŸçŸ¥ä¸è‡ªç„¶è¯­è¨€ï¼Œä½†å½“å‰çš„åŒ»å­¦å®šä½æµç¨‹ä»ç„¶ä¾èµ–äºå¸¦æœ‰æ˜ç¡®ç©ºé—´æç¤ºçš„ç›‘ç£å¾®è°ƒï¼Œè¿™ä½¿å¾—å®ƒä»¬éš¾ä»¥åº”å¯¹ä¸´åºŠå®è·µä¸­çš„éšå¼æŸ¥è¯¢ã€‚æœ¬å·¥ä½œåšå‡ºäº†ä¸‰ä¸ªæ ¸å¿ƒè´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰äº†ç»Ÿä¸€åŒ»å­¦æ¨ç†å®šä½ï¼ˆUMRGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€ä»»åŠ¡ï¼Œéœ€è¦ä¸´åºŠæ¨ç†å’Œåƒç´ çº§å®šä½ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å‘å¸ƒäº†U-MRG-14Kæ•°æ®é›†ï¼ŒåŒ…å«14Kä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½æœ‰åƒç´ çº§æ©è†œã€éšå¼ä¸´åºŠæŸ¥è¯¢å’Œæ¨ç†è½¨è¿¹ï¼Œæ¶µç›–10ç§æ¨¡æ€ã€15ä¸ªè¶…ç±»åˆ«å’Œ108ä¸ªç‰¹å®šç±»åˆ«ã€‚æœ€åï¼Œæˆ‘ä»¬æ¨å‡ºäº†MedReasonerï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œå°†æ¨ç†ä¸åˆ†å‰²æ˜ç¡®åŒºåˆ†å¼€æ¥ï¼šMLLMæ¨ç†å™¨é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ï¼Œè€Œå†»ç»“çš„åˆ†å‰²ä¸“å®¶å°†ç©ºé—´æç¤ºè½¬æ¢ä¸ºæ©è†œï¼Œé€šè¿‡æ ¼å¼å’Œç²¾åº¦å¥–åŠ±å®ç°å¯¹é½ã€‚MedReasoneråœ¨U-MRG-14Kä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å¯¹æœªè§è¿‡çš„ä¸´åºŠæŸ¥è¯¢è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¿™çªæ˜¾äº†å¼ºåŒ–å­¦ä¹ åœ¨å¯è§£é‡Šçš„åŒ»å­¦å®šä½ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08177v1">PDF</a> 37 pages</p>
<p><strong>Summary</strong><br>åŒ»å­¦æˆåƒä¸­çš„å‡†ç¡®åŒºåŸŸå®šä½ï¼ˆROIsï¼‰å¯¹äºè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚å½“å‰ï¼ŒåŒ»ç–—å®šä½æµç¨‹ä»ç„¶ä¾èµ–äºå¸¦æœ‰æ˜ç¡®ç©ºé—´æç¤ºçš„ç›‘ç£å¾®è°ƒï¼Œæ— æ³•å¤„ç†ä¸´åºŠå®è·µä¸­çš„éšå«æŸ¥è¯¢ã€‚æœ¬æ–‡å®šä¹‰äº†ç»Ÿä¸€åŒ»å­¦æ¨ç†å®šä½ï¼ˆUMRGï¼‰è¿™ä¸€æ–°é¢–çš„è§†è§‰è¯­è¨€ä»»åŠ¡ï¼Œå¹¶å‘å¸ƒU-MRG-14Kæ•°æ®é›†ï¼ŒåŒ…å«è·¨è¶Šåç§æ¨¡æ€ã€åäº”ä¸ªè¶…çº§ç±»åˆ«å’Œä¸€ç™¾é›¶å…«ç§ç‰¹å®šç±»åˆ«çš„éšæ€§ä¸´åºŠæŸ¥è¯¢å’Œæ¨ç†ç—•è¿¹çš„ä¸€ä¸‡å››åƒä¸ªæ ·æœ¬ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†MedReasoneræ¨¡å—åŒ–æ¡†æ¶ï¼Œå°†æ¨ç†ä¸åˆ†å‰²åˆ†ç¦»ï¼Œä¼˜åŒ–äº†å¼ºåŒ–å­¦ä¹ ä¸‹çš„MLLMæ¨ç†å™¨ï¼Œå°†ç©ºé—´æç¤ºè½¬æ¢ä¸ºæ©è†œã€‚MedReasoneråœ¨U-MRG-14Kä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨æœªè§è¿‡çš„ä¸´åºŠæŸ¥è¯¢ä¸Šå±•ç¤ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å±•ç°å…¶åœ¨å¼ºåŒ–å­¦ä¹ ä¸Šçš„å·¨å¤§æ½œåŠ›è¯æ˜äº†æŠ€æœ¯é‡è¦æ€§ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒºåŸŸå®šä½ï¼ˆROIsï¼‰åœ¨åŒ»å­¦æˆåƒä¸­çš„è¯Šæ–­ä¸æ²»ç–—çš„è§’è‰²æ˜¯æ ¸å¿ƒçš„ã€‚</li>
<li>å½“å‰åŒ»ç–—å®šä½æµç¨‹ä¾èµ–ç›‘ç£å¾®è°ƒåŠæ˜ç¡®çš„ç©ºé—´æç¤ºï¼Œæ— æ³•é€‚åº”ä¸´åºŠå®è·µä¸­çš„éšå«æŸ¥è¯¢éœ€æ±‚ã€‚</li>
<li>ç»Ÿä¸€åŒ»å­¦æ¨ç†å®šä½ï¼ˆUMRGï¼‰å®šä¹‰äº†ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€ä»»åŠ¡ï¼Œè¦æ±‚ä¸´åºŠæ¨ç†å’Œåƒç´ çº§å®šä½ã€‚</li>
<li>U-MRG-14Kæ•°æ®é›†åŒ…å«å¤§é‡æ ·æœ¬ï¼Œæ¶µç›–å¤šç§æ¨¡æ€ã€ç±»åˆ«å’Œä¸´åºŠæŸ¥è¯¢åŠæ¨ç†ç—•è¿¹ã€‚</li>
<li>MedReasoneræ¨¡å—åŒ–æ¡†æ¶å®ç°äº†æ¨ç†ä¸åˆ†å‰²çš„åˆ†ç¦»ï¼Œå¹¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–MLLMæ¨ç†å™¨æ€§èƒ½ã€‚</li>
<li>MedReasoneråœ¨U-MRG-14Kæ•°æ®é›†ä¸Šè¡¨ç°å“è¶Šï¼Œå¹¶åœ¨æœªè§è¿‡çš„ä¸´åºŠæŸ¥è¯¢ä¸Šå±•ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08177">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1f06984455fc0a851fbffcb0d6207f70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a378eeee81f6ec3a4240577c60bd85dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27209ab38731dc4fccd7894dcb778d1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f1f90c911d27ca95264e0ad8d6239bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee783866ffd89bcf426bcca3b2ed5aec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11a3bb59d06b2e899f246a04a58dad60.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="REX-RAG-Reasoning-Exploration-with-Policy-Correction-in-Retrieval-Augmented-Generation"><a href="#REX-RAG-Reasoning-Exploration-with-Policy-Correction-in-Retrieval-Augmented-Generation" class="headerlink" title="REX-RAG: Reasoning Exploration with Policy Correction in   Retrieval-Augmented Generation"></a>REX-RAG: Reasoning Exploration with Policy Correction in   Retrieval-Augmented Generation</h2><p><strong>Authors:Wentao Jiang, Xiang Feng, Zengmao Wang, Yong Luo, Pingbo Xu, Zhe Chen, Bo Du, Jing Zhang</strong></p>
<p>Reinforcement learning (RL) is emerging as a powerful paradigm for enabling large language models (LLMs) to perform complex reasoning tasks. Recent advances indicate that integrating RL with retrieval-augmented generation (RAG) allows LLMs to dynamically incorporate external knowledge, leading to more informed and robust decision making. However, we identify a critical challenge during policy-driven trajectory sampling: LLMs are frequently trapped in unproductive reasoning paths, which we refer to as â€œdead endsâ€, committing to overconfident yet incorrect conclusions. This severely hampers exploration and undermines effective policy optimization. To address this challenge, we propose REX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation), a novel framework that explores alternative reasoning paths while maintaining rigorous policy learning through principled distributional corrections. Our approach introduces two key innovations: (1) Mixed Sampling Strategy, which combines a novel probe sampling method with exploratory prompts to escape dead ends; and (2) Policy Correction Mechanism, which employs importance sampling to correct distribution shifts induced by mixed sampling, thereby mitigating gradient estimation bias. We evaluate it on seven question-answering benchmarks, and the experimental results show that REX-RAG achieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B over strong baselines, demonstrating competitive results across multiple datasets. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/MiliLab/REX-RAG">https://github.com/MiliLab/REX-RAG</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ­£æˆä¸ºä¸€ç§å¼ºå¤§çš„èŒƒå¼ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿæ‰§è¡Œå¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚æœ€è¿‘çš„è¿›å±•è¡¨æ˜ï¼Œå°†RLä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç›¸ç»“åˆï¼Œå¯ä»¥ä½¿LLMåŠ¨æ€åœ°èå…¥å¤–éƒ¨çŸ¥è¯†ï¼Œä»è€Œå®ç°æ›´åŠ æ˜æ™ºå’Œç¨³å¥çš„å†³ç­–ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬åœ¨ç­–ç•¥é©±åŠ¨çš„è½¨è¿¹é‡‡æ ·è¿‡ç¨‹ä¸­å‘ç°äº†ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼šLLMç»å¸¸é™·å…¥æ— ç»“æœçš„æ¨ç†è·¯å¾„ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæ­»èƒ¡åŒâ€ï¼Œå¯¼è‡´è¿‡äºè‡ªä¿¡çš„é”™è¯¯ç»“è®ºã€‚è¿™ä¸¥é‡é˜»ç¢äº†æ¢ç´¢å¹¶ç ´åäº†æœ‰æ•ˆçš„ç­–ç•¥ä¼˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†REX-RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆä¸­çš„å¸¦ç­–ç•¥ä¿®æ­£çš„æ¨ç†æ¢ç´¢ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåœ¨ä¿æŒä¸¥è°¨çš„ç­–ç•¥å­¦ä¹ çš„æƒ…å†µä¸‹ï¼Œæ¢ç´¢æ›¿ä»£çš„æ¨ç†è·¯å¾„ï¼Œå¹¶é€šè¿‡æœ‰åŸåˆ™çš„åˆ†å¸ƒä¿®æ­£æ¥è¿›è¡Œç­–ç•¥ä¿®æ­£ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸¤ä¸ªå…³é”®çš„åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰æ··åˆé‡‡æ ·ç­–ç•¥ï¼Œå®ƒå°†ä¸€ç§æ–°çš„æ¢é’ˆé‡‡æ ·æ–¹æ³•ä¸æ¢ç´¢æ€§æç¤ºç›¸ç»“åˆï¼Œä»¥é€ƒç¦»æ­»èƒ¡åŒï¼›ï¼ˆ2ï¼‰ç­–ç•¥ä¿®æ­£æœºåˆ¶ï¼Œå®ƒé‡‡ç”¨é‡è¦æ€§é‡‡æ ·æ¥çº æ­£æ··åˆé‡‡æ ·å¼•èµ·çš„åˆ†å¸ƒåç§»ï¼Œä»è€Œå‡è½»æ¢¯åº¦ä¼°è®¡åå·®ã€‚æˆ‘ä»¬åœ¨ä¸ƒä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸Šå¯¹å…¶å®éªŒè¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨Qwen2.5-3Bä¸ŠREX-RAGå¹³å‡æ€§èƒ½æå‡5.1%ï¼Œåœ¨Qwen2.5-7Bä¸Šæ€§èƒ½æå‡3.6%ï¼Œç›¸è¾ƒäºå¼ºå¤§çš„åŸºçº¿æ¨¡å‹è¡¨ç°å‡ºç«äº‰åŠ›ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/MiliLab/REX-RAG">https://github.com/MiliLab/REX-RAG</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08149v1">PDF</a> 17 pages, 4 figures</p>
<p><strong>Summary</strong>ï¼šå¼ºåŒ–å­¦ä¹ æ­£åœ¨æˆä¸ºä½¿å¤§å‹è¯­è¨€æ¨¡å‹æ‰§è¡Œå¤æ‚æ¨ç†ä»»åŠ¡çš„ä¸€ç§å¼ºå¤§èŒƒå¼ã€‚å°†å¼ºåŒ–å­¦ä¹ ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆç›¸ç»“åˆï¼Œå¯ä»¥åŠ¨æ€åœ°ä½¿è¯­è¨€æ¨¡å‹èå…¥å¤–éƒ¨çŸ¥è¯†ï¼Œä»è€Œå®ç°æ›´åŠ æ˜æ™ºå’Œç¨³å¥çš„å†³ç­–ã€‚ç„¶è€Œï¼Œç­–ç•¥é©±åŠ¨çš„è½¨è¿¹é‡‡æ ·é¢ä¸´ä¸€ä¸ªæŒ‘æˆ˜ï¼šè¯­è¨€æ¨¡å‹ç»å¸¸é™·å…¥ä¸äº§ç”Ÿæ•ˆç›Šçš„æ¨ç†è·¯å¾„ï¼Œå³â€œæ­»èƒ¡åŒâ€ï¼Œå¯¼è‡´è¿‡åº¦è‡ªä¿¡çš„é”™è¯¯ç»“è®ºã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†REX-RAGæ¡†æ¶ï¼Œé€šè¿‡æ··åˆé‡‡æ ·ç­–ç•¥å’Œç­–ç•¥æ ¡æ­£æœºåˆ¶ï¼Œåœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆä¸­è¿›è¡Œæ¨ç†æ¢ç´¢ï¼ŒåŒæ—¶ä¿æŒä¸¥è°¨çš„ç­–ç•¥å­¦ä¹ ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ æ­£åœ¨æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æ‰§è¡Œå¤æ‚æ¨ç†ä»»åŠ¡çš„é‡è¦å·¥å…·ã€‚</li>
<li>æ•´åˆå¼ºåŒ–å­¦ä¹ ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œèƒ½è®©è¯­è¨€æ¨¡å‹åŠ¨æ€èå…¥å¤–éƒ¨çŸ¥è¯†ï¼Œæé«˜å†³ç­–è´¨é‡ã€‚</li>
<li>ç­–ç•¥é©±åŠ¨çš„è½¨è¿¹é‡‡æ ·åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å­˜åœ¨æŒ‘æˆ˜ï¼Œè¯­è¨€æ¨¡å‹æ˜“é™·å…¥ä¸äº§ç”Ÿæ•ˆç›Šçš„æ¨ç†è·¯å¾„ã€‚</li>
<li>REX-RAGæ¡†æ¶é€šè¿‡æ··åˆé‡‡æ ·ç­–ç•¥å’Œç­–ç•¥æ ¡æ­£æœºåˆ¶è§£å†³æ­¤é—®é¢˜ï¼Œå®ç°æ¨ç†æ¢ç´¢ä¸ä¸¥è°¨çš„ç­–ç•¥å­¦ä¹ ã€‚</li>
<li>REX-RAGçš„æ··åˆé‡‡æ ·ç­–ç•¥ç»“åˆäº†æ–°å‹æ¢é’ˆé‡‡æ ·æ–¹æ³•å’Œæ¢ç´¢æ€§æç¤ºï¼Œæœ‰åŠ©äºé€ƒç¦»â€œæ­»èƒ¡åŒâ€ã€‚</li>
<li>REX-RAGçš„ç­–ç•¥æ ¡æ­£æœºåˆ¶é‡‡ç”¨é‡è¦æ€§é‡‡æ ·ï¼Œçº æ­£ç”±æ··åˆé‡‡æ ·å¼•èµ·çš„åˆ†å¸ƒåç§»ï¼Œå‡è½»æ¢¯åº¦ä¼°è®¡åå·®ã€‚</li>
<li>åœ¨ä¸ƒä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒREX-RAGç›¸è¾ƒäºå¼ºå¤§çš„åŸºå‡†æ¨¡å‹å–å¾—äº†å¹³å‡æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08149">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cfa98f649e8488f071ed73149da553de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-925c386281592c204daba0d2804ce46d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-549fc36f7a7c6ac274135a62634286c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7f705e40df0d0e2f2145460fd805f54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eda33d128ca0a01c8ffa82a0b9b7edec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-362fcfbb60b1eb85011d6e0922cda71c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="HierSearch-A-Hierarchical-Enterprise-Deep-Search-Framework-Integrating-Local-and-Web-Searches"><a href="#HierSearch-A-Hierarchical-Enterprise-Deep-Search-Framework-Integrating-Local-and-Web-Searches" class="headerlink" title="HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating   Local and Web Searches"></a>HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating   Local and Web Searches</h2><p><strong>Authors:Jiejun Tan, Zhicheng Dou, Yan Yu, Jiehan Cheng, Qiang Ju, Jian Xie, Ji-Rong Wen</strong></p>
<p>Recently, large reasoning models have demonstrated strong mathematical and coding abilities, and deep search leverages their reasoning capabilities in challenging information retrieval tasks. Existing deep search works are generally limited to a single knowledge source, either local or the Web. However, enterprises often require private deep search systems that can leverage search tools over both local and the Web corpus. Simply training an agent equipped with multiple search tools using flat reinforcement learning (RL) is a straightforward idea, but it has problems such as low training data efficiency and poor mastery of complex tools. To address the above issue, we propose a hierarchical agentic deep search framework, HierSearch, trained with hierarchical RL. At the low level, a local deep search agent and a Web deep search agent are trained to retrieve evidence from their corresponding domains. At the high level, a planner agent coordinates low-level agents and provides the final answer. Moreover, to prevent direct answer copying and error propagation, we design a knowledge refiner that filters out hallucinations and irrelevant evidence returned by low-level agents. Experiments show that HierSearch achieves better performance compared to flat RL, and outperforms various deep search and multi-source retrieval-augmented generation baselines in six benchmarks across general, finance, and medical domains. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹æ¨ç†æ¨¡å‹å±•ç¤ºäº†å¼ºå¤§çš„æ•°å­¦å’Œç¼–ç èƒ½åŠ›ï¼Œæ·±åº¦æœç´¢åˆ™åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­åˆ©ç”¨äº†è¿™äº›æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰çš„æ·±åº¦æœç´¢å·¥ä½œé€šå¸¸å±€é™äºå•ä¸ªçŸ¥è¯†æºï¼Œæ— è®ºæ˜¯æœ¬åœ°è¿˜æ˜¯ç½‘ç»œã€‚ç„¶è€Œï¼Œä¼ä¸šé€šå¸¸éœ€è¦èƒ½å¤Ÿåœ¨æœ¬åœ°å’Œç½‘ç»œè¯­æ–™åº“ä¸Šä½¿ç”¨æœç´¢å·¥å…·çš„ç§äººæ·±åº¦æœç´¢ç³»ç»Ÿã€‚ä½¿ç”¨å¹³é¢å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒé…å¤‡å¤šç§æœç´¢å·¥å…·çš„æ™ºèƒ½ä»£ç†æ˜¯ä¸€ä¸ªç›´æ¥çš„æƒ³æ³•ï¼Œä½†å®ƒå­˜åœ¨è®­ç»ƒæ•°æ®æ•ˆç‡ä½å’Œéš¾ä»¥æŒæ¡å¤æ‚å·¥å…·ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿ç”¨åˆ†å±‚å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„åˆ†å±‚ä»£ç†æ·±åº¦æœç´¢æ¡†æ¶HierSearchã€‚åœ¨ä½çº§å±‚é¢ï¼Œæœ¬åœ°æ·±åº¦æœç´¢ä»£ç†å’Œç½‘ç»œæ·±åº¦æœç´¢ä»£ç†è¢«è®­ç»ƒä»å„è‡ªå¯¹åº”çš„é¢†åŸŸæ£€ç´¢è¯æ®ã€‚åœ¨é«˜çº§å±‚é¢ï¼Œè§„åˆ’ä»£ç†åè°ƒä½çº§ä»£ç†å¹¶æä¾›æœ€ç»ˆç­”æ¡ˆã€‚æ­¤å¤–ï¼Œä¸ºäº†é˜²æ­¢ç›´æ¥ç­”æ¡ˆå¤åˆ¶å’Œé”™è¯¯ä¼ æ’­ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªçŸ¥è¯†ç²¾ç‚¼å™¨ï¼Œå¯ä»¥è¿‡æ»¤æ‰ä½çº§ä»£ç†äº§ç”Ÿçš„å¹»è§‰å’Œæ— å…³è¯æ®ã€‚å®éªŒè¡¨æ˜ï¼ŒHierSearchåœ¨æ€§èƒ½ä¸Šä¼˜äºå¹³é¢RLï¼Œå¹¶ä¸”åœ¨é€šç”¨ã€é‡‘èå’ŒåŒ»ç–—é¢†åŸŸçš„å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å„ç§æ·±åº¦æœç´¢å’Œå¤šæºæ£€ç´¢å¢å¼ºç”ŸæˆåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08088v1">PDF</a> Code and datasets are available at   <a target="_blank" rel="noopener" href="https://github.com/plageon/HierSearch">https://github.com/plageon/HierSearch</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ•°å­¦ä¸ç¼–ç èƒ½åŠ›ï¼Œå¹¶åœ¨ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­åˆ©ç”¨è¿™äº›èƒ½åŠ›è¿›è¡Œæ·±åº¦æœç´¢ã€‚ç„¶è€Œï¼Œç°æœ‰æ·±åº¦æœç´¢å·¥ä½œä¸»è¦å±€é™äºå•ä¸€çŸ¥è¯†æºï¼Œéš¾ä»¥åŒæ—¶è¿ç”¨æœ¬åœ°åŠç½‘ç»œæ•°æ®è¿›è¡Œæœç´¢ã€‚ä¸ºè§£å†³ä¼ä¸šéœ€æ±‚ä¸­çš„ç§äººæ·±åº¦æœç´¢ç³»ç»Ÿé—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºä½¿ç”¨å±‚æ¬¡åŒ–çš„æ™ºèƒ½æ·±åº¦æœç´¢æ¡†æ¶â€”â€”HierSearchï¼Œé€šè¿‡å±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒã€‚è¯¥æ¡†æ¶åœ¨åº•å±‚è®­ç»ƒæœ¬åœ°å’Œç½‘ç»œæ·±åº¦æœç´¢ä»£ç†ï¼Œä»¥ä»ç›¸åº”é¢†åŸŸæ£€ç´¢è¯æ®ï¼›åœ¨é«˜å±‚è®¾ç½®è§„åˆ’ä»£ç†è¿›è¡Œåè°ƒå¹¶ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªçŸ¥è¯†ç²¾ç‚¼å™¨è¿‡æ»¤æ‰åº•å±‚ä»£ç†äº§ç”Ÿçš„å¹»è§‰å’Œæ— å…³è¯æ®ã€‚å®éªŒè¡¨æ˜ï¼ŒHierSearchç›¸è¾ƒäºå¹³é¢å¼ºåŒ–å­¦ä¹ æœ‰æ›´å¥½çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶åœ¨é€šç”¨ã€é‡‘èå’ŒåŒ»å­¦é¢†åŸŸçš„å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å„ç§æ·±åº¦æœç´¢å’Œå¤šæºæ£€ç´¢å¢å¼ºç”ŸæˆåŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹å…·å¤‡æ•°å­¦ä¸ç¼–ç èƒ½åŠ›ï¼Œå¹¶åœ¨ä¿¡æ¯æ£€ç´¢ä¸­å±•ç°æ·±åº¦æœç´¢èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ·±åº¦æœç´¢ä¸»è¦ä¾èµ–å•ä¸€çŸ¥è¯†æºï¼Œæ— æ³•æ»¡è¶³ä¼ä¸šå¯¹æœ¬åœ°å’Œç½‘ç»œæ•°æ®åŒæ—¶è¿ç”¨çš„éœ€æ±‚ã€‚</li>
<li>HierSearchæ¡†æ¶ä½¿ç”¨å±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ è®­ç»ƒä»£ç†ï¼Œå®ç°æœ¬åœ°å’Œç½‘ç»œæ·±åº¦æœç´¢ã€‚</li>
<li>HierSearchåœ¨åº•å±‚è®­ç»ƒä»£ç†æ£€ç´¢è¯æ®ï¼Œé«˜å±‚è§„åˆ’ä»£ç†è¿›è¡Œåè°ƒå¹¶æä¾›ç­”æ¡ˆã€‚</li>
<li>çŸ¥è¯†ç²¾ç‚¼å™¨ç”¨äºè¿‡æ»¤åº•å±‚ä»£ç†äº§ç”Ÿçš„å¹»è§‰å’Œæ— å…³è¯æ®ã€‚</li>
<li>HierSearchç›¸è¾ƒäºå¹³é¢å¼ºåŒ–å­¦ä¹ æ€§èƒ½æ›´ä¼˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08088">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-dd5a735d80ff24cfd98b7b5e8a5e74b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7d637486c148c9aecf5e7d2ea9850dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de51fd03b24b2515ba8e45cb265d12bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42a14850a1edea3b2345afd65a825ce5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-877785939347d6ae2cebd382974d74e1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AdaptFlow-Adaptive-Workflow-Optimization-via-Meta-Learning"><a href="#AdaptFlow-Adaptive-Workflow-Optimization-via-Meta-Learning" class="headerlink" title="AdaptFlow: Adaptive Workflow Optimization via Meta-Learning"></a>AdaptFlow: Adaptive Workflow Optimization via Meta-Learning</h2><p><strong>Authors:Runchuan Zhu, Bowen Jiang, Lingrui Mei, Fangkai Yang, Lu Wang, Haoxiang Gao, Fengshuo Bai, Pu Zhao, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang</strong></p>
<p>Recent advances in large language models (LLMs) have sparked growing interest in agentic workflows, which are structured sequences of LLM invocations intended to solve complex tasks. However, existing approaches often rely on static templates or manually designed workflows, which limit adaptability to diverse tasks and hinder scalability. We propose AdaptFlow, a natural language-based meta-learning framework inspired by model-agnostic meta-learning (MAML). AdaptFlow learns a generalizable workflow initialization that enables rapid subtask-level adaptation. It employs a bi-level optimization scheme: the inner loop refines the workflow for a specific subtask using LLM-generated feedback, while the outer loop updates the shared initialization to perform well across tasks. This setup allows AdaptFlow to generalize effectively to unseen tasks by adapting the initialized workflow through language-guided modifications. Evaluated across question answering, code generation, and mathematical reasoning benchmarks, AdaptFlow consistently outperforms both manually crafted and automatically searched baselines, achieving state-of-the-art results with strong generalization across tasks and models. The source code and data are available at <a target="_blank" rel="noopener" href="https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow">https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•å¼•å‘äº†äººä»¬å¯¹ä»£ç†å·¥ä½œæµçš„æ—¥ç›Šå…³æ³¨ï¼Œä»£ç†å·¥ä½œæµæ˜¯ç”±ä¸€ç³»åˆ—LLMè°ƒç”¨ç»„æˆçš„ç»“æ„åŒ–åºåˆ—ï¼Œæ—¨åœ¨è§£å†³å¤æ‚ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºé™æ€æ¨¡æ¿æˆ–æ‰‹åŠ¨è®¾è®¡çš„å·¥ä½œæµï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å¯¹ä¸åŒä»»åŠ¡çš„é€‚åº”æ€§ï¼Œå¹¶é˜»ç¢äº†å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬æå‡ºäº†AdaptFlowï¼Œè¿™æ˜¯ä¸€ä¸ªå—æ¨¡å‹æ— å…³å…ƒå­¦ä¹ ï¼ˆMAMLï¼‰å¯å‘çš„åŸºäºè‡ªç„¶è¯­è¨€å…ƒå­¦ä¹ æ¡†æ¶ã€‚AdaptFlowå­¦ä¹ ä¸€ç§å¯é€šç”¨çš„å·¥ä½œæµåˆå§‹åŒ–æ–¹æ³•ï¼Œä»¥åŠ å¿«ç‰¹å®šå­ä»»åŠ¡çš„é€‚åº”æ€§ã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§ä¸¤çº§ä¼˜åŒ–æ–¹æ¡ˆï¼šå†…å¾ªç¯åˆ©ç”¨LLMç”Ÿæˆçš„åé¦ˆå¯¹ç‰¹å®šå­ä»»åŠ¡çš„å·¥ä½œæµç¨‹è¿›è¡Œç²¾ç»†åŒ–æ”¹è¿›ï¼Œè€Œå¤–å¾ªç¯åˆ™æ›´æ–°å…±äº«åˆå§‹åŒ–è®¾ç½®ä»¥åœ¨ä»»åŠ¡ä¹‹é—´è¡¨ç°è‰¯å¥½ã€‚è¿™ç§è®¾ç½®å…è®¸AdaptFlowé€šè¿‡è¯­è¨€æŒ‡å¯¼çš„ä¿®æ”¹æ¥é€‚åº”åˆå§‹åŒ–å·¥ä½œæµï¼Œä»è€Œæœ‰æ•ˆåœ°æ¨å¹¿åˆ°æœªè§è¿‡çš„ä»»åŠ¡ã€‚åœ¨é—®ç­”ã€ä»£ç ç”Ÿæˆå’Œæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒAdaptFlowå§‹ç»ˆè¡¨ç°å‡ºè‰²ï¼Œä¸ä»…åœ¨æ‰‹åŠ¨è®¾è®¡å’Œè‡ªåŠ¨æœç´¢çš„åŸºå‡†çº¿ä¸Šæœ‰æ‰€è¶…è¶Šï¼Œè€Œä¸”åœ¨ä»»åŠ¡å’Œæ¨¡å‹ä¹‹é—´å®ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æºä»£ç å’Œæ•°æ®åœ¨<a target="_blank" rel="noopener" href="https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow%E4%B8%8A%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlowä¸Šå¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08053v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€è¿‘è¿›å±•å¼•å‘äº†äººä»¬å¯¹ä»£ç†å·¥ä½œæµï¼ˆagentic workflowsï¼‰çš„æµ“åšå…´è¶£ã€‚å·¥ä½œæµæ˜¯ä¸€ç³»åˆ—LLMè°ƒç”¨çš„ç»“æ„åŒ–åºåˆ—ï¼Œæ—¨åœ¨è§£å†³å¤æ‚ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºé™æ€æ¨¡æ¿æˆ–æ‰‹åŠ¨è®¾è®¡çš„å·¥ä½œæµï¼Œè¿™é™åˆ¶äº†å…¶å¯¹å„ç§ä»»åŠ¡çš„é€‚åº”èƒ½åŠ›å¹¶é˜»ç¢å…¶å¯æ‰©å±•æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†AdaptFlowï¼Œä¸€ä¸ªå—æ¨¡å‹ä¸å¯çŸ¥å…ƒå­¦ä¹ ï¼ˆMAMLï¼‰å¯å‘çš„è‡ªç„¶è¯­è¨€é©±åŠ¨çš„å…ƒå­¦ä¹ æ¡†æ¶ã€‚AdaptFlowå­¦ä¹ ä¸€ç§å¯é€šç”¨çš„å·¥ä½œæµåˆå§‹åŒ–æ–¹æ¡ˆï¼Œä»¥å®ç°å¿«é€Ÿå­ä»»åŠ¡çº§åˆ«çš„é€‚åº”ã€‚å®ƒé‡‡ç”¨åŒå±‚ä¼˜åŒ–æ–¹æ¡ˆï¼šå†…å¾ªç¯é’ˆå¯¹ç‰¹å®šå­ä»»åŠ¡ä¼˜åŒ–å·¥ä½œæµå¹¶ä½¿ç”¨LLMç”Ÿæˆçš„åé¦ˆè¿›è¡Œè°ƒæ•´ï¼Œè€Œå¤–å¾ªç¯æ›´æ–°å…±äº«åˆå§‹åŒ–æ–¹æ¡ˆä»¥å®ç°è·¨ä»»åŠ¡çš„è‰¯å¥½è¡¨ç°ã€‚è¿™ä½¿å¾—AdaptFlowèƒ½å¤Ÿæœ‰æ•ˆåœ°é€‚åº”æœªè§è¿‡çš„ä»»åŠ¡ï¼Œå¹¶é€šè¿‡è¯­è¨€æŒ‡å¯¼çš„ä¿®æ”¹æ¥è°ƒæ•´åˆå§‹åŒ–å·¥ä½œæµã€‚åœ¨é—®ç­”ã€ä»£ç ç”Ÿæˆå’Œæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼ŒAdaptFlowåœ¨æ‰‹åŠ¨è®¾è®¡å’Œè‡ªåŠ¨æœç´¢çš„åŸºå‡†æµ‹è¯•ä¸­éƒ½è¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†è·¨ä»»åŠ¡å’Œæ¨¡å‹çš„æœ€ä½³ç»“æœã€‚å…¶æºä»£ç å’Œæ•°æ®åœ¨ <a target="_blank" rel="noopener" href="https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow">https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow</a> ä¸Šå¯ä¾›æŸ¥é˜…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•æ¨åŠ¨äº†ä»£ç†å·¥ä½œæµï¼ˆagentic workflowsï¼‰çš„ç ”ç©¶ã€‚</li>
<li>ç°æœ‰å·¥ä½œæµæ–¹æ³•ä¸»è¦ä¾èµ–é™æ€æ¨¡æ¿æˆ–æ‰‹åŠ¨è®¾è®¡ï¼Œé™åˆ¶äº†å…¶åœ¨å¤šæ ·ä»»åŠ¡ä¸­çš„é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>AdaptFlowæ˜¯ä¸€ä¸ªè‡ªç„¶è¯­è¨€é©±åŠ¨çš„å…ƒå­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é™åˆ¶ã€‚</li>
<li>AdaptFlowé€šè¿‡åŒå±‚ä¼˜åŒ–æ–¹æ¡ˆå­¦ä¹ é€šç”¨çš„å·¥ä½œæµåˆå§‹åŒ–ï¼Œä»¥å®ç°å¿«é€Ÿå­ä»»åŠ¡çº§åˆ«çš„é€‚åº”ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†å†…å¾ªç¯çš„ç‰¹å®šå­ä»»åŠ¡ä¼˜åŒ–å’Œå¤–å¾ªç¯çš„è·¨ä»»åŠ¡è¡¨ç°ä¼˜åŒ–ã€‚</li>
<li>AdaptFlowèƒ½å¤Ÿé€‚åº”æœªè§è¿‡çš„ä»»åŠ¡ï¼Œå¹¶é€šè¿‡è¯­è¨€æŒ‡å¯¼ä¿®æ”¹åˆå§‹åŒ–å·¥ä½œæµã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08053">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fdc1617189d86e78a3caaf818953b5ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13007b66af4cf1674b8462743c81a552.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e30785803f66dc7fbd257a02e8b586f7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5070a0040e1cac5a49bb2ec05de162a9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Audio-Thinker-Guiding-Audio-Language-Model-When-and-How-to-Think-via-Reinforcement-Learning"><a href="#Audio-Thinker-Guiding-Audio-Language-Model-When-and-How-to-Think-via-Reinforcement-Learning" class="headerlink" title="Audio-Thinker: Guiding Audio Language Model When and How to Think via   Reinforcement Learning"></a>Audio-Thinker: Guiding Audio Language Model When and How to Think via   Reinforcement Learning</h2><p><strong>Authors:Shu Wu, Chenxing Li, Wenfu Wang, Hao Zhang, Hualei Wang, Meng Yu, Dong Yu</strong></p>
<p>Recent advancements in large language models, multimodal large language models, and large audio language models (LALMs) have significantly improved their reasoning capabilities through reinforcement learning with rule-based rewards. However, the explicit reasoning process has yet to show significant benefits for audio question answering, and effectively leveraging deep reasoning remains an open challenge, with LALMs still falling short of human-level auditory-language reasoning. To address these limitations, we propose Audio-Thinker, a reinforcement learning framework designed to enhance the reasoning capabilities of LALMs, with a focus on improving adaptability, consistency, and effectiveness. Our approach introduces an adaptive think accuracy reward, enabling the model to adjust its reasoning strategies based on task complexity dynamically. Furthermore, we incorporate an external reward model to evaluate the overall consistency and quality of the reasoning process, complemented by think-based rewards that help the model distinguish between valid and flawed reasoning paths during training. Experimental results demonstrate that our Audio-Thinker model outperforms existing reasoning-oriented LALMs across various benchmark tasks, exhibiting superior reasoning and generalization capabilities. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ã€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„è¿›æ­¥ï¼Œé€šè¿‡åŸºäºè§„åˆ™çš„å¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œæ˜¾è‘—æå‡äº†å…¶æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ˜ç¡®çš„æ¨ç†è¿‡ç¨‹åœ¨éŸ³é¢‘é—®ç­”ä¸­å°šæœªæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œæœ‰æ•ˆåˆ©ç”¨æ·±åº¦æ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ï¼ŒLALMåœ¨éŸ³é¢‘è¯­è¨€æ¨ç†æ–¹é¢ä»æœªèƒ½è¾¾åˆ°äººç±»æ°´å¹³ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†Audio-Thinkerï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¢å¼ºLALMæ¨ç†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé‡ç‚¹å…³æ³¨é€‚åº”æ€§ã€ä¸€è‡´æ€§å’Œæœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”çš„æ€è€ƒå‡†ç¡®æ€§å¥–åŠ±ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŸºäºä»»åŠ¡çš„å¤æ‚æ€§åŠ¨æ€åœ°è°ƒæ•´å…¶æ¨ç†ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤–éƒ¨å¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°æ¨ç†è¿‡ç¨‹çš„æ•´ä½“ä¸€è‡´æ€§å’Œè´¨é‡ï¼Œè¾…ä»¥åŸºäºæ€è€ƒçš„å¥–åŠ±ï¼Œå¸®åŠ©æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒºåˆ†æœ‰æ•ˆçš„å’Œé”™è¯¯çš„æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„Audio-Thinkeræ¨¡å‹åœ¨å„ç§åŸºå‡†ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„é¢å‘æ¨ç†çš„LALMï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08039v1">PDF</a> preprint</p>
<p><strong>Summary</strong>ï¼š<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ã€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„è¿‘æœŸè¿›å±•ï¼Œé€šè¿‡åŸºäºè§„åˆ™çš„å¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œå®ƒä»¬çš„æ¨ç†èƒ½åŠ›å·²æ˜¾è‘—æé«˜ã€‚ç„¶è€Œï¼Œåœ¨éŸ³é¢‘é—®ç­”æ–¹é¢ï¼Œæ˜ç¡®çš„æ¨ç†è¿‡ç¨‹å°šæœªæ˜¾ç¤ºå‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œæœ‰æ•ˆåˆ©ç”¨æ·±åº¦æ¨ç†ä»æ˜¯å¼€æ”¾æŒ‘æˆ˜ï¼ŒLALMä»æœªèƒ½è¾¾åˆ°äººç±»æ°´å¹³çš„å¬è§‰è¯­è¨€æ¨ç†ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Audio-Thinkerï¼Œä¸€ä¸ªæ—¨åœ¨æé«˜LALMæ¨ç†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé‡ç‚¹æ”¹å–„é€‚åº”æ€§ã€ä¸€è‡´æ€§å’Œæœ‰æ•ˆæ€§ã€‚é€šè¿‡å¼•å…¥è‡ªé€‚åº”æ€è€ƒç²¾åº¦å¥–åŠ±ï¼Œä½¿æ¨¡å‹èƒ½æ ¹æ®ä»»åŠ¡å¤æ‚åº¦åŠ¨æ€è°ƒæ•´æ¨ç†ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆå¤–éƒ¨å¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°æ¨ç†è¿‡ç¨‹çš„ä¸€è‡´æ€§å’Œè´¨é‡ï¼Œè¾…ä»¥åŸºäºæ€è€ƒçš„å¥–åŠ±ï¼Œå¸®åŠ©æ¨¡å‹åœ¨è®­ç»ƒæ—¶åŒºåˆ†æœ‰æ•ˆçš„å’Œé”™è¯¯çš„æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„Audio-Thinkeræ¨¡å‹åœ¨å„é¡¹åŸºå‡†ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„æ¨ç†å¯¼å‘å‹LALMï¼Œå±•ç°å‡ºå“è¶Šçš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆåŒ…æ‹¬éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼‰çš„æ¨ç†èƒ½åŠ›é€šè¿‡å¼ºåŒ–å­¦ä¹ å¾—åˆ°æ˜¾è‘—æé«˜ã€‚</li>
<li>åœ¨éŸ³é¢‘é—®ç­”æ–¹é¢ï¼Œæ˜ç¡®æ¨ç†è¿‡ç¨‹å°šæœªæ˜¾ç°æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸”æœ‰æ•ˆåˆ©ç”¨æ·±åº¦æ¨ç†ä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥Audio-Thinkeræ¡†æ¶ï¼Œæ—¨åœ¨æé«˜LALMçš„æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨é€‚åº”æ€§ã€ä¸€è‡´æ€§å’Œæœ‰æ•ˆæ€§æ–¹é¢ã€‚</li>
<li>Audio-Thinkeré‡‡ç”¨è‡ªé€‚åº”æ€è€ƒç²¾åº¦å¥–åŠ±ï¼Œä½¿æ¨¡å‹èƒ½æ ¹æ®ä»»åŠ¡å¤æ‚åº¦åŠ¨æ€è°ƒæ•´æ¨ç†ç­–ç•¥ã€‚</li>
<li>ç»“åˆå¤–éƒ¨å¥–åŠ±æ¨¡å‹è¯„ä¼°æ¨ç†è¿‡ç¨‹çš„ä¸€è‡´æ€§å’Œè´¨é‡ã€‚</li>
<li>åŸºäºæ€è€ƒçš„å¥–åŠ±å¸®åŠ©æ¨¡å‹åŒºåˆ†æœ‰æ•ˆå’Œé”™è¯¯çš„æ¨ç†è·¯å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-174b475aeeeea115c9bd3b93b4c6125d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac49b30637545901965c2041fff1d0ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80681e9b6da53937bbadf3abefc1bafa.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Prompt-Guided-Relational-Reasoning-for-Social-Behavior-Understanding-with-Vision-Foundation-Models"><a href="#Prompt-Guided-Relational-Reasoning-for-Social-Behavior-Understanding-with-Vision-Foundation-Models" class="headerlink" title="Prompt-Guided Relational Reasoning for Social Behavior Understanding   with Vision Foundation Models"></a>Prompt-Guided Relational Reasoning for Social Behavior Understanding   with Vision Foundation Models</h2><p><strong>Authors:Thinesh Thiyakesan Ponbagavathi, Chengzheng Yang, Alina Roitberg</strong></p>
<p>Group Activity Detection (GAD) involves recognizing social groups and their collective behaviors in videos. Vision Foundation Models (VFMs), like DinoV2, offer excellent features, but are pretrained primarily on object-centric data and remain underexplored for modeling group dynamics. While they are a promising alternative to highly task-specific GAD architectures that require full fine-tuning, our initial investigation reveals that simply swapping CNN backbones used in these methods with VFMs brings little gain, underscoring the need for structured, group-aware reasoning on top.   We introduce Prompt-driven Group Activity Detection (ProGraD) â€“ a method that bridges this gap through 1) learnable group prompts to guide the VFM attention toward social configurations, and 2) a lightweight two-layer GroupContext Transformer that infers actor-group associations and collective behavior. We evaluate our approach on two recent GAD benchmarks: Cafe, which features multiple concurrent social groups, and Social-CAD, which focuses on single-group interactions. While we surpass state-of-the-art in both settings, our method is especially effective in complex multi-group scenarios, where we yield a gain of 6.5% (Group mAP@1.0) and 8.2% (Group mAP@0.5) using only 10M trainable parameters. Furthermore, our experiments reveal that ProGraD produces interpretable attention maps, offering insights into actor-group reasoning. Code and models will be released. </p>
<blockquote>
<p>ç¾¤ä½“æ´»åŠ¨æ£€æµ‹ï¼ˆGADï¼‰æ¶‰åŠè¯†åˆ«è§†é¢‘ä¸­çš„ç¤¾ä¼šç¾¤ä½“åŠå…¶é›†ä½“è¡Œä¸ºã€‚è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰ï¼Œå¦‚DinoV2ï¼Œå…·æœ‰å‡ºè‰²çš„åŠŸèƒ½ï¼Œä½†ä¸»è¦æ˜¯åŸºäºä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶ä¸”åœ¨æ¨¡æ‹Ÿç¾¤ä½“åŠ¨æ€æ–¹é¢ä»ç„¶è¢«è¾ƒå°‘ç ”ç©¶ã€‚å°½ç®¡å®ƒä»¬æ˜¯æœ‰å‰é€”çš„æ›¿ä»£é«˜åº¦é’ˆå¯¹ä»»åŠ¡çš„GADæ¶æ„ï¼Œè¿™äº›æ¶æ„éœ€è¦å…¨é¢å¾®è°ƒï¼Œä½†æˆ‘ä»¬çš„åˆæ­¥è°ƒæŸ¥è¡¨æ˜ï¼Œä»…ä»…ç”¨VFMæ›¿æ¢è¿™äº›æ–¹æ³•ä¸­çš„CNNéª¨å¹²æ‰€å¸¦æ¥çš„æ”¶ç›Šç”šå¾®ï¼Œè¿™å¼ºè°ƒäº†éœ€è¦åœ¨é¡¶ç«¯è¿›è¡Œç»“æ„åŒ–ã€ç¾¤ä½“æ„ŸçŸ¥æ¨ç†çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†Prompté©±åŠ¨ç¾¤ä½“æ´»åŠ¨æ£€æµ‹ï¼ˆProGraDï¼‰â€”â€”ä¸€ç§å¼¥åˆè¿™ä¸€é¸¿æ²Ÿçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬1ï¼‰å¯å­¦ä¹ çš„ç¾¤ä½“æç¤ºï¼Œå¼•å¯¼VFMæ³¨æ„åŠ›å…³æ³¨ç¤¾ä¼šé…ç½®ï¼Œä»¥åŠ2ï¼‰è½»é‡çº§çš„ä¸¤å±‚ç»„ä¸Šä¸‹æ–‡è½¬æ¢å™¨ï¼Œæ¨æ–­æ¼”å‘˜-ç»„å…³è”å’Œé›†ä½“è¡Œä¸ºã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæœ€æ–°çš„GADåŸºå‡†æµ‹è¯•é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼šCafeï¼Œä»¥å¤šä¸ªå¹¶å‘ç¤¾ä¼šç¾¤ä½“ä¸ºç‰¹è‰²ï¼›ä»¥åŠSocial-CADï¼Œä¸“æ³¨äºå•ä¸€ç¾¤ä½“äº’åŠ¨ã€‚å°½ç®¡æˆ‘ä»¬åœ¨ä¸¤ç§è®¾ç½®ä¸­éƒ½è¶…è¶Šäº†æœ€æ–°æŠ€æœ¯çŠ¶æ€ï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤æ‚çš„å¤šç¾¤ä½“åœºæ™¯ä¸­å°¤å…¶æœ‰æ•ˆï¼Œä»…ä½¿ç”¨10Mä¸ªå¯è®­ç»ƒå‚æ•°å°±å®ç°äº†ç¾¤å¹³å‡å‡†ç¡®ç‡æé«˜6.5%ï¼ˆGroup <a href="mailto:&#109;&#65;&#80;&#x40;&#x31;&#x2e;&#48;">&#109;&#65;&#80;&#x40;&#x31;&#x2e;&#48;</a>ï¼‰å’Œ8.2%ï¼ˆGroup <a href="mailto:&#x6d;&#65;&#x50;&#x40;&#48;&#x2e;&#53;">&#x6d;&#65;&#x50;&#x40;&#48;&#x2e;&#53;</a>ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒProGraDäº§ç”Ÿå¯è§£é‡Šçš„å…³æ³¨å›¾ï¼Œä¸ºæ¼”å‘˜-ç»„æ¨ç†æä¾›äº†æ·±å…¥è§è§£ã€‚ä»£ç å’Œæ¨¡å‹å°†å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07996v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>ç¾¤ä½“æ´»åŠ¨æ£€æµ‹ï¼ˆGADï¼‰æ—¨åœ¨è¯†åˆ«è§†é¢‘ä¸­çš„ç¤¾äº¤ç¾¤ä½“åŠå…¶é›†ä½“è¡Œä¸ºã€‚è™½ç„¶DinoV2ç­‰è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰å…·æœ‰å‡ºè‰²çš„ç‰¹å¾ï¼Œä½†å®ƒä»¬ä¸»è¦åŸºäºå¯¹è±¡ä¸­å¿ƒæ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œå¯¹äºç¾¤ä½“åŠ¨æ€å»ºæ¨¡ä»æ˜¾ä¸è¶³ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ç§åä¸ºPrompté©±åŠ¨ç¾¤ä½“æ´»åŠ¨æ£€æµ‹ï¼ˆProGraDï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡å¯å­¦ä¹ çš„ç¾¤ä½“æç¤ºå’Œå¼•å¯¼VFMæ³¨æ„åŠ›å…³æ³¨ç¤¾äº¤é…ç½®ï¼Œä»¥åŠä¸€ä¸ªè½»é‡çº§çš„ä¸¤å±‚GroupContext Transformeræ¥æ¨æ–­æ¼”å‘˜ç¾¤ä½“å…³è”å’Œé›†ä½“è¡Œä¸ºã€‚è¯¥æ–¹æ³•åœ¨æœ€è¿‘çš„GADåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨å¤æ‚çš„å¤šå…ƒç¾¤ç»„åœºæ™¯ä¸­ä¼˜åŠ¿æ˜æ˜¾ï¼Œä½¿ç”¨ä»…10Må¯è®­ç»ƒå‚æ•°ä¾¿å®ç°äº†æ˜¾è‘—çš„å¢ç›Šã€‚æ­¤å¤–ï¼ŒProGraDäº§ç”Ÿçš„æ³¨æ„åŠ›åœ°å›¾å…·æœ‰å¯è§£é‡Šæ€§ï¼Œä¸ºæ¼”å‘˜ç¾¤ä½“æ¨ç†æä¾›äº†è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Group Activity Detection (GAD) æ—¨åœ¨è¯†åˆ«è§†é¢‘ä¸­çš„ç¤¾äº¤ç¾¤ä½“åŠå…¶è¡Œä¸ºã€‚</li>
<li>è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰å¦‚DinoV2åœ¨GADæ–¹é¢çš„åº”ç”¨å°šå¾…æ¢ç´¢ã€‚</li>
<li>ç®€å•çš„å°†CNNéª¨å¹²ç½‘ç»œä¸VFMsæ›¿æ¢å¯¹æå‡GADæ•ˆæœæœ‰é™ï¼Œéœ€è¦ç»“æ„åŒ–çš„ç¾¤ä½“æ„ŸçŸ¥æ¨ç†ã€‚</li>
<li>ProGraDæ–¹æ³•é€šè¿‡å¯å­¦ä¹ çš„ç¾¤ä½“æç¤ºå’ŒGroupContext Transformeræ¥ä¼˜åŒ–VFMåœ¨GADä¸Šçš„è¡¨ç°ã€‚</li>
<li>ProGraDåœ¨å¤šä¸ªGADåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶åœ¨å¤æ‚å¤šç¾¤ç»„åœºæ™¯ä¸­æ•ˆæœçªå‡ºã€‚</li>
<li>ProGraDä»…ä½¿ç”¨10Må¯è®­ç»ƒå‚æ•°ä¾¿å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07996">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-17435b49eff7d363ef0a085e2d860409.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a07710f75150ac62207536bedda76cb4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d4eab6c6e5c87e14908577aa58d6537.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-678e44e83a7364a07e1809f3fab9da2e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-645e2d49f698365af6a20fb53d516be6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8319b42474eb71bb5747a8cfe9f7b095.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DIVER-A-Multi-Stage-Approach-for-Reasoning-intensive-Information-Retrieval"><a href="#DIVER-A-Multi-Stage-Approach-for-Reasoning-intensive-Information-Retrieval" class="headerlink" title="DIVER: A Multi-Stage Approach for Reasoning-intensive Information   Retrieval"></a>DIVER: A Multi-Stage Approach for Reasoning-intensive Information   Retrieval</h2><p><strong>Authors:Meixiu Long, Duolin Sun, Dan Yang, Junjie Wang, Yue Shen, Jian Wang, Peng Wei, Jinjie Gu, Jiahai Wang</strong></p>
<p>Retrieval-augmented generation has achieved strong performance on knowledge-intensive tasks where query-document relevance can be identified through direct lexical or semantic matches. However, many real-world queries involve abstract reasoning, analogical thinking, or multi-step inference, which existing retrievers often struggle to capture. To address this challenge, we present \textbf{DIVER}, a retrieval pipeline tailored for reasoning-intensive information retrieval. DIVER consists of four components: document processing to improve input quality, LLM-driven query expansion via iterative document interaction, a reasoning-enhanced retriever fine-tuned on synthetic multi-domain data with hard negatives, and a pointwise reranker that combines LLM-assigned helpfulness scores with retrieval scores. On the BRIGHT benchmark, DIVER achieves state-of-the-art nDCG@10 scores of 41.6 and 28.9 on original queries, consistently outperforming competitive reasoning-aware models. These results demonstrate the effectiveness of reasoning-aware retrieval strategies in complex real-world tasks. Our code and retrieval model will be released soon. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œå…¶ä¸­é€šè¿‡ç›´æ¥è¯æ±‡æˆ–è¯­ä¹‰åŒ¹é…å¯ä»¥è¯†åˆ«æŸ¥è¯¢æ–‡æ¡£çš„ç›¸å…³æ€§ã€‚ç„¶è€Œï¼Œè®¸å¤šç°å®ä¸–ç•Œä¸­çš„æŸ¥è¯¢æ¶‰åŠæŠ½è±¡æ¨ç†ã€ç±»æ¯”æ€ç»´æˆ–å¤šæ­¥æ¨ç†ï¼Œç°æœ‰æ£€ç´¢å™¨å¾€å¾€éš¾ä»¥æ•æ‰ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹æ¨ç†å¯†é›†å‹ä¿¡æ¯æ£€ç´¢é‡èº«å®šåˆ¶çš„<strong>DIVER</strong>æ£€ç´¢ç®¡é“ã€‚DIVERç”±å››ä¸ªç»„ä»¶æ„æˆï¼šæ”¹è¿›è¾“å…¥è´¨é‡çš„æ–‡æ¡£å¤„ç†ï¼Œé€šè¿‡è¿­ä»£æ–‡æ¡£äº¤äº’é©±åŠ¨çš„LLMæŸ¥è¯¢æ‰©å±•ï¼Œåœ¨åˆæˆå¤šåŸŸæ•°æ®ä¸Šå¾®è°ƒå¹¶å¸¦æœ‰ç¡¬é˜´æ€§çš„æ¨ç†å¢å¼ºæ£€ç´¢å™¨ï¼Œä»¥åŠç»“åˆLLMåˆ†é…çš„æœ‰ç”¨æ€§åˆ†æ•°å’Œæ£€ç´¢åˆ†æ•°çš„é€ç‚¹é‡æ–°æ’åºå™¨ã€‚åœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDIVERåœ¨åŸå§‹æŸ¥è¯¢ä¸Šå–å¾—äº†æœ€æ–°çš„nDCG@10åˆ†æ•°ï¼Œåˆ†åˆ«ä¸º41.6å’Œ28.9ï¼ŒæŒç»­è¶…è¶Šç«äº‰æ€§çš„æ¨ç†æ„ŸçŸ¥æ¨¡å‹ã€‚è¿™äº›ç»“æœè¯æ˜äº†æ¨ç†æ„ŸçŸ¥æ£€ç´¢ç­–ç•¥åœ¨å¤æ‚ç°å®ä¸–ç•Œä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ£€ç´¢æ¨¡å‹å¾ˆå¿«å°±ä¼šå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07995v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹éœ€è¦æ¨ç†èƒ½åŠ›çš„ä¿¡æ¯æ£€ç´¢ä»»åŠ¡è€Œè®¾è®¡çš„æ£€ç´¢ç®¡é“DIVERã€‚DIVERåŒ…å«å››ä¸ªç»„ä»¶ï¼šæ”¹è¿›è¾“å…¥è´¨é‡çš„æ–‡æ¡£å¤„ç†ã€é€šè¿‡è¿­ä»£æ–‡æ¡£äº¤äº’é©±åŠ¨LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰è¿›è¡ŒæŸ¥è¯¢æ‰©å±•ã€åœ¨åˆæˆå¤šé¢†åŸŸæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒä¸”åŒ…å«ç¡¬è´Ÿæ ·æœ¬çš„æ¨ç†å¢å¼ºæ£€ç´¢å™¨ä»¥åŠç»“åˆLLMåˆ†é…çš„å¸®åŠ©çš„å¾—åˆ†ä¸æ£€ç´¢å¾—åˆ†çš„é€ç‚¹é‡æ’å™¨ã€‚åœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸Šï¼ŒDIVERåœ¨åŸå§‹æŸ¥è¯¢ä¸Šå–å¾—äº†é¢†å…ˆçš„nDCG@10å¾—åˆ†ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚ç°å®ä»»åŠ¡ä¸­æ¨ç†æ„ŸçŸ¥æ£€ç´¢ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½å·²ç»å¾—åˆ°éªŒè¯ï¼Œä½†å¯¹äºéœ€è¦æŠ½è±¡æ¨ç†ã€ç±»æ¯”æ€ç»´æˆ–å¤šæ­¥æ¨ç†çš„æŸ¥è¯¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸“ä¸ºæ¨ç†å¯†é›†å‹ä¿¡æ¯æ£€ç´¢è®¾è®¡çš„DIVERæ£€ç´¢ç®¡é“ã€‚</li>
<li>DIVERåŒ…å«å››ä¸ªä¸»è¦ç»„ä»¶ï¼ŒåŒ…æ‹¬æ–‡æ¡£å¤„ç†ã€LLMé©±åŠ¨çš„æŸ¥è¯¢æ‰©å±•ã€æ¨ç†å¢å¼ºæ£€ç´¢å™¨ä»¥åŠé€ç‚¹é‡æ’å™¨ã€‚</li>
<li>DIVERåœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„nDCG@10å¾—åˆ†ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚ç°å®ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>DIVERé€šè¿‡ç»“åˆLLMçš„å¸®åŠ©çš„å¾—åˆ†ä¸æ£€ç´¢å¾—åˆ†ï¼Œæé«˜äº†æ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>DIVERçš„ä»£ç å’Œæ£€ç´¢æ¨¡å‹å°†å¾ˆå¿«å‘å¸ƒï¼Œä¸ºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›æä¾›åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07995">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f235e20de2e2137121d18e691489ed94.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a4c776bf9308df28d6721c4a4009d2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-412beb53088c7ed8037915006ee32cbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55189ace169207efb82148918d1530c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a026615b274442b18b2e0d5a4cb1e26.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Expert-Preference-based-Evaluation-of-Automated-Related-Work-Generation"><a href="#Expert-Preference-based-Evaluation-of-Automated-Related-Work-Generation" class="headerlink" title="Expert Preference-based Evaluation of Automated Related Work Generation"></a>Expert Preference-based Evaluation of Automated Related Work Generation</h2><p><strong>Authors:Furkan ÅahinuÃ§, Subhabrata Dutta, Iryna Gurevych</strong></p>
<p>Expert domain writing, such as scientific writing, typically demands extensive domain knowledge. Recent advances in LLMs show promising potential in reducing the expert workload. However, evaluating the quality of automatically generated scientific writing is a crucial open issue, as it requires knowledge of domain-specific evaluation criteria and the ability to discern expert preferences. Conventional automatic metrics and LLM-as-a-judge systems are insufficient to grasp expert preferences and domain-specific quality standards. To address this gap and support human-AI collaborative writing, we focus on related work generation, one of the most challenging scientific tasks, as an exemplar. We propose GREP, a multi-turn evaluation framework that integrates classical related work evaluation criteria with expert-specific preferences. Instead of assigning a single score, our framework decomposes the evaluation into fine-grained dimensions. This localized evaluation approach is further augmented with contrastive few-shot examples to provide detailed contextual guidance for the evaluation dimensions. The design principles allow our framework to deliver cardinal assessment of quality, which can facilitate better post-training compared to ordinal preference data. For better accessibility, we design two variants of GREP: a more precise variant with proprietary LLMs as evaluators, and a cheaper alternative with open-weight LLMs. Empirical investigation reveals that our framework is able to assess the quality of related work sections in a much more robust manner compared to standard LLM judges, reflects natural scenarios of scientific writing, and bears a strong correlation with the human expert assessment. We also observe that generations from state-of-the-art LLMs struggle to satisfy validation constraints of a suitable related work section. They (mostly) fail to improve based on feedback as well. </p>
<blockquote>
<p>ä¸“ä¸šé¢†åŸŸå†™ä½œï¼Œå¦‚ç§‘å­¦å†™ä½œï¼Œé€šå¸¸éœ€è¦å¹¿æ³›çš„ä¸“ä¸šçŸ¥è¯†ã€‚æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ˜¾ç¤ºå‡ºå‡å°‘ä¸“å®¶å·¥ä½œé‡çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¯„ä¼°è‡ªåŠ¨ç”Ÿæˆçš„ç§‘å­¦å†™ä½œçš„è´¨é‡æ˜¯ä¸€ä¸ªå…³é”®çš„å¼€æ”¾æ€§é—®é¢˜ï¼Œå› ä¸ºå®ƒéœ€è¦äº†è§£ç‰¹å®šé¢†åŸŸçš„è¯„ä¼°æ ‡å‡†å’Œè¯†åˆ«ä¸“å®¶åå¥½çš„èƒ½åŠ›ã€‚ä¼ ç»Ÿçš„è‡ªåŠ¨æŒ‡æ ‡å’ŒLLMä½œä¸ºè¯„åˆ¤ç³»ç»Ÿçš„èƒ½åŠ›ä¸è¶³ä»¥æŠŠæ¡ä¸“å®¶åå¥½å’Œç‰¹å®šé¢†åŸŸçš„è´¨é‡æ ‡å‡†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·å¹¶æ”¯æŒäººæœºåä½œå†™ä½œï¼Œæˆ‘ä»¬ä»¥ç›¸å…³å·¥ä½œç”Ÿæˆè¿™ä¸€æœ€å…·æŒ‘æˆ˜æ€§çš„ç§‘å­¦ä»»åŠ¡ä¸ºä¾‹ã€‚æˆ‘ä»¬æå‡ºäº†GREPï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šè½®è¯„ä¼°æ¡†æ¶ï¼Œå®ƒå°†ç»å…¸çš„ç›¸å…³å·¥ä½œè¯„ä»·æ ‡å‡†ä¸ç‰¹å®šä¸“å®¶çš„åå¥½ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸æ˜¯èµ‹äºˆå•ä¸€åˆ†æ•°ï¼Œè€Œæ˜¯å°†è¯„ä¼°åˆ†è§£æˆç²¾ç»†çš„ç»´åº¦ã€‚è¿™ç§å±€éƒ¨è¯„ä¼°æ–¹æ³•ä¸å¯¹æ¯”çš„å°‘æ•°æ¡ˆä¾‹ç›¸ç»“åˆï¼Œä¸ºè¯„ä¼°ç»´åº¦æä¾›äº†è¯¦ç»†çš„ä¸Šä¸‹æ–‡æŒ‡å¯¼ã€‚æˆ‘ä»¬çš„è®¾è®¡åŸåˆ™ä½¿æˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿæä¾›è´¨é‡çš„åŸºæ•°è¯„ä¼°ï¼Œè¿™å¯ä»¥ä¿ƒè¿›ä¸æ’åºåå¥½æ•°æ®ç›¸æ¯”æ›´å¥½çš„åæœŸè®­ç»ƒã€‚ä¸ºäº†æ›´å®¹æ˜“è®¿é—®ï¼Œæˆ‘ä»¬è®¾è®¡äº†GREPçš„ä¸¤ä¸ªå˜ä½“ï¼šä¸€ä¸ªæ›´ç²¾ç¡®çš„ç‰ˆæœ¬ï¼Œä½¿ç”¨ä¸“ç”¨çš„LLMä½œä¸ºè¯„ä¼°å™¨ï¼Œå’Œä¸€ä¸ªæ›´ä¾¿å®œçš„ç‰ˆæœ¬ï¼Œä½¿ç”¨å¼€æ”¾çš„LLMã€‚å®è¯ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿæ›´ç¨³å¥åœ°è¯„ä¼°ç›¸å…³å·¥ä½œéƒ¨åˆ†çš„è´¨é‡ï¼Œä¸æ ‡å‡†LLMåˆ¤æ–­ç›¸æ¯”ï¼Œå®ƒåæ˜ äº†ç§‘å­¦å†™ä½œçš„è‡ªç„¶åœºæ™¯ï¼Œå¹¶ä¸äººç±»ä¸“å®¶è¯„ä¼°å…·æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ï¼Œæ¥è‡ªæœ€æ–°LLMçš„ç”Ÿæˆåœ¨æ»¡è¶³ç›¸å…³å·¥ä½œéƒ¨åˆ†çš„éªŒè¯çº¦æŸæ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä»–ä»¬å¤§å¤šæ•°ä¸èƒ½æ ¹æ®åé¦ˆè¿›è¡Œæ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07955v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://ukplab.github.io/arxiv2025-expert-eval-rw/">https://ukplab.github.io/arxiv2025-expert-eval-rw/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨ç§‘å­¦å†™ä½œé¢†åŸŸï¼Œå¦‚ä½•è¯„ä¼°è‡ªåŠ¨ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ã€‚æ–‡ç« æŒ‡å‡ºï¼Œç°æœ‰çš„è‡ªåŠ¨è¯„ä¼°å·¥å…·å’ŒLLMè¯„åˆ¤ç³»ç»Ÿæ— æ³•å‡†ç¡®æŠŠæ¡ä¸“å®¶åå¥½å’Œé¢†åŸŸç‰¹å®šè´¨é‡æ ‡å‡†ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„å¤šå›åˆè¯„ä¼°æ¡†æ¶GREPï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç»å…¸çš„ç›¸å…³å·¥ä½œè¯„ä»·æ ‡å‡†ä¸ç‰¹å®šä¸“å®¶åå¥½ã€‚GREPé€šè¿‡ç»†åŒ–è¯„ä»·æ ‡å‡†ï¼Œæä¾›è¯¦ç»†çš„ä¸Šä¸‹æ–‡æŒ‡å¯¼ï¼Œæ›´å‡†ç¡®åœ°è¯„ä¼°æ–‡æœ¬è´¨é‡ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†GREPçš„ä¸¤ç§å˜ä½“ï¼Œä¸€ç§ä½¿ç”¨ä¸“æœ‰LLMä½œä¸ºè¯„ä¼°å™¨ï¼Œå¦ä¸€ç§ä½¿ç”¨å¼€æºLLMï¼Œä»¥é™ä½æˆæœ¬ã€‚å®éªŒè¡¨æ˜ï¼ŒGREPæ¡†æ¶èƒ½æ›´ç¨³å¥åœ°è¯„ä¼°ç›¸å…³å·¥ä½œéƒ¨åˆ†çš„è´¨é‡ï¼Œä¸äººç±»ä¸“å®¶è¯„ä¼°ç»“æœé«˜åº¦ç›¸å…³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§‘å­¦å†™ä½œéœ€è¦å¹¿æ³›çš„é¢†åŸŸçŸ¥è¯†ï¼ŒLLMsçš„æœ€è¿‘è¿›å±•åœ¨å‡å°‘ä¸“å®¶å·¥ä½œé‡æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚</li>
<li>è¯„ä¼°è‡ªåŠ¨ç”Ÿæˆçš„ç§‘å­¦å†™ä½œçš„è´¨é‡æ˜¯ä¸€ä¸ªé‡è¦ä½†æœªè§£å†³çš„é—®é¢˜ï¼Œéœ€è¦äº†è§£é¢†åŸŸç‰¹å®šçš„è¯„ä¼°æ ‡å‡†å’Œä¸“å®¶åå¥½ã€‚</li>
<li>ç°æœ‰è‡ªåŠ¨è¯„ä¼°å·¥å…·å’ŒLLMè¯„åˆ¤ç³»ç»Ÿä¸è¶³ä»¥æŠŠæ¡ä¸“å®¶åå¥½å’Œé¢†åŸŸç‰¹å®šæ ‡å‡†ã€‚</li>
<li>GREPæ˜¯ä¸€ä¸ªæ–°çš„å¤šå›åˆè¯„ä¼°æ¡†æ¶ï¼Œç»“åˆäº†ç»å…¸çš„ç›¸å…³å·¥ä½œè¯„ä»·æ ‡å‡†ä¸ç‰¹å®šä¸“å®¶åå¥½ï¼Œè¿›è¡Œç²¾ç»†åŒ–è¯„ä»·ã€‚</li>
<li>GREPæ¡†æ¶æä¾›äº†è¯¦ç»†çš„ä¸Šä¸‹æ–‡æŒ‡å¯¼ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°æ–‡æœ¬è´¨é‡ã€‚</li>
<li>æ–‡ç« ä»‹ç»äº†GREPçš„ä¸¤ç§å˜ä½“ï¼Œä¸€ç§ä½¿ç”¨ä¸“æœ‰LLMï¼Œå¦ä¸€ç§ä½¿ç”¨å¼€æºLLMä»¥é™ä½æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20ffa31dbc769fa55393018f62a9be0d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc70bcf6eaf7389ad704f67398ccb6b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d6235f2e8f18706e15cedd4ea1c4a22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2739cad01faa6e64f7b3079b60a9d357.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-506530f93f23cf9bbec07d05522e794e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space"><a href="#MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space" class="headerlink" title="MolmoAct: Action Reasoning Models that can Reason in Space"></a>MolmoAct: Action Reasoning Models that can Reason in Space</h2><p><strong>Authors:Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, Winson Han, Wilbert Pumacay, Angelica Wu, Rose Hendrix, Karen Farley, Eli VanderBilt, Ali Farhadi, Dieter Fox, Ranjay Krishna</strong></p>
<p>Reasoning is central to purposeful action, yet most robotic foundation models map perception and instructions directly to control, which limits adaptability, generalization, and semantic grounding. We introduce Action Reasoning Models (ARMs), a class of vision-language-action models that integrate perception, planning, and control through a structured three-stage pipeline. Our model, MolmoAct, encodes observations and instructions into depth-aware perception tokens, generates mid-level spatial plans as editable trajectory traces, and predicts precise low-level actions, enabling explainable and steerable behavior. MolmoAct-7B-D achieves strong performance across simulation and real-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching tasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success on LIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks; and in real-world fine-tuning, an additional 10% (single-arm) and an additional 22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines by an additional 23.3% on out-of-distribution generalization and achieves top human-preference scores for open-ended instruction following and trajectory steering. Furthermore, we release, for the first time, the MolmoAct Dataset â€“ a mid-training robot dataset comprising over 10,000 high quality robot trajectories across diverse scenarios and tasks. Training with this dataset yields an average 5.5% improvement in general performance over the base model. We release all model weights, training code, our collected dataset, and our action reasoning dataset, establishing MolmoAct as both a state-of-the-art robotics foundation model and an open blueprint for building ARMs that transform perception into purposeful action through structured reasoning. Blogpost: <a target="_blank" rel="noopener" href="https://allenai.org/blog/molmoact">https://allenai.org/blog/molmoact</a> </p>
<blockquote>
<p>æ¨ç†æ˜¯ç›®æ ‡è¡ŒåŠ¨çš„æ ¸å¿ƒï¼Œç„¶è€Œå¤§å¤šæ•°æœºå™¨äººåŸºç¡€æ¨¡å‹ç›´æ¥å°†æ„ŸçŸ¥å’ŒæŒ‡ä»¤æ˜ å°„åˆ°æ§åˆ¶ä¸Šï¼Œè¿™é™åˆ¶äº†é€‚åº”æ€§ã€æ³›åŒ–å’Œè¯­ä¹‰åŸºç¡€ã€‚æˆ‘ä»¬å¼•å…¥äº†è¡ŒåŠ¨æ¨ç†æ¨¡å‹ï¼ˆARMsï¼‰ï¼Œè¿™æ˜¯ä¸€ç±»èåˆæ„ŸçŸ¥ã€è§„åˆ’å’Œæ§åˆ¶çš„è§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹ï¼Œé€šè¿‡ä¸€ä¸ªç»“æ„åŒ–çš„ä¸‰é˜¶æ®µç®¡é“æ¥å®ç°ã€‚æˆ‘ä»¬çš„æ¨¡å‹MolmoActå°†è§‚å¯Ÿå’ŒæŒ‡ä»¤ç¼–ç ä¸ºæ·±åº¦æ„ŸçŸ¥æ ‡è®°ï¼Œç”Ÿæˆå¯ç¼–è¾‘çš„è½¨è¿¹è·Ÿè¸ªä½œä¸ºä¸­çº§ç©ºé—´è®¡åˆ’ï¼Œå¹¶é¢„æµ‹ç²¾ç¡®çš„ä½çº§è¡ŒåŠ¨ï¼Œä»è€Œå®ç°å¯è§£é‡Šå’Œå¯å¼•å¯¼çš„è¡Œä¸ºã€‚MolmoAct-7B-Dåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒè®¾ç½®ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼šåœ¨SimplerEnvè§†è§‰åŒ¹é…ä»»åŠ¡ä¸Šè¾¾åˆ°70.5%çš„é›¶å°„å‡»å‡†ç¡®ç‡ï¼Œè¶…è¶Šé—­æºPi-0å’ŒGR00TN1ï¼›åœ¨LIBEROä¸Šå¹³å‡æˆåŠŸç‡è¾¾åˆ°86.6%ï¼Œå…¶ä¸­é•¿æœŸä»»åŠ¡è¾ƒThinkActå¢åŠ äº†é¢å¤–çš„6.3%çš„ä¼˜åŠ¿ï¼›åœ¨çœŸå®ä¸–ç•Œçš„å¾®è°ƒä¸­ï¼Œç›¸è¾ƒäºPi-0-FASTï¼Œå•è‡‚ä»»åŠ¡å¢åŠ äº†é¢å¤–çš„10%ï¼ŒåŒæ‰‹åŠ¨ä»»åŠ¡å¢åŠ äº†é¢å¤–çš„22.7%ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜ä»¥é¢å¤–çš„23.3%çš„ä¼˜åŠ¿è¶…è¶ŠåŸºå‡†çº¿åœ¨è¶…å‡ºåˆ†å¸ƒæ³›åŒ–æ–¹é¢å–å¾—æœ€ä½³è¡¨ç°ï¼Œå¹¶ä¸”åœ¨å¼€æ”¾å¼æŒ‡ä»¤è·Ÿéšå’Œè½¨è¿¹å¼•å¯¼æ–¹é¢è·å¾—äº†æœ€é«˜çš„äººç±»åå¥½åˆ†æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é¦–æ¬¡å‘å¸ƒMolmoActæ•°æ®é›†â€”â€”ä¸€ä¸ªä¸­æœŸè®­ç»ƒæœºå™¨äººæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡10,000æ¡é«˜è´¨é‡æœºå™¨äººè½¨è¿¹ï¼Œæ¶µç›–å„ç§åœºæ™¯å’Œä»»åŠ¡ã€‚ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè®­ç»ƒç›¸è¾ƒäºåŸºç¡€æ¨¡å‹åœ¨æ•´ä½“æ€§èƒ½ä¸Šå¹³å‡æé«˜äº†5.5%ã€‚æˆ‘ä»¬å…¬å¼€äº†æ‰€æœ‰æ¨¡å‹æƒé‡ã€è®­ç»ƒä»£ç ã€æ”¶é›†çš„æ•°æ®é›†ä»¥åŠæˆ‘ä»¬çš„è¡ŒåŠ¨æ¨ç†æ•°æ®é›†ï¼Œç¡®ç«‹äº†MolmoActä½œä¸ºæœ€å…ˆè¿›çš„æœºå™¨äººåŸºç¡€æ¨¡å‹ï¼Œå¹¶ä¸ºå»ºç«‹é€šè¿‡ç»“æ„åŒ–æ¨ç†å°†æ„ŸçŸ¥è½¬åŒ–ä¸ºæœ‰ç›®çš„è¡ŒåŠ¨çš„ARMsæä¾›äº†ä¸€ä¸ªå¼€æ”¾çš„è“å›¾ã€‚åšå®¢æ–‡ç« ï¼š<a target="_blank" rel="noopener" href="https://allenai.org/blog/molmoact">https://allenai.org/blog/molmoact</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07917v1">PDF</a> Appendix on Blogpost: <a target="_blank" rel="noopener" href="https://allenai.org/blog/molmoact">https://allenai.org/blog/molmoact</a></p>
<p><strong>Summary</strong></p>
<p>æœºå™¨äººåŠ¨ä½œæ¨ç†æ¨¡å‹ï¼ˆARMsï¼‰æ˜¯å®ç°æ„ŸçŸ¥è½¬åŒ–ä¸ºæœ‰ç›®çš„è¡ŒåŠ¨çš„å…³é”®ã€‚å½“å‰å¤šæ•°æœºå™¨äººåŸºç¡€æ¨¡å‹ç›´æ¥å°†æ„ŸçŸ¥å’ŒæŒ‡ä»¤æ˜ å°„åˆ°æ§åˆ¶ï¼Œè¿™é™åˆ¶äº†å…¶é€‚åº”æ€§ã€æ³›åŒ–å’Œè¯­ä¹‰å®šä½èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŠ¨ä½œæ¨ç†æ¨¡å‹ï¼ˆARMsï¼‰ï¼Œè¿™æ˜¯ä¸€ç§èåˆæ„ŸçŸ¥ã€è§„åˆ’å’Œæ§åˆ¶çš„è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹MolmoActé€šè¿‡ç»“æ„åŒ–ä¸‰é˜¶æ®µç®¡é“å®ç°æ·±åº¦æ„ŸçŸ¥æ ‡è®°å’ŒæŒ‡ä»¤ç¼–ç ã€å¯ç¼–è¾‘è½¨è¿¹è½¨è¿¹çš„ç©ºé—´è§„åˆ’ä»¥åŠç²¾ç¡®çš„ä½å±‚æ¬¡åŠ¨ä½œé¢„æµ‹ï¼Œä»è€Œå®ç°äº†å¯è§£é‡Šå’Œå¯å¼•å¯¼çš„è¡Œä¸ºã€‚æ¨¡å‹åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸‹çš„è¡¨ç°ä¼˜å¼‚ï¼Œå¦‚åœ¨SimplerEnvè§†è§‰åŒ¹é…ä»»åŠ¡ä¸Šè¾¾åˆ°70.5%çš„é›¶æ ·æœ¬å‡†ç¡®ç‡ï¼Œè¶…è¶ŠPi-0å’ŒGR00T N1ç­‰å°é—­æºä»£ç æ¨¡å‹ï¼›åœ¨LIBEROä¸Šå¹³å‡æˆåŠŸç‡è¾¾åˆ°86.6%ï¼Œåœ¨é•¿å‘¨æœŸä»»åŠ¡ä¸Šçš„æå‡å¹…åº¦æ¯”ThinkActé«˜å‡º6.3%ï¼›åœ¨çœŸå®ä¸–ç•Œå¾®è°ƒä¸­ï¼Œç›¸è¾ƒäºPi-0-FASTæ¨¡å‹ï¼Œå•è‡‚ä»»åŠ¡è¿›æ­¥å¹…åº¦æé«˜äº†é¢å¤–çš„10%ï¼ŒåŒè‚¢ååŒä»»åŠ¡æé«˜äº†é¢å¤–çš„22.7%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é¦–æ¬¡å‘å¸ƒäº†MolmoActæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€å¥—åŒ…å«è¶…è¿‡ä¸€ä¸‡æ¡é«˜è´¨é‡æœºå™¨äººè½¨è¿¹çš„ä¸­æœŸè®­ç»ƒæ•°æ®é›†ï¼Œæ¶µç›–å„ç§åœºæ™¯å’Œä»»åŠ¡ã€‚ä½¿ç”¨æ­¤æ•°æ®é›†è¿›è¡Œè®­ç»ƒç›¸è¾ƒäºåŸºç¡€æ¨¡å‹åœ¨æ€»ä½“ä¸Šæå‡äº†å¹³å‡5.5%çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªå…¼å…·å‰æ²¿æ°´å¹³çš„æœºå™¨äººåŸºç¡€æ¨¡å‹MolmoActï¼Œå¹¶ä¸ºæ„å»ºé€šè¿‡ç»“æ„åŒ–æ¨ç†å°†æ„ŸçŸ¥è½¬åŒ–ä¸ºæœ‰ç›®çš„è¡ŒåŠ¨çš„ARMsæä¾›äº†å¼€æ”¾çš„è“å›¾ã€‚æ›´å¤šä¿¡æ¯è¯·å‚é˜…<a target="_blank" rel="noopener" href="https://allenai.org/blog/molmoact%E3%80%82">https://allenai.org/blog/molmoactã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ¨ä½œæ¨ç†æ¨¡å‹ï¼ˆARMsï¼‰åœ¨æœºå™¨äººé¢†åŸŸæ˜¯å…³é”®å®ç°æ„ŸçŸ¥åˆ°åŠ¨ä½œè½¬åŒ–çš„æŠ€æœ¯ã€‚</li>
<li>ç°æœ‰æœºå™¨äººåŸºç¡€æ¨¡å‹ç›´æ¥æ˜ å°„æ„ŸçŸ¥å’ŒæŒ‡ä»¤åˆ°æ§åˆ¶ï¼Œé™åˆ¶äº†é€‚åº”æ€§ã€æ³›åŒ–èƒ½åŠ›å’Œè¯­ä¹‰å®šä½ã€‚</li>
<li>MolmoActæ¨¡å‹é€šè¿‡ç»“æ„åŒ–ä¸‰é˜¶æ®µç®¡é“å®ç°æ·±åº¦æ„ŸçŸ¥å’ŒæŒ‡ä»¤å¤„ç†ã€ç©ºé—´è§„åˆ’å’Œç²¾ç¡®åŠ¨ä½œé¢„æµ‹ã€‚</li>
<li>MolmoActåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šå¤šä¸ªç°æœ‰æ¨¡å‹ã€‚</li>
<li>å‘å¸ƒäº†MolmoActæ•°æ®é›†ï¼ŒåŒ…å«å¤šç§åœºæ™¯å’Œä»»åŠ¡çš„é«˜è´¨é‡æœºå™¨äººè½¨è¿¹ã€‚</li>
<li>ä½¿ç”¨MolmoActæ•°æ®é›†è®­ç»ƒå¯æé«˜æ¨¡å‹æ•´ä½“è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07917">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4c31b98a6690031afab73628907f472d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12c925d22d36e169454bf219ba4b6646.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-237703af41711c21d22ada7fec5cb183.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d14ed58322518cc94b350a0869d298c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63e6dbdae407e68b5b1b966a7e2cd985.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Autonomous-Navigation-of-Cloud-Controlled-Quadcopters-in-Confined-Spaces-Using-Multi-Modal-Perception-and-LLM-Driven-High-Semantic-Reasoning"><a href="#Autonomous-Navigation-of-Cloud-Controlled-Quadcopters-in-Confined-Spaces-Using-Multi-Modal-Perception-and-LLM-Driven-High-Semantic-Reasoning" class="headerlink" title="Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces   Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning"></a>Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces   Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning</h2><p><strong>Authors:Shoaib Ahmmad, Zubayer Ahmed Aditto, Md Mehrab Hossain, Noushin Yeasmin, Shorower Hossain</strong></p>
<p>This paper introduces an advanced AI-driven perception system for autonomous quadcopter navigation in GPS-denied indoor environments. The proposed framework leverages cloud computing to offload computationally intensive tasks and incorporates a custom-designed printed circuit board (PCB) for efficient sensor data acquisition, enabling robust navigation in confined spaces. The system integrates YOLOv11 for object detection, Depth Anything V2 for monocular depth estimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial Measurement Unit (IMU), and a cloud-based Large Language Model (LLM) for context-aware decision-making. A virtual safety envelope, enforced by calibrated sensor offsets, ensures collision avoidance, while a multithreaded architecture achieves low-latency processing. Enhanced spatial awareness is facilitated by 3D bounding box estimation with Kalman filtering. Experimental results in an indoor testbed demonstrate strong performance, with object detection achieving a mean Average Precision (mAP50) of 0.6, depth estimation Mean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42 trials over approximately 11 minutes, and end-to-end system latency below 1 second. This cloud-supported, high-intelligence framework serves as an auxiliary perception and navigation system, complementing state-of-the-art drone autonomy for GPS-denied confined spaces. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å…ˆè¿›çš„AIé©±åŠ¨æ„ŸçŸ¥ç³»ç»Ÿï¼Œç”¨äºGPSæ‹’æ­¢çš„å®¤å†…ç¯å¢ƒä¸­çš„è‡ªä¸»å››è½´é£è¡Œå™¨å¯¼èˆªã€‚æ‰€æå‡ºçš„æ¡†æ¶åˆ©ç”¨äº‘è®¡ç®—æ¥å¤„ç†è®¡ç®—å¯†é›†å‹ä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨å®šåˆ¶è®¾è®¡çš„å°åˆ·ç”µè·¯æ¿ï¼ˆPCBï¼‰è¿›è¡Œé«˜æ•ˆä¼ æ„Ÿå™¨æ•°æ®é‡‡é›†ï¼Œä»è€Œåœ¨æœ‰é™ç©ºé—´å†…å®ç°ç¨³å¥å¯¼èˆªã€‚è¯¥ç³»ç»Ÿé›†æˆäº†YOLOv11è¿›è¡Œç›®æ ‡æ£€æµ‹ã€Depth Anything V2è¿›è¡Œå•ç›®æ·±åº¦ä¼°è®¡ã€é…å¤‡é£è¡Œæ—¶é—´ï¼ˆToFï¼‰ä¼ æ„Ÿå™¨å’Œæƒ¯æ€§æµ‹é‡å•å…ƒï¼ˆIMUï¼‰çš„PCBï¼Œä»¥åŠåŸºäºäº‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å†³ç­–ã€‚é€šè¿‡æ ¡å‡†çš„ä¼ æ„Ÿå™¨åç§»å®æ–½è™šæ‹Ÿå®‰å…¨åŒ…ç»œï¼Œä»¥ç¡®ä¿é¿å…ç¢°æ’ï¼ŒåŒæ—¶å¤šçº¿ç¨‹æ¶æ„å®ç°ä½å»¶è¿Ÿå¤„ç†ã€‚é€šè¿‡å¡å°”æ›¼æ»¤æ³¢çš„3Dè¾¹ç•Œæ¡†ä¼°è®¡å¢å¼ºäº†ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚åœ¨å®¤å†…æµ‹è¯•å¹³å°ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿæ€§èƒ½å¼ºåŠ²ï¼Œç›®æ ‡æ£€æµ‹å¹³å‡ç²¾åº¦ï¼ˆmAP50ï¼‰è¾¾åˆ°0.6ï¼Œæ·±åº¦ä¼°è®¡å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ä¸º7.2å˜ç±³ï¼Œåœ¨çº¦11åˆ†é’Ÿçš„42æ¬¡è¯•éªŒä¸­åªæœ‰16æ¬¡å®‰å…¨åŒ…ç»œè¿åï¼Œå¹¶ä¸”ç«¯åˆ°ç«¯ç³»ç»Ÿå»¶è¿Ÿä½äº1ç§’ã€‚è¿™ä¸€å—äº‘æ”¯æŒçš„é«˜æ™ºèƒ½æ¡†æ¶ä½œä¸ºè¾…åŠ©æ„ŸçŸ¥å’Œå¯¼èˆªç³»ç»Ÿï¼Œè¡¥å……äº†æœ€æ–°æ— äººæœºåœ¨GPSæ‹’æ­¢çš„å°é—­ç©ºé—´ä¸­çš„è‡ªä¸»æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07885v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥è®ºæ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºGPSæ‹’æ­¢å®¤å†…ç¯å¢ƒä¸­å››è½´é£è¡Œå™¨è‡ªä¸»å¯¼èˆªçš„é«˜çº§AIé©±åŠ¨æ„ŸçŸ¥ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨äº‘è®¡ç®—å¸è½½è®¡ç®—å¯†é›†å‹ä»»åŠ¡ï¼Œé‡‡ç”¨å®šåˆ¶å°åˆ·ç”µè·¯æ¿è¿›è¡Œé«˜æ•ˆä¼ æ„Ÿå™¨æ•°æ®é‡‡é›†ï¼Œå¹¶ç»“åˆYOLOv11ç›®æ ‡æ£€æµ‹ã€Depth Anything V2å•ç›®æ·±åº¦ä¼°è®¡ç­‰æŠ€æœ¯ï¼Œå®ç°å®¤å†…ç¯å¢ƒä¸­çš„ç¨³å¥å¯¼èˆªã€‚é€šè¿‡é£è¡Œæ—¶é—´ä¼ æ„Ÿå™¨å’Œæƒ¯æ€§æµ‹é‡è£…ç½®çš„æ•°æ®ï¼Œç»“åˆäº‘ç«¯å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¯­å¢ƒå†³ç­–ï¼Œæ„å»ºè™šæ‹Ÿå®‰å…¨åŒ…ç»œä»¥å®ç°é¿éšœã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨å®¤å†…æµ‹è¯•ç¯å¢ƒä¸­çš„è¡¨ç°å¼ºåŠ²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æè¿°äº†ä¸€ç§åœ¨GPSæ‹’æ­¢å®¤å†…ç¯å¢ƒä¸­ä½¿ç”¨çš„å…ˆè¿›AIé©±åŠ¨å››è½´é£è¡Œå™¨æ„ŸçŸ¥ç³»ç»Ÿã€‚</li>
<li>ç³»ç»Ÿåˆ©ç”¨äº‘è®¡ç®—å¤„ç†è®¡ç®—å¯†é›†å‹ä»»åŠ¡ï¼Œæå‡æ•ˆç‡ã€‚</li>
<li>é€šè¿‡å®šåˆ¶çš„å°åˆ·ç”µè·¯æ¿å®ç°é«˜æ•ˆä¼ æ„Ÿå™¨æ•°æ®é‡‡é›†ã€‚</li>
<li>æ•´åˆå¤šç§æŠ€æœ¯ï¼šYOLOv11ç›®æ ‡æ£€æµ‹ã€Depth Anything V2å•ç›®æ·±åº¦ä¼°è®¡ç­‰ã€‚</li>
<li>ç»“åˆé£è¡Œæ—¶é—´ä¼ æ„Ÿå™¨å’Œæƒ¯æ€§æµ‹é‡è£…ç½®æ•°æ®ï¼Œå®ç°è™šæ‹Ÿå®‰å…¨åŒ…ç»œï¼Œç¡®ä¿é¿éšœã€‚</li>
<li>é€šè¿‡äº‘ç«¯å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¯­å¢ƒå†³ç­–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07885">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ea57cce7db94a9d8adc395a1ff1e3377.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e408c9813f93122c87bcdd508f503dd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99c6efae89fd414584247ce40489458f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TAR-TVG-Enhancing-VLMs-with-Timestamp-Anchor-Constrained-Reasoning-for-Temporal-Video-Grounding"><a href="#TAR-TVG-Enhancing-VLMs-with-Timestamp-Anchor-Constrained-Reasoning-for-Temporal-Video-Grounding" class="headerlink" title="TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for   Temporal Video Grounding"></a>TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for   Temporal Video Grounding</h2><p><strong>Authors:Chaohong Guo, Xun Mo, Yongwei Nie, Xuemiao Xu, Chao Xu, Fei Yu, Chengjiang Long</strong></p>
<p>Temporal Video Grounding (TVG) aims to precisely localize video segments corresponding to natural language queries, which is a critical capability for long-form video understanding. Although existing reinforcement learning approaches encourage models to generate reasoning chains before predictions, they fail to explicitly constrain the reasoning process to ensure the quality of the final temporal predictions. To address this limitation, we propose Timestamp Anchor-constrained Reasoning for Temporal Video Grounding (TAR-TVG), a novel framework that introduces timestamp anchors within the reasoning process to enforce explicit supervision to the thought content. These anchors serve as intermediate verification points. More importantly, we require each reasoning step to produce increasingly accurate temporal estimations, thereby ensuring that the reasoning process contributes meaningfully to the final prediction. To address the challenge of low-probability anchor generation in models (e.g., Qwen2.5-VL-3B), we develop an efficient self-distillation training strategy: (1) initial GRPO training to collect 30K high-quality reasoning traces containing multiple timestamp anchors, (2) supervised fine-tuning (SFT) on distilled data, and (3) final GRPO optimization on the SFT-enhanced model. This three-stage training strategy enables robust anchor generation while maintaining reasoning quality. Experiments show that our model achieves state-of-the-art performance while producing interpretable, verifiable reasoning chains with progressively refined temporal estimations. </p>
<blockquote>
<p>æ—¶åºè§†é¢‘å®šä½ï¼ˆTVGï¼‰æ—¨åœ¨å‡†ç¡®åœ°å¯¹åº”è‡ªç„¶è¯­è¨€æŸ¥è¯¢å®šä½è§†é¢‘ç‰‡æ®µï¼Œè¿™å¯¹äºé•¿æ ¼å¼è§†é¢‘ç†è§£æ˜¯ä¸€é¡¹å…³é”®èƒ½åŠ›ã€‚å°½ç®¡ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•é¼“åŠ±æ¨¡å‹åœ¨é¢„æµ‹å‰ç”Ÿæˆæ¨ç†é“¾ï¼Œä½†å®ƒä»¬æœªèƒ½æ˜¾å¼çº¦æŸæ¨ç†è¿‡ç¨‹ä»¥ç¡®ä¿æœ€ç»ˆæ—¶åºé¢„æµ‹çš„è´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†æ—¶åºè§†é¢‘å®šä½çš„æ—¶é—´æˆ³é”šç‚¹çº¦æŸæ¨ç†ï¼ˆTAR-TVGï¼‰æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥æ—¶é—´æˆ³é”šç‚¹ï¼Œä»¥å¼ºåˆ¶æ‰§è¡Œå¯¹æ€æƒ³å†…å®¹çš„æ˜¾å¼ç›‘ç£ã€‚è¿™äº›é”šç‚¹å……å½“ä¸­é—´éªŒè¯ç‚¹ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬è¦æ±‚æ¯ä¸ªæ¨ç†æ­¥éª¤äº§ç”Ÿè¶Šæ¥è¶Šå‡†ç¡®çš„æ—¶åºä¼°è®¡ï¼Œä»è€Œç¡®ä¿æ¨ç†è¿‡ç¨‹å¯¹æœ€ç»ˆé¢„æµ‹äº§ç”Ÿæœ‰æ„ä¹‰çš„è´¡çŒ®ã€‚ä¸ºäº†è§£å†³æ¨¡å‹ä¸­ä½æ¦‚ç‡é”šç‚¹ç”Ÿæˆçš„é—®é¢˜ï¼ˆä¾‹å¦‚Qwen2.5-VL-3Bï¼‰ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æœ‰æ•ˆçš„è‡ªè’¸é¦è®­ç»ƒç­–ç•¥ï¼šï¼ˆ1ï¼‰åˆå§‹GRPOè®­ç»ƒä»¥æ”¶é›†åŒ…å«å¤šä¸ªæ—¶é—´æˆ³é”šç‚¹çš„é«˜è´¨é‡æ¨ç†è½¨è¿¹3ä¸‡æ¡ï¼Œï¼ˆ2ï¼‰è’¸é¦æ•°æ®çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œï¼ˆ3ï¼‰åœ¨SFTå¢å¼ºæ¨¡å‹ä¸Šè¿›è¡Œæœ€ç»ˆçš„GRPOä¼˜åŒ–ã€‚è¿™ç§ä¸‰é˜¶æ®µçš„è®­ç»ƒç­–ç•¥èƒ½å¤Ÿåœ¨ä¿æŒæ¨ç†è´¨é‡çš„åŒæ—¶å®ç°ç¨³å¥çš„é”šç‚¹ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨äº§ç”Ÿå¯è§£é‡Šã€å¯éªŒè¯çš„æ¨ç†é“¾çš„åŒæ—¶ï¼Œå…·æœ‰é€æ­¥æ”¹è¿›çš„æ—¶åºä¼°è®¡ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07683v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†Temporal Video Groundingï¼ˆTVGï¼‰çš„ç›®æ ‡æ˜¯é€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢ç²¾ç¡®å®šä½è§†é¢‘æ®µè½ã€‚ä¸ºè§£å†³ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„å±€é™ï¼Œæå‡ºäº†ä¸€ç§åä¸ºTAR-TVGçš„æ–°æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥æ—¶é—´æˆ³é”šç‚¹æ¥çº¦æŸæ¨ç†è¿‡ç¨‹ï¼Œç¡®ä¿æœ€ç»ˆæ—¶é—´é¢„æµ‹çš„ç²¾å‡†æ€§ã€‚ä¸ºåº”å¯¹æ¨¡å‹ä½æ¦‚ç‡é”šç‚¹ç”Ÿæˆçš„é—®é¢˜ï¼Œå¼€å‘äº†ä¸€ç§é«˜æ•ˆè‡ªè’¸é¦è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬åˆå§‹GRPOè®­ç»ƒæ”¶é›†é«˜è´¨é‡æ¨ç†è½¨è¿¹ã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œæœ€ç»ˆGRPOä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨äº§ç”Ÿå¯è§£é‡Šã€å¯éªŒè¯çš„æ¨ç†é“¾çš„åŒæ—¶ï¼Œé€æ­¥ä¼˜åŒ–æ—¶é—´ä¼°è®¡ï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Temporal Video Grounding (TVG) çš„ç›®æ ‡æ˜¯ç²¾ç¡®å®šä½ä¸è‡ªç„¶è¯­è¨€æŸ¥è¯¢ç›¸å¯¹åº”çš„è§†é¢‘æ®µè½ã€‚</li>
<li>ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨TVGä¸­å­˜åœ¨æ¨ç†è´¨é‡ä¸é«˜çš„é—®é¢˜ã€‚</li>
<li>TAR-TVGæ¡†æ¶é€šè¿‡å¼•å…¥æ—¶é—´æˆ³é”šç‚¹æ¥çº¦æŸæ¨ç†è¿‡ç¨‹ï¼Œæé«˜æœ€ç»ˆæ—¶é—´é¢„æµ‹çš„è´¨é‡ã€‚</li>
<li>æ—¶é—´æˆ³é”šç‚¹ä½œä¸ºä¸­é—´éªŒè¯ç‚¹ï¼Œè¦æ±‚æ¯ä¸ªæ¨ç†æ­¥éª¤äº§ç”Ÿè¶Šæ¥è¶Šç²¾ç¡®çš„æ—¶é—´ä¼°è®¡ã€‚</li>
<li>æ¨¡å‹é¢ä¸´ä½æ¦‚ç‡é”šç‚¹ç”Ÿæˆçš„é—®é¢˜ï¼Œéœ€å¼€å‘é«˜æ•ˆè‡ªè’¸é¦è®­ç»ƒç­–ç•¥æ¥è§£å†³ã€‚</li>
<li>è‡ªè’¸é¦è®­ç»ƒç­–ç•¥åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šåˆå§‹GRPOè®­ç»ƒã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œæœ€ç»ˆGRPOä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07683">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b4b16276b25dfb1be711697b7f2214d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7781f7ef8e95f13a8277c281c4d7ac89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f959ebee0e35dd90e7d8e58599d6b0d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b1c6a89c3de1a406cd0c5ffb1dec3bfc.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="1-2-3-Check-Enhancing-Contextual-Privacy-in-LLM-via-Multi-Agent-Reasoning"><a href="#1-2-3-Check-Enhancing-Contextual-Privacy-in-LLM-via-Multi-Agent-Reasoning" class="headerlink" title="1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent   Reasoning"></a>1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent   Reasoning</h2><p><strong>Authors:Wenkai Li, Liwen Sun, Zhenxiang Guan, Xuhui Zhou, Maarten Sap</strong></p>
<p>Addressing contextual privacy concerns remains challenging in interactive settings where large language models (LLMs) process information from multiple sources (e.g., summarizing meetings with private and public information). We introduce a multi-agent framework that decomposes privacy reasoning into specialized subtasks (extraction, classification), reducing the information load on any single agent while enabling iterative validation and more reliable adherence to contextual privacy norms. To understand how privacy errors emerge and propagate, we conduct a systematic ablation over information-flow topologies, revealing when and why upstream detection mistakes cascade into downstream leakage. Experiments on the ConfAIde and PrivacyLens benchmark with several open-source and closed-sourced LLMs demonstrate that our best multi-agent configuration substantially reduces private information leakage (\textbf{18%} on ConfAIde and \textbf{19%} on PrivacyLens with GPT-4o) while preserving the fidelity of public content, outperforming single-agent baselines. These results highlight the promise of principled information-flow design in multi-agent systems for contextual privacy with LLMs. </p>
<blockquote>
<p>åœ¨å¤„ç†æ¥è‡ªå¤šä¸ªæ¥æºçš„ä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼Œæ€»ç»“åŒ…å«ç§äººä¿¡æ¯å’Œå…¬å…±ä¿¡æ¯çš„ä¼šè®®ï¼‰çš„è¯­è¨€æ¨¡å‹æ—¶ï¼Œå¦‚ä½•åœ¨äº¤äº’å¼ç¯å¢ƒä¸­è§£å†³ä¸Šä¸‹æ–‡éšç§é—®é¢˜ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†éšç§æ¨ç†åˆ†è§£æˆä¸“é—¨çš„å­ä»»åŠ¡ï¼ˆæå–ã€åˆ†ç±»ï¼‰ï¼Œä»è€Œå‡è½»äº†ä»»ä½•å•ä¸ªæ™ºèƒ½ä½“çš„ä¿¡æ¯è´Ÿè½½ï¼ŒåŒæ—¶å®ç°äº†è¿­ä»£éªŒè¯å’Œæ›´å¯é åœ°éµå®ˆä¸Šä¸‹æ–‡éšç§è§„èŒƒã€‚ä¸ºäº†äº†è§£éšç§é”™è¯¯æ˜¯å¦‚ä½•äº§ç”Ÿå’Œä¼ æ’­çš„ï¼Œæˆ‘ä»¬å¯¹ä¿¡æ¯æµæ‹“æ‰‘è¿›è¡Œäº†ç³»ç»Ÿçš„åˆ‡é™¤ç ”ç©¶ï¼Œæ­ç¤ºäº†ä¸Šæ¸¸æ£€æµ‹é”™è¯¯ä½•æ—¶ä»¥åŠä¸ºä½•ä¼šçº§è”æˆä¸‹æ¸¸æ³„æ¼ã€‚åœ¨ConfAIdeå’ŒPrivacyLensåŸºå‡†æµ‹è¯•ä¸Šå¯¹å¤šä¸ªå¼€æºå’Œé—­æºçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æœ€å¥½çš„å¤šæ™ºèƒ½ä½“é…ç½®å¤§å¹…å‡å°‘äº†ç§äººä¿¡æ¯çš„æ³„æ¼ï¼ˆåœ¨ConfAIdeä¸Šå‡å°‘äº†\textbf{18}%ï¼Œåœ¨PrivacyLensä¸Šä¸GPT-4oä¸€èµ·å‡å°‘äº†\textbf{19}%ï¼‰ï¼ŒåŒæ—¶ä¿æŒäº†å…¬å…±å†…å®¹çš„ä¿çœŸåº¦ï¼Œè¶…è¶Šäº†å•æ™ºèƒ½ä½“çš„åŸºçº¿ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­è¿›è¡Œæœ‰åŸåˆ™çš„ä¿¡æ¯æµè®¾è®¡ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä¸Šä¸‹æ–‡éšç§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07667v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ¥è‡ªå¤šä¸ªæºçš„ä¸Šä¸‹æ–‡éšç§ä¿¡æ¯æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å¤šä»£ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†éšç§æ¨ç†åˆ†è§£ä¸ºä¸“é—¨çš„å­ä»»åŠ¡ï¼Œå‡å°‘å•ä¸ªä»£ç†çš„ä¿¡æ¯è´Ÿè½½ï¼ŒåŒæ—¶å®ç°è¿­ä»£éªŒè¯å’Œæ›´å¯é åœ°éµå®ˆä¸Šä¸‹æ–‡éšç§è§„èŒƒã€‚æ–‡ç« é€šè¿‡ç³»ç»Ÿæ€§åœ°ç ”ç©¶ä¿¡æ¯æµåŠ¨æ‹“æ‰‘æ¥äº†è§£éšç§é”™è¯¯å¦‚ä½•äº§ç”Ÿå’Œæ‰©æ•£ï¼Œæ­ç¤ºä¸Šæ¸¸æ£€æµ‹é”™è¯¯ä½•æ—¶ä»¥åŠå¦‚ä½•æ¼”å˜ä¸ºä¸‹æ¸¸æ³„éœ²çš„åŸå› ã€‚åœ¨ConfAIdeå’ŒPrivacyLensåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæœ€ä½³å¤šä»£ç†é…ç½®èƒ½æ˜¾è‘—å‡å°‘ç§äººä¿¡æ¯æ³„éœ²ï¼ŒåŒæ—¶ä¿æŒå…¬å…±å†…å®¹çš„ä¿çœŸåº¦ï¼Œä¼˜äºå•ä»£ç†åŸºçº¿ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åŸåˆ™æ€§ä¿¡æ¯æµåŠ¨è®¾è®¡åœ¨å¤šä»£ç†ç³»ç»Ÿä¸å¤§å‹è¯­è¨€æ¨¡å‹å…±åŒè¿›è¡Œä¸Šä¸‹æ–‡éšç§ä¿æŠ¤çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ¥è‡ªå¤šä¸ªæºçš„ä¸Šä¸‹æ–‡éšç§ä¿¡æ¯æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šä»£ç†æ¡†æ¶ï¼Œå°†éšç§æ¨ç†åˆ†è§£ä¸ºä¸“é—¨çš„å­ä»»åŠ¡ï¼Œå‡å°‘å•ä¸ªä»£ç†çš„ä¿¡æ¯è´Ÿè½½ã€‚</li>
<li>é€šè¿‡ç ”ç©¶ä¿¡æ¯æµåŠ¨æ‹“æ‰‘æ¥äº†è§£éšç§é”™è¯¯å¦‚ä½•äº§ç”Ÿå’Œæ‰©æ•£ã€‚</li>
<li>æ­ç¤ºäº†ä¸Šæ¸¸æ£€æµ‹é”™è¯¯å¦‚ä½•æ¼”å˜ä¸ºä¸‹æ¸¸æ³„éœ²çš„åŸå› ã€‚</li>
<li>åœ¨ConfAIdeå’ŒPrivacyLensåŸºå‡†æµ‹è¯•ä¸Šï¼Œå¤šä»£ç†é…ç½®æ˜¾è‘—å‡å°‘äº†ç§äººä¿¡æ¯æ³„éœ²ã€‚</li>
<li>å¤šä»£ç†é…ç½®åœ¨å‡å°‘ç§äººä¿¡æ¯æ³„éœ²çš„åŒæ—¶ï¼Œä¿æŒäº†å…¬å…±å†…å®¹çš„ä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-59887f98cc2c2daa146f890c1bb3877e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-38e2366ef585ad9ab8f0bae303ba73a3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9fd6aa00c0f832aab4dc49f4f445cd49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb22eb3084b01f69b8800284f5f333fd.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="GraphCoT-VLA-A-3D-Spatial-Aware-Reasoning-Vision-Language-Action-Model-for-Robotic-Manipulation-with-Ambiguous-Instructions"><a href="#GraphCoT-VLA-A-3D-Spatial-Aware-Reasoning-Vision-Language-Action-Model-for-Robotic-Manipulation-with-Ambiguous-Instructions" class="headerlink" title="GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model   for Robotic Manipulation with Ambiguous Instructions"></a>GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model   for Robotic Manipulation with Ambiguous Instructions</h2><p><strong>Authors:Helong Huang, Min Cen, Kai Tan, Xingyue Quan, Guowei Huang, Hong Zhang</strong></p>
<p>Vision-language-action models have emerged as a crucial paradigm in robotic manipulation. However, existing VLA models exhibit notable limitations in handling ambiguous language instructions and unknown environmental states. Furthermore, their perception is largely constrained to static two-dimensional observations, lacking the capability to model three-dimensional interactions between the robot and its environment. To address these challenges, this paper proposes GraphCoT-VLA, an efficient end-to-end model. To enhance the modelâ€™s ability to interpret ambiguous instructions and improve task planning, we design a structured Chain-of-Thought reasoning module that integrates high-level task understanding and planning, failed task feedback, and low-level imaginative reasoning about future object positions and robot actions. Additionally, we construct a real-time updatable 3D Pose-Object graph, which captures the spatial configuration of robot joints and the topological relationships between objects in 3D space, enabling the model to better understand and manipulate their interactions. We further integrates a dropout hybrid reasoning strategy to achieve efficient control outputs. Experimental results across multiple real-world robotic tasks demonstrate that GraphCoT-VLA significantly outperforms existing methods in terms of task success rate and response speed, exhibiting strong generalization and robustness in open environments and under uncertain instructions. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹å·²ç»æˆä¸ºæœºå™¨äººæ“ä½œä¸­çš„å…³é”®èŒƒå¼ã€‚ç„¶è€Œï¼Œç°æœ‰çš„VLAæ¨¡å‹åœ¨å¤„ç†æ¨¡ç³Šçš„è¯­éŸ³æŒ‡ä»¤å’ŒæœªçŸ¥çš„ç¯å¢ƒçŠ¶æ€æ—¶å­˜åœ¨æ˜æ˜¾çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œå®ƒä»¬çš„æ„ŸçŸ¥ä¸»è¦å±€é™äºé™æ€çš„äºŒç»´è§‚å¯Ÿï¼Œç¼ºä¹æœºå™¨äººä¸å…¶ç¯å¢ƒä¹‹é—´ä¸‰ç»´äº¤äº’çš„å»ºæ¨¡èƒ½åŠ›ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†GraphCoT-VLAè¿™ä¸€é«˜æ•ˆçš„ç«¯åˆ°ç«¯æ¨¡å‹ã€‚ä¸ºäº†å¢å¼ºæ¨¡å‹å¯¹æ¨¡ç³ŠæŒ‡ä»¤çš„è§£è¯»èƒ½åŠ›å’Œä»»åŠ¡è§„åˆ’èƒ½åŠ›ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç»“æ„åŒ–çš„æ€ç»´é“¾æ¨ç†æ¨¡å—ï¼Œè¯¥æ¨¡å—èåˆäº†é«˜çº§ä»»åŠ¡ç†è§£å’Œè§„åˆ’ã€ä»»åŠ¡å¤±è´¥åé¦ˆä»¥åŠå…³äºæœªæ¥ç‰©ä½“ä½ç½®å’Œæœºå™¨äººåŠ¨ä½œçš„åº•å±‚æ¨ç†æƒ³è±¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¯å®æ—¶æ›´æ–°çš„ä¸‰ç»´å§¿æ€ç‰©ä½“å›¾ï¼Œæ•æ‰æœºå™¨äººå…³èŠ‚çš„ç©ºé—´é…ç½®ä»¥åŠç‰©ä½“åœ¨ä¸‰ç»´ç©ºé—´ä¸­çš„æ‹“æ‰‘å…³ç³»ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œæ“ä½œç‰©ä½“é—´çš„äº¤äº’ã€‚æˆ‘ä»¬è¿˜é‡‡ç”¨äº†ä¸€ç§æ··åˆæ¨ç†ç­–ç•¥æ¥å®ç°æœ‰æ•ˆçš„æ§åˆ¶è¾“å‡ºã€‚åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæœºå™¨äººä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGraphCoT-VLAåœ¨ä»»åŠ¡æˆåŠŸç‡å’Œå“åº”é€Ÿåº¦æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨å¼€æ”¾ç¯å¢ƒå’Œä¸ç¡®å®šæŒ‡ä»¤ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07650v1">PDF</a> 10 pages, 6 figures</p>
<p><strong>Summary</strong><br>åŸºäºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„æœºå™¨äººæ“ä½œæŠ€æœ¯ä¸­ï¼ŒGraphCoT-VLAæ¨¡å‹èƒ½æœ‰æ•ˆè§£å†³ç°æœ‰æ¨¡å‹åœ¨å¤„ç†æ¨¡ç³Šè¯­è¨€æŒ‡ä»¤å’ŒæœªçŸ¥ç¯å¢ƒçŠ¶æ€æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡è®¾è®¡ç»“æ„åŒ–æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰æ¨ç†æ¨¡å—ï¼Œæå‡å¯¹æ¨¡ç³ŠæŒ‡ä»¤çš„è§£è¯»å’Œä»»åŠ¡è§„åˆ’èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜æ„å»ºäº†å¯å®æ—¶æ›´æ–°çš„ä¸‰ç»´å§¿æ€ç‰©ä½“å›¾ï¼ˆ3D Pose-Object graphï¼‰ï¼Œèƒ½æ•æ‰æœºå™¨äººå…³èŠ‚çš„ç©ºé—´é…ç½®å’Œç‰©ä½“é—´çš„æ‹“æ‰‘å…³ç³»ï¼Œä»è€Œæå‡æ¨¡å‹å¯¹ç‰©ä½“äº¤äº’çš„ç†è§£å’Œæ“ä½œèƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒGraphCoT-VLAåœ¨å¤šä¸ªçœŸå®æœºå™¨äººä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä»»åŠ¡æˆåŠŸç‡å’Œå“åº”é€Ÿåº¦å‡æœ‰æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰Vision-Language-Actionï¼ˆVLAï¼‰æ¨¡å‹åœ¨å¤„ç†æ¨¡ç³Šè¯­è¨€æŒ‡ä»¤å’ŒæœªçŸ¥ç¯å¢ƒçŠ¶æ€æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>GraphCoT-VLAæ¨¡å‹é€šè¿‡è®¾è®¡ç»“æ„åŒ–æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰æ¨ç†æ¨¡å—ï¼Œæå‡å¯¹æ¨¡ç³ŠæŒ‡ä»¤çš„è§£è¯»å’Œä»»åŠ¡è§„åˆ’èƒ½åŠ›ã€‚</li>
<li>GraphCoT-VLAæ„å»ºäº†å¯å®æ—¶æ›´æ–°çš„ä¸‰ç»´å§¿æ€ç‰©ä½“å›¾ï¼ˆ3D Pose-Object graphï¼‰ï¼Œèƒ½æ•æ‰æœºå™¨äººå…³èŠ‚çš„ç©ºé—´é…ç½®å’Œç‰©ä½“é—´çš„æ‹“æ‰‘å…³ç³»ã€‚</li>
<li>æ¨¡å‹å…·å¤‡æ›´å¥½çš„ç‰©ä½“äº¤äº’ç†è§£å’Œæ“ä½œèƒ½åŠ›ã€‚</li>
<li>GraphCoT-VLAåœ¨å¤šä¸ªçœŸå®æœºå™¨äººä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä»»åŠ¡æˆåŠŸç‡å’Œå“åº”é€Ÿåº¦å‡æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>GraphCoT-VLAæ¨¡å‹é€šè¿‡æ•´åˆé«˜å±‚æ¬¡çš„ä»»åŠ¡ç†è§£å’Œè§„åˆ’ã€å¤±è´¥ä»»åŠ¡åé¦ˆä»¥åŠæœªæ¥ç‰©ä½“ä½ç½®å’Œæœºå™¨äººåŠ¨ä½œçš„ä½ä½æƒ³è±¡æ¨ç†ï¼Œæé«˜äº†æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07650">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ba7e60610cf82958be15df45bb69f868.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b9eecad3318d9b8d3b69662a3ab2c82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab87b8336bf376c3f1d13ca115dd4e2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b474feb0d30c202b3b6ccab0e4e9454.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2397179ba93225aeeb2ef69a89bf9a33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7e9845ad83e0cc7db1a8bb05276e4ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f18a69731f790ceca8a9371d00b6a17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-232546a85488a49eef9ace3e48f96ad9.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="InterChart-Benchmarking-Visual-Reasoning-Across-Decomposed-and-Distributed-Chart-Information"><a href="#InterChart-Benchmarking-Visual-Reasoning-Across-Decomposed-and-Distributed-Chart-Information" class="headerlink" title="InterChart: Benchmarking Visual Reasoning Across Decomposed and   Distributed Chart Information"></a>InterChart: Benchmarking Visual Reasoning Across Decomposed and   Distributed Chart Information</h2><p><strong>Authors:Anirudh Iyengar Kaniyar Narayana Iyengar, Srija Mukhopadhyay, Adnan Qidwai, Shubhankar Singh, Dan Roth, Vivek Gupta</strong></p>
<p>We introduce InterChart, a diagnostic benchmark that evaluates how well vision-language models (VLMs) reason across multiple related charts, a task central to real-world applications such as scientific reporting, financial analysis, and public policy dashboards. Unlike prior benchmarks focusing on isolated, visually uniform charts, InterChart challenges models with diverse question types ranging from entity inference and trend correlation to numerical estimation and abstract multi-step reasoning grounded in 2-3 thematically or structurally related charts. We organize the benchmark into three tiers of increasing difficulty: (1) factual reasoning over individual charts, (2) integrative analysis across synthetically aligned chart sets, and (3) semantic inference over visually complex, real-world chart pairs. Our evaluation of state-of-the-art open and closed-source VLMs reveals consistent and steep accuracy declines as chart complexity increases. We find that models perform better when we decompose multi-entity charts into simpler visual units, underscoring their struggles with cross-chart integration. By exposing these systematic limitations, InterChart provides a rigorous framework for advancing multimodal reasoning in complex, multi-visual environments. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†InterChartï¼Œè¿™æ˜¯ä¸€ä¸ªè¯Šæ–­åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¤šå¼ ç›¸å…³å›¾è¡¨ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚è¿™ä¸€ä»»åŠ¡æ˜¯ç°å®ä¸–ç•Œåº”ç”¨ï¼ˆå¦‚ç§‘å­¦æŠ¥å‘Šã€é‡‘èåˆ†æå’Œå…¬å…±æ”¿ç­–ä»ªè¡¨æ¿ï¼‰çš„æ ¸å¿ƒã€‚ä¸ä»¥å¾€ä¾§é‡äºå­¤ç«‹ã€è§†è§‰ç»Ÿä¸€çš„å›¾è¡¨çš„åŸºå‡†æµ‹è¯•ä¸åŒï¼ŒInterCharté€šè¿‡å¤šæ ·çš„é—®é¢˜ç±»å‹ï¼ˆå¦‚å®ä½“æ¨ç†ã€è¶‹åŠ¿ç›¸å…³æ€§ã€æ•°å€¼ä¼°ç®—å’ŒåŸºäºä¸¤ä¸‰ä¸ªä¸»é¢˜æˆ–ç»“æ„ä¸Šç›¸å…³å›¾è¡¨çš„æŠ½è±¡å¤šæ­¥éª¤æ¨ç†ç­‰ï¼‰æ¥æŒ‘æˆ˜æ¨¡å‹ã€‚æˆ‘ä»¬å°†åŸºå‡†æµ‹è¯•åˆ†ä¸ºä¸‰ä¸ªéš¾åº¦é€’å¢çš„å±‚æ¬¡ï¼šï¼ˆ1ï¼‰å•ä¸ªå›¾è¡¨çš„æ¨ç†ï¼Œï¼ˆ2ï¼‰åˆæˆå›¾è¡¨é›†çš„ç»¼åˆåˆ†æï¼Œï¼ˆ3ï¼‰è§†è§‰ä¸Šå¤æ‚ã€çœŸå®ä¸–ç•Œçš„å›¾è¡¨å¯¹çš„è¯­ä¹‰æ¨ç†ã€‚æˆ‘ä»¬å¯¹æœ€æ–°å¼€æºå’Œé—­æºçš„VLMè¯„ä¼°æ˜¾ç¤ºï¼Œéšç€å›¾è¡¨å¤æ‚æ€§çš„å¢åŠ ï¼Œå‡†ç¡®æ€§æŒç»­ä¸”æ€¥å‰§ä¸‹é™ã€‚æˆ‘ä»¬å‘ç°ï¼Œå½“æˆ‘ä»¬å°†å¤šå®ä½“å›¾è¡¨åˆ†è§£ä¸ºæ›´ç®€å•çš„è§†è§‰å•å…ƒæ—¶ï¼Œæ¨¡å‹çš„æ€§èƒ½æ›´å¥½ï¼Œè¿™çªå‡ºäº†å®ƒä»¬åœ¨è·¨å›¾è¡¨æ•´åˆæ–¹é¢çš„å›°éš¾ã€‚é€šè¿‡æ­ç¤ºè¿™äº›ç³»ç»Ÿå±€é™ï¼ŒInterChartä¸ºå¤æ‚å¤šè§†è§‰ç¯å¢ƒä¸­å¤šæ¨¡å¼æ¨ç†çš„è¿›æ­¥æä¾›äº†ä¸¥æ ¼æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07630v1">PDF</a> 18 pages, 6 figures, 12 tables. Benchmark dataset and evaluation code   will be publicly made available</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æˆ‘ä»¬ä»‹ç»äº†InterChartè¿™ä¸€è¯Šæ–­åŸºå‡†æµ‹è¯•ï¼Œå®ƒæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šå¼ ç›¸å…³å›¾è¡¨ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æµ‹è¯•æ¶µç›–å¤šç§ç±»å‹çš„å›¾è¡¨é—®é¢˜ï¼ŒåŒ…æ‹¬å®ä½“æ¨æ–­ã€è¶‹åŠ¿å…³è”ã€æ•°å€¼ä¼°ç®—å’ŒåŸºäºä¸¤åˆ°ä¸‰å¼ ä¸»é¢˜æˆ–ç»“æ„ä¸Šç›¸å…³å›¾è¡¨çš„æŠ½è±¡å¤šæ­¥éª¤æ¨ç†ç­‰ã€‚åŸºå‡†æµ‹è¯•åˆ†ä¸ºä¸‰ä¸ªéš¾åº¦é€’å¢çš„å±‚æ¬¡ï¼šä¸ªäººå›¾è¡¨çš„æ¨ç†èƒ½åŠ›ã€åˆæˆå›¾è¡¨é›†çš„æ•´åˆåˆ†æä»¥åŠè§†è§‰å¤æ‚å›¾è¡¨å¯¹çš„è¯­ä¹‰æ¨ç†èƒ½åŠ›ã€‚å¯¹æœ€å…ˆè¿›çš„å¼€æ”¾å’Œå°é—­æºä»£ç è§†è§‰è¯­è¨€æ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œéšç€å›¾è¡¨å¤æ‚æ€§çš„å¢åŠ ï¼Œå‡†ç¡®æ€§æŒç»­ä¸”æ€¥å‰§ä¸‹é™ã€‚æˆ‘ä»¬å‘ç°å°†å¤šå®ä½“å›¾è¡¨åˆ†è§£ä¸ºæ›´ç®€å•çš„è§†è§‰å•å…ƒå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œçªæ˜¾å‡ºè·¨å›¾è¡¨æ•´åˆæ–¹é¢çš„å›°éš¾ã€‚é€šè¿‡æ­ç¤ºè¿™äº›ç³»ç»Ÿæ€§å±€é™ï¼ŒInterChartä¸ºå¤æ‚å¤šè§†è§‰ç¯å¢ƒä¸­çš„å¤šæ¨¡æ€æ¨ç†æä¾›äº†ä¸€ä¸ªä¸¥æ ¼çš„æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>InterChartæ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šå¼ ç›¸å…³å›¾è¡¨ä¸Šæ¨ç†èƒ½åŠ›çš„è¯Šæ–­åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å®ƒæ¶µç›–äº†å¤šç§ç±»å‹çš„å›¾è¡¨é—®é¢˜ï¼ŒåŒ…æ‹¬å®ä½“æ¨æ–­ã€è¶‹åŠ¿å…³è”ç­‰ã€‚</li>
<li>åŸºå‡†æµ‹è¯•åˆ†ä¸ºä¸‰ä¸ªå±‚æ¬¡ï¼Œéš¾åº¦é€’å¢ã€‚</li>
<li>éšç€å›¾è¡¨å¤æ‚æ€§çš„å¢åŠ ï¼Œç°æœ‰æ¨¡å‹çš„å‡†ç¡®æ€§æ€¥å‰§ä¸‹é™ã€‚</li>
<li>å°†å¤šå®ä½“å›¾è¡¨åˆ†è§£ä¸ºæ›´ç®€å•çš„è§†è§‰å•å…ƒå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>InterChartæ­ç¤ºäº†è·¨å›¾è¡¨æ•´åˆæ–¹é¢çš„å›°éš¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e6b7542a1682200998776c7189f020c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ff7efbbf67f1d42b6013abddfdd4cee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e85551ef20a32ef49808c74ed1f8329d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31a05086f7760686d76ef4f4c08d07dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-461f3b74639c97f1b7178d003ba6537d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-336b8db91dc0ef3b946cb02f0a901381.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-10b2ecb8600b87ee98c69ad5c9d2b437.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb018cf332e64e269c482ce53d039512.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Klear-Reasoner-Advancing-Reasoning-Capability-via-Gradient-Preserving-Clipping-Policy-Optimization"><a href="#Klear-Reasoner-Advancing-Reasoning-Capability-via-Gradient-Preserving-Clipping-Policy-Optimization" class="headerlink" title="Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving   Clipping Policy Optimization"></a>Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving   Clipping Policy Optimization</h2><p><strong>Authors:Zhenpeng Su, Leiyu Pan, Xue Bai, Dening Liu, Guanting Dong, Jiaming Huang, Wenping Hu, Guorui Zhou</strong></p>
<p>We present Klear-Reasoner, a model with long reasoning capabilities that demonstrates careful deliberation during problem solving, achieving outstanding performance across multiple benchmarks. Although there are already many excellent works related to inference models in the current community, there are still many problems with reproducing high-performance inference models due to incomplete disclosure of training details. This report provides an in-depth analysis of the reasoning model, covering the entire post-training workflow from data preparation and long Chain-of-Thought supervised fine-tuning (long CoT SFT) to reinforcement learning (RL), along with detailed ablation studies for each experimental component. For SFT data, our experiments show that a small number of high-quality data sources are more effective than a large number of diverse data sources, and that difficult samples can achieve better results without accuracy filtering. In addition, we investigate two key issues with current clipping mechanisms in RL: Clipping suppresses critical exploration signals and ignores suboptimal trajectories. To address these challenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO) that gently backpropagates gradients from clipped tokens. GPPO not only enhances the modelâ€™s exploration capacity but also improves its efficiency in learning from negative samples. Klear-Reasoner exhibits exceptional reasoning abilities in mathematics and programming, scoring 90.5% on AIME 2024, 83.2% on AIME 2025, 66.0% on LiveCodeBench V5 and 58.1% on LiveCodeBench V6. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Klear-Reasonerï¼Œè¿™æ˜¯ä¸€æ¬¾å…·æœ‰é•¿æœŸæ¨ç†èƒ½åŠ›çš„æ¨¡å‹ï¼Œåœ¨è§£å†³é—®é¢˜æ—¶å±•ç°å‡ºè°¨æ…çš„è€ƒè™‘ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å°½ç®¡å½“å‰ç¤¾åŒºå·²ç»æœ‰å¾ˆå¤šä¸æ¨ç†æ¨¡å‹ç›¸å…³çš„ä¼˜ç§€ä½œå“ï¼Œä½†ç”±äºåŸ¹è®­ç»†èŠ‚æŠ«éœ²ä¸å®Œæ•´ï¼Œå› æ­¤é‡ç°é«˜æ€§èƒ½æ¨ç†æ¨¡å‹ä»ç„¶å­˜åœ¨è®¸å¤šé—®é¢˜ã€‚æœ¬æŠ¥å‘Šå¯¹æ¨ç†æ¨¡å‹è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œæ¶µç›–äº†ä»æ•°æ®å‡†å¤‡å’Œé•¿é“¾æ€ç»´ç›‘ç£å¾®è°ƒï¼ˆlong CoT SFTï¼‰åˆ°å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ•´ä¸ªè®­ç»ƒåå·¥ä½œæµç¨‹ï¼Œå¹¶å¯¹æ¯ä¸ªå®éªŒç»„ä»¶è¿›è¡Œäº†è¯¦ç»†çš„æ¶ˆèç ”ç©¶ã€‚å¯¹äºSFTæ•°æ®ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå°‘æ•°é«˜è´¨é‡çš„æ•°æ®æºæ¯”å¤§é‡å¤šæ ·çš„æ•°æ®æºæ›´æœ‰æ•ˆï¼Œè€Œä¸”å›°éš¾æ ·æœ¬å¯ä»¥åœ¨æ— éœ€ç²¾åº¦è¿‡æ»¤çš„æƒ…å†µä¸‹å®ç°æ›´å¥½çš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å½“å‰å¼ºåŒ–å­¦ä¹ ä¸­çš„è£å‰ªæœºåˆ¶çš„ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šè£å‰ªä¼šæŠ‘åˆ¶å…³é”®æ¢ç´¢ä¿¡å·å¹¶å¿½ç•¥æ¬¡ä¼˜è½¨è¿¹ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ¢¯åº¦ä¿æŒè£å‰ªç­–ç•¥ä¼˜åŒ–ï¼ˆGPPOï¼‰ï¼Œè¯¥ä¼˜åŒ–èƒ½æ¸©å’Œåœ°åå‘ä¼ æ’­è¢«è£å‰ªä»¤ç‰Œçš„æ¢¯åº¦ã€‚GPPOä¸ä»…æé«˜äº†æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›ï¼Œè€Œä¸”æé«˜äº†å…¶ä»è´Ÿæ ·æœ¬ä¸­å­¦ä¹ çš„æ•ˆç‡ã€‚Klear-Reasoneråœ¨æ•°å­¦å’Œç¼–ç¨‹æ–¹é¢å±•ç°å‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨AIME 2024ä¸Šå¾—åˆ†90.5%ï¼Œåœ¨AIME 2025ä¸Šå¾—åˆ†83.2%ï¼Œåœ¨LiveCodeBench V5ä¸Šå¾—åˆ†66.0%ï¼Œåœ¨LiveCodeBench V6ä¸Šå¾—åˆ†58.1%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07629v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºKlear-Reasoneræ¨¡å‹çš„æ·±å…¥ç ”ç©¶ï¼Œè¯¥æ¨¡å‹å…·å¤‡å‡ºè‰²çš„é•¿æœŸæ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æŠ¥å‘Šè¯¦ç»†åˆ†æäº†æ¨ç†æ¨¡å‹ï¼Œä»å¤´å¼€å§‹ä»‹ç»äº†æ•°æ®å‡†å¤‡å’Œé•¿æœŸæ€ç»´é“¾ç›‘ç£ç²¾ç»†è°ƒæ•´ï¼ˆlong CoT SFTï¼‰åˆ°å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ•´ä¸ªè®­ç»ƒæµç¨‹ï¼Œå¹¶å¯¹æ¯ä¸ªå®éªŒç»„ä»¶è¿›è¡Œäº†è¯¦ç»†çš„æ¶ˆèç ”ç©¶ã€‚å®éªŒè¡¨æ˜ï¼Œå°‘é‡é«˜è´¨é‡æ•°æ®æºæ¯”å¤§é‡å¤šæ ·åŒ–æ•°æ®æºæ›´æœ‰æ•ˆï¼Œä¸”å›°éš¾æ ·æœ¬æ— éœ€è¿‡æ»¤å³å¯è·å¾—æ›´å¥½çš„ç»“æœã€‚é’ˆå¯¹å½“å‰RLä¸­è£å‰ªæœºåˆ¶çš„ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼Œæå‡ºäº†æ¢¯åº¦ä¿ç•™è£å‰ªç­–ç•¥ä¼˜åŒ–ï¼ˆGPPOï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯æ¸©å’Œåœ°åå‘ä¼ æ’­è¢«è£å‰ªä»¤ç‰Œçš„æ¢¯åº¦ã€‚GPPOä¸ä»…æé«˜äº†æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›ï¼Œè¿˜æé«˜äº†å…¶ä»è´Ÿæ ·æœ¬ä¸­å­¦ä¹ çš„æ•ˆç‡ã€‚Klear-Reasoneråœ¨æ•°å­¦å’Œç¼–ç¨‹æ–¹é¢å±•ç°å‡ºéå‡¡çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨AIME 2024ã€AIME 2025å’ŒLiveCodeBench V5&#x2F;V6ç­‰è€ƒè¯•ä¸­å–å¾—æ˜¾è‘—æˆç»©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Klear-Reasoneræ˜¯ä¸€ä¸ªå…·æœ‰é•¿æœŸæ¨ç†èƒ½åŠ›çš„æ¨¡å‹ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šã€‚</li>
<li>æŠ¥å‘Šæä¾›äº†ä»æ•°æ®å‡†å¤‡åˆ°å¼ºåŒ–å­¦ä¹ çš„å…¨é¢è®­ç»ƒæµç¨‹åˆ†æã€‚</li>
<li>å®éªŒæ˜¾ç¤ºå°‘é‡é«˜è´¨é‡æ•°æ®æºæ¯”å¤§é‡å¤šæ ·åŒ–æ•°æ®æºæ›´æœ‰æ•ˆã€‚</li>
<li>å›°éš¾æ ·æœ¬æ— éœ€è¿‡æ»¤å³å¯è·å¾—æ›´å¥½çš„ç»“æœã€‚</li>
<li>é’ˆå¯¹RLä¸­çš„è£å‰ªæœºåˆ¶é—®é¢˜ï¼Œæå‡ºäº†GPPOæ–¹æ³•ä»¥æé«˜æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›å’Œä»è´Ÿæ ·æœ¬ä¸­å­¦ä¹ æ•ˆç‡ã€‚</li>
<li>Klear-Reasoneråœ¨æ•°å­¦å’Œç¼–ç¨‹æ–¹é¢å±•ç°å‡ºéå‡¡çš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07629">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9a06e5ed042d5db484f4427a7f20a944.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="AR-VRM-Imitating-Human-Motions-for-Visual-Robot-Manipulation-with-Analogical-Reasoning"><a href="#AR-VRM-Imitating-Human-Motions-for-Visual-Robot-Manipulation-with-Analogical-Reasoning" class="headerlink" title="AR-VRM: Imitating Human Motions for Visual Robot Manipulation with   Analogical Reasoning"></a>AR-VRM: Imitating Human Motions for Visual Robot Manipulation with   Analogical Reasoning</h2><p><strong>Authors:Dejie Yang, Zijing Zhao, Yang Liu</strong></p>
<p>Visual Robot Manipulation (VRM) aims to enable a robot to follow natural language instructions based on robot states and visual observations, and therefore requires costly multi-modal data. To compensate for the deficiency of robot data, existing approaches have employed vision-language pretraining with large-scale data. However, they either utilize web data that differs from robotic tasks, or train the model in an implicit way (e.g., predicting future frames at the pixel level), thus showing limited generalization ability under insufficient robot data. In this paper, we propose to learn from large-scale human action video datasets in an explicit way (i.e., imitating human actions from hand keypoints), introducing Visual Robot Manipulation with Analogical Reasoning (AR-VRM). To acquire action knowledge explicitly from human action videos, we propose a keypoint Vision-Language Model (VLM) pretraining scheme, enabling the VLM to learn human action knowledge and directly predict human hand keypoints. During fine-tuning on robot data, to facilitate the robotic arm in imitating the action patterns of human motions, we first retrieve human action videos that perform similar manipulation tasks and have similar historical observations , and then learn the Analogical Reasoning (AR) map between human hand keypoints and robot components. Taking advantage of focusing on action keypoints instead of irrelevant visual cues, our method achieves leading performance on the CALVIN benchmark {and real-world experiments}. In few-shot scenarios, our AR-VRM outperforms previous methods by large margins , underscoring the effectiveness of explicitly imitating human actions under data scarcity. </p>
<blockquote>
<p>è§†è§‰æœºå™¨äººæ“æ§ï¼ˆVRMï¼‰çš„ç›®æ ‡æ˜¯ä½¿æœºå™¨äººèƒ½å¤Ÿæ ¹æ®æœºå™¨äººçŠ¶æ€å’Œè§†è§‰è§‚å¯Ÿæ¥æ‰§è¡Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå› æ­¤éœ€è¦æ˜‚è´µçš„å¤šæ¨¡å¼æ•°æ®ã€‚ä¸ºäº†å¼¥è¡¥æœºå™¨äººæ•°æ®çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å·²ç»é‡‡ç”¨äº†å¤§è§„æ¨¡æ•°æ®çš„è§†è§‰è¯­è¨€é¢„è®­ç»ƒã€‚ç„¶è€Œï¼Œå®ƒä»¬è¦ä¹ˆä½¿ç”¨ä¸æœºå™¨äººä»»åŠ¡ä¸åŒçš„ç½‘ç»œæ•°æ®ï¼Œè¦ä¹ˆä»¥éšå¼æ–¹å¼è®­ç»ƒæ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œåœ¨åƒç´ çº§åˆ«é¢„æµ‹æœªæ¥å¸§ï¼‰ï¼Œå› æ­¤åœ¨æœºå™¨äººæ•°æ®ä¸è¶³çš„æƒ…å†µä¸‹ï¼Œæ˜¾ç¤ºå‡ºæœ‰é™çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºä»¥æ˜ç¡®çš„æ–¹å¼ä»å¤§è§„æ¨¡äººç±»è¡Œä¸ºè§†é¢‘æ•°æ®é›†ä¸­è¿›è¡Œå­¦ä¹ ï¼ˆå³é€šè¿‡æ‰‹å…³é”®ç‚¹æ¨¡ä»¿äººç±»è¡Œä¸ºï¼‰ï¼Œå¼•å…¥ç±»æ¯”æ¨ç†è§†è§‰æœºå™¨äººæ“æ§ï¼ˆAR-VRMï¼‰ã€‚ä¸ºäº†ä»äººç±»è¡Œä¸ºè§†é¢‘ä¸­æ˜ç¡®è·å–è¡ŒåŠ¨çŸ¥è¯†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…³é”®ç‚¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é¢„è®­ç»ƒæ–¹æ¡ˆï¼Œä½¿VLMèƒ½å¤Ÿå­¦ä¹ äººç±»è¡Œä¸ºçŸ¥è¯†å¹¶ç›´æ¥é¢„æµ‹äººç±»æ‰‹å…³é”®ç‚¹ã€‚åœ¨æœºå™¨äººæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒæ—¶ï¼Œä¸ºäº†ä¿ƒè¿›æœºæ¢°è‡‚æ¨¡ä»¿äººç±»è¿åŠ¨çš„è¡Œä¸ºæ¨¡å¼ï¼Œæˆ‘ä»¬é¦–å…ˆæ£€ç´¢æ‰§è¡Œç±»ä¼¼æ“æ§ä»»åŠ¡å¹¶å…·æœ‰ç›¸ä¼¼å†å²è§‚å¯Ÿè®°å½•çš„äººç±»è¡Œä¸ºè§†é¢‘ï¼Œç„¶åå­¦ä¹ äººç±»æ‰‹å…³é”®ç‚¹å’Œæœºå™¨äººç»„ä»¶ä¹‹é—´çš„ç±»æ¯”æ¨ç†ï¼ˆARï¼‰æ˜ å°„ã€‚é€šè¿‡å…³æ³¨åŠ¨ä½œå…³é”®ç‚¹è€Œä¸æ˜¯æ— å…³çš„è§†è§‰çº¿ç´¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨CALVINåŸºå‡†æµ‹è¯•{å’ŒçœŸå®ä¸–ç•Œå®éªŒ}ä¸­å–å¾—äº†é¢†å…ˆçš„æˆç»©ã€‚åœ¨å°‘æ•°åœºæ™¯çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„AR-VRMå¤§å¹…è¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ï¼Œçªæ˜¾äº†åœ¨æ•°æ®ç¨€ç¼ºæƒ…å†µä¸‹æ˜ç¡®æ¨¡ä»¿äººç±»åŠ¨ä½œçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07626v1">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong></p>
<p>è§†è§‰æœºå™¨äººæ“æ§ï¼ˆVRMï¼‰æ—¨åœ¨ä½¿æœºå™¨äººèƒ½å¤Ÿæ ¹æ®æœºå™¨äººçŠ¶æ€ä¸è§†è§‰è§‚å¯Ÿæ¥æ‰§è¡Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå› æ­¤éœ€ä¾èµ–æ˜‚è´µçš„å¤šæ¨¡æ€æ•°æ®ã€‚ä¸ºå¼¥è¡¥æœºå™¨äººæ•°æ®çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•é‡‡ç”¨è§†è§‰è¯­è¨€é¢„è®­ç»ƒç»“åˆå¤§è§„æ¨¡æ•°æ®çš„æ–¹å¼ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä½¿ç”¨ä¸æœºå™¨äººä»»åŠ¡ä¸åŒçš„ç½‘ç»œæ•°æ®æˆ–é‡‡ç”¨éšå¼è®­ç»ƒæ–¹æ³•ï¼ˆå¦‚é¢„æµ‹æœªæ¥å¸§åƒç´ çº§åˆ«ï¼‰ï¼Œåœ¨æœºå™¨äººæ•°æ®ä¸è¶³æ—¶è¡¨ç°å‡ºæœ‰é™çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºä»å¤§è§„æ¨¡äººç±»åŠ¨ä½œè§†é¢‘æ•°æ®é›†ä¸­æ˜¾å¼å­¦ä¹ çš„æ–¹æ³•ï¼ˆå³æ ¹æ®æ‰‹éƒ¨å…³é”®ç‚¹æ¨¡ä»¿äººç±»åŠ¨ä½œï¼‰ï¼Œå¼•å…¥å…·æœ‰ç±»æ¯”æ¨ç†ï¼ˆARï¼‰çš„è§†è§‰æœºå™¨äººæ“æ§ï¼ˆAR-VRMï¼‰ã€‚æˆ‘ä»¬ä»äººç±»åŠ¨ä½œè§†é¢‘ä¸­æ˜¾å¼è·å–åŠ¨ä½œçŸ¥è¯†ï¼Œå¹¶æå‡ºä¸€ç§å…³é”®ç‚¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„é¢„è®­ç»ƒæ–¹æ¡ˆï¼Œä½¿VLMèƒ½å¤Ÿå­¦ä¹ äººç±»åŠ¨ä½œçŸ¥è¯†å¹¶ç›´æ¥é¢„æµ‹äººæ‰‹å…³é”®ç‚¹ã€‚åœ¨æœºå™¨äººæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒæ—¶ï¼Œä¸ºå¸®åŠ©æœºæ¢°è‡‚æ¨¡ä»¿äººç±»åŠ¨ä½œæ¨¡å¼ï¼Œæˆ‘ä»¬é¦–å…ˆæ£€ç´¢æ‰§è¡Œç±»ä¼¼æ“æ§ä»»åŠ¡å¹¶å…·æœ‰ç›¸ä¼¼å†å²è§‚å¯Ÿè®°å½•çš„äººç±»åŠ¨ä½œè§†é¢‘ï¼Œç„¶åå­¦ä¹ äººæ‰‹å…³é”®ç‚¹ä¸æœºå™¨äººç»„ä»¶ä¹‹é—´çš„ç±»æ¯”æ¨ç†ï¼ˆARï¼‰æ˜ å°„ã€‚é€šè¿‡å…³æ³¨åŠ¨ä½œå…³é”®ç‚¹è€Œéæ— å…³çš„è§†è§‰çº¿ç´¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨CALVINåŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œå®éªŒä¸­å–å¾—äº†é¢†å…ˆçš„è¡¨ç°ã€‚åœ¨å°‘é‡åœºæ™¯æ•°æ®ä¸‹ï¼Œæˆ‘ä»¬çš„AR-VRMè¾ƒä¹‹å‰çš„æ–¹æ³•æœ‰è¾ƒå¤§ä¼˜åŠ¿ï¼Œçªæ˜¾äº†åœ¨æ•°æ®ç¨€ç¼ºæ—¶æ˜¾å¼æ¨¡ä»¿äººç±»åŠ¨ä½œçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VRMéœ€è¦å¤šæ¨¡æ€æ•°æ®æ¥ä½¿æœºå™¨äººæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ‰§è¡Œæ“ä½œã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä½¿ç”¨ç½‘ç»œæ•°æ®ä¸æœºå™¨äººä»»åŠ¡ä¸åŒæˆ–éšå¼è®­ç»ƒæ¨¡å‹ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›å—é™ã€‚</li>
<li>æœ¬æ–‡æå‡ºä»å¤§è§„æ¨¡äººç±»åŠ¨ä½œè§†é¢‘æ•°æ®ä¸­æ˜¾å¼å­¦ä¹ çš„æ–¹æ³•ï¼Œå³AR-VRMã€‚</li>
<li>é€šè¿‡å…³é”®ç‚¹VLMé¢„è®­ç»ƒæ–¹æ¡ˆï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ äººç±»åŠ¨ä½œçŸ¥è¯†å¹¶é¢„æµ‹æ‰‹éƒ¨å…³é”®ç‚¹ã€‚</li>
<li>åœ¨å¾®è°ƒé˜¶æ®µï¼Œåˆ©ç”¨ç±»æ¯”æ¨ç†ï¼ˆARï¼‰æ˜ å°„æœºå™¨äººä¸äººç±»çš„åŠ¨ä½œæ¨¡å¼ã€‚</li>
<li>æ–¹æ³•ä¸“æ³¨äºåŠ¨ä½œå…³é”®ç‚¹è€Œéæ— å…³è§†è§‰çº¿ç´¢ï¼Œåœ¨åŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œå®éªŒä¸­è¡¨ç°é¢†å…ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07626">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9f749d0224f483b5590b7b39f11cab8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0bdcde10a1102ff93eedeeff1289f7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-102edc52aef97d2cebab71cde41934ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4484466e0d552d29d56193e0d2235468.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="ThinkTuning-Instilling-Cognitive-Reflections-without-Distillation"><a href="#ThinkTuning-Instilling-Cognitive-Reflections-without-Distillation" class="headerlink" title="ThinkTuning: Instilling Cognitive Reflections without Distillation"></a>ThinkTuning: Instilling Cognitive Reflections without Distillation</h2><p><strong>Authors:Aswin RRV, Jacob Dineen, Divij Handa, Md Nayem Uddin, Mihir Parmar, Chitta Baral, Ben Zhou</strong></p>
<p>Recent advances in test-time scaling have led to the emergence of thinking LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL drives this self-improvement paradigm, a recent study (Gandhi et al., 2025) shows that RL alone does not truly instill these new reasoning abilities - it merely draws out behaviors already present in the base models. This raises a question: How can we train the models that donâ€™t exhibit such thinking behavior to develop it in the first place? To this end, we propose ThinkTuning, a GRPO-based interactive training approach where we augment the rollouts of a student model with the guidance from a teacher model. A simple idea from classroom practice inspires our method: a teacher poses a problem, lets the student try an answer, then gives corrective feedback â€“ enough to point the mind in the right direction and then show the solution. Each piece of feedback reshapes the studentâ€™s thoughts, leading them to arrive at the correct solution. Similarly, we find that this type of implicit supervision through feedback from a teacher model of the same size improves the reasoning capabilities of the student model. In particular, on average, our method shows a 3.85% improvement over zero-shot baselines across benchmarks, and on MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements over the vanilla-GRPO baseline. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/3rdAT/ThinkTuning">https://github.com/3rdAT/ThinkTuning</a>. </p>
<blockquote>
<p>æœ€è¿‘æµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹é¢çš„è¿›å±•å‚¬ç”Ÿå‡ºäº†å±•ç°è‡ªæˆ‘åæ€è¡Œä¸ºå’Œå¤šæ­¥æ¨ç†çš„æ€è€ƒå‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¨åŠ¨äº†è¿™ç§è‡ªæˆ‘æ”¹è¿›çš„æ¨¡å¼ï¼Œä½†æœ€è¿‘çš„ä¸€é¡¹ç ”ç©¶ï¼ˆç”˜åœ°ç­‰äººï¼Œ2025ï¼‰è¡¨æ˜ï¼Œä»…ä¾é RLå¹¶ä¸èƒ½çœŸæ­£èµ‹äºˆè¿™äº›æ–°çš„æ¨ç†èƒ½åŠ›â€”â€”å®ƒåªæ˜¯æ¿€å‘å‡ºåŸºç¡€æ¨¡å‹ä¸­å·²ç»å­˜åœ¨çš„è¡Œä¸ºã€‚è¿™å°±æå‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼šå¦‚ä½•è®­ç»ƒé‚£äº›åŸæœ¬ä¸å…·å¤‡è¿™ç§æ€è€ƒè¡Œä¸ºçš„æ¨¡å‹ï¼Œè®©å®ƒä»¬é¦–å…ˆå…·å¤‡è¿™ç§èƒ½åŠ›å‘¢ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ThinkTuningï¼Œè¿™æ˜¯ä¸€ç§åŸºäºGRPOçš„äº¤äº’å¼è®­ç»ƒæ–¹æ³•ï¼Œæˆ‘ä»¬å€ŸåŠ©æ•™å¸ˆæ¨¡å‹çš„æŒ‡å¯¼æ¥å¢å¼ºå­¦ç”Ÿæ¨¡å‹çš„æ»šåŠ¨è¾“å‡ºã€‚æˆ‘ä»¬çš„æ–¹æ³•çµæ„Ÿæ¥è‡ªäºè¯¾å ‚å®è·µçš„ç®€å•æƒ³æ³•ï¼šæ•™å¸ˆæå‡ºé—®é¢˜ï¼Œè®©å­¦ç”Ÿå°è¯•å›ç­”ï¼Œç„¶åç»™å‡ºçº æ­£åé¦ˆâ€”â€”è¶³ä»¥æŒ‡å‡ºæ­£ç¡®çš„æ–¹å‘å¹¶å±•ç¤ºè§£å†³æ–¹æ¡ˆã€‚æ¯ä¸€ä»½åé¦ˆéƒ½ä¼šé‡å¡‘å­¦ç”Ÿçš„æ€è·¯ï¼Œå¼•å¯¼ä»–ä»¬æ‰¾åˆ°æ­£ç¡®çš„ç­”æ¡ˆã€‚åŒæ ·ï¼Œæˆ‘ä»¬å‘ç°ï¼Œé€šè¿‡æ•™å¸ˆæ¨¡å‹çš„åé¦ˆè¿›è¡Œè¿™ç§éšå¼ç›‘ç£ï¼Œå¯ä»¥æé«˜å­¦ç”Ÿæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡è¡¨ç°æ¯”é›¶åŸºå‡†çº¿é«˜å‡º3.85%ï¼Œåœ¨MATH-500ã€AIMEå’ŒGPQA-Diamondä¸Šçš„è¡¨ç°åˆ™åˆ†åˆ«æ¯”åŸºç¡€GRPOé«˜å‡º2.08%ã€2.23%å’Œ3.99%ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/3rdAT/ThinkTuning">https://github.com/3rdAT/ThinkTuning</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07616v1">PDF</a> 15 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æœ€æ–°æµ‹è¯•æ—¶é—´ç¼©æ”¾æŠ€æœ¯çš„è¿›æ­¥ï¼Œä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºè‡ªæˆ‘åæ€è¡Œä¸ºå’Œè·¨å¤šæ­¥éª¤çš„æ¨ç†èƒ½åŠ›ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ é©±åŠ¨äº†è¿™ç§è‡ªæˆ‘æ”¹è¿›çš„æ¨¡å¼ï¼Œä½†ç ”ç©¶è¡¨æ˜ä»…é å¼ºåŒ–å­¦ä¹ å¹¶ä¸èƒ½çœŸæ­£èµ‹äºˆæ¨¡å‹è¿™äº›æ–°çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œåªæ˜¯æ¿€å‘æ¨¡å‹ä¸­å·²æœ‰çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ThinkTuningæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºGRPOçš„äº¤äº’å¼è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡æ•™å¸ˆæ¨¡å‹çš„æŒ‡å¯¼æ¥å¢å¼ºå­¦ç”Ÿæ¨¡å‹çš„rolloutã€‚è¯¥æ–¹æ³•å—åˆ°è¯¾å ‚å®è·µçš„å¯å‘ï¼Œé€šè¿‡æ•™å¸ˆæå‡ºé—®é¢˜ã€å­¦ç”Ÿå°è¯•å›ç­”ã€æ•™å¸ˆæä¾›çº æ­£åé¦ˆçš„æ–¹å¼ï¼Œé€æ­¥å¼•å¯¼å­¦ç”Ÿæ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆã€‚ç±»ä¼¼åœ°ï¼Œæœ¬æ–‡å‘ç°æ¥è‡ªç›¸åŒè§„æ¨¡æ•™å¸ˆæ¨¡å‹çš„åé¦ˆèƒ½æé«˜å­¦ç”Ÿçš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå¹³å‡æ¯”é›¶åŸºå‡†é«˜å‡º3.85%ï¼Œå¹¶åœ¨MATH-500ã€AIMEå’ŒGPQA-Diamondä¸Šåˆ†åˆ«å®ç°äº†2.08%ã€2.23%å’Œ3.99%çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶é—´ç¼©æ”¾æŠ€æœ¯çš„æœ€æ–°è¿›å±•å¯¼è‡´å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºè‡ªæˆ‘åæ€è¡Œä¸ºå’Œè·¨å¤šæ­¥éª¤æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ è™½åœ¨è¿™ç§è‡ªæˆ‘æ”¹è¿›æ¨¡å¼ä¸­èµ·å…³é”®ä½œç”¨ï¼Œä½†å•ç‹¬ä½¿ç”¨å¹¶ä¸èƒ½çœŸæ­£èµ‹äºˆæ¨¡å‹æ–°çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ThinkTuningæ–¹æ³•æ˜¯ä¸€ç§åŸºäºGRPOçš„äº¤äº’å¼è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡æ•™å¸ˆæ¨¡å‹çš„æŒ‡å¯¼æ¥å¢å¼ºå­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ThinkTuningæ–¹æ³•å—åˆ°è¯¾å ‚å®è·µçš„å¯å‘ï¼Œé€šè¿‡æ•™å¸ˆä¸å­¦ç”Ÿçš„äº’åŠ¨æ¥æé«˜å­¦ç”Ÿçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ•™å¸ˆæ¨¡å‹çš„åé¦ˆèƒ½å¤Ÿé‡å¡‘å­¦ç”Ÿæ¨¡å‹çš„æƒ³æ³•ï¼Œå¹¶å¼•å¯¼å…¶æ‰¾åˆ°æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ThinkTuningæ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå¹³å‡é«˜äºé›¶åŸºå‡†3.85%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07616">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d6a899d67263b1d9a099840360cf8515.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccd28137d612c00d12112dd39483cb29.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="CoT-Pose-Chain-of-Thought-Reasoning-for-3D-Pose-Generation-from-Abstract-Prompts"><a href="#CoT-Pose-Chain-of-Thought-Reasoning-for-3D-Pose-Generation-from-Abstract-Prompts" class="headerlink" title="CoT-Pose: Chain-of-Thought Reasoning for 3D Pose Generation from   Abstract Prompts"></a>CoT-Pose: Chain-of-Thought Reasoning for 3D Pose Generation from   Abstract Prompts</h2><p><strong>Authors:Junuk Cha, Jihyeon Kim</strong></p>
<p>Recent advances in multi-modal large language models (MLLMs) and chain-of-thought (CoT) reasoning have led to significant progress in image and text generation tasks. However, the field of 3D human pose generation still faces critical limitations. Most existing text-to-pose models rely heavily on detailed (low-level) prompts that explicitly describe joint configurations. In contrast, humans tend to communicate actions and intentions using abstract (high-level) language. This mismatch results in a practical challenge for deploying pose generation systems in real-world scenarios. To bridge this gap, we introduce a novel framework that incorporates CoT reasoning into the pose generation process, enabling the interpretation of abstract prompts into accurate 3D human poses. We further propose a data synthesis pipeline that automatically generates triplets of abstract prompts, detailed prompts, and corresponding 3D poses for training process. Experimental results demonstrate that our reasoning-enhanced model, CoT-Pose, can effectively generate plausible and semantically aligned poses from abstract textual inputs. This work highlights the importance of high-level understanding in pose generation and opens new directions for reasoning-enhanced approach for human pose generation. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†çš„è¿›å±•åœ¨å›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨3Däººä½“å§¿æ€ç”Ÿæˆé¢†åŸŸï¼Œä»ç„¶å­˜åœ¨å…³é”®æ€§å±€é™ã€‚å¤§å¤šæ•°ç°æœ‰çš„æ–‡æœ¬åˆ°å§¿æ€æ¨¡å‹ä¸¥é‡ä¾èµ–äºè¯¦ç»†æè¿°å…³èŠ‚é…ç½®çš„ä½ä½æç¤ºã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäººç±»å€¾å‘äºä½¿ç”¨æŠ½è±¡ï¼ˆé«˜çº§ï¼‰è¯­è¨€æ¥äº¤æµåŠ¨ä½œå’Œæ„å›¾ã€‚è¿™ç§ä¸åŒ¹é…ä¸ºåœ¨å®é™…åœºæ™¯ä¸­éƒ¨ç½²å§¿æ€ç”Ÿæˆç³»ç»Ÿå¸¦æ¥äº†å®é™…æŒ‘æˆ˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå°†æ€ç»´é“¾æ¨ç†èå…¥å§¿æ€ç”Ÿæˆè¿‡ç¨‹çš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå°†æŠ½è±¡æç¤ºè§£é‡Šä¸ºå‡†ç¡®å¯é çš„3Däººä½“å§¿æ€ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªæ•°æ®åˆæˆç®¡é“ï¼Œè¯¥ç®¡é“èƒ½å¤Ÿè‡ªåŠ¨ç”ŸæˆæŠ½è±¡æç¤ºã€è¯¦ç»†æç¤ºå’Œç›¸åº”çš„3Då§¿æ€çš„ä¸‰å…ƒç»„ï¼Œç”¨äºè®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬å¢å¼ºçš„æ¨ç†æ¨¡å‹CoT-Poseå¯ä»¥æœ‰æ•ˆåœ°ä»æŠ½è±¡çš„æ–‡æœ¬è¾“å…¥ä¸­ç”Ÿæˆåˆç†ä¸”è¯­ä¹‰å¯¹é½çš„å§¿æ€ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†é«˜çº§ç†è§£åœ¨å§¿æ€ç”Ÿæˆä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå¢å¼ºæ¨ç†æ–¹æ³•åœ¨äººç±»å§¿æ€ç”Ÿæˆé¢†åŸŸå¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07540v1">PDF</a> ICCVWâ€™25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†åœ¨å›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­çš„æœ€æ–°è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨3Däººä½“å§¿æ€ç”Ÿæˆé¢†åŸŸä»å­˜åœ¨å…³é”®é™åˆ¶ã€‚å¤§å¤šæ•°ç°æœ‰çš„æ–‡æœ¬åˆ°å§¿æ€æ¨¡å‹ä¾èµ–äºè¯¦ç»†çš„ä½çº§åˆ«æç¤ºæ¥æè¿°å…³èŠ‚é…ç½®ã€‚ä¸ä¹‹ç›¸åï¼Œäººç±»å€¾å‘äºä½¿ç”¨æŠ½è±¡çš„é«˜çº§è¯­è¨€æ¥è¡¨è¾¾åŠ¨ä½œå’Œæ„å›¾ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆCoTæ¨ç†çš„æ–°å‹æ¡†æ¶ï¼Œä½¿æŠ½è±¡æç¤ºèƒ½å¤Ÿè½¬åŒ–ä¸ºå‡†ç¡®çš„3Däººä½“å§¿æ€ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ•°æ®åˆæˆç®¡é“ï¼Œå¯è‡ªåŠ¨ç”ŸæˆæŠ½è±¡æç¤ºã€è¯¦ç»†æç¤ºå’Œç›¸åº”çš„3Då§¿æ€ç”¨äºè®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¢å¼ºå‹æ¨ç†æ¨¡å‹CoT-Poseå¯ä»¥æœ‰æ•ˆåœ°ä»æŠ½è±¡æ–‡æœ¬è¾“å…¥ä¸­ç”Ÿæˆåˆç†ä¸”è¯­ä¹‰å¯¹é½çš„å§¿æ€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ€ç»´é“¾æ¨ç†åœ¨å›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­æœ‰æ˜¾è‘—è¿›å±•ã€‚</li>
<li>3Däººä½“å§¿æ€ç”Ÿæˆé¢†åŸŸä»é¢ä¸´ä¾èµ–ä½çº§åˆ«æç¤ºçš„å±€é™æ€§ã€‚</li>
<li>äººç±»ä½¿ç”¨é«˜çº§è¯­è¨€æ¥è¡¨è¾¾åŠ¨ä½œå’Œæ„å›¾ï¼Œè€Œç°æœ‰æ¨¡å‹é€šå¸¸ä¾èµ–ä½çº§åˆ«æç¤ºã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆæ€ç»´é“¾æ¨ç†çš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºä»æŠ½è±¡æç¤ºç”Ÿæˆå‡†ç¡®çš„3Däººä½“å§¿æ€ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ•°æ®åˆæˆç®¡é“ï¼Œè‡ªåŠ¨ç”Ÿæˆç”¨äºè®­ç»ƒçš„æ•°æ®ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œå¢å¼ºå‹æ¨ç†æ¨¡å‹CoT-Poseèƒ½ä»æŠ½è±¡æ–‡æœ¬è¾“å…¥ä¸­ç”Ÿæˆåˆç†ä¸”è¯­ä¹‰å¯¹é½çš„å§¿æ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07540">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-39671d54f746cb5e908a9e3bbd01414b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b5c6728e10c3743acc2b3b497061785.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-476025197f67548c4f0b9afeb8fcf246.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0110d458b376975058e89be6b036a913.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-13/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-13/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-13/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d2f526f54f9eff944fd028d31717a3c2.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-13  ODYSSEY Open-World Quadrupeds Exploration and Manipulation for   Long-Horizon Tasks
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-13/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-e3eed16a328474636c13c8f2ebadfa3d.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-13  Think Before You Talk Enhancing Meaningful Dialogue Generation in   Full-Duplex Speech Language Models with Planning-Inspired Text Guidance
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32271.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
