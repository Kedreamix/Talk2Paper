<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-13  ODYSSEY Open-World Quadrupeds Exploration and Manipulation for   Long-Horizon Tasks">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d2f526f54f9eff944fd028d31717a3c2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-13-æ›´æ–°"><a href="#2025-08-13-æ›´æ–°" class="headerlink" title="2025-08-13 æ›´æ–°"></a>2025-08-13 æ›´æ–°</h1><h2 id="ODYSSEY-Open-World-Quadrupeds-Exploration-and-Manipulation-for-Long-Horizon-Tasks"><a href="#ODYSSEY-Open-World-Quadrupeds-Exploration-and-Manipulation-for-Long-Horizon-Tasks" class="headerlink" title="ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for   Long-Horizon Tasks"></a>ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for   Long-Horizon Tasks</h2><p><strong>Authors:Kaijun Wang, Liqin Lu, Mingyu Liu, Jianuo Jiang, Zeju Li, Bolin Zhang, Wancai Zheng, Xinyi Yu, Hao Chen, Chunhua Shen</strong></p>
<p>Language-guided long-horizon mobile manipulation has long been a grand challenge in embodied semantic reasoning, generalizable manipulation, and adaptive locomotion. Three fundamental limitations hinder progress: First, although large language models have improved spatial reasoning and task planning through semantic priors, existing implementations remain confined to tabletop scenarios, failing to address the constrained perception and limited actuation ranges of mobile platforms. Second, current manipulation strategies exhibit insufficient generalization when confronted with the diverse object configurations encountered in open-world environments. Third, while crucial for practical deployment, the dual requirement of maintaining high platform maneuverability alongside precise end-effector control in unstructured settings remains understudied.   In this work, we present ODYSSEY, a unified mobile manipulation framework for agile quadruped robots equipped with manipulators, which seamlessly integrates high-level task planning with low-level whole-body control. To address the challenge of egocentric perception in language-conditioned tasks, we introduce a hierarchical planner powered by a vision-language model, enabling long-horizon instruction decomposition and precise action execution. At the control level, our novel whole-body policy achieves robust coordination across challenging terrains. We further present the first benchmark for long-horizon mobile manipulation, evaluating diverse indoor and outdoor scenarios. Through successful sim-to-real transfer, we demonstrate the systemâ€™s generalization and robustness in real-world deployments, underscoring the practicality of legged manipulators in unstructured environments. Our work advances the feasibility of generalized robotic assistants capable of complex, dynamic tasks. Our project page: <a target="_blank" rel="noopener" href="https://kaijwang.github.io/odyssey.github.io/">https://kaijwang.github.io/odyssey.github.io/</a> </p>
<blockquote>
<p>è¯­è¨€å¼•å¯¼çš„é•¿æœŸç§»åŠ¨æ“ä½œä¸€ç›´æ˜¯è¯­ä¹‰ç†è§£ã€é€šç”¨æ“ä½œå’Œè‡ªé€‚åº”ç§»åŠ¨æ–¹é¢çš„ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ã€‚å­˜åœ¨ä¸‰ä¸ªåŸºæœ¬å±€é™æ€§é˜»ç¢è¿›å±•ï¼šé¦–å…ˆï¼Œè™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡è¯­ä¹‰å…ˆéªŒæ”¹è¿›äº†ç©ºé—´æ¨ç†å’Œä»»åŠ¡è§„åˆ’ï¼Œä½†ç°æœ‰å®ç°ä»…é™äºæ¡Œé¢åœºæ™¯ï¼Œæ— æ³•è§£å†³ç§»åŠ¨å¹³å°çš„å—é™æ„ŸçŸ¥å’Œæœ‰é™çš„åŠ¨ä½œèŒƒå›´ã€‚å…¶æ¬¡ï¼Œå½“å‰çš„æ“ä½œç­–ç•¥åœ¨é¢å¯¹å¼€æ”¾ç¯å¢ƒä¸­é‡åˆ°çš„å„ç§å¯¹è±¡é…ç½®æ—¶ï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ä»¥åº”å¯¹ã€‚ç¬¬ä¸‰ï¼Œè™½ç„¶åœ¨å®é™…éƒ¨ç½²ä¸­è‡³å…³é‡è¦ï¼Œä½†åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­åŒæ—¶ä¿æŒé«˜å¹³å°æœºåŠ¨æ€§å’Œç²¾ç¡®æœ«ç«¯æ‰§è¡Œå™¨æ§åˆ¶çš„åŒé‡è¦æ±‚åœ¨ç ”ç©¶ä¸­ä»ç„¶è¢«å¿½è§†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08240v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨åŸºäºè¯­è¨€çš„é•¿æœŸç§»åŠ¨æ“çºµæ–¹é¢ï¼Œé¢ä¸´ç€è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚è¯­ä¹‰ç†è§£ã€é€šç”¨åŒ–æ“çºµå’Œè‡ªé€‚åº”ç§»åŠ¨ç­‰ã€‚ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹è™½æå‡äº†ç©ºé—´æ¨ç†å’Œä»»åŠ¡è§„åˆ’èƒ½åŠ›ï¼Œä½†ä»å±€é™äºæ¡Œé¢åœºæ™¯ï¼Œéš¾ä»¥è§£å†³ç§»åŠ¨å¹³å°æ„ŸçŸ¥å—é™å’ŒåŠ¨ä½œèŒƒå›´æœ‰é™çš„é—®é¢˜ã€‚å½“å‰æ“çºµç­–ç•¥åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­é¢å¯¹å¤šæ ·åŒ–çš„å¯¹è±¡é…ç½®æ—¶ï¼Œå…¶é€šç”¨æ€§ä¸è¶³ã€‚åŒæ—¶ï¼Œåœ¨å¤æ‚ã€éç»“æ„åŒ–ç¯å¢ƒä¸­ï¼Œä¿æŒå¹³å°é«˜åº¦æœºåŠ¨æ€§å’Œç²¾ç¡®æœ«ç«¯æ‰§è¡Œå™¨æ§åˆ¶çš„è¦æ±‚å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æœ¬ç ”ç©¶æå‡ºODYSSEYï¼Œä¸€ä¸ªç”¨äºæ•æ·å››è¶³æœºå™¨äººçš„ç»Ÿä¸€ç§»åŠ¨æ“çºµæ¡†æ¶ï¼Œé…å¤‡æ“çºµå™¨ï¼Œå®ç°é«˜çº§ä»»åŠ¡è§„åˆ’ä¸ä½çº§å…¨èº«æ§åˆ¶çš„æ— ç¼é›†æˆã€‚é€šè¿‡å¼•å…¥è§†è§‰è¯­è¨€æ¨¡å‹çš„åˆ†å±‚è§„åˆ’å™¨ï¼Œè§£å†³è¯­è¨€æ¡ä»¶ä¸‹çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ„ŸçŸ¥æŒ‘æˆ˜ï¼Œå®ç°é•¿æœŸæŒ‡ä»¤åˆ†è§£å’Œç²¾ç¡®åŠ¨ä½œæ‰§è¡Œã€‚åœ¨æ§åˆ¶å±‚é¢ï¼Œæˆ‘ä»¬çš„å…¨æ–°å…¨èº«ç­–ç•¥å®ç°äº†å¤æ‚åœ°å½¢ä¸­çš„ç¨³å¥åè°ƒã€‚æˆ‘ä»¬é¦–æ¬¡å»ºç«‹äº†é•¿æœŸç§»åŠ¨æ“çºµçš„åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°äº†å¤šç§å®¤å†…å’Œå®¤å¤–åœºæ™¯ã€‚é€šè¿‡æˆåŠŸçš„æ¨¡æ‹Ÿåˆ°ç°å®çš„è½¬ç§»ï¼Œæˆ‘ä»¬è¯æ˜äº†ç³»ç»Ÿåœ¨ç°å®éƒ¨ç½²ä¸­çš„é€šç”¨æ€§å’Œç¨³å¥æ€§ï¼Œå¼ºè°ƒäº†å››è‚¢æ“ä½œå™¨åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­çš„å®ç”¨æ€§ã€‚æœ¬ç ”ç©¶æ¨åŠ¨äº†é€šç”¨æœºå™¨äººåŠ©æ‰‹å®Œæˆå¤æ‚åŠ¨æ€ä»»åŠ¡çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æŒ‡å¯¼çš„é•¿æœŸç§»åŠ¨æ“çºµä»å­˜åœ¨è¯­ä¹‰ç†è§£ã€é€šç”¨åŒ–æ“çºµå’Œè‡ªé€‚åº”ç§»åŠ¨ç­‰æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¡Œé¢åœºæ™¯å¤–æ„ŸçŸ¥å’ŒåŠ¨ä½œèŒƒå›´æ–¹é¢çš„å±€é™æ€§ã€‚</li>
<li>å½“å‰æ“çºµç­–ç•¥åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å¯¹è±¡é…ç½®å¤šæ ·æ€§é¢å‰çš„é€šç”¨æ€§ä¸è¶³ã€‚</li>
<li>ç§»åŠ¨å¹³å°é«˜åº¦æœºåŠ¨æ€§ä¸ç²¾ç¡®æœ«ç«¯æ‰§è¡Œå™¨æ§åˆ¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„åŒé‡éœ€æ±‚å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚</li>
<li>ODYSSEYæ¡†æ¶å®ç°äº†å››è¶³æœºå™¨äººçš„ç§»åŠ¨æ“çºµï¼Œé›†æˆäº†ä»»åŠ¡è§„åˆ’å’Œå…¨èº«æ§åˆ¶ã€‚</li>
<li>é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹åˆ†å±‚è§„åˆ’å™¨è§£å†³ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ„ŸçŸ¥æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08240">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-13576d33da3e9f4fe02ab23a28047a1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ffa7fb6067b4433822d8dbe514719195.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba80e561fbbeac1f1c3ba69a63a63062.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c1903622f683292d55613d310e4f114.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed9d4cd7c0db06dac1c1e879ca8395a6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Exploring-Safety-Alignment-Evaluation-of-LLMs-in-Chinese-Mental-Health-Dialogues-via-LLM-as-Judge"><a href="#Exploring-Safety-Alignment-Evaluation-of-LLMs-in-Chinese-Mental-Health-Dialogues-via-LLM-as-Judge" class="headerlink" title="Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health   Dialogues via LLM-as-Judge"></a>Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health   Dialogues via LLM-as-Judge</h2><p><strong>Authors:Yunna Cai, Fan Wang, Haowei Wang, Kun Wang, Kailai Yang, Sophia Ananiadou, Moyan Li, Mingming Fan</strong></p>
<p>Evaluating the safety alignment of LLM responses in high-risk mental health dialogues is particularly difficult due to missing gold-standard answers and the ethically sensitive nature of these interactions. To address this challenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark based on real-world Chinese mental health dialogues. It evaluates whether the model responses align with the safety principles defined by experts. Specifically designed for settings without standard references, our method adopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation using expert-defined reasoning chains grounded in psychological intervention principles. We employ binary point-wise scoring across multiple safety dimensions to enhance the explainability and traceability of the evaluation. Additionally, we present a manually curated, high-quality Chinese-language dataset covering self-harm, suicidal ideation, and existential distress, derived from real-world online discourse. Experiments on 3600 judgments show that our method achieves the highest agreement with expert assessments and produces more interpretable evaluation rationales compared to existing approaches. Our dataset and evaluation tool are publicly available to facilitate further research. </p>
<blockquote>
<p>è¯„ä¼°é«˜é£é™©å¿ƒç†å¥åº·å¯¹è¯ä¸­LLMå›ç­”çš„å®‰å…¨ä¸€è‡´æ€§ç‰¹åˆ«å›°éš¾ï¼Œè¿™æ˜¯ç”±äºç¼ºä¹é‡‘æ ‡å‡†ç­”æ¡ˆä»¥åŠè¿™äº›äº¤äº’çš„ä¼¦ç†æ•æ„Ÿæ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†PsyCrisis-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºçœŸå®ä¸–ç•Œä¸­æ–‡å¿ƒç†å¥åº·å¯¹è¯çš„å‚è€ƒè‡ªç”±è¯„ä¼°åŸºå‡†ã€‚å®ƒè¯„ä¼°æ¨¡å‹å“åº”æ˜¯å¦ä¸ä¸“å®¶å®šä¹‰çš„å®‰å…¨åŸåˆ™ä¸€è‡´ã€‚æˆ‘ä»¬ä¸“ä¸ºæ²¡æœ‰æ ‡å‡†å‚è€ƒçš„è®¾ç½®è®¾è®¡çš„æ–¹æ³•é‡‡ç”¨äº†åŸºäºæç¤ºçš„LLM-as-Judgeæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨ä¸“å®¶å®šä¹‰çš„åŸºäºå¿ƒç†å¹²é¢„åŸåˆ™çš„æ¨ç†é“¾è¿›è¡Œä¸Šä¸‹æ–‡è¯„ä¼°ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªå®‰å…¨ç»´åº¦ä¸Šé‡‡ç”¨äºŒè¿›åˆ¶ç‚¹è¯„åˆ†ï¼Œä»¥æé«˜è¯„ä¼°çš„å¯è§£é‡Šæ€§å’Œå¯è¿½æº¯æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªæ‰‹åŠ¨æ•´ç†çš„é«˜è´¨é‡ä¸­æ–‡æ•°æ®é›†ï¼Œæ¶µç›–è‡ªæˆ‘ä¼¤å®³ã€è‡ªæ€æ„å¿µå’Œå­˜åœ¨æ€§ç—›è‹¦ï¼Œæ¥æºäºçœŸå®ä¸–ç•Œçš„åœ¨çº¿è¯è¯­ã€‚å¯¹3600æ¬¡åˆ¤æ–­çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä¸“å®¶è¯„ä¼°çš„å¥‘åˆåº¦æœ€é«˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½äº§ç”Ÿæ›´å¯è§£é‡Šçš„è¯„ä»·ç†ç”±ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œè¯„ä¼°å·¥å…·å…¬å¼€å¯ç”¨ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08236v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹é«˜é£é™©å¿ƒç†å¥åº·å¯¹è¯ä¸­LLMå“åº”çš„å®‰å…¨å¯¹é½è¯„ä¼°çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†PsyCrisis-Benchè¿™ä¸€æ— å‚è€ƒè¯„ä»·çš„åŸºå‡†ï¼ŒåŸºäºçœŸå®ä¸–ç•Œçš„ä¸­æ–‡å¿ƒç†å¥åº·å¯¹è¯ã€‚å®ƒè¯„ä»·æ¨¡å‹å“åº”æ˜¯å¦ä¸ä¸“å®¶å®šä¹‰çš„å®‰å…¨åŸåˆ™å¯¹é½ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åŸºäºæç¤ºçš„LLM-as-Judgeæ–¹å¼è¿›è¡Œä¸Šä¸‹æ–‡è¯„ä»·ï¼Œä½¿ç”¨ä¸“å®¶å®šä¹‰çš„åŸºäºå¿ƒç†å¹²é¢„åŸç†çš„æ¨ç†é“¾ï¼Œå¹¶åœ¨å¤šä¸ªå®‰å…¨ç»´åº¦ä¸Šè¿›è¡ŒäºŒå…ƒç‚¹çº§è¯„åˆ†ï¼Œä»¥æé«˜è¯„ä»·çš„è§£é‡Šæ€§å’Œå¯è¿½æº¯æ€§ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†ä¸€ä¸ªé«˜è´¨é‡çš„æ‰‹åŠ¨æ•´ç†ä¸­æ–‡æ•°æ®é›†ï¼Œæ¶µç›–è‡ªæˆ‘ä¼¤å®³ã€è‡ªæ€æ„å¿µå’Œå­˜åœ¨æ€§ç„¦è™‘ç­‰çœŸå®ä¸–ç•Œåœ¨çº¿è¯è¯­ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä¸“å®¶è¯„ä¼°è¾¾åˆ°æœ€é«˜ä¸€è‡´æ€§ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½äº§ç”Ÿæ›´å¯è§£é‡Šçš„è¯„ä»·ä¾æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯„ä¼°LLMåœ¨é«˜é£é™©å¿ƒç†å¥åº·å¯¹è¯ä¸­çš„å®‰å…¨å“åº”æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºæ²¡æœ‰é»„é‡‘æ ‡å‡†ç­”æ¡ˆå’Œä¼¦ç†æ•æ„Ÿçš„äº¤äº’ã€‚</li>
<li>PsyCrisis-Benchæ˜¯ä¸€ä¸ªæ— å‚è€ƒè¯„ä»·çš„åŸºå‡†ï¼ŒåŸºäºçœŸå®ä¸–ç•Œçš„ä¸­æ–‡å¿ƒç†å¥åº·å¯¹è¯ï¼Œç”¨äºè¯„ä»·æ¨¡å‹å“åº”æ˜¯å¦ç¬¦åˆå®‰å…¨åŸåˆ™ã€‚</li>
<li>é‡‡ç”¨åŸºäºæç¤ºçš„LLM-as-Judgeæ–¹å¼è¿›è¡Œä¸Šä¸‹æ–‡è¯„ä»·ã€‚</li>
<li>ä½¿ç”¨ä¸“å®¶å®šä¹‰çš„åŸºäºå¿ƒç†å¹²é¢„åŸç†çš„æ¨ç†é“¾è¿›è¡Œå®‰å…¨ç»´åº¦çš„äºŒå…ƒç‚¹çº§è¯„åˆ†ã€‚</li>
<li>æ–¹æ³•æé«˜äº†è¯„ä»·çš„è§£é‡Šæ€§å’Œå¯è¿½æº¯æ€§ã€‚</li>
<li>æä¾›äº†ä¸€ä¸ªé«˜è´¨é‡çš„æ‰‹åŠ¨æ•´ç†ä¸­æ–‡æ•°æ®é›†ï¼Œæ¶µç›–è‡ªæˆ‘ä¼¤å®³ã€è‡ªæ€æ„å¿µå’Œå­˜åœ¨æ€§ç„¦è™‘ç­‰çœŸå®ä¸–ç•Œåœ¨çº¿è¯è¯­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08236">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e98e95e14192497c63011781c6394776.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a04ae3e13c09b573353649af7af70e62.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4e461423067a95b0a25f3c6e4c5b144d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e854244aa0db970a846d18438133ddc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc91bfbceab37c845a83315259bf1d3e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9c2d5424ebe0c6b4ac7fed7f03b97543.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60e6fbbeed8219eb30754f7a227f9341.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LL3M-Large-Language-3D-Modelers"><a href="#LL3M-Large-Language-3D-Modelers" class="headerlink" title="LL3M: Large Language 3D Modelers"></a>LL3M: Large Language 3D Modelers</h2><p><strong>Authors:Sining Lu, Guan Chen, Nam Anh Dinh, Itai Lang, Ari Holtzman, Rana Hanocka</strong></p>
<p>We present LL3M, a multi-agent system that leverages pretrained large language models (LLMs) to generate 3D assets by writing interpretable Python code in Blender. We break away from the typical generative approach that learns from a collection of 3D data. Instead, we reformulate shape generation as a code-writing task, enabling greater modularity, editability, and integration with artist workflows. Given a text prompt, LL3M coordinates a team of specialized LLM agents to plan, retrieve, write, debug, and refine Blender scripts that generate and edit geometry and appearance. The generated code works as a high-level, interpretable, human-readable, well-documented representation of scenes and objects, making full use of sophisticated Blender constructs (e.g. B-meshes, geometry modifiers, shader nodes) for diverse, unconstrained shapes, materials, and scenes. This code presents many avenues for further agent and human editing and experimentation via code tweaks or procedural parameters. This medium naturally enables a co-creative loop in our system: agents can automatically self-critique using code and visuals, while iterative user instructions provide an intuitive way to refine assets. A shared code context across agents enables awareness of previous attempts, and a retrieval-augmented generation knowledge base built from Blender API documentation, BlenderRAG, equips agents with examples, types, and functions empowering advanced modeling operations and code correctness. We demonstrate the effectiveness of LL3M across diverse shape categories, style and material edits, and user-driven refinements. Our experiments showcase the power of code as a generative and interpretable medium for 3D asset creation. Our project page is at <a target="_blank" rel="noopener" href="https://threedle.github.io/ll3m">https://threedle.github.io/ll3m</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†LL3Mï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç¼–å†™å¯åœ¨Blenderä¸­è§£é‡Šçš„Pythonä»£ç æ¥ç”Ÿæˆ3Dèµ„äº§ã€‚æˆ‘ä»¬æ‘’å¼ƒäº†ä»ä¸€ç»„3Dæ•°æ®ä¸­å­¦ä¹ çš„ä¼ ç»Ÿç”Ÿæˆæ–¹æ³•ã€‚ç›¸åï¼Œæˆ‘ä»¬å°†å½¢çŠ¶ç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºä»£ç ç¼–å†™ä»»åŠ¡ï¼Œä»è€Œå®ç°æ›´å¤§çš„æ¨¡å—åŒ–ã€å¯ç¼–è¾‘æ€§å’Œä¸è‰ºæœ¯å®¶å·¥ä½œæµç¨‹çš„é›†æˆã€‚ç»™å®šæ–‡æœ¬æç¤ºï¼ŒLL3Måè°ƒä¸“ä¸šLLMæ™ºèƒ½ä½“å›¢é˜Ÿæ¥è§„åˆ’ã€æ£€ç´¢ã€ç¼–å†™ã€è°ƒè¯•å’Œç²¾ç‚¼Blenderè„šæœ¬ï¼Œä»¥ç”Ÿæˆå’Œç¼–è¾‘å‡ ä½•å½¢çŠ¶å’Œå¤–è§‚ã€‚ç”Ÿæˆçš„ä»£ç ä½œä¸ºåœºæ™¯å’Œå¯¹è±¡çš„é«˜çº§ã€å¯è§£é‡Šã€äººç±»å¯è¯»çš„è‰¯å¥½æ–‡æ¡£è¡¨ç¤ºï¼Œå……åˆ†åˆ©ç”¨Blenderçš„å¤æ‚ç»“æ„ï¼ˆä¾‹å¦‚Bç½‘æ ¼ã€å‡ ä½•ä¿®é¥°ç¬¦ã€ç€è‰²å™¨èŠ‚ç‚¹ï¼‰æ¥åˆ›å»ºå¤šæ ·åŒ–çš„ã€æ— çº¦æŸçš„å½¢çŠ¶ã€æè´¨å’Œåœºæ™¯ã€‚è¯¥ä»£ç ä¸ºè¿›ä¸€æ­¥çš„æ™ºèƒ½ä½“å’Œäººç±»ç¼–è¾‘å’Œå®éªŒæä¾›äº†è®¸å¤šé€”å¾„ï¼Œå¯ä»¥é€šè¿‡ä»£ç å¾®è°ƒæˆ–ç¨‹åºå‚æ•°è¿›è¡Œè°ƒæ•´ã€‚è¿™ç§åª’ä»‹è‡ªç„¶åœ°åœ¨æˆ‘ä»¬ç³»ç»Ÿä¸­å¯ç”¨äº†ååŒåˆ›ä½œå¾ªç¯ï¼šæ™ºèƒ½ä½“å¯ä»¥ä½¿ç”¨ä»£ç å’Œè§†è§‰æ•ˆæœè‡ªåŠ¨è¿›è¡Œè‡ªæˆ‘æ‰¹è¯„ï¼Œè€Œè¿­ä»£ç”¨æˆ·æŒ‡ä»¤æä¾›äº†ä¸€ç§å®Œå–„èµ„äº§ç›´è§‚æ–¹å¼ã€‚æ™ºèƒ½ä½“ä¹‹é—´çš„å…±äº«ä»£ç ä¸Šä¸‹æ–‡ä½¿å®ƒä»¬èƒ½å¤Ÿæ„è¯†åˆ°ä¹‹å‰çš„å°è¯•ï¼Œå¹¶ä¸”ä»Blender APIæ–‡æ¡£æ„å»ºçš„å¢å¼ºæ£€ç´¢çŸ¥è¯†åº“BlenderRAGä¸ºæ™ºèƒ½ä½“æä¾›ç¤ºä¾‹ã€ç±»å‹å’ŒåŠŸèƒ½ï¼Œæ”¯æŒé«˜çº§å»ºæ¨¡æ“ä½œå’Œä»£ç æ­£ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨å„ç§å½¢çŠ¶ç±»åˆ«ã€æ ·å¼å’Œææ–™ç¼–è¾‘ä»¥åŠç”¨æˆ·é©±åŠ¨çš„ä¼˜åŒ–æ–¹é¢å±•ç¤ºäº†LL3Mçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„å®éªŒå±•ç¤ºäº†ä»£ç ä½œä¸º3Dèµ„äº§åˆ›å»ºçš„å¯ç”Ÿæˆå’Œå¯è§£é‡Šåª’ä»‹çš„åŠ›é‡ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://threedle.github.io/ll3m%E3%80%82">https://threedle.github.io/ll3mã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08228v1">PDF</a> Our project page is at <a target="_blank" rel="noopener" href="https://threedle.github.io/ll3m">https://threedle.github.io/ll3m</a></p>
<p><strong>Summary</strong></p>
<p>LL3Mæ˜¯ä¸€ä¸ªåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆä¸‰ç»´èµ„äº§çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå®ƒé€šè¿‡ç¼–å†™å¯åœ¨Blenderä¸­è§£é‡Šçš„Pythonä»£ç æ¥å®ç°ã€‚ä¸ä¼ ç»Ÿçš„ä»ä¸‰ç»´æ•°æ®ä¸­å­¦ä¹ çš„ç”Ÿæˆæ–¹æ³•ä¸åŒï¼ŒLL3Må°†å½¢çŠ¶ç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºä»£ç ç¼–å†™ä»»åŠ¡ï¼Œæé«˜äº†æ¨¡å—åŒ–ã€å¯ç¼–è¾‘æ€§å’Œä¸è‰ºæœ¯å®¶å·¥ä½œæµç¨‹çš„é›†æˆåº¦ã€‚é€šè¿‡æ–‡æœ¬æç¤ºï¼ŒLL3Måè°ƒä¸“ä¸šLLMæ™ºèƒ½ä½“å›¢é˜Ÿæ¥è§„åˆ’ã€æ£€ç´¢ã€ç¼–å†™ã€è°ƒè¯•å’Œä¼˜åŒ–Blenderè„šæœ¬ï¼Œç”Ÿæˆå’Œç¼–è¾‘å‡ ä½•ä½“å’Œå¤–è§‚ã€‚ç”Ÿæˆçš„ä»£ç ä½œä¸ºåœºæ™¯å’Œå¯¹è±¡çš„å¯è§£é‡Šã€äººç±»å¯è¯»å’Œæœ‰å……åˆ†æ–‡æ¡£çš„é«˜çº§è¡¨ç¤ºå½¢å¼ï¼Œå……åˆ†åˆ©ç”¨Blenderçš„å¤æ‚ç»“æ„æ¥åˆ›å»ºå¤šæ ·åŒ–çš„æ— çº¦æŸå½¢çŠ¶ã€æè´¨å’Œåœºæ™¯ã€‚è¿™ä¸ºè¿›ä¸€æ­¥çš„æ™ºèƒ½ä½“å’Œäººç±»ç¼–è¾‘ä»¥åŠé€šè¿‡ä»£ç å¾®è°ƒæˆ–ç¨‹åºå‚æ•°è¿›è¡Œçš„å®éªŒæä¾›äº†è®¸å¤šé€”å¾„ã€‚è¿™ç§åª’ä»‹è‡ªç„¶åœ°åœ¨æˆ‘ä»¬ç³»ç»Ÿä¸­å¯ç”¨äº†ååŒåˆ›ä½œå¾ªç¯ï¼šæ™ºèƒ½ä½“å¯ä»¥ä½¿ç”¨ä»£ç å’Œè§†è§‰è‡ªåŠ¨è¿›è¡Œè‡ªæˆ‘è¯„ä»·ï¼Œè€Œè¿­ä»£ç”¨æˆ·æŒ‡ä»¤ä¸ºå®Œå–„èµ„äº§æä¾›äº†ä¸€ç§ç›´è§‚çš„æ–¹å¼ã€‚æ™ºèƒ½ä½“ä¹‹é—´çš„å…±äº«ä»£ç ä¸Šä¸‹æ–‡ä½¿å®ƒä»¬èƒ½å¤Ÿæ„è¯†åˆ°ä¹‹å‰çš„å°è¯•ï¼Œè€Œæ¥è‡ªBlender APIæ–‡æ¡£çš„æ£€ç´¢å¢å¼ºç”ŸæˆçŸ¥è¯†åº“åˆ™ä¸ºæ™ºèƒ½ä½“æä¾›äº†ç¤ºä¾‹ã€ç±»å‹å’Œå‡½æ•°ï¼Œæ”¯æŒé«˜çº§å»ºæ¨¡æ“ä½œå’Œä»£ç æ­£ç¡®æ€§ã€‚æˆ‘ä»¬çš„å®éªŒå±•ç¤ºäº†ä»£ç ä½œä¸ºä¸‰ç»´èµ„äº§åˆ›å»ºä¸­ç”Ÿæˆå’Œå¯è§£é‡Šåª’ä»‹çš„åŠ›é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LL3Mæ˜¯ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸‰ç»´èµ„äº§çš„ç³»ç»Ÿï¼Œé€šè¿‡ç¼–å†™Blenderè„šæœ¬å®ç°ã€‚</li>
<li>LL3Mé‡‡ç”¨ä»£ç ç¼–å†™ä»»åŠ¡çš„æ–¹å¼ï¼Œä¸ä¼ ç»Ÿä»ä¸‰ç»´æ•°æ®ä¸­å­¦ä¹ çš„ç”Ÿæˆæ–¹æ³•ä¸åŒï¼Œå¼ºè°ƒæ¨¡å—åŒ–ã€å¯ç¼–è¾‘æ€§å’Œä¸è‰ºæœ¯å®¶å·¥ä½œæµç¨‹çš„é›†æˆã€‚</li>
<li>LL3Mèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå’Œç¼–è¾‘å‡ ä½•ä½“å’Œå¤–è§‚ï¼Œç”Ÿæˆçš„ä»£ç ä¸ºåœºæ™¯å’Œå¯¹è±¡çš„é«˜çº§ã€å¯è§£é‡Šã€äººç±»å¯è¯»å’Œæœ‰æ–‡æ¡£è¡¨ç¤ºã€‚</li>
<li>ç”Ÿæˆçš„ä»£ç åˆ©ç”¨Blenderçš„å¤æ‚ç»“æ„ï¼Œå¯ä»¥åˆ›å»ºå¤šæ ·åŒ–çš„æ— çº¦æŸå½¢çŠ¶ã€æè´¨å’Œåœºæ™¯ã€‚</li>
<li>LL3Mç³»ç»Ÿä¸­çš„æ™ºèƒ½ä½“å¯ä»¥è‡ªåŠ¨è‡ªæˆ‘è¯„ä»·ï¼Œå¹¶é€šè¿‡è¿­ä»£ç”¨æˆ·æŒ‡ä»¤è¿›è¡Œèµ„äº§å®Œå–„ã€‚</li>
<li>æ™ºèƒ½ä½“ä¹‹é—´çš„å…±äº«ä»£ç ä¸Šä¸‹æ–‡ä½¿å®ƒä»¬èƒ½å¤Ÿæ„è¯†åˆ°ä¹‹å‰çš„å°è¯•ï¼Œè€Œæ£€ç´¢å¢å¼ºç”ŸæˆçŸ¥è¯†åº“åˆ™ä¸ºæ™ºèƒ½ä½“æä¾›æ”¯æŒé«˜çº§å»ºæ¨¡æ“ä½œå’Œä»£ç æ­£ç¡®æ€§çš„èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08228">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-69d5577b233773565ab9690cf7c2981a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4104ff9db0b0165d416c71081e582c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c963b0ca086246923682168a19af2b90.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12f112fa2f8180e4b4bfda7bf940456a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ab28d9679c40ffe33e932899c17a829.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56428fe45b6827de23997c246e0889ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a54cdb7fd30d8db913ce93c98f9d532.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Capabilities-of-GPT-5-on-Multimodal-Medical-Reasoning"><a href="#Capabilities-of-GPT-5-on-Multimodal-Medical-Reasoning" class="headerlink" title="Capabilities of GPT-5 on Multimodal Medical Reasoning"></a>Capabilities of GPT-5 on Multimodal Medical Reasoning</h2><p><strong>Authors:Shansong Wang, Mingzhe Hu, Qiang Li, Mojtaba Safari, Xiaofeng Yang</strong></p>
<p>Recent advances in large language models (LLMs) have enabled general-purpose systems to perform increasingly complex domain-specific reasoning without extensive fine-tuning. In the medical domain, decision-making often requires integrating heterogeneous information sources, including patient narratives, structured data, and medical images. This study positions GPT-5 as a generalist multimodal reasoner for medical decision support and systematically evaluates its zero-shot chain-of-thought reasoning performance on both text-based question answering and visual question answering tasks under a unified protocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20 against standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU medical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that GPT-5 consistently outperforms all baselines, achieving state-of-the-art accuracy across all QA benchmarks and delivering substantial gains in multimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and understanding scores by +29.62% and +36.18% over GPT-4o, respectively, and surpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in understanding. In contrast, GPT-4o remains below human expert performance in most dimensions. A representative case study demonstrates GPT-5â€™s ability to integrate visual and textual cues into a coherent diagnostic reasoning chain, recommending appropriate high-stakes interventions. Our results show that, on these controlled multimodal reasoning benchmarks, GPT-5 moves from human-comparable to above human-expert performance. This improvement may substantially inform the design of future clinical decision-support systems. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä½¿å¾—é€šç”¨ç³»ç»Ÿèƒ½å¤Ÿåœ¨ä¸éœ€è¦å¹¿æ³›å¾®è°ƒçš„æƒ…å†µä¸‹æ‰§è¡Œè¶Šæ¥è¶Šå¤æ‚çš„ç‰¹å®šé¢†åŸŸæ¨ç†ã€‚åœ¨åŒ»å­¦é¢†åŸŸï¼Œå†³ç­–åˆ¶å®šé€šå¸¸éœ€è¦æ•´åˆå¤šç§å¼‚è´¨çš„ä¿¡æ¯æ¥æºï¼ŒåŒ…æ‹¬æ‚£è€…å™è¿°ã€ç»“æ„åŒ–æ•°æ®å’ŒåŒ»å­¦å›¾åƒã€‚æœ¬ç ”ç©¶å°†GPT-5å®šä½ä¸ºåŒ»å­¦å†³ç­–æ”¯æŒçš„ä¸€èˆ¬æ€§å¤šæ¨¡å¼æ¨ç†å™¨ï¼Œå¹¶åœ¨ç»Ÿä¸€åè®®ä¸‹ï¼Œå¯¹å…¶åœ¨åŸºäºæ–‡æœ¬çš„é—®é¢˜å›ç­”å’Œè§†è§‰é—®é¢˜å›ç­”ä»»åŠ¡ä¸Šçš„é›¶èµ·ç‚¹æ€ç»´é“¾æ¨ç†æ€§èƒ½è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚æˆ‘ä»¬ä»¥MedQAã€MedXpertQAï¼ˆæ–‡æœ¬å’Œå¤šåª’ä½“ï¼‰ã€MMLUåŒ»å­¦å­é›†ã€USMLEè‡ªæˆ‘è¯„ä¼°è€ƒè¯•å’ŒVQA-RADçš„æ ‡å‡†åˆ†å‰²ä¸ºåŸºå‡†ï¼Œå¯¹GPT-5ã€GPT-5-miniã€GPT-5-nanoä»¥åŠGPT-4o-2024-11-20è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒGPT-5æŒç»­è¶…è¶Šæ‰€æœ‰åŸºçº¿ï¼Œåœ¨æ‰€æœ‰çš„é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨å¤šæ¨¡å¼æ¨ç†ä¸­å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚åœ¨MedXpertQA MMä¸Šï¼ŒGPT-5çš„æ¨ç†å’Œç†è§£åˆ†æ•°åˆ†åˆ«æ¯”GPT-4oæé«˜äº†+29.62%å’Œ+36.18%ï¼Œå¹¶ä¸”å…¶æ¨ç†å’Œç†è§£çš„å¾—åˆ†è¶…è¿‡äº†é¢„å…ˆæˆæƒçš„ä¸“å®¶+24.23%å’Œ+29.40%ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGPT-4oåœ¨å¤§å¤šæ•°ç»´åº¦ä¸Šä»ç„¶ä½äºäººç±»ä¸“å®¶çš„è¡¨ç°ã€‚ä¸€ä¸ªå…¸å‹çš„æ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†GPT-5å°†è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢æ•´åˆåˆ°è¿è´¯çš„è¯Šæ–­æ¨ç†é“¾ä¸­çš„èƒ½åŠ›ï¼Œå¹¶æ¨èäº†é€‚å½“çš„é«˜é£é™©å¹²é¢„æªæ–½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨è¿™äº›å—æ§çš„å¤šæ¨¡å¼æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒGPT-5çš„è¡¨ç°ä»ä¸äººç±»ç›¸å½“æå‡è‡³è¶…è¿‡äººç±»ä¸“å®¶ã€‚è¿™ä¸€è¿›æ­¥å¯èƒ½ä¼šä¸ºæœªæ¥çš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿæä¾›é‡è¦ä¿¡æ¯å‚è€ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08224v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä½¿å¾—é€šç”¨ç³»ç»Ÿèƒ½å¤Ÿåœ¨ä¸éœ€è¦å¹¿æ³›å¾®è°ƒçš„æƒ…å†µä¸‹æ‰§è¡Œè¶Šæ¥è¶Šå¤æ‚çš„é¢†åŸŸç‰¹å®šæ¨ç†ã€‚æœ¬ç ”ç©¶å°†GPT-5å®šä½ä¸ºåŒ»ç–—å†³ç­–æ”¯æŒçš„ä¸€èˆ¬æ€§å¤šæ¨¡å¼æ¨ç†å™¨ï¼Œå¹¶ç³»ç»Ÿåœ°è¯„ä¼°å…¶åœ¨ç»Ÿä¸€åè®®ä¸‹æ–‡æœ¬é—®é¢˜å’Œè§†è§‰é—®é¢˜å›ç­”ä»»åŠ¡ä¸Šçš„é›¶å°„å‡»é“¾å¼æ€ç»´æ¨ç†æ€§èƒ½ã€‚æˆ‘ä»¬å¯¹GPT-5ã€GPT-5-miniã€GPT-5-nanoä»¥åŠGPT-4o-2024-11-20è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œä¸MedQAã€MedXpertQAï¼ˆæ–‡æœ¬å’Œå¤šæ¨¡å¼ï¼‰ã€MMLUåŒ»ç–—å­é›†ã€USMLEè‡ªæˆ‘è¯„ä¼°è€ƒè¯•å’ŒVQA-RADçš„æ ‡å‡†åˆ†å‰²æ•°æ®è¿›è¡Œæ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒGPT-5å§‹ç»ˆä¼˜äºæ‰€æœ‰åŸºçº¿ï¼Œåœ¨é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨å¤šæ¨¡å¼æ¨ç†ä¸­å–å¾—äº†é‡å¤§è¿›å±•ã€‚åœ¨MedXpertQA MMä¸Šï¼ŒGPT-5åœ¨æ¨ç†å’Œç†è§£æ–¹é¢çš„å¾—åˆ†åˆ†åˆ«æ¯”GPT-4oæé«˜äº†+29.62%å’Œ+36.18%ï¼Œå¹¶ä¸”è¶…è¶Šäº†é¢„å…ˆæˆæƒçš„äººç±»ä¸“å®¶åœ¨æ¨ç†å’Œç†è§£æ–¹é¢çš„+24.23%å’Œ+29.40%ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGPT-4oåœ¨å¤§å¤šæ•°ç»´åº¦ä¸Šä»ä½äºäººç±»ä¸“å®¶çš„æ€§èƒ½ã€‚ä¸€ä¸ªå…¸å‹çš„æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼ŒGPT-5èƒ½å¤Ÿæ•´åˆè§†è§‰å’Œæ–‡æœ¬çº¿ç´¢ï¼Œå½¢æˆä¸€ä¸ªè¿è´¯çš„è¯Šæ–­æ¨ç†é“¾ï¼Œæ¨èé€‚å½“çš„é«˜é£é™©å¹²é¢„æªæ–½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨è¿™äº›å—æ§çš„å¤šæ¨¡å¼æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒGPT-5çš„è¡¨ç°å·²ä»äººç±»ç›¸å½“æ°´å¹³æå‡è‡³è¶…è¶Šäººç±»ä¸“å®¶ã€‚è¿™ä¸€è¿›æ­¥å¯èƒ½å¤§å¤§å½±å“æœªæ¥ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿçš„è®¾è®¡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMçš„æœ€æ–°è¿›å±•ä½¿å¾—é€šç”¨ç³»ç»Ÿåœ¨æ‰§è¡Œå¤æ‚çš„é¢†åŸŸç‰¹å®šæ¨ç†ä»»åŠ¡æ—¶è¡¨ç°å‡ºè‰²ï¼Œæ— éœ€å¹¿æ³›å¾®è°ƒã€‚</li>
<li>GPT-5è¢«å®šä½ä¸ºåŒ»ç–—å†³ç­–æ”¯æŒä¸­çš„ä¸€èˆ¬æ€§å¤šæ¨¡å¼æ¨ç†å™¨ï¼Œå¹¶è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚</li>
<li>GPT-5åœ¨å¤šç§æ ‡å‡†åŒ–é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†è®¸å¤šåŸºçº¿æ¨¡å‹ã€‚</li>
<li>GPT-5åœ¨MedXpertQAä¸Šçš„è¡¨ç°ä¼˜äºGPT-4oå’Œäººç±»ä¸“å®¶ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ¨ç†å’Œç†è§£èƒ½åŠ›ã€‚</li>
<li>GPT-5èƒ½å¤Ÿæ•´åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œå½¢æˆè¿è´¯çš„è¯Šæ–­æ¨ç†é“¾ã€‚</li>
<li>GPT-5åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°å·²è¶…è¶Šäººç±»ä¸“å®¶ï¼Œè¿™å¯èƒ½å¯¹æœªæ¥çš„ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿäº§ç”Ÿé‡å¤§å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08224">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0ce9395c05b976a91aebde4e3bec7c85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bb998a42975d483a08aa26d2dc35a92.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a73b4ff65d0519f7b792e2d55a2305e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-580156970b0d96670ede817ccd990adf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ce5ac46480a182d7a80c946aa269693a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-160ae7e01499b34e236ee50f7948eac8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c5912ddd6627ade3fc53834262caea54.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Learning-User-Preferences-for-Image-Generation-Model"><a href="#Learning-User-Preferences-for-Image-Generation-Model" class="headerlink" title="Learning User Preferences for Image Generation Model"></a>Learning User Preferences for Image Generation Model</h2><p><strong>Authors:Wenyi Mo, Ying Ba, Tianyu Zhang, Yalong Bai, Biye Li</strong></p>
<p>User preference prediction requires a comprehensive and accurate understanding of individual tastes. This includes both surface-level attributes, such as color and style, and deeper content-related aspects, such as themes and composition. However, existing methods typically rely on general human preferences or assume static user profiles, often neglecting individual variability and the dynamic, multifaceted nature of personal taste. To address these limitations, we propose an approach built upon Multimodal Large Language Models, introducing contrastive preference loss and preference tokens to learn personalized user preferences from historical interactions. The contrastive preference loss is designed to effectively distinguish between user â€˜â€™likesâ€™â€™ and â€˜â€™dislikesâ€™â€™, while the learnable preference tokens capture shared interest representations among existing users, enabling the model to activate group-specific preferences and enhance consistency across similar users. Extensive experiments demonstrate our model outperforms other methods in preference prediction accuracy, effectively identifying users with similar aesthetic inclinations and providing more precise guidance for generating images that align with individual tastes. The project page is \texttt{<a target="_blank" rel="noopener" href="https://learn-user-pref.github.io/%7D">https://learn-user-pref.github.io/}</a>. </p>
<blockquote>
<p>ç”¨æˆ·åå¥½é¢„æµ‹éœ€è¦å¯¹ä¸ªäººå£å‘³æœ‰å…¨é¢å‡†ç¡®çš„ç†è§£ã€‚è¿™åŒ…æ‹¬è¡¨é¢çº§åˆ«çš„å±æ€§ï¼Œå¦‚é¢œè‰²å’Œé£æ ¼ï¼Œä»¥åŠæ›´æ·±çš„å†…å®¹ç›¸å…³æ–¹é¢ï¼Œå¦‚ä¸»é¢˜å’Œç»„æˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºä¸€èˆ¬çš„äººç±»åå¥½æˆ–å‡è®¾ç”¨æˆ·é…ç½®æ–‡ä»¶æ˜¯é™æ€çš„ï¼Œå¾€å¾€å¿½ç•¥äº†ä¸ªäººå·®å¼‚æ€§ä»¥åŠä¸ªäººå£å‘³çš„åŠ¨æ€ã€å¤šå…ƒæ€§è´¨ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œå¼•å…¥å¯¹æ¯”åå¥½æŸå¤±å’Œåå¥½ä»¤ç‰Œï¼Œä»å†å²äº¤äº’ä¸­å­¦ä¹ ä¸ªæ€§åŒ–ç”¨æˆ·åå¥½ã€‚å¯¹æ¯”åå¥½æŸå¤±æ—¨åœ¨æœ‰æ•ˆåŒºåˆ†ç”¨æˆ·â€œå–œæ¬¢â€å’Œâ€œä¸å–œæ¬¢â€çš„ç‰©å“ï¼Œè€Œå¯å­¦ä¹ çš„åå¥½ä»¤ç‰Œå¯ä»¥æ•è·ç°æœ‰ç”¨æˆ·ä¹‹é—´çš„å…±äº«å…´è¶£è¡¨ç¤ºï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ¿€æ´»ç‰¹å®šç¾¤ä½“çš„åå¥½ï¼Œå¹¶å¢å¼ºç›¸ä¼¼ç”¨æˆ·ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨åå¥½é¢„æµ‹å‡†ç¡®æ€§æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œèƒ½å¤Ÿè¯†åˆ«å…·æœ‰ç›¸ä¼¼å®¡ç¾å€¾å‘çš„ç”¨æˆ·ï¼Œå¹¶ä¸ºç”Ÿæˆç¬¦åˆä¸ªäººå£å‘³çš„å›¾åƒæä¾›æ›´ç²¾ç¡®çš„æŒ‡å¯¼ã€‚é¡¹ç›®é¡µé¢æ˜¯[<a target="_blank" rel="noopener" href="https://learn-user-pref.github.io/]%E3%80%82">https://learn-user-pref.github.io/]ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08220v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”¨æˆ·å¤šæ¨¡æ€äº¤äº’çš„å¤§è¯­è¨€æ¨¡å‹ä¸ªæ€§åŒ–åå¥½é¢„æµ‹æ–¹æ³•ï¼Œç»“åˆäº†ç”¨æˆ·è¡¨é¢çš„å±æ€§å–œå¥½ä¸æ·±å±‚æ¬¡çš„å†…å®¹åå¥½ã€‚å¼•å…¥å¯¹æ¯”åå¥½æŸå¤±å’Œä¸ªäººåå¥½ä»¤ç‰Œï¼Œé€šè¿‡ç”¨æˆ·å†å²äº’åŠ¨å­¦ä¹ ä¸ªæ€§åŒ–åå¥½ã€‚å¯¹æ¯”åå¥½æŸå¤±èƒ½åŒºåˆ†ç”¨æˆ·çš„â€œå–œæ¬¢â€å’Œâ€œä¸å–œæ¬¢â€ï¼Œè€Œå­¦ä¹ åå¥½ä»¤ç‰Œå¯æ•è·ç°æœ‰ç”¨æˆ·é—´çš„å…±äº«å…´è¶£è¡¨ç¤ºã€‚æ­¤æ¨¡å‹æé«˜äº†é¢„æµ‹å‡†ç¡®ç‡ï¼Œèƒ½è¯†åˆ«æœ‰ç›¸ä¼¼å®¡ç¾å€¾å‘çš„ç”¨æˆ·ï¼Œå¹¶ä¸ºç”Ÿæˆç¬¦åˆä¸ªäººå–œå¥½çš„å›¾åƒæä¾›æ›´ç²¾ç¡®æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”¨æˆ·åå¥½é¢„æµ‹éœ€è¦å…¨é¢å‡†ç¡®åœ°äº†è§£ä¸ªäººå£å‘³ï¼ŒåŒ…æ‹¬è¡¨é¢å±‚æ¬¡çš„å±æ€§å’Œæ·±å±‚æ¬¡çš„å†…å®¹æ–¹é¢ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¸¸å¸¸å¿½ç•¥ä¸ªä½“å·®å¼‚æ€§ä»¥åŠä¸ªäººå–œå¥½çš„åŠ¨æ€ã€å¤šé¢æ€§ã€‚</li>
<li>æå‡ºåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§£å†³æ–¹æ³•ï¼Œå¼•å…¥å¯¹æ¯”åå¥½æŸå¤±ä»¥åŒºåˆ†ç”¨æˆ·çš„â€œå–œæ¬¢â€å’Œâ€œä¸å–œæ¬¢â€ã€‚</li>
<li>å¼•å…¥å­¦ä¹ åå¥½ä»¤ç‰Œï¼Œèƒ½å¤Ÿæ•æ‰ç°æœ‰ç”¨æˆ·ä¹‹é—´çš„å…±äº«å…´è¶£è¡¨ç¤ºï¼Œæ¿€æ´»ç‰¹å®šç¾¤ä½“çš„åå¥½ï¼Œå¹¶å¢å¼ºç›¸ä¼¼ç”¨æˆ·ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨åå¥½é¢„æµ‹å‡†ç¡®æ€§æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å…·æœ‰ç›¸ä¼¼å®¡ç¾å€¾å‘çš„ç”¨æˆ·ï¼Œä¸ºç”Ÿæˆç¬¦åˆä¸ªäººå–œå¥½çš„å›¾åƒæä¾›æ›´ç²¾ç¡®æŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08220">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-93e69afede3eac17c25b7bb8af34a022.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0a57569b99e6c86c46b49b4f7cd8952.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0313d644f3368e49175529fc4a9e6f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4ba94a9e042bf2313701f19a5243b69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b6e7ea8e9561507a70fd6c5aa4d651a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a12132d0598306422bc145c109a70e0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Efficient-Speculative-Decoding-for-Llama-at-Scale-Challenges-and-Solutions"><a href="#Efficient-Speculative-Decoding-for-Llama-at-Scale-Challenges-and-Solutions" class="headerlink" title="Efficient Speculative Decoding for Llama at Scale: Challenges and   Solutions"></a>Efficient Speculative Decoding for Llama at Scale: Challenges and   Solutions</h2><p><strong>Authors:Bangsheng Tang, Carl Chengyan Fu, Fei Kou, Grigory Sizov, Haoci Zhang, Jason Park, Jiawen Liu, Jie You, Qirui Yang, Sachin Mehta, Shengyong Cai, Xiaodong Wang, Xingyu Liu, Yunlu Li, Yanjun Zhou, Wei Wei, Zhiwei Zhao, Zixi Qi, Adolfo Victoria, Aya Ibrahim, Bram Wasti, Changkyu Kim, Daniel Haziza, Fei Sun, Giancarlo Delfin, Emily Guo, Jialin Ouyang, Jaewon Lee, Jianyu Huang, Jeremy Reizenstein, Lu Fang, Quinn Zhu, Ria Verma, Vlad Mihailescu, Xingwen Guo, Yan Cui, Ye Hu, Yejin Lee</strong></p>
<p>Speculative decoding is a standard method for accelerating the inference speed of large language models. However, scaling it for production environments poses several engineering challenges, including efficiently implementing different operations (e.g., tree attention and multi-round speculative decoding) on GPU. In this paper, we detail the training and inference optimization techniques that we have implemented to enable EAGLE-based speculative decoding at a production scale for Llama models. With these changes, we achieve a new state-of-the-art inference latency for Llama models. For example, Llama4 Maverick decodes at a speed of about 4 ms per token (with a batch size of one) on 8 NVIDIA H100 GPUs, which is 10% faster than the previously best known method. Furthermore, for EAGLE-based speculative decoding, our optimizations enable us to achieve a speed-up for large batch sizes between 1.4x and 2.0x at production scale. </p>
<blockquote>
<p>æ¨æµ‹è§£ç æ˜¯åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†é€Ÿåº¦çš„ä¸€ç§æ ‡å‡†æ–¹æ³•ã€‚ç„¶è€Œï¼Œå°†å…¶æ‰©å±•åˆ°ç”Ÿäº§ç¯å¢ƒé¢ä¸´è®¸å¤šå·¥ç¨‹æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åœ¨GPUä¸Šæœ‰æ•ˆåœ°å®ç°ä¸åŒæ“ä½œï¼ˆä¾‹å¦‚æ ‘çŠ¶æ³¨æ„åŠ›å’Œå¤šè½®æ¨æµ‹è§£ç ï¼‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†ä¸ºåº”å¯¹ç”Ÿäº§è§„æ¨¡ä¸‹åŸºäºEAGLEçš„æ¨æµ‹è§£ç è€Œå®æ–½çš„è®­ç»ƒå’Œæ¨ç†ä¼˜åŒ–æŠ€æœ¯ã€‚é€šè¿‡è¿™äº›æ›´æ”¹ï¼Œæˆ‘ä»¬ä¸ºLlamaæ¨¡å‹å®ç°äº†æœ€æ–°çš„æ¨ç†å»¶è¿Ÿè®°å½•ã€‚ä¾‹å¦‚ï¼ŒLlama4 Maverickåœ¨8ä¸ªNVIDIA H100 GPUä¸Šå®ç°äº†çº¦æ¯ä»¤ç‰Œ4æ¯«ç§’çš„è§£ç é€Ÿåº¦ï¼ˆæ‰¹å¤„ç†å¤§å°ä¸º1ï¼‰ï¼Œæ¯”å·²çŸ¥çš„æœ€ä½³æ–¹æ³•å¿«10%ã€‚æ­¤å¤–ï¼Œå¯¹äºåŸºäºEAGLEçš„æ¨æµ‹è§£ç ï¼Œæˆ‘ä»¬çš„ä¼˜åŒ–åœ¨ç”Ÿäº§è§„æ¨¡ä¸‹å®ç°äº†å¤§æ‰¹é‡é€Ÿåº¦çš„1.4å€è‡³2å€æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08192v1">PDF</a> 15 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ¨ç†é€Ÿåº¦çš„æå‡é€šå¸¸é‡‡ç”¨æŠ•æœºè§£ç è¿™ä¸€æ ‡å‡†æ–¹æ³•ã€‚ç„¶è€Œï¼Œåœ¨ç”Ÿäº§ç¯å¢ƒä¸­å®ç°è§„æ¨¡åŒ–å­˜åœ¨è¯¸å¤šå·¥ç¨‹æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨GPUä¸Šæœ‰æ•ˆå®æ–½ä¸åŒæ“ä½œï¼ˆå¦‚æ ‘çŠ¶æ³¨æ„åŠ›ä¸å¤šè½®æŠ•æœºè§£ç ï¼‰ã€‚æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†æˆ‘ä»¬ä¸ºLlamaæ¨¡å‹å®ç°åŸºäºEAGLEçš„æŠ•æœºè§£ç æ‰€è¿›è¡Œçš„è®­ç»ƒå’Œæ¨ç†ä¼˜åŒ–æŠ€æœ¯ã€‚é€šè¿‡è¿™äº›æ”¹è¿›ï¼ŒLlamaæ¨¡å‹è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆçš„æ¨ç†é€Ÿåº¦ã€‚ä¾‹å¦‚ï¼ŒLlama4 Maverickåœ¨8ä¸ªNVIDIA H100 GPUä¸Šå®ç°äº†çº¦æ¯ä»¤ç‰Œ4æ¯«ç§’çš„è§£ç é€Ÿåº¦ï¼ˆæ‰¹å¤„ç†å¤§å°ä¸º1ï¼‰ï¼Œæ¯”å·²çŸ¥çš„æœ€ä½³æ–¹æ³•å¿«10%ã€‚æ­¤å¤–ï¼Œå¯¹äºåŸºäºEAGLEçš„æŠ•æœºè§£ç ï¼Œæˆ‘ä»¬çš„ä¼˜åŒ–åœ¨ç”Ÿäº§è§„æ¨¡ä¸‹å®ç°äº†å¯¹å¤§è§„æ¨¡æ‰¹å¤„ç†çš„åŠ é€Ÿï¼Œæé€Ÿä»‹äº1.4å€è‡³2å€ä¹‹é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŠ•æœºå‹è§£ç æ˜¯ç”¨äºåŠ é€Ÿå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ¨ç†çš„æ ‡å‡†æ–¹æ³•ã€‚</li>
<li>ç”Ÿäº§ç¯å¢ƒä¸­å®ç°è§„æ¨¡åŒ–æŠ•æœºè§£ç é¢ä¸´è¯¸å¤šå·¥ç¨‹æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹Llamaæ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†ä¼˜åŒ–æŠ€æœ¯ã€‚</li>
<li>é€šè¿‡è¿™äº›æ”¹è¿›ï¼ŒLlamaæ¨¡å‹è¾¾åˆ°äº†æ–°çš„ä¸šç•Œé¢†å…ˆçš„æ¨ç†é€Ÿåº¦ã€‚</li>
<li>Llamaæ¨¡å‹çš„ä¼˜åŒ–èƒ½å¤Ÿåœ¨å¤§è§„æ¨¡æ‰¹æ¬¡ä¸‹å®ç°åŠ é€Ÿã€‚</li>
<li>åœ¨åŸºäºEAGLEçš„æŠ•æœºè§£ç ä¸­ï¼Œä¼˜åŒ–æŠ€æœ¯æ˜¾è‘—æå‡äº†ç”Ÿäº§è§„æ¨¡ä¸‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08192">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0821d780f86a628e2ffec1a1f9685a7b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-79d1a07354de9d79eeaebf45dd50d025.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce0ba2d5844a5493e3255efaae488567.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-711d43cadf11984081533596aa79347d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-159fd0f28aece46bf38a9a3e3d76e350.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45a1d1422bbba1e1650ce2dc5783e594.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-in-Vision-A-Survey"><a href="#Reinforcement-Learning-in-Vision-A-Survey" class="headerlink" title="Reinforcement Learning in Vision: A Survey"></a>Reinforcement Learning in Vision: A Survey</h2><p><strong>Authors:Weijia Wu, Chen Gao, Joya Chen, Kevin Qinghong Lin, Qingwei Meng, Yiming Zhang, Yuke Qiu, Hong Zhou, Mike Zheng Shou</strong></p>
<p>Recent advances at the intersection of reinforcement learning (RL) and visual intelligence have enabled agents that not only perceive complex visual scenes but also reason, generate, and act within them. This survey offers a critical and up-to-date synthesis of the field. We first formalize visual RL problems and trace the evolution of policy-optimization strategies from RLHF to verifiable reward paradigms, and from Proximal Policy Optimization to Group Relative Policy Optimization. We then organize more than 200 representative works into four thematic pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models. For each pillar we examine algorithmic design, reward engineering, benchmark progress, and we distill trends such as curriculum-driven training, preference-aligned diffusion, and unified reward modeling. Finally, we review evaluation protocols spanning set-level fidelity, sample-level preference, and state-level stability, and we identify open challenges that include sample efficiency, generalization, and safe deployment. Our goal is to provide researchers and practitioners with a coherent map of the rapidly expanding landscape of visual RL and to highlight promising directions for future inquiry. Resources are available at: <a target="_blank" rel="noopener" href="https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning">https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning</a>. </p>
<blockquote>
<p>è¿‘æœŸå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸è§†è§‰æ™ºèƒ½äº¤å‰é¢†åŸŸçš„è¿›å±•ä½¿å¾—æ™ºèƒ½ä½“ä¸ä»…èƒ½å¤Ÿæ„ŸçŸ¥å¤æ‚çš„è§†è§‰åœºæ™¯ï¼Œè¿˜èƒ½åœ¨è¿™äº›åœºæ™¯ä¸­è¿›è¡Œæ¨ç†ã€ç”Ÿæˆå’Œè¡ŒåŠ¨ã€‚è¿™ç¯‡ç»¼è¿°å¯¹è¯¥é¢†åŸŸè¿›è¡Œäº†æ‰¹åˆ¤æ€§å’Œæœ€æ–°çš„ç»¼åˆã€‚æˆ‘ä»¬é¦–å…ˆæ­£å¼æå‡ºè§†è§‰RLé—®é¢˜ï¼Œå¹¶è¿½æº¯ç­–ç•¥ä¼˜åŒ–ç­–ç•¥ä»RLHFåˆ°å¯éªŒè¯å¥–åŠ±èŒƒå¼çš„å‘å±•ï¼Œä»¥åŠä»è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–åˆ°ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„å‘å±•ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¶…è¿‡200ç¯‡ä»£è¡¨æ€§ä½œå“æ•´ç†ä¸ºå››ä¸ªä¸»é¢˜æ”¯æŸ±ï¼šå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€è§†è§‰ç”Ÿæˆã€ç»Ÿä¸€æ¨¡å‹æ¡†æ¶å’Œè§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹ã€‚å¯¹äºæ¯ä¸ªæ”¯æŸ±ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç®—æ³•è®¾è®¡ã€å¥–åŠ±å·¥ç¨‹ã€åŸºå‡†è¿›åº¦ï¼Œå¹¶æ€»ç»“äº†è¶‹åŠ¿ï¼Œå¦‚è¯¾ç¨‹é©±åŠ¨è®­ç»ƒã€åå¥½å¯¹é½æ‰©æ•£å’Œç»Ÿä¸€å¥–åŠ±å»ºæ¨¡ã€‚æœ€åï¼Œæˆ‘ä»¬å›é¡¾äº†åŒ…æ‹¬é›†åˆçº§ä¿çœŸåº¦ã€æ ·æœ¬çº§åå¥½å’ŒçŠ¶æ€çº§ç¨³å®šæ€§çš„è¯„ä¼°åè®®ï¼Œå¹¶ç¡®å®šäº†å¼€æ”¾æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ ·æœ¬æ•ˆç‡ã€æ³›åŒ–å’Œå®‰å…¨éƒ¨ç½²ç­‰ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›è§†è§‰RLå¿«é€Ÿæ‰©å±•æ™¯è§‚çš„ä¸€è‡´åœ°å›¾ï¼Œå¹¶çªå‡ºæœªæ¥æŸ¥è¯¢çš„æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚èµ„æºå¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning%E3%80%82">https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learningã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08189v1">PDF</a> 22 pages</p>
<p><strong>Summary</strong><br>è§†è§‰å¼ºåŒ–å­¦ä¹ ï¼ˆVisual RLï¼‰æ˜¯å¼ºåŒ–å­¦ä¹ ä¸è§†è§‰æ™ºèƒ½çš„äº¤å‰ç‚¹ï¼Œè¿‘å¹´æ¥å–å¾—äº†é‡è¦è¿›å±•ã€‚æœ¬æ–‡ç»¼è¿°äº†è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ï¼Œä»‹ç»äº†è§†è§‰RLé—®é¢˜çš„å½¢å¼åŒ–å®šä¹‰ï¼Œè¿½è¸ªäº†ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ä»RLHFåˆ°å¯éªŒè¯å¥–åŠ±æ¨¡å¼çš„æ¼”å˜ï¼Œå¹¶æ•´ç†äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€è§†è§‰ç”Ÿæˆã€ç»Ÿä¸€æ¨¡å‹æ¡†æ¶å’Œè§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹ç­‰å››å¤§æ”¯æŸ±çš„ç ”ç©¶å·¥ä½œã€‚æœ¬æ–‡è¿˜æ¢è®¨äº†ç®—æ³•è®¾è®¡ã€å¥–åŠ±å·¥ç¨‹ã€åŸºå‡†æµ‹è¯•è¿›å±•ç­‰è¶‹åŠ¿ï¼Œå¹¶å›é¡¾äº†è¯„ä¼°åè®®å’Œå¼€æ”¾æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰å¼ºåŒ–å­¦ä¹ ç»“åˆäº†å¼ºåŒ–å­¦ä¹ ä¸è§†è§‰æ™ºèƒ½ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å¤æ‚è§†è§‰åœºæ™¯ä¸­æ„ŸçŸ¥ã€æ¨ç†ã€ç”Ÿæˆå’Œè¡ŒåŠ¨ã€‚</li>
<li>æœ¬æ–‡å½¢å¼åŒ–äº†è§†è§‰RLé—®é¢˜çš„å®šä¹‰ï¼Œå¹¶è¿½è¸ªäº†ç­–ç•¥ä¼˜åŒ–æ–¹æ³•çš„æ¼”å˜ã€‚</li>
<li>å››å¤§æ”¯æŸ±çš„ç ”ç©¶å·¥ä½œåŒ…æ‹¬å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€è§†è§‰ç”Ÿæˆã€ç»Ÿä¸€æ¨¡å‹æ¡†æ¶å’Œè§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹ã€‚</li>
<li>è¶‹åŠ¿åŒ…æ‹¬è¯¾ç¨‹é©±åŠ¨è®­ç»ƒã€åå¥½å¯¹é½æ‰©æ•£å’Œç»Ÿä¸€å¥–åŠ±å»ºæ¨¡ç­‰ã€‚</li>
<li>è¯„ä¼°åè®®åŒ…æ‹¬é›†åˆçº§ä¿çœŸåº¦ã€æ ·æœ¬çº§åå¥½å’ŒçŠ¶æ€çº§ç¨³å®šæ€§ã€‚</li>
<li>æ ·æœ¬æ•ˆç‡ã€æ³›åŒ–å’Œå®‰å…¨éƒ¨ç½²æ˜¯è§†è§‰å¼ºåŒ–å­¦ä¹ çš„å¼€æ”¾æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08189">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4d66785c9fa360f6dbe459f20c592480.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a516597ac992add14c481d36867c359c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afd40acd9948d5a52371a5e494c50ca8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95f89a4e2c0b25c1fade35b3973523da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e6699aee2862070236411b972d3c41f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MedReasoner-Reinforcement-Learning-Drives-Reasoning-Grounding-from-Clinical-Thought-to-Pixel-Level-Precision"><a href="#MedReasoner-Reinforcement-Learning-Drives-Reasoning-Grounding-from-Clinical-Thought-to-Pixel-Level-Precision" class="headerlink" title="MedReasoner: Reinforcement Learning Drives Reasoning Grounding from   Clinical Thought to Pixel-Level Precision"></a>MedReasoner: Reinforcement Learning Drives Reasoning Grounding from   Clinical Thought to Pixel-Level Precision</h2><p><strong>Authors:Zhonghao Yan, Muxi Diao, Yuxuan Yang, Jiayuan Xu, Kaizhou Zhang, Ruoyan Jing, Lele Yang, Yanxi Liu, Kongming Liang, Zhanyu Ma</strong></p>
<p>Accurately grounding regions of interest (ROIs) is critical for diagnosis and treatment planning in medical imaging. While multimodal large language models (MLLMs) combine visual perception with natural language, current medical-grounding pipelines still rely on supervised fine-tuning with explicit spatial hints, making them ill-equipped to handle the implicit queries common in clinical practice. This work makes three core contributions. We first define Unified Medical Reasoning Grounding (UMRG), a novel vision-language task that demands clinical reasoning and pixel-level grounding. Second, we release U-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside implicit clinical queries and reasoning traces, spanning 10 modalities, 15 super-categories, and 108 specific categories. Finally, we introduce MedReasoner, a modular framework that distinctly separates reasoning from segmentation: an MLLM reasoner is optimized with reinforcement learning, while a frozen segmentation expert converts spatial prompts into masks, with alignment achieved through format and accuracy rewards. MedReasoner achieves state-of-the-art performance on U-MRG-14K and demonstrates strong generalization to unseen clinical queries, underscoring the significant promise of reinforcement learning for interpretable medical grounding. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒçš„è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’åˆ¶å®šä¸­ï¼Œå‡†ç¡®åœ°å¯¹æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIsï¼‰è¿›è¡Œå®šä½è‡³å…³é‡è¦ã€‚è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç»“åˆäº†è§†è§‰æ„ŸçŸ¥ä¸è‡ªç„¶è¯­è¨€ï¼Œä½†å½“å‰çš„åŒ»å­¦å®šä½æµç¨‹ä»ä¾èµ–äºå…·æœ‰æ˜ç¡®ç©ºé—´æç¤ºçš„ç›‘ç£å¾®è°ƒï¼Œå› æ­¤éš¾ä»¥åº”å¯¹ä¸´åºŠå®è·µä¸­å¸¸è§çš„éšå«æŸ¥è¯¢ã€‚æœ¬æ–‡ä½œå‡ºä¸‰é¡¹æ ¸å¿ƒè´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰äº†ç»Ÿä¸€åŒ»å­¦æ¨ç†å®šä½ï¼ˆUMRGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€ä»»åŠ¡ï¼Œè¦æ±‚è¿›è¡Œä¸´åºŠæ¨ç†å’Œåƒç´ çº§å®šä½ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å‘å¸ƒäº†U-MRG-14Kæ•°æ®é›†ï¼ŒåŒ…å«14Kä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬å…·æœ‰åƒç´ çº§æ©è†œã€éšå«çš„ä¸´åºŠæŸ¥è¯¢å’Œæ¨ç†è½¨è¿¹ï¼Œæ¶µç›–10ç§æ¨¡æ€ã€15ä¸ªè¶…ç±»åˆ«å’Œ108ä¸ªç‰¹å®šç±»åˆ«ã€‚æœ€åï¼Œæˆ‘ä»¬æ¨å‡ºäº†MedReasonerï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œå°†æ¨ç†ä¸åˆ†å‰²åŒºåˆ†å¼€ï¼šMLLMæ¨ç†å™¨é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ï¼Œè€Œå†»ç»“çš„åˆ†å‰²ä¸“å®¶å°†ç©ºé—´æç¤ºè½¬æ¢ä¸ºæ©è†œï¼Œé€šè¿‡æ ¼å¼å’Œç²¾åº¦å¥–åŠ±å®ç°å¯¹é½ã€‚MedReasoneråœ¨U-MRG-14Kä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å¯¹æœªè§è¿‡çš„ä¸´åºŠæŸ¥è¯¢è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œçªæ˜¾äº†å¼ºåŒ–å­¦ä¹ åœ¨å¯è§£é‡Šçš„åŒ»å­¦å®šä½ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08177v1">PDF</a> 37 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºåŒ»ç–—æˆåƒä¸­çš„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰çš„å‡†ç¡®å®šä½å¯¹äºè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç»“åˆäº†è§†è§‰æ„ŸçŸ¥å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œä½†ç°æœ‰çš„åŒ»ç–—å®šä½æµç¨‹ä»ä¾èµ–äºå¸¦æœ‰æ˜ç¡®ç©ºé—´æç¤ºçš„ç›‘ç£å¾®è°ƒï¼Œæ— æ³•åº”å¯¹ä¸´åºŠå®è·µä¸­çš„éšå¼æŸ¥è¯¢ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸‰é¡¹æ ¸å¿ƒè´¡çŒ®ã€‚é¦–å…ˆï¼Œå®šä¹‰äº†ç»Ÿä¸€åŒ»å­¦æ¨ç†å®šä½ï¼ˆUMRGï¼‰è¿™ä¸€æ–°å‹è§†è§‰è¯­è¨€ä»»åŠ¡ï¼Œéœ€è¦ä¸´åºŠæ¨ç†å’Œåƒç´ çº§å®šä½ã€‚å…¶æ¬¡ï¼Œæ¨å‡ºäº†åŒ…å«è¶…è¿‡1ä¸‡ä¸ªæ ·æœ¬çš„U-MRG-14Kæ•°æ®é›†ï¼Œæ•°æ®é›†ä¸­çš„æ¯ä¸ªæ ·æœ¬åŒ…å«åƒç´ çº§è’™ç‰ˆå’Œä¼´éšçš„éšå¼ä¸´åºŠæŸ¥è¯¢åŠæ¨ç†è½¨è¿¹ï¼Œæ¶µç›–åç§æ¨¡æ€ã€åäº”ç§è¶…çº§ç±»åˆ«å’Œä¸€ç™¾é›¶å…«ç§ç‰¹å®šç±»åˆ«ã€‚æœ€åï¼Œæ¨å‡ºäº†MedReasoneræ¨¡å—åŒ–æ¡†æ¶ï¼Œå°†æ¨ç†ä¸åˆ†å‰²åˆ†ç¦»ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–MLLMæ¨ç†å™¨ï¼Œå†»ç»“çš„åˆ†å‰²ä¸“å®¶å°†ç©ºé—´æç¤ºè½¬æ¢ä¸ºè’™ç‰ˆï¼Œé€šè¿‡æ ¼å¼å’Œå‡†ç¡®æ€§å¥–åŠ±å®ç°å¯¹é½ã€‚MedReasoneråœ¨U-MRG-14Kæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶ä¸”å¯¹æœªè§è¿‡çš„ä¸´åºŠæŸ¥è¯¢å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œçªæ˜¾å¼ºåŒ–å­¦ä¹ åœ¨å¯è§£é‡Šçš„åŒ»å­¦å®šä½æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»Ÿä¸€åŒ»å­¦æ¨ç†å®šä½ï¼ˆUMRGï¼‰æˆä¸ºå¿…è¦çš„æ–°å‹è§†è§‰è¯­è¨€ä»»åŠ¡ï¼Œå› ä¸ºå®ƒè¦æ±‚ç»“åˆä¸´åºŠæ¨ç†å’Œåƒç´ çº§å®šä½æŠ€æœ¯æ¥å¤„ç†å¤æ‚çš„åŒ»å­¦æˆåƒæ•°æ®ã€‚</li>
<li>U-MRG-14Kæ•°æ®é›†å¡«è¡¥äº†é’ˆå¯¹åŒ»å­¦æˆåƒçš„éšå¼æŸ¥è¯¢ç ”ç©¶çš„ç©ºç™½ï¼ŒåŒ…å«äº†ä¸°å¯Œçš„æ ·æœ¬æ•°æ®å’Œå¤šæ ·åŒ–çš„ä¸´åºŠä¿¡æ¯ã€‚</li>
<li>MedReasoneræ¨¡å—åŒ–æ¡†æ¶é¦–æ¬¡æˆåŠŸå°†æ¨ç†ä¸åˆ†å‰²åˆ†ç¦»ï¼Œæé«˜äº†æ¨¡å‹çš„çµæ´»æ€§å’Œæ•ˆç‡ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨MedReasonerä¸­å‘æŒ¥äº†å…³é”®ä½œç”¨ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çš„åŒ»å­¦æ•°æ®å¹¶æé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>MedReasoneråœ¨U-MRG-14Kæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚</li>
<li>å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†åŒ»å­¦æˆåƒæ•°æ®æ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦é€šè¿‡ç ”ç©¶æ–°çš„æ–¹æ³•å’Œç­–ç•¥æ¥æé«˜å…¶æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08177">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1f06984455fc0a851fbffcb0d6207f70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a378eeee81f6ec3a4240577c60bd85dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27209ab38731dc4fccd7894dcb778d1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f1f90c911d27ca95264e0ad8d6239bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee783866ffd89bcf426bcca3b2ed5aec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-11a3bb59d06b2e899f246a04a58dad60.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="REX-RAG-Reasoning-Exploration-with-Policy-Correction-in-Retrieval-Augmented-Generation"><a href="#REX-RAG-Reasoning-Exploration-with-Policy-Correction-in-Retrieval-Augmented-Generation" class="headerlink" title="REX-RAG: Reasoning Exploration with Policy Correction in   Retrieval-Augmented Generation"></a>REX-RAG: Reasoning Exploration with Policy Correction in   Retrieval-Augmented Generation</h2><p><strong>Authors:Wentao Jiang, Xiang Feng, Zengmao Wang, Yong Luo, Pingbo Xu, Zhe Chen, Bo Du, Jing Zhang</strong></p>
<p>Reinforcement learning (RL) is emerging as a powerful paradigm for enabling large language models (LLMs) to perform complex reasoning tasks. Recent advances indicate that integrating RL with retrieval-augmented generation (RAG) allows LLMs to dynamically incorporate external knowledge, leading to more informed and robust decision making. However, we identify a critical challenge during policy-driven trajectory sampling: LLMs are frequently trapped in unproductive reasoning paths, which we refer to as â€œdead endsâ€, committing to overconfident yet incorrect conclusions. This severely hampers exploration and undermines effective policy optimization. To address this challenge, we propose REX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation), a novel framework that explores alternative reasoning paths while maintaining rigorous policy learning through principled distributional corrections. Our approach introduces two key innovations: (1) Mixed Sampling Strategy, which combines a novel probe sampling method with exploratory prompts to escape dead ends; and (2) Policy Correction Mechanism, which employs importance sampling to correct distribution shifts induced by mixed sampling, thereby mitigating gradient estimation bias. We evaluate it on seven question-answering benchmarks, and the experimental results show that REX-RAG achieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B over strong baselines, demonstrating competitive results across multiple datasets. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/MiliLab/REX-RAG">https://github.com/MiliLab/REX-RAG</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ­£æˆä¸ºä¸€ç§å¼ºå¤§çš„èŒƒå¼ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿæ‰§è¡Œå¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚æœ€è¿‘çš„è¿›å±•è¡¨æ˜ï¼Œå°†RLä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç›¸ç»“åˆï¼Œå¯ä»¥ä½¿LLMåŠ¨æ€åœ°èå…¥å¤–éƒ¨çŸ¥è¯†ï¼Œä»è€Œå®ç°æ›´åŠ æ˜æ™ºå’Œç¨³å¥çš„å†³ç­–ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬åœ¨æ”¿ç­–é©±åŠ¨çš„è½¨è¿¹é‡‡æ ·è¿‡ç¨‹ä¸­å‘ç°äº†ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼šLLMç»å¸¸é™·å…¥ä¸äº§ç”Ÿç»“æœçš„æ¨ç†è·¯å¾„ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæ­»èƒ¡åŒâ€ï¼Œå¹¶å¯¼è‡´è¿‡äºè‡ªä¿¡ä¸”é”™è¯¯çš„ç»“è®ºã€‚è¿™ä¸¥é‡é˜»ç¢äº†æ¢ç´¢å¹¶ç ´åäº†æœ‰æ•ˆçš„æ”¿ç­–ä¼˜åŒ–ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†REX-RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆä¸­çš„æ¨ç†æ¢ç´¢ä¸æ”¿ç­–æ ¡æ­£ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒä¸¥æ ¼æ”¿ç­–å­¦ä¹ çš„æƒ…å†µä¸‹æ¢ç´¢æ›¿ä»£çš„æ¨ç†è·¯å¾„ï¼Œå¹¶é€šè¿‡åŸåˆ™æ€§çš„åˆ†å¸ƒæ ¡æ­£æ¥è¿›è¡Œè°ƒæ•´ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸¤ä¸ªå…³é”®çš„åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰æ··åˆé‡‡æ ·ç­–ç•¥ï¼Œå®ƒå°†ä¸€ç§æ–°çš„æ¢é’ˆé‡‡æ ·æ–¹æ³•ä¸æ¢ç´¢æ€§æç¤ºç›¸ç»“åˆï¼Œä»¥é€ƒç¦»æ­»èƒ¡åŒï¼›ï¼ˆ2ï¼‰æ”¿ç­–æ ¡æ­£æœºåˆ¶ï¼Œå®ƒé‡‡ç”¨é‡è¦æ€§é‡‡æ ·æ¥æ ¡æ­£ç”±æ··åˆé‡‡æ ·å¼•èµ·çš„åˆ†å¸ƒåç§»ï¼Œä»è€Œå‡è½»æ¢¯åº¦ä¼°è®¡åå·®ã€‚æˆ‘ä»¬åœ¨ä¸ƒä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸Šå¯¹å…¶å®éªŒè¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒREX-RAGåœ¨Qwen2.5-3Bä¸Šå¹³å‡æ€§èƒ½æå‡5.1%ï¼Œåœ¨Qwen2.5-7Bä¸Šç›¸å¯¹äºå¼ºå¤§çš„åŸºå‡†æµ‹è¯•å¹³å‡æå‡3.6%ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/MiliLab/REX-RAG">https://github.com/MiliLab/REX-RAG</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08149v1">PDF</a> 17 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆï¼Œå¯å®ç°å¤æ‚æ¨ç†ä»»åŠ¡ã€‚è¿‘æœŸç ”ç©¶æ˜¾ç¤ºï¼Œå°†RLä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç»“åˆï¼Œå¯ä½¿LLMsåŠ¨æ€èå…¥å¤–éƒ¨çŸ¥è¯†ï¼Œå®ç°æ›´æ˜æ™ºå’Œç¨³å¥çš„å†³ç­–ã€‚ç„¶è€Œï¼Œæ”¿ç­–é©±åŠ¨è½¨è¿¹é‡‡æ ·å­˜åœ¨æŒ‘æˆ˜ï¼šLLMsæ˜“é™·å…¥æ— æ•ˆæ¨ç†è·¯å¾„ï¼ˆå³â€œæ­»èƒ¡åŒâ€ï¼‰ï¼Œå¯¼è‡´è¿‡åº¦è‡ªä¿¡çš„é”™è¯¯ç»“è®ºã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºREX-RAGæ¡†æ¶ï¼Œåœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆä¸­è¿›è¡Œæ¨ç†æ¢ç´¢ä¸ç­–ç•¥ä¿®æ­£ã€‚è¯¥æ¡†æ¶å¼•å…¥ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼š1ï¼‰æ··åˆé‡‡æ ·ç­–ç•¥ï¼Œç»“åˆæ–°å‹æ¢é’ˆé‡‡æ ·æ–¹æ³•ä¸æ¢ç´¢æ€§æç¤ºä»¥é€ƒç¦»æ­»èƒ¡åŒï¼›2ï¼‰ç­–ç•¥ä¿®æ­£æœºåˆ¶ï¼Œé‡‡ç”¨é‡è¦æ€§é‡‡æ ·çº æ­£æ··åˆé‡‡æ ·å¼•èµ·çš„åˆ†å¸ƒåç§»ï¼Œå‡è½»æ¢¯åº¦ä¼°è®¡åå·®ã€‚åœ¨ä¸ƒä¸ªé—®ç­”åŸºå‡†æµ‹è¯•é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒREX-RAGåœ¨Qwen2.5-3Bä¸Šå¹³å‡æ€§èƒ½æå‡5.1%ï¼Œåœ¨Qwen2.5-7Bä¸Šæå‡3.6%ï¼Œç›¸è¾ƒäºå¼ºåŸºçº¿å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆå¯å®ç°å¤æ‚æ¨ç†ä»»åŠ¡ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä½¿LLMsèƒ½åŠ¨æ€èå…¥å¤–éƒ¨çŸ¥è¯†ï¼Œå®ç°æ›´æ˜æ™ºå’Œç¨³å¥çš„å†³ç­–ã€‚</li>
<li>æ”¿ç­–é©±åŠ¨è½¨è¿¹é‡‡æ ·å­˜åœ¨æŒ‘æˆ˜ï¼ŒLLMsæ˜“é™·å…¥æ— æ•ˆæ¨ç†è·¯å¾„ï¼ˆå³â€œæ­»èƒ¡åŒâ€ï¼‰ã€‚</li>
<li>REX-RAGæ¡†æ¶é€šè¿‡æ··åˆé‡‡æ ·ç­–ç•¥å’Œç­–ç•¥ä¿®æ­£æœºåˆ¶è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>æ··åˆé‡‡æ ·ç­–ç•¥ç»“åˆæ¢é’ˆé‡‡æ ·å’Œæ¢ç´¢æ€§æç¤ºé€ƒç¦»æ­»èƒ¡åŒã€‚</li>
<li>ç­–ç•¥ä¿®æ­£æœºåˆ¶é‡‡ç”¨é‡è¦æ€§é‡‡æ ·çº æ­£åˆ†å¸ƒåç§»ï¼Œå‡è½»æ¢¯åº¦ä¼°è®¡åå·®ã€‚</li>
<li>REX-RAGåœ¨å¤šä¸ªé—®ç­”åŸºå‡†æµ‹è¯•é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡æ€§èƒ½æœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08149">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cfa98f649e8488f071ed73149da553de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-925c386281592c204daba0d2804ce46d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-549fc36f7a7c6ac274135a62634286c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7f705e40df0d0e2f2145460fd805f54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eda33d128ca0a01c8ffa82a0b9b7edec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-362fcfbb60b1eb85011d6e0922cda71c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Data-Efficient-Biomedical-In-Context-Learning-A-Diversity-Enhanced-Submodular-Perspective"><a href="#Data-Efficient-Biomedical-In-Context-Learning-A-Diversity-Enhanced-Submodular-Perspective" class="headerlink" title="Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced   Submodular Perspective"></a>Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced   Submodular Perspective</h2><p><strong>Authors:Jun Wang, Zaifu Zhan, Qixin Zhang, Mingquan Lin, Meijia Song, Rui Zhang</strong></p>
<p>Recent progress in large language models (LLMs) has leveraged their in-context learning (ICL) abilities to enable quick adaptation to unseen biomedical NLP tasks. By incorporating only a few input-output examples into prompts, LLMs can rapidly perform these new tasks. While the impact of these demonstrations on LLM performance has been extensively studied, most existing approaches prioritize representativeness over diversity when selecting examples from large corpora. To address this gap, we propose Dual-Div, a diversity-enhanced data-efficient framework for demonstration selection in biomedical ICL. Dual-Div employs a two-stage retrieval and ranking process: First, it identifies a limited set of candidate examples from a corpus by optimizing both representativeness and diversity (with optional annotation for unlabeled data). Second, it ranks these candidates against test queries to select the most relevant and non-redundant demonstrations. Evaluated on three biomedical NLP tasks (named entity recognition (NER), relation extraction (RE), and text classification (TC)) using LLaMA 3.1 and Qwen 2.5 for inference, along with three retrievers (BGE-Large, BMRetriever, MedCPT), Dual-Div consistently outperforms baselines-achieving up to 5% higher macro-F1 scores-while demonstrating robustness to prompt permutations and class imbalance. Our findings establish that diversity in initial retrieval is more critical than ranking-stage optimization, and limiting demonstrations to 3-5 examples maximizes performance efficiency. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•åˆ©ç”¨å…¶ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¿«é€Ÿé€‚åº”æœªè§è¿‡çš„ç”Ÿç‰©åŒ»å­¦NLPä»»åŠ¡ã€‚é€šè¿‡å°†åœ¨å°‘æ•°è¾“å…¥-è¾“å‡ºç¤ºä¾‹èå…¥æç¤ºä¸­ï¼ŒLLMå¯ä»¥å¿«é€Ÿæ‰§è¡Œè¿™äº›æ–°ä»»åŠ¡ã€‚è™½ç„¶è¿™äº›æ¼”ç¤ºå¯¹LLMæ€§èƒ½çš„å½±å“å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†ç°æœ‰å¤§å¤šæ•°æ–¹æ³•åœ¨é€‰æ‹©ä¾‹å­æ—¶æ›´ä¾§é‡äºä»£è¡¨æ€§è€Œéå¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Dual-Divï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç”Ÿç‰©åŒ»å­¦ICLæ¼”ç¤ºé€‰æ‹©çš„å¢å¼ºå¤šæ ·æ€§å’Œæ•°æ®é«˜æ•ˆçš„æ¡†æ¶ã€‚Dual-Divé‡‡ç”¨ä¸¤é˜¶æ®µæ£€ç´¢å’Œæ’åè¿‡ç¨‹ï¼šé¦–å…ˆï¼Œå®ƒé€šè¿‡ä¼˜åŒ–ä»£è¡¨æ€§å’Œå¤šæ ·æ€§ï¼ˆæœªæ ‡è®°æ•°æ®å¯è¿›è¡Œå¯é€‰æ³¨é‡Šï¼‰ä»è¯­æ–™åº“ä¸­ç¡®å®šä¸€ç»„æœ‰é™çš„å€™é€‰ç¤ºä¾‹ã€‚å…¶æ¬¡ï¼Œå®ƒæ ¹æ®æµ‹è¯•æŸ¥è¯¢å¯¹è¿™äº›å€™é€‰äººè¿›è¡Œæ’åï¼Œä»¥é€‰æ‹©æœ€ç›¸å…³ä¸”éå†—ä½™çš„æ¼”ç¤ºå†…å®¹ã€‚é€šè¿‡LLaMA 3.1å’ŒQwen 2.5è¿›è¡Œæ¨ç†ï¼Œä»¥åŠåœ¨ä¸‰ä¸ªç”Ÿç‰©åŒ»å­¦NLPä»»åŠ¡ï¼ˆå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€å…³ç³»æå–ï¼ˆREï¼‰å’Œæ–‡æœ¬åˆ†ç±»ï¼ˆTCï¼‰ï¼‰ä¸Šï¼Œä¸ä¸‰ç§æ£€ç´¢å™¨ï¼ˆBGE-Largeã€BMRetrieverã€MedCPTï¼‰ä¸€èµ·è¯„ä¼°ï¼ŒDual-Divå§‹ç»ˆä¼˜äºåŸºçº¿ï¼Œè¾¾åˆ°é«˜è¾¾5%çš„å®è§‚F1åˆ†æ•°ï¼ŒåŒæ—¶æ˜¾ç¤ºå‡ºå¯¹æç¤ºæ’åˆ—å’Œç±»åˆ«ä¸å¹³è¡¡çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆå§‹æ£€ç´¢ä¸­çš„å¤šæ ·æ€§æ¯”æ’åé˜¶æ®µçš„ä¼˜åŒ–æ›´ä¸ºé‡è¦ï¼Œå°†æ¼”ç¤ºé™åˆ¶åœ¨3-5ä¸ªç¤ºä¾‹å†…å¯ä»¥æœ€å¤§åŒ–æ€§èƒ½æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08140v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿç‰©åŒ»å­¦è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼ˆICLï¼‰ã€‚é€šè¿‡ä»…ä½¿ç”¨å°‘é‡è¾“å…¥-è¾“å‡ºç¤ºä¾‹ä½œä¸ºæç¤ºï¼ŒLLMå¯ä»¥å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚å°½ç®¡è¿™äº›æ¼”ç¤ºå¯¹LLMæ€§èƒ½çš„å½±å“å·²è¢«å¹¿æ³›ç ”ç©¶ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨é€‰æ‹©ç¤ºä¾‹æ—¶å¤§å¤šæ³¨é‡ä»£è¡¨æ€§è€Œå¿½è§†å¤šæ ·æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Dual-Divæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªå¢å¼ºå¤šæ ·æ€§çš„æ•°æ®é«˜æ•ˆæ¼”ç¤ºé€‰æ‹©æ–¹æ³•ã€‚Dual-Divé‡‡ç”¨ä¸¤é˜¶æ®µæ£€ç´¢å’Œæ’åè¿‡ç¨‹ï¼Œé¦–å…ˆä¼˜åŒ–ä»£è¡¨æ€§å’Œå¤šæ ·æ€§ä»è¯­æ–™åº“ä¸­è¯†åˆ«ä¸€ç»„æœ‰é™çš„å€™é€‰ç¤ºä¾‹ï¼ˆå¯å¯¹æœªæ ‡è®°æ•°æ®è¿›è¡Œå¯é€‰æ³¨é‡Šï¼‰ã€‚ç„¶åï¼Œå®ƒæ ¹æ®æµ‹è¯•æŸ¥è¯¢å¯¹è¿™äº›å€™é€‰äººè¿›è¡Œæ’åï¼Œä»¥é€‰æ‹©æœ€ç›¸å…³å’Œéå†—ä½™çš„æ¼”ç¤ºã€‚åœ¨ä¸‰ä¸ªç”Ÿç‰©åŒ»å­¦NLPä»»åŠ¡ä¸Šè¯„ä¼°ï¼ŒåŒ…æ‹¬å®ä½“å‘½åè¯†åˆ«ï¼ˆNERï¼‰ã€å…³ç³»æŠ½å–ï¼ˆREï¼‰å’Œæ–‡æœ¬åˆ†ç±»ï¼ˆTCï¼‰ï¼Œä½¿ç”¨LLaMA 3.1å’ŒQwen 2.5è¿›è¡Œæ¨ç†ï¼Œä»¥åŠä¸‰ç§æ£€ç´¢å™¨ï¼ˆBGE-Largeã€BMRetrieverã€MedCPTï¼‰ï¼ŒDual-Divå§‹ç»ˆä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œè¾¾åˆ°é«˜è¾¾5%çš„å®è§‚F1åˆ†æ•°æé«˜ï¼ŒåŒæ—¶æ˜¾ç¤ºå‡ºå¯¹æç¤ºæ’åˆ—å’Œç±»åˆ«ä¸å¹³è¡¡çš„ç¨³å¥æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåˆå§‹æ£€ç´¢é˜¶æ®µçš„å¤šæ ·æ€§æ¯”æ’åé˜¶æ®µä¼˜åŒ–æ›´ä¸ºé‡è¦ï¼Œå°†æ¼”ç¤ºç¤ºä¾‹é™åˆ¶åœ¨3-5ä¸ªèŒƒå›´å†…å¯ä»¥æœ€å¤§åŒ–æ€§èƒ½æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså…·å¤‡é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>åœ¨ç”Ÿç‰©åŒ»å­¦NLPä»»åŠ¡ä¸­ï¼Œæ¼”ç¤ºç¤ºä¾‹çš„é€‰æ‹©æ—¢éœ€è¦ä»£è¡¨æ€§åˆéœ€è¦å¤šæ ·æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨ç¤ºä¾‹é€‰æ‹©æ—¶å¾€å¾€æ›´æ³¨é‡ä»£è¡¨æ€§è€Œå¿½è§†å¤šæ ·æ€§ã€‚</li>
<li>Dual-Divæ¡†æ¶é€šè¿‡ä¸¤é˜¶æ®µæ£€ç´¢å’Œæ’åè¿‡ç¨‹æ¥å¢å¼ºæ¼”ç¤ºç¤ºä¾‹çš„å¤šæ ·æ€§å’Œæ•ˆç‡ã€‚</li>
<li>Dual-Divæ¡†æ¶åœ¨ä¸‰ä¸ªç”Ÿç‰©åŒ»å­¦NLPä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œåˆå§‹æ£€ç´¢é˜¶æ®µçš„å¤šæ ·æ€§å¯¹LLMæ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-96b3d78e291056d3cf5332efa34b6aaf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d8247fac8bde31ea4dfd7b3bb1544cce.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MuaLLM-A-Multimodal-Large-Language-Model-Agent-for-Circuit-Design-Assistance-with-Hybrid-Contextual-Retrieval-Augmented-Generation"><a href="#MuaLLM-A-Multimodal-Large-Language-Model-Agent-for-Circuit-Design-Assistance-with-Hybrid-Contextual-Retrieval-Augmented-Generation" class="headerlink" title="MuaLLM: A Multimodal Large Language Model Agent for Circuit Design   Assistance with Hybrid Contextual Retrieval-Augmented Generation"></a>MuaLLM: A Multimodal Large Language Model Agent for Circuit Design   Assistance with Hybrid Contextual Retrieval-Augmented Generation</h2><p><strong>Authors:Pravallika Abbineni, Saoud Aldowaish, Colin Liechty, Soroosh Noorzad, Ali Ghazizadeh, Morteza Fayazi</strong></p>
<p>Conducting a comprehensive literature review is crucial for advancing circuit design methodologies. However, the rapid influx of state-of-the-art research, inconsistent data representation, and the complexity of optimizing circuit design objectives make this task significantly challenging. In this paper, we propose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for circuit design assistance that integrates a hybrid Retrieval-Augmented Generation (RAG) framework with an adaptive vector database of circuit design research papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason + Act (ReAct) workflow for iterative reasoning, goal-setting, and multi-step information retrieval. It functions as a question-answering design assistant, capable of interpreting complex queries and providing reasoned responses grounded in circuit literature. Its multimodal capabilities enable processing of both textual and visual data, facilitating more efficient and comprehensive analysis. The system dynamically adapts using intelligent search tools, automated document retrieval from the internet, and real-time database updates. Unlike conventional approaches constrained by model context limits, MuaLLM decouples retrieval from inference, enabling scalable reasoning over arbitrarily large corpora. At the maximum context length supported by standard LLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining the same accuracy. This allows rapid, no-human-in-the-loop database generation, overcoming the bottleneck of simulation-based dataset creation for circuits. To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8% accuracy on Reas-100. </p>
<blockquote>
<p>å¯¹ç”µè·¯è®¾è®¡æ–¹æ³•è¿›è¡Œå…¨é¢çš„æ–‡çŒ®ç»¼è¿°å¯¹äºæ¨åŠ¨ç”µè·¯è®¾è®¡æ–¹æ³•çš„è¿›æ­¥è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå°–ç«¯ç ”ç©¶çš„å¿«é€Ÿæ¶Œå…¥ã€æ•°æ®è¡¨ç¤ºçš„ä¸ä¸€è‡´æ€§ä»¥åŠä¼˜åŒ–ç”µè·¯è®¾è®¡ç›®æ ‡çš„å¤æ‚æ€§ä½¿è¿™é¡¹ä»»åŠ¡æå…·æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MuaLLMï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç”µè·¯è®¾è®¡è¾…åŠ©çš„å¼€æºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†ã€‚å®ƒé›†æˆäº†æ··åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶å’Œç”µè·¯è®¾è®¡ç ”ç©¶è®ºæ–‡çš„è‡ªé€‚åº”å‘é‡æ•°æ®åº“ã€‚ä¸ä¼ ç»Ÿçš„LLMä¸åŒï¼ŒMuaLLMä»£ç†é‡‡ç”¨Reason + Actï¼ˆReActï¼‰å·¥ä½œæµç¨‹è¿›è¡Œè¿­ä»£æ¨ç†ã€ç›®æ ‡è®¾å®šå’Œå¤šæ­¥ä¿¡æ¯æ£€ç´¢ã€‚å®ƒä½œä¸ºé—®ç­”è®¾è®¡åŠ©ç†ï¼Œèƒ½å¤Ÿè§£é‡Šå¤æ‚æŸ¥è¯¢å¹¶æä¾›åŸºäºç”µè·¯æ–‡çŒ®çš„åˆç†è§£ç­”ã€‚å®ƒçš„å¤šæ¨¡æ€åŠŸèƒ½èƒ½å¤Ÿå¤„ç†æ–‡æœ¬å’Œè§†è§‰æ•°æ®ï¼Œä¿ƒè¿›æ›´é«˜æ•ˆå’Œå…¨é¢çš„åˆ†æã€‚è¯¥ç³»ç»Ÿä½¿ç”¨æ™ºèƒ½æœç´¢å·¥å…·ã€ä»äº’è”ç½‘è‡ªåŠ¨æ–‡æ¡£æ£€ç´¢å’Œå®æ—¶æ•°æ®åº“æ›´æ–°è¿›è¡ŒåŠ¨æ€é€‚åº”ã€‚ä¸ä¼ ç»Ÿçš„å—æ¨¡å‹ä¸Šä¸‹æ–‡é™åˆ¶çš„æ–¹æ³•ä¸åŒï¼ŒMuaLLMå°†æ£€ç´¢ä¸æ¨ç†è§£è€¦ï¼Œå®ç°å¯¹ä»»æ„å¤§è§„æ¨¡è¯­æ–™åº“çš„å¯æ‰©å±•æ¨ç†ã€‚åœ¨æ ‡å‡†LLMæ”¯æŒçš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ä¸‹ï¼ŒMuaLLMçš„æˆæœ¬é™ä½é«˜è¾¾10å€ï¼Œé€Ÿåº¦æé«˜1.6å€ï¼ŒåŒæ—¶ä¿æŒç›¸åŒçš„å‡†ç¡®æ€§ã€‚è¿™å…è®¸å¿«é€Ÿã€æ— éœ€äººå·¥ä»‹å…¥çš„æ•°æ®åº“ç”Ÿæˆï¼Œçªç ´äº†åŸºäºæ¨¡æ‹Ÿçš„æ•°æ®é›†åˆ›å»ºç“¶é¢ˆï¼Œä¸ºç”µè·¯è®¾è®¡æä¾›ä¾¿åˆ©ã€‚ä¸ºäº†è¯„ä¼°MuaLLMï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªè‡ªå®šä¹‰æ•°æ®é›†ï¼šé’ˆå¯¹æ£€ç´¢å’Œå¼•ç”¨æ€§èƒ½çš„RAG-250ï¼Œä»¥åŠä¸“æ³¨äºç”µè·¯è®¾è®¡å¤šæ­¥æ¨ç†çš„Reasoning-100ï¼ˆReas-100ï¼‰ã€‚MuaLLMåœ¨RAG-250ä¸Šçš„å¬å›ç‡ä¸º90.1%ï¼Œåœ¨Reas-100ä¸Šçš„å‡†ç¡®ç‡ä¸º86.8%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08137v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MuaLLMæ˜¯ä¸€æ¬¾ç”¨äºç”µè·¯è®¾è®¡è¾…åŠ©çš„å¼€æºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†ï¼Œå®ƒç»“åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶å’Œç”µè·¯è®¾è®¡ç ”ç©¶è®ºæ–‡çš„è‡ªé€‚åº”å‘é‡æ•°æ®åº“ã€‚è¯¥ä»£ç†é‡‡ç”¨Reason + Actï¼ˆReActï¼‰å·¥ä½œæµç¨‹ï¼Œå®ç°è¿­ä»£æ¨ç†ã€ç›®æ ‡è®¾å®šå’Œå¤šæ­¥ä¿¡æ¯æ£€ç´¢ï¼Œå¯ä½œä¸ºé—®ç­”è®¾è®¡åŠ©ç†ã€‚å…¶å¤šæ¨¡æ€åŠŸèƒ½å¯å¤„ç†æ–‡æœ¬å’Œè§†è§‰æ•°æ®ï¼Œæé«˜åˆ†æå’Œæ•ˆç‡çš„ç»¼åˆæ€§ã€‚ç³»ç»Ÿå¯æ™ºèƒ½æœç´¢å·¥å…·ã€è‡ªåŠ¨ä»äº’è”ç½‘æ£€ç´¢æ–‡æ¡£å’Œå®æ—¶æ›´æ–°æ•°æ®åº“ã€‚ç›¸è¾ƒäºæ ‡å‡†LLMï¼ŒMuaLLMåœ¨æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦æ”¯æŒä¸‹ï¼Œæˆæœ¬é™ä½10å€ï¼Œé€Ÿåº¦æé«˜1.6å€ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MuaLLMæ˜¯ä¸€ä¸ªç”¨äºç”µè·¯è®¾è®¡è¾…åŠ©çš„å¼€æºå¤šæ¨¡æ€LLMä»£ç†ï¼Œé›†æˆäº†RAGæ¡†æ¶å’Œç”µè·¯è®¾è®¡ç ”ç©¶è®ºæ–‡æ•°æ®åº“ã€‚</li>
<li>é‡‡ç”¨ReActå·¥ä½œæµç¨‹ï¼Œæ”¯æŒè¿­ä»£æ¨ç†ã€ç›®æ ‡è®¾å®šå’Œå¤šæ­¥ä¿¡æ¯æ£€ç´¢ã€‚</li>
<li>ä½œä¸ºé—®ç­”è®¾è®¡åŠ©ç†ï¼Œèƒ½è§£é‡Šå¤æ‚æŸ¥è¯¢å¹¶æä¾›åŸºäºç”µè·¯æ–‡çŒ®çš„åˆç†è§£ç­”ã€‚</li>
<li>å¤šæ¨¡æ€èƒ½åŠ›å¯å¤„ç†æ–‡æœ¬å’Œè§†è§‰æ•°æ®ï¼Œæé«˜åˆ†æå’Œæ•ˆç‡ã€‚</li>
<li>ç³»ç»Ÿå…·æœ‰æ™ºèƒ½æœç´¢å·¥å…·ã€è‡ªåŠ¨äº’è”ç½‘æ–‡æ¡£æ£€ç´¢å’Œå®æ—¶æ•°æ®åº“æ›´æ–°åŠŸèƒ½ã€‚</li>
<li>MuaLLMåœ¨æ ‡å‡†LLMæœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦æ”¯æŒä¸‹ï¼Œæˆæœ¬é™ä½10å€ï¼Œé€Ÿåº¦æé«˜1.6å€ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08137">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db2f43cfe5fb81ad144979452f25803b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86542e5321101efa41b5c18faeae1b89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-571423ca80f3e9830f2a43dc1f83e934.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c177c1b6dfb301c5faa834c3f7437e6a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="BlindGuard-Safeguarding-LLM-based-Multi-Agent-Systems-under-Unknown-Attacks"><a href="#BlindGuard-Safeguarding-LLM-based-Multi-Agent-Systems-under-Unknown-Attacks" class="headerlink" title="BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown   Attacks"></a>BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown   Attacks</h2><p><strong>Authors:Rui Miao, Yixin Liu, Yili Wang, Xu Shen, Yue Tan, Yiwei Dai, Shirui Pan, Xin Wang</strong></p>
<p>The security of LLM-based multi-agent systems (MAS) is critically threatened by propagation vulnerability, where malicious agents can distort collective decision-making through inter-agent message interactions. While existing supervised defense methods demonstrate promising performance, they may be impractical in real-world scenarios due to their heavy reliance on labeled malicious agents to train a supervised malicious detection model. To enable practical and generalizable MAS defenses, in this paper, we propose BlindGuard, an unsupervised defense method that learns without requiring any attack-specific labels or prior knowledge of malicious behaviors. To this end, we establish a hierarchical agent encoder to capture individual, neighborhood, and global interaction patterns of each agent, providing a comprehensive understanding for malicious agent detection. Meanwhile, we design a corruption-guided detector that consists of directional noise injection and contrastive learning, allowing effective detection model training solely on normal agent behaviors. Extensive experiments show that BlindGuard effectively detects diverse attack types (i.e., prompt injection, memory poisoning, and tool attack) across MAS with various communication patterns while maintaining superior generalizability compared to supervised baselines. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/MR9812/BlindGuard">https://github.com/MR9812/BlindGuard</a>. </p>
<blockquote>
<p>åŸºäºLLMçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰çš„å®‰å…¨æ€§å—åˆ°ä¼ æ’­æ¼æ´çš„ä¸¥é‡å¨èƒï¼Œæ¶æ„æ™ºèƒ½ä½“å¯ä»¥é€šè¿‡æ™ºèƒ½ä½“ä¹‹é—´çš„æ¶ˆæ¯äº¤äº’æ‰­æ›²é›†ä½“å†³ç­–ã€‚è™½ç„¶ç°æœ‰çš„ç›‘ç£é˜²å¾¡æ–¹æ³•è¡¨ç°å‡ºæœ‰å¸Œæœ›çš„æ€§èƒ½ï¼Œä½†ç”±äºå®ƒä»¬ä¸¥é‡ä¾èµ–äºå¸¦æœ‰æ ‡ç­¾çš„æ¶æ„æ™ºèƒ½ä½“æ¥è®­ç»ƒç›‘ç£æ¶æ„æ£€æµ‹æ¨¡å‹ï¼Œå› æ­¤åœ¨ç°å®åœºæ™¯ä¸­å¯èƒ½ä¸åˆ‡å®é™…ã€‚ä¸ºäº†å®ç°å¯¹MASçš„å®é™…å’Œé€šç”¨é˜²å¾¡ï¼Œæœ¬æ–‡æå‡ºäº†BlindGuardï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€ä»»ä½•æ”»å‡»ç‰¹å®šæ ‡ç­¾æˆ–æ¶æ„è¡Œä¸ºå…ˆéªŒçŸ¥è¯†çš„æ— ç›‘ç£é˜²å¾¡æ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªåˆ†å±‚æ™ºèƒ½ä½“ç¼–ç å™¨ï¼Œä»¥æ•è·æ¯ä¸ªæ™ºèƒ½ä½“çš„ä¸ªäººã€é‚»è¿‘å’Œå…¨å±€äº¤äº’æ¨¡å¼ï¼Œä¸ºæ¶æ„æ™ºèƒ½ä½“æ£€æµ‹æä¾›å…¨é¢çš„ç†è§£ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè…è´¥å¼•å¯¼æ£€æµ‹å™¨ï¼Œå®ƒç”±æ–¹å‘æ€§å™ªå£°æ³¨å…¥å’Œå¯¹æ¯”å­¦ä¹ ç»„æˆï¼Œå…è®¸ä»…åœ¨æ­£å¸¸æ™ºèƒ½ä½“è¡Œä¸ºä¸Šè¿›è¡Œæœ‰æ•ˆçš„æ£€æµ‹æ¨¡å‹è®­ç»ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBlindGuardåœ¨å…·æœ‰å„ç§é€šä¿¡æ¨¡å¼çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­æœ‰æ•ˆåœ°æ£€æµ‹äº†å„ç§æ”»å‡»ç±»å‹ï¼ˆå³æç¤ºæ³¨å…¥ã€å†…å­˜ä¸­æ¯’å’Œå·¥å…·æ”»å‡»ï¼‰ï¼ŒåŒæ—¶ä¸ç›‘ç£åŸºçº¿ç›¸æ¯”ä¿æŒäº†å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å¯ä»ä»¥ä¸‹ç½‘ç«™è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/MR9812/BlindGuard">https://github.com/MR9812/BlindGuard</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08127v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºåŸºç¡€çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰å®‰å…¨æ€§å—åˆ°ä¼ æ’­æ¼æ´çš„ä¸¥é‡å¨èƒï¼Œæ¶æ„æ™ºèƒ½ä½“å¯ä»¥é€šè¿‡æ™ºèƒ½ä½“ä¹‹é—´çš„æ¶ˆæ¯äº¤äº’æ‰­æ›²é›†ä½“å†³ç­–ã€‚ç°æœ‰ç›‘ç£é˜²å¾¡æ–¹æ³•è™½ç„¶è¡¨ç°è‰¯å¥½ï¼Œä½†ç”±äºä¸¥é‡ä¾èµ–æ ‡è®°æ¶æ„æ™ºèƒ½ä½“æ¥è®­ç»ƒç›‘ç£æ¶æ„æ£€æµ‹æ¨¡å‹ï¼Œå¯èƒ½åœ¨ç°å®åœºæ™¯ä¸­ä¸å¤ªå®ç”¨ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ— éœ€ä»»ä½•æ”»å‡»ç‰¹å®šæ ‡ç­¾æˆ–æ¶æ„è¡Œä¸ºå…ˆéªŒçŸ¥è¯†çš„æ— ç›‘ç£é˜²å¾¡æ–¹æ³•â€”â€”BlindGuardã€‚é€šè¿‡æ„å»ºåˆ†å±‚æ™ºèƒ½ä½“ç¼–ç å™¨æ¥æ•æ‰å•ä¸ªã€é‚»è¿‘å’Œå…¨å±€çš„æ™ºèƒ½ä½“äº¤äº’æ¨¡å¼ï¼Œä¸ºæ¶æ„æ™ºèƒ½ä½“æ£€æµ‹æä¾›å…¨é¢ç†è§£ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ä¸€ç§è…è´¥å¼•å¯¼æ£€æµ‹å™¨ï¼ŒåŒ…æ‹¬å®šå‘å™ªå£°æ³¨å…¥å’Œå¯¹æ¯”å­¦ä¹ ï¼Œä»…é€šè¿‡æ­£å¸¸æ™ºèƒ½ä½“è¡Œä¸ºå³å¯è¿›è¡Œæœ‰æ•ˆæ£€æµ‹æ¨¡å‹è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒBlindGuardèƒ½æœ‰æ•ˆæ£€æµ‹å¤šç§æ”»å‡»ç±»å‹ï¼Œå¦‚æç¤ºæ³¨å…¥ã€å†…å­˜ä¸­æ¯’å’Œå·¥å…·æ”»å‡»ç­‰ï¼Œåœ¨å„ç§é€šä¿¡æ¨¡å¼çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ä¿æŒå‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based MASé¢ä¸´ä¼ æ’­æ¼æ´å¨èƒï¼Œæ¶æ„æ™ºèƒ½ä½“èƒ½å¹²æ‰°é›†ä½“å†³ç­–ã€‚</li>
<li>ç°æœ‰ç›‘ç£é˜²å¾¡æ–¹æ³•å› ä¾èµ–æ ‡è®°æ•°æ®è€Œåœ¨ç°å®åœºæ™¯ä¸­å¯èƒ½ä¸å®ç”¨ã€‚</li>
<li>BlindGuardæ˜¯ä¸€ç§æ— ç›‘ç£é˜²å¾¡æ–¹æ³•ï¼Œæ— éœ€æ”»å‡»ç‰¹å®šæ ‡ç­¾æˆ–æ¶æ„è¡Œä¸ºå…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>BlindGuardé€šè¿‡åˆ†å±‚æ™ºèƒ½ä½“ç¼–ç å™¨æ•æ‰æ™ºèƒ½ä½“äº¤äº’æ¨¡å¼ï¼ŒåŠ©åŠ›äºæ¶æ„æ™ºèƒ½ä½“æ£€æµ‹ã€‚</li>
<li>BlindGuardè®¾è®¡äº†ä¸€ç§è…è´¥å¼•å¯¼æ£€æµ‹å™¨ï¼ŒåŒ…æ‹¬å®šå‘å™ªå£°æ³¨å…¥å’Œå¯¹æ¯”å­¦ä¹ ã€‚</li>
<li>BlindGuardèƒ½æœ‰æ•ˆæ£€æµ‹å¤šç§æ”»å‡»ç±»å‹ï¼Œåœ¨å„ç§é€šä¿¡æ¨¡å¼çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08127">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-699b1b8fd8c0d2c695ba81073a90dd11.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-db78fbfa4503912c803c0c7eb27bd571.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c85e81424722608b9347b548016824d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-950b672b69dc99eb45f5f189eb9ab1cc.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Grounding-Multilingual-Multimodal-LLMs-With-Cultural-Knowledge"><a href="#Grounding-Multilingual-Multimodal-LLMs-With-Cultural-Knowledge" class="headerlink" title="Grounding Multilingual Multimodal LLMs With Cultural Knowledge"></a>Grounding Multilingual Multimodal LLMs With Cultural Knowledge</h2><p><strong>Authors:Jean de Dieu Nyandwi, Yueqi Song, Simran Khanuja, Graham Neubig</strong></p>
<p>Multimodal Large Language Models excel in high-resource settings, but often misinterpret long-tail cultural entities and underperform in low-resource languages. To address this gap, we propose a data-centric approach that directly grounds MLLMs in cultural knowledge. Leveraging a large scale knowledge graph from Wikidata, we collect images that represent culturally significant entities, and generate synthetic multilingual visual question answering data. The resulting dataset, CulturalGround, comprises 22 million high-quality, culturally-rich VQA pairs spanning 42 countries and 39 languages. We train an open-source MLLM CulturalPangea on CulturalGround, interleaving standard multilingual instruction-tuning data to preserve general abilities. CulturalPangea achieves state-of-the-art performance among open models on various culture-focused multilingual multimodal benchmarks, outperforming prior models by an average of 5.0 without degrading results on mainstream vision-language tasks. Our findings show that our targeted, culturally grounded approach could substantially narrow the cultural gap in MLLMs and offer a practical path towards globally inclusive multimodal systems. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é«˜èµ„æºç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç»å¸¸è¯¯è§£é•¿å°¾æ–‡åŒ–å®ä½“å¹¶åœ¨ä½èµ„æºè¯­è¨€ä¸­è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æ–¹æ³•ï¼Œç›´æ¥è®©å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æ ¹æ¤äºæ–‡åŒ–çŸ¥è¯†ä¸­ã€‚æˆ‘ä»¬åˆ©ç”¨ç»´åŸºç™¾ç§‘çš„å¤§è§„æ¨¡çŸ¥è¯†å›¾è°±æ”¶é›†ä»£è¡¨æ–‡åŒ–ä¸Šé‡è¦å®ä½“çš„å›¾åƒï¼Œå¹¶ç”Ÿæˆåˆæˆå¤šè¯­è¨€è§†è§‰é—®ç­”æ•°æ®ã€‚æ‰€å¾—åˆ°çš„CulturalGroundæ•°æ®é›†åŒ…å«æ¶µç›–42ä¸ªå›½å®¶å’Œ39ç§è¯­è¨€çš„22ç™¾ä¸‡é«˜è´¨é‡ã€æ–‡åŒ–ä¸°å¯Œçš„é—®ç­”å¯¹ã€‚æˆ‘ä»¬åœ¨CulturalGroundä¸Šè®­ç»ƒå¼€æºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹CulturalPangeaï¼ŒåŒæ—¶èå…¥æ ‡å‡†çš„å¤šè¯­è¨€æŒ‡ä»¤å¾®è°ƒæ•°æ®ä»¥ä¿ç•™å…¶ä¸€èˆ¬èƒ½åŠ›ã€‚CulturalPangeaåœ¨å„ç§ä»¥æ–‡åŒ–ä¸ºé‡ç‚¹çš„å¤šè¯­è¨€å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œç›¸è¾ƒäºå…ˆå‰çš„æ¨¡å‹å¹³å‡æé«˜äº†5%ï¼Œå¹¶ä¸”åœ¨ä¸»æµè§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šçš„ç»“æœå¹¶æœªé™ä½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æœ‰é’ˆå¯¹æ€§çš„æ–‡åŒ–å®šä½æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—ç¼©å°å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä¸­çš„æ–‡åŒ–å·®è·ï¼Œå¹¶ä¸ºå®ç°å…¨çƒåŒ…å®¹æ€§å¤šæ¨¡æ€ç³»ç»Ÿæä¾›äº†å®é™…é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07414v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šåª’ä½“å¤§è¯­è¨€æ¨¡å‹åœ¨é«˜èµ„æºåœºæ™¯ä¸‹è¡¨ç°å“è¶Šï¼Œä½†åœ¨é•¿å°¾æ–‡åŒ–å®ä½“æ–¹é¢å­˜åœ¨è¯¯è§£ï¼Œå¹¶ä¸”åœ¨ä½èµ„æºè¯­è¨€æ–¹é¢è¡¨ç°ä¸ä½³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æ–¹æ³•ï¼Œç›´æ¥ä»¥æ–‡åŒ–ä¸ºåŸºç¡€æ„å»ºå¤šåª’ä½“å¤§è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡åˆ©ç”¨ç»´åŸºç™¾ç§‘çš„å¤§è§„æ¨¡çŸ¥è¯†å›¾è°±ï¼Œç ”ç©¶å›¢é˜Ÿæ”¶é›†å…·æœ‰ä»£è¡¨æ€§çš„æ–‡åŒ–å®ä½“å›¾åƒå¹¶ç”Ÿæˆåˆæˆå‹å¤šè¯­ç§è§†è§‰é—®ç­”æ•°æ®ã€‚æ„å»ºçš„CulturalGroundæ•°æ®é›†åŒ…å«äº†æ¶µç›–å¤šä¸ªå›½å®¶ä¸æ–‡åŒ–è¯­è¨€çš„ä¼˜è´¨é—®ç­”å¯¹æ•°æ®ã€‚åŸºäºè¯¥æ•°æ®é›†è®­ç»ƒå¼€æºçš„å¤šåª’ä½“å¤§è¯­è¨€æ¨¡å‹CulturalPangeaï¼Œæ—¢æå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¹Ÿå¢å¼ºäº†å…¶å¯¹ç‰¹å®šæ–‡åŒ–èƒŒæ™¯çš„æƒ…å¢ƒè§£è¯»èƒ½åŠ›ã€‚æµ‹è¯•è¡¨æ˜CulturalPangeaåœ¨å¤šæ–‡åŒ–èƒŒæ™¯çš„è·¨è¯­è¨€ç¯å¢ƒä¸‹å–å¾—äº†ä¸šç•Œé¢†å…ˆæ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå¤§å¹…ç¼©å°å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ–‡åŒ–å·®è·ï¼Œä¸ºæ„å»ºå…¨çƒæ€§åŒ…å®¹çš„å¤šæ¨¡æ€ç³»ç»Ÿæä¾›äº†å®é™…è·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤šåª’ä½“å¤§è¯­è¨€æ¨¡å‹åœ¨é«˜èµ„æºç¯å¢ƒä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œä½†åœ¨é•¿å°¾æ–‡åŒ–å®ä½“åŠä½èµ„æºè¯­è¨€ä¸­å­˜åœ¨é—®é¢˜ã€‚</li>
<li>é’ˆå¯¹è¿™ä¸€ä¸è¶³ï¼Œé‡‡ç”¨ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„ç­–ç•¥ï¼Œç»“åˆç»´åŸºç™¾ç§‘çŸ¥è¯†å›¾è°±ï¼Œæ„å»ºæ–‡åŒ–åŸºç¡€æ¨¡å‹ã€‚</li>
<li>é€šè¿‡æ”¶é›†æ–‡åŒ–å®ä½“å›¾åƒå’Œç”Ÿæˆåˆæˆå¤šè¯­ç§è§†è§‰é—®ç­”æ•°æ®ï¼Œåˆ›å»ºäº†CulturalGroundæ•°æ®é›†ã€‚</li>
<li>CulturalGroundæ•°æ®é›†åŒ…å«è·¨è¶Šå¤šä¸ªå›½å®¶ä¸è¯­è¨€çš„ä¼˜è´¨é—®ç­”å¯¹æ•°æ®ã€‚</li>
<li>åŸºäºCulturalGroundæ•°æ®é›†è®­ç»ƒçš„CulturalPangeaæ¨¡å‹åœ¨å¤šç§æ–‡åŒ–èƒŒæ™¯ä¸‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>CulturalPangeaæ¨¡å‹é€šè¿‡ç»“åˆæ ‡å‡†çš„å¤šè¯­ç§æŒ‡ä»¤å¾®è°ƒæ•°æ®ï¼Œæ—¢ä¿ç•™äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›åˆæå‡äº†ç‰¹å®šæ–‡åŒ–èƒŒæ™¯æƒ…å¢ƒè§£è¯»èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07414">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ddc7dbe3a24a8c255095661bd1009fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2f526f54f9eff944fd028d31717a3c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bef8b3cfc62d3e27b46cc426138d74d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da0c55689a98f2eb73691fb612b3135d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8af23f8485987357fc17fe0703914799.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MCITlib-Multimodal-Continual-Instruction-Tuning-Library-and-Benchmark"><a href="#MCITlib-Multimodal-Continual-Instruction-Tuning-Library-and-Benchmark" class="headerlink" title="MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark"></a>MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark</h2><p><strong>Authors:Haiyang Guo, Fei Zhu, Hongbo Zhao, Fanhu Zeng, Wenzhuo Liu, Shijie Ma, Da-Han Wang, Xu-Yao Zhang</strong></p>
<p>Continual learning aims to equip AI systems with the ability to continuously acquire and adapt to new knowledge without forgetting previously learned information, similar to human learning. While traditional continual learning methods focusing on unimodal tasks have achieved notable success, the emergence of Multimodal Large Language Models has brought increasing attention to Multimodal Continual Learning tasks involving multiple modalities, such as vision and language. In this setting, models are expected to not only mitigate catastrophic forgetting but also handle the challenges posed by cross-modal interactions and coordination. To facilitate research in this direction, we introduce MCITlib, a comprehensive and constantly evolving code library for continual instruction tuning of Multimodal Large Language Models. In MCITlib, we have currently implemented 8 representative algorithms for Multimodal Continual Instruction Tuning and systematically evaluated them on 2 carefully selected benchmarks. MCITlib will be continuously updated to reflect advances in the Multimodal Continual Learning field. The codebase is released at <a target="_blank" rel="noopener" href="https://github.com/Ghy0501/MCITlib">https://github.com/Ghy0501/MCITlib</a>. </p>
<blockquote>
<p>æŒç»­å­¦ä¹ æ—¨åœ¨ä½¿AIç³»ç»Ÿå…·å¤‡æŒç»­è·å–å¹¶é€‚åº”æ–°çŸ¥è¯†çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¸é—å¿˜å…ˆå‰å­¦ä¹ çš„ä¿¡æ¯ï¼Œç±»ä¼¼äºäººç±»å­¦ä¹ ã€‚è™½ç„¶ä¸“æ³¨äºå•æ¨¡æ€ä»»åŠ¡çš„ä¼ ç»ŸæŒç»­å­¦ä¹ æ–¹æ³•å·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆæœï¼Œä½†éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å…´èµ·ï¼Œè¶Šæ¥è¶Šå¤šçš„å…³æ³¨è¢«æŠ•å‘æ¶‰åŠå¤šä¸ªæ¨¡æ€ï¼ˆå¦‚è§†è§‰å’Œè¯­è¨€ï¼‰çš„å¤šæ¨¡æ€æŒç»­å­¦ä¹ ä»»åŠ¡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä¸ä»…é¢„æœŸèƒ½å¤Ÿå‡è½»ç¾éš¾æ€§é—å¿˜ï¼Œè¿˜é¢„æœŸèƒ½å¤Ÿåº”å¯¹è·¨æ¨¡æ€äº¤äº’å’Œåè°ƒæ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ä¸ºäº†ä¿ƒè¿›è¿™ä¸€æ–¹å‘çš„ç ”ç©¶ï¼Œæˆ‘ä»¬ä»‹ç»äº†MCITlibï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒç»­æŒ‡ä»¤è°ƒæ•´çš„ç»¼åˆæ€§ä¸”ä¸æ–­å‘å±•çš„ä»£ç åº“ã€‚åœ¨MCITlibä¸­ï¼Œæˆ‘ä»¬ç›®å‰å·²ç»å®ç°äº†8ç§å…·æœ‰ä»£è¡¨æ€§çš„å¤šæ¨¡æ€æŒç»­æŒ‡ä»¤è°ƒæ•´ç®—æ³•ï¼Œå¹¶åœ¨ç²¾å¿ƒé€‰æ‹©çš„2ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚MCITlibå°†æŒç»­æ›´æ–°ä»¥åæ˜ å¤šæ¨¡æ€æŒç»­å­¦ä¹ é¢†åŸŸçš„è¿›å±•ã€‚ä»£ç åº“å‘å¸ƒäº<a target="_blank" rel="noopener" href="https://github.com/Ghy0501/MCITlib%E3%80%82">https://github.com/Ghy0501/MCITlibã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07307v1">PDF</a> Preprint</p>
<p><strong>æ‘˜è¦</strong></p>
<p>äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æŒç»­å­¦ä¹ æ—¨åœ¨ä½¿å…¶å…·å¤‡è¿ç»­è·å–å’Œé€‚åº”æ–°çŸ¥è¯†çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¸å¿˜å·²å­¦åˆ°çš„ä¿¡æ¯ï¼Œä¸äººç±»å­¦ä¹ ç›¸ä¼¼ã€‚éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å…´èµ·ï¼Œæ¶‰åŠå¤šæ¨¡æ€ï¼ˆå¦‚è§†è§‰å’Œè¯­è¨€ï¼‰çš„å¤šæ¨¡æ€æŒç»­å­¦ä¹ ä»»åŠ¡å¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä¸ä»…æœŸæœ›å‡è½»ç¾éš¾æ€§é—å¿˜ï¼Œè¿˜è¦åº”å¯¹è·¨æ¨¡æ€äº¤äº’å’Œåè°ƒå¸¦æ¥çš„æŒ‘æˆ˜ã€‚ä¸ºäº†ä¿ƒè¿›è¿™ä¸€æ–¹å‘çš„ç ”ç©¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MCITlibä»£ç åº“ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸æ–­è¿›åŒ–çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒç»­æŒ‡ä»¤è°ƒæ•´åº“ã€‚åœ¨MCITlibä¸­ï¼Œæˆ‘ä»¬ç›®å‰å·²å®ç°äº†å…«ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„å¤šæ¨¡æ€æŒç»­æŒ‡ä»¤è°ƒæ•´ç®—æ³•ï¼Œå¹¶åœ¨ä¸¤ä¸ªç²¾å¿ƒé€‰æ‹©çš„åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚MCITlibå°†æŒç»­æ›´æ–°ä»¥åæ˜ å¤šæ¨¡æ€æŒç»­å­¦ä¹ é¢†åŸŸçš„è¿›å±•ã€‚è¯¥ä»£ç åº“å‘å¸ƒåœ¨GitHubä¸Šï¼š<a target="_blank" rel="noopener" href="https://github.com/Ghy0501/MCITlib%E3%80%82">https://github.com/Ghy0501/MCITlibã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æŒç»­å­¦ä¹ çš„ç›®æ ‡æ˜¯ä½¿AIç³»ç»Ÿå…·å¤‡åƒäººç±»ä¸€æ ·çš„è¿ç»­è·å–å’Œé€‚åº”æ–°çŸ¥è¯†çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¸å¿˜æ‰å·²å­¦ä¿¡æ¯ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å…´èµ·ä½¿å¾—å¤šæ¨¡æ€æŒç»­å­¦ä¹ ä»»åŠ¡å—åˆ°å…³æ³¨ã€‚</li>
<li>å¤šæ¨¡æ€æŒç»­å­¦ä¹ é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å‡è½»ç¾éš¾æ€§é—å¿˜å’Œåº”å¯¹è·¨æ¨¡æ€äº¤äº’ä¸åè°ƒçš„é—®é¢˜ã€‚</li>
<li>MCITlibæ˜¯ä¸€ä¸ªå…¨é¢çš„ã€ä¸æ–­è¿›åŒ–çš„ä»£ç åº“ï¼Œç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒç»­æŒ‡ä»¤è°ƒæ•´ã€‚</li>
<li>MCITlibç›®å‰å®ç°äº†å…«ä¸ªä»£è¡¨æ€§ç®—æ³•ï¼Œå¹¶åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚</li>
<li>MCITlibå°†æŒç»­æ›´æ–°ä»¥åæ˜ å¤šæ¨¡æ€æŒç»­å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è¿›å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07307">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2f9aa2e7c847a7ad595db92ded85c838.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e76d6868f4df2b32bb05177de601952a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e186b4ec572c4f3f606ba4aa4f72603.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37ce45cc36687b65a0f9c70098665930.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc9522276118b56310e850e3dd253edb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c10fea043fb88e9f9da95692dc77fdde.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Two-Stage-Quranic-QA-via-Ensemble-Retrieval-and-Instruction-Tuned-Answer-Extraction"><a href="#Two-Stage-Quranic-QA-via-Ensemble-Retrieval-and-Instruction-Tuned-Answer-Extraction" class="headerlink" title="Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer   Extraction"></a>Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer   Extraction</h2><p><strong>Authors:Mohamed Basem, Islam Oshallah, Ali Hamdi, Khaled Shaban, Hozaifa Kassab</strong></p>
<p>Quranic Question Answering presents unique challenges due to the linguistic complexity of Classical Arabic and the semantic richness of religious texts. In this paper, we propose a novel two-stage framework that addresses both passage retrieval and answer extraction. For passage retrieval, we ensemble fine-tuned Arabic language models to achieve superior ranking performance. For answer extraction, we employ instruction-tuned large language models with few-shot prompting to overcome the limitations of fine-tuning on small datasets. Our approach achieves state-of-the-art results on the Quran QA 2023 Shared Task, with a MAP@10 of 0.3128 and MRR@10 of 0.5763 for retrieval, and a pAP@10 of 0.669 for extraction, substantially outperforming previous methods. These results demonstrate that combining model ensembling and instruction-tuned language models effectively addresses the challenges of low-resource question answering in specialized domains. </p>
<blockquote>
<p>ç”±äºå¤å…¸é˜¿æ‹‰ä¼¯è¯­çš„è¯­è¨€å¤æ‚æ€§å’Œå®—æ•™æ–‡æœ¬ä¸°å¯Œçš„è¯­ä¹‰å†…æ¶µï¼Œä¼Šæ–¯å…°é—®ç­”å‘ˆç°å‡ºç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œè¯¥æ¡†æ¶è§£å†³äº†æ®µè½æ£€ç´¢å’Œç­”æ¡ˆæå–çš„é—®é¢˜ã€‚å¯¹äºæ®µè½æ£€ç´¢ï¼Œæˆ‘ä»¬ç»„åˆäº†ç»è¿‡ç²¾ç»†è°ƒæ•´çš„é˜¿æ‹‰ä¼¯è¯­è¯­è¨€æ¨¡å‹ï¼Œä»¥å®ç°å‡ºè‰²çš„æ’åæ€§èƒ½ã€‚å¯¹äºç­”æ¡ˆæå–ï¼Œæˆ‘ä»¬é‡‡ç”¨æŒ‡ä»¤è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå°æ ·æœ¬æç¤ºï¼Œä»¥å…‹æœåœ¨å°æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒæ—¶çš„å±€é™æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¼Šæ–¯å…°é—®ç­”2023å…±äº«ä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æˆæœï¼Œæ£€ç´¢çš„MAP@10ä¸º0.3128ï¼ŒMRR@10ä¸º0.5763ï¼Œæå–çš„pAP@10ä¸º0.669ï¼Œå¤§å¹…ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç»“åˆæ¨¡å‹ç»„åˆå’ŒæŒ‡ä»¤è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œèƒ½æœ‰æ•ˆè§£å†³ç‰¹å®šé¢†åŸŸä½èµ„æºé—®ç­”çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06971v1">PDF</a> 8 pages , 4 figures , Accepted in Aiccsa 2025 ,   <a target="_blank" rel="noopener" href="https://conferences.sigappfr.org/aiccsa2025/">https://conferences.sigappfr.org/aiccsa2025/</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºè§£å†³å¤å…°ç»é—®ç­”ä¸­çš„æŒ‘æˆ˜ã€‚é€šè¿‡é‡‡ç”¨ç²¾ç»†åŒ–è°ƒæ•´çš„é˜¿æ‹‰ä¼¯è¯­è¯­è¨€æ¨¡å‹é›†åˆå’ŒæŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ç°é«˜æ•ˆçš„ç¯‡ç« æ£€ç´¢å’Œç­”æ¡ˆæå–ã€‚è¯¥æ¡†æ¶åœ¨å¤å…°ç»é—®ç­”ä»»åŠ¡ä¸­å–å¾—å“è¶Šæ€§èƒ½ï¼Œè·å¾—é¢†å…ˆæ°´å¹³çš„ç»“æœï¼Œä¸ºåç»­å·¥ä½œæä¾›äº†å¯ç¤ºå’Œæ–¹å‘ã€‚è¯¥æ–¹æ³•å¯æœ‰æ•ˆè§£å†³ä¸“ä¸šé¢†åŸŸä¸­èµ„æºç¨€ç¼ºé—®ç­”æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶çš„MAPå’ŒMRRç­‰æŒ‡æ ‡åœ¨å¤å…°ç»é—®ç­”å…±äº«ä»»åŠ¡ä¸­æœ‰è¾ƒå¥½çš„è¡¨ç°ã€‚æœ¬æ–‡ç ”ç©¶çš„å…³é”®æ˜¯æå‡ºä½¿ç”¨ç²¾ç»†åŒ–è¯­è¨€æ¨¡å‹é›†åˆå’ŒæŒ‡ä»¤åŒ–è¯­è¨€æ¨¡å‹ç»“åˆçš„æ€è·¯ã€‚æ­¤å¤–ï¼Œè¿™ç§æ–¹æ³•çš„åº”ç”¨èƒ½å¤Ÿæ”¹è¿›å’Œæå‡ç‰¹æ®Šé¢†åŸŸï¼ˆå¦‚å®—æ•™é¢†åŸŸï¼‰çš„è¯­è¨€ç†è§£å’Œæ™ºèƒ½é—®ç­”èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯æœ¬æ–‡çš„å·¥ä½œæä¾›äº†ä¸€ç§å®ç”¨çš„æŠ€æœ¯æ‰‹æ®µè§£å†³é˜¿æ‹‰ä¼¯æ–‡çš„å¤æ‚æ€§åŠå…¶åœ¨å®—æ•™é¢†åŸŸçš„è¯­è¨€å¤šæ ·æ€§éš¾é¢˜ï¼Œå¹¶å°†æä¾›å¯Œæœ‰ä»·å€¼çš„ç ”ç©¶æˆæœåŠæ›´æ·±å…¥çš„æŠ€æœ¯çªç ´é€”å¾„åº”ç”¨äºçœŸå®åœºæ™¯çš„ç­”æ¡ˆå¯»æ‰¾å’Œä¿¡æ¯æå–è¿‡ç¨‹ä¹‹ä¸­å±•ç¤ºæ–¹æ³•çš„ç‹¬ç‰¹ä»·å€¼åº”ç”¨æ½œåŠ›å¯¹å®é™…åº”ç”¨åœºæ™¯æä¾›æœ‰æ•ˆæ”¯æŒã€‚å¯¹äºä½èµ„æºé¢†åŸŸçš„é—®ç­”ç³»ç»Ÿç ”ç©¶å…·æœ‰é‡è¦çš„æ¨åŠ¨ä½œç”¨ã€‚æ€»ä½“æ¥è¯´ï¼Œæœ¬æ–‡çš„åˆ›æ–°åœ¨äºé€šè¿‡æ¨¡å‹é›†æˆå’ŒæŒ‡ä»¤åŒ–è¯­è¨€æ¨¡å‹çš„ä½¿ç”¨è§£å†³äº†ä½èµ„æºé¢†åŸŸçš„é—®ç­”æŒ‘æˆ˜ã€‚å…¶ç ”ç©¶å…·æœ‰é‡è¦çš„ç†è®ºå’Œå®è·µæ„ä¹‰ã€‚åœ¨æœªæ¥çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢å¦‚ä½•åˆ©ç”¨æ›´å¤šçš„æ— ç›‘ç£å­¦ä¹ æŠ€æœ¯æ¥æå‡æ¨¡å‹çš„æ€§èƒ½ï¼Œä»¥åŠå¦‚ä½•åœ¨æ›´å¤§çš„æ•°æ®é›†ä¸Šæµ‹è¯•æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥å°è¯•å°†è¿™ç§æ–¹æ³•åº”ç”¨äºå…¶ä»–ä½èµ„æºé¢†åŸŸçš„é—®ç­”ä»»åŠ¡ä¸­ï¼Œä»¥éªŒè¯å…¶é€šç”¨æ€§å’Œé€‚ç”¨æ€§ã€‚æ€»ä½“æ¥è¯´ï¼Œæœ¬æ–‡æ‰€æå‡ºçš„æ¨¡å‹ä¸ä»…æ¨åŠ¨äº†ç‰¹æ®Šé¢†åŸŸè¯­è¨€ç†è§£çš„è¿›æ­¥ä¹Ÿä¸ºç›¸å…³æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•è¿™äº›å®éªŒæ•°æ®éƒ½åæ˜ äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§è¿›ä¸€æ­¥æé«˜äº†ç®—æ³•çš„ç²¾ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›å¯ä»¥é¢„è§æœªæ¥è¿™ä¸€é¢†åŸŸå°†ä¼šæœ‰æ›´å¤šçš„ç ”ç©¶æ¶Œç°å¹¶æ¨åŠ¨ç›¸å…³æŠ€æœ¯çš„è¿›æ­¥ã€‚ </p>
<p><strong>Key Takeaways</strong>: </p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„ä¸¤é˜¶æ®µæ¡†æ¶æ¥è§£å†³å¤å…°ç»é—®ç­”ä¸­çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç¯‡ç« æ£€ç´¢å’Œç­”æ¡ˆæå–ä¸¤ä¸ªå…³é”®æ­¥éª¤ã€‚ </li>
<li>é‡‡ç”¨ç²¾ç»†åŒ–è°ƒæ•´çš„é˜¿æ‹‰ä¼¯è¯­è¯­è¨€æ¨¡å‹é›†åˆå®ç°é«˜æ•ˆçš„ç¯‡ç« æ£€ç´¢ã€‚ </li>
<li>é‡‡ç”¨æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç­”æ¡ˆæå–ï¼Œå…‹æœäº†åœ¨å°å‹æ•°æ®é›†ä¸Šç²¾ç»†è°ƒæ•´æ¨¡å‹çš„å±€é™æ€§ã€‚ </li>
<li>åœ¨å¤å…°ç»é—®ç­”ä»»åŠ¡ä¸­å–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼Œè·å¾—é¢†å…ˆæ°´å¹³çš„ç»“æœï¼Œè¡¨ç°å‡ºæ¨¡å‹ç»“åˆçš„æœ‰æ•ˆæ€§ã€‚ </li>
<li>è¯¥æ–¹æ³•åœ¨å¤å…°ç»é—®ç­”å…±äº«ä»»åŠ¡ä¸­çš„MAP@10å’ŒMRR@10ç­‰æŒ‡æ ‡è¡¨ç°è‰¯å¥½ï¼Œè¯æ˜å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚ </li>
<li>æœ¬æ–‡å±•ç¤ºäº†é€šè¿‡æ¨¡å‹é›†æˆå’ŒæŒ‡ä»¤åŒ–è¯­è¨€æ¨¡å‹ç»“åˆçš„æ–¹å¼è§£å†³äº†ä½èµ„æºé¢†åŸŸçš„é—®ç­”æŒ‘æˆ˜çš„æ–¹æ³•åœ¨å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œéƒ½å…·æœ‰é‡è¦çš„å€Ÿé‰´æ„ä¹‰æ¨åŠ¨äº†ç›¸å…³é¢†åŸŸçš„ç ”ç©¶å‘å±•å¹¶ä¸ºæœªæ¥å·¥ä½œæä¾›äº†æ–¹å‘æ€§çš„å¯ç¤ºåŒæ—¶ä¹Ÿä¸ºè¯¥æ¡†æ¶åœ¨å®—æ•™é¢†åŸŸæˆ–å…¶ä»–ç±»ä¼¼å¤æ‚æ–‡æœ¬å¤„ç†ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†å¯èƒ½æ€§å’Œå¹¿é˜”å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06971">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d5057cb8669579c4091002b84f57df23.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f174a703aa606e5fb90aa5d12d7e0e90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-694babdddc054092041f7205cf54e789.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebcf539e975d67e61820fab04c526ff7.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MathSmith-Towards-Extremely-Hard-Mathematical-Reasoning-by-Forging-Synthetic-Problems-with-a-Reinforced-Policy"><a href="#MathSmith-Towards-Extremely-Hard-Mathematical-Reasoning-by-Forging-Synthetic-Problems-with-a-Reinforced-Policy" class="headerlink" title="MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging   Synthetic Problems with a Reinforced Policy"></a>MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging   Synthetic Problems with a Reinforced Policy</h2><p><strong>Authors:Shaoxiong Zhan, Yanlin Lai, Ziyu Lu, Dahua Lin, Ziqing Yang, Fei Tan</strong></p>
<p>Large language models have achieved substantial progress in mathematical reasoning, yet their advancement is limited by the scarcity of high-quality, high-difficulty training data. Existing synthesis methods largely rely on transforming human-written templates, limiting both diversity and scalability. We propose MathSmith, a novel framework for synthesizing challenging mathematical problems to enhance LLM reasoning. Rather than modifying existing problems, MathSmith constructs new ones from scratch by randomly sampling concept-explanation pairs from PlanetMath, ensuring data independence and avoiding contamination. To increase difficulty, we design nine predefined strategies as soft constraints during rationales. We further adopts reinforcement learning to jointly optimize structural validity, reasoning complexity, and answer consistency. The length of the reasoning trace generated under autoregressive prompting is used to reflect cognitive complexity, encouraging the creation of more demanding problems aligned with long-chain-of-thought reasoning. Experiments across five benchmarks, categorized as easy &amp; medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025, OlympiadBench), show that MathSmith consistently outperforms existing baselines under both short and long CoT settings. Additionally, a weakness-focused variant generation module enables targeted improvement on specific concepts. Overall, MathSmith exhibits strong scalability, generalization, and transferability, highlighting the promise of high-difficulty synthetic data in advancing LLM reasoning capabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶è¿›å±•å—é™äºé«˜è´¨é‡ã€é«˜éš¾åº¦è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºæ€§ã€‚ç°æœ‰çš„åˆæˆæ–¹æ³•å¤§å¤šä¾èµ–äºè½¬æ¢äººå·¥ç¼–å†™çš„æ¨¡æ¿ï¼Œè¿™é™åˆ¶äº†å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬æå‡ºäº†MathSmithï¼Œè¿™æ˜¯ä¸€ä¸ªåˆæˆå…·æœ‰æŒ‘æˆ˜æ€§æ•°å­¦é—®é¢˜ä»¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°å‹æ¡†æ¶ã€‚MathSmithä¸æ˜¯ä¿®æ”¹ç°æœ‰é—®é¢˜ï¼Œè€Œæ˜¯ä»é›¶å¼€å§‹æ„å»ºæ–°é—®é¢˜ï¼Œé€šè¿‡ä»PlanetMathéšæœºæŠ½å–æ¦‚å¿µè§£é‡Šå¯¹ï¼Œç¡®ä¿æ•°æ®ç‹¬ç«‹ï¼Œé¿å…æ±¡æŸ“ã€‚ä¸ºäº†æé«˜éš¾åº¦ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¹ç§é¢„è®¾ç­–ç•¥ä½œä¸ºæ¨ç†è¿‡ç¨‹ä¸­çš„è½¯çº¦æŸã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¥è”åˆä¼˜åŒ–ç»“æ„æœ‰æ•ˆæ€§ã€æ¨ç†å¤æ‚æ€§å’Œç­”æ¡ˆä¸€è‡´æ€§ã€‚åœ¨è‡ªåŠ¨å›å½’æç¤ºä¸‹ç”Ÿæˆçš„æ¨ç†è½¨è¿¹é•¿åº¦è¢«ç”¨æ¥åæ˜ è®¤çŸ¥å¤æ‚æ€§ï¼Œé¼“åŠ±åˆ›å»ºä¸é•¿é“¾æ€ç»´æ¨ç†ç›¸åŒ¹é…çš„æ›´å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒï¼Œè¢«åˆ†ç±»ä¸ºç®€å•ä¸ä¸­ç­‰ï¼ˆGSM8Kï¼ŒMATH-500ï¼‰å’Œå›°éš¾ï¼ˆAIME2024ï¼ŒAIME2025ï¼ŒOlympiadBenchï¼‰ï¼Œç»“æœè¡¨æ˜ï¼ŒMathSmithåœ¨çŸ­é“¾å’Œé•¿é“¾æ€ç»´è®¾ç½®ä¸‹å‡å§‹ç»ˆä¼˜äºç°æœ‰åŸºçº¿ã€‚æ­¤å¤–ï¼Œå¼±ç‚¹èšç„¦çš„å˜ä½“ç”Ÿæˆæ¨¡å—èƒ½å¤Ÿå®ç°ç‰¹å®šæ¦‚å¿µçš„é’ˆå¯¹æ€§æ”¹è¿›ã€‚æ€»çš„æ¥è¯´ï¼ŒMathSmithè¡¨ç°å‡ºå¼ºå¤§çš„å¯æ‰©å±•æ€§ã€æ³›åŒ–èƒ½åŠ›å’Œè¿ç§»èƒ½åŠ›ï¼Œçªæ˜¾äº†é«˜éš¾åº¦åˆæˆæ•°æ®åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05592v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å—é™äºé«˜è´¨é‡ã€é«˜éš¾åº¦è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºæ€§ã€‚ç°æœ‰åˆæˆæ–¹æ³•ä¸»è¦ä¾èµ–äººå·¥ç¼–å†™æ¨¡æ¿çš„è½¬æ¢ï¼Œé™åˆ¶äº†å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºMathSmithæ¡†æ¶ï¼Œé€šè¿‡ä»PlanetMathä¸­éšæœºæŠ½å–æ¦‚å¿µè§£é‡Šå¯¹æ¥æ„å»ºå…¨æ–°æ•°å­¦é—®é¢˜ï¼Œç¡®ä¿æ•°æ®ç‹¬ç«‹å¹¶é¿å…æ±¡æŸ“ã€‚ä¸ºå¢åŠ éš¾åº¦ï¼Œæˆ‘ä»¬åœ¨åŸç†ä¸­è®¾è®¡äº†ä¹ç§é¢„è®¾ç­–ç•¥ä½œä¸ºè½¯çº¦æŸã€‚æ­¤å¤–ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¥è”åˆä¼˜åŒ–ç»“æ„æœ‰æ•ˆæ€§ã€æ¨ç†å¤æ‚æ€§å’Œç­”æ¡ˆä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMathSmithåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ— è®ºæ˜¯ç®€å•ä¸ä¸­ç­‰éš¾åº¦ï¼ˆGSM8Kã€MATH-500ï¼‰è¿˜æ˜¯é«˜éš¾åº¦ï¼ˆAIME2024ã€AIME2025ã€OlympiadBenchï¼‰çš„æµ‹è¯•ç¯å¢ƒä¸‹ï¼Œå‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚æ€»ä½“è€Œè¨€ï¼ŒMathSmithè¡¨ç°å‡ºå¼ºå¤§çš„å¯æ‰©å±•æ€§ã€é€šç”¨æ€§å’Œè¿ç§»æ€§ï¼Œçªæ˜¾é«˜éš¾åº¦åˆæˆæ•°æ®åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢å·²å–å¾—è¿›å±•ï¼Œä½†å—é™äºè®­ç»ƒæ•°æ®çš„è´¨é‡å’Œéš¾åº¦ã€‚</li>
<li>ç°æœ‰æ•°å­¦é—®é¢˜çš„åˆæˆæ–¹æ³•ä¸»è¦ä¾èµ–äººå·¥ç¼–å†™æ¨¡æ¿ï¼Œç¼ºä¹å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>MathSmithæ¡†æ¶é€šè¿‡éšæœºæŠ½å–æ¦‚å¿µè§£é‡Šå¯¹æ¥æ„å»ºå…¨æ–°æ•°å­¦é—®é¢˜ï¼Œç¡®ä¿æ•°æ®ç‹¬ç«‹æ€§å’Œå¤šæ ·æ€§ã€‚</li>
<li>MathSmithé‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–ç»“æ„æœ‰æ•ˆæ€§ã€æ¨ç†å¤æ‚æ€§å’Œç­”æ¡ˆä¸€è‡´æ€§ã€‚</li>
<li>MathSmithåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œé€‚ç”¨äºä¸åŒéš¾åº¦çš„æ•°å­¦é—®é¢˜ã€‚</li>
<li>MathSmithå…·æœ‰å¼ºå¤§çš„å¯æ‰©å±•æ€§ã€é€šç”¨æ€§å’Œè¿ç§»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05592">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0f1a5c7dcb451556e01a89338779f633.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9636410a6ac706d604480825f40f636a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47117c257600b3dc307d1807363174d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25a56c6761c0480b91ef3be33a13768d.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="TextQuests-How-Good-are-LLMs-at-Text-Based-Video-Games"><a href="#TextQuests-How-Good-are-LLMs-at-Text-Based-Video-Games" class="headerlink" title="TextQuests: How Good are LLMs at Text-Based Video Games?"></a>TextQuests: How Good are LLMs at Text-Based Video Games?</h2><p><strong>Authors:Long Phan, Mantas Mazeika, Andy Zou, Dan Hendrycks</strong></p>
<p>Evaluating AI agents within complex, interactive environments that mirror real-world challenges is critical for understanding their practical capabilities. While existing agent benchmarks effectively assess skills like tool use or performance on structured tasks, they often do not fully capture an agentâ€™s ability to operate autonomously in exploratory environments that demand sustained, self-directed reasoning over a long and growing context. To spur the development of agents capable of more robust intrinsic reasoning over long horizons, we introduce TextQuests, a benchmark based on the Infocom suite of interactive fiction games. These text-based adventures, which can take human players over 30 hours and require hundreds of precise actions to solve, serve as an effective proxy for evaluating AI agents on focused, stateful tasks. The benchmark is specifically designed to assess an LLM agentâ€™s capacity for self-contained problem-solving by precluding the use of external tools, thereby focusing on intrinsic long-context reasoning capabilities in an exploratory environment characterized by the need for trial-and-error learning and sustained problem-solving within a single interactive session. We release TextQuests at <a target="_blank" rel="noopener" href="https://textquests.ai/">https://textquests.ai</a>. </p>
<blockquote>
<p>è¯„ä¼°åœ¨å¤æ‚ã€äº¤äº’å¼ç¯å¢ƒä¸­è¿è¡Œçš„AIä»£ç†ï¼Œè¿™äº›ç¯å¢ƒåæ˜ äº†ç°å®ä¸–ç•Œçš„æŒ‘æˆ˜ï¼Œå¯¹äºäº†è§£å…¶å®è·µèƒ½åŠ›è‡³å…³é‡è¦ã€‚å°½ç®¡ç°æœ‰çš„ä»£ç†åŸºå‡†æµ‹è¯•å¯ä»¥æœ‰æ•ˆåœ°è¯„ä¼°å·¥å…·ä½¿ç”¨æˆ–ç»“æ„åŒ–ä»»åŠ¡ä¸Šçš„æŠ€èƒ½ï¼Œä½†å®ƒä»¬é€šå¸¸ä¸èƒ½å®Œå…¨æ•æ‰åˆ°ä»£ç†åœ¨æ¢ç´¢ç¯å¢ƒä¸­è‡ªä¸»è¡ŒåŠ¨çš„èƒ½åŠ›ï¼Œè¿™äº›ç¯å¢ƒè¦æ±‚åœ¨ä¸€ä¸ªä¸æ–­å‘å±•å’Œå¢é•¿çš„èƒŒæ™¯ä¸‹è¿›è¡ŒæŒç»­ã€è‡ªæˆ‘å¯¼å‘çš„æ¨ç†ã€‚ä¸ºäº†ä¿ƒè¿›èƒ½å¤Ÿåœ¨é•¿æœŸå†…å…·å¤‡æ›´å¼ºå†…åœ¨æ¨ç†èƒ½åŠ›çš„ä»£ç†çš„å‘å±•ï¼Œæˆ‘ä»¬æ¨å‡ºäº†åŸºäºInfocomç³»åˆ—äº¤äº’å¼å°è¯´æ¸¸æˆçš„TextQuestsåŸºå‡†æµ‹è¯•ã€‚è¿™äº›åŸºäºæ–‡æœ¬çš„æŒ‘æˆ˜æ€§å†’é™©æ´»åŠ¨ï¼Œå¯ä»¥è®©äººç±»ç©å®¶èŠ±è´¹è¶…è¿‡30å°æ—¶çš„æ—¶é—´ï¼Œéœ€è¦æ•°ç™¾ä¸ªç²¾ç¡®çš„åŠ¨ä½œæ¥è§£å†³ã€‚å®ƒå¯ä½œä¸ºè¯„ä¼°AIä»£ç†åœ¨ä¸“æ³¨ã€æœ‰çŠ¶æ€ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆä»£ç†ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°LLMä»£ç†åœ¨ç¦æ­¢å¤–éƒ¨å·¥å…·çš„æƒ…å†µä¸‹è¿›è¡Œç‹¬ç«‹è§£å†³é—®é¢˜çš„èƒ½åŠ›ï¼Œä»è€Œä¸“æ³¨äºæ¢ç´¢ç¯å¢ƒä¸­å›ºæœ‰çš„é•¿æœŸä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ï¼Œè¿™éœ€è¦å°è¯•å’Œé”™è¯¯å­¦ä¹ ä»¥åŠåœ¨å•ä¸ªäº¤äº’å¼ä¼šè¯å†…æŒç»­è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://textquests.aiå‘å¸ƒtextquests./">https://textquests.aiå‘å¸ƒTextQuestsã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23701v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    è¯„ä¼°AIä»£ç†åœ¨æ¨¡æ‹Ÿç°å®æŒ‘æˆ˜çš„å¤æ‚äº¤äº’ç¯å¢ƒä¸­è¡¨ç°è‡³å…³é‡è¦ï¼Œä»¥ç†è§£å…¶å®è·µèƒ½åŠ›ã€‚ç°æœ‰ä»£ç†åŸºå‡†æµ‹è¯•è™½èƒ½æœ‰æ•ˆè¯„ä¼°å·¥å…·ä½¿ç”¨æˆ–ç»“æ„åŒ–ä»»åŠ¡ä¸Šçš„æŠ€èƒ½ï¼Œä½†å¾€å¾€æ— æ³•å…¨é¢æ•æ‰ä»£ç†åœ¨æ¢ç´¢ç¯å¢ƒä¸­è‡ªä¸»è¡ŒåŠ¨çš„èƒ½åŠ›ï¼Œè¿™ç§ç¯å¢ƒéœ€è¦é•¿æœŸæŒç»­çš„è‡ªå¯¼æ¨ç†ã€‚ä¸ºåˆºæ¿€å¼€å‘èƒ½åœ¨é•¿æœŸè§†é‡ä¸­è¿›è¡Œæ›´ç¨³å¥å†…åœ¨æ¨ç†çš„ä»£ç†ï¼Œæˆ‘ä»¬æ¨å‡ºäº†åŸºäºInfocomäº¤äº’å¼å°è¯´æ¸¸æˆç³»åˆ—çš„TextQuestsåŸºå‡†æµ‹è¯•ã€‚è¿™äº›åŸºäºæ–‡æœ¬çš„æŒ‘æˆ˜ï¼Œäººç±»ç©å®¶éœ€è¦è¶…è¿‡30å°æ—¶å’Œæ•°ç™¾ä¸ªç²¾ç¡®åŠ¨ä½œæ¥è§£å†³ï¼Œå¯ä½œä¸ºè¯„ä¼°AIä»£ç†åœ¨ä¸“æ³¨ã€æœ‰çŠ¶æ€ä»»åŠ¡ä¸Šçš„èƒ½åŠ›çš„æœ‰æ•ˆä»£ç†ã€‚è¯¥åŸºå‡†æµ‹è¯•æ—¨åœ¨è¯„ä¼°LLMä»£ç†çš„è‡ªæˆ‘è§£å†³é—®é¢˜çš„èƒ½åŠ›ï¼Œç¦æ­¢å¤–éƒ¨å·¥å…·çš„ä½¿ç”¨ï¼Œé‡ç‚¹æ˜¯åœ¨æ¢ç´¢ç¯å¢ƒä¸­å†…åœ¨é•¿æœŸä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ï¼Œè¿™éœ€è¦è¯•é”™å­¦ä¹ å’Œå•æ¬¡äº’åŠ¨ä¼šè¯ä¸­çš„æŒç»­é—®é¢˜è§£å†³èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://textquests.aiå‘å¸ƒtextquests./">https://textquests.aiå‘å¸ƒTextQuestsã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¯„ä¼°AIä»£ç†åœ¨æ¨¡æ‹Ÿç°å®æŒ‘æˆ˜çš„å¤æ‚äº¤äº’ç¯å¢ƒä¸­çš„è¡¨ç°è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ä»£ç†åŸºå‡†æµ‹è¯•æ— æ³•å…¨é¢è¯„ä¼°ä»£ç†åœ¨æ¢ç´¢ç¯å¢ƒä¸­çš„è‡ªä¸»è¡ŒåŠ¨èƒ½åŠ›ã€‚</li>
<li>TextQuestsåŸºå‡†æµ‹è¯•åŸºäºInfocomäº¤äº’å¼å°è¯´æ¸¸æˆï¼Œæ—¨åœ¨è¯„ä¼°AIä»£ç†çš„é•¿æœŸç¨³å¥å†…åœ¨æ¨ç†èƒ½åŠ›ã€‚</li>
<li>TextQuestsåŸºå‡†æµ‹è¯•é€šè¿‡ç¦æ­¢å¤–éƒ¨å·¥å…·çš„ä½¿ç”¨ï¼Œé‡ç‚¹æµ‹è¯•LLMä»£ç†çš„è‡ªæˆ‘è§£å†³é—®é¢˜èƒ½åŠ›ã€‚</li>
<li>æ¢ç´¢ç¯å¢ƒéœ€è¦è¯•é”™å­¦ä¹ å’Œå•æ¬¡äº’åŠ¨ä¼šè¯ä¸­çš„æŒç»­é—®é¢˜è§£å†³èƒ½åŠ›ã€‚</li>
<li>TextQuestsä¸ºè¯„ä¼°AIåœ¨å¤„ç†å¤æ‚é—®é¢˜å’Œä»»åŠ¡æ—¶çš„å®é™…èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„å¹³å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23701">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cb55dbc6dafe7c33680b5033d99086f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce0b650df9a3eacf3b6093c3b73d3200.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e93490c7ad5e97e7f13367d0d8322a38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b35045405f9ff9dc9f47afcd506155a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-efc0e3bda7fdcab09c8ce6ffd4768792.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12b6bf515245a558f0870be5d6d93f91.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-97c4fa650506701452877a119603c682.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ARAG-Agentic-Retrieval-Augmented-Generation-for-Personalized-Recommendation"><a href="#ARAG-Agentic-Retrieval-Augmented-Generation-for-Personalized-Recommendation" class="headerlink" title="ARAG: Agentic Retrieval Augmented Generation for Personalized   Recommendation"></a>ARAG: Agentic Retrieval Augmented Generation for Personalized   Recommendation</h2><p><strong>Authors:Reza Yousefi Maragheh, Pratheek Vadla, Priyank Gupta, Kai Zhao, Aysenur Inan, Kehui Yao, Jianpeng Xu, Praveen Kanumala, Jason Cho, Sushant Kumar</strong></p>
<p>Retrieval-Augmented Generation (RAG) has shown promise in enhancing recommendation systems by incorporating external context into large language model prompts. However, existing RAG-based approaches often rely on static retrieval heuristics and fail to capture nuanced user preferences in dynamic recommendation scenarios. In this work, we introduce ARAG, an Agentic Retrieval-Augmented Generation framework for Personalized Recommendation, which integrates a multi-agent collaboration mechanism into the RAG pipeline. To better understand the long-term and session behavior of the user, ARAG leverages four specialized LLM-based agents: a User Understanding Agent that summarizes user preferences from long-term and session contexts, a Natural Language Inference (NLI) Agent that evaluates semantic alignment between candidate items retrieved by RAG and inferred intent, a context summary agent that summarizes the findings of NLI agent, and an Item Ranker Agent that generates a ranked list of recommendations based on contextual fit. We evaluate ARAG accross three datasets. Experimental results demonstrate that ARAG significantly outperforms standard RAG and recency-based baselines, achieving up to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an ablation study to analyse the effect by different components of ARAG. Our findings highlight the effectiveness of integrating agentic reasoning into retrieval-augmented recommendation and provide new directions for LLM-based personalization. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æç¤ºä¸­èå…¥å¤–éƒ¨ä¸Šä¸‹æ–‡ï¼Œåœ¨æå‡æ¨èç³»ç»Ÿæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºRAGçš„æ–¹æ³•å¾€å¾€ä¾èµ–äºé™æ€æ£€ç´¢å¯å‘å¼ç­–ç•¥ï¼Œæ— æ³•æ•æ‰åˆ°åŠ¨æ€æ¨èåœºæ™¯ä¸­çš„å¾®å¦™ç”¨æˆ·åå¥½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ARAGï¼ˆä¸ªæ€§åŒ–æ¨èçš„Agenticæ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼‰ï¼Œå®ƒå°†å¤šæ™ºèƒ½ä½“åä½œæœºåˆ¶é›†æˆåˆ°RAGç®¡é“ä¸­ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£ç”¨æˆ·çš„é•¿æœŸå’Œä¼šè¯è¡Œä¸ºï¼ŒARAGé‡‡ç”¨äº†å››ä¸ªåŸºäºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„ä¸“ç”¨æ™ºèƒ½ä½“ï¼šä¸€ä¸ªç”¨æˆ·ç†è§£æ™ºèƒ½ä½“ï¼Œç”¨äºä»é•¿æœŸå’Œä¼šè¯ä¸Šä¸‹æ–‡ä¸­æ€»ç»“ç”¨æˆ·åå¥½ï¼›ä¸€ä¸ªè‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰æ™ºèƒ½ä½“ï¼Œç”¨äºè¯„ä¼°ç”±RAGæ£€ç´¢åˆ°çš„å€™é€‰é¡¹ç›®å’Œæ¨æ–­æ„å›¾ä¹‹é—´çš„è¯­ä¹‰å¯¹é½ç¨‹åº¦ï¼›ä¸€ä¸ªä¸Šä¸‹æ–‡æ‘˜è¦æ™ºèƒ½ä½“ï¼Œç”¨äºæ€»ç»“NLIæ™ºèƒ½ä½“çš„å‘ç°ï¼›ä»¥åŠä¸€ä¸ªé¡¹ç›®æ’åæ™ºèƒ½ä½“ï¼Œæ ¹æ®ä¸Šä¸‹æ–‡åŒ¹é…åº¦ç”Ÿæˆæ’åæ¨èåˆ—è¡¨ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°äº†ARAGã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒARAGæ˜¾è‘—ä¼˜äºæ ‡å‡†RAGå’ŒåŸºäºæ—¶æ•ˆæ€§çš„åŸºå‡†çº¿ï¼Œåœ¨NDCG@5ä¸Šæé«˜äº†42.1%ï¼Œåœ¨Hit@5ä¸Šæé«˜äº†35.5%ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†ä¸€é¡¹æ¶ˆèç ”ç©¶ï¼Œä»¥åˆ†æARAGä¸åŒç»„ä»¶çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜å°†æ™ºèƒ½ä½“æ¨ç†é›†æˆåˆ°æ£€ç´¢å¢å¼ºæ¨èä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºåŸºäºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„ä¸ªæ€§åŒ–æ¨èæä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21931v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„Retrieval-Augmented Generationï¼ˆRAGï¼‰åœ¨æ¨èç³»ç»Ÿä¸­å±•ç°å‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¾èµ–é™æ€æ£€ç´¢å¯å‘å¼ç­–ç•¥ï¼Œæ— æ³•æ•æ‰åŠ¨æ€æ¨èåœºæ™¯ä¸­çš„ç”¨æˆ·åå¥½ç»†å¾®å·®åˆ«ã€‚ä¸ºæ­¤ï¼Œæœ¬ç ”ç©¶æå‡ºARAGæ¡†æ¶ï¼Œé€šè¿‡é›†æˆå¤šä»£ç†åä½œæœºåˆ¶ï¼Œæ”¹å–„RAGçš„æ€§èƒ½ã€‚ARAGåˆ©ç”¨å››ä¸ªä¸“é—¨çš„å¤§è¯­è¨€æ¨¡å‹ä»£ç†ç†è§£ç”¨æˆ·é•¿æœŸå’Œä¼šè¯è¡Œä¸ºï¼Œå¹¶é€šè¿‡è¯„ä¼°å€™é€‰é¡¹ç›®å’Œæ¨æ–­æ„å›¾ä¹‹é—´çš„è¯­ä¹‰å¯¹é½ç¨‹åº¦æ¥ç”Ÿæˆä¸ªæ€§åŒ–æ¨èã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒARAGæ˜¾è‘—ä¼˜äºæ ‡å‡†RAGå’ŒåŸºäºæ—¶é—´çš„æ–°é¢–æ€§æ–¹æ³•ï¼Œå…¶åœ¨NDCG@5ä¸Šæå‡äº†42.1%ï¼ŒHit@5ä¸Šæå‡äº†35.5%ã€‚æ­¤ç ”ç©¶ä¸ºåŸºäºLLMçš„ä¸ªäººåŒ–æ¨èæä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ARAGæ¡†æ¶é€šè¿‡é›†æˆå¤šä»£ç†åä½œæœºåˆ¶æ”¹è¿›äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼ˆRAGï¼‰ã€‚</li>
<li>ARAGä½¿ç”¨å››ä¸ªä¸“é—¨çš„å¤§è¯­è¨€æ¨¡å‹ä»£ç†ï¼šç”¨æˆ·ç†è§£ä»£ç†ã€è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰ä»£ç†ã€ä¸Šä¸‹æ–‡æ‘˜è¦ä»£ç†å’Œé¡¹ç›®æ’åä»£ç†ï¼Œä»¥ç†è§£ç”¨æˆ·è¡Œä¸ºå¹¶ç”Ÿæˆä¸ªæ€§åŒ–æ¨èã€‚</li>
<li>ARAGæ˜¾è‘—ä¼˜äºæ ‡å‡†RAGå’ŒåŸºäºæ—¶é—´çš„æ–°é¢–æ€§æ–¹æ³•ï¼Œåœ¨NDCG@5å’ŒHit@5ç­‰æŒ‡æ ‡ä¸Šæœ‰æ˜¾è‘—çš„æå‡ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†ARAGæ¡†æ¶ä¸­æ¯ä¸ªç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21931">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-da3a7790e518066c985382b5a14a59cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a60348237877c064fe7d60a17d9696aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48b8911cc41247821f139f12e9134750.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Scaling-Transformers-for-Discriminative-Recommendation-via-Generative-Pretraining"><a href="#Scaling-Transformers-for-Discriminative-Recommendation-via-Generative-Pretraining" class="headerlink" title="Scaling Transformers for Discriminative Recommendation via Generative   Pretraining"></a>Scaling Transformers for Discriminative Recommendation via Generative   Pretraining</h2><p><strong>Authors:Chunqi Wang, Bingchao Wu, Zheng Chen, Lei Shen, Bing Wang, Xiaoyi Zeng</strong></p>
<p>Discriminative recommendation tasks, such as CTR (click-through rate) and CVR (conversion rate) prediction, play critical roles in the ranking stage of large-scale industrial recommender systems. However, training a discriminative model encounters a significant overfitting issue induced by data sparsity. Moreover, this overfitting issue worsens with larger models, causing them to underperform smaller ones. To address the overfitting issue and enhance model scalability, we propose a framework named GPSD (\textbf{G}enerative \textbf{P}retraining for \textbf{S}calable \textbf{D}iscriminative Recommendation), drawing inspiration from generative training, which exhibits no evident signs of overfitting. GPSD leverages the parameters learned from a pretrained generative model to initialize a discriminative model, and subsequently applies a sparse parameter freezing strategy. Extensive experiments conducted on both industrial-scale and publicly available datasets demonstrate the superior performance of GPSD. Moreover, it delivers remarkable improvements in online A&#x2F;B tests. GPSD offers two primary advantages: 1) it substantially narrows the generalization gap in model training, resulting in better test performance; and 2) it leverages the scalability of Transformers, delivering consistent performance gains as models are scaled up. Specifically, we observe consistent performance improvements as the model dense parameters scale from 13K to 0.3B, closely adhering to power laws. These findings pave the way for unifying the architectures of recommendation models and language models, enabling the direct application of techniques well-established in large language models to recommendation models. The code is available at <a target="_blank" rel="noopener" href="https://github.com/chqiwang/gpsd-rec">https://github.com/chqiwang/gpsd-rec</a>. </p>
<blockquote>
<p>åˆ¤åˆ«å¼æ¨èä»»åŠ¡ï¼Œå¦‚ç‚¹å‡»ç‡ï¼ˆCTRï¼‰å’Œè½¬åŒ–ç‡ï¼ˆCVRï¼‰é¢„æµ‹ï¼Œåœ¨å¤§è§„æ¨¡å·¥ä¸šæ¨èç³»ç»Ÿçš„æ’åºé˜¶æ®µèµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œè®­ç»ƒåˆ¤åˆ«æ¨¡å‹ä¼šé‡åˆ°ç”±æ•°æ®ç¨€ç–å¼•èµ·çš„ä¸¥é‡è¿‡æ‹Ÿåˆé—®é¢˜ã€‚è€Œä¸”ï¼Œéšç€æ¨¡å‹ä½“ç§¯çš„å¢å¤§ï¼Œè¿‡æ‹Ÿåˆé—®é¢˜ä¼šæ¶åŒ–ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸å¦‚è¾ƒå°çš„æ¨¡å‹ã€‚ä¸ºäº†è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜å¹¶å¢å¼ºæ¨¡å‹çš„å¯æ‰©å±•æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºGPSDï¼ˆé¢å‘å¯æ‰©å±•åˆ¤åˆ«æ¨èçš„ç”Ÿæˆé¢„è®­ç»ƒæ¡†æ¶ï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å€Ÿé‰´äº†ç”Ÿæˆå¼è®­ç»ƒï¼Œå‡ ä¹ä¸ä¼šå‡ºç°æ˜æ˜¾çš„è¿‡æ‹Ÿåˆè¿¹è±¡ã€‚GPSDåˆ©ç”¨ä»é¢„è®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹ä¸­å­¦ä¹ çš„å‚æ•°æ¥åˆå§‹åŒ–åˆ¤åˆ«æ¨¡å‹ï¼Œéšååº”ç”¨ç¨€ç–å‚æ•°å†»ç»“ç­–ç•¥ã€‚åœ¨å·¥ä¸šçº§å’Œå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜äº†GPSDçš„å“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨åœ¨çº¿A&#x2F;Bæµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚GPSDå…·æœ‰ä¸¤ä¸ªä¸»è¦ä¼˜åŠ¿ï¼š1ï¼‰å®ƒå¤§å¤§ç¼©å°äº†æ¨¡å‹è®­ç»ƒä¸­çš„æ³›åŒ–å·®è·ï¼Œä»è€Œæé«˜äº†æµ‹è¯•æ€§èƒ½ï¼›2ï¼‰å®ƒåˆ©ç”¨Transformerçš„å¯æ‰©å±•æ€§ï¼Œéšç€æ¨¡å‹çš„è§„æ¨¡æ‰©å¤§ï¼Œæ€§èƒ½æŒç»­æé«˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°éšç€æ¨¡å‹å¯†é›†å‚æ•°ä»13Kæ‰©å±•åˆ°0.3Bï¼Œæ€§èƒ½æŒç»­æé«˜ï¼Œè¿™ç´§å¯†éµå¾ªå¹‚å¾‹ã€‚è¿™äº›å‘ç°ä¸ºç»Ÿä¸€æ¨èæ¨¡å‹å’Œè¯­è¨€æ¨¡å‹çš„æ¶æ„é“ºå¹³äº†é“è·¯ï¼Œä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æˆç†Ÿçš„æŠ€æœ¯å¯ä»¥ç›´æ¥åº”ç”¨äºæ¨èæ¨¡å‹ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/chqiwang/gpsd-rec%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/chqiwang/gpsd-recæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03699v2">PDF</a> KDDâ€™25</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹æ¨èç³»ç»Ÿä¸­çš„è¿‡æ‹Ÿåˆé—®é¢˜æå‡ºäº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆGPSDæ¡†æ¶ï¼Œåˆ©ç”¨ç”Ÿæˆå¼é¢„è®­ç»ƒä¸ºé‰´åˆ«å¼æ¨èç³»ç»Ÿæä¾›åˆå§‹å‚æ•°ï¼Œå¹¶ç»“åˆç¨€ç–å‚æ•°å†»ç»“ç­–ç•¥æ¥è§£å†³æ•°æ®ç¨€ç–å¯¼è‡´çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚è¯¥æ¡†æ¶åœ¨å¤§å‹å·¥ä¸šæ¨èç³»ç»Ÿå’Œå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œåœ¨çº¿A&#x2F;Bæµ‹è¯•ä¹Ÿå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚å…¶ä¸»è¦ä¼˜åŠ¿åœ¨äºç¼©å°äº†æ¨¡å‹è®­ç»ƒä¸­çš„æ³›åŒ–å·®è·ï¼Œå¹¶å€ŸåŠ©Transformerçš„å¯æ‰©å±•æ€§ï¼Œéšç€æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§ï¼Œæ€§èƒ½ä¸æ–­æå‡ã€‚è¯¥æ¡†æ¶ä¸ºæ¨èæ¨¡å‹å’Œè¯­è¨€æ¨¡å‹çš„æ¶æ„ç»Ÿä¸€å¥ å®šäº†åŸºç¡€ï¼Œä½¿å¾—è¯­è¨€æ¨¡å‹ä¸­çš„æŠ€æœ¯å¯ä»¥ç›´æ¥åº”ç”¨äºæ¨èæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‰´åˆ«å¼æ¨èä»»åŠ¡åœ¨å¤§å‹å·¥ä¸šæ¨èç³»ç»Ÿä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œå¦‚CTRå’ŒCVRé¢„æµ‹ï¼Œä½†åœ¨æ•°æ®ç¨€ç–åœºæ™¯ä¸‹è®­ç»ƒæ—¶å­˜åœ¨ä¸¥é‡çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>GPSDæ¡†æ¶ç»“åˆç”Ÿæˆå¼é¢„è®­ç»ƒè§£å†³äº†è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>GPSDæ¡†æ¶åˆ©ç”¨ç”Ÿæˆå¼é¢„è®­ç»ƒæ¨¡å‹å‚æ•°åˆå§‹åŒ–é‰´åˆ«å¼æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨ç¨€ç–å‚æ•°å†»ç»“ç­–ç•¥è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½ã€‚</li>
<li>åœ¨å·¥ä¸šçº§å’Œå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†GPSDæ¡†æ¶çš„ä¼˜è¶Šæ€§èƒ½ã€‚åœ¨çº¿A&#x2F;Bæµ‹è¯•ä¹Ÿæ˜¾ç¤ºäº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</li>
<li>GPSDæ¡†æ¶çš„ä¸»è¦ä¼˜åŠ¿åœ¨äºç¼©å°äº†æ¨¡å‹è®­ç»ƒçš„æ³›åŒ–å·®è·ï¼Œå¹¶éšç€æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§ï¼Œæ€§èƒ½æŒç»­æé«˜ã€‚</li>
<li>GPSDæ¡†æ¶æ¨åŠ¨äº†æ¨èæ¨¡å‹å’Œè¯­è¨€æ¨¡å‹çš„æ¶æ„ç»Ÿä¸€ï¼Œä½¿å¾—è¯­è¨€æ¨¡å‹çš„æŠ€æœ¯å¯ä»¥ç›´æ¥åº”ç”¨äºæ¨èæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03699">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ee61e14e814664f91990fc3d108d6cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80ff2e363173d6ab38e6be88bd5a5632.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb62b7a0f425bc97d23a0f906f96c4b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a83dc258dfa35a1e2d9f32f4127037b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0402bacf94288b133099f4b020055c70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02e2203e613cc6d80b415ec0cffd2631.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="TechniqueRAG-Retrieval-Augmented-Generation-for-Adversarial-Technique-Annotation-in-Cyber-Threat-Intelligence-Text"><a href="#TechniqueRAG-Retrieval-Augmented-Generation-for-Adversarial-Technique-Annotation-in-Cyber-Threat-Intelligence-Text" class="headerlink" title="TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique   Annotation in Cyber Threat Intelligence Text"></a>TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique   Annotation in Cyber Threat Intelligence Text</h2><p><strong>Authors:Ahmed Lekssays, Utsav Shukla, Husrev Taha Sencar, Md Rizwan Parvez</strong></p>
<p>Accurately identifying adversarial techniques in security texts is critical for effective cyber defense. However, existing methods face a fundamental trade-off: they either rely on generic models with limited domain precision or require resource-intensive pipelines that depend on large labeled datasets and task-specific optimizations, such as custom hard-negative mining and denoising, resources rarely available in specialized domains.   We propose TechniqueRAG, a domain-specific retrieval-augmented generation (RAG) framework that bridges this gap by integrating off-the-shelf retrievers, instruction-tuned LLMs, and minimal text-technique pairs. Our approach addresses data scarcity by fine-tuning only the generation component on limited in-domain examples, circumventing the need for resource-intensive retrieval training. While conventional RAG mitigates hallucination by coupling retrieval and generation, its reliance on generic retrievers often introduces noisy candidates, limiting domain-specific precision. To address this, we enhance retrieval quality and domain specificity through zero-shot LLM re-ranking, which explicitly aligns retrieved candidates with adversarial techniques.   Experiments on multiple security benchmarks demonstrate that TechniqueRAG achieves state-of-the-art performance without extensive task-specific optimizations or labeled data, while comprehensive analysis provides further insights. </p>
<blockquote>
<p>å‡†ç¡®åœ°è¯†åˆ«å®‰å…¨æ–‡æœ¬ä¸­çš„å¯¹æŠ—æŠ€æœ¯æ˜¯æœ‰æ•ˆç½‘ç»œé˜²å¾¡çš„å…³é”®ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´ä¸€ä¸ªåŸºæœ¬æƒè¡¡ï¼šå®ƒä»¬è¦ä¹ˆä¾èµ–äºå…·æœ‰æœ‰é™åŸŸç²¾åº¦çš„é€šç”¨æ¨¡å‹ï¼Œè¦ä¹ˆéœ€è¦ä¾èµ–å¤§é‡æ ‡è®°æ•°æ®é›†å’Œä»»åŠ¡ç‰¹å®šä¼˜åŒ–ï¼ˆå¦‚è‡ªå®šä¹‰çš„ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜å’Œå»å™ªï¼‰çš„èµ„æºå¯†é›†å‹ç®¡é“ï¼Œè¿™äº›èµ„æºåœ¨ç‰¹å®šé¢†åŸŸå¾ˆå°‘å¯ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†TechniqueRAGï¼Œè¿™æ˜¯ä¸€ä¸ªåŸŸç‰¹å®šçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œå®ƒé€šè¿‡é›†æˆç°æˆçš„æ£€ç´¢å™¨ã€æŒ‡ä»¤è°ƒä¼˜çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œå°‘é‡çš„æ–‡æœ¬æŠ€æœ¯å¯¹æ¥å¼¥è¡¥è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä»…å¯¹åŸŸå†…æœ‰é™ç¤ºä¾‹è¿›è¡Œç”Ÿæˆç»„ä»¶çš„å¾®è°ƒæ¥è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œä»è€Œé¿å…äº†èµ„æºå¯†é›†å‹çš„æ£€ç´¢è®­ç»ƒéœ€æ±‚ã€‚è™½ç„¶ä¼ ç»Ÿçš„RAGé€šè¿‡è€¦åˆæ£€ç´¢å’Œç”Ÿæˆæ¥ç¼“è§£è™šæ„é—®é¢˜ï¼Œä½†å®ƒå¯¹é€šç”¨æ£€ç´¢å™¨çš„ä¾èµ–å¾€å¾€ä¼šå¼•å…¥å˜ˆæ‚çš„å€™é€‰å¯¹è±¡ï¼Œé™åˆ¶äº†åŸŸç‰¹å®šçš„ç²¾åº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡é›¶æ ·æœ¬å¤§å‹è¯­è¨€æ¨¡å‹é‡æ–°æ’åºæ¥æé«˜æ£€ç´¢è´¨é‡å’ŒåŸŸç‰¹å¼‚æ€§ï¼Œè¿™æ˜ç¡®åœ°ä½¿æ£€ç´¢åˆ°çš„å€™é€‰å¯¹è±¡ä¸å¯¹æŠ—æŠ€æœ¯ç›¸åŒ¹é…ã€‚åœ¨å¤šä¸ªå®‰å…¨åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTechniqueRAGåœ¨ä¸è¿›è¡Œå¤§é‡ç‰¹å®šä»»åŠ¡ä¼˜åŒ–æˆ–ä½¿ç”¨æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè€Œç»¼åˆåˆ†æåˆ™æä¾›äº†è¿›ä¸€æ­¥çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11988v2">PDF</a> Accepted at ACL (Findings) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å®‰å…¨æ–‡æœ¬ä¸­å‡†ç¡®è¯†åˆ«å¯¹æŠ—æ€§æŠ€æœ¯å¯¹äºæœ‰æ•ˆç½‘ç»œå®‰å…¨é˜²å¾¡çš„é‡è¦æ€§ã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨é€šç”¨æ€§ä¸ç²¾ç¡®åº¦ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†TechniqueRAGæ¡†æ¶ï¼Œé€šè¿‡é›†æˆç°æˆçš„æ£€ç´¢å™¨ã€æŒ‡ä»¤ä¼˜åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå°‘é‡çš„æ–‡æœ¬æŠ€æœ¯é…å¯¹ï¼Œä»¥å¼¥è¡¥è¿™ä¸€å·®è·ã€‚è¯¥æ¡†æ¶è§£å†³äº†æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œåªéœ€å¯¹ç”Ÿæˆç»„ä»¶è¿›è¡Œå¾®è°ƒï¼Œè€Œæ— éœ€è¿›è¡Œèµ„æºå¯†é›†å‹çš„æ£€ç´¢è®­ç»ƒã€‚å®éªŒè¯æ˜ï¼ŒTechniqueRAGåœ¨ä¸è¿›è¡Œå¤§é‡ç‰¹å®šä»»åŠ¡ä¼˜åŒ–æˆ–ä½¿ç”¨æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå³å¯å®ç°å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡†ç¡®è¯†åˆ«å®‰å…¨æ–‡æœ¬ä¸­çš„å¯¹æŠ—æ€§æŠ€æœ¯å¯¹äºæœ‰æ•ˆç½‘ç»œå®‰å…¨é˜²å¾¡è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´é€šç”¨æ€§ä¸ç²¾ç¡®åº¦ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>TechniqueRAGæ¡†æ¶é€šè¿‡é›†æˆç°æˆçš„æ£€ç´¢å™¨ã€LLMå’Œæ–‡æœ¬æŠ€æœ¯é…å¯¹æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶è§£å†³äº†æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œåªéœ€å¯¹ç”Ÿæˆç»„ä»¶è¿›è¡Œå¾®è°ƒã€‚</li>
<li>TechniqueRAGå¢å¼ºäº†æ£€ç´¢è´¨é‡å’Œé¢†åŸŸç‰¹å¼‚æ€§ï¼Œé€šè¿‡é›¶æ ·æœ¬LLMé‡æ–°æ’åºï¼Œæ˜ç¡®å°†æ£€ç´¢åˆ°çš„å€™é€‰å†…å®¹ä¸å¯¹æŠ—æ€§æŠ€æœ¯å¯¹é½ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒTechniqueRAGåœ¨å¤šä¸ªå®‰å…¨åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-59561e59b9fc5cff8c2739fe8cb2e2e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64105975f73050a0d216b0acb495578e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0449a68ec7301665b6daf4c9bd9acd7b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6006da90d7c9ba6c535904fde0b31051.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-13/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-13/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-13/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fe227313b697f8ef6630d0cca633b67d.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-13  MuaLLM A Multimodal Large Language Model Agent for Circuit Design   Assistance with Hybrid Contextual Retrieval-Augmented Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-13/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2a026615b274442b18b2e0d5a4cb1e26.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-13  Capabilities of GPT-5 on Multimodal Medical Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28051.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
