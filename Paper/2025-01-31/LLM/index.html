<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-01-31  LEKALLM-Enhanced Knowledge Augmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-899c1f7b0f51210def275e2cffe56f3a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-31
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    41 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-31-更新"><a href="#2025-01-31-更新" class="headerlink" title="2025-01-31 更新"></a>2025-01-31 更新</h1><h2 id="LEKA-LLM-Enhanced-Knowledge-Augmentation"><a href="#LEKA-LLM-Enhanced-Knowledge-Augmentation" class="headerlink" title="LEKA:LLM-Enhanced Knowledge Augmentation"></a>LEKA:LLM-Enhanced Knowledge Augmentation</h2><p><strong>Authors:Xinhao Zhang, Jinghan Zhang, Fengran Mo, Dongjie Wang, Yanjie Fu, Kunpeng Liu</strong></p>
<p>Humans excel in analogical learning and knowledge transfer and, more importantly, possess a unique understanding of identifying appropriate sources of knowledge. From a model’s perspective, this presents an interesting challenge. If models could autonomously retrieve knowledge useful for transfer or decision-making to solve problems, they would transition from passively acquiring to actively accessing and learning from knowledge. However, filling models with knowledge is relatively straightforward – it simply requires more training and accessible knowledge bases. The more complex task is teaching models about which knowledge can be analogized and transferred. Therefore, we design a knowledge augmentation method LEKA for knowledge transfer that actively searches for suitable knowledge sources that can enrich the target domain’s knowledge. This LEKA method extracts key information from textual information from the target domain, retrieves pertinent data from external data libraries, and harmonizes retrieved data with the target domain data in feature space and marginal probability measures. We validate the effectiveness of our approach through extensive experiments across various domains and demonstrate significant improvements over traditional methods in reducing computational costs, automating data alignment, and optimizing transfer learning outcomes. </p>
<blockquote>
<p>人类在类比学习和知识迁移方面表现出卓越的能力，更重要的是，人类拥有识别合适知识来源的独特理解力。从模型的角度来看，这是一个有趣的挑战。如果模型能够自主检索对迁移或决策有用的知识来解决新问题，它们将实现从被动获取知识到主动访问和学习知识的转变。然而，填充模型的知识相对简单，只需更多的训练和可访问的知识库即可。更复杂的任务是教会模型哪些知识可以进行类比和迁移。因此，我们设计了一种名为LEKA的知识增强方法用于知识迁移，该方法可以积极搜索合适的能丰富目标领域知识的知识来源。这种LEKA方法从目标领域的文本信息中提取关键信息，从外部数据库检索相关数据，并在特征空间和边缘概率度量中将检索到的数据与目标领域数据进行协调。我们通过在不同领域的广泛实验验证了我们的方法的有效性，并在降低计算成本、自动化数据对齐和优化迁移学习结果方面显示出对传统方法的显著改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.17802v1">PDF</a> </p>
<p><strong>Summary</strong><br>人类擅长类比学习和知识迁移，并能独特地识别适当的知识来源。模型若能自主检索有用的知识来解决迁移或决策问题，将从被动获取转变为主动访问和学习知识。设计了一种名为LEKA的知识增强方法，用于知识迁移，该方法能积极搜索合适的知识来源以丰富目标领域的知识。通过从目标领域提取关键信息、从外部数据库检索相关数据以及在特征空间和边际概率度量中将检索的数据与目标领域数据进行协调，实现知识增强。通过实验验证，该方法在减少计算成本、自动化数据对齐和优化迁移学习结果方面显著优于传统方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人类擅长类比学习和知识迁移，能识别适当的知识来源。</li>
<li>模型自主检索知识对解决迁移和决策问题至关重要，需要从被动获取向主动访问和学习转变。</li>
<li>设计了LEKA知识增强方法用于知识迁移，能积极搜索并丰富目标领域的知识。</li>
<li>LEKA方法通过提取目标领域的关键信息，检索外部数据并进行协调，实现知识增强。</li>
<li>LEKA方法在减少计算成本、自动化数据对齐方面优于传统方法。</li>
<li>LEKA方法能有效优化迁移学习结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.17802">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a94422720dad56802c1f87e8c2782912.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-def788e98352fadf33578ad3db05fc69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc46011f5fffeaef2d3bd4784c9b49eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a5aad512c5a34d3816e320312553eb9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Leveraging-Multimodal-LLM-for-Inspirational-User-Interface-Search"><a href="#Leveraging-Multimodal-LLM-for-Inspirational-User-Interface-Search" class="headerlink" title="Leveraging Multimodal LLM for Inspirational User Interface Search"></a>Leveraging Multimodal LLM for Inspirational User Interface Search</h2><p><strong>Authors:Seokhyeon Park, Yumin Song, Soohyun Lee, Jaeyoung Kim, Jinwook Seo</strong></p>
<p>Inspirational search, the process of exploring designs to inform and inspire new creative work, is pivotal in mobile user interface (UI) design. However, exploring the vast space of UI references remains a challenge. Existing AI-based UI search methods often miss crucial semantics like target users or the mood of apps. Additionally, these models typically require metadata like view hierarchies, limiting their practical use. We used a multimodal large language model (MLLM) to extract and interpret semantics from mobile UI images. We identified key UI semantics through a formative study and developed a semantic-based UI search system. Through computational and human evaluations, we demonstrate that our approach significantly outperforms existing UI retrieval methods, offering UI designers a more enriched and contextually relevant search experience. We enhance the understanding of mobile UI design semantics and highlight MLLMs’ potential in inspirational search, providing a rich dataset of UI semantics for future studies. </p>
<blockquote>
<p>灵感搜索是探索设计以启发新的创造性工作的重要过程，在手机用户界面（UI）设计中起着至关重要的作用。然而，探索广阔的UI参考空间仍然是一个挑战。现有的基于人工智能的UI搜索方法往往会忽略目标用户或应用程序情绪等关键语义。此外，这些模型通常需要视图层次结构等元数据，从而限制了它们的实际应用。我们使用多模态大型语言模型（MLLM）从移动UI图像中提取和解释语义。我们通过形成性研究确定了关键的UI语义，并开发了一个基于语义的UI搜索系统。通过计算和人类评估，我们证明我们的方法显著优于现有的UI检索方法，为UI设计师提供更丰富、语境更相关的搜索体验。我们增强了移动UI设计语义的理解，突出了大型语言模型在灵感搜索中的潜力，并为未来的研究提供了丰富的UI语义数据集。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.17799v1">PDF</a> In Proceedings of the SIGCHI Conference on Human Factors in Computing   Systems (CHI ‘25)</p>
<p><strong>Summary</strong><br>移动用户界面设计灵感搜索对于寻找新创意至关重要，然而现有的AI用户界面搜索方法缺乏针对目标用户和应用程序氛围等重要语义的理解，并且通常需要视图层次结构等元数据。通过使用多模态大型语言模型，我们从移动用户界面图像中提取并解释语义信息，开发了一种基于语义的用户界面搜索系统。通过计算和人类评估，我们证明了该方法显著优于现有用户界面检索方法，为UI设计师提供更丰富和上下文相关的搜索体验。这项研究增强了我们对移动用户界面设计语义的理解，并突出了多模态大型语言模型在灵感搜索中的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>移动用户界面设计灵感搜索具有挑战性，但探索设计的巨大空间对于新创意至关重要。</li>
<li>现有AI用户界面搜索方法缺乏对目标用户和应用程序氛围等重要语义的理解。</li>
<li>多模态大型语言模型能够提取和解释移动用户界面图像中的语义信息。</li>
<li>基于语义的用户界面搜索系统通过计算和人类评估，证明其在用户界面检索方面的优越性。</li>
<li>该研究增强了我们对移动用户界面设计语义的理解。</li>
<li>多模态大型语言模型在灵感搜索中具有巨大潜力，可以为UI设计师提供更丰富和上下文相关的搜索体验。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.17799">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5bbec8acaba51c76ea74d4b4817e53fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3a9d78cfb0d0d464622247a064670cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-97b43d1e44a56500e427eef4acb63586.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="2SSP-A-Two-Stage-Framework-for-Structured-Pruning-of-LLMs"><a href="#2SSP-A-Two-Stage-Framework-for-Structured-Pruning-of-LLMs" class="headerlink" title="2SSP: A Two-Stage Framework for Structured Pruning of LLMs"></a>2SSP: A Two-Stage Framework for Structured Pruning of LLMs</h2><p><strong>Authors:Fabrizio Sandri, Elia Cunegatti, Giovanni Iacca</strong></p>
<p>We propose a novel Two-Stage framework for Structured Pruning (2SSP) for pruning Large Language Models (LLMs), which combines two different strategies of pruning, namely Width and Depth Pruning. The first stage (Width Pruning) removes entire neurons, hence their corresponding rows and columns, aiming to preserve the connectivity among the pruned structures in the intermediate state of the Feed-Forward Networks in each Transformer block. This is done based on an importance score measuring the impact of each neuron over the output magnitude. The second stage (Depth Pruning), instead, removes entire Attention submodules. This is done by applying an iterative process that removes the Attention submodules with the minimum impact on a given metric of interest (in our case, perplexity). We also propose a novel mechanism to balance the sparsity rate of the two stages w.r.t. to the desired global sparsity. We test 2SSP on four LLM families and three sparsity rates (25%, 37.5%, and 50%), measuring the resulting perplexity over three language modeling datasets as well as the performance over six downstream tasks. Our method consistently outperforms five state-of-the-art competitors over three language modeling and six downstream tasks, with an up to two-order-of-magnitude gain in terms of pruning time. The code is available at available at \url{<a target="_blank" rel="noopener" href="https://github.com/FabrizioSandri/2SSP%7D">https://github.com/FabrizioSandri/2SSP}</a>. </p>
<blockquote>
<p>我们提出了一种针对大型语言模型（LLM）进行结构化剪枝的新型两阶段框架（2SSP），该框架结合了两种不同剪枝策略，即宽度剪枝和深度剪枝。第一阶段（宽度剪枝）会移除整个神经元及其对应的行和列，旨在保留前馈网络中每个Transformer块中间状态的修剪结构之间的连接性。这是基于一个重要性分数进行的，该分数衡量每个神经元对输出幅度的影响。第二阶段（深度剪枝）则移除整个注意力子模块。这是通过应用一个迭代过程来完成的，该过程会移除对给定指标（在我们的案例中为困惑度）影响最小的注意力子模块。我们还提出了一种新型机制来平衡两个阶段相对于所需全局稀疏率的稀疏率。我们在四个LLM家族、三种稀疏率（25%、37.5%和50%）下测试了2SSP，测量了三个语言建模数据集上的困惑度以及六个下游任务上的性能。我们的方法在三套语言建模和六个下游任务上均优于五种最先进的竞争对手，并且在剪枝时间上取得了高达两个数量级的收益。代码可在<a target="_blank" rel="noopener" href="https://github.com/FabrizioSandri/2SSP">https://github.com/FabrizioSandri/2SSP</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.17771v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种针对大型语言模型（LLM）的两阶段结构化剪枝（2SSP）框架，结合了宽度剪枝和深度剪枝两种策略。第一阶段宽度剪枝移除整个神经元及其对应的行列，旨在保留前馈网络中剪枝结构的连通性。第二阶段深度剪枝则移除注意力子模块，通过迭代过程移除对给定指标（在本研究中为困惑度）影响最小的子模块。该框架在四个LLM家族和三种稀疏率上进行了测试，并在三个语言建模数据集和六个下游任务上进行了评估，证明其优于五种现有方法，剪枝时间提高了两个数量级。代码已公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>2SSP框架结合了宽度剪枝和深度剪枝两种策略，用于大型语言模型（LLM）的剪枝。</li>
<li>第一阶段宽度剪枝旨在保留前馈网络中剪枝结构的连通性。</li>
<li>第二阶段深度剪枝通过迭代过程移除对给定指标（如困惑度）影响最小的注意力子模块。</li>
<li>2SSP框架在多个LLM家族、稀疏率、语言建模数据集和下游任务上的表现均优于其他五种现有方法。</li>
<li>2SSP框架的剪枝时间有显著提高，达到了两个数量级的提升。</li>
<li>框架的代码已经公开，便于他人使用和研究。</li>
<li>该方法为提高LLM的效率和性能提供了新的思路。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.17771">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e9ff53e3b9c53418306bcb1c3f710173.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c1a4504874eca68c37c159bc21a808a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-48458c1f786b6bafa964fa724b6c6519.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a79dbb5a93e9a249b62b7a8d305f8190.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-902a516e35b2dc58de0930687e7e09c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0763548bb947e1a9f1aa738d6e88de65.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Hybrid-Graphs-for-Table-and-Text-based-Question-Answering-using-LLMs"><a href="#Hybrid-Graphs-for-Table-and-Text-based-Question-Answering-using-LLMs" class="headerlink" title="Hybrid Graphs for Table-and-Text based Question Answering using LLMs"></a>Hybrid Graphs for Table-and-Text based Question Answering using LLMs</h2><p><strong>Authors:Ankush Agarwal, Ganesh S, Chaitanya Devaguptapu</strong></p>
<p>Answering questions that require reasoning and aggregation across both structured (tables) and unstructured (raw text) data sources presents significant challenges. Current methods rely on fine-tuning and high-quality, human-curated data, which is difficult to obtain. Recent advances in Large Language Models (LLMs) have shown promising results for multi-hop question answering (QA) over single-source text data in a zero-shot setting, yet exploration into multi-source Table-Text QA remains limited. In this paper, we present a novel Hybrid Graph-based approach for Table-Text QA that leverages LLMs without fine-tuning. Our method constructs a unified Hybrid Graph from textual and tabular data, pruning information based on the input question to provide the LLM with relevant context concisely. We evaluate our approach on the challenging Hybrid-QA and OTT-QA datasets using state-of-the-art LLMs, including GPT-3.5, GPT-4, and LLaMA-3. Our method achieves the best zero-shot performance on both datasets, improving Exact Match scores by up to 10% on Hybrid-QA and 5.4% on OTT-QA. Moreover, our approach reduces token usage by up to 53% compared to the original context. </p>
<blockquote>
<p>回答涉及跨结构化（表格）和非结构化（原始文本）数据源推理和聚合的问题存在重大挑战。当前的方法依赖于精细调整和高质量的人工编制数据，这些数据很难获得。最近大型语言模型（LLM）的进步在单源文本数据的多跳问答（QA）中显示出有前途的结果，为零样本设置下，但对多源表文本问答的探索仍然有限。在本文中，我们提出了一种基于混合图表的表文本问答新方法，该方法利用LLM而无需精细调整。我们的方法从文本和表格数据中构建了一个统一的混合图，根据输入问题精简信息，为LLM提供简洁的上下文。我们在具有挑战性的Hybrid-QA和OTT-QA数据集上使用了最先进的LLM进行评估，包括GPT-3.5、GPT-4和LLaMA-3。我们的方法在这两个数据集上都实现了零样本的最佳性能，在Hybrid-QA上精确匹配分数提高了高达10%，在OTT-QA上提高了5.4%。此外，我们的方法与原始上下文相比，令牌使用量减少了高达53%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.17767v1">PDF</a> Accepted at NAACL 2025 Main Track</p>
<p><strong>Summary</strong></p>
<p>基于LLM的混合图方法在跨表格和文本数据源的多源问答任务中展现出卓越性能。该方法无需微调，通过构建统一混合图，将文本和表格数据整合，根据问题精简信息，为LLM提供简洁的相关上下文。在Hybrid-QA和OTT-QA数据集上的实验表明，该方法在零样本场景下表现最佳，提高Exact Match得分高达10%和5.4%。此外，该方法减少了高达53%的令牌使用量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在跨表格和文本数据源的多源问答任务中面临挑战。</li>
<li>当前方法依赖高质量的人为整理数据，获取困难。</li>
<li>最新LLM技术在零样本设置下的单源文本数据多跳问答中显示出希望。</li>
<li>提出了一种基于混合图的新方法，无需微调即可处理表文本QA。</li>
<li>该方法整合文本和表格数据，构建统一混合图。</li>
<li>根据问题精简信息，为LLM提供简洁的相关上下文。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.17767">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bd17176596b24c84e7627a4a86eede16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0094a361ac0830c20b746bb34aed6bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3896c24ce760290d4185e08944097fff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e9a6c2cd316f171fdb9b7ef1060d85c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ca62202f0641bf23bc8f1cc819b2d95.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AxBench-Steering-LLMs-Even-Simple-Baselines-Outperform-Sparse-Autoencoders"><a href="#AxBench-Steering-LLMs-Even-Simple-Baselines-Outperform-Sparse-Autoencoders" class="headerlink" title="AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse   Autoencoders"></a>AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse   Autoencoders</h2><p><strong>Authors:Zhengxuan Wu, Aryaman Arora, Atticus Geiger, Zheng Wang, Jing Huang, Dan Jurafsky, Christopher D. Manning, Christopher Potts</strong></p>
<p>Fine-grained steering of language model outputs is essential for safety and reliability. Prompting and finetuning are widely used to achieve these goals, but interpretability researchers have proposed a variety of representation-based techniques as well, including sparse autoencoders (SAEs), linear artificial tomography, supervised steering vectors, linear probes, and representation finetuning. At present, there is no benchmark for making direct comparisons between these proposals. Therefore, we introduce AxBench, a large-scale benchmark for steering and concept detection, and report experiments on Gemma-2-2B and 9B. For steering, we find that prompting outperforms all existing methods, followed by finetuning. For concept detection, representation-based methods such as difference-in-means, perform the best. On both evaluations, SAEs are not competitive. We introduce a novel weakly-supervised representational method (Rank-1 Representation Finetuning; ReFT-r1), which is competitive on both tasks while providing the interpretability advantages that prompting lacks. Along with AxBench, we train and publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean. </p>
<blockquote>
<p>精细控制语言模型输出对于安全和可靠性至关重要。提示和微调被广泛用于实现这些目标，但解释性研究人员也提出了各种基于表示的技术，包括稀疏自动编码器（SAE）、线性人工层析成像、监督控制向量、线性探针和表示微调。目前，还没有对这些提议进行直接比较的基准测试。因此，我们引入了AxBench，这是一个用于控制和概念检测的大规模基准测试，并报告了在Gemma-2-2B和9B上的实验。对于控制任务，我们发现提示表现优于所有现有方法，其次是微调。在概念检测方面，基于表示的方法（如均值差异法）表现最好。在这两项评估中，SAE都不具备竞争力。我们引入了一种新型的弱监督表示方法（Rank-1表示微调；ReFT-r1），它在两个任务上都表现良好，同时提供了提示所缺乏的解读优势。我们训练了用于ReFT-r1和DiffMean的SAE规模特征字典并公开发布，以便与AxBench一起使用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.17148v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了语言模型输出精细控制的重要性和方法，包括提示、微调等常用方法以及基于表示的技术，如稀疏自动编码器（SAE）、线性人工断层扫描等。由于缺乏直接比较这些方法的基准测试，作者引入了AxBench基准测试，用于控制和概念检测。实验表明，提示在所有现有方法中表现最佳，其次是微调。在概念检测方面，基于表示的方法如差异均值表现最佳。作者还提出了一种新型的弱监督表示方法——ReFT-r1，在两项任务中均表现出竞争力，同时提供了提示所缺乏的可解释性优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语言模型输出的精细控制对于安全和可靠性至关重要。</li>
<li>现有方法如提示和微调被广泛用于实现这一目标。</li>
<li>基于表示的技术，如稀疏自动编码器（SAE）和线性人工断层扫描也被用于控制语言模型输出。</li>
<li>目前缺乏直接比较这些方法的基准测试。</li>
<li>引入AxBench基准测试用于控制和概念检测。</li>
<li>在实验评估中，提示在现有方法中表现最佳，其次是微调。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.17148">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0f4bd11a1c19faccfbf7aa2f5e086300.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-867354169710a72b5c66e2fa4783d5b7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TAID-Temporally-Adaptive-Interpolated-Distillation-for-Efficient-Knowledge-Transfer-in-Language-Models"><a href="#TAID-Temporally-Adaptive-Interpolated-Distillation-for-Efficient-Knowledge-Transfer-in-Language-Models" class="headerlink" title="TAID: Temporally Adaptive Interpolated Distillation for Efficient   Knowledge Transfer in Language Models"></a>TAID: Temporally Adaptive Interpolated Distillation for Efficient   Knowledge Transfer in Language Models</h2><p><strong>Authors:Makoto Shing, Kou Misaki, Han Bao, Sho Yokoi, Takuya Akiba</strong></p>
<p>Causal language models have demonstrated remarkable capabilities, but their size poses significant challenges for deployment in resource-constrained environments. Knowledge distillation, a widely-used technique for transferring knowledge from a large teacher model to a small student model, presents a promising approach for model compression. A significant remaining issue lies in the major differences between teacher and student models, namely the substantial capacity gap, mode averaging, and mode collapse, which pose barriers during distillation. To address these issues, we introduce $\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel knowledge distillation approach that dynamically interpolates student and teacher distributions through an adaptive intermediate distribution, gradually shifting from the student’s initial distribution towards the teacher’s distribution. We provide a theoretical analysis demonstrating TAID’s ability to prevent mode collapse and empirically show its effectiveness in addressing the capacity gap while balancing mode averaging and mode collapse. Our comprehensive experiments demonstrate TAID’s superior performance across various model sizes and architectures in both instruction tuning and pre-training scenarios. Furthermore, we showcase TAID’s practical impact by developing two state-of-the-art compact foundation models: $\texttt{TAID-LLM-1.5B}$ for language tasks and $\texttt{TAID-VLM-2B}$ for vision-language tasks. These results demonstrate TAID’s effectiveness in creating high-performing and efficient models, advancing the development of more accessible AI technologies. </p>
<blockquote>
<p>因果语言模型已经展现出显著的能力，但它们的规模对在资源受限环境中的部署构成了重大挑战。知识蒸馏是一种广泛使用的技术，可以从大型教师模型转移到小型学生模型，这为模型压缩提供了有前景的方法。一个主要的剩余问题在于教师和学生模型之间的巨大差异，即显著的能力差距、模式平均和模式崩溃，这些在蒸馏过程中构成了障碍。为了解决这些问题，我们引入了”时间自适应插值蒸馏法（TAID）”，这是一种新型的知识蒸馏方法，通过自适应中间分布动态插值学生和教师的分布，从学生的初始分布逐渐转向教师的分布。我们提供了理论分析，证明了TAID在防止模式崩溃方面的能力，并通过经验证明了它在解决能力差距、平衡模式平均和模式崩溃方面的有效性。我们的综合实验表明，TAID在各种模型和架构的大小、指令调整和预训练场景中均表现出卓越的性能。此外，我们通过开发两个最先进的紧凑基础模型：用于语言任务的”TAID-LLM-1.5B”和用于视觉语言任务的”TAID-VLM-2B”，展示了TAID的实际影响。这些结果证明了TAID在创建高性能和高效模型方面的有效性，推动了更可访问的AI技术的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16937v2">PDF</a> To appear at the 13th International Conference on Learning   Representations (ICLR 2025)</p>
<p><strong>Summary</strong></p>
<p>知识蒸馏是一种从大模型到小模型的转移知识技术，但仍存在显著的问题，例如教师与学生模型间巨大的能力差距、模式平均化和模式崩溃。为此，本文提出一种新的知识蒸馏方法——暂时自适应插值蒸馏（TAID），它通过自适应中间分布动态插值学生和教师的分布，逐步从学生初始分布转向教师分布。本文理论分析了TAID防止模式崩溃的能力，并通过实验验证了其在解决能力差距、平衡模式平均和模式崩溃方面的有效性。此外，TAID还在多种模型和架构的大小、预训练场景以及开发两个紧凑基础模型TAID-LLM-1.5B和TAID-VLM-2B中展现出卓越性能。这些成果证明了TAID在创建高性能、高效模型方面的有效性，推动了更普及的AI技术的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>知识蒸馏在模型压缩中具有潜力，但面临教师和学生模型间的能力差距、模式平均化和模式崩溃等问题。</li>
<li>引入新的知识蒸馏方法——暂时自适应插值蒸馏（TAID），通过动态插值学生和教师的分布来解决上述问题。</li>
<li>TAID具有防止模式崩溃的理论分析能力，并通过实验验证了其在解决能力差距方面的有效性。</li>
<li>TAID在多种模型和架构中表现优越，适用于指令调优和预训练场景。</li>
<li>开发两个紧凑基础模型TAID-LLM-1.5B和TAID-VLM-2B，为语言任务和视觉语言任务提供高性能解决方案。</li>
<li>TAID创造了高效模型，推动了AI技术的发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16937">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-67e8f2ff8c16e8a039f184c5170e52c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d19b6b370cd647f9e7364ec50c82a4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65bc89ef8169783c6875b2aeb18c91a1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Online-Prompt-Selection-for-Program-Synthesis"><a href="#Online-Prompt-Selection-for-Program-Synthesis" class="headerlink" title="Online Prompt Selection for Program Synthesis"></a>Online Prompt Selection for Program Synthesis</h2><p><strong>Authors:Yixuan Li, Lewis Frampton, Federico Mora, Elizabeth Polgreen</strong></p>
<p>Large Language Models (LLMs) demonstrate impressive capabilities in the domain of program synthesis. This level of performance is not, however, universal across all tasks, all LLMs and all prompting styles. There are many areas where one LLM dominates, one prompting style dominates, or where calling a symbolic solver is a better choice than an LLM. A key challenge for the user then, is to identify not only when an LLM is the right choice of solver, and the appropriate LLM to call for a given synthesis task, but also the right way to call it. A non-expert user who makes the wrong choice, incurs a cost both in terms of results (number of tasks solved, and the time it takes to solve them) and financial cost, if using a closed-source language model via a commercial API. We frame this choice as an online learning problem. We use a multi-armed bandit algorithm to select which symbolic solver, or LLM and prompt combination to deploy in order to maximize a given reward function (which may prioritize solving time, number of synthesis tasks solved, or financial cost of solving). We implement an instance of this approach, called CYANEA, and evaluate it on synthesis queries from the literature in ranking function synthesis, from the syntax-guided synthesis competition, and fresh, unseen queries generated from SMT problems. CYANEA solves 37.2% more queries than the best single solver and achieves results within 4% of the virtual best solver. </p>
<blockquote>
<p>大型语言模型（LLM）在程序合成领域展现出了令人印象深刻的能力。然而，这种表现并非在所有任务、所有LLM以及所有提示风格中都是普遍的。在许多领域，一个LLM会占据主导，一种提示风格会占据主导，或者调用符号求解器是一个比LLM更好的选择。因此，用户面临的关键挑战是，不仅要识别何时选择LLM作为求解器以及针对给定的合成任务调用哪个LLM是恰当的，而且还要以正确的方式调用它。非专家用户如果做出了错误的选择，不仅会从结果（解决的任务数量和所需的时间）上付出代价，如果使用商业API的闭源语言模型，还会产生经济成本。我们将这种选择框架作为一个在线学习问题。我们使用多臂老虎机算法来选择要部署的符号求解器或LLM和提示组合，以最大化给定的奖励函数（可能优先考虑解决时间、解决的任务数量或解决的经济成本）。我们实现了这种方法的一个实例，称为CYANEA，并对其在排名函数合成中的合成查询、语法指导的合成竞赛以及来自SMT问题的新鲜未见过的查询进行了评估。CYANEA比最佳单一求解器解决了37.2%以上的查询，并取得了与虚拟最佳求解器相差4%的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05247v2">PDF</a> Accepted at the 39th AAAI Conference on Artificial Intelligence   (AAAI-25) Main Track</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在程序合成领域表现出令人印象深刻的能力，但其性能并非普遍适用于所有任务、所有LLM和所有提示风格。关键挑战在于用户需要识别何时使用LLM作为求解器是最佳选择，以及如何正确调用适当的LLM来完成给定的合成任务。我们将其选择作为一个在线学习问题，并使用多臂老虎机算法来选择部署哪种符号求解器或LLM和提示组合，以最大化给定的奖励函数。我们实施了一种称为CYANEA的方法，并在排名函数合成、语法指导合成竞赛以及由SMT问题生成的新鲜未见查询上对其进行了评估。CYANEA比最佳单一求解器解决了更多的查询问题，并取得了接近虚拟最佳求解器的结果。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM在程序合成领域具有显著能力，但性能因任务、LLM和提示风格而异。</li>
<li>用户面临的挑战是识别何时以及如何使用LLM进行最佳求解。</li>
<li>将此选择视为在线学习问题，使用多臂老虎机算法进行优化。</li>
<li>CYANEA方法实施用于选择适当的求解器或LLM及提示组合。</li>
<li>CYANEA在多个评估中表现出优于单一求解器的性能，接近虚拟最佳求解器。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05247">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ebf0aa281d9db16cb4cd25ba026a9b45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-926b0a4c0eb84adae1f6b0b690265944.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07e5ab2ad110d0ff87a298ea14fa8ed3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b644b64cefde02e61f0d474c29de8c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8788e1f9c52b477f9dce3e2f7131be1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Planning-Anything-with-Rigor-General-Purpose-Zero-Shot-Planning-with-LLM-based-Formalized-Programming"><a href="#Planning-Anything-with-Rigor-General-Purpose-Zero-Shot-Planning-with-LLM-based-Formalized-Programming" class="headerlink" title="Planning Anything with Rigor: General-Purpose Zero-Shot Planning with   LLM-based Formalized Programming"></a>Planning Anything with Rigor: General-Purpose Zero-Shot Planning with   LLM-based Formalized Programming</h2><p><strong>Authors:Yilun Hao, Yang Zhang, Chuchu Fan</strong></p>
<p>While large language models (LLMs) have recently demonstrated strong potential in solving planning problems, there is a trade-off between flexibility and complexity. LLMs, as zero-shot planners themselves, are still not capable of directly generating valid plans for complex planning problems such as multi-constraint or long-horizon tasks. On the other hand, many frameworks aiming to solve complex planning problems often rely on task-specific preparatory efforts, such as task-specific in-context examples and pre-defined critics&#x2F;verifiers, which limits their cross-task generalization capability. In this paper, we tackle these challenges by observing that the core of many planning problems lies in optimization problems: searching for the optimal solution (best plan) with goals subject to constraints (preconditions and effects of decisions). With LLMs’ commonsense, reasoning, and programming capabilities, this opens up the possibilities of a universal LLM-based approach to planning problems. Inspired by this observation, we propose LLMFP, a general-purpose framework that leverages LLMs to capture key information from planning problems and formally formulate and solve them as optimization problems from scratch, with no task-specific examples needed. We apply LLMFP to 9 planning problems, ranging from multi-constraint decision making to multi-step planning problems, and demonstrate that LLMFP achieves on average 83.7% and 86.8% optimal rate across 9 tasks for GPT-4o and Claude 3.5 Sonnet, significantly outperforming the best baseline (direct planning with OpenAI o1-preview) with 37.6% and 40.7% improvements. We also validate components of LLMFP with ablation experiments and analyzed the underlying success and failure reasons. Project page: <a target="_blank" rel="noopener" href="https://sites.google.com/view/llmfp">https://sites.google.com/view/llmfp</a>. </p>
<blockquote>
<p>虽然大型语言模型（LLM）最近在解决规划问题方面展现出了强大的潜力，但在灵活性和复杂性之间存在权衡。作为零启动规划者本身，LLM仍然无法直接为复杂的规划问题（如多约束或长期任务）生成有效的计划。另一方面，许多旨在解决复杂规划问题的框架通常依赖于特定任务的准备性工作，例如特定于任务的上下文示例和预先定义的评论家&#x2F;验证器，这限制了它们的跨任务泛化能力。在本文中，我们通过观察许多规划问题的核心在于优化问题来解决这些挑战：在目标受到约束（决策的先决条件和影响）的情况下搜索最优解（最佳计划）。利用LLM的常识、推理和编程能力，这为基于LLM的通用规划方法打开了可能性。受此观察的启发，我们提出了LLMFP，这是一个通用框架，它利用LLM从规划问题中提取关键信息，并将其正式制定为优化问题并解决，无需特定任务的示例。我们将LLMFP应用于9个规划问题，从多约束决策制定到多步骤规划问题，并证明LLMFP在GPT-4o和Claude 3.5 Sonnet上平均达到了83.7%和86.8%的最优率，显著优于最佳基线（使用OpenAI o1-preview的直接规划方法），提高了37.6%和40.7%。我们还通过消融实验验证了LLMFP的组件，并分析了其成功和失败的根本原因。项目页面：<a target="_blank" rel="noopener" href="https://sites.google.com/view/llmfp%E3%80%82">https://sites.google.com/view/llmfp。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12112v2">PDF</a> 57 pages, 25 figures, 15 tables</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在解决规划问题上展现了潜力，但在灵活性与复杂性之间存在权衡。LLM作为零规划者，尚不能直接为复杂规划问题生成有效方案。现有框架常依赖特定任务的预备工作，限制了跨任务泛化能力。本研究观察到规划问题的核心在于优化问题，并提出LLMFP框架，利用LLM捕捉规划问题的关键信息，并将其形式化为优化问题。在多个任务上的实验显示，LLMFP显著优于基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在解决规划问题时存在灵活性与复杂性之间的权衡。</li>
<li>LLM尚不能直接生成复杂规划问题的有效方案。</li>
<li>现有框架解决复杂规划问题常依赖任务特定预备工作，限制了其跨任务泛化能力。</li>
<li>规划问题的核心在于寻找满足约束的最优解决方案。</li>
<li>LLMFP框架利用LLM的常识、推理和编程能力，以通用方式解决规划问题。</li>
<li>LLMFP在多个任务上的实验表现优于基线方法。</li>
<li>LLMFP的成功与失败原因得到了实验验证和分析。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.12112">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d0b7f0a26b569ed599b86d26357bbec5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4203b4f39d4e4fdeb98331ef7d3f8b87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cde2dc88f73f44617ade54956f161167.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ATTNChecker-Highly-Optimized-Fault-Tolerant-Attention-for-Large-Language-Model-Training"><a href="#ATTNChecker-Highly-Optimized-Fault-Tolerant-Attention-for-Large-Language-Model-Training" class="headerlink" title="ATTNChecker: Highly-Optimized Fault Tolerant Attention for Large   Language Model Training"></a>ATTNChecker: Highly-Optimized Fault Tolerant Attention for Large   Language Model Training</h2><p><strong>Authors:Yuhang Liang, Xinyi Li, Jie Ren, Ang Li, Bo Fang, Jieyang Chen</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable performance in various natural language processing tasks. However, the training of these models is computationally intensive and susceptible to faults, particularly in the attention mechanism, which is a critical component of transformer-based LLMs. In this paper, we investigate the impact of faults on LLM training, focusing on INF, NaN, and near-INF values in the computation results with systematic fault injection experiments. We observe the propagation patterns of these errors, which can trigger non-trainable states in the model and disrupt training, forcing the procedure to load from checkpoints. To mitigate the impact of these faults, we propose ATTNChecker, the first Algorithm-Based Fault Tolerance (ABFT) technique tailored for the attention mechanism in LLMs. ATTNChecker is designed based on fault propagation patterns of LLM and incorporates performance optimization to adapt to both system reliability and model vulnerability while providing lightweight protection for fast LLM training. Evaluations on four LLMs show that ATTNChecker incurs on average 7% overhead on training while detecting and correcting all extreme errors. Compared with the state-of-the-art checkpoint&#x2F;restore approach, ATTNChecker reduces recovery overhead by up to 49x. </p>
<blockquote>
<p>大型语言模型（LLM）在各种自然语言处理任务中表现出了卓越的性能。然而，这些模型的训练计算密集且容易出错，特别是在基于转换器的LLM中起到关键作用的注意力机制。在本文中，我们研究了故障对LLM训练的影响，重点关注计算结果中的无穷大（INF）、非数字（NaN）和接近无穷大的值，并通过系统的故障注入实验进行研究。我们观察了这些错误的传播模式，这些错误可能会触发模型中的不可训练状态并中断训练，迫使程序从检查点加载。为了减轻这些故障的影响，我们提出了ATTNChecker，这是第一种针对LLM中注意力机制的基于算法的容错（ABFT）技术。ATTNChecker的设计基于LLM的故障传播模式，并融入了性能优化，以适应系统可靠性和模型脆弱性，同时为快速LLM训练提供轻量级保护。对四种LLM的评估表明，ATTNChecker在训练过程中的平均开销为7%，同时能够检测和纠正所有极端错误。与最新的检查点&#x2F;恢复方法相比，ATTNChecker将恢复开销减少了高达49倍。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11720v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在多个自然语言处理任务中展现出卓越性能，但其训练计算量大且易出错，特别是基于transformer的LLM中的注意力机制。本文通过故障注入实验研究了故障对LLM训练的影响，并观察到错误传播模式可能导致模型不可训练状态并中断训练。为缓解这些故障的影响，本文提出了针对LLM注意力机制的算法级容错技术ATTNChecker。ATTNChecker根据LLM的故障传播模式设计，融入了性能优化以适应系统可靠性和模型脆弱性，同时为快速LLM训练提供轻量级保护。评估显示，ATTNChecker平均训练开销为7%，可检测并修正所有极端错误。相较于当前先进的检查点恢复方法，ATTNChecker恢复开销降低了高达49倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在自然语言处理任务中具有出色表现，但其训练计算量大且存在易错点，特别是在注意力机制方面。</li>
<li>故障注入实验用于研究故障对LLM训练的影响，揭示了错误传播模式可能导致模型进入不可训练状态。</li>
<li>ATTNChecker是首个针对LLM注意力机制的算法级容错技术，旨在减轻故障对训练的影响。</li>
<li>ATTNC福CKER基于LLM的故障传播模式设计，融合了性能优化以适应系统可靠性和模型脆弱性。</li>
<li>ATTNC福CKER提供了轻量级的保护，以确保快速LLM训练。</li>
<li>评估显示，ATTNChecker在训练过程中的平均开销为7%，并能有效检测及修正所有极端错误。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.11720">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-56d961e8cdd081e3cdd7739086eeb59a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2eefce84363c239d8e6eac3d037cd4aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f57e62ea90b9d63edd7c3df2bd679f34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-899c1f7b0f51210def275e2cffe56f3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f3c1eb1925551b78084a8a9c44b6c50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fecdd928d97b0e62c33c1292a1afaf25.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-Can-Solve-Real-World-Planning-Rigorously-with-Formal-Verification-Tools"><a href="#Large-Language-Models-Can-Solve-Real-World-Planning-Rigorously-with-Formal-Verification-Tools" class="headerlink" title="Large Language Models Can Solve Real-World Planning Rigorously with   Formal Verification Tools"></a>Large Language Models Can Solve Real-World Planning Rigorously with   Formal Verification Tools</h2><p><strong>Authors:Yilun Hao, Yongchao Chen, Yang Zhang, Chuchu Fan</strong></p>
<p>Large Language Models (LLMs) struggle to directly generate correct plans for complex multi-constraint planning problems, even with self-verification and self-critique. For example, a U.S. domestic travel planning benchmark TravelPlanner was proposed in Xie et al. (2024), where the best LLM OpenAI o1-preview can only find viable travel plans with a 10% success rate given all needed information. In this work, we tackle this by proposing an LLM-based planning framework that formalizes and solves complex multi-constraint planning problems as constrained satisfiability problems, which are further consumed by sound and complete satisfiability solvers. We start with TravelPlanner as the primary use case and show that our framework achieves a success rate of 93.9% and is effective with diverse paraphrased prompts. More importantly, our framework has strong zero-shot generalizability, successfully handling unseen constraints in our newly created unseen international travel dataset and generalizing well to new fundamentally different domains. Moreover, when user input queries are infeasible, our framework can identify the unsatisfiable core, provide failure reasons, and offers personalized modification suggestions. We show that our framework can modify and solve for an average of 81.6% and 91.7% unsatisfiable queries from two datasets and prove with ablations that all key components of our framework are effective and necessary. Project page: <a target="_blank" rel="noopener" href="https://sites.google.com/view/llm-rwplanning">https://sites.google.com/view/llm-rwplanning</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在解决复杂的、具有多重约束的规划问题时，即使在具备自我验证和自批能力的情况下，也难以直接生成正确的规划方案。例如，在谢等人提出的美国国内旅行规划基准测试TravelPlanner中（Xie et al., 2024），即使提供所有必要信息，最佳LLM OpenAI o1-preview也只能以10%的成功率找到可行的旅行计划。在这项工作中，我们通过提出一个基于LLM的规划框架来解决这个问题，该框架将复杂的、具有多重约束的规划问题形式化为约束可满足性问题，并进一步通过健全和完整的可满足性求解器进行求解。我们以TravelPlanner作为主要用例进行展示，证明我们的框架成功率达到了93.9%，并在多样化的同义换词提示下取得了良好效果。更重要的是，我们的框架具有较强的零样本泛化能力，能够成功处理我们新创建的未见国际旅行数据集中所包含的新增未见约束，并在新的根本不同领域进行良好的泛化。此外，当用户输入的查询不可行时，我们的框架可以识别出不满足要求的核心部分、提供失败原因以及个性化的修改建议。我们展示，我们的框架可以对两个数据集的不可满足查询进行平均达到81.6%和91.7%的修改并解决，并通过消融实验证明我们框架的所有关键组成部分都是有效且必要的。项目页面：<a target="_blank" rel="noopener" href="https://sites.google.com/view/llm-rwplanning%E3%80%82">https://sites.google.com/view/llm-rwplanning。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.11891v3">PDF</a> 50 pages, 6 figures, 8 tables</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在处理复杂的带有多种约束的规划问题时，即便具备自我验证与自我批判的能力，仍难以直接生成正确的计划。例如，在Xie等人提出的TravelPlanner美国国内旅行规划基准测试中，最佳LLM OpenAI o1-preview即使获得所有必要信息，也只能以10%的成功率找到可行的旅行计划。本研究通过提出一个基于LLM的规划框架来解决这一问题，该框架将复杂的带有多种约束的规划问题形式化为约束可满足性问题，并使用健全且完整的可满足性求解器来解决。以TravelPlanner为主要用例，我们的框架成功率为93.9%，并在多样化的改述提示中表现出良好的效果。更重要的是，我们的框架具有很强的零样本泛化能力，能够成功处理我们新创建的无约束国际旅行数据集，并在全新且根本不同的领域实现良好的泛化。此外，在用户输入的查询不可行时，我们的框架可以识别出不满足核心条件的原因，并提供个性化的修改建议。实验证明，我们的框架平均可以解决81.6%和91.7%的来自两个数据集的不满足要求的查询，并通过分析证明框架的所有关键组成部分都是有效且必要的。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在处理复杂的多约束规划问题时存在困难，直接生成正确计划的成功率较低。</li>
<li>提出一个基于LLM的规划框架，将复杂的多约束规划问题形式化为约束可满足性问题并求解。</li>
<li>以TravelPlanner为主要用例，框架成功率高，并表现出良好的泛化能力。</li>
<li>框架具备零样本泛化能力，可处理未见过的国际旅行数据集。</li>
<li>当用户查询不可行时，框架能识别核心问题并提供修改建议。</li>
<li>框架对不满足要求的查询具有较高的解决率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.11891">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5f996bfbd428a5dfe1ef7d03f7414faf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6929c5740a12dc68b6973921b49634ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ccd2e78238256e079dedbc89e9f248ac.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-31/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-31/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-31/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-40fd698afd250054ea2b435a592027fa.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-01-31  Actions Speak Louder than Words Agent Decisions Reveal Implicit Biases   in Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-31
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-30/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e9431ea423467aa86ee279c2aa5c72f7.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion 方向最新论文已更新，请持续关注 Update in 2025-01-30  PackDiT Joint Human Motion and Text Generation via Mutual Prompting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">15444.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
