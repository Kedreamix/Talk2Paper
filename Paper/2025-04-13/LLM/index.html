<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-04-13  An Adversarial Perspective on Machine Unlearning for AI Safety">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-461a70fadb968159e1382603dad2aba5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    21 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-13-更新"><a href="#2025-04-13-更新" class="headerlink" title="2025-04-13 更新"></a>2025-04-13 更新</h1><h2 id="An-Adversarial-Perspective-on-Machine-Unlearning-for-AI-Safety"><a href="#An-Adversarial-Perspective-on-Machine-Unlearning-for-AI-Safety" class="headerlink" title="An Adversarial Perspective on Machine Unlearning for AI Safety"></a>An Adversarial Perspective on Machine Unlearning for AI Safety</h2><p><strong>Authors:Jakub Łucki, Boyi Wei, Yangsibo Huang, Peter Henderson, Florian Tramèr, Javier Rando</strong></p>
<p>Large language models are finetuned to refuse questions about hazardous knowledge, but these protections can often be bypassed. Unlearning methods aim at completely removing hazardous capabilities from models and make them inaccessible to adversaries. This work challenges the fundamental differences between unlearning and traditional safety post-training from an adversarial perspective. We demonstrate that existing jailbreak methods, previously reported as ineffective against unlearning, can be successful when applied carefully. Furthermore, we develop a variety of adaptive methods that recover most supposedly unlearned capabilities. For instance, we show that finetuning on 10 unrelated examples or removing specific directions in the activation space can recover most hazardous capabilities for models edited with RMU, a state-of-the-art unlearning method. Our findings challenge the robustness of current unlearning approaches and question their advantages over safety training. </p>
<blockquote>
<p>大型语言模型经过微调拒绝涉及危险知识的问题，但这些保护往往可以被绕过。遗忘方法旨在从模型中完全移除危险能力，使其无法被对手访问。本文从一个对抗性的角度，挑战了遗忘和传统安全后训练之间的根本差异。我们证明，之前被认为对遗忘无效的越狱方法，在谨慎应用时是可以成功的。此外，我们开发了各种自适应方法，恢复了大多数所谓的遗忘能力。例如，我们展示了在10个无关的例子上进行微调或移除激活空间中的特定方向，可以恢复使用最新遗忘方法RMU编辑的模型的大部分危险能力。我们的发现对当前遗忘方法的稳健性提出了挑战，并对其在安全训练方面的优势提出了质疑。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18025v5">PDF</a> Final version published in Transactions on Machine Learning Research   (TMLR); Best technical paper at Neurips 2024 SoLaR workshop</p>
<p><strong>Summary</strong>：大型语言模型通过微调拒绝涉及危险知识的问题，但这些保护往往可以被绕过。本研究挑战了从对抗性角度看待遗忘和传统安全训练之间的差异。我们证明，先前被认为对遗忘无效的越狱方法，在谨慎应用时可能会成功。此外，我们还开发了一系列自适应方法，可以恢复大多数被认为已经遗忘的能力。我们的发现对现有的遗忘方法提出了挑战，并对其在安全训练方面的优势提出了质疑。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型会拒绝涉及危险知识的问题，但这并非绝对安全，存在绕过措施的可能性。</li>
<li>存在针对模型能力遗忘的方法被质疑的验证，并从对抗性角度探讨了这些方法的有效性。</li>
<li>现存一些对遗忘无效的策略可以通过仔细应用来实现成功越狱效果。例如我们开发的一系列自适应方法成功恢复了大多数被认为已经遗忘的能力。这表明现有的遗忘方法可能并不稳健。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.18025">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fd9e61708583813d101812c1adc08070.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4bdbddc32fbfe9399dfb582a36b3f641.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5825b725ddded1c04aca6cdb51e82cc4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f213b1e49fad400461401f1aa5a3c76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-abaf54356455feb1dc0908e916093912.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Flash-STU-Fast-Spectral-Transform-Units"><a href="#Flash-STU-Fast-Spectral-Transform-Units" class="headerlink" title="Flash STU: Fast Spectral Transform Units"></a>Flash STU: Fast Spectral Transform Units</h2><p><strong>Authors:Y. Isabel Liu, Windsor Nguyen, Yagiz Devre, Evan Dogariu, Anirudha Majumdar, Elad Hazan</strong></p>
<p>Recent advances in state-space model architectures have shown great promise for efficient sequence modeling, but challenges remain in balancing computational efficiency with model expressiveness. We propose the Flash STU architecture, a hybrid model that interleaves spectral state space model layers with sliding window attention, enabling scalability to billions of parameters for language modeling while maintaining a near-linear time complexity. We evaluate the Flash STU and its variants on diverse sequence prediction tasks, including linear dynamical systems, robotics control, and language modeling. We find that, given a fixed parameter budget, the Flash STU architecture consistently outperforms the Transformer and other leading state-space models such as S4 and Mamba-2. </p>
<blockquote>
<p>近期状态空间模型架构的进展为高效序列建模展现出了巨大的潜力，但在平衡计算效率和模型表现力方面仍存在挑战。我们提出了Flash STU架构，这是一种混合模型，它交替使用谱状态空间模型层和滑动窗口注意力，能够在语言建模中扩展到数十亿参数，同时保持接近线性的时间复杂度。我们在各种序列预测任务上评估了Flash STU及其变体，包括线性动力系统、机器人控制和语言建模。我们发现，在固定的参数预算下，Flash STU架构始终优于Transformer以及其他的领先状态空间模型，如S4和Mamba-2。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.10489v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期状态空间模型架构的进展为高效序列建模展现了巨大潜力，但仍需在计算效率和模型表达能力之间取得平衡。本文提出了Flash STU架构，这是一种混合模型，通过交织谱状态空间模型层和滑动窗口注意力，实现了对数十亿参数的扩展性，同时保持了近线性时间复杂度。在多种序列预测任务上评估Flash STU及其变体，包括线性动力系统、机器人控制和语言建模。在固定参数预算下，Flash STU架构始终优于Transformer和其他领先的状态空间模型，如S4和Mamba-2。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>状态空间模型架构的近期进展展现了高效序列建模的潜力。</li>
<li>Flash STU架构是一种混合模型，结合了谱状态空间模型层和滑动窗口注意力。</li>
<li>Flash STU架构能够实现数十亿参数的扩展性，并保持近线性时间复杂度。</li>
<li>在多种序列预测任务上评估了Flash STU及其变体，包括线性动力系统、机器人控制和语言建模。</li>
<li>Flash STU架构在固定参数预算下表现出优异的性能。</li>
<li>Flash STU架构的性能优于其他领先的模型和架构，如Transformer、S4和Mamba-2。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.10489">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-68edabbeded540fbb9c8c0c627205301.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5019955552352ac2c2ede3e5552a8929.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45838e588f37f2d4e7bcab1d59109256.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92c7c3d02cfc80563e34539aa0c29a10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f29820bedbc72cd2472daae46d2ded20.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40775f7cf3bd583f6cae511f859fa11f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1576db5d00b1825972f77976aba656e3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Demystifying-Language-Model-Forgetting-with-Low-rank-Example-Associations"><a href="#Demystifying-Language-Model-Forgetting-with-Low-rank-Example-Associations" class="headerlink" title="Demystifying Language Model Forgetting with Low-rank Example   Associations"></a>Demystifying Language Model Forgetting with Low-rank Example   Associations</h2><p><strong>Authors:Xisen Jin, Xiang Ren</strong></p>
<p>Large Language models (LLMs) suffer from forgetting of upstream data when fine-tuned. Despite efforts on mitigating forgetting, few have investigated whether, and how forgotten upstream examples are dependent on newly learned tasks. Insights on such dependencies enable efficient and targeted mitigation of forgetting. In this paper, we empirically analyze forgetting that occurs in $N$ upstream examples of language modeling or instruction-tuning after fine-tuning LLMs on one of $M$ new tasks, visualized in $M\times N$ matrices. We show that the matrices are often well-approximated with low-rank matrices, indicating the dominance of simple associations between the learned tasks and forgotten upstream examples. Leveraging the analysis, we predict forgetting of upstream examples when fine-tuning on unseen tasks with matrix completion over the empirical associations. This enables fast identification of most forgotten examples without expensive inference on the entire upstream data. The approach, despite simplicity, outperforms prior approaches that learn semantic relationships of learned tasks and upstream examples with LMs for predicting forgetting. We demonstrate the practical utility of our analysis by showing statistically significantly reduced forgetting as we upweight predicted examples for replay at fine-tuning. Project page: <a target="_blank" rel="noopener" href="https://inklab.usc.edu/lm-forgetting-prediction/">https://inklab.usc.edu/lm-forgetting-prediction/</a> </p>
<blockquote>
<p>大型语言模型（LLM）在微调时会遗忘上游数据。尽管有人努力减轻遗忘，但很少有人研究遗忘的上游示例是否以及如何依赖于新学习的任务。关于这些依赖性的见解有助于有效且有针对性地缓解遗忘。在本文中，我们对在一个新的M任务上微调LLM后发生的N个上游语言建模或指令调整的示例中的遗忘进行了实证分析，并在$M\times N$矩阵中进行可视化。我们发现这些矩阵通常可以用低阶矩阵近似，这表明已学习到的任务和遗忘的上游示例之间的简单关联占主导地位。利用这一分析，我们通过在经验关联上应用矩阵补全来预测在未见任务上进行微调时的上游示例遗忘。这可以在无需对整个上游数据进行昂贵推理的情况下快速识别出最被遗忘的示例。尽管我们的方法很简单，但它优于先前的方法，后者使用语言模型来学习已学任务和上游示例之间的语义关系来预测遗忘。我们通过展示在微调时通过回放预测的例子来减少遗忘，从而证明了我们的分析的实际效用。项目页面：<a target="_blank" rel="noopener" href="https://inklab.usc.edu/lm-forgetting-prediction/">https://inklab.usc.edu/lm-forgetting-prediction/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14026v5">PDF</a> 8 pages; preprint, fixed Table 5 in Appendix D</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在微调时会遗忘上游数据。本文实证分析了在微调LLM后，对上游数据中的语言建模或指令调优的遗忘情况。分析显示，这些数据的遗忘可以通过低秩矩阵近似来预测，并指出新任务与遗忘的上游数据之间的简单关联。利用这一分析，我们可以在微调未见任务时预测上游数据的遗忘，通过矩阵补全实证关联来实现预测。此方法虽然简单，但优于使用语言模型学习新任务和上游数据语义关系的方法。通过重播预测示例并在微调时加权，我们展示了分析的实际效用，显著减少了遗忘。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在微调时会遗忘上游数据。</li>
<li>通过低秩矩阵近似预测遗忘情况。</li>
<li>新任务和遗忘的上游数据间存在简单关联。</li>
<li>使用矩阵补全实证关联实现预测。</li>
<li>此方法虽简单但优于其他预测遗忘的方法。</li>
<li>通过重播预测示例并在微调时加权，显著减少遗忘。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14026">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8ce24c00474706f1a077b227748ad086.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd249bb0d6b5b3e5ed6f9d9bcfb16fce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b36019530b6e976f7e21e03b8718db74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0c7fe028f5de826dc20f779e16df0e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c7ad26b8fba8b43b39e61fddc5802f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-440967d431f24aa5a4324cfc1d684f85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-076de3fa9b49e4d1259254ef4466c1ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a8714858485c9fc5f2b25cf0b58b94b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="The-Right-Time-Matters-Data-Arrangement-Affects-Zero-Shot-Generalization-in-Instruction-Tuning"><a href="#The-Right-Time-Matters-Data-Arrangement-Affects-Zero-Shot-Generalization-in-Instruction-Tuning" class="headerlink" title="The Right Time Matters: Data Arrangement Affects Zero-Shot   Generalization in Instruction Tuning"></a>The Right Time Matters: Data Arrangement Affects Zero-Shot   Generalization in Instruction Tuning</h2><p><strong>Authors:Bingxiang He, Ning Ding, Cheng Qian, Jia Deng, Ganqu Cui, Lifan Yuan, Haiwen Hong, Huan-ang Gao, Longtao Huang, Hui Xue, Huimin Chen, Zhiyuan Liu, Maosong Sun</strong></p>
<p>Understanding alignment techniques begins with comprehending zero-shot generalization brought by instruction tuning, but little of the mechanism has been understood. Existing work has largely been confined to the task level, without considering that tasks are artificially defined and, to LLMs, merely consist of tokens and representations. To bridge this gap, we investigate zero-shot generalization from the perspective of the data itself. We first demonstrate that zero-shot generalization happens very early during instruction tuning, with loss serving as a stable indicator. Next, we investigate training data arrangement through similarity and granularity perspectives, confirming that the timing of exposure to certain training examples may greatly facilitate generalization on unseen tasks. Finally, we propose a more grounded training data arrangement framework, Test-centric Multi-turn Arrangement, and show its effectiveness in promoting continual learning and further loss reduction. For the first time, we show that zero-shot generalization during instruction tuning is a form of similarity-based generalization between training and test data at the instance level. Our code is released at <a target="_blank" rel="noopener" href="https://github.com/thunlp/Dynamics-of-Zero-Shot-Generalization">https://github.com/thunlp/Dynamics-of-Zero-Shot-Generalization</a>. </p>
<blockquote>
<p>理解对齐技术首先需要对指令微调带来的零样本泛化能力有所认识，但目前对于这种机制的理解还不够深入。现有的工作主要集中在任务层面，没有考虑到任务是人为定义的，对于大型语言模型来说，任务仅仅是由标记和表示组成的。为了弥补这一差距，我们从数据本身的角度来研究零样本泛化。我们首先证明零样本泛化发生在指令微调的早期阶段，损失作为稳定的指标起着关键作用。接下来，我们通过相似性和粒度视角研究训练数据的安排，证实接触某些训练样本的时间可以对未见任务的泛化产生很大影响。最后，我们提出了一个更实际的训练数据安排框架——“以测试为中心的多轮安排”，并展示了它在促进持续学习和进一步减少损失方面的有效性。我们首次展示了指令微调过程中的零样本泛化是训练数据和测试数据实例之间的一种基于相似性的泛化。我们的代码已发布在<a target="_blank" rel="noopener" href="https://github.com/thunlp/Dynamics-of-Zero-Shot-Generalization%E4%B8%8A%E3%80%82">https://github.com/thunlp/Dynamics-of-Zero-Shot-Generalization上。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.11721v2">PDF</a> 22 pages, 16 figures</p>
<p><strong>Summary</strong><br>     研究指令微调中的零样本泛化，揭示其在任务泛化方面的作用机制。研究从数据本身的角度探究零样本泛化，发现泛化发生在指令调教的早期阶段，同时发现训练数据的排列方式与未见过任务的泛化能力有关。为此，提出基于测试的多轮安排框架，促进持续学习和进一步降低损失。零样本泛化是训练数据和测试数据在实例层面的相似泛化的一种表现。具体通过公开代码可以在<a target="_blank" rel="noopener" href="https://github.com/thunlp/Dynamics-of-Zero-Shot-Generalization%E4%B8%8A%E6%9F%A5%E7%9C%8B%E3%80%82">https://github.com/thunlp/Dynamics-of-Zero-Shot-Generalization上查看。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>零样本泛化在指令调教的早期阶段发生，并且损失是一个稳定的指标来衡量这一过程。</li>
<li>训练数据的排列方式对于泛化能力有很大影响。</li>
<li>训练数据的相似性角度和粒度角度对零样本泛化的研究至关重要。</li>
<li>训练数据呈现给模型的时间点对泛化效果有影响。</li>
<li>基于测试的多轮安排框架有助于促进持续学习和降低损失。</li>
<li>零样本泛化是一种基于训练数据和测试数据实例之间相似性的泛化表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.11721">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a156207522ea07985633d6fd603f17bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c1bb8a0d345a77b8e8b29c5c5a9af3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4ecc66c0706e465a8931c92cbb67fe6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2cf62160a756108cfeb98743a02ce836.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="REvolve-Reward-Evolution-with-Large-Language-Models-using-Human-Feedback"><a href="#REvolve-Reward-Evolution-with-Large-Language-Models-using-Human-Feedback" class="headerlink" title="REvolve: Reward Evolution with Large Language Models using Human   Feedback"></a>REvolve: Reward Evolution with Large Language Models using Human   Feedback</h2><p><strong>Authors:Rishi Hazra, Alkis Sygkounas, Andreas Persson, Amy Loutfi, Pedro Zuidberg Dos Martires</strong></p>
<p>Designing effective reward functions is crucial to training reinforcement learning (RL) algorithms. However, this design is non-trivial, even for domain experts, due to the subjective nature of certain tasks that are hard to quantify explicitly. In recent works, large language models (LLMs) have been used for reward generation from natural language task descriptions, leveraging their extensive instruction tuning and commonsense understanding of human behavior. In this work, we hypothesize that LLMs, guided by human feedback, can be used to formulate reward functions that reflect human implicit knowledge. We study this in three challenging settings – autonomous driving, humanoid locomotion, and dexterous manipulation – wherein notions of &#96;&#96;good” behavior are tacit and hard to quantify. To this end, we introduce REvolve, a truly evolutionary framework that uses LLMs for reward design in RL. REvolve generates and refines reward functions by utilizing human feedback to guide the evolution process, effectively translating implicit human knowledge into explicit reward functions for training (deep) RL agents. Experimentally, we demonstrate that agents trained on REvolve-designed rewards outperform other state-of-the-art baselines. </p>
<blockquote>
<p>设计有效的奖励函数对于训练强化学习（RL）算法至关重要。然而，这一设计即使对于领域专家来说也是非平凡的，这是由于某些任务的主观性质难以明确量化。在最近的研究中，大型语言模型（LLM）已被用于根据自然语言任务描述生成奖励，利用它们广泛的指令调整以及对人类行为的常识理解。在这项工作中，我们假设在人类反馈的指导下，LLM可用于制定反映人类隐性知识的奖励函数。我们在三个具有挑战性的场景——自动驾驶、人形运动和灵巧操作——中研究这一点，在这些场景中，“良好”行为的观念是隐性的且难以量化。为此，我们引入了REvolve，这是一个真正使用LLM进行奖励设计的进化框架。REvolve通过利用人类反馈来指导进化过程，生成并优化奖励函数，有效地将隐性的人类知识转化为明确的奖励函数来训练（深度）强化学习代理。实验表明，采用REvolve设计的奖励训练的代理表现优于其他最新技术水平的基准测试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01309v3">PDF</a> Published in ICLR 2025. Project page:   <a target="_blank" rel="noopener" href="https://rishihazra.github.io/REvolve">https://rishihazra.github.io/REvolve</a></p>
<p><strong>Summary</strong><br>强化学习（RL）算法中设计有效的奖励函数至关重要。然而，由于某些任务的主观性质难以明确量化，使得设计奖励函数成为一项非平凡的任务，即使是对于领域专家也是如此。最近的研究利用大型语言模型（LLM）从自然语言任务描述中生成奖励，通过其广泛的指令调整和对人类行为的常识理解来利用这一点。本研究假设，在人类的反馈指导下，LLM可用于制定反映人类隐性知识的奖励函数。我们在三个具有挑战性的场景（自动驾驶、人形运动和灵巧操作）中对此进行了研究，其中良好的行为概念是隐含的，难以量化。为此，我们引入了REvolve，这是一个真正利用LLM进行RL奖励设计的进化框架。REvolve通过利用人类反馈来指导进化过程，生成并优化奖励函数，有效地将人类隐性知识转化为训练深度强化学习代理的明确奖励函数。实验表明，采用REvolve设计的奖励训练的代理在表现上优于其他最先进基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习中奖励函数的设计非常重要且充满挑战。</li>
<li>大型语言模型（LLMs）在奖励生成方面具有潜力，可以从自然语言任务描述中利用。</li>
<li>LLMs能够在人类反馈的指导下反映人类的隐性知识，用于制定奖励函数。</li>
<li>REvolve是一个利用LLM进行RL奖励设计的进化框架。</li>
<li>REvolve通过人类反馈生成和优化奖励函数，将隐性人类知识转化为明确的奖励函数。</li>
<li>在三个具有挑战性的场景中（自动驾驶、人形运动和灵巧操作），使用REvolve设计的奖励训练的代理表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.01309">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4634d14584fad2ceab9607ad5a9f5ece.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5f2ede9c89f3a0bb86dd9a2e4086b76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84c7a6f3332afd04cea90cbf758f48ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-461a70fadb968159e1382603dad2aba5.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Emojis-Decoded-Leveraging-ChatGPT-for-Enhanced-Understanding-in-Social-Media-Communications"><a href="#Emojis-Decoded-Leveraging-ChatGPT-for-Enhanced-Understanding-in-Social-Media-Communications" class="headerlink" title="Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social   Media Communications"></a>Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social   Media Communications</h2><p><strong>Authors:Yuhang Zhou, Paiheng Xu, Xiyao Wang, Xuan Lu, Ge Gao, Wei Ai</strong></p>
<p>Emojis, which encapsulate semantics beyond mere words or phrases, have become prevalent in social network communications. This has spurred increasing scholarly interest in exploring their attributes and functionalities. However, emoji-related research and application face two primary challenges. First, researchers typically rely on crowd-sourcing to annotate emojis in order to understand their sentiments, usage intentions, and semantic meanings. Second, subjective interpretations by users can often lead to misunderstandings of emojis and cause the communication barrier. Large Language Models (LLMs) have achieved significant success in various annotation tasks, with ChatGPT demonstrating expertise across multiple domains. In our study, we assess ChatGPT’s effectiveness in handling previously annotated and downstream tasks. Our objective is to validate the hypothesis that ChatGPT can serve as a viable alternative to human annotators in emoji research and that its ability to explain emoji meanings can enhance clarity and transparency in online communications. Our findings indicate that ChatGPT has extensive knowledge of emojis. It is adept at elucidating the meaning of emojis across various application scenarios and demonstrates the potential to replace human annotators in a range of tasks. </p>
<blockquote>
<p>表情符号已经广泛普及于社交网络通信中，它们可以包含超越简单词汇或短语的语义。这引发了越来越多的学术兴趣来探索其属性和功能。然而，关于表情符号的研究和应用面临两大挑战。首先，研究人员通常依赖于众包来对表情符号进行注释，以了解它们的情感、使用意图和语义。其次，用户的主观解读往往会导致对表情符号的误解，造成沟通障碍。大型语言模型（LLM）在各种注释任务中取得了巨大成功，ChatGPT在多个领域展现了专业知识。在我们的研究中，我们评估了ChatGPT在处理已注释和下游任务方面的有效性。我们的目标是验证假设：ChatGPT可以作为人类注释者在表情符号研究中的可行替代方案，并且其解释表情符号意义的能力可以提高在线通信的清晰度和透明度。我们发现ChatGPT拥有丰富的表情符号知识。它在各种应用场景中擅长阐释表情符号的含义，并显示出在一系列任务中替代人类注释者的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.01681v3">PDF</a> Accepted by the 19th International AAAI Conference on Web and Social   Media (ICWSM 2025)</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）如ChatGPT在emoji研究领域表现出强大的潜力。通过替代人工标注方式，ChatGPT不仅能有效理解emoji的情感、使用意图和语义含义，还能提升在线通信的清晰度和透明度。本研究验证了ChatGPT在emoji研究中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Emojis已逐渐成为社交网络通讯中的重要组成部分，其属性和功能的研究吸引了越来越多的学者关注。</li>
<li>当前emoji相关研究面临两个主要挑战：依赖人工标注的方式以及用户主观解读可能导致的误解和沟通障碍。</li>
<li>大型语言模型（LLM）如ChatGPT在各类标注任务中取得了显著成功，具备跨多个领域的专业知识。</li>
<li>ChatGPT具备丰富的emoji知识，能够阐释emoji在不同应用场景下的含义。</li>
<li>ChatGPT有潜力替代人工标注方式，提高emoji研究的效率。</li>
<li>ChatGPT能够增强在线通讯的清晰度和透明度，通过解释emoji含义来促进准确沟通。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.01681">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-76c54f813c72ec1c9ea5d8bf1f303fd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d055ba6d64b963a64b17da9edcff01db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6083670f68cc7897062fa5b37143819.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5cdedc1937de5e60aa0d6c04e5e401bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd8a4611749f37401183f2efae9baa9c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-13/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-13/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-15/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9622a0e405fc2a4437ce21c7de2d4c72.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-04-15  DocAgent A Multi-Agent System for Automated Code Documentation   Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-13/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c5f257b02c3494de39d23bd3d097551a.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-04-13  Kimi-VL Technical Report
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25243.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
