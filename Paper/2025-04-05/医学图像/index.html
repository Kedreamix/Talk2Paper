<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-05  STING-BEE Towards Vision-Language Model for Real-World X-ray Baggage   Security Inspection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-87db798d15356e64a92dde81090c2fa9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    78 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-05-æ›´æ–°"><a href="#2025-04-05-æ›´æ–°" class="headerlink" title="2025-04-05 æ›´æ–°"></a>2025-04-05 æ›´æ–°</h1><h2 id="STING-BEE-Towards-Vision-Language-Model-for-Real-World-X-ray-Baggage-Security-Inspection"><a href="#STING-BEE-Towards-Vision-Language-Model-for-Real-World-X-ray-Baggage-Security-Inspection" class="headerlink" title="STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage   Security Inspection"></a>STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage   Security Inspection</h2><p><strong>Authors:Divya Velayudhan, Abdelfatah Ahmed, Mohamad Alansari, Neha Gour, Abderaouf Behouch, Taimur Hassan, Syed Talal Wasim, Nabil Maalej, Muzammal Naseer, Juergen Gall, Mohammed Bennamoun, Ernesto Damiani, Naoufel Werghi</strong></p>
<p>Advancements in Computer-Aided Screening (CAS) systems are essential for improving the detection of security threats in X-ray baggage scans. However, current datasets are limited in representing real-world, sophisticated threats and concealment tactics, and existing approaches are constrained by a closed-set paradigm with predefined labels. To address these challenges, we introduce STCray, the first multimodal X-ray baggage security dataset, comprising 46,642 image-caption paired scans across 21 threat categories, generated using an X-ray scanner for airport security. STCray is meticulously developed with our specialized protocol that ensures domain-aware, coherent captions, that lead to the multi-modal instruction following data in X-ray baggage security. This allows us to train a domain-aware visual AI assistant named STING-BEE that supports a range of vision-language tasks, including scene comprehension, referring threat localization, visual grounding, and visual question answering (VQA), establishing novel baselines for multi-modal learning in X-ray baggage security. Further, STING-BEE shows state-of-the-art generalization in cross-domain settings. Code, data, and models are available at <a target="_blank" rel="noopener" href="https://divs1159.github.io/STING-BEE/">https://divs1159.github.io/STING-BEE/</a>. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©ç­›æŸ¥ï¼ˆCASï¼‰ç³»ç»Ÿçš„è¿›æ­¥å¯¹äºæé«˜Xå…‰è¡Œææ‰«æä¸­å®‰å…¨å¨èƒçš„æ£€æµ‹è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ•°æ®é›†åœ¨è¡¨ç¤ºç°å®ä¸–ç•Œä¸­å¤æ‚å¨èƒå’Œéšè”½ç­–ç•¥æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œç°æœ‰æ–¹æ³•å—åˆ°å°é—­é›†æ¨¡å¼çš„çº¦æŸï¼Œå…·æœ‰é¢„å®šä¹‰æ ‡ç­¾ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†STCrayï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤šæ¨¡æ€Xå…‰è¡Œæå®‰å…¨æ•°æ®é›†ï¼ŒåŒ…å«46642å¼ å›¾åƒå’Œæ ‡é¢˜é…å¯¹æ‰«æï¼Œè·¨è¶Š21ä¸ªå¨èƒç±»åˆ«ï¼Œä½¿ç”¨Xå…‰æ‰«æä»ªç”Ÿæˆç”¨äºæœºåœºå®‰å…¨ã€‚STCrayç»è¿‡æˆ‘ä»¬ä¸“é—¨å¼€å‘çš„åè®®ç²¾å¿ƒå¼€å‘ï¼Œç¡®ä¿é¢†åŸŸæ„ŸçŸ¥ã€è¿è´¯çš„æ ‡é¢˜ï¼Œå¼•å¯¼Xå…‰è¡Œæå®‰å…¨ä¸­çš„å¤šæ¨¡æ€æŒ‡ä»¤è·Ÿéšæ•°æ®ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿè®­ç»ƒä¸€ä¸ªåä¸ºSTING-BEEçš„é¢†åŸŸæ„ŸçŸ¥è§†è§‰äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæ”¯æŒä¸€ç³»åˆ—è§†è§‰è¯­è¨€ä»»åŠ¡ï¼ŒåŒ…æ‹¬åœºæ™¯ç†è§£ã€å¨èƒå®šä½ã€è§†è§‰å®šä½å’Œè§†è§‰é—®ç­”ï¼ˆVQAï¼‰ï¼Œä¸ºXå…‰è¡Œæå®‰å…¨ä¸­çš„å¤šæ¨¡æ€å­¦ä¹ å»ºç«‹æ–°çš„åŸºå‡†ã€‚æ­¤å¤–ï¼ŒSTING-BEEåœ¨è·¨åŸŸè®¾ç½®ä¸­æ˜¾ç¤ºå‡ºæœ€å…ˆè¿›çš„æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://divs1159.github.io/STING-BEE/">https://divs1159.github.io/STING-BEE/</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02823v1">PDF</a> Accepted at CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æ–°ä¸€ä»£è®¡ç®—æœºè¾…åŠ©ç­›æŸ¥ç³»ç»Ÿå¯¹æå‡Xå…‰è¡Œææ‰«æä¸­çš„å®‰å…¨å¨èƒæ£€æµ‹è‡³å…³é‡è¦ã€‚å½“å‰æ•°æ®é›†é¢ä¸´ç°å®ä¸–ç•Œä¸­å¤æ‚å¨èƒå’Œéšè”½è—åŒ¿æ–¹å¼çš„ä»£è¡¨æ€§ä¸è¶³çš„é—®é¢˜ï¼Œä¸”ç°æœ‰æ–¹æ³•å—é™äºé¢„è®¾æ ‡ç­¾çš„å°é—­é›†èŒƒå¼ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºSTCrayâ€”â€”é¦–ä¸ªå¤šæ¨¡æ€Xå…‰è¡Œæå®‰å…¨æ•°æ®é›†ï¼ŒåŒ…å«46,642å¼ å›¾åƒå’Œæè¿°é…å¯¹æ‰«æï¼Œæ¶µç›–21ç±»å¨èƒï¼Œä½¿ç”¨æœºåœºå®‰å…¨Xå…‰æ‰«æä»ªç”Ÿæˆã€‚STCrayé‡‡ç”¨ä¸“ä¸šåè®®ç¡®ä¿é¢†åŸŸæ„ŸçŸ¥ã€è¿è´¯çš„æè¿°ï¼Œæ”¯æŒXå…‰è¡Œæå®‰å…¨ä¸­çš„å¤šæ¨¡æ€æŒ‡ä»¤è·Ÿéšæ•°æ®ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿè®­ç»ƒä¸€ä¸ªåä¸ºSTING-BEEçš„é¢†åŸŸæ„ŸçŸ¥è§†è§‰äººå·¥æ™ºèƒ½åŠ©ç†ï¼Œæ”¯æŒåœºæ™¯ç†è§£ã€æŒ‡ä»£å¨èƒå®šä½ã€è§†è§‰æ¥åœ°å’Œè§†è§‰é—®ç­”ç­‰è§†è§‰è¯­è¨€ä»»åŠ¡ï¼Œä¸ºXå…‰è¡Œæå®‰å…¨ä¸­çš„å¤šæ¨¡æ€å­¦ä¹ å»ºç«‹æ–°åŸºå‡†ã€‚æ­¤å¤–ï¼ŒSTING-BEEåœ¨è·¨åŸŸè®¾ç½®ä¸­æ˜¾ç¤ºå‡ºæœ€å…ˆè¿›çš„æ³›åŒ–èƒ½åŠ›ã€‚ç›¸å…³ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å¯è®¿é—®<a target="_blank" rel="noopener" href="https://divs1159.github.io/STING-BEE/%E8%8E%B7%E5%8F%96%E3%80%82">https://divs1159.github.io/STING-BEE/è·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—æœºè¾…åŠ©ç­›æŸ¥ç³»ç»Ÿå¯¹æå‡Xå…‰è¡Œææ‰«æä¸­çš„å®‰å…¨å¨èƒæ£€æµ‹è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ•°æ®é›†é¢ä¸´ä»£è¡¨æ€§ä¸è¶³å’Œå°é—­é›†èŒƒå¼çš„é™åˆ¶ã€‚</li>
<li>æ¨å‡ºSTCrayå¤šæ¨¡æ€Xå…‰è¡Œæå®‰å…¨æ•°æ®é›†ï¼Œæ¶µç›–å¤šç§å¨èƒç±»åˆ«ï¼Œä½¿ç”¨æœºåœºå®‰å…¨Xå…‰æ‰«æä»ªç”Ÿæˆã€‚</li>
<li>STCrayé‡‡ç”¨ä¸“ä¸šåè®®ç¡®ä¿é¢†åŸŸæ„ŸçŸ¥ã€è¿è´¯çš„æè¿°ï¼Œæ”¯æŒå¤šæ¨¡æ€æŒ‡ä»¤è·Ÿéšæ•°æ®ã€‚</li>
<li>è®­ç»ƒå‡ºé¢†åŸŸæ„ŸçŸ¥è§†è§‰äººå·¥æ™ºèƒ½åŠ©ç†STING-BEEï¼Œæ”¯æŒå¤šç§è§†è§‰è¯­è¨€ä»»åŠ¡ã€‚</li>
<li>STING-BEEåœ¨è·¨åŸŸè®¾ç½®ä¸­å…·æœ‰å…ˆè¿›çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02823">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-885fa3a07b305eaf0220b37593a7685b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd1224e591427c3b6386e9ec97e7f3e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a24f046360b1229cbf8fca6c18b4c79e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e3d7127b73238411f8ba6f05d84c039f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6036f3fe37aecac7ec4a605d4dc5561e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6240b8b1d30bff034131b2b9908c56d5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Two-Stage-nnU-Net-for-Automatic-Multi-class-Bi-Atrial-Segmentation-from-LGE-MRIs"><a href="#Two-Stage-nnU-Net-for-Automatic-Multi-class-Bi-Atrial-Segmentation-from-LGE-MRIs" class="headerlink" title="Two-Stage nnU-Net for Automatic Multi-class Bi-Atrial Segmentation from   LGE-MRIs"></a>Two-Stage nnU-Net for Automatic Multi-class Bi-Atrial Segmentation from   LGE-MRIs</h2><p><strong>Authors:Y. On, C. Galazis, C. Chiu, M. Varela</strong></p>
<p>Late gadolinium enhancement magnetic resonance imaging (LGE-MRI) is used to visualise atrial fibrosis and scars, providing important information for personalised atrial fibrillation (AF) treatments. Since manual analysis and delineations of these images can be both labour-intensive and subject to variability, we develop an automatic pipeline to perform segmentation of the left atrial (LA) cavity, the right atrial (RA) cavity, and the wall of both atria on LGE-MRI. Our method is based on a two-stage nnU-Net architecture, combining 2D and 3D convolutional networks, and incorporates adaptive histogram equalisation to improve tissue contrast in the input images and morphological operations on the output segmentation maps. We achieve Dice similarity coefficients of 0.92 +&#x2F;- 0.03, 0.93 +&#x2F;- 0.03, 0.71 +&#x2F;- 0.05 and 95% Hausdorff distances of (3.89 +&#x2F;- 6.67) mm, (4.42 +&#x2F;- 1.66) mm and (3.94 +&#x2F;- 1.83) mm for LA, RA, and wall, respectively. The accurate delineation of the LA, RA and the myocardial wall is the first step in analysing atrial structure in cardiovascular patients, especially those with AF. This can allow clinicians to provide adequate and personalised treatment plans in a timely manner. </p>
<blockquote>
<p>é’†å»¶è¿Ÿå¢å¼ºç£å…±æŒ¯æˆåƒï¼ˆLate Gadolinium Enhancement MRIï¼Œç®€ç§° LGE-MRIï¼‰è¢«ç”¨æ¥å¯è§†åŒ–å¿ƒæˆ¿çº¤ç»´åŒ–å’Œç˜¢ç—•ï¼Œä¸ºä¸ªæ€§åŒ–æ²»ç–—å¿ƒæˆ¿é¢¤åŠ¨ï¼ˆAFï¼‰æä¾›é‡è¦ä¿¡æ¯ã€‚ç”±äºå¯¹è¿™äº›å›¾åƒçš„æ‰‹åŠ¨åˆ†æå’Œè½®å»“æç»˜æ—¢è€—æ—¶åˆå­˜åœ¨å·®å¼‚æ€§ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨ç®¡é“ï¼Œç”¨äºå¯¹ LGE-MRI ä¸Šçš„å·¦å¿ƒæˆ¿ï¼ˆLAï¼‰ã€å³å¿ƒæˆ¿ï¼ˆRAï¼‰ä»¥åŠå¿ƒæˆ¿å£è¿›è¡Œåˆ†å‰²ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºä¸¤é˜¶æ®µ nnU-Net æ¶æ„ï¼Œç»“åˆäºŒç»´å’Œä¸‰ç»´å·ç§¯ç½‘ç»œï¼Œå¹¶èå…¥è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ–ï¼Œä»¥æé«˜è¾“å…¥å›¾åƒä¸­çš„ç»„ç»‡å¯¹æ¯”åº¦ä»¥åŠè¾“å‡ºåˆ†å‰²å›¾ä¸Šçš„å½¢æ€æ“ä½œã€‚æˆ‘ä»¬å®ç°äº†ç‹„å…‹ç›¸ä¼¼ç³»æ•°åˆ†åˆ«ä¸º 0.92 Â± 0.03ã€0.93 Â± 0.03ã€0.71 Â± 0.05 ä»¥åŠ Hausdorff è·ç¦»çš„ 95% åˆ†åˆ«ä¸º (3.89 Â± 6.67) æ¯«ç±³ã€ï¼ˆ4.42 Â± 1.66ï¼‰æ¯«ç±³å’Œï¼ˆ3.94 Â± 1.83ï¼‰æ¯«ç±³ï¼Œåˆ†åˆ«å¯¹åº” LAã€RA å’Œå¿ƒæˆ¿å£ã€‚å‡†ç¡®æç»˜ LAã€RA å’Œå¿ƒè‚Œå£æ˜¯åˆ†æå¿ƒè¡€ç®¡ç–¾ç—…æ‚£è€…å¿ƒæˆ¿ç»“æ„çš„ç¬¬ä¸€æ­¥ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé‚£äº›æ‚£æœ‰å¿ƒæˆ¿é¢¤åŠ¨çš„æ‚£è€…ã€‚è¿™å¯ä»¥ä½¿ä¸´åºŠåŒ»ç”ŸåŠæ—¶æä¾›å……è¶³ä¸”ä¸ªæ€§åŒ–çš„æ²»ç–—è®¡åˆ’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02668v1">PDF</a> MBAS Challenge, STACOM, MICCAI 2024</p>
<p><strong>Summary</strong></p>
<p>åŸºäºLGE-MRIå›¾åƒï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§è‡ªåŠ¨åˆ†å‰²å·¦å¿ƒæˆ¿ï¼ˆLAï¼‰ã€å³å¿ƒæˆ¿ï¼ˆRAï¼‰ä»¥åŠå¿ƒæˆ¿å£çš„ä¸¤é˜¶æ®µnnU-Netæ¶æ„æ–¹æ³•ã€‚æ­¤æ–¹æ³•ç»“åˆäº†äºŒç»´å’Œä¸‰ç»´å·ç§¯ç½‘ç»œï¼Œå¹¶é‡‡ç”¨è‡ªé€‚åº”ç›´æ–¹å‡è¡¡åŒ–æé«˜å›¾åƒç»„ç»‡çš„å¯¹æ¯”åº¦ï¼Œä»¥åŠå¯¹è¾“å‡ºåˆ†å‰²å›¾è¿›è¡Œå½¢æ€å­¦æ“ä½œã€‚è¯¥æ–¹æ³•çš„Diceç›¸ä¼¼ç³»æ•°è¾¾åˆ°é«˜å€¼ï¼Œè¡¨æ˜å‡†ç¡®åº¦é«˜ã€‚å‡†ç¡®åˆ†å‰²å¿ƒæˆ¿ç»“æ„æœ‰åŠ©äºå¿ƒè¡€ç®¡ç—…æ‚£è€…ï¼Œå°¤å…¶æ˜¯æˆ¿é¢¤æ‚£è€…çš„ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’åˆ¶å®šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LGE-MRIç”¨äºå¯è§†åŒ–å¿ƒæˆ¿çº¤ç»´åŒ–å’Œç–¤ç—•ï¼Œä¸ºä¸ªæ€§åŒ–æ²»ç–—æˆ¿é¢¤æä¾›é‡è¦ä¿¡æ¯ã€‚</li>
<li>è‡ªåŠ¨åŒ–ç®¡é“åŸºäºä¸¤é˜¶æ®µnnU-Netæ¶æ„ï¼Œç»“åˆäºŒç»´å’Œä¸‰ç»´å·ç§¯ç½‘ç»œè¿›è¡Œå¿ƒæˆ¿åŠå…¶å£çš„åˆ†å‰²ã€‚</li>
<li>é‡‡ç”¨è‡ªé€‚åº”ç›´æ–¹å‡è¡¡åŒ–æ”¹å–„å›¾åƒç»„ç»‡å¯¹æ¯”åº¦ï¼Œå½¢æ€å­¦æ“ä½œä¼˜åŒ–åˆ†å‰²ç»“æœã€‚</li>
<li>æ–¹æ³•å®ç°äº†é«˜Diceç›¸ä¼¼ç³»æ•°å’Œä½Hausdorffè·ç¦»ï¼Œæ˜¾ç¤ºåˆ†å‰²å‡†ç¡®æ€§é«˜ã€‚</li>
<li>å‡†ç¡®çš„å¿ƒæˆ¿ç»“æ„åˆ†ææœ‰åŠ©äºå¿ƒè¡€ç®¡ç–¾ç—…çš„è¯Šæ–­å’Œæ²»ç–—ã€‚</li>
<li>è‡ªåŠ¨åˆ†å‰²æŠ€æœ¯å¯å‡è½»åŒ»ç”Ÿæ‰‹åŠ¨åˆ†æçš„å·¥ä½œé‡ï¼Œæé«˜åˆ†æä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-23a2b90fb610746ffbb339110010c106.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c05f21b87d750d9f583ffc2e3d631d2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-427e52a19de25cf52a73169d908b81f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83d2e96ae1c812d0d53d0c74141d796d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f24ccc35a6c719523eca1beb96b06441.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Enhancing-Compton-telescope-imaging-with-maximum-a-posteriori-estimation-a-modified-Richardson-Lucy-algorithm-for-the-Compton-Spectrometer-and-Imager"><a href="#Enhancing-Compton-telescope-imaging-with-maximum-a-posteriori-estimation-a-modified-Richardson-Lucy-algorithm-for-the-Compton-Spectrometer-and-Imager" class="headerlink" title="Enhancing Compton telescope imaging with maximum a posteriori   estimation: a modified Richardson-Lucy algorithm for the Compton Spectrometer   and Imager"></a>Enhancing Compton telescope imaging with maximum a posteriori   estimation: a modified Richardson-Lucy algorithm for the Compton Spectrometer   and Imager</h2><p><strong>Authors:Hiroki Yoneda, Thomas Siegert, Israel Martinez-Castellanos, Savitri Gallego, Chris Karwin, Hugh Bates, Steven E. Boggs, Chien-You Huang, Alyson Joens, Shigeki Matsumoto, Saurabh Mittal, Eliza Neights, Michela Negro, Uwe Oberlack, Keigo Okuma, Sean N. Pike, Jarred Roberts, Field Rogers, Yong Sheng, Tadayuki Takahashi, Anaya Valluvan, Yu Watanabe, Dieter Hartmann, Carolyn Kierans, John Tomsick, Andreas Zoglauer</strong></p>
<p>We present a modified Richardson-Lucy (RL) algorithm tailored for image reconstruction in MeV gamma-ray observations, focusing on its application to the upcoming Compton Spectrometer and Imager (COSI) mission. Our method addresses key challenges in MeV gamma-ray astronomy by incorporating Bayesian priors for sparseness and smoothness while optimizing background components simultaneously. We introduce a novel sparsity term suitable for Poisson-sampled data in addition to a smoothness prior, allowing for flexible reconstruction of both point sources and extended emission. The performance of the algorithm is evaluated using simulated three-month COSI observations of gamma-ray lines of $^{44}$Ti (1.157 MeV), $^{26}$Al (1.809 MeV), and positron annihilation (0.511 MeV), respectively, representing various spatial features. Our results demonstrate significant improvements over conventional RL methods, particularly in suppressing artificial structures in point source reconstructions and retaining diffuse spatial structures. This work represents an important step towards establishing a robust data analysis for studying nucleosynthesis, positron annihilation, and other high-energy phenomena in our Galaxy. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹MeVä¼½é©¬å°„çº¿è§‚æµ‹çš„å›¾åƒé‡å»ºçš„æ”¹è¿›Richardson-Lucyï¼ˆRLï¼‰ç®—æ³•ï¼Œé‡ç‚¹å…³æ³¨å…¶åœ¨å³å°†è¿›è¡Œçš„åº·æ™®é¡¿å…‰è°±ä»ªå’Œæˆåƒå™¨ï¼ˆCOSIï¼‰ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡èå…¥è´å¶æ–¯å…ˆéªŒæ¥è§£å†³MeVä¼½é©¬å°„çº¿å¤©æ–‡å­¦é¢†åŸŸçš„å…³é”®æŒ‘æˆ˜ï¼Œä¼˜åŒ–èƒŒæ™¯æˆåˆ†çš„åŒæ—¶å…¼é¡¾ç¨€ç–æ€§å’Œå¹³æ»‘æ€§ã€‚é™¤äº†å¹³æ»‘å…ˆéªŒä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§é€‚ç”¨äºPoissoné‡‡æ ·æ•°æ®çš„æ–°ç¨€ç–é¡¹ï¼Œèƒ½å¤Ÿå®ç°ç‚¹æºå’Œæ‰©å±•å‘å°„çš„çµæ´»é‡å»ºã€‚æˆ‘ä»¬é€šè¿‡æ¨¡æ‹Ÿä¸‰ä¸ªæœˆçš„COSIå¯¹$^{44}$Tiï¼ˆ1.157 MeVï¼‰ã€$^{26}$Alï¼ˆ1.809 MeVï¼‰å’Œæ­£ç”µå­é€€ç«ï¼ˆ0.511 MeVï¼‰çš„ä¼½é©¬å°„çº¿çº¿è§‚æµ‹æ¥è¯„ä¼°ç®—æ³•æ€§èƒ½ï¼Œåˆ†åˆ«ä»£è¡¨ä¸åŒçš„ç©ºé—´ç‰¹å¾ã€‚æˆ‘ä»¬çš„ç»“æœç›¸è¾ƒäºä¼ ç»Ÿçš„RLæ–¹æ³•æœ‰æ˜æ˜¾çš„æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨æŠ‘åˆ¶ç‚¹æºé‡å»ºä¸­çš„äººå·¥ç»“æ„å’Œä¿ç•™æ‰©æ•£ç©ºé—´ç»“æ„æ–¹é¢ã€‚è¿™é¡¹å·¥ä½œå¯¹äºå»ºç«‹ç¨³å¥çš„æ•°æ®åˆ†ææ¥ç ”ç©¶æ ¸åˆæˆã€æ­£ç”µå­é€€ç«ä»¥åŠæˆ‘ä»¬é“¶æ²³ç³»ä¸­çš„å…¶ä»–é«˜èƒ½ç°è±¡å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02468v1">PDF</a> 19 pages, accepted by Astronomy &amp; Astrophysics</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹MeVä¼½é©¬å°„çº¿è§‚æµ‹çš„å›¾åƒé‡å»ºï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¹è¿›çš„Richardson-Lucyï¼ˆRLï¼‰ç®—æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºå³å°†å‘å°„çš„Comptonå…‰è°±ä»ªå’Œæˆåƒä»ªï¼ˆCOSIï¼‰ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥è´å¶æ–¯å…ˆéªŒæ¥å®ç°ç¨€ç–æ€§å’Œå¹³æ»‘æ€§ï¼ŒåŒæ—¶ä¼˜åŒ–èƒŒæ™¯æˆåˆ†ï¼Œè§£å†³äº†MeVä¼½é©¬å°„çº¿å¤©æ–‡å­¦ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚é™¤äº†å¹³æ»‘æ€§å…ˆéªŒå¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªé€‚åˆPoissoné‡‡æ ·æ•°æ®çš„æ–°ç¨€ç–æ€§æœ¯è¯­ï¼Œå¯ä»¥å®ç°ç‚¹æºå’Œæ‰©å±•å‘å°„çš„çµæ´»é‡å»ºã€‚æˆ‘ä»¬ä½¿ç”¨æ¨¡æ‹Ÿçš„COSIå¯¹$^{44}$Tiï¼ˆ1.157 MeVï¼‰ã€$^{26}$Alï¼ˆ1.809 MeVï¼‰å’Œæ­£ç”µå­é€€ç«ï¼ˆ0.511 MeVï¼‰çš„ä¼½é©¬å°„çº¿çº¿è§‚æµ‹ç»“æœæ¥è¯„ä¼°ç®—æ³•æ€§èƒ½ï¼Œè¿™äº›è§‚æµ‹ç»“æœä»£è¡¨äº†ä¸åŒçš„ç©ºé—´ç‰¹å¾ã€‚ç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»ŸRLæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç®—æ³•åœ¨æŠ‘åˆ¶ç‚¹æºé‡å»ºä¸­çš„äººå·¥ç»“æ„ä»¥åŠä¿ç•™æ‰©æ•£ç©ºé—´ç»“æ„æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ”¹å–„ã€‚è¿™é¡¹å·¥ä½œå¯¹äºå»ºç«‹ç ”ç©¶æ ¸åˆæˆã€æ­£ç”µå­é€€ç«ä»¥åŠé“¶æ²³ç³»ä¸­å…¶ä»–é«˜èƒ½ç°è±¡çš„å¼ºå¤§æ•°æ®åˆ†ææ–¹æ³•å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ”¹è¿›Richardson-Lucyç®—æ³•ï¼Œé€‚ç”¨äºMeVä¼½é©¬å°„çº¿è§‚æµ‹çš„å›¾åƒé‡å»ºã€‚</li>
<li>å¼•å…¥è´å¶æ–¯å…ˆéªŒå®ç°ç¨€ç–æ€§å’Œå¹³æ»‘æ€§ï¼Œä¼˜åŒ–èƒŒæ™¯æˆåˆ†ã€‚</li>
<li>å¼•å…¥é€‚åˆPoissoné‡‡æ ·æ•°æ®çš„æ–°ç¨€ç–æ€§æœ¯è¯­ï¼Œå®ç°çµæ´»é‡å»ºç‚¹æºå’Œæ‰©å±•å‘å°„ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹Ÿçš„COSIè§‚æµ‹ç»“æœè¯„ä¼°ç®—æ³•æ€§èƒ½ã€‚</li>
<li>ç›¸è¾ƒäºä¼ ç»ŸRLæ–¹æ³•ï¼Œè¯¥ç®—æ³•åœ¨æŠ‘åˆ¶äººå·¥ç»“æ„ä¿ç•™æ‰©æ•£ç©ºé—´ç»“æ„æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ”¹å–„ã€‚</li>
<li>å¯¹äºç ”ç©¶æ ¸åˆæˆã€æ­£ç”µå­é€€ç«ç­‰é«˜èƒ½ç°è±¡å…·æœ‰å¼ºå¤§çš„æ•°æ®åˆ†ææ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02468">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5fbc3e3275c37c19ecae3320d7c88f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-769a3ae48546414c7dc465bb40abbe39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47cfcd155170efa31b2da7dd766fa112.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Benchmark-of-Segmentation-Techniques-for-Pelvic-Fracture-in-CT-and-X-ray-Summary-of-the-PENGWIN-2024-Challenge"><a href="#Benchmark-of-Segmentation-Techniques-for-Pelvic-Fracture-in-CT-and-X-ray-Summary-of-the-PENGWIN-2024-Challenge" class="headerlink" title="Benchmark of Segmentation Techniques for Pelvic Fracture in CT and   X-ray: Summary of the PENGWIN 2024 Challenge"></a>Benchmark of Segmentation Techniques for Pelvic Fracture in CT and   X-ray: Summary of the PENGWIN 2024 Challenge</h2><p><strong>Authors:Yudi Sang, Yanzhen Liu, Sutuke Yibulayimu, Yunning Wang, Benjamin D. Killeen, Mingxu Liu, Ping-Cheng Ku, Ole Johannsen, Karol Gotkowski, Maximilian Zenk, Klaus Maier-Hein, Fabian Isensee, Peiyan Yue, Yi Wang, Haidong Yu, Zhaohong Pan, Yutong He, Xiaokun Liang, Daiqi Liu, Fuxin Fan, Artur Jurgas, Andrzej Skalski, Yuxi Ma, Jing Yang, Szymon PÅ‚otka, RafaÅ‚ Litka, Gang Zhu, Yingchun Song, Mathias Unberath, Mehran Armand, Dan Ruan, S. Kevin Zhou, Qiyong Cao, Chunpeng Zhao, Xinbao Wu, Yu Wang</strong></p>
<p>The segmentation of pelvic fracture fragments in CT and X-ray images is crucial for trauma diagnosis, surgical planning, and intraoperative guidance. However, accurately and efficiently delineating the bone fragments remains a significant challenge due to complex anatomy and imaging limitations. The PENGWIN challenge, organized as a MICCAI 2024 satellite event, aimed to advance automated fracture segmentation by benchmarking state-of-the-art algorithms on these complex tasks. A diverse dataset of 150 CT scans was collected from multiple clinical centers, and a large set of simulated X-ray images was generated using the DeepDRR method. Final submissions from 16 teams worldwide were evaluated under a rigorous multi-metric testing scheme. The top-performing CT algorithm achieved an average fragment-wise intersection over union (IoU) of 0.930, demonstrating satisfactory accuracy. However, in the X-ray task, the best algorithm attained an IoU of 0.774, highlighting the greater challenges posed by overlapping anatomical structures. Beyond the quantitative evaluation, the challenge revealed methodological diversity in algorithm design. Variations in instance representation, such as primary-secondary classification versus boundary-core separation, led to differing segmentation strategies. Despite promising results, the challenge also exposed inherent uncertainties in fragment definition, particularly in cases of incomplete fractures. These findings suggest that interactive segmentation approaches, integrating human decision-making with task-relevant information, may be essential for improving model reliability and clinical applicability. </p>
<blockquote>
<p>éª¨ç›†éª¨æŠ˜ç¢ç‰‡åœ¨CTå’ŒXå°„çº¿å›¾åƒä¸­çš„åˆ†å‰²å¯¹äºåˆ›ä¼¤è¯Šæ–­ã€æ‰‹æœ¯è§„åˆ’å’Œæœ¯ä¸­æŒ‡å¯¼è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºå¤æ‚çš„è§£å‰–ç»“æ„å’Œæˆåƒé™åˆ¶ï¼Œå‡†ç¡®é«˜æ•ˆåœ°æç»˜éª¨ç¢ç‰‡ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚PENGWINæŒ‘æˆ˜ä½œä¸ºMICCAI 2024çš„å«æ˜Ÿæ´»åŠ¨ï¼Œæ—¨åœ¨é€šè¿‡æœ€æ–°ç®—æ³•åœ¨è¿™äº›å¤æ‚ä»»åŠ¡ä¸Šçš„åŸºå‡†æµ‹è¯•ï¼Œæ¨åŠ¨è‡ªåŠ¨éª¨æŠ˜åˆ†å‰²çš„å‘å±•ã€‚ä»å¤šä¸ªä¸´åºŠä¸­å¿ƒæ”¶é›†äº†150ä»½CTæ‰«æçš„å¤šæ ·æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨DeepDRRæ–¹æ³•ç”Ÿæˆäº†å¤§é‡æ¨¡æ‹ŸXå°„çº¿å›¾åƒã€‚æ¥è‡ªå…¨çƒ16ä¸ªå›¢é˜Ÿçš„æœ€ç»ˆæäº¤ä½œå“åœ¨ä¸¥æ ¼çš„å¤šæŒ‡æ ‡æµ‹è¯•æ–¹æ¡ˆä¸‹è¿›è¡Œäº†è¯„ä¼°ã€‚è¡¨ç°æœ€ä½³çš„CTç®—æ³•å¹³å‡ç‰‡æ®µçº§äº¤å¹¶æ¯”ï¼ˆIoUï¼‰è¾¾åˆ°0.930ï¼Œæ˜¾ç¤ºå‡ºä»¤äººæ»¡æ„çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œåœ¨Xå°„çº¿ä»»åŠ¡ä¸­ï¼Œæœ€ä½³ç®—æ³•çš„IoUä¸º0.774ï¼Œè¿™çªå‡ºäº†ç”±é‡å çš„è§£å‰–ç»“æ„å¸¦æ¥çš„æ›´å¤§æŒ‘æˆ˜ã€‚é™¤äº†å®šé‡è¯„ä¼°ä¹‹å¤–ï¼Œè¯¥æŒ‘æˆ˜è¿˜æ­ç¤ºäº†ç®—æ³•è®¾è®¡ä¸­çš„æ–¹æ³•å¤šæ ·æ€§ã€‚å®ä¾‹è¡¨ç¤ºä¸­çš„å˜åŒ–ï¼Œå¦‚ä¸»æ¬¡åˆ†ç±»ä¸è¾¹ç•Œæ ¸å¿ƒåˆ†ç¦»ï¼Œå¯¼è‡´äº†ä¸åŒçš„åˆ†å‰²ç­–ç•¥ã€‚å°½ç®¡ç»“æœå…·æœ‰å‰æ™¯ï¼Œä½†æŒ‘æˆ˜ä¹Ÿæš´éœ²äº†ç¢ç‰‡å®šä¹‰ä¸­çš„å›ºæœ‰ä¸ç¡®å®šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸å®Œå…¨éª¨æŠ˜çš„æƒ…å†µä¸‹ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œèåˆäººç±»å†³ç­–ä¸ä»»åŠ¡ç›¸å…³ä¿¡æ¯çš„äº¤äº’å¼åˆ†å‰²æ–¹æ³•å¯èƒ½å¯¹äºæé«˜æ¨¡å‹å¯é æ€§å’Œä¸´åºŠé€‚ç”¨æ€§è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02382v1">PDF</a> PENGWIN 2024 Challenge Report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç›†è…”éª¨æŠ˜ç¢ç‰‡åœ¨CTå’ŒXå°„çº¿å›¾åƒä¸­çš„åˆ†å‰²å¯¹äºåˆ›ä¼¤è¯Šæ–­ã€æ‰‹æœ¯è§„åˆ’å’Œæœ¯ä¸­æŒ‡å¯¼çš„é‡è¦æ€§ã€‚ç„¶è€Œï¼Œç”±äºå¤æ‚çš„è§£å‰–ç»“æ„å’Œæˆåƒé™åˆ¶ï¼Œå‡†ç¡®é«˜æ•ˆåœ°è¿›è¡Œéª¨ç¢ç‰‡åˆ†å‰²ä»æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚PENGWINæŒ‘æˆ˜ä½œä¸ºMICCAI 2024çš„å«æ˜Ÿæ´»åŠ¨ï¼Œæ—¨åœ¨é€šè¿‡è¯„ä¼°æœ€å…ˆè¿›çš„ç®—æ³•åœ¨è¿™äº›å¤æ‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½æ¥ä¿ƒè¿›è‡ªåŠ¨éª¨æŠ˜åˆ†å‰²çš„å‘å±•ã€‚ä½¿ç”¨ä»å¤šä¸ªä¸´åºŠä¸­å¿ƒæ”¶é›†çš„150ä¸ªCTæ‰«ææ„å»ºçš„å¤šæ ·åŒ–æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨DeepDRRæ–¹æ³•ç”Ÿæˆäº†å¤§é‡æ¨¡æ‹ŸXå°„çº¿å›¾åƒã€‚æ¥è‡ªå…¨çƒ16ä¸ªå›¢é˜Ÿçš„æœ€ç»ˆæäº¤ä½œå“åœ¨ä¸¥æ ¼çš„å¤šæŒ‡æ ‡æµ‹è¯•æ–¹æ¡ˆä¸‹è¿›è¡Œäº†è¯„ä¼°ã€‚åœ¨CTä»»åŠ¡ä¸­ï¼Œé¡¶çº§ç®—æ³•çš„å¹³å‡ç‰‡æ®µçº§äº¤å¹¶æ¯”ï¼ˆIoUï¼‰è¾¾åˆ°0.930ï¼Œæ˜¾ç¤ºå‡ºä»¤äººæ»¡æ„çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œåœ¨Xå°„çº¿ä»»åŠ¡ä¸­ï¼Œæœ€ä½³ç®—æ³•çš„IoUä¸º0.774ï¼Œçªæ˜¾å‡ºç”±é‡å è§£å‰–ç»“æ„å¸¦æ¥çš„æ›´å¤§æŒ‘æˆ˜ã€‚æŒ‘æˆ˜è¿˜æ­ç¤ºäº†ç®—æ³•è®¾è®¡ä¸­çš„æ–¹æ³•å¤šæ ·æ€§ï¼Œå¦‚ä¸»è¦æ¬¡çº§åˆ†ç±»ä¸è¾¹ç•Œæ ¸å¿ƒåˆ†ç¦»ç­‰ä¸åŒå®ä¾‹è¡¨ç¤ºä¹‹é—´çš„å·®å¼‚ã€‚å°½ç®¡ç»“æœå…·æœ‰å‰æ™¯ï¼Œä½†æŒ‘æˆ˜ä¹Ÿæš´éœ²äº†ç¢ç‰‡å®šä¹‰ä¸­çš„å›ºæœ‰ä¸ç¡®å®šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸å®Œå…¨éª¨æŠ˜çš„æƒ…å†µä¸‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›†è…”éª¨æŠ˜ç¢ç‰‡çš„åˆ†å‰²å¯¹äºåˆ›ä¼¤è¯Šæ–­ã€æ‰‹æœ¯è§„åˆ’å’Œæœ¯ä¸­æŒ‡å¯¼è‡³å…³é‡è¦ã€‚</li>
<li>CTå’ŒXå°„çº¿å›¾åƒä¸­çš„éª¨ç¢ç‰‡åˆ†å‰²æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä¸»è¦ç”±äºå¤æ‚çš„è§£å‰–ç»“æ„å’Œæˆåƒé™åˆ¶ã€‚</li>
<li>PENGWINæŒ‘æˆ˜æ—¨åœ¨é€šè¿‡è¯„ä¼°æœ€å…ˆè¿›ç®—æ³•åœ¨CTå’ŒXå°„çº¿å›¾åƒä¸­çš„éª¨æŠ˜åˆ†å‰²æ€§èƒ½æ¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„å‘å±•ã€‚</li>
<li>åœ¨CTä»»åŠ¡ä¸­ï¼Œé¡¶çº§ç®—æ³•è¡¨ç°å‡ºé«˜å‡†ç¡®æ€§ï¼Œå¹³å‡ç‰‡æ®µçº§äº¤å¹¶æ¯”ï¼ˆIoUï¼‰è¾¾åˆ°0.930ã€‚</li>
<li>åœ¨Xå°„çº¿ä»»åŠ¡ä¸­ï¼Œæœ€ä½³ç®—æ³•çš„IoUä¸º0.774ï¼Œè¡¨æ˜å­˜åœ¨æ›´å¤§çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ç”±äºé‡å è§£å‰–ç»“æ„å¯¼è‡´çš„ã€‚</li>
<li>æŒ‘æˆ˜æ­ç¤ºäº†éª¨æŠ˜åˆ†å‰²ç®—æ³•è®¾è®¡ä¸­çš„æ–¹æ³•å¤šæ ·æ€§ï¼ŒåŒ…æ‹¬ä¸åŒçš„å®ä¾‹è¡¨ç¤ºç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02382">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ea3d89ab6cd170be64be189a2bbb5ce6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecb58be37b40d6be3b6fb2c354956514.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SemiISP-SemiIE-Semi-Supervised-Image-Signal-Processor-and-Image-Enhancement-Leveraging-One-to-Many-Mapping-sRGB-to-RAW"><a href="#SemiISP-SemiIE-Semi-Supervised-Image-Signal-Processor-and-Image-Enhancement-Leveraging-One-to-Many-Mapping-sRGB-to-RAW" class="headerlink" title="SemiISP&#x2F;SemiIE: Semi-Supervised Image Signal Processor and Image   Enhancement Leveraging One-to-Many Mapping sRGB-to-RAW"></a>SemiISP&#x2F;SemiIE: Semi-Supervised Image Signal Processor and Image   Enhancement Leveraging One-to-Many Mapping sRGB-to-RAW</h2><p><strong>Authors:Masakazu Yoshimura, Junji Otsuka, Radu Berdan, Takeshi Ohashi</strong></p>
<p>DNN-based methods have been successful in Image Signal Processor (ISP) and image enhancement (IE) tasks. However, the cost of creating training data for these tasks is considerably higher than for other tasks, making it difficult to prepare large-scale datasets. Also, creating personalized ISP and IE with minimal training data can lead to new value streams since preferred image quality varies depending on the person and use case. While semi-supervised learning could be a potential solution in such cases, it has rarely been utilized for these tasks. In this paper, we realize semi-supervised learning for ISP and IE leveraging a RAW image reconstruction (sRGB-to-RAW) method. Although existing sRGB-to-RAW methods can generate pseudo-RAW image datasets that improve the accuracy of RAW-based high-level computer vision tasks such as object detection, their quality is not sufficient for ISP and IE tasks that require precise image quality definition. Therefore, we also propose a sRGB-to-RAW method that can improve the image quality of these tasks. The proposed semi-supervised learning with the proposed sRGB-to-RAW method successfully improves the image quality of various models on various datasets. </p>
<blockquote>
<p>åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰çš„æ–¹æ³•åœ¨å›¾åƒä¿¡å·å¤„ç†å™¨ï¼ˆISPï¼‰å’Œå›¾åƒå¢å¼ºï¼ˆIEï¼‰ä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸã€‚ç„¶è€Œï¼Œè¿™äº›ä»»åŠ¡çš„è®­ç»ƒæ•°æ®åˆ¶ä½œæˆæœ¬æ˜æ˜¾é«˜äºå…¶ä»–ä»»åŠ¡ï¼Œéš¾ä»¥å‡†å¤‡å¤§è§„æ¨¡æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œä½¿ç”¨å°‘é‡è®­ç»ƒæ•°æ®åˆ›å»ºä¸ªæ€§åŒ–ISPå’ŒIEå¯ä»¥å¸¦æ¥æ–°ä»·å€¼æµï¼Œå› ä¸ºåå¥½çš„å›¾åƒè´¨é‡ä¼šå› äººè€Œå¼‚ï¼Œå–å†³äºç”¨ä¾‹ã€‚è™½ç„¶åŠç›‘ç£å­¦ä¹ å¯èƒ½æ˜¯æ­¤ç±»æƒ…å†µçš„æ½œåœ¨è§£å†³æ–¹æ¡ˆï¼Œä½†å¾ˆå°‘ç”¨äºè¿™äº›ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨RAWå›¾åƒé‡å»ºï¼ˆsRGB-to-RAWï¼‰æ–¹æ³•å®ç°ISPå’ŒIEçš„åŠç›‘ç£å­¦ä¹ ã€‚å°½ç®¡ç°æœ‰çš„sRGB-to-RAWæ–¹æ³•å¯ä»¥ç”Ÿæˆä¼ªRAWå›¾åƒæ•°æ®é›†ï¼Œæé«˜äº†åŸºäºRAWçš„é«˜çº§è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„å‡†ç¡®æ€§ï¼Œå¦‚ç›®æ ‡æ£€æµ‹ï¼Œä½†å®ƒä»¬çš„è´¨é‡å¯¹äºéœ€è¦ç²¾ç¡®å›¾åƒè´¨é‡å®šä¹‰çš„ISPå’ŒIEä»»åŠ¡æ¥è¯´è¿˜ä¸è¶³ä»¥æ»¡è¶³éœ€æ±‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ”¹è¿›çš„sRGB-to-RAWæ–¹æ³•ï¼Œå¯ä»¥æé«˜è¿™äº›ä»»åŠ¡çš„å›¾åƒè´¨é‡ã€‚æ‰€æå‡ºçš„ç»“åˆæ”¹è¿›sRGB-to-RAWæ–¹æ³•çš„åŠç›‘ç£å­¦ä¹ æˆåŠŸæé«˜äº†å„ç§æ•°æ®é›†ä¸Šå„ç§æ¨¡å‹çš„å›¾åƒè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02345v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„æ–¹æ³•åœ¨å›¾åƒä¿¡å·å¤„ç†å™¨ï¼ˆISPï¼‰å’Œå›¾åƒå¢å¼ºï¼ˆIEï¼‰ä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸã€‚ç„¶è€Œï¼Œè¿™äº›ä»»åŠ¡åˆ›å»ºè®­ç»ƒæ•°æ®çš„æˆæœ¬è¿œé«˜äºå…¶ä»–ä»»åŠ¡ï¼Œéš¾ä»¥å‡†å¤‡å¤§è§„æ¨¡æ•°æ®é›†ã€‚æœ¬æ–‡é€šè¿‡ç»“åˆRAWå›¾åƒé‡å»ºï¼ˆsRGB-to-RAWï¼‰æ–¹æ³•å®ç°äº†ISPå’ŒIEçš„åŠç›‘ç£å­¦ä¹ ï¼Œè§£å†³è¿™ä¸€éš¾é¢˜ã€‚æå‡ºçš„sRGB-to-RAWæ–¹æ³•èƒ½å¤Ÿæå‡ISPå’ŒIEä»»åŠ¡çš„å›¾åƒè´¨é‡ï¼Œå¹¶ç»“åˆåŠç›‘ç£å­¦ä¹ æˆåŠŸæé«˜äº†ä¸åŒæ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„å›¾åƒè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DNNæ–¹æ³•åœ¨ISPå’ŒIEä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†è®­ç»ƒæ•°æ®æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†ã€‚</li>
<li>ä¸ªæ€§åŒ–çš„ISPå’ŒIEéœ€æ±‚å› äººå’Œç”¨ä¾‹è€Œå¼‚ï¼Œå¯¹è®­ç»ƒæ•°æ®é‡æœ‰æ›´é«˜è¦æ±‚ã€‚</li>
<li>åŠç›‘ç£å­¦ä¹ åœ¨ISPå’ŒIEä»»åŠ¡ä¸­æœ‰æ½œåŠ›ï¼Œä½†åº”ç”¨è¾ƒå°‘ã€‚</li>
<li>å­˜åœ¨sRGB-to-RAWæ–¹æ³•ç”Ÿæˆä¼ªRAWå›¾åƒæ•°æ®é›†ï¼Œç”¨äºæé«˜åŸºäºRAWçš„é«˜çº§è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç°æœ‰sRGB-to-RAWæ–¹æ³•å¯¹äºéœ€è¦ç²¾ç¡®å›¾åƒè´¨é‡å®šä¹‰çš„ISPå’ŒIEä»»åŠ¡æ•ˆæœä¸è¶³ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„sRGB-to-RAWæ–¹æ³•ï¼Œèƒ½å¤Ÿæé«˜ISPå’ŒIEä»»åŠ¡çš„å›¾åƒè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02345">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0d04130b1b5a59212011b3c72b197b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d2910483caea962b72e59ec08473415.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93cef8097578a3fe9f72175e20ebb4eb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Beyond-Conventional-Transformers-The-Medical-X-ray-Attention-MXA-Block-for-Improved-Multi-Label-Diagnosis-Using-Knowledge-Distillation"><a href="#Beyond-Conventional-Transformers-The-Medical-X-ray-Attention-MXA-Block-for-Improved-Multi-Label-Diagnosis-Using-Knowledge-Distillation" class="headerlink" title="Beyond Conventional Transformers: The Medical X-ray Attention (MXA)   Block for Improved Multi-Label Diagnosis Using Knowledge Distillation"></a>Beyond Conventional Transformers: The Medical X-ray Attention (MXA)   Block for Improved Multi-Label Diagnosis Using Knowledge Distillation</h2><p><strong>Authors:Amit Rand, Hadi Ibrahim</strong></p>
<p>Medical imaging, particularly X-ray analysis, often involves detecting multiple conditions simultaneously within a single scan, making multi-label classification crucial for real-world clinical applications. We present the Medical X-ray Attention (MXA) block, a novel attention mechanism tailored specifically to address the unique challenges of X-ray abnormality detection. The MXA block enhances traditional Multi-Head Self Attention (MHSA) by integrating a specialized module that efficiently captures both detailed local information and broader global context. To the best of our knowledge, this is the first work to propose a task-specific attention mechanism for diagnosing chest X-rays, as well as to attempt multi-label classification using an Efficient Vision Transformer (EfficientViT). By embedding the MXA block within the EfficientViT architecture and employing knowledge distillation, our proposed model significantly improves performance on the CheXpert dataset, a widely used benchmark for multi-label chest X-ray abnormality detection. Our approach achieves an area under the curve (AUC) of 0.85, an absolute improvement of 0.19 compared to our baseline modelâ€™s AUC of 0.66, corresponding to a substantial approximate 233% relative improvement over random guessing (AUC &#x3D; 0.5). </p>
<blockquote>
<p>åŒ»å­¦æˆåƒï¼Œç‰¹åˆ«æ˜¯Xå°„çº¿åˆ†æï¼Œç»å¸¸éœ€è¦åœ¨å•æ¬¡æ‰«æä¸­æ£€æµ‹å¤šç§çŠ¶å†µï¼Œè¿™ä½¿å¾—å¤šæ ‡ç­¾åˆ†ç±»åœ¨ç°å®ä¸–ç•Œä¸´åºŠåº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†åŒ»å­¦Xå°„çº¿æ³¨æ„åŠ›ï¼ˆMXAï¼‰å—ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸“é—¨è®¾è®¡æ¥è§£å†³Xå°„çº¿å¼‚å¸¸æ£€æµ‹çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚MXAå—é€šè¿‡é›†æˆä¸€ä¸ªä¸“é—¨æ¨¡å—æ¥å¢å¼ºä¼ ç»Ÿçš„å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMHSAï¼‰ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿé«˜æ•ˆæ•è·è¯¦ç»†çš„å±€éƒ¨ä¿¡æ¯å’Œæ›´å¹¿æ³›çš„å…¨å±€ä¸Šä¸‹æ–‡ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡æå‡ºé’ˆå¯¹èƒ¸éƒ¨Xå°„çº¿è¯Šæ–­çš„ä»»åŠ¡ç‰¹å®šæ³¨æ„åŠ›æœºåˆ¶çš„å·¥ä½œï¼Œä¹Ÿæ˜¯é¦–æ¬¡å°è¯•ä½¿ç”¨é«˜æ•ˆè§†è§‰è½¬æ¢å™¨ï¼ˆEfficientViTï¼‰è¿›è¡Œå¤šæ ‡ç­¾åˆ†ç±»ã€‚é€šè¿‡å°†MXAå—åµŒå…¥EfficientViTæ¶æ„å¹¶é‡‡ç”¨çŸ¥è¯†è’¸é¦ï¼Œæˆ‘ä»¬æå‡ºçš„æ¨¡å‹åœ¨CheXpertæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼ŒCheXpertæ•°æ®é›†æ˜¯ç”¨äºå¤šæ ‡ç­¾èƒ¸éƒ¨Xå°„çº¿å¼‚å¸¸æ£€æµ‹çš„å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰ä¸º0.85ï¼Œä¸åŸºçº¿æ¨¡å‹çš„AUC 0.66ç›¸æ¯”ï¼Œç»å¯¹æé«˜äº†0.19ï¼Œè¿™æ„å‘³ç€ç›¸å¯¹äºéšæœºçŒœæµ‹æœ‰å¤§çº¦233%çš„ç›¸å¯¹æ”¹è¿›ï¼ˆAUC &#x3D; 0.5ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02277v1">PDF</a> 16 pages, 4 figures, 5 tables. For supplementary material and code,   see <a target="_blank" rel="noopener" href="https://github.com/Hadi-M-Ibrahim/Beyond-Conventional-Transformers/">https://github.com/Hadi-M-Ibrahim/Beyond-Conventional-Transformers/</a></p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé¢†åŸŸä¸­çš„Xå°„çº¿åˆ†æç»å¸¸éœ€è¦åŒæ—¶æ£€æµ‹å¤šç§ç–¾ç—…çŠ¶å†µï¼Œä¸ºæ­¤å¤šæ ‡ç­¾åˆ†ç±»æ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†Medical X-ray Attentionï¼ˆMXAï¼‰æ¨¡å—ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹Xå°„çº¿å¼‚å¸¸æ£€æµ‹ç‹¬ç‰¹æŒ‘æˆ˜çš„æ–°å‹æ³¨æ„åŠ›æœºåˆ¶ã€‚MXAæ¨¡å—é€šè¿‡é›†æˆä¸€ä¸ªä¸“é—¨è®¾è®¡çš„æ¨¡å—ï¼Œå¢å¼ºäº†ä¼ ç»Ÿçš„å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMHSAï¼‰ï¼Œä»¥æœ‰æ•ˆåœ°æ•è·è¯¦ç»†çš„å±€éƒ¨ä¿¡æ¯å’Œæ›´å¹¿æ³›çš„å…¨çƒä¸Šä¸‹æ–‡ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡æå‡ºç”¨äºè¯Šæ–­èƒ¸éƒ¨Xå°„çº¿çš„ç‰¹å®šä»»åŠ¡æ³¨æ„åŠ›æœºåˆ¶çš„å·¥ä½œï¼Œä¹Ÿæ˜¯é¦–æ¬¡å°è¯•ä½¿ç”¨é«˜æ•ˆè§†è§‰è½¬æ¢å™¨ï¼ˆEfficientViTï¼‰è¿›è¡Œå¤šæ ‡ç­¾åˆ†ç±»ã€‚é€šè¿‡åœ¨EfficientViTæ¶æ„ä¸­åµŒå…¥MXAæ¨¡å—å¹¶é‡‡ç”¨çŸ¥è¯†è’¸é¦çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ç”¨äºå¤šæ ‡ç­¾èƒ¸éƒ¨Xå°„çº¿å¼‚å¸¸æ£€æµ‹çš„å¹¿æ³›ä½¿ç”¨çš„CheXpertæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰ä¸º0.85ï¼Œä¸åŸºçº¿æ¨¡å‹çš„AUCç›¸æ¯”ï¼Œç»å¯¹æé«˜äº†0.19ï¼Œç›¸å¯¹äºéšæœºçŒœæµ‹çš„AUC &#x3D; 0.5ï¼Œæœ‰å¤§çº¦233%çš„ç›¸å¯¹æ”¹å–„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒ»å­¦å›¾åƒé¢†åŸŸä¸­çš„Xå°„çº¿åˆ†æéœ€è¦åŒæ—¶è¿›è¡Œå¤šæ ‡ç­¾åˆ†ç±»ä»¥åº”å¯¹å®é™…ä¸´åºŠåº”ç”¨ä¸­çš„å¤šç§ç–¾ç—…æ£€æµ‹æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†å…¨æ–°çš„Medical X-ray Attentionï¼ˆMXAï¼‰æ¨¡å—ï¼Œç»“åˆäº†MHSAå’Œä¸“é—¨è®¾è®¡çš„æ¨¡å—ä»¥æ•æ‰è¯¦ç»†ä¿¡æ¯å’Œå…¨å±€ä¸Šä¸‹æ–‡ã€‚</li>
<li>MXAæ¨¡å—é¦–æ¬¡è¢«åº”ç”¨äºèƒ¸éƒ¨Xå°„çº¿çš„è¯Šæ–­ä»»åŠ¡ä¸­ï¼Œå¹¶ä¸”ç»“åˆäº†EfficientViTè¿›è¡Œå¤šæ ‡ç­¾åˆ†ç±»ã€‚</li>
<li>åœ¨CheXpertæ•°æ®é›†ä¸Šï¼ŒåµŒå…¥MXAæ¨¡å—çš„æ¨¡å‹æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼ŒAUCè¾¾åˆ°äº†0.85ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02277">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d664e97a488765132ee5db7520d764d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09531855ecddab47b5fd30facad93917.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Investigating-the-Recursive-Short-X-ray-Burst-Behavior-of-Magnetars-Through-Crustal-Interactions"><a href="#Investigating-the-Recursive-Short-X-ray-Burst-Behavior-of-Magnetars-Through-Crustal-Interactions" class="headerlink" title="Investigating the Recursive Short X-ray Burst Behavior of Magnetars   Through Crustal Interactions"></a>Investigating the Recursive Short X-ray Burst Behavior of Magnetars   Through Crustal Interactions</h2><p><strong>Authors:Ozge Keskin, Samuel K. Lander, Ersin Gogus</strong></p>
<p>Energetic bursts from strongly magnetized neutron stars, known as magnetars, are typically detected in clusters. Once an active episode begins, anywhere from a few to thousands of hard X-ray bursts can occur over durations ranging from days to months. The temporal clustering of these recurrent bursts during an active episode suggests an underlying mechanism that triggers multiple bursts in rapid succession. These burst clusters are likely crucial for understanding the processes driving magnetar activity. In this study, we investigate the repetitive short X-ray burst behavior of magnetars through crustal interactions, employing the cellular automaton model for the magnetar crust proposed by Lander (2023). Our simulations, based on physically motivated criteria, successfully reproduce burst clustering. Additionally, the durations and energetics of active episodes in our simulations agree well with observational data. We discuss the potential physical mechanisms underlying burst clusters observed in numerous magnetars, as well as the reactivations of an individual magnetar. </p>
<blockquote>
<p>æ¥è‡ªå¼ºç£åŒ–ä¸­å­æ˜Ÿçš„èƒ½é‡çˆ†å‘ï¼Œè¢«ç§°ä¸ºç£æ˜Ÿï¼Œé€šå¸¸ä¼šåœ¨æ˜Ÿç¾¤ä¸­è¢«æ£€æµ‹åˆ°ã€‚ä¸€æ—¦æ´»è·ƒæœŸå¼€å§‹ï¼Œä»å‡ å¤©åˆ°å‡ ä¸ªæœˆçš„æ—¶é—´é‡Œï¼Œå¯èƒ½ä¼šçˆ†å‘ä»å‡ æ¬¡åˆ°æ•°åƒæ¬¡çš„ç¡¬Xå°„çº¿ã€‚è¿™äº›åå¤çˆ†å‘çš„æ—¶åºèšé›†è¡¨æ˜å­˜åœ¨ä¸€ä¸ªæ½œåœ¨æœºåˆ¶ï¼Œè§¦å‘è¿ç»­å¤šæ¬¡çˆ†å‘ã€‚è¿™äº›çˆ†å‘ç¾¤å¯¹äºç†è§£é©±åŠ¨ç£æ˜Ÿæ´»åŠ¨çš„è¿‡ç¨‹å¯èƒ½æ˜¯è‡³å…³é‡è¦çš„ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡é‡‡ç”¨åœ°å£³ç›¸äº’ä½œç”¨æ¥æ¢ç©¶ç£æ˜Ÿçš„é‡å¤æ€§çŸ­æœŸXå°„çº¿çˆ†å‘è¡Œä¸ºï¼Œè¿ç”¨Landerï¼ˆ2023ï¼‰ä¸ºç£æ˜Ÿåœ°å£³æå‡ºçš„å…ƒèƒè‡ªåŠ¨æœºæ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¨¡æ‹ŸåŸºäºç‰©ç†é©±åŠ¨æ ‡å‡†ï¼ŒæˆåŠŸå†ç°äº†çˆ†å‘èšé›†ç°è±¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡æ‹Ÿä¸­çš„æ´»è·ƒæœŸæ—¶é•¿å’Œèƒ½é‡ä¸è§‚æµ‹æ•°æ®éå¸¸å»åˆã€‚æˆ‘ä»¬è®¨è®ºäº†ä¼—å¤šç£æ˜Ÿè§‚æµ‹åˆ°çš„çˆ†å‘ç¾¤ä»¥åŠå•ä¸ªç£æ˜Ÿé‡æ–°æ´»åŒ–èƒŒåçš„æ½œåœ¨ç‰©ç†æœºåˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02140v1">PDF</a> Accepted for publication in ApJ</p>
<p><strong>Summary</strong></p>
<p>æ­¤æ–‡æœ¬æè¿°äº†å¼ºç£åŒ–ä¸­å­æ˜Ÿï¼ˆä¹Ÿç§°ä¸ºç£æ˜Ÿï¼‰çš„èƒ½é‡çˆ†å‘ï¼Œè¿™äº›çˆ†å‘é€šå¸¸ä»¥é›†ç¾¤çš„å½¢å¼å‡ºç°ã€‚æ´»è·ƒæœŸå¼€å§‹åï¼Œå¯èƒ½äº§ç”Ÿæ•°é‡ä¸ç­‰çš„ç¡¬Xå°„çº¿çˆ†å‘ï¼ŒæŒç»­æ—¶é—´ä»å‡ å¤©åˆ°å‡ ä¸ªæœˆä¸ç­‰ã€‚çˆ†å‘é›†ç¾¤è¡¨æ˜å­˜åœ¨è§¦å‘å¤šæ¬¡å¿«é€Ÿè¿ç»­çš„çˆ†å‘æœºåˆ¶ã€‚å¯¹äºç†è§£ç£æ˜Ÿæ´»åŠ¨çš„é©±åŠ¨è¿‡ç¨‹ï¼Œè¿™äº›çˆ†å‘é›†ç¾¤è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶é€šè¿‡æ¨¡æ‹Ÿç£æ˜Ÿåœ°å£³ç›¸äº’ä½œç”¨æ¥ç ”ç©¶ç£æ˜Ÿçš„é‡å¤æ€§çŸ­Xå°„çº¿çˆ†å‘è¡Œä¸ºï¼Œé‡‡ç”¨Landerï¼ˆ2023ï¼‰æå‡ºçš„ç£æ˜Ÿåœ°å£³çš„å…ƒèƒè‡ªåŠ¨æœºæ¨¡å‹ã€‚åŸºäºç‰©ç†é©±åŠ¨æ ‡å‡†çš„æ¨¡æ‹ŸæˆåŠŸå†ç°äº†çˆ†å‘é›†ç¾¤ï¼Œå¹¶ä¸”æ¨¡æ‹Ÿçš„æ´»è·ƒæœŸæŒç»­æ—¶é—´ä¸è§‚æµ‹æ•°æ®å»åˆè‰¯å¥½ã€‚æˆ‘ä»¬è®¨è®ºäº†ç£æ˜Ÿä¸­è§‚å¯Ÿåˆ°çš„çˆ†å‘é›†ç¾¤ä»¥åŠå•ä¸ªç£æ˜Ÿé‡æ–°æ¿€æ´»çš„æ½œåœ¨ç‰©ç†æœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç£æ˜Ÿçš„èƒ½é‡çˆ†å‘é€šå¸¸ä»¥é›†ç¾¤å½¢å¼å‡ºç°ã€‚</li>
<li>æ´»è·ƒæœŸå¼€å§‹åï¼Œä¼šæœ‰å¤šæ¬¡ç¡¬Xå°„çº¿çˆ†å‘ï¼ŒæŒç»­æ—¶é—´ä¸ä¸€ã€‚</li>
<li>çˆ†å‘é›†ç¾¤è¡¨æ˜å­˜åœ¨è§¦å‘å¤šæ¬¡å¿«é€Ÿè¿ç»­çš„çˆ†å‘çš„æœºåˆ¶ã€‚</li>
<li>çˆ†å‘é›†ç¾¤å¯¹äºç†è§£ç£æ˜Ÿæ´»åŠ¨çš„é©±åŠ¨è¿‡ç¨‹è‡³å…³é‡è¦ã€‚</li>
<li>é‡‡ç”¨å…ƒèƒè‡ªåŠ¨æœºæ¨¡å‹æ¨¡æ‹Ÿç£æ˜Ÿåœ°å£³ç›¸äº’ä½œç”¨æ¥ç ”ç©¶å…¶é‡å¤æ€§çŸ­Xå°„çº¿çˆ†å‘è¡Œä¸ºã€‚</li>
<li>æ¨¡æ‹ŸæˆåŠŸå†ç°äº†çˆ†å‘é›†ç¾¤ï¼Œæ´»è·ƒæœŸæŒç»­æ—¶é—´ä¸è§‚æµ‹æ•°æ®å»åˆè‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d592ccd13c25252790c3cac0d061af88.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Diffusion-models-applied-to-skin-and-oral-cancer-classification"><a href="#Diffusion-models-applied-to-skin-and-oral-cancer-classification" class="headerlink" title="Diffusion models applied to skin and oral cancer classification"></a>Diffusion models applied to skin and oral cancer classification</h2><p><strong>Authors:JosÃ© J. M. Uliana, Renato A. Krohling</strong></p>
<p>This study investigates the application of diffusion models in medical image classification (DiffMIC), focusing on skin and oral lesions. Utilizing the datasets PAD-UFES-20 for skin cancer and P-NDB-UFES for oral cancer, the diffusion model demonstrated competitive performance compared to state-of-the-art deep learning models like Convolutional Neural Networks (CNNs) and Transformers. Specifically, for the PAD-UFES-20 dataset, the model achieved a balanced accuracy of 0.6457 for six-class classification and 0.8357 for binary classification (cancer vs. non-cancer). For the P-NDB-UFES dataset, it attained a balanced accuracy of 0.9050. These results suggest that diffusion models are viable models for classifying medical images of skin and oral lesions. In addition, we investigate the robustness of the model trained on PAD-UFES-20 for skin cancer but tested on the clinical images of the HIBA dataset. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ï¼ˆDiffMICï¼‰ä¸­çš„åº”ç”¨ï¼Œé‡ç‚¹å…³æ³¨çš®è‚¤å’Œå£è…”ç—…å˜ã€‚ç ”ç©¶ä½¿ç”¨PAD-UFES-20æ•°æ®é›†å¯¹çš®è‚¤ç™Œè¿›è¡Œç ”ç©¶ï¼Œä»¥åŠä½¿ç”¨P-NDB-UFESæ•°æ®é›†å¯¹å£è…”ç™Œè¿›è¡Œç ”ç©¶ã€‚æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ä¸å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒTransformerï¼‰ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œå¯¹äºPAD-UFES-20æ•°æ®é›†ï¼Œè¯¥æ¨¡å‹åœ¨å…­ç±»åˆ†ç±»ä¸­å®ç°äº†0.6457çš„å¹³è¡¡å‡†ç¡®ç‡ï¼Œåœ¨äºŒå…ƒåˆ†ç±»ï¼ˆç™Œç—‡ä¸éç™Œç—‡ï¼‰ä¸­å®ç°äº†0.8357çš„å¹³è¡¡å‡†ç¡®ç‡ã€‚å¯¹äºP-NDB-UFESæ•°æ®é›†ï¼Œå®ƒè¾¾åˆ°äº†0.9050çš„å¹³è¡¡å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹æ˜¯åˆ†ç±»çš®è‚¤å’Œå£è…”ç—…å˜åŒ»å­¦å›¾åƒçš„å¯è¡Œæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†åœ¨PAD-UFES-20æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹å¯¹HIBAä¸´åºŠå›¾åƒæ•°æ®é›†çš„é²æ£’æ€§æµ‹è¯•ã€‚è¯¥æ¨¡å‹åœ¨ä¸´åºŠå›¾åƒä¸Šçš„è¡¨ç°è¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00026v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ï¼ˆDiffMICï¼‰ä¸­çš„åº”ç”¨ï¼Œé‡ç‚¹ç ”ç©¶çš®è‚¤ä¸å£è…”ç—…ç¶ã€‚ç ”ç©¶ä½¿ç”¨PAD-UFES-20çš®è‚¤ç™Œæ•°æ®é›†å’ŒP-NDB-UFESå£è…”ç™Œæ•°æ®é›†ï¼Œæ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ä¸å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’ŒTransformerï¼‰ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚å¯¹äºPAD-UFES-20æ•°æ®é›†ï¼Œè¯¥æ¨¡å‹åœ¨å…­ç±»åˆ†ç±»ä¸­è¾¾åˆ°0.6457çš„å¹³è¡¡å‡†ç¡®ç‡ï¼Œåœ¨äºŒå…ƒåˆ†ç±»ï¼ˆç™Œç—‡ä¸éç™Œç—‡ï¼‰ä¸­è¾¾åˆ°0.8357çš„å¹³è¡¡å‡†ç¡®ç‡ã€‚å¯¹äºP-NDB-UFESæ•°æ®é›†ï¼Œå…¶å¹³è¡¡å‡†ç¡®ç‡ä¸º0.9050ã€‚è¿™äº›ç»“æœè¡¨æ˜æ‰©æ•£æ¨¡å‹æ˜¯åˆ†ç±»çš®è‚¤å’Œå£è…”ç—…å˜åŒ»å­¦å›¾åƒçš„å¯è¡Œæ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¢è®¨äº†ç”¨PAD-UFES-20çš®è‚¤ç™Œæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹åœ¨HIBAä¸´åºŠå›¾åƒä¸Šçš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨çš®è‚¤å’Œå£è…”ç—…ç¶çš„åˆ†ç±»ä¸Šã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†PAD-UFES-20çš®è‚¤ç™Œæ•°æ®é›†å’ŒP-NDB-UFESå£è…”ç™Œæ•°æ®é›†è¿›è¡Œæ‰©æ•£æ¨¡å‹çš„è®­ç»ƒå’ŒéªŒè¯ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨åˆ†ç±»æ€§èƒ½ä¸Šå±•ç°å‡ºç«äº‰åŠ›ï¼Œä¸å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹å¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’ŒTransformerç›¸æ¯”è¡¨ç°è‰¯å¥½ã€‚</li>
<li>åœ¨PAD-UFES-20æ•°æ®é›†ä¸Šï¼Œæ‰©æ•£æ¨¡å‹å®ç°äº†è¾ƒé«˜çš„å¹³è¡¡å‡†ç¡®ç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨äºŒå…ƒåˆ†ç±»ä»»åŠ¡ä¸­ã€‚</li>
<li>åœ¨P-NDB-UFESæ•°æ®é›†ä¸Šï¼Œæ‰©æ•£æ¨¡å‹çš„å¹³è¡¡å‡†ç¡®ç‡è¾¾åˆ°äº†è¾ƒé«˜çš„æ°´å¹³ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜æ‰©æ•£æ¨¡å‹æ˜¯åˆ†ç±»åŒ»å­¦å›¾åƒçš„å¯è¡Œæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨çš®è‚¤å’Œå£è…”ç—…ç¶çš„åˆ†ç±»ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00026">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-45e539abd1fadc02c4ec475bb28202bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7dd24cf7dfa28280ae1d446829db07c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfd6dcf829db92965091830416fe0fdc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-262bfaff5d57d210b6f8297f3c77434d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1744eff4ddbeb0f30962a4d4376bfff1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f39ee8e49d30c70ec3c266f35784dc2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92ea8999c3130dba5e9748c3cb6b5643.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c50c1cf9d2ab834935bf39946d6c810b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c473263dca5b6d4a3d5e9f5d5a219b9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MedCL-Learning-Consistent-Anatomy-Distribution-for-Scribble-supervised-Medical-Image-Segmentation"><a href="#MedCL-Learning-Consistent-Anatomy-Distribution-for-Scribble-supervised-Medical-Image-Segmentation" class="headerlink" title="MedCL: Learning Consistent Anatomy Distribution for Scribble-supervised   Medical Image Segmentation"></a>MedCL: Learning Consistent Anatomy Distribution for Scribble-supervised   Medical Image Segmentation</h2><p><strong>Authors:Ke Zhang, Vishal M. Patel</strong></p>
<p>Curating large-scale fully annotated datasets is expensive, laborious, and cumbersome, especially for medical images. Several methods have been proposed in the literature that make use of weak annotations in the form of scribbles. However, these approaches require large amounts of scribble annotations, and are only applied to the segmentation of regular organs, which are often unavailable for the disease species that fall in the long-tailed distribution. Motivated by the fact that the medical labels have anatomy distribution priors, we propose a scribble-supervised clustering-based framework, called MedCL, to learn the inherent anatomy distribution of medical labels. Our approach consists of two steps: i) Mix the features with intra- and inter-image mix operations, and ii) Perform feature clustering and regularize the anatomy distribution at both local and global levels. Combined with a small amount of weak supervision, the proposed MedCL is able to segment both regular organs and challenging irregular pathologies. We implement MedCL based on SAM and UNet backbones, and evaluate the performance on three open datasets of regular structure (MSCMRseg), multiple organs (BTCV) and irregular pathology (MyoPS). It is shown that even with less scribble supervision, MedCL substantially outperforms the conventional segmentation methods. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/BWGZK/MedCL">https://github.com/BWGZK/MedCL</a>. </p>
<blockquote>
<p>æ„å»ºå¤§è§„æ¨¡å…¨æ³¨é‡Šæ•°æ®é›†æ—¢æ˜‚è´µåˆè€—æ—¶ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸã€‚å°½ç®¡æ–‡çŒ®ä¸­æå‡ºäº†å‡ ç§åˆ©ç”¨æ¶‚é¸¦å½¢å¼å¼±æ³¨é‡Šçš„æ–¹æ³•ï¼Œä½†è¿™äº›æ–¹æ³•éœ€è¦å¤§é‡æ¶‚é¸¦æ³¨é‡Šï¼Œå¹¶ä¸”ä»…åº”ç”¨äºå¸¸è§„å™¨å®˜çš„åˆ†å‰²ï¼Œå¯¹äºé•¿å°¾åˆ†å¸ƒä¸­çš„ç–¾ç—…ç§ç±»åˆ™æ— æ³•é€‚ç”¨ã€‚å—åˆ°åŒ»å­¦æ ‡ç­¾å…·æœ‰è§£å‰–å­¦åˆ†å¸ƒå…ˆéªŒäº‹å®å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ¶‚é¸¦ç›‘ç£èšç±»çš„æ¡†æ¶ï¼Œåä¸ºMedCLï¼Œç”¨äºå­¦ä¹ åŒ»å­¦æ ‡ç­¾çš„å›ºæœ‰è§£å‰–å­¦åˆ†å¸ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ†ä¸ºä¸¤æ­¥ï¼šiï¼‰é€šè¿‡å›¾åƒå†…å’Œå›¾åƒé—´æ··åˆæ“ä½œæ··åˆç‰¹å¾ï¼›iiï¼‰æ‰§è¡Œç‰¹å¾èšç±»å¹¶åœ¨å±€éƒ¨å’Œå…¨å±€å±‚é¢è°ƒæ•´è§£å‰–å­¦åˆ†å¸ƒã€‚ç»“åˆå°‘é‡çš„å¼±ç›‘ç£ï¼Œæ‰€æå‡ºMedCLèƒ½å¤Ÿå¯¹å¸¸è§„å™¨å®˜å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„ä¸è§„åˆ™ç—…ç†è¿›è¡Œåˆ†å‰²ã€‚æˆ‘ä»¬åŸºäºSAMå’ŒUNetéª¨å¹²ç½‘å®ç°äº†MedCLï¼Œå¹¶åœ¨ä¸‰ä¸ªå¼€æ”¾æ•°æ®é›†ï¼ˆå¸¸è§„ç»“æ„MSCMRsegã€å¤šå™¨å®˜BTCVå’Œä¸è§„åˆ™ç—…ç†MyoPSï¼‰ä¸Šè¯„ä¼°å…¶æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œå³ä½¿ä½¿ç”¨è¾ƒå°‘çš„æ¶‚é¸¦ç›‘ç£ï¼ŒMedCLä¹Ÿå¤§å¤§ä¼˜äºä¼ ç»Ÿåˆ†å‰²æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/BWGZK/MedCL%E3%80%82">https://github.com/BWGZK/MedCLã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22890v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ¶‚é¸¦ç›‘ç£çš„èšç±»æ–¹æ³•ï¼ˆMedCLï¼‰ï¼Œç”¨äºå­¦ä¹ åŒ»å­¦æ ‡ç­¾çš„å›ºæœ‰è§£å‰–åˆ†å¸ƒã€‚è¯¥æ–¹æ³•æ··åˆå›¾åƒå†…å’Œå›¾åƒé—´çš„ç‰¹å¾ï¼Œè¿›è¡Œç‰¹å¾èšç±»ï¼Œå¹¶åœ¨å±€éƒ¨å’Œå…¨å±€å±‚é¢è°ƒæ•´è§£å‰–åˆ†å¸ƒã€‚ç»“åˆå°‘é‡å¼±ç›‘ç£ï¼ŒMedCLèƒ½å¤Ÿåˆ†å‰²å¸¸è§„å™¨å®˜å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„ä¸è§„åˆ™ç—…ç†ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå³ä½¿ä½¿ç”¨è¾ƒå°‘çš„æ¶‚é¸¦ç›‘ç£ï¼ŒMedCLä¹Ÿèƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿåˆ†å‰²æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedCLæ˜¯ä¸€ç§åŸºäºæ¶‚é¸¦ç›‘ç£çš„èšç±»æ–¹æ³•ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>MedCLåˆ©ç”¨åŒ»å­¦æ ‡ç­¾çš„è§£å‰–åˆ†å¸ƒå…ˆéªŒè¿›è¡Œå­¦ä¹ ã€‚</li>
<li>MedCLåˆ†ä¸ºä¸¤æ­¥ï¼šç‰¹å¾æ··åˆå’Œç‰¹å¾èšç±»ã€‚</li>
<li>ç‰¹å¾æ··åˆåŒ…æ‹¬å›¾åƒå†…å’Œå›¾åƒé—´çš„æ“ä½œã€‚</li>
<li>èšç±»åè¿›è¡Œå±€éƒ¨å’Œå…¨å±€çš„è§£å‰–åˆ†å¸ƒè°ƒæ•´ã€‚</li>
<li>ç»“åˆå°‘é‡å¼±ç›‘ç£ï¼ŒMedCLèƒ½åˆ†å‰²å¸¸è§„å™¨å®˜å’Œä¸è§„åˆ™ç—…ç†ã€‚</li>
<li>åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMedCLæ€§èƒ½ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22890">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6dccc8c2e40b9ef75984a554230dd350.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c94fe7e5a18327c71eb3b24560f3229b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-afe6a49f50d59998076f503c7945434e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Deterministic-Medical-Image-Translation-via-High-fidelity-Brownian-Bridges"><a href="#Deterministic-Medical-Image-Translation-via-High-fidelity-Brownian-Bridges" class="headerlink" title="Deterministic Medical Image Translation via High-fidelity Brownian   Bridges"></a>Deterministic Medical Image Translation via High-fidelity Brownian   Bridges</h2><p><strong>Authors:Qisheng He, Nicholas Summerfield, Peiyong Wang, Carri Glide-Hurst, Ming Dong</strong></p>
<p>Recent studies have shown that diffusion models produce superior synthetic images when compared to Generative Adversarial Networks (GANs). However, their outputs are often non-deterministic and lack high fidelity to the ground truth due to the inherent randomness. In this paper, we propose a novel High-fidelity Brownian bridge model (HiFi-BBrg) for deterministic medical image translations. Our model comprises two distinct yet mutually beneficial mappings: a generation mapping and a reconstruction mapping. The Brownian bridge training process is guided by the fidelity loss and adversarial training in the reconstruction mapping. This ensures that translated images can be accurately reversed to their original forms, thereby achieving consistent translations with high fidelity to the ground truth. Our extensive experiments on multiple datasets show HiFi-BBrg outperforms state-of-the-art methods in multi-modal image translation and multi-image super-resolution. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ç›¸æ¯”ï¼Œæ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ›´ä¼˜è´¨çš„åˆæˆå›¾åƒã€‚ç„¶è€Œï¼Œç”±äºå…¶å›ºæœ‰çš„éšæœºæ€§ï¼Œå…¶è¾“å‡ºé€šå¸¸æ˜¯éç¡®å®šçš„ï¼Œå¹¶ä¸”ä¸çœŸå®æƒ…å†µçš„é«˜ä¿çœŸåº¦æœ‰æ‰€ç¼ºå¤±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºç¡®å®šæ€§åŒ»å­¦å›¾åƒç¿»è¯‘çš„é«˜ä¿çœŸå¸ƒæœ—æ¡¥æ¨¡å‹ï¼ˆHiFi-BBrgï¼‰ã€‚æˆ‘ä»¬çš„æ¨¡å‹åŒ…å«ä¸¤ç§ä¸åŒä½†ç›¸äº’ä¿ƒè¿›çš„æ˜ å°„ï¼šç”Ÿæˆæ˜ å°„å’Œé‡å»ºæ˜ å°„ã€‚å¸ƒæœ—æ¡¥è®­ç»ƒè¿‡ç¨‹ç”±é‡å»ºæ˜ å°„ä¸­çš„ä¿çœŸåº¦æŸå¤±å’Œå¯¹æŠ—è®­ç»ƒå¼•å¯¼ï¼Œè¿™ç¡®ä¿äº†ç¿»è¯‘åçš„å›¾åƒå¯ä»¥å‡†ç¡®åœ°æ¢å¤åˆ°å…¶åŸå§‹å½¢å¼ï¼Œä»è€Œå®ç°ä¸çœŸå®æƒ…å†µé«˜åº¦ä¸€è‡´çš„ç¿»è¯‘ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒHiFi-BBrgåœ¨è·¨æ¨¡æ€å›¾åƒç¿»è¯‘å’Œå¤šå›¾åƒè¶…åˆ†è¾¨ç‡æ–¹é¢ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22531v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ç¡®å®šæ€§åŒ»å­¦å›¾åƒç¿»è¯‘æ¨¡å‹â€”â€”é«˜ä¿çœŸå¸ƒæœ—æ¡¥æ¨¡å‹ï¼ˆHiFi-BBrgï¼‰ã€‚è¯¥æ¨¡å‹é€šè¿‡ç”Ÿæˆæ˜ å°„å’Œé‡å»ºæ˜ å°„ä¸¤ä¸ªç›¸äº’è¡¥å……çš„è¿‡ç¨‹ï¼Œå®ç°äº†åŒ»å­¦å›¾åƒçš„ç¡®å®šæ€§ç¿»è¯‘ï¼Œå¹¶é€šè¿‡å¯¹é‡å»ºæ˜ å°„ä¸­çš„ä¿çœŸåº¦æŸå¤±å’Œå¯¹æŠ—æ€§è®­ç»ƒè¿›è¡Œå¼•å¯¼ï¼Œç¡®ä¿äº†ç¿»è¯‘åçš„å›¾åƒèƒ½å¤Ÿå‡†ç¡®è¿˜åŸä¸ºåŸå§‹å½¢å¼ï¼Œå®ç°äº†é«˜ä¿çœŸåº¦çš„å›¾åƒç¿»è¯‘ã€‚å®éªŒè¯æ˜ï¼ŒHiFi-BBrgåœ¨å¤šæ¨¡æ€å›¾åƒç¿»è¯‘å’Œå¤šå›¾åƒè¶…åˆ†è¾¨ç‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆåˆæˆå›¾åƒæ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œä½†è¾“å‡ºå¸¸å…·æœ‰éç¡®å®šæ€§å’Œä½ä¿çœŸåº¦ã€‚</li>
<li>æå‡ºäº†é«˜ä¿çœŸå¸ƒæœ—æ¡¥æ¨¡å‹ï¼ˆHiFi-BBrgï¼‰ç”¨äºç¡®å®šæ€§åŒ»å­¦å›¾åƒç¿»è¯‘ã€‚</li>
<li>HiFi-BBrgæ¨¡å‹åŒ…å«ç”Ÿæˆæ˜ å°„å’Œé‡å»ºæ˜ å°„ä¸¤ä¸ªäº’è¡¥éƒ¨åˆ†ã€‚</li>
<li>é€šè¿‡é‡å»ºæ˜ å°„ä¸­çš„å¸ƒæœ—æ¡¥è®­ç»ƒè¿‡ç¨‹ï¼ŒHiFi-BBrgç¡®ä¿äº†å›¾åƒç¿»è¯‘çš„å¯é€†æ€§ã€‚</li>
<li>HiFi-BBrgæ¨¡å‹å®ç°äº†é«˜ä¿çœŸåº¦çš„å›¾åƒç¿»è¯‘ï¼Œä¿æŒä¸åŸå§‹å›¾åƒçš„ç›¸ä¼¼æ€§ã€‚</li>
<li>å¹¿æ³›å®éªŒè¯æ˜HiFi-BBrgåœ¨å¤šæ¨¡æ€å›¾åƒç¿»è¯‘å’Œå¤šå›¾åƒè¶…åˆ†è¾¨ç‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22531">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-89b720f2289ff49b5d424b1d1bb39466.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42c63a9a154920b748723ea6f60d03e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10b4c05035d61db74240beef8ac3ded3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74dd698f33980edc4ea7e2552f99da80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c4519fed2f33f938908a378ab11a3e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5c85f0176bb823177a0938160c03c26.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ArchCAD-400K-An-Open-Large-Scale-Architectural-CAD-Dataset-and-New-Baseline-for-Panoptic-Symbol-Spotting"><a href="#ArchCAD-400K-An-Open-Large-Scale-Architectural-CAD-Dataset-and-New-Baseline-for-Panoptic-Symbol-Spotting" class="headerlink" title="ArchCAD-400K: An Open Large-Scale Architectural CAD Dataset and New   Baseline for Panoptic Symbol Spotting"></a>ArchCAD-400K: An Open Large-Scale Architectural CAD Dataset and New   Baseline for Panoptic Symbol Spotting</h2><p><strong>Authors:Ruifeng Luo, Zhengjie Liu, Tianxiao Cheng, Jie Wang, Tongjie Wang, Xingguang Wei, Haomin Wang, YanPeng Li, Fu Chai, Fei Cheng, Shenglong Ye, Wenhai Wang, Yanting Zhang, Yu Qiao, Hongjie Zhang, Xianzhong Zhao</strong></p>
<p>Recognizing symbols in architectural CAD drawings is critical for various advanced engineering applications. In this paper, we propose a novel CAD data annotation engine that leverages intrinsic attributes from systematically archived CAD drawings to automatically generate high-quality annotations, thus significantly reducing manual labeling efforts. Utilizing this engine, we construct ArchCAD-400K, a large-scale CAD dataset consisting of 413,062 chunks from 5538 highly standardized drawings, making it over 26 times larger than the largest existing CAD dataset. ArchCAD-400K boasts an extended drawing diversity and broader categories, offering line-grained annotations. Furthermore, we present a new baseline model for panoptic symbol spotting, termed Dual-Pathway Symbol Spotter (DPSS). It incorporates an adaptive fusion module to enhance primitive features with complementary image features, achieving state-of-the-art performance and enhanced robustness. Extensive experiments validate the effectiveness of DPSS, demonstrating the value of ArchCAD-400K and its potential to drive innovation in architectural design and construction. </p>
<blockquote>
<p>è¯†åˆ«å»ºç­‘CADå›¾çº¸ä¸­çš„ç¬¦å·å¯¹äºå„ç§é«˜çº§å·¥ç¨‹åº”ç”¨è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„CADæ•°æ®æ ‡æ³¨å¼•æ“ï¼Œè¯¥å¼•æ“åˆ©ç”¨ç³»ç»Ÿå½’æ¡£çš„CADå›¾çº¸ä¸­çš„å†…åœ¨å±æ€§ï¼Œè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡æ ‡æ³¨ï¼Œä»è€Œæ˜¾è‘—å‡å°‘æ‰‹åŠ¨æ ‡æ³¨çš„å·¥ä½œé‡ã€‚åˆ©ç”¨æ­¤å¼•æ“ï¼Œæˆ‘ä»¬æ„å»ºäº†ArchCAD-400Kï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„CADæ•°æ®é›†ï¼Œç”±5538å¼ é«˜åº¦æ ‡å‡†åŒ–çš„å›¾çº¸ä¸­çš„413,062ä¸ªæ•°æ®å—ç»„æˆï¼Œæ˜¯ç°æœ‰æœ€å¤§CADæ•°æ®é›†çš„26å€ä»¥ä¸Šã€‚ArchCAD-400Kæ‹¥æœ‰æ‰©å±•çš„ç»˜å›¾å¤šæ ·æ€§å’Œæ›´å¹¿æ³›çš„ç±»åˆ«ï¼Œæä¾›ç²¾ç»†çš„çº¿æ¡æ ‡æ³¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºå…¨æ™¯ç¬¦å·æ£€æµ‹ä»»åŠ¡æå‡ºäº†ä¸€ç§æ–°çš„åŸºçº¿æ¨¡å‹ï¼Œç§°ä¸ºåŒè·¯å¾„ç¬¦å·æ£€æµ‹å™¨ï¼ˆDPSSï¼‰ã€‚å®ƒç»“åˆäº†ä¸€ä¸ªè‡ªé€‚åº”èåˆæ¨¡å—ï¼Œä»¥å¢å¼ºåŸå§‹ç‰¹å¾ä¸äº’è¡¥å›¾åƒç‰¹å¾çš„èåˆï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½å’Œå¢å¼ºçš„ç¨³å¥æ€§ã€‚å¤§é‡å®éªŒéªŒè¯äº†DPSSçš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†ArchCAD-400Kçš„ä»·å€¼åŠå…¶åœ¨å»ºç­‘è®¾è®¡é¢†åŸŸçš„æ½œåŠ›ä»¥åŠæ¨åŠ¨åˆ›æ–°çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22346v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„CADæ•°æ®æ ‡æ³¨å¼•æ“ï¼Œè¯¥å¼•æ“åˆ©ç”¨ç³»ç»Ÿå½’æ¡£çš„CADå›¾çº¸ä¸­çš„å†…åœ¨å±æ€§è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡æ ‡æ³¨ï¼Œæ˜¾è‘—å‡å°‘æ‰‹åŠ¨æ ‡æ³¨å·¥ä½œé‡ã€‚åŸºäºæ­¤å¼•æ“æ„å»ºçš„å¤§å‹CADæ•°æ®é›†ArchCAD-400Kï¼ŒåŒ…å«ä»é«˜åº¦æ ‡å‡†åŒ–çš„å›¾çº¸ä¸­æå–çš„413,062ä¸ªæ•°æ®å—ï¼Œè¾ƒç°æœ‰çš„æœ€å¤§CADæ•°æ®é›†å¤§å‡ºé€¾26å€ã€‚ArchCAD-400Kå±•ç°å‡ºçš„ç»˜å›¾å¤šæ ·æ€§å’Œæ›´å¹¿æ³›çš„ç±»åˆ«ï¼Œæä¾›äº†è¯¦ç»†çš„æ ‡æ³¨ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†å…¨æ–°çš„å…¨æ™¯ç¬¦å·è¯†åˆ«åŸºå‡†æ¨¡å‹â€”â€”åŒè·¯å¾„ç¬¦å·è¯†åˆ«å™¨ï¼ˆDPSSï¼‰ï¼Œå®ƒé€šè¿‡è‡ªé€‚åº”èåˆæ¨¡å—å¼ºåŒ–åŸå§‹ç‰¹å¾ä¸äº’è¡¥å›¾åƒç‰¹å¾çš„èåˆï¼Œå®ç°äº†ä¸šç•Œé¢†å…ˆçš„æ€§èƒ½å’Œç¨³å¥æ€§ã€‚å¤šé¡¹å®éªŒéªŒè¯äº†DPSSçš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†ArchCAD-400Kçš„ä»·å€¼åŠå…¶åœ¨å»ºç­‘è®¾è®¡é¢†åŸŸçš„åˆ›æ–°æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CADæ•°æ®æ ‡æ³¨å¼•æ“èƒ½åˆ©ç”¨CADå›¾çº¸çš„å†…åœ¨å±æ€§è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡æ ‡æ³¨ï¼Œé™ä½æ‰‹åŠ¨æ ‡æ³¨å·¥ä½œé‡ã€‚</li>
<li>ArchCAD-400Kæ•°æ®é›†æ˜¯å¤§å‹çš„CADæ•°æ®é›†ï¼Œç”±é«˜åº¦æ ‡å‡†åŒ–çš„å›¾çº¸æ„æˆï¼Œæ¯”ç°æœ‰æ•°æ®é›†è§„æ¨¡æ›´å¤§ã€‚</li>
<li>ArchCAD-400Kæä¾›å¤šæ ·çš„ç»˜å›¾å’Œå¹¿æ³›çš„ç±»åˆ«ï¼Œå…·æœ‰è¯¦ç»†çš„æ ‡æ³¨ã€‚</li>
<li>æå‡ºäº†å…¨æ–°çš„å…¨æ™¯ç¬¦å·è¯†åˆ«æ¨¡å‹â€”â€”åŒè·¯å¾„ç¬¦å·è¯†åˆ«å™¨ï¼ˆDPSSï¼‰ã€‚</li>
<li>DPSSæ¨¡å‹èƒ½èåˆåŸå§‹ç‰¹å¾å’Œäº’è¡¥å›¾åƒç‰¹å¾ï¼Œå¢å¼ºè¯†åˆ«æ€§èƒ½ã€‚</li>
<li>DPSSæ¨¡å‹å®ç°äº†ä¸šç•Œé¢†å…ˆçš„æ€§èƒ½å’Œç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22346">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1a1bd29c1e4421ac63a9566fb09477e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3436732cfbe99a1400b5c0a0a66f443.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-917ce01fca627b9ed44e50ca2f53fe00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87db798d15356e64a92dde81090c2fa9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0605cfd8df631b234133fcbcaf62fb91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e1fd165086493b1ad30514a8333962b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a9c304db735b649b87f2267c7c745c4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9a3f196f0c5e3eaed27e9544aa01819c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Eyes-Tell-the-Truth-GazeVal-Highlights-Shortcomings-of-Generative-AI-in-Medical-Imaging"><a href="#Eyes-Tell-the-Truth-GazeVal-Highlights-Shortcomings-of-Generative-AI-in-Medical-Imaging" class="headerlink" title="Eyes Tell the Truth: GazeVal Highlights Shortcomings of Generative AI in   Medical Imaging"></a>Eyes Tell the Truth: GazeVal Highlights Shortcomings of Generative AI in   Medical Imaging</h2><p><strong>Authors:David Wong, Bin Wang, Gorkem Durak, Marouane Tliba, Akshay Chaudhari, Aladine Chetouani, Ahmet Enis Cetin, Cagdas Topel, Nicolo Gennaro, Camila Lopes Vendrami, Tugce Agirlar Trabzonlu, Amir Ali Rahsepar, Laetitia Perronne, Matthew Antalek, Onural Ozturk, Gokcan Okur, Andrew C. Gordon, Ayis Pyrros, Frank H. Miller, Amir Borhani, Hatice Savas, Eric Hart, Drew Torigian, Jayaram K. Udupa, Elizabeth Krupinski, Ulas Bagci</strong></p>
<p>The demand for high-quality synthetic data for model training and augmentation has never been greater in medical imaging. However, current evaluations predominantly rely on computational metrics that fail to align with human expert recognition. This leads to synthetic images that may appear realistic numerically but lack clinical authenticity, posing significant challenges in ensuring the reliability and effectiveness of AI-driven medical tools. To address this gap, we introduce GazeVal, a practical framework that synergizes expert eye-tracking data with direct radiological evaluations to assess the quality of synthetic medical images. GazeVal leverages gaze patterns of radiologists as they provide a deeper understanding of how experts perceive and interact with synthetic data in different tasks (i.e., diagnostic or Turing tests). Experiments with sixteen radiologists revealed that 96.6% of the generated images (by the most recent state-of-the-art AI algorithm) were identified as fake, demonstrating the limitations of generative AI in producing clinically accurate images. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œå¯¹äºé«˜è´¨é‡åˆæˆæ•°æ®ç”¨äºæ¨¡å‹è®­ç»ƒå’Œå¢å¼ºçš„éœ€æ±‚ä»æœªå¦‚æ­¤ä¹‹å¤§ã€‚ç„¶è€Œï¼Œå½“å‰çš„è¯„ä¼°ä¸»è¦ä¾èµ–äºè®¡ç®—æŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡ä¸äººç±»ä¸“å®¶è¯†åˆ«çš„åŒ¹é…åº¦å¾ˆä½ã€‚è¿™å¯¼è‡´åˆæˆå›¾åƒè™½ç„¶åœ¨æ•°å€¼ä¸Šçœ‹èµ·æ¥å¾ˆçœŸå®ï¼Œä½†åœ¨ä¸´åºŠçœŸå®æ€§ä¸Šå´æœ‰æ‰€æ¬ ç¼ºï¼Œä»è€Œåœ¨ä¿è¯äººå·¥æ™ºèƒ½é©±åŠ¨çš„åŒ»ç–—å·¥å…·çš„å¯é æ€§å’Œæœ‰æ•ˆæ€§æ–¹é¢å¸¦æ¥é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†GazeValï¼Œè¿™æ˜¯ä¸€ä¸ªå®ç”¨çš„æ¡†æ¶ï¼Œå®ƒå°†ä¸“å®¶çœ¼åŠ¨æ•°æ®ä¸ç›´æ¥æ”¾å°„å­¦è¯„ä¼°ç›¸ç»“åˆï¼Œä»¥è¯„ä¼°åˆæˆåŒ»å­¦å›¾åƒçš„è´¨é‡ã€‚GazeValåˆ©ç”¨æ”¾å°„ç§‘åŒ»ç”Ÿåœ¨è¯„ä¼°åˆæˆæ•°æ®æ—¶è¡¨ç°å‡ºçš„æ³¨è§†æ¨¡å¼ï¼Œä»¥æ›´æ·±å…¥åœ°äº†è§£ä¸“å®¶åœ¨ä¸åŒä»»åŠ¡ï¼ˆå¦‚è¯Šæ–­æˆ–å›¾çµæµ‹è¯•ï¼‰ä¸­å¦‚ä½•æ„ŸçŸ¥å’Œä¸åˆæˆæ•°æ®äº’åŠ¨ã€‚æœ‰åå…­åæ”¾å°„ç§‘åŒ»ç”Ÿå‚ä¸çš„å®éªŒæ˜¾ç¤ºï¼Œæœ€æ–°æœ€å…ˆè¿›çš„ç®—æ³•ç”Ÿæˆçš„å›¾åƒä¸­ï¼Œæœ‰96.6%è¢«è¯†åˆ«ä¸ºå‡çš„ï¼Œè¿™æ˜¾ç¤ºäº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨ç”Ÿæˆä¸´åºŠå‡†ç¡®å›¾åƒæ–¹é¢çš„å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20967v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œé«˜è´¨é‡åˆæˆæ•°æ®çš„éœ€æ±‚å·¨å¤§ï¼Œä½†ç°æœ‰è¯„ä¼°ä¸»è¦ä¾èµ–è®¡ç®—æŒ‡æ ‡ï¼Œä¸äººç±»ä¸“å®¶è¯†åˆ«å­˜åœ¨åå·®ã€‚è¿™å¯¼è‡´åˆæˆå›¾åƒè™½ç„¶æ•°å€¼ä¸Šçœ‹èµ·æ¥çœŸå®ï¼Œä½†ç¼ºä¹ä¸´åºŠçœŸå®æ€§ï¼Œç»™äººå·¥æ™ºèƒ½é©±åŠ¨çš„åŒ»å­¦å·¥å…·å¸¦æ¥å¯é æ€§å’Œæœ‰æ•ˆæ€§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºGazeValæ¡†æ¶ï¼Œç»“åˆä¸“å®¶çœ¼åŠ¨è¿½è¸ªæ•°æ®å’Œç›´æ¥æ”¾å°„å­¦è¯„ä¼°ï¼Œè¯„ä¼°åˆæˆåŒ»å­¦å›¾åƒçš„è´¨é‡ã€‚å®éªŒæ˜¾ç¤ºï¼Œåå…­ä½æ”¾å°„ç§‘åŒ»ç”Ÿä¸­ï¼Œæœ‰ä¹æˆä»¥ä¸Šè®¤ä¸ºæœ€æ–°äººå·¥æ™ºèƒ½ç®—æ³•ç”Ÿæˆçš„å›¾åƒä¸ºå‡ï¼Œè¡¨æ˜ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨ä¸´åºŠå‡†ç¡®æ€§æ–¹é¢å­˜åœ¨å±€é™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒé¢†åŸŸå¯¹é«˜è´¨é‡åˆæˆæ•°æ®çš„éœ€æ±‚å·¨å¤§ã€‚</li>
<li>å½“å‰è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–è®¡ç®—æŒ‡æ ‡ï¼Œå¿½ç•¥äº†ä¸äººç±»ä¸“å®¶è¯†åˆ«çš„åŒ¹é…ç¨‹åº¦ã€‚</li>
<li>åˆæˆå›¾åƒå¯èƒ½åœ¨æ•°å€¼ä¸Šçœ‹èµ·æ¥å¾ˆçœŸå®ï¼Œä½†åœ¨ä¸´åºŠçœŸå®æ€§æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>æå‡ºGazeValæ¡†æ¶ä»¥è¯„ä¼°åˆæˆåŒ»å­¦å›¾åƒè´¨é‡ï¼Œç»“åˆäº†ä¸“å®¶çœ¼åŠ¨æ•°æ®å’Œç›´æ¥æ”¾å°„å­¦è¯„ä¼°ã€‚</li>
<li>GazeValæ¡†æ¶èƒ½æ›´æ·±å…¥åœ°äº†è§£ä¸“å®¶å¦‚ä½•æ„ŸçŸ¥å’Œä¸åˆæˆæ•°æ®äº’åŠ¨ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œæœ€æ–°äººå·¥æ™ºèƒ½ç®—æ³•ç”Ÿæˆçš„åŒ»å­¦å›¾åƒè¢«å¤§å¤šæ•°æ”¾å°„ç§‘åŒ»ç”Ÿè¯†åˆ«ä¸ºå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20967">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e129966c29299c442854f73aad8064ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6317b68b3904e6f275a5d6f81eeb8a1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2743f03391a8de18d6f00e3bcf77723.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59805d7c406dc50b0be5ac44c69846fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb163b3f3211cc8edbb12c1144279430.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="VGAT-A-Cancer-Survival-Analysis-Framework-Transitioning-from-Generative-Visual-Question-Answering-to-Genomic-Reconstruction"><a href="#VGAT-A-Cancer-Survival-Analysis-Framework-Transitioning-from-Generative-Visual-Question-Answering-to-Genomic-Reconstruction" class="headerlink" title="VGAT: A Cancer Survival Analysis Framework Transitioning from Generative   Visual Question Answering to Genomic Reconstruction"></a>VGAT: A Cancer Survival Analysis Framework Transitioning from Generative   Visual Question Answering to Genomic Reconstruction</h2><p><strong>Authors:Zizhi Chen, Minghao Han, Xukun Zhang, Shuwei Ma, Tao Liu, Xing Wei, Lihua Zhang</strong></p>
<p>Multimodal learning combining pathology images and genomic sequences enhances cancer survival analysis but faces clinical implementation barriers due to limited access to genomic sequencing in under-resourced regions. To enable survival prediction using only whole-slide images (WSI), we propose the Visual-Genomic Answering-Guided Transformer (VGAT), a framework integrating Visual Question Answering (VQA) techniques for genomic modality reconstruction. By adapting VQAâ€™s text feature extraction approach, we derive stable genomic representations that circumvent dimensionality challenges in raw genomic data. Simultaneously, a cluster-based visual prompt module selectively enhances discriminative WSI patches, addressing noise from unfiltered image regions. Evaluated across five TCGA datasets, VGAT outperforms existing WSI-only methods, demonstrating the viability of genomic-informed inference without sequencing. This approach bridges multimodal research and clinical feasibility in resource-constrained settings. The code link is <a target="_blank" rel="noopener" href="https://github.com/CZZZZZZZZZZZZZZZZZ/VGAT">https://github.com/CZZZZZZZZZZZZZZZZZ/VGAT</a>. </p>
<blockquote>
<p>å°†ç—…ç†å›¾åƒä¸åŸºå› ç»„åºåˆ—ç›¸ç»“åˆçš„å¤šæ¨¡æ€å­¦ä¹ æé«˜äº†ç™Œç—‡ç”Ÿå­˜åˆ†æèƒ½åŠ›ï¼Œä½†ç”±äºèµ„æºåŒ®ä¹åœ°åŒºåŸºå› ç»„æµ‹åºçš„æœ‰é™è®¿é—®æ€§ï¼Œé¢ä¸´ç€ä¸´åºŠå®æ–½éšœç¢ã€‚ä¸ºäº†ä»…ä½¿ç”¨å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰è¿›è¡Œç”Ÿå­˜é¢„æµ‹ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰åŸºå› ç»„é—®ç­”å¼•å¯¼è½¬æ¢å™¨ï¼ˆVGATï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æŠ€æœ¯ç”¨äºåŸºå› ç»„æ¨¡å¼é‡å»ºã€‚é€šè¿‡é€‚åº”VQAçš„æ–‡æœ¬ç‰¹å¾æå–æ–¹æ³•ï¼Œæˆ‘ä»¬å¾—å‡ºç¨³å®šçš„åŸºå› ç»„è¡¨ç¤ºï¼Œè¿™é¿å…äº†åŸå§‹åŸºå› ç»„æ•°æ®ä¸­çš„ç»´åº¦æŒ‘æˆ˜ã€‚åŒæ—¶ï¼ŒåŸºäºé›†ç¾¤çš„è§†è§‰æç¤ºæ¨¡å—ä¼šé€‰æ‹©æ€§å¢å¼ºåˆ¤åˆ«WSIè¡¥ä¸ï¼Œè§£å†³æœªè¿‡æ»¤å›¾åƒåŒºåŸŸçš„å™ªå£°é—®é¢˜ã€‚åœ¨äº”ä¸ªTCGAæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒVGATåœ¨ä»…ä½¿ç”¨WSIçš„æ–¹æ³•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†åœ¨ä¸éœ€è¦æµ‹åºçš„æƒ…å†µä¸‹è¿›è¡ŒåŸºå› ç»„ä¿¡æ¯æ¨æ–­çš„å¯è¡Œæ€§ã€‚è¿™ç§æ–¹æ³•åœ¨å¤šæ¨¡æ€ç ”ç©¶å’Œèµ„æºå—é™ç¯å¢ƒä¸­çš„ä¸´åºŠå¯è¡Œæ€§ä¹‹é—´æ­å»ºäº†æ¡¥æ¢ã€‚ä»£ç é“¾æ¥æ˜¯<a target="_blank" rel="noopener" href="https://github.com/CZZZZZZZZZZZZZZZZZ/VGAT">https://github.com/CZZZZZZZZZZZZZZZZZ/VGAT</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19367v2">PDF</a> Acceppted by ICME2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç»“åˆç—…ç†å­¦å›¾åƒå’ŒåŸºå› ç»„åºåˆ—çš„å¤šæ¨¡æ€å­¦ä¹ æå‡äº†ç™Œç—‡ç”Ÿå­˜åˆ†æçš„æ•ˆæœï¼Œä½†åœ¨èµ„æºåŒ®ä¹åœ°åŒºç”±äºåŸºå› ç»„æµ‹åºçš„æœ‰é™è®¿é—®è€Œé¢ä¸´ä¸´åºŠå®æ–½çš„éšœç¢ã€‚ä¸ºäº†ä»…ä½¿ç”¨å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰è¿›è¡Œç”Ÿå­˜é¢„æµ‹ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰åŸºå› ç»„é—®ç­”å¼•å¯¼è½¬æ¢å™¨ï¼ˆVGATï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ•´åˆäº†è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æŠ€æœ¯ç”¨äºåŸºå› ç»„æ¨¡æ€é‡å»ºã€‚é€šè¿‡é€‚åº”VQAçš„æ–‡æœ¬ç‰¹å¾æå–æ–¹æ³•ï¼Œæˆ‘ä»¬å¾—å‡ºäº†ç¨³å®šçš„åŸºå› ç»„è¡¨ç¤ºï¼Œé¿å…äº†åŸå§‹åŸºå› ç»„æ•°æ®çš„ç»´åº¦æŒ‘æˆ˜ã€‚åŒæ—¶ï¼ŒåŸºäºé›†ç¾¤çš„è§†è§‰æç¤ºæ¨¡å—é€‰æ‹©æ€§åœ°å¢å¼ºäº†é‰´åˆ«æ€§çš„WSIè¡¥ä¸ï¼Œè§£å†³äº†æœªè¿‡æ»¤å›¾åƒåŒºåŸŸçš„å™ªå£°é—®é¢˜ã€‚åœ¨äº”ä¸ªTCGAæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒVGATåœ¨ä»…ä½¿ç”¨WSIçš„æƒ…å†µä¸‹è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œè¯æ˜äº†æ— æµ‹åºçš„åŸºå› ç»„ä¿¡æ¯æ¨æ–­çš„å¯è¡Œæ€§ã€‚è¿™ç§æ–¹æ³•åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°äº†å¤šæ¨¡æ€ç ”ç©¶ä¸ä¸´åºŠå¯è¡Œæ€§çš„ç»“åˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å­¦ä¹ ç»“åˆç—…ç†å›¾åƒå’ŒåŸºå› ç»„åºåˆ—å¢å¼ºäº†ç™Œç—‡ç”Ÿå­˜åˆ†æã€‚</li>
<li>åœ¨èµ„æºæœ‰é™åœ°åŒºï¼Œä¸´åºŠå®æ–½é¢ä¸´åŸºå› ç»„æµ‹åºæœ‰é™è®¿é—®çš„éšœç¢ã€‚</li>
<li>æå‡ºäº†VGATæ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰é—®ç­”æŠ€æœ¯å®ç°åŸºå› ç»„æ¨¡æ€é‡å»ºã€‚</li>
<li>VQAçš„æ–‡æœ¬ç‰¹å¾æå–æ–¹æ³•ç”¨äºç¨³å®šåŸºå› ç»„è¡¨ç¤ºï¼Œé™ä½ç»´åº¦æŒ‘æˆ˜ã€‚</li>
<li>åŸºäºé›†ç¾¤çš„è§†è§‰æç¤ºæ¨¡å—å¢å¼ºé‰´åˆ«æ€§WSIè¡¥ä¸ï¼Œå‡å°‘æœªè¿‡æ»¤å›¾åƒåŒºåŸŸçš„å™ªå£°ã€‚</li>
<li>VGATåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œè¯æ˜äº†æ— æµ‹åºçš„åŸºå› ç»„ä¿¡æ¯æ¨æ–­çš„å¯è¡Œæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19367">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-977358174bf8c5f04a5eac2602712185.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-832917e08e6d9470f2d6d8137c783037.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09a3d4d5585b895dd0efdb66b1e0879f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59e504c5c98b17cc8d9e22464901c320.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7ecea5958615700f6cb7868676a6405.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-98f8164d1d306337ce7ca27561231d59.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Imaging-Ultrafast-Dynamical-Diffraction-wavefronts-of-femtosecond-laser-induced-lattice-distortions-inside-crystalline-semiconductors"><a href="#Imaging-Ultrafast-Dynamical-Diffraction-wavefronts-of-femtosecond-laser-induced-lattice-distortions-inside-crystalline-semiconductors" class="headerlink" title="Imaging Ultrafast Dynamical Diffraction wavefronts of femtosecond   laser-induced lattice distortions inside crystalline semiconductors"></a>Imaging Ultrafast Dynamical Diffraction wavefronts of femtosecond   laser-induced lattice distortions inside crystalline semiconductors</h2><p><strong>Authors:Angel RodrÃ­guez-FernÃ¡ndez, Jan-Etienne Pudell, Roman Shayduk, Wonhyuk Jo, James Wrigley, Johannes MÃ¶ller, Peter Zalden, Alexey Zozulya, JÃ¶rg Hallmann, Anders Madsen, Pablo Villanueva-Perez, Zdenek Matej, Thies J. Albert, Dominik Kaczmarek, Klaus Sokolowski-Tinten, Antonowicz Jerzy, Ryszard Sobierajski, Rahimi Mosafer, Oleksii I. Liubchenko, Javier Solis, Jan Siegel</strong></p>
<p>Material processing with femtosecond lasers has attracted enormous attention because of its potential for technology and industry applications. In parallel, time-resolved x-ray diffraction has been successfully used to study ultrafast structural distortion dynamics in semiconductor thin films. Gracing incident x-ray geometry has been also used to look to distortion dynamics, but this technique is only sensitive to the surface of bulk materials with a limited temporal resolution. However, â€˜real-worldâ€™ processing applications deal mostly with bulk materials, which prevent the use of such techniques. For processing applications, a fast and depth-sensitive probe is needed. To address this, we present a novel technique based on ultrafast dynamical diffraction (UDD) capable of imaging transient strain distributions inside bulk crystals upon single-pulse excitation. This pump-probe technique provides a complete picture of the temporal evolution of ultrafast distortion depth profiles. Our measurements were obtained in a thin crystalline Si wafer upon single pulse femtosecond optical excitation revealing that even below the melting threshold strong lattice distortions appear on ps time scales due to the formation and propagation of high-amplitude strain waves into the bulk. </p>
<blockquote>
<p>ææ–™é€šè¿‡é£ç§’æ¿€å…‰è¿›è¡Œå¤„ç†å·²ç»å¼•èµ·äº†ç§‘æŠ€å’Œå·¥ä¸šåº”ç”¨é¢†åŸŸçš„å¤§é‡å…³æ³¨ã€‚åŒæ—¶ï¼Œæ—¶é—´è§£æXå°„çº¿è¡å°„æŠ€æœ¯å·²æˆåŠŸåº”ç”¨äºç ”ç©¶åŠå¯¼ä½“è–„è†œä¸­çš„è¶…å¿«ç»“æ„ç•¸å˜åŠ¨åŠ›å­¦ã€‚å€¾æ–œå…¥å°„çš„Xå°„çº¿å‡ ä½•æŠ€æœ¯ä¹Ÿè¢«ç”¨æ¥è§‚å¯Ÿç•¸å˜åŠ¨åŠ›å­¦ï¼Œä½†è¯¥æŠ€æœ¯ä»…å¯¹å¤§å—ææ–™çš„è¡¨é¢æ•æ„Ÿï¼Œä¸”æ—¶é—´åˆ†è¾¨ç‡æœ‰é™ã€‚ç„¶è€Œï¼Œâ€œç°å®ä¸–ç•Œâ€å¤„ç†åº”ç”¨å¤§å¤šæ¶‰åŠå¤§å—ææ–™ï¼Œè¿™ä½¿å¾—è¿™äº›æŠ€æœ¯çš„åº”ç”¨å—åˆ°é™åˆ¶ã€‚é’ˆå¯¹å¤„ç†åº”ç”¨ï¼Œéœ€è¦ä¸€ä¸ªå¿«é€Ÿä¸”å¯¹æ·±åº¦æ•æ„Ÿçš„æ¢é’ˆã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¶…å¿«åŠ¨åŠ›å­¦è¡å°„ï¼ˆUDDï¼‰çš„æ–°æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨å•è„‰å†²æ¿€å‘ä¸‹å¯¹æ™¶ä½“å†…éƒ¨ç¬æ—¶åº”å˜åˆ†å¸ƒè¿›è¡Œæˆåƒã€‚è¿™ç§æ³µæµ¦æ¢é’ˆæŠ€æœ¯æä¾›äº†è¶…å¿«ç•¸å˜æ·±åº¦åˆ†å¸ƒçš„æ—¶é—´æ¼”åŒ–çš„å®Œæ•´å›¾åƒã€‚æˆ‘ä»¬çš„æµ‹é‡æ˜¯åœ¨å•æ™¶ç¡…è–„ç‰‡ä¸Šè¿›è¡Œçš„ï¼Œé€šè¿‡å•è„‰å†²é£ç§’å…‰å­¦æ¿€å‘åå‘ç°ï¼Œå³ä½¿åœ¨ä½äºç†”ç‚¹é˜ˆå€¼çš„æ¡ä»¶ä¸‹ï¼Œç”±äºé«˜æŒ¯å¹…åº”å˜æ³¢çš„å½¢æˆå’Œå‘å†…éƒ¨çš„ä¼ æ’­ï¼Œçš®ç§’æ—¶é—´å°ºåº¦ä¸Šä¹Ÿä¼šå‡ºç°å¼ºçƒˆçš„æ™¶æ ¼ç•¸å˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10420v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ææ–™åŠ å·¥ä¸­çš„é£ç§’æ¿€å…‰æŠ€æœ¯å› å…¶å¯¹æŠ€æœ¯å’Œå·¥ä¸šåº”ç”¨çš„æ½œåŠ›è€Œå¤‡å—å…³æ³¨ã€‚åŒæ—¶ï¼Œæ—¶é—´åˆ†è¾¨Xå°„çº¿è¡å°„å·²æˆåŠŸåº”ç”¨äºç ”ç©¶åŠå¯¼ä½“è–„è†œä¸­çš„è¶…å¿«ç»“æ„ç•¸å˜åŠ¨åŠ›å­¦ã€‚è™½ç„¶å€¾æ–œå…¥å°„Xå°„çº¿å‡ ä½•æŠ€æœ¯ä¹Ÿå¯ç”¨äºè§‚å¯Ÿç•¸å˜åŠ¨åŠ›å­¦ï¼Œä½†å…¶ä»…å¯¹ä½“ææ–™è¡¨é¢æ•æ„Ÿï¼Œä¸”æ—¶é—´åˆ†è¾¨ç‡æœ‰é™ï¼Œæ— æ³•é€‚ç”¨äºå¤§å¤šæ•°ç°å®ä¸–ç•Œä¸­çš„åŠ å·¥åº”ç”¨ï¼Œæ¶‰åŠçš„ä¸»è¦ææ–™å¤šä¸ºä½“ææ–™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¶…å¿«åŠ¨åŠ›å­¦è¡å°„ï¼ˆUDDï¼‰çš„æ–°æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨å•è„‰å†²æ¿€å‘ä¸‹æˆåƒæ™¶ä½“å†…éƒ¨çš„ç¬æ€åº”å˜åˆ†å¸ƒã€‚è¿™ç§æ³µæµ¦æ¢é’ˆæŠ€æœ¯æä¾›äº†è¶…å¿«ç•¸å˜æ·±åº¦åˆ†å¸ƒçš„æš‚æ—¶æ¼”åŒ–çš„å®Œæ•´å›¾åƒã€‚æˆ‘ä»¬åœ¨å•æ™¶ç¡…è–„è†œä¸Šçš„å•æ¬¡è„‰å†²é£ç§’å…‰å­¦æ¿€å‘æµ‹é‡ä¸­å‘ç°ï¼Œå³ä½¿åœ¨ä½äºç†”åŒ–é˜ˆå€¼çš„æ¡ä»¶ä¸‹ï¼Œä¹Ÿä¼šå‡ºç°å¼ºæ™¶æ ¼ç•¸å˜ï¼Œè¿™æ˜¯ç”±äºåœ¨çš®ç§’æ—¶é—´å°ºåº¦ä¸Šå½¢æˆäº†é«˜æŒ¯å¹…åº”å˜æ³¢å¹¶å‘ä½“ææ–™å†…éƒ¨ä¼ æ’­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é£ç§’æ¿€å…‰æŠ€æœ¯åœ¨ææ–™åŠ å·¥é¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
<li>æ—¶é—´åˆ†è¾¨Xå°„çº¿è¡å°„æŠ€æœ¯å·²æˆåŠŸåº”ç”¨äºç ”ç©¶è¶…å¿«ç»“æ„ç•¸å˜åŠ¨åŠ›å­¦ã€‚</li>
<li>å€¾æ–œå…¥å°„Xå°„çº¿å‡ ä½•æŠ€æœ¯è™½ç„¶å¯ç”¨äºè§‚å¯Ÿç•¸å˜åŠ¨åŠ›å­¦ï¼Œä½†ä¸»è¦å±€é™äºä½“ææ–™è¡¨é¢çš„ç ”ç©¶ï¼Œä¸”æ—¶é—´åˆ†è¾¨ç‡æœ‰é™ã€‚</li>
<li>éœ€è¦ä¸€ç§å¿«é€Ÿä¸”å¯¹æ·±åº¦æ•æ„Ÿçš„æ£€æµ‹æ–¹æ³•ï¼Œä»¥åº”å¯¹æ¶‰åŠä½“ææ–™çš„å®é™…åŠ å·¥åº”ç”¨ã€‚</li>
<li>åŸºäºè¶…å¿«åŠ¨åŠ›å­¦è¡å°„ï¼ˆUDDï¼‰çš„æ–°æŠ€æœ¯èƒ½å¤Ÿæä¾›ä½“ææ–™å†…éƒ¨çš„ç¬æ€åº”å˜åˆ†å¸ƒçš„å›¾åƒã€‚</li>
<li>è¿™ç§æ–°æŠ€æœ¯å¯ä»¥æ­ç¤ºè¶…å¿«ç•¸å˜çš„æ·±åº¦åˆ†å¸ƒå’Œæš‚æ—¶æ¼”åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10420">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-49f12c7466c329475609ba1e38491b02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18171008d41608e7a62f9efd8538c5d6.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="UnPuzzle-A-Unified-Framework-for-Pathology-Image-Analysis"><a href="#UnPuzzle-A-Unified-Framework-for-Pathology-Image-Analysis" class="headerlink" title="UnPuzzle: A Unified Framework for Pathology Image Analysis"></a>UnPuzzle: A Unified Framework for Pathology Image Analysis</h2><p><strong>Authors:Dankai Liao, Sicheng Chen, Nuwa Xi, Qiaochu Xue, Jieyu Li, Lingxuan Hou, Zeyu Liu, Chang Han Low, Yufeng Wu, Yiling Liu, Yanqin Jiang, Dandan Li, Shangqing Lyu</strong></p>
<p>Pathology image analysis plays a pivotal role in medical diagnosis, with deep learning techniques significantly advancing diagnostic accuracy and research. While numerous studies have been conducted to address specific pathological tasks, the lack of standardization in pre-processing methods and model&#x2F;database architectures complicates fair comparisons across different approaches. This highlights the need for a unified pipeline and comprehensive benchmarks to enable consistent evaluation and accelerate research progress. In this paper, we present UnPuzzle, a novel and unified framework for pathological AI research that covers a broad range of pathology tasks with benchmark results. From high-level to low-level, upstream to downstream tasks, UnPuzzle offers a modular pipeline that encompasses data pre-processing, model composition,taskconfiguration,andexperimentconduction.Specifically, it facilitates efficient benchmarking for both Whole Slide Images (WSIs) and Region of Interest (ROI) tasks. Moreover, the framework supports variouslearningparadigms,includingself-supervisedlearning,multi-task learning,andmulti-modallearning,enablingcomprehensivedevelopment of pathology AI models. Through extensive benchmarking across multiple datasets, we demonstrate the effectiveness of UnPuzzle in streamlining pathology AI research and promoting reproducibility. We envision UnPuzzle as a cornerstone for future advancements in pathology AI, providing a more accessible, transparent, and standardized approach to model evaluation. The UnPuzzle repository is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Puzzle-AI/UnPuzzle">https://github.com/Puzzle-AI/UnPuzzle</a>. </p>
<blockquote>
<p>ç—…ç†å­¦å›¾åƒåˆ†æåœ¨åŒ»å­¦è¯Šæ–­ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œæ·±åº¦å­¦ä¹ æŠ€æœ¯æ˜¾è‘—æé«˜äº†è¯Šæ–­å‡†ç¡®æ€§å’Œç ”ç©¶æ°´å¹³ã€‚è™½ç„¶å·²è¿›è¡Œäº†å¤§é‡ç ”ç©¶æ¥è§£å†³ç‰¹å®šçš„ç—…ç†ä»»åŠ¡ï¼Œä½†åœ¨é¢„å¤„ç†æ–¹æ³•å’Œæ¨¡å‹&#x2F;æ•°æ®åº“æ¶æ„æ–¹é¢ç¼ºä¹æ ‡å‡†åŒ–ï¼Œä½¿å¾—ä¸åŒæ–¹æ³•ä¹‹é—´çš„å…¬å¹³æ¯”è¾ƒå˜å¾—å¤æ‚ã€‚è¿™çªæ˜¾äº†éœ€è¦ä¸€ä¸ªç»Ÿä¸€çš„æµç¨‹å’Œç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œä»¥è¿›è¡Œä¸€è‡´æ€§çš„è¯„ä¼°å¹¶åŠ é€Ÿç ”ç©¶è¿›å±•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†UnPuzzleï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç—…ç†äººå·¥æ™ºèƒ½ç ”ç©¶çš„æ–°å‹ç»Ÿä¸€æ¡†æ¶ï¼Œæ¶µç›–å¹¿æ³›çš„ç—…ç†ä»»åŠ¡å¹¶æä¾›åŸºå‡†ç»“æœã€‚ä»é«˜çº§åˆ°ä½çº§ï¼Œä»ä¸Šæ¸¸åˆ°ä¸‹æ¸¸ä»»åŠ¡ï¼ŒUnPuzzleæä¾›äº†ä¸€ä¸ªæ¨¡å—åŒ–æµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹ç»„åˆã€ä»»åŠ¡é…ç½®å’Œå®éªŒæ‰§è¡Œã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒæœ‰åŠ©äºå¯¹æ•´ä¸ªå¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰å’Œæ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰ä»»åŠ¡è¿›è¡Œé«˜æ•ˆçš„åŸºå‡†æµ‹è¯•ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ”¯æŒå¤šç§å­¦ä¹ èŒƒå¼ï¼ŒåŒ…æ‹¬è‡ªæˆ‘ç›‘ç£å­¦ä¹ ã€å¤šä»»åŠ¡å­¦ä¹ å’Œå¤šæ¨¡å¼å­¦ä¹ ï¼Œä»è€Œå…¨é¢å¼€å‘ç—…ç†äººå·¥æ™ºèƒ½æ¨¡å‹ã€‚é€šè¿‡å¤šä¸ªæ•°æ®é›†çš„åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬è¯æ˜äº†UnPuzzleåœ¨ç®€åŒ–ç—…ç†äººå·¥æ™ºèƒ½ç ”ç©¶å¹¶ä¿ƒè¿›å¯é‡å¤æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æœŸæœ›UnPuzzleèƒ½æˆä¸ºæœªæ¥ç—…ç†äººå·¥æ™ºèƒ½å‘å±•çš„åŸºçŸ³ï¼Œä¸ºæ¨¡å‹è¯„ä¼°æä¾›æ›´ä¾¿æ·ã€é€æ˜å’Œæ ‡å‡†åŒ–çš„æ–¹æ³•ã€‚UnPuzzleä»“åº“å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Puzzle-AI/UnPuzzle%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Puzzle-AI/UnPuzzleå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03152v2">PDF</a> 11 pages,2 figures</p>
<p><strong>Summary</strong><br>     ç—…ç†å­¦å›¾åƒåˆ†æåœ¨åŒ»å­¦è¯Šæ–­ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œæ·±åº¦å­¦ä¹ æŠ€æœ¯æ˜¾è‘—æé«˜äº†è¯Šæ–­å‡†ç¡®æ€§å’Œç ”ç©¶æ°´å¹³ã€‚å½“å‰ç¼ºä¹æ ‡å‡†åŒ–é¢„å¤„ç†æ–¹æ³•å’Œæ¨¡å‹&#x2F;æ•°æ®åº“æ¶æ„ï¼Œå¯¼è‡´ä¸åŒæ–¹æ³•ä¹‹é—´çš„å…¬å¹³æ¯”è¾ƒå˜å¾—å¤æ‚ã€‚æœ¬æ–‡æå‡ºUnPuzzleï¼Œä¸€ä¸ªé’ˆå¯¹ç—…ç†å­¦äººå·¥æ™ºèƒ½ç ”ç©¶çš„æ–°å‹ç»Ÿä¸€æ¡†æ¶ï¼Œæ¶µç›–å¹¿æ³›ç—…ç†å­¦ä»»åŠ¡å¹¶æä¾›åŸºå‡†ç»“æœã€‚UnPuzzleæä¾›æ¨¡å—åŒ–ç®¡é“ï¼Œæ¶µç›–ä»é«˜çº§åˆ°ä½çº§ã€ä¸Šæ¸¸åˆ°ä¸‹æ¸¸ä»»åŠ¡çš„æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹ç»„åˆã€ä»»åŠ¡é…ç½®å’Œå®éªŒæ‰§è¡Œã€‚å®ƒæ”¯æŒå¤šç§å­¦ä¹ èŒƒå¼ï¼ŒåŒ…æ‹¬è‡ªç›‘ç£å­¦ä¹ ã€å¤šä»»åŠ¡å­¦ä¹ å’Œå¤šæ¨¡æ€å­¦ä¹ ï¼Œä¿ƒè¿›ç—…ç†å­¦AIæ¨¡å‹çš„å…¨é¢å‘å±•ã€‚é€šè¿‡è·¨å¤šä¸ªæ•°æ®é›†çš„å¹¿æ³›åŸºå‡†æµ‹è¯•ï¼Œè¯æ˜äº†UnPuzzleåœ¨ç®€åŒ–ç—…ç†å­¦äººå·¥æ™ºèƒ½ç ”ç©¶å’Œä¿ƒè¿›å¯é‡å¤æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æœŸæœ›UnPuzzleæˆä¸ºæœªæ¥ç—…ç†å­¦äººå·¥æ™ºèƒ½å‘å±•çš„åŸºçŸ³ï¼Œæä¾›æ›´æ˜“äºè®¿é—®ã€é€æ˜å’Œæ ‡å‡†åŒ–çš„æ¨¡å‹è¯„ä¼°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æŠ€æœ¯æ˜¾è‘—æé«˜äº†ç—…ç†å­¦å›¾åƒåˆ†æçš„å‡†ç¡®æ€§å’Œç ”ç©¶æ°´å¹³ã€‚</li>
<li>ç¼ºä¹æ ‡å‡†åŒ–çš„é¢„å¤„ç†æ–¹æ³•å’Œæ¨¡å‹æ¶æ„é˜»ç¢äº†ç—…ç†å­¦ç ”ç©¶çš„æ¯”è¾ƒå’Œè¿›å±•ã€‚</li>
<li>UnPuzzleæä¾›äº†ä¸€ä¸ªæ–°å‹ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå¹¶æ¶µç›–å¹¿æ³›çš„ç—…ç†å­¦ä»»åŠ¡ã€‚</li>
<li>UnPuzzleæä¾›æ¨¡å—åŒ–ç®¡é“ï¼ŒåŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹ç»„åˆã€ä»»åŠ¡é…ç½®å’Œå®éªŒæ‰§è¡Œã€‚</li>
<li>è¯¥æ¡†æ¶æ”¯æŒå¤šç§å­¦ä¹ èŒƒå¼ï¼Œä¿ƒè¿›äº†ç—…ç†å­¦AIæ¨¡å‹çš„å…¨é¢å‘å±•ã€‚</li>
<li>UnPuzzleé€šè¿‡è·¨å¤šä¸ªæ•°æ®é›†çš„åŸºå‡†æµ‹è¯•è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03152">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db7f120043dabcc276d88f2170691491.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e743da5759513c05a4a336103f14321a.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Convergence-of-Ray-and-Pixel-Driven-Discretization-Frameworks-in-the-Strong-Operator-Topology"><a href="#Convergence-of-Ray-and-Pixel-Driven-Discretization-Frameworks-in-the-Strong-Operator-Topology" class="headerlink" title="Convergence of Ray- and Pixel-Driven Discretization Frameworks in the   Strong Operator Topology"></a>Convergence of Ray- and Pixel-Driven Discretization Frameworks in the   Strong Operator Topology</h2><p><strong>Authors:Richard Huber</strong></p>
<p>Tomography is a central tool in medical applications, allowing doctors to investigate patientsâ€™ interior features. The Radon transform (in two dimensions) is commonly used to model the measurement process in parallel-beam CT. Suitable discretization of the Radon transform and its adjoint (called the backprojection) is crucial. The most commonly used discretization approach combines the ray-driven Radon transform with the pixel-driven backprojection, as anecdotal reports describe these as showing the best approximation performance. However, there is little rigorous understanding of induced approximation errors. These methods involve three discretization parameters: the spatial-, detector-, and angular resolutions. Most commonly, balanced resolutions are used, i.e., the same (or similar) spatial- and detector resolutions are employed. We present a novel interpretation of ray- and pixel-driven discretizations as &#96;convolutional methodsâ€™. This allows for a structured analysis that can explain observed behavior. In particular, we prove convergence in the strong operator topology of the ray-driven Radon transform and the pixel-driven backprojection under balanced resolutions, thus theoretically justifying this approach. In particular, with high enough resolutions one can approximate the Radon transform arbitrarily well. </p>
<blockquote>
<p>æ–­å±‚æ‰«ææŠ€æœ¯æ˜¯åŒ»ç–—åº”ç”¨ä¸­çš„æ ¸å¿ƒå·¥å…·ï¼Œå®ƒå¯ä»¥è®©åŒ»ç”Ÿæ£€æŸ¥æ‚£è€…çš„å†…éƒ¨ç»“æ„ã€‚æ‹‰ä¸œå˜æ¢ï¼ˆäºŒç»´ï¼‰å¸¸ç”¨äºå¹³è¡ŒæŸCTçš„æµ‹é‡è¿‡ç¨‹å»ºæ¨¡ã€‚æ‹‰ä¸œå˜æ¢åŠå…¶ä¼´éšï¼ˆç§°ä¸ºåå‘æŠ•å½±ï¼‰çš„é€‚å½“ç¦»æ•£åŒ–è‡³å…³é‡è¦ã€‚æœ€å¸¸ç”¨çš„ç¦»æ•£åŒ–æ–¹æ³•æ˜¯å°†å°„çº¿é©±åŠ¨çš„æ‹‰ä¸œå˜æ¢ä¸åƒç´ é©±åŠ¨çš„èƒŒæŠ•å½±ç›¸ç»“åˆï¼Œæ­£å¦‚ä¸ªåˆ«æŠ¥å‘Šæ‰€è¿°ï¼Œè¿™äº›æ–¹æ³•åœ¨è¿‘ä¼¼æ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³ã€‚ç„¶è€Œï¼Œå¯¹äºè¯±å¯¼çš„è¿‘ä¼¼è¯¯å·®ï¼Œäººä»¬å¯¹å…¶äº†è§£ç”šå°‘ã€‚è¿™äº›æ–¹æ³•æ¶‰åŠä¸‰ä¸ªç¦»æ•£åŒ–å‚æ•°ï¼šç©ºé—´åˆ†è¾¨ç‡ã€æ£€æµ‹å™¨åˆ†è¾¨ç‡å’Œè§’åº¦åˆ†è¾¨ç‡ã€‚é€šå¸¸ä½¿ç”¨çš„æ˜¯å¹³è¡¡åˆ†è¾¨ç‡ï¼Œå³é‡‡ç”¨ç›¸åŒï¼ˆæˆ–ç›¸ä¼¼ï¼‰çš„ç©ºé—´åˆ†è¾¨ç‡å’Œæ£€æµ‹å™¨åˆ†è¾¨ç‡ã€‚æˆ‘ä»¬å°†å°„çº¿é©±åŠ¨å’Œåƒç´ é©±åŠ¨çš„ç¦»æ•£åŒ–ä½œä¸ºä¸€ç§æ–°å‹çš„â€œå·ç§¯æ–¹æ³•â€è¿›è¡Œè§£è¯»ã€‚è¿™å…è®¸è¿›è¡Œç»“æ„åŒ–åˆ†æï¼Œä»è€Œè§£é‡Šè§‚å¯Ÿåˆ°çš„è¡Œä¸ºã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬è¯æ˜äº†åœ¨å¹³è¡¡åˆ†è¾¨ç‡ä¸‹ï¼Œå°„çº¿é©±åŠ¨çš„æ‹‰ä¸œå˜æ¢å’Œåƒç´ é©±åŠ¨çš„èƒŒæŠ•å½±åœ¨å¼ºç®—å­æ‹“æ‰‘ä¸­ä¼šæ”¶æ•›ï¼Œä»è€Œä¸ºè¿™ç§æ–¹æ³•æä¾›äº†ç†è®ºæ”¯æŒã€‚ç‰¹åˆ«æ˜¯ï¼Œåˆ†è¾¨ç‡è¶³å¤Ÿé«˜æ—¶ï¼Œå¯ä»¥ä»»æ„å¥½åœ°è¿‘ä¼¼æ‹‰ä¸œå˜æ¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03069v2">PDF</a> 29 pages, 10 figures, Preprint was substantially updated with   inclusion of section 4 concerning numerical experiments, as well as   improvments in all sections</p>
<p><strong>Summary</strong><br>     åŒ»å­¦æˆåƒä¸­ï¼Œæ–­å±‚æ‘„å½±æ˜¯å…³é”®å·¥å…·ã€‚æ‹‰å†¬å˜æ¢å¸¸ç”¨äºå¹³è¡Œå…‰æŸCTçš„æµ‹é‡è¿‡ç¨‹å»ºæ¨¡ã€‚æ‹‰å†¬å˜æ¢åŠå…¶ä¼´éšå˜æ¢ï¼ˆåå‘æŠ•å½±ï¼‰çš„é€‚å½“ç¦»æ•£åŒ–éå¸¸é‡è¦ã€‚å¸¸ç”¨çš„ç¦»æ•£åŒ–æ–¹æ³•æ˜¯ç»“åˆå°„çº¿é©±åŠ¨çš„æ‹‰å†¬å˜æ¢å’Œåƒç´ é©±åŠ¨çš„èƒŒæŠ•å½±ï¼Œè™½ç„¶è¿™æä¾›äº†æœ€ä½³è¿‘ä¼¼æ€§èƒ½ï¼Œä½†å¯¹è¿‘ä¼¼è¯¯å·®çš„äº†è§£è¿˜ä¸å¤Ÿæ·±å…¥ã€‚æ­¤æ–¹æ³•æ¶‰åŠä¸‰ä¸ªç¦»æ•£å‚æ•°ï¼šç©ºé—´åˆ†è¾¨ç‡ã€æ£€æµ‹å™¨åˆ†è¾¨ç‡å’Œè§’åº¦åˆ†è¾¨ç‡ã€‚é‡‡ç”¨å¹³è¡¡åˆ†è¾¨ç‡çš„æ–¹æ³•æœ€ä¸ºå¸¸è§ã€‚æˆ‘ä»¬æå‡ºä¸€ç§æ–°çš„è§£é‡Šæ–¹æ³•ï¼Œå°†å°„çº¿é©±åŠ¨å’Œåƒç´ é©±åŠ¨çš„ç¦»æ•£åŒ–è§†ä¸ºå·ç§¯æ–¹æ³•ï¼Œè¿™æœ‰åŠ©äºè¿›è¡Œç»“æ„åŒ–åˆ†æå¹¶è§£é‡Šè§‚å¯Ÿåˆ°çš„è¡Œä¸ºã€‚åœ¨å¹³è¡¡åˆ†è¾¨ç‡ä¸‹ï¼Œæˆ‘ä»¬è¯æ˜äº†å°„çº¿é©±åŠ¨çš„æ‹‰å†¬å˜æ¢å’Œåƒç´ é©±åŠ¨çš„èƒŒæŠ•å½±åœ¨å¼ºç®—å­æ‹“æ‰‘ä¸­çš„æ”¶æ•›æ€§ï¼Œä»è€Œä»ç†è®ºä¸Šè¯å®äº†è¿™ç§æ–¹æ³•ã€‚åˆ†è¾¨ç‡è¶³å¤Ÿé«˜æ—¶ï¼Œå¯ä»¥ä»»æ„é€¼è¿‘æ‹‰å†¬å˜æ¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ–­å±‚æ‘„å½±æ˜¯åŒ»å­¦åº”ç”¨ä¸­çš„å…³é”®å·¥å…·ï¼Œå…è®¸åŒ»ç”Ÿç ”ç©¶æ‚£è€…çš„å†…éƒ¨ç»“æ„ã€‚</li>
<li>æ‹‰å†¬å˜æ¢å¸¸ç”¨äºå¹³è¡Œå…‰æŸCTçš„æµ‹é‡è¿‡ç¨‹å»ºæ¨¡ã€‚</li>
<li>å°„çº¿é©±åŠ¨å’Œåƒç´ é©±åŠ¨çš„ç¦»æ•£åŒ–æ˜¯å¸¸ç”¨çš„æ‹‰å†¬å˜æ¢å¤„ç†æ–¹æ³•ã€‚</li>
<li>ç›®å‰çš„å¸¸ç”¨æ–¹æ³•åœ¨ç†è®ºä¸Šå¯¹è¿‘ä¼¼è¯¯å·®çš„è§£é‡Šä¸è¶³ã€‚</li>
<li>ä¸‰ä¸ªå…³é”®çš„ç¦»æ•£å‚æ•°åŒ…æ‹¬ç©ºé—´åˆ†è¾¨ç‡ã€æ£€æµ‹å™¨åˆ†è¾¨ç‡å’Œè§’åº¦åˆ†è¾¨ç‡ã€‚</li>
<li>é‡‡ç”¨å¹³è¡¡åˆ†è¾¨ç‡çš„æ–¹æ³•æœ€ä¸ºå¸¸è§ï¼Œå³é‡‡ç”¨ç›¸åŒæˆ–ç›¸ä¼¼çš„ç©ºé—´å’Œæ£€æµ‹å™¨åˆ†è¾¨ç‡ã€‚</li>
<li>å°†å°„çº¿é©±åŠ¨å’Œåƒç´ é©±åŠ¨çš„ç¦»æ•£åŒ–è§†ä¸ºå·ç§¯æ–¹æ³•æ˜¯ä¸€ç§æ–°çš„è§£é‡Šæ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03069">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b05bd2450d77a560cfe55e7b0e69e724.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1703a67a7b99f1bb9e0a2509b718fd22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07abdd990e240a61887695611cc250a5.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Boltzmann-Attention-Sampling-for-Image-Analysis-with-Small-Objects"><a href="#Boltzmann-Attention-Sampling-for-Image-Analysis-with-Small-Objects" class="headerlink" title="Boltzmann Attention Sampling for Image Analysis with Small Objects"></a>Boltzmann Attention Sampling for Image Analysis with Small Objects</h2><p><strong>Authors:Theodore Zhao, Sid Kiblawi, Naoto Usuyama, Ho Hin Lee, Sam Preston, Hoifung Poon, Mu Wei</strong></p>
<p>Detecting and segmenting small objects, such as lung nodules and tumor lesions, remains a critical challenge in image analysis. These objects often occupy less than 0.1% of an image, making traditional transformer architectures inefficient and prone to performance degradation due to redundant attention computations on irrelevant regions. Existing sparse attention mechanisms rely on rigid hierarchical structures, which are poorly suited for detecting small, variable, and uncertain object locations. In this paper, we propose BoltzFormer, a novel transformer-based architecture designed to address these challenges through dynamic sparse attention. BoltzFormer identifies and focuses attention on relevant areas by modeling uncertainty using a Boltzmann distribution with an annealing schedule. Initially, a higher temperature allows broader area sampling in early layers, when object location uncertainty is greatest. As the temperature decreases in later layers, attention becomes more focused, enhancing efficiency and accuracy. BoltzFormer seamlessly integrates into existing transformer architectures via a modular Boltzmann attention sampling mechanism. Comprehensive evaluations on benchmark datasets demonstrate that BoltzFormer significantly improves segmentation performance for small objects while reducing attention computation by an order of magnitude compared to previous state-of-the-art methods. </p>
<blockquote>
<p>æ£€æµ‹å¹¶åˆ†å‰²å›¾åƒä¸­å°ç›®æ ‡ç‰©ä½“ï¼ˆå¦‚è‚ºç»“èŠ‚å’Œè‚¿ç˜¤ç—…ç¶ï¼‰ä»ç„¶æ˜¯å›¾åƒåˆ†æä¸­çš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚è¿™äº›ç›®æ ‡ç‰©ä½“å¾€å¾€ä»…å å›¾åƒçš„ä¸åˆ°0.1%ï¼Œè¿™ä½¿å¾—ä¼ ç»Ÿçš„transformeræ¶æ„å› å¯¹æ— å…³ç´§è¦åŒºåŸŸçš„å†—ä½™æ³¨æ„åŠ›è®¡ç®—è€Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ï¼Œå¹¶ä¸”å®¹æ˜“å‘ç”Ÿæ€§èƒ½ä¸‹é™ã€‚ç°æœ‰çš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ä¾èµ–äºåƒµç¡¬çš„å±‚æ¬¡ç»“æ„ï¼Œè¿™å¯¹äºæ£€æµ‹ä½ç½®å°ã€å¤šå˜ä¸”ä¸ç¡®å®šçš„ç›®æ ‡ç‰©ä½“æ¥è¯´å¹¶ä¸é€‚åˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†BoltzFormerï¼Œè¿™æ˜¯ä¸€ç§åŸºäºtransformerçš„æ–°å‹æ¶æ„ï¼Œé€šè¿‡åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚BoltzFormeré€šè¿‡åˆ©ç”¨å¸¦æœ‰é€€ç«è®¡åˆ’çš„ç»å°”å…¹æ›¼åˆ†å¸ƒå¯¹ä¸ç¡®å®šæ€§è¿›è¡Œå»ºæ¨¡æ¥è¯†åˆ«å’Œé›†ä¸­å…³æ³¨ç›¸å…³åŒºåŸŸã€‚åœ¨å¯¹è±¡ä½ç½®ä¸ç¡®å®šæ€§æœ€å¤§çš„æ—©æœŸå±‚ä¸­ï¼Œè¾ƒé«˜çš„åˆå§‹æ¸©åº¦å…è®¸æ›´å¤§çš„åŒºåŸŸé‡‡æ ·ã€‚éšç€åç»­å±‚ä¸­çš„æ¸©åº¦é™ä½ï¼Œæ³¨æ„åŠ›å˜å¾—æ›´åŠ é›†ä¸­ï¼Œæé«˜äº†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚BoltzFormeré€šè¿‡æ¨¡å—åŒ–çš„ç»å°”å…¹æ›¼æ³¨æ„åŠ›é‡‡æ ·æœºåˆ¶æ— ç¼é›†æˆåˆ°ç°æœ‰çš„transformeræ¶æ„ä¸­ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒBoltzFormeråœ¨åˆ†å‰²å°ç›®æ ‡ç‰©ä½“æ–¹é¢æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼ŒåŒæ—¶å°†æ³¨æ„åŠ›è®¡ç®—æ—¶é—´å‡å°‘äº†æ•°å€äºç°æœ‰å…ˆè¿›æŠ€æœ¯æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02841v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºåŠ¨æ€ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶çš„BoltzFormeræ¨¡å‹ï¼Œç”¨äºè§£å†³å›¾åƒåˆ†æä¸­æ£€æµ‹ä¸åˆ†å‰²å°å¯¹è±¡ï¼ˆå¦‚è‚ºç»“èŠ‚å’Œè‚¿ç˜¤ç—…å˜ï¼‰çš„æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œå¼•å…¥æ³¢å°”å…¹æ›¼æ³¨æ„åŠ›é‡‡æ ·æœºåˆ¶å¯¹å›¾åƒä¸­çš„å°å¯¹è±¡è¿›è¡Œè¯†åˆ«å’Œå®šä½ï¼Œå¹¶åœ¨é™ä½è®¡ç®—å†—ä½™çš„åŒæ—¶æé«˜æ£€æµ‹æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ£€æµ‹ä¸åˆ†å‰²å›¾åƒä¸­çš„å°å¯¹è±¡æ˜¯å½“å‰å›¾åƒåˆ†æé¢†åŸŸçš„æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿtransformeræ¶æ„åœ¨å¤„ç†å°å¯¹è±¡æ—¶å­˜åœ¨æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œä¸»è¦åŸå› æ˜¯å†—ä½™çš„æ³¨æ„åŠ›è®¡ç®—å’Œä¸å…³æ³¨åŒºåŸŸçš„è¿‡åº¦å…³æ³¨ã€‚</li>
<li>ç°æœ‰çš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ä¾èµ–äºåˆšæ€§çš„å±‚æ¬¡ç»“æ„ï¼Œéš¾ä»¥é€‚åº”æ£€æµ‹å°ã€å¯å˜å’Œä¸ç¡®å®šçš„å¯¹è±¡ä½ç½®ã€‚</li>
<li>BoltzFormeré€šè¿‡åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶è§£å†³è¿™äº›é—®é¢˜ï¼Œå®ƒä½¿ç”¨æ³¢å°”å…¹æ›¼åˆ†å¸ƒå¯¹ä¸ç¡®å®šæ€§è¿›è¡Œå»ºæ¨¡å¹¶åˆ©ç”¨é€€ç«è®¡åˆ’æ¥è°ƒæ•´å…³æ³¨åº¦ã€‚</li>
<li>BoltzFormeræ¨¡å‹å¯ä»¥åœ¨æ—©æœŸå±‚ä¸­é‡‡æ ·æ›´å¹¿æ³›çš„åŒºåŸŸæ¥åº”å¯¹æ›´å¤§çš„ä½ç½®ä¸ç¡®å®šæ€§ï¼Œéšç€æ¸©åº¦é™ä½ï¼Œå…³æ³¨åº¦å˜å¾—æ›´åŠ é›†ä¸­ä»¥æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>BoltzFormeré€šè¿‡æ¨¡å—åŒ–è®¾è®¡æ— ç¼é›†æˆåˆ°ç°æœ‰transformeræ¶æ„ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02841">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f4ca6e1e250f7a2b90bb84055b17a1a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9bd705b3ee4a88386a7c7d809129cb5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e8cd93fb93885e304e017bfd6b4c6ec6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9065d5572c48cdde1f0238d85cc7e695.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-297af957db6629ad2cbcc0b477da2de0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="CADDreamer-CAD-Object-Generation-from-Single-view-Images"><a href="#CADDreamer-CAD-Object-Generation-from-Single-view-Images" class="headerlink" title="CADDreamer: CAD Object Generation from Single-view Images"></a>CADDreamer: CAD Object Generation from Single-view Images</h2><p><strong>Authors:Yuan Li, Cheng Lin, Yuan Liu, Xiaoxiao Long, Chenxu Zhang, Ningna Wang, Xin Li, Wenping Wang, Xiaohu Guo</strong></p>
<p>Diffusion-based 3D generation has made remarkable progress in recent years. However, existing 3D generative models often produce overly dense and unstructured meshes, which stand in stark contrast to the compact, structured, and sharply-edged Computer-Aided Design (CAD) models crafted by human designers. To address this gap, we introduce CADDreamer, a novel approach for generating boundary representations (B-rep) of CAD objects from a single image. CADDreamer employs a primitive-aware multi-view diffusion model that captures both local geometric details and high-level structural semantics during the generation process. By encoding primitive semantics into the color domain, the method leverages the strong priors of pre-trained diffusion models to align with well-defined primitives. This enables the inference of multi-view normal maps and semantic maps from a single image, facilitating the reconstruction of a mesh with primitive labels. Furthermore, we introduce geometric optimization techniques and topology-preserving extraction methods to mitigate noise and distortion in the generated primitives. These enhancements result in a complete and seamless B-rep of the CAD model. Experimental results demonstrate that our method effectively recovers high-quality CAD objects from single-view images. Compared to existing 3D generation techniques, the B-rep models produced by CADDreamer are compact in representation, clear in structure, sharp in edges, and watertight in topology. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒåŸºäºæ‰©æ•£çš„3Dç”ŸæˆæŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„3Dç”Ÿæˆæ¨¡å‹å¾€å¾€ä¼šäº§ç”Ÿè¿‡äºå¯†é›†ä¸”æ— ç»“æ„çš„ç½‘æ ¼ï¼Œè¿™ä¸äººç±»è®¾è®¡å¸ˆç²¾å¿ƒåˆ¶ä½œçš„ç´§å‡‘ã€ç»“æ„åŒ–ã€è¾¹ç¼˜æ¸…æ™°çš„è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹å½¢æˆé²œæ˜å¯¹æ¯”ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†CADDreamerï¼Œè¿™æ˜¯ä¸€ç§ä»å•å¼ å›¾åƒç”Ÿæˆè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰å¯¹è±¡è¾¹ç•Œè¡¨ç¤ºï¼ˆB-repï¼‰çš„æ–°æ–¹æ³•ã€‚CADDreameré‡‡ç”¨äº†ä¸€ç§åŸå§‹æ„ŸçŸ¥çš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­èƒ½å¤Ÿæ•æ‰å±€éƒ¨å‡ ä½•ç»†èŠ‚å’Œé«˜çº§ç»“æ„è¯­ä¹‰ã€‚é€šè¿‡å°†åŸå§‹è¯­ä¹‰ç¼–ç åˆ°é¢œè‰²åŸŸä¸­ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å¼ºå¤§å…ˆéªŒçŸ¥è¯†ä¸å®šä¹‰è‰¯å¥½çš„åŸå§‹æ•°æ®å¯¹é½ã€‚è¿™ä½¿å¾—èƒ½å¤Ÿä»å•å¼ å›¾åƒæ¨æ–­å¤šè§†è§’æ³•çº¿è´´å›¾å’Œè¯­ä¹‰è´´å›¾ï¼Œä¾¿äºå¸¦æœ‰åŸå§‹æ ‡ç­¾çš„ç½‘æ ¼é‡å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å‡ ä½•ä¼˜åŒ–æŠ€æœ¯å’Œæ‹“æ‰‘ä¿ç•™æå–æ–¹æ³•ï¼Œä»¥å‡è½»ç”ŸæˆåŸå§‹æ•°æ®ä¸­çš„å™ªå£°å’Œå¤±çœŸã€‚è¿™äº›å¢å¼ºåŠŸèƒ½å¯¼è‡´äº†ä¸€ä¸ªå®Œæ•´ä¸”æ— ç¼éš™çš„CADæ¨¡å‹çš„B-repã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿä»å•è§†å›¾å›¾åƒæœ‰æ•ˆåœ°æ¢å¤é«˜è´¨é‡çš„CADå¯¹è±¡ã€‚ä¸ç°æœ‰çš„3Dç”ŸæˆæŠ€æœ¯ç›¸æ¯”ï¼ŒCADDreameräº§ç”Ÿçš„B-repæ¨¡å‹åœ¨è¡¨ç¤ºä¸Šæ›´ç´§å‡‘ã€ç»“æ„ä¸Šæ›´æ¸…æ™°ã€è¾¹ç¼˜æ›´é”‹åˆ©ã€æ‹“æ‰‘ä¸Šæ›´æ— æ¸—æ¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20732v2">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>CADDreameræ˜¯ä¸€ä¸ªä»å•å¼ å›¾åƒç”ŸæˆCADå¯¹è±¡è¾¹ç•Œè¡¨ç¤ºï¼ˆB-repï¼‰çš„æ–°å‹æ–¹æ³•ï¼Œé€šè¿‡é‡‡ç”¨å…·å¤‡åŸå§‹æ„ŸçŸ¥èƒ½åŠ›çš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆäº†å±€éƒ¨å‡ ä½•ç»†èŠ‚å’Œé«˜çº§ç»“æ„è¯­ä¹‰ã€‚é€šè¿‡ç¼–ç åŸå§‹è¯­ä¹‰åˆ°è‰²å½©é¢†åŸŸï¼Œå¹¶å¼•å…¥å‡ ä½•ä¼˜åŒ–æŠ€æœ¯å’Œæ‹“æ‰‘ä¿ç•™æå–æ–¹æ³•ï¼Œæœ‰æ•ˆé‡å»ºç½‘æ ¼å¹¶å¸¦æœ‰åŸå§‹æ ‡ç­¾ã€‚ä¸ç°æœ‰3Dç”ŸæˆæŠ€æœ¯ç›¸æ¯”ï¼ŒCADDreamerç”Ÿæˆçš„B-repæ¨¡å‹åœ¨è¡¨ç¤ºä¸Šæ›´ç´§å‡‘ã€ç»“æ„ä¸Šæ›´æ¸…æ™°ã€è¾¹ç¼˜æ›´é”‹åˆ©ã€æ‹“æ‰‘æ›´é˜²æ°´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CADDreameræ˜¯ä¸€ç§ä»å•å¼ å›¾åƒç”ŸæˆCADå¯¹è±¡B-repçš„æ–°å‹æ–¹æ³•ã€‚</li>
<li>CADDreameré‡‡ç”¨åŸå§‹æ„ŸçŸ¥å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆå±€éƒ¨å‡ ä½•ç»†èŠ‚å’Œé«˜çº§ç»“æ„è¯­ä¹‰ã€‚</li>
<li>é€šè¿‡ç¼–ç åŸå§‹è¯­ä¹‰åˆ°è‰²å½©é¢†åŸŸï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å¼ºå…ˆéªŒçŸ¥è¯†ä¸å®šä¹‰çš„åŸå§‹ç›¸ç¬¦åˆã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥æ¨æ–­å‡ºå¤šè§†è§’æ³•çº¿å’Œè¯­ä¹‰åœ°å›¾ï¼Œä¾¿äºä»å•å¼ å›¾åƒé‡å»ºå¸¦æœ‰åŸå§‹æ ‡ç­¾çš„ç½‘æ ¼ã€‚</li>
<li>å¼•å…¥å‡ ä½•ä¼˜åŒ–æŠ€æœ¯å’Œæ‹“æ‰‘ä¿ç•™æå–æ–¹æ³•ï¼Œå‡å°‘ç”ŸæˆåŸå§‹å™ªå£°å’Œå¤±çœŸã€‚</li>
<li>CADDreamerç”Ÿæˆçš„B-repæ¨¡å‹åœ¨è¡¨ç¤ºä¸Šæ›´ç´§å‡‘ã€ç»“æ„ä¸Šæ¸…æ™°ã€è¾¹ç¼˜é”‹åˆ©ä¸”æ‹“æ‰‘é˜²æ°´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20732">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8241e6ab2f1a4da09068a4f91fe82679.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3feed073baa8899c7b8b5c68f3ca6fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88d4d65b7a44368689de3e1c7f4f7e02.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f4e5504f0d321d3936930a2d55856d03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89255493c700c559acf969ecf34de62e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="A-Comparative-Tutorial-of-the-Histogram-based-Image-Segmentation-Methods"><a href="#A-Comparative-Tutorial-of-the-Histogram-based-Image-Segmentation-Methods" class="headerlink" title="A Comparative Tutorial of the Histogram-based Image Segmentation Methods"></a>A Comparative Tutorial of the Histogram-based Image Segmentation Methods</h2><p><strong>Authors:ZhenZhou Wang</strong></p>
<p>The histogram of an image is the accurate graphical representation of the numerical grayscale distribution and it is also an estimate of the probability distribution of image pixels. Therefore, histogram has been widely adopted to calculate the clustering means and partitioning thresholds for image segmentation. There have been many classical histogram-based image segmentation methods proposed and played important roles in both academics and industry. In this tutorial, the histories and recent advances of the histogram-based image segmentation techniques are first reviewed and then they are divided into four categories: (1) the means-based method, (2) the Gaussian-mixture-model-based method, (3) the entropy-based method and (4) the feature-points-based method. The purpose of this tutorial is threefold: 1) to teach the principles of the classical histogram-based image segmentation methods to the interested readers; 2) to evaluate the advantages and disadvantages of these classical histogram-based image segmentation methods objectively; 3) to compare the performances of these classical histogram-based image segmentation methods with state-of-the-art deep learning based methods objectively. </p>
<blockquote>
<p>å›¾åƒçš„ç›´æ–¹å›¾æ˜¯æ•°å€¼ç°åº¦åˆ†å¸ƒçš„å‡†ç¡®å›¾å½¢è¡¨ç¤ºï¼Œä¹Ÿæ˜¯å¯¹å›¾åƒåƒç´ æ¦‚ç‡åˆ†å¸ƒçš„ä¼°è®¡ã€‚å› æ­¤ï¼Œç›´æ–¹å›¾å·²è¢«å¹¿æ³›åº”ç”¨äºè®¡ç®—å›¾åƒåˆ†å‰²çš„èšç±»å‡å€¼å’Œåˆ†åŒºé˜ˆå€¼ã€‚å·²æœ‰è®¸å¤šåŸºäºç›´æ–¹å›¾çš„å›¾åƒåˆ†å‰²æ–¹æ³•è¢«æå‡ºï¼Œåœ¨å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œéƒ½å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œé¦–å…ˆå›é¡¾äº†åŸºäºç›´æ–¹å›¾çš„å›¾åƒåˆ†å‰²æŠ€æœ¯çš„å†å²å’Œæœ€æ–°è¿›å±•ï¼Œç„¶åå°†å…¶åˆ†ä¸ºå››ç±»ï¼šï¼ˆ1ï¼‰åŸºäºå‡å€¼çš„æ–¹æ³•ï¼Œï¼ˆ2ï¼‰åŸºäºé«˜æ–¯æ··åˆæ¨¡å‹çš„æ–¹æ³•ï¼Œï¼ˆ3ï¼‰åŸºäºç†µçš„æ–¹æ³•å’Œï¼ˆ4ï¼‰åŸºäºç‰¹å¾ç‚¹çš„æ–¹æ³•ã€‚æœ¬æ•™ç¨‹çš„ç›®çš„æœ‰ä¸‰ç‚¹ï¼š1ï¼‰å‘æ„Ÿå…´è¶£çš„è¯»è€…ä¼ æˆåŸºäºç›´æ–¹å›¾çš„ç»å…¸å›¾åƒåˆ†å‰²æ–¹æ³•çš„åŸç†ï¼›2ï¼‰å®¢è§‚åœ°è¯„ä¼°è¿™äº›åŸºäºç›´æ–¹å›¾çš„ç»å…¸å›¾åƒåˆ†å‰²æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼›3ï¼‰å®¢è§‚åœ°æ¯”è¾ƒè¿™äº›åŸºäºç›´æ–¹å›¾çš„ç»å…¸å›¾åƒåˆ†å‰²æ–¹æ³•ä¸æœ€æ–°çš„åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18550v2">PDF</a> </p>
<p><strong>Summary</strong><br>     å›¾åƒç›´æ–¹å›¾æ˜¯ç°åº¦å€¼åˆ†å¸ƒçš„å‡†ç¡®å›¾å½¢è¡¨ç¤ºï¼Œä¹Ÿæ˜¯å›¾åƒåƒç´ æ¦‚ç‡åˆ†å¸ƒçš„ä¼°è®¡ã€‚ç›´æ–¹å›¾å·²è¢«å¹¿æ³›åº”ç”¨äºè®¡ç®—å›¾åƒåˆ†å‰²çš„èšç±»å‡å€¼å’Œåˆ†å‰²é˜ˆå€¼ã€‚æœ¬æ–‡é¦–å…ˆå›é¡¾äº†åŸºäºç›´æ–¹å›¾çš„å›¾åƒåˆ†å‰²æŠ€æœ¯çš„å†å²å’Œæœ€æ–°è¿›å±•ï¼Œç„¶åå°†å…¶åˆ†ä¸ºå››ç±»ï¼šï¼ˆ1ï¼‰åŸºäºå‡å€¼çš„æ–¹æ³•ã€ï¼ˆ2ï¼‰åŸºäºé«˜æ–¯æ··åˆæ¨¡å‹çš„æ–¹æ³•ã€ï¼ˆ3ï¼‰åŸºäºç†µçš„æ–¹æ³•å’Œï¼ˆ4ï¼‰åŸºäºç‰¹å¾ç‚¹çš„æ–¹æ³•ã€‚æœ¬æ–‡çš„ç›®çš„æ˜¯ï¼š1ï¼‰å‘æ„Ÿå…´è¶£çš„è¯»è€…ä¼ æˆåŸºäºç›´æ–¹å›¾çš„ç»å…¸å›¾åƒåˆ†å‰²æ–¹æ³•çš„åŸç†ï¼›2ï¼‰å®¢è§‚åœ°è¯„ä¼°è¿™äº›åŸºäºç›´æ–¹å›¾çš„ç»å…¸å›¾åƒåˆ†å‰²æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼›3ï¼‰æ¯”è¾ƒè¿™äº›åŸºäºç›´æ–¹å›¾çš„ç»å…¸å›¾åƒåˆ†å‰²æ–¹æ³•ä¸æœ€æ–°çš„æ·±åº¦å­¦ä¹ æ–¹æ³•çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒç›´æ–¹å›¾æ˜¯ç°åº¦å€¼åˆ†å¸ƒçš„å‡†ç¡®å›¾å½¢è¡¨ç¤ºï¼Œä¹Ÿæ˜¯å›¾åƒåƒç´ æ¦‚ç‡åˆ†å¸ƒçš„ä¼°è®¡ã€‚</li>
<li>ç›´æ–¹å›¾å·²è¢«å¹¿æ³›åº”ç”¨äºè®¡ç®—å›¾åƒåˆ†å‰²çš„èšç±»å‡å€¼å’Œåˆ†å‰²é˜ˆå€¼ã€‚</li>
<li>åŸºäºç›´æ–¹å›¾çš„å›¾åƒåˆ†å‰²æ–¹æ³•åˆ†ä¸ºå››ç±»ï¼šåŸºäºå‡å€¼ã€åŸºäºé«˜æ–¯æ··åˆæ¨¡å‹ã€åŸºäºç†µå’ŒåŸºäºç‰¹å¾ç‚¹çš„æ–¹æ³•ã€‚</li>
<li>æœ¬æ–‡æ—¨åœ¨ä¼ æˆåŸºäºç›´æ–¹å›¾çš„ç»å…¸å›¾åƒåˆ†å‰²æ–¹æ³•çš„åŸç†ã€‚</li>
<li>æ–‡ç« å®¢è§‚åœ°è¯„ä¼°äº†åŸºäºç›´æ–¹å›¾çš„ç»å…¸å›¾åƒåˆ†å‰²æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ã€‚</li>
<li>æœ¬æ–‡æ¯”è¾ƒäº†åŸºäºç›´æ–¹å›¾çš„ç»å…¸å›¾åƒåˆ†å‰²æ–¹æ³•ä¸æ·±åº¦å­¦ä¹ æ–¹æ³•çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18550">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-79f4e69cc374e76f0b680e95145e6a82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a68b99accae9a9ebf50811cd96f25ae0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bacea3c03fa4f137164f656923f0cd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07aad2a09e1225e4dc1bdd25e7cecae1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7ad088c782b51038b6f3486a15d01ce.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Tool-or-Tutor-Experimental-evidence-from-AI-deployment-in-cancer-diagnosis"><a href="#Tool-or-Tutor-Experimental-evidence-from-AI-deployment-in-cancer-diagnosis" class="headerlink" title="Tool or Tutor? Experimental evidence from AI deployment in cancer   diagnosis"></a>Tool or Tutor? Experimental evidence from AI deployment in cancer   diagnosis</h2><p><strong>Authors:Vivianna Fang He, Sihan Li, Phanish Puranam, Feng Lin</strong></p>
<p>Professionals increasingly use Artificial Intelligence (AI) to enhance their capabilities and assist with task execution. While prior research has examined these uses separately, their potential interaction remains underexplored. We propose that AI-driven training (â€œtutorâ€) and AI-assisted task completion (â€œtoolâ€) can have a joint effect on human capability and test this hypothesis in the context of lung cancer diagnosis. In a field experiment with 336 medical students, we manipulated AI deployment in training, in practice, and in both. Our findings reveal that while AI-integrated training and AI assistance independently improved diagnostic performance, their combination yielded the highest accuracy. These results underscore AIâ€™s dual role in enhancing human performance through both learning and real-time support, offering insights into AI deployment in professional settings where human expertise remains essential. </p>
<blockquote>
<p>ä¸“ä¸šäººå‘˜è¶Šæ¥è¶Šå¤šåœ°ä½¿ç”¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ¥å¢å¼ºè‡ªèº«èƒ½åŠ›å¹¶è¾…åŠ©æ‰§è¡Œä»»åŠ¡ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶å·²ç»åˆ†åˆ«ç ”ç©¶äº†è¿™äº›ç”¨é€”ï¼Œä½†å®ƒä»¬ä¹‹é—´çš„æ½œåœ¨äº¤äº’ä»ç„¶è¢«å¿½è§†ã€‚æˆ‘ä»¬æå‡ºäººå·¥æ™ºèƒ½é©±åŠ¨çš„è®­ç»ƒï¼ˆâ€œå¯¼å¸ˆâ€ï¼‰å’Œäººå·¥æ™ºèƒ½è¾…åŠ©çš„ä»»åŠ¡å®Œæˆï¼ˆâ€œå·¥å…·â€ï¼‰å¯ä»¥å¯¹äººç±»èƒ½åŠ›äº§ç”Ÿè”åˆæ•ˆåº”ï¼Œå¹¶åœ¨è‚ºç™Œè¯Šæ–­çš„èƒŒæ™¯ä¸‹æµ‹è¯•è¿™ä¸€å‡è®¾ã€‚åœ¨ä¸€é¡¹ç”±336ååŒ»å­¦ç”Ÿå‚åŠ çš„ç°åœºå®éªŒä¸­ï¼Œæˆ‘ä»¬å¯¹åŸ¹è®­ã€å®è·µå’Œä¸¤è€…éƒ½æ¶‰åŠçš„äººå·¥æ™ºèƒ½éƒ¨ç½²è¿›è¡Œäº†æ“ä½œã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè™½ç„¶äººå·¥æ™ºèƒ½é›†æˆåŸ¹è®­å’Œäººå·¥æ™ºèƒ½è¾…åŠ©ç‹¬ç«‹æé«˜äº†è¯Šæ–­æ€§èƒ½ï¼Œä½†å®ƒä»¬çš„ç»„åˆå´è·å¾—äº†æœ€é«˜çš„å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†äººå·¥æ™ºèƒ½é€šè¿‡å­¦ä¹ å’Œå®æ—¶æ”¯æŒå¢å¼ºäººç±»æ€§èƒ½çš„åŒé‡ä½œç”¨ï¼Œä¸ºåœ¨ä»éœ€è¦äººç±»ä¸“ä¸šçŸ¥è¯†çš„æƒ…å†µä¸‹åœ¨ä¸“ä¸šç¯å¢ƒä¸­éƒ¨ç½²äººå·¥æ™ºèƒ½æä¾›äº†è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16411v3">PDF</a> </p>
<p><strong>Summary</strong><br>     äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œæœ¬æ–‡æ¢è®¨äº†AIé©±åŠ¨çš„åŸ¹è®­ï¼ˆå¯¼å¸ˆï¼‰å’ŒAIè¾…åŠ©ä»»åŠ¡å®Œæˆï¼ˆå·¥å…·ï¼‰çš„è”åˆæ•ˆåº”ï¼Œç‰¹åˆ«æ˜¯åœ¨è‚ºç™Œè¯Šæ–­æ–¹é¢çš„åº”ç”¨ã€‚é€šè¿‡å¯¹336ååŒ»å­¦ä¸“ä¸šçš„å­¦ç”Ÿè¿›è¡Œå®åœ°å®éªŒï¼Œå‘ç°AIé›†æˆåŸ¹è®­å’ŒAIè¾…åŠ©æ”¯æŒèƒ½å„è‡ªæé«˜è¯Šæ–­æ€§èƒ½ï¼ŒäºŒè€…çš„ç»“åˆä½¿ç”¨èƒ½è¾¾åˆ°æœ€é«˜å‡†ç¡®ç‡ã€‚è¿™è¡¨æ˜AIåœ¨æå‡äººç±»è¡¨ç°æ–¹é¢æ‰®æ¼”ç€åŒé‡è§’è‰²ï¼Œæ—¢å¯ä»¥é€šè¿‡å­¦ä¹ æå‡èƒ½åŠ›ï¼Œä¹Ÿå¯ä»¥åœ¨å®æ—¶æ”¯æŒæ–¹é¢å‘æŒ¥ä¼˜åŠ¿ã€‚åœ¨ä¸“ä¸šç¯å¢ƒä¸­éƒ¨ç½²AIæ—¶ï¼Œåº”ç»“åˆä¸¤è€…æ¥å‘æŒ¥AIçš„æœ€å¤§æ•ˆç”¨ã€‚åŒæ—¶åº”ç»§ç»­åˆ©ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯ä¸æ–­æé«˜è¯Šæ–­å‡†ç¡®æ€§ï¼ŒåŒæ—¶ç¡®ä¿äººç±»ä¸“å®¶çš„ä½œç”¨å¾—åˆ°å……åˆ†å‘æŒ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AIåœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨é€æ¸æ™®åŠï¼Œå°¤å…¶åœ¨è‚ºç™Œè¯Šæ–­æ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚</li>
<li>AIé©±åŠ¨çš„åŸ¹è®­ï¼ˆå¯¼å¸ˆï¼‰å’ŒAIè¾…åŠ©ä»»åŠ¡å®Œæˆï¼ˆå·¥å…·ï¼‰åœ¨æé«˜è¯Šæ–­æ€§èƒ½æ–¹é¢å…·æœ‰è”åˆæ•ˆåº”ã€‚</li>
<li>é€šè¿‡å®åœ°å®éªŒå‘ç°ï¼ŒAIé›†æˆåŸ¹è®­å’ŒAIè¾…åŠ©æ”¯æŒå„è‡ªèƒ½å¤Ÿæé«˜è¯Šæ–­æ€§èƒ½ï¼ŒäºŒè€…ç»“åˆä½¿ç”¨èƒ½è¾¾åˆ°æœ€é«˜å‡†ç¡®ç‡ã€‚</li>
<li>AIåœ¨æé«˜äººç±»è¡¨ç°æ–¹é¢æ‰®æ¼”ç€åŒé‡è§’è‰²ï¼Œæ—¢å¯ä»¥é€šè¿‡å­¦ä¹ æå‡èƒ½åŠ›ï¼Œä¹Ÿå¯ä»¥åœ¨å®æ—¶æ”¯æŒæ–¹é¢å‘æŒ¥ä¼˜åŠ¿ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16411">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a128e73275aec75057661c01932b2b24.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-05/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-4d0253dd96c8675b9fc9a2d1416e1913.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-05  OmniTalker Real-Time Text-Driven Talking Head Generation with   In-Context Audio-Visual Style Replication
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-05/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b00cced02d5d259e181a498f1dffd934.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-05  Concept Lancet Image Editing with Compositional Representation   Transplant
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18723.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
