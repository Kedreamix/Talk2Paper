<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-04-05  TailedCore Few-Shot Sampling for Unsupervised Long-Tail Noisy Anomaly   Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-38120bc5a0120949d143f7e53f661fbe.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    33 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-05-更新"><a href="#2025-04-05-更新" class="headerlink" title="2025-04-05 更新"></a>2025-04-05 更新</h1><h2 id="TailedCore-Few-Shot-Sampling-for-Unsupervised-Long-Tail-Noisy-Anomaly-Detection"><a href="#TailedCore-Few-Shot-Sampling-for-Unsupervised-Long-Tail-Noisy-Anomaly-Detection" class="headerlink" title="TailedCore: Few-Shot Sampling for Unsupervised Long-Tail Noisy Anomaly   Detection"></a>TailedCore: Few-Shot Sampling for Unsupervised Long-Tail Noisy Anomaly   Detection</h2><p><strong>Authors:Yoon Gyo Jung, Jaewoo Park, Jaeho Yoon, Kuan-Chuan Peng, Wonchul Kim, Andrew Beng Jin Teoh, Octavia Camps</strong></p>
<p>We aim to solve unsupervised anomaly detection in a practical challenging environment where the normal dataset is both contaminated with defective regions and its product class distribution is tailed but unknown. We observe that existing models suffer from tail-versus-noise trade-off where if a model is robust against pixel noise, then its performance deteriorates on tail class samples, and vice versa. To mitigate the issue, we handle the tail class and noise samples independently. To this end, we propose TailSampler, a novel class size predictor that estimates the class cardinality of samples based on a symmetric assumption on the class-wise distribution of embedding similarities. TailSampler can be utilized to sample the tail class samples exclusively, allowing to handle them separately. Based on these facets, we build a memory-based anomaly detection model TailedCore, whose memory both well captures tail class information and is noise-robust. We extensively validate the effectiveness of TailedCore on the unsupervised long-tail noisy anomaly detection setting, and show that TailedCore outperforms the state-of-the-art in most settings. </p>
<blockquote>
<p>我们致力于在实际具有挑战性的环境中解决无监督异常检测问题，在这种环境中，正常数据集既受到缺陷区域的污染，其产品类别分布又存在尾部但未知。我们发现现有模型存在尾部与噪声之间的权衡问题，即模型如果对像素噪声具有很强的鲁棒性，那么它在尾部类别样本上的性能就会下降，反之亦然。为了解决这一问题，我们对尾部类别和噪声样本进行了独立处理。为此，我们提出了TailSampler，这是一种新型的类别大小预测器，它基于嵌入相似性在类别分布上的对称假设来估计样本的类别基数。TailSampler可用于仅对尾部类别样本进行采样，从而可以单独处理它们。基于这些方面，我们构建了一个基于内存的异常检测模型TailedCore，其内存能够很好地捕获尾部类别信息并且具有抗噪声性。我们在无监督的长尾噪声异常检测设置上进行了广泛的验证，结果表明TailedCore在大多数设置中都优于最新技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02775v1">PDF</a> Accepted to CVPR2025</p>
<p><strong>Summary</strong></p>
<p>本文旨在解决在实际挑战环境下无监督异常检测问题，其中正常数据集含有缺陷区域且其产品类别分布长尾未知。现有模型面临尾部与噪声之间的权衡问题。为解决此问题，本文独立处理尾部类别和噪声样本。为此，提出了TailSampler，一种基于嵌入相似性对称假设的新型类别大小预测器，可估算样本类别基数。TailSampler可用于单独采样尾部类别样本。基于此，构建了一个基于内存的异常检测模型TailedCore，其内存能够很好地捕捉尾部类别信息且对噪声具有鲁棒性。在无人监督的长期尾部噪声异常检测设置中验证了TailedCore的有效性，并显示其在大多数设置中优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>解决实际挑战环境下的无监督异常检测问题，其中数据集存在缺陷和未知的长尾产品类别分布。</li>
<li>现有模型面临尾部类别与噪声之间的权衡问题。</li>
<li>提出TailSampler，一种新型类别大小预测器，可估算样本类别基数，用于独立处理尾部类别样本。</li>
<li>基于TailSampler构建TailedCore模型，该模型在捕捉尾部类别信息和噪声鲁棒性方面表现出色。</li>
<li>TailedCore通过单独处理尾部类别和噪声样本，提高了异常检测性能。</li>
<li>在长期尾部噪声异常检测设置的无人监督环境中验证了TailedCore的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02775">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5a241123f69eeee427e5e9a114f5a358.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50b2dd23442f3c2d0c9faeac7e2fb2b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e05e7325fff8f256f63e15632f3e2d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-805a33ed7942518d834513117d10372e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="BOOST-Bootstrapping-Strategy-Driven-Reasoning-Programs-for-Program-Guided-Fact-Checking"><a href="#BOOST-Bootstrapping-Strategy-Driven-Reasoning-Programs-for-Program-Guided-Fact-Checking" class="headerlink" title="BOOST: Bootstrapping Strategy-Driven Reasoning Programs for   Program-Guided Fact-Checking"></a>BOOST: Bootstrapping Strategy-Driven Reasoning Programs for   Program-Guided Fact-Checking</h2><p><strong>Authors:Qisheng Hu, Quanyu Long, Wenya Wang</strong></p>
<p>Program-guided reasoning has shown promise in complex claim fact-checking by decomposing claims into function calls and executing reasoning programs. However, prior work primarily relies on few-shot in-context learning (ICL) with ad-hoc demonstrations, which limit program diversity and require manual design with substantial domain knowledge. Fundamentally, the underlying principles of effective reasoning program generation still remain underexplored, making it challenging to construct effective demonstrations. To address this, we propose BOOST, a bootstrapping-based framework for few-shot reasoning program generation. BOOST explicitly integrates claim decomposition and information-gathering strategies as structural guidance for program generation, iteratively refining bootstrapped demonstrations in a strategy-driven and data-centric manner without human intervention. This enables a seamless transition from zero-shot to few-shot strategic program-guided learning, enhancing interpretability and effectiveness. Experimental results show that BOOST outperforms prior few-shot baselines in both zero-shot and few-shot settings for complex claim verification. </p>
<blockquote>
<p>程序引导推理通过将声明分解为函数调用并执行推理程序，在复杂的声明查证中显示出良好的前景。然而，早期的工作主要依赖于基于上下文的学习(ICL)少数样本演示，这限制了程序的多样性并需要大量手动设计知识和经验的投入。从根本上讲，有效推理程序生成的基本原理尚未得到充分研究，这增加了构建有效演示的挑战性。为了解决这一问题，我们提出了基于BOOST策略的少数样本推理程序生成框架。BOOST明确地将声明分解和信息收集策略整合为程序生成的结构性指导，以策略驱动和数据为中心的方式无人工干预地迭代优化自举演示。这实现了从零样本到少数样本战略程序引导学习的无缝过渡，提高了可解释性和有效性。实验结果表明，在零样本和少数样本场景下，BOOST对复杂的声明验证任务都超过了之前的少数样本基线模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02467v1">PDF</a> 18 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>本文探讨了基于程序的推理在复杂声明核查中的应用前景。然而，先前的工作主要依赖于少数场景下的上下文学习（ICL），这种方式限制了程序多样性并需要大量领域知识来设计。针对这一问题，本文提出了基于助推技术的少数场景推理程序生成框架BOOST。BOOST通过整合声明分解和信息收集策略作为程序生成的引导结构，并以策略驱动和数据中心的方式进行迭代改进示范程序，提高了程序引导学习的效果。实验结果显示，BOOST在零场景和少数场景下的复杂声明验证中均优于先前的少数场景基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>程序引导推理在复杂声明核查中展现潜力。</li>
<li>先前的工作主要依赖少数场景下的上下文学习（ICL），限制了程序多样性并需要大量领域知识。</li>
<li>BOOST框架通过整合声明分解和信息收集策略来引导程序生成。</li>
<li>BOOST通过迭代改进示范程序，实现了从零场景到少数场景的程序引导学习的无缝过渡。</li>
<li>BOOST提高了程序引导学习的解释性和有效性。</li>
<li>实验结果显示，BOOST在复杂声明验证方面优于先前的少数场景基线方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02467">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a5c192266e1860c5a77401245391d540.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a1e955f7979a2312daee97c56cca5bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed2210debdbbaafc3deb27f8bf5ac23d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Taylor-Series-Inspired-Local-Structure-Fitting-Network-for-Few-shot-Point-Cloud-Semantic-Segmentation"><a href="#Taylor-Series-Inspired-Local-Structure-Fitting-Network-for-Few-shot-Point-Cloud-Semantic-Segmentation" class="headerlink" title="Taylor Series-Inspired Local Structure Fitting Network for Few-shot   Point Cloud Semantic Segmentation"></a>Taylor Series-Inspired Local Structure Fitting Network for Few-shot   Point Cloud Semantic Segmentation</h2><p><strong>Authors:Changshuo Wang, Shuting He, Xiang Fang, Meiqing Wu, Siew-Kei Lam, Prayag Tiwari</strong></p>
<p>Few-shot point cloud semantic segmentation aims to accurately segment “unseen” new categories in point cloud scenes using limited labeled data. However, pretraining-based methods not only introduce excessive time overhead but also overlook the local structure representation among irregular point clouds. To address these issues, we propose a pretraining-free local structure fitting network for few-shot point cloud semantic segmentation, named TaylorSeg. Specifically, inspired by Taylor series, we treat the local structure representation of irregular point clouds as a polynomial fitting problem and propose a novel local structure fitting convolution, called TaylorConv. This convolution learns the low-order basic information and high-order refined information of point clouds from explicit encoding of local geometric structures. Then, using TaylorConv as the basic component, we construct two variants of TaylorSeg: a non-parametric TaylorSeg-NN and a parametric TaylorSeg-PN. The former can achieve performance comparable to existing parametric models without pretraining. For the latter, we equip it with an Adaptive Push-Pull (APP) module to mitigate the feature distribution differences between the query set and the support set. Extensive experiments validate the effectiveness of the proposed method. Notably, under the 2-way 1-shot setting, TaylorSeg-PN achieves improvements of +2.28% and +4.37% mIoU on the S3DIS and ScanNet datasets respectively, compared to the previous state-of-the-art methods. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/changshuowang/TaylorSeg">https://github.com/changshuowang/TaylorSeg</a>. </p>
<blockquote>
<p>少量点云语义分割旨在利用有限的标记数据准确分割点云场景中的“未见”新类别。然而，基于预训练的方法不仅引入了过多的时间开销，而且忽略了不规则点云之间的局部结构表示。为了解决这些问题，我们提出了一种用于少量点云语义分割的无预训练局部结构拟合网络，名为TaylorSeg。具体地说，受到泰勒级数（Taylor series）的启发，我们将不规则点云的局部结构表示视为多项式拟合问题，并提出了一种新的局部结构拟合卷积，称为TaylorConv。这种卷积通过显式编码局部几何结构来学习点云的低阶基本信息和高阶精细信息。然后，以TaylorConv为基本组件，我们构建了TaylorSeg的两个变体：非参数的TaylorSeg-NN和参数的TaylorSeg-PN。前者可以在没有预训练的情况下实现与现有参数模型相当的性能。对于后者，我们为其配备了一个自适应推拉（APP）模块，以减轻查询集和支持集之间的特征分布差异。大量实验验证了所提出方法的有效性。值得注意的是，在2路1炮的设置下，TaylorSeg-PN在S3DIS和ScanNet数据集上的mIoU分别提高了+2.28%和+4.37%，与之前的最新方法相比。我们的代码位于<a target="_blank" rel="noopener" href="https://github.com/changshuowang/TaylorSeg%E3%80%82">https://github.com/changshuowang/TaylorSeg。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02454v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>针对少样本点云语义分割任务，我们提出了一个无需预训练的局部结构拟合网络——TaylorSeg。它采用泰勒级数作为灵感来源，将不规则点云的局部结构表示作为多项式拟合问题，并创新性地提出了泰勒卷积（TaylorConv）。该卷积能从局部几何结构的显式编码中学习点云的低阶基础信息和高阶精细信息。使用TaylorConv作为基础组件，我们构建了非参数的TaylorSeg-NN和参数化的TaylorSeg-PN两种变体。前者无需预训练即可达到与现有参数模型相当的性能，而后者配备了自适应推拉（APP）模块以减小查询集和支持集之间的特征分布差异。实验证明，该方法有效。在2路1射击设置下，TaylorSeg-PN在S3DIS和ScanNet数据集上的mIoU分别提高了+2.28%和+4.37%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shot point cloud semantic segmentation旨在利用有限的标记数据准确分割点云场景中的“未见”新类别。</li>
<li>提出的TaylorSeg网络无需预训练，专注于处理不规则点云的局部结构表示。</li>
<li>泰勒卷积（TaylorConv）作为核心组件，用于学习点云的局部几何结构信息。</li>
<li>TaylorSeg包括非参数的TaylorSeg-NN和参数化的TaylorSeg-PN两种变体。</li>
<li>TaylorSeg-PN配备了自适应推拉（APP）模块以减小查询集和支持集之间的特征分布差异。</li>
<li>实验结果显示，TaylorSeg在少样本点云语义分割任务中取得了显著效果。</li>
<li>在特定设置下，TaylorSeg-PN相较于现有方法提高了mIoU，表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02454">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8a6b0e359b6099e1553fa85fc0d6df1e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a5634c7d06f00c3fe463e8263f19dc37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c65537fe7e15ca34eeaf590fc05e6b00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c729b5f97491c5b733a8e56508c33ef2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Towards-Interpretable-Soft-Prompts"><a href="#Towards-Interpretable-Soft-Prompts" class="headerlink" title="Towards Interpretable Soft Prompts"></a>Towards Interpretable Soft Prompts</h2><p><strong>Authors:Oam Patel, Jason Wang, Nikhil Shivakumar Nayak, Suraj Srinivas, Himabindu Lakkaraju</strong></p>
<p>Soft prompts have been popularized as a cheap and easy way to improve task-specific LLM performance beyond few-shot prompts. Despite their origin as an automated prompting method, however, soft prompts and other trainable prompts remain a black-box method with no immediately interpretable connections to prompting. We create a novel theoretical framework for evaluating the interpretability of trainable prompts based on two desiderata: faithfulness and scrutability. We find that existing methods do not naturally satisfy our proposed interpretability criterion. Instead, our framework inspires a new direction of trainable prompting methods that explicitly optimizes for interpretability. To this end, we formulate and test new interpretability-oriented objective functions for two state-of-the-art prompt tuners: Hard Prompts Made Easy (PEZ) and RLPrompt. Our experiments with GPT-2 demonstrate a fundamental trade-off between interpretability and the task-performance of the trainable prompt, explicating the hardness of the soft prompt interpretability problem and revealing odd behavior that arises when one optimizes for an interpretability proxy. </p>
<blockquote>
<p>软提示作为一种低成本、易操作的方法来提升特定任务的LLM性能，在超出少数几种场景提示下受到广泛欢迎。然而，尽管其起源于一种自动化提示方法，软提示和其他可训练的提示仍然是黑箱方法，与提示之间没有直接的、可解释的关联。我们基于两个期望（忠实性和可审查性）为评估可训练提示的可解释性创建了一个新型的理论框架。我们发现现有方法并不能自然满足我们提出的可解释性标准。相反，我们的框架为可训练提示方法提供了新的方向，该方法旨在明确优化可解释性。为此，我们为两种先进的提示调整器制定了新的面向可解释性的目标函数，并对它们进行了测试：易操作硬提示（PEZ）和RLPrompt。我们在GPT-2上的实验展示了可训练提示的可解释性与任务性能之间的基本权衡，揭示了软提示的可解释性问题的复杂性，并揭示了当优化一个可解释性的代理时出现的异常行为。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02144v1">PDF</a> 9 pages, 8 figures</p>
<p><strong>Summary</strong><br>     本文介绍了软提示作为提高任务特定大型语言模型性能的一种廉价且简单的方法，但其作为自动化提示方法存在黑箱问题，缺乏与提示的直接联系。因此，作者提出了一个评估可训练提示解释性的新理论框架，包括忠实度和审查性两个要素。实验表明，现有方法无法满足作者提出的解释性标准，新框架启发了一种新的可训练提示方法，该方法明确优化了解释性。同时作者提出了面向两种先进提示调整器的新解释性目标函数，并通过GPT-2的实验验证了软提示解释性问题的基本权衡，展示了优化解释性代理时的异常行为。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>软提示作为一种提高任务特定大型语言模型性能的方法受到欢迎，但其存在黑箱问题。</li>
<li>作者提出了一个评估可训练提示解释性的新理论框架，包括忠实度和审查性两个要素。</li>
<li>现有方法无法满足作者提出的解释性标准，需要新的方向来优化可训练提示方法。</li>
<li>作者提出了面向两种先进提示调整器的新解释性目标函数。</li>
<li>实验表明软提示存在解释性问题，优化解释性时存在权衡。</li>
<li>软提示的异常行为在优化解释性代理时更为明显。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02144">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-54aea866b74eff92612ecbd58e71f96b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22919fce618f61138ee9cc55feaf68aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15e835dc4e572ffa8a70ff4f9829bf42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01e2f1547816490d0c596fad46f8f004.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-584ea76ff559d454c8eb5df9e407c9df.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MageSQL-Enhancing-In-context-Learning-for-Text-to-SQL-Applications-with-Large-Language-Models"><a href="#MageSQL-Enhancing-In-context-Learning-for-Text-to-SQL-Applications-with-Large-Language-Models" class="headerlink" title="MageSQL: Enhancing In-context Learning for Text-to-SQL Applications with   Large Language Models"></a>MageSQL: Enhancing In-context Learning for Text-to-SQL Applications with   Large Language Models</h2><p><strong>Authors:Chen Shen, Jin Wang, Sajjadur Rahman, Eser Kandogan</strong></p>
<p>The text-to-SQL problem aims to translate natural language questions into SQL statements to ease the interaction between database systems and end users. Recently, Large Language Models (LLMs) have exhibited impressive capabilities in a variety of tasks, including text-to-SQL. While prior works have explored various strategies for prompting LLMs to generate SQL statements, they still fall short of fully harnessing the power of LLM due to the lack of (1) high-quality contextual information when constructing the prompts and (2) robust feedback mechanisms to correct translation errors. To address these challenges, we propose MageSQL, a text-to-SQL approach based on in-context learning over LLMs. MageSQL explores a suite of techniques that leverage the syntax and semantics of SQL queries to identify relevant few-shot demonstrations as context for prompting LLMs. In particular, we introduce a graph-based demonstration selection method – the first of its kind in the text-to-SQL problem – that leverages graph contrastive learning adapted with SQL-specific data augmentation strategies. Furthermore, an error correction module is proposed to detect and fix potential inaccuracies in the generated SQL query. We conduct comprehensive evaluations on several benchmarking datasets. The results show that our proposed methods outperform state-of-the-art methods by an obvious margin. </p>
<blockquote>
<p>文本到SQL的问题旨在将自然语言问题翻译成SQL语句，以简化数据库系统与最终用户之间的交互。最近，大型语言模型（LLM）在各种任务中表现出了令人印象深刻的能力，包括文本到SQL。虽然先前的工作已经探索了各种策略来提示LLM生成SQL语句，但由于（1）在构建提示时缺乏高质量上下文信息，以及（2）缺乏纠正翻译错误的稳健反馈机制，它们还没有充分利用LLM的威力。为了解决这些挑战，我们提出了MageSQL，这是一种基于LLM的上下文学习的文本到SQL方法。MageSQL探索了一系列技术，利用SQL查询的语法和语义来识别相关的小规模演示作为提示LLM的上下文。特别是，我们引入了一种基于图的演示选择方法——这是文本到SQL问题中的第一种——该方法利用与SQL特定的数据增强策略相适应的图对比学习。此外，还提出了一个错误修正模块，用于检测和修复生成的SQL查询中可能存在的错误。我们在几个基准数据集上进行了全面评估。结果表明，我们提出的方法明显优于最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02055v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于自然语言的大型语言模型（LLM）在文本到SQL转换任务中展现出强大的能力，但仍面临缺乏高质量上下文信息和纠错机制的问题。本研究提出了一种基于LLM的上下文学习的新方法MageSQL，利用SQL查询的语法和语义，通过图形对比学习等技术选取相关示范作为上下文提示。同时，MageSQL还具备错误修正模块，可以检测和修复生成的SQL查询中的潜在错误。在多个基准数据集上的综合评估显示，该方法显著优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在文本到SQL转换任务中表现出强大的能力。</li>
<li>当前研究面临缺乏高质量上下文信息和纠错机制的挑战。</li>
<li>MageSQL是一种基于LLM的上下文学习新方法，旨在解决这些问题。</li>
<li>MageSQL利用SQL查询的语法和语义，通过图形对比学习等技术选取相关示范作为上下文提示。</li>
<li>MageSQL具备错误修正模块，可以检测和修复生成的SQL查询中的潜在错误。</li>
<li>在多个基准数据集上的综合评估显示，MageSQL显著优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02055">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-38120bc5a0120949d143f7e53f661fbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-666f33b22f2e983bf20bebe43746cf5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e15b417a92b28bf5e46a64c498c81b58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d0ff038a2c97e7383909ac2b37353d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2ef98634c156fd31f735a4875d97a9b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85aad909007c1f18c0f3553cd2fb692e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Model-Agnostic-Meta-Learning-for-Fault-Diagnosis-of-Induction-Motors-in-Data-Scarce-Environments-with-Varying-Operating-Conditions-and-Electric-Drive-Noise"><a href="#Model-Agnostic-Meta-Learning-for-Fault-Diagnosis-of-Induction-Motors-in-Data-Scarce-Environments-with-Varying-Operating-Conditions-and-Electric-Drive-Noise" class="headerlink" title="Model-Agnostic Meta-Learning for Fault Diagnosis of Induction Motors in   Data-Scarce Environments with Varying Operating Conditions and Electric Drive   Noise"></a>Model-Agnostic Meta-Learning for Fault Diagnosis of Induction Motors in   Data-Scarce Environments with Varying Operating Conditions and Electric Drive   Noise</h2><p><strong>Authors:Ali Pourghoraba, MohammadSadegh KhajueeZadeh, Ali Amini, Abolfazl Vahedi, Gholam Reza Agah, Akbar Rahideh</strong></p>
<p>Reliable mechanical fault detection with limited data is crucial for the effective operation of induction machines, particularly given the real-world challenges present in industrial datasets, such as significant imbalances between healthy and faulty samples and the scarcity of data representing faulty conditions. This research introduces an innovative meta-learning approach to address these issues, focusing on mechanical fault detection in induction motors across diverse operating conditions while mitigating the adverse effects of drive noise in scenarios with limited data. The process of identifying faults under varying operating conditions is framed as a few-shot classification challenge and approached through a model-agnostic meta-learning strategy. Specifically, this approach begins with training a meta-learner across multiple interconnected fault-diagnosis tasks conducted under different operating conditions. In this stage, cross-entropy is utilized to optimize parameters and develop a robust representation of the tasks. Subsequently, the parameters of the meta-learner are fine-tuned for new tasks, enabling rapid adaptation using only a small number of samples. This method achieves excellent accuracy in fault detection across various conditions, even when data availability is restricted. The findings indicate that the proposed model outperforms other sophisticated techniques, providing enhanced generalization and quicker adaptation. The accuracy of fault diagnosis reaches a minimum of 99%, underscoring the model’s effectiveness for reliable fault identification. </p>
<blockquote>
<p>在有限的条件下实现可靠的机械故障检测对于感应电机的有效运行至关重要，尤其是在面对工业数据集中的现实挑战时，如健康与故障样本之间的显著差异以及代表故障状况的数据稀缺。本研究引入了一种创新的元学习方法来解决这些问题，专注于在多种操作条件下对感应电机进行机械故障检测，同时减轻在数据有限的情况下驱动噪声的不利影响。在不同操作条件下识别故障的过程被构建为一个小样本分类挑战，并通过模型无关的元学习策略来解决。具体来说，该方法首先在一个由多个操作条件下的互联故障诊断任务组成的元学习器上进行训练。在这个阶段，利用交叉熵来优化参数并发展出任务的稳健表示。随后，元学习器的参数被微调以适应新任务，使用少量样本即可实现快速适应。即使在数据有限的情况下，该方法在各种条件下的故障检测中也实现了出色的准确性。研究结果表明，所提出的模型优于其他复杂技术，提供了增强的泛化能力和更快的适应速度。故障诊断的准确性至少达到99%，突显了该模型在可靠故障识别方面的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04255v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究引入了一种元学习的方法来解决有限的工业数据下的机械故障检测问题。通过模型无关的元学习策略，对在不同操作条件下的故障检测任务进行训练和学习。此模型不仅能实现不同操作条件下的可靠机械故障检测，并在数据量小的情况下减少噪声干扰的负面影响，还具有针对新任务的快速适应和卓越的诊断准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究背景强调了机械故障检测在工业环境中的重要性，特别是在数据有限的情况下。</li>
<li>提出了一种创新的元学习方法来解决机械故障检测问题。</li>
<li>方法涉及在多种操作条件下进行故障检测任务的训练和学习。</li>
<li>使用交叉熵优化参数，并开发稳健的任务表示。</li>
<li>模型可以通过微调快速适应新任务，即使只有少量样本。</li>
<li>故障检测准确率极高，最低达到99%，证明了模型的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04255">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3a973c67c4f79310a77d6235de635eee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73124c9f8078b3de61c30238e194cad4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbf90416ae160baa63bf4613ceb79daa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-741ef20bf3a8f4c33cf54211851a6cb4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TAMT-Temporal-Aware-Model-Tuning-for-Cross-Domain-Few-Shot-Action-Recognition"><a href="#TAMT-Temporal-Aware-Model-Tuning-for-Cross-Domain-Few-Shot-Action-Recognition" class="headerlink" title="TAMT: Temporal-Aware Model Tuning for Cross-Domain Few-Shot Action   Recognition"></a>TAMT: Temporal-Aware Model Tuning for Cross-Domain Few-Shot Action   Recognition</h2><p><strong>Authors:Yilong Wang, Zilin Gao, Qilong Wang, Zhaofeng Chen, Peihua Li, Qinghua Hu</strong></p>
<p>Going beyond few-shot action recognition (FSAR), cross-domain FSAR (CDFSAR) has attracted recent research interests by solving the domain gap lying in source-to-target transfer learning. Existing CDFSAR methods mainly focus on joint training of source and target data to mitigate the side effect of domain gap. However, such kind of methods suffer from two limitations: First, pair-wise joint training requires retraining deep models in case of one source data and multiple target ones, which incurs heavy computation cost, especially for large source and small target data. Second, pre-trained models after joint training are adopted to target domain in a straightforward manner, hardly taking full potential of pre-trained models and then limiting recognition performance. To overcome above limitations, this paper proposes a simple yet effective baseline, namely Temporal-Aware Model Tuning (TAMT) for CDFSAR. Specifically, our TAMT involves a decoupled paradigm by performing pre-training on source data and fine-tuning target data, which avoids retraining for multiple target data with single source. To effectively and efficiently explore the potential of pre-trained models in transferring to target domain, our TAMT proposes a Hierarchical Temporal Tuning Network (HTTN), whose core involves local temporal-aware adapters (TAA) and a global temporal-aware moment tuning (GTMT). Particularly, TAA learns few parameters to recalibrate the intermediate features of frozen pre-trained models, enabling efficient adaptation to target domains. Furthermore, GTMT helps to generate powerful video representations, improving match performance on the target domain. Experiments on several widely used video benchmarks show our TAMT outperforms the recently proposed counterparts by 13%$\sim$31%, achieving new state-of-the-art CDFSAR results. </p>
<blockquote>
<p>超越少样本动作识别（FSAR），跨域FSAR（CDFSAR）最近引起了研究兴趣，解决了源到目标迁移学习中的域差距问题。现有的CDFSAR方法主要集中在源和目标数据的联合训练，以减轻域差距的副作用。然而，这种方法存在两个局限性：首先，配对联合训练需要在源数据单一而目标数据多样的情况下重新训练深度模型，这带来了沉重的计算成本，特别是在源数据量大而目标数据量小的情况下。其次，联合训练后的预训练模型直接应用于目标域，很难充分利用预训练模型的潜力，从而限制了识别性能。为了克服上述局限性，本文提出了一种简单有效的基线方法，即用于CDFSAR的Temporal-Aware模型调整（TAMT）。具体来说，我们的TAMT通过源数据预训练和目标数据微调的方式实现了分离的模式，避免了使用单一源数据对多个目标数据进行重新训练。为了有效且高效地探索预训练模型转移到目标域的潜力，我们的TAMT提出了分层时间调整网络（HTTN），其核心包括局部时间感知适配器（TAA）和全局时间感知时刻调整（GTMT）。特别是，TAA学习少量参数以重新校准冻结的预训练模型的中间特征，使其能够高效地适应目标域。此外，GTMT有助于生成强大的视频表示，提高在目标域上的匹配性能。在几个广泛使用的视频基准测试上的实验表明，我们的TAMT相较于最近提出的同类产品，性能提高了13%~31%，取得了最新的CDFSAR结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19041v2">PDF</a> Accepted by CVPR 2025; Project page:   <a target="_blank" rel="noopener" href="https://github.com/TJU-YDragonW/TAMT">https://github.com/TJU-YDragonW/TAMT</a></p>
<p><strong>Summary</strong></p>
<p>该文本介绍了跨域少样本动作识别（CDFSAR）的研究现状。现有方法主要通过联合训练源和目标数据来减少域差距的副作用，但存在计算成本高昂和利用预训练模型潜力不足的问题。为克服这些限制，本文提出了名为Temporal-Aware Model Tuning（TAMT）的基线方法。TAMT采用解耦范式，先在源数据进行预训练，再对目标数据进行微调，避免了为多个目标数据重新训练的需求。同时，TAMT提出了Hierarchical Temporal Tuning Network（HTTN），包括局部时间感知适配器（TAA）和全局时间感知时刻微调（GTMT）。TAA学习少量参数来重新校准冻结的预训练模型的中间特征，适应目标域。GTMT则有助于生成强大的视频表示，提高目标域的匹配性能。实验表明，TAMT相较于现有方法表现更优，取得了最新的CDFSAR结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>跨域少样本动作识别（CDFSAR）是近期研究热点，旨在解决源到目标域转移学习中的域差距问题。</li>
<li>现有CDFSAR方法主要通过联合训练源和目标数据来减少域差距，但存在计算成本高和预训练模型潜力利用不足的问题。</li>
<li>本文提出的Temporal-Aware Model Tuning（TAMT）方法采用解耦范式，先预训练源数据，再微调目标数据，避免重训多个目标数据。</li>
<li>TAMT的Hierarchical Temporal Tuning Network（HTTN）包括局部时间感知适配器（TAA）和全局时间感知时刻微调（GTMT），能更有效地探索预训练模型的潜力并适应目标域。</li>
<li>TAA通过学习少量参数重新校准预训练模型的中间特征，适应目标域。</li>
<li>GTMT有助于生成强大的视频表示，提高目标域的匹配性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.19041">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-51c5fa8c2d9f0ceed66ae53be3121aa2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07ff266fe07017a32f83fab11a923caa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88332bc35417b9051be5cd5265bbc739.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4df25b047ed7a042bb7d592f7ba16c41.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ICPL-Few-shot-In-context-Preference-Learning-via-LLMs"><a href="#ICPL-Few-shot-In-context-Preference-Learning-via-LLMs" class="headerlink" title="ICPL: Few-shot In-context Preference Learning via LLMs"></a>ICPL: Few-shot In-context Preference Learning via LLMs</h2><p><strong>Authors:Chao Yu, Qixin Tan, Hong Lu, Jiaxuan Gao, Xinting Yang, Yu Wang, Yi Wu, Eugene Vinitsky</strong></p>
<p>Preference-based reinforcement learning is an effective way to handle tasks where rewards are hard to specify but can be exceedingly inefficient as preference learning is often tabula rasa. We demonstrate that Large Language Models (LLMs) have native preference-learning capabilities that allow them to achieve sample-efficient preference learning, addressing this challenge. We propose In-Context Preference Learning (ICPL), which uses in-context learning capabilities of LLMs to reduce human query inefficiency. ICPL uses the task description and basic environment code to create sets of reward functions which are iteratively refined by placing human feedback over videos of the resultant policies into the context of an LLM and then requesting better rewards. We first demonstrate ICPL’s effectiveness through a synthetic preference study, providing quantitative evidence that it significantly outperforms baseline preference-based methods with much higher performance and orders of magnitude greater efficiency. We observe that these improvements are not solely coming from LLM grounding in the task but that the quality of the rewards improves over time, indicating preference learning capabilities. Additionally, we perform a series of real human preference-learning trials and observe that ICPL extends beyond synthetic settings and can work effectively with humans-in-the-loop. </p>
<blockquote>
<p>基于偏好的强化学习是处理奖励难以明确指定的任务的一种有效方法，但其效率低下的主要原因是偏好学习往往是空白的。我们证明大型语言模型（LLM）具有天生的偏好学习能力，允许它们实现高效的样本偏好学习，从而应对这一挑战。我们提出了上下文偏好学习（ICPL），它利用LLM的上下文学习能力来减少人类查询的不效率。ICPL使用任务描述和基本环境代码来创建奖励函数集，通过放置人类反馈在结果政策的视频上，将其置于LLM的语境中，然后进行更好的奖励，从而迭代优化这些函数集。我们首先通过一个合成偏好研究来证明ICPL的有效性，定量证据表明它显著优于基于偏好的基准方法，具有更高的性能和更高的效率。我们发现这些改进不仅来自LLM在任务中的基础，而且随着时间的推移奖励的质量有所提高，这显示了其偏好学习能力。此外，我们还进行了一系列真实的人类偏好学习试验，并观察到ICPL超出合成设置，可以有效地与人类合作。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17233v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大语言模型（LLM）具有内在偏好学习能力，可实现样本高效的偏好学习，解决了奖励难以指定的问题。我们提出基于上下文偏好学习（ICPL），利用LLM的上下文学习能力减少人类查询的不效率。ICPL利用任务描述和基本环境代码来创建奖励函数集，通过向LLM提供视频的反馈来改善奖励。在合成偏好研究中证明ICPL的有效性显示其在性能上明显优于基于偏好的基准方法，并具有更高的效率。此外，我们还发现奖励质量随时间提高，表明偏好学习能力的重要性。同时，我们在真实的人类偏好学习试验中发现ICPL不仅适用于合成设置，还能有效地与人类合作。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大语言模型（LLM）具有内在的偏好学习能力。</li>
<li>ICPL利用LLM进行样本高效的偏好学习，解决了奖励难以指定的问题。</li>
<li>ICPL使用任务描述和基本环境代码来创建奖励函数集，并利用人类反馈改善奖励函数。</li>
<li>合成偏好研究显示ICPL性能优于基准方法，并具有更高的效率。</li>
<li>ICPL的奖励质量随时间提高，显示了其偏好学习能力的重要性。</li>
<li>ICPL在真实的人类偏好学习试验中被证明是有效的。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.17233">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bff40cec3e0efe9ee39f83700138bddc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c683840824b6985bf67783226bc97ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bf076dc4f32a10e639d81027a0ab3bb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-05/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-05/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-05/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-f7d5dd5192adb708a977f34013859e18.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-04-05  F-ViTA Foundation Model Guided Visible to Thermal Translation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-05/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2694f8002e4b19099d99115786a21729.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-04-05  CHARMS Cognitive Hierarchical Agent with Reasoning and Motion Styles
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24417.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
