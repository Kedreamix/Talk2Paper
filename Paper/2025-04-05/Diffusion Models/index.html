<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-05  Concept Lancet Image Editing with Compositional Representation   Transplant">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b00cced02d5d259e181a498f1dffd934.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    64 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-05-æ›´æ–°"><a href="#2025-04-05-æ›´æ–°" class="headerlink" title="2025-04-05 æ›´æ–°"></a>2025-04-05 æ›´æ–°</h1><h2 id="Concept-Lancet-Image-Editing-with-Compositional-Representation-Transplant"><a href="#Concept-Lancet-Image-Editing-with-Compositional-Representation-Transplant" class="headerlink" title="Concept Lancet: Image Editing with Compositional Representation   Transplant"></a>Concept Lancet: Image Editing with Compositional Representation   Transplant</h2><p><strong>Authors:Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, Hancheng Min, Chris Callison-Burch, RenÃ© Vidal</strong></p>
<p>Diffusion models are widely used for image editing tasks. Existing editing methods often design a representation manipulation procedure by curating an edit direction in the text embedding or score space. However, such a procedure faces a key challenge: overestimating the edit strength harms visual consistency while underestimating it fails the editing task. Notably, each source image may require a different editing strength, and it is costly to search for an appropriate strength via trial-and-error. To address this challenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play framework for principled representation manipulation in diffusion-based image editing. At inference time, we decompose the source input in the latent (text embedding or diffusion score) space as a sparse linear combination of the representations of the collected visual concepts. This allows us to accurately estimate the presence of concepts in each image, which informs the edit. Based on the editing task (replace&#x2F;add&#x2F;remove), we perform a customized concept transplant process to impose the corresponding editing direction. To sufficiently model the concept space, we curate a conceptual representation dataset, CoLan-150K, which contains diverse descriptions and scenarios of visual terms and phrases for the latent dictionary. Experiments on multiple diffusion-based image editing baselines show that methods equipped with CoLan achieve state-of-the-art performance in editing effectiveness and consistency preservation. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å¹¿æ³›åº”ç”¨äºå›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚ç°æœ‰çš„ç¼–è¾‘æ–¹æ³•é€šå¸¸é€šè¿‡ç­–åˆ’æ–‡æœ¬åµŒå…¥æˆ–åˆ†æ•°ç©ºé—´ä¸­çš„ç¼–è¾‘æ–¹å‘æ¥è®¾è®¡ä¸€ä¸ªè¡¨ç¤ºæ“çºµè¿‡ç¨‹ã€‚ç„¶è€Œï¼Œè¿™ä¸€è¿‡ç¨‹é¢ä¸´ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼šé«˜ä¼°ç¼–è¾‘å¼ºåº¦ä¼šæŸå®³è§†è§‰ä¸€è‡´æ€§ï¼Œè€Œä½ä¼°åˆ™æ— æ³•å®Œæˆç¼–è¾‘ä»»åŠ¡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ¯ä¸ªæºå›¾åƒå¯èƒ½éœ€è¦ä¸åŒçš„ç¼–è¾‘å¼ºåº¦ï¼Œé€šè¿‡è¯•é”™æœç´¢é€‚å½“çš„å¼ºåº¦æˆæœ¬å¾ˆé«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Concept Lancetï¼ˆCoLanï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ‰©æ•£å¼å›¾åƒç¼–è¾‘ä¸­æœ‰åŸåˆ™è¡¨ç¤ºæ“ä½œçš„é›¶æ ·æœ¬å³æ’å³ç”¨æ¡†æ¶ã€‚åœ¨æ¨ç†æ—¶é—´ï¼Œæˆ‘ä»¬å°†æºè¾“å…¥åœ¨æ½œåœ¨ï¼ˆæ–‡æœ¬åµŒå…¥æˆ–æ‰©æ•£åˆ†æ•°ï¼‰ç©ºé—´ä¸­åˆ†è§£ä¸ºæ‰€æ”¶é›†çš„è§†è§‰æ¦‚å¿µè¡¨ç¤ºçš„ç¨€ç–çº¿æ€§ç»„åˆã€‚è¿™å…è®¸æˆ‘ä»¬å‡†ç¡®ä¼°è®¡æ¯ä¸ªå›¾åƒä¸­æ¦‚å¿µçš„å­˜åœ¨ï¼Œä»è€Œä¸ºç¼–è¾‘æä¾›ä¿¡æ¯ã€‚åŸºäºç¼–è¾‘ä»»åŠ¡ï¼ˆæ›¿æ¢&#x2F;æ·»åŠ &#x2F;åˆ é™¤ï¼‰ï¼Œæˆ‘ä»¬æ‰§è¡Œå®šåˆ¶çš„æ¦‚å¿µç§»æ¤è¿‡ç¨‹ä»¥å¼ºåˆ¶æ‰§è¡Œç›¸åº”çš„ç¼–è¾‘æ–¹å‘ã€‚ä¸ºäº†å……åˆ†å»ºæ¨¡æ¦‚å¿µç©ºé—´ï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªæ¦‚å¿µè¡¨ç¤ºæ•°æ®é›†CoLan-150Kï¼Œå…¶ä¸­åŒ…å«è§†è§‰æœ¯è¯­å’ŒçŸ­è¯­çš„å¤šæ ·æè¿°å’Œåœºæ™¯ï¼Œç”¨äºæ½œåœ¨å­—å…¸ã€‚åœ¨å¤šä¸ªåŸºäºæ‰©æ•£çš„å›¾åƒç¼–è¾‘åŸºå‡†å®éªŒä¸Šè¡¨æ˜ï¼Œé…å¤‡CoLançš„æ–¹æ³•åœ¨ç¼–è¾‘æ•ˆæœå’Œä¸€è‡´æ€§ä¿æŒæ–¹é¢è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02828v1">PDF</a> Accepted in CVPR 2025. Project page at   <a target="_blank" rel="noopener" href="https://peterljq.github.io/project/colan">https://peterljq.github.io/project/colan</a></p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹å¹¿æ³›åº”ç”¨äºå›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚ç°æœ‰ç¼–è¾‘æ–¹æ³•é€šè¿‡åœ¨æ–‡æœ¬åµŒå…¥æˆ–åˆ†æ•°ç©ºé—´ç­–åˆ’ç¼–è¾‘æ–¹å‘æ¥è®¾è®¡è¡¨ç¤ºæ“çºµç¨‹åºï¼Œä½†é¢ä¸´ä¼°ç®—ç¼–è¾‘å¼ºåº¦æŒ‘æˆ˜ï¼šè¿‡åº¦ä¼°è®¡ä¼šæŸå®³è§†è§‰ä¸€è‡´æ€§ï¼Œè€Œä½ä¼°åˆ™æ— æ³•å®Œæˆç¼–è¾‘ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºConcept Lancetï¼ˆCoLanï¼‰æ¡†æ¶ï¼Œç”¨äºæ‰©æ•£å¼å›¾åƒç¼–è¾‘ä¸­çš„åŸåˆ™æ€§è¡¨ç¤ºæ“çºµã€‚åœ¨æ¨ç†æ—¶ï¼Œæˆ‘ä»¬å°†æºè¾“å…¥åœ¨æ½œåœ¨ï¼ˆæ–‡æœ¬åµŒå…¥æˆ–æ‰©æ•£åˆ†æ•°ï¼‰ç©ºé—´ä¸­åˆ†è§£ä¸ºæ”¶é›†çš„è§†è§‰æ¦‚å¿µçš„è¡¨ç¤ºçš„ç¨€ç–çº¿æ€§ç»„åˆï¼Œå‡†ç¡®ä¼°è®¡å›¾åƒä¸­æ¦‚å¿µçš„å­˜åœ¨ï¼Œä¸ºç¼–è¾‘æä¾›ä¿¡æ¯ã€‚åŸºäºç¼–è¾‘ä»»åŠ¡ï¼ˆæ›¿æ¢&#x2F;æ·»åŠ &#x2F;åˆ é™¤ï¼‰ï¼Œæˆ‘ä»¬æ‰§è¡Œå®šåˆ¶çš„æ¦‚å¿µç§»æ¤è¿‡ç¨‹ä»¥æ–½åŠ ç›¸åº”çš„ç¼–è¾‘æ–¹å‘ã€‚é€šè¿‡æ„å»ºæ¦‚å¿µè¡¨ç¤ºæ•°æ®é›†CoLan-150Kï¼ŒåŒ…å«è§†è§‰æœ¯è¯­å’ŒçŸ­è¯­çš„å¤šæ ·æè¿°å’Œåœºæ™¯ä½œä¸ºæ½œåœ¨å­—å…¸ï¼Œå®éªŒè¡¨æ˜é…å¤‡CoLançš„æ–¹æ³•åœ¨ç¼–è¾‘æ•ˆæœå’Œä¸€è‡´æ€§ä¿æŒæ–¹é¢è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å¸¸ç”¨äºå›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰ç¼–è¾‘æ–¹æ³•è®¾è®¡è¡¨ç¤ºæ“çºµç¨‹åºæ—¶é¢ä¸´ä¼°ç®—ç¼–è¾‘å¼ºåº¦çš„æŒ‘æˆ˜ã€‚</li>
<li>Concept Lancetï¼ˆCoLanï¼‰æ¡†æ¶ç”¨äºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œé€šè¿‡åˆ†è§£æºè¾“å…¥åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„è¡¨ç¤ºæ¥å‡†ç¡®ä¼°è®¡å›¾åƒä¸­çš„æ¦‚å¿µã€‚</li>
<li>CoLanåŸºäºç¼–è¾‘ä»»åŠ¡æ‰§è¡Œå®šåˆ¶çš„æ¦‚å¿µç§»æ¤è¿‡ç¨‹ã€‚</li>
<li>CoLané€šè¿‡æ„å»ºCoLan-150Kæ•°æ®é›†æ¥å……åˆ†å»ºæ¨¡æ¦‚å¿µç©ºé—´ã€‚</li>
<li>é…å¤‡CoLançš„å®éªŒæ–¹æ³•åœ¨å¤šä¸ªæ‰©æ•£å¼å›¾åƒç¼–è¾‘åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02828">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7caf3c5518e1646ef2c4ebfc563c26c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-744340da7c6f3cdd26998a6a6fd4b242.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff5f6835fc6753e643a9ba7ca680df8b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b2c353d9fcc0026a5780aaf1bc23f3f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d8fa2e19ca4ac30f1348ea0727f0506.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f8bb99afea72e61787dad90e8b8af977.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="F-ViTA-Foundation-Model-Guided-Visible-to-Thermal-Translation"><a href="#F-ViTA-Foundation-Model-Guided-Visible-to-Thermal-Translation" class="headerlink" title="F-ViTA: Foundation Model Guided Visible to Thermal Translation"></a>F-ViTA: Foundation Model Guided Visible to Thermal Translation</h2><p><strong>Authors:Jay N. Paranjape, Celso de Melo, Vishal M. Patel</strong></p>
<p>Thermal imaging is crucial for scene understanding, particularly in low-light and nighttime conditions. However, collecting large thermal datasets is costly and labor-intensive due to the specialized equipment required for infrared image capture. To address this challenge, researchers have explored visible-to-thermal image translation. Most existing methods rely on Generative Adversarial Networks (GANs) or Diffusion Models (DMs), treating the task as a style transfer problem. As a result, these approaches attempt to learn both the modality distribution shift and underlying physical principles from limited training data. In this paper, we propose F-ViTA, a novel approach that leverages the general world knowledge embedded in foundation models to guide the diffusion process for improved translation. Specifically, we condition an InstructPix2Pix Diffusion Model with zero-shot masks and labels from foundation models such as SAM and Grounded DINO. This allows the model to learn meaningful correlations between scene objects and their thermal signatures in infrared imagery. Extensive experiments on five public datasets demonstrate that F-ViTA outperforms state-of-the-art (SOTA) methods. Furthermore, our model generalizes well to out-of-distribution (OOD) scenarios and can generate Long-Wave Infrared (LWIR), Mid-Wave Infrared (MWIR), and Near-Infrared (NIR) translations from the same visible image. Code: <a target="_blank" rel="noopener" href="https://github.com/JayParanjape/F-ViTA/tree/master">https://github.com/JayParanjape/F-ViTA/tree/master</a>. </p>
<blockquote>
<p>çƒ­æˆåƒå¯¹äºåœºæ™¯ç†è§£è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½å…‰å’Œå¤œé—´æ¡ä»¶ä¸‹ã€‚ç„¶è€Œï¼Œç”±äºçº¢å¤–å›¾åƒæ•è·æ‰€éœ€çš„ä¸“ä¸šè®¾å¤‡ï¼Œæ”¶é›†å¤§é‡çš„çƒ­æ•°æ®é›†æˆæœ¬é«˜æ˜‚ä¸”åŠ³åŠ¨å¯†é›†ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œç ”ç©¶äººå‘˜å·²ç»æ¢ç´¢äº†å¯è§å…‰åˆ°çƒ­æˆåƒçš„å›¾åƒç¿»è¯‘æŠ€æœ¯ã€‚ç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•ä¾èµ–äºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰æˆ–æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ï¼Œå°†ä»»åŠ¡è§†ä¸ºé£æ ¼è½¬æ¢é—®é¢˜ã€‚å› æ­¤ï¼Œè¿™äº›æ–¹æ³•è¯•å›¾ä»æœ‰é™çš„è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ æ¨¡æ€åˆ†å¸ƒå˜åŒ–å’Œæ½œåœ¨çš„ç‰©ç†åŸç†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•F-ViTAï¼Œå®ƒåˆ©ç”¨åŸºç¡€æ¨¡å‹ä¸­åµŒå…¥çš„ä¸–ç•Œé€šç”¨çŸ¥è¯†æ¥æŒ‡å¯¼æ‰©æ•£è¿‡ç¨‹ï¼Œä»¥æ”¹è¿›ç¿»è¯‘æ•ˆæœã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨InstructPix2Pixæ‰©æ•£æ¨¡å‹ï¼Œä»¥åŸºç¡€æ¨¡å‹ï¼ˆå¦‚SAMå’ŒGrounded DINOï¼‰çš„é›¶æ ·æœ¬é®ç½©å’Œæ ‡ç­¾ä½œä¸ºæ¡ä»¶ã€‚è¿™å…è®¸æ¨¡å‹å­¦ä¹ åœºæ™¯ç‰©ä½“ä¸å…¶åœ¨çº¢å¤–å›¾åƒä¸­çš„çƒ­ç‰¹å¾ä¹‹é—´çš„æœ‰æ„ä¹‰å…³è”ã€‚åœ¨äº”ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒF-ViTAä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿå¾ˆå¥½åœ°æ³›åŒ–åˆ°åŸŸå¤–åœºæ™¯ï¼Œå¹¶ä¸”å¯ä»¥ä»åŒä¸€å¯è§å›¾åƒç”Ÿæˆé•¿æ³¢çº¢å¤–ï¼ˆLWIRï¼‰ã€ä¸­æ³¢çº¢å¤–ï¼ˆMWIRï¼‰å’Œè¿‘çº¢å¤–ï¼ˆNIRï¼‰ç¿»è¯‘ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/JayParanjape/F-ViTA/tree/master%E3%80%82">https://github.com/JayParanjape/F-ViTA/tree/masterã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02801v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨åŸºç¡€æ¨¡å‹ä¸­çš„é€šç”¨ä¸–ç•ŒçŸ¥è¯†æ¥æŒ‡å¯¼æ‰©æ•£è¿‡ç¨‹çš„æ–°å‹å¯è§å…‰åˆ°çƒ­æˆåƒç¿»è¯‘æ–¹æ³•â€”â€”F-ViTAã€‚é€šè¿‡ç»“åˆInstructPix2Pixæ‰©æ•£æ¨¡å‹å’ŒåŸºç¡€æ¨¡å‹ï¼ˆå¦‚SAMå’ŒGrounded DINOï¼‰çš„é›¶æ ·æœ¬æ©ç å’Œæ ‡ç­¾ï¼ŒF-ViTAèƒ½åœ¨åœºæ™¯ç‰©ä½“ä¸å®ƒä»¬åœ¨çº¢å¤–æˆåƒä¸­çš„çƒ­ç‰¹å¾ä¹‹é—´å»ºç«‹æœ‰æ„ä¹‰çš„å…³è”ã€‚åœ¨äº”ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒF-ViTAä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½ä»åŒä¸€å¯è§å›¾åƒç”Ÿæˆé•¿æ³¢çº¢å¤–ã€ä¸­æ³¢çº¢å¤–å’Œè¿‘çº¢å¤–ç¿»è¯‘ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¯è§å…‰åˆ°çƒ­æˆåƒç¿»è¯‘æ˜¯è§£å†³çƒ­æˆåƒæ•°æ®æ”¶é›†æˆæœ¬é«˜æ˜‚å’ŒåŠ³åŠ¨å¯†é›†çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>F-ViTAåˆ©ç”¨åŸºç¡€æ¨¡å‹ä¸­çš„é€šç”¨ä¸–ç•ŒçŸ¥è¯†æ¥æŒ‡å¯¼æ‰©æ•£è¿‡ç¨‹ï¼Œå®ç°æ›´å¥½çš„ç¿»è¯‘æ•ˆæœã€‚</li>
<li>F-ViTAç»“åˆäº†InstructPix2Pixæ‰©æ•£æ¨¡å‹å’ŒåŸºç¡€æ¨¡å‹çš„é›¶æ ·æœ¬æ©ç å’Œæ ‡ç­¾ã€‚</li>
<li>F-ViTAèƒ½åœ¨åœºæ™¯ç‰©ä½“ä¸çƒ­ç‰¹å¾ä¹‹é—´å»ºç«‹æœ‰æ„ä¹‰çš„å…³è”ã€‚</li>
<li>F-ViTAåœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>F-ViTAå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½ç”Ÿæˆä¸åŒçº¢å¤–æ³¢æ®µçš„ç¿»è¯‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02801">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d58058993689ae2148ae227f856269b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81de8b10a9991239b5966a1002c5eb0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b00cced02d5d259e181a498f1dffd934.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7d5dd5192adb708a977f34013859e18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e92e2b8fd1ed6a5281e477cc6a4bcbb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e83781afdfdffe32dda10dac22e4bc25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fa901a8f38e24f08ce9eeccb045c51a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09bb4ad23917336ee5eb9f023ae32fad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c303caa2dea22db222113799c968f99.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Scene-Splatter-Momentum-3D-Scene-Generation-from-Single-Image-with-Video-Diffusion-Model"><a href="#Scene-Splatter-Momentum-3D-Scene-Generation-from-Single-Image-with-Video-Diffusion-Model" class="headerlink" title="Scene Splatter: Momentum 3D Scene Generation from Single Image with   Video Diffusion Model"></a>Scene Splatter: Momentum 3D Scene Generation from Single Image with   Video Diffusion Model</h2><p><strong>Authors:Shengjun Zhang, Jinzhao Li, Xin Fei, Hao Liu, Yueqi Duan</strong></p>
<p>In this paper, we propose Scene Splatter, a momentum-based paradigm for video diffusion to generate generic scenes from single image. Existing methods, which employ video generation models to synthesize novel views, suffer from limited video length and scene inconsistency, leading to artifacts and distortions during further reconstruction. To address this issue, we construct noisy samples from original features as momentum to enhance video details and maintain scene consistency. However, for latent features with the perception field that spans both known and unknown regions, such latent-level momentum restricts the generative ability of video diffusion in unknown regions. Therefore, we further introduce the aforementioned consistent video as a pixel-level momentum to a directly generated video without momentum for better recovery of unseen regions. Our cascaded momentum enables video diffusion models to generate both high-fidelity and consistent novel views. We further finetune the global Gaussian representations with enhanced frames and render new frames for momentum update in the next step. In this manner, we can iteratively recover a 3D scene, avoiding the limitation of video length. Extensive experiments demonstrate the generalization capability and superior performance of our method in high-fidelity and consistent scene generation. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†Scene Splatterï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåŠ¨é‡çš„è§†é¢‘æ‰©æ•£èŒƒå¼ï¼Œç”¨äºä»å•å¹…å›¾åƒç”Ÿæˆé€šç”¨åœºæ™¯ã€‚ç°æœ‰æ–¹æ³•é‡‡ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹æ¥åˆæˆæ–°è§†è§’ï¼Œå­˜åœ¨è§†é¢‘é•¿åº¦æœ‰é™å’Œåœºæ™¯ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå¯¼è‡´åœ¨è¿›ä¸€æ­¥é‡å»ºæ—¶å‡ºç°ä¼ªå½±å’Œå¤±çœŸã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ä»åŸå§‹ç‰¹å¾ä¸­æ„å»ºå™ªå£°æ ·æœ¬ä½œä¸ºåŠ¨é‡ï¼Œä»¥å¢å¼ºè§†é¢‘ç»†èŠ‚å¹¶ä¿æŒåœºæ™¯ä¸€è‡´æ€§ã€‚ç„¶è€Œï¼Œå¯¹äºæ„ŸçŸ¥åœºåŒæ—¶è¦†ç›–å·²çŸ¥å’ŒæœªçŸ¥åŒºåŸŸçš„æ½œåœ¨ç‰¹å¾ï¼Œè¿™ç§æ½œåœ¨å±‚æ¬¡çš„åŠ¨é‡é™åˆ¶äº†è§†é¢‘æ‰©æ•£åœ¨æœªçŸ¥åŒºåŸŸçš„ç”Ÿæˆèƒ½åŠ›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸Šè¿°ä¸€è‡´è§†é¢‘ä½œä¸ºæ— åŠ¨é‡çš„ç›´æ¥ç”Ÿæˆè§†é¢‘çš„åƒç´ çº§åŠ¨é‡ï¼Œä»¥æ›´å¥½åœ°æ¢å¤æœªè§åŒºåŸŸã€‚æˆ‘ä»¬çš„çº§è”åŠ¨é‡ä½¿è§†é¢‘æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸä¸”ä¸€è‡´çš„å…¨æ–°è§†è§’ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä½¿ç”¨å¢å¼ºå¸§å¾®è°ƒå…¨å±€é«˜æ–¯è¡¨ç¤ºï¼Œå¹¶åœ¨ä¸‹ä¸€æ­¥æ¸²æŸ“æ–°å¸§ä»¥è¿›è¡ŒåŠ¨é‡æ›´æ–°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥è¿­ä»£åœ°æ¢å¤3Dåœºæ™¯ï¼Œé¿å…è§†é¢‘é•¿åº¦çš„é™åˆ¶ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é«˜ä¿çœŸå’Œä¸€è‡´åœºæ™¯ç”Ÿæˆæ–¹é¢å…·æœ‰æ³›åŒ–èƒ½åŠ›å’Œä¼˜è¶Šæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02764v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†Scene Splatteræ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåŠ¨é‡çš„è§†é¢‘æ‰©æ•£èŒƒå¼ï¼Œèƒ½å¤Ÿä»å•å¹…å›¾åƒç”Ÿæˆé€šç”¨åœºæ™¯ã€‚ç°æœ‰æ–¹æ³•ä½¿ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹åˆæˆæ–°è§†å›¾æ—¶ï¼Œå­˜åœ¨è§†é¢‘é•¿åº¦æœ‰é™å’Œåœºæ™¯ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå¯¼è‡´è¿›ä¸€æ­¥é‡å»ºæ—¶å‡ºç°ä¼ªå½±å’Œå¤±çœŸã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡é€šè¿‡åŸå§‹ç‰¹å¾æ„å»ºå™ªå£°æ ·æœ¬ä½œä¸ºåŠ¨é‡ï¼Œå¢å¼ºè§†é¢‘ç»†èŠ‚å¹¶ä¿æŒåœºæ™¯ä¸€è‡´æ€§ã€‚ç„¶è€Œï¼Œå¯¹äºåŒæ—¶åŒ…å«å·²çŸ¥å’ŒæœªçŸ¥åŒºåŸŸçš„æ„ŸçŸ¥åœºåŸŸä¸­çš„æ½œåœ¨ç‰¹å¾ï¼Œè¿™ç§æ½œåœ¨å±‚é¢çš„åŠ¨é‡é™åˆ¶äº†è§†é¢‘æ‰©æ•£åœ¨æœªçŸ¥åŒºåŸŸçš„ç”Ÿæˆèƒ½åŠ›ã€‚å› æ­¤ï¼Œæœ¬æ–‡è¿›ä¸€æ­¥å¼•å…¥äº†ä¸Šè¿°ä¸€è‡´è§†é¢‘ä½œä¸ºæ— åŠ¨é‡çš„ç›´æ¥ç”Ÿæˆè§†é¢‘çš„åƒç´ çº§åŠ¨é‡ï¼Œä»¥æ›´å¥½åœ°æ¢å¤æœªè§åŒºåŸŸã€‚çº§è”åŠ¨é‡ä½¿è§†é¢‘æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸä¸”ä¸€è‡´çš„å…¨æ–°è§†å›¾ã€‚é€šè¿‡å¾®è°ƒå…¨å±€é«˜æ–¯è¡¨ç¤ºå’Œå¢å¼ºå¸§è¿›è¡Œæ¸²æŸ“ï¼Œä¸ºä¸‹ä¸€æ­¥æ›´æ–°åŠ¨é‡æä¾›æ–°å¸§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥è¿­ä»£æ¢å¤3Dåœºæ™¯ï¼Œé¿å…è§†é¢‘é•¿åº¦çš„é™åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Scene Splatteræ˜¯ä¸€ç§åŸºäºåŠ¨é‡çš„è§†é¢‘æ‰©æ•£æ–¹æ³•ï¼Œèƒ½å¤Ÿä»å•å¹…å›¾åƒç”Ÿæˆé€šç”¨åœºæ™¯ã€‚</li>
<li>ç°æœ‰è§†é¢‘ç”Ÿæˆæ–¹æ³•å­˜åœ¨è§†é¢‘é•¿åº¦æœ‰é™å’Œåœºæ™¯ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡æ„å»ºå™ªå£°æ ·æœ¬ä½œä¸ºåŠ¨é‡ï¼ŒScene Splatterèƒ½å¢å¼ºè§†é¢‘ç»†èŠ‚å¹¶ä¿æŒåœºæ™¯ä¸€è‡´æ€§ã€‚</li>
<li>æ½œåœ¨å±‚é¢çš„åŠ¨é‡åœ¨å·²çŸ¥å’ŒæœªçŸ¥åŒºåŸŸçš„æ„ŸçŸ¥åœºåŸŸä¸­é™åˆ¶äº†è§†é¢‘æ‰©æ•£çš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>Scene Splatterå¼•å…¥äº†åƒç´ çº§åŠ¨é‡æ¥æ¢å¤æœªè§åŒºåŸŸï¼Œå¹¶ç”Ÿæˆé«˜ä¿çœŸä¸”ä¸€è‡´çš„å…¨æ–°è§†å›¾ã€‚</li>
<li>é€šè¿‡å¾®è°ƒå…¨å±€é«˜æ–¯è¡¨ç¤ºå’Œå¢å¼ºå¸§è¿›è¡Œæ¸²æŸ“ï¼Œå¯ä»¥è¿­ä»£æ¢å¤3Dåœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02764">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b94e211c633f7628d84afeccef8b7ece.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-867943fcb69504c3b63f87115e08cd8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de8140c6f9fc0fd225df494923195ecb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2999f4e3eb11418481eea9860ad29b45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-987326529c3d782296ea053341f5ae40.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MD-ProjTex-Texturing-3D-Shapes-with-Multi-Diffusion-Projection"><a href="#MD-ProjTex-Texturing-3D-Shapes-with-Multi-Diffusion-Projection" class="headerlink" title="MD-ProjTex: Texturing 3D Shapes with Multi-Diffusion Projection"></a>MD-ProjTex: Texturing 3D Shapes with Multi-Diffusion Projection</h2><p><strong>Authors:Ahmet Burak Yildirim, Mustafa Utku Aydogdu, Duygu Ceylan, Aysegul Dundar</strong></p>
<p>We introduce MD-ProjTex, a method for fast and consistent text-guided texture generation for 3D shapes using pretrained text-to-image diffusion models. At the core of our approach is a multi-view consistency mechanism in UV space, which ensures coherent textures across different viewpoints. Specifically, MD-ProjTex fuses noise predictions from multiple views at each diffusion step and jointly updates the per-view denoising directions to maintain 3D consistency. In contrast to existing state-of-the-art methods that rely on optimization or sequential view synthesis, MD-ProjTex is computationally more efficient and achieves better quantitative and qualitative results. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†MD-ProjTexæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œå¿«é€Ÿä¸”ä¸€è‡´çš„æ–‡æœ¬å¼•å¯¼çº¹ç†ç”Ÿæˆçš„3Då½¢çŠ¶å¤„ç†æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºUVç©ºé—´ä¸­çš„å¤šè§†è§’ä¸€è‡´æ€§æœºåˆ¶ï¼Œå®ƒèƒ½ç¡®ä¿ä¸åŒè§†è§’ä¹‹é—´çš„çº¹ç†ä¸€è‡´æ€§ã€‚å…·ä½“æ¥è¯´ï¼ŒMD-ProjTexåœ¨æ¯ä¸ªæ‰©æ•£æ­¥éª¤èåˆå¤šä¸ªè§†è§’çš„å™ªå£°é¢„æµ‹ï¼Œå¹¶è”åˆæ›´æ–°æ¯ä¸ªè§†è§’çš„å»å™ªæ–¹å‘ï¼Œä»¥ä¿æŒ3Dä¸€è‡´æ€§ã€‚ä¸ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºä¼˜åŒ–æˆ–é¡ºåºè§†å›¾åˆæˆï¼ŒMD-ProjTexè®¡ç®—æ•ˆç‡æ›´é«˜ï¼Œå¹¶ä¸”åœ¨æ•°é‡å’Œè´¨é‡ä¸Šéƒ½èƒ½å®ç°æ›´å¥½çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02762v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æˆ‘ä»¬æ¨å‡ºäº†MD-ProjTexæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†å¿«é€Ÿä¸”ä¸€è‡´çš„æ–‡æœ¬å¼•å¯¼çº¹ç†ç”ŸæˆæŠ€æœ¯ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºUVç©ºé—´çš„å¤šè§†è§’ä¸€è‡´æ€§æœºåˆ¶ï¼Œç¡®ä¿ä¸åŒè§†è§’çš„çº¹ç†è¿è´¯æ€§ã€‚MD-ProjTexèåˆäº†å¤šä¸ªè§†è§’çš„å™ªå£°é¢„æµ‹ï¼Œå¹¶åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­è”åˆæ›´æ–°æ¯ä¸ªè§†è§’çš„å»å™ªæ–¹å‘ï¼Œä»¥ä¿æŒä¸‰ç»´ä¸€è‡´æ€§ã€‚ç›¸è¾ƒäºä¾èµ–ä¼˜åŒ–æˆ–é¡ºåºè§†è§’åˆæˆçš„ç°æœ‰æ–¹æ³•ï¼ŒMD-ProjTexè®¡ç®—æ•ˆç‡æ›´é«˜ï¼Œä¸”å–å¾—äº†æ›´å¥½çš„å®šé‡å’Œå®šæ€§ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>MD-ProjTexæ˜¯ä¸€ç§åŸºäºé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿä¸”ä¸€è‡´çš„æ–‡æœ¬å¼•å¯¼çº¹ç†ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨å¤šè§†è§’ä¸€è‡´æ€§æœºåˆ¶ï¼Œç¡®ä¿ä¸åŒè§†è§’çš„çº¹ç†è¿è´¯æ€§ã€‚</li>
<li>MD-ProjTexèåˆäº†å¤šä¸ªè§†è§’çš„å™ªå£°é¢„æµ‹ï¼Œå¹¶åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­æ›´æ–°å»å™ªæ–¹å‘ã€‚</li>
<li>MD-ProjTexç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼Œè®¡ç®—æ•ˆç‡æ›´é«˜ã€‚</li>
<li>MD-ProjTexåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­éƒ½å–å¾—äº†æ›´å¥½çš„ç»“æœã€‚</li>
<li>è¯¥æ–¹æ³•çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºåˆ©ç”¨æ–‡æœ¬æŒ‡å¯¼ç”Ÿæˆä¸æ–‡æœ¬å†…å®¹ç›¸åŒ¹é…çš„3Då½¢çŠ¶çº¹ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02762">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58c29ece383b0ff9831f39b9141f81c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8f8168a4de9b2efa62b6151302fa382.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ba2ea73c5f780ff6e839d2ab97b22d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8052627d531a5ca90d21c5111ec29c16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70dd7c4ccc5bd21e768a1605ea1c4e23.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Bridging-the-Gap-between-Gaussian-Diffusion-Models-and-Universal-Quantization-for-Image-Compression"><a href="#Bridging-the-Gap-between-Gaussian-Diffusion-Models-and-Universal-Quantization-for-Image-Compression" class="headerlink" title="Bridging the Gap between Gaussian Diffusion Models and Universal   Quantization for Image Compression"></a>Bridging the Gap between Gaussian Diffusion Models and Universal   Quantization for Image Compression</h2><p><strong>Authors:Lucas Relic, Roberto Azevedo, Yang Zhang, Markus Gross, Christopher Schroers</strong></p>
<p>Generative neural image compression supports data representation at extremely low bitrate, synthesizing details at the client and consistently producing highly realistic images. By leveraging the similarities between quantization error and additive noise, diffusion-based generative image compression codecs can be built using a latent diffusion model to â€œdenoiseâ€ the artifacts introduced by quantization. However, we identify three critical gaps in previous approaches following this paradigm (namely, the noise level, noise type, and discretization gaps) that result in the quantized data falling out of the data distribution known by the diffusion model. In this work, we propose a novel quantization-based forward diffusion process with theoretical foundations that tackles all three aforementioned gaps. We achieve this through universal quantization with a carefully tailored quantization schedule and a diffusion model trained with uniform noise. Compared to previous work, our proposal produces consistently realistic and detailed reconstructions, even at very low bitrates. In such a regime, we achieve the best rate-distortion-realism performance, outperforming previous related works. </p>
<blockquote>
<p>ç”Ÿæˆå¼ç¥ç»ç½‘ç»œå‹ç¼©æ”¯æŒä»¥æä½çš„æ¯”ç‰¹ç‡è¿›è¡Œæ•°æ®è¡¨ç¤ºï¼Œå¯åœ¨å®¢æˆ·ç«¯åˆæˆç»†èŠ‚ï¼Œå¹¶å§‹ç»ˆç”Ÿæˆé«˜åº¦é€¼çœŸçš„å›¾åƒã€‚é€šè¿‡åˆ©ç”¨é‡åŒ–è¯¯å·®å’Œæ·»åŠ æ€§å™ªå£°ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¯ä»¥ä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹æ„å»ºåŸºäºæ‰©æ•£çš„ç”Ÿæˆå¼å›¾åƒå‹ç¼©ç¼–è§£ç å™¨ï¼Œä»¥â€œå»å™ªâ€é‡åŒ–å¼•èµ·çš„ä¼ªå½±ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°äº†éµå¾ªæ­¤æ¨¡å¼ï¼ˆå³å™ªå£°æ°´å¹³ã€å™ªå£°ç±»å‹å’Œç¦»æ•£åŒ–å·®è·ï¼‰çš„å…ˆå‰æ–¹æ³•ä¸­çš„ä¸‰ä¸ªå…³é”®å·®è·ï¼Œå¯¼è‡´é‡åŒ–æ•°æ®è„±ç¦»æ‰©æ•£æ¨¡å‹æ‰€çŸ¥é“çš„æ•°æ®åˆ†å¸ƒã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé‡åŒ–ç†è®ºåŸºç¡€çš„å‘å‰æ‰©æ•£è¿‡ç¨‹æ–°æ–¹æ¡ˆï¼Œè§£å†³äº†ä¸Šè¿°æ‰€æœ‰ä¸‰ä¸ªå·®è·ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨é€šç”¨é‡åŒ–ä»¥åŠç²¾å¿ƒå®šåˆ¶çš„é‡åŒ–æ—¶é—´è¡¨ï¼Œä»¥åŠä½¿ç”¨å‡åŒ€å™ªå£°è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ä¸ä¹‹å‰çš„å·¥ä½œç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ¡ˆå³ä½¿åœ¨éå¸¸ä½çš„æ¯”ç‰¹ç‡ä¸‹ä¹Ÿèƒ½å§‹ç»ˆäº§ç”Ÿé€¼çœŸçš„ç»†èŠ‚ä¸°å¯Œçš„é‡å»ºã€‚åœ¨è¿™ç§æ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬å®ç°äº†æœ€ä½³çš„é€Ÿç‡å¤±çœŸé€¼çœŸåº¦æ€§èƒ½ï¼Œè¶…è¶Šäº†ä»¥å‰çš„ç›¸å…³ä½œå“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02579v1">PDF</a> To appear at CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå¼å›¾åƒå‹ç¼©æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯èƒ½å¤Ÿåœ¨æä½ç ç‡ä¸‹æ”¯æŒæ•°æ®è¡¨ç¤ºï¼Œåˆæˆç»†èŠ‚ï¼Œå¹¶å§‹ç»ˆäº§ç”Ÿé«˜åº¦é€¼çœŸçš„å›¾åƒã€‚æ–‡ç« æŒ‡å‡ºå…ˆå‰çš„æ–¹æ³•åœ¨å™ªå£°æ°´å¹³ã€å™ªå£°ç±»å‹å’Œç¦»æ•£åŒ–æ–¹é¢å­˜åœ¨å·®è·ï¼Œå¯¼è‡´é‡åŒ–æ•°æ®è„±ç¦»æ‰©æ•£æ¨¡å‹æ‰€çŸ¥çš„æ•°æ®åˆ†å¸ƒã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé‡åŒ–çš„å‰å‘æ‰©æ•£è¿‡ç¨‹ï¼Œé€šè¿‡é€šç”¨é‡åŒ–å’Œç²¾å¿ƒå®šåˆ¶çš„é‡åŒ–æ—¶é—´è¡¨ä»¥åŠä½¿ç”¨å‡åŒ€å™ªå£°è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚ä¸ä»¥å‰çš„å·¥ä½œç›¸æ¯”ï¼Œè¯¥æ–¹æ¡ˆå³ä½¿åœ¨ç ç‡æä½çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å§‹ç»ˆäº§ç”Ÿé€¼çœŸä¸”è¯¦ç»†çš„é‡å»ºï¼Œå®ç°äº†æœ€ä½³çš„ç‡å¤±çœŸç°å®æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼ç¥ç»å›¾åƒå‹ç¼©åˆ©ç”¨æ‰©æ•£æ¨¡å‹åœ¨æä½ç ç‡ä¸‹å®ç°æ•°æ®è¡¨ç¤ºï¼Œèƒ½åˆæˆç»†èŠ‚å¹¶ç”Ÿæˆé«˜åº¦é€¼çœŸçš„å›¾åƒã€‚</li>
<li>æŒ‡å‡ºå…ˆå‰æ–¹æ³•åœ¨å™ªå£°æ°´å¹³ã€ç±»å‹å’Œç¦»æ•£åŒ–æ–¹é¢çš„å·®è·ï¼Œå¯¼è‡´é‡åŒ–æ•°æ®ä¸æ‰©æ•£æ¨¡å‹æ‰€çŸ¥çš„æ•°æ®åˆ†å¸ƒä¸åŒ¹é…ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºé‡åŒ–çš„å‰å‘æ‰©æ•£è¿‡ç¨‹ï¼Œå…·æœ‰ç†è®ºæ ¹åŸºï¼Œè§£å†³äº†ä¸Šè¿°å·®è·ã€‚</li>
<li>ä½¿ç”¨é€šç”¨é‡åŒ–å’Œç²¾å¿ƒå®šåˆ¶çš„é‡åŒ–æ—¶é—´è¡¨ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹é‡‡ç”¨å‡åŒ€å™ªå£°è®­ç»ƒã€‚</li>
<li>è¯¥æ–¹æ¡ˆåœ¨æä½ç ç‡ä¸‹ä»èƒ½äº§ç”Ÿé€¼çœŸä¸”è¯¦ç»†çš„é‡å»ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02579">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-463d19d90e361175b448fbeeb4c0d691.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edb285be6d943f67e625ef0d2d08902d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a5efc155736f3d2a3965ee65d0eb7a7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="OmniCam-Unified-Multimodal-Video-Generation-via-Camera-Control"><a href="#OmniCam-Unified-Multimodal-Video-Generation-via-Camera-Control" class="headerlink" title="OmniCam: Unified Multimodal Video Generation via Camera Control"></a>OmniCam: Unified Multimodal Video Generation via Camera Control</h2><p><strong>Authors:Xiaoda Yang, Jiayang Xu, Kaixuan Luan, Xinyu Zhan, Hongshun Qiu, Shijun Shi, Hao Li, Shuai Yang, Li Zhang, Checheng Yu, Cewu Lu, Lixin Yang</strong></p>
<p>Camera control, which achieves diverse visual effects by changing camera position and pose, has attracted widespread attention. However, existing methods face challenges such as complex interaction and limited control capabilities. To address these issues, we present OmniCam, a unified multimodal camera control framework. Leveraging large language models and video diffusion models, OmniCam generates spatio-temporally consistent videos. It supports various combinations of input modalities: the user can provide text or video with expected trajectory as camera path guidance, and image or video as content reference, enabling precise control over camera motion. To facilitate the training of OmniCam, we introduce the OmniTr dataset, which contains a large collection of high-quality long-sequence trajectories, videos, and corresponding descriptions. Experimental results demonstrate that our model achieves state-of-the-art performance in high-quality camera-controlled video generation across various metrics. </p>
<blockquote>
<p>æ‘„åƒæœºæ§åˆ¶é€šè¿‡æ”¹å˜æ‘„åƒæœºçš„ä½ç½®å’Œå§¿æ€æ¥å®ç°å¤šç§è§†è§‰æ•ˆæœï¼Œå·²ç»å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´äº¤äº’å¤æ‚å’Œæ§åˆ¶èƒ½åŠ›æœ‰é™çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†OmniCamï¼Œä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡å¼æ‘„åƒæœºæ§åˆ¶æ¡†æ¶ã€‚OmniCamåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆæ—¶ç©ºä¸€è‡´çš„è§†é¢‘ã€‚å®ƒæ”¯æŒå„ç§è¾“å…¥æ¨¡å¼çš„ç»„åˆï¼šç”¨æˆ·å¯ä»¥æä¾›æ–‡æœ¬æˆ–è§†é¢‘ä½œä¸ºé¢„æœŸçš„è½¨è¿¹ä½œä¸ºæ‘„åƒæœºè·¯å¾„æŒ‡å¯¼ï¼Œä»¥åŠå›¾åƒæˆ–è§†é¢‘ä½œä¸ºå†…å®¹å‚è€ƒï¼Œå®ç°å¯¹æ‘„åƒæœºè¿åŠ¨çš„ç²¾ç¡®æ§åˆ¶ã€‚ä¸ºäº†ä¿ƒè¿›OmniCamçš„è®­ç»ƒï¼Œæˆ‘ä»¬å¼•å…¥äº†OmniTræ•°æ®é›†ï¼Œå®ƒåŒ…å«å¤§é‡é«˜è´¨é‡çš„é•¿åºåˆ—è½¨è¿¹ã€è§†é¢‘å’Œç›¸åº”æè¿°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å„ç§æŒ‡æ ‡ä¸Šå®ç°äº†é«˜è´¨é‡çš„æ‘„åƒæœºæ§åˆ¶è§†é¢‘ç”Ÿæˆçš„å‰æ²¿æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02312v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºOmniCamçš„å¤šæ¨¡æ€ç›¸æœºæ§åˆ¶æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡æ”¹å˜ç›¸æœºä½ç½®å’Œå§¿æ€å®ç°å¤šæ ·åŒ–çš„è§†è§‰æ•ˆæœã€‚OmniCamæ”¯æŒå„ç§è¾“å…¥æ¨¡æ€çš„ç»„åˆï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡æ–‡æœ¬æˆ–è§†é¢‘æä¾›é¢„æœŸçš„è½¨è¿¹ä½œä¸ºç›¸æœºè·¯å¾„æŒ‡å¯¼ï¼Œå›¾åƒæˆ–è§†é¢‘ä½œä¸ºå†…å®¹å‚è€ƒï¼Œå®ç°å¯¹ç›¸æœºè¿åŠ¨çš„ç²¾ç¡®æ§åˆ¶ã€‚ä¸ºè®­ç»ƒOmniCamï¼Œå¼•å…¥äº†OmniTræ•°æ®é›†ï¼ŒåŒ…å«å¤§é‡é«˜è´¨é‡çš„é•¿åºåˆ—è½¨è¿¹ã€è§†é¢‘å’Œç›¸åº”æè¿°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨é«˜è´¨é‡ç›¸æœºæ§åˆ¶è§†é¢‘ç”Ÿæˆæ–¹é¢å®ç°äº†å…ˆè¿›æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OmniCamæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ç›¸æœºæ§åˆ¶æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>OmniCamå¯ä»¥é€šè¿‡æ”¹å˜ç›¸æœºä½ç½®å’Œå§¿æ€å®ç°å¤šæ ·åŒ–çš„è§†è§‰æ•ˆæœã€‚</li>
<li>OmniCamæ”¯æŒå¤šç§è¾“å…¥æ¨¡æ€ç»„åˆï¼ŒåŒ…æ‹¬æ–‡æœ¬æˆ–è§†é¢‘è½¨è¿¹ä½œä¸ºç›¸æœºè·¯å¾„æŒ‡å¯¼ï¼Œå›¾åƒæˆ–è§†é¢‘ä½œä¸ºå†…å®¹å‚è€ƒã€‚</li>
<li>OmniCamçš„å¼•å…¥è§£å†³äº†ç°æœ‰ç›¸æœºæ§åˆ¶æ–¹æ³•é¢ä¸´çš„å¤æ‚äº¤äº’å’Œæœ‰é™æ§åˆ¶èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¸ºè®­ç»ƒOmniCamï¼Œå¼•å…¥äº†OmniTræ•°æ®é›†ï¼ŒåŒ…å«å¤§é‡é«˜è´¨é‡çš„é•¿åºåˆ—è½¨è¿¹ã€è§†é¢‘å’Œç›¸åº”æè¿°ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniCamåœ¨é«˜è´¨é‡ç›¸æœºæ§åˆ¶è§†é¢‘ç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†å…ˆè¿›æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02312">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fa125b58d77a025089a4bd4fcf073776.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3af8f2576760ab57e20f0d52d3d0811f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fc108b9094582084d596c39d04da770.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd69b8550d20604321ead714191ae032.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Random-Conditioning-with-Distillation-for-Data-Efficient-Diffusion-Model-Compression"><a href="#Random-Conditioning-with-Distillation-for-Data-Efficient-Diffusion-Model-Compression" class="headerlink" title="Random Conditioning with Distillation for Data-Efficient Diffusion Model   Compression"></a>Random Conditioning with Distillation for Data-Efficient Diffusion Model   Compression</h2><p><strong>Authors:Dohyun Kim, Sehwan Park, Geonhee Han, Seung Wook Kim, Paul Hongsuck Seo</strong></p>
<p>Diffusion models generate high-quality images through progressive denoising but are computationally intensive due to large model sizes and repeated sampling. Knowledge distillation, which transfers knowledge from a complex teacher to a simpler student model, has been widely studied in recognition tasks, particularly for transferring concepts unseen during student training. However, its application to diffusion models remains underexplored, especially in enabling student models to generate concepts not covered by the training images. In this work, we propose Random Conditioning, a novel approach that pairs noised images with randomly selected text conditions to enable efficient, image-free knowledge distillation. By leveraging this technique, we show that the student can generate concepts unseen in the training images. When applied to conditional diffusion model distillation, our method allows the student to explore the condition space without generating condition-specific images, resulting in notable improvements in both generation quality and efficiency. This promotes resource-efficient deployment of generative diffusion models, broadening their accessibility for both research and real-world applications. Code, models, and datasets are available at <a target="_blank" rel="noopener" href="https://dohyun-as.github.io/Random-Conditioning">https://dohyun-as.github.io/Random-Conditioning</a> . </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹é€šè¿‡é€æ­¥å»å™ªç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†ç”±äºæ¨¡å‹ä½“ç§¯åºå¤§å’Œé‡å¤é‡‡æ ·ï¼Œè®¡ç®—é‡è¾ƒå¤§ã€‚çŸ¥è¯†è’¸é¦æ˜¯ä¸€ç§å°†çŸ¥è¯†ä»å¤æ‚çš„æ•™å¸ˆæ¨¡å‹è½¬ç§»åˆ°ç®€å•çš„å­¦æ¨¡å‹çš„æŠ€æœ¯ï¼Œåœ¨è¯†åˆ«ä»»åŠ¡ä¸­å·²è¢«å¹¿æ³›åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å­¦ç”Ÿè®­ç»ƒæœŸé—´æœªè§çš„æ¦‚å¿µçš„è½¬ç§»æ–¹é¢ã€‚ç„¶è€Œï¼Œå…¶åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿å­¦ç”Ÿæ¨¡å‹ç”Ÿæˆæœªæ¶µç›–åœ¨è®­ç»ƒå›¾åƒä¸­çš„æ¦‚å¿µæ–¹é¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†éšæœºæ¡ä»¶åŒ–æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§å°†å™ªå£°å›¾åƒä¸éšæœºé€‰æ‹©çš„æ–‡æœ¬æ¡ä»¶é…å¯¹çš„æ–°æ–¹æ³•ï¼Œä»¥å®ç°é«˜æ•ˆçš„æ— å›¾åƒçŸ¥è¯†è’¸é¦ã€‚é€šè¿‡åˆ©ç”¨è¿™é¡¹æŠ€æœ¯ï¼Œæˆ‘ä»¬è¯æ˜äº†å­¦ç”Ÿæ¨¡å‹å¯ä»¥ç”Ÿæˆåœ¨è®­ç»ƒå›¾åƒä¸­æœªè§çš„æ¦‚å¿µã€‚å½“åº”ç”¨äºæ¡ä»¶æ‰©æ•£æ¨¡å‹è’¸é¦æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…è®¸å­¦ç”Ÿåœ¨ä¸ç”Ÿæˆç‰¹å®šæ¡ä»¶å›¾åƒçš„æƒ…å†µä¸‹æ¢ç´¢æ¡ä»¶ç©ºé—´ï¼Œå¯¼è‡´ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡çš„æ˜¾è‘—æé«˜ã€‚è¿™ä¿ƒè¿›äº†ç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„èµ„æºé«˜æ•ˆéƒ¨ç½²ï¼Œæ‰©å¤§äº†å…¶åœ¨ç ”ç©¶å’Œå®é™…åº”ç”¨ä¸­çš„å¯åŠæ€§ã€‚ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://dohyun-as.github.io/Random-Conditioning%E8%8E%B7%E5%8F%96%E3%80%82">https://dohyun-as.github.io/Random-Conditioningè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02011v1">PDF</a> Accepted to CVPR 2025. 8 pages main paper + 4 pages references + 5   pages supplementary, 9 figures in total</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹é€šè¿‡é€æ­¥å»å™ªç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†ç”±äºæ¨¡å‹è§„æ¨¡åºå¤§ä¸”æ¶‰åŠé‡å¤é‡‡æ ·ï¼Œè®¡ç®—é‡å¤§ã€‚çŸ¥è¯†è’¸é¦åœ¨è¯†åˆ«ä»»åŠ¡ä¸­å¹¿æ³›åº”ç”¨ï¼Œå¯ä»å¤æ‚çš„æ•™å¸ˆæ¨¡å‹å‘ç®€å•çš„æ¨¡å‹è½¬ç§»çŸ¥è¯†ï¼Œç‰¹åˆ«æ˜¯åœ¨å­¦ç”Ÿè®­ç»ƒæœŸé—´æœªè§çš„æ¦‚å¿µè½¬ç§»æ–¹é¢ã€‚ç„¶è€Œï¼Œå…¶åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨ä»è¢«è¾ƒå°‘æ¢ç´¢ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿å­¦ç”Ÿæ¨¡å‹ç”Ÿæˆè®­ç»ƒå›¾åƒæœªæ¶µç›–çš„æ¦‚å¿µæ–¹é¢ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åä¸ºRandom Conditioningçš„æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡éšæœºé€‰æ‹©çš„æ–‡æœ¬æ¡ä»¶é…å¯¹å™ªå£°å›¾åƒï¼Œå®ç°äº†é«˜æ•ˆã€æ— éœ€å›¾åƒçš„çŸ¥è¯†è’¸é¦ã€‚åˆ©ç”¨æ­¤æŠ€æœ¯ï¼Œå­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿåœ¨è®­ç»ƒå›¾åƒä¸­ç”Ÿæˆæœªè§çš„æ¦‚å¿µã€‚åœ¨åº”ç”¨äºæ¡ä»¶æ‰©æ•£æ¨¡å‹è’¸é¦æ—¶ï¼Œè¯¥æ–¹æ³•ä½¿å­¦ç”Ÿèƒ½å¤Ÿåœ¨æ— éœ€ç”Ÿæˆç‰¹å®šæ¡ä»¶å›¾åƒçš„æƒ…å†µä¸‹æ¢ç´¢æ¡ä»¶ç©ºé—´ï¼Œä»è€Œåœ¨ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡æ–¹é¢å–å¾—æ˜¾è‘—æ”¹å–„ã€‚è¿™ä¿ƒè¿›äº†ç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„èµ„æºé«˜æ•ˆéƒ¨ç½²ï¼Œæ‰©å¤§äº†å…¶åœ¨ç ”ç©¶å’Œå®é™…åº”ç”¨ä¸­çš„å¯åŠæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹é€šè¿‡é€æ­¥å»å™ªç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†è®¡ç®—é‡å¤§ã€‚</li>
<li>çŸ¥è¯†è’¸é¦åœ¨è¯†åˆ«ä»»åŠ¡ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨ä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚</li>
<li>Random Conditioningæ–¹æ³•é€šè¿‡éšæœºé€‰æ‹©çš„æ–‡æœ¬æ¡ä»¶é…å¯¹å™ªå£°å›¾åƒï¼Œå®ç°äº†é«˜æ•ˆã€æ— éœ€å›¾åƒçš„çŸ¥è¯†è’¸é¦ã€‚</li>
<li>å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿåœ¨è®­ç»ƒå›¾åƒä¸­ç”Ÿæˆæœªè§çš„æ¦‚å¿µã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡ã€‚</li>
<li>è¯¥æ–¹æ³•ä¿ƒè¿›äº†ç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„èµ„æºé«˜æ•ˆéƒ¨ç½²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02011">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-840c853a3cc8714d04900ef508507eb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-009a7d8ed86139a9bb3a70defa99815e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc152ec1571a03b161a1816a50b94c58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f174d6abac710557281a75b9bcc27b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85fe979e433a4dc3775f5ef4084ee372.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8bcde627e2daffe1a79aed09e143738.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="OccludeNeRF-Geometric-aware-3D-Scene-Inpainting-with-Collaborative-Score-Distillation-in-NeRF"><a href="#OccludeNeRF-Geometric-aware-3D-Scene-Inpainting-with-Collaborative-Score-Distillation-in-NeRF" class="headerlink" title="OccludeNeRF: Geometric-aware 3D Scene Inpainting with Collaborative   Score Distillation in NeRF"></a>OccludeNeRF: Geometric-aware 3D Scene Inpainting with Collaborative   Score Distillation in NeRF</h2><p><strong>Authors:Jingyu Shi, Achleshwar Luthra, Jiazhi Li, Xiang Gao, Xiyun Song, Zongfang Lin, David Gu, Heather Yu</strong></p>
<p>With Neural Radiance Fields (NeRFs) arising as a powerful 3D representation, research has investigated its various downstream tasks, including inpainting NeRFs with 2D images. Despite successful efforts addressing the view consistency and geometry quality, prior methods yet suffer from occlusion in NeRF inpainting tasks, where 2D prior is severely limited in forming a faithful reconstruction of the scene to inpaint.   To address this, we propose a novel approach that enables cross-view information sharing during knowledge distillation from a diffusion model, effectively propagating occluded information across limited views. Additionally, to align the distillation direction across multiple sampled views, we apply a grid-based denoising strategy and incorporate additional rendered views to enhance cross-view consistency. To assess our approachâ€™s capability of handling occlusion cases, we construct a dataset consisting of challenging scenes with severe occlusion, in addition to existing datasets. Compared with baseline methods, our method demonstrates better performance in cross-view consistency and faithfulness in reconstruction, while preserving high rendering quality and fidelity. </p>
<blockquote>
<p>éšç€ç¥ç»è¾å°„åœºï¼ˆNeRFsï¼‰ä½œä¸ºä¸€ç§å¼ºå¤§çš„3Dè¡¨ç¤ºæ–¹æ³•çš„å‘å±•ï¼Œç ”ç©¶å·²ç»æ¢ç´¢äº†å…¶å„ç§ä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒ…æ‹¬ä½¿ç”¨2Då›¾åƒå¡«å……NeRFsã€‚å°½ç®¡å·²ç»å‡ºç°äº†é’ˆå¯¹è§†å›¾ä¸€è‡´æ€§å’Œå‡ ä½•è´¨é‡æå‡çš„æˆåŠŸå°è¯•ï¼Œå…ˆå‰çš„æ–¹æ³•åœ¨NeRFå¡«å……ä»»åŠ¡ä¸­ä»å—åˆ°é®æŒ¡é—®é¢˜çš„å›°æ‰°ï¼Œå…¶ä¸­ç”±äºäºŒç»´å…ˆéªŒçš„å±€é™æ€§ï¼Œæ— æ³•å‡†ç¡®é‡å»ºåœºæ™¯ä»¥è¿›è¡Œå¡«å……ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹çš„çŸ¥è¯†è’¸é¦è¿‡ç¨‹å®ç°è·¨è§†å›¾ä¿¡æ¯å…±äº«ï¼Œæœ‰æ•ˆåœ°ä¼ æ’­äº†æœ‰é™çš„è§†å›¾ä¸­çš„é®æŒ¡ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¯¹é½å¤šä¸ªé‡‡æ ·è§†å›¾ä¸­çš„è’¸é¦æ–¹å‘ï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºç½‘æ ¼çš„å»å™ªç­–ç•¥å¹¶èå…¥é¢å¤–çš„æ¸²æŸ“è§†å›¾ä»¥å¢å¼ºè·¨è§†å›¾ä¸€è‡´æ€§ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤„ç†é®æŒ¡æ¡ˆä¾‹æ–¹é¢çš„èƒ½åŠ›ï¼Œé™¤äº†ç°æœ‰çš„æ•°æ®é›†å¤–ï¼Œæˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªåŒ…å«å…·æœ‰ä¸¥é‡é®æŒ¡çš„æŒ‘æˆ˜æ€§åœºæ™¯çš„æ•°æ®é›†ã€‚ä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è·¨è§†å›¾ä¸€è‡´æ€§ã€é‡å»ºçš„å¿ å®æ€§å’Œä¿æŒé«˜è´¨é‡æ¸²æŸ“æ–¹é¢è¡¨ç°å‡ºäº†æ›´å¥½çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02007v1">PDF</a> CVPR 2025 CV4Metaverse</p>
<p><strong>Summary</strong></p>
<p>åŸºäºNeRFï¼ˆç¥ç»è¾å°„åœºï¼‰çš„å¼ºå¤§3Dè¡¨ç¤ºèƒ½åŠ›ï¼Œç°æœ‰ç ”ç©¶å¯¹å…¶ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œäº†æ·±å…¥æ¢ç´¢ï¼ŒåŒ…æ‹¬åˆ©ç”¨2Då›¾åƒè¿›è¡ŒNeRFè¡¥å…¨ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•åœ¨ä¿æŒè§†å›¾ä¸€è‡´æ€§å’Œå‡ ä½•è´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨NeRFè¡¥å…¨ä»»åŠ¡ä¸­ä»é¢ä¸´é®æŒ¡é—®é¢˜ï¼Œ2Då…ˆéªŒä¿¡æ¯åœ¨é‡å»ºå¾…è¡¥å…¨åœºæ™¯æ—¶éš¾ä»¥å½¢æˆå¿ å®è¡¨è¾¾ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„çŸ¥è¯†è’¸é¦è¿‡ç¨‹å®ç°è·¨è§†å›¾ä¿¡æ¯å…±äº«ï¼Œæœ‰æ•ˆä¼ æ’­è·¨æœ‰é™è§†å›¾çš„é®æŒ¡ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨åŸºäºç½‘æ ¼çš„å»å™ªç­–ç•¥ï¼Œå¹¶é€šè¿‡æ·»åŠ é¢å¤–æ¸²æŸ“è§†å›¾æ¥å¯¹é½å¤šä¸ªé‡‡æ ·è§†å›¾çš„è’¸é¦æ–¹å‘ï¼Œä»¥æé«˜è·¨è§†å›¾ä¸€è‡´æ€§ã€‚é€šè¿‡æ„å»ºåŒ…å«ä¸¥é‡é®æŒ¡æŒ‘æˆ˜åœºæ™¯çš„æ•°æ®é›†ï¼Œå¹¶ä¸å…¶ä»–åŸºå‡†æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è·¨è§†å›¾ä¸€è‡´æ€§ã€é‡å»ºå¿ å®åº¦ã€æ¸²æŸ“è´¨é‡å’Œä¿çœŸåº¦æ–¹é¢è¡¨ç°å‡ºæ›´ä½³çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFä½œä¸ºä¸€ç§å¼ºå¤§çš„3Dè¡¨ç¤ºæ–¹æ³•ï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡å¦‚NeRFè¡¥å…¨ä¸­å—åˆ°å…³æ³¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨è§£å†³NeRFè¡¥å…¨ä»»åŠ¡ä¸­çš„é®æŒ¡é—®é¢˜æ—¶å­˜åœ¨å±€é™ï¼Œ2Då…ˆéªŒä¿¡æ¯éš¾ä»¥å½¢æˆå¯¹åœºæ™¯çš„å¿ å®é‡å»ºã€‚</li>
<li>æå‡ºäº†ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹çŸ¥è¯†è’¸é¦çš„è·¨è§†å›¾ä¿¡æ¯å…±äº«æ–°æ–¹æ³•ï¼Œä»¥è§£å†³é®æŒ¡é—®é¢˜ã€‚</li>
<li>é€šè¿‡åŸºäºç½‘æ ¼çš„å»å™ªç­–ç•¥å’Œé¢å¤–æ¸²æŸ“è§†å›¾ï¼Œæé«˜è·¨è§†å›¾ä¸€è‡´æ€§ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªåŒ…å«å…·æœ‰ä¸¥é‡é®æŒ¡æŒ‘æˆ˜åœºæ™¯çš„æ•°æ®é›†ï¼Œä»¥è¯„ä¼°æ–¹æ³•æ€§èƒ½ã€‚</li>
<li>ä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨è·¨è§†å›¾ä¸€è‡´æ€§ã€é‡å»ºå¿ å®åº¦ã€æ¸²æŸ“è´¨é‡å’Œä¿çœŸåº¦æ–¹é¢è¡¨ç°æ›´ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02007">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-46f4f4ff08fe22007f2ab856133bcf55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6cfbb90b0daf0363c0726abf49ae20a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07334afa6fe040341a95e853e3952925.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-18bb89cb2d91e4e8eed69b7146fc466a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ILLUME-Illuminating-Unified-MLLM-with-Dual-Visual-Tokenization-and-Diffusion-Refinement"><a href="#ILLUME-Illuminating-Unified-MLLM-with-Dual-Visual-Tokenization-and-Diffusion-Refinement" class="headerlink" title="ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and   Diffusion Refinement"></a>ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and   Diffusion Refinement</h2><p><strong>Authors:Runhui Huang, Chunwei Wang, Junwei Yang, Guansong Lu, Yunlong Yuan, Jianhua Han, Lu Hou, Wei Zhang, Lanqing Hong, Hengshuang Zhao, Hang Xu</strong></p>
<p>We present ILLUME+ that leverages dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities in a unified model: understanding, generation, and editing. Models like Chameleon and EMU3 utilize VQGAN for image discretization, due to the lack of deep semantic interaction, they lag behind specialist models like LLaVA in visual understanding tasks. To mitigate this, LaViT and ILLUME employ semantic encoders for tokenization, but they struggle with image editing due to poor texture preservation. Meanwhile, Janus series decouples the input and output image representation, limiting their abilities to seamlessly handle interleaved image-text understanding and generation. In contrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which preserves both fine-grained textures and text-aligned semantics while enabling a coarse-to-fine image representation strategy for multimodal understanding and generation. Additionally, we employ a diffusion model as the image detokenizer for enhanced generation quality and efficient super-resolution. ILLUME+ follows a continuous-input, discrete-output scheme within the unified MLLM and adopts a progressive training procedure that supports dynamic resolution across the vision tokenizer, MLLM, and diffusion decoder. This design allows for flexible and efficient context-aware image editing and generation across diverse tasks. ILLUME+ (3B) exhibits competitive performance against existing unified MLLMs and specialized models across multimodal understanding, generation, and editing benchmarks. With its strong performance, ILLUME+ provides a scalable and versatile foundation for future multimodal applications. Project Page: <a target="_blank" rel="noopener" href="https://illume-unified-mllm.github.io/">https://illume-unified-mllm.github.io/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ILLUME+ï¼Œå®ƒåˆ©ç”¨åŒé‡è§†è§‰æ ‡è®°åŒ–å’Œæ‰©æ•£è§£ç å™¨æ¥æé«˜æ·±åº¦è¯­ä¹‰ç†è§£å’Œé«˜ä¿çœŸå›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚ç°æœ‰çš„ç»Ÿä¸€æ¨¡å‹åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­åŒæ—¶å¤„ç†ä¸‰ç§åŸºæœ¬èƒ½åŠ›ï¼ˆç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ï¼‰æ—¶é‡åˆ°äº†å›°éš¾ã€‚è¯¸å¦‚å˜è‰²é¾™å’ŒEMU3ä¹‹ç±»çš„æ¨¡å‹ä½¿ç”¨VQGANè¿›è¡Œå›¾åƒç¦»æ•£åŒ–ï¼Œç”±äºç¼ºä¹æ·±åº¦è¯­ä¹‰äº¤äº’ï¼Œå®ƒä»¬åœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸Šè½åäºLLaVAç­‰ä¸“ç”¨æ¨¡å‹ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼ŒLaViTå’ŒILLUMEé‡‡ç”¨è¯­ä¹‰ç¼–ç å™¨è¿›è¡Œæ ‡è®°åŒ–ï¼Œä½†å®ƒä»¬åœ¨è¿›è¡Œå›¾åƒç¼–è¾‘æ—¶ç”±äºçº¹ç†ä¿å­˜ä¸ä½³è€Œé‡åˆ°å›°éš¾ã€‚ä¸æ­¤åŒæ—¶ï¼ŒJanusç³»åˆ—å°†è¾“å…¥å’Œè¾“å‡ºå›¾åƒè¡¨ç¤ºåˆ†ç¦»ï¼Œé™åˆ¶äº†å®ƒä»¬æ— ç¼å¤„ç†äº¤é”™å¼å›¾åƒæ–‡æœ¬ç†è§£å’Œç”Ÿæˆçš„èƒ½åŠ›ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒILLUME+å¼•å…¥äº†ä¸€ä¸ªç»Ÿä¸€çš„åŒé‡è§†è§‰æ ‡è®°å™¨DualViTokï¼Œå®ƒæ—¢èƒ½ä¿ç•™ç²¾ç»†çº¹ç†åˆèƒ½ä¿æŒæ–‡æœ¬å¯¹é½è¯­ä¹‰ï¼ŒåŒæ—¶å®ç°ç²—ç»†ç»“åˆçš„å›¾åƒè¡¨ç¤ºç­–ç•¥ï¼Œç”¨äºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨æ‰©æ•£æ¨¡å‹ä½œä¸ºå›¾åƒè§£æ ‡è®°å™¨ï¼Œä»¥æé«˜ç”Ÿæˆè´¨é‡å’Œé«˜æ•ˆè¶…åˆ†è¾¨ç‡ã€‚ILLUME+åœ¨ç»Ÿä¸€MLLMå†…é‡‡ç”¨è¿ç»­è¾“å…¥ã€ç¦»æ•£è¾“å‡ºçš„æ–¹æ¡ˆï¼Œå¹¶é‡‡ç”¨æ¸è¿›å¼è®­ç»ƒç¨‹åºï¼Œæ”¯æŒè§†è§‰æ ‡è®°å™¨ã€MLLMå’Œæ‰©æ•£è§£ç å™¨ä¹‹é—´çš„åŠ¨æ€åˆ†è¾¨ç‡è°ƒæ•´ã€‚è¿™ç§è®¾è®¡ä½¿å¾—è·¨ä¸åŒä»»åŠ¡çš„çµæ´»å’Œé«˜æ•ˆä¸Šä¸‹æ–‡æ„ŸçŸ¥å›¾åƒç¼–è¾‘å’Œç”Ÿæˆæˆä¸ºå¯èƒ½ã€‚ILLUME+ï¼ˆ3Bï¼‰åœ¨å¤šæ¨¡æ€ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¸ç°æœ‰ç»Ÿä¸€MLLMå’Œä¸“ç”¨æ¨¡å‹ç›¸ç«äº‰çš„æ€§èƒ½ã€‚å‡­å€Ÿå¼ºå¤§çš„æ€§èƒ½ï¼ŒILLUME+ä¸ºæœªæ¥å¤šæ¨¡æ€åº”ç”¨æä¾›äº†å¯æ‰©å±•å’Œå¤šåŠŸèƒ½çš„åŸºç¡€ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://illume-unified-mllm.github.io/">https://illume-unified-mllm.github.io/</a>.</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01934v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    ILUME+é€šè¿‡åˆ©ç”¨åŒé‡è§†è§‰æ ‡è®°åŒ–å’Œæ‰©æ•£è§£ç å™¨ï¼Œæ”¹è¿›äº†æ·±åº¦è¯­ä¹‰ç†è§£å’Œé«˜ä¿çœŸå›¾åƒç”Ÿæˆã€‚ç°æœ‰ç»Ÿä¸€æ¨¡å‹åœ¨ä¸‰é¡¹åŸºæœ¬èƒ½åŠ›ä¸Šéš¾ä»¥å…¼é¡¾ï¼šç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ã€‚ILLUME+å¼•å…¥ç»Ÿä¸€åŒé‡è§†è§‰æ ‡è®°å™¨DualViTokï¼Œä¿ç•™ç²¾ç»†çº¹ç†å’Œæ–‡æœ¬å¯¹é½è¯­ä¹‰ï¼Œé‡‡ç”¨ç”±ç²—åˆ°ç»†å›¾åƒè¡¨ç¤ºç­–ç•¥è¿›è¡Œå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚æ­¤å¤–ï¼Œé‡‡ç”¨æ‰©æ•£æ¨¡å‹ä½œä¸ºå›¾åƒè§£æ ‡è®°å™¨ï¼Œæé«˜ç”Ÿæˆè´¨é‡å’Œè¶…åˆ†è¾¨ç‡æ•ˆç‡ã€‚ILLUME+é‡‡ç”¨è¿ç»­è¾“å…¥ã€ç¦»æ•£è¾“å‡ºçš„ç»Ÿä¸€MLLMæ–¹æ¡ˆï¼Œé‡‡ç”¨æ¸è¿›å¼è®­ç»ƒç¨‹åºï¼Œæ”¯æŒè§†è§‰æ ‡è®°å™¨ã€MLLMå’Œæ‰©æ•£è§£ç å™¨ä¹‹é—´çš„åŠ¨æ€åˆ†è¾¨ç‡ã€‚è¿™ä¸ºè·¨ä¸åŒä»»åŠ¡çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å›¾åƒç¼–è¾‘å’Œç”Ÿæˆæä¾›äº†çµæ´»å’Œé«˜æ•ˆçš„æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ILLUME+ç»“åˆäº†åŒé‡è§†è§‰æ ‡è®°åŒ–å’Œæ‰©æ•£è§£ç å™¨ï¼Œæå‡äº†æ·±åº¦è¯­ä¹‰ç†è§£å’Œé«˜ä¿çœŸå›¾åƒç”Ÿæˆã€‚</li>
<li>ç°æœ‰æ¨¡å‹å¦‚Chameleonå’ŒEMU3åœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸Šè½åäºä¸“å®¶æ¨¡å‹ï¼Œå¦‚LLaVAã€‚</li>
<li>LaViTå’ŒILLUMEä½¿ç”¨è¯­ä¹‰ç¼–ç å™¨è¿›è¡Œæ ‡è®°åŒ–ï¼Œä½†åœ¨å›¾åƒç¼–è¾‘æ–¹é¢å› çº¹ç†ä¿ç•™ä¸è¶³è€Œé‡åˆ°å›°éš¾ã€‚</li>
<li>Janusç³»åˆ—å°†è¾“å…¥å’Œè¾“å‡ºå›¾åƒè¡¨ç¤ºè§£è€¦ï¼Œé™åˆ¶äº†ä»–ä»¬å¤„ç†äº¤ç»‡çš„å›¾åƒ-æ–‡æœ¬ç†è§£å’Œç”Ÿæˆçš„èƒ½åŠ›ã€‚</li>
<li>ILLUME+çš„DualViTokä¿ç•™äº†ç²¾ç»†çº¹ç†å’Œæ–‡æœ¬å¯¹é½è¯­ä¹‰ï¼Œå¹¶å®ç°äº†ç²—åˆ°ç»†çš„å›¾åƒè¡¨ç¤ºç­–ç•¥ã€‚</li>
<li>é‡‡ç”¨æ‰©æ•£æ¨¡å‹ä½œä¸ºå›¾åƒè§£æ ‡è®°å™¨ï¼Œæé«˜äº†ç”Ÿæˆè´¨é‡å’Œè¶…åˆ†è¾¨ç‡æ•ˆç‡ã€‚</li>
<li>ILLUME+çš„æ¸è¿›å¼è®­ç»ƒç¨‹åºæ”¯æŒåŠ¨æ€åˆ†è¾¨ç‡è°ƒæ•´ï¼Œä¸ºè·¨ä»»åŠ¡ä¸Šä¸‹æ–‡æ„ŸçŸ¥å›¾åƒç¼–è¾‘å’Œç”Ÿæˆæä¾›äº†çµæ´»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01934">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-309a8e660f4a6803bbcbdb1b2cbe128c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc92edbce4f88d4f781481b545d1475d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6b0e79eb6b7dd27324f19e04b9430c4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Distilling-Multi-view-Diffusion-Models-into-3D-Generators"><a href="#Distilling-Multi-view-Diffusion-Models-into-3D-Generators" class="headerlink" title="Distilling Multi-view Diffusion Models into 3D Generators"></a>Distilling Multi-view Diffusion Models into 3D Generators</h2><p><strong>Authors:Hao Qin, Luyuan Chen, Ming Kong, Mengxu Lu, Qiang Zhu</strong></p>
<p>We introduce DD3G, a formulation that Distills a multi-view Diffusion model (MV-DM) into a 3D Generator using gaussian splatting. DD3G compresses and integrates extensive visual and spatial geometric knowledge from the MV-DM by simulating its ordinary differential equation (ODE) trajectory, ensuring the distilled generator generalizes better than those trained solely on 3D data. Unlike previous amortized optimization approaches, we align the MV-DM and 3D generator representation spaces to transfer the teacherâ€™s probabilistic flow to the student, thus avoiding inconsistencies in optimization objectives caused by probabilistic sampling. The introduction of probabilistic flow and the coupling of various attributes in 3D Gaussians introduce challenges in the generation process. To tackle this, we propose PEPD, a generator consisting of Pattern Extraction and Progressive Decoding phases, which enables efficient fusion of probabilistic flow and converts a single image into 3D Gaussians within 0.06 seconds. Furthermore, to reduce knowledge loss and overcome sparse-view supervision, we design a joint optimization objective that ensures the quality of generated samples through explicit supervision and implicit verification. Leveraging existing 2D generation models, we compile 120k high-quality RGBA images for distillation. Experiments on synthetic and public datasets demonstrate the effectiveness of our method. Our project is available at: <a target="_blank" rel="noopener" href="https://qinbaigao.github.io/DD3G_project/">https://qinbaigao.github.io/DD3G_project/</a> </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†DD3Gï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡é«˜æ–¯æ‹¼è´´æ³•å°†å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼ˆMV-DMï¼‰è’¸é¦åˆ°3Dç”Ÿæˆå™¨çš„æ–¹æ³•ã€‚DD3Gé€šè¿‡æ¨¡æ‹ŸMV-DMçš„å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰è½¨è¿¹ï¼Œå‹ç¼©å¹¶é›†æˆäº†å¹¿æ³›çš„è§†è§‰å’Œç©ºé—´å‡ ä½•çŸ¥è¯†ï¼Œç¡®ä¿è’¸é¦å‡ºçš„ç”Ÿæˆå™¨æ¯”ä»…åŸºäº3Dæ•°æ®è®­ç»ƒçš„ç”Ÿæˆå™¨å…·æœ‰æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚ä¸å‰æœŸçš„æ‘Šé”€ä¼˜åŒ–æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬å°†MV-DMå’Œ3Dç”Ÿæˆå™¨çš„è¡¨ç¤ºç©ºé—´å¯¹é½ï¼Œä»¥å°†æ•™å¸ˆçš„æ¦‚ç‡æµä¼ è¾“ç»™å­¦ç”Ÿï¼Œä»è€Œé¿å…ç”±æ¦‚ç‡é‡‡æ ·å¼•èµ·çš„ä¼˜åŒ–ç›®æ ‡ä¸ä¸€è‡´ã€‚æ¦‚ç‡æµçš„å¼•å…¥å’Œ3Dé«˜æ–¯ä¸­å„ç§å±æ€§çš„è€¦åˆç»™ç”Ÿæˆè¿‡ç¨‹å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†PEPDç”Ÿæˆå™¨ï¼ŒåŒ…æ‹¬æ¨¡å¼æå–å’Œæ¸è¿›è§£ç ä¸¤ä¸ªé˜¶æ®µï¼Œèƒ½å¤Ÿå®ç°æ¦‚ç‡æµçš„æœ‰æ•ˆèåˆï¼Œå¹¶åœ¨0.06ç§’å†…å°†å•å¹…å›¾åƒè½¬æ¢ä¸º3Dé«˜æ–¯ã€‚æ­¤å¤–ï¼Œä¸ºäº†å‡å°‘çŸ¥è¯†æŸå¤±å¹¶å…‹æœç¨€ç–è§†å›¾ç›‘ç£ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè”åˆä¼˜åŒ–ç›®æ ‡ï¼Œé€šè¿‡æ˜ç¡®ç›‘ç£å’Œéšå¼éªŒè¯ç¡®ä¿ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ã€‚æˆ‘ä»¬åˆ©ç”¨ç°æœ‰çš„2Dç”Ÿæˆæ¨¡å‹ï¼Œç¼–è¯‘äº†12ä¸‡å¼ é«˜è´¨é‡RGBAå›¾åƒç”¨äºè’¸é¦ã€‚åœ¨åˆæˆå’Œå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„é¡¹ç›®å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://qinbaigao.github.io/DD3G_project/">https://qinbaigao.github.io/DD3G_project/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00457v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>DD3Gæ˜¯ä¸€ä¸ªå°†å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ï¼ˆMV-DMï¼‰è’¸é¦åˆ°3Dç”Ÿæˆå™¨çš„æ¨¡å‹ã€‚å®ƒé€šè¿‡æ¨¡æ‹ŸMV-DMçš„å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰è½¨è¿¹ï¼Œå‹ç¼©å¹¶æ•´åˆè§†è§‰å’Œå‡ ä½•ç©ºé—´çŸ¥è¯†ã€‚DD3Gé‡‡ç”¨æ¦‚ç‡æµå¯¹é½çš„æ–¹å¼ï¼Œé¿å…ä¼˜åŒ–ç›®æ ‡çš„ä¸ä¸€è‡´æ€§ã€‚ä¸ºåº”å¯¹ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æŒ‘æˆ˜ï¼Œå¼•å…¥PEPDç”Ÿæˆå™¨ï¼Œå®ç°æ¦‚ç‡æµçš„èåˆå’Œé«˜æ•ˆè½¬æ¢ã€‚é€šè¿‡è”åˆä¼˜åŒ–ç›®æ ‡ï¼Œå‡å°‘çŸ¥è¯†æŸå¤±å¹¶å…‹æœç¨€ç–ç›‘ç£é—®é¢˜ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨åˆæˆå’Œå…¬å¼€æ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DD3Gæ˜¯ä¸€ä¸ªå¤šè§†å›¾æ‰©æ•£æ¨¡å‹çš„è’¸é¦ç‰ˆæœ¬ï¼Œè¢«è’¸é¦æˆä¸€ä¸ª3Dç”Ÿæˆå™¨ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹ŸMV-DMçš„ODEè½¨è¿¹ï¼ŒDD3Gæ•´åˆäº†è§†è§‰å’Œå‡ ä½•ç©ºé—´çŸ¥è¯†ã€‚</li>
<li>é‡‡ç”¨æ¦‚ç‡æµå¯¹é½æ–¹å¼ï¼Œç¡®ä¿å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ•™å¸ˆçš„æ¦‚ç‡æµï¼Œé¿å…äº†ä¼˜åŒ–ç›®æ ‡çš„ä¸ä¸€è‡´æ€§ã€‚</li>
<li>PEPDç”Ÿæˆå™¨çš„å¼•å…¥è§£å†³äº†ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æŒ‘æˆ˜ï¼Œå®ç°äº†æ¦‚ç‡æµçš„èåˆå’Œé«˜æ•ˆè½¬æ¢ã€‚</li>
<li>é€šè¿‡è”åˆä¼˜åŒ–ç›®æ ‡ï¼Œå‡å°‘çŸ¥è¯†æŸå¤±å¹¶å…‹æœç¨€ç–ç›‘ç£é—®é¢˜ï¼Œç¡®ä¿äº†ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨åˆæˆå’Œå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00457">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1ce28162b577c4f3fe5dba65a99d4f1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd5ced50961b209c9ff6ef2cd5c0663e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7b487ff167a29735997ac5d402d1d2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6eb6f5bc327dbaba705f3566712c6fc2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b572b2194b81b3883cafe56abae3366.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Flow-to-the-Mode-Mode-Seeking-Diffusion-Autoencoders-for-State-of-the-Art-Image-Tokenization"><a href="#Flow-to-the-Mode-Mode-Seeking-Diffusion-Autoencoders-for-State-of-the-Art-Image-Tokenization" class="headerlink" title="Flow to the Mode: Mode-Seeking Diffusion Autoencoders for   State-of-the-Art Image Tokenization"></a>Flow to the Mode: Mode-Seeking Diffusion Autoencoders for   State-of-the-Art Image Tokenization</h2><p><strong>Authors:Kyle Sargent, Kyle Hsu, Justin Johnson, Li Fei-Fei, Jiajun Wu</strong></p>
<p>Since the advent of popular visual generation frameworks like VQGAN and latent diffusion models, state-of-the-art image generation systems have generally been two-stage systems that first tokenize or compress visual data into a lower-dimensional latent space before learning a generative model. Tokenizer training typically follows a standard recipe in which images are compressed and reconstructed subject to a combination of MSE, perceptual, and adversarial losses. Diffusion autoencoders have been proposed in prior work as a way to learn end-to-end perceptually-oriented image compression, but have not yet shown state-of-the-art performance on the competitive task of ImageNet-1K reconstruction. We propose FlowMo, a transformer-based diffusion autoencoder that achieves a new state-of-the-art for image tokenization at multiple compression rates without using convolutions, adversarial losses, spatially-aligned two-dimensional latent codes, or distilling from other tokenizers. Our key insight is that FlowMo training should be broken into a mode-matching pre-training stage and a mode-seeking post-training stage. In addition, we conduct extensive analyses and explore the training of generative models atop the FlowMo tokenizer. Our code and models will be available at <a target="_blank" rel="noopener" href="http://kylesargent.github.io/flowmo">http://kylesargent.github.io/flowmo</a> . </p>
<blockquote>
<p>è‡ªä»å‡ºç°äº†VQGANå’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ç­‰æµè¡Œçš„è§†è§‰ç”Ÿæˆæ¡†æ¶ä»¥æ¥ï¼Œæœ€å…ˆè¿›çš„å›¾åƒç”Ÿæˆç³»ç»Ÿé€šå¸¸æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µç³»ç»Ÿï¼Œå®ƒé¦–å…ˆä¼šå°†è§†è§‰æ•°æ®æ ‡è®°åŒ–æˆ–å‹ç¼©åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œç„¶åå­¦ä¹ ç”Ÿæˆæ¨¡å‹ã€‚æ ‡è®°å™¨è®­ç»ƒé€šå¸¸éµå¾ªä¸€ä¸ªæ ‡å‡†æµç¨‹ï¼Œå›¾åƒè¢«å‹ç¼©å’Œé‡å»ºï¼ŒåŒæ—¶å—åˆ°å‡æ–¹è¯¯å·®ã€æ„ŸçŸ¥å’Œå¯¹æŠ—æŸå¤±çš„åˆ¶çº¦ã€‚å…ˆå‰çš„å·¥ä½œå·²ç»æå‡ºäº†æ‰©æ•£è‡ªç¼–ç å™¨ä½œä¸ºå­¦ä¹ ç«¯åˆ°ç«¯æ„ŸçŸ¥å¯¼å‘çš„å›¾åƒå‹ç¼©çš„ä¸€ç§æ–¹æ³•ï¼Œä½†åœ¨ImageNet-1Ké‡å»ºçš„ç«èµ›ä»»åŠ¡ä¸­å°šæœªè¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†FlowMoï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè½¬æ¢å™¨çš„æ‰©æ•£è‡ªç¼–ç å™¨ï¼Œå®ƒå®ç°äº†å›¾åƒæ ‡è®°åŒ–çš„æ–°æœ€ä½³æ€§èƒ½ï¼Œæ”¯æŒå¤šç§å‹ç¼©ç‡ï¼Œæ— éœ€ä½¿ç”¨å·ç§¯ã€å¯¹æŠ—æ€§æŸå¤±ã€ç©ºé—´å¯¹é½çš„äºŒç»´æ½œåœ¨ä»£ç æˆ–å…¶ä»–æ ‡è®°å™¨è¿›è¡Œæç‚¼ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯FlowMoè®­ç»ƒåº”è¯¥åˆ†ä¸ºæ¨¡å¼åŒ¹é…çš„é¢„è®­ç»ƒé˜¶æ®µå’Œæ¨¡å¼æœç´¢çš„åè®­ç»ƒé˜¶æ®µã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å¹¿æ³›çš„åˆ†æå’Œæ¢ç´¢äº†åŸºäºFlowMoæ ‡è®°å™¨çš„ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="http://kylesargent.github.io/flowmo%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">http://kylesargent.github.io/flowmoä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11056v2">PDF</a> 18 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºVQGANå’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ç­‰æµè¡Œè§†è§‰ç”Ÿæˆæ¡†æ¶çš„å‡ºç°ï¼Œç›®å‰å…ˆè¿›çš„å›¾åƒç”Ÿæˆç³»ç»Ÿé€šå¸¸é‡‡ç”¨ä¸¤é˜¶æ®µç³»ç»Ÿï¼Œç¬¬ä¸€é˜¶æ®µå°†è§†è§‰æ•°æ®ä»¤ç‰ŒåŒ–æˆ–å‹ç¼©åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œç„¶åå­¦ä¹ ç”Ÿæˆæ¨¡å‹ã€‚æœ¬æ–‡æå‡ºFlowMoï¼Œä¸€ç§åŸºäºè½¬æ¢å™¨çš„æ‰©æ•£è‡ªåŠ¨ç¼–ç å™¨ï¼Œæ— éœ€å·ç§¯ã€å¯¹æŠ—æ€§æŸå¤±ã€ç©ºé—´å¯¹é½çš„äºŒç»´æ½œåœ¨ä»£ç æˆ–è’¸é¦ï¼Œå³å¯åœ¨å¤šä¸ªå‹ç¼©ç‡ä¸‹å®ç°å›¾åƒä»¤ç‰ŒåŒ–çš„æ–°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å…³é”®è§è§£æ˜¯FlowMoè®­ç»ƒåº”åˆ†ä¸ºæ¨¡å¼åŒ¹é…çš„é¢„è®­ç»ƒé˜¶æ®µå’Œæ¨¡å¼æœç´¢çš„åè®­ç»ƒé˜¶æ®µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å›¾åƒç”Ÿæˆç³»ç»Ÿå¤šé‡‡ç”¨ä¸¤é˜¶æ®µç³»ç»Ÿï¼ŒåŒ…æ‹¬æ•°æ®ä»¤ç‰ŒåŒ–æˆ–å‹ç¼©åˆ°æ½œåœ¨ç©ºé—´ä»¥åŠç”Ÿæˆæ¨¡å‹çš„å­¦ä¹ ã€‚</li>
<li>FlowMoæ˜¯ä¸€ç§æ–°å‹çš„æ‰©æ•£è‡ªåŠ¨ç¼–ç å™¨ï¼Œèƒ½å¤Ÿåœ¨ä¸ä½¿ç”¨å·ç§¯ã€å¯¹æŠ—æ€§æŸå¤±ç­‰æŠ€æœ¯çš„æƒ…å†µä¸‹å®ç°å›¾åƒä»¤ç‰ŒåŒ–çš„å…ˆè¿›æ€§èƒ½ã€‚</li>
<li>FlowMoè®­ç»ƒåˆ†ä¸ºæ¨¡å¼åŒ¹é…çš„é¢„è®­ç»ƒé˜¶æ®µå’Œæ¨¡å¼æœç´¢çš„åè®­ç»ƒé˜¶æ®µã€‚</li>
<li>FlowMoåœ¨å¤šä¸ªå‹ç¼©ç‡ä¸‹å‡è¡¨ç°å‡ºä¼˜ç§€çš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ›æ–°çš„å›¾åƒå‹ç¼©æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å›¾åƒç”Ÿæˆç³»ç»Ÿçš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>ç ”ç©¶è€…è¿˜è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒå’Œåˆ†æï¼Œæ¢è®¨äº†åŸºäºFlowMoä»¤ç‰Œå™¨çš„ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11056">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-74dc1eb48fa7317325f1867a561e2cbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11bda9be28ec5cde3a68f07295a96a1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09ca456d44611bd649a84c3b96bc3170.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca834b418c67b8f33077ab35a86c85de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-699a98f748a0a888ca5cf2f802825ca9.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GenDR-Lightning-Generative-Detail-Restorator"><a href="#GenDR-Lightning-Generative-Detail-Restorator" class="headerlink" title="GenDR: Lightning Generative Detail Restorator"></a>GenDR: Lightning Generative Detail Restorator</h2><p><strong>Authors:Yan Wang, Shijie Zhao, Kai Chen, Kexin Zhang, Junlin Li, Li Zhang</strong></p>
<p>Recent research applying text-to-image (T2I) diffusion models to real-world super-resolution (SR) has achieved remarkable success. However, fundamental misalignments between T2I and SR targets result in a dilemma between inference speed and detail fidelity. Specifically, T2I tasks prioritize multi-step inversion to synthesize coherent outputs aligned with textual prompts and shrink the latent space to reduce generating complexity. Contrariwise, SR tasks preserve most information from low-resolution input while solely restoring high-frequency details, thus necessitating sufficient latent space and fewer inference steps. To bridge the gap, we present a one-step diffusion model for generative detail restoration, GenDR, distilled from a tailored diffusion model with larger latent space. In detail, we train a new SD2.1-VAE16 (0.9B) via representation alignment to expand latent space without enlarging the model size. Regarding step-distillation, we propose consistent score identity distillation (CiD) that incorporates SR task-specific loss into score distillation to leverage more SR priors and align the training target. Furthermore, we extend CiD with adversarial learning and representation alignment (CiDA) to enhance perceptual quality and accelerate training. We also polish the pipeline to achieve a more efficient inference. Experimental results demonstrate that GenDR achieves state-of-the-art performance in both quantitative metrics and visual fidelity. </p>
<blockquote>
<p>è¿‘æœŸå°†æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åº”ç”¨äºç°å®ä¸–ç•Œçš„è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰ç ”ç©¶å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼ŒT2Iå’ŒSRç›®æ ‡ä¹‹é—´çš„åŸºæœ¬ä¸å¯¹é½å¯¼è‡´äº†æ¨ç†é€Ÿåº¦ä¸ç»†èŠ‚ä¿çœŸåº¦ä¹‹é—´çš„å›°å¢ƒã€‚å…·ä½“æ¥è¯´ï¼ŒT2Iä»»åŠ¡ä¼˜å…ˆè¿›è¡Œå¤šæ­¥åè½¬ï¼Œä»¥åˆæˆä¸æ–‡æœ¬æç¤ºå¯¹é½çš„è¾“å‡ºï¼Œå¹¶ç¼©å°æ½œåœ¨ç©ºé—´ä»¥å‡å°‘ç”Ÿæˆå¤æ‚æ€§ã€‚ç›¸åï¼ŒSRä»»åŠ¡ä¿ç•™æ¥è‡ªä½åˆ†è¾¨ç‡è¾“å…¥çš„æœ€å¤šä¿¡æ¯ï¼ŒåŒæ—¶ä»…æ¢å¤é«˜é¢‘ç»†èŠ‚ï¼Œå› æ­¤éœ€è¦è¶³å¤Ÿçš„æ½œåœ¨ç©ºé—´å’Œè¾ƒå°‘çš„æ¨ç†æ­¥éª¤ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºç”Ÿæˆç»†èŠ‚æ¢å¤çš„æ‰©æ•£æ¨¡å‹ï¼ˆGenDRï¼‰ï¼Œå®ƒæ˜¯ä»ä¸€ä¸ªå…·æœ‰æ›´å¤§æ½œåœ¨ç©ºé—´çš„å®šåˆ¶æ‰©æ•£æ¨¡å‹ä¸­æç‚¼å‡ºæ¥çš„ä¸€æ­¥æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡è¡¨ç¤ºå¯¹é½è®­ç»ƒäº†ä¸€ä¸ªæ–°çš„SD2.1-VAE16ï¼ˆ0.9Bï¼‰æ¥æ‰©å±•æ½œåœ¨ç©ºé—´ï¼Œè€Œæ— éœ€æ‰©å¤§æ¨¡å‹å¤§å°ã€‚å…³äºæ­¥éª¤è’¸é¦ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºä¸€è‡´è¯„åˆ†èº«ä»½è’¸é¦ï¼ˆCiDï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†SRä»»åŠ¡ç‰¹å®šæŸå¤±çº³å…¥è¯„åˆ†è’¸é¦ä¸­ï¼Œä»¥åˆ©ç”¨æ›´å¤šçš„SRå…ˆéªŒçŸ¥è¯†å¹¶è°ƒæ•´è®­ç»ƒç›®æ ‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†å¯¹æŠ—æ€§å­¦ä¹ ä¸è¡¨ç¤ºå¯¹é½æ‰©å±•åˆ°CiDAï¼Œä»¥æé«˜æ„ŸçŸ¥è´¨é‡å’ŒåŠ é€Ÿè®­ç»ƒã€‚æˆ‘ä»¬è¿˜ä¼˜åŒ–äº†ç®¡é“ï¼Œä»¥å®ç°æ›´æœ‰æ•ˆçš„æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGenDRåœ¨å®šé‡æŒ‡æ ‡å’Œè§†è§‰ä¿çœŸåº¦æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06790v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰åº”ç”¨ä¸Šå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†T2Iå’ŒSRç›®æ ‡ä¹‹é—´çš„åŸºæœ¬ä¸åŒ¹é…å¯¼è‡´æ¨ç†é€Ÿåº¦ä¸ç»†èŠ‚ä¿çœŸåº¦ä¹‹é—´çš„å›°å¢ƒã€‚ä¸ºå¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºç”Ÿæˆç»†èŠ‚æ¢å¤çš„ä¸€æ¬¡æ€§æ‰©æ•£æ¨¡å‹GenDRï¼Œå®ƒç”±ä¸€ä¸ªå…·æœ‰æ›´å¤§æ½œåœ¨ç©ºé—´çš„å®šåˆ¶æ‰©æ•£æ¨¡å‹æç‚¼è€Œæˆã€‚é€šè¿‡æ–°çš„SD2.1-VAE16ï¼ˆ0.9Bï¼‰æ¨¡å‹å’Œä¸€è‡´åˆ†æ•°èº«ä»½è’¸é¦ï¼ˆCiDï¼‰ç­‰æŠ€æœ¯ï¼Œæé«˜æ„ŸçŸ¥è´¨é‡å¹¶åŠ é€Ÿè®­ç»ƒã€‚GenDRåœ¨å®šé‡æŒ‡æ ‡å’Œè§†è§‰ä¿çœŸåº¦ä¸Šéƒ½è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2Iæ‰©æ•£æ¨¡å‹åœ¨SRåº”ç”¨ä¸Šè¡¨ç°å“è¶Šï¼Œä½†å­˜åœ¨æ¨ç†é€Ÿåº¦ä¸ç»†èŠ‚ä¿çœŸåº¦çš„å›°å¢ƒã€‚</li>
<li>GenDRæ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆç»†èŠ‚æ¢å¤çš„ä¸€æ¬¡æ€§æ‰©æ•£æ¨¡å‹ï¼Œè§£å†³äº†T2Iå’ŒSRç›®æ ‡ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>GenDRé€šè¿‡æ‰©å¤§æ½œåœ¨ç©ºé—´æ¥æé«˜æ€§èƒ½ï¼Œé‡‡ç”¨æ–°çš„SD2.1-VAE16ï¼ˆ0.9Bï¼‰æ¨¡å‹ã€‚</li>
<li>ä¸€è‡´åˆ†æ•°èº«ä»½è’¸é¦ï¼ˆCiDï¼‰æŠ€æœ¯è¢«æå‡ºï¼Œç»“åˆSRä»»åŠ¡ç‰¹å®šæŸå¤±è¿›è¡Œåˆ†æ•°è’¸é¦ï¼Œåˆ©ç”¨æ›´å¤šSRå…ˆéªŒå¹¶è°ƒæ•´è®­ç»ƒç›®æ ‡ã€‚</li>
<li>CiDä¸å¯¹æŠ—æ€§å­¦ä¹ åŠè¡¨ç¤ºå¯¹é½ç›¸ç»“åˆï¼Œå¢å¼ºäº†æ„ŸçŸ¥è´¨é‡å’ŒåŠ é€Ÿäº†è®­ç»ƒã€‚</li>
<li>GenDRçš„å®éªŒç»“æœè¾¾åˆ°äº†åœ¨å®šé‡æŒ‡æ ‡å’Œè§†è§‰ä¿çœŸåº¦ä¸Šçš„æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06790">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d46cd61add6d808d59b0268b14c02369.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccc1509ae82a4324d6df7f3d0ba559c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e7cdc52b9e12d3a3570f51bdea1e91b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-737c27ccadd5bfc3b446fdccca4db200.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd02029d47a3f51126415e1d63bae6fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1486e38227fd0f76401aea444ceea6a7.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Bias-Free-Training-Paradigm-for-More-General-AI-generated-Image-Detection"><a href="#A-Bias-Free-Training-Paradigm-for-More-General-AI-generated-Image-Detection" class="headerlink" title="A Bias-Free Training Paradigm for More General AI-generated Image   Detection"></a>A Bias-Free Training Paradigm for More General AI-generated Image   Detection</h2><p><strong>Authors:Fabrizio Guillaro, Giada Zingarini, Ben Usman, Avneesh Sud, Davide Cozzolino, Luisa Verdoliva</strong></p>
<p>Successful forensic detectors can produce excellent results in supervised learning benchmarks but struggle to transfer to real-world applications. We believe this limitation is largely due to inadequate training data quality. While most research focuses on developing new algorithms, less attention is given to training data selection, despite evidence that performance can be strongly impacted by spurious correlations such as content, format, or resolution. A well-designed forensic detector should detect generator specific artifacts rather than reflect data biases. To this end, we propose B-Free, a bias-free training paradigm, where fake images are generated from real ones using the conditioning procedure of stable diffusion models. This ensures semantic alignment between real and fake images, allowing any differences to stem solely from the subtle artifacts introduced by AI generation. Through content-based augmentation, we show significant improvements in both generalization and robustness over state-of-the-art detectors and more calibrated results across 27 different generative models, including recent releases, like FLUX and Stable Diffusion 3.5. Our findings emphasize the importance of a careful dataset design, highlighting the need for further research on this topic. Code and data are publicly available at <a target="_blank" rel="noopener" href="https://grip-unina.github.io/B-Free/">https://grip-unina.github.io/B-Free/</a>. </p>
<blockquote>
<p>åœ¨ç›‘ç£å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­ï¼ŒæˆåŠŸçš„æ³•åŒ»æ£€æµ‹å™¨å¯ä»¥äº§ç”Ÿå‡ºè‰²çš„ç»“æœï¼Œä½†åœ¨è½¬ç§»åˆ°ç°å®ä¸–ç•Œåº”ç”¨æ—¶å´é‡åˆ°äº†å›°éš¾ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ç§å±€é™æ€§å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºè®­ç»ƒæ•°æ®è´¨é‡ä¸è¶³ã€‚è™½ç„¶å¤§å¤šæ•°ç ”ç©¶éƒ½é›†ä¸­åœ¨å¼€å‘æ–°ç®—æ³•ä¸Šï¼Œä½†å¯¹äºæ•°æ®é€‰æ‹©æ–¹é¢å…³æ³¨çš„å´å¾ˆå°‘ã€‚å°½ç®¡æœ‰è¯æ®è¡¨æ˜ï¼Œæ€§èƒ½å¯èƒ½ä¼šå—åˆ°å†…å®¹ã€æ ¼å¼æˆ–åˆ†è¾¨ç‡ç­‰è™šå‡å…³è”å› ç´ çš„å¼ºçƒˆå½±å“ã€‚ä¸€ä¸ªè®¾è®¡è‰¯å¥½çš„æ³•åŒ»æ£€æµ‹å™¨åº”è¯¥æ£€æµ‹ç”Ÿæˆå™¨ç‰¹å®šçš„ä¼ªå½±ï¼Œè€Œä¸æ˜¯åæ˜ æ•°æ®åè§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ— åè§è®­ç»ƒèŒƒå¼B-Freeï¼Œåœ¨è¯¥èŒƒå¼ä¸­ï¼Œä½¿ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹çš„è°ƒèŠ‚ç¨‹åºä»çœŸå®å›¾åƒç”Ÿæˆè™šå‡å›¾åƒã€‚è¿™ç¡®ä¿äº†çœŸå®å’Œè™šå‡å›¾åƒä¹‹é—´çš„è¯­ä¹‰å¯¹é½ï¼Œä½¿å¾—ä»»ä½•å·®å¼‚éƒ½ä»…æºäºäººå·¥æ™ºèƒ½ç”Ÿæˆæ‰€å¼•å…¥çš„å¾®å¦™ä¼ªå½±ã€‚é€šè¿‡åŸºäºå†…å®¹çš„å¢å¼ºæ–¹æ³•ï¼Œæˆ‘ä»¬æ˜¾ç¤ºå‡ºä¸æœ€æ–°æ£€æµ‹å™¨ç›¸æ¯”åœ¨é€šç”¨æ€§å’Œç¨³å¥æ€§æ–¹é¢çš„æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶ä¸”åœ¨åŒ…æ‹¬æœ€æ–°å‘å¸ƒçš„FLUXå’ŒStable Diffusion 3.5ç­‰27ç§ä¸åŒçš„ç”Ÿæˆæ¨¡å‹ä¸Šè·å¾—äº†æ›´ç²¾ç¡®çš„ç»“æœã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†ç²¾å¿ƒè®¾è®¡æ•°æ®é›†çš„é‡è¦æ€§ï¼Œå¹¶çªå‡ºäº†å¯¹è¿™ä¸€ä¸»é¢˜è¿›è¡Œè¿›ä¸€æ­¥ç ”ç©¶çš„å¿…è¦æ€§ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://grip-unina.github.io/B-Free/%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://grip-unina.github.io/B-Free/ä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17671v2">PDF</a> </p>
<p><strong>Summary</strong><br>     ç°æœ‰å¸æ³•é‰´å®šæ¢æµ‹å™¨åœ¨ç›‘ç£å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­å´å­˜åœ¨å›°éš¾ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºè®­ç»ƒæ•°æ®è´¨é‡ä¸è¶³ã€‚å°½ç®¡å¤§å¤šæ•°ç ”ç©¶å…³æ³¨æ–°ç®—æ³•çš„å¼€å‘ï¼Œä½†å¯¹è®­ç»ƒæ•°æ®é€‰æ‹©çš„é‡è¦æ€§å´è®¤è¯†ä¸è¶³ã€‚æ•°æ®ä¸­çš„è™šå‡å…³è”ä¼šå½±å“æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ— åè®­ç»ƒèŒƒå¼B-Freeï¼Œé€šè¿‡çœŸå®å›¾åƒç”Ÿæˆè™šå‡å›¾åƒï¼Œç¡®ä¿çœŸå®ä¸è™šå‡å›¾åƒä¹‹é—´çš„è¯­ä¹‰å¯¹é½ï¼Œä»…é€šè¿‡AIç”Ÿæˆå¼•å…¥ç»†å¾®å·®å¼‚ã€‚é€šè¿‡åŸºäºå†…å®¹çš„å¢å¼ºæŠ€æœ¯ï¼Œæˆ‘ä»¬æ˜¾ç¤ºå‡ºç›¸è¾ƒäºæœ€æ–°é‰´å®šå™¨åœ¨æ³›åŒ–å’Œç¨³å¥æ€§æ–¹é¢çš„æ˜¾è‘—æ”¹å–„ï¼Œå¹¶åœ¨åŒ…æ‹¬æœ€æ–°å‘å¸ƒçš„FLUXå’ŒStable Diffusion 3.5åœ¨å†…çš„27ç§ç”Ÿæˆæ¨¡å‹ä¸Šè·å¾—æ›´å‡†ç¡®çš„æ ¡å‡†ç»“æœã€‚æˆ‘ä»¬å¼ºè°ƒäº†ç²¾å¿ƒæ„å»ºæ•°æ®é›†çš„é‡è¦æ€§ï¼Œå¹¶çªæ˜¾å¯¹è¿™æ–¹é¢çš„ç ”ç©¶çš„å¿…è¦ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://grip-unina.github.io/B-Free/%E8%AE%BF%E9%97%AE%E3%80%82">https://grip-unina.github.io/B-Free/è®¿é—®ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¸æ³•é‰´å®šæ¢æµ‹å™¨é¢ä¸´ä»ç›‘ç£å­¦ä¹ æµ‹è¯•è½¬ç§»åˆ°å®é™…åº”ç”¨æ—¶çš„æŒ‘æˆ˜ï¼Œä¸»è¦åŸå› æ˜¯è®­ç»ƒæ•°æ®è´¨é‡ä¸è¶³ã€‚</li>
<li>ç ”ç©¶è¿‡äºå…³æ³¨æ–°ç®—æ³•å¼€å‘ï¼Œå¿½è§†äº†è®­ç»ƒæ•°æ®é€‰æ‹©çš„é‡è¦æ€§ã€‚</li>
<li>æ•°æ®ä¸­çš„è™šå‡å…³è”ä¼šå½±å“é‰´å®šæ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºB-Freeçš„æ— åè®­ç»ƒèŒƒå¼ï¼Œåˆ©ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹çš„æ¡ä»¶ç¨‹åºä»çœŸå®å›¾åƒç”Ÿæˆè™šå‡å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•ç¡®ä¿äº†çœŸå®å’Œè™šå‡å›¾åƒä¹‹é—´çš„è¯­ä¹‰å¯¹é½ï¼Œå·®å¼‚ä»…æºäºAIç”Ÿæˆçš„ç»†å¾®ç‰¹å¾ã€‚</li>
<li>é€šè¿‡åŸºäºå†…å®¹çš„å¢å¼ºæŠ€æœ¯ï¼Œåœ¨æ³›åŒ–å’Œç¨³å¥æ€§æ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹è¿›ï¼Œå¹¶åœ¨å¤šç§ç”Ÿæˆæ¨¡å‹ä¸Šè·å¾—æ›´å‡†ç¡®çš„æ ¡å‡†ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17671">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8185169c50f3aceaa84ddc8e26ca0952.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-686712de054d1f801372b4c6e169c12e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9c23ebd8b942dae9af1a156ee57ba9ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed03d90d2e830ac7f9dd1997705c2190.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9865df032a57a1f38970eb275288791b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3043e432c6559ae0eee214f9098e322.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ROBIN-Robust-and-Invisible-Watermarks-for-Diffusion-Models-with-Adversarial-Optimization"><a href="#ROBIN-Robust-and-Invisible-Watermarks-for-Diffusion-Models-with-Adversarial-Optimization" class="headerlink" title="ROBIN: Robust and Invisible Watermarks for Diffusion Models with   Adversarial Optimization"></a>ROBIN: Robust and Invisible Watermarks for Diffusion Models with   Adversarial Optimization</h2><p><strong>Authors:Huayang Huang, Yu Wu, Qian Wang</strong></p>
<p>Watermarking generative content serves as a vital tool for authentication, ownership protection, and mitigation of potential misuse. Existing watermarking methods face the challenge of balancing robustness and concealment. They empirically inject a watermark that is both invisible and robust and passively achieve concealment by limiting the strength of the watermark, thus reducing the robustness. In this paper, we propose to explicitly introduce a watermark hiding process to actively achieve concealment, thus allowing the embedding of stronger watermarks. To be specific, we implant a robust watermark in an intermediate diffusion state and then guide the model to hide the watermark in the final generated image. We employ an adversarial optimization algorithm to produce the optimal hiding prompt guiding signal for each watermark. The prompt embedding is optimized to minimize artifacts in the generated image, while the watermark is optimized to achieve maximum strength. The watermark can be verified by reversing the generation process. Experiments on various diffusion models demonstrate the watermark remains verifiable even under significant image tampering and shows superior invisibility compared to other state-of-the-art robust watermarking methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Hannah1102/ROBIN">https://github.com/Hannah1102/ROBIN</a>. </p>
<blockquote>
<p>æ°´å°ä½œä¸ºç”Ÿæˆå†…å®¹è®¤è¯ã€ç‰ˆæƒä¿æŠ¤ä»¥åŠæ½œåœ¨è¯¯ç”¨ç¼“è§£çš„é‡è¦å·¥å…·ï¼Œç°æœ‰çš„æ°´å°æ–¹æ³•é¢ä¸´å¹³è¡¡ç¨³å¥æ€§å’Œéšè”½æ€§çš„æŒ‘æˆ˜ã€‚ä»–ä»¬é€šè¿‡ç»éªŒæ³¨å…¥æ—¢ä¸å¯è§åˆç¨³å¥çš„æ°´å°ï¼Œå¹¶é€šè¿‡é™åˆ¶æ°´å°å¼ºåº¦è¢«åŠ¨åœ°å®ç°éšè”½æ€§ï¼Œä»è€Œé™ä½äº†ç¨³å¥æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æè®®æ˜ç¡®åœ°å¼•å…¥ä¸€ä¸ªæ°´å°éšè—è¿‡ç¨‹æ¥ä¸»åŠ¨å®ç°éšè”½æ€§ï¼Œä»è€Œå…è®¸åµŒå…¥æ›´å¼ºçš„æ°´å°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨ä¸­é—´æ‰©æ•£çŠ¶æ€æ¤å…¥ç¨³å¥çš„æ°´å°ï¼Œç„¶åå¼•å¯¼æ¨¡å‹å°†æ°´å°éšè—åœ¨æœ€ç»ˆç”Ÿæˆçš„å›¾åƒä¸­ã€‚æˆ‘ä»¬é‡‡ç”¨å¯¹æŠ—ä¼˜åŒ–ç®—æ³•ä¸ºæ¯ä¸ªæ°´å°ç”Ÿæˆæœ€ä½³éšè—æç¤ºå¼•å¯¼ä¿¡å·ã€‚æç¤ºåµŒå…¥çš„ä¼˜åŒ–æ—¨åœ¨å‡å°‘ç”Ÿæˆå›¾åƒä¸­çš„ä¼ªå½±ï¼Œè€Œæ°´å°çš„ä¼˜åŒ–åˆ™ä¸ºäº†å®ç°æœ€å¤§å¼ºåº¦ã€‚é€šè¿‡åè½¬ç”Ÿæˆè¿‡ç¨‹å¯ä»¥éªŒè¯æ°´å°ã€‚åœ¨å¤šç§æ‰©æ•£æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨å›¾åƒé­åˆ°é‡å¤§ç¯¡æ”¹çš„æƒ…å†µä¸‹ï¼Œæ°´å°ä»ç„¶å¯éªŒè¯ï¼Œå¹¶ä¸”ä¸å…¶ä»–æœ€å…ˆè¿›çš„ç¨³å¥æ°´å°æ–¹æ³•ç›¸æ¯”å…·æœ‰å‡ºè‰²çš„éšè”½æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Hannah1102/ROBIN%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Hannah1102/ROBINæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.03862v2">PDF</a> Accept to NeurIPS 2024</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç”Ÿæˆå†…å®¹çš„æ°´å°æŠ€æœ¯å¯¹äºéªŒè¯ã€ç‰ˆæƒä¿æŠ¤å’Œæ½œåœ¨è¯¯ç”¨çš„ç¼“è§£è‡³å…³é‡è¦ã€‚å½“å‰çš„æ°´å°æ–¹æ³•é¢ä¸´å¹³è¡¡ç¨³å¥æ€§å’ŒéšåŒ¿æ€§çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡é€šè¿‡ä¸»åŠ¨å¼•å…¥æ°´å°éšè—è¿‡ç¨‹ï¼Œå…è®¸åµŒå…¥æ›´å¼ºå¤§çš„æ°´å°ï¼Œè§£å†³äº†ä»¥å¾€é€šè¿‡é™åˆ¶æ°´å°å¼ºåº¦å®ç°éšåŒ¿çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨ä¸­é—´æ‰©æ•£çŠ¶æ€æ¤å…¥ç¨³å¥æ°´å°ï¼Œç„¶åå¼•å¯¼æ¨¡å‹å°†æ°´å°éšè—åœ¨æœ€ç»ˆç”Ÿæˆçš„å›¾åƒä¸­ã€‚æˆ‘ä»¬ä½¿ç”¨å¯¹æŠ—ä¼˜åŒ–ç®—æ³•ä¸ºæ¯ä¸ªæ°´å°ç”Ÿæˆæœ€ä½³éšè—æç¤ºå¼•å¯¼ä¿¡å·ã€‚æç¤ºåµŒå…¥çš„ä¼˜åŒ–æ—¨åœ¨å‡å°‘ç”Ÿæˆå›¾åƒä¸­çš„ä¼ªå½±ï¼Œè€Œæ°´å°çš„ä¼˜åŒ–åˆ™æ—¨åœ¨å®ç°æœ€å¤§å¼ºåº¦ã€‚é€šè¿‡åè½¬ç”Ÿæˆè¿‡ç¨‹å¯ä»¥éªŒè¯æ°´å°ã€‚åœ¨å„ç§æ‰©æ•£æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨é‡å¤§å›¾åƒç¯¡æ”¹ä¸‹ï¼Œè¯¥æ°´å°ä»ç„¶å¯éªŒè¯ï¼Œä¸”ç›¸æ¯”å…¶ä»–å…ˆè¿›çš„æ°´å°æ–¹æ³•å…·æœ‰æ›´å¥½çš„éšè”½æ€§ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/Hannah1102/ROBIN%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Hannah1102/ROBINè·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ°´å°åœ¨ç”Ÿæˆå†…å®¹ä¸­ç”¨äºéªŒè¯ã€ç‰ˆæƒä¿æŠ¤å’Œæ½œåœ¨è¯¯ç”¨ç¼“è§£è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ°´å°æ–¹æ³•é¢ä¸´ç¨³å¥æ€§å’ŒéšåŒ¿æ€§ä¹‹é—´çš„å¹³è¡¡æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡ä¸»åŠ¨å¼•å…¥æ°´å°éšè—è¿‡ç¨‹è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œå…è®¸åµŒå…¥æ›´å¼ºå¥çš„æ°´å°ã€‚</li>
<li>åœ¨ä¸­é—´æ‰©æ•£çŠ¶æ€æ¤å…¥æ°´å°ï¼Œç„¶åå¼•å¯¼æ¨¡å‹éšè—æ°´å°åœ¨æœ€ç»ˆç”Ÿæˆçš„å›¾åƒä¸­ã€‚</li>
<li>ä½¿ç”¨å¯¹æŠ—ä¼˜åŒ–ç®—æ³•ä¸ºæ¯ä¸ªæ°´å°ç”Ÿæˆæœ€ä½³çš„éšè—æç¤ºå¼•å¯¼ä¿¡å·ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–æç¤ºåµŒå…¥å’Œæ°´å°å¼ºåº¦ï¼Œå®ç°äº†åœ¨ç”Ÿæˆå›¾åƒä¸­å‡å°‘ä¼ªå½±å’Œæœ€å¤§åŒ–æ°´å°å¼ºåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.03862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a3e2062f86d1ee2a58c7737f4c9eb2df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-410ec5396ffdba29741c2ce7719489fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-760c04ba132e227c3881ed367d33e47d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SaRA-High-Efficient-Diffusion-Model-Fine-tuning-with-Progressive-Sparse-Low-Rank-Adaptation"><a href="#SaRA-High-Efficient-Diffusion-Model-Fine-tuning-with-Progressive-Sparse-Low-Rank-Adaptation" class="headerlink" title="SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse   Low-Rank Adaptation"></a>SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse   Low-Rank Adaptation</h2><p><strong>Authors:Teng Hu, Jiangning Zhang, Ran Yi, Hongrui Huang, Yabiao Wang, Lizhuang Ma</strong></p>
<p>In recent years, the development of diffusion models has led to significant progress in image and video generation tasks, with pre-trained models like the Stable Diffusion series playing a crucial role. Inspired by model pruning which lightens large pre-trained models by removing unimportant parameters, we propose a novel model fine-tuning method to make full use of these ineffective parameters and enable the pre-trained model with new task-specified capabilities. In this work, we first investigate the importance of parameters in pre-trained diffusion models, and discover that the smallest 10% to 20% of parameters by absolute values do not contribute to the generation process. Based on this observation, we propose a method termed SaRA that re-utilizes these temporarily ineffective parameters, equating to optimizing a sparse weight matrix to learn the task-specific knowledge. To mitigate overfitting, we propose a nuclear-norm-based low-rank sparse training scheme for efficient fine-tuning. Furthermore, we design a new progressive parameter adjustment strategy to make full use of the re-trained&#x2F;finetuned parameters. Finally, we propose a novel unstructural backpropagation strategy, which significantly reduces memory costs during fine-tuning. Our method enhances the generative capabilities of pre-trained models in downstream applications and outperforms traditional fine-tuning methods like LoRA in maintaining modelâ€™s generalization ability. We validate our approach through fine-tuning experiments on SD models, demonstrating significant improvements. SaRA also offers a practical advantage that requires only a single line of code modification for efficient implementation and is seamlessly compatible with existing methods. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹çš„å‘å±•åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œé¢„è®­ç»ƒæ¨¡å‹å¦‚Stable Diffusionç³»åˆ—èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚å—æ¨¡å‹ä¿®å‰ªçš„å¯å‘ï¼Œå³é€šè¿‡ç§»é™¤ä¸é‡è¦å‚æ•°æ¥å‡è½»å¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„è´Ÿæ‹…ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¨¡å‹å¾®è°ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå……åˆ†åˆ©ç”¨è¿™äº›æ— æ•ˆå‚æ•°ï¼Œå¹¶ä¸ºé¢„è®­ç»ƒæ¨¡å‹èµ‹äºˆæ–°çš„ä»»åŠ¡ç‰¹å®šåŠŸèƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆç ”ç©¶äº†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­å‚æ•°çš„é‡è¦æ€§ï¼Œå¹¶å‘ç°ç»å¯¹å€¼æœ€å°çš„10%è‡³20%çš„å‚æ•°å¯¹ç”Ÿæˆè¿‡ç¨‹æ²¡æœ‰è´¡çŒ®ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºSaRAçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é‡æ–°åˆ©ç”¨è¿™äº›æš‚æ—¶æ— æ•ˆçš„å‚æ•°ï¼Œç›¸å½“äºä¼˜åŒ–ç¨€ç–æƒé‡çŸ©é˜µæ¥å­¦ä¹ ä»»åŠ¡ç‰¹å®šçŸ¥è¯†ã€‚ä¸ºäº†å‡å°‘è¿‡æ‹Ÿåˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ ¸èŒƒæ•°çš„ä½ç§©ç¨€ç–è®­ç»ƒæ–¹æ¡ˆï¼Œä»¥å®ç°é«˜æ•ˆçš„å¾®è°ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°çš„æ¸è¿›å‚æ•°è°ƒæ•´ç­–ç•¥ï¼Œä»¥å……åˆ†åˆ©ç”¨é‡æ–°è®­ç»ƒ&#x2F;å¾®è°ƒçš„å‚æ•°ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„éç»“æ„åå‘ä¼ æ’­ç­–ç•¥ï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ˜¾è‘—å‡å°‘äº†å†…å­˜æˆæœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†é¢„è®­ç»ƒæ¨¡å‹åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨ä¿æŒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ï¼Œå¦‚LoRAã€‚æˆ‘ä»¬é€šè¿‡SDæ¨¡å‹çš„å¾®è°ƒå®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†æ˜¾è‘—çš„æ”¹è¿›ã€‚SaRAè¿˜æä¾›äº†ä¸€ä¸ªå®é™…ä¼˜åŠ¿ï¼Œå³åªéœ€ä¸€è¡Œä»£ç ä¿®æ”¹å³å¯å®ç°é«˜æ•ˆå®æ–½ï¼Œå¹¶ä¸”ä¸ç°æœ‰æ–¹æ³•æ— ç¼å…¼å®¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.06633v2">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œå¹¶æ¢è®¨äº†å¦‚ä½•å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥æé«˜å…¶æ€§èƒ½ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºSaRAçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ä¸­æš‚æ—¶ä¸èµ·ä½œç”¨çš„å‚æ•°è¿›è¡Œå¾®è°ƒï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­çš„ç”Ÿæˆèƒ½åŠ›ã€‚é€šè¿‡æ ¸èŒƒæ•°ä½ç§©ç¨€ç–è®­ç»ƒæ–¹æ¡ˆå’Œæ¸è¿›å‚æ•°è°ƒæ•´ç­–ç•¥ï¼ŒSaRAæ–¹æ³•åœ¨ä¿æŒæ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶ï¼Œä¼˜åŒ–äº†æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°å‹çš„éç»“æ„åŒ–åå‘ä¼ æ’­ç­–ç•¥ï¼Œæ˜¾è‘—é™ä½äº†å¾®è°ƒè¿‡ç¨‹ä¸­çš„å†…å­˜æˆæœ¬ã€‚å®éªŒè¯æ˜ï¼ŒSaRAæ–¹æ³•æ˜¾è‘—æé«˜äº†é¢„è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œé¢„è®­ç»ƒæ¨¡å‹å¦‚Stable Diffusionç³»åˆ—èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„æ¨¡å‹å¾®è°ƒæ–¹æ³•SaRAï¼Œèƒ½å¤Ÿåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ä¸­æš‚æ—¶ä¸èµ·ä½œç”¨çš„å‚æ•°ã€‚</li>
<li>SaRAæ–¹æ³•é€šè¿‡ä¼˜åŒ–ç¨€ç–æƒé‡çŸ©é˜µæ¥å­¦ä¹ ç‰¹å®šä»»åŠ¡çš„çŸ¥è¯†ï¼Œå¹¶é€šè¿‡æ ¸èŒƒæ•°ä½ç§©ç¨€ç–è®­ç»ƒæ–¹æ¡ˆè¿›è¡Œé«˜æ•ˆå¾®è°ƒã€‚</li>
<li>æ¸è¿›å‚æ•°è°ƒæ•´ç­–ç•¥å’Œæ–°å‹éç»“æ„åŒ–åå‘ä¼ æ’­ç­–ç•¥è¢«è®¾è®¡æ¥å……åˆ†åˆ©ç”¨é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒåçš„å‚æ•°ï¼Œå¹¶é™ä½å†…å­˜æˆæœ¬ã€‚</li>
<li>SaRAæ–¹æ³•æé«˜äº†é¢„è®­ç»ƒæ¨¡å‹åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶ä¼˜äºä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•å¦‚LoRAã€‚</li>
<li>SaRAæ–¹æ³•å…·æœ‰å®ç”¨æ€§ä¼˜åŠ¿ï¼Œåªéœ€å¯¹ä»£ç è¿›è¡Œä¸€è¡Œä¿®æ”¹å³å¯å®ç°é«˜æ•ˆå®æ–½ï¼Œå¹¶ä¸ç°æœ‰æ–¹æ³•æ— ç¼å…¼å®¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.06633">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-87fc4dd6a30074c3e60f9feabe710051.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05edac631d6de0d88bfbfad24dc08ea5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0fd1989187f1ecdd4b4c8c4e31b8e6a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b08135e65645ec87017c4061f1057c8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="DreamScape-3D-Scene-Creation-via-Gaussian-Splatting-joint-Correlation-Modeling"><a href="#DreamScape-3D-Scene-Creation-via-Gaussian-Splatting-joint-Correlation-Modeling" class="headerlink" title="DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation   Modeling"></a>DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation   Modeling</h2><p><strong>Authors:Yueming Zhao, Xuening Yuan, Hongyu Yang, Di Huang</strong></p>
<p>Recent advances in text-to-3D creation integrate the potent prior of Diffusion Models from text-to-image generation into 3D domain. Nevertheless, generating 3D scenes with multiple objects remains challenging. Therefore, we present DreamScape, a method for generating 3D scenes from text. Utilizing Gaussian Splatting for 3D representation, DreamScape introduces 3D Gaussian Guide that encodes semantic primitives, spatial transformations and relationships from text using LLMs, enabling local-to-global optimization. Progressive scale control is tailored during local object generation, addressing training instability issue arising from simple blending in the global optimization stage. Collision relationships between objects are modeled at the global level to mitigate biases in LLMs priors, ensuring physical correctness. Additionally, to generate pervasive objects like rain and snow distributed extensively across the scene, we design specialized sparse initialization and densification strategy. Experiments demonstrate that DreamScape achieves state-of-the-art performance, enabling high-fidelity, controllable 3D scene generation. </p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬åˆ°3Dåˆ›ä½œçš„è¿›å±•å°†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„Diffusion Modelsçš„å¼ºå¤§å…ˆéªŒçŸ¥è¯†æ•´åˆåˆ°3Dé¢†åŸŸã€‚ç„¶è€Œï¼Œåœ¨3Dåœºæ™¯ä¸­ç”Ÿæˆå¤šä¸ªå¯¹è±¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†DreamScapeæ–¹æ³•ï¼Œä¸€ç§ä»æ–‡æœ¬ç”Ÿæˆ3Dåœºæ™¯çš„æ–¹æ³•ã€‚åˆ©ç”¨é«˜æ–¯è´´å›¾è¿›è¡Œ3Dè¡¨ç¤ºï¼ŒDreamScapeå¼•å…¥äº†3Dé«˜æ–¯æŒ‡å—ï¼Œè¯¥æŒ‡å—ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»æ–‡æœ¬ç¼–ç è¯­ä¹‰åŸå§‹ä¿¡æ¯ã€ç©ºé—´å˜æ¢å’Œå…³ç³»ï¼Œå®ç°å±€éƒ¨åˆ°å…¨å±€çš„ä¼˜åŒ–ã€‚å±€éƒ¨å¯¹è±¡ç”Ÿæˆè¿‡ç¨‹ä¸­è¿›è¡Œäº†æ¸è¿›çš„å°ºåº¦æ§åˆ¶ï¼Œè§£å†³äº†å…¨å±€ä¼˜åŒ–é˜¶æ®µç®€å•æ··åˆå¼•èµ·çš„è®­ç»ƒä¸ç¨³å®šé—®é¢˜ã€‚åœ¨å…¨å±€å±‚é¢å»ºç«‹äº†ç‰©ä½“é—´çš„ç¢°æ’å…³ç³»ï¼Œä»¥å‡è½»LLMså…ˆéªŒä¸­çš„åè§ï¼Œç¡®ä¿ç‰©ç†æ­£ç¡®æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç”Ÿæˆå¦‚é›¨é›ªç­‰å¹¿æ³›åˆ†å¸ƒåœ¨åœºæ™¯ä¸­çš„æ™®éç‰©ä½“ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸“é—¨çš„ç¨€ç–åˆå§‹åŒ–å’Œå¯†é›†åŒ–ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼ŒDreamScapeè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿå®ç°é«˜ä¿çœŸã€å¯æ§çš„3Dåœºæ™¯ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.09227v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºDiffusion Modelsçš„æœ€æ–°æŠ€æœ¯ï¼Œå°†æ–‡æœ¬è½¬åŒ–ä¸ºä¸‰ç»´åœºæ™¯ã€‚é€šè¿‡å¼•å…¥Gaussian Splattingè¿›è¡Œä¸‰ç»´è¡¨ç¤ºå’Œ3D Gaussian GuideæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿä»æ–‡æœ¬ä¸­ç”Ÿæˆä¸‰ç»´åœºæ™¯ï¼Œå¹¶å¤„ç†å¤šç‰©ä½“ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚DreamScapeæ–¹æ³•å®ç°äº†å±€éƒ¨åˆ°å…¨å±€çš„ä¼˜åŒ–ï¼Œé€šè¿‡æ¸è¿›å¼å°ºåº¦æ§åˆ¶å’Œç¢°æ’å…³ç³»å»ºæ¨¡ç­‰æŠ€æœ¯ï¼Œè§£å†³äº†è®­ç»ƒä¸ç¨³å®šå’Œç‰©ä½“é—´ç‰©ç†æ­£ç¡®æ€§çš„é—®é¢˜ã€‚å®éªŒè¯æ˜ï¼ŒDreamScapeåœ¨ç”Ÿæˆé«˜ä¿çœŸã€å¯æ§çš„ä¸‰ç»´åœºæ™¯æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DreamScapeå°†Diffusion Modelsä»æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ½œåŠ›åº”ç”¨äºä¸‰ç»´é¢†åŸŸã€‚</li>
<li>åˆ©ç”¨Gaussian Splattingè¿›è¡Œä¸‰ç»´è¡¨ç¤ºã€‚</li>
<li>å¼•å…¥3D Gaussian Guideï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»æ–‡æœ¬ä¸­ç¼–ç è¯­ä¹‰åŸå§‹ä¿¡æ¯ã€ç©ºé—´å˜æ¢å’Œå…³ç³»ã€‚</li>
<li>å®ç°å±€éƒ¨åˆ°å…¨å±€çš„ä¼˜åŒ–ï¼ŒåŒ…æ‹¬æ¸è¿›å¼å°ºåº¦æ§åˆ¶å’Œç‰©ä½“é—´ç¢°æ’å…³ç³»çš„å»ºæ¨¡ã€‚</li>
<li>é’ˆå¯¹å…¨å±€ä¼˜åŒ–é˜¶æ®µå‡ºç°çš„è®­ç»ƒä¸ç¨³å®šé—®é¢˜ï¼Œé‡‡å–äº†æœ‰æ•ˆæªæ–½ã€‚</li>
<li>å¼€å‘äº†ä¸“é—¨çš„æŠ€æœ¯æ¥ç”Ÿæˆåœºæ™¯ä¸­çš„æ™®éç‰©ä½“ï¼Œå¦‚é›¨å’Œé›ªã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.09227">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7bb46a44e19fde605abc7d5e84986cdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14f7d4c4563a1443997b4a4a5f9b4f40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-640e099afc274f23153a99cded40dd25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2dc2dd2244e82a220d5f9fe9e00a813a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb4f731f2b0d30d20b6c3cc50828430c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-05/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-05/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-87db798d15356e64a92dde81090c2fa9.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-05  STING-BEE Towards Vision-Language Model for Real-World X-ray Baggage   Security Inspection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-05/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e15f469bbc5c7833fc0865375a2de227.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-05  MultiNeRF Multiple Watermark Embedding for Neural Radiance Fields
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19939k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
