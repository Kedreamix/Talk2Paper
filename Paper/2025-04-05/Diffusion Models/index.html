<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-04-05  Concept Lancet Image Editing with Compositional Representation   Transplant">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b00cced02d5d259e181a498f1dffd934.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    15.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    64 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-05-更新"><a href="#2025-04-05-更新" class="headerlink" title="2025-04-05 更新"></a>2025-04-05 更新</h1><h2 id="Concept-Lancet-Image-Editing-with-Compositional-Representation-Transplant"><a href="#Concept-Lancet-Image-Editing-with-Compositional-Representation-Transplant" class="headerlink" title="Concept Lancet: Image Editing with Compositional Representation   Transplant"></a>Concept Lancet: Image Editing with Compositional Representation   Transplant</h2><p><strong>Authors:Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, Hancheng Min, Chris Callison-Burch, René Vidal</strong></p>
<p>Diffusion models are widely used for image editing tasks. Existing editing methods often design a representation manipulation procedure by curating an edit direction in the text embedding or score space. However, such a procedure faces a key challenge: overestimating the edit strength harms visual consistency while underestimating it fails the editing task. Notably, each source image may require a different editing strength, and it is costly to search for an appropriate strength via trial-and-error. To address this challenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play framework for principled representation manipulation in diffusion-based image editing. At inference time, we decompose the source input in the latent (text embedding or diffusion score) space as a sparse linear combination of the representations of the collected visual concepts. This allows us to accurately estimate the presence of concepts in each image, which informs the edit. Based on the editing task (replace&#x2F;add&#x2F;remove), we perform a customized concept transplant process to impose the corresponding editing direction. To sufficiently model the concept space, we curate a conceptual representation dataset, CoLan-150K, which contains diverse descriptions and scenarios of visual terms and phrases for the latent dictionary. Experiments on multiple diffusion-based image editing baselines show that methods equipped with CoLan achieve state-of-the-art performance in editing effectiveness and consistency preservation. </p>
<blockquote>
<p>扩散模型广泛应用于图像编辑任务。现有的编辑方法通常通过策划文本嵌入或分数空间中的编辑方向来设计一个表示操纵过程。然而，这一过程面临一个关键挑战：高估编辑强度会损害视觉一致性，而低估则无法完成编辑任务。值得注意的是，每个源图像可能需要不同的编辑强度，通过试错搜索适当的强度成本很高。为了解决这一挑战，我们提出了Concept Lancet（CoLan），这是一种用于扩散式图像编辑中有原则表示操作的零样本即插即用框架。在推理时间，我们将源输入在潜在（文本嵌入或扩散分数）空间中分解为所收集的视觉概念表示的稀疏线性组合。这允许我们准确估计每个图像中概念的存在，从而为编辑提供信息。基于编辑任务（替换&#x2F;添加&#x2F;删除），我们执行定制的概念移植过程以强制执行相应的编辑方向。为了充分建模概念空间，我们整理了一个概念表示数据集CoLan-150K，其中包含视觉术语和短语的多样描述和场景，用于潜在字典。在多个基于扩散的图像编辑基准实验上表明，配备CoLan的方法在编辑效果和一致性保持方面达到最新技术水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02828v1">PDF</a> Accepted in CVPR 2025. Project page at   <a target="_blank" rel="noopener" href="https://peterljq.github.io/project/colan">https://peterljq.github.io/project/colan</a></p>
<p><strong>Summary</strong><br>     扩散模型广泛应用于图像编辑任务。现有编辑方法通过在文本嵌入或分数空间策划编辑方向来设计表示操纵程序，但面临估算编辑强度挑战：过度估计会损害视觉一致性，而低估则无法完成编辑任务。为此，我们提出Concept Lancet（CoLan）框架，用于扩散式图像编辑中的原则性表示操纵。在推理时，我们将源输入在潜在（文本嵌入或扩散分数）空间中分解为收集的视觉概念的表示的稀疏线性组合，准确估计图像中概念的存在，为编辑提供信息。基于编辑任务（替换&#x2F;添加&#x2F;删除），我们执行定制的概念移植过程以施加相应的编辑方向。通过构建概念表示数据集CoLan-150K，包含视觉术语和短语的多样描述和场景作为潜在字典，实验表明配备CoLan的方法在编辑效果和一致性保持方面达到最新技术水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型常用于图像编辑任务。</li>
<li>现有编辑方法设计表示操纵程序时面临估算编辑强度的挑战。</li>
<li>Concept Lancet（CoLan）框架用于解决这一挑战，通过分解源输入在潜在空间中的表示来准确估计图像中的概念。</li>
<li>CoLan基于编辑任务执行定制的概念移植过程。</li>
<li>CoLan通过构建CoLan-150K数据集来充分建模概念空间。</li>
<li>配备CoLan的实验方法在多个扩散式图像编辑基准测试中达到最新技术水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02828">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7caf3c5518e1646ef2c4ebfc563c26c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-744340da7c6f3cdd26998a6a6fd4b242.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff5f6835fc6753e643a9ba7ca680df8b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b2c353d9fcc0026a5780aaf1bc23f3f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d8fa2e19ca4ac30f1348ea0727f0506.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f8bb99afea72e61787dad90e8b8af977.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="F-ViTA-Foundation-Model-Guided-Visible-to-Thermal-Translation"><a href="#F-ViTA-Foundation-Model-Guided-Visible-to-Thermal-Translation" class="headerlink" title="F-ViTA: Foundation Model Guided Visible to Thermal Translation"></a>F-ViTA: Foundation Model Guided Visible to Thermal Translation</h2><p><strong>Authors:Jay N. Paranjape, Celso de Melo, Vishal M. Patel</strong></p>
<p>Thermal imaging is crucial for scene understanding, particularly in low-light and nighttime conditions. However, collecting large thermal datasets is costly and labor-intensive due to the specialized equipment required for infrared image capture. To address this challenge, researchers have explored visible-to-thermal image translation. Most existing methods rely on Generative Adversarial Networks (GANs) or Diffusion Models (DMs), treating the task as a style transfer problem. As a result, these approaches attempt to learn both the modality distribution shift and underlying physical principles from limited training data. In this paper, we propose F-ViTA, a novel approach that leverages the general world knowledge embedded in foundation models to guide the diffusion process for improved translation. Specifically, we condition an InstructPix2Pix Diffusion Model with zero-shot masks and labels from foundation models such as SAM and Grounded DINO. This allows the model to learn meaningful correlations between scene objects and their thermal signatures in infrared imagery. Extensive experiments on five public datasets demonstrate that F-ViTA outperforms state-of-the-art (SOTA) methods. Furthermore, our model generalizes well to out-of-distribution (OOD) scenarios and can generate Long-Wave Infrared (LWIR), Mid-Wave Infrared (MWIR), and Near-Infrared (NIR) translations from the same visible image. Code: <a target="_blank" rel="noopener" href="https://github.com/JayParanjape/F-ViTA/tree/master">https://github.com/JayParanjape/F-ViTA/tree/master</a>. </p>
<blockquote>
<p>热成像对于场景理解至关重要，特别是在低光和夜间条件下。然而，由于红外图像捕获所需的专业设备，收集大量的热数据集成本高昂且劳动密集。为了应对这一挑战，研究人员已经探索了可见光到热成像的图像翻译技术。现有的大多数方法依赖于生成对抗网络（GANs）或扩散模型（DMs），将任务视为风格转换问题。因此，这些方法试图从有限的训练数据中学习模态分布变化和潜在的物理原理。在本文中，我们提出了一种新方法F-ViTA，它利用基础模型中嵌入的世界通用知识来指导扩散过程，以改进翻译效果。具体来说，我们使用InstructPix2Pix扩散模型，以基础模型（如SAM和Grounded DINO）的零样本遮罩和标签作为条件。这允许模型学习场景物体与其在红外图像中的热特征之间的有意义关联。在五个公共数据集上的广泛实验表明，F-ViTA优于最新技术方法。此外，我们的模型能够很好地泛化到域外场景，并且可以从同一可见图像生成长波红外（LWIR）、中波红外（MWIR）和近红外（NIR）翻译。代码：<a target="_blank" rel="noopener" href="https://github.com/JayParanjape/F-ViTA/tree/master%E3%80%82">https://github.com/JayParanjape/F-ViTA/tree/master。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02801v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本文提出一种利用基础模型中的通用世界知识来指导扩散过程的新型可见光到热成像翻译方法——F-ViTA。通过结合InstructPix2Pix扩散模型和基础模型（如SAM和Grounded DINO）的零样本掩码和标签，F-ViTA能在场景物体与它们在红外成像中的热特征之间建立有意义的关联。在五个公共数据集上的实验表明，F-ViTA优于现有技术，并具有良好的泛化能力，能从同一可见图像生成长波红外、中波红外和近红外翻译。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>可见光到热成像翻译是解决热成像数据收集成本高昂和劳动密集的有效方法。</li>
<li>F-ViTA利用基础模型中的通用世界知识来指导扩散过程，实现更好的翻译效果。</li>
<li>F-ViTA结合了InstructPix2Pix扩散模型和基础模型的零样本掩码和标签。</li>
<li>F-ViTA能在场景物体与热特征之间建立有意义的关联。</li>
<li>F-ViTA在多个公共数据集上的实验表现优于现有技术。</li>
<li>F-ViTA具有良好的泛化能力，能生成不同红外波段的翻译。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02801">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d58058993689ae2148ae227f856269b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81de8b10a9991239b5966a1002c5eb0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b00cced02d5d259e181a498f1dffd934.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7d5dd5192adb708a977f34013859e18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e92e2b8fd1ed6a5281e477cc6a4bcbb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e83781afdfdffe32dda10dac22e4bc25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fa901a8f38e24f08ce9eeccb045c51a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09bb4ad23917336ee5eb9f023ae32fad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c303caa2dea22db222113799c968f99.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Scene-Splatter-Momentum-3D-Scene-Generation-from-Single-Image-with-Video-Diffusion-Model"><a href="#Scene-Splatter-Momentum-3D-Scene-Generation-from-Single-Image-with-Video-Diffusion-Model" class="headerlink" title="Scene Splatter: Momentum 3D Scene Generation from Single Image with   Video Diffusion Model"></a>Scene Splatter: Momentum 3D Scene Generation from Single Image with   Video Diffusion Model</h2><p><strong>Authors:Shengjun Zhang, Jinzhao Li, Xin Fei, Hao Liu, Yueqi Duan</strong></p>
<p>In this paper, we propose Scene Splatter, a momentum-based paradigm for video diffusion to generate generic scenes from single image. Existing methods, which employ video generation models to synthesize novel views, suffer from limited video length and scene inconsistency, leading to artifacts and distortions during further reconstruction. To address this issue, we construct noisy samples from original features as momentum to enhance video details and maintain scene consistency. However, for latent features with the perception field that spans both known and unknown regions, such latent-level momentum restricts the generative ability of video diffusion in unknown regions. Therefore, we further introduce the aforementioned consistent video as a pixel-level momentum to a directly generated video without momentum for better recovery of unseen regions. Our cascaded momentum enables video diffusion models to generate both high-fidelity and consistent novel views. We further finetune the global Gaussian representations with enhanced frames and render new frames for momentum update in the next step. In this manner, we can iteratively recover a 3D scene, avoiding the limitation of video length. Extensive experiments demonstrate the generalization capability and superior performance of our method in high-fidelity and consistent scene generation. </p>
<blockquote>
<p>本文提出了Scene Splatter，这是一种基于动量的视频扩散范式，用于从单幅图像生成通用场景。现有方法采用视频生成模型来合成新视角，存在视频长度有限和场景不一致的问题，导致在进一步重建时出现伪影和失真。为了解决这一问题，我们从原始特征中构建噪声样本作为动量，以增强视频细节并保持场景一致性。然而，对于感知场同时覆盖已知和未知区域的潜在特征，这种潜在层次的动量限制了视频扩散在未知区域的生成能力。因此，我们进一步引入了上述一致视频作为无动量的直接生成视频的像素级动量，以更好地恢复未见区域。我们的级联动量使视频扩散模型能够生成高保真且一致的全新视角。我们进一步使用增强帧微调全局高斯表示，并在下一步渲染新帧以进行动量更新。通过这种方式，我们可以迭代地恢复3D场景，避免视频长度的限制。大量实验表明，我们的方法在高保真和一致场景生成方面具有泛化能力和优越性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02764v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong><br>     本文提出了Scene Splatter方法，这是一种基于动量的视频扩散范式，能够从单幅图像生成通用场景。现有方法使用视频生成模型合成新视图时，存在视频长度有限和场景不一致的问题，导致进一步重建时出现伪影和失真。为解决这一问题，本文通过原始特征构建噪声样本作为动量，增强视频细节并保持场景一致性。然而，对于同时包含已知和未知区域的感知场域中的潜在特征，这种潜在层面的动量限制了视频扩散在未知区域的生成能力。因此，本文进一步引入了上述一致视频作为无动量的直接生成视频的像素级动量，以更好地恢复未见区域。级联动量使视频扩散模型能够生成高保真且一致的全新视图。通过微调全局高斯表示和增强帧进行渲染，为下一步更新动量提供新帧。通过这种方式，可以迭代恢复3D场景，避免视频长度的限制。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Scene Splatter是一种基于动量的视频扩散方法，能够从单幅图像生成通用场景。</li>
<li>现有视频生成方法存在视频长度有限和场景不一致的问题。</li>
<li>通过构建噪声样本作为动量，Scene Splatter能增强视频细节并保持场景一致性。</li>
<li>潜在层面的动量在已知和未知区域的感知场域中限制了视频扩散的生成能力。</li>
<li>Scene Splatter引入了像素级动量来恢复未见区域，并生成高保真且一致的全新视图。</li>
<li>通过微调全局高斯表示和增强帧进行渲染，可以迭代恢复3D场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02764">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b94e211c633f7628d84afeccef8b7ece.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-867943fcb69504c3b63f87115e08cd8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de8140c6f9fc0fd225df494923195ecb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2999f4e3eb11418481eea9860ad29b45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-987326529c3d782296ea053341f5ae40.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MD-ProjTex-Texturing-3D-Shapes-with-Multi-Diffusion-Projection"><a href="#MD-ProjTex-Texturing-3D-Shapes-with-Multi-Diffusion-Projection" class="headerlink" title="MD-ProjTex: Texturing 3D Shapes with Multi-Diffusion Projection"></a>MD-ProjTex: Texturing 3D Shapes with Multi-Diffusion Projection</h2><p><strong>Authors:Ahmet Burak Yildirim, Mustafa Utku Aydogdu, Duygu Ceylan, Aysegul Dundar</strong></p>
<p>We introduce MD-ProjTex, a method for fast and consistent text-guided texture generation for 3D shapes using pretrained text-to-image diffusion models. At the core of our approach is a multi-view consistency mechanism in UV space, which ensures coherent textures across different viewpoints. Specifically, MD-ProjTex fuses noise predictions from multiple views at each diffusion step and jointly updates the per-view denoising directions to maintain 3D consistency. In contrast to existing state-of-the-art methods that rely on optimization or sequential view synthesis, MD-ProjTex is computationally more efficient and achieves better quantitative and qualitative results. </p>
<blockquote>
<p>我们介绍了MD-ProjTex方法，这是一种利用预训练的文本到图像扩散模型进行快速且一致的文本引导纹理生成的3D形状处理方法。我们的方法的核心在于UV空间中的多视角一致性机制，它能确保不同视角之间的纹理一致性。具体来说，MD-ProjTex在每个扩散步骤融合多个视角的噪声预测，并联合更新每个视角的去噪方向，以保持3D一致性。与现有的最先进方法相比，这些方法依赖于优化或顺序视图合成，MD-ProjTex计算效率更高，并且在数量和质量上都能实现更好的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02762v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>我们推出了MD-ProjTex方法，它利用预训练的文本到图像扩散模型，实现了快速且一致的文本引导纹理生成技术。该方法的核心在于UV空间的多视角一致性机制，确保不同视角的纹理连贯性。MD-ProjTex融合了多个视角的噪声预测，并在扩散过程中联合更新每个视角的去噪方向，以保持三维一致性。相较于依赖优化或顺序视角合成的现有方法，MD-ProjTex计算效率更高，且取得了更好的定量和定性结果。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>MD-ProjTex是一种基于预训练的文本到图像扩散模型的快速且一致的文本引导纹理生成方法。</li>
<li>该方法采用多视角一致性机制，确保不同视角的纹理连贯性。</li>
<li>MD-ProjTex融合了多个视角的噪声预测，并在扩散过程中更新去噪方向。</li>
<li>MD-ProjTex相比现有方法，计算效率更高。</li>
<li>MD-ProjTex在定量和定性评估中都取得了更好的结果。</li>
<li>该方法的核心贡献在于利用文本指导生成与文本内容相匹配的3D形状纹理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02762">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-58c29ece383b0ff9831f39b9141f81c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8f8168a4de9b2efa62b6151302fa382.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ba2ea73c5f780ff6e839d2ab97b22d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8052627d531a5ca90d21c5111ec29c16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70dd7c4ccc5bd21e768a1605ea1c4e23.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Bridging-the-Gap-between-Gaussian-Diffusion-Models-and-Universal-Quantization-for-Image-Compression"><a href="#Bridging-the-Gap-between-Gaussian-Diffusion-Models-and-Universal-Quantization-for-Image-Compression" class="headerlink" title="Bridging the Gap between Gaussian Diffusion Models and Universal   Quantization for Image Compression"></a>Bridging the Gap between Gaussian Diffusion Models and Universal   Quantization for Image Compression</h2><p><strong>Authors:Lucas Relic, Roberto Azevedo, Yang Zhang, Markus Gross, Christopher Schroers</strong></p>
<p>Generative neural image compression supports data representation at extremely low bitrate, synthesizing details at the client and consistently producing highly realistic images. By leveraging the similarities between quantization error and additive noise, diffusion-based generative image compression codecs can be built using a latent diffusion model to “denoise” the artifacts introduced by quantization. However, we identify three critical gaps in previous approaches following this paradigm (namely, the noise level, noise type, and discretization gaps) that result in the quantized data falling out of the data distribution known by the diffusion model. In this work, we propose a novel quantization-based forward diffusion process with theoretical foundations that tackles all three aforementioned gaps. We achieve this through universal quantization with a carefully tailored quantization schedule and a diffusion model trained with uniform noise. Compared to previous work, our proposal produces consistently realistic and detailed reconstructions, even at very low bitrates. In such a regime, we achieve the best rate-distortion-realism performance, outperforming previous related works. </p>
<blockquote>
<p>生成式神经网络压缩支持以极低的比特率进行数据表示，可在客户端合成细节，并始终生成高度逼真的图像。通过利用量化误差和添加性噪声之间的相似性，可以使用潜在扩散模型构建基于扩散的生成式图像压缩编解码器，以“去噪”量化引起的伪影。然而，我们发现了遵循此模式（即噪声水平、噪声类型和离散化差距）的先前方法中的三个关键差距，导致量化数据脱离扩散模型所知道的数据分布。在本研究中，我们提出了一种基于量化理论基础的向前扩散过程新方案，解决了上述所有三个差距。我们通过使用通用量化以及精心定制的量化时间表，以及使用均匀噪声训练的扩散模型来实现这一点。与之前的工作相比，我们的方案即使在非常低的比特率下也能始终产生逼真的细节丰富的重建。在这种模式下，我们实现了最佳的速率失真逼真度性能，超越了以前的相关作品。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02579v1">PDF</a> To appear at CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于扩散模型的生成式图像压缩技术。该技术能够在极低码率下支持数据表示，合成细节，并始终产生高度逼真的图像。文章指出先前的方法在噪声水平、噪声类型和离散化方面存在差距，导致量化数据脱离扩散模型所知的数据分布。为此，本文提出了一种基于量化的前向扩散过程，通过通用量化和精心定制的量化时间表以及使用均匀噪声训练的扩散模型，解决了上述问题。与以前的工作相比，该方案即使在码率极低的情况下也能始终产生逼真且详细的重建，实现了最佳的率失真现实性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成式神经图像压缩利用扩散模型在极低码率下实现数据表示，能合成细节并生成高度逼真的图像。</li>
<li>指出先前方法在噪声水平、类型和离散化方面的差距，导致量化数据与扩散模型所知的数据分布不匹配。</li>
<li>提出一种基于量化的前向扩散过程，具有理论根基，解决了上述差距。</li>
<li>使用通用量化和精心定制的量化时间表。</li>
<li>扩散模型采用均匀噪声训练。</li>
<li>该方案在极低码率下仍能产生逼真且详细的重建。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02579">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-463d19d90e361175b448fbeeb4c0d691.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edb285be6d943f67e625ef0d2d08902d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a5efc155736f3d2a3965ee65d0eb7a7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="OmniCam-Unified-Multimodal-Video-Generation-via-Camera-Control"><a href="#OmniCam-Unified-Multimodal-Video-Generation-via-Camera-Control" class="headerlink" title="OmniCam: Unified Multimodal Video Generation via Camera Control"></a>OmniCam: Unified Multimodal Video Generation via Camera Control</h2><p><strong>Authors:Xiaoda Yang, Jiayang Xu, Kaixuan Luan, Xinyu Zhan, Hongshun Qiu, Shijun Shi, Hao Li, Shuai Yang, Li Zhang, Checheng Yu, Cewu Lu, Lixin Yang</strong></p>
<p>Camera control, which achieves diverse visual effects by changing camera position and pose, has attracted widespread attention. However, existing methods face challenges such as complex interaction and limited control capabilities. To address these issues, we present OmniCam, a unified multimodal camera control framework. Leveraging large language models and video diffusion models, OmniCam generates spatio-temporally consistent videos. It supports various combinations of input modalities: the user can provide text or video with expected trajectory as camera path guidance, and image or video as content reference, enabling precise control over camera motion. To facilitate the training of OmniCam, we introduce the OmniTr dataset, which contains a large collection of high-quality long-sequence trajectories, videos, and corresponding descriptions. Experimental results demonstrate that our model achieves state-of-the-art performance in high-quality camera-controlled video generation across various metrics. </p>
<blockquote>
<p>摄像机控制通过改变摄像机的位置和姿态来实现多种视觉效果，已经引起了广泛关注。然而，现有方法面临交互复杂和控制能力有限的挑战。为了解决这些问题，我们提出了OmniCam，一个统一的多模式摄像机控制框架。OmniCam利用大型语言模型和视频扩散模型，生成时空一致的视频。它支持各种输入模式的组合：用户可以提供文本或视频作为预期的轨迹作为摄像机路径指导，以及图像或视频作为内容参考，实现对摄像机运动的精确控制。为了促进OmniCam的训练，我们引入了OmniTr数据集，它包含大量高质量的长序列轨迹、视频和相应描述。实验结果表明，我们的模型在各种指标上实现了高质量的摄像机控制视频生成的前沿性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02312v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为OmniCam的多模态相机控制框架，它利用大型语言模型和视频扩散模型，通过改变相机位置和姿态实现多样化的视觉效果。OmniCam支持各种输入模态的组合，用户可以通过文本或视频提供预期的轨迹作为相机路径指导，图像或视频作为内容参考，实现对相机运动的精确控制。为训练OmniCam，引入了OmniTr数据集，包含大量高质量的长序列轨迹、视频和相应描述。实验结果表明，该模型在高质量相机控制视频生成方面实现了先进性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OmniCam是一个多模态相机控制框架，利用大型语言模型和视频扩散模型。</li>
<li>OmniCam可以通过改变相机位置和姿态实现多样化的视觉效果。</li>
<li>OmniCam支持多种输入模态组合，包括文本或视频轨迹作为相机路径指导，图像或视频作为内容参考。</li>
<li>OmniCam的引入解决了现有相机控制方法面临的复杂交互和有限控制能力的挑战。</li>
<li>为训练OmniCam，引入了OmniTr数据集，包含大量高质量的长序列轨迹、视频和相应描述。</li>
<li>实验结果表明，OmniCam在高质量相机控制视频生成方面达到了先进性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02312">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fa125b58d77a025089a4bd4fcf073776.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3af8f2576760ab57e20f0d52d3d0811f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fc108b9094582084d596c39d04da770.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd69b8550d20604321ead714191ae032.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Random-Conditioning-with-Distillation-for-Data-Efficient-Diffusion-Model-Compression"><a href="#Random-Conditioning-with-Distillation-for-Data-Efficient-Diffusion-Model-Compression" class="headerlink" title="Random Conditioning with Distillation for Data-Efficient Diffusion Model   Compression"></a>Random Conditioning with Distillation for Data-Efficient Diffusion Model   Compression</h2><p><strong>Authors:Dohyun Kim, Sehwan Park, Geonhee Han, Seung Wook Kim, Paul Hongsuck Seo</strong></p>
<p>Diffusion models generate high-quality images through progressive denoising but are computationally intensive due to large model sizes and repeated sampling. Knowledge distillation, which transfers knowledge from a complex teacher to a simpler student model, has been widely studied in recognition tasks, particularly for transferring concepts unseen during student training. However, its application to diffusion models remains underexplored, especially in enabling student models to generate concepts not covered by the training images. In this work, we propose Random Conditioning, a novel approach that pairs noised images with randomly selected text conditions to enable efficient, image-free knowledge distillation. By leveraging this technique, we show that the student can generate concepts unseen in the training images. When applied to conditional diffusion model distillation, our method allows the student to explore the condition space without generating condition-specific images, resulting in notable improvements in both generation quality and efficiency. This promotes resource-efficient deployment of generative diffusion models, broadening their accessibility for both research and real-world applications. Code, models, and datasets are available at <a target="_blank" rel="noopener" href="https://dohyun-as.github.io/Random-Conditioning">https://dohyun-as.github.io/Random-Conditioning</a> . </p>
<blockquote>
<p>扩散模型通过逐步去噪生成高质量图像，但由于模型体积庞大和重复采样，计算量较大。知识蒸馏是一种将知识从复杂的教师模型转移到简单的学模型的技术，在识别任务中已被广泛应用，特别是在学生训练期间未见的概念的转移方面。然而，其在扩散模型中的应用仍然被探索不足，特别是在使学生模型生成未涵盖在训练图像中的概念方面。在这项工作中，我们提出了随机条件化方法，这是一种将噪声图像与随机选择的文本条件配对的新方法，以实现高效的无图像知识蒸馏。通过利用这项技术，我们证明了学生模型可以生成在训练图像中未见的概念。当应用于条件扩散模型蒸馏时，我们的方法允许学生在不生成特定条件图像的情况下探索条件空间，导致生成质量和效率的显著提高。这促进了生成扩散模型的资源高效部署，扩大了其在研究和实际应用中的可及性。代码、模型和数据集可通过<a target="_blank" rel="noopener" href="https://dohyun-as.github.io/Random-Conditioning%E8%8E%B7%E5%8F%96%E3%80%82">https://dohyun-as.github.io/Random-Conditioning获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02011v1">PDF</a> Accepted to CVPR 2025. 8 pages main paper + 4 pages references + 5   pages supplementary, 9 figures in total</p>
<p><strong>Summary</strong></p>
<p>扩散模型通过逐步去噪生成高质量图像，但由于模型规模庞大且涉及重复采样，计算量大。知识蒸馏在识别任务中广泛应用，可从复杂的教师模型向简单的模型转移知识，特别是在学生训练期间未见的概念转移方面。然而，其在扩散模型中的应用仍被较少探索，特别是在使学生模型生成训练图像未涵盖的概念方面。本研究提出一种名为Random Conditioning的新方法，它通过随机选择的文本条件配对噪声图像，实现了高效、无需图像的知识蒸馏。利用此技术，学生模型能够在训练图像中生成未见的概念。在应用于条件扩散模型蒸馏时，该方法使学生能够在无需生成特定条件图像的情况下探索条件空间，从而在生成质量和效率方面取得显著改善。这促进了生成扩散模型的资源高效部署，扩大了其在研究和实际应用中的可及性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型通过逐步去噪生成高质量图像，但计算量大。</li>
<li>知识蒸馏在识别任务中广泛应用，但在扩散模型中的应用仍被较少探索。</li>
<li>Random Conditioning方法通过随机选择的文本条件配对噪声图像，实现了高效、无需图像的知识蒸馏。</li>
<li>学生模型能够在训练图像中生成未见的概念。</li>
<li>该方法提高了生成质量和效率。</li>
<li>该方法促进了生成扩散模型的资源高效部署。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02011">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-840c853a3cc8714d04900ef508507eb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-009a7d8ed86139a9bb3a70defa99815e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc152ec1571a03b161a1816a50b94c58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f174d6abac710557281a75b9bcc27b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85fe979e433a4dc3775f5ef4084ee372.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8bcde627e2daffe1a79aed09e143738.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="OccludeNeRF-Geometric-aware-3D-Scene-Inpainting-with-Collaborative-Score-Distillation-in-NeRF"><a href="#OccludeNeRF-Geometric-aware-3D-Scene-Inpainting-with-Collaborative-Score-Distillation-in-NeRF" class="headerlink" title="OccludeNeRF: Geometric-aware 3D Scene Inpainting with Collaborative   Score Distillation in NeRF"></a>OccludeNeRF: Geometric-aware 3D Scene Inpainting with Collaborative   Score Distillation in NeRF</h2><p><strong>Authors:Jingyu Shi, Achleshwar Luthra, Jiazhi Li, Xiang Gao, Xiyun Song, Zongfang Lin, David Gu, Heather Yu</strong></p>
<p>With Neural Radiance Fields (NeRFs) arising as a powerful 3D representation, research has investigated its various downstream tasks, including inpainting NeRFs with 2D images. Despite successful efforts addressing the view consistency and geometry quality, prior methods yet suffer from occlusion in NeRF inpainting tasks, where 2D prior is severely limited in forming a faithful reconstruction of the scene to inpaint.   To address this, we propose a novel approach that enables cross-view information sharing during knowledge distillation from a diffusion model, effectively propagating occluded information across limited views. Additionally, to align the distillation direction across multiple sampled views, we apply a grid-based denoising strategy and incorporate additional rendered views to enhance cross-view consistency. To assess our approach’s capability of handling occlusion cases, we construct a dataset consisting of challenging scenes with severe occlusion, in addition to existing datasets. Compared with baseline methods, our method demonstrates better performance in cross-view consistency and faithfulness in reconstruction, while preserving high rendering quality and fidelity. </p>
<blockquote>
<p>随着神经辐射场（NeRFs）作为一种强大的3D表示方法的发展，研究已经探索了其各种下游任务，包括使用2D图像填充NeRFs。尽管已经出现了针对视图一致性和几何质量提升的成功尝试，先前的方法在NeRF填充任务中仍受到遮挡问题的困扰，其中由于二维先验的局限性，无法准确重建场景以进行填充。为了解决这一问题，我们提出了一种新方法，通过扩散模型的知识蒸馏过程实现跨视图信息共享，有效地传播了有限的视图中的遮挡信息。此外，为了对齐多个采样视图中的蒸馏方向，我们采用基于网格的去噪策略并融入额外的渲染视图以增强跨视图一致性。为了评估我们的方法在处理遮挡案例方面的能力，除了现有的数据集外，我们还构建了一个包含具有严重遮挡的挑战性场景的数据集。与基准方法相比，我们的方法在跨视图一致性、重建的忠实性和保持高质量渲染方面表现出了更好的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02007v1">PDF</a> CVPR 2025 CV4Metaverse</p>
<p><strong>Summary</strong></p>
<p>基于NeRF（神经辐射场）的强大3D表示能力，现有研究对其下游任务进行了深入探索，包括利用2D图像进行NeRF补全。尽管现有方法在保持视图一致性和几何质量方面取得了显著进展，但在NeRF补全任务中仍面临遮挡问题，2D先验信息在重建待补全场景时难以形成忠实表达。为解决这一问题，我们提出了一种新方法，利用扩散模型的知识蒸馏过程实现跨视图信息共享，有效传播跨有限视图的遮挡信息。此外，我们还采用基于网格的去噪策略，并通过添加额外渲染视图来对齐多个采样视图的蒸馏方向，以提高跨视图一致性。通过构建包含严重遮挡挑战场景的数据集，并与其他基准方法进行比较，我们的方法在跨视图一致性、重建忠实度、渲染质量和保真度方面表现出更佳的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRF作为一种强大的3D表示方法，在下游任务如NeRF补全中受到关注。</li>
<li>现有方法在解决NeRF补全任务中的遮挡问题时存在局限，2D先验信息难以形成对场景的忠实重建。</li>
<li>提出了一种利用扩散模型知识蒸馏的跨视图信息共享新方法，以解决遮挡问题。</li>
<li>通过基于网格的去噪策略和额外渲染视图，提高跨视图一致性。</li>
<li>构建了一个包含具有严重遮挡挑战场景的数据集，以评估方法性能。</li>
<li>与基准方法相比，该方法在跨视图一致性、重建忠实度、渲染质量和保真度方面表现更佳。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02007">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-46f4f4ff08fe22007f2ab856133bcf55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6cfbb90b0daf0363c0726abf49ae20a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07334afa6fe040341a95e853e3952925.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-18bb89cb2d91e4e8eed69b7146fc466a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ILLUME-Illuminating-Unified-MLLM-with-Dual-Visual-Tokenization-and-Diffusion-Refinement"><a href="#ILLUME-Illuminating-Unified-MLLM-with-Dual-Visual-Tokenization-and-Diffusion-Refinement" class="headerlink" title="ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and   Diffusion Refinement"></a>ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and   Diffusion Refinement</h2><p><strong>Authors:Runhui Huang, Chunwei Wang, Junwei Yang, Guansong Lu, Yunlong Yuan, Jianhua Han, Lu Hou, Wei Zhang, Lanqing Hong, Hengshuang Zhao, Hang Xu</strong></p>
<p>We present ILLUME+ that leverages dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities in a unified model: understanding, generation, and editing. Models like Chameleon and EMU3 utilize VQGAN for image discretization, due to the lack of deep semantic interaction, they lag behind specialist models like LLaVA in visual understanding tasks. To mitigate this, LaViT and ILLUME employ semantic encoders for tokenization, but they struggle with image editing due to poor texture preservation. Meanwhile, Janus series decouples the input and output image representation, limiting their abilities to seamlessly handle interleaved image-text understanding and generation. In contrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which preserves both fine-grained textures and text-aligned semantics while enabling a coarse-to-fine image representation strategy for multimodal understanding and generation. Additionally, we employ a diffusion model as the image detokenizer for enhanced generation quality and efficient super-resolution. ILLUME+ follows a continuous-input, discrete-output scheme within the unified MLLM and adopts a progressive training procedure that supports dynamic resolution across the vision tokenizer, MLLM, and diffusion decoder. This design allows for flexible and efficient context-aware image editing and generation across diverse tasks. ILLUME+ (3B) exhibits competitive performance against existing unified MLLMs and specialized models across multimodal understanding, generation, and editing benchmarks. With its strong performance, ILLUME+ provides a scalable and versatile foundation for future multimodal applications. Project Page: <a target="_blank" rel="noopener" href="https://illume-unified-mllm.github.io/">https://illume-unified-mllm.github.io/</a>. </p>
<blockquote>
<p>我们提出了ILLUME+，它利用双重视觉标记化和扩散解码器来提高深度语义理解和高保真图像生成能力。现有的统一模型在统一模型中同时处理三种基本能力（理解、生成和编辑）时遇到了困难。诸如变色龙和EMU3之类的模型使用VQGAN进行图像离散化，由于缺乏深度语义交互，它们在视觉理解任务上落后于LLaVA等专用模型。为了缓解这一问题，LaViT和ILLUME采用语义编码器进行标记化，但它们在进行图像编辑时由于纹理保存不佳而遇到困难。与此同时，Janus系列将输入和输出图像表示分离，限制了它们无缝处理交错式图像文本理解和生成的能力。相比之下，ILLUME+引入了一个统一的双重视觉标记器DualViTok，它既能保留精细纹理又能保持文本对齐语义，同时实现粗细结合的图像表示策略，用于多模态理解和生成。此外，我们采用扩散模型作为图像解标记器，以提高生成质量和高效超分辨率。ILLUME+在统一MLLM内采用连续输入、离散输出的方案，并采用渐进式训练程序，支持视觉标记器、MLLM和扩散解码器之间的动态分辨率调整。这种设计使得跨不同任务的灵活和高效上下文感知图像编辑和生成成为可能。ILLUME+（3B）在多模态理解、生成和编辑基准测试中表现出与现有统一MLLM和专用模型相竞争的性能。凭借强大的性能，ILLUME+为未来多模态应用提供了可扩展和多功能的基础。项目页面：<a target="_blank" rel="noopener" href="https://illume-unified-mllm.github.io/">https://illume-unified-mllm.github.io/</a>.</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01934v2">PDF</a> </p>
<p><strong>摘要</strong><br>    ILUME+通过利用双重视觉标记化和扩散解码器，改进了深度语义理解和高保真图像生成。现有统一模型在三项基本能力上难以兼顾：理解、生成和编辑。ILLUME+引入统一双重视觉标记器DualViTok，保留精细纹理和文本对齐语义，采用由粗到细图像表示策略进行多模态理解和生成。此外，采用扩散模型作为图像解标记器，提高生成质量和超分辨率效率。ILLUME+采用连续输入、离散输出的统一MLLM方案，采用渐进式训练程序，支持视觉标记器、MLLM和扩散解码器之间的动态分辨率。这为跨不同任务的上下文感知图像编辑和生成提供了灵活和高效的方法。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>ILLUME+结合了双重视觉标记化和扩散解码器，提升了深度语义理解和高保真图像生成。</li>
<li>现有模型如Chameleon和EMU3在视觉理解任务上落后于专家模型，如LLaVA。</li>
<li>LaViT和ILLUME使用语义编码器进行标记化，但在图像编辑方面因纹理保留不足而遇到困难。</li>
<li>Janus系列将输入和输出图像表示解耦，限制了他们处理交织的图像-文本理解和生成的能力。</li>
<li>ILLUME+的DualViTok保留了精细纹理和文本对齐语义，并实现了粗到细的图像表示策略。</li>
<li>采用扩散模型作为图像解标记器，提高了生成质量和超分辨率效率。</li>
<li>ILLUME+的渐进式训练程序支持动态分辨率调整，为跨任务上下文感知图像编辑和生成提供了灵活性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01934">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-309a8e660f4a6803bbcbdb1b2cbe128c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc92edbce4f88d4f781481b545d1475d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6b0e79eb6b7dd27324f19e04b9430c4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Distilling-Multi-view-Diffusion-Models-into-3D-Generators"><a href="#Distilling-Multi-view-Diffusion-Models-into-3D-Generators" class="headerlink" title="Distilling Multi-view Diffusion Models into 3D Generators"></a>Distilling Multi-view Diffusion Models into 3D Generators</h2><p><strong>Authors:Hao Qin, Luyuan Chen, Ming Kong, Mengxu Lu, Qiang Zhu</strong></p>
<p>We introduce DD3G, a formulation that Distills a multi-view Diffusion model (MV-DM) into a 3D Generator using gaussian splatting. DD3G compresses and integrates extensive visual and spatial geometric knowledge from the MV-DM by simulating its ordinary differential equation (ODE) trajectory, ensuring the distilled generator generalizes better than those trained solely on 3D data. Unlike previous amortized optimization approaches, we align the MV-DM and 3D generator representation spaces to transfer the teacher’s probabilistic flow to the student, thus avoiding inconsistencies in optimization objectives caused by probabilistic sampling. The introduction of probabilistic flow and the coupling of various attributes in 3D Gaussians introduce challenges in the generation process. To tackle this, we propose PEPD, a generator consisting of Pattern Extraction and Progressive Decoding phases, which enables efficient fusion of probabilistic flow and converts a single image into 3D Gaussians within 0.06 seconds. Furthermore, to reduce knowledge loss and overcome sparse-view supervision, we design a joint optimization objective that ensures the quality of generated samples through explicit supervision and implicit verification. Leveraging existing 2D generation models, we compile 120k high-quality RGBA images for distillation. Experiments on synthetic and public datasets demonstrate the effectiveness of our method. Our project is available at: <a target="_blank" rel="noopener" href="https://qinbaigao.github.io/DD3G_project/">https://qinbaigao.github.io/DD3G_project/</a> </p>
<blockquote>
<p>我们介绍了DD3G，这是一种通过高斯拼贴法将多视角扩散模型（MV-DM）蒸馏到3D生成器的方法。DD3G通过模拟MV-DM的常微分方程（ODE）轨迹，压缩并集成了广泛的视觉和空间几何知识，确保蒸馏出的生成器比仅基于3D数据训练的生成器具有更好的泛化性能。与前期的摊销优化方法不同，我们将MV-DM和3D生成器的表示空间对齐，以将教师的概率流传输给学生，从而避免由概率采样引起的优化目标不一致。概率流的引入和3D高斯中各种属性的耦合给生成过程带来了挑战。为此，我们提出了PEPD生成器，包括模式提取和渐进解码两个阶段，能够实现概率流的有效融合，并在0.06秒内将单幅图像转换为3D高斯。此外，为了减少知识损失并克服稀疏视图监督，我们设计了一个联合优化目标，通过明确监督和隐式验证确保生成样本的质量。我们利用现有的2D生成模型，编译了12万张高质量RGBA图像用于蒸馏。在合成和公开数据集上的实验证明了我们的方法的有效性。我们的项目可在以下网址找到：<a target="_blank" rel="noopener" href="https://qinbaigao.github.io/DD3G_project/">https://qinbaigao.github.io/DD3G_project/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00457v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>DD3G是一个将多视图扩散模型（MV-DM）蒸馏到3D生成器的模型。它通过模拟MV-DM的常微分方程（ODE）轨迹，压缩并整合视觉和几何空间知识。DD3G采用概率流对齐的方式，避免优化目标的不一致性。为应对生成过程中的挑战，引入PEPD生成器，实现概率流的融合和高效转换。通过联合优化目标，减少知识损失并克服稀疏监督问题。实验证明该方法在合成和公开数据集上的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DD3G是一个多视图扩散模型的蒸馏版本，被蒸馏成一个3D生成器。</li>
<li>通过模拟MV-DM的ODE轨迹，DD3G整合了视觉和几何空间知识。</li>
<li>采用概率流对齐方式，确保学生模型能够学习教师的概率流，避免了优化目标的不一致性。</li>
<li>PEPD生成器的引入解决了生成过程中的挑战，实现了概率流的融合和高效转换。</li>
<li>通过联合优化目标，减少知识损失并克服稀疏监督问题，确保了生成样本的质量。</li>
<li>该方法在合成和公开数据集上的实验证明了其有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00457">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1ce28162b577c4f3fe5dba65a99d4f1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd5ced50961b209c9ff6ef2cd5c0663e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7b487ff167a29735997ac5d402d1d2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6eb6f5bc327dbaba705f3566712c6fc2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b572b2194b81b3883cafe56abae3366.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Flow-to-the-Mode-Mode-Seeking-Diffusion-Autoencoders-for-State-of-the-Art-Image-Tokenization"><a href="#Flow-to-the-Mode-Mode-Seeking-Diffusion-Autoencoders-for-State-of-the-Art-Image-Tokenization" class="headerlink" title="Flow to the Mode: Mode-Seeking Diffusion Autoencoders for   State-of-the-Art Image Tokenization"></a>Flow to the Mode: Mode-Seeking Diffusion Autoencoders for   State-of-the-Art Image Tokenization</h2><p><strong>Authors:Kyle Sargent, Kyle Hsu, Justin Johnson, Li Fei-Fei, Jiajun Wu</strong></p>
<p>Since the advent of popular visual generation frameworks like VQGAN and latent diffusion models, state-of-the-art image generation systems have generally been two-stage systems that first tokenize or compress visual data into a lower-dimensional latent space before learning a generative model. Tokenizer training typically follows a standard recipe in which images are compressed and reconstructed subject to a combination of MSE, perceptual, and adversarial losses. Diffusion autoencoders have been proposed in prior work as a way to learn end-to-end perceptually-oriented image compression, but have not yet shown state-of-the-art performance on the competitive task of ImageNet-1K reconstruction. We propose FlowMo, a transformer-based diffusion autoencoder that achieves a new state-of-the-art for image tokenization at multiple compression rates without using convolutions, adversarial losses, spatially-aligned two-dimensional latent codes, or distilling from other tokenizers. Our key insight is that FlowMo training should be broken into a mode-matching pre-training stage and a mode-seeking post-training stage. In addition, we conduct extensive analyses and explore the training of generative models atop the FlowMo tokenizer. Our code and models will be available at <a target="_blank" rel="noopener" href="http://kylesargent.github.io/flowmo">http://kylesargent.github.io/flowmo</a> . </p>
<blockquote>
<p>自从出现了VQGAN和潜在扩散模型等流行的视觉生成框架以来，最先进的图像生成系统通常是一个两阶段系统，它首先会将视觉数据标记化或压缩到低维潜在空间，然后学习生成模型。标记器训练通常遵循一个标准流程，图像被压缩和重建，同时受到均方误差、感知和对抗损失的制约。先前的工作已经提出了扩散自编码器作为学习端到端感知导向的图像压缩的一种方法，但在ImageNet-1K重建的竞赛任务中尚未表现出最佳性能。我们提出了FlowMo，这是一个基于转换器的扩散自编码器，它实现了图像标记化的新最佳性能，支持多种压缩率，无需使用卷积、对抗性损失、空间对齐的二维潜在代码或其他标记器进行提炼。我们的关键见解是FlowMo训练应该分为模式匹配的预训练阶段和模式搜索的后训练阶段。此外，我们还进行了广泛的分析和探索了基于FlowMo标记器的生成模型的训练。我们的代码和模型将在<a target="_blank" rel="noopener" href="http://kylesargent.github.io/flowmo%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">http://kylesargent.github.io/flowmo上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11056v2">PDF</a> 18 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>基于VQGAN和潜在扩散模型等流行视觉生成框架的出现，目前先进的图像生成系统通常采用两阶段系统，第一阶段将视觉数据令牌化或压缩到低维潜在空间，然后学习生成模型。本文提出FlowMo，一种基于转换器的扩散自动编码器，无需卷积、对抗性损失、空间对齐的二维潜在代码或蒸馏，即可在多个压缩率下实现图像令牌化的新最先进的性能。关键见解是FlowMo训练应分为模式匹配的预训练阶段和模式搜索的后训练阶段。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前图像生成系统多采用两阶段系统，包括数据令牌化或压缩到潜在空间以及生成模型的学习。</li>
<li>FlowMo是一种新型的扩散自动编码器，能够在不使用卷积、对抗性损失等技术的情况下实现图像令牌化的先进性能。</li>
<li>FlowMo训练分为模式匹配的预训练阶段和模式搜索的后训练阶段。</li>
<li>FlowMo在多个压缩率下均表现出优秀的性能。</li>
<li>该研究提出了一种创新的图像压缩方法，旨在提高图像生成系统的效率和性能。</li>
<li>研究者还进行了广泛的实验和分析，探讨了基于FlowMo令牌器的生成模型的训练。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11056">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-74dc1eb48fa7317325f1867a561e2cbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11bda9be28ec5cde3a68f07295a96a1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09ca456d44611bd649a84c3b96bc3170.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca834b418c67b8f33077ab35a86c85de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-699a98f748a0a888ca5cf2f802825ca9.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GenDR-Lightning-Generative-Detail-Restorator"><a href="#GenDR-Lightning-Generative-Detail-Restorator" class="headerlink" title="GenDR: Lightning Generative Detail Restorator"></a>GenDR: Lightning Generative Detail Restorator</h2><p><strong>Authors:Yan Wang, Shijie Zhao, Kai Chen, Kexin Zhang, Junlin Li, Li Zhang</strong></p>
<p>Recent research applying text-to-image (T2I) diffusion models to real-world super-resolution (SR) has achieved remarkable success. However, fundamental misalignments between T2I and SR targets result in a dilemma between inference speed and detail fidelity. Specifically, T2I tasks prioritize multi-step inversion to synthesize coherent outputs aligned with textual prompts and shrink the latent space to reduce generating complexity. Contrariwise, SR tasks preserve most information from low-resolution input while solely restoring high-frequency details, thus necessitating sufficient latent space and fewer inference steps. To bridge the gap, we present a one-step diffusion model for generative detail restoration, GenDR, distilled from a tailored diffusion model with larger latent space. In detail, we train a new SD2.1-VAE16 (0.9B) via representation alignment to expand latent space without enlarging the model size. Regarding step-distillation, we propose consistent score identity distillation (CiD) that incorporates SR task-specific loss into score distillation to leverage more SR priors and align the training target. Furthermore, we extend CiD with adversarial learning and representation alignment (CiDA) to enhance perceptual quality and accelerate training. We also polish the pipeline to achieve a more efficient inference. Experimental results demonstrate that GenDR achieves state-of-the-art performance in both quantitative metrics and visual fidelity. </p>
<blockquote>
<p>近期将文本到图像（T2I）扩散模型应用于现实世界的超分辨率（SR）研究取得了显著的成功。然而，T2I和SR目标之间的基本不对齐导致了推理速度与细节保真度之间的困境。具体来说，T2I任务优先进行多步反转，以合成与文本提示对齐的输出，并缩小潜在空间以减少生成复杂性。相反，SR任务保留来自低分辨率输入的最多信息，同时仅恢复高频细节，因此需要足够的潜在空间和较少的推理步骤。为了弥补这一差距，我们提出了一种用于生成细节恢复的扩散模型（GenDR），它是从一个具有更大潜在空间的定制扩散模型中提炼出来的一步模型。具体来说，我们通过表示对齐训练了一个新的SD2.1-VAE16（0.9B）来扩展潜在空间，而无需扩大模型大小。关于步骤蒸馏，我们提出了基于一致评分身份蒸馏（CiD）的方法，该方法将SR任务特定损失纳入评分蒸馏中，以利用更多的SR先验知识并调整训练目标。此外，我们将对抗性学习与表示对齐扩展到CiDA，以提高感知质量和加速训练。我们还优化了管道，以实现更有效的推理。实验结果表明，GenDR在定量指标和视觉保真度方面都达到了最新技术水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06790v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本到图像（T2I）扩散模型在真实世界超分辨率（SR）应用上取得了显著成功，但T2I和SR目标之间的基本不匹配导致推理速度与细节保真度之间的困境。为弥补这一差距，提出了一种用于生成细节恢复的一次性扩散模型GenDR，它由一个具有更大潜在空间的定制扩散模型提炼而成。通过新的SD2.1-VAE16（0.9B）模型和一致分数身份蒸馏（CiD）等技术，提高感知质量并加速训练。GenDR在定量指标和视觉保真度上都达到了最新水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2I扩散模型在SR应用上表现卓越，但存在推理速度与细节保真度的困境。</li>
<li>GenDR是一个用于生成细节恢复的一次性扩散模型，解决了T2I和SR目标之间的不匹配问题。</li>
<li>GenDR通过扩大潜在空间来提高性能，采用新的SD2.1-VAE16（0.9B）模型。</li>
<li>一致分数身份蒸馏（CiD）技术被提出，结合SR任务特定损失进行分数蒸馏，利用更多SR先验并调整训练目标。</li>
<li>CiD与对抗性学习及表示对齐相结合，增强了感知质量和加速了训练。</li>
<li>GenDR的实验结果达到了在定量指标和视觉保真度上的最新水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06790">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d46cd61add6d808d59b0268b14c02369.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccc1509ae82a4324d6df7f3d0ba559c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e7cdc52b9e12d3a3570f51bdea1e91b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-737c27ccadd5bfc3b446fdccca4db200.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd02029d47a3f51126415e1d63bae6fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1486e38227fd0f76401aea444ceea6a7.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Bias-Free-Training-Paradigm-for-More-General-AI-generated-Image-Detection"><a href="#A-Bias-Free-Training-Paradigm-for-More-General-AI-generated-Image-Detection" class="headerlink" title="A Bias-Free Training Paradigm for More General AI-generated Image   Detection"></a>A Bias-Free Training Paradigm for More General AI-generated Image   Detection</h2><p><strong>Authors:Fabrizio Guillaro, Giada Zingarini, Ben Usman, Avneesh Sud, Davide Cozzolino, Luisa Verdoliva</strong></p>
<p>Successful forensic detectors can produce excellent results in supervised learning benchmarks but struggle to transfer to real-world applications. We believe this limitation is largely due to inadequate training data quality. While most research focuses on developing new algorithms, less attention is given to training data selection, despite evidence that performance can be strongly impacted by spurious correlations such as content, format, or resolution. A well-designed forensic detector should detect generator specific artifacts rather than reflect data biases. To this end, we propose B-Free, a bias-free training paradigm, where fake images are generated from real ones using the conditioning procedure of stable diffusion models. This ensures semantic alignment between real and fake images, allowing any differences to stem solely from the subtle artifacts introduced by AI generation. Through content-based augmentation, we show significant improvements in both generalization and robustness over state-of-the-art detectors and more calibrated results across 27 different generative models, including recent releases, like FLUX and Stable Diffusion 3.5. Our findings emphasize the importance of a careful dataset design, highlighting the need for further research on this topic. Code and data are publicly available at <a target="_blank" rel="noopener" href="https://grip-unina.github.io/B-Free/">https://grip-unina.github.io/B-Free/</a>. </p>
<blockquote>
<p>在监督学习基准测试中，成功的法医检测器可以产生出色的结果，但在转移到现实世界应用时却遇到了困难。我们认为这种局限性很大程度上是由于训练数据质量不足。虽然大多数研究都集中在开发新算法上，但对于数据选择方面关注的却很少。尽管有证据表明，性能可能会受到内容、格式或分辨率等虚假关联因素的强烈影响。一个设计良好的法医检测器应该检测生成器特定的伪影，而不是反映数据偏见。为此，我们提出了无偏见训练范式B-Free，在该范式中，使用稳定扩散模型的调节程序从真实图像生成虚假图像。这确保了真实和虚假图像之间的语义对齐，使得任何差异都仅源于人工智能生成所引入的微妙伪影。通过基于内容的增强方法，我们显示出与最新检测器相比在通用性和稳健性方面的显著改进，并且在包括最新发布的FLUX和Stable Diffusion 3.5等27种不同的生成模型上获得了更精确的结果。我们的研究强调了精心设计数据集的重要性，并突出了对这一主题进行进一步研究的必要性。相关代码和数据公开在<a target="_blank" rel="noopener" href="https://grip-unina.github.io/B-Free/%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://grip-unina.github.io/B-Free/上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17671v2">PDF</a> </p>
<p><strong>Summary</strong><br>     现有司法鉴定探测器在监督学习基准测试中表现优异，但在实际应用中却存在困难。这主要是因为训练数据质量不足。尽管大多数研究关注新算法的开发，但对训练数据选择的重要性却认识不足。数据中的虚假关联会影响性能。为此，我们提出了无偏训练范式B-Free，通过真实图像生成虚假图像，确保真实与虚假图像之间的语义对齐，仅通过AI生成引入细微差异。通过基于内容的增强技术，我们显示出相较于最新鉴定器在泛化和稳健性方面的显著改善，并在包括最新发布的FLUX和Stable Diffusion 3.5在内的27种生成模型上获得更准确的校准结果。我们强调了精心构建数据集的重要性，并突显对这方面的研究的必要。更多详细信息可通过<a target="_blank" rel="noopener" href="https://grip-unina.github.io/B-Free/%E8%AE%BF%E9%97%AE%E3%80%82">https://grip-unina.github.io/B-Free/访问。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前司法鉴定探测器面临从监督学习测试转移到实际应用时的挑战，主要原因是训练数据质量不足。</li>
<li>研究过于关注新算法开发，忽视了训练数据选择的重要性。</li>
<li>数据中的虚假关联会影响鉴定性能。</li>
<li>提出了一种名为B-Free的无偏训练范式，利用稳定扩散模型的条件程序从真实图像生成虚假图像。</li>
<li>该方法确保了真实和虚假图像之间的语义对齐，差异仅源于AI生成的细微特征。</li>
<li>通过基于内容的增强技术，在泛化和稳健性方面显示出显著改进，并在多种生成模型上获得更准确的校准结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17671">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8185169c50f3aceaa84ddc8e26ca0952.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-686712de054d1f801372b4c6e169c12e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9c23ebd8b942dae9af1a156ee57ba9ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed03d90d2e830ac7f9dd1997705c2190.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9865df032a57a1f38970eb275288791b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3043e432c6559ae0eee214f9098e322.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ROBIN-Robust-and-Invisible-Watermarks-for-Diffusion-Models-with-Adversarial-Optimization"><a href="#ROBIN-Robust-and-Invisible-Watermarks-for-Diffusion-Models-with-Adversarial-Optimization" class="headerlink" title="ROBIN: Robust and Invisible Watermarks for Diffusion Models with   Adversarial Optimization"></a>ROBIN: Robust and Invisible Watermarks for Diffusion Models with   Adversarial Optimization</h2><p><strong>Authors:Huayang Huang, Yu Wu, Qian Wang</strong></p>
<p>Watermarking generative content serves as a vital tool for authentication, ownership protection, and mitigation of potential misuse. Existing watermarking methods face the challenge of balancing robustness and concealment. They empirically inject a watermark that is both invisible and robust and passively achieve concealment by limiting the strength of the watermark, thus reducing the robustness. In this paper, we propose to explicitly introduce a watermark hiding process to actively achieve concealment, thus allowing the embedding of stronger watermarks. To be specific, we implant a robust watermark in an intermediate diffusion state and then guide the model to hide the watermark in the final generated image. We employ an adversarial optimization algorithm to produce the optimal hiding prompt guiding signal for each watermark. The prompt embedding is optimized to minimize artifacts in the generated image, while the watermark is optimized to achieve maximum strength. The watermark can be verified by reversing the generation process. Experiments on various diffusion models demonstrate the watermark remains verifiable even under significant image tampering and shows superior invisibility compared to other state-of-the-art robust watermarking methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Hannah1102/ROBIN">https://github.com/Hannah1102/ROBIN</a>. </p>
<blockquote>
<p>水印作为生成内容认证、版权保护以及潜在误用缓解的重要工具，现有的水印方法面临平衡稳健性和隐蔽性的挑战。他们通过经验注入既不可见又稳健的水印，并通过限制水印强度被动地实现隐蔽性，从而降低了稳健性。在本文中，我们提议明确地引入一个水印隐藏过程来主动实现隐蔽性，从而允许嵌入更强的水印。具体来说，我们在中间扩散状态植入稳健的水印，然后引导模型将水印隐藏在最终生成的图像中。我们采用对抗优化算法为每个水印生成最佳隐藏提示引导信号。提示嵌入的优化旨在减少生成图像中的伪影，而水印的优化则为了实现最大强度。通过反转生成过程可以验证水印。在多种扩散模型上的实验表明，即使在图像遭到重大篡改的情况下，水印仍然可验证，并且与其他最先进的稳健水印方法相比具有出色的隐蔽性。代码可在<a target="_blank" rel="noopener" href="https://github.com/Hannah1102/ROBIN%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Hannah1102/ROBIN找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.03862v2">PDF</a> Accept to NeurIPS 2024</p>
<p><strong>摘要</strong></p>
<p>生成内容的水印技术对于验证、版权保护和潜在误用的缓解至关重要。当前的水印方法面临平衡稳健性和隐匿性的挑战。本文通过主动引入水印隐藏过程，允许嵌入更强大的水印，解决了以往通过限制水印强度实现隐匿的问题。具体来说，我们在中间扩散状态植入稳健水印，然后引导模型将水印隐藏在最终生成的图像中。我们使用对抗优化算法为每个水印生成最佳隐藏提示引导信号。提示嵌入的优化旨在减少生成图像中的伪影，而水印的优化则旨在实现最大强度。通过反转生成过程可以验证水印。在各种扩散模型上的实验表明，即使在重大图像篡改下，该水印仍然可验证，且相比其他先进的水印方法具有更好的隐蔽性。代码可通过<a target="_blank" rel="noopener" href="https://github.com/Hannah1102/ROBIN%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Hannah1102/ROBIN获取。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>水印在生成内容中用于验证、版权保护和潜在误用缓解至关重要。</li>
<li>当前水印方法面临稳健性和隐匿性之间的平衡挑战。</li>
<li>本文通过主动引入水印隐藏过程解决了这个问题，允许嵌入更强健的水印。</li>
<li>在中间扩散状态植入水印，然后引导模型隐藏水印在最终生成的图像中。</li>
<li>使用对抗优化算法为每个水印生成最佳的隐藏提示引导信号。</li>
<li>该方法通过优化提示嵌入和水印强度，实现了在生成图像中减少伪影和最大化水印强度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.03862">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a3e2062f86d1ee2a58c7737f4c9eb2df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-410ec5396ffdba29741c2ce7719489fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-760c04ba132e227c3881ed367d33e47d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SaRA-High-Efficient-Diffusion-Model-Fine-tuning-with-Progressive-Sparse-Low-Rank-Adaptation"><a href="#SaRA-High-Efficient-Diffusion-Model-Fine-tuning-with-Progressive-Sparse-Low-Rank-Adaptation" class="headerlink" title="SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse   Low-Rank Adaptation"></a>SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse   Low-Rank Adaptation</h2><p><strong>Authors:Teng Hu, Jiangning Zhang, Ran Yi, Hongrui Huang, Yabiao Wang, Lizhuang Ma</strong></p>
<p>In recent years, the development of diffusion models has led to significant progress in image and video generation tasks, with pre-trained models like the Stable Diffusion series playing a crucial role. Inspired by model pruning which lightens large pre-trained models by removing unimportant parameters, we propose a novel model fine-tuning method to make full use of these ineffective parameters and enable the pre-trained model with new task-specified capabilities. In this work, we first investigate the importance of parameters in pre-trained diffusion models, and discover that the smallest 10% to 20% of parameters by absolute values do not contribute to the generation process. Based on this observation, we propose a method termed SaRA that re-utilizes these temporarily ineffective parameters, equating to optimizing a sparse weight matrix to learn the task-specific knowledge. To mitigate overfitting, we propose a nuclear-norm-based low-rank sparse training scheme for efficient fine-tuning. Furthermore, we design a new progressive parameter adjustment strategy to make full use of the re-trained&#x2F;finetuned parameters. Finally, we propose a novel unstructural backpropagation strategy, which significantly reduces memory costs during fine-tuning. Our method enhances the generative capabilities of pre-trained models in downstream applications and outperforms traditional fine-tuning methods like LoRA in maintaining model’s generalization ability. We validate our approach through fine-tuning experiments on SD models, demonstrating significant improvements. SaRA also offers a practical advantage that requires only a single line of code modification for efficient implementation and is seamlessly compatible with existing methods. </p>
<blockquote>
<p>近年来，扩散模型的发展在图像和视频生成任务中取得了显著进展，预训练模型如Stable Diffusion系列起到了关键作用。受模型修剪的启发，即通过移除不重要参数来减轻大型预训练模型的负担，我们提出了一种新型模型微调方法，该方法能够充分利用这些无效参数，并为预训练模型赋予新的任务特定功能。在这项工作中，我们首先研究了预训练扩散模型中参数的重要性，并发现绝对值最小的10%至20%的参数对生成过程没有贡献。基于此观察，我们提出了一种名为SaRA的方法，该方法重新利用这些暂时无效的参数，相当于优化稀疏权重矩阵来学习任务特定知识。为了减少过拟合，我们提出了一种基于核范数的低秩稀疏训练方案，以实现高效的微调。此外，我们设计了一种新的渐进参数调整策略，以充分利用重新训练&#x2F;微调的参数。最后，我们提出了一种新型的非结构反向传播策略，在微调过程中显著减少了内存成本。我们的方法提高了预训练模型在下游应用中的生成能力，并且在保持模型的泛化能力方面优于传统的微调方法，如LoRA。我们通过SD模型的微调实验验证了我们的方法，并展示了显著的改进。SaRA还提供了一个实际优势，即只需一行代码修改即可实现高效实施，并且与现有方法无缝兼容。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.06633v2">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>本文研究了扩散模型在图像和视频生成任务中的应用，并探讨了如何对预训练模型进行微调以提高其性能。文章提出了一种名为SaRA的新方法，该方法利用预训练模型中暂时不起作用的参数进行微调，从而提高模型在下游应用中的生成能力。通过核范数低秩稀疏训练方案和渐进参数调整策略，SaRA方法在保持模型泛化能力的同时，优化了模型的性能。此外，本文还提出了一种新型的非结构化反向传播策略，显著降低了微调过程中的内存成本。实验证明，SaRA方法显著提高了预训练模型的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在图像和视频生成任务中取得了显著进展，预训练模型如Stable Diffusion系列起到了关键作用。</li>
<li>提出了一种新型的模型微调方法SaRA，能够利用预训练模型中暂时不起作用的参数。</li>
<li>SaRA方法通过优化稀疏权重矩阵来学习特定任务的知识，并通过核范数低秩稀疏训练方案进行高效微调。</li>
<li>渐进参数调整策略和新型非结构化反向传播策略被设计来充分利用重新训练或微调后的参数，并降低内存成本。</li>
<li>SaRA方法提高了预训练模型在下游应用中的生成能力，并优于传统的微调方法如LoRA。</li>
<li>SaRA方法具有实用性优势，只需对代码进行一行修改即可实现高效实施，并与现有方法无缝兼容。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.06633">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-87fc4dd6a30074c3e60f9feabe710051.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05edac631d6de0d88bfbfad24dc08ea5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0fd1989187f1ecdd4b4c8c4e31b8e6a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b08135e65645ec87017c4061f1057c8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="DreamScape-3D-Scene-Creation-via-Gaussian-Splatting-joint-Correlation-Modeling"><a href="#DreamScape-3D-Scene-Creation-via-Gaussian-Splatting-joint-Correlation-Modeling" class="headerlink" title="DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation   Modeling"></a>DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation   Modeling</h2><p><strong>Authors:Yueming Zhao, Xuening Yuan, Hongyu Yang, Di Huang</strong></p>
<p>Recent advances in text-to-3D creation integrate the potent prior of Diffusion Models from text-to-image generation into 3D domain. Nevertheless, generating 3D scenes with multiple objects remains challenging. Therefore, we present DreamScape, a method for generating 3D scenes from text. Utilizing Gaussian Splatting for 3D representation, DreamScape introduces 3D Gaussian Guide that encodes semantic primitives, spatial transformations and relationships from text using LLMs, enabling local-to-global optimization. Progressive scale control is tailored during local object generation, addressing training instability issue arising from simple blending in the global optimization stage. Collision relationships between objects are modeled at the global level to mitigate biases in LLMs priors, ensuring physical correctness. Additionally, to generate pervasive objects like rain and snow distributed extensively across the scene, we design specialized sparse initialization and densification strategy. Experiments demonstrate that DreamScape achieves state-of-the-art performance, enabling high-fidelity, controllable 3D scene generation. </p>
<blockquote>
<p>近期文本到3D创作的进展将文本到图像生成的Diffusion Models的强大先验知识整合到3D领域。然而，在3D场景中生成多个对象仍然具有挑战性。因此，我们提出了DreamScape方法，一种从文本生成3D场景的方法。利用高斯贴图进行3D表示，DreamScape引入了3D高斯指南，该指南使用大型语言模型（LLMs）从文本编码语义原始信息、空间变换和关系，实现局部到全局的优化。局部对象生成过程中进行了渐进的尺度控制，解决了全局优化阶段简单混合引起的训练不稳定问题。在全局层面建立了物体间的碰撞关系，以减轻LLMs先验中的偏见，确保物理正确性。此外，为了生成如雨雪等广泛分布在场景中的普遍物体，我们设计了专门的稀疏初始化和密集化策略。实验表明，DreamScape达到了最先进的性能，能够实现高保真、可控的3D场景生成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.09227v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于Diffusion Models的最新技术，将文本转化为三维场景。通过引入Gaussian Splatting进行三维表示和3D Gaussian Guide技术，该技术能够从文本中生成三维场景，并处理多物体之间的相互作用。DreamScape方法实现了局部到全局的优化，通过渐进式尺度控制和碰撞关系建模等技术，解决了训练不稳定和物体间物理正确性的问题。实验证明，DreamScape在生成高保真、可控的三维场景方面达到了最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DreamScape将Diffusion Models从文本到图像生成的潜力应用于三维领域。</li>
<li>利用Gaussian Splatting进行三维表示。</li>
<li>引入3D Gaussian Guide，通过大型语言模型（LLMs）从文本中编码语义原始信息、空间变换和关系。</li>
<li>实现局部到全局的优化，包括渐进式尺度控制和物体间碰撞关系的建模。</li>
<li>针对全局优化阶段出现的训练不稳定问题，采取了有效措施。</li>
<li>开发了专门的技术来生成场景中的普遍物体，如雨和雪。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.09227">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7bb46a44e19fde605abc7d5e84986cdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14f7d4c4563a1443997b4a4a5f9b4f40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-640e099afc274f23153a99cded40dd25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2dc2dd2244e82a220d5f9fe9e00a813a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb4f731f2b0d30d20b6c3cc50828430c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-05/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-05/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-87db798d15356e64a92dde81090c2fa9.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-04-05  STING-BEE Towards Vision-Language Model for Real-World X-ray Baggage   Security Inspection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-05/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e15f469bbc5c7833fc0865375a2de227.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-04-05  MultiNeRF Multiple Watermark Embedding for Neural Radiance Fields
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">19939k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
