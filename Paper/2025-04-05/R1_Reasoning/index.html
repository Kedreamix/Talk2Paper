<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-05  Envisioning Beyond the Pixels Benchmarking Reasoning-Informed Visual   Editing">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ce8ff264396fcfd5004160cdd3558dc5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-05-æ›´æ–°"><a href="#2025-04-05-æ›´æ–°" class="headerlink" title="2025-04-05 æ›´æ–°"></a>2025-04-05 æ›´æ–°</h1><h2 id="Envisioning-Beyond-the-Pixels-Benchmarking-Reasoning-Informed-Visual-Editing"><a href="#Envisioning-Beyond-the-Pixels-Benchmarking-Reasoning-Informed-Visual-Editing" class="headerlink" title="Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual   Editing"></a>Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual   Editing</h2><p><strong>Authors:Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Hao Li, Zicheng Zhang, Guangtao Zhai, Junchi Yan, Hua Yang, Xue Yang, Haodong Duan</strong></p>
<p>Large Multi-modality Models (LMMs) have made significant progress in visual understanding and generation, but they still face challenges in General Visual Editing, particularly in following complex instructions, preserving appearance consistency, and supporting flexible input formats. To address this gap, we introduce RISEBench, the first benchmark for evaluating Reasoning-Informed viSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal, Causal, Spatial, and Logical Reasoning. We curate high-quality test cases for each category and propose an evaluation framework that assesses Instruction Reasoning, Appearance Consistency, and Visual Plausibility with both human judges and an LMM-as-a-judge approach. Our experiments reveal that while GPT-4o-Native significantly outperforms other open-source and proprietary models, even this state-of-the-art system struggles with logical reasoning tasks, highlighting an area that remains underexplored. As an initial effort, RISEBench aims to provide foundational insights into reasoning-aware visual editing and to catalyze future research. Though still in its early stages, we are committed to continuously expanding and refining the benchmark to support more comprehensive, reliable, and scalable evaluations of next-generation multimodal systems. Our code and data will be released at <a target="_blank" rel="noopener" href="https://github.com/PhoenixZ810/RISEBench">https://github.com/PhoenixZ810/RISEBench</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é€šç”¨è§†è§‰ç¼–è¾‘æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éµå¾ªå¤æ‚æŒ‡ä»¤ã€ä¿æŒå¤–è§‚ä¸€è‡´æ€§å’Œæ”¯æŒçµæ´»è¾“å…¥æ ¼å¼æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†RISEBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°æ¨ç†é©±åŠ¨è§†è§‰ç¼–è¾‘ï¼ˆRISEï¼‰çš„åŸºå‡†æµ‹è¯•ã€‚RISEBenchä¸“æ³¨äºå››ç§å…³é”®æ¨ç†ç±»å‹ï¼šæ—¶é—´æ¨ç†ã€å› æœæ¨ç†ã€ç©ºé—´æ¨ç†å’Œé€»è¾‘æ¨ç†ã€‚æˆ‘ä»¬ä¸ºæ¯ä¸€ç±»åˆ«ç­–åˆ’äº†é«˜è´¨é‡çš„æµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡äººç±»è¯„å§”å’ŒLMM-as-a-judgeæ–¹æ³•è¯„ä¼°æŒ‡ä»¤æ¨ç†ã€å¤–è§‚ä¸€è‡´æ€§å’Œè§†è§‰å¯è¡Œæ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒGPT-4o-Nativeæ˜¾è‘—ä¼˜äºå…¶ä»–å¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼Œä½†å³ä½¿æ˜¯è¿™ä¸€æœ€å…ˆè¿›çš„ç³»ç»Ÿåœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šä¹Ÿé‡åˆ°å›°éš¾ï¼Œè¿™å‡¸æ˜¾äº†ä¸€ä¸ªä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢çš„é¢†åŸŸã€‚ä½œä¸ºåˆæ­¥å°è¯•ï¼ŒRISEBenchæ—¨åœ¨æä¾›å¯¹æ¨ç†æ„ŸçŸ¥è§†è§‰ç¼–è¾‘çš„åŸºç¡€è§è§£ï¼Œå¹¶å‚¬åŒ–æœªæ¥çš„ç ”ç©¶ã€‚å°½ç®¡ä»å¤„äºæ—©æœŸé˜¶æ®µï¼Œæˆ‘ä»¬è‡´åŠ›äºä¸æ–­æ‰©å±•å’Œç²¾ç‚¼è¿™ä¸€åŸºå‡†æµ‹è¯•ï¼Œä»¥æ”¯æŒå¯¹ä¸‹ä¸€ä»£å¤šæ¨¡æ€ç³»ç»Ÿè¿›è¡Œæ›´å…¨é¢ã€å¯é å’Œå¯æ‰©å±•çš„è¯„ä¼°ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/PhoenixZ8t0/RISEBench">https://github.com/PhoenixZ8t0/RISEBench</a>ä¸Šå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02826v1">PDF</a> 27 pages, 23 figures, 1 table. Technical Report</p>
<p><strong>Summary</strong><br>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é€šç”¨è§†è§‰ç¼–è¾‘æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éµå¾ªå¤æ‚æŒ‡ä»¤ã€ä¿æŒå¤–è§‚ä¸€è‡´æ€§å’Œæ”¯æŒçµæ´»è¾“å…¥æ ¼å¼æ–¹é¢ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†RISEBenchï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹æ¨ç†è¾…åŠ©è§†è§‰ç¼–è¾‘ï¼ˆRISEï¼‰çš„è¯„ä¼°åŸºå‡†ã€‚RISEBenchä¸“æ³¨äºå››ç§å…³é”®æ¨ç†ç±»å‹ï¼šæ—¶é—´æ¨ç†ã€å› æœæ¨ç†ã€ç©ºé—´æ¨ç†å’Œé€»è¾‘æ¨ç†ã€‚æˆ‘ä»¬ä¸ºæ¯ä¸ªç±»åˆ«ç­–åˆ’äº†é«˜è´¨é‡çš„æµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡äººç±»è¯„å§”å’ŒLMM-as-a-judgeçš„æ–¹æ³•è¯„ä¼°æŒ‡ä»¤æ¨ç†ã€å¤–è§‚ä¸€è‡´æ€§å’Œè§†è§‰å¯ä¿¡åº¦ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒGPT-4o-Nativeè™½ç„¶æ˜¾è‘—ä¼˜äºå…¶ä»–å¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼Œä½†åœ¨é€»è¾‘æ¨ç†ä»»åŠ¡æ–¹é¢ä»å­˜åœ¨å›°éš¾ï¼Œè¿™å‡¸æ˜¾äº†ä¸€ä¸ªä»ç„¶è¢«å¿½è§†çš„é¢†åŸŸã€‚ä½œä¸ºåˆæ­¥å°è¯•ï¼ŒRISEBenchæ—¨åœ¨ä¸ºæ¨ç†æ„ŸçŸ¥è§†è§‰ç¼–è¾‘æä¾›åŸºç¡€è§è§£ï¼Œå¹¶å‚¬åŒ–æœªæ¥çš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—è¿›å±•ï¼Œä½†åœ¨é€šç”¨è§†è§‰ç¼–è¾‘ä¸­ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>RISEBenchæ˜¯é¦–ä¸ªé’ˆå¯¹æ¨ç†è¾…åŠ©è§†è§‰ç¼–è¾‘çš„è¯„ä¼°åŸºå‡†ï¼Œä¸“æ³¨äºå››ç§å…³é”®æ¨ç†ç±»å‹ã€‚</li>
<li>RISEBenchåŒ…æ‹¬é«˜è´¨é‡çš„æµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶æœ‰ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–æŒ‡ä»¤æ¨ç†ã€å¤–è§‚ä¸€è‡´æ€§å’Œè§†è§‰å¯ä¿¡åº¦ã€‚</li>
<li>GPT-4o-Nativeè™½ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œä½†åœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šè¿˜æœ‰å›°éš¾ã€‚</li>
<li>RISEBenchæ—¨åœ¨ä¸ºæ¨ç†æ„ŸçŸ¥è§†è§‰ç¼–è¾‘æä¾›åŸºç¡€è§è§£ï¼Œå¹¶ä¿ƒè¿›æœªæ¥ç ”ç©¶ã€‚</li>
<li>RISEBenchä»åœ¨åˆæœŸé˜¶æ®µï¼Œæ‰¿è¯ºä¸æ–­æ‰©å±•å’Œç²¾ç‚¼åŸºå‡†æµ‹è¯•ï¼Œä»¥æ”¯æŒæ›´å…¨é¢ã€å¯é å’Œå¯æ‰©å±•çš„ä¸‹ä¸€ä»£å¤šæ¨¡æ€ç³»ç»Ÿè¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02826">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6ec66ebd060e564477da1ae1989a277c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b224c6dc21a4b101242b8066d25ced8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67227a73fb8c3bb609df8d641074f313.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Generative-Evaluation-of-Complex-Reasoning-in-Large-Language-Models"><a href="#Generative-Evaluation-of-Complex-Reasoning-in-Large-Language-Models" class="headerlink" title="Generative Evaluation of Complex Reasoning in Large Language Models"></a>Generative Evaluation of Complex Reasoning in Large Language Models</h2><p><strong>Authors:Haowei Lin, Xiangyu Wang, Ruilin Yan, Baizhou Huang, Haotian Ye, Jianhua Zhu, Zihao Wang, James Zou, Jianzhu Ma, Yitao Liang</strong></p>
<p>With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMOâ€™s value as a robust, enduring assessment tool for genuine LLM reasoning capabilities. </p>
<blockquote>
<p>éšç€å¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºè¶…äººç±»çš„æ¨ç†èƒ½åŠ›ï¼Œä¸€ä¸ªé‡è¦çš„é—®é¢˜å‡ºç°äº†ï¼šLLMæ˜¯çœŸæ­£è¿›è¡Œæ¨ç†ï¼Œè¿˜æ˜¯ä»…ä»…ä»å®ƒä»¬å¹¿æ³›ã€ç½‘ç»œçˆ¬å–çš„è®­ç»ƒæ•°æ®é›†ä¸­å›å¿†ç­”æ¡ˆï¼Ÿä¸€æ—¦çº³å…¥åç»­çš„LLMè®­ç»ƒé›†ï¼Œå…¬å¼€å‘å¸ƒçš„åŸºå‡†æµ‹è¯•ä¸å¯é¿å…åœ°ä¼šå—åˆ°æ±¡æŸ“ï¼Œä»è€Œç ´åå®ƒä»¬ä½œä¸ºå¿ å®è¯„ä¼°çš„å¯é æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†KUMOï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°LLMæ¨ç†èƒ½åŠ›çš„ç”Ÿæˆè¯„ä¼°æ¡†æ¶ã€‚KUMOååŒç»“åˆäº†LLMå’Œç¬¦å·å¼•æ“ï¼Œä»¥åŠ¨æ€ç”Ÿæˆå¤šæ ·åŒ–ã€å¤šå›åˆçš„æ¨ç†ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡æ˜¯éƒ¨åˆ†å¯è§‚å¯Ÿçš„ï¼Œéš¾åº¦å¯è°ƒã€‚é€šè¿‡è‡ªåŠ¨åŒ–ç®¡é“ï¼ŒKUMOä¸æ–­ç”Ÿæˆè·¨å¼€æ”¾å¼é¢†åŸŸçš„å…¨æ–°ä»»åŠ¡ï¼Œä¿ƒä½¿æ¨¡å‹å±•ç¤ºçœŸæ­£çš„æ³›åŒ–èƒ½åŠ›è€Œéè®°å¿†åŠ›ã€‚æˆ‘ä»¬åœ¨KUMOåˆ›å»ºçš„5000ä¸ªä»»åŠ¡ã€100ä¸ªé¢†åŸŸä¸­å¯¹23ç§æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå°†å…¶æ¨ç†èƒ½åŠ›ä¸å¤§å­¦ç”Ÿè¿›è¡ŒåŸºå‡†æµ‹è¯•æ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè®¸å¤šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç®€å•çš„æ¨ç†ä»»åŠ¡ä¸Šå·²ç»è¶…è¶Šäº†å¤§å­¦æ°´å¹³ï¼Œè€Œåœ¨å¤æ‚çš„æ¨ç†æŒ‘æˆ˜ä¸­ï¼Œç»è¿‡æ¨ç†è¯„ä¼°çš„å¤§å‹è¯­è¨€æ¨¡å‹è¾¾åˆ°äº†å¤§å­¦æ°´å¹³ã€‚æ­¤å¤–ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨KUMOä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸æ–°å‘å¸ƒçš„ç°å®ä¸–ç•Œæ¨ç†åŸºå‡†æµ‹è¯•çš„ç»“æœå…·æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ï¼Œè¿™å‡¸æ˜¾äº†KUMOä½œä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çœŸå®æ¨ç†èƒ½åŠ›çš„ç¨³å¥ã€æŒä¹…è¯„ä¼°å·¥å…·çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02810v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°è¶…äººçš„æ¨ç†èƒ½åŠ›ï¼Œå¼•å‘å…³é”®é—®é¢˜ï¼šLLMæ˜¯å¦çœŸæ­£è¿›è¡Œæ¨ç†ï¼ŒæŠ‘æˆ–ä»…ä»å¤§é‡çš„ç½‘ç»œæŠ“å–è®­ç»ƒæ•°æ®é›†ä¸­å›å¿†ç­”æ¡ˆã€‚ä¸ºè§£å†³åŸºå‡†æµ‹è¯•çš„æ±¡æŸ“é—®é¢˜ï¼Œå¼•å…¥KUMOè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯é’ˆå¯¹LLMçš„æ¨ç†èƒ½åŠ›è¿›è¡Œä¸“é—¨è¯„ä¼°ã€‚KUMOååŒç»“åˆLLMå’Œç¬¦å·å¼•æ“ï¼ŒåŠ¨æ€ç”Ÿæˆå¤šæ ·åŒ–ã€å¤šå›åˆçš„æ¨ç†ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡æ˜¯éƒ¨åˆ†å¯è§‚å¯Ÿçš„å’Œå¯è°ƒèŠ‚éš¾åº¦çš„ã€‚KUMOé€šè¿‡è‡ªåŠ¨åŒ–ç®¡é“æŒç»­ç”Ÿæˆå…¨æ–°ä»»åŠ¡ï¼Œä¿ƒä½¿æ¨¡å‹å±•ç¤ºçœŸæ­£çš„æ³›åŒ–èƒ½åŠ›è€Œéè®°å¿†èƒ½åŠ›ã€‚å¯¹23ä¸ªæœ€å…ˆè¿›çš„LLMåœ¨KUMOåˆ›å»ºçš„100ä¸ªé¢†åŸŸçš„5000ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œä¸å¤§å­¦ç”Ÿè¿›è¡ŒåŸºå‡†æµ‹è¯•æ¯”è¾ƒã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè®¸å¤šLLMå·²åœ¨ç®€å•æ¨ç†ä»»åŠ¡ä¸Šè¶…è¶Šäº†å¤§å­¦æ°´å¹³ï¼Œåœ¨å¤æ‚çš„æ¨ç†æŒ‘æˆ˜ä¸­è¾¾åˆ°å¤§å­¦æ°´å¹³ã€‚æ­¤å¤–ï¼ŒLLMåœ¨KUMOä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸æ–°å‘å¸ƒçš„ç°å®ä¸–ç•Œæ¨ç†åŸºå‡†æµ‹è¯•ç»“æœå¯†åˆ‡ç›¸å…³ï¼Œå‡¸æ˜¾äº†KUMOä½œä¸ºè¯„ä¼°LLMçœŸæ­£æ¨ç†èƒ½åŠ›çš„ç¨³å¥ã€æŒä¹…å·¥å…·çš„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°è¶…äººæ¨ç†èƒ½åŠ›ï¼Œä½†å­˜åœ¨æ˜¯å¦çœŸæ­£æ¨ç†çš„è´¨ç–‘ã€‚</li>
<li>LLMå¯èƒ½ä»ç½‘ç»œæŠ“å–çš„è®­ç»ƒæ•°æ®é›†ä¸­å›å¿†ç­”æ¡ˆï¼Œå¼•å‘å¯¹åŸºå‡†æµ‹è¯•çš„æ±¡æŸ“é—®é¢˜ã€‚</li>
<li>å¼•å…¥KUMOè¯„ä¼°æ¡†æ¶ï¼Œä¸“é—¨è¯„ä¼°LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>KUMOç»“åˆLLMå’Œç¬¦å·å¼•æ“ï¼Œç”Ÿæˆå¤šæ ·åŒ–ã€å¤šå›åˆçš„æ¨ç†ä»»åŠ¡ã€‚</li>
<li>KUMOä»»åŠ¡éƒ¨åˆ†å¯è§‚ã€éš¾åº¦å¯è°ƒèŠ‚ï¼Œä¿ƒè¿›æ¨¡å‹å±•ç¤ºçœŸæ­£çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¯¹å¤šä¸ªLLMåœ¨å¤šç§ä»»åŠ¡ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå®ƒä»¬åœ¨ç®€å•å’Œå¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸åŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed25ba3de363f72487caf1dceddb5297.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b302c448b5383e95b012f6a37d95def4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f52f5abde00ea7eaa9d38b4b59d9118.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Multi-SWE-bench-A-Multilingual-Benchmark-for-Issue-Resolving"><a href="#Multi-SWE-bench-A-Multilingual-Benchmark-for-Issue-Resolving" class="headerlink" title="Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving"></a>Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving</h2><p><strong>Authors:Daoguang Zan, Zhirong Huang, Wei Liu, Hanwu Chen, Linhao Zhang, Shulin Xin, Lu Chen, Qi Liu, Xiaojian Zhong, Aoyan Li, Siyao Liu, Yongsheng Xiao, Liangqiang Chen, Yuyu Zhang, Jing Su, Tianyu Liu, Rui Long, Kai Shen, Liang Xiang</strong></p>
<p>The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To address this, we introduce a multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a total of 1,632 high-quality instances, which were carefully annotated from 2,456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation. Based on Multi-SWE-bench, we evaluate a series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with key empirical insights. In addition, we launch a Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks. As an initial contribution, we release a set of 4,723 well-structured instances spanning seven programming languages, laying a solid foundation for RL research in this domain. More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI. </p>
<blockquote>
<p>é—®é¢˜è§£å†³çš„ä»»åŠ¡æ˜¯å¯¹ä»£ç åº“è¿›è¡Œä¿®æ”¹ï¼Œä»¥ç”Ÿæˆè§£å†³ç»™å®šé—®é¢˜çš„è¡¥ä¸ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ï¼Œå¦‚SWE-benchï¼Œå‡ ä¹åªä¸“æ³¨äºPythonï¼Œè¿™ä½¿å¾—å®ƒä»¬ä¸è¶³ä»¥è¯„ä¼°è·¨å¤šç§è½¯ä»¶ç”Ÿæ€ç³»ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤šè¯­è¨€é—®é¢˜è§£å†³åŸºå‡†æµ‹è¯•ï¼Œåä¸ºMulti-SWE-benchï¼Œæ¶µç›–Javaã€TypeScriptã€JavaScriptã€Goã€Rustã€Cå’ŒC++ã€‚å®ƒåŒ…å«æ€»å…±1632ä¸ªé«˜è´¨é‡å®ä¾‹ï¼Œè¿™äº›å®ä¾‹æ˜¯ä»2456ä¸ªå€™é€‰è€…ä¸­ç”±68ä½ä¸“å®¶æ³¨é‡Šè€…ä»”ç»†æ ‡æ³¨çš„ï¼Œç¡®ä¿åŸºå‡†æµ‹è¯•èƒ½å¤Ÿæä¾›å‡†ç¡®å’Œå¯é çš„è¯„ä»·ã€‚åŸºäºMulti-SWE-benchï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸‰ç§å…·æœ‰ä»£è¡¨æ€§çš„æ–¹æ³•ï¼ˆæ— ä»£ç†ã€SWE-agentå’ŒOpenHandsï¼‰è¯„ä¼°äº†ä¸€ç³»åˆ—æœ€æ–°æ¨¡å‹ï¼Œå¹¶è¿›è¡Œäº†ç»¼åˆåˆ†æï¼Œæä¾›äº†å…³é”®çš„å®è¯è§è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸€ä¸ªåä¸ºMulti-SWE-RLçš„å¼€æºç¤¾åŒºï¼Œæ—¨åœ¨æ„å»ºç”¨äºé—®é¢˜è§£å†³ä»»åŠ¡çš„å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒæ•°æ®é›†ã€‚ä½œä¸ºåˆæ­¥è´¡çŒ®ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ç»„æ¶µç›–ä¸ƒç§ç¼–ç¨‹è¯­è¨€çš„4723ä¸ªç»“æ„è‰¯å¥½çš„å®ä¾‹ï¼Œä¸ºè¿™ä¸€é¢†åŸŸçš„RLç ”ç©¶å¥ å®šäº†åšå®åŸºç¡€ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å¼€æºäº†æ•´ä¸ªæ•°æ®ç”Ÿäº§ç®¡é“ä»¥åŠè¯¦ç»†çš„æ•™ç¨‹ï¼Œé¼“åŠ±å¼€æºç¤¾åŒºæŒç»­è´¡çŒ®å¹¶æ‰©å±•æ•°æ®é›†ã€‚æˆ‘ä»¬æœŸæœ›æˆ‘ä»¬çš„Multi-SWE-benchå’Œä¸æ–­å‘å±•çš„Multi-SWE-RLç¤¾åŒºèƒ½æˆä¸ºæ¨åŠ¨RLå‘æŒ¥å…¨éƒ¨æ½œåŠ›çš„å‚¬åŒ–å‰‚ï¼Œä½¿æˆ‘ä»¬ç¦»AGIçš„é»æ˜æ›´è¿‘ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02605v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†ä¸ºäº†è§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•ï¼ˆå¦‚SWE-benchï¼‰åœ¨å¤šè¯­è¨€æ¨¡å‹è¯„ä¼°ä¸Šçš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ä¸ªå¤šè¯­è¨€é—®é¢˜è§£å†³çš„åŸºå‡†æµ‹è¯•â€”â€”Multi-SWE-benchã€‚è¯¥åŸºå‡†æµ‹è¯•æ¶µç›–äº†Javaã€TypeScriptã€JavaScriptã€Goã€Rustã€Cå’ŒC++ç­‰ç¼–ç¨‹è¯­è¨€ï¼ŒåŒ…å«ä»å€™é€‰è€…ä¸­ç²¾å¿ƒæ ‡æ³¨çš„1,632ä¸ªé«˜è´¨é‡å®ä¾‹ã€‚æ–‡ç« è¿˜åŸºäºMulti-SWE-benchè¯„ä¼°äº†ä¸€ç³»åˆ—æœ€æ–°æ¨¡å‹ï¼Œå¹¶ä»‹ç»äº†Multi-SWE-RLå¼€æºç¤¾åŒºçš„å»ºè®¾æƒ…å†µï¼Œæ—¨åœ¨ä¸ºé—®é¢˜è§£å†³çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†ã€‚åŒæ—¶ï¼Œå…¬å¼€äº†æ•´ä¸ªæ•°æ®ç”Ÿäº§ç®¡é“å’Œè¯¦ç»†æ•™ç¨‹ï¼Œé¼“åŠ±å¼€æºç¤¾åŒºæŒç»­è´¡çŒ®å’Œæ‰©å±•æ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•å¦‚SWE-benchä¸»è¦å…³æ³¨Pythonï¼Œä¸è¶³ä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šç”Ÿæ€ç³»ç»Ÿä¸­çš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†å¤šè¯­è¨€é—®é¢˜è§£å†³åŸºå‡†æµ‹è¯•Multi-SWE-benchï¼Œæ¶µç›–äº†å¤šç§ç¼–ç¨‹è¯­è¨€ã€‚</li>
<li>Multi-SWE-benchåŒ…å«ç»è¿‡ä¸“å®¶ç²¾å¿ƒæ ‡æ³¨çš„1,632ä¸ªé«˜è´¨é‡å®ä¾‹ï¼Œç¡®ä¿å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
<li>åŸºäºMulti-SWE-benchè¯„ä¼°äº†ä¸€ç³»åˆ—æœ€æ–°æ¨¡å‹ï¼Œæä¾›äº†å…³é”®å®è¯è§è§£ã€‚</li>
<li>å»ºç«‹äº†Multi-SWE-RLå¼€æºç¤¾åŒºï¼Œæ—¨åœ¨æ„å»ºé—®é¢˜è§£å†³çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚</li>
<li>å…¬å¼€äº†æ•°æ®é›†å’Œæ•°æ®ç”Ÿäº§ç®¡é“ï¼Œé¼“åŠ±å¼€æºç¤¾åŒºæŒç»­è´¡çŒ®å’Œæ‰©å±•æ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02605">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ca7452374cee9bca7e1e218457c99dae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f70bdc2019a098c4bdcf4f1da70484ee.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Rethinking-RL-Scaling-for-Vision-Language-Models-A-Transparent-From-Scratch-Framework-and-Comprehensive-Evaluation-Scheme"><a href="#Rethinking-RL-Scaling-for-Vision-Language-Models-A-Transparent-From-Scratch-Framework-and-Comprehensive-Evaluation-Scheme" class="headerlink" title="Rethinking RL Scaling for Vision Language Models: A Transparent,   From-Scratch Framework and Comprehensive Evaluation Scheme"></a>Rethinking RL Scaling for Vision Language Models: A Transparent,   From-Scratch Framework and Comprehensive Evaluation Scheme</h2><p><strong>Authors:Yan Ma, Steffi Chern, Xuyang Shen, Yiran Zhong, Pengfei Liu</strong></p>
<p>Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs). However, existing RL applications in VLMs often rely on heavily engineered frameworks that hinder reproducibility and accessibility, while lacking standardized evaluation protocols, making it difficult to compare results or interpret training dynamics. This work introduces a transparent, from-scratch framework for RL in VLMs, offering a minimal yet functional four-step pipeline validated across multiple models and datasets. In addition, a standardized evaluation scheme is proposed to assess training dynamics and reflective behaviors. Extensive experiments on visual reasoning tasks uncover key empirical findings: response length is sensitive to random seeds, reflection correlates with output length, and RL consistently outperforms supervised fine-tuning (SFT) in generalization, even with high-quality data. These findings, together with the proposed framework, aim to establish a reproducible baseline and support broader engagement in RL-based VLM research. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å·²æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œç›®å‰æ­£åœ¨ç§¯ææ‹“å±•åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å¼ºåŒ–å­¦ä¹ åº”ç”¨å¾€å¾€ä¾èµ–äºé‡åº¦å·¥ç¨‹çš„æ¡†æ¶ï¼Œè¿™äº›æ¡†æ¶é˜»ç¢äº†å¯é‡å¤æ€§å’Œå¯è®¿é—®æ€§ï¼ŒåŒæ—¶ç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ï¼Œä½¿å¾—éš¾ä»¥æ¯”è¾ƒç»“æœæˆ–è§£é‡Šè®­ç»ƒåŠ¨æ€ã€‚è¿™é¡¹å·¥ä½œå¼•å…¥äº†ä¸€ä¸ªé€æ˜ã€ä»å¤´å¼€å§‹çš„è§†è§‰è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæä¾›äº†ä¸€ä¸ªç®€æ´è€Œå®ç”¨çš„å››æ­¥æµç¨‹ï¼Œå¹¶åœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ ‡å‡†åŒ–çš„è¯„ä¼°æ–¹æ¡ˆï¼Œä»¥è¯„ä¼°è®­ç»ƒåŠ¨æ€å’Œåæ€è¡Œä¸ºã€‚åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒæ­ç¤ºäº†å…³é”®ç»éªŒå‘ç°ï¼šå“åº”é•¿åº¦å¯¹éšæœºç§å­æ•æ„Ÿï¼Œåæ€ä¸è¾“å‡ºé•¿åº¦ç›¸å…³ï¼Œå¼ºåŒ–å­¦ä¹ åœ¨æ³›åŒ–æ–¹é¢æŒç»­ä¼˜äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå³ä½¿åœ¨é«˜è´¨é‡æ•°æ®çš„æƒ…å†µä¸‹äº¦æ˜¯å¦‚æ­¤ã€‚è¿™äº›å‘ç°ä¸æ‰€æå‡ºçš„æ¡†æ¶ä¸€èµ·ï¼Œæ—¨åœ¨å»ºç«‹ä¸€ä¸ªå¯é‡å¤çš„åŸºæœ¬çº¿ï¼Œå¹¶æ”¯æŒæ›´å¹¿æ³›åœ°å‚ä¸åŸºäºå¼ºåŒ–å­¦ä¹ çš„è§†è§‰è¯­è¨€æ¨¡å‹ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02587v1">PDF</a> Code is public and available at: <a target="_blank" rel="noopener" href="https://github.com/GAIR-NLP/MAYE">https://github.com/GAIR-NLP/MAYE</a></p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå¼ºå¤§æ½œåŠ›ï¼Œç›®å‰æ­£åœ¨ç§¯ææ‹“å±•è‡³è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„VLMsä¸­çš„RLåº”ç”¨é€šå¸¸ä¾èµ–äºå¤æ‚çš„æ¡†æ¶ï¼Œé˜»ç¢äº†å¯é‡å¤æ€§å’Œå¯è®¿é—®æ€§ï¼Œä¸”ç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ï¼Œä½¿å¾—ç»“æœéš¾ä»¥æ¯”è¾ƒæˆ–è§£é‡Šè®­ç»ƒåŠ¨æ€ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ä¸ªé€æ˜ã€ä»å¤´å¼€å§‹çš„RLåœ¨VLMsä¸­çš„æ¡†æ¶ï¼Œæä¾›äº†ä¸€ä¸ªç®€æ´è€Œå®ç”¨çš„å››æ­¥æµç¨‹ï¼Œå¹¶åœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§æ ‡å‡†åŒ–çš„è¯„ä¼°æ–¹æ¡ˆï¼Œä»¥è¯„ä¼°è®­ç»ƒåŠ¨æ€å’Œåå°„è¡Œä¸ºã€‚åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒå‘ç°äº†å…³é”®ç»éªŒï¼šå“åº”é•¿åº¦å¯¹éšæœºç§å­æ•æ„Ÿï¼Œåå°„ä¸è¾“å‡ºé•¿åº¦ç›¸å…³ï¼ŒRLåœ¨æ³›åŒ–æ–¹é¢æŒç»­ä¼˜äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå³ä½¿åœ¨é«˜è´¨é‡æ•°æ®ä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æå‡è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢æœ‰å¾ˆå¤§æ½œåŠ›ï¼Œå°¤å…¶å¯¹äºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚</li>
<li>ç°æœ‰æ¡†æ¶åœ¨VLMsä¸­çš„RLåº”ç”¨å­˜åœ¨å¯é‡å¤æ€§å’Œå¯è®¿é—®æ€§é—®é¢˜ã€‚</li>
<li>ç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ï¼Œå¯¼è‡´éš¾ä»¥æ¯”è¾ƒä¸åŒRLæ¨¡å‹çš„ç»“æœæˆ–è§£é‡Šå…¶è®­ç»ƒåŠ¨æ€ã€‚</li>
<li>æœ¬ç ”ç©¶æä¾›äº†ä¸€ä¸ªç®€æ´çš„RLæ¡†æ¶ï¼Œé€‚ç”¨äºVLMsï¼Œå¹¶åŒ…å«å››ä¸ªä¸»è¦æ­¥éª¤ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ ‡å‡†åŒ–çš„è¯„ä¼°æ–¹æ¡ˆæ¥è¯„ä¼°è®­ç»ƒåŠ¨æ€å’Œæ¨¡å‹çš„åå°„è¡Œä¸ºã€‚</li>
<li>å®éªŒå‘ç°å“åº”é•¿åº¦å¯¹éšæœºç§å­æ•æ„Ÿï¼Œåå°„ä¸è¾“å‡ºé•¿åº¦ç›¸å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02587">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e34f5b888d6c224436611f22ba88a3c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9596c2fc7072a45b402a40ed9b7f7bc4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a43dcd1a5d5b938ef39edd76fca9f026.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6561a9a0661d1bccabdbbcf605593329.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GPG-A-Simple-and-Strong-Reinforcement-Learning-Baseline-for-Model-Reasoning"><a href="#GPG-A-Simple-and-Strong-Reinforcement-Learning-Baseline-for-Model-Reasoning" class="headerlink" title="GPG: A Simple and Strong Reinforcement Learning Baseline for Model   Reasoning"></a>GPG: A Simple and Strong Reinforcement Learning Baseline for Model   Reasoning</h2><p><strong>Authors:Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, Yong Wang</strong></p>
<p>Reinforcement Learning (RL) can directly enhance the reasoning capabilities of large language models without extensive reliance on Supervised Fine-Tuning (SFT). In this work, we revisit the traditional Policy Gradient (PG) mechanism and propose a minimalist RL approach termed Group Policy Gradient (GPG). Unlike conventional methods, GPG directly optimize the original RL objective, thus obviating the need for surrogate loss functions. As illustrated in our paper, by eliminating both the critic and reference models, and avoiding KL divergence constraints, our approach significantly simplifies the training process when compared to Group Relative Policy Optimization (GRPO). Our approach achieves superior performance without relying on auxiliary techniques or adjustments. Extensive experiments demonstrate that our method not only reduces computational costs but also consistently outperforms GRPO across various unimodal and multimodal tasks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/GPG">https://github.com/AMAP-ML/GPG</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥ç›´æ¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€è¿‡å¤šä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†ä¼ ç»Ÿçš„ç­–ç•¥æ¢¯åº¦ï¼ˆPGï¼‰æœºåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§æç®€çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºç¾¤ç»„ç­–ç•¥æ¢¯åº¦ï¼ˆGPGï¼‰ã€‚ä¸ä¼ ç»Ÿçš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¸åŒï¼ŒGPGç›´æ¥ä¼˜åŒ–åŸå§‹çš„å¼ºåŒ–å­¦ä¹ ç›®æ ‡ï¼Œä»è€Œæ— éœ€ä½¿ç”¨æ›¿ä»£æŸå¤±å‡½æ•°ã€‚æ­£å¦‚æˆ‘ä»¬è®ºæ–‡ä¸­æ‰€å±•ç¤ºçš„ï¼Œé€šè¿‡æ¶ˆé™¤æ‰¹è¯„è€…å’Œå‚è€ƒæ¨¡å‹ï¼Œå¹¶é¿å…KLæ•£åº¦çº¦æŸï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¤§ç®€åŒ–äº†ä¸ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç›¸æ¯”çš„è®­ç»ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸ä¾èµ–è¾…åŠ©æŠ€æœ¯æˆ–è°ƒæ•´çš„æƒ…å†µä¸‹å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œè€Œä¸”åœ¨å„ç§å•æ¨¡æ€å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºGRPOã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/GPG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AMAP-ML/GPGæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02546v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ç›´æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€è¿‡åº¦ä¾èµ–æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚æœ¬ç ”ç©¶é‡æ–°å®¡è§†äº†ä¼ ç»Ÿçš„ç­–ç•¥æ¢¯åº¦ï¼ˆPGï¼‰æœºåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§æç®€ä¸»ä¹‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•â€”â€”ç¾¤ç»„ç­–ç•¥æ¢¯åº¦ï¼ˆGPGï¼‰ã€‚GPGç›´æ¥ä¼˜åŒ–åŸå§‹çš„RLç›®æ ‡ï¼Œä»è€Œæ— éœ€æ›¿ä»£æŸå¤±å‡½æ•°ã€‚é€šè¿‡çœç•¥è¯„è®ºå®¶å’Œå‚è€ƒæ¨¡å‹ï¼Œå¹¶é¿å…KLæ•£åº¦çº¦æŸï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¤§ç®€åŒ–äº†ä¸ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç›¸æ¯”çš„è®­ç»ƒè¿‡ç¨‹ã€‚åœ¨ä¸ä¾èµ–è¾…åŠ©æŠ€æœ¯æˆ–è°ƒæ•´çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œè€Œä¸”åœ¨å„ç§å•æ¨¡æ€å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºGRPOã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½å¤Ÿç›´æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†æ–°çš„æ–¹æ³•â€”â€”ç¾¤ç»„ç­–ç•¥æ¢¯åº¦ï¼ˆGPGï¼‰ï¼Œè¯¥æ–¹æ³•ç®€åŒ–è®­ç»ƒè¿‡ç¨‹å¹¶ç›´æ¥ä¼˜åŒ–RLç›®æ ‡ã€‚</li>
<li>GPGæ–¹æ³•æ— éœ€æ›¿ä»£æŸå¤±å‡½æ•°ï¼Œçœç•¥äº†ç­–ç•¥è¯„ä¼°ä¸­çš„è¯„è®ºå®¶å’Œå‚è€ƒæ¨¡å‹ã€‚</li>
<li>GPGæ–¹æ³•é¿å…äº†KLæ•£åº¦çº¦æŸï¼Œè¿›ä¸€æ­¥ç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>GPGæ–¹æ³•åœ¨ä¸ä¾èµ–è¾…åŠ©æŠ€æœ¯æˆ–è°ƒæ•´çš„æƒ…å†µä¸‹å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒGPGæ–¹æ³•ä¸ä»…é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œè€Œä¸”åœ¨å¤šç§ä»»åŠ¡ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02546">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-18a8991598fdb665a2301edb61c53e7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53cb52787c1e34710c8540ea635d1f4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ce8af86272448d2b10c7cae73ed6be8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="BOOST-Bootstrapping-Strategy-Driven-Reasoning-Programs-for-Program-Guided-Fact-Checking"><a href="#BOOST-Bootstrapping-Strategy-Driven-Reasoning-Programs-for-Program-Guided-Fact-Checking" class="headerlink" title="BOOST: Bootstrapping Strategy-Driven Reasoning Programs for   Program-Guided Fact-Checking"></a>BOOST: Bootstrapping Strategy-Driven Reasoning Programs for   Program-Guided Fact-Checking</h2><p><strong>Authors:Qisheng Hu, Quanyu Long, Wenya Wang</strong></p>
<p>Program-guided reasoning has shown promise in complex claim fact-checking by decomposing claims into function calls and executing reasoning programs. However, prior work primarily relies on few-shot in-context learning (ICL) with ad-hoc demonstrations, which limit program diversity and require manual design with substantial domain knowledge. Fundamentally, the underlying principles of effective reasoning program generation still remain underexplored, making it challenging to construct effective demonstrations. To address this, we propose BOOST, a bootstrapping-based framework for few-shot reasoning program generation. BOOST explicitly integrates claim decomposition and information-gathering strategies as structural guidance for program generation, iteratively refining bootstrapped demonstrations in a strategy-driven and data-centric manner without human intervention. This enables a seamless transition from zero-shot to few-shot strategic program-guided learning, enhancing interpretability and effectiveness. Experimental results show that BOOST outperforms prior few-shot baselines in both zero-shot and few-shot settings for complex claim verification. </p>
<blockquote>
<p>ç¨‹åºå¼•å¯¼æ¨ç†é€šè¿‡å°†å£°æ˜åˆ†è§£ä¸ºå‡½æ•°è°ƒç”¨å¹¶æ‰§è¡Œæ¨ç†ç¨‹åºï¼Œåœ¨å¤æ‚çš„å£°æ˜æŸ¥è¯ä¸­æ˜¾ç¤ºå‡ºè‰¯å¥½çš„å‰æ™¯ã€‚ç„¶è€Œï¼Œæ—©æœŸçš„å·¥ä½œä¸»è¦ä¾èµ–äºå°‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å’Œä¸“é—¨çš„æ¼”ç¤ºï¼Œè¿™é™åˆ¶äº†ç¨‹åºçš„å¤šæ ·æ€§ï¼Œå¹¶éœ€è¦å¤§é‡é¢†åŸŸçŸ¥è¯†çš„æ”¯æŒæ¥è¿›è¡Œæ‰‹åŠ¨è®¾è®¡ã€‚ä»æ ¹æœ¬ä¸Šè¯´ï¼Œæœ‰æ•ˆçš„æ¨ç†ç¨‹åºç”Ÿæˆçš„åŸºæœ¬åŸç†ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œè¿™ä½¿å¾—æ„å»ºæœ‰æ•ˆçš„æ¼”ç¤ºå˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†BOOSTï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè‡ªä¸¾çš„å°‘é‡æ¨ç†ç¨‹åºç”Ÿæˆæ¡†æ¶ã€‚BOOSTæ˜¾å¼åœ°å°†å£°æ˜åˆ†è§£å’Œä¿¡æ¯æ”¶é›†ç­–ç•¥æ•´åˆä¸ºç¨‹åºç”Ÿæˆçš„ç»“æ„æ€§æŒ‡å¯¼ï¼Œä»¥ç­–ç•¥é©±åŠ¨å’Œæ•°æ®ä¸ºä¸­å¿ƒçš„æ–¹å¼è¿­ä»£ä¼˜åŒ–è‡ªä¸¾æ¼”ç¤ºï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚è¿™ä½¿å¾—ä»é›¶æ ·æœ¬åˆ°å°‘é‡æ ·æœ¬çš„æˆ˜ç•¥ç¨‹åºå¼•å¯¼å­¦ä¹ æ— ç¼è¿‡æ¸¡ï¼Œæé«˜äº†å¯è§£é‡Šæ€§å’Œæœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBOOSTåœ¨é›¶æ ·æœ¬å’Œå°‘é‡æ ·æœ¬çš„å¤æ‚å£°æ˜éªŒè¯è®¾ç½®ä¸­ï¼Œä¼˜äºå…ˆå‰çš„å°‘é‡æ ·æœ¬åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02467v1">PDF</a> 18 pages, 5 figures</p>
<p><strong>Summary</strong>ï¼š<br>ç¨‹åºå¼•å¯¼æ¨ç†åœ¨å¤æ‚å£°æ˜æ ¸æŸ¥ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œé€šè¿‡æŠŠå£°æ˜åˆ†è§£æˆå‡½æ•°è°ƒç”¨å¹¶æ‰§è¡Œæ¨ç†ç¨‹åºè¿›è¡Œå·¥ä½œã€‚ç„¶è€Œï¼Œæ—©æœŸå·¥ä½œä¸»è¦ä¾èµ–äºå°‘æ•°åœºæ™¯ä¸‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ï¼Œéœ€è¦ç‰¹å®šç¤ºèŒƒï¼Œè¿™é™åˆ¶äº†ç¨‹åºå¤šæ ·æ€§ï¼Œå¹¶éœ€è¦å®è´¨æ€§çš„é¢†åŸŸçŸ¥è¯†çš„å‚ä¸è®¾è®¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†BOOSTï¼Œä¸€ä¸ªåŸºäºå¯åŠ¨æ³•çš„æ¨ç†ç¨‹åºç”Ÿæˆæ¡†æ¶ã€‚BOOSTæ˜¾å¼åœ°é›†æˆäº†å£°æ˜åˆ†è§£å’Œä¿¡æ¯æ”¶é›†ç­–ç•¥ä½œä¸ºç¨‹åºç”Ÿæˆçš„æŒ‡å¯¼ç»“æ„ï¼Œä»¥ç­–ç•¥é©±åŠ¨å’Œæ•°æ®ä¸ºä¸­å¿ƒçš„æ–¹å¼è¿­ä»£ä¼˜åŒ–å¯åŠ¨ç¤ºèŒƒï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚è¿™ä½¿å¾—ä»é›¶æ ·æœ¬åˆ°å°‘æ•°æ ·æœ¬çš„æˆ˜ç•¥æ€§ç¨‹åºå¼•å¯¼å­¦ä¹ æ— ç¼è¿‡æ¸¡ï¼Œæé«˜äº†å¯è§£é‡Šæ€§å’Œæœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBOOSTåœ¨é›¶æ ·æœ¬å’Œå°‘æ•°æ ·æœ¬è®¾ç½®ä¸‹çš„å¤æ‚å£°æ˜éªŒè¯ä¸­ä¼˜äºå…ˆå‰çš„å°‘æ•°æ ·æœ¬åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç¨‹åºå¼•å¯¼æ¨ç†åœ¨å¤æ‚å£°æ˜æ ¸æŸ¥ä¸­æœ‰åº”ç”¨æ½œåŠ›ï¼Œé€šè¿‡åˆ†è§£å£°æ˜å¹¶æ‰§è¡Œæ¨ç†ç¨‹åºè¿›è¡Œå·¥ä½œã€‚</li>
<li>æ—©æœŸå·¥ä½œä¸»è¦ä¾èµ–å°‘æ•°åœºæ™¯ä¸‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å’Œç‰¹å®šç¤ºèŒƒï¼Œè¿™é™åˆ¶äº†ç¨‹åºå¤šæ ·æ€§å¹¶éœ€è¦ä¸°å¯Œçš„é¢†åŸŸçŸ¥è¯†ã€‚</li>
<li>BOOSTæ¡†æ¶æå‡ºç”¨äºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œé€šè¿‡é›†æˆå£°æ˜åˆ†è§£å’Œä¿¡æ¯æ”¶é›†ç­–ç•¥æ¥æŒ‡å¯¼ç¨‹åºç”Ÿæˆã€‚</li>
<li>BOOSTé‡‡ç”¨ç­–ç•¥é©±åŠ¨å’Œæ•°æ®ä¸ºä¸­å¿ƒçš„æ–¹å¼è¿­ä»£ä¼˜åŒ–ç¤ºèŒƒï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚</li>
<li>BOOSTå®ç°äº†ä»é›¶æ ·æœ¬åˆ°å°‘æ•°æ ·æœ¬çš„æˆ˜ç•¥æ€§ç¨‹åºå¼•å¯¼å­¦ä¹ çš„æ— ç¼è¿‡æ¸¡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBOOSTåœ¨å¤æ‚å£°æ˜éªŒè¯æ–¹é¢ä¼˜äºå°‘æ•°æ ·æœ¬åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a5c192266e1860c5a77401245391d540.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a1e955f7979a2312daee97c56cca5bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed2210debdbbaafc3deb27f8bf5ac23d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CHARMS-Cognitive-Hierarchical-Agent-with-Reasoning-and-Motion-Styles"><a href="#CHARMS-Cognitive-Hierarchical-Agent-with-Reasoning-and-Motion-Styles" class="headerlink" title="CHARMS: Cognitive Hierarchical Agent with Reasoning and Motion Styles"></a>CHARMS: Cognitive Hierarchical Agent with Reasoning and Motion Styles</h2><p><strong>Authors:Jingyi Wang, Duanfeng Chu, Zejian Deng, Liping Lu</strong></p>
<p>To address the current challenges of low intelligence and simplistic vehicle behavior modeling in autonomous driving simulation scenarios, this paper proposes the Cognitive Hierarchical Agent with Reasoning and Motion Styles (CHARMS). The model can reason about the behavior of other vehicles like a human driver and respond with different decision-making styles, thereby improving the intelligence and diversity of the surrounding vehicles in the driving scenario. By introducing the Level-k behavioral game theory, the paper models the decision-making process of human drivers and employs deep reinforcement learning to train the models with diverse decision styles, simulating different reasoning approaches and behavioral characteristics. Building on the Poisson cognitive hierarchy theory, this paper also presents a novel driving scenario generation method. The method controls the proportion of vehicles with different driving styles in the scenario using Poisson and binomial distributions, thus generating controllable and diverse driving environments. Experimental results demonstrate that CHARMS not only exhibits superior decision-making capabilities as ego vehicles, but also generates more complex and diverse driving scenarios as surrounding vehicles. We will release code for CHARMS at <a target="_blank" rel="noopener" href="https://github.com/WUTAD-Wjy/CHARMS">https://github.com/WUTAD-Wjy/CHARMS</a>. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶æ¨¡æ‹Ÿåœºæ™¯ä¸­ä½æ™ºèƒ½å’Œç®€å•è½¦è¾†è¡Œä¸ºå»ºæ¨¡çš„å½“å‰æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†å¸¦æœ‰æ¨ç†å’Œè¿åŠ¨é£æ ¼çš„è®¤çŸ¥åˆ†å±‚ä»£ç†ï¼ˆCHARMSï¼‰ã€‚è¯¥æ¨¡å‹å¯ä»¥åƒäººç±»é©¾é©¶å‘˜ä¸€æ ·æ¨ç†å…¶ä»–è½¦è¾†çš„è¡Œä¸ºï¼Œå¹¶ä»¥ä¸åŒçš„å†³ç­–é£æ ¼åšå‡ºååº”ï¼Œä»è€Œæé«˜äº†é©¾é©¶åœºæ™¯ä¸­å‘¨å›´è½¦è¾†çš„æ™ºèƒ½å’Œå¤šæ ·æ€§ã€‚é€šè¿‡å¼•å…¥Level-kè¡Œä¸ºåšå¼ˆç†è®ºï¼Œè¯¥è®ºæ–‡å¯¹äººç±»é©¾é©¶å‘˜çš„å†³ç­–è¿‡ç¨‹è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶é‡‡ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒå…·æœ‰ä¸åŒå†³ç­–é£æ ¼çš„æ¨¡å‹ï¼Œæ¨¡æ‹Ÿä¸åŒçš„æ¨ç†æ–¹æ³•å’Œè¡Œä¸ºç‰¹å¾ã€‚åŸºäºPoissonè®¤çŸ¥å±‚æ¬¡ç†è®ºï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°çš„é©¾é©¶åœºæ™¯ç”Ÿæˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•ä½¿ç”¨Poissonå’ŒäºŒé¡¹åˆ†å¸ƒæ¥æ§åˆ¶åœºæ™¯ä¸­ä¸åŒé©¾é©¶é£æ ¼çš„è½¦è¾†æ¯”ä¾‹ï¼Œä»è€Œç”Ÿæˆå¯æ§ä¸”å¤šæ ·åŒ–çš„é©¾é©¶ç¯å¢ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCHARMSä¸ä»…ä½œä¸ºè‡ªæˆ‘è½¦è¾†è¡¨ç°å‡ºå“è¶Šçš„å†³ç­–èƒ½åŠ›ï¼Œè€Œä¸”ä½œä¸ºå‘¨å›´è½¦è¾†ç”Ÿæˆäº†æ›´å¤æ‚å’Œå¤šæ ·åŒ–çš„é©¾é©¶åœºæ™¯ã€‚æˆ‘ä»¬å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/WUTAD-Wjy/CHARMS%E4%B8%8A%E5%8F%91%E5%B8%83CHARMS%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/WUTAD-Wjy/CHARMSä¸Šå‘å¸ƒCHARMSçš„ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02450v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºè®¤çŸ¥å±‚æ¬¡ç»“æ„å’Œæ¨ç†ä¸è¿åŠ¨é£æ ¼çš„è‡ªä¸»é©¾é©¶æ¨¡æ‹Ÿæ¨¡å‹CHARMSï¼Œç”¨äºè§£å†³å½“å‰è‡ªä¸»é©¾é©¶æ¨¡æ‹Ÿåœºæ™¯ä¸­ä½æ™ºèƒ½å’Œè½¦è¾†è¡Œä¸ºå»ºæ¨¡è¿‡äºç®€å•çš„é—®é¢˜ã€‚CHARMSæ¨¡å‹èƒ½å¤Ÿåƒäººç±»é©¾é©¶å‘˜ä¸€æ ·å¯¹å…¶ä»–è½¦è¾†çš„è¡Œä¸ºè¿›è¡Œæ¨ç†ï¼Œå¹¶æ ¹æ®ä¸åŒçš„å†³ç­–é£æ ¼åšå‡ºå“åº”ï¼Œä»è€Œæé«˜åœºæ™¯ä¸­è½¦è¾†è¡Œä¸ºçš„æ™ºèƒ½æ€§å’Œå¤šæ ·æ€§ã€‚å¼•å…¥Level-kè¡Œä¸ºåšå¼ˆç†è®ºæ¥æ¨¡æ‹Ÿäººç±»é©¾é©¶å‘˜çš„å†³ç­–è¿‡ç¨‹ï¼Œå¹¶åº”ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å‹ï¼Œå½¢æˆä¸åŒçš„å†³ç­–é£æ ¼ã€‚åŒæ—¶ï¼ŒåŸºäºPoissonè®¤çŸ¥å±‚æ¬¡ç†è®ºæå‡ºäº†ä¸€ç§æ–°çš„é©¾é©¶åœºæ™¯ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡æ§åˆ¶ä¸åŒé©¾é©¶é£æ ¼çš„è½¦è¾†æ¯”ä¾‹ç”Ÿæˆå¯æ§ä¸”å¤šæ ·åŒ–çš„é©¾é©¶ç¯å¢ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCHARMSä¸ä»…ä½œä¸ºè‡ªä¸»è½¦è¾†çš„å†³ç­–èƒ½åŠ›å‡ºè‰²ï¼Œè€Œä¸”åœ¨ç”Ÿæˆå¤æ‚å¤šå˜çš„é©¾é©¶åœºæ™¯æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CHARMSæ¨¡å‹è¢«æå‡ºä»¥è§£å†³è‡ªä¸»é©¾é©¶æ¨¡æ‹Ÿä¸­çš„ä½æ™ºèƒ½å’Œç®€å•è½¦è¾†è¡Œä¸ºå»ºæ¨¡é—®é¢˜ã€‚</li>
<li>CHARMSèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»é©¾é©¶å‘˜çš„è¡Œä¸ºæ¨ç†å’Œå†³ç­–é£æ ¼ã€‚</li>
<li>Level-kè¡Œä¸ºåšå¼ˆç†è®ºç”¨äºæ¨¡æ‹Ÿäººç±»é©¾é©¶å‘˜çš„å†³ç­–è¿‡ç¨‹ã€‚</li>
<li>æ·±åº¦å¼ºåŒ–å­¦ä¹ è¢«ç”¨æ¥è®­ç»ƒæ¨¡å‹ï¼Œå½¢æˆä¸åŒçš„å†³ç­–é£æ ¼ã€‚</li>
<li>åŸºäºPoissonè®¤çŸ¥å±‚æ¬¡ç†è®ºæå‡ºäº†æ–°å‹é©¾é©¶åœºæ™¯ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆå¯æ§ä¸”å¤šæ ·åŒ–çš„é©¾é©¶ç¯å¢ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02450">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-00da2d4f95a29d7dd5813b05244115f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45217ef16780a49b075b8e7a372d1a89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e30df4f4256080a718979f47d407d21b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07b273a34cccafbad7547ef9b5d7bc06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c84fb924b0f5f13aaf804b2ba097bb4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f21d28d5ab28954f1bd77c9a1665337f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="F5R-TTS-Improving-Flow-Matching-based-Text-to-Speech-with-Group-Relative-Policy-Optimization"><a href="#F5R-TTS-Improving-Flow-Matching-based-Text-to-Speech-with-Group-Relative-Policy-Optimization" class="headerlink" title="F5R-TTS: Improving Flow Matching based Text-to-Speech with Group   Relative Policy Optimization"></a>F5R-TTS: Improving Flow Matching based Text-to-Speech with Group   Relative Policy Optimization</h2><p><strong>Authors:Xiaohui Sun, Ruitong Xiao, Jianye Mo, Bowen Wu, Qun Yu, Baoxun Wang</strong></p>
<p>We present F5R-TTS, a novel text-to-speech (TTS) system that integrates Gradient Reward Policy Optimization (GRPO) into a flow-matching based architecture. By reformulating the deterministic outputs of flow-matching TTS into probabilistic Gaussian distributions, our approach enables seamless integration of reinforcement learning algorithms. During pretraining, we train a probabilistically reformulated flow-matching based model which is derived from F5-TTS with an open-source dataset. In the subsequent reinforcement learning (RL) phase, we employ a GRPO-driven enhancement stage that leverages dual reward metrics: word error rate (WER) computed via automatic speech recognition and speaker similarity (SIM) assessed by verification models. Experimental results on zero-shot voice cloning demonstrate that F5R-TTS achieves significant improvements in both speech intelligibility (relatively 29.5% WER reduction) and speaker similarity (relatively 4.6% SIM score increase) compared to conventional flow-matching based TTS systems. Audio samples are available at <a target="_blank" rel="noopener" href="https://frontierlabs.github.io/F5R">https://frontierlabs.github.io/F5R</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†F5R-TTSï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿï¼Œå®ƒå°†æ¢¯åº¦å¥–åŠ±ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰é›†æˆåˆ°åŸºäºæµåŒ¹é…çš„æ¶æ„ä¸­ã€‚é€šè¿‡å°†å¯¹æµåŒ¹é…TTSçš„ç¡®å®šæ€§è¾“å‡ºé‡æ–°åˆ¶å®šä¸ºæ¦‚ç‡é«˜æ–¯åˆ†å¸ƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ— ç¼é›†æˆã€‚åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å¼€æºæ•°æ®é›†å¯¹åŸºäºæ¦‚ç‡æ”¹é©æµåŒ¹é…æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ¨¡å‹æºäºF5-TTSã€‚åœ¨éšåçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨GRPOé©±åŠ¨çš„å¢å¼ºé˜¶æ®µï¼Œåˆ©ç”¨åŒé‡å¥–åŠ±æŒ‡æ ‡ï¼šé€šè¿‡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«è®¡ç®—çš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å’Œé€šè¿‡éªŒè¯æ¨¡å‹è¯„ä¼°çš„æ¼”è®²è€…ç›¸ä¼¼æ€§ï¼ˆSIMï¼‰ã€‚åœ¨é›¶æ ·æœ¬å£°éŸ³å…‹éš†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„åŸºäºæµåŒ¹é…çš„TTSç³»ç»Ÿç›¸æ¯”ï¼ŒF5R-TTSåœ¨è¯­éŸ³æ¸…æ™°åº¦ï¼ˆç›¸å¯¹é™ä½29.5%çš„WERï¼‰å’Œæ¼”è®²è€…ç›¸ä¼¼æ€§ï¼ˆç›¸å¯¹å¢åŠ 4.6%çš„SIMåˆ†æ•°ï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://frontierlabs.github.io/F5R%E5%A4%84%E8%8E%B7%E5%8F%96%E3%80%82">https://frontierlabs.github.io/F5Rå¤„è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02407v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>F5R-TTSæ˜¯ä¸€ä¸ªé›†æˆäº†æ¢¯åº¦å¥–åŠ±ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„åŸºäºæµåŒ¹é…æ¶æ„çš„æ–°å‹æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿã€‚é€šè¿‡é‡‡ç”¨æ¦‚ç‡é«˜æ–¯åˆ†å¸ƒé‡æ–°æ„å»ºæµåŒ¹é…TTSçš„ç¡®å®šæ€§è¾“å‡ºï¼Œå®ç°äº†å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ— ç¼é›†æˆã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨å¼€æºæ•°æ®é›†å¯¹åŸºäºæµåŒ¹é…çš„æ¨¡å‹è¿›è¡Œæ¦‚ç‡é‡æ„ã€‚åœ¨éšåçš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œé‡‡ç”¨GRPOé©±åŠ¨çš„å¢å¼ºé˜¶æ®µï¼Œåˆ©ç”¨åŒå¥–åŠ±æŒ‡æ ‡ï¼šé€šè¿‡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«è®¡ç®—çš„å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å’Œé€šè¿‡éªŒè¯æ¨¡å‹è¯„ä¼°çš„è¯´è¯äººç›¸ä¼¼æ€§ï¼ˆSIMï¼‰ã€‚åœ¨é›¶æ ·æœ¬è¯­éŸ³å…‹éš†æ–¹é¢çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒF5R-TTSåœ¨è¯­éŸ³æ¸…æ™°åº¦æ–¹é¢å®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ˆç›¸å¯¹é™ä½äº†29.5%çš„WERï¼‰ï¼Œå¹¶ä¸”åœ¨è¯´è¯äººç›¸ä¼¼æ€§æ–¹é¢ä¹Ÿæœ‰ç›¸å¯¹æé«˜ï¼ˆSIMå¾—åˆ†å¢åŠ äº†4.6%ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>F5R-TTSæ˜¯ä¸€ä¸ªç»“åˆäº†æ¢¯åº¦å¥–åŠ±ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„æ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿã€‚</li>
<li>é€šè¿‡å°†æµåŒ¹é…TTSçš„ç¡®å®šæ€§è¾“å‡ºè½¬åŒ–ä¸ºæ¦‚ç‡é«˜æ–¯åˆ†å¸ƒï¼Œä½¿å¾—å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯ä»¥æ— ç¼é›†æˆã€‚</li>
<li>ç³»ç»Ÿç»è¿‡é¢„è®­ç»ƒé˜¶æ®µï¼Œåˆ©ç”¨å¼€æºæ•°æ®é›†å¯¹åŸºäºæµåŒ¹é…çš„æ¨¡å‹è¿›è¡Œæ¦‚ç‡é‡æ„ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ é˜¶æ®µé‡‡ç”¨GRPOé©±åŠ¨çš„å¢å¼ºé˜¶æ®µï¼Œåˆ©ç”¨å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å’Œè¯´è¯äººç›¸ä¼¼æ€§ï¼ˆSIMï¼‰ä½œä¸ºåŒå¥–åŠ±æŒ‡æ ‡ã€‚</li>
<li>F5R-TTSåœ¨é›¶æ ·æœ¬è¯­éŸ³å…‹éš†æ–¹é¢çš„å®éªŒç»“æœè¡¨æ˜å…¶æ˜¾è‘—æé«˜äº†è¯­éŸ³æ¸…æ™°åº¦å’Œè¯´è¯äººç›¸ä¼¼æ€§ã€‚</li>
<li>F5R-TTSç›¸å¯¹ä¼ ç»Ÿçš„æµåŒ¹é…TTSç³»ç»Ÿæœ‰æ˜æ˜¾çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02407">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3f531372532fe4c4c8f59937e32c207d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf599466467c1a34ae35a74ee566b760.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5f88421f16b25d3ba91405a249ffd86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23ad0ad70d5d46832db0d11e5dfb6ea0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d12144657fdb0de65af673114315066a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="AnesBench-Multi-Dimensional-Evaluation-of-LLM-Reasoning-in-Anesthesiology"><a href="#AnesBench-Multi-Dimensional-Evaluation-of-LLM-Reasoning-in-Anesthesiology" class="headerlink" title="AnesBench: Multi-Dimensional Evaluation of LLM Reasoning in   Anesthesiology"></a>AnesBench: Multi-Dimensional Evaluation of LLM Reasoning in   Anesthesiology</h2><p><strong>Authors:Xiang Feng, Wentao Jiang, Zengmao Wang, Yong Luo, Pingbo Xu, Baosheng Yu, Hua Jin, Bo Du, Jing Zhang</strong></p>
<p>The application of large language models (LLMs) in the medical field has gained significant attention, yet their reasoning capabilities in more specialized domains like anesthesiology remain underexplored. In this paper, we systematically evaluate the reasoning capabilities of LLMs in anesthesiology and analyze key factors influencing their performance. To this end, we introduce AnesBench, a cross-lingual benchmark designed to assess anesthesiology-related reasoning across three levels: factual retrieval (System 1), hybrid reasoning (System 1.x), and complex decision-making (System 2). Through extensive experiments, we first explore how model characteristics, including model scale, Chain of Thought (CoT) length, and language transferability, affect reasoning performance. Then, we further evaluate the effectiveness of different training strategies, leveraging our curated anesthesiology-related dataset, including continuous pre-training (CPT) and supervised fine-tuning (SFT). Additionally, we also investigate how the test-time reasoning techniques, such as Best-of-N sampling and beam search, influence reasoning performance, and assess the impact of reasoning-enhanced model distillation, specifically DeepSeek-R1. We will publicly release AnesBench, along with our CPT and SFT training datasets and evaluation code at <a target="_blank" rel="noopener" href="https://github.com/MiliLab/AnesBench">https://github.com/MiliLab/AnesBench</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨å·²å¼•èµ·å¹¿æ³›å…³æ³¨ï¼Œä½†å®ƒä»¬åœ¨éº»é†‰å­¦ç­‰æ›´ä¸“ä¸šé¢†åŸŸçš„æ¨ç†èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†LLMåœ¨éº»é†‰å­¦ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åˆ†æäº†å½±å“å®ƒä»¬æ€§èƒ½çš„å…³é”®å› ç´ ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†AnesBenchï¼Œè¿™æ˜¯ä¸€ä¸ªè·¨è¯­è¨€åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°éº»é†‰å­¦ç›¸å…³çš„ä¸‰ä¸ªå±‚æ¬¡çš„æ¨ç†èƒ½åŠ›ï¼šäº‹å®æ£€ç´¢ï¼ˆç³»ç»Ÿ1ï¼‰ã€æ··åˆæ¨ç†ï¼ˆç³»ç»Ÿ1.xï¼‰å’Œå¤æ‚å†³ç­–ï¼ˆç³»ç»Ÿ2ï¼‰ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬é¦–å…ˆæ¢ç´¢äº†æ¨¡å‹ç‰¹æ€§ï¼ŒåŒ…æ‹¬æ¨¡å‹è§„æ¨¡ã€æ€ç»´é“¾ï¼ˆCoTï¼‰é•¿åº¦å’Œè¯­è¨€å¯è¿ç§»æ€§å¦‚ä½•å½±å“æ¨ç†æ€§èƒ½ã€‚ç„¶åï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ä½¿ç”¨æˆ‘ä»¬ç²¾é€‰çš„éº»é†‰å­¦ç›¸å…³æ•°æ®é›†è¯„ä¼°äº†ä¸åŒçš„è®­ç»ƒç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰å’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†æµ‹è¯•æ—¶çš„æ¨ç†æŠ€æœ¯ï¼Œå¦‚Né€‰æœ€ä½³é‡‡æ ·å’Œé›†æŸæœç´¢å¦‚ä½•å½±å“æ¨ç†æ€§èƒ½ï¼Œå¹¶è¯„ä¼°äº†æ¨ç†å¢å¼ºæ¨¡å‹è’¸é¦ï¼Œç‰¹åˆ«æ˜¯DeepSeek-R1çš„å½±å“ã€‚æˆ‘ä»¬å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/MiliLab/AnesBench">https://github.com/MiliLab/AnesBench</a>ä¸Šå…¬å¼€å‘å¸ƒAnesBenchï¼Œä»¥åŠæˆ‘ä»¬çš„CPTå’ŒSFTè®­ç»ƒæ•°æ®é›†å’Œè¯„ä¼°ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02404v1">PDF</a> 23 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»å­¦é¢†åŸŸçš„åº”ç”¨å·²å¼•èµ·å¹¿æ³›å…³æ³¨ï¼Œä½†åœ¨éº»é†‰å­¦ç­‰ä¸“ä¸šé¢†åŸŸä¸­çš„æ¨ç†èƒ½åŠ›å°šå¾…æ¢ç´¢ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°è¯„ä¼°äº†LLMsåœ¨éº»é†‰å­¦ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åˆ†æäº†å½±å“å®ƒä»¬æ€§èƒ½çš„å…³é”®å› ç´ ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AnesBenchï¼Œä¸€ä¸ªè·¨è¯­è¨€åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°éº»é†‰å­¦ç›¸å…³çš„ä¸‰ä¸ªçº§åˆ«çš„æ¨ç†èƒ½åŠ›ï¼šäº‹å®æ£€ç´¢ï¼ˆSystem 1ï¼‰ã€æ··åˆæ¨ç†ï¼ˆSystem 1.xï¼‰å’Œå¤æ‚å†³ç­–åˆ¶å®šï¼ˆSystem 2ï¼‰ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬æ¢è®¨äº†æ¨¡å‹ç‰¹æ€§ã€åŒ…æ‹¬æ¨¡å‹è§„æ¨¡ã€æ€ç»´é“¾é•¿åº¦å’Œè¯­è¨€å¯è¿ç§»æ€§å¯¹æ¨ç†æ€§èƒ½çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†ä½¿ç”¨æˆ‘ä»¬æ•´ç†çš„éº»é†‰å­¦ç›¸å…³æ•°æ®é›†çš„ä¸åŒè®­ç»ƒç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰å’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éº»é†‰å­¦é¢†åŸŸçš„æ¨ç†èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>æ¨å‡ºäº†AnesBenchè·¨è¯­è¨€åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°éº»é†‰å­¦ç›¸å…³çš„ä¸‰ä¸ªçº§åˆ«çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹è§„æ¨¡ã€æ€ç»´é“¾é•¿åº¦å’Œè¯­è¨€å¯è¿ç§»æ€§æ˜¯å½±å“LLMsåœ¨éº»é†‰å­¦é¢†åŸŸæ¨ç†æ€§èƒ½çš„å…³é”®å› ç´ ã€‚</li>
<li>ä¸åŒçš„è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰å’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå¯¹LLMsçš„æ¨ç†èƒ½åŠ›æœ‰å½±å“ã€‚</li>
<li>æµ‹è¯•æ—¶çš„æ¨ç†æŠ€æœ¯ï¼Œå¦‚Best-of-Né‡‡æ ·å’Œé›†æŸæœç´¢ï¼Œä¹Ÿä¼šå½±å“æ¨ç†æ€§èƒ½ã€‚</li>
<li>æ¨ç†å¢å¼ºæ¨¡å‹è’¸é¦ï¼Œç‰¹åˆ«æ˜¯DeepSeek-R1ï¼Œå¯¹LLMsåœ¨éº»é†‰å­¦é¢†åŸŸçš„æ¨ç†æ€§èƒ½æœ‰æå‡ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02404">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ce8ff264396fcfd5004160cdd3558dc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46c95381762ae71ec24c9a46ef544b92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ace1756f2bf71e7f2c8974cbc3188f74.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CrystalFormer-RL-Reinforcement-Fine-Tuning-for-Materials-Design"><a href="#CrystalFormer-RL-Reinforcement-Fine-Tuning-for-Materials-Design" class="headerlink" title="CrystalFormer-RL: Reinforcement Fine-Tuning for Materials Design"></a>CrystalFormer-RL: Reinforcement Fine-Tuning for Materials Design</h2><p><strong>Authors:Zhendong Cao, Lei Wang</strong></p>
<p>Reinforcement fine-tuning has instrumental enhanced the instruction-following and reasoning abilities of large language models. In this work, we explore the applications of reinforcement fine-tuning to the autoregressive transformer-based materials generative model CrystalFormer (arXiv:2403.15734) using discriminative machine learning models such as interatomic potentials and property prediction models. By optimizing reward signals-such as energy above the convex hull and material property figures of merit-reinforcement fine-tuning infuses knowledge from discriminative models into generative models. The resulting model, CrystalFormer-RL, shows enhanced stability in generated crystals and successfully discovers crystals with desirable yet conflicting material properties, such as substantial dielectric constant and band gap simultaneously. Notably, we observe that reinforcement fine-tuning enables not only the property-guided novel material design ability of generative pre-trained model but also unlocks property-driven material retrieval from the unsupervised pre-training dataset. Leveraging rewards from discriminative models to fine-tune materials generative models opens an exciting gateway to the synergies of the machine learning ecosystem for materials. </p>
<blockquote>
<p>å¼ºåŒ–å¾®è°ƒï¼ˆReinforcement fine-tuningï¼‰å·²ç»å¤§å¤§æé«˜äº†å¤§è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¼ºåŒ–å¾®è°ƒåœ¨åŸºäºè‡ªå›å½’è½¬æ¢å™¨ï¼ˆautoregressive transformerï¼‰çš„ææ–™ç”Ÿæˆæ¨¡å‹CrystalFormerï¼ˆarXiv:2403.1473ï¼‰ä¸­çš„åº”ç”¨ï¼Œé‡‡ç”¨åˆ¤åˆ«æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¦‚åŸå­é—´åŠ¿èƒ½å’Œå±æ€§é¢„æµ‹æ¨¡å‹ã€‚é€šè¿‡ä¼˜åŒ–å¥–åŠ±ä¿¡å·ï¼ˆå¦‚å‡¸åŒ…ä¸Šèƒ½é‡å’Œææ–™å±æ€§æ€§èƒ½æŒ‡æ ‡ï¼‰ï¼Œå¼ºåŒ–å¾®è°ƒå°†åˆ¤åˆ«æ¨¡å‹çš„çŸ¥è¯†æ³¨å…¥ç”Ÿæˆæ¨¡å‹ä¸­ã€‚å¾—åˆ°çš„æ¨¡å‹CrystalFormer-RLåœ¨ç”Ÿæˆçš„æ™¶ä½“ä¸­è¡¨ç°å‡ºæ›´é«˜çš„ç¨³å®šæ€§ï¼Œå¹¶èƒ½æˆåŠŸå‘ç°å…·æœ‰ç†æƒ³ä½†ç›¸äº’å†²çªçš„ææ–™å±æ€§çš„æ™¶ä½“ï¼Œä¾‹å¦‚å…·æœ‰ç›¸å½“å¤§çš„ä»‹ç”µå¸¸æ•°å’Œå¸¦éš™ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å¼ºåŒ–å¾®è°ƒä¸ä»…èµ‹äºˆäº†é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹ä»¥å±æ€§ä¸ºå¯¼å‘çš„æ–°å‹ææ–™è®¾è®¡èƒ½åŠ›ï¼Œè¿˜è§£é”äº†ä»éç›‘ç£é¢„è®­ç»ƒæ•°æ®é›†ä¸­è¿›è¡Œå±æ€§é©±åŠ¨çš„ææ–™æ£€ç´¢ã€‚åˆ©ç”¨åˆ¤åˆ«æ¨¡å‹çš„å¥–åŠ±æ¥å¾®è°ƒææ–™ç”Ÿæˆæ¨¡å‹ï¼Œä¸ºæœºå™¨å­¦ä¹ ç”Ÿæ€ç³»ç»Ÿåœ¨ææ–™é¢†åŸŸçš„ååŒä½œç”¨æ‰“å¼€äº†ä»¤äººå…´å¥‹çš„å¤§é—¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02367v1">PDF</a> 8 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å¾®è°ƒæé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†å°†å¼ºåŒ–å¾®è°ƒåº”ç”¨äºåŸºäºè‡ªå›å½’transformerçš„ææ–™ç”Ÿæˆæ¨¡å‹CrystalFormerï¼ˆarXiv:2403.15734ï¼‰ï¼ŒåŒæ—¶ä½¿ç”¨åˆ¤åˆ«å¼æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¦‚åŸå­é—´åŠ¿å’Œå±æ€§é¢„æµ‹æ¨¡å‹ã€‚é€šè¿‡ä¼˜åŒ–å¥–åŠ±ä¿¡å·ï¼Œå¦‚å‡¸åŒ…ä¸Šæ–¹çš„èƒ½é‡å’Œç‰©è´¨å±æ€§è¯„ä¼°æŒ‡æ ‡ï¼Œå¼ºåŒ–å¾®è°ƒå°†åˆ¤åˆ«æ¨¡å‹çš„çŸ¥è¯†æ³¨å…¥ç”Ÿæˆæ¨¡å‹ä¸­ã€‚ç»“æœæ¨¡å‹CrystalFormer-RLåœ¨ç”Ÿæˆçš„æ™¶ä½“ä¸­è¡¨ç°å‡ºå¢å¼ºçš„ç¨³å®šæ€§ï¼Œå¹¶æˆåŠŸå‘ç°äº†å…·æœ‰ç†æƒ³ä½†ç›¸äº’å†²çªçš„ææ–™å±æ€§çš„æ™¶ä½“ï¼Œå¦‚é«˜ä»‹ç”µå¸¸æ•°å’Œå®½ç¦å¸¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°å¼ºåŒ–å¾®è°ƒä¸ä»…èµ‹äºˆäº†é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹ä»¥å±æ€§ä¸ºå¯¼å‘çš„æ–°å‹ææ–™è®¾è®¡èƒ½åŠ›ï¼Œè¿˜å¯ç”¨äº†ä»æ— ç›‘ç£é¢„è®­ç»ƒæ•°æ®é›†ä¸­è¿›è¡Œå±æ€§é©±åŠ¨çš„ææ–™æ£€ç´¢ã€‚åˆ©ç”¨åˆ¤åˆ«æ¨¡å‹çš„å¥–åŠ±å¯¹ææ–™ç”Ÿæˆæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä¸ºæœºå™¨å­¦ä¹ ç”Ÿæ€ç³»ç»Ÿåœ¨ææ–™é¢†åŸŸçš„ååŒä½œç”¨æ‰“å¼€äº†ä»¤äººå…´å¥‹çš„å¤§é—¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å¾®è°ƒå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶å°†å¼ºåŒ–å¾®è°ƒåº”ç”¨äºåŸºäºè‡ªå›å½’transformerçš„ææ–™ç”Ÿæˆæ¨¡å‹CrystalFormerã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–å¥–åŠ±ä¿¡å·ï¼Œå¦‚å‡¸åŒ…ä¸Šæ–¹çš„èƒ½é‡å’Œç‰©è´¨å±æ€§è¯„ä¼°æŒ‡æ ‡ï¼Œå®ç°ç”Ÿæˆæ¨¡å‹çš„å¼ºåŒ–å¾®è°ƒã€‚</li>
<li>å¼ºåŒ–å¾®è°ƒåçš„æ¨¡å‹CrystalFormer-RLåœ¨ç”Ÿæˆçš„æ™¶ä½“ä¸­è¡¨ç°å‡ºå¢å¼ºçš„ç¨³å®šæ€§ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿå‘ç°å…·æœ‰ç†æƒ³ä½†ç›¸äº’å†²çªçš„ææ–™å±æ€§çš„æ™¶ä½“ï¼Œå¦‚åŒæ—¶å…·æœ‰é«˜ä»‹ç”µå¸¸æ•°å’Œå®½ç¦å¸¦çš„æ™¶ä½“ã€‚</li>
<li>å¼ºåŒ–å¾®è°ƒèµ‹äºˆäº†é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹ä»¥å±æ€§ä¸ºå¯¼å‘çš„æ–°å‹ææ–™è®¾è®¡èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02367">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6c03cdc2f0a0b30291c094029c24ca75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8eed197f83bf24dd13e112a9a3a5f68e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cc5a3d8fe8d7c8a3d20d263ef7c5200.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c002685956b80cd8c0a86ab07b62e6af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59bb8ccdcbbca67ad936f7c799c1e062.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a507c2e126a97016817fa0fb9b47c9c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="More-is-Less-The-Pitfalls-of-Multi-Model-Synthetic-Preference-Data-in-DPO-Safety-Alignment"><a href="#More-is-Less-The-Pitfalls-of-Multi-Model-Synthetic-Preference-Data-in-DPO-Safety-Alignment" class="headerlink" title="More is Less: The Pitfalls of Multi-Model Synthetic Preference Data in   DPO Safety Alignment"></a>More is Less: The Pitfalls of Multi-Model Synthetic Preference Data in   DPO Safety Alignment</h2><p><strong>Authors:Yifan Wang, Runjin Chen, Bolian Li, David Cho, Yihe Deng, Ruqi Zhang, Tianlong Chen, Zhangyang Wang, Ananth Grama, Junyuan Hong</strong></p>
<p>Aligning large language models (LLMs) with human values is an increasingly critical step in post-training. Direct Preference Optimization (DPO) has emerged as a simple, yet effective alternative to reinforcement learning from human feedback (RLHF). Synthetic preference data with its low cost and high quality enable effective alignment through single- or multi-model generated preference data. Our study reveals a striking, safety-specific phenomenon associated with DPO alignment: Although multi-model generated data enhances performance on general tasks (ARC, Hellaswag, MMLU, TruthfulQA, Winogrande) by providing diverse responses, it also tends to facilitate reward hacking during training. This can lead to a high attack success rate (ASR) when models encounter jailbreaking prompts. The issue is particularly pronounced when employing stronger models like GPT-4o or larger models in the same family to generate chosen responses paired with target model self-generated rejected responses, resulting in dramatically poorer safety outcomes. Furthermore, with respect to safety, using solely self-generated responses (single-model generation) for both chosen and rejected pairs significantly outperforms configurations that incorporate responses from stronger models, whether used directly as chosen data or as part of a multi-model response pool. We demonstrate that multi-model preference data exhibits high linear separability between chosen and rejected responses, which allows models to exploit superficial cues rather than internalizing robust safety constraints. Our experiments, conducted on models from the Llama, Mistral, and Qwen families, consistently validate these findings. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚å¯¹é½æ˜¯è®­ç»ƒåè¶Šæ¥è¶Šå…³é”®çš„ä¸€æ­¥ã€‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä½œä¸ºäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰çš„ç®€å•æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆè€Œå‡ºç°ã€‚åˆæˆåå¥½æ•°æ®ä»¥å…¶ä½æˆæœ¬ã€é«˜è´¨é‡çš„ç‰¹ç‚¹ï¼Œèƒ½å¤Ÿé€šè¿‡å•ä¸€æˆ–å¤šç§æ¨¡å‹ç”Ÿæˆçš„åå¥½æ•°æ®å®ç°æœ‰æ•ˆçš„å¯¹é½ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†ä¸€ä¸ªä¸DPOå¯¹é½ç›¸å…³çš„ä»¤äººæƒŠè®¶çš„ç‰¹å®šå®‰å…¨ç°è±¡ï¼šè™½ç„¶å¤šæ¨¡å‹ç”Ÿæˆçš„æ•°æ®é€šè¿‡æä¾›å¤šæ ·åŒ–çš„å›åº”æé«˜äº†åœ¨ä¸€èˆ¬ä»»åŠ¡ï¼ˆARCã€Hellaswagã€MMLUã€TruthfulQAã€Winograndeï¼‰ä¸Šçš„æ€§èƒ½ï¼Œä½†å®ƒä¹Ÿå€¾å‘äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿ƒè¿›å¥–åŠ±ç ´è§£ã€‚å½“æ¨¡å‹é‡åˆ°è¶Šç‹±æç¤ºæ—¶ï¼Œè¿™å¯èƒ½å¯¼è‡´è¾ƒé«˜çš„æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ã€‚å½“ä½¿ç”¨æ›´å¼ºæˆ–åŒä¸€ç³»åˆ—ä¸­æ›´å¤§çš„æ¨¡å‹æ¥ç”Ÿæˆé€‰æ‹©çš„å“åº”ä¸ç›®æ ‡çš„è‡ªæˆ‘ç”Ÿæˆè¢«æ‹’ç»å“åº”é…å¯¹æ—¶ï¼Œé—®é¢˜å°¤ä¸ºçªå‡ºï¼Œå¯¼è‡´å®‰å…¨ç»“æœæ˜¾è‘—è¾ƒå·®ã€‚æ­¤å¤–ï¼Œåœ¨å®‰å…¨æ€§æ–¹é¢ï¼Œä»…ä½¿ç”¨è‡ªæˆ‘ç”Ÿæˆçš„å“åº”ï¼ˆå•ä¸€æ¨¡å‹ç”Ÿæˆï¼‰ä½œä¸ºé€‰æ‹©å’Œæ‹’ç»å¯¹çš„é…å¯¹ï¼Œæ˜¾è‘—ä¼˜äºå°†å“åº”çº³å…¥æ›´å¼ºæ¨¡å‹çš„é…ç½®ï¼Œæ— è®ºè¿™äº›å“åº”æ˜¯ç›´æ¥ç”¨ä½œé€‰æ‹©æ•°æ®è¿˜æ˜¯ä½œä¸ºå¤šæ¨¡å‹å“åº”æ± çš„ä¸€éƒ¨åˆ†ã€‚æˆ‘ä»¬è¯æ˜å¤šæ¨¡å‹åå¥½æ•°æ®åœ¨é€‰æ‹©å’Œæ‹’ç»å“åº”ä¹‹é—´è¡¨ç°å‡ºé«˜çº¿æ€§å¯åˆ†ç¦»æ€§ï¼Œè¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨è¡¨é¢çº¿ç´¢ï¼Œè€Œä¸æ˜¯å†…åŒ–ç¨³å¥çš„å®‰å…¨çº¦æŸã€‚æˆ‘ä»¬åœ¨Llamaã€Mistralå’ŒQwenç³»åˆ—æ¨¡å‹ä¸Šè¿›è¡Œçš„å®éªŒå§‹ç»ˆéªŒè¯äº†è¿™äº›å‘ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02193v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚çš„å¯¹é½æ˜¯è®­ç»ƒåçš„é‡è¦æ­¥éª¤ã€‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä½œä¸ºäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰çš„ç®€å•æœ‰æ•ˆæ›¿ä»£æ–¹æ³•ï¼Œé€šè¿‡ä½æˆæœ¬ã€é«˜è´¨é‡çš„äººå·¥åˆæˆåå¥½æ•°æ®å®ç°äº†æœ‰æ•ˆçš„å¯¹é½ã€‚ç„¶è€Œï¼Œç ”ç©¶å‘ç°DPOå¯¹é½ä¸­å­˜åœ¨ä¸€ä¸ªç‰¹å®šå®‰å…¨ç°è±¡ï¼šè™½ç„¶å¤šæ¨¡å‹ç”Ÿæˆçš„æ•°æ®å¯ä»¥æé«˜ä¸€èˆ¬ä»»åŠ¡æ€§èƒ½ï¼Œä½†å®ƒä¹Ÿå€¾å‘äºåœ¨è®­ç»ƒæœŸé—´ä¿ƒè¿›å¥–åŠ±é»‘å®¢æ”»å‡»ã€‚è¿™ä¼šå¯¼è‡´æ¨¡å‹åœ¨é‡åˆ°è¶Šç‹±æç¤ºæ—¶çš„é«˜æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ã€‚ç‰¹åˆ«æ˜¯å½“ä½¿ç”¨æ›´å¼ºæˆ–åŒå®¶æ—çš„å¤§å‹æ¨¡å‹æ¥ç”Ÿæˆé€‰å®šå“åº”ä¸ç›®æ ‡æ¨¡å‹è‡ªæˆ‘ç”Ÿæˆçš„æ‹’ç»å“åº”é…å¯¹æ—¶ï¼Œå®‰å…¨ç»“æœä¼šæ˜¾è‘—æ¶åŒ–ã€‚åœ¨å®‰å…¨æ–¹é¢ï¼Œä»…ä½¿ç”¨è‡ªæˆ‘ç”Ÿæˆçš„å“åº”ï¼ˆå•æ¨¡å‹ç”Ÿæˆï¼‰å¯¹äºé€‰å®šå’Œæ‹’ç»å¯¹çš„é…å¯¹æ˜¾è‘—ä¼˜äºç»“åˆä½¿ç”¨æ›´å¼ºæ¨¡å‹å“åº”çš„é…ç½®ã€‚å®éªŒè¡¨æ˜å¤šæ¨¡å‹åå¥½æ•°æ®åœ¨é€‰å®šå’Œæ‹’ç»å“åº”ä¹‹é—´å…·æœ‰é«˜çº¿æ€§å¯åˆ†ç¦»æ€§ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥åˆ©ç”¨è¡¨é¢çº¿ç´¢è€Œéå†…åŒ–ç¨³å¥çš„å®‰å…¨çº¦æŸã€‚å¯¹æ­¤å‘ç°ï¼ŒLlamaã€Mistralå’ŒQwenç­‰å®¶æ—æ¨¡å‹çš„å®éªŒå‡å¾—åˆ°äº†ä¸€è‡´éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DPOæ˜¯ä¸€ç§æœ‰æ•ˆçš„LLMä¸äººç±»ä»·å€¼è§‚å¯¹é½æ–¹æ³•ï¼Œåˆ©ç”¨ä½æˆæœ¬é«˜è´¨é‡çš„åˆæˆåå¥½æ•°æ®ã€‚</li>
<li>å¤šæ¨¡å‹ç”Ÿæˆæ•°æ®èƒ½æé«˜ä¸€èˆ¬ä»»åŠ¡æ€§èƒ½ï¼Œä½†å¯èƒ½ä¿ƒè¿›å¥–åŠ±é»‘å®¢æ”»å‡»ï¼Œå¯¼è‡´é«˜æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ã€‚</li>
<li>ä½¿ç”¨æ›´å¼ºæˆ–åŒå®¶æ—çš„å¤§å‹æ¨¡å‹ç”Ÿæˆé€‰å®šä¸æ‹’ç»å“åº”é…å¯¹æ—¶ï¼Œå®‰å…¨ç»“æœæ˜¾è‘—æ¶åŒ–ã€‚</li>
<li>åœ¨å®‰å…¨æ–¹é¢ï¼Œä»…ä½¿ç”¨è‡ªæˆ‘ç”Ÿæˆçš„å“åº”ï¼ˆå•æ¨¡å‹ç”Ÿæˆï¼‰è¡¨ç°æœ€ä½³ã€‚</li>
<li>å¤šæ¨¡å‹åå¥½æ•°æ®åœ¨é€‰å®šå’Œæ‹’ç»å“åº”é—´å…·æœ‰é«˜çº¿æ€§å¯åˆ†ç¦»æ€§ï¼Œä½¿æ¨¡å‹å€¾å‘äºåˆ©ç”¨è¡¨é¢çº¿ç´¢è€Œéå†…åŒ–å®‰å…¨çº¦æŸã€‚</li>
<li>è¿™ä¸€ç°è±¡åœ¨å¤šç§ä¸åŒå®¶æ—çš„æ¨¡å‹ä¸­å‡å¾—åˆ°éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02193">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4c7fdc4fa8c71696ea7d8075ba1af889.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a807dd67382e6f2d5c156b80d246d81.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-761b61feaa6240176d90fa79993e111e" align="middle">
<img src="https://picx.zhimg.com/v2-9fe219d954bf68f34e284fbbfa15626a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Exploring-LLM-Reasoning-Through-Controlled-Prompt-Variations"><a href="#Exploring-LLM-Reasoning-Through-Controlled-Prompt-Variations" class="headerlink" title="Exploring LLM Reasoning Through Controlled Prompt Variations"></a>Exploring LLM Reasoning Through Controlled Prompt Variations</h2><p><strong>Authors:Giannis Chatziveroglou, Richard Yun, Maura Kelleher</strong></p>
<p>This study investigates the reasoning robustness of large language models (LLMs) on mathematical problem-solving tasks under systematically introduced input perturbations. Using the GSM8K dataset as a controlled testbed, we evaluate how well state-of-the-art models maintain logical consistency and correctness when confronted with four categories of prompt perturbations: irrelevant context, pathological instructions, factually relevant but non-essential context, and a combination of the latter two. Our experiments, conducted on thirteen open-source and closed-source LLMs, reveal that introducing irrelevant context within the modelâ€™s context window significantly degrades performance, suggesting that distinguishing essential from extraneous details remains a pressing challenge. Surprisingly, performance regressions are relatively insensitive to the complexity of the reasoning task, as measured by the number of steps required, and are not strictly correlated with model size. Moreover, we observe that certain perturbations inadvertently trigger chain-of-thought-like reasoning behaviors, even without explicit prompting. Our findings highlight critical vulnerabilities in current LLMs and underscore the need for improved robustness against noisy, misleading, and contextually dense inputs, paving the way for more resilient and reliable reasoning in real-world applications. </p>
<blockquote>
<p>æœ¬ç ”ç©¶è°ƒæŸ¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç³»ç»Ÿå¼•å…¥è¾“å…¥æ‰°åŠ¨çš„æƒ…å†µä¸‹è§£å†³æ•°å­¦é—®é¢˜çš„æ¨ç†ç¨³å¥æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨GSM8Kæ•°æ®é›†ä½œä¸ºå—æ§æµ‹è¯•å¹³å°ï¼Œè¯„ä¼°å…ˆè¿›æ¨¡å‹åœ¨é¢å¯¹å››ç±»æç¤ºæ‰°åŠ¨æ—¶ï¼Œå¦‚ä½•ä¿æŒé€»è¾‘ä¸€è‡´æ€§å’Œæ­£ç¡®æ€§ï¼šæ— å…³ä¸Šä¸‹æ–‡ã€ç—…æ€æŒ‡ä»¤ã€äº‹å®ç›¸å…³ä½†éå¿…è¦ä¸Šä¸‹æ–‡ï¼Œä»¥åŠåä¸¤è€…çš„ç»„åˆã€‚æˆ‘ä»¬åœ¨13ä¸ªå¼€æºå’Œé—­æºLLMä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œåœ¨æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£ä¸­å¼•å…¥æ— å…³ä¸Šä¸‹æ–‡ä¼šæ˜¾è‘—é™ä½æ€§èƒ½ï¼Œè¿™è¡¨æ˜åŒºåˆ†å¿…è¦ç»†èŠ‚å’Œå¤šä½™ç»†èŠ‚ä»ç„¶æ˜¯ä¸€ä¸ªç´§è¿«çš„æŒ‘æˆ˜ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæ€§èƒ½å›å½’å¯¹æ‰€éœ€æ­¥éª¤æ•°é‡çš„æ¨ç†ä»»åŠ¡å¤æ‚æ€§çš„æ•æ„Ÿåº¦ç›¸å¯¹è¾ƒä½ï¼Œå¹¶ä¸”ä¸æ¨¡å‹å¤§å°æ²¡æœ‰ä¸¥æ ¼çš„ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æŸäº›æ‰°åŠ¨ä¼šæ— æ„ä¸­è§¦å‘ç±»ä¼¼æ€ç»´é“¾çš„æ¨ç†è¡Œä¸ºï¼Œå³ä½¿æ²¡æœ‰æ˜ç¡®çš„æç¤ºã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†å½“å‰LLMçš„å…³é”®æ¼æ´ï¼Œå¹¶å¼ºè°ƒäº†æé«˜å¯¹æŠ—å˜ˆæ‚ã€è¯¯å¯¼æ€§å’Œä¸Šä¸‹æ–‡å¯†é›†è¾“å…¥çš„ç¨³å¥æ€§çš„å¿…è¦æ€§ï¼Œä¸ºåœ¨å®é™…åº”ç”¨ä¸­å®ç°æ›´å…·å¼¹æ€§å’Œå¯é çš„æ¨ç†é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02111v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³æ•°å­¦é—®é¢˜ä»»åŠ¡æ—¶çš„æ¨ç†ç¨³å¥æ€§å—åˆ°ç ”ç©¶ã€‚é€šè¿‡å¯¹GSM8Kæ•°æ®é›†çš„ç³»ç»Ÿè¾“å…¥æ‰°åŠ¨æµ‹è¯•ï¼Œè¯„ä¼°äº†å…ˆè¿›æ¨¡å‹åœ¨é¢å¯¹å››ç§ä¸åŒç±»åˆ«çš„æç¤ºæ‰°åŠ¨æ—¶çš„é€»è¾‘è¿è´¯æ€§å’Œæ­£ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæ¨¡å‹ä¸Šä¸‹æ–‡çª—å£å†…å¼•å…¥ä¸ç›¸å…³ä¸Šä¸‹æ–‡ä¼šæ˜¾è‘—å½±å“æ€§èƒ½ï¼ŒåŒºåˆ†å…³é”®ä¿¡æ¯ä¸ç»†èŠ‚ä»ç„¶æ˜¯ä¸€ä¸ªç´§è¿«çš„æŒ‘æˆ˜ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæ€§èƒ½å›å½’å¯¹æ‰€éœ€æ­¥éª¤æ•°çš„æ¨ç†ä»»åŠ¡å¤æ‚åº¦å¹¶ä¸æ•æ„Ÿï¼Œå¹¶ä¸”ä¸æ¨¡å‹å¤§å°ä¸ä¸¥æ ¼ç›¸å…³ã€‚æ­¤å¤–ï¼ŒæŸäº›æ‰°åŠ¨ä¼šè§¦å‘é“¾å¼æ€ç»´æ¨ç†è¡Œä¸ºï¼Œå³ä½¿æ²¡æœ‰æ˜ç¡®çš„æç¤ºã€‚ç ”ç©¶æ­ç¤ºäº†å½“å‰LLMçš„å…³é”®æ¼æ´ï¼Œå¼ºè°ƒäº†æ”¹è¿›å¯¹å˜ˆæ‚ã€è¯¯å¯¼æ€§å’Œè¯­å¢ƒå¯†é›†è¾“å…¥çš„ç¨³å¥æ€§çš„å¿…è¦æ€§ã€‚è¿™ä¸ºæ›´åšéŸ§å’Œå¯é çš„å®é™…åº”ç”¨ä¸­çš„æ¨ç†é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³æ•°å­¦é—®é¢˜æ—¶çš„æ¨ç†ç¨³å¥æ€§å—åˆ°ç ”ç©¶ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨GSM8Kæ•°æ®é›†è¿›è¡Œç³»ç»Ÿæµ‹è¯•ï¼Œå‘ç°æ¨¡å‹åœ¨é¢å¯¹æç¤ºæ‰°åŠ¨æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>ä¸ç›¸å…³ä¸Šä¸‹æ–‡å¯¹æ¨¡å‹æ€§èƒ½äº§ç”Ÿé‡å¤§å½±å“ï¼ŒåŒºåˆ†å…³é”®ä¿¡æ¯ä¸ç»†èŠ‚æ˜¯é‡è¦æŒ‘æˆ˜ã€‚</li>
<li>æ€§èƒ½é€€åŒ–ä¸æ¨ç†ä»»åŠ¡çš„å¤æ‚åº¦ä¸æ•æ„Ÿï¼Œä¸æ¨¡å‹å¤§å°æ— ä¸¥æ ¼ç›¸å…³æ€§ã€‚</li>
<li>å­˜åœ¨æŸäº›æ‰°åŠ¨ä¼šè§¦å‘æ¨¡å‹é‡‡å–é“¾å¼æ€ç»´æ¨ç†è¡Œä¸ºã€‚</li>
<li>å½“å‰LLMå­˜åœ¨å…³é”®æ¼æ´ï¼Œéœ€è¦æ”¹è¿›å¯¹å™ªå£°ã€è¯¯å¯¼å’Œè¯­å¢ƒå¯†é›†è¾“å…¥çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02111">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-92720d81875a7f6665dc0065864be347.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-caa1e97a48c07b376e46f1df6571b5af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-793956ce59d78226596fd20fe7b29713.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="From-Flatland-to-Space-Teaching-Vision-Language-Models-to-Perceive-and-Reason-in-3D"><a href="#From-Flatland-to-Space-Teaching-Vision-Language-Models-to-Perceive-and-Reason-in-3D" class="headerlink" title="From Flatland to Space: Teaching Vision-Language Models to Perceive and   Reason in 3D"></a>From Flatland to Space: Teaching Vision-Language Models to Perceive and   Reason in 3D</h2><p><strong>Authors:Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, Xingyue Quan, Hang Xu, Li Zhang</strong></p>
<p>Recent advances in LVLMs have improved vision-language understanding, but they still struggle with spatial perception, limiting their ability to reason about complex 3D scenes. Unlike previous approaches that incorporate 3D representations into models to improve spatial understanding, we aim to unlock the potential of VLMs by leveraging spatially relevant image data. To this end, we introduce a novel 2D spatial data generation and annotation pipeline built upon scene data with 3D ground-truth. This pipeline enables the creation of a diverse set of spatial tasks, ranging from basic perception tasks to more complex reasoning tasks. Leveraging this pipeline, we construct SPAR-7M, a large-scale dataset generated from thousands of scenes across multiple public datasets. In addition, we introduce SPAR-Bench, a benchmark designed to offer a more comprehensive evaluation of spatial capabilities compared to existing spatial benchmarks, supporting both single-view and multi-view inputs. Training on both SPAR-7M and large-scale 2D datasets enables our models to achieve state-of-the-art performance on 2D spatial benchmarks. Further fine-tuning on 3D task-specific datasets yields competitive results, underscoring the effectiveness of our dataset in enhancing spatial reasoning. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è¿›æ­¥æå‡äº†è§†è§‰è¯­è¨€çš„ç†è§£èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨ç©ºé—´æ„ŸçŸ¥æ–¹é¢ä»å­˜åœ¨å›°éš¾ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å¯¹å¤æ‚ä¸‰ç»´åœºæ™¯çš„ç†è§£èƒ½åŠ›ã€‚ä¸åŒäºå°†ä¸‰ç»´è¡¨ç¤ºèå…¥æ¨¡å‹ä»¥æé«˜ç©ºé—´ç†è§£èƒ½åŠ›çš„ä¼ ç»Ÿæ–¹æ³•ï¼Œæˆ‘ä»¬æ—¨åœ¨é€šè¿‡åˆ©ç”¨ç©ºé—´ç›¸å…³çš„å›¾åƒæ•°æ®æ¥è§£é”è§†è§‰è¯­è¨€æ¨¡å‹çš„æ½œåŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºä¸‰ç»´çœŸå®åœºæ™¯æ•°æ®çš„æ–°å‹äºŒç»´ç©ºé—´æ•°æ®ç”Ÿæˆå’Œæ ‡æ³¨æµç¨‹ã€‚è¯¥æµç¨‹èƒ½å¤Ÿåˆ›å»ºä¸€ç³»åˆ—å¤šæ ·åŒ–çš„ç©ºé—´ä»»åŠ¡ï¼Œä»åŸºæœ¬çš„æ„ŸçŸ¥ä»»åŠ¡åˆ°æ›´å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚åˆ©ç”¨è¿™ä¸€æµç¨‹ï¼Œæˆ‘ä»¬æ„å»ºäº†SPAR-7Mæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç”±æ¥è‡ªå¤šä¸ªå…¬å…±æ•°æ®é›†çš„æ•°åƒä¸ªåœºæ™¯ç”Ÿæˆçš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SPAR-BenchåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ä¸ç°æœ‰ç©ºé—´åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œæä¾›æ›´å…¨é¢çš„ç©ºé—´èƒ½åŠ›è¯„ä¼°ï¼Œæ”¯æŒå•è§†å›¾å’Œå¤šè§†å›¾è¾“å…¥ã€‚åœ¨SPAR-7Må’Œå¤§è§„æ¨¡äºŒç»´æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½¿æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿåœ¨äºŒç»´ç©ºé—´åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿›ä¸€æ­¥åœ¨ç‰¹å®šä¸‰ç»´ä»»åŠ¡æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œäº§ç”Ÿäº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œè¿™å‡¸æ˜¾äº†æˆ‘ä»¬çš„æ•°æ®é›†åœ¨æé«˜ç©ºé—´æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22976v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://fudan-zvg.github.io/spar">https://fudan-zvg.github.io/spar</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼ŒLVLMçš„è¿›æ­¥æå‡äº†è§†è§‰è¯­è¨€çš„ç†è§£èƒ½åŠ›ï¼Œä½†åœ¨ç©ºé—´æ„ŸçŸ¥æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œå½±å“äº†å¯¹å¤æ‚3Dåœºæ™¯çš„ç†è§£ã€‚ä¸ºæŒ–æ˜VLMsçš„æ½œåŠ›ï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ç©ºé—´ç›¸å…³å›¾åƒæ•°æ®ï¼Œå¼•å…¥äº†ä¸€ç§æ–°å‹2Dç©ºé—´æ•°æ®ç”Ÿæˆä¸æ³¨é‡Šæµç¨‹ã€‚åŸºäºæ­¤æµç¨‹ï¼Œæ„å»ºäº†SPAR-7Mæ•°æ®é›†ï¼Œæ¶µç›–å¤šä¸ªå…¬å¼€æ•°æ®é›†æ•°åƒåœºæ™¯çš„æ•°æ®ã€‚åŒæ—¶ï¼Œå›¢é˜Ÿè¿˜æ¨å‡ºäº†SPAR-BenchåŸºå‡†æµ‹è¯•ï¼Œä»¥æ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹çš„ç©ºé—´èƒ½åŠ›ï¼Œæ”¯æŒå•è§†å›¾å’Œå¤šè§†å›¾è¾“å…¥ã€‚åœ¨SPAR-7MåŠå¤§è§„æ¨¡2Dæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒåï¼Œæ¨¡å‹åœ¨2Dç©ºé—´åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å“è¶Šï¼›è¿›ä¸€æ­¥åœ¨ç‰¹å®š3Dä»»åŠ¡æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒåï¼Œå–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LVLMsè™½æœ‰æ‰€è¿›æ­¥ï¼Œä½†åœ¨ç©ºé—´æ„ŸçŸ¥æ–¹é¢ä»æœ‰å±€é™ï¼Œå½±å“å¯¹å¤æ‚3Dåœºæ™¯çš„ç†è§£ã€‚</li>
<li>ä¸ºæ”¹å–„ç©ºé—´æ„ŸçŸ¥ï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ç©ºé—´ç›¸å…³å›¾åƒæ•°æ®ï¼Œåˆ›æ–°äº†2Dç©ºé—´æ•°æ®ç”Ÿæˆä¸æ³¨é‡Šæµç¨‹ã€‚</li>
<li>åŸºäºæ­¤æµç¨‹æ„å»ºäº†SPAR-7Mæ•°æ®é›†ï¼Œæ¶µç›–æ•°åƒåœºæ™¯æ•°æ®ï¼Œç”¨äºæå‡æ¨¡å‹çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>æ¨å‡ºäº†SPAR-BenchåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„ç©ºé—´èƒ½åŠ›ï¼Œæ”¯æŒå•è§†å›¾å’Œå¤šè§†å›¾è¾“å…¥ã€‚</li>
<li>åœ¨SPAR-7MåŠå¤§è§„æ¨¡2Dæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ï¼Œåœ¨2Dç©ºé—´åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>åœ¨ç‰¹å®š3Dä»»åŠ¡æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒåï¼Œæ¨¡å‹è¡¨ç°æœ‰ç«äº‰åŠ›ï¼Œè¯æ˜æ•°æ®é›†æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22976">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fb3ee952a384b6a8b6aeeac45194ea7c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62a1a1e3bf5c546842a30dadb0491290.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b37a18b918c7731167b00b015d60945d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3331aca7265e0a484c8140642cf68927.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d97803cba8f7f36ff93cb298ee9763e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ea3c77d4a902709a6f9eb45b68198c1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ShieldAgent-Shielding-Agents-via-Verifiable-Safety-Policy-Reasoning"><a href="#ShieldAgent-Shielding-Agents-via-Verifiable-Safety-Policy-Reasoning" class="headerlink" title="ShieldAgent: Shielding Agents via Verifiable Safety Policy Reasoning"></a>ShieldAgent: Shielding Agents via Verifiable Safety Policy Reasoning</h2><p><strong>Authors:Zhaorun Chen, Mintong Kang, Bo Li</strong></p>
<p>Autonomous agents powered by foundation models have seen widespread adoption across various real-world applications. However, they remain highly vulnerable to malicious instructions and attacks, which can result in severe consequences such as privacy breaches and financial losses. More critically, existing guardrails for LLMs are not applicable due to the complex and dynamic nature of agents. To tackle these challenges, we propose ShieldAgent, the first guardrail agent designed to enforce explicit safety policy compliance for the action trajectory of other protected agents through logical reasoning. Specifically, ShieldAgent first constructs a safety policy model by extracting verifiable rules from policy documents and structuring them into a set of action-based probabilistic rule circuits. Given the action trajectory of the protected agent, ShieldAgent retrieves relevant rule circuits and generates a shielding plan, leveraging its comprehensive tool library and executable code for formal verification. In addition, given the lack of guardrail benchmarks for agents, we introduce ShieldAgent-Bench, a dataset with 3K safety-related pairs of agent instructions and action trajectories, collected via SOTA attacks across 6 web environments and 7 risk categories. Experiments show that ShieldAgent achieves SOTA on ShieldAgent-Bench and three existing benchmarks, outperforming prior methods by 11.3% on average with a high recall of 90.1%. Additionally, ShieldAgent reduces API queries by 64.7% and inference time by 58.2%, demonstrating its high precision and efficiency in safeguarding agents. </p>
<blockquote>
<p>è‡ªä¸»æ™ºèƒ½ä½“é€šè¿‡åŸºç¡€æ¨¡å‹è¿›è¡Œé©±åŠ¨ï¼Œå·²åœ¨å„ç§ç°å®åº”ç”¨ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¾ˆå®¹æ˜“å—åˆ°æ¶æ„æŒ‡ä»¤å’Œæ”»å‡»çš„å½±å“ï¼Œå¯èƒ½å¯¼è‡´ä¸¥é‡åæœï¼Œå¦‚éšç§æ³„éœ²å’Œè´¢åŠ¡æŸå¤±ã€‚æ›´ä¸ºå…³é”®çš„æ˜¯ï¼Œç”±äºæ™ºèƒ½ä½“çš„å¤æ‚æ€§å’ŒåŠ¨æ€æ€§ï¼Œç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹é˜²æŠ¤æ å¹¶ä¸é€‚ç”¨ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ShieldAgentï¼Œè¿™æ˜¯ä¸€æ¬¾é¦–æ¬¾æ—¨åœ¨é€šè¿‡é€»è¾‘æ¨ç†å¼ºåˆ¶æ‰§è¡Œå…¶ä»–å—ä¿æŠ¤æ™ºèƒ½ä½“è¡ŒåŠ¨è½¨è¿¹æ˜ç¡®å®‰å…¨ç­–ç•¥åˆè§„æ€§çš„é˜²æŠ¤æ æ™ºèƒ½ä½“ã€‚å…·ä½“æ¥è¯´ï¼ŒShieldAgenté¦–å…ˆé€šè¿‡ä»æ”¿ç­–æ–‡ä»¶ä¸­æå–å¯éªŒè¯è§„åˆ™å¹¶å°†å…¶ç»“æ„åŒ–ä¸ºä¸€ç»„åŸºäºè¡ŒåŠ¨çš„æ¦‚ç‡è§„åˆ™ç”µè·¯æ¥æ„å»ºå®‰å…¨ç­–ç•¥æ¨¡å‹ã€‚é’ˆå¯¹å—ä¿æŠ¤æ™ºèƒ½ä½“çš„è¡ŒåŠ¨è½¨è¿¹ï¼ŒShieldAgentä¼šæ£€ç´¢ç›¸å…³è§„åˆ™ç”µè·¯å¹¶ç”Ÿæˆé˜²æŠ¤è®¡åˆ’ï¼Œåˆ©ç”¨å…¶å…¨é¢çš„å·¥å…·åº“å’Œå¯æ‰§è¡Œä»£ç è¿›è¡Œå½¢å¼åŒ–éªŒè¯ã€‚æ­¤å¤–ï¼Œé‰´äºæ™ºèƒ½ä½“ç¼ºä¹é˜²æŠ¤æ åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ShieldAgent-Benchæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«3000æ¡ä¸å®‰å…¨ç›¸å…³çš„æ™ºèƒ½ä½“æŒ‡ä»¤å’Œè¡ŒåŠ¨è½¨è¿¹å¯¹ï¼Œè¿™äº›æ ·æœ¬æ˜¯é€šè¿‡6ç§ç½‘ç»œç¯å¢ƒå’Œ7ç§é£é™©ç±»åˆ«çš„æœ€æ–°æ”»å‡»æ‰‹æ®µæ”¶é›†çš„ã€‚å®éªŒè¡¨æ˜ï¼ŒShieldAgentåœ¨ShieldAgent-Benchå’Œä¸‰ä¸ªç°æœ‰åŸºå‡†æµ‹è¯•ä¸Šå‡è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œå¹³å‡ä¼˜äºä»¥å‰çš„æ–¹æ³•11.3%ï¼Œå¬å›ç‡é«˜è¾¾9.1%ã€‚æ­¤å¤–ï¼ŒShieldAgentå‡å°‘äº†64.7%çš„APIæŸ¥è¯¢å’Œç¼©çŸ­äº†58.2%çš„æ¨ç†æ—¶é—´ï¼Œè¯æ˜å…¶åœ¨ä¿æŠ¤æ™ºèƒ½ä½“æ–¹é¢å…·æœ‰é«˜ç²¾åº¦å’Œé«˜æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22738v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†è‡ªä¸»ä»£ç†çš„å¹¿æ³›åº”ç”¨åŠå…¶é¢ä¸´çš„å®‰å…¨æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ShieldAgentï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªè®¾è®¡çš„é˜²æŠ¤ä»£ç†ï¼Œé€šè¿‡é€»è¾‘æ¨ç†å¼ºåˆ¶å…¶ä»–å—ä¿æŠ¤ä»£ç†çš„è¡ŒåŠ¨è½¨è¿¹ç¬¦åˆæ˜ç¡®çš„å®‰å…¨æ”¿ç­–ã€‚ShieldAgentæ„å»ºäº†å®‰å…¨æ”¿ç­–æ¨¡å‹ï¼Œä»æ”¿ç­–æ–‡ä»¶ä¸­æå–å¯éªŒè¯çš„è§„åˆ™ï¼Œå¹¶å°†å…¶ç»“æ„åŒ–ä¸ºåŸºäºè¡ŒåŠ¨çš„æ¦‚ç‡è§„åˆ™ç”µè·¯ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¼ºä¹ä»£ç†çš„é˜²æŠ¤åŸºå‡†çº¿ï¼Œå¼•å…¥äº†ShieldAgent-Benchæ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼ŒShieldAgentåœ¨æ€§èƒ½ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå¹³å‡ä¼˜äºä»¥å‰çš„æ–¹æ³•å¹³å‡é«˜å‡º11.3%ï¼Œå¬å›ç‡é«˜è¾¾90.1%ï¼Œå¹¶æ˜¾ç¤ºå‡ºå…¶ç²¾ç¡®æ€§å’Œé«˜æ•ˆæ€§ã€‚é€šè¿‡å‡å°‘APIæŸ¥è¯¢å’Œç¼©çŸ­æ¨ç†æ—¶é—´è¯æ˜äº†å…¶é«˜ç²¾ç¡®åº¦å’Œé«˜æ•ˆç‡ã€‚ShieldAgentä¸ä»…æˆåŠŸæé«˜äº†å—ä¿æŠ¤ä»£ç†çš„å®‰å…¨æ€§ï¼Œè¿˜æé«˜äº†å·¥ä½œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è‡ªä¸»ä»£ç†å¹¿æ³›åº”ç”¨äºå„ç§ç°å®ä¸–ç•Œåº”ç”¨ï¼Œä½†æ˜“å—æ¶æ„æŒ‡ä»¤å’Œæ”»å‡»å½±å“ï¼Œå¯¼è‡´ä¸¥é‡åæœã€‚</li>
<li>å­˜åœ¨é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„é˜²æŠ¤æœºåˆ¶ä¸é€‚ç”¨äºè‡ªä¸»ä»£ç†çš„é—®é¢˜ã€‚</li>
<li>ShieldAgentæ˜¯é¦–ä¸ªè®¾è®¡çš„é˜²æŠ¤ä»£ç†ï¼Œé€šè¿‡é€»è¾‘æ¨ç†å¼ºåˆ¶å…¶ä»–å—ä¿æŠ¤ä»£ç†éµå®ˆå®‰å…¨æ”¿ç­–ã€‚</li>
<li>ShieldAgentæ„å»ºäº†å®‰å…¨æ”¿ç­–æ¨¡å‹ï¼Œä»æ”¿ç­–æ–‡ä»¶ä¸­æå–è§„åˆ™å¹¶ç»“æ„åŒ–ã€‚</li>
<li>å¼•å…¥äº†ShieldAgent-Benchæ•°æ®é›†ï¼Œç”¨äºç¼ºä¹ä»£ç†é˜²æŠ¤åŸºå‡†çº¿çš„é—®é¢˜ã€‚</li>
<li>å®éªŒè¡¨æ˜ShieldAgentæ€§èƒ½ä¼˜è¶Šï¼Œåœ¨å®‰å…¨æ€§å’Œæ•ˆç‡æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22738">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a72ea9238741c792f862583b334f3309.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-123062712a25799dee6c1a1b1ecfacc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acd8ec36af05e735b3d0b4307c375a12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b10e0db6e6cdea457a96354f7bdab799.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Sharpe-Ratio-Guided-Active-Learning-for-Preference-Optimization-in-RLHF"><a href="#Sharpe-Ratio-Guided-Active-Learning-for-Preference-Optimization-in-RLHF" class="headerlink" title="Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF"></a>Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF</h2><p><strong>Authors:Syrine Belakaria, Joshua Kazdan, Charles Marx, Chris Cundy, Willie Neiswanger, Sanmi Koyejo, Barbara E. Engelhardt, Stefano Ermon</strong></p>
<p>Reinforcement learning from human feedback (RLHF) has become a cornerstone of the training and alignment pipeline for large language models (LLMs). Recent advances, such as direct preference optimization (DPO), have simplified the preference learning step. However, collecting preference data remains a challenging and costly process, often requiring expert annotation. This cost can be mitigated by carefully selecting the data points presented for annotation. In this work, we propose an active learning approach to efficiently select prompt and preference pairs using a risk assessment strategy based on the Sharpe Ratio. To address the challenge of unknown preferences prior to annotation, our method evaluates the gradients of all potential preference annotations to assess their impact on model updates. These gradient-based evaluations enable risk assessment of data points regardless of the annotation outcome. By leveraging the DPO loss derivations, we derive a closed-form expression for computing these Sharpe ratios on a per-tuple basis, ensuring our approach remains both tractable and computationally efficient. We also introduce two variants of our method, each making different assumptions about prior information. Experimental results demonstrate that our method outperforms the baseline by up to 5% in win rates against the chosen completion with limited human preference data across several language models and real-world datasets. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒå’Œå¯¹é½æµç¨‹çš„æ ¸å¿ƒã€‚æœ€è¿‘çš„è¿›å±•ï¼Œå¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œç®€åŒ–äº†åå¥½å­¦ä¹ æ­¥éª¤ã€‚ç„¶è€Œï¼Œæ”¶é›†åå¥½æ•°æ®ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬é«˜æ˜‚çš„è¿‡ç¨‹ï¼Œé€šå¸¸éœ€è¦ä¸“å®¶æ ‡æ³¨ã€‚é€šè¿‡ä»”ç»†é€‰æ‹©ç”¨äºæ ‡æ³¨çš„æ•°æ®ç‚¹ï¼Œå¯ä»¥å‡è½»è¿™ç§æˆæœ¬ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤æ™®æ¯”ç‡çš„é£é™©è¯„ä¼°ç­–ç•¥çš„ä¸»åŠ¨å­¦ä¹ æ–¹æ³•æ¥æœ‰æ•ˆåœ°é€‰æ‹©æç¤ºå’Œåå¥½å¯¹ã€‚ä¸ºäº†è§£å†³æ ‡æ³¨å‰æœªçŸ¥åå¥½çš„é—®é¢˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¯„ä¼°æ‰€æœ‰æ½œåœ¨åå¥½æ ‡æ³¨çš„æ¢¯åº¦ï¼Œä»¥è¯„ä¼°å®ƒä»¬å¯¹æ¨¡å‹æ›´æ–°çš„å½±å“ã€‚è¿™äº›åŸºäºæ¢¯åº¦çš„è¯„ä¼°èƒ½å¤Ÿå¯¹æ•°æ®ç‚¹è¿›è¡Œé£é™©è¯„ä¼°ï¼Œè€Œæ— è®ºæ ‡æ³¨ç»“æœå¦‚ä½•ã€‚é€šè¿‡åˆ©ç”¨DPOæŸå¤±æ¨å¯¼ï¼Œæˆ‘ä»¬ä¸ºè®¡ç®—è¿™äº›æ¯å…ƒç»„çš„å¤æ™®æ¯”ç‡æä¾›äº†å°é—­å½¢å¼çš„è¡¨è¾¾å¼ï¼Œç¡®ä¿æˆ‘ä»¬çš„æ–¹æ³•æ—¢å®ç”¨åˆè®¡ç®—é«˜æ•ˆã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†è¯¥æ–¹æ³•çš„ä¸¤ä¸ªå˜ä½“ï¼Œæ¯ä¸ªå˜ä½“éƒ½å¯¹å…ˆéªŒä¿¡æ¯åšå‡ºäº†ä¸åŒçš„å‡è®¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æœ‰é™çš„äººç±»åå¥½æ•°æ®ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªè¯­è¨€æ¨¡å‹å’ŒçœŸå®æ•°æ®é›†ä¸Šç›¸å¯¹äºæ‰€é€‰å®Œæˆçš„èƒœç‡æé«˜äº†é«˜è¾¾5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22137v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒå’Œå¯¹é½æµç¨‹çš„æ ¸å¿ƒã€‚æœ€è¿‘å‡ºç°çš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç®€åŒ–äº†åå¥½å­¦ä¹ çš„æ­¥éª¤ï¼Œä½†æ”¶é›†åå¥½æ•°æ®ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬é«˜æ˜‚çš„è¿‡ç¨‹ï¼Œé€šå¸¸éœ€è¦ä¸“å®¶æ ‡æ³¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤æ™®æ¯”ç‡çš„é£é™©è¯„ä¼°ç­–ç•¥çš„æœ‰æ•ˆä¸»åŠ¨å­¦ä¹ æ–¹æ³•æ¥é€‰æ‹©æç¤ºå’Œåå¥½å¯¹ã€‚ä¸ºäº†åº”å¯¹æ ‡æ³¨å‰æœªçŸ¥åå¥½çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¯„ä¼°æ‰€æœ‰æ½œåœ¨åå¥½æ ‡æ³¨çš„æ¢¯åº¦æ¥è¯„ä¼°å®ƒä»¬å¯¹æ¨¡å‹æ›´æ–°çš„å½±å“ã€‚è¿™äº›åŸºäºæ¢¯åº¦çš„è¯„ä¼°ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿæ ¹æ®æ ‡æ³¨ç»“æœæ¥è¯„ä¼°æ•°æ®ç‚¹çš„é£é™©ã€‚é€šè¿‡åˆ©ç”¨DPOæŸå¤±æ¨å¯¼ï¼Œæˆ‘ä»¬ä¸ºè®¡ç®—è¿™äº›å¤æ™®æ¯”ç‡æä¾›äº†é—­å¼è¡¨è¾¾å¼ï¼Œç¡®ä¿æˆ‘ä»¬çš„æ–¹æ³•åœ¨æœ‰é™çš„æ ‡æ³¨æ•°æ®ä¸‹å…·æœ‰å¯è¡Œæ€§å’Œè®¡ç®—æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æœ‰é™çš„åå¥½æ•°æ®ä¸‹ï¼Œåœ¨å¤šä¸ªè¯­è¨€æ¨¡å‹å’ŒçœŸå®æ•°æ®é›†ä¸Šçš„èƒœç‡æé«˜äº†é«˜è¾¾5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒçš„æ ¸å¿ƒã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç®€åŒ–äº†åå¥½å­¦ä¹ æ­¥éª¤ã€‚</li>
<li>æ”¶é›†åå¥½æ•°æ®ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ä¸”æˆæœ¬é«˜æ˜‚ï¼Œéœ€è¦ä¸“å®¶æ ‡æ³¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¤æ™®æ¯”ç‡çš„é£é™©è¯„ä¼°ç­–ç•¥çš„æœ‰æ•ˆä¸»åŠ¨å­¦ä¹ æ–¹æ³•æ¥é€‰æ‹©æç¤ºå’Œåå¥½å¯¹ã€‚</li>
<li>æ–¹æ³•é€šè¿‡è¯„ä¼°æ½œåœ¨åå¥½æ ‡æ³¨çš„æ¢¯åº¦æ¥åº”å¯¹æœªçŸ¥åå¥½çš„æŒ‘æˆ˜ã€‚</li>
<li>åˆ©ç”¨DPOæŸå¤±æ¨å¯¼ï¼Œæä¾›è®¡ç®—å¤æ™®æ¯”ç‡çš„é—­å¼è¡¨è¾¾å¼ï¼Œç¡®ä¿æ–¹æ³•çš„å¯è¡Œæ€§å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22137">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-62fdebb138e26f17aa625f883ce88a8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d04c7a60e3a456a5475bab73855e5b2.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Learning-to-Lie-Reinforcement-Learning-Attacks-Damage-Human-AI-Teams-and-Teams-of-LLMs"><a href="#Learning-to-Lie-Reinforcement-Learning-Attacks-Damage-Human-AI-Teams-and-Teams-of-LLMs" class="headerlink" title="Learning to Lie: Reinforcement Learning Attacks Damage Human-AI Teams   and Teams of LLMs"></a>Learning to Lie: Reinforcement Learning Attacks Damage Human-AI Teams   and Teams of LLMs</h2><p><strong>Authors:Abed Kareem Musaffar, Anand Gokhale, Sirui Zeng, Rasta Tadayon, Xifeng Yan, Ambuj Singh, Francesco Bullo</strong></p>
<p>As artificial intelligence (AI) assistants become more widely adopted in safety-critical domains, it becomes important to develop safeguards against potential failures or adversarial attacks. A key prerequisite to developing these safeguards is understanding the ability of these AI assistants to mislead human teammates. We investigate this attack problem within the context of an intellective strategy game where a team of three humans and one AI assistant collaborate to answer a series of trivia questions. Unbeknownst to the humans, the AI assistant is adversarial. Leveraging techniques from Model-Based Reinforcement Learning (MBRL), the AI assistant learns a model of the humansâ€™ trust evolution and uses that model to manipulate the group decision-making process to harm the team. We evaluate two models â€“ one inspired by literature and the other data-driven â€“ and find that both can effectively harm the human team. Moreover, we find that in this setting our data-driven model is capable of accurately predicting how human agents appraise their teammates given limited information on prior interactions. Finally, we compare the performance of state-of-the-art LLM models to human agents on our influence allocation task to evaluate whether the LLMs allocate influence similarly to humans or if they are more robust to our attack. These results enhance our understanding of decision-making dynamics in small human-AI teams and lay the foundation for defense strategies. </p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åŠ©æ‰‹åœ¨å®‰å…¨å…³é”®é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œå¼€å‘å¯¹æŠ—æ½œåœ¨æ•…éšœæˆ–å¯¹æŠ—æ€§æ”»å‡»çš„å®‰å…¨ä¿éšœæªæ–½å˜å¾—è‡³å…³é‡è¦ã€‚å¼€å‘è¿™äº›ä¿éšœæªæ–½çš„å…³é”®å…ˆå†³æ¡ä»¶æ˜¯äº†è§£è¿™äº›AIåŠ©æ‰‹è¯¯å¯¼äººç±»é˜Ÿå‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªä¸‰äººäººç±»å›¢é˜Ÿå’Œä¸€ä¸ªAIåŠ©æ‰‹çš„æ™ºåŠ›ç­–ç•¥æ¸¸æˆèƒŒæ™¯ä¸‹ï¼Œç ”ç©¶è¿™ä¸€æ”»å‡»é—®é¢˜ï¼Œå…±åŒå›ç­”ä¸€ç³»åˆ—é—®é¢˜ã€‚äººç±»ä¸çŸ¥é“çš„æ˜¯ï¼ŒAIåŠ©æ‰‹æ˜¯å¯¹æŠ—æ€§çš„ã€‚åˆ©ç”¨åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ï¼ˆMBRLï¼‰æŠ€æœ¯ï¼ŒAIåŠ©æ‰‹å­¦ä¹ äººç±»ä¿¡ä»»æ¼”å˜æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨è¯¥æ¨¡å‹æ“çºµç¾¤ä½“å†³ç­–è¿‡ç¨‹ä»¥æŸå®³å›¢é˜Ÿã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸¤ç§æ¨¡å‹â€”â€”ä¸€ç§å—æ–‡çŒ®å¯å‘ï¼Œå¦ä¸€ç§æ•°æ®é©±åŠ¨â€”â€”æˆ‘ä»¬å‘ç°ä¸¤è€…éƒ½èƒ½æœ‰æ•ˆåœ°æŸå®³äººç±»å›¢é˜Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°åœ¨æ­¤ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬çš„æ•°æ®é©±åŠ¨æ¨¡å‹èƒ½å¤Ÿåœ¨æœ‰é™çš„å…ˆå‰äº¤äº’ä¿¡æ¯ä¸‹å‡†ç¡®é¢„æµ‹äººç±»ä»£ç†å¦‚ä½•è¯„ä¼°ä»–ä»¬çš„é˜Ÿå‹ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨å½±å“åŠ›åˆ†é…ä»»åŠ¡ä¸Šæ¯”è¾ƒäº†æœ€å…ˆè¿›çš„LLMæ¨¡å‹ä¸äººç±»ä»£ç†çš„è¡¨ç°ï¼Œä»¥è¯„ä¼°LLMæ˜¯å¦åƒäººç±»ä¸€æ ·åˆ†é…å½±å“åŠ›ï¼Œæˆ–è€…å®ƒä»¬æ˜¯å¦å¯¹æˆ‘ä»¬çš„æ”»å‡»å…·æœ‰æ›´å¼ºçš„æŠµæŠ—åŠ›ã€‚è¿™äº›ç»“æœå¢å¼ºäº†æˆ‘ä»¬å¯¹äºå°å‹äººç±»-AIå›¢é˜Ÿä¸­å†³ç­–åŠ¨æ€çš„ç†è§£ï¼Œå¹¶ä¸ºé˜²å¾¡ç­–ç•¥å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21983v2">PDF</a> 17 pages, 9 figures, accepted to ICLR 2025 Workshop on Human-AI   Coevolution</p>
<p><strong>Summary</strong>ï¼šäººå·¥æ™ºèƒ½åŠ©æ‰‹åœ¨å®‰å…¨é¢†åŸŸå¹¿æ³›åº”ç”¨ï¼Œå…¶æ½œåœ¨å¤±è¯¯æˆ–æ”»å‡»é£é™©éœ€é˜²èŒƒã€‚ç ”ç©¶AIåŠ©æ‰‹è¯¯å¯¼äººç±»é˜Ÿå‹çš„é—®é¢˜ï¼Œåœ¨ä¸€ä¸ªå›¢é˜Ÿä¸­AIåŠ©ç†æƒ³åœ¨è§£ç­”ç›Šæ™ºé¢˜çš„è¿‡ç¨‹ä¸­é€šè¿‡å¼ºåŒ–å­¦ä¹ æŠ€å·§æ¬ºéª—å›¢é˜Ÿåˆä½œï¼Œè¿›è€Œä¼¤å®³å›¢é˜Ÿåˆ©ç›Šã€‚ç ”ç©¶äº†ä¸¤ç§æ¨¡å‹çš„è¡¨ç°å¹¶å‘ç°å¯¹äººç±»çš„è¯„ä»·å‡†ç¡®åº¦é«˜ã€‚ä¸å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ¯”å¯¹è¯„ä¼°äº†äººç±»åœ¨é¢å¯¹ä¿¡ä»»åˆ†é…æ—¶çš„ååº”å’Œå·®å¼‚ï¼Œä»¥æ­¤åŠ å¼ºå¯¹äºAIåŠ©æ‰‹æ”»å‡»çš„é˜²å¾¡ã€‚è¯¥ç ”ç©¶å¯¹AIä¸äººçš„å°å‹å›¢é˜Ÿåˆä½œå†³ç­–æœºåˆ¶çš„æ·±å…¥æ¢ç©¶æœ‰é‡è¦æŒ‡å¯¼æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>AIåŠ©æ‰‹åœ¨å…³é”®å®‰å…¨é¢†åŸŸçš„åº”ç”¨éœ€è¦é˜²èŒƒæ½œåœ¨å¤±è¯¯æˆ–æ”»å‡»é£é™©ã€‚</li>
<li>AIåŠ©æ‰‹çš„æ¬ºéª—è¡Œä¸ºä¼šå½±å“å›¢é˜Ÿåˆä½œå†³ç­–è¿‡ç¨‹å¹¶å¯èƒ½æŸå®³å›¢é˜Ÿåˆ©ç›Šã€‚</li>
<li>åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æŠ€å·§æ„å»ºçš„AIæ¨¡å‹èƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»ä¿¡ä»»æ¼”åŒ–å¹¶æ“çºµå›¢é˜Ÿåˆä½œè¿‡ç¨‹ã€‚</li>
<li>ä¸¤ç§æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œæ•°æ®é©±åŠ¨æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹äººç±»å¦‚ä½•è¯„ä¼°é˜Ÿå‹è¡¨ç°ã€‚</li>
<li>ä¸å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œäººç±»åœ¨é¢å¯¹ä¿¡ä»»åˆ†é…ä»»åŠ¡æ—¶è¡¨ç°å‡ºä¸åŒçš„ååº”æ¨¡å¼ã€‚</li>
<li>è¿™äº›ç ”ç©¶æ­ç¤ºäº†AIåŠ©æ‰‹æ”»å‡»çš„å¯èƒ½æ€§åŠå…¶å½±å“ï¼Œä¸ºé˜²èŒƒç­–ç•¥çš„åˆ¶å®šæä¾›äº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21983">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f783996d5e73d7b607b62838c55e4fe2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b585e7fb42d92f45f83e68dc235a55a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f05a96d7d3a9adbd107482f576c2eb58.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Optimizing-Safe-and-Aligned-Language-Generation-A-Multi-Objective-GRPO-Approach"><a href="#Optimizing-Safe-and-Aligned-Language-Generation-A-Multi-Objective-GRPO-Approach" class="headerlink" title="Optimizing Safe and Aligned Language Generation: A Multi-Objective GRPO   Approach"></a>Optimizing Safe and Aligned Language Generation: A Multi-Objective GRPO   Approach</h2><p><strong>Authors:Xuying Li, Zhuo Li, Yuji Kosuga, Victor Bian</strong></p>
<p>Aligning large language models (LLMs) with human values and safety constraints is challenging, especially when objectives like helpfulness, truthfulness, and avoidance of harm conflict. Reinforcement Learning from Human Feedback (RLHF) has achieved notable success in steering models, but is complex and can be unstable. Recent approaches such as Direct Preference Optimization (DPO) simplify preference-based fine-tuning but may introduce bias or trade-off certain objectives<del>\cite{dpo}. In this work, we propose a Group Relative Policy Optimization (GRPO) framework with a multi-label reward regression model to achieve safe and aligned language generation. The GRPO algorithm optimizes a policy by comparing groups of sampled responses, eliminating the need for a separate value critic and improving training efficiency</del>\cite{grpo}. We train a reward model to predict multiple alignment scores (e.g., safety, helpfulness, etc.), which are combined into a single reward signal. We provide a theoretical derivation for using this learned multi-aspect reward within GRPO and discuss its advantages and limitations. Empirically, our approach improves all the safety and quality metrics evaluated in language generation tasks on model scales (0.5B, 7B, and 14B parameters), demonstrating a robust balance of objectives. We compare GRPO to PPO-based RLHF and DPO, highlighting that GRPO achieves alignment with significantly lower computational cost and explicit multi-objective handling. \textbf{We will open-source all trained models at <a target="_blank" rel="noopener" href="https://huggingface.co/hydroxai">https://huggingface.co/hydroxai</a>. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚å’Œå®‰å…¨çº¦æŸç›¸ç»“åˆæ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨è¯¸å¦‚æœ‰ç”¨æ€§ã€çœŸå®æ€§å’Œé¿å…ä¼¤å®³ç­‰ç›®æ ‡å‘ç”Ÿå†²çªæ—¶ã€‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰åœ¨å¼•å¯¼æ¨¡å‹æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å…¶å¤æ‚ä¸”å¯èƒ½ä¸ç¨³å®šã€‚æœ€è¿‘çš„æ–¹æ³•ï¼Œå¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œç®€åŒ–äº†åŸºäºåå¥½çš„å¾®è°ƒï¼Œä½†å¯èƒ½å¼•å…¥åè§æˆ–æƒè¡¡æŸäº›ç›®æ ‡\cite{dpo}ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤šæ ‡ç­¾å¥–åŠ±å›å½’æ¨¡å‹çš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¡†æ¶ï¼Œä»¥å®ç°å®‰å…¨å’Œå¯¹é½çš„è¯­è¨€ç”Ÿæˆã€‚GRPOç®—æ³•é€šè¿‡æ¯”è¾ƒä¸€ç»„é‡‡æ ·å“åº”æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹å•ç‹¬çš„ä»·å€¼æ‰¹è¯„å®¶çš„éœ€æ±‚ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡\cite{grpo}ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå¥–åŠ±æ¨¡å‹æ¥é¢„æµ‹å¤šä¸ªå¯¹é½åˆ†æ•°ï¼ˆä¾‹å¦‚å®‰å…¨æ€§ã€æœ‰ç”¨æ€§ç­‰ï¼‰ï¼Œè¿™äº›åˆ†æ•°è¢«åˆå¹¶æˆä¸€ä¸ªå•ä¸€çš„å¥–åŠ±ä¿¡å·ã€‚æˆ‘ä»¬ä¸ºåœ¨GRPOä¸­ä½¿ç”¨è¿™ç§å­¦ä¹ çš„å¤šæ–¹é¢å¥–åŠ±æä¾›äº†ç†è®ºæ¨å¯¼ï¼Œå¹¶è®¨è®ºäº†å…¶ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚ç»éªŒä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¯­è¨€ç”Ÿæˆä»»åŠ¡ä¸­æ”¹è¿›äº†æ‰€æœ‰å®‰å…¨æ€§å’Œè´¨é‡è¯„ä»·æŒ‡æ ‡ï¼ˆè§„æ¨¡ä¸º0.5Bã€7Bå’Œ14Bå‚æ•°çš„æ¨¡å‹ï¼‰ï¼Œè¯æ˜äº†ç›®æ ‡ä¹‹é—´çš„ç¨³å¥å¹³è¡¡ã€‚æˆ‘ä»¬å°†GRPOä¸åŸºäºPPOçš„RLHFå’ŒDPOè¿›è¡Œæ¯”è¾ƒï¼Œçªå‡ºæ˜¾ç¤ºGRPOåœ¨å®ç°å¯¹é½æ—¶å…·æœ‰æ›´ä½çš„è®¡ç®—æˆæœ¬å’Œæ˜ç¡®çš„å¤šç›®æ ‡å¤„ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å°†åœ¨[<a target="_blank" rel="noopener" href="https://huggingface.co/hydroxai%E4%B8%8A%E5%85%AC%E5%BC%80%E6%89%80%E6%9C%89%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E3%80%82]">https://huggingface.co/hydroxaiä¸Šå…¬å¼€æ‰€æœ‰è®­ç»ƒæ¨¡å‹ã€‚]</a>(<a target="_blank" rel="noopener" href="https://huggingface.co/hydroxai%E4%B8%8A%E5%BC%80%E6%9C%AC%E6%89%80%E6%A3%AE%E7%9A%84%E6%AF%94%E8%BE%BD%E5%AF%BCGRPO%E5%AF%BC%E5%9C%B0%E3%80%82]">https://huggingface.co/hydroxai%E4%B8%8A%E5%BC%80%E6%9C%AC%E6%89%80%E6%A3%AE%E7%9A%84%E6%AF%94%E8%BE%BD%E5%AF%BCGRPO%E5%AF%BC%E5%9C%B0ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21819v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºGroup Relative Policy Optimizationï¼ˆGRPOï¼‰çš„æ¡†æ¶ï¼Œç”¨äºå®ç°å®‰å…¨ä¸”å¯¹é½çš„è¯­è¨€ç”Ÿæˆã€‚GRPOé€šè¿‡æ¯”è¾ƒä¸åŒé‡‡æ ·å“åº”ç»„æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œæ— éœ€å•ç‹¬çš„ä»·å€¼è¯„ä¼°è€…ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚è®­ç»ƒå¥–åŠ±æ¨¡å‹ä»¥é¢„æµ‹å¤šä¸ªå¯¹é½åˆ†æ•°ï¼ˆå¦‚å®‰å…¨æ€§ã€æœ‰åŠ©æ€§ç­‰ï¼‰ï¼Œå¹¶ç»“åˆæˆä¸€ä¸ªå•ä¸€å¥–åŠ±ä¿¡å·ã€‚åœ¨å¤šç§æ¨¡å‹è§„æ¨¡çš„è¯­è¨€ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒGRPOåœ¨å®‰å…¨å’Œè´¨é‡æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå®ç°äº†ç›®æ ‡ä¹‹é—´çš„ç¨³å¥å¹³è¡¡ã€‚ç›¸è¾ƒäºå…¶ä»–æ–¹æ³•ï¼ŒGRPOåœ¨è¿ç®—æˆæœ¬ä¸Šæ˜¾è‘—é™ä½ï¼Œå¹¶èƒ½æ˜¾å¼å¤„ç†å¤šç›®æ ‡é—®é¢˜ã€‚æˆ‘ä»¬å°†å…¬å¼€æ‰€æœ‰è®­ç»ƒæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GRPOæ¡†æ¶æ—¨åœ¨å®ç°å®‰å…¨ä¸”å¯¹é½çš„è¯­è¨€ç”Ÿæˆã€‚</li>
<li>GRPOé€šè¿‡æ¯”è¾ƒé‡‡æ ·å“åº”ç»„æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œç®€åŒ–äº†åŸºäºåå¥½çš„å¾®è°ƒæ–¹æ³•ã€‚</li>
<li>å¥–åŠ±æ¨¡å‹é¢„æµ‹å¤šä¸ªå¯¹é½åˆ†æ•°ï¼Œå¦‚å®‰å…¨æ€§ã€æœ‰åŠ©æ€§ç­‰ï¼Œå¹¶ç»“åˆæˆå•ä¸€å¥–åŠ±ä¿¡å·ã€‚</li>
<li>GRPOåœ¨å¤šç§æ¨¡å‹è§„æ¨¡çš„è¯­è¨€ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå®ç°äº†å®‰å…¨å’Œè´¨é‡çš„ç¨³å¥å¹³è¡¡ã€‚</li>
<li>GRPOç›¸è¾ƒäºå…¶ä»–æ–¹æ³•ï¼ˆå¦‚PPO-based RLHFå’ŒDPOï¼‰åœ¨è¿ç®—æˆæœ¬ä¸Šæ˜¾è‘—é™ä½ï¼Œå¹¶èƒ½æ˜¾å¼å¤„ç†å¤šç›®æ ‡é—®é¢˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-79e64b16a9dac5545c0fcc7bb75d5496.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Enhancing-Retrieval-Systems-with-Inference-Time-Logical-Reasoning"><a href="#Enhancing-Retrieval-Systems-with-Inference-Time-Logical-Reasoning" class="headerlink" title="Enhancing Retrieval Systems with Inference-Time Logical Reasoning"></a>Enhancing Retrieval Systems with Inference-Time Logical Reasoning</h2><p><strong>Authors:Felix Faltings, Wei Wei, Yujia Bao</strong></p>
<p>Traditional retrieval methods rely on transforming user queries into vector representations and retrieving documents based on cosine similarity within an embedding space. While efficient and scalable, this approach often fails to handle complex queries involving logical constructs such as negations, conjunctions, and disjunctions. In this paper, we propose a novel inference-time logical reasoning framework that explicitly incorporates logical reasoning into the retrieval process. Our method extracts logical reasoning structures from natural language queries and then composes the individual cosine similarity scores to formulate the final document scores. This approach enables the retrieval process to handle complex logical reasoning without compromising computational efficiency. Our results on both synthetic and real-world benchmarks demonstrate that the proposed method consistently outperforms traditional retrieval methods across different models and datasets, significantly improving retrieval performance for complex queries. </p>
<blockquote>
<p>ä¼ ç»Ÿæ£€ç´¢æ–¹æ³•ä¾èµ–äºå°†ç”¨æˆ·æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºï¼Œå¹¶åœ¨åµŒå…¥ç©ºé—´ä¸­åŸºäºä½™å¼¦ç›¸ä¼¼æ€§è¿›è¡Œæ–‡æ¡£æ£€ç´¢ã€‚è™½ç„¶è¿™ç§æ–¹æ³•æ•ˆç‡é«˜ä¸”å¯æ‰©å±•æ€§å¼ºï¼Œä½†å®ƒå¾€å¾€éš¾ä»¥å¤„ç†æ¶‰åŠé€»è¾‘ç»“æ„ï¼ˆå¦‚å¦å®šã€åˆå–å’Œæå–ï¼‰çš„å¤æ‚æŸ¥è¯¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ—¶é—´é€»è¾‘æ¨ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ˜¾å¼åœ°å°†é€»è¾‘æ¨ç†çº³å…¥æ£€ç´¢è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»è‡ªç„¶è¯­è¨€æŸ¥è¯¢ä¸­æå–é€»è¾‘æ¨ç†ç»“æ„ï¼Œç„¶åç»„åˆå„ä¸ªä½™å¼¦ç›¸ä¼¼æ€§å¾—åˆ†ä»¥å½¢æˆæœ€ç»ˆçš„æ–‡æ¡£å¾—åˆ†ã€‚è¿™ç§æ–¹æ³•ä½¿æ£€ç´¢è¿‡ç¨‹èƒ½å¤Ÿå¤„ç†å¤æ‚çš„é€»è¾‘æ¨ç†ï¼ŒåŒæ—¶ä¸æŸå®³è®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬åœ¨åˆæˆå’Œç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šçš„ç»“æœéƒ½è¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†ä¸Šå§‹ç»ˆä¼˜äºä¼ ç»Ÿæ£€ç´¢æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†å¤æ‚æŸ¥è¯¢çš„æ£€ç´¢æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17860v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>ä¼ ç»Ÿæ£€ç´¢æ–¹æ³•ä¸»è¦ä¾èµ–å°†ç”¨æˆ·æŸ¥è¯¢è½¬åŒ–ä¸ºå‘é‡è¡¨ç¤ºï¼Œå¹¶åœ¨åµŒå…¥ç©ºé—´ä¸­åŸºäºä½™å¼¦ç›¸ä¼¼æ€§è¿›è¡Œæ–‡æ¡£æ£€ç´¢ã€‚å°½ç®¡è¿™ç§æ–¹æ³•æ•ˆç‡é«˜ä¸”å¯æ‰©å±•æ€§å¼ºï¼Œä½†å¯¹äºæ¶‰åŠé€»è¾‘ç»“æ„å¦‚å¦å®šã€è¿è¯å’Œæå–çš„å¤æ‚æŸ¥è¯¢å¸¸å¸¸å¤„ç†ä¸ä½³ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°çš„æ¨ç†æ—¶é€»è¾‘æ¨ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ˜¾å¼åœ°å°†é€»è¾‘æ¨ç†çº³å…¥æ£€ç´¢è¿‡ç¨‹ã€‚æ­¤æ–¹æ³•ä»è‡ªç„¶è¯­è¨€æŸ¥è¯¢ä¸­æå–é€»è¾‘ç»“æ„ï¼Œç„¶åå°†å„ä¸ªä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†ç»„åˆå½¢æˆæœ€ç»ˆæ–‡æ¡£å¾—åˆ†ã€‚æ­¤æ–¹æ³•ä¸ä»…ä½¿æ£€ç´¢è¿‡ç¨‹èƒ½å¤Ÿå¤„ç†å¤æ‚çš„é€»è¾‘æ¨ç†ï¼Œè€Œä¸”ä¸æŸå¤±è®¡ç®—æ•ˆç‡ã€‚åœ¨åˆæˆå’Œç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šçš„ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†ä¸Šå‡è¡¨ç°ä¼˜äºä¼ ç»Ÿæ£€ç´¢æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤æ‚æŸ¥è¯¢çš„æ£€ç´¢æ€§èƒ½æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä¼ ç»Ÿæ£€ç´¢æ–¹æ³•ä¸»è¦åŸºäºä½™å¼¦ç›¸ä¼¼æ€§è¿›è¡Œæ–‡æ¡£æ£€ç´¢ï¼Œå¯¹äºå¤æ‚é€»è¾‘æŸ¥è¯¢å¤„ç†æ•ˆæœä¸ä½³ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ—¶é€»è¾‘æ¨ç†æ¡†æ¶ï¼Œå°†é€»è¾‘æ¨ç†çº³å…¥æ£€ç´¢è¿‡ç¨‹ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½ä»è‡ªç„¶è¯­è¨€æŸ¥è¯¢ä¸­æå–é€»è¾‘ç»“æ„ã€‚</li>
<li>é€šè¿‡ç»„åˆä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†å½¢æˆæœ€ç»ˆæ–‡æ¡£å¾—åˆ†ï¼Œå¤„ç†å¤æ‚é€»è¾‘æŸ¥è¯¢ã€‚</li>
<li>æ–°æ–¹æ³•åœ¨åˆæˆå’Œç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>æ‰€æå‡ºæ–¹æ³•åœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†ä¸Šå‡é€‚ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17860">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9e4fd531fa2f09d3bcd67ecc83111b2c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d48e8be386f75aed4fcd78b43926d2ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c5c90ed94686b3e592a9456a33ec2bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e728fd7f4c13a9f32ab5e6af8d16c91.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="OmniScience-A-Domain-Specialized-LLM-for-Scientific-Reasoning-and-Discovery"><a href="#OmniScience-A-Domain-Specialized-LLM-for-Scientific-Reasoning-and-Discovery" class="headerlink" title="OmniScience: A Domain-Specialized LLM for Scientific Reasoning and   Discovery"></a>OmniScience: A Domain-Specialized LLM for Scientific Reasoning and   Discovery</h2><p><strong>Authors:Vignesh Prabhakar, Md Amirul Islam, Adam Atanas, Yao-Ting Wang, Joah Han, Aastha Jhunjhunwala, Rucha Apte, Robert Clark, Kang Xu, Zihan Wang, Kai Liu</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable potential in advancing scientific knowledge and addressing complex challenges. In this work, we introduce OmniScience, a specialized large reasoning model for general science, developed through three key components: (1) domain adaptive pretraining on a carefully curated corpus of scientific literature, (2) instruction tuning on a specialized dataset to guide the model in following domain-specific tasks, and (3) reasoning-based knowledge distillation through fine-tuning to significantly enhance its ability to generate contextually relevant and logically sound responses. We demonstrate the versatility of OmniScience by developing a battery agent that efficiently ranks molecules as potential electrolyte solvents or additives. Comprehensive evaluations reveal that OmniScience is competitive with state-of-the-art large reasoning models on the GPQA Diamond and domain-specific battery benchmarks, while outperforming all public reasoning and non-reasoning models with similar parameter counts. We further demonstrate via ablation experiments that domain adaptive pretraining and reasoning-based knowledge distillation are critical to attain our performance levels, across benchmarks. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨è¿›ç§‘å­¦çŸ¥è¯†å’Œåº”å¯¹å¤æ‚æŒ‘æˆ˜æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—æ½œåŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†OmniScienceï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é€šç”¨ç§‘å­¦çš„ä¸“é—¨å¤§å‹æ¨ç†æ¨¡å‹ï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶å¼€å‘è€Œæˆï¼šï¼ˆ1ï¼‰åœ¨ç²¾å¿ƒæŒ‘é€‰çš„ç§‘å­¦æ–‡çŒ®è¯­æ–™åº“ä¸Šè¿›è¡Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼›ï¼ˆ2ï¼‰åœ¨ä¸“é—¨çš„æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹æ‰§è¡Œç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ï¼›ï¼ˆ3ï¼‰é€šè¿‡å¾®è°ƒè¿›è¡ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦ï¼Œä»¥æ˜¾ç€æé«˜å…¶ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³å’Œé€»è¾‘ä¸¥è°¨å›åº”çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡å¼€å‘ä¸€ç§ç”µæ± ä»£ç†æ¥å±•ç¤ºOmniScienceçš„é€šç”¨æ€§ï¼Œè¯¥ä»£ç†èƒ½å¤Ÿé«˜æ•ˆåœ°æ’åˆ—åˆ†å­ä½œä¸ºæ½œåœ¨çš„ç”µè§£è´¨æº¶å‰‚æˆ–æ·»åŠ å‰‚ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒOmniScienceåœ¨GPQA Diamondå’Œç‰¹å®šé¢†åŸŸçš„ç”µæ± åŸºå‡†æµ‹è¯•ä¸Šä¸æœ€æ–°çš„å¤§å‹æ¨ç†æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶åœ¨å‚æ•°æ•°é‡ç›¸ä¼¼çš„æƒ…å†µä¸‹è¶…è¶Šäº†æ‰€æœ‰å…¬å¼€çš„æ¨ç†å’Œéæ¨ç†æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜é€šè¿‡æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯æ˜ï¼Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒå’ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦å¯¹äºè¾¾åˆ°æˆ‘ä»¬çš„æ€§èƒ½æ°´å¹³è‡³å…³é‡è¦ï¼Œè·¨è¶Šå„ç§åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17604v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨åŠ¨ç§‘å­¦çŸ¥è¯†å’Œåº”å¯¹å¤æ‚æŒ‘æˆ˜æ–¹é¢å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€æ¬¾é’ˆå¯¹é€šç”¨ç§‘å­¦çš„ä¸“é—¨å¤§å‹æ¨ç†æ¨¡å‹OmniScienceï¼Œå…¶å¼€å‘åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šä¸€æ˜¯åœ¨ç§‘å­¦æ–‡çŒ®çš„ç²¾å¿ƒç­›é€‰è¯­æ–™åº“ä¸Šè¿›è¡Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼ŒäºŒæ˜¯åœ¨ä¸“é—¨æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹æ‰§è¡Œç‰¹å®šä»»åŠ¡ï¼Œä¸‰æ˜¯é€šè¿‡å¾®è°ƒè¿›è¡ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦ï¼Œä»¥æ˜¾è‘—å¢å¼ºå…¶åœ¨ä¸Šä¸‹æ–‡ç›¸å…³å’Œé€»è¾‘ä¸¥è°¨æ–¹é¢çš„åº”ç­”èƒ½åŠ›ã€‚OmniScienceåœ¨ç”µæ± å‰‚å¼€å‘ä¸­çš„åº”ç”¨å±•ç¤ºäº†å…¶å¤šåŠŸèƒ½æ€§ï¼Œå®ƒèƒ½æœ‰æ•ˆåœ°å¯¹åˆ†å­è¿›è¡Œæ½œåœ¨ç”µè§£è´¨æº¶å‰‚æˆ–æ·»åŠ å‰‚çš„æ’åã€‚ç»¼åˆè¯„ä»·è¡¨æ˜ï¼ŒOmniScienceåœ¨GPQA Diamondå’Œç‰¹å®šç”µæ± åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¸æœ€æ–°å¤§å‹æ¨ç†æ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶ä¼˜äºå‚æ•°ç›¸è¿‘çš„æ‰€æœ‰å…¬å…±æ¨ç†å’Œéæ¨ç†æ¨¡å‹ã€‚é€šè¿‡æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯æ˜ï¼Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒå’ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦å¯¹äºè¾¾åˆ°æˆ‘ä»¬çš„æ€§èƒ½æ°´å¹³è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨åŠ¨ç§‘å­¦çŸ¥è¯†å’Œåº”å¯¹å¤æ‚æŒ‘æˆ˜æ–¹é¢å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>OmniScienceæ˜¯ä¸€æ¬¾é’ˆå¯¹é€šç”¨ç§‘å­¦çš„ä¸“é—¨å¤§å‹æ¨ç†æ¨¡å‹ï¼Œé€šè¿‡é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒã€æŒ‡ä»¤è°ƒæ•´å’ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦ä¸‰ä¸ªå…³é”®æ­¥éª¤è¿›è¡Œå¼€å‘ã€‚</li>
<li>OmniScienceåœ¨ç”µæ± å‰‚å¼€å‘ä¸­çš„åº”ç”¨å±•ç¤ºäº†å…¶å¤šåŠŸèƒ½æ€§ï¼Œèƒ½æœ‰æ•ˆåœ°å¯¹åˆ†å­è¿›è¡Œæ’åã€‚</li>
<li>OmniScienceåœ¨GPQA Diamondå’Œç‰¹å®šç”µæ± åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¸æœ€æ–°å¤§å‹æ¨ç†æ¨¡å‹ç›¸å½“ã€‚</li>
<li>ä¸å…¶ä»–å‚æ•°ç›¸è¿‘çš„æ¨¡å‹ç›¸æ¯”ï¼ŒOmniScienceè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>æ¶ˆèå®éªŒè¯æ˜é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒå’ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦å¯¹äºOmniScienceçš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>OmniScienceçš„å¼€å‘ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦çŸ¥è¯†æ¨è¿›å’Œå¤æ‚æŒ‘æˆ˜åº”å¯¹æ–¹é¢çš„åº”ç”¨æä¾›äº†æ–°æ€è·¯å’Œå·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8f04ec23b178acf57080ea79185a9fbb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5cddc57a4b1e46185ec103df103a43ee.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="LaMOuR-Leveraging-Language-Models-for-Out-of-Distribution-Recovery-in-Reinforcement-Learning"><a href="#LaMOuR-Leveraging-Language-Models-for-Out-of-Distribution-Recovery-in-Reinforcement-Learning" class="headerlink" title="LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in   Reinforcement Learning"></a>LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in   Reinforcement Learning</h2><p><strong>Authors:Chan Kim, Seung-Woo Seo, Seong-Woo Kim</strong></p>
<p>Deep Reinforcement Learning (DRL) has demonstrated strong performance in robotic control but remains susceptible to out-of-distribution (OOD) states, often resulting in unreliable actions and task failure. While previous methods have focused on minimizing or preventing OOD occurrences, they largely neglect recovery once an agent encounters such states. Although the latest research has attempted to address this by guiding agents back to in-distribution states, their reliance on uncertainty estimation hinders scalability in complex environments. To overcome this limitation, we introduce Language Models for Out-of-Distribution Recovery (LaMOuR), which enables recovery learning without relying on uncertainty estimation. LaMOuR generates dense reward codes that guide the agent back to a state where it can successfully perform its original task, leveraging the capabilities of LVLMs in image description, logical reasoning, and code generation. Experimental results show that LaMOuR substantially enhances recovery efficiency across diverse locomotion tasks and even generalizes effectively to complex environments, including humanoid locomotion and mobile manipulation, where existing methods struggle. The code and supplementary materials are available at <a target="_blank" rel="noopener" href="https://lamour-rl.github.io/">https://lamour-rl.github.io/</a>. </p>
<blockquote>
<p>æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰åœ¨æœºå™¨äººæ§åˆ¶æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†ä»æ˜“å—åˆ°è¶…å‡ºåˆ†å¸ƒï¼ˆOODï¼‰çŠ¶æ€çš„å½±å“ï¼Œè¿™å¾€å¾€ä¼šå¯¼è‡´è¡Œä¸ºä¸å¯é å’Œä»»åŠ¡å¤±è´¥ã€‚è™½ç„¶ä¹‹å‰çš„æ–¹æ³•ä¾§é‡äºå‡å°‘æˆ–é˜²æ­¢OODçš„å‘ç”Ÿï¼Œä½†å®ƒä»¬åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†ä»£ç†åœ¨é‡åˆ°æ­¤ç±»çŠ¶æ€æ—¶å¦‚ä½•è¿›è¡Œæ¢å¤ã€‚è™½ç„¶æœ€æ–°çš„ç ”ç©¶è¯•å›¾é€šè¿‡å¼•å¯¼ä»£ç†æ¢å¤åˆ°åˆ†å¸ƒå†…çš„çŠ¶æ€æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†å®ƒä»¬å¯¹ä¸ç¡®å®šæ€§ä¼°è®¡çš„ä¾èµ–é˜»ç¢äº†åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¯æ‰©å±•æ€§ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç”¨äºè¶…å‡ºåˆ†å¸ƒæ¢å¤çš„è¯­è¨€æ¨¡å‹ï¼ˆLaMOuRï¼‰ï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¸ä¾èµ–ä¸ç¡®å®šæ€§ä¼°è®¡çš„æƒ…å†µä¸‹è¿›è¡Œæ¢å¤å­¦ä¹ ã€‚LaMOuRç”Ÿæˆå¯†é›†çš„å¥–åŠ±ä»£ç ï¼Œå¼•å¯¼ä»£ç†æ¢å¤åˆ°èƒ½å¤ŸæˆåŠŸæ‰§è¡Œå…¶åŸå§‹ä»»åŠ¡çš„çŠ¶æ€ï¼Œåˆ©ç”¨LVLMsåœ¨å›¾åƒæè¿°ã€é€»è¾‘æ¨ç†å’Œä»£ç ç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaMOuRåœ¨å¤šç§è¿åŠ¨ä»»åŠ¡ä¸­å¤§å¤§æé«˜äº†æ¢å¤æ•ˆç‡ï¼Œå¹¶ä¸”åœ¨åŒ…æ‹¬äººå½¢è¿åŠ¨å’Œç§»åŠ¨æ“çºµåœ¨å†…çš„å¤æ‚ç¯å¢ƒä¸­ä¹Ÿå®ç°äº†æœ‰æ•ˆçš„æ³›åŒ–ï¼Œç°æœ‰æ–¹æ³•åœ¨è¿™äº›ç¯å¢ƒä¸­å¾ˆéš¾åº”å¯¹ã€‚ç›¸å…³ä»£ç å’Œè¡¥å……ææ–™å¯é€šè¿‡ <a target="_blank" rel="noopener" href="https://lamour-rl.github.io/">https://lamour-rl.github.io</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17125v5">PDF</a> 14 pages, 16 figures</p>
<p><strong>Summary</strong></p>
<p>DRLåœ¨æœºå™¨äººæ§åˆ¶ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†æ˜“å—åˆ†å¸ƒå¤–çŠ¶æ€çš„å½±å“ï¼Œå¯èƒ½å¯¼è‡´ä¸å¯é çš„åŠ¨ä½œå’Œä»»åŠ¡å¤±è´¥ã€‚å°½ç®¡æœ€æ–°ç ”ç©¶å°è¯•é€šè¿‡ä¸ç¡®å®šæ€§ä¼°è®¡å¼•å¯¼æ™ºèƒ½ä½“æ¢å¤çŠ¶æ€ï¼Œä½†å®ƒä»¬åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¯æ‰©å±•æ€§å—é™ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œå¼•å…¥LaMOuRï¼ˆç”¨äºåˆ†å¸ƒå¤–æ¢å¤çš„è¯­è¨€æ¨¡å‹ï¼‰ï¼Œå¯åœ¨ä¸ä¾èµ–ä¸ç¡®å®šæ€§ä¼°è®¡çš„æƒ…å†µä¸‹å®ç°æ¢å¤å­¦ä¹ ã€‚LaMOuRç”Ÿæˆå¯†é›†å¥–åŠ±ä»£ç ï¼Œå¼•å¯¼æ™ºèƒ½ä½“å›åˆ°æˆåŠŸæ‰§è¡ŒåŸå§‹ä»»åŠ¡çš„çŠ¶æ€ï¼Œåˆ©ç”¨LVLMsåœ¨å›¾åƒæè¿°ã€é€»è¾‘æ¨ç†å’Œä»£ç ç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaMOuRåœ¨å¤šç§è¿åŠ¨ä»»åŠ¡ä¸­å¤§å¹…æé«˜æ¢å¤æ•ˆç‡ï¼Œå¹¶åœ¨åŒ…æ‹¬äººå½¢è¿åŠ¨å’Œç§»åŠ¨æ“ä½œåœ¨å†…çš„å¤æ‚ç¯å¢ƒä¸­æœ‰æ•ˆæ¨å¹¿ã€‚ç›¸å…³ä»£ç å’Œè¡¥å……ææ–™å¯åœ¨[<a target="_blank" rel="noopener" href="https://lamour-rl.github.io/]%E6%89%BE%E5%88%B0%E3%80%82">https://lamour-rl.github.io/]æ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DRLåœ¨æœºå™¨äººæ§åˆ¶ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†ä»é¢ä¸´åˆ†å¸ƒå¤–çŠ¶æ€çš„é—®é¢˜ï¼Œå¯èƒ½å¯¼è‡´ä»»åŠ¡å¤±è´¥ã€‚</li>
<li>è™½ç„¶å­˜åœ¨ä¾èµ–ä¸ç¡®å®šæ€§ä¼°è®¡çš„OODæ¢å¤æ–¹æ³•ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†å¤æ‚ç¯å¢ƒæ—¶çš„å¯æ‰©å±•æ€§å—é™ã€‚</li>
<li>LaMOuRæ˜¯ä¸€ä¸ªæ–°çš„æ–¹æ³•ï¼Œèƒ½åœ¨ä¸ä¾èµ–ä¸ç¡®å®šæ€§ä¼°è®¡çš„æƒ…å†µä¸‹å®ç°æ¢å¤å­¦ä¹ ã€‚</li>
<li>LaMOuRç”Ÿæˆå¯†é›†å¥–åŠ±ä»£ç ä»¥å¼•å¯¼æ™ºèƒ½ä½“è¿”å›æ­£å¸¸çŠ¶æ€ã€‚</li>
<li>LaMOuRåˆ©ç”¨äº†LVLMsçš„å›¾åƒæè¿°ã€é€»è¾‘æ¨ç†å’Œä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒLaMOuRåœ¨å„ç§è¿åŠ¨ä»»åŠ¡ä¸­å¤§å¤§æé«˜äº†æ¢å¤æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17125">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c19b44eeddf784c296bf26900a528eb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-846259e84acc5c01c91b10780fdf95a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22f2dd9c5c03bc7a882fcae157287934.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f23aaf7b04a41070fb8013f55b5d00cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eea075579f8f2967ed417412e6b2532e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52aade789578d1b7a7e495e40608dd50.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-18a3dfa8e96354d0847c69819873b152.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-05/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-05/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-05/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5171bf48e6d88508100cb555a7695ef3.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-05  Sparse Autoencoders Learn Monosemantic Features in Vision-Language   Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-617852c0f38d79df3722e8dd0badeec2.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  Progressive Human Motion Generation Based on Text and Few Motion Frames
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18179.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
