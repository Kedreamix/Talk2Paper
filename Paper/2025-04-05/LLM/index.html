<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-05  Sparse Autoencoders Learn Monosemantic Features in Vision-Language   Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-5171bf48e6d88508100cb555a7695ef3.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-05-æ›´æ–°"><a href="#2025-04-05-æ›´æ–°" class="headerlink" title="2025-04-05 æ›´æ–°"></a>2025-04-05 æ›´æ–°</h1><h2 id="Sparse-Autoencoders-Learn-Monosemantic-Features-in-Vision-Language-Models"><a href="#Sparse-Autoencoders-Learn-Monosemantic-Features-in-Vision-Language-Models" class="headerlink" title="Sparse Autoencoders Learn Monosemantic Features in Vision-Language   Models"></a>Sparse Autoencoders Learn Monosemantic Features in Vision-Language   Models</h2><p><strong>Authors:Mateusz Pach, Shyamgopal Karthik, Quentin Bouniot, Serge Belongie, Zeynep Akata</strong></p>
<p>Sparse Autoencoders (SAEs) have recently been shown to enhance interpretability and steerability in Large Language Models (LLMs). In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce a comprehensive framework for evaluating monosemanticity in vision representations. Our experimental results reveal that SAEs trained on VLMs significantly enhance the monosemanticity of individual neurons while also exhibiting hierarchical representations that align well with expert-defined structures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that applying SAEs to intervene on a CLIP vision encoder, directly steer output from multimodal LLMs (e.g., LLaVA) without any modifications to the underlying model. These findings emphasize the practicality and efficacy of SAEs as an unsupervised approach for enhancing both the interpretability and control of VLMs. </p>
<blockquote>
<p>ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSparse Autoencodersï¼ŒSAEsï¼‰æœ€è¿‘è¢«è¯æ˜å¯ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelsï¼ŒLLMsï¼‰çš„å¯è§£é‡Šæ€§å’Œå¯æ“æ§æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†SAEçš„åº”ç”¨æ‰©å±•åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVision-Language Modelsï¼ŒVLMsï¼‰ï¼Œå¦‚CLIPï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶æ¥è¯„ä¼°è§†è§‰è¡¨ç¤ºä¸­çš„å•è¯­ä¹‰æ€§ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨VLMä¸Šè®­ç»ƒçš„SAEæ˜¾è‘—æé«˜äº†å•ä¸ªç¥ç»å…ƒçš„å•è¯­ä¹‰æ€§ï¼ŒåŒæ—¶è¡¨ç°å‡ºä¸ä¸“å®¶å®šä¹‰ç»“æ„ï¼ˆå¦‚iNaturaliståˆ†ç±»æ³•ï¼‰å¯¹é½çš„å±‚æ¬¡è¡¨ç¤ºã€‚å°¤å…¶å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜äº†å°†SAEåº”ç”¨äºCLIPè§†è§‰ç¼–ç å™¨è¿›è¡Œå¹²é¢„ï¼Œå¯ä»¥ç›´æ¥æ§åˆ¶å¤šæ¨¡æ€LLMï¼ˆå¦‚LLaVAï¼‰çš„è¾“å‡ºï¼Œè€Œæ— éœ€å¯¹åº•å±‚æ¨¡å‹è¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†SAEä½œä¸ºæé«˜VLMså¯è§£é‡Šæ€§å’Œæ§åˆ¶èƒ½åŠ›çš„æ— ç›‘ç£æ–¹æ³•çš„å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02821v1">PDF</a> Preprint. The code is available at   <a target="_blank" rel="noopener" href="https://github.com/ExplainableML/sae-for-vlm">https://github.com/ExplainableML/sae-for-vlm</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†å°†ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰åº”ç”¨äºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¦‚CLIPçš„å¢å¼ºæ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªè¯„ä¼°è§†è§‰è¡¨ç¤ºä¸­monosemanticityçš„å…¨é¢æ¡†æ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨VLMä¸Šè®­ç»ƒçš„SAEèƒ½å¤Ÿæ˜¾è‘—æé«˜å•ä¸ªç¥ç»å…ƒçš„monosemanticityï¼Œå¹¶å±•ç°å‡ºä¸ä¸“å®¶å®šä¹‰ç»“æ„å¯¹é½çš„å±‚æ¬¡è¡¨ç¤ºï¼ˆå¦‚iNaturaliståˆ†ç±»å­¦ï¼‰ã€‚ç‰¹åˆ«æ˜¯ï¼Œç ”ç©¶å‘ç°å¯¹CLIPçš„è§†è§‰ç¼–ç å™¨åº”ç”¨SAEï¼Œå¯ä»¥ç›´æ¥æ§åˆ¶å¤šæ¨¡æ€LLMï¼ˆå¦‚LLaVAï¼‰çš„è¾“å‡ºï¼Œè€Œæ— éœ€ä¿®æ”¹åº•å±‚æ¨¡å‹ã€‚è¿™å¼ºè°ƒäº†SAEä½œä¸ºæé«˜VLMè§£é‡Šæ€§å’Œæ§åˆ¶èƒ½åŠ›çš„æ— ç›‘ç£æ–¹æ³•çš„å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAEè¢«æˆåŠŸåº”ç”¨äºå¢å¼ºVision-Language Modelsï¼ˆVLMsï¼‰çš„monosemanticityã€‚</li>
<li>SAEè®­ç»ƒèƒ½æ˜¾è‘—æé«˜VLMä¸­å•ä¸ªç¥ç»å…ƒçš„monosemanticityã€‚</li>
<li>SAEå±•ç°ä¸ä¸“å®¶å®šä¹‰ç»“æ„å¯¹é½çš„å±‚æ¬¡è¡¨ç¤ºã€‚</li>
<li>å¯¹CLIPçš„è§†è§‰ç¼–ç å™¨åº”ç”¨SAEå¯ä»¥ç›´æ¥æ§åˆ¶å¤šæ¨¡æ€LLMçš„è¾“å‡ºã€‚</li>
<li>SAEå¯ä½œä¸ºæé«˜VLMè§£é‡Šæ€§å’Œæ§åˆ¶èƒ½åŠ›çš„æ— ç›‘ç£æ–¹æ³•ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†SAEåœ¨VLMä¸­çš„å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02821">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7ca44d3aa3634af339399c89df48c7a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29539167a6cb456acbfb1d22c53e9abd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-44e70f7ce24214acb576e0a4e9815006.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64d3271be83941d955036350394b8d05.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Generative-Evaluation-of-Complex-Reasoning-in-Large-Language-Models"><a href="#Generative-Evaluation-of-Complex-Reasoning-in-Large-Language-Models" class="headerlink" title="Generative Evaluation of Complex Reasoning in Large Language Models"></a>Generative Evaluation of Complex Reasoning in Large Language Models</h2><p><strong>Authors:Haowei Lin, Xiangyu Wang, Ruilin Yan, Baizhou Huang, Haotian Ye, Jianhua Zhu, Zihao Wang, James Zou, Jianzhu Ma, Yitao Liang</strong></p>
<p>With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMOâ€™s value as a robust, enduring assessment tool for genuine LLM reasoning capabilities. </p>
<blockquote>
<p>éšç€å¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºè¶…äººç±»çš„æ¨ç†èƒ½åŠ›ï¼Œä¸€ä¸ªé‡è¦çš„é—®é¢˜å‡ºç°äº†ï¼šLLMæ˜¯çœŸæ­£è¿›è¡Œæ¨ç†ï¼Œè¿˜æ˜¯ä»…ä»…ä»å®ƒä»¬å¹¿æ³›çš„ç½‘ç»œæŠ“å–è®­ç»ƒæ•°æ®é›†ä¸­å›å¿†ç­”æ¡ˆï¼Ÿä¸€æ—¦çº³å…¥åç»­çš„LLMè®­ç»ƒé›†ï¼Œå…¬å¼€å‘å¸ƒçš„åŸºå‡†æµ‹è¯•ä¸å¯é¿å…åœ°ä¼šè¢«æ±¡æŸ“ï¼Œä»è€Œç ´åäº†å®ƒä»¬ä½œä¸ºå¿ å®è¯„ä¼°çš„å¯é æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†KUMOï¼Œä¸€ä¸ªä¸“é—¨ä¸ºè¯„ä¼°LLMä¸­çš„æ¨ç†èƒ½åŠ›è€Œè®¾è®¡çš„ç”Ÿæˆè¯„ä¼°æ¡†æ¶ã€‚KUMOååŒåœ°å°†LLMä¸ç¬¦å·å¼•æ“ç›¸ç»“åˆï¼Œä»¥åŠ¨æ€ç”Ÿæˆå¤šæ ·åŒ–ã€å¤šå›åˆçš„æ¨ç†ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡æ˜¯éƒ¨åˆ†å¯è§‚å¯Ÿçš„å’Œå¯è°ƒèŠ‚éš¾åº¦çš„ã€‚é€šè¿‡è‡ªåŠ¨åŒ–ç®¡é“ï¼ŒKUMOä¸æ–­ç”Ÿæˆè·¨å¼€æ”¾é¢†åŸŸçš„å…¨æ–°ä»»åŠ¡ï¼Œä¿ƒä½¿æ¨¡å‹å±•ç¤ºçœŸæ­£çš„æ³›åŒ–èƒ½åŠ›è€Œéè®°å¿†èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨KUMOåˆ›å»ºçš„5000ä¸ªä»»åŠ¡ã€100ä¸ªé¢†åŸŸä¸­å¯¹23ä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œä»¥è¯„ä¼°å…¶æ¨ç†èƒ½åŠ›ä¸å¤§å­¦ç”Ÿçš„æ°´å¹³ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè®¸å¤šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç®€å•çš„æ¨ç†ä»»åŠ¡ä¸Šè¶…è¶Šäº†å¤§å­¦æ°´å¹³çš„è¡¨ç°ï¼Œå¹¶ä¸”åœ¨å¤æ‚çš„æ¨ç†æŒ‘æˆ˜ä¸Šè¾¾åˆ°äº†å¤§å­¦æ°´å¹³çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨KUMOä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸æ–°å‘å¸ƒçš„ç°å®ä¸–ç•Œæ¨ç†åŸºå‡†æµ‹è¯•çš„ç»“æœä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ï¼Œè¿™å‡¸æ˜¾äº†KUMOä½œä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çœŸå®æ¨ç†èƒ½åŠ›çš„ç¨³å¥ã€æŒä¹…è¯„ä¼°å·¥å…·çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02810v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°çš„è¶…å‡¡æ¨ç†èƒ½åŠ›ï¼Œäººä»¬å¼€å§‹è´¨ç–‘ï¼šLLMæ˜¯å¦çœŸçš„å…·å¤‡æ¨ç†èƒ½åŠ›ï¼Œè¿˜æ˜¯ä»…ä»…æ˜¯ä¾é ä»åºå¤§çš„ç½‘ç»œçˆ¬è™«è®­ç»ƒæ•°æ®é›†ä¸­å›å¿†ç­”æ¡ˆï¼Ÿä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºKUMOè¯„ä¼°æ¡†æ¶ï¼Œä¸“ä¸ºè¯„ä¼°LLMçš„æ¨ç†èƒ½åŠ›è€Œè®¾è®¡ã€‚KUMOèƒ½å¤ŸååŒç»“åˆLLMä¸ç¬¦å·å¼•æ“ï¼ŒåŠ¨æ€ç”Ÿæˆå¤šæ ·åŒ–ã€å¤šå›åˆçš„æ¨ç†ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡å…·æœ‰éƒ¨åˆ†å¯è§‚æ€§å’Œå¯è°ƒæ•´çš„éš¾åº¦ã€‚é€šè¿‡è‡ªåŠ¨åŒ–ç®¡é“ï¼ŒKUMOèƒ½å¤ŸæŒç»­ç”Ÿæˆè·¨è¶Šå¼€æ”¾å¼é¢†åŸŸçš„å…¨æ–°ä»»åŠ¡ï¼Œä¿ƒä½¿æ¨¡å‹å±•ç¤ºçœŸæ­£çš„æ³›åŒ–èƒ½åŠ›è€Œéè®°å¿†åŠ›ã€‚å¯¹23æ¬¾æœ€å…ˆè¿›çš„LLMè¿›è¡Œ5000ä¸ªä»»åŠ¡çš„è¯„ä¼°ï¼Œå¹¶ä¸å¤§å­¦ç”Ÿè¿›è¡ŒåŸºå‡†æµ‹è¯•å¯¹æ¯”ï¼Œå‘ç°è®¸å¤šLLMåœ¨ç®€å•æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè¶…è¶Šå¤§å­¦ç”Ÿçš„èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨å¤æ‚çš„æ¨ç†æŒ‘æˆ˜ä¸­è¾¾åˆ°å¤§å­¦ç”Ÿæ°´å¹³ã€‚æ­¤å¤–ï¼ŒLLMåœ¨KUMOä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸æ–°å‘å¸ƒçš„ç°å®ä¸–ç•Œæ¨ç†åŸºå‡†æµ‹è¯•çš„ç»“æœä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ï¼Œå‡¸æ˜¾KUMOä½œä¸ºè¯„ä¼°LLMçœŸå®æ¨ç†èƒ½åŠ›çš„ç¨³å¥ã€æŒä¹…å·¥å…·çš„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºè¶…å‡¡çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¯¹LLMæ˜¯å¦çœŸæ­£å…·å¤‡æ¨ç†èƒ½åŠ›å­˜åœ¨è´¨ç–‘ï¼Œæ‹…å¿§å…¶ä»…ä¾èµ–è®­ç»ƒæ•°æ®é›†è¿›è¡Œå›ç­”ã€‚</li>
<li>å¼•å…¥KUMOè¯„ä¼°æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>KUMOç»“åˆLLMä¸ç¬¦å·å¼•æ“ï¼Œç”Ÿæˆå¤šæ ·åŒ–ã€å¤šå›åˆçš„æ¨ç†ä»»åŠ¡ã€‚</li>
<li>KUMOä»»åŠ¡å…·æœ‰éƒ¨åˆ†å¯è§‚æ€§å’Œå¯è°ƒæ•´çš„éš¾åº¦ï¼Œèƒ½å¤ŸæŒç»­ç”Ÿæˆæ–°ä»»åŠ¡ã€‚</li>
<li>å¯¹æ¯”è¯„ä¼°æ˜¾ç¤ºï¼Œè®¸å¤šLLMåœ¨ç®€å•å’Œå¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¶Šæˆ–è¾¾åˆ°å¤§å­¦ç”Ÿæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed25ba3de363f72487caf1dceddb5297.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b302c448b5383e95b012f6a37d95def4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f52f5abde00ea7eaa9d38b4b59d9118.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Framework-for-Robust-Cognitive-Evaluation-of-LLMs"><a href="#A-Framework-for-Robust-Cognitive-Evaluation-of-LLMs" class="headerlink" title="A Framework for Robust Cognitive Evaluation of LLMs"></a>A Framework for Robust Cognitive Evaluation of LLMs</h2><p><strong>Authors:Karin de Langis, Jong Inn Park, Bin Hu, Khanh Chi Le, Andreas Schramm, Michael C. Mensink, Andrew Elfenbein, Dongyeop Kang</strong></p>
<p>Emergent cognitive abilities in large language models (LLMs) have been widely observed, but their nature and underlying mechanisms remain poorly understood. A growing body of research draws on cognitive science to investigate LLM cognition, but standard methodologies and experimen-tal pipelines have not yet been established. To address this gap we develop CognitivEval, a framework for systematically evaluating the artificial cognitive capabilities of LLMs, with a particular emphasis on robustness in response collection. The key features of CognitivEval include: (i) automatic prompt permutations, and (ii) testing that gathers both generations and model probability estimates. Our experiments demonstrate that these features lead to more robust experimental outcomes. Using CognitivEval, we replicate five classic experiments in cognitive science, illustrating the frameworkâ€™s generalizability across various experimental tasks and obtaining a cognitive profile of several state of the art LLMs. CognitivEval will be released publicly to foster broader collaboration within the cognitive science community. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ–°å…´è®¤çŸ¥èƒ½åŠ›å·²è¢«å¹¿æ³›è§‚å¯Ÿï¼Œä½†å…¶æœ¬è´¨å’Œæ½œåœ¨æœºåˆ¶ä»çŸ¥ä¹‹ç”šå°‘ã€‚è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶åˆ©ç”¨è®¤çŸ¥ç§‘å­¦æ¥æ¢ç©¶LLMçš„è®¤çŸ¥èƒ½åŠ›ï¼Œä½†å°šæœªå»ºç«‹æ ‡å‡†çš„æ–¹æ³•å’Œå®éªŒæµç¨‹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼€å‘äº†CognitivEvalï¼Œè¿™æ˜¯ä¸€ä¸ªç³»ç»Ÿåœ°è¯„ä¼°LLMäººå·¥è®¤çŸ¥èƒ½åŠ›çš„æ¡†æ¶ï¼Œç‰¹åˆ«ä¾§é‡äºå“åº”æ”¶é›†çš„ç¨³å¥æ€§ã€‚CognitivEvalçš„å…³é”®åŠŸèƒ½åŒ…æ‹¬ï¼šï¼ˆiï¼‰è‡ªåŠ¨æç¤ºæ’åˆ—ç»„åˆï¼Œï¼ˆiiï¼‰æµ‹è¯•æ”¶é›†æ—¢åŒ…å«ç”Ÿæˆå†…å®¹åˆåŒ…å«æ¨¡å‹æ¦‚ç‡ä¼°è®¡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™äº›åŠŸèƒ½å¸¦æ¥äº†æ›´ç¨³å¥çš„å®éªŒç»“æœã€‚é€šè¿‡ä½¿ç”¨CognitivEvalï¼Œæˆ‘ä»¬å¤åˆ¶äº†è®¤çŸ¥ç§‘å­¦ä¸­çš„äº”ä¸ªç»å…¸å®éªŒï¼Œè¯´æ˜äº†è¯¥æ¡†æ¶åœ¨å„ç§å®éªŒä»»åŠ¡ä¸­çš„é€šç”¨æ€§ï¼Œå¹¶è·å¾—äº†å¤šä¸ªæœ€æ–°LLMçš„è®¤çŸ¥ç‰¹å¾ã€‚æˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒCognitivEvalï¼Œä»¥ä¿ƒè¿›è®¤çŸ¥ç§‘å­¦ç•Œçš„å¹¿æ³›åˆä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02789v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMçš„æ–°å…´è®¤çŸ¥èƒ½åŠ›å¾—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†å…¶æœ¬è´¨å’Œå†…åœ¨æœºåˆ¶ä»ç†è§£æœ‰é™ã€‚ç ”ç©¶è€…ä»¬åˆ©ç”¨è®¤çŸ¥ç§‘å­¦ç†è®ºæ¥æ¢ç´¢LLMçš„è®¤çŸ¥èƒ½åŠ›ï¼Œä½†ç›®å‰å°šæœªå½¢æˆç»Ÿä¸€çš„æ–¹æ³•è®ºå’Œå®éªŒæµç¨‹ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œç ”ç©¶è€…å¼€å‘äº†ä¸€ä¸ªåä¸ºCognitivEvalçš„æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°LLMçš„è®¤çŸ¥èƒ½åŠ›ï¼Œç‰¹åˆ«å¼ºè°ƒå“åº”æ”¶é›†æ—¶çš„ç¨³å¥æ€§ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬è‡ªåŠ¨æç¤ºæ’åˆ—å’Œæµ‹è¯•ç”Ÿæˆä¸æ¨¡å‹æ¦‚ç‡ä¼°è®¡ä¸¤ä¸ªå…³é”®åŠŸèƒ½ã€‚å®éªŒè¯æ˜è¿™äº›åŠŸèƒ½æé«˜äº†å®éªŒç»“æœçš„ç¨³å¥æ€§ã€‚é€šè¿‡CognitivEvalæ¡†æ¶å¤åˆ¶äº†äº”ä¸ªç»å…¸çš„è®¤çŸ¥ç§‘å­¦å®éªŒï¼Œè¯æ˜äº†å…¶è·¨ä¸åŒå®éªŒä»»åŠ¡çš„é€šç”¨æ€§ï¼Œå¹¶è·å–äº†å‡ ç§å…ˆè¿›LLMçš„è®¤çŸ¥ç‰¹æ€§ã€‚è¯¥æ¡†æ¶å°†å…¬å¼€å‘å¸ƒä»¥ä¿ƒè¿›è®¤çŸ¥ç§‘å­¦ç•Œçš„åˆä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„æ–°å…´è®¤çŸ¥èƒ½åŠ›å¤‡å—å…³æ³¨ï¼Œä½†å…¶æœ¬è´¨å’Œå†…åœ¨æœºåˆ¶ä»éœ€æ·±å…¥ç ”ç©¶ã€‚</li>
<li>ç›®å‰ç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°LLMè®¤çŸ¥èƒ½åŠ›çš„æ–¹æ³•è®ºå’Œå®éªŒæµç¨‹ã€‚</li>
<li>CognitivEvalæ¡†æ¶æ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°LLMçš„è®¤çŸ¥èƒ½åŠ›ï¼Œç‰¹åˆ«å¼ºè°ƒå“åº”æ”¶é›†çš„ç¨³å¥æ€§ã€‚</li>
<li>CognitivEvalåŒ…æ‹¬è‡ªåŠ¨æç¤ºæ’åˆ—å’Œæµ‹è¯•ç”Ÿæˆä¸æ¨¡å‹æ¦‚ç‡ä¼°è®¡ä¸¤ä¸ªå…³é”®åŠŸèƒ½ã€‚</li>
<li>å®éªŒè¯æ˜è¿™ä¸¤ä¸ªå…³é”®åŠŸèƒ½æé«˜äº†å®éªŒç»“æœçš„ç¨³å¥æ€§ã€‚</li>
<li>é€šè¿‡CognitivEvalæ¡†æ¶æˆåŠŸå¤åˆ¶äº†äº”ä¸ªç»å…¸çš„è®¤çŸ¥ç§‘å­¦å®éªŒï¼ŒéªŒè¯äº†å…¶è·¨ä¸åŒå®éªŒä»»åŠ¡çš„é€šç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02789">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c2e6bdbb9dad5abd18f280a24d051878.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-77382e82379c9fd69e803909377fd927.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a6bda32dcbbdc924da0ebe8a9f2eb6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-110361a3fc47dc58d08fbc16bb85e818.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b62f31807c27d6c2b005f788721d04aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c82a3b6afbed898ea8ac11304fb3034.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="BT-ACTION-A-Test-Driven-Approach-for-Modular-Understanding-of-User-Instruction-Leveraging-Behaviour-Trees-and-LLMs"><a href="#BT-ACTION-A-Test-Driven-Approach-for-Modular-Understanding-of-User-Instruction-Leveraging-Behaviour-Trees-and-LLMs" class="headerlink" title="BT-ACTION: A Test-Driven Approach for Modular Understanding of User   Instruction Leveraging Behaviour Trees and LLMs"></a>BT-ACTION: A Test-Driven Approach for Modular Understanding of User   Instruction Leveraging Behaviour Trees and LLMs</h2><p><strong>Authors:Alexander Leszczynski, Sarah Gillet, Iolanda Leite, Fethiye Irmak Dogan</strong></p>
<p>Natural language instructions are often abstract and complex, requiring robots to execute multiple subtasks even for seemingly simple queries. For example, when a user asks a robot to prepare avocado toast, the task involves several sequential steps. Moreover, such instructions can be ambiguous or infeasible for the robot or may exceed the robotâ€™s existing knowledge. While Large Language Models (LLMs) offer strong language reasoning capabilities to handle these challenges, effectively integrating them into robotic systems remains a key challenge. To address this, we propose BT-ACTION, a test-driven approach that combines the modular structure of Behavior Trees (BT) with LLMs to generate coherent sequences of robot actions for following complex user instructions, specifically in the context of preparing recipes in a kitchen-assistance setting. We evaluated BT-ACTION in a comprehensive user study with 45 participants, comparing its performance to direct LLM prompting. Results demonstrate that the modular design of BT-ACTION helped the robot make fewer mistakes and increased user trust, and participants showed a significant preference for the robot leveraging BT-ACTION. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/1Eggbert7/BT_LLM">https://github.com/1Eggbert7/BT_LLM</a>. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€æŒ‡ä»¤é€šå¸¸æ˜¯æŠ½è±¡ä¸”å¤æ‚çš„ï¼Œç”šè‡³å¯¹äºçœ‹ä¼¼ç®€å•çš„æŸ¥è¯¢ï¼Œä¹Ÿéœ€è¦æœºå™¨äººæ‰§è¡Œå¤šä¸ªå­ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œå½“ç”¨æˆ·è¦æ±‚æœºå™¨äººå‡†å¤‡é³„æ¢¨åå¸æ—¶ï¼Œè¯¥ä»»åŠ¡æ¶‰åŠå¤šä¸ªè¿ç»­æ­¥éª¤ã€‚æ­¤å¤–ï¼Œè¿™äº›æŒ‡ä»¤å¯¹äºæœºå™¨äººæ¥è¯´å¯èƒ½æ˜¯æ¨¡ç³Šæˆ–ä¸å¯è¡Œçš„ï¼Œæˆ–è€…å¯èƒ½è¶…å‡ºæœºå™¨äººçš„ç°æœ‰çŸ¥è¯†èŒƒå›´ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›äº†å¼ºå¤§çš„è¯­è¨€æ¨ç†èƒ½åŠ›æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œä½†å¦‚ä½•æœ‰æ•ˆåœ°å°†å®ƒä»¬æ•´åˆåˆ°æœºå™¨äººç³»ç»Ÿä¸­ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†BT-ACTIONï¼Œè¿™æ˜¯ä¸€ç§æµ‹è¯•é©±åŠ¨çš„æ–¹æ³•ï¼Œå®ƒå°†è¡Œä¸ºæ ‘ï¼ˆBTï¼‰çš„æ¨¡å—åŒ–ç»“æ„ä¸LLMç›¸ç»“åˆï¼Œä»¥ç”Ÿæˆæœºå™¨äººæ‰§è¡Œå¤æ‚ç”¨æˆ·æŒ‡ä»¤çš„è¿è´¯è¡ŒåŠ¨åºåˆ—ï¼Œç‰¹åˆ«æ˜¯åœ¨å¨æˆ¿è¾…åŠ©ç¯å¢ƒä¸­å‡†å¤‡é£Ÿè°±çš„æƒ…å¢ƒä¸‹ã€‚æˆ‘ä»¬åœ¨ä¸€é¡¹æœ‰45åå‚ä¸è€…çš„ç»¼åˆç”¨æˆ·ç ”ç©¶ä¸­è¯„ä¼°äº†BT-ACTIONçš„æ€§èƒ½ï¼Œå¹¶å°†å…¶ä¸ç›´æ¥LLMæç¤ºè¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒBT-ACTIONçš„æ¨¡å—åŒ–è®¾è®¡å¸®åŠ©æœºå™¨äººå‡å°‘äº†é”™è¯¯ï¼Œå¹¶å¢åŠ äº†ç”¨æˆ·ä¿¡ä»»ï¼Œå‚ä¸è€…æ˜¾è‘—æ›´å€¾å‘äºä½¿ç”¨BT-ACTIONçš„æœºå™¨äººã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/1Eggbert7/BT_LLM">https://github.com/1Eggbert7/BT_LLM</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02779v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æœºå™¨äººåœ¨æ‰§è¡Œå¤æ‚è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å‡†å¤‡é£Ÿè°±ç­‰ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†BT-ACTIONæ–¹æ³•ï¼Œç»“åˆè¡Œä¸ºæ ‘ï¼ˆBTï¼‰çš„æ¨¡å—åŒ–ç»“æ„å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ç”Ÿæˆè¿è´¯çš„æœºå™¨äººè¡ŒåŠ¨åºåˆ—ã€‚é€šè¿‡ç”¨æˆ·ç ”ç©¶è¯„ä¼°ï¼ŒBT-ACTIONåœ¨å¨æˆ¿è¾…åŠ©ç¯å¢ƒä¸­æ‰§è¡Œå¤æ‚ç”¨æˆ·æŒ‡ä»¤æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œæé«˜äº†ç”¨æˆ·ä¿¡ä»»åº¦å¹¶è·å¾—äº†ç”¨æˆ·çš„æ˜¾è‘—åå¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç„¶è¯­è¨€æŒ‡ä»¤å¯¹äºæœºå™¨äººæ¥è¯´å¸¸å¸¸æŠ½è±¡ä¸”å¤æ‚ï¼Œéœ€è¦æ‰§è¡Œå¤šä¸ªå­ä»»åŠ¡ã€‚</li>
<li>åœ¨å¨æˆ¿è¾…åŠ©ç¯å¢ƒä¸­ï¼Œæœºå™¨äººå¤„ç†å¤æ‚æŒ‡ä»¤å¦‚å‡†å¤‡é£Ÿè°±æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡å¼ºå¤§çš„è¯­è¨€æ¨ç†èƒ½åŠ›ï¼Œä½†å°†å…¶æœ‰æ•ˆé›†æˆåˆ°æœºå™¨äººç³»ç»Ÿä¸­æ˜¯å…³é”®ã€‚</li>
<li>BT-ACTIONæ–¹æ³•ç»“åˆäº†è¡Œä¸ºæ ‘ï¼ˆBTï¼‰çš„æ¨¡å—åŒ–ç»“æ„ä¸LLMï¼Œç”¨äºç”Ÿæˆè¿è´¯çš„æœºå™¨äººè¡ŒåŠ¨åºåˆ—ï¼Œä»¥éµå¾ªå¤æ‚çš„ç”¨æˆ·æŒ‡ä»¤ã€‚</li>
<li>BT-ACTIONé€šè¿‡ç”¨æˆ·ç ”ç©¶è¯„ä¼°ï¼Œè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºç›´æ¥LLMæç¤ºï¼Œç”¨æˆ·å¯¹å…¶è¡¨ç°å‡ºæ˜¾è‘—åå¥½ã€‚</li>
<li>BT-ACTIONçš„æ¨¡å—åŒ–è®¾è®¡æœ‰åŠ©äºæœºå™¨äººå‡å°‘é”™è¯¯ï¼Œå¹¶å¢åŠ ç”¨æˆ·ä¿¡ä»»åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02779">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-62fa30f70933e63c21bcdfc1c466c2cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0a5b847c019b2cef8d6a84fd0a8d724.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0a9cb251c7ba5eb67d1bdf77f6f8f23e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-909d4443b07b59e92f6f5a8da86c6bd2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdad0e2efb4fc7f6daccb95f919ee2d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dab9040d836769b6dd6634b59e916f8a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cfa8a518606de53d24ca0bbb0e3d1efd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e77a493b1dc9ab38707d9184ecfb4c30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42880178b7a54c5a1315ba42faf88778.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9314a1f7c85d82e1400ec9a94e4d5cab.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Enhancing-LLM-Robustness-to-Perturbed-Instructions-An-Empirical-Study"><a href="#Enhancing-LLM-Robustness-to-Perturbed-Instructions-An-Empirical-Study" class="headerlink" title="Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study"></a>Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study</h2><p><strong>Authors:Aryan Agrawal, Lisa Alazraki, Shahin Honarvar, Marek Rei</strong></p>
<p>Large Language Models (LLMs) are highly vulnerable to input perturbations, as even a small prompt change may result in a substantially different output. Existing methods to enhance LLM robustness are primarily focused on perturbed data samples, whereas improving resiliency to perturbations of task-level instructions has remained relatively underexplored. In this work, we focus on character- and word-level edits of task-specific instructions, which substantially degrade downstream performance. We experiment with a variety of techniques to enhance the robustness of LLMs, including self-denoising and representation alignment, testing different models (Llama 3 and Flan-T5), datasets (CoLA, QNLI, SST-2) and instructions (both task-oriented and role-oriented). We find that, on average, self-denoising â€“ whether performed by a frozen LLM or a fine-tuned model â€“ achieves substantially higher performance gains than alternative strategies, including more complex baselines such as ensembling and supervised methods. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹è¾“å…¥æ‰°åŠ¨å…·æœ‰å¾ˆé«˜çš„è„†å¼±æ€§ï¼Œå³ä½¿æ˜¯å¾ˆå°çš„æç¤ºå˜åŒ–ä¹Ÿå¯èƒ½å¯¼è‡´è¾“å‡ºå¤§ä¸ç›¸åŒã€‚ç°æœ‰çš„æé«˜LLMç¨³å¥æ€§çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ‰°åŠ¨æ•°æ®æ ·æœ¬ä¸Šï¼Œè€Œæé«˜ä»»åŠ¡çº§åˆ«æŒ‡ä»¤çš„æŠ—å¹²æ‰°èƒ½åŠ›åˆ™ç›¸å¯¹æœªè¢«å……åˆ†ç ”ç©¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å…³æ³¨äºä»»åŠ¡ç‰¹å®šæŒ‡ä»¤çš„å­—ç¬¦å’Œå•è¯çº§åˆ«çš„ç¼–è¾‘ï¼Œè¿™ä¼šæ˜¾è‘—é™ä½ä¸‹æ¸¸æ€§èƒ½ã€‚æˆ‘ä»¬å°è¯•äº†å„ç§æŠ€æœ¯æ¥æé«˜LLMçš„ç¨³å¥æ€§ï¼ŒåŒ…æ‹¬è‡ªæˆ‘å»å™ªå’Œè¡¨ç¤ºå¯¹é½ï¼Œæµ‹è¯•äº†ä¸åŒçš„æ¨¡å‹ï¼ˆLlama 3å’ŒFlan-T5ï¼‰ã€æ•°æ®é›†ï¼ˆCoLAã€QNLIã€SST-2ï¼‰å’ŒæŒ‡ä»¤ï¼ˆä»»åŠ¡å¯¼å‘å‹å’Œè§’è‰²å¯¼å‘å‹ï¼‰ã€‚æˆ‘ä»¬å‘ç°ï¼Œå¹³å‡è€Œè¨€ï¼Œæ— è®ºæ˜¯é€šè¿‡å†»ç»“çš„LLMè¿˜æ˜¯ç»è¿‡å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œè‡ªæˆ‘å»å™ªï¼Œç›¸æ¯”å…¶ä»–ç­–ç•¥éƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒ…æ‹¬é›†æˆæ–¹æ³•å’Œç›‘ç£å­¦ä¹ ç­‰æ›´å¤æ‚çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02733v1">PDF</a> Building Trust Workshop, ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>LLMsæ˜“å—è¾“å…¥æ‰°åŠ¨å½±å“ï¼Œå¾®å°æç¤ºå˜åŒ–å¯èƒ½å¯¼è‡´æ˜¾è‘—ä¸åŒçš„è¾“å‡ºã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ‰°åŠ¨æ•°æ®æ ·æœ¬çš„å¢å¼ºï¼Œè€Œä»»åŠ¡çº§æŒ‡ä»¤çš„æ‰°åŠ¨æŠ—æ€§æ”¹å–„ç›¸å¯¹è¾ƒå°‘æ¢ç´¢ã€‚æœ¬ç ”ç©¶å…³æ³¨ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤çš„å­—ç¬¦å’Œå•è¯çº§ç¼–è¾‘ï¼Œè¿™æ˜¾è‘—é™ä½äº†ä¸‹æ¸¸æ€§èƒ½ã€‚å®éªŒé‡‡ç”¨è‡ªæˆ‘å»å™ªå’Œè¡¨ç¤ºå¯¹é½ç­‰æŠ€æœ¯æé«˜LLMçš„é²æ£’æ€§ï¼Œæµ‹è¯•ä¸åŒæ¨¡å‹ï¼ˆLlama 3å’ŒFlan-T5ï¼‰ã€æ•°æ®é›†ï¼ˆCoLAï¼ŒQNLIï¼ŒSST-2ï¼‰å’ŒæŒ‡ä»¤ï¼ˆä»»åŠ¡å¯¼å‘å’Œè§’è‰²å¯¼å‘ï¼‰ã€‚ç ”ç©¶å‘ç°ï¼Œå¹³å‡è€Œè¨€ï¼Œæ— è®ºæ˜¯ç”±å†»ç»“çš„LLMè¿˜æ˜¯ç»è¿‡å¾®è°ƒæ¨¡å‹çš„è‡ªæˆ‘å»å™ªï¼Œå‡å®ç°äº†æ˜¾è‘—ä¼˜äºå…¶ä»–ç­–ç•¥çš„æ€§èƒ½æå‡ï¼ŒåŒ…æ‹¬é›†æˆå’Œç›‘ç£æ–¹æ³•ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså¯¹è¾“å…¥æ‰°åŠ¨é«˜åº¦æ•æ„Ÿï¼Œå¾®å°æç¤ºå˜åŒ–å¯å¯¼è‡´æ˜¾è‘—ä¸åŒçš„è¾“å‡ºã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ‰°åŠ¨æ•°æ®æ ·æœ¬ä»¥å¢å¼ºLLMçš„é²æ£’æ€§ã€‚</li>
<li>ä»»åŠ¡çº§æŒ‡ä»¤çš„æ‰°åŠ¨æŠ—æ€§æ”¹å–„å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>å­—ç¬¦å’Œå•è¯çº§ç¼–è¾‘çš„ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤æ˜¾è‘—é™ä½äº†ä¸‹æ¸¸æ€§èƒ½ã€‚</li>
<li>è‡ªæˆ‘å»å™ªæŠ€æœ¯æ˜¯æé«˜LLMé²æ£’æ€§çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>å†»ç»“çš„LLMå’Œç»è¿‡å¾®è°ƒæ¨¡å‹çš„è‡ªæˆ‘å»å™ªæ€§èƒ½ä¼˜äºå…¶ä»–ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c7d038e3211c59374bf98543407f112a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-135a8ac4155912dab85c668904fb1295.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8126cd0521bef105c70e1b14d20e1fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed0a7d0c911ff503560b7ea93f16364e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c63ef9024d4516f8bbf9b805794d958.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Affordable-AI-Assistants-with-Knowledge-Graph-of-Thoughts"><a href="#Affordable-AI-Assistants-with-Knowledge-Graph-of-Thoughts" class="headerlink" title="Affordable AI Assistants with Knowledge Graph of Thoughts"></a>Affordable AI Assistants with Knowledge Graph of Thoughts</h2><p><strong>Authors:Maciej Besta, Lorenzo Paleari, Jia Hao Andrea Jiang, Robert Gerstenberger, You Wu, Patrick Iff, Ales Kubicek, Piotr Nyczyk, Diana Khimey, JÃ³n Gunnar Hannesson, Grzegorz KwaÅ›niewski, Marcin Copik, Hubert Niewiadomski, Torsten Hoefler</strong></p>
<p>Large Language Models (LLMs) are revolutionizing the development of AI assistants capable of performing diverse tasks across domains. However, current state-of-the-art LLM-driven agents face significant challenges, including high operational costs and limited success rates on complex benchmarks like GAIA. To address these issues, we propose the Knowledge Graph of Thoughts (KGoT), an innovative AI assistant architecture that integrates LLM reasoning with dynamically constructed knowledge graphs (KGs). KGoT extracts and structures task-relevant knowledge into a dynamic KG representation, iteratively enhanced through external tools such as math solvers, web crawlers, and Python scripts. Such structured representation of task-relevant knowledge enables low-cost models to solve complex tasks effectively. For example, KGoT achieves a 29% improvement in task success rates on the GAIA benchmark compared to Hugging Face Agents with GPT-4o mini, while reducing costs by over 36x compared to GPT-4o. Improvements for recent reasoning models are similar, e.g., 36% and 37.5% for Qwen2.5-32B and Deepseek-R1-70B, respectively. KGoT offers a scalable, affordable, and high-performing solution for AI assistants. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨æ¨åŠ¨äººå·¥æ™ºèƒ½åŠ©æ‰‹åœ¨è·¨é¢†åŸŸæ‰§è¡Œå¤šæ ·åŒ–ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›å‘å±•ã€‚ç„¶è€Œï¼Œç›®å‰æœ€å…ˆè¿›çš„LLMé©±åŠ¨çš„æ™ºèƒ½ä»£ç†é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¿è¥æˆæœ¬é«˜å’Œåœ¨å¤æ‚åŸºå‡†æµ‹è¯•ï¼ˆå¦‚GAIAï¼‰ä¸Šçš„æˆåŠŸç‡æœ‰é™ã€‚ä¸ºäº†åº”å¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ€æƒ³çŸ¥è¯†å›¾è°±ï¼ˆKGoTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åˆ›æ–°çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹æ¶æ„ï¼Œå®ƒå°†LLMæ¨ç†ä¸åŠ¨æ€æ„å»ºçš„çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ç›¸ç»“åˆã€‚KGoTæå–å¹¶ç»“æ„åŒ–ä»»åŠ¡ç›¸å…³çŸ¥è¯†ï¼Œå½¢æˆåŠ¨æ€KGè¡¨ç¤ºï¼Œé€šè¿‡æ•°å­¦æ±‚è§£å™¨ã€ç½‘ç»œçˆ¬è™«å’ŒPythonè„šæœ¬ç­‰å¤–éƒ¨å·¥å…·è¿›è¡Œè¿­ä»£å¢å¼ºã€‚è¿™ç§ç»“æ„åŒ–è¡¨ç¤ºçš„ä»»åŠ¡ç›¸å…³çŸ¥è¯†ä½¿å¾—ä½æˆæœ¬æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è§£å†³å¤æ‚ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œåœ¨GAIAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒKGoTä¸GPT-4o miniç›¸æ¯”ï¼Œä»»åŠ¡æˆåŠŸç‡æé«˜äº†29%ï¼ŒåŒæ—¶æˆæœ¬é™ä½äº†è¶…è¿‡36å€ã€‚è¿‘æœŸçš„æ¨ç†æ¨¡å‹æ”¹è¿›ä¹Ÿç±»ä¼¼ï¼Œå¦‚Qwen2.5-32Bå’ŒDeepseek-R1-70Båˆ†åˆ«æé«˜äº†36%å’Œ37.5%ã€‚KGoTä¸ºäººå·¥æ™ºèƒ½åŠ©æ‰‹æä¾›äº†å¯æ‰©å±•ã€ç»æµå®æƒ ä¸”é«˜æ€§èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02670v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>çŸ¥è¯†å›¾è°±æ€ç»´ï¼ˆKGoTï¼‰æ˜¯ä¸€ç§åˆ›æ–°çš„AIåŠ©æ‰‹æ¶æ„ï¼Œå®ƒå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä¸åŠ¨æ€æ„å»ºçš„çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ç›¸ç»“åˆï¼Œè§£å†³äº†AIåŠ©ç†åœ¨å¤æ‚ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³å’Œæˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚KGoTé€šè¿‡æå–å’Œç»“æ„åŒ–ä»»åŠ¡ç›¸å…³çŸ¥è¯†ï¼Œå½¢æˆåŠ¨æ€çŸ¥è¯†å›¾è°±è¡¨ç¤ºï¼Œå¹¶é€šè¿‡æ•°å­¦æ±‚è§£å™¨ã€ç½‘ç»œçˆ¬è™«å’ŒPythonè„šæœ¬ç­‰å¤–éƒ¨å·¥å…·è¿›è¡Œè¿­ä»£å¢å¼ºã€‚KGoTåœ¨GAIAåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œç›¸è¾ƒäºGPT-4o miniæé«˜äº†ä»»åŠ¡æˆåŠŸç‡è¾¾29%ï¼Œæˆæœ¬é™ä½äº†è¶…è¿‡36å€ã€‚å¯¹äºå…¶ä»–æ¨ç†æ¨¡å‹ï¼Œå¦‚Qwen2.5-32Bå’ŒDeepseek-R1-70Bï¼Œä¹Ÿæœ‰ç±»ä¼¼çš„æ”¹è¿›ã€‚æ€»çš„æ¥è¯´ï¼ŒKGoTæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€ç»æµé«˜æ•ˆä¸”é«˜æ€§èƒ½çš„AIåŠ©æ‰‹è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†å›¾è°±æ€ç»´ï¼ˆKGoTï¼‰æ˜¯ä¸€ç§ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸åŠ¨æ€çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰çš„AIåŠ©æ‰‹æ¶æ„ã€‚</li>
<li>KGoTè§£å†³äº†AIåŠ©ç†åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é«˜æ“ä½œæˆæœ¬å’Œæœ‰é™çš„æˆåŠŸç‡ã€‚</li>
<li>KGoTé€šè¿‡æå–å’Œç»“æ„åŒ–ä»»åŠ¡ç›¸å…³çŸ¥è¯†ï¼Œå½¢æˆåŠ¨æ€çŸ¥è¯†å›¾è°±è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨å¤–éƒ¨å·¥å…·è¿›è¡Œè¿­ä»£å¢å¼ºã€‚</li>
<li>KGoTåœ¨GAIAåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œç›¸è¾ƒäºå…¶ä»–æ¨¡å‹æé«˜äº†ä»»åŠ¡æˆåŠŸç‡å¹¶å¤§å¹…é™ä½äº†æˆæœ¬ã€‚</li>
<li>KGoTå¯¹å…¶ä»–æ¨ç†æ¨¡å‹ä¹Ÿæœ‰æ”¹è¿›æ•ˆæœã€‚</li>
<li>KGoTæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€ç»æµé«˜æ•ˆä¸”é«˜æ€§èƒ½çš„AIåŠ©æ‰‹è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02670">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e234990e9d26c106d3abb388533e139.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64a6d3b370382fa74d901700ce822610.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-372470a62e8bc23c06daf8561c5ab814.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Multi-SWE-bench-A-Multilingual-Benchmark-for-Issue-Resolving"><a href="#Multi-SWE-bench-A-Multilingual-Benchmark-for-Issue-Resolving" class="headerlink" title="Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving"></a>Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving</h2><p><strong>Authors:Daoguang Zan, Zhirong Huang, Wei Liu, Hanwu Chen, Linhao Zhang, Shulin Xin, Lu Chen, Qi Liu, Xiaojian Zhong, Aoyan Li, Siyao Liu, Yongsheng Xiao, Liangqiang Chen, Yuyu Zhang, Jing Su, Tianyu Liu, Rui Long, Kai Shen, Liang Xiang</strong></p>
<p>The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To address this, we introduce a multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a total of 1,632 high-quality instances, which were carefully annotated from 2,456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation. Based on Multi-SWE-bench, we evaluate a series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with key empirical insights. In addition, we launch a Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks. As an initial contribution, we release a set of 4,723 well-structured instances spanning seven programming languages, laying a solid foundation for RL research in this domain. More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI. </p>
<blockquote>
<p>é—®é¢˜è§£å†³çš„ä»»åŠ¡æ˜¯å¯¹ä»£ç åº“è¿›è¡Œä¿®æ”¹ä»¥ç”Ÿæˆä¸€ä¸ªè§£å†³ç»™å®šé—®é¢˜çš„è¡¥ä¸ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ï¼Œå¦‚SWE-benchï¼Œå‡ ä¹åªä¸“æ³¨äºPythonï¼Œè¿™ä½¿å¾—å®ƒä»¬ä¸è¶³ä»¥è¯„ä¼°è·¨å¤šç§è½¯ä»¶ç”Ÿæ€ç³»ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤šè¯­è¨€é—®é¢˜è§£å†³åŸºå‡†æµ‹è¯•ï¼Œåä¸ºMulti-SWE-benchï¼Œæ¶µç›–Javaã€TypeScriptã€JavaScriptã€Goã€Rustã€Cå’ŒC++ã€‚å®ƒåŒ…å«æ€»å…±1632ä¸ªé«˜è´¨é‡å®ä¾‹ï¼Œè¿™äº›å®ä¾‹æ˜¯ä»2456ä¸ªå€™é€‰è€…ä¸­ç”±68ä½ä¸“ä¸šæ³¨é‡Šè€…ä»”ç»†æ³¨é‡Šçš„ï¼Œç¡®ä¿åŸºå‡†æµ‹è¯•èƒ½å¤Ÿæä¾›å‡†ç¡®å’Œå¯é çš„è¯„ä»·ã€‚åŸºäºMulti-SWE-benchï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸‰ç§å…·æœ‰ä»£è¡¨æ€§çš„æ–¹æ³•ï¼ˆæ— ä»£ç†ã€SWE-agentå’ŒOpenHandsï¼‰è¯„ä¼°äº†ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¹¶è¿›è¡Œäº†ç»¼åˆåˆ†æï¼Œæä¾›äº†å…³é”®çš„å®è¯è§è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Multi-SWE-RLå¼€æºç¤¾åŒºï¼Œæ—¨åœ¨æ„å»ºç”¨äºé—®é¢˜è§£å†³ä»»åŠ¡çš„å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒæ•°æ®é›†ã€‚ä½œä¸ºåˆæ­¥è´¡çŒ®ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€å¥—æ¶µç›–ä¸ƒç§ç¼–ç¨‹è¯­è¨€çš„4723ä¸ªç»“æ„è‰¯å¥½çš„å®ä¾‹ï¼Œä¸ºè¿™ä¸€é¢†åŸŸçš„RLç ”ç©¶å¥ å®šäº†åšå®åŸºç¡€ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å¼€æºäº†æ•´ä¸ªæ•°æ®ç”Ÿäº§ç®¡é“ä»¥åŠè¯¦ç»†çš„æ•™ç¨‹ï¼Œé¼“åŠ±å¼€æºç¤¾åŒºæŒç»­è´¡çŒ®å¹¶æ‰©å±•æ•°æ®é›†ã€‚æˆ‘ä»¬æœŸæœ›æˆ‘ä»¬çš„Multi-SWE-benchå’Œä¸æ–­å‘å±•çš„Multi-SWE-RLç¤¾åŒºèƒ½æ¨åŠ¨RLå‘æŒ¥å…¶å…¨éƒ¨æ½œåŠ›ï¼Œä½¿æˆ‘ä»¬è·ç¦»é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„é»æ˜æ›´è¿‘ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02605v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>ä¸ºè§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•å¦‚SWE-benchå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§è½¯ä»¶ç”Ÿæ€ç³»ç»Ÿä¸­çš„è¯„ä¼°ä¸è¶³çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šè¯­è¨€é—®é¢˜è§£å†³çš„åŸºå‡†æµ‹è¯•Multi-SWE-benchã€‚å®ƒæ¶µç›–äº†Javaã€TypeScriptã€JavaScriptã€Goã€Rustã€Cå’ŒC++ç­‰è¯­è¨€ï¼ŒåŒ…å«ä»å€™é€‰è€…ä¸­ç²¾å¿ƒæŒ‘é€‰çš„1,632ä¸ªé«˜è´¨é‡å®ä¾‹ã€‚æˆ‘ä»¬åˆ©ç”¨æ­¤åŸºå‡†æµ‹è¯•å¯¹ä¸€ç³»åˆ—å‰æ²¿æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶æå‡ºäº†åŒ…å«å®è¯æ´å¯Ÿçš„ç»¼åˆåˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å»ºç«‹äº†Multi-SWE-RLå¼€æºç¤¾åŒºï¼Œè‡´åŠ›äºæ„å»ºç”¨äºé—®é¢˜è§£å†³çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒæ•°æ®é›†ã€‚æˆ‘ä»¬å…¬å¼€äº†æ•°æ®ç”Ÿäº§ç®¡é“å’Œè¯¦ç»†æ•™ç¨‹ï¼Œé¼“åŠ±å¼€æºç¤¾åŒºæŒç»­è´¡çŒ®å¹¶æ‰©å±•æ•°æ®é›†ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ¨åŠ¨RLçš„å‘å±•ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½é€šç”¨ï¼ˆAGIï¼‰çš„åˆ°æ¥ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼•å…¥äº†å¤šè¯­è¨€é—®é¢˜è§£å†³åŸºå‡†æµ‹è¯•Multi-SWE-benchï¼Œæ¶µç›–å¤šç§ç¼–ç¨‹è¯­è¨€ã€‚</li>
<li>é€šè¿‡ç²¾å¿ƒæŒ‘é€‰å’Œæ ‡æ³¨ï¼Œç¡®ä¿åŸºå‡†æµ‹è¯•åŒ…å«é«˜è´¨é‡çš„å®ä¾‹ä»¥å‡†ç¡®å¯é åœ°è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨Multi-SWE-benchè¯„ä¼°äº†ä¸€ç³»åˆ—å‰æ²¿æ¨¡å‹ï¼Œå¹¶æå‡ºäº†åŒ…å«å®è¯æ´å¯Ÿçš„ç»¼åˆåˆ†æã€‚</li>
<li>å»ºç«‹äº†ä¸€ä¸ªå¼€æºç¤¾åŒºMulti-SWE-RLï¼Œä¸“æ³¨äºæ„å»ºç”¨äºé—®é¢˜è§£å†³çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ•°æ®é›†ã€‚</li>
<li>ä¸ºç¤¾åŒºæˆå‘˜å…¬å¼€äº†æ•°æ®ç”Ÿäº§ç®¡é“å’Œè¯¦ç»†æ•™ç¨‹ï¼Œä¾¿äºå…¶æŒç»­è´¡çŒ®å’Œæ‰©å±•æ•°æ®é›†ã€‚</li>
<li>Multi-SWE-benchå’ŒMulti-SWE-RLç¤¾åŒºçš„ç›®æ ‡æ˜¯æ¨è¿›å¼ºåŒ–å­¦ä¹ çš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02605">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ca7452374cee9bca7e1e218457c99dae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f70bdc2019a098c4bdcf4f1da70484ee.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Aligned-Better-Listen-Better-for-Audio-Visual-Large-Language-Models"><a href="#Aligned-Better-Listen-Better-for-Audio-Visual-Large-Language-Models" class="headerlink" title="Aligned Better, Listen Better for Audio-Visual Large Language Models"></a>Aligned Better, Listen Better for Audio-Visual Large Language Models</h2><p><strong>Authors:Yuxin Guo, Shuailei Ma, Shijie Ma, Xiaoyi Bao, Chen-Wei Xie, Kecheng Zheng, Tingyu Weng, Siyang Sun, Yun Zheng, Wei Zou</strong></p>
<p>Audio is essential for multimodal video understanding. On the one hand, video inherently contains audio, which supplies complementary information to vision. Besides, video large language models (Video-LLMs) can encounter many audio-centric settings. However, existing Video-LLMs and Audio-Visual Large Language Models (AV-LLMs) exhibit deficiencies in exploiting audio information, leading to weak understanding and hallucinations. To solve the issues, we delve into the model architecture and dataset. (1) From the architectural perspective, we propose a fine-grained AV-LLM, namely Dolphin. The concurrent alignment of audio and visual modalities in both temporal and spatial dimensions ensures a comprehensive and accurate understanding of videos. Specifically, we devise an audio-visual multi-scale adapter for multi-scale information aggregation, which achieves spatial alignment. For temporal alignment, we propose audio-visual interleaved merging. (2) From the dataset perspective, we curate an audio-visual caption and instruction-tuning dataset, called AVU. It comprises 5.2 million diverse, open-ended data tuples (video, audio, question, answer) and introduces a novel data partitioning strategy. Extensive experiments show our model not only achieves remarkable performance in audio-visual understanding, but also mitigates potential hallucinations. </p>
<blockquote>
<p>éŸ³é¢‘å¯¹äºå¤šæ¨¡æ€è§†é¢‘ç†è§£è‡³å…³é‡è¦ã€‚ä¸€æ–¹é¢ï¼Œè§†é¢‘æœ¬èº«å°±åŒ…å«éŸ³é¢‘ï¼Œä¸ºè§†è§‰æä¾›äº†è¡¥å……ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰å¯èƒ½ä¼šé‡åˆ°è®¸å¤šä»¥éŸ³é¢‘ä¸ºä¸­å¿ƒçš„åœºæ™¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„Video-LLMså’Œè§†å¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆAV-LLMsï¼‰åœ¨åˆ©ç”¨éŸ³é¢‘ä¿¡æ¯æ–¹é¢å­˜åœ¨ç¼ºé™·ï¼Œå¯¼è‡´ç†è§£ä¸è¶³å’Œå¹»è§‰ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹æ¶æ„å’Œæ•°æ®é›†è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚ï¼ˆ1ï¼‰ä»æ¶æ„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç²¾ç»†çš„AV-LLMï¼Œå³Dolphinã€‚éŸ³é¢‘å’Œè§†è§‰æ¨¡æ€åœ¨æ—¶é—´å’Œç©ºé—´ç»´åº¦ä¸Šçš„å¹¶è¡Œå¯¹é½ï¼Œç¡®ä¿äº†è§†é¢‘çš„å…¨é¢å’Œå‡†ç¡®ç†è§£ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè§†å¬å¤šå°ºåº¦é€‚é…å™¨è¿›è¡Œå¤šå°ºåº¦ä¿¡æ¯èšåˆï¼Œå®ç°ç©ºé—´å¯¹é½ã€‚å¯¹äºæ—¶é—´å¯¹é½ï¼Œæˆ‘ä»¬æå‡ºäº†è§†å¬äº¤é”™åˆå¹¶ã€‚ï¼ˆ2ï¼‰ä»æ•°æ®é›†çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªè§†å¬å­—å¹•å’ŒæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œç§°ä¸ºAVUã€‚å®ƒåŒ…å«äº†520ä¸‡ä¸ªå¤šæ ·åŒ–ã€å¼€æ”¾å¼çš„æ•°æ®å…ƒç»„ï¼ˆè§†é¢‘ã€éŸ³é¢‘ã€é—®é¢˜ã€ç­”æ¡ˆï¼‰ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„æ•°æ®åˆ†åŒºç­–ç•¥ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸ä»…åœ¨è§†å¬ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆç»©ï¼Œè€Œä¸”å‡è½»äº†æ½œåœ¨çš„å¹»è§‰é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02061v1">PDF</a> Accepted to ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡æœ¬ä»‹ç»äº†éŸ³é¢‘åœ¨å¤šæ¨¡æ€è§†é¢‘ç†è§£ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰å’Œè§†å¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆAV-LLMsï¼‰åœ¨åˆ©ç”¨éŸ³é¢‘ä¿¡æ¯æ–¹é¢çš„ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œè¯¥æ–‡ä»æ¨¡å‹æ¶æ„å’Œæ•°æ®é›†ä¸¤ä¸ªè§’åº¦å…¥æ‰‹ï¼Œæå‡ºäº†ä¸€ä¸ªç²¾ç»†çš„AV-LLMâ€”â€”æµ·è±šæ¨¡å‹ï¼Œä»¥åŠä¸€ä¸ªè§†å¬å­—å¹•å’ŒæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†AVUã€‚è¿™äº›åˆ›æ–°æ—¨åœ¨æé«˜è§†é¢‘ç†è§£çš„å‡†ç¡®æ€§å’Œå…¨é¢æ€§ï¼Œå¹¶å‡å°‘æ½œåœ¨çš„å¹»è§‰ç°è±¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘å¯¹äºå¤šæ¨¡æ€è§†é¢‘ç†è§£è‡³å…³é‡è¦ï¼Œæä¾›äº†ä¸è§†è§‰ç›¸è¡¥å……çš„ä¿¡æ¯ã€‚</li>
<li>ç°æœ‰Video-LLMså’ŒAV-LLMsåœ¨åˆ©ç”¨éŸ³é¢‘ä¿¡æ¯æ–¹é¢å­˜åœ¨ç¼ºé™·ï¼Œå¯¼è‡´ç†è§£ä¸è¶³å’Œå¹»è§‰ç°è±¡ã€‚</li>
<li>æ¨¡å‹æ¶æ„è§’åº¦ï¼šæå‡ºäº†ç²¾ç»†çš„AV-LLMâ€”â€”æµ·è±šæ¨¡å‹ï¼Œé€šè¿‡éŸ³é¢‘å’Œè§†è§‰æ¨¡æ€çš„æ—¶ç©ºç»´åº¦å¯¹é½ï¼Œå®ç°äº†å¯¹è§†é¢‘çš„å…¨é¢å‡†ç¡®ç†è§£ã€‚</li>
<li>æµ·è±šæ¨¡å‹é‡‡ç”¨è§†å¬å¤šå°ºåº¦é€‚é…å™¨è¿›è¡Œå¤šå°ºåº¦ä¿¡æ¯èšåˆï¼Œå®ç°ç©ºé—´å¯¹é½ï¼›é€šè¿‡è§†å¬äº¤é”™åˆå¹¶å®ç°æ—¶é—´å¯¹é½ã€‚</li>
<li>æ•°æ®é›†è§’åº¦ï¼šæ¨å‡ºäº†è§†å¬å­—å¹•å’ŒæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†AVUï¼ŒåŒ…å«520ä¸‡ä¸ªå¼€æ”¾å¼çš„å¤šå…ƒæ•°æ®å…ƒç»„ï¼ˆè§†é¢‘ã€éŸ³é¢‘ã€é—®é¢˜ã€ç­”æ¡ˆï¼‰ï¼Œå¹¶é‡‡ç”¨äº†æ–°å‹çš„æ•°æ®åˆ†åŒºç­–ç•¥ã€‚</li>
<li>æµ·è±šæ¨¡å‹åœ¨è§†å¬ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02061">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ccb6477bd9cfd90b33bfeb70406ba4ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a2d3991112d8af27e52da479014ca42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4ef1776a7b879c15cc27d09df4d959c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcfff9f0f1d67c8336c68e7e30d9bed9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c56ef0c5b3fcba23a7f18536b04fed0c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ScholarCopilot-Training-Large-Language-Models-for-Academic-Writing-with-Accurate-Citations"><a href="#ScholarCopilot-Training-Large-Language-Models-for-Academic-Writing-with-Accurate-Citations" class="headerlink" title="ScholarCopilot: Training Large Language Models for Academic Writing with   Accurate Citations"></a>ScholarCopilot: Training Large Language Models for Academic Writing with   Accurate Citations</h2><p><strong>Authors:Yubo Wang, Xueguang Ma, Ping Nie, Huaye Zeng, Zhiheng Lyu, Yuxuan Zhang, Benjamin Schneider, Yi Lu, Xiang Yue, Wenhu Chen</strong></p>
<p>Academic writing requires both coherent text generation and precise citation of relevant literature. Although recent Retrieval-Augmented Generation (RAG) systems have significantly improved factual accuracy in general-purpose text generation, their ability to support professional academic writing remains limited. In this work, we introduce ScholarCopilot, a unified framework designed to enhance existing large language models for generating professional academic articles with accurate and contextually relevant citations. ScholarCopilot dynamically determines when to retrieve scholarly references by generating a retrieval token [RET], which is then used to query a citation database. The retrieved references are fed into the model to augment the generation process. We jointly optimize both the generation and citation tasks within a single framework to improve efficiency. Our model is built upon Qwen-2.5-7B and trained on 500K papers from arXiv. It achieves a top-1 retrieval accuracy of 40.1% on our evaluation dataset, outperforming baselines such as E5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic writing samples, ScholarCopilot scores 16.2&#x2F;25 in generation quality â€“ measured across relevance, coherence, academic rigor, completeness, and innovation â€“ significantly surpassing all existing models, including much larger ones like the Retrieval-Augmented Qwen2.5-72B-Instruct. Human studies further demonstrate that ScholarCopilot, despite being a 7B model, significantly outperforms ChatGPT, achieving 100% preference in citation quality and over 70% in overall usefulness. </p>
<blockquote>
<p>å­¦æœ¯å†™ä½œæ—¢éœ€è¦è¿è´¯çš„æ–‡æœ¬ç”Ÿæˆï¼Œåˆéœ€è¦ç²¾ç¡®çš„æ–‡çŒ®å¼•ç”¨ã€‚å°½ç®¡æœ€è¿‘çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿåœ¨é€šç”¨æ–‡æœ¬ç”Ÿæˆä¸­æ˜¾è‘—æé«˜äº†äº‹å®å‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬åœ¨æ”¯æŒä¸“ä¸šå­¦æœ¯å†™ä½œæ–¹é¢çš„èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ScholarCopilotï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¢å¼ºç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå…·æœ‰å‡†ç¡®å’Œä¸Šä¸‹æ–‡ç›¸å…³å¼•ç”¨çš„ä¸“ä¸šå­¦æœ¯è®ºæ–‡ã€‚ScholarCopiloté€šè¿‡ç”Ÿæˆä¸€ä¸ªæ£€ç´¢ä»¤ç‰Œ[RET]æ¥åŠ¨æ€ç¡®å®šä½•æ—¶æ£€ç´¢å­¦æœ¯å‚è€ƒï¼Œç„¶åä½¿ç”¨è¯¥ä»¤ç‰ŒæŸ¥è¯¢å¼•ç”¨æ•°æ®åº“ã€‚æ£€ç´¢åˆ°çš„å¼•ç”¨è¢«è¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼Œä»¥å¢å¼ºç”Ÿæˆè¿‡ç¨‹ã€‚æˆ‘ä»¬åœ¨å•ä¸ªæ¡†æ¶ä¸­è”åˆä¼˜åŒ–ç”Ÿæˆå’Œå¼•ç”¨ä»»åŠ¡ï¼Œä»¥æé«˜æ•ˆç‡ã€‚æˆ‘ä»¬çš„æ¨¡å‹å»ºç«‹åœ¨Qwen-2.5-7Bä¹‹ä¸Šï¼Œå¹¶åœ¨arXivçš„50ä¸‡ç¯‡è®ºæ–‡ä¸Šè¿›è¡Œè®­ç»ƒã€‚åœ¨æˆ‘ä»¬çš„è¯„ä¼°æ•°æ®é›†ä¸Šï¼Œå®ƒè¾¾åˆ°äº†40.1%çš„top-1æ£€ç´¢å‡†ç¡®ç‡ï¼Œä¼˜äºE5-Mistral-7B-Instructï¼ˆ15.0%ï¼‰å’ŒBM25ï¼ˆ9.8%ï¼‰ç­‰åŸºçº¿æ¨¡å‹ã€‚åœ¨1000ä¸ªå­¦æœ¯å†™ä½œæ ·æœ¬çš„æ•°æ®é›†ä¸Šï¼ŒScholarCopilotåœ¨ç›¸å…³æ€§ã€è¿è´¯æ€§ã€å­¦æœ¯ä¸¥è°¨æ€§ã€å®Œæ•´æ€§å’Œåˆ›æ–°æ€§æ–¹é¢çš„ç”Ÿæˆè´¨é‡å¾—åˆ†ä¸º16.2&#x2F;25ï¼Œæ˜¾è‘—è¶…è¶Šäº†åŒ…æ‹¬æ›´å¤§æ¨¡å‹ï¼ˆå¦‚æ£€ç´¢å¢å¼ºQwen2.5-72B-Instructï¼‰åœ¨å†…çš„æ‰€æœ‰ç°æœ‰æ¨¡å‹ã€‚äººç±»ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œå°½ç®¡ScholarCopilotæ˜¯ä¸€ä¸ª7Bæ¨¡å‹ï¼Œä½†åœ¨å¼•ç”¨è´¨é‡å’Œæ€»ä½“æœ‰ç”¨æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºChatGPTï¼Œå¼•ç”¨è´¨é‡è¾¾åˆ°100%çš„åå¥½ï¼Œæ€»ä½“å®ç”¨æ€§è¶…è¿‡70%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00824v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ScholarCopilotï¼Œä¸€ä¸ªä¸“ä¸ºå­¦æœ¯å†™ä½œè®¾è®¡çš„å¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºæ¡†æ¶ã€‚å®ƒé€šè¿‡ç”Ÿæˆæ£€ç´¢ä»¤ç‰Œ[RET]æ¥åŠ¨æ€æ£€ç´¢å­¦æœ¯å‚è€ƒæ–‡çŒ®ï¼Œå¹¶åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä½¿ç”¨è¿™äº›å¼•ç”¨ã€‚ScholarCopilotåœ¨å•ä¸€æ¡†æ¶ä¸­è”åˆä¼˜åŒ–ç”Ÿæˆå’Œå¼•ç”¨ä»»åŠ¡ï¼Œä»¥æé«˜æ•ˆç‡ã€‚å®ƒåœ¨arXivçš„50ä¸‡ç¯‡è®ºæ–‡ä¸Šæ„å»ºå¹¶è®­ç»ƒï¼Œåœ¨è¯„ä¼°æ•°æ®é›†ä¸Šå®ç°äº†é«˜è¾¾40.1%çš„top-1æ£€ç´¢å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–åŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œåœ¨å­¦æœ¯å†™ä½œæ ·æœ¬æ•°æ®é›†ä¸Šï¼ŒScholarCopilotåœ¨ç”Ÿæˆè´¨é‡æ–¹é¢å¾—åˆ†è¾ƒé«˜ï¼Œå¹¶åœ¨ç›¸å…³æ€§ã€è¿è´¯æ€§ã€å­¦æœ¯ä¸¥è°¨æ€§ã€å®Œæ•´æ€§å’Œåˆ›æ–°æ€§ç­‰æ–¹é¢æ˜¾è‘—è¶…è¶Šäº†å…¶ä»–ç°æœ‰æ¨¡å‹ï¼ŒåŒ…æ‹¬æ›´å¤§çš„æ¨¡å‹ã€‚äººç±»ç ”ç©¶è¿›ä¸€æ­¥è¡¨æ˜ï¼Œå°½ç®¡åªæœ‰7Bå‚æ•°ï¼Œä½†ScholarCopilotåœ¨å¼•ç”¨è´¨é‡å’Œæ€»ä½“æœ‰ç”¨æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºChatGPTã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ScholarCopilotæ˜¯ä¸€ä¸ªé’ˆå¯¹å­¦æœ¯å†™ä½œçš„å¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºæ¡†æ¶ã€‚</li>
<li>é€šè¿‡ç”Ÿæˆæ£€ç´¢ä»¤ç‰Œ[RET]æ¥åŠ¨æ€æ£€ç´¢å­¦æœ¯å‚è€ƒæ–‡çŒ®ã€‚</li>
<li>æ¡†æ¶è”åˆä¼˜åŒ–ç”Ÿæˆå’Œå¼•ç”¨ä»»åŠ¡ï¼Œæé«˜æ•ˆç‡ã€‚</li>
<li>åœ¨arXivçš„50ä¸‡ç¯‡è®ºæ–‡ä¸Šæ„å»ºå¹¶è®­ç»ƒï¼Œå®ç°40.1%çš„top-1æ£€ç´¢å‡†ç¡®ç‡ã€‚</li>
<li>åœ¨å­¦æœ¯å†™ä½œæ ·æœ¬æ•°æ®é›†ä¸Šï¼ŒScholarCopilotåœ¨ç”Ÿæˆè´¨é‡ä¸Šä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>åœ¨å¼•ç”¨è´¨é‡å’Œæ€»ä½“æœ‰ç”¨æ€§æ–¹é¢ï¼ŒScholarCopilotæ˜¾è‘—ä¼˜äºChatGPTã€‚</li>
<li>äººæœºç ”ç©¶éªŒè¯äº†ScholarCopilotåœ¨å­¦æœ¯å†™ä½œä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00824">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-736f8d1a6ad3a1aa3127fba377d3daa5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1981820bc481e4a83ad855b5dc4864e6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-52b0b26efacdefd7111574d8a50aee69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edc72dfc5ed9a83073d31db2e42de7aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dec5eff62cabd396ccc10c2160241fca.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="OmniScience-A-Domain-Specialized-LLM-for-Scientific-Reasoning-and-Discovery"><a href="#OmniScience-A-Domain-Specialized-LLM-for-Scientific-Reasoning-and-Discovery" class="headerlink" title="OmniScience: A Domain-Specialized LLM for Scientific Reasoning and   Discovery"></a>OmniScience: A Domain-Specialized LLM for Scientific Reasoning and   Discovery</h2><p><strong>Authors:Vignesh Prabhakar, Md Amirul Islam, Adam Atanas, Yao-Ting Wang, Joah Han, Aastha Jhunjhunwala, Rucha Apte, Robert Clark, Kang Xu, Zihan Wang, Kai Liu</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable potential in advancing scientific knowledge and addressing complex challenges. In this work, we introduce OmniScience, a specialized large reasoning model for general science, developed through three key components: (1) domain adaptive pretraining on a carefully curated corpus of scientific literature, (2) instruction tuning on a specialized dataset to guide the model in following domain-specific tasks, and (3) reasoning-based knowledge distillation through fine-tuning to significantly enhance its ability to generate contextually relevant and logically sound responses. We demonstrate the versatility of OmniScience by developing a battery agent that efficiently ranks molecules as potential electrolyte solvents or additives. Comprehensive evaluations reveal that OmniScience is competitive with state-of-the-art large reasoning models on the GPQA Diamond and domain-specific battery benchmarks, while outperforming all public reasoning and non-reasoning models with similar parameter counts. We further demonstrate via ablation experiments that domain adaptive pretraining and reasoning-based knowledge distillation are critical to attain our performance levels, across benchmarks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨è¿›ç§‘å­¦çŸ¥è¯†å’Œåº”å¯¹å¤æ‚æŒ‘æˆ˜æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—æ½œåŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†OmniScienceï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é€šç”¨ç§‘å­¦çš„ç‰¹æ®Šå¤§å‹æ¨ç†æ¨¡å‹ï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶å¼€å‘ï¼šï¼ˆ1ï¼‰åœ¨ç²¾å¿ƒæŒ‘é€‰çš„ç§‘å­¦æ–‡çŒ®è¯­æ–™åº“ä¸Šè¿›è¡Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼Œï¼ˆ2ï¼‰åœ¨ä¸“é—¨çš„æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹æ‰§è¡Œç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ï¼Œï¼ˆ3ï¼‰é€šè¿‡å¾®è°ƒè¿›è¡ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦ï¼Œä»¥æ˜¾è‘—æé«˜å…¶ç”Ÿæˆè¯­å¢ƒç›¸å…³å’Œé€»è¾‘ä¸¥è°¨å“åº”çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡å¼€å‘ç”µæ± ä»£ç†æ¥å±•ç¤ºOmniScienceçš„é€šç”¨æ€§ï¼Œè¯¥ä»£ç†èƒ½å¤Ÿé«˜æ•ˆåœ°æ’åˆ—åˆ†å­ä½œä¸ºæ½œåœ¨çš„ç”µè§£è´¨æº¶å‰‚æˆ–æ·»åŠ å‰‚ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒOmniScienceåœ¨GPQA Diamondå’Œç‰¹å®šé¢†åŸŸçš„ç”µæ± åŸºå‡†æµ‹è¯•ä¸Šï¼Œä¸æœ€æ–°çš„å¤§å‹æ¨ç†æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶åœ¨å‚æ•°æ•°é‡ç›¸ä¼¼çš„æ‰€æœ‰å…¬å…±æ¨ç†å’Œéæ¨ç†æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ã€‚æˆ‘ä»¬è¿˜é€šè¿‡æ¶ˆé™¤å®éªŒè¿›ä¸€æ­¥è¯æ˜ï¼Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒå’ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦å¯¹äºè¾¾åˆ°æˆ‘ä»¬çš„æ€§èƒ½æ°´å¹³è‡³å…³é‡è¦ï¼Œè·¨è¶Šå„ä¸ªåŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17604v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨åŠ¨ç§‘å­¦çŸ¥è¯†å’Œåº”å¯¹å¤æ‚æŒ‘æˆ˜æ–¹é¢å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€æ¬¾é’ˆå¯¹é€šç”¨ç§‘å­¦çš„ä¸“é—¨å¤§å‹æ¨ç†æ¨¡å‹â€”â€”OmniScienceï¼Œå…¶å¼€å‘åŒ…æ‹¬ä¸‰ä¸ªå…³é”®éƒ¨åˆ†ï¼šä¸€æ˜¯åœ¨ç§‘å­¦æ–‡çŒ®çš„ç²¾å¿ƒç­›é€‰è¯­æ–™åº“ä¸Šè¿›è¡Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼›äºŒæ˜¯åœ¨ä¸“é—¨æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹æ‰§è¡Œç‰¹å®šé¢†åŸŸä»»åŠ¡ï¼›ä¸‰æ˜¯é€šè¿‡å¾®è°ƒè¿›è¡ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦ï¼Œä»¥æ˜¾è‘—æé«˜å…¶ç”Ÿæˆè¯­å¢ƒç›¸å…³å’Œé€»è¾‘ä¸¥è°¨å›åº”çš„èƒ½åŠ›ã€‚OmniScienceçš„é€šç”¨æ€§é€šè¿‡å¼€å‘ç”µæ± ä»£ç†å¾—åˆ°äº†éªŒè¯ï¼Œè¯¥ä»£ç†èƒ½å¤Ÿé«˜æ•ˆåœ°å¯¹åˆ†å­è¿›è¡Œç”µè§£è´¨æº¶å‰‚æˆ–æ·»åŠ å‰‚çš„æ’åã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒOmniScienceåœ¨GPQA Diamondå’Œç‰¹å®šé¢†åŸŸç”µæ± åŸºå‡†æµ‹è¯•ä¸Šï¼Œä¸æœ€æ–°å¤§å‹æ¨ç†æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶åœ¨å‚æ•°æ•°é‡ç›¸ä¼¼çš„å…¬å…±æ¨ç†å’Œéæ¨ç†æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ã€‚é€šè¿‡æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯æ˜ï¼Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒå’ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦å¯¹äºè¾¾åˆ°æˆ‘ä»¬çš„æ€§èƒ½æ°´å¹³è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OmniScienceæ˜¯ä¸€ä¸ªé’ˆå¯¹é€šç”¨ç§‘å­¦çš„ä¸“é—¨å¤§å‹æ¨ç†æ¨¡å‹ã€‚</li>
<li>OmniScienceçš„å¼€å‘åŒ…æ‹¬é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒã€æŒ‡ä»¤è°ƒæ•´ä»¥åŠåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦ä¸‰ä¸ªå…³é”®éƒ¨åˆ†ã€‚</li>
<li>OmniScienceèƒ½å¤Ÿé«˜æ•ˆåœ°å¯¹åˆ†å­è¿›è¡Œç”µè§£è´¨æº¶å‰‚æˆ–æ·»åŠ å‰‚çš„æ’åï¼Œå±•ç°äº†å…¶é€šç”¨æ€§ã€‚</li>
<li>ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼ŒOmniScienceåœ¨GPQA Diamondå’Œç‰¹å®šé¢†åŸŸç”µæ± åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä¸å…¶ä»–å…¬å…±æ¨ç†å’Œéæ¨ç†æ¨¡å‹ç›¸æ¯”ï¼ŒOmniScienceåœ¨ç±»ä¼¼å‚æ•°æ•°é‡ä¸‹è¡¨ç°æœ€ä½³ã€‚</li>
<li>æ¶ˆèå®éªŒè¯æ˜é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒå¯¹äºOmniScienceçš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8f04ec23b178acf57080ea79185a9fbb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5cddc57a4b1e46185ec103df103a43ee.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Fine-Tuning-Transformer-Based-Vision-Language-Models-for-Robust-Object-Detection-in-Unstructured-Environments"><a href="#Fine-Tuning-Transformer-Based-Vision-Language-Models-for-Robust-Object-Detection-in-Unstructured-Environments" class="headerlink" title="Fine-Tuning Transformer-Based Vision-Language Models for Robust Object   Detection in Unstructured Environments"></a>Fine-Tuning Transformer-Based Vision-Language Models for Robust Object   Detection in Unstructured Environments</h2><p><strong>Authors:Aysegul Ucar, Soumyadeep Ro, Sanapala Satwika, Pamarthi Yasoda Gayathri, Mohmmad Ghaith Balsha</strong></p>
<p>Vision-Language Models (VLMs) have emerged as powerful tools in artificial intelli-gence, capable of integrating textual and visual data for a unified understanding of complex scenes. While models such as Florence2, built on transformer architectures, have shown promise across general tasks, their performance in object detection within unstructured or cluttered environments remains underexplored. In this study, we fi-ne-tuned the Florence2 model for object detection tasks in non-constructed, complex environments. A comprehensive experimental framework was established involving multiple hardware configurations (NVIDIA T4, L4, and A100 GPUs), optimizers (AdamW, SGD), and varied hyperparameters including learning rates and LoRA (Low-Rank Adaptation) setups. Model training and evaluation were conducted on challenging datasets representative of real-world, disordered settings. The optimized Florence2 models exhibited significant improvements in object detection accuracy, with Mean Average Precision (mAP) metrics approaching or matching those of estab-lished models such as YOLOv8, YOLOv9, and YOLOv10. The integration of LoRA and careful fine-tuning of transformer layers contributed notably to these gains. Our find-ings highlight the adaptability of transformer-based VLMs like Florence2 for do-main-specific tasks, particularly in visually complex environments. The study under-scores the potential of fine-tuned VLMs to rival traditional convolution-based detec-tors, offering a flexible and scalable approach for advanced vision applications in re-al-world, unstructured settings. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä½œä¸ºäººå·¥æ™ºèƒ½çš„å¼ºå¤§å·¥å…·ï¼Œèƒ½å¤Ÿæ•´åˆæ–‡æœ¬å’Œè§†è§‰æ•°æ®ï¼Œå¯¹å¤æ‚åœºæ™¯è¿›è¡Œç»Ÿä¸€ç†è§£ã€‚è™½ç„¶åŸºäºè½¬æ¢å™¨æ¶æ„çš„Florence2ç­‰æ¨¡å‹åœ¨ä¸€èˆ¬ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„å‰æ™¯ï¼Œä½†åœ¨éç»“æ„åŒ–æˆ–æ‚ä¹±ç¯å¢ƒä¸­è¿›è¡Œç›®æ ‡æ£€æµ‹çš„æ€§èƒ½ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿå……åˆ†ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹éæ„é€ çš„å¤æ‚ç¯å¢ƒä¸­çš„ç›®æ ‡æ£€æµ‹ä»»åŠ¡å¯¹Florence2æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚å»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„å®éªŒæ¡†æ¶ï¼Œæ¶‰åŠå¤šç§ç¡¬ä»¶é…ç½®ï¼ˆNVIDIA T4ã€L4å’ŒA100 GPUï¼‰ã€ä¼˜åŒ–å™¨ï¼ˆAdamWã€SGDï¼‰ä»¥åŠåŒ…æ‹¬å­¦ä¹ ç‡å’ŒLoRAï¼ˆä½ç§©é€‚åº”ï¼‰åœ¨å†…çš„å„ç§è¶…å‚æ•°è®¾ç½®ã€‚åœ¨å…·æœ‰ä»£è¡¨æ€§çš„ç°å®ä¸–ç•Œæ··ä¹±è®¾ç½®çš„æŒ‘æˆ˜æ€§æ•°æ®é›†ä¸Šè¿›è¡Œäº†æ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚ä¼˜åŒ–åçš„Florence2æ¨¡å‹åœ¨ç›®æ ‡æ£€æµ‹ç²¾åº¦ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œå…¶å¹³å‡ç²¾åº¦å‡å€¼ï¼ˆmAPï¼‰æŒ‡æ ‡æ¥è¿‘æˆ–è¾¾åˆ°äº†YOLOv8ã€YOLOv9å’ŒYOLOv10ç­‰å·²å»ºç«‹æ¨¡å‹çš„æ€§èƒ½ã€‚LoRAçš„é›†æˆä»¥åŠå¯¹è½¬æ¢å™¨å±‚çš„ç²¾ç»†å¾®è°ƒå¯¹è¿™äº›æˆæœè´¡çŒ®æ˜¾è‘—ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†åŸºäºè½¬æ¢å™¨çš„VLMsï¼ˆå¦‚Florence2ï¼‰å¯¹ç‰¹å®šé¢†åŸŸçš„é€‚åº”æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰å¤æ‚çš„ç¯å¢ƒä¸­ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†ç»è¿‡ç²¾ç»†è°ƒæ•´çš„VLMsä¸ä¼ ç»ŸåŸºäºå·ç§¯çš„æ£€æµ‹å™¨ç«äº‰æ½œåŠ›ï¼Œä¸ºç°å®ä¸–ç•Œéç»“æ„åŒ–ç¯å¢ƒä¸­çš„é«˜çº§è§†è§‰åº”ç”¨æä¾›äº†çµæ´»å’Œå¯æ‰©å±•çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04918v2">PDF</a> 22 pages, 13 Figures, 6 Tables</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶ä¼˜åŒ–äº†åŸºäºè½¬æ¢å™¨æ¶æ„çš„Florence2æ¨¡å‹ï¼Œä½¿å…¶é€‚ç”¨äºéç»“æ„åŒ–å’Œå¤æ‚ç¯å¢ƒä¸­çš„å¯¹è±¡æ£€æµ‹ä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡ç²¾ç»†è®­ç»ƒçš„Florence2æ¨¡å‹åœ¨ä»£è¡¨æ€§çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¯¹è±¡æ£€æµ‹ç²¾åº¦æ˜¾è‘—æé«˜ï¼Œå¹³å‡ç²¾åº¦å‡å€¼ï¼ˆmAPï¼‰æŒ‡æ ‡æ¥è¿‘æˆ–åŒ¹é…YOLOv8ã€YOLOv9å’ŒYOLOv10ç­‰ç°æœ‰æ¨¡å‹ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†è½¬æ¢å™¨ä¸ºåŸºç¡€çš„VLMæ¨¡å‹ï¼ˆå¦‚Florence2ï¼‰å¯¹äºç‰¹å®šé¢†åŸŸçš„é€‚åº”æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰å¤æ‚çš„ç°å®ä¸–ç•Œä¸­å±•ç°å‡ºä¸ä¼ ç»ŸåŸºäºå·ç§¯çš„æ£€æµ‹å™¨ç›¸æŠ—è¡¡çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç²¾ç»†è°ƒæ•´å’ŒLoRAé›†æˆæŠ€æœ¯æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚è¿™äº›æˆæœä¸ºå…ˆè¿›è§†è§‰åº”ç”¨æä¾›äº†çµæ´»ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04918">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-98db4665358a36f424d9bc07ef7a2277.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2cad3459aa076ed1ee0673f0a1018e24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1905fcaa64f23f604c5c098173f59be.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ViCaS-A-Dataset-for-Combining-Holistic-and-Pixel-level-Video-Understanding-using-Captions-with-Grounded-Segmentation"><a href="#ViCaS-A-Dataset-for-Combining-Holistic-and-Pixel-level-Video-Understanding-using-Captions-with-Grounded-Segmentation" class="headerlink" title="ViCaS: A Dataset for Combining Holistic and Pixel-level Video   Understanding using Captions with Grounded Segmentation"></a>ViCaS: A Dataset for Combining Holistic and Pixel-level Video   Understanding using Captions with Grounded Segmentation</h2><p><strong>Authors:Ali Athar, Xueqing Deng, Liang-Chieh Chen</strong></p>
<p>Recent advances in multimodal large language models (MLLMs) have expanded research in video understanding, primarily focusing on high-level tasks such as video captioning and question-answering. Meanwhile, a smaller body of work addresses dense, pixel-precise segmentation tasks, which typically involve category-guided or referral-based object segmentation. Although both directions are essential for developing models with human-level video comprehension, they have largely evolved separately, with distinct benchmarks and architectures. This paper aims to unify these efforts by introducing ViCaS, a new dataset containing thousands of challenging videos, each annotated with detailed, human-written captions and temporally consistent, pixel-accurate masks for multiple objects with phrase grounding. Our benchmark evaluates models on both holistic&#x2F;high-level understanding and language-guided, pixel-precise segmentation. We also present carefully validated evaluation measures and propose an effective model architecture that can tackle our benchmark. Project page: <a target="_blank" rel="noopener" href="https://ali2500.github.io/vicas-project/">https://ali2500.github.io/vicas-project/</a> </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥æ¨åŠ¨äº†è§†é¢‘ç†è§£é¢†åŸŸçš„ç ”ç©¶ï¼Œä¸»è¦å…³æ³¨é«˜çº§ä»»åŠ¡ï¼Œå¦‚è§†é¢‘æè¿°å’Œé—®ç­”ã€‚åŒæ—¶ï¼Œè¾ƒå°‘çš„ç ”ç©¶å·¥ä½œæ¶‰åŠå¯†é›†ã€åƒç´ ç²¾ç¡®çš„åˆ†å‰²ä»»åŠ¡ï¼Œé€šå¸¸æ¶‰åŠç±»åˆ«å¼•å¯¼æˆ–åŸºäºå¼•ç”¨çš„å¯¹è±¡åˆ†å‰²ã€‚è™½ç„¶è¿™ä¸¤ä¸ªæ–¹å‘å¯¹äºå¼€å‘å…·æœ‰äººç±»æ°´å¹³è§†é¢‘ç†è§£èƒ½åŠ›çš„æ¨¡å‹éƒ½è‡³å…³é‡è¦ï¼Œä½†å®ƒä»¬å¤§å¤šç‹¬ç«‹å‘å±•ï¼Œå…·æœ‰ä¸åŒçš„åŸºå‡†å’Œæ¶æ„ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥ViCaSæ•°æ®é›†æ¥ç»Ÿä¸€è¿™äº›åŠªåŠ›ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ•°åƒä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è§†é¢‘ï¼Œæ¯ä¸ªè§†é¢‘éƒ½ç»è¿‡è¯¦ç»†çš„äººç±»æ‰‹å†™æè¿°å’Œä¸æ—¶é—´æ®µä¸€è‡´çš„åƒç´ ç²¾ç¡®çš„å¤šå¯¹è±¡æ©è†œæ ‡æ³¨ï¼Œç”¨äºçŸ­è¯­å®šä½ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•è¯„ä¼°æ¨¡å‹åœ¨æ•´ä½“&#x2F;é«˜çº§ç†è§£å’Œè¯­è¨€å¼•å¯¼ã€åƒç´ ç²¾ç¡®åˆ†å‰²æ–¹é¢çš„è¡¨ç°ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ç»è¿‡ä»”ç»†éªŒè¯çš„è¯„ä¼°æªæ–½ï¼Œå¹¶æå‡ºäº†èƒ½æœ‰æ•ˆåº”å¯¹åŸºå‡†æµ‹è¯•çš„æ¨¡å‹æ¶æ„ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://ali2500.github.io/vicas-project/">https://ali2500.github.io/vicas-project/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09754v3">PDF</a> Accepted to CVPR 2025. Project page:   <a target="_blank" rel="noopener" href="https://ali2500.github.io/vicas-project/">https://ali2500.github.io/vicas-project/</a></p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æœ€æ–°è¿›å±•ä¿ƒè¿›äº†è§†é¢‘ç†è§£ç ”ç©¶çš„æ‰©å±•ï¼Œä¸»è¦é›†ä¸­åœ¨è§†é¢‘å­—å¹•å’Œé—®ç­”ç­‰é«˜çº§ä»»åŠ¡ä¸Šã€‚åŒæ—¶ï¼Œä¸€å°éƒ¨åˆ†å·¥ä½œå…³æ³¨å¯†é›†çš„åƒç´ ç²¾ç¡®åˆ†å‰²ä»»åŠ¡ï¼Œé€šå¸¸æ¶‰åŠç±»åˆ«å¼•å¯¼æˆ–åŸºäºå¼•ç”¨çš„å¯¹è±¡åˆ†å‰²ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥ViCaSæ•°æ®é›†ç»Ÿä¸€è¿™äº›åŠªåŠ›ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ•°åƒä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è§†é¢‘ï¼Œæ¯ä¸ªè§†é¢‘éƒ½å¸¦æœ‰è¯¦ç»†çš„äººç±»ä¹¦å†™å­—å¹•å’Œä¸å¤šå¯¹è±¡æœ¯è¯­å®šä½ç›¸åŒ¹é…çš„åƒç´ ç²¾ç¡®æ©æ¨¡ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨æ•´ä½“é«˜çº§ç†è§£å’Œè¯­è¨€æŒ‡å¯¼ä¸‹çš„åƒç´ ç²¾ç¡®åˆ†å‰²èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—æœ€æ–°è¿›å±•ï¼Œé›†ä¸­åœ¨é«˜çº§ä»»åŠ¡å¦‚è§†é¢‘å­—å¹•å’Œé—®ç­”ã€‚</li>
<li>å­˜åœ¨ä¸€éƒ¨åˆ†å·¥ä½œå…³æ³¨å¯†é›†çš„åƒç´ ç²¾ç¡®åˆ†å‰²ä»»åŠ¡ï¼Œæ¶‰åŠç±»åˆ«å¼•å¯¼æˆ–åŸºäºå¼•ç”¨çš„å¯¹è±¡åˆ†å‰²ã€‚</li>
<li>ç°æœ‰ç ”ç©¶åœ¨é«˜çº§ç†è§£å’Œåƒç´ ç²¾ç¡®åˆ†å‰²ä»»åŠ¡ä¸Šå¤§å¤šç‹¬ç«‹å‘å±•ï¼Œå…·æœ‰ä¸åŒçš„åŸºå‡†æµ‹è¯•å’Œæ¶æ„ã€‚</li>
<li>æœ¬æ–‡å¼•å…¥ViCaSæ•°æ®é›†ï¼ŒåŒ…å«è¯¦ç»†å­—å¹•å’Œå¤šä¸ªå¯¹è±¡çš„åƒç´ ç²¾ç¡®æ©æ¨¡æ³¨é‡Šçš„æŒ‘æˆ˜æ€§è§†é¢‘ã€‚</li>
<li>åŸºå‡†æµ‹è¯•æ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨æ•´ä½“é«˜çº§ç†è§£å’Œè¯­è¨€æŒ‡å¯¼ä¸‹çš„åƒç´ ç²¾ç¡®åˆ†å‰²èƒ½åŠ›ã€‚</li>
<li>æä¾›äº†ç»è¿‡ä»”ç»†éªŒè¯çš„è¯„ä»·æªæ–½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09754">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d9c88c0f2a67ad71ca8455a2865c8961.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bf707d228d94be4e631531aec3940c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-420c2e36d293dc1ebfb7356b47d9ae81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5d5605cb17a078fe96ce72eb9ec44f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-724dc023c14b3930fb47b03d7f39e9db.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Reducing-Reasoning-Costs-The-Path-of-Optimization-for-Chain-of-Thought-via-Sparse-Attention-Mechanism"><a href="#Reducing-Reasoning-Costs-The-Path-of-Optimization-for-Chain-of-Thought-via-Sparse-Attention-Mechanism" class="headerlink" title="Reducing Reasoning Costs: The Path of Optimization for Chain of Thought   via Sparse Attention Mechanism"></a>Reducing Reasoning Costs: The Path of Optimization for Chain of Thought   via Sparse Attention Mechanism</h2><p><strong>Authors:Libo Wang</strong></p>
<p>In order to address the chain of thought in the large language model inference cost surge, this research proposes to use a sparse attention mechanism that only focuses on a few relevant tokens. The researcher constructed a new attention mechanism and used GiantRabbit trained with custom GPTs as an experimental tool. The experiment tested and compared the reasoning time, correctness score and chain of thought length of this model and o1 Preview in solving the linear algebra test questions of MIT OpenCourseWare. The results show that GiantRabbitâ€™s reasoning time and chain of thought length are significantly lower than o1 Preview. It verifies the feasibility of sparse attention mechanism for optimizing chain of thought reasoning. Detailed architectural details and experimental process have been uploaded to Github, the link is:<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git">https://github.com/brucewang123456789/GeniusTrail.git</a>. </p>
<blockquote>
<p>ä¸ºäº†åº”å¯¹å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†æˆæœ¬é£™å‡ä¸­çš„æ€ç»´é“¾é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åªå…³æ³¨å°‘æ•°ç›¸å…³ä»¤ç‰Œã€‚ç ”ç©¶è€…æ„å»ºäº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶ä½¿ç”¨ç»è¿‡è‡ªå®šä¹‰GPTè®­ç»ƒçš„GiantRabbitä½œä¸ºå®éªŒå·¥å…·ã€‚å®éªŒæµ‹è¯•å¹¶æ¯”è¾ƒäº†è¯¥æ¨¡å‹ä¸o1 Previewåœ¨è§£å†³MIT OpenCourseWareçš„çº¿æ€§ä»£æ•°æµ‹è¯•é—®é¢˜æ—¶çš„æ¨ç†æ—¶é—´ã€æ­£ç¡®ç‡å¾—åˆ†å’Œæ€ç»´é“¾é•¿åº¦ã€‚ç»“æœè¡¨æ˜ï¼ŒGiantRabbitçš„æ¨ç†æ—¶é—´å’Œæ€ç»´é“¾é•¿åº¦æ˜æ˜¾ä½äºo1 Previewã€‚è¿™éªŒè¯äº†ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–æ€ç»´é“¾æ¨ç†çš„å¯è¡Œæ€§ã€‚è¯¦ç»†çš„æ¶æ„ç»†èŠ‚å’Œå®éªŒè¿‡ç¨‹å·²ä¸Šä¼ åˆ°Githubï¼Œé“¾æ¥ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git%E3%80%82">https://github.com/brucewang123456789/GeniusTrail.gitã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.09111v6">PDF</a> The main text is 5 pages, totaling 9 pages; 4 figures, 1 table. It   have been submitted to NeurIPS 2024 Workshop MusIML and OpenReview</p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶ä¸ºåº”å¯¹å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†æˆæœ¬é£™å‡çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶ä»…å…³æ³¨å°‘æ•°ç›¸å…³ä»¤ç‰Œã€‚ç ”ç©¶è€…æ„å»ºäº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶ä½¿ç”¨GiantRabbitä½œä¸ºå®éªŒå·¥å…·è¿›è¡Œæµ‹è¯•ï¼Œå…¶å·²ä½¿ç”¨è‡ªå®šä¹‰GPTè¿›è¡Œè®­ç»ƒã€‚å®éªŒé€šè¿‡è§£å†³MIT OpenCourseWareçš„çº¿æ€§ä»£æ•°æµ‹è¯•é—®é¢˜ï¼Œå¯¹æ¯”äº†è¯¥æ¨¡å‹ä¸o1 Previewçš„æ¨ç†æ—¶é—´ã€æ­£ç¡®ç‡å¾—åˆ†å’Œæ€ç»´é“¾é•¿åº¦ã€‚ç»“æœæ˜¾ç¤ºGiantRabbitçš„æ¨ç†æ—¶é—´å’Œæ€ç»´é“¾é•¿åº¦å‡æ˜¾è‘—ä½äºo1 Previewï¼ŒéªŒè¯äº†ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶åœ¨ä¼˜åŒ–æ€ç»´é“¾æ¨ç†æ–¹é¢çš„å¯è¡Œæ€§ã€‚å…·ä½“æ¶æ„ç»†èŠ‚å’Œå®éªŒè¿‡ç¨‹å·²ä¸Šä¼ è‡³Githubã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶æ¥ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æˆæœ¬ã€‚</li>
<li>GiantRabbitä½œä¸ºå®éªŒå·¥å…·ï¼Œé‡‡ç”¨è‡ªå®šä¹‰GPTè¿›è¡Œè®­ç»ƒã€‚</li>
<li>å®éªŒå¯¹æ¯”äº†GiantRabbitä¸o1 Previewåœ¨è§£å†³MITçº¿æ€§ä»£æ•°æµ‹è¯•é—®é¢˜æ—¶çš„æ¨ç†æ—¶é—´ã€æ­£ç¡®ç‡å¾—åˆ†å’Œæ€ç»´é“¾é•¿åº¦ã€‚</li>
<li>GiantRabbitçš„æ¨ç†æ—¶é—´å’Œæ€ç»´é“¾é•¿åº¦æ˜¾è‘—ä¼˜äºo1 Previewã€‚</li>
<li>ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶åœ¨ä¼˜åŒ–æ€ç»´é“¾æ¨ç†æ–¹é¢çš„å¯è¡Œæ€§å¾—åˆ°éªŒè¯ã€‚</li>
<li>ç ”ç©¶çš„å…·ä½“æ¶æ„ç»†èŠ‚å’Œå®éªŒè¿‡ç¨‹å·²ä¸Šä¼ è‡³Githubï¼Œæ–¹ä¾¿åç»­ç ”ç©¶è€…å‚è€ƒå’Œæ‹“å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.09111">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f9e75f08d9f4f952f567f735438c52fe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c401dea16aa3e7f14055388a9fa78e9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c036e7afe31e6727d6202e1bb5a7bc19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-394bfa20f17dfec94a77f107384829e8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Mixture-of-Attentions-For-Speculative-Decoding"><a href="#Mixture-of-Attentions-For-Speculative-Decoding" class="headerlink" title="Mixture of Attentions For Speculative Decoding"></a>Mixture of Attentions For Speculative Decoding</h2><p><strong>Authors:Matthieu Zimmer, Milan Gritta, Gerasimos Lampouras, Haitham Bou Ammar, Jun Wang</strong></p>
<p>The growth in the number of parameters of Large Language Models (LLMs) has led to a significant surge in computational requirements, making them challenging and costly to deploy. Speculative decoding (SD) leverages smaller models to efficiently propose future tokens, which are then verified by the LLM in parallel. Small models that utilise activations from the LLM currently achieve the fastest decoding speeds. However, we identify several limitations of SD models including the lack of on-policyness during training and partial observability. To address these shortcomings, we propose a more grounded architecture for small models by introducing a Mixture of Attentions for SD. Our novel architecture can be applied in two scenarios: a conventional single device deployment and a novel client-server deployment where the small model is hosted on a consumer device and the LLM on a server. In a single-device scenario, we demonstrate state-of-the-art speedups improving EAGLE-2 by 9.5% and its acceptance length by 25%. In a client-server setting, our experiments demonstrate: 1) state-of-the-art latencies with minimal calls to the server for different network conditions, and 2) in the event of a complete disconnection, our approach can maintain higher accuracy compared to other SD methods and demonstrates advantages over API calls to LLMs, which would otherwise be unable to continue the generation process. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‚æ•°æ•°é‡çš„å¢é•¿å¯¼è‡´è®¡ç®—éœ€æ±‚æ€¥å‰§å¢åŠ ï¼Œä½¿å…¶éƒ¨ç½²å…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬é«˜æ˜‚ã€‚æŠ•æœºè§£ç ï¼ˆSDï¼‰åˆ©ç”¨è¾ƒå°çš„æ¨¡å‹æœ‰æ•ˆåœ°æå‡ºæœªæ¥ä»¤ç‰Œï¼Œç„¶åç”±LLMå¹¶è¡ŒéªŒè¯ã€‚ç›®å‰ï¼Œåˆ©ç”¨LLMæ¿€æ´»çš„å°å‹æ¨¡å‹å®ç°äº†æœ€å¿«çš„è§£ç é€Ÿåº¦ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°äº†SDæ¨¡å‹çš„å‡ ä¸ªå±€é™æ€§ï¼ŒåŒ…æ‹¬è®­ç»ƒè¿‡ç¨‹ä¸­ç¼ºä¹ç­–ç•¥æ€§å’Œéƒ¨åˆ†å¯è§‚å¯Ÿæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›ä¸è¶³ï¼Œæˆ‘ä»¬é€šè¿‡ä¸ºSDå¼•å…¥æ··åˆæ³¨æ„åŠ›ï¼Œæå‡ºäº†å°å‹æ¨¡å‹æ›´å®ç”¨çš„æ¶æ„ã€‚æˆ‘ä»¬çš„æ–°å‹æ¶æ„å¯åº”ç”¨äºä¸¤ç§åœºæ™¯ï¼šä¼ ç»Ÿçš„å•ä¸€è®¾å¤‡éƒ¨ç½²å’Œæ–°å‹å®¢æˆ·ç«¯-æœåŠ¡å™¨éƒ¨ç½²ï¼Œå…¶ä¸­å°å‹æ¨¡å‹æ‰˜ç®¡åœ¨æ¶ˆè´¹è®¾å¤‡ä¸Šï¼Œè€ŒLLMæ‰˜ç®¡åœ¨æœåŠ¡å™¨ä¸Šã€‚åœ¨å•ä¸€è®¾å¤‡åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æœ€å…ˆè¿›çš„åŠ é€Ÿæ•ˆæœï¼Œå°†EAGLE-2çš„é€Ÿåº¦æé«˜äº†9.5%ï¼Œå¹¶æé«˜äº†å…¶æ¥å—é•¿åº¦25%ã€‚åœ¨å®¢æˆ·ç«¯-æœåŠ¡å™¨è®¾ç½®ä¸­ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼š1ï¼‰åœ¨ä¸åŒç½‘ç»œæ¡ä»¶ä¸‹ï¼Œä»¥æœ€å°çš„æœåŠ¡å™¨è°ƒç”¨å®ç°äº†æœ€å…ˆè¿›çš„å»¶è¿Ÿï¼›2ï¼‰åœ¨å®Œå…¨æ–­å¼€è¿æ¥çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½ç»´æŒæ¯”å…¶ä»–SDæ–¹æ³•å’ŒAPIè°ƒç”¨LLMæ›´é«˜çš„å‡†ç¡®æ€§ï¼Œå¦åˆ™æ— æ³•ç»§ç»­ç”Ÿæˆè¿‡ç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03804v2">PDF</a> Accepted at International Conference on Learning Representations   (ICLR 2025)</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‚æ•°å¢é•¿å¯¼è‡´è®¡ç®—éœ€æ±‚æ€¥å‰§å¢åŠ ï¼Œä½¿å…¶éƒ¨ç½²å…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬ã€‚æŠ•æœºè§£ç ï¼ˆSDï¼‰åˆ©ç”¨å°å‹æ¨¡å‹æœ‰æ•ˆæå‡ºæœªæ¥ä»¤ç‰Œï¼Œç„¶åç”±LLMå¹¶è¡ŒéªŒè¯ã€‚ç›®å‰ï¼Œåˆ©ç”¨LLMæ¿€æ´»çš„å°å‹æ¨¡å‹å®ç°äº†æœ€å¿«çš„è§£ç é€Ÿåº¦ã€‚ä½†SDæ¨¡å‹å­˜åœ¨ç¼ºä¹è®­ç»ƒæ—¶çš„ç­–ç•¥æ€§å’Œéƒ¨åˆ†è§‚æµ‹æ€§çš„å±€é™ã€‚ä¸ºè§£å†³è¿™äº›ä¸è¶³ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ›´ä¸ºåŸºç¡€çš„æ¶æ„ï¼Œä¸ºå°å‹æ¨¡å‹å¼•å…¥æ··åˆæ³¨æ„åŠ›æœºåˆ¶ã€‚æˆ‘ä»¬çš„æ–°å‹æ¶æ„å¯åº”ç”¨äºä¸¤ç§åœºæ™¯ï¼šä¼ ç»Ÿçš„å•ä¸€è®¾å¤‡éƒ¨ç½²å’Œæ–°å‹å®¢æˆ·ç«¯-æœåŠ¡å™¨éƒ¨ç½²ï¼Œå…¶ä¸­å°å‹æ¨¡å‹æ‰˜ç®¡åœ¨æ¶ˆè´¹è®¾å¤‡ä¸Šï¼Œè€ŒLLMæ‰˜ç®¡åœ¨æœåŠ¡å™¨ä¸Šã€‚åœ¨å•ä¸€è®¾å¤‡åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬å®ç°äº†æœ€å…ˆè¿›çš„é€Ÿåº¦æå‡ï¼Œä½¿EAGLE-2çš„é€Ÿåº¦æå‡9.5%ï¼Œæ¥å—é•¿åº¦æé«˜25%ã€‚åœ¨å®¢æˆ·ç«¯-æœåŠ¡å™¨è®¾ç½®ä¸­ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼š1ï¼‰ä¸åŒç½‘ç»œæ¡ä»¶ä¸‹çš„æœ€å…ˆè¿›å»¶è¿Ÿï¼Œå¯¹æœåŠ¡å™¨çš„è°ƒç”¨æ¬¡æ•°æœ€å°‘ï¼›2ï¼‰åœ¨å®Œå…¨æ–­å¼€è¿æ¥çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½ç»´æŒæ¯”å…¶ä»–SDæ–¹æ³•å’ŒAPIè°ƒç”¨æ›´é«˜çš„å‡†ç¡®æ€§ã€‚è¿™ä½¿å¾—å³ä½¿åœ¨ç¦»çº¿æƒ…å†µä¸‹ä¹Ÿèƒ½ä¿è¯LLMçš„ç”Ÿæˆè¿‡ç¨‹ç»§ç»­è¿›è¡Œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå‚æ•°å¢é•¿å¯¼è‡´è®¡ç®—éœ€æ±‚æ¿€å¢ï¼Œä½¿å¾—éƒ¨ç½²å›°éš¾ä¸”æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>æŠ•æœºè§£ç ï¼ˆSDï¼‰åˆ©ç”¨å°å‹æ¨¡å‹æå‡ºæœªæ¥ä»¤ç‰Œï¼Œå¹¶ç”±LLMéªŒè¯ã€‚</li>
<li>ç›®å‰æœ€å¿«è§£ç é€Ÿåº¦ç”±åˆ©ç”¨LLMæ¿€æ´»çš„å°å‹æ¨¡å‹å®ç°ã€‚</li>
<li>SDæ¨¡å‹å­˜åœ¨è®­ç»ƒç­–ç•¥æ€§å’Œéƒ¨åˆ†è§‚æµ‹æ€§çš„å±€é™æ€§ã€‚</li>
<li>å¼•å…¥æ··åˆæ³¨æ„åŠ›æœºåˆ¶çš„æ–°å‹æ¶æ„æ—¨åœ¨è§£å†³è¿™äº›ä¸è¶³ã€‚</li>
<li>æ–°å‹æ¶æ„é€‚ç”¨äºå•ä¸€è®¾å¤‡éƒ¨ç½²å’Œå®¢æˆ·ç«¯-æœåŠ¡å™¨éƒ¨ç½²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-48429319bde24af2e416889d1f344f9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-585b3baeae71de5fe5fae77a3f9ec5a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08b7a28f084328548d21cf8f7dffcf56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86e295418a028ec4ba7c1ce2ce333a48.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="How-to-Train-Long-Context-Language-Models-Effectively"><a href="#How-to-Train-Long-Context-Language-Models-Effectively" class="headerlink" title="How to Train Long-Context Language Models (Effectively)"></a>How to Train Long-Context Language Models (Effectively)</h2><p><strong>Authors:Tianyu Gao, Alexander Wettig, Howard Yen, Danqi Chen</strong></p>
<p>We study continued training and supervised fine-tuning (SFT) of a language model (LM) to make effective use of long-context information. We first establish a reliable evaluation protocol to guide model development â€“ instead of perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set of long-context downstream tasks, and we evaluate models after SFT as this better reveals long-context abilities. Supported by our robust evaluations, we run thorough experiments to decide the data mix for continued pre-training, the instruction tuning dataset, and many other design choices such as position extrapolation. We find that (1) code repositories and books are excellent sources of long data, but it is crucial to combine them with high-quality short-context data; (2) training with a sequence length beyond the evaluation length boosts long-context performance; (3) for SFT, using only short instruction datasets yields strong performance on long-context tasks. Our final model, ProLong-8B, which is initialized from Llama-3 and trained on 40B tokens, demonstrates state-of-the-art long-context performance among similarly sized models at a length of 128K. ProLong outperforms Llama-3.1-8B-Instruct on the majority of long-context tasks despite using only 5% as many tokens during long-context training. Additionally, ProLong can effectively process up to 512K tokens, one of the longest context windows of publicly available LMs. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†ä¸€ç§è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰çš„æŒç»­è®­ç»ƒå’ŒåŸºäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„æ–¹æ³•ï¼Œä»¥æœ‰æ•ˆåˆ©ç”¨é•¿ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå¯é çš„è¯„ä¼°åè®®æ¥æŒ‡å¯¼æ¨¡å‹çš„å‘å±•â€”â€”æˆ‘ä»¬ä¸æ˜¯ä½¿ç”¨å›°æƒ‘åº¦æˆ–ç®€å•çš„â€œå¤§æµ·æé’ˆâ€ï¼ˆNIAHï¼‰æµ‹è¯•ï¼Œè€Œæ˜¯ä½¿ç”¨ä¸€ç³»åˆ—å¹¿æ³›çš„ä¸‹æ¸¸é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡æ¥è¯„ä¼°æ¨¡å‹ï¼Œå¹¶åœ¨å¾®è°ƒåè¯„ä¼°æ¨¡å‹ï¼Œå› ä¸ºè¿™èƒ½æ›´å¥½åœ°æ­ç¤ºæ¨¡å‹å¤„ç†é•¿ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ã€‚å‡­å€Ÿæˆ‘ä»¬ç¨³å¥çš„è¯„ä¼°ç»“æœï¼Œæˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„å®éªŒæ¥å†³å®šæŒç»­é¢„è®­ç»ƒçš„æ•°æ®æ··åˆæ–¹å¼ã€æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ä»¥åŠå…¶ä»–è®¸å¤šè®¾è®¡é€‰æ‹©ï¼Œå¦‚ä½ç½®å¤–æ¨ç­‰ã€‚æˆ‘ä»¬å‘ç°ï¼šï¼ˆ1ï¼‰ä»£ç ä»“åº“å’Œä¹¦ç±æ˜¯é•¿æ•°æ®çš„ä¼˜ç§€æ¥æºï¼Œä½†å°†å®ƒä»¬ä¸é«˜è´¨é‡çŸ­ä¸Šä¸‹æ–‡æ•°æ®ç›¸ç»“åˆè‡³å…³é‡è¦ï¼›ï¼ˆ2ï¼‰ä½¿ç”¨è¶…è¿‡è¯„ä¼°é•¿åº¦çš„åºåˆ—é•¿åº¦è¿›è¡Œè®­ç»ƒå¯ä»¥æé«˜é•¿ä¸Šä¸‹æ–‡æ€§èƒ½ï¼›ï¼ˆ3ï¼‰å¯¹äºSFTæ¥è¯´ï¼Œä»…ä½¿ç”¨çŸ­æŒ‡ä»¤æ•°æ®é›†å³å¯åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šå®ç°å‡ºè‰²æ€§èƒ½ã€‚æˆ‘ä»¬çš„æœ€ç»ˆæ¨¡å‹ProLong-8Bä»¥Llama-3ä¸ºåŸºç¡€è¿›è¡Œåˆå§‹åŒ–ï¼Œå¹¶åœ¨40Bä¸ªæ ‡è®°ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåœ¨é•¿åº¦ä¸º128Kçš„æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„ä¸Šä¸‹æ–‡æ€§èƒ½ã€‚ProLongåœ¨å¤§å¤šæ•°é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºLlama-3.1-8B-Instructï¼Œå°½ç®¡åœ¨é•¿ä¸Šä¸‹æ–‡è®­ç»ƒæœŸé—´ä½¿ç”¨çš„æ ‡è®°æ•°é‡ä»…å å…¶5%ã€‚æ­¤å¤–ï¼ŒProLongå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†é•¿è¾¾512Kçš„ä»¤ç‰Œï¼Œè¿™æ˜¯ç›®å‰å…¬å¼€å¯ç”¨çš„è¯­è¨€æ¨¡å‹ä¸­å¤„ç†æœ€é•¿ä¸Šä¸‹æ–‡çª—å£ä¹‹ä¸€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02660v2">PDF</a> Our code, data, and models are available at   <a target="_blank" rel="noopener" href="https://github.com/princeton-nlp/ProLong">https://github.com/princeton-nlp/ProLong</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ç ”ç©¶å¦‚ä½•é€šè¿‡æŒç»­è®­ç»ƒå’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥æœ‰æ•ˆåˆ©ç”¨è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰çš„é•¿ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚é€šè¿‡å¯é çš„è¯„ä¼°åè®®ï¼Œå¯¹æ¨¡å‹è¿›è¡Œé•¿æœŸé¢„è®­ç»ƒæ•°æ®æ··åˆã€æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ä»¥åŠä½ç½®å¤–æ¨ç­‰è®¾è®¡é€‰æ‹©è¿›è¡Œå½»åº•å®éªŒã€‚æœ€ç»ˆï¼ŒProLongæ¨¡å‹åœ¨ç±»ä¼¼è§„æ¨¡æ¨¡å‹ä¸­å±•ç°å‡ºå“è¶Šçš„é•¿ä¸Šä¸‹æ–‡æ€§èƒ½ï¼Œå¹¶èƒ½æœ‰æ•ˆå¤„ç†é•¿è¾¾512Kçš„ä»¤ç‰Œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äº†å¦‚ä½•é€šè¿‡æŒç»­è®­ç»ƒå’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æœ‰æ•ˆåˆ©ç”¨è¯­è¨€æ¨¡å‹çš„é•¿ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>ä½¿ç”¨å¹¿æ³›çš„é•¿ä¸Šä¸‹æ–‡ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œæ¨¡å‹è¯„ä¼°ï¼Œå¹¶åœ¨SFTåè¿›è¡Œæ¨¡å‹è¯„ä¼°ï¼Œä»¥æ›´å¥½åœ°æ­ç¤ºæ¨¡å‹çš„é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ã€‚</li>
<li>ä»£ç ä»“åº“å’Œä¹¦ç±æ˜¯é•¿æ•°æ®çš„ä¼˜ç§€æ¥æºï¼Œä½†éœ€è¦ä¸é«˜è´¨é‡çš„çŸ­ä¸Šä¸‹æ–‡æ•°æ®ç›¸ç»“åˆã€‚</li>
<li>åºåˆ—é•¿åº¦è¶…è¿‡è¯„ä¼°é•¿åº¦çš„è®­ç»ƒå¯ä»¥æé«˜é•¿ä¸Šä¸‹æ–‡æ€§èƒ½ã€‚</li>
<li>å¯¹äºSFTï¼Œä»…ä½¿ç”¨çŸ­æŒ‡ä»¤æ•°æ®é›†å¯ä»¥åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šäº§ç”Ÿå¼ºå¤§æ€§èƒ½ã€‚</li>
<li>ProLongæ¨¡å‹åœ¨ç±»ä¼¼è§„æ¨¡æ¨¡å‹ä¸­å±•ç°å‡ºå“è¶Šçš„é•¿ä¸Šä¸‹æ–‡æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02660">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1eae1ef776cdf35216957d6381603c2e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-34f34b6ffe649d9de080bb4617f89f92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aed065e23ea272904d95ce638704299e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5820846114188741d15b3e53769ae23a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b71c55be417ae92faad9353cc22fa6bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3326f563de1d166e4d62e4188135732d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a9bbae7375666dfad4304ddfa2a2b3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae5c71e8b4626ea45ea09b4754294741.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e810f3c2879c6ba8645790a9fcf86eda.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="HATFormer-Historic-Handwritten-Arabic-Text-Recognition-with-Transformers"><a href="#HATFormer-Historic-Handwritten-Arabic-Text-Recognition-with-Transformers" class="headerlink" title="HATFormer: Historic Handwritten Arabic Text Recognition with   Transformers"></a>HATFormer: Historic Handwritten Arabic Text Recognition with   Transformers</h2><p><strong>Authors:Adrian Chan, Anupam Mijar, Mehreen Saeed, Chau-Wai Wong, Akram Khater</strong></p>
<p>Arabic handwritten text recognition (HTR) is challenging, especially for historical texts, due to diverse writing styles and the intrinsic features of Arabic script. Additionally, Arabic handwriting datasets are smaller compared to English ones, making it difficult to train generalizable Arabic HTR models. To address these challenges, we propose HATFormer, a transformer-based encoder-decoder architecture that builds on a state-of-the-art English HTR model. By leveraging the transformerâ€™s attention mechanism, HATFormer captures spatial contextual information to address the intrinsic challenges of Arabic script through differentiating cursive characters, decomposing visual representations, and identifying diacritics. Our customization to historical handwritten Arabic includes an image processor for effective ViT information preprocessing, a text tokenizer for compact Arabic text representation, and a training pipeline that accounts for a limited amount of historic Arabic handwriting data. HATFormer achieves a character error rate (CER) of 8.6% on the largest public historical handwritten Arabic dataset, with a 51% improvement over the best baseline in the literature. HATFormer also attains a comparable CER of 4.2% on the largest private non-historical dataset. Our work demonstrates the feasibility of adapting an English HTR method to a low-resource language with complex, language-specific challenges, contributing to advancements in document digitization, information retrieval, and cultural preservation. </p>
<blockquote>
<p>é˜¿æ‹‰ä¼¯æ‰‹å†™æ–‡æœ¬è¯†åˆ«ï¼ˆHTRï¼‰å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶å¯¹äºå†å²æ–‡æœ¬è€Œè¨€ï¼Œå…¶æŒ‘æˆ˜æºäºå¤šç§ä¹¦å†™é£æ ¼å’Œé˜¿æ‹‰ä¼¯æ–‡è„šæœ¬çš„å†…åœ¨ç‰¹å¾ã€‚æ­¤å¤–ï¼Œä¸è‹±æ–‡æ•°æ®é›†ç›¸æ¯”ï¼Œé˜¿æ‹‰ä¼¯æ–‡æ‰‹å†™æ•°æ®é›†è§„æ¨¡è¾ƒå°ï¼Œè¿™ä½¿å¾—è®­ç»ƒé€šç”¨çš„é˜¿æ‹‰ä¼¯æ–‡HTRæ¨¡å‹æ›´åŠ å›°éš¾ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå˜å‹å™¨çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„HATFormeræ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºæœ€å…ˆè¿›çš„è‹±æ–‡HTRæ¨¡å‹æ„å»ºã€‚é€šè¿‡åˆ©ç”¨å˜å‹å™¨çš„æ³¨æ„åŠ›æœºåˆ¶ï¼ŒHATFormeræ•è·ç©ºé—´ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé€šè¿‡åŒºåˆ†è¿ç¬”å­—ç¬¦ã€åˆ†è§£è§†è§‰è¡¨å¾å’Œè¯†åˆ«å˜éŸ³ç¬¦æ¥è§£å†³é˜¿æ‹‰ä¼¯æ–‡è„šæœ¬çš„å†…åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¯¹å†å²æ‰‹å†™é˜¿æ‹‰ä¼¯æ–‡çš„å®šåˆ¶åŒ…æ‹¬ç”¨äºæœ‰æ•ˆé¢„å¤„ç†ViTä¿¡æ¯çš„å›¾åƒå¤„ç†å™¨ã€ç”¨äºç®€æ´é˜¿æ‹‰ä¼¯æ–‡æœ¬è¡¨ç¤ºçš„æ–‡æœ¬æ ‡è®°å™¨ä»¥åŠè€ƒè™‘åˆ°æœ‰é™çš„å†å²é˜¿æ‹‰ä¼¯æ‰‹å†™æ•°æ®çš„è®­ç»ƒç®¡é“ã€‚åœ¨æœ€å¤§çš„å…¬å…±å†å²æ‰‹å†™é˜¿æ‹‰ä¼¯æ–‡æ•°æ®é›†ä¸Šï¼ŒHATFormerå®ç°äº†8.6%çš„å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ï¼Œæ¯”æ–‡çŒ®ä¸­çš„æœ€ä½³åŸºçº¿æ¨¡å‹æé«˜äº†51%ã€‚åœ¨æœ€å¤§çš„ç§æœ‰éå†å²æ•°æ®é›†ä¸Šï¼ŒHATFormerä¹Ÿè¾¾åˆ°äº†å¯æ¯”çš„4.2%çš„CERã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜äº†å°†è‹±æ–‡HTRæ–¹æ³•é€‚åº”å…·æœ‰å¤æ‚ç‰¹å®šè¯­è¨€æŒ‘æˆ˜çš„ä½èµ„æºè¯­è¨€çš„å¯è¡Œæ€§ï¼Œæ¨åŠ¨äº†æ–‡æ¡£æ•°å­—åŒ–ã€ä¿¡æ¯æ£€ç´¢å’Œæ–‡åŒ–ä¿å­˜çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02179v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é˜¿æ‹‰ä¼¯æ‰‹å†™æ–‡æœ¬è¯†åˆ«ï¼ˆHTRï¼‰æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå†å²æ–‡æœ¬ï¼Œå› ä¸ºå­˜åœ¨å¤šæ ·çš„ä¹¦å†™é£æ ¼å’Œé˜¿æ‹‰ä¼¯æ–‡å­—æœ¬èº«çš„ç‰¹æ€§ã€‚æ­¤å¤–ï¼Œé˜¿æ‹‰ä¼¯æ‰‹å†™æ•°æ®é›†æ¯”è‹±è¯­æ•°æ®é›†å°ï¼Œä½¿å¾—è®­ç»ƒé€šç”¨çš„é˜¿æ‹‰ä¼¯HTRæ¨¡å‹æ›´åŠ å›°éš¾ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå˜å‹å™¨çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„HATFormerï¼Œå®ƒå»ºç«‹åœ¨æœ€å…ˆè¿›çš„è‹±è¯­HTRæ¨¡å‹ä¹‹ä¸Šã€‚é€šè¿‡åˆ©ç”¨å˜å‹å™¨çš„æ³¨æ„åŠ›æœºåˆ¶ï¼ŒHATFormeræ•è·ç©ºé—´ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»¥è§£å†³é˜¿æ‹‰ä¼¯æ–‡å­—æœ¬èº«çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åŒºåˆ†è¿ç¬”å­—ç¬¦ã€åˆ†è§£è§†è§‰è¡¨ç¤ºå’Œè¯†åˆ«å˜éŸ³ç¬¦ã€‚æˆ‘ä»¬å¯¹å†å²æ‰‹å†™é˜¿æ‹‰ä¼¯æ–‡çš„å®šåˆ¶åŒ…æ‹¬æœ‰æ•ˆçš„ViTä¿¡æ¯é¢„å¤„ç†å›¾åƒå¤„ç†å™¨ã€ç´§å‡‘çš„é˜¿æ‹‰ä¼¯æ–‡æœ¬è¡¨ç¤ºæ–‡æœ¬æ ‡è®°å™¨ï¼Œä»¥åŠè€ƒè™‘åˆ°æœ‰é™çš„å†å²é˜¿æ‹‰ä¼¯æ‰‹å†™æ•°æ®çš„è®­ç»ƒç®¡é“ã€‚HATFormeråœ¨æœ€å¤§çš„å…¬å…±å†å²æ‰‹å†™é˜¿æ‹‰ä¼¯æ•°æ®é›†ä¸Šå®ç°äº†8.6%çš„å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ï¼Œæ¯”æ–‡çŒ®ä¸­çš„æœ€ä½³åŸºçº¿æé«˜äº†51%ã€‚åœ¨æœ€å¤§çš„ç§æœ‰éå†å²æ•°æ®é›†ä¸Šï¼ŒHATFormerä¹Ÿè¾¾åˆ°äº†å¯æ¯”çš„4.2% CERã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜äº†é€‚åº”è‹±è¯­HTRæ–¹æ³•åœ¨ä½èµ„æºè¯­è¨€ä¸­çš„å¯è¡Œæ€§ï¼Œè¿™ç§è¯­è¨€å…·æœ‰å¤æ‚çš„ç‰¹å®šè¯­è¨€æŒ‘æˆ˜ï¼Œä¸ºæ–‡æ¡£æ•°å­—åŒ–ã€ä¿¡æ¯æ£€ç´¢å’Œæ–‡åŒ–ä¿å­˜åšå‡ºäº†è´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é˜¿æ‹‰ä¼¯æ‰‹å†™æ–‡æœ¬è¯†åˆ«ï¼ˆHTRï¼‰é¢ä¸´å¤šæ ·ä¹¦å†™é£æ ¼å’Œé˜¿æ‹‰ä¼¯æ–‡å­—ç‰¹æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>é˜¿æ‹‰ä¼¯æ‰‹å†™æ•°æ®é›†ç›¸å¯¹è¾ƒå°ï¼Œä½¿å¾—è®­ç»ƒé€šç”¨æ¨¡å‹æ›´å…·æŒ‘æˆ˜æ€§ã€‚</li>
<li>HATFormeræ˜¯ä¸€ä¸ªåŸºäºå˜å‹å™¨çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œæ—¨åœ¨è§£å†³é˜¿æ‹‰ä¼¯HTRçš„æŒ‘æˆ˜ã€‚</li>
<li>HATFormeråˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥åŒºåˆ†è¿ç¬”å­—ç¬¦ã€åˆ†è§£è§†è§‰è¡¨ç¤ºå’Œè¯†åˆ«å˜éŸ³ç¬¦ã€‚</li>
<li>ä¸ºå†å²æ‰‹å†™é˜¿æ‹‰ä¼¯æ–‡å®šåˆ¶äº†å›¾åƒå¤„ç†å™¨ã€æ–‡æœ¬æ ‡è®°å™¨å’Œè®­ç»ƒç®¡é“ã€‚</li>
<li>HATFormeråœ¨å…¬å…±å†å²æ‰‹å†™é˜¿æ‹‰ä¼¯æ•°æ®é›†ä¸Šå®ç°äº†è¾ƒä½çš„å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02179">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0b4c526358f18a164408df0cfa78d6e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17e786d49c0f09cdc7b0dda80523d116.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e17419d7e9f0ba0895b3238f8703f1a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5171bf48e6d88508100cb555a7695ef3.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GRACE-Generating-Socially-Appropriate-Robot-Actions-Leveraging-LLMs-and-Human-Explanations"><a href="#GRACE-Generating-Socially-Appropriate-Robot-Actions-Leveraging-LLMs-and-Human-Explanations" class="headerlink" title="GRACE: Generating Socially Appropriate Robot Actions Leveraging LLMs and   Human Explanations"></a>GRACE: Generating Socially Appropriate Robot Actions Leveraging LLMs and   Human Explanations</h2><p><strong>Authors:Fethiye Irmak Dogan, Umut Ozyurt, Gizem Cinar, Hatice Gunes</strong></p>
<p>When operating in human environments, robots need to handle complex tasks while both adhering to social norms and accommodating individual preferences. For instance, based on common sense knowledge, a household robot can predict that it should avoid vacuuming during a social gathering, but it may still be uncertain whether it should vacuum before or after having guests. In such cases, integrating common-sense knowledge with human preferences, often conveyed through human explanations, is fundamental yet a challenge for existing systems. In this paper, we introduce GRACE, a novel approach addressing this while generating socially appropriate robot actions. GRACE leverages common sense knowledge from LLMs, and it integrates this knowledge with human explanations through a generative network. The bidirectional structure of GRACE enables robots to refine and enhance LLM predictions by utilizing human explanations and makes robots capable of generating such explanations for human-specified actions. Our evaluations show that integrating human explanations boosts GRACEâ€™s performance, where it outperforms several baselines and provides sensible explanations. </p>
<blockquote>
<p>å½“æœºå™¨äººåœ¨äººç±»ç¯å¢ƒä¸­æ“ä½œæ—¶ï¼Œéœ€è¦å¤„ç†å¤æ‚çš„ä»»åŠ¡ï¼Œæ—¢è¦éµå®ˆç¤¾ä¼šè§„èŒƒåˆè¦é€‚åº”ä¸ªäººåå¥½ã€‚ä¾‹å¦‚ï¼ŒåŸºäºå¸¸è¯†çŸ¥è¯†ï¼Œå®¶ç”¨æœºå™¨äººå¯ä»¥é¢„æµ‹åœ¨ç¤¾äº¤èšä¼šæ—¶åº”è¯¥é¿å…å¸å°˜ï¼Œä½†å®ƒå¯èƒ½ä»ç„¶ä¸ç¡®å®šåº”è¯¥åœ¨å®¢äººæ¥ä¹‹å‰è¿˜æ˜¯ä¹‹åå¸å°˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå°†å¸¸è¯†çŸ¥è¯†ä¸äººç±»åå¥½ï¼ˆé€šå¸¸é€šè¿‡äººç±»è§£é‡Šä¼ è¾¾ï¼‰ç»“åˆèµ·æ¥ï¼Œå¯¹äºç°æœ‰ç³»ç»Ÿè€Œè¨€æ˜¯åŸºæœ¬ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†GRACEï¼Œè¿™æ˜¯ä¸€ç§è§£å†³è¯¥é—®é¢˜å¹¶ç”Ÿæˆé€‚å½“çš„ç¤¾ä¼šæœºå™¨äººåŠ¨ä½œçš„æ–°æ–¹æ³•ã€‚GRACEåˆ©ç”¨LLMçš„å¸¸è¯†çŸ¥è¯†ï¼Œå¹¶é€šè¿‡ç”Ÿæˆç½‘ç»œå°†çŸ¥è¯†ä¸äººç±»è§£é‡Šç›¸ç»“åˆã€‚GRACEçš„åŒå‘ç»“æ„ä½¿æœºå™¨äººèƒ½å¤Ÿé€šè¿‡åˆ©ç”¨äººç±»è§£é‡Šæ¥å®Œå–„å’Œå¢å¼ºLLMé¢„æµ‹ï¼Œå¹¶ä½¿æœºå™¨äººèƒ½å¤Ÿé’ˆå¯¹äººç±»æŒ‡å®šçš„åŠ¨ä½œç”Ÿæˆæ­¤ç±»è§£é‡Šã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œèå…¥äººç±»è§£é‡Šå¯ä»¥å¢å¼ºGRACEçš„æ€§èƒ½ï¼Œå®ƒçš„è¡¨ç°ä¼˜äºå‡ ä¸ªåŸºå‡†æµ‹è¯•ï¼Œå¹¶æä¾›åˆç†çš„è§£é‡Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16879v2">PDF</a> 2025 IEEE International Conference on Robotics &amp; Automation (ICRA),   Supplementary video: <a target="_blank" rel="noopener" href="https://youtu.be/GTNCC1GkiQ4">https://youtu.be/GTNCC1GkiQ4</a></p>
<p><strong>Summary</strong></p>
<p>åœ¨å®¶åº­ç¯å¢ƒä¸­ï¼Œæœºå™¨äººéœ€è¦æ‰§è¡Œå¤æ‚ä»»åŠ¡ï¼Œæ—¢è¦éµå®ˆç¤¾ä¼šè§„èŒƒåˆè¦ç…§é¡¾ä¸ªäººå–œå¥½ã€‚æœºå™¨äººå°†å¸¸è¯†çŸ¥è¯†ä¸äººç±»è§£é‡Šç›¸ç»“åˆï¼Œç”Ÿæˆç¬¦åˆç¤¾äº¤è§„èŒƒçš„æœºå™¨äººè¡Œä¸ºæ˜¯ä¸€é¡¹åŸºæœ¬ä¸”å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•GRACEï¼Œå®ƒé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è·å–å¸¸è¯†çŸ¥è¯†ï¼Œå¹¶é€šè¿‡ç”Ÿæˆç½‘ç»œä¸äººçš„è§£é‡Šç›¸ç»“åˆã€‚GRACEçš„åŒå‘ç»“æ„å…è®¸æœºå™¨äººåˆ©ç”¨äººç±»è§£é‡Šæ¥ä¼˜åŒ–å’Œæ”¹è¿›LLMçš„é¢„æµ‹ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿé’ˆå¯¹äººç±»æŒ‡å®šçš„åŠ¨ä½œç”Ÿæˆè§£é‡Šã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæ•´åˆäººç±»è§£é‡Šæé«˜äº†GRACEçš„æ€§èƒ½ï¼Œå…¶è¡¨ç°ä¼˜äºå¤šä¸ªåŸºçº¿å¹¶æä¾›äº†åˆç†çš„è§£é‡Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨äººéœ€è¦åœ¨éµå®ˆç¤¾ä¼šè§„èŒƒå’Œé€‚åº”ä¸ªäººå–œå¥½çš„åŒæ—¶æ‰§è¡Œå¤æ‚ä»»åŠ¡ã€‚</li>
<li>å°†å¸¸è¯†çŸ¥è¯†ä¸äººç±»è§£é‡Šç»“åˆæ˜¯æœºå™¨äººç”Ÿæˆç¤¾äº¤è¡Œä¸ºçš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>GRACEæ˜¯ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è·å–å¸¸è¯†çŸ¥è¯†çš„æ–°æ–¹æ³•ã€‚</li>
<li>GRACEé€šè¿‡ç”Ÿæˆç½‘ç»œä¸äººçš„è§£é‡Šç›¸ç»“åˆï¼Œåˆ©ç”¨äº†åŒå‘ç»“æ„ã€‚</li>
<li>äººç±»è§£é‡Šå¯ä»¥ä¼˜åŒ–å’Œæ”¹è¿›LLMçš„é¢„æµ‹ï¼Œæé«˜GRACEçš„æ€§èƒ½ã€‚</li>
<li>GRACEåœ¨è¯„ä¼°ä¸­è¡¨ç°ä¼˜äºå¤šä¸ªåŸºçº¿ï¼Œå¹¶èƒ½æä¾›åˆç†çš„è§£é‡Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.16879">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-06206cc535db44f6b972b8533549c016.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0056d73d2e72568437535b704f5d303.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56345d92bea7664a0d9a5926dcf335a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a53bcaeb28a2bbddb995147a28027cb6.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Finding-Transformer-Circuits-with-Edge-Pruning"><a href="#Finding-Transformer-Circuits-with-Edge-Pruning" class="headerlink" title="Finding Transformer Circuits with Edge Pruning"></a>Finding Transformer Circuits with Edge Pruning</h2><p><strong>Authors:Adithya Bhaskar, Alexander Wettig, Dan Friedman, Danqi Chen</strong></p>
<p>The path to interpreting a language model often proceeds via analysis of circuits â€“ sparse computational subgraphs of the model that capture specific aspects of its behavior. Recent work has automated the task of discovering circuits. Yet, these methods have practical limitations, as they rely either on inefficient search algorithms or inaccurate approximations. In this paper, we frame automated circuit discovery as an optimization problem and propose <em>Edge Pruning</em> as an effective and scalable solution. Edge Pruning leverages gradient-based pruning techniques, but instead of removing neurons or components, it prunes the \emph{edges} between components. Our method finds circuits in GPT-2 that use less than half the number of edges compared to circuits found by previous methods while being equally faithful to the full model predictions on standard circuit-finding tasks. Edge Pruning is efficient even with as many as 100K examples, outperforming previous methods in speed and producing substantially better circuits. It also perfectly recovers the ground-truth circuits in two models compiled with Tracr. Thanks to its efficiency, we scale Edge Pruning to CodeLlama-13B, a model over 100x the scale that prior methods operate on. We use this setting for a case study comparing the mechanisms behind instruction prompting and in-context learning. We find two circuits with more than 99.96% sparsity that match the performance of the full model and reveal that the mechanisms in the two settings overlap substantially. Our case study shows that Edge Pruning is a practical and scalable tool for interpretability and sheds light on behaviors that only emerge in large models. </p>
<blockquote>
<p>åˆ†æè¯­è¨€æ¨¡å‹çš„è·¯å¾„é€šå¸¸æ˜¯é€šè¿‡åˆ†æç”µè·¯æ¥å®Œæˆçš„ï¼Œè¿™äº›ç”µè·¯æ˜¯æ¨¡å‹ä¸­ç¨€ç–çš„è®¡ç®—å­å›¾ï¼Œèƒ½å¤Ÿæ•æ‰æ¨¡å‹è¡Œä¸ºçš„ç‰¹å®šæ–¹é¢ã€‚è¿‘æœŸçš„å·¥ä½œå·²ç»å®ç°äº†ç”µè·¯çš„è‡ªåŠ¨å‘ç°ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨å®é™…å±€é™æ€§ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºä½æ•ˆçš„æœç´¢ç®—æ³•æˆ–ä¸å‡†ç¡®çš„è¿‘ä¼¼æ–¹æ³•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†è‡ªåŠ¨åŒ–ç”µè·¯å‘ç°æè¿°ä¸ºä¸€ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œå¹¶æå‡ºâ€œè¾¹ç¼˜ä¿®å‰ªâ€ä½œä¸ºä¸€ç§æœ‰æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚â€œè¾¹ç¼˜ä¿®å‰ªâ€åˆ©ç”¨åŸºäºæ¢¯åº¦çš„ä¿®å‰ªæŠ€æœ¯ï¼Œä½†ä¸æ˜¯ç§»é™¤ç¥ç»å…ƒæˆ–ç»„ä»¶ï¼Œè€Œæ˜¯ä¿®å‰ªç»„ä»¶ä¹‹é—´çš„è¾¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨GPT-2ä¸­æ‰¾åˆ°äº†ç”µè·¯ï¼Œè¿™äº›ç”µè·¯ä½¿ç”¨çš„è¾¹æ¯”ä»¥å‰çš„æ–¹æ³•åœ¨æ‰¾åˆ°çš„ç”µè·¯ä¸­ä½¿ç”¨çš„è¾¹å°‘ä¸€åŠï¼ŒåŒæ—¶åœ¨æ ‡å‡†çš„ç”µè·¯å‘ç°ä»»åŠ¡ä¸Šå¿ å®äºå…¨æ¨¡å‹çš„é¢„æµ‹ã€‚å³ä½¿åœ¨å¤šè¾¾10ä¸‡ä¸ªæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œâ€œè¾¹ç¼˜ä¿®å‰ªâ€ä¾ç„¶éå¸¸é«˜æ•ˆï¼Œé€Ÿåº¦å’Œç”µè·¯è´¨é‡å‡ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚å®ƒè¿˜èƒ½å®Œç¾åœ°æ¢å¤ä½¿ç”¨Tracrç¼–è¯‘çš„ä¸¤ä¸ªæ¨¡å‹çš„åŸºå‡†ç”µè·¯ã€‚ç”±äºå…¶é«˜æ•ˆæ€§ï¼Œâ€œè¾¹ç¼˜ä¿®å‰ªâ€è¢«æ‰©å±•åˆ°CodeLlama-13Bæ¨¡å‹ä¸Šï¼Œè¿™æ˜¯ä¸€ä¸ªæ¯”å…ˆå‰æ–¹æ³•å¤„ç†è¿‡çš„æ¨¡å‹å¤§100å€çš„æ¨¡å‹ã€‚æˆ‘ä»¬åˆ©ç”¨è¿™ä¸€è®¾ç½®è¿›è¡Œæ¡ˆä¾‹ç ”ç©¶ï¼Œæ¯”è¾ƒæŒ‡ä»¤æç¤ºå’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒŒåçš„æœºåˆ¶ã€‚æˆ‘ä»¬å‘ç°ä¸¤ä¸ªç”µè·¯çš„ç¨€ç–åº¦è¶…è¿‡99.96%ï¼Œä¸å…¨æ¨¡å‹çš„æ€§èƒ½ç›¸åŒ¹é…ï¼Œå¹¶æ­ç¤ºä¸¤ç§è®¾ç½®ä¸­çš„æœºåˆ¶å­˜åœ¨å¤§é‡é‡å ã€‚æˆ‘ä»¬çš„æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼Œâ€œè¾¹ç¼˜ä¿®å‰ªâ€æ˜¯ä¸€ä¸ªå®ç”¨ä¸”å¯æ‰©å±•çš„è§£è¯»å·¥å…·ï¼Œèƒ½å¤Ÿæ­ç¤ºä»…åœ¨å¤§å‹æ¨¡å‹ä¸­å‡ºç°çš„è¡Œä¸ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.16778v3">PDF</a> NeurIPS 2024 (Spotlight), code available at   <a target="_blank" rel="noopener" href="https://github.com/princeton-nlp/Edge-Pruning">https://github.com/princeton-nlp/Edge-Pruning</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºä¼˜åŒ–çš„è‡ªåŠ¨ç”µè·¯å‘ç°æ¡†æ¶ï¼Œé€šè¿‡è¾¹ç¼˜ä¿®å‰ªï¼ˆEdge Pruningï¼‰æŠ€æœ¯å®ç°é«˜æ•ˆä¸”å¯ä¼¸ç¼©çš„ç”µè·¯å‘ç°ã€‚è¾¹ç¼˜ä¿®å‰ªåˆ©ç”¨åŸºäºæ¢¯åº¦çš„ä¿®å‰ªæŠ€æœ¯ï¼Œä¿®å‰ªç»„ä»¶ä¹‹é—´çš„è¾¹ç¼˜è€Œéç¥ç»å…ƒæˆ–ç»„ä»¶æœ¬èº«ã€‚è¯¥æ–¹æ³•åœ¨GPT-2ä¸­å‘ç°çš„ç”µè·¯ä½¿ç”¨ä¸åˆ°å…ˆå‰æ–¹æ³•å‘ç°ç”µè·¯çš„ä¸€åŠè¾¹ç¼˜æ•°é‡ï¼ŒåŒæ—¶åœ¨æ ‡å‡†ç”µè·¯å‘ç°ä»»åŠ¡ä¸Šå¿ å®äºå…¨æ¨¡å‹é¢„æµ‹ã€‚å³ä½¿åœ¨å¤šè¾¾10ä¸‡ä¸ªä¾‹å­ä¸­ï¼Œè¾¹ç¼˜ä¿®å‰ªä¹Ÿè¡¨ç°å‡ºé«˜æ•ˆæ€§ï¼Œä¸”åœ¨é€Ÿåº¦å’Œç”µè·¯è´¨é‡ä¸Šä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯å®Œç¾åœ°æ¢å¤ä½¿ç”¨Tracrç¼–è¯‘çš„ä¸¤ä¸ªæ¨¡å‹çš„åŸºå‡†ç”µè·¯ã€‚é€šè¿‡æ‰©å±•Edge Pruningåˆ°CodeLlama-13Bæ¨¡å‹ï¼ˆè§„æ¨¡æ˜¯å…ˆå‰æ–¹æ³•çš„100å€ä»¥ä¸Šï¼‰ï¼Œæœ¬ç ”ç©¶æ¢è®¨äº†æŒ‡ä»¤æç¤ºå’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒŒåçš„æœºåˆ¶ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºå‡ºä¸¤ä¸ªé«˜åº¦ç¨€ç–çš„ç”µè·¯ï¼ˆç¨€ç–æ€§è¶…è¿‡99.96%ï¼‰ï¼Œå®ƒä»¬åŒ¹é…å…¨æ¨¡å‹çš„æ€§èƒ½å¹¶æ­ç¤ºäº†ä¸¤ç§è®¾ç½®æœºåˆ¶çš„æ˜¾è‘—é‡å ã€‚æ€»ä½“è€Œè¨€ï¼Œè¾¹ç¼˜ä¿®å‰ªæ˜¯ä¸€ä¸ªå®ç”¨ä¸”å¯ä¼¸ç¼©çš„å·¥å…·ï¼Œä¸ºæ¨¡å‹è¡Œä¸ºæä¾›äº†å¯è§£é‡Šæ€§ï¼Œå¹¶å¯¹å¤§å‹æ¨¡å‹ä¸­å‡ºç°çš„è¡Œä¸ºæä¾›äº†è§£é‡Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡é€šè¿‡ä¼˜åŒ–è‡ªåŠ¨ç”µè·¯å‘ç°çš„æ¡†æ¶æ¥è§£å†³å½“å‰è‡ªåŠ¨ç”µè·¯å‘ç°æ–¹æ³•å­˜åœ¨çš„å®é™…é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„è¾¹ç¼˜ä¿®å‰ªï¼ˆEdge Pruningï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é€šè¿‡ä¿®å‰ªæ¨¡å‹ç»„ä»¶ä¹‹é—´çš„è¾¹ç¼˜æ¥å®ç°ç”µè·¯å‘ç°ã€‚</li>
<li>è¾¹ç¼˜ä¿®å‰ªæŠ€æœ¯åœ¨GPT-2æ¨¡å‹ä¸­å‘ç°çš„ç”µè·¯å…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œè´¨é‡ã€‚</li>
<li>è¾¹ç¼˜ä¿®å‰ªå¯æ‰©å±•åˆ°å¤§å‹æ¨¡å‹ï¼Œå¦‚CodeLlama-13Bï¼Œæ­ç¤ºæŒ‡ä»¤æç¤ºå’Œä¸Šä¸‹æ–‡å­¦ä¹ çš„æœºåˆ¶ã€‚</li>
<li>é€šè¿‡åœ¨CodeLlama-13Bæ¨¡å‹ä¸­çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œå‘ç°äº†é«˜åº¦ç¨€ç–çš„ç”µè·¯ï¼ŒåŒ¹é…å…¨æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶æ˜¾ç¤ºæŒ‡ä»¤æç¤ºå’Œä¸Šä¸‹æ–‡å­¦ä¹ çš„æœºåˆ¶å­˜åœ¨æ˜¾è‘—é‡å ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.16778">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4737a0dd2a3e9ed538140ce078070c43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa66e8e8d6e252cd5968941867ab7f22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4de578c9bd8c84fe51664fa27d2df042.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="ProTrix-Building-Models-for-Planning-and-Reasoning-over-Tables-with-Sentence-Context"><a href="#ProTrix-Building-Models-for-Planning-and-Reasoning-over-Tables-with-Sentence-Context" class="headerlink" title="ProTrix: Building Models for Planning and Reasoning over Tables with   Sentence Context"></a>ProTrix: Building Models for Planning and Reasoning over Tables with   Sentence Context</h2><p><strong>Authors:Zirui Wu, Yansong Feng</strong></p>
<p>Tables play a crucial role in conveying information in various domains. We propose a Plan-then-Reason framework to answer different types of user queries over tables with sentence context. The framework first plans the reasoning paths over the context, then assigns each step to program-based or textual reasoning to reach the final answer. This framework enhances the table reasoning abilities for both in-context learning and fine-tuning methods. GPT-3.5-Turbo following Plan-then-Reason framework surpasses other prompting baselines without self-consistency while using less API calls and in-context demonstrations. We also construct an instruction tuning set TrixInstruct to evaluate the effectiveness of fine-tuning with this framework. We present ProTrix model family by finetuning models on TrixInstruct. Our experiments show that ProTrix family generalizes to diverse unseen tabular tasks with only 6k training instances. We further demonstrate that ProTrix can generate accurate and faithful explanations to answer complex free-form questions. Our work underscores the importance of the planning and reasoning abilities towards a model over tabular tasks with generalizability and interpretability. We open-source our dataset and models at <a target="_blank" rel="noopener" href="https://github.com/WilliamZR/ProTrix">https://github.com/WilliamZR/ProTrix</a>. </p>
<blockquote>
<p>è¡¨æ ¼åœ¨å„ç§é¢†åŸŸçš„ä¿¡æ¯ä¼ è¾¾ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªâ€è®¡åˆ’-æ¨ç†â€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ ¹æ®å¥å­ä¸Šä¸‹æ–‡å›ç­”ä¸åŒç±»å‹çš„ç”¨æˆ·æŸ¥è¯¢è¡¨æ ¼ã€‚è¯¥æ¡†æ¶é¦–å…ˆè®¡åˆ’ä¸Šä¸‹æ–‡ä¸­çš„æ¨ç†è·¯å¾„ï¼Œç„¶åå°†æ¯ä¸€æ­¥åˆ†é…ç»™åŸºäºç¨‹åºæˆ–æ–‡æœ¬æ¨ç†ï¼Œä»¥å¾—å‡ºæœ€ç»ˆç­”æ¡ˆã€‚æ­¤æ¡†æ¶æé«˜äº†ä¸Šä¸‹æ–‡å­¦ä¹ å’Œå¾®è°ƒæ–¹æ³•çš„è¡¨æ ¼æ¨ç†èƒ½åŠ›ã€‚éµå¾ªâ€œè®¡åˆ’-æ¨ç†â€æ¡†æ¶çš„GPT-3.5 Turboè¶…è¶Šäº†å…¶ä»–å…·æœ‰è‡ªæˆ‘ä¸€è‡´æ€§æç¤ºçš„åŸºçº¿ï¼ŒåŒæ—¶ä½¿ç”¨æ›´å°‘çš„APIè°ƒç”¨å’Œä¸Šä¸‹æ–‡æ¼”ç¤ºã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†æŒ‡ä»¤è°ƒæ•´é›†TrixInstructï¼Œä»¥è¯„ä¼°æ­¤æ¡†æ¶è¿›è¡Œå¾®è°ƒçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬é€šè¿‡å¾®è°ƒæ¨¡å‹åœ¨TrixInstructä¸Šæ¨å‡ºäº†ProTrixæ¨¡å‹ç³»åˆ—ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒProTrixç³»åˆ—ä»…ä½¿ç”¨6000ä¸ªè®­ç»ƒå®ä¾‹å°±èƒ½æ¨å¹¿åˆ°å¤šæ ·åŒ–çš„æœªè§è¡¨æ ¼ä»»åŠ¡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼ŒProTrixå¯ä»¥ç”Ÿæˆå‡†ç¡®å’Œå¿ è¯šçš„è§£é‡Šæ¥å›ç­”å¤æ‚çš„è‡ªç”±å½¢å¼é—®é¢˜ã€‚æˆ‘ä»¬çš„å·¥ä½œå¼ºè°ƒäº†è§„åˆ’å’Œæ¨ç†èƒ½åŠ›å¯¹äºæ¨¡å‹åœ¨å¤„ç†è¡¨æ ¼ä»»åŠ¡æ—¶çš„é€šç”¨æ€§å’Œå¯è§£é‡Šæ€§çš„é‡è¦æ€§ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/WilliamZR/ProTrix%E5%85%AC%E5%BC%80%E6%88%91%E4%BB%AC%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%A8%A1%E5%9E%8B%E3%80%82">https://github.com/WilliamZR/ProTrixå…¬å¼€æˆ‘ä»¬çš„æ•°æ®é›†å’Œæ¨¡å‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.02177v3">PDF</a> EMNLP 2024 Findings</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªåä¸ºâ€œPlan-then-Reasonâ€çš„æ¡†æ¶ï¼Œç”¨äºåœ¨è¡¨æ ¼ä¸­æ ¹æ®å¥å­ä¸Šä¸‹æ–‡å›ç­”ä¸åŒç±»å‹çš„ç”¨æˆ·æŸ¥è¯¢ã€‚è¯¥æ¡†æ¶é¦–å…ˆè§„åˆ’æ¨ç†è·¯å¾„ï¼Œç„¶åå°†æ¯ä¸€æ­¥åˆ†é…ç»™åŸºäºç¨‹åºæˆ–æ–‡æœ¬æ¨ç†ï¼Œä»¥å¾—å‡ºæœ€ç»ˆç­”æ¡ˆã€‚æ­¤æ¡†æ¶æé«˜äº†ä¸Šä¸‹æ–‡å­¦ä¹ å’Œå¾®è°ƒæ–¹æ³•çš„è¡¨æ ¼æ¨ç†èƒ½åŠ›ã€‚ä½¿ç”¨æ­¤æ¡†æ¶çš„GPT-3.5 Turboåœ¨æç¤ºåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä½¿ç”¨è¾ƒå°‘çš„APIè°ƒç”¨å’Œä¸Šä¸‹æ–‡æ¼”ç¤ºã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ„å»ºäº†ç”¨äºè¯„ä¼°åœ¨æ­¤æ¡†æ¶ä¸‹å¾®è°ƒæœ‰æ•ˆæ€§çš„æŒ‡ä»¤è°ƒæ•´é›†TrixInstructï¼Œå¹¶æ¨å‡ºäº†ç”±TrixInstructè®­ç»ƒçš„ProTrixæ¨¡å‹å®¶æ—ã€‚å®éªŒè¡¨æ˜ï¼ŒProTrixå®¶æ—ä»…ä½¿ç”¨6kè®­ç»ƒå®ä¾‹å³å¯æ¨å¹¿åˆ°å¤šæ ·åŒ–çš„æœªè§è¡¨æ ¼ä»»åŠ¡ï¼Œå¹¶èƒ½ç”Ÿæˆå‡†ç¡®ä¸”å¿ å®çš„é—®é¢˜è§£ç­”å¤æ‚è‡ªç”±å½¢å¼é—®é¢˜çš„è§£é‡Šã€‚æœ¬æ–‡å¼ºè°ƒåœ¨è¡¨æ ¼ä»»åŠ¡ä¸­é¢å‘æ¨¡å‹çš„è§„åˆ’èƒ½åŠ›å’Œæ¨ç†èƒ½åŠ›çš„é‡è¦æ€§ï¼Œå¹¶å¼€æºäº†æ•°æ®é›†å’Œæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æå‡ºäº†â€œPlan-then-Reasonâ€æ¡†æ¶ï¼Œç”¨äºå›ç­”è¡¨æ ¼ä¸­çš„ç”¨æˆ·æŸ¥è¯¢ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡è§„åˆ’æ¨ç†è·¯å¾„å¹¶ç»“åˆç¨‹åºä¸æ–‡æœ¬æ¨ç†æ¥å¾—å‡ºç­”æ¡ˆã€‚</li>
<li>â€œPlan-then-Reasonâ€æ¡†æ¶å¢å¼ºäº†ä¸Šä¸‹æ–‡å­¦ä¹ å’Œå¾®è°ƒæ–¹æ³•çš„è¡¨æ ¼æ¨ç†èƒ½åŠ›ã€‚</li>
<li>GPT-3.5 Turboéµå¾ªæ­¤æ¡†æ¶åœ¨æç¤ºåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶å‡å°‘APIè°ƒç”¨å’Œä¸Šä¸‹æ–‡æ¼”ç¤ºéœ€æ±‚ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªåä¸ºTrixInstructçš„æŒ‡ä»¤è°ƒæ•´é›†ï¼Œç”¨äºè¯„ä¼°å¾®è°ƒæ•ˆæœã€‚</li>
<li>æ¨å‡ºäº†ProTrixæ¨¡å‹å®¶æ—ï¼Œè¯¥å®¶æ—åœ¨å¤šæ ·åŒ–æœªè§è¡¨æ ¼ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä»…ä½¿ç”¨å°‘é‡è®­ç»ƒå®ä¾‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.02177">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3544b2998d816f01180c51e0bf4d2515.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82ac0e43ec80598215a50fa214e52cf7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cfb4eebf3a4e821b5c8a9a0d7dec2b64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fcd3e4b5e925cc6f57156f529bca926.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MoTCoder-Elevating-Large-Language-Models-with-Modular-of-Thought-for-Challenging-Programming-Tasks"><a href="#MoTCoder-Elevating-Large-Language-Models-with-Modular-of-Thought-for-Challenging-Programming-Tasks" class="headerlink" title="MoTCoder: Elevating Large Language Models with Modular of Thought for   Challenging Programming Tasks"></a>MoTCoder: Elevating Large Language Models with Modular of Thought for   Challenging Programming Tasks</h2><p><strong>Authors:Jingyao Li, Pengguang Chen, Bin Xia, Hong Xu, Jiaya Jia</strong></p>
<p>Large Language Models (LLMs) have showcased impressive capabilities in handling straightforward programming tasks. However, their performance tends to falter when confronted with more challenging programming problems. We observe that conventional models often generate solutions as monolithic code blocks, restricting their effectiveness in tackling intricate questions. To overcome this limitation, we present Module-of-Thought Coder (MoTCoder). We introduce a framework for MoT instruction tuning, designed to promote the decomposition of tasks into logical sub-tasks and sub-modules. Our investigations reveal that, through the cultivation and utilization of sub-modules, MoTCoder significantly improves both the modularity and correctness of the generated solutions, leading to substantial pass@1 improvements of 5.9% on APPS and 5.8% on CodeContests. MoTCoder also achieved significant improvements in self-correction capabilities, surpassing the current SOTA by 3.3%. Additionally, we provide an analysis of between problem complexity and optimal module decomposition and evaluate the maintainability index, confirming that the code generated by MoTCoder is easier to understand and modify, which can be beneficial for long-term code maintenance and evolution. Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/dvlab-research/MoTCoder">https://github.com/dvlab-research/MoTCoder</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†ç®€å•çš„ç¼–ç¨‹ä»»åŠ¡æ—¶å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“é¢å¯¹æ›´å…·æŒ‘æˆ˜æ€§çš„ç¼–ç¨‹é—®é¢˜æ—¶ï¼Œå®ƒä»¬çš„æ€§èƒ½å¾€å¾€ä¼šé‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¼ ç»Ÿæ¨¡å‹å¾€å¾€å°†è§£å†³æ–¹æ¡ˆç”Ÿæˆä¸ºå•ä¸€çš„ä»£ç å—ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨è§£å†³å¤æ‚é—®é¢˜æ—¶çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†â€œæ€ç»´æ¨¡å—ç¼–ç è€…â€ï¼ˆMoTCoderï¼‰ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§MoTæŒ‡ä»¤è°ƒæ•´çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä¿ƒè¿›ä»»åŠ¡åˆ†è§£ä¸ºé€»è¾‘å­ä»»åŠ¡å’Œå­æ¨¡å—ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œé€šè¿‡åŸ¹å…»å’Œåˆ©ç”¨å­æ¨¡å—ï¼ŒMoTCoderæ˜¾è‘—æé«˜äº†ç”Ÿæˆè§£å†³æ–¹æ¡ˆçš„æ¨¡å—åŒ–å’Œæ­£ç¡®æ€§ï¼Œåœ¨APPSå’ŒCodeContestsä¸Šåˆ†åˆ«å®ç°äº†5.9%å’Œ5.8%çš„pass@1æ”¹è¿›ã€‚MoTCoderåœ¨è‡ªæˆ‘çº é”™èƒ½åŠ›æ–¹é¢ä¹Ÿå–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œè¶…è¿‡äº†å½“å‰çš„æœ€ä½³æ°´å¹³3.3%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹é—®é¢˜å¤æ‚æ€§ä¸æœ€ä½³æ¨¡å—åˆ†è§£è¿›è¡Œäº†åˆ†æï¼Œå¹¶è¯„ä¼°äº†å¯ç»´æŠ¤æ€§æŒ‡æ•°ï¼Œè¯å®MoTCoderç”Ÿæˆçš„ä»£ç æ›´å®¹æ˜“ç†è§£å’Œä¿®æ”¹ï¼Œè¿™æœ‰åˆ©äºé•¿æœŸä»£ç ç»´æŠ¤å’Œè¿›åŒ–ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/dvlab-research/MoTCoder%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/dvlab-research/MoTCoderæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.15960v5">PDF</a> Data:   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/JingyaoLi/MoTCode-Data,MoTCoder-32B">https://huggingface.co/datasets/JingyaoLi/MoTCode-Data,MoTCoder-32B</a>:   <a target="_blank" rel="noopener" href="https://huggingface.co/JingyaoLi/MoTCoder-32B-V1.5,MoTCoder-7B">https://huggingface.co/JingyaoLi/MoTCoder-32B-V1.5,MoTCoder-7B</a>:   <a target="_blank" rel="noopener" href="https://huggingface.co/JingyaoLi/MoTCoder-7B-v1.5,Code">https://huggingface.co/JingyaoLi/MoTCoder-7B-v1.5,Code</a>:   <a target="_blank" rel="noopener" href="https://github.com/dvlab-research/MoTCoder">https://github.com/dvlab-research/MoTCoder</a>, Paper: arXiv:2312.15960</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†ç®€å•çš„ç¼–ç¨‹ä»»åŠ¡æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨é¢å¯¹å¤æ‚çš„ç¼–ç¨‹é—®é¢˜æ—¶æ€§èƒ½æœ‰æ‰€ä¸‹é™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ¨¡å—åŒ–æ€ç»´ç¼–ç æ¡†æ¶ï¼ˆMoTCoderï¼‰ã€‚é€šè¿‡ä»»åŠ¡åˆ†è§£ä¸ºé€»è¾‘å­ä»»åŠ¡å’Œå­æ¨¡å—ï¼ŒMoTCoderæ˜¾è‘—æé«˜äº†ç”Ÿæˆè§£å†³æ–¹æ¡ˆçš„æ¨¡å—åŒ–å’Œæ­£ç¡®æ€§ï¼Œåœ¨APPSå’ŒCodeContestsä¸Šåˆ†åˆ«å®ç°äº†5.9%å’Œ5.8%çš„é€šè¿‡ç‡æå‡ã€‚æ­¤å¤–ï¼ŒMoTCoderè¿˜å¢å¼ºäº†è‡ªæˆ‘çº é”™èƒ½åŠ›ï¼Œè¾ƒå½“å‰æœ€ä½³æ¨¡å‹æé«˜äº†3.3%ã€‚ä»£ç æ˜“äºç†è§£å’Œä¿®æ”¹ï¼Œæœ‰åˆ©äºé•¿æœŸä»£ç ç»´æŠ¤å’Œè¿›åŒ–ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤„ç†å¤æ‚ç¼–ç¨‹é—®é¢˜æ—¶æ€§èƒ½æœ‰é™ã€‚</li>
<li>MoTCoderæ¡†æ¶æ—¨åœ¨é€šè¿‡ä»»åŠ¡åˆ†è§£ä¸ºé€»è¾‘å­ä»»åŠ¡å’Œå­æ¨¡å—æ¥æé«˜è§£å†³æ–¹æ¡ˆçš„æ¨¡å—åŒ–ã€‚</li>
<li>MoTCoderæ˜¾è‘—æé«˜äº†ç”Ÿæˆè§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§ï¼Œåœ¨ç‰¹å®šæµ‹è¯•ä¸Šå®ç°äº†é€šè¿‡ç‡æå‡ã€‚</li>
<li>MoTCoderå¢å¼ºäº†è‡ªæˆ‘çº é”™èƒ½åŠ›ï¼Œè¡¨ç°ä¼˜äºå½“å‰æœ€ä½³æ¨¡å‹ã€‚</li>
<li>MoTCoderç”Ÿæˆçš„ä»£ç æ˜“äºç†è§£å’Œä¿®æ”¹ï¼Œæœ‰åˆ©äºé•¿æœŸä»£ç ç»´æŠ¤å’Œè¿›åŒ–ã€‚</li>
<li>ç ”ç©¶æä¾›äº†å¯¹é—®é¢˜å¤æ‚æ€§ä¸æœ€ä¼˜æ¨¡å—åˆ†è§£ä¹‹é—´çš„åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.15960">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fc8dcc4e92475a04cf2806e41ae3db19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d07a1f57d1f36514a9173940b5e1e53.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4dcd590ed7cf0a61a8ab2496da6aab83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a1ad8e7744a29d2fef06f431239df38.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e43970c26f3913c655dc5cad77030b50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a97f6a0fa4bce59cb61d848f50a0882a.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-05/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-05/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-05/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2694f8002e4b19099d99115786a21729.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-05  CHARMS Cognitive Hierarchical Agent with Reasoning and Motion Styles
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-05/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ce8ff264396fcfd5004160cdd3558dc5.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-05  Envisioning Beyond the Pixels Benchmarking Reasoning-Informed Visual   Editing
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24231k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
