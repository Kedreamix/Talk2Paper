<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-10-06  LitterBox+ An Extensible Framework for LLM-enhanced Scratch Static Code   Analysis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-2cd7d9e316a461ba1a249fa172d19ecc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028658&auth_key=1760028658-0-0-91c357543e5f372af096827fbce7ce6f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-10-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    38 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-10-06-更新"><a href="#2025-10-06-更新" class="headerlink" title="2025-10-06 更新"></a>2025-10-06 更新</h1><h2 id="LitterBox-An-Extensible-Framework-for-LLM-enhanced-Scratch-Static-Code-Analysis"><a href="#LitterBox-An-Extensible-Framework-for-LLM-enhanced-Scratch-Static-Code-Analysis" class="headerlink" title="LitterBox+: An Extensible Framework for LLM-enhanced Scratch Static Code   Analysis"></a>LitterBox+: An Extensible Framework for LLM-enhanced Scratch Static Code   Analysis</h2><p><strong>Authors:Benedikt Fein, Florian Obermüller, Gordon Fraser</strong></p>
<p>Large language models (LLMs) have become an essential tool to support developers using traditional text-based programming languages, but the graphical notation of the block-based Scratch programming environment inhibits the use of LLMs. To overcome this limitation, we propose the LitterBox+ framework that extends the Scratch static code analysis tool LitterBox with the generative abilities of LLMs. By converting block-based code to a textual representation suitable for LLMs, LitterBox+ allows users to query LLMs about their programs, about quality issues reported by LitterBox, and it allows generating code fixes. Besides offering a programmatic API for these functionalities, LitterBox+ also extends the Scratch user interface to make these functionalities available directly in the environment familiar to learners. The framework is designed to be easily extensible with other prompts, LLM providers, and new features combining the program analysis capabilities of LitterBox with the generative features of LLMs. We provide a screencast demonstrating the tool at <a target="_blank" rel="noopener" href="https://youtu.be/RZ6E0xgrIgQ">https://youtu.be/RZ6E0xgrIgQ</a>. </p>
<blockquote>
<p>大型语言模型（LLM）已经成为支持开发者使用传统文本编程语言的重要工具，但基于块的Scratch编程环境的图形标记阻碍了LLM的使用。为了克服这一限制，我们提出了LitterBox+框架，该框架扩展了Scratch静态代码分析工具LitterBox，并增加了LLM的生成能力。通过将基于块的代码转换为适合LLM的文本表示形式，LitterBox+允许用户查询LLM关于他们的程序、关于LitterBox报告的质量问题，并允许生成代码修复。除了为这些功能提供程序化API外，LitterBox+还扩展了Scratch用户界面，使这些功能在熟悉的学习者环境中直接使用。该框架设计易于与其他提示、LLM提供商和新功能扩展结合，将LitterBox的程序分析功能与LLM的生成功能相结合。我们提供了一个屏幕录像演示工具：<a target="_blank" rel="noopener" href="https://youtu.be/RZ6E0xgrIgQ">点击这里查看</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12021v2">PDF</a> ASE 2025 Tool Demonstration Track</p>
<p><strong>摘要</strong></p>
<p>LLM已成为开发者使用传统文本编程语言的重要工具，但由于Scratch编程环境的图形符号表示限制了LLM的使用。为克服这一局限，提出LitterBox+框架，该框架扩展了Scratch静态代码分析工具LitterBox，融合了LLM的生成能力。LitterBox+能将基于块的代码转换为适合LLM的文本表示形式，使用户能够查询LLM关于他们的程序、关于LitterBox报告的质量问题，并生成代码修复。除了通过这些功能的程序化API提供这些功能外，LitterBox+还扩展了Scratch用户界面，使这些功能在学习者熟悉的环境中可用。该框架设计易于与其他提示、LLM提供商和新功能扩展结合，将LitterBox的程序分析功能与LLM的生成功能相结合。可通过<a target="_blank" rel="noopener" href="https://youtu.be/RZ6E0xgrIgQ">https://youtu.be/RZ6E0xgrIgQ</a>观看演示该工具的视频。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>LLM已成为传统文本编程语言开发的重要支持工具。</li>
<li>Scratch编程环境的图形表示限制了LLM的使用。</li>
<li>LitterBox+框架扩展了LitterBox，融合了LLM的生成能力，以克服这一限制。</li>
<li>LitterBox+能将基于块的代码转换为适合LLM的文本格式。</li>
<li>用户可以利用LitterBox+查询LLM关于程序和质量问题，并生成代码修复。</li>
<li>LitterBox+提供了程序化API和扩展的Scratch用户界面，使功能更易于使用。</li>
<li>LitterBox+框架设计具有可扩展性，易于与其他提示、LLM提供商和新功能结合。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12021">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-871b0f0f8c193dee0c5278ff27419de3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028665&auth_key=1760028665-0-0-1794363f97591246e15334f42a0eadce&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5aa61a701968048a3bf19e6365089073~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028673&auth_key=1760028673-0-0-df7e069873483fcf23abda1859b29ec9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Probabilistic-Reasoning-with-LLMs-for-k-anonymity-Estimation"><a href="#Probabilistic-Reasoning-with-LLMs-for-k-anonymity-Estimation" class="headerlink" title="Probabilistic Reasoning with LLMs for k-anonymity Estimation"></a>Probabilistic Reasoning with LLMs for k-anonymity Estimation</h2><p><strong>Authors:Jonathan Zheng, Sauvik Das, Alan Ritter, Wei Xu</strong></p>
<p>Probabilistic reasoning is a key aspect of both human and artificial intelligence that allows for handling uncertainty and ambiguity in decision-making. In this paper, we introduce a new numerical reasoning task under uncertainty for large language models, focusing on estimating the privacy risk of user-generated documents containing privacy-sensitive information. We propose BRANCH, a new LLM methodology that estimates the k-privacy value of a text-the size of the population matching the given information. BRANCH factorizes a joint probability distribution of personal information as random variables. The probability of each factor in a population is estimated separately using a Bayesian network and combined to compute the final k-value. Our experiments show that this method successfully estimates the k-value 73% of the time, a 13% increase compared to o3-mini with chain-of-thought reasoning. We also find that LLM uncertainty is a good indicator for accuracy, as high-variance predictions are 37.47% less accurate on average. </p>
<blockquote>
<p>概率推理是人类和人工智能决策中的关键方面，它允许处理不确定性和模糊性。在本文中，我们针对大型语言模型引入了一种新的不确定性数值推理任务，重点是对包含隐私敏感信息的用户生成文档进行隐私风险评估。我们提出了一种新的LLM方法BRANCH，用于估计文本的k隐私值——与给定信息相匹配的人口规模。BRANCH将个人信息的联合概率分布分解为随机变量。分别使用贝叶斯网络估计人口中每个因素的概值，并将其组合起来计算最终的k值。我们的实验表明，该方法能够成功估算出大多数时间的k值，即估算正确率达到为73%，与带有链思维推理的o3-mini相比增加了13%。我们还发现大型语言模型的不确定性是准确性的良好指标，因为高方差预测的平均准确性降低了37.47%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09674v4">PDF</a> 10 pages, Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>文本介绍了概率推理在人工智能与人类决策中的重要性，并指出处理不确定性和模糊性是其主要特点。文章提出了一种新的数值推理任务，针对大型语言模型估计用户生成文档中隐私敏感信息的隐私风险。文章介绍了一种新的LLM方法BRANCH，用于估计文本的k隐私值，即与给定信息匹配的人口规模。BRANCH将个人信息作为随机变量分解联合概率分布。分别使用贝叶斯网络估计人口中每个因素的概率为单独，并组合计算最终的k值。实验表明，该方法成功估计k值的概率为百分之七十三，相较于使用思维链推理的o3-mini提高了百分之十三。还发现LLM的不确定性是准确性的良好指标，高方差预测的平均准确性降低了百分之三十七点四七。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>概率推理是人工智能与人类决策处理不确定性和模糊性的关键方面。</li>
<li>提出了一种新的针对大型语言模型的数值推理任务，重点在于估计用户生成文档的隐私风险。</li>
<li>介绍了一种新的LLM方法BRANCH，用于估计文本的k隐私值。</li>
<li>BRANCH通过分解个人信息的联合概率分布并单独估计每个因素的概率为，来计算k值。</li>
<li>实验显示BRANCH方法成功估计k值的概率较高。</li>
<li>与其他方法相比，BRANCH在估计隐私风险的准确性方面有所提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09674">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-0e8cee6243a92026b54f6dd47d841c4f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028680&auth_key=1760028680-0-0-68dff82f676ec874c2137a6103a36acb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cd153f37ab648c086cd1d8dbb58452cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028687&auth_key=1760028687-0-0-79e48bd0aaa0a0ddc4e6b7236db8a024&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d8d0d19d15d759de44e71673d10996df~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028694&auth_key=1760028694-0-0-d32fed60dc2300950bb9112a1a4d0c22&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Forget-Forgetting-Continual-Learning-in-a-World-of-Abundant-Memory"><a href="#Forget-Forgetting-Continual-Learning-in-a-World-of-Abundant-Memory" class="headerlink" title="Forget Forgetting: Continual Learning in a World of Abundant Memory"></a>Forget Forgetting: Continual Learning in a World of Abundant Memory</h2><p><strong>Authors:Dongkyu Cho, Taesup Moon, Rumi Chunara, Kyunghyun Cho, Sungmin Cha</strong></p>
<p>Continual learning (CL) has traditionally focused on minimizing exemplar memory, a constraint often misaligned with modern systems where GPU time, not storage, is the primary bottleneck. This paper challenges this paradigm by investigating a more realistic regime: one where memory is abundant enough to mitigate forgetting, but full retraining from scratch remains prohibitively expensive. In this practical “middle ground”, we find that the core challenge shifts from stability to plasticity, as models become biased toward prior tasks and struggle to learn new ones. Conversely, improved stability allows simple replay baselines to outperform the state-of-the-art methods at a fraction of the GPU cost. To address this newly surfaced trade-off, we propose Weight Space Consolidation, a lightweight method that combines (1) rank-based parameter resets to restore plasticity with (2) weight averaging to enhance stability. Validated on both class-incremental learning with image classifiers and continual instruction tuning with large language models, our approach outperforms strong baselines while matching the low computational cost of replay, offering a scalable alternative to expensive full-retraining. These findings challenge long-standing CL assumptions and establish a new, cost-efficient baseline for real-world CL systems where exemplar memory is no longer the limiting factor. </p>
<blockquote>
<p>持续学习（CL）传统上主要关注减少样本内存，但这种约束与现代系统往往不匹配，因为现代系统的主要瓶颈在于GPU时间而非存储空间。本文挑战了这一范式，探讨了一个更现实的情境：一个内存充足足以缓解遗忘但完全从头开始重新训练仍然过于昂贵的情境。在这种实际的“中间地带”中，我们发现核心挑战从稳定性转向了可塑性，因为模型偏向于先前的任务，难以学习新任务。相反，增强稳定性使得简单的回放基线可以在GPU成本极小的情况下超越最新技术方法。为了应对新出现的这种权衡，我们提出了权重空间巩固（Weight Space Consolidation），这是一种轻量级的方法，它将（1）基于排名的参数重置用于恢复可塑性，（2）权重平均用于增强稳定性。我们的方法经过图像分类器的类增量学习和大型语言模型的持续指令调整验证，在超越强大基线的同时匹配了回放的低计算成本，为昂贵的完全重新训练提供了可伸缩的替代方案。这些发现挑战了长期存在的CL假设，并为现实世界中的CL系统建立了新的低成本基线，其中样本内存不再是限制因素。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07274v4">PDF</a> 24 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>在内存充足但仍需考虑GPU成本的现实情况下，传统的持续学习（CL）方法不再适用。模型在新任务学习方面存在挑战，主要在于偏向先前的任务并失去对新任务的适应力。针对此，本文提出了Weight Space Consolidation方法，通过结合基于排名的参数重置和权重平均来解决稳定性和可塑性之间的权衡问题。该方法在图像分类器的类增量学习和大型语言模型的连续指令调整中均表现出优异的性能，且计算成本低。这一发现挑战了长期的CL假设，为现实世界的CL系统建立了新的、成本效益高的基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>传统持续学习（CL）主要关注最小化示例内存，但在内存充足但GPU成本高昂的现实情况下，这种方法并不适用。</li>
<li>在这种现实情况下，模型面临的主要挑战从稳定性转向了可塑性，因为模型偏向于先前的任务并难以学习新任务。</li>
<li>为了解决稳定性和可塑性之间的权衡问题，本文提出了Weight Space Consolidation方法，结合了基于排名的参数重置和权重平均技术。</li>
<li>该方法在图像分类器的类增量学习和大型语言模型的连续指令调整中进行了验证，性能优于强基线，同时匹配了回放（replay）的低计算成本。</li>
<li>该方法挑战了长期的CL假设，并为现实世界的CL系统提供了新的、成本效益高的基准。</li>
<li>该方法注重在不需要昂贵的全量训练的前提下，实现模型的持续学习和适应新任务的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07274">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-39102b07c4f431e0569ca197467d48a8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028702&auth_key=1760028702-0-0-9392996442c7173bced003a5583188eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7dd6e28f2557dd10d876fef993aa0c13~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028709&auth_key=1760028709-0-0-5965ef32010644d0add6169a6fe72012&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dcfa2b8d84c081efa8ea43a439d04399~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028716&auth_key=1760028716-0-0-652cebf44ac688eea2dae1c348b979a0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ab16f77c53eca15a44921625f5118e2e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028723&auth_key=1760028723-0-0-5cbc64a1798a7ed59fcd801427310a6c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-747f92f7434b40604dec4baefed0f761~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028729&auth_key=1760028729-0-0-2f2ed192ed0d18a4965861c1ee8ae6a6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Adapting-Large-Language-Models-for-Character-based-Augmentative-and-Alternative-Communication"><a href="#Adapting-Large-Language-Models-for-Character-based-Augmentative-and-Alternative-Communication" class="headerlink" title="Adapting Large Language Models for Character-based Augmentative and   Alternative Communication"></a>Adapting Large Language Models for Character-based Augmentative and   Alternative Communication</h2><p><strong>Authors:Dylan Gaines, Keith Vertanen</strong></p>
<p>Users of Augmentative and Alternative Communication (AAC) may write letter-by-letter via an interface that uses a character language model. However, most state-of-the-art large pretrained language models predict subword tokens of variable length. We investigate how to practically use such models to make accurate and efficient character predictions. Our algorithm for producing character predictions from a subword large language model (LLM) provides more accurate predictions than using a classification layer, a byte-level LLM, or an n-gram model. Additionally, we investigate a domain adaptation procedure based on a large dataset of sentences we curated based on scoring how useful each sentence might be for spoken or written AAC communication. We find our procedure further improves model performance on simple, conversational text. </p>
<blockquote>
<p>增强和替代通信（AAC）的用户可以通过使用字符语言模型的界面逐字进行书写。然而，大多数最先进的大型预训练语言模型预测的是可变长度的子词标记。我们调查了如何实际使用此类模型进行准确高效的字符预测。我们从子词大型语言模型（LLM）中产生字符预测的算法，比使用分类层、字节级LLM或n-gram模型提供更准确的预测。此外，我们还调查了一种基于我们根据每个句子对口语或书面AAC通信的潜在作用评分而精心制作的大量数据集进行的域自适应过程。我们发现我们的过程在简单的对话文本上进一步提高了模型性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10582v3">PDF</a> To appear in Findings of EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>使用大型预训练语言模型进行字符预测的实践方法，对于增强和替代交流（AAC）的用户来说至关重要。该研究提出了一种从子词大型语言模型（LLM）中产生字符预测的算法，相较于分类层、字节级LLM或n-gram模型，该算法预测更为准确。此外，研究还基于我们根据句子对口语或书面AAC交流的潜在价值进行评分而精心挑选的大规模数据集，探索了一种领域适应程序。该程序可进一步提升模型在简单对话文本上的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型预训练语言模型可用于增强和替代交流（AAC）用户的字符预测。</li>
<li>提出的算法可以从子词大型语言模型（LLM）中产生更准确的字符预测。</li>
<li>与分类层、字节级LLM或n-gram模型相比，该算法的预测准确性更高。</li>
<li>研究利用大规模数据集进行领域适应，以提高模型在简单对话文本上的性能。</li>
<li>数据集的选取基于句子对口语或书面AAC交流潜在价值的评分。</li>
<li>该研究强调了大型语言模型在AAC交流中的实际应用价值。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10582">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-14ef9b5ce5b21384b46899c59816180f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028737&auth_key=1760028737-0-0-9a363069de33df1e127bccf2d1c9b2fb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a21dee43293d8d64774a074adfb39f67~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028744&auth_key=1760028744-0-0-f3a7131d1225c3161b347ed2db18e272&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a47433adbe492618fce723ee27dd9941~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028751&auth_key=1760028751-0-0-f674a2bb930137f18772ffbc65814418&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LongProc-Benchmarking-Long-Context-Language-Models-on-Long-Procedural-Generation"><a href="#LongProc-Benchmarking-Long-Context-Language-Models-on-Long-Procedural-Generation" class="headerlink" title="LongProc: Benchmarking Long-Context Language Models on Long Procedural   Generation"></a>LongProc: Benchmarking Long-Context Language Models on Long Procedural   Generation</h2><p><strong>Authors:Xi Ye, Fangcong Yin, Yinghui He, Joie Zhang, Howard Yen, Tianyu Gao, Greg Durrett, Danqi Chen</strong></p>
<p>Existing benchmarks for evaluating long-context language models (LCLMs) primarily focus on long-context recall, requiring models to produce short responses based on a few critical snippets while processing thousands of irrelevant tokens. We introduce LongProc (Long Procedural Generation), a new benchmark that requires both the integration of highly dispersed information and long-form generation. LongProc consists of six diverse procedural generation tasks, such as extracting structured information from HTML pages into a TSV format and executing complex search procedures to create travel plans. These tasks challenge LCLMs by testing their ability to follow detailed procedural instructions, synthesize and reason over dispersed information, and generate structured, long-form outputs (up to 8K tokens). Furthermore, as these tasks adhere to deterministic procedures and yield structured outputs, they enable reliable rule-based evaluation. We evaluated 23 LCLMs, including instruction-tuned models and recent reasoning models, on LongProc at three difficulty levels, with the maximum number of output tokens set at 500, 2K, and 8K. Notably, while all tested models claim a context window size above 32K tokens, open-weight models typically falter on 2K-token tasks, and closed-source models like GPT-4o show significant degradation on 8K-token tasks. Reasoning models achieve stronger overall performance in long-form generation, benefiting from long CoT training. Further analysis reveals that LCLMs struggle to maintain long-range coherence in long-form generations. These findings highlight critical limitations in current LCLMs and suggest substantial room for improvement. Data and code available at: <a target="_blank" rel="noopener" href="https://princeton-pli.github.io/LongProc">https://princeton-pli.github.io/LongProc</a>. </p>
<blockquote>
<p>现有评估长语境语言模型（LCLM）的基准测试主要集中在长语境回忆上，要求模型在处理成千上万的无关标记时，基于几个关键片段产生短回复。我们推出了LongProc（长程序生成）新基准测试，它要求高度分散的信息整合和长篇生成。LongProc包含六个不同的程序生成任务，例如从HTML页面提取结构化信息并转换为TSV格式，执行复杂的搜索程序以创建旅行计划。这些任务通过测试LCLM遵循详细程序指令、合成和推理分散信息、生成结构化长篇输出（最多达8K令牌）的能力来挑战LCLM。此外，由于这些任务遵循确定性程序并产生结构化输出，因此它们可以进行可靠的基于规则的评价。我们在三个难度级别上使用LongProc评估了23个LCLM，包括指令调整模型和最新的推理模型，最大输出令牌数设定为500、2K和8K。值得注意的是，尽管所有测试模型的上下文窗口大小都超过32K令牌，但开放式权重模型通常在2K令牌任务上表现不佳，而闭源模型如GPT-4o在8K令牌任务上表现出显著退化。推理模型在长篇文章生成方面总体表现更强，得益于长期CoT训练。进一步分析表明，LCLM在长篇生成中难以维持长期连贯性。这些发现突出了当前LCLM的关键局限性，并表明有很大的改进空间。[数据集和代码可在：<a target="_blank" rel="noopener" href="https://princeton-pli.github.io/LongProc%E8%8E%B7%E5%8F%96%E3%80%82">https://princeton-pli.github.io/LongProc 获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05414v3">PDF</a> COLM 2025. Data and code available at:   <a target="_blank" rel="noopener" href="https://princeton-pli.github.io/LongProc">https://princeton-pli.github.io/LongProc</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对长语境语言模型（LCLM）的新基准测试LongProc，它要求模型在集成高度分散的信息的同时进行长文本生成。LongProc包含六个不同的过程生成任务，挑战了LCLM遵循详细程序指令、合成和推理分散信息以及生成结构化长文本的能力。评估发现，现有模型在长文本生成方面存在局限性，如开放权重模型在2K标记任务上表现不佳，封闭源模型如GPT-4o在8K标记任务上显著退化。尽管所有测试模型都声称拥有超过32K标记的上下文窗口大小，但整体而言，推理模型在长文本生成方面表现更好，得益于长期训练。建议改善现有模型并优化其在长文本上下文处理方面的性能。更多信息和代码可在<a target="_blank" rel="noopener" href="https://princeton-pli.github.io/LongProc%E8%AE%BF%E9%97%AE%E3%80%82">https://princeton-pli.github.io/LongProc访问。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入LongProc作为评估长语境语言模型（LCLM）的新基准测试，涵盖六个不同的过程生成任务。</li>
<li>LongProc任务要求模型集成高度分散的信息并进行长文本生成，挑战了模型的详细程序指令遵循能力、信息合成和推理能力。</li>
<li>现有LCLM模型在长文本生成方面存在局限性，不同模型在不同难度的任务中表现不同。</li>
<li>开放权重模型在2K标记任务上表现不佳，而封闭源模型如GPT-4o在8K标记任务上显著退化。</li>
<li>尽管所有测试模型都声称拥有较大的上下文窗口，但推理模型在长文本生成方面表现更好，得益于长期训练。</li>
<li>分析发现LCLM在长文本生成的连贯性方面存在问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05414">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-0c6d883e4626dd3513358850e78eddeb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028758&auth_key=1760028758-0-0-b392e1d687af1433e36ed26f36358cd3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3b41204dfde1ef1e80938de7e11f102c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028766&auth_key=1760028766-0-0-d63c66197f447f82326122365a7495f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eb0733cef48bc83fe87a3759e887d292~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028773&auth_key=1760028773-0-0-a62aa953b027893d0189d5888f3fd8bb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0b5b630e9789166049f6a9833fbeb87b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028779&auth_key=1760028779-0-0-497e6fc6e4d146fc823a927951a98ca5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CART-Compositional-Auto-Regressive-Transformer-for-Image-Generation"><a href="#CART-Compositional-Auto-Regressive-Transformer-for-Image-Generation" class="headerlink" title="CART: Compositional Auto-Regressive Transformer for Image Generation"></a>CART: Compositional Auto-Regressive Transformer for Image Generation</h2><p><strong>Authors:Siddharth Roheda, Rohit Chowdhury, Aniruddha Bala, Rohan Jaiswal</strong></p>
<p>We propose a novel Auto-Regressive (AR) image generation approach that models images as hierarchical compositions of interpretable visual layers. While AR models have achieved transformative success in language modeling, replicating this success in vision tasks has presented unique challenges due to inherent spatial dependencies in images. Addressing the unique challenges of vision tasks, our method (CART) adds image details iteratively via semantically meaningful decompositions. We demonstrate the flexibility and generality of CART by applying it across three distinct decomposition strategies: (i) Base-Detail Decomposition (Mumford-Shah smoothness), (ii) Intrinsic Decomposition (albedo&#x2F;shading), and (iii) Specularity Decomposition (diffuse&#x2F;specular). This “next-detail” strategy outperforms traditional “next-token” and “next-scale” approaches, improving controllability, semantic interpretability, and resolution scalability. Experiments show CART generates visually compelling results while enabling structured image manipulation, opening new directions for controllable generative modeling via physically or perceptually motivated image factorization. </p>
<blockquote>
<p>我们提出了一种新型的自动回归（AR）图像生成方法，该方法将图像建模为可解释视觉层次结构的分层组合。虽然AR模型在语言建模方面取得了革命性的成功，但在视觉任务中复制这一成功却面临了独特的挑战，因为图像存在固有的空间依赖性。为了应对视觉任务的独特挑战，我们的方法（CART）通过语义有意义的分解迭代地添加图像细节。我们展示了CART的灵活性和通用性，将其应用于三种不同的分解策略：（i）基础细节分解（Mumford-Shah平滑），（ii）内在分解（亮度&#x2F;阴影），以及（iii）光泽分解（漫反射&#x2F;光泽）。这种“下一个细节”策略优于传统的“下一个标记”和“下一个尺度”方法，提高了可控性、语义可解释性和分辨率的可扩展性。实验表明，CART在生成视觉上引人注目的结果的同时，能够实现结构化图像操作，为通过物理或感知驱动的图像分解实现可控生成建模开辟了新的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10180v2">PDF</a> figures compressed to meet arxiv size limit</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型的Auto-Regressive（AR）图像生成方法，该方法将图像建模为可解释视觉层的层次组合。尽管AR模型在语言建模中取得了突破性成功，但在视觉任务中复制这一成功却面临了独特的挑战，这是由于图像固有的空间依赖性所导致的。本文的方法（CART）通过语义上有意义的分解迭代地添加图像细节，并展示了其在三种不同分解策略上的灵活性和通用性。实验表明，CART生成了视觉吸引人的结果，同时实现了结构化图像操纵，为可控生成建模提供了新的方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了Auto-Regressive（AR）图像生成方法，将图像建模为层次化的可解释视觉层组合。</li>
<li>AR模型在视觉任务中面临了独特的挑战，如图像的空间依赖性。</li>
<li>CART方法通过语义上有意义的分解迭代地添加图像细节。</li>
<li>CART可应用于三种不同的分解策略：Base-Detail分解、Intrinsic分解和Specularity分解。</li>
<li>CART的“next-detail”策略优于传统的“next-token”和“next-scale”方法，提高了可控性、语义可解释性和分辨率可扩展性。</li>
<li>实验表明CART能生成视觉上吸引人的结果，并实现了结构化图像操作。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10180">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-b006987d7c1a699f1b4b91b21769ef7e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028787&auth_key=1760028787-0-0-0599d723baba94a96ce16f1a5975406b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2cd7d9e316a461ba1a249fa172d19ecc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028794&auth_key=1760028794-0-0-42d9e821c71d4940aaf87c227064c99d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-caede51e3bc23972eddea158f78ce659~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028800&auth_key=1760028800-0-0-3362d9a89c01418ba0dbe80f61d0d606&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7632e4d32b8fc97788301f21c5f2ff7d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028807&auth_key=1760028807-0-0-92607c853753ba8a8b148a408070a0fb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3c721b352917b7c9574a823ad0ac99f7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028814&auth_key=1760028814-0-0-f3ed9cffc87f1dcd2b33800ffe62f1a8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-511a880beacfd7d30a58e3cd8abc14fe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028820&auth_key=1760028820-0-0-2899a0ecd89376130321b559c6160b72&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-94ce1033db46907d9ca6cc1b29bdf094~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028827&auth_key=1760028827-0-0-5e4dfbc912be7770213998c3f3a172ab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="PETAH-Parameter-Efficient-Task-Adaptation-for-Hybrid-Transformers-in-a-resource-limited-Context"><a href="#PETAH-Parameter-Efficient-Task-Adaptation-for-Hybrid-Transformers-in-a-resource-limited-Context" class="headerlink" title="PETAH: Parameter Efficient Task Adaptation for Hybrid Transformers in a   resource-limited Context"></a>PETAH: Parameter Efficient Task Adaptation for Hybrid Transformers in a   resource-limited Context</h2><p><strong>Authors:Maximilian Augustin, Syed Shakib Sarwar, Mostafa Elhoushi, Sai Qian Zhang, Yuecheng Li, Barbara De Salvo</strong></p>
<p>Following their success in natural language processing (NLP), there has been a shift towards transformer models in computer vision. While transformers perform well and offer promising multi-tasking performance, due to their high compute requirements, many resource-constrained applications still rely on convolutional or hybrid models that combine the benefits of convolution and attention layers and achieve the best results in the sub 100M parameter range. Simultaneously, task adaptation techniques that allow for the use of one shared transformer backbone for multiple downstream tasks, resulting in great storage savings at negligible cost in performance, have not yet been adopted for hybrid transformers. In this work, we investigate how to achieve the best task-adaptation performance and introduce PETAH: Parameter Efficient Task Adaptation for Hybrid Transformers. We further combine PETAH adaptation with pruning to achieve highly performant and storage friendly models for multi-tasking. In our extensive evaluation on classification and other vision tasks, we demonstrate that our PETAH-adapted hybrid models outperform established task-adaptation techniques for ViTs while requiring fewer parameters and being more efficient on mobile hardware. </p>
<blockquote>
<p>继其在自然语言处理（NLP）领域的成功后，计算机视觉领域也开始转向使用Transformer模型。虽然Transformer表现良好并提供有前景的多任务性能，但由于其计算需求较高，许多资源受限的应用程序仍然依赖于卷积或混合模型，这些模型结合了卷积层和注意力层的优点，并在小于100M的参数范围内取得了最佳结果。同时，允许使用一个共享的Transformer主干进行多个下游任务的任务适配技术，可以在性能损失极小的情况下实现巨大的存储节省，但尚未被混合Transformer所采用。在这项工作中，我们研究了如何实现最佳的任务适配性能，并介绍了PETAH：混合Transformer的参数高效任务适配。我们还将PETAH适配与修剪相结合，以实现多任务的高性能且存储友好的模型。在分类和其他视觉任务的广泛评估中，我们证明了我们的PETAH适配的混合模型在参数更少、移动硬件上效率更高的同时，优于ViT的现有任务适配技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17661v2">PDF</a> Published in CVPRW 2025</p>
<p><strong>Summary</strong><br>     随着自然语言处理（NLP）的成功，计算机视觉领域开始转向使用Transformer模型。虽然Transformer表现良好并提供有前景的多任务性能，但由于其高计算需求，许多资源受限的应用仍依赖于卷积或混合模型。本文探讨了如何实现最佳的任务适应性，并引入了PETAH：混合Transformer的参数高效任务适应性技术。我们还通过剪枝进一步提高PETAH适应性，以实现对多任务的高效处理。在分类和其他视觉任务上的广泛评估表明，我们的PETAH自适应混合模型在参数需求更少且移动端硬件效率更高的同时，优于ViT的任务适应性技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer模型在计算机视觉领域的应用逐渐普及。</li>
<li>虽然Transformer具有良好的多任务性能，但其高计算需求使得资源受限的应用仍依赖卷积或混合模型。</li>
<li>PETAH技术旨在实现混合Transformer的最佳任务适应性。</li>
<li>PETAH技术与剪枝相结合，提高模型的性能和存储效率。</li>
<li>在分类和其他视觉任务上，PETAH自适应混合模型的性能优于ViT的任务适应性技术。</li>
<li>PETAH自适应混合模型具有更少的参数需求和高移动端硬件效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.17661">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-337a1d3aef8f1246f6374d5bc7ba1597~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028835&auth_key=1760028835-0-0-dfa6c2289ca612a18b893821dd6a14fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-77f08df2448ecba569a455fbfeabfe2f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028843&auth_key=1760028843-0-0-69a0f6632340abe1910cc9bae9345dc6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f170797bf79683da221e12b3094d4890~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028849&auth_key=1760028849-0-0-90f0d547603024c4956ce371ca1c7150&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e05bf91bb2bc2125a2b286d0e6c8b7f4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028856&auth_key=1760028856-0-0-1204b0f111260e343152a92e3655115a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cfa7ab545164f40598806c707a3cff3f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028862&auth_key=1760028862-0-0-38e37308ba309c800af8c9fbcf11031b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0a2ad968f7528629e2157efbe757326a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028869&auth_key=1760028869-0-0-55aafe5bf36ee3c96e6df332b2464347&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="M6-GPT-3-Generating-Multitrack-Modifiable-Multi-Minute-MIDI-Music-from-Text-using-Genetic-algorithms-Probabilistic-methods-and-GPT-Models-in-any-Progression-and-Time-Signature"><a href="#M6-GPT-3-Generating-Multitrack-Modifiable-Multi-Minute-MIDI-Music-from-Text-using-Genetic-algorithms-Probabilistic-methods-and-GPT-Models-in-any-Progression-and-Time-Signature" class="headerlink" title="M6(GPT)3: Generating Multitrack Modifiable Multi-Minute MIDI Music from   Text using Genetic algorithms, Probabilistic methods and GPT Models in any   Progression and Time Signature"></a>M6(GPT)3: Generating Multitrack Modifiable Multi-Minute MIDI Music from   Text using Genetic algorithms, Probabilistic methods and GPT Models in any   Progression and Time Signature</h2><p><strong>Authors:Jakub Poćwiardowski, Mateusz Modrzejewski, Marek S. Tatara</strong></p>
<p>This work introduces the M6(GPT)3 composer system, capable of generating complete, multi-minute musical compositions with complex structures in any time signature, in the MIDI domain from input descriptions in natural language. The system utilizes an autoregressive transformer language model to map natural language prompts to composition parameters in JSON format. The defined structure includes time signature, scales, chord progressions, and valence-arousal values, from which accompaniment, melody, bass, motif, and percussion tracks are created. We propose a genetic algorithm for the generation of melodic elements. The algorithm incorporates mutations with musical significance and a fitness function based on normal distribution and predefined musical feature values. The values adaptively evolve, influenced by emotional parameters and distinct playing styles. The system for generating percussion in any time signature utilises probabilistic methods, including Markov chains. Through both human and objective evaluations, we demonstrate that our music generation approach outperforms baselines on specific, musically meaningful metrics, offering a viable alternative to purely neural network-based systems. </p>
<blockquote>
<p>本文介绍了M6（GPT）3作曲系统，该系统能够在MIDI领域中根据自然语言描述生成完整、多分钟的具有复杂结构的音乐。系统采用自回归转换器语言模型，将自然语言提示映射到JSON格式的作曲参数上。所定义的参数结构包括时间签名、音阶、和弦进展以及情感值，由此生成伴奏、旋律、低音、动机和打击乐轨道。我们提出了一种用于生成旋律元素的遗传算法。该算法结合了具有音乐意义的突变和基于正态分布和预定义音乐特征值的适应度函数。这些值会适应性地演变，受到情绪参数和独特演奏风格的影响。系统生成任何时间签名的打击乐部分采用了概率方法，包括马尔可夫链。通过人类和客观评估，我们证明了我们的音乐生成方法在特定的音乐意义上优于基线方法，为仅基于神经网络的系统提供了可行的替代方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.12638v3">PDF</a> Published in 2025 IEEE International Conference on Multimedia and   Expo Workshops (ICMEW)</p>
<p><strong>Summary</strong><br>该工作介绍了M6（GPT）3作曲系统，该系统可根据自然语言描述生成完整、多分钟的音乐作品，具有复杂的结构并能适应任何的时间签名。系统采用自回归转换器语言模型，将自然语言提示映射到JSON格式的创作参数上。通过遗传算法生成旋律元素，结合音乐意义的突变和基于正态分布及预定义音乐特征值的适应度函数。系统可生成任何时间签名的打击乐，采用概率方法，包括马尔可夫链。评估结果表明，该音乐生成方法在某些音乐相关指标上优于基线方法，是纯粹基于神经网络系统的可行替代方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>M6(GPT)3作曲系统能够根据自然语言描述生成具有复杂结构的完整音乐作品。</li>
<li>系统采用自回归转换器语言模型将自然语言提示转换为JSON格式的创作参数。</li>
<li>采用遗传算法生成旋律元素，结合音乐意义的突变和适应度函数。</li>
<li>系统可以适应不同的时间签名并生成相应的音乐作品。</li>
<li>打击乐的生成采用了概率方法，包括使用马尔可夫链。</li>
<li>该音乐生成方法在特定音乐指标上的表现优于基线方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.12638">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-0212663e00b02c36ef653cffc78568a1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028877&auth_key=1760028877-0-0-e2d13456bb2adc3ff9a0931cd89efcd3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-498cc76d1b953b767a27843b64e735a0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028884&auth_key=1760028884-0-0-f61e9eb79ce92b5641668a2f9c3f1c0f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f0f6529b794f1aca9594611a6019da13~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028891&auth_key=1760028891-0-0-16b38fea5bdb2eaf3136f40aa3e73591&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3fab5191981212fed0af77e6beb93138~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028898&auth_key=1760028898-0-0-8beabe7c2a2fa9bd1b97945271be2983&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-998c4d3192ad006e1674b8952472dac4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028904&auth_key=1760028904-0-0-3cfdaedcd31113fea80dddd422a00f20&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="StarTrail-Concentric-Ring-Sequence-Parallelism-for-Efficient-Near-Infinite-Context-Transformer-Model-Training"><a href="#StarTrail-Concentric-Ring-Sequence-Parallelism-for-Efficient-Near-Infinite-Context-Transformer-Model-Training" class="headerlink" title="StarTrail: Concentric Ring Sequence Parallelism for Efficient   Near-Infinite-Context Transformer Model Training"></a>StarTrail: Concentric Ring Sequence Parallelism for Efficient   Near-Infinite-Context Transformer Model Training</h2><p><strong>Authors:Ziming Liu, Shaoyu Wang, Shenggan Cheng, Zhongkai Zhao, Kai Wang, Xuanlei Zhao, James Demmel, Yang You</strong></p>
<p>Training Transformer models on long sequences in a distributed setting poses significant challenges in terms of efficiency and scalability. Current methods are either constrained by the number of attention heads or excessive communication overheads. To address this problem, we propose StarTrail, a multi-dimensional concentric distributed training system for long sequences, fostering an efficient communication paradigm and providing additional tuning flexibility for communication arrangements. Specifically, StarTrail introduces an extra parallel dimension and divides the peer-to-peer communication into sub-rings to substantially reduce communication volume and avoid bandwidth bottlenecks. Through comprehensive experiments across diverse hardware environments and on both Natural Language Processing (NLP) and Computer Vision (CV) tasks, we demonstrate that our approach significantly surpasses state-of-the-art methods that support Long sequence lengths, achieving performance improvements of up to 77.12% on GPT-style models and up to 114.33% on DiT (Diffusion Transformer) models without affecting the computations results. </p>
<blockquote>
<p>在分布式环境中对长序列进行Transformer模型训练，在效率和可扩展性方面面临着巨大挑战。当前的方法要么受到注意力头数量的限制，要么存在过多的通信开销。为了解决这一问题，我们提出了StarTrail，这是一个用于长序列的多维同心分布式训练系统，它促进了一种高效的通信范式，并为通信安排提供了额外的调整灵活性。具体来说，StarTrail引入了一个额外的并行维度，将点对点通信分成子环，从而大幅度减少了通信量并避免了带宽瓶颈。我们通过在不同硬件环境上进行的全面实验，以及自然语言处理（NLP）和计算机视觉（CV）任务，证明了我们的方法显著超过了支持长序列长度的最新方法，在GPT风格模型上实现了高达77.12%的性能提升，在DiT（扩散变换器）模型上实现了高达114.33%的性能提升，且不影响计算结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.00611v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在分布式环境中训练长序列的Transformer模型面临效率和可扩展性的挑战。针对这一问题，本文提出StarTrail系统，该系统采用多维同心分布式训练架构，优化了通信模式并为通信安排提供了额外的调整灵活性。StarTrail通过引入额外的并行维度和将点对点通信划分为子环来减少通信量并避免带宽瓶颈。实验表明，StarTrail在支持长序列长度方面显著优于现有技术方法，GPT风格模型和DiT（扩散转换器）模型的性能分别提高了77.12%和最高可达上超过我们的做法通过简化的创新算法表明序列比现有技术方法提高了高达上提高了高达上超过我们的做法。通过在多样化的硬件环境和自然语言处理（NLP）和计算机视觉（CV）任务上的全面实验，证明了StarTrail的有效性。这些方法在不影响计算结果的情况下实现了显著的性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>训练Transformer模型在长序列的分布式环境中存在效率和可扩展性问题。</li>
<li>当前方法受到注意力头数量的限制或通信开销过大的影响。</li>
<li>StarTrail系统提出一个多维同心分布式训练架构来解决这个问题。</li>
<li>StarTrail引入额外的并行维度，将点对点通信划分为子环来减少通信量和避免带宽瓶颈。</li>
<li>StarTrail通过优化通信模式为通信安排提供额外的调整灵活性。</li>
<li>实验证明，StarTrail在支持长序列长度方面显著优于现有技术方法，GPT风格模型和DiT模型的性能提升显著。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.00611">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-6edfdef04fa474554a054ae66ffa5d0e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028912&auth_key=1760028912-0-0-491ceadd7f6aee88c0ff632760958e0d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4a9b25a29efb6ff1e712c0138fffd119~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028919&auth_key=1760028919-0-0-b938e0539be66f79de3c6d935e5feb50&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0e07c7d52eb7c2f55dcdce056cc9db65~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028926&auth_key=1760028926-0-0-79cf4ef1a9bc75d4c84ecd28eedb7c7f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-af27c6f5b038b256e8e4ee1f10a58c8b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028934&auth_key=1760028934-0-0-242de3e8f5fc513869f5504af8533a4f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-00132758286cf18ae89b5d7a543080a4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028941&auth_key=1760028941-0-0-70272f925ecb5b27e1f7d64b5124da91&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4d1f91b25e2e70a50863c90c3ac4acc6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028948&auth_key=1760028948-0-0-3f1f8661dcc49a97c60b8addfb9cd420&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="PaECTER-Patent-level-Representation-Learning-using-Citation-informed-Transformers"><a href="#PaECTER-Patent-level-Representation-Learning-using-Citation-informed-Transformers" class="headerlink" title="PaECTER: Patent-level Representation Learning using Citation-informed   Transformers"></a>PaECTER: Patent-level Representation Learning using Citation-informed   Transformers</h2><p><strong>Authors:Mainak Ghosh, Michael E. Rose, Sebastian Erhardt, Erik Buunk, Dietmar Harhoff</strong></p>
<p>PaECTER is an open-source document-level encoder specific for patents. We fine-tune BERT for Patents with examiner-added citation information to generate numerical representations for patent documents. PaECTER performs better in similarity tasks than current state-of-the-art models used in the patent domain. More specifically, our model outperforms the patent specific pre-trained language model (BERT for Patents) and general-purpose text embedding models (e.g., E5, GTE, and BGE) on our patent citation prediction test dataset on different rank evaluation metrics. PaECTER predicts at least one most similar patent at a rank of 1.32 on average when compared against 25 irrelevant patents. Numerical representations generated by PaECTER from patent text can be used for downstream tasks such as classification, tracing knowledge flows, or semantic similarity search. Semantic similarity search is especially relevant in the context of prior art search for both inventors and patent examiners. </p>
<blockquote>
<p>PaECTER是一个针对专利的开源文档级别编码器。我们使用审查员添加的引文信息对专利的BERT模型进行微调，以生成专利文档的数值表示。在相似度任务方面，PaECTER的表现优于当前专利领域中使用的最先进的模型。更具体地说，我们的模型在专利引文预测测试数据集上的不同排名评估指标上，超越了针对专利的预训练语言模型（专利BERT）和通用文本嵌入模型（例如E5、GTE和BGE）。当与25篇不相关的专利相比时，PaECTER平均在排名第一时至少预测出一篇最相似的专利，相似度平均值为1.32。由PaECTER从专利文本生成的数值表示可用于下游任务，如分类、追踪知识流或语义相似性搜索。语义相似性搜索对于发明家和专利审查员的现有技术搜索来说尤其重要。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.19411v2">PDF</a> 8 pages, 3 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>PaECTER是一种针对专利的开源文档级别编码器。通过利用审查员添加的引文信息对BERT for Patents进行微调，生成专利文档的数值表示。在专利相似性任务中，PaECTER的表现优于当前专利领域的最先进模型。它能够在专利引文预测测试数据集上超越专利特定预训练语言模型（如BERT for Patents）和通用文本嵌入模型（如E5、GTE和BGE），在不同排名评估指标上表现优异。PaECTER预测的至少一个最相似专利的平均排名为1.32，与25个不相关专利相比。由PaECTER从专利文本生成的数值表示可用于下游任务，如分类、追踪知识流或语义相似性搜索。语义相似性搜索对于发明人和专利审查员的先前技术搜索尤为重要。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PaECTER是一个针对专利的开源文档级别编码器。</li>
<li>通过审查员添加的引文信息对BERT进行微调，生成专利文档的数值表示。</li>
<li>PaECTER在专利相似性任务上的表现优于其他模型。</li>
<li>PaECTER在专利引文预测测试数据集上的表现优于其他模型，包括专利特定预训练语言模型和通用文本嵌入模型。</li>
<li>PaECTER能够预测至少一个最相似专利的平均排名为1.32。</li>
<li>PaECTER生成的数值表示可用于多种下游任务，如分类、追踪知识流和语义相似性搜索。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.19411">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-8f7df05397f26ed416962b99326fb799~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028956&auth_key=1760028956-0-0-145c71ac50be31dd68148632c0c0ba2e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4389f31b4dcd8d459f6911114ae20e22~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028963&auth_key=1760028963-0-0-c9d87fb3e634a87962b4618585f022a9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2b8cf70ddb1e1e7b79f7df242d17ad78~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028969&auth_key=1760028969-0-0-14a2c17c4315cdf94d823c18c1f1d939&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8a0825a515f273f6a03e93fe4916ca1a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028976&auth_key=1760028976-0-0-b49b263823ff8fba295c16a51067a88a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-457d8203f475f19b472783edf2c2c074~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028982&auth_key=1760028982-0-0-3b734765e3d73d1f9b2f856fb57f9d84&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-541eee299be9eb985b90f72a6bb9cf18~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028989&auth_key=1760028989-0-0-28eb30954b69b16ecd3800086423b88e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-06/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-06/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-06/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-247a0b8c6ce3d0ad2feaa6dfd33a5440~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028995&auth_key=1760028995-0-0-f18537dd2b7a72360c5bfd86771e16e6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-10-06  Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level   Preference Optimization
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-06/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-4cb31ae91ffa917d13a3b2f3319f48e2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028160&auth_key=1760028160-0-0-251f60f2f8ecbe87122d64049ef1f9bc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-10-06  Octax Accelerated CHIP-8 Arcade Environments for Reinforcement Learning   in JAX
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
