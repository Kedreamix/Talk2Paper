<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-04-25  GreenMind A Next-Generation Vietnamese Large Language Model for   Structured and Logical Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-480c930456ca8586b28e7967335f3609.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-04-28
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    37 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-25-更新"><a href="#2025-04-25-更新" class="headerlink" title="2025-04-25 更新"></a>2025-04-25 更新</h1><h2 id="GreenMind-A-Next-Generation-Vietnamese-Large-Language-Model-for-Structured-and-Logical-Reasoning"><a href="#GreenMind-A-Next-Generation-Vietnamese-Large-Language-Model-for-Structured-and-Logical-Reasoning" class="headerlink" title="GreenMind: A Next-Generation Vietnamese Large Language Model for   Structured and Logical Reasoning"></a>GreenMind: A Next-Generation Vietnamese Large Language Model for   Structured and Logical Reasoning</h2><p><strong>Authors:Luu Quy Tung, Hoang Quoc Viet, Vo Trong Thu</strong></p>
<p>Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that require intermediate reasoning steps prior to generating a final answer. In this paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model inspired by the finetuning strategy based on Group Relative Policy Optimization. We also leverage a high-quality Vietnamese synthesized reasoning dataset and design two reward functions to tackle the main limitations of this technique: (i) language mixing, where we explicitly detect the presence of biased language characters during the process of sampling tokens, and (ii) we leverage Sentence Transformer-based models to ensure that the generated reasoning content maintains factual correctness and does not distort the final output. Experimental results on the Vietnamese dataset from the VLSP 2023 Challenge demonstrate that our model outperforms prior works and enhances linguistic consistency in its responses. Furthermore, we extend our evaluation to SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of our reasoning method compared to few-shot prompting techniques. </p>
<blockquote>
<p>链式思维（CoT）是一种强大的方法，用于解决大型语言模型（LLM）任务，这些任务在生成最终答案之前需要中间推理步骤。在本文中，我们介绍了GreenMind-Medium-14B-R1，这是一个基于越南语和基于群体相对策略优化微调策略的推理模型。我们还利用高质量的越南语合成推理数据集，并设计两种奖励函数来解决这项技术的两个主要局限性：（i）语言混合问题，即在采样标记过程中明确检测是否存在有偏见的语言字符；（ii）我们利用基于句子转换器的模型确保生成的推理内容保持事实正确性，并不歪曲最终输出。在VLSP 2023挑战赛提供的越南数据集上的实验结果表明，我们的模型优于先前的工作，并提高了响应的语言一致性。此外，我们将评估扩展到SeaExam多语言选择题数据集，以展示我们的推理方法与少样本提示技术相比的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16832v1">PDF</a> </p>
<p><strong>Summary</strong><br>大模型在面对需要中间推理步骤的任务时面临挑战，如语言混合等问题。该研究提出了一个越南推理模型GreenMind-Medium-14B-R1，它结合了集团相对政策优化策略进行微调。同时利用高质量越南合成推理数据集和设计了两个奖励函数来解决这些问题。实验结果表明，该模型在越南数据集上表现优于先前的工作，增强了语言一致性，并在多语言选择题数据集SeaExam上的评估中也验证了其推理方法的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GreenMind-Medium-14B-R1是一个用于解决大型语言模型（LLM）任务的越南推理模型。</li>
<li>该模型基于集团相对政策优化策略进行微调。</li>
<li>利用了高质量越南合成推理数据集进行训练。</li>
<li>设计了两个奖励函数来解决语言混合问题，确保生成的推理内容保持事实正确性并不扭曲最终输出。</li>
<li>在越南数据集VLSP 2023挑战上的实验结果表明，该模型表现优于先前的工作并增强了语言一致性。</li>
<li>该模型在多语言选择题数据集SeaExam上的评估验证了其推理方法的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16832">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-120fe8321b6ca7f69943f097207e9573.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91a207c19aeb0f019583136ae6f11b78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d8be04f2bedb17ef2ecac18a332b022.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-683e0f869ff59f9ba7110afbb57bbe00.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="PIN-WM-Learning-Physics-INformed-World-Models-for-Non-Prehensile-Manipulation"><a href="#PIN-WM-Learning-Physics-INformed-World-Models-for-Non-Prehensile-Manipulation" class="headerlink" title="PIN-WM: Learning Physics-INformed World Models for Non-Prehensile   Manipulation"></a>PIN-WM: Learning Physics-INformed World Models for Non-Prehensile   Manipulation</h2><p><strong>Authors:Wenxuan Li, Hang Zhao, Zhiyuan Yu, Yu Du, Qin Zou, Ruizhen Hu, Kai Xu</strong></p>
<p>While non-prehensile manipulation (e.g., controlled pushing&#x2F;poking) constitutes a foundational robotic skill, its learning remains challenging due to the high sensitivity to complex physical interactions involving friction and restitution. To achieve robust policy learning and generalization, we opt to learn a world model of the 3D rigid body dynamics involved in non-prehensile manipulations and use it for model-based reinforcement learning. We propose PIN-WM, a Physics-INformed World Model that enables efficient end-to-end identification of a 3D rigid body dynamical system from visual observations. Adopting differentiable physics simulation, PIN-WM can be learned with only few-shot and task-agnostic physical interaction trajectories. Further, PIN-WM is learned with observational loss induced by Gaussian Splatting without needing state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM into a group of Digital Cousins via physics-aware randomizations which perturb physics and rendering parameters to generate diverse and meaningful variations of the PIN-WM. Extensive evaluations on both simulation and real-world tests demonstrate that PIN-WM, enhanced with physics-aware digital cousins, facilitates learning robust non-prehensile manipulation skills with Sim2Real transfer, surpassing the Real2Sim2Real state-of-the-arts. </p>
<blockquote>
<p>非握持操作（例如，受控推动&#x2F;戳刺）构成了一项基本的机器人技能，但其学习仍然具有挑战性，因为其对涉及摩擦和恢复力的复杂物理交互的高度敏感性。为了实现稳健的策略学习和泛化，我们选择学习涉及非握持操作中的3D刚体动力学的世界模型，并将其用于基于模型增强学习。我们提出PIN-WM，这是一个Physics-INformed世界模型，能够高效地从视觉观察中端到端地识别出用于非握持操作的3D刚体动力学系统。通过采用可微分物理模拟，PIN-WM仅通过少量和任务无关的物理交互轨迹就可以学习。此外，PIN-WM通过高斯平铺产生的观测损失进行学习，无需状态估计。为了弥仿真与现实之间的差距，我们通过物理感知随机化将学习到的PIN-WM转化为一组数字分身，通过扰动物理和渲染参数来生成PIN-WM的多样化和有意义的变体。在仿真和真实世界测试中的广泛评估表明，增强以物理感知的数字分身后，PIN-WM能够借助Sim2Real转换学习稳健的非握持操作技能，超越了最新的Real2Sim2Real技术水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16693v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>非握持式操作（如控制推动&#x2F;戳刺）是机器人技术的基本技能之一，但其学习仍然面临挑战，因为它涉及到摩擦和恢复等复杂物理交互的高度敏感性。为了实现稳健的策略学习和泛化，我们学习了一个非握持操作所涉及的三维刚体动力学世界模型，并将其用于基于模型的强化学习。我们提出了PIN-WM，这是一种基于物理信息的世界模型，能够高效地从视觉观察中端到端地识别三维刚体动力学系统。通过采用可微分物理仿真，PIN-WM仅通过少量任务无关的实物交互轨迹即可学习。此外，PIN-WM通过高斯斑点（Gaussian Splatting）产生的观测损失进行学习，无需进行状态估计。为了缩短仿真与真实世界之间的差距，我们将学习到的PIN-WM转化为一系列物理感知随机化的数字同胞。在仿真和真实世界测试中的广泛评估表明，借助物理感知的数字同胞增强的PIN-WM，能学习到具有仿真到真实世界迁移能力的稳健的非握持式操作技能，超越了当前最先进的状态技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>非握持式操作是机器人技术中的重要基础技能，但对复杂物理交互的敏感性使其学习具有挑战性。</li>
<li>提出了PIN-WM模型，一个基于物理信息的世界模型，用于高效识别三维刚体动力学系统。</li>
<li>通过采用可微分物理仿真和仅使用少量任务无关的实物交互轨迹，PIN-WM能够实现学习。</li>
<li>PIN-WM通过高斯斑点产生的观测损失进行学习，无需进行状态估计。</li>
<li>为了缩短仿真与真实世界之间的差距，将PIN-WM转化为一系列物理感知随机化的数字同胞。</li>
<li>通过广泛评估证明，PIN-WM在仿真和真实世界测试中均表现出卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16693">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6c6bac654c6fe6374ea09970c5f0ce40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-650dc498b4b6715cc538deb1c022f2e5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Less-is-More-Enhancing-Structured-Multi-Agent-Reasoning-via-Quality-Guided-Distillation"><a href="#Less-is-More-Enhancing-Structured-Multi-Agent-Reasoning-via-Quality-Guided-Distillation" class="headerlink" title="Less is More: Enhancing Structured Multi-Agent Reasoning via   Quality-Guided Distillation"></a>Less is More: Enhancing Structured Multi-Agent Reasoning via   Quality-Guided Distillation</h2><p><strong>Authors:Jiahao Yuan, Xingzhe Sun, Xing Yu, Jingwen Wang, Dehui Du, Zhiqing Cui, Zixiang Di</strong></p>
<p>The XLLM@ACL2025 Shared Task-III formulates a low-resource structural reasoning task that challenges LLMs to generate interpretable, step-by-step rationales with minimal labeled data. We present Less is More, the third-place winning approach in the XLLM@ACL2025 Shared Task-III, which focuses on structured reasoning from only 24 labeled examples. Our approach leverages a multi-agent framework with reverse-prompt induction, retrieval-augmented reasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to distill high-quality supervision across three subtasks: question parsing, CoT parsing, and step-level verification. All modules are fine-tuned from Meta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure validation with reward filtering across few-shot and zero-shot prompts, our pipeline consistently improves structure reasoning quality. These results underscore the value of controllable data distillation in enhancing structured inference under low-resource constraints. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Jiahao-Yuan/Less-is-More">https://github.com/Jiahao-Yuan/Less-is-More</a>. </p>
<blockquote>
<p>XLLM@ACL2025共享任务III建立了一个低资源结构推理任务，该任务挑战大型语言模型在少量标注数据的情况下生成可解释的、逐步的合理性解释。我们提出了”更少即是更多”的方法，这是XLLM@ACL2025共享任务III的第三名获奖方案，重点是从仅有的24个标注样本中进行结构化推理。我们的方法利用多智能体框架进行逆向提示归纳，通过GPT-4o增强检索推理合成，以及两阶段奖励引导过滤，以提炼出三个子任务的高质量监督信息：问题解析、认知轨迹解析和步骤级验证。所有模块都在统一的LoRA+设置下，以Meta-Llama-3-8B-Instruct为基础进行微调。通过结合少数样本和零样本提示的结构验证与奖励过滤，我们的管道在结构推理质量上持续提高。这些结果突显了在低资源约束下可控数据蒸馏在增强结构化推断中的价值。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/Jiahao-Yuan/Less-is-More%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Jiahao-Yuan/Less-is-More找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16408v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了在XLLM@ACL2025共享任务III中，一种名为“Less is More”的第三名胜出策略。该策略在仅有24个标注样本的情况下，运用多代理框架、反向提示诱导、检索增强推理合成和双重奖励引导过滤等技术，实现结构化推理。通过精细调整各个模块并统一于LoRA+设置下，结合结构验证和奖励过滤，该策略在少量样本和零样本提示下不断提高结构推理质量，突显可控数据蒸馏在增强低资源约束下的结构化推断的价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>XLLM@ACL2025 Shared Task-III提出了一个低资源结构推理任务，挑战LLMs在最小标注数据下生成逐步的、可解释的理由。</li>
<li>“Less is More”策略仅使用24个标注样本，强调结构化推理。</li>
<li>该策略采用多代理框架，结合反向提示诱导、检索增强推理合成和双重奖励引导过滤技术。</li>
<li>使用Meta-Llama-3-8B-Instruct对各个模块进行微调，并在统一LoRA+设置下运作。</li>
<li>通过结构验证和奖励过滤，该策略在少量样本和零样本提示下提高结构推理质量。</li>
<li>该策略的代码可在<a target="_blank" rel="noopener" href="https://github.com/Jiahao-Yuan/Less-is-More">https://github.com/Jiahao-Yuan/Less-is-More</a>处获取。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16408">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e8645278a22cf2cf6e3f6962e9e256bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d18922a55dba8e7096aa4eaa35157bd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c1266e302aa68a292d589d7247c120c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76c8df71fc8d5131af3b83050508ac25.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-712f7c0ec721062da8b311031b9b6f25.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Detecting-Actionable-Requests-and-Offers-on-Social-Media-During-Crises-Using-LLMs"><a href="#Detecting-Actionable-Requests-and-Offers-on-Social-Media-During-Crises-Using-LLMs" class="headerlink" title="Detecting Actionable Requests and Offers on Social Media During Crises   Using LLMs"></a>Detecting Actionable Requests and Offers on Social Media During Crises   Using LLMs</h2><p><strong>Authors:Ahmed El Fekih Zguir, Ferda Ofli, Muhammad Imran</strong></p>
<p>Natural disasters often result in a surge of social media activity, including requests for assistance, offers of help, sentiments, and general updates. To enable humanitarian organizations to respond more efficiently, we propose a fine-grained hierarchical taxonomy to systematically organize crisis-related information about requests and offers into three critical dimensions: supplies, emergency personnel, and actions. Leveraging the capabilities of Large Language Models (LLMs), we introduce Query-Specific Few-shot Learning (QSF Learning) that retrieves class-specific labeled examples from an embedding database to enhance the model’s performance in detecting and classifying posts. Beyond classification, we assess the actionability of messages to prioritize posts requiring immediate attention. Extensive experiments demonstrate that our approach outperforms baseline prompting strategies, effectively identifying and prioritizing actionable requests and offers. </p>
<blockquote>
<p>自然灾害往往导致社交媒体活动激增，包括求助请求、帮助提供、情感和一般更新。为了使人道主义组织能够更有效地应对，我们提出了一种精细的层次分类体系，将关于请求和提供的危机相关信息系统地组织为三个关键维度：物资、紧急人员和行动。我们借助大型语言模型（LLM）的能力，引入了查询特定少样本学习（QSF学习），从嵌入数据库中检索类别特定的标记示例，以提高模型检测和分类帖子的性能。除了分类之外，我们还评估了消息的可操作性，以优先处理需要立即关注的帖子。大量实验表明，我们的方法优于基线提示策略，能够更有效地识别和优先处理可操作性的请求和提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16144v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于精细粒度层次分类的危机相关信息组织方法，将救援需求和援助信息分为供应、紧急人员和行动三个关键维度。借助大型语言模型（LLM）的能力，引入查询特定少样本学习（QSF Learning）技术，从嵌入数据库中检索特定类别的标签样本，以提高模型对帖子进行检测和分类的性能。同时评估信息的可操作性，以优先处理需要立即关注的帖子。实验表明，该方法优于基线提示策略，能有效识别和优先处理可操作的要求和援助信息。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自然灾害会导致社交媒体活动激增，包括救援请求、援助提供、情感和一般更新等信息。</li>
<li>提出了一种基于精细粒度层次的危机信息分类方法，将相关信息分为供应、紧急人员和行动三个维度。</li>
<li>利用大型语言模型（LLM）的能力进行危机信息的分类和处理。</li>
<li>引入查询特定少样本学习（QSF Learning）以提高模型在检测和分类帖子方面的性能。</li>
<li>除了分类外，还评估了信息的可操作性，以优先处理需要立即关注的消息。</li>
<li>实验表明，该方法在识别和优先处理可操作要求和援助信息方面优于基线提示策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16144">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-723fae82311b714d69d1ce08b69265af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c86efc2b755aa63c8a55f2b90d6d2fc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5fb38280df04f93bc9c8fb71ec25502.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CAPO-Cost-Aware-Prompt-Optimization"><a href="#CAPO-Cost-Aware-Prompt-Optimization" class="headerlink" title="CAPO: Cost-Aware Prompt Optimization"></a>CAPO: Cost-Aware Prompt Optimization</h2><p><strong>Authors:Tom Zehle, Moritz Schlager, Timo Heiß, Matthias Feurer</strong></p>
<p>Large language models (LLMs) have revolutionized natural language processing by solving a wide range of tasks simply guided by a prompt. Yet their performance is highly sensitive to prompt formulation. While automated prompt optimization addresses this challenge by finding optimal prompts, current methods require a substantial number of LLM calls and input tokens, making prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt Optimization), an algorithm that enhances prompt optimization efficiency by integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as operators, incorporating racing to save evaluations and multi-objective optimization to balance performance with prompt length. It jointly optimizes instructions and few-shot examples while leveraging task descriptions for improved robustness. Our extensive experiments across diverse datasets and LLMs demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization methods in 11&#x2F;15 cases with improvements up to 21%p. Our algorithm achieves better performances already with smaller budgets, saves evaluations through racing, and decreases average prompt length via a length penalty, making it both cost-efficient and cost-aware. Even without few-shot examples, CAPO outperforms its competitors and generally remains robust to initial prompts. CAPO represents an important step toward making prompt optimization more powerful and accessible by improving cost-efficiency. </p>
<blockquote>
<p>大型语言模型（LLM）通过简单的提示指导解决了多种任务，从而彻底改变了自然语言处理的格局。然而，它们的性能对提示的构思非常敏感。虽然自动提示优化可以通过找到最佳提示来解决这一挑战，但当前的方法需要大量的LLM调用和输入令牌，这使得提示优化成本高昂。我们引入了CAPO（基于成本的提示优化），这是一种通过集成AutoML技术提高提示优化效率的算法。CAPO是一种进化方法，以LLM作为操作员，结合比赛来节省评估和多目标优化来平衡性能和提示长度。它联合优化指令和少量示例，并利用任务描述来提高稳健性。我们在多个数据集和LLM上进行的广泛实验表明，在15个案例中，CAPO在11个案例中的性能优于最先进的离散提示优化方法，改进幅度高达21%。我们的算法在较小的预算下就达到了更好的性能，通过比赛节省了评估工作，并通过长度惩罚减少了平均提示长度，这使得它既经济又注重成本效益。即使没有少量示例，CAPO也能超越竞争对手，并且对初始提示保持稳健。CAPO代表着通过提高成本效益，朝着使提示优化更具力量和可访问性的重要一步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16005v2">PDF</a> Submitted to AutoML 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）通过提示引导解决多种任务，但性能对提示制定非常敏感。当前自动化提示优化方法需要大量LLM调用和输入令牌，使得提示优化成本高昂。我们引入CAPO（成本感知提示优化），通过集成AutoML技术提高提示优化效率。CAPO采用进化方法，以LLM作为操作员，融入竞赛以节省评估和多目标优化以平衡性能与提示长度。它联合优化指令和少量示例，并利用任务描述提高稳健性。实验表明，CAPO在多数情况下优于最新离散提示优化方法，并在预算较小的情况下实现更好的性能，通过竞赛节省评估，并通过长度惩罚减少平均提示长度，既节约成本又高效。即使不依赖少量示例，CAPO也能稳健应对初始提示。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）通过提示引导完成多种任务，但提示制定对其性能影响显著。</li>
<li>当前自动化提示优化方法成本高昂，需要改进效率。</li>
<li>CAPO算法通过集成AutoML技术提高提示优化效率，采用进化方法并结合竞赛以节省评估和多目标优化。</li>
<li>CAPO联合优化指令和少量示例，利用任务描述增强稳健性。</li>
<li>实验表明，CAPO在多数情况下优于其他方法，并在预算有限的情况下实现更好的性能。</li>
<li>CAPO通过节省评估、减少平均提示长度等方式降低成本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16005">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-96d2b66d95ef53b3c1effb07481879e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b116fd3254b73260dad8b5aba0aa636b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fba975ea5ea648e831c5bf75967937cd.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Textual-Reasoning-for-Few-shot-Temporal-Action-Localization"><a href="#Chain-of-Thought-Textual-Reasoning-for-Few-shot-Temporal-Action-Localization" class="headerlink" title="Chain-of-Thought Textual Reasoning for Few-shot Temporal Action   Localization"></a>Chain-of-Thought Textual Reasoning for Few-shot Temporal Action   Localization</h2><p><strong>Authors:Hongwei Ji, Wulian Yun, Mengshi Qi, Huadong Ma</strong></p>
<p>Traditional temporal action localization (TAL) methods rely on large amounts of detailed annotated data, whereas few-shot TAL reduces this dependence by using only a few training samples to identify unseen action categories. However, existing few-shot TAL methods typically focus solely on video-level information, neglecting textual information, which can provide valuable semantic support for the localization task. Therefore, we propose a new few-shot temporal action localization method by Chain-of-Thought textual reasoning to improve localization performance. Specifically, we design a novel few-shot learning framework that leverages textual semantic information to enhance the model’s ability to capture action commonalities and variations, which includes a semantic-aware text-visual alignment module designed to align the query and support videos at different levels. Meanwhile, to better express the temporal dependencies and causal relationships between actions at the textual level to assist action localization, we design a Chain of Thought (CoT)-like reasoning method that progressively guides the Vision Language Model (VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for videos. The generated texts can capture more variance of action than visual features. We conduct extensive experiments on the publicly available ActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named Human-related Anomaly Localization and explore the application of the TAL task in human anomaly detection. The experimental results demonstrate that our proposed method significantly outperforms existing methods in single-instance and multi-instance scenarios. We will release our code, data and benchmark. </p>
<blockquote>
<p>传统的时间动作定位（TAL）方法依赖于大量的详细标注数据，而少样本TAL通过仅使用少量的训练样本来减少未见动作类别的识别依赖。然而，现有的少样本TAL方法通常只关注视频层面的信息，忽略了文本信息，这些文本信息可以为定位任务提供有价值的语义支持。因此，我们通过Chain-of-Thought的文本推理提出了一种新的少样本时间动作定位方法，以提高定位性能。具体来说，我们设计了一种新颖的基于文本语义信息的少样本学习框架，以提高模型捕捉动作共性和变化的能力。该框架包括一个语义感知的文本视觉对齐模块，旨在在不同层次上对齐查询和支持视频。同时，为了更好地表达文本层面动作的时空依赖和因果关系，辅助动作定位，我们设计了一种类似Chain of Thought（CoT）的推理方法，逐步引导视觉语言模型（VLM）和大型语言模型（LLM）生成针对视频的CoT文本描述。生成的文本可以捕获比视觉特征更多的动作变化。我们在公开可用的ActivityNet1.3和THUMOS14数据集上进行了大量实验。我们还引入了名为Human-related Anomaly Localization的新数据集，并探讨了TAL任务在人类异常检测中的应用。实验结果表明，我们提出的方法在单实例和多实例场景中均显著优于现有方法。我们将公开我们的代码、数据和基准测试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13460v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于Chain-of-Thought文本推理的新的少样本时序动作定位方法。该方法利用文本语义信息，设计了一个语义感知的文本视觉对齐模块，并在查询和支持视频的不同层次上进行对齐。同时，为了更好地在文本层面表达动作的时空依赖和因果关系，设计了一种类似Chain of Thought（CoT）的推理方法，逐步引导视觉语言模型和大语言模型生成视频的CoT文本描述。实验结果表明，该方法在单实例和多实例场景中显著优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新的少样本时序动作定位方法，结合Chain-of-Thought文本推理，降低对大量详细标注数据的依赖。</li>
<li>设计了语义感知的文本视觉对齐模块，利用文本语义信息提高模型捕捉动作共性和变化的能力。</li>
<li>引入了类似Chain of Thought（CoT）的推理方法，更好地在文本层面表达动作的时空依赖和因果关系。</li>
<li>生成的文本描述能捕捉比视觉特征更多的动作变化。</li>
<li>在公开数据集ActivityNet1.3和THUMOS14上进行了广泛实验，验证了所提方法的有效性。</li>
<li>构建了名为Human-related Anomaly Localization的新数据集，探索了时序动作定位在人体异常检测中的应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13460">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c82b1047594130439a760a575b62acce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-531c4a7812ecaeccde9b4e989a71d861.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7c213dbbcc1e0229555b467cf4aa97d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="7B-Fully-Open-Source-Moxin-LLM-–-From-Pretraining-to-GRPO-based-Reinforcement-Learning-Enhancement"><a href="#7B-Fully-Open-Source-Moxin-LLM-–-From-Pretraining-to-GRPO-based-Reinforcement-Learning-Enhancement" class="headerlink" title="7B Fully Open Source Moxin-LLM – From Pretraining to GRPO-based   Reinforcement Learning Enhancement"></a>7B Fully Open Source Moxin-LLM – From Pretraining to GRPO-based   Reinforcement Learning Enhancement</h2><p><strong>Authors:Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Weiyan Shi, Xingchen Xu, Yu Huang, Wei Jiang, Wei Wang, Yue Chen, Yong He, Yanzhi Wang</strong></p>
<p>Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Although open-source LLMs present unprecedented opportunities for innovation and research, the commercialization of LLMs has raised concerns about transparency, reproducibility, and safety. Many open-source LLMs fail to meet fundamental transparency requirements by withholding essential components like training code and data, which may hinder further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed, adhering to principles of open science, open source, open data, and open access. We release the pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints, aiming to make continuous commitments to fully open-source LLMs. After pre-training and obtaining the base model, we finetune the Moxin Base model with SOTA post-training framework and instruction data to obtain Moxin Instruct model. To improve the reasoning capability, we further finetune our Instruct model with chain-of-thought data distilled from DeepSeek R1, and then use Group Relative Policy Optimization (GRPO), an efficient and effective reinforcement learning algorithm following DeepSeek R1, to finetune our model, leading to the Moxin Reasoning model. Experiments show that our models achieve superior performance in various evaluations such as zero-shot evaluation, few-shot evaluation, and CoT evaluation. </p>
<blockquote>
<p>最近，大型语言模型（LLM）经历了重大转变，其受欢迎程度和能力都迅速上升。引领这一变革的是像GPT-4和GPT-o1这样的专有大型语言模型，由于它们出色的性能和多功能性，它们在人工智能领域引起了广泛关注。同时，开源的大型语言模型，如LLaMA，由于对模型进行定制和跨不同应用程序部署的便利性，也为大型语言模型日益普及做出了巨大贡献。然而，大型语言模型的商业化引发了关于透明度、可复制性和安全的担忧。许多开源的大型语言模型未能满足基本的透明度要求，因为他们隐瞒了关键的组成部分，如训练代码和数据，这可能阻碍大型语言模型的进一步创新。为了缓解这一问题，我们推出了完全符合公开科学原则的大型语言模型——墨心7B。该模型采用开源、开放数据、开放访问的原则。我们公开了预训练代码和配置、训练和微调数据集以及中间和最终检查点，致力于完全开源的大型语言模型。在预训练和获取基础模型之后，我们使用最新的后训练框架和指令数据对墨心基础模型进行微调，以获取墨心指令模型。为了提高推理能力，我们进一步使用来自DeepSeek R1的蒸馏思维链数据对指令模型进行微调，并采用跟随DeepSeek R1的高效有效强化学习算法——组相对策略优化（GRPO）来微调我们的模型，从而得到墨心推理模型。实验表明，我们的模型在零样本评估、少样本评估和思维链评估等各个方面都取得了卓越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06845v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）领域正经历一次重大变革，以GPT-4和GPT-o1为代表的专有LLM以及LLaMA等开源LLM的兴起推动了这一变革。然而，商业化的同时，也存在透明度、可重复性和安全性等方面的担忧。为此，我们推出了遵循公开科学、开源、开放数据和开放访问原则的全新开源LLM——Moxin 7B。该模型公开了预训练代码和配置、训练和微调数据集以及中间和最终检查点。通过一系列的训练和优化过程，Moxin模型表现出卓越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）正在经历重大变革，专有和开源LLM的兴起推动了这一变革。</li>
<li>商业化的大型语言模型引发了透明度、可重复性和安全性的担忧。</li>
<li>Moxin 7B是一个全新的开源LLM，遵循公开科学、开源、开放数据和开放访问原则。</li>
<li>Moxin 7B公开了预训练代码和配置、训练和微调数据集。</li>
<li>Moxin模型通过一系列的训练和优化过程，包括使用SOTA后训练框架、指令数据、思维链数据和Group Relative Policy Optimization（GRPO）强化学习算法进行微调。</li>
<li>实验表明，Moxin模型在零样本、少样本和思维链评估中表现出卓越性能。</li>
<li>开源大型语言模型有助于推动创新和研究的进步。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06845">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-480c930456ca8586b28e7967335f3609.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b06a6046dedb172c22ce330e93623dc8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Synthetic-Lyrics-Detection-Across-Languages-and-Genres"><a href="#Synthetic-Lyrics-Detection-Across-Languages-and-Genres" class="headerlink" title="Synthetic Lyrics Detection Across Languages and Genres"></a>Synthetic Lyrics Detection Across Languages and Genres</h2><p><strong>Authors:Yanis Labrak, Markus Frohmann, Gabriel Meseguer-Brocal, Elena V. Epure</strong></p>
<p>In recent years, the use of large language models (LLMs) to generate music content, particularly lyrics, has gained in popularity. These advances provide valuable tools for artists and enhance their creative processes, but they also raise concerns about copyright violations, consumer satisfaction, and content spamming. Previous research has explored content detection in various domains. However, no work has focused on the text modality, lyrics, in music. To address this gap, we curated a diverse dataset of real and synthetic lyrics from multiple languages, music genres, and artists. The generation pipeline was validated using both humans and automated methods. We performed a thorough evaluation of existing synthetic text detection approaches on lyrics, a previously unexplored data type. We also investigated methods to adapt the best-performing features to lyrics through unsupervised domain adaptation. Following both music and industrial constraints, we examined how well these approaches generalize across languages, scale with data availability, handle multilingual language content, and perform on novel genres in few-shot settings. Our findings show promising results that could inform policy decisions around AI-generated music and enhance transparency for users. </p>
<blockquote>
<p>近年来，使用大型语言模型（LLM）生成音乐内容，特别是歌词，越来越受欢迎。这些进步为艺术家提供了有价值的工具，增强了他们的创作过程，但也引发了关于版权侵犯、消费者满意度和内容垃圾广告的担忧。之前的研究已经在各个领域探索了内容检测。然而，没有研究专注于音乐中的文本模式——歌词。为了弥补这一空白，我们从多种语言、音乐流派和艺术家中精心策划了一个真实和合成歌词的多样化数据集。生成管道通过人工和自动化方法进行了验证。我们对歌词的现有合成文本检测方法进行了全面评估，这是一种以前未被探索过的数据类型。我们还研究了通过无监督域适应调整最佳性能特征以适应歌词的方法。遵循音乐和工业约束，我们研究了这些方法在不同语言中的泛化能力、随着数据可用性而扩展的能力、处理多语言环境内容和在少数情况下的新流派表现能力。我们的研究结果呈现出可喜的结果，可以为关于人工智能生成音乐的政策决策提供信息，并增强用户的透明度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.15231v3">PDF</a> Published in the workshop TrustNLP @ NAACL</p>
<p><strong>摘要</strong><br>大型语言模型在音乐内容生成中的应用日益普及，特别是在生成歌词方面。这些进展为艺术家提供了有价值的工具并促进了他们的创造性过程，但同时也引发了关于版权侵犯、消费者满意度和内容滥用的担忧。以前的研究已经探索了不同领域的内容检测，但尚未关注音乐中的文本模式——歌词。为了填补这一空白，我们编纂了一个包含多种语言、音乐流派和艺术家创作的真实和合成歌词的多样化数据集。利用人类和自动化方法验证了生成管道的有效性。我们对现有合成文本检测方法进行全面评估，针对歌词这一以前未被探索的数据类型，我们还调查了通过无监督域适应调整最佳性能特征的方法。遵循音乐和工业约束，我们研究了这些方法在多语言、数据可用性、处理多语言内容和少数镜头设置中表现新流派方面的泛化能力。我们的研究结果为人工智能生成的音乐的政策决策提供了信息，并提高了对用户的透明度。</p>
<p><strong>要点</strong></p>
<ol>
<li>大型语言模型在音乐内容生成中的应用正在普及，特别是在生成歌词方面。</li>
<li>歌词生成带来了版权侵犯等担忧。</li>
<li>目前尚未有针对歌词的文本合成检测研究。</li>
<li>编纂了一个包含真实和合成歌词的多样化数据集，涵盖多种语言和音乐流派。</li>
<li>对现有合成文本检测方法进行了歌词上的全面评估。</li>
<li>探讨了如何通过无监督域适应调整最佳性能特征的方法。</li>
<li>研究发现表明，对于人工智能生成的音乐的政策决策和用户透明度有重要参考价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.15231">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6fe3fd6026add57d3e1c612bd5075438.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec2c466c35d546e3e826f88641b2b6fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c919f86581e5fce6e0c5fdfcce47b56.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27a26204eb36c3479255134958020fb4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CLAP-Isolating-Content-from-Style-through-Contrastive-Learning-with-Augmented-Prompts"><a href="#CLAP-Isolating-Content-from-Style-through-Contrastive-Learning-with-Augmented-Prompts" class="headerlink" title="CLAP: Isolating Content from Style through Contrastive Learning with   Augmented Prompts"></a>CLAP: Isolating Content from Style through Contrastive Learning with   Augmented Prompts</h2><p><strong>Authors:Yichao Cai, Yuhang Liu, Zhen Zhang, Javen Qinfeng Shi</strong></p>
<p>Contrastive vision-language models, such as CLIP, have garnered considerable attention for various downstream tasks, mainly due to the remarkable ability of the learned features for generalization. However, the features they learned often blend content and style information, which somewhat limits their generalization capabilities under distribution shifts. To address this limitation, we adopt a causal generative perspective for multimodal data and propose contrastive learning with data augmentation to disentangle content features from the original representations. To achieve this, we begin with exploring image augmentation techniques and develop a method to seamlessly integrate them into pre-trained CLIP-like models to extract pure content features. Taking a step further, recognizing the inherent semantic richness and logical structure of text data, we explore the use of text augmentation to isolate latent content from style features. This enables CLIP-like model’s encoders to concentrate on latent content information, refining the learned representations by pre-trained CLIP-like models. Our extensive experiments across diverse datasets demonstrate significant improvements in zero-shot and few-shot classification tasks, alongside enhanced robustness to various perturbations. These results underscore the effectiveness of our proposed methods in refining vision-language representations and advancing the state-of-the-art in multimodal learning. </p>
<blockquote>
<p>对比视觉语言模型，如CLIP，在各种下游任务中受到了极大的关注，这主要归功于其学习特征的出色泛化能力。然而，它们学习的特征往往会混合内容和风格信息，这在某种程度上限制了它们在分布转移下的泛化能力。为了解决这一局限性，我们采用因果生成视角进行多模态数据研究，并提出结合数据增强的对比学习来从原始表示中分离内容特征。为此，我们首先探索图像增强技术，并开发了一种方法，将其无缝集成到预训练的CLIP类模型中，以提取纯内容特征。更进一步，我们认识到文本数据固有的语义丰富性和逻辑结构，探索使用文本增强来分离潜在内容与风格特征。这可以让CLIP类模型的编码器专注于潜在内容信息，通过预训练的CLIP类模型优化学习表示。我们在多个数据集上的大量实验表明，在零样本和少样本分类任务中取得了显著的改进，对各种扰动的鲁棒性也有所增强。这些结果突显了我们提出的方法在优化视觉语言表示和多模态学习方面的先进性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.16445v6">PDF</a> Accepted as a conference paper at ECCV 2024</p>
<p><strong>Summary</strong></p>
<p>本文探讨了对比视觉语言模型（如CLIP）在处理下游任务时的局限性，其学到的特征往往融合了内容和风格信息，限制了其在分布变化下的泛化能力。为解决这一问题，本文采用因果生成视角和多模态数据，提出使用对比学习和数据增强来分离内容特征。通过探索图像增强技术和将其无缝集成到预训练CLIP模型中，提取纯内容特征。同时，利用文本增强来隔离潜在内容与风格特征，使CLIP模型的编码器专注于潜在内容信息，从而优化预训练CLIP模型的表示。实验结果表明，该方法在零样本和少样本分类任务上取得了显著改进，并增强了对各种扰动的鲁棒性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>对比视觉语言模型（如CLIP）在下游任务中表现出色，但存在泛化能力受限的问题。</li>
<li>问题源于模型学到的特征融合了内容和风格信息。</li>
<li>采用因果生成视角和多模态数据，提出使用对比学习和数据增强来分离内容特征。</li>
<li>通过图像增强技术提取纯内容特征，并无缝集成到预训练CLIP模型中。</li>
<li>利用文本增强来隔离潜在内容与风格特征，优化CLIP模型的表示学习。</li>
<li>实验结果显示，该方法在零样本和少样本分类任务上表现更佳。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.16445">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-da491de90cd83a2e7e9a5113c3d92b5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-446e6ccc083ed653e398a47c6d8c386e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d262133074fb44db337ef4127a68261.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-25/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-25/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-25/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-4b49c639be684822e53da3e9ba6000a3.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-04-25  Pix2Next Leveraging Vision Foundation Models for RGB to NIR Image   Translation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-25/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-221bd3f73cedfd92fa5f7929a7829834.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-04-25  OptimAI Optimization from Natural Language Using LLM-Powered AI Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">16668k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
