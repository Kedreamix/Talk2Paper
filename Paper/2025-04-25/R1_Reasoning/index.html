<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-25  Tracing Thought Using Chain-of-Thought Reasoning to Identify the LLM   Behind AI-Generated Text">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-76c8df71fc8d5131af3b83050508ac25.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-25-æ›´æ–°"><a href="#2025-04-25-æ›´æ–°" class="headerlink" title="2025-04-25 æ›´æ–°"></a>2025-04-25 æ›´æ–°</h1><h2 id="Tracing-Thought-Using-Chain-of-Thought-Reasoning-to-Identify-the-LLM-Behind-AI-Generated-Text"><a href="#Tracing-Thought-Using-Chain-of-Thought-Reasoning-to-Identify-the-LLM-Behind-AI-Generated-Text" class="headerlink" title="Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM   Behind AI-Generated Text"></a>Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM   Behind AI-Generated Text</h2><p><strong>Authors:Shifali Agrahari, Sanasam Ranbir Singh</strong></p>
<p>In recent years, the detection of AI-generated text has become a critical area of research due to concerns about academic integrity, misinformation, and ethical AI deployment. This paper presents COT Fine-tuned, a novel framework for detecting AI-generated text and identifying the specific language model. responsible for generating the text. We propose a dual-task approach, where Task A involves classifying text as AI-generated or human-written, and Task B identifies the specific LLM behind the text. The key innovation of our method lies in the use of Chain-of-Thought reasoning, which enables the model to generate explanations for its predictions, enhancing transparency and interpretability. Our experiments demonstrate that COT Fine-tuned achieves high accuracy in both tasks, with strong performance in LLM identification and human-AI classification. We also show that the CoT reasoning process contributes significantly to the models effectiveness and interpretability. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œç”±äºäººä»¬æ‹…å¿ƒå­¦æœ¯è¯šä¿¡ã€è™šå‡ä¿¡æ¯å’Œäººå·¥æ™ºèƒ½éƒ¨ç½²çš„é“å¾·é—®é¢˜ï¼Œæ£€æµ‹AIç”Ÿæˆçš„æ–‡æœ¬å·²æˆä¸ºç ”ç©¶çš„å…³é”®é¢†åŸŸã€‚æœ¬æ–‡æå‡ºäº†COT Fine-tunedï¼Œä¸€ä¸ªç”¨äºæ£€æµ‹AIç”Ÿæˆæ–‡æœ¬å¹¶è¯†åˆ«ç”Ÿæˆæ–‡æœ¬çš„å…·ä½“è¯­è¨€æ¨¡å‹çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒé‡ä»»åŠ¡çš„æ–¹æ³•ï¼Œå…¶ä¸­ä»»åŠ¡Aæ¶‰åŠå°†æ–‡æœ¬åˆ†ç±»ä¸ºAIç”Ÿæˆæˆ–äººç±»æ’°å†™ï¼Œä»»åŠ¡Båˆ™è¯†åˆ«æ–‡æœ¬èƒŒåçš„ç‰¹å®šå¤§å‹è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬æ–¹æ³•çš„å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºä½¿ç”¨Chain-of-Thoughtæ¨ç†ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¸ºå…¶é¢„æµ‹ç”Ÿæˆè§£é‡Šï¼Œæé«˜é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒCOT Fine-tunedåœ¨ä¸¤é¡¹ä»»åŠ¡ä¸­éƒ½å–å¾—äº†è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œåœ¨å¤§å‹è¯­è¨€æ¨¡å‹è¯†åˆ«å’Œäººæœºåˆ†ç±»æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼ŒCoTæ¨ç†è¿‡ç¨‹å¯¹æ¨¡å‹çš„æ•ˆç‡å’Œå¯è§£é‡Šæ€§åšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16913v1">PDF</a> De-Factify 4: 4th Workshop on Multimodal Fact Checking and Hate   Speech Detection, co-located with AAAI 2025. Pennsylvania</p>
<p><strong>Summary</strong>ï¼š<br>è¿‘å¹´æ¥ï¼Œæ£€æµ‹AIç”Ÿæˆæ–‡æœ¬æˆä¸ºäº†ç ”ç©¶é¢†åŸŸçš„é‡è¦è¯¾é¢˜ï¼Œæ¶‰åŠåˆ°å­¦æœ¯è¯šä¿¡ã€è™šå‡ä¿¡æ¯å’Œä¼¦ç†é—®é¢˜ç­‰æ–¹é¢ã€‚æœ¬æ–‡æå‡ºäº†COT Fine-tunedæ¡†æ¶ï¼Œé‡‡ç”¨åŒé‡ä»»åŠ¡çš„æ–¹æ³•æ£€æµ‹AIç”Ÿæˆçš„æ–‡æœ¬å¹¶è¯†åˆ«ç”Ÿæˆæ–‡æœ¬çš„ç‰¹å®šè¯­è¨€æ¨¡å‹ã€‚ä»»åŠ¡Aæ¶‰åŠå°†æ–‡æœ¬åˆ†ç±»ä¸ºAIç”Ÿæˆæˆ–äººç±»æ’°å†™ï¼Œä»»åŠ¡Båˆ™è¯†åˆ«æ–‡æœ¬èƒŒåçš„ç‰¹å®šå¤§å‹è¯­è¨€æ¨¡å‹ã€‚å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨Chain-of-Thoughtæ¨ç†ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¸ºå…¶é¢„æµ‹ç”Ÿæˆè§£é‡Šï¼Œæé«˜é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒCOT Fine-tunedåœ¨ä¸¤é¡¹ä»»åŠ¡ä¸­éƒ½å–å¾—äº†è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹è¯†åˆ«å’Œäººæœºåˆ†ç±»æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚CoTæ¨ç†è¿‡ç¨‹å¯¹æ¨¡å‹çš„æ•ˆèƒ½å’Œå¯è§£é‡Šæ€§æœ‰æ˜¾è‘—è´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>AIç”Ÿæˆæ–‡æœ¬æ£€æµ‹å·²æˆä¸ºé‡è¦ç ”ç©¶é¢†åŸŸï¼Œæ¶‰åŠå­¦æœ¯è¯šä¿¡ã€è™šå‡ä¿¡æ¯å’Œä¼¦ç†é—®é¢˜ã€‚</li>
<li>COT Fine-tunedæ¡†æ¶é‡‡ç”¨åŒé‡ä»»åŠ¡æ–¹æ³•ï¼Œèƒ½æ£€æµ‹AIç”Ÿæˆçš„æ–‡æœ¬å¹¶è¯†åˆ«èƒŒåçš„è¯­è¨€æ¨¡å‹ã€‚</li>
<li>è¯¥æ¡†æ¶çš„å…³é”®åˆ›æ–°åœ¨äºä½¿ç”¨Chain-of-Thoughtæ¨ç†ï¼Œæé«˜æ¨¡å‹çš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒCOT Fine-tunedåœ¨äººæœºåˆ†ç±»å’Œå¤§å‹è¯­è¨€æ¨¡å‹è¯†åˆ«æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>COT Fine-tunedæ¡†æ¶åœ¨æ£€æµ‹AIç”Ÿæˆæ–‡æœ¬æ–¹é¢å…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚</li>
<li>CoTæ¨ç†è¿‡ç¨‹å¯¹æ¨¡å‹çš„æ•ˆèƒ½å’Œå¯è§£é‡Šæ€§æœ‰é‡è¦è´¡çŒ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16913">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94bc7ccd14bab0a82c086205ec93b2b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-50895ae4116888dee53a8e58bb341b57.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b349b9c39daf91fbbfe23a040063478.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb086bcb345302ca24789fcf65bd22bb.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AIMO-2-Winning-Solution-Building-State-of-the-Art-Mathematical-Reasoning-Models-with-OpenMathReasoning-dataset"><a href="#AIMO-2-Winning-Solution-Building-State-of-the-Art-Mathematical-Reasoning-Models-with-OpenMathReasoning-dataset" class="headerlink" title="AIMO-2 Winning Solution: Building State-of-the-Art Mathematical   Reasoning Models with OpenMathReasoning dataset"></a>AIMO-2 Winning Solution: Building State-of-the-Art Mathematical   Reasoning Models with OpenMathReasoning dataset</h2><p><strong>Authors:Ivan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schifferer, Wei Du, Igor Gitman</strong></p>
<p>This paper presents our winning submission to the AI Mathematical Olympiad - Progress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art mathematical reasoning models relies on three key pillars. First, we create a large-scale dataset comprising 540K unique high-quality math problems, including olympiad-level problems, and their 3.2M long-reasoning solutions. Second, we develop a novel method to integrate code execution with long reasoning models through iterative training, generation, and quality filtering, resulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we create a pipeline to train models to select the most promising solution from many candidates. We show that such generative solution selection (GenSelect) can significantly improve upon majority voting baseline. Combining these ideas, we train a series of models that achieve state-of-the-art results on mathematical reasoning benchmarks. To facilitate further research, we release our code, models, and the complete OpenMathReasoning dataset under a commercially permissive license. </p>
<blockquote>
<p>æœ¬æ–‡å‘ˆç°äº†æˆ‘ä»¬å‚åŠ äººå·¥æ™ºèƒ½æ•°å­¦å¥¥æ—åŒ¹å…‹ç«èµ›â€”â€”è¿›æ­¥å¥–ç¬¬2æœŸï¼ˆAIMO-2ï¼‰çš„è·å¥–æäº¤ä½œå“ã€‚æˆ‘ä»¬æ„å»ºæœ€å‰æ²¿æ•°å­¦æ¨ç†æ¨¡å‹çš„é…æ–¹ä¾èµ–äºä¸‰ä¸ªå…³é”®æ”¯æŸ±ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«54ä¸‡ä¸ªç‹¬ç‰¹çš„é«˜è´¨é‡æ•°å­¦é—®é¢˜ï¼ŒåŒ…æ‹¬å¥¥æ—åŒ¹å…‹çº§åˆ«çš„æ•°å­¦é—®é¢˜åŠå…¶320ä¸‡ä¸ªé•¿æœŸè§£å†³æ–¹æ¡ˆã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£è®­ç»ƒã€ç”Ÿæˆå’Œè´¨é‡æ§åˆ¶ï¼Œå°†ä»£ç æ‰§è¡Œä¸é•¿æœŸæ¨ç†æ¨¡å‹ç›¸ç»“åˆï¼Œäº§ç”Ÿäº†170ä¸‡ä¸ªé«˜è´¨é‡çš„é›†æˆå·¥å…·æ¨ç†è§£å†³æ–¹æ¡ˆã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªç®¡é“æ¥è®­ç»ƒæ¨¡å‹ï¼Œä»å¤šä¸ªå€™é€‰ç­”æ¡ˆä¸­é€‰æ‹©æœ€æœ‰å‰é€”çš„ç­”æ¡ˆã€‚æˆ‘ä»¬è¯æ˜äº†è¿™ç§ç”Ÿæˆè§£å†³æ–¹æ¡ˆé€‰æ‹©ï¼ˆGenSelectï¼‰å¯ä»¥å¤§å¤§æ”¹è¿›å¤šæ•°æŠ•ç¥¨åŸºå‡†ã€‚ç»“åˆè¿™äº›æ€æƒ³ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ç³»åˆ—åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°ç»“æœçš„æ¨¡å‹ã€‚ä¸ºäº†ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨å•†ä¸šè®¸å¯ä¸‹å‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ã€æ¨¡å‹å’Œå®Œæ•´çš„OpenMathReasoningæ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16891v1">PDF</a> Report of AIMO-2 winning submission</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å‚åŠ AIMO-2ç«èµ›çš„è·å¥–ä½œå“ã€‚è¯¥ä½œå“é€šè¿‡æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†ã€å¼€å‘æ–°å‹ä»£ç æ‰§è¡Œä¸é•¿æœŸæ¨ç†æ¨¡å‹æ•´åˆæ–¹æ³•ï¼Œä»¥åŠåˆ›å»ºæ¨¡å‹é€‰æ‹©ç®¡é“ç­‰ä¸‰å¤§æ”¯æŸ±æŠ€æœ¯ï¼Œæ„å»ºäº†æœ€å…ˆè¿›çš„æ•°å­¦æ¨ç†æ¨¡å‹ã€‚é€šè¿‡ç»“åˆè¿™äº›æŠ€æœ¯ï¼Œè®­ç»ƒå‡ºçš„æ¨¡å‹åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€ä½³æˆç»©ã€‚åŒæ—¶ï¼Œè¯¥ä½œå“å…¬å¼€äº†ä»£ç ã€æ¨¡å‹å’Œå®Œæ•´çš„OpenMathReasoningæ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªèµ¢å¾—AIMO-2ç«èµ›çš„å…ˆè¿›æ•°å­¦æ¨ç†æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹æ„å»ºä¾èµ–äºä¸‰ä¸ªä¸»è¦æ”¯æŸ±ï¼šå¤§è§„æ¨¡æ•°æ®é›†ã€æ–°å‹ä»£ç æ‰§è¡Œä¸é•¿æœŸæ¨ç†æ¨¡å‹æ•´åˆæ–¹æ³•ï¼Œä»¥åŠæ¨¡å‹é€‰æ‹©ç®¡é“ã€‚</li>
<li>æ¨¡å‹åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«54ä¸‡ä¸ªç‹¬ç‰¹çš„é«˜è´¨é‡æ•°å­¦é—®é¢˜åŠå…¶é•¿è¾¾32ä¸‡ä¸ªé•¿æœŸè§£å†³æ–¹æ¡ˆã€‚</li>
<li>å¼€å‘äº†ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œå°†ä»£ç æ‰§è¡Œä¸é•¿æœŸæ¨ç†æ¨¡å‹è¿›è¡Œè¿­ä»£è®­ç»ƒã€ç”Ÿæˆå’Œè´¨é‡æ§åˆ¶ï¼Œäº§ç”Ÿäº†é«˜è¾¾170ä¸‡ä»½é«˜è´¨é‡çš„å·¥å…·é›†æˆæ¨ç†è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ¨¡å‹é€‰æ‹©ç®¡é“é€šè¿‡ç”Ÿæˆè§£å†³æ–¹æ¡ˆé€‰æ‹©ï¼ˆGenSelectï¼‰æŠ€æœ¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜ä»ä¼—å¤šå€™é€‰è§£å†³æ–¹æ¡ˆä¸­é€‰æ‹©æœ€ä½³æ–¹æ¡ˆçš„èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€ä½³æˆç»©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16891">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-31a69f4c47d1fa28076eed12d3b7e4e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0636377f7771c6799e0d2ccf6821990.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-273be936064d872850ef226098501d1b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-030d5ff0f09abcf2a5552f6d0aa61660.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4c85b8bb7a2568b3d046104088a7693.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GreenMind-A-Next-Generation-Vietnamese-Large-Language-Model-for-Structured-and-Logical-Reasoning"><a href="#GreenMind-A-Next-Generation-Vietnamese-Large-Language-Model-for-Structured-and-Logical-Reasoning" class="headerlink" title="GreenMind: A Next-Generation Vietnamese Large Language Model for   Structured and Logical Reasoning"></a>GreenMind: A Next-Generation Vietnamese Large Language Model for   Structured and Logical Reasoning</h2><p><strong>Authors:Luu Quy Tung, Hoang Quoc Viet, Vo Trong Thu</strong></p>
<p>Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that require intermediate reasoning steps prior to generating a final answer. In this paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model inspired by the finetuning strategy based on Group Relative Policy Optimization. We also leverage a high-quality Vietnamese synthesized reasoning dataset and design two reward functions to tackle the main limitations of this technique: (i) language mixing, where we explicitly detect the presence of biased language characters during the process of sampling tokens, and (ii) we leverage Sentence Transformer-based models to ensure that the generated reasoning content maintains factual correctness and does not distort the final output. Experimental results on the Vietnamese dataset from the VLSP 2023 Challenge demonstrate that our model outperforms prior works and enhances linguistic consistency in its responses. Furthermore, we extend our evaluation to SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of our reasoning method compared to few-shot prompting techniques. </p>
<blockquote>
<p>æ€ç»´é“¾ï¼ˆCoTï¼‰æ˜¯ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»»åŠ¡çš„ç¨³å¥æ–¹æ³•ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦åœ¨ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆä¹‹å‰è¿›è¡Œä¸­é—´æ¨ç†æ­¥éª¤ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†GreenMind-Medium-14B-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªå—åŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç­–ç•¥çš„å¾®è°ƒç­–ç•¥å¯å‘çš„è¶Šå—è¯­æ¨ç†æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜åˆ©ç”¨é«˜è´¨é‡çš„è¶Šå—è¯­åˆæˆæ¨ç†æ•°æ®é›†ï¼Œå¹¶è®¾è®¡ä¸¤ä¸ªå¥–åŠ±å‡½æ•°æ¥è§£å†³è¿™é¡¹æŠ€æœ¯çš„ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šï¼ˆiï¼‰è¯­è¨€æ··åˆé—®é¢˜ï¼Œæˆ‘ä»¬åœ¨é‡‡æ ·ä»¤ç‰Œçš„è¿‡ç¨‹ä¸­æ˜¾å¼æ£€æµ‹æ˜¯å¦å­˜åœ¨åå‘çš„è¯­è¨€å­—ç¬¦ï¼›ï¼ˆiiï¼‰æˆ‘ä»¬åˆ©ç”¨åŸºäºå¥å­è½¬æ¢å™¨çš„æ¨¡å‹ç¡®ä¿ç”Ÿæˆçš„æ¨ç†å†…å®¹ä¿æŒäº‹å®æ­£ç¡®æ€§ï¼Œå¹¶ä¸æ‰­æ›²æœ€ç»ˆè¾“å‡ºã€‚åœ¨VLSP 2023æŒ‘æˆ˜èµ›æä¾›çš„è¶Šå—æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è¡¨ç°ä¸Šè¶…è¿‡äº†å…ˆå‰çš„å·¥ä½œï¼Œå¹¶åœ¨å“åº”ä¸­å¢å¼ºäº†è¯­è¨€ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†è¯„ä¼°æ‰©å±•åˆ°äº†å¤šè¯­è¨€é€‰æ‹©é¢˜æ•°æ®é›†SeaExamä¸Šï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ¨ç†æ–¹æ³•ä¸å°‘æ ·æœ¬æç¤ºæŠ€æœ¯ç›¸æ¯”çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16832v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>ç»¿æ€ç»´-ä¸­æ¨¡å‹é‡‡ç”¨é›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ç­–ç•¥è¿›è¡Œå¾®è°ƒï¼Œä»¥è§£å†³åœ¨è¶Šå—è¯­ä¸­ç”Ÿæˆæ¨¡å‹ä¸­çš„ä»»åŠ¡é“¾æ–¹æ³•é¢ä¸´çš„éš¾é¢˜ã€‚å®ƒé€šè¿‡æ”¹è¿›æ¨¡å‹çš„è®¾è®¡å’Œä¸¤ä¸ªå¥–åŠ±å‡½æ•°çš„ä¼˜åŒ–å…‹æœäº†è¯­è¨€æ··åˆé—®é¢˜ï¼Œç¡®ä¿ç”Ÿæˆçš„æ¨ç†å†…å®¹ä¿æŒäº‹å®æ­£ç¡®æ€§ä¸”ä¸ä¼šæ‰­æ›²è¾“å‡ºã€‚åœ¨è¶Šå—æ•°æ®é›†VLSP 2023æŒ‘æˆ˜èµ›å’ŒSeaExamçš„å¤šè¯­è¨€é€‰æ‹©é¢˜æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ¯”ç°æœ‰æŠ€æœ¯å’Œå°‘æ•°å‡ ç§æç¤ºæŠ€æœ¯æ›´æœ‰æ•ˆã€‚ </p>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li>GreenMind-Medium-14B-R1æ¨¡å‹æ˜¯è¶Šå—æ¨ç†æ¨¡å‹ï¼Œé‡‡ç”¨ä»»åŠ¡é“¾æ–¹æ³•å¤„ç†å¤§å‹è¯­è¨€æ¨¡å‹ä»»åŠ¡ã€‚</li>
<li>æ¨¡å‹åŸºäºé›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ç­–ç•¥è¿›è¡Œå¾®è°ƒã€‚</li>
<li>æ¨¡å‹è®¾è®¡åŒ…å«ä¸¤ä¸ªå¥–åŠ±å‡½æ•°ä»¥è§£å†³è¯­è¨€æ··åˆé—®é¢˜ï¼Œç¡®ä¿ç”Ÿæˆçš„æ¨ç†å†…å®¹ä¿æŒäº‹å®æ­£ç¡®æ€§ã€‚</li>
<li>æ¨¡å‹åœ¨è¶Šå—æ•°æ®é›†VLSP 2023æŒ‘æˆ˜èµ›ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶å±•ç¤ºå…¶è¶…è¶Šç°æœ‰æŠ€æœ¯çš„æ•ˆæœã€‚</li>
<li>æ¨¡å‹é€šè¿‡åº”ç”¨å¥å­è½¬æ¢å™¨æŠ€æœ¯ï¼Œç¡®ä¿æ¨ç†å†…å®¹ä¸æ‰­æ›²è¾“å‡ºã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16832">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-120fe8321b6ca7f69943f097207e9573.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91a207c19aeb0f019583136ae6f11b78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d8be04f2bedb17ef2ecac18a332b022.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-683e0f869ff59f9ba7110afbb57bbe00.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Process-Reward-Models-That-Think"><a href="#Process-Reward-Models-That-Think" class="headerlink" title="Process Reward Models That Think"></a>Process Reward Models That Think</h2><p><strong>Authors:Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, Lu Wang</strong></p>
<p>Step-by-step verifiers â€“ also known as process reward models (PRMs) â€“ are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers â€“ using only 1% of the process labels in PRM800K â€“ across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME â€˜24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models will be released at <a target="_blank" rel="noopener" href="https://github.com/mukhal/thinkprm">https://github.com/mukhal/thinkprm</a>. </p>
<blockquote>
<p>æ­¥éª¤çº§éªŒè¯å™¨ä¹Ÿè¢«ç§°ä¸ºæµç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ï¼Œæ˜¯æµ‹è¯•æ—¶é—´ç¼©æ”¾çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚PRMéœ€è¦æ­¥éª¤çº§çš„ç›‘ç£ï¼Œä½¿å¾—å®ƒä»¬çš„è®­ç»ƒæˆæœ¬é«˜æ˜‚ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å»ºç«‹æ•°æ®é«˜æ•ˆçš„PRMï¼Œä½œä¸ºè¡¨è¿°æ­¥éª¤å¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡ç”ŸæˆéªŒè¯æ€ç»´é“¾ï¼ˆCoTï¼‰æ¥éªŒè¯è§£å†³æ–¹æ¡ˆä¸­çš„æ¯ä¸€æ­¥ã€‚æˆ‘ä»¬æå‡ºäº†ThinkPRMï¼Œå®ƒæ˜¯ä¸€ç§é•¿CoTéªŒè¯å™¨ï¼Œåœ¨æµç¨‹æ ‡ç­¾çš„æ•°é‡ä¸Šæ¯”åˆ¤åˆ«PRMæ‰€éœ€çš„æ ‡ç­¾å°‘äº†å‡ ä¸ªæ•°é‡çº§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨é•¿CoTæ¨¡å‹å›ºæœ‰çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†LLMä½œä¸ºæ³•å®˜å’Œåˆ¤åˆ«éªŒè¯å™¨ã€‚å…·ä½“æ¥è¯´ï¼ŒThinkPRMåœ¨ProcessBenchã€MATH-500å’ŒAIME â€˜24çš„æœ€ä½³Né€‰å’Œå¥–åŠ±å¯¼å‘æœç´¢æ–¹é¢å‡»è´¥äº†åŸºçº¿ã€‚åœ¨GPQA-Diamondå’ŒLiveCodeBenchçš„å­é›†ä¸Šè¿›è¡ŒåŸŸå¤–è¯„ä¼°ï¼Œæˆ‘ä»¬çš„PRMåœ¨å®Œæ•´çš„PRM800Kä¸Šè®­ç»ƒçš„åˆ¤åˆ«éªŒè¯å™¨åˆ†åˆ«æé«˜äº†8%å’Œ4.5%ã€‚æœ€åï¼Œåœ¨ç›¸åŒçš„ä»¤ç‰Œé¢„ç®—ä¸‹ï¼ŒThinkPRMåœ¨ProcessBenchçš„å­é›†ä¸Šæ›´æœ‰æ•ˆåœ°æ‰©å±•äº†éªŒè¯è®¡ç®—ï¼Œæ¯”LLM-as-a-Judgeé«˜å‡º7.2%ã€‚æˆ‘ä»¬çš„å·¥ä½œçªå‡ºäº†ç”Ÿæˆå¼ã€é•¿CoT PRMçš„ä»·å€¼ï¼Œå®ƒå¯ä»¥åœ¨æµ‹è¯•æ—¶æ‰©å±•éªŒè¯è®¡ç®—ï¼ŒåŒæ—¶è®­ç»ƒæ—¶éœ€è¦çš„ç›‘ç£å¾ˆå°‘ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/mukhal/thinkprm%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/mukhal/thinkprmå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16828v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ­¥éª¤å¼éªŒè¯å™¨ï¼ˆä¹Ÿç§°ä¸ºè¿‡ç¨‹å¥–åŠ±æ¨¡å‹PRMï¼‰åœ¨æµ‹è¯•æ—¶é—´ç¼©æ”¾ä¸­çš„å…³é”®ä½œç”¨ã€‚ç”±äºPRMéœ€è¦æ­¥éª¤çº§åˆ«çš„ç›‘ç£ï¼Œå› æ­¤è®­ç»ƒæˆæœ¬è¾ƒé«˜ã€‚æœ¬æ–‡æ—¨åœ¨å»ºç«‹æ•°æ®é«˜æ•ˆçš„PRMï¼Œé€šè¿‡ç”ŸæˆéªŒè¯æ€ç»´é“¾ï¼ˆCoTï¼‰æ¥éªŒè¯è§£å†³æ–¹æ¡ˆä¸­çš„æ¯ä¸€æ­¥ã€‚æå‡ºThinkPRMæ–¹æ³•ï¼Œåœ¨è¿œä½äºä¼ ç»Ÿåˆ¤åˆ«å¼PRMæ‰€éœ€çš„è¿‡ç¨‹æ ‡ç­¾æ•°é‡çš„æƒ…å†µä¸‹è¿›è¡Œå¾®è°ƒï¼Œåˆ©ç”¨é•¿CoTæ¨¡å‹çš„å†…åœ¨æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè¶…è¶ŠLLM-as-a-Judgeå’Œåˆ¤åˆ«å¼éªŒè¯å™¨çš„æ€§èƒ½ã€‚ThinkPRMåœ¨PRM800Kä»…ä½¿ç”¨1%çš„è¿‡ç¨‹æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œåœ¨ProcessBenchã€MATH-500å’ŒAIME â€˜24ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†åŸºçº¿ã€‚æ­¤å¤–ï¼Œåœ¨GPQA-Diamondå’ŒLiveCodeBenchçš„å­é›†ä¸Šè¿›è¡ŒåŸŸå¤–è¯„ä¼°æ—¶ï¼Œæˆ‘ä»¬çš„PRMåœ¨è®­ç»ƒå…¨è¿‡ç¨‹çš„PRM800Kåˆ¤åˆ«å¼éªŒè¯å™¨ä¸Šè¶…è¶Šäº†å®ƒä»¬ï¼Œåˆ†åˆ«æé«˜äº†8%å’Œ4.5%ã€‚æœ€åï¼Œåœ¨ç›¸åŒçš„ä»¤ç‰Œé¢„ç®—ä¸‹ï¼ŒThinkPRMåœ¨ProcessBenchçš„å­é›†ä¸Šæ›´æœ‰æ•ˆåœ°æ‰©å±•äº†éªŒè¯è®¡ç®—ï¼Œæ¯”LLM-as-a-Judgeé«˜å‡º7.2%ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„å·¥ä½œçªæ˜¾äº†ç”Ÿæˆå¼ã€é•¿CoT PRMçš„ä»·å€¼ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥åœ¨éªŒè¯æ—¶æ‰©å±•æµ‹è¯•æ—¶é—´è®¡ç®—ï¼ŒåŒæ—¶è®­ç»ƒæ—¶éœ€è¦çš„ç›‘ç£è¾ƒå°‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PRMsï¼ˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼‰æ˜¯æµ‹è¯•æ—¶é—´ç¼©æ”¾çš„å…³é”®ã€‚</li>
<li>ThinkPRMæ˜¯ä¸€ç§æ•°æ®é«˜æ•ˆçš„PRMæ–¹æ³•ï¼Œé€šè¿‡ç”ŸæˆéªŒè¯æ€ç»´é“¾ï¼ˆCoTï¼‰æ¥éªŒè¯æ¯ä¸€æ­¥ã€‚</li>
<li>ThinkPRMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œä½¿ç”¨çš„è¿‡ç¨‹æ ‡ç­¾æ•°é‡è¿œä½äºä¼ ç»Ÿåˆ¤åˆ«å¼PRMã€‚</li>
<li>åœ¨åŸŸå¤–è¯„ä¼°ä¸­ï¼ŒThinkPRMè¶…è¶Šäº†å…¨è¿‡ç¨‹çš„PRM800Kåˆ¤åˆ«å¼éªŒè¯å™¨ã€‚</li>
<li>ThinkPRMæ›´æœ‰æ•ˆåœ°æ‰©å±•äº†éªŒè¯è®¡ç®—ï¼Œä¸LLM-as-a-Judgeç›¸æ¯”å…·æœ‰æ›´é«˜çš„æ€§èƒ½ã€‚</li>
<li>ThinkPRMåˆ©ç”¨é•¿CoTæ¨¡å‹çš„å†…åœ¨æ¨ç†èƒ½åŠ›ï¼Œçªæ˜¾äº†ç”Ÿæˆå¼PRMçš„ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16828">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-155a7c5cee3836db4785fdc9f63c1f4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-faff1173770be211c85c476ecacf46b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5481824a3b2a71e347559554de28b426.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0e1f1bbd876eb7ecaf11cc4741763ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-59a0bd7bf8a55aee54374ab08267a640.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Credible-plan-driven-RAG-method-for-Multi-hop-Question-Answering"><a href="#Credible-plan-driven-RAG-method-for-Multi-hop-Question-Answering" class="headerlink" title="Credible plan-driven RAG method for Multi-hop Question Answering"></a>Credible plan-driven RAG method for Multi-hop Question Answering</h2><p><strong>Authors:Ningning Zhang, Chi Zhang, Zhizhong Tan, Xingxing Yang, Weiping Deng, Wenyong Wang</strong></p>
<p>Multi-hop question answering (QA) presents a considerable challenge for Retrieval-Augmented Generation (RAG), requiring the structured decomposition of complex queries into logical reasoning paths and the generation of dependable intermediate results. However, deviations in reasoning paths or errors in intermediate results, which are common in current RAG methods, may propagate and accumulate throughout the reasoning process, diminishing the accuracy of the answer to complex queries. To address this challenge, we propose the Plan-then-Act-and-Review (PAR RAG) framework, which is organized into three key stages: planning, act, and review, and aims to offer an interpretable and incremental reasoning paradigm for accurate and reliable multi-hop question answering by mitigating error propagation.PAR RAG initially applies a top-down problem decomposition strategy, formulating a comprehensive plan that integrates multiple executable steps from a holistic viewpoint. This approach avoids the pitfalls of local optima common in traditional RAG methods, ensuring the accuracy of the entire reasoning path. Subsequently, PAR RAG incorporates a plan execution mechanism based on multi-granularity verification. By utilizing both coarse-grained similarity information and fine-grained relevant data, the framework thoroughly checks and adjusts intermediate results, ensuring process accuracy while effectively managing error propagation and amplification. Experimental results on multi-hop QA datasets demonstrate that the PAR RAG framework substantially outperforms existing state-of-the-art methods in key metrics, including EM and F1 scores. </p>
<blockquote>
<p>å¤šè·³é—®ç­”ï¼ˆQAï¼‰ä¸ºåŸºäºæ£€ç´¢çš„ç”Ÿæˆæ¨¡å‹ï¼ˆRAGï¼‰å¸¦æ¥äº†ç›¸å½“å¤§çš„æŒ‘æˆ˜ã€‚å®ƒè¦æ±‚å°†å¤æ‚çš„æŸ¥è¯¢ç»“æ„åŒ–åœ°åˆ†è§£ä¸ºé€»è¾‘æ¨ç†è·¯å¾„å¹¶ç”Ÿæˆå¯é çš„ä¸­é—´ç»“æœã€‚ç„¶è€Œï¼Œå½“å‰RAGæ–¹æ³•ä¸­çš„æ¨ç†è·¯å¾„åå·®æˆ–ä¸­é—´ç»“æœé”™è¯¯å¯èƒ½ä¼šåœ¨æ•´ä¸ªæ¨ç†è¿‡ç¨‹ä¸­ä¼ æ’­å’Œç´¯ç§¯ï¼Œä»è€Œé™ä½å¯¹å¤æ‚æŸ¥è¯¢çš„ç­”æ¡ˆå‡†ç¡®æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†è®¡åˆ’-è¡ŒåŠ¨-è¯„å®¡ï¼ˆPAR RAGï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ†ä¸ºè§„åˆ’ã€è¡ŒåŠ¨å’Œè¯„å®¡ä¸‰ä¸ªé˜¶æ®µï¼Œæ—¨åœ¨é€šè¿‡ç¼“è§£é”™è¯¯ä¼ æ’­ä¸ºå‡†ç¡®å¯é çš„å¤šè·³é—®ç­”æä¾›å¯è§£é‡Šå’Œå¢é‡æ¨ç†èŒƒå¼ã€‚</p>
</blockquote>
<p>PAR RAGé¦–å…ˆé‡‡ç”¨è‡ªä¸Šè€Œä¸‹çš„é—®é¢˜åˆ†è§£ç­–ç•¥ï¼Œä»å…¨å±€è§’åº¦åˆ¶å®šåŒ…å«å¤šä¸ªå¯æ‰§è¡Œæ­¥éª¤çš„ç»¼åˆè®¡åˆ’ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ä¼ ç»ŸRAGæ–¹æ³•ä¸­å¸¸è§çš„å±€éƒ¨æœ€ä¼˜é™·é˜±ï¼Œç¡®ä¿äº†æ•´ä¸ªæ¨ç†è·¯å¾„çš„å‡†ç¡®æ€§ã€‚æ¥ç€ï¼ŒPAR RAGèå…¥åŸºäºå¤šç²’åº¦éªŒè¯çš„è®¡åˆ’æ‰§è¡Œæœºåˆ¶ã€‚é€šè¿‡åˆ©ç”¨ç²—ç²’åº¦ç›¸ä¼¼æ€§ä¿¡æ¯å’Œç»†ç²’åº¦ç›¸å…³æ•°æ®ï¼Œè¯¥æ¡†æ¶å½»åº•æ£€æŸ¥å’Œè°ƒæ•´ä¸­é—´ç»“æœï¼Œç¡®ä¿æµç¨‹å‡†ç¡®æ€§ï¼ŒåŒæ—¶æœ‰æ•ˆç®¡ç†é”™è¯¯ä¼ æ’­å’Œæ”¾å¤§ã€‚åœ¨å¤šè·³QAæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒPAR RAGæ¡†æ¶åœ¨å…³é”®æŒ‡æ ‡ï¼ˆåŒ…æ‹¬EMå’ŒF1å¾—åˆ†ï¼‰ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16787v1">PDF</a> 18 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä¸»è¦ä»‹ç»äº†é’ˆå¯¹å¤šè·³é—®ç­”ä¸­çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§åä¸ºPlan-then-Act-and-Reviewï¼ˆPAR RAGï¼‰çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è§„åˆ’ã€æ‰§è¡Œå’Œå®¡æŸ¥ä¸‰ä¸ªé˜¶æ®µï¼Œæ—¨åœ¨æä¾›ä¸€ç§å¯è§£é‡Šå’Œå¢é‡æ¨ç†èŒƒå¼ï¼Œä»¥æé«˜å¤šè·³é—®ç­”çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œå¹¶å‡å°‘é”™è¯¯ä¼ æ’­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤šè·³é—®ç­”å¯¹Retrieval-Augmented Generationï¼ˆRAGï¼‰æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œéœ€å°†å¤æ‚æŸ¥è¯¢åˆ†è§£æˆé€»è¾‘æ¨ç†è·¯å¾„ï¼Œå¹¶ç”Ÿæˆå¯é çš„ä¸­é—´ç»“æœã€‚</li>
<li>å½“å‰RAGæ–¹æ³•åœ¨å¤„ç†å¤šè·³é—®ç­”æ—¶ï¼Œå­˜åœ¨çš„æ¨ç†è·¯å¾„åå·®æˆ–ä¸­é—´ç»“æœé”™è¯¯å¯èƒ½ä¼šç´¯ç§¯å¹¶å½±å“æœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚</li>
<li>PAR RAGæ¡†æ¶åŒ…æ‹¬è§„åˆ’ã€æ‰§è¡Œå’Œå®¡æŸ¥ä¸‰ä¸ªé˜¶æ®µï¼Œæ—¨åœ¨é€šè¿‡å¯è§£é‡Šå’Œå¢é‡æ¨ç†æé«˜å¤šè·³é—®ç­”çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
<li>PAR RAGæ¡†æ¶ä½¿ç”¨è‡ªä¸Šè€Œä¸‹çš„é—®é¢˜åˆ†è§£ç­–ç•¥ï¼Œåˆ¶å®šå…¨é¢è®¡åˆ’ï¼Œä»æ•´ä½“ä¸Šç¡®ä¿æ•´ä¸ªæ¨ç†è·¯å¾„çš„å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨æ‰§è¡Œé˜¶æ®µï¼ŒPAR RAGåˆ©ç”¨å¤šç²’åº¦éªŒè¯æœºåˆ¶æ£€æŸ¥å¹¶è°ƒæ•´ä¸­é—´ç»“æœï¼Œç¡®ä¿è¿‡ç¨‹çš„å‡†ç¡®æ€§å¹¶æœ‰æ•ˆç®¡ç†é”™è¯¯ä¼ æ’­å’Œæ”¾å¤§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒPAR RAGæ¡†æ¶åœ¨å…³é”®æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬EMå’ŒF1åˆ†æ•°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16787">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c9173a2e331c9240e95f7bf0f3487ff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b98cdf19ba3b73391f538fcccd36eb6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ba49b907ebdfdb13eb040b185bb04a4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Skywork-R1V2-Multimodal-Hybrid-Reinforcement-Learning-for-Reasoning"><a href="#Skywork-R1V2-Multimodal-Hybrid-Reinforcement-Learning-for-Reasoning" class="headerlink" title="Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning"></a>Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning</h2><p><strong>Authors: Chris, Yichen Wei, Yi Peng, Xiaokun Wang, Weijie Qiu, Wei Shen, Tianyidan Xie, Jiangbo Pei, Jianhao Zhang, Yunzhuo Hao, Xuchen Song, Yang Liu, Yahui Zhou</strong></p>
<p>We present Skywork R1V2, a next-generation multimodal reasoning model and a major leap forward from its predecessor, Skywork R1V. At its core, R1V2 introduces a hybrid reinforcement learning paradigm that harmonizes reward-model guidance with rule-based strategies, thereby addressing the long-standing challenge of balancing sophisticated reasoning capabilities with broad generalization. To further enhance training efficiency, we propose the Selective Sample Buffer (SSB) mechanism, which effectively counters the &#96;&#96;Vanishing Advantagesâ€™â€™ dilemma inherent in Group Relative Policy Optimization (GRPO) by prioritizing high-value samples throughout the optimization process. Notably, we observe that excessive reinforcement signals can induce visual hallucinationsâ€“a phenomenon we systematically monitor and mitigate through calibrated reward thresholds throughout the training process. Empirical results affirm the exceptional capability of R1V2, with benchmark-leading performances such as 62.6 on OlympiadBench, 79.0 on AIME2024, 63.6 on LiveCodeBench, and 74.0 on MMMU. These results underscore R1V2â€™s superiority over existing open-source models and demonstrate significant progress in closing the performance gap with premier proprietary systems, including Gemini 2.5 and OpenAI o4-mini. The Skywork R1V2 model weights have been publicly released to promote openness and reproducibility <a target="_blank" rel="noopener" href="https://huggingface.co/Skywork/Skywork-R1V2-38B">https://huggingface.co/Skywork/Skywork-R1V2-38B</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Skywork R1V2ï¼Œè¿™æ˜¯ä¸‹ä¸€ä»£å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œä¹Ÿæ˜¯å…¶å‰èº«Skywork R1Vçš„ä¸€æ¬¡é‡å¤§é£è·ƒã€‚R1V2çš„æ ¸å¿ƒæ˜¯å¼•å…¥äº†ä¸€ç§æ··åˆå¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œè¯¥èŒƒå¼å°†å¥–åŠ±æ¨¡å‹æŒ‡å¯¼ä¸åŸºäºè§„åˆ™çš„ç­–ç•¥ç›¸åè°ƒï¼Œä»è€Œè§£å†³äº†é•¿æœŸå­˜åœ¨çš„åœ¨å¹³è¡¡å¤æ‚æ¨ç†èƒ½åŠ›å’Œå¹¿æ³›æ³›åŒ–æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜è®­ç»ƒæ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºäº†é€‰æ‹©æ€§æ ·æœ¬ç¼“å†²ï¼ˆSSBï¼‰æœºåˆ¶ï¼Œå®ƒé€šè¿‡åœ¨æ•´ä¸ªä¼˜åŒ–è¿‡ç¨‹ä¸­ä¼˜å…ˆå¤„ç†é«˜ä»·å€¼æ ·æœ¬ï¼Œæœ‰æ•ˆåº”å¯¹ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ä¸­å›ºæœ‰çš„â€œä¼˜åŠ¿æ¶ˆå¤±â€å›°å¢ƒã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è¿‡å¤šçš„å¼ºåŒ–ä¿¡å·å¯èƒ½å¯¼è‡´è§†è§‰å¹»è§‰â€”â€”æˆ‘ä»¬é€šè¿‡å¯¹è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¥–åŠ±é˜ˆå€¼è¿›è¡Œæ ¡å‡†æ¥ç³»ç»Ÿç›‘æµ‹å’Œç¼“è§£è¿™ä¸€ç°è±¡ã€‚ç»éªŒç»“æœè¯å®äº†R1V2çš„å“è¶Šèƒ½åŠ›ï¼Œå…¶åœ¨OlympiadBenchä¸Šçš„å¾—åˆ†é«˜è¾¾62.6ï¼ŒAIME2024ä¸Šçš„å¾—åˆ†ä¸º79.0ï¼ŒLiveCodeBenchä¸Šçš„å¾—åˆ†ä¸º63.6ï¼ŒMMMUä¸Šçš„å¾—åˆ†ä¸º74.0ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†R1V2åœ¨ç°æœ‰å¼€æºæ¨¡å‹ä¸­çš„ä¼˜è¶Šæ€§ï¼Œå¹¶å±•ç¤ºäº†ä¸é¡¶çº§ä¸“æœ‰ç³»ç»Ÿï¼ˆåŒ…æ‹¬Gemini 2.5å’ŒOpenAI o4-miniï¼‰çš„æ€§èƒ½å·®è·å¾—åˆ°æ˜¾è‘—ç¼©å°ã€‚Skywork R1V2æ¨¡å‹æƒé‡å·²ç»å…¬å¼€å‘å¸ƒï¼Œä»¥ä¿ƒè¿›å¼€æ”¾æ€§å’Œå¯é‡å¤æ€§ï¼Œè®¿é—®åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/Skywork/Skywork-R1V2-38B%E3%80%82">https://huggingface.co/Skywork/Skywork-R1V2-38Bã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16656v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>Skywork R1V2æ˜¯æ–°ä¸€ä»£çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œæ˜¯Skywork R1Vçš„é‡å¤§çªç ´ã€‚å…¶æ ¸å¿ƒå¼•å…¥äº†æ··åˆå¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œèåˆäº†å¥–åŠ±æ¨¡å‹æŒ‡å¯¼å’ŒåŸºäºè§„åˆ™çš„ç­–ç•¥ï¼Œè§£å†³äº†é•¿æœŸå­˜åœ¨çš„å¹³è¡¡å¤æ‚æ¨ç†èƒ½åŠ›å’Œå¹¿æ³›æ³›åŒ–èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚ä¸ºæé«˜è®­ç»ƒæ•ˆç‡ï¼Œæå‡ºäº†é€‰æ‹©æ€§æ ·æœ¬ç¼“å†²æœºåˆ¶ï¼ˆSSBï¼‰ï¼Œæœ‰æ•ˆè§£å†³äº†ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ä¸­çš„â€œä¼˜åŠ¿æ¶ˆå¤±â€å›°å¢ƒã€‚R1V2æ¨¡å‹çš„æƒé‡å·²å…¬å¼€å‘å¸ƒï¼Œä»¥ä¿ƒè¿›å¼€æ”¾æ€§å’Œå¯é‡å¤æ€§ã€‚å…¶å®åŠ›å¾—åˆ°äº†è¯„ä¼°çš„éªŒè¯ï¼ŒåŒ…æ‹¬åœ¨OlympiadBenchä¸Šçš„å¾—åˆ†æ˜¯é¢†å…ˆçš„ï¼Œä¸ç°æœ‰å¼€æºæ¨¡å‹ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ï¼Œå¹¶ç¼©å°äº†ä¸é«˜ç«¯ä¸“æœ‰ç³»ç»Ÿçš„æ€§èƒ½å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Skywork R1V2æ˜¯æ–°ä¸€ä»£å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼ŒSkywork R1Vçš„é‡å¤§çªç ´ã€‚</li>
<li>R1V2å¼•å…¥äº†æ··åˆå¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œèåˆäº†å¥–åŠ±æ¨¡å‹æŒ‡å¯¼å’ŒåŸºäºè§„åˆ™çš„ç­–ç•¥ã€‚</li>
<li>R1V2è§£å†³äº†å¹³è¡¡å¤æ‚æ¨ç†èƒ½åŠ›å’Œå¹¿æ³›æ³›åŒ–èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºé€‰æ‹©æ€§æ ·æœ¬ç¼“å†²æœºåˆ¶ï¼ˆSSBï¼‰ä»¥æé«˜è®­ç»ƒæ•ˆç‡å¹¶è§£å†³â€œä¼˜åŠ¿æ¶ˆå¤±â€é—®é¢˜ã€‚</li>
<li>R1V2æ¨¡å‹çš„æ€§èƒ½ç»è¿‡å®è¯éªŒè¯ï¼ŒåŒ…æ‹¬åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°é¢†å…ˆã€‚</li>
<li>R1V2æ¨¡å‹æƒé‡å·²å…¬å¼€å‘å¸ƒï¼Œä¿ƒè¿›å¼€æ”¾æ€§å’Œå¯é‡å¤æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16656">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-76f08ef0c956678cc2a0872e2f66821f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dda8f93ba3a34c5d51cf9723a2722154.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="PIS-Linking-Importance-Sampling-and-Attention-Mechanisms-for-Efficient-Prompt-Compression"><a href="#PIS-Linking-Importance-Sampling-and-Attention-Mechanisms-for-Efficient-Prompt-Compression" class="headerlink" title="PIS: Linking Importance Sampling and Attention Mechanisms for Efficient   Prompt Compression"></a>PIS: Linking Importance Sampling and Attention Mechanisms for Efficient   Prompt Compression</h2><p><strong>Authors:Lizhe Chen, Binjia Zhou, Yuyao Ge, Jiayi Chen, Shiguang NI</strong></p>
<p>Large language models (LLMs) have achieved remarkable progress, demonstrating unprecedented capabilities across various natural language processing tasks. However, the high costs associated with such exceptional performance limit the widespread adoption of LLMs, highlighting the need for prompt compression. Existing prompt compression methods primarily rely on heuristic truncation or abstractive summarization techniques, which fundamentally overlook the intrinsic mechanisms of LLMs and lack a systematic evaluation of token importance for generation. In this work, we introduce Prompt Importance Sampling (PIS), a novel compression framework that dynamically compresses prompts by sampling important tokens based on the analysis of attention scores of hidden states. PIS employs a dual-level compression mechanism: 1) at the token level, we quantify saliency using LLM-native attention scores and implement adaptive compression through a lightweight 9-layer reinforcement learning (RL) network; 2) at the semantic level, we propose a Russian roulette sampling strategy for sentence-level importance sampling. Comprehensive evaluations across multiple domain benchmarks demonstrate that our method achieves state-of-the-art compression performance. Notably, our framework serendipitously enhances reasoning efficiency through optimized context structuring. This work advances prompt engineering by offering both theoretical grounding and practical efficiency in context management for LLMs. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œåœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç¤ºäº†å‰æ‰€æœªæœ‰çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¸è¿™ç§å“è¶Šæ€§èƒ½ç›¸å…³çš„é«˜æ˜‚æˆæœ¬é™åˆ¶äº†LLMçš„å¹¿æ³›åº”ç”¨ï¼Œä»è€Œå‡¸æ˜¾äº†æç¤ºå‹ç¼©çš„å¿…è¦æ€§ã€‚ç°æœ‰çš„æç¤ºå‹ç¼©æ–¹æ³•ä¸»è¦ä¾èµ–äºå¯å‘å¼æˆªæ–­æˆ–æŠ½è±¡æ‘˜è¦æŠ€æœ¯ï¼Œè¿™ä»æ ¹æœ¬ä¸Šå¿½è§†äº†LLMçš„å†…åœ¨æœºåˆ¶ï¼Œç¼ºä¹å¯¹ç”Ÿæˆä»¤ç‰Œé‡è¦æ€§çš„ç³»ç»Ÿè¯„ä¼°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æç¤ºé‡è¦æ€§é‡‡æ ·ï¼ˆPISï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å‹ç¼©æ¡†æ¶ï¼Œå®ƒé€šè¿‡åŸºäºéšè—çŠ¶æ€æ³¨æ„åŠ›åˆ†æ•°çš„åˆ†ææ¥é‡‡æ ·é‡è¦ä»¤ç‰Œï¼Œä»è€ŒåŠ¨æ€å‹ç¼©æç¤ºã€‚PISé‡‡ç”¨åŒé‡å‹ç¼©æœºåˆ¶ï¼š1ï¼‰åœ¨ä»¤ç‰Œçº§åˆ«ï¼Œæˆ‘ä»¬ä½¿ç”¨LLMæœ¬åœ°æ³¨æ„åŠ›åˆ†æ•°é‡åŒ–æ˜¾è‘—æ€§ï¼Œå¹¶é€šè¿‡è½»é‡çº§çš„9å±‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç½‘ç»œå®ç°è‡ªé€‚åº”å‹ç¼©ï¼›2ï¼‰åœ¨è¯­ä¹‰å±‚é¢ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¿„ç½—æ–¯è½®ç›˜é‡‡æ ·ç­–ç•¥ï¼Œç”¨äºå¥å­çº§åˆ«çš„é‡è¦æ€§é‡‡æ ·ã€‚åœ¨å¤šä¸ªé¢†åŸŸåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„å‹ç¼©æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡ä¼˜åŒ–ä¸Šä¸‹æ–‡ç»“æ„æ„å¤–åœ°æé«˜äº†æ¨ç†æ•ˆç‡ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡ä¸ºLLMæä¾›ç†è®ºæ”¯æŒå’Œå®ç”¨é«˜æ•ˆçš„ä¸Šä¸‹æ–‡ç®¡ç†ï¼Œæ¨åŠ¨äº†æç¤ºå·¥ç¨‹çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16574v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¼—å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å…¶é«˜æ˜‚æˆæœ¬é™åˆ¶äº†å¹¿æ³›åº”ç”¨ï¼Œè¿«åˆ‡éœ€è¦å‹ç¼©æç¤ºã€‚ç°æœ‰æç¤ºå‹ç¼©æ–¹æ³•ä¸»è¦ä¾èµ–å¯å‘å¼æˆªæ–­æˆ–æŠ½è±¡æ‘˜è¦æŠ€æœ¯ï¼Œå¿½ç•¥äº†LLMçš„å†…åœ¨æœºåˆ¶ï¼Œç¼ºä¹ç”Ÿæˆè¿‡ç¨‹ä¸­tokené‡è¦æ€§çš„ç³»ç»Ÿè¯„ä¼°ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ–°çš„å‹ç¼©æ¡†æ¶â€”â€”æç¤ºé‡è¦æ€§é‡‡æ ·ï¼ˆPISï¼‰ï¼Œå®ƒé€šè¿‡é‡‡æ ·é‡è¦tokenåŠ¨æ€å‹ç¼©æç¤ºï¼ŒåŸºäºéšè—çŠ¶æ€çš„æ³¨æ„åŠ›åˆ†æ•°è¿›è¡Œåˆ†æã€‚PISé‡‡ç”¨åŒé‡å‹ç¼©æœºåˆ¶ï¼š1ï¼‰åœ¨tokenå±‚é¢ï¼Œåˆ©ç”¨LLMåŸç”Ÿæ³¨æ„åŠ›åˆ†æ•°é‡åŒ–æ˜¾è‘—æ€§ï¼Œé€šè¿‡è½»é‡çº§9å±‚å¼ºåŒ–å­¦ä¹ ç½‘ç»œå®ç°è‡ªé€‚åº”å‹ç¼©ï¼›2ï¼‰åœ¨è¯­ä¹‰å±‚é¢ï¼Œæå‡ºä¿„ç½—æ–¯è½®ç›˜é‡‡æ ·ç­–ç•¥è¿›è¡Œå¥å­çº§åˆ«çš„é‡è¦æ€§é‡‡æ ·ã€‚åœ¨å¤šä¸ªé¢†åŸŸåŸºå‡†æµ‹è¯•ä¸Šçš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„å‹ç¼©æ€§èƒ½ï¼Œå¹¶æ„å¤–åœ°æé«˜äº†æ¨ç†æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†é«˜æˆæœ¬é™åˆ¶äº†å…¶å¹¿æ³›åº”ç”¨ã€‚</li>
<li>ç°æœ‰æç¤ºå‹ç¼©æ–¹æ³•å¿½ç•¥LLMçš„å†…åœ¨æœºåˆ¶ï¼Œç¼ºä¹ç”Ÿæˆè¿‡ç¨‹ä¸­tokené‡è¦æ€§çš„ç³»ç»Ÿè¯„ä¼°ã€‚</li>
<li>æ–°å‹å‹ç¼©æ¡†æ¶â€”â€”æç¤ºé‡è¦æ€§é‡‡æ ·ï¼ˆPISï¼‰é€šè¿‡é‡‡æ ·é‡è¦tokenåŠ¨æ€å‹ç¼©æç¤ºã€‚</li>
<li>PISé‡‡ç”¨åŒé‡å‹ç¼©æœºåˆ¶ï¼Œç»“åˆtokenå’Œè¯­ä¹‰å±‚é¢çš„é‡è¦æ€§é‡‡æ ·ã€‚</li>
<li>PISåˆ©ç”¨LLMåŸç”Ÿæ³¨æ„åŠ›åˆ†æ•°è¿›è¡Œtokené‡è¦æ€§é‡åŒ–ï¼Œå¹¶ç»“åˆå¼ºåŒ–å­¦ä¹ ç½‘ç»œå®ç°è‡ªé€‚åº”å‹ç¼©ã€‚</li>
<li>PISåœ¨å¤šä¸ªé¢†åŸŸåŸºå‡†æµ‹è¯•ä¸Šå®ç°æœ€å…ˆè¿›çš„å‹ç¼©æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a1170d1a65707a71f2aa732ef5411297.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e8168e1d8dd238db487c6fa5b0d79b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-669a363c63b5689cbdf3b30fa8e7012c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d326f290a1da4f2fa5d4bb269dfc1dd.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Think-Hierarchically-Act-Dynamically-Hierarchical-Multi-modal-Fusion-and-Reasoning-for-Vision-and-Language-Navigation"><a href="#Think-Hierarchically-Act-Dynamically-Hierarchical-Multi-modal-Fusion-and-Reasoning-for-Vision-and-Language-Navigation" class="headerlink" title="Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion   and Reasoning for Vision-and-Language Navigation"></a>Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion   and Reasoning for Vision-and-Language Navigation</h2><p><strong>Authors:Junrong Yue, Yifan Zhang, Chuan Qin, Bo Li, Xiaomin Lie, Xinlei Yu, Wenxin Zhang, Zhendong Zhao</strong></p>
<p>Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow natural language instructions and reach target locations in real-world environments. While prior methods often rely on either global scene representations or object-level features, these approaches are insufficient for capturing the complex interactions across modalities required for accurate navigation. In this paper, we propose a Multi-level Fusion and Reasoning Architecture (MFRA) to enhance the agentâ€™s ability to reason over visual observations, language instructions and navigation history. Specifically, MFRA introduces a hierarchical fusion mechanism that aggregates multi-level features-ranging from low-level visual cues to high-level semantic concepts-across multiple modalities. We further design a reasoning module that leverages fused representations to infer navigation actions through instruction-guided attention and dynamic context integration. By selectively capturing and combining relevant visual, linguistic, and temporal signals, MFRA improves decision-making accuracy in complex navigation scenarios. Extensive experiments on benchmark VLN datasets including REVERIE, R2R, and SOON demonstrate that MFRA achieves superior performance compared to state-of-the-art methods, validating the effectiveness of multi-level modal fusion for embodied navigation. </p>
<blockquote>
<p>è§†è§‰ä¸è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰çš„ç›®æ ‡æ˜¯ä½¿å®ä½“ä»£ç†èƒ½å¤Ÿéµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­åˆ°è¾¾ç›®æ ‡ä½ç½®ã€‚å°½ç®¡å…ˆå‰çš„æ–¹æ³•ç»å¸¸ä¾èµ–äºå…¨å±€åœºæ™¯è¡¨ç¤ºæˆ–å¯¹è±¡çº§ç‰¹å¾ï¼Œä½†è¿™äº›æ–¹æ³•å¯¹äºæ•è·è·¨ä¸åŒæ¨¡å¼æ‰€éœ€è¿›è¡Œçš„å¤æ‚äº¤äº’ä¸å¤Ÿå……åˆ†ï¼Œæ— æ³•è¿›è¡Œç²¾ç¡®çš„å¯¼èˆªã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šå±‚çº§èåˆæ¨ç†æ¶æ„ï¼ˆMFRAï¼‰ï¼Œä»¥å¢å¼ºä»£ç†åœ¨è§†è§‰è§‚å¯Ÿã€è¯­è¨€æŒ‡ä»¤å’Œå¯¼èˆªå†å²è®°å½•æ–¹é¢çš„æ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒMFRAå¼•å…¥äº†ä¸€ç§åˆ†å±‚èåˆæœºåˆ¶ï¼Œè¯¥æœºåˆ¶èƒ½å¤Ÿèåˆè·¨å¤šä¸ªæ¨¡å¼çš„å¤šå±‚æ¬¡ç‰¹å¾ï¼ŒèŒƒå›´ä»ä½çº§åˆ«çš„è§†è§‰çº¿ç´¢åˆ°é«˜çº§åˆ«çš„è¯­ä¹‰æ¦‚å¿µã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ä¸ªæ¨ç†æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨èåˆåçš„è¡¨ç¤ºå½¢å¼ï¼Œé€šè¿‡æŒ‡ä»¤å¼•å¯¼æ³¨æ„åŠ›ä»¥åŠåŠ¨æ€ä¸Šä¸‹æ–‡æ•´åˆæ¥æ¨æ–­å¯¼èˆªåŠ¨ä½œã€‚é€šè¿‡æœ‰é€‰æ‹©åœ°æ•è·å’Œç»“åˆç›¸å…³çš„è§†è§‰ã€è¯­è¨€å’Œæ—¶é—´ä¿¡å·ï¼ŒMFRAæé«˜äº†åœ¨å¤æ‚å¯¼èˆªåœºæ™¯ä¸­çš„å†³ç­–å‡†ç¡®æ€§ã€‚åœ¨åŒ…æ‹¬REVERIEã€R2Rå’ŒSOONåœ¨å†…çš„åŸºå‡†VLNæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒMFRAå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ŒéªŒè¯äº†å¤šå±‚çº§æ¨¡å¼èåˆåœ¨å®ä½“å¯¼èˆªä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16516v1">PDF</a> 11 pages, 4 figures, Submitted to ACM MM 2025</p>
<p><strong>Summary</strong><br>åœ¨è¿™ä¸ªä»»åŠ¡ä¸­ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºMulti-level Fusion and Reasoning Architectureï¼ˆMFRAï¼‰çš„æ–°æ¶æ„ï¼Œæ—¨åœ¨å¢å¼ºä»£ç†åœ¨è§†è§‰è§‚å¯Ÿã€è¯­è¨€æŒ‡ä»¤å’Œå¯¼èˆªå†å²æ–¹é¢çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¶æ„å¼•å…¥äº†å¤šå±‚æ¬¡èåˆæœºåˆ¶ï¼Œèšåˆä¸åŒå±‚çº§çš„å¤šæ¨¡æ€ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨èåˆåçš„è¡¨ç¤ºè¿›è¡Œæ¨ç†ï¼Œé€šè¿‡æŒ‡ä»¤å¼•å¯¼æ³¨æ„åŠ›å’ŒåŠ¨æ€ä¸Šä¸‹æ–‡æ•´åˆæ¥æ¨æ–­å¯¼èˆªåŠ¨ä½œã€‚åœ¨å¤æ‚çš„å¯¼èˆªåœºæ™¯ä¸­ï¼ŒMFRAé€šè¿‡é€‰æ‹©æ€§åœ°æ•è·å’Œç»“åˆç›¸å…³çš„è§†è§‰ã€è¯­è¨€å’Œæ—¶åºä¿¡å·ï¼Œæé«˜äº†å†³ç­–å‡†ç¡®æ€§ã€‚åœ¨åŸºå‡†VLNæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMFRAç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ–¹æ³•å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLNä»»åŠ¡æ—¨åœ¨ä½¿å®ä½“ä»£ç†èƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤åœ¨çœŸå®ç¯å¢ƒä¸­å¯¼èˆªåˆ°ç›®æ ‡ä½ç½®ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºå…¨å±€åœºæ™¯è¡¨ç¤ºæˆ–å¯¹è±¡çº§ç‰¹å¾ï¼Œä½†ä¸è¶³ä»¥æ•æ‰è·¨æ¨¡æ€çš„å¤æ‚äº¤äº’ã€‚</li>
<li>MFRAæ¶æ„å¼•å…¥äº†ä¸€ä¸ªå¤šå±‚æ¬¡èåˆæœºåˆ¶ï¼Œèšåˆä»ä½çº§åˆ«è§†è§‰çº¿ç´¢åˆ°é«˜çº§è¯­ä¹‰æ¦‚å¿µçš„å¤šå±‚æ¬¡ç‰¹å¾ã€‚</li>
<li>MFRAè®¾è®¡äº†ä¸€ä¸ªæ¨ç†æ¨¡å—ï¼Œåˆ©ç”¨èåˆåçš„è¡¨ç¤ºè¿›è¡Œæ¨ç†ï¼Œé€šè¿‡æŒ‡ä»¤å¼•å¯¼æ³¨æ„åŠ›å’ŒåŠ¨æ€ä¸Šä¸‹æ–‡æ•´åˆæ¥æ¨æ–­å¯¼èˆªåŠ¨ä½œã€‚</li>
<li>MFRAé€šè¿‡ç»“åˆè§†è§‰ã€è¯­è¨€å’Œæ—¶åºä¿¡å·ï¼Œæé«˜äº†å¤æ‚å¯¼èˆªåœºæ™¯ä¸­çš„å†³ç­–å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨åŸºå‡†VLNæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMFRAç›¸è¾ƒäºå…¶ä»–æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16516">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e5e6f3da7fec64fe8de736d7e82768f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1224dcd24430c6e910eb778412493e41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3079044c9a74284ea7385bcbe662067.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cb32a47f4167c26ff6d5be9e3b66621.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Evaluating-Multi-Hop-Reasoning-in-Large-Language-Models-A-Chemistry-Centric-Case-Study"><a href="#Evaluating-Multi-Hop-Reasoning-in-Large-Language-Models-A-Chemistry-Centric-Case-Study" class="headerlink" title="Evaluating Multi-Hop Reasoning in Large Language Models: A   Chemistry-Centric Case Study"></a>Evaluating Multi-Hop Reasoning in Large Language Models: A   Chemistry-Centric Case Study</h2><p><strong>Authors:Mohammad Khodadad, Ali Shiraee Kasmaee, Mahdi Astaraki, Nicholas Sherck, Hamidreza Mahyar, Soheila Samiee</strong></p>
<p>In this study, we introduced a new benchmark consisting of a curated dataset and a defined evaluation process to assess the compositional reasoning capabilities of large language models within the chemistry domain. We designed and validated a fully automated pipeline, verified by subject matter experts, to facilitate this task. Our approach integrates OpenAI reasoning models with named entity recognition (NER) systems to extract chemical entities from recent literature, which are then augmented with external knowledge bases to form a comprehensive knowledge graph. By generating multi-hop questions across these graphs, we assess LLM performance in both context-augmented and non-context augmented settings. Our experiments reveal that even state-of-the-art models face significant challenges in multi-hop compositional reasoning. The results reflect the importance of augmenting LLMs with document retrieval, which can have a substantial impact on improving their performance. However, even perfect retrieval accuracy with full context does not eliminate reasoning errors, underscoring the complexity of compositional reasoning. This work not only benchmarks and highlights the limitations of current LLMs but also presents a novel data generation pipeline capable of producing challenging reasoning datasets across various domains. Overall, this research advances our understanding of reasoning in computational linguistics. </p>
<blockquote>
<p>åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•åŒ…æ‹¬ä¸€ä¸ªç²¾é€‰çš„æ•°æ®é›†å’Œä¸€ä¸ªæ˜ç¡®çš„è¯„ä¼°è¿‡ç¨‹ï¼Œä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ–å­¦é¢†åŸŸçš„ç»„åˆæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è®¾è®¡å¹¶éªŒè¯äº†ä¸€ä¸ªå…¨è‡ªåŠ¨åŒ–çš„æµç¨‹ç®¡é“ï¼Œé€šè¿‡ç›¸å…³é¢†åŸŸçš„ä¸“å®¶éªŒè¯æ¥ä¿ƒè¿›è¿™ä¸€ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†OpenAIæ¨ç†æ¨¡å‹ä¸å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ç³»ç»Ÿç›¸ç»“åˆï¼Œä»æœ€æ–°æ–‡çŒ®ä¸­æå–åŒ–å­¦å®ä½“ï¼Œç„¶åä¸å¤–éƒ¨çŸ¥è¯†åº“ç›¸ç»“åˆå½¢æˆå…¨é¢çš„çŸ¥è¯†å›¾è°±ã€‚é€šè¿‡åœ¨è¿™äº›å›¾è°±ä¸Šç”Ÿæˆå¤šè·³é—®é¢˜ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å¢å¼ºå’Œéä¸Šä¸‹æ–‡å¢å¼ºç¯å¢ƒä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æŠ€æœ¯æ¨¡å‹ä¹Ÿé¢ä¸´ç€å¤šè·³ç»„åˆæ¨ç†çš„é‡å¤§æŒ‘æˆ˜ã€‚ç»“æœåæ˜ äº†ä½¿ç”¨æ–‡æ¡£æ£€ç´¢å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„é‡è¦æ€§ï¼Œè¿™å¯èƒ½ä¼šå¯¹æ”¹å–„å…¶æ€§èƒ½äº§ç”Ÿé‡å¤§å½±å“ã€‚ç„¶è€Œï¼Œå³ä½¿åœ¨å…·æœ‰å®Œæ•´ä¸Šä¸‹æ–‡çš„å®Œç¾æ£€ç´¢å‡†ç¡®ç‡ä¸‹ï¼Œä¹Ÿä¸èƒ½æ¶ˆé™¤æ¨ç†é”™è¯¯ï¼Œè¿™å‡¸æ˜¾äº†ç»„åˆæ¨ç†çš„å¤æ‚æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ä»…è¡¡é‡å¹¶çªå‡ºäº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å±€é™æ€§ï¼Œè€Œä¸”è¿˜æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®ç”Ÿæˆæµç¨‹ç®¡é“ï¼Œèƒ½å¤Ÿåœ¨å„ä¸ªé¢†åŸŸç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†æ•°æ®é›†ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™é¡¹ç ”ç©¶æ¨åŠ¨äº†æˆ‘ä»¬å¯¹è®¡ç®—è¯­è¨€å­¦ä¸­æ¨ç†çš„ç†è§£çš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16414v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ä¸€ä¸ªç²¾é€‰çš„æ•°æ®é›†å’Œä¸€ä¸ªæ˜ç¡®çš„è¯„ä¼°è¿‡ç¨‹ï¼Œä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ–å­¦é¢†åŸŸçš„ç»„åˆæ¨ç†èƒ½åŠ›ã€‚è®¾è®¡å¹¶éªŒè¯äº†ä¸€ä¸ªå…¨è‡ªåŠ¨çš„ç®¡é“ï¼Œç»è¿‡ä¸»é¢˜ä¸“å®¶éªŒè¯ï¼Œä»¥ä¿ƒè¿›è¿™ä¸€ä»»åŠ¡ã€‚è¯¥æ–¹æ³•ç»“åˆäº†OpenAIæ¨ç†æ¨¡å‹ä¸å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ç³»ç»Ÿï¼Œä»æœ€æ–°æ–‡çŒ®ä¸­æå–åŒ–å­¦å®ä½“ï¼Œç„¶åä¸å¤–éƒ¨çŸ¥è¯†åº“ç›¸ç»“åˆï¼Œå½¢æˆå…¨é¢çš„çŸ¥è¯†å›¾è°±ã€‚é€šè¿‡åœ¨è¿™äº›å›¾ä¸Šç”Ÿæˆå¤šè·³é—®é¢˜ï¼Œæˆ‘ä»¬è¯„ä¼°äº†LLMåœ¨å¢å¼ºä¸Šä¸‹æ–‡å’Œéå¢å¼ºä¸Šä¸‹æ–‡è®¾ç½®ä¸­çš„æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¹Ÿé¢ä¸´ç€å¤šè·³ç»„åˆæ¨ç†çš„è¯¸å¤šæŒ‘æˆ˜ã€‚ç»“æœåæ˜ äº†å¢åŠ æ–‡æ¡£æ£€ç´¢å¯¹æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„é‡è¦æ€§ã€‚ç„¶è€Œï¼Œå³ä½¿è¾¾åˆ°å®Œç¾çš„æ£€ç´¢ç²¾åº¦å’Œå…¨ä¸Šä¸‹æ–‡ï¼Œä¹Ÿæ— æ³•æ¶ˆé™¤æ¨ç†é”™è¯¯ï¼Œè¿™å‡¸æ˜¾äº†ç»„åˆæ¨ç†çš„å¤æ‚æ€§ã€‚æœ¬æ–‡ä¸ä»…è¯„ä¼°å’Œçªå‡ºäº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å±€é™æ€§ï¼Œè€Œä¸”å±•ç¤ºäº†ä¸€ç§èƒ½å¤Ÿç”Ÿæˆè·¨ä¸åŒé¢†åŸŸå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†æ•°æ®é›†çš„æ–°å‹æ•°æ®ç”Ÿæˆç®¡é“ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶æ¨åŠ¨äº†è®¡ç®—è¯­è¨€å­¦ä¸­çš„æ¨ç†ç†è§£çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥æ–°çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ–å­¦é¢†åŸŸçš„ç»„åˆæ¨ç†èƒ½åŠ›ã€‚</li>
<li>è®¾è®¡å¹¶éªŒè¯äº†ä¸€ä¸ªå…¨è‡ªåŠ¨çš„æ•°æ®å¤„ç†ç®¡é“ï¼Œç”¨äºç”ŸæˆåŒ–å­¦é¢†åŸŸçš„æ¨ç†é—®é¢˜ã€‚</li>
<li>ç»“åˆOpenAIæ¨ç†æ¨¡å‹å’Œå‘½åå®ä½“è¯†åˆ«ç³»ç»Ÿæå–åŒ–å­¦å®ä½“ï¼Œå¹¶æ„å»ºçŸ¥è¯†å›¾è°±ã€‚</li>
<li>é€šè¿‡å¤šè·³é—®é¢˜è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½ï¼Œå¼ºè°ƒæ–‡æ¡£æ£€ç´¢åœ¨æ”¹å–„æ¨¡å‹æ€§èƒ½ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¹Ÿé¢ä¸´ç»„åˆæ¨ç†çš„æŒ‘æˆ˜ã€‚</li>
<li>å®Œç¾çš„æ£€ç´¢ç²¾åº¦å’Œå…¨ä¸Šä¸‹æ–‡å¹¶ä¸èƒ½å®Œå…¨æ¶ˆé™¤æ¨ç†é”™è¯¯ï¼Œå‡¸æ˜¾äº†ç»„åˆæ¨ç†çš„å¤æ‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16414">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-550a74be7242d89cf36e26d233985ae2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ae35b49b1ad542b5a98bb8d355b31be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0f469e52d14d3a4360890f989dec437.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88ef3e39b84c406f7736d022e83fe7dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68f821db2be3e910b5e9cfc06127cf6a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Less-is-More-Enhancing-Structured-Multi-Agent-Reasoning-via-Quality-Guided-Distillation"><a href="#Less-is-More-Enhancing-Structured-Multi-Agent-Reasoning-via-Quality-Guided-Distillation" class="headerlink" title="Less is More: Enhancing Structured Multi-Agent Reasoning via   Quality-Guided Distillation"></a>Less is More: Enhancing Structured Multi-Agent Reasoning via   Quality-Guided Distillation</h2><p><strong>Authors:Jiahao Yuan, Xingzhe Sun, Xing Yu, Jingwen Wang, Dehui Du, Zhiqing Cui, Zixiang Di</strong></p>
<p>The XLLM@ACL2025 Shared Task-III formulates a low-resource structural reasoning task that challenges LLMs to generate interpretable, step-by-step rationales with minimal labeled data. We present Less is More, the third-place winning approach in the XLLM@ACL2025 Shared Task-III, which focuses on structured reasoning from only 24 labeled examples. Our approach leverages a multi-agent framework with reverse-prompt induction, retrieval-augmented reasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to distill high-quality supervision across three subtasks: question parsing, CoT parsing, and step-level verification. All modules are fine-tuned from Meta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure validation with reward filtering across few-shot and zero-shot prompts, our pipeline consistently improves structure reasoning quality. These results underscore the value of controllable data distillation in enhancing structured inference under low-resource constraints. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Jiahao-Yuan/Less-is-More">https://github.com/Jiahao-Yuan/Less-is-More</a>. </p>
<blockquote>
<p>XLLM@ACL2025å…±äº«ä»»åŠ¡IIIåˆ¶å®šäº†ä¸€ä¸ªä½èµ„æºç»“æ„æ¨ç†ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡æŒ‘æˆ˜äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°‘é‡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ç”Ÿæˆå¯è§£é‡Šçš„ã€é€æ­¥çš„ç†æ€§æ¨ç†ã€‚æˆ‘ä»¬æå‡ºäº†â€å°‘å³æ˜¯å¤šâ€ï¼Œè¿™æ˜¯XLLM@ACL2025å…±äº«ä»»åŠ¡IIIçš„ç¬¬ä¸‰åè·å¥–æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆä»…å…³æ³¨äºä»24ä¸ªæ ‡æ³¨ç¤ºä¾‹ä¸­è¿›è¡Œç»“æ„åŒ–æ¨ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¤šæ™ºèƒ½ä½“æ¡†æ¶è¿›è¡Œé€†å‘æç¤ºå½’çº³ï¼Œé€šè¿‡GPT-4oå¢å¼ºæ¨ç†åˆæˆï¼Œä»¥åŠä¸¤é˜¶æ®µå¥–åŠ±å¼•å¯¼è¿‡æ»¤ï¼Œåœ¨ä¸‰ä¸ªå­ä»»åŠ¡ä¸­æç‚¼é«˜è´¨é‡ç›‘ç£ï¼šé—®é¢˜è§£æã€è®¤çŸ¥è½¨è¿¹è§£æå’Œæ­¥éª¤çº§éªŒè¯ã€‚æ‰€æœ‰æ¨¡å—éƒ½åœ¨ç»Ÿä¸€çš„LoRA+è®¾ç½®ä¸‹ä½¿ç”¨Meta-Llama-3-8B-Instructè¿›è¡Œå¾®è°ƒã€‚é€šè¿‡ç»“åˆç»“æ„éªŒè¯å’Œå¥–åŠ±è¿‡æ»¤åœ¨å°‘æ•°å’Œé›¶æ ·æœ¬æç¤ºä¹‹é—´ï¼Œæˆ‘ä»¬çš„ç®¡é“å§‹ç»ˆæé«˜äº†ç»“æ„æ¨ç†è´¨é‡ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†å¯æ§æ•°æ®è’¸é¦åœ¨å¢å¼ºä½èµ„æºçº¦æŸä¸‹çš„ç»“æ„åŒ–æ¨ç†ä¸­çš„ä»·å€¼ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Jiahao-Yuan/Less-is-More%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Jiahao-Yuan/Less-is-Moreä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16408v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¿™æ˜¯ä¸€ç¯‡å…³äºåœ¨XLLM@ACL2025å…±äº«ä»»åŠ¡ä¸­æŒ‘æˆ˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œä½èµ„æºç»“æ„æ¨ç†çš„æ–‡ç« ã€‚è¯¥æ–‡ç« æå‡ºäº†Less is Moreçš„æ–¹æ³•ï¼Œé€šè¿‡ç»“æ„åŒ–çš„æ–¹å¼ï¼Œä½¿ç”¨å°‘é‡çš„æ ‡ç­¾æ•°æ®ï¼Œåˆ©ç”¨å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€åå‘æç¤ºå½’çº³æ³•ã€GPT-4oå¢å¼ºæ¨ç†åˆæˆç­‰å®ç°é«˜è´¨é‡æ¨ç†ç›‘ç£çš„ä¸‰ä¸ªå­ä»»åŠ¡ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æé«˜ç»“æ„åŒ–æ¨ç†çš„è´¨é‡ï¼Œå¼ºè°ƒäº†åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œå¯æ§æ•°æ®è’¸é¦çš„ä»·å€¼ã€‚å…·ä½“ç»†èŠ‚å¯ä»¥é€šè¿‡è®¿é—®æ‰€æä¾›çš„é“¾æ¥è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³äºè¯¥æ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ul>
<li>XLLM@ACL2025å…±äº«ä»»åŠ¡æå‡ºäº†ä¸€ä¸ªä½èµ„æºç»“æ„æ¨ç†çš„æŒ‘æˆ˜ã€‚é‡ç‚¹åœ¨äºå¦‚ä½•åœ¨ç¼ºä¹å¤§è§„æ¨¡æ•°æ®é›†çš„æƒ…å†µä¸‹å®ç°æ¨¡å‹çš„ä¼˜ç§€æ€§èƒ½ã€‚è¯¥æŒ‘æˆ˜ç‰¹åˆ«é‡è§†æ¨¡å‹ç”Ÿæˆçš„å¯è§£é‡Šæ€§ï¼Œä»¥åŠæ­¥éª¤é—´çš„é€»è¾‘æ¨ç†ã€‚</li>
<li>Less is Moreæ–¹æ³•æˆåŠŸè·å¾—äº†è¯¥ä»»åŠ¡çš„ç¬¬ä¸‰åï¼Œå…¶æ ¸å¿ƒæ€æƒ³åœ¨äºé€šè¿‡ç»“æ„åŒ–æ–¹æ³•ä»å°‘é‡æ ‡ç­¾æ•°æ®ä¸­è·å–é«˜è´¨é‡çš„æ¨ç†ç»“æœã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡ä¸€ç§å¤šæ™ºèƒ½ä½“æ¡†æ¶å®ç°æ­¤ç›®æ ‡ï¼Œç»“åˆäº†åå‘æç¤ºå½’çº³æ³•ä»¥åŠé€šè¿‡GPT-4oå¢å¼ºæ¨ç†åˆæˆæŠ€æœ¯ã€‚è¿™äº›æ–¹æ³•ç»“åˆæˆä¸€ä¸ªç³»ç»ŸåŒ–çš„è¿‡ç¨‹ï¼Œç¡®ä¿åœ¨æœ‰é™çš„æ•°æ®æ¡ä»¶ä¸‹ä¹Ÿèƒ½è·å¾—é«˜è´¨é‡çš„æ¨ç†ç»“æœã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16408">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e8645278a22cf2cf6e3f6962e9e256bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d18922a55dba8e7096aa4eaa35157bd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c1266e302aa68a292d589d7247c120c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76c8df71fc8d5131af3b83050508ac25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-712f7c0ec721062da8b311031b9b6f25.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FinNLI-Novel-Dataset-for-Multi-Genre-Financial-Natural-Language-Inference-Benchmarking"><a href="#FinNLI-Novel-Dataset-for-Multi-Genre-Financial-Natural-Language-Inference-Benchmarking" class="headerlink" title="FinNLI: Novel Dataset for Multi-Genre Financial Natural Language   Inference Benchmarking"></a>FinNLI: Novel Dataset for Multi-Genre Financial Natural Language   Inference Benchmarking</h2><p><strong>Authors:Jabez Magomere, Elena Kochkina, Samuel Mensah, Simerjot Kaur, Charese H. Smiley</strong></p>
<p>We introduce FinNLI, a benchmark dataset for Financial Natural Language Inference (FinNLI) across diverse financial texts like SEC Filings, Annual Reports, and Earnings Call transcripts. Our dataset framework ensures diverse premise-hypothesis pairs while minimizing spurious correlations. FinNLI comprises 21,304 pairs, including a high-quality test set of 3,304 instances annotated by finance experts. Evaluations show that domain shift significantly degrades general-domain NLI performance. The highest Macro F1 scores for pre-trained (PLMs) and large language models (LLMs) baselines are 74.57% and 78.62%, respectively, highlighting the datasetâ€™s difficulty. Surprisingly, instruction-tuned financial LLMs perform poorly, suggesting limited generalizability. FinNLI exposes weaknesses in current LLMs for financial reasoning, indicating room for improvement. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†FinNLIï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé‡‘èè‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆFinNLIï¼‰çš„åŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–å„ç§é‡‘èæ–‡æœ¬ï¼Œå¦‚SECæ–‡ä»¶ã€å¹´åº¦æŠ¥å‘Šå’Œæ”¶ç›Šç”µè¯è®°å½•ã€‚æˆ‘ä»¬çš„æ•°æ®é›†æ¡†æ¶å¯ç¡®ä¿å¤šæ ·åŒ–çš„å‰æå‡è®¾å¯¹ï¼ŒåŒæ—¶æœ€å°åŒ–å¶ç„¶ç›¸å…³æ€§ã€‚FinNLIåŒ…å«21,304å¯¹æ ·æœ¬ï¼Œå…¶ä¸­åŒ…æ‹¬ç”±é‡‘èä¸“å®¶æ ‡æ³¨çš„é«˜è´¨é‡æµ‹è¯•é›†ï¼Œå…±æœ‰3,304ä¸ªå®ä¾‹ã€‚è¯„ä¼°è¡¨æ˜ï¼Œé¢†åŸŸåç§»ä¼šæ˜¾è‘—å½±å“ä¸€èˆ¬é¢†åŸŸçš„NLIæ€§èƒ½ã€‚å¯¹äºé¢„è®­ç»ƒæ¨¡å‹ï¼ˆPLMï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºçº¿ï¼Œæœ€é«˜å®F1åˆ†æ•°åˆ†åˆ«ä¸º74.57%å’Œ78.62%ï¼Œè¿™çªå‡ºäº†æ•°æ®é›†çš„éš¾åº¦ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œç»è¿‡æŒ‡ä»¤è°ƒæ•´çš„é‡‘èLLMè¡¨ç°ä¸ä½³ï¼Œè¡¨æ˜å…¶æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚FinNLIæš´éœ²äº†å½“å‰ç”¨äºé‡‘èæ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼±ç‚¹ï¼Œè¡¨æ˜ä»æœ‰æ”¹è¿›ç©ºé—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16188v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æ¨å‡ºFinNLIæ•°æ®é›†ï¼Œç”¨äºé‡‘èè‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆFinancial Natural Language Inferenceï¼ŒFinNLIï¼‰ã€‚è¯¥æ•°æ®é›†æ¶µç›–SECæ–‡ä»¶ã€å¹´æŠ¥å’Œæ”¶ç›ŠæŠ¥å‘Šç­‰å¤šç§é‡‘èæ–‡æœ¬ï¼Œç¡®ä¿å¤šæ ·åŒ–çš„å‰æå‡è®¾å¯¹ï¼ŒåŒæ—¶å‡å°‘å¶ç„¶å…³è”ã€‚åŒ…å«é«˜è´¨é‡æµ‹è¯•é›†ï¼Œå®ä¾‹æ ‡æ³¨ç”±é‡‘èä¸“å®¶å®Œæˆã€‚è¯„ä¼°æ˜¾ç¤ºé¢†åŸŸå·®å¼‚æ˜¾è‘—å½±å“ä¸€èˆ¬åŸŸNLIæ€§èƒ½ã€‚æœ€é«˜å®F1å¾—åˆ†åˆ†åˆ«ä¸ºé¢„è®­ç»ƒæ¨¡å‹ï¼ˆPLMsï¼‰å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åŸºçº¿ï¼Œåˆ†åˆ«çªå‡ºæ•°æ®é›†çš„éš¾åº¦ã€‚é‡‘èLLMè¡¨ç°ä»¤äººæƒŠè®¶åœ°ä¸ä½³ï¼Œè¯´æ˜é€šç”¨æ€§æœ‰é™ã€‚FinNLIæ­ç¤ºäº†å½“å‰é‡‘èæ¨ç†å¤§æ¨¡å‹çš„å¼±ç‚¹ï¼Œæ˜¾ç¤ºå‡ºæ”¹è¿›çš„ç©ºé—´ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>FinNLIæ˜¯ä¸€ä¸ªç”¨äºé‡‘èè‡ªç„¶è¯­è¨€æ¨ç†çš„åŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–å¤šç§é‡‘èæ–‡æœ¬ç±»å‹ã€‚</li>
<li>æ•°æ®é›†ç¡®ä¿å¤šæ ·åŒ–çš„å‰æå‡è®¾é…å¯¹ï¼ŒåŒæ—¶å‡å°‘å¶ç„¶å…³è”ã€‚</li>
<li>æ•°æ®é›†åŒ…å«ç”±é‡‘èä¸“å®¶æ ‡æ³¨çš„é«˜è´¨é‡æµ‹è¯•é›†ã€‚</li>
<li>é¢†åŸŸå·®å¼‚æ˜¾è‘—å½±å“è‡ªç„¶è¯­è¨€æ¨ç†æ€§èƒ½ã€‚</li>
<li>é¢„è®­ç»ƒæ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹åœ¨FinNLIä¸Šçš„æœ€é«˜å®F1å¾—åˆ†è¡¨æ˜æ•°æ®é›†çš„éš¾åº¦ã€‚</li>
<li>é‡‘èé¢†åŸŸçš„æŒ‡ä»¤å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°ä¸ä½³ï¼Œè¯´æ˜å…¶é€šç”¨æ€§æœ‰é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16188">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5b8c4d885ef257564def92af405024a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9ea66d2eb68710e83aaa72ea02d4832.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e22380338ccf91f1453352ea55bba2e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b65c3b44403b7f39d8979334e8493e8a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7a3cf28fa41aa382c98d594f51fcf768.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DATETIME-A-new-benchmark-to-measure-LLM-translation-and-reasoning-capabilities"><a href="#DATETIME-A-new-benchmark-to-measure-LLM-translation-and-reasoning-capabilities" class="headerlink" title="DATETIME: A new benchmark to measure LLM translation and reasoning   capabilities"></a>DATETIME: A new benchmark to measure LLM translation and reasoning   capabilities</h2><p><strong>Authors:Edward Gaere, Florian Wangenheim</strong></p>
<p>This paper introduces DATETIME, a new high-quality benchmark designed to evaluate the translation and reasoning abilities of a Large Language Model (LLM) on datetimes. A datetime is simply a date and a time, for example â€˜11th.february.2023 ,1:12:31â€™. Datetimes are an interesting domain because they are intuitive and straightforward for humans to process but present significant challenges for LLMs. At the time of writing, no publicly available benchmark exists for systematically evaluating LLMs on datetime processing. Our experiments show that state-of-the-art models exhibit significant difficulty with tasks involving reasoning on datetimes, and that General Artificial Intelligence is still a distant aspiration. We hypothesize that working with datetimes necessitates translation and&#x2F;or computation capabilities, and the tasks of the benchmark are organized accordingly. Significant dispersion in performance across models is observed with surprisingly poor performance even on apparently trivial tasks. Whilst frontier models such as ChatGPT, Claude and Llama3.1 have evidently been built and trained with datetime reasoning abilities, significant improvement is required for the open-source models. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†DATETIMEè¿™ä¸€å…¨æ–°é«˜è´¨é‡åŸºå‡†æµ‹è¯•ï¼Œå…¶ç›®çš„æ˜¯è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ—¥æœŸå’Œæ—¶é—´æ–¹é¢çš„ç¿»è¯‘å’Œæ¨ç†èƒ½åŠ›ã€‚æ—¥æœŸå’Œæ—¶é—´æ˜¯æŒ‡ä¸€ä¸ªç®€å•çš„æ—¥æœŸå’Œæ—¶é—´ç»„åˆï¼Œä¾‹å¦‚â€œ2023å¹´2æœˆ11æ—¥ï¼Œä¸‹åˆ1ç‚¹12åˆ†31ç§’â€ã€‚æ—¥æœŸæ—¶é—´æ˜¯ä¸€ä¸ªæœ‰è¶£çš„é¢†åŸŸï¼Œå› ä¸ºäººç±»å¯ä»¥å¾ˆå®¹æ˜“åœ°å¤„ç†å®ƒï¼Œä½†å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯´å´æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚åœ¨æ’°å†™æœ¬æ–‡æ—¶ï¼Œå°šæ— å…¬å¼€å¯ç”¨çš„åŸºå‡†æµ‹è¯•æ¥ç³»ç»Ÿåœ°è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†æ—¥æœŸæ—¶é—´çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹åœ¨å¤„ç†æ¶‰åŠæ—¥æœŸæ—¶é—´çš„æ¨ç†ä»»åŠ¡æ—¶é¢ä¸´å¾ˆå¤§çš„å›°éš¾ï¼Œé€šç”¨äººå·¥æ™ºèƒ½ä»ç„¶æ˜¯ä¸€ä¸ªé¥è¿œçš„æ„¿æ™¯ã€‚æˆ‘ä»¬å‡è®¾å¤„ç†æ—¥æœŸæ—¶é—´éœ€è¦ç¿»è¯‘å’Œ&#x2F;æˆ–è®¡ç®—èƒ½åŠ›ï¼ŒåŸºå‡†æµ‹è¯•çš„ä»»åŠ¡ä¹Ÿç›¸åº”åœ°è¿›è¡Œäº†ç»„ç»‡ã€‚å„æ¨¡å‹ä¹‹é—´æ€§èƒ½è¡¨ç°å‡ºæ˜æ˜¾çš„åˆ†æ•£æ€§ï¼Œç”šè‡³åœ¨è¡¨é¢ä¸Šçœ‹èµ·æ¥å¾ˆç®€å•ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¹Ÿå¾ˆç³Ÿç³•ã€‚å°½ç®¡ChatGPTã€Claudeå’ŒLlama 3.1ç­‰å‰æ²¿æ¨¡å‹æ˜¾ç„¶å·²ç»æ„å»ºå¹¶è®­ç»ƒäº†å¤„ç†æ—¥æœŸæ—¶é—´çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å¯¹äºå¼€æºæ¨¡å‹æ¥è¯´ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16155v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„é«˜è´¨é‡åŸºå‡†æµ‹è¯•DATETIMEï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†æ—¥æœŸå’Œæ—¶é—´ï¼ˆdatetimeï¼‰æ–¹é¢çš„ç¿»è¯‘å’Œæ¨ç†èƒ½åŠ›ã€‚æ—¥æœŸæ—¶é—´å¯¹äººç±»æ¥è¯´ç›´è§‚ä¸”ç®€å•å¤„ç†ï¼Œä½†å¯¹LLMå´å­˜åœ¨é‡å¤§æŒ‘æˆ˜ã€‚å½“å‰ï¼Œæ²¡æœ‰å…¬å¼€çš„åŸºå‡†æµ‹è¯•æ¥ç³»ç»Ÿåœ°è¯„ä¼°LLMåœ¨æ—¥æœŸæ—¶é—´å¤„ç†æ–¹é¢çš„èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹åœ¨æ¶‰åŠæ—¥æœŸæ—¶é—´çš„æ¨ç†ä»»åŠ¡ä¸Šå­˜åœ¨å›°éš¾ï¼Œé€šç”¨äººå·¥æ™ºèƒ½ä»æ˜¯é¥è¿œçš„ç›®æ ‡ã€‚å‡è®¾å¤„ç†æ—¥æœŸæ—¶é—´éœ€è¦ç¿»è¯‘å’Œ&#x2F;æˆ–è®¡ç®—èƒ½åŠ›ï¼Œè€ŒåŸºå‡†æµ‹è¯•çš„ä»»åŠ¡ä¹Ÿç›¸åº”å®‰æ’ã€‚ä¸åŒæ¨¡å‹çš„æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œç”šè‡³åœ¨æ˜¾ç„¶ç®€å•çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¹Ÿä»¤äººæƒŠè®¶åœ°ç³Ÿç³•ã€‚å°½ç®¡å‰æ²¿æ¨¡å‹å¦‚ChatGPTã€Claudeå’ŒLlama3.1å·²ç»å…·å¤‡æ—¥æœŸæ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œä½†å¼€æºæ¨¡å‹ä»éœ€è¦é‡å¤§æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥æ–°çš„åŸºå‡†æµ‹è¯•DATETIMEï¼Œä¸“æ³¨äºè¯„ä¼°LLMå¤„ç†æ—¥æœŸå’Œæ—¶é—´çš„èƒ½åŠ›ã€‚</li>
<li>æ—¥æœŸæ—¶é—´å¯¹äººç±»ç®€å•ï¼Œä½†å¯¹LLMå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç›®å‰æ²¡æœ‰å…¬å¼€çš„åŸºå‡†æµ‹è¯•æ¥ç³»ç»Ÿè¯„ä¼°LLMåœ¨æ—¥æœŸæ—¶é—´å¤„ç†ä¸Šçš„è¡¨ç°ã€‚</li>
<li>æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨æ—¥æœŸæ—¶é—´çš„æ¨ç†ä»»åŠ¡ä¸Šé‡åˆ°å›°éš¾ã€‚</li>
<li>å¤„ç†æ—¥æœŸæ—¶é—´éœ€è¦ç¿»è¯‘å’Œ&#x2F;æˆ–è®¡ç®—èƒ½åŠ›ã€‚</li>
<li>ä¸åŒæ¨¡å‹çš„æ€§èƒ½åœ¨å¤„ç†æ—¥æœŸæ—¶é—´ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16155">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c68a7d4c8fea3426c03bf10bdd2b4986.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c596fd70f42b3eb3f22782d06c2afd69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-597f4156892cf14c183af7705867ccc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f778e489b84b1e2823e17453811692b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7544c14fd0471d7e27f72c0dbbaca7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1f5fac30282a2c923cd4a21de5fc8ed.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Evaluating-Menu-OCR-and-Translation-A-Benchmark-for-Aligning-Human-and-Automated-Evaluations-in-Large-Vision-Language-Models"><a href="#Evaluating-Menu-OCR-and-Translation-A-Benchmark-for-Aligning-Human-and-Automated-Evaluations-in-Large-Vision-Language-Models" class="headerlink" title="Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and   Automated Evaluations in Large Vision-Language Models"></a>Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and   Automated Evaluations in Large Vision-Language Models</h2><p><strong>Authors:Zhanglin Wu, Tengfei Song, Ning Xie, Mengli Zhu, Weidong Zhang, Shuang Wu, Pengfei Li, Chong Li, Junhao Zhu, Hao Yang, Shiliang Sun</strong></p>
<p>The rapid advancement of large vision-language models (LVLMs) has significantly propelled applications in document understanding, particularly in optical character recognition (OCR) and multilingual translation. However, current evaluations of LVLMs, like the widely used OCRBench, mainly focus on verifying the correctness of their short-text responses and long-text responses with simple layout, while the evaluation of their ability to understand long texts with complex layout design is highly significant but largely overlooked. In this paper, we propose Menu OCR and Translation Benchmark (MOTBench), a specialized evaluation framework emphasizing the pivotal role of menu translation in cross-cultural communication. MOTBench requires LVLMs to accurately recognize and translate each dish, along with its price and unit items on a menu, providing a comprehensive assessment of their visual understanding and language processing capabilities. Our benchmark is comprised of a collection of Chinese and English menus, characterized by intricate layouts, a variety of fonts, and culturally specific elements across different languages, along with precise human annotations. Experiments show that our automatic evaluation results are highly consistent with professional human evaluation. We evaluate a range of publicly available state-of-the-art LVLMs, and through analyzing their output to identify the strengths and weaknesses in their performance, offering valuable insights to guide future advancements in LVLM development. MOTBench is available at <a target="_blank" rel="noopener" href="https://github.com/gitwzl/MOTBench">https://github.com/gitwzl/MOTBench</a>. </p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å¿«é€Ÿå‘å±•æå¤§åœ°æ¨åŠ¨äº†æ–‡æ¡£ç†è§£åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰å’Œå¤šè¯­ç§ç¿»è¯‘é¢†åŸŸã€‚ç„¶è€Œï¼Œç›®å‰å¯¹LVLMsçš„è¯„ä¼°ï¼Œå¦‚å¹¿æ³›ä½¿ç”¨çš„OCRBenchï¼Œä¸»è¦ä¾§é‡äºéªŒè¯å…¶ç®€çŸ­æ–‡æœ¬å›åº”å’Œç®€å•å¸ƒå±€é•¿æ–‡æœ¬å›åº”çš„æ­£ç¡®æ€§ï¼Œè€Œå¯¹äºä»–ä»¬ç†è§£å…·æœ‰å¤æ‚å¸ƒå±€è®¾è®¡é•¿æ–‡æœ¬çš„èƒ½åŠ›è¯„ä¼°è‡³å…³é‡è¦ï¼Œå´è¢«å¤§å¤§å¿½è§†äº†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†èœå•OCRå’Œç¿»è¯‘åŸºå‡†æµ‹è¯•ï¼ˆMOTBenchï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°è·¨æ–‡åŒ–äº¤æµä¸­èœå•ç¿»è¯‘çš„é‡è¦ä½œç”¨çš„è¯„ä¼°æ¡†æ¶ã€‚MOTBenchè¦æ±‚LVLMså‡†ç¡®è¯†åˆ«å¹¶ç¿»è¯‘èœå•ä¸Šçš„æ¯ä¸ªèœå“ã€ä»·æ ¼ä»¥åŠå•ä½é¡¹ç›®ï¼Œä»è€Œå…¨é¢è¯„ä¼°å…¶è§†è§‰ç†è§£å’Œè¯­è¨€å¤„ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•åŒ…å«ä¸€ç³»åˆ—å…·æœ‰å¤æ‚å¸ƒå±€ã€å¤šç§å­—ä½“ä»¥åŠä¸åŒè¯­è¨€æ–‡åŒ–ç‰¹å®šå…ƒç´ çš„ä¸­æ–‡å’Œè‹±æ–‡èœå•ï¼Œä»¥åŠç²¾ç¡®çš„äººç±»æ³¨é‡Šã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è‡ªåŠ¨è¯„ä¼°ç»“æœä¸ä¸“ä¸šäººå·¥è¯„ä¼°é«˜åº¦ä¸€è‡´ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸€ç³»åˆ—å…¬å¼€å‘å¸ƒçš„å…ˆè¿›LVLMsï¼Œé€šè¿‡åˆ†æä»–ä»¬çš„è¾“å‡ºæ¥ç¡®å®šå…¶æ€§èƒ½çš„ä¼˜ç¼ºç‚¹ï¼Œä¸ºLVLMæœªæ¥çš„å‘å±•æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚MOTBenchå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/gitwzl/MOTBench%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/gitwzl/MOTBenchè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13945v3">PDF</a> 12 pages, 5 figures, 5 Tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶Menu OCRå’Œç¿»è¯‘åŸºå‡†æµ‹è¯•ï¼ˆMOTBenchï¼‰ï¼Œä¸“æ³¨äºèœå•ç¿»è¯‘çš„è·¨æ–‡åŒ–ä¼ æ’­é‡è¦æ€§ã€‚è¯¥æ¡†æ¶å¼ºè°ƒå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å¯¹èœå•ä¸Šçš„èœå“ã€ä»·æ ¼åŠå•ä½é¡¹çš„å‡†ç¡®è¯†åˆ«å’Œç¿»è¯‘èƒ½åŠ›ï¼Œå…¨é¢è¯„ä¼°å…¶è§†è§‰ç†è§£å’Œè¯­è¨€å¤„ç†èƒ½åŠ›ã€‚MOTBenchåŒ…å«ä¸­è‹±æ–‡èœå•ï¼Œå…·æœ‰å¤æ‚çš„å¸ƒå±€ã€å¤šç§å­—ä½“å’Œè·¨æ–‡åŒ–å…ƒç´ ï¼Œä»¥åŠç²¾ç¡®çš„äººå·¥æ³¨é‡Šã€‚å®éªŒè¡¨æ˜ï¼Œå…¶è‡ªåŠ¨è¯„ä¼°ç»“æœä¸ä¸“ä¸šäººå·¥è¯„ä¼°é«˜åº¦ä¸€è‡´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨æ–‡æ¡£ç†è§£åº”ç”¨ï¼Œå¦‚å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰å’Œå¤šè¯­ç§ç¿»è¯‘æ–¹é¢å–å¾—å¿«é€Ÿè¿›å±•ã€‚</li>
<li>ç›®å‰å¯¹LVLMsçš„è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨ç®€å•æ–‡æœ¬å“åº”çš„æ­£ç¡®æ€§ä¸Šï¼Œä½†å¯¹äºå¤„ç†å¤æ‚å¸ƒå±€çš„é•¿æ–‡æœ¬æ–‡æ¡£çš„èƒ½åŠ›è¯„ä¼°è¢«å¿½è§†ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶MOTBenchï¼Œä¸“æ³¨äºèœå•ç¿»è¯‘çš„è·¨æ–‡åŒ–ä¼ æ’­é‡è¦æ€§ã€‚</li>
<li>MOTBenchè¦æ±‚LVLMså‡†ç¡®è¯†åˆ«å’Œç¿»è¯‘èœå•ä¸Šçš„èœå“ã€ä»·æ ¼åŠå•ä½é¡¹ã€‚</li>
<li>MOTBenchåŒ…å«å…·æœ‰å¤æ‚å¸ƒå±€ã€å¤šç§å­—ä½“å’Œè·¨æ–‡åŒ–å…ƒç´ çš„ä¸­è‹±æ–‡èœå•ï¼Œä»¥åŠç²¾ç¡®çš„äººå·¥æ³¨é‡Šã€‚</li>
<li>è‡ªåŠ¨è¯„ä¼°ä¸ä¸“ä¸šçš„æ‰‹åŠ¨è¯„ä¼°ç»“æœé«˜åº¦ä¸€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13945">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-45468dfb5ef8e4a47afbdfe5b14c3f6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-829c697fef70f51df49f75b03a16332a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d60ee56e5714908066d10c13d26bac4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a038a47b8b58911626c3b2a8c080a5f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-65ddcdb91972372f442096eb0830d400.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5bb69a11481c6fb054eb3298ae2c4f0a.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Textual-Reasoning-for-Few-shot-Temporal-Action-Localization"><a href="#Chain-of-Thought-Textual-Reasoning-for-Few-shot-Temporal-Action-Localization" class="headerlink" title="Chain-of-Thought Textual Reasoning for Few-shot Temporal Action   Localization"></a>Chain-of-Thought Textual Reasoning for Few-shot Temporal Action   Localization</h2><p><strong>Authors:Hongwei Ji, Wulian Yun, Mengshi Qi, Huadong Ma</strong></p>
<p>Traditional temporal action localization (TAL) methods rely on large amounts of detailed annotated data, whereas few-shot TAL reduces this dependence by using only a few training samples to identify unseen action categories. However, existing few-shot TAL methods typically focus solely on video-level information, neglecting textual information, which can provide valuable semantic support for the localization task. Therefore, we propose a new few-shot temporal action localization method by Chain-of-Thought textual reasoning to improve localization performance. Specifically, we design a novel few-shot learning framework that leverages textual semantic information to enhance the modelâ€™s ability to capture action commonalities and variations, which includes a semantic-aware text-visual alignment module designed to align the query and support videos at different levels. Meanwhile, to better express the temporal dependencies and causal relationships between actions at the textual level to assist action localization, we design a Chain of Thought (CoT)-like reasoning method that progressively guides the Vision Language Model (VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for videos. The generated texts can capture more variance of action than visual features. We conduct extensive experiments on the publicly available ActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named Human-related Anomaly Localization and explore the application of the TAL task in human anomaly detection. The experimental results demonstrate that our proposed method significantly outperforms existing methods in single-instance and multi-instance scenarios. We will release our code, data and benchmark. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„æ—¶é—´åŠ¨ä½œå®šä½ï¼ˆTALï¼‰æ–¹æ³•ä¾èµ–äºå¤§é‡çš„è¯¦ç»†æ ‡æ³¨æ•°æ®ï¼Œè€Œå°‘æ ·æœ¬TALåˆ™é€šè¿‡ä»…ä½¿ç”¨å°‘é‡è®­ç»ƒæ ·æœ¬æ¥è¯†åˆ«æœªè§è¿‡çš„åŠ¨ä½œç±»åˆ«ï¼Œé™ä½äº†å¯¹è¿™ç§ä¾èµ–ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å°‘æ ·æœ¬TALæ–¹æ³•é€šå¸¸åªå…³æ³¨è§†é¢‘å±‚é¢çš„ä¿¡æ¯ï¼Œå¿½è§†äº†æ–‡æœ¬ä¿¡æ¯ï¼Œè¿™å¯ä»¥ä¸ºå®šä½ä»»åŠ¡æä¾›æœ‰ä»·å€¼çš„è¯­ä¹‰æ”¯æŒã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å°‘æ ·æœ¬æ—¶é—´åŠ¨ä½œå®šä½æ–¹æ³•ï¼Œé€šè¿‡é“¾å¼æ€ç»´æ–‡æœ¬æ¨ç†æ¥æ”¹å–„å®šä½æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ–°é¢–çš„å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ–‡æœ¬è¯­ä¹‰ä¿¡æ¯æ¥æé«˜æ¨¡å‹æ•æ‰åŠ¨ä½œå…±æ€§å’Œå˜åŒ–çš„èƒ½åŠ›ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ä¸ªè¯­ä¹‰æ„ŸçŸ¥çš„æ–‡æœ¬-è§†è§‰å¯¹é½æ¨¡å—ï¼Œæ—¨åœ¨åœ¨ä¸åŒå±‚æ¬¡ä¸Šå¯¹æŸ¥è¯¢å’Œæ”¯æŒè§†é¢‘è¿›è¡Œå¯¹é½ã€‚åŒæ—¶ï¼Œä¸ºäº†æ›´å¥½åœ°åœ¨æ–‡æœ¬å±‚é¢è¡¨è¾¾åŠ¨ä½œä¹‹é—´çš„æ—¶é—´ä¾èµ–å…³ç³»å’Œå› æœå…³ç³»ï¼Œä»¥è¾…åŠ©åŠ¨ä½œå®šä½ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç±»ä¼¼é“¾å¼æ€ç»´ï¼ˆCoTï¼‰çš„æ¨ç†æ–¹æ³•ï¼Œé€æ­¥å¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºè§†é¢‘ç”Ÿæˆç±»ä¼¼CoTçš„æ–‡æœ¬æè¿°ã€‚ç”Ÿæˆçš„æ–‡æœ¬å¯ä»¥æ•æ‰æ¯”è§†è§‰ç‰¹å¾æ›´å¤šçš„åŠ¨ä½œå˜åŒ–ã€‚æˆ‘ä»¬åœ¨å…¬å¼€å¯ç”¨çš„ActivityNet1.3å’ŒTHUMOS14æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚æˆ‘ä»¬å¼•å…¥äº†åä¸ºHuman-related Anomaly Localizationçš„ç¬¬ä¸€ä¸ªæ•°æ®é›†ï¼Œå¹¶æ¢ç´¢äº†TALä»»åŠ¡åœ¨äººç±»å¼‚å¸¸æ£€æµ‹ä¸­çš„åº”ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å•å®ä¾‹å’Œå¤šå®ä¾‹åœºæ™¯ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬å°†å…¬å¼€æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’ŒåŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13460v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºChain-of-Thoughtæ–‡æœ¬æ¨ç†çš„æ–°æ–¹æ³•æ¥è§£å†³å°‘æ ·æœ¬æ¡ä»¶ä¸‹çš„æ—¶åºåŠ¨ä½œå®šä½é—®é¢˜ã€‚è¯¥è®¾è®¡åŒ…æ‹¬è¯­ä¹‰æ„ŸçŸ¥çš„æ–‡æœ¬è§†è§‰å¯¹é½æ¨¡å—ä¸è®¤çŸ¥é“¾å¼çš„æ¨ç†æ¨¡å‹ã€‚è¿™ä¸€æ–°ç­–ç•¥å€ŸåŠ©å°‘é‡è®­ç»ƒæ ·æœ¬å³èƒ½æœ‰æ•ˆå®šä½æœªè§‚å¯Ÿåˆ°çš„åŠ¨ä½œç±»åˆ«ï¼Œé€šè¿‡æ–‡æœ¬è¯­ä¹‰ä¿¡æ¯æ•æ‰åŠ¨ä½œçš„å…±åŒç‚¹å’Œå˜åŒ–ï¼Œå¹¶åœ¨ä¸åŒå±‚çº§ä¸Šå¯¹é½æŸ¥è¯¢å’Œæ”¯æ’‘è§†é¢‘ã€‚åŒæ—¶ï¼Œè®¾è®¡è®¤çŸ¥é“¾å¼æ¨ç†æ¨¡å‹ï¼Œä»¥é€æ­¥å¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆé’ˆå¯¹è§†é¢‘çš„è®¤çŸ¥é“¾å¼æ–‡æœ¬æè¿°ï¼Œæ•è·åŠ¨ä½œçš„æ›´å¤šå·®å¼‚ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ–°æ„å»ºçš„å¼‚å¸¸æ£€æµ‹æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨å•å®ä¾‹å’Œå¤šå®ä¾‹åœºæ™¯ä¸‹å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºä¸€ç§åŸºäºChain-of-Thoughtæ–‡æœ¬æ¨ç†çš„å°‘æ ·æœ¬æ—¶åºåŠ¨ä½œå®šä½æ–¹æ³•ã€‚</li>
<li>è®¾è®¡æ–°çš„å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨æ–‡æœ¬è¯­ä¹‰ä¿¡æ¯æå‡æ¨¡å‹æ•æ‰åŠ¨ä½œå…±åŒç‚¹å’Œå˜åŒ–çš„èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨è¯­ä¹‰æ„ŸçŸ¥çš„æ–‡æœ¬è§†è§‰å¯¹é½æ¨¡å—ï¼Œå¯¹é½æŸ¥è¯¢å’Œæ”¯æ’‘è§†é¢‘çš„ä¸åŒå±‚çº§ä¿¡æ¯ã€‚</li>
<li>åˆ©ç”¨è®¤çŸ¥é“¾å¼æ¨ç†æ¨¡å‹ï¼Œåœ¨æ–‡æœ¬å±‚é¢è¡¨è¾¾åŠ¨ä½œçš„æ—¶ç©ºä¾èµ–å’Œå› æœå…³ç³»ã€‚</li>
<li>é€šè¿‡ç”Ÿæˆæ–‡æœ¬æè¿°æ•è·æ›´å¤šåŠ¨ä½œå·®å¼‚ç‰¹å¾ï¼Œä¼˜äºä»…ä¾èµ–è§†è§‰ç‰¹å¾çš„æ–¹æ³•ã€‚</li>
<li>åœ¨ActivityNet1.3å’ŒTHUMOS14å…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒéªŒè¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13460">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c82b1047594130439a760a575b62acce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-531c4a7812ecaeccde9b4e989a71d861.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7c213dbbcc1e0229555b467cf4aa97d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="OnRL-RAG-Real-Time-Personalized-Mental-Health-Dialogue-System"><a href="#OnRL-RAG-Real-Time-Personalized-Mental-Health-Dialogue-System" class="headerlink" title="OnRL-RAG: Real-Time Personalized Mental Health Dialogue System"></a>OnRL-RAG: Real-Time Personalized Mental Health Dialogue System</h2><p><strong>Authors:Ahsan Bilal, Beiyu Lin</strong></p>
<p>Large language models (LLMs) have been widely used for various tasks and applications. However, LLMs and fine-tuning are limited to the pre-trained data. For example, ChatGPTâ€™s world knowledge until 2021 can be outdated or inaccurate. To enhance the capabilities of LLMs, Retrieval-Augmented Generation (RAG), is proposed to augment LLMs with additional, new, latest details and information to LLMs. While RAG offers the correct information, it may not best present it, especially to different population groups with personalizations. Reinforcement Learning from Human Feedback (RLHF) adapts to user needs by aligning model responses with human preference through feedback loops. In real-life applications, such as mental health problems, a dynamic and feedback-based model would continuously adapt to new information and offer personalized assistance due to complex factors fluctuating in a daily environment. Thus, we propose an Online Reinforcement Learning-based Retrieval-Augmented Generation (OnRL-RAG) system to detect and personalize the responding systems to mental health problems, such as stress, anxiety, and depression. We use an open-source dataset collected from 2028 College Students with 28 survey questions for each student to demonstrate the performance of our proposed system with the existing systems. Our system achieves superior performance compared to standard RAG and simple LLM via GPT-4o, GPT-4o-mini, Gemini-1.5, and GPT-3.5. This work would open up the possibilities of real-life applications of LLMs for personalized services in the everyday environment. The results will also help researchers in the fields of sociology, psychology, and neuroscience to align their theories more closely with the actual human daily environment. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å¹¿æ³›åº”ç”¨äºå„ç§ä»»åŠ¡å’Œåº”ç”¨ã€‚ç„¶è€Œï¼ŒLLMå’Œå¾®è°ƒéƒ½å—é™äºé¢„è®­ç»ƒæ•°æ®ã€‚ä¾‹å¦‚ï¼ŒChatGPTæˆªè‡³2021å¹´çš„ä¸–ç•ŒçŸ¥è¯†å¯èƒ½ä¼šè¿‡æ—¶æˆ–ä¸å‡†ç¡®ã€‚ä¸ºäº†å¢å¼ºLLMçš„èƒ½åŠ›ï¼Œæå‡ºäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¥å‘LLMæ·»åŠ é¢å¤–ã€æœ€æ–°ã€æœ€æ–°çš„ç»†èŠ‚å’Œä¿¡æ¯ã€‚è™½ç„¶RAGæä¾›äº†æ­£ç¡®çš„ä¿¡æ¯ï¼Œä½†å®ƒå¯èƒ½æ— æ³•æœ€å¥½åœ°å‘ˆç°å®ƒï¼Œå°¤å…¶æ˜¯å¯¹äºå…·æœ‰ä¸ªæ€§åŒ–çš„ä¸åŒäººç¾¤ã€‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰é€šè¿‡åé¦ˆå¾ªç¯ä½¿æ¨¡å‹å“åº”ä¸äººç±»åå¥½ç›¸é€‚åº”ï¼Œä»è€Œé€‚åº”ç”¨æˆ·éœ€æ±‚ã€‚åœ¨ç°å®ç”Ÿæ´»åº”ç”¨ï¼Œå¦‚å¿ƒç†å¥åº·é—®é¢˜ä¸­ï¼Œä¸€ä¸ªåŠ¨æ€ä¸”åŸºäºåé¦ˆçš„æ¨¡å‹å°†ä¸æ–­é€‚åº”æ–°ä¿¡æ¯ï¼Œå¹¶æä¾›ä¸ªæ€§åŒ–çš„å¸®åŠ©ï¼Œè¿™æ˜¯ç”±äºæ—¥å¸¸ç¯å¢ƒä¸­å¤æ‚å› ç´ çš„æ³¢åŠ¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆOnRL-RAGï¼‰ç³»ç»Ÿï¼Œç”¨äºæ£€æµ‹å’Œä¸ªæ€§åŒ–åº”å¯¹å¿ƒç†å¥åº·é—®é¢˜ï¼Œå¦‚å‹åŠ›ã€ç„¦è™‘å’ŒæŠ‘éƒã€‚æˆ‘ä»¬ä½¿ç”¨ä»2028åå¤§å­¦ç”Ÿæ”¶é›†çš„å¼€æºæ•°æ®é›†è¿›è¡Œæ¼”ç¤ºï¼Œæ¯ä¸ªå­¦ç”Ÿæ¥å—28ä¸ªè°ƒæŸ¥é—®é¢˜ä»¥å±•ç¤ºæˆ‘ä»¬ç³»ç»Ÿçš„æ€§èƒ½ä¸ç°æœ‰ç³»ç»Ÿç›¸æ¯”ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨ä¸æ ‡å‡†RAGå’Œç®€å•çš„LLMï¼ˆå¦‚GPT-4oã€GPT-4o-miniã€Gemini-1.5å’ŒGPT-3.5ï¼‰ç›¸æ¯”æ—¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œå°†ä¸ºLLMåœ¨æ—¥å¸¸ç”Ÿæ´»ç¯å¢ƒä¸­çš„ä¸ªæ€§åŒ–æœåŠ¡æä¾›å®é™…åº”ç”¨çš„å¯èƒ½æ€§ã€‚ç»“æœè¿˜å°†å¸®åŠ©ç¤¾ä¼šå­¦ã€å¿ƒç†å­¦å’Œç¥ç»ç§‘å­¦é¢†åŸŸçš„ç ”ç©¶äººå‘˜å°†å…¶ç†è®ºæ›´åŠ ç´§å¯†åœ°ä¸å®é™…çš„æ—¥å¸¸äººç±»ç¯å¢ƒç›¸ç»“åˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02894v3">PDF</a> It needs more revisions. I am currently working on it with my   co-author</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¹¿æ³›åº”ç”¨äºå„ç§ä»»åŠ¡å’Œåº”ç”¨ï¼Œä½†å…¶çŸ¥è¯†å’Œèƒ½åŠ›å—é™äºé¢„è®­ç»ƒæ•°æ®ã€‚ä¸ºå¢å¼ºLLMsçš„èƒ½åŠ›ï¼Œæå‡ºäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œå¹¶ç»“åˆå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä»¥é€‚åº”ä¸åŒç”¨æˆ·çš„éœ€æ±‚ã€‚é’ˆå¯¹çœŸå®ä¸–ç•Œåº”ç”¨å¦‚å¿ƒç†å¥åº·é—®é¢˜ï¼Œæå‡ºåŸºäºåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿï¼ˆOnRL-RAGï¼‰ï¼Œç”¨äºæ£€æµ‹å¹¶ä¸ªæ€§åŒ–å“åº”ç³»ç»Ÿä»¥åº”å¯¹å¦‚å‹åŠ›ã€ç„¦è™‘å’ŒæŠ‘éƒç­‰å¿ƒç†é—®é¢˜ã€‚ä½¿ç”¨æ¥è‡ª2028åå¤§å­¦ç”Ÿçš„å¼€æºæ•°æ®é›†å±•ç¤ºç³»ç»Ÿæ€§èƒ½ï¼Œç›¸è¾ƒäºå…¶ä»–ç³»ç»Ÿå±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æ­¤ç ”ç©¶å¼€å¯äº†LLMsåœ¨ä¸ªæ€§åŒ–æœåŠ¡ä¸­çš„å®é™…åº”ç”¨å¯èƒ½æ€§ï¼Œå¹¶æœ‰åŠ©äºç¤¾ä¼šå­¦ã€å¿ƒç†å­¦å’Œç¥ç»ç§‘å­¦é¢†åŸŸçš„ç ”ç©¶è€…æ›´è´´è¿‘å®é™…äººç±»æ—¥å¸¸ç¯å¢ƒè¿›è¡Œç†è®ºç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså—é™äºé¢„è®­ç»ƒæ•°æ®ï¼ŒçŸ¥è¯†å’Œèƒ½åŠ›æœ‰é™ã€‚</li>
<li>RAGæ–¹æ³•ç”¨äºå¢å¼ºLLMsçš„èƒ½åŠ›ï¼Œæä¾›æœ€æ–°ä¿¡æ¯ã€‚</li>
<li>RLHFç»“åˆï¼Œä½¿æ¨¡å‹é€‚åº”ä¸åŒç”¨æˆ·éœ€æ±‚ã€‚</li>
<li>æå‡ºOnRL-RAGç³»ç»Ÿï¼Œç”¨äºæ£€æµ‹å’Œä¸ªæ€§åŒ–å“åº”å¿ƒç†å¥åº·é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨å¤§å­¦ç”Ÿæ•°æ®é›†å±•ç¤ºç³»ç»Ÿæ€§èƒ½ï¼Œä¼˜äºå…¶ä»–ç³»ç»Ÿã€‚</li>
<li>ç ”ç©¶å¼€å¯äº†LLMsåœ¨ä¸ªæ€§åŒ–æœåŠ¡ä¸­çš„å®é™…åº”ç”¨å¯èƒ½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02894">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c8a98243aaff6eae1eeb5298102889b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c7fee3d6574c7b44579867702d433e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec1ff3bb0a697938e71c8865e7f52fc9.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="F5R-TTS-Improving-Flow-Matching-based-Text-to-Speech-with-Group-Relative-Policy-Optimization"><a href="#F5R-TTS-Improving-Flow-Matching-based-Text-to-Speech-with-Group-Relative-Policy-Optimization" class="headerlink" title="F5R-TTS: Improving Flow-Matching based Text-to-Speech with Group   Relative Policy Optimization"></a>F5R-TTS: Improving Flow-Matching based Text-to-Speech with Group   Relative Policy Optimization</h2><p><strong>Authors:Xiaohui Sun, Ruitong Xiao, Jianye Mo, Bowen Wu, Qun Yu, Baoxun Wang</strong></p>
<p>We present F5R-TTS, a novel text-to-speech (TTS) system that integrates Group Relative Policy Optimization (GRPO) into a flow-matching based architecture. By reformulating the deterministic outputs of flow-matching TTS into probabilistic Gaussian distributions, our approach enables seamless integration of reinforcement learning algorithms. During pretraining, we train a probabilistically reformulated flow-matching based model which is derived from F5-TTS with an open-source dataset. In the subsequent reinforcement learning (RL) phase, we employ a GRPO-driven enhancement stage that leverages dual reward metrics: word error rate (WER) computed via automatic speech recognition and speaker similarity (SIM) assessed by verification models. Experimental results on zero-shot voice cloning demonstrate that F5R-TTS achieves significant improvements in both speech intelligibility (a 29.5% relative reduction in WER) and speaker similarity (a 4.6% relative increase in SIM score) compared to conventional flow-matching based TTS systems. Audio samples are available at <a target="_blank" rel="noopener" href="https://frontierlabs.github.io/F5R">https://frontierlabs.github.io/F5R</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†F5R-TTSï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿï¼Œå®ƒå°†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰é›†æˆåˆ°åŸºäºæµåŒ¹é…çš„æ¶æ„ä¸­ã€‚é€šè¿‡è®²æµåŒ¹é…TTSçš„ç¡®å®šæ€§è¾“å‡ºé‡æ–°å…¬å¼åŒ–ä¸ºæ¦‚ç‡é«˜æ–¯åˆ†å¸ƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ— ç¼é›†æˆã€‚åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å…¬å¼€æ•°æ®é›†å¯¹æ¥è‡ªF5-TTSçš„æ¦‚ç‡é‡æ„æµåŒ¹é…æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚åœ¨éšåçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨GRPOé©±åŠ¨çš„å¢å¼ºé˜¶æ®µï¼Œåˆ©ç”¨åŒé‡å¥–åŠ±æŒ‡æ ‡ï¼šé€šè¿‡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«è®¡ç®—çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å’Œé€šè¿‡éªŒè¯æ¨¡å‹è¯„ä¼°çš„å‘éŸ³äººç›¸ä¼¼æ€§ï¼ˆSIMï¼‰ã€‚é›¶æ ·æœ¬å£°éŸ³å…‹éš†çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„åŸºäºæµåŒ¹é…çš„TTSç³»ç»Ÿç›¸æ¯”ï¼ŒF5R-TTSåœ¨è¯­éŸ³æ¸…æ™°åº¦ï¼ˆç›¸å¯¹å‡å°‘29.5%çš„WERï¼‰å’Œå‘éŸ³äººç›¸ä¼¼æ€§ï¼ˆSIMå¾—åˆ†ç›¸å¯¹æé«˜4.6%ï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://frontierlabs.github.io/F5R%E8%8E%B7%E5%8F%96%E3%80%82">https://frontierlabs.github.io/F5Rè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02407v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>F5R-TTSæ˜¯ä¸€ä¸ªé›†æˆGroup Relative Policy Optimization (GRPO)çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿã€‚å®ƒé€šè¿‡æ¦‚ç‡åŒ–æ”¹é©æµåŒ¹é…TTSçš„ç¡®å®šæ€§è¾“å‡ºæ¥å®ç°å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ— ç¼é›†æˆã€‚ç³»ç»Ÿä½¿ç”¨æ¦‚ç‡åŒ–æ”¹é©çš„æµåŒ¹é…æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶å¼•å…¥åŸºäºGRPOçš„å¢å¼ºé˜¶æ®µä»¥æå‡æ€§èƒ½ï¼Œé€šè¿‡é‡‡ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«çš„è¯é”™è¯¯ç‡å’ŒéªŒè¯æ¨¡å‹çš„è¯´è¯äººç›¸ä¼¼æ€§ä½œä¸ºåŒé‡å¥–åŠ±æŒ‡æ ‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒF5R-TTSåœ¨é›¶æ ·æœ¬è¯­éŸ³å…‹éš†ä¸Šæ˜¾è‘—æé«˜äº†è¯­éŸ³çš„æ¸…æ™°åº¦å’Œè¯´è¯äººçš„ç›¸ä¼¼æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>F5R-TTSæ˜¯ä¸€ä¸ªæ–°çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿï¼Œæ•´åˆäº†Group Relative Policy Optimization (GRPO)ã€‚</li>
<li>ç³»ç»Ÿé‡‡ç”¨æµåŒ¹é…æ¶æ„ï¼Œå¹¶é€šè¿‡æ¦‚ç‡åŒ–æ”¹é©ç¡®å®šæ€§è¾“å‡ºæ¥å®ç°å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ— ç¼é›†æˆã€‚</li>
<li>åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨æ¦‚ç‡åŒ–æ”¹é©çš„æµåŒ¹é…æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ¨¡å‹åŸºäºF5-TTSå¹¶ä½¿ç”¨äº†å¼€æºæ•°æ®é›†ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ é˜¶æ®µé‡‡ç”¨GRPOé©±åŠ¨çš„å¢å¼ºé˜¶æ®µï¼Œåˆ©ç”¨è¯é”™è¯¯ç‡å’Œè¯´è¯äººç›¸ä¼¼æ€§ä½œä¸ºåŒé‡å¥–åŠ±æŒ‡æ ‡ã€‚</li>
<li>F5R-TTSå®ç°äº†é›¶æ ·æœ¬è¯­éŸ³å…‹éš†çš„æ˜¾è‘—æ€§èƒ½æå‡ï¼Œåœ¨è¯­éŸ³æ¸…æ™°åº¦å’Œè¯´è¯äººç›¸ä¼¼æ€§æ–¹é¢éƒ½æœ‰æ˜æ˜¾æ”¹å–„ã€‚</li>
<li>F5R-TTSç³»ç»Ÿçš„éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://frontierlabs.github.io/F5R%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://frontierlabs.github.io/F5Rä¸Šæ‰¾åˆ°ã€‚</a></li>
<li>è¯¥ç³»ç»Ÿä¸ºæ–‡æœ¬è½¬è¯­éŸ³æŠ€æœ¯çš„æ–°å‘å±•æä¾›äº†æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­éŸ³å…‹éš†å’Œè‡ªç„¶æ€§æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02407">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6732a7cd9ac0e5706df1996abc35d1fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7fadc489444d8b07a9c1126e55aec6bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9219b3f94034443a15eaf5a1e73ff82b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23ad0ad70d5d46832db0d11e5dfb6ea0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d12144657fdb0de65af673114315066a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Accelerate-Parallelizable-Reasoning-via-Parallel-Decoding-within-One-Sequence"><a href="#Accelerate-Parallelizable-Reasoning-via-Parallel-Decoding-within-One-Sequence" class="headerlink" title="Accelerate Parallelizable Reasoning via Parallel Decoding within One   Sequence"></a>Accelerate Parallelizable Reasoning via Parallel Decoding within One   Sequence</h2><p><strong>Authors:Yijiong Yu</strong></p>
<p>Recent advances in reasoning models have demonstrated significant improvements in accuracy, particularly for complex tasks such as mathematical reasoning, by employing detailed and comprehensive reasoning processes. However, generating these lengthy reasoning sequences is computationally expensive and time-consuming. To address this inefficiency, we leverage the inherent parallelizability of certain tasks to accelerate the reasoning process. Specifically, when multiple parallel reasoning branches exist, we decode multiple tokens per step using a specialized attention mask, processing them within a single sequence, avoiding additional memory usage. Experimental results show that our method achieves over 100% speedup in decoding time while maintaining the answer quality. </p>
<blockquote>
<p>è¿‘æœŸæ¨ç†æ¨¡å‹çš„å‘å±•æ˜¾ç¤ºï¼Œé€šè¿‡é‡‡ç”¨è¯¦ç»†è€Œå…¨é¢çš„æ¨ç†è¿‡ç¨‹ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦æ¨ç†ç­‰å¤æ‚ä»»åŠ¡æ–¹é¢ï¼Œå…¶å‡†ç¡®æ€§æœ‰äº†æ˜¾è‘—çš„æé«˜ã€‚ç„¶è€Œï¼Œç”Ÿæˆè¿™äº›å†—é•¿çš„æ¨ç†åºåˆ—åœ¨è®¡ç®—ä¸Šå¾ˆæ˜‚è´µä¸”è€—æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ç§ä½æ•ˆé—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨æŸäº›ä»»åŠ¡çš„å›ºæœ‰å¹¶è¡Œæ€§æ¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œå½“å­˜åœ¨å¤šä¸ªå¹¶è¡Œæ¨ç†åˆ†æ”¯æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸“é—¨çš„æ³¨æ„åŠ›æ©ç æ¥ä¸€æ¬¡æ€§å¤„ç†å¤šä¸ªæ ‡è®°ï¼Œé¿å…é¢å¤–çš„å†…å­˜ä½¿ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§£ç æ—¶é—´ä¸Šå®ç°äº†è¶…è¿‡100%çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒäº†ç­”æ¡ˆçš„è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20533v3">PDF</a> Our code is available in   <a target="_blank" rel="noopener" href="https://github.com/yuyijiong/parallel-decoding-in-one-sequence">https://github.com/yuyijiong/parallel-decoding-in-one-sequence</a></p>
<p><strong>Summary</strong>ï¼š<br>è¿‘æœŸæ¨ç†æ¨¡å‹çš„è¿›å±•åœ¨å‡†ç¡®ç‡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦æ¨ç†ç­‰å¤æ‚ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œç”Ÿæˆè¿™äº›å†—é•¿çš„æ¨ç†åºåˆ—åœ¨è®¡ç®—ä¸Šå¾ˆè€—è´¹æ—¶é—´å’Œèµ„æºã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡åˆ©ç”¨æŸäº›ä»»åŠ¡çš„å›ºæœ‰å¹¶è¡Œæ€§æ¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚å½“å­˜åœ¨å¤šä¸ªå¹¶è¡Œæ¨ç†åˆ†æ”¯æ—¶ï¼Œæˆ‘ä»¬é‡‡ç”¨ç‰¹æ®Šæ³¨æ„åŠ›æ©ç ï¼Œåœ¨å•åºåˆ—å†…è§£ç å¤šä¸ªæ ‡è®°ï¼Œé¿å…é¢å¤–çš„å†…å­˜ä½¿ç”¨ã€‚å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§£ç æ—¶é—´ä¸Šå®ç°äº†è¶…è¿‡100%çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒäº†ç­”æ¡ˆè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ¨ç†æ¨¡å‹åœ¨å‡†ç¡®ç‡ä¸Šå–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦æ¨ç†ç­‰å¤æ‚ä»»åŠ¡ä¸­ã€‚</li>
<li>ç”Ÿæˆæ¨ç†åºåˆ—çš„è¿‡ç¨‹åœ¨è®¡ç®—ä¸Šå¾ˆè€—æ—¶ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨ä»»åŠ¡çš„å›ºæœ‰å¹¶è¡Œæ€§æ¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚</li>
<li>é‡‡ç”¨ç‰¹æ®Šæ³¨æ„åŠ›æ©ç ï¼Œåœ¨å•åºåˆ—å†…è§£ç å¤šä¸ªæ ‡è®°ã€‚</li>
<li>æ–¹æ³•å®ç°äº†è¶…è¿‡100%çš„è§£ç é€Ÿåº¦æå‡ã€‚</li>
<li>æé€Ÿçš„åŒæ—¶ä¿æŒäº†ç­”æ¡ˆè´¨é‡ã€‚</li>
<li>è¿™ç§æ–¹æ³•çš„å®ç°æœ‰åŠ©äºæå‡æ¨ç†æ¨¡å‹çš„æ•ˆç‡å’Œå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20533">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c13337d58794db1a166d08c1c1accef4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c333b345d717c1a999ec69f34e7362d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0c723d3bda73364f052bff19502d39f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-154689d9769f97a8fcb935b9d498eee2.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="OmniScience-A-Domain-Specialized-LLM-for-Scientific-Reasoning-and-Discovery"><a href="#OmniScience-A-Domain-Specialized-LLM-for-Scientific-Reasoning-and-Discovery" class="headerlink" title="OmniScience: A Domain-Specialized LLM for Scientific Reasoning and   Discovery"></a>OmniScience: A Domain-Specialized LLM for Scientific Reasoning and   Discovery</h2><p><strong>Authors:Vignesh Prabhakar, Md Amirul Islam, Adam Atanas, Yao-Ting Wang, Joah Han, Aastha Jhunjhunwala, Rucha Apte, Robert Clark, Kang Xu, Zihan Wang, Kai Liu</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable potential in advancing scientific knowledge and addressing complex challenges. In this work, we introduce OmniScience, a specialized large reasoning model for general science, developed through three key components: (1) domain adaptive pretraining on a carefully curated corpus of scientific literature, (2) instruction tuning on a specialized dataset to guide the model in following domain-specific tasks, and (3) reasoning-based knowledge distillation through fine-tuning to significantly enhance its ability to generate contextually relevant and logically sound responses. We demonstrate the versatility of OmniScience by developing a battery agent that efficiently ranks molecules as potential electrolyte solvents or additives. Comprehensive evaluations reveal that OmniScience is competitive with state-of-the-art large reasoning models on the GPQA Diamond and domain-specific battery benchmarks, while outperforming all public reasoning and non-reasoning models with similar parameter counts. We further demonstrate via ablation experiments that domain adaptive pretraining and reasoning-based knowledge distillation are critical to attain our performance levels, across benchmarks. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨è¿›ç§‘å­¦çŸ¥è¯†å’Œåº”å¯¹å¤æ‚æŒ‘æˆ˜æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—æ½œåŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†OmniScienceï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é€šç”¨ç§‘å­¦çš„ä¸“é—¨å¤§å‹æ¨ç†æ¨¡å‹ï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶å¼€å‘è€Œæˆï¼šï¼ˆ1ï¼‰åœ¨ç²¾å¿ƒæŒ‘é€‰çš„ç§‘å­¦æ–‡çŒ®è¯­æ–™åº“ä¸Šè¿›è¡Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼›ï¼ˆ2ï¼‰åœ¨ä¸“é—¨çš„æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹æ‰§è¡Œç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ï¼›ï¼ˆ3ï¼‰é€šè¿‡å¾®è°ƒè¿›è¡ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦ï¼Œä»¥æ˜¾è‘—æé«˜å…¶ç”Ÿæˆè¯­å¢ƒç›¸å…³å’Œé€»è¾‘ä¸¥è°¨å›åº”çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡å¼€å‘ä¸€ç§ç”µæ± ä»£ç†æ¥å±•ç¤ºOmniScienceçš„é€šç”¨æ€§ï¼Œè¯¥ä»£ç†èƒ½å¤Ÿé«˜æ•ˆåœ°æ’åˆ—åˆ†å­ä½œä¸ºæ½œåœ¨çš„ç”µè§£è´¨æº¶å‰‚æˆ–æ·»åŠ å‰‚ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒOmniScienceåœ¨GPQA Diamondå’Œç‰¹å®šé¢†åŸŸçš„ç”µæ± åŸºå‡†æµ‹è¯•ä¸Šï¼Œä¸æœ€æ–°çš„å¤§å‹æ¨ç†æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶ï¼Œåœ¨å‚æ•°æ•°é‡ç›¸ä¼¼çš„æ‰€æœ‰å…¬å…±æ¨ç†å’Œéæ¨ç†æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ã€‚æˆ‘ä»¬è¿˜é€šè¿‡æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯æ˜ï¼Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒå’ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦å¯¹äºè¾¾åˆ°æˆ‘ä»¬çš„æ€§èƒ½æ°´å¹³è‡³å…³é‡è¦ï¼Œè·¨è¶Šå„ç§åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17604v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>OmniScienceæ¨¡å‹æ˜¯ä¸€ç§ç”¨äºé€šç”¨ç§‘å­¦çš„ä¸“é—¨å¤§å‹æ¨ç†æ¨¡å‹ï¼Œå®ƒé€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶å¼€å‘è€Œæˆï¼šé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒä»¥åŠæ¨ç†åŸºç¡€çš„çŸ¥è¯†è’¸é¦ã€‚è¯¥æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œç”¨äºæ¨è¿›ç§‘å­¦çŸ¥è¯†å¹¶åº”å¯¹å¤æ‚æŒ‘æˆ˜ã€‚åœ¨ç”µæ± ä»£ç†çš„å¼€å‘ä¸­å±•ç¤ºäº†OmniScienceçš„é€šç”¨æ€§ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°æ’åˆ—åˆ†å­ä½œä¸ºæ½œåœ¨çš„ç”µè§£è´¨æº¶å‰‚æˆ–æ·»åŠ å‰‚ã€‚ç»¼åˆè¯„ä»·è¡¨æ˜ï¼ŒOmniScienceåœ¨GPQA Diamondå’Œç‰¹å®šé¢†åŸŸç”µæ± åŸºå‡†æµ‹è¯•ä¸­å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶ä¼˜äºæ‰€æœ‰å…·æœ‰ç›¸ä¼¼å‚æ•°è®¡æ•°çš„å…¬å…±æ¨ç†å’Œéæ¨ç†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OmniScienceæ˜¯ä¸€ä¸ªä¸“é—¨çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼Œç”¨äºé€šç”¨ç§‘å­¦é¢†åŸŸã€‚</li>
<li>æ¨¡å‹é€šè¿‡é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒä»¥åŠæ¨ç†åŸºç¡€çš„çŸ¥è¯†è’¸é¦ä¸‰ä¸ªå…³é”®ç»„ä»¶å¼€å‘ã€‚</li>
<li>OmniScienceå±•ç°å‡ºåœ¨æ¨è¿›ç§‘å­¦çŸ¥è¯†å’Œåº”å¯¹å¤æ‚æŒ‘æˆ˜æ–¹é¢çš„å¼ºå¤§æ½œåŠ›ã€‚</li>
<li>æ¨¡å‹åœ¨å¼€å‘ç”µæ± ä»£ç†æ—¶è¡¨ç°å‡ºé€šç”¨æ€§ï¼Œèƒ½é«˜æ•ˆæ’åˆ—åˆ†å­ä½œä¸ºç”µè§£è´¨æº¶å‰‚æˆ–æ·»åŠ å‰‚ã€‚</li>
<li>ç»¼åˆè¯„ä»·æ˜¾ç¤ºï¼ŒOmniScienceåœ¨åŸºå‡†æµ‹è¯•ä¸­å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶ä¼˜äºå…¶ä»–å…·æœ‰ç›¸ä¼¼å‚æ•°è®¡æ•°çš„æ¨¡å‹ã€‚</li>
<li>æ¶ˆèå®éªŒè¡¨æ˜ï¼Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒå’Œæ¨ç†åŸºç¡€çš„çŸ¥è¯†è’¸é¦å¯¹äºè¾¾åˆ°æ€§èƒ½æ°´å¹³è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20e8cc74e03be7685752af6aaee7dfe8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5cddc57a4b1e46185ec103df103a43ee.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="DRESS-Diffusion-Reasoning-based-Reward-Shaping-Scheme-For-Intelligent-Networks"><a href="#DRESS-Diffusion-Reasoning-based-Reward-Shaping-Scheme-For-Intelligent-Networks" class="headerlink" title="DRESS: Diffusion Reasoning-based Reward Shaping Scheme For Intelligent   Networks"></a>DRESS: Diffusion Reasoning-based Reward Shaping Scheme For Intelligent   Networks</h2><p><strong>Authors:Feiran You, Hongyang Du, Xiangwang Hou, Yong Ren, Kaibin Huang</strong></p>
<p>Network optimization remains fundamental in wireless communications, with Artificial Intelligence (AI)-based solutions gaining widespread adoption. As Sixth-Generation (6G) communication networks pursue full-scenario coverage, optimization in complex extreme environments presents unprecedented challenges. The dynamic nature of these environments, combined with physical constraints, makes it difficult for AI solutions such as Deep Reinforcement Learning (DRL) to obtain effective reward feedback for the training process. However, many existing DRL-based network optimization studies overlook this challenge through idealized environment settings. Inspired by the powerful capabilities of Generative AI (GenAI), especially diffusion models, in capturing complex latent distributions, we introduce a novel Diffusion Reasoning-based Reward Shaping Scheme (DRESS) to achieve robust network optimization. By conditioning on observed environmental states and executed actions, DRESS leverages diffusion modelsâ€™ multi-step denoising process as a form of deep reasoning, progressively refining latent representations to generate meaningful auxiliary reward signals that capture patterns of network systems. Moreover, DRESS is designed for seamless integration with any DRL framework, allowing DRESS-aided DRL (DRESSed-DRL) to enable stable and efficient DRL training even under extreme network environments. Experimental results demonstrate that DRESSed-DRL achieves about 1.5x times faster convergence than its original version in sparse-reward wireless environments and significant performance improvements in multiple general DRL benchmark environments compared to baseline methods. The code of DRESS is available at <a target="_blank" rel="noopener" href="https://github.com/NICE-HKU/DRESS">https://github.com/NICE-HKU/DRESS</a>. </p>
<blockquote>
<p>ç½‘ç»œä¼˜åŒ–åœ¨æ— çº¿é€šä¿¡ä¸­ä»ç„¶å…·æœ‰æ ¹æœ¬é‡è¦æ€§ï¼ŒåŸºäºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„è§£å†³æ–¹æ¡ˆå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚éšç€ç¬¬å…­ä»£ï¼ˆ6Gï¼‰é€šä¿¡ç½‘ç»œçš„æ™®åŠï¼Œè¿½æ±‚å…¨åœºæ™¯è¦†ç›–çš„å¤æ‚æç«¯ç¯å¢ƒä¸­çš„ä¼˜åŒ–é¢ä¸´ç€å‰æ‰€æœªæœ‰çš„æŒ‘æˆ˜ã€‚è¿™äº›ç¯å¢ƒçš„åŠ¨æ€æ€§ä»¥åŠç‰©ç†çº¦æŸçš„ç»“åˆï¼Œä½¿å¾—äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆï¼ˆå¦‚æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰ï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éš¾ä»¥è·å¾—æœ‰æ•ˆçš„å¥–åŠ±åé¦ˆã€‚ç„¶è€Œï¼Œè®¸å¤šç°æœ‰çš„åŸºäºDRLçš„ç½‘ç»œä¼˜åŒ–ç ”ç©¶é€šè¿‡ç†æƒ³åŒ–çš„ç¯å¢ƒè®¾ç½®å¿½ç•¥äº†è¿™ä¸€æŒ‘æˆ˜ã€‚å—ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰çš„å¼ºå¤§èƒ½åŠ›çš„å¯å‘ï¼Œå°¤å…¶æ˜¯æ‰©æ•£æ¨¡å‹åœ¨æ•æ‰å¤æ‚æ½œåœ¨åˆ†å¸ƒæ–¹é¢çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæ‰©æ•£æ¨ç†çš„å¥–åŠ±å¡‘å½¢æ–¹æ¡ˆï¼ˆDRESSï¼‰ï¼Œä»¥å®ç°ç¨³å¥çš„ç½‘ç»œä¼˜åŒ–ã€‚DRESSé€šè¿‡è§‚æµ‹åˆ°çš„ç¯å¢ƒçŠ¶æ€å’Œæ‰€æ‰§è¡Œçš„åŠ¨ä½œæ¥è¿›è¡Œè°ƒèŠ‚ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¤šæ­¥å»å™ªè¿‡ç¨‹ä½œä¸ºæ·±åº¦æ¨ç†çš„ä¸€ç§å½¢å¼ï¼Œé€æ­¥ä¼˜åŒ–æ½œåœ¨è¡¨ç¤ºï¼Œç”Ÿæˆæœ‰æ„ä¹‰çš„è¾…åŠ©å¥–åŠ±ä¿¡å·ï¼Œæ•æ‰ç½‘ç»œç³»ç»Ÿæ¨¡å¼ã€‚æ­¤å¤–ï¼ŒDRESSè®¾è®¡ç”¨äºæ— ç¼é›†æˆä»»ä½•DRLæ¡†æ¶ï¼Œå…è®¸DRESSè¾…åŠ©çš„DRLï¼ˆDRESSed-DRLï¼‰å³ä½¿åœ¨æç«¯ç½‘ç»œç¯å¢ƒä¸‹ä¹Ÿèƒ½å®ç°ç¨³å®šå’Œé«˜æ•ˆçš„DRLè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç¨€ç–å¥–åŠ±çš„æ— çº¿ç¯å¢ƒä¸­ï¼ŒDRESSed-DRLçš„æ”¶æ•›é€Ÿåº¦æ¯”å…¶åŸå§‹ç‰ˆæœ¬å¿«çº¦1.5å€ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªé€šç”¨çš„DRLåŸºå‡†ç¯å¢ƒä¸­ä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚DRESSçš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NICE-HKU/DRESS%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NICE-HKU/DRESSä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07433v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>éšç€æ— çº¿é€šè®¯ä¸­ç½‘ç»œä¼˜åŒ–çš„é‡è¦æ€§ä¸æ–­æå‡ï¼ŒåŸºäºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„è§£å†³æ–¹æ¡ˆå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚åœ¨è¿½æ±‚å…¨åœºæ™¯è¦†ç›–çš„6Gé€šè®¯ç½‘ç»œä¸­ï¼Œæç«¯å¤æ‚ç¯å¢ƒä¸‹çš„ä¼˜åŒ–é¢ä¸´å‰æ‰€æœªæœ‰çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å—ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰çš„å¯å‘ï¼Œç‰¹åˆ«æ˜¯æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ‰©æ•£æ¨ç†çš„å¥–åŠ±å¡‘å½¢æ–¹æ¡ˆï¼ˆDRESSï¼‰ï¼Œä»¥å®ç°ç¨³å¥çš„ç½‘ç»œä¼˜åŒ–ã€‚DRESSåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¤šæ­¥å»å™ªè¿‡ç¨‹ä½œä¸ºæ·±åº¦æ¨ç†ï¼Œç”Ÿæˆæœ‰æ„ä¹‰çš„è¾…åŠ©å¥–åŠ±ä¿¡å·ï¼Œæ•æ‰ç½‘ç»œç³»ç»Ÿæ¨¡å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDRESSåœ¨ç¨€ç–å¥–åŠ±çš„æ— çº¿ç¯å¢ƒä¸­å®ç°äº†çº¦1.5å€çš„å¿«é€Ÿæ”¶æ•›ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªé€šç”¨DRLåŸºå‡†ç¯å¢ƒä¸­ç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æœ‰ç€æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç½‘ç»œä¼˜åŒ–åœ¨æ— çº¿é€šè®¯ä¸­ä»ç„¶è‡³å…³é‡è¦ï¼ŒAIè§£å†³æ–¹æ¡ˆï¼ˆå¦‚æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼‰åœ¨å¤„ç†å¤æ‚æç«¯ç¯å¢ƒæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹æœ‰èƒ½åŠ›æ•æ‰å¤æ‚çš„æ½œåœ¨åˆ†å¸ƒï¼Œå¯ç”¨äºç½‘ç»œä¼˜åŒ–ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±å¡‘å½¢æ–¹æ¡ˆDRESSï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œæ·±åº¦æ¨ç†ï¼Œç”Ÿæˆè¾…åŠ©å¥–åŠ±ä¿¡å·ã€‚</li>
<li>DRESSå¯ä»¥æ— ç¼é›†æˆåˆ°ä»»ä½•DRLæ¡†æ¶ä¸­ï¼Œå®ç°ç¨³å®šçš„è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>DRESSåœ¨ç¨€ç–å¥–åŠ±çš„æ— çº¿ç¯å¢ƒä¸­å®ç°äº†å¿«é€Ÿæ”¶æ•›ã€‚</li>
<li>DRESSåœ¨å¤šä¸ªé€šç”¨DRLåŸºå‡†ç¯å¢ƒä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07433">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-61819c8d7b67983a22fad40948b00249.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-739b517339d233fcd52da77913c3f9ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfab5f64d3ef09c847efba565e97ec2c.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="EPO-Explicit-Policy-Optimization-for-Strategic-Reasoning-in-LLMs-via-Reinforcement-Learning"><a href="#EPO-Explicit-Policy-Optimization-for-Strategic-Reasoning-in-LLMs-via-Reinforcement-Learning" class="headerlink" title="EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via   Reinforcement Learning"></a>EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via   Reinforcement Learning</h2><p><strong>Authors:Xiaoqian Liu, Ke Wang, Yongbin Li, Yuchuan Wu, Wentao Ma, Aobo Kong, Fei Huang, Jianbin Jiao, Junge Zhang</strong></p>
<p>Large Language Models (LLMs) have shown impressive reasoning capabilities in well-defined problems with clear solutions, such as mathematics and coding. However, they still struggle with complex real-world scenarios like business negotiations, which require strategic reasoning-an ability to navigate dynamic environments and align long-term goals amidst uncertainty. Existing methods for strategic reasoning face challenges in adaptability, scalability, and transferring strategies to new contexts. To address these issues, we propose explicit policy optimization (EPO) for strategic reasoning, featuring an LLM that provides strategies in open-ended action space and can be plugged into arbitrary LLM agents to motivate goal-directed behavior. To improve adaptability and policy transferability, we train the strategic reasoning model via multi-turn reinforcement learning (RL) using process rewards and iterative self-play, without supervised fine-tuning (SFT) as a preliminary step. Experiments across social and physical domains demonstrate EPOâ€™s ability of long-term goal alignment through enhanced strategic reasoning, achieving state-of-the-art performance on social dialogue and web navigation tasks. Our findings reveal various collaborative reasoning mechanisms emergent in EPO and its effectiveness in generating novel strategies, underscoring its potential for strategic reasoning in real-world applications. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®šä¹‰æ˜ç¡®ã€è§£å†³æ–¹æ¡ˆæ¸…æ™°çš„é—®é¢˜ä¸­å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼Œä¾‹å¦‚åœ¨æ•°å­¦å’Œç¼–ç¨‹é¢†åŸŸã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤„ç†å¤æ‚çš„ç°å®ä¸–ç•Œåœºæ™¯ï¼Œå¦‚éœ€è¦æˆ˜ç•¥æ¨ç†çš„å•†ä¸šè°ˆåˆ¤ä¸­ï¼Œä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æˆ˜ç•¥æ¨ç†éœ€è¦åº”å¯¹åŠ¨æ€ç¯å¢ƒï¼Œå¹¶åœ¨ä¸ç¡®å®šæ€§ä¸­è°ƒæ•´é•¿æœŸç›®æ ‡ã€‚ç°æœ‰çš„æˆ˜ç•¥æ¨ç†æ–¹æ³•é¢ä¸´ç€é€‚åº”æ€§ã€å¯æ‰©å±•æ€§å’Œç­–ç•¥è½¬ç§»ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºæˆ˜ç•¥æ¨ç†çš„æ˜¾å¼ç­–ç•¥ä¼˜åŒ–ï¼ˆEPOï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä½¿ç”¨ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å¼€æ”¾çš„åŠ¨ä½œç©ºé—´ä¸­æä¾›ç­–ç•¥ï¼Œå¹¶ä¸”å¯ä»¥æ’å…¥åˆ°ä»»æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ä¸­ï¼Œä»¥æ¿€åŠ±ç›®æ ‡å¯¼å‘çš„è¡Œä¸ºã€‚ä¸ºäº†æ”¹å–„é€‚åº”æ€§å’Œç­–ç•¥å¯è½¬ç§»æ€§ï¼Œæˆ‘ä»¬é€šè¿‡å¤šå›åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥è®­ç»ƒæˆ˜ç•¥æ¨ç†æ¨¡å‹ï¼Œä½¿ç”¨è¿‡ç¨‹å¥–åŠ±å’Œè¿­ä»£è‡ªæˆ‘æ¸¸æˆï¼Œæ— éœ€åˆæ­¥çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚åœ¨ç¤¾ä¼šå’Œç‰©ç†é¢†åŸŸçš„å®éªŒè¡¨æ˜ï¼ŒEPOèƒ½å¤Ÿé€šè¿‡å¢å¼ºçš„æˆ˜ç•¥æ¨ç†å®ç°é•¿æœŸç›®æ ‡å¯¹é½çš„èƒ½åŠ›ï¼Œåœ¨ç¤¾ä¼šå¯¹è¯å’Œç½‘é¡µå¯¼èˆªä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†EPOä¸­å‡ºç°çš„å„ç§åä½œæ¨ç†æœºåˆ¶åŠå…¶åœ¨ç”Ÿæˆæ–°ç­–ç•¥æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œè¿™å¼ºè°ƒäº†å…¶åœ¨ç°å®ä¸–ç•Œæˆ˜ç•¥æ¨ç†åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12486v4">PDF</a> 22 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å…·æœ‰æ˜ç¡®è§£å†³æ–¹æ¡ˆçš„æ˜ç¡®é—®é¢˜ä¸­å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼Œå¦‚æ•°å­¦å’Œç¼–ç¨‹ã€‚ç„¶è€Œï¼Œåœ¨å¤„ç†éœ€è¦æˆ˜ç•¥æ¨ç†çš„å¤æ‚ç°å®ä¸–ç•Œåœºæ™¯ï¼ˆå¦‚å•†åŠ¡è°ˆåˆ¤ï¼‰æ—¶ï¼Œå®ƒä»¬ä»é¢ä¸´æŒ‘æˆ˜ã€‚æˆ˜ç•¥æ¨ç†è¦æ±‚èƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­å¯¼èˆªï¼Œå¹¶åœ¨ä¸ç¡®å®šæ€§ä¸­è°ƒæ•´é•¿æœŸç›®æ ‡ã€‚ä¸ºè§£å†³ç°æœ‰æˆ˜ç•¥æ¨ç†æ–¹æ³•é¢ä¸´çš„é€‚åº”æ€§ã€å¯æ‰©å±•æ€§å’Œç­–ç•¥è½¬ç§»æ–°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ˜¾å¼ç­–ç•¥ä¼˜åŒ–ï¼ˆEPOï¼‰è¿›è¡Œæˆ˜ç•¥æ¨ç†ã€‚è¯¥æ–¹æ³•è®©LLMåœ¨å¼€æ”¾è¡ŒåŠ¨ç©ºé—´ä¸­æä¾›ç­–ç•¥ï¼Œå¹¶èƒ½è¢«åµŒå…¥åˆ°ä»»æ„LLMä»£ç†ä¸­ä»¥é©±åŠ¨ç›®æ ‡å¯¼å‘è¡Œä¸ºã€‚é€šè¿‡å¤šå›åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒæˆ˜ç•¥æ¨ç†æ¨¡å‹ï¼Œåˆ©ç”¨è¿‡ç¨‹å¥–åŠ±å’Œè¿­ä»£è‡ªæˆ‘æ¸¸æˆæ¥æé«˜é€‚åº”æ€§å’Œç­–ç•¥è¿ç§»èƒ½åŠ›ï¼Œæ— éœ€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä½œä¸ºåˆæ­¥æ­¥éª¤ã€‚å®éªŒè¡¨æ˜ï¼ŒEPOåœ¨ç¤¾ä¼šå’Œç‰©ç†é¢†åŸŸçš„é•¿æœŸç›®æ ‡å¯¹é½èƒ½åŠ›é€šè¿‡å¢å¼ºçš„æˆ˜ç•¥æ¨ç†å®ç°äº†å“è¶Šæ€§èƒ½ï¼Œåœ¨ç¤¾ä¼šå¯¹è¯å’Œç½‘é¡µå¯¼èˆªä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚ç ”ç©¶å‘ç°äº†EPOä¸­å„ç§åä½œæ¨ç†æœºåˆ¶çš„æ¶Œç°åŠå…¶åœ¨ç”Ÿæˆæ–°ç­–ç•¥æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œçªæ˜¾å…¶åœ¨ç°å®ä¸–ç•Œæˆ˜ç•¥æ¨ç†åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså±•ç°å‡ºåœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰é¢†åŸŸçš„å¼ºå¤§æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å¤æ‚ç°å®ä¸–ç•Œåœºæ™¯å¦‚å•†åŠ¡è°ˆåˆ¤ä¸­çš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æˆ˜ç•¥æ¨ç†æ–¹æ³•é¢ä¸´é€‚åº”æ€§ã€å¯æ‰©å±•æ€§å’Œç­–ç•¥è½¬ç§»éš¾é¢˜ã€‚</li>
<li>æå‡ºäº†æ˜¾å¼ç­–ç•¥ä¼˜åŒ–ï¼ˆEPOï¼‰æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œä½¿LLMèƒ½åœ¨å¼€æ”¾è¡ŒåŠ¨ç©ºé—´ä¸­æä¾›ç­–ç•¥å¹¶åµŒå…¥åˆ°ä»»æ„LLMä»£ç†ä¸­ã€‚</li>
<li>é€šè¿‡å¤šå›åˆå¼ºåŒ–å­¦ä¹ è®­ç»ƒæˆ˜ç•¥æ¨ç†æ¨¡å‹ï¼Œæ— éœ€ç›‘ç£å¾®è°ƒã€‚</li>
<li>EPOåœ¨ç¤¾ä¼šå’Œç‰©ç†é¢†åŸŸçš„å®éªŒè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>ç ”ç©¶å‘ç°EPOä¸­åä½œæ¨ç†æœºåˆ¶çš„æ¶Œç°åŠå…¶åœ¨ç”Ÿæˆæ–°ç­–ç•¥æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12486">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3140a060bd0080c543db7b412020f695.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b362a1bd20af45a1de73d92c66aa409.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f45650ec969b9afc3b1d361fb227243.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9977c767b3fceb5b7122b8173e95db9.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-25/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-25/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-25/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-346cd188e5dbc974e3ae403a7765b07e.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-25  OptimAI Optimization from Natural Language Using LLM-Powered AI Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-24/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-11c830ce7daabf935a3f8cf681c26694.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-24  Talk is Not Always Cheap Promoting Wireless Sensing Models with Text   Prompts
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31180k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
