<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-25  OptimAI Optimization from Natural Language Using LLM-Powered AI Agents">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-346cd188e5dbc974e3ae403a7765b07e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-25-æ›´æ–°"><a href="#2025-04-25-æ›´æ–°" class="headerlink" title="2025-04-25 æ›´æ–°"></a>2025-04-25 æ›´æ–°</h1><h2 id="OptimAI-Optimization-from-Natural-Language-Using-LLM-Powered-AI-Agents"><a href="#OptimAI-Optimization-from-Natural-Language-Using-LLM-Powered-AI-Agents" class="headerlink" title="OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents"></a>OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents</h2><p><strong>Authors:Raghav Thind, Youran Sun, Ling Liang, Haizhao Yang</strong></p>
<p>Optimization plays a vital role in scientific research and practical applications, but formulating a concrete optimization problem described in natural language into a mathematical form and selecting a suitable solver to solve the problem requires substantial domain expertise. We introduce \textbf{OptimAI}, a framework for solving \underline{Optim}ization problems described in natural language by leveraging LLM-powered \underline{AI} agents, achieving superior performance over current state-of-the-art methods. Our framework is built upon four key roles: (1) a \emph{formulator} that translates natural language problem descriptions into precise mathematical formulations; (2) a \emph{planner} that constructs a high-level solution strategy prior to execution; and (3) a \emph{coder} and a \emph{code critic} capable of interacting with the environment and reflecting on outcomes to refine future actions. Ablation studies confirm that all roles are essential; removing the planner or code critic results in $5.8\times$ and $3.1\times$ drops in productivity, respectively. Furthermore, we introduce UCB-based debug scheduling to dynamically switch between alternative plans, yielding an additional $3.3\times$ productivity gain. Our design emphasizes multi-agent collaboration, allowing us to conveniently explore the synergistic effect of combining diverse models within a unified system. Our approach attains 88.1% accuracy on the NLP4LP dataset and 71.2% on the Optibench (non-linear w&#x2F;o table) subset, reducing error rates by 58% and 50% respectively over prior best results. </p>
<blockquote>
<p>ä¼˜åŒ–åœ¨ç§‘å­¦ç ”ç©¶å’Œå®é™…åº”ç”¨ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œç„¶è€Œï¼Œå°†è‡ªç„¶è¯­è¨€æè¿°çš„å…·ä½“ä¼˜åŒ–é—®é¢˜è½¬åŒ–ä¸ºæ•°å­¦å½¢å¼ï¼Œå¹¶é€‰æ‹©é€‚åˆçš„æ±‚è§£å™¨æ¥è§£å†³é—®é¢˜éœ€è¦å¤§é‡çš„ä¸“ä¸šçŸ¥è¯†ã€‚æˆ‘ä»¬å¼•å…¥äº†<strong>OptimAI</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„<strong>AI</strong>ä»£ç†æ¥è§£å†³è‡ªç„¶è¯­è¨€æè¿°çš„ä¼˜åŒ–é—®é¢˜çš„æ¡†æ¶ï¼Œå…¶æ€§èƒ½è¶…è¶Šäº†å½“å‰å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ¡†æ¶å»ºç«‹åœ¨å››ä¸ªå…³é”®è§’è‰²ä¹‹ä¸Šï¼šï¼ˆ1ï¼‰ä¸€ä¸ªèƒ½å°†è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°è½¬åŒ–ä¸ºç²¾ç¡®æ•°å­¦å½¢å¼çš„<strong>è¡¨è¿°å™¨</strong>ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªåœ¨æ‰§è¡Œå‰æ„å»ºé«˜çº§è§£å†³æ–¹æ¡ˆç­–ç•¥çš„<strong>è§„åˆ’å™¨</strong>ï¼›ï¼ˆ3ï¼‰ä¸€ä¸ªèƒ½å¤Ÿä¸ç¯å¢ƒäº’åŠ¨å¹¶å¯¹ç»“æœè¿›è¡Œåæ€ä»¥æ”¹è¿›æœªæ¥è¡ŒåŠ¨çš„<strong>ç¼–ç è€…</strong>å’Œ<strong>ä»£ç è¯„è®ºå®¶</strong>ã€‚åˆ‡é™¤ç ”ç©¶è¡¨æ˜ï¼Œæ‰€æœ‰è§’è‰²éƒ½æ˜¯ä¸å¯æˆ–ç¼ºçš„ï¼›ç§»é™¤è§„åˆ’è€…æˆ–ä»£ç è¯„è®ºå®¶å°†å¯¼è‡´ç”Ÿäº§ç‡åˆ†åˆ«ä¸‹é™5.8å€å’Œ3.1å€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºUCBçš„è°ƒè¯•è°ƒåº¦æ¥åŠ¨æ€åˆ‡æ¢æ›¿ä»£æ–¹æ¡ˆï¼Œäº§ç”Ÿäº†é¢å¤–çš„3.3å€ç”Ÿäº§ç‡å¢ç›Šã€‚æˆ‘ä»¬çš„è®¾è®¡å¼ºè°ƒå¤šæ™ºèƒ½ä½“åä½œï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿæ–¹ä¾¿åœ°æ¢ç´¢åœ¨ç»Ÿä¸€ç³»ç»Ÿä¸­ç»“åˆä¸åŒæ¨¡å‹çš„ååŒä½œç”¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨NLP4LPæ•°æ®é›†ä¸Šè¾¾åˆ°88.1%çš„å‡†ç¡®ç‡ï¼Œåœ¨Optibenchï¼ˆéçº¿æ€§æ— è¡¨ï¼‰å­é›†ä¸Šè¾¾åˆ°71.2%çš„å‡†ç¡®ç‡ï¼Œä¸ä¹‹å‰çš„æœ€ä½³ç»“æœç›¸æ¯”ï¼Œè¯¯å·®ç‡åˆ†åˆ«é™ä½äº†58%å’Œ50%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16918v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>ä¼˜åŒ–åœ¨è‡ªç„¶è¯­è¨€æè¿°å’Œå®é™…åº”ç”¨ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œä½†å°†è‡ªç„¶è¯­è¨€æè¿°çš„å…·ä½“ä¼˜åŒ–é—®é¢˜è½¬åŒ–ä¸ºæ•°å­¦å½¢å¼å¹¶é€‰æ‹©é€‚å½“çš„æ±‚è§£å™¨æ¥è§£å†³é—®é¢˜éœ€è¦å¤§é‡çš„ä¸“ä¸šçŸ¥è¯†ã€‚æˆ‘ä»¬å¼•å…¥äº†OptimAIæ¡†æ¶ï¼Œåˆ©ç”¨LLMé©±åŠ¨çš„AIä»£ç†è§£å†³è‡ªç„¶è¯­è¨€æè¿°çš„ä¼˜åŒ–é—®é¢˜ï¼Œå®ç°äº†ä¼˜äºå½“å‰å…ˆè¿›æ–¹æ³•çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶å»ºç«‹åœ¨å››ä¸ªå…³é”®è§’è‰²ä¸Šï¼šå°†è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°è½¬åŒ–ä¸ºç²¾ç¡®æ•°å­¦å½¢å¼çš„è¡¨è¿°è€…ã€æ„å»ºé«˜çº§è§£å†³æ–¹æ¡ˆç­–ç•¥çš„è§„åˆ’è€…ã€èƒ½å¤Ÿä¸ç¯å¢ƒäº¤äº’å¹¶åæ€ç»“æœä»¥æ”¹è¿›æœªæ¥è¡ŒåŠ¨çš„ç¼–ç å™¨å’Œä»£ç è¯„è®ºå®¶ã€‚æˆ‘ä»¬çš„è®¾è®¡å¼ºè°ƒå¤šæ™ºèƒ½ä½“åä½œï¼Œä¾¿äºåœ¨ç»Ÿä¸€ç³»ç»Ÿä¸­æ¢ç´¢ç»“åˆä¸åŒæ¨¡å‹çš„ååŒä½œç”¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨NLP4LPæ•°æ®é›†ä¸Šè¾¾åˆ°äº†88.1%çš„å‡†ç¡®ç‡ï¼Œåœ¨Optibenchï¼ˆéçº¿æ€§æ— è¡¨ï¼‰å­é›†ä¸Šè¾¾åˆ°äº†71.2%çš„å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºä¹‹å‰æœ€ä½³ç»“æœåˆ†åˆ«é™ä½äº†58%å’Œ50%çš„é”™è¯¯ç‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>OptimAIæ¡†æ¶åˆ©ç”¨LLMé©±åŠ¨çš„AIä»£ç†è§£å†³è‡ªç„¶è¯­è¨€æè¿°çš„ä¼˜åŒ–é—®é¢˜ï¼Œå®ç°é«˜æ€§èƒ½ã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬å››ä¸ªå…³é”®è§’è‰²ï¼šè¡¨è¿°è€…ã€è§„åˆ’è€…ã€ç¼–ç å™¨å’Œä»£ç è¯„è®ºå®¶ï¼Œå„è‡ªæ‰¿æ‹…ä¸åŒçš„ä»»åŠ¡ã€‚</li>
<li>å»é™¤è§„åˆ’è€…æˆ–ä»£ç è¯„è®ºå®¶ä¼šå¯¼è‡´ç”Ÿäº§åŠ›å¤§å¹…ä¸‹é™ï¼Œè¡¨æ˜è¿™äº›è§’è‰²å¯¹æ¡†æ¶æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>é€šè¿‡å¼•å…¥åŸºäºUCBçš„è°ƒè¯•è°ƒåº¦ï¼Œå¯ä»¥åŠ¨æ€åˆ‡æ¢ä¸åŒè®¡åˆ’ï¼Œè¿›ä¸€æ­¥æé«˜ç”Ÿäº§åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶å¼ºè°ƒå¤šæ™ºèƒ½ä½“åä½œï¼Œä¾¿äºåœ¨ç»Ÿä¸€ç³»ç»Ÿä¸­ç»“åˆä¸åŒæ¨¡å‹ï¼Œå‘æŒ¥ååŒä½œç”¨ã€‚</li>
<li>åœ¨NLP4LPæ•°æ®é›†å’ŒOptibenchå­é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å…·æœ‰æ˜¾è‘—çš„é«˜å‡†ç¡®ç‡ã€‚</li>
<li>ä¸ä¹‹å‰çš„æœ€ä½³ç»“æœç›¸æ¯”ï¼Œè¯¥æ¡†æ¶åœ¨é”™è¯¯ç‡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16918">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cbb451bffca58548136f60e1505a8856.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d666c5932dc3a62754102c0db044d5dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b4ec94cddc5604313f06ef772ba0c08.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca292c2e0d75e544772921d8d233326d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-623f935129944ff9a28bb5344dfbaa88.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Tracing-Thought-Using-Chain-of-Thought-Reasoning-to-Identify-the-LLM-Behind-AI-Generated-Text"><a href="#Tracing-Thought-Using-Chain-of-Thought-Reasoning-to-Identify-the-LLM-Behind-AI-Generated-Text" class="headerlink" title="Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM   Behind AI-Generated Text"></a>Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM   Behind AI-Generated Text</h2><p><strong>Authors:Shifali Agrahari, Sanasam Ranbir Singh</strong></p>
<p>In recent years, the detection of AI-generated text has become a critical area of research due to concerns about academic integrity, misinformation, and ethical AI deployment. This paper presents COT Fine-tuned, a novel framework for detecting AI-generated text and identifying the specific language model. responsible for generating the text. We propose a dual-task approach, where Task A involves classifying text as AI-generated or human-written, and Task B identifies the specific LLM behind the text. The key innovation of our method lies in the use of Chain-of-Thought reasoning, which enables the model to generate explanations for its predictions, enhancing transparency and interpretability. Our experiments demonstrate that COT Fine-tuned achieves high accuracy in both tasks, with strong performance in LLM identification and human-AI classification. We also show that the CoT reasoning process contributes significantly to the models effectiveness and interpretability. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œç”±äºäººä»¬å¯¹å­¦æœ¯è¯šä¿¡ã€è¯¯å¯¼ä¿¡æ¯å’Œé“å¾·äººå·¥æ™ºèƒ½éƒ¨ç½²çš„æ‹…å¿§ï¼Œæ£€æµ‹äººå·¥æ™ºèƒ½ç”Ÿæˆçš„æ–‡æœ¬å·²æˆä¸ºç ”ç©¶çš„å…³é”®é¢†åŸŸã€‚æœ¬æ–‡æå‡ºäº†COT Fine-tunedï¼Œä¸€ä¸ªç”¨äºæ£€æµ‹äººå·¥æ™ºèƒ½ç”Ÿæˆæ–‡æœ¬å¹¶è¯†åˆ«ç”Ÿæˆæ–‡æœ¬çš„å…·ä½“è¯­è¨€æ¨¡å‹çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬é‡‡ç”¨åŒä»»åŠ¡æ–¹æ³•ï¼Œä»»åŠ¡Aæ¶‰åŠå°†æ–‡æœ¬åˆ†ç±»ä¸ºäººå·¥æ™ºèƒ½ç”Ÿæˆæˆ–äººç±»æ’°å†™ï¼Œä»»åŠ¡Båˆ™è¯†åˆ«æ–‡æœ¬èƒŒåçš„ç‰¹å®šå¤§å‹è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºä½¿ç”¨Chain-of-Thoughtæ¨ç†ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¸ºå…¶é¢„æµ‹ç”Ÿæˆè§£é‡Šï¼Œæé«˜é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒCOT Fine-tunedåœ¨ä¸¤é¡¹ä»»åŠ¡ä¸­éƒ½å®ç°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œåœ¨å¤§å‹è¯­è¨€æ¨¡å‹è¯†åˆ«å’Œäººæœºåˆ†ç±»æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼ŒCoTæ¨ç†è¿‡ç¨‹å¯¹æ¨¡å‹çš„æ•ˆç‡å’Œå¯è§£é‡Šæ€§åšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16913v1">PDF</a> De-Factify 4: 4th Workshop on Multimodal Fact Checking and Hate   Speech Detection, co-located with AAAI 2025. Pennsylvania</p>
<p><strong>Summary</strong><br>AIç”Ÿæˆæ–‡æœ¬æ£€æµ‹å·²æˆä¸ºç ”ç©¶çš„å…³é”®é¢†åŸŸï¼Œæ¶‰åŠå­¦æœ¯è¯šä¿¡ã€è™šå‡ä¿¡æ¯å’Œä¼¦ç†AIéƒ¨ç½²ç­‰é—®é¢˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ¡†æ¶COT Fine-tunedï¼Œé‡‡ç”¨åŒé‡ä»»åŠ¡æ–¹æ³•æ£€æµ‹AIç”Ÿæˆçš„æ–‡æœ¬å¹¶è¯†åˆ«ç‰¹å®šè¯­è¨€æ¨¡å‹ã€‚å…¶åˆ›æ–°ä¹‹å¤„åœ¨äºé‡‡ç”¨æ€ç»´é“¾æ¨ç†æŠ€æœ¯ï¼Œä½¿æ¨¡å‹èƒ½å¯¹å…¶é¢„æµ‹ç”Ÿæˆè§£é‡Šï¼Œå¢å¼ºé€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚å®éªŒè¯æ˜ï¼ŒCOT Fine-tunedåœ¨ä¸¤é¡¹ä»»åŠ¡ä¸­éƒ½å–å¾—äº†é«˜å‡†ç¡®ç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­è¨€æ¨¡å‹è¯†åˆ«å’Œäººæœºåˆ†ç±»æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚æ€ç»´é“¾æ¨ç†è¿‡ç¨‹å¯¹æ¨¡å‹çš„æ•ˆèƒ½å’Œå¯è§£é‡Šæ€§æœ‰æ˜¾è‘—è´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIç”Ÿæˆæ–‡æœ¬æ£€æµ‹æ˜¯å½“å‰çš„çƒ­é—¨ç ”ç©¶é¢†åŸŸï¼Œä¸»è¦å…³æ³¨å­¦æœ¯è¯šä¿¡ã€è™šå‡ä¿¡æ¯å’Œä¼¦ç†AIéƒ¨ç½²é—®é¢˜ã€‚</li>
<li>COT Fine-tunedæ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œç”¨äºæ£€æµ‹AIç”Ÿæˆçš„æ–‡æœ¬å¹¶è¯†åˆ«ç”Ÿæˆæ–‡æœ¬çš„ç‰¹å®šè¯­è¨€æ¨¡å‹ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨åŒé‡ä»»åŠ¡æ–¹æ³•ï¼ŒåŒ…æ‹¬åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ç”±AIç”Ÿæˆä»¥åŠè¯†åˆ«ç‰¹å®šè¯­è¨€æ¨¡å‹ã€‚</li>
<li>COT Fine-tunedçš„ä¸»è¦åˆ›æ–°ç‚¹æ˜¯é‡‡ç”¨æ€ç»´é“¾æ¨ç†æŠ€æœ¯ï¼Œå¢å¼ºæ¨¡å‹çš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCOT Fine-tunedåœ¨ä¸¤é¡¹ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºé«˜å‡†ç¡®ç‡ã€‚</li>
<li>æ€ç»´é“¾æ¨ç†è¿‡ç¨‹å¯¹æå‡æ¨¡å‹çš„æ•ˆèƒ½å’Œå¯è§£é‡Šæ€§æœ‰é‡è¦è´¡çŒ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16913">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94bc7ccd14bab0a82c086205ec93b2b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50895ae4116888dee53a8e58bb341b57.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b349b9c39daf91fbbfe23a040063478.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb086bcb345302ca24789fcf65bd22bb.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Enhancing-Critical-Thinking-with-AI-A-Tailored-Warning-System-for-RAG-Models"><a href="#Enhancing-Critical-Thinking-with-AI-A-Tailored-Warning-System-for-RAG-Models" class="headerlink" title="Enhancing Critical Thinking with AI: A Tailored Warning System for RAG   Models"></a>Enhancing Critical Thinking with AI: A Tailored Warning System for RAG   Models</h2><p><strong>Authors:Xuyang Zhu, Sejoon Chang, Andrew Kuik</strong></p>
<p>Retrieval-Augmented Generation (RAG) systems offer a powerful approach to enhancing large language model (LLM) outputs by incorporating fact-checked, contextually relevant information. However, fairness and reliability concerns persist, as hallucinations can emerge at both the retrieval and generation stages, affecting usersâ€™ reasoning and decision-making. Our research explores how tailored warning messages â€“ whose content depends on the specific context of hallucination â€“ shape user reasoning and actions in an educational quiz setting. Preliminary findings suggest that while warnings improve accuracy and awareness of high-level hallucinations, they may also introduce cognitive friction, leading to confusion and diminished trust in the system. By examining these interactions, this work contributes to the broader goal of AI-augmented reasoning: developing systems that actively support human reflection, critical thinking, and informed decision-making rather than passive information consumption. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿé€šè¿‡èå…¥ç»è¿‡äº‹å®æ ¸æŸ¥çš„ã€è¯­å¢ƒç›¸å…³çš„ä¿¡æ¯ï¼Œä¸ºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºæä¾›äº†ä¸€ç§å¼ºå¤§æ–¹æ³•ã€‚ç„¶è€Œï¼Œå…¬å¹³æ€§å’Œå¯é æ€§é—®é¢˜ä¾ç„¶å­˜åœ¨ï¼Œå› ä¸ºåœ¨æ£€ç´¢å’Œç”Ÿæˆé˜¶æ®µéƒ½å¯èƒ½å‡ºç°å¹»è§‰ï¼Œå½±å“ç”¨æˆ·çš„æ¨ç†å’Œå†³ç­–ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ¢ç´¢äº†å®šåˆ¶è­¦å‘Šä¿¡æ¯â€”â€”å…¶å†…å®¹å–å†³äºå¹»è§‰çš„å…·ä½“è¯­å¢ƒâ€”â€”å¦‚ä½•å¡‘é€ ç”¨æˆ·åœ¨æ•™è‚²æµ‹éªŒç¯å¢ƒä¸­çš„æ¨ç†å’Œè¡ŒåŠ¨ã€‚åˆæ­¥ç»“æœè¡¨æ˜ï¼Œè™½ç„¶è­¦å‘Šä¿¡æ¯æé«˜äº†å¯¹é«˜çº§å¹»è§‰çš„å‡†ç¡®æ€§å’Œæ„è¯†ï¼Œä½†å®ƒä»¬ä¹Ÿå¯èƒ½å¼•å‘è®¤çŸ¥æ‘©æ“¦ï¼Œå¯¼è‡´ç”¨æˆ·æ··æ·†å¹¶å¯¹ç³»ç»Ÿå¤±å»ä¿¡ä»»ã€‚é€šè¿‡æ£€æŸ¥è¿™äº›äº¤äº’ï¼Œè¿™é¡¹å·¥ä½œæœ‰åŠ©äºå®ç°äººå·¥æ™ºèƒ½è¾…åŠ©æ¨ç†çš„æ›´å¹¿æ³›ç›®æ ‡ï¼šå¼€å‘ç§¯ææ”¯æŒäººç±»åæ€ã€æ‰¹åˆ¤æ€§æ€ç»´å’ŒçŸ¥æƒ…å†³ç­–çš„ç³»ç»Ÿï¼Œè€Œä¸æ˜¯ä»…ä»…è¢«åŠ¨åœ°æ¶ˆè´¹ä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16883v1">PDF</a> Presented at the 2025 ACM Workshop on Human-AI Interaction for   Augmented Reasoning</p>
<p><strong>Summary</strong></p>
<p>RAGç³»ç»Ÿé€šè¿‡èå…¥äº‹å®æ ¸æŸ¥å’Œè¯­å¢ƒç›¸å…³ä¿¡æ¯ï¼Œå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„è¾“å‡ºèƒ½åŠ›ã€‚ä½†å­˜åœ¨å…¬å¹³æ€§å’Œå¯é æ€§é—®é¢˜ï¼Œå¦‚åœ¨æ£€ç´¢å’Œç”Ÿæˆé˜¶æ®µå¯èƒ½å‡ºç°å¹»è§‰ï¼Œå½±å“ç”¨æˆ·æ¨ç†å’Œå†³ç­–ã€‚ç ”ç©¶é€šè¿‡æ•™è‚²æµ‹éªŒç¯å¢ƒæ¢ç´¢å®šåˆ¶è­¦å‘Šä¿¡æ¯å¯¹ç”¨æˆ·è¡Œä¸ºçš„å½±å“ï¼Œåˆæ­¥å‘ç°è­¦å‘Šè™½èƒ½æé«˜å¯¹é«˜çº§å¹»è§‰çš„å‡†ç¡®æ€§å’Œæ„è¯†ï¼Œä½†ä¹Ÿå¯èƒ½é€ æˆè®¤çŸ¥æ‘©æ“¦ï¼Œå¯¼è‡´æ··æ·†å’Œå¯¹ç³»ç»Ÿä¿¡ä»»çš„é™ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAGç³»ç»Ÿé€šè¿‡ç»“åˆäº‹å®æ ¸æŸ¥å’Œè¯­å¢ƒç›¸å…³ä¿¡æ¯ï¼Œå¢å¼ºäº†LLMçš„è¾“å‡ºã€‚</li>
<li>å…¬å¹³æ€§å’Œå¯é æ€§é—®é¢˜æ˜¯RAGç³»ç»Ÿé¢ä¸´çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå­˜åœ¨å‡ºç°å¹»è§‰çš„é£é™©ã€‚</li>
<li>è­¦å‘Šä¿¡æ¯å¯ä»¥æ”¹å–„ç”¨æˆ·å¯¹é«˜çº§å¹»è§‰çš„å‡†ç¡®æ€§å’Œæ„è¯†ã€‚</li>
<li>è­¦å‘Šä¿¡æ¯ä¹Ÿå¯èƒ½é€ æˆè®¤çŸ¥æ‘©æ“¦ï¼Œå¯¼è‡´ç”¨æˆ·æ··æ·†å’Œå¯¹ç³»ç»Ÿä¿¡ä»»çš„é™ä½ã€‚</li>
<li>ç ”ç©¶åœ¨æ•™è‚²æµ‹éªŒç¯å¢ƒä¸­æ¢ç´¢äº†å®šåˆ¶è­¦å‘Šä¿¡æ¯å¯¹ç”¨æˆ·è¡Œä¸ºçš„å½±å“ã€‚</li>
<li>æ­¤ç ”ç©¶å¯¹AIå¢å¼ºæ¨ç†çš„è´¡çŒ®åœ¨äºå¼€å‘ç§¯ææ”¯æŒäººç±»åæ€ã€æ‰¹åˆ¤æ€ç»´å’ŒçŸ¥æƒ…å†³ç­–çš„ç³»ç»Ÿï¼Œè€Œéä»…ä¾›è¢«åŠ¨æ¶ˆè´¹ä¿¡æ¯çš„ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a845c4fbac609c69bc1d8d0cc881424d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86b6c16d1db93144195d0e6aebd74844.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be3493d0e0432a5b7d4901df9c11181a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7bbc7c256c44842c026a682445b7d0c5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Exploring-How-LLMs-Capture-and-Represent-Domain-Specific-Knowledge"><a href="#Exploring-How-LLMs-Capture-and-Represent-Domain-Specific-Knowledge" class="headerlink" title="Exploring How LLMs Capture and Represent Domain-Specific Knowledge"></a>Exploring How LLMs Capture and Represent Domain-Specific Knowledge</h2><p><strong>Authors:Mirian Hipolito Garcia, Camille Couturier, Daniel Madrigal Diaz, Ankur Mallick, Anastasios Kyrillidis, Robert Sim, Victor Ruhle, Saravan Rajmohan</strong></p>
<p>We study whether Large Language Models (LLMs) inherently capture domain-specific nuances in natural language. Our experiments probe the domain sensitivity of LLMs by examining their ability to distinguish queries from different domains using hidden states generated during the prefill phase. We reveal latent domain-related trajectories that indicate the modelâ€™s internal recognition of query domains. We also study the robustness of these domain representations to variations in prompt styles and sources. Our approach leverages these representations for model selection, mapping the LLM that best matches the domain trace of the input query (i.e., the model with the highest performance on similar traces). Our findings show that LLMs can differentiate queries for related domains, and that the fine-tuned model is not always the most accurate. Unlike previous work, our interpretations apply to both closed and open-ended generative tasks </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦èƒ½åœ¨è‡ªç„¶è¯­è¨€ä¸­æ•è·ç‰¹å®šé¢†åŸŸçš„ç»†å¾®å·®åˆ«ã€‚æˆ‘ä»¬çš„å®éªŒé€šè¿‡æ£€æŸ¥é¢„å¡«å……é˜¶æ®µç”Ÿæˆçš„éšè—çŠ¶æ€æ¥æ¢ç´¢LLMçš„é¢†åŸŸæ•æ„Ÿæ€§ï¼Œä»¥æ£€éªŒå®ƒä»¬åŒºåˆ†ä¸åŒé¢†åŸŸæŸ¥è¯¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æ­ç¤ºäº†ä¸é¢†åŸŸç›¸å…³çš„æ½œåœ¨è½¨è¿¹ï¼Œè¡¨æ˜æ¨¡å‹å¯¹æŸ¥è¯¢é¢†åŸŸçš„å†…éƒ¨è¯†åˆ«ã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†è¿™äº›é¢†åŸŸè¡¨ç¤ºå¯¹ä¸åŒæç¤ºé£æ ¼å’Œæ¥æºçš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨è¿™äº›è¡¨ç¤ºæ¥è¿›è¡Œæ¨¡å‹é€‰æ‹©ï¼Œæ‰¾å‡ºæœ€ç¬¦åˆè¾“å…¥æŸ¥è¯¢é¢†åŸŸè½¨è¿¹çš„LLMï¼ˆå³åœ¨ç±»ä¼¼è½¨è¿¹ä¸Šè¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMèƒ½å¤ŸåŒºåˆ†ç›¸å…³é¢†åŸŸçš„æŸ¥è¯¢ï¼Œè€Œä¸”å¾®è°ƒæ¨¡å‹å¹¶ä¸æ€»æ˜¯æœ€å‡†ç¡®çš„ã€‚ä¸ä¹‹å‰çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬çš„è§£é‡Šé€‚ç”¨äºå°é—­ä»»åŠ¡å’Œå¼€æ”¾å¼çš„ç”Ÿæˆä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16871v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿæ•æ‰è‡ªç„¶è¯­è¨€ä¸­çš„é¢†åŸŸç‰¹å®šç»†å¾®å·®åˆ«ã€‚é€šè¿‡å®éªŒæ¢ç©¶LLMså¯¹æ¥è‡ªä¸åŒé¢†åŸŸæŸ¥è¯¢çš„åŒºåˆ†èƒ½åŠ›ï¼Œæ­ç¤ºæ¨¡å‹å†…éƒ¨å¯¹æŸ¥è¯¢é¢†åŸŸçš„è¯†åˆ«è½¨è¿¹ã€‚ç ”ç©¶è¿™äº›é¢†åŸŸè¡¨ç¤ºçš„ç¨³å¥æ€§ï¼Œå¹¶ç”¨äºæ¨¡å‹é€‰æ‹©ï¼Œæ‰¾åˆ°ä¸è¾“å…¥æŸ¥è¯¢é¢†åŸŸè½¨è¿¹æœ€åŒ¹é…çš„LLMã€‚ç ”ç©¶å‘ç°LLMsèƒ½åŒºåˆ†ç›¸å…³é¢†åŸŸçš„æŸ¥è¯¢ï¼Œä¸”å¾®è°ƒæ¨¡å‹å¹¶éæ€»æ˜¯æœ€å‡†ç¡®ã€‚æ­¤è§£è¯»é€‚ç”¨äºå°é—­å’Œå¼€æ”¾ç”Ÿæˆä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsèƒ½å¤Ÿæ•æ‰é¢†åŸŸç‰¹å®šçš„ç»†å¾®å·®åˆ«ã€‚</li>
<li>å®éªŒæ¢ç©¶LLMså¯¹æ¥è‡ªä¸åŒé¢†åŸŸçš„æŸ¥è¯¢çš„åŒºåˆ†èƒ½åŠ›ã€‚</li>
<li>LLMså†…éƒ¨å­˜åœ¨è¯†åˆ«æŸ¥è¯¢é¢†åŸŸçš„è½¨è¿¹ã€‚</li>
<li>é¢†åŸŸè¡¨ç¤ºçš„ç¨³å¥æ€§ç ”ç©¶å¯¹äºæ¨¡å‹é€‰æ‹©è‡³å…³é‡è¦ã€‚</li>
<li>è¾“å…¥æŸ¥è¯¢çš„é¢†åŸŸè½¨è¿¹ä¸LLMçš„åŒ¹é…æƒ…å†µå½±å“æ¨¡å‹é€‰æ‹©çš„å‡†ç¡®æ€§ã€‚</li>
<li>LLMsèƒ½å¤ŸåŒºåˆ†ç›¸å…³é¢†åŸŸçš„æŸ¥è¯¢ï¼Œè¿™æ˜¯ä¹‹å‰çš„ä½œå“æ‰€æœªæ¶‰åŠçš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16871">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-705bb25b907b9e0b1f210f3f2c39406e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5b9022b5c9c212f88a312e966fbfc77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f2e6e15b6691708ac19d9748b540dc0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Emo-Pillars-Knowledge-Distillation-to-Support-Fine-Grained-Context-Aware-and-Context-Less-Emotion-Classification"><a href="#Emo-Pillars-Knowledge-Distillation-to-Support-Fine-Grained-Context-Aware-and-Context-Less-Emotion-Classification" class="headerlink" title="Emo Pillars: Knowledge Distillation to Support Fine-Grained   Context-Aware and Context-Less Emotion Classification"></a>Emo Pillars: Knowledge Distillation to Support Fine-Grained   Context-Aware and Context-Less Emotion Classification</h2><p><strong>Authors:Alexander Shvets</strong></p>
<p>Most datasets for sentiment analysis lack context in which an opinion was expressed, often crucial for emotion understanding, and are mainly limited by a few emotion categories. Foundation large language models (LLMs) like GPT-4 suffer from over-predicting emotions and are too resource-intensive. We design an LLM-based data synthesis pipeline and leverage a large model, Mistral-7b, for the generation of training examples for more accessible, lightweight BERT-type encoder models. We focus on enlarging the semantic diversity of examples and propose grounding the generation into a corpus of narratives to produce non-repetitive story-character-centered utterances with unique contexts over 28 emotion classes. By running 700K inferences in 450 GPU hours, we contribute with the dataset of 100K contextual and also 300K context-less examples to cover both scenarios. We use it for fine-tuning pre-trained encoders, which results in several Emo Pillars models. We show that Emo Pillars models are highly adaptive to new domains when tuned to specific tasks such as GoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on the first three. We also validate our dataset, conducting statistical analysis and human evaluation, and confirm the success of our measures in utterance diversification (although less for the neutral class) and context personalization, while pointing out the need for improved handling of out-of-taxonomy labels within the pipeline. </p>
<blockquote>
<p>å¤§å¤šæ•°æƒ…æ„Ÿåˆ†ææ•°æ®é›†ç¼ºä¹è¡¨è¾¾æ„è§çš„èƒŒæ™¯ï¼Œè¿™å¯¹äºæƒ…ç»ªç†è§£å¾€å¾€è‡³å…³é‡è¦ï¼Œå¹¶ä¸”ä¸»è¦å—åˆ°æœ‰é™çš„æƒ…ç»ªç±»åˆ«çš„é™åˆ¶ã€‚åƒGPT-4è¿™æ ·çš„åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­˜åœ¨è¿‡åº¦é¢„æµ‹æƒ…ç»ªå’Œè¿‡äºèµ„æºå¯†é›†çš„é—®é¢˜ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºLLMçš„æ•°æ®åˆæˆç®¡é“ï¼Œå¹¶åˆ©ç”¨ä¸€ä¸ªå¤§å‹æ¨¡å‹Mistral-7bæ¥ç”Ÿæˆæ›´å®¹æ˜“è®¿é—®ã€è½»é‡çº§çš„BERTç±»å‹ç¼–ç å™¨æ¨¡å‹çš„è®­ç»ƒç¤ºä¾‹ã€‚æˆ‘ä»¬ä¸“æ³¨äºå¢åŠ ç¤ºä¾‹çš„è¯­ä¹‰å¤šæ ·æ€§ï¼Œå¹¶æå‡ºå°†ç”Ÿæˆæ ¹æ¤äºå™äº‹è¯­æ–™åº“ä¸­ï¼Œä»¥äº§ç”Ÿä»¥æ•…äº‹è§’è‰²ä¸ºä¸­å¿ƒçš„ã€å…·æœ‰ç‹¬ç‰¹èƒŒæ™¯çš„ã€éé‡å¤æ€§çš„å‘è¨€ï¼Œæ¶µç›–28ä¸ªæƒ…ç»ªç±»åˆ«ã€‚é€šè¿‡è¿è¡Œ70ä¸‡æ¬¡çš„æ¨ç†åˆ†æï¼Œåœ¨45ä¸‡GPUå°æ—¶å†…ï¼Œæˆ‘ä»¬åˆ›å»ºäº†åŒ…å«10ä¸‡æ¡ä¸Šä¸‹æ–‡å’Œ30ä¸‡æ¡æ— ä¸Šä¸‹æ–‡ç¤ºä¾‹çš„æ•°æ®é›†ï¼Œä»¥è¦†ç›–è¿™ä¸¤ç§åœºæ™¯ã€‚æˆ‘ä»¬ç”¨å®ƒæ¥å¾®è°ƒé¢„è®­ç»ƒç¼–ç å™¨ï¼Œä»è€Œäº§ç”Ÿäº†å¤šä¸ªEmo Pillarsæ¨¡å‹ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå½“é’ˆå¯¹ç‰¹å®šä»»åŠ¡ï¼ˆå¦‚GoEmotionsã€ISEARã€IEMOCAPå’ŒEmoContextï¼‰è¿›è¡Œè°ƒæ•´æ—¶ï¼ŒEmo Pillarsæ¨¡å‹éå¸¸é€‚åº”æ–°é¢†åŸŸï¼Œå¹¶åœ¨å‰ä¸‰ä¸ªä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å¯¹æˆ‘ä»¬çš„æ•°æ®é›†è¿›è¡Œäº†ç»Ÿè®¡åˆ†æå’Œäººå·¥è¯„ä¼°ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æªæ–½åœ¨å‘è¨€å¤šæ ·åŒ–å’ŒèƒŒæ™¯ä¸ªæ€§åŒ–æ–¹é¢çš„æˆåŠŸï¼ˆå°½ç®¡ä¸­æ€§ç±»åˆ«çš„æ•ˆæœè¾ƒå°ï¼‰ï¼ŒåŒæ—¶æŒ‡å‡ºäº†ç®¡é“ä¸­å¯¹è¶…å‡ºåˆ†ç±»æ ‡å‡†çš„æ ‡ç­¾å¤„ç†çš„éœ€è¦æ”¹è¿›ä¹‹å¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16856v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æƒ…æ„Ÿåˆ†ææ•°æ®é›†ç¼ºä¹ä¸Šä¸‹æ–‡çš„é—®é¢˜ï¼Œè®¾è®¡äº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°æ®åˆæˆç®¡é“ï¼Œåˆ©ç”¨Mistral-7bæ¨¡å‹ç”Ÿæˆè®­ç»ƒæ ·æœ¬ï¼Œä¸ºæ›´æ˜“äºè®¿é—®ã€è½»é‡çº§çš„BERTç±»å‹ç¼–ç å™¨æ¨¡å‹æä¾›è®­ç»ƒæ•°æ®ã€‚è¯¥ç ”ç©¶é€šè¿‡æ‰©å¤§è¯­ä¹‰å¤šæ ·æ€§å¹¶æå‡ºåŸºäºå™äº‹è¯­æ–™åº“çš„ç”Ÿæˆæ–¹æ³•ï¼Œåˆ›å»ºäº†åŒ…å«ç‹¬ç‰¹ä¸Šä¸‹æ–‡å’Œ28ä¸ªæƒ…æ„Ÿç±»åˆ«çš„æ•°æ®é›†ã€‚ç»è¿‡å¯¹é¢„è®­ç»ƒç¼–ç å™¨çš„å¾®è°ƒï¼Œç”Ÿæˆäº†é«˜åº¦é€‚åº”æ–°é¢†åŸŸä»»åŠ¡çš„Emo Pillarsæ¨¡å‹ï¼Œå¹¶åœ¨GoEmotionsã€ISEARã€IEMOCAPå’ŒEmoContextç­‰ä»»åŠ¡ä¸Šè¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶è¿˜è¿›è¡Œäº†ç»Ÿè®¡åˆ†æå’Œäººç±»è¯„ä¼°ï¼ŒéªŒè¯äº†æ•°æ®é›†çš„æœ‰æ•ˆæ€§å’ŒæˆåŠŸæªæ–½çš„å®æ–½æƒ…å†µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æƒ…æ„Ÿåˆ†ææ•°æ®é›†ç¼ºä¹ä¸Šä¸‹æ–‡ï¼Œè¿™å¯¹äºæƒ…æ„Ÿç†è§£è‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°æ®åˆæˆç®¡é“ï¼Œè§£å†³äº†ç°æœ‰æƒ…æ„Ÿåˆ†ææ•°æ®é›†çš„å±€é™æ€§ã€‚</li>
<li>åˆ©ç”¨Mistral-7bæ¨¡å‹ç”Ÿæˆè®­ç»ƒæ ·æœ¬ï¼Œä¸ºBERTç±»å‹ç¼–ç å™¨æ¨¡å‹æä¾›æ•°æ®ã€‚</li>
<li>å¼ºè°ƒæ‰©å¤§è¯­ä¹‰å¤šæ ·æ€§å’ŒåŸºäºå™äº‹è¯­æ–™åº“çš„ç”Ÿæˆæ–¹æ³•çš„é‡è¦æ€§ã€‚</li>
<li>åˆ›å»ºäº†åŒ…å«ç‹¬ç‰¹ä¸Šä¸‹æ–‡å’Œå¤šç§æƒ…æ„Ÿç±»åˆ«çš„æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒç¼–ç å™¨ï¼Œç”Ÿæˆäº†é€‚åº”æ–°é¢†åŸŸä»»åŠ¡çš„Emo Pillarsæ¨¡å‹ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16856">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f8d9641e5a529833cb4c19177a36f6c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e12793d4b99436e913b5b12c97291e40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41cc06ebeb9a28c1471ddddaf918d3ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70c17942555f9f8954434afd05912bbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eaa8290de4a3093ad1dffb44dd55dbbe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6abdb6dda3f170a4905e1038e57ae931.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LRASGen-LLM-based-RESTful-API-Specification-Generation"><a href="#LRASGen-LLM-based-RESTful-API-Specification-Generation" class="headerlink" title="LRASGen: LLM-based RESTful API Specification Generation"></a>LRASGen: LLM-based RESTful API Specification Generation</h2><p><strong>Authors:Sida Deng, Rubing Huang, Man Zhang, Chenhui Cui, Dave Towey, Rongcun Wang</strong></p>
<p>REpresentation State Transfer (REST) is an architectural style for designing web applications that enable scalable, stateless communication between clients and servers via common HTTP techniques. Web APIs that employ the REST style are known as RESTful (or REST) APIs. When using or testing a RESTful API, developers may need to employ its specification, which is often defined by open-source standards such as the OpenAPI Specification (OAS). However, it can be very time-consuming and error-prone to write and update these specifications, which may negatively impact the use of RESTful APIs, especially when the software requirements change. Many tools and methods have been proposed to solve this problem, such as Respector and Swagger Core. OAS generation can be regarded as a common text-generation task that creates a formal description of API endpoints derived from the source code. A potential solution for this may involve using Large Language Models (LLMs), which have strong capabilities in both code understanding and text generation. Motivated by this, we propose a novel approach for generating the OASs of RESTful APIs using LLMs: LLM-based RESTful API-Specification Generation (LRASGen). To the best of our knowledge, this is the first use of LLMs and API source code to generate OASs for RESTful APIs. Compared with existing tools and methods, LRASGen can generate the OASs, even when the implementation is incomplete (with partial code, and&#x2F;or missing annotations&#x2F;comments, etc.). To evaluate the LRASGen performance, we conducted a series of empirical studies on 20 real-world RESTful APIs. The results show that two LLMs (GPT-4o mini and DeepSeek V3) can both support LARSGen to generate accurate specifications, and LRASGen-generated specifications cover an average of 48.85% more missed entities than the developer-provided specifications. </p>
<blockquote>
<p>RESTï¼ˆRepresentation State Transferï¼‰æ˜¯ä¸€ç§ç”¨äºè®¾è®¡Webåº”ç”¨ç¨‹åºçš„æ¶æ„é£æ ¼ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡å¸¸è§çš„HTTPæŠ€æœ¯å®ç°å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ä¹‹é—´å¯ä¼¸ç¼©ã€æ— çŠ¶æ€é€šä¿¡ã€‚é‡‡ç”¨RESTé£æ ¼çš„Web APIè¢«ç§°ä¸ºRESTfulï¼ˆæˆ–RESTï¼‰APIã€‚åœ¨ä½¿ç”¨æˆ–æµ‹è¯•RESTful APIæ—¶ï¼Œå¼€å‘äººå‘˜å¯èƒ½éœ€è¦ä½¿ç”¨å…¶è§„èŒƒï¼Œè¯¥è§„èŒƒé€šå¸¸ç”±å¼€æ”¾æºä»£ç æ ‡å‡†ï¼ˆå¦‚OpenAPIè§„èŒƒï¼ˆOASï¼‰ï¼‰æ‰€å®šä¹‰ã€‚ç„¶è€Œï¼Œç¼–å†™å’Œæ›´æ–°è¿™äº›è§„èŒƒå¯èƒ½ä¼šéå¸¸è€—æ—¶ä¸”å®¹æ˜“å‡ºé”™ï¼Œè¿™å¯èƒ½ä¼šç»™RESTful APIçš„ä½¿ç”¨å¸¦æ¥è´Ÿé¢å½±å“ï¼Œå°¤å…¶æ˜¯åœ¨è½¯ä»¶éœ€æ±‚å‘ç”Ÿå˜åŒ–æ—¶ã€‚å·²ç»æå‡ºäº†è®¸å¤šå·¥å…·å’Œæ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä¾‹å¦‚Respectorå’ŒSwagger Coreã€‚OASç”Ÿæˆå¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªå¸¸è§çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œå®ƒä»æºä»£ç ä¸­åˆ›å»ºAPIç«¯ç‚¹çš„æ­£å¼æè¿°ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜çš„ä¸€ä¸ªæ½œåœ¨è§£å†³æ–¹æ¡ˆæ˜¯è¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼ŒLLMåœ¨ä»£ç ç†è§£å’Œæ–‡æœ¬ç”Ÿæˆæ–¹é¢éƒ½å…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºLLMç”ŸæˆRESTful APIçš„OASsçš„æ–°æ–¹æ³•ï¼šåŸºäºLLMçš„RESTful APIè§„èŒƒç”Ÿæˆï¼ˆLRASGenï¼‰ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡ä½¿ç”¨LLMå’ŒAPIæºä»£ç æ¥ä¸ºRESTful APIç”ŸæˆOASsã€‚ä¸ç°æœ‰å·¥å…·å’Œæ–¹æ³•ç›¸æ¯”ï¼ŒLRASGenèƒ½å¤Ÿåœ¨å®ç°ä¸å®Œæ•´çš„æƒ…å†µä¸‹ç”ŸæˆOASï¼ˆåŒ…æ‹¬éƒ¨åˆ†ä»£ç ã€ç¼ºå°‘æ³¨é‡Š&#x2F;æ³¨é‡Šç­‰ï¼‰ã€‚ä¸ºäº†è¯„ä¼°LRASGençš„æ€§èƒ½ï¼Œæˆ‘ä»¬å¯¹20ä¸ªçœŸå®çš„RESTful APIè¿›è¡Œäº†ä¸€ç³»åˆ—å®è¯ç ”ç©¶ã€‚ç»“æœè¡¨æ˜ï¼Œä¸¤ç§LLMï¼ˆGPT-4o miniå’ŒDeepSeek V3ï¼‰éƒ½èƒ½æ”¯æŒLRASGenç”Ÿæˆå‡†ç¡®çš„è§„èŒƒï¼Œå¹¶ä¸”LRASGenç”Ÿæˆçš„è§„èŒƒå¹³å‡è¦†ç›–å¼€å‘è€…æä¾›çš„è§„èŒƒä¸­é—æ¼å®ä½“çš„48.85%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16833v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>RESTï¼ˆè¡¨ç°å±‚çŠ¶æ€è½¬ç§»ï¼‰æ˜¯ä¸€ç§ç”¨äºè®¾è®¡Webåº”ç”¨ç¨‹åºçš„æ¶æ„é£æ ¼ï¼Œæ”¯æŒå®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ä¹‹é—´é€šè¿‡å¸¸è§çš„HTTPæŠ€æœ¯è¿›è¡Œå¯æ‰©å±•ã€æ— çŠ¶æ€é€šä¿¡ã€‚å½“ä½¿ç”¨æˆ–æµ‹è¯•RESTful APIæ—¶ï¼Œå¼€å‘è€…éœ€è¦éµå¾ªå¼€æ”¾åº”ç”¨ç¨‹åºæ¥å£è§„èŒƒï¼ˆOpenAPI Specificationï¼Œç®€ç§°OASï¼‰ã€‚ç„¶è€Œï¼Œç¼–å†™å’Œæ›´æ–°è¿™äº›è§„èŒƒå¯èƒ½ä¼šéå¸¸è€—æ—¶ä¸”å®¹æ˜“å‡ºé”™ï¼Œè¿™å¯èƒ½ä¼šå¯¹RESTful APIçš„ä½¿ç”¨äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œäººä»¬æå‡ºäº†å¤šç§å·¥å…·å’Œè§£å†³æ–¹æ¡ˆã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œé¦–æ¬¡å°è¯•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç‰¹æ€§ç”ŸæˆRESTful APIçš„OASã€‚ä¸€é¡¹æè®®æ˜¯é‡‡ç”¨LLMç”Ÿæˆçš„APIè§„èŒƒç”Ÿæˆæ–¹æ³•ï¼ˆLRASGenï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»APIæºä»£ç ç”Ÿæˆæ­£å¼çš„ç«¯ç‚¹æè¿°ã€‚é€šè¿‡å®è¯ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨LLMç”Ÿæˆçš„è§„èŒƒèƒ½å¤Ÿè¦†ç›–æ›´å¤šè¢«é—æ¼çš„å®ä½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RESTæ˜¯ä¸€ç§Webåº”ç”¨ç¨‹åºæ¶æ„é£æ ¼ï¼Œæ”¯æŒå¯æ‰©å±•ã€æ— çŠ¶æ€é€šä¿¡ã€‚</li>
<li>OpenAPI Specificationï¼ˆOASï¼‰æ˜¯RESTful APIçš„è§„èŒƒæ ‡å‡†ã€‚</li>
<li>ç¼–å†™å’Œæ›´æ–°OASå¯èƒ½ä¼šéå¸¸è€—æ—¶ä¸”å®¹æ˜“å‡ºé”™ã€‚</li>
<li>LLMå…·æœ‰ä»£ç ç†è§£å’Œæ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œå¯ç”¨äºè§£å†³è¯¥é—®é¢˜ã€‚</li>
<li>LRASGenæ˜¯é¦–æ¬¡åˆ©ç”¨LLMå’ŒAPIæºä»£ç ç”ŸæˆRESTful APIçš„OASçš„æ–¹æ³•ã€‚</li>
<li>LRASGenèƒ½å¤Ÿåœ¨APIå®ç°ä¸å®Œæ•´çš„æƒ…å†µä¸‹ç”ŸæˆOASã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16833">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24a9bb9991da6680807dc91e4eb76901.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Decoupled-Global-Local-Alignment-for-Improving-Compositional-Understanding"><a href="#Decoupled-Global-Local-Alignment-for-Improving-Compositional-Understanding" class="headerlink" title="Decoupled Global-Local Alignment for Improving Compositional   Understanding"></a>Decoupled Global-Local Alignment for Improving Compositional   Understanding</h2><p><strong>Authors:Xiaoxing Hu, Kaicheng Yang, Jun Wang, Haoran Xu, Ziyong Feng, Yupei Wang</strong></p>
<p>Contrastive Language-Image Pre-training (CLIP) has achieved success on multiple downstream tasks by aligning image and text modalities. However, the nature of global contrastive learning limits CLIPâ€™s ability to comprehend compositional concepts, such as relations and attributes. Although recent studies employ global hard negative samples to improve compositional understanding, these methods significantly compromise the modelâ€™s inherent general capabilities by forcibly distancing textual negative samples from images in the embedding space. To overcome this limitation, we introduce a Decoupled Global-Local Alignment (DeGLA) framework that improves compositional understanding while substantially mitigating losses in general capabilities. To optimize the retention of the modelâ€™s inherent capabilities, we incorporate a self-distillation mechanism within the global alignment process, aligning the learnable image-text encoder with a frozen teacher model derived from an exponential moving average. Under the constraint of self-distillation, it effectively mitigates the catastrophic forgetting of pretrained knowledge during fine-tuning. To improve compositional understanding, we first leverage the in-context learning capability of Large Language Models (LLMs) to construct about 2M high-quality negative captions across five types. Subsequently, we propose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC) loss to enhance vision-language compositionally. Extensive experimental results demonstrate the effectiveness of the DeGLA framework. Compared to previous state-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across the VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average performance improvement of 13.0% on zero-shot classification tasks across eleven datasets. Our code will be released at <a target="_blank" rel="noopener" href="https://github.com/xiaoxing2001/DeGLA">https://github.com/xiaoxing2001/DeGLA</a> </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰é€šè¿‡å›¾åƒå’Œæ–‡æœ¬æ¨¡æ€çš„å¯¹é½ï¼Œåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šå–å¾—äº†æˆåŠŸã€‚ç„¶è€Œï¼Œå…¨å±€å¯¹æ¯”å­¦ä¹ çš„æ€§è´¨é™åˆ¶äº†CLIPå¯¹ç»„åˆæ¦‚å¿µï¼ˆå¦‚å…³ç³»å’Œå±æ€§ï¼‰çš„ç†è§£èƒ½åŠ›ã€‚å°½ç®¡æœ€è¿‘æœ‰ç ”ç©¶é‡‡ç”¨å…¨å±€ç¡¬è´Ÿæ ·æœ¬æ”¹è¿›ç»„åˆç†è§£ï¼Œä½†è¿™äº›æ–¹æ³•é€šè¿‡å¼ºåˆ¶åœ¨åµŒå…¥ç©ºé—´ä¸­æ‹‰å¼€æ–‡æœ¬è´Ÿæ ·æœ¬ä¸å›¾åƒçš„è·ç¦»ï¼Œä»è€Œæå¤§åœ°å‰Šå¼±äº†æ¨¡å‹å›ºæœ‰çš„é€šç”¨èƒ½åŠ›ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†å»è€¦å…¨å±€-å±€éƒ¨å¯¹é½ï¼ˆDeGLAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æé«˜äº†ç»„åˆç†è§£èƒ½åŠ›ï¼ŒåŒæ—¶å¤§å¤§å‡è½»äº†é€šç”¨èƒ½åŠ›çš„æŸå¤±ã€‚ä¸ºäº†ä¼˜åŒ–æ¨¡å‹å›ºæœ‰èƒ½åŠ›çš„ä¿ç•™ï¼Œæˆ‘ä»¬åœ¨å…¨å±€å¯¹é½è¿‡ç¨‹ä¸­èå…¥è‡ªæˆ‘è’¸é¦æœºåˆ¶ï¼Œå°†å¯å­¦ä¹ çš„å›¾åƒæ–‡æœ¬ç¼–ç å™¨ä¸åŸºäºæŒ‡æ•°ç§»åŠ¨å¹³å‡çš„å†»ç»“æ•™å¸ˆæ¨¡å‹è¿›è¡Œå¯¹é½ã€‚åœ¨è‡ªæˆ‘è’¸é¦çš„çº¦æŸä¸‹ï¼Œå®ƒæœ‰æ•ˆåœ°å‡è½»äº†å¾®è°ƒè¿‡ç¨‹ä¸­é¢„è®­ç»ƒçŸ¥è¯†çš„ç¾éš¾æ€§é—å¿˜ã€‚ä¸ºæé«˜ç»„åˆç†è§£èƒ½åŠ›ï¼Œæˆ‘ä»¬é¦–å…ˆå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œæ„å»ºçº¦2Mç§é«˜è´¨é‡è´Ÿæè¿°ï¼ˆè·¨è¶Šäº”ç§ç±»å‹ï¼‰ã€‚éšåï¼Œæˆ‘ä»¬æå‡ºå›¾åƒåŸºç¡€å¯¹æ¯”ï¼ˆIGCï¼‰æŸå¤±å’Œæ–‡æœ¬åŸºç¡€å¯¹æ¯”ï¼ˆTGCï¼‰æŸå¤±ï¼Œä»¥å¢å¼ºè§†è§‰è¯­è¨€çš„ç»„åˆèƒ½åŠ›ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜äº†DeGLAæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚ä¸ä¹‹å‰çš„å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼ŒDeGLAåœ¨VALSEã€SugarCrepeå’ŒAROåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†3.5%çš„æ€§èƒ½ã€‚åŒæ—¶ï¼Œå®ƒåœ¨11ä¸ªæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬åˆ†ç±»ä»»åŠ¡ä¸­å¹³å‡æ€§èƒ½æé«˜äº†13.0%ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/xiaoxing2001/DeGLA%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/xiaoxing2001/DeGLAä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16801v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>DeGLAæ¡†æ¶é€šè¿‡å¼•å…¥è§£è€¦å…¨å±€å±€éƒ¨å¯¹é½æŠ€æœ¯ï¼Œå…‹æœäº†CLIPæ¨¡å‹åœ¨å¤„ç†ç»„åˆæ¦‚å¿µæ—¶çš„å±€é™æ€§ï¼ŒåŒæ—¶å‡å°‘äº†æ¨¡å‹é€šç”¨èƒ½åŠ›çš„æŸå¤±ã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªæˆ‘è’¸é¦æœºåˆ¶ä¼˜åŒ–å…¨å±€å¯¹é½è¿‡ç¨‹ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›æ„å»ºé«˜è´¨é‡è´Ÿæ ·æœ¬ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æå‡ºäº†å›¾åƒåŸºç¡€å¯¹æ¯”ï¼ˆIGCï¼‰æŸå¤±å’Œæ–‡å­—åŸºç¡€å¯¹æ¯”ï¼ˆTGCï¼‰æŸå¤±ï¼Œä»¥åŠ å¼ºè§†è§‰è¯­è¨€çš„ç»„åˆç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeGLAæ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>DeGLAæ¡†æ¶è§£å†³äº†CLIPåœ¨å¤„ç†ç»„åˆæ¦‚å¿µä¸Šçš„å±€é™æ€§ï¼Œå¦‚å…³ç³»å’Œå±æ€§ã€‚</li>
<li>é€šè¿‡è§£è€¦å…¨å±€å±€éƒ¨å¯¹é½æŠ€æœ¯ï¼ŒDeGLAæé«˜äº†æ¨¡å‹çš„ç»„åˆç†è§£èƒ½åŠ›ã€‚</li>
<li>è‡ªæˆ‘è’¸é¦æœºåˆ¶ç”¨äºä¼˜åŒ–å…¨å±€å¯¹é½è¿‡ç¨‹ï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹çš„å›ºæœ‰é€šç”¨èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›æ„å»ºé«˜è´¨é‡è´Ÿæ ·æœ¬ï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†å›¾åƒåŸºç¡€å¯¹æ¯”ï¼ˆIGCï¼‰æŸå¤±å’Œæ–‡å­—åŸºç¡€å¯¹æ¯”ï¼ˆTGCï¼‰æŸå¤±ï¼Œä»¥å¢å¼ºè§†è§‰è¯­è¨€çš„ç»„åˆç†è§£ã€‚</li>
<li>DeGLAæ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡æé«˜äº†3.5%çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16801">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2db2f3addc94b743fbf65af3a5710c51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdf704306236c6de10570f5602da6d58.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f43663f2e0204696ae807215ca5e7312.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f081ca317cdc985401093df7db721ec1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-829f7495220d609dc47157424815c160.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87fbee24d7d205a9d595a552c32ee41b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Towards-Explainable-AI-Multi-Modal-Transformer-for-Video-based-Image-Description-Generation"><a href="#Towards-Explainable-AI-Multi-Modal-Transformer-for-Video-based-Image-Description-Generation" class="headerlink" title="Towards Explainable AI: Multi-Modal Transformer for Video-based Image   Description Generation"></a>Towards Explainable AI: Multi-Modal Transformer for Video-based Image   Description Generation</h2><p><strong>Authors:Lakshita Agarwal, Bindu Verma</strong></p>
<p>Understanding and analyzing video actions are essential for producing insightful and contextualized descriptions, especially for video-based applications like intelligent monitoring and autonomous systems. The proposed work introduces a novel framework for generating natural language descriptions from video datasets by combining textual and visual modalities. The suggested architecture makes use of ResNet50 to extract visual features from video frames that are taken from the Microsoft Research Video Description Corpus (MSVD), and Berkeley DeepDrive eXplanation (BDD-X) datasets. The extracted visual characteristics are converted into patch embeddings and then run through an encoder-decoder model based on Generative Pre-trained Transformer-2 (GPT-2). In order to align textual and visual representations and guarantee high-quality description production, the system uses multi-head self-attention and cross-attention techniques. The modelâ€™s efficacy is demonstrated by performance evaluation using BLEU (1-4), CIDEr, METEOR, and ROUGE-L. The suggested framework outperforms traditional methods with BLEU-4 scores of 0.755 (BDD-X) and 0.778 (MSVD), CIDEr scores of 1.235 (BDD-X) and 1.315 (MSVD), METEOR scores of 0.312 (BDD-X) and 0.329 (MSVD), and ROUGE-L scores of 0.782 (BDD-X) and 0.795 (MSVD). By producing human-like, contextually relevant descriptions, strengthening interpretability, and improving real-world applications, this research advances explainable AI. </p>
<blockquote>
<p>ç†è§£å’Œåˆ†æè§†é¢‘åŠ¨ä½œå¯¹äºç”Ÿæˆæœ‰æ´å¯ŸåŠ›å’Œæƒ…å¢ƒåŒ–çš„æè¿°è‡³å…³é‡è¦ï¼Œå°¤å…¶å¯¹äºåŸºäºè§†é¢‘çš„åº”ç”¨ï¼Œå¦‚æ™ºèƒ½ç›‘æ§å’Œè‡ªä¸»ç³»ç»Ÿã€‚æ‰€æå‡ºçš„å·¥ä½œå¼•å…¥äº†ä¸€ä¸ªæ–°é¢–æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ–‡æœ¬å’Œè§†è§‰æ¨¡æ€æ¥ä»è§†é¢‘æ•°æ®é›†ä¸­ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°ã€‚å»ºè®®çš„æ¶æ„åˆ©ç”¨ResNet50ä»å¾®è½¯ç ”ç©¶è§†é¢‘æè¿°è¯­æ–™åº“ï¼ˆMSVDï¼‰å’ŒBerkeley DeepDrive eXplanationï¼ˆBDD-Xï¼‰æ•°æ®é›†ä¸­æå–è§†é¢‘å¸§çš„è§†è§‰ç‰¹å¾ã€‚æå–çš„è§†è§‰ç‰¹å¾è¢«è½¬æ¢ä¸ºè¡¥ä¸åµŒå…¥ï¼Œç„¶åé€šè¿‡åŸºäºç”Ÿæˆé¢„è®­ç»ƒè½¬æ¢å™¨-2ï¼ˆGPT-2ï¼‰çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹è¿›è¡Œå¤„ç†ã€‚ä¸ºäº†å¯¹é½æ–‡æœ¬å’Œè§†è§‰è¡¨ç¤ºå¹¶ç¡®ä¿é«˜è´¨é‡æè¿°çš„äº§ç”Ÿï¼Œç³»ç»Ÿä½¿ç”¨äº†å¤šå¤´è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›æŠ€æœ¯ã€‚è¯¥æ¨¡å‹çš„æ•ˆèƒ½é€šè¿‡BLEUï¼ˆ1-4ï¼‰ã€CIDErã€METEORå’ŒROUGE-Lçš„æ€§èƒ½è¯„ä¼°æ¥è¯æ˜ã€‚å»ºè®®çš„æ¡†æ¶åœ¨BLEUåˆ†æ•°ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå…¶ä¸­BDD-Xçš„BLEU-4åˆ†æ•°ä¸º0.755ï¼ŒMSVDçš„BLEU-4åˆ†æ•°ä¸º0.778ï¼›åœ¨CIDEråˆ†æ•°ä¸Šï¼ŒBDD-Xçš„CIDEråˆ†æ•°ä¸º1.235ï¼ŒMSVDçš„CIDEråˆ†æ•°ä¸º1.315ï¼›åœ¨METEORåˆ†æ•°ä¸Šï¼ŒBDD-Xçš„METEORåˆ†æ•°ä¸º0.312ï¼ŒMSVDçš„METEORåˆ†æ•°ä¸º0.329ï¼›åœ¨ROUGE-Låˆ†æ•°ä¸Šï¼ŒBDD-Xçš„ROUGE-Låˆ†æ•°ä¸º0.782ï¼ŒMSVDçš„ROUGE-Låˆ†æ•°ä¸º0.795ã€‚è¯¥ç ”ç©¶é€šè¿‡ç”Ÿæˆäººç±»èˆ¬çš„ã€ä¸Šä¸‹æ–‡ç›¸å…³çš„æè¿°ï¼ŒåŠ å¼ºäº†è§£é‡Šæ€§ï¼Œå¹¶æ”¹è¿›äº†ç°å®ä¸–ç•Œçš„åº”ç”¨ï¼Œä»è€Œæ¨åŠ¨äº†å¯è§£é‡Šäººå·¥æ™ºèƒ½çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16788v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆæ–‡æœ¬å’Œè§†è§‰æ¨¡æ€ï¼Œä»è§†é¢‘æ•°æ®é›†ä¸­ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°çš„æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ResNet50ä»å¾®è½¯ç ”ç©¶è§†é¢‘æè¿°è¯­æ–™åº“ï¼ˆMSVDï¼‰å’ŒBerkeley DeepDrive eXplanationï¼ˆBDD-Xï¼‰æ•°æ®é›†ä¸­æå–è§†é¢‘å¸§çš„è§†è§‰ç‰¹å¾ï¼Œå°†æå–çš„è§†è§‰ç‰¹å¾è½¬æ¢ä¸ºè¡¥ä¸åµŒå…¥ï¼Œç„¶åé€šè¿‡åŸºäºç”Ÿæˆé¢„è®­ç»ƒTransformer-2ï¼ˆGPT-2ï¼‰çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹è¿›è¡Œå¤„ç†ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›æŠ€æœ¯ï¼Œä»¥å®ç°æ–‡æœ¬å’Œè§†è§‰è¡¨ç¤ºçš„å¯¹é½ï¼Œä¿è¯é«˜è´¨é‡æè¿°çš„äº§ç”Ÿã€‚è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§é€šè¿‡BLEUï¼ˆ1-4ï¼‰ã€CIDErã€METEORå’ŒROUGE-Lç­‰è¯„ä¼°æŒ‡æ ‡è¿›è¡Œå±•ç¤ºï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå…¶åœ¨BDD-Xå’ŒMSVDæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡æœ‰æ‰€è¶…è¶Šã€‚è¯¥ç ”ç©¶ä¸ºäººå·¥æ™ºèƒ½çš„å¯è§£é‡Šæ€§å¸¦æ¥äº†è¿›æ­¥ï¼Œèƒ½å¤Ÿäº§ç”Ÿäººç±»èˆ¬çš„ã€ä¸Šä¸‹æ–‡ç›¸å…³çš„æè¿°ï¼Œå¹¶æ”¹å–„ç°å®ä¸–ç•Œçš„åº”ç”¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°æ¡†æ¶ï¼Œå¯ä»¥ä»è§†é¢‘æ•°æ®é›†ä¸­ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°ã€‚</li>
<li>åˆ©ç”¨ResNet50ä»è§†é¢‘å¸§ä¸­æå–è§†è§‰ç‰¹å¾ã€‚</li>
<li>é‡‡ç”¨GPT-2ä¸ºåŸºç¡€çš„çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹æ¥å¤„ç†è§†è§‰ç‰¹å¾å’Œç”Ÿæˆæ–‡æœ¬æè¿°ã€‚</li>
<li>ä½¿ç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›æŠ€æœ¯å®ç°æ–‡æœ¬å’Œè§†è§‰è¡¨ç¤ºçš„å¯¹é½ã€‚</li>
<li>åœ¨BDD-Xå’ŒMSVDæ•°æ®é›†ä¸Šå±•ç¤ºäº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>äº§ç”Ÿçš„æè¿°å…·æœ‰äººç±»èˆ¬çš„ã€ä¸Šä¸‹æ–‡ç›¸å…³æ€§ï¼Œå¢å¼ºäº†äººå·¥æ™ºèƒ½çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>æœ‰åŠ©äºæ”¹å–„ç°å®ä¸–ç•Œçš„è§†é¢‘åº”ç”¨ï¼Œå¦‚æ™ºèƒ½ç›‘æ§å’Œè‡ªä¸»ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16788">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-86800311a5343ed6a56c2d3794a79526.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-346cd188e5dbc974e3ae403a7765b07e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4bf1fc33e0752df3a570a3cc04af00f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa1504bc6a1fe157a9982c77d9fa4928.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Graph2Nav-3D-Object-Relation-Graph-Generation-to-Robot-Navigation"><a href="#Graph2Nav-3D-Object-Relation-Graph-Generation-to-Robot-Navigation" class="headerlink" title="Graph2Nav: 3D Object-Relation Graph Generation to Robot Navigation"></a>Graph2Nav: 3D Object-Relation Graph Generation to Robot Navigation</h2><p><strong>Authors:Tixiao Shan, Abhinav Rajvanshi, Niluthpol Mithun, Han-Pang Chiu</strong></p>
<p>We propose Graph2Nav, a real-time 3D object-relation graph generation framework, for autonomous navigation in the real world. Our framework fully generates and exploits both 3D objects and a rich set of semantic relationships among objects in a 3D layered scene graph, which is applicable to both indoor and outdoor scenes. It learns to generate 3D semantic relations among objects, by leveraging and advancing state-of-the-art 2D panoptic scene graph works into the 3D world via 3D semantic mapping techniques. This approach avoids previous training data constraints in learning 3D scene graphs directly from 3D data. We conduct experiments to validate the accuracy in locating 3D objects and labeling object-relations in our 3D scene graphs. We also evaluate the impact of Graph2Nav via integration with SayNav, a state-of-the-art planner based on large language models, on an unmanned ground robot to object search tasks in real environments. Our results demonstrate that modeling object relations in our scene graphs improves search efficiency in these navigation tasks. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºGraph2Navï¼Œè¿™æ˜¯ä¸€ä¸ªå®æ—¶3Då¯¹è±¡å…³ç³»å›¾ç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºç°å®ä¸–ç•Œçš„è‡ªä¸»å¯¼èˆªã€‚æˆ‘ä»¬çš„æ¡†æ¶å…¨é¢ç”Ÿæˆå¹¶åˆ©ç”¨3Då¯¹è±¡ä»¥åŠ3Dåˆ†å±‚åœºæ™¯å›¾ä¸­å¯¹è±¡ä¹‹é—´ä¸°å¯Œçš„è¯­ä¹‰å…³ç³»ã€‚è¯¥æ¡†æ¶é€‚ç”¨äºå®¤å†…å’Œå®¤å¤–åœºæ™¯ã€‚å®ƒé€šè¿‡åˆ©ç”¨æœ€å…ˆè¿›çš„2Då…¨æ™¯åœºæ™¯å›¾å·¥ä½œï¼Œå€ŸåŠ©3Dè¯­ä¹‰æ˜ å°„æŠ€æœ¯å°†å…¶æ‰©å±•åˆ°3Dä¸–ç•Œï¼Œå­¦ä¹ ç”Ÿæˆå¯¹è±¡ä¹‹é—´çš„3Dè¯­ä¹‰å…³ç³»ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ä¹‹å‰ç›´æ¥ä»3Dæ•°æ®å­¦ä¹ 3Dåœºæ™¯å›¾æ—¶çš„è®­ç»ƒæ•°æ®çº¦æŸã€‚æˆ‘ä»¬è¿›è¡Œäº†å®éªŒï¼ŒéªŒè¯äº†åœ¨æˆ‘ä»¬3Dåœºæ™¯å›¾ä¸­å®šä½3Då¯¹è±¡å’Œæ ‡æ³¨å¯¹è±¡å…³ç³»çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬è¿˜é€šè¿‡å°†ä¸SayNavï¼ˆåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æœ€æ–°è§„åˆ’å™¨ï¼‰é›†æˆï¼Œåœ¨æ— äººå‘˜åœ°é¢æœºå™¨äººä¸Šè¿›è¡Œå®é™…ç¯å¢ƒä¸­çš„å¯¹è±¡æœç´¢ä»»åŠ¡ï¼Œä»¥è¯„ä¼°Graph2Navçš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨æˆ‘ä»¬çš„åœºæ™¯å›¾ä¸­å»ºæ¨¡å¯¹è±¡å…³ç³»å¯ä»¥æé«˜è¿™äº›å¯¼èˆªä»»åŠ¡çš„æœç´¢æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16782v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–°ä¸€ä»£å¯¼èˆªæŠ€æœ¯Graph2Navï¼Œé€šè¿‡å®æ—¶ç”Ÿæˆ3Dç‰©ä½“å…³ç³»å›¾å®ç°å®¤å†…å¤–è‡ªä¸»å¯¼èˆªã€‚è¯¥æŠ€æœ¯é€šè¿‡åˆ©ç”¨å…ˆè¿›çš„äºŒç»´å…¨æ™¯åœºæ™¯å›¾æŠ€æœ¯ï¼Œå€ŸåŠ©ä¸‰ç»´è¯­ä¹‰æ˜ å°„æŠ€æœ¯ç”Ÿæˆä¸‰ç»´è¯­ä¹‰å…³ç³»å›¾ï¼Œé¿å…ç›´æ¥ä»ä¸‰ç»´æ•°æ®ä¸­å­¦ä¹ ä¸‰ç»´åœºæ™¯å›¾çš„è®­ç»ƒæ•°æ®é™åˆ¶ã€‚å®éªŒè¯æ˜å…¶åœ¨å®šä½ä¸‰ç»´ç‰©ä½“å’Œæ ‡æ³¨ç‰©ä½“å…³ç³»ä¸Šçš„å‡†ç¡®æ€§ï¼Œå¹¶ç»“åˆSayNavæŠ€æœ¯ï¼Œæé«˜æ— äººåœ°é¢æœºå™¨äººåœ¨çœŸå®ç¯å¢ƒä¸­çš„æœç´¢æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Graph2Navæ˜¯ä¸€ä¸ªå®æ—¶3Dç‰©ä½“å…³ç³»å›¾ç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºå®¤å†…å¤–è‡ªä¸»å¯¼èˆªã€‚</li>
<li>è¯¥æŠ€æœ¯ç»“åˆäº†å…ˆè¿›çš„äºŒç»´å…¨æ™¯åœºæ™¯å›¾æŠ€æœ¯å’Œä¸‰ç»´è¯­ä¹‰æ˜ å°„æŠ€æœ¯ï¼Œç”Ÿæˆä¸‰ç»´è¯­ä¹‰å…³ç³»å›¾ã€‚</li>
<li>é¿å…ç›´æ¥ä»ä¸‰ç»´æ•°æ®ä¸­å­¦ä¹ ä¸‰ç»´åœºæ™¯å›¾çš„è®­ç»ƒæ•°æ®é™åˆ¶ã€‚</li>
<li>å®éªŒè¯æ˜Graph2Navåœ¨å®šä½ä¸‰ç»´ç‰©ä½“å’Œæ ‡æ³¨ç‰©ä½“å…³ç³»ä¸Šçš„å‡†ç¡®æ€§ã€‚</li>
<li>Graph2Navä¸SayNavæŠ€æœ¯ç»“åˆï¼Œæé«˜äº†æ— äººåœ°é¢æœºå™¨äººåœ¨çœŸå®ç¯å¢ƒä¸­çš„æœç´¢æ•ˆç‡ã€‚</li>
<li>å»ºæ¨¡ç‰©ä½“å…³ç³»åœ¨å¯¼èˆªä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f918232734a8b6ea8fbc3fc8184250d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9ebb4c6ca6db6b0bd7a1c03875a0159.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bfc025c91a2fdd5f6a82c410f48c2c71.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69f5f5a24bcf7d52246e92d6b069fd06.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="How-Effective-are-Generative-Large-Language-Models-in-Performing-Requirements-Classification"><a href="#How-Effective-are-Generative-Large-Language-Models-in-Performing-Requirements-Classification" class="headerlink" title="How Effective are Generative Large Language Models in Performing   Requirements Classification?"></a>How Effective are Generative Large Language Models in Performing   Requirements Classification?</h2><p><strong>Authors:Waad Alhoshan, Alessio Ferrari, Liping Zhao</strong></p>
<p>In recent years, transformer-based large language models (LLMs) have revolutionised natural language processing (NLP), with generative models opening new possibilities for tasks that require context-aware text generation. Requirements engineering (RE) has also seen a surge in the experimentation of LLMs for different tasks, including trace-link detection, regulatory compliance, and others. Requirements classification is a common task in RE. While non-generative LLMs like BERT have been successfully applied to this task, there has been limited exploration of generative LLMs. This gap raises an important question: how well can generative LLMs, which produce context-aware outputs, perform in requirements classification? In this study, we explore the effectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing both binary and multi-class requirements classification. We design an extensive experimental study involving over 400 experiments across three widely used datasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes that while factors like prompt design and LLM architecture are universally important, others-such as dataset variations-have a more situational impact, depending on the complexity of the classification task. This insight can guide future model development and deployment strategies, focusing on optimising prompt structures and aligning model architectures with task-specific needs for improved performance. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒåŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„æ ¼å±€ï¼Œç”Ÿæˆæ¨¡å‹ä¸ºéœ€è¦ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ–‡æœ¬ç”Ÿæˆçš„ä»»åŠ¡å¸¦æ¥äº†æ–°çš„å¯èƒ½æ€§ã€‚éœ€æ±‚å·¥ç¨‹ï¼ˆREï¼‰é¢†åŸŸåœ¨LLMçš„ä¸åŒä»»åŠ¡å®éªŒä¸­ä¹Ÿå‡ºç°äº†æ¿€å¢ï¼ŒåŒ…æ‹¬è·Ÿè¸ªé“¾æ¥æ£€æµ‹ã€æ³•è§„åˆè§„æ€§ç­‰ä»»åŠ¡ã€‚éœ€æ±‚åˆ†ç±»æ˜¯REä¸­çš„å¸¸è§ä»»åŠ¡ã€‚è™½ç„¶åƒBERTè¿™æ ·çš„éç”Ÿæˆæ€§LLMå·²ç»æˆåŠŸåº”ç”¨äºæ­¤ä»»åŠ¡ï¼Œä½†å¯¹ç”Ÿæˆæ€§LLMçš„æ¢ç´¢ä»ç„¶æœ‰é™ã€‚è¿™ä¸€å·®è·å¼•å‘äº†ä¸€ä¸ªé‡è¦é—®é¢˜ï¼šèƒ½å¤Ÿäº§ç”Ÿä¸Šä¸‹æ–‡æ„ŸçŸ¥è¾“å‡ºçš„ç”Ÿæˆæ€§LLMåœ¨éœ€æ±‚åˆ†ç±»ä¸­çš„è¡¨ç°å¦‚ä½•ï¼Ÿåœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸‰ç§ç”Ÿæˆæ€§LLMâ€”â€”Bloomã€Gemmaå’ŒLlamaåœ¨äºŒå…ƒå’Œå¤šå…ƒéœ€æ±‚åˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€é¡¹å¹¿æ³›çš„å®éªŒç ”ç©¶ï¼Œæ¶‰åŠä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ï¼ˆPROMISE NFRã€Functional-Qualityå’ŒSecReqï¼‰çš„400å¤šä¸ªå®éªŒã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œè™½ç„¶æç¤ºè®¾è®¡å’ŒLLMæ¶æ„ç­‰å› ç´ å…·æœ‰æ™®éé‡è¦æ€§ï¼Œä½†å…¶ä»–å› ç´ ï¼Œå¦‚æ•°æ®é›†å˜åŒ–ï¼Œå¯¹åˆ†ç±»ä»»åŠ¡çš„å¤æ‚æ€§å…·æœ‰æ›´æƒ…å¢ƒåŒ–çš„å½±å“ã€‚è¿™ä¸€è§è§£å¯ä»¥ä¸ºæœªæ¥çš„æ¨¡å‹å¼€å‘å’Œéƒ¨ç½²ç­–ç•¥æä¾›æŒ‡å¯¼ï¼Œä¾§é‡äºä¼˜åŒ–æç¤ºç»“æ„ï¼Œå¹¶æ ¹æ®ç‰¹å®šä»»åŠ¡éœ€æ±‚è°ƒæ•´æ¨¡å‹æ¶æ„ä»¥æé«˜æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16768v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘å¹´æ¥ï¼ŒåŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸæ€èµ·äº†ä¸€åœºé©å‘½ï¼Œç”Ÿæˆå¼æ¨¡å‹ä¸ºéœ€è¦ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡å¸¦æ¥äº†æ–°çš„å¯èƒ½æ€§ã€‚åœ¨éœ€æ±‚å·¥ç¨‹ï¼ˆREï¼‰é¢†åŸŸï¼ŒLLMçš„åº”ç”¨å®éªŒä¹Ÿå±‚å‡ºä¸ç©·ï¼ŒåŒ…æ‹¬è·Ÿè¸ªé“¾æ¥æ£€æµ‹ã€æ³•è§„åˆè§„ç­‰ä»»åŠ¡ã€‚éœ€æ±‚åˆ†ç±»æ˜¯REä¸­çš„å¸¸è§ä»»åŠ¡ã€‚è™½ç„¶éç”Ÿæˆå¼LLMï¼Œå¦‚BERTï¼Œå·²è¢«æˆåŠŸåº”ç”¨äºæ­¤ä»»åŠ¡ï¼Œä½†å¯¹ç”Ÿæˆå¼LLMçš„æ¢ç´¢å´æœ‰é™ã€‚æœ¬æ–‡æ¢ç´¢äº†ä¸‰ç§ç”Ÿæˆå¼LLMâ€”â€”Bloomã€Gemmaå’ŒLlamaåœ¨äºŒå…ƒå’Œå¤šå…ƒéœ€æ±‚åˆ†ç±»ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€é¡¹å¹¿æ³›çš„å®éªŒç ”ç©¶ï¼Œæ¶‰åŠä¸‰ä¸ªå¸¸ç”¨æ•°æ®é›†ï¼ˆPROMISE NFRã€Functional-Qualityå’ŒSecReqï¼‰çš„400å¤šæ¬¡å®éªŒã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶æç¤ºè®¾è®¡å’ŒLLMæ¶æ„ç­‰å› ç´ å…·æœ‰æ™®éé‡è¦æ€§ï¼Œä½†æ•°æ®é›†å˜åŒ–ç­‰å› ç´ çš„å½±å“æ›´å…·æƒ…å¢ƒæ€§ï¼Œå–å†³äºåˆ†ç±»ä»»åŠ¡çš„å¤æ‚æ€§ã€‚è¿™ä¸€è§è§£å¯ä»¥ä¸ºæœªæ¥çš„æ¨¡å‹å¼€å‘å’Œéƒ¨ç½²ç­–ç•¥æä¾›æŒ‡å¯¼ï¼Œé‡ç‚¹ä¼˜åŒ–æç¤ºç»“æ„ï¼Œå¹¶æ ¹æ®ä»»åŠ¡éœ€æ±‚è°ƒæ•´æ¨¡å‹æ¶æ„ï¼Œä»¥æé«˜æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å˜å‹å™¨ä¸ºåŸºç¡€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸæœ‰é‡å¤§çªç ´ï¼Œç”Ÿæˆå¼æ¨¡å‹å°¤å…¶åœ¨éœ€è¦ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¦æ±‚å·¥ç¨‹ï¼ˆREï¼‰å·²å¼€å§‹å¹¿æ³›å®éªŒLLMåœ¨å„ç§ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬éœ€æ±‚åˆ†ç±»ã€‚</li>
<li>è™½ç„¶éç”Ÿæˆå¼LLMå·²è¢«æˆåŠŸåº”ç”¨äºéœ€æ±‚åˆ†ç±»ä»»åŠ¡ï¼Œä½†å¯¹ç”Ÿæˆå¼LLMåœ¨æ­¤æ–¹é¢çš„æ¢ç´¢ä»æœ‰é™ã€‚</li>
<li>æœ¬ç ”ç©¶æ¢ç´¢äº†ä¸‰ç§ç”Ÿæˆå¼LLMï¼ˆBloomã€Gemmaå’ŒLlamaï¼‰åœ¨äºŒå…ƒå’Œå¤šå…ƒéœ€æ±‚åˆ†ç±»ä¸­çš„è¡¨ç°ã€‚</li>
<li>å®éªŒç ”ç©¶è¡¨æ˜ï¼Œæç¤ºè®¾è®¡å’ŒLLMæ¶æ„å¯¹æ¨¡å‹æ€§èƒ½æœ‰æ™®éå½±å“ã€‚</li>
<li>æ•°æ®é›†å˜åŒ–å¯¹åˆ†ç±»ä»»åŠ¡æ€§èƒ½çš„å½±å“å…·æœ‰æƒ…å¢ƒæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16768">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-727e796c41a9be3e85e2aab64a7f7360.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Lightweight-Latent-Verifiers-for-Efficient-Meta-Generation-Strategies"><a href="#Lightweight-Latent-Verifiers-for-Efficient-Meta-Generation-Strategies" class="headerlink" title="Lightweight Latent Verifiers for Efficient Meta-Generation Strategies"></a>Lightweight Latent Verifiers for Efficient Meta-Generation Strategies</h2><p><strong>Authors:Bartosz Piotrowski, Witold Drzewakowski, Konrad Staniszewski, Piotr MiÅ‚oÅ›</strong></p>
<p>Verifiers are auxiliary models that assess the correctness of outputs generated by base large language models (LLMs). They play a crucial role in many strategies for solving reasoning-intensive problems with LLMs. Typically, verifiers are LLMs themselves, often as large (or larger) than the base model they support, making them computationally expensive. In this work, we introduce a novel lightweight verification approach, LiLaVe, which reliably extracts correctness signals from the hidden states of the base LLM. A key advantage of LiLaVe is its ability to operate with only a small fraction of the computational budget required by traditional LLM-based verifiers. To demonstrate its practicality, we couple LiLaVe with popular meta-generation strategies, like best-of-n or self-consistency. Moreover, we design novel LiLaVe-based approaches, like conditional self-correction or conditional majority voting, that significantly improve both accuracy and efficiency in generation tasks with smaller LLMs. Our work demonstrates the fruitfulness of extracting latent information from the hidden states of LLMs, and opens the door to scalable and resource-efficient solutions for reasoning-intensive applications. </p>
<blockquote>
<p>éªŒè¯å™¨æ˜¯è¾…åŠ©æ¨¡å‹ï¼Œç”¨äºè¯„ä¼°åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„è¾“å‡ºçš„æ­£ç¡®æ€§ã€‚åœ¨LLMè§£å†³æ¨ç†å¯†é›†å‹é—®é¢˜çš„å¤šç§ç­–ç•¥ä¸­ï¼Œå®ƒä»¬æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚é€šå¸¸ï¼ŒéªŒè¯å™¨æœ¬èº«å°±æ˜¯LLMï¼Œé€šå¸¸ä¸å…¶æ”¯æŒçš„åŸºç¡€æ¨¡å‹å¤§å°ç›¸åŒï¼ˆæˆ–æ›´å¤§ï¼‰ï¼Œä½¿å…¶åœ¨è®¡ç®—ä¸Šæˆæœ¬é«˜æ˜‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°å‹è½»é‡çº§éªŒè¯æ–¹æ³•LiLaVeï¼Œå®ƒå¯ä»¥ä»åŸºç¡€LLMçš„éšè—çŠ¶æ€ä¸­å¯é åœ°æå–æ­£ç¡®æ€§ä¿¡å·ã€‚LiLaVeçš„ä¸»è¦ä¼˜ç‚¹æ˜¯å…¶ä»…éœ€ä¼ ç»ŸLLMéªŒè¯å™¨æ‰€éœ€è®¡ç®—é¢„ç®—çš„ä¸€å°éƒ¨åˆ†å³å¯è¿è¡Œã€‚ä¸ºäº†è¯æ˜å…¶å®ç”¨æ€§ï¼Œæˆ‘ä»¬å°†LiLaVeä¸æµè¡Œçš„å…ƒç”Ÿæˆç­–ç•¥ï¼ˆå¦‚né€‰æœ€ä½³æˆ–è‡ªæˆ‘ä¸€è‡´æ€§ï¼‰ç›¸ç»“åˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†åŸºäºLiLaVeçš„æ–°æ–¹æ³•ï¼Œå¦‚æ¡ä»¶è‡ªæˆ‘æ ¡æ­£æˆ–æ¡ä»¶å¤šæ•°æŠ•ç¥¨ï¼Œè¿™äº›æ–¹æ³•åœ¨å°å‹LLMçš„ç”Ÿæˆä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æˆ‘ä»¬çš„å·¥ä½œå±•ç¤ºäº†ä»LLMçš„éšè—çŠ¶æ€ä¸­æå–æ½œåœ¨ä¿¡æ¯çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºæ¨ç†å¯†é›†å‹åº”ç”¨æ‰“å¼€äº†å¯æ‰©å±•å’Œèµ„æºé«˜æ•ˆè§£å†³æ–¹æ¡ˆçš„å¤§é—¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16760v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„è½»é‡çº§éªŒè¯æ–¹æ³•LiLaVeï¼Œè¯¥æ–¹æ³•å¯ä»åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„éšè—çŠ¶æ€ä¸­æå–æ­£ç¡®æ€§ä¿¡å·ï¼Œç”¨äºéªŒè¯LLMç”Ÿæˆçš„è¾“å‡ºã€‚LiLaVeå…·æœ‰æ˜¾è‘—çš„è®¡ç®—æ•ˆç‡ä¼˜åŠ¿ï¼Œåªéœ€ä¼ ç»ŸLLMéªŒè¯å™¨æ‰€éœ€è®¡ç®—é¢„ç®—çš„ä¸€å°éƒ¨åˆ†å³å¯è¿è¡Œã€‚æ­¤å¤–ï¼ŒLiLaVeä¸æµè¡Œçš„å…ƒç”Ÿæˆç­–ç•¥ç›¸ç»“åˆï¼Œå¦‚æœ€ä½³næˆ–è‡ªæˆ‘ä¸€è‡´æ€§ç­‰ï¼Œä»¥æé«˜ç”Ÿæˆä»»åŠ¡çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æœ¬ç ”ç©¶è¯æ˜äº†ä»LLMçš„éšè—çŠ¶æ€ä¸­æŒ–æ˜æ½œåœ¨ä¿¡æ¯çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºèµ„æºå¯†é›†å‹åº”ç”¨æ‰“å¼€äº†å¯æ‰©å±•ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆä¹‹é—¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LiLaVeæ˜¯ä¸€ç§æ–°å‹çš„è½»é‡çº§éªŒè¯æ–¹æ³•ï¼Œèƒ½å¤Ÿä»LLMçš„éšè—çŠ¶æ€ä¸­æå–æ­£ç¡®æ€§ä¿¡å·ã€‚</li>
<li>LiLaVeå…·æœ‰æ˜¾è‘—çš„è®¡ç®—æ•ˆç‡ä¼˜åŠ¿ï¼Œå¯ä»¥å‡å°‘å¯¹ä¼ ç»ŸLLMéªŒè¯å™¨çš„è®¡ç®—éœ€æ±‚ã€‚</li>
<li>LiLaVeå¯ä»¥ä¸æµè¡Œçš„å…ƒç”Ÿæˆç­–ç•¥ç»“åˆï¼Œå¦‚æœ€ä½³nå’Œè‡ªæˆ‘ä¸€è‡´æ€§ç­‰ï¼Œä»¥æé«˜ç”Ÿæˆä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚</li>
<li>æ–°å‹LiLaVeæ–¹æ³•å¦‚æ¡ä»¶è‡ªæˆ‘æ ¡æ­£å’Œæ¡ä»¶å¤šæ•°æŠ•ç¥¨ç­‰ï¼Œå¯ä»¥æ˜¾è‘—æé«˜ç”Ÿæˆä»»åŠ¡çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶è¯æ˜äº†ä»LLMéšè—çŠ¶æ€ä¸­æŒ–æ˜æ½œåœ¨ä¿¡æ¯çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>LiLaVeä¸ºå¯æ‰©å±•æ€§å’Œèµ„æºæ•ˆç‡æä¾›äº†è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†å¯†é›†å‹åº”ç”¨ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d3ffcfc10f5e0dc094085af20dea9f31.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="IRIS-Interactive-Research-Ideation-System-for-Accelerating-Scientific-Discovery"><a href="#IRIS-Interactive-Research-Ideation-System-for-Accelerating-Scientific-Discovery" class="headerlink" title="IRIS: Interactive Research Ideation System for Accelerating Scientific   Discovery"></a>IRIS: Interactive Research Ideation System for Accelerating Scientific   Discovery</h2><p><strong>Authors:Aniketh Garikaparthi, Manasi Patwardhan, Lovekesh Vig, Arman Cohan</strong></p>
<p>The rapid advancement in capabilities of large language models (LLMs) raises a pivotal question: How can LLMs accelerate scientific discovery? This work tackles the crucial first stage of research, generating novel hypotheses. While recent work on automated hypothesis generation focuses on multi-agent frameworks and extending test-time compute, none of the approaches effectively incorporate transparency and steerability through a synergistic Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS: Interactive Research Ideation System, an open-source platform designed for researchers to leverage LLM-assisted scientific ideation. IRIS incorporates innovative features to enhance ideation, including adaptive test-time compute expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism, and query-based literature synthesis. Designed to empower researchers with greater control and insight throughout the ideation process. We additionally conduct a user study with researchers across diverse disciplines, validating the effectiveness of our system in enhancing ideation. We open-source our code at <a target="_blank" rel="noopener" href="https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System">https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½åŠ›çš„å¿«é€Ÿå‘å±•å¼•å‘äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šLLMå¦‚ä½•åŠ é€Ÿç§‘å­¦å‘ç°ï¼Ÿè¿™é¡¹å·¥ä½œè§£å†³äº†ç ”ç©¶çš„å…³é”®ç¬¬ä¸€é˜¶æ®µï¼Œå³äº§ç”Ÿæ–°å‡è®¾ã€‚è™½ç„¶æœ€è¿‘åœ¨è‡ªåŠ¨å‡è®¾ç”Ÿæˆæ–¹é¢çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨å¤šä»£ç†æ¡†æ¶å’Œæ‰©å±•æµ‹è¯•æ—¶é—´è®¡ç®—ä¸Šï¼Œä½†æ²¡æœ‰ä¸€ä¸ªæ–¹æ³•èƒ½æœ‰æ•ˆåœ°é€šè¿‡ååŒäººæœºäº¤äº’ï¼ˆHITLï¼‰æ–¹æ³•çº³å…¥é€æ˜åº¦å’Œå¯æ§åˆ¶æ€§æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†IRISï¼šäº¤äº’å¼ç ”ç©¶åˆ›æ„ç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºå¹³å°ï¼Œæ—¨åœ¨è®©ç ”ç©¶äººå‘˜åˆ©ç”¨LLMè¾…åŠ©ç§‘å­¦åˆ›æ„ã€‚IRISç»“åˆäº†åˆ›æ–°åŠŸèƒ½æ¥å¢å¼ºåˆ›æ„ï¼ŒåŒ…æ‹¬é€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰çš„è‡ªé€‚åº”æµ‹è¯•æ—¶é—´è®¡ç®—æ‰©å±•ã€ç²¾ç»†çš„åé¦ˆæœºåˆ¶å’ŒåŸºäºæŸ¥è¯¢çš„æ–‡çŒ®ç»¼è¿°ã€‚è®¾è®¡æ­¤ç³»ç»Ÿæ—¨åœ¨èµ‹äºˆç ”ç©¶äººå‘˜åœ¨æ•´ä¸ªåˆ›æ„è¿‡ç¨‹ä¸­æ›´å¤§çš„æ§åˆ¶å’Œæ´å¯ŸåŠ›ã€‚æˆ‘ä»¬è¿˜ä¸ä¸åŒå­¦ç§‘çš„ç ”ç©¶äººå‘˜è¿›è¡Œäº†ç”¨æˆ·ç ”ç©¶ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨å¢å¼ºåˆ›æ„æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System">https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System</a>ä¸Šå…¬å¼€äº†æˆ‘ä»¬çš„ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16728v1">PDF</a> 6 pages main-text, 2 pages appendix</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•å¼•å‘äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šå¦‚ä½•åŠ é€Ÿç§‘å­¦å‘ç°ã€‚æœ¬ç ”ç©¶è§£å†³äº†ç ”ç©¶çš„å…³é”®ç¬¬ä¸€é˜¶æ®µï¼Œå³ç”Ÿæˆæ–°å‡è®¾ã€‚å°½ç®¡è¿‘æœŸè‡ªåŠ¨åŒ–å‡è®¾ç”Ÿæˆçš„ç ”ç©¶èšç„¦äºå¤šæ™ºèƒ½ä½“æ¡†æ¶å’Œæ‰©å±•æµ‹è¯•æ—¶é—´è®¡ç®—ï¼Œä½†æ²¡æœ‰ä»»ä½•æ–¹æ³•èƒ½æœ‰æ•ˆç»“åˆé€æ˜åº¦å’Œå¯æ“æ§æ€§é€šè¿‡äººæœºååŒå¾ªç¯æ–¹æ³•å®ç°ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†IRISç³»ç»Ÿï¼Œä¸€ä¸ªè®¾è®¡ç”¨äºç ”ç©¶äººå‘˜åˆ©ç”¨LLMè¾…åŠ©ç§‘å­¦æ„æ€çš„å¼€æºå¹³å°ã€‚IRISç»“åˆäº†åˆ›æ–°åŠŸèƒ½ä»¥å¢å¼ºæ„æ€èƒ½åŠ›ï¼ŒåŒ…æ‹¬è‡ªé€‚åº”æµ‹è¯•æ—¶é—´è®¡ç®—æ‰©å±•çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢æ–¹æ³•ã€ç²¾ç»†åé¦ˆæœºåˆ¶å’ŒåŸºäºæŸ¥è¯¢çš„æ–‡çŒ®ç»¼è¿°ç­‰ã€‚æˆ‘ä»¬è‡´åŠ›äºèµ‹äºˆç ”ç©¶äººå‘˜åœ¨æ„æ€è¿‡ç¨‹ä¸­æ›´å¤§çš„æ§åˆ¶å’Œæ´å¯ŸåŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹ä¸åŒå­¦ç§‘çš„å­¦è€…è¿›è¡Œäº†ç”¨æˆ·ç ”ç©¶ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨å¢å¼ºæ„æ€æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å·²åœ¨GitHubä¸Šå…¬å¼€ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System">https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŠ é€Ÿç§‘å­¦å‘ç°çš„å…³é”®åœ¨äºå¦‚ä½•ç”Ÿæˆæ–°å‡è®¾ã€‚</li>
<li>ç›®å‰çš„æ–¹æ³•ç¼ºä¹é€æ˜åº¦å’Œå¯æ“æ§æ€§ï¼Œæœ¬ç ”ç©¶é€šè¿‡äººæœºååŒå¾ªç¯æ–¹æ³•è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>IRISç³»ç»Ÿæ˜¯ä¸€ä¸ªå¼€æºå¹³å°ï¼Œè®¾è®¡ç”¨äºç ”ç©¶äººå‘˜åˆ©ç”¨LLMè¾…åŠ©ç§‘å­¦æ„æ€ã€‚</li>
<li>IRISç»“åˆäº†å¤šç§åˆ›æ–°åŠŸèƒ½ä»¥å¢å¼ºæ„æ€èƒ½åŠ›ï¼Œå¦‚è’™ç‰¹å¡æ´›æ ‘æœç´¢æ–¹æ³•ã€ç²¾ç»†åé¦ˆæœºåˆ¶å’ŒåŸºäºæŸ¥è¯¢çš„æ–‡çŒ®ç»¼è¿°ç­‰ã€‚</li>
<li>ç”¨æˆ·ç ”ç©¶éªŒè¯äº†IRISç³»ç»Ÿåœ¨å¢å¼ºæ„æ€æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16728">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-22465be5978cb85a6794fc25cad92c61.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d33c69219c52c5e5003f6defda4869a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c0249b6544f55b8a0e8cb5b3eaee9435.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb69cfdb80e1f3d0e112ef90ed36e52c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad0b98b8d7625819ed92e6eaa5af219b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Can-Large-Language-Models-Help-Multimodal-Language-Analysis-MMLA-A-Comprehensive-Benchmark"><a href="#Can-Large-Language-Models-Help-Multimodal-Language-Analysis-MMLA-A-Comprehensive-Benchmark" class="headerlink" title="Can Large Language Models Help Multimodal Language Analysis? MMLA: A   Comprehensive Benchmark"></a>Can Large Language Models Help Multimodal Language Analysis? MMLA: A   Comprehensive Benchmark</h2><p><strong>Authors:Hanlei Zhang, Zhuohang Li, Yeshuang Zhu, Hua Xu, Peiwu Wang, Jinchao Zhang, Jie Zhou, Haige Zhu</strong></p>
<p>Multimodal language analysis is a rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA comprises over 61K multimodal utterances drawn from both staged and real-world scenarios, covering six core dimensions of multimodal semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. We evaluate eight mainstream branches of LLMs and MLLMs using three methods: zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive experiments reveal that even fine-tuned models achieve only about 60%~70% accuracy, underscoring the limitations of current MLLMs in understanding complex human language. We believe that MMLA will serve as a solid foundation for exploring the potential of large language models in multimodal language analysis and provide valuable resources to advance this field. The datasets and code are open-sourced at <a target="_blank" rel="noopener" href="https://github.com/thuiar/MMLA">https://github.com/thuiar/MMLA</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€è¯­è¨€åˆ†ææ˜¯ä¸€ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼Œå®ƒåˆ©ç”¨å¤šç§æ¨¡æ€æ¥æé«˜å¯¹äººç±»ä¼šè¯è¯è¯­ä¸­é«˜å±‚æ¬¡è¯­ä¹‰çš„ç†è§£ã€‚å°½ç®¡è¿™ä¸€é¢†åŸŸéå¸¸é‡è¦ï¼Œä½†å…³äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨ç†è§£è®¤çŸ¥å±‚æ¬¡è¯­ä¹‰æ–¹é¢çš„èƒ½åŠ›çš„ç ”ç©¶å´å¾ˆå°‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MMLAï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè§£å†³è¿™ä¸€å·®è·çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚MMLAåŒ…å«è¶…è¿‡61,000æ¡æ¥è‡ªèˆå°å’ŒçœŸå®ä¸–ç•Œåœºæ™¯çš„å¤šæ¨¡æ€è¯è¯­ï¼Œæ¶µç›–å…­ä¸ªæ ¸å¿ƒçš„å¤šæ¨¡æ€è¯­ä¹‰ç»´åº¦ï¼šæ„å›¾ã€æƒ…æ„Ÿã€å¯¹è¯è¡Œä¸ºã€æƒ…æ„Ÿå€¾å‘ã€è®²è¯é£æ ¼å’Œæ²Ÿé€šè¡Œä¸ºã€‚æˆ‘ä»¬ä½¿ç”¨ä¸‰ç§æ–¹æ³•è¯„ä¼°äº†å…«ä¸ªä¸»æµçš„LLMå’ŒMLLMåˆ†æ”¯ï¼šé›¶æ ·æœ¬æ¨ç†ã€ç›‘ç£å¾®è°ƒä»¥åŠæŒ‡ä»¤å¾®è°ƒã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼Œå³ä½¿ç»è¿‡ç²¾ç»†è®­ç»ƒçš„æ¨¡å‹ä¹Ÿåªèƒ½è¾¾åˆ°çº¦60%~70%çš„å‡†ç¡®ç‡ï¼Œè¿™çªæ˜¾äº†å½“å‰MLLMåœ¨ç†è§£å¤æ‚äººç±»è¯­è¨€æ–¹é¢çš„å±€é™æ€§ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼ŒMMLAå°†ä¸ºæ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€è¯­è¨€åˆ†ææ–¹é¢çš„æ½œåŠ›æä¾›åšå®çš„åŸºç¡€ï¼Œå¹¶ä¸ºæ¨åŠ¨è¿™ä¸€é¢†åŸŸçš„å‘å±•æä¾›å®è´µçš„èµ„æºã€‚æ•°æ®é›†å’Œä»£ç å·²å¼€æºï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/thuiar/MMLA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/thuiar/MMLAè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16427v1">PDF</a> 23 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†å¤šæ¨¡æ€è¯­è¨€åˆ†æé¢†åŸŸçš„é‡è¦æ€§å’ŒæŒ‘æˆ˜ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªå…¨æ–°çš„è¯„ä¼°åŸºå‡†MMLAã€‚MMLAæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç†è§£è®¤çŸ¥çº§è¯­ä¹‰æ–¹é¢çš„èƒ½åŠ›ï¼Œæ¶µç›–äº†æ„å›¾ã€æƒ…æ„Ÿã€å¯¹è¯è¡Œä¸ºç­‰å…­ä¸ªæ ¸å¿ƒç»´åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰MLLMsåœ¨è¯¥é¢†åŸŸçš„ç†è§£ä»å­˜åœ¨å±€é™æ€§ã€‚MMLAçš„å¼€æºæ•°æ®é›†å’Œä»£ç ä¸ºæ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€è¯­è¨€åˆ†æä¸­çš„æ½œåŠ›æä¾›äº†å®è´µçš„èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€è¯­è¨€åˆ†ææ˜¯ä¸€ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼Œåˆ©ç”¨å¤šç§æ¨¡å¼å¢å¼ºå¯¹äººç±»ä¼šè¯è¨€å¤–ä¹‹æ„çš„é«˜çº§è¯­ä¹‰ç†è§£ã€‚</li>
<li>ç›®å‰é’ˆå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç†è§£è®¤çŸ¥çº§è¯­ä¹‰çš„ç ”ç©¶è¾ƒå°‘ã€‚</li>
<li>MMLAåŸºå‡†æ¶µç›–äº†æ„å›¾ã€æƒ…æ„Ÿã€å¯¹è¯è¡Œä¸ºç­‰å…­ä¸ªæ ¸å¿ƒç»´åº¦çš„å¤šæ¨¡æ€è¯­ä¹‰ã€‚</li>
<li>é€šè¿‡é›¶æ ·æœ¬æ¨ç†ã€ç›‘ç£å¾®è°ƒã€æŒ‡ä»¤å¾®è°ƒä¸‰ç§æ–¹æ³•ï¼Œå¯¹å…«ç§ä¸»æµLLMså’ŒMLLMsè¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œå³ä½¿ç»è¿‡ç²¾ç»†è°ƒæ•´çš„æ¨¡å‹ï¼Œå‡†ç¡®ç‡ä¹Ÿåªæœ‰çº¦60%~70%ï¼Œè¡¨æ˜å½“å‰MLLMsåœ¨ç†è§£å¤æ‚äººç±»è¯­è¨€æ–¹é¢çš„å±€é™æ€§ã€‚</li>
<li>MMLAä¸ºæ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€è¯­è¨€åˆ†æä¸­çš„æ½œåŠ›æä¾›äº†å®è´µçš„èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16427">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ce9bd974980754940082ff4b815cb593.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3292eb7453076a3df09ae79787826dca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd028fc0228a4ef56110959c9f7f4004.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-306600ee29d6c0f4af7c100cb15f0060.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Transformer-Based-Extraction-of-Statutory-Definitions-from-the-U-S-Code"><a href="#Transformer-Based-Extraction-of-Statutory-Definitions-from-the-U-S-Code" class="headerlink" title="Transformer-Based Extraction of Statutory Definitions from the U.S. Code"></a>Transformer-Based Extraction of Statutory Definitions from the U.S. Code</h2><p><strong>Authors:Arpana Hosabettu, Harsh Shah</strong></p>
<p>Automatic extraction of definitions from legal texts is critical for enhancing the comprehension and clarity of complex legal corpora such as the United States Code (U.S.C.). We present an advanced NLP system leveraging transformer-based architectures to automatically extract defined terms, their definitions, and their scope from the U.S.C. We address the challenges of automatically identifying legal definitions, extracting defined terms, and determining their scope within this complex corpus of over 200,000 pages of federal statutory law. Building upon previous feature-based machine learning methods, our updated model employs domain-specific transformers (Legal-BERT) fine-tuned specifically for statutory texts, significantly improving extraction accuracy. Our work implements a multi-stage pipeline that combines document structure analysis with state-of-the-art language models to process legal text from the XML version of the U.S. Code. Each paragraph is first classified using a fine-tuned legal domain BERT model to determine if it contains a definition. Our system then aggregates related paragraphs into coherent definitional units and applies a combination of attention mechanisms and rule-based patterns to extract defined terms and their jurisdictional scope. The definition extraction system is evaluated on multiple titles of the U.S. Code containing thousands of definitions, demonstrating significant improvements over previous approaches. Our best model achieves 96.8% precision and 98.9% recall (98.2% F1-score), substantially outperforming traditional machine learning classifiers. This work contributes to improving accessibility and understanding of legal information while establishing a foundation for downstream legal reasoning tasks. </p>
<blockquote>
<p>ä»æ³•å¾‹æ–‡æœ¬ä¸­è‡ªåŠ¨æå–å®šä¹‰å¯¹äºæé«˜å¯¹ç¾å›½æ³•å…¸ï¼ˆU.S.C.ï¼‰ç­‰å¤æ‚æ³•å¾‹è¯­æ–™çš„ç†è§£åŠ›å’Œæ¸…æ™°åº¦è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å…ˆè¿›çš„NLPç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨åŸºäºtransformerçš„æ¶æ„è‡ªåŠ¨ä»U.S.C.ä¸­æå–å®šä¹‰æœ¯è¯­ã€å®šä¹‰åŠå…¶èŒƒå›´ã€‚æˆ‘ä»¬è§£å†³äº†è‡ªåŠ¨è¯†åˆ«æ³•å¾‹å®šä¹‰ã€æå–å®šä¹‰æœ¯è¯­ä»¥åŠç¡®å®šå…¶åœ¨è¶…è¿‡20ä¸‡é¡µçš„è”é‚¦æ³•å®šæ³•å¾‹è¿™ä¸€å¤æ‚è¯­æ–™åº“ä¸­çš„èŒƒå›´çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ¨¡å‹å»ºç«‹åœ¨åŸºäºç‰¹å¾çš„æœºå™¨å­¦ä¹ æ–¹æ³•ä¹‹ä¸Šï¼Œé‡‡ç”¨é’ˆå¯¹æ³•å®šæ–‡æœ¬è¿›è¡Œå¾®è°ƒé¢†åŸŸçš„ç‰¹å®štransformerï¼ˆLegal-BERTï¼‰ï¼Œæ˜¾è‘—æé«˜äº†æå–å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œå®ç°äº†ä¸€ä¸ªå¤šé˜¶æ®µç®¡é“ï¼Œå®ƒå°†æ–‡æ¡£ç»“æ„åˆ†æä¸æœ€æ–°çš„è¯­è¨€æ¨¡å‹ç›¸ç»“åˆï¼Œå¤„ç†æ¥è‡ªç¾å›½æ³•å…¸XMLç‰ˆæœ¬çš„æ³•å¾‹æ–‡æœ¬ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨å¾®è°ƒçš„æ³•å¾‹é¢†åŸŸBERTæ¨¡å‹å¯¹æ¯ä¸€æ®µè¿›è¡Œåˆ†ç±»ï¼Œä»¥ç¡®å®šå…¶æ˜¯å¦åŒ…å«å®šä¹‰ã€‚ç„¶åï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿå°†ç›¸å…³çš„æ®µè½èšé›†æˆè¿è´¯çš„å®šä¹‰å•å…ƒï¼Œå¹¶ç»“åˆä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶å’ŒåŸºäºè§„åˆ™çš„æ¨¡å¼æ¥æå–å®šä¹‰æœ¯è¯­åŠå…¶å¸æ³•ç®¡è¾–èŒƒå›´ã€‚å®šä¹‰æå–ç³»ç»Ÿåœ¨åŒ…å«æ•°åƒä¸ªå®šä¹‰çš„ç¾å›½æ³•å…¸å¤šä¸ªæ ‡é¢˜ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†ä¸ä¹‹å‰æ–¹æ³•çš„æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬æœ€å¥½çš„æ¨¡å‹è¾¾åˆ°96.8%çš„ç²¾ç¡®åº¦å’Œ98.9%çš„å¬å›ç‡ï¼ˆ98.2%çš„F1åˆ†æ•°ï¼‰ï¼Œå¤§å¤§ä¼˜äºä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ åˆ†ç±»å™¨ã€‚è¿™é¡¹å·¥ä½œæœ‰åŠ©äºæé«˜æ³•å¾‹ä¿¡æ¯çš„å¯åŠæ€§å’Œç†è§£æ€§ï¼ŒåŒæ—¶ä¸ºä¸‹æ¸¸æ³•å¾‹æ¨ç†ä»»åŠ¡å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16353v1">PDF</a> 7 pages, to be published in IEEE AIIoT 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåˆ©ç”¨åŸºäºè½¬æ¢å™¨æ¶æ„çš„é«˜çº§NLPç³»ç»Ÿï¼Œè‡ªåŠ¨ä»ç¾å›½æ³•å…¸ï¼ˆU.S.Cï¼‰ä¸­æå–å®šä¹‰ã€æœ¯è¯­åŠå…¶èŒƒå›´ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ç»“åˆæ–‡æ¡£ç»“æ„åˆ†æå’Œæœ€æ–°è¯­è¨€æ¨¡å‹ï¼Œå®ç°äº†å¯¹æ³•å¾‹æ–‡æœ¬çš„å¤šé˜¶æ®µå¤„ç†ã€‚é€šè¿‡ç²¾ç»†è°ƒæ•´çš„æ³•å¾‹é¢†åŸŸBERTæ¨¡å‹ï¼Œç³»ç»Ÿèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«å’Œæå–å®šä¹‰ï¼Œå¹¶ç¡®å®šæœ¯è¯­çš„ç®¡è¾–èŒƒå›´ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨ç²¾åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°æ–¹é¢å‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚è¿™ä¸€ç ”ç©¶æœ‰åŠ©äºæé«˜æ³•å¾‹ä¿¡æ¯çš„å¯è®¿é—®æ€§å’Œç†è§£ï¼Œå¹¶ä¸ºä¸‹æ¸¸æ³•å¾‹æ¨ç†ä»»åŠ¡å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NLPç³»ç»Ÿç”¨äºè‡ªåŠ¨æå–æ³•å¾‹æ–‡æœ¬ä¸­çš„å®šä¹‰ã€æœ¯è¯­å’ŒèŒƒå›´ã€‚</li>
<li>ç³»ç»ŸåŸºäºè½¬æ¢å™¨æ¶æ„æ„å»ºï¼Œç»“åˆæ–‡æ¡£ç»“æ„åˆ†æå’Œæœ€æ–°è¯­è¨€æ¨¡å‹å¤„ç†æ³•å¾‹æ–‡æœ¬ã€‚</li>
<li>ä½¿ç”¨ç²¾ç»†è°ƒæ•´çš„æ³•å¾‹é¢†åŸŸBERTæ¨¡å‹æ¥è¯†åˆ«å¹¶æå–å®šä¹‰ã€‚</li>
<li>é€šè¿‡å¤šé˜¶æ®µå¤„ç†å®ç°ç›¸å…³æ®µè½çš„èšåˆå’Œæœ¯è¯­çš„ç®¡è¾–èŒƒå›´ç¡®å®šã€‚</li>
<li>ç³»ç»Ÿè¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œåœ¨ç²¾åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>æ­¤ç ”ç©¶æœ‰åŠ©äºæé«˜æ³•å¾‹ä¿¡æ¯çš„å¯è®¿é—®æ€§å’Œç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16353">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-47e6a395ea6e6afdb7f547694511028b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b444f428360b50a76b5ecd662abe5ff8.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="COBRA-Algorithm-Architecture-Co-optimized-Binary-Transformer-Accelerator-for-Edge-Inference"><a href="#COBRA-Algorithm-Architecture-Co-optimized-Binary-Transformer-Accelerator-for-Edge-Inference" class="headerlink" title="COBRA: Algorithm-Architecture Co-optimized Binary Transformer   Accelerator for Edge Inference"></a>COBRA: Algorithm-Architecture Co-optimized Binary Transformer   Accelerator for Edge Inference</h2><p><strong>Authors:Ye Qiao, Zhiheng Cheng, Yian Wang, Yifan Zhang, Yunzhe Deng, Sitao Huang</strong></p>
<p>Transformer-based models have demonstrated superior performance in various fields, including natural language processing and computer vision. However, their enormous model size and high demands in computation, memory, and communication limit their deployment to edge platforms for local, secure inference. Binary transformers offer a compact, low-complexity solution for edge deployment with reduced bandwidth needs and acceptable accuracy. However, existing binary transformers perform inefficiently on current hardware due to the lack of binary specific optimizations. To address this, we introduce COBRA, an algorithm-architecture co-optimized binary Transformer accelerator for edge computing. COBRA features a real 1-bit binary multiplication unit, enabling matrix operations with -1, 0, and +1 values, surpassing ternary methods. With further hardware-friendly optimizations in the attention block, COBRA achieves up to 3,894.7 GOPS throughput and 448.7 GOPS&#x2F;Watt energy efficiency on edge FPGAs, delivering a 311x energy efficiency improvement over GPUs and a 3.5x throughput improvement over the state-of-the-art binary accelerator, with only negligible inference accuracy degradation. </p>
<blockquote>
<p>åŸºäºTransformerçš„æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸè¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åºå¤§çš„æ¨¡å‹è§„æ¨¡ä»¥åŠå¯¹è®¡ç®—ã€å†…å­˜å’Œé€šä¿¡çš„é«˜è¦æ±‚ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨è¾¹ç¼˜å¹³å°ä¸Šçš„æœ¬åœ°å®‰å…¨æ¨ç†éƒ¨ç½²ã€‚äºŒè¿›åˆ¶è½¬æ¢å™¨æä¾›äº†ä¸€ç§ç´§å‡‘ã€ä½å¤æ‚åº¦çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºè¾¹ç¼˜éƒ¨ç½²ï¼Œå‡å°‘äº†å¸¦å®½éœ€æ±‚ï¼Œå¹¶ä¸”ä¿æŒäº†å¯æ¥å—çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç”±äºç¼ºå°‘é’ˆå¯¹äºŒè¿›åˆ¶çš„ç‰¹å®šä¼˜åŒ–ï¼Œç°æœ‰çš„äºŒè¿›åˆ¶è½¬æ¢å™¨åœ¨å½“å‰ç¡¬ä»¶ä¸Šçš„è¿è¡Œæ•ˆç‡ä¸é«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†COBRAï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè¾¹ç¼˜è®¡ç®—çš„ååŒä¼˜åŒ–çš„äºŒè¿›åˆ¶è½¬æ¢å™¨åŠ é€Ÿå™¨ã€‚COBRAå…·å¤‡çœŸæ­£çš„1ä½äºŒè¿›åˆ¶ä¹˜æ³•å•å…ƒï¼Œèƒ½å¤Ÿå®ç°ä½¿ç”¨-1ã€0å’Œ+1å€¼çš„çŸ©é˜µè¿ç®—ï¼Œè¶…è¶Šäº†ä¸‰å…ƒæ–¹æ³•ã€‚é€šè¿‡æ³¨æ„åŠ›æ¨¡å—çš„æ›´å¤šç¡¬ä»¶å‹å¥½ä¼˜åŒ–ï¼ŒCOBRAåœ¨è¾¹ç¼˜FPGAä¸Šå®ç°äº†é«˜è¾¾3894.7 GOPSçš„ååé‡ï¼Œèƒ½æ•ˆä¸ºæ¯ç“¦ç‰¹448.7 GOPSï¼Œç›¸è¾ƒäºGPUå®ç°äº†é«˜è¾¾311å€çš„èƒ½æ•ˆæå‡ï¼Œç›¸è¾ƒäºå½“å‰ä¸»æµçš„äºŒè¿›åˆ¶åŠ é€Ÿå™¨æé«˜äº†çº¦3.5å€çš„ååé‡æå‡ï¼ŒåŒæ—¶æ¨ç†ç²¾åº¦çš„æŸå¤±å‡ ä¹å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16269v1">PDF</a> </p>
<p><strong>Summary</strong><br>    äºŒè¿›åˆ¶TransformeråŠ é€Ÿå™¨COBRAï¼Œä¸“ä¸ºè¾¹ç¼˜è®¡ç®—è®¾è®¡ï¼Œå…·æœ‰1ä½äºŒè¿›åˆ¶ä¹˜æ³•å•å…ƒï¼Œå¯å®ç°çŸ©é˜µæ“ä½œï¼Œæé«˜èƒ½æºæ•ˆç‡å’Œååé‡ï¼Œé™ä½æ¨ç†ç²¾åº¦æŸå¤±ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ä¸­å­˜åœ¨è®¡ç®—ã€å†…å­˜å’Œé€šä¿¡éœ€æ±‚é«˜çš„æŒ‘æˆ˜ã€‚</li>
<li>äºŒè¿›åˆ¶Transformerä¸ºè§£å†³è¾¹ç¼˜éƒ¨ç½²é—®é¢˜æä¾›äº†ç´§å‡‘ã€ä½å¤æ‚åº¦çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç°æœ‰äºŒè¿›åˆ¶Transformeråœ¨å½“å‰ç¡¬ä»¶ä¸Šçš„æ€§èƒ½ä¸ä½³ï¼Œç¼ºä¹é’ˆå¯¹äºŒè¿›åˆ¶çš„ç‰¹å®šä¼˜åŒ–ã€‚</li>
<li>COBRAæ˜¯ä¸€ä¸ªå…±ä¼˜åŒ–çš„äºŒè¿›åˆ¶TransformeråŠ é€Ÿå™¨ï¼Œç”¨äºè¾¹ç¼˜è®¡ç®—ï¼Œå…·æœ‰1ä½äºŒè¿›åˆ¶ä¹˜æ³•å•å…ƒï¼Œå¯è¶…è¶Šä¸‰å…ƒæ–¹æ³•ã€‚</li>
<li>COBRAåœ¨è¾¹ç¼˜FPGAä¸Šå®ç°äº†é«˜è¾¾3,894.7GOPSçš„ååé‡å’Œ448.7GOPS&#x2F;Wattçš„èƒ½æ•ˆã€‚</li>
<li>ä¸GPUç›¸æ¯”ï¼ŒCOBRAå®ç°äº†311å€èƒ½æ•ˆæ”¹è¿›ï¼›ä¸ç°æœ‰äºŒè¿›åˆ¶åŠ é€Ÿå™¨ç›¸æ¯”ï¼Œååé‡æé«˜äº†3.5å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16269">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-baf037fd707d260918f7d88208a8c646.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcc253801077ecf6c67848506e72e235.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-269d2b206a1831a0bdaad93cdddcd488.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73ecb18e9a038a0704b6821ed52cbff5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fb387527664c227993eca561800e0226.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3df1f9c10a7fe870730eccfbe44ba18b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="FinNLI-Novel-Dataset-for-Multi-Genre-Financial-Natural-Language-Inference-Benchmarking"><a href="#FinNLI-Novel-Dataset-for-Multi-Genre-Financial-Natural-Language-Inference-Benchmarking" class="headerlink" title="FinNLI: Novel Dataset for Multi-Genre Financial Natural Language   Inference Benchmarking"></a>FinNLI: Novel Dataset for Multi-Genre Financial Natural Language   Inference Benchmarking</h2><p><strong>Authors:Jabez Magomere, Elena Kochkina, Samuel Mensah, Simerjot Kaur, Charese H. Smiley</strong></p>
<p>We introduce FinNLI, a benchmark dataset for Financial Natural Language Inference (FinNLI) across diverse financial texts like SEC Filings, Annual Reports, and Earnings Call transcripts. Our dataset framework ensures diverse premise-hypothesis pairs while minimizing spurious correlations. FinNLI comprises 21,304 pairs, including a high-quality test set of 3,304 instances annotated by finance experts. Evaluations show that domain shift significantly degrades general-domain NLI performance. The highest Macro F1 scores for pre-trained (PLMs) and large language models (LLMs) baselines are 74.57% and 78.62%, respectively, highlighting the datasetâ€™s difficulty. Surprisingly, instruction-tuned financial LLMs perform poorly, suggesting limited generalizability. FinNLI exposes weaknesses in current LLMs for financial reasoning, indicating room for improvement. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†FinNLIï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé‡‘èè‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆFinNLIï¼‰çš„åŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–SECæ–‡ä»¶ã€å¹´æŠ¥å’Œæ”¶ç›Šç”µè¯è®°å½•ç­‰å¤šæ ·åŒ–çš„é‡‘èæ–‡æœ¬ã€‚æˆ‘ä»¬çš„æ•°æ®é›†æ¡†æ¶ç¡®ä¿å¤šæ ·åŒ–çš„å‰æå‡è®¾å¯¹ï¼ŒåŒæ—¶æœ€å°åŒ–å¶ç„¶å…³è”ã€‚FinNLIåŒ…å«21,304å¯¹æ ·æœ¬ï¼Œå…¶ä¸­åŒ…æ‹¬ç”±é‡‘èä¸“å®¶æ³¨é‡Šçš„é«˜è´¨é‡æµ‹è¯•é›†ï¼Œå…±3,304ä¸ªå®ä¾‹ã€‚è¯„ä¼°è¡¨æ˜ï¼Œé¢†åŸŸè½¬ç§»ä¼šæ˜¾è‘—å½±å“é€šç”¨é¢†åŸŸçš„NLIæ€§èƒ½ã€‚é¢„è®­ç»ƒï¼ˆPLMï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºå‡†çš„æœ€é«˜å®è§‚F1åˆ†æ•°åˆ†åˆ«ä¸º74.57%å’Œ78.62%ï¼Œçªæ˜¾äº†æ•°æ®é›†çš„éš¾åº¦ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼ŒæŒ‰æŒ‡ä»¤è°ƒæ•´çš„é‡‘èLLMè¡¨ç°ä¸ä½³ï¼Œè¡¨æ˜å…¶æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚FinNLIæš´éœ²äº†å½“å‰ç”¨äºé‡‘èæ¨ç†çš„LLMçš„å¼±ç‚¹ï¼Œè¡¨æ˜è¿˜æœ‰æ”¹è¿›çš„ç©ºé—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16188v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æˆ‘ä»¬æ¨å‡ºäº†FinNLIï¼Œè¿™æ˜¯ä¸€ä¸ªé¢å‘é‡‘èæ–‡æœ¬çš„è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆFinNLIï¼‰åŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–äº†SECæ–‡ä»¶ã€å¹´æŠ¥å’Œæ”¶ç›ŠæŠ¥å‘Šç­‰é‡‘èæ–‡æœ¬ã€‚æ•°æ®é›†æ¡†æ¶ç¡®ä¿äº†å¤šæ ·çš„å‰æå‡è®¾å¯¹ï¼ŒåŒæ—¶æœ€å°åŒ–äº†å¶ç„¶å…³è”ã€‚FinNLIåŒ…å«æœ‰ä¸“å®¶æ ‡æ³¨çš„é«˜è´¨é‡æµ‹è¯•é›†å®ä¾‹æ•°è¾¾3304ä¸ªçš„å…±è®¡æœ‰é«˜è¾¾æ•°ä¸‡æ•°æ®æ ·æœ¬ã€‚è¯„ä¼°è¡¨æ˜ï¼Œè·¨åŸŸæƒ…å†µä¸‹æ™®é€šNLIçš„æ€§èƒ½ä¼šå—åˆ°æ˜¾è‘—å½±å“ã€‚åŸºäºå¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„å¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°ç¨ä½³ï¼Œæœ€é«˜Macro F1åˆ†æ•°è¾¾åˆ°ç™¾åˆ†ä¹‹ä¸ƒåå…«ç‚¹å…­äºŒã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼ŒæŒ‰ç…§æŒ‡ä»¤è°ƒæ•´çš„é‡‘èLLMè¡¨ç°ä¸ä½³ï¼Œè¡¨æ˜å…¶æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚FinNLIæ­ç¤ºäº†å½“å‰LLMåœ¨é‡‘èæ¨ç†æ–¹é¢çš„å¼±ç‚¹ï¼Œè¡¨æ˜ä»æœ‰æå‡ç©ºé—´ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>FinNLIæ˜¯ä¸€ä¸ªé’ˆå¯¹é‡‘èæ–‡æœ¬çš„è‡ªç„¶è¯­è¨€æ¨ç†åŸºå‡†æ•°æ®é›†ã€‚æ¶µç›–ä¸åŒç±»å‹çš„é‡‘èæ–‡æœ¬ï¼Œå¦‚SECæ–‡ä»¶ã€å¹´æŠ¥å’Œæ”¶ç›ŠæŠ¥å‘Šç­‰ã€‚æ•°æ®é›†çš„æ¡†æ¶è®¾è®¡æ—¨åœ¨ç¡®ä¿å‡è®¾å¤šæ ·æ€§åŒæ—¶å‡å°‘å¶ç„¶æ€§ã€‚è¿™ä¸ªæ•°æ®é›†å¯¹äºè§£å†³ç‰¹å®šäºé‡‘èé¢†åŸŸçš„æ¨ç†ä»»åŠ¡å…·æœ‰é‡è¦çš„ä½œç”¨å’Œä»·å€¼ã€‚å®ƒèƒ½å¤Ÿç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨ç†è§£å¤æ‚é‡‘èè¯­å¢ƒä¸‹çš„ä¿¡æ¯å¹¶æ®æ­¤è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ã€‚æ•°æ®é›†çš„è´¨é‡å¾ˆé«˜ï¼Œç»è¿‡ä¸“å®¶æ ‡æ³¨çš„æ•°æ®æ ·æœ¬åŒ…æ‹¬åœ¨å¤šè¾¾å‡ åƒåˆ°å‡ ä¸‡çš„å®ä¾‹æ•°é‡å†…éªŒè¯çš„å¤§è§„æ¨¡æ¡†æ¶æœ‰æ•ˆæé«˜äº†è®­ç»ƒå’ŒéªŒè¯ç»“æœå‡†ç¡®æ€§æŒ‡æ ‡çš„ä¸€è‡´æ€§å’Œç¨³å®šå€¼è°±çš„å¤§å°å¾ˆå¤§ç¨‹åº¦ä¸Šå¢åŠ æ›´é«˜çµæ´»åº¦çš„ç³»ç»Ÿçš„æ¶æ„å¢å¼ºäº†åˆ©ç”¨ç‰¹å®šçš„ç›‘ç£ç­–ç•¥å’Œæ›´å¤§çš„å¯ç”¨ç©ºé—´åˆ©ç”¨ç‡æœ€åæ‰æ˜¯è¿›ä¸€æ­¥çš„å®è´¨æ€§å¢åŠ ä¸Šçš„å·¥ä½œæ•ˆç‡â€ã€‚æ ¸å¿ƒæ˜¯é’ˆå¯¹å¤§æ•°æ®çš„ç‰¹å¾å»ºç«‹é«˜çº§ç»“æ„å¹¶åˆ©ç”¨åŸºäºæœ€æ–°å¼€å‘ç®—æ³•çš„å®æ—¶æ“ä½œå¢å¼ºå·¥å…·ï¼›åŒæ—¶ä¹Ÿé€‚ç”¨äºæ•°æ®æœ¬èº«å˜åŒ–æå¤§çš„åº”ç”¨åœºæ™¯åŒæ—¶å»ºç«‹ç‰¹å®šæ•°æ®é›†é¢†åŸŸè®­ç»ƒæµ‹è¯•ç­‰åˆ†ææ–¹æ³•çš„å…¨é¢ç»¼åˆå¹³å°æ„å»ºé€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå°†èƒ½æå¤§åœ°æ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è¿›å±•å¹¶åŠ é€Ÿç›¸å…³é¢†åŸŸçš„ç ”ç©¶æ­¥ä¼ä»¥åŠæ™ºèƒ½æŠ€æœ¯åº”ç”¨å¸‚åœºæŠ•å…¥çš„æ—¶é—´å’Œæµç¨‹é•¿åº¦æ¨¡å‹æœ‰åŠ©äºæä¾›é€šç”¨çš„å·¥å…·å’Œé€šç”¨æœåŠ¡æ™ºèƒ½å•†ä¸šå’Œå·¥ä¸šæ–¹é¢å¤æ‚è®¡ç®—å’Œç»¼åˆç§‘æŠ€åœºæ™¯çš„ç”Ÿæˆä½œç”¨ä¸å®¹å¿½è§†ç›¸æ¯”æ•°æ®è‡ªåŠ¨å¡«å……çš„å¼€å‘ç»´æŠ¤æ›´å…·æ ‡å‡†åŒ–å±æ€§è¿™æ ·çš„æˆåŠŸå¾ˆå¤§ç¨‹åº¦è¦å½’åŠŸäºè¢«å¼€å‘è€…ä¸æ–­æé«˜è‡ªèº«ç´ è´¨å’Œå­¦ä¹ æ–°æŠ€æœ¯æ–°æŠ€èƒ½ä»¥åŠè‰¯å¥½çš„è¡Œä¸šè§„èŒƒä»¥åŠå¼€å‘å›¢é˜Ÿçš„ååŒåˆä½œå’Œå‹¤å¥‹åˆ›æ–°æ›´å¤šä¾æ®å¼ºè°ƒåŸºæœ¬å®šä¹‰å’Œèƒ½åŠ›æ¶µç›–ç°ä»£é›†æˆæ•°æ®åº“ç‰¹ç‚¹è¢«å­¦ç•Œå¹¿æ³›å…³æ³¨å°¤å…¶åœ¨æ›´è´´è¿‘ç”Ÿäº§å®è·µå’Œé‡å¤§ä»·å€¼è¿ç”¨ç ”ç©¶ç­‰å¤šä¸ªé¢†åŸŸçš„æ•°æ®ä½¿ç”¨æ–¹é¢ä¹Ÿè·å¾—äº†ä¸€å®šè®¤å¯åœ°ä½å¾—åˆ°äº†æŒç»­è€Œç¨³å¥çš„æå‡è¿›å±•ä½œä¸ºå®ç°å…¶å¯èƒ½æ€§å’Œç»æµæ”¶ç›Šä¹‹é—´çš„å‡è¡¡è¶‹åŠ¿ä¸€ä¸ªå…¬å¼€å¯åˆ©ç”¨çš„è®­ç»ƒåº“æ—¥ç›Šé‡è¦çš„è®¤çŸ¥æ™®æå‡äº†ä¸€èˆ¬çº§åˆ«ç¨‹åºçš„ç®€åŒ–æˆä¸ºæ²Ÿé€šæ›´å¤æ‚æ¨¡å¼æ€æƒ³å½¢æˆçš„ç»ä½³ææ–™æœ¬æ–‡ç®€æ´è¡¨è¾¾è¦æ±‚èƒ½åŠ›çš„æœ€å¼ºè¯­è¨€è¡¨è¾¾æœ€é«˜ä»·å€¼çš„è¿›æ­¥è®¤è¯†æ›´å¤šæ™ºèƒ½æ„ŸçŸ¥å†…å®¹æé«˜æ•ˆç‡å’Œèµ„æºä½¿ç”¨ç‡å› æ­¤å…·æœ‰è‰¯å¥½çš„ç¤¾ä¼šæ„ä¹‰åŠæœªæ¥ä»·å€¼æ¢ç´¢å¹¿é˜”å‘å±•å‰æ™¯æ–¹å‘é‡å¤§å¼•é¢†å‹å…³é”®æŠ€æœ¯çš„åº”ç”¨ç©ºé—´æ„ˆå‘æ˜¾è‘—è¶‹åŠ¿æ–¹é¢è¿˜æœ‰å¾…è¿›ä¸€æ­¥ç ”ç©¶é€šè¿‡é‡‘èè¡Œä¸šç†è§£å»ºç«‹è‡ªé€‚åº”è°ƒèŠ‚æ™ºèƒ½åŒ–èµ„æºæœ€ä¼˜åˆ†é…ç»“æ„æ‰èƒ½å‘æŒ¥æœ€å¤§æ•ˆèƒ½åŠ©åŠ›æ¨åŠ¨ç¤¾ä¼šè¿›æ­¥å’Œå‘å±•è¶‹åŠ¿ç ”ç©¶è¡Œä¸šå†…çš„åˆ›æ–°çªç ´ç­‰éœ€æ±‚å°†ä¿ƒè¿›è¯¥é¢†åŸŸä¸æ–­å‘å‰å‘å±•å°†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå’Œé‡‘èé¢†åŸŸæ›´åŠ ç´§å¯†åœ°ç»“åˆèµ·æ¥ä¸ºè§£å†³çœŸå®ä¸–ç•Œé—®é¢˜æä¾›æ–°çš„æ€è·¯å’Œæ–¹æ³•</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16188">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e5b8c4d885ef257564def92af405024a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9ea66d2eb68710e83aaa72ea02d4832.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e22380338ccf91f1453352ea55bba2e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b65c3b44403b7f39d8979334e8493e8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a3cf28fa41aa382c98d594f51fcf768.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="A-LoRA-Based-Approach-to-Fine-Tuning-LLMs-for-Educational-Guidance-in-Resource-Constrained-Settings"><a href="#A-LoRA-Based-Approach-to-Fine-Tuning-LLMs-for-Educational-Guidance-in-Resource-Constrained-Settings" class="headerlink" title="A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in   Resource-Constrained Settings"></a>A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in   Resource-Constrained Settings</h2><p><strong>Authors:Md Millat Hosen</strong></p>
<p>The current study describes a cost-effective method for adapting large language models (LLMs) for academic advising with study-abroad contexts in mind and for application in low-resource methods for acculturation. With the Mistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and a 4-bit quantization method, the model underwent training in two distinct stages related to this studyâ€™s purpose to enhance domain specificity while maintaining computational efficiency. In Phase 1, the model was conditioned with a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained with manually curated datasets from the StudyAbroadGPT project to achieve enhanced, contextualized responses. Technical innovations entailed memory-efficient quantization, parameter-efficient adaptation, and continuous training analytics via Weights &amp; Biases. After training, this study demonstrated a reduction in training loss by 52.7%, 92% accuracy in domain-specific recommendations, achieved 95% markdown-based formatting support, and a median run-rate of 100 samples per second on off-the-shelf GPU equipment. These findings support the effective application of instruction-tuned LLMs within educational advisers, especially in low-resource institutional scenarios. Limitations included decreased generalizability and the application of a synthetically generated dataset, but this framework is scalable for adding new multilingual-augmented and real-time academic advising processes. Future directions may include plans for the integration of retrieval-augmented generation, applying dynamic quantization routines, and connecting to real-time academic databases to increase adaptability and accuracy. </p>
<blockquote>
<p>å½“å‰ç ”ç©¶æè¿°äº†ä¸€ç§å…·æœ‰æˆæœ¬æ•ˆç›Šçš„æ–¹æ³•ï¼Œç”¨äºé’ˆå¯¹å­¦æœ¯å’¨è¯¢åœºæ™¯è°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¹¶è€ƒè™‘å‡ºå›½ç•™å­¦èƒŒæ™¯ï¼Œå°†å…¶åº”ç”¨äºä½èµ„æºæ–¹æ³•çš„é€‚åº”æ–‡åŒ–è¿‡ç¨‹ã€‚é€šè¿‡åº”ç”¨Mistral-7B-Instructæ¨¡å‹å’ŒLow-Rank Adaptationï¼ˆLoRAï¼‰æ–¹æ³•ä¸4ä½é‡åŒ–æ–¹æ³•ï¼Œè¯¥æ¨¡å‹åœ¨ä¸¤ä¸ªä¸æœ¬ç ”ç©¶ç›®çš„ç›¸å…³çš„ç‹¬ç‰¹é˜¶æ®µä¸­è¿›è¡Œäº†è®­ç»ƒï¼Œä»¥æé«˜é¢†åŸŸç‰¹å¼‚æ€§åŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œè¯¥æ¨¡å‹é€šè¿‡Gemini Pro APIä½¿ç”¨åˆæˆæ•°æ®é›†è¿›è¡Œæ¡ä»¶è®­ç»ƒï¼›åœ¨ç¬¬äºŒé˜¶æ®µï¼Œä½¿ç”¨StudyAbroadGPTé¡¹ç›®çš„æ‰‹åŠ¨æ•´ç†æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œä»¥å®ç°å¢å¼ºã€æƒ…å¢ƒåŒ–çš„å“åº”ã€‚æŠ€æœ¯åˆ›æ–°åŒ…æ‹¬å†…å­˜é«˜æ•ˆçš„é‡åŒ–ã€å‚æ•°é«˜æ•ˆçš„é€‚åº”æ€§å’Œé€šè¿‡Weights &amp; Biasesçš„æŒç»­è®­ç»ƒåˆ†æã€‚è®­ç»ƒåï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†è®­ç»ƒæŸå¤±å‡å°‘52.7%ï¼Œé¢†åŸŸç‰¹å®šå»ºè®®çš„å‡†ç¡®æ€§è¾¾åˆ°92%ï¼Œæ”¯æŒåŸºäºæ ‡è®°çš„æ ¼å¼åŒ–è¾¾åˆ°95%ï¼Œå¹¶åœ¨ç°æˆçš„GPUè®¾å¤‡ä¸Šå®ç°æ¯ç§’å¤„ç†100ä¸ªæ ·æœ¬çš„ä¸­å€¼è¿è¡Œé€Ÿç‡ã€‚è¿™äº›å‘ç°æ”¯æŒåœ¨æ•™è‚²é¡¾é—®ä¸­æœ‰æ•ˆåº”ç”¨æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºçš„æœºæ„åœºæ™¯ä¸­ã€‚å±€é™æ€§åŒ…æ‹¬é€šç”¨æ€§é™ä½å’Œåº”ç”¨åˆæˆç”Ÿæˆçš„æ•°æ®é›†ï¼Œä½†è¿™ä¸ªæ¡†æ¶å¯æ‰©å±•ï¼Œå¯æ·»åŠ æ–°çš„å¤šè¯­è¨€å¢å¼ºå’Œå®æ—¶å­¦æœ¯å’¨è¯¢æµç¨‹ã€‚æœªæ¥æ–¹å‘å¯èƒ½åŒ…æ‹¬é›†æˆæ£€ç´¢å¢å¼ºç”Ÿæˆã€åº”ç”¨åŠ¨æ€é‡åŒ–ä¾‹è¡Œç¨‹åºä»¥åŠä¸å®æ—¶å­¦æœ¯æ•°æ®åº“è¿æ¥ï¼Œä»¥æé«˜é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15610v2">PDF</a> 18 pages, 6 figures (3 graphs + 3 flowchart&#x2F;architecture diagrams),   submitted as a preprint for review consideration in AI for Education or   Machine Learning applications in low-resource settings. Includes detailed   experiments with LoRA and quantization methods for efficient LLM fine-tuning</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æè¿°äº†ä¸€ç§å…·æœ‰æˆæœ¬æ•ˆç›Šçš„æ–¹æ³•ï¼Œç”¨äºé€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»¥ç”¨äºå­¦æœ¯å’¨è¯¢å¹¶è€ƒè™‘ç•™å­¦èƒŒæ™¯ï¼Œä»¥åŠç”¨äºä½èµ„æºæ–¹æ³•çš„é€‚åº”ã€‚é€šè¿‡åº”ç”¨Mistral-7B-Instructæ¨¡å‹å’ŒLoRAæ–¹æ³•ä»¥åŠ4ä½é‡åŒ–æ–¹æ³•ï¼Œè¯¥æ¨¡å‹ç»è¿‡ä¸¤ä¸ªé˜¶æ®µçš„ä¸“ä¸šè®­ç»ƒï¼Œæ—¨åœ¨æé«˜é¢†åŸŸç‰¹å¼‚æ€§åŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚ç¬¬ä¸€é˜¶æ®µä½¿ç”¨åˆæˆæ•°æ®é›†é€šè¿‡Gemini Pro APIè¿›è¡Œæ¡ä»¶è®­ç»ƒï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨StudyAbroadGPTé¡¹ç›®çš„æ‰‹åŠ¨æ•´ç†æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œä»¥å®ç°å¢å¼ºå’Œä¸Šä¸‹æ–‡åŒ–çš„å“åº”ã€‚ç»è¿‡è®­ç»ƒåï¼Œè¯¥ç ”ç©¶è¯æ˜äº†è®­ç»ƒæŸå¤±é™ä½äº†52.7%ï¼Œé¢†åŸŸç‰¹å®šå»ºè®®çš„å‡†ç¡®æ€§è¾¾åˆ°92%ï¼Œå¹¶æ”¯æŒäº†95%çš„markdownæ ¼å¼è®¾ç½®ã€‚è¿™é¡¹ç ”ç©¶æœ‰æ•ˆè¯æ˜äº†æŒ‡ä»¤å¾®è°ƒLLMåœ¨æ•™è‚²é¡¾é—®ä¸­çš„é€‚ç”¨æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºæœºæ„åœºæ™¯ä¸­ã€‚å°½ç®¡å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œå¦‚é€šç”¨æ€§çš„é™ä½å’Œåº”ç”¨åˆæˆæ•°æ®é›†çš„é—®é¢˜ï¼Œä½†è¯¥æ¡†æ¶å¯æ‰©å±•ï¼Œå¯æ·»åŠ æ–°çš„å¤šè¯­è¨€å¢å¼ºå’Œå®æ—¶å­¦æœ¯å’¨è¯¢æµç¨‹ã€‚æœªæ¥çš„æ–¹å‘å¯èƒ½åŒ…æ‹¬é›†æˆæ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ã€åº”ç”¨åŠ¨æ€é‡åŒ–ä¾‹è¡Œç¨‹åºä»¥åŠä¸å®æ—¶å­¦æœ¯æ•°æ®åº“çš„è¿æ¥ï¼Œä»¥æé«˜é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æè¿°äº†ä¸€ç§æˆæœ¬æ•ˆç›Šé«˜çš„æ–¹æ³•ï¼Œç”¨äºé€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç‰¹åˆ«é€‚ç”¨äºå­¦æœ¯å’¨è¯¢å’Œç•™å­¦èƒŒæ™¯ã€‚</li>
<li>é€šè¿‡ä¸¤ä¸ªé˜¶æ®µçš„è®­ç»ƒæé«˜äº†æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚</li>
<li>åº”ç”¨äº†åˆæˆæ•°æ®é›†å’Œæ‰‹åŠ¨æ•´ç†çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å“åº”ã€‚</li>
<li>ç ”ç©¶è¯æ˜äº†è®­ç»ƒæŸå¤±æ˜¾è‘—å‡å°‘ï¼Œé¢†åŸŸç‰¹å®šå»ºè®®çš„å‡†ç¡®æ€§é«˜ã€‚</li>
<li>æ”¯æŒmarkdownæ ¼å¼è®¾ç½®ï¼Œè¡¨æ˜æ¨¡å‹åœ¨æ ¼å¼åŒ–è¾“å‡ºæ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥ç ”ç©¶éªŒè¯äº†æŒ‡ä»¤å¾®è°ƒLLMåœ¨æ•™è‚²å’¨è¯¢é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºç¯å¢ƒä¸‹ã€‚</li>
<li>è™½ç„¶å­˜åœ¨å±€é™æ€§ï¼Œä½†è¯¥æ¡†æ¶å…·æœ‰å¯æ‰©å±•æ€§ï¼Œæœªæ¥å¯è¿›ä¸€æ­¥èå…¥å¤šè¯­è¨€æ”¯æŒå’Œå®æ—¶å­¦æœ¯å’¨è¯¢æµç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15610">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d97da549f2dbd1533d8b1005e699b29c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3eea65f96692f375915e9720a478d3d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5d384078e0fce050f919f0f190d6a12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a70efbbfc9ad8561a9e3092647fe8b17.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="TALES-Text-Adventure-Learning-Environment-Suite"><a href="#TALES-Text-Adventure-Learning-Environment-Suite" class="headerlink" title="TALES: Text Adventure Learning Environment Suite"></a>TALES: Text Adventure Learning Environment Suite</h2><p><strong>Authors:Christopher Zhang Cui, Xingdi Yuan, Ziang Xiao, Prithviraj Ammanabrolu, Marc-Alexandre CÃ´tÃ©</strong></p>
<p>Reasoning is an essential skill to enable Large Language Models (LLMs) to interact with the world. As tasks become more complex, they demand increasingly sophisticated and diverse reasoning capabilities for sequential decision-making, requiring structured reasoning over the context history to determine the next best action. We introduce TALES, a diverse collection of synthetic and human-written text-adventure games designed to challenge and evaluate diverse reasoning capabilities. We present results over a range of LLMs, open- and closed-weights, performing a qualitative analysis on the top performing models. Despite an impressive showing on synthetic games, even the top LLM-driven agents fail to achieve 15% on games designed for human enjoyment. Code and visualization of the experiments can be found at <a target="_blank" rel="noopener" href="https://microsoft.github.io/tales">https://microsoft.github.io/tales</a>. </p>
<blockquote>
<p>æ¨ç†æ˜¯ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ä¸–ç•Œäº’åŠ¨çš„é‡è¦æŠ€èƒ½ã€‚éšç€ä»»åŠ¡å˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼Œå®ƒä»¬éœ€è¦æ›´é«˜çº§å’Œå¤šæ ·åŒ–çš„æ¨ç†èƒ½åŠ›æ¥è¿›è¡Œåºåˆ—å†³ç­–ï¼Œéœ€è¦åœ¨èƒŒæ™¯å†å²ä¸­è¿›è¡Œç»“æ„åŒ–æ¨ç†ä»¥ç¡®å®šä¸‹ä¸€ä¸ªæœ€ä½³è¡ŒåŠ¨ã€‚æˆ‘ä»¬ä»‹ç»äº†TALESï¼Œè¿™æ˜¯ä¸€ç³»åˆ—åˆæˆå’Œäººç±»ç¼–å†™çš„æ–‡å­—å†’é™©æ¸¸æˆçš„é›†åˆï¼Œæ—¨åœ¨æŒ‘æˆ˜å’Œè¯„ä¼°å¤šæ ·åŒ–çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ä¸€ç³»åˆ—LLMä¸Šå±•ç¤ºäº†ç»“æœï¼ŒåŒ…æ‹¬å¼€æºå’Œå°é—­æƒé‡ï¼Œå¹¶å¯¹è¡¨ç°æœ€å¥½çš„æ¨¡å‹è¿›è¡Œäº†å®šæ€§åˆ†æã€‚å³ä½¿åœ¨åˆæˆæ¸¸æˆä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†æœ€å¥½çš„LLMé©±åŠ¨çš„æ™ºèƒ½ä½“åœ¨äººç±»è®¾è®¡çš„æ¸¸æˆä¸Šä¹Ÿä»…è¾¾åˆ°äº†15%çš„é€šè¿‡ç‡ã€‚å®éªŒçš„ä»£ç å’Œå¯è§†åŒ–éƒ¨åˆ†å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://microsoft.github.io/tales%E6%89%BE%E5%88%B0%E3%80%82">https://microsoft.github.io/talesæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14128v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­éœ€è¦æ¨ç†èƒ½åŠ›æ¥ä¸ä¸–ç•Œäº’åŠ¨ã€‚ä½œè€…å¼•å…¥äº†TALESæ–‡æœ¬å†’é™©æ¸¸æˆé›†ï¼Œæ—¨åœ¨æŒ‘æˆ˜å’Œè¯„ä¼°LLMçš„å¤šæ ·åŒ–æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿åœ¨åˆæˆæ¸¸æˆä¸­è¡¨ç°ä»¤äººå°è±¡æ·±åˆ»ï¼Œé¡¶çº§LLMé©±åŠ¨çš„ä»£ç†åœ¨äººç±»è®¾è®¡çš„æ¸¸æˆä¸­çš„è¡¨ç°ä¹Ÿä»…è¾¾åˆ°15%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éœ€è¦æ¨ç†èƒ½åŠ›ä»¥åº”å¯¹å¤æ‚ä»»åŠ¡ã€‚</li>
<li>TALESæ–‡æœ¬å†’é™©æ¸¸æˆé›†æ—¨åœ¨è¯„ä¼°å’ŒæŒ‘æˆ˜LLMçš„å¤šæ ·åŒ–æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åˆæˆæ¸¸æˆä¸­LLMè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨äººç±»è®¾è®¡çš„æ¸¸æˆä¸­è¡¨ç°è¾ƒå·®ã€‚</li>
<li>é¡¶çº§LLMä»£ç†åœ¨äººç±»è®¾è®¡çš„æ¸¸æˆä¸­çš„è¡¨ç°ä»…è¾¾åˆ°15%ã€‚</li>
<li>LLMåœ¨å†³ç­–è¿‡ç¨‹ä¸­éœ€è¦ç»“æ„åŒ–æ¨ç†æ¥åº”å¯¹ä¸Šä¸‹æ–‡ä¸­çš„ä¸åŒæƒ…å†µã€‚</li>
<li>ä»£ç å’Œå®éªŒå¯è§†åŒ–å¯åœ¨<a target="_blank" rel="noopener" href="https://microsoft.github.io/tales%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://microsoft.github.io/talesä¸Šæ‰¾åˆ°ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14128">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-818993ca92180e47bd69ba87b8d7b2eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b59a501786aaa23285a0fabd128ceea2.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Decoding-Vision-Transformers-the-Diffusion-Steering-Lens"><a href="#Decoding-Vision-Transformers-the-Diffusion-Steering-Lens" class="headerlink" title="Decoding Vision Transformers: the Diffusion Steering Lens"></a>Decoding Vision Transformers: the Diffusion Steering Lens</h2><p><strong>Authors:Ryota Takatsuki, Sonia Joseph, Ippei Fujisawa, Ryota Kanai</strong></p>
<p>Logit Lens is a widely adopted method for mechanistic interpretability of transformer-based language models, enabling the analysis of how internal representations evolve across layers by projecting them into the output vocabulary space. Although applying Logit Lens to Vision Transformers (ViTs) is technically straightforward, its direct use faces limitations in capturing the richness of visual representations. Building on the work of Toker et al. (2024)~\cite{Toker2024-ve}, who introduced Diffusion Lens to visualize intermediate representations in the text encoders of text-to-image diffusion models, we demonstrate that while Diffusion Lens can effectively visualize residual stream representations in image encoders, it fails to capture the direct contributions of individual submodules. To overcome this limitation, we propose \textbf{Diffusion Steering Lens} (DSL), a novel, training-free approach that steers submodule outputs and patches subsequent indirect contributions. We validate our method through interventional studies, showing that DSL provides an intuitive and reliable interpretation of the internal processing in ViTs. </p>
<blockquote>
<p>Logit Lensæ˜¯å¹¿æ³›åº”ç”¨äºåŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹çš„æœºæ¢°è§£é‡Šæ€§çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡å°†è¿™äº›å†…éƒ¨è¡¨ç¤ºæŠ•å½±åˆ°è¾“å‡ºè¯æ±‡ç©ºé—´æ¥åˆ†æè¿™äº›è¡¨ç¤ºå¦‚ä½•åœ¨å„å±‚ä¸­æ¼”å˜ã€‚è™½ç„¶å°†Logit Lensåº”ç”¨äºè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰åœ¨æŠ€æœ¯ä¸Šå¾ˆç›´æ¥ï¼Œä½†å…¶ç›´æ¥ä½¿ç”¨åœ¨æ•æ‰è§†è§‰è¡¨ç¤ºçš„ä¸°å¯Œæ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æˆ‘ä»¬åŸºäºæ‰˜å…‹å°”ç­‰äººï¼ˆToker et al.ï¼‰ï¼ˆå¼•ç”¨æ–‡çŒ®å°†åœ¨æ­£æ–‡ä¸­å‘ˆç°ï¼‰çš„å·¥ä½œï¼Œä»–ä»¬å¼•å…¥äº†æ‰©æ•£é€é•œï¼ˆDiffusion Lensï¼‰æ¥å¯è§†åŒ–æ–‡æœ¬ç¼–ç å™¨çš„ä¸­é—´è¡¨ç¤ºå½¢å¼ä¸­çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬è¯æ˜äº†è™½ç„¶æ‰©æ•£é€é•œå¯ä»¥æœ‰æ•ˆåœ°å¯è§†åŒ–å›¾åƒç¼–ç å™¨çš„æ®‹å·®æµè¡¨ç¤ºå½¢å¼ï¼Œä½†å®ƒæ— æ³•æ•è·å•ä¸ªå­æ¨¡å—çš„ç›´æ¥å½±å“ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†è®­ç»ƒå‰çš„ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºæ‰©æ•£å¯¼å‘é€é•œï¼ˆDiffusion Steering Lensï¼ŒDSLï¼‰ã€‚è¿™ç§æ–¹æ³•é€šè¿‡è°ƒæ•´å­æ¨¡å—è¾“å‡ºå¹¶ä¿®è¡¥åç»­çš„é—´æ¥è´¡çŒ®æ¥æä¾›å¯¹ViTså†…éƒ¨å¤„ç†çš„ç›´è§‚å’Œå¯é è§£é‡Šã€‚æˆ‘ä»¬é€šè¿‡å¹²é¢„ç ”ç©¶éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13763v2">PDF</a> 12 pages, 17 figures. Accepted to the CVPR 2025 Workshop on   Mechanistic Interpretability for Vision (MIV)</p>
<p><strong>Summary</strong></p>
<p>Logit Lensæ–¹æ³•å¹¿æ³›åº”ç”¨äºè§£é‡ŠåŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹ï¼Œä½†ç›´æ¥åº”ç”¨äºè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰æ—¶å­˜åœ¨å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„æ— è®­ç»ƒæ–¹æ³•â€”â€”Diffusion Steering Lensï¼ˆDSLï¼‰ï¼Œæ—¨åœ¨å¯è§†åŒ–è§†è§‰è½¬æ¢å™¨çš„å†…éƒ¨å¤„ç†è¿‡ç¨‹ï¼Œé€šè¿‡å¹²é¢„æ€§ç ”ç©¶éªŒè¯äº†å…¶ç›´è§‚æ€§å’Œå¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Logit Lenså¹¿æ³›ç”¨äºè§£é‡ŠåŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å°†å†…éƒ¨è¡¨ç¤ºæŠ•å½±åˆ°è¾“å‡ºè¯æ±‡ç©ºé—´æ¥åˆ†æå…¶æ¼”å˜ã€‚</li>
<li>è™½ç„¶Logit Lensåœ¨åº”ç”¨äºè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰æ—¶æŠ€æœ¯ä¸Šå¾ˆç›´è§‚ï¼Œä½†å…¶ç›´æ¥ä½¿ç”¨å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•å……åˆ†æ•æ‰è§†è§‰è¡¨ç¤ºçš„ä¸°å¯Œæ€§ã€‚</li>
<li>Diffusion Lensè™½ç„¶å¯ä»¥æœ‰æ•ˆåœ°å¯è§†åŒ–æ–‡æœ¬ç¼–ç å™¨ä¸­æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ä¸­é—´è¡¨ç¤ºï¼Œä½†åœ¨æ•æ‰å›¾åƒç¼–ç å™¨çš„æ®‹å·®æµè¡¨ç¤ºæ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13763">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c614db78e777eab4cafcf8551d97cd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82479636c07454ea4c1330f24f22b4f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1568148bc789594619a449bd8dc24b24.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="OmniScience-A-Domain-Specialized-LLM-for-Scientific-Reasoning-and-Discovery"><a href="#OmniScience-A-Domain-Specialized-LLM-for-Scientific-Reasoning-and-Discovery" class="headerlink" title="OmniScience: A Domain-Specialized LLM for Scientific Reasoning and   Discovery"></a>OmniScience: A Domain-Specialized LLM for Scientific Reasoning and   Discovery</h2><p><strong>Authors:Vignesh Prabhakar, Md Amirul Islam, Adam Atanas, Yao-Ting Wang, Joah Han, Aastha Jhunjhunwala, Rucha Apte, Robert Clark, Kang Xu, Zihan Wang, Kai Liu</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable potential in advancing scientific knowledge and addressing complex challenges. In this work, we introduce OmniScience, a specialized large reasoning model for general science, developed through three key components: (1) domain adaptive pretraining on a carefully curated corpus of scientific literature, (2) instruction tuning on a specialized dataset to guide the model in following domain-specific tasks, and (3) reasoning-based knowledge distillation through fine-tuning to significantly enhance its ability to generate contextually relevant and logically sound responses. We demonstrate the versatility of OmniScience by developing a battery agent that efficiently ranks molecules as potential electrolyte solvents or additives. Comprehensive evaluations reveal that OmniScience is competitive with state-of-the-art large reasoning models on the GPQA Diamond and domain-specific battery benchmarks, while outperforming all public reasoning and non-reasoning models with similar parameter counts. We further demonstrate via ablation experiments that domain adaptive pretraining and reasoning-based knowledge distillation are critical to attain our performance levels, across benchmarks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨è¿›ç§‘å­¦çŸ¥è¯†å’Œåº”å¯¹å¤æ‚æŒ‘æˆ˜æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—æ½œåŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†OmniScienceï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é€šç”¨ç§‘å­¦çš„ä¸“é—¨å¤§å‹æ¨ç†æ¨¡å‹ï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶å¼€å‘è€Œæˆï¼šï¼ˆ1ï¼‰åœ¨ç²¾å¿ƒç­›é€‰çš„ç§‘å­¦æ–‡çŒ®è¯­æ–™åº“ä¸Šè¿›è¡Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼›ï¼ˆ2ï¼‰åœ¨ä¸“é—¨çš„æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹æ‰§è¡Œç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ï¼›ï¼ˆ3ï¼‰é€šè¿‡å¾®è°ƒè¿›è¡ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦ï¼Œä»¥æ˜¾è‘—æé«˜å…¶ç”Ÿæˆè¯­å¢ƒç›¸å…³å’Œé€»è¾‘ä¸¥è°¨å“åº”çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡å¼€å‘ä¸€ç§ç”µæ± ä»£ç†æ¥å±•ç¤ºOmniScienceçš„é€šç”¨æ€§ï¼Œè¯¥ä»£ç†èƒ½å¤Ÿé«˜æ•ˆåœ°å¯¹åˆ†å­è¿›è¡Œæ’åï¼Œä½œä¸ºæ½œåœ¨çš„ç”µè§£è´¨æº¶å‰‚æˆ–æ·»åŠ å‰‚ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒOmniScienceåœ¨GPQA Diamondå’Œç‰¹å®šé¢†åŸŸçš„ç”µæ± åŸºå‡†æµ‹è¯•ä¸Šï¼Œä¸æœ€æ–°çš„å¤§å‹æ¨ç†æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶åœ¨å‚æ•°æ•°é‡ç›¸ä¼¼çš„æ‰€æœ‰å…¬å…±æ¨ç†å’Œéæ¨ç†æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ã€‚æˆ‘ä»¬è¿˜é€šè¿‡æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯æ˜ï¼Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒå’ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦å¯¹äºè¾¾åˆ°æˆ‘ä»¬çš„æ€§èƒ½æ°´å¹³è‡³å…³é‡è¦ï¼Œè·¨è¶Šå„ç§åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17604v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>OmniScienceæ˜¯ä¸€ä¸ªé’ˆå¯¹é€šç”¨ç§‘å­¦çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼Œé€šè¿‡é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒã€æŒ‡ä»¤è°ƒä¼˜å’Œæ¨ç†çŸ¥è¯†è’¸é¦ç­‰æŠ€æœ¯æ˜¾è‘—å¢å¼ºäº†å…¶åœ¨ç§‘å­¦é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚åœ¨ç”µæ± ä»£ç†å¼€å‘ä¸­çš„åº”ç”¨å±•ç¤ºäº†å…¶å¤šåŠŸèƒ½æ€§ï¼Œå¹¶åœ¨GPQA Diamondå’Œç‰¹å®šç”µæ± åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OmniScienceæ˜¯ä¸€ä¸ªé’ˆå¯¹é€šç”¨ç§‘å­¦çš„å¤§å‹æ¨ç†æ¨¡å‹ã€‚</li>
<li>å®ƒé€šè¿‡é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒã€æŒ‡ä»¤è°ƒä¼˜å’Œæ¨ç†çŸ¥è¯†è’¸é¦ç­‰æŠ€æœ¯è¿›è¡Œå¼€å‘ã€‚</li>
<li>OmniScienceåœ¨ç”µæ± ä»£ç†å¼€å‘ä¸­çš„åº”ç”¨å±•ç¤ºäº†å…¶å¤šåŠŸèƒ½æ€§ã€‚</li>
<li>åœ¨GPQA Diamondå’Œç‰¹å®šç”µæ± åŸºå‡†æµ‹è¯•ä¸­ï¼ŒOmniScienceè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œä¼˜äºæ‰€æœ‰ç±»ä¼¼çš„å…¬å…±æ¨ç†å’Œéæ¨ç†æ¨¡å‹ã€‚</li>
<li>é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒå’Œæ¨ç†çŸ¥è¯†è’¸é¦å¯¹äºè¾¾åˆ°é«˜æ€§èƒ½æ°´å¹³è‡³å…³é‡è¦ã€‚</li>
<li>OmniScienceæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆä¸ä¸Šä¸‹æ–‡ç›¸å…³ä¸”é€»è¾‘ä¸¥è°¨çš„å›ç­”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20e8cc74e03be7685752af6aaee7dfe8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5cddc57a4b1e46185ec103df103a43ee.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-25/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-25/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-25/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-221bd3f73cedfd92fa5f7929a7829834.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-25  OptimAI Optimization from Natural Language Using LLM-Powered AI Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-25/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-76c8df71fc8d5131af3b83050508ac25.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-25  Tracing Thought Using Chain-of-Thought Reasoning to Identify the LLM   Behind AI-Generated Text
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17663.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
