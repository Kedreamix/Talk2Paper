<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-04-25  OptimAI Optimization from Natural Language Using LLM-Powered AI Agents">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-346cd188e5dbc974e3ae403a7765b07e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    79 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-25-更新"><a href="#2025-04-25-更新" class="headerlink" title="2025-04-25 更新"></a>2025-04-25 更新</h1><h2 id="OptimAI-Optimization-from-Natural-Language-Using-LLM-Powered-AI-Agents"><a href="#OptimAI-Optimization-from-Natural-Language-Using-LLM-Powered-AI-Agents" class="headerlink" title="OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents"></a>OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents</h2><p><strong>Authors:Raghav Thind, Youran Sun, Ling Liang, Haizhao Yang</strong></p>
<p>Optimization plays a vital role in scientific research and practical applications, but formulating a concrete optimization problem described in natural language into a mathematical form and selecting a suitable solver to solve the problem requires substantial domain expertise. We introduce \textbf{OptimAI}, a framework for solving \underline{Optim}ization problems described in natural language by leveraging LLM-powered \underline{AI} agents, achieving superior performance over current state-of-the-art methods. Our framework is built upon four key roles: (1) a \emph{formulator} that translates natural language problem descriptions into precise mathematical formulations; (2) a \emph{planner} that constructs a high-level solution strategy prior to execution; and (3) a \emph{coder} and a \emph{code critic} capable of interacting with the environment and reflecting on outcomes to refine future actions. Ablation studies confirm that all roles are essential; removing the planner or code critic results in $5.8\times$ and $3.1\times$ drops in productivity, respectively. Furthermore, we introduce UCB-based debug scheduling to dynamically switch between alternative plans, yielding an additional $3.3\times$ productivity gain. Our design emphasizes multi-agent collaboration, allowing us to conveniently explore the synergistic effect of combining diverse models within a unified system. Our approach attains 88.1% accuracy on the NLP4LP dataset and 71.2% on the Optibench (non-linear w&#x2F;o table) subset, reducing error rates by 58% and 50% respectively over prior best results. </p>
<blockquote>
<p>优化在科学研究和实际应用中发挥着至关重要的作用，然而，将自然语言描述的具体优化问题转化为数学形式，并选择适合的求解器来解决问题需要大量的专业知识。我们引入了<strong>OptimAI</strong>，这是一个利用大型语言模型驱动的<strong>AI</strong>代理来解决自然语言描述的优化问题的框架，其性能超越了当前先进的方法。我们的框架建立在四个关键角色之上：（1）一个能将自然语言问题描述转化为精确数学形式的<strong>表述器</strong>；（2）一个在执行前构建高级解决方案策略的<strong>规划器</strong>；（3）一个能够与环境互动并对结果进行反思以改进未来行动的<strong>编码者</strong>和<strong>代码评论家</strong>。切除研究表明，所有角色都是不可或缺的；移除规划者或代码评论家将导致生产率分别下降5.8倍和3.1倍。此外，我们引入了基于UCB的调试调度来动态切换替代方案，产生了额外的3.3倍生产率增益。我们的设计强调多智能体协作，使我们能够方便地探索在统一系统中结合不同模型的协同作用。我们的方法在NLP4LP数据集上达到88.1%的准确率，在Optibench（非线性无表）子集上达到71.2%的准确率，与之前的最佳结果相比，误差率分别降低了58%和50%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16918v1">PDF</a> </p>
<p><strong>摘要</strong><br>优化在自然语言描述和实际应用中发挥着重要作用，但将自然语言描述的具体优化问题转化为数学形式并选择适当的求解器来解决问题需要大量的专业知识。我们引入了OptimAI框架，利用LLM驱动的AI代理解决自然语言描述的优化问题，实现了优于当前先进方法的性能。该框架建立在四个关键角色上：将自然语言问题描述转化为精确数学形式的表述者、构建高级解决方案策略的规划者、能够与环境交互并反思结果以改进未来行动的编码器和代码评论家。我们的设计强调多智能体协作，便于在统一系统中探索结合不同模型的协同作用。我们的方法在NLP4LP数据集上达到了88.1%的准确率，在Optibench（非线性无表）子集上达到了71.2%的准确率，相较于之前最佳结果分别降低了58%和50%的错误率。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>OptimAI框架利用LLM驱动的AI代理解决自然语言描述的优化问题，实现高性能。</li>
<li>框架包括四个关键角色：表述者、规划者、编码器和代码评论家，各自承担不同的任务。</li>
<li>去除规划者或代码评论家会导致生产力大幅下降，表明这些角色对框架性能至关重要。</li>
<li>通过引入基于UCB的调试调度，可以动态切换不同计划，进一步提高生产力。</li>
<li>该框架强调多智能体协作，便于在统一系统中结合不同模型，发挥协同作用。</li>
<li>在NLP4LP数据集和Optibench子集上的实验结果表明，该框架具有显著的高准确率。</li>
<li>与之前的最佳结果相比，该框架在错误率方面取得了显著的改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16918">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cbb451bffca58548136f60e1505a8856.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d666c5932dc3a62754102c0db044d5dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b4ec94cddc5604313f06ef772ba0c08.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca292c2e0d75e544772921d8d233326d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-623f935129944ff9a28bb5344dfbaa88.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Tracing-Thought-Using-Chain-of-Thought-Reasoning-to-Identify-the-LLM-Behind-AI-Generated-Text"><a href="#Tracing-Thought-Using-Chain-of-Thought-Reasoning-to-Identify-the-LLM-Behind-AI-Generated-Text" class="headerlink" title="Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM   Behind AI-Generated Text"></a>Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM   Behind AI-Generated Text</h2><p><strong>Authors:Shifali Agrahari, Sanasam Ranbir Singh</strong></p>
<p>In recent years, the detection of AI-generated text has become a critical area of research due to concerns about academic integrity, misinformation, and ethical AI deployment. This paper presents COT Fine-tuned, a novel framework for detecting AI-generated text and identifying the specific language model. responsible for generating the text. We propose a dual-task approach, where Task A involves classifying text as AI-generated or human-written, and Task B identifies the specific LLM behind the text. The key innovation of our method lies in the use of Chain-of-Thought reasoning, which enables the model to generate explanations for its predictions, enhancing transparency and interpretability. Our experiments demonstrate that COT Fine-tuned achieves high accuracy in both tasks, with strong performance in LLM identification and human-AI classification. We also show that the CoT reasoning process contributes significantly to the models effectiveness and interpretability. </p>
<blockquote>
<p>近年来，由于人们对学术诚信、误导信息和道德人工智能部署的担忧，检测人工智能生成的文本已成为研究的关键领域。本文提出了COT Fine-tuned，一个用于检测人工智能生成文本并识别生成文本的具体语言模型的新型框架。我们采用双任务方法，任务A涉及将文本分类为人工智能生成或人类撰写，任务B则识别文本背后的特定大型语言模型。我们的方法的关键创新之处在于使用Chain-of-Thought推理，使模型能够为其预测生成解释，提高透明度和可解释性。我们的实验表明，COT Fine-tuned在两项任务中都实现了较高的准确性，在大型语言模型识别和人机分类方面表现出强大的性能。我们还表明，CoT推理过程对模型的效率和可解释性做出了重大贡献。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16913v1">PDF</a> De-Factify 4: 4th Workshop on Multimodal Fact Checking and Hate   Speech Detection, co-located with AAAI 2025. Pennsylvania</p>
<p><strong>Summary</strong><br>AI生成文本检测已成为研究的关键领域，涉及学术诚信、虚假信息和伦理AI部署等问题。本文提出一种新型框架COT Fine-tuned，采用双重任务方法检测AI生成的文本并识别特定语言模型。其创新之处在于采用思维链推理技术，使模型能对其预测生成解释，增强透明度和可解释性。实验证明，COT Fine-tuned在两项任务中都取得了高准确率，特别是在语言模型识别和人机分类方面表现优异。思维链推理过程对模型的效能和可解释性有显著贡献。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI生成文本检测是当前的热门研究领域，主要关注学术诚信、虚假信息和伦理AI部署问题。</li>
<li>COT Fine-tuned是一种新型框架，用于检测AI生成的文本并识别生成文本的特定语言模型。</li>
<li>该框架采用双重任务方法，包括判断文本是否由AI生成以及识别特定语言模型。</li>
<li>COT Fine-tuned的主要创新点是采用思维链推理技术，增强模型的透明度和可解释性。</li>
<li>实验结果显示，COT Fine-tuned在两项任务中均表现出高准确率。</li>
<li>思维链推理过程对提升模型的效能和可解释性有重要贡献。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16913">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-94bc7ccd14bab0a82c086205ec93b2b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50895ae4116888dee53a8e58bb341b57.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b349b9c39daf91fbbfe23a040063478.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb086bcb345302ca24789fcf65bd22bb.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Enhancing-Critical-Thinking-with-AI-A-Tailored-Warning-System-for-RAG-Models"><a href="#Enhancing-Critical-Thinking-with-AI-A-Tailored-Warning-System-for-RAG-Models" class="headerlink" title="Enhancing Critical Thinking with AI: A Tailored Warning System for RAG   Models"></a>Enhancing Critical Thinking with AI: A Tailored Warning System for RAG   Models</h2><p><strong>Authors:Xuyang Zhu, Sejoon Chang, Andrew Kuik</strong></p>
<p>Retrieval-Augmented Generation (RAG) systems offer a powerful approach to enhancing large language model (LLM) outputs by incorporating fact-checked, contextually relevant information. However, fairness and reliability concerns persist, as hallucinations can emerge at both the retrieval and generation stages, affecting users’ reasoning and decision-making. Our research explores how tailored warning messages – whose content depends on the specific context of hallucination – shape user reasoning and actions in an educational quiz setting. Preliminary findings suggest that while warnings improve accuracy and awareness of high-level hallucinations, they may also introduce cognitive friction, leading to confusion and diminished trust in the system. By examining these interactions, this work contributes to the broader goal of AI-augmented reasoning: developing systems that actively support human reflection, critical thinking, and informed decision-making rather than passive information consumption. </p>
<blockquote>
<p>检索增强生成（RAG）系统通过融入经过事实核查的、语境相关的信息，为增强大型语言模型（LLM）的输出提供了一种强大方法。然而，公平性和可靠性问题依然存在，因为在检索和生成阶段都可能出现幻觉，影响用户的推理和决策。我们的研究探索了定制警告信息——其内容取决于幻觉的具体语境——如何塑造用户在教育测验环境中的推理和行动。初步结果表明，虽然警告信息提高了对高级幻觉的准确性和意识，但它们也可能引发认知摩擦，导致用户混淆并对系统失去信任。通过检查这些交互，这项工作有助于实现人工智能辅助推理的更广泛目标：开发积极支持人类反思、批判性思维和知情决策的系统，而不是仅仅被动地消费信息。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16883v1">PDF</a> Presented at the 2025 ACM Workshop on Human-AI Interaction for   Augmented Reasoning</p>
<p><strong>Summary</strong></p>
<p>RAG系统通过融入事实核查和语境相关信息，增强了大型语言模型的输出能力。但存在公平性和可靠性问题，如在检索和生成阶段可能出现幻觉，影响用户推理和决策。研究通过教育测验环境探索定制警告信息对用户行为的影响，初步发现警告虽能提高对高级幻觉的准确性和意识，但也可能造成认知摩擦，导致混淆和对系统信任的降低。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAG系统通过结合事实核查和语境相关信息，增强了LLM的输出。</li>
<li>公平性和可靠性问题是RAG系统面临的挑战，因为存在出现幻觉的风险。</li>
<li>警告信息可以改善用户对高级幻觉的准确性和意识。</li>
<li>警告信息也可能造成认知摩擦，导致用户混淆和对系统信任的降低。</li>
<li>研究在教育测验环境中探索了定制警告信息对用户行为的影响。</li>
<li>此研究对AI增强推理的贡献在于开发积极支持人类反思、批判思维和知情决策的系统，而非仅供被动消费信息的系统。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16883">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a845c4fbac609c69bc1d8d0cc881424d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86b6c16d1db93144195d0e6aebd74844.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be3493d0e0432a5b7d4901df9c11181a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7bbc7c256c44842c026a682445b7d0c5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Exploring-How-LLMs-Capture-and-Represent-Domain-Specific-Knowledge"><a href="#Exploring-How-LLMs-Capture-and-Represent-Domain-Specific-Knowledge" class="headerlink" title="Exploring How LLMs Capture and Represent Domain-Specific Knowledge"></a>Exploring How LLMs Capture and Represent Domain-Specific Knowledge</h2><p><strong>Authors:Mirian Hipolito Garcia, Camille Couturier, Daniel Madrigal Diaz, Ankur Mallick, Anastasios Kyrillidis, Robert Sim, Victor Ruhle, Saravan Rajmohan</strong></p>
<p>We study whether Large Language Models (LLMs) inherently capture domain-specific nuances in natural language. Our experiments probe the domain sensitivity of LLMs by examining their ability to distinguish queries from different domains using hidden states generated during the prefill phase. We reveal latent domain-related trajectories that indicate the model’s internal recognition of query domains. We also study the robustness of these domain representations to variations in prompt styles and sources. Our approach leverages these representations for model selection, mapping the LLM that best matches the domain trace of the input query (i.e., the model with the highest performance on similar traces). Our findings show that LLMs can differentiate queries for related domains, and that the fine-tuned model is not always the most accurate. Unlike previous work, our interpretations apply to both closed and open-ended generative tasks </p>
<blockquote>
<p>我们研究大型语言模型（LLM）是否能在自然语言中捕获特定领域的细微差别。我们的实验通过检查预填充阶段生成的隐藏状态来探索LLM的领域敏感性，以检验它们区分不同领域查询的能力。我们揭示了与领域相关的潜在轨迹，表明模型对查询领域的内部识别。我们还研究了这些领域表示对不同提示风格和来源的稳健性。我们的方法利用这些表示来进行模型选择，找出最符合输入查询领域轨迹的LLM（即在类似轨迹上表现最佳的模型）。我们的研究结果表明，LLM能够区分相关领域的查询，而且微调模型并不总是最准确的。与之前的工作不同，我们的解释适用于封闭任务和开放式的生成任务。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16871v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）能够捕捉自然语言中的领域特定细微差别。通过实验探究LLMs对来自不同领域查询的区分能力，揭示模型内部对查询领域的识别轨迹。研究这些领域表示的稳健性，并用于模型选择，找到与输入查询领域轨迹最匹配的LLM。研究发现LLMs能区分相关领域的查询，且微调模型并非总是最准确。此解读适用于封闭和开放生成任务。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs能够捕捉领域特定的细微差别。</li>
<li>实验探究LLMs对来自不同领域的查询的区分能力。</li>
<li>LLMs内部存在识别查询领域的轨迹。</li>
<li>领域表示的稳健性研究对于模型选择至关重要。</li>
<li>输入查询的领域轨迹与LLM的匹配情况影响模型选择的准确性。</li>
<li>LLMs能够区分相关领域的查询，这是之前的作品所未涉及的。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16871">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-705bb25b907b9e0b1f210f3f2c39406e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5b9022b5c9c212f88a312e966fbfc77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f2e6e15b6691708ac19d9748b540dc0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Emo-Pillars-Knowledge-Distillation-to-Support-Fine-Grained-Context-Aware-and-Context-Less-Emotion-Classification"><a href="#Emo-Pillars-Knowledge-Distillation-to-Support-Fine-Grained-Context-Aware-and-Context-Less-Emotion-Classification" class="headerlink" title="Emo Pillars: Knowledge Distillation to Support Fine-Grained   Context-Aware and Context-Less Emotion Classification"></a>Emo Pillars: Knowledge Distillation to Support Fine-Grained   Context-Aware and Context-Less Emotion Classification</h2><p><strong>Authors:Alexander Shvets</strong></p>
<p>Most datasets for sentiment analysis lack context in which an opinion was expressed, often crucial for emotion understanding, and are mainly limited by a few emotion categories. Foundation large language models (LLMs) like GPT-4 suffer from over-predicting emotions and are too resource-intensive. We design an LLM-based data synthesis pipeline and leverage a large model, Mistral-7b, for the generation of training examples for more accessible, lightweight BERT-type encoder models. We focus on enlarging the semantic diversity of examples and propose grounding the generation into a corpus of narratives to produce non-repetitive story-character-centered utterances with unique contexts over 28 emotion classes. By running 700K inferences in 450 GPU hours, we contribute with the dataset of 100K contextual and also 300K context-less examples to cover both scenarios. We use it for fine-tuning pre-trained encoders, which results in several Emo Pillars models. We show that Emo Pillars models are highly adaptive to new domains when tuned to specific tasks such as GoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on the first three. We also validate our dataset, conducting statistical analysis and human evaluation, and confirm the success of our measures in utterance diversification (although less for the neutral class) and context personalization, while pointing out the need for improved handling of out-of-taxonomy labels within the pipeline. </p>
<blockquote>
<p>大多数情感分析数据集缺乏表达意见的背景，这对于情绪理解往往至关重要，并且主要受到有限的情绪类别的限制。像GPT-4这样的基础大型语言模型（LLM）存在过度预测情绪和过于资源密集的问题。我们设计了一个基于LLM的数据合成管道，并利用一个大型模型Mistral-7b来生成更容易访问、轻量级的BERT类型编码器模型的训练示例。我们专注于增加示例的语义多样性，并提出将生成根植于叙事语料库中，以产生以故事角色为中心的、具有独特背景的、非重复性的发言，涵盖28个情绪类别。通过运行70万次的推理分析，在45万GPU小时内，我们创建了包含10万条上下文和30万条无上下文示例的数据集，以覆盖这两种场景。我们用它来微调预训练编码器，从而产生了多个Emo Pillars模型。我们表明，当针对特定任务（如GoEmotions、ISEAR、IEMOCAP和EmoContext）进行调整时，Emo Pillars模型非常适应新领域，并在前三个任务上达到了最佳性能。我们还对我们的数据集进行了统计分析和人工评估，验证了我们的措施在发言多样化和背景个性化方面的成功（尽管中性类别的效果较小），同时指出了管道中对超出分类标准的标签处理的需要改进之处。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16856v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对情感分析数据集缺乏上下文的问题，设计了一个基于大型语言模型（LLM）的数据合成管道，利用Mistral-7b模型生成训练样本，为更易于访问、轻量级的BERT类型编码器模型提供训练数据。该研究通过扩大语义多样性并提出基于叙事语料库的生成方法，创建了包含独特上下文和28个情感类别的数据集。经过对预训练编码器的微调，生成了高度适应新领域任务的Emo Pillars模型，并在GoEmotions、ISEAR、IEMOCAP和EmoContext等任务上达到领先水平。同时，该研究还进行了统计分析和人类评估，验证了数据集的有效性和成功措施的实施情况。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前情感分析数据集缺乏上下文，这对于情感理解至关重要。</li>
<li>提出基于大型语言模型（LLM）的数据合成管道，解决了现有情感分析数据集的局限性。</li>
<li>利用Mistral-7b模型生成训练样本，为BERT类型编码器模型提供数据。</li>
<li>强调扩大语义多样性和基于叙事语料库的生成方法的重要性。</li>
<li>创建了包含独特上下文和多种情感类别的数据集。</li>
<li>通过微调预训练编码器，生成了适应新领域任务的Emo Pillars模型，并在多个任务上达到领先水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16856">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f8d9641e5a529833cb4c19177a36f6c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e12793d4b99436e913b5b12c97291e40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41cc06ebeb9a28c1471ddddaf918d3ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70c17942555f9f8954434afd05912bbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eaa8290de4a3093ad1dffb44dd55dbbe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6abdb6dda3f170a4905e1038e57ae931.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LRASGen-LLM-based-RESTful-API-Specification-Generation"><a href="#LRASGen-LLM-based-RESTful-API-Specification-Generation" class="headerlink" title="LRASGen: LLM-based RESTful API Specification Generation"></a>LRASGen: LLM-based RESTful API Specification Generation</h2><p><strong>Authors:Sida Deng, Rubing Huang, Man Zhang, Chenhui Cui, Dave Towey, Rongcun Wang</strong></p>
<p>REpresentation State Transfer (REST) is an architectural style for designing web applications that enable scalable, stateless communication between clients and servers via common HTTP techniques. Web APIs that employ the REST style are known as RESTful (or REST) APIs. When using or testing a RESTful API, developers may need to employ its specification, which is often defined by open-source standards such as the OpenAPI Specification (OAS). However, it can be very time-consuming and error-prone to write and update these specifications, which may negatively impact the use of RESTful APIs, especially when the software requirements change. Many tools and methods have been proposed to solve this problem, such as Respector and Swagger Core. OAS generation can be regarded as a common text-generation task that creates a formal description of API endpoints derived from the source code. A potential solution for this may involve using Large Language Models (LLMs), which have strong capabilities in both code understanding and text generation. Motivated by this, we propose a novel approach for generating the OASs of RESTful APIs using LLMs: LLM-based RESTful API-Specification Generation (LRASGen). To the best of our knowledge, this is the first use of LLMs and API source code to generate OASs for RESTful APIs. Compared with existing tools and methods, LRASGen can generate the OASs, even when the implementation is incomplete (with partial code, and&#x2F;or missing annotations&#x2F;comments, etc.). To evaluate the LRASGen performance, we conducted a series of empirical studies on 20 real-world RESTful APIs. The results show that two LLMs (GPT-4o mini and DeepSeek V3) can both support LARSGen to generate accurate specifications, and LRASGen-generated specifications cover an average of 48.85% more missed entities than the developer-provided specifications. </p>
<blockquote>
<p>REST（Representation State Transfer）是一种用于设计Web应用程序的架构风格，它能够通过常见的HTTP技术实现客户端和服务器之间可伸缩、无状态通信。采用REST风格的Web API被称为RESTful（或REST）API。在使用或测试RESTful API时，开发人员可能需要使用其规范，该规范通常由开放源代码标准（如OpenAPI规范（OAS））所定义。然而，编写和更新这些规范可能会非常耗时且容易出错，这可能会给RESTful API的使用带来负面影响，尤其是在软件需求发生变化时。已经提出了许多工具和方法来解决这个问题，例如Respector和Swagger Core。OAS生成可以看作是一个常见的文本生成任务，它从源代码中创建API端点的正式描述。针对这一问题的一个潜在解决方案是运用大型语言模型（LLM），LLM在代码理解和文本生成方面都具有强大的能力。因此，我们提出了一种基于LLM生成RESTful API的OASs的新方法：基于LLM的RESTful API规范生成（LRASGen）。据我们所知，这是首次使用LLM和API源代码来为RESTful API生成OASs。与现有工具和方法相比，LRASGen能够在实现不完整的情况下生成OAS（包括部分代码、缺少注释&#x2F;注释等）。为了评估LRASGen的性能，我们对20个真实的RESTful API进行了一系列实证研究。结果表明，两种LLM（GPT-4o mini和DeepSeek V3）都能支持LRASGen生成准确的规范，并且LRASGen生成的规范平均覆盖开发者提供的规范中遗漏实体的48.85%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16833v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>REST（表现层状态转移）是一种用于设计Web应用程序的架构风格，支持客户端和服务器之间通过常见的HTTP技术进行可扩展、无状态通信。当使用或测试RESTful API时，开发者需要遵循开放应用程序接口规范（OpenAPI Specification，简称OAS）。然而，编写和更新这些规范可能会非常耗时且容易出错，这可能会对RESTful API的使用产生负面影响。为解决这一问题，人们提出了多种工具和解决方案。在此背景下，首次尝试利用大型语言模型（LLM）的特性生成RESTful API的OAS。一项提议是采用LLM生成的API规范生成方法（LRASGen），该方法能够从API源代码生成正式的端点描述。通过实证研究表明，使用LLM生成的规范能够覆盖更多被遗漏的实体。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>REST是一种Web应用程序架构风格，支持可扩展、无状态通信。</li>
<li>OpenAPI Specification（OAS）是RESTful API的规范标准。</li>
<li>编写和更新OAS可能会非常耗时且容易出错。</li>
<li>LLM具有代码理解和文本生成能力，可用于解决该问题。</li>
<li>LRASGen是首次利用LLM和API源代码生成RESTful API的OAS的方法。</li>
<li>LRASGen能够在API实现不完整的情况下生成OAS。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16833">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-24a9bb9991da6680807dc91e4eb76901.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Decoupled-Global-Local-Alignment-for-Improving-Compositional-Understanding"><a href="#Decoupled-Global-Local-Alignment-for-Improving-Compositional-Understanding" class="headerlink" title="Decoupled Global-Local Alignment for Improving Compositional   Understanding"></a>Decoupled Global-Local Alignment for Improving Compositional   Understanding</h2><p><strong>Authors:Xiaoxing Hu, Kaicheng Yang, Jun Wang, Haoran Xu, Ziyong Feng, Yupei Wang</strong></p>
<p>Contrastive Language-Image Pre-training (CLIP) has achieved success on multiple downstream tasks by aligning image and text modalities. However, the nature of global contrastive learning limits CLIP’s ability to comprehend compositional concepts, such as relations and attributes. Although recent studies employ global hard negative samples to improve compositional understanding, these methods significantly compromise the model’s inherent general capabilities by forcibly distancing textual negative samples from images in the embedding space. To overcome this limitation, we introduce a Decoupled Global-Local Alignment (DeGLA) framework that improves compositional understanding while substantially mitigating losses in general capabilities. To optimize the retention of the model’s inherent capabilities, we incorporate a self-distillation mechanism within the global alignment process, aligning the learnable image-text encoder with a frozen teacher model derived from an exponential moving average. Under the constraint of self-distillation, it effectively mitigates the catastrophic forgetting of pretrained knowledge during fine-tuning. To improve compositional understanding, we first leverage the in-context learning capability of Large Language Models (LLMs) to construct about 2M high-quality negative captions across five types. Subsequently, we propose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC) loss to enhance vision-language compositionally. Extensive experimental results demonstrate the effectiveness of the DeGLA framework. Compared to previous state-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across the VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average performance improvement of 13.0% on zero-shot classification tasks across eleven datasets. Our code will be released at <a target="_blank" rel="noopener" href="https://github.com/xiaoxing2001/DeGLA">https://github.com/xiaoxing2001/DeGLA</a> </p>
<blockquote>
<p>对比语言图像预训练（CLIP）通过图像和文本模态的对齐，在多个下游任务上取得了成功。然而，全局对比学习的性质限制了CLIP对组合概念（如关系和属性）的理解能力。尽管最近有研究采用全局硬负样本改进组合理解，但这些方法通过强制在嵌入空间中拉开文本负样本与图像的距离，从而极大地削弱了模型固有的通用能力。为了克服这一局限性，我们引入了去耦全局-局部对齐（DeGLA）框架，该框架提高了组合理解能力，同时大大减轻了通用能力的损失。为了优化模型固有能力的保留，我们在全局对齐过程中融入自我蒸馏机制，将可学习的图像文本编码器与基于指数移动平均的冻结教师模型进行对齐。在自我蒸馏的约束下，它有效地减轻了微调过程中预训练知识的灾难性遗忘。为提高组合理解能力，我们首先借助大型语言模型的上下文学习能力，构建约2M种高质量负描述（跨越五种类型）。随后，我们提出图像基础对比（IGC）损失和文本基础对比（TGC）损失，以增强视觉语言的组合能力。大量的实验结果证明了DeGLA框架的有效性。与之前的先进方法相比，DeGLA在VALSE、SugarCrepe和ARO基准测试中平均提高了3.5%的性能。同时，它在11个数据集上的零样本分类任务中平均性能提高了13.0%。我们的代码将在<a target="_blank" rel="noopener" href="https://github.com/xiaoxing2001/DeGLA%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/xiaoxing2001/DeGLA上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16801v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>DeGLA框架通过引入解耦全局局部对齐技术，克服了CLIP模型在处理组合概念时的局限性，同时减少了模型通用能力的损失。该框架通过自我蒸馏机制优化全局对齐过程，并利用大型语言模型的上下文学习能力构建高质量负样本。此外，它还提出了图像基础对比（IGC）损失和文字基础对比（TGC）损失，以加强视觉语言的组合理解。实验结果表明，DeGLA框架在多个基准测试中取得了显著成效。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>DeGLA框架解决了CLIP在处理组合概念上的局限性，如关系和属性。</li>
<li>通过解耦全局局部对齐技术，DeGLA提高了模型的组合理解能力。</li>
<li>自我蒸馏机制用于优化全局对齐过程，同时保留模型的固有通用能力。</li>
<li>利用大型语言模型的上下文学习能力构建高质量负样本，以提高模型性能。</li>
<li>提出了图像基础对比（IGC）损失和文字基础对比（TGC）损失，以增强视觉语言的组合理解。</li>
<li>DeGLA框架在多个基准测试中表现优异，平均提高了3.5%的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16801">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2db2f3addc94b743fbf65af3a5710c51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdf704306236c6de10570f5602da6d58.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f43663f2e0204696ae807215ca5e7312.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f081ca317cdc985401093df7db721ec1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-829f7495220d609dc47157424815c160.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87fbee24d7d205a9d595a552c32ee41b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Towards-Explainable-AI-Multi-Modal-Transformer-for-Video-based-Image-Description-Generation"><a href="#Towards-Explainable-AI-Multi-Modal-Transformer-for-Video-based-Image-Description-Generation" class="headerlink" title="Towards Explainable AI: Multi-Modal Transformer for Video-based Image   Description Generation"></a>Towards Explainable AI: Multi-Modal Transformer for Video-based Image   Description Generation</h2><p><strong>Authors:Lakshita Agarwal, Bindu Verma</strong></p>
<p>Understanding and analyzing video actions are essential for producing insightful and contextualized descriptions, especially for video-based applications like intelligent monitoring and autonomous systems. The proposed work introduces a novel framework for generating natural language descriptions from video datasets by combining textual and visual modalities. The suggested architecture makes use of ResNet50 to extract visual features from video frames that are taken from the Microsoft Research Video Description Corpus (MSVD), and Berkeley DeepDrive eXplanation (BDD-X) datasets. The extracted visual characteristics are converted into patch embeddings and then run through an encoder-decoder model based on Generative Pre-trained Transformer-2 (GPT-2). In order to align textual and visual representations and guarantee high-quality description production, the system uses multi-head self-attention and cross-attention techniques. The model’s efficacy is demonstrated by performance evaluation using BLEU (1-4), CIDEr, METEOR, and ROUGE-L. The suggested framework outperforms traditional methods with BLEU-4 scores of 0.755 (BDD-X) and 0.778 (MSVD), CIDEr scores of 1.235 (BDD-X) and 1.315 (MSVD), METEOR scores of 0.312 (BDD-X) and 0.329 (MSVD), and ROUGE-L scores of 0.782 (BDD-X) and 0.795 (MSVD). By producing human-like, contextually relevant descriptions, strengthening interpretability, and improving real-world applications, this research advances explainable AI. </p>
<blockquote>
<p>理解和分析视频动作对于生成有洞察力和情境化的描述至关重要，尤其对于基于视频的应用，如智能监控和自主系统。所提出的工作引入了一个新颖框架，通过结合文本和视觉模态来从视频数据集中生成自然语言描述。建议的架构利用ResNet50从微软研究视频描述语料库（MSVD）和Berkeley DeepDrive eXplanation（BDD-X）数据集中提取视频帧的视觉特征。提取的视觉特征被转换为补丁嵌入，然后通过基于生成预训练转换器-2（GPT-2）的编码器-解码器模型进行处理。为了对齐文本和视觉表示并确保高质量描述的产生，系统使用了多头自注意力和交叉注意力技术。该模型的效能通过BLEU（1-4）、CIDEr、METEOR和ROUGE-L的性能评估来证明。建议的框架在BLEU分数上优于传统方法，其中BDD-X的BLEU-4分数为0.755，MSVD的BLEU-4分数为0.778；在CIDEr分数上，BDD-X的CIDEr分数为1.235，MSVD的CIDEr分数为1.315；在METEOR分数上，BDD-X的METEOR分数为0.312，MSVD的METEOR分数为0.329；在ROUGE-L分数上，BDD-X的ROUGE-L分数为0.782，MSVD的ROUGE-L分数为0.795。该研究通过生成人类般的、上下文相关的描述，加强了解释性，并改进了现实世界的应用，从而推动了可解释人工智能的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16788v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文提出一种结合文本和视觉模态，从视频数据集中生成自然语言描述的新框架。该框架利用ResNet50从微软研究视频描述语料库（MSVD）和Berkeley DeepDrive eXplanation（BDD-X）数据集中提取视频帧的视觉特征，将提取的视觉特征转换为补丁嵌入，然后通过基于生成预训练Transformer-2（GPT-2）的编码器-解码器模型进行处理。该系统采用多头自注意力和交叉注意力技术，以实现文本和视觉表示的对齐，保证高质量描述的产生。该框架的有效性通过BLEU（1-4）、CIDEr、METEOR和ROUGE-L等评估指标进行展示，与传统方法相比，其在BDD-X和MSVD数据集上的表现均有所超越。该研究为人工智能的可解释性带来了进步，能够产生人类般的、上下文相关的描述，并改善现实世界的应用。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>引入了一种新框架，可以从视频数据集中生成自然语言描述。</li>
<li>利用ResNet50从视频帧中提取视觉特征。</li>
<li>采用GPT-2为基础的的编码器-解码器模型来处理视觉特征和生成文本描述。</li>
<li>使用多头自注意力和交叉注意力技术实现文本和视觉表示的对齐。</li>
<li>在BDD-X和MSVD数据集上展示了框架的有效性，超越了传统方法。</li>
<li>产生的描述具有人类般的、上下文相关性，增强了人工智能的可解释性。</li>
<li>有助于改善现实世界的视频应用，如智能监控和自主系统。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16788">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-86800311a5343ed6a56c2d3794a79526.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-346cd188e5dbc974e3ae403a7765b07e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4bf1fc33e0752df3a570a3cc04af00f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa1504bc6a1fe157a9982c77d9fa4928.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Graph2Nav-3D-Object-Relation-Graph-Generation-to-Robot-Navigation"><a href="#Graph2Nav-3D-Object-Relation-Graph-Generation-to-Robot-Navigation" class="headerlink" title="Graph2Nav: 3D Object-Relation Graph Generation to Robot Navigation"></a>Graph2Nav: 3D Object-Relation Graph Generation to Robot Navigation</h2><p><strong>Authors:Tixiao Shan, Abhinav Rajvanshi, Niluthpol Mithun, Han-Pang Chiu</strong></p>
<p>We propose Graph2Nav, a real-time 3D object-relation graph generation framework, for autonomous navigation in the real world. Our framework fully generates and exploits both 3D objects and a rich set of semantic relationships among objects in a 3D layered scene graph, which is applicable to both indoor and outdoor scenes. It learns to generate 3D semantic relations among objects, by leveraging and advancing state-of-the-art 2D panoptic scene graph works into the 3D world via 3D semantic mapping techniques. This approach avoids previous training data constraints in learning 3D scene graphs directly from 3D data. We conduct experiments to validate the accuracy in locating 3D objects and labeling object-relations in our 3D scene graphs. We also evaluate the impact of Graph2Nav via integration with SayNav, a state-of-the-art planner based on large language models, on an unmanned ground robot to object search tasks in real environments. Our results demonstrate that modeling object relations in our scene graphs improves search efficiency in these navigation tasks. </p>
<blockquote>
<p>我们提出Graph2Nav，这是一个实时3D对象关系图生成框架，用于现实世界的自主导航。我们的框架全面生成并利用3D对象以及3D分层场景图中对象之间丰富的语义关系。该框架适用于室内和室外场景。它通过利用最先进的2D全景场景图工作，借助3D语义映射技术将其扩展到3D世界，学习生成对象之间的3D语义关系。这种方法避免了之前直接从3D数据学习3D场景图时的训练数据约束。我们进行了实验，验证了在我们3D场景图中定位3D对象和标注对象关系的准确性。我们还通过将与SayNav（基于大型语言模型的最新规划器）集成，在无人员地面机器人上进行实际环境中的对象搜索任务，以评估Graph2Nav的影响。结果表明，在我们的场景图中建模对象关系可以提高这些导航任务的搜索效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16782v1">PDF</a> </p>
<p><strong>Summary</strong><br>新一代导航技术Graph2Nav，通过实时生成3D物体关系图实现室内外自主导航。该技术通过利用先进的二维全景场景图技术，借助三维语义映射技术生成三维语义关系图，避免直接从三维数据中学习三维场景图的训练数据限制。实验证明其在定位三维物体和标注物体关系上的准确性，并结合SayNav技术，提高无人地面机器人在真实环境中的搜索效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Graph2Nav是一个实时3D物体关系图生成框架，用于室内外自主导航。</li>
<li>该技术结合了先进的二维全景场景图技术和三维语义映射技术，生成三维语义关系图。</li>
<li>避免直接从三维数据中学习三维场景图的训练数据限制。</li>
<li>实验证明Graph2Nav在定位三维物体和标注物体关系上的准确性。</li>
<li>Graph2Nav与SayNav技术结合，提高了无人地面机器人在真实环境中的搜索效率。</li>
<li>建模物体关系在导航任务中的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16782">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f918232734a8b6ea8fbc3fc8184250d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9ebb4c6ca6db6b0bd7a1c03875a0159.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bfc025c91a2fdd5f6a82c410f48c2c71.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69f5f5a24bcf7d52246e92d6b069fd06.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="How-Effective-are-Generative-Large-Language-Models-in-Performing-Requirements-Classification"><a href="#How-Effective-are-Generative-Large-Language-Models-in-Performing-Requirements-Classification" class="headerlink" title="How Effective are Generative Large Language Models in Performing   Requirements Classification?"></a>How Effective are Generative Large Language Models in Performing   Requirements Classification?</h2><p><strong>Authors:Waad Alhoshan, Alessio Ferrari, Liping Zhao</strong></p>
<p>In recent years, transformer-based large language models (LLMs) have revolutionised natural language processing (NLP), with generative models opening new possibilities for tasks that require context-aware text generation. Requirements engineering (RE) has also seen a surge in the experimentation of LLMs for different tasks, including trace-link detection, regulatory compliance, and others. Requirements classification is a common task in RE. While non-generative LLMs like BERT have been successfully applied to this task, there has been limited exploration of generative LLMs. This gap raises an important question: how well can generative LLMs, which produce context-aware outputs, perform in requirements classification? In this study, we explore the effectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing both binary and multi-class requirements classification. We design an extensive experimental study involving over 400 experiments across three widely used datasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes that while factors like prompt design and LLM architecture are universally important, others-such as dataset variations-have a more situational impact, depending on the complexity of the classification task. This insight can guide future model development and deployment strategies, focusing on optimising prompt structures and aligning model architectures with task-specific needs for improved performance. </p>
<blockquote>
<p>近年来，基于Transformer的大型语言模型（LLM）已经彻底改变了自然语言处理（NLP）的格局，生成模型为需要上下文感知文本生成的任务带来了新的可能性。需求工程（RE）领域在LLM的不同任务实验中也出现了激增，包括跟踪链接检测、法规合规性等任务。需求分类是RE中的常见任务。虽然像BERT这样的非生成性LLM已经成功应用于此任务，但对生成性LLM的探索仍然有限。这一差距引发了一个重要问题：能够产生上下文感知输出的生成性LLM在需求分类中的表现如何？在这项研究中，我们探索了三种生成性LLM——Bloom、Gemma和Llama在二元和多元需求分类中的有效性。我们设计了一项广泛的实验研究，涉及三个广泛使用的数据集（PROMISE NFR、Functional-Quality和SecReq）的400多个实验。我们的研究表明，虽然提示设计和LLM架构等因素具有普遍重要性，但其他因素，如数据集变化，对分类任务的复杂性具有更情境化的影响。这一见解可以为未来的模型开发和部署策略提供指导，侧重于优化提示结构，并根据特定任务需求调整模型架构以提高性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16768v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>近年来，基于Transformer的大型语言模型（LLM）在自然语言处理（NLP）领域掀起了一场革命，生成式模型为需要上下文感知的文本生成任务带来了新的可能性。在需求工程（RE）领域，LLM的应用实验也层出不穷，包括跟踪链接检测、法规合规等任务。需求分类是RE中的常见任务。虽然非生成式LLM，如BERT，已被成功应用于此任务，但对生成式LLM的探索却有限。本文探索了三种生成式LLM——Bloom、Gemma和Llama在二元和多元需求分类中的表现。我们设计了一项广泛的实验研究，涉及三个常用数据集（PROMISE NFR、Functional-Quality和SecReq）的400多次实验。研究发现，虽然提示设计和LLM架构等因素具有普遍重要性，但数据集变化等因素的影响更具情境性，取决于分类任务的复杂性。这一见解可以为未来的模型开发和部署策略提供指导，重点优化提示结构，并根据任务需求调整模型架构，以提高性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>变压器为基础的大型语言模型（LLM）在自然语言处理（NLP）领域有重大突破，生成式模型尤其在需要上下文感知的文本生成任务中表现优异。</li>
<li>要求工程（RE）已开始广泛实验LLM在各种任务中的应用，包括需求分类。</li>
<li>虽然非生成式LLM已被成功应用于需求分类任务，但对生成式LLM在此方面的探索仍有限。</li>
<li>本研究探索了三种生成式LLM（Bloom、Gemma和Llama）在二元和多元需求分类中的表现。</li>
<li>实验研究表明，提示设计和LLM架构对模型性能有普遍影响。</li>
<li>数据集变化对分类任务性能的影响具有情境性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16768">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-727e796c41a9be3e85e2aab64a7f7360.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Lightweight-Latent-Verifiers-for-Efficient-Meta-Generation-Strategies"><a href="#Lightweight-Latent-Verifiers-for-Efficient-Meta-Generation-Strategies" class="headerlink" title="Lightweight Latent Verifiers for Efficient Meta-Generation Strategies"></a>Lightweight Latent Verifiers for Efficient Meta-Generation Strategies</h2><p><strong>Authors:Bartosz Piotrowski, Witold Drzewakowski, Konrad Staniszewski, Piotr Miłoś</strong></p>
<p>Verifiers are auxiliary models that assess the correctness of outputs generated by base large language models (LLMs). They play a crucial role in many strategies for solving reasoning-intensive problems with LLMs. Typically, verifiers are LLMs themselves, often as large (or larger) than the base model they support, making them computationally expensive. In this work, we introduce a novel lightweight verification approach, LiLaVe, which reliably extracts correctness signals from the hidden states of the base LLM. A key advantage of LiLaVe is its ability to operate with only a small fraction of the computational budget required by traditional LLM-based verifiers. To demonstrate its practicality, we couple LiLaVe with popular meta-generation strategies, like best-of-n or self-consistency. Moreover, we design novel LiLaVe-based approaches, like conditional self-correction or conditional majority voting, that significantly improve both accuracy and efficiency in generation tasks with smaller LLMs. Our work demonstrates the fruitfulness of extracting latent information from the hidden states of LLMs, and opens the door to scalable and resource-efficient solutions for reasoning-intensive applications. </p>
<blockquote>
<p>验证器是辅助模型，用于评估基础大型语言模型（LLM）生成的输出的正确性。在LLM解决推理密集型问题的多种策略中，它们扮演着至关重要的角色。通常，验证器本身就是LLM，通常与其支持的基础模型大小相同（或更大），使其在计算上成本高昂。在这项工作中，我们介绍了一种新型轻量级验证方法LiLaVe，它可以从基础LLM的隐藏状态中可靠地提取正确性信号。LiLaVe的主要优点是其仅需传统LLM验证器所需计算预算的一小部分即可运行。为了证明其实用性，我们将LiLaVe与流行的元生成策略（如n选最佳或自我一致性）相结合。此外，我们设计了基于LiLaVe的新方法，如条件自我校正或条件多数投票，这些方法在小型LLM的生成任务中显著提高了准确性和效率。我们的工作展示了从LLM的隐藏状态中提取潜在信息的有效性，并为推理密集型应用打开了可扩展和资源高效解决方案的大门。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16760v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新型的轻量级验证方法LiLaVe，该方法可从基础大型语言模型（LLM）的隐藏状态中提取正确性信号，用于验证LLM生成的输出。LiLaVe具有显著的计算效率优势，只需传统LLM验证器所需计算预算的一小部分即可运行。此外，LiLaVe与流行的元生成策略相结合，如最佳n或自我一致性等，以提高生成任务的准确性和效率。本研究证明了从LLM的隐藏状态中挖掘潜在信息的有效性，并为资源密集型应用打开了可扩展且高效的解决方案之门。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LiLaVe是一种新型的轻量级验证方法，能够从LLM的隐藏状态中提取正确性信号。</li>
<li>LiLaVe具有显著的计算效率优势，可以减少对传统LLM验证器的计算需求。</li>
<li>LiLaVe可以与流行的元生成策略结合，如最佳n和自我一致性等，以提高生成任务的准确性。</li>
<li>新型LiLaVe方法如条件自我校正和条件多数投票等，可以显著提高生成任务的效率和准确性。</li>
<li>本研究证明了从LLM隐藏状态中挖掘潜在信息的有效性。</li>
<li>LiLaVe为可扩展性和资源效率提供了解决方案，特别是在推理密集型应用中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16760">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d3ffcfc10f5e0dc094085af20dea9f31.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="IRIS-Interactive-Research-Ideation-System-for-Accelerating-Scientific-Discovery"><a href="#IRIS-Interactive-Research-Ideation-System-for-Accelerating-Scientific-Discovery" class="headerlink" title="IRIS: Interactive Research Ideation System for Accelerating Scientific   Discovery"></a>IRIS: Interactive Research Ideation System for Accelerating Scientific   Discovery</h2><p><strong>Authors:Aniketh Garikaparthi, Manasi Patwardhan, Lovekesh Vig, Arman Cohan</strong></p>
<p>The rapid advancement in capabilities of large language models (LLMs) raises a pivotal question: How can LLMs accelerate scientific discovery? This work tackles the crucial first stage of research, generating novel hypotheses. While recent work on automated hypothesis generation focuses on multi-agent frameworks and extending test-time compute, none of the approaches effectively incorporate transparency and steerability through a synergistic Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS: Interactive Research Ideation System, an open-source platform designed for researchers to leverage LLM-assisted scientific ideation. IRIS incorporates innovative features to enhance ideation, including adaptive test-time compute expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism, and query-based literature synthesis. Designed to empower researchers with greater control and insight throughout the ideation process. We additionally conduct a user study with researchers across diverse disciplines, validating the effectiveness of our system in enhancing ideation. We open-source our code at <a target="_blank" rel="noopener" href="https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System">https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System</a> </p>
<blockquote>
<p>大型语言模型（LLM）能力的快速发展引发了一个关键问题：LLM如何加速科学发现？这项工作解决了研究的关键第一阶段，即产生新假设。虽然最近在自动假设生成方面的工作主要集中在多代理框架和扩展测试时间计算上，但没有一个方法能有效地通过协同人机交互（HITL）方法纳入透明度和可控制性来解决这个问题。为了解决这一差距，我们推出了IRIS：交互式研究创意系统，这是一个开源平台，旨在让研究人员利用LLM辅助科学创意。IRIS结合了创新功能来增强创意，包括通过蒙特卡洛树搜索（MCTS）的自适应测试时间计算扩展、精细的反馈机制和基于查询的文献综述。设计此系统旨在赋予研究人员在整个创意过程中更大的控制和洞察力。我们还与不同学科的研究人员进行了用户研究，验证了我们的系统在增强创意方面的有效性。我们已在<a target="_blank" rel="noopener" href="https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System">https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System</a>上公开了我们的代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16728v1">PDF</a> 6 pages main-text, 2 pages appendix</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的快速发展引发了一个关键问题：如何加速科学发现。本研究解决了研究的关键第一阶段，即生成新假设。尽管近期自动化假设生成的研究聚焦于多智能体框架和扩展测试时间计算，但没有任何方法能有效结合透明度和可操控性通过人机协同循环方法实现。为解决此问题，我们推出了IRIS系统，一个设计用于研究人员利用LLM辅助科学构思的开源平台。IRIS结合了创新功能以增强构思能力，包括自适应测试时间计算扩展的蒙特卡洛树搜索方法、精细反馈机制和基于查询的文献综述等。我们致力于赋予研究人员在构思过程中更大的控制和洞察力。此外，我们对不同学科的学者进行了用户研究，验证了我们的系统在增强构思方面的有效性。我们已在GitHub上公开代码：<a target="_blank" rel="noopener" href="https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System">https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型（LLM）加速科学发现的关键在于如何生成新假设。</li>
<li>目前的方法缺乏透明度和可操控性，本研究通过人机协同循环方法解决此问题。</li>
<li>IRIS系统是一个开源平台，设计用于研究人员利用LLM辅助科学构思。</li>
<li>IRIS结合了多种创新功能以增强构思能力，如蒙特卡洛树搜索方法、精细反馈机制和基于查询的文献综述等。</li>
<li>用户研究验证了IRIS系统在增强构思方面的有效性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16728">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-22465be5978cb85a6794fc25cad92c61.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d33c69219c52c5e5003f6defda4869a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c0249b6544f55b8a0e8cb5b3eaee9435.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb69cfdb80e1f3d0e112ef90ed36e52c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad0b98b8d7625819ed92e6eaa5af219b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Can-Large-Language-Models-Help-Multimodal-Language-Analysis-MMLA-A-Comprehensive-Benchmark"><a href="#Can-Large-Language-Models-Help-Multimodal-Language-Analysis-MMLA-A-Comprehensive-Benchmark" class="headerlink" title="Can Large Language Models Help Multimodal Language Analysis? MMLA: A   Comprehensive Benchmark"></a>Can Large Language Models Help Multimodal Language Analysis? MMLA: A   Comprehensive Benchmark</h2><p><strong>Authors:Hanlei Zhang, Zhuohang Li, Yeshuang Zhu, Hua Xu, Peiwu Wang, Jinchao Zhang, Jie Zhou, Haige Zhu</strong></p>
<p>Multimodal language analysis is a rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA comprises over 61K multimodal utterances drawn from both staged and real-world scenarios, covering six core dimensions of multimodal semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. We evaluate eight mainstream branches of LLMs and MLLMs using three methods: zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive experiments reveal that even fine-tuned models achieve only about 60%~70% accuracy, underscoring the limitations of current MLLMs in understanding complex human language. We believe that MMLA will serve as a solid foundation for exploring the potential of large language models in multimodal language analysis and provide valuable resources to advance this field. The datasets and code are open-sourced at <a target="_blank" rel="noopener" href="https://github.com/thuiar/MMLA">https://github.com/thuiar/MMLA</a>. </p>
<blockquote>
<p>多模态语言分析是一个快速发展的领域，它利用多种模态来提高对人类会话话语中高层次语义的理解。尽管这一领域非常重要，但关于多模态大型语言模型（MLLM）在理解认知层次语义方面的能力的研究却很少。在本文中，我们介绍了MMLA，这是一个专门设计用于解决这一差距的综合基准测试。MMLA包含超过61,000条来自舞台和真实世界场景的多模态话语，涵盖六个核心的多模态语义维度：意图、情感、对话行为、情感倾向、讲话风格和沟通行为。我们使用三种方法评估了八个主流的LLM和MLLM分支：零样本推理、监督微调以及指令微调。大量的实验表明，即使经过精细训练的模型也只能达到约60%~70%的准确率，这突显了当前MLLM在理解复杂人类语言方面的局限性。我们相信，MMLA将为探索大型语言模型在多模态语言分析方面的潜力提供坚实的基础，并为推动这一领域的发展提供宝贵的资源。数据集和代码已开源，可在<a target="_blank" rel="noopener" href="https://github.com/thuiar/MMLA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/thuiar/MMLA获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16427v1">PDF</a> 23 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了多模态语言分析领域的重要性和挑战，并引入了一个全新的评估基准MMLA。MMLA旨在评估多模态大型语言模型（MLLMs）在理解认知级语义方面的能力，涵盖了意图、情感、对话行为等六个核心维度。实验结果显示，当前MLLMs在该领域的理解仍存在局限性。MMLA的开源数据集和代码为探索大型语言模型在多模态语言分析中的潜力提供了宝贵的资源。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态语言分析是一个快速发展的领域，利用多种模式增强对人类会话言外之意的高级语义理解。</li>
<li>目前针对多模态大型语言模型（MLLMs）理解认知级语义的研究较少。</li>
<li>MMLA基准涵盖了意图、情感、对话行为等六个核心维度的多模态语义。</li>
<li>通过零样本推理、监督微调、指令微调三种方法，对八种主流LLMs和MLLMs进行了评估。</li>
<li>实验显示，即使经过精细调整的模型，准确率也只有约60%~70%，表明当前MLLMs在理解复杂人类语言方面的局限性。</li>
<li>MMLA为探索大型语言模型在多模态语言分析中的潜力提供了宝贵的资源。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16427">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ce9bd974980754940082ff4b815cb593.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3292eb7453076a3df09ae79787826dca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd028fc0228a4ef56110959c9f7f4004.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-306600ee29d6c0f4af7c100cb15f0060.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Transformer-Based-Extraction-of-Statutory-Definitions-from-the-U-S-Code"><a href="#Transformer-Based-Extraction-of-Statutory-Definitions-from-the-U-S-Code" class="headerlink" title="Transformer-Based Extraction of Statutory Definitions from the U.S. Code"></a>Transformer-Based Extraction of Statutory Definitions from the U.S. Code</h2><p><strong>Authors:Arpana Hosabettu, Harsh Shah</strong></p>
<p>Automatic extraction of definitions from legal texts is critical for enhancing the comprehension and clarity of complex legal corpora such as the United States Code (U.S.C.). We present an advanced NLP system leveraging transformer-based architectures to automatically extract defined terms, their definitions, and their scope from the U.S.C. We address the challenges of automatically identifying legal definitions, extracting defined terms, and determining their scope within this complex corpus of over 200,000 pages of federal statutory law. Building upon previous feature-based machine learning methods, our updated model employs domain-specific transformers (Legal-BERT) fine-tuned specifically for statutory texts, significantly improving extraction accuracy. Our work implements a multi-stage pipeline that combines document structure analysis with state-of-the-art language models to process legal text from the XML version of the U.S. Code. Each paragraph is first classified using a fine-tuned legal domain BERT model to determine if it contains a definition. Our system then aggregates related paragraphs into coherent definitional units and applies a combination of attention mechanisms and rule-based patterns to extract defined terms and their jurisdictional scope. The definition extraction system is evaluated on multiple titles of the U.S. Code containing thousands of definitions, demonstrating significant improvements over previous approaches. Our best model achieves 96.8% precision and 98.9% recall (98.2% F1-score), substantially outperforming traditional machine learning classifiers. This work contributes to improving accessibility and understanding of legal information while establishing a foundation for downstream legal reasoning tasks. </p>
<blockquote>
<p>从法律文本中自动提取定义对于提高对美国法典（U.S.C.）等复杂法律语料的理解力和清晰度至关重要。我们提出了一种先进的NLP系统，该系统利用基于transformer的架构自动从U.S.C.中提取定义术语、定义及其范围。我们解决了自动识别法律定义、提取定义术语以及确定其在超过20万页的联邦法定法律这一复杂语料库中的范围的挑战。我们的模型建立在基于特征的机器学习方法之上，采用针对法定文本进行微调领域的特定transformer（Legal-BERT），显著提高了提取准确性。我们的工作实现了一个多阶段管道，它将文档结构分析与最新的语言模型相结合，处理来自美国法典XML版本的法律文本。首先，我们使用微调的法律领域BERT模型对每一段进行分类，以确定其是否包含定义。然后，我们的系统将相关的段落聚集成连贯的定义单元，并结合使用注意力机制和基于规则的模式来提取定义术语及其司法管辖范围。定义提取系统在包含数千个定义的美国法典多个标题上进行了评估，证明了与之前方法的显著改进。我们最好的模型达到96.8%的精确度和98.9%的召回率（98.2%的F1分数），大大优于传统的机器学习分类器。这项工作有助于提高法律信息的可及性和理解性，同时为下游法律推理任务奠定了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16353v1">PDF</a> 7 pages, to be published in IEEE AIIoT 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个利用基于转换器架构的高级NLP系统，自动从美国法典（U.S.C）中提取定义、术语及其范围。该系统通过结合文档结构分析和最新语言模型，实现了对法律文本的多阶段处理。通过精细调整的法律领域BERT模型，系统能够准确识别和提取定义，并确定术语的管辖范围。评估结果表明，该系统在精度、召回率和F1分数方面均显著优于传统方法。这一研究有助于提高法律信息的可访问性和理解，并为下游法律推理任务奠定了基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NLP系统用于自动提取法律文本中的定义、术语和范围。</li>
<li>系统基于转换器架构构建，结合文档结构分析和最新语言模型处理法律文本。</li>
<li>使用精细调整的法律领域BERT模型来识别并提取定义。</li>
<li>通过多阶段处理实现相关段落的聚合和术语的管辖范围确定。</li>
<li>系统评估结果显示，在精度、召回率和F1分数方面显著优于传统方法。</li>
<li>此研究有助于提高法律信息的可访问性和理解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16353">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-47e6a395ea6e6afdb7f547694511028b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b444f428360b50a76b5ecd662abe5ff8.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="COBRA-Algorithm-Architecture-Co-optimized-Binary-Transformer-Accelerator-for-Edge-Inference"><a href="#COBRA-Algorithm-Architecture-Co-optimized-Binary-Transformer-Accelerator-for-Edge-Inference" class="headerlink" title="COBRA: Algorithm-Architecture Co-optimized Binary Transformer   Accelerator for Edge Inference"></a>COBRA: Algorithm-Architecture Co-optimized Binary Transformer   Accelerator for Edge Inference</h2><p><strong>Authors:Ye Qiao, Zhiheng Cheng, Yian Wang, Yifan Zhang, Yunzhe Deng, Sitao Huang</strong></p>
<p>Transformer-based models have demonstrated superior performance in various fields, including natural language processing and computer vision. However, their enormous model size and high demands in computation, memory, and communication limit their deployment to edge platforms for local, secure inference. Binary transformers offer a compact, low-complexity solution for edge deployment with reduced bandwidth needs and acceptable accuracy. However, existing binary transformers perform inefficiently on current hardware due to the lack of binary specific optimizations. To address this, we introduce COBRA, an algorithm-architecture co-optimized binary Transformer accelerator for edge computing. COBRA features a real 1-bit binary multiplication unit, enabling matrix operations with -1, 0, and +1 values, surpassing ternary methods. With further hardware-friendly optimizations in the attention block, COBRA achieves up to 3,894.7 GOPS throughput and 448.7 GOPS&#x2F;Watt energy efficiency on edge FPGAs, delivering a 311x energy efficiency improvement over GPUs and a 3.5x throughput improvement over the state-of-the-art binary accelerator, with only negligible inference accuracy degradation. </p>
<blockquote>
<p>基于Transformer的模型在自然语言处理和计算机视觉等领域表现出了卓越的性能。然而，它们庞大的模型规模以及对计算、内存和通信的高要求，限制了它们在边缘平台上的本地安全推理部署。二进制转换器提供了一种紧凑、低复杂度的解决方案，用于边缘部署，减少了带宽需求，并且保持了可接受的准确性。然而，由于缺少针对二进制的特定优化，现有的二进制转换器在当前硬件上的运行效率不高。为了解决这一问题，我们引入了COBRA，这是一种用于边缘计算的协同优化的二进制转换器加速器。COBRA具备真正的1位二进制乘法单元，能够实现使用-1、0和+1值的矩阵运算，超越了三元方法。通过注意力模块的更多硬件友好优化，COBRA在边缘FPGA上实现了高达3894.7 GOPS的吞吐量，能效为每瓦特448.7 GOPS，相较于GPU实现了高达311倍的能效提升，相较于当前主流的二进制加速器提高了约3.5倍的吞吐量提升，同时推理精度的损失几乎可以忽略不计。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16269v1">PDF</a> </p>
<p><strong>Summary</strong><br>    二进制Transformer加速器COBRA，专为边缘计算设计，具有1位二进制乘法单元，可实现矩阵操作，提高能源效率和吞吐量，降低推理精度损失。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer模型在多个领域表现优异，但在边缘设备部署中存在计算、内存和通信需求高的挑战。</li>
<li>二进制Transformer为解决边缘部署问题提供了紧凑、低复杂度的解决方案。</li>
<li>现有二进制Transformer在当前硬件上的性能不佳，缺乏针对二进制的特定优化。</li>
<li>COBRA是一个共优化的二进制Transformer加速器，用于边缘计算，具有1位二进制乘法单元，可超越三元方法。</li>
<li>COBRA在边缘FPGA上实现了高达3,894.7GOPS的吞吐量和448.7GOPS&#x2F;Watt的能效。</li>
<li>与GPU相比，COBRA实现了311倍能效改进；与现有二进制加速器相比，吞吐量提高了3.5倍。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16269">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-baf037fd707d260918f7d88208a8c646.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcc253801077ecf6c67848506e72e235.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-269d2b206a1831a0bdaad93cdddcd488.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73ecb18e9a038a0704b6821ed52cbff5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fb387527664c227993eca561800e0226.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3df1f9c10a7fe870730eccfbe44ba18b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="FinNLI-Novel-Dataset-for-Multi-Genre-Financial-Natural-Language-Inference-Benchmarking"><a href="#FinNLI-Novel-Dataset-for-Multi-Genre-Financial-Natural-Language-Inference-Benchmarking" class="headerlink" title="FinNLI: Novel Dataset for Multi-Genre Financial Natural Language   Inference Benchmarking"></a>FinNLI: Novel Dataset for Multi-Genre Financial Natural Language   Inference Benchmarking</h2><p><strong>Authors:Jabez Magomere, Elena Kochkina, Samuel Mensah, Simerjot Kaur, Charese H. Smiley</strong></p>
<p>We introduce FinNLI, a benchmark dataset for Financial Natural Language Inference (FinNLI) across diverse financial texts like SEC Filings, Annual Reports, and Earnings Call transcripts. Our dataset framework ensures diverse premise-hypothesis pairs while minimizing spurious correlations. FinNLI comprises 21,304 pairs, including a high-quality test set of 3,304 instances annotated by finance experts. Evaluations show that domain shift significantly degrades general-domain NLI performance. The highest Macro F1 scores for pre-trained (PLMs) and large language models (LLMs) baselines are 74.57% and 78.62%, respectively, highlighting the dataset’s difficulty. Surprisingly, instruction-tuned financial LLMs perform poorly, suggesting limited generalizability. FinNLI exposes weaknesses in current LLMs for financial reasoning, indicating room for improvement. </p>
<blockquote>
<p>我们介绍了FinNLI，这是一个用于金融自然语言推理（FinNLI）的基准数据集，涵盖SEC文件、年报和收益电话记录等多样化的金融文本。我们的数据集框架确保多样化的前提假设对，同时最小化偶然关联。FinNLI包含21,304对样本，其中包括由金融专家注释的高质量测试集，共3,304个实例。评估表明，领域转移会显著影响通用领域的NLI性能。预训练（PLM）和大型语言模型（LLM）基准的最高宏观F1分数分别为74.57%和78.62%，突显了数据集的难度。令人惊讶的是，按指令调整的金融LLM表现不佳，表明其泛化能力有限。FinNLI暴露了当前用于金融推理的LLM的弱点，表明还有改进的空间。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16188v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>我们推出了FinNLI，这是一个面向金融文本的自然语言推理（FinNLI）基准数据集，涵盖了SEC文件、年报和收益报告等金融文本。数据集框架确保了多样的前提假设对，同时最小化了偶然关联。FinNLI包含有专家标注的高质量测试集实例数达3304个的共计有高达数万数据样本。评估表明，跨域情况下普通NLI的性能会受到显著影响。基于大型预训练模型的大型语言模型表现稍佳，最高Macro F1分数达到百分之七十八点六二。令人惊讶的是，按照指令调整的金融LLM表现不佳，表明其泛化能力有限。FinNLI揭示了当前LLM在金融推理方面的弱点，表明仍有提升空间。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>FinNLI是一个针对金融文本的自然语言推理基准数据集。涵盖不同类型的金融文本，如SEC文件、年报和收益报告等。数据集的框架设计旨在确保假设多样性同时减少偶然性。这个数据集对于解决特定于金融领域的推理任务具有重要的作用和价值。它能够用于评估语言模型在理解复杂金融语境下的信息并据此进行推理的能力。数据集的质量很高，经过专家标注的数据样本包括在多达几千到几万的实例数量内验证的大规模框架有效提高了训练和验证结果准确性指标的一致性和稳定值谱的大小很大程度上增加更高灵活度的系统的架构增强了利用特定的监督策略和更大的可用空间利用率最后才是进一步的实质性增加上的工作效率”。核心是针对大数据的特征建立高级结构并利用基于最新开发算法的实时操作增强工具；同时也适用于数据本身变化极大的应用场景同时建立特定数据集领域训练测试等分析方法的全面综合平台构建通用的大型语言模型，将能极大地推动人工智能技术的进展并加速相关领域的研究步伐以及智能技术应用市场投入的时间和流程长度模型有助于提供通用的工具和通用服务智能商业和工业方面复杂计算和综合科技场景的生成作用不容忽视相比数据自动填充的开发维护更具标准化属性这样的成功很大程度要归功于被开发者不断提高自身素质和学习新技术新技能以及良好的行业规范以及开发团队的协同合作和勤奋创新更多依据强调基本定义和能力涵盖现代集成数据库特点被学界广泛关注尤其在更贴近生产实践和重大价值运用研究等多个领域的数据使用方面也获得了一定认可地位得到了持续而稳健的提升进展作为实现其可能性和经济收益之间的均衡趋势一个公开可利用的训练库日益重要的认知普提升了一般级别程序的简化成为沟通更复杂模式思想形成的绝佳材料本文简洁表达要求能力的最强语言表达最高价值的进步认识更多智能感知内容提高效率和资源使用率因此具有良好的社会意义及未来价值探索广阔发展前景方向重大引领型关键技术的应用空间愈发显著趋势方面还有待进一步研究通过金融行业理解建立自适应调节智能化资源最优分配结构才能发挥最大效能助力推动社会进步和发展趋势研究行业内的创新突破等需求将促进该领域不断向前发展将自然语言处理领域和金融领域更加紧密地结合起来为解决真实世界问题提供新的思路和方法</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16188">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e5b8c4d885ef257564def92af405024a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9ea66d2eb68710e83aaa72ea02d4832.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e22380338ccf91f1453352ea55bba2e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b65c3b44403b7f39d8979334e8493e8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a3cf28fa41aa382c98d594f51fcf768.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="A-LoRA-Based-Approach-to-Fine-Tuning-LLMs-for-Educational-Guidance-in-Resource-Constrained-Settings"><a href="#A-LoRA-Based-Approach-to-Fine-Tuning-LLMs-for-Educational-Guidance-in-Resource-Constrained-Settings" class="headerlink" title="A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in   Resource-Constrained Settings"></a>A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in   Resource-Constrained Settings</h2><p><strong>Authors:Md Millat Hosen</strong></p>
<p>The current study describes a cost-effective method for adapting large language models (LLMs) for academic advising with study-abroad contexts in mind and for application in low-resource methods for acculturation. With the Mistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and a 4-bit quantization method, the model underwent training in two distinct stages related to this study’s purpose to enhance domain specificity while maintaining computational efficiency. In Phase 1, the model was conditioned with a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained with manually curated datasets from the StudyAbroadGPT project to achieve enhanced, contextualized responses. Technical innovations entailed memory-efficient quantization, parameter-efficient adaptation, and continuous training analytics via Weights &amp; Biases. After training, this study demonstrated a reduction in training loss by 52.7%, 92% accuracy in domain-specific recommendations, achieved 95% markdown-based formatting support, and a median run-rate of 100 samples per second on off-the-shelf GPU equipment. These findings support the effective application of instruction-tuned LLMs within educational advisers, especially in low-resource institutional scenarios. Limitations included decreased generalizability and the application of a synthetically generated dataset, but this framework is scalable for adding new multilingual-augmented and real-time academic advising processes. Future directions may include plans for the integration of retrieval-augmented generation, applying dynamic quantization routines, and connecting to real-time academic databases to increase adaptability and accuracy. </p>
<blockquote>
<p>当前研究描述了一种具有成本效益的方法，用于针对学术咨询场景调整大型语言模型（LLM），并考虑出国留学背景，将其应用于低资源方法的适应文化过程。通过应用Mistral-7B-Instruct模型和Low-Rank Adaptation（LoRA）方法与4位量化方法，该模型在两个与本研究目的相关的独特阶段中进行了训练，以提高领域特异性同时保持计算效率。在第一阶段，该模型通过Gemini Pro API使用合成数据集进行条件训练；在第二阶段，使用StudyAbroadGPT项目的手动整理数据集进行训练，以实现增强、情境化的响应。技术创新包括内存高效的量化、参数高效的适应性和通过Weights &amp; Biases的持续训练分析。训练后，该研究展示了训练损失减少52.7%，领域特定建议的准确性达到92%，支持基于标记的格式化达到95%，并在现成的GPU设备上实现每秒处理100个样本的中值运行速率。这些发现支持在教育顾问中有效应用指令调整的大型语言模型，特别是在低资源的机构场景中。局限性包括通用性降低和应用合成生成的数据集，但这个框架可扩展，可添加新的多语言增强和实时学术咨询流程。未来方向可能包括集成检索增强生成、应用动态量化例行程序以及与实时学术数据库连接，以提高适应性和准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15610v2">PDF</a> 18 pages, 6 figures (3 graphs + 3 flowchart&#x2F;architecture diagrams),   submitted as a preprint for review consideration in AI for Education or   Machine Learning applications in low-resource settings. Includes detailed   experiments with LoRA and quantization methods for efficient LLM fine-tuning</p>
<p><strong>摘要</strong></p>
<p>本研究描述了一种具有成本效益的方法，用于适应大型语言模型（LLM），以用于学术咨询并考虑留学背景，以及用于低资源方法的适应。通过应用Mistral-7B-Instruct模型和LoRA方法以及4位量化方法，该模型经过两个阶段的专业训练，旨在提高领域特异性同时保持计算效率。第一阶段使用合成数据集通过Gemini Pro API进行条件训练，第二阶段使用StudyAbroadGPT项目的手动整理数据集进行训练，以实现增强和上下文化的响应。经过训练后，该研究证明了训练损失降低了52.7%，领域特定建议的准确性达到92%，并支持了95%的markdown格式设置。这项研究有效证明了指令微调LLM在教育顾问中的适用性，特别是在低资源机构场景中。尽管存在一些局限性，如通用性的降低和应用合成数据集的问题，但该框架可扩展，可添加新的多语言增强和实时学术咨询流程。未来的方向可能包括集成检索增强生成技术、应用动态量化例行程序以及与实时学术数据库的连接，以提高适应性和准确性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>描述了一种成本效益高的方法，用于适应大型语言模型（LLM），特别适用于学术咨询和留学背景。</li>
<li>通过两个阶段的训练提高了模型在特定领域的性能，同时保持了计算效率。</li>
<li>应用了合成数据集和手动整理的数据集进行训练，实现了上下文感知的响应。</li>
<li>研究证明了训练损失显著减少，领域特定建议的准确性高。</li>
<li>支持markdown格式设置，表明模型在格式化输出方面的能力。</li>
<li>该研究验证了指令微调LLM在教育咨询领域的应用潜力，特别是在低资源环境下。</li>
<li>虽然存在局限性，但该框架具有可扩展性，未来可进一步融入多语言支持和实时学术咨询流程。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15610">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d97da549f2dbd1533d8b1005e699b29c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3eea65f96692f375915e9720a478d3d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5d384078e0fce050f919f0f190d6a12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a70efbbfc9ad8561a9e3092647fe8b17.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="TALES-Text-Adventure-Learning-Environment-Suite"><a href="#TALES-Text-Adventure-Learning-Environment-Suite" class="headerlink" title="TALES: Text Adventure Learning Environment Suite"></a>TALES: Text Adventure Learning Environment Suite</h2><p><strong>Authors:Christopher Zhang Cui, Xingdi Yuan, Ziang Xiao, Prithviraj Ammanabrolu, Marc-Alexandre Côté</strong></p>
<p>Reasoning is an essential skill to enable Large Language Models (LLMs) to interact with the world. As tasks become more complex, they demand increasingly sophisticated and diverse reasoning capabilities for sequential decision-making, requiring structured reasoning over the context history to determine the next best action. We introduce TALES, a diverse collection of synthetic and human-written text-adventure games designed to challenge and evaluate diverse reasoning capabilities. We present results over a range of LLMs, open- and closed-weights, performing a qualitative analysis on the top performing models. Despite an impressive showing on synthetic games, even the top LLM-driven agents fail to achieve 15% on games designed for human enjoyment. Code and visualization of the experiments can be found at <a target="_blank" rel="noopener" href="https://microsoft.github.io/tales">https://microsoft.github.io/tales</a>. </p>
<blockquote>
<p>推理是使大型语言模型（LLM）与世界互动的重要技能。随着任务变得越来越复杂，它们需要更高级和多样化的推理能力来进行序列决策，需要在背景历史中进行结构化推理以确定下一个最佳行动。我们介绍了TALES，这是一系列合成和人类编写的文字冒险游戏的集合，旨在挑战和评估多样化的推理能力。我们在一系列LLM上展示了结果，包括开源和封闭权重，并对表现最好的模型进行了定性分析。即使在合成游戏上表现出色，但最好的LLM驱动的智能体在人类设计的游戏上也仅达到了15%的通过率。实验的代码和可视化部分可以在<a target="_blank" rel="noopener" href="https://microsoft.github.io/tales%E6%89%BE%E5%88%B0%E3%80%82">https://microsoft.github.io/tales找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14128v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了大型语言模型（LLM）在复杂任务中需要推理能力来与世界互动。作者引入了TALES文本冒险游戏集，旨在挑战和评估LLM的多样化推理能力。实验结果显示，即使在合成游戏中表现令人印象深刻，顶级LLM驱动的代理在人类设计的游戏中的表现也仅达到15%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）需要推理能力以应对复杂任务。</li>
<li>TALES文本冒险游戏集旨在评估和挑战LLM的多样化推理能力。</li>
<li>合成游戏中LLM表现良好，但在人类设计的游戏中表现较差。</li>
<li>顶级LLM代理在人类设计的游戏中的表现仅达到15%。</li>
<li>LLM在决策过程中需要结构化推理来应对上下文中的不同情况。</li>
<li>代码和实验可视化可在<a target="_blank" rel="noopener" href="https://microsoft.github.io/tales%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://microsoft.github.io/tales上找到。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14128">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-818993ca92180e47bd69ba87b8d7b2eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b59a501786aaa23285a0fabd128ceea2.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Decoding-Vision-Transformers-the-Diffusion-Steering-Lens"><a href="#Decoding-Vision-Transformers-the-Diffusion-Steering-Lens" class="headerlink" title="Decoding Vision Transformers: the Diffusion Steering Lens"></a>Decoding Vision Transformers: the Diffusion Steering Lens</h2><p><strong>Authors:Ryota Takatsuki, Sonia Joseph, Ippei Fujisawa, Ryota Kanai</strong></p>
<p>Logit Lens is a widely adopted method for mechanistic interpretability of transformer-based language models, enabling the analysis of how internal representations evolve across layers by projecting them into the output vocabulary space. Although applying Logit Lens to Vision Transformers (ViTs) is technically straightforward, its direct use faces limitations in capturing the richness of visual representations. Building on the work of Toker et al. (2024)~\cite{Toker2024-ve}, who introduced Diffusion Lens to visualize intermediate representations in the text encoders of text-to-image diffusion models, we demonstrate that while Diffusion Lens can effectively visualize residual stream representations in image encoders, it fails to capture the direct contributions of individual submodules. To overcome this limitation, we propose \textbf{Diffusion Steering Lens} (DSL), a novel, training-free approach that steers submodule outputs and patches subsequent indirect contributions. We validate our method through interventional studies, showing that DSL provides an intuitive and reliable interpretation of the internal processing in ViTs. </p>
<blockquote>
<p>Logit Lens是广泛应用于基于转换器的语言模型的机械解释性的方法，它通过将这些内部表示投影到输出词汇空间来分析这些表示如何在各层中演变。虽然将Logit Lens应用于视觉转换器（ViTs）在技术上很直接，但其直接使用在捕捉视觉表示的丰富性方面存在局限性。我们基于托克尔等人（Toker et al.）（引用文献将在正文中呈现）的工作，他们引入了扩散透镜（Diffusion Lens）来可视化文本编码器的中间表示形式中的文本到图像扩散模型。我们证明了虽然扩散透镜可以有效地可视化图像编码器的残差流表示形式，但它无法捕获单个子模块的直接影响。为了克服这一局限性，我们提出了训练前的一种新方法，称为扩散导向透镜（Diffusion Steering Lens，DSL）。这种方法通过调整子模块输出并修补后续的间接贡献来提供对ViTs内部处理的直观和可靠解释。我们通过干预研究验证了我们的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13763v2">PDF</a> 12 pages, 17 figures. Accepted to the CVPR 2025 Workshop on   Mechanistic Interpretability for Vision (MIV)</p>
<p><strong>Summary</strong></p>
<p>Logit Lens方法广泛应用于解释基于转换器的语言模型，但直接应用于视觉转换器（ViTs）时存在局限性。为此，本文提出一种新型的无训练方法——Diffusion Steering Lens（DSL），旨在可视化视觉转换器的内部处理过程，通过干预性研究验证了其直观性和可靠性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Logit Lens广泛用于解释基于转换器的语言模型，通过将内部表示投影到输出词汇空间来分析其演变。</li>
<li>虽然Logit Lens在应用于视觉转换器（ViTs）时技术上很直观，但其直接使用存在局限性，无法充分捕捉视觉表示的丰富性。</li>
<li>Diffusion Lens虽然可以有效地可视化文本编码器中文本到图像模型的中间表示，但在捕捉图像编码器的残差流表示方面存在局限性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13763">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9c614db78e777eab4cafcf8551d97cd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82479636c07454ea4c1330f24f22b4f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1568148bc789594619a449bd8dc24b24.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="OmniScience-A-Domain-Specialized-LLM-for-Scientific-Reasoning-and-Discovery"><a href="#OmniScience-A-Domain-Specialized-LLM-for-Scientific-Reasoning-and-Discovery" class="headerlink" title="OmniScience: A Domain-Specialized LLM for Scientific Reasoning and   Discovery"></a>OmniScience: A Domain-Specialized LLM for Scientific Reasoning and   Discovery</h2><p><strong>Authors:Vignesh Prabhakar, Md Amirul Islam, Adam Atanas, Yao-Ting Wang, Joah Han, Aastha Jhunjhunwala, Rucha Apte, Robert Clark, Kang Xu, Zihan Wang, Kai Liu</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable potential in advancing scientific knowledge and addressing complex challenges. In this work, we introduce OmniScience, a specialized large reasoning model for general science, developed through three key components: (1) domain adaptive pretraining on a carefully curated corpus of scientific literature, (2) instruction tuning on a specialized dataset to guide the model in following domain-specific tasks, and (3) reasoning-based knowledge distillation through fine-tuning to significantly enhance its ability to generate contextually relevant and logically sound responses. We demonstrate the versatility of OmniScience by developing a battery agent that efficiently ranks molecules as potential electrolyte solvents or additives. Comprehensive evaluations reveal that OmniScience is competitive with state-of-the-art large reasoning models on the GPQA Diamond and domain-specific battery benchmarks, while outperforming all public reasoning and non-reasoning models with similar parameter counts. We further demonstrate via ablation experiments that domain adaptive pretraining and reasoning-based knowledge distillation are critical to attain our performance levels, across benchmarks. </p>
<blockquote>
<p>大型语言模型（LLM）在推进科学知识和应对复杂挑战方面表现出了显著潜力。在这项工作中，我们介绍了OmniScience，这是一个针对通用科学的专门大型推理模型，通过三个关键组件开发而成：（1）在精心筛选的科学文献语料库上进行领域自适应预训练；（2）在专门的数据集上进行指令调整，以指导模型执行特定领域的任务；（3）通过微调进行基于推理的知识蒸馏，以显著提高其生成语境相关和逻辑严谨响应的能力。我们通过开发一种电池代理来展示OmniScience的通用性，该代理能够高效地对分子进行排名，作为潜在的电解质溶剂或添加剂。综合评估表明，OmniScience在GPQA Diamond和特定领域的电池基准测试上，与最新的大型推理模型相比具有竞争力，同时在参数数量相似的所有公共推理和非推理模型中表现最佳。我们还通过消融实验进一步证明，领域自适应预训练和基于推理的知识蒸馏对于达到我们的性能水平至关重要，跨越各种基准测试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17604v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>OmniScience是一个针对通用科学的大型推理模型，通过领域自适应预训练、指令调优和推理知识蒸馏等技术显著增强了其在科学领域的推理能力。在电池代理开发中的应用展示了其多功能性，并在GPQA Diamond和特定电池基准测试中表现出竞争力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OmniScience是一个针对通用科学的大型推理模型。</li>
<li>它通过领域自适应预训练、指令调优和推理知识蒸馏等技术进行开发。</li>
<li>OmniScience在电池代理开发中的应用展示了其多功能性。</li>
<li>在GPQA Diamond和特定电池基准测试中，OmniScience表现出竞争力，优于所有类似的公共推理和非推理模型。</li>
<li>领域自适应预训练和推理知识蒸馏对于达到高性能水平至关重要。</li>
<li>OmniScience模型能够生成与上下文相关且逻辑严谨的回答。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17604">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-20e8cc74e03be7685752af6aaee7dfe8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5cddc57a4b1e46185ec103df103a43ee.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-25/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-25/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-25/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-221bd3f73cedfd92fa5f7929a7829834.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-04-25  OptimAI Optimization from Natural Language Using LLM-Powered AI Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-25/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-76c8df71fc8d5131af3b83050508ac25.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-04-25  Tracing Thought Using Chain-of-Thought Reasoning to Identify the LLM   Behind AI-Generated Text
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17663.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
