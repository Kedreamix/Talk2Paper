<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-04-25  Advanced Chest X-Ray Analysis via Transformer-Based Image Descriptors   and Cross-Model Attention Mechanism">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-2de2ab782bb738a685dc213474d3390b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    48 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-25-更新"><a href="#2025-04-25-更新" class="headerlink" title="2025-04-25 更新"></a>2025-04-25 更新</h1><h2 id="Advanced-Chest-X-Ray-Analysis-via-Transformer-Based-Image-Descriptors-and-Cross-Model-Attention-Mechanism"><a href="#Advanced-Chest-X-Ray-Analysis-via-Transformer-Based-Image-Descriptors-and-Cross-Model-Attention-Mechanism" class="headerlink" title="Advanced Chest X-Ray Analysis via Transformer-Based Image Descriptors   and Cross-Model Attention Mechanism"></a>Advanced Chest X-Ray Analysis via Transformer-Based Image Descriptors   and Cross-Model Attention Mechanism</h2><p><strong>Authors:Lakshita Agarwal, Bindu Verma</strong></p>
<p>The examination of chest X-ray images is a crucial component in detecting various thoracic illnesses. This study introduces a new image description generation model that integrates a Vision Transformer (ViT) encoder with cross-modal attention and a GPT-4-based transformer decoder. The ViT captures high-quality visual features from chest X-rays, which are fused with text data through cross-modal attention to improve the accuracy, context, and richness of image descriptions. The GPT-4 decoder transforms these fused features into accurate and relevant captions. The model was tested on the National Institutes of Health (NIH) and Indiana University (IU) Chest X-ray datasets. On the IU dataset, it achieved scores of 0.854 (B-1), 0.883 (CIDEr), 0.759 (METEOR), and 0.712 (ROUGE-L). On the NIH dataset, it achieved the best performance on all metrics: BLEU 1–4 (0.825, 0.788, 0.765, 0.752), CIDEr (0.857), METEOR (0.726), and ROUGE-L (0.705). This framework has the potential to enhance chest X-ray evaluation, assisting radiologists in more precise and efficient diagnosis. </p>
<blockquote>
<p>胸部X射线图像的检查是检测各种胸部疾病的关键环节。本研究介绍了一种新的图像描述生成模型，该模型集成了Vision Transformer（ViT）编码器、跨模态注意力和基于GPT-4的变压器解码器。ViT从胸部X射线图像中捕获高质量视觉特征，通过跨模态注意力与文本数据融合，提高了图像描述的准确性、上下文关联度和丰富性。GPT-4解码器将这些融合的特征转化为准确且相关的标题。该模型在美国国立卫生研究院（NIH）和印第安纳大学（IU）的胸部X射线数据集上进行了测试。在IU数据集上，它实现了B-1得分0.854、CIDEr得分0.883、METEOR得分0.759和ROUGE-L得分0.712。在NIH数据集上，它在所有指标上都取得了最佳性能：BLEU 1-4（0.825、0.788、0.765、0.752），CIDEr（0.857），METEOR（0.726）和ROUGE-L（0.705）。该框架有潜力增强胸部X射线的评估能力，帮助放射科医生进行更准确和高效的诊断。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16774v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本研究介绍了一种新的胸X光图像描述生成模型。该模型结合了Vision Transformer（ViT）编码器和基于GPT-4的转换器解码器，并通过跨模态注意力机制实现图像与文本的融合。ViT从胸X光图像中提取高质量视觉特征，通过跨模态注意力与文本数据融合，提高了图像描述的准确性、上下文关联性和丰富性。GPT-4解码器将这些融合的特征转化为准确且相关的描述。该模型在国立卫生研究院（NIH）和印第安纳大学（IU）的胸X光射线数据集上进行了测试，并在IU数据集上取得了较高的BLEU、CIDEr、METEOR和ROUGE-L等指标分数。在NIH数据集上，该模型在所有指标上都取得了最佳性能。该框架具有提高胸X光评价，协助放射科医生进行更准确、更高效的诊断的潜力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>胸X光图像分析对于检测胸部疾病至关重要。</li>
<li>研究提出了一种新的图像描述生成模型，结合了ViT编码器和GPT-4解码器。</li>
<li>ViT编码器从胸X光图像中提取视觉特征，通过跨模态注意力与文本数据融合。</li>
<li>GPT-4解码器将融合的特征转化为准确的描述。</li>
<li>模型在NIH和IU数据集上进行了测试，并在多个评估指标上取得了良好成绩。</li>
<li>该模型在所有指标上的性能均优于先前的研究。</li>
<li>该框架有潜力提高胸X光的评估效率，辅助放射科医生进行更精确的诊断。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16774">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2939bae2037d6e04c7ecacce576d0468.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a673af6a856e82bb9f463302dd0c6f53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-814d6825852c1dcf8309345cd1c24bfa.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SAIP-Net-Enhancing-Remote-Sensing-Image-Segmentation-via-Spectral-Adaptive-Information-Propagation"><a href="#SAIP-Net-Enhancing-Remote-Sensing-Image-Segmentation-via-Spectral-Adaptive-Information-Propagation" class="headerlink" title="SAIP-Net: Enhancing Remote Sensing Image Segmentation via Spectral   Adaptive Information Propagation"></a>SAIP-Net: Enhancing Remote Sensing Image Segmentation via Spectral   Adaptive Information Propagation</h2><p><strong>Authors:Zhongtao Wang, Xizhe Cao, Yisong Chen, Guoping Wang</strong></p>
<p>Semantic segmentation of remote sensing imagery demands precise spatial boundaries and robust intra-class consistency, challenging conventional hierarchical models. To address limitations arising from spatial domain feature fusion and insufficient receptive fields, this paper introduces SAIP-Net, a novel frequency-aware segmentation framework that leverages Spectral Adaptive Information Propagation. SAIP-Net employs adaptive frequency filtering and multi-scale receptive field enhancement to effectively suppress intra-class feature inconsistencies and sharpen boundary lines. Comprehensive experiments demonstrate significant performance improvements over state-of-the-art methods, highlighting the effectiveness of spectral-adaptive strategies combined with expanded receptive fields for remote sensing image segmentation. </p>
<blockquote>
<p>遥感影像的语义分割要求精确的空间边界和稳健的类内一致性，这挑战了传统的分层模型。为了解决由空间域特征融合和感受野不足而产生的局限性，本文引入了SAIP-Net，这是一种利用光谱自适应信息传播的新型频率感知分割框架。SAIP-Net采用自适应频率滤波和多尺度感受野增强，有效地抑制了类内特征的不一致性，提高了边界线的清晰度。综合实验表明，该方法在最新方法的基础上显著提高了性能，突出了结合扩展感受野的光谱自适应策略在遥感图像分割中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16564v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于谱自适应信息传播的频率感知分割框架SAIP-Net，用于遥感图像语义分割。针对空间域特征融合和感受野不足的问题，该网络采用自适应频率滤波和多尺度感受野增强技术，有效抑制了类内特征不一致性，提高了边界线的清晰度。实验表明，与最新技术相比，该网络在遥感图像分割方面表现出显著的性能改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>遥感图像语义分割需要精确的空间边界和稳健的类内一致性，挑战了传统层次模型。</li>
<li>SAIP-Net是一种新型频率感知分割框架，利用谱自适应信息传播来解决这个问题。</li>
<li>SAIP-Net通过自适应频率滤波和多尺度感受野增强技术，有效提高了遥感图像分割的性能。</li>
<li>自适应频率滤波有助于抑制类内特征的不一致性。</li>
<li>多尺度感受野增强可以扩大网络对上下文信息的捕获能力，从而更准确地识别边界。</li>
<li>实验结果表明，SAIP-Net在遥感图像分割方面显著优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16564">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-405fecb1bfb3eb59ce2c20d8444a38cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84334e8ae0002cf31231c8b89c6aff68.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e2e04599d980ba3079900431170f3a1e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ffef3813c40a0c21d339a6e1b01bb0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bda3f5dbd5c3b3948ceb0a4ccdec05e6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e8feaa1c1a4c55cc04b4275aac39afcd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Cross-Paradigm-Representation-and-Alignment-Transformer-for-Image-Deraining"><a href="#Cross-Paradigm-Representation-and-Alignment-Transformer-for-Image-Deraining" class="headerlink" title="Cross Paradigm Representation and Alignment Transformer for Image   Deraining"></a>Cross Paradigm Representation and Alignment Transformer for Image   Deraining</h2><p><strong>Authors:Shun Zou, Yi Zou, Juncheng Li, Guangwei Gao, Guojun Qi</strong></p>
<p>Transformer-based networks have achieved strong performance in low-level vision tasks like image deraining by utilizing spatial or channel-wise self-attention. However, irregular rain patterns and complex geometric overlaps challenge single-paradigm architectures, necessitating a unified framework to integrate complementary global-local and spatial-channel representations. To address this, we propose a novel Cross Paradigm Representation and Alignment Transformer (CPRAformer). Its core idea is the hierarchical representation and alignment, leveraging the strengths of both paradigms (spatial-channel and global-local) to aid image reconstruction. It bridges the gap within and between paradigms, aligning and coordinating them to enable deep interaction and fusion of features. Specifically, we use two types of self-attention in the Transformer blocks: sparse prompt channel self-attention (SPC-SA) and spatial pixel refinement self-attention (SPR-SA). SPC-SA enhances global channel dependencies through dynamic sparsity, while SPR-SA focuses on spatial rain distribution and fine-grained texture recovery. To address the feature misalignment and knowledge differences between them, we introduce the Adaptive Alignment Frequency Module (AAFM), which aligns and interacts with features in a two-stage progressive manner, enabling adaptive guidance and complementarity. This reduces the information gap within and between paradigms. Through this unified cross-paradigm dynamic interaction framework, we achieve the extraction of the most valuable interactive fusion information from the two paradigms. Extensive experiments demonstrate that our model achieves state-of-the-art performance on eight benchmark datasets and further validates CPRAformer’s robustness in other image restoration tasks and downstream applications. </p>
<blockquote>
<p>基于Transformer的网络通过利用空间或通道级的自注意力在低级别视觉任务（如图像去雨）中取得了强大的性能。然而，不规则的雨型和复杂的几何重叠对单一范式架构提出了挑战，需要一种统一的框架来整合互补的全局-局部和空间-通道表示。为了解决这一问题，我们提出了一种新的跨范式表示和对齐Transformer（CPRAformer）。其核心思想是分层表示和对齐，利用两种范式（空间通道和全局局部）的优势来辅助图像重建。它弥合了范式内部和之间的鸿沟，对齐并协调它们，以实现特征的深度交互和融合。具体来说，我们在Transformer块中使用了两种类型的自注意力：稀疏提示通道自注意力（SPC-SA）和空间像素细化自注意力（SPR-SA）。SPC-SA通过动态稀疏性增强全局通道依赖性，而SPR-SA专注于空间雨分布和细粒度纹理恢复。为了解决特征不对齐和他们之间知识差异的问题，我们引入了自适应对齐频率模块（AAFM），以两阶段渐进的方式与特征对齐和交互，实现自适应指导和互补性。这减少了范式内部和之间的信息鸿沟。通过这个统一的跨范式动态交互框架，我们从两个范式中提取了最有价值的交互式融合信息。大量实验表明，我们的模型在八个基准数据集上达到了最先进的性能，并进一步验证了CPRAformer在其他图像恢复任务和下游应用中的稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16455v1">PDF</a> code: <a target="_blank" rel="noopener" href="https://github.com/zs1314/CPRAformer">https://github.com/zs1314/CPRAformer</a></p>
<p><strong>Summary</strong></p>
<p>本文提出一种新颖的跨范式表示与对齐Transformer（CPRAformer），用于解决图像去雨等低级别视觉任务中的挑战。CPRAformer融合了全局-局部和空间-通道两种互补表示，通过层次表示与对齐技术，实现了跨范式动态交互。使用两种自注意力机制，提高了特征提取和融合的效果。实验证明，CPRAformer在八个基准数据集上取得了最佳性能，并在其他图像恢复任务和下游应用中验证了其稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer网络在图像去雨等低级别视觉任务中表现出强性能，但面临不规则雨型和复杂几何重叠的挑战。</li>
<li>单一范式架构无法应对这些挑战，需要整合全局-局部和空间-通道表示的统一框架。</li>
<li>CPRAformer提出层次表示与对齐技术，结合两种范式的优点，提高图像重建效果。</li>
<li>CPRAformer使用两种自注意力机制：稀疏提示通道自注意力（SPC-SA）和空间像素细化自注意力（SPR-SA），分别增强通道依赖性和空间雨分布的恢复。</li>
<li>引入自适应对齐频率模块（AAFM），实现特征对齐和交互，缩小范式内的信息差距。</li>
<li>CPRAformer在八个基准数据集上取得最佳性能，证明了其跨范式的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16455">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2de2ab782bb738a685dc213474d3390b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-77d75a25a5c277abaab4aceb7dbacb56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e1f92a401699402185e6d3c695f5226.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50b0e9a17dc99780626b3e3f4663106e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ef003fae5859520b5248f232e38f619.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cd9bf294eaa41f078fc87c0d80c305a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Evolution-of-QPO-during-Rising-Phase-of-Discovery-Outburst-of-Swift-J1727-8-1613-Estimation-of-Mass-from-Spectro-Temporal-Study"><a href="#Evolution-of-QPO-during-Rising-Phase-of-Discovery-Outburst-of-Swift-J1727-8-1613-Estimation-of-Mass-from-Spectro-Temporal-Study" class="headerlink" title="Evolution of QPO during Rising Phase of Discovery Outburst of Swift   J1727.8-1613: Estimation of Mass from Spectro-Temporal Study"></a>Evolution of QPO during Rising Phase of Discovery Outburst of Swift   J1727.8-1613: Estimation of Mass from Spectro-Temporal Study</h2><p><strong>Authors:Dipak Debnath, Hsiang-Kuang Chang, Sujoy Kumar Nath, Lev Titarchuk</strong></p>
<p>The rising phase of the 2023-24 outburst of the recently discovered bright transient black hole candidate Swift J1727.8-1613 was monitored by \textit{Insight}-HXMT. We study the evolution of hard ($4$-$150$ keV) and soft ($2$-$4$ keV) band photon count rates, the hardness ratio (HR), and QPO frequencies using daily observations from the HXMT&#x2F;LE, ME, and HE instruments between August 25 and October 5, 2023. The QPO frequency is found to be strongly correlated with the soft-band X-ray count rates, and spectral photon indices. In contrast, a strong anti-correlation is observed between HR and QPO frequency, as well as between HR and photon index. Based on the evolution of the QPO frequency, the rising phase of the outburst is subdivided into six parts, with parts 1-5 fitted using the propagating oscillatory shock (POS) solution to understand the nature of the evolution from a physical perspective. The best-fitted POS model is obtained with a black hole mass of $13.34\pm0.02<del>M_\odot$. An inward-propagating shock with weakening strength (except in part 4) is observed during the period of our study. The POS model-fitted mass of the source is further confirmed using the QPO frequency ($\nu$)-photon index ($\Gamma$) scaling method. From this method, the estimated probable mass of Swift J1727.8-1613 is obtained to be $13.54\pm1.87</del>M_\odot$. </p>
<blockquote>
<p>洞察号HXMT监测了最近发现的明亮瞬态黑洞候选体Swift J1727.8-1613在2023-24年爆发期的上升阶段。我们研究了硬度比为HR以及QPO频率的变化情况。这些研究基于HXMT&#x2F;LE、ME和HE仪器在2023年8月25日至10月5日之间的每日观测数据，涉及硬波段（4-150千电子伏）和软波段（2-4千电子伏）光子计数率的演化。我们发现QPO频率与软波段X射线计数率和光谱光子指数之间存在强烈的相关性。相反，HR与QPO频率和光子指数之间则显示出强烈的反相关性。基于QPO频率的演化，爆发的上升阶段被分为六个部分，其中第1至第5部分使用传播振荡冲击（POS）解决方案进行拟合，以从物理角度了解演化的性质。使用POS模型的最佳拟合黑洞质量为13.34±0.02太阳质量。在我们的研究期间观察到向内传播的冲击波强度减弱（除第4部分外）。使用QPO频率（ν）-光子指数（Γ）标度方法进一步证实了源头的POS模型拟合质量。由此估计，Swift J1727.8-1613的可能质量为13.54±1.87太阳质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16391v1">PDF</a> 11 Pages, 4 Figures, 1 Table (In-communication to ApJ)</p>
<p><strong>Summary</strong><br>     基于Insight-HXMT的观测，发现2023年至2024年短暂爆发中Sw J1727.8-1613硬和软光子计数率的演化特点及其变化特征。发现QPO频率与软波段X射线计数率和光子指数存在强相关性，而硬度比与QPO频率和光子指数之间存在强反相关关系。基于QPO频率演化，爆发上升阶段可分为六部分，其中部分阶段采用传播振荡冲击（POS）解决方案进行拟合分析。通过POS模型估计源质量约为$13.34\pm0.02M_\odot$，并通过QPO频率与光子指数关联得到验证。估算Sw J1727.8-1613的可能质量大约为$13.54\pm1.87M_\odot$。结果对于理解瞬态黑洞候选物的性质具有重要意义。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Insight-HXMT监测了Swift J1727.8-1613黑洞候选体的爆发过程。</li>
<li>硬波段和软波段光子计数率随观测时间变化而变化。</li>
<li>QPO频率与软波段X射线计数率和光子指数存在强相关性。</li>
<li>HR与QPO频率和光子指数之间存在反相关关系。</li>
<li>基于QPO频率演化，爆发上升阶段可分为六个部分。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16391">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9068121d6ebf4b4ba5512d5baeff4b00.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee5a5f4afc77f1bbb667af8daf287f3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e3a438789c6273b43244b85ed5b26fe.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Environmental-Dependence-of-X-Ray-Emission-From-The-Least-Massive-Galaxies"><a href="#Environmental-Dependence-of-X-Ray-Emission-From-The-Least-Massive-Galaxies" class="headerlink" title="Environmental Dependence of X-Ray Emission From The Least Massive   Galaxies"></a>Environmental Dependence of X-Ray Emission From The Least Massive   Galaxies</h2><p><strong>Authors:Marko Mićić, Xinyu Dai, Nick Shumate, Khoa Nguyen Tran, Heechan Yuk</strong></p>
<p>The low-mass end of low-mass galaxies is largely unexplored in AGN studies, but it is essential for extending our understanding of the black hole-galaxy coevolution. We surveyed the 3D-HST catalog and collected a sample of 546 dwarf galaxies with stellar masses log(M$<em>*$&#x2F;(M_\odot))$&lt;$8.7, residing in the GOODS-South deep field. We then used the unprecedented depth of Chandra available in the GOODS-South field to search for AGN. We carefully investigated the factors that could play roles in the AGN detectability, such as Chandra’s point-spread function and the redshift- and off-axis-dependent detection limits. We identified 16 X-ray sources that are likely associated with AGN activity. Next, we evaluated the environment density of each galaxy by computing tidal indices. We uncovered a dramatic impact of the environment on AGN triggering as dwarfs from high-density environments showed an AGN fraction of 22.5%, while the median stellar mass of this subset of dwarfs is only log(M$</em>*$&#x2F;(M_\odot))&#x3D;8.1. In contrast, the low-density environment dwarfs showed an AGN fraction of only 1.4%, in line with typically reported values from the literature. This highlights the fact that massive central black holes are ubiquitous even at the lowest mass scales and demonstrates the importance of the environment in triggering black hole accretion, as well as the necessity for deep X-ray data and proper evaluation of the X-ray data quality. Alternatively, even if the detected X-ray sources are related to stellar mass accretors rather than AGN, the environmental dependence persists, signaling the impact of the environment on galaxy evolution and star formation processes at the lowest mass scales. Additionally, we stacked the X-ray images of non-detected galaxies from high- and low-density environments, revealing similar trends. </p>
<blockquote>
<p>在天文研究领域中，对于低质量星系的小质量端部分的研究尚处于未开发状态，但是为了深入了解黑洞与星系的共演化过程，这部分研究是至关重要的。我们调查了3D-HST目录，收集了一个样本，其中包括居住在GOODS-South深场的546个恒星质量为log(M*_*&#x2F;M⊙)&lt;8.7的矮星系。然后，我们利用GOODS-South区域中前所未有的深度观察来寻找活动星系核（Active Galactic Nucleus，简称AGNs）。我们仔细研究了可能影响活动星系核探测的因素，如钱德拉望远镜的点扩散函数以及红移和离轴距离相关的检测极限。我们确定了可能与活动星系核活动相关的16个X射线源。接下来，我们通过计算潮汐指数来评估每个星系的环境密度。我们发现环境对触发活动星系核的影响非常显著，因为来自高密度的矮星系表现出高达22.5%的活动星系核比例，而这一矮星系子集的中等恒星质量仅为log(M*_*&#x2F;M⊙)&#x3D;8.1。相比之下，低密度环境中的矮星系仅显示出1.4%的活动星系核比例，这与文献中通常报道的值相符。这强调了即使在最低质量尺度上，巨大的中心黑洞也是普遍存在的，并证明了环境在触发黑洞吸积中的重要性，以及深度X射线数据和正确评估X射线数据质量的重要性。即使检测到的X射线源与恒星质量吸积体有关而非活动星系核，环境的依赖性仍然存在，这预示着环境对最低质量尺度上的星系演化及恒星形成过程的影响。此外，我们还叠加了高密度和低密度环境中未检测到X射线的星系图像，显示出相似的趋势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16285v1">PDF</a> Six pages, eight figures. Submitted to MNRAS. Comments are welcome</p>
<p><strong>Summary</strong></p>
<p>在深入研究低质量星系低质量端对活动星系核（AGN）的理解时，我们对GOODS-South深场的矮星系进行了调查。结合先进深度的Chandra数据搜索相关星系核，并对探测影响因素进行了研究。结果表明，环境密度显著影响星系核活动触发，高密度环境下的矮星系表现出更高的星系核活动率。这一发现强调了环境在触发黑洞吸积过程中的重要性，并凸显出深入X射线数据和正确评估其质量的重要性。无论X射线源是否与恒星质量吸积有关，环境对最低质量尺度上的星系演化和恒星形成过程的影响都是显著的。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>低质量星系的低质量端对于理解黑洞与星系的协同演化至关重要。</li>
<li>利用3D-HST目录收集了GOODS-South深场的546个矮星系样本。</li>
<li>通过深入的Chandra数据寻找相关星系核活动。</li>
<li>探测因素包括Chandra的点扩散函数和与红移及离轴距离相关的检测限制。</li>
<li>高密度环境下的矮星系表现出更高的星系核活动率（22.5%），而低密度的环境则表现出较低的活动率（仅1.4%）。</li>
<li>环境在触发黑洞吸积过程中的作用显著。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16285">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3d4b060320f837120404dea312c72de5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43116246431db7456aa88900538d71b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-65bcee06e53634eb53925b97bf8b29ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-660eea652fdf205e544b41df256985c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19a988551ec57aded247342e58310cd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4044e470ae22723b149520d9cdfbf1fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63ff47686940458a847f8e415a5326b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2b20bcd88de8b6eead365b8f4b5cdfb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Multiobjective-optimization-for-scattering-mitigation-and-scattering-screen-reconstruction-in-VLBI-observations-of-the-Galactic-Center"><a href="#Multiobjective-optimization-for-scattering-mitigation-and-scattering-screen-reconstruction-in-VLBI-observations-of-the-Galactic-Center" class="headerlink" title="Multiobjective optimization for scattering mitigation and scattering   screen reconstruction in VLBI observations of the Galactic Center"></a>Multiobjective optimization for scattering mitigation and scattering   screen reconstruction in VLBI observations of the Galactic Center</h2><p><strong>Authors:Alejandro Mus, Teresa Toscano, Hendrik Müller, Guang-Yao Zhao, Andrei Lobanov, Ciriaco Goddi</strong></p>
<p>Imaging reconstruction of interferometric data is a hard ill-posed inverse problem. Its difficulty is increased when observing the Galactic Center, which is obscured by a scattering screen. This is because the scattering breaks the one-to-one correspondence between images and visibilities. Solving the scattering problem is one of the biggest challenges in radio imaging of the Galactic Center. In this work we present a novel strategy to mitigate its effect and constrain the screen itself using multiobjective optimization. We exploit the potential of evolutionary algorithms to describe the optimization landscape to recover the intrinsic source structure and the scattering screen affecting the data. We successfully recover both the screen and the source in a wide range of simulated cases, including the speed of a moving screen at 230 GHz. Particularly, we can recover a ring structure in scattered data at 86 GHz. Our analysis demonstrates the huge potential that recent advancements in imaging and optimization algorithms offer to recover image structures, even in weakly constrained and degenerated, possibly multi-modal settings. The successful reconstruction of the scattering screen opens the window to event horizon scale works on the Galactic Center at 86G Hz up to 116 GHz, and the study of the scattering screen itself. </p>
<blockquote>
<p>成像重建干涉数据是一个难以解决的逆向问题。当观测银河中心时，难度会增加，因为银河中心被一个散射屏遮蔽。这是因为散射破坏了图像和可见度之间的一一对应关系。解决散射问题是银河中心射电成像面临的最大挑战之一。在这项工作中，我们提出了一种新的策略来缓解其影响，并使用多目标优化来约束屏幕本身。我们利用进化算法描述优化景观的潜力，以恢复内在源结构和影响数据的散射屏。我们在广泛的模拟案例中成功地恢复了屏幕和源，包括移动屏幕的速度在230 GHz时的情况。尤其值得一提的是，我们可以在散射数据中恢复环形结构，频率为86 GHz。我们的分析表明，成像和优化算法的最新进展在恢复图像结构方面具有巨大的潜力，即使在弱约束和退化、可能是多模式设置的情况下也是如此。成功重建散射屏为在86 GHz至116 GHz上对银河中心的事件视界规模进行研究打开了窗口，以及研究散射屏本身。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16257v1">PDF</a> To appear in A&amp;A</p>
<p><strong>Summary</strong><br>     本文提出一种新型策略，利用多目标优化解决干涉数据成像的逆问题，特别是在观测银河系中心时遇到的散射屏问题。通过进化算法描述优化景观，恢复影响数据的固有源结构和散射屏。在模拟案例中成功恢复了屏幕和源，包括移动屏幕的速度。对散射数据的环形结构的恢复显示了在图像和优化算法方面的最新进展在恢复图像结构方面的巨大潜力，即使在最弱的约束和退化的多模态设置中也是如此。成功重建散射屏为在86G至116千兆赫频率上对银河系中心的事件视界规模工作打开了窗口，并对散射屏本身进行了研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>干涉数据成像是一个困难的逆问题，特别是在观测银河系中心时。</li>
<li>散射屏的存在打破了图像与可见度之间的一一对应关系，增加了问题的难度。</li>
<li>本文提出了一种使用多目标优化策略来解决散射问题的新方法。</li>
<li>进化算法被用来描述优化景观，以恢复固有源结构和影响数据的散射屏。</li>
<li>在模拟案例中成功恢复了屏幕和源，包括移动屏幕的速度。</li>
<li>能够恢复散射数据中的环形结构，显示出图像和优化算法最新进展的巨大潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16257">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-36d2410d9654267eb597a45024d53bf9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Comprehensive-Evaluation-of-Quantitative-Measurements-from-Automated-Deep-Segmentations-of-PSMA-PET-CT-Images"><a href="#Comprehensive-Evaluation-of-Quantitative-Measurements-from-Automated-Deep-Segmentations-of-PSMA-PET-CT-Images" class="headerlink" title="Comprehensive Evaluation of Quantitative Measurements from Automated   Deep Segmentations of PSMA PET&#x2F;CT Images"></a>Comprehensive Evaluation of Quantitative Measurements from Automated   Deep Segmentations of PSMA PET&#x2F;CT Images</h2><p><strong>Authors:Obed Korshie Dzikunu, Amirhossein Toosi, Shadab Ahamed, Sara Harsini, Francois Benard, Xiaoxiao Li, Arman Rahmim</strong></p>
<p>This study performs a comprehensive evaluation of quantitative measurements as extracted from automated deep-learning-based segmentation methods, beyond traditional Dice Similarity Coefficient assessments, focusing on six quantitative metrics, namely SUVmax, SUVmean, total lesion activity (TLA), tumor volume (TMTV), lesion count, and lesion spread. We analyzed 380 prostate-specific membrane antigen (PSMA) targeted [18F]DCFPyL PET&#x2F;CT scans of patients with biochemical recurrence of prostate cancer, training deep neural networks, U-Net, Attention U-Net and SegResNet with four loss functions: Dice Loss, Dice Cross Entropy, Dice Focal Loss, and our proposed L1 weighted Dice Focal Loss (L1DFL). Evaluations indicated that Attention U-Net paired with L1DFL achieved the strongest correlation with the ground truth (concordance correlation &#x3D; 0.90-0.99 for SUVmax and TLA), whereas models employing the Dice Loss and the other two compound losses, particularly with SegResNet, underperformed. Equivalence testing (TOST, alpha &#x3D; 0.05, Delta &#x3D; 20%) confirmed high performance for SUV metrics, lesion count and TLA, with L1DFL yielding the best performance. By contrast, tumor volume and lesion spread exhibited greater variability. Bland-Altman, Coverage Probability, and Total Deviation Index analyses further highlighted that our proposed L1DFL minimizes variability in quantification of the ground truth clinical measures. The code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/ObedDzik/pca/_segment.git">https://github.com/ObedDzik/pca\_segment.git</a>. </p>
<blockquote>
<p>本研究对基于深度学习的自动化分割方法所提取的定量测量值进行了全面评估，这些评估超越了传统的Dice相似系数评估，主要关注六个定量指标，即SUVmax、SUVmean、总病灶活性（TLA）、肿瘤体积（TMTV）、病灶数量和病灶扩散。我们分析了380例前列腺特异性膜抗原（PSMA）靶向[18F]DCFPyL PET&#x2F;CT扫描的生化复发前列腺癌患者，训练深度神经网络，包括U-Net、Attention U-Net和SegResNet，采用四种损失函数：Dice Loss、Dice Cross Entropy、Dice Focal Loss以及我们提出的L1加权Dice Focal Loss（L1DFL）。评估结果表明，Attention U-Net与L1DFL的结合与真实值之间的相关性最强（SUVmax和TLA的符合度相关性为0.90-0.99），而采用Dice Loss和其他两种复合损失的模型，特别是与SegResNet结合的模型，表现较差。等效性检验（TOST，α&#x3D;0.05，Δ&#x3D;20%）证实了SUV指标、病灶计数和TLA的高性能表现，其中L1DFL表现最佳。相比之下，肿瘤体积和病灶扩散表现出更大的变化性。Bland-Altman分析、覆盖概率和总偏差指数分析进一步强调了我们提出的L1DFL在量化真实临床指标方面的变化性最小化。相关代码公开在：<a target="_blank" rel="noopener" href="https://github.com/ObedDzik/pca_segment.git%E3%80%82">https://github.com/ObedDzik/pca_segment.git。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16237v1">PDF</a> 12 pages, 8 figures</p>
<p><strong>摘要</strong><br>     本研究对基于深度学习的分割方法提取的定量测量进行了全面评估，这些测量包括SUVmax、SUVmean、总病变活性（TLA）、肿瘤体积（TMTV）、病变计数和病变扩散等六个定量指标，并分析了380例前列腺特异性膜抗原（PSMA）靶向[18F]DCFPyL PET&#x2F;CT扫描的前列腺癌生化复发患者数据。研究结果显示，使用Attention U-Net与L1DFL结合的模型与真实值的相关性最强（SUVmax和TLA的符合度相关性为0.90-0.99），而采用Dice Loss和其他复合损失的模型，特别是与SegResNet结合的模型表现较差。等价测试表明SUV指标、病变计数和TLA性能较高，其中L1DFL表现最佳。相比之下，肿瘤体积和病变扩散的变异性较大。Bland-Altman分析、覆盖概率和总偏差指数分析进一步表明，所提出的L1DFL能最小化对真实临床指标的量化差异。相关代码已公开于：<a target="_blank" rel="noopener" href="https://github.com/ObedDzik/pca_segment.git%E3%80%82">https://github.com/ObedDzik/pca_segment.git。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>研究对基于深度学习的分割方法进行了全面的定量测量评估，涉及SUVmax、SUVmean等六个指标。</li>
<li>Attention U-Net与L1DFL结合模型在SUVmax和TLA指标上与真实值的符合度相关性最高。</li>
<li>L1DFL在SUV指标、病变计数和TLA的等价测试中表现最佳。</li>
<li>肿瘤体积和病变扩散的量化结果表现出较大的变异性。</li>
<li>公开的代码有助于其他研究者利用此研究的模型和数据进行进一步的分析和研究。</li>
<li>提出的L1DFL能最小化真实临床指标量化的差异，得到了Bland-Altman分析等的支持。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16237">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d2a2cfcbc6bb8da206c1c49a73dbf8cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3964a7eadbe4f0550b16c04c26a1d7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c4e6911c1b783137d0f40bcc68766a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ac63dc524f2128dc8455fb91cede65b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1fc713f9fbe03a17426653c5ba40a1d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Automatically-Detecting-Numerical-Instability-in-Machine-Learning-Applications-via-Soft-Assertions"><a href="#Automatically-Detecting-Numerical-Instability-in-Machine-Learning-Applications-via-Soft-Assertions" class="headerlink" title="Automatically Detecting Numerical Instability in Machine Learning   Applications via Soft Assertions"></a>Automatically Detecting Numerical Instability in Machine Learning   Applications via Soft Assertions</h2><p><strong>Authors:Shaila Sharmin, Anwar Hossain Zahid, Subhankar Bhattacharjee, Chiamaka Igwilo, Miryung Kim, Wei Le</strong></p>
<p>Machine learning (ML) applications have become an integral part of our lives. ML applications extensively use floating-point computation and involve very large&#x2F;small numbers; thus, maintaining the numerical stability of such complex computations remains an important challenge. Numerical bugs can lead to system crashes, incorrect output, and wasted computing resources. In this paper, we introduce a novel idea, namely soft assertions (SA), to encode safety&#x2F;error conditions for the places where numerical instability can occur. A soft assertion is an ML model automatically trained using the dataset obtained during unit testing of unstable functions. Given the values at the unstable function in an ML application, a soft assertion reports how to change these values in order to trigger the instability. We then use the output of soft assertions as signals to effectively mutate inputs to trigger numerical instability in ML applications. In the evaluation, we used the GRIST benchmark, a total of 79 programs, as well as 15 real-world ML applications from GitHub. We compared our tool with 5 state-of-the-art (SOTA) fuzzers. We found all the GRIST bugs and outperformed the baselines. We found 13 numerical bugs in real-world code, one of which had already been confirmed by the GitHub developers. While the baselines mostly found the bugs that report NaN and INF, our tool \tool found numerical bugs with incorrect output. We showed one case where the Tumor Detection Model, trained on Brain MRI images, should have predicted “tumor”, but instead, it incorrectly predicted “no tumor” due to the numerical bugs. Our replication package is located at <a target="_blank" rel="noopener" href="https://figshare.com/s/6528d21ccd28bea94c32">https://figshare.com/s/6528d21ccd28bea94c32</a>. </p>
<blockquote>
<p>机器学习（ML）应用已成为我们生活中不可或缺的一部分。机器学习应用广泛地使用浮点计算并涉及非常大的数字或非常小的数字；因此，保持此类复杂计算的数值稳定性仍然是一个重要的挑战。数值错误可能导致系统崩溃、输出错误和计算资源浪费。在本文中，我们介绍了一种新的概念，即软断言（SA），用于编码可能发生数值不稳定的地方的安全&#x2F;错误条件。软断言是一种使用在不稳定函数单元测试期间获得的数据集自动训练的机器学习模型。给定机器学习应用中不稳定函数的值，软断言会报告如何更改这些值以触发不稳定。然后，我们使用软断言的输出作为信号来有效地更改输入，以触发机器学习应用中的数值不稳定。在评估中，我们使用了包含总共79个程序的GRIST基准测试以及来自GitHub的15个真实世界的机器学习应用。我们将工具与五种最新（SOTA）模糊测试工具进行了比较。我们发现所有GRIST错误并超越了基线工具。我们在真实世界代码中发现了13个数值错误，其中一个是GitHub开发人员已经确认的。虽然基线工具大多发现了报告NaN和INF的错误，但我们的工具发现了具有错误输出的数值错误。我们展示了一个案例，即基于脑部MRI图像训练的肿瘤检测模型应该预测为“有肿瘤”，但由于数值错误，却错误地预测为“无肿瘤”。我们的复制包位于<a target="_blank" rel="noopener" href="https://figshare.com/s/6528d21ccd28bea94c32%E3%80%82">https://figshare.com/s/6528d21ccd28bea94c32。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15507v2">PDF</a> 22 pages, 5 figures. Accepted at FSE 2025</p>
<p><strong>摘要</strong><br>    本文引入了一种新的方法——软断言（SA），用于编码可能出现数值不稳定情况的安全&#x2F;错误条件。软断言是一种自动训练的机器学习模型，它使用在不稳定函数单元测试期间获得的数据集。给定机器学习应用程序中不稳定函数的值，软断言报告如何改变这些值以触发不稳定。然后，我们使用软断言的输出作为信号，有效地改变输入，以触发机器学习应用程序中的数值不稳定。评估中，我们使用了GRIST基准测试中的79个程序以及GitHub上的15个真实世界机器学习应用程序。与五个最先进（SOTA）的模糊测试工具相比，我们的工具发现了所有GRIST中的错误，并且表现优于基线工具。在真实世界的代码中发现了13个数值错误，其中一个是GitHub开发者已经确认的错误。与其他工具主要发现报告NaN和INF的错误不同，我们的工具还能发现输出错误的数值错误。我们展示了这样一个案例：基于脑部MRI图像训练的肿瘤检测模型本应预测为“肿瘤”，但由于数值错误而错误地预测为“无肿瘤”。</p>
<p><strong>关键要点</strong></p>
<ol>
<li>引入软断言（SA）概念，用于机器学习中的数值稳定性挑战。</li>
<li>软断言是一种自动训练的机器学习模型，用于识别和报告数值不稳定的情况。</li>
<li>通过软断言的输出作为信号来触发机器学习应用中的数值不稳定。</li>
<li>使用GRIST基准测试和真实世界机器学习应用进行评估。</li>
<li>与其他先进工具相比，该工具能够发现所有GRIST中的错误以及更多的真实世界代码中的数值错误。</li>
<li>该工具不仅能发现报告NaN和INF的错误，还能发现输出错误的数值错误。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15507">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c1f79c751bcad32d6d164b2dac5e483f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bfca7e08be3d065fde427f89b00f208.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Embedding-Radiomics-into-Vision-Transformers-for-Multimodal-Medical-Image-Classification"><a href="#Embedding-Radiomics-into-Vision-Transformers-for-Multimodal-Medical-Image-Classification" class="headerlink" title="Embedding Radiomics into Vision Transformers for Multimodal Medical   Image Classification"></a>Embedding Radiomics into Vision Transformers for Multimodal Medical   Image Classification</h2><p><strong>Authors:Zhenyu Yang, Haiming Zhu, Rihui Zhang, Haipeng Zhang, Jianliang Wang, Chunhao Wang, Minbin Chen, Fang-Fang Yin</strong></p>
<p>Background: Deep learning has significantly advanced medical image analysis, with Vision Transformers (ViTs) offering a powerful alternative to convolutional models by modeling long-range dependencies through self-attention. However, ViTs are inherently data-intensive and lack domain-specific inductive biases, limiting their applicability in medical imaging. In contrast, radiomics provides interpretable, handcrafted descriptors of tissue heterogeneity but suffers from limited scalability and integration into end-to-end learning frameworks. In this work, we propose the Radiomics-Embedded Vision Transformer (RE-ViT) that combines radiomic features with data-driven visual embeddings within a ViT backbone.   Purpose: To develop a hybrid RE-ViT framework that integrates radiomics and patch-wise ViT embeddings through early fusion, enhancing robustness and performance in medical image classification.   Methods: Following the standard ViT pipeline, images were divided into patches. For each patch, handcrafted radiomic features were extracted and fused with linearly projected pixel embeddings. The fused representations were normalized, positionally encoded, and passed to the ViT encoder. A learnable [CLS] token aggregated patch-level information for classification. We evaluated RE-ViT on three public datasets (including BUSI, ChestXray2017, and Retinal OCT) using accuracy, macro AUC, sensitivity, and specificity. RE-ViT was benchmarked against CNN-based (VGG-16, ResNet) and hybrid (TransMed) models.   Results: RE-ViT achieved state-of-the-art results: on BUSI, AUC&#x3D;0.950+&#x2F;-0.011; on ChestXray2017, AUC&#x3D;0.989+&#x2F;-0.004; on Retinal OCT, AUC&#x3D;0.986+&#x2F;-0.001, which outperforms other comparison models.   Conclusions: The RE-ViT framework effectively integrates radiomics with ViT architectures, demonstrating improved performance and generalizability across multimodal medical image classification tasks. </p>
<blockquote>
<p>背景：深度学习在医学图像分析方面取得了重大进展，而Vision Transformers（ViTs）通过自注意力建模长距离依赖关系，为卷积模型提供了一种强大的替代方案。然而，ViTs本质上需要大量的数据，并且缺乏针对特定领域的归纳偏见，这在医学成像中限制了其适用性。相比之下，放射组学提供了组织异质性的可解释、手工描述器，但受限于可扩展性和集成到端到端学习框架的能力。在这项工作中，我们提出了Radiomics-Embedded Vision Transformer（RE-ViT），它将放射组学特征与数据驱动的视觉嵌入相结合，在一个ViT主干中。目的：开发一个混合RE-ViT框架，通过早期融合集成放射组学和补丁式ViT嵌入，提高在医学图像分类中的稳健性和性能。方法：遵循标准的ViT管道，将图像分成补丁。对于每个补丁，提取手工制作的放射组学特征，并与线性投影的像素嵌入相融合。融合后的表示经过归一化、位置编码，然后传递给ViT编码器。一个可学习的[CLS]标记聚合了补丁级别的信息用于分类。我们在三个公共数据集（包括BUSI、ChestXray2017和Retinal OCT）上评估了RE-ViT，使用了准确率、宏AUC、灵敏度和特异性。RE-ViT与基于CNN（VGG-16、ResNet）和混合（TransMed）模型进行了对比评估。结果：RE-ViT达到了最新的结果：在BUSI上，AUC&#x3D;0.950±0.011；在ChestXray2017上，AUC&#x3D;0.989±0.004；在视网膜OCT上，AUC&#x3D;0.986±0.001，优于其他对比模型。结论：RE-ViT框架有效地将放射组学与ViT架构相结合，展示了在多模态医学图像分类任务中的改进性能和泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10916v2">PDF</a> 27 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出一种融合放射组学与Vision Transformer（ViT）的混合框架RE-ViT，通过早期融合放射组学特征和ViT数据驱动视觉嵌入，提高了在医学图像分类中的稳健性和性能。在多个公开数据集上的评估表明，RE-ViT取得了先进的结果，证明其在多模态医学图像分类任务中的性能和泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RE-ViT结合了放射组学特征和Vision Transformer（ViT）数据驱动视觉嵌入，提供了一个新的医学图像分析框架。</li>
<li>通过早期融合放射组学特征和ViT嵌入，RE-ViT增强了医学图像分类的稳健性和性能。</li>
<li>在多个公开数据集上的评估结果显示，RE-ViT达到了先进性能，证明了其有效性和泛化能力。</li>
<li>RE-ViT框架通过融合放射组学特征与ViT架构，提高了医学图像分类的性能。</li>
<li>该方法实现了对多模态医学图像分类任务的优秀表现。</li>
<li>RE-ViT相对于传统的CNN模型和混合模型表现出了优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10916">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6d0a5afd995995fc6811059576260981.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Solving-Inverse-Problems-in-Protein-Space-Using-Diffusion-Based-Priors"><a href="#Solving-Inverse-Problems-in-Protein-Space-Using-Diffusion-Based-Priors" class="headerlink" title="Solving Inverse Problems in Protein Space Using Diffusion-Based Priors"></a>Solving Inverse Problems in Protein Space Using Diffusion-Based Priors</h2><p><strong>Authors:Axel Levy, Eric R. Chan, Sara Fridovich-Keil, Frédéric Poitevin, Ellen D. Zhong, Gordon Wetzstein</strong></p>
<p>The interaction of a protein with its environment can be understood and controlled via its 3D structure. Experimental methods for protein structure determination, such as X-ray crystallography or cryogenic electron microscopy, shed light on biological processes but introduce challenging inverse problems. Learning-based approaches have emerged as accurate and efficient methods to solve these inverse problems for 3D structure determination, but are specialized for a predefined type of measurement. Here, we introduce a versatile framework to turn biophysical measurements, such as cryo-EM density maps, into 3D atomic models. Our method combines a physics-based forward model of the measurement process with a pretrained generative model providing a task-agnostic, data-driven prior. Our method outperforms posterior sampling baselines on linear and non-linear inverse problems. In particular, it is the first diffusion-based method for refining atomic models from cryo-EM maps and building atomic models from sparse distance matrices. </p>
<blockquote>
<p>蛋白质与其环境的相互作用可以通过其三维结构来理解并控制。用于确定蛋白质结构的实验方法，如X射线晶体学或冷冻电子显微镜，为理解生物过程提供了启示，但同时也带来了具有挑战性的反问题。基于学习的方法已经作为准确高效的方法来解决这些反问题以进行三维结构测定，但这些方法仅限于特定类型的测量。在这里，我们介绍了一个通用框架，可将生物物理测量（如冷冻电子显微镜密度图）转化为三维原子模型。我们的方法结合了基于物理的测量过程前向模型与预训练的生成模型，该生成模型提供了任务无关的数据驱动先验。我们的方法在解决线性和非线性反问题上优于后采样基线。尤其是，它是第一个基于扩散的方法，用于从冷冻电子显微镜图谱中优化原子模型并从稀疏距离矩阵构建原子模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.04239v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>蛋白质与其环境之间的相互作用可通过其三维结构进行理解和控制。实验方法如X射线晶体学或冷冻电子显微镜为生物过程提供了见解，但带来了逆向问题的挑战。基于学习的方法已准确高效地解决了这些逆向问题以实现三维结构测定，但仅限于特定类型的测量。本文介绍了一个通用框架，可将生物物理测量（如冷冻电子显微镜密度图）转化为三维原子模型。该方法结合了测量过程的物理前向模型与预训练的生成模型，提供了一项任务无关的数据驱动先验。该方法在线性和非线性逆向问题上优于后采样基线，尤其是第一个基于扩散的方法，可优化冷冻电子显微镜地图的原子模型并从稀疏距离矩阵构建原子模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>蛋白质与其环境互动可通过其三维结构理解并控制。</li>
<li>实验方法如X射线晶体学和冷冻电子显微镜为生物过程研究提供了工具，但存在逆向问题的挑战。</li>
<li>基于学习的方法已解决逆向问题用于三维结构测定，但局限于特定类型测量。</li>
<li>介绍了一个通用框架来转化生物物理测量到三维原子模型。</li>
<li>该方法结合了物理前向模型和预训练的生成模型（任务无关的数据驱动先验）。</li>
<li>此方法在多种逆向问题上表现出优越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.04239">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f4e5f13e44d9e6edf66fc5f3a01df68a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd86b05bddbcab767e11b36e32ad0a96.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Effective-Lymph-Nodes-Detection-in-CT-Scans-Using-Location-Debiased-Query-Selection-and-Contrastive-Query-Representation-in-Transformer"><a href="#Effective-Lymph-Nodes-Detection-in-CT-Scans-Using-Location-Debiased-Query-Selection-and-Contrastive-Query-Representation-in-Transformer" class="headerlink" title="Effective Lymph Nodes Detection in CT Scans Using Location Debiased   Query Selection and Contrastive Query Representation in Transformer"></a>Effective Lymph Nodes Detection in CT Scans Using Location Debiased   Query Selection and Contrastive Query Representation in Transformer</h2><p><strong>Authors:Yirui Wang, Qinji Yu, Ke Yan, Haoshen Li, Dazhou Guo, Li Zhang, Le Lu, Na Shen, Qifeng Wang, Xiaowei Ding, Xianghua Ye, Dakai Jin</strong></p>
<p>Lymph node (LN) assessment is a critical, indispensable yet very challenging task in the routine clinical workflow of radiology and oncology. Accurate LN analysis is essential for cancer diagnosis, staging, and treatment planning. Finding scatteredly distributed, low-contrast clinically relevant LNs in 3D CT is difficult even for experienced physicians under high inter-observer variations. Previous automatic LN detection works typically yield limited recall and high false positives (FPs) due to adjacent anatomies with similar image intensities, shapes, or textures (vessels, muscles, esophagus, etc). In this work, we propose a new LN DEtection TRansformer, named LN-DETR, to achieve more accurate performance. By enhancing the 2D backbone with a multi-scale 2.5D feature fusion to incorporate 3D context explicitly, more importantly, we make two main contributions to improve the representation quality of LN queries. 1) Considering that LN boundaries are often unclear, an IoU prediction head and a location debiased query selection are proposed to select LN queries of higher localization accuracy as the decoder query’s initialization. 2) To reduce FPs, query contrastive learning is employed to explicitly reinforce LN queries towards their best-matched ground-truth queries over unmatched query predictions. Trained and tested on 3D CT scans of 1067 patients (with 10,000+ labeled LNs) via combining seven LN datasets from different body parts (neck, chest, and abdomen) and pathologies&#x2F;cancers, our method significantly improves the performance of previous leading methods by &gt; 4-5% average recall at the same FP rates in both internal and external testing. We further evaluate on the universal lesion detection task using NIH DeepLesion benchmark, and our method achieves the top performance of 88.46% averaged recall across 0.5 to 4 FPs per image, compared with other leading reported results. </p>
<blockquote>
<p>淋巴结（LN）评估是放射学和肿瘤学常规临床工作中不可或缺且极具挑战性的任务。准确的淋巴结分析对于癌症的诊断、分期和治疗计划至关重要。即使在经验丰富的医生之间也存在较高的观察者间变异，在3D计算机断层扫描（CT）中寻找分散、低对比度的临床相关淋巴结仍然具有挑战性。之前的自动淋巴结检测工作通常由于相邻结构具有相似的图像强度、形状或纹理（如血管、肌肉、食管等）而召回率有限，误报率高。在这项工作中，我们提出了一种新的淋巴结检测转换器，名为LN-DETR，以实现更准确的性能。我们通过增强2D主干网络，采用多尺度2.5D特征融合来显式地融入3D上下文，更重要的是，我们在提高淋巴结查询的表示质量方面做出了两个主要贡献。1）考虑到淋巴结边界通常不清晰，我们提出了一个IoU预测头和位置偏差查询选择，以选择具有较高定位精度的淋巴结查询作为解码器查询的初始化。2）为了减少误报，采用查询对比学习，明确加强淋巴结查询与其最佳匹配的真实查询，超过未匹配的查询预测。我们的方法在1067名患者的3D CT扫描图像上进行训练和测试（包含10,000多个标记淋巴结），结合了来自不同部位（颈部、胸部和腹部）和病理&#x2F;癌症的七个淋巴结数据集，在内部和外部测试中，我们的方法在同一误报率下将之前领先方法平均召回率提高了4-5%。我们进一步在NIH DeepLesion基准测试上进行通用病变检测任务评估，与其他报告的最佳结果相比，我们的方法在平均每张图像0.5到4个假阳性的情况下达到了88.46%的召回率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.03819v2">PDF</a> Accepted by ECCV24</p>
<p><strong>Summary</strong></p>
<p>本论文提出了一种名为LN-DETR的新方法，用于更精确地检测淋巴节点（LNs）。此方法采用多尺度混合特征的融合策略，通过融入3D上下文信息以增强二维主干特征的表达效果。为提高定位精度并降低误报率，论文提出了IoU预测头与位置偏差查询选择机制。此外，通过查询对比学习强化匹配度，减少了误报率。实验结果表明，该方法在多个淋巴结数据集上的性能显著优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>淋巴节点检测在临床诊断和治疗计划中至关重要且极具挑战性。</li>
<li>LN-DETR方法通过融合多尺度特征和多维度上下文信息提高了检测准确性。</li>
<li>采用IoU预测头与位置偏差查询选择机制提高了定位精度。</li>
<li>查询对比学习技术用于强化匹配度并降低误报率。</li>
<li>在多个淋巴结数据集上的实验结果显示，该方法显著提高了检测性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.03819">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5db1570717a6b907a4ca12a9af051b4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c9de5004dfe536ab852148217041afa.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-25/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-25/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-25/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-678dae6a5a73578c56cc62201304accf.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-04-25  Planning with Diffusion Models for Target-Oriented Dialogue Systems
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-25/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f096359489037ae35bd135190185b633.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-04-25  VideoMark A Distortion-Free Robust Watermarking Framework for Video   Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28315.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
