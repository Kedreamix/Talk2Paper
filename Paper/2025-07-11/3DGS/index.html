<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2025-07-11  Photometric Stereo using Gaussian Splatting and inverse rendering">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-8bd004b5f78674ff83934463707a8df0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    40 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-11-更新"><a href="#2025-07-11-更新" class="headerlink" title="2025-07-11 更新"></a>2025-07-11 更新</h1><h2 id="Photometric-Stereo-using-Gaussian-Splatting-and-inverse-rendering"><a href="#Photometric-Stereo-using-Gaussian-Splatting-and-inverse-rendering" class="headerlink" title="Photometric Stereo using Gaussian Splatting and inverse rendering"></a>Photometric Stereo using Gaussian Splatting and inverse rendering</h2><p><strong>Authors:Matéo Ducastel, David Tschumperlé, Yvain Quéau</strong></p>
<p>Recent state-of-the-art algorithms in photometric stereo rely on neural networks and operate either through prior learning or inverse rendering optimization. Here, we revisit the problem of calibrated photometric stereo by leveraging recent advances in 3D inverse rendering using the Gaussian Splatting formalism. This allows us to parameterize the 3D scene to be reconstructed and optimize it in a more interpretable manner. Our approach incorporates a simplified model for light representation and demonstrates the potential of the Gaussian Splatting rendering engine for the photometric stereo problem. </p>
<blockquote>
<p>近期最先进的光度立体算法依赖于神经网络，并通过先验学习或逆渲染优化进行操作。在这里，我们借助最近基于高斯点绘技术的三维逆渲染技术进展重新考虑校准光度立体问题。这使我们能够以更具解释性的方式对重建的三维场景进行参数化并对其进行优化。我们的方法采用了简化的光照表示模型，展示了高斯点绘渲染引擎在光度立体问题上的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06684v1">PDF</a> in French language. GRETSI 2025, Association GRETSI, Aug 2025,   Strasbourg, France</p>
<p><strong>Summary</strong></p>
<p>最新先进的光度立体视觉算法依赖于神经网络，通过预先学习或逆向渲染优化来操作。本研究利用最新的三维逆向渲染高斯平铺形式的进展，重新审视了校准光度立体视觉问题。通过参数化待重建的三维场景，能以更易理解的方式进行优化。研究采用简化的光照表示模型，展示了高斯平铺渲染引擎在光度立体视觉问题中的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文利用高斯平铺形式的最新三维逆向渲染技术来改进光度立体视觉问题。</li>
<li>该方法采用参数化方式重建三维场景，使优化过程更为直观。</li>
<li>通过简化光照表示模型，提升了算法的效率和准确性。</li>
<li>论文展示了高斯平铺渲染引擎在光度立体视觉中的潜力。</li>
<li>该方法主要依赖神经网络来解决问题，使用预先学习和逆向渲染优化两种方式操作。</li>
<li>该方法可能提高场景重建的精度和可靠性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06684">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2741544dc3513e7005d71d200f313d91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b701594318f3775d42e1c97f80ecc9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43e1f7439e79fbd702b950138bea0951.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fb847427d412a8de0e0bffd0f6f721f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bd004b5f78674ff83934463707a8df0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FlexGaussian-Flexible-and-Cost-Effective-Training-Free-Compression-for-3D-Gaussian-Splatting"><a href="#FlexGaussian-Flexible-and-Cost-Effective-Training-Free-Compression-for-3D-Gaussian-Splatting" class="headerlink" title="FlexGaussian: Flexible and Cost-Effective Training-Free Compression for   3D Gaussian Splatting"></a>FlexGaussian: Flexible and Cost-Effective Training-Free Compression for   3D Gaussian Splatting</h2><p><strong>Authors:Boyuan Tian, Qizhe Gao, Siran Xianyu, Xiaotong Cui, Minjia Zhang</strong></p>
<p>3D Gaussian splatting has become a prominent technique for representing and rendering complex 3D scenes, due to its high fidelity and speed advantages. However, the growing demand for large-scale models calls for effective compression to reduce memory and computation costs, especially on mobile and edge devices with limited resources. Existing compression methods effectively reduce 3D Gaussian parameters but often require extensive retraining or fine-tuning, lacking flexibility under varying compression constraints.   In this paper, we introduce FlexGaussian, a flexible and cost-effective method that combines mixed-precision quantization with attribute-discriminative pruning for training-free 3D Gaussian compression. FlexGaussian eliminates the need for retraining and adapts easily to diverse compression targets. Evaluation results show that FlexGaussian achieves up to 96.4% compression while maintaining high rendering quality (&lt;1 dB drop in PSNR), and is deployable on mobile devices. FlexGaussian delivers high compression ratios within seconds, being 1.7-2.1x faster than state-of-the-art training-free methods and 10-100x faster than training-involved approaches. The code is being prepared and will be released soon at: <a target="_blank" rel="noopener" href="https://github.com/Supercomputing-System-AI-Lab/FlexGaussian">https://github.com/Supercomputing-System-AI-Lab/FlexGaussian</a> </p>
<blockquote>
<p>3D高斯贴图技术因其高保真和速度优势而成为表示和呈现复杂3D场景的重要技术。然而，对大规模模型日益增长的需求要求对内存和计算成本进行有效压缩，尤其是在资源和计算能力有限的移动和边缘设备上。现有压缩方法虽然可以有效地减少3D高斯参数，但通常需要大量的重新训练或微调，在不同的压缩约束下缺乏灵活性。在本文中，我们介绍了FlexGaussian，这是一种灵活且经济高效的压缩方法，结合了混合精度量化和属性判别修剪，无需训练即可进行3D高斯压缩。FlexGaussian无需重新训练，可轻松适应不同的压缩目标。评估结果表明，FlexGaussian在保持高渲染质量的同时（峰值信噪比下降小于1dB），实现了高达96.4%的压缩率，并可在移动设备上部署。FlexGaussian可在几秒内实现高压缩比，比最新的无训练方法快1.7-2.1倍，比涉及训练的方法快10-100倍。代码正在准备中，很快将在<a target="_blank" rel="noopener" href="https://github.com/Supercomputing-System-AI-Lab/FlexGaussian">https://github.com/Supercomputing-System-AI-Lab/FlexGaussian</a>发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06671v1">PDF</a> To appear at ACM MM 2025</p>
<p><strong>Summary</strong></p>
<p>3D高斯贴片技术因其高保真和速度优势而成为表示和呈现复杂3D场景的重要技术。随着对大规模模型的需求增长，需要在不损失性能的情况下有效压缩模型，特别是在资源有限的移动和边缘设备上。本文介绍了一种名为FlexGaussian的灵活且经济高效的方法，它结合了混合精度量化和属性判别剪枝，实现了无需训练的3D高斯压缩。FlexGaussian无需重新训练，可轻松适应不同的压缩目标，在保持高质量渲染的同时实现了高达96.4%的压缩率。此外，FlexGaussian部署在移动设备上的速度也很快。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D高斯贴片技术因高保真和速度而受到青睐。</li>
<li>大规模模型需要有效的压缩方法以降低内存和计算成本。</li>
<li>FlexGaussian结合了混合精度量化和属性判别剪枝，实现了无需训练的3D高斯压缩。</li>
<li>FlexGaussian无需重新训练，可适应不同的压缩目标。</li>
<li>FlexGaussian实现了高达96.4%的压缩率，同时保持高质量的渲染。</li>
<li>FlexGaussian比现有的无训练压缩方法和涉及训练的方法更快。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06671">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-2bfccdf185f364be06511d3e925cfda8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-494b79e5142487047fb2b74bfba1c709.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c34d69acc7383bffbcd31ef887664b5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a5b425492b741e882579862a80bda3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-308011bbb3d6035b1463a2521d3d1b13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ebfa0eec321bfdb01ce5d03b25c4114e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ClipGS-Clippable-Gaussian-Splatting-for-Interactive-Cinematic-Visualization-of-Volumetric-Medical-Data"><a href="#ClipGS-Clippable-Gaussian-Splatting-for-Interactive-Cinematic-Visualization-of-Volumetric-Medical-Data" class="headerlink" title="ClipGS: Clippable Gaussian Splatting for Interactive Cinematic   Visualization of Volumetric Medical Data"></a>ClipGS: Clippable Gaussian Splatting for Interactive Cinematic   Visualization of Volumetric Medical Data</h2><p><strong>Authors:Chengkun Li, Yuqi Tong, Kai Chen, Zhenya Yang, Ruiyang Li, Shi Qiu, Jason Ying-Kuen Chan, Pheng-Ann Heng, Qi Dou</strong></p>
<p>The visualization of volumetric medical data is crucial for enhancing diagnostic accuracy and improving surgical planning and education. Cinematic rendering techniques significantly enrich this process by providing high-quality visualizations that convey intricate anatomical details, thereby facilitating better understanding and decision-making in medical contexts. However, the high computing cost and low rendering speed limit the requirement of interactive visualization in practical applications. In this paper, we introduce ClipGS, an innovative Gaussian splatting framework with the clipping plane supported, for interactive cinematic visualization of volumetric medical data. To address the challenges posed by dynamic interactions, we propose a learnable truncation scheme that automatically adjusts the visibility of Gaussian primitives in response to the clipping plane. Besides, we also design an adaptive adjustment model to dynamically adjust the deformation of Gaussians and refine the rendering performance. We validate our method on five volumetric medical data (including CT and anatomical slice data), and reach an average 36.635 PSNR rendering quality with 156 FPS and 16.1 MB model size, outperforming state-of-the-art methods in rendering quality and efficiency. </p>
<blockquote>
<p>医学体积数据的可视化对于提高诊断准确性、改善手术规划和教育具有重要意义。电影渲染技术通过提供高质量的可视化效果，展现复杂的解剖细节，从而丰富这一过程，促进在医学环境中的理解和决策。然而，高昂的计算成本和较低的渲染速度限制了其在实际应用中的交互式可视化需求。在本文中，我们介绍了ClipGS，这是一种支持裁剪平面的创新高斯贴图框架，用于医学体积数据的交互式电影可视化。为了解决动态交互所带来的挑战，我们提出了一种可学习的截断方案，该方案可以自动根据裁剪平面调整高斯原始数据的可见性。此外，我们还设计了一个自适应调整模型，以动态调整高斯值的变形并优化渲染性能。我们在五种医学体积数据（包括CT和解剖切片数据）上验证了我们的方法，达到了平均36.635的峰值信噪比（PSNR）渲染质量，帧率为每秒156帧，模型大小为16.1MB，在渲染质量和效率方面均优于现有先进技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06647v1">PDF</a> Early accepted by MICCAI 2025. Project is available at:   <a target="_blank" rel="noopener" href="https://med-air.github.io/ClipGS">https://med-air.github.io/ClipGS</a></p>
<p><strong>Summary</strong></p>
<p>体积医疗数据的可视化对于提高诊断准确性、改善手术规划和教育至关重要。电影渲染技术通过提供高质量的可视化，展示复杂的解剖细节，丰富了这一过程。然而，高昂的计算成本和较低的渲染速度限制了其在实践中的互动可视化需求。本文介绍了一种创新的Gaussian splatting框架ClipGS，支持裁剪平面，用于体积医疗数据的交互式电影渲染。为解决动态交互带来的挑战，我们提出了一种可学习的截断方案，该方案可自动根据裁剪平面调整高斯原始数据的可见性。此外，我们还设计了一个自适应调整模型，以动态调整高斯变形并优化渲染性能。在五个体积医疗数据（包括CT和解剖切片数据）上的验证结果表明，我们的方法在渲染质量和效率方面均优于现有先进技术，平均PSNR渲染质量为36.635，帧速率为156 FPS，模型大小为16.1 MB。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>体积医疗数据可视化对于诊断、手术规划和医学教育至关重要。</li>
<li>电影渲染技术能够提供高质量的医疗数据可视化，展示解剖细节。</li>
<li>现有可视化方法在计算成本和渲染速度上存在限制。</li>
<li>本文介绍了一种新的Gaussian splatting框架ClipGS，支持裁剪平面，用于体积医疗数据的交互式电影渲染。</li>
<li>ClipGS通过可学习的截断方案和自适应调整模型，提高了渲染性能和视觉效果。</li>
<li>在多个体积医疗数据上的验证结果表明ClipGS在渲染质量和效率上均表现优异。</li>
<li>ClipGS有望改善医疗领域的诊断、治疗和教育工作。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06647">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-282bfce4c5ac0dac28640b251dd10122.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0a84b11284d88fd4e3efe2f8b4f154d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Probabilistic-Approach-to-Uncertainty-Quantification-Leveraging-3D-Geometry"><a href="#A-Probabilistic-Approach-to-Uncertainty-Quantification-Leveraging-3D-Geometry" class="headerlink" title="A Probabilistic Approach to Uncertainty Quantification Leveraging 3D   Geometry"></a>A Probabilistic Approach to Uncertainty Quantification Leveraging 3D   Geometry</h2><p><strong>Authors:Rushil Desai, Frederik Warburg, Trevor Darrell, Marissa Ramirez de Chanlatte</strong></p>
<p>Quantifying uncertainty in neural implicit 3D representations, particularly those utilizing Signed Distance Functions (SDFs), remains a substantial challenge due to computational inefficiencies, scalability issues, and geometric inconsistencies. Existing methods typically neglect direct geometric integration, leading to poorly calibrated uncertainty maps. We introduce BayesSDF, a novel probabilistic framework for uncertainty quantification in neural implicit SDF models, motivated by scientific simulation applications with 3D environments (e.g., forests) such as modeling fluid flow through forests, where precise surface geometry and awareness of fidelity surface geometric uncertainty are essential. Unlike radiance-based models such as NeRF or 3D Gaussian splatting, which lack explicit surface formulations, SDFs define continuous and differentiable geometry, making them better suited for physical modeling and analysis. BayesSDF leverages a Laplace approximation to quantify local surface instability via Hessian-based metrics, enabling computationally efficient, surface-aware uncertainty estimation. Our method shows that uncertainty predictions correspond closely with poorly reconstructed geometry, providing actionable confidence measures for downstream use. Extensive evaluations on synthetic and real-world datasets demonstrate that BayesSDF outperforms existing methods in both calibration and geometric consistency, establishing a strong foundation for uncertainty-aware 3D scene reconstruction, simulation, and robotic decision-making. </p>
<blockquote>
<p>在神经隐式三维表示中量化不确定性，特别是那些使用带符号距离函数（SDFs）的表示，仍然是一个巨大的挑战，主要是由于计算效率低下、可扩展性问题以及几何不一致性。现有方法通常忽略直接几何集成，导致校准不良的不确定性地图。我们引入了BayesSDF，这是一种用于神经隐式SDF模型中的不确定性量化的新型概率框架，其灵感来源于具有三维环境（例如森林）的科学模拟应用程序，例如模拟森林中的水流，其中精确的曲面几何和对保真度曲面几何不确定性的认识是至关重要的。不同于基于辐射的模型（如NeRF或三维高斯平铺），缺乏明确的表面公式，SDF定义了连续且可微分的几何形状，使其更适合物理建模和分析。BayesSDF利用拉普拉斯近似来通过基于Hessian的指标量化局部表面不稳定性，从而实现计算效率高且具备表面感知的不确定性估计。我们的方法显示出不确定性预测与重建不良的几何形状紧密相关，为下游使用提供了可操作的可信措施。在合成和真实世界数据集上的广泛评估表明，在标定和几何一致性方面，BayesSDF优于现有方法，为具有不确定性的三维场景重建、模拟和机器人决策制定建立了坚实的基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06269v1">PDF</a> ICCV 2025 Workshops (8 Pages, 6 Figures, 2 Tables)</p>
<p><strong>Summary</strong></p>
<p>神经网络隐式三维表示中的不确定性量化，特别是使用符号距离函数（SDFs）的方法，面临计算效率低下、可扩展性差和几何不一致等挑战。现有方法通常忽视直接几何集成，导致不确定性映射校准不良。本文介绍了一种基于贝叶斯的新概率框架BayesSDF，用于神经隐式SDF模型的不确定性量化，其动机来源于需要精确表面几何和表面几何不确定性的三维环境模拟应用（如森林模型）。BayesSDF利用Laplace近似法通过Hessian度量计算局部表面不稳定度，实现了计算高效、具有表面感知的不确定性估计。该方法显示不确定性预测与重建不良的几何结构紧密对应，为下游应用提供了可操作的可信度量。在合成和真实数据集上的广泛评估表明，BayesSDF在校准和几何一致性方面优于现有方法，为不确定性感知的3D场景重建、模拟和机器人决策制定奠定了坚实基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经网络隐式三维表示中的不确定性量化存在挑战，包括计算效率低下、可扩展性差和几何不一致。</li>
<li>现有方法忽视直接几何集成，导致不确定性映射校准不良。</li>
<li>BayesSDF是一种新型概率框架，用于神经隐式SDF模型的不确定性量化。</li>
<li>BayesSDF适用于需要精确表面几何和表面几何不确定性的三维环境模拟应用。</li>
<li>BayesSDF利用Laplace近似法通过Hessian度量计算局部表面不稳定度，实现高效、具有表面感知的不确定性估计。</li>
<li>BayesSDF能对应出预测结果与重建不良的几何结构之间的关系。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06269">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fe8184c9272a63f5194938db703da1f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-771fe127879ac8408dcaf77e676851dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0ddb395f8cdf54faa9e45af92c043fc.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Gaussian-LIC2-LiDAR-Inertial-Camera-Gaussian-Splatting-SLAM"><a href="#Gaussian-LIC2-LiDAR-Inertial-Camera-Gaussian-Splatting-SLAM" class="headerlink" title="Gaussian-LIC2: LiDAR-Inertial-Camera Gaussian Splatting SLAM"></a>Gaussian-LIC2: LiDAR-Inertial-Camera Gaussian Splatting SLAM</h2><p><strong>Authors:Xiaolei Lang, Jiajun Lv, Kai Tang, Laijian Li, Jianxin Huang, Lina Liu, Yong Liu, Xingxing Zuo</strong></p>
<p>This paper presents the first photo-realistic LiDAR-Inertial-Camera Gaussian Splatting SLAM system that simultaneously addresses visual quality, geometric accuracy, and real-time performance. The proposed method performs robust and accurate pose estimation within a continuous-time trajectory optimization framework, while incrementally reconstructing a 3D Gaussian map using camera and LiDAR data, all in real time. The resulting map enables high-quality, real-time novel view rendering of both RGB images and depth maps. To effectively address under-reconstruction in regions not covered by the LiDAR, we employ a lightweight zero-shot depth model that synergistically combines RGB appearance cues with sparse LiDAR measurements to generate dense depth maps. The depth completion enables reliable Gaussian initialization in LiDAR-blind areas, significantly improving system applicability for sparse LiDAR sensors. To enhance geometric accuracy, we use sparse but precise LiDAR depths to supervise Gaussian map optimization and accelerate it with carefully designed CUDA-accelerated strategies. Furthermore, we explore how the incrementally reconstructed Gaussian map can improve the robustness of odometry. By tightly incorporating photometric constraints from the Gaussian map into the continuous-time factor graph optimization, we demonstrate improved pose estimation under LiDAR degradation scenarios. We also showcase downstream applications via extending our elaborate system, including video frame interpolation and fast 3D mesh extraction. To support rigorous evaluation, we construct a dedicated LiDAR-Inertial-Camera dataset featuring ground-truth poses, depth maps, and extrapolated trajectories for assessing out-of-sequence novel view synthesis. Both the dataset and code will be made publicly available on project page <a target="_blank" rel="noopener" href="https://xingxingzuo.github.io/gaussian_lic2">https://xingxingzuo.github.io/gaussian_lic2</a>. </p>
<blockquote>
<p>本文介绍了首个逼真的激光雷达惯性相机高斯混合SLAM系统，该系统同时解决了视觉质量、几何精度和实时性能的问题。所提出的方法在连续时间轨迹优化框架内进行稳健而准确的姿态估计，同时实时地利用相机和激光雷达数据逐步重建一个三维高斯地图。所得的地图能够实现高质量、实时的RGB图像和深度图的新型视图渲染。为了有效解决激光雷达未覆盖区域的重建不足问题，我们采用了一种轻量级的零射击深度模型，该模型协同结合了RGB外观线索和稀疏的激光雷达测量数据，以生成密集的深度图。深度补全使得在激光雷达盲区实现可靠的高斯初始化，大大提高了系统对于稀疏激光雷达传感器的适用性。为了提高几何精度，我们使用稀疏但精确的激光雷达深度来监督高斯地图的优化，并借助精心设计的CUDA加速策略来加速优化过程。此外，我们还探讨了逐步重建的高斯地图如何增强里程计的稳健性。通过将高斯地图的光度约束紧密地融入连续时间因子图优化中，我们在激光雷达退化场景下展示了改进的姿态估计。我们还通过扩展我们精致的系统来展示下游应用，包括视频帧插值和快速三维网格提取。为了支持严格的评估，我们构建了一个专用的激光雷达惯性相机数据集，其中包含用于评估不按顺序的新型视图合成的真实姿态、深度图和推断轨迹。数据集和代码都将在项目页面上进行公开，网址为：<a target="_blank" rel="noopener" href="https://xingxingzuo.github.io/gaussian_lic2%E3%80%82">https://xingxingzuo.github.io/gaussian_lic2。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04004v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文提出了首个逼真的激光雷达惯性相机高斯模糊SLAM系统，该系统同时解决了视觉质量、几何精度和实时性能的问题。该方法在连续时间轨迹优化框架内进行稳健和准确的姿态估计，同时增量构建三维高斯地图，使用相机和激光雷达数据，均可在实时环境下完成。所得地图可实现高质量实时渲染RGB图像和深度图。为解决激光雷达未覆盖区域的重建不足问题，我们采用轻量级零射深度模型，结合RGB外观线索和稀疏激光雷达测量值生成密集深度图。深度完成功能可在激光雷达盲区实现可靠的高斯初始化，显著提高系统对稀疏激光雷达传感器的适用性。为提高几何精度，我们使用稀疏但精确的激光雷达深度值监督高斯地图优化，并采用精心设计CUDA加速策略进行加速。此外，我们探索了增量重建高斯地图如何提升里程计的稳健性。通过紧密地将高斯地图的光度约束纳入连续时间因子图优化，我们在激光雷达退化场景下展示了改进的姿态估计。我们还通过扩展我们精致的系统展示了下游应用，包括视频帧插值和快速三维网格提取。为支持严格评估，我们构建了一个专用的激光雷达惯性相机数据集，包含用于评估序列外新视图合成的真实姿态、深度图和轨迹外推。数据集和代码均将在项目页面公开，网址为<a target="_blank" rel="noopener" href="https://xingxingzuo.github.io/gaussian_lic2%E3%80%82">https://xingxingzuo.github.io/gaussian_lic2。</a></p>
<p><strong>要点</strong></p>
<ol>
<li>提出了首个结合视觉质量、几何精度和实时性能的光谱级激光雷达惯性相机高斯模糊SLAM系统。</li>
<li>在连续时间轨迹优化框架内进行稳健和准确的姿态估计，并增量构建三维高斯地图。</li>
<li>通过结合RGB外观线索和稀疏激光雷达测量值生成密集深度图，解决了激光雷达未覆盖区域的重建问题。</li>
<li>利用稀疏但精确的激光雷达深度值监督高斯地图优化，并加速优化过程。</li>
<li>高斯地图的引入提高了里程计的稳健性，尤其在激光雷达退化场景下。</li>
<li>系统下游应用包括视频帧插值和快速三维网格提取等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04004">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8bfb626bbaa7aa9c2b7e0cd3de84668a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c6765e8c59e8c029fd75d69794afaac.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Hier-SLAM-Neuro-Symbolic-Semantic-SLAM-with-a-Hierarchically-Categorical-Gaussian-Splatting"><a href="#Hier-SLAM-Neuro-Symbolic-Semantic-SLAM-with-a-Hierarchically-Categorical-Gaussian-Splatting" class="headerlink" title="Hier-SLAM++: Neuro-Symbolic Semantic SLAM with a Hierarchically   Categorical Gaussian Splatting"></a>Hier-SLAM++: Neuro-Symbolic Semantic SLAM with a Hierarchically   Categorical Gaussian Splatting</h2><p><strong>Authors:Boying Li, Vuong Chi Hao, Peter J. Stuckey, Ian Reid, Hamid Rezatofighi</strong></p>
<p>We propose Hier-SLAM++, a comprehensive Neuro-Symbolic semantic 3D Gaussian Splatting SLAM method with both RGB-D and monocular input featuring an advanced hierarchical categorical representation, which enables accurate pose estimation as well as global 3D semantic mapping. The parameter usage in semantic SLAM systems increases significantly with the growing complexity of the environment, making scene understanding particularly challenging and costly. To address this problem, we introduce a novel hierarchical representation that encodes both semantic and geometric information in a compact form into 3D Gaussian Splatting, leveraging the capabilities of large language models (LLMs) as well as the 3D generative model. By utilizing the proposed hierarchical tree structure, semantic information is symbolically represented and learned in an end-to-end manner. We further introduce an advanced semantic loss designed to optimize hierarchical semantic information through both Intra-level and Inter-level optimizations. Additionally, we propose an improved SLAM system to support both RGB-D and monocular inputs using a feed-forward model. To the best of our knowledge, this is the first semantic monocular Gaussian Splatting SLAM system, significantly reducing sensor requirements for 3D semantic understanding and broadening the applicability of semantic Gaussian SLAM system. We conduct experiments on both synthetic and real-world datasets, demonstrating superior or on-par performance with state-of-the-art methods, while significantly reducing storage and training time requirements. Our project page is available at: <a target="_blank" rel="noopener" href="https://hierslampp.github.io/">https://hierslampp.github.io/</a> </p>
<blockquote>
<p>我们提出了Hier-SLAM++，这是一种全面的神经符号语义3D高斯混叠SLAM方法，支持RGB-D和单目输入，并带有高级分层类别表示。该方法能够实现精确的姿势估计和全局3D语义映射。随着环境复杂性的增加，语义SLAM系统中的参数使用量也显著增加，使得场景理解变得特别具有挑战性和成本高昂。为了解决这一问题，我们引入了一种新的分层表示方法，以紧凑的形式将语义和几何信息编码到3D高斯混洗中，利用大型语言模型和3D生成模型的能力。通过利用所提出的层次树结构，语义信息以符号方式表示并以端到端的方式进行学习。我们进一步引入了一种先进的语义损失，旨在通过跨级别优化来优化分层语义信息。此外，我们提出了一种改进的SLAM系统，该系统支持RGB-D和单目输入，并使用前馈模型。据我们所知，这是第一个语义单目高斯混洗SLAM系统，大大降低了对3D语义理解的传感器要求并扩大了语义高斯SLAM系统的应用范围。我们在合成数据集和真实世界数据集上进行了实验，展示了卓越或相当的性能，同时显著降低了存储和训练时间要求。我们的项目页面可在：<a target="_blank" rel="noopener" href="https://hierslampp.github.io/%EF%BC%88%E9%93%BE%E6%8E%A5%E6%97%A0%E6%B3%95%E7%9B%B4%E6%8E%A5%E4%BF%AE%E9%A2%84%EF%BC%89%E3%80%82">https://hierslampp.github.io/（链接无法直接访问）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14931v2">PDF</a> 18 pages. Under review</p>
<p><strong>摘要</strong><br>高效三维语义理解的新方法：基于Hierarchical语义表达和Neuro-Symbolic模型的三维高斯飞溅SLAM。使用先进的层次化分类表达，实现了精确的姿态估计和全局三维语义地图构建。引入新型层次化表达方法，将语义和几何信息以紧凑形式编码于三维高斯飞溅中，利用大型语言模型和三维生成模型的优势进行信息学习。创新地设计了层次化语义损失，用于优化内部和外部层次的语义信息。提出的改进型SLAM系统支持RGB-D和单目相机输入，实现了领先的性能，降低了对传感器的依赖，扩展了语义高斯SLAM系统的应用范围。在合成和真实数据集上的实验证明了该方法的优越性。更多信息可访问项目网页：<a target="_blank" rel="noopener" href="https://hierslampp.github.io/">https://hierslampp.github.io/</a>。</p>
<p><strong>关键发现点</strong></p>
<p>一、Hierarchical语义表达和Neuro-Symbolic模型结合：此方法结合了层次化语义表达和Neuro-Symbolic模型的优势，实现了高效的三维语义理解。</p>
<p>二、紧凑的三维高斯飞溅表达：通过引入新型层次化表达方法，将语义和几何信息编码于三维高斯飞溅中，提高信息的存储和处理效率。</p>
<p>三、利用大型语言模型和三维生成模型：利用大型语言模型和三维生成模型的能力，增强系统的语义理解和三维重建能力。</p>
<p>四、层次化语义损失设计：创新的层次化语义损失设计能够优化内部和外部层次的语义信息，提高系统的准确性和鲁棒性。</p>
<p>五、支持RGB-D和单目相机输入：提出的改进型SLAM系统不仅支持RGB-D输入，还支持单目相机输入，降低了传感器要求，扩大了应用范围。</p>
<p>六、优越的性能表现：在合成和真实数据集上的实验证明了该方法具有领先的性能，与现有技术相比具有优势。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14931">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9c85c82e0537338a75e6119bafcb717d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a732c2b97ac4072621289717b0a9a210.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-134dd9418dafaa6c184eb166a00eaee6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CULTURE3D-A-Large-Scale-and-Diverse-Dataset-of-Cultural-Landmarks-and-Terrains-for-Gaussian-Based-Scene-Rendering"><a href="#CULTURE3D-A-Large-Scale-and-Diverse-Dataset-of-Cultural-Landmarks-and-Terrains-for-Gaussian-Based-Scene-Rendering" class="headerlink" title="CULTURE3D: A Large-Scale and Diverse Dataset of Cultural Landmarks and   Terrains for Gaussian-Based Scene Rendering"></a>CULTURE3D: A Large-Scale and Diverse Dataset of Cultural Landmarks and   Terrains for Gaussian-Based Scene Rendering</h2><p><strong>Authors:Xinyi Zheng, Steve Zhang, Weizhe Lin, Aaron Zhang, Walterio W. Mayol-Cuevas, Yunze Liu, Junxiao Shen</strong></p>
<p>Current state-of-the-art 3D reconstruction models face limitations in building extra-large scale outdoor scenes, primarily due to the lack of sufficiently large-scale and detailed datasets. In this paper, we present a extra-large fine-grained dataset with 10 billion points composed of 41,006 drone-captured high-resolution aerial images, covering 20 diverse and culturally significant scenes from worldwide locations such as Cambridge Uni main buildings, the Pyramids, and the Forbidden City Palace. Compared to existing datasets, ours offers significantly larger scale and higher detail, uniquely suited for fine-grained 3D applications. Each scene contains an accurate spatial layout and comprehensive structural information, supporting detailed 3D reconstruction tasks. By reconstructing environments using these detailed images, our dataset supports multiple applications, including outputs in the widely adopted COLMAP format, establishing a novel benchmark for evaluating state-of-the-art large-scale Gaussian Splatting methods.The dataset’s flexibility encourages innovations and supports model plug-ins, paving the way for future 3D breakthroughs. All datasets and code will be open-sourced for community use. </p>
<blockquote>
<p>当前最先进的3D重建模型在构建超大规模户外场景时面临局限，这主要是由于缺乏足够大规模和详细的数据库。在本文中，我们介绍了一个超大规模精细粒度的数据集，由包含一亿点以上的高度精细化高分辨率图像构成，含有世界各地独特、多元、高价值的场景共计高达四万零六百零六张无人机拍摄的照片，包括剑桥大学的主要建筑、金字塔和紫禁城等。与现有数据集相比，我们的数据集规模更大，细节更丰富，更独特地适合用于精细粒度级的分析与应用场景重建任务中每个场景都有精准的空间布局和综合结构信息数据来支撑这些详细的三维重建任务。通过这些详尽图像进行环境重建我们的数据集可以支持多种应用，包括使用广泛支持的COLMAP格式输出文件；这为评估最新的大规模高斯延展法技术提供了全新基准线。数据集的灵活性有助于创新并可通过模型插件拓展应用范围铺平未来开展第三次数字建模科技革新的道路所有数据集及代码均将以开源形式供公众使用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06927v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个超大规模精细粒度的数据集，包含由无人机拍摄的高分辨率空中图像组成的10亿个点，涵盖全球20个多样且文化意义重大的场景，如剑桥大学主要建筑、金字塔和紫禁城。该数据集规模更大、细节更丰富，适用于精细粒度的3D应用。它支持详细的环境重建任务，并鼓励未来的创新模型开发。数据集以开源形式提供使用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了针对超大规模户外场景的3D重建模型面临的挑战。</li>
<li>提出了一种超大规模精细粒度的数据集，包含由无人机拍摄的高分辨率空中图像组成的10亿个点。</li>
<li>数据集涵盖全球多样化的文化场景，如剑桥大学主要建筑、金字塔和紫禁城等。</li>
<li>数据集规模更大、细节更丰富，适用于精细粒度的3D应用。</li>
<li>数据集支持详细的环境重建任务，包括采用广泛使用的COLMAP格式输出。</li>
<li>数据集的灵活性鼓励创新并支持模型插件，为未来3D技术的突破打下基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06927">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6e91f304fe71e97170022fe7866365b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cd9ca42770d4e90d54eac42ee710e4d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f87a1b40f013ca76ca9e3e3ea76f4103.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd171020e959a1cfc472a80afba75b70.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="EMD-Explicit-Motion-Modeling-for-High-Quality-Street-Gaussian-Splatting"><a href="#EMD-Explicit-Motion-Modeling-for-High-Quality-Street-Gaussian-Splatting" class="headerlink" title="EMD: Explicit Motion Modeling for High-Quality Street Gaussian Splatting"></a>EMD: Explicit Motion Modeling for High-Quality Street Gaussian Splatting</h2><p><strong>Authors:Xiaobao Wei, Qingpo Wuwu, Zhongyu Zhao, Zhuangzhe Wu, Nan Huang, Ming Lu, Ningning MA, Shanghang Zhang</strong></p>
<p>Photorealistic reconstruction of street scenes is essential for developing real-world simulators in autonomous driving. While recent methods based on 3D&#x2F;4D Gaussian Splatting (GS) have demonstrated promising results, they still encounter challenges in complex street scenes due to the unpredictable motion of dynamic objects. Current methods typically decompose street scenes into static and dynamic objects, learning the Gaussians in either a supervised manner (e.g., w&#x2F; 3D bounding-box) or a self-supervised manner (e.g., w&#x2F;o 3D bounding-box). However, these approaches do not effectively model the motions of dynamic objects (e.g., the motion speed of pedestrians is clearly different from that of vehicles), resulting in suboptimal scene decomposition. To address this, we propose Explicit Motion Decomposition (EMD), which models the motions of dynamic objects by introducing learnable motion embeddings to the Gaussians, enhancing the decomposition in street scenes. The proposed plug-and-play EMD module compensates for the lack of motion modeling in self-supervised street Gaussian splatting methods. We also introduce tailored training strategies to extend EMD to supervised approaches. Comprehensive experiments demonstrate the effectiveness of our method, achieving state-of-the-art novel view synthesis performance in self-supervised settings. The code is available at: <a target="_blank" rel="noopener" href="https://qingpowuwu.github.io/emd">https://qingpowuwu.github.io/emd</a>. </p>
<blockquote>
<p>街道场景的逼真重建对于开发自动驾驶现实世界模拟器至关重要。虽然最近基于3D&#x2F;4D高斯拼贴（GS）的方法已经取得了有前景的结果，但由于动态对象的不可预测运动，它们在复杂的街道场景中仍然面临挑战。当前的方法通常将街道场景分解为静态和动态对象，以监督的方式（例如，使用3D边界框）或自监督的方式（例如，不使用3D边界框）学习高斯。然而，这些方法并不能有效地对动态对象（例如行人与车辆的运动速度明显不同）进行建模，导致场景分解不理想。为了解决这个问题，我们提出了显式运动分解（EMD），它通过向高斯引入可学习的运动嵌入来对动态对象的运动进行建模，增强了街道场景的分解。所提出的即插即用的EMD模块弥补了自监督街道高斯拼贴方法中运动建模的不足。我们还引入了定制的训练策略，将EMD扩展到监督方法。综合实验证明了我们方法的有效性，在自监督设置中实现了最先进的全新视图合成性能。代码可在：<a target="_blank" rel="noopener" href="https://qingpowuwu.github.io/emd%E8%8E%B7%E5%8F%96%E3%80%82">https://qingpowuwu.github.io/emd获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15582v2">PDF</a> Acccpeted by ICCV2025</p>
<p><strong>Summary</strong><br>     针对自动驾驶中的真实街道场景重建，现有的基于3D&#x2F;4D高斯散斑技术的方法虽表现出潜力，但在复杂场景中处理动态对象时仍面临挑战。新方法显式运动分解（EMD）通过引入可学习的运动嵌入到高斯中，对动态对象的运动进行建模，提高了街道场景的分解效果。该方法在自我监督的街道高斯散斑方法中弥补了运动建模的不足，并通过定制的训练策略扩展到监督方法。实验证明，该方法在自我监督设置下实现了新颖的视图合成性能的最佳效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>真实街道场景的重建对于开发自动驾驶中的现实世界模拟器至关重要。</li>
<li>当前基于3D&#x2F;4D高斯散斑的方法在复杂街道场景中处理动态对象时存在挑战。</li>
<li>现有方法通常将街道场景分解为静态和动态对象，但缺乏有效的动态对象运动建模。</li>
<li>显式运动分解（EMD）方法通过引入可学习的运动嵌入到高斯中，提高了街道场景分解的效果。</li>
<li>EMD方法弥补了自我监督的街道高斯散斑方法中运动建模的不足。</li>
<li>定制的训练策略使EMD方法能够扩展到监督方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15582">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ec55da168bf08102fbbb9f9186de7dc2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ea0d8bc0a5ec1b282ed07936d51ee34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7cd29ba34d93dd632030856c3d92f12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-793060023e743ff00024cfe0f84286a2.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GazeGaussian-High-Fidelity-Gaze-Redirection-with-3D-Gaussian-Splatting"><a href="#GazeGaussian-High-Fidelity-Gaze-Redirection-with-3D-Gaussian-Splatting" class="headerlink" title="GazeGaussian: High-Fidelity Gaze Redirection with 3D Gaussian Splatting"></a>GazeGaussian: High-Fidelity Gaze Redirection with 3D Gaussian Splatting</h2><p><strong>Authors:Xiaobao Wei, Peng Chen, Guangyu Li, Ming Lu, Hui Chen, Feng Tian</strong></p>
<p>Gaze estimation encounters generalization challenges when dealing with out-of-distribution data. To address this problem, recent methods use neural radiance fields (NeRF) to generate augmented data. However, existing methods based on NeRF are computationally expensive and lack facial details. 3D Gaussian Splatting (3DGS) has become the prevailing representation of neural fields. While 3DGS has been extensively examined in head avatars, it faces challenges with accurate gaze control and generalization across different subjects. In this work, we propose GazeGaussian, the first high-fidelity gaze redirection method that uses a two-stream 3DGS model to represent the face and eye regions separately. Leveraging the unstructured nature of 3DGS, we develop a novel representation of the eye for rigid eye rotation based on the target gaze direction. To enable synthesis generalization across various subjects, we integrate an expression-guided module to inject subject-specific information into the neural renderer. Comprehensive experiments show that GazeGaussian outperforms existing methods in rendering speed, gaze redirection accuracy, and facial synthesis across multiple datasets. The code is available at: <a target="_blank" rel="noopener" href="https://ucwxb.github.io/GazeGaussian">https://ucwxb.github.io/GazeGaussian</a>. </p>
<blockquote>
<p>目光估计在处理离群分布数据时面临泛化挑战。为了解决这个问题，最近的方法使用神经辐射场（NeRF）来生成增强数据。然而，基于NeRF的现有方法计算量大且缺乏面部细节。3D高斯拼贴（3DGS）已成为神经场的流行表示方法。虽然3DGS在头像中得到了广泛的研究，但在准确的目光控制和不同主题的泛化方面仍面临挑战。在这项工作中，我们提出了GazeGaussian，这是第一个使用双流3DGS模型进行目光重定向的高保真方法，该模型能够分别表示面部和眼部区域。利用3DGS的无结构特性，我们开发了一种基于目标注视方向进行刚性眼球旋转的眼部表示新方法。为了实现跨各种主题的合成泛化，我们整合了一个表情引导模块，将主体特定信息注入到神经渲染器中。综合实验表明，GazeGaussian在渲染速度、目光重定向准确性和面部合成方面均优于现有方法，跨越多个数据集。代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://ucwxb.github.io/GazeGaussian">网站链接</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12981v2">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于神经辐射场的高保真眼神重定向方法——GazeGaussian。该方法采用双流3DGS模型分别表示面部和眼部区域，通过引入表情引导模块实现跨不同主题的合成泛化。相较于现有方法，GazeGaussian在渲染速度、眼神重定向精度和面部合成效果方面表现更佳。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GazeGaussian使用双流3DGS模型，将面部和眼部区域分开表示，以提高眼神重定向的准确性。</li>
<li>通过利用3DGS的无结构特性，开发了一种基于目标眼神方向的新型眼睛表示方法，实现刚性眼球旋转。</li>
<li>为了实现跨不同主题的合成泛化，引入了表情引导模块，将主题特定信息注入到神经渲染器中。</li>
<li>GazeGaussian具有快速渲染、准确的眼神重定向和优秀的面部合成效果。</li>
<li>该方法在多个数据集上的表现优于现有方法。</li>
<li>GazeGaussian的代码已公开在相关网站上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.12981">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-2576e650245972645f80e97bd406e99d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dcb7642f6ed7864e4437eff989fec9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dac90eb1a043aebfbd4b4089c7812169.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Reconstructing-Satellites-in-3D-from-Amateur-Telescope-Images"><a href="#Reconstructing-Satellites-in-3D-from-Amateur-Telescope-Images" class="headerlink" title="Reconstructing Satellites in 3D from Amateur Telescope Images"></a>Reconstructing Satellites in 3D from Amateur Telescope Images</h2><p><strong>Authors:Zhiming Chang, Boyang Liu, Yifei Xia, Youming Guo, Boxin Shi, He Sun</strong></p>
<p>Monitoring space objects is crucial for space situational awareness, yet reconstructing 3D satellite models from ground-based telescope images is challenging due to atmospheric turbulence, long observation distances, limited viewpoints, and low signal-to-noise ratios. In this paper, we propose a novel computational imaging framework that overcomes these obstacles by integrating a hybrid image pre-processing pipeline with a joint pose estimation and 3D reconstruction module based on controlled Gaussian Splatting (GS) and Branch-and-Bound (BnB) search. We validate our approach on both synthetic satellite datasets and on-sky observations of China’s Tiangong Space Station and the International Space Station, achieving robust 3D reconstructions of low-Earth orbit satellites from ground-based data. Quantitative evaluations using SSIM, PSNR, LPIPS, and Chamfer Distance demonstrate that our method outperforms state-of-the-art NeRF-based approaches, and ablation studies confirm the critical role of each component. Our framework enables high-fidelity 3D satellite monitoring from Earth, offering a cost-effective alternative for space situational awareness. Project page: <a target="_blank" rel="noopener" href="https://ai4scientificimaging.org/ReconstructingSatellites">https://ai4scientificimaging.org/ReconstructingSatellites</a> </p>
<blockquote>
<p>对空间目标进行监测对于了解空间态势至关重要，然而从地面望远镜图像重建三维卫星模型是一个巨大的挑战，主要由于大气扰动、观测距离长、视角有限和信噪比低等原因。在本文中，我们提出了一种新颖的计算机成像框架，通过混合图像预处理管道与基于受控的高斯溅射（GS）和分形搜索（BnB）的联合姿态估计和三维重建模块相结合，克服了这些障碍。我们在合成卫星数据集和中国天宫空间站以及国际空间站的星空观测上验证了我们的方法，实现了从地面数据对低地球轨道卫星的稳健三维重建。使用结构相似性度量（SSIM）、峰值信噪比（PSNR）、局部感知图像相似性（LPIPS）和Chamfer距离进行的定量评估表明，我们的方法优于最新的基于NeRF的方法，消融研究证实了每个组件的关键作用。我们的框架实现了从地球进行的高保真三维卫星监测，为态势感知提供了一种经济高效的替代方案。项目页面：<a target="_blank" rel="noopener" href="https://ai4scientificimaging.org/ReconstructingSatellites">https://ai4scientificimaging.org/ReconstructingSatellites</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.18394v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型的计算成像框架，通过混合图像预处理管道与基于受控高斯斑点和分支界定搜索的联合姿态估计和三维重建模块，克服了大气湍流、长观测距离、有限观测点和低信噪比等困难，实现了从地面望远镜图像重建三维卫星模型的难题。该研究不仅在合成卫星数据集上进行了验证，还对中国天宫空间站和国际空间站的在轨观测进行了实验，证明该方法能够从地面数据实现低地球轨道卫星的稳健三维重建。研究表明，该方法优于当前先进的NeRF方法，并具有成本低廉的优点。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>该论文关注从地面望远镜图像重建三维卫星模型的问题。</li>
<li>提出的计算成像框架整合了混合图像预处理管道和联合姿态估计与三维重建模块。</li>
<li>框架克服了大气湍流、长观测距离、有限观测点和低信噪比等挑战。</li>
<li>在合成卫星数据集及实际在轨观测中进行了验证，实现了稳健的三维重建。</li>
<li>该方法优于现有的NeRF方法，并具备成本低廉的优势。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.18394">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-947cfbec642a973312be43bb3f56a1ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28d4a30d8ad04557ac90a8ee07eea01b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-030990e8d2df91410b287448f8f1f3da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08e3595173a70e8a9ffe72c60d1f071a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18a874e996c80d7295ea73551dee67e7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-11/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-11/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-11/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-18a874e996c80d7295ea73551dee67e7.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-07-11  A Probabilistic Approach to Uncertainty Quantification Leveraging 3D   Geometry
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-11/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-db194f98bd94c084f68a365b3bf6c8cf.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-07-11  AI-GenBench A New Ongoing Benchmark for AI-Generated Image Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26384.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
