<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-11  Towards Multimodal Understanding via Stable Diffusion as a Task-Aware   Feature Extractor">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-efe497fba0a0396a563ae6238a3f68b8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-11-æ›´æ–°"><a href="#2025-07-11-æ›´æ–°" class="headerlink" title="2025-07-11 æ›´æ–°"></a>2025-07-11 æ›´æ–°</h1><h2 id="Towards-Multimodal-Understanding-via-Stable-Diffusion-as-a-Task-Aware-Feature-Extractor"><a href="#Towards-Multimodal-Understanding-via-Stable-Diffusion-as-a-Task-Aware-Feature-Extractor" class="headerlink" title="Towards Multimodal Understanding via Stable Diffusion as a Task-Aware   Feature Extractor"></a>Towards Multimodal Understanding via Stable Diffusion as a Task-Aware   Feature Extractor</h2><p><strong>Authors:Vatsal Agarwal, Matthew Gwilliam, Gefen Kohavi, Eshan Verma, Daniel Ulbricht, Abhinav Shrivastava</strong></p>
<p>Recent advances in multimodal large language models (MLLMs) have enabled image-based question-answering capabilities. However, a key limitation is the use of CLIP as the visual encoder; while it can capture coarse global information, it often can miss fine-grained details that are relevant to the input query. To address these shortcomings, this work studies whether pre-trained text-to-image diffusion models can serve as instruction-aware visual encoders. Through an analysis of their internal representations, we find diffusion features are both rich in semantics and can encode strong image-text alignment. Moreover, we find that we can leverage text conditioning to focus the model on regions relevant to the input question. We then investigate how to align these features with large language models and uncover a leakage phenomenon, where the LLM can inadvertently recover information from the original diffusion prompt. We analyze the causes of this leakage and propose a mitigation strategy. Based on these insights, we explore a simple fusion strategy that utilizes both CLIP and conditional diffusion features. We evaluate our approach on both general VQA and specialized MLLM benchmarks, demonstrating the promise of diffusion models for visual understanding, particularly in vision-centric tasks that require spatial and compositional reasoning. Our project page can be found <a target="_blank" rel="noopener" href="https://vatsalag99.github.io/mustafar/">https://vatsalag99.github.io/mustafar/</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥å·²ç»å®ç°äº†åŸºäºå›¾åƒçš„é—®ç­”åŠŸèƒ½ã€‚ç„¶è€Œï¼Œä¸€ä¸ªå…³é”®é™åˆ¶æ˜¯ä½¿ç”¨CLIPä½œä¸ºè§†è§‰ç¼–ç å™¨ï¼›è™½ç„¶å®ƒå¯ä»¥æ•æ‰ç²—ç³™çš„å…¨å±€ä¿¡æ¯ï¼Œä½†å®ƒç»å¸¸ä¼šé”™è¿‡ä¸è¾“å…¥æŸ¥è¯¢ç›¸å…³çš„ç»†å¾®ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™äº›ç¼ºç‚¹ï¼Œè¿™é¡¹å·¥ä½œç ”ç©¶äº†é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ˜¯å¦å¯ä»¥ä½œä¸ºæŒ‡ä»¤æ„ŸçŸ¥çš„è§†è§‰ç¼–ç å™¨ã€‚é€šè¿‡å¯¹å®ƒä»¬å†…éƒ¨è¡¨ç¤ºçš„åˆ†æï¼Œæˆ‘ä»¬å‘ç°æ‰©æ•£ç‰¹å¾æ—¢ä¸°å¯Œè¯­ä¹‰ï¼Œåˆèƒ½ç¼–ç å¼ºå¤§çš„å›¾åƒæ–‡æœ¬å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°å¯ä»¥åˆ©ç”¨æ–‡æœ¬æ¡ä»¶æ¥ä½¿æ¨¡å‹å…³æ³¨ä¸è¾“å…¥é—®é¢˜ç›¸å…³çš„åŒºåŸŸã€‚ç„¶åï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¦‚ä½•å°†è¿™äº›ç‰¹å¾ä¸å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½ï¼Œå¹¶å‘ç°äº†ä¸€ä¸ªæ³„éœ²ç°è±¡ï¼Œå…¶ä¸­LLMå¯èƒ½æ— æ„ä¸­ä»åŸå§‹çš„æ‰©æ•£æç¤ºä¸­æ¢å¤ä¿¡æ¯ã€‚æˆ‘ä»¬åˆ†æäº†æ³„éœ²çš„åŸå› å¹¶æå‡ºäº†ç¼“è§£ç­–ç•¥ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸€ç§ç®€å•çš„èåˆç­–ç•¥ï¼Œåˆ©ç”¨CLIPå’Œæ¡ä»¶æ‰©æ•£ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨é€šç”¨çš„VQAå’Œä¸“é—¨çš„MLLMåŸºå‡†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯æ˜äº†æ‰©æ•£æ¨¡å‹åœ¨è§†è§‰ç†è§£æ–¹é¢çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç©ºé—´å’Œç†è§£ç»„åˆæ¨ç†çš„ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸­ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://vatsalag99.github.io/mustafar/">https://vatsalag99.github.io/mustafar/</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07106v1">PDF</a> Website: see <a target="_blank" rel="noopener" href="https://vatsalag99.github.io/mustafar/">https://vatsalag99.github.io/mustafar/</a></p>
<p><strong>Summary</strong><br>     æœ€æ–°ç ”ç©¶åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä½œä¸ºæŒ‡ä»¤æ„ŸçŸ¥è§†è§‰ç¼–ç å™¨ï¼Œä»¥æ”¹è¿›åŸºäºå›¾åƒçš„é—®ç­”ç³»ç»Ÿã€‚ç ”ç©¶å‘ç°æ‰©æ•£æ¨¡å‹ç‰¹å¾ä¸°å¯Œï¼Œæ—¢å¯è¿›è¡Œå›¾åƒæ–‡æœ¬å¯¹é½ï¼Œåˆèƒ½é€šè¿‡æ–‡æœ¬æ¡ä»¶ä½¿æ¨¡å‹å…³æ³¨ä¸è¾“å…¥é—®é¢˜ç›¸å…³çš„åŒºåŸŸã€‚ç„¶è€Œï¼Œå­˜åœ¨ä¿¡æ¯æ³„éœ²ç°è±¡ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å¯èƒ½æ— æ„ä¸­ä»æ‰©æ•£æç¤ºä¸­æ¢å¤ä¿¡æ¯ã€‚ç ”ç©¶æå‡ºäº†ç¼“è§£ç­–ç•¥ï¼Œå¹¶æ¢ç´¢äº†ç»“åˆCLIPå’Œæ¡ä»¶æ‰©æ•£ç‰¹å¾çš„æ–¹æ³•ã€‚è¯¥ç­–ç•¥åœ¨ä¸€èˆ¬é—®ç­”å’Œç‰¹å®šçš„å¤§å‹è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾ç¤ºå‡ºæ‰©æ•£æ¨¡å‹åœ¨è§†è§‰ç†è§£æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²å…·å¤‡å›¾åƒé—®ç­”èƒ½åŠ›ï¼Œä½†ä½¿ç”¨CLIPä½œä¸ºè§†è§‰ç¼–ç å™¨å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ç‰¹å¾ä¸°å¯Œï¼Œå¯è¿›è¡Œå›¾åƒæ–‡æœ¬å¯¹é½ã€‚</li>
<li>é€šè¿‡æ–‡æœ¬æ¡ä»¶ä½¿æ¨¡å‹å…³æ³¨ä¸è¾“å…¥é—®é¢˜ç›¸å…³çš„åŒºåŸŸã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å¯èƒ½ä¼šä»æ‰©æ•£æç¤ºä¸­æ¢å¤ä¿¡æ¯ï¼Œå­˜åœ¨ä¿¡æ¯æ³„éœ²ç°è±¡ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¿¡æ¯æ³„éœ²çš„ç¼“è§£ç­–ç•¥ã€‚</li>
<li>ç»“åˆCLIPå’Œæ¡ä»¶æ‰©æ•£ç‰¹å¾çš„æ–¹æ³•åœ¨è§†è§‰ç†è§£æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07106">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a3bd296c2cc41c3895a4da677fca36d9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7a3b0ae5de83e19d634e9a524067c5c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2969ef7a776d00463a0820055285301b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d54614c3a827accab264a475f4a9e50e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65f3148d7fbf1643917ef50c1151545b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06e288cc174c3953b7cf67651f5f7f5f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Vision-Language-Vision-Auto-Encoder-Scalable-Knowledge-Distillation-from-Diffusion-Models"><a href="#Vision-Language-Vision-Auto-Encoder-Scalable-Knowledge-Distillation-from-Diffusion-Models" class="headerlink" title="Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation   from Diffusion Models"></a>Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation   from Diffusion Models</h2><p><strong>Authors:Tiezheng Zhang, Yitong Li, Yu-cheng Chou, Jieneng Chen, Alan Yuille, Chen Wei, Junfei Xiao</strong></p>
<p>Building state-of-the-art Vision-Language Models (VLMs) with strong captioning capabilities typically necessitates training on billions of high-quality image-text pairs, requiring millions of GPU hours. This paper introduces the Vision-Language-Vision (VLV) auto-encoder framework, which strategically leverages key pretrained components: a vision encoder, the decoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large Language Model (LLM). Specifically, we establish an information bottleneck by regularizing the language representation space, achieved through freezing the pretrained T2I diffusion decoder. Our VLV pipeline effectively distills knowledge from the text-conditioned diffusion model using continuous embeddings, demonstrating comprehensive semantic understanding via high-quality reconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the intermediate language representations into detailed descriptions, we construct a state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o and Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and significantly reduces data requirements; by primarily utilizing single-modal images for training and maximizing the utility of existing pretrained models (image encoder, T2I diffusion model, and LLM), it circumvents the need for massive paired image-text datasets, keeping the total training expenditure under $1,000 USD. </p>
<blockquote>
<p>æ„å»ºå…·æœ‰å¼ºå¤§æè¿°åŠŸèƒ½çš„æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€šå¸¸éœ€è¦è®­ç»ƒæ•°åäº¿ä¸ªé«˜è´¨é‡çš„å›¾ç‰‡æ–‡æœ¬å¯¹ï¼Œè¿™éœ€è¦æ•°ç™¾ä¸‡çš„GPUå°æ—¶ã€‚æœ¬æ–‡ä»‹ç»äº†è§†è§‰è¯­è¨€è§†è§‰ï¼ˆVLVï¼‰è‡ªåŠ¨ç¼–ç å™¨æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æˆ˜ç•¥æ€§åœ°åˆ©ç”¨äº†å…³é”®é¢„è®­ç»ƒç»„ä»¶ï¼šè§†è§‰ç¼–ç å™¨ã€æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„è§£ç å™¨ï¼Œä»¥åŠéšåçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡æ­£åˆ™åŒ–è¯­è¨€è¡¨ç¤ºç©ºé—´æ¥å»ºç«‹ä¿¡æ¯ç“¶é¢ˆï¼Œè¿™æ˜¯é€šè¿‡å†»ç»“é¢„è®­ç»ƒçš„T2Iæ‰©æ•£è§£ç å™¨æ¥å®ç°çš„ã€‚æˆ‘ä»¬çš„VLVç®¡é“æœ‰æ•ˆåœ°ä»æ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸­è’¸é¦çŸ¥è¯†ï¼Œä½¿ç”¨è¿ç»­åµŒå…¥æ¥å±•ç¤ºé«˜è´¨é‡é‡æ„çš„ç»¼åˆè¯­ä¹‰ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥è§£ç ä¸­é—´è¯­è¨€è¡¨ç¤ºå½¢å¼ä»¥ç”Ÿæˆè¯¦ç»†æè¿°ï¼Œä»è€Œæ„å»ºäº†ä¸€ä¸ªä¸GPT-4oå’ŒGemini 2.0 Flashç­‰é¢†å…ˆæ¨¡å‹ç›¸å½“çš„æœ€å…ˆè¿›çš„æè¿°å™¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•å±•ç¤ºäº†å“è¶Šçš„æˆæœ¬æ•ˆç›Šï¼Œå¤§å¤§é™ä½äº†æ•°æ®è¦æ±‚ï¼›ä¸»è¦é€šè¿‡ä½¿ç”¨å•æ¨¡æ€å›¾åƒè¿›è¡Œè®­ç»ƒå¹¶æœ€å¤§é™åº¦åœ°åˆ©ç”¨ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå›¾åƒç¼–ç å™¨ã€T2Iæ‰©æ•£æ¨¡å‹å’ŒLLMï¼‰ï¼Œä»è€Œé¿å…äº†å¤§è§„æ¨¡é…å¯¹å›¾åƒæ–‡æœ¬æ•°æ®é›†çš„éœ€æ±‚ï¼Œå°†æ€»è®­ç»ƒè´¹ç”¨ä¿æŒåœ¨1000ç¾å…ƒä»¥ä¸‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07104v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Vision-Language-Visionï¼ˆVLVï¼‰è‡ªç¼–ç å™¨æ¡†æ¶ï¼Œé€šè¿‡æˆ˜ç•¥æ€§åœ°åˆ©ç”¨å…³é”®é¢„è®­ç»ƒç»„ä»¶ï¼Œå¦‚è§†è§‰ç¼–ç å™¨ã€æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„è§£ç å™¨ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå®ç°äº†å…·æœ‰å¼ºå¤§æè¿°èƒ½åŠ›çš„å…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ„å»ºã€‚è¯¥æ–¹æ³•é€šè¿‡æ­£åˆ™åŒ–è¯­è¨€è¡¨ç¤ºç©ºé—´å»ºç«‹ä¿¡æ¯ç“¶é¢ˆï¼Œåˆ©ç”¨æ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„çŸ¥è¯†è’¸é¦ï¼Œå¹¶é€šè¿‡å¾®è°ƒLLMæ¥è§£ç ä¸­é—´è¯­è¨€è¡¨ç¤ºï¼Œä»è€Œç”Ÿæˆè¯¦ç»†çš„æè¿°ã€‚æ­¤æ–¹æ³•å…·æœ‰å“è¶Šçš„æˆæœ¬æ•ˆç›Šï¼Œæ˜¾è‘—å‡å°‘äº†æ•°æ®éœ€æ±‚ï¼Œä¸»è¦ä½¿ç”¨å•æ¨¡æ€å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œå¹¶æœ€å¤§é™åº¦åœ°åˆ©ç”¨ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ— éœ€å¤§è§„æ¨¡é…å¯¹å›¾åƒæ–‡æœ¬æ•°æ®é›†ï¼Œæ€»è®­ç»ƒè´¹ç”¨ä½äº1000ç¾å…ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†Vision-Language-Visionï¼ˆVLVï¼‰è‡ªç¼–ç å™¨æ¡†æ¶ï¼Œå®ç°äº†å…·æœ‰å¼ºå¤§æè¿°èƒ½åŠ›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒç»„ä»¶ï¼šè§†è§‰ç¼–ç å™¨ã€æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„è§£ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>é€šè¿‡æ­£åˆ™åŒ–è¯­è¨€è¡¨ç¤ºç©ºé—´å»ºç«‹ä¿¡æ¯ç“¶é¢ˆï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨æ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„çŸ¥è¯†è’¸é¦ã€‚</li>
<li>é€šè¿‡å¾®è°ƒLLMç”Ÿæˆé«˜è´¨é‡æè¿°ã€‚</li>
<li>æ–¹æ³•å…·æœ‰å“è¶Šçš„æˆæœ¬æ•ˆç›Šï¼Œæ˜¾è‘—å‡å°‘æ•°æ®éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07104">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-01f3564eba957419033f393fac383075.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57502e97dfe7bdfd13ab7190e87cff60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a90638225c131c88ce6ac334ea28fad3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Democratizing-High-Fidelity-Co-Speech-Gesture-Video-Generation"><a href="#Democratizing-High-Fidelity-Co-Speech-Gesture-Video-Generation" class="headerlink" title="Democratizing High-Fidelity Co-Speech Gesture Video Generation"></a>Democratizing High-Fidelity Co-Speech Gesture Video Generation</h2><p><strong>Authors:Xu Yang, Shaoli Huang, Shenbo Xie, Xuelin Chen, Yifei Liu, Changxing Ding</strong></p>
<p>Co-speech gesture video generation aims to synthesize realistic, audio-aligned videos of speakers, complete with synchronized facial expressions and body gestures. This task presents challenges due to the significant one-to-many mapping between audio and visual content, further complicated by the scarcity of large-scale public datasets and high computational demands. We propose a lightweight framework that utilizes 2D full-body skeletons as an efficient auxiliary condition to bridge audio signals with visual outputs. Our approach introduces a diffusion model conditioned on fine-grained audio segments and a skeleton extracted from the speakerâ€™s reference image, predicting skeletal motions through skeleton-audio feature fusion to ensure strict audio coordination and body shape consistency. The generated skeletons are then fed into an off-the-shelf human video generation model with the speakerâ€™s reference image to synthesize high-fidelity videos. To democratize research, we present CSG-405-the first public dataset with 405 hours of high-resolution videos across 71 speech types, annotated with 2D skeletons and diverse speaker demographics. Experiments show that our method exceeds state-of-the-art approaches in visual quality and synchronization while generalizing across speakers and contexts. </p>
<blockquote>
<p>ååŒè¯­éŸ³æ‰‹åŠ¿è§†é¢‘ç”Ÿæˆæ—¨åœ¨åˆæˆä¸éŸ³é¢‘å¯¹é½çš„é€¼çœŸè§†é¢‘ï¼ŒåŒ…å«è¯´è¯è€…çš„é¢éƒ¨è¡¨æƒ…å’Œä½“æ€åŠ¨ä½œã€‚æ­¤ä»»åŠ¡é¢ä¸´æŒ‘æˆ˜ï¼ŒåŸå› åœ¨äºéŸ³é¢‘å’Œè§†è§‰å†…å®¹ä¹‹é—´çš„ä¸€åˆ°å¤šæ˜ å°„å…³ç³»ï¼Œå†åŠ ä¸Šå¤§è§„æ¨¡å…¬å¼€æ•°æ®é›†çš„ç¨€ç¼ºå’Œé«˜è®¡ç®—éœ€æ±‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè½»é‡çº§çš„æ¡†æ¶ï¼Œåˆ©ç”¨2Då…¨èº«éª¨æ¶ä½œä¸ºæœ‰æ•ˆçš„è¾…åŠ©æ¡ä»¶æ¥è¿æ¥éŸ³é¢‘ä¿¡å·å’Œè§†è§‰è¾“å‡ºã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ ¹æ®ç²¾ç»†çš„éŸ³é¢‘ç‰‡æ®µå’Œä»è¯´è¯è€…çš„å‚è€ƒå›¾åƒä¸­æå–çš„éª¨æ¶è¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œé€šè¿‡éª¨æ¶éŸ³é¢‘ç‰¹å¾èåˆæ¥é¢„æµ‹éª¨éª¼è¿åŠ¨ï¼Œä»¥ç¡®ä¿ä¸¥æ ¼çš„éŸ³é¢‘åè°ƒå’Œèº«ä½“å½¢çŠ¶ä¸€è‡´æ€§ã€‚ç”Ÿæˆçš„éª¨æ¶ç„¶åè¾“å…¥åˆ°å¸¦æœ‰è¯´è¯è€…å‚è€ƒå›¾åƒçš„äººåƒè§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œåˆæˆé«˜ä¿çœŸè§†é¢‘ã€‚ä¸ºäº†æ¨åŠ¨ç ”ç©¶æ°‘ä¸»åŒ–ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CSG-405æ•°æ®é›†â€”â€”ç¬¬ä¸€ä¸ªå…¬å…±æ•°æ®é›†ï¼ŒåŒ…å«71ç§è¯­éŸ³ç±»å‹çš„405å°æ—¶é«˜åˆ†è¾¨ç‡è§†é¢‘ï¼Œæ ‡æ³¨äº†2Déª¨æ¶å’Œå¤šæ ·çš„è¯´è¯è€…äººå£ç»Ÿè®¡æ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’ŒåŒæ­¥æ–¹é¢è¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨è·¨è¯´è¯è€…å’Œä¸Šä¸‹æ–‡æ–¹é¢å…·æœ‰å¾ˆå¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06812v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ååŒè¯­éŸ³æ‰‹åŠ¿è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨äºŒç»´å…¨èº«éª¨éª¼ä½œä¸ºè¾…åŠ©æ¡ä»¶ï¼Œå°†éŸ³é¢‘ä¿¡å·ä¸è§†è§‰è¾“å‡ºç›¸è¿æ¥ã€‚è¯¥æ–¹æ³•é€šè¿‡ç²¾ç»†éŸ³é¢‘ç‰‡æ®µå’Œä»å‚è€ƒå›¾åƒä¸­æå–çš„éª¨éª¼è¿›è¡Œç‰¹å¾èåˆï¼Œé¢„æµ‹éª¨éª¼è¿åŠ¨ï¼Œç¡®ä¿ä¸¥æ ¼çš„éŸ³é¢‘åè°ƒå’Œä½“æ€ä¸€è‡´æ€§ã€‚ç”Ÿæˆçš„éª¨éª¼æ•°æ®å°†è¾“å…¥åˆ°ç°æˆçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œç»“åˆå‚è€ƒå›¾åƒåˆæˆé«˜è´¨é‡è§†é¢‘ã€‚ä¸ºæ¨è¿›ç ”ç©¶ï¼Œæœ¬æ–‡è¿˜å‘å¸ƒäº†é¦–ä¸ªå…¬å…±æ•°æ®é›†CSG-405ï¼ŒåŒ…å«405å°æ—¶çš„é«˜åˆ†è¾¨ç‡è§†é¢‘ï¼Œæ¶µç›–71ç§è¯­éŸ³ç±»å‹ï¼Œå¹¶æ ‡æ³¨äº†äºŒç»´éª¨éª¼å’Œå¤šæ ·çš„æ¼”è®²è€…ç‰¹å¾ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’ŒåŒæ­¥æ€§æ–¹é¢è¶…è¿‡äº†ç°æœ‰æŠ€æœ¯ï¼Œå¹¶åœ¨ä¸åŒæ¼”è®²è€…å’Œè¯­å¢ƒä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ååŒè¯­éŸ³æ‰‹åŠ¿è§†é¢‘ç”Ÿæˆæ—¨åœ¨åˆæˆä¸éŸ³é¢‘åŒæ­¥çš„ã€é€¼çœŸçš„è§†é¢‘ï¼ŒåŒ…æ‹¬é¢éƒ¨è¡¨è¾¾å’Œä½“æ€ã€‚</li>
<li>è¯¥ä»»åŠ¡é¢ä¸´éŸ³é¢‘ä¸è§†è§‰å†…å®¹ä¹‹é—´ä¸€å¯¹ä¸€æ˜ å°„çš„éš¾é¢˜ï¼Œä»¥åŠå¤§è§„æ¨¡å…¬å…±æ•°æ®é›†ç¼ºä¹å’Œè®¡ç®—éœ€æ±‚é«˜çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„è½»é‡çº§æ¡†æ¶ï¼Œåˆ©ç”¨äºŒç»´å…¨èº«éª¨éª¼ä½œä¸ºéŸ³é¢‘å’Œè§†è§‰ä¹‹é—´çš„æ¡¥æ¢ã€‚</li>
<li>æ¡†æ¶é€šè¿‡ç²¾ç»†éŸ³é¢‘ç‰‡æ®µå’Œå‚è€ƒå›¾åƒä¸­çš„éª¨éª¼ç‰¹å¾èåˆæ¥é¢„æµ‹éª¨éª¼è¿åŠ¨ã€‚</li>
<li>å‘å¸ƒäº†é¦–ä¸ªå…¬å…±æ•°æ®é›†CSG-405ï¼ŒåŒ…å«æ ‡æ³¨çš„äºŒç»´éª¨éª¼å’Œå¤šæ ·çš„æ¼”è®²è€…ç‰¹å¾ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’ŒåŒæ­¥æ€§èƒ½ä¸Šè¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06812">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d56c3aedaaabd1fc5ef90e15ad865f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bb2f58015d7c178821f33a2f8c01770.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-476cc2fb5fb803380038a86e820c4c25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cb3a2d68602d34282c105f2bc933174.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Enhancing-Diffusion-Model-Stability-for-Image-Restoration-via-Gradient-Management"><a href="#Enhancing-Diffusion-Model-Stability-for-Image-Restoration-via-Gradient-Management" class="headerlink" title="Enhancing Diffusion Model Stability for Image Restoration via Gradient   Management"></a>Enhancing Diffusion Model Stability for Image Restoration via Gradient   Management</h2><p><strong>Authors:Hongjie Wu, Mingqin Zhang, Linchao He, Ji-Zhe Zhou, Jiancheng Lv</strong></p>
<p>Diffusion models have shown remarkable promise for image restoration by leveraging powerful priors. Prominent methods typically frame the restoration problem within a Bayesian inference framework, which iteratively combines a denoising step with a likelihood guidance step. However, the interactions between these two components in the generation process remain underexplored. In this paper, we analyze the underlying gradient dynamics of these components and identify significant instabilities. Specifically, we demonstrate conflicts between the prior and likelihood gradient directions, alongside temporal fluctuations in the likelihood gradient itself. We show that these instabilities disrupt the generative process and compromise restoration performance. To address these issues, we propose Stabilized Progressive Gradient Diffusion (SPGD), a novel gradient management technique. SPGD integrates two synergistic components: (1) a progressive likelihood warm-up strategy to mitigate gradient conflicts; and (2) adaptive directional momentum (ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive experiments across diverse restoration tasks demonstrate that SPGD significantly enhances generation stability, leading to state-of-the-art performance in quantitative metrics and visually superior results. Code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/74587887/SPGD%7D%7Bhere%7D">https://github.com/74587887/SPGD}{here}</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹é€šè¿‡åˆ©ç”¨å¼ºå¤§çš„å…ˆéªŒä¿¡æ¯åœ¨å›¾åƒä¿®å¤é¢†åŸŸå±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚æ˜¾è‘—çš„æ–¹æ³•é€šå¸¸åœ¨è´å¶æ–¯æ¨æ–­æ¡†æ¶ä¸‹æ„å»ºä¿®å¤é—®é¢˜ï¼Œè¯¥æ¡†æ¶è¿­ä»£åœ°å°†å»å™ªæ­¥éª¤ä¸ä¼¼ç„¶å¼•å¯¼æ­¥éª¤ç›¸ç»“åˆã€‚ç„¶è€Œï¼Œç”Ÿæˆè¿‡ç¨‹ä¸­è¿™ä¸¤ä¸ªç»„ä»¶ä¹‹é—´çš„äº¤äº’ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†è¿™äº›ç»„ä»¶çš„åº•å±‚æ¢¯åº¦åŠ¨æ€ï¼Œå¹¶è¯†åˆ«å‡ºé‡å¤§ä¸ç¨³å®šå› ç´ ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å…ˆéªŒå’Œä¼¼ç„¶æ¢¯åº¦æ–¹å‘ä¹‹é—´çš„å†²çªï¼Œä»¥åŠä¼¼ç„¶æ¢¯åº¦æœ¬èº«çš„æš‚æ—¶æ³¢åŠ¨ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè¿™äº›ä¸ç¨³å®šå› ç´ ç ´åäº†ç”Ÿæˆè¿‡ç¨‹å¹¶å½±å“äº†ä¿®å¤æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç¨³å®šçš„æ¸è¿›æ¢¯åº¦æ‰©æ•£ï¼ˆSPGDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ¢¯åº¦ç®¡ç†æŠ€æœ¯ã€‚SPGDé›†æˆä¸¤ä¸ªååŒç»„ä»¶ï¼šï¼ˆ1ï¼‰æ¸è¿›çš„ä¼¼ç„¶é¢„çƒ­ç­–ç•¥ï¼Œä»¥ç¼“è§£æ¢¯åº¦å†²çªï¼›ï¼ˆ2ï¼‰è‡ªé€‚åº”æ–¹å‘åŠ¨é‡ï¼ˆADMï¼‰å¹³æ»‘ï¼Œä»¥å‡å°‘ä¼¼ç„¶æ¢¯åº¦çš„æ³¢åŠ¨ã€‚åœ¨å¤šç§ä¿®å¤ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSPGDæ˜¾è‘—æé«˜äº†ç”Ÿæˆç¨³å®šæ€§ï¼Œåœ¨å®šé‡æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨è§†è§‰ä¸Šäº§ç”Ÿäº†æ›´ä¼˜è´¨çš„ç»“æœã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/74587887/SPGD">https://github.com/74587887/SPGD</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06656v1">PDF</a> Accepted to ACM Multimedia 2025. Preprint version</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒä¿®å¤ä¸­çš„æ½œåŠ›ï¼Œåˆ†æäº†ç°æœ‰æ–¹æ³•ä¸­çš„æ¢¯åº¦åŠ¨æ€é—®é¢˜ï¼Œå¹¶æå‡ºäº†ç¨³å®šçš„æ¸è¿›æ¢¯åº¦æ‰©æ•£ï¼ˆSPGDï¼‰æŠ€æœ¯æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚é€šè¿‡é‡‡ç”¨æ¸è¿›å¯èƒ½æ€§é¢„çƒ­ç­–ç•¥å’Œè‡ªé€‚åº”æ–¹å‘åŠ¨é‡å¹³æ»‘æŠ€æœ¯ï¼ŒSPGDå¢å¼ºäº†ç”Ÿæˆè¿‡ç¨‹çš„ç¨³å®šæ€§ï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒä¿®å¤çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åˆ©ç”¨å…ˆéªŒçŸ¥è¯†åœ¨å›¾åƒä¿®å¤ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸åœ¨è´å¶æ–¯æ¨æ–­æ¡†æ¶ä¸‹è§£å†³ä¿®å¤é—®é¢˜ï¼Œç»“åˆå»å™ªå’Œä¼¼ç„¶æ€§æŒ‡å¯¼æ­¥éª¤ã€‚</li>
<li>è®ºæ–‡åˆ†æäº†ç°æœ‰æ–¹æ³•ä¸­çš„æ¢¯åº¦åŠ¨æ€é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å…ˆéªŒå’Œä¼¼ç„¶æ€§æ¢¯åº¦æ–¹å‘çš„å†²çªï¼Œä»¥åŠä¼¼ç„¶æ€§æ¢¯åº¦æœ¬èº«çš„æ—¶ç©ºæ³¢åŠ¨ã€‚</li>
<li>è¿™äº›é—®é¢˜ä¼šç ´åç”Ÿæˆè¿‡ç¨‹å¹¶å½±å“ä¿®å¤æ€§èƒ½ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†ç¨³å®šçš„æ¸è¿›æ¢¯åº¦æ‰©æ•£ï¼ˆSPGDï¼‰æŠ€æœ¯ã€‚</li>
<li>SPGDåŒ…æ‹¬ä¸¤ä¸ªååŒç»„ä»¶ï¼šæ¸è¿›çš„ä¼¼ç„¶æ€§é¢„çƒ­ç­–ç•¥ï¼Œç”¨äºç¼“è§£æ¢¯åº¦å†²çªï¼›è‡ªé€‚åº”æ–¹å‘åŠ¨é‡ï¼ˆADMï¼‰å¹³æ»‘ï¼Œç”¨äºå‡å°‘ä¼¼ç„¶æ€§æ¢¯åº¦æ³¢åŠ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06656">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2e72d9a1245be5932841606486b8e4f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f33056c8ae3c733ea420a077e7a3b848.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86d37cfe66aadc7bfd0435dd12a717e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-114a979c9929f9d6eff74674580c60e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f9f2f5e15b45cad5bd216b0748ee494.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Diff-2-I2P-Differentiable-Image-to-Point-Cloud-Registration-with-Diffusion-Prior"><a href="#Diff-2-I2P-Differentiable-Image-to-Point-Cloud-Registration-with-Diffusion-Prior" class="headerlink" title="Diff$^2$I2P: Differentiable Image-to-Point Cloud Registration with   Diffusion Prior"></a>Diff$^2$I2P: Differentiable Image-to-Point Cloud Registration with   Diffusion Prior</h2><p><strong>Authors:Juncheng Mu, Chengwei Ren, Weixiang Zhang, Liang Pan, Xiao-Ping Zhang, Yue Gao</strong></p>
<p>Learning cross-modal correspondences is essential for image-to-point cloud (I2P) registration. Existing methods achieve this mostly by utilizing metric learning to enforce feature alignment across modalities, disregarding the inherent modality gap between image and point data. Consequently, this paradigm struggles to ensure accurate cross-modal correspondences. To this end, inspired by the cross-modal generation success of recent large diffusion models, we propose Diff$^2$I2P, a fully Differentiable I2P registration framework, leveraging a novel and effective Diffusion prior for bridging the modality gap. Specifically, we propose a Control-Side Score Distillation (CSD) technique to distill knowledge from a depth-conditioned diffusion model to directly optimize the predicted transformation. However, the gradients on the transformation fail to backpropagate onto the cross-modal features due to the non-differentiability of correspondence retrieval and PnP solver. To this end, we further propose a Deformable Correspondence Tuning (DCT) module to estimate the correspondences in a differentiable way, followed by the transformation estimation using a differentiable PnP solver. With these two designs, the Diffusion model serves as a strong prior to guide the cross-modal feature learning of image and point cloud for forming robust correspondences, which significantly improves the registration. Extensive experimental results demonstrate that Diff$^2$I2P consistently outperforms SoTA I2P registration methods, achieving over 7% improvement in registration recall on the 7-Scenes benchmark. </p>
<blockquote>
<p>å­¦ä¹ è·¨æ¨¡æ€å¯¹åº”å…³ç³»å¯¹äºå›¾åƒåˆ°ç‚¹äº‘ï¼ˆI2Pï¼‰æ³¨å†Œè‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡åˆ©ç”¨åº¦é‡å­¦ä¹ æ¥å¼ºåˆ¶æ‰§è¡Œè·¨æ¨¡æ€çš„ç‰¹å¾å¯¹é½æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼Œå¿½ç•¥äº†å›¾åƒå’Œç‚¹æ•°æ®ä¹‹é—´å›ºæœ‰çš„æ¨¡æ€å·®è·ã€‚å› æ­¤ï¼Œè¿™ç§èŒƒå¼éš¾ä»¥ç¡®ä¿å‡†ç¡®çš„è·¨æ¨¡æ€å¯¹åº”å…³ç³»ã€‚é‰´äºæ­¤ï¼Œå—è¿‘æœŸå¤§å‹æ‰©æ•£æ¨¡å‹åœ¨è·¨æ¨¡æ€ç”Ÿæˆæ–¹é¢æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†Diff$^2$I2Pï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¯å¾®åˆ†çš„I2Pæ³¨å†Œæ¡†æ¶ï¼Œåˆ©ç”¨æ–°å‹æœ‰æ•ˆçš„æ‰©æ•£å…ˆéªŒæ¥å¼¥åˆæ¨¡æ€å·®è·ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ§åˆ¶ä¾§åˆ†æ•°è’¸é¦ï¼ˆCSDï¼‰æŠ€æœ¯ï¼Œä»æ·±åº¦æ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸­æç‚¼çŸ¥è¯†ä»¥ç›´æ¥ä¼˜åŒ–é¢„æµ‹çš„å˜æ¢ã€‚ç„¶è€Œï¼Œç”±äºå¯¹åº”æ£€ç´¢å’ŒPnPæ±‚è§£å™¨çš„ä¸å¯å¾®æ€§ï¼Œå˜æ¢ä¸Šçš„æ¢¯åº¦æ— æ³•åå‘ä¼ æ’­åˆ°è·¨æ¨¡æ€ç‰¹å¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†å¯å˜å½¢å¯¹åº”è°ƒæ•´ï¼ˆDCTï¼‰æ¨¡å—ï¼Œä»¥å¯å¾®åˆ†çš„æ–¹å¼ä¼°è®¡å¯¹åº”ï¼Œç„¶åä½¿ç”¨å¯å¾®åˆ†PnPæ±‚è§£å™¨è¿›è¡Œå˜æ¢ä¼°è®¡ã€‚é€šè¿‡è¿™ä¸¤ç§è®¾è®¡ï¼Œæ‰©æ•£æ¨¡å‹å……å½“äº†ä¸€ä¸ªå¼ºå¤§çš„å…ˆéªŒæ¥æŒ‡å¯¼å›¾åƒå’Œç‚¹äº‘çš„è·¨æ¨¡æ€ç‰¹å¾å­¦ä¹ ï¼Œå½¢æˆç¨³å¥çš„å¯¹åº”å…³ç³»ï¼Œè¿™æ˜¾è‘—æé«˜äº†æ³¨å†Œæ•ˆæœã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒDiff$^2$I2PæŒç»­è¶…è¶Šæœ€æ–°æŠ€æœ¯I2Pæ³¨å†Œæ–¹æ³•ï¼Œåœ¨7åœºæ™¯åŸºå‡†æµ‹è¯•ä¸Šæ³¨å†Œå¬å›ç‡æé«˜äº†è¶…è¿‡7%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06651v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸ºäº†è§£å†³å›¾åƒåˆ°ç‚¹äº‘ï¼ˆI2Pï¼‰æ³¨å†Œä¸­çš„è·¨æ¨¡æ€å¯¹åº”é—®é¢˜ï¼Œæå‡ºäº†Diff$^2$I2Pæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹ä½œä¸ºå¼ºå…ˆéªŒï¼Œé€šè¿‡Control-Side Score Distillationï¼ˆCSDï¼‰æŠ€æœ¯ä¼˜åŒ–é¢„æµ‹è½¬æ¢ã€‚ä¸ºè§£å†³å¯¹åº”æ£€ç´¢å’ŒPnPæ±‚è§£å™¨çš„ä¸å¯å¾®æ€§ï¼Œè¿›ä¸€æ­¥æå‡ºäº†Deformable Correspondence Tuningï¼ˆDCTï¼‰æ¨¡å—ä»¥å¯å¾®æ–¹å¼ä¼°è®¡å¯¹åº”å…³ç³»ï¼Œå¹¶ä½¿ç”¨å¯å¾®PnPæ±‚è§£å™¨è¿›è¡Œè½¬æ¢ä¼°è®¡ã€‚è¯¥æ¡†æ¶åœ¨è·¨æ¨¡æ€ç‰¹å¾å­¦ä¹ ä¸­å®ç°äº†ç¨³å¥çš„å¯¹åº”å…³ç³»ï¼Œæ˜¾è‘—æé«˜äº†æ³¨å†Œæ€§èƒ½ï¼Œå¹¶åœ¨7-ScenesåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è¶…è¿‡ç°æœ‰æŠ€æœ¯7%çš„æ³¨å†Œå¬å›ç‡æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§£å†³å›¾åƒåˆ°ç‚¹äº‘ï¼ˆI2Pï¼‰æ³¨å†Œä¸­çš„è·¨æ¨¡æ€å¯¹åº”é—®é¢˜æ˜¯å…³é”®ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡åº¦é‡å­¦ä¹ å®ç°ç‰¹å¾å¯¹é½ï¼Œä½†å¿½ç•¥äº†æ¨¡æ€é—´çš„å›ºæœ‰å·®è·ã€‚</li>
<li>Diff$^2$I2Pæ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹ä½œä¸ºå¼ºå…ˆéªŒï¼Œé€šè¿‡CSDæŠ€æœ¯ä¼˜åŒ–é¢„æµ‹è½¬æ¢ã€‚</li>
<li>DCTæ¨¡å—ä»¥å¯å¾®æ–¹å¼ä¼°è®¡å¯¹åº”å…³ç³»ï¼Œè§£å†³å¯¹åº”æ£€ç´¢å’ŒPnPæ±‚è§£å™¨çš„ä¸å¯å¾®æ€§é—®é¢˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨è·¨æ¨¡æ€ç‰¹å¾å­¦ä¹ ä¸­èµ·åˆ°é‡è¦ä½œç”¨ï¼Œå½¢æˆç¨³å¥çš„å¯¹åº”å…³ç³»ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨æ³¨å†Œæ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06651">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ba971fb134b43073f5fb0a5be87a6158.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bde74ece517d9d9d3c07d24cdab6db26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7685ab662ea696047843da7252daeaa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95b08edb379878ec916be549f6c057d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-229ee4afed7d53544fba6193d4881866.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FedPhD-Federated-Pruning-with-Hierarchical-Learning-of-Diffusion-Models"><a href="#FedPhD-Federated-Pruning-with-Hierarchical-Learning-of-Diffusion-Models" class="headerlink" title="FedPhD: Federated Pruning with Hierarchical Learning of Diffusion Models"></a>FedPhD: Federated Pruning with Hierarchical Learning of Diffusion Models</h2><p><strong>Authors:Qianyu Long, Qiyuan Wang, Christos Anagnostopoulos, Daning Bi</strong></p>
<p>Federated Learning (FL), as a distributed learning paradigm, trains models over distributed clientsâ€™ data. FL is particularly beneficial for distributed training of Diffusion Models (DMs), which are high-quality image generators that require diverse data. However, challenges such as high communication costs and data heterogeneity persist in training DMs similar to training Transformers and Convolutional Neural Networks. Limited research has addressed these issues in FL environments. To address this gap and challenges, we introduce a novel approach, FedPhD, designed to efficiently train DMs in FL environments. FedPhD leverages Hierarchical FL with homogeneity-aware model aggregation and selection policy to tackle data heterogeneity while reducing communication costs. The distributed structured pruning of FedPhD enhances computational efficiency and reduces model storage requirements in clients. Our experiments across multiple datasets demonstrate that FedPhD achieves high model performance regarding Fr&#39;echet Inception Distance (FID) scores while reducing communication costs by up to $88%$. FedPhD outperforms baseline methods achieving at least a $34%$ improvement in FID, while utilizing only $56%$ of the total computation and communication resources. </p>
<blockquote>
<p>è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ä½œä¸ºä¸€ç§åˆ†å¸ƒå¼å­¦ä¹ èŒƒå¼ï¼Œèƒ½å¤Ÿåœ¨åˆ†å¸ƒå¼å®¢æˆ·ç«¯æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ã€‚å¯¹äºéœ€è¦å¤šæ ·åŒ–æ•°æ®çš„æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„é«˜è´¨é‡å›¾åƒç”Ÿæˆæ¥è¯´ï¼ŒFLç‰¹åˆ«æœ‰ç›Šã€‚ç„¶è€Œï¼Œä¸è®­ç»ƒTransformerå’Œå·ç§¯ç¥ç»ç½‘ç»œç±»ä¼¼ï¼Œè®­ç»ƒDMsä¹Ÿå­˜åœ¨é«˜é€šä¿¡æˆæœ¬å’Œæ•°æ®å¼‚è´¨æ€§çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹è”é‚¦å­¦ä¹ ç¯å¢ƒä¸­çš„è¿™äº›é—®é¢˜è¿›è¡Œçš„ç ”ç©¶æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç©ºç™½ä»¥åŠé¢ä¸´çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹æ–¹æ³•FedPhDï¼Œæ—¨åœ¨åœ¨è”é‚¦å­¦ä¹ ç¯å¢ƒä¸­æœ‰æ•ˆåœ°è®­ç»ƒDMsã€‚FedPhDåˆ©ç”¨åˆ†å±‚è”é‚¦å­¦ä¹ ã€åŒè´¨æ€§æ„ŸçŸ¥æ¨¡å‹èšåˆå’Œé€‰æ‹©ç­–ç•¥æ¥è§£å†³æ•°æ®å¼‚è´¨æ€§é—®é¢˜ï¼ŒåŒæ—¶é™ä½é€šä¿¡æˆæœ¬ã€‚FedPhDçš„åˆ†å¸ƒå¼ç»“æ„åŒ–å‰ªææé«˜äº†è®¡ç®—æ•ˆç‡ï¼Œé™ä½äº†å®¢æˆ·ç«¯çš„æ¨¡å‹å­˜å‚¨éœ€æ±‚ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFedPhDåœ¨é™ä½é€šä¿¡æˆæœ¬é«˜è¾¾88%çš„åŒæ—¶ï¼Œå®ç°äº†é«˜æ°´å¹³çš„æ¨¡å‹æ€§èƒ½ï¼Œåœ¨FrÃ©chet Inception Distanceï¼ˆFIDï¼‰å¾—åˆ†æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚FedPhDä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œåœ¨FIDä¸Šè‡³å°‘æé«˜äº†34%ï¼ŒåŒæ—¶ä»…ä½¿ç”¨56%çš„æ€»è®¡ç®—å’Œé€šä¿¡èµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06449v1">PDF</a> 12 pages, 8 figures, 5 tables. This paper introduces FedPhD, a novel   hierarchical federated learning framework for training diffusion models that   addresses data heterogeneity and communication costs through   homogeneity-aware aggregation and structured pruning. Submitted to IEEE   Transactions on Cybernetics and is under review</p>
<p><strong>Summary</strong></p>
<p>åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹ï¼ŒFederated Learningï¼ˆFLï¼‰ä¸ºè®­ç»ƒDiffusion Modelsï¼ˆDMsï¼‰æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆï¼Œå°¤å…¶å¯¹äºéœ€è¦å¤šæ ·åŒ–æ•°æ®çš„é«˜å“è´¨å›¾åƒç”Ÿæˆå™¨æ¥è¯´ã€‚ç„¶è€Œï¼ŒFLç¯å¢ƒä¸­è®­ç»ƒDMsé¢ä¸´é«˜é€šä¿¡æˆæœ¬å’Œæ•°æ®å¼‚æ„æ€§çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FedPhDæ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ©ç”¨åˆ†å±‚FLå’ŒåŒæ„æ„ŸçŸ¥æ¨¡å‹èšåˆä¸é€‰æ‹©ç­–ç•¥æ¥åº”å¯¹æ•°æ®å¼‚æ„æ€§å¹¶é™ä½é€šä¿¡æˆæœ¬ã€‚å®éªŒè¯æ˜ï¼ŒFedPhDåœ¨é™ä½é€šä¿¡æˆæœ¬çš„åŒæ—¶ï¼Œå®ç°äº†é«˜æ¨¡å‹æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Federated Learning (FL) åœ¨è®­ç»ƒDiffusion Models (DMs)æ—¶å…·æœ‰ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯å¯¹äºéœ€è¦å¤šæ ·åŒ–æ•°æ®çš„é«˜å“è´¨å›¾åƒç”Ÿæˆå™¨ã€‚</li>
<li>FLç¯å¢ƒä¸­è®­ç»ƒDMsé¢ä¸´é«˜é€šä¿¡æˆæœ¬å’Œæ•°æ®å¼‚æ„æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>FedPhDæ–¹æ³•æ—¨åœ¨é«˜æ•ˆåœ°åœ¨FLç¯å¢ƒä¸­è®­ç»ƒDMsã€‚</li>
<li>FedPhDåˆ©ç”¨åˆ†å±‚FLå’ŒåŒæ„æ„ŸçŸ¥æ¨¡å‹èšåˆä¸é€‰æ‹©ç­–ç•¥æ¥è§£å†³æ•°æ®å¼‚æ„æ€§é—®é¢˜å¹¶é™ä½é€šä¿¡æˆæœ¬ã€‚</li>
<li>FedPhDé€šè¿‡åˆ†å¸ƒå¼ç»“æ„åŒ–å‰ªææé«˜è®¡ç®—æ•ˆç‡å¹¶å‡å°‘å®¢æˆ·ç«¯çš„æ¨¡å‹å­˜å‚¨éœ€æ±‚ã€‚</li>
<li>å®éªŒè¯æ˜FedPhDåœ¨é™ä½é€šä¿¡æˆæœ¬çš„åŒæ—¶ï¼Œå®ç°äº†é«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cca7b4a15b549928aa898353513a41a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cea5711556cf13bed4afdf9a84eebf84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dd61a0fdb3413c3c985930cce744b0c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Mitigating-Multi-Sequence-3D-Prostate-MRI-Data-Scarcity-through-Domain-Adaptation-using-Locally-Trained-Latent-Diffusion-Models-for-Prostate-Cancer-Detection"><a href="#Mitigating-Multi-Sequence-3D-Prostate-MRI-Data-Scarcity-through-Domain-Adaptation-using-Locally-Trained-Latent-Diffusion-Models-for-Prostate-Cancer-Detection" class="headerlink" title="Mitigating Multi-Sequence 3D Prostate MRI Data Scarcity through Domain   Adaptation using Locally-Trained Latent Diffusion Models for Prostate Cancer   Detection"></a>Mitigating Multi-Sequence 3D Prostate MRI Data Scarcity through Domain   Adaptation using Locally-Trained Latent Diffusion Models for Prostate Cancer   Detection</h2><p><strong>Authors:Emerson P. Grabke, Babak Taati, Masoom A. Haider</strong></p>
<p>Objective: Latent diffusion models (LDMs) could mitigate data scarcity challenges affecting machine learning development for medical image interpretation. The recent CCELLA LDM improved prostate cancer detection performance using synthetic MRI for classifier training but was limited to the axial T2-weighted (AxT2) sequence, did not investigate inter-institutional domain shift, and prioritized radiology over histopathology outcomes. We propose CCELLA++ to address these limitations and improve clinical utility. Methods: CCELLA++ expands CCELLA for simultaneous biparametric prostate MRI (bpMRI) generation, including the AxT2, high b-value diffusion series (HighB) and apparent diffusion coefficient map (ADC). Domain adaptation was investigated by pretraining classifiers on real or LDM-generated synthetic data from an internal institution, followed with fine-tuning on progressively smaller fractions of an out-of-distribution, external dataset. Results: CCELLA++ improved 3D FID for HighB and ADC but not AxT2 (0.013, 0.012, 0.063 respectively) sequences compared to CCELLA (0.060). Classifier pretraining with CCELLA++ bpMRI outperformed real bpMRI in AP and AUC for all domain adaptation scenarios. CCELLA++ pretraining achieved highest classifier performance below 50% (n&#x3D;665) external dataset volume. Conclusion: Synthetic bpMRI generated by our method can improve downstream classifier generalization and performance beyond real bpMRI or CCELLA-generated AxT2-only images. Future work should seek to quantify medical image sample quality, balance multi-sequence LDM training, and condition the LDM with additional information. Significance: The proposed CCELLA++ LDM can generate synthetic bpMRI that outperforms real data for domain adaptation with a limited target institution dataset. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/grabkeem/CCELLA-plus-plus">https://github.com/grabkeem/CCELLA-plus-plus</a> </p>
<blockquote>
<p>ç›®æ ‡ï¼šæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰å¯ä»¥ç¼“è§£å½±å“åŒ»å­¦å›¾åƒè§£é‡Šæœºå™¨å­¦ä¹ å‘å±•çš„æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜ã€‚æœ€è¿‘çš„CCELLA LDMä½¿ç”¨åˆæˆMRIæé«˜äº†å‰åˆ—è…ºç™Œæ£€æµ‹æ€§èƒ½ï¼Œç”¨äºåˆ†ç±»å™¨è®­ç»ƒï¼Œä½†ä»…é™äºè½´T2åŠ æƒï¼ˆAxT2ï¼‰åºåˆ—ï¼Œæ²¡æœ‰ç ”ç©¶è·¨æœºæ„é¢†åŸŸè½¬ç§»ï¼Œå¹¶ä¼˜å…ˆäºæ”¾å°„å­¦è€Œéç»„ç»‡ç—…ç†å­¦ç»“æœã€‚æˆ‘ä»¬æå‡ºCCELLA++æ¥è§£å†³è¿™äº›é™åˆ¶å¹¶æé«˜ä¸´åºŠå®ç”¨æ€§ã€‚æ–¹æ³•ï¼šCCELLA++æ‰©å±•äº†CCELLAï¼Œç”¨äºåŒæ—¶ç”ŸæˆåŒå‚æ•°å‰åˆ—è…ºMRIï¼ˆbpMRIï¼‰ï¼ŒåŒ…æ‹¬AxT2ã€é«˜bå€¼æ‰©æ•£ç³»åˆ—ï¼ˆHighBï¼‰å’Œæ‰©æ•£ç³»æ•°å›¾ï¼ˆADCï¼‰ã€‚é€šè¿‡é¢„è®­ç»ƒåˆ†ç±»å™¨åœ¨å†…éƒ¨æœºæ„çš„çœŸå®æˆ–LDMç”Ÿæˆåˆæˆæ•°æ®ä¸Šï¼Œç„¶ååœ¨é€æ¸è¾ƒå°çš„å¤–éƒ¨æ•°æ®é›†éƒ¨åˆ†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œæ¥ç ”ç©¶é¢†åŸŸé€‚åº”æ€§é—®é¢˜ã€‚ç»“æœï¼šä¸CCELLAç›¸æ¯”ï¼ŒCCELLA++æ”¹è¿›äº†HighBå’ŒADCçš„3D FIDï¼ˆåˆ†åˆ«ä¸º0.013å’Œ0.012ï¼‰ï¼Œè€ŒAxT2åºåˆ—æ²¡æœ‰æ”¹è¿›ï¼ˆä¸º0.063ï¼‰ã€‚ä½¿ç”¨CCELLA++ bpMRIé¢„è®­ç»ƒçš„åˆ†ç±»å™¨åœ¨æ‰€æœ‰é¢†åŸŸé€‚åº”åœºæ™¯ä¸­ï¼Œå…¶APå’ŒAUCå‡ä¼˜äºçœŸå®bpMRIã€‚å½“å¤–éƒ¨æ•°æ®é›†ä½“ç§¯ä½äº50%ï¼ˆn&#x3D;665ï¼‰æ—¶ï¼ŒCCELLA++é¢„è®­ç»ƒè¾¾åˆ°æœ€ä½³åˆ†ç±»å™¨æ€§èƒ½ã€‚ç»“è®ºï¼šé€šè¿‡æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„åˆæˆbpMRIå¯ä»¥æé«˜ä¸‹æ¸¸åˆ†ç±»å™¨çš„æ³›åŒ–å’Œæ€§èƒ½ï¼Œè¶…è¶Šäº†çœŸå®bpMRIæˆ–CCELLAç”Ÿæˆçš„ä»…AxT2å›¾åƒã€‚æœªæ¥çš„å·¥ä½œåº”è‡´åŠ›äºé‡åŒ–åŒ»å­¦å›¾åƒæ ·æœ¬è´¨é‡ï¼Œå¹³è¡¡å¤šåºåˆ—LDMè®­ç»ƒï¼Œå¹¶ç”¨é¢å¤–ä¿¡æ¯è°ƒæ•´LDMã€‚æ„ä¹‰ï¼šæ‰€æå‡ºçš„CCELLA++ LDMå¯ä»¥ç”ŸæˆåˆæˆbpMRIï¼Œåœ¨æœ‰é™çš„ç‰¹å®šæœºæ„æ•°æ®é›†ä¸Šå®ç°é¢†åŸŸé€‚åº”æ€§çš„è¶…è¶ŠçœŸå®æ•°æ®è¡¨ç°ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/grabkeem/CCELLA-plus-plus%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/grabkeem/CCELLA-plus-plusæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06384v1">PDF</a> BT and MAH are co-senior authors on the work. This work has been   submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelsï¼Œç®€ç§°LDMsï¼‰åœ¨è§£å†³åŒ»å­¦å½±åƒè§£è¯»ä¸­æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜æ–¹é¢çš„åº”ç”¨ã€‚é’ˆå¯¹åŸæœ‰CCELLAæ¨¡å‹åœ¨å‰åˆ—è…ºç™Œç—‡æ£€æµ‹ä¸­çš„å±€é™æ€§ï¼Œæå‡ºäº†CCELLA++æ¨¡å‹ã€‚è¯¥æ¨¡å‹ä¸ä»…æ‰©å±•äº†åŸæœ‰çš„å•åºåˆ—ç”Ÿæˆèƒ½åŠ›ï¼Œå®ç°äº†åŒå‚æ•°å‰åˆ—è…ºMRIï¼ˆbpMRIï¼‰çš„ç”Ÿæˆï¼Œå¹¶è¿›è¡Œäº†è·¨æœºæ„åŸŸé€‚åº”æ€§çš„ç ”ç©¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCCELLA++ç”Ÿæˆçš„åˆæˆbpMRIåœ¨åˆ†ç±»å™¨é¢„è®­ç»ƒä¸­çš„è¡¨ç°è¶…è¶Šäº†çœŸå®bpMRIæ•°æ®ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤–éƒ¨æ•°æ®é›†ä½“ç§¯è¾ƒå°çš„æƒ…å†µä¸‹ã€‚è¯¥æ¨¡å‹çš„ä»£ç å·²å…¬å¼€äºGitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CCELLA++æ‰©å±•äº†åŸæœ‰çš„CCELLAæ¨¡å‹ï¼Œå®ç°äº†åŒå‚æ•°å‰åˆ—è…ºMRIï¼ˆbpMRIï¼‰çš„ç”Ÿæˆï¼ŒåŒ…æ‹¬AxT2ã€HighBå’ŒADCåºåˆ—ã€‚</li>
<li>CCELLA++è¿›è¡Œäº†è·¨æœºæ„åŸŸé€‚åº”æ€§çš„ç ”ç©¶ï¼Œé€šè¿‡é¢„è®­ç»ƒåˆ†ç±»å™¨ä»¥é€‚åº”ä¸åŒæœºæ„çš„æ•°æ®é›†ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCCELLA++ç”Ÿæˆçš„åˆæˆbpMRIæ•°æ®åœ¨åˆ†ç±»å™¨é¢„è®­ç»ƒä¸­çš„è¡¨ç°ä¼˜äºçœŸå®bpMRIæ•°æ®ã€‚</li>
<li>åœ¨å¤–éƒ¨æ•°æ®é›†ä½“ç§¯è¾ƒå°çš„æƒ…å†µä¸‹ï¼ŒCCELLA++é¢„è®­ç»ƒçš„åˆ†ç±»å™¨æ€§èƒ½æœ€ä½³ã€‚</li>
<li>CCELLA++æ¨¡å‹å…¬å¼€çš„ä»£ç å¯ä¾›å…¶ä»–ç ”ç©¶è€…ä½¿ç”¨ã€‚</li>
<li>æœªæ¥çš„ç ”ç©¶åº”å…³æ³¨é‡åŒ–åŒ»å­¦å›¾åƒæ ·æœ¬è´¨é‡ï¼Œå¹³è¡¡å¤šåºåˆ—LDMè®­ç»ƒï¼Œä»¥åŠé€šè¿‡é¢å¤–ä¿¡æ¯è°ƒæ•´LDMã€‚</li>
<li>LDMçš„å¼•å…¥æœ‰åŠ©äºè§£å†³åŒ»å­¦å½±åƒè§£è¯»ä¸­æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œå…·æœ‰æ½œåœ¨çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06384">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-098329fb5878107e4a2dc83d7153a59a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77e5f2061814b5907559e7e881126956.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3abcb762ee9e314c4caeb4470c30e9c4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CultureCLIP-Empowering-CLIP-with-Cultural-Awareness-through-Synthetic-Images-and-Contextualized-Captions"><a href="#CultureCLIP-Empowering-CLIP-with-Cultural-Awareness-through-Synthetic-Images-and-Contextualized-Captions" class="headerlink" title="CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic   Images and Contextualized Captions"></a>CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic   Images and Contextualized Captions</h2><p><strong>Authors:Yuchen Huang, Zhiyuan Fan, Zhitao He, Sandeep Polisetty, Wenyan Li, Yi R. Fung</strong></p>
<p>Pretrained vision-language models (VLMs) such as CLIP excel in multimodal understanding but struggle with contextually relevant fine-grained visual features, making it difficult to distinguish visually similar yet culturally distinct concepts. This limitation stems from the scarcity of high-quality culture-specific datasets, the lack of integrated contextual knowledge, and the absence of hard negatives highlighting subtle distinctions. To address these challenges, we first design a data curation pipeline that leverages open-sourced VLMs and text-to-image diffusion models to construct CulTwin, a synthetic cultural dataset. This dataset consists of paired concept-caption-image triplets, where concepts visually resemble each other but represent different cultural contexts. Then, we fine-tune CLIP on CulTwin to create CultureCLIP, which aligns cultural concepts with contextually enhanced captions and synthetic images through customized contrastive learning, enabling finer cultural differentiation while preserving generalization capabilities. Experiments on culturally relevant benchmarks show that CultureCLIP outperforms the base CLIP, achieving up to a notable 5.49% improvement in fine-grained concept recognition on certain tasks, while preserving CLIPâ€™s original generalization ability, validating the effectiveness of our data synthesis and VLM backbone training paradigm in capturing subtle cultural distinctions. </p>
<blockquote>
<p>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨å¤šåª’ä½“ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„ç²¾ç»†è§†è§‰ç‰¹å¾æ–¹é¢å´è¡¨ç°æŒ£æ‰ï¼Œéš¾ä»¥åŒºåˆ†è§†è§‰ä¸Šç›¸ä¼¼ä½†æ–‡åŒ–ä¸Šè¿¥å¼‚çš„æ¦‚å¿µã€‚è¿™ä¸€å±€é™æ€§æºäºé«˜è´¨é‡çš„æ–‡åŒ–ç‰¹å®šæ•°æ®é›†çš„ç¨€ç¼ºã€ç¼ºä¹é›†æˆåŒ–çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ä»¥åŠç¼ºä¹å¼ºè°ƒç»†å¾®åŒºåˆ«çš„ç¡¬é˜´æ€§æ ·æœ¬ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€æ¡æ•°æ®æ•´ç†ç®¡é“ï¼Œåˆ©ç”¨å¼€æºçš„VLMså’Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ¥æ„å»ºCulTwinï¼Œä¸€ä¸ªåˆæˆæ–‡åŒ–æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«é…å¯¹çš„æ¦‚å¿µ-å­—å¹•-å›¾åƒä¸‰å…ƒç»„ï¼Œå…¶ä¸­æ¦‚å¿µåœ¨è§†è§‰ä¸Šç›¸äº’ç±»ä¼¼ï¼Œä½†ä»£è¡¨äº†ä¸åŒçš„æ–‡åŒ–èƒŒæ™¯ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨CulTwinä¸Šå¯¹CLIPè¿›è¡Œå¾®è°ƒï¼Œåˆ›å»ºäº†CultureCLIPï¼Œå®ƒé€šè¿‡å®šåˆ¶å¯¹æ¯”å­¦ä¹ ï¼Œå°†æ–‡åŒ–æ¦‚å¿µä¸ä¸Šä¸‹æ–‡å¢å¼ºçš„å­—å¹•å’Œåˆæˆå›¾åƒå¯¹é½ï¼Œä»è€Œå®ç°æ›´ç²¾ç»†çš„æ–‡åŒ–åŒºåˆ†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ç›¸å…³æ–‡åŒ–åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCultureCLIPä¼˜äºåŸºç¡€CLIPï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šå®ç°äº†é«˜è¾¾5.49%çš„ç²¾ç»†æ¦‚å¿µè¯†åˆ«æ”¹è¿›ï¼ŒåŒæ—¶ä¿ç•™äº†CLIPçš„åŸå§‹æ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æ•°æ®åˆæˆå’ŒVLMä¸»å¹²è®­ç»ƒèŒƒå¼åœ¨æ•æ‰å¾®å¦™æ–‡åŒ–åŒºåˆ«æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06210v1">PDF</a> 25 pages, COLM 2025</p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŒºåˆ†è§†è§‰ç›¸ä¼¼ä½†æ–‡åŒ–ä¸åŒçš„æ¦‚å¿µæ—¶é‡åˆ°å›°éš¾ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€ä¸ªæ•°æ®æ”¶é›†æµç¨‹ï¼Œåˆ©ç”¨å¼€æºçš„VLMså’Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ„å»ºäº†ä¸€ä¸ªåä¸ºCulTwinçš„æ–‡åŒ–æ•°æ®é›†ã€‚æ­¤æ•°æ®é›†åŒ…å«é…å¯¹æ¦‚å¿µã€æ ‡é¢˜å’Œå›¾åƒä¸‰å…ƒç»„ï¼Œæ¦‚å¿µè§†è§‰ç›¸ä¼¼ä½†ä»£è¡¨ä¸åŒæ–‡åŒ–èƒŒæ™¯ã€‚é€šè¿‡å¯¹CLIPåœ¨CulTwinæ•°æ®é›†ä¸Šçš„å¾®è°ƒï¼Œç ”ç©¶å›¢é˜Ÿåˆ›å»ºäº†CultureCLIPï¼Œé€šè¿‡å®šåˆ¶å¯¹æ¯”å­¦ä¹ ï¼Œä½¿æ–‡åŒ–æ¦‚å¿µä¸ä¸Šä¸‹æ–‡å¢å¼ºçš„æ ‡é¢˜å’Œåˆæˆå›¾åƒå¯¹é½ï¼Œå®ç°æ›´ç²¾ç»†çš„æ–‡åŒ–åŒºåˆ†ï¼ŒåŒæ—¶ä¿æŒæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒCultureCLIPåœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„ç²¾ç»†æ¦‚å¿µè¯†åˆ«èƒ½åŠ›è¾ƒåŸºç¡€CLIPæœ‰æ˜¾è‘—æå‡ï¼Œæå‡å¹…åº¦è¾¾5.49%ï¼ŒåŒæ—¶ä¿æŒäº†CLIPçš„åŸå§‹æ³›åŒ–èƒ½åŠ›ï¼ŒéªŒè¯äº†æ•°æ®åˆæˆå’ŒVLMéª¨å¹²è®­ç»ƒæ¨¡å¼åœ¨æ•æ‰å¾®å¦™æ–‡åŒ–å·®å¼‚æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨åŒºåˆ†è§†è§‰ç›¸ä¼¼ä½†æ–‡åŒ–ä¸åŒçš„æ¦‚å¿µæ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æ•°æ®é›†ç¨€ç¼ºå’Œæ–‡åŒ–ç‰¹å®šçŸ¥è¯†çš„ç¼ºä¹æ˜¯å½±å“æ¨¡å‹æ€§èƒ½çš„ä¸»è¦åŸå› ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿé€šè¿‡è®¾è®¡æ•°æ®æ”¶é›†æµç¨‹å¹¶æ„å»ºåˆæˆæ–‡åŒ–æ•°æ®é›†CulTwinæ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>CulTwinæ•°æ®é›†åŒ…å«é…å¯¹æ¦‚å¿µã€æ ‡é¢˜å’Œå›¾åƒä¸‰å…ƒç»„ï¼Œæ—¨åœ¨ä½“ç°ä¸åŒæ–‡åŒ–èƒŒæ™¯ä¸‹çš„è§†è§‰ç›¸ä¼¼æ€§ã€‚</li>
<li>é€šè¿‡åœ¨CulTwinæ•°æ®é›†ä¸Šå¾®è°ƒCLIPï¼Œåˆ›å»ºäº†CultureCLIPæ¨¡å‹ã€‚</li>
<li>CultureCLIPé€šè¿‡å®šåˆ¶å¯¹æ¯”å­¦ä¹ å®ç°å¯¹æ–‡åŒ–æ¦‚å¿µçš„ç²¾ç»†åŒºåˆ†ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06210">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b3ed9c05f05876a0ea804567f06574ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f80c51afbfe984d813f0bc66d785f1c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b2f1d96909315dbf66f06debe953811.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5290a13cedcfedd368bf1a9f6c6768cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-465b45d2b2c94fc10ffd257d83cf96c9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Prompt-Free-Conditional-Diffusion-for-Multi-object-Image-Augmentation"><a href="#Prompt-Free-Conditional-Diffusion-for-Multi-object-Image-Augmentation" class="headerlink" title="Prompt-Free Conditional Diffusion for Multi-object Image Augmentation"></a>Prompt-Free Conditional Diffusion for Multi-object Image Augmentation</h2><p><strong>Authors:Haoyu Wang, Lei Zhang, Wei Wei, Chen Ding, Yanning Zhang</strong></p>
<p>Diffusion models has underpinned much recent advances of dataset augmentation in various computer vision tasks. However, when involving generating multi-object images as real scenarios, most existing methods either rely entirely on text condition, resulting in a deviation between the generated objects and the original data, or rely too much on the original images, resulting in a lack of diversity in the generated images, which is of limited help to downstream tasks. To mitigate both problems with one stone, we propose a prompt-free conditional diffusion framework for multi-object image augmentation. Specifically, we introduce a local-global semantic fusion strategy to extract semantics from images to replace text, and inject knowledge into the diffusion model through LoRA to alleviate the category deviation between the original model and the target dataset. In addition, we design a reward model based counting loss to assist the traditional reconstruction loss for model training. By constraining the object counts of each category instead of pixel-by-pixel constraints, bridging the quantity deviation between the generated data and the original data while improving the diversity of the generated data. Experimental results demonstrate the superiority of the proposed method over several representative state-of-the-art baselines and showcase strong downstream task gain and out-of-domain generalization capabilities. Code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/00why00/PFCD%7D%7Bhere%7D">https://github.com/00why00/PFCD}{here}</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ä¸ºå„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„æ•°æ®é›†å¢å¼ºæä¾›äº†è®¸å¤šæœ€æ–°çš„è¿›å±•æ”¯æŒã€‚ç„¶è€Œï¼Œåœ¨æ¶‰åŠç”Ÿæˆå¤šå¯¹è±¡å›¾åƒä½œä¸ºçœŸå®åœºæ™¯æ—¶ï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•è¦ä¹ˆå®Œå…¨ä¾èµ–äºæ–‡æœ¬æ¡ä»¶ï¼Œå¯¼è‡´ç”Ÿæˆçš„å¯¹è±¡ä¸åŸå§‹æ•°æ®ä¹‹é—´å­˜åœ¨åå·®ï¼Œè¦ä¹ˆè¿‡äºä¾èµ–åŸå§‹å›¾åƒï¼Œå¯¼è‡´ç”Ÿæˆçš„å›¾åƒç¼ºä¹å¤šæ ·æ€§ï¼Œå¯¹ä¸‹æ¸¸ä»»åŠ¡å¸®åŠ©æœ‰é™ã€‚ä¸ºäº†ç”¨ä¸€ä¸ªæ–¹æ³•åŒæ—¶è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ— éœ€æç¤ºçš„å¤šå¯¹è±¡å›¾åƒå¢å¼ºæ¡ä»¶æ‰©æ•£æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å±€éƒ¨å…¨å±€è¯­ä¹‰èåˆç­–ç•¥ï¼Œä»å›¾åƒä¸­æå–è¯­ä¹‰æ¥æ›¿ä»£æ–‡æœ¬ï¼Œå¹¶é€šè¿‡LoRAå°†çŸ¥è¯†æ³¨å…¥æ‰©æ•£æ¨¡å‹ï¼Œä»¥å‡è½»åŸå§‹æ¨¡å‹ä¸ç›®æ ‡æ•°æ®é›†ä¹‹é—´çš„ç±»åˆ«åå·®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºå¥–åŠ±æ¨¡å‹çš„è®¡æ•°æŸå¤±æ¥è¾…åŠ©ä¼ ç»Ÿé‡å»ºæŸå¤±è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚é€šè¿‡çº¦æŸæ¯ä¸ªç±»åˆ«çš„å¯¹è±¡è®¡æ•°ï¼Œè€Œä¸æ˜¯é€åƒç´ çº¦æŸï¼Œæˆ‘ä»¬ç¼©å°äº†ç”Ÿæˆæ•°æ®ä¸åŸå§‹æ•°æ®ä¹‹é—´çš„æ•°é‡åå·®ï¼ŒåŒæ—¶æé«˜äº†ç”Ÿæˆæ•°æ®çš„å¤šæ ·æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºå‡ ç§å…·æœ‰ä»£è¡¨æ€§çš„æœ€æ–°åŸºçº¿æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å¼ºå¤§çš„ä¸‹æ¸¸ä»»åŠ¡å¢ç›Šå’Œè·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/00why00/PFCD%E8%BF%99%E5%A4%84%E5%8F%AF%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/00why00/PFCDæ­¤å¤„è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06146v1">PDF</a> Accepted at IJCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨å¤šç›®æ ‡å›¾åƒå¢å¼ºæ–¹é¢çš„åº”ç”¨ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¸­å­˜åœ¨çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ— æç¤ºæ¡ä»¶æ‰©æ•£æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥å±€éƒ¨å…¨å±€è¯­ä¹‰èåˆç­–ç•¥å’ŒåŸºäºè®¡æ•°çš„å¥–åŠ±æ¨¡å‹ï¼Œè§£å†³äº†ç”Ÿæˆç›®æ ‡ä¸åŸå§‹æ•°æ®ä¹‹é—´çš„åå·®å’Œç”Ÿæˆå›¾åƒç¼ºä¹å¤šæ ·æ€§ä¸¤ä¸ªé—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªä»£è¡¨æ€§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶å±•ç¤ºäº†å¼ºå¤§çš„ä¸‹æ¸¸ä»»åŠ¡å¢ç›Šå’Œè·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å¤šç›®æ ‡å›¾åƒå¢å¼ºæ–¹é¢å–å¾—é‡è¦è¿›å±•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨ç”Ÿæˆç›®æ ‡ä¸åŸå§‹æ•°æ®åå·®ã€ç”Ÿæˆå›¾åƒç¼ºä¹å¤šæ ·æ€§ç­‰é—®é¢˜ã€‚</li>
<li>å¼•å…¥æ— æç¤ºæ¡ä»¶æ‰©æ•£æ¡†æ¶ï¼Œç»“åˆå±€éƒ¨å…¨å±€è¯­ä¹‰èåˆç­–ç•¥ï¼Œè§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>é€šè¿‡LoRAæ³¨å…¥çŸ¥è¯†åˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå‡è½»åŸå§‹æ¨¡å‹ä¸ç›®æ ‡æ•°æ®é›†ä¹‹é—´çš„ç±»åˆ«åå·®ã€‚</li>
<li>è®¾è®¡åŸºäºè®¡æ•°çš„å¥–åŠ±æ¨¡å‹ï¼Œè¾…åŠ©ä¼ ç»Ÿé‡å»ºæŸå¤±è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚</li>
<li>é€šè¿‡çº¦æŸæ¯ç±»å¯¹è±¡è®¡æ•°è€Œéé€åƒç´ çº¦æŸï¼Œç¼©å°ç”Ÿæˆæ•°æ®ä¸åŸå§‹æ•°æ®ä¹‹é—´çš„æ•°é‡åå·®ï¼Œæé«˜ç”Ÿæˆæ•°æ®å¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06146">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0428c1f2936a6dfba8748fc069a7fb3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2af2a734231a46ba72db5d575ec37bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-983d38607d2168a9c655731e92d0cef7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59ea17a5b75338af9c08bb8ccc51cee1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54892a8bc2aa214948514f9f437c9044.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af79c8c1e465c05a4d0a16022b0074be.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ScoreAdv-Score-based-Targeted-Generation-of-Natural-Adversarial-Examples-via-Diffusion-Models"><a href="#ScoreAdv-Score-based-Targeted-Generation-of-Natural-Adversarial-Examples-via-Diffusion-Models" class="headerlink" title="ScoreAdv: Score-based Targeted Generation of Natural Adversarial   Examples via Diffusion Models"></a>ScoreAdv: Score-based Targeted Generation of Natural Adversarial   Examples via Diffusion Models</h2><p><strong>Authors:Chihan Huang, Hao Tang</strong></p>
<p>Despite the success of deep learning across various domains, it remains vulnerable to adversarial attacks. Although many existing adversarial attack methods achieve high success rates, they typically rely on $\ell_{p}$-norm perturbation constraints, which do not align with human perceptual capabilities. Consequently, researchers have shifted their focus toward generating natural, unrestricted adversarial examples (UAEs). GAN-based approaches suffer from inherent limitations, such as poor image quality due to instability and mode collapse. Meanwhile, diffusion models have been employed for UAE generation, but they still rely on iterative PGD perturbation injection, without fully leveraging their central denoising capabilities. In this paper, we introduce a novel approach for generating UAEs based on diffusion models, named ScoreAdv. This method incorporates an interpretable adversarial guidance mechanism to gradually shift the sampling distribution towards the adversarial distribution, while using an interpretable saliency map to inject the visual information of a reference image into the generated samples. Notably, our method is capable of generating an unlimited number of natural adversarial examples and can attack not only classification models but also retrieval models. We conduct extensive experiments on ImageNet and CelebA datasets, validating the performance of ScoreAdv across ten target models in both black-box and white-box settings. Our results demonstrate that ScoreAdv achieves state-of-the-art attack success rates and image quality. Furthermore, the dynamic balance between denoising and adversarial perturbation enables ScoreAdv to remain robust even under defensive measures. </p>
<blockquote>
<p>å°½ç®¡æ·±åº¦å­¦ä¹ åœ¨å„ä¸ªé¢†åŸŸçš„æˆåŠŸåº”ç”¨ï¼Œå®ƒä»ç„¶å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„å¨èƒã€‚è™½ç„¶è®¸å¤šç°æœ‰çš„å¯¹æŠ—æ€§æ”»å‡»æ–¹æ³•è¾¾åˆ°äº†é«˜æˆåŠŸç‡ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äº$\ell_{p}$-normæ‰°åŠ¨çº¦æŸï¼Œè¿™äº›çº¦æŸå¹¶ä¸ç¬¦åˆäººç±»çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚å› æ­¤ï¼Œç ”ç©¶è€…ä»¬å¼€å§‹å…³æ³¨ç”Ÿæˆè‡ªç„¶ã€æ— é™åˆ¶çš„å¯¹æŠ—æ ·æœ¬ï¼ˆUAEsï¼‰ã€‚åŸºäºGANçš„æ–¹æ³•å­˜åœ¨å›ºæœ‰çš„å±€é™æ€§ï¼Œä¾‹å¦‚ç”±äºä¸ç¨³å®šå’Œæ¨¡å¼å´©æºƒå¯¼è‡´çš„å›¾åƒè´¨é‡å·®ã€‚åŒæ—¶ï¼Œæ‰©æ•£æ¨¡å‹å·²è¢«ç”¨äºUAEç”Ÿæˆï¼Œä½†å®ƒä»¬ä»ç„¶ä¾èµ–äºè¿­ä»£PGDæ‰°åŠ¨æ³¨å…¥ï¼Œå¹¶æœªå……åˆ†åˆ©ç”¨å…¶æ ¸å¿ƒçš„é™å™ªèƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹ç”ŸæˆUAEsçš„æ–°å‹æ–¹æ³•ï¼Œåä¸ºScoreAdvã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¯è§£é‡Šçš„å¯¹æŠ—æ€§æŒ‡å¯¼æœºåˆ¶ï¼Œé€æ­¥å°†é‡‡æ ·åˆ†å¸ƒè½¬å‘å¯¹æŠ—æ€§åˆ†å¸ƒï¼ŒåŒæ—¶ä½¿ç”¨å¯è§£é‡Šçš„æ˜¾è‘—æ€§å›¾å°†å‚è€ƒå›¾åƒçš„å¯è§†ä¿¡æ¯æ³¨å…¥ç”Ÿæˆçš„æ ·æœ¬ä¸­ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ç”Ÿæˆæ— é™æ•°é‡çš„è‡ªç„¶å¯¹æŠ—æ ·æœ¬ï¼Œå¹¶ä¸”å¯ä»¥æ”»å‡»ä¸ä»…æ˜¯åˆ†ç±»æ¨¡å‹ï¼Œè¿˜æœ‰æ£€ç´¢æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨ImageNetå’ŒCelebAæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒéªŒè¯äº†ScoreAdvåœ¨é»‘ç™½ç›’è®¾ç½®ä¸­å¯¹åä¸ªç›®æ ‡æ¨¡å‹çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒScoreAdvè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ”»å‡»æˆåŠŸç‡å’Œå›¾åƒè´¨é‡ã€‚æ­¤å¤–ï¼Œé™å™ªå’Œå¯¹æŠ—æ€§æ‰°åŠ¨ä¹‹é—´çš„åŠ¨æ€å¹³è¡¡ä½¿ScoreAdvåœ¨é˜²å¾¡æªæ–½ä¸‹ä»èƒ½ä¿æŒç¨³å®šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06078v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹ç”Ÿæˆæ— é™åˆ¶å¯¹æŠ—æ ·æœ¬çš„æ–¹æ³•ï¼Œåä¸ºScoreAdvã€‚è¯¥æ–¹æ³•é€šè¿‡å¯è§£é‡Šçš„å¯¹æŠ—æ€§å¼•å¯¼æœºåˆ¶é€æ­¥å°†é‡‡æ ·åˆ†å¸ƒè½¬å‘å¯¹æŠ—æ€§åˆ†å¸ƒï¼ŒåŒæ—¶ä½¿ç”¨å¯è§£é‡Šçš„æ˜¾è‘—æ€§å›¾å°†å‚è€ƒå›¾åƒçš„å¯è§†ä¿¡æ¯æ³¨å…¥ç”Ÿæˆçš„æ ·æœ¬ä¸­ã€‚ScoreAdvèƒ½åœ¨ä¸é™æ•°é‡çš„è‡ªç„¶å¯¹æŠ—æ ·æœ¬ä¸Šç”Ÿæˆæ”»å‡»ï¼Œå¹¶èƒ½åœ¨é»‘ç®±å’Œç™½ç®±è®¾ç½®ä¸­æ”»å‡»åˆ†ç±»æ¨¡å‹å’Œæ£€ç´¢æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒScoreAdvåœ¨æ”»å‡»æˆåŠŸç‡å’Œå›¾åƒè´¨é‡ä¸Šå‡è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œä¸”åœ¨é˜²å¾¡æªæ–½ä¸‹ä»èƒ½ä¿æŒç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹æ˜“å—åŸºäº$\ell_{p}$-èŒƒæ•°çš„å¯¹æŠ—æ”»å‡»å½±å“ï¼Œè¿™ä¸ç¬¦åˆäººç±»æ„ŸçŸ¥èƒ½åŠ›ã€‚å› æ­¤ï¼Œç ”ç©¶è€…å¼€å§‹ä¸“æ³¨äºç”Ÿæˆè‡ªç„¶æ— é™åˆ¶çš„å¯¹æŠ—æ ·æœ¬ï¼ˆUAEsï¼‰ã€‚</li>
<li>ç”Ÿæˆå¯¹æŠ—æ ·æœ¬çš„ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºè¿­ä»£PGDæ‰°åŠ¨æ³¨å…¥ï¼Œæ²¡æœ‰å……åˆ†åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ä¸­å¿ƒå»å™ªèƒ½åŠ›ã€‚</li>
<li>ScoreAdvæ–¹æ³•åŸºäºæ‰©æ•£æ¨¡å‹æå‡ºï¼Œç»“åˆäº†å¯è§£é‡Šçš„å¯¹æŠ—å¼•å¯¼æœºåˆ¶å’Œæ˜¾è‘—æ€§å›¾æ³¨å…¥å‚è€ƒå›¾åƒä¿¡æ¯ã€‚</li>
<li>ScoreAdvèƒ½ç”Ÿæˆæ— é™æ•°é‡çš„è‡ªç„¶å¯¹æŠ—æ ·æœ¬ï¼Œå¹¶èƒ½æ”»å‡»åˆ†ç±»æ¨¡å‹å’Œæ£€ç´¢æ¨¡å‹ã€‚</li>
<li>ScoreAdvåœ¨ImageNetå’ŒCelebAæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜å…¶åœ¨æ”»å‡»æˆåŠŸç‡å’Œå›¾åƒè´¨é‡ä¸Šè¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ã€‚</li>
<li>ScoreAdvé€šè¿‡åŠ¨æ€å¹³è¡¡å»å™ªå’Œå¯¹æŠ—æ‰°åŠ¨ï¼Œåœ¨é˜²å¾¡æªæ–½ä¸‹ä»èƒ½ä¿æŒç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06078">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-561547cb60bd5b506d8c3d983ad38af8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-296bdf390a868f8be9b6a13df241d6d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-025f8005fda863d20e54905b0d38203d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb0fa0a1c11e440719d655aa9a1a20f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63e38b3b63aeabfdccc926b29f32a7be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7784bcde25be85510efcc3e4c05862df.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="TextPixs-Glyph-Conditioned-Diffusion-with-Character-Aware-Attention-and-OCR-Guided-Supervision"><a href="#TextPixs-Glyph-Conditioned-Diffusion-with-Character-Aware-Attention-and-OCR-Guided-Supervision" class="headerlink" title="TextPixs: Glyph-Conditioned Diffusion with Character-Aware Attention and   OCR-Guided Supervision"></a>TextPixs: Glyph-Conditioned Diffusion with Character-Aware Attention and   OCR-Guided Supervision</h2><p><strong>Authors:Syeda Anshrah Gillani, Mirza Samad Ahmed Baig, Osama Ahmed Khan, Shahid Munir Shah, Umema Mujeeb, Maheen Ali</strong></p>
<p>The modern text-to-image diffusion models boom has opened a new era in digital content production as it has proven the previously unseen ability to produce photorealistic and stylistically diverse imagery based on the semantics of natural-language descriptions. However, the consistent disadvantage of these models is that they cannot generate readable, meaningful, and correctly spelled text in generated images, which significantly limits the use of practical purposes like advertising, learning, and creative design. This paper introduces a new framework, namely Glyph-Conditioned Diffusion with Character-Aware Attention (GCDA), using which a typical diffusion backbone is extended by three well-designed modules. To begin with, the model has a dual-stream text encoder that encodes both semantic contextual information and explicit glyph representations, resulting in a character-aware representation of the input text that is rich in nature. Second, an attention mechanism that is aware of the character is proposed with a new attention segregation loss that aims to limit the attention distribution of each character independently in order to avoid distortion artifacts. Lastly, GCDA has an OCR-in-the-loop fine-tuning phase, where a full text perceptual loss, directly optimises models to be legible and accurately spell. Large scale experiments to benchmark datasets, such as MARIO-10M and T2I-CompBench, reveal that GCDA sets a new state-of-the-art on all metrics, with better character based metrics on text rendering (Character Error Rate: 0.08 vs 0.21 for the previous best; Word Error Rate: 0.15 vs 0.25), human perception, and comparable image synthesis quality on high-fidelity (FID: 14.3). </p>
<blockquote>
<p>ç°ä»£æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç¹è£å¼€å¯äº†æ•°å­—å†…å®¹ç”Ÿäº§çš„æ–°æ—¶ä»£ï¼Œå› ä¸ºå®ƒè¯æ˜äº†åŸºäºè‡ªç„¶è¯­è¨€æè¿°çš„è¯­ä¹‰ç”Ÿæˆæ‘„å½±é€¼çœŸä¸”é£æ ¼å¤šæ ·çš„å›¾åƒçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä¸€ç›´å­˜åœ¨ä¸€ä¸ªç¼ºç‚¹ï¼Œé‚£å°±æ˜¯å®ƒä»¬æ— æ³•ç”Ÿæˆå¯è¯»ã€æœ‰æ„ä¹‰ã€æ‹¼å†™æ­£ç¡®çš„æ–‡æœ¬å›¾åƒï¼Œè¿™æå¤§åœ°é™åˆ¶äº†å®ƒä»¬åœ¨å¹¿å‘Šã€å­¦ä¹ å’Œåˆ›æ„è®¾è®¡ç­‰å®ç”¨åœºåˆçš„åº”ç”¨ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°æ¡†æ¶ï¼Œå³å¸¦æœ‰å­—ç¬¦æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶çš„Glyphæ¡ä»¶æ‰©æ•£ï¼ˆGCDAï¼‰ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä½¿ç”¨ä¸‰ä¸ªç²¾å¿ƒè®¾è®¡çš„æ¨¡å—æ‰©å±•äº†å…¸å‹çš„æ‰©æ•£ä¸»å¹²ã€‚é¦–å…ˆï¼Œè¯¥æ¨¡å‹å…·æœ‰ä¸€ä¸ªåŒæµæ–‡æœ¬ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨ç¼–ç è¯­ä¹‰ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œæ˜ç¡®çš„å­—å½¢è¡¨ç¤ºï¼Œä»è€Œå¾—åˆ°è¾“å…¥æ–‡æœ¬çš„è‡ªç„¶ä¸°å¯Œå­—ç¬¦æ„ŸçŸ¥è¡¨ç¤ºã€‚å…¶æ¬¡ï¼Œæå‡ºäº†ä¸€ç§å­—ç¬¦æ„ŸçŸ¥çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡æ–°çš„æ³¨æ„åŠ›åˆ†å‰²æŸå¤±æ¥é™åˆ¶æ¯ä¸ªå­—ç¬¦çš„ç‹¬ç«‹æ³¨æ„åŠ›åˆ†å¸ƒï¼Œä»¥é¿å…å¤±çœŸä¼ªå½±ã€‚æœ€åï¼ŒGCDAæœ‰ä¸€ä¸ªOCRå¾ªç¯å¾®è°ƒé˜¶æ®µï¼Œå…¶ä¸­å…¨æ–‡æœ¬æ„ŸçŸ¥æŸå¤±ç›´æ¥ä¼˜åŒ–æ¨¡å‹ï¼Œä½¿å…¶å¯è¯»ä¸”æ‹¼å†™å‡†ç¡®ã€‚å¤§è§„æ¨¡å®éªŒåœ¨MARIO-10Må’ŒT2I-CompBenchç­‰åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œè¡¨æ˜GCDAåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œæ–‡æœ¬æ¸²æŸ“çš„å­—ç¬¦åŸºç¡€æŒ‡æ ‡è¡¨ç°æ›´å¥½ï¼ˆå­—ç¬¦é”™è¯¯ç‡ï¼š0.08å¯¹æ¯”ä¹‹å‰æœ€ä½³çš„0.21ï¼›å•è¯é”™è¯¯ç‡ï¼š0.15å¯¹æ¯”ä¹‹å‰çš„æœ€ä½³å€¼0.25ï¼‰ï¼Œåœ¨äººç±»æ„ŸçŸ¥å’Œé«˜ä¿çœŸå›¾åƒåˆæˆè´¨é‡ä¸Šè¡¨ç°ç›¸å½“ï¼ˆFIDä¸º14.3ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06033v1">PDF</a> 30 pages</p>
<p><strong>Summary</strong><br>     æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è¿›æ­¥å¼€å¯äº†æ•°å­—å†…å®¹ç”Ÿäº§çš„æ–°æ—¶ä»£ï¼Œå±•ç°å‡ºåŸºäºè‡ªç„¶è¯­è¨€æè¿°çš„è¯­ä¹‰ç”Ÿæˆé€¼çœŸä¸”é£æ ¼å¤šæ ·çš„å›¾åƒçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹æ— æ³•ç”Ÿæˆå¯è¯»ã€æœ‰æ„ä¹‰ä¸”æ‹¼å†™æ­£ç¡®çš„æ–‡æœ¬å›¾åƒï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å¹¿å‘Šã€å­¦ä¹ å’Œåˆ›æ„è®¾è®¡ç­‰å®ç”¨é¢†åŸŸçš„è¿ç”¨ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°çš„æ¡†æ¶â€”â€”Glyph-Conditioned Diffusion with Character-Aware Attentionï¼ˆGCDAï¼‰ï¼Œé€šè¿‡ä¸‰ä¸ªç²¾å¿ƒè®¾è®¡æ¨¡å—æ‰©å±•å…¸å‹çš„æ‰©æ•£ä¸»å¹²ã€‚è¯¥æ¨¡å‹å…·å¤‡åŒæ–‡æœ¬ç¼–ç å™¨ï¼Œç¼–ç è¯­ä¹‰ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œæ˜ç¡®çš„å­—å½¢è¡¨ç¤ºï¼Œå½¢æˆä¸°å¯Œçš„å­—ç¬¦æ„ŸçŸ¥æ–‡æœ¬è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæå‡ºå­—ç¬¦æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡æ–°çš„æ³¨æ„åŠ›åˆ†å‰²æŸå¤±æ¥ç‹¬ç«‹é™åˆ¶æ¯ä¸ªå­—ç¬¦çš„æ³¨æ„åŠ›åˆ†å¸ƒï¼Œé¿å…å¤±çœŸä¼ªå½±ã€‚æœ€åï¼ŒGCDAå…·æœ‰OCRé—­ç¯å¾®è°ƒé˜¶æ®µï¼Œé€šè¿‡å…¨æ–‡æ„ŸçŸ¥æŸå¤±ç›´æ¥ä¼˜åŒ–æ¨¡å‹çš„å¯è¯»æ€§å’Œæ‹¼å†™å‡†ç¡®æ€§ã€‚å¤§è§„æ¨¡å®éªŒç»“æœè¡¨æ˜ï¼ŒGCDAåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œæ–‡å­—æ¸²æŸ“çš„å­—ç¬¦åŸºç¡€æŒ‡æ ‡è¡¨ç°æ›´ä½³ï¼ˆå­—ç¬¦é”™è¯¯ç‡ï¼š0.08å¯¹æ¯”ä¹‹å‰æœ€ä½³çš„0.21ï¼›è¯é”™è¯¯ç‡ï¼š0.15å¯¹æ¯”0.25ï¼‰ï¼Œäººç±»æ„ŸçŸ¥ä¸å›¾åƒåˆæˆè´¨é‡ä¹Ÿç›¸å½“å‡ºè‰²ï¼ˆFIDï¼š14.3ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ–°æ—¶ä»£ï¼šå±•ç¤ºäº†åŸºäºè‡ªç„¶è¯­è¨€æè¿°çš„è¯­ä¹‰ç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„èƒ½åŠ›ã€‚</li>
<li>å½“å‰æ¨¡å‹çš„ä¸»è¦é™åˆ¶ï¼šæ— æ³•ç”Ÿæˆå¯è¯»ã€æœ‰æ„ä¹‰ä¸”æ‹¼å†™æ­£ç¡®çš„æ–‡æœ¬å›¾åƒã€‚</li>
<li>GCDAæ¡†æ¶çš„å¼•å…¥ï¼šé€šè¿‡ä¸‰ä¸ªæ¨¡å—æ‰©å±•å…¸å‹çš„æ‰©æ•£æ¨¡å‹ï¼ŒåŒ…æ‹¬åŒæ–‡æœ¬ç¼–ç å™¨ã€å­—ç¬¦æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶å’ŒOCRé—­ç¯å¾®è°ƒé˜¶æ®µã€‚</li>
<li>åŒæ–‡æœ¬ç¼–ç å™¨ï¼šç¼–ç è¯­ä¹‰ä¸Šä¸‹æ–‡å’Œå­—å½¢è¡¨ç¤ºï¼Œå½¢æˆä¸°å¯Œçš„å­—ç¬¦æ„ŸçŸ¥æ–‡æœ¬è¡¨ç¤ºã€‚</li>
<li>å­—ç¬¦æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶ï¼šé€šè¿‡æ³¨æ„åŠ›åˆ†å‰²æŸå¤±ç‹¬ç«‹é™åˆ¶æ¯ä¸ªå­—ç¬¦çš„æ³¨æ„åŠ›åˆ†å¸ƒã€‚</li>
<li>OCRé—­ç¯å¾®è°ƒé˜¶æ®µï¼šé€šè¿‡å…¨æ–‡æ„ŸçŸ¥æŸå¤±ä¼˜åŒ–æ¨¡å‹çš„å¯è¯»æ€§å’Œæ‹¼å†™å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06033">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-87a70a52c347fe38dee2d1b2abaec3a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd81f05d302501ff159c683e450e24e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b79ea19060780eba6af02e2b7e43ab7.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="T-LoRA-Single-Image-Diffusion-Model-Customization-Without-Overfitting"><a href="#T-LoRA-Single-Image-Diffusion-Model-Customization-Without-Overfitting" class="headerlink" title="T-LoRA: Single Image Diffusion Model Customization Without Overfitting"></a>T-LoRA: Single Image Diffusion Model Customization Without Overfitting</h2><p><strong>Authors:Vera Soboleva, Aibek Alanov, Andrey Kuznetsov, Konstantin Sobolev</strong></p>
<p>While diffusion model fine-tuning offers a powerful approach for customizing pre-trained models to generate specific objects, it frequently suffers from overfitting when training samples are limited, compromising both generalization capability and output diversity. This paper tackles the challenging yet most impactful task of adapting a diffusion model using just a single concept image, as single-image customization holds the greatest practical potential. We introduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework specifically designed for diffusion model personalization. In our work we show that higher diffusion timesteps are more prone to overfitting than lower ones, necessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates two key innovations: (1) a dynamic fine-tuning strategy that adjusts rank-constrained updates based on diffusion timesteps, and (2) a weight parametrization technique that ensures independence between adapter components through orthogonal initialization. Extensive experiments show that T-LoRA and its individual components outperform standard LoRA and other diffusion model personalization techniques. They achieve a superior balance between concept fidelity and text alignment, highlighting the potential of T-LoRA in data-limited and resource-constrained scenarios. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ControlGenAI/T-LoRA">https://github.com/ControlGenAI/T-LoRA</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å¾®è°ƒæä¾›äº†ä¸€ç§å¼ºå¤§çš„æ–¹æ³•æ¥å®šåˆ¶é¢„è®­ç»ƒæ¨¡å‹ä»¥ç”Ÿæˆç‰¹å®šå¯¹è±¡ï¼Œä½†å½“è®­ç»ƒæ ·æœ¬æœ‰é™æ—¶ï¼Œå®ƒç»å¸¸é­å—è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œè¿™æŸå®³äº†å…¶æ³›åŒ–èƒ½åŠ›å’Œè¾“å‡ºå¤šæ ·æ€§ã€‚æœ¬æ–‡è§£å†³äº†ä½¿ç”¨ä»…ä¸€ä¸ªæ¦‚å¿µå›¾åƒé€‚åº”æ‰©æ•£æ¨¡å‹çš„å…·æœ‰æŒ‘æˆ˜æ€§å’Œå½±å“åŠ›çš„ä»»åŠ¡ï¼Œå› ä¸ºå•å›¾åƒå®šåˆ¶å…·æœ‰æœ€å¤§çš„å®é™…åº”ç”¨æ½œåŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†T-LoRAï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºæ‰©æ•£æ¨¡å‹ä¸ªæ€§åŒ–çš„æ—¶åºä¾èµ–ä½ç§©é€‚åº”æ¡†æ¶ã€‚æˆ‘ä»¬çš„å·¥ä½œè¡¨æ˜ï¼Œè¾ƒé«˜çš„æ‰©æ•£æ—¶é—´æ­¥æ¯”ä½çš„æ—¶é—´æ­¥æ›´å®¹æ˜“å‡ºç°è¿‡æ‹Ÿåˆï¼Œè¿™éœ€è¦ä¸€ç§å¯¹æ—¶é—´æ­¥æ•æ„Ÿçš„å¾®è°ƒç­–ç•¥ã€‚T-LoRAåŒ…å«ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼š(1)ä¸€ç§åŠ¨æ€å¾®è°ƒç­–ç•¥ï¼Œæ ¹æ®æ‰©æ•£æ—¶é—´æ­¥é•¿è°ƒæ•´ç§©çº¦æŸæ›´æ–°ï¼›(2)ä¸€ç§æƒé‡å‚æ•°åŒ–æŠ€æœ¯ï¼Œé€šè¿‡æ­£äº¤åˆå§‹åŒ–ç¡®ä¿é€‚é…å™¨ç»„ä»¶ä¹‹é—´çš„ç‹¬ç«‹æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒT-LoRAåŠå…¶å„ä¸ªç»„ä»¶çš„æ€§èƒ½è¶…è¿‡äº†æ ‡å‡†çš„LoRAå’Œå…¶ä»–æ‰©æ•£æ¨¡å‹ä¸ªæ€§åŒ–æŠ€æœ¯ã€‚ä»–ä»¬åœ¨æ¦‚å¿µä¿çœŸåº¦å’Œæ–‡æœ¬å¯¹é½ä¹‹é—´è¾¾åˆ°äº†å‡ºè‰²çš„å¹³è¡¡ï¼Œçªæ˜¾äº†T-LoRAåœ¨æ•°æ®æœ‰é™å’Œèµ„æºå—é™åœºæ™¯ä¸­çš„æ½œåŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ControlGenAI/T-LoRA%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ControlGenAI/T-LoRAä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05964v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹å¾®è°ƒè™½ç„¶èƒ½å¤Ÿé’ˆå¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œä¸ªæ€§åŒ–å®šåˆ¶ä»¥ç”Ÿæˆç‰¹å®šå¯¹è±¡ï¼Œä½†åœ¨è®­ç»ƒæ ·æœ¬æœ‰é™çš„æƒ…å†µä¸‹å¸¸å¸¸ä¼šå‡ºç°è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå½±å“æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œè¾“å‡ºå¤šæ ·æ€§ã€‚æœ¬æ–‡å¼•å…¥T-LoRAæ¡†æ¶ï¼Œé‡‡ç”¨æ—¶é—´æ­¥ä¾èµ–çš„ä½ç§©é€‚åº”ç­–ç•¥ï¼Œé’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„ä¸ªæ€§åŒ–å®šåˆ¶æå‡ºè§£å†³æ–¹æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼ŒT-LoRAåŠå…¶ç»„ä»¶åœ¨æ¦‚å¿µä¿çœŸåº¦å’Œæ–‡æœ¬å¯¹é½æ–¹é¢å®ç°äº†å“è¶Šå¹³è¡¡ï¼Œå…·æœ‰æ•°æ®æœ‰é™å’Œèµ„æºå—é™åœºæ™¯ä¸‹çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å¾®è°ƒåœ¨æœ‰é™è®­ç»ƒæ ·æœ¬æ—¶å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œå½±å“æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œè¾“å‡ºå¤šæ ·æ€§ã€‚</li>
<li>T-LoRAæ¡†æ¶è¢«è®¾è®¡ç”¨äºæ‰©æ•£æ¨¡å‹çš„ä¸ªæ€§åŒ–å®šåˆ¶ã€‚</li>
<li>T-LoRAé‡‡ç”¨æ—¶é—´æ­¥ä¾èµ–çš„ä½ç§©é€‚åº”ç­–ç•¥ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ä¸­çš„é«˜æ—¶é—´æ­¥é•¿æ¯”ä½æ—¶é—´æ­¥é•¿æ›´å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œéœ€è¦ä¸€ç§æ—¶é—´æ­¥æ•æ„Ÿçš„å¾®è°ƒç­–ç•¥ã€‚</li>
<li>T-LoRAæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šåŠ¨æ€å¾®è°ƒç­–ç•¥å’Œæƒé‡å‚æ•°åŒ–æŠ€æœ¯ã€‚</li>
<li>åŠ¨æ€å¾®è°ƒç­–ç•¥æ ¹æ®æ‰©æ•£æ—¶é—´æ­¥é•¿è°ƒæ•´ç­‰çº§çº¦æŸæ›´æ–°ã€‚</li>
<li>T-LoRAåŠå…¶ç»„ä»¶åœ¨æ¦‚å¿µä¿çœŸåº¦å’Œæ–‡æœ¬å¯¹é½æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå…·æœ‰åœ¨æ•°æ®æœ‰é™å’Œèµ„æºå—é™åœºæ™¯ä¸‹çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05964">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a78521366488e7d9f0658f6026cb244e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3f0246fe0fdf475c0044997ced96568.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e7f44ce5affde97d672828da11976a0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Diffusion-Dataset-Condensation-Training-Your-Diffusion-Model-Faster-with-Less-Data"><a href="#Diffusion-Dataset-Condensation-Training-Your-Diffusion-Model-Faster-with-Less-Data" class="headerlink" title="Diffusion Dataset Condensation: Training Your Diffusion Model Faster   with Less Data"></a>Diffusion Dataset Condensation: Training Your Diffusion Model Faster   with Less Data</h2><p><strong>Authors:Rui Huang, Shitong Shao, Zikai Zhou, Pukun Zhao, Hangyu Guo, Tian Ye, Lichen Bai, Shuo Yang, Zeke Xie</strong></p>
<p>Diffusion models have achieved remarkable success in various generative tasks, but training them remains highly resource-intensive, often requiring millions of images and many days of GPU computation. From a data-centric perspective addressing this limitation, we study diffusion dataset condensation as a new and challenging problem setting. The goal is to construct a â€œsyntheticâ€ sub-dataset with significantly fewer samples than the original dataset, enabling high-quality diffusion model training with greatly reduced cost. To the best of our knowledge, we are the first to formally investigate dataset condensation for diffusion models, whereas prior work focused on training discriminative models. To tackle this new challenge, we propose a novel Diffusion Dataset Condensation (D2C) framework, which consists of two phases: Select and Attach. The Select phase identifies a compact and diverse subset using a diffusion difficulty score and interval sampling. The Attach phase enhances the selected subset by attaching rich semantic and visual representations to strengthen the conditional signals. Extensive experiments across various dataset sizes, model architectures, and resolutions show that our D2C framework enables significantly faster diffusion model training with dramatically fewer data, while preserving high visual quality. Notably, for the SiT-XL&#x2F;2 architecture, D2C achieves a 100x training speed-up, reaching a FID score of 4.3 in just 40k steps using only 0.8% of the training data. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å„ç§ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å…¶è®­ç»ƒä»ç„¶éœ€è¦å¤§é‡èµ„æºï¼Œé€šå¸¸éœ€è¦æ•°ç™¾ä¸‡å¼ å›¾åƒå’Œæ•°å¤©çš„GPUè®¡ç®—ã€‚ä»æ•°æ®ä¸­å¿ƒçš„è§†è§’æ¥è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬ç ”ç©¶æ‰©æ•£æ•°æ®é›†å‡èšä½œä¸ºä¸€ä¸ªæ–°çš„æœ‰æŒ‘æˆ˜æ€§çš„è®¾ç½®ã€‚ç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªä¸åŸå§‹æ•°æ®é›†ç›¸æ¯”æ ·æœ¬æ•°é‡æ˜¾è‘—è¾ƒå°‘çš„â€œåˆæˆâ€å­æ•°æ®é›†ï¼Œä»è€Œå®ç°é«˜è´¨é‡æ‰©æ•£æ¨¡å‹çš„ä½æˆæœ¬è®­ç»ƒã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯é¦–æ¬¡æ­£å¼ç ”ç©¶æ‰©æ•£æ¨¡å‹çš„æ•°æ®é›†å‡èšï¼Œè€Œä¹‹å‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨è®­ç»ƒåˆ¤åˆ«æ¨¡å‹ä¸Šã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æ–°æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ‰©æ•£æ•°æ®é›†å‡èšï¼ˆD2Cï¼‰æ¡†æ¶ï¼Œå®ƒåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé€‰æ‹©å’Œé™„åŠ ã€‚é€‰æ‹©é˜¶æ®µä½¿ç”¨æ‰©æ•£éš¾åº¦åˆ†æ•°å’Œé—´éš”é‡‡æ ·æ¥ç¡®å®šç´§å‡‘ä¸”å¤šæ ·åŒ–çš„å­é›†ã€‚é™„åŠ é˜¶æ®µé€šè¿‡ä¸ºæ‰€é€‰å­é›†æ·»åŠ ä¸°å¯Œçš„è¯­ä¹‰å’Œè§†è§‰è¡¨ç¤ºæ¥å¢å¼ºæ¡ä»¶ä¿¡å·ã€‚åœ¨ä¸åŒæ•°æ®é›†å¤§å°ã€æ¨¡å‹æ¶æ„å’Œåˆ†è¾¨ç‡çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„D2Cæ¡†æ¶èƒ½å¤Ÿåœ¨æ˜¾è‘—å‡å°‘æ•°æ®çš„åŒæ—¶å®ç°æ›´å¿«çš„æ‰©æ•£æ¨¡å‹è®­ç»ƒï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„è§†è§‰è´¨é‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¯¹äºSiT-XL&#x2F;2æ¶æ„ï¼ŒD2Cå®ç°äº†100å€çš„åŠ é€Ÿè®­ç»ƒé€Ÿåº¦ï¼Œä»…ä½¿ç”¨0.8%çš„è®­ç»ƒæ•°æ®åœ¨çŸ­çŸ­4ä¸‡æ­¥å†…è¾¾åˆ°äº†FIDåˆ†æ•°ä¸º4.3ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05914v1">PDF</a> Iintroduces D2C: a novel framework for diffusion dataset condensation</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨å„ç§ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å…¶è®­ç»ƒä»ç„¶éœ€è¦å¤§é‡çš„èµ„æºå’Œè®¡ç®—ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡ç ”ç©¶äº†æ‰©æ•£æ•°æ®é›†å‹ç¼©è¿™ä¸€æ–°çš„éš¾é¢˜ã€‚ç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªä¸åŸå§‹æ•°æ®é›†ç›¸æ¯”æ ·æœ¬æ•°é‡æ˜¾è‘—å‡å°‘çš„â€œåˆæˆâ€å­é›†ï¼Œä»è€Œä»¥å¤§å¹…é™ä½çš„æˆæœ¬å®ç°é«˜è´¨é‡çš„æ‰©æ•£æ¨¡å‹è®­ç»ƒã€‚æœ¬æ–‡é¦–æ¬¡æ­£å¼ç ”ç©¶æ‰©æ•£æ¨¡å‹çš„æ•°æ®é›†å‹ç¼©é—®é¢˜ï¼Œè€Œä¹‹å‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨åˆ¤åˆ«æ¨¡å‹çš„è®­ç»ƒä¸Šã€‚ä¸ºäº†è§£å†³è¿™ä¸€æ–°æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†Diffusion Dataset Condensation (D2C)æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬Selectå’ŒAttachä¸¤ä¸ªé˜¶æ®µã€‚Selecté˜¶æ®µä½¿ç”¨æ‰©æ•£éš¾åº¦åˆ†æ•°å’Œé—´éš”é‡‡æ ·æ¥è¯†åˆ«ç´§å‡‘ä¸”å¤šæ ·åŒ–çš„å­é›†ã€‚Attaché˜¶æ®µé€šè¿‡ä¸ºæ‰€é€‰å­é›†æ·»åŠ ä¸°å¯Œçš„è¯­ä¹‰å’Œè§†è§‰è¡¨ç¤ºæ¥åŠ å¼ºæ¡ä»¶ä¿¡å·ã€‚å®éªŒè¡¨æ˜ï¼ŒD2Cæ¡†æ¶èƒ½å¤Ÿåœ¨å¤§å¹…å‡å°‘æ•°æ®çš„åŒæ—¶ï¼Œå®ç°æ›´å¿«çš„æ‰©æ•£æ¨¡å‹è®­ç»ƒï¼Œå¹¶ä¿æŒè¾ƒé«˜çš„è§†è§‰è´¨é‡ã€‚ç‰¹åˆ«æ˜¯åœ¨SiT-XL&#x2F;2æ¶æ„ä¸Šï¼Œä½¿ç”¨D2Cæ–¹æ³•å®ç°äº†è®­ç»ƒé€Ÿåº¦çš„ç™¾å€åŠ é€Ÿï¼Œåœ¨ä»…ä½¿ç”¨0.8%çš„è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†FIDåˆ†æ•°ä¸º4.3çš„ä¼˜å¼‚è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜ç§€ï¼Œä½†è®­ç»ƒéœ€è¦å¤§é‡èµ„æºå’Œè®¡ç®—æˆæœ¬ã€‚</li>
<li>æ‰©æ•£æ•°æ®é›†å‹ç¼©æ—¨åœ¨æ„å»ºå«æœ‰è¾ƒå°‘æ ·æœ¬çš„åˆæˆå­é›†ï¼Œé™ä½æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒæˆæœ¬ã€‚</li>
<li>Diffusion Dataset Condensation (D2C)æ¡†æ¶åŒ…æ‹¬Selectå’ŒAttachä¸¤ä¸ªé˜¶æ®µæ¥è§£å†³æ‰©æ•£æ•°æ®é›†å‹ç¼©é—®é¢˜ã€‚</li>
<li>Selecté˜¶æ®µé€šè¿‡è¯†åˆ«ä¸€ä¸ªç´§å‡‘ä¸”å¤šæ ·åŒ–çš„å­é›†ä»¥å‹ç¼©æ•°æ®é›†è§„æ¨¡ã€‚</li>
<li>Attaché˜¶æ®µå¼ºåŒ–äº†æ¡ä»¶ä¿¡å·ï¼Œé€šè¿‡ä¸ºæ‰€é€‰å­é›†æ·»åŠ ä¸°å¯Œçš„è¯­ä¹‰å’Œè§†è§‰è¡¨ç¤ºæ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜D2Cæ¡†æ¶èƒ½æ˜¾è‘—åŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„è®­ç»ƒé€Ÿåº¦å¹¶å‡å°‘æ•°æ®ä½¿ç”¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05914">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c1024ae9d8adc3cb6feb27c9617129fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57521d83bf8a1ea1ba3c019a80ad609d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-338153e57650334a88faf5717bac01e9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-edebc364385139d64b4fa7b9284c7e55.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1657356beaf9b674683e73c8d4b14cea.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="RichControl-Structure-and-Appearance-Rich-Training-Free-Spatial-Control-for-Text-to-Image-Generation"><a href="#RichControl-Structure-and-Appearance-Rich-Training-Free-Spatial-Control-for-Text-to-Image-Generation" class="headerlink" title="RichControl: Structure- and Appearance-Rich Training-Free Spatial   Control for Text-to-Image Generation"></a>RichControl: Structure- and Appearance-Rich Training-Free Spatial   Control for Text-to-Image Generation</h2><p><strong>Authors:Liheng Zhang, Lexi Pang, Hang Ye, Xiaoxuan Ma, Yizhou Wang</strong></p>
<p>Text-to-image (T2I) diffusion models have shown remarkable success in generating high-quality images from text prompts. Recent efforts extend these models to incorporate conditional images (e.g., depth or pose maps) for fine-grained spatial control. Among them, feature injection methods have emerged as a training-free alternative to traditional fine-tuning approaches. However, they often suffer from structural misalignment, condition leakage, and visual artifacts, especially when the condition image diverges significantly from natural RGB distributions. By revisiting existing methods, we identify a core limitation: the synchronous injection of condition features fails to account for the trade-off between domain alignment and structural preservation during denoising. Inspired by this observation, we propose a flexible feature injection framework that decouples the injection timestep from the denoising process. At its core is a structure-rich injection module, which enables the model to better adapt to the evolving interplay between alignment and structure preservation throughout the diffusion steps, resulting in more faithful structural generation. In addition, we introduce appearance-rich prompting and a restart refinement strategy to further enhance appearance control and visual quality. Together, these designs enable training-free generation that is both structure-rich and appearance-rich. Extensive experiments show that our approach achieves state-of-the-art performance across diverse zero-shot conditioning scenarios. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚æœ€è¿‘çš„ç ”ç©¶åŠªåŠ›å°†è¿™äº›æ¨¡å‹æ‰©å±•åˆ°ç»“åˆæ¡ä»¶å›¾åƒï¼ˆä¾‹å¦‚æ·±åº¦æˆ–å§¿æ€å›¾ï¼‰ä»¥å®ç°ç²¾ç»†çš„ç©ºé—´æ§åˆ¶ã€‚å…¶ä¸­ï¼Œç‰¹å¾æ³¨å…¥æ–¹æ³•ä½œä¸ºä¸€ç§æ— éœ€è®­ç»ƒçš„ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•çš„æ›¿ä»£æ–¹æ¡ˆè€Œå‡ºç°ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¸¸å¸¸é­å—ç»“æ„é”™ä½ã€æ¡ä»¶æ³„éœ²å’Œè§†è§‰ä¼ªå½±ç­‰é—®é¢˜ï¼Œå°¤å…¶æ˜¯å½“æ¡ä»¶å›¾åƒä¸è‡ªç„¶RGBåˆ†å¸ƒç›¸å·®è¾ƒå¤§æ—¶ã€‚é€šè¿‡å¯¹ç°æœ‰æ–¹æ³•çš„é‡æ–°å®¡è§†ï¼Œæˆ‘ä»¬ç¡®å®šäº†ä¸€ä¸ªæ ¸å¿ƒé™åˆ¶ï¼šæ¡ä»¶ç‰¹å¾çš„åŒæ­¥æ³¨å…¥æœªèƒ½è€ƒè™‘åˆ°å»å™ªè¿‡ç¨‹ä¸­çš„åŸŸå¯¹é½ä¸ç»“æ„ä¿æŒä¹‹é—´çš„æƒè¡¡ã€‚å—æ­¤è§‚å¯Ÿçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§çµæ´»çš„ç‰¹å¾æ³¨å…¥æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†æ³¨å…¥æ—¶é—´ä¸å»å™ªè¿‡ç¨‹è§£è€¦ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªç»“æ„ä¸°å¯Œçš„æ³¨å…¥æ¨¡å—ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”æ‰©æ•£æ­¥éª¤ä¸­å¯¹é½ä¸ç»“æ„ä¿æŒä¹‹é—´çš„ä¸æ–­å˜åŒ–çš„ç›¸äº’ä½œç”¨ï¼Œä»è€Œå®ç°æ›´å¿ å®çš„ç»“æ„ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸°å¯Œçš„å¤–è§‚æç¤ºå’Œé‡å¯ç»†åŒ–ç­–ç•¥ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºå¤–è§‚æ§åˆ¶å’Œè§†è§‰è´¨é‡ã€‚è¿™äº›è®¾è®¡å…±åŒå®ç°äº†æ— éœ€è®­ç»ƒå³å¯ç”Ÿæˆæ—¢ä¸°å¯Œç»“æ„åˆä¸°å¯Œå¤–è§‚çš„å›¾åƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§é›¶æ ·æœ¬æ¡ä»¶åœºæ™¯ä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02792v2">PDF</a> arXiv admin note: text overlap with arXiv:2406.07540 by other authors</p>
<p><strong>æ‘˜è¦</strong><br>    æ–‡æœ¬è½¬å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹é€šè¿‡æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚è¿‘æœŸç ”ç©¶å°è¯•å°†æ¡ä»¶å›¾åƒï¼ˆå¦‚æ·±åº¦æˆ–å§¿æ€å›¾ï¼‰èå…¥è¿™äº›æ¨¡å‹ï¼Œä»¥å®ç°ç²¾ç»†çš„ç©ºé—´æ§åˆ¶ã€‚ç‰¹å¾æ³¨å…¥æ–¹æ³•ä½œä¸ºä¸€ç§æ— éœ€è®­ç»ƒçš„ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•çš„æ›¿ä»£æ–¹æ¡ˆåº”è¿è€Œç”Ÿã€‚ç„¶è€Œï¼Œå½“æ¡ä»¶å›¾åƒä¸è‡ªç„¶çš„RGBåˆ†å¸ƒå·®å¼‚è¾ƒå¤§æ—¶ï¼Œå®ƒä»¬å¸¸é¢ä¸´ç»“æ„é”™ä½ã€æ¡ä»¶æ³„éœ²å’Œè§†è§‰ä¼ªå½±ç­‰é—®é¢˜ã€‚æœ¬æ–‡é€šè¿‡é‡æ–°å®¡è§†ç°æœ‰æ–¹æ³•ï¼Œå‘ç°æ ¸å¿ƒé™åˆ¶åœ¨äºåŒæ­¥æ³¨å…¥æ¡ä»¶ç‰¹å¾æ—¶ï¼Œæœªèƒ½è€ƒè™‘åŸŸå¯¹é½ä¸ç»“æ„ä¿ç•™ä¹‹é—´çš„æƒè¡¡ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæœ¬æ–‡æå‡ºä¸€ç§çµæ´»çš„ç‰¹å¾æ³¨å…¥æ¡†æ¶ï¼Œå°†æ³¨å…¥æ—¶é—´ä¸å»å™ªè¿‡ç¨‹è§£è€¦ã€‚å…¶æ ¸å¿ƒçš„ç»“æ„ä¸°å¯Œæ³¨å…¥æ¨¡å—ä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”æ‰©æ•£æ­¥éª¤ä¸­åŸŸå¯¹é½ä¸ç»“æ„ä¿ç•™ä¹‹é—´çš„ä¸æ–­å˜åŒ–çš„æƒè¡¡ï¼Œä»è€Œå®ç°æ›´å¿ å®äºåŸç»“æ„çš„ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥å¤–è§‚ä¸°å¯Œçš„æç¤ºå’Œé‡å¯ç»†åŒ–ç­–ç•¥ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†å¤–è§‚æ§åˆ¶å’Œè§†è§‰è´¨é‡ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›è®¾è®¡å®ç°äº†æ— éœ€è®­ç»ƒçš„ç»“æ„ä¸°å¯Œå’Œå¤–è§‚ä¸°å¯Œçš„ç”Ÿæˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨ä¸åŒé›¶æ ·æœ¬æ¡ä»¶åœºæ™¯ä¸‹é¢å‘åº”ç”¨è·å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡æœ¬è½¬å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿé€šè¿‡æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚</li>
<li>è¿‘æœŸç ”ç©¶å°è¯•èå…¥æ¡ä»¶å›¾åƒä»¥å®ç°æ›´ç²¾ç»†çš„ç©ºé—´æ§åˆ¶ã€‚</li>
<li>ç‰¹å¾æ³¨å…¥æ–¹æ³•ä½œä¸ºä¸€ç§æ— éœ€è®­ç»ƒçš„å¾®è°ƒæ–¹æ³•çš„æ›¿ä»£æ–¹æ¡ˆå—åˆ°å…³æ³¨ã€‚</li>
<li>ç‰¹å¾æ³¨å…¥é¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜æ˜¯ç»“æ„é”™ä½ã€æ¡ä»¶æ³„éœ²å’Œè§†è§‰ä¼ªå½±ç­‰é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åŒæ­¥æ³¨å…¥æ¡ä»¶ç‰¹å¾æ—¶æœªå……åˆ†æƒè¡¡åŸŸå¯¹é½ä¸ç»“æ„ä¿ç•™ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„çµæ´»ç‰¹å¾æ³¨å…¥æ¡†æ¶èƒ½å¤Ÿè§£è€¦æ³¨å…¥æ—¶é—´ä¸å»å™ªè¿‡ç¨‹ï¼Œå®ç°æ›´å¿ å®äºåŸç»“æ„çš„ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02792">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb95d74a25d8959653dcb1b0f6b67f2c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9fb874ca598f953741aa1e7fa806ade.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e027642ba4dc78cc06f07534d38dbb4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9680d97bd7ad50a53bbbc66a97534a0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CaO-2-Rectifying-Inconsistencies-in-Diffusion-Based-Dataset-Distillation"><a href="#CaO-2-Rectifying-Inconsistencies-in-Diffusion-Based-Dataset-Distillation" class="headerlink" title="CaO$_2$: Rectifying Inconsistencies in Diffusion-Based Dataset   Distillation"></a>CaO$_2$: Rectifying Inconsistencies in Diffusion-Based Dataset   Distillation</h2><p><strong>Authors:Haoxuan Wang, Zhenghao Zhao, Junyi Wu, Yuzhang Shang, Gaowen Liu, Yan Yan</strong></p>
<p>The recent introduction of diffusion models in dataset distillation has shown promising potential in creating compact surrogate datasets for large, high-resolution target datasets, offering improved efficiency and performance over traditional bi-level&#x2F;uni-level optimization methods. However, current diffusion-based dataset distillation approaches overlook the evaluation process and exhibit two critical inconsistencies in the distillation process: (1) Objective Inconsistency, where the distillation process diverges from the evaluation objective, and (2) Condition Inconsistency, leading to mismatches between generated images and their corresponding conditions. To resolve these issues, we introduce Condition-aware Optimization with Objective-guided Sampling (CaO$_2$), a two-stage diffusion-based framework that aligns the distillation process with the evaluation objective. The first stage employs a probability-informed sample selection pipeline, while the second stage refines the corresponding latent representations to improve conditional likelihood. CaO$_2$ achieves state-of-the-art performance on ImageNet and its subsets, surpassing the best-performing baselines by an average of 2.3% accuracy. </p>
<blockquote>
<p>åœ¨æ•°æ®é›†è’¸é¦ä¸­æœ€è¿‘å¼•å…¥çš„æ‰©æ•£æ¨¡å‹æ˜¾ç¤ºå‡ºåœ¨ä¸ºç›®æ ‡çš„å¤§å‹é«˜åˆ†è¾¨ç‡æ•°æ®é›†åˆ›å»ºç´§å‡‘æ›¿ä»£æ•°æ®é›†æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„åŒå‘æˆ–å•å‘ä¼˜åŒ–æ–¹æ³•ï¼Œå®ƒæé«˜äº†æ•ˆç‡å’Œæ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“å‰çš„åŸºäºæ‰©æ•£çš„æ•°æ®é›†è’¸é¦æ–¹æ³•å¿½ç•¥äº†è¯„ä¼°è¿‡ç¨‹ï¼Œå¹¶åœ¨è’¸é¦è¿‡ç¨‹ä¸­è¡¨ç°å‡ºä¸¤ä¸ªå…³é”®çš„ä¸ä¸€è‡´æ€§ï¼šï¼ˆ1ï¼‰ç›®æ ‡ä¸ä¸€è‡´æ€§ï¼Œå³è’¸é¦è¿‡ç¨‹ä¸è¯„ä¼°ç›®æ ‡ç›¸åç¦»ï¼›ï¼ˆ2ï¼‰æ¡ä»¶ä¸ä¸€è‡´æ€§ï¼Œå¯¼è‡´ç”Ÿæˆçš„å›¾åƒä¸å…¶å¯¹åº”çš„æ¡ä»¶ä¸åŒ¹é…ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæ¡ä»¶æ„ŸçŸ¥ä¼˜åŒ–ä¸ç›®æ ‡å¯¼å‘é‡‡æ ·ï¼ˆCaO$_2$ï¼‰çš„ä¸¤é˜¶æ®µæ‰©æ•£æ¡†æ¶ï¼Œä½¿è’¸é¦è¿‡ç¨‹ä¸è¯„ä¼°ç›®æ ‡ä¿æŒä¸€è‡´ã€‚ç¬¬ä¸€é˜¶æ®µé‡‡ç”¨åŸºäºæ¦‚ç‡çš„æ ·æœ¬é€‰æ‹©ç®¡é“ï¼Œè€Œç¬¬äºŒé˜¶æ®µåˆ™å¯¹ç›¸åº”çš„æ½œåœ¨è¡¨ç¤ºè¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜æ¡ä»¶æ¦‚ç‡ã€‚CaO$_2$åœ¨ImageNetåŠå…¶å­é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡å‡†ç¡®ç‡é«˜äºæœ€ä½³åŸºçº¿2.3%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22637v2">PDF</a> ICCV 2025. Code is available at   <a target="_blank" rel="noopener" href="https://github.com/hatchetProject/CaO2">https://github.com/hatchetProject/CaO2</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨æ•°æ®é›†è’¸é¦ä¸­çš„æœ€æ–°å¼•å…¥ï¼Œä¸ºåˆ›å»ºé’ˆå¯¹å¤§å‹é«˜åˆ†è¾¨ç‡ç›®æ ‡æ•°æ®é›†çš„é«˜æ•ˆç´§å‡‘æ›¿ä»£æ•°æ®é›†å±•ç¤ºäº†æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºæ‰©æ•£çš„æ•°æ®é›†è’¸é¦æ–¹æ³•å¿½è§†äº†è¯„ä¼°è¿‡ç¨‹ï¼Œå¹¶åœ¨è’¸é¦è¿‡ç¨‹ä¸­è¡¨ç°å‡ºä¸¤ä¸ªå…³é”®çš„ä¸ä¸€è‡´æ€§ï¼šç›®æ ‡ä¸ä¸€è‡´å’Œæ¡ä»¶ä¸ä¸€è‡´ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºç›®æ ‡å¼•å¯¼é‡‡æ ·çš„æ¡ä»¶æ„ŸçŸ¥ä¼˜åŒ–ï¼ˆCaO2ï¼‰çš„ä¸¤é˜¶æ®µæ‰©æ•£æ¡†æ¶ï¼Œä½¿è’¸é¦è¿‡ç¨‹ä¸è¯„ä¼°ç›®æ ‡ä¿æŒä¸€è‡´ã€‚CaO2åœ¨ImageNetåŠå…¶å­é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡å‡†ç¡®ç‡æ¯”æœ€ä½³åŸºçº¿é«˜å‡º2.3%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ•°æ®é›†è’¸é¦ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œèƒ½åˆ›å»ºé’ˆå¯¹å¤§å‹é«˜åˆ†è¾¨ç‡ç›®æ ‡æ•°æ®é›†çš„ç´§å‡‘æ›¿ä»£æ•°æ®é›†ã€‚</li>
<li>å½“å‰æ‰©æ•£æ¨¡å‹åœ¨è’¸é¦è¿‡ç¨‹ä¸­å­˜åœ¨ä¸¤ä¸ªå…³é”®çš„ä¸ä¸€è‡´æ€§ï¼šç›®æ ‡ä¸ä¸€è‡´å’Œæ¡ä»¶ä¸ä¸€è‡´ã€‚</li>
<li>æ¡ä»¶æ„ŸçŸ¥ä¼˜åŒ–ï¼ˆCaO2ï¼‰æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸¤ä¸ªä¸ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>CaO2é€šè¿‡æ¦‚ç‡ä¿¡æ¯é‡‡æ ·ç®¡é“çš„ç¬¬ä¸€é˜¶æ®µå’Œç¬¬äºŒé˜¶æ®µæ”¹è¿›æ½œåœ¨è¡¨ç¤ºï¼Œæé«˜æ¡ä»¶æ¦‚ç‡çš„ç²¾åº¦ã€‚</li>
<li>CaO2åœ¨ImageNetåŠå…¶å­é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>CaO2ç›¸æ¯”æœ€ä½³åŸºçº¿ï¼Œå¹³å‡å‡†ç¡®ç‡æé«˜äº†2.3%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22637">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2327670de84d1dcd3ee8051a99bcb597.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ead7216f3b385e39978f6932e90426c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d4741d41743cd2d952c8a0e34b14d65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bc47176477158cb9fbcce1e6bf65ebe.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="AI-GenBench-A-New-Ongoing-Benchmark-for-AI-Generated-Image-Detection"><a href="#AI-GenBench-A-New-Ongoing-Benchmark-for-AI-Generated-Image-Detection" class="headerlink" title="AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection"></a>AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection</h2><p><strong>Authors:Lorenzo Pellegrini, Davide Cozzolino, Serafino Pandolfini, Davide Maltoni, Matteo Ferrara, Luisa Verdoliva, Marco Prati, Marco Ramilli</strong></p>
<p>The rapid advancement of generative AI has revolutionized image creation, enabling high-quality synthesis from text prompts while raising critical challenges for media authenticity. We present Ai-GenBench, a novel benchmark designed to address the urgent need for robust detection of AI-generated images in real-world scenarios. Unlike existing solutions that evaluate models on static datasets, Ai-GenBench introduces a temporal evaluation framework where detection methods are incrementally trained on synthetic images, historically ordered by their generative models, to test their ability to generalize to new generative models, such as the transition from GANs to diffusion models. Our benchmark focuses on high-quality, diverse visual content and overcomes key limitations of current approaches, including arbitrary dataset splits, unfair comparisons, and excessive computational demands. Ai-GenBench provides a comprehensive dataset, a standardized evaluation protocol, and accessible tools for both researchers and non-experts (e.g., journalists, fact-checkers), ensuring reproducibility while maintaining practical training requirements. By establishing clear evaluation rules and controlled augmentation strategies, Ai-GenBench enables meaningful comparison of detection methods and scalable solutions. Code and data are publicly available to ensure reproducibility and to support the development of robust forensic detectors to keep pace with the rise of new synthetic generators. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ç”Ÿæˆçš„å›¾åƒç”ŸæˆæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œå·²ç»å½»åº•æ”¹å˜äº†å›¾åƒåˆ›ä½œçš„é¢è²Œï¼Œä½¿å¾—å¯ä»¥æ ¹æ®æ–‡å­—æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼ŒåŒæ—¶ä¹Ÿä¸ºåª’ä½“çœŸå®æ€§å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†Ai-GenBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³ç°å®åœºæ™¯ä¸­æ£€æµ‹AIç”Ÿæˆå›¾åƒçš„éœ€æ±‚çš„è¿«åˆ‡é—®é¢˜ã€‚ä¸ç°æœ‰åœ¨é™æ€æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹çš„æ–¹æ³•ä¸åŒï¼ŒAi-GenBenchå¼•å…¥äº†ä¸€ä¸ªæ—¶é—´è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æŒ‰ç”Ÿæˆæ¨¡å‹çš„é¡ºåºé€æ­¥è®­ç»ƒæ£€æµ‹ç®—æ³•ï¼Œä»¥æµ‹è¯•å…¶é€‚åº”æ–°ç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›ï¼Œä¾‹å¦‚ä»ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰åˆ°æ‰©æ•£æ¨¡å‹çš„è¿‡æ¸¡ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¾§é‡äºé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„è§†è§‰å†…å®¹ï¼Œå¹¶å…‹æœäº†å½“å‰æ–¹æ³•çš„å…³é”®å±€é™æ€§ï¼ŒåŒ…æ‹¬ä»»æ„çš„æ•°æ®é›†åˆ†å‰²ã€ä¸å…¬å¹³çš„æ¯”è¾ƒå’Œè¿‡é«˜çš„è®¡ç®—éœ€æ±‚ã€‚Ai-GenBenchä¸ºç ”ç©¶äººå‘˜å’Œéä¸“ä¸šäººå£«ï¼ˆä¾‹å¦‚è®°è€…ã€äº‹å®æ ¸æŸ¥äººå‘˜ï¼‰æä¾›äº†ç»¼åˆæ•°æ®é›†ã€æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®å’Œæ˜“äºä½¿ç”¨çš„å·¥å…·ï¼Œç¡®ä¿å®é™…è®­ç»ƒçš„å®ç”¨æ€§åŒæ—¶ä¿æŒå¯é‡å¤æ€§ã€‚é€šè¿‡åˆ¶å®šæ˜ç¡®çš„è¯„ä¼°è§„åˆ™å’Œæ§åˆ¶çš„å¢å¼ºç­–ç•¥ï¼ŒAi-GenBenchä½¿å¾—æ£€æµ‹æ–¹æ³•çš„æ¯”è¾ƒå’Œå¯æ‰©å±•è§£å†³æ–¹æ¡ˆå˜å¾—æ›´æœ‰æ„ä¹‰ã€‚ä»£ç å’Œæ•°æ®å…¬å¼€å¯ç”¨ï¼Œä»¥ç¡®ä¿å¯é‡å¤æ€§å¹¶æ”¯æŒç¨³å¥çš„å–è¯æ£€æµ‹å™¨çš„å¼€å‘ï¼Œä»¥è·Ÿä¸Šæ–°çš„åˆæˆç”Ÿæˆå™¨çš„æ­¥ä¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20865v2">PDF</a> Accepted at Verimedia workshop, IJCNN 2025. 9 pages, 6 figures, 4   tables, code available: <a target="_blank" rel="noopener" href="https://github.com/MI-BioLab/AI-GenBench">https://github.com/MI-BioLab/AI-GenBench</a></p>
<p><strong>Summary</strong><br>     ç”Ÿæˆå¼AIçš„å¿«é€Ÿè¿›æ­¥ä¸ºå›¾åƒåˆ›ä½œå¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ï¼Œå®ç°äº†åŸºäºæ–‡æœ¬æç¤ºçš„é«˜è´¨é‡åˆæˆï¼ŒåŒæ—¶å¯¹åª’ä½“çœŸå®æ€§æå‡ºäº†ä¸¥å³»æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†Ai-GenBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨è§£å†³ç°å®åœºæ™¯ä¸­æ£€æµ‹AIç”Ÿæˆå›¾åƒçš„éœ€æ±‚ã€‚ä¸åŒäºç°æœ‰ä»…åœ¨é™æ€æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹çš„è§£å†³æ–¹æ¡ˆï¼ŒAi-GenBenchå¼•å…¥äº†ä¸€ä¸ªä¸´æ—¶è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æŒ‰ç”Ÿæˆæ¨¡å‹çš„é¡ºåºé€æ­¥å¯¹åˆæˆå›¾åƒè¿›è¡Œè®­ç»ƒï¼Œä»¥æµ‹è¯•å…¶é€‚åº”æ–°ç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚ä»ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å‘æ‰©æ•£æ¨¡å‹è¿‡æ¸¡ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å¹³å°ä¾§é‡äºé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„è§†è§‰å†…å®¹ï¼Œå¹¶å…‹æœäº†å½“å‰æ–¹æ³•çš„å…³é”®å±€é™æ€§ï¼ŒåŒ…æ‹¬ä»»æ„æ•°æ®é›†åˆ†å‰²ã€ä¸å…¬å¹³æ¯”è¾ƒå’Œè¿‡é«˜çš„è®¡ç®—éœ€æ±‚ã€‚Ai-GenBenchä¸ºç ”ç©¶äººå‘˜å’Œéä¸“å®¶ï¼ˆå¦‚è®°è€…ã€äº‹å®æ ¸æŸ¥äººå‘˜ï¼‰æä¾›äº†ç»¼åˆæ•°æ®é›†ã€æ ‡å‡†åŒ–è¯„ä¼°åè®®å’Œå¯ç”¨å·¥å…·ï¼Œç¡®ä¿äº†å¯é‡å¤æ€§å¹¶æ»¡è¶³äº†å®é™…çš„è®­ç»ƒéœ€æ±‚ã€‚é€šè¿‡åˆ¶å®šæ˜ç¡®çš„è¯„ä¼°è§„åˆ™å’Œå—æ§çš„å¢å¼ºç­–ç•¥ï¼ŒAi-GenBenchä½¿å¾—æ£€æµ‹æ–¹æ³•çš„æ¯”è¾ƒå’Œå¯æ‰©å±•è§£å†³æ–¹æ¡ˆå˜å¾—æ›´æœ‰æ„ä¹‰ã€‚ä»£ç å’Œæ•°æ®å…¬å¼€å¯ç”¨ï¼Œä»¥ç¡®ä¿å¯é‡å¤æ€§å’Œæ”¯æŒç¨³å¥çš„å–è¯æ£€æµ‹å™¨çš„å¼€å‘ï¼Œä»¥åº”å¯¹æ–°çš„åˆæˆç”Ÿæˆå™¨çš„å´›èµ·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼AIçš„è¿›æ­¥æ¨åŠ¨äº†å›¾åƒåˆ›ä½œçš„é©å‘½ï¼Œå®ç°äº†åŸºäºæ–‡æœ¬çš„é«˜è´¨é‡å›¾åƒåˆæˆã€‚</li>
<li>Ai-GenBenchæ˜¯ä¸€ä¸ªæ–°å‹åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨è§£å†³ç°å®åœºæ™¯ä¸­æ£€æµ‹AIç”Ÿæˆå›¾åƒçš„éœ€æ±‚ã€‚</li>
<li>ä¸åŒäºå…¶ä»–è§£å†³æ–¹æ¡ˆï¼ŒAi-GenBenché‡‡ç”¨ä¸´æ—¶è¯„ä¼°æ¡†æ¶æ¥æµ‹è¯•æ¨¡å‹é€‚åº”æ–°ç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›ã€‚</li>
<li>Ai-GenBenchä¾§é‡äºé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„è§†è§‰å†…å®¹ï¼Œå¹¶å…‹æœäº†å½“å‰æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>å¹³å°ä¸ºç ”ç©¶äººå‘˜å’Œéä¸“å®¶æä¾›äº†ç»¼åˆæ•°æ®é›†ã€æ ‡å‡†åŒ–è¯„ä¼°åè®®å’Œå·¥å…·ã€‚</li>
<li>Ai-GenBenchå»ºç«‹äº†æ˜ç¡®çš„è¯„ä¼°è§„åˆ™å’Œå—æ§çš„å¢å¼ºç­–ç•¥ï¼Œä¾¿äºæ£€æµ‹æ–¹æ³•çš„æ¯”è¾ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20865">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-efe497fba0a0396a563ae6238a3f68b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76d01afa0c1f7ea9d95ed022e36361c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b0c3c597ba9d9d43689f3d7779b01c44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ba876161a9a9a455a9fb21fe5e44811.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db194f98bd94c084f68a365b3bf6c8cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ceb9d0654a8712c1fbe0c3fa606fcec8.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Revisiting-Likelihood-Based-Out-of-Distribution-Detection-by-Modeling-Representations"><a href="#Revisiting-Likelihood-Based-Out-of-Distribution-Detection-by-Modeling-Representations" class="headerlink" title="Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling   Representations"></a>Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling   Representations</h2><p><strong>Authors:Yifan Ding, Arturas Aleksandraus, Amirhossein Ahmadian, Jonas Unger, Fredrik Lindsten, Gabriel Eilertsen</strong></p>
<p>Out-of-distribution (OOD) detection is critical for ensuring the reliability of deep learning systems, particularly in safety-critical applications. Likelihood-based deep generative models have historically faced criticism for their unsatisfactory performance in OOD detection, often assigning higher likelihood to OOD data than in-distribution samples when applied to image data. In this work, we demonstrate that likelihood is not inherently flawed. Rather, several properties in the images space prohibit likelihood as a valid detection score. Given a sufficiently good likelihood estimator, specifically using the probability flow formulation of a diffusion model, we show that likelihood-based methods can still perform on par with state-of-the-art methods when applied in the representation space of pre-trained encoders. The code of our work can be found at $\href{<a target="_blank" rel="noopener" href="https://github.com/limchaos/Likelihood-OOD.git%7D%7B/texttt%7Bhttps://github.com/limchaos/Likelihood-OOD.git%7D%7D$">https://github.com/limchaos/Likelihood-OOD.git}{\texttt{https://github.com/limchaos/Likelihood-OOD.git}}$</a>. </p>
<blockquote>
<p>å¼‚å¸¸æ£€æµ‹ï¼ˆOODï¼‰å¯¹äºç¡®ä¿æ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„å¯é æ€§è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­ã€‚åŸºäºå¯èƒ½æ€§çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨å†å²ä¸Šé¢ä¸´ç€åœ¨å¼‚å¸¸æ£€æµ‹ä¸­è¡¨ç°ä¸ä½³çš„æ‰¹è¯„ï¼Œåœ¨åº”ç”¨äºå›¾åƒæ•°æ®æ—¶ï¼Œå®ƒä»¬å¾€å¾€ä¼šç»™å¼‚å¸¸æ•°æ®åˆ†é…æ›´é«˜çš„å¯èƒ½æ€§ï¼Œè€Œä¸æ˜¯åˆ†é…ç»™å†…éƒ¨æ•°æ®æ ·æœ¬ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†å¯èƒ½æ€§æœ¬èº«å¹¶æ²¡æœ‰ç¼ºé™·ã€‚ç›¸åï¼Œå›¾åƒç©ºé—´ä¸­çš„å‡ ä¸ªå±æ€§ç¦æ­¢å°†å¯èƒ½æ€§ä½œä¸ºæœ‰æ•ˆçš„æ£€æµ‹åˆ†æ•°ã€‚ç»™å®šä¸€ä¸ªè¶³å¤Ÿå¥½çš„å¯èƒ½æ€§ä¼°è®¡å™¨ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„æ¦‚ç‡æµå…¬å¼ï¼Œæˆ‘ä»¬è¡¨æ˜åœ¨åº”ç”¨äºé¢„è®­ç»ƒç¼–ç å™¨çš„è¡¨ç¤ºç©ºé—´æ—¶ï¼ŒåŸºäºå¯èƒ½æ€§çš„æ–¹æ³•ä»ç„¶å¯ä»¥ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸åª²ç¾ã€‚æˆ‘ä»¬å·¥ä½œçš„ä»£ç å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/limchaos/Likelihood-OOD.git">https://github.com/limchaos/Likelihood-OOD.git</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07793v2">PDF</a> Scandinavian Conference on Image Analysis 2025 (oral)</p>
<p><strong>Summary</strong><br>     æœ¬å·¥ä½œæŒ‡å‡ºï¼Œå¯¹äºå›¾åƒæ•°æ®ï¼Œåˆ©ç”¨é¢„è®­ç»ƒç¼–ç å™¨çš„è¡¨ç¤ºç©ºé—´ï¼Œåˆ©ç”¨æ¦‚ç‡æµå…¬å¼çš„æ‰©æ•£æ¨¡å‹ä½œä¸ºå¥½çš„å¯èƒ½æ€§ä¼°è®¡å™¨ï¼ŒåŸºäºå¯èƒ½æ€§çš„æ–¹æ³•ä»èƒ½ä¸æœ€å…ˆè¿›çš„OODæ£€æµ‹æ–¹æ³•è¡¨ç°ç›¸å½“ã€‚ä»¥å¾€å¯¹åŸºäºå¯èƒ½æ€§çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨OODæ£€æµ‹ä¸­çš„è¡¨ç°ä¸ä½³çš„æ‰¹è¯„å¹¶éæºäºå…¶æœ¬è´¨ç¼ºé™·ã€‚ç›¸åï¼Œå›¾åƒç©ºé—´ä¸­çš„æŸäº›ç‰¹æ€§é˜»ç¢äº†å¯èƒ½æ€§ä½œä¸ºæœ‰æ•ˆçš„æ£€æµ‹åˆ†æ•°ã€‚å› æ­¤ï¼Œåœ¨åˆé€‚çš„æ¡ä»¶ä¸‹ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹æ”¹è¿›åçš„å¯èƒ½æ€§è¯„ä¼°æœºåˆ¶èƒ½å¤Ÿæé«˜æ¨¡å‹çš„å¯é æ€§ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡é“¾æ¥è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡è¦æ€§ï¼šOODæ£€æµ‹å¯¹äºç¡®ä¿æ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„å¯é æ€§è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­ã€‚</li>
<li>åŸºäºå¯èƒ½æ€§çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨è¿‡å»å› å…¶åœ¨OODæ£€æµ‹ä¸­çš„è¡¨ç°ä¸ä½³è€Œå—åˆ°æ‰¹è¯„ã€‚ä½†åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼ˆä¾‹å¦‚ä½¿ç”¨é€‚å½“çš„å¯èƒ½æ€§ä¼°è®¡å™¨ï¼‰ï¼Œå…¶æ€§èƒ½å¯ä»¥å¾—åˆ°æå‡ã€‚</li>
<li>å›¾åƒç©ºé—´çš„æŸäº›ç‰¹æ€§å¯èƒ½ä¼šå¦¨ç¢åŸºäºå¯èƒ½æ€§çš„æ£€æµ‹æ–¹æ³•åœ¨OODè¯†åˆ«æ–¹é¢çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œåœ¨é¢„è®­ç»ƒç¼–ç å™¨çš„è¡¨ç¤ºç©ºé—´ä¸­åº”ç”¨è¿™äº›æ–¹æ³•æ—¶ï¼Œå¯èƒ½è·å¾—æ›´å¥½çš„ç»“æœã€‚</li>
<li>ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„æ¦‚ç‡æµå…¬å¼ä½œä¸ºå¯èƒ½æ€§ä¼°è®¡å™¨æ˜¯æœ¬æ–‡çš„ä¸€ä¸ªå…³é”®åˆ›æ–°ç‚¹ã€‚è¿™è¡¨æ˜é€‚å½“çš„æ¨¡å‹æ¶æ„æ”¹è¿›å¯ä»¥æ˜¾è‘—æé«˜åŸºäºå¯èƒ½æ€§çš„æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>åŸºäºå¯èƒ½æ€§çš„æ–¹æ³•ä¸æœ€å…ˆè¿›çš„OODæ£€æµ‹æ–¹æ³•è¡¨ç°ç›¸å½“ï¼Œè¡¨æ˜åœ¨åˆé€‚çš„æ¡ä»¶ä¸‹ï¼Œè¿™äº›æ–¹æ³•å…·æœ‰æ½œåœ¨çš„ä¼˜è¶Šæ€§ã€‚è¿™ä¸ºæœªæ¥å¼€å‘æ›´ä¸ºå¯é çš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿæä¾›äº†é‡è¦çš„å‚è€ƒæ–¹å‘ã€‚  </li>
<li>åœ¨å®éªŒå’Œæµ‹è¯•ä¸­åº”æ³¨æ„è‰¯å¥½çš„å¯èƒ½æ€§ä¼°è®¡å™¨çš„æœ‰æ•ˆæ€§ä»¥åŠå›¾åƒç©ºé—´ç‰¹æ€§çš„å½±å“ã€‚é€šè¿‡ä¼˜åŒ–è¿™äº›å› ç´ ï¼Œå¯ä»¥æé«˜åŸºäºå¯èƒ½æ€§çš„æ–¹æ³•åœ¨OODæ£€æµ‹ä¸­çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07793">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d4cd3b4032dc892568eb11e06e1c41e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5277f8e9841b48329f4ac1092e4de5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30a5d7b2d126d6163d0396a867c39b66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a2be2096465d08fc90357df24ea8d8b.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="OMR-Diffusion-Optimizing-Multi-Round-Enhanced-Training-in-Diffusion-Models-for-Improved-Intent-Understanding"><a href="#OMR-Diffusion-Optimizing-Multi-Round-Enhanced-Training-in-Diffusion-Models-for-Improved-Intent-Understanding" class="headerlink" title="OMR-Diffusion:Optimizing Multi-Round Enhanced Training in Diffusion   Models for Improved Intent Understanding"></a>OMR-Diffusion:Optimizing Multi-Round Enhanced Training in Diffusion   Models for Improved Intent Understanding</h2><p><strong>Authors:Kun Li, Jianhui Wang, Miao Zhang, Xueqian Wang</strong></p>
<p>Generative AI has significantly advanced text-driven image generation, but it still faces challenges in producing outputs that consistently align with evolving user preferences and intents, particularly in multi-turn dialogue scenarios. In this research, We present a Visual Co-Adaptation (VCA) framework that incorporates human-in-the-loop feedback, utilizing a well-trained reward model specifically designed to closely align with human preferences. Using a diverse multi-turn dialogue dataset, the framework applies multiple reward functions (such as diversity, consistency, and preference feedback) to refine the diffusion model through LoRA, effectively optimizing image generation based on user input. We also constructed multi-round dialogue datasets with prompts and image pairs that well-fit user intent. Experiments show the model achieves 508 wins in human evaluation, outperforming DALL-E 3 (463 wins) and others. It also achieves 3.4 rounds in dialogue efficiency (vs. 13.7 for DALL-E 3) and excels in metrics like LPIPS (0.15) and BLIP (0.59). Various experiments demonstrate the effectiveness of the proposed method over state-of-the-art baselines, with significant improvements in image consistency and alignment with user intent. </p>
<blockquote>
<p>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨æ–‡æœ¬é©±åŠ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨æŒç»­ç”Ÿæˆç¬¦åˆä¸æ–­å˜åŒ–çš„ç”¨æˆ·åå¥½å’Œæ„å›¾çš„è¾“å‡ºæ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šè½®å¯¹è¯åœºæ™¯ä¸­ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è§†è§‰ååŒé€‚åº”ï¼ˆVCAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†äººç±»åé¦ˆå¾ªç¯ï¼Œå¹¶åˆ©ç”¨ä¸“é—¨è®­ç»ƒæœ‰ç´ çš„å¥–åŠ±æ¨¡å‹ï¼Œä»¥ä¸äººç±»åå¥½ç´§å¯†å¯¹é½ã€‚ä½¿ç”¨å¤šæ ·åŒ–çš„å¤šè½®å¯¹è¯æ•°æ®é›†ï¼Œè¯¥æ¡†æ¶åº”ç”¨å¤šä¸ªå¥–åŠ±å‡½æ•°ï¼ˆå¦‚å¤šæ ·æ€§ã€ä¸€è‡´æ€§å’Œåå¥½åé¦ˆï¼‰æ¥é€šè¿‡LoRAç»†åŒ–æ‰©æ•£æ¨¡å‹ï¼Œæœ‰æ•ˆåœ°æ ¹æ®ç”¨æˆ·è¾“å…¥ä¼˜åŒ–å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†å¤šè½®å¯¹è¯æ•°æ®é›†ï¼Œå¸¦æœ‰ç¬¦åˆç”¨æˆ·æ„å›¾çš„æç¤ºå’Œå›¾åƒå¯¹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨äººå·¥è¯„ä¼°ä¸­å–å¾—4æ¬¡èƒœå‡ºï¼Œè¶…è¿‡DALL-E 3ï¼ˆèµ¢å¾—å¯¹æ‰‹å››æ¬¡èƒœå‡ºï¼‰ï¼Œå¹¶å±•ç°å‡ºå‡ºè‰²çš„è¡¨ç°ã€‚å®ƒåœ¨å¯¹è¯æ•ˆç‡æ–¹é¢ä¹Ÿå–å¾—äº†è¾ƒé«˜çš„è¡¨ç°ï¼ˆå¯¹æ¯”å¯¹æ‰‹éœ€è¦è½®è¯¢äº¤æµï¼›ä½†ä¸æ­¤æ¨¡å‹æ¯”å¯¹è¯åªéœ€è¦å°†è¿‘ä¸‰ç§’é’Ÿå·¦å³ï¼‰ã€‚å®ƒä¹Ÿåœ¨æŒ‡æ ‡å¦‚LPIPSï¼ˆç›¸ä¼¼åº¦ä¸ºè¾¾åˆ°é¢†å…ˆçš„å¾—åˆ†0.15ï¼‰å’ŒBLIPï¼ˆå¾—åˆ†ä¸ºæ¥è¿‘æ»¡åˆ†ï¼Œå³å¤§çº¦æ¥è¿‘ä¸€ä¸ªæ°´å¹³ä¸ºé«˜çš„åˆ†æ•°çº¿ï¼‰ç­‰æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚å„ç§å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„åŸºçº¿ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œåœ¨å›¾åƒä¸€è‡´æ€§å’Œç¬¦åˆç”¨æˆ·æ„å›¾æ–¹é¢å–å¾—äº†é‡å¤§æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17660v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†åŸºäºè§†è§‰ååŒé€‚åº”ï¼ˆVCAï¼‰æ¡†æ¶çš„ç ”ç©¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†äººç±»åé¦ˆå’Œå¥–åŠ±æ¨¡å‹ï¼Œä¼˜åŒ–äº†æ–‡æœ¬é©±åŠ¨å›¾åƒç”Ÿæˆçš„å¤šè½®å¯¹è¯åœºæ™¯ã€‚é€šè¿‡åº”ç”¨å¤šä¸ªå¥–åŠ±å‡½æ•°ï¼Œå¦‚å¤šæ ·æ€§ã€ä¸€è‡´æ€§å’Œåå¥½åé¦ˆï¼Œè¯¥æ¡†æ¶èƒ½å¤ŸåŸºäºç”¨æˆ·è¾“å…¥ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨äººç±»è¯„ä»·ä¸­èƒœå‡ºæ¬¡æ•°æ›´å¤šï¼Œå¯¹è¯æ•ˆç‡æ›´é«˜ï¼Œä¸”åœ¨å›¾åƒä¸€è‡´æ€§ä¸ç”¨æˆ·æ„å›¾å¯¹é½æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸ºVisual Co-Adaptationï¼ˆVCAï¼‰çš„æ¡†æ¶ï¼Œç”¨äºæ”¹è¿›æ–‡æœ¬é©±åŠ¨çš„å›¾åƒç”Ÿæˆã€‚</li>
<li>VCAæ¡†æ¶ç»“åˆäº†äººç±»åé¦ˆå’Œå¥–åŠ±æ¨¡å‹ï¼Œä»¥æ›´å¥½åœ°é€‚åº”ç”¨æˆ·åå¥½å’Œæ„å›¾ã€‚</li>
<li>è¯¥æ¡†æ¶åº”ç”¨å¤šä¸ªå¥–åŠ±å‡½æ•°ï¼ŒåŒ…æ‹¬å¤šæ ·æ€§ã€ä¸€è‡´æ€§å’Œåå¥½åé¦ˆï¼Œä»¥ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨å¤šè½®å¯¹è¯æ•°æ®é›†å’Œé€‚å½“çš„æç¤ºå’Œå›¾åƒå¯¹ï¼Œè¯¥æ¨¡å‹å¯ä»¥æ›´å¥½åœ°é€‚åº”ç”¨æˆ·æ„å›¾ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨äººç±»è¯„ä»·å’Œå¯¹è¯æ•ˆç‡æ–¹é¢å‡ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å›¾åƒä¸€è‡´æ€§å’Œä¸ç”¨æˆ·æ„å›¾å¯¹é½æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17660">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0b19d69f8d138ea98a2ba6858c408428.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e3632e7ecc663c35dd673dd519537c4.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Sparse-Autoencoder-as-a-Zero-Shot-Classifier-for-Concept-Erasing-in-Text-to-Image-Diffusion-Models"><a href="#Sparse-Autoencoder-as-a-Zero-Shot-Classifier-for-Concept-Erasing-in-Text-to-Image-Diffusion-Models" class="headerlink" title="Sparse Autoencoder as a Zero-Shot Classifier for Concept Erasing in   Text-to-Image Diffusion Models"></a>Sparse Autoencoder as a Zero-Shot Classifier for Concept Erasing in   Text-to-Image Diffusion Models</h2><p><strong>Authors:Zhihua Tian, Sirun Nan, Ming Xu, Shengfang Zhai, Wenjie Qu, Jian Liu, Ruoxi Jia, Jiaheng Zhang</strong></p>
<p>Text-to-image (T2I) diffusion models have achieved remarkable progress in generating high-quality images but also raise peopleâ€™s concerns about generating harmful or misleading content. While extensive approaches have been proposed to erase unwanted concepts without requiring retraining from scratch, they inadvertently degrade performance on normal generation tasks. In this work, we propose Interpret then Deactivate (ItD), a novel framework to enable precise concept removal in T2I diffusion models while preserving overall performance. ItD first employs a sparse autoencoder (SAE) to interpret each concept as a combination of multiple features. By permanently deactivating the specific features associated with target concepts, we repurpose SAE as a zero-shot classifier that identifies whether the input prompt includes target concepts, allowing selective concept erasure in diffusion models. Moreover, we demonstrate that ItD can be easily extended to erase multiple concepts without requiring further training. Comprehensive experiments across celebrity identities, artistic styles, and explicit content demonstrate ItDâ€™s effectiveness in eliminating targeted concepts without interfering with normal concept generation. Additionally, ItD is also robust against adversarial prompts designed to circumvent content filters. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/NANSirun/Interpret-then-deactivate">https://github.com/NANSirun/Interpret-then-deactivate</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†ä¹Ÿå¼•å‘äº†äººä»¬å¯¹ç”Ÿæˆæœ‰å®³æˆ–è¯¯å¯¼æ€§å†…å®¹çš„æ‹…å¿§ã€‚è™½ç„¶å·²æå‡ºå¹¿æ³›çš„æ–¹æ³•åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹æ¶ˆé™¤ä¸éœ€è¦çš„æ¦‚å¿µï¼Œä½†å®ƒä»¬ä¼šæ— æ„ä¸­é™ä½æ­£å¸¸ç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†â€œå…ˆè§£é‡Šååœç”¨â€ï¼ˆItDï¼‰è¿™ä¸€æ–°æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨T2Iæ‰©æ•£æ¨¡å‹ä¸­å®ç°ç²¾ç¡®çš„æ¦‚å¿µå»é™¤ï¼ŒåŒæ—¶ä¿æŒæ•´ä½“æ€§èƒ½ã€‚ItDé¦–å…ˆé‡‡ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰æ¥è§£é‡Šæ¯ä¸ªæ¦‚å¿µæ˜¯å¤šä¸ªç‰¹å¾çš„ç»„åˆã€‚é€šè¿‡æ°¸ä¹…åœç”¨ä¸ç›®æ ‡æ¦‚å¿µç›¸å…³çš„ç‰¹å®šç‰¹å¾ï¼Œæˆ‘ä»¬å°†SAEé‡æ–°å®šä½ä¸ºä¸€ç§é›¶å°„å‡»åˆ†ç±»å™¨ï¼Œå¯ä»¥è¯†åˆ«è¾“å…¥æç¤ºæ˜¯å¦åŒ…å«ç›®æ ‡æ¦‚å¿µï¼Œä»è€Œåœ¨æ‰©æ•£æ¨¡å‹ä¸­å®ç°é€‰æ‹©æ€§æ¦‚å¿µåˆ é™¤ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜ItDå¯ä»¥å¾ˆå®¹æ˜“åœ°æ‰©å±•åˆ°åˆ é™¤å¤šä¸ªæ¦‚å¿µï¼Œè€Œæ— éœ€è¿›ä¸€æ­¥è®­ç»ƒã€‚åœ¨åäººèº«ä»½ã€è‰ºæœ¯é£æ ¼å’Œæ˜ç¡®å†…å®¹æ–¹é¢çš„ç»¼åˆå®éªŒè¯æ˜äº†ItDåœ¨æ¶ˆé™¤ç›®æ ‡æ¦‚å¿µæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œè€Œä¸ä¼šå¹²æ‰°æ­£å¸¸çš„æ¦‚å¿µç”Ÿæˆã€‚æ­¤å¤–ï¼ŒItDå¯¹äºè®¾è®¡ç”¨äºè§„é¿å†…å®¹è¿‡æ»¤å™¨çš„å¯¹æŠ—æ€§æç¤ºä¹Ÿå…·æœ‰é²æ£’æ€§ã€‚ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/NANSirun/Interpret-then-deactivate%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NANSirun/Interpret-then-deactivateæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09446v3">PDF</a> 25 pages</p>
<p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºä¸€ç§åä¸ºInterpret then Deactivateï¼ˆItDï¼‰çš„æ–°æ¡†æ¶ï¼Œç”¨äºåœ¨æ–‡æœ¬è½¬å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ä¸­å®ç°ç²¾ç¡®çš„æ¦‚å¿µå»é™¤ï¼ŒåŒæ—¶ä¿ç•™æ•´ä½“æ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰è§£é‡Šæ¯ä¸ªæ¦‚å¿µï¼Œå¹¶æ°¸ä¹…åœç”¨ä¸ç›®æ ‡æ¦‚å¿µç›¸å…³çš„ç‰¹å®šç‰¹å¾ï¼Œå®ç°é€‰æ‹©æ€§æ¦‚å¿µåˆ é™¤ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ‰©å±•åˆ°åˆ é™¤å¤šä¸ªæ¦‚å¿µè€Œæ— éœ€è¿›ä¸€æ­¥è®­ç»ƒï¼Œä¸”å¯¹å¯¹æŠ—æ€§æç¤ºå…·æœ‰é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ItDæ¡†æ¶èƒ½å¤Ÿåœ¨ä¸æŸå®³æ•´ä½“æ€§èƒ½çš„å‰æä¸‹ï¼Œç²¾ç¡®åœ°ä»æ–‡æœ¬è½¬å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ä¸­ç§»é™¤æœ‰å®³æˆ–è¯¯å¯¼æ€§çš„æ¦‚å¿µã€‚</li>
<li>ItDåˆ©ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰è§£é‡Šæ¯ä¸ªæ¦‚å¿µï¼Œå¹¶å°†å…¶è§†ä¸ºå¤šä¸ªç‰¹å¾çš„ç»„åˆã€‚</li>
<li>é€šè¿‡æ°¸ä¹…åœç”¨ä¸ç›®æ ‡æ¦‚å¿µç›¸å…³çš„ç‰¹å®šç‰¹å¾ï¼ŒItDèƒ½å¤Ÿå®ç°é€‰æ‹©æ€§æ¦‚å¿µåˆ é™¤ï¼Œå¹¶å…è®¸å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œç²¾ç¡®è°ƒæ§ã€‚</li>
<li>ItDæ¡†æ¶èƒ½å¤Ÿè½»æ¾æ‰©å±•åˆ°åˆ é™¤å¤šä¸ªæ¦‚å¿µï¼Œæ— éœ€è¿›ä¸€æ­¥è®­ç»ƒã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒItDåœ¨æ¶ˆé™¤ç‰¹å®šæ¦‚å¿µçš„åŒæ—¶ï¼Œä¸ä¼šå¹²æ‰°æ­£å¸¸çš„æ¦‚å¿µç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>ItDæ¡†æ¶å¯¹äºå¯¹æŠ—æ€§çš„æç¤ºå…·æœ‰é²æ£’æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé¿å…å†…å®¹è¿‡æ»¤çš„ç»•è¿‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-81a286b9b7ce0df9978c50ef4e8295db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c322fa3ef505d0b52d09d945585f0b5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fb10ceff75e36cc91fe50a32da8864e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c2d0638668346a34ff1360f7da4ace7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6d8f1235058c49815889c6d3c75c941e.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="UniCombine-Unified-Multi-Conditional-Combination-with-Diffusion-Transformer"><a href="#UniCombine-Unified-Multi-Conditional-Combination-with-Diffusion-Transformer" class="headerlink" title="UniCombine: Unified Multi-Conditional Combination with Diffusion   Transformer"></a>UniCombine: Unified Multi-Conditional Combination with Diffusion   Transformer</h2><p><strong>Authors:Haoxuan Wang, Jinlong Peng, Qingdong He, Hao Yang, Ying Jin, Jiafu Wu, Xiaobin Hu, Yanjie Pan, Zhenye Gan, Mingmin Chi, Bo Peng, Yabiao Wang</strong></p>
<p>With the rapid development of diffusion models in image generation, the demand for more powerful and flexible controllable frameworks is increasing. Although existing methods can guide generation beyond text prompts, the challenge of effectively combining multiple conditional inputs while maintaining consistency with all of them remains unsolved. To address this, we introduce UniCombine, a DiT-based multi-conditional controllable generative framework capable of handling any combination of conditions, including but not limited to text prompts, spatial maps, and subject images. Specifically, we introduce a novel Conditional MMDiT Attention mechanism and incorporate a trainable LoRA module to build both the training-free and training-based versions. Additionally, we propose a new pipeline to construct SubjectSpatial200K, the first dataset designed for multi-conditional generative tasks covering both the subject-driven and spatially-aligned conditions. Extensive experimental results on multi-conditional generation demonstrate the outstanding universality and powerful capability of our approach with state-of-the-art performance. </p>
<blockquote>
<p>éšç€æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œå¯¹æ›´å¼ºå¤§ã€æ›´çµæ´»çš„å¯æ§æ¡†æ¶çš„éœ€æ±‚ä¹Ÿåœ¨å¢åŠ ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•å¯ä»¥åœ¨æ–‡æœ¬æç¤ºä¹‹å¤–å¼•å¯¼ç”Ÿæˆï¼Œä½†å¦‚ä½•æœ‰æ•ˆåœ°ç»“åˆå¤šä¸ªæ¡ä»¶è¾“å…¥å¹¶ä¿æŒä¸æ‰€æœ‰æ¡ä»¶çš„ä¸€è‡´æ€§ä»æ˜¯æœªè§£å†³çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†UniCombineï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºDiTçš„å¤šæ¡ä»¶å¯æ§ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿå¤„ç†ä»»ä½•ç»„åˆçš„æ¡ä»¶ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ–‡æœ¬æç¤ºã€ç©ºé—´åœ°å›¾å’Œä¸»é¢˜å›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„Conditional MMDiT Attentionæœºåˆ¶ï¼Œå¹¶èå…¥äº†ä¸€ä¸ªå¯è®­ç»ƒçš„LoRAæ¨¡å—ï¼Œä»¥æ„å»ºå…è®­ç»ƒå’ŒåŸºç¡€è®­ç»ƒä¸¤ä¸ªç‰ˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æµç¨‹æ¥æ„å»ºSubjectSpatial200Kæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºå¤šæ¡ä»¶ç”Ÿæˆä»»åŠ¡è®¾è®¡çš„ç¬¬ä¸€ä¸ªæ•°æ®é›†ï¼Œæ¶µç›–äº†ä¸»é¢˜é©±åŠ¨å’Œç©ºé—´å¯¹é½çš„æ¡ä»¶ã€‚åœ¨å¤šæ¡ä»¶ç”Ÿæˆæ–¹é¢çš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰å‡ºè‰²çš„é€šç”¨æ€§å’Œå¼ºå¤§çš„èƒ½åŠ›ï¼Œå¹¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09277v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œå¯¹æ›´å¼ºå¤§ã€æ›´çµæ´»çš„å¯æ§æ¡†æ¶çš„éœ€æ±‚ä¸æ–­å¢åŠ ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶èƒ½å¤Ÿè¶…è¶Šæ–‡æœ¬æç¤ºå¼•å¯¼ç”Ÿæˆï¼Œä½†å¦‚ä½•æœ‰æ•ˆç»“åˆå¤šç§æ¡ä»¶è¾“å…¥ï¼ŒåŒæ—¶ä¿æŒä¸æ‰€æœ‰è¾“å…¥çš„ä¸€è‡´æ€§ä»æ˜¯æœªè§£å†³çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†UniCombineï¼Œä¸€ä¸ªåŸºäºDiTçš„å¤šæ¡ä»¶å¯æ§ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿå¤„ç†å„ç§æ¡ä»¶ç»„åˆï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ–‡æœ¬æç¤ºã€ç©ºé—´åœ°å›¾å’Œä¸»é¢˜å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„åº”ç”¨å¿«é€Ÿå‘å±•ï¼Œå¯¹æ›´å…ˆè¿›ã€æ›´çµæ´»çš„å¯æ§æ¡†æ¶çš„éœ€æ±‚å¢åŠ ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨ç»“åˆå¤šç§æ¡ä»¶è¾“å…¥æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œæ— æ³•æœ‰æ•ˆç»´æŒæ‰€æœ‰è¾“å…¥çš„ä¸€è‡´æ€§ã€‚</li>
<li>UniCombineæ˜¯ä¸€ä¸ªåŸºäºDiTçš„å¤šæ¡ä»¶å¯æ§ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿå¤„ç†å„ç§æ¡ä»¶ç»„åˆã€‚</li>
<li>æå‡ºäº†æ–°å‹çš„Conditional MMDiT Attentionæœºåˆ¶ï¼Œå¢å¼ºäº†æ¡†æ¶çš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†å¯è®­ç»ƒçš„LoRAæ¨¡å—ï¼Œç”¨äºæ„å»ºæ— éœ€è®­ç»ƒå’Œéœ€è¦è®­ç»ƒçš„ä¸¤ä¸ªç‰ˆæœ¬ã€‚</li>
<li>æ„å»ºäº†SubjectSpatial200Kæ•°æ®é›†ï¼Œä¸“ä¸ºå¤šæ¡ä»¶ç”Ÿæˆä»»åŠ¡è®¾è®¡ï¼Œæ¶µç›–ä¸»é¢˜é©±åŠ¨å’Œç©ºé—´å¯¹é½æ¡ä»¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09277">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-89b79ee2e0afccaff433f05ce9a416ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40d8fb92b95258746273ffe01283dc1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c847aa88a7a0b80830017929e6f57718.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4950abf13e55b8cf46b275226e31996d.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-11/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-11/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-11/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-803770d7cf7f5bba743215eb9070e24a.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-11  PWD Prior-Guided and Wavelet-Enhanced Diffusion Model for Limited-Angle   CT
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-11/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-18a874e996c80d7295ea73551dee67e7.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-11  A Probabilistic Approach to Uncertainty Quantification Leveraging 3D   Geometry
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25243.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
