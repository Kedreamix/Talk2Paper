<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-11  Towards Multimodal Understanding via Stable Diffusion as a Task-Aware   Feature Extractor">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-35b6636740bec71d403330fedf176861.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-11-æ›´æ–°"><a href="#2025-07-11-æ›´æ–°" class="headerlink" title="2025-07-11 æ›´æ–°"></a>2025-07-11 æ›´æ–°</h1><h2 id="Towards-Multimodal-Understanding-via-Stable-Diffusion-as-a-Task-Aware-Feature-Extractor"><a href="#Towards-Multimodal-Understanding-via-Stable-Diffusion-as-a-Task-Aware-Feature-Extractor" class="headerlink" title="Towards Multimodal Understanding via Stable Diffusion as a Task-Aware   Feature Extractor"></a>Towards Multimodal Understanding via Stable Diffusion as a Task-Aware   Feature Extractor</h2><p><strong>Authors:Vatsal Agarwal, Matthew Gwilliam, Gefen Kohavi, Eshan Verma, Daniel Ulbricht, Abhinav Shrivastava</strong></p>
<p>Recent advances in multimodal large language models (MLLMs) have enabled image-based question-answering capabilities. However, a key limitation is the use of CLIP as the visual encoder; while it can capture coarse global information, it often can miss fine-grained details that are relevant to the input query. To address these shortcomings, this work studies whether pre-trained text-to-image diffusion models can serve as instruction-aware visual encoders. Through an analysis of their internal representations, we find diffusion features are both rich in semantics and can encode strong image-text alignment. Moreover, we find that we can leverage text conditioning to focus the model on regions relevant to the input question. We then investigate how to align these features with large language models and uncover a leakage phenomenon, where the LLM can inadvertently recover information from the original diffusion prompt. We analyze the causes of this leakage and propose a mitigation strategy. Based on these insights, we explore a simple fusion strategy that utilizes both CLIP and conditional diffusion features. We evaluate our approach on both general VQA and specialized MLLM benchmarks, demonstrating the promise of diffusion models for visual understanding, particularly in vision-centric tasks that require spatial and compositional reasoning. Our project page can be found <a target="_blank" rel="noopener" href="https://vatsalag99.github.io/mustafar/">https://vatsalag99.github.io/mustafar/</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥å·²ç»å®ç°äº†åŸºäºå›¾åƒçš„é—®ç­”åŠŸèƒ½ã€‚ç„¶è€Œï¼Œä¸€ä¸ªå…³é”®é™åˆ¶åœ¨äºä½¿ç”¨CLIPä½œä¸ºè§†è§‰ç¼–ç å™¨ï¼›è™½ç„¶å®ƒå¯ä»¥æ•æ‰ç²—ç³™çš„å…¨å±€ä¿¡æ¯ï¼Œä½†å®ƒå¾€å¾€ä¼šé”™è¿‡ä¸è¾“å…¥æŸ¥è¯¢ç›¸å…³çš„ç»†å¾®ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™äº›ç¼ºç‚¹ï¼Œè¿™é¡¹å·¥ä½œç ”ç©¶äº†é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ˜¯å¦èƒ½ä½œä¸ºæŒ‡ä»¤æ„ŸçŸ¥çš„è§†è§‰ç¼–ç å™¨ã€‚é€šè¿‡å¯¹å®ƒä»¬å†…éƒ¨è¡¨ç¤ºçš„åˆ†æï¼Œæˆ‘ä»¬å‘ç°æ‰©æ•£ç‰¹å¾æ—¢ä¸°å¯Œè¯­ä¹‰ï¼Œåˆèƒ½ç¼–ç å¼ºå¤§çš„å›¾åƒæ–‡æœ¬å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬å¯ä»¥åˆ©ç”¨æ–‡æœ¬æ¡ä»¶æ¥ä½¿æ¨¡å‹å…³æ³¨ä¸è¾“å…¥é—®é¢˜ç›¸å…³çš„åŒºåŸŸã€‚ç„¶åï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¦‚ä½•å°†è¿™äº›ç‰¹å¾ä¸å¤§è¯­è¨€æ¨¡å‹å¯¹é½ï¼Œå¹¶å‘ç°äº†ä¸€ä¸ªæ³„æ¼ç°è±¡ï¼Œå³å¤§å‹è¯­è¨€æ¨¡å‹å¯èƒ½ä¼šæ— æ„ä¸­ä»åŸå§‹çš„æ‰©æ•£æç¤ºä¸­æ¢å¤ä¿¡æ¯ã€‚æˆ‘ä»¬åˆ†æäº†è¿™ç§æ³„æ¼çš„åŸå› å¹¶æå‡ºäº†ç¼“è§£ç­–ç•¥ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸€ç§ç®€å•çš„èåˆç­–ç•¥ï¼Œåˆ©ç”¨CLIPå’Œæ¡ä»¶æ‰©æ•£ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨ä¸€èˆ¬çš„VQAå’Œä¸“é—¨çš„MLLMåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯æ˜äº†æ‰©æ•£æ¨¡å‹åœ¨è§†è§‰ç†è§£æ–¹é¢çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç©ºé—´å’Œç»„åˆæ¨ç†çš„ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸­ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://vatsalag99.github.io/mustafar/%E3%80%82">https://vatsalag99.github.io/mustafar/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07106v1">PDF</a> Website: see <a target="_blank" rel="noopener" href="https://vatsalag99.github.io/mustafar/">https://vatsalag99.github.io/mustafar/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä½œä¸ºæŒ‡ä»¤æ„ŸçŸ¥è§†è§‰ç¼–ç å™¨çš„æ½œåŠ›ï¼Œç”¨äºæ”¹è¿›åŸºäºå›¾åƒçš„é—®ç­”ç³»ç»Ÿã€‚é€šè¿‡åˆ†ææ‰©æ•£æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºï¼Œå‘ç°å…¶è¯­ä¹‰ä¸°å¯Œä¸”å›¾åƒä¸æ–‡æœ¬å¯¹é½èƒ½åŠ›å¼ºã€‚ç ”ç©¶è¿˜æ¢è®¨äº†å¦‚ä½•å°†è¿™äº›ç‰¹å¾ä¸å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½ï¼Œå¹¶æ­ç¤ºäº†ä¿¡æ¯æ³„æ¼ç°è±¡åŠåŸå› ã€‚ä¸ºæ­¤æå‡ºäº†ç¼“è§£ç­–ç•¥ï¼Œå¹¶é€šè¿‡èåˆCLIPå’Œæ¡ä»¶æ‰©æ•£ç‰¹å¾ç®€å•ç­–ç•¥è¿›è¡Œè¯„ä»·ï¼Œè¯æ˜äº†æ‰©æ•£æ¨¡å‹åœ¨è§†è§‰ç†è§£æ–¹é¢çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç©ºé—´å’Œç»„åˆæ¨ç†çš„ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æœ€è¿‘çš„è¿›å±•å·²ä½¿åŸºäºå›¾åƒçš„é—®ç­”åŠŸèƒ½æˆä¸ºå¯èƒ½ã€‚</li>
<li>ä½¿ç”¨CLIPä½œä¸ºè§†è§‰ç¼–ç å™¨å­˜åœ¨å±€é™æ€§ï¼Œå…¶å¯èƒ½é”™è¿‡ä¸è¾“å…¥æŸ¥è¯¢ç›¸å…³çš„ç»†å¾®ç»†èŠ‚ã€‚</li>
<li>é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹è¯­ä¹‰ä¸°å¯Œï¼Œå›¾åƒä¸æ–‡æœ¬å¯¹é½èƒ½åŠ›å¼ºã€‚</li>
<li>æ–‡æœ¬æ¡ä»¶å¯ä»¥å¼•å¯¼æ¨¡å‹å…³æ³¨ä¸è¾“å…¥é—®é¢˜ç›¸å…³çš„åŒºåŸŸã€‚</li>
<li>ä¸å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½æ—¶ä¼šå‡ºç°ä¿¡æ¯æ³„æ¼ç°è±¡ï¼Œç ”ç©¶åˆ†æäº†å…¶åŸå› å¹¶æå‡ºäº†ç¼“è§£ç­–ç•¥ã€‚</li>
<li>é€šè¿‡èåˆCLIPå’Œæ¡ä»¶æ‰©æ•£ç‰¹å¾çš„è¯„ä»·ï¼Œè¯æ˜äº†æ‰©æ•£æ¨¡å‹åœ¨è§†è§‰ç†è§£æ–¹é¢çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07106">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a3bd296c2cc41c3895a4da677fca36d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a3b0ae5de83e19d634e9a524067c5c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2969ef7a776d00463a0820055285301b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d54614c3a827accab264a475f4a9e50e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-65f3148d7fbf1643917ef50c1151545b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06e288cc174c3953b7cf67651f5f7f5f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Vision-Language-Vision-Auto-Encoder-Scalable-Knowledge-Distillation-from-Diffusion-Models"><a href="#Vision-Language-Vision-Auto-Encoder-Scalable-Knowledge-Distillation-from-Diffusion-Models" class="headerlink" title="Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation   from Diffusion Models"></a>Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation   from Diffusion Models</h2><p><strong>Authors:Tiezheng Zhang, Yitong Li, Yu-cheng Chou, Jieneng Chen, Alan Yuille, Chen Wei, Junfei Xiao</strong></p>
<p>Building state-of-the-art Vision-Language Models (VLMs) with strong captioning capabilities typically necessitates training on billions of high-quality image-text pairs, requiring millions of GPU hours. This paper introduces the Vision-Language-Vision (VLV) auto-encoder framework, which strategically leverages key pretrained components: a vision encoder, the decoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large Language Model (LLM). Specifically, we establish an information bottleneck by regularizing the language representation space, achieved through freezing the pretrained T2I diffusion decoder. Our VLV pipeline effectively distills knowledge from the text-conditioned diffusion model using continuous embeddings, demonstrating comprehensive semantic understanding via high-quality reconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the intermediate language representations into detailed descriptions, we construct a state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o and Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and significantly reduces data requirements; by primarily utilizing single-modal images for training and maximizing the utility of existing pretrained models (image encoder, T2I diffusion model, and LLM), it circumvents the need for massive paired image-text datasets, keeping the total training expenditure under $1,000 USD. </p>
<blockquote>
<p>æ„å»ºå…·æœ‰å¼ºå¤§æè¿°åŠŸèƒ½çš„å…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVision-Language Modelsï¼Œç®€ç§°VLMï¼‰é€šå¸¸éœ€è¦è®­ç»ƒæ•°åäº¿é«˜è´¨é‡å›¾åƒæ–‡æœ¬å¯¹ï¼Œå¹¶éœ€è¦æ•°ç™¾ä¸‡GPUå°æ—¶ã€‚æœ¬æ–‡ä»‹ç»äº†è§†è§‰è¯­è¨€è§†è§‰ï¼ˆVision-Language-Visionï¼Œç®€ç§°VLVï¼‰è‡ªåŠ¨ç¼–ç å™¨æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç­–ç•¥æ€§åœ°åˆ©ç”¨äº†å…³é”®é¢„è®­ç»ƒç»„ä»¶ï¼šè§†è§‰ç¼–ç å™¨ã€æ–‡æœ¬åˆ°å›¾åƒï¼ˆText-to-Imageï¼Œç®€ç§°T2Iï¼‰æ‰©æ•£æ¨¡å‹çš„è§£ç å™¨ï¼Œç„¶åæ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelï¼Œç®€ç§°LLMï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡æ­£åˆ™åŒ–è¯­è¨€è¡¨ç¤ºç©ºé—´æ¥å»ºç«‹ä¿¡æ¯ç“¶é¢ˆï¼Œè¿™æ˜¯é€šè¿‡å†»ç»“é¢„è®­ç»ƒçš„T2Iæ‰©æ•£è§£ç å™¨æ¥å®ç°çš„ã€‚æˆ‘ä»¬çš„VLVç®¡é“æœ‰æ•ˆåœ°ä»æ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸­è’¸é¦çŸ¥è¯†ï¼Œä½¿ç”¨è¿ç»­åµŒå…¥æ¥å±•ç¤ºé«˜è´¨é‡é‡å»ºçš„ç»¼åˆè¯­ä¹‰ç†è§£ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„LLMä»¥å°†ä¸­é—´è¯­è¨€è¡¨ç¤ºè§£ç ä¸ºè¯¦ç»†æè¿°ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªä¸GPT-4oå’ŒGemini 2.0 Flashç­‰é¢†å…ˆæ¨¡å‹ç›¸åª²ç¾çš„å…ˆè¿›æè¿°ç”Ÿæˆå™¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¯æ˜äº†å‡ºè‰²çš„æˆæœ¬æ•ˆç›Šå¹¶æ˜¾è‘—å‡å°‘äº†æ•°æ®éœ€æ±‚ï¼›ä¸»è¦é€šè¿‡åˆ©ç”¨å•æ¨¡æ€å›¾åƒè¿›è¡Œè®­ç»ƒå¹¶æœ€å¤§é™åº¦åœ°åˆ©ç”¨ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå›¾åƒç¼–ç å™¨ã€T2Iæ‰©æ•£æ¨¡å‹å’ŒLLMï¼‰ï¼Œé¿å…äº†éœ€è¦å¤§é‡é…å¯¹å›¾åƒæ–‡æœ¬æ•°æ®é›†çš„éœ€æ±‚ï¼Œå°†æ€»è®­ç»ƒè´¹ç”¨æ§åˆ¶åœ¨1000ç¾å…ƒä»¥å†…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07104v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºVision-Language-Visionï¼ˆVLVï¼‰çš„è‡ªåŠ¨ç¼–ç å™¨æ¡†æ¶ï¼Œç”¨äºæ„å»ºå…·æœ‰å¼ºå¤§æè¿°èƒ½åŠ›çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ã€æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è§£ç å™¨ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ç°äº†é«˜æ•ˆçš„çŸ¥è¯†è’¸é¦ã€‚é€šè¿‡æ­£åˆ™åŒ–è¯­è¨€è¡¨ç¤ºç©ºé—´å»ºç«‹ä¿¡æ¯ç“¶é¢ˆï¼Œåˆ©ç”¨æ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„è¿ç»­åµŒå…¥è¿›è¡Œé«˜è´¨é‡é‡å»ºï¼Œå¹¶é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥è§£ç ä¸­é—´è¯­è¨€è¡¨ç¤ºä»¥ç”Ÿæˆè¯¦ç»†æè¿°ã€‚è¯¥æ–¹æ³•æˆæœ¬æ•ˆç›Šé«˜ï¼Œæ˜¾è‘—å‡å°‘äº†æ•°æ®éœ€æ±‚ï¼Œä¸»è¦åˆ©ç”¨å•æ¨¡æ€å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œæœ€å¤§é™åº¦åœ°åˆ©ç”¨ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ— éœ€å¤§è§„æ¨¡é…å¯¹å›¾åƒæ–‡æœ¬æ•°æ®é›†ï¼Œæ€»è®­ç»ƒè´¹ç”¨æ§åˆ¶åœ¨1000ç¾å…ƒä»¥å†…ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†Vision-Language-Visionï¼ˆVLVï¼‰è‡ªåŠ¨ç¼–ç å™¨æ¡†æ¶ï¼Œç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹çš„å…³é”®é¢„è®­ç»ƒç»„ä»¶ã€‚</li>
<li>é€šè¿‡å»ºç«‹ä¿¡æ¯ç“¶é¢ˆå’Œæ­£åˆ™åŒ–è¯­è¨€è¡¨ç¤ºç©ºé—´ï¼Œå®ç°äº†é«˜æ•ˆçŸ¥è¯†è’¸é¦ã€‚</li>
<li>åˆ©ç”¨æ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„è¿ç»­åµŒå…¥è¿›è¡Œé«˜è´¨é‡é‡å»ºã€‚</li>
<li>é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæé«˜äº†æè¿°ç”Ÿæˆçš„è´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•æˆæœ¬æ•ˆç›Šé«˜ï¼Œæ˜¾è‘—å‡å°‘äº†æ•°æ®éœ€æ±‚ï¼Œä¸»è¦åˆ©ç”¨å•æ¨¡æ€å›¾åƒè¿›è¡Œè®­ç»ƒã€‚</li>
<li>æœ€å¤§åŒ–åœ°åˆ©ç”¨äº†ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå›¾åƒç¼–ç å™¨ã€æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å’ŒLLMï¼‰ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€å¤§è§„æ¨¡é…å¯¹å›¾åƒæ–‡æœ¬æ•°æ®é›†ï¼Œæ€»è®­ç»ƒè´¹ç”¨æ§åˆ¶åœ¨1000ç¾å…ƒä»¥å†…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07104">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-01f3564eba957419033f393fac383075.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-57502e97dfe7bdfd13ab7190e87cff60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a90638225c131c88ce6ac334ea28fad3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Boosting-Parameter-Efficiency-in-LLM-Based-Recommendation-through-Sophisticated-Pruning"><a href="#Boosting-Parameter-Efficiency-in-LLM-Based-Recommendation-through-Sophisticated-Pruning" class="headerlink" title="Boosting Parameter Efficiency in LLM-Based Recommendation through   Sophisticated Pruning"></a>Boosting Parameter Efficiency in LLM-Based Recommendation through   Sophisticated Pruning</h2><p><strong>Authors:Shanle Zheng, Keqin Bao, Jizhi Zhang, Yang Zhang, Fuli Feng, Xiangnan He</strong></p>
<p>LLM-based recommender systems have made significant progress; however, the deployment cost associated with the large parameter volume of LLMs still hinders their real-world applications. This work explores parameter pruning to improve parameter efficiency while maintaining recommendation quality, thereby enabling easier deployment. Unlike existing approaches that focus primarily on inter-layer redundancy, we uncover intra-layer redundancy within components such as self-attention and MLP modules. Building on this analysis, we propose a more fine-grained pruning approach that integrates both intra-layer and layer-wise pruning. Specifically, we introduce a three-stage pruning strategy that progressively prunes parameters at different levels and parts of the model, moving from intra-layer to layer-wise pruning, or from width to depth. Each stage also includes a performance restoration step using distillation techniques, helping to strike a balance between performance and parameter efficiency. Empirical results demonstrate the effectiveness of our approach: across three datasets, our models achieve an average of 88% of the original modelâ€™s performance while pruning more than 95% of the non-embedding parameters. This underscores the potential of our method to significantly reduce resource requirements without greatly compromising recommendation quality. Our code will be available at: <a target="_blank" rel="noopener" href="https://github.com/zheng-sl/PruneRec">https://github.com/zheng-sl/PruneRec</a> </p>
<blockquote>
<p>åŸºäºLLMçš„æ¨èç³»ç»Ÿå·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼›ç„¶è€Œï¼Œç”±äºLLMçš„å·¨å¤§å‚æ•°ä½“ç§¯æ‰€å¸¦æ¥çš„éƒ¨ç½²æˆæœ¬ä»ç„¶é˜»ç¢äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†å‚æ•°ä¿®å‰ªæŠ€æœ¯ï¼Œä»¥æé«˜å‚æ•°æ•ˆç‡åŒæ—¶ä¿æŒæ¨èè´¨é‡ï¼Œä»è€Œæ›´å®¹æ˜“è¿›è¡Œéƒ¨ç½²ã€‚ä¸ä¸»è¦å…³æ³¨å±‚é—´å†—ä½™çš„ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬å‘ç°äº†ç»„ä»¶ï¼ˆå¦‚è‡ªæ³¨æ„åŠ›æœºåˆ¶å’ŒMLPæ¨¡å—ï¼‰å†…çš„å±‚å†…å†—ä½™ã€‚åŸºäºè¿™ä¸€åˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ›´ç²¾ç»†çš„ä¿®å‰ªæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å±‚å†…å’Œå±‚é—´ä¿®å‰ªã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¸‰é˜¶æ®µä¿®å‰ªç­–ç•¥ï¼Œè¯¥ç­–ç•¥æŒ‰ä¸åŒçº§åˆ«å’Œæ¨¡å‹çš„ä¸åŒéƒ¨åˆ†é€æ­¥ä¿®å‰ªå‚æ•°ï¼Œä»å±‚å†…ä¿®å‰ªåˆ°å±‚é—´ä¿®å‰ªï¼Œæˆ–ä»å®½åº¦åˆ°æ·±åº¦ã€‚æ¯ä¸ªé˜¶æ®µè¿˜åŒ…æ‹¬ä½¿ç”¨è’¸é¦æŠ€æœ¯çš„æ€§èƒ½æ¢å¤æ­¥éª¤ï¼Œæœ‰åŠ©äºåœ¨æ€§èƒ½å’Œå‚æ•°æ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ç»éªŒç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆï¼šåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¿®å‰ªè¶…è¿‡95%çš„éåµŒå…¥å‚æ•°çš„åŒæ—¶ï¼Œå¹³å‡è¾¾åˆ°åŸå§‹æ¨¡å‹æ€§èƒ½çš„88%ã€‚è¿™çªå‡ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ˜¾è‘—é™ä½èµ„æºè¦æ±‚çš„åŒæ—¶ï¼Œä¸ä¼šè¿‡åº¦ç‰ºç‰²æ¨èè´¨é‡ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/zheng-sl/PruneRec%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/zheng-sl/PruneRecä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07064v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMæ¨èç³»ç»Ÿè™½æœ‰æ‰€è¿›å±•ï¼Œä½†éƒ¨ç½²æˆæœ¬é«˜æ˜‚é™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚æœ¬ç ”ç©¶é€šè¿‡å‚æ•°ä¿®å‰ªæé«˜å‚æ•°æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒæ¨èè´¨é‡ï¼Œä¿ƒè¿›éƒ¨ç½²ã€‚ç ”ç©¶ä¸åŒå±‚å†…å†—ä½™ï¼Œæå‡ºç²¾ç»†ä¿®å‰ªæ–¹æ³•ï¼Œç»“åˆå±‚å†…å’Œå±‚é—´ä¿®å‰ªã€‚ä¸‰é˜¶æ®µä¿®å‰ªç­–ç•¥é€æ­¥ä¿®å‰ªæ¨¡å‹ä¸åŒå±‚æ¬¡å’Œéƒ¨åˆ†çš„å‚æ•°ï¼Œä»å±‚å†…åˆ°å±‚é—´ï¼Œä»å®½åº¦åˆ°æ·±åº¦ã€‚ä½¿ç”¨è’¸é¦æŠ€æœ¯æ¢å¤æ€§èƒ½ï¼Œå¹³è¡¡æ€§èƒ½ä¸å‚æ•°æ•ˆç‡ã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šï¼Œæ¨¡å‹ä¿®å‰ªè¶…è¿‡95%çš„éåµŒå…¥å‚æ•°ï¼Œä»èƒ½ä¿æŒåŸå§‹æ€§èƒ½çš„88%ï¼Œæ˜¾è‘—é™ä½èµ„æºè¦æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMæ¨èç³»ç»Ÿè™½æœ‰è¿›å±•ï¼Œä½†éƒ¨ç½²æˆæœ¬é«˜æ˜‚é™åˆ¶äº†å®é™…åº”ç”¨ã€‚</li>
<li>ç ”ç©¶é€šè¿‡å‚æ•°ä¿®å‰ªæé«˜LLMæ¨èç³»ç»Ÿçš„å‚æ•°æ•ˆç‡ã€‚</li>
<li>æå‡ºç²¾ç»†çš„ä¿®å‰ªæ–¹æ³•ï¼Œç»“åˆå±‚å†…å’Œå±‚é—´ä¿®å‰ªç­–ç•¥ã€‚</li>
<li>é‡‡ç”¨ä¸‰é˜¶æ®µä¿®å‰ªç­–ç•¥ï¼Œä»å±‚å†…åˆ°å±‚é—´ï¼Œé€æ­¥ä¿®å‰ªæ¨¡å‹å‚æ•°ã€‚</li>
<li>ä½¿ç”¨è’¸é¦æŠ€æœ¯æ¢å¤æ€§èƒ½ï¼Œä¿æŒæ¨èè´¨é‡çš„åŒæ—¶é™ä½èµ„æºéœ€æ±‚ã€‚</li>
<li>åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šæµ‹è¯•ï¼Œä¿®å‰ªè¶…è¿‡95%çš„éåµŒå…¥å‚æ•°ï¼Œä»èƒ½ä¿æŒåŸå§‹æ€§èƒ½çš„88%ã€‚</li>
<li>ä»£ç å°†å…¬å¼€åœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07064">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e33baff84a12b9b65b43bb91dbd6ccf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f85e069ef0a81917d4663657d81b6c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f1519541e15ed24e63a8a93480eedf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6477a4fca00abaec445bc526059d37e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f608ecafcdb2b336ff78c289b0e100ba.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="5C-Prompt-Contracts-A-Minimalist-Creative-Friendly-Token-Efficient-Design-Framework-for-Individual-and-SME-LLM-Usage"><a href="#5C-Prompt-Contracts-A-Minimalist-Creative-Friendly-Token-Efficient-Design-Framework-for-Individual-and-SME-LLM-Usage" class="headerlink" title="5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient   Design Framework for Individual and SME LLM Usage"></a>5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient   Design Framework for Individual and SME LLM Usage</h2><p><strong>Authors:Ugur Ari</strong></p>
<p>The progression from traditional prompt engineering to a more rigorous discipline of prompt design marks a pivotal shift in human-LLM interaction. As Large Language Models (LLMs) become increasingly embedded in mission-critical applications, there emerges a pressing need for frameworks that are not only explicit and systematic but also minimal enough to remain practical and broadly accessible. While many existing approaches address prompt structuring through elaborate Domain-Specific Languages (DSLs) or multi-layered templates, such methods can impose significant token and cognitive overhead, potentially constraining the modelâ€™s creative capacity. In this context, we propose the 5C Prompt Contract, a framework that distills prompt design into five intuitive components: Character, Cause, Constraint, Contingency, and Calibration. This minimal cognitive schema explicitly integrates fallback and output optimization directives, fostering reliable, interpretable, and creatively flexible AI interactions. Experimental results demonstrate that the 5C framework consistently achieves superior input token efficiency while maintaining rich and consistent outputs across diverse LLM architectures (OpenAI, Anthropic, DeepSeek, and Gemini), making it particularly suited for individuals and Small-to-Medium Enterprises (SMEs) with limited AI engineering resources. </p>
<blockquote>
<p>ä»ä¼ ç»Ÿæç¤ºå·¥ç¨‹åˆ°æ›´ä¸¥æ ¼çš„æç¤ºè®¾è®¡å­¦ç§‘çš„è½¬å˜ï¼Œæ ‡å¿—ç€äººç±»ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äº¤äº’ä¸­çš„å…³é”®æ€§å˜é©ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å…³é”®ä»»åŠ¡åº”ç”¨ä¸­è¶Šæ¥è¶Šæ·±å…¥çš„åµŒå…¥ï¼Œå¯¹äºä¸ä»…æ˜¯æ˜¾æ€§ä¸”ç³»ç»ŸåŒ–çš„ã€åŒæ—¶ä¹Ÿè¶³å¤Ÿç²¾ç®€ä»¥ä¿æŒå®ç”¨æ€§å’Œå¹¿æ³›å¯è®¿é—®æ€§çš„æ¡†æ¶çš„éœ€æ±‚æ—¥ç›Šè¿«åˆ‡ã€‚è™½ç„¶è®¸å¤šç°æœ‰æ–¹æ³•é€šè¿‡å¤æ‚çš„é¢†åŸŸç‰¹å®šè¯­è¨€ï¼ˆDSLï¼‰æˆ–å¤šå±‚æ¨¡æ¿æ¥è§£å†³æç¤ºç»“æ„é—®é¢˜ï¼Œä½†æ­¤ç±»æ–¹æ³•å¯èƒ½ä¼šäº§ç”Ÿå¤§é‡çš„æ ‡è®°å’Œè®¤çŸ¥è´Ÿæ‹…ï¼Œä»è€Œå¯èƒ½é™åˆ¶æ¨¡å‹çš„åˆ›é€ åŠ›ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†5Cæç¤ºåˆçº¦ï¼ˆ5C Prompt Contractï¼‰æ¡†æ¶ï¼Œå®ƒå°†æç¤ºè®¾è®¡æç‚¼ä¸ºäº”ä¸ªç›´è§‚ç»„æˆéƒ¨åˆ†ï¼šè§’è‰²ï¼ˆCharacterï¼‰ã€åŸå› ï¼ˆCauseï¼‰ã€çº¦æŸï¼ˆConstraintï¼‰ã€æ¡ä»¶ï¼ˆContingencyï¼‰å’Œæ ¡å‡†ï¼ˆCalibrationï¼‰ã€‚è¿™ä¸€æç®€çš„è®¤çŸ¥æ¨¡å¼æ˜¾å¼åœ°æ•´åˆäº†åå¤‡å’Œè¾“å‡ºä¼˜åŒ–æŒ‡ä»¤ï¼Œä¿ƒè¿›äº†å¯é ã€å¯è§£é‡Šå’Œå¯Œæœ‰åˆ›é€ åŠ›çš„çµæ´»äººå·¥æ™ºèƒ½äº¤äº’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ5Cæ¡†æ¶åœ¨ä¿æŒè¾“å‡ºä¸°å¯Œä¸”ä¸€è‡´çš„åŒæ—¶ï¼Œå§‹ç»ˆå®ç°äº†å‡ºè‰²çš„è¾“å…¥æ ‡è®°æ•ˆç‡ï¼Œé€‚ç”¨äºå„ç§å¤§å‹è¯­è¨€æ¨¡å‹æ¶æ„ï¼ˆOpenAIã€Anthropicã€DeepSeekå’ŒGeminiï¼‰ï¼Œç‰¹åˆ«é€‚ç”¨äºæ‹¥æœ‰æœ‰é™äººå·¥æ™ºèƒ½å·¥ç¨‹èµ„æºçš„ä¸ªäººå’Œä¸­å°å‹ä¼ä¸šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07045v1">PDF</a> 5 pages, 5 tables. Includes comparative experimental results across   OpenAI, Anthropic, DeepSeek, and Gemini LLMs</p>
<p><strong>Summary</strong></p>
<p>ä¼ ç»Ÿæç¤ºå·¥ç¨‹å‘æ›´ä¸¥æ ¼çš„æç¤ºè®¾è®¡å­¦ç§‘çš„è½¬å˜æ ‡å¿—ç€äººç±»ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äº’åŠ¨çš„é‡å¤§å˜é©ã€‚éšç€LLMåœ¨å…³é”®ä¸šåŠ¡åº”ç”¨ä¸­çš„æ—¥ç›Šé›†æˆï¼Œå‡ºç°äº†å¯¹ä¸ä»…æ˜¯æ˜ç¡®å’Œç³»ç»ŸåŒ–çš„æ¡†æ¶ï¼Œè€Œä¸”æ˜¯è¶³å¤Ÿç²¾ç®€ä»¥ä¿æŒå®ç”¨å’Œå¹¿æ³›å¯åŠæ€§çš„è¿«åˆ‡éœ€æ±‚ã€‚æå‡ºçš„5Cæç¤ºåˆçº¦æ˜¯ä¸€ä¸ªå°†æç¤ºè®¾è®¡æç‚¼ä¸ºäº”ä¸ªç›´è§‚ç»„æˆéƒ¨åˆ†çš„æ¡†æ¶ï¼šè§’è‰²ã€åŸå› ã€çº¦æŸã€è¿ç»­æ€§å’Œæ ¡å‡†ã€‚è¿™ä¸ªæœ€å°çš„è®¤çŸ¥æ¨¡å¼æ˜¾å¼åœ°é›†æˆäº†åå¤‡å’Œè¾“å‡ºä¼˜åŒ–æŒ‡ä»¤ï¼Œä¿ƒè¿›äº†å¯é ã€å¯è§£é‡Šå’Œå¯Œæœ‰åˆ›é€ åŠ›çš„AIäº¤äº’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒä¸°å¯Œå’Œä¸€è‡´è¾“å‡ºçš„åŒæ—¶ï¼Œå®ç°äº†è¾ƒé«˜çš„è¾“å…¥ä»¤ç‰Œæ•ˆç‡ï¼Œé€‚ç”¨äºèµ„æºæœ‰é™çš„ä¸ªäººå’Œä¸­å°ä¼ä¸šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…³é”®ä¸šåŠ¡åº”ç”¨ä¸­çš„é›†æˆä¸æ–­å¢é•¿ï¼Œéœ€è¦æ›´æ˜ç¡®çš„æç¤ºè®¾è®¡æ¡†æ¶ã€‚</li>
<li>ä¼ ç»Ÿæç¤ºå·¥ç¨‹å‘æç¤ºè®¾è®¡å­¦ç§‘çš„è½¬å˜æ˜¯LLMä¸äººç±»äº’åŠ¨çš„é‡è¦å˜é©ã€‚</li>
<li>ç°æœ‰æç¤ºè®¾è®¡æ–¹æ³•æœ‰è¾ƒé«˜çš„ç¬¦å·å’Œè®¤çŸ¥å¼€é”€ï¼Œå¯èƒ½é™åˆ¶æ¨¡å‹çš„åˆ›é€ åŠ›ã€‚</li>
<li>æå‡ºçš„5Cæç¤ºåˆçº¦æ˜¯ä¸€ä¸ªç®€æ´ã€ç›´è§‚çš„æ¡†æ¶ï¼ŒåŒ…æ‹¬è§’è‰²ã€åŸå› ã€çº¦æŸç­‰äº”ä¸ªå…ƒç´ ã€‚</li>
<li>5Cæ¡†æ¶ç»“åˆäº†åå¤‡å’Œè¾“å‡ºä¼˜åŒ–æŒ‡ä»¤ï¼Œå¢å¼ºäº†AIäº¤äº’çš„å¯é æ€§ã€å¯è§£é‡Šæ€§å’Œåˆ›é€ åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¶æ„ä¸­å®ç°äº†é«˜æ•ˆçš„è¾“å…¥ä»¤ç‰Œæ•ˆç‡å’Œä¸€è‡´çš„è¾“å‡ºè¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07045">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-70cf3625e10791cea3daf1abe85c3965.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1c3ddffc4fa45a9599b85b23de71239f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6679f22eb94c2ad576609c5b72c4e3d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-868578b5826abb5bf527b7d522316588.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d69e271f3943d4b3ec0a82f0f3bf8b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf077c8eb37cfb7038370ed03ecfd868.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GNN-ViTCap-GNN-Enhanced-Multiple-Instance-Learning-with-Vision-Transformers-for-Whole-Slide-Image-Classification-and-Captioning"><a href="#GNN-ViTCap-GNN-Enhanced-Multiple-Instance-Learning-with-Vision-Transformers-for-Whole-Slide-Image-Classification-and-Captioning" class="headerlink" title="GNN-ViTCap: GNN-Enhanced Multiple Instance Learning with Vision   Transformers for Whole Slide Image Classification and Captioning"></a>GNN-ViTCap: GNN-Enhanced Multiple Instance Learning with Vision   Transformers for Whole Slide Image Classification and Captioning</h2><p><strong>Authors:S M Taslim Uddin Raju, Md. Milon Islam, Md Rezwanul Haque, Hamdi Altaheri, Fakhri Karray</strong></p>
<p>Microscopic assessment of histopathology images is vital for accurate cancer diagnosis and treatment. Whole Slide Image (WSI) classification and captioning have become crucial tasks in computer-aided pathology. However, microscopic WSI face challenges such as redundant patches and unknown patch positions due to subjective pathologist captures. Moreover, generating automatic pathology captions remains a significant challenge. To address these issues, we introduce a novel GNN-ViTCap framework for classification and caption generation from histopathological microscopic images. First, a visual feature extractor generates patch embeddings. Redundant patches are then removed by dynamically clustering these embeddings using deep embedded clustering and selecting representative patches via a scalar dot attention mechanism. We build a graph by connecting each node to its nearest neighbors in the similarity matrix and apply a graph neural network to capture both local and global context. The aggregated image embeddings are projected into the language modelâ€™s input space through a linear layer and combined with caption tokens to fine-tune a large language model. We validate our method on the BreakHis and PatchGastric datasets. GNN-ViTCap achieves an F1 score of 0.934 and an AUC of 0.963 for classification, along with a BLEU-4 score of 0.811 and a METEOR score of 0.569 for captioning. Experimental results demonstrate that GNN-ViTCap outperforms state of the art approaches, offering a reliable and efficient solution for microscopy based patient diagnosis. </p>
<blockquote>
<p>ç—…ç†å›¾åƒçš„å¾®è§‚è¯„ä¼°å¯¹äºå‡†ç¡®çš„ç™Œç—‡è¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†ç±»å’Œæ ‡æ³¨å·²æˆä¸ºè®¡ç®—æœºè¾…åŠ©ç—…ç†å­¦ä¸­çš„å…³é”®ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå¾®è§‚WSIé¢ä¸´ç€ç”±äºä¸»è§‚ç—…ç†å­¦å®¶æ•è·è€Œå¯¼è‡´çš„å†—ä½™æ–‘å—å’ŒæœªçŸ¥æ–‘å—ä½ç½®çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè‡ªåŠ¨ç”Ÿæˆç—…ç†å­¦æ ‡æ³¨ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„GNN-ViTCapæ¡†æ¶ï¼Œç”¨äºä»ç—…ç†æ˜¾å¾®å›¾åƒä¸­è¿›è¡Œåˆ†ç±»å’Œç”Ÿæˆæ ‡æ³¨ã€‚é¦–å…ˆï¼Œè§†è§‰ç‰¹å¾æå–å™¨ç”Ÿæˆæ–‘å—åµŒå…¥ã€‚ç„¶åï¼Œé€šè¿‡æ·±åº¦åµŒå…¥èšç±»åŠ¨æ€åœ°èšç±»è¿™äº›åµŒå…¥ï¼Œå¹¶é€šè¿‡æ ‡é‡ç‚¹æ³¨æ„åŠ›æœºåˆ¶é€‰æ‹©ä»£è¡¨æ€§æ–‘å—ï¼Œä»è€Œå»é™¤å†—ä½™æ–‘å—ã€‚æˆ‘ä»¬é€šè¿‡å°†æ¯ä¸ªèŠ‚ç‚¹è¿æ¥åˆ°ç›¸ä¼¼åº¦çŸ©é˜µä¸­çš„æœ€è¿‘é‚»å±…æ¥æ„å»ºå›¾ï¼Œå¹¶åº”ç”¨å›¾ç¥ç»ç½‘ç»œæ¥æ•è·å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ã€‚èšåˆçš„å›¾åƒåµŒå…¥é€šè¿‡çº¿æ€§å±‚æŠ•å°„åˆ°è¯­è¨€æ¨¡å‹çš„è¾“å…¥ç©ºé—´ï¼Œå¹¶ä¸æ ‡æ³¨ä»¤ç‰Œç»“åˆï¼Œä»¥å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨BreakHiså’ŒPatchGastricæ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚GNN-ViTCapåœ¨åˆ†ç±»æ–¹é¢è¾¾åˆ°äº†F1åˆ†æ•°0.934å’ŒAUCåˆ†æ•°0.963ï¼Œåœ¨æ ‡æ³¨æ–¹é¢è¾¾åˆ°äº†BLEU-4åˆ†æ•°0.811å’ŒMETEORåˆ†æ•°0.569ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGNN-ViTCapä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºåŸºäºæ˜¾å¾®é•œçš„æ‚£è€…è¯Šæ–­æä¾›äº†å¯é é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07006v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºè®¡ç®—æœºè¾…åŠ©ç—…ç†å­¦çš„æ˜¾å¾®é•œå›¾åƒåˆ†æçš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç™Œç—‡è¯Šæ–­å’Œæ²»ç–—ä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹å…¨æ˜¾å¾®é•œå›¾åƒï¼ˆWSIï¼‰åˆ†ç±»å’Œæè¿°é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚å†—ä½™è¡¥ä¸å’ŒæœªçŸ¥è¡¥ä¸ä½ç½®ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„GNN-ViTCapæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åŠ¨æ€èšç±»å»é™¤å†—ä½™è¡¥ä¸ï¼Œåˆ©ç”¨å›¾ç¥ç»ç½‘ç»œæ•æ‰å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶ç»“åˆè¯­è¨€æ¨¡å‹è¿›è¡Œå›¾åƒåˆ†ç±»å’Œæè¿°ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨åˆ†ç±»å’Œæè¿°æ–¹é¢å‡å–å¾—äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼Œä¸ºåŸºäºæ˜¾å¾®é•œçš„æ‚£è€…è¯Šæ–­æä¾›äº†å¯é è€Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æ–‡æœ¬çš„ä¸»è¦è§è§£å’Œå…³é”®ç‚¹ï¼š</p>
<ol>
<li>è®¡ç®—æœºè¾…åŠ©ç—…ç†å­¦ä¸­çš„æ˜¾å¾®é•œå›¾åƒåˆ†æå¯¹ç™Œç—‡è¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚</li>
<li>å…¨æ˜¾å¾®é•œå›¾åƒï¼ˆWSIï¼‰åˆ†ç±»å’Œæè¿°æ˜¯è®¡ç®—æœºè¾…åŠ©ç—…ç†å­¦ä¸­çš„å…³é”®ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰çš„æ˜¾å¾®é•œå›¾åƒåˆ†æé¢ä¸´å†—ä½™è¡¥ä¸å’ŒæœªçŸ¥è¡¥ä¸ä½ç½®ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„GNN-ViTCapæ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>GNN-ViTCapæ¡†æ¶é€šè¿‡åŠ¨æ€èšç±»å»é™¤å†—ä½™è¡¥ä¸ï¼Œå¹¶åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œæ•æ‰å›¾åƒä¿¡æ¯ã€‚</li>
<li>GNN-ViTCapç»“åˆäº†è¯­è¨€æ¨¡å‹è¿›è¡Œå›¾åƒåˆ†ç±»å’Œæè¿°ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9e6bdbce55d3eb751487c960ab36dcd7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a845ac848654b2c488ae7eaf20e07158.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Learning-Deliberately-Acting-Intuitively-Unlocking-Test-Time-Reasoning-in-Multimodal-LLMs"><a href="#Learning-Deliberately-Acting-Intuitively-Unlocking-Test-Time-Reasoning-in-Multimodal-LLMs" class="headerlink" title="Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning   in Multimodal LLMs"></a>Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning   in Multimodal LLMs</h2><p><strong>Authors:Yahan Yu, Yuyang Dong, Masafumi Oyamada</strong></p>
<p>Reasoning is a key capability for large language models (LLMs), particularly when applied to complex tasks such as mathematical problem solving. However, multimodal reasoning research still requires further exploration of modality alignment and training costs. Many of these approaches rely on additional data annotation and relevant rule-based rewards to enhance the understanding and reasoning ability, which significantly increases training costs and limits scalability. To address these challenges, we propose the Deliberate-to-Intuitive reasoning framework (D2I) that improves the understanding and reasoning ability of multimodal LLMs (MLLMs) without extra annotations and complex rewards. Specifically, our method sets deliberate reasoning strategies to enhance modality alignment only through the rule-based format reward during training. While evaluating, the reasoning style shifts to intuitive, which removes deliberate reasoning strategies during training and implicitly reflects the modelâ€™s acquired abilities in the response. D2I outperforms baselines across both in-domain and out-of-domain benchmarks. Our findings highlight the role of format reward in fostering transferable reasoning skills in MLLMs, and inspire directions for decoupling training-time reasoning depth from test-time response flexibility. </p>
<blockquote>
<p>æ¨ç†æ˜¯å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…³é”®èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨åº”ç”¨äºæ•°å­¦é—®é¢˜è§£å†³ç­‰å¤æ‚ä»»åŠ¡æ—¶ã€‚ç„¶è€Œï¼Œå¤šæ¨¡æ€æ¨ç†ç ”ç©¶ä»ç„¶éœ€è¦è¿›ä¸€æ­¥æ¢ç´¢æ¨¡æ€å¯¹é½å’Œè®­ç»ƒæˆæœ¬ã€‚è®¸å¤šè¿™äº›æ–¹æ³•ä¾èµ–äºé¢å¤–çš„æ•°æ®æ ‡æ³¨å’Œç›¸å…³åŸºäºè§„åˆ™çš„å¥–åŠ±æ¥æé«˜ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œè¿™æ˜¾è‘—å¢åŠ äº†è®­ç»ƒæˆæœ¬å¹¶é™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Deliberate-to-Intuitiveæ¨ç†æ¡†æ¶ï¼ˆD2Iï¼‰ï¼Œè¯¥æ¡†æ¶æé«˜äº†å¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„æ³¨é‡Šå’Œå¤æ‚çš„å¥–åŠ±ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä»…åŸºäºè§„åˆ™æ ¼å¼çš„å¥–åŠ±æ¥è®¾ç½®æ·±æ€ç†Ÿè™‘çš„æ¨ç†ç­–ç•¥ï¼Œä»¥æé«˜æ¨¡æ€å¯¹é½çš„è®­ç»ƒè¿‡ç¨‹ã€‚åœ¨è¯„ä¼°æ—¶ï¼Œæ¨ç†é£æ ¼è½¬å˜ä¸ºç›´è§‰å¼ï¼Œè¿™åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¶ˆé™¤äº†æ·±æ€ç†Ÿè™‘çš„æ¨ç†ç­–ç•¥ï¼Œå¹¶åœ¨å“åº”ä¸­éšå«åœ°åæ˜ äº†æ¨¡å‹æ‰€è·å¾—çš„æŠ€èƒ½ã€‚D2Iåœ¨åŸŸå†…å’ŒåŸŸå¤–åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºåŸºçº¿ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†æ ¼å¼å¥–åŠ±åœ¨ä¿ƒè¿›å¤šæ¨¡æ€LLMä¸­çš„å¯è¿ç§»æ¨ç†æŠ€èƒ½æ–¹é¢çš„ä½œç”¨ï¼Œå¹¶ä¸ºä»è®­ç»ƒæ—¶çš„æ¨ç†æ·±åº¦ä¸æµ‹è¯•æ—¶çš„å“åº”çµæ´»æ€§ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ç‚¹æä¾›äº†çµæ„Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06999v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†èƒ½åŠ›ä¸Šè‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å†³æ•°å­¦é—®é¢˜ç­‰å¤æ‚ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œå¤šæ¨¡æ€æ¨ç†ç ”ç©¶ä»éœ€è¦æ¢ç´¢æ¨¡æ€å¯¹é½å’Œè®­ç»ƒæˆæœ¬ã€‚ä¸ºæé«˜å¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†Deliberate-to-Intuitiveæ¨ç†æ¡†æ¶ï¼ˆD2Iï¼‰ï¼Œæ— éœ€é¢å¤–æ ‡æ³¨å’Œå¤æ‚çš„å¥–åŠ±ã€‚è¯¥æ–¹æ³•é€šè¿‡è®­ç»ƒæ—¶è®¾ç½®åˆ»æ„æ¨ç†ç­–ç•¥å¢å¼ºæ¨¡æ€å¯¹é½ï¼Œä»…é‡‡ç”¨åŸºäºè§„åˆ™çš„æ ¼å¼å¥–åŠ±ã€‚è¯„ä¼°æ—¶ï¼Œæ¨ç†é£æ ¼è½¬å˜ä¸ºç›´è§‰å¼ï¼Œåæ˜ æ¨¡å‹çš„ä¹ å¾—èƒ½åŠ›ã€‚D2Iåœ¨åŸŸå†…å’ŒåŸŸå¤–åŸºå‡†æµ‹è¯•ä¸­éƒ½ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œæ ¼å¼å¥–åŠ±åœ¨åŸ¹å…»MLLMçš„å¯è¿ç§»æ¨ç†æŠ€èƒ½ä¸­å‘æŒ¥ä½œç”¨ï¼Œå¹¶ä¸ºè®­ç»ƒå’Œæµ‹è¯•æ—¶çš„æ¨ç†æ·±åº¦ä¸å“åº”çµæ´»æ€§è§£è€¦æä¾›äº†å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚ä»»åŠ¡å¦‚æ•°å­¦é—®é¢˜è§£å†³ä¸­éœ€è¦å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¤šæ¨¡æ€æ¨ç†ç ”ç©¶é¢ä¸´æ¨¡æ€å¯¹é½å’Œè®­ç»ƒæˆæœ¬çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰çš„å¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰æ–¹æ³•å¸¸ä¾èµ–é¢å¤–çš„æ•°æ®æ ‡æ³¨å’Œè§„åˆ™å¥–åŠ±æ¥æå‡ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œè¿™å¢åŠ äº†è®­ç»ƒæˆæœ¬å’Œé™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚</li>
<li>æå‡ºçš„Deliberate-to-Intuitiveæ¨ç†æ¡†æ¶ï¼ˆD2Iï¼‰é€šè¿‡è®­ç»ƒæ—¶çš„åˆ»æ„æ¨ç†ç­–ç•¥å’ŒåŸºäºè§„åˆ™çš„æ ¼å¼å¥–åŠ±å¢å¼ºæ¨¡æ€å¯¹é½ã€‚</li>
<li>D2Iåœ¨è¯„ä¼°å’Œæµ‹è¯•æ—¶å±•ç°å‡ºä¼˜ç§€çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸŸå†…å’ŒåŸŸå¤–åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
<li>æ ¼å¼å¥–åŠ±åœ¨åŸ¹å…»MLLMçš„å¯è¿ç§»æ¨ç†æŠ€èƒ½ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06999">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f0fab860fdba6b5314d3912f42599149.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d8a3e15acf8b42a76dc64737d722c70.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MCA-RG-Enhancing-LLMs-with-Medical-Concept-Alignment-for-Radiology-Report-Generation"><a href="#MCA-RG-Enhancing-LLMs-with-Medical-Concept-Alignment-for-Radiology-Report-Generation" class="headerlink" title="MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology   Report Generation"></a>MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology   Report Generation</h2><p><strong>Authors:Qilong Xing, Zikai Song, Youjia Zhang, Na Feng, Junqing Yu, Wei Yang</strong></p>
<p>Despite significant advancements in adapting Large Language Models (LLMs) for radiology report generation (RRG), clinical adoption remains challenging due to difficulties in accurately mapping pathological and anatomical features to their corresponding text descriptions. Additionally, semantic agnostic feature extraction further hampers the generation of accurate diagnostic reports. To address these challenges, we introduce Medical Concept Aligned Radiology Report Generation (MCA-RG), a knowledge-driven framework that explicitly aligns visual features with distinct medical concepts to enhance the report generation process. MCA-RG utilizes two curated concept banks: a pathology bank containing lesion-related knowledge, and an anatomy bank with anatomical descriptions. The visual features are aligned with these medical concepts and undergo tailored enhancement. We further propose an anatomy-based contrastive learning procedure to improve the generalization of anatomical features, coupled with a matching loss for pathological features to prioritize clinically relevant regions. Additionally, a feature gating mechanism is employed to filter out low-quality concept features. Finally, the visual features are corresponding to individual medical concepts, and are leveraged to guide the report generation process. Experiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate that MCA-RG achieves superior performance, highlighting its effectiveness in radiology report generation. </p>
<blockquote>
<p>å°½ç®¡åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€‚åº”äºæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç”±äºå°†ç—…ç†å’Œè§£å‰–ç‰¹å¾å‡†ç¡®æ˜ å°„åˆ°å…¶ç›¸åº”æ–‡æœ¬æè¿°ä¸­çš„å›°éš¾ï¼Œä¸´åºŠé‡‡ç”¨ä»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œè¯­ä¹‰æ— å…³çš„ç‰¹å¾æå–è¿›ä¸€æ­¥é˜»ç¢äº†å‡†ç¡®è¯Šæ–­æŠ¥å‘Šçš„ç”Ÿæˆã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒ»å­¦æ¦‚å¿µå¯¹é½æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆMCA-RGï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªçŸ¥è¯†é©±åŠ¨æ¡†æ¶ï¼Œé€šè¿‡æ˜ç¡®å¯¹é½è§†è§‰ç‰¹å¾ä¸ä¸åŒçš„åŒ»å­¦æ¦‚å¿µï¼Œä»¥å¢å¼ºæŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ã€‚MCA-RGåˆ©ç”¨ä¸¤ä¸ªå®šåˆ¶çš„æ¦‚å¿µåº“ï¼šä¸€ä¸ªåŒ…å«ç—…å˜ç›¸å…³çŸ¥è¯†çš„ç—…ç†åº“å’Œä¸€ä¸ªåŒ…å«è§£å‰–æè¿°çš„è§£å‰–åº“ã€‚è§†è§‰ç‰¹å¾ä¸è¿™äº›åŒ»å­¦æ¦‚å¿µå¯¹é½ï¼Œå¹¶è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å¢å¼ºã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§åŸºäºè§£å‰–å­¦çš„å¯¹æ¯”å­¦ä¹ ç¨‹åºï¼Œä»¥æé«˜è§£å‰–ç‰¹å¾çš„æ³›åŒ–èƒ½åŠ›ï¼Œç»“åˆç—…ç†ç‰¹å¾çš„åŒ¹é…æŸå¤±ï¼Œä»¥ä¼˜å…ˆå¤„ç†ä¸´åºŠç›¸å…³åŒºåŸŸã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨ç‰¹å¾é—¨æ§æœºåˆ¶æ¥è¿‡æ»¤æ‰ä½è´¨é‡çš„æ¦‚å¿µç‰¹å¾ã€‚æœ€ç»ˆï¼Œè§†è§‰ç‰¹å¾ä¸ä¸ªåˆ«åŒ»å­¦æ¦‚å¿µç›¸å¯¹åº”ï¼Œå¹¶ç”¨äºæŒ‡å¯¼æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ã€‚åœ¨MIMIC-CXRå’ŒCheXpert Plusä¸¤ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMCA-RGå–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ï¼Œçªæ˜¾å…¶åœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06992v1">PDF</a> MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>åœ¨é€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”¨äºæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä¸´åºŠé‡‡çº³ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚éš¾ä»¥å°†ç—…ç†å’Œè§£å‰–ç‰¹å¾å‡†ç¡®æ˜ å°„åˆ°ç›¸åº”çš„æ–‡æœ¬æè¿°ä¸­ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºåŒ»å­¦æ¦‚å¿µå¯¹é½æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆMCA-RGï¼‰çŸ¥è¯†é©±åŠ¨æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ˜¾å¼å¯¹é½è§†è§‰ç‰¹å¾ä¸ä¸åŒçš„åŒ»å­¦æ¦‚å¿µï¼Œä»¥æå‡æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMCA-RGåœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ–¹é¢å…·æœ‰å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ–¹é¢çš„åº”ç”¨è™½ç„¶å·²ç»å–å¾—è¿›å±•ï¼Œä½†ä»é¢ä¸´ä¸´åºŠé‡‡çº³çš„æŒ‘æˆ˜ã€‚</li>
<li>åŒ»å­¦æ¦‚å¿µå¯¹é½æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ¡†æ¶ï¼ˆMCA-RGï¼‰è¢«æå‡ºæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>MCA-RGåˆ©ç”¨ä¸¤ä¸ªåŒ»å­¦æ¦‚å¿µåº“ï¼šç—…ç†åº“åŒ…å«ç—…å˜ç›¸å…³çŸ¥è¯†ï¼Œè§£å‰–åº“åŒ…å«è§£å‰–æè¿°ã€‚</li>
<li>MCA-RGé€šè¿‡å¯¹é½è§†è§‰ç‰¹å¾ä¸åŒ»å­¦æ¦‚å¿µæ¥å¢å¼ºæŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>è§£å‰–å­¦å¯¹æ¯”å­¦ä¹ ç¨‹åºå’ŒåŒ¹é…æŸå¤±è¢«ç”¨æ¥æ”¹å–„è§£å‰–ç‰¹å¾çš„æ³›åŒ–èƒ½åŠ›å’Œä¼˜å…ˆå¤„ç†ä¸´åºŠç›¸å…³åŒºåŸŸã€‚</li>
<li>ç‰¹å¾é—¨æ§æœºåˆ¶è¢«ç”¨æ¥è¿‡æ»¤æ‰ä½è´¨é‡çš„åŒ»å­¦æ¦‚å¿µç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06992">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15b81fe7dad1533ac3403bd93cd5d4c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-872b9774df4bb8a8be8b77965a2f750f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dfa6df4a223444bbe292cc0df5aa4d3.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Exploring-LLMs-for-Predicting-Tutor-Strategy-and-Student-Outcomes-in-Dialogues"><a href="#Exploring-LLMs-for-Predicting-Tutor-Strategy-and-Student-Outcomes-in-Dialogues" class="headerlink" title="Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in   Dialogues"></a>Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in   Dialogues</h2><p><strong>Authors:Fareya Ikram, Alexander Scarlatos, Andrew Lan</strong></p>
<p>Tutoring dialogues have gained significant attention in recent years, given the prominence of online learning and the emerging tutoring abilities of artificial intelligence (AI) agents powered by large language models (LLMs). Recent studies have shown that the strategies used by tutors can have significant effects on student outcomes, necessitating methods to predict how tutors will behave and how their actions impact students. However, few works have studied predicting tutor strategy in dialogues. Therefore, in this work we investigate the ability of modern LLMs, particularly Llama 3 and GPT-4o, to predict both future tutor moves and student outcomes in dialogues, using two math tutoring dialogue datasets. We find that even state-of-the-art LLMs struggle to predict future tutor strategy while tutor strategy is highly indicative of student outcomes, outlining a need for more powerful methods to approach this task. </p>
<blockquote>
<p>è¾…å¯¼å¯¹è¯è¿‘å¹´æ¥å¤‡å—å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨çº¿å­¦ä¹ çš„æ™®åŠä»¥åŠç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ä»£ç†çš„è¾…å¯¼èƒ½åŠ›çš„å…´èµ·ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¯¼å¸ˆä½¿ç”¨çš„ç­–ç•¥å¯¹å­¦ç”Ÿçš„æˆæœæœ‰é‡å¤§å½±å“ï¼Œå› æ­¤éœ€è¦æ–¹æ³•é¢„æµ‹å¯¼å¸ˆçš„è¡Œä¸ºä»¥åŠä»–ä»¬çš„è¡Œä¸ºå¦‚ä½•å½±å“å­¦ç”Ÿã€‚ç„¶è€Œï¼Œå¾ˆå°‘æœ‰å·¥ä½œç ”ç©¶é¢„æµ‹å¯¹è¯ä¸­çš„è¾…å¯¼ç­–ç•¥ã€‚å› æ­¤ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç°ä»£LLMï¼ˆç‰¹åˆ«æ˜¯Llama 3å’ŒGPT-4oï¼‰åœ¨å¯¹è¯ä¸­é¢„æµ‹æœªæ¥å¯¼å¸ˆçš„åŠ¨ä½œå’Œå­¦ç”Ÿæˆæœçš„èƒ½åŠ›ï¼Œä½¿ç”¨äº†ä¸¤ä¸ªæ•°å­¦è¾…å¯¼å¯¹è¯æ•°æ®é›†ã€‚æˆ‘ä»¬å‘ç°ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„LLMåœ¨é¢„æµ‹æœªæ¥è¾…å¯¼ç­–ç•¥æ—¶ä¹Ÿä¼šé‡åˆ°å›°éš¾ï¼Œè€Œè¾…å¯¼ç­–ç•¥éå¸¸èƒ½è¯´æ˜å­¦ç”Ÿçš„æˆæœï¼Œè¿™çªå‡ºäº†éœ€è¦æ›´å¼ºå¤§çš„æ–¹æ³•æ¥åº”å¯¹è¿™é¡¹ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06910v1">PDF</a> Published in BEA 2025: 20th Workshop on Innovative Use of NLP for   Building Educational Applications</p>
<p><strong>Summary</strong>ï¼šè¿‘å¹´æ¥ï¼Œç”±äºåœ¨çº¿å­¦ä¹ çš„æ™®åŠä»¥åŠç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ä»£ç†çš„è¾…å¯¼èƒ½åŠ›çªæ˜¾ï¼Œè¾…å¯¼å¯¹è¯å·²å¼•èµ·å¹¿æ³›å…³æ³¨ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¯¼å¸ˆçš„ç­–ç•¥å¯¹å­¦ç”Ÿæˆç»©æœ‰é‡è¦å½±å“ï¼Œå› æ­¤éœ€è¦æ–¹æ³•é¢„æµ‹å¯¼å¸ˆçš„è¡Œä¸ºåŠå…¶å¯¹å­¦ç”Ÿå½±å“çš„ç¨‹åº¦ã€‚æœ¬ç ”ç©¶è°ƒæŸ¥äº†ç°ä»£LLMsï¼ˆç‰¹åˆ«æ˜¯Llama 3å’ŒGPT-4oï¼‰åœ¨å¯¹è¯ä¸­é¢„æµ‹æœªæ¥å¯¼å¸ˆåŠ¨ä½œå’Œå­¦ç”Ÿæˆç»©çš„èƒ½åŠ›ï¼Œä½¿ç”¨ä¸¤ä¸ªæ•°å­¦è¾…å¯¼å¯¹è¯æ•°æ®é›†ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¹Ÿå¾ˆéš¾é¢„æµ‹æœªæ¥çš„å¯¼å¸ˆç­–ç•¥ï¼Œè€Œå¯¼å¸ˆç­–ç•¥é«˜åº¦é¢„ç¤ºå­¦ç”Ÿæˆç»©ï¼Œè¿™çªæ˜¾äº†éœ€è¦æ›´å¼ºå¤§çš„æ–¹æ³•æ¥åº”å¯¹è¿™é¡¹ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>è¾…å¯¼å¯¹è¯è¿‘å¹´æ¥å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä¸»è¦ç”±äºåœ¨çº¿å­¦ä¹ çš„æ™®åŠå’ŒAIä»£ç†çš„è¾…å¯¼èƒ½åŠ›æé«˜ã€‚</li>
<li>å¯¼å¸ˆçš„ç­–ç•¥å¯¹å­¦ç”Ÿæˆç»©æœ‰é‡è¦çš„å½±å“ã€‚</li>
<li>LLMsï¼Œå¦‚Llama 3å’ŒGPT-4oï¼Œè¢«ç”¨äºé¢„æµ‹å¯¹è¯ä¸­çš„æœªæ¥å¯¼å¸ˆåŠ¨ä½œå’Œå­¦ç”Ÿæˆç»©ã€‚</li>
<li>æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢„æµ‹æœªæ¥å¯¼å¸ˆç­–ç•¥æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å¯¼å¸ˆçš„ç­–ç•¥é«˜åº¦é¢„ç¤ºå­¦ç”Ÿæˆç»©ã€‚</li>
<li>éœ€è¦æ›´å¼ºå¤§çš„æ–¹æ³•æ¥é¢„æµ‹å¯¼å¸ˆçš„è¡Œä¸ºåŠå…¶å¯¹å­¦ç”Ÿå½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06910">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eeba1b1332411093e8fdd73b9928bd43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa7c123067e6970fa7661bca9a6ee314.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ab4ee37f6ffa420181255bd67e6169b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MultiJustice-A-Chinese-Dataset-for-Multi-Party-Multi-Charge-Legal-Prediction"><a href="#MultiJustice-A-Chinese-Dataset-for-Multi-Party-Multi-Charge-Legal-Prediction" class="headerlink" title="MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal   Prediction"></a>MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal   Prediction</h2><p><strong>Authors:Xiao Wang, Jiahuan Pei, Diancheng Shui, Zhiguang Han, Xin Sun, Dawei Zhu, Xiaoyu Shen</strong></p>
<p>Legal judgment prediction offers a compelling method to aid legal practitioners and researchers. However, the research question remains relatively under-explored: Should multiple defendants and charges be treated separately in LJP? To address this, we introduce a new dataset namely multi-person multi-charge prediction (MPMCP), and seek the answer by evaluating the performance of several prevailing legal large language models (LLMs) on four practical legal judgment scenarios: (S1) single defendant with a single charge, (S2) single defendant with multiple charges, (S3) multiple defendants with a single charge, and (S4) multiple defendants with multiple charges. We evaluate the dataset across two LJP tasks, i.e., charge prediction and penalty term prediction. We have conducted extensive experiments and found that the scenario involving multiple defendants and multiple charges (S4) poses the greatest challenges, followed by S2, S3, and S1. The impact varies significantly depending on the model. For example, in S4 compared to S1, InternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD, while Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD. Our dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/lololo-xiao/MultiJustice-MPMCP">https://github.com/lololo-xiao/MultiJustice-MPMCP</a>. </p>
<blockquote>
<p>æ³•å¾‹åˆ¤å†³é¢„æµ‹ä¸ºæ³•å¾‹ä»ä¸šè€…å’Œç ”ç©¶äººå‘˜æä¾›äº†ä¸€ç§å¸å¼•äººçš„è¾…åŠ©æ–¹æ³•ã€‚ç„¶è€Œï¼Œç›¸å…³ç ”ç©¶é—®é¢˜ä»ç„¶ç›¸å¯¹è¢«å¿½è§†ï¼šåœ¨æ³•å¾‹åˆ¤å†³é¢„æµ‹ä¸­æ˜¯å¦åº”è¯¥å°†å¤šåè¢«å‘Šå’Œå¤šé¡¹æŒ‡æ§åˆ†åˆ«å¤„ç†ï¼Ÿä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œå³å¤šäººå¤šæŒ‡æ§é¢„æµ‹ï¼ˆMPMCPï¼‰ï¼Œå¹¶é€šè¿‡è¯„ä¼°å‡ ä¸ªæµè¡Œçš„æ³•å¾‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å››ç§å®é™…æ³•å¾‹åˆ¤å†³åœºæ™¯ä¸­çš„è¡¨ç°æ¥å¯»æ‰¾ç­”æ¡ˆï¼šï¼ˆS1ï¼‰å•ä¸€è¢«å‘Šå•ä¸€æŒ‡æ§ï¼Œï¼ˆS2ï¼‰å•ä¸€è¢«å‘Šå¤šé¡¹æŒ‡æ§ï¼Œï¼ˆS3ï¼‰å¤šåè¢«å‘Šå•ä¸€æŒ‡æ§ï¼Œä»¥åŠï¼ˆS4ï¼‰å¤šåè¢«å‘Šå¤šé¡¹æŒ‡æ§ã€‚æˆ‘ä»¬å¯¹æ•°æ®é›†è¿›è¡Œäº†ä¸¤é¡¹æ³•å¾‹åˆ¤å†³é¢„æµ‹ä»»åŠ¡çš„è¯„ä¼°ï¼Œå³ç½ªåé¢„æµ‹å’Œå¤„ç½šæ¡æ¬¾é¢„æµ‹ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œå‘ç°æ¶‰åŠå¤šåè¢«å‘Šå’Œå¤šé¡¹æŒ‡æ§çš„åœºæ™¯ï¼ˆS4ï¼‰æœ€å…·æŒ‘æˆ˜æ€§ï¼Œå…¶æ¬¡æ˜¯S2ã€S3å’ŒS1ã€‚å½±å“ç¨‹åº¦å› æ¨¡å‹è€Œå¼‚ã€‚ä¾‹å¦‚ï¼Œåœ¨S4ä¸S1ç›¸æ¯”ï¼ŒInternLM2çš„F1åˆ†æ•°é™ä½äº†çº¦4.5%ï¼ŒLogDå¢åŠ äº†2.8%ï¼Œè€ŒLawformerçš„F1åˆ†æ•°é™ä½äº†çº¦19.7%ï¼ŒLogDå¢åŠ äº†19.0%ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lololo-xiao/MultiJustice-MPMCP">https://github.com/lololo-xiao/MultiJustice-MPMCP</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06909v1">PDF</a> Accepted by NLPCC 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>é’ˆå¯¹æ³•å¾‹åˆ¤å†³é¢„æµ‹ä¸­å¤šè¢«å‘Šäººå¤šæŒ‡æ§æƒ…å†µçš„ç ”ç©¶é—®é¢˜ï¼Œå¼•å…¥å¤šäººç‰©å¤šæŒ‡æ§é¢„æµ‹ï¼ˆMPMCPï¼‰æ•°æ®é›†ï¼Œé€šè¿‡è¯„ä¼°å¤šç§ä¸»æµæ³•å¾‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å››ç§å®é™…æ³•å¾‹åˆ¤å†³åœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œæ¢è®¨æ˜¯å¦åº”å°†å¤šè¢«å‘Šäººå’Œå¤šé¡¹æŒ‡æ§åˆ†å¼€å¤„ç†ã€‚ç ”ç©¶å‘ç°ï¼Œæ¶‰åŠå¤šè¢«å‘Šäººå¤šæŒ‡æ§çš„åœºæ™¯æœ€å…·æŒ‘æˆ˜æ€§ï¼Œä¸åŒæ¨¡å‹çš„å½±å“å·®å¼‚æ˜¾è‘—ã€‚æ•°æ®é›†å’Œä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ³•å¾‹åˆ¤å†³é¢„æµ‹æ˜¯è¾…åŠ©æ³•å¾‹ä»ä¸šè€…å’Œç ”ç©¶äººå‘˜çš„æœ‰åŠ›æ–¹æ³•ã€‚</li>
<li>å¼•å…¥å¤šäººç‰©å¤šæŒ‡æ§é¢„æµ‹ï¼ˆMPMCPï¼‰æ•°æ®é›†ï¼Œé’ˆå¯¹å¤šè¢«å‘Šäººå’Œå¤šé¡¹æŒ‡æ§æƒ…å†µè¿›è¡Œç ”ç©¶ã€‚</li>
<li>é€šè¿‡è¯„ä¼°å¤šç§ä¸»æµæ³•å¾‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å››ç§å®é™…æ³•å¾‹åˆ¤å†³åœºæ™¯çš„è¡¨ç°æ¥æ¢è®¨ç ”ç©¶é—®é¢˜ã€‚</li>
<li>æ¶‰åŠå¤šè¢«å‘Šäººå¤šæŒ‡æ§çš„åœºæ™¯ï¼ˆS4ï¼‰æœ€å…·æŒ‘æˆ˜æ€§ã€‚</li>
<li>ä¸åŒæ¨¡å‹åœ¨å¤„ç†ä¸åŒåœºæ™¯æ—¶çš„è¡¨ç°å­˜åœ¨å·®å¼‚ã€‚</li>
<li>InternLM2å’ŒLawformeråœ¨S4ç›¸æ¯”S1åœºæ™¯ä¸‹çš„è¡¨ç°å—åˆ°è¾ƒå¤§å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06909">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15b80ab0ac1672cea05be4b3918fac74.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35b6636740bec71d403330fedf176861.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0d01b46c438a7332525570efed2d4c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a5daaf9eab5b7bae91b05e957ad72e48.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d5dfafed5d92590cc6108a0f791f63af.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SCoRE-Streamlined-Corpus-based-Relation-Extraction-using-Multi-Label-Contrastive-Learning-and-Bayesian-kNN"><a href="#SCoRE-Streamlined-Corpus-based-Relation-Extraction-using-Multi-Label-Contrastive-Learning-and-Bayesian-kNN" class="headerlink" title="SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label   Contrastive Learning and Bayesian kNN"></a>SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label   Contrastive Learning and Bayesian kNN</h2><p><strong>Authors:Luca Mariotti, Veronica Guidetti, Federica Mandreoli</strong></p>
<p>The growing demand for efficient knowledge graph (KG) enrichment leveraging external corpora has intensified interest in relation extraction (RE), particularly under low-supervision settings. To address the need for adaptable and noise-resilient RE solutions that integrate seamlessly with pre-trained large language models (PLMs), we introduce SCoRE, a modular and cost-effective sentence-level RE system. SCoRE enables easy PLM switching, requires no finetuning, and adapts smoothly to diverse corpora and KGs. By combining supervised contrastive learning with a Bayesian k-Nearest Neighbors (kNN) classifier for multi-label classification, it delivers robust performance despite the noisy annotations of distantly supervised corpora. To improve RE evaluation, we propose two novel metrics: Correlation Structure Distance (CSD), measuring the alignment between learned relational patterns and KG structures, and Precision at R (P@R), assessing utility as a recommender system. We also release Wiki20d, a benchmark dataset replicating real-world RE conditions where only KG-derived annotations are available. Experiments on five benchmarks show that SCoRE matches or surpasses state-of-the-art methods while significantly reducing energy consumption. Further analyses reveal that increasing model complexity, as seen in prior work, degrades performance, highlighting the advantages of SCoREâ€™s minimal design. Combining efficiency, modularity, and scalability, SCoRE stands as an optimal choice for real-world RE applications. </p>
<blockquote>
<p>å¯¹äºåˆ©ç”¨å¤–éƒ¨è¯­æ–™åº“è¿›è¡Œé«˜æ•ˆçŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰å¢å¼ºçš„æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ï¼Œå·²ç»åŠ å‰§äº†äººä»¬å¯¹å…³ç³»æŠ½å–ï¼ˆREï¼‰çš„å…´è¶£ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½ç›‘ç£è®¾ç½®ä¸‹ã€‚ä¸ºäº†è§£å†³éœ€è¦ä¸é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰æ— ç¼é›†æˆçš„å¯é€‚åº”å’Œå™ªå£°è€ç”¨çš„REè§£å†³æ–¹æ¡ˆçš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SCoREï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–ä¸”ç»æµå®æƒ çš„å¥å­çº§REç³»ç»Ÿã€‚SCoREå¯ä»¥è½»æ¾åˆ‡æ¢PLMï¼Œæ— éœ€å¾®è°ƒï¼Œå¹¶å¯é¡ºåˆ©é€‚åº”å„ç§è¯­æ–™åº“å’ŒçŸ¥è¯†å›¾è°±ã€‚å®ƒé€šè¿‡ç»“åˆæœ‰ç›‘ç£çš„å¯¹æ¯”å­¦ä¹ ä¸è´å¶æ–¯kæœ€è¿‘é‚»ï¼ˆkNNï¼‰åˆ†ç±»å™¨è¿›è¡Œå¤šæ ‡ç­¾åˆ†ç±»ï¼Œå³ä½¿åœ¨è¿œç¨‹ç›‘ç£è¯­æ–™åº“çš„å™ªå£°æ ‡æ³¨ä¸‹ä¹Ÿèƒ½å®ç°ç¨³å¥çš„æ€§èƒ½ã€‚ä¸ºäº†æ”¹å–„REè¯„ä¼°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªæ–°æŒ‡æ ‡ï¼šå…³è”ç»“æ„è·ç¦»ï¼ˆCSDï¼‰ï¼Œç”¨äºè¡¡é‡å­¦ä¹ åˆ°çš„å…³ç³»æ¨¡å¼ä¸çŸ¥è¯†å›¾è°±ç»“æ„ä¹‹é—´çš„å¯¹é½ç¨‹åº¦ï¼›ä»¥åŠç²¾å‡†åº¦Rï¼ˆP@Rï¼‰ï¼Œç”¨äºè¯„ä¼°ä½œä¸ºæ¨èç³»ç»Ÿçš„å®ç”¨æ€§ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†Wiki20dï¼Œè¿™æ˜¯ä¸€ä¸ªå¤åˆ¶ç°å®REæ¡ä»¶çš„åŸºå‡†æ•°æ®é›†ï¼Œå…¶ä¸­åªæœ‰ä»çŸ¥è¯†å›¾è°±æ´¾ç”Ÿçš„æ³¨é‡Šå¯ç”¨ã€‚åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSCoREçš„æ–¹æ³•ä¸æœ€æ–°æŠ€æœ¯ç›¸åŒ¹é…æˆ–è¶…è¶Šï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†èƒ½è€—ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œå¦‚å…ˆå‰å·¥ä½œæ‰€è§ï¼Œå¢åŠ æ¨¡å‹å¤æ‚æ€§ä¼šé™ä½æ€§èƒ½ï¼Œä»è€Œçªå‡ºäº†SCoREè®¾è®¡çš„ä¼˜åŠ¿ã€‚é€šè¿‡ç»“åˆæ•ˆç‡ã€æ¨¡å—åŒ–å’Œå¯æ‰©å±•æ€§ï¼ŒSCoREæˆä¸ºé€‚ç”¨äºç°å®REåº”ç”¨çš„æœ€ä½³é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06895v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ä¸ªåä¸ºSCoREçš„æ¨¡å—åŒ–ã€ä½æˆæœ¬å¥å­çº§å…³ç³»æŠ½å–ç³»ç»Ÿï¼Œç”¨äºæ»¡è¶³åˆ©ç”¨å¤–éƒ¨è¯­æ–™åº“è¿›è¡Œé«˜æ•ˆçŸ¥è¯†å›¾è°±å¯ŒåŒ–çš„æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ã€‚SCoREç³»ç»Ÿæ˜“äºä¸é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆï¼Œæ— éœ€å¾®è°ƒï¼Œå¹¶èƒ½é€‚åº”å„ç§è¯­æ–™åº“å’ŒçŸ¥è¯†å›¾è°±ã€‚é€šè¿‡ç»“åˆç›‘ç£å¯¹æ¯”å­¦ä¹ ä¸è´å¶æ–¯kæœ€è¿‘é‚»åˆ†ç±»å™¨è¿›è¡Œå¤šæ ‡ç­¾åˆ†ç±»ï¼ŒSCoREåœ¨å™ªå£°æ ‡æ³¨çš„è¿œç¨‹ç›‘ç£è¯­æ–™åº“ä¸­å®ç°äº†ç¨³å¥çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸¤ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼šç”¨äºè¡¡é‡æ‰€å­¦å…³ç³»æ¨¡å¼ä¸çŸ¥è¯†å›¾è°±ç»“æ„å¯¹é½ç¨‹åº¦çš„ç›¸å…³æ€§ç»“æ„è·ç¦»ï¼ˆCSDï¼‰å’Œç”¨äºè¯„ä¼°æ¨èç³»ç»Ÿå®ç”¨æ€§çš„ç²¾ç¡®åº¦Rï¼ˆP@Rï¼‰ã€‚å®éªŒè¡¨æ˜ï¼ŒSCoREåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æˆ–è¶…è¶Šäº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†èƒ½è€—ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œä¸å…¶ä»–å¤æ‚æ¨¡å‹ç›¸æ¯”ï¼ŒSCoREçš„ç®€æ´è®¾è®¡å¸¦æ¥äº†ä¼˜åŠ¿ã€‚è¯¥ç³»ç»Ÿå…·æœ‰é«˜æ•ˆæ€§ã€æ¨¡å—åŒ–ä»¥åŠå¯æ‰©å±•æ€§ï¼Œæ˜¯ç°å®å…³ç³»æŠ½å–åº”ç”¨çš„ç†æƒ³é€‰æ‹©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SCoREæ˜¯ä¸€ä¸ªæ¨¡å—åŒ–ã€ä½æˆæœ¬çš„å…³ç³»æŠ½å–ç³»ç»Ÿï¼Œé€‚ç”¨äºé«˜æ•ˆçŸ¥è¯†å›¾è°±å¯ŒåŒ–ã€‚</li>
<li>SCoREæ˜“äºä¸é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆï¼Œä¸”èƒ½é€‚åº”ä¸åŒçš„è¯­æ–™åº“å’ŒçŸ¥è¯†å›¾è°±ã€‚</li>
<li>ç»“åˆç›‘ç£å¯¹æ¯”å­¦ä¹ ä¸è´å¶æ–¯kæœ€è¿‘é‚»åˆ†ç±»å™¨çš„å¤šæ ‡ç­¾åˆ†ç±»æ–¹æ³•ä½¿å¾—SCoREåœ¨å™ªå£°æ ‡æ³¨çš„è¯­æ–™åº“ä¸­è¡¨ç°ç¨³å¥ã€‚</li>
<li>æå‡ºäº†ä¸¤ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡CSDå’ŒP@Ræ¥è¡¡é‡å…³ç³»æŠ½å–çš„æ•ˆæœã€‚</li>
<li>SCoREåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜ç§€ï¼ŒåŒæ—¶é™ä½äº†èƒ½è€—ã€‚</li>
<li>ä¸å¤æ‚æ¨¡å‹ç›¸æ¯”ï¼ŒSCoREçš„ç®€æ´è®¾è®¡å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06895">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ce0122e96ef5d78bd382735894eaa72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d08bb32000e70c0a1a37a89aac02ce83.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="TokenShapley-Token-Level-Context-Attribution-with-Shapley-Value"><a href="#TokenShapley-Token-Level-Context-Attribution-with-Shapley-Value" class="headerlink" title="TokenShapley: Token Level Context Attribution with Shapley Value"></a>TokenShapley: Token Level Context Attribution with Shapley Value</h2><p><strong>Authors:Yingtai Xiao, Yuqing Zhu, Sirat Samyoun, Wanrong Zhang, Jiachen T. Wang, Jian Du</strong></p>
<p>Large language models (LLMs) demonstrate strong capabilities in in-context learning, but verifying the correctness of their generated responses remains a challenge. Prior work has explored attribution at the sentence level, but these methods fall short when users seek attribution for specific keywords within the response, such as numbers, years, or names. To address this limitation, we propose TokenShapley, a novel token-level attribution method that combines Shapley value-based data attribution with KNN-based retrieval techniques inspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed datastore for contextual retrieval and computing Shapley values to quantify token importance, TokenShapley provides a fine-grained data attribution approach. Extensive evaluations on four benchmarks show that TokenShapley outperforms state-of-the-art baselines in token-level attribution, achieving an 11-23% improvement in accuracy. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†éªŒè¯å…¶ç”Ÿæˆå“åº”çš„æ­£ç¡®æ€§ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æ—©æœŸçš„å·¥ä½œå·²ç»æ¢ç´¢äº†å¥å­çº§åˆ«çš„å½’å› ï¼Œä½†å½“ç”¨æˆ·å¯»æ±‚å¯¹å“åº”å†…ç‰¹å®šå…³é”®è¯ï¼ˆå¦‚æ•°å­—ã€å¹´ä»½æˆ–åç§°ï¼‰çš„å½’å› æ—¶ï¼Œè¿™äº›æ–¹æ³•å°±æ˜¾å¾—ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†TokenShapleyï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è¯çº§å½’å› æ–¹æ³•ï¼Œå®ƒç»“åˆäº†åŸºäºShapleyå€¼çš„æ•°æ®å½’å› å’Œå—æœ€è¿‘KNNå¢å¼ºLLMsæŠ€æœ¯å¯å‘çš„åŸºäºKNNçš„æ£€ç´¢æŠ€æœ¯ã€‚é€šè¿‡åˆ©ç”¨é¢„è®¡ç®—çš„æ•°æ®å­˜å‚¨è¿›è¡Œä¸Šä¸‹æ–‡æ£€ç´¢å¹¶è®¡ç®—Shapleyå€¼æ¥é‡åŒ–ä»¤ç‰Œçš„é‡è¦æ€§ï¼ŒTokenShapleyæä¾›äº†ä¸€ç§ç²¾ç»†çš„æ•°æ®å½’å› æ–¹æ³•ã€‚åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒTokenShapleyåœ¨è¯çº§å½’å› æ–¹é¢ä¼˜äºæœ€æ–°åŸºçº¿æŠ€æœ¯ï¼Œå‡†ç¡®ç‡æé«˜äº†11-23%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05261v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å…¶ç”Ÿæˆå“åº”çš„æ­£ç¡®æ€§éªŒè¯ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å¥å­çº§åˆ«çš„å½’å› ä¸Šï¼Œä½†å½“ç”¨æˆ·éœ€è¦é’ˆå¯¹å“åº”ä¸­çš„ç‰¹å®šå…³é”®è¯ï¼ˆå¦‚æ•°å­—ã€å¹´ä»½æˆ–åç§°ï¼‰è¿›è¡Œå½’å› æ—¶ï¼Œè¿™äº›æ–¹æ³•å°±æ˜¾å¾—ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†TokenShapleyï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ä»¤ç‰Œçº§åˆ«å½’å› æ–¹æ³•ï¼Œå®ƒç»“åˆäº†åŸºäºShapleyå€¼çš„æ•°æ®å½’å› å’ŒåŸºäºKNNçš„æ£€ç´¢æŠ€æœ¯ã€‚TokenShapleyåˆ©ç”¨é¢„å…ˆè®¡ç®—çš„æ•°æ®å­˜å‚¨è¿›è¡Œä¸Šä¸‹æ–‡æ£€ç´¢ï¼Œå¹¶é€šè¿‡è®¡ç®—Shapleyå€¼æ¥é‡åŒ–ä»¤ç‰Œçš„é‡è¦æ€§ï¼Œä»è€Œæä¾›äº†ä¸€ç§ç²¾ç»†åŒ–çš„æ•°æ®å½’å› æ–¹æ³•ã€‚åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒTokenShapleyåœ¨ä»¤ç‰Œçº§åˆ«å½’å› æ–¹é¢ä¼˜äºæœ€æ–°åŸºçº¿æŠ€æœ¯ï¼Œå‡†ç¡®ç‡æé«˜äº†11-23%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†éªŒè¯å…¶ç”Ÿæˆå“åº”çš„æ­£ç¡®æ€§ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å¥å­çº§åˆ«çš„å½’å› ä¸Šï¼Œæ— æ³•é’ˆå¯¹å“åº”ä¸­çš„ç‰¹å®šå…³é”®è¯è¿›è¡Œå½’å› ã€‚</li>
<li>TokenShapleyæ˜¯ä¸€ç§æ–°çš„ä»¤ç‰Œçº§åˆ«å½’å› æ–¹æ³•ï¼Œç»“åˆäº†åŸºäºShapleyå€¼çš„æ•°æ®å½’å› å’ŒåŸºäºKNNçš„æ£€ç´¢æŠ€æœ¯ã€‚</li>
<li>TokenShapleyåˆ©ç”¨é¢„å…ˆè®¡ç®—çš„æ•°æ®å­˜å‚¨è¿›è¡Œä¸Šä¸‹æ–‡æ£€ç´¢ã€‚</li>
<li>TokenShapleyé€šè¿‡è®¡ç®—Shapleyå€¼æ¥é‡åŒ–ä»¤ç‰Œçš„é‡è¦æ€§ï¼Œæä¾›ç²¾ç»†åŒ–çš„æ•°æ®å½’å› ã€‚</li>
<li>åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒTokenShapleyåœ¨ä»¤ç‰Œçº§åˆ«å½’å› æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05261">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0fa2ef95883f340382c887df1e3bccb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce3a365b6743664f5fec8c713ee30653.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5675bf4d3ad35aabbb5370419363704f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="XY-Tokenizer-Mitigating-the-Semantic-Acoustic-Conflict-in-Low-Bitrate-Speech-Codecs"><a href="#XY-Tokenizer-Mitigating-the-Semantic-Acoustic-Conflict-in-Low-Bitrate-Speech-Codecs" class="headerlink" title="XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate   Speech Codecs"></a>XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate   Speech Codecs</h2><p><strong>Authors:Yitian Gong, Luozhijie Jin, Ruifan Deng, Dong Zhang, Xin Zhang, Qinyuan Cheng, Zhaoye Fei, Shimin Li, Xipeng Qiu</strong></p>
<p>Speech codecs serve as bridges between speech signals and large language models. An ideal codec for speech language models should not only preserve acoustic information but also capture rich semantic information. However, existing speech codecs struggle to balance high-quality audio reconstruction with ease of modeling by language models. In this study, we analyze the limitations of previous codecs in balancing semantic richness and acoustic fidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict between semantic and acoustic capabilities through multi-stage, multi-task learning. Experimental results demonstrate that XY-Tokenizer achieves performance in both semantic and acoustic tasks comparable to that of state-of-the-art codecs operating at similar bitrates, even though those existing codecs typically excel in only one aspect. Specifically, XY-Tokenizer achieves strong text alignment, surpassing distillation-based semantic modeling methods such as SpeechTokenizer and Mimi, while maintaining a speaker similarity score of 0.83 between reconstructed and original audio. The reconstruction performance of XY-Tokenizer is comparable to that of BigCodec, the current state-of-the-art among acoustic-only codecs, which achieves a speaker similarity score of 0.84 at a similar bitrate. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/gyt1145028706/XY-Tokenizer">https://github.com/gyt1145028706/XY-Tokenizer</a>. </p>
<blockquote>
<p>è¯­éŸ³ç¼–è§£ç å™¨ä½œä¸ºè¯­éŸ³ä¿¡å·ä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹é—´çš„æ¡¥æ¢ã€‚å¯¹äºè¯­éŸ³è¯­è¨€æ¨¡å‹è€Œè¨€ï¼Œç†æƒ³çš„ç¼–è§£ç å™¨ä¸ä»…è¦ä¿ç•™å£°éŸ³ä¿¡æ¯ï¼Œè¿˜è¦æ•æ‰ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯­éŸ³ç¼–è§£ç å™¨åœ¨å¹³è¡¡é«˜è´¨é‡éŸ³é¢‘é‡å»ºä¸è¯­è¨€æ¨¡å‹çš„è½»æ¾å»ºæ¨¡æ–¹é¢å­˜åœ¨å›°éš¾ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†å…ˆå‰ç¼–è§£ç å™¨åœ¨å¹³è¡¡è¯­ä¹‰ä¸°å¯Œåº¦å’Œå£°éŸ³ä¿çœŸåº¦æ–¹é¢çš„å±€é™æ€§ã€‚æˆ‘ä»¬æå‡ºäº†XY-Tokenizerè¿™ä¸€æ–°å‹ç¼–è§£ç å™¨ï¼Œå®ƒé€šè¿‡å¤šé˜¶æ®µå¤šä»»åŠ¡å­¦ä¹ æ¥ç¼“è§£è¯­ä¹‰å’Œå£°éŸ³èƒ½åŠ›ä¹‹é—´çš„å†²çªã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒXY-Tokenizeråœ¨è¯­ä¹‰å’Œå£°éŸ³ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸ç±»ä¼¼æ¯”ç‰¹ç‡çš„å…ˆè¿›ç¼–è§£ç å™¨ç›¸å½“ï¼Œå°½ç®¡è¿™äº›ç°æœ‰ç¼–è§£ç å™¨é€šå¸¸åªåœ¨ä¸€æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚å…·ä½“æ¥è¯´ï¼ŒXY-Tokenizerå®ç°äº†å¼ºå¤§çš„æ–‡æœ¬å¯¹é½ï¼Œè¶…è¶Šäº†åŸºäºè’¸é¦çš„è¯­ä¹‰å»ºæ¨¡æ–¹æ³•ï¼Œå¦‚SpeechTokenizerå’ŒMimiï¼ŒåŒæ—¶ä¿æŒé‡å»ºéŸ³é¢‘ä¸åŸå§‹éŸ³é¢‘ä¹‹é—´çš„è¯´è¯äººç›¸ä¼¼åº¦ä¸º0.83ã€‚XY-Tokenizerçš„é‡å»ºæ€§èƒ½ä¸å½“å‰å…ˆè¿›çš„å£°éŸ³ç¼–è§£ç å™¨BigCodecç›¸å½“ï¼ŒBigCodecåœ¨ç±»ä¼¼æ¯”ç‰¹ç‡ä¸‹å®ç°äº†è¯´è¯äººç›¸ä¼¼åº¦å¾—åˆ†ä¸º0.84ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/gyt1145028706/XY-Tokenizer%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/gyt1145028706/XY-Tokenizeræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23325v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è¯­éŸ³ç¼–è§£ç å™¨åœ¨å¹³è¡¡è¯­ä¹‰ä¸°å¯Œæ€§å’Œå£°éŸ³ä¿çœŸåº¦æ–¹é¢çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„ç¼–è§£ç å™¨XY-Tokenizerã€‚é€šè¿‡å¤šé˜¶æ®µå¤šä»»åŠ¡å­¦ä¹ ï¼ŒXY-Tokenizerèƒ½å¤Ÿç¼“è§£è¯­ä¹‰å’Œå£°éŸ³èƒ½åŠ›ä¹‹é—´çš„å†²çªã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒXY-Tokenizeråœ¨è¯­ä¹‰å’Œå£°éŸ³ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸å½“å‰å…ˆè¿›çš„ç¼–è§£ç å™¨ç›¸å½“ï¼Œç”šè‡³åœ¨æŸäº›æ–¹é¢å®ç°äº†è¶…è¶Šã€‚ç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬å¯¹é½æ–¹é¢ï¼ŒXY-Tokenizerè¶…è¶Šäº†åŸºäºè’¸é¦çš„è¯­ä¹‰å»ºæ¨¡æ–¹æ³•ï¼Œå¦‚SpeechTokenizerå’ŒMimiï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„å£°éŸ³ç›¸ä¼¼æ€§å¾—åˆ†ã€‚è¯¥ç¼–è§£ç å™¨çš„ä»£ç å’Œæ¨¡å‹å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰è¯­éŸ³ç¼–è§£ç å™¨åœ¨å¹³è¡¡è¯­ä¹‰ä¸°å¯Œæ€§å’Œå£°éŸ³ä¿çœŸåº¦æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>XY-Tokenizeræ˜¯ä¸€ç§æ–°å‹ç¼–è§£ç å™¨ï¼Œæ—¨åœ¨ç¼“è§£è¯­ä¹‰å’Œå£°éŸ³èƒ½åŠ›ä¹‹é—´çš„å†²çªã€‚</li>
<li>XY-Tokenizeré€šè¿‡å¤šé˜¶æ®µå¤šä»»åŠ¡å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œæé«˜äº†æ€§èƒ½ã€‚</li>
<li>XY-Tokenizeråœ¨è¯­ä¹‰å’Œå£°éŸ³ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸å½“å‰å…ˆè¿›çš„ç¼–è§£ç å™¨ç›¸å½“ã€‚</li>
<li>XY-Tokenizeråœ¨æ–‡æœ¬å¯¹é½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†æŸäº›åŸºäºè’¸é¦çš„è¯­ä¹‰å»ºæ¨¡æ–¹æ³•ã€‚</li>
<li>XY-Tokenizerä¿æŒäº†è¾ƒé«˜çš„å£°éŸ³ç›¸ä¼¼æ€§å¾—åˆ†ï¼Œè¯´æ˜å…¶éŸ³é¢‘é‡å»ºæ€§èƒ½è‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23325">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d14a2ce612fdaf353163ef3c2694cf65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce4435ee0c2c82c62a8e5370514a84ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76021186d9547dc09caa8595beecf637.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41b8173205f48429d37e2912035bce47.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Multi-Attribute-Steering-of-Language-Models-via-Targeted-Intervention"><a href="#Multi-Attribute-Steering-of-Language-Models-via-Targeted-Intervention" class="headerlink" title="Multi-Attribute Steering of Language Models via Targeted Intervention"></a>Multi-Attribute Steering of Language Models via Targeted Intervention</h2><p><strong>Authors:Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal</strong></p>
<p>Inference-time intervention (ITI) has emerged as a promising method for steering large language model (LLM) behavior in a particular direction (e.g., improving helpfulness) by intervening on token representations without costly updates to the LLMâ€™s parameters. However, existing ITI approaches fail to scale to multi-attribute settings with conflicts, such as enhancing helpfulness while also reducing toxicity. To address this, we introduce Multi-Attribute Targeted Steering (MAT-Steer), a novel steering framework designed for selective token-level intervention across multiple attributes. MAT-Steer learns steering vectors using an alignment objective that shifts the modelâ€™s internal representations of undesirable outputs closer to those of desirable ones while enforcing sparsity and orthogonality among vectors for different attributes, thereby reducing inter-attribute conflicts. We evaluate MAT-Steer in two distinct settings: (i) on question answering (QA) tasks where we balance attributes like truthfulness, bias, and toxicity; (ii) on generative tasks where we simultaneously improve attributes like helpfulness, correctness, and coherence. MAT-Steer outperforms existing ITI and parameter-efficient fine-tuning approaches across both task types (e.g., 3% average accuracy gain across QA tasks and 55.82% win rate against the best ITI baseline). </p>
<blockquote>
<p>è¿è¡Œæ—¶å¹²é¢„ï¼ˆITIï¼‰ä½œä¸ºä¸€ç§æ–¹æ³•å·²ç»å´­éœ²å¤´è§’ï¼Œå®ƒé€šè¿‡å¹²é¢„ä»¤ç‰Œè¡¨ç¤ºæ¥å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœç‰¹å®šæ–¹å‘ï¼ˆä¾‹å¦‚ï¼Œæé«˜æœ‰ç”¨æ€§ï¼‰è¡Œä¸ºï¼Œè€Œæ— éœ€å¯¹LLMå‚æ•°è¿›è¡Œæ˜‚è´µçš„æ›´æ–°ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ITIæ–¹æ³•æ— æ³•æ‰©å±•åˆ°å…·æœ‰å†²çªçš„å¤šå±æ€§è®¾ç½®ï¼Œä¾‹å¦‚å¢å¼ºæœ‰ç”¨æ€§åŒæ—¶å‡å°‘æ¯’æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šå±æ€§ç›®æ ‡å¯¼å‘é©¾é©¶ï¼ˆMAT-Steerï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹é©¾é©¶æ¡†æ¶ï¼Œä¸“ä¸ºé€‰æ‹©æ€§ä»¤ç‰Œçº§å¹²é¢„å¤šä¸ªå±æ€§è€Œè®¾è®¡ã€‚MAT-Steerä½¿ç”¨å¯¹é½ç›®æ ‡å­¦ä¹ é©¾é©¶å‘é‡ï¼Œå°†æ¨¡å‹å¯¹ä¸è‰¯è¾“å‡ºçš„å†…éƒ¨è¡¨ç¤ºè½¬å‘è‰¯å¥½è¾“å‡ºçš„å†…éƒ¨è¡¨ç¤ºï¼ŒåŒæ—¶åœ¨ä¸åŒå±æ€§ä¹‹é—´å¼ºåˆ¶æ‰§è¡Œç¨€ç–æ€§å’Œæ­£äº¤æ€§ï¼Œä»è€Œå‡å°‘å±æ€§é—´çš„å†²çªã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªç‹¬ç‰¹çš„ç¯å¢ƒä¸­è¯„ä¼°äº†MAT-Steerï¼šï¼ˆiï¼‰é—®ç­”ï¼ˆQAï¼‰ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å¹³è¡¡çœŸå®æ€§ã€åè§å’Œæ¯’æ€§ç­‰å±æ€§ï¼›ï¼ˆiiï¼‰ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬åŒæ—¶æé«˜æœ‰ç”¨æ€§ã€æ­£ç¡®æ€§å’Œè¿è´¯æ€§ç­‰å±æ€§ã€‚MAT-Steeråœ¨ä¸¤ç§ä»»åŠ¡ç±»å‹ä¸­éƒ½ä¼˜äºç°æœ‰çš„ITIå’Œå‚æ•°æœ‰æ•ˆå¾®è°ƒæ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œåœ¨QAä»»åŠ¡ä¸­å¹³å‡ç²¾åº¦æé«˜3%ï¼Œå¯¹æœ€ä½³ITIåŸºå‡†çš„èƒœç‡ä¸º55.82%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12446v2">PDF</a> ACL 2025 camera-ready, code link:   <a target="_blank" rel="noopener" href="https://github.com/duykhuongnguyen/MAT-Steer">https://github.com/duykhuongnguyen/MAT-Steer</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¡Œä¸ºå¯ä»¥é€šè¿‡å¹²é¢„ä»¤ç‰Œè¡¨ç¤ºæ¥è¿›è¡Œå¼•å¯¼ï¼Œè€Œæ— éœ€æ˜‚è´µçš„æ¨¡å‹å‚æ•°æ›´æ–°ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†å¤šå±æ€§è®¾ç½®ä¸­çš„å†²çªé—®é¢˜ï¼Œå¦‚åŒæ—¶æé«˜æœ‰ç”¨æ€§å’Œé™ä½æ¯’æ€§ã€‚æˆ‘ä»¬æå‡ºäº†å¤šå±æ€§ç›®æ ‡å¼•å¯¼ï¼ˆMAT-Steerï¼‰æ¡†æ¶ï¼Œç”¨äºé€‰æ‹©æ€§åœ°å¯¹å¤šä¸ªå±æ€§çš„ä»¤ç‰Œçº§åˆ«è¿›è¡Œå¹²é¢„ã€‚MAT-Steerä½¿ç”¨å¯¹é½ç›®æ ‡æ¥å­¦ä¹ å¼•å¯¼å‘é‡ï¼Œå°†æ¨¡å‹å¯¹ä¸è‰¯è¾“å‡ºçš„å†…éƒ¨è¡¨ç¤ºè½¬å‘è‰¯å¥½è¾“å‡ºçš„è¡¨ç¤ºï¼ŒåŒæ—¶åœ¨ä¸åŒå±æ€§ä¹‹é—´å¼ºåˆ¶æ‰§è¡Œç¨€ç–æ€§å’Œæ­£äº¤æ€§ï¼Œä»è€Œå‡å°‘å±æ€§é—´çš„å†²çªã€‚åœ¨é—®ç­”å’Œç”Ÿæˆä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMAT-Steeråœ¨å¹³è¡¡çœŸå®æ€§ã€åè§å’Œæ¯’æ€§ç­‰å±æ€§ä»¥åŠåŒæ—¶æé«˜æœ‰ç”¨æ€§ã€æ­£ç¡®æ€§å’Œè¿è´¯æ€§ç­‰å±æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¹²é¢„æ–¹æ³•ä¸»è¦å…³æ³¨å•ä¸€å±æ€§çš„å¹²é¢„ã€‚</li>
<li>å¤šå±æ€§ç›®æ ‡å¼•å¯¼ï¼ˆMAT-Steerï¼‰æ¡†æ¶è¢«æå‡ºï¼Œç”¨äºå¤„ç†å¤šå±æ€§è®¾ç½®ä¸­çš„å†²çªé—®é¢˜ã€‚</li>
<li>MAT-Steerä½¿ç”¨å¯¹é½ç›®æ ‡æ¥å­¦ä¹ å¼•å¯¼å‘é‡ï¼Œå°†ä¸è‰¯è¾“å‡ºå‘è‰¯å¥½è¾“å‡ºè½¬ç§»ã€‚</li>
<li>MAT-Steeré€šè¿‡é€‰æ‹©æ€§ä»¤ç‰Œçº§åˆ«çš„å¹²é¢„æ¥è§£å†³å¤šä¸ªå±æ€§é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å¼ºåˆ¶æ‰§è¡Œç¨€ç–æ€§å’Œæ­£äº¤æ€§æ¥å‡å°‘ä¸åŒå±æ€§ä¹‹é—´çš„å†²çªã€‚</li>
<li>åœ¨é—®ç­”å’Œç”Ÿæˆä»»åŠ¡ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒMAT-Steeråœ¨å¹³è¡¡å¤šç§å±æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec88b4c5c3a26c13e37924db6c501a39.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47c2fc10386a6a0602e02b346e03a7bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dd1af4a59b38301352c1cfff4a5a04b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="NoLiMa-Long-Context-Evaluation-Beyond-Literal-Matching"><a href="#NoLiMa-Long-Context-Evaluation-Beyond-Literal-Matching" class="headerlink" title="NoLiMa: Long-Context Evaluation Beyond Literal Matching"></a>NoLiMa: Long-Context Evaluation Beyond Literal Matching</h2><p><strong>Authors:Ali Modarressi, Hanieh Deilamsalehy, Franck Dernoncourt, Trung Bui, Ryan A. Rossi, Seunghyun Yoon, Hinrich SchÃ¼tze</strong></p>
<p>Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a â€œneedleâ€ (relevant information) from a â€œhaystackâ€ (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. To address this, we introduce NoLiMa, a benchmark extending NIAH with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 13 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts (&lt;1K), performance degrades significantly as context length increases. At 32K, for instance, 11 models drop below 50% of their strong short-length baselines. Even GPT-4o, one of the top-performing exceptions, experiences a reduction from an almost-perfect baseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information. Even models enhanced with reasoning capabilities or CoT prompting struggle to maintain performance in long contexts. We publicly release the dataset and evaluation code at <a target="_blank" rel="noopener" href="https://github.com/adobe-research/NoLiMa">https://github.com/adobe-research/NoLiMa</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ”¯æŒä»128Kåˆ°1Mä»¤ç‰Œçš„é•¿ä¸Šä¸‹æ–‡ã€‚è¯„ä¼°è¿™äº›èƒ½åŠ›çš„ä¸€ç§æµè¡Œæ–¹æ³•æ˜¯â€œå¯»æ‰¾å¤§å¤´é’ˆâ€ï¼ˆneedle-in-a-haystackï¼ŒNIAHï¼‰æµ‹è¯•ï¼Œè¯¥æµ‹è¯•æ¶‰åŠä»ä¸€å †æ— å…³çš„æ–‡æœ¬ä¸­æ‰¾å‡ºç›¸å…³çš„ç‰¹å®šä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•çš„æ‰©å±•åŒ…æ‹¬å¢åŠ å¹²æ‰°å› ç´ ã€äº‹å®é“¾æ¡å’Œåœ¨æƒ…å¢ƒä¸­è¿›è¡Œæ¨ç†ã€‚ç„¶è€Œï¼Œåœ¨è¿™äº›åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¨¡å‹å¯èƒ½ä¼šåˆ©ç”¨å¤§å¤´é’ˆå’Œä¸€å †æ–‡æœ¬ä¹‹é—´çš„ç°æœ‰æ–‡å­—åŒ¹é…æ¥ç®€åŒ–ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†NoLiMaåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•åœ¨NIAHçš„åŸºç¡€ä¸Šè¿›è¡Œäº†æ‰©å±•ï¼Œæä¾›äº†ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„å¤§å¤´é’ˆé›†ï¼Œå…¶ä¸­çš„é—®é¢˜å’Œç­”æ¡ˆä¹‹é—´å…·æœ‰æœ€å°çš„è¯æ±‡é‡å ï¼Œè¦æ±‚æ¨¡å‹æ¨æ–­æ½œåœ¨å…³è”ä»¥åœ¨å¤§é‡æ–‡æœ¬ä¸­æ‰¾åˆ°ç­”æ¡ˆã€‚æˆ‘ä»¬è¯„ä¼°äº†å£°ç§°æ”¯æŒè‡³å°‘åŒ…å«128Kä»¤ç‰Œä¸Šä¸‹æ–‡çš„13ä¸ªæµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚å®ƒä»¬åœ¨çŸ­ä¸Šä¸‹æ–‡ä¸­çš„è¡¨ç°è‰¯å¥½ï¼ˆ&lt;1Kï¼‰ï¼Œä½†éšç€ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ ï¼Œæ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚ä¾‹å¦‚ï¼Œåœ¨32Kçš„æƒ…å†µä¸‹ï¼Œæœ‰11ä¸ªæ¨¡å‹çš„æ€§èƒ½ä¸‹é™åˆ°å…¶å¼ºå¤§çš„çŸ­åŸºçº¿ä»¥ä¸‹çš„ä¸€åŠã€‚å³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„ä¾‹å¤–ä¹‹ä¸€GPT-4oï¼Œä¹Ÿä»è¿‘ä¹å®Œç¾çš„åŸºçº¿ï¼ˆ99.3%ï¼‰ä¸‹é™åˆ°åŸºçº¿ä»¥ä¸‹çš„æ­£å¸¸æ°´å¹³ï¼ˆä»å­¦ä¹ åˆ°ä¸€äº›åå¸¸ç‰¹æ€§çš„æé†’è½¬å˜å‡å°‘ï¼‰ï¼Œé¢å¯¹æ¨¡å‹çš„å‡†ç¡®åº¦è¾ƒå·®ä¹Ÿåªèƒ½ä»¤å…¶æœ›å°˜è«åŠä¹Ÿè¿œä½äºå¸¸ç†ååº”çš„å¿«é€Ÿç¡®è®¤å›å½’æŒ‡æ•°é—®é¢˜éšä¹‹æˆä¸ºç°å®ä¸­è¿›ä¸€æ­¥åŠªåŠ›çš„å®è·µè·¯ã€‚æˆ‘ä»¬é€šè¿‡é“¾æ¥ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/adobe-research/NoLiMa%EF%BC%89%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E8%AF%84%E4%BC%B0%E4%BB%A3%E7%A0%81%E4%BB%A5%E9%9D%A2%E5%AF%B9%E8%BE%83%E5%85%88%E8%BF%9B%E7%9A%84%E8%BE%93%E5%85%A5%E8%AF%84%E6%B5%8B%E6%AD%A4%E7%B1%BB%E6%96%B9%E9%9D%A2%E7%A7%91%E7%A0%94%E5%85%AC%E5%85%B3%E7%9A%84%E4%BF%A1%E5%BF%83%E5%B9%B6%E4%BF%9D%E6%8C%81%E5%85%AC%E4%BF%A1%E8%AF%9A%E5%85%8D%E8%81%8C%E8%B4%A3%E6%96%B9%E7%9A%84%E4%BA%BA%E5%91%98%E4%BF%9D%E6%8C%81%E7%9D%80%E9%A2%84%E6%B5%8B%E8%87%AA%E5%BE%8B%E5%92%8C%E5%B9%B3%E9%9D%A2%E5%BC%A0%E8%BF%9B%E5%BA%A6%E5%85%AC%E5%B8%83%E7%9A%84%E4%BD%93%E9%AA%8C%E5%8A%9B%E4%BA%89%E8%B7%9F%E5%AE%9A%E7%9A%84%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E9%80%9F%E5%BA%A6%E8%80%8C%E4%B8%94%E4%BF%9D%E8%AF%81%E5%87%86%E7%A1%AE%E6%80%A7%E5%B9%B6%E6%8E%A5%E5%8F%97%E5%85%AC%E4%BC%97%E7%9B%91%E7%9D%A3%E3%80%82%E6%88%91%E4%BB%AC%E7%9A%84%E5%88%86%E6%9E%90%E8%A1%A8%E6%98%8E%E8%BF%99%E4%BA%9B%E4%B8%8B%E9%99%8D%E6%BA%90%E4%BA%8E%E5%9C%A8%E8%BE%83%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E4%B8%AD%E5%BD%93%E5%AD%97%E9%9D%A2%E5%8C%B9%E9%85%8D%E7%BC%BA%E5%A4%B1%E6%97%B6%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%89%80%E9%9D%A2%E4%B8%B4%E7%9A%84%E9%9A%BE%E5%BA%A6%E5%A2%9E%E5%8A%A0%EF%BC%8C%E8%BF%99%E4%BD%BF%E5%BE%97%E6%A3%80%E7%B4%A2%E7%9B%B8%E5%85%B3%E4%BF%A1%E6%81%AF%E5%8F%98%E5%BE%97%E6%9B%B4%E5%8A%A0%E5%9B%B0%E9%9A%BE%E3%80%82%E5%8D%B3%E4%BD%BF%E5%9C%A8%E6%A8%A1%E5%9E%8B%E4%B8%AD%E5%A2%9E%E5%8A%A0%E4%BA%86%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B%E6%88%96%E4%BD%BF%E7%94%A8%E5%A4%8D%E6%9D%82%E6%8E%A8%E7%90%86%E6%8F%90%E7%A4%BA%E4%B9%9F%E6%97%A0%E6%B3%95%E5%9C%A8%E9%95%BF%E6%9C%9F%E4%B8%8A%E4%B8%8B%E6%96%87%E4%B8%AD%E4%BF%9D%E6%8C%81%E6%80%A7%E8%83%BD%E7%A8%B3%E5%AE%9A%E3%80%82">https://github.com/adobe-research/NoLiMaï¼‰å…¬å¼€å‘å¸ƒæ•°æ®é›†å’Œè¯„ä¼°ä»£ç ä»¥é¢å¯¹è¾ƒå…ˆè¿›çš„è¾“å…¥è¯„æµ‹æ­¤ç±»æ–¹é¢ç§‘ç ”å…¬å…³çš„ä¿¡å¿ƒå¹¶ä¿æŒå…¬ä¿¡è¯šå…èŒè´£æ–¹çš„äººå‘˜ä¿æŒç€é¢„æµ‹è‡ªå¾‹å’Œå¹³é¢å¼ è¿›åº¦å…¬å¸ƒçš„ä½“éªŒåŠ›äº‰è·Ÿå®šçš„ç§‘å­¦æŠ€æœ¯é€Ÿåº¦è€Œä¸”ä¿è¯å‡†ç¡®æ€§å¹¶æ¥å—å…¬ä¼—ç›‘ç£ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜è¿™äº›ä¸‹é™æºäºåœ¨è¾ƒé•¿ä¸Šä¸‹æ–‡ä¸­å½“å­—é¢åŒ¹é…ç¼ºå¤±æ—¶æ³¨æ„åŠ›æœºåˆ¶æ‰€é¢ä¸´çš„éš¾åº¦å¢åŠ ï¼Œè¿™ä½¿å¾—æ£€ç´¢ç›¸å…³ä¿¡æ¯å˜å¾—æ›´åŠ å›°éš¾ã€‚å³ä½¿åœ¨æ¨¡å‹ä¸­å¢åŠ äº†æ¨ç†èƒ½åŠ›æˆ–ä½¿ç”¨å¤æ‚æ¨ç†æç¤ºä¹Ÿæ— æ³•åœ¨é•¿æœŸä¸Šä¸‹æ–‡ä¸­ä¿æŒæ€§èƒ½ç¨³å®šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05167v3">PDF</a> Accepted at ICML 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ”¯æŒä»128Kåˆ°1Mçš„é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡ã€‚è¯„ä¼°è¿™äº›èƒ½åŠ›çš„ä¸€ç§æµè¡Œæ–¹æ³•æ˜¯â€œä¼—é‡Œå¯»é’ˆâ€ï¼ˆneedle-in-a-haystackï¼‰æµ‹è¯•ï¼Œè¯¥æµ‹è¯•æ—¨åœ¨ä»å¤§é‡çš„ä¸ç›¸å…³æ–‡æœ¬ï¼ˆå¦‚æµ·é‡çš„è‰å †ä¸­æ‰¾åˆ°â€œé’ˆâ€ï¼Œå³ç›¸å…³çš„ä¿¡æ¯ï¼‰ã€‚ä½†åœ¨æ­¤åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¨¡å‹ä¼šåˆ©ç”¨ç°æœ‰å­—é¢åŒ¹é…ç®€åŒ–ä»»åŠ¡ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†NoLiMaåŸºå‡†æµ‹è¯•ï¼Œå®ƒæ˜¯NIAHçš„æ‰©å±•ç‰ˆï¼Œå…¶ä¸­åŒ…å«ç²¾å¿ƒè®¾è®¡çš„â€œé’ˆâ€é›†åˆï¼Œé—®é¢˜ä¸â€œé’ˆâ€ä¹‹é—´çš„è¯æ±‡é‡å æœ€å°‘ï¼Œè¦æ±‚æ¨¡å‹å‘ç°æ½œåœ¨çš„å…³è”ä»¥åœ¨å¤§é‡çš„è‰å †ä¸­æ‰¾åˆ°é‚£æ ¹â€œé’ˆâ€ã€‚æˆ‘ä»¬è¯„ä¼°äº†å£°ç§°èƒ½æ”¯æŒè‡³å°‘128Kä»¤ç‰Œé•¿åº¦çš„13ä¸ªæµè¡ŒLLMã€‚å®ƒä»¬åœ¨çŸ­æ–‡æœ¬ä¸­çš„è¡¨ç°è‰¯å¥½ï¼Œä½†éšç€ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ ï¼Œæ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚ä¾‹å¦‚ï¼Œåœ¨32Kæ—¶ï¼Œæœ‰11ä¸ªæ¨¡å‹çš„æ€§èƒ½ä¸‹é™åˆ°å…¶å¼ºå¤§çš„çŸ­åŸºçº¿çš„ä¸€åŠä»¥ä¸‹ã€‚å³ä½¿æ˜¯è¡¨ç°æœ€ä½³çš„GPT-4oæ¨¡å‹ï¼Œä¹Ÿä»è¿‘ä¹å®Œç¾çš„åŸºçº¿ï¼ˆ99.3%ï¼‰é™è‡³69.7%ã€‚åˆ†æè¡¨æ˜ï¼Œè¿™äº›ä¸‹é™æºäºåœ¨æ²¡æœ‰å­—é¢åŒ¹é…çš„æƒ…å†µä¸‹ï¼Œé•¿ä¸Šä¸‹æ–‡ä¸­æ³¨æ„åŠ›æœºåˆ¶æ‰€é¢ä¸´çš„éš¾åº¦å¢åŠ ï¼Œä½¿å¾—éš¾ä»¥æ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚å³ä½¿ä½¿ç”¨æ¨ç†èƒ½åŠ›å¢å¼ºæˆ–å¸¦æœ‰CoTæç¤ºçš„æ¨¡å‹ä¹Ÿå¾ˆéš¾ç»´æŒé•¿æœŸæ€§èƒ½ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/adobe-research/NoLiMa%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E8%AF%84%E4%BC%B0%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/adobe-research/NoLiMaå…¬å¼€å‘å¸ƒæ•°æ®é›†å’Œè¯„ä¼°ä»£ç ã€‚</a></p>
<p><strong>è¦ç‚¹å½’çº³</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ”¯æŒä»é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡ä¸­è·å–ä¿¡æ¯ã€‚</li>
<li>â€œä¼—é‡Œå¯»é’ˆâ€ï¼ˆNIAHï¼‰æµ‹è¯•æ˜¯è¯„ä¼°LLMå¤„ç†é•¿æ–‡æœ¬èƒ½åŠ›çš„æ–¹æ³•ä¹‹ä¸€ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•å­˜åœ¨æ¨¡å‹åˆ©ç”¨å­—é¢åŒ¹é…çš„å±€é™æ€§ã€‚</li>
<li>NoLiMaåŸºå‡†æµ‹è¯•æ‰©å±•äº†NIAHæµ‹è¯•ï¼Œè¦æ±‚æ¨¡å‹å‘ç°æ½œåœ¨å…³è”è€Œéä¾èµ–å­—é¢åŒ¹é…ã€‚</li>
<li>åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶ï¼Œå¤§å¤šæ•°LLMæ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>æ€§èƒ½ä¸‹é™æºäºæ³¨æ„åŠ›æœºåˆ¶åœ¨é•¿æ–‡æœ¬ä¸­é¢ä¸´éš¾åº¦å¢åŠ çš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05167">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ec99428b4f191a22c3f5fd6b60c74c84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-779cc797cdb3109c611b80bd7993c44e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecdb0b8510cfccacc76066b2698e2077.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0dbc4eb1c8aa02c61cc337457ecbc19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13b6b08067cf8686f21b8840eedf0de9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c9a604be10033903dfb075421348e63.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="LCFO-Long-Context-and-Long-Form-Output-Dataset-and-Benchmarking"><a href="#LCFO-Long-Context-and-Long-Form-Output-Dataset-and-Benchmarking" class="headerlink" title="LCFO: Long Context and Long Form Output Dataset and Benchmarking"></a>LCFO: Long Context and Long Form Output Dataset and Benchmarking</h2><p><strong>Authors:Marta R. Costa-jussÃ , Pierre Andrews, Mariano Coria Meglioli, Joy Chen, Joe Chuang, David Dale, Christophe Ropers, Alexandre Mourachko, Eduardo SÃ¡nchez, Holger Schwenk, Tuan Tran, Arina Turkatenko, Carleigh Wood</strong></p>
<p>This paper presents the Long Context and Form Output (LCFO) benchmark, a novel evaluation framework for assessing gradual summarization and summary expansion capabilities across diverse domains. LCFO consists of long input documents (5k words average length), each of which comes with three summaries of different lengths (20%, 10%, and 5% of the input text), as well as approximately 15 questions and answers (QA) related to the input content. Notably, LCFO also provides alignments between specific QA pairs and corresponding summaries in 7 domains. The primary motivation behind providing summaries of different lengths is to establish a controllable framework for generating long texts from shorter inputs, i.e. summary expansion. To establish an evaluation metric framework for summarization and summary expansion, we provide human evaluation scores for human-generated outputs, as well as results from various state-of-the-art large language models (LLMs). GPT-4o-mini achieves best human scores among automatic systems in both summarization and summary expansion tasks (~ +10% and +20%, respectively). It even surpasses human output quality in the case of short summaries (~ +7%). Overall automatic metrics achieve low correlations with human evaluation scores (~ 0.4) but moderate correlation on specific evaluation aspects such as fluency and attribution (~ 0.6). </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†é•¿è¯­å¢ƒä¸å½¢å¼è¾“å‡ºï¼ˆLCFOï¼‰åŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°ä¸åŒé¢†åŸŸçš„æ¸è¿›æ‘˜è¦å’Œæ‘˜è¦æ‰©å±•èƒ½åŠ›ã€‚LCFOåŒ…å«é•¿è¾“å…¥æ–‡æ¡£ï¼ˆå¹³å‡é•¿åº¦5000å­—ï¼‰ï¼Œæ¯ä¸ªæ–‡æ¡£éƒ½å¸¦æœ‰ä¸‰ä¸ªä¸åŒé•¿åº¦çš„æ‘˜è¦ï¼ˆåˆ†åˆ«ä¸ºè¾“å…¥æ–‡æœ¬çš„20%ã€10%å’Œ5%ï¼‰ï¼Œä»¥åŠå¤§çº¦ä¸è¾“å…¥å†…å®¹ç›¸å…³çš„15ä¸ªé—®é¢˜å’Œç­”æ¡ˆï¼ˆQAï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒLCFOè¿˜æä¾›äº†7ä¸ªé¢†åŸŸä¸­ç‰¹å®šQAå¯¹å’Œç›¸åº”æ‘˜è¦ä¹‹é—´çš„å¯¹é½ã€‚æä¾›ä¸åŒé•¿åº¦çš„æ‘˜è¦çš„ä¸»è¦åŠ¨æœºæ˜¯å»ºç«‹ä¸€ä¸ªå¯æ§çš„æ¡†æ¶ï¼Œç”¨äºä»è¾ƒçŸ­çš„è¾“å…¥ç”Ÿæˆè¾ƒé•¿çš„æ–‡æœ¬ï¼Œå³æ‘˜è¦æ‰©å±•ã€‚ä¸ºäº†å»ºç«‹æ‘˜è¦å’Œæ‘˜è¦æ‰©å±•çš„è¯„ä»·æŒ‡æ ‡æ¡†æ¶ï¼Œæˆ‘ä»¬æä¾›äº†äººç±»ç”Ÿæˆè¾“å‡ºçš„äººç±»è¯„ä»·åˆ†æ•°ï¼Œä»¥åŠæ¥è‡ªå„ç§æœ€æ–°å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç»“æœã€‚GPT-4o-miniåœ¨æ‘˜è¦å’Œæ‘˜è¦æ‰©å±•ä»»åŠ¡ä¸­å‡è·å¾—æœ€ä½³äººç±»è¯„åˆ†ï¼ˆåˆ†åˆ«æé«˜äº†çº¦10%å’Œ20%ï¼‰ï¼Œç”šè‡³åœ¨çŸ­æ‘˜è¦çš„æƒ…å†µä¸‹è¶…è¿‡äº†äººç±»è¾“å‡ºçš„è´¨é‡ï¼ˆæé«˜äº†çº¦7%ï¼‰ã€‚æ€»ä½“è€Œè¨€ï¼Œè‡ªåŠ¨åº¦é‡ä¸äººç±»è¯„ä»·åˆ†æ•°çš„ç›¸å…³æ€§è¾ƒä½ï¼ˆçº¦0.4ï¼‰ï¼Œä½†åœ¨ç‰¹å®šçš„è¯„ä»·æ–¹é¢ï¼ˆå¦‚æµç•…åº¦å’Œå±æ€§ï¼‰è¾¾åˆ°ä¸­ç­‰ç›¸å…³æ€§ï¼ˆçº¦0.6ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08268v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºäº†Long Context and Form Outputï¼ˆLCFOï¼‰åŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°è·¨ä¸åŒé¢†åŸŸçš„æ¸è¿›æ‘˜è¦å’Œæ‘˜è¦æ‰©å±•èƒ½åŠ›ã€‚LCFOåŒ…å«å¹³å‡é•¿åº¦5000å­—çš„é•¿æœŸè¾“å…¥æ–‡æ¡£ï¼Œæ¯ä¸ªæ–‡æ¡£éƒ½åŒ…å«ä¸‰ä¸ªä¸åŒé•¿åº¦çš„æ‘˜è¦ï¼ˆåˆ†åˆ«å è¾“å…¥æ–‡æœ¬çš„20%ã€10%å’Œ5%ï¼‰ï¼Œä»¥åŠå¤§çº¦ä¸è¾“å…¥å†…å®¹ç›¸å…³çš„15ä¸ªé—®é¢˜å’Œç­”æ¡ˆã€‚LCFOè¿˜æä¾›7ä¸ªé¢†åŸŸä¸­çš„ç‰¹å®šé—®ç­”å¯¹ä¸ç›¸åº”æ‘˜è¦ä¹‹é—´çš„å¯¹é½ã€‚è¯¥åŸºå‡†æµ‹è¯•çš„ä¸»è¦åŠ¨æœºæ˜¯å»ºç«‹ä»è¾ƒçŸ­çš„è¾“å…¥ç”Ÿæˆé•¿æ–‡æœ¬çš„å¯æ§æ¡†æ¶ï¼Œå³æ‘˜è¦æ‰©å±•ã€‚ä¸ºäº†å»ºç«‹æ‘˜è¦å’Œæ‘˜è¦æ‰©å±•çš„è¯„ä»·æŒ‡æ ‡æ¡†æ¶ï¼Œæˆ‘ä»¬æä¾›äº†å¯¹äººå·¥ç”Ÿæˆè¾“å‡ºçš„äººç±»è¯„ä»·åˆ†æ•°ï¼Œä»¥åŠæ¥è‡ªå„ç§æœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç»“æœã€‚GPT-4o-miniåœ¨æ‘˜è¦å’Œæ‘˜è¦æ‰©å±•ä»»åŠ¡ä¸­çš„è‡ªåŠ¨ç³»ç»Ÿä¸­è¡¨ç°æœ€ä½³ï¼Œåˆ†åˆ«æé«˜äº†çº¦10%å’Œ20%ï¼Œç”šè‡³åœ¨çŸ­æ‘˜è¦çš„æƒ…å†µä¸‹è¶…è¿‡äº†äººç±»è¾“å‡ºè´¨é‡ã€‚æ€»ä½“è€Œè¨€ï¼Œè‡ªåŠ¨æŒ‡æ ‡ä¸äººç±»è¯„ä»·åˆ†æ•°çš„ç›¸å…³æ€§è¾ƒä½ï¼ˆçº¦0.4ï¼‰ï¼Œä½†åœ¨ç‰¹å®šè¯„ä»·æ–¹é¢å¦‚æµç•…åº¦å’Œå½’å±åº¦æ–¹é¢çš„ç›¸å…³æ€§ä¸ºä¸­ç­‰ï¼ˆçº¦0.6ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LCFOåŸºå‡†æµ‹è¯•åŒ…å«é•¿æ–‡æ¡£å’Œå¤šç§é•¿åº¦çš„æ‘˜è¦ï¼Œä»¥åŠç›¸å…³çš„é—®ç­”å¯¹ï¼Œç”¨äºè¯„ä¼°æ‘˜è¦å’Œæ‘˜è¦æ‰©å±•çš„èƒ½åŠ›ã€‚</li>
<li>æ‘˜è¦ä¸åŒé•¿åº¦çš„æä¾›æ˜¯ä¸ºäº†å»ºç«‹ä»çŸ­è¾“å…¥ç”Ÿæˆé•¿æ–‡æœ¬çš„å¯æ§æ¡†æ¶ã€‚</li>
<li>GPT-4o-miniåœ¨æ‘˜è¦å’Œæ‘˜è¦æ‰©å±•ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ï¼Œè¶…è¿‡äº†ä¸€äº›è‡ªåŠ¨ç³»ç»Ÿå’Œå…¶ä»–LLMã€‚</li>
<li>åœ¨çŸ­æ‘˜è¦çš„æƒ…å†µä¸‹ï¼ŒGPT-4o-miniç”šè‡³è¶…è¿‡äº†äººç±»è¾“å‡ºè´¨é‡ã€‚</li>
<li>è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ä¸äººç±»è¯„ä»·åˆ†æ•°çš„æ€»ä½“ç›¸å…³æ€§è¾ƒä½ï¼Œä½†åœ¨ç‰¹å®šæ–¹é¢å¦‚æµç•…åº¦å’Œå½’å±åº¦çš„ç›¸å…³æ€§ä¸ºä¸­ç­‰ã€‚</li>
<li>LCFOåŸºå‡†æµ‹è¯•æä¾›äº†äººç±»è¯„ä»·åˆ†æ•°ï¼Œè¿™æœ‰åŠ©äºæ›´å‡†ç¡®åœ°è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.08268">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-97150f485f7f2c8a324ff62033fe2ccf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab576032c3f35d17cb299204187e9a9d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="A-Runtime-Adaptive-Transformer-Neural-Network-Accelerator-on-FPGAs"><a href="#A-Runtime-Adaptive-Transformer-Neural-Network-Accelerator-on-FPGAs" class="headerlink" title="A Runtime-Adaptive Transformer Neural Network Accelerator on FPGAs"></a>A Runtime-Adaptive Transformer Neural Network Accelerator on FPGAs</h2><p><strong>Authors:Ehsan Kabir, Austin R. J. Downey, Jason D. Bakos, David Andrews, Miaoqing Huang</strong></p>
<p>Transformer neural networks (TNN) excel in natural language processing (NLP), machine translation, and computer vision (CV) without relying on recurrent or convolutional layers. However, they have high computational and memory demands, particularly on resource-constrained devices like FPGAs. Moreover, transformer models vary in processing time across applications, requiring custom models with specific parameters. Designing custom accelerators for each model is complex and time-intensive. Some custom accelerators exist with no runtime adaptability, and they often rely on sparse matrices to reduce latency. However, hardware designs become more challenging due to the need for application-specific sparsity patterns. This paper introduces ADAPTOR, a runtime-adaptive accelerator for dense matrix computations in transformer encoders and decoders on FPGAs. ADAPTOR enhances the utilization of processing elements and on-chip memory, enhancing parallelism and reducing latency. It incorporates efficient matrix tiling to distribute resources across FPGA platforms and is fully quantized for computational efficiency and portability. Evaluations on Xilinx Alveo U55C data center cards and embedded platforms like VC707 and ZCU102 show that our design is 1.2$\times$ and 2.87$\times$ more power efficient than the NVIDIA K80 GPU and the i7-8700K CPU respectively. Additionally, it achieves a speedup of 1.7 to 2.25$\times$ compared to some state-of-the-art FPGA-based accelerators. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºFPGAä¸Šå˜å‹å™¨ç¼–ç å™¨è§£ç å™¨ä¸­å¯†é›†çŸ©é˜µè¿ç®—çš„è¿è¡Œæ—¶è‡ªé€‚åº”åŠ é€Ÿå™¨ADAPTORã€‚å˜å‹å™¨ç¥ç»ç½‘ç»œï¼ˆTNNï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€æœºå™¨ç¿»è¯‘å’Œè®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ— éœ€ä¾èµ–å¾ªç¯æˆ–å·ç§¯å±‚ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹è®¡ç®—å’Œå†…å­˜çš„éœ€æ±‚å¾ˆé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨FPGAç­‰èµ„æºå—é™çš„è®¾å¤‡ä¸Šã€‚æ­¤å¤–ï¼Œå˜å‹å™¨æ¨¡å‹åœ¨ä¸åŒåº”ç”¨ä¸­çš„å¤„ç†æ—¶é—´å„ä¸ç›¸åŒï¼Œéœ€è¦å…·æœ‰ç‰¹å®šå‚æ•°çš„å®šåˆ¶æ¨¡å‹ã€‚è®¾è®¡æ¯ä¸ªæ¨¡å‹çš„å®šåˆ¶åŠ é€Ÿå™¨æ˜¯å¤æ‚ä¸”è€—æ—¶çš„ã€‚è™½ç„¶å­˜åœ¨ä¸€äº›æ²¡æœ‰è¿è¡Œæ—¶é€‚åº”æ€§çš„å®šåˆ¶åŠ é€Ÿå™¨ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºç¨€ç–çŸ©é˜µæ¥å‡å°‘å»¶è¿Ÿã€‚ç„¶è€Œï¼Œç”±äºéœ€è¦åº”ç”¨ç‰¹å®šçš„ç¨€ç–æ¨¡å¼ï¼Œç¡¬ä»¶è®¾è®¡å˜å¾—æ›´åŠ å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å°½ç®¡æœ‰äº›æŒ‘æˆ˜æ‘†åœ¨é¢å‰ï¼Œä½†è¯¥è®ºæ–‡æ¨å‡ºçš„ADAPTORèƒ½æœ‰æ•ˆæå‡å¤„ç†å•å…ƒå’Œç‰‡ä¸Šå†…å­˜çš„åˆ©ç”¨ç‡ï¼Œå¼ºåŒ–å¹¶è¡Œå¤„ç†å¹¶å‡å°‘å»¶è¿Ÿã€‚å®ƒé‡‡ç”¨é«˜æ•ˆçš„çŸ©é˜µåˆ‡ç‰‡æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨FPGAå¹³å°ä¸Šåˆ†é…èµ„æºï¼Œå¹¶ä¸ºäº†è®¡ç®—æ•ˆç‡å’Œä¾¿æºæ€§è¿›è¡Œå…¨é¢é‡åŒ–ã€‚åœ¨Xilinx Alveo U55Cæ•°æ®ä¸­å¿ƒå¡å’ŒVC707åŠZCU102ç­‰åµŒå…¥å¼å¹³å°ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„è®¾è®¡ç›¸è¾ƒäºNVIDIA K80 GPUå’Œi7-8700K CPUåˆ†åˆ«æé«˜äº†1.2å€å’Œ2.87å€çš„èƒ½æ•ˆã€‚æ­¤å¤–ï¼Œç›¸è¾ƒäºä¸€äº›å…ˆè¿›çš„FPGAåŠ é€Ÿå™¨ï¼Œå®ƒå®ç°äº†1.7è‡³2.25å€çš„é€Ÿåº¦æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18148v3">PDF</a> arXiv admin note: text overlap with arXiv:2409.14023</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºADAPTORçš„ã€èƒ½åœ¨FPGAä¸Šè¿è¡Œçš„ã€å…·æœ‰è‡ªé€‚åº”æ€§çš„åŠ é€Ÿå™¨ï¼Œç”¨äºæå‡Transformerç¥ç»ç½‘ç»œä¸­çš„å¯†é›†çŸ©é˜µè¿ç®—æ•ˆç‡ã€‚è¯¥åŠ é€Ÿå™¨é€šè¿‡ä¼˜åŒ–èµ„æºåˆ†é…å’Œæé«˜å¹¶è¡Œæ€§ï¼Œå¢å¼ºäº†å¤„ç†å•å…ƒå’Œç‰‡ä¸Šå†…å­˜çš„åˆ©ç”¨ç‡ï¼Œä»è€Œé™ä½å»¶è¿Ÿã€‚å®ƒåœ¨ä¸åŒFPGAå¹³å°é—´é‡‡ç”¨çŸ©é˜µåˆ†å—æŠ€æœ¯ä»¥æé«˜æ•ˆç‡å¹¶ä¿éšœå¯ç§»æ¤æ€§ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œç›¸æ¯”äºNVIDIA K80 GPUå’Œi7-8700K CPUï¼ŒADAPTORè®¾è®¡æ›´åŠ èŠ‚èƒ½é«˜æ•ˆï¼ŒåŒæ—¶ä¸ä¸€äº›å…ˆè¿›çš„FPGAåŠ é€Ÿå™¨ç›¸æ¯”ï¼Œå…¶é€Ÿåº¦æå‡äº†1.7è‡³2.25å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ADAPTORæ˜¯ä¸€ç§é’ˆå¯¹FPGAçš„è‡ªé€‚åº”åŠ é€Ÿå™¨ï¼Œç”¨äºæå‡Transformerç¥ç»ç½‘ç»œä¸­çš„å¯†é›†çŸ©é˜µè¿ç®—æ•ˆç‡ã€‚</li>
<li>è¯¥åŠ é€Ÿå™¨é€šè¿‡ä¼˜åŒ–èµ„æºåˆ†é…å’Œæé«˜å¹¶è¡Œæ€§ï¼Œå¢å¼ºäº†å¤„ç†å•å…ƒå’Œç‰‡ä¸Šå†…å­˜çš„åˆ©ç”¨ç‡ã€‚</li>
<li>ADAPTORé€šè¿‡çŸ©é˜µåˆ†å—æŠ€æœ¯å®ç°èµ„æºåˆ†é…çš„ä¼˜åŒ–ï¼Œä»¥é€‚åº”ä¸åŒçš„FPGAå¹³å°ã€‚</li>
<li>è¯¥è®¾è®¡å…·æœ‰é«˜åº¦çš„è®¡ç®—æ•ˆç‡å’Œå¯ç§»æ¤æ€§ã€‚</li>
<li>ä¸NVIDIA K80 GPUå’Œi7-8700K CPUç›¸æ¯”ï¼ŒADAPTORè®¾è®¡æ›´åŠ èŠ‚èƒ½é«˜æ•ˆã€‚</li>
<li>ä¸ä¸€äº›å…ˆè¿›çš„FPGAåŠ é€Ÿå™¨ç›¸æ¯”ï¼ŒADAPTORå®ç°äº†1.7è‡³2.25å€çš„é€Ÿåº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18148">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f2807f2cd50d72bcf7cfc294e4ba4ad9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8da3239a17c51291e70d73dc11754b2f.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="LATST-Are-Transformers-Necessarily-Complex-for-Time-Series-Forecasting"><a href="#LATST-Are-Transformers-Necessarily-Complex-for-Time-Series-Forecasting" class="headerlink" title="LATST: Are Transformers Necessarily Complex for Time-Series Forecasting"></a>LATST: Are Transformers Necessarily Complex for Time-Series Forecasting</h2><p><strong>Authors:Dizhen Liang</strong></p>
<p>Transformer-based architectures have achieved remarkable success in natural language processing and computer vision. However, their performance in multivariate long-term forecasting often falls short compared to simpler linear baselines. Previous research has identified the traditional attention mechanism as a key factor limiting their effectiveness in this domain. To bridge this gap, we introduce LATST, a novel approach designed to mitigate entropy collapse and training instability common challenges in Transformer-based time series forecasting. We rigorously evaluate LATST across multiple real-world multivariate time series datasets, demonstrating its ability to outperform existing state-of-the-art Transformer models. Notably, LATST manages to achieve competitive performance with fewer parameters than some linear models on certain datasets, highlighting its efficiency and effectiveness. </p>
<blockquote>
<p>åŸºäºTransformerçš„æ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤šå…ƒé•¿æœŸé¢„æµ‹æ–¹é¢çš„æ€§èƒ½å¾€å¾€ä¸å¦‚ç®€å•çš„çº¿æ€§åŸºçº¿ã€‚ä¹‹å‰çš„ç ”ç©¶å·²ç»ç¡®å®šä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯é™åˆ¶å…¶åœ¨è¯¥é¢†åŸŸæ•ˆåŠ›çš„å…³é”®å› ç´ ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†LATSTï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨ç¼“è§£åŸºäºTransformerçš„æ—¶é—´åºåˆ—é¢„æµ‹ä¸­å¸¸è§çš„ç†µå´©æºƒå’Œè®­ç»ƒä¸ç¨³å®šé—®é¢˜çš„æ–°å‹æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªç°å®ä¸–ç•Œå¤šå…ƒæ—¶é—´åºåˆ—æ•°æ®é›†ä¸Šå¯¹LATSTè¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ï¼Œè¯æ˜äº†å…¶è¶…è¶Šç°æœ‰æœ€å…ˆè¿›çš„Transformeræ¨¡å‹çš„èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒLATSTåœ¨æŸäº›æ•°æ®é›†ä¸Šç”¨æ›´å°‘çš„å‚æ•°å®ç°äº†ä¸æŸäº›çº¿æ€§æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œè¿™å‡¸æ˜¾äº†å…¶é«˜æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.23749v9">PDF</a> 8 pages with referencing, 1 figure, 5 tables</p>
<p><strong>Summary</strong>ï¼šTransformeræ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨å¤šå…ƒé•¿æœŸé¢„æµ‹æ–¹é¢è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç¼ºé™·ï¼Œæå‡ºäº†åä¸ºLATSTçš„æ–°æ–¹æ³•ï¼Œè§£å†³äº†Transformeræ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„ç†µå´©æºƒå’Œè®­ç»ƒä¸ç¨³å®šé—®é¢˜ã€‚åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œçš„å¤šå…ƒæ—¶é—´åºåˆ—æ•°æ®é›†ä¸Šï¼ŒLATSTè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä¸ä¸€äº›çº¿æ€§æ¨¡å‹ç›¸æ¯”å‚æ•°æ›´å°‘ï¼Œæ€§èƒ½æ›´ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Transformeræ¶æ„åœ¨NLPå’ŒCVé¢†åŸŸè¡¨ç°ä¼˜ç§€ï¼Œä½†åœ¨å¤šå…ƒé•¿æœŸé¢„æµ‹æ–¹é¢å­˜åœ¨æ€§èƒ½çŸ­æ¿ã€‚</li>
<li>ä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯é™åˆ¶Transformeråœ¨å¤šå…ƒæ—¶é—´åºåˆ—é¢„æµ‹ä¸­æ•ˆåŠ›çš„å…³é”®å› ç´ ã€‚</li>
<li>LATSTæ–¹æ³•æ—¨åœ¨è§£å†³Transformeré¢ä¸´çš„ç†µå´©æºƒå’Œè®­ç»ƒä¸ç¨³å®šé—®é¢˜ã€‚</li>
<li>LATSTåœ¨å¤šä¸ªçœŸå®ä¸–ç•Œçš„å¤šå…ƒæ—¶é—´åºåˆ—æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>LATSTåœ¨æŸäº›æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¸çº¿æ€§æ¨¡å‹ç›¸å½“ï¼Œä½†å‚æ•°æ›´å°‘ï¼Œä½“ç°äº†å…¶é«˜æ•ˆæ€§ã€‚</li>
<li>LATSTçš„å¼•å…¥ä¸ºTransformeræ¨¡å‹åœ¨å¤šå…ƒæ—¶é—´åºåˆ—é¢„æµ‹æ–¹é¢çš„æ”¹è¿›æä¾›äº†æ–°æ€è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.23749">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f4d7dbb5c8e071a1b7de89eb1379a6e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9f7d61f7795c390e07fa39336d2baba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16d38108e49ad70e7c927891f2ea0154.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18b886021ba7a3070f77c4dfad24f3b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c48dac254322f14efdf7380f2d899723.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a29da162baf0cbc2330805b2d384472.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-306e27eb3ec203e88fac2ab2bb896ac6.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Planning-Anything-with-Rigor-General-Purpose-Zero-Shot-Planning-with-LLM-based-Formalized-Programming"><a href="#Planning-Anything-with-Rigor-General-Purpose-Zero-Shot-Planning-with-LLM-based-Formalized-Programming" class="headerlink" title="Planning Anything with Rigor: General-Purpose Zero-Shot Planning with   LLM-based Formalized Programming"></a>Planning Anything with Rigor: General-Purpose Zero-Shot Planning with   LLM-based Formalized Programming</h2><p><strong>Authors:Yilun Hao, Yang Zhang, Chuchu Fan</strong></p>
<p>While large language models (LLMs) have recently demonstrated strong potential in solving planning problems, there is a trade-off between flexibility and complexity. LLMs, as zero-shot planners themselves, are still not capable of directly generating valid plans for complex planning problems such as multi-constraint or long-horizon tasks. On the other hand, many frameworks aiming to solve complex planning problems often rely on task-specific preparatory efforts, such as task-specific in-context examples and pre-defined critics&#x2F;verifiers, which limits their cross-task generalization capability. In this paper, we tackle these challenges by observing that the core of many planning problems lies in optimization problems: searching for the optimal solution (best plan) with goals subject to constraints (preconditions and effects of decisions). With LLMsâ€™ commonsense, reasoning, and programming capabilities, this opens up the possibilities of a universal LLM-based approach to planning problems. Inspired by this observation, we propose LLMFP, a general-purpose framework that leverages LLMs to capture key information from planning problems and formally formulate and solve them as optimization problems from scratch, with no task-specific examples needed. We apply LLMFP to 9 planning problems, ranging from multi-constraint decision making to multi-step planning problems, and demonstrate that LLMFP achieves on average 83.7% and 86.8% optimal rate across 9 tasks for GPT-4o and Claude 3.5 Sonnet, significantly outperforming the best baseline (direct planning with OpenAI o1-preview) with 37.6% and 40.7% improvements. We also validate components of LLMFP with ablation experiments and analyzed the underlying success and failure reasons. Project page: <a target="_blank" rel="noopener" href="https://sites.google.com/view/llmfp">https://sites.google.com/view/llmfp</a>. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘åœ¨è§£å†³è§„åˆ’é—®é¢˜æ–¹é¢è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ½œåŠ›ï¼Œä½†åœ¨çµæ´»æ€§å’Œå¤æ‚æ€§ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚LLMä½œä¸ºé›¶å°„å‡»è§„åˆ’è€…æœ¬èº«ï¼Œä»ç„¶æ— æ³•ä¸ºå¤æ‚çš„è§„åˆ’é—®é¢˜å¦‚å¤šçº¦æŸæˆ–é•¿æœŸä»»åŠ¡ç›´æ¥ç”Ÿæˆæœ‰æ•ˆçš„è®¡åˆ’ã€‚å¦ä¸€æ–¹é¢ï¼Œè®¸å¤šæ—¨åœ¨è§£å†³å¤æ‚è§„åˆ’é—®é¢˜çš„æ¡†æ¶é€šå¸¸ä¾èµ–äºç‰¹å®šçš„ä»»åŠ¡å‡†å¤‡åŠªåŠ›ï¼Œå¦‚ç‰¹å®šä»»åŠ¡çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹å’Œé¢„å…ˆå®šä¹‰çš„æ‰¹è¯„è€…&#x2F;éªŒè¯å™¨ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p>åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è§‚å¯Ÿè®¸å¤šè§„åˆ’é—®é¢˜çš„æ ¸å¿ƒåœ¨äºä¼˜åŒ–é—®é¢˜æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼šåœ¨ç›®æ ‡å—çº¦æŸï¼ˆå†³ç­–çš„å…ˆå†³æ¡ä»¶å’Œå½±å“ï¼‰çš„æƒ…å†µä¸‹ï¼Œå¯»æ‰¾æœ€ä¼˜è§£å†³æ–¹æ¡ˆï¼ˆæœ€ä½³è®¡åˆ’ï¼‰ã€‚åˆ©ç”¨LLMçš„å¸¸è¯†ã€æ¨ç†å’Œç¼–ç¨‹èƒ½åŠ›ï¼Œè¿™ä¸ºåŸºäºLLMçš„é€šç”¨è§„åˆ’æ–¹æ³•æ‰“å¼€äº†å¯èƒ½æ€§ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12112v3">PDF</a> 57 pages, 25 figures, 15 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³è§„åˆ’é—®é¢˜ä¸Šå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨çµæ´»æ€§ä¸å¤æ‚æ€§ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚LLMä½œä¸ºé›¶æ ·æœ¬è§„åˆ’å™¨ï¼Œå°šæ— æ³•ç›´æ¥ä¸ºå¤æ‚è§„åˆ’é—®é¢˜ç”Ÿæˆæœ‰æ•ˆæ–¹æ¡ˆï¼Œå¦‚å¤šçº¦æŸæˆ–é•¿æœŸä»»åŠ¡ã€‚è€Œè®¸å¤šæ—¨åœ¨è§£å†³å¤æ‚è§„åˆ’é—®é¢˜çš„æ¡†æ¶ï¼Œéœ€è¦ä¾èµ–ç‰¹å®šä»»åŠ¡çš„å‡†å¤‡æ€§å·¥ä½œï¼Œå¦‚ç‰¹å®šä¸Šä¸‹æ–‡ç¤ºä¾‹å’Œé¢„å…ˆå®šä¹‰çš„æ‰¹è¯„è€…&#x2F;éªŒè¯å™¨ï¼Œè¿™é™åˆ¶äº†å…¶è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡è§‚å¯Ÿåˆ°è§„åˆ’é—®é¢˜çš„æ ¸å¿ƒåœ¨äºä¼˜åŒ–é—®é¢˜ï¼Œå³å¯»æ‰¾å—çº¦æŸçš„ç›®æ ‡ä¸‹çš„æœ€ä¼˜è§£å†³æ–¹æ¡ˆã€‚åˆ©ç”¨LLMçš„å¸¸è¯†ã€æ¨ç†å’Œç¼–ç¨‹èƒ½åŠ›ï¼Œä¸ºè§„åˆ’é—®é¢˜æå‡ºäº†ä¸€ç§åŸºäºLLMçš„é€šç”¨æ–¹æ³•ã€‚å—æ­¤å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†LLMFPæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨LLMä»è§„åˆ’é—®é¢˜ä¸­æ•è·å…³é”®ä¿¡æ¯ï¼Œå¹¶å°†å…¶æ­£å¼åˆ¶å®šä¸ºè§£å†³ä¼˜åŒ–é—®é¢˜ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡ç¤ºä¾‹ã€‚åœ¨å¤šç§è§„åˆ’é—®é¢˜ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLLMFPæ¡†æ¶åœ¨9é¡¹ä»»åŠ¡ä¸­å¹³å‡è¾¾åˆ°83.7%å’Œ86.8%çš„æœ€ä¼˜ç‡ï¼Œæ˜¾è‘—ä¼˜äºæœ€ä½³åŸºçº¿æ–¹æ³•ï¼Œåˆ†åˆ«æé«˜äº†37.6%å’Œ40.7%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨è§£å†³è§„åˆ’é—®é¢˜æ—¶é¢ä¸´çµæ´»æ€§ä¸å¤æ‚æ€§ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>LLMå°šæ— æ³•ç›´æ¥ç”Ÿæˆå¤æ‚è§„åˆ’é—®é¢˜çš„æœ‰æ•ˆæ–¹æ¡ˆã€‚</li>
<li>è®¸å¤šè§£å†³è§„åˆ’é—®é¢˜çš„æ¡†æ¶å› ä¾èµ–ç‰¹å®šä»»åŠ¡å‡†å¤‡è€Œé™åˆ¶äº†å…¶è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è§„åˆ’é—®é¢˜çš„æ ¸å¿ƒåœ¨äºå¯»æ‰¾å—çº¦æŸç›®æ ‡ä¸‹çš„æœ€ä¼˜è§£å†³æ–¹æ¡ˆã€‚</li>
<li>LLMFPæ¡†æ¶åˆ©ç”¨LLMçš„èƒ½åŠ›å°†è§„åˆ’é—®é¢˜åˆ¶å®šä¸ºè§£å†³ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>LLMFPæ¡†æ¶åœ¨å¤šç§è§„åˆ’é—®é¢˜ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æ•ˆæœï¼Œä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.12112">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-38686b5920b00e8e34caa48afe16ca56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4203b4f39d4e4fdeb98331ef7d3f8b87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cde2dc88f73f44617ade54956f161167.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="LASeR-Learning-to-Adaptively-Select-Reward-Models-with-Multi-Armed-Bandits"><a href="#LASeR-Learning-to-Adaptively-Select-Reward-Models-with-Multi-Armed-Bandits" class="headerlink" title="LASeR: Learning to Adaptively Select Reward Models with Multi-Armed   Bandits"></a>LASeR: Learning to Adaptively Select Reward Models with Multi-Armed   Bandits</h2><p><strong>Authors:Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal</strong></p>
<p>Reward Models (RMs) are crucial to aligning large language models (LLMs), but the degree to which an RM specialized to one task (e.g. writing) generalizes to new tasks (e.g. math) is often not known a priori, often making using only one fixed RM to train LLMs suboptimal. However, optimizing LLMs with multiple RMs simultaneously can incur a prohibitively high computational cost and lead to conflicting signals from different RMs that may degrade performance. To address these challenges, we introduce LASeR (Learning to Adaptively Select Rewards), which frames reward model selection as a multi-armed bandit problem, efficiently and iteratively training LLMs using multiple RMs by selecting the most well-suited RM for each instance. On commonsense and math reasoning tasks, we show that LASeR boosts iterative LLM training, improving the absolute average accuracy of Llama-3-8B over three datasets by 2.67% over an ensemble of RM scores while also showing superior efficiency (e.g., a 2x speedup). Moreover, on WildChat (open-ended instruction-following tasks), LASeR leads to a 72.69% AlpacaEval win rate over the RM score ensemble baseline. Extending to long-context generation, LASeR improves by 2.96 F1 points (avg.) on single-document QA tasks and 2.97 F1 points on few-shot learning over the RM score ensemble baseline with best-of-n sampling. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰å¯¹äºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡³å…³é‡è¦ï¼Œä½†å¯¹äºé’ˆå¯¹æŸä¸€ä»»åŠ¡ï¼ˆä¾‹å¦‚å†™ä½œï¼‰ä¸“ä¸šåŒ–çš„RMæ¨å¹¿åˆ°æ–°ä»»åŠ¡ï¼ˆä¾‹å¦‚æ•°å­¦ï¼‰çš„ç¨‹åº¦å¾€å¾€äº‹å…ˆæœªçŸ¥ï¼Œå› æ­¤é€šå¸¸ä½¿ç”¨å•ä¸€å›ºå®šRMæ¥è®­ç»ƒLLMå¹¶ä¸ç†æƒ³ã€‚ç„¶è€Œï¼ŒåŒæ—¶ä½¿ç”¨å¤šä¸ªRMä¼˜åŒ–LLMä¼šå¸¦æ¥è¿‡é«˜çš„è®¡ç®—æˆæœ¬ï¼Œå¹¶å¯èƒ½å¯¼è‡´æ¥è‡ªä¸åŒRMçš„ç›¸äº’å†²çªçš„ä¿¡å·ï¼Œä»è€Œå¯èƒ½é™ä½æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†LASeRï¼ˆè‡ªé€‚åº”é€‰æ‹©å¥–åŠ±å­¦ä¹ ï¼‰ï¼Œå®ƒå°†å¥–åŠ±æ¨¡å‹çš„é€‰æ‹©æ„é€ æˆä¸€ä¸ªå¤šè‡‚è€è™æœºé—®é¢˜ï¼Œé€šè¿‡ä¸ºæ¯ä¸ªå®ä¾‹é€‰æ‹©æœ€åˆé€‚çš„RMï¼Œæœ‰æ•ˆåœ°è¿­ä»£è®­ç»ƒLLMã€‚åœ¨å¸¸è¯†å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†LASeRå¯ä»¥æå‡è¿­ä»£LLMè®­ç»ƒçš„æ•ˆæœï¼Œåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šï¼ŒLlama-3-8Bçš„ç»å¯¹å¹³å‡å‡†ç¡®ç‡ç›¸å¯¹äºRMåˆ†æ•°é›†åˆæé«˜äº†2.67%ï¼ŒåŒæ—¶æ˜¾ç¤ºå‡ºæ›´é«˜çš„æ•ˆç‡ï¼ˆä¾‹å¦‚ï¼Œé€Ÿåº¦æé«˜ä¸€å€ï¼‰ã€‚æ­¤å¤–ï¼Œåœ¨WildChatï¼ˆå¼€æ”¾å¼æŒ‡ä»¤éµå¾ªä»»åŠ¡ï¼‰ä¸Šï¼ŒLASeRçš„èƒœç‡é«˜äºRMåˆ†æ•°é›†åˆåŸºçº¿ï¼Œè¾¾åˆ°72.69%ã€‚åœ¨é•¿æ–‡æœ¬ç”Ÿæˆæ–¹é¢ï¼ŒLASeRåœ¨å•æ–‡æ¡£é—®ç­”ä»»åŠ¡ä¸Šçš„F1åˆ†æ•°æé«˜äº†2.96ç‚¹ï¼ˆå¹³å‡å€¼ï¼‰ï¼Œåœ¨few-shotå­¦ä¹ ä¸Šçš„F1åˆ†æ•°æé«˜äº†2.97ç‚¹ï¼Œä¼˜äºRMåˆ†æ•°é›†åˆåŸºçº¿å¹¶é‡‡ç”¨äº†æœ€ä½³né‡‡æ ·ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.01735v2">PDF</a> 28 pages; First two authors contributed equally. Code:   <a target="_blank" rel="noopener" href="https://github.com/duykhuongnguyen/LASeR-MAB">https://github.com/duykhuongnguyen/LASeR-MAB</a></p>
<p><strong>Summary</strong></p>
<p>å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰å¯¹äºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡³å…³é‡è¦ï¼Œä½†é’ˆå¯¹ç‰¹å®šä»»åŠ¡ï¼ˆä¾‹å¦‚å†™ä½œï¼‰çš„RMåœ¨æ–°ä»»åŠ¡ï¼ˆä¾‹å¦‚æ•°å­¦ï¼‰ä¸Šçš„é€šç”¨æ€§å¾€å¾€æœªçŸ¥ï¼Œè¿™ä½¿å¾—ä»…ä½¿ç”¨ä¸€ä¸ªå›ºå®šRMæ¥è®­ç»ƒLLMé€šå¸¸ä¸æ˜¯æœ€ä¼˜çš„ã€‚ç„¶è€Œï¼ŒåŒæ—¶ä½¿ç”¨å¤šä¸ªRMä¼˜åŒ–LLMä¼šå¸¦æ¥è¿‡é«˜çš„è®¡ç®—æˆæœ¬ï¼Œå¹¶ä¸”å¯èƒ½ç”±äºä¸åŒRMä¹‹é—´çš„å†²çªä¿¡å·è€Œé™ä½æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†LASeRï¼ˆå­¦ä¹ è‡ªé€‚åº”é€‰æ‹©å¥–åŠ±ï¼‰ï¼Œå®ƒå°†å¥–åŠ±æ¨¡å‹çš„é€‰æ‹©è§†ä¸ºä¸€ä¸ªå¤šè‡‚è€è™æœºé—®é¢˜ï¼Œé€šè¿‡é€‰æ‹©æœ€é€‚åˆæ¯ä¸ªå®ä¾‹çš„RMï¼Œæœ‰æ•ˆåœ°è¿­ä»£è®­ç»ƒLLMã€‚åœ¨å¸¸è¯†å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šï¼ŒLASeRæå‡äº†è¿­ä»£LLMçš„è®­ç»ƒæ•ˆæœï¼Œæé«˜äº†Llama-3-8Bæ¨¡å‹åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡å‡†ç¡®ç‡2.67%ï¼ŒåŒæ—¶æ˜¾ç¤ºå‡ºæ›´é«˜çš„æ•ˆç‡ï¼ˆä¾‹å¦‚ï¼Œé€Ÿåº¦æå‡2å€ï¼‰ã€‚æ­¤å¤–ï¼Œåœ¨WildChatï¼ˆå¼€æ”¾å¼æŒ‡ä»¤éµå¾ªä»»åŠ¡ï¼‰ä¸Šï¼ŒLASeRçš„èƒœç‡è¾ƒRMåˆ†æ•°é›†åˆåŸºçº¿æé«˜äº†72.69%ã€‚åœ¨é•¿æœŸä¸Šä¸‹æ–‡ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒLASeRåœ¨å•æ–‡æ¡£é—®ç­”ä»»åŠ¡å’Œå°‘æ ·æœ¬å­¦ä¹ ä¸Šçš„F1åˆ†æ•°åˆ†åˆ«æé«˜äº†2.96å’Œ2.97ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹é½ä¸­èµ·å…³é”®ä½œç”¨ï¼Œä½†ç‰¹å®šä»»åŠ¡çš„RMåœ¨æ–°ä»»åŠ¡ä¸Šçš„é€šç”¨æ€§æœªçŸ¥ã€‚</li>
<li>ä»…ä½¿ç”¨å›ºå®šRMè®­ç»ƒLLMé€šå¸¸ä¸æ˜¯æœ€ä¼˜çš„ã€‚</li>
<li>åŒæ—¶ä½¿ç”¨å¤šä¸ªRMè¿›è¡Œä¼˜åŒ–å¯èƒ½å¯¼è‡´é«˜è®¡ç®—æˆæœ¬å’Œæ€§èƒ½é™ä½ï¼Œå› ä¸ºä¸åŒRMä¹‹é—´å¯èƒ½å­˜åœ¨å†²çªä¿¡å·ã€‚</li>
<li>LASeRé€šè¿‡é€‰æ‹©æœ€é€‚åˆæ¯ä¸ªå®ä¾‹çš„RMï¼Œæœ‰æ•ˆåœ°è¿­ä»£è®­ç»ƒLLMã€‚</li>
<li>LASeRåœ¨å¸¸è¯†å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæå‡äº†LLMçš„è®­ç»ƒæ•ˆæœã€‚</li>
<li>LASeRåœ¨WildChatä¸Šçš„èƒœç‡è¾ƒRMåˆ†æ•°é›†åˆåŸºçº¿æœ‰æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.01735">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0a2045acf048a52ae68b57727b76e6f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9700072ce3d0839561ab9c813feef840.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="FAMOUS-Flexible-Accelerator-for-the-Attention-Mechanism-of-Transformer-on-UltraScale-FPGAs"><a href="#FAMOUS-Flexible-Accelerator-for-the-Attention-Mechanism-of-Transformer-on-UltraScale-FPGAs" class="headerlink" title="FAMOUS: Flexible Accelerator for the Attention Mechanism of Transformer   on UltraScale+ FPGAs"></a>FAMOUS: Flexible Accelerator for the Attention Mechanism of Transformer   on UltraScale+ FPGAs</h2><p><strong>Authors:Ehsan Kabir, Md. Arafat Kabir, Austin R. J. Downey, Jason D. Bakos, David Andrews, Miaoqing Huang</strong></p>
<p>Transformer neural networks (TNNs) are being applied across a widening range of application domains, including natural language processing (NLP), machine translation, and computer vision (CV). Their popularity is largely attributed to the exceptional performance of their multi-head self-attention blocks when analyzing sequential data and extracting features. To date, there are limited hardware accelerators tailored for this mechanism, which is the first step before designing an accelerator for a complete model. This paper proposes \textit{FAMOUS}, a flexible hardware accelerator for dense multi-head attention (MHA) computation of TNNs on field-programmable gate arrays (FPGAs). It is optimized for high utilization of processing elements and on-chip memories to improve parallelism and reduce latency. An efficient tiling of large matrices has been employed to distribute memory and computing resources across different modules on various FPGA platforms. The design is evaluated on Xilinx Alveo U55C and U200 data center cards containing Ultrascale+ FPGAs. Experimental results are presented that show that it can attain a maximum throughput, number of parallel attention heads, embedding dimension and tile size of 328 (giga operations&#x2F;second (GOPS)), 8, 768 and 64 respectively on the U55C. Furthermore, it is 3.28$\times$ and 2.6$\times$ faster than the Intel Xeon Gold 5220R CPU and NVIDIA V100 GPU respectively. It is also 1.3$\times$ faster than the fastest state-of-the-art FPGA-based accelerator. </p>
<blockquote>
<p>Transformerç¥ç»ç½‘ç»œï¼ˆTNNsï¼‰æ­£è¢«å¹¿æ³›åº”ç”¨äºåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€æœºå™¨ç¿»è¯‘å’Œè®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰åœ¨å†…çš„å¤šä¸ªåº”ç”¨é¢†åŸŸã€‚å®ƒä»¬çš„å—æ¬¢è¿ç¨‹åº¦ä¸»è¦å½’åŠŸäºå…¶å¤šå¤´è‡ªæ³¨æ„åŠ›å—åœ¨åˆ†æåºåˆ—æ•°æ®å’Œæå–ç‰¹å¾æ—¶çš„å‡ºè‰²æ€§èƒ½ã€‚è¿„ä»Šä¸ºæ­¢ï¼Œé’ˆå¯¹è¿™ä¸€æœºåˆ¶çš„ç¡¬ä»¶åŠ é€Ÿå™¨è¿˜å¾ˆæœ‰é™ï¼Œè¿™æ˜¯è®¾è®¡å®Œæ•´çš„åŠ é€Ÿå™¨ä¹‹å‰çš„ç¬¬ä¸€æ­¥ã€‚æœ¬æ–‡æå‡ºäº†é¢å‘FPGAçš„Transformerç¥ç»ç½‘ç»œå¯†é›†å¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰è®¡ç®—çš„çµæ´»ç¡¬ä»¶åŠ é€Ÿå™¨FAMOUSã€‚å®ƒé’ˆå¯¹å¤„ç†å…ƒä»¶å’Œç‰‡ä¸Šå†…å­˜çš„é«˜åˆ©ç”¨ç‡è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥æé«˜å¹¶è¡Œæ€§å’Œé™ä½å»¶è¿Ÿã€‚é‡‡ç”¨å¤§å‹çŸ©é˜µçš„æœ‰æ•ˆåˆ†å—æŠ€æœ¯ï¼Œå°†å†…å­˜å’Œè®¡ç®—èµ„æºåˆ†é…ç»™ä¸åŒFPGAå¹³å°ä¸Šçš„ä¸åŒæ¨¡å—ã€‚è¯¥è®¾è®¡åœ¨Xilinx Alveo U55Cå’ŒU200æ•°æ®ä¸­å¿ƒå¡ï¼ˆåŒ…å«Ultrascale+ FPGAï¼‰ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨U55Cä¸Šï¼Œå…¶æœ€å¤§ååé‡ã€å¹¶è¡Œæ³¨æ„åŠ›å¤´æ•°ã€åµŒå…¥ç»´åº¦å’Œåˆ†å—å¤§å°åˆ†åˆ«ä¸º328ï¼ˆå‰æ“ä½œ&#x2F;ç§’ï¼ˆGOPSï¼‰ï¼‰ã€8ã€768å’Œ64ã€‚æ­¤å¤–ï¼Œå®ƒæ¯”Intel Xeon Gold 5220R CPUå’ŒNVIDIA V100 GPUåˆ†åˆ«å¿«3.28å€å’Œ2.6å€ã€‚å®ƒè¿˜æ¯”ç›®å‰æœ€å…ˆè¿›çš„FPGAåŠ é€Ÿå™¨å¿«1.3å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.14023v3">PDF</a> arXiv admin note: text overlap with arXiv:2409.13975</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹ç°åœºå¯ç¼–ç¨‹é—¨é˜µåˆ—ï¼ˆFPGAï¼‰ä¸ŠTransformerç¥ç»ç½‘ç»œï¼ˆTNNï¼‰å¯†é›†å¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰è®¡ç®—çš„çµæ´»ç¡¬ä»¶åŠ é€Ÿå™¨FAMOUSã€‚è¯¥è®¾è®¡ä¼˜åŒ–å¤„ç†å…ƒä»¶å’Œç‰‡ä¸Šå†…å­˜çš„åˆ©ç”¨ç‡ï¼Œä»¥æé«˜å¹¶è¡Œæ€§å¹¶å‡å°‘å»¶è¿Ÿã€‚é‡‡ç”¨å¤§å‹çŸ©é˜µçš„æœ‰æ•ˆåˆ†å—ï¼Œåœ¨ä¸åŒFPGAå¹³å°ä¸Šçš„å„ä¸ªæ¨¡å—é—´åˆ†é…å†…å­˜å’Œè®¡ç®—èµ„æºã€‚åœ¨Xilinx Alveo U55Cå’ŒU200æ•°æ®ä¸­å¿ƒå¡ä¸Šè¯„ä¼°è®¾è®¡ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œå…¶æœ€å¤§ååé‡ã€å¹¶è¡Œæ³¨æ„åŠ›å¤´æ•°ã€åµŒå…¥ç»´åº¦å’Œåˆ†å—å¤§å°åˆ†åˆ«ä¸º328GOPSã€8ã€768å’Œ64ã€‚ä¸Intel Xeon Gold 5220R CPUå’ŒNVIDIA V100 GPUç›¸æ¯”ï¼Œåˆ†åˆ«å¿«3.28å€å’Œ2.6å€ã€‚ä¸ç›®å‰æœ€å…ˆè¿›çš„FPGAåŠ é€Ÿå™¨ç›¸æ¯”ï¼Œå¿«1.3å€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹Transformerç¥ç»ç½‘ç»œï¼ˆTNNï¼‰çš„å¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰è®¡ç®—çš„ç¡¬ä»¶åŠ é€Ÿå™¨FAMOUSï¼Œç‰¹åˆ«é€‚ç”¨äºç°åœºå¯ç¼–ç¨‹é—¨é˜µåˆ—ï¼ˆFPGAï¼‰ã€‚</li>
<li>FAMOUSè®¾è®¡ä¼˜åŒ–äº†å¤„ç†å…ƒä»¶å’Œç‰‡ä¸Šå†…å­˜çš„åˆ©ç”¨ç‡ï¼Œä»¥æé«˜å¹¶è¡Œæ€§å’Œå‡å°‘å»¶è¿Ÿã€‚</li>
<li>é€šè¿‡çŸ©é˜µåˆ†å—ï¼Œæ›´æœ‰æ•ˆåœ°åˆ†é…å†…å­˜å’Œè®¡ç®—èµ„æºã€‚</li>
<li>åœ¨Xilinx Alveo U55Cå’ŒU200å¹³å°ä¸Šè¿›è¡Œçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒFAMOUSå…·æœ‰é«˜æ€§èƒ½è¡¨ç°ã€‚</li>
<li>ä¸å…¶ä»–å¤„ç†å™¨ç›¸æ¯”ï¼Œå¦‚Intel Xeon Gold 5220R CPUå’ŒNVIDIA V100 GPUï¼ŒFAMOUSè¡¨ç°å‡ºæ›´é«˜çš„é€Ÿåº¦ä¼˜åŠ¿ã€‚</li>
<li>FAMOUSçš„æ€§èƒ½ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„FPGAåŠ é€Ÿå™¨ã€‚</li>
<li>æ­¤æŠ€æœ¯ä¸ºé’ˆå¯¹TNNsçš„ç¡¬ä»¶åŠ é€Ÿå™¨çš„è¿›ä¸€æ­¥å‘å±•å¥ å®šäº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.14023">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-30d8afc6a76c772e2f76e474b5d6d70c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d847d9befb52ffdc2da0e0dee6a5d6c3.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-11/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-11/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-11/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a1f514967f88243f8dbff11f05d11160.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-11  4KAgent Agentic Any Image to 4K Super-Resolution
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-11/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7692149af91a8a4aaf1e5c40b713ca01.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-11  Go to Zero Towards Zero-shot Motion Generation with Million-scale Data
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32251.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
