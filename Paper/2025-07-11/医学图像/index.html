<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-11  4KAgent Agentic Any Image to 4K Super-Resolution">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-bba2fb1c9a2026a8f2a34fcb96fd0815.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-11-æ›´æ–°"><a href="#2025-07-11-æ›´æ–°" class="headerlink" title="2025-07-11 æ›´æ–°"></a>2025-07-11 æ›´æ–°</h1><h2 id="4KAgent-Agentic-Any-Image-to-4K-Super-Resolution"><a href="#4KAgent-Agentic-Any-Image-to-4K-Super-Resolution" class="headerlink" title="4KAgent: Agentic Any Image to 4K Super-Resolution"></a>4KAgent: Agentic Any Image to 4K Super-Resolution</h2><p><strong>Authors:Yushen Zuo, Qi Zheng, Mingyang Wu, Xinrui Jiang, Renjie Li, Jian Wang, Yide Zhang, Gengchen Mai, Lihong V. Wang, James Zou, Xiaoyu Wang, Ming-Hsuan Yang, Zhengzhong Tu</strong></p>
<p>We present 4KAgent, a unified agentic super-resolution generalist system designed to universally upscale any image to 4K resolution (and even higher, if applied iteratively). Our system can transform images from extremely low resolutions with severe degradations, for example, highly distorted inputs at 256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three core components: (1) Profiling, a module that customizes the 4KAgent pipeline based on bespoke use cases; (2) A Perception Agent, which leverages vision-language models alongside image quality assessment experts to analyze the input image and make a tailored restoration plan; and (3) A Restoration Agent, which executes the plan, following a recursive execution-reflection paradigm, guided by a quality-driven mixture-of-expert policy to select the optimal output for each step. Additionally, 4KAgent embeds a specialized face restoration pipeline, significantly enhancing facial details in portrait and selfie photos. We rigorously evaluate our 4KAgent across 11 distinct task categories encompassing a total of 26 diverse benchmarks, setting new state-of-the-art on a broad spectrum of imaging domains. Our evaluations cover natural images, portrait photos, AI-generated content, satellite imagery, fluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and X-ray, demonstrating superior performance in terms of both perceptual (e.g., NIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic paradigm for low-level vision tasks, we aim to catalyze broader interest and innovation within vision-centric autonomous agents across diverse research communities. We will release all the code, models, and results at: <a target="_blank" rel="noopener" href="https://4kagent.github.io/">https://4kagent.github.io</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†4KAgentï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ™ºèƒ½è¶…åˆ†è¾¨ç‡é€šç”¨ç³»ç»Ÿï¼Œæ—¨åœ¨å°†ä»»ä½•å›¾åƒé€šç”¨åœ°æå‡åˆ°4Kåˆ†è¾¨ç‡ï¼ˆå¦‚æœè¿­ä»£åº”ç”¨ï¼Œç”šè‡³å¯ä»¥æ›´é«˜ï¼‰ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿèƒ½å¤Ÿè½¬æ¢æä½åˆ†è¾¨ç‡ä¸”å…·æœ‰ä¸¥é‡é€€åŒ–çš„å›¾åƒï¼Œä¾‹å¦‚256x256çš„é«˜å¤±çœŸè¾“å…¥ï¼Œå°†å…¶è½¬åŒ–ä¸ºæ¸…æ™°é€¼çœŸçš„4Kè¾“å‡ºã€‚4KAgentåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šï¼ˆ1ï¼‰åˆ†ææ¨¡å—ï¼Œæ ¹æ®ç‰¹å®šçš„ç”¨ä¾‹å®šåˆ¶4KAgentç®¡é“ï¼›ï¼ˆ2ï¼‰æ„ŸçŸ¥ä»£ç†ï¼Œå®ƒåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä»¥åŠå›¾åƒè´¨é‡è¯„ä¼°ä¸“å®¶æ¥åˆ†æè¾“å…¥å›¾åƒå¹¶åˆ¶å®šç›¸åº”çš„æ¢å¤è®¡åˆ’ï¼›ï¼ˆ3ï¼‰æ¢å¤ä»£ç†ï¼Œæ ¹æ®é€’å½’çš„æ‰§è¡Œ-åæ€æ¨¡å¼æ‰§è¡Œè¯¥è®¡åˆ’ï¼Œå¹¶ç”±è´¨é‡é©±åŠ¨çš„ä¸“å®¶æ··åˆç­–ç•¥æŒ‡å¯¼ä»¥é€‰æ‹©æ¯ä¸ªæ­¥éª¤çš„æœ€ä½³è¾“å‡ºã€‚æ­¤å¤–ï¼Œ4KAgentåµŒå…¥äº†ä¸€ä¸ªä¸“é—¨çš„é¢éƒ¨æ¢å¤ç®¡é“ï¼Œå¯æ˜¾ç€æé«˜è‚–åƒå’Œè‡ªæ‹ä¸­çš„é¢éƒ¨ç»†èŠ‚ã€‚æˆ‘ä»¬å¯¹4KAgentè¿›è¡Œäº†ä¸¥æ ¼çš„è¯„ä¼°ï¼Œæ¶µç›–äº†åŒ…æ‹¬26ä¸ªä¸åŒåŸºå‡†æµ‹è¯•çš„11ä¸ªä¸åŒä»»åŠ¡ç±»åˆ«ï¼Œåœ¨å¹¿æ³›çš„æˆåƒé¢†åŸŸä¸Šåˆ›é€ äº†æœ€æ–°çš„æŠ€æœ¯ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¶µç›–äº†è‡ªç„¶å›¾åƒã€è‚–åƒç…§ç‰‡ã€AIç”Ÿæˆå†…å®¹ã€å«æ˜Ÿå›¾åƒã€è§å…‰æ˜¾å¾®é•œä»¥åŠè¯¸å¦‚çœ¼åº•æ£€æŸ¥ã€è¶…å£°æ³¢å’ŒXå°„çº¿ç­‰åŒ»å­¦å½±åƒï¼Œåœ¨æ„ŸçŸ¥ï¼ˆä¾‹å¦‚NIQEã€MUSIQï¼‰å’Œä¿çœŸåº¦ï¼ˆä¾‹å¦‚PSNRï¼‰æŒ‡æ ‡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚é€šè¿‡ä¸ºä½çº§è§†è§‰ä»»åŠ¡å»ºç«‹æ–°çš„æ™ºèƒ½èŒƒå¼ï¼Œæˆ‘ä»¬æ—¨åœ¨æ¿€å‘ä¸åŒç ”ç©¶ç¤¾åŒºä¸­å¯¹ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„è‡ªä¸»æ™ºèƒ½ä½“çš„æ›´å¹¿æ³›å…´è¶£å’Œåˆ›æ–°ã€‚æˆ‘ä»¬ä¼šåœ¨<a target="_blank" rel="noopener" href="https://4kagent.github.io/">https://4kagent.github.io</a>å‘å¸ƒæ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07105v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://4kagent.github.io/">https://4kagent.github.io</a></p>
<p><strong>Summary</strong></p>
<p>4KAgentæ˜¯ä¸€ç§ç»Ÿä¸€çš„å¤šæ™ºèƒ½ä½“è¶…åˆ†è¾¨ç‡é€šç”¨ç³»ç»Ÿï¼Œå¯å°†ä»»ä½•å›¾åƒæå‡åˆ°4Kåˆ†è¾¨ç‡ï¼Œç”šè‡³æ›´é«˜ã€‚ç³»ç»ŸåŒ…æ‹¬ä¸‰å¤§æ ¸å¿ƒç»„ä»¶ï¼šåˆ†ææ¨¡å—ã€æ„ŸçŸ¥æ™ºèƒ½ä½“ã€æ¢å¤æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿå¤„ç†æç«¯ä½åˆ†è¾¨ç‡ä¸”ä¸¥é‡é€€åŒ–çš„å›¾åƒï¼Œå¦‚é«˜åº¦æ‰­æ›²çš„è¾“å…¥å›¾åƒï¼ˆåˆ†è¾¨ç‡ä¸º256x256ï¼‰ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºæ¸…æ™°é€¼çœŸçš„4Kè¾“å‡ºå›¾åƒã€‚åŒæ—¶ï¼Œç³»ç»Ÿè¿˜åŒ…æ‹¬ä¸“é—¨çš„é¢éƒ¨æ¢å¤ç®¡é“ï¼Œèƒ½æ˜¾è‘—æ”¹å–„è‚–åƒå’Œè‡ªæ‹ä¸­çš„é¢éƒ¨ç»†èŠ‚ã€‚åœ¨æ¶µç›–å¹¿æ³›çš„å›¾åƒé¢†åŸŸè¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ï¼ŒåŒ…æ‹¬è‡ªç„¶å›¾åƒã€è‚–åƒç…§ç‰‡ã€äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ã€å«æ˜Ÿå›¾åƒç­‰ï¼Œåœ¨æ„ŸçŸ¥å’Œä¿çœŸåº¦æŒ‡æ ‡ä¸Šå‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æˆ‘ä»¬æ—¨åœ¨å»ºç«‹ä½æ°´å¹³è§†è§‰ä»»åŠ¡çš„æ–°å‹æ™ºèƒ½ä½“èŒƒå¼ï¼Œæ¿€å‘ä¸åŒç ”ç©¶ç¤¾åŒºå¯¹è§†è§‰ä¸ºä¸­å¿ƒçš„è‡ªä¸»æ™ºèƒ½ä½“çš„æ›´å¹¿æ³›å…´è¶£å’Œåˆ›æ–°ã€‚ç›¸å…³ä»£ç ã€æ¨¡å‹å’Œç»“æœå°†åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€ï¼š<a target="_blank" rel="noopener" href="https://4kagent.github.io/">https://4kagent.github.io</a>ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>4KAgentæ˜¯ä¸€ç§ç»Ÿä¸€çš„å¤šæ™ºèƒ½ä½“è¶…åˆ†è¾¨ç‡ç³»ç»Ÿï¼Œå¯å°†ä»»ä½•å›¾åƒæå‡åˆ°è‡³å°‘4Kåˆ†è¾¨ç‡ã€‚</li>
<li>ç³»ç»ŸåŒ…æ‹¬ä¸‰å¤§æ ¸å¿ƒç»„ä»¶ï¼šåˆ†ææ¨¡å—ã€æ„ŸçŸ¥æ™ºèƒ½ä½“å’Œæ¢å¤æ™ºèƒ½ä½“ã€‚</li>
<li>èƒ½å¤Ÿå¤„ç†æç«¯ä½åˆ†è¾¨ç‡å’Œä¸¥é‡é€€åŒ–çš„å›¾åƒï¼Œå¹¶è½¬åŒ–ä¸ºæ¸…æ™°é€¼çœŸçš„è¾“å‡ºã€‚</li>
<li>åŒ…å«äº†ä¸“é—¨ç”¨äºå¢å¼ºè‚–åƒå’Œè‡ªæ‹ä¸­çš„é¢éƒ¨ç»†èŠ‚çš„é¢éƒ¨æ¢å¤ç®¡é“ã€‚</li>
<li>åœ¨å¤šç§ä¸åŒé¢†åŸŸè¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬è‡ªç„¶å›¾åƒã€è‚–åƒç…§ç‰‡ç­‰ï¼Œè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>åœ¨æ„ŸçŸ¥å’Œä¿çœŸåº¦æŒ‡æ ‡ä¸Šéƒ½æœ‰å‡ºè‰²çš„è¡¨ç°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07105">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-762e759115502a266cd35b4af89f7af3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-63b6f9787e21b2998341b1bfd63aa89b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Deep-Brain-Net-An-Optimized-Deep-Learning-Model-for-Brain-tumor-Detection-in-MRI-Images-Using-EfficientNetB0-and-ResNet50-with-Transfer-Learning"><a href="#Deep-Brain-Net-An-Optimized-Deep-Learning-Model-for-Brain-tumor-Detection-in-MRI-Images-Using-EfficientNetB0-and-ResNet50-with-Transfer-Learning" class="headerlink" title="Deep Brain Net: An Optimized Deep Learning Model for Brain tumor   Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer   Learning"></a>Deep Brain Net: An Optimized Deep Learning Model for Brain tumor   Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer   Learning</h2><p><strong>Authors:Daniel Onah, Ravish Desai</strong></p>
<p>In recent years, deep learning has shown great promise in the automated detection and classification of brain tumors from MRI images. However, achieving high accuracy and computational efficiency remains a challenge. In this research, we propose Deep Brain Net, a novel deep learning system designed to optimize performance in the detection of brain tumors. The model integrates the strengths of two advanced neural network architectures which are EfficientNetB0 and ResNet50, combined with transfer learning to improve generalization and reduce training time. The EfficientNetB0 architecture enhances model efficiency by utilizing mobile inverted bottleneck blocks, which incorporate depth wise separable convolutions. This design significantly reduces the number of parameters and computational cost while preserving the ability of models to learn complex feature representations. The ResNet50 architecture, pre trained on large scale datasets like ImageNet, is fine tuned for brain tumor classification. Its use of residual connections allows for training deeper networks by mitigating the vanishing gradient problem and avoiding performance degradation. The integration of these components ensures that the proposed system is both computationally efficient and highly accurate. Extensive experiments performed on publicly available MRI datasets demonstrate that Deep Brain Net consistently outperforms existing state of the art methods in terms of classification accuracy, precision, recall, and computational efficiency. The result is an accuracy of 88 percent, a weighted F1 score of 88.75 percent, and a macro AUC ROC score of 98.17 percent which demonstrates the robustness and clinical potential of Deep Brain Net in assisting radiologists with brain tumor diagnosis. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ åœ¨é€šè¿‡MRIå›¾åƒè‡ªåŠ¨æ£€æµ‹å’Œåˆ†ç±»è„‘è‚¿ç˜¤æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ç°é«˜å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Deep Brain Netï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿï¼Œæ—¨åœ¨ä¼˜åŒ–è„‘è‚¿ç˜¤æ£€æµ‹çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹èåˆäº†EfficientNetB0å’ŒResNet50ä¸¤ç§å…ˆè¿›ç¥ç»ç½‘ç»œæ¶æ„çš„ä¼˜åŠ¿ï¼Œç»“åˆè¿ç§»å­¦ä¹ æ¥æé«˜é€šç”¨æ€§å’Œå‡å°‘è®­ç»ƒæ—¶é—´ã€‚EfficientNetB0æ¶æ„é€šè¿‡åˆ©ç”¨ç§»åŠ¨å€’ç½®ç“¶é¢ˆå—æé«˜äº†æ¨¡å‹æ•ˆç‡ï¼Œè¿™äº›å—ç»“åˆäº†æ·±åº¦å¯åˆ†ç¦»å·ç§¯ã€‚è¿™ç§è®¾è®¡æ˜¾è‘—å‡å°‘äº†å‚æ•°æ•°é‡å’Œè®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹å­¦ä¹ å¤æ‚ç‰¹å¾è¡¨ç¤ºçš„èƒ½åŠ›ã€‚ResNet50æ¶æ„åœ¨å¤§å‹æ•°æ®é›†ï¼ˆå¦‚ImageNetï¼‰ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç»è¿‡å¾®è°ƒç”¨äºè„‘è‚¿ç˜¤åˆ†ç±»ã€‚å…¶æ®‹å·®è¿æ¥çš„ä½¿ç”¨å…è®¸é€šè¿‡è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜å’Œé¿å…æ€§èƒ½ä¸‹é™æ¥è®­ç»ƒæ›´æ·±çš„ç½‘ç»œã€‚è¿™äº›ç»„ä»¶çš„é›†æˆç¡®ä¿æ‰€æå‡ºçš„ç³»ç»Ÿæ—¢è®¡ç®—é«˜æ•ˆåˆé«˜åº¦å‡†ç¡®ã€‚åœ¨å…¬å…±å¯ç”¨çš„MRIæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDeep Brain Netåœ¨åˆ†ç±»å‡†ç¡®æ€§ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å§‹ç»ˆä¼˜äºç°æœ‰æœ€æ–°æ–¹æ³•ã€‚å…¶å‡†ç¡®åº¦è¾¾åˆ°88%ï¼ŒåŠ æƒF1åˆ†æ•°ä¸º88.75%ï¼Œå®AUC ROCåˆ†æ•°ä¸º98.17%ï¼Œè¿™è¯æ˜äº†Deep Brain Netçš„ç¨³å¥æ€§å’Œåœ¨ä¸´åºŠè¾…åŠ©æ”¾å°„ç§‘åŒ»ç”Ÿè¯Šæ–­è„‘è‚¿ç˜¤çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07011v1">PDF</a> 9 pages, 14 figures, 4 tables. To be submitted to a conference</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ·±åº¦å­¦ä¹ åœ¨è‡ªåŠ¨æ£€æµ‹ä¸åˆ†ç±»è„‘è‚¿ç˜¤MRIå›¾åƒä¸­çš„æ½œåŠ›ã€‚ä¸ºå®ç°é«˜æ•ˆç‡ä¸é«˜ç²¾åº¦ï¼Œç ”ç©¶è€…æå‡ºäº†Deep Brain Netç³»ç»Ÿï¼Œå®ƒæ•´åˆäº†EfficientNetB0ä¸ResNet50ä¸¤ç§å…ˆè¿›çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œå¹¶ç»“åˆè¿ç§»å­¦ä¹ æ¥æå‡æ³›åŒ–èƒ½åŠ›å¹¶ç¼©çŸ­è®­ç»ƒæ—¶é—´ã€‚EfficientNetB0åˆ©ç”¨ç§»åŠ¨å€’ç½®ç“¶é¢ˆå—æ¥æå‡æ¨¡å‹æ•ˆç‡ï¼Œè€ŒResNet50åˆ™åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šé¢„è®­ç»ƒï¼Œå¾®è°ƒç”¨äºè„‘è‚¿ç˜¤åˆ†ç±»ã€‚å®éªŒè¯æ˜ï¼ŒDeep Brain Netåœ¨åˆ†ç±»ç²¾åº¦ã€ç²¾ç¡®ç‡ã€å¬å›ç‡åŠè®¡ç®—æ•ˆç‡ä¸Šå‡è¶…è¶Šç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œå‡†ç¡®ç‡è¾¾88%ï¼ŒåŠ æƒF1åˆ†æ•°ä¸º88.75%ï¼Œå®è§‚AUC ROCåˆ†æ•°ä¸º98.17%ï¼Œæ˜¾ç¤ºå…¶åœ¨è¾…åŠ©æ”¾å°„ç§‘åŒ»ç”Ÿè¯Šæ–­è„‘è‚¿ç˜¤æ–¹é¢çš„ç¨³å¥æ€§ä¸ä¸´åºŠæ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Deep learningåœ¨è„‘è‚¿ç˜¤MRIå›¾åƒè‡ªåŠ¨æ£€æµ‹ä¸åˆ†ç±»æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>Deep Brain Netç³»ç»Ÿç»“åˆäº†EfficientNetB0ä¸ResNet50ä¸¤ç§ç¥ç»ç½‘ç»œæ¶æ„ï¼Œæ—¨åœ¨æé«˜æ£€æµ‹è„‘è‚¿ç˜¤çš„æ€§èƒ½ã€‚</li>
<li>EfficientNetB0åˆ©ç”¨ç§»åŠ¨å€’ç½®ç“¶é¢ˆå—ä»¥æé«˜æ¨¡å‹æ•ˆç‡ï¼Œé™ä½å‚æ•°æ•°é‡å’Œè®¡ç®—æˆæœ¬ã€‚</li>
<li>ResNet50åœ¨å¤§å‹æ•°æ®é›†ä¸Šé¢„è®­ç»ƒï¼Œå¹¶å¾®è°ƒç”¨äºè„‘è‚¿ç˜¤åˆ†ç±»ï¼Œè§£å†³äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œé¿å…äº†æ€§èƒ½ä¸‹é™ã€‚</li>
<li>Deep Brain Netåœ¨åˆ†ç±»ç²¾åº¦ã€ç²¾ç¡®ç‡ã€å¬å›ç‡å’Œè®¡ç®—æ•ˆç‡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>è¯¥ç³»ç»Ÿè¾¾åˆ°äº†88%çš„å‡†ç¡®ç‡ã€88.75%çš„åŠ æƒF1åˆ†æ•°å’Œ98.17%çš„å®è§‚AUC ROCåˆ†æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07011">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ffe48d1a064017b5f258f8126fc9365a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-666fdb062ebd48c7811caefd754e1e27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edd913d1d438a200db79c0001969a71f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da8d3d799c2a697e889b29d39c4e8714.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bdaab236635c572d46b5142ec4a7d728.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d9e1153c9571f5ae837c0f76d66f441.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84a4fce528e3f059d6d68489a7cc8984.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d1fa742207d9290d48b6a05994ef8d9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Cross-Modality-Masked-Learning-for-Survival-Prediction-in-ICI-Treated-NSCLC-Patients"><a href="#Cross-Modality-Masked-Learning-for-Survival-Prediction-in-ICI-Treated-NSCLC-Patients" class="headerlink" title="Cross-Modality Masked Learning for Survival Prediction in ICI Treated   NSCLC Patients"></a>Cross-Modality Masked Learning for Survival Prediction in ICI Treated   NSCLC Patients</h2><p><strong>Authors:Qilong Xing, Zikai Song, Bingxin Gong, Lian Yang, Junqing Yu, Wei Yang</strong></p>
<p>Accurate prognosis of non-small cell lung cancer (NSCLC) patients undergoing immunotherapy is essential for personalized treatment planning, enabling informed patient decisions, and improving both treatment outcomes and quality of life. However, the lack of large, relevant datasets and effective multi-modal feature fusion strategies pose significant challenges in this domain. To address these challenges, we present a large-scale dataset and introduce a novel framework for multi-modal feature fusion aimed at enhancing the accuracy of survival prediction. The dataset comprises 3D CT images and corresponding clinical records from NSCLC patients treated with immune checkpoint inhibitors (ICI), along with progression-free survival (PFS) and overall survival (OS) data. We further propose a cross-modality masked learning approach for medical feature fusion, consisting of two distinct branches, each tailored to its respective modality: a Slice-Depth Transformer for extracting 3D features from CT images and a graph-based Transformer for learning node features and relationships among clinical variables in tabular data. The fusion process is guided by a masked modality learning strategy, wherein the model utilizes the intact modality to reconstruct missing components. This mechanism improves the integration of modality-specific features, fostering more effective inter-modality relationships and feature interactions. Our approach demonstrates superior performance in multi-modal integration for NSCLC survival prediction, surpassing existing methods and setting a new benchmark for prognostic models in this context. </p>
<blockquote>
<p>å¯¹éå°ç»†èƒè‚ºç™Œï¼ˆNSCLCï¼‰æ‚£è€…è¿›è¡Œå…ç–«æ²»ç–—æ—¶çš„å‡†ç¡®é¢„åè¯„ä¼°å¯¹ä¸ªæ€§åŒ–æ²»ç–—æ–¹æ¡ˆåˆ¶å®šã€æ‚£è€…å†³ç­–ä¿¡æ¯é€æ˜ä»¥åŠæ”¹å–„æ²»ç–—æ•ˆæœå’Œç”Ÿæ´»è´¨é‡å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç„¶è€Œï¼Œç¼ºä¹ç›¸å…³çš„å¤§å‹æ•°æ®é›†å’Œæœ‰æ•ˆçš„å¤šæ¨¡å¼ç‰¹å¾èåˆç­–ç•¥ï¼Œç»™è¿™ä¸€é¢†åŸŸå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶ä»‹ç»äº†ä¸€ä¸ªæ—¨åœ¨æé«˜ç”Ÿå­˜é¢„æµ‹å‡†ç¡®æ€§çš„å¤šæ¨¡å¼ç‰¹å¾èåˆæ–°æ¡†æ¶ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥å—å…ç–«æ£€æŸ¥ç‚¹æŠ‘åˆ¶å‰‚ï¼ˆICIï¼‰æ²»ç–—çš„NSCLCæ‚£è€…çš„3D CTå›¾åƒå’Œç›¸åº”çš„ä¸´åºŠè®°å½•ï¼Œä»¥åŠæ— è¿›å±•ç”Ÿå­˜ï¼ˆPFSï¼‰å’Œæ€»ç”Ÿå­˜ï¼ˆOSï¼‰æ•°æ®ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç”¨äºåŒ»å­¦ç‰¹å¾èåˆçš„è·¨æ¨¡æ€æ©è†œå­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªé’ˆå¯¹ä¸åŒæ¨¡æ€çš„åˆ†æ”¯ï¼šç”¨äºä»CTå›¾åƒä¸­æå–3Dç‰¹å¾çš„Slice-Depth Transformerï¼Œä»¥åŠç”¨äºå­¦ä¹ è¡¨æ ¼æ•°æ®ä¸­çš„èŠ‚ç‚¹ç‰¹å¾å’Œä¸´åºŠå˜é‡ä¹‹é—´å…³ç³»çš„åŸºäºå›¾çš„Transformerã€‚èåˆè¿‡ç¨‹ç”±æ©è†œæ¨¡æ€å­¦ä¹ ç­–ç•¥å¼•å¯¼ï¼Œè¯¥ç­–ç•¥ä½¿æ¨¡å‹åˆ©ç”¨å®Œæ•´æ¨¡æ€æ¥é‡å»ºç¼ºå¤±ç»„ä»¶ã€‚è¿™ç§æœºåˆ¶æ”¹å–„äº†æ¨¡æ€ç‰¹å®šç‰¹å¾çš„é›†æˆï¼Œä¿ƒè¿›äº†æ›´æœ‰æ•ˆçš„è·¨æ¨¡æ€å…³ç³»å’Œç‰¹å¾äº¤äº’ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨NSCLCç”Ÿå­˜é¢„æµ‹çš„å¤šæ¨¡æ€é›†æˆä¸­å±•ç¤ºäº†å“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œä¸ºè¿™ä¸€èƒŒæ™¯ä¸‹çš„é¢„åæ¨¡å‹è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06994v1">PDF</a> MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹éå°ç»†èƒè‚ºç™Œï¼ˆNSCLCï¼‰æ‚£è€…æ¥å—å…ç–«æ²»ç–—æ—¶çš„é¢„åå‡†ç¡®é¢„æµ‹çš„é‡è¦æ€§ï¼Œå¹¶æå‡ºä¸€ä¸ªå¤§å‹æ•°æ®é›†å’Œä¸€ç§æ–°å‹å¤šæ¨¡æ€ç‰¹å¾èåˆæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ç”Ÿå­˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥å—å…ç–«æ£€æŸ¥ç‚¹æŠ‘åˆ¶å‰‚ï¼ˆICIï¼‰æ²»ç–—çš„NSCLCæ‚£è€…çš„3D CTå›¾åƒå’Œç›¸å…³ä¸´åºŠè®°å½•ï¼Œä»¥åŠæ— è¿›å±•ç”Ÿå­˜ï¼ˆPFSï¼‰å’Œæ€»ç”Ÿå­˜ï¼ˆOSï¼‰æ•°æ®ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§è·¨æ¨¡æ€æ©è†œå­¦ä¹ æ³•è¿›è¡ŒåŒ»å­¦ç‰¹å¾èåˆï¼ŒåŒ…æ‹¬é’ˆå¯¹CTå›¾åƒå’Œè¡¨æ ¼æ•°æ®çš„ä¸¤ä¸ªå®šåˆ¶åˆ†æ”¯ã€‚èåˆè¿‡ç¨‹é€šè¿‡æ©è†œæ¨¡æ€å­¦ä¹ ç­–ç•¥å¼•å¯¼ï¼Œæé«˜äº†æ¨¡æ€ç‰¹å®šç‰¹å¾çš„é›†æˆã€‚è¯¥ç ”ç©¶åœ¨éå°ç»†èƒè‚ºç™Œç”Ÿå­˜é¢„æµ‹çš„å¤šæ¨¡æ€æ•´åˆä¸­è¡¨ç°ä¼˜è¶Šï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œä¸ºè¯¥é¢†åŸŸçš„é¢„åæ¨¡å‹è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éå°ç»†èƒè‚ºç™Œï¼ˆNSCLCï¼‰æ‚£è€…æ¥å—å…ç–«æ²»ç–—æ—¶çš„å‡†ç¡®é¢„åå¯¹ä¸ªæ€§åŒ–æ²»ç–—è®¡åˆ’ã€æ‚£è€…å†³ç­–ä»¥åŠæ²»ç–—ç»“æœå’Œç”Ÿæ´»è´¨é‡æ”¹å–„è‡³å…³é‡è¦ã€‚</li>
<li>ç¼ºä¹å¤§å‹ç›¸å…³æ•°æ®é›†å’Œæœ‰æ•ˆçš„å¤šæ¨¡æ€ç‰¹å¾èåˆç­–ç•¥æ˜¯è¯¥é¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶äººå‘˜æå‡ºä¸€ä¸ªåŒ…å«3D CTå›¾åƒå’Œä¸´åºŠè®°å½•çš„å¤§å‹æ•°æ®é›†ï¼Œä»¥åŠæ¥å—å…ç–«æ£€æŸ¥ç‚¹æŠ‘åˆ¶å‰‚æ²»ç–—çš„NSCLCæ‚£è€…çš„ç”Ÿå­˜æ•°æ®ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹å¤šæ¨¡æ€ç‰¹å¾èåˆæ¡†æ¶ï¼Œä»¥æé«˜ç”Ÿå­˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>æå‡ºäº†è·¨æ¨¡æ€æ©è†œå­¦ä¹ æ³•ï¼ŒåŒ…æ‹¬é’ˆå¯¹CTå›¾åƒå’Œè¡¨æ ¼æ•°æ®çš„ä¸¤ä¸ªå®šåˆ¶åˆ†æ”¯ï¼Œæ¯ä¸ªåˆ†æ”¯éƒ½é’ˆå¯¹å…¶ç›¸åº”çš„æ¨¡æ€è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>èåˆè¿‡ç¨‹é€šè¿‡æ©è†œæ¨¡æ€å­¦ä¹ ç­–ç•¥è¿›è¡Œå¼•å¯¼ï¼Œæ”¹å–„äº†ä¸åŒæ¨¡æ€ç‰¹å¾çš„é›†æˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06994">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-95caa707fddfdc6969c3d1bbd9dec67b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6a91c6b1ccc6d77ad6e8601895211e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42ad508ede0e6fb7a61e6d81ea77d246.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MCA-RG-Enhancing-LLMs-with-Medical-Concept-Alignment-for-Radiology-Report-Generation"><a href="#MCA-RG-Enhancing-LLMs-with-Medical-Concept-Alignment-for-Radiology-Report-Generation" class="headerlink" title="MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology   Report Generation"></a>MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology   Report Generation</h2><p><strong>Authors:Qilong Xing, Zikai Song, Youjia Zhang, Na Feng, Junqing Yu, Wei Yang</strong></p>
<p>Despite significant advancements in adapting Large Language Models (LLMs) for radiology report generation (RRG), clinical adoption remains challenging due to difficulties in accurately mapping pathological and anatomical features to their corresponding text descriptions. Additionally, semantic agnostic feature extraction further hampers the generation of accurate diagnostic reports. To address these challenges, we introduce Medical Concept Aligned Radiology Report Generation (MCA-RG), a knowledge-driven framework that explicitly aligns visual features with distinct medical concepts to enhance the report generation process. MCA-RG utilizes two curated concept banks: a pathology bank containing lesion-related knowledge, and an anatomy bank with anatomical descriptions. The visual features are aligned with these medical concepts and undergo tailored enhancement. We further propose an anatomy-based contrastive learning procedure to improve the generalization of anatomical features, coupled with a matching loss for pathological features to prioritize clinically relevant regions. Additionally, a feature gating mechanism is employed to filter out low-quality concept features. Finally, the visual features are corresponding to individual medical concepts, and are leveraged to guide the report generation process. Experiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate that MCA-RG achieves superior performance, highlighting its effectiveness in radiology report generation. </p>
<blockquote>
<p>å°½ç®¡åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€‚åº”äºæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç”±äºå°†ç—…ç†å’Œè§£å‰–ç‰¹å¾å‡†ç¡®æ˜ å°„åˆ°ç›¸åº”çš„æ–‡æœ¬æè¿°ä¸­çš„å›°éš¾ï¼Œå…¶åœ¨ä¸´åºŠä¸Šçš„åº”ç”¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œè¯­ä¹‰æ— å…³çš„ç‰¹å¾æå–è¿›ä¸€æ­¥é˜»ç¢äº†å‡†ç¡®è¯Šæ–­æŠ¥å‘Šçš„äº§ç”Ÿã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒ»å­¦æ¦‚å¿µå¯¹é½æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆMCA-RGï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªçŸ¥è¯†é©±åŠ¨æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ˜ç¡®å¯¹é½è§†è§‰ç‰¹å¾ä¸ç‹¬ç‰¹çš„åŒ»å­¦æ¦‚å¿µæ¥å¢å¼ºæŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ã€‚MCA-RGåˆ©ç”¨ä¸¤ä¸ªå®šåˆ¶çš„æ¦‚å¿µåº“ï¼šä¸€ä¸ªç—…ç†åº“ï¼ŒåŒ…å«ä¸ç—…ç¶ç›¸å…³çš„çŸ¥è¯†ï¼Œå’Œä¸€ä¸ªè§£å‰–åº“ï¼ŒåŒ…å«è§£å‰–æè¿°ã€‚è§†è§‰ç‰¹å¾ä¼šè¿™äº›åŒ»å­¦æ¦‚å¿µè¿›è¡Œå¯¹é½ï¼Œå¹¶è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å¢å¼ºã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§åŸºäºè§£å‰–å­¦çš„å¯¹æ¯”å­¦ä¹ ç¨‹åºï¼Œä»¥æé«˜è§£å‰–ç‰¹å¾çš„æ³›åŒ–èƒ½åŠ›ï¼Œç»“åˆç—…ç†ç‰¹å¾çš„åŒ¹é…æŸå¤±æ¥ä¼˜å…ˆå¤„ç†ä¸´åºŠç›¸å…³åŒºåŸŸã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†ä¸€ç§ç‰¹å¾é—¨æ§æœºåˆ¶æ¥è¿‡æ»¤æ‰ä½è´¨é‡çš„æ¦‚å¿µç‰¹å¾ã€‚æœ€åï¼Œå°†è§†è§‰ç‰¹å¾ä¸å•ä¸ªåŒ»å­¦æ¦‚å¿µç›¸å¯¹åº”ï¼Œå¹¶ç”¨äºæŒ‡å¯¼æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ã€‚åœ¨ä¸¤ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ï¼ˆMIMIC-CXRå’ŒCheXpert Plusï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMCA-RGå–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06992v1">PDF</a> MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰é¢†åŸŸå­˜åœ¨çš„é—®é¢˜ï¼Œæå‡ºä¸€ç§çŸ¥è¯†é©±åŠ¨æ¡†æ¶Medical Concept Aligned Radiology Report Generationï¼ˆMCA-RGï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡æ˜ç¡®å°†è§†è§‰ç‰¹å¾å¯¹åº”äºä¸åŒçš„åŒ»å­¦æ¦‚å¿µï¼Œè§£å†³åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€‚åº”æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ä¸­çš„æŒ‘æˆ˜ã€‚é€šè¿‡ä¸¤ä¸ªæ¦‚å¿µåº“â€”â€”ç—…ç†åº“å’Œè§£å‰–åº“ï¼ŒMCA-RGå¯¹è§†è§‰ç‰¹å¾è¿›è¡Œå¯¹é½å’Œå¢å¼ºã€‚åŒæ—¶é‡‡ç”¨åŸºäºè§£å‰–ç»“æ„çš„å¯¹æ¯”å­¦ä¹ ç¨‹åºï¼Œæé«˜è§£å‰–ç‰¹å¾çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶é€šè¿‡åŒ¹é…æŸå¤±å¯¹ç—…ç†ç‰¹å¾è¿›è¡Œä¼˜å…ˆå¤„ç†ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨ç‰¹å¾é—¨æ§æœºåˆ¶è¿‡æ»¤æ‰ä½è´¨é‡çš„æ¦‚å¿µç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMCA-RGåœ¨å…¬å…±æ•°æ®é›†ä¸Šå–å¾—äº†å“è¶Šæ€§èƒ½ï¼Œæœ‰æ•ˆæå‡äº†æ”¾å°„å­¦æŠ¥å‘Šçš„ç”Ÿæˆè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MCA-RGæ¡†æ¶è§£å†³äº†å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä¸­å‡†ç¡®æ˜ å°„ç—…ç†å’Œè§£å‰–ç‰¹å¾çš„é—®é¢˜ã€‚</li>
<li>æ¡†æ¶åŒ…å«ä¸¤ä¸ªæ¦‚å¿µåº“ï¼šç—…ç†åº“å’Œè§£å‰–åº“ï¼Œç”¨äºå¯¹é½å’Œå¢å¼ºè§†è§‰ç‰¹å¾ã€‚</li>
<li>é‡‡ç”¨åŸºäºè§£å‰–ç»“æ„çš„å¯¹æ¯”å­¦ä¹ ç¨‹åºï¼Œæé«˜è§£å‰–ç‰¹å¾çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åŒ¹é…æŸå¤±ç”¨äºä¼˜å…ˆå¤„ç†ä¸ä¸´åºŠç›¸å…³çš„ç—…ç†ç‰¹å¾ã€‚</li>
<li>ç‰¹å¾é—¨æ§æœºåˆ¶ç”¨äºè¿‡æ»¤ä½è´¨é‡çš„æ¦‚å¿µç‰¹å¾ã€‚</li>
<li>MCA-RGé€šè¿‡è§†è§‰ç‰¹å¾ä¸åŒ»å­¦æ¦‚å¿µçš„ç»“åˆï¼ŒæŒ‡å¯¼æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06992">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15b81fe7dad1533ac3403bd93cd5d4c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-872b9774df4bb8a8be8b77965a2f750f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dfa6df4a223444bbe292cc0df5aa4d3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CheXPO-Preference-Optimization-for-Chest-X-ray-VLMs-with-Counterfactual-Rationale"><a href="#CheXPO-Preference-Optimization-for-Chest-X-ray-VLMs-with-Counterfactual-Rationale" class="headerlink" title="CheXPO: Preference Optimization for Chest X-ray VLMs with Counterfactual   Rationale"></a>CheXPO: Preference Optimization for Chest X-ray VLMs with Counterfactual   Rationale</h2><p><strong>Authors:Xiao Liang, Jiawei Hu, Di Wang, Zhi Ma, Lin Zhao, Ronghan Li, Bo Wan, Quan Wang</strong></p>
<p>Vision-language models (VLMs) are prone to hallucinations that critically compromise reliability in medical applications. While preference optimization can mitigate these hallucinations through clinical feedback, its implementation faces challenges such as clinically irrelevant training samples, imbalanced data distributions, and prohibitive expert annotation costs. To address these challenges, we introduce CheXPO, a Chest X-ray Preference Optimization strategy that combines confidence-similarity joint mining with counterfactual rationale. Our approach begins by synthesizing a unified, fine-grained multi-task chest X-ray visual instruction dataset across different question types for supervised fine-tuning (SFT). We then identify hard examples through token-level confidence analysis of SFT failures and use similarity-based retrieval to expand hard examples for balancing preference sample distributions, while synthetic counterfactual rationales provide fine-grained clinical preferences, eliminating the need for additional expert input. Experiments show that CheXPO achieves 8.93% relative performance gain using only 5% of SFT samples, reaching state-of-the-art performance across diverse clinical tasks and providing a scalable, interpretable solution for real-world radiology applications. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å®¹æ˜“å‡ºç°å¹»è§†ç°è±¡ï¼Œè¿™åœ¨åŒ»ç–—åº”ç”¨ä¸­ä¼šä¸¥é‡æŸå®³å¯é æ€§ã€‚è™½ç„¶åå¥½ä¼˜åŒ–å¯ä»¥é€šè¿‡ä¸´åºŠåé¦ˆæ¥ç¼“è§£è¿™äº›å¹»è§†ç°è±¡ï¼Œä½†å…¶å®ç°é¢ä¸´ç€ä¸´åºŠä¸ç›¸å…³è®­ç»ƒæ ·æœ¬ã€æ•°æ®åˆ†å¸ƒä¸å¹³è¡¡ä»¥åŠä¸“å®¶æ³¨é‡Šæˆæœ¬é«˜æ˜‚ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†CheXPOï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆç½®ä¿¡åº¦-ç›¸ä¼¼æ€§è”åˆæŒ–æ˜å’Œåå‘äº‹å®ç†ç”±çš„èƒ¸éƒ¨Xå…‰ç‰‡åå¥½ä¼˜åŒ–ç­–ç•¥ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆé€šè¿‡åˆæˆä¸€ä¸ªç»Ÿä¸€ã€ç²¾ç»†ç²’åº¦çš„å¤šä»»åŠ¡èƒ¸éƒ¨Xå…‰ç‰‡è§†è§‰æŒ‡ä»¤æ•°æ®é›†ï¼Œæ¶µç›–ä¸åŒç±»å‹çš„é—®é¢˜ï¼Œç”¨äºæœ‰ç›‘ç£çš„å¾®è°ƒï¼ˆSFTï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡SFTå¤±è´¥çš„æ ‡è®°çº§ç½®ä¿¡åˆ†ææ¥ç¡®å®šå›°éš¾æ ·æœ¬ï¼Œå¹¶ä½¿ç”¨åŸºäºç›¸ä¼¼æ€§çš„æ£€ç´¢æ¥æ‰©å±•å›°éš¾æ ·æœ¬ï¼Œä»¥å¹³è¡¡åå¥½æ ·æœ¬åˆ†å¸ƒï¼ŒåŒæ—¶åˆæˆåå‘äº‹å®ç†ç”±æä¾›ç²¾ç»†ç²’åº¦çš„ä¸´åºŠåå¥½ï¼Œæ— éœ€é¢å¤–çš„ä¸“å®¶è¾“å…¥ã€‚å®éªŒè¡¨æ˜ï¼ŒCheXPOä»…ä½¿ç”¨5%çš„SFTæ ·æœ¬å°±å®ç°äº†8.93%çš„ç›¸å¯¹æ€§èƒ½æå‡ï¼Œåœ¨å¤šç§ä¸´åºŠä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸ºç°å®ä¸–ç•Œä¸­çš„æ”¾å°„å­¦åº”ç”¨æä¾›äº†å¯ä¼¸ç¼©ã€å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06959v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŒ»å­¦å½±åƒé¢†åŸŸä¸­çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»å­¦åº”ç”¨ä¸­çš„å¯é æ€§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºCheXPOçš„åå¥½ä¼˜åŒ–ç­–ç•¥ã€‚è¯¥ç­–ç•¥é€šè¿‡åˆæˆä¸€ä¸ªç»Ÿä¸€çš„å¤šä»»åŠ¡èƒ¸é€æŒ‡ä»¤æ•°æ®é›†æ¥å¼ºåŒ–è®­ç»ƒï¼Œå¹¶ä½¿ç”¨è”åˆä¿¡å¿ƒç›¸ä¼¼æ€§æŒ–æ˜ä»¥åŠåå‘å‡è®¾åŸåˆ™æ¥å¤„ç†é—®é¢˜æ•°æ®æ ·æœ¬ä¸å¹³è¡¡ä»¥åŠé«˜éš¾åº¦çš„ä»»åŠ¡ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ï¼Œä½¿å…¶æˆä¸ºé€‚åˆç”¨äºåŒ»å­¦å›¾åƒåº”ç”¨çš„ä¸€ç§å¯é ä¸”å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚ç›¸è¾ƒäºä¸“å®¶æ ‡æ³¨çš„æ–¹æ³•ï¼Œè¯¥ç­–ç•¥å¤§å¤§é™ä½äº†æˆæœ¬æŠ•å…¥ã€‚æœ€ç»ˆå®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨CheXPOç­–ç•¥çš„æ¨¡å‹ç›¸è¾ƒäºä½¿ç”¨å¸¸è§„æ–¹æ³•è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½æé«˜äº†çº¦8.93%ï¼Œä»…ä½¿ç”¨äº†å¤§çº¦5%çš„è®­ç»ƒæ ·æœ¬ä¾¿å®ç°äº†å“è¶Šçš„è¡¨ç°ã€‚è¿™ä¸ºçœŸå®ä¸–ç•Œä¸­çš„åŒ»å­¦å½±åƒè¯Šæ–­æä¾›äº†æ–°çš„æ€è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»å­¦åº”ç”¨ä¸­å­˜åœ¨å¯é æ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†åŒ»å­¦å½±åƒæ—¶æ˜“å‡ºç°å¹»è§‰ã€‚</li>
<li>CheXPOç­–ç•¥æ—¨åœ¨é€šè¿‡åˆæˆç»Ÿä¸€çš„å¤šä»»åŠ¡èƒ¸é€æŒ‡ä»¤æ•°æ®é›†è¿›è¡Œå¼ºåŒ–è®­ç»ƒä»¥æå‡æ¨¡å‹çš„å¯é æ€§ã€‚æ•°æ®é›†çš„è®¾è®¡å¯ä»¥åº”å¯¹ä¸åŒç±»å‹çš„åŒ»å­¦é—®é¢˜ã€‚</li>
<li>é€šè¿‡åŸºäºä¿¡å¿ƒçš„ç›¸ä¼¼æ€§åˆ†ææŒ–æ˜éš¾ä»¥åº”å¯¹çš„å®ä¾‹å¹¶å¯»æ‰¾ç¡¬å®ä¾‹çš„æ‰©å±•æ–¹æ³•ï¼Œä»¥å¹³è¡¡åå¥½æ ·æœ¬åˆ†å¸ƒã€‚</li>
<li>åˆæˆåå‘å‡è®¾ç†ç”±çš„æ–¹æ³•èƒ½å¤Ÿç²¾ç»†åœ°åæ˜ ä¸´åºŠåå¥½ï¼Œå‡å°‘äº†é¢å¤–çš„ä¸“å®¶è¾“å…¥éœ€æ±‚ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨CheXPOç­–ç•¥çš„æ¨¡å‹æ€§èƒ½æ˜¾è‘—æé«˜ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚ä»…ä½¿ç”¨å°‘é‡æ ·æœ¬å³å¯å®ç°å“è¶Šè¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-75bd1925950056ef5847364fc64c4ca1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-085b08f91eb464d62c8a326ec32db8cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-257cd4da80d34b9bd04934063129056d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6fa7fdfc2eae9e843b25efca128d95d5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2100d990cde06cec8fd72ccda2cf5447.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SimCortex-Collision-free-Simultaneous-Cortical-Surfaces-Reconstruction"><a href="#SimCortex-Collision-free-Simultaneous-Cortical-Surfaces-Reconstruction" class="headerlink" title="SimCortex: Collision-free Simultaneous Cortical Surfaces Reconstruction"></a>SimCortex: Collision-free Simultaneous Cortical Surfaces Reconstruction</h2><p><strong>Authors:Kaveh Moradkhani, R Jarrett Rushmore, Sylvain Bouix</strong></p>
<p>Accurate cortical surface reconstruction from magnetic resonance imaging (MRI) data is crucial for reliable neuroanatomical analyses. Current methods have to contend with complex cortical geometries, strict topological requirements, and often produce surfaces with overlaps, self-intersections, and topological defects. To overcome these shortcomings, we introduce SimCortex, a deep learning framework that simultaneously reconstructs all brain surfaces (left&#x2F;right white-matter and pial) from T1-weighted(T1w) MRI volumes while preserving topological properties. Our method first segments the T1w image into a nine-class tissue label map. From these segmentations, we generate subject-specific, collision-free initial surface meshes. These surfaces serve as precise initializations for subsequent multiscale diffeomorphic deformations. Employing stationary velocity fields (SVFs) integrated via scaling-and-squaring, our approach ensures smooth, topology-preserving transformations with significantly reduced surface collisions and self-intersections. Evaluations on standard datasets demonstrate that SimCortex dramatically reduces surface overlaps and self-intersections, surpassing current methods while maintaining state-of-the-art geometric accuracy. </p>
<blockquote>
<p>ä»ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ•°æ®å‡†ç¡®é‡å»ºçš®å±‚è¡¨é¢å¯¹äºå¯é çš„ç¥ç»è§£å‰–å­¦åˆ†æè‡³å…³é‡è¦ã€‚å½“å‰çš„æ–¹æ³•å¿…é¡»å¤„ç†å¤æ‚çš„çš®å±‚å‡ ä½•ç»“æ„ã€ä¸¥æ ¼çš„åœ°å½¢è¦æ±‚ï¼Œå¹¶ä¸”ç»å¸¸äº§ç”Ÿå…·æœ‰é‡å ã€è‡ªç›¸äº¤å’Œåœ°å½¢ç¼ºé™·çš„è¡¨é¢ã€‚ä¸ºäº†å…‹æœè¿™äº›ç¼ºç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†SimCortexï¼Œè¿™æ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶ä»T1åŠ æƒï¼ˆT1wï¼‰MRIä½“ç§¯æ•°æ®ä¸­é‡å»ºæ‰€æœ‰å¤§è„‘è¡¨é¢ï¼ˆå·¦&#x2F;å³ç™½è´¨å’Œçš®å±‚ï¼‰ï¼ŒåŒæ—¶ä¿ç•™åœ°å½¢å±æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆå°†T1wå›¾åƒåˆ†å‰²æˆä¹ç±»ç»„ç»‡æ ‡ç­¾å›¾ã€‚ä»è¿™äº›åˆ†å‰²ä¸­ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†é’ˆå¯¹ä¸ªä½“ã€æ— ç¢°æ’çš„åˆå§‹è¡¨é¢ç½‘æ ¼ã€‚è¿™äº›è¡¨é¢ä¸ºåç»­çš„å¤šå°ºåº¦å¾®åˆ†åŒèƒšå˜å½¢æä¾›äº†ç²¾ç¡®åˆå§‹åŒ–ã€‚æˆ‘ä»¬é‡‡ç”¨é€šè¿‡ç¼©æ”¾å’Œå¹³æ–¹é›†æˆçš„ç¨³æ€é€Ÿåº¦åœºï¼ˆSVFï¼‰ï¼Œç¡®ä¿å¹³æ»‘ã€ä¿æŒåœ°å½¢çš„å˜å½¢ï¼Œå¤§å¤§é™ä½äº†è¡¨é¢ç¢°æ’å’Œè‡ªç›¸äº¤ã€‚åœ¨æ ‡å‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒSimCortexæ˜¾è‘—å‡å°‘äº†è¡¨é¢é‡å å’Œè‡ªç›¸äº¤ï¼Œè¶…è¶Šäº†å½“å‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†æœ€å…ˆè¿›çš„å‡ ä½•ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06955v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSimCortexçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»T1åŠ æƒMRIæ•°æ®ä¸­å‡†ç¡®é‡å»ºæ‰€æœ‰å¤§è„‘è¡¨é¢ï¼ŒåŒæ—¶ä¿ç•™æ‹“æ‰‘å±æ€§ã€‚å®ƒé€šè¿‡åˆ†å‰²T1åŠ æƒå›¾åƒç”Ÿæˆåˆå§‹è¡¨é¢ç½‘æ ¼ï¼Œç„¶åé‡‡ç”¨å¤šå°ºåº¦å¾®åˆ†åŒèƒšå˜å½¢æŠ€æœ¯ï¼Œç¡®ä¿è¡¨é¢å…‰æ»‘ä¸”æ‹“æ‰‘ä¿æŒå˜æ¢ï¼Œæ˜¾è‘—å‡å°‘è¡¨é¢ç¢°æ’å’Œè‡ªç›¸äº¤ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒSimCortexåœ¨å‡å°‘è¡¨é¢é‡å å’Œè‡ªç›¸äº¤æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒå‡ ä½•ç²¾åº¦å¤„äºä¸šç•Œé¢†å…ˆåœ°ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SimCortexæ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå¯ä»¥ä»MRIæ•°æ®ä¸­é‡å»ºå¤§è„‘è¡¨é¢ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿå¤„ç†å¤æ‚çš„çš®è´¨å‡ ä½•ç»“æ„å’Œä¸¥æ ¼æ‹“æ‰‘è¦æ±‚ã€‚</li>
<li>SimCortexèƒ½å¤ŸåŒæ—¶é‡å»ºæ‰€æœ‰å¤§è„‘è¡¨é¢ï¼ˆå·¦å³è„‘ç™½è´¨å’Œçš®å±‚è¡¨é¢ï¼‰ã€‚</li>
<li>é€šè¿‡åˆ†å‰²T1åŠ æƒå›¾åƒç”Ÿæˆåˆå§‹è¡¨é¢ç½‘æ ¼ï¼Œä½œä¸ºåç»­å˜å½¢çš„ç²¾ç¡®åˆå§‹åŒ–ã€‚</li>
<li>é‡‡ç”¨å¤šå°ºåº¦å¾®åˆ†åŒèƒšå˜å½¢æŠ€æœ¯ï¼Œç¡®ä¿è¡¨é¢å…‰æ»‘ä¸”æ‹“æ‰‘ä¿æŒå˜æ¢ã€‚</li>
<li>ä½¿ç”¨åŸºäºé™æ­¢é€Ÿåº¦åœºçš„ç¼©æ”¾å’Œå¹³æ–¹æ³•ï¼Œå‡å°‘è¡¨é¢ç¢°æ’å’Œè‡ªç›¸äº¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-31901447d05d480d0b24bcde4bdaa4fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ced8aede27cb11f559aa9490fd6dd4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-faaa16bfedd0747ba859249da495fb93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2609ddff18a67164e5fae7a6215e210e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Dataset-and-Benchmark-for-Enhancing-Critical-Retained-Foreign-Object-Detection"><a href="#Dataset-and-Benchmark-for-Enhancing-Critical-Retained-Foreign-Object-Detection" class="headerlink" title="Dataset and Benchmark for Enhancing Critical Retained Foreign Object   Detection"></a>Dataset and Benchmark for Enhancing Critical Retained Foreign Object   Detection</h2><p><strong>Authors:Yuli Wang, Victoria R. Shi, Liwei Zhou, Richard Chin, Yuwei Dai, Yuanyun Hu, Cheng-Yi Li, Haoyue Guan, Jiashu Cheng, Yu Sun, Cheng Ting Lin, Ihab Kamel, Premal Trivedi, Pamela Johnson, John Eng, Harrison Bai</strong></p>
<p>Critical retained foreign objects (RFOs), including surgical instruments like sponges and needles, pose serious patient safety risks and carry significant financial and legal implications for healthcare institutions. Detecting critical RFOs using artificial intelligence remains challenging due to their rarity and the limited availability of chest X-ray datasets that specifically feature critical RFOs cases. Existing datasets only contain non-critical RFOs, like necklace or zipper, further limiting their utility for developing clinically impactful detection algorithms. To address these limitations, we introduce â€œHopkins RFOs Benchâ€, the first and largest dataset of its kind, containing 144 chest X-ray images of critical RFO cases collected over 18 years from the Johns Hopkins Health System. Using this dataset, we benchmark several state-of-the-art object detection models, highlighting the need for enhanced detection methodologies for critical RFO cases. Recognizing data scarcity challenges, we further explore image synthetic methods to bridge this gap. We evaluate two advanced synthetic image methods, DeepDRR-RFO, a physics-based method, and RoentGen-RFO, a diffusion-based method, for creating realistic radiographs featuring critical RFOs. Our comprehensive analysis identifies the strengths and limitations of each synthetic method, providing insights into effectively utilizing synthetic data to enhance model training. The Hopkins RFOs Bench and our findings significantly advance the development of reliable, generalizable AI-driven solutions for detecting critical RFOs in clinical chest X-rays. </p>
<blockquote>
<p>é—ç•™çš„è‡´å‘½å¼‚ç‰©ï¼ˆRFOsï¼‰ï¼ŒåŒ…æ‹¬æµ·ç»µã€é’ˆå¤´ç­‰æ‰‹æœ¯å™¨æ¢°ï¼Œå¯¹ç—…äººå®‰å…¨æ„æˆä¸¥é‡å¨èƒï¼Œå¹¶ä¸ºåŒ»ç–—æœºæ„å¸¦æ¥é‡å¤§ç»æµå’Œæ³•å¾‹é£é™©ã€‚ä½¿ç”¨äººå·¥æ™ºèƒ½æ£€æµ‹å…³é”®çš„RFOsæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºè¿™äº›å¼‚ç‰©çš„ç¨€æœ‰æ€§ä»¥åŠä¸“é—¨åŒ…å«å…³é”®RFOsç—…ä¾‹çš„èƒ¸éƒ¨Xå…‰æ•°æ®é›†æœ‰é™ã€‚ç°æœ‰æ•°æ®é›†ä»…åŒ…å«éå…³é”®çš„RFOsï¼Œå¦‚é¡¹é“¾æˆ–æ‹‰é“¾ï¼Œè¿™è¿›ä¸€æ­¥é™åˆ¶äº†å®ƒä»¬åœ¨å¼€å‘å…·æœ‰ä¸´åºŠå½±å“åŠ›çš„æ£€æµ‹ç®—æ³•æ–¹é¢çš„å®ç”¨æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†â€œéœæ™®é‡‘æ–¯RFOsåŸºå‡†æµ‹è¯•â€ï¼Œè¿™æ˜¯è¯¥é¢†åŸŸé¦–ä¸ªä¹Ÿæ˜¯æœ€å¤§çš„æ•°æ®é›†ï¼ŒåŒ…å«çº¦ç¿°æ–¯Â·éœæ™®é‡‘æ–¯åŒ»ç–—ç³»ç»Ÿ18å¹´æ¥æ”¶é›†çš„åŒ…å«å…³é”®RFOç—…ä¾‹çš„144å¼ èƒ¸éƒ¨Xå…‰ç‰‡ã€‚ä½¿ç”¨è¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯¹å‡ ç§æœ€å…ˆè¿›çš„ç‰©ä½“æ£€æµ‹æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¼ºè°ƒäº†é’ˆå¯¹å…³é”®RFOç—…ä¾‹éœ€è¦æé«˜æ£€æµ‹æ–¹æ³•çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬è®¤è¯†åˆ°æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œè¿›ä¸€æ­¥æ¢ç´¢å›¾åƒåˆæˆæ–¹æ³•æ¥å¼¥è¡¥è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸¤ç§å…ˆè¿›çš„åˆæˆå›¾åƒæ–¹æ³•ï¼Œä¸€ç§æ˜¯åŸºäºç‰©ç†çš„DeepDRR-RFOæ–¹æ³•ï¼Œå¦ä¸€ç§æ˜¯æ‰©æ•£åŸºç¡€çš„RoentGen-RFOæ–¹æ³•ï¼Œå®ƒä»¬å¯ä»¥ç”Ÿæˆå…·æœ‰çœŸå®æ„Ÿçš„Xå…‰ç‰‡ï¼Œå…¶ä¸­åŒ…å«å…³é”®çš„RFOsã€‚æˆ‘ä»¬çš„ç»¼åˆåˆ†æç¡®å®šäº†æ¯ç§åˆæˆæ–¹æ³•çš„ä¼˜ç‚¹å’Œå±€é™æ€§ï¼Œä¸ºæœ‰æ•ˆåˆ©ç”¨åˆæˆæ•°æ®ä»¥æé«˜æ¨¡å‹è®­ç»ƒæä¾›äº†è§è§£ã€‚â€œéœæ™®é‡‘æ–¯RFOsåŸºå‡†æµ‹è¯•â€ä»¥åŠæˆ‘ä»¬çš„å‘ç°æå¤§åœ°æ¨åŠ¨äº†å¼€å‘å¯é çš„ã€å¯æ¨å¹¿çš„AIé©±åŠ¨è§£å†³æ–¹æ¡ˆï¼Œç”¨äºæ£€æµ‹ä¸´åºŠèƒ¸éƒ¨Xå…‰ç‰‡ä¸­çš„å…³é”®RFOsã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06937v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¯¹å…³é”®æ®‹ç•™å¼‚ç‰©ï¼ˆRFOsï¼‰æ£€æµ‹çš„æŒ‘æˆ˜ï¼Œä»¥åŠå¦‚ä½•é€šè¿‡éœæ™®é‡‘æ–¯RFOsåŸºå‡†æ•°æ®é›†æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªçº¦ç¿°éœæ™®é‡‘æ–¯åŒ»ç–—ç³»ç»Ÿçš„144å¼ èƒ¸éƒ¨Xå…‰ç‰‡ï¼Œæ—¨åœ¨è§£å†³å…³é”®RFOç—…ä¾‹æ£€æµ‹çš„å…³é”®æŒ‘æˆ˜ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜æ¢è®¨äº†æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œé€šè¿‡æ¢è®¨åˆæˆå›¾åƒæ–¹æ³•æ¥ç¼©å°å·®è·ã€‚æœ€ç»ˆæ–‡ç« å…¨é¢è¯„ä¼°äº†ä¸¤ç§å…ˆè¿›çš„åˆæˆå›¾åƒæ–¹æ³•å¹¶æŒ‡å‡ºäº†å…¶ä¼˜ç¼ºç‚¹ï¼Œä¸ºæœ‰æ•ˆåˆ©ç”¨åˆæˆæ•°æ®æé«˜æ¨¡å‹è®­ç»ƒæ•ˆæœæä¾›äº†è§è§£ã€‚è¯¥ç ”ç©¶å’Œæ•°æ®é›†ä¸ºå¼€å‘å¯é çš„AIé©±åŠ¨è§£å†³æ–¹æ¡ˆä»¥æ£€æµ‹ä¸´åºŠèƒ¸éƒ¨Xå…‰ç‰‡ä¸­çš„å…³é”®RFOsæä¾›äº†é‡è¦æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å…³é”®ä¿ç•™å¼‚ç‰©ï¼ˆRFOsï¼‰æ˜¯åŒ»ç–—é¢†åŸŸçš„é‡è¦é—®é¢˜ï¼ŒåŒ…æ‹¬æµ·ç»µå’Œé’ˆç­‰æ‰‹æœ¯å™¨æ¢°ï¼Œå¯¹æ‚£è€…å®‰å…¨æ„æˆä¸¥é‡å¨èƒã€‚</li>
<li>éœæ™®é‡‘æ–¯RFOsåŸºå‡†æ•°æ®é›†çš„æ¨å‡ºè§£å†³äº†æ£€æµ‹å…³é”®RFOç—…ä¾‹çš„æŒ‘æˆ˜ï¼ŒåŒ…å«æ¥è‡ªçº¦ç¿°éœæ™®é‡‘æ–¯åŒ»ç–—ç³»ç»Ÿçš„ç¨€æœ‰ç—…ä¾‹æ•°æ®ã€‚</li>
<li>æ–‡ç« æŒ‡å‡ºå½“å‰æ•°æ®é›†å­˜åœ¨å±€é™æ€§ï¼Œå³åªåŒ…å«éå…³é”®RFOsï¼Œå› æ­¤éœ€è¦å¼€å‘æ›´æœ‰æ•ˆçš„æ£€æµ‹ç®—æ³•ã€‚</li>
<li>æ•°æ®ç¨€ç¼ºé—®é¢˜é™åˆ¶äº†æ¨¡å‹è®­ç»ƒçš„æœ‰æ•ˆæ€§ï¼Œå› æ­¤æ¢è®¨äº†åˆæˆå›¾åƒæ–¹æ³•æ¥ç¼©å°å·®è·ã€‚</li>
<li>æ–‡ç« è¯„ä¼°äº†ä¸¤ç§å…ˆè¿›çš„åˆæˆå›¾åƒæ–¹æ³•â€”â€”åŸºäºç‰©ç†çš„DeepDRR-RFOå’ŒåŸºäºæ‰©æ•£çš„RoentGen-RFOæ–¹æ³•ï¼Œç”¨ä»¥ç”Ÿæˆé€¼çœŸçš„åŒ…å«å…³é”®RFOsçš„æ”¾å°„å›¾åƒã€‚</li>
<li>ç ”ç©¶æŒ‡å‡ºäº†æ¯ç§åˆæˆæ–¹æ³•çš„ä¼˜ç‚¹å’Œå±€é™æ€§ï¼Œä¸ºæœ‰æ•ˆåˆ©ç”¨åˆæˆæ•°æ®å¢å¼ºæ¨¡å‹è®­ç»ƒæä¾›äº†è§è§£ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06937">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9f71de9c080452ba81456c92424ee1b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-641648333eed9652aa3410e6d907fcc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eceb8f0aa39912366cbbd51cac9a8ed1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70e571adb327c237a8c9ac2cd39b61b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c59dfe46eeef8bc6bd55e69d73c5f2e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Speckle2Self-Self-Supervised-Ultrasound-Speckle-Reduction-Without-Clean-Data"><a href="#Speckle2Self-Self-Supervised-Ultrasound-Speckle-Reduction-Without-Clean-Data" class="headerlink" title="Speckle2Self: Self-Supervised Ultrasound Speckle Reduction Without Clean   Data"></a>Speckle2Self: Self-Supervised Ultrasound Speckle Reduction Without Clean   Data</h2><p><strong>Authors:Xuesong Li, Nassir Navab, Zhongliang Jiang</strong></p>
<p>Image denoising is a fundamental task in computer vision, particularly in medical ultrasound (US) imaging, where speckle noise significantly degrades image quality. Although recent advancements in deep neural networks have led to substantial improvements in denoising for natural images, these methods cannot be directly applied to US speckle noise, as it is not purely random. Instead, US speckle arises from complex wave interference within the body microstructure, making it tissue-dependent. This dependency means that obtaining two independent noisy observations of the same scene, as required by pioneering Noise2Noise, is not feasible. Additionally, blind-spot networks also cannot handle US speckle noise due to its high spatial dependency. To address this challenge, we introduce Speckle2Self, a novel self-supervised algorithm for speckle reduction using only single noisy observations. The key insight is that applying a multi-scale perturbation (MSP) operation introduces tissue-dependent variations in the speckle pattern across different scales, while preserving the shared anatomical structure. This enables effective speckle suppression by modeling the clean image as a low-rank signal and isolating the sparse noise component. To demonstrate its effectiveness, Speckle2Self is comprehensively compared with conventional filter-based denoising algorithms and SOTA learning-based methods, using both realistic simulated US images and human carotid US images. Additionally, data from multiple US machines are employed to evaluate model generalization and adaptability to images from unseen domains. \textit{Code and datasets will be released upon acceptance. </p>
<blockquote>
<p>å›¾åƒå»å™ªæ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦è¶…å£°ï¼ˆUSï¼‰æˆåƒä¸­ï¼Œæ–‘ç‚¹å™ªå£°ä¼šä¸¥é‡é™ä½å›¾åƒè´¨é‡ã€‚å°½ç®¡æœ€è¿‘æ·±åº¦ç¥ç»ç½‘ç»œæŠ€æœ¯çš„è¿›æ­¥æå¤§åœ°æ”¹è¿›äº†è‡ªç„¶å›¾åƒçš„é™å™ªæ•ˆæœï¼Œä½†è¿™äº›æ–¹æ³•ä¸èƒ½ç›´æ¥åº”ç”¨äºè¶…å£°æ–‘ç‚¹å™ªå£°ï¼Œå› ä¸ºæ–‘ç‚¹å™ªå£°å¹¶éçº¯éšæœºäº§ç”Ÿã€‚ç›¸åï¼Œè¶…å£°æ–‘ç‚¹æ˜¯ç”±ä½“å†…å¾®è§‚ç»“æ„çš„å¤æ‚æ³¢å¹²æ¶‰äº§ç”Ÿçš„ï¼Œå…·æœ‰ç»„ç»‡ä¾èµ–æ€§ã€‚è¿™ç§ä¾èµ–æ€§æ„å‘³ç€è·å–åŒä¸€åœºæ™¯çš„ä¸¤ä¸ªç‹¬ç«‹å™ªå£°è§‚æµ‹ç»“æœï¼ˆå¦‚Noise2Noiseæ‰€è¦æ±‚çš„ï¼‰å¹¶ä¸å¯è¡Œã€‚æ­¤å¤–ï¼Œç”±äºæ–‘ç‚¹å™ªå£°çš„é«˜ç©ºé—´ä¾èµ–æ€§ï¼Œç›²ç‚¹ç½‘ç»œä¹Ÿæ— æ³•å¤„ç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Speckle2Selfï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨ä»…å•ä¸ªå™ªå£°è§‚æµ‹å€¼è¿›è¡Œæ–‘ç‚¹å‡å°‘çš„æ–°å‹è‡ªç›‘ç£ç®—æ³•ã€‚å…³é”®æ€æƒ³æ˜¯åº”ç”¨å¤šå°ºåº¦æ‰°åŠ¨ï¼ˆMSPï¼‰æ“ä½œï¼Œåœ¨ä¸åŒå°ºåº¦ä¸Šå¼•å…¥ç»„ç»‡ä¾èµ–æ€§çš„æ–‘ç‚¹å›¾æ¡ˆå˜åŒ–ï¼ŒåŒæ—¶ä¿ç•™å…±äº«çš„è§£å‰–ç»“æ„ã€‚è¿™èƒ½å¤Ÿé€šè¿‡å°†æ¸…æ´å›¾åƒå»ºæ¨¡ä¸ºä½ç§©ä¿¡å·å¹¶éš”ç¦»ç¨€ç–å™ªå£°æˆåˆ†ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„æ–‘ç‚¹æŠ‘åˆ¶ã€‚ä¸ºäº†è¯æ˜å…¶æœ‰æ•ˆæ€§ï¼ŒSpeckle2Selfä¸åŸºäºä¼ ç»Ÿæ»¤æ³¢çš„é™å™ªç®—æ³•å’ŒåŸºäºå­¦ä¹ çš„æœ€æ–°æ–¹æ³•è¿›è¡Œäº†å…¨é¢çš„æ¯”è¾ƒï¼Œä½¿ç”¨äº†é€¼çœŸçš„æ¨¡æ‹Ÿè¶…å£°å›¾åƒå’Œäººä½“é¢ˆåŠ¨è„‰è¶…å£°å›¾åƒã€‚æ­¤å¤–ï¼Œè¿˜ä½¿ç”¨äº†å¤šå°è¶…å£°æœºå™¨çš„æ•°æ®æ¥è¯„ä¼°æ¨¡å‹åœ¨æœªè§é¢†åŸŸçš„å›¾åƒä¸­çš„é€šç”¨æ€§å’Œé€‚åº”æ€§ã€‚\textit{ä»£ç å’Œæ•°æ®é›†å°†åœ¨æ¥å—åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06828v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŒ»å­¦è¶…å£°å›¾åƒä¸­çš„æ–‘ç‚¹å™ªå£°é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„è‡ªç›‘ç£ç®—æ³•Speckle2Selfï¼Œä»…ä½¿ç”¨å•å¼ å™ªå£°å›¾åƒè¿›è¡Œå»å™ªã€‚è¯¥ç®—æ³•é€šè¿‡å¤šå°ºåº¦æ‰°åŠ¨æ“ä½œï¼Œåœ¨ä¿ç•™å…±äº«è§£å‰–ç»“æ„çš„åŒæ—¶ï¼Œå¼•å…¥ç»„ç»‡ä¾èµ–çš„æ–‘ç‚¹æ¨¡å¼å˜åŒ–ï¼Œæœ‰æ•ˆæŠ‘åˆ¶æ–‘ç‚¹ã€‚ä¸åŸºäºæ»¤æ³¢å™¨çš„ä¼ ç»Ÿå»å™ªç®—æ³•å’Œå…ˆè¿›çš„å­¦ä¹ å‹æ–¹æ³•ç›¸æ¯”ï¼ŒSpeckle2Selfåœ¨æ¨¡æ‹Ÿçš„è¶…å£°å›¾åƒå’Œäººä½“é¢ˆåŠ¨è„‰è¶…å£°å›¾åƒä¸Šå‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†è‰¯å¥½çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦è¶…å£°å›¾åƒä¸­çš„æ–‘ç‚¹å™ªå£°æ˜¯ä¸€ä¸ªé‡è¦çš„é—®é¢˜ï¼Œå®ƒæ˜¾è‘—é™ä½äº†å›¾åƒè´¨é‡ã€‚</li>
<li>ç°æœ‰çš„æ·±åº¦ç¥ç»ç½‘ç»œå»å™ªæ–¹æ³•ä¸èƒ½ç›´æ¥åº”ç”¨äºè¶…å£°æ–‘ç‚¹å™ªå£°ï¼Œå› ä¸ºæ–‘ç‚¹å™ªå£°å…·æœ‰ç»„ç»‡ä¾èµ–æ€§ã€‚</li>
<li>Speckle2Selfæ˜¯ä¸€ç§æ–°å‹è‡ªç›‘ç£ç®—æ³•ï¼Œå¯ç”¨äºå‡å°‘æ–‘ç‚¹å™ªå£°ï¼Œä»…ä½¿ç”¨å•ä¸ªå™ªå£°è§‚å¯Ÿå€¼ã€‚</li>
<li>å¤šå°ºåº¦æ‰°åŠ¨æ“ä½œæ˜¯Speckle2Selfçš„å…³é”®ï¼Œèƒ½åœ¨ä¸åŒå°ºåº¦ä¸Šå¼•å…¥ç»„ç»‡ä¾èµ–çš„æ–‘ç‚¹æ¨¡å¼å˜åŒ–ã€‚</li>
<li>è¯¥ç®—æ³•èƒ½å¤Ÿæ¨¡æ‹Ÿæ¸…æ´å›¾åƒä½œä¸ºä½ç§©ä¿¡å·ï¼Œå¹¶éš”ç¦»ç¨€ç–å™ªå£°æˆåˆ†ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„æ–‘ç‚¹æŠ‘åˆ¶ã€‚</li>
<li>Speckle2Selfåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®çš„è¶…å£°å›¾åƒä¸Šå‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06828">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eb32fe9162633f6a46e629ed422d7b57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-745957136c8898cf7debbe05e0bb6d04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a523672f2e947308e1eff416e068a2d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b47c90c4b891954739b60032ac51f5b3.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="FOLC-Net-A-Federated-Optimized-Lightweight-Architecture-for-Enhanced-MRI-Disease-Diagnosis-across-Axial-Coronal-and-Sagittal-Views"><a href="#FOLC-Net-A-Federated-Optimized-Lightweight-Architecture-for-Enhanced-MRI-Disease-Diagnosis-across-Axial-Coronal-and-Sagittal-Views" class="headerlink" title="FOLC-Net: A Federated-Optimized Lightweight Architecture for Enhanced   MRI Disease Diagnosis across Axial, Coronal, and Sagittal Views"></a>FOLC-Net: A Federated-Optimized Lightweight Architecture for Enhanced   MRI Disease Diagnosis across Axial, Coronal, and Sagittal Views</h2><p><strong>Authors:Saif Ur Rehman Khan, Muhammad Nabeel Asim, Sebastian Vollmer, Andreas Dengel</strong></p>
<p>The framework is designed to improve performance in the analysis of combined as well as single anatomical perspectives for MRI disease diagnosis. It specifically addresses the performance degradation observed in state-of-the-art (SOTA) models, particularly when processing axial, coronal, and sagittal anatomical planes. The paper introduces the FOLC-Net framework, which incorporates a novel federated-optimized lightweight architecture with approximately 1.217 million parameters and a storage requirement of only 0.9 MB. FOLC-Net integrates Manta-ray foraging optimization (MRFO) mechanisms for efficient model structure generation, global model cloning for scalable training, and ConvNeXt for enhanced client adaptability. The model was evaluated on combined multi-view data as well as individual views, such as axial, coronal, and sagittal, to assess its robustness in various medical imaging scenarios. Moreover, FOLC-Net tests a ShallowFed model on different data to evaluate its ability to generalize beyond the training dataset. The results show that FOLC-Net outperforms existing models, particularly in the challenging sagittal view. For instance, FOLC-Net achieved an accuracy of 92.44% on the sagittal view, significantly higher than the 88.37% accuracy of study method (DL + Residual Learning) and 88.95% of DL models. Additionally, FOLC-Net demonstrated improved accuracy across all individual views, providing a more reliable and robust solution for medical image analysis in decentralized environments. FOLC-Net addresses the limitations of existing SOTA models by providing a framework that ensures better adaptability to individual views while maintaining strong performance in multi-view settings. The incorporation of MRFO, global model cloning, and ConvNeXt ensures that FOLC-Net performs better in real-world medical applications. </p>
<blockquote>
<p>è¯¥æ¡†æ¶æ—¨åœ¨æé«˜MRIç–¾ç—…è¯Šæ–­ä¸­ç»“åˆä»¥åŠå•ä¸€è§£å‰–è§’åº¦çš„åˆ†ææ€§èƒ½ã€‚å®ƒç‰¹åˆ«è§£å†³äº†æœ€æ–°æ¨¡å‹åœ¨å¤„ç†è½´å‘ã€å† çŠ¶å’ŒçŸ¢çŠ¶è§£å‰–å¹³é¢æ—¶å‡ºç°çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚æœ¬æ–‡ä»‹ç»äº†FOLC-Netæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æ–°å‹è”é‚¦ä¼˜åŒ–è½»é‡çº§æ¶æ„ï¼ŒåŒ…å«çº¦121ä¸‡å‚æ•°ï¼Œå­˜å‚¨éœ€æ±‚ä»…ä¸º0.9MBã€‚FOLC-Neté›†æˆäº†è é²¼è§…é£Ÿä¼˜åŒ–ï¼ˆMRFOï¼‰æœºåˆ¶ä»¥å®ç°é«˜æ•ˆæ¨¡å‹ç»“æ„ç”Ÿæˆã€å…¨å±€æ¨¡å‹å…‹éš†ä»¥å®ç°å¯æ‰©å±•è®­ç»ƒï¼Œä»¥åŠConvNeXtä»¥å¢å¼ºå®¢æˆ·ç«¯é€‚åº”æ€§ã€‚è¯¥æ¨¡å‹åœ¨å¯¹ç»„åˆå¤šè§†å›¾æ•°æ®ä»¥åŠå•ç‹¬çš„è½´å‘ã€å† çŠ¶å’ŒçŸ¢çŠ¶è§†å›¾ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œä»¥æµ‹è¯•å…¶åœ¨å„ç§åŒ»å­¦æˆåƒåœºæ™¯ä¸­çš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼ŒFOLC-Netåœ¨ä¸åŒæ•°æ®ä¸Šæµ‹è¯•äº†ShallowFedæ¨¡å‹ï¼Œä»¥è¯„ä¼°å…¶è¶…è¶Šè®­ç»ƒæ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼ŒFOLC-Netåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„çŸ¢çŠ¶è§†å›¾æ–¹é¢è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ã€‚ä¾‹å¦‚ï¼ŒFOLC-Netåœ¨çŸ¢çŠ¶è§†å›¾ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†92.44%ï¼Œæ˜¾è‘—é«˜äºç ”ç©¶æ–¹æ³•ï¼ˆDL+æ®‹å·®å­¦ä¹ ï¼‰çš„88.37%å’ŒDLæ¨¡å‹çš„88.95%ã€‚æ­¤å¤–ï¼ŒFOLC-Netåœ¨æ‰€æœ‰å•ä¸ªè§†å›¾ä¸Šçš„å‡†ç¡®æ€§éƒ½æœ‰æ‰€æé«˜ï¼Œä¸ºåˆ†å¸ƒå¼ç¯å¢ƒä¸­çš„åŒ»å­¦å›¾åƒåˆ†ææä¾›äº†æ›´å¯é å’Œç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚FOLC-Neté€šè¿‡æä¾›ä¸€ä¸ªæ¡†æ¶æ¥è§£å†³ç°æœ‰æœ€æ–°æ¨¡å‹åœ¨é€‚åº”ä¸ªåˆ«è§†å›¾æ—¶æ€§èƒ½ä¸‹é™çš„å±€é™æ€§ï¼ŒåŒæ—¶ä¿æŒå¤šè§†å›¾ç¯å¢ƒä¸­çš„å¼ºåŠ²è¡¨ç°ã€‚MRFOã€å…¨å±€æ¨¡å‹å…‹éš†å’ŒConvNeXtçš„é›†æˆç¡®ä¿äº†FOLC-Netåœ¨çœŸå®ä¸–ç•ŒåŒ»å­¦åº”ç”¨ä¸­çš„ä¼˜å¼‚æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06763v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥æ¡†æ¶æ—¨åœ¨æé«˜MRIç–¾ç—…è¯Šæ–­ä¸­å¯¹å•ä¸€å’Œå¤šè§’åº¦è§£å‰–å­¦è§†è§’åˆ†æçš„æ€§èƒ½ã€‚é’ˆå¯¹ç°æœ‰å…ˆè¿›æŠ€æœ¯æ¨¡å‹åœ¨å¤„ç†è½´å‘ã€å† çŠ¶å’ŒçŸ¢çŠ¶è§£å‰–å¹³é¢æ—¶çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œå¼•å…¥FOLC-Netæ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ä¼˜åŒ–çš„è½»é‡çº§æ¶æ„ã€è é²¼è§…é£Ÿä¼˜åŒ–æœºåˆ¶å’Œå…¨å±€æ¨¡å‹å…‹éš†ç­‰æŠ€æœ¯ï¼Œæé«˜äº†æ¨¡å‹åœ¨å¤šç§åŒ»å­¦æˆåƒåœºæ™¯ä¸­çš„ç¨³å¥æ€§ã€‚FOLC-Netåœ¨å•è§†è§’å’Œå¤šè§†è§’æ•°æ®ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå…¶æ€§èƒ½ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨æŒ‘æˆ˜æ€§çš„çŸ¢çŠ¶é¢ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>FOLC-Netæ¡†æ¶æ—¨åœ¨æé«˜MRIç–¾ç—…è¯Šæ–­ä¸­è§£å‰–å­¦è§†è§’åˆ†æçš„æ€§èƒ½ã€‚</li>
<li>æ¡†æ¶è§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨å¤„ç†è½´å‘ã€å† çŠ¶å’ŒçŸ¢çŠ¶è§£å‰–å¹³é¢æ—¶çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</li>
<li>FOLC-Netç»“åˆäº†ä¼˜åŒ–çš„è½»é‡çº§æ¶æ„ã€è é²¼è§…é£Ÿä¼˜åŒ–æœºåˆ¶å’Œå…¨å±€æ¨¡å‹å…‹éš†ç­‰æŠ€æœ¯ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒFOLC-Netåœ¨å•è§†è§’å’Œå¤šè§†è§’æ•°æ®ä¸Šçš„æ€§èƒ½å‡ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
<li>ç‰¹åˆ«åœ¨çŸ¢çŠ¶é¢ä¸Šï¼ŒFOLC-Netçš„å‡†ç¡®ç‡è¾¾åˆ°äº†92.44%ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>FOLC-Netæä¾›äº†åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­æ›´å¯é å’Œç¨³å¥çš„åŒ»å­¦å›¾åƒåˆ†æè§£å†³æ–¹æ¡ˆã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06763">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-31a36336b46746f6aa3b919c03f1fbfb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df3c6e1ad67d66482b8086aaae377926.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ClipGS-Clippable-Gaussian-Splatting-for-Interactive-Cinematic-Visualization-of-Volumetric-Medical-Data"><a href="#ClipGS-Clippable-Gaussian-Splatting-for-Interactive-Cinematic-Visualization-of-Volumetric-Medical-Data" class="headerlink" title="ClipGS: Clippable Gaussian Splatting for Interactive Cinematic   Visualization of Volumetric Medical Data"></a>ClipGS: Clippable Gaussian Splatting for Interactive Cinematic   Visualization of Volumetric Medical Data</h2><p><strong>Authors:Chengkun Li, Yuqi Tong, Kai Chen, Zhenya Yang, Ruiyang Li, Shi Qiu, Jason Ying-Kuen Chan, Pheng-Ann Heng, Qi Dou</strong></p>
<p>The visualization of volumetric medical data is crucial for enhancing diagnostic accuracy and improving surgical planning and education. Cinematic rendering techniques significantly enrich this process by providing high-quality visualizations that convey intricate anatomical details, thereby facilitating better understanding and decision-making in medical contexts. However, the high computing cost and low rendering speed limit the requirement of interactive visualization in practical applications. In this paper, we introduce ClipGS, an innovative Gaussian splatting framework with the clipping plane supported, for interactive cinematic visualization of volumetric medical data. To address the challenges posed by dynamic interactions, we propose a learnable truncation scheme that automatically adjusts the visibility of Gaussian primitives in response to the clipping plane. Besides, we also design an adaptive adjustment model to dynamically adjust the deformation of Gaussians and refine the rendering performance. We validate our method on five volumetric medical data (including CT and anatomical slice data), and reach an average 36.635 PSNR rendering quality with 156 FPS and 16.1 MB model size, outperforming state-of-the-art methods in rendering quality and efficiency. </p>
<blockquote>
<p>åŒ»å­¦ä½“ç§¯æ•°æ®çš„å¯è§†åŒ–å¯¹äºæé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œæ”¹å–„æ‰‹æœ¯è§„åˆ’å’Œæ•™è‚²è‡³å…³é‡è¦ã€‚ç”µå½±æ¸²æŸ“æŠ€æœ¯é€šè¿‡æä¾›é«˜è´¨é‡çš„å¯è§†åŒ–ï¼Œå±•ç°å¤æ‚çš„è§£å‰–ç»†èŠ‚ï¼Œä»è€Œä¸°å¯Œè¿™ä¸€è¿‡ç¨‹ï¼Œä¿ƒè¿›åŒ»å­¦ç¯å¢ƒä¸­çš„æ›´å¥½ç†è§£å’Œå†³ç­–ã€‚ç„¶è€Œï¼Œé«˜è®¡ç®—æˆæœ¬å’Œä½æ¸²æŸ“é€Ÿåº¦é™åˆ¶äº†å®é™…åº”ç”¨ä¸­å¯¹äº¤äº’å¼å¯è§†åŒ–çš„éœ€æ±‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ClipGSï¼Œè¿™æ˜¯ä¸€ä¸ªæ”¯æŒè£å‰ªå¹³é¢äº¤äº’çš„ç”µå½±ä½“ç§¯åŒ»å­¦æ•°æ®å¯è§†åŒ–é«˜æ–¯å–·æ¶‚æ¡†æ¶ã€‚ä¸ºäº†è§£å†³åŠ¨æ€äº¤äº’å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯å­¦ä¹ çš„æˆªæ–­æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå¯ä»¥è‡ªåŠ¨æ ¹æ®è£å‰ªå¹³é¢è°ƒæ•´é«˜æ–¯åŸå§‹æ•°æ®çš„å¯è§æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªè‡ªé€‚åº”è°ƒæ•´æ¨¡å‹ï¼Œå¯ä»¥åŠ¨æ€è°ƒæ•´é«˜æ–¯å˜å½¢å¹¶ä¼˜åŒ–æ¸²æŸ“æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªåŒ»å­¦ä½“ç§¯æ•°æ®ï¼ˆåŒ…æ‹¬CTå’Œè§£å‰–åˆ‡ç‰‡æ•°æ®ï¼‰ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¾¾åˆ°äº†å¹³å‡36.635çš„PSNRæ¸²æŸ“è´¨é‡ï¼Œå¸§é€Ÿç‡ä¸ºæ¯ç§’156å¸§ï¼Œæ¨¡å‹å¤§å°ä¸º16.1MBï¼Œåœ¨æ¸²æŸ“è´¨é‡å’Œæ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06647v1">PDF</a> Early accepted by MICCAI 2025. Project is available at:   <a target="_blank" rel="noopener" href="https://med-air.github.io/ClipGS">https://med-air.github.io/ClipGS</a></p>
<p><strong>Summary</strong></p>
<p>æ–‡ä¸­ä»‹ç»äº†ä½“ç§¯åŒ»å­¦æ•°æ®å¯è§†åŒ–å¯¹äºæé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œæ”¹å–„æ‰‹æœ¯è§„åˆ’ä¸æ•™è‚²çš„é‡è¦æ€§ã€‚ç”µå½±æ¸²æŸ“æŠ€æœ¯ä¸°å¯Œäº†è¿™ä¸€è¿‡ç¨‹ï¼Œæä¾›äº†é«˜è´¨é‡çš„å¯è§†åŒ–è¡¨ç°ï¼Œå±•ç¤ºäº†å¤æ‚çš„è§£å‰–ç»†èŠ‚ï¼Œæœ‰åŠ©äºæ›´å¥½åœ°ç†è§£å’Œå†³ç­–ã€‚é’ˆå¯¹å®é™…åº”ç”¨ä¸­äº¤äº’å¼å¯è§†åŒ–çš„é«˜è®¡ç®—æˆæœ¬å’Œä½æ¸²æŸ“é€Ÿåº¦é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ClipGSï¼Œä¸€ç§æ”¯æŒè£å‰ªå¹³é¢çš„é«˜æ–¯æ¶‚æŠ¹æ¡†æ¶ï¼Œç”¨äºäº¤äº’å¼ç”µå½±æ¸²æŸ“æŠ€æœ¯ã€‚é€šè¿‡æå‡ºä¸€ç§å¯å­¦ä¹ çš„æˆªæ–­æ–¹æ¡ˆå’Œè‡ªé€‚åº”è°ƒæ•´æ¨¡å‹ï¼Œè¯¥æŠ€æœ¯åœ¨ä¿è¯é«˜è´¨é‡æ¸²æŸ“çš„åŒæ—¶æé«˜äº†æ•ˆç‡ã€‚åœ¨äº”ä¸ªä½“ç§¯åŒ»å­¦æ•°æ®é›†ä¸Šçš„éªŒè¯æ˜¾ç¤ºï¼Œå…¶å¹³å‡å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ä¸º36.635ï¼Œå¸§ç‡ä¸ºæ¯ç§’156å¸§ï¼Œæ¨¡å‹å¤§å°ä¸º16.1MBï¼Œåœ¨æ¸²æŸ“è´¨é‡å’Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½“ç§¯åŒ»å­¦æ•°æ®å¯è§†åŒ–å¯¹äºè¯Šæ–­ã€æ‰‹æœ¯è§„åˆ’å’Œæ•™è‚²çš„é‡è¦æ€§ã€‚</li>
<li>ç”µå½±æ¸²æŸ“æŠ€æœ¯æä¾›é«˜è´¨é‡åŒ»å­¦æ•°æ®å¯è§†åŒ–ï¼Œå±•ç¤ºå¤æ‚è§£å‰–ç»†èŠ‚ã€‚</li>
<li>äº¤äº’å¼å¯è§†åŒ–åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´é«˜è®¡ç®—æˆæœ¬å’Œä½æ¸²æŸ“é€Ÿåº¦çš„æŒ‘æˆ˜ã€‚</li>
<li>ClipGSæ˜¯ä¸€ä¸ªåŸºäºé«˜æ–¯æ¶‚æŠ¹çš„æ¡†æ¶ï¼Œæ”¯æŒè£å‰ªå¹³é¢ï¼Œç”¨äºäº¤äº’å¼ç”µå½±æ¸²æŸ“æŠ€æœ¯ã€‚</li>
<li>æå‡ºå¯å­¦ä¹ çš„æˆªæ–­æ–¹æ¡ˆå’Œè‡ªé€‚åº”è°ƒæ•´æ¨¡å‹ï¼Œä»¥æé«˜æ¸²æŸ“æ•ˆç‡å’Œè´¨é‡ã€‚</li>
<li>åœ¨äº”ä¸ªä½“ç§¯åŒ»å­¦æ•°æ®é›†ä¸Šçš„éªŒè¯ç»“æœè¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡PSNRä¸º36.635ï¼Œå¸§ç‡ä¸ºæ¯ç§’156å¸§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06647">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-282bfce4c5ac0dac28640b251dd10122.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0a84b11284d88fd4e3efe2f8b4f154d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Omni-Fusion-of-Spatial-and-Spectral-for-Hyperspectral-Image-Segmentation"><a href="#Omni-Fusion-of-Spatial-and-Spectral-for-Hyperspectral-Image-Segmentation" class="headerlink" title="Omni-Fusion of Spatial and Spectral for Hyperspectral Image Segmentation"></a>Omni-Fusion of Spatial and Spectral for Hyperspectral Image Segmentation</h2><p><strong>Authors:Qing Zhang, Guoquan Pei, Yan Wang</strong></p>
<p>Medical Hyperspectral Imaging (MHSI) has emerged as a promising tool for enhanced disease diagnosis, particularly in computational pathology, offering rich spectral information that aids in identifying subtle biochemical properties of tissues. Despite these advantages, effectively fusing both spatial-dimensional and spectral-dimensional information from MHSIs remains challenging due to its high dimensionality and spectral redundancy inherent characteristics. To solve the above challenges, we propose a novel spatial-spectral omni-fusion network for hyperspectral image segmentation, named as Omni-Fuse. Here, we introduce abundant cross-dimensional feature fusion operations, including a cross-dimensional enhancement module that refines both spatial and spectral features through bidirectional attention mechanisms, a spectral-guided spatial query selection to select the most spectral-related spatial feature as the query, and a two-stage cross-dimensional decoder which dynamically guide the model to focus on the selected spatial query. Despite of numerous attention blocks, Omni-Fuse remains efficient in execution. Experiments on two microscopic hyperspectral image datasets show that our approach can significantly improve the segmentation performance compared with the state-of-the-art methods, with over 5.73 percent improvement in DSC. Code available at: <a target="_blank" rel="noopener" href="https://github.com/DeepMed-Lab-ECNU/Omni-Fuse">https://github.com/DeepMed-Lab-ECNU/Omni-Fuse</a>. </p>
<blockquote>
<p>åŒ»å­¦é«˜å…‰è°±æˆåƒï¼ˆMHSIï¼‰ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„å·¥å…·ï¼Œåœ¨å¢å¼ºç–¾ç—…è¯Šæ–­ä¸­è¡¨ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è®¡ç®—ç—…ç†å­¦é¢†åŸŸã€‚å®ƒæä¾›äº†ä¸°å¯Œçš„å…‰è°±ä¿¡æ¯ï¼Œæœ‰åŠ©äºè¯†åˆ«ç»„ç»‡çš„ç»†å¾®ç”Ÿç‰©åŒ–å­¦ç‰¹æ€§ã€‚å°½ç®¡å…·æœ‰è¿™äº›ä¼˜åŠ¿ï¼Œä½†ç”±äºé«˜å…‰è°±æˆåƒçš„é«˜ç»´åº¦å’Œå…‰è°±å†—ä½™ç­‰å›ºæœ‰ç‰¹æ€§ï¼Œæœ‰æ•ˆèåˆç©ºé—´ç»´åº¦å’Œå…‰è°±ç»´åº¦çš„ä¿¡æ¯ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºé«˜å…‰è°±å›¾åƒåˆ†å‰²çš„ç©ºé—´å…‰è°±å…¨èåˆç½‘ç»œï¼Œå‘½åä¸ºOmni-Fuseã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸°å¯Œçš„è·¨ç»´ç‰¹å¾èåˆæ“ä½œï¼ŒåŒ…æ‹¬è·¨ç»´å¢å¼ºæ¨¡å—ï¼Œè¯¥æ¨¡å—é€šè¿‡åŒå‘æ³¨æ„åŠ›æœºåˆ¶ç»†åŒ–ç©ºé—´å’Œå…‰è°±ç‰¹å¾ã€å…‰è°±å¼•å¯¼çš„ç©ºé—´æŸ¥è¯¢é€‰æ‹©ï¼Œé€‰æ‹©æœ€ç›¸å…³çš„å…‰è°±ç©ºé—´ç‰¹å¾ä½œä¸ºæŸ¥è¯¢ï¼Œä»¥åŠä¸¤é˜¶æ®µè·¨ç»´è§£ç å™¨ï¼ŒåŠ¨æ€å¼•å¯¼æ¨¡å‹å…³æ³¨æ‰€é€‰çš„ç©ºé—´æŸ¥è¯¢ã€‚å°½ç®¡å­˜åœ¨å¤§é‡çš„æ³¨æ„åŠ›å—ï¼ŒOmni-Fuseåœ¨æ‰§è¡Œæ—¶ä»ä¿æŒé«˜æ•ˆã€‚åœ¨ä¸¤ä¸ªæ˜¾å¾®é«˜å…‰è°±å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ–¹æ³•å¯ä»¥æ˜¾è‘—æé«˜åˆ†å‰²æ€§èƒ½ï¼Œåœ¨DSCä¸Šæé«˜äº†5.73%ä»¥ä¸Šã€‚ä»£ç å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/DeepMed-Lab-ECNU/Omni-Fuse%E3%80%82">https://github.com/DeepMed-Lab-ECNU/Omni-Fuseã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06606v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦è¶…å…‰è°±æˆåƒï¼ˆMHSIï¼‰åœ¨ç–¾ç—…è¯Šæ–­ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è®¡ç®—ç—…ç†å­¦é¢†åŸŸã€‚ä¸ºæé«˜è¶…å…‰è°±å›¾åƒåˆ†å‰²æ•ˆæœï¼Œæå‡ºäº†ä¸€ç§åä¸ºOmni-Fuseçš„ç©ºé—´å…‰è°±å…¨èåˆç½‘ç»œã€‚è¯¥ç½‘ç»œåŒ…å«è·¨ç»´åº¦ç‰¹å¾èåˆæ“ä½œï¼Œå¦‚åŒå‘æ³¨æ„åŠ›æœºåˆ¶çš„è·¨ç»´åº¦å¢å¼ºæ¨¡å—ã€å…‰è°±å¼•å¯¼çš„ç©ºé—´æŸ¥è¯¢é€‰æ‹©åŠä¸¤é˜¶æ®µè·¨ç»´åº¦è§£ç å™¨ã€‚åœ¨ä¸¤ç§æ˜¾å¾®è¶…å…‰è°±å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”èƒ½æ˜¾è‘—æé«˜åˆ†å‰²æ€§èƒ½ï¼ŒDSCæå‡è¶…è¿‡5.73%ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦è¶…å…‰è°±æˆåƒï¼ˆMHSIï¼‰åœ¨ç–¾ç—…è¯Šæ–­ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è®¡ç®—ç—…ç†å­¦é¢†åŸŸã€‚</li>
<li>Omni-Fuseæ˜¯ä¸€ç§æ–°å‹çš„ç©ºé—´å…‰è°±å…¨èåˆç½‘ç»œï¼Œç”¨äºè¶…å…‰è°±å›¾åƒåˆ†å‰²ã€‚</li>
<li>Omni-Fuseç½‘ç»œåŒ…å«ä¸°å¯Œçš„è·¨ç»´åº¦ç‰¹å¾èåˆæ“ä½œï¼ŒåŒ…æ‹¬è·¨ç»´åº¦å¢å¼ºæ¨¡å—ã€å…‰è°±å¼•å¯¼çš„ç©ºé—´æŸ¥è¯¢é€‰æ‹©å’Œä¸¤é˜¶æ®µè·¨ç»´åº¦è§£ç å™¨ã€‚</li>
<li>Omni-Fuseé€šè¿‡åŒå‘æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–ç©ºé—´åŠå…‰è°±ç‰¹å¾ã€‚</li>
<li>Omni-Fuseé€šè¿‡å…‰è°±å¼•å¯¼é€‰æ‹©æœ€ç›¸å…³çš„ç©ºé—´ç‰¹å¾ä½œä¸ºæŸ¥è¯¢ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒOmni-Fuseåœ¨ä¸¤ç§æ˜¾å¾®è¶…å…‰è°±å›¾åƒæ•°æ®é›†ä¸Šçš„åˆ†å‰²æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06606">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc07553569e965c961d03b0f3d3c182f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fc75671467a830eebdac844f1cc1ed7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad9c7358c1198292769e7c4f3143b8eb.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Capsule-ConvKAN-A-Hybrid-Neural-Approach-to-Medical-Image-Classification"><a href="#Capsule-ConvKAN-A-Hybrid-Neural-Approach-to-Medical-Image-Classification" class="headerlink" title="Capsule-ConvKAN: A Hybrid Neural Approach to Medical Image   Classification"></a>Capsule-ConvKAN: A Hybrid Neural Approach to Medical Image   Classification</h2><p><strong>Authors:Laura PitukovÃ¡, Peter SinÄÃ¡k, LÃ¡szlÃ³ JÃ³zsef KovÃ¡cs</strong></p>
<p>This study conducts a comprehensive comparison of four neural network architectures: Convolutional Neural Network, Capsule Network, Convolutional Kolmogorovâ€“Arnold Network, and the newly proposed Capsuleâ€“Convolutional Kolmogorovâ€“Arnold Network. The proposed Capsule-ConvKAN architecture combines the dynamic routing and spatial hierarchy capabilities of Capsule Network with the flexible and interpretable function approximation of Convolutional Kolmogorovâ€“Arnold Networks. This novel hybrid model was developed to improve feature representation and classification accuracy, particularly in challenging real-world biomedical image data. The architectures were evaluated on a histopathological image dataset, where Capsule-ConvKAN achieved the highest classification performance with an accuracy of 91.21%. The results demonstrate the potential of the newly introduced Capsule-ConvKAN in capturing spatial patterns, managing complex features, and addressing the limitations of traditional convolutional models in medical image classification. </p>
<blockquote>
<p>æœ¬ç ”ç©¶å¯¹å››ç§ç¥ç»ç½‘ç»œæ¶æ„è¿›è¡Œäº†å…¨é¢æ¯”è¾ƒï¼šå·ç§¯ç¥ç»ç½‘ç»œã€èƒ¶å›Šç½‘ç»œã€å·ç§¯Kolmogorov-Arnoldç½‘ç»œä»¥åŠæ–°æå‡ºçš„èƒ¶å›Š-å·ç§¯Kolmogorov-Arnoldç½‘ç»œã€‚æ‰€æå‡ºçš„Capsule-ConvKANæ¶æ„ç»“åˆäº†èƒ¶å›Šç½‘ç»œçš„åŠ¨æ€è·¯ç”±å’Œç©ºé—´å±‚æ¬¡ç»“æ„èƒ½åŠ›ï¼Œä»¥åŠå·ç§¯Kolmogorov-Arnoldç½‘ç»œçš„çµæ´»å’Œå¯è§£é‡Šå‡½æ•°é€¼è¿‘èƒ½åŠ›ã€‚è¯¥æ–°å‹æ··åˆæ¨¡å‹æ—¨åœ¨æ”¹è¿›ç‰¹å¾è¡¨ç¤ºå’Œåˆ†ç±»ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®ä¸–ç•Œç”Ÿç‰©åŒ»å­¦å›¾åƒæ•°æ®ä¸­ã€‚è¿™äº›æ¶æ„åœ¨ç—…ç†å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå…¶ä¸­Capsule-ConvKANçš„åˆ†ç±»æ€§èƒ½æœ€é«˜ï¼Œå‡†ç¡®ç‡ä¸º9 1.2 1%ã€‚ç»“æœè¡¨æ˜ï¼Œæ–°å¼•å…¥çš„Capsule-ConvKANåœ¨æ•è·ç©ºé—´æ¨¡å¼ã€ç®¡ç†å¤æ‚ç‰¹å¾ä»¥åŠè§£å†³åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­ä¼ ç»Ÿå·ç§¯æ¨¡å‹çš„å±€é™æ€§æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06417v1">PDF</a> Preprint version. Accepted to IEEE SMC 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…¨é¢æ¯”è¾ƒäº†å››ç§ç¥ç»ç½‘ç»œæ¶æ„ï¼ŒåŒ…æ‹¬å·ç§¯ç¥ç»ç½‘ç»œã€èƒ¶å›Šç½‘ç»œã€å·ç§¯Kolmogorov-Arnoldç½‘ç»œå’Œæ–°å…´çš„èƒ¶å›Šå·ç§¯Kolmogorov-Arnoldç½‘ç»œï¼ˆCapsule-ConvKANï¼‰ã€‚å…¶ä¸­ï¼ŒCapsule-ConvKANç»“åˆäº†èƒ¶å›Šç½‘ç»œçš„åŠ¨æ€è·¯ç”±å’Œç©ºé—´å±‚æ¬¡ç»“æ„èƒ½åŠ›ï¼Œä»¥åŠå·ç§¯Kolmogorov-Arnoldç½‘ç»œçš„çµæ´»å’Œå¯è§£é‡Šå‡½æ•°é€¼è¿‘èƒ½åŠ›ã€‚è¯¥æ–°å‹æ··åˆæ¨¡å‹æ—¨åœ¨æ”¹è¿›ç‰¹å¾è¡¨ç¤ºå’Œåˆ†ç±»ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®ä¸–ç•Œç”Ÿç‰©åŒ»å­¦å›¾åƒæ•°æ®ä¸­ã€‚åœ¨ç—…ç†å›¾åƒæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒCapsule-ConvKANçš„åˆ†ç±»æ€§èƒ½æœ€é«˜ï¼Œå‡†ç¡®ç‡ä¸º91.21%ã€‚è¿™è¯æ˜äº†Capsule-ConvKANåœ¨æ•æ‰ç©ºé—´æ¨¡å¼ã€ç®¡ç†å¤æ‚ç‰¹å¾ä»¥åŠè§£å†³åŒ»ç–—å›¾åƒåˆ†ç±»ä¸­ä¼ ç»Ÿå·ç§¯æ¨¡å‹çš„å±€é™æ€§æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å››ç§ç¥ç»ç½‘ç»œæ¶æ„è¿›è¡Œäº†å…¨é¢æ¯”è¾ƒã€‚</li>
<li>æå‡ºäº†æ–°å‹çš„Capsule-ConvKANæ¶æ„ï¼Œç»“åˆäº†èƒ¶å›Šç½‘ç»œå’Œå·ç§¯Kolmogorov-Arnoldç½‘ç»œçš„ä¼˜ç‚¹ã€‚</li>
<li>Capsule-ConvKANæ—¨åœ¨æ”¹è¿›ç‰¹å¾è¡¨ç¤ºå’Œåˆ†ç±»ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒæ•°æ®ä¸­ã€‚</li>
<li>Capsule-ConvKANåœ¨ç—…ç†å›¾åƒæ•°æ®é›†ä¸Šçš„åˆ†ç±»æ€§èƒ½æœ€é«˜ï¼Œå‡†ç¡®ç‡ä¸º91.21%ã€‚</li>
<li>è¯¥ç ”ç©¶è¯æ˜äº†Capsule-ConvKANåœ¨æ•æ‰ç©ºé—´æ¨¡å¼å’Œç®¡ç†å¤æ‚ç‰¹å¾æ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>Capsule-ConvKANè§£å†³äº†ä¼ ç»Ÿå·ç§¯æ¨¡å‹åœ¨åŒ»ç–—å›¾åƒåˆ†ç±»ä¸­çš„å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06417">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24c047e543f1e857d4258a4c067c822f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76739e0ee23568a5d06359c2c644e379.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f57cd882bef36fd0117bea17eeb9e83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-816bd119cd804f6dac4a9509a2b3503f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e22ebba064282e676603011b35fbaa8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f8eb2700b9ca3286e9eac981ea2cde9.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Mitigating-Multi-Sequence-3D-Prostate-MRI-Data-Scarcity-through-Domain-Adaptation-using-Locally-Trained-Latent-Diffusion-Models-for-Prostate-Cancer-Detection"><a href="#Mitigating-Multi-Sequence-3D-Prostate-MRI-Data-Scarcity-through-Domain-Adaptation-using-Locally-Trained-Latent-Diffusion-Models-for-Prostate-Cancer-Detection" class="headerlink" title="Mitigating Multi-Sequence 3D Prostate MRI Data Scarcity through Domain   Adaptation using Locally-Trained Latent Diffusion Models for Prostate Cancer   Detection"></a>Mitigating Multi-Sequence 3D Prostate MRI Data Scarcity through Domain   Adaptation using Locally-Trained Latent Diffusion Models for Prostate Cancer   Detection</h2><p><strong>Authors:Emerson P. Grabke, Babak Taati, Masoom A. Haider</strong></p>
<p>Objective: Latent diffusion models (LDMs) could mitigate data scarcity challenges affecting machine learning development for medical image interpretation. The recent CCELLA LDM improved prostate cancer detection performance using synthetic MRI for classifier training but was limited to the axial T2-weighted (AxT2) sequence, did not investigate inter-institutional domain shift, and prioritized radiology over histopathology outcomes. We propose CCELLA++ to address these limitations and improve clinical utility. Methods: CCELLA++ expands CCELLA for simultaneous biparametric prostate MRI (bpMRI) generation, including the AxT2, high b-value diffusion series (HighB) and apparent diffusion coefficient map (ADC). Domain adaptation was investigated by pretraining classifiers on real or LDM-generated synthetic data from an internal institution, followed with fine-tuning on progressively smaller fractions of an out-of-distribution, external dataset. Results: CCELLA++ improved 3D FID for HighB and ADC but not AxT2 (0.013, 0.012, 0.063 respectively) sequences compared to CCELLA (0.060). Classifier pretraining with CCELLA++ bpMRI outperformed real bpMRI in AP and AUC for all domain adaptation scenarios. CCELLA++ pretraining achieved highest classifier performance below 50% (n&#x3D;665) external dataset volume. Conclusion: Synthetic bpMRI generated by our method can improve downstream classifier generalization and performance beyond real bpMRI or CCELLA-generated AxT2-only images. Future work should seek to quantify medical image sample quality, balance multi-sequence LDM training, and condition the LDM with additional information. Significance: The proposed CCELLA++ LDM can generate synthetic bpMRI that outperforms real data for domain adaptation with a limited target institution dataset. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/grabkeem/CCELLA-plus-plus">https://github.com/grabkeem/CCELLA-plus-plus</a> </p>
<blockquote>
<p>ç›®æ ‡ï¼šæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰å¯ä»¥ç¼“è§£åŒ»å­¦å›¾åƒè§£è¯»æœºå™¨å­¦ä¹ å¼€å‘ä¸­çš„æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜ã€‚æœ€è¿‘çš„CCELLA LDMé€šè¿‡ä½¿ç”¨åˆæˆMRIæé«˜äº†å‰åˆ—è…ºç™Œæ£€æµ‹æ€§èƒ½ï¼Œç”¨äºåˆ†ç±»å™¨è®­ç»ƒï¼Œä½†ä»…é™äºè½´å‘T2åŠ æƒï¼ˆAxT2ï¼‰åºåˆ—ï¼Œæœªç ”ç©¶è·¨æœºæ„é¢†åŸŸåç§»ï¼Œå¹¶ä¼˜å…ˆé‡è§†æ”¾å°„å­¦è€Œéç»„ç»‡ç—…ç†å­¦ç»“æœã€‚æˆ‘ä»¬æå‡ºCCELLA++æ¥è§£å†³è¿™äº›å±€é™æ€§å¹¶æé«˜ä¸´åºŠå®ç”¨æ€§ã€‚æ–¹æ³•ï¼šCCELLA++æ‰©å±•äº†CCELLAï¼Œç”¨äºåŒæ—¶ç”ŸæˆåŒå‚æ•°å‰åˆ—è…ºMRIï¼ˆbpMRIï¼‰ï¼ŒåŒ…æ‹¬AxT2ã€é«˜bå€¼æ‰©æ•£ç³»åˆ—ï¼ˆHighBï¼‰å’Œæ‰©æ•£ç³»æ•°å›¾ï¼ˆADCï¼‰ã€‚é€šè¿‡ç”¨å†…éƒ¨æœºæ„çš„çœŸå®æ•°æ®æˆ–LDMåˆæˆæ•°æ®é¢„è®­ç»ƒåˆ†ç±»å™¨ï¼Œç„¶ååœ¨é€æ¸è¾ƒå°çš„å¤–éƒ¨æ•°æ®é›†éƒ¨åˆ†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œæ¥ç ”ç©¶é¢†åŸŸé€‚åº”æ€§é—®é¢˜ã€‚ç»“æœï¼šä¸CCELLAç›¸æ¯”ï¼ŒCCELLA++åœ¨HighBå’ŒADCåºåˆ—ä¸Šæ”¹è¿›äº†3D FIDï¼ˆåˆ†åˆ«ä¸º0.013ã€0.012å’Œ0.063ï¼‰ï¼Œè€ŒAxT2åºåˆ—åˆ™æ²¡æœ‰ï¼ˆ0.060ï¼‰ã€‚ä½¿ç”¨CCELLA++ bpMRIé¢„è®­ç»ƒçš„åˆ†ç±»å™¨åœ¨æ‰€æœ‰é¢†åŸŸé€‚åº”åœºæ™¯ä¸­ï¼Œå…¶APå’ŒAUCå‡ä¼˜äºçœŸå®bpMRIã€‚CCELLA++é¢„è®­ç»ƒåœ¨å¤–éƒ¨æ•°æ®é›†ä½“ç§¯ä½äº50%ï¼ˆn&#x3D;665ï¼‰æ—¶è¾¾åˆ°äº†æœ€é«˜çš„åˆ†ç±»å™¨æ€§èƒ½ã€‚ç»“è®ºï¼šæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„åˆæˆbpMRIå¯ä»¥æ”¹å–„ä¸‹æ¸¸åˆ†ç±»å™¨çš„æ³›åŒ–å’Œæ€§èƒ½ï¼Œä¼˜äºçœŸå®bpMRIæˆ–CCELLAç”Ÿæˆçš„ä»…AxT2å›¾åƒã€‚æœªæ¥çš„å·¥ä½œåº”è¯¥è‡´åŠ›äºé‡åŒ–åŒ»å­¦å›¾åƒæ ·æœ¬è´¨é‡ï¼Œå¹³è¡¡å¤šåºåˆ—LDMè®­ç»ƒï¼Œå¹¶ç”¨é¢å¤–ä¿¡æ¯è°ƒæ•´LDMã€‚æ„ä¹‰ï¼šæ‰€æå‡ºçš„CCELLA++ LDMå¯ä»¥ç”ŸæˆåˆæˆbpMRIï¼Œåœ¨é¢†åŸŸé€‚åº”æ–¹é¢ä¼˜äºçœŸå®æ•°æ®ï¼Œå³ä½¿ç›®æ ‡æœºæ„æ•°æ®é›†æœ‰é™ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/grabkeem/CCELLA-plus-plus%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/grabkeem/CCELLA-plus-plusæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06384v1">PDF</a> BT and MAH are co-senior authors on the work. This work has been   submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelsï¼ŒLDMsï¼‰åœ¨è§£å†³åŒ»å­¦å›¾åƒè§£é‡Šæœºå™¨å­¦ä¹ å¼€å‘ä¸­çš„æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜æ–¹é¢çš„åº”ç”¨ã€‚æå‡ºçš„CCELLA++æ¨¡å‹æ—¨åœ¨è§£å†³CCELLAæ¨¡å‹çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬ä»…é™äºè½´çŠ¶T2åŠ æƒåºåˆ—çš„é—®é¢˜ï¼Œå¹¶é’ˆå¯¹ä¸åŒåŒ»é™¢æ•°æ®ä¹‹é—´çš„å˜åŒ–å±•å¼€è°ƒæŸ¥ï¼Œå¹¶åœ¨é‡è§†æ”¾å°„å­¦çš„åŒæ—¶å¹³è¡¡ä¸ç—…ç†ç»„ç»‡å­¦ç»“æœçš„å…³ç³»ã€‚é€šè¿‡æ‹“å±•ç”¨äºåŒæ—¶ç”ŸæˆåŒå‚æ•°å‰åˆ—è…ºMRIçš„CCELLA++æ¨¡å‹ä»¥åŠè°ƒæŸ¥åˆ†ç±»å™¨åœ¨å¤šä¸ªé¢†åŸŸçš„é€‚åº”åŠ›ç­‰æ–¹é¢æ¥åŠ å¼ºä¸´åºŠåº”ç”¨æ•ˆæœã€‚è¯¥æ¨¡å‹æˆåŠŸæ”¹è¿›äº†æŸäº›é¢†åŸŸçš„é«˜åˆ†è¾¨ç‡æ ‡è®°è¡¨ç°ã€‚å®ƒç»“åˆäº†å®é™…çš„è½´çŠ¶T2ã€é«˜bå€¼æ‰©æ•£åºåˆ—å’Œè¡¨è§‚æ‰©æ•£ç³»æ•°å›¾çš„æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä¸”æˆåŠŸæ”¹å–„äº†åˆ†ç±»å™¨çš„æ€§èƒ½è¡¨ç°ã€‚CCELLA++æ¨¡å‹åœ¨ç”Ÿæˆåˆæˆæ•°æ®æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿæé«˜ä¸‹æ¸¸åˆ†ç±»å™¨çš„æ³›åŒ–èƒ½åŠ›å’Œæ€§èƒ½è¡¨ç°ã€‚æœªæ¥ç ”ç©¶åº”å…³æ³¨é‡åŒ–åŒ»å­¦å›¾åƒæ ·æœ¬è´¨é‡ï¼Œå¹³è¡¡å¤šåºåˆ—LDMè®­ç»ƒï¼Œå¹¶åœ¨ç»™å®šé¢å¤–ä¿¡æ¯çš„æƒ…å†µä¸‹è°ƒæ•´LDMã€‚CCELLA++æ¨¡å‹çš„ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šä¾›ç ”ç©¶ä½¿ç”¨ã€‚æ€»ç»“ä¸ºï¼šæ–°æ¨¡å‹CCELLA++èƒ½ç”ŸæˆåˆæˆåŒå‚æ•°MRIå›¾åƒï¼Œä¼˜äºçœŸå®æ•°æ®åœ¨ç‰¹å®šé¢†åŸŸé€‚åº”æ€§çš„è¡¨ç°ï¼Œå…·æœ‰ç¼“è§£åŒ»ç–—å½±åƒåˆ†æä¸­æ•°æ®ç¨€ç¼ºé—®é¢˜çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CCELLA++æ—¨åœ¨æ‰©å±•å’Œæ”¹è¿›åŸå§‹CCELLAæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸçš„é€‚ç”¨æ€§ï¼ŒåŒ…æ‹¬ç”ŸæˆåŒå‚æ•°MRIå›¾åƒã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡ç»“åˆå¤šç§MRIåºåˆ—ï¼ˆå¦‚è½´çŠ¶T2åŠ æƒåºåˆ—ã€é«˜bå€¼æ‰©æ•£ç³»åˆ—å’Œè¡¨è§‚æ‰©æ•£ç³»æ•°å›¾ï¼‰è¿›è¡Œè®­ç»ƒï¼Œæé«˜äº†åˆ†ç±»å™¨çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>CCELLA++é€šè¿‡ç”Ÿæˆåˆæˆæ•°æ®æé«˜äº†ä¸‹æ¸¸åˆ†ç±»å™¨çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°å‡ºè¶…è¶ŠçœŸå®æ•°æ®çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿé€‚åº”ä¸åŒåŒ»é™¢çš„æ•°æ®å˜åŒ–ï¼ˆå³é¢†åŸŸé€‚åº”æ€§ï¼‰ï¼Œå¹¶ä¸”åœ¨å°è§„æ¨¡å¤–éƒ¨æ•°æ®é›†ä¸Šä¹Ÿèƒ½å®ç°è¾ƒå¥½çš„åˆ†ç±»å™¨æ€§èƒ½è¡¨ç°ã€‚</li>
<li>ä¸åŸå§‹CCELLAæ¨¡å‹ç›¸æ¯”ï¼ŒCCELLA++åœ¨æŸäº›é¢†åŸŸï¼ˆå¦‚é«˜bå€¼æ‰©æ•£åºåˆ—å’Œè¡¨è§‚æ‰©æ•£ç³»æ•°å›¾ï¼‰çš„æ”¹è¿›æ•ˆæœæ›´ä¸ºæ˜¾è‘—ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06384">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-098329fb5878107e4a2dc83d7153a59a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77e5f2061814b5907559e7e881126956.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3abcb762ee9e314c4caeb4470c30e9c4.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Mamba-Goes-HoME-Hierarchical-Soft-Mixture-of-Experts-for-3D-Medical-Image-Segmentation"><a href="#Mamba-Goes-HoME-Hierarchical-Soft-Mixture-of-Experts-for-3D-Medical-Image-Segmentation" class="headerlink" title="Mamba Goes HoME: Hierarchical Soft Mixture-of-Experts for 3D Medical   Image Segmentation"></a>Mamba Goes HoME: Hierarchical Soft Mixture-of-Experts for 3D Medical   Image Segmentation</h2><p><strong>Authors:Szymon PÅ‚otka, Maciej Chrabaszcz, Gizem Mert, Ewa Szczurek, Arkadiusz Sitek</strong></p>
<p>In recent years, artificial intelligence has significantly advanced medical image segmentation. However, challenges remain, including efficient 3D medical image processing across diverse modalities and handling data variability. In this work, we introduce Hierarchical Soft Mixture-of-Experts (HoME), a two-level token-routing layer for efficient long-context modeling, specifically designed for 3D medical image segmentation. Built on the Mamba state-space model (SSM) backbone, HoME enhances sequential modeling through sparse, adaptive expert routing. The first stage employs a Soft Mixture-of-Experts (SMoE) layer to partition input sequences into local groups, routing tokens to specialized per-group experts for localized feature extraction. The second stage aggregates these outputs via a global SMoE layer, enabling cross-group information fusion and global context refinement. This hierarchical design, combining local expert routing with global expert refinement improves generalizability and segmentation performance, surpassing state-of-the-art results across datasets from the three most commonly used 3D medical imaging modalities and data quality. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œäººå·¥æ™ºèƒ½åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¤„ç†ä¸åŒæ¨¡å¼çš„é«˜æ•ˆ3DåŒ»å­¦å›¾åƒå’Œå¤„ç†æ•°æ®å˜åŒ–ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆ†å±‚è½¯æ··åˆä¸“å®¶ï¼ˆHoMEï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸¤çº§ä»¤ç‰Œè·¯ç”±å±‚ï¼Œç”¨äºé«˜æ•ˆçš„é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œä¸“ä¸º3DåŒ»å­¦å›¾åƒåˆ†å‰²è€Œè®¾è®¡ã€‚å»ºç«‹åœ¨MambaçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ä¸»å¹²ç½‘ç»œä¸Šï¼ŒHoMEé€šè¿‡ç¨€ç–ã€è‡ªé€‚åº”çš„ä¸“å®¶è·¯ç”±å¢å¼ºäº†åºåˆ—å»ºæ¨¡ã€‚ç¬¬ä¸€é˜¶æ®µé‡‡ç”¨è½¯æ··åˆä¸“å®¶ï¼ˆSMoEï¼‰å±‚å°†è¾“å…¥åºåˆ—åˆ†å‰²æˆå±€éƒ¨ç»„ï¼Œå°†ä»¤ç‰Œè·¯ç”±åˆ°é’ˆå¯¹æ¯ç»„çš„ä¸“å®¶è¿›è¡Œå±€éƒ¨ç‰¹å¾æå–ã€‚ç¬¬äºŒé˜¶æ®µé€šè¿‡å…¨å±€SMoEå±‚èšåˆè¿™äº›è¾“å‡ºï¼Œå®ç°è·¨ç»„ä¿¡æ¯èåˆå’Œå…¨å±€ä¸Šä¸‹æ–‡ç»†åŒ–ã€‚è¿™ç§ç»“åˆå±€éƒ¨ä¸“å®¶è·¯ç”±å’Œå…¨å±€ä¸“å®¶ç»†åŒ–çš„åˆ†å±‚è®¾è®¡æé«˜äº†é€šç”¨æ€§å’Œåˆ†å‰²æ€§èƒ½ï¼Œè¶…è¶Šäº†ä¸‰ç§æœ€å¸¸ç”¨çš„3DåŒ»å­¦æˆåƒæ¨¡å¼å’Œæ•°æ®è´¨é‡çš„æ•°æ®é›†ä¸Šçš„æœ€æ–°ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06363v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºäººå·¥æ™ºèƒ½çš„åŒ»å­¦å›¾åƒåˆ†å‰²æŠ€æœ¯çš„æœ€æ–°è¿›å±•ï¼Œå¹¶æå‡ºäº†Hierarchical Soft Mixture-of-Expertsï¼ˆHoMEï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨ä¸¤çº§ä»¤ç‰Œè·¯ç”±å±‚è¿›è¡Œé«˜æ•ˆçš„é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œä¸“é—¨ç”¨äº3DåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚HoMEæ¨¡å‹ç»“åˆå±€éƒ¨ä¸“å®¶è·¯ç”±å’Œå…¨å±€ä¸“å®¶ç»†åŒ–ï¼Œæé«˜äº†æ¨¡å‹çš„é€šç”¨æ€§å’Œåˆ†å‰²æ€§èƒ½ï¼Œåœ¨ä¸‰ç§æœ€å¸¸ç”¨çš„3DåŒ»å­¦æˆåƒæ¨¡æ€å’Œæ•°æ®è´¨é‡çš„å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜äºæœ€æ–°æŠ€æœ¯çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢çš„è¿›å±•ä¸æŒ‘æˆ˜ï¼šè™½ç„¶å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†ä»é¢ä¸´å¦‚è·¨ä¸åŒæ¨¡æ€çš„é«˜æ•ˆ3DåŒ»å­¦å›¾åƒå¤„ç†å’Œæ•°æ®å˜åŒ–å¤„ç†ç­‰å¤šæ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥HoMEæ¨¡å‹ï¼šè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸º3DåŒ»å­¦å›¾åƒåˆ†å‰²è®¾è®¡çš„ä¸¤é˜¶æ®µæ¨¡å‹ï¼Œé€šè¿‡ä¸¤çº§ä»¤ç‰Œè·¯ç”±å±‚è¿›è¡Œé«˜æ•ˆçš„é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚</li>
<li>HoMEæ¨¡å‹çš„ç¬¬ä¸€é˜¶æ®µï¼šé‡‡ç”¨Soft Mixture-of-Experts (SMoE)å±‚å¯¹è¾“å…¥åºåˆ—è¿›è¡Œå±€éƒ¨åˆ†ç»„ï¼Œå¹¶å¯¹ä»¤ç‰Œè¿›è¡Œè·¯ç”±ï¼Œä»¥è¿›è¡Œå±€éƒ¨ç‰¹å¾æå–ã€‚</li>
<li>HoMEæ¨¡å‹çš„ç¬¬äºŒé˜¶æ®µï¼šé€šè¿‡å…¨å±€SMoEå±‚èšåˆå±€éƒ¨ç‰¹å¾æå–çš„è¾“å‡ºï¼Œå®ç°è·¨ç»„ä¿¡æ¯èåˆå’Œå…¨å±€ä¸Šä¸‹æ–‡ç»†åŒ–ã€‚</li>
<li>åŸºäºMambaçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰çš„HoMEæ¨¡å‹ï¼šåˆ©ç”¨SSMä½œä¸ºä¸»å¹²ï¼Œå¢å¼ºäº†åºåˆ—å»ºæ¨¡èƒ½åŠ›ï¼Œå¹¶é€šè¿‡ç¨€ç–ã€è‡ªé€‚åº”çš„ä¸“å®¶è·¯ç”±æé«˜æ€§èƒ½ã€‚</li>
<li>HoMEæ¨¡å‹çš„ä¼˜ç‚¹ï¼šç»“åˆå±€éƒ¨ä¸“å®¶è·¯ç”±å’Œå…¨å±€ä¸“å®¶ç»†åŒ–ï¼Œæé«˜äº†æ¨¡å‹çš„é€šç”¨æ€§å’Œåˆ†å‰²æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06363">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f9541d50f97c96f43eb4bee96769ddd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f4ebecb70c35e36d2b5986c24e4100e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75a42f2257d5ac7e30233ac9e8869370.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bba2fb1c9a2026a8f2a34fcb96fd0815.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="RSRefSeg-2-Decoupling-Referring-Remote-Sensing-Image-Segmentation-with-Foundation-Models"><a href="#RSRefSeg-2-Decoupling-Referring-Remote-Sensing-Image-Segmentation-with-Foundation-Models" class="headerlink" title="RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with   Foundation Models"></a>RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with   Foundation Models</h2><p><strong>Authors:Keyan Chen, Chenyang Liu, Bowen Chen, Jiafan Zhang, Zhengxia Zou, Zhenwei Shi</strong></p>
<p>Referring Remote Sensing Image Segmentation provides a flexible and fine-grained framework for remote sensing scene analysis via vision-language collaborative interpretation. Current approaches predominantly utilize a three-stage pipeline encompassing dual-modal encoding, cross-modal interaction, and pixel decoding. These methods demonstrate significant limitations in managing complex semantic relationships and achieving precise cross-modal alignment, largely due to their coupled processing mechanism that conflates target localization with boundary delineation. This architectural coupling amplifies error propagation under semantic ambiguity while restricting model generalizability and interpretability. To address these issues, we propose RSRefSeg 2, a decoupling paradigm that reformulates the conventional workflow into a collaborative dual-stage framework: coarse localization followed by fine segmentation. RSRefSeg 2 integrates CLIPâ€™s cross-modal alignment strength with SAMâ€™s segmentation generalizability through strategic foundation model collaboration. Specifically, CLIP is employed as the dual-modal encoder to activate target features within its pre-aligned semantic space and generate localization prompts. To mitigate CLIPâ€™s misactivation challenges in multi-entity scenarios described by referring texts, a cascaded second-order prompter is devised, which enhances precision through implicit reasoning via decomposition of text embeddings into complementary semantic subspaces. These optimized semantic prompts subsequently direct the SAM to generate pixel-level refined masks, thereby completing the semantic transmission pipeline. Extensive experiments (RefSegRS, RRSIS-D, and RISBench) demonstrate that RSRefSeg 2 surpasses contemporary methods in segmentation accuracy (+~3% gIoU) and complex semantic interpretation. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/KyanChen/RSRefSeg2">https://github.com/KyanChen/RSRefSeg2</a>. </p>
<blockquote>
<p>å¼•ç”¨é¥æ„Ÿå›¾åƒåˆ†å‰²ï¼ˆRSRefSeg 2ï¼‰é€šè¿‡è§†è§‰è¯­è¨€ååŒè§£é‡Šæä¾›äº†ä¸€ç§çµæ´»ã€ç²¾ç»†çš„æ¡†æ¶è¿›è¡Œé¥æ„Ÿåœºæ™¯åˆ†æã€‚ç›®å‰ä¸»æµçš„æ–¹æ³•ä¸»è¦ä½¿ç”¨åŒ…å«åŒæ¨¡æ€ç¼–ç ã€è·¨æ¨¡æ€äº¤äº’å’Œåƒç´ è§£ç çš„ä¸‰é˜¶æ®µæµç¨‹ã€‚è¿™äº›æ–¹æ³•åœ¨å¤„ç†å¤æ‚çš„è¯­ä¹‰å…³ç³»å’Œå®ç°ç²¾ç¡®çš„è·¨æ¨¡æ€å¯¹é½æ–¹é¢å­˜åœ¨æ˜æ˜¾çš„å±€é™æ€§ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå…¶è€¦åˆå¤„ç†æœºåˆ¶æ··æ·†äº†ç›®æ ‡å®šä½å’Œè¾¹ç•Œåˆ’åˆ†ã€‚è¿™ç§æ¶æ„è€¦åˆåœ¨è¯­ä¹‰æ¨¡ç³Šçš„æƒ…å†µä¸‹æ”¾å¤§äº†è¯¯å·®ä¼ æ’­ï¼ŒåŒæ—¶é™åˆ¶äº†æ¨¡å‹çš„é€šç”¨æ€§å’Œå¯è§£é‡Šæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RSRefSeg 2ï¼Œè¿™æ˜¯ä¸€ç§è§£è€¦èŒƒå¼ï¼Œå®ƒå°†ä¼ ç»Ÿçš„å·¥ä½œæµç¨‹é‡æ–°è®¾è®¡ä¸ºååŒåŒé˜¶æ®µæ¡†æ¶ï¼šå…ˆè¿›è¡Œç²—ç•¥å®šä½ï¼Œç„¶åè¿›è¡Œç²¾ç»†åˆ†å‰²ã€‚RSRefSeg 2é€šè¿‡æˆ˜ç•¥åŸºç¡€æ¨¡å‹åä½œï¼Œå°†CLIPçš„è·¨æ¨¡æ€å¯¹é½èƒ½åŠ›ä¸SAMçš„åˆ†å‰²é€šç”¨æ€§ç›¸ç»“åˆã€‚å…·ä½“æ¥è¯´ï¼ŒCLIPè¢«ç”¨ä½œåŒæ¨¡æ€ç¼–ç å™¨ï¼Œç”¨äºæ¿€æ´»å…¶é¢„å¯¹é½è¯­ä¹‰ç©ºé—´ä¸­çš„ç›®æ ‡ç‰¹å¾å¹¶ç”Ÿæˆå®šä½æç¤ºã€‚ä¸ºäº†ç¼“è§£CLIPåœ¨å¤šå®ä½“åœºæ™¯ä¸­ç”±å¼•ç”¨æ–‡æœ¬æè¿°çš„è¯¯æ¿€æ´»æŒ‘æˆ˜ï¼Œè®¾è®¡äº†ä¸€ç§çº§è”çš„äºŒé˜¶æç¤ºå™¨ï¼Œå®ƒé€šè¿‡åˆ†è§£æ–‡æœ¬åµŒå…¥åˆ°äº’è¡¥è¯­ä¹‰å­ç©ºé—´æ¥å¢å¼ºç²¾åº¦ï¼Œå¹¶é€šè¿‡éšæ€§æ¨ç†å®ç°ç²¾å‡†æç¤ºã€‚è¿™äº›ä¼˜åŒ–çš„è¯­ä¹‰æç¤ºéšåæŒ‡å¯¼SAMç”Ÿæˆåƒç´ çº§ç»†åŒ–æ©ç ï¼Œä»è€Œå®Œæˆè¯­ä¹‰ä¼ è¾“ç®¡é“ã€‚å¹¿æ³›çš„å®éªŒï¼ˆRefSegRSã€RRSIS-Då’ŒRISBenchï¼‰è¡¨æ˜ï¼ŒRSRefSeg 2åœ¨åˆ†å‰²ç²¾åº¦ï¼ˆ+~3% gIoUï¼‰å’Œå¤æ‚è¯­ä¹‰è§£é‡Šæ–¹é¢è¶…è¶Šäº†å½“ä»£æ–¹æ³•ã€‚ç›¸å…³ä»£ç å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://github.com/KyanChen/RSRefSeg2%E3%80%82">https://github.com/KyanChen/RSRefSeg2ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06231v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¿œç¨‹é¥æ„Ÿå›¾åƒåˆ†å‰²çš„æ–°æ–¹æ³•RSRefSeg 2ã€‚è¯¥æ–¹æ³•é‡‡ç”¨è§£è€¦èŒƒå¼ï¼Œå°†ä¼ ç»Ÿå·¥ä½œæµç¨‹é‡æ„ä¸ºåä½œçš„åŒé˜¶æ®µæ¡†æ¶ï¼Œå³å…ˆè¿›è¡Œç²—ç•¥å®šä½ï¼Œå†è¿›è¡Œç²¾ç»†åˆ†å‰²ã€‚è¯¥æ–¹æ³•ç»“åˆäº†CLIPçš„è·¨æ¨¡æ€å¯¹é½ä¼˜åŠ¿ä¸SAMçš„åˆ†å‰²æ³›åŒ–èƒ½åŠ›ï¼Œé€šè¿‡æˆ˜ç•¥åŸºç¡€æ¨¡å‹åä½œï¼Œæé«˜äº†é¥æ„Ÿå›¾åƒåˆ†æçš„çµæ´»æ€§å’Œç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RSRefSeg 2æå‡ºäº†ä¸€ç§æ–°çš„é¥æ„Ÿå›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œé‡‡ç”¨è§£è€¦èŒƒå¼ï¼Œåˆ†ä¸ºç²—å®šä½ä¸ç²¾ç»†åˆ†å‰²ä¸¤ä¸ªé˜¶æ®µã€‚</li>
<li>æ–¹æ³•ç»“åˆäº†CLIPçš„è·¨æ¨¡æ€å¯¹é½ä¼˜åŠ¿ï¼Œæœ‰æ•ˆç®¡ç†å¤æ‚è¯­ä¹‰å…³ç³»ã€‚</li>
<li>é€šè¿‡æˆ˜ç•¥åŸºç¡€æ¨¡å‹åä½œï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œè§£é‡Šæ€§ã€‚</li>
<li>é‡‡ç”¨äº†SAMè¿›è¡Œåƒç´ çº§ç²¾ç»†æ©è†œç”Ÿæˆï¼Œå®Œæˆäº†è¯­ä¹‰ä¼ è¾“ç®¡é“ã€‚</li>
<li>RSRefSeg 2åœ¨åˆ†å‰²ç²¾åº¦ä¸Šè¶…è¶Šäº†å½“ä»£æ–¹æ³•ï¼Œè¾¾åˆ°äº†çº¦3%çš„gIoUæå‡ã€‚</li>
<li>æ–¹æ³•åœ¨RefSegRSã€RRSIS-Då’ŒRISBenchç­‰å¤šä¸ªå®éªŒä¸­å¾—åˆ°äº†éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06231">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a048863fee9910406758d187394d13a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-237add32f3d735c6eb5cbf28396eda57.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-779b0681be06c106ed4f914cbdab2624.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LangMamba-A-Language-driven-Mamba-Framework-for-Low-dose-CT-Denoising-with-Vision-language-Models"><a href="#LangMamba-A-Language-driven-Mamba-Framework-for-Low-dose-CT-Denoising-with-Vision-language-Models" class="headerlink" title="LangMamba: A Language-driven Mamba Framework for Low-dose CT Denoising   with Vision-language Models"></a>LangMamba: A Language-driven Mamba Framework for Low-dose CT Denoising   with Vision-language Models</h2><p><strong>Authors:Zhihao Chen, Tao Chen, Chenhui Wang, Qi Gao, Huidong Xie, Chuang Niu, Ge Wang, Hongming Shan</strong></p>
<p>Low-dose computed tomography (LDCT) reduces radiation exposure but often degrades image quality, potentially compromising diagnostic accuracy. Existing deep learning-based denoising methods focus primarily on pixel-level mappings, overlooking the potential benefits of high-level semantic guidance. Recent advances in vision-language models (VLMs) suggest that language can serve as a powerful tool for capturing structured semantic information, offering new opportunities to improve LDCT reconstruction. In this paper, we introduce LangMamba, a Language-driven Mamba framework for LDCT denoising that leverages VLM-derived representations to enhance supervision from normal-dose CT (NDCT). LangMamba follows a two-stage learning strategy. First, we pre-train a Language-guided AutoEncoder (LangAE) that leverages frozen VLMs to map NDCT images into a semantic space enriched with anatomical information. Second, we synergize LangAE with two key components to guide LDCT denoising: Semantic-Enhanced Efficient Denoiser (SEED), which enhances NDCT-relevant local semantic while capturing global features with efficient Mamba mechanism, and Language-engaged Dual-space Alignment (LangDA) Loss, which ensures that denoised images align with NDCT in both perceptual and semantic spaces. Extensive experiments on two public datasets demonstrate that LangMamba outperforms conventional state-of-the-art methods, significantly improving detail preservation and visual fidelity. Remarkably, LangAE exhibits strong generalizability to unseen datasets, thereby reducing training costs. Furthermore, LangDA loss improves explainability by integrating language-guided insights into image reconstruction and offers a plug-and-play fashion. Our findings shed new light on the potential of language as a supervisory signal to advance LDCT denoising. The code is publicly available on <a target="_blank" rel="noopener" href="https://github.com/hao1635/LangMamba">https://github.com/hao1635/LangMamba</a>. </p>
<blockquote>
<p>ä½å‰‚é‡è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆLDCTï¼‰é™ä½äº†è¾å°„æš´éœ²ï¼Œä½†é€šå¸¸ä¼šé™ä½å›¾åƒè´¨é‡ï¼Œä»è€Œå¯èƒ½å½±å“åˆ°è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚ç°æœ‰çš„åŸºäºæ·±åº¦å­¦ä¹ çš„å»å™ªæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨åƒç´ çº§æ˜ å°„ä¸Šï¼Œå¿½ç•¥äº†é«˜çº§è¯­ä¹‰æŒ‡å¯¼çš„æ½œåœ¨ä¼˜åŠ¿ã€‚æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è¿›æ­¥è¡¨æ˜ï¼Œè¯­è¨€å¯ä»¥ä½œä¸ºæ•è·ç»“æ„åŒ–è¯­ä¹‰ä¿¡æ¯çš„å¼ºå¤§å·¥å…·ï¼Œä¸ºæ”¹å–„LDCTé‡å»ºæä¾›äº†æ–°çš„æœºä¼šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†LangMambaï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºLDCTå»å™ªçš„è¯­è¨€é©±åŠ¨Mambaæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨VLMæ´¾ç”Ÿè¡¨ç¤ºæ¥å¢å¼ºæ­£å¸¸å‰‚é‡CTï¼ˆNDCTï¼‰çš„ç›‘ç£ã€‚LangMambaé‡‡ç”¨ä¸¤é˜¶æ®µå­¦ä¹ ç­–ç•¥ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é¢„è®­ç»ƒä¸€ä¸ªè¯­è¨€å¼•å¯¼è‡ªåŠ¨ç¼–ç å™¨ï¼ˆLangAEï¼‰ï¼Œåˆ©ç”¨å†»ç»“çš„VLMå°†NDCTå›¾åƒæ˜ å°„åˆ°ä¸€ä¸ªä¸°å¯Œçš„è¯­ä¹‰ç©ºé—´ï¼Œå…¶ä¸­åŒ…å«è§£å‰–ä¿¡æ¯ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†LangAEä¸ä¸¤ä¸ªå…³é”®ç»„ä»¶ç›¸ç»“åˆï¼Œä»¥æŒ‡å¯¼LDCTå»å™ªï¼šè¯­ä¹‰å¢å¼ºé«˜æ•ˆå»å™ªå™¨ï¼ˆSEEDï¼‰ï¼Œå®ƒåœ¨æ•æ‰å…¨å±€ç‰¹å¾çš„åŒæ—¶ï¼Œå¢å¼ºäº†ä¸NDCTç›¸å…³çš„å±€éƒ¨è¯­ä¹‰ï¼Œå¹¶ä½¿ç”¨é«˜æ•ˆçš„Mambaæœºåˆ¶ï¼›ä»¥åŠè¯­è¨€å‚ä¸çš„åŒç©ºé—´å¯¹é½ï¼ˆLangDAï¼‰æŸå¤±ï¼Œå®ƒç¡®ä¿å»å™ªå›¾åƒåœ¨æ„ŸçŸ¥å’Œè¯­ä¹‰ç©ºé—´ä¸Šä¸NDCTå¯¹é½ã€‚åœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLangMambaä¼˜äºä¼ ç»Ÿçš„ä¸»æµæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†ç»†èŠ‚ä¿ç•™å’Œè§†è§‰ä¿çœŸåº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒLangAEå¯¹æœªè§æ•°æ®é›†è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œé™ä½äº†è®­ç»ƒæˆæœ¬ã€‚æ­¤å¤–ï¼ŒLangDAæŸå¤±é€šè¿‡æ•´åˆè¯­è¨€å¼•å¯¼æ´å¯Ÿåˆ°å›¾åƒé‡å»ºä¸­ï¼Œæé«˜äº†å¯è§£é‡Šæ€§ï¼Œå¹¶ä»¥å³æ’å³ç”¨æ–¹å¼å‘ˆç°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºè¯­è¨€ä½œä¸ºç›‘ç£ä¿¡å·åœ¨LDCTå»å™ªä¸­çš„æ½œåŠ›å¸¦æ¥äº†æ–°çš„å¯ç¤ºã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/hao1635/LangMamba">https://github.com/hao1635/LangMamba</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06140v1">PDF</a> 11 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºè¯­è¨€çš„Mambaæ¡†æ¶LangMambaï¼Œç”¨äºä½å‰‚é‡è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆLDCTï¼‰å»å™ªã€‚è¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è¡¨ç¤ºä¸ºæ­£å¸¸å‰‚é‡CTï¼ˆNDCTï¼‰å¢å¼ºç›‘ç£ä¿¡æ¯ã€‚LangMambaé‡‡ç”¨ä¸¤é˜¶æ®µå­¦ä¹ ç­–ç•¥ï¼Œé¦–å…ˆé¢„è®­ç»ƒè¯­è¨€å¼•å¯¼è‡ªåŠ¨ç¼–ç å™¨ï¼ˆLangAEï¼‰å°†NDCTå›¾åƒæ˜ å°„åˆ°ä¸°å¯Œçš„è¯­ä¹‰ç©ºé—´ï¼Œç„¶åç»“åˆä¸¤ä¸ªå…³é”®ç»„ä»¶è¿›è¡ŒLDCTå»å™ªï¼ŒåŒ…æ‹¬è¯­ä¹‰å¢å¼ºé«˜æ•ˆå»å™ªå™¨å’Œè¯­è¨€å‚ä¸åŒç©ºé—´å¯¹é½æŸå¤±ã€‚å®éªŒè¡¨æ˜ï¼ŒLangMambaåœ¨ç»†èŠ‚ä¿ç•™å’Œè§†è§‰ä¿çœŸåº¦æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä¸”LangAEå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œé™ä½äº†è®­ç»ƒæˆæœ¬ã€‚æ­¤å¤–ï¼ŒLangDAæŸå¤±é€šè¿‡æ•´åˆè¯­è¨€æŒ‡å¯¼çš„è§è§£æ¥æé«˜äº†è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LDCTå‡å°‘è¾å°„æš´éœ²ä½†å¯èƒ½é™ä½å›¾åƒè´¨é‡ï¼Œå½±å“è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>ç°æœ‰æ·±åº¦å­¦ä¹ å»å™ªæ–¹æ³•ä¸»è¦å…³æ³¨åƒç´ çº§æ˜ å°„ï¼Œå¿½ç•¥äº†é«˜çº§è¯­ä¹‰æŒ‡å¯¼çš„æ½œåœ¨ä¼˜åŠ¿ã€‚</li>
<li>LangMambaåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è¡¨ç¤ºä¸ºNDCTå¢å¼ºç›‘ç£ä¿¡æ¯ã€‚</li>
<li>LangMambaé‡‡ç”¨ä¸¤é˜¶æ®µå­¦ä¹ ç­–ç•¥ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒLangAEå’Œç»“åˆä¸¤ä¸ªå…³é”®ç»„ä»¶è¿›è¡ŒLDCTå»å™ªã€‚</li>
<li>LangMambaåœ¨ç»†èŠ‚ä¿ç•™å’Œè§†è§‰ä¿çœŸåº¦æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>LangAEå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œé™ä½äº†è®­ç»ƒæˆæœ¬ã€‚</li>
<li>LangDAæŸå¤±é€šè¿‡æ•´åˆè¯­è¨€æŒ‡å¯¼çš„è§è§£æé«˜äº†æ¨¡å‹è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fed929a1fe55de201d3d0bc6ffff6576.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb778d18f71362883ad50f4f27df5741.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5f122bb8a03464d362202fc0ea2644b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ba671915819286fef8e1a06d9363e17.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-153f4abeaa212f50fd3272cbf1c8c52c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Just-Say-Better-or-Worse-A-Human-AI-Collaborative-Framework-for-Medical-Image-Segmentation-Without-Manual-Annotations"><a href="#Just-Say-Better-or-Worse-A-Human-AI-Collaborative-Framework-for-Medical-Image-Segmentation-Without-Manual-Annotations" class="headerlink" title="Just Say Better or Worse: A Human-AI Collaborative Framework for Medical   Image Segmentation Without Manual Annotations"></a>Just Say Better or Worse: A Human-AI Collaborative Framework for Medical   Image Segmentation Without Manual Annotations</h2><p><strong>Authors:Yizhe Zhang</strong></p>
<p>Manual annotation of medical images is a labor-intensive and time-consuming process, posing a significant bottleneck in the development and deployment of robust medical imaging AI systems. This paper introduces a novel Human-AI collaborative framework for medical image segmentation that substantially reduces the annotation burden by eliminating the need for explicit manual pixel-level labeling. The core innovation lies in a preference learning paradigm, where human experts provide minimal, intuitive feedback â€“ simply indicating whether an AI-generated segmentation is better or worse than a previous version. The framework comprises four key components: (1) an adaptable foundation model (FM) for feature extraction, (2) label propagation based on feature similarity, (3) a clicking agent that learns from human better-or-worse feedback to decide where to click and with which label, and (4) a multi-round segmentation learning procedure that trains a state-of-the-art segmentation network using pseudo-labels generated by the clicking agent and FM-based label propagation. Experiments on three public datasets demonstrate that the proposed approach achieves competitive segmentation performance using only binary preference feedback, without requiring experts to directly manually annotate the images. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒçš„æ‰‹åŠ¨æ ‡æ³¨æ˜¯ä¸€ä¸ªåŠ³åŠ¨å¯†é›†ä¸”è€—æ—¶çš„è¿‡ç¨‹ï¼Œè¿™æˆä¸ºäº†å¼€å‘å’Œåº”ç”¨ç¨³å¥çš„åŒ»å­¦æˆåƒäººå·¥æ™ºèƒ½ç³»ç»Ÿçš„é‡è¦ç“¶é¢ˆã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°å‹äººæœºåä½œæ¡†æ¶ï¼Œé€šè¿‡æ¶ˆé™¤å¯¹æ˜ç¡®æ‰‹åŠ¨åƒç´ çº§æ ‡ç­¾çš„éœ€æ±‚ï¼Œæå¤§åœ°å‡è½»äº†æ ‡æ³¨è´Ÿæ‹…ã€‚æ ¸å¿ƒåˆ›æ–°ç‚¹åœ¨äºä¸€ç§åå¥½å­¦ä¹ èŒƒå¼ï¼Œäººç±»ä¸“å®¶æä¾›æœ€å°åŒ–çš„ã€ç›´è§‚çš„åé¦ˆï¼Œåªéœ€æŒ‡ç¤ºAIç”Ÿæˆçš„åˆ†å‰²ç»“æœæ˜¯å¦æ¯”ä¹‹å‰ç‰ˆæœ¬æ›´å¥½æˆ–æ›´å·®ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬å››ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰ç”¨äºç‰¹å¾æå–çš„å¯é€‚åº”åŸºç¡€æ¨¡å‹ï¼ˆFMï¼‰ï¼Œï¼ˆ2ï¼‰åŸºäºç‰¹å¾ç›¸ä¼¼æ€§çš„æ ‡ç­¾ä¼ æ’­ï¼Œï¼ˆ3ï¼‰ç‚¹å‡»ä»£ç†ï¼Œå®ƒä»äººç±»æ›´å¥½æˆ–æ›´å·®çš„åé¦ˆä¸­å­¦ä¹ ï¼Œä»¥å†³å®šç‚¹å‡»å“ªé‡Œä»¥åŠä½¿ç”¨å“ªä¸ªæ ‡ç­¾ï¼Œï¼ˆ4ï¼‰å¤šè½®åˆ†å‰²å­¦ä¹ ç¨‹åºï¼Œè¯¥ç¨‹åºä½¿ç”¨ç‚¹å‡»ä»£ç†å’ŒåŸºäºFMçš„æ ‡ç­¾ä¼ æ’­ç”Ÿæˆçš„ä¼ªæ ‡ç­¾æ¥è®­ç»ƒæœ€å…ˆè¿›çš„åˆ†å‰²ç½‘ç»œã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä»…ä½¿ç”¨äºŒè¿›åˆ¶åå¥½åé¦ˆå°±å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„åˆ†å‰²æ€§èƒ½ï¼Œæ— éœ€ä¸“å®¶ç›´æ¥æ‰‹åŠ¨æ ‡æ³¨å›¾åƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05815v1">PDF</a> 10 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹äººæœºåä½œæ¡†æ¶ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œé€šè¿‡æ¶ˆé™¤å¯¹æ˜ç¡®æ‰‹åŠ¨åƒç´ çº§æ ‡ç­¾çš„éœ€æ±‚ï¼Œæ˜¾è‘—å‡å°‘äº†æ ‡æ³¨è´Ÿæ‹…ã€‚æ ¸å¿ƒåˆ›æ–°ç‚¹åœ¨äºé‡‡ç”¨åå¥½å­¦ä¹ æ¨¡å¼ï¼Œäººç±»ä¸“å®¶åªéœ€æä¾›æœ€å°åŒ–çš„ç›´è§‰åé¦ˆï¼Œå³å¯åˆ¤æ–­äººå·¥æ™ºèƒ½ç”Ÿæˆçš„åˆ†å‰²ç»“æœç›¸è¾ƒäºä¹‹å‰ç‰ˆæœ¬æ˜¯å¥½è¿˜æ˜¯åã€‚è¯¥æ¡†æ¶åŒ…æ‹¬å››ä¸ªå…³é”®ç»„ä»¶ï¼šå¯é€‚åº”çš„åŸºç¡€æ¨¡å‹ï¼ˆFMï¼‰è¿›è¡Œç‰¹å¾æå–ã€åŸºäºç‰¹å¾ç›¸ä¼¼æ€§çš„æ ‡ç­¾ä¼ æ’­ã€ç‚¹å‡»ä»£ç†ï¼ˆå­¦ä¹ äººç±»çš„æ›´å¥½æˆ–æ›´å·®åé¦ˆæ¥å†³å®šç‚¹å‡»ä½ç½®å’Œæ ‡ç­¾ï¼‰ã€ä»¥åŠä½¿ç”¨ç‚¹å‡»ä»£ç†å’ŒFMåŸºç¡€æ ‡ç­¾ä¼ æ’­ç”Ÿæˆçš„å¤šè½®åˆ†å‰²å­¦ä¹ ç¨‹åºæ¥è®­ç»ƒæœ€æ–°åˆ†å‰²ç½‘ç»œã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨äºŒå…ƒåå¥½åé¦ˆå³å¯å®ç°æœ‰ç«äº‰åŠ›çš„åˆ†å‰²æ€§èƒ½ï¼Œæ— éœ€ä¸“å®¶ç›´æ¥æ‰‹åŠ¨æ ‡æ³¨å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒçš„æ‰‹åŠ¨æ ‡æ³¨æ˜¯åŠ³åŠ¨å¯†é›†å’Œæ—¶é—´æ¶ˆè€—çš„è¿‡ç¨‹ï¼Œé™åˆ¶äº†åŒ»å­¦å½±åƒAIç³»ç»Ÿçš„å‘å±•å’Œéƒ¨ç½²ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹äººæœºåä½œæ¡†æ¶ï¼Œæ—¨åœ¨å¤§å¹…å‡å°‘åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ ‡æ³¨è´Ÿæ‹…ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨åå¥½å­¦ä¹ æ¨¡å¼ï¼Œä»…éœ€è¦ä¸“å®¶çš„æœ€å°åŒ–ç›´è§‰åé¦ˆã€‚</li>
<li>æ¡†æ¶åŒ…å«å››ä¸ªå…³é”®ç»„ä»¶ï¼šå¯é€‚åº”çš„åŸºç¡€æ¨¡å‹ã€æ ‡ç­¾ä¼ æ’­ã€ç‚¹å‡»ä»£ç†å’Œå¤šè½®åˆ†å‰²å­¦ä¹ ç¨‹åºã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä»…ä½¿ç”¨äºŒå…ƒåå¥½åé¦ˆçš„æƒ…å†µä¸‹ï¼Œå®ç°äº†æœ‰ç«äº‰åŠ›çš„åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€ä¸“å®¶ç›´æ¥æ‰‹åŠ¨æ ‡æ³¨å›¾åƒï¼Œé™ä½äº†åŠ³åŠ¨å¼ºåº¦å’Œæ—¶é—´æˆæœ¬ã€‚</li>
<li>æ­¤æ¡†æ¶å¯¹æœªæ¥åŒ»å­¦å›¾åƒåˆ†å‰²å’Œäººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨å…·æœ‰æ½œåœ¨çš„ç§¯æå½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05815">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-31569ceb656665e76aec202113f1e9b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb78b951f2c1f99b1f5703d8106c5d9a.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="From-Motion-to-Meaning-Biomechanics-Informed-Neural-Network-for-Explainable-Cardiovascular-Disease-Identification"><a href="#From-Motion-to-Meaning-Biomechanics-Informed-Neural-Network-for-Explainable-Cardiovascular-Disease-Identification" class="headerlink" title="From Motion to Meaning: Biomechanics-Informed Neural Network for   Explainable Cardiovascular Disease Identification"></a>From Motion to Meaning: Biomechanics-Informed Neural Network for   Explainable Cardiovascular Disease Identification</h2><p><strong>Authors:Comte Valentin, Gemma Piella, Mario Ceresa, Miguel A. Gonzalez Ballester</strong></p>
<p>Cardiac diseases are among the leading causes of morbidity and mortality worldwide, which requires accurate and timely diagnostic strategies. In this study, we introduce an innovative approach that combines deep learning image registration with physics-informed regularization to predict the biomechanical properties of moving cardiac tissues and extract features for disease classification. We utilize the energy strain formulation of Neo-Hookean material to model cardiac tissue deformations, optimizing the deformation field while ensuring its physical and biomechanical coherence. This explainable approach not only improves image registration accuracy, but also provides insights into the underlying biomechanical processes of the cardiac tissues. Evaluation on the Automated Cardiac Diagnosis Challenge (ACDC) dataset achieved Dice scores of 0.945 for the left ventricular cavity, 0.908 for the right ventricular cavity, and 0.905 for the myocardium. Subsequently, we estimate the local strains within the moving heart and extract a detailed set of features used for cardiovascular disease classification. We evaluated five classification algorithms, Logistic Regression, Multi-Layer Perceptron, Support Vector Classifier, Random Forest, and Nearest Neighbour, and identified the most relevant features using a feature selection algorithm. The best performing classifier obtained a classification accuracy of 98% in the training set and 100% in the test set of the ACDC dataset. By integrating explainable artificial intelligence, this method empowers clinicians with a transparent understanding of the modelâ€™s predictions based on cardiac mechanics, while also significantly improving the accuracy and reliability of cardiac disease diagnosis, paving the way for more personalized and effective patient care. </p>
<blockquote>
<p>å¿ƒè„ç—…æ˜¯å…¨çƒå‘ç—…ç‡å’Œæ­»äº¡ç‡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œéœ€è¦å‡†ç¡®åŠæ—¶çš„è¯Šæ–­ç­–ç•¥ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„æ–¹æ³•ï¼Œç»“åˆæ·±åº¦å­¦ä¹ å›¾åƒé…å‡†å’Œç‰©ç†ä¿¡æ¯æ­£åˆ™åŒ–ï¼Œé¢„æµ‹å¿ƒè„ç»„ç»‡è¿åŠ¨çš„ç”Ÿç‰©åŠ›å­¦ç‰¹æ€§ï¼Œå¹¶æå–ç–¾ç—…åˆ†ç±»çš„ç‰¹å¾ã€‚æˆ‘ä»¬ä½¿ç”¨Neo-Hookeanææ–™çš„èƒ½é‡åº”å˜å…¬å¼æ¥æ¨¡æ‹Ÿå¿ƒè„ç»„ç»‡å˜å½¢ï¼Œä¼˜åŒ–å˜å½¢åœºçš„åŒæ—¶ç¡®ä¿å…¶ç‰©ç†å’Œç”Ÿç‰©åŠ›å­¦çš„ä¸€è‡´æ€§ã€‚è¿™ç§å¯è§£é‡Šçš„æ–¹æ³•ä¸ä»…æé«˜äº†å›¾åƒé…å‡†çš„å‡†ç¡®æ€§ï¼Œè€Œä¸”æ·±å…¥äº†è§£äº†å¿ƒè„ç»„ç»‡çš„æ½œåœ¨ç”Ÿç‰©åŠ›å­¦è¿‡ç¨‹ã€‚åœ¨è‡ªåŠ¨å¿ƒè„è¯Šæ–­æŒ‘æˆ˜ï¼ˆACDCï¼‰æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå·¦å¿ƒå®¤è…”çš„Diceå¾—åˆ†ä¸º0.945ï¼Œå³å¿ƒå®¤è…”ä¸º0.908ï¼Œå¿ƒè‚Œä¸º0.905ã€‚éšåï¼Œæˆ‘ä»¬ä¼°è®¡äº†å¿ƒè„è¿åŠ¨è¿‡ç¨‹ä¸­çš„å±€éƒ¨åº”å˜ï¼Œå¹¶æå–äº†ä¸€ç»„è¯¦ç»†ç‰¹å¾ç”¨äºå¿ƒè¡€ç®¡ç–¾ç—…åˆ†ç±»ã€‚æˆ‘ä»¬è¯„ä¼°äº†äº”ç§åˆ†ç±»ç®—æ³•ï¼ŒåŒ…æ‹¬é€»è¾‘å›å½’ã€å¤šå±‚æ„ŸçŸ¥å™¨ã€æ”¯æŒå‘é‡åˆ†ç±»å™¨ã€éšæœºæ£®æ—å’Œæœ€è¿‘é‚»ç®—æ³•ï¼Œå¹¶ä½¿ç”¨ç‰¹å¾é€‰æ‹©ç®—æ³•ç¡®å®šäº†æœ€ç›¸å…³çš„ç‰¹å¾ã€‚è¡¨ç°æœ€ä½³çš„åˆ†ç±»å™¨åœ¨ACDCæ•°æ®é›†çš„è®­ç»ƒé›†ä¸Šè¾¾åˆ°äº†98%çš„åˆ†ç±»ç²¾åº¦ï¼Œåœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†100%çš„åˆ†ç±»ç²¾åº¦ã€‚é€šè¿‡é›†æˆå¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼Œè¯¥æ–¹æ³•ä½¿ä¸´åºŠåŒ»ç”Ÿèƒ½å¤ŸåŸºäºå¿ƒè„åŠ›å­¦å¯¹æ¨¡å‹çš„é¢„æµ‹è¿›è¡Œé€æ˜ç†è§£ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜å¿ƒè„ç—…è¯Šæ–­çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œä¸ºæ›´ä¸ªæ€§åŒ–å’Œæœ‰æ•ˆçš„æ‚£è€…æŠ¤ç†é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05783v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶ç»“åˆæ·±åº¦å­¦ä¹ å›¾åƒé…å‡†ä¸ç‰©ç†ä¿¡æ¯æ­£åˆ™åŒ–ï¼Œé¢„æµ‹å¿ƒè„ç»„ç»‡åŠ¨æ€çš„ç”Ÿç‰©åŠ›å­¦ç‰¹æ€§ï¼Œå¹¶æå–ç‰¹å¾è¿›è¡Œç–¾ç—…åˆ†ç±»ã€‚é‡‡ç”¨Neo-Hookeanææ–™çš„èƒ½é‡åº”å˜å…¬å¼å¯¹å¿ƒè„ç»„ç»‡å˜å½¢è¿›è¡Œå»ºæ¨¡ï¼Œä¼˜åŒ–å˜å½¢åœºï¼Œç¡®ä¿ç‰©ç†å’Œç”Ÿç‰©åŠ›å­¦çš„ä¸€è‡´æ€§ã€‚æ­¤æ–¹æ³•ä¸ä»…æé«˜äº†å›¾åƒé…å‡†çš„å‡†ç¡®åº¦ï¼Œè¿˜ä¸ºå¿ƒè„ç»„ç»‡çš„ç”Ÿç‰©åŠ›å­¦è¿‡ç¨‹æä¾›äº†æ·±å…¥è§è§£ã€‚åœ¨è‡ªåŠ¨åŒ–å¿ƒè„è¯Šæ–­æŒ‘æˆ˜ï¼ˆACDCï¼‰æ•°æ®é›†ä¸Šï¼Œå¯¹å·¦å¿ƒå®¤è…”ã€å³å¿ƒå®¤è…”å’Œå¿ƒè‚Œçš„Diceè¯„åˆ†åˆ†åˆ«è¾¾åˆ°äº†0.945ã€0.908å’Œ0.905ã€‚ç„¶åä¼°è®¡å¿ƒè„å±€éƒ¨åº”å˜ï¼Œæå–è¯¦ç»†ç‰¹å¾ç”¨äºå¿ƒè¡€ç®¡ç–¾ç—…åˆ†ç±»ã€‚è¯„ä¼°äº†äº”ç§åˆ†ç±»ç®—æ³•ï¼Œå¹¶å€ŸåŠ©ç‰¹å¾é€‰æ‹©ç®—æ³•ç¡®å®šäº†æœ€ç›¸å…³çš„ç‰¹å¾ã€‚æœ€ä½³åˆ†ç±»å™¨åœ¨ACDCæ•°æ®é›†çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„åˆ†ç±»å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°äº†98%å’Œ100%ã€‚é€šè¿‡ç»“åˆå¯è§£é‡Šçš„äººå·¥æ™ºèƒ½ï¼Œè¯¥æ–¹æ³•ä½¿ä¸´åºŠåŒ»ç”Ÿèƒ½å¤Ÿé€æ˜åœ°äº†è§£æ¨¡å‹åŸºäºå¿ƒè„æœºç†çš„é¢„æµ‹ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜å¿ƒè„ç–¾ç—…è¯Šæ–­çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œä¸ºæ›´ä¸ªæ€§åŒ–å’Œæœ‰æ•ˆçš„æ‚£è€…æŠ¤ç†é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶ç»“åˆäº†æ·±åº¦å­¦ä¹ å›¾åƒé…å‡†ä¸ç‰©ç†ä¿¡æ¯æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œä¸ºå¿ƒè„ç–¾ç—…çš„è¯Šæ–­æä¾›äº†æ–°çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡Neo-Hookeanææ–™çš„èƒ½é‡åº”å˜å…¬å¼å»ºæ¨¡å¿ƒè„ç»„ç»‡å˜å½¢ï¼Œæå‡äº†å›¾åƒé…å‡†çš„å‡†ç¡®æ€§ï¼Œå¹¶æ­ç¤ºäº†å¿ƒè„ç»„ç»‡çš„ç”Ÿç‰©åŠ›å­¦è¿‡ç¨‹ã€‚</li>
<li>åœ¨ACDCæ•°æ®é›†ä¸Šå–å¾—äº†è¾ƒé«˜çš„å›¾åƒé…å‡†æ•ˆæœï¼ŒDiceè¯„åˆ†è¾¾åˆ°90%ä»¥ä¸Šã€‚</li>
<li>é€šè¿‡ä¼°è®¡å¿ƒè„å±€éƒ¨åº”å˜ï¼Œæå–äº†è¯¦ç»†ç‰¹å¾ç”¨äºå¿ƒè¡€ç®¡ç–¾ç—…åˆ†ç±»ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†å¤šç§åˆ†ç±»ç®—æ³•ï¼Œå¹¶æ‰¾åˆ°äº†æœ€ç›¸å…³çš„ç‰¹å¾ç”¨äºç–¾ç—…åˆ†ç±»ã€‚</li>
<li>æœ€ä½³åˆ†ç±»å™¨çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„åˆ†ç±»å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°äº†98%å’Œ100%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05783">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-00d5faf0d33a19ce5902652d8807345f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d10a107eb96a130f86815bba84998a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4757e9fa2b955c599b01d8620ad01e67.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Learning-Segmentation-from-Radiology-Reports"><a href="#Learning-Segmentation-from-Radiology-Reports" class="headerlink" title="Learning Segmentation from Radiology Reports"></a>Learning Segmentation from Radiology Reports</h2><p><strong>Authors:Pedro R. A. S. Bassi, Wenxuan Li, Jieneng Chen, Zheren Zhu, Tianyu Lin, Sergio Decherchi, Andrea Cavalli, Kang Wang, Yang Yang, Alan L. Yuille, Zongwei Zhou</strong></p>
<p>Tumor segmentation in CT scans is key for diagnosis, surgery, and prognosis, yet segmentation masks are scarce because their creation requires time and expertise. Public abdominal CT datasets have from dozens to a couple thousand tumor masks, but hospitals have hundreds of thousands of tumor CTs with radiology reports. Thus, leveraging reports to improve segmentation is key for scaling. In this paper, we propose a report-supervision loss (R-Super) that converts radiology reports into voxel-wise supervision for tumor segmentation AI. We created a dataset with 6,718 CT-Report pairs (from the UCSF Hospital), and merged it with public CT-Mask datasets (from AbdomenAtlas 2.0). We used our R-Super to train with these masks and reports, and strongly improved tumor segmentation in internal and external validationâ€“F1 Score increased by up to 16% with respect to training with masks only. By leveraging readily available radiology reports to supplement scarce segmentation masks, R-Super strongly improves AI performance both when very few training masks are available (e.g., 50), and when many masks were available (e.g., 1.7K).   Project: <a target="_blank" rel="noopener" href="https://github.com/MrGiovanni/R-Super">https://github.com/MrGiovanni/R-Super</a> </p>
<blockquote>
<p>è‚¿ç˜¤åœ¨CTæ‰«æä¸­çš„åˆ†å‰²å¯¹äºè¯Šæ–­ã€æ‰‹æœ¯å’Œé¢„åè‡³å…³é‡è¦ï¼Œä½†ç”±äºå…¶åˆ›å»ºéœ€è¦æ—¶é—´å’Œä¸“ä¸šçŸ¥è¯†ï¼Œåˆ†å‰²æ©è†œéå¸¸ç¨€ç¼ºã€‚å…¬å…±è…¹éƒ¨CTæ•°æ®é›†æœ‰æ•°ååˆ°æ•°åƒä¸ªè‚¿ç˜¤æ©è†œï¼Œä½†åŒ»é™¢æœ‰æ•°åä¸‡ä¸ªå¸¦æœ‰æ”¾å°„æŠ¥å‘Šçš„è‚¿ç˜¤CTã€‚å› æ­¤ï¼Œåˆ©ç”¨æŠ¥å‘Šæ¥æ”¹å–„åˆ†å‰²æ˜¯æ‰©å±•çš„å…³é”®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æŠ¥å‘Šç›‘ç£æŸå¤±ï¼ˆR-Superï¼‰ï¼Œå®ƒå°†æ”¾å°„å­¦æŠ¥å‘Šè½¬åŒ–ä¸ºä½“ç´ çº§çš„ç›‘ç£ï¼Œç”¨äºè‚¿ç˜¤åˆ†å‰²äººå·¥æ™ºèƒ½ã€‚æˆ‘ä»¬åˆ›å»ºäº†åŒ…å«6718ä¸ªCTæŠ¥å‘Šå¯¹çš„æ•°æ®é›†ï¼ˆæ¥è‡ªUCSFåŒ»é™¢ï¼‰ï¼Œå¹¶ä¸å…¬å…±CTé®ç½©æ•°æ®é›†ï¼ˆæ¥è‡ªAbdomenAtlas 2.0ï¼‰åˆå¹¶ã€‚æˆ‘ä»¬ä½¿ç”¨å¸¦æœ‰è¿™äº›æ©è†œå’ŒæŠ¥å‘Šçš„R-Superè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨å†…éƒ¨å’Œå¤–éƒ¨éªŒè¯ä¸­æ˜¾è‘—æé«˜äº†è‚¿ç˜¤åˆ†å‰²æ•ˆæœâ€”â€”ä¸ä»…ä½¿ç”¨æ©è†œè¿›è¡Œè®­ç»ƒç›¸æ¯”ï¼ŒF1åˆ†æ•°æé«˜äº†é«˜è¾¾16%ã€‚é€šè¿‡åˆ©ç”¨ç°æˆçš„æ”¾å°„å­¦æŠ¥å‘Šæ¥è¡¥å……ç¨€ç¼ºçš„åˆ†å‰²æ©è†œï¼Œå½“å¯ç”¨çš„è®­ç»ƒæ©è†œå¾ˆå°‘ï¼ˆä¾‹å¦‚ï¼Œ50ä¸ªï¼‰æˆ–å¾ˆå¤šï¼ˆä¾‹å¦‚ï¼Œ1700ä¸ªï¼‰æ—¶ï¼ŒR-Superéƒ½èƒ½æå¤§åœ°æé«˜äººå·¥æ™ºèƒ½çš„æ€§èƒ½ã€‚é¡¹ç›®åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/MrGiovanni/R-Super">https://github.com/MrGiovanni/R-Super</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05582v1">PDF</a> Accepted to MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ç ”ç©¶äº†åˆ©ç”¨æ”¾å°„å­¦æŠ¥å‘Šæé«˜è‚¿ç˜¤åˆ†å‰²çš„AIæ€§èƒ½ã€‚é’ˆå¯¹CTæ‰«æä¸­çš„è‚¿ç˜¤åˆ†å‰²é—®é¢˜ï¼Œæå‡ºäº†æŠ¥å‘Šç›‘ç£æŸå¤±ï¼ˆR-Superï¼‰æ–¹æ³•ï¼Œå°†æ”¾å°„å­¦æŠ¥å‘Šè½¬åŒ–ä¸ºä½“ç´ çº§ç›‘ç£ä¿¡æ¯ï¼Œä»¥æ”¹å–„è‚¿ç˜¤åˆ†å‰²çš„æ•ˆæœã€‚é€šè¿‡ç»“åˆå…¬å…±CT-Maskæ•°æ®é›†å’Œè‡ªåˆ¶çš„CT-Reportæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼ŒR-Superåœ¨å†…éƒ¨å’Œå¤–éƒ¨éªŒè¯ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„è‚¿ç˜¤åˆ†å‰²æ•ˆæœæå‡ï¼Œå°¤å…¶æ˜¯å½“è®­ç»ƒæ©è†œæ•°é‡è¾ƒå°‘æ—¶æå‡æ›´ä¸ºæ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚¿ç˜¤åˆ†å‰²åœ¨CTæ‰«æä¸­å¯¹äºè¯Šæ–­ã€æ‰‹æœ¯å’Œé¢„åè¯„ä¼°å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä½†åˆ†å‰²æ©è†œçš„åˆ¶ä½œéœ€è¦æ—¶é—´å’Œä¸“ä¸šçŸ¥è¯†ï¼Œå› æ­¤æ•°é‡æœ‰é™ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†æŠ¥å‘Šç›‘ç£æŸå¤±ï¼ˆR-Superï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨å°†æ”¾å°„å­¦æŠ¥å‘Šè½¬åŒ–ä¸ºä½“ç´ çº§ç›‘ç£ä¿¡æ¯ï¼Œä»¥æ”¹å–„è‚¿ç˜¤åˆ†å‰²çš„AIæ€§èƒ½ã€‚</li>
<li>èåˆäº†å…¬å…±CT-Maskæ•°æ®é›†å’Œè‡ªåˆ¶çš„CT-Reportæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†åœ¨è‚¿ç˜¤åˆ†å‰²ä¸Šçš„æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>R-Superåœ¨å†…éƒ¨å’Œå¤–éƒ¨éªŒè¯ä¸­å‡å–å¾—äº†è‰¯å¥½çš„ç»“æœï¼Œä¸ä»…ä½¿ç”¨æ©è†œè®­ç»ƒç›¸æ¯”ï¼ŒF1åˆ†æ•°æé«˜äº†é«˜è¾¾16%ã€‚</li>
<li>R-Superæ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºè®­ç»ƒæ©è†œæ•°é‡è¾ƒå°‘çš„æƒ…å†µï¼ŒåŒæ—¶ä¹Ÿèƒ½åœ¨æ©è†œæ•°é‡è¾ƒå¤šçš„æƒ…å†µä¸‹å–å¾—è‰¯å¥½æ•ˆæœã€‚</li>
<li>è¯¥æ–¹æ³•å¼ºåŒ–äº†æ”¾å°„å­¦æŠ¥å‘Šåœ¨AIè®­ç»ƒä¸­çš„åº”ç”¨ä»·å€¼ï¼Œä¸ºåˆ©ç”¨ä¸°å¯Œçš„éç»“æ„åŒ–æ•°æ®èµ„æºæä¾›äº†æ–°çš„æ€è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05582">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3e91e976c7b590e4b4326fe309e03244.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-87f3a05fa02c6fe6fdc5efab122f158c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5882c853eb151bc8831f90041ba876e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f04bc802e62c1a0ebaddd2119f3e2239.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34b3dd89671966b0b186ab7f69d8e6c9.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="OpenWorldSAM-Extending-SAM2-for-Universal-Image-Segmentation-with-Language-Prompts"><a href="#OpenWorldSAM-Extending-SAM2-for-Universal-Image-Segmentation-with-Language-Prompts" class="headerlink" title="OpenWorldSAM: Extending SAM2 for Universal Image Segmentation with   Language Prompts"></a>OpenWorldSAM: Extending SAM2 for Universal Image Segmentation with   Language Prompts</h2><p><strong>Authors:Shiting Xiao, Rishabh Kabra, Yuhang Li, Donghyun Lee, Joao Carreira, Priyadarshini Panda</strong></p>
<p>The ability to segment objects based on open-ended language prompts remains a critical challenge, requiring models to ground textual semantics into precise spatial masks while handling diverse and unseen categories. We present OpenWorldSAM, a framework that extends the prompt-driven Segment Anything Model v2 (SAM2) to open-vocabulary scenarios by integrating multi-modal embeddings extracted from a lightweight vision-language model (VLM). Our approach is guided by four key principles: i) Unified prompting: OpenWorldSAM supports a diverse range of prompts, including category-level and sentence-level language descriptions, providing a flexible interface for various segmentation tasks. ii) Efficiency: By freezing the pre-trained components of SAM2 and the VLM, we train only 4.5 million parameters on the COCO-stuff dataset, achieving remarkable resource efficiency. iii) Instance Awareness: We enhance the modelâ€™s spatial understanding through novel positional tie-breaker embeddings and cross-attention layers, enabling effective segmentation of multiple instances. iv) Generalization: OpenWorldSAM exhibits strong zero-shot capabilities, generalizing well on unseen categories and an open vocabulary of concepts without additional training. Extensive experiments demonstrate that OpenWorldSAM achieves state-of-the-art performance in open-vocabulary semantic, instance, and panoptic segmentation across multiple benchmarks, including ADE20k, PASCAL, ScanNet, and SUN-RGBD. </p>
<blockquote>
<p>åŸºäºå¼€æ”¾è¯­è¨€æç¤ºå¯¹ç‰©ä½“è¿›è¡Œåˆ†å‰²çš„èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œè¿™è¦æ±‚æ¨¡å‹å°†æ–‡æœ¬è¯­ä¹‰è½¬åŒ–ä¸ºç²¾ç¡®çš„ç©ºé—´æ©è†œï¼ŒåŒæ—¶å¤„ç†å¤šæ ·ä¸”æœªçŸ¥çš„ç±»åˆ«ã€‚æˆ‘ä»¬æå‡ºäº†OpenWorldSAMæ¡†æ¶ï¼Œå®ƒé€šè¿‡é›†æˆè½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹(VLM)æå–çš„å¤šæ¨¡å¼åµŒå…¥ï¼Œå°†æç¤ºé©±åŠ¨çš„Segment Anything Model v2 (SAM2)æ‰©å±•åˆ°å¼€æ”¾è¯æ±‡åœºæ™¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•éµå¾ªå››ä¸ªå…³é”®åŸåˆ™ï¼šä¸€ã€ç»Ÿä¸€æç¤ºï¼šOpenWorldSAMæ”¯æŒå„ç§æç¤ºï¼ŒåŒ…æ‹¬ç±»åˆ«çº§åˆ«å’Œå¥å­çº§åˆ«çš„è¯­è¨€æè¿°ï¼Œä¸ºå„ç§åˆ†å‰²ä»»åŠ¡æä¾›äº†ä¸€ä¸ªçµæ´»çš„ç•Œé¢ã€‚äºŒã€é«˜æ•ˆæ€§ï¼šæˆ‘ä»¬é€šè¿‡å†»ç»“SAM2å’ŒVLMçš„é¢„è®­ç»ƒç»„ä»¶ï¼Œåªåœ¨COCO-stuffæ•°æ®é›†ä¸Šè®­ç»ƒäº†450ä¸‡ä¸ªå‚æ•°ï¼Œå®ç°äº†æ˜¾è‘—çš„èµ„æºæ•ˆç‡ã€‚ä¸‰ã€å®ä¾‹æ„ŸçŸ¥ï¼šæˆ‘ä»¬é€šè¿‡æ–°å‹çš„ä½ç½®å†³èƒœåµŒå…¥å’Œäº¤å‰æ³¨æ„åŠ›å±‚å¢å¼ºæ¨¡å‹çš„ç©ºé—´ç†è§£èƒ½åŠ›ï¼Œå®ç°å¯¹å¤šä¸ªå®ä¾‹çš„æœ‰æ•ˆåˆ†å‰²ã€‚å››ã€æ³›åŒ–èƒ½åŠ›ï¼šOpenWorldSAMè¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œåœ¨æœªè§è¿‡çš„ç±»åˆ«å’Œå¼€æ”¾çš„æ¦‚å¿µè¯æ±‡è¡¨ä¸Šæ— éœ€é¢å¤–è®­ç»ƒå°±èƒ½å¾ˆå¥½åœ°æ¨å¹¿ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒOpenWorldSAMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å¼€æ”¾è¯æ±‡è¡¨çš„è¯­ä¹‰ã€å®ä¾‹å’Œå…¨æ™¯åˆ†å‰²çš„å…ˆè¿›æ°´å¹³ï¼ŒåŒ…æ‹¬ADE20kã€PASCALã€ScanNetå’ŒSUN-RGBDã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05427v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>OpenWorldSAMæ¡†æ¶æ‰©å±•äº†åŸºäºæ–‡æœ¬æç¤ºè¿›è¡Œç‰©ä½“åˆ†å‰²çš„SAM2æ¨¡å‹ï¼Œé€‚ç”¨äºå¼€æ”¾è¯æ±‡åœºæ™¯ã€‚é€šè¿‡æ•´åˆè½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€åµŒå…¥ï¼Œæ”¯æŒå¤šç§æç¤ºï¼Œå®ç°é«˜æ•ˆã€å®ä¾‹æ„ŸçŸ¥å’Œæ³›åŒ–èƒ½åŠ›å¼ºçš„å¼€æ”¾è¯æ±‡è¯­ä¹‰ã€å®ä¾‹å’Œå…¨æ™¯åˆ†å‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenWorldSAMæ¡†æ¶æ‰©å±•äº†Segment Anything Model v2ï¼ˆSAM2ï¼‰ï¼Œæ”¯æŒå¼€æ”¾è¯æ±‡åœºæ™¯ä¸­çš„ç‰©ä½“åˆ†å‰²ã€‚</li>
<li>é€šè¿‡å¤šæ¨¡æ€åµŒå…¥å’Œè½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹çš„æ•´åˆï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ç»Ÿä¸€æç¤ºï¼šæ”¯æŒç±»åˆ«çº§åˆ«å’Œå¥å­çº§åˆ«çš„è¯­è¨€æè¿°ï¼Œä¸ºå„ç§åˆ†å‰²ä»»åŠ¡æä¾›çµæ´»æ¥å£ã€‚</li>
<li>é«˜æ•ˆæ€§ï¼šé€šè¿‡å†»ç»“SAM2å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒç»„ä»¶ï¼Œä»…åœ¨COCO-stuffæ•°æ®é›†ä¸Šè®­ç»ƒ4.5ç™¾ä¸‡å‚æ•°ï¼Œå®ç°èµ„æºé«˜æ•ˆåˆ©ç”¨ã€‚</li>
<li>å®ä¾‹æ„ŸçŸ¥ï¼šé€šè¿‡æ–°å‹å®šä½tie-breakeråµŒå…¥å’Œäº¤å‰æ³¨æ„åŠ›å±‚ï¼Œå¢å¼ºæ¨¡å‹å¯¹å¤šä¸ªå®ä¾‹çš„ç©ºé—´ç†è§£ï¼Œå®ç°æœ‰æ•ˆåˆ†å‰²ã€‚</li>
<li>æ³›åŒ–èƒ½åŠ›å¼ºï¼šOpenWorldSAMå…·æœ‰å¾ˆå¼ºçš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œèƒ½å¾ˆå¥½åœ°æ³›åŒ–æœªè§è¿‡çš„ç±»åˆ«å’Œå¼€æ”¾è¯æ±‡æ¦‚å¿µï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05427">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f8d259d805e9649f8c2f142ee9b4c151.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b64659c4b3e4cd13697bb4b6400dfab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-392e6346d4fcce1ff11df8298f09e9f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c78306956b610a4726ed40386d13332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9031f69701f1dd844d163f0d6cfb630.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9fad57310a122643a3077633fd6f6d8.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-11/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-11/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-11/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-83cb962fd27f21fcf841f8ae401703c9.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-11  Differentiable Reward Optimization for LLM based TTS system
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-11/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-803770d7cf7f5bba743215eb9070e24a.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-11  PWD Prior-Guided and Wavelet-Enhanced Diffusion Model for Limited-Angle   CT
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23523.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
