<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-07-11  KAConvText Novel Approach to Burmese Sentence Classification using   Kolmogorov-Arnold Convolution">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-5d9c1c5da5d60fb6408812c5d1c19025.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    23 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-11-更新"><a href="#2025-07-11-更新" class="headerlink" title="2025-07-11 更新"></a>2025-07-11 更新</h1><h2 id="KAConvText-Novel-Approach-to-Burmese-Sentence-Classification-using-Kolmogorov-Arnold-Convolution"><a href="#KAConvText-Novel-Approach-to-Burmese-Sentence-Classification-using-Kolmogorov-Arnold-Convolution" class="headerlink" title="KAConvText: Novel Approach to Burmese Sentence Classification using   Kolmogorov-Arnold Convolution"></a>KAConvText: Novel Approach to Burmese Sentence Classification using   Kolmogorov-Arnold Convolution</h2><p><strong>Authors:Ye Kyaw Thu, Thura Aung, Thazin Myint Oo, Thepchai Supnithi</strong></p>
<p>This paper presents the first application of Kolmogorov-Arnold Convolution for Text (KAConvText) in sentence classification, addressing three tasks: imbalanced binary hate speech detection, balanced multiclass news classification, and imbalanced multiclass ethnic language identification. We investigate various embedding configurations, comparing random to fastText embeddings in both static and fine-tuned settings, with embedding dimensions of 100 and 300 using CBOW and Skip-gram models. Baselines include standard CNNs and CNNs augmented with a Kolmogorov-Arnold Network (CNN-KAN). In addition, we investigated KAConvText with different classification heads - MLP and KAN, where using KAN head supports enhanced interpretability. Results show that KAConvText-MLP with fine-tuned fastText embeddings achieves the best performance of 91.23% accuracy (F1-score &#x3D; 0.9109) for hate speech detection, 92.66% accuracy (F1-score &#x3D; 0.9267) for news classification, and 99.82% accuracy (F1-score &#x3D; 0.9982) for language identification. </p>
<blockquote>
<p>本文首次将Kolmogorov-Arnold卷积应用于文本（KAConvText）应用于句子分类，涉及三个任务：不平衡二进制仇恨言论检测、平衡多类新闻分类和不平衡多类民族语言识别。我们研究了各种嵌入配置，比较了静态和微调设置中的随机到fastText嵌入，使用CBOW和Skip-gram模型，嵌入维度为100和300。基准线包括标准CNN和增强Kolmogorov-Arnold网络（CNN-KAN）的CNN。此外，我们还研究了不同分类头与KAConvText的结合，包括MLP和KAN，其中使用KAN头支持增强可解释性。结果表明，使用微调fastText嵌入的KAConvText-MLP在仇恨言论检测方面达到了91.23%的准确率（F1分数为0.9109），新闻分类方面的准确率为92.66%（F1分数为0.9267），语言识别方面的准确率为99.82%（F1分数为0.9982）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06753v1">PDF</a> 10 pages, 3 figures, 4 tables</p>
<p><strong>Summary</strong><br>文本介绍了Kolmogorov-Arnold卷积在文本中的应用，重点研究了三项任务：平衡不平衡二元仇恨言论检测、平衡多元新闻分类和不平衡多元种族语言识别。通过对随机与fastText嵌入在静态和微调设置中的比较，采用CBOW和Skip-gram模型的嵌入维度为100和300的实验设置进行研究。初步工作包括使用CNN模型和标准CNNs，并提出了增强解释性的KAConvText模型。实验结果表明，使用fine-tuned fastText嵌入的KAConvText-MLP模型在仇恨言论检测方面取得了最佳性能，准确率为91.23%，F1分数为0.9109；在新闻分类方面准确率为92.66%，F1分数为0.9267；在语言识别方面准确率为99.82%，F1分数为0.9982。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>该论文首次将Kolmogorov-Arnold卷积应用于文本中的句子分类任务。</li>
<li>研究涵盖了三种任务：仇恨言论检测、新闻分类和民族语言识别。</li>
<li>对比了随机嵌入与fastText嵌入在静态和微调状态下的性能差异。</li>
<li>初步研究包括标准CNN模型和结合Kolmogorov-Arnold网络的CNN模型（CNN-KAN）。</li>
<li>研究发现KAConvText使用不同的分类头（MLP和KAN）时表现出不同的性能，其中使用KAN头增强了模型的解释性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06753">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2dc21fc17a627fd1e097999529208590.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d76875b03ea3e99613ebe0572470d0a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ae47c2ac9e8e5997f86704147f2e6e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7bfcd13e4079c5bc0748ddfb6e52c18.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3523670c03301d0c7315d800631a77ba.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Pronunciation-Lexicon-Free-Training-for-Phoneme-based-Crosslingual-ASR-via-Joint-Stochastic-Approximation"><a href="#Pronunciation-Lexicon-Free-Training-for-Phoneme-based-Crosslingual-ASR-via-Joint-Stochastic-Approximation" class="headerlink" title="Pronunciation-Lexicon Free Training for Phoneme-based Crosslingual ASR   via Joint Stochastic Approximation"></a>Pronunciation-Lexicon Free Training for Phoneme-based Crosslingual ASR   via Joint Stochastic Approximation</h2><p><strong>Authors:Saierdaer Yusuyin, Te Ma, Hao Huang, Zhijian Ou</strong></p>
<p>Recently, pre-trained models with phonetic supervision have demonstrated their advantages for crosslingual speech recognition in data efficiency and information sharing across languages. However, a limitation is that a pronunciation lexicon is needed for such phoneme-based crosslingual speech recognition. In this study, we aim to eliminate the need for pronunciation lexicons and propose a latent variable model based method, with phonemes being treated as discrete latent variables. The new method consists of a speech-to-phoneme (S2P) model and a phoneme-to-grapheme (P2G) model, and a grapheme-to-phoneme (G2P) model is introduced as an auxiliary inference model. To jointly train the three models, we utilize the joint stochastic approximation (JSA) algorithm, which is a stochastic extension of the EM (expectation-maximization) algorithm and has demonstrated superior performance particularly in estimating discrete latent variable models. Based on the Whistle multilingual pre-trained S2P model, crosslingual experiments are conducted in Polish (130 h) and Indonesian (20 h). With only 10 minutes of phoneme supervision, the new method, JSA-SPG, achieves 5% error rate reductions compared to the best crosslingual fine-tuning approach using subword or full phoneme supervision. Furthermore, it is found that in language domain adaptation (i.e., utilizing cross-domain text-only data), JSA-SPG outperforms the standard practice of language model fusion via the auxiliary support of the G2P model by 9% error rate reductions. To facilitate reproducibility and encourage further exploration in this field, we open-source the JSA-SPG training code and complete pipeline. </p>
<blockquote>
<p>最近，带有语音监督的预训练模型在跨语言语音识别中显示出其在数据效率和跨语言信息共享方面的优势。然而，一个限制是需要一个发音词典来进行这种基于音素的跨语言语音识别。本研究旨在消除对发音词典的需求，并提出一种基于潜在变量模型的方法，其中音素被视为离散潜在变量。新方法包括语音到音素（S2P）模型和音素到字母（P2G）模型，并引入字母到音素（G2P）模型作为辅助推理模型。为了联合训练这三个模型，我们利用联合随机逼近（JSA）算法，它是EM（期望最大化）算法的随机扩展，在估计离散潜在变量模型方面表现出卓越的性能。基于Whistle多语言预训练S2P模型的波兰语（130小时）和印尼语（20小时）的跨语言实验表明，仅需10分钟的音素监督，新方法JSA-SPG与采用子词或全音素监督的最佳跨语言微调方法相比，实现了5%的错误率降低。此外，研究发现，在语言领域适应（即利用跨域文本数据），JSA-SPG通过G2P模型的辅助支持，实现了9%的错误率降低，优于语言模型融合的标准实践。为了促进该领域的可重复性和进一步探索，我们公开了JSA-SPG训练代码和完整流程。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06249v1">PDF</a> submitted to IEEE TASLP</p>
<p><strong>摘要</strong><br>基于Whistle多语言预训练S2P模型，本研究提出了一种无需语音字典的跨语言语音识别方法。该方法采用潜在变量模型，将语音作为离散潜在变量处理。通过联合训练语音到语音（S2P）、语音到字母（P2G）模型，并引入字母到语音（G2P）模型作为辅助推理模型，利用联合随机逼近（JSA）算法优化模型性能。在波兰语（130小时）和印尼语（20小时）的跨语言实验中，新方法JSA-SPG仅需10分钟的语音监督即可实现最佳跨语言微调方法的5%误差率降低。此外，在语言域适应（即利用跨域文本数据）方面，JSA-SPG通过G2P模型的辅助支持实现了9%的错误率降低。为便于复制和推动该领域的进一步探索，我们公开了JSA-SPG的训练代码和完整流程。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>跨语言语音识别中，无需语音字典的方法被提出，解决了依赖语音字典的问题。</li>
<li>采用潜在变量模型处理语音数据，其中语音被视为离散潜在变量。</li>
<li>联合训练了语音到语音（S2P）、语音到字母（P2G）模型，同时引入了字母到语音（G2P）模型作为辅助。</li>
<li>利用联合随机逼近（JSA）算法优化模型性能，特别是在估计离散潜在变量模型方面表现优越。</li>
<li>在波兰语和印尼语的跨语言实验中，新方法实现了显著的性能提升，与最佳跨语言微调方法相比，误差率降低了5%。</li>
<li>在语言域适应方面，新方法通过利用跨域文本数据实现了显著的性能改进，错误率降低了9%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06249">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5d0aafd32ef8d5d99f9f48a76b6f8f5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe2dabbe71ddf31c75ae3eee6dc51b04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4f40bf637f50428c1c779573693624e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f675f99009897db2a893b108b42432a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="How-to-Evaluate-Automatic-Speech-Recognition-Comparing-Different-Performance-and-Bias-Measures"><a href="#How-to-Evaluate-Automatic-Speech-Recognition-Comparing-Different-Performance-and-Bias-Measures" class="headerlink" title="How to Evaluate Automatic Speech Recognition: Comparing Different   Performance and Bias Measures"></a>How to Evaluate Automatic Speech Recognition: Comparing Different   Performance and Bias Measures</h2><p><strong>Authors:Tanvina Patel, Wiebke Hutiri, Aaron Yi Ding, Odette Scharenborg</strong></p>
<p>There is increasingly more evidence that automatic speech recognition (ASR) systems are biased against different speakers and speaker groups, e.g., due to gender, age, or accent. Research on bias in ASR has so far primarily focused on detecting and quantifying bias, and developing mitigation approaches. Despite this progress, the open question is how to measure the performance and bias of a system. In this study, we compare different performance and bias measures, from literature and proposed, to evaluate state-of-the-art end-to-end ASR systems for Dutch. Our experiments use several bias mitigation strategies to address bias against different speaker groups. The findings reveal that averaged error rates, a standard in ASR research, alone is not sufficient and should be supplemented by other measures. The paper ends with recommendations for reporting ASR performance and bias to better represent a system’s performance for diverse speaker groups, and overall system bias. </p>
<blockquote>
<p>自动语音识别（ASR）系统对不同说话者和说话者群体存在偏见，例如性别、年龄或口音等方面的偏见，这一证据越来越多。迄今为止，关于ASR偏见的研究主要集中在检测和量化偏见以及开发缓解方法上。尽管取得了进展，但开放的问题是如何衡量系统的性能和偏见。本研究中，我们比较了文献中提出的和自行提出的不同性能和偏见度量方法，以评估荷兰最先进的端到端ASR系统。我们的实验采用了多种偏见缓解策略，以解决针对不同说话者群体的偏见问题。研究结果表明，仅使用平均错误率（ASR研究中的标准）是不够的，应辅以其他度量方法。本文最后提出了报告ASR性能和偏见的建议，以更好地代表系统对不同说话者群体的性能以及整体的系统偏见。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05885v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本研究对比了不同性能与偏见衡量标准，以评估荷兰语端到端的自动语音识别系统。实验采用多种偏见缓解策略，以应对对不同说话群体的偏见问题。研究发现，仅使用平均错误率这一传统衡量标准并不足够，应辅以其他衡量手段。文章最后给出了关于报告自动语音识别性能和偏见的建议，以更好地代表系统对不同说话群体的性能以及整体的系统偏见。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动语音识别（ASR）系统存在对不同说话者和说话群体（如性别、年龄或口音）的偏见。</li>
<li>目前的研究主要集中在检测、量化偏见和制定缓解方法上。</li>
<li>仅使用平均错误率作为衡量ASR系统性能和偏见的单一标准是不够的。</li>
<li>需要采用多种衡量手段来评估ASR系统的性能和偏见。</li>
<li>实验采用了多种偏见缓解策略来处理对不同说话群体的偏见问题。</li>
<li>推荐使用更全面的报告方式来展示ASR系统对不同说话群体的性能以及整体的系统偏见。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05885">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3137d8c798a1730d4c5cc82657625ea7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4bede235abb4447bda7f512a1107fdf9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-02f4f6c105dca70eadcbf2f9966dcebb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5fbf99740c9db704885a78beab64eda9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DeepTalk-Towards-Seamless-and-Smart-Speech-Interaction-with-Adaptive-Modality-Specific-MoE"><a href="#DeepTalk-Towards-Seamless-and-Smart-Speech-Interaction-with-Adaptive-Modality-Specific-MoE" class="headerlink" title="DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive   Modality-Specific MoE"></a>DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive   Modality-Specific MoE</h2><p><strong>Authors:Hang Shao, Heting Gao, Yunhang Shen, Jiawei Chen, Lijiang Li, Zuwei Long, Bo Tong, Ke Li, Xing Sun</strong></p>
<p>Native multimodal large language models (MLLMs) restructure a single large language model (LLM) into a spoken language model (SLM) capable of both speech and text generation. Compared to modular and aligned MLLMs, native MLLMs preserve richer paralinguistic features such as emotion and prosody, and generate speech responses directly within the backbone LLM rather than using a separate speech decoder. This integration also results in lower response latency and smoother interaction. However, native MLLMs suffer from catastrophic forgetting and performance degradation because the available paired speech-text data is insufficient to support the pretraining of MLLMs compared to the vast amount of text data required to pretrain text LLMs. To address this issue, we propose DeepTalk, a framework for adaptive modality expert learning based on a Mixture of Experts (MoE) architecture. DeepTalk first adaptively distinguishes modality experts according to their modality load within the LLM. Each modality expert then undergoes specialized single-modality training, followed by joint multimodal collaborative training. As a result, DeepTalk incurs only a 5.5% performance drop compared to the original LLM, which is significantly lower than the average performance drop of over 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par with modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within 0.5 seconds, ensuring a seamless and intelligent speech interaction experience. Code and models are released at <a target="_blank" rel="noopener" href="https://github.com/talkking/DeepTalk">https://github.com/talkking/DeepTalk</a>. </p>
<blockquote>
<p>原生多模态大型语言模型（MLLMs）将单一的大型语言模型（LLM）重构为能够同时处理语音和文本生成的口语语言模型（SLM）。与模块化和对齐的MLLMs相比，原生MLLMs保留了更丰富的副语言特征，如情感和语调，并在主干LLM内直接生成语音响应，而不是使用单独的语音解码器。这种集成还带来了更低的响应延迟和更流畅的互动。然而，由于可用的配对语音-文本数据不足以支持MLLMs的预训练，与需要预训练文本LLM的大量文本数据相比，原生MLLMs遭受灾难性遗忘和性能下降的问题。为了解决这个问题，我们提出了DeepTalk，这是一个基于专家混合（MoE）架构的自适应模态专家学习框架。DeepTalk首先根据LLM内的模态负载自适应地区分模态专家。然后，每个模态专家接受专门的单模态训练，接着进行联合多模态协作训练。因此，DeepTalk的性能仅比原始LLM下降5.5%，远低于原生MLLMs通常出现的超过20%的平均性能下降，并与模块化MLLMs不相上下。同时，端到端对话延迟保持在0.5秒内，确保无缝、智能的语音交互体验。代码和模型已发布在<a target="_blank" rel="noopener" href="https://github.com/talkking/DeepTalk%E3%80%82">https://github.com/talkking/DeepTalk。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21864v2">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>本文介绍了原生多模态大型语言模型（MLLMs）的特点及其在语音和文本生成方面的优势。相较于模块化和对齐的MLLMs，原生MLLMs能保留更丰富的副语言特征，如情感和语调，并在主干LLM内直接生成语音响应。然而，由于可用的配对语音-文本数据不足以支持MLLMs的预训练，导致原生MLLMs容易出现灾难性遗忘和性能下降。为解决这一问题，本文提出了DeepTalk框架，采用基于混合专家（MoE）架构的自适应模态专家学习。DeepTalk首先根据LLM内的模态负载自适应地区分模态专家，随后进行专门的单模态训练和联合多模态协同训练。相较于原生LLM，DeepTalk的性能仅下降5.5%，远低于原生MLLMs通常出现的超过20%的性能下降，且与模块化MLLM相当。同时，端到端对话延迟保持在0.5秒内，确保流畅的智能语音交互体验。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>原生多模态大型语言模型（MLLMs）能够同时处理语音和文本生成，保留丰富的副语言特征。</li>
<li>相较于模块化和对齐的MLLMs，原生MLLMs在交互方面具有更低的响应延迟和更流畅的体验。</li>
<li>原生MLLMs面临因数据不足导致的灾难性遗忘和性能下降问题。</li>
<li>DeepTalk框架通过自适应模态专家学习来提高MLLMs的性能。</li>
<li>DeepTalk仅导致5.5%的性能下降，远低于原生MLLMs通常的性能下降幅度。</li>
<li>DeepTalk框架的性能与模块化MLLM相当。</li>
<li>DeepTalk确保了流畅的智能语音交互体验，端到端对话延迟低于0.5秒。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21864">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-424f5ee7b37e6fc570614a26bd9f7515.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0db036c150f14fdfbbe04b38108d8dae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa288d4ac1e5c4dd8a488e54ac864e17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d9c1c5da5d60fb6408812c5d1c19025.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="EgoVIS-CVPR-PAIR-Net-Enhancing-Egocentric-Speaker-Detection-via-Pretrained-Audio-Visual-Fusion-and-Alignment-Loss"><a href="#EgoVIS-CVPR-PAIR-Net-Enhancing-Egocentric-Speaker-Detection-via-Pretrained-Audio-Visual-Fusion-and-Alignment-Loss" class="headerlink" title="EgoVIS@CVPR: PAIR-Net: Enhancing Egocentric Speaker Detection via   Pretrained Audio-Visual Fusion and Alignment Loss"></a>EgoVIS@CVPR: PAIR-Net: Enhancing Egocentric Speaker Detection via   Pretrained Audio-Visual Fusion and Alignment Loss</h2><p><strong>Authors:Yu Wang, Juhyung Ha, David J. Crandall</strong></p>
<p>Active speaker detection (ASD) in egocentric videos presents unique challenges due to unstable viewpoints, motion blur, and off-screen speech sources - conditions under which traditional visual-centric methods degrade significantly. We introduce PAIR-Net (Pretrained Audio-Visual Integration with Regularization Network), an effective model that integrates a partially frozen Whisper audio encoder with a fine-tuned AV-HuBERT visual backbone to robustly fuse cross-modal cues. To counteract modality imbalance, we introduce an inter-modal alignment loss that synchronizes audio and visual representations, enabling more consistent convergence across modalities. Without relying on multi-speaker context or ideal frontal views, PAIR-Net achieves state-of-the-art performance on the Ego4D ASD benchmark with 76.6% mAP, surpassing LoCoNet and STHG by 8.2% and 12.9% mAP, respectively. Our results highlight the value of pretrained audio priors and alignment-based fusion for robust ASD under real-world egocentric conditions. </p>
<blockquote>
<p>以自我为中心的视频中的主动说话人检测（ASD）面临着独特的挑战，如视角不稳定、运动模糊以及屏幕外的语音源等条件，这些条件下传统视觉中心的方法会显著退化。我们引入了PAIR-Net（具有正则化的预训练视听集成网络），这是一个有效的模型，它将部分冻结的Whisper音频编码器与微调后的AV-HuBERT视觉主干相结合，以稳健地融合跨模态线索。为了克服模态不平衡的问题，我们引入了一种跨模态对齐损失，以同步音频和视觉表示，从而实现跨模态的更一致收敛。在不依赖多说话人上下文或理想正面视角的情况下，PAIR-Net在Ego4D ASD基准测试上达到了最先进的性能，平均准确率为76.6%，分别比LoCoNet和STHG高出8.2%和12.9%的平均准确率。我们的结果突显了在现实世界的以自我为中心的环境下，预训练的音频先验和对齐融合的价值，为稳健的ASD提供了重要参考。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02247v2">PDF</a> 4 pages, 1 figure, and 1 table</p>
<p><strong>Summary</strong></p>
<p>预训练音频优先与正则化网络（PAIR-Net）能有效解决主动说话人检测（ASD）在自我中心视频中的挑战性问题。通过集成部分冻结的Whisper音频编码器与微调后的AV-HuBERT视觉主干，实现跨模态线索的稳健融合。引入跨模态对齐损失以对抗模态不平衡问题，同步音频和视觉表征，使各模态间收敛更一致。在不依赖多说话人语境或理想正面视角的情况下，PAIR-Net在Ego4D ASD基准测试中达到76.6%的mAP，较LoCoNet和STHG分别高出8.2%和12.9%的mAP。结果突显预训练音频先验和对齐融合在真实世界自我中心条件下的稳健ASD的价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PAIR-Net模型解决了主动说话人检测在自我中心视频中的挑战，包括不稳定视角、运动模糊和离屏语音源等问题。</li>
<li>PAIR-Net集成了预训练的Whisper音频编码器和AV-HuBERT视觉主干，实现跨模态信息的稳健融合。</li>
<li>引入跨模态对齐损失以对抗模态不平衡问题，确保音频和视觉表征的同步。</li>
<li>PAIR-Net在Ego4D ASD基准测试中表现最佳，达到76.6%的mAP。</li>
<li>相较于其他模型，PAIR-Net在性能上有显著的提升。</li>
<li>预训练音频先验对于解决ASD问题具有重要价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02247">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fbddce00bef96376085ba662782a0b76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71d628bd3cdb8002645d67afa55c614a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-892009e66ce0cbd845e3f4568509f304.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Joint-Beamforming-and-Speaker-Attributed-ASR-for-Real-Distant-Microphone-Meeting-Transcription"><a href="#Joint-Beamforming-and-Speaker-Attributed-ASR-for-Real-Distant-Microphone-Meeting-Transcription" class="headerlink" title="Joint Beamforming and Speaker-Attributed ASR for Real Distant-Microphone   Meeting Transcription"></a>Joint Beamforming and Speaker-Attributed ASR for Real Distant-Microphone   Meeting Transcription</h2><p><strong>Authors:Can Cui, Imran Ahamad Sheikh, Mostafa Sadeghi, Emmanuel Vincent</strong></p>
<p>Distant-microphone meeting transcription is a challenging task. State-of-the-art end-to-end speaker-attributed automatic speech recognition (SA-ASR) architectures lack a multichannel noise and reverberation reduction front-end, which limits their performance. In this paper, we introduce a joint beamforming and SA-ASR approach for real meeting transcription. We first describe a data alignment and augmentation method to pretrain a neural beamformer on real meeting data. We then compare fixed, hybrid, and fully neural beamformers as front-ends to the SA-ASR model. Finally, we jointly optimize the fully neural beamformer and the SA-ASR model. Experiments on the real AMI corpus show that, while state-of-the-art multi-frame cross-channel attention based channel fusion fails to improve ASR performance, fine-tuning SA-ASR on the fixed beamformer’s output and jointly fine-tuning SA-ASR with the neural beamformer reduce the word error rate by 8% and 9% relative, respectively. </p>
<blockquote>
<p>远程麦克风会议转录是一项具有挑战性的任务。当前最先进的端到端说话人属性自动语音识别（SA-ASR）架构缺乏多通道噪声和混响前端，这限制了其性能。在本文中，我们介绍了一种联合波束形成和SA-ASR的实时会议转录方法。首先，我们描述了一种数据对齐和增强方法，用于在真实会议数据上预训练神经波束形成器。然后，我们将固定、混合和完全神经波束形成器与SA-ASR模型进行比较。最后，我们对完全神经波束形成器和SA-ASR模型进行联合优化。在真实的AMI语料库上的实验表明，虽然基于多帧跨通道注意力的通道融合并未提高ASR性能，但在固定波束形成器的输出上对SA-ASR进行微调，以及将SA-ASR与神经波束形成器进行联合微调，分别将单词错误率降低了8%和9%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21849v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种联合波束形成和说话人属性自动语音识别（SA-ASR）的方法，用于远程麦克风会议转录。文章首先描述了一种用于预训练神经网络波束形成器的数据对齐和增强方法。随后对比了固定、混合和完全神经网络波束形成器作为SA-ASR模型的前端。最后，对完全神经网络波束形成器和SA-ASR模型进行联合优化。实验表明，虽然基于多帧跨通道注意力的通道融合未能提高语音识别性能，但使用固定波束形成器的输出对SA-ASR进行微调，以及联合微调SA-ASR与神经网络波束形成器，分别将词错误率降低了8%和9%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>远程麦克风会议转录是一项具有挑战性的任务。</li>
<li>当前先进的端到端说话人属性自动语音识别（SA-ASR）架构缺乏多通道噪声和混响减少的前端，限制了其性能。</li>
<li>文章介绍了一种联合波束形成和SA-ASR的方法，用于真实会议转录。</li>
<li>通过数据对齐和增强方法预训练神经网络波束形成器。</li>
<li>对比了固定、混合和完全神经网络波束形成器作为SA-ASR前端的效果。</li>
<li>实验表明，多帧跨通道注意力通道融合未提高语音识别性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21849">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ce3d1ec6ac2ffa25f8bf509f8eb9d105.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58b26d364a1e7c6dcb9d6ef7213f805b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3585584537afd84549150d2598566ce2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12297d9df168722d05a06d14f6a4e9a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5823bafda972fd730f9e69768b4170a6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-11/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-11/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-11/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-db194f98bd94c084f68a365b3bf6c8cf.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-07-11  AI-GenBench A New Ongoing Benchmark for AI-Generated Image Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-11/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-75b57bdec5e52b3b8668fbc58ce348ec.jpg" class="responsive-img" alt="人脸相关">
                        
                        <span class="card-title">人脸相关</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            人脸相关 方向最新论文已更新，请持续关注 Update in 2025-07-11  DArFace Deformation Aware Robustness for Low Quality Face Recognition
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    人脸相关
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">人脸相关</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29301k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
