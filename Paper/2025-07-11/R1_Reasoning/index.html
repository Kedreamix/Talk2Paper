<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-11  Go to Zero Towards Zero-shot Motion Generation with Million-scale Data">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-7692149af91a8a4aaf1e5c40b713ca01.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-11-æ›´æ–°"><a href="#2025-07-11-æ›´æ–°" class="headerlink" title="2025-07-11 æ›´æ–°"></a>2025-07-11 æ›´æ–°</h1><h2 id="Go-to-Zero-Towards-Zero-shot-Motion-Generation-with-Million-scale-Data"><a href="#Go-to-Zero-Towards-Zero-shot-Motion-Generation-with-Million-scale-Data" class="headerlink" title="Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data"></a>Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data</h2><p><strong>Authors:Ke Fan, Shunlin Lu, Minyue Dai, Runyi Yu, Lixing Xiao, Zhiyang Dou, Junting Dong, Lizhuang Ma, Jingbo Wang</strong></p>
<p>Generating diverse and natural human motion sequences based on textual descriptions constitutes a fundamental and challenging research area within the domains of computer vision, graphics, and robotics. Despite significant advancements in this field, current methodologies often face challenges regarding zero-shot generalization capabilities, largely attributable to the limited size of training datasets. Moreover, the lack of a comprehensive evaluation framework impedes the advancement of this task by failing to identify directions for improvement. In this work, we aim to push text-to-motion into a new era, that is, to achieve the generalization ability of zero-shot. To this end, firstly, we develop an efficient annotation pipeline and introduce MotionMillion-the largest human motion dataset to date, featuring over 2,000 hours and 2 million high-quality motion sequences. Additionally, we propose MotionMillion-Eval, the most comprehensive benchmark for evaluating zero-shot motion generation. Leveraging a scalable architecture, we scale our model to 7B parameters and validate its performance on MotionMillion-Eval. Our results demonstrate strong generalization to out-of-domain and complex compositional motions, marking a significant step toward zero-shot human motion generation. The code is available at <a target="_blank" rel="noopener" href="https://github.com/VankouF/MotionMillion-Codes">https://github.com/VankouF/MotionMillion-Codes</a>. </p>
<blockquote>
<p>åŸºäºæ–‡æœ¬æè¿°ç”Ÿæˆå¤šæ ·ä¸”è‡ªç„¶çš„äººç±»åŠ¨ä½œåºåˆ—ï¼Œæ˜¯è®¡ç®—æœºè§†è§‰ã€å›¾å½¢å­¦å’Œæœºå™¨äººé¢†åŸŸä¸­çš„ä¸€ä¸ªåŸºæœ¬ä¸”å…·æŒ‘æˆ˜æ€§çš„ç ”ç©¶æ–¹å‘ã€‚å°½ç®¡è¯¥é¢†åŸŸå·²å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å½“å‰çš„æ–¹æ³•å¸¸å¸¸é¢ä¸´é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›çš„æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦å½’å› äºè®­ç»ƒæ•°æ®é›†çš„å¤§å°æœ‰é™ã€‚æ­¤å¤–ï¼Œç”±äºç¼ºä¹å…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œæ— æ³•ç¡®å®šæ”¹è¿›æ–¹å‘ï¼Œé˜»ç¢äº†è¯¥ä»»åŠ¡çš„è¿›å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è‡´åŠ›äºå°†æ–‡æœ¬åˆ°è¿åŠ¨çš„è½¬æ¢æ¨å‘ä¸€ä¸ªæ–°çš„æ—¶ä»£ï¼Œå³å®ç°é›¶æ ·æœ¬çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆå¼€å‘äº†ä¸€ä¸ªé«˜æ•ˆçš„æ³¨é‡Šç®¡é“ï¼Œå¹¶å¼•å…¥äº†MotionMillionï¼ˆè¿„ä»Šä¸ºæ­¢æœ€å¤§çš„äººç±»è¿åŠ¨æ•°æ®é›†ï¼‰ï¼ŒåŒ…å«è¶…è¿‡2000å°æ—¶å’Œ200ä¸‡æ¡é«˜è´¨é‡çš„è¿åŠ¨åºåˆ—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†MotionMillion-Evalï¼Œè¿™æ˜¯è¯„ä¼°é›¶æ ·æœ¬è¿åŠ¨ç”Ÿæˆçš„æœ€å…¨é¢çš„åŸºå‡†ã€‚åˆ©ç”¨å¯æ‰©å±•çš„æ¶æ„ï¼Œæˆ‘ä»¬å°†æ¨¡å‹æ‰©å±•åˆ°7äº¿å‚æ•°ï¼Œå¹¶åœ¨MotionMillion-Evalä¸ŠéªŒè¯äº†å…¶æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¯¹äºåŸŸå¤–å’Œå¤æ‚çš„ç»„åˆåŠ¨ä½œå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œè¿™æ˜¯æœç€é›¶æ ·æœ¬äººç±»è¿åŠ¨ç”Ÿæˆè¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/VankouF/MotionMillion-Codes%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/VankouF/MotionMillion-Codesæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07095v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://vankouf.github.io/MotionMillion/">https://vankouf.github.io/MotionMillion/</a></p>
<p><strong>Summary</strong><br>æ–‡æœ¬æè¿°äº†ä¸€ä¸ªå…³äºè®¡ç®—æœºè§†è§‰ã€å›¾å½¢å­¦å’Œæœºå™¨äººé¢†åŸŸä¸­åŸºäºæ–‡æœ¬æè¿°ç”Ÿæˆå¤šæ ·åŒ–å’Œè‡ªç„¶çš„äººç±»è¿åŠ¨åºåˆ—çš„ç ”ç©¶é¢†åŸŸã€‚æ–‡ç« ä»‹ç»äº†å½“å‰é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›å’Œè¯„ä¼°æ¡†æ¶çš„ä¸è¶³ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªé«˜æ•ˆæ ‡æ³¨ç®¡é“ï¼Œå¹¶å¼•å…¥äº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„äººç±»è¿åŠ¨æ•°æ®é›†MotionMillionï¼ŒåŒ…å«è¶…è¿‡2000å°æ—¶å’Œ2ç™¾ä¸‡é«˜è´¨é‡è¿åŠ¨åºåˆ—ã€‚æ­¤å¤–ï¼Œä»–ä»¬æå‡ºäº†æœ€å…¨é¢çš„è¯„ä¼°åŸºå‡†MotionMillion-Evalï¼Œé‡‡ç”¨å¯æ‰©å±•æ¶æ„ï¼Œå°†æ¨¡å‹æ‰©å±•åˆ°7äº¿å‚æ•°ï¼Œå¹¶åœ¨MotionMillion-Evalä¸ŠéªŒè¯äº†å…¶æ€§èƒ½ã€‚è¯¥ç ”ç©¶æˆæœåœ¨é›¶æ ·æœ¬äººç±»è¿åŠ¨ç”Ÿæˆæ–¹é¢å–å¾—äº†é‡è¦è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é¢†åŸŸï¼šæ–‡ç« æ¶‰åŠè®¡ç®—æœºè§†è§‰ã€å›¾å½¢å­¦å’Œæœºå™¨äººé¢†åŸŸä¸­åŸºäºæ–‡æœ¬æè¿°ç”Ÿæˆäººç±»è¿åŠ¨åºåˆ—çš„ç ”ç©¶ã€‚</li>
<li>æŒ‘æˆ˜ï¼šå½“å‰æ–¹æ³•é¢ä¸´é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›å’Œç¼ºä¹å…¨é¢è¯„ä¼°æ¡†æ¶çš„æŒ‘æˆ˜ã€‚</li>
<li>æ•°æ®é›†ï¼šå¼•å…¥è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„äººç±»è¿åŠ¨æ•°æ®é›†MotionMillionï¼ŒåŒ…å«2000å¤šå°æ—¶å’Œ2ç™¾ä¸‡é«˜è´¨é‡è¿åŠ¨åºåˆ—ã€‚</li>
<li>è¯„ä¼°åŸºå‡†ï¼šæå‡ºæœ€å…¨é¢çš„è¯„ä¼°åŸºå‡†MotionMillion-Evalç”¨äºè¯„ä¼°é›¶æ ·æœ¬è¿åŠ¨ç”Ÿæˆã€‚</li>
<li>æ¨¡å‹æ€§èƒ½ï¼šé‡‡ç”¨å¯æ‰©å±•æ¶æ„ï¼Œæ¨¡å‹æ‰©å±•åˆ°7äº¿å‚æ•°ï¼Œå¹¶åœ¨MotionMillion-Evalä¸ŠéªŒè¯äº†æ€§èƒ½ã€‚</li>
<li>æˆæœï¼šç ”ç©¶æˆæœåœ¨é›¶æ ·æœ¬äººç±»è¿åŠ¨ç”Ÿæˆæ–¹é¢å–å¾—é‡è¦è¿›å±•ï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆå¤æ‚ç»„åˆè¿åŠ¨ã€‚</li>
<li>å¯ç”¨èµ„æºï¼šä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6916ac8c80d823244db5712a4b5139a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30212fa0cc9d66ce0ff0859fc33ea28b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7692149af91a8a4aaf1e5c40b713ca01.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-446e179f1a2fe097beadeb99aab95444.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DeepRetro-Retrosynthetic-Pathway-Discovery-using-Iterative-LLM-Reasoning"><a href="#DeepRetro-Retrosynthetic-Pathway-Discovery-using-Iterative-LLM-Reasoning" class="headerlink" title="DeepRetro: Retrosynthetic Pathway Discovery using Iterative LLM   Reasoning"></a>DeepRetro: Retrosynthetic Pathway Discovery using Iterative LLM   Reasoning</h2><p><strong>Authors:Shreyas Vinaya Sathyanarayana, Rahil Shah, Sharanabasava D. Hiremath, Rishikesh Panda, Rahul Jana, Riya Singh, Rida Irfan, Ashwin Murali, Bharath Ramsundar</strong></p>
<p>Retrosynthesis, the identification of precursor molecules for a target compound, is pivotal for synthesizing complex molecules, but faces challenges in discovering novel pathways beyond predefined templates. Recent large language model (LLM) approaches to retrosynthesis have shown promise but effectively harnessing LLM reasoning capabilities for effective multi-step planning remains an open question. To address this challenge, we introduce DeepRetro, an open-source, iterative, hybrid LLM-based retrosynthetic framework. Our approach integrates the strengths of conventional template-based&#x2F;Monte Carlo tree search tools with the generative power of LLMs in a step-wise, feedback-driven loop. Initially, synthesis planning is attempted with a template-based engine. If this fails, the LLM subsequently proposes single-step retrosynthetic disconnections. Crucially, these suggestions undergo rigorous validity, stability, and hallucination checks before the resulting precursors are recursively fed back into the pipeline for further evaluation. This iterative refinement allows for dynamic pathway exploration and correction. We demonstrate the potential of this pipeline through benchmark evaluations and case studies, showcasing its ability to identify viable and potentially novel retrosynthetic routes. In particular, we develop an interactive graphical user interface that allows expert human chemists to provide human-in-the-loop feedback to the reasoning algorithm. This approach successfully generates novel pathways for complex natural product compounds, demonstrating the potential for iterative LLM reasoning to advance state-of-art in complex chemical syntheses. </p>
<blockquote>
<p>å›æº¯åˆæˆæ˜¯è¯†åˆ«ç›®æ ‡åŒ–åˆç‰©çš„å‰ä½“åˆ†å­ï¼Œå¯¹äºåˆæˆå¤æ‚åˆ†å­è‡³å…³é‡è¦ï¼Œä½†åœ¨å‘ç°è¶…å‡ºé¢„å®šæ¨¡æ¿çš„æ–°å‹é€”å¾„æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ€è¿‘çš„ç”¨äºå›æº¯åˆæˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹æ³•æ˜¾ç¤ºå‡ºäº†ä¸€å®šçš„æ½œåŠ›ï¼Œä½†å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›æ¥è¿›è¡Œæœ‰æ•ˆçš„å¤šæ­¥éª¤è§„åˆ’ä»æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DeepRetroï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„ã€è¿­ä»£çš„ã€åŸºäºLLMçš„å›æº¯åˆæˆæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†åŸºäºæ¨¡æ¿&#x2F;è’™ç‰¹å¡ç½—æ ‘æœç´¢å·¥å…·çš„ä¼ ç»Ÿä¼˜åŠ¿ä¸LLMçš„ç”Ÿæˆèƒ½åŠ›ï¼Œé‡‡ç”¨é€æ­¥çš„ã€åé¦ˆé©±åŠ¨çš„å›è·¯ã€‚é¦–å…ˆï¼Œå°è¯•ç”¨åŸºäºæ¨¡æ¿çš„å¼•æ“è¿›è¡Œåˆæˆè§„åˆ’ã€‚å¦‚æœè¿™å¤±è´¥äº†ï¼ŒLLMéšåä¼šæå‡ºå•æ­¥å›æº¯åˆæˆæ–­è£‚ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™äº›å»ºè®®åœ¨è¿›è¡Œä¸¥æ ¼çš„æœ‰æ•ˆæ€§ã€ç¨³å®šæ€§å’Œå¹»è§‰æ£€æŸ¥åï¼Œå°†äº§ç”Ÿçš„å‰ä½“é€’å½’åœ°åé¦ˆåˆ°ç®¡é“ä¸­è¿›è¡Œè¿›ä¸€æ­¥è¯„ä¼°ã€‚è¿™ç§è¿­ä»£ç»†åŒ–å…è®¸åŠ¨æ€è·¯å¾„æ¢ç´¢å’Œæ ¡æ­£ã€‚æˆ‘ä»¬é€šè¿‡åŸºå‡†è¯„ä¼°å’Œæ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†è¯¥ç®¡é“çš„æ½œåŠ›ï¼Œå±•ç¤ºäº†å…¶å‘ç°å¯è¡Œä¸”å¯èƒ½çš„æ–°å‹å›æº¯åˆæˆé€”å¾„çš„èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªäº¤äº’å¼çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼Œå…è®¸ä¸“ä¸šäººç±»åŒ–å­¦å®¶å‘æ¨ç†ç®—æ³•æä¾›äººç±»å‚ä¸çš„åé¦ˆã€‚è¿™ç§æ–¹æ³•æˆåŠŸåœ°ä¸ºå¤æ‚çš„å¤©ç„¶äº§ç‰©åŒ–åˆç‰©ç”Ÿæˆäº†æ–°å‹é€”å¾„ï¼Œè¯æ˜äº†è¿­ä»£LLMæ¨ç†åœ¨å¤æ‚åŒ–å­¦åˆæˆä¸­æ¨åŠ¨æœ€æ–°æŠ€æœ¯çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07060v1">PDF</a> 51 pages,</p>
<p><strong>Summary</strong>ï¼š<br>æ·±ååˆæˆï¼ˆDeepRetroï¼‰æ˜¯ä¸€ä¸ªå¼€æºçš„ã€è¿­ä»£çš„ã€åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é€†åˆæˆæ¡†æ¶ï¼Œç»“åˆäº†ä¼ ç»Ÿæ¨¡æ¿åŒ–å·¥å…·å’ŒLLMçš„ä¼˜åŠ¿ã€‚å®ƒé€šè¿‡é€æ­¥åé¦ˆç¯è·¯è¿›è¡Œåˆæˆè§„åˆ’ï¼Œå…ˆå°è¯•ä½¿ç”¨æ¨¡æ¿å¼•æ“ï¼Œå¤±è´¥ååˆ™é€šè¿‡LLMæå‡ºå•æ­¥é€†åˆæˆæ–­è£‚ã€‚æ¡†æ¶ç»è¿‡ä¸¥æ ¼çš„éªŒè¯æ€§ã€ç¨³å®šæ€§å’Œè™šæ„æ€§æ£€æŸ¥ï¼Œèƒ½å¤ŸåŠ¨æ€æ¢ç´¢ä¿®æ­£è·¯å¾„ã€‚è¯„ä¼°ç»“æœè¡¨æ˜å…¶å‘ç°å¤æ‚å¤©ç„¶äº§ç‰©åŒ–åˆç‰©çš„é€†åˆæˆè·¯å¾„çš„èƒ½åŠ›æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>æ·±ååˆæˆï¼ˆDeepRetroï¼‰æ˜¯ä¸€ä¸ªæ–°å‹çš„é€†åˆæˆæ¡†æ¶ï¼Œç»“åˆäº†æ¨¡æ¿å·¥å…·å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¼˜åŠ¿ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨é€æ­¥åé¦ˆç¯è·¯è¿›è¡Œåˆæˆè§„åˆ’ï¼Œé¦–å…ˆå°è¯•æ¨¡æ¿å¼•æ“ï¼Œå¤±è´¥åä½¿ç”¨LLMæå‡ºå•æ­¥æ–­è£‚å»ºè®®ã€‚</li>
<li>DeepRetroå…·å¤‡ä¸¥æ ¼çš„éªŒè¯æ€§ã€ç¨³å®šæ€§å’Œè™šæ„æ€§æ£€æŸ¥æœºåˆ¶ï¼Œç¡®ä¿æå‡ºçš„è·¯å¾„å¯é æ€§ã€‚</li>
<li>é€šè¿‡åŸºå‡†è¯„ä¼°å’Œæ¡ˆä¾‹ç ”ç©¶ï¼Œå±•ç¤ºäº†DeepRetroåœ¨å‘ç°å¤æ‚å¤©ç„¶äº§ç‰©åŒ–åˆç‰©çš„æ½œåœ¨é€†åˆæˆè·¯å¾„ä¸Šçš„èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07060">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2ea0a4669a72695609b835365e081cf2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-163cd473c10d6120aaed649cec55bb05.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Learning-Deliberately-Acting-Intuitively-Unlocking-Test-Time-Reasoning-in-Multimodal-LLMs"><a href="#Learning-Deliberately-Acting-Intuitively-Unlocking-Test-Time-Reasoning-in-Multimodal-LLMs" class="headerlink" title="Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning   in Multimodal LLMs"></a>Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning   in Multimodal LLMs</h2><p><strong>Authors:Yahan Yu, Yuyang Dong, Masafumi Oyamada</strong></p>
<p>Reasoning is a key capability for large language models (LLMs), particularly when applied to complex tasks such as mathematical problem solving. However, multimodal reasoning research still requires further exploration of modality alignment and training costs. Many of these approaches rely on additional data annotation and relevant rule-based rewards to enhance the understanding and reasoning ability, which significantly increases training costs and limits scalability. To address these challenges, we propose the Deliberate-to-Intuitive reasoning framework (D2I) that improves the understanding and reasoning ability of multimodal LLMs (MLLMs) without extra annotations and complex rewards. Specifically, our method sets deliberate reasoning strategies to enhance modality alignment only through the rule-based format reward during training. While evaluating, the reasoning style shifts to intuitive, which removes deliberate reasoning strategies during training and implicitly reflects the modelâ€™s acquired abilities in the response. D2I outperforms baselines across both in-domain and out-of-domain benchmarks. Our findings highlight the role of format reward in fostering transferable reasoning skills in MLLMs, and inspire directions for decoupling training-time reasoning depth from test-time response flexibility. </p>
<blockquote>
<p>æ¨ç†æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…³é”®èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨åº”ç”¨äºè§£å†³æ•°å­¦é—®é¢˜ç­‰å¤æ‚ä»»åŠ¡æ—¶ã€‚ç„¶è€Œï¼Œå¤šæ¨¡æ€æ¨ç†ç ”ç©¶ä»éœ€è¦è¿›ä¸€æ­¥æ¢ç´¢æ¨¡æ€å¯¹é½å’Œè®­ç»ƒæˆæœ¬ã€‚è®¸å¤šè¿™äº›æ–¹æ³•ä¾èµ–äºé¢å¤–çš„æ•°æ®æ ‡æ³¨å’Œç›¸å…³åŸºäºè§„åˆ™çš„å¥–åŠ±æ¥æé«˜ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œè¿™æ˜¾è‘—å¢åŠ äº†è®­ç»ƒæˆæœ¬å¹¶é™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Deliberate-to-Intuitiveæ¨ç†æ¡†æ¶ï¼ˆD2Iï¼‰ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨æ— éœ€é¢å¤–æ ‡æ³¨å’Œå¤æ‚å¥–åŠ±çš„æƒ…å†µä¸‹ï¼Œæé«˜å¤šæ¨¡æ€LLMçš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è®¾å®šæœ‰æ„è¯†çš„æ¨ç†ç­–ç•¥æ¥å¢å¼ºæ¨¡æ€å¯¹é½ï¼Œä»…é€šè¿‡åŸºäºè§„åˆ™çš„æ ¼å¼å¥–åŠ±æ¥è¿›è¡Œè®­ç»ƒã€‚åœ¨è¯„ä¼°æ—¶ï¼Œæ¨ç†é£æ ¼è½¬å‘ç›´è§‰å‹ï¼Œè¿™åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¶ˆé™¤äº†æœ‰æ„è¯†çš„æ¨ç†ç­–ç•¥ï¼Œå¹¶åœ¨å“åº”ä¸­éšå«åœ°åæ˜ äº†æ¨¡å‹æ‰€è·å¾—çš„èƒ½åŠ›ã€‚D2Iåœ¨åŸŸå†…å’Œè·¨åŸŸåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºåŸºçº¿ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†æ ¼å¼å¥–åŠ±åœ¨ä¿ƒè¿›å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„å¯è½¬ç§»æ¨ç†æŠ€èƒ½æ–¹é¢çš„ä½œç”¨ï¼Œå¹¶ä¸ºä»è®­ç»ƒæ—¶é—´çš„æ¨ç†æ·±åº¦ä¸æµ‹è¯•æ—¶é—´çš„å“åº”çµæ´»æ€§è§£è€¦æä¾›äº†çµæ„Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06999v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong><br>å¤šåª’ä½“LLMsæ¨¡å‹çš„æ¨ç†ç ”ç©¶å·²é€æ­¥å¼€å±•ï¼Œä½†å…¶é¢å¯¹å¤æ‚çš„æ•°å­¦é—®é¢˜è§£å†³ä»»åŠ¡ä»éœ€è¿›ä¸€æ­¥ç ”ç©¶ï¼Œå…¶éš¾åº¦åœ¨äºæ¨¡æ€å¯¹é½å’Œè®­ç»ƒæˆæœ¬çš„é—®é¢˜ã€‚è®¸å¤šç°æœ‰æ–¹æ³•ä¾èµ–é¢å¤–çš„æ•°æ®æ ‡æ³¨å’ŒåŸºäºè§„åˆ™çš„å¥–åŠ±æ¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™å¢åŠ äº†è®­ç»ƒæˆæœ¬å¹¶é™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Deliberate-to-Intuitiveï¼ˆD2Iï¼‰æ¨ç†æ¡†æ¶ï¼Œåœ¨ä¸éœ€è¦é¢å¤–æ ‡æ³¨å’Œå¤æ‚å¥–åŠ±çš„æƒ…å†µä¸‹ï¼Œä¼˜åŒ–äº†å¤šåª’ä½“LLMsçš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡è®­ç»ƒæ—¶çš„åˆ»æ„æ¨ç†ç­–ç•¥å¢å¼ºäº†æ¨¡æ€å¯¹é½ï¼Œåªä½¿ç”¨åŸºäºè§„åˆ™çš„æ ¼å¼å¥–åŠ±ï¼›è¯„ä¼°æ—¶åˆ™è½¬å˜ä¸ºç›´è§‰æ¨ç†é£æ ¼ï¼Œé€šè¿‡å“åº”éšå¼åæ˜ æ¨¡å‹ä¹ å¾—çš„èƒ½åŠ›ã€‚D2Iæ¡†æ¶åœ¨é¢†åŸŸå†…å’Œè·¨é¢†åŸŸåŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºä¼˜äºåŸºçº¿çš„æ•ˆæœã€‚ç ”ç©¶å‘ç°ï¼Œæ ¼å¼å¥–åŠ±åœ¨ä¿ƒè¿›MLLMsçš„å¯è¿ç§»æ¨ç†æŠ€èƒ½æ–¹é¢å‘æŒ¥äº†ä½œç”¨ï¼Œå¹¶ä¸ºè®­ç»ƒæ—¶çš„æ¨ç†æ·±åº¦ä¸æµ‹è¯•æ—¶çš„å“åº”çµæ´»æ€§ä¹‹é—´çš„è§£è€¦æä¾›äº†çµæ„Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€LLMåœ¨å¤æ‚ä»»åŠ¡å¦‚æ•°å­¦é—®é¢˜è§£å†³ä¸Šçš„æ¨ç†èƒ½åŠ›éœ€è¦æ›´å¤šç ”ç©¶ã€‚</li>
<li>å½“å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–é¢å¤–çš„æ•°æ®æ ‡æ³¨å’ŒåŸºäºè§„åˆ™çš„å¥–åŠ±æ¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™å¢åŠ äº†è®­ç»ƒæˆæœ¬å’Œé™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚</li>
<li>Deliberate-to-Intuitiveï¼ˆD2Iï¼‰æ¨ç†æ¡†æ¶ä¼˜åŒ–äº†å¤šåª’ä½“LLMsçš„æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„æ ‡æ³¨å’Œå¤æ‚çš„å¥–åŠ±æœºåˆ¶ã€‚</li>
<li>D2Ié€šè¿‡è®­ç»ƒæ—¶çš„åˆ»æ„æ¨ç†ç­–ç•¥å¢å¼ºäº†æ¨¡æ€å¯¹é½ï¼Œå¹¶åˆ©ç”¨åŸºäºè§„åˆ™çš„æ ¼å¼å¥–åŠ±ã€‚</li>
<li>åœ¨è¯„ä¼°é˜¶æ®µï¼Œæ¨¡å‹å±•ç°å‡ºç›´è§‰æ¨ç†é£æ ¼ï¼Œé€šè¿‡å“åº”éšå¼åæ˜ æ¨¡å‹ä¹ å¾—çš„èƒ½åŠ›ã€‚</li>
<li>D2Iæ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06999">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f0fab860fdba6b5314d3912f42599149.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d8a3e15acf8b42a76dc64737d722c70.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Generating-Multi-Table-Time-Series-EHR-from-Latent-Space-with-Minimal-Preprocessing"><a href="#Generating-Multi-Table-Time-Series-EHR-from-Latent-Space-with-Minimal-Preprocessing" class="headerlink" title="Generating Multi-Table Time Series EHR from Latent Space with Minimal   Preprocessing"></a>Generating Multi-Table Time Series EHR from Latent Space with Minimal   Preprocessing</h2><p><strong>Authors:Eunbyeol Cho, Jiyoun Kim, Minjae Lee, Sungjin Park, Edward Choi</strong></p>
<p>Electronic Health Records (EHR) are time-series relational databases that record patient interactions and medical events over time, serving as a critical resource for healthcare research and applications. However, privacy concerns and regulatory restrictions limit the sharing and utilization of such sensitive data, necessitating the generation of synthetic EHR datasets. Unlike previous EHR synthesis methods, which typically generate medical records consisting of expert-chosen features (e.g. a few vital signs or structured codes only), we introduce RawMed, the first framework to synthesize multi-table, time-series EHR data that closely resembles raw EHRs. Using text-based representation and compression techniques, RawMed captures complex structures and temporal dynamics with minimal preprocessing. We also propose a new evaluation framework for multi-table time-series synthetic EHRs, assessing distributional similarity, inter-table relationships, temporal dynamics, and privacy. Validated on two open-source EHR datasets, RawMed outperforms baseline models in fidelity and utility. The code is available at <a target="_blank" rel="noopener" href="https://github.com/eunbyeol-cho/RawMed">https://github.com/eunbyeol-cho/RawMed</a>. </p>
<blockquote>
<p>ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰æ˜¯æ—¶é—´åºåˆ—å…³ç³»æ•°æ®åº“ï¼Œè®°å½•æ‚£è€…äº¤äº’å’ŒåŒ»ç–—äº‹ä»¶éšæ—¶é—´å˜åŒ–çš„æƒ…å†µï¼Œæ˜¯åŒ»ç–—æŠ¤ç†ç ”ç©¶ä¸åº”ç”¨çš„å…³é”®èµ„æºã€‚ç„¶è€Œï¼Œéšç§æ‹…å¿§å’Œç›‘ç®¡é™åˆ¶é˜»ç¢äº†è¿™ç§æ•æ„Ÿæ•°æ®çš„å…±äº«å’Œåˆ©ç”¨ï¼Œå› æ­¤éœ€è¦ç”ŸæˆåˆæˆEHRæ•°æ®é›†ã€‚ä¸åŒäºé€šå¸¸åªç”Ÿæˆç”±ä¸“å®¶é€‰æ‹©ç‰¹å¾æ„æˆçš„åŒ»ç–—è®°å½•çš„ä»¥å¾€EHRåˆæˆæ–¹æ³•ï¼ˆä¾‹å¦‚ä»…åŒ…å«å°‘æ•°ç”Ÿå‘½ä½“å¾æˆ–ç»“æ„åŒ–ä»£ç ï¼‰ï¼Œæˆ‘ä»¬æ¨å‡ºäº†RawMedï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåˆæˆå¤šè¡¨æ—¶é—´åºåˆ—EHRæ•°æ®çš„æ¡†æ¶ï¼Œå®ƒç´§å¯†æ¨¡ä»¿åŸå§‹EHRã€‚RawMedä½¿ç”¨åŸºäºæ–‡æœ¬çš„è¡¨ç¤ºå’Œå‹ç¼©æŠ€æœ¯ï¼Œä»¥æœ€å°çš„é¢„å¤„ç†æ•è·å¤æ‚ç»“æ„å’Œæ—¶é—´åŠ¨æ€ã€‚æˆ‘ä»¬è¿˜ä¸ºå¤šè¡¨æ—¶é—´åºåˆ—åˆæˆEHRæå‡ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯„ä¼°åˆ†å¸ƒç›¸ä¼¼æ€§ã€è¡¨é—´å…³ç³»ã€æ—¶é—´åŠ¨æ€å’Œéšç§ã€‚åœ¨ä¸¤ä¸ªå¼€æºEHRæ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯ï¼ŒRawMedåœ¨ä¿çœŸåº¦å’Œå®ç”¨æ€§æ–¹é¢ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/eunbyeol-cho/RawMed">https://github.com/eunbyeol-cho/RawMed</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06996v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰æ˜¯è®°å½•æ‚£è€…äº’åŠ¨å’ŒåŒ»ç–—äº‹ä»¶çš„æ—¶é—´åºåˆ—å…³ç³»å‹æ•°æ®åº“ï¼Œå¯¹åŒ»ç–—ç ”ç©¶ä¸åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œéšç§é—®é¢˜å’Œç›‘ç®¡é™åˆ¶é˜»ç¢äº†è¿™ç±»æ•æ„Ÿæ•°æ®çš„å…±äº«å’Œåˆ©ç”¨ï¼Œå› æ­¤äº§ç”Ÿäº†åˆæˆEHRæ•°æ®é›†çš„éœ€æ±‚ã€‚æœ¬æ–‡ä»‹ç»äº†RawMedæ¡†æ¶ï¼Œå®ƒèƒ½åˆæˆå¤šè¡¨æ—¶é—´åºåˆ—EHRæ•°æ®ï¼Œç´§å¯†æ¨¡æ‹ŸåŸå§‹EHRsã€‚ä½¿ç”¨åŸºäºæ–‡æœ¬çš„è¡¨ç¤ºå’Œå‹ç¼©æŠ€æœ¯ï¼ŒRawMedèƒ½æ•æ‰å¤æ‚ç»“æ„å’Œæ—¶é—´åŠ¨æ€ï¼Œè€Œæ— éœ€è¿‡å¤šé¢„å¤„ç†ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†é’ˆå¯¹å¤šè¡¨æ—¶é—´åºåˆ—åˆæˆEHRçš„æ–°è¯„ä¼°æ¡†æ¶ï¼Œè¯„ä¼°å…¶åˆ†å¸ƒç›¸ä¼¼æ€§ã€è¡¨é—´å…³ç³»ã€æ—¶é—´åŠ¨æ€å’Œéšç§ã€‚åœ¨ä¸¤é¡¹å¼€æºEHRæ•°æ®é›†ä¸Šçš„éªŒè¯æ˜¾ç¤ºï¼ŒRawMedåœ¨ä¿çœŸåº¦å’Œå®ç”¨æ€§æ–¹é¢ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰æ˜¯åŒ»ç–—é¢†åŸŸé‡è¦çš„æ—¶é—´åºåˆ—å…³ç³»å‹æ•°æ®åº“ï¼Œç”¨äºè®°å½•æ‚£è€…äº’åŠ¨å’ŒåŒ»ç–—äº‹ä»¶ã€‚</li>
<li>éšç§å’Œç›‘ç®¡é—®é¢˜æ˜¯EHRæ•°æ®å…±äº«å’Œåˆ©ç”¨çš„ä¸»è¦éšœç¢ï¼Œéœ€è¦åˆæˆEHRæ•°æ®é›†ã€‚</li>
<li>RawMedæ˜¯é¦–ä¸ªèƒ½åˆæˆå¤šè¡¨æ—¶é—´åºåˆ—EHRæ•°æ®çš„æ¡†æ¶ï¼Œæ¨¡æ‹ŸåŸå§‹EHRsã€‚</li>
<li>RawMedä½¿ç”¨æ–‡æœ¬è¡¨ç¤ºå’Œå‹ç¼©æŠ€æœ¯ï¼Œèƒ½æ•æ‰å¤æ‚ç»“æ„å’Œæ—¶é—´åŠ¨æ€ï¼Œå‡å°‘é¢„å¤„ç†éœ€æ±‚ã€‚</li>
<li>æå‡ºäº†æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œä»åˆ†å¸ƒç›¸ä¼¼æ€§ã€è¡¨é—´å…³ç³»ã€æ—¶é—´åŠ¨æ€å’Œéšç§å››ä¸ªæ–¹é¢è¯„ä¼°åˆæˆEHRæ•°æ®ã€‚</li>
<li>RawMedåœ¨ä¸¤é¡¹å¼€æºEHRæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå…·æœ‰è¾ƒé«˜çš„ä¿çœŸåº¦å’Œå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06996">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c98e708be726f115b35c0ba30714c648.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8874969c5102841a58d41988a9f078b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b517c3eef32045744293b2e65471f701.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Squeeze-the-Soaked-Sponge-Efficient-Off-policy-Reinforcement-Finetuning-for-Large-Language-Model"><a href="#Squeeze-the-Soaked-Sponge-Efficient-Off-policy-Reinforcement-Finetuning-for-Large-Language-Model" class="headerlink" title="Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning   for Large Language Model"></a>Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning   for Large Language Model</h2><p><strong>Authors:Jing Liang, Hongyao Tang, Yi Ma, Jinyi Liu, Yan Zheng, Shuyue Hu, Lei Bai, Jianye Hao</strong></p>
<p>Reinforcement Learning (RL) has demonstrated its potential to improve the reasoning ability of Large Language Models (LLMs). One major limitation of most existing Reinforcement Finetuning (RFT) methods is that they are on-policy RL in nature, i.e., data generated during the past learning process is not fully utilized. This inevitably comes at a significant cost of compute and time, posing a stringent bottleneck on continuing economic and efficient scaling. To this end, we launch the renaissance of off-policy RL and propose Reincarnating Mix-policy Proximal Policy Gradient (ReMix), a general approach to enable on-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix consists of three major components: (1) Mix-policy proximal policy gradient with an increased Update-To-Data (UTD) ratio for efficient training; (2) KL-Convex policy constraint to balance the trade-off between stability and flexibility; (3) Policy reincarnation to achieve a seamless transition from efficient early-stage learning to steady asymptotic improvement. In our experiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base models. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with 0.079M response rollouts, 350 training steps and achieves 63.27%&#x2F;64.39% (for 7B model) with 0.007M&#x2F;0.011M response rollouts, 50&#x2F;75 training steps, on five math reasoning benchmarks (i.e., AIMEâ€™24, AMCâ€™23, Minerva, OlympiadBench, and MATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level performance with an over 30x to 450x reduction in training cost in terms of rollout data volume. In addition, we reveal insightful findings via multifaceted analysis, including the implicit preference for shorter responses due to the Whipping Effect of off-policy discrepancy, the collapse mode of self-reflection behavior under the presence of severe off-policyness, etc. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²ç»æ˜¾ç¤ºå‡ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„æ½œåŠ›ã€‚å¤§å¤šæ•°ç°æœ‰å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰æ–¹æ³•çš„ä¸€ä¸ªä¸»è¦å±€é™æ€§åœ¨äºå®ƒä»¬æœ¬è´¨ä¸Šæ˜¯åŸºäºç­–ç•¥å†…å¼ºåŒ–å­¦ä¹ ï¼ˆOn-Policy RLï¼‰ï¼Œè¿™æ„å‘³ç€è¿‡å»å­¦ä¹ è¿‡ç¨‹ä¸­ç”Ÿæˆçš„æ•°æ®æ²¡æœ‰å¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚è¿™æ— ç–‘ä¼šå¸¦æ¥å·¨å¤§çš„è®¡ç®—å’Œæ—¶é—´æˆæœ¬ï¼Œå¯¹æŒç»­çš„ç»æµå’Œé«˜æ•ˆæ‰©å±•é€ æˆäº†ä¸¥æ ¼çš„ç“¶é¢ˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å‘èµ·äº†ç¦»ç­–ç•¥å¼ºåŒ–å­¦ä¹ ï¼ˆOff-Policy RLï¼‰çš„å¤å…´ï¼Œå¹¶æå‡ºäº†â€œå†ç”Ÿæ··åˆç­–ç•¥è¿‘ç«¯ç­–ç•¥æ¢¯åº¦â€ï¼ˆReMixï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨æ–¹æ³•ï¼Œä½¿åƒPPOå’ŒGRPOè¿™æ ·çš„ç­–ç•¥å†…å¾®è°ƒæ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨ç¦»ç­–ç•¥æ•°æ®ã€‚ReMixä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰é€šè¿‡å¢åŠ æ•°æ®æ›´æ–°æ¯”ä¾‹ï¼ˆUpdate-To-Data, UTDï¼‰å®ç°æ··åˆç­–ç•¥è¿‘ç«¯ç­–ç•¥æ¢¯åº¦ä»¥æé«˜è®­ç»ƒæ•ˆç‡ï¼›ï¼ˆ2ï¼‰ä½¿ç”¨KLæ•£åº¦å‡¸ç­–ç•¥çº¦æŸä»¥å¹³è¡¡ç¨³å®šæ€§å’Œçµæ´»æ€§ä¹‹é—´çš„æƒè¡¡ï¼›ï¼ˆ3ï¼‰ç­–ç•¥é‡ç”Ÿï¼Œå®ç°ä»é«˜æ•ˆæ—©æœŸå­¦ä¹ åˆ°ç¨³å®šæ¸è¿›æ”¹å–„çš„æ— ç¼è¿‡æ¸¡ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬åœ¨PPOã€GRPOä»¥åŠåŸºäº1.5Bå’Œ7Bçš„æ¨¡å‹ä¸Šè®­ç»ƒäº†ä¸€ç³»åˆ—ReMixæ¨¡å‹ã€‚ReMixåœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå³AIMEâ€™24ã€AMCâ€™23ã€Minervaã€OlympiadBenchå’ŒMATH500ï¼‰ä¸Šï¼Œä½¿ç”¨è¾ƒå°‘çš„å“åº”æ»šåŠ¨æ•°æ®ï¼ˆå¯¹äº1.5Bæ¨¡å‹ä¸ºå¹³å‡Pass@1å‡†ç¡®ç‡52.1%ï¼Œä½¿ç”¨0.079Må“åº”æ»šåŠ¨æ•°æ®å’Œ350æ­¥è®­ç»ƒï¼›å¯¹äº7Bæ¨¡å‹ä½¿ç”¨æ»šåŠ¨æ•°æ®0.007Mæˆ–æ›´å¤šæ—¶éœ€è¦ç»è¿‡çš„è°ƒæ•´å“åº”é‡å’Œè®­ç»ƒæ­¥éª¤æ›´å°‘ï¼‰ï¼Œæ˜¾ç¤ºå‡ºå“è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚ç›¸è¾ƒäºå…¶ä»–è¿‘æœŸçš„å…ˆè¿›æ¨¡å‹ï¼ŒReMixåœ¨æ»šåŠ¨æ•°æ®é‡æ–¹é¢å°†è®­ç»ƒæˆæœ¬å‡å°‘äº†é«˜è¾¾æ•°ç™¾å€ï¼ˆæœ€å¤šè¾¾çº¦å››ç™¾äº”åå€ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡å¤šæ–¹é¢çš„åˆ†ææ­ç¤ºäº†æ·±å…¥çš„è§è§£ï¼ŒåŒ…æ‹¬ç”±äºç¦»ç­–ç•¥å·®å¼‚é€ æˆçš„é­æ‰“æ•ˆåº”å¯¼è‡´çš„å¯¹è¾ƒçŸ­ç­”æ¡ˆçš„éšæ€§åå¥½ï¼Œä»¥åŠåœ¨ä¸¥é‡ç¦»ç­–ç•¥æƒ…å†µä¸‹è‡ªæˆ‘åæ€è¡Œä¸ºçš„å´©æºƒæ¨¡å¼ç­‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06892v1">PDF</a> Preliminary version. Project page:   <a target="_blank" rel="noopener" href="https://anitaleungxx.github.io/ReMix">https://anitaleungxx.github.io/ReMix</a></p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰å¤§éƒ¨åˆ†å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰æ–¹æ³•éƒ½æ˜¯åŸºäºç­–ç•¥çš„ï¼Œé™åˆ¶äº†æ•°æ®çš„å……åˆ†åˆ©ç”¨å¹¶å¸¦æ¥æ—¶é—´å’Œæˆæœ¬çš„æµªè´¹ã€‚æœ¬æ–‡æå‡ºReMixæ–¹æ³•ï¼Œç»“åˆæ··åˆç­–ç•¥è¿‘ç«¯ç­–ç•¥æ¢¯åº¦ç®—æ³•ã€KLæ•£åº¦å‡¸ç­–ç•¥çº¦æŸå’Œç­–ç•¥é‡ç”ŸæŠ€æœ¯ï¼Œä½¿RFTæ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨éç­–ç•¥æ•°æ®ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReMixåœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå–å¾—æœ€ä½³è¡¨ç°ï¼Œä¸”åœ¨æ»šåŠ¨æ•°æ®é‡ä¸Šå‡å°‘è®­ç»ƒæˆæœ¬è¾¾å‡ åå€ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜é€šè¿‡å¤šæ–¹é¢çš„åˆ†ææ­ç¤ºäº†ä¸€äº›æœ‰è¶£çš„å‘ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>ç°æœ‰çš„å¼ºåŒ–å¾®è°ƒæ–¹æ³•å¤§å¤šå­˜åœ¨æµªè´¹è®¡ç®—å’Œæ—¶é—´èµ„æºçš„ç“¶é¢ˆã€‚</li>
<li>ReMixæ˜¯ä¸€ä¸ªåŸºäºç­–ç•¥ä¸éç­–ç•¥çš„æ··åˆå­¦ä¹ æŠ€æœ¯ï¼Œä½¿å¾—å¼ºåŒ–å¾®è°ƒæ–¹æ³•å¯ä»¥å……åˆ†åˆ©ç”¨å†å²æ•°æ®ã€‚</li>
<li>ReMixé€šè¿‡ç»“åˆæ··åˆç­–ç•¥è¿‘ç«¯ç­–ç•¥æ¢¯åº¦ç®—æ³•ã€KLæ•£åº¦å‡¸ç­–ç•¥çº¦æŸå’Œç­–ç•¥é‡ç”ŸæŠ€æœ¯å®ç°é«˜æ•ˆè®­ç»ƒã€‚</li>
<li>ReMixåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œä¸å…ˆè¿›æ¨¡å‹ç›¸æ¯”å¤§å¹…é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚</li>
<li>ReMixæ­ç¤ºäº†æœ‰è¶£çš„ç°è±¡ï¼Œä¾‹å¦‚éç­–ç•¥æ•°æ®äº§ç”Ÿçš„é­å­æ•ˆåº”å¯¼è‡´å¯¹ç®€çŸ­å›åº”çš„éšæ€§åå¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06892">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db11c9c148a43e2abb4e4ac929407d1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58b0e54a9d9e342f20d565a3ce828df8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="GreenHyperSpectra-A-multi-source-hyperspectral-dataset-for-global-vegetation-trait-prediction"><a href="#GreenHyperSpectra-A-multi-source-hyperspectral-dataset-for-global-vegetation-trait-prediction" class="headerlink" title="GreenHyperSpectra: A multi-source hyperspectral dataset for global   vegetation trait prediction"></a>GreenHyperSpectra: A multi-source hyperspectral dataset for global   vegetation trait prediction</h2><p><strong>Authors:Eya Cherif, Arthur Ouaknine, Luke A. Brown, Phuong D. Dao, Kyle R. Kovach, Bing Lu, Daniel Mederer, Hannes Feilhauer, Teja Kattenborn, David Rolnick</strong></p>
<p>Plant traits such as leaf carbon content and leaf mass are essential variables in the study of biodiversity and climate change. However, conventional field sampling cannot feasibly cover trait variation at ecologically meaningful spatial scales. Machine learning represents a valuable solution for plant trait prediction across ecosystems, leveraging hyperspectral data from remote sensing. Nevertheless, trait prediction from hyperspectral data is challenged by label scarcity and substantial domain shifts (\eg across sensors, ecological distributions), requiring robust cross-domain methods. Here, we present GreenHyperSpectra, a pretraining dataset encompassing real-world cross-sensor and cross-ecosystem samples designed to benchmark trait prediction with semi- and self-supervised methods. We adopt an evaluation framework encompassing in-distribution and out-of-distribution scenarios. We successfully leverage GreenHyperSpectra to pretrain label-efficient multi-output regression models that outperform the state-of-the-art supervised baseline. Our empirical analyses demonstrate substantial improvements in learning spectral representations for trait prediction, establishing a comprehensive methodological framework to catalyze research at the intersection of representation learning and plant functional traits assessment. All code and data are available at: <a target="_blank" rel="noopener" href="https://github.com/echerif18/HyspectraSSL">https://github.com/echerif18/HyspectraSSL</a>. </p>
<blockquote>
<p>æ¤ç‰©ç‰¹æ€§ï¼Œå¦‚å¶ç‰‡ç¢³å«é‡å’Œå¶ç‰‡è´¨é‡ï¼Œåœ¨ç”Ÿç‰©å¤šæ ·æ€§å’Œæ°”å€™å˜åŒ–çš„ç ”ç©¶ä¸­éƒ½æ˜¯é‡è¦çš„å˜é‡ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„ç”°é—´é‡‡æ ·æ— æ³•åœ¨å®é™…ç”Ÿæ€ç©ºé—´å°ºåº¦ä¸Šè¦†ç›–ç‰¹æ€§çš„å˜å¼‚ã€‚æœºå™¨å­¦ä¹ åœ¨åˆ©ç”¨é¥æ„Ÿè¶…å…‰è°±æ•°æ®è¿›è¡Œç”Ÿæ€ç³»ç»Ÿé—´ç‰¹æ€§é¢„æµ‹æ–¹é¢è¡¨ç°å‡ºå®è´µçš„ä»·å€¼ã€‚ç„¶è€Œï¼Œä»è¶…å…‰è°±æ•°æ®è¿›è¡Œç‰¹æ€§é¢„æµ‹é¢ä¸´ç€æ ‡ç­¾ç¨€ç¼ºå’Œæ˜¾è‘—é¢†åŸŸè½¬ç§»ï¼ˆä¾‹å¦‚è·¨ä¼ æ„Ÿå™¨ã€ç”Ÿæ€åˆ†å¸ƒï¼‰çš„æŒ‘æˆ˜ï¼Œè¿™éœ€è¦ç¨³å¥çš„è·¨åŸŸæ–¹æ³•ã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†GreenHyperSpectraæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«ç°å®ä¸–ç•Œè·¨ä¼ æ„Ÿå™¨å’Œè·¨ç”Ÿæ€ç³»ç»Ÿæ ·æœ¬çš„é¢„è®­ç»ƒæ•°æ®é›†ï¼Œæ—¨åœ¨ä½¿ç”¨åŠç›‘ç£å’Œè‡ªç›‘ç£æ–¹æ³•å¯¹ç‰¹æ€§é¢„æµ‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ä¸ªæ¶µç›–å†…éƒ¨åˆ†å¸ƒå’Œå¤–éƒ¨åˆ†å¸ƒåœºæ™¯çš„è¯„ä»·æ¡†æ¶ã€‚æˆ‘ä»¬æˆåŠŸåœ°åˆ©ç”¨GreenHyperSpectraå¯¹æ ‡ç­¾é«˜æ•ˆçš„å¤šè¾“å‡ºå›å½’æ¨¡å‹è¿›è¡Œäº†é¢„è®­ç»ƒï¼Œè¯¥æ¨¡å‹ä¼˜äºæœ€æ–°çš„ç›‘ç£åŸºçº¿ã€‚æˆ‘ä»¬çš„å®è¯åˆ†ææ˜¾ç¤ºï¼Œåœ¨å­¦ä¹ ç‰¹æ€§é¢„æµ‹çš„è°±è¡¨ç¤ºæ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„æ–¹æ³•è®ºæ¡†æ¶ï¼Œä»¥æ¨åŠ¨è¡¨å¾å­¦ä¹ å’Œæ¤ç‰©åŠŸèƒ½ç‰¹æ€§è¯„ä¼°äº¤å‰é¢†åŸŸçš„ç ”ç©¶ã€‚æ‰€æœ‰ä»£ç å’Œæ•°æ®å‡å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/echerif18/HyspectraSSL">https://github.com/echerif18/HyspectraSSL</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06806v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ¤ç‰©ç‰¹æ€§ï¼ˆå¦‚å¶ç‰‡ç¢³å«é‡å’Œå¶ç‰‡è´¨é‡ï¼‰åœ¨ç”Ÿç‰©å¤šæ ·æ€§å’Œæ°”å€™å˜åŒ–ç ”ç©¶ä¸­çš„é‡è¦æ€§ã€‚ä¼ ç»Ÿé‡‡æ ·æ–¹æ³•æ— æ³•è¦†ç›–ç”Ÿæ€ä¸Šæœ‰æ„ä¹‰çš„ç©ºé—´å°ºåº¦çš„ç‰¹æ€§å˜åŒ–ã€‚æœºå™¨å­¦ä¹ åˆ©ç”¨é¥æ„Ÿé«˜å…‰è°±æ•°æ®è¿›è¡Œæ¤ç‰©ç‰¹æ€§é¢„æµ‹å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºæ ‡ç­¾ç¨€ç¼ºå’Œé¢†åŸŸå·®å¼‚æ˜¾è‘—ï¼ˆå¦‚ä¸åŒä¼ æ„Ÿå™¨ã€ç”Ÿæ€åˆ†å¸ƒï¼‰ï¼Œç‰¹æ€§é¢„æµ‹é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†GreenHyperSpectraæ•°æ®é›†ï¼ŒåŒ…å«çœŸå®è·¨ä¼ æ„Ÿå™¨å’Œè·¨ç”Ÿæ€ç³»ç»Ÿæ ·æœ¬ï¼Œæ—¨åœ¨é€šè¿‡åŠç›‘ç£å’Œè‡ªç›‘ç£æ–¹æ³•è¯„ä¼°ç‰¹æ€§é¢„æµ‹æ€§èƒ½ã€‚é€šè¿‡é‡‡ç”¨æ¶µç›–å†…éƒ¨å’Œå¤–éƒ¨åˆ†å¸ƒåœºæ™¯çš„è¯„ä»·æ¡†æ¶ï¼ŒæˆåŠŸåˆ©ç”¨GreenHyperSpectraè¿›è¡Œé¢„è®­ç»ƒæ ‡ç­¾é«˜æ•ˆçš„å¤šè¾“å‡ºå›å½’æ¨¡å‹ï¼Œä¼˜äºç°æœ‰ç›‘ç£åŸºçº¿æ–¹æ³•ã€‚å®è¯åˆ†ææ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç‰¹æ€§é¢„æµ‹å­¦ä¹ å…‰è°±è¡¨ç¤ºæ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œä¸ºè¡¨å¾å­¦ä¹ å’Œæ¤ç‰©åŠŸèƒ½ç‰¹æ€§è¯„ä¼°ç ”ç©¶æä¾›äº†å…¨é¢çš„æ–¹æ³•è®ºæ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¤ç‰©ç‰¹æ€§ï¼ˆå¦‚å¶ç‰‡ç¢³å«é‡å’Œå¶ç‰‡è´¨é‡ï¼‰åœ¨ç”Ÿç‰©å¤šæ ·æ€§å’Œæ°”å€™å˜åŒ–ç ”ç©¶ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>ä¼ ç»Ÿé‡‡æ ·æ–¹æ³•æ— æ³•è¦†ç›–ç”Ÿæ€ä¸Šæœ‰æ„ä¹‰çš„ç©ºé—´å°ºåº¦çš„ç‰¹æ€§å˜åŒ–ã€‚</li>
<li>æœºå™¨å­¦ä¹ åˆ©ç”¨é¥æ„Ÿé«˜å…‰è°±æ•°æ®è¿›è¡Œæ¤ç‰©ç‰¹æ€§é¢„æµ‹å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>ç‰¹æ€§é¢„æµ‹é¢ä¸´æ ‡ç­¾ç¨€ç¼ºå’Œé¢†åŸŸå·®å¼‚æ˜¾è‘—çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºGreenHyperSpectraæ•°æ®é›†ï¼Œç”¨äºè·¨ä¼ æ„Ÿå™¨å’Œè·¨ç”Ÿæ€ç³»ç»Ÿçš„æ¤ç‰©ç‰¹æ€§é¢„æµ‹è¯„ä¼°ã€‚</li>
<li>é€šè¿‡é¢„è®­ç»ƒæ ‡ç­¾é«˜æ•ˆçš„å¤šè¾“å‡ºå›å½’æ¨¡å‹ï¼ŒGreenHyperSpectraè¡¨ç°ä¼˜äºç°æœ‰ç›‘ç£åŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06806">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-021fa0dab89cdaa423173c10475524cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d080fbaf1e3d5169a0ee24a03aae395.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95b7b590c909e67b5a9fc5e6cca7baaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7051ec2e4c3963ea2770a723be4fb108.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43eaab85631bc18431454d62ce45b54b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3845f37a33bc76a4ff33b3a1fd67c56f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Towards-Solving-More-Challenging-IMO-Problems-via-Decoupled-Reasoning-and-Proving"><a href="#Towards-Solving-More-Challenging-IMO-Problems-via-Decoupled-Reasoning-and-Proving" class="headerlink" title="Towards Solving More Challenging IMO Problems via Decoupled Reasoning   and Proving"></a>Towards Solving More Challenging IMO Problems via Decoupled Reasoning   and Proving</h2><p><strong>Authors:Zhenwen Liang, Linfeng Song, Yang Li, Tao Yang, Feng Zhang, Haitao Mi, Dong Yu</strong></p>
<p>Automated Theorem Proving (ATP) in formal languages is a foundational challenge for AI. While Large Language Models (LLMs) have driven remarkable progress, a significant gap remains between their powerful informal reasoning capabilities and their weak formal proving performance. Recent studies show that the informal accuracy exceeds 80% while formal success remains below 8% on benchmarks like PutnamBench. We argue this gap persists because current state-of-the-art provers, by tightly coupling reasoning and proving, are trained with paradigms that inadvertently punish deep reasoning in favor of shallow, tactic-based strategies. To bridge this fundamental gap, we propose a novel framework that decouples high-level reasoning from low-level proof generation. Our approach utilizes two distinct, specialized models: a powerful, general-purpose Reasoner to generate diverse, strategic subgoal lemmas, and an efficient Prover to rigorously verify them. This modular design liberates the modelâ€™s full reasoning potential and bypasses the pitfalls of end-to-end training. We evaluate our method on a challenging set of post-2000 IMO problems, a problem set on which no prior open-source prover has reported success. Our decoupled framework successfully solves 5 of these problems, demonstrating a significant step towards automated reasoning on exceptionally difficult mathematical challenges. To foster future research, we release our full dataset of generated and verified lemmas for a wide range of IMO problems, available at <a target="_blank" rel="noopener" href="https://tencent-imo.github.io/">https://tencent-imo.github.io/</a> . </p>
<blockquote>
<p>å½¢å¼åŒ–è¯­è¨€ä¸­çš„è‡ªåŠ¨åŒ–å®šç†è¯æ˜ï¼ˆATPï¼‰æ˜¯äººå·¥æ™ºèƒ½é¢ä¸´çš„ä¸€é¡¹åŸºç¡€æŒ‘æˆ˜ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨éæ­£å¼æ¨ç†èƒ½åŠ›å’Œæ­£å¼è¯æ˜æ€§èƒ½ä¹‹é—´ä»å­˜åœ¨å¾ˆå¤§å·®è·ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œåœ¨PutnamBenchç­‰åŸºå‡†æµ‹è¯•ä¸Šï¼Œéæ­£å¼å‡†ç¡®æ€§è¶…è¿‡80%ï¼Œè€Œæ­£å¼æˆåŠŸç‡ä»ä½äº8%ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ç§å·®è·æŒç»­å­˜åœ¨ï¼Œæ˜¯å› ä¸ºå½“å‰æœ€å…ˆè¿›çš„è¯æ˜å™¨é€šè¿‡ç´§å¯†è€¦åˆæ¨ç†å’Œè¯æ˜ï¼Œé‡‡ç”¨äº†ä¸€ç§èŒƒå¼ï¼Œè¿™ç§èŒƒå¼æ— æ„ä¸­æƒ©ç½šäº†æ·±åº¦æ¨ç†è€Œåå‘äºæµ…å±‚æ¬¡çš„ã€åŸºäºç­–ç•¥çš„æ–¹æ³•ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€åŸºæœ¬å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå®ƒå°†é«˜çº§æ¨ç†ä¸ä½çº§è¯æ˜ç”Ÿæˆç›¸åˆ†ç¦»ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ä¸¤ä¸ªæˆªç„¶ä¸åŒçš„ä¸“ä¸šæ¨¡å‹ï¼šä¸€ä¸ªåŠŸèƒ½å¼ºå¤§ã€é€šç”¨çš„æ¨ç†æœºï¼Œç”¨äºç”Ÿæˆå¤šæ ·åŒ–ã€ç­–ç•¥æ€§çš„å­ç›®æ ‡å¼•ç†ï¼Œä»¥åŠä¸€ä¸ªé«˜æ•ˆçš„è¯æ˜å™¨ï¼Œç”¨äºä¸¥æ ¼éªŒè¯è¿™äº›å¼•ç†ã€‚è¿™ç§æ¨¡å—åŒ–è®¾è®¡é‡Šæ”¾äº†æ¨¡å‹çš„å…¨éƒ¨æ¨ç†æ½œåŠ›ï¼Œå¹¶é¿å…äº†ç«¯åˆ°ç«¯åŸ¹è®­çš„é™·é˜±ã€‚æˆ‘ä»¬åœ¨ä¸€ç»„å…·æœ‰æŒ‘æˆ˜æ€§çš„2000å¹´ä»¥åä¸¾åŠçš„å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ç«èµ›é—®é¢˜ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚åœ¨æ­¤é—®é¢˜ä¸Šï¼Œå°šæ— å…ˆå‰çš„å¼€æºè¯æ˜å™¨æŠ¥å‘ŠæˆåŠŸã€‚æˆ‘ä»¬çš„è§£è€¦æ¡†æ¶æˆåŠŸè§£å†³äº†å…¶ä¸­çš„äº”ä¸ªé—®é¢˜ï¼Œæœç€åœ¨æå…¶å›°éš¾çš„æ•°å­¦æŒ‘æˆ˜ä¸­å®ç°è‡ªåŠ¨åŒ–æ¨ç†è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚ä¸ºäº†ä¿ƒè¿›æœªæ¥ç ”ç©¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ç³»åˆ—ç”Ÿæˆçš„å·²éªŒè¯å¼•ç†çš„å…¨æ•°æ®é›†ï¼Œæ¶µç›–å¹¿æ³›çš„IMOé—®é¢˜ï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://tencent-imo.github.io/%E8%8E%B7%E5%8F%96%E3%80%82">https://tencent-imo.github.io/è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06804v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong>ï¼šäººå·¥æ™ºèƒ½åœ¨å½¢å¼è¯­è¨€ä¸­çš„è‡ªåŠ¨å®šç†è¯æ˜ï¼ˆATPï¼‰æ˜¯ä¸€ä¸ªåŸºç¡€æŒ‘æˆ˜ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨è¯¸å¦‚PutnamBenchç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œå®ƒä»¬åœ¨å½¢å¼è¯æ˜æ–¹é¢çš„è¡¨ç°ä»ç„¶è¾ƒå¼±ï¼Œä¸åˆ°ç™¾åˆ†ä¹‹å…«çš„æ­£å¼è¯æ˜æˆåŠŸç‡ã€‚ç ”ç©¶è€…æå‡ºè¿™ç§å·®è·å­˜åœ¨æ˜¯å› ä¸ºç›®å‰é¡¶å°–çš„è‡ªåŠ¨è¯æ˜å™¨åœ¨è®¾è®¡ä¸Šè¿‡äºç´§å¯†åœ°è€¦åˆäº†æ¨ç†å’Œè¯æ˜è¿‡ç¨‹ï¼Œè®­ç»ƒèŒƒå¼æ— æ„ä¸­åå‘äºæµ…å±‚æ¬¡çš„æˆ˜æœ¯ç­–ç•¥è€Œéæ·±åº¦æ¨ç†ã€‚ä¸ºäº†ç¼©å°è¿™ä¸€åŸºæœ¬å·®è·ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†é«˜çº§æ¨ç†ä¸ä½çº§è¯æ˜ç”Ÿæˆè¿‡ç¨‹è§£è€¦ã€‚è¿™ä¸€æ–¹æ³•é‡‡ç”¨ä¸¤ä¸ªä¸“é—¨è®¾è®¡çš„æ¨¡å‹ï¼šä¸€ä¸ªå¼ºå¤§çš„é€šç”¨æ¨ç†å™¨ç”Ÿæˆå¤šç§ç­–ç•¥æ€§å­ç›®æ ‡å¼•ç†ï¼Œä¸€ä¸ªé«˜æ•ˆçš„è¯æ˜å™¨åˆ™å¯¹å…¶è¿›è¡Œä¸¥æ ¼éªŒè¯ã€‚è¿™ç§æ¨¡å—åŒ–è®¾è®¡é‡Šæ”¾äº†æ¨¡å‹çš„å…¨éƒ¨æ¨ç†æ½œåŠ›ï¼Œé¿å…äº†ç«¯åˆ°ç«¯è®­ç»ƒçš„å±€é™æ€§ã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å2000å¹´IMOé—®é¢˜ä¸Šå–å¾—äº†æˆåŠŸï¼Œè§£å†³äº†å…¶ä¸­çš„äº”ä¸ªé—®é¢˜ã€‚ä¸ºäº†æ¨åŠ¨æœªæ¥çš„ç ”ç©¶ï¼Œç ”ç©¶è€…å‘å¸ƒäº†å¹¿æ³›çš„IMOé—®é¢˜ç”Ÿæˆçš„éªŒè¯å¼•ç†æ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li>å½“å‰äººå·¥æ™ºèƒ½åœ¨è‡ªåŠ¨å®šç†è¯æ˜é¢†åŸŸå­˜åœ¨ä»éæ­£å¼åˆ°æ­£å¼è¯æ˜çš„å·¨å¤§æ€§èƒ½å·®è·ã€‚</li>
<li>ä¸»æµè¯æ˜å™¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åå‘äºæˆ˜æœ¯æ€§ç­–ç•¥è€Œéæ·±åº¦æ¨ç†ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå°†é«˜çº§æ¨ç†ä¸ä½çº§è¯æ˜è¿‡ç¨‹è§£è€¦ï¼Œä»¥æé«˜è¯æ˜æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼ŒåŒ…æ‹¬ä¸€ä¸ªæ¨ç†å™¨ç”Ÿæˆæˆ˜ç•¥æ€§å­ç›®æ ‡å¼•ç†å’Œä¸€ä¸ªè¯æ˜å™¨è¿›è¡Œä¸¥æ ¼éªŒè¯ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„å2000å¹´IMOé—®é¢˜ä¸Šå–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼ŒæˆåŠŸè§£å†³äº†äº”ä¸ªé—®é¢˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e4abae0dcf08c1af685d6bf29aeb76d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3b535a3d9677727662bfd7c715dbf8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed3081b55dc8df4ba942343d275a7ccf.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Decoder-Hybrid-Decoder-Architecture-for-Efficient-Reasoning-with-Long-Generation"><a href="#Decoder-Hybrid-Decoder-Architecture-for-Efficient-Reasoning-with-Long-Generation" class="headerlink" title="Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long   Generation"></a>Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long   Generation</h2><p><strong>Authors:Liliang Ren, Congcong Chen, Haoran Xu, Young Jin Kim, Adam Atkinson, Zheng Zhan, Jiankai Sun, Baolin Peng, Liyuan Liu, Shuohang Wang, Hao Cheng, Jianfeng Gao, Weizhu Chen, Yelong Shen</strong></p>
<p>Recent advances in language modeling have demonstrated the effectiveness of State Space Models (SSMs) for efficient sequence modeling. While hybrid architectures such as Samba and the decoder-decoder architecture, YOCO, have shown promising performance gains over Transformers, prior works have not investigated the efficiency potential of representation sharing between SSM layers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet effective mechanism for efficient memory sharing across layers. We apply it to create SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in the cross-decoder to share memory readout states from a Samba-based self-decoder. SambaY significantly enhances decoding efficiency, preserves linear pre-filling time complexity, and boosts long-context performance, all while eliminating the need for explicit positional encoding. Through extensive scaling experiments, we demonstrate that our model exhibits a significantly lower irreducible loss compared to a strong YOCO baseline, indicating superior performance scalability under large-scale compute regimes. Our largest model enhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves significantly better performance than Phi4-mini-Reasoning on reasoning tasks such as Math500, AIME24&#x2F;25, and GPQA Diamond without any reinforcement learning, while delivering up to 10x higher decoding throughput on 2K-length prompts with 32K generation length under the vLLM inference framework. We release our training codebase on open-source data at <a target="_blank" rel="noopener" href="https://github.com/microsoft/ArchScale">https://github.com/microsoft/ArchScale</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„è¯­è¨€å»ºæ¨¡è¿›å±•è¡¨æ˜çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰åœ¨é«˜æ•ˆåºåˆ—å»ºæ¨¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚è™½ç„¶æ··åˆæ¶æ„ï¼ˆå¦‚sambaå’Œç¼–ç å™¨-è§£ç å™¨æ¶æ„YOCOï¼‰åœ¨Transformerä¸Šæ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„æ€§èƒ½æå‡ï¼Œä½†ä¹‹å‰çš„ç ”ç©¶å°šæœªæ¢è®¨SSMå±‚ä¹‹é—´è¡¨ç¤ºå…±äº«çš„æ½œåœ¨æ•ˆç‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†é—¨æ§å†…å­˜å•å…ƒï¼ˆGMUï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„è·¨å±‚é«˜æ•ˆå†…å­˜å…±äº«æœºåˆ¶ã€‚æˆ‘ä»¬å°†å…¶åº”ç”¨äºåˆ›å»ºsambaYï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†GMUçš„è§£ç å™¨æ··åˆè§£ç å™¨æ¶æ„ï¼Œå¯ä»åŸºäºsambaçš„è‡ªæˆ‘è§£ç å™¨ä¸­å…±äº«å†…å­˜è¯»å–çŠ¶æ€ã€‚sambaYåœ¨æ˜¾è‘—æé«˜è§£ç æ•ˆç‡çš„åŒæ—¶ï¼Œä¿ç•™äº†çº¿æ€§é¢„å¡«å……æ—¶é—´å¤æ‚åº¦ï¼Œå¹¶æé«˜äº†é•¿ä¸Šä¸‹æ–‡æ€§èƒ½ï¼ŒåŒæ—¶æ— éœ€æ˜¾å¼ä½ç½®ç¼–ç ã€‚é€šè¿‡å¹¿æ³›çš„è§„æ¨¡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜ä¸å¼ºå¤§çš„YOCOåŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å…·æœ‰æ›´ä½çš„ä¸å¯çº¦æŸå¤±ï¼Œè¿™è¡¨æ˜åœ¨å¤§è§„æ¨¡è®¡ç®—ç¯å¢ƒä¸‹å…·æœ‰å“è¶Šçš„æ€§èƒ½å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬é‡‡ç”¨å·®åˆ†æ³¨æ„åŠ›å¢å¼ºçš„æœ€å¤§æ¨¡å‹ï¼Œåœ¨Math500ã€AIME24&#x2F;25å’ŒGPQA Diamondç­‰æ¨ç†ä»»åŠ¡ä¸Šï¼Œç›¸è¾ƒäºPhi4-mini-Reasoningå–å¾—äº†æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œè¿™ä¸€åˆ‡éƒ½æ˜¯åœ¨æœªä½¿ç”¨å¼ºåŒ–å­¦ä¹ çš„æƒ…å†µä¸‹å®ç°çš„ã€‚æ­¤å¤–ï¼Œåœ¨vLLMæ¨ç†æ¡†æ¶ä¸‹ï¼Œå¯¹2Ké•¿åº¦æç¤ºè¿›è¡Œ32Kç”Ÿæˆé•¿åº¦æ—¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è§£ç ååé‡æé«˜äº†é«˜è¾¾10å€ã€‚æˆ‘ä»¬åœ¨å¼€æºæ•°æ®ä¸Šå‘å¸ƒæˆ‘ä»¬çš„è®­ç»ƒä»£ç åº“ï¼š<a target="_blank" rel="noopener" href="https://github.com/microsoft/ArchScale%E3%80%82">https://github.com/microsoft/ArchScaleã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06607v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºState Space Modelsï¼ˆSSMsï¼‰çš„Gated Memory Unitï¼ˆGMUï¼‰æœºåˆ¶ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„è·¨å±‚å†…å­˜å…±äº«æ–¹æ³•ã€‚é€šè¿‡åˆ›å»ºSambaYæ¶æ„ï¼Œå°†GMUåº”ç”¨äºè§£ç å™¨æ··åˆæ¶æ„ä¸­ï¼Œå®ç°äº†é«˜æ•ˆçš„å†…å­˜å…±äº«ã€‚SambaYæ¶æ„æé«˜äº†è§£ç æ•ˆç‡ï¼Œä¿æŒçº¿æ€§é¢„å¡«å……æ—¶é—´å¤æ‚åº¦ï¼Œæé«˜äº†é•¿ä¸Šä¸‹æ–‡æ€§èƒ½ï¼Œå¹¶æ¶ˆé™¤äº†å¯¹æ˜¾å¼ä½ç½®ç¼–ç çš„éœ€æ±‚ã€‚åŒæ—¶å±•ç¤ºçš„å¤§è§„æ¨¡è®¡ç®—å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹ç›¸æ¯”åŸºå‡†æ¨¡å‹YOCOå…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿å’Œæ›´ä½çš„ä¸å¯å‡å°‘çš„æŸå¤±ã€‚æœ€ç»ˆå®ç°çš„æ¨¡å‹åœ¨æ— éœ€å¼ºåŒ–å­¦ä¹ çš„æƒ…å†µä¸‹åœ¨å¤šä¸ªæ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜äºå…¶ä»–æ¨¡å‹çš„è¡¨ç°ï¼ŒåŒæ—¶åœ¨å¤§è§„æ¨¡ç”Ÿæˆåœºæ™¯ä¸‹å®ç°äº†æ›´é«˜çš„è§£ç æ•ˆç‡ã€‚è¯¥é¡¹ç›®å·²ç»å¼€æºç›¸å…³è®­ç»ƒä»£ç å’Œæ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†State Space Modelsï¼ˆSSMsï¼‰çš„Gated Memory Unitï¼ˆGMUï¼‰æœºåˆ¶ç”¨äºé«˜æ•ˆåºåˆ—å»ºæ¨¡ã€‚</li>
<li>æå‡ºSambaYæ¶æ„ï¼Œå°†GMUåº”ç”¨äºè§£ç å™¨æ··åˆæ¶æ„ä¸­å®ç°è·¨å±‚å†…å­˜é«˜æ•ˆå…±äº«ã€‚</li>
<li>SambaYæ¶æ„æé«˜äº†è§£ç æ•ˆç‡ï¼Œä¿æŒçº¿æ€§é¢„å¡«å……æ—¶é—´å¤æ‚åº¦ï¼Œæå‡é•¿ä¸Šä¸‹æ–‡æ€§èƒ½ã€‚</li>
<li>æ¶ˆé™¤å¯¹æ˜¾å¼ä½ç½®ç¼–ç çš„éœ€æ±‚ã€‚</li>
<li>æ¨¡å‹ç›¸æ¯”åŸºå‡†æ¨¡å‹YOCOå…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿å’Œæ›´ä½çš„ä¸å¯å‡å°‘çš„æŸå¤±ã€‚</li>
<li>æœ€ç»ˆæ¨¡å‹åœ¨å¤šä¸ªæ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ— éœ€å¼ºåŒ–å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06607">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-313e385a51a07b8814941cf6e9b83a7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff8fb1507919b735dbe7d5fc20b238d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea2b3d659bcb1fb0d93f036ebf5becf2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-672da5fe9223a1c4ee637839ed66de11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36cb5eebef5d59de9be9e79f7603b360.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="From-Data-Centric-to-Sample-Centric-Enhancing-LLM-Reasoning-via-Progressive-Optimization"><a href="#From-Data-Centric-to-Sample-Centric-Enhancing-LLM-Reasoning-via-Progressive-Optimization" class="headerlink" title="From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via   Progressive Optimization"></a>From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via   Progressive Optimization</h2><p><strong>Authors:Xinjie Chen, Minpeng Liao, Guoxin Chen, Chengxi Li, Biao Fu, Kai Fan, Xinggao Liu</strong></p>
<p>Reinforcement learning with verifiable rewards (RLVR) has recently advanced the reasoning capabilities of large language models (LLMs). While prior work has emphasized algorithmic design, data curation, and reward shaping, we investigate RLVR from a sample-centric perspective and introduce LPPO (Learning-Progress and Prefix-guided Optimization), a framework of progressive optimization techniques. Our work addresses a critical question: how to best leverage a small set of trusted, high-quality demonstrations, rather than simply scaling up data volume. First, motivated by how hints aid human problem-solving, we propose prefix-guided sampling, an online data augmentation method that incorporates partial solution prefixes from expert demonstrations to guide the policy, particularly for challenging instances. Second, inspired by how humans focus on important questions aligned with their current capabilities, we introduce learning-progress weighting, a dynamic strategy that adjusts each training sampleâ€™s influence based on model progression. We estimate sample-level learning progress via an exponential moving average of per-sample pass rates, promoting samples that foster learning and de-emphasizing stagnant ones. Experiments on mathematical-reasoning benchmarks demonstrate that our methods outperform strong baselines, yielding faster convergence and a higher performance ceiling. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æœ€è¿‘æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è™½ç„¶ä»¥å‰çš„ç ”ç©¶é‡ç‚¹åœ¨ç®—æ³•è®¾è®¡ã€æ•°æ®æ•´ç†å’Œå¥–åŠ±å¡‘é€ ä¸Šï¼Œä½†æˆ‘ä»¬ä»æ ·æœ¬ä¸­å¿ƒçš„è§’åº¦ç ”ç©¶RLVRï¼Œå¹¶å¼•å…¥äº†LPPOï¼ˆå­¦ä¹ è¿›åº¦å’Œå‰ç¼€å¼•å¯¼ä¼˜åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€å¥—æ¸è¿›ä¼˜åŒ–æŠ€æœ¯çš„æ¡†æ¶ã€‚æˆ‘ä»¬çš„å·¥ä½œè§£å†³äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šå¦‚ä½•æœ€å¥½åœ°åˆ©ç”¨ä¸€å°éƒ¨åˆ†å¯ä¿¡èµ–çš„é«˜è´¨é‡æ¼”ç¤ºï¼Œè€Œä¸æ˜¯ç®€å•åœ°æ‰©å¤§æ•°æ®é‡ã€‚é¦–å…ˆï¼Œå—æç¤ºå¦‚ä½•å¸®åŠ©äººç±»è§£å†³é—®é¢˜çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†å‰ç¼€å¼•å¯¼é‡‡æ ·ï¼Œè¿™æ˜¯ä¸€ç§åœ¨çº¿æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œå®ƒç»“åˆäº†ä¸“å®¶æ¼”ç¤ºä¸­çš„éƒ¨åˆ†è§£å†³æ–¹æ¡ˆå‰ç¼€æ¥å¼•å¯¼ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æƒ…å†µä¸‹ã€‚å…¶æ¬¡ï¼Œå—äººç±»å¦‚ä½•å…³æ³¨ä¸å…¶å½“å‰èƒ½åŠ›ç›¸ç¬¦çš„é‡è¦é—®é¢˜çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†å­¦ä¹ è¿›åº¦åŠ æƒï¼Œè¿™æ˜¯ä¸€ç§åŠ¨æ€ç­–ç•¥ï¼Œæ ¹æ®æ¨¡å‹çš„è¿›å±•è°ƒæ•´æ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„å½±å“ã€‚æˆ‘ä»¬é€šè¿‡æ¯ä¸ªæ ·æœ¬é€šè¿‡ç‡çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡æ¥ä¼°è®¡æ ·æœ¬çº§å­¦ä¹ è¿›åº¦ï¼Œä»¥ä¿ƒè¿›é‚£äº›ä¿ƒè¿›å­¦ä¹ çš„æ ·æœ¬å¹¶å‡å°‘åœæ»çš„æ ·æœ¬çš„é‡è¦æ€§ã€‚åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼Œå®ç°äº†æ›´å¿«çš„æ”¶æ•›å’Œæ›´é«˜çš„æ€§èƒ½ä¸Šé™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06573v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚æœ¬ç ”ç©¶ä»æ ·æœ¬ä¸­å¿ƒè§†è§’å‡ºå‘ï¼Œå¼•å…¥LPPOï¼ˆå­¦ä¹ è¿›åº¦å’Œå‰ç¼€å¼•å¯¼ä¼˜åŒ–ï¼‰æ¡†æ¶ï¼Œèšç„¦ä¼˜åŒ–æŠ€æœ¯ã€‚ç ”ç©¶é‡ç‚¹åœ¨äºå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å°‘é‡å¯ä¿¡èµ–çš„é«˜è´¨é‡æ¼”ç¤ºï¼Œè€Œéå•çº¯æ‰©å¤§æ•°æ®é‡ã€‚é€šè¿‡å‰ç¼€å¼•å¯¼é‡‡æ ·å’ŒåŸºäºå­¦ä¹ è¿›åº¦çš„åŠ æƒç­–ç•¥ï¼Œå®ç°åœ¨çº¿æ•°æ®æ‰©å……å’ŒåŠ¨æ€è°ƒæ•´æ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„å½±å“ã€‚åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºå¼ºåŠ²åŸºå‡†æµ‹è¯•ï¼Œå®ç°æ›´å¿«æ”¶æ•›å’Œæ›´é«˜æ€§èƒ½ä¸Šé™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLVRå¼ºåŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä»æ ·æœ¬ä¸­å¿ƒè§†è§’ç ”ç©¶RLVRï¼Œå¼•å…¥LPPOæ¡†æ¶ã€‚</li>
<li>æœ‰æ•ˆåˆ©ç”¨å°‘é‡é«˜è´¨é‡æ¼”ç¤ºæ•°æ®ã€‚</li>
<li>æå‡ºå‰ç¼€å¼•å¯¼é‡‡æ ·ï¼Œèåˆä¸“å®¶æ¼”ç¤ºä¸­çš„éƒ¨åˆ†è§£å†³æ–¹æ¡ˆå‰ç¼€ï¼Œä»¥æŒ‡å¯¼ç­–ç•¥ï¼Œå°¤å…¶é’ˆå¯¹æŒ‘æˆ˜å®ä¾‹ã€‚</li>
<li>å—äººç±»å…³æ³¨ä¸å½“å‰èƒ½åŠ›ç›¸ç¬¦çš„é‡è¦é—®é¢˜çš„å¯å‘ï¼Œå¼•å…¥åŸºäºå­¦ä¹ è¿›åº¦çš„åŠ æƒç­–ç•¥ã€‚</li>
<li>é€šè¿‡æ ·æœ¬çº§åˆ«çš„å­¦ä¹ è¿›åº¦ä¼°è®¡ï¼ŒåŠ¨æ€è°ƒæ•´æ ·æœ¬å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06573">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dbfa96d719191589600a394079cbed97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd8b3ca11f86fecc7960051bdc125b6d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Perception-Aware-Policy-Optimization-for-Multimodal-Reasoning"><a href="#Perception-Aware-Policy-Optimization-for-Multimodal-Reasoning" class="headerlink" title="Perception-Aware Policy Optimization for Multimodal Reasoning"></a>Perception-Aware Policy Optimization for Multimodal Reasoning</h2><p><strong>Authors:Zhenhailong Wang, Xuehang Guo, Sofia Stoica, Haiyang Xu, Hongru Wang, Hyeonjeong Ha, Xiusi Chen, Yangyi Chen, Ming Yan, Fei Huang, Heng Ji</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a highly effective strategy for endowing Large Language Models (LLMs) with robust multi-step reasoning abilities. However, its design and optimizations remain tailored to purely textual domains, resulting in suboptimal performance when applied to multimodal reasoning tasks. In particular, we observe that a major source of error in current multimodal reasoning lies in the perception of visual inputs. To address this bottleneck, we propose Perception-Aware Policy Optimization (PAPO), a simple yet effective extension of GRPO that encourages the model to learn to perceive while learning to reason, entirely from internal supervision signals. Notably, PAPO does not rely on additional data curation, external reward models, or proprietary models. Specifically, we introduce the Implicit Perception Loss in the form of a KL divergence term to the GRPO objective, which, despite its simplicity, yields significant overall improvements (4.4%) on diverse multimodal benchmarks. The improvements are more pronounced, approaching 8.0%, on tasks with high vision dependency. We also observe a substantial reduction (30.5%) in perception errors, indicating improved perceptual capabilities with PAPO. We conduct comprehensive analysis of PAPO and identify a unique loss hacking issue, which we rigorously analyze and mitigate through a Double Entropy Loss. Overall, our work introduces a deeper integration of perception-aware supervision into RLVR learning objectives and lays the groundwork for a new RL framework that encourages visually grounded reasoning. Project page: <a target="_blank" rel="noopener" href="https://mikewangwzhl.github.io/PAPO">https://mikewangwzhl.github.io/PAPO</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²è¢«è¯æ˜æ˜¯èµ‹äºˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼ºå¤§çš„å¤šæ­¥æ¨ç†èƒ½åŠ›çš„ä¸€ç§é«˜æ•ˆç­–ç•¥ã€‚ç„¶è€Œï¼Œå…¶è®¾è®¡å’Œä¼˜åŒ–ä»ç„¶é’ˆå¯¹çº¯æ–‡æœ¬é¢†åŸŸï¼Œå¯¼è‡´åœ¨åº”ç”¨äºå¤šæ¨¡æ€æ¨ç†ä»»åŠ¡æ—¶çš„æ€§èƒ½ä¸ä½³ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å½“å‰å¤šæ¨¡æ€æ¨ç†ä¸­çš„è¯¯å·®ä¸»è¦æ¥æºäºè§†è§‰è¾“å…¥çš„æ„ŸçŸ¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç“¶é¢ˆï¼Œæˆ‘ä»¬æå‡ºäº†æ„ŸçŸ¥æ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–ï¼ˆPAPOï¼‰ï¼Œè¿™æ˜¯GRPOçš„ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æ‰©å±•ï¼Œé¼“åŠ±æ¨¡å‹åœ¨å­¦ä¹ æ¨ç†çš„åŒæ—¶å­¦ä¹ æ„ŸçŸ¥ï¼Œå®Œå…¨åŸºäºå†…éƒ¨ç›‘ç£ä¿¡å·ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒPAPOä¸ä¾èµ–äºé¢å¤–çš„æ•°æ®æ•´ç†ã€å¤–éƒ¨å¥–åŠ±æ¨¡å‹æˆ–ä¸“æœ‰æ¨¡å‹ã€‚å…·ä½“åœ°ï¼Œæˆ‘ä»¬åœ¨GRPOç›®æ ‡ä¸­å¼•å…¥äº†éšæ„ŸçŸ¥æŸå¤±çš„å½¢å¼ä¸ºKLæ•£åº¦é¡¹ï¼Œå°½ç®¡å…¶ç®€å•æ€§ï¼Œå®ƒåœ¨å„ç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šäº§ç”Ÿäº†æ˜¾è‘—çš„æ€»ä½“æ”¹è¿›ï¼ˆ4.4%ï¼‰ã€‚åœ¨é«˜åº¦ä¾èµ–è§†è§‰çš„ä»»åŠ¡ä¸Šï¼Œæ”¹è¿›æ›´ä¸ºæ˜¾è‘—ï¼Œæ¥è¿‘8.0%ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°æ„ŸçŸ¥é”™è¯¯çš„å¤§å¹…å‡å°‘ï¼ˆ30.5%ï¼‰ï¼Œè¿™è¡¨æ˜PAPOæé«˜äº†æ„ŸçŸ¥èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹PAPOè¿›è¡Œäº†ç»¼åˆåˆ†æï¼Œå¹¶å‘ç°äº†ä¸€ä¸ªç‹¬ç‰¹çš„æŸå¤±ç ´è§£é—®é¢˜ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸¥æ ¼çš„åˆ†æï¼Œå¹¶é€šè¿‡åŒé‡ç†µæŸå¤±å‡è½»äº†è¯¥é—®é¢˜ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„å·¥ä½œå°†æ„ŸçŸ¥æ„ŸçŸ¥ç›‘ç£æ›´æ·±å…¥åœ°é›†æˆåˆ°RLVRå­¦ä¹ ç›®æ ‡ä¸­ï¼Œå¹¶ä¸ºé¼“åŠ±è§†è§‰è¾…åŠ©æ¨ç†çš„æ–°RLæ¡†æ¶å¥ å®šäº†åŸºç¡€ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://mikewangwzhl.github.io/PAPO%E3%80%82">https://mikewangwzhl.github.io/PAPOã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06448v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ å¯é€šè¿‡èµ‹äºˆå¥–åŠ±è¯æ˜å¥–åŠ±ï¼Œå…·å¤‡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¨³å¥çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè®¾è®¡ä¼˜åŒ–ä»…é™äºçº¯æ–‡æœ¬é¢†åŸŸï¼Œå¯¼è‡´åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºæ„ŸçŸ¥æ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–ï¼ˆPAPOï¼‰ï¼Œé€šè¿‡å¼•å…¥éšæ„ŸçŸ¥æŸå¤±ä½œä¸ºKLæ•£åº¦é¡¹ï¼Œé¼“åŠ±æ¨¡å‹ä»å†…éƒ¨ç›‘ç£ä¿¡å·ä¸­å­¦ä¹ æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•åœ¨ä¸ä¾èµ–é¢å¤–æ•°æ®ã€å¤–éƒ¨å¥–åŠ±æ¨¡å‹æˆ–ä¸“æœ‰æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œåœ¨å¤šæ ·åŒ–å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“åœ°ï¼Œåœ¨è§†è§‰ä¾èµ–æ€§é«˜çš„ä»»åŠ¡ä¸Šï¼Œæ€§èƒ½æå‡æ¥è¿‘8%ï¼Œæ„ŸçŸ¥é”™è¯¯å‡å°‘äº†çº¦ä¸‰æˆã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å¯¹PAPOè¿›è¡Œäº†å…¨é¢çš„åˆ†æï¼Œå‘ç°å¹¶è§£å†³äº†ç‹¬ç‰¹çš„æŸå¤±é—®é¢˜ï¼Œå¼•å…¥äº†åŒé‡ç†µæŸå¤±ä»¥å‡è½»è¯¥é—®é¢˜ã€‚æœ¬ç ”ç©¶æ·±å…¥æ•´åˆæ„ŸçŸ¥æ„ŸçŸ¥ç›‘ç£è¿›å…¥å¼ºåŒ–å­¦ä¹ å¥–åŠ±è¯æ˜çš„ç›®æ ‡ä¸­ï¼Œä¸ºåç»­é¼“åŠ±è§†è§‰åŒ–æ¨ç†çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶å¥ å®šåŸºç¡€ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://mikewangwzhl.github.io/PAPO">é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ é€šè¿‡å¥–åŠ±è¯æ˜å¥–åŠ±ç­–ç•¥æœ‰æ•ˆå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰å¤šæ¨¡æ€æ¨ç†çš„ä¸»è¦è¯¯å·®æ¥æºäºè§†è§‰è¾“å…¥çš„æ„ŸçŸ¥é—®é¢˜ã€‚</li>
<li>æå‡ºæ„ŸçŸ¥æ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–ï¼ˆPAPOï¼‰æ–¹æ³•ï¼Œå¼•å…¥éšæ„ŸçŸ¥æŸå¤±æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>PAPOæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•æ€§èƒ½ï¼Œåœ¨è§†è§‰ä¾èµ–æ€§é«˜çš„ä»»åŠ¡ä¸Šè¡¨ç°å°¤å…¶æ˜æ˜¾ã€‚</li>
<li>é€šè¿‡åŒé‡ç†µæŸå¤±è§£å†³ç‹¬ç‰¹çš„æŸå¤±é—®é¢˜ã€‚</li>
<li>PAPOæ–¹æ³•æé«˜äº†æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå‡å°‘äº†æ„ŸçŸ¥é”™è¯¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06448">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-91dc84a459988f20d56d36e2655b67a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d05e2b6b625f6d23e5c17f98db36925e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e70740f8fb860c2851342b4dd6b67316.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97dc35774ae59ad9e354ff606af123fc.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="One-task-to-rule-them-all-A-closer-look-at-traffic-classification-generalizability"><a href="#One-task-to-rule-them-all-A-closer-look-at-traffic-classification-generalizability" class="headerlink" title="One task to rule them all: A closer look at traffic classification   generalizability"></a>One task to rule them all: A closer look at traffic classification   generalizability</h2><p><strong>Authors:Elham Akbari, Zihao Zhou, Mohammad Ali Salahuddin, Noura Limam, Raouf Boutaba, Bertrand Mathieu, Stephanie Moteau, Stephane Tuffin</strong></p>
<p>Existing website fingerprinting and traffic classification solutions do not work well when the evaluation context changes, as their performances often heavily rely on context-specific assumptions. To clarify this problem, we take three prior solutions presented for different but similar traffic classification and website fingerprinting tasks, and apply each solutionâ€™s model to another solutionâ€™s dataset. We pinpoint dataset-specific and model-specific properties that lead each of them to overperform in their specific evaluation context.   As a realistic evaluation context that takes practical labeling constraints into account, we design an evaluation framework using two recent real-world TLS traffic datasets from large-scale networks. The framework simulates a futuristic scenario in which SNIs are hidden in some networks but not in others, and the classifierâ€™s goal is to predict destination services in one networkâ€™s traffic, having been trained on a labelled dataset collected from a different network. Our framework has the distinction of including real-world distribution shift, while excluding concept drift. We show that, even when abundant labeled data is available, the best solutionsâ€™ performances under distribution shift are between 30% and 40%, and a simple 1-Nearest Neighbor classifierâ€™s performance is not far behind. We depict all performances measured on different models, not just the best ones, for a fair representation of traffic models in practice. </p>
<blockquote>
<p>ç°æœ‰ç½‘ç«™æŒ‡çº¹å’Œæµé‡åˆ†ç±»è§£å†³æ–¹æ¡ˆåœ¨è¯„ä¼°ä¸Šä¸‹æ–‡å‘ç”Ÿå˜åŒ–æ—¶æ•ˆæœä¸ä½³ï¼Œå› ä¸ºå®ƒä»¬çš„æ€§èƒ½å¾€å¾€ä¸¥é‡ä¾èµ–äºç‰¹å®šä¸Šä¸‹æ–‡çš„å‡è®¾ã€‚ä¸ºäº†æ¾„æ¸…è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é€‰å–äº†ä¸‰ä¸ªé’ˆå¯¹ä¸åŒä½†ç›¸ä¼¼çš„æµé‡åˆ†ç±»å’Œç½‘ç«™æŒ‡çº¹ä»»åŠ¡çš„å…ˆå‰è§£å†³æ–¹æ¡ˆï¼Œå¹¶å°†æ¯ä¸ªè§£å†³æ–¹æ¡ˆçš„æ¨¡å‹åº”ç”¨åˆ°å¦ä¸€ä¸ªè§£å†³æ–¹æ¡ˆçš„æ•°æ®é›†ä¸Šã€‚æˆ‘ä»¬æŒ‡å‡ºäº†æ•°æ®é›†ç‰¹å®šå’Œæ¨¡å‹ç‰¹å®šçš„å±æ€§ï¼Œè¿™äº›å±æ€§ä½¿å®ƒä»¬åœ¨ç‰¹å®šçš„è¯„ä¼°ä¸Šä¸‹æ–‡ä¸­è¡¨ç°å‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06430v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç½‘ç»œæµé‡åˆ†ç±»å’Œç½‘ç«™æŒ‡çº¹è¯†åˆ«æ–¹æ¡ˆåœ¨ä¸åŒè¯„ä¼°ç¯å¢ƒä¸‹è¡¨ç°ä¸ä¸€ï¼Œæ€§èƒ½å–å†³äºç‰¹å®šçš„å‡è®¾ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡åˆ†æäº†ä¸‰ä¸ªä¸åŒä½†ç›¸ä¼¼çš„ä»»åŠ¡ä¸­çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä½¿ç”¨å„è‡ªæ¨¡å‹å¤„ç†å…¶ä»–æ–¹æ¡ˆçš„æ•°æ®é›†ã€‚é€šè¿‡è®¾è®¡åŸºäºä¸¤ä¸ªçœŸå®TLSæµé‡æ•°æ®é›†çš„è¯„ä¼°æ¡†æ¶ï¼Œè€ƒè™‘äº†å®é™…åº”ç”¨ä¸­çš„æ ‡ç­¾çº¦æŸï¼Œæ¨¡æ‹Ÿäº†SNIéšè—çš„åœºæ™¯ï¼Œå¹¶è¯„ä¼°äº†æ¨¡å‹åœ¨ä¸åŒç½‘ç»œç¯å¢ƒä¸­çš„æ€§èƒ½å·®å¼‚ã€‚ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿åœ¨æœ‰è¶³å¤Ÿæ ‡ç­¾æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæœ€ä½³è§£å†³æ–¹æ¡ˆçš„æ€§èƒ½åœ¨åˆ†å¸ƒè½¬ç§»ä¸‹ä»ä»‹äº30%è‡³40%ä¹‹é—´ï¼Œç®€å•çš„1è¿‘é‚»åˆ†ç±»å™¨çš„æ€§èƒ½ä¸ä¹‹ä¸ç›¸ä¸Šä¸‹ã€‚å¯¹æ‰€æœ‰æ¨¡å‹è¿›è¡Œäº†æ€§èƒ½è¯„ä¼°ï¼Œè€Œéä»…é™äºæœ€ä½³æ¨¡å‹ï¼Œä»¥å…¬å¹³åœ°å±•ç¤ºå®é™…åº”ç”¨ä¸­çš„æµé‡æ¨¡å‹ã€‚æ­¤ç ”ç©¶è§£å†³äº†å®é™…åœºæ™¯ä¸­å¯èƒ½å‡ºç°çš„è¯„ä¼°å’Œè¿ç§»æŒ‘æˆ˜é—®é¢˜ã€‚ç®€è¨€ä¹‹ï¼Œå¦‚ä½•åœ¨å¤æ‚çš„ç½‘ç»œç¯å¢ƒæ”¹å˜ä¸­è¿›è¡Œé«˜æ•ˆçš„ç½‘ç«™æŒ‡çº¹ä¸æµé‡è¯†åˆ«ä¾æ—§æ˜¯ä¸€é¡¹å€¼å¾—æ·±å…¥ç ”ç©¶çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç½‘ç»œæµé‡åˆ†ç±»å’Œç½‘ç«™æŒ‡çº¹è¯†åˆ«æ–¹æ¡ˆåœ¨ä¸åŒè¯„ä¼°ç¯å¢ƒä¸‹è¡¨ç°ä¸ç¨³å®šï¼Œæ€§èƒ½å—ç‰¹å®šå‡è®¾å½±å“ã€‚</li>
<li>å¯¹ä¸‰ä¸ªä¸åŒä»»åŠ¡ä¸­çš„è§£å†³æ–¹æ¡ˆè¿›è¡Œæ¯”è¾ƒåˆ†æï¼Œæ˜¾ç¤ºå„è‡ªåœ¨ä¸åŒæ•°æ®é›†å’Œæ¨¡å‹ä¸Šçš„è¡¨ç°ã€‚</li>
<li>è®¾è®¡è€ƒè™‘å®é™…åº”ç”¨ä¸­æ ‡ç­¾çº¦æŸçš„è¯„ä¼°æ¡†æ¶ï¼Œæ¨¡æ‹ŸSNIéšè—åœºæ™¯ï¼Œçªå‡ºå®é™…ç½‘ç»œç¯å¢ƒå¤æ‚æ€§å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚</li>
<li>åœ¨åˆ†å¸ƒè½¬ç§»æƒ…å†µä¸‹ï¼Œæœ€ä½³è§£å†³æ–¹æ¡ˆçš„æ€§èƒ½ä»‹äº30%è‡³40%ï¼Œç®€å•åˆ†ç±»å™¨æ€§èƒ½ä¸ä¹‹ç›¸è¿‘ã€‚</li>
<li>è¯„ä¼°ä¸åŒæ¨¡å‹çš„æ€§èƒ½è¡¨ç°ï¼Œä¸ä»…é™äºæœ€ä½³æ¨¡å‹ï¼Œä»¥åæ˜ å®é™…åº”ç”¨ä¸­çš„çœŸå®æƒ…å†µã€‚</li>
<li>ç ”ç©¶è§£å†³äº†å®é™…åœºæ™¯ä¸­å¯èƒ½å‡ºç°çš„è¯„ä¼°å’Œè¿ç§»æŒ‘æˆ˜é—®é¢˜ã€‚å¼ºè°ƒäº†åœ¨å¤æ‚ç½‘ç»œç¯å¢ƒä¸‹è¿›è¡Œé«˜æ•ˆç½‘ç«™æŒ‡çº¹ä¸æµé‡è¯†åˆ«çš„æŒ‘æˆ˜æ€§å’Œé‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06430">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0246e5cbf28e16b55cbb0f8cbd2ca34c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7a5f253d4c4d8d420cb723ed177c8d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b31baa30e53d2cbc358a7b2f9cee4599.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f71f655db7427de94eda5137bb53ad3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="PERK-Long-Context-Reasoning-as-Parameter-Efficient-Test-Time-Learning"><a href="#PERK-Long-Context-Reasoning-as-Parameter-Efficient-Test-Time-Learning" class="headerlink" title="PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning"></a>PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning</h2><p><strong>Authors:Zeming Chen, Angelika Romanou, Gail Weiss, Antoine Bosselut</strong></p>
<p>Long-context reasoning requires accurately identifying relevant information in extensive, noisy input contexts. Previous research shows that using test-time learning to encode context directly into model parameters can effectively enable reasoning over noisy information. However, meta-learning methods for enabling test-time learning are prohibitively memory-intensive, preventing their application to long context settings. In this work, we propose PERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for learning to encode long input contexts using gradient updates to a lightweight model adapter at test time. Specifically, PERK employs two nested optimization loops in a meta-training phase. The inner loop rapidly encodes contexts into a low-rank adapter (LoRA) that serves as a parameter-efficient memory module for the base model. Concurrently, the outer loop learns to use the updated adapter to accurately recall and reason over relevant information from the encoded long context. Our evaluations on several long-context reasoning tasks show that PERK significantly outperforms the standard prompt-based long-context baseline, achieving average absolute performance gains of up to 90% for smaller models (GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In general, PERK is more robust to reasoning complexity, length extrapolation, and the locations of relevant information in contexts. Finally, we show that while PERK is memory-intensive during training, it scales more efficiently at inference time than prompt-based long-context inference. </p>
<blockquote>
<p>é•¿è¯­å¢ƒæ¨ç†éœ€è¦å‡†ç¡®è¯†åˆ«å¹¿æ³›ã€å˜ˆæ‚è¾“å…¥è¯­å¢ƒä¸­çš„ç›¸å…³ä¿¡æ¯ã€‚å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼Œåœ¨æµ‹è¯•æ—¶å°†è¯­å¢ƒç›´æ¥ç¼–ç åˆ°æ¨¡å‹å‚æ•°ä¸­ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¯¹å˜ˆæ‚ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚ç„¶è€Œï¼Œå®ç°æµ‹è¯•æ—¶å­¦ä¹ çš„å…ƒå­¦ä¹ æ–¹æ³•éœ€è¦å¤§é‡å†…å­˜ï¼Œé˜»ç¢äº†å…¶åœ¨é•¿è¯­å¢ƒè®¾ç½®ä¸­çš„åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PERKï¼ˆåŸºäºçŸ¥è¯†çš„å‚æ•°é«˜æ•ˆæ¨ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå­¦ä¹ åœ¨æµ‹è¯•æ—¶å¯¹é•¿è¾“å…¥è¯­å¢ƒè¿›è¡Œç¼–ç çš„å¯æ‰©å±•æ–¹æ³•ï¼Œé€šè¿‡æ¢¯åº¦æ›´æ–°å¯¹åŸºç¡€æ¨¡å‹çš„è½»é‡çº§æ¨¡å‹é€‚é…å™¨è¿›è¡Œè°ƒæ•´ã€‚å…·ä½“æ¥è¯´ï¼ŒPERKåœ¨å…ƒè®­ç»ƒé˜¶æ®µé‡‡ç”¨ä¸¤ä¸ªåµŒå¥—ä¼˜åŒ–å¾ªç¯ã€‚å†…å¾ªç¯è¿…é€Ÿå°†è¯­å¢ƒç¼–ç ä¸ºä½é˜¶é€‚é…å™¨ï¼ˆLoRAï¼‰ï¼Œä½œä¸ºåŸºç¡€æ¨¡å‹çš„å‚æ•°æœ‰æ•ˆå†…å­˜æ¨¡å—ã€‚åŒæ—¶ï¼Œå¤–å¾ªç¯å­¦ä¹ ä½¿ç”¨æ›´æ–°çš„é€‚é…å™¨æ¥å‡†ç¡®å›å¿†å’Œæ¨ç†ç¼–ç é•¿è¯­å¢ƒä¸­çš„ç›¸å…³ä¿¡æ¯ã€‚æˆ‘ä»¬åœ¨å‡ ä¸ªé•¿è¯­å¢ƒæ¨ç†ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒPERKæ˜¾è‘—ä¼˜äºåŸºäºæç¤ºçš„é•¿è¯­å¢ƒåŸºçº¿ï¼Œå¯¹äºè¾ƒå°çš„æ¨¡å‹ï¼ˆGPT-2ï¼‰å¹³å‡ç»å¯¹æ€§èƒ½æå‡é«˜è¾¾90%ï¼Œå¯¹äºæˆ‘ä»¬è¯„ä¼°çš„æœ€å¤§çš„æ¨¡å‹Qwen-2.5-0.5Bï¼Œæå‡é«˜è¾¾27%ã€‚æ€»çš„æ¥è¯´ï¼ŒPERKå¯¹äºæ¨ç†å¤æ‚æ€§ã€é•¿åº¦æ‰©å±•å’Œè¯­å¢ƒä¸­ç›¸å…³ä¿¡æ¯çš„ä½ç½®æ›´åŠ ç¨³å¥ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†è™½ç„¶PERKåœ¨è®­ç»ƒæ—¶å†…å­˜å¯†é›†ï¼Œä½†åœ¨æ¨ç†æ—¶é—´ä¸Šçš„æ‰©å±•æ•ˆç‡æ¯”åŸºäºæç¤ºçš„é•¿è¯­å¢ƒæ¨ç†æ›´é«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06415v1">PDF</a> 10 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†PERKï¼ˆåŸºäºçŸ¥è¯†çš„å‚æ•°æœ‰æ•ˆæ¨ç†ï¼‰æ–¹æ³•ï¼Œç”¨äºåœ¨æµ‹è¯•æ—¶å­¦ä¹ ç¼–ç é•¿è¾“å…¥ä¸Šä¸‹æ–‡ã€‚è¯¥æ–¹æ³•é€šè¿‡æ¢¯åº¦æ›´æ–°å¯¹åŸºç¡€æ¨¡å‹çš„è½»é‡çº§æ¨¡å‹é€‚é…å™¨è¿›è¡Œç¼–ç ï¼Œé‡‡ç”¨ä¸¤å±‚ä¼˜åŒ–å¾ªç¯è¿›è¡Œå…ƒè®­ç»ƒã€‚å†…å¾ªç¯å¿«é€Ÿå°†ä¸Šä¸‹æ–‡ç¼–ç ä¸ºä½é˜¶é€‚é…å™¨ï¼ˆLoRAï¼‰ï¼Œä½œä¸ºåŸºç¡€æ¨¡å‹çš„å‚æ•°æœ‰æ•ˆå†…å­˜æ¨¡å—ã€‚å¤–å¾ªç¯å­¦ä¹ ä½¿ç”¨æ›´æ–°çš„é€‚é…å™¨æ¥å‡†ç¡®å›å¿†å’Œæ¨ç†é•¿ä¸Šä¸‹æ–‡ä¸­çš„ç›¸å…³ä¿¡æ¯ã€‚åœ¨å¤šä¸ªé•¿ä¸Šä¸‹æ–‡æ¨ç†ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒPERKæ˜¾è‘—ä¼˜äºåŸºäºæç¤ºçš„é•¿ä¸Šä¸‹æ–‡åŸºçº¿ï¼Œå¯¹å°æ¨¡å‹ï¼ˆGPT-2ï¼‰çš„å¹³å‡ç»å¯¹æ€§èƒ½æå‡é«˜è¾¾90%ï¼Œå¯¹æœ€å¤§è¯„ä¼°æ¨¡å‹ï¼ˆQwen-2.5-0.5Bï¼‰çš„æå‡çº¦ä¸º27%ã€‚æ€»ä½“è€Œè¨€ï¼ŒPERKå¯¹æ¨ç†å¤æ‚æ€§ã€é•¿åº¦æ¨æ–­å’Œä¸Šä¸‹æ–‡ä¸­ç›¸å…³ä¿¡æ¯çš„ä½ç½®æ›´å…·é²æ£’æ€§ã€‚åŒæ—¶ï¼Œè™½ç„¶PERKåœ¨è®­ç»ƒæœŸé—´å†…å­˜å¯†é›†ï¼Œä½†åœ¨æ¨ç†æ—¶æ¯”åŸºäºæç¤ºçš„é•¿ä¸Šä¸‹æ–‡æ¨ç†æ›´æœ‰æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PERKæ˜¯ä¸€ç§ç”¨äºåœ¨æµ‹è¯•æ—¶å­¦ä¹ ç¼–ç é•¿è¾“å…¥ä¸Šä¸‹æ–‡çš„æ–¹æ³•ã€‚</li>
<li>PERKé‡‡ç”¨ä¸¤å±‚ä¼˜åŒ–å¾ªç¯è¿›è¡Œå…ƒè®­ç»ƒï¼Œå†…å¾ªç¯å¿«é€Ÿç¼–ç ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¤–å¾ªç¯å­¦ä¹ å‡†ç¡®å›å¿†å’Œæ¨ç†ç›¸å…³ä¿¡æ¯ã€‚</li>
<li>PERKé€šè¿‡æ¢¯åº¦æ›´æ–°ä½¿ç”¨è½»é‡çº§æ¨¡å‹é€‚é…å™¨è¿›è¡Œç¼–ç ï¼Œæ˜¯ä¸€ç§å‚æ•°æœ‰æ•ˆçš„æ–¹æ³•ã€‚</li>
<li>åœ¨å¤šä¸ªé•¿ä¸Šä¸‹æ–‡æ¨ç†ä»»åŠ¡ä¸Šï¼ŒPERKæ˜¾è‘—ä¼˜äºåŸºäºæç¤ºçš„æ–¹æ³•ã€‚</li>
<li>PERKå¯¹å°æ¨¡å‹çš„æ€§èƒ½æå‡æ›´å¤§ï¼ŒåŒæ—¶ä¹Ÿé€‚ç”¨äºè¾ƒå¤§æ¨¡å‹ã€‚</li>
<li>PERKå¯¹æ¨ç†å¤æ‚æ€§ã€é•¿åº¦æ¨æ–­å’Œä¸Šä¸‹æ–‡ä¸­ç›¸å…³ä¿¡æ¯çš„ä½ç½®å…·æœ‰é²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06415">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b212bf13d15ea18849e715148ebaedd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb529869e810530ff47e95202aeda756.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46715e08cd20fee9614aaf8e3e92bbd9.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Skywork-R1V3-Technical-Report"><a href="#Skywork-R1V3-Technical-Report" class="headerlink" title="Skywork-R1V3 Technical Report"></a>Skywork-R1V3 Technical Report</h2><p><strong>Authors:Wei Shen, Jiangbo Pei, Yi Peng, Xuchen Song, Yang Liu, Jian Peng, Haofeng Sun, Yunzhuo Hao, Peiyu Wang, Jianhao Zhang, Yahui Zhou</strong></p>
<p>We introduce Skywork-R1V3, an advanced, open-source vision-language model (VLM) that pioneers a new approach to visual reasoning. Its key innovation lies in effectively transferring reasoning skills from text-only Large Language Models (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily stems from our elaborate post-training RL framework, which effectively activates and enhances the modelâ€™s reasoning ability, without the need for additional continue pre-training. Through this framework, we further uncover the fundamental role of the connector module in achieving robust cross-modal alignment for multimodal reasoning models. In addition, we introduce a unique indicator of reasoning capability, the entropy of critical reasoning tokens, which has proven highly effective for checkpoint selection during RL training. Skywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving from 64.3% to 76.0%. This performance matches entry-level human capabilities. Remarkably, our RL-powered post-training approach enables even the 38B parameter model to rival top closed-source VLMs. The implementation successfully transfers mathematical reasoning to other subject-related reasoning tasks. We also include an analysis of curriculum learning and reinforcement finetuning strategies, along with a broader discussion on multimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal reasoning, showcasing RL as a powerful engine for advancing open-source VLM capabilities. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Skywork-R1V3ï¼Œè¿™æ˜¯ä¸€ä¸ªå…ˆè¿›çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå®ƒå¼€åˆ›äº†ä¸€ç§æ–°çš„è§†è§‰æ¨ç†æ–¹æ³•ã€‚å…¶ä¸»è¦åˆ›æ–°ç‚¹åœ¨äºæœ‰æ•ˆåœ°å°†ä»æ–‡æœ¬ä¸­è·å¾—çš„æ¨ç†èƒ½åŠ›ä»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è½¬ç§»åˆ°è§†è§‰ä»»åŠ¡ä¸Šã€‚Skywork-R1V3çš„å‡ºè‰²æ€§èƒ½ä¸»è¦æºäºæˆ‘ä»¬ç²¾å¿ƒè®¾è®¡çš„åè®­ç»ƒå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆåœ°æ¿€æ´»å¹¶å¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€é¢å¤–çš„ç»§ç»­é¢„è®­ç»ƒã€‚é€šè¿‡è¿™ä¸€æ¡†æ¶ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å‘ç°äº†è¿æ¥å™¨æ¨¡å—åœ¨å®ç°ç¨³å¥çš„è·¨æ¨¡æ€å¯¹é½çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç‹¬ç‰¹çš„æ¨ç†èƒ½åŠ›æŒ‡æ ‡â€”â€”å…³é”®æ¨ç†ä»¤ç‰Œçš„ç†µï¼Œè¯¥æŒ‡æ ‡åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ£€æŸ¥ç‚¹é€‰æ‹©ä¸­è¢«è¯æ˜æ˜¯éå¸¸æœ‰æ•ˆçš„ã€‚Skywork-R1V3åœ¨MMMUä¸Šå–å¾—äº†æœ€æ–°ç»“æœï¼Œä»64.3%æ˜¾è‘—æé«˜è‡³76.0%ï¼Œä¸äººç±»å…¥é—¨çº§èƒ½åŠ›ç›¸åŒ¹é…ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒæ–¹æ³•ä½¿å¾—38Bå‚æ•°æ¨¡å‹ä¹Ÿèƒ½ä¸é¡¶çº§é—­æºVLMç›¸ç«äº‰ã€‚æˆ‘ä»¬çš„å®ç°æˆåŠŸåœ°å°†æ•°å­¦æ¨ç†è½¬ç§»åˆ°å…¶ä»–ç›¸å…³æ¨ç†ä»»åŠ¡ä¸­ã€‚æˆ‘ä»¬è¿˜å¯¹è¯¾ç¨‹å­¦ä¹ å’Œå¼ºåŒ–å¾®è°ƒç­–ç•¥è¿›è¡Œäº†åˆ†æï¼Œå¹¶å¯¹å¤šæ¨¡æ€æ¨ç†è¿›è¡Œäº†æ›´å¹¿æ³›çš„è®¨è®ºã€‚Skywork-R1V3åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­å®ç°äº†é‡å¤§çªç ´ï¼Œå±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ ä½œä¸ºæ¨åŠ¨å¼€æºVLMèƒ½åŠ›è¿›æ­¥çš„å¼ºå¤§å¼•æ“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06167v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Skywork-R1V3æ˜¯ä¸€æ¬¾å…ˆè¿›çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå®ƒå¼€åˆ›äº†ä¸€ç§æ–°çš„è§†è§‰æ¨ç†æ–¹æ³•ã€‚è¯¥æ¨¡å‹çš„å…³é”®åˆ›æ–°åœ¨äºå°†çº¯æ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æœ‰æ•ˆåœ°è½¬ç§»åˆ°è§†è§‰ä»»åŠ¡ä¸Šã€‚å…¶å¼ºå¤§çš„æ€§èƒ½ä¸»è¦æ¥è‡ªäºç²¾ç»†çš„åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¡†æ¶è®­ç»ƒï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆåœ°æ¿€æ´»å¹¶å¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„é¢„è®­ç»ƒã€‚Skywork-R1V3å®ç°äº†æœ€å…ˆè¿›çš„æˆç»©ï¼Œä¸åŸºçº¿ç›¸æ¯”æœ‰æ˜¾è‘—æ”¹å–„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Skywork-R1V3æ˜¯ä¸€ä¸ªå…ˆè¿›çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿæ‰§è¡Œè§†è§‰æ¨ç†ä»»åŠ¡ã€‚</li>
<li>è¯¥æ¨¡å‹æˆåŠŸåœ°å°†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°è§†è§‰ä»»åŠ¡ä¸Šã€‚</li>
<li>Skywork-R1V3é‡‡ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¿æ¥å™¨æ¨¡å—åœ¨è¾¾æˆç¨³å¥çš„è·¨æ¨¡æ€å¯¹é½ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>æ¨¡å‹çš„æ€§èƒ½è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨MMMUæµ‹è¯•ä¸­ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„åè®­ç»ƒç­–ç•¥ä½¿å¾—è¾ƒå°çš„æ¨¡å‹ä¹Ÿèƒ½ä¸é¡¶çº§çš„é—­æºè§†è§‰è¯­è¨€æ¨¡å‹ç›¸ç«äº‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06167">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7d19ed12fbb145b53449bc344bab0646.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6828544e25f077d8f088b10a12f9b2dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ec6cf3b2d1b4652d9d5c00941cb925c.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="FEVO-Financial-Knowledge-Expansion-and-Reasoning-Evolution-for-Large-Language-Models"><a href="#FEVO-Financial-Knowledge-Expansion-and-Reasoning-Evolution-for-Large-Language-Models" class="headerlink" title="FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large   Language Models"></a>FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large   Language Models</h2><p><strong>Authors:Bo Pang, Yalu Ouyang, Hangfei Xu, Ziqi Jia, Panpan Li, Shengzhao Wen, Lu Wang, Shiyong Li, Yanpeng Wang</strong></p>
<p>Advancements in reasoning for large language models (LLMs) have lead to significant performance improvements for LLMs in various fields such as mathematics and programming. However, research applying these advances to the financial domain, where considerable domain-specific knowledge is necessary to complete tasks, remains limited. To address this gap, we introduce FEVO (Financial Evolution), a multi-stage enhancement framework developed to enhance LLM performance in the financial domain. FEVO systemically enhances LLM performance by using continued pre-training (CPT) to expand financial domain knowledge, supervised fine-tuning (SFT) to instill structured, elaborate reasoning patterns, and reinforcement learning (RL) to further integrate the expanded financial domain knowledge with the learned structured reasoning. To ensure effective and efficient training, we leverage frontier reasoning models and rule-based filtering to curate FEVO-Train, high-quality datasets specifically designed for the different post-training phases. Using our framework, we train the FEVO series of models - C32B, S32B, R32B - from Qwen2.5-32B and evaluate them on seven benchmarks to assess financial and general capabilities, with results showing that FEVO-R32B achieves state-of-the-art performance on five financial benchmarks against much larger models as well as specialist models. More significantly, FEVO-R32B demonstrates markedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct using only RL), thus validating the effectiveness of financial domain knowledge expansion and structured, logical reasoning distillation </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ–¹é¢çš„è¿›æ­¥ï¼ŒLLMåœ¨æ•°å­¦ã€ç¼–ç¨‹ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œå°†è¿™äº›è¿›å±•åº”ç”¨äºé‡‘èé¢†åŸŸçš„ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œé‡‘èé¢†åŸŸå®Œæˆä»»åŠ¡éœ€è¦å¤§é‡çš„ç‰¹å®šé¢†åŸŸçŸ¥è¯†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†FEVOï¼ˆé‡‘èè¿›åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºå¢å¼ºLLMåœ¨é‡‘èé¢†åŸŸçš„æ€§èƒ½è€Œå¼€å‘çš„å¤šé˜¶æ®µå¢å¼ºæ¡†æ¶ã€‚FEVOç³»ç»Ÿé€šè¿‡æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰æ‰©å±•é‡‘èé¢†åŸŸçŸ¥è¯†ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åŸ¹å…»ç»“æ„åŒ–å’Œé€»è¾‘åŒ–çš„æ¨ç†æ¨¡å¼ï¼Œä»¥åŠé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›ä¸€æ­¥å°†æ‰©å±•çš„é‡‘èé¢†åŸŸçŸ¥è¯†ä¸å­¦ä¹ çš„ç»“æ„åŒ–æ¨ç†ç›¸ç»“åˆï¼Œä»è€Œå¢å¼ºLLMçš„æ€§èƒ½ã€‚ä¸ºç¡®ä¿æœ‰æ•ˆå’Œé«˜æ•ˆçš„è®­ç»ƒï¼Œæˆ‘ä»¬åˆ©ç”¨å‰æ²¿çš„æ¨ç†æ¨¡å‹å’ŒåŸºäºè§„åˆ™çš„è¿‡æ»¤æ¥åˆ›å»ºä¸“é—¨ä¸ºä¸åŒè®­ç»ƒåé˜¶æ®µè®¾è®¡çš„FEVO-Trainé«˜è´¨é‡æ•°æ®é›†ã€‚ä½¿ç”¨æˆ‘ä»¬çš„æ¡†æ¶ï¼Œæˆ‘ä»¬è®­ç»ƒäº†FEVOç³»åˆ—æ¨¡å‹ - C32Bã€S32Bã€R32B - åŸºäºQwen2.5-32Bï¼Œå¹¶åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šå¯¹å®ƒä»¬è¿›è¡Œè¯„ä¼°ï¼Œä»¥è¯„ä¼°å…¶é‡‘èå’Œä¸€èˆ¬èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼ŒFEVO-R32Båœ¨äº”é¡¹é‡‘èåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œç›¸è¾ƒäºæ›´å¤§çš„æ¨¡å‹å’Œä¸“ä¸šæ¨¡å‹ä¹Ÿæœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒFEVO-R32Bçš„æ€§èƒ½æ˜æ˜¾ä¼˜äºFEVO-R32B-0ï¼ˆä»…ä½¿ç”¨RLä»Qwen2.5-32B-Instructè¿›è¡Œè®­ç»ƒï¼‰ï¼Œä»è€ŒéªŒè¯äº†é‡‘èé¢†åŸŸçŸ¥è¯†æ‰©å±•å’Œç»“æ„åŒ–é€»è¾‘æ¨ç†è’¸é¦çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06057v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMåœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨é‡‘èé¢†åŸŸçš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†FEVOæ¡†æ¶ï¼Œé€šè¿‡æŒç»­é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒåŠå¼ºåŒ–å­¦ä¹ ä¸‰ä¸ªé˜¶æ®µå¢å¼ºLLMåœ¨é‡‘èé¢†åŸŸçš„æ€§èƒ½ã€‚FEVOç³»åˆ—æ¨¡å‹åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é‡‘èé¢†åŸŸçš„äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé‡‘èé¢†åŸŸçŸ¥è¯†çš„æ‰©å±•å’Œç»“æ„é€»è¾‘æ¨ç†è’¸é¦çš„ç»“åˆæœ‰åŠ©äºæå‡æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨é‡‘èé¢†åŸŸçš„æ€§èƒ½æå‡éœ€æ±‚è¿«åˆ‡ï¼Œä½†ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>FEVOæ¡†æ¶æ—¨åœ¨å¢å¼ºLLMåœ¨é‡‘èé¢†åŸŸçš„æ€§èƒ½ï¼Œé€šè¿‡æŒç»­é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒåŠå¼ºåŒ–å­¦ä¹ ä¸‰ä¸ªé˜¶æ®µå®ç°ã€‚</li>
<li>FEVOç³»åˆ—æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜ç§€ï¼Œç‰¹åˆ«æ˜¯åœ¨é‡‘èé¢†åŸŸçš„åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
<li>é‡‘èé¢†åŸŸçŸ¥è¯†çš„æ‰©å±•å¯¹æ¨¡å‹æ€§èƒ½æå‡è‡³å…³é‡è¦ã€‚</li>
<li>ç»“æ„åŒ–ã€é€»è¾‘åŒ–çš„æ¨ç†æ¨¡å¼æœ‰åŠ©äºæå‡æ¨¡å‹åœ¨é‡‘èé¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>FEVOæ¡†æ¶é€šè¿‡å‰æ²¿çš„æ¨ç†æ¨¡å‹å’Œè§„åˆ™è¿‡æ»¤æ–¹æ³•ï¼Œä¸ºä¸åŒçš„è®­ç»ƒé˜¶æ®µè®¾è®¡äº†é«˜è´¨é‡çš„æ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06057">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9b570d8b3da543665c6ad70b10f5ea71.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25af8dd35405c59ef930028bf48cb0b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8455eab949d3c3384bb9b2aa0b3c8f04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e8fc8bf273f3fddee2ac1530d2fd45b1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e98a8f549f38a7cd8cb4b456fcdc8ecb.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MedGround-R1-Advancing-Medical-Image-Grounding-via-Spatial-Semantic-Rewarded-Group-Relative-Policy-Optimization"><a href="#MedGround-R1-Advancing-Medical-Image-Grounding-via-Spatial-Semantic-Rewarded-Group-Relative-Policy-Optimization" class="headerlink" title="MedGround-R1: Advancing Medical Image Grounding via Spatial-Semantic   Rewarded Group Relative Policy Optimization"></a>MedGround-R1: Advancing Medical Image Grounding via Spatial-Semantic   Rewarded Group Relative Policy Optimization</h2><p><strong>Authors:Huihui Xu, Yuanpeng Nie, Hualiang Wang, Ying Chen, Wei Li, Junzhi Ning, Lihao Liu, Hongqiu Wang, Lei Zhu, Jiyao Liu, Xiaomeng Li, Junjun He</strong></p>
<p>Medical Image Grounding (MIG), which involves localizing specific regions in medical images based on textual descriptions, requires models to not only perceive regions but also deduce spatial relationships of these regions. Existing Vision-Language Models (VLMs) for MIG often rely on Supervised Fine-Tuning (SFT) with large amounts of Chain-of-Thought (CoT) reasoning annotations, which are expensive and time-consuming to acquire. Recently, DeepSeek-R1 demonstrated that Large Language Models (LLMs) can acquire reasoning abilities through Group Relative Policy Optimization (GRPO) without requiring CoT annotations. In this paper, we adapt the GRPO reinforcement learning framework to VLMs for Medical Image Grounding. We propose the Spatial-Semantic Rewarded Group Relative Policy Optimization to train the model without CoT reasoning annotations. Specifically, we introduce Spatial-Semantic Rewards, which combine spatial accuracy reward and semantic consistency reward to provide nuanced feedback for both spatially positive and negative completions. Additionally, we propose to use the Chain-of-Box template, which integrates visual information of referring bounding boxes into the <think> reasoning process, enabling the model to explicitly reason about spatial regions during intermediate steps. Experiments on three datasets MS-CXR, ChestX-ray8, and M3D-RefSeg demonstrate that our method achieves state-of-the-art performance in Medical Image Grounding. Ablation studies further validate the effectiveness of each component in our approach. Code, checkpoints, and datasets are available at <a target="_blank" rel="noopener" href="https://github.com/bio-mlhui/MedGround-R1">https://github.com/bio-mlhui/MedGround-R1</a> </p>
<blockquote>
<p>åŒ»ç–—å›¾åƒå®šä½ï¼ˆMIGï¼‰éœ€è¦æ ¹æ®æ–‡æœ¬æè¿°å®šä½åŒ»å­¦å›¾åƒä¸­çš„ç‰¹å®šåŒºåŸŸï¼Œè¿™è¦æ±‚æ¨¡å‹ä¸ä»…è¦æ„ŸçŸ¥åŒºåŸŸï¼Œè¿˜è¦æ¨æ–­è¿™äº›åŒºåŸŸçš„ç©ºé—´å…³ç³»ã€‚ç°æœ‰çš„ç”¨äºMIGçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šå¸¸ä¾èµ–äºå¤§é‡æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†æ³¨é‡Šçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿™äº›æ³¨é‡Šçš„è·å–æ—¢æ˜‚è´µåˆè€—æ—¶ã€‚æœ€è¿‘ï¼ŒDeepSeek-R1è¯æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è·å¾—æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€CoTæ³¨é‡Šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†GRPOå¼ºåŒ–å­¦ä¹ æ¡†æ¶é€‚åº”äºç”¨äºåŒ»ç–—å›¾åƒå®šä½çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºç©ºé—´è¯­ä¹‰å¥–åŠ±ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆSpatial-Semantic Rewarded Group Relative Policy Optimizationï¼‰ï¼Œå¯åœ¨æ— éœ€CoTæ¨ç†æ³¨é‡Šçš„æƒ…å†µä¸‹è®­ç»ƒæ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç©ºé—´è¯­ä¹‰å¥–åŠ±ï¼Œå®ƒå°†ç©ºé—´ç²¾åº¦å¥–åŠ±å’Œè¯­ä¹‰ä¸€è‡´æ€§å¥–åŠ±ç›¸ç»“åˆï¼Œä¸ºæ­£é¢å’Œè´Ÿé¢å®Œæˆçš„ç©ºé—´æä¾›ç»†å¾®çš„åé¦ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨â€œChain-of-Boxâ€æ¨¡æ¿ï¼Œè¯¥æ¨¡æ¿å°†æŒ‡ä»£è¾¹ç•Œæ¡†çš„è§†è§‰ä¿¡æ¯æ•´åˆåˆ°<think>æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸­é—´æ­¥éª¤ä¸­æ˜ç¡®æ¨ç†ç©ºé—´åŒºåŸŸã€‚åœ¨MS-CXRã€ChestX-ray8å’ŒM3D-RefSegä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŒ»ç–—å›¾åƒå®šä½æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ä¸­æ¯ä¸ªç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³ä»£ç ã€æ£€æŸ¥ç‚¹å’Œæ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/bio-mlhui/MedGround-R1%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/bio-mlhui/MedGround-R1è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02994v1">PDF</a> MICCAI2025 Early Accept</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŒ»ç–—å›¾åƒå®šä½ï¼ˆMIGï¼‰ä»»åŠ¡çš„æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ç©ºé—´è¯­ä¹‰å¥–åŠ±å¼ºåŒ–ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆSpatial-Semantic Rewarded Group Relative Policy Optimizationï¼‰ï¼Œæ— éœ€é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æ³¨é‡Šå³å¯è®­ç»ƒæ¨¡å‹ã€‚é€šè¿‡å¼•å…¥ç©ºé—´ç²¾åº¦å¥–åŠ±å’Œè¯­ä¹‰ä¸€è‡´æ€§å¥–åŠ±ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ç©ºé—´ä¸Šæ­£å‘å’Œè´Ÿå‘å®Œæˆä¸­æä¾›ç²¾ç»†åé¦ˆã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä½¿ç”¨é“¾å¼æ¡†æ¨¡æ¿ï¼ˆChain-of-Box templateï¼‰ï¼Œå°†å‚ç…§è¾¹ç•Œæ¡†çš„è§†è§‰ä¿¡æ¯èå…¥æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸­é—´æ­¥éª¤ä¸­æ˜ç¡®æ¨ç†ç©ºé—´åŒºåŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒ»ç–—å›¾åƒå®šä½ä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä»‹ç»äº†é’ˆå¯¹åŒ»ç–—å›¾åƒå®šä½çš„æ–°å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ— éœ€ä¾èµ–å¤§é‡æ˜‚è´µçš„é“¾å¼æ€ç»´æ¨ç†æ³¨é‡Šã€‚</li>
<li>æå‡ºç©ºé—´è¯­ä¹‰å¥–åŠ±ï¼Œç»“åˆç©ºé—´ç²¾åº¦å¥–åŠ±å’Œè¯­ä¹‰ä¸€è‡´æ€§å¥–åŠ±ï¼Œä¸ºæ¨¡å‹æä¾›ç²¾ç»†åé¦ˆã€‚</li>
<li>é‡‡ç”¨é“¾å¼æ¡†æ¨¡æ¿ï¼Œå°†è§†è§‰ä¿¡æ¯èå…¥æ¨ç†è¿‡ç¨‹ï¼Œæé«˜æ¨¡å‹åœ¨ç©ºé—´åŒºåŸŸçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒ»ç–—å›¾åƒå®šä½ä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02994">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-443837bf02534cba3ba3658b45940bb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79e36f732e0c22d97fbbd3483cc8666d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="RAG-R1-Incentivize-the-Search-and-Reasoning-Capabilities-of-LLMs-through-Multi-query-Parallelism"><a href="#RAG-R1-Incentivize-the-Search-and-Reasoning-Capabilities-of-LLMs-through-Multi-query-Parallelism" class="headerlink" title="RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs   through Multi-query Parallelism"></a>RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs   through Multi-query Parallelism</h2><p><strong>Authors:Zhiwen Tan, Jiaming Huang, Qintong Wu, Hongxuan Zhang, Chenyi Zhuang, Jinjie Gu</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, while they remain prone to generating hallucinated or outdated responses due to their static internal knowledge. Recent advancements in Retrieval-Augmented Generation (RAG) methods have explored enhancing modelsâ€™ search and reasoning capabilities through reinforcement learning (RL). Although these methods demonstrate promising results, they face challenges in training stability and encounter issues such as substantial inference time and restricted capabilities due to the single-query mode. In this paper, we propose RAG-R1, a novel training framework designed to enable LLMs to adaptively leverage internal and external knowledge during the reasoning process. We further expand the generation and retrieval processes within the framework from single-query mode to multi-query parallelism, aimed at reducing inference time and enhancing the modelâ€™s capabilities. Extensive experiments on seven question-answering benchmarks demonstrate that our method outperforms the strongest baseline by up to 13.2% and decreases inference time by 11.1%. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†ç”±äºå…¶å†…éƒ¨çŸ¥è¯†çš„é™æ€æ€§ï¼Œå®ƒä»¬å®¹æ˜“äº§ç”Ÿè™šæ„æˆ–è¿‡æ—¶çš„å›åº”ã€‚æœ€è¿‘ï¼Œå¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•çš„è¿›æ­¥æ¢ç´¢äº†é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢å¼ºæ¨¡å‹çš„æœç´¢å’Œæ¨ç†èƒ½åŠ›ã€‚å°½ç®¡è¿™äº›æ–¹æ³•æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†å®ƒä»¬é¢ä¸´ç€è®­ç»ƒç¨³å®šæ€§æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå¹¶ç”±äºå•æŸ¥è¯¢æ¨¡å¼è€Œé¢ä¸´æ¨ç†æ—¶é—´é•¿å’Œå—é™çš„èƒ½åŠ›ç­‰é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†RAG-R1ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨ä½¿LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°åˆ©ç”¨å†…éƒ¨å’Œå¤–éƒ¨çŸ¥è¯†ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°†æ¡†æ¶å†…çš„ç”Ÿæˆå’Œæ£€ç´¢è¿‡ç¨‹ä»å•æŸ¥è¯¢æ¨¡å¼æ‰©å±•åˆ°å¤šæŸ¥è¯¢å¹¶è¡Œæ¨¡å¼ï¼Œæ—¨åœ¨å‡å°‘æ¨ç†æ—¶é—´å¹¶æé«˜æ¨¡å‹çš„èƒ½åŠ›ã€‚åœ¨ä¸ƒä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”æœ€å¼ºåŸºçº¿é«˜å‡º13.2%ï¼Œå¹¶å°†æ¨ç†æ—¶é—´å‡å°‘äº†11.1%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02962v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬å®¹æ˜“äº§ç”Ÿè™šæ„æˆ–è¿‡æ—¶å›åº”ï¼Œå› ä¸ºå®ƒä»¬çš„å†…éƒ¨çŸ¥è¯†æ˜¯é™æ€çš„ã€‚æœ€è¿‘ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•çš„è¿›æ­¥é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æé«˜äº†æ¨¡å‹çš„æœç´¢å’Œæ¨ç†èƒ½åŠ›ã€‚å°½ç®¡è¿™äº›æ–¹æ³•æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†å®ƒä»¬é¢ä¸´è®­ç»ƒç¨³å®šæ€§æŒ‘æˆ˜ï¼Œå¹¶é‡åˆ°æ¨ç†æ—¶é—´é•¿å’Œå› å•ä¸€æŸ¥è¯¢æ¨¡å¼å¯¼è‡´çš„èƒ½åŠ›å—é™ç­‰é—®é¢˜ã€‚æœ¬æ–‡æå‡ºRAG-R1ï¼Œä¸€ç§æ–°å‹è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨ä½¿LLMsåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°åˆ©ç”¨å†…éƒ¨å’Œå¤–éƒ¨çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ¡†æ¶å†…å°†ç”Ÿæˆå’Œæ£€ç´¢è¿‡ç¨‹ä»å•ä¸€æŸ¥è¯¢æ¨¡å¼æ‰©å±•åˆ°å¤šæŸ¥è¯¢å¹¶è¡Œæ¨¡å¼ï¼Œæ—¨åœ¨å‡å°‘æ¨ç†æ—¶é—´å¹¶æé«˜æ¨¡å‹èƒ½åŠ›ã€‚åœ¨ä¸ƒä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”æœ€å¼ºåŸºçº¿é«˜å‡º13.2%ï¼Œå¹¶å°†æ¨ç†æ—¶é—´å‡å°‘11.1%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½ç„¶å…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å®¹æ˜“äº§ç”Ÿè™šæ„æˆ–è¿‡æ—¶å›åº”ï¼Œå› ä¸ºå®ƒä»¬åŸºäºé™æ€å†…éƒ¨çŸ¥è¯†ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æé«˜äº†æ¨¡å‹çš„æœç´¢å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>RAGæ–¹æ³•è™½ç„¶æ˜¾ç¤ºå‡ºæœ‰æ•ˆç»“æœï¼Œä½†ä»é¢ä¸´è®­ç»ƒç¨³å®šæ€§æŒ‘æˆ˜ã€æ¨ç†æ—¶é—´é•¿å’Œå› å•ä¸€æŸ¥è¯¢æ¨¡å¼å¯¼è‡´çš„èƒ½åŠ›å—é™ç­‰é—®é¢˜ã€‚</li>
<li>è®ºæ–‡æå‡ºçš„RAG-R1æ˜¯ä¸€ç§æ–°å‹è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨ä½¿LLMsåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°åˆ©ç”¨å†…éƒ¨å’Œå¤–éƒ¨çŸ¥è¯†ã€‚</li>
<li>RAG-R1æ¡†æ¶å°†ç”Ÿæˆå’Œæ£€ç´¢è¿‡ç¨‹ä»å•ä¸€æŸ¥è¯¢æ¨¡å¼æ‰©å±•åˆ°å¤šæŸ¥è¯¢å¹¶è¡Œæ¨¡å¼ï¼Œä»¥æé«˜æ•ˆç‡å’Œèƒ½åŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒRAG-R1åœ¨ä¸ƒä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºæœ€å¼ºåŸºçº¿ï¼Œæ€§èƒ½æå‡é«˜è¾¾13.2%ï¼Œå¹¶å‡å°‘æ¨ç†æ—¶é—´11.1%ã€‚</li>
<li>RAG-R1æ¡†æ¶å…·æœ‰æ½œåŠ›æ”¹å–„LLMsçš„é€‚åº”æ€§å’Œå“åº”è´¨é‡ï¼ŒåŒæ—¶æé«˜æ•ˆç‡å’Œå‡å°‘æ¨ç†æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02962">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8d93d78c66c53bfcdb78033294f2d49f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b281ab18f826b1ede85842c931c6b842.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35fc32b724226ee011ee72b5b2e34efb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18f9a6dbac11e4ba7490735e518c7673.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="EFRame-Deeper-Reasoning-via-Exploration-Filter-Replay-Reinforcement-Learning-Framework"><a href="#EFRame-Deeper-Reasoning-via-Exploration-Filter-Replay-Reinforcement-Learning-Framework" class="headerlink" title="EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement   Learning Framework"></a>EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement   Learning Framework</h2><p><strong>Authors:Chen Wang, Lai Wei, Yanzhi Zhang, Chenyang Shao, Zedong Dan, Weiran Huang, Yue Wang, Yuzhi Zhang</strong></p>
<p>Recent advances in reinforcement learning (RL) have significantly enhanced the reasoning capabilities of large language models (LLMs). Group Relative Policy Optimization (GRPO), an efficient variant of PPO that lowers RLâ€™s computational cost, still faces limited exploration, low sample efficiency and instability, constraining its performance on complex reasoning tasks. To address these limitations, we introduce EFRame, an Exploration-Filter-Replay framework that systematically augments GRPO along three critical dimensions. EFRame performs additional rollouts to explore high-quality trajectories, applies online filtering to eliminate low-quality samples that introduce noise and variance, and leverages experience replay to repeatedly exploit rare but informative samples. EFRame establishes a complete and stable learning cycle, guiding the model through a structured transition from exploration to convergence. Our experiments across a variety of reasoning benchmarks demonstrate that EFRame not only improves the robustness and efficiency of training, but also enables access to deeper reasoning capabilities that remain unattainable under vanilla GRPO. Furthermore, EFRame not only enables fine-grained categorization of training samples for deeper insight into their contributions, but also introduces an efficient and precise mechanism for entropy control, which is critical for balancing exploration and convergence in RL training. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/597358816/EFRame">https://github.com/597358816/EFRame</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æœ€æ–°è¿›å±•æå¤§åœ°æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚Group Relative Policy Optimizationï¼ˆGRPOï¼‰æ˜¯PPOçš„ä¸€ä¸ªæœ‰æ•ˆå˜ä½“ï¼Œé™ä½äº†RLçš„è®¡ç®—æˆæœ¬ï¼Œä½†ä»é¢ä¸´æ¢ç´¢æœ‰é™ã€æ ·æœ¬æ•ˆç‡è¾ƒä½å’Œä¸ç¨³å®šçš„é—®é¢˜ï¼Œåœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸Šé™åˆ¶äº†å…¶æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†EFRameï¼Œè¿™æ˜¯ä¸€ä¸ªæ¢ç´¢-è¿‡æ»¤-å›æ”¾æ¡†æ¶ï¼Œç³»ç»Ÿåœ°å¢å¼ºäº†GRPOçš„ä¸‰ä¸ªå…³é”®ç»´åº¦ã€‚EFRameæ‰§è¡Œé¢å¤–çš„rolloutsæ¥æ¢ç´¢é«˜è´¨é‡çš„è½¨è¿¹ï¼Œåº”ç”¨åœ¨çº¿è¿‡æ»¤æ¥æ¶ˆé™¤å¼•å…¥å™ªå£°å’Œæ–¹å·®çš„ä½è´¨é‡æ ·æœ¬ï¼Œå¹¶åˆ©ç”¨ç»éªŒå›æ”¾æ¥åå¤åˆ©ç”¨ç¨€æœ‰ä½†å…·æœ‰ä¿¡æ¯é‡çš„æ ·æœ¬ã€‚EFRameå»ºç«‹äº†ä¸€ä¸ªå®Œæ•´ç¨³å®šçš„å­¦ä¹ å‘¨æœŸï¼Œå¼•å¯¼æ¨¡å‹ä»æ¢ç´¢åˆ°æ”¶æ•›çš„ç»“æ„åŒ–è¿‡æ¸¡ã€‚æˆ‘ä»¬åœ¨å¤šç§æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEFRameä¸ä»…æé«˜äº†è®­ç»ƒçš„ç¨³å¥æ€§å’Œæ•ˆç‡ï¼Œè¿˜å®ç°äº†æ›´æ·±å…¥çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™åœ¨æ™®é€šGRPOä¸‹æ˜¯æ— æ³•è¾¾åˆ°çš„ã€‚æ­¤å¤–ï¼ŒEFRameä¸ä»…èƒ½å¤Ÿå¯¹è®­ç»ƒæ ·æœ¬è¿›è¡Œç²¾ç»†çš„åˆ†ç±»ï¼Œä»¥æ·±å…¥äº†è§£å…¶è´¡çŒ®ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§é«˜æ•ˆä¸”ç²¾ç¡®çš„ç†µæ§åˆ¶æœºåˆ¶ï¼Œè¿™å¯¹äºå¹³è¡¡å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­çš„æ¢ç´¢å’Œæ”¶æ•›è‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/597358816/EFRame%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/597358816/EFRameæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22200v3">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æœ€æ–°è¿›å±•å¤§å¤§æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚é’ˆå¯¹Group Relative Policy Optimizationï¼ˆGRPOï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†EFRameæ¡†æ¶ï¼Œä»æ¢ç´¢ã€è¿‡æ»¤å’Œå›æ”¾ä¸‰ä¸ªæ–¹é¢å¯¹GRPOè¿›è¡Œäº†ç³»ç»Ÿå¢å¼ºã€‚å®éªŒè¡¨æ˜ï¼ŒEFRameä¸ä»…æé«˜äº†è®­ç»ƒçš„ç¨³å¥æ€§å’Œæ•ˆç‡ï¼Œè¿˜å®ç°äº†æ›´æ·±å…¥çš„æ¨ç†èƒ½åŠ›ã€‚åŒæ—¶ï¼ŒEFRameè¿˜æä¾›äº†ä¸€ç§ç²¾ç»†çš„æ ·æœ¬åˆ†ç±»æœºåˆ¶å’Œæœ‰æ•ˆçš„ç†µæ§åˆ¶æœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Group Relative Policy Optimizationï¼ˆGRPOï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå­˜åœ¨å±€é™æ€§ï¼Œå¦‚æœ‰é™æ¢ç´¢ã€ä½æ ·æœ¬æ•ˆç‡å’Œä¸ç¨³å®šã€‚</li>
<li>EFRameæ¡†æ¶é€šè¿‡æ¢ç´¢ã€è¿‡æ»¤å’Œå›æ”¾ä¸‰ä¸ªå…³é”®æ–¹é¢å¢å¼ºGRPOã€‚</li>
<li>EFRameé€šè¿‡é¢å¤–çš„rolloutæ¢ç´¢é«˜è´¨é‡çš„è½¨è¿¹ï¼Œåœ¨çº¿è¿‡æ»¤æ¶ˆé™¤ä½è´¨é‡æ ·æœ¬çš„å™ªå£°å’Œæ–¹å·®ï¼Œå¹¶åˆ©ç”¨ç»éªŒå›æ”¾å¤šæ¬¡åˆ©ç”¨ç¨€æœ‰ä½†å…·æœ‰ä¿¡æ¯ä»·å€¼çš„æ ·æœ¬ã€‚</li>
<li>EFRameå»ºç«‹äº†ä¸€ä¸ªå®Œæ•´ç¨³å®šçš„å­¦ä¹ å‘¨æœŸï¼Œå¼•å¯¼æ¨¡å‹ä»æ¢ç´¢åˆ°æ”¶æ•›çš„ç»“æ„åŒ–è¿‡æ¸¡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒEFRameæé«˜äº†è®­ç»ƒçš„ç¨³å¥æ€§å’Œæ•ˆç‡ï¼Œå¹¶å®ç°äº†æ›´æ·±å…¥çš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22200">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-66b028eb1eb46ea1ee4262ed4d45903b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb8b9090618d67d815fd608b7c4c4198.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81639a2048fe9265d82e4fa27324f4f6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9f30139609dbb662684573dbaea8d27e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Double-Checker-Enhancing-Reasoning-of-Slow-Thinking-LLMs-via-Self-Critical-Fine-Tuning"><a href="#Double-Checker-Enhancing-Reasoning-of-Slow-Thinking-LLMs-via-Self-Critical-Fine-Tuning" class="headerlink" title="Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via   Self-Critical Fine-Tuning"></a>Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via   Self-Critical Fine-Tuning</h2><p><strong>Authors:Xin Xu, Tianhao Chen, Fan Zhang, Wanlong Liu, Pengxiang Li, Ajay Kumar Jaiswal, Yuchen Yan, Jishan Hu, Yang Wang, Hao Chen, Shiwei Liu, Shizhe Diao, Can Yang, Lu Yin</strong></p>
<p>While slow-thinking large language models (LLMs) exhibit reflection-like reasoning, commonly referred to as the â€œaha moment:, their ability to generate informative critiques and refine prior solutions remains limited. In this paper, we introduce Double-Checker, a principled framework designed to enhance the reasoning capabilities of slow-thinking LLMs by fostering explicit self-critique and iterative refinement of their previous solutions. By fine-tuning on our curated 1,730 self-critical instances, Double-Checker empowers long-CoT LLMs to iteratively critique and refine their outputs during inference until they evaluate their solutions as correct under self-generated critiques. We validate the efficacy of Double-Checker across a comprehensive suite of reasoning benchmarks, demonstrating that iterative self-critique significantly enhances the reasoning capabilities of long-CoT LLMs. Notably, our Double-Checker increases the pass@1 performance on challenging AIME benchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These results highlight a promising direction for developing more trustworthy and effective LLMs capable of structured self-critique. Our codes and data are available at <a target="_blank" rel="noopener" href="https://github.com/XinXU-USTC/DoubleChecker">https://github.com/XinXU-USTC/DoubleChecker</a> </p>
<blockquote>
<p>è™½ç„¶æ…¢æ€è€ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºç±»ä¼¼åæ€çš„æ¨ç†èƒ½åŠ›ï¼Œé€šå¸¸è¢«ç§°ä¸ºâ€œé¡¿æ‚Ÿæ—¶åˆ»â€ï¼Œä½†å®ƒä»¬ç”Ÿæˆæœ‰ä¿¡æ¯é‡çš„æ‰¹åˆ¤å’Œç²¾ç‚¼å…ˆå‰è§£å†³æ–¹æ¡ˆçš„èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Double-Checkerï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é€šè¿‡ä¿ƒè¿›æ˜ç¡®çš„è‡ªæˆ‘æ‰¹åˆ¤å’Œè¿­ä»£å®Œå–„å…¶å…ˆå‰è§£å†³æ–¹æ¡ˆï¼Œå¢å¼ºæ…¢æ€è€ƒå‹LLMæ¨ç†èƒ½åŠ›çš„åŸåˆ™æ€§æ¡†æ¶ã€‚é€šè¿‡åœ¨æˆ‘ä»¬ç²¾é€‰çš„1730ä¸ªè‡ªæˆ‘æ‰¹åˆ¤å®ä¾‹ä¸Šè¿›è¡Œå¾®è°ƒï¼ŒDouble-Checkerèµ‹äºˆäº†é•¿é“¾æ¨ç†LLMèƒ½åŠ›ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿­ä»£åœ°æ‰¹åˆ¤å’Œä¿®æ­£ä»–ä»¬çš„è¾“å‡ºï¼Œç›´åˆ°ä»–ä»¬æ ¹æ®è‡ªæˆ‘ç”Ÿæˆçš„æ‰¹åˆ¤è¯„ä¼°è‡ªå·±çš„è§£å†³æ–¹æ¡ˆä¸ºæ­£ç¡®ã€‚æˆ‘ä»¬åœ¨ä¸€ç³»åˆ—å…¨é¢çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†Double-Checkerçš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜è¿­ä»£çš„è‡ªæˆ‘æ‰¹åˆ¤æ˜¾è‘—å¢å¼ºäº†é•¿é“¾æ¨ç†LLMçš„æ¨ç†èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„Double-Checkeråœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„AIMEåŸºå‡†æµ‹è¯•ä¸Šå°†å‰1åé€šè¿‡ç‡ä»4.4%æé«˜åˆ°18.2%ï¼Œç›¸è¾ƒäºåŸå§‹çš„é•¿é“¾æ¨ç†LLMã€‚è¿™äº›ç»“æœçªæ˜¾äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ï¼Œå³å¼€å‘æ›´å…·å¯ä¿¡åº¦å’Œæœ‰æ•ˆæ€§çš„LLMï¼Œèƒ½å¤Ÿè¿›è¡Œè‡ªæˆ‘æ‰¹åˆ¤ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/XinXU-USTC/DoubleChecker">https://github.com/XinXU-USTC/DoubleChecker</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21285v2">PDF</a> 10 pages</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†Double-Checkeræ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¿ƒè¿›æ˜ç¡®çš„è‡ªæˆ‘æ‰¹åˆ¤å’Œè¿­ä»£ä¼˜åŒ–æ…¢é€Ÿæ€è€ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡åœ¨æˆ‘ä»¬çš„ç²¾é€‰çš„1,730ä¸ªè‡ªæˆ‘æ‰¹åˆ¤å®ä¾‹ä¸Šè¿›è¡Œå¾®è°ƒï¼ŒDouble-Checkerèµ‹èƒ½é•¿è®¤çŸ¥é“¾LLMsåœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿›è¡Œè¿­ä»£æ‰¹åˆ¤å’Œä¼˜åŒ–ä»–ä»¬çš„è¾“å‡ºï¼Œç›´åˆ°ä»–ä»¬æ ¹æ®è‡ªæˆ‘ç”Ÿæˆçš„æ‰¹åˆ¤è¯„ä¼°è‡ªå·±çš„è§£å†³æ–¹æ¡ˆä¸ºæ­£ç¡®ã€‚éªŒè¯ç»“æœè¡¨æ˜ï¼Œè¿­ä»£è‡ªæˆ‘æ‰¹åˆ¤æ˜¾è‘—æé«˜äº†é•¿è®¤çŸ¥é“¾LLMsçš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„Double-Checkeråœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„AIMEåŸºå‡†æµ‹è¯•ä¸­ï¼Œå°†pass@1æ€§èƒ½ä»4.4%æé«˜åˆ°18.2%ã€‚è¿™ä¸ºå¼€å‘èƒ½å¤Ÿè¿›è¡Œç»“æ„åŒ–è‡ªæˆ‘æ‰¹åˆ¤çš„æ›´å¯é ã€æ›´æœ‰æ•ˆçš„LLMsæŒ‡æ˜äº†æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Double-Checkeræ¡†æ¶æ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡ä¿ƒè¿›è‡ªæˆ‘æ‰¹åˆ¤å’Œè¿­ä»£ä¼˜åŒ–ã€‚</li>
<li>Double-Checkerèµ‹èƒ½LLMsåœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿›è¡Œè¿­ä»£æ‰¹åˆ¤å’Œä¼˜åŒ–è¾“å‡ºã€‚</li>
<li>é€šè¿‡åœ¨ç²¾é€‰çš„1,730ä¸ªè‡ªæˆ‘æ‰¹åˆ¤å®ä¾‹ä¸Šè¿›è¡Œå¾®è°ƒï¼ŒDouble-Checkeræé«˜LLMsçš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>éªŒè¯ç»“æœè¡¨æ˜ï¼Œè¿­ä»£è‡ªæˆ‘æ‰¹åˆ¤å¯¹LLMsçš„æ¨ç†èƒ½åŠ›æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>Double-Checkeråœ¨AIMEåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°æ˜¾è‘—ï¼Œå°†pass@1æ€§èƒ½ä»4.4%æé«˜åˆ°18.2%ã€‚</li>
<li>å¼€å‘çš„Double-Checkeræ¡†æ¶ä¸ºå¼€å‘æ›´å¯é ã€æ›´æœ‰æ•ˆçš„LLMsæŒ‡æ˜äº†æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21285">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b49a98502fcf428f77375acbc3d1d8a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74a77e4eb7a2317d61ae7bacabacc63d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6dd4b7c8276cb0d1853ea9cbcd9232bd.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="KnowRL-Exploring-Knowledgeable-Reinforcement-Learning-for-Factuality"><a href="#KnowRL-Exploring-Knowledgeable-Reinforcement-Learning-for-Factuality" class="headerlink" title="KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality"></a>KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality</h2><p><strong>Authors:Baochang Ren, Shuofei Qiao, Wenhao Yu, Huajun Chen, Ningyu Zhang</strong></p>
<p>Large Language Models (LLMs), particularly slow-thinking models, often exhibit severe hallucination, outputting incorrect content due to an inability to accurately recognize knowledge boundaries during reasoning. While Reinforcement Learning (RL) can enhance complex reasoning abilities, its outcome-oriented reward mechanism often lacks factual supervision over the thinking process, further exacerbating the hallucination problem. To address the high hallucination in slow-thinking models, we propose Knowledge-enhanced RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. This targeted factual input during RL training enables the model to learn and internalize fact-based reasoning strategies. By directly rewarding adherence to facts within the reasoning steps, KnowRL fosters a more reliable thinking process. Experimental results on three hallucination evaluation datasets and two reasoning evaluation datasets demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking models while maintaining their original strong reasoning capabilities. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/zjunlp/KnowRL">https://github.com/zjunlp/KnowRL</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå°¤å…¶æ˜¯æ…¢æ€è€ƒæ¨¡å‹ï¼Œå¸¸å¸¸è¡¨ç°å‡ºä¸¥é‡çš„å¹»è§‰ï¼Œç”±äºæ¨ç†è¿‡ç¨‹ä¸­æ— æ³•å‡†ç¡®è¯†åˆ«çŸ¥è¯†è¾¹ç•Œï¼Œä»è€Œè¾“å‡ºé”™è¯¯å†…å®¹ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥å¢å¼ºå¤æ‚æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶ä»¥ç»“æœä¸ºå¯¼å‘çš„å¥–åŠ±æœºåˆ¶å¾€å¾€ç¼ºä¹å¯¹æ€è€ƒè¿‡ç¨‹çš„çœŸå®ç›‘ç£ï¼Œä»è€ŒåŠ å‰§äº†å¹»è§‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³æ…¢æ€è€ƒæ¨¡å‹ä¸­çš„é«˜å¹»è§‰é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†çŸ¥è¯†å¢å¼ºRLï¼Œå³KnowRLã€‚KnowRLé€šè¿‡å°†åŸºäºçŸ¥è¯†éªŒè¯çš„çœŸå®æ€§å¥–åŠ±èå…¥RLè®­ç»ƒè¿‡ç¨‹ï¼ŒæŒ‡å¯¼æ¨¡å‹è¿›è¡ŒåŸºäºäº‹å®çš„æ…¢æ€è€ƒï¼Œå¸®åŠ©å®ƒä»¬è¯†åˆ«çŸ¥è¯†è¾¹ç•Œã€‚KnowRLçš„ç‰¹æ®Šè®¾è®¡æ—¨åœ¨é€šè¿‡å¥–åŠ±æ¨¡å‹åœ¨æ¨ç†æ­¥éª¤ä¸­éµå¾ªäº‹å®ï¼Œæ¥åŸ¹è‚²ä¸€ä¸ªæ›´å¯é çš„æ€è€ƒè¿‡ç¨‹ã€‚åœ¨ä¸‰ä»½å¹»è§‰è¯„ä¼°æ•°æ®é›†å’Œä¸¤ä»½æ¨ç†è¯„ä¼°æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒKnowRLåœ¨å‡è½»æ…¢æ€è€ƒæ¨¡å‹çš„å¹»è§‰é—®é¢˜çš„åŒæ—¶ï¼Œä¿æŒäº†å…¶åŸæœ‰çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/zjunlp/KnowRL%E8%8E%B7%E5%8F%96%E3%80%82]">https://github.com/zjunlp/KnowRLè·å–ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19807v2">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨çŸ¥è¯†è¾¹ç•Œè¯†åˆ«å›°éš¾çš„é—®é¢˜ï¼Œå¯¼è‡´ä¸¥é‡çš„å†…å®¹å¹»è§‰è¾“å‡ºã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è™½ç„¶èƒ½æé«˜å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶ç»“æœå¯¼å‘çš„å¥–åŠ±æœºåˆ¶ç¼ºä¹æ€è€ƒè¿‡ç¨‹çš„å®é™…ç›‘ç£ï¼ŒåŠ å‰§äº†å¹»è§‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç»“åˆçŸ¥è¯†éªŒè¯çš„åŸºäºçŸ¥è¯†å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ ï¼ˆKnowRLï¼‰æ–¹æ³•ã€‚é€šè¿‡é›†æˆåŸºäºçŸ¥è¯†éªŒè¯çš„äº‹å®å¥–åŠ±æ¥æŒ‡å¯¼æ¨¡å‹è¿›è¡ŒåŸºäºäº‹å®çš„æ…¢é€Ÿæ€è€ƒï¼Œå¸®åŠ©æ¨¡å‹è®¤è¯†å…¶çŸ¥è¯†è¾¹ç•Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKnowRLåœ¨å‡å°‘æ…¢æ€è€ƒæ¨¡å‹çš„å¹»è§‰è¾“å‡ºåŒæ—¶ï¼Œä¿æŒäº†å…¶åŸæœ‰çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ—¶å­˜åœ¨çŸ¥è¯†è¾¹ç•Œè¯†åˆ«å›°éš¾çš„é—®é¢˜ï¼Œå¯¼è‡´å¹»è§‰è¾“å‡ºã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½æé«˜æ¨¡å‹çš„å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œä½†å¥–åŠ±æœºåˆ¶å¯èƒ½å¯¼è‡´å¹»è§‰é—®é¢˜è¿›ä¸€æ­¥åŠ å‰§ã€‚</li>
<li>KnowRLæ–¹æ³•é€šè¿‡é›†æˆåŸºäºçŸ¥è¯†éªŒè¯çš„äº‹å®å¥–åŠ±æ¥æŒ‡å¯¼æ¨¡å‹è¿›è¡ŒåŸºäºäº‹å®çš„æ…¢é€Ÿæ€è€ƒã€‚</li>
<li>KnowRLå¸®åŠ©æ¨¡å‹è®¤è¯†å…¶çŸ¥è¯†è¾¹ç•Œï¼Œå‡å°‘å¹»è§‰è¾“å‡ºï¼ŒåŒæ—¶ä¿æŒåŸæœ‰çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ã€‚</li>
<li>KnowRLçš„å®éªŒç»“æœå·²åœ¨ä¸‰ä¸ªå¹»è§‰è¯„ä¼°æ•°æ®é›†å’Œä¸¤ä¸ªæ¨ç†è¯„ä¼°æ•°æ®é›†ä¸Šå¾—åˆ°éªŒè¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19807">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2509db1eb86b6cb6d8984fde072a57cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0da5a2585a18f83144dba3f2c0d54196.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70987b07e3f7e65165a76cc2505811dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ef3315d839d735af0a184699ff4fd4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f7ead2cce0b09c4d586712e89566e39.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Multi-Preference-Lambda-weighted-Listwise-DPO-for-Dynamic-Preference-Alignment"><a href="#Multi-Preference-Lambda-weighted-Listwise-DPO-for-Dynamic-Preference-Alignment" class="headerlink" title="Multi-Preference Lambda-weighted Listwise DPO for Dynamic Preference   Alignment"></a>Multi-Preference Lambda-weighted Listwise DPO for Dynamic Preference   Alignment</h2><p><strong>Authors:Yuhui Sun, Xiyao Wang, Zixi Li, Jinman Zhao</strong></p>
<p>While large-scale unsupervised language models (LMs) capture broad world knowledge and reasoning capabilities, steering their behavior toward desired objectives remains challenging due to the lack of explicit supervision. Existing alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on training a reward model and performing reinforcement learning to align with human preferences. However, RLHF is often computationally intensive, unstable, and sensitive to hyperparameters.   To address these limitations, Direct Preference Optimization (DPO) was introduced as a lightweight and stable alternative, enabling direct alignment of language models with pairwise preference data via classification loss. However, DPO and its extensions generally assume a single static preference distribution, limiting flexibility in multi-objective or dynamic alignment settings.   In this paper, we propose a novel framework: Multi-Preference Lambda-weighted Listwise DPO, which extends DPO to incorporate multiple human preference dimensions (e.g., helpfulness, harmlessness, informativeness) and enables dynamic interpolation through a controllable simplex-weighted formulation. Our method supports both listwise preference feedback and flexible alignment across varying user intents without re-training. Empirical and theoretical analysis demonstrates that our method is as effective as traditional DPO on static objectives while offering greater generality and adaptability for real-world deployment. </p>
<blockquote>
<p>å¤§è§„æ¨¡æ— ç›‘ç£è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰è™½ç„¶æ•æ‰åˆ°äº†å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œä½†ç”±äºç¼ºä¹æ˜ç¡®çš„ç›‘ç£ï¼Œå°†å…¶è¡Œä¸ºå¯¼å‘æ—¢å®šç›®æ ‡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰çš„å¯¹é½æŠ€æœ¯ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ï¼Œä¾èµ–äºè®­ç»ƒå¥–åŠ±æ¨¡å‹å’Œè¿›è¡Œå¼ºåŒ–å­¦ä¹ ä»¥ç¬¦åˆäººç±»åå¥½ã€‚ç„¶è€Œï¼ŒRLHFé€šå¸¸è®¡ç®—é‡å¤§ã€ä¸ç¨³å®šï¼Œå¯¹è¶…å‚æ•°æ•æ„Ÿã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œå¼•å…¥äº†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä½œä¸ºè½»é‡çº§å’Œç¨³å®šçš„æ›¿ä»£æ–¹æ¡ˆï¼Œé€šè¿‡åˆ†ç±»æŸå¤±ç›´æ¥å¯¹é½è¯­è¨€æ¨¡å‹ä¸æˆå¯¹åå¥½æ•°æ®ã€‚ç„¶è€Œï¼ŒDPOåŠå…¶æ‰©å±•é€šå¸¸å‡è®¾å•ä¸ªé™æ€åå¥½åˆ†å¸ƒï¼Œè¿™åœ¨å¤šç›®æ ‡æˆ–åŠ¨æ€å¯¹é½è®¾ç½®ä¸­é™åˆ¶äº†çµæ´»æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼šå¤šåå¥½LambdaåŠ æƒåˆ—è¡¨å¼DPOï¼Œå®ƒæ‰©å±•äº†DPOä»¥èå…¥å¤šç§äººç±»åå¥½ç»´åº¦ï¼ˆä¾‹å¦‚ï¼Œæœ‰ç”¨æ€§ã€æ— å®³æ€§ã€ä¿¡æ¯é‡ï¼‰ï¼Œå¹¶é€šè¿‡å¯æ§çš„å•çº¯å½¢åŠ æƒå…¬å¼å®ç°åŠ¨æ€æ’å€¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒåˆ—è¡¨å¼åå¥½åé¦ˆï¼Œå¹¶åœ¨ä¸åŒçš„ç”¨æˆ·æ„å›¾ä¹‹é—´å®ç°çµæ´»å¯¹é½ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚å®è¯å’Œç†è®ºåˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é™æ€ç›®æ ‡ä¸Šä¸ä¼ ç»ŸDPOåŒæ ·æœ‰æ•ˆï¼ŒåŒæ—¶æä¾›äº†æ›´å¤§çš„é€šç”¨æ€§å’Œé€‚åº”æ€§ï¼Œé€‚ç”¨äºç°å®ä¸–ç•Œéƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19780v3">PDF</a> 11 pages, 6 figures, appendix included. To appear in Proceedings of   AAAI 2026. Code:   <a target="_blank" rel="noopener" href="https://github.com/yuhui15/Multi-Preference-Lambda-weighted-DPO">https://github.com/yuhui15/Multi-Preference-Lambda-weighted-DPO</a></p>
<p><strong>Summary</strong><br>å¤§å‹æ— ç›‘ç£è¯­è¨€æ¨¡å‹æ•æ‰å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œä½†ç¼ºä¹æ˜ç¡®çš„ç›‘ç£å¯¼è‡´éš¾ä»¥æ§åˆ¶å…¶è¡Œä¸ºä»¥è¾¾åˆ°æœŸæœ›ç›®æ ‡ã€‚ç°æœ‰å¯¹é½æŠ€æœ¯å¦‚å¼ºåŒ–å­¦ä¹ ç»“åˆäººç±»åé¦ˆï¼ˆRLHFï¼‰ä¾èµ–å¥–åŠ±æ¨¡å‹çš„è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ ä»¥å¯¹é½äººç±»åå¥½ï¼Œä½†RLHFè®¡ç®—é‡å¤§ã€ä¸ç¨³å®šä¸”å¯¹è¶…å‚æ•°æ•æ„Ÿã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œå¼•å…¥è½»é‡çº§ä¸”ç¨³å®šçš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ã€‚ç„¶è€Œï¼ŒDPOåŠå…¶æ‰©å±•å‡è®¾å•ä¸€é™æ€åå¥½åˆ†å¸ƒï¼Œé™åˆ¶äº†å¤šç›®æ ‡æˆ–åŠ¨æ€å¯¹é½è®¾ç½®çš„çµæ´»æ€§ã€‚æœ¬æ–‡æå‡ºæ–°å‹æ¡†æ¶ï¼šå¤šåå¥½Î»åŠ æƒåˆ—è¡¨å¼DPOï¼Œå®ƒå°†DPOæ‰©å±•åˆ°åŒ…å«å¤šé‡äººç±»åå¥½ç»´åº¦ï¼ˆå¦‚å¸®åŠ©æ€§ã€æ— å®³æ€§å’Œä¿¡æ¯é‡ï¼‰ï¼Œå¹¶é€šè¿‡å¯æ§çš„å•çº¯åŠ æƒå…¬å¼å®ç°åŠ¨æ€æ’å€¼ã€‚è¯¥æ–¹æ³•æ”¯æŒåˆ—è¡¨å¼åå¥½åé¦ˆï¼Œå¹¶åœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹çµæ´»å¯¹é½ä¸åŒç”¨æˆ·æ„å›¾ã€‚ç»éªŒåˆ†æå’Œç†è®ºè¯æ˜æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨é™æ€ç›®æ ‡ä¸Šä¸ä¼ ç»ŸDPOåŒæ ·æœ‰æ•ˆï¼ŒåŒæ—¶æä¾›æ›´å¤§çš„é€šç”¨æ€§å’Œé€‚åº”æ€§ä»¥é€‚åº”ç°å®ä¸–ç•Œçš„éƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ— ç›‘ç£è¯­è¨€æ¨¡å‹é¢ä¸´å¦‚ä½•æ§åˆ¶è¡Œä¸ºä»¥è¾¾åˆ°æœŸæœ›ç›®æ ‡çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰å¯¹é½æŠ€æœ¯å¦‚RLHFå­˜åœ¨è®¡ç®—é‡å¤§ã€ä¸ç¨³å®šåŠå¯¹è¶…å‚æ•°æ•æ„Ÿçš„é—®é¢˜ã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä½œä¸ºä¸€ç§è½»é‡çº§ã€ç¨³å®šçš„æ›¿ä»£æ–¹æ³•è¢«å¼•å…¥ã€‚</li>
<li>DPOåŠå…¶æ‰©å±•æœ‰å•ä¸€é™æ€åå¥½åˆ†å¸ƒçš„å±€é™æ€§ã€‚</li>
<li>æ–°å‹æ¡†æ¶â€”â€”å¤šåå¥½Î»åŠ æƒåˆ—è¡¨å¼DPOæ‰©å±•äº†DPOï¼Œçº³å…¥å¤šé‡äººç±»åå¥½ç»´åº¦ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å¯æ§çš„å•çº¯åŠ æƒå…¬å¼å®ç°åŠ¨æ€æ’å€¼ï¼Œæ”¯æŒåˆ—è¡¨å¼åå¥½åé¦ˆã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨é™æ€ç›®æ ‡ä¸Šä¸ä¼ ç»ŸDPOæ•ˆæœç›¸å½“ï¼ŒåŒæ—¶æä¾›æ›´å¤§çš„é€šç”¨æ€§å’Œé€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19780">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3d8f540b559bc4332539001c39c1778a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db6c536fb58ef2196cde29851f55b41e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ee72711aa04367af42f1c219aa78e52.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-11/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-11/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-11/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-35b6636740bec71d403330fedf176861.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-11  Towards Multimodal Understanding via Stable Diffusion as a Task-Aware   Feature Extractor
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-10/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-00cf902c75882df6f145b070e38129d1.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-10  Reflections Unlock Geometry-Aware Reflection Disentanglement in 3D   Gaussian Splatting for Photorealistic Scenes Rendering
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30191.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
