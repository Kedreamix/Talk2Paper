<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-08-12  SPARSE Data, Rich Results Few-Shot Semi-Supervised Learning via   Class-Conditioned Image Translation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-9d643a662c122ed8f40465c8e194d007.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    41 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-12-更新"><a href="#2025-08-12-更新" class="headerlink" title="2025-08-12 更新"></a>2025-08-12 更新</h1><h2 id="SPARSE-Data-Rich-Results-Few-Shot-Semi-Supervised-Learning-via-Class-Conditioned-Image-Translation"><a href="#SPARSE-Data-Rich-Results-Few-Shot-Semi-Supervised-Learning-via-Class-Conditioned-Image-Translation" class="headerlink" title="SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via   Class-Conditioned Image Translation"></a>SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via   Class-Conditioned Image Translation</h2><p><strong>Authors:Guido Manni, Clemente Lauretti, Loredana Zollo, Paolo Soda</strong></p>
<p>Deep learning has revolutionized medical imaging, but its effectiveness is severely limited by insufficient labeled training data. This paper introduces a novel GAN-based semi-supervised learning framework specifically designed for low labeled-data regimes, evaluated across settings with 5 to 50 labeled samples per class. Our approach integrates three specialized neural networks – a generator for class-conditioned image translation, a discriminator for authenticity assessment and classification, and a dedicated classifier – within a three-phase training framework. The method alternates between supervised training on limited labeled data and unsupervised learning that leverages abundant unlabeled images through image-to-image translation rather than generation from noise. We employ ensemble-based pseudo-labeling that combines confidence-weighted predictions from the discriminator and classifier with temporal consistency through exponential moving averaging, enabling reliable label estimation for unlabeled data. Comprehensive evaluation across eleven MedMNIST datasets demonstrates that our approach achieves statistically significant improvements over six state-of-the-art GAN-based semi-supervised methods, with particularly strong performance in the extreme 5-shot setting where the scarcity of labeled data is most challenging. The framework maintains its superiority across all evaluated settings (5, 10, 20, and 50 shots per class). Our approach offers a practical solution for medical imaging applications where annotation costs are prohibitive, enabling robust classification performance even with minimal labeled data. Code is available at <a target="_blank" rel="noopener" href="https://github.com/GuidoManni/SPARSE">https://github.com/GuidoManni/SPARSE</a>. </p>
<blockquote>
<p>深度学习已经彻底改变了医学影像领域，但其有效性因缺乏足够的标注训练数据而受到严重限制。本文介绍了一种基于GAN的半监督学习框架，该框架专为低标注数据环境设计，并在每类有5到50个标注样本的各种环境中进行了评估。我们的方法整合了三个专用神经网络——用于类条件图像翻译的生成器、用于真实性和分类评估的鉴别器以及专用分类器——在一个三阶段训练框架中。该方法在有限的标注数据上进行监督训练，并在无监督学习中利用大量未标注图像进行图像到图像的翻译，而不是通过噪声生成。我们采用基于集合的伪标签方法，该方法结合了鉴别器和分类器的加权预测，并通过指数移动平均实现时间一致性，从而为未标注数据提供可靠的标签估计。在11个MedMNIST数据集上的综合评估表明，我们的方法在六个最先进的基于GAN的半监督方法上实现了统计学上的显著改善，特别是在极端的5个样本设置中，标注数据的稀缺性最具挑战性。该框架在所有评估环境（每类5、10、20和50个样本）中均保持其优势。我们的方法为医学影像应用提供了实际解决方案，其中标注成本高昂，即使在少量标注数据的情况下也能实现稳健的分类性能。代码可从<a target="_blank" rel="noopener" href="https://github.com/GuidoManni/SPARSE%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/GuidoManni/SPARSE获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06429v1">PDF</a> </p>
<p><strong>Summary</strong><br>深度学习在医学成像领域有革命性进展，但受限于缺乏标注的训练数据。本文提出一种基于GAN的半监督学习框架，适用于少量标注数据的场景。该框架包含三个神经网络，通过监督学习和无监督学习相结合的方式进行训练，并在MedMNIST数据集上取得了显著成果。该框架在标注数据稀缺时具有显著优势，且适用于医学成像应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文介绍了一种基于GAN的半监督学习框架，适用于医学成像中标注数据不足的情况。</li>
<li>该框架集成了三个神经网络：用于类条件图像翻译的发电机、用于真实性和分类评估的鉴别器以及专用分类器。</li>
<li>该方法通过监督学习和无监督学习相结合的方式训练模型，利用有限标注数据和大量未标注图像。</li>
<li>采用基于集合的伪标签方法，结合鉴别器和分类器的预测结果，通过指数移动平均实现时间一致性，为未标注数据提供可靠的标签估计。</li>
<li>在多个MedMNIST数据集上的综合评估表明，该框架在少量标注数据的设置下表现优异，相对于六种最先进的GAN半监督方法具有统计上的显著改善。</li>
<li>该框架在标注数据极度稀缺的场景下（如每类仅有5个样本）表现出特别强的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06429">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7511d74c69ca1d831ead8fbab9ceae67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dae2068346be72f0e9500126a0e81124.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Cyberbullying-Detection-via-Aggression-Enhanced-Prompting"><a href="#Cyberbullying-Detection-via-Aggression-Enhanced-Prompting" class="headerlink" title="Cyberbullying Detection via Aggression-Enhanced Prompting"></a>Cyberbullying Detection via Aggression-Enhanced Prompting</h2><p><strong>Authors:Aisha Saeid, Anu Sabu, Girish A. Koushik, Ferrante Neri, Diptesh Kanojia</strong></p>
<p>Detecting cyberbullying on social media remains a critical challenge due to its subtle and varied expressions. This study investigates whether integrating aggression detection as an auxiliary task within a unified training framework can enhance the generalisation and performance of large language models (LLMs) in cyberbullying detection. Experiments are conducted on five aggression datasets and one cyberbullying dataset using instruction-tuned LLMs. We evaluated multiple strategies: zero-shot, few-shot, independent LoRA fine-tuning, and multi-task learning (MTL). Given the inconsistent results of MTL, we propose an enriched prompt pipeline approach in which aggression predictions are embedded into cyberbullying detection prompts to provide contextual augmentation. Preliminary results show that the enriched prompt pipeline consistently outperforms standard LoRA fine-tuning, indicating that aggression-informed context significantly boosts cyberbullying detection. This study highlights the potential of auxiliary tasks, such as aggression detection, to improve the generalisation of LLMs for safety-critical applications on social networks. </p>
<blockquote>
<p>检测社交媒体上的网络欺凌仍然是一个关键的挑战，因为其表达具有细微性和多样性。本研究旨在探究在统一训练框架内整合攻击检测作为辅助任务，能否增强大型语言模型（LLM）在网络欺凌检测中的通用性和性能。本研究使用指令调整的大型语言模型，在五个攻击数据集和一个网络欺凌数据集上进行了实验。我们评估了多种策略：零样本、小样本、独立LoRA微调和多任务学习（MTL）。鉴于多任务学习的不一致结果，我们提出了一种丰富的提示管道方法，其中攻击预测被嵌入到网络欺凌检测提示中以提供上下文增强。初步结果表明，丰富的提示管道始终优于标准的LoRA微调，这表明攻击信息上下文显著提高了网络欺凌检测效果。本研究突出了辅助任务（如攻击检测）的潜力，可以改进大型语言模型在社交媒体安全关键应用的通用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06360v1">PDF</a> Accepted to RANLP 2025</p>
<p><strong>Summary</strong>：本研究探讨了将攻击检测作为辅助任务融入统一训练框架，以提高大型语言模型在网络欺凌检测中的通用性和性能。实验表明，将攻击检测嵌入到网络欺凌检测提示中的丰富提示管道方法能够一致地优于标准LoRA微调，显示出攻击检测提供的上下文信息对网络欺凌检测的增强作用。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>网络欺凌的微妙和多变表达使其检测成为一项关键挑战。</li>
<li>集成攻击检测作为辅助任务可以提高大型语言模型在网络欺凌检测中的性能。</li>
<li>实验涉及多种策略，包括零样本、少样本、独立LoRA微调和多任务学习。</li>
<li>多任务学习的结果不一致，提出将攻击检测嵌入网络欺凌检测提示的丰富提示管道方法。</li>
<li>初步结果表明，丰富提示管道方法优于标准LoRA微调。</li>
<li>攻击检测提供的上下文信息对网络欺凌检测有显著提升作用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06360">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-72bfe1cacb6dcea9dee99550405c239a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28d984c26a74d681aa4b4117dfa35a50.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4d65f73996b250f0e66b84d6ac7b3bf1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="FedMeNF-Privacy-Preserving-Federated-Meta-Learning-for-Neural-Fields"><a href="#FedMeNF-Privacy-Preserving-Federated-Meta-Learning-for-Neural-Fields" class="headerlink" title="FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields"></a>FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields</h2><p><strong>Authors:Junhyeog Yun, Minui Hong, Gunhee Kim</strong></p>
<p>Neural fields provide a memory-efficient representation of data, which can effectively handle diverse modalities and large-scale data. However, learning to map neural fields often requires large amounts of training data and computations, which can be limited to resource-constrained edge devices. One approach to tackle this limitation is to leverage Federated Meta-Learning (FML), but traditional FML approaches suffer from privacy leakage. To address these issues, we introduce a novel FML approach called FedMeNF. FedMeNF utilizes a new privacy-preserving loss function that regulates privacy leakage in the local meta-optimization. This enables the local meta-learner to optimize quickly and efficiently without retaining the client’s private data. Our experiments demonstrate that FedMeNF achieves fast optimization speed and robust reconstruction performance, even with few-shot or non-IID data across diverse data modalities, while preserving client data privacy. </p>
<blockquote>
<p>神经网络场提供了一种数据表示方法，这种方法具有高效的内存占用，并能够有效地处理多种模态和大规模数据。然而，学习映射神经网络通常需要大量的训练数据和计算资源，这在资源受限的边缘设备上可能会受到限制。一种解决此限制的方法是采用联邦元学习（FML），但传统的FML方法存在隐私泄露问题。为了解决这些问题，我们引入了一种新型的FML方法，称为FedMeNF。FedMeNF利用了一种新的隐私保护损失函数，该函数可以在本地元优化过程中控制隐私泄露。这使得本地元学习者能够在不保留客户端私有数据的情况下快速有效地进行优化。我们的实验表明，即使在多种模态数据的少量样本或非独立同分布（non-IID）数据情况下，FedMeNF也能实现快速优化和稳健的重建性能，同时保护客户端数据隐私。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06301v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>神经网络场为数据提供了内存高效的表示，可处理多样模态和大规模数据。然而，学习映射神经网络场通常需要大量的训练数据和计算资源，这在资源受限的边缘设备上可能受限。为解决这一问题，我们引入了名为FedMeNF的新型联邦元学习方法。FedMeNF利用新的隐私保护损失函数，对本地元优化中的隐私泄露进行规范。这允许本地元学习者快速高效地进行优化，同时无需保留客户端的私有数据。实验表明，即使在各种数据模态的少量样本或非独立同分布数据下，FedMeNF也能实现快速优化和稳健的重建性能，同时保护客户端数据隐私。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经网络场是内存高效的数据表示方法，可处理多样模态和大规模数据。</li>
<li>学习映射神经网络场通常需要大量的训练数据和计算资源。</li>
<li>联邦元学习（FML）是解决资源受限设备上的训练数据限制的一种方法。</li>
<li>传统的FML方法存在隐私泄露问题。</li>
<li>FedMeNF是一种新型的联邦元学习方法，利用隐私保护损失函数规范本地元优化中的隐私泄露。</li>
<li>FedMeNF允许快速高效的本地元优化，同时保护客户端的私有数据。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06301">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c866c16930bd278006e8ff2cbe3d520d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1de5f6798be3bacc4eaf018d10924d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb688b443ac454b7dd4f03b283525e6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1c2484cd46b5858cb058a793654922d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="IOCC-Aligning-Semantic-and-Cluster-Centers-for-Few-shot-Short-Text-Clustering"><a href="#IOCC-Aligning-Semantic-and-Cluster-Centers-for-Few-shot-Short-Text-Clustering" class="headerlink" title="IOCC: Aligning Semantic and Cluster Centers for Few-shot Short Text   Clustering"></a>IOCC: Aligning Semantic and Cluster Centers for Few-shot Short Text   Clustering</h2><p><strong>Authors:Jixuan Yin, Zhihao Yao, Wenshuai Huo, Xinmiao Yu, Xiaocheng Feng, Bo Li</strong></p>
<p>In clustering tasks, it is essential to structure the feature space into clear, well-separated distributions. However, because short text representations have limited expressiveness, conventional methods struggle to identify cluster centers that truly capture each category’s underlying semantics, causing the representations to be optimized in suboptimal directions. To address this issue, we propose IOCC, a novel few-shot contrastive learning method that achieves alignment between the cluster centers and the semantic centers. IOCC consists of two key modules: Interaction-enhanced Optimal Transport (IEOT) and Center-aware Contrastive Learning (CACL). Specifically, IEOT incorporates semantic interactions between individual samples into the conventional optimal transport problem, and generate pseudo-labels. Based on these pseudo-labels, we aggregate high-confidence samples to construct pseudo-centers that approximate the semantic centers. Next, CACL optimizes text representations toward their corresponding pseudo-centers. As training progresses, the collaboration between the two modules gradually reduces the gap between cluster centers and semantic centers. Therefore, the model will learn a high-quality distribution, improving clustering performance. Extensive experiments on eight benchmark datasets show that IOCC outperforms previous methods, achieving up to 7.34% improvement on challenging Biomedical dataset and also excelling in clustering stability and efficiency. The code is available at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/IOCC-C438">https://anonymous.4open.science/r/IOCC-C438</a>. </p>
<blockquote>
<p>在聚类任务中，将特征空间结构化为清晰、分离良好的分布至关重要。然而，由于短文本表示的表达能力有限，传统方法难以识别真正捕捉每个类别底层语义的聚类中心，导致表示的优化方向不佳。为了解决这一问题，我们提出了IOCC，这是一种新的少样本对比学习方法，实现了聚类中心与语义中心之间的对齐。IOCC由两个关键模块组成：交互增强最优传输（IEOT）和中心感知对比学习（CACL）。具体来说，IEOT将个体样本之间的语义交互纳入传统最优传输问题中，并生成伪标签。基于这些伪标签，我们聚合高置信度样本来构建近似语义中心的伪中心。接下来，CACL将文本表示优化朝向其对应的伪中心。随着训练的进行，两个模块之间的协作逐渐缩小了聚类中心和语义中心之间的差距。因此，模型将学习高质量分布，提高聚类性能。在八个基准数据集上的广泛实验表明，IOCC优于以前的方法，在具有挑战性的生物医学数据集上实现了高达7.34%的改进，同时在聚类稳定性和效率方面也表现出色。代码可用在：<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/IOCC-C438%E3%80%82">https://anonymous.4open.science/r/IOCC-C438。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06126v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文提出了一种基于少数样本对比学习的聚类方法IOCC，旨在解决短文本表示的问题。通过构建交互增强的最优传输模块（IEOT）和中心感知对比学习模块（CACL），IOCC实现了对聚类中心和语义中心的匹配。实验结果显示，IOCC在多个基准数据集上取得了显著的改进，特别是在具有挑战性的生物医学数据集上，实现了高达7.34%的性能提升，且在聚类稳定性和效率方面也表现出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IOCC是一种用于聚类任务的少数样本对比学习方法，旨在解决短文本表示的问题。</li>
<li>IOCC通过构建IEOT模块，将样本间的语义交互纳入传统最优传输问题中，生成伪标签。</li>
<li>基于伪标签，IOCC聚合高置信度样本构建伪中心，近似语义中心。</li>
<li>CACL模块则致力于优化文本表示，使其朝向对应的伪中心。</li>
<li>随着训练的进行，IEOT和CACL两个模块的协作逐渐缩小了聚类中心和语义中心的差距。</li>
<li>实验结果显示，IOCC在多个数据集上取得了显著性能提升，特别是在生物医学数据集上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06126">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-88bbbed53dfce0b2040c7c1fd345d5a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d769d5af080da0ad6e6ea441610e2e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a7d9b4f0ade5f3a3ef00d30466fc247.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec37e894812c8598909a2946ac8c7899.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Prompting-for-Extractive-Quranic-QA-with-Instruction-Tuned-LLMs"><a href="#Few-Shot-Prompting-for-Extractive-Quranic-QA-with-Instruction-Tuned-LLMs" class="headerlink" title="Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs"></a>Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs</h2><p><strong>Authors:Mohamed Basem, Islam Oshallah, Ali Hamdi, Ammar Mohammed</strong></p>
<p>This paper presents two effective approaches for Extractive Question Answering (QA) on the Quran. It addresses challenges related to complex language, unique terminology, and deep meaning in the text. The second uses few-shot prompting with instruction-tuned large language models such as Gemini and DeepSeek. A specialized Arabic prompt framework is developed for span extraction. A strong post-processing system integrates subword alignment, overlap suppression, and semantic filtering. This improves precision and reduces hallucinations. Evaluations show that large language models with Arabic instructions outperform traditional fine-tuned models. The best configuration achieves a pAP10 score of 0.637. The results confirm that prompt-based instruction tuning is effective for low-resource, semantically rich QA tasks. </p>
<blockquote>
<p>本文介绍了两种针对《古兰经》的提取式问答（QA）的有效方法。它解决了与复杂语言、独特术语和文本深层意义相关的挑战。第二种方法使用少量提示与经过指令调整的 大型语言模型，如Gemini和DeepSeek。开发了一个专门的阿拉伯语提示框架，用于跨度提取。强大的后处理系统集成了子词对齐、重叠抑制和语义过滤。这提高了精度并减少了幻觉。评估表明，带有阿拉伯语指令的大型语言模型优于传统微调模型。最佳配置达到pAP10得分为0.637。结果证实，基于提示的指令调整对于低资源、语义丰富的问答任务是有效的。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06103v1">PDF</a> 6 pages , 2 figures , Accepted in IMSA 2025,Egypt ,   <a target="_blank" rel="noopener" href="https://imsa.msa.edu.eg/">https://imsa.msa.edu.eg/</a></p>
<p><strong>Summary</strong><br>     本文介绍了两种针对《古兰经》的有效提取式问答方法。第二种方法采用基于指令调整的大型语言模型（如Gemini和DeepSeek）进行少量提示，开发了一个专用的阿拉伯文提示框架用于跨提取。通过整合子词对齐、抑制重叠和语义过滤的强大的后处理系统提高了精度并减少了误生成。评估显示，带有阿拉伯指令的大型语言模型优于传统微调模型，最佳配置达到pAP10分数为0.637。证明基于指令调整的提示对于资源匮乏但语义丰富的问答任务是有效的。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>该论文提出了两种针对《古兰经》的提取式问答的有效方法。</li>
<li>第二种方法使用基于指令调整的大型语言模型进行少量提示，开发了阿拉伯文提示框架用于跨提取。</li>
<li>论文强调了挑战，包括复杂语言、独特术语和文本中的深层含义。</li>
<li>通过强大的后处理系统提高了精度并减少了误生成。</li>
<li>评估显示，带有阿拉伯指令的大型语言模型性能优于传统微调模型。</li>
<li>最佳配置的pAP10分数为0.637，表明其性能较高。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06103">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-2c94999826fc14ecfc31b63e060ccaf9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62b07b47f75c32fd720fe3f5de63ad39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d643a662c122ed8f40465c8e194d007.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c435702f1ea57c62176f82b140290901.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-63b7e700cfd18f3a9d68fc5c8a2bd642.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17501311f9e37f3128bfbfd7acf183b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f408f58a717874407744f415a2f9bbe.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Dean-of-LLM-Tutors-Exploring-Comprehensive-and-Automated-Evaluation-of-LLM-generated-Educational-Feedback-via-LLM-Feedback-Evaluators"><a href="#Dean-of-LLM-Tutors-Exploring-Comprehensive-and-Automated-Evaluation-of-LLM-generated-Educational-Feedback-via-LLM-Feedback-Evaluators" class="headerlink" title="Dean of LLM Tutors: Exploring Comprehensive and Automated Evaluation of   LLM-generated Educational Feedback via LLM Feedback Evaluators"></a>Dean of LLM Tutors: Exploring Comprehensive and Automated Evaluation of   LLM-generated Educational Feedback via LLM Feedback Evaluators</h2><p><strong>Authors:Keyang Qian, Yixin Cheng, Rui Guan, Wei Dai, Flora Jin, Kaixun Yang, Sadia Nawaz, Zachari Swiecki, Guanliang Chen, Lixiang Yan, Dragan Gašević</strong></p>
<p>The use of LLM tutors to provide automated educational feedback to students on student assignment submissions has received much attention in the AI in Education field. However, the stochastic nature and tendency for hallucinations in LLMs can undermine both quality of learning experience and adherence to ethical standards. To address this concern, we propose a method that uses LLM feedback evaluators (DeanLLMs) to automatically and comprehensively evaluate feedback generated by LLM tutor for submissions on university assignments before it is delivered to students. This allows low-quality feedback to be rejected and enables LLM tutors to improve the feedback they generated based on the evaluation results. We first proposed a comprehensive evaluation framework for LLM-generated educational feedback, comprising six dimensions for feedback content, seven for feedback effectiveness, and three for hallucination types. Next, we generated a virtual assignment submission dataset covering 85 university assignments from 43 computer science courses using eight commonly used commercial LLMs. We labelled and open-sourced the assignment dataset to support the fine-tuning and evaluation of LLM feedback evaluators. Our findings show that o3-pro demonstrated the best performance in zero-shot labelling of feedback while o4-mini demonstrated the best performance in few-shot labelling of feedback. Moreover, GPT-4.1 achieved human expert level performance after fine-tuning (Accuracy 79.8%, F1-score 79.4%; human average Accuracy 78.3%, F1-score 82.6%). Finally, we used our best-performance model to evaluate 2,000 assignment feedback instances generated by 10 common commercial LLMs, 200 each, to compare the quality of feedback generated by different LLMs. Our LLM feedback evaluator method advances our ability to automatically provide high-quality and reliable educational feedback to students. </p>
<blockquote>
<p>使用大型语言模型（LLM）辅导工具为学生提供自动化教育反馈在人工智能教育领域引起了广泛关注。然而，大型语言模型的随机性和出现幻觉的倾向可能会破坏学习体验的质量和遵守道德标准。为了解决这一担忧，我们提出了一种使用LLM反馈评估器（DeanLLMs）的方法，该方法可以自动和全面地评估LLM辅导工具针对大学作业提交的反馈，然后再将其递送给学生。这允许拒绝低质量的反馈，并使得LLM辅导工具可以根据评估结果改进其生成的反馈。我们首次为LLM生成的教育反馈提出了一个全面的评估框架，该框架包括六个关于反馈内容的维度，七个关于反馈效果的维度和三个关于幻觉类型的维度。接下来，我们使用八种常见商业LLM生成了一个虚拟的作业提交数据集，该数据集涵盖了来自43门计算机科学课程的85项作业。我们对作业数据集进行了标注并开源，以支持LLM反馈评估器的微调与评估。我们的研究结果表明，在零样本标记反馈方面，o3-pro表现出最佳性能，而在少样本标记反馈方面，o4-mini表现出最佳性能。此外，GPT-4.1在经过微调后达到了人类专家的水平（准确率79.8%，F1分数79.4%；人类平均准确率78.3%，F1分数82.6%）。最后，我们使用性能最佳的模型评估了由十种常见商业LLM生成的2000个作业反馈实例（每种LLM 200个），以比较不同LLM生成的反馈质量。我们的LLM反馈评估器方法提高了我们为学生提供高质量和可靠教育反馈的自动化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05952v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLM辅导工具为学生作业提供自动化反馈在教育界备受关注。然而，LLM的随机性和幻觉倾向可能影响学习体验和遵守道德标准。为此，研究团队提出了使用LLM反馈评估系统（DeanLLMs）自动全面评估LLM辅导工具提供的反馈的方法。此方法能在学生作业提交后对学生的作业反馈进行自动评价，对低质量的反馈进行筛选。该研究团队提出了全面的评估框架，并为评估反馈质量开发了一个虚拟作业提交数据集。最终，通过评估模型，团队发现GPT-4.1在经过微调后达到人类专家级别的性能。此外，该研究还使用最佳性能模型评估了不同LLM生成的反馈质量。此研究提高了为学生自动提供高质量可靠教育反馈的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM辅导工具提供自动化教育反馈引起关注。</li>
<li>LLM的随机性和幻觉可能影响学习体验和遵守道德标准。</li>
<li>使用DeanLLMs方法自动全面评估LLM提供的反馈。</li>
<li>提出一个包含多维度评价体系的综合评估框架来评估LLM生成的反馈质量。</li>
<li>开发了一个虚拟作业提交数据集支持LLM反馈评估模型的微调与评估。</li>
<li>GPT-4.1经过微调后性能接近人类专家水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05952">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e9150ad5054987fda80cf103440fa8c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-517867bb87f1821ddb70e1acf73b126b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4fc45a0845b250564b010c4793d59210.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64bed3f9c6e5b220b70231eeb6987589.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VISTA-Vision-Language-Imitation-of-Situational-Thinking-and-Attention-for-Human-Like-Driver-Focus-in-Dynamic-Environments"><a href="#VISTA-Vision-Language-Imitation-of-Situational-Thinking-and-Attention-for-Human-Like-Driver-Focus-in-Dynamic-Environments" class="headerlink" title="VISTA: Vision-Language Imitation of Situational Thinking and Attention   for Human-Like Driver Focus in Dynamic Environments"></a>VISTA: Vision-Language Imitation of Situational Thinking and Attention   for Human-Like Driver Focus in Dynamic Environments</h2><p><strong>Authors:Kaiser Hamid, Khandakar Ashrafi Akbar, Nade Liang</strong></p>
<p>Driver visual attention prediction is a critical task in autonomous driving and human-computer interaction (HCI) research. Most prior studies focus on estimating attention allocation at a single moment in time, typically using static RGB images such as driving scene pictures. In this work, we propose a vision-language framework that models the changing landscape of drivers’ gaze through natural language, using few-shot and zero-shot learning on single RGB images. We curate and refine high-quality captions from the BDD-A dataset using human-in-the-loop feedback, then fine-tune LLaVA to align visual perception with attention-centric scene understanding. Our approach integrates both low-level cues and top-down context (e.g., route semantics, risk anticipation), enabling language-based descriptions of gaze behavior. We evaluate performance across training regimes (few shot, and one-shot) and introduce domain-specific metrics for semantic alignment and response diversity. Results show that our fine-tuned model outperforms general-purpose VLMs in attention shift detection and interpretability. To our knowledge, this is among the first attempts to generate driver visual attention allocation and shifting predictions in natural language, offering a new direction for explainable AI in autonomous driving. Our approach provides a foundation for downstream tasks such as behavior forecasting, human-AI teaming, and multi-agent coordination. </p>
<blockquote>
<p>驾驶者视觉注意力预测在自动驾驶和人机交互（HCI）研究中是一项至关重要的任务。早期的大部分研究主要集中在估计单一时刻的注意力分配，通常使用静态RGB图像，如驾驶场景图片。在这项工作中，我们提出了一个视觉语言框架，通过自然语言对驾驶者视线变化进行建模，利用单RGB图像的少样本和零样本学习。我们通过人类参与反馈来精炼和筛选BDD-A数据集的高质量标题，然后对LLaVA进行微调，使视觉感知与以注意力为中心的场景理解保持一致。我们的方法结合了低级线索和自上而下上下文（例如，路线语义、风险预测），实现了基于语言的视线行为描述。我们评估了不同训练模式（少样本和一次学习）的性能，并引入了用于语义对齐和响应多样性的领域特定指标。结果表明，经过微调的模型在注意力转移检测和可解释性方面优于通用VLMs。据我们所知，这是首次尝试用自然语言生成驾驶者视觉注意力分配和转移预测，为自动驾驶中的可解释人工智能提供了新的方向。我们的方法为下游任务（如行为预测、人机协作和多智能体协调）提供了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05852v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种结合视觉与语言的框架，利用少量样本学习与零样本学习技术，通过自然语言描述来模拟驾驶者视线的动态变化。该研究使用BDD-A数据集的高质量描述，通过人类反馈进行微调，并采用LLaVA模型进行视觉感知与注意力集中场景理解的校准。该方法结合了底层线索和顶层上下文信息，如路线语义和风险评估，以语言描述驾驶者的视线行为。在多种训练模式下进行评估，并引入特定领域的语义对齐和响应多样性指标。结果表明，经过微调后的模型在注意力转移检测和可解释性方面优于通用视觉语言模型。这是首次尝试用自然语言生成驾驶者视觉注意力分配和转移预测的研究之一，为自动驾驶中的可解释性AI提供了新的方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新的视觉与语言框架，用于模拟驾驶者视线的动态变化。</li>
<li>利用少量样本学习与零样本学习技术，通过自然语言描述来预测驾驶者的视觉注意力。</li>
<li>使用BDD-A数据集的高质量描述，并结合人类反馈进行微调。</li>
<li>采用LLaVA模型校准视觉感知与注意力集中场景理解。</li>
<li>结合底层线索和顶层上下文信息进行分析。</li>
<li>在多种训练模式下评估模型性能，并引入特定领域的语义对齐和响应多样性指标。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05852">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7f3335e7b0ada2d0e984e0daa10e4eb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c26850fef748711a180758de7b944429.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c7c5c02bb1c2527ff95768d4daf826da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3581ed0f2b19eb83645e520f02a7f03b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Deployment-of-Pretrained-MRI-Transformers-in-Brain-Imaging-Tasks"><a href="#Few-Shot-Deployment-of-Pretrained-MRI-Transformers-in-Brain-Imaging-Tasks" class="headerlink" title="Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging   Tasks"></a>Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging   Tasks</h2><p><strong>Authors:Mengyu Li, Guoyao Shen, Chad W. Farris, Xin Zhang</strong></p>
<p>Machine learning using transformers has shown great potential in medical imaging, but its real-world applicability remains limited due to the scarcity of annotated data. In this study, we propose a practical framework for the few-shot deployment of pretrained MRI transformers in diverse brain imaging tasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a large-scale, multi-cohort brain MRI dataset comprising over 31 million slices, we obtain highly transferable latent representations that generalize well across tasks and datasets. For high-level tasks such as classification, a frozen MAE encoder combined with a lightweight linear head achieves state-of-the-art accuracy in MRI sequence identification with minimal supervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a hybrid architecture that fuses multiscale CNN features with pretrained MAE embeddings. This model consistently outperforms other strong baselines in both skull stripping and multi-class anatomical segmentation under data-limited conditions. With extensive quantitative and qualitative evaluations, our framework demonstrates efficiency, stability, and scalability, suggesting its suitability for low-resource clinical environments and broader neuroimaging applications. </p>
<blockquote>
<p>利用转换器进行机器学习在医学成像中显示出巨大潜力，但由于缺乏标注数据，其在现实世界中的适用性仍然有限。在这项研究中，我们提出了一个实用的框架，用于在多种脑成像任务中部署少量的预训练MRI转换器。通过在一个大规模、多队列的脑MRI数据集（包含超过3100万张切片）上采用Masked Autoencoder（MAE）预训练策略，我们获得了高度可迁移的潜在表示，这些表示在任务和数据集之间具有很好的通用性。对于高级任务，如分类，使用冻结的MAE编码器结合轻量级线性头，可以在几乎无监督的情况下实现MRI序列识别的最新准确性。对于低级任务（如分割），我们提出了MAE-FUnet，这是一种混合架构，融合了多尺度CNN特征与预训练的MAE嵌入。该模型在颅骨剥离和多类解剖分割任务中均持续超越其他强大的基线模型，展现出其在数据有限条件下的优越性。通过广泛的定量和定性评估，我们的框架表现出高效性、稳定性和可扩展性，表明其适用于资源有限的临床环境和更广泛的神经影像应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05783v1">PDF</a> 30 pages, 8 figures, 7 tables</p>
<p><strong>Summary</strong><br>     本研究提出了一个实用的框架，用于在医学图像领域部署预训练的MRI转换器。通过大规模多队列MRI数据集采用Masked Autoencoder（MAE）预训练策略，获得高度可迁移的潜在表示，实现跨任务和数据集的良好泛化。在高级任务（如分类）中，冻结的MAE编码器结合轻量级线性头实现了在MRI序列识别方面的最先进的准确度，并可在数据受限的条件下取得较好的效果。在低级任务（如分割）中，本研究提出了MAE-FUnet混合架构，融合了多尺度CNN特征与预训练的MAE嵌入。该模型在颅骨剥离和多类解剖分割任务中均优于其他强大的基线模型，并通过广泛的定量和定性评估证明了其效率、稳定性和可扩展性，适用于低资源临床环境和更广泛的神经影像应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用Masked Autoencoder (MAE) 预训练策略实现了MRI转换器在医学图像领域的优异表现。</li>
<li>通过大规模MRI数据集的训练，获得了高度可迁移的潜在表示，提升了模型的泛化能力。</li>
<li>在高级任务中，结合冻结的MAE编码器和轻量级线性头实现了MRI序列识别的最先进的准确度。</li>
<li>针对低级任务如分割，提出了MAE-FUnet混合架构，融合了多尺度CNN特征与预训练的MAE嵌入，表现出色。</li>
<li>模型在数据受限的条件下仍能保持优越性能。</li>
<li>模型通过了广泛的定量和定性评估，证明了其效率、稳定性和可扩展性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05783">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-902914f04553879e8aca889e34e38ebb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de58066bf898e3df5e6f283375875c4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b270e043182bf3326e98bc3533e1cb45.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Generalized-Few-Shot-Out-of-Distribution-Detection"><a href="#Generalized-Few-Shot-Out-of-Distribution-Detection" class="headerlink" title="Generalized Few-Shot Out-of-Distribution Detection"></a>Generalized Few-Shot Out-of-Distribution Detection</h2><p><strong>Authors:Pinxuan Li, Bing Cao, Changqing Zhang, Qinghua Hu</strong></p>
<p>Few-shot Out-of-Distribution (OOD) detection has emerged as a critical research direction in machine learning for practical deployment. Most existing Few-shot OOD detection methods suffer from insufficient generalization capability for the open world. Due to the few-shot learning paradigm, the OOD detection ability is often overfit to the limited training data itself, thus degrading the performance on generalized data and performing inconsistently across different scenarios. To address this challenge, we proposed a Generalized Few-shot OOD Detection (GOOD) framework, which empowers the general knowledge of the OOD detection model with an auxiliary General Knowledge Model (GKM), instead of directly learning from few-shot data. We proceed to reveal the few-shot OOD detection from a generalization perspective and theoretically derive the Generality-Specificity balance (GS-balance) for OOD detection, which provably reduces the upper bound of generalization error with a general knowledge model. Accordingly, we propose a Knowledge Dynamic Embedding (KDE) mechanism to adaptively modulate the guidance of general knowledge. KDE dynamically aligns the output distributions of the OOD detection model to the general knowledge model based on the Generalized Belief (G-Belief) of GKM, thereby boosting the GS-balance. Experiments on real-world OOD benchmarks demonstrate our superiority. Codes will be available. </p>
<blockquote>
<p>少样本分布外（OOD）检测作为机器学习在实际部署中的关键研究方向已经出现。大多数现有的少样本OOD检测方法对于开放世界的泛化能力不足以应对挑战。由于少样本学习模式，OOD检测能力往往过度拟合有限的训练数据本身，从而在广义数据上的表现下降，并且在不同场景中的表现不一致。为了应对这一挑战，我们提出了广义少样本OOD检测（GOOD）框架，该框架通过辅助通用知识模型（GKM）赋予OOD检测模型的通用知识，而不是直接从少量数据中学习。我们从泛化角度揭示少样本OOD检测，并理论上推导出OOD检测的泛化性-特异性平衡（GS-balance），使用通用知识模型可以证明降低泛化误差的上界。相应地，我们提出了知识动态嵌入（KDE）机制，以自适应地调节通用知识的指导。KDE基于GKM的广义信念（G-Belief）动态调整OOD检测模型的输出分布，以匹配通用知识模型，从而提高GS-balance。在真实世界的OOD基准测试上的实验证明了我们方法的优越性。相关代码将公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05732v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了面向实际应用部署的机器学习领域中新兴的关键研究方向——Few-shot Out-of-Distribution（OOD）检测。现有方法普遍存在泛化能力不足的问题，导致在开放世界环境下表现不佳。针对此挑战，本文提出了Generalized Few-shot OOD Detection（GOOD）框架，借助辅助通用知识模型（GKM）为OOD检测模型赋予通用知识能力，而不是直接从少量数据中学习。本文还从泛化角度揭示了few-shot OOD检测，并理论上推导了用于减少泛化误差上界的Generality-Specificity平衡（GS-balance）。同时，提出了Knowledge Dynamic Embedding（KDE）机制，根据GKM的Generalized Belief自适应调整通用知识的指导，提升GS-balance。在真实世界的OOD基准测试上的实验证明了其优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shot Out-of-Distribution (OOD) 检测是机器学习领域的重要研究方向，特别是在实际应用部署中。</li>
<li>现有方法存在泛化能力不足的问题，导致在开放世界环境下性能下降。</li>
<li>本文提出了Generalized Few-shot OOD Detection (GOOD) 框架，借助辅助通用知识模型（GKM）增强模型的泛化能力。</li>
<li>揭示了从泛化角度看待few-shot OOD检测，并推导了Generality-Specificity balance（GS-balance）以减少泛化误差上界。</li>
<li>提出了Knowledge Dynamic Embedding (KDE) 机制，根据GKM的Generalized Belief自适应调整通用知识的指导，增强模型的性能。</li>
<li>在真实世界的OOD基准测试上进行了实验验证，证明了所提方法的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05732">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d45784f0c76d1ed0178171de8bd28015.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a745f490a8e7a3c8287beb0f9144069.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afd68642de3e6293a710630b31358f38.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-356f0196447049779deae95b8197611d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="M-2-IV-Towards-Efficient-and-Fine-grained-Multimodal-In-Context-Learning-via-Representation-Engineering"><a href="#M-2-IV-Towards-Efficient-and-Fine-grained-Multimodal-In-Context-Learning-via-Representation-Engineering" class="headerlink" title="M$^2$IV: Towards Efficient and Fine-grained Multimodal In-Context   Learning via Representation Engineering"></a>M$^2$IV: Towards Efficient and Fine-grained Multimodal In-Context   Learning via Representation Engineering</h2><p><strong>Authors:Yanshu Li, Yi Cao, Hongyang He, Qisen Cheng, Xiang Fu, Xi Xiao, Tianyang Wang, Ruixiang Tang</strong></p>
<p>Multimodal in-context learning (ICL) equips Large Vision-language Models (LVLMs) with the ability to adapt to new tasks via multiple user-provided demonstrations, without requiring any model parameter updates. However, its effectiveness is constrained by the token-intensive nature of multimodal inputs and the complexity of cross-modal few-shot reasoning, which together hinder LVLMs from extracting useful patterns from demonstrations. To address these challenges, we propose \textbf{M$^2$IV}, a novel representation engineering approach that replaces explicit token-level demonstrations with a set of learnable Multimodal In-context Vectors directly injected into the residual streams of LVLMs. By analyzing the distinct roles of multi-head attention (MHA) and multi-layer perceptrons (MLP) in the ICL process, we design a training strategy that enables M$^2$IV to perform fine-grained semantic distillation and robust cross-modal representation learning. M$^2$IV not only improves performance across diverse tasks and LVLMs but also significantly reduces token overhead, enabling graceful scaling to many-shot scenarios. To further enhance usability, we introduce \textbf{VLibrary}, a repository that stores trained M$^2$IVs for flexible retrieval and injection. With VLibrary, users can steer pre-trained LVLMs in a customized manner that meets diverse requirements. Extensive experiments demonstrate that M$^2$IV consistently outperforms vanilla ICL and prior representation engineering baselines, achieving an average accuracy gain of 3.74% with substantial improvements in overall efficiency. </p>
<blockquote>
<p>多模态上下文学习（ICL）使大型视觉语言模型（LVLMs）能够通过多个用户提供的演示来适应新任务，而无需进行任何模型参数更新。然而，其有效性受到多模态输入中令牌密集性质和跨模态少量推理的复杂性的限制，这两者共同阻碍LVLMs从演示中提取有用的模式。为了解决这些挑战，我们提出了\textbf{M$^2$IV}，这是一种新型表示工程方法，它用一组可学习的多模态上下文向量替换显式令牌级演示，并直接注入LVLMs的残差流中。通过分析多头注意力（MHA）和多层感知器（MLP）在ICL过程中的不同作用，我们设计了一种训练策略，使M$^2$IV能够执行精细的语义蒸馏和稳健的跨模态表示学习。M$^2$IV不仅提高了不同任务和LVLMs的性能，而且大大降低了令牌开销，能够灵活地扩展到许多场景。为了进一步提高可用性，我们引入了\textbf{VLibrary}，一个存储训练好的M$^2$IV的仓库，用于灵活检索和注入。借助VLibrary，用户可以以符合各种需求的方式引导预训练的LVLMs。大量实验表明，M$^2$IV始终优于普通ICL和先前的表示工程基线，在平均准确率上提高了3.74%，并且在总体效率上有显著提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04633v2">PDF</a> COLM 2025, 30 pages, 10 figures, 16 tables</p>
<p><strong>Summary</strong></p>
<p>多模态上下文学习（ICL）通过用户提供的多个示范来使大型视觉语言模型（LVLMs）适应新任务，无需更新任何模型参数。然而，其有效性受到多模态输入令牌密集和跨模态少样本推理的复杂性的限制，阻碍了LVLMs从演示中提取有用模式。为解决这些挑战，我们提出了M$^2$IV，这是一种新型表示工程方法，它用一组可学习的多模态上下文向量替换明确的令牌级演示，直接注入LVLMs的残差流中。通过分析和ICL过程中多头注意力（MHA）和多层感知器（MLP）的不同作用，我们设计了一种训练策略，使M$^2$IV能够进行精细的语义蒸馏和稳健的跨模态表示学习。M$^2$IV不仅提高了不同任务和LVLMs的性能，而且显著减少了令牌开销，实现平滑扩展至多实例场景。此外，我们引入了VLibrary存储库，用于存储训练好的M$^2$IV向量以供灵活检索和注入。用户可以利用VLibrary以符合各种需求的方式定制预训练的LVLMs。实验表明，M$^2$IV始终优于基本的ICL和先前的表示工程基线，平均准确度提高了3.74%，整体效率也大大提高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态上下文学习（ICL）允许大型视觉语言模型（LVLMs）通过用户提供的示范适应新任务，无需更新模型参数。</li>
<li>M$^2$IV是一种新型表示工程方法，旨在解决多模态输入的令牌密集性和跨模态少样本推理的复杂性挑战。</li>
<li>M$^2$IV通过引入多模态上下文向量来改进ICL，这些向量直接注入LVLMs的残差流中，提高了性能和效率。</li>
<li>M$^2$IV结合了多头注意力（MHA）和多层感知器（MLP）的分析，实现了精细的语义蒸馏和跨模态表示学习。</li>
<li>VLibrary是一个存储库，用于存储训练好的M$^2$IV向量，以便用户可以根据各种需求定制预训练的LVLMs。</li>
<li>实验表明，M$^2$IV在多个任务和LVLMs上的性能优于传统的ICL方法和表示工程基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04633">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d40c1c3d4b16eb223e26f4618502c499.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-254bbab1f840c76245a32159cdb884d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8e8178acf0e30627e015f9b56298b5e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-12/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-12/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-12/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3a52a6c3190b8d8432f33e8f8ec143a0.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-08-12  SPARSE Data, Rich Results Few-Shot Semi-Supervised Learning via   Class-Conditioned Image Translation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-12/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1d484d4af7b1b5b3a516d355b31b255a.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-08-12  GLM-4.5 Agentic, Reasoning, and Coding (ARC) Foundation Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29885.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
