<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-12  SPARSE Data, Rich Results Few-Shot Semi-Supervised Learning via   Class-Conditioned Image Translation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-9d643a662c122ed8f40465c8e194d007.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    41 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-12-æ›´æ–°"><a href="#2025-08-12-æ›´æ–°" class="headerlink" title="2025-08-12 æ›´æ–°"></a>2025-08-12 æ›´æ–°</h1><h2 id="SPARSE-Data-Rich-Results-Few-Shot-Semi-Supervised-Learning-via-Class-Conditioned-Image-Translation"><a href="#SPARSE-Data-Rich-Results-Few-Shot-Semi-Supervised-Learning-via-Class-Conditioned-Image-Translation" class="headerlink" title="SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via   Class-Conditioned Image Translation"></a>SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via   Class-Conditioned Image Translation</h2><p><strong>Authors:Guido Manni, Clemente Lauretti, Loredana Zollo, Paolo Soda</strong></p>
<p>Deep learning has revolutionized medical imaging, but its effectiveness is severely limited by insufficient labeled training data. This paper introduces a novel GAN-based semi-supervised learning framework specifically designed for low labeled-data regimes, evaluated across settings with 5 to 50 labeled samples per class. Our approach integrates three specialized neural networks â€“ a generator for class-conditioned image translation, a discriminator for authenticity assessment and classification, and a dedicated classifier â€“ within a three-phase training framework. The method alternates between supervised training on limited labeled data and unsupervised learning that leverages abundant unlabeled images through image-to-image translation rather than generation from noise. We employ ensemble-based pseudo-labeling that combines confidence-weighted predictions from the discriminator and classifier with temporal consistency through exponential moving averaging, enabling reliable label estimation for unlabeled data. Comprehensive evaluation across eleven MedMNIST datasets demonstrates that our approach achieves statistically significant improvements over six state-of-the-art GAN-based semi-supervised methods, with particularly strong performance in the extreme 5-shot setting where the scarcity of labeled data is most challenging. The framework maintains its superiority across all evaluated settings (5, 10, 20, and 50 shots per class). Our approach offers a practical solution for medical imaging applications where annotation costs are prohibitive, enabling robust classification performance even with minimal labeled data. Code is available at <a target="_blank" rel="noopener" href="https://github.com/GuidoManni/SPARSE">https://github.com/GuidoManni/SPARSE</a>. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ å·²ç»å½»åº•æ”¹å˜äº†åŒ»å­¦å½±åƒé¢†åŸŸï¼Œä½†å…¶æœ‰æ•ˆæ€§å› ç¼ºä¹è¶³å¤Ÿçš„æ ‡æ³¨è®­ç»ƒæ•°æ®è€Œå—åˆ°ä¸¥é‡é™åˆ¶ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºGANçš„åŠç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸“ä¸ºä½æ ‡æ³¨æ•°æ®ç¯å¢ƒè®¾è®¡ï¼Œå¹¶åœ¨æ¯ç±»æœ‰5åˆ°50ä¸ªæ ‡æ³¨æ ·æœ¬çš„å„ç§ç¯å¢ƒä¸­è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ•´åˆäº†ä¸‰ä¸ªä¸“ç”¨ç¥ç»ç½‘ç»œâ€”â€”ç”¨äºç±»æ¡ä»¶å›¾åƒç¿»è¯‘çš„ç”Ÿæˆå™¨ã€ç”¨äºçœŸå®æ€§å’Œåˆ†ç±»è¯„ä¼°çš„é‰´åˆ«å™¨ä»¥åŠä¸“ç”¨åˆ†ç±»å™¨â€”â€”åœ¨ä¸€ä¸ªä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶ä¸­ã€‚è¯¥æ–¹æ³•åœ¨æœ‰é™çš„æ ‡æ³¨æ•°æ®ä¸Šè¿›è¡Œç›‘ç£è®­ç»ƒï¼Œå¹¶åœ¨æ— ç›‘ç£å­¦ä¹ ä¸­åˆ©ç”¨å¤§é‡æœªæ ‡æ³¨å›¾åƒè¿›è¡Œå›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ï¼Œè€Œä¸æ˜¯é€šè¿‡å™ªå£°ç”Ÿæˆã€‚æˆ‘ä»¬é‡‡ç”¨åŸºäºé›†åˆçš„ä¼ªæ ‡ç­¾æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†é‰´åˆ«å™¨å’Œåˆ†ç±»å™¨çš„åŠ æƒé¢„æµ‹ï¼Œå¹¶é€šè¿‡æŒ‡æ•°ç§»åŠ¨å¹³å‡å®ç°æ—¶é—´ä¸€è‡´æ€§ï¼Œä»è€Œä¸ºæœªæ ‡æ³¨æ•°æ®æä¾›å¯é çš„æ ‡ç­¾ä¼°è®¡ã€‚åœ¨11ä¸ªMedMNISTæ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…­ä¸ªæœ€å…ˆè¿›çš„åŸºäºGANçš„åŠç›‘ç£æ–¹æ³•ä¸Šå®ç°äº†ç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—æ”¹å–„ï¼Œç‰¹åˆ«æ˜¯åœ¨æç«¯çš„5ä¸ªæ ·æœ¬è®¾ç½®ä¸­ï¼Œæ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºæ€§æœ€å…·æŒ‘æˆ˜æ€§ã€‚è¯¥æ¡†æ¶åœ¨æ‰€æœ‰è¯„ä¼°ç¯å¢ƒï¼ˆæ¯ç±»5ã€10ã€20å’Œ50ä¸ªæ ·æœ¬ï¼‰ä¸­å‡ä¿æŒå…¶ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºåŒ»å­¦å½±åƒåº”ç”¨æä¾›äº†å®é™…è§£å†³æ–¹æ¡ˆï¼Œå…¶ä¸­æ ‡æ³¨æˆæœ¬é«˜æ˜‚ï¼Œå³ä½¿åœ¨å°‘é‡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°ç¨³å¥çš„åˆ†ç±»æ€§èƒ½ã€‚ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/GuidoManni/SPARSE%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/GuidoManni/SPARSEè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06429v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦æˆåƒé¢†åŸŸæœ‰é©å‘½æ€§è¿›å±•ï¼Œä½†å—é™äºç¼ºä¹æ ‡æ³¨çš„è®­ç»ƒæ•°æ®ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºGANçš„åŠç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œé€‚ç”¨äºå°‘é‡æ ‡æ³¨æ•°æ®çš„åœºæ™¯ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªç¥ç»ç½‘ç»œï¼Œé€šè¿‡ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ ç›¸ç»“åˆçš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨MedMNISTæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœã€‚è¯¥æ¡†æ¶åœ¨æ ‡æ³¨æ•°æ®ç¨€ç¼ºæ—¶å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸”é€‚ç”¨äºåŒ»å­¦æˆåƒåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºGANçš„åŠç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œé€‚ç”¨äºåŒ»å­¦æˆåƒä¸­æ ‡æ³¨æ•°æ®ä¸è¶³çš„æƒ…å†µã€‚</li>
<li>è¯¥æ¡†æ¶é›†æˆäº†ä¸‰ä¸ªç¥ç»ç½‘ç»œï¼šç”¨äºç±»æ¡ä»¶å›¾åƒç¿»è¯‘çš„å‘ç”µæœºã€ç”¨äºçœŸå®æ€§å’Œåˆ†ç±»è¯„ä¼°çš„é‰´åˆ«å™¨ä»¥åŠä¸“ç”¨åˆ†ç±»å™¨ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ ç›¸ç»“åˆçš„æ–¹å¼è®­ç»ƒæ¨¡å‹ï¼Œåˆ©ç”¨æœ‰é™æ ‡æ³¨æ•°æ®å’Œå¤§é‡æœªæ ‡æ³¨å›¾åƒã€‚</li>
<li>é‡‡ç”¨åŸºäºé›†åˆçš„ä¼ªæ ‡ç­¾æ–¹æ³•ï¼Œç»“åˆé‰´åˆ«å™¨å’Œåˆ†ç±»å™¨çš„é¢„æµ‹ç»“æœï¼Œé€šè¿‡æŒ‡æ•°ç§»åŠ¨å¹³å‡å®ç°æ—¶é—´ä¸€è‡´æ€§ï¼Œä¸ºæœªæ ‡æ³¨æ•°æ®æä¾›å¯é çš„æ ‡ç­¾ä¼°è®¡ã€‚</li>
<li>åœ¨å¤šä¸ªMedMNISTæ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å°‘é‡æ ‡æ³¨æ•°æ®çš„è®¾ç½®ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œç›¸å¯¹äºå…­ç§æœ€å…ˆè¿›çš„GANåŠç›‘ç£æ–¹æ³•å…·æœ‰ç»Ÿè®¡ä¸Šçš„æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨æ ‡æ³¨æ•°æ®æåº¦ç¨€ç¼ºçš„åœºæ™¯ä¸‹ï¼ˆå¦‚æ¯ç±»ä»…æœ‰5ä¸ªæ ·æœ¬ï¼‰è¡¨ç°å‡ºç‰¹åˆ«å¼ºçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06429">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7511d74c69ca1d831ead8fbab9ceae67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dae2068346be72f0e9500126a0e81124.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Cyberbullying-Detection-via-Aggression-Enhanced-Prompting"><a href="#Cyberbullying-Detection-via-Aggression-Enhanced-Prompting" class="headerlink" title="Cyberbullying Detection via Aggression-Enhanced Prompting"></a>Cyberbullying Detection via Aggression-Enhanced Prompting</h2><p><strong>Authors:Aisha Saeid, Anu Sabu, Girish A. Koushik, Ferrante Neri, Diptesh Kanojia</strong></p>
<p>Detecting cyberbullying on social media remains a critical challenge due to its subtle and varied expressions. This study investigates whether integrating aggression detection as an auxiliary task within a unified training framework can enhance the generalisation and performance of large language models (LLMs) in cyberbullying detection. Experiments are conducted on five aggression datasets and one cyberbullying dataset using instruction-tuned LLMs. We evaluated multiple strategies: zero-shot, few-shot, independent LoRA fine-tuning, and multi-task learning (MTL). Given the inconsistent results of MTL, we propose an enriched prompt pipeline approach in which aggression predictions are embedded into cyberbullying detection prompts to provide contextual augmentation. Preliminary results show that the enriched prompt pipeline consistently outperforms standard LoRA fine-tuning, indicating that aggression-informed context significantly boosts cyberbullying detection. This study highlights the potential of auxiliary tasks, such as aggression detection, to improve the generalisation of LLMs for safety-critical applications on social networks. </p>
<blockquote>
<p>æ£€æµ‹ç¤¾äº¤åª’ä½“ä¸Šçš„ç½‘ç»œæ¬ºå‡Œä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå…¶è¡¨è¾¾å…·æœ‰ç»†å¾®æ€§å’Œå¤šæ ·æ€§ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç©¶åœ¨ç»Ÿä¸€è®­ç»ƒæ¡†æ¶å†…æ•´åˆæ”»å‡»æ£€æµ‹ä½œä¸ºè¾…åŠ©ä»»åŠ¡ï¼Œèƒ½å¦å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç½‘ç»œæ¬ºå‡Œæ£€æµ‹ä¸­çš„é€šç”¨æ€§å’Œæ€§èƒ½ã€‚æœ¬ç ”ç©¶ä½¿ç”¨æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåœ¨äº”ä¸ªæ”»å‡»æ•°æ®é›†å’Œä¸€ä¸ªç½‘ç»œæ¬ºå‡Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šç§ç­–ç•¥ï¼šé›¶æ ·æœ¬ã€å°æ ·æœ¬ã€ç‹¬ç«‹LoRAå¾®è°ƒå’Œå¤šä»»åŠ¡å­¦ä¹ ï¼ˆMTLï¼‰ã€‚é‰´äºå¤šä»»åŠ¡å­¦ä¹ çš„ä¸ä¸€è‡´ç»“æœï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸°å¯Œçš„æç¤ºç®¡é“æ–¹æ³•ï¼Œå…¶ä¸­æ”»å‡»é¢„æµ‹è¢«åµŒå…¥åˆ°ç½‘ç»œæ¬ºå‡Œæ£€æµ‹æç¤ºä¸­ä»¥æä¾›ä¸Šä¸‹æ–‡å¢å¼ºã€‚åˆæ­¥ç»“æœè¡¨æ˜ï¼Œä¸°å¯Œçš„æç¤ºç®¡é“å§‹ç»ˆä¼˜äºæ ‡å‡†çš„LoRAå¾®è°ƒï¼Œè¿™è¡¨æ˜æ”»å‡»ä¿¡æ¯ä¸Šä¸‹æ–‡æ˜¾è‘—æé«˜äº†ç½‘ç»œæ¬ºå‡Œæ£€æµ‹æ•ˆæœã€‚æœ¬ç ”ç©¶çªå‡ºäº†è¾…åŠ©ä»»åŠ¡ï¼ˆå¦‚æ”»å‡»æ£€æµ‹ï¼‰çš„æ½œåŠ›ï¼Œå¯ä»¥æ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¤¾äº¤åª’ä½“å®‰å…¨å…³é”®åº”ç”¨çš„é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06360v1">PDF</a> Accepted to RANLP 2025</p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶æ¢è®¨äº†å°†æ”»å‡»æ£€æµ‹ä½œä¸ºè¾…åŠ©ä»»åŠ¡èå…¥ç»Ÿä¸€è®­ç»ƒæ¡†æ¶ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç½‘ç»œæ¬ºå‡Œæ£€æµ‹ä¸­çš„é€šç”¨æ€§å’Œæ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œå°†æ”»å‡»æ£€æµ‹åµŒå…¥åˆ°ç½‘ç»œæ¬ºå‡Œæ£€æµ‹æç¤ºä¸­çš„ä¸°å¯Œæç¤ºç®¡é“æ–¹æ³•èƒ½å¤Ÿä¸€è‡´åœ°ä¼˜äºæ ‡å‡†LoRAå¾®è°ƒï¼Œæ˜¾ç¤ºå‡ºæ”»å‡»æ£€æµ‹æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å¯¹ç½‘ç»œæ¬ºå‡Œæ£€æµ‹çš„å¢å¼ºä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç½‘ç»œæ¬ºå‡Œçš„å¾®å¦™å’Œå¤šå˜è¡¨è¾¾ä½¿å…¶æ£€æµ‹æˆä¸ºä¸€é¡¹å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>é›†æˆæ”»å‡»æ£€æµ‹ä½œä¸ºè¾…åŠ©ä»»åŠ¡å¯ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç½‘ç»œæ¬ºå‡Œæ£€æµ‹ä¸­çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒæ¶‰åŠå¤šç§ç­–ç•¥ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å°‘æ ·æœ¬ã€ç‹¬ç«‹LoRAå¾®è°ƒå’Œå¤šä»»åŠ¡å­¦ä¹ ã€‚</li>
<li>å¤šä»»åŠ¡å­¦ä¹ çš„ç»“æœä¸ä¸€è‡´ï¼Œæå‡ºå°†æ”»å‡»æ£€æµ‹åµŒå…¥ç½‘ç»œæ¬ºå‡Œæ£€æµ‹æç¤ºçš„ä¸°å¯Œæç¤ºç®¡é“æ–¹æ³•ã€‚</li>
<li>åˆæ­¥ç»“æœè¡¨æ˜ï¼Œä¸°å¯Œæç¤ºç®¡é“æ–¹æ³•ä¼˜äºæ ‡å‡†LoRAå¾®è°ƒã€‚</li>
<li>æ”»å‡»æ£€æµ‹æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å¯¹ç½‘ç»œæ¬ºå‡Œæ£€æµ‹æœ‰æ˜¾è‘—æå‡ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06360">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-72bfe1cacb6dcea9dee99550405c239a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28d984c26a74d681aa4b4117dfa35a50.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4d65f73996b250f0e66b84d6ac7b3bf1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="FedMeNF-Privacy-Preserving-Federated-Meta-Learning-for-Neural-Fields"><a href="#FedMeNF-Privacy-Preserving-Federated-Meta-Learning-for-Neural-Fields" class="headerlink" title="FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields"></a>FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields</h2><p><strong>Authors:Junhyeog Yun, Minui Hong, Gunhee Kim</strong></p>
<p>Neural fields provide a memory-efficient representation of data, which can effectively handle diverse modalities and large-scale data. However, learning to map neural fields often requires large amounts of training data and computations, which can be limited to resource-constrained edge devices. One approach to tackle this limitation is to leverage Federated Meta-Learning (FML), but traditional FML approaches suffer from privacy leakage. To address these issues, we introduce a novel FML approach called FedMeNF. FedMeNF utilizes a new privacy-preserving loss function that regulates privacy leakage in the local meta-optimization. This enables the local meta-learner to optimize quickly and efficiently without retaining the clientâ€™s private data. Our experiments demonstrate that FedMeNF achieves fast optimization speed and robust reconstruction performance, even with few-shot or non-IID data across diverse data modalities, while preserving client data privacy. </p>
<blockquote>
<p>ç¥ç»ç½‘ç»œåœºæä¾›äº†ä¸€ç§æ•°æ®è¡¨ç¤ºæ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•å…·æœ‰é«˜æ•ˆçš„å†…å­˜å ç”¨ï¼Œå¹¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å¤šç§æ¨¡æ€å’Œå¤§è§„æ¨¡æ•°æ®ã€‚ç„¶è€Œï¼Œå­¦ä¹ æ˜ å°„ç¥ç»ç½‘ç»œé€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºï¼Œè¿™åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šå¯èƒ½ä¼šå—åˆ°é™åˆ¶ã€‚ä¸€ç§è§£å†³æ­¤é™åˆ¶çš„æ–¹æ³•æ˜¯é‡‡ç”¨è”é‚¦å…ƒå­¦ä¹ ï¼ˆFMLï¼‰ï¼Œä½†ä¼ ç»Ÿçš„FMLæ–¹æ³•å­˜åœ¨éšç§æ³„éœ²é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„FMLæ–¹æ³•ï¼Œç§°ä¸ºFedMeNFã€‚FedMeNFåˆ©ç”¨äº†ä¸€ç§æ–°çš„éšç§ä¿æŠ¤æŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°å¯ä»¥åœ¨æœ¬åœ°å…ƒä¼˜åŒ–è¿‡ç¨‹ä¸­æ§åˆ¶éšç§æ³„éœ²ã€‚è¿™ä½¿å¾—æœ¬åœ°å…ƒå­¦ä¹ è€…èƒ½å¤Ÿåœ¨ä¸ä¿ç•™å®¢æˆ·ç«¯ç§æœ‰æ•°æ®çš„æƒ…å†µä¸‹å¿«é€Ÿæœ‰æ•ˆåœ°è¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨å¤šç§æ¨¡æ€æ•°æ®çš„å°‘é‡æ ·æœ¬æˆ–éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰æ•°æ®æƒ…å†µä¸‹ï¼ŒFedMeNFä¹Ÿèƒ½å®ç°å¿«é€Ÿä¼˜åŒ–å’Œç¨³å¥çš„é‡å»ºæ€§èƒ½ï¼ŒåŒæ—¶ä¿æŠ¤å®¢æˆ·ç«¯æ•°æ®éšç§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06301v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œåœºä¸ºæ•°æ®æä¾›äº†å†…å­˜é«˜æ•ˆçš„è¡¨ç¤ºï¼Œå¯å¤„ç†å¤šæ ·æ¨¡æ€å’Œå¤§è§„æ¨¡æ•°æ®ã€‚ç„¶è€Œï¼Œå­¦ä¹ æ˜ å°„ç¥ç»ç½‘ç»œåœºé€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºï¼Œè¿™åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šå¯èƒ½å—é™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åä¸ºFedMeNFçš„æ–°å‹è”é‚¦å…ƒå­¦ä¹ æ–¹æ³•ã€‚FedMeNFåˆ©ç”¨æ–°çš„éšç§ä¿æŠ¤æŸå¤±å‡½æ•°ï¼Œå¯¹æœ¬åœ°å…ƒä¼˜åŒ–ä¸­çš„éšç§æ³„éœ²è¿›è¡Œè§„èŒƒã€‚è¿™å…è®¸æœ¬åœ°å…ƒå­¦ä¹ è€…å¿«é€Ÿé«˜æ•ˆåœ°è¿›è¡Œä¼˜åŒ–ï¼ŒåŒæ—¶æ— éœ€ä¿ç•™å®¢æˆ·ç«¯çš„ç§æœ‰æ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨å„ç§æ•°æ®æ¨¡æ€çš„å°‘é‡æ ·æœ¬æˆ–éç‹¬ç«‹åŒåˆ†å¸ƒæ•°æ®ä¸‹ï¼ŒFedMeNFä¹Ÿèƒ½å®ç°å¿«é€Ÿä¼˜åŒ–å’Œç¨³å¥çš„é‡å»ºæ€§èƒ½ï¼ŒåŒæ—¶ä¿æŠ¤å®¢æˆ·ç«¯æ•°æ®éšç§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œåœºæ˜¯å†…å­˜é«˜æ•ˆçš„æ•°æ®è¡¨ç¤ºæ–¹æ³•ï¼Œå¯å¤„ç†å¤šæ ·æ¨¡æ€å’Œå¤§è§„æ¨¡æ•°æ®ã€‚</li>
<li>å­¦ä¹ æ˜ å°„ç¥ç»ç½‘ç»œåœºé€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºã€‚</li>
<li>è”é‚¦å…ƒå­¦ä¹ ï¼ˆFMLï¼‰æ˜¯è§£å†³èµ„æºå—é™è®¾å¤‡ä¸Šçš„è®­ç»ƒæ•°æ®é™åˆ¶çš„ä¸€ç§æ–¹æ³•ã€‚</li>
<li>ä¼ ç»Ÿçš„FMLæ–¹æ³•å­˜åœ¨éšç§æ³„éœ²é—®é¢˜ã€‚</li>
<li>FedMeNFæ˜¯ä¸€ç§æ–°å‹çš„è”é‚¦å…ƒå­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨éšç§ä¿æŠ¤æŸå¤±å‡½æ•°è§„èŒƒæœ¬åœ°å…ƒä¼˜åŒ–ä¸­çš„éšç§æ³„éœ²ã€‚</li>
<li>FedMeNFå…è®¸å¿«é€Ÿé«˜æ•ˆçš„æœ¬åœ°å…ƒä¼˜åŒ–ï¼ŒåŒæ—¶ä¿æŠ¤å®¢æˆ·ç«¯çš„ç§æœ‰æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06301">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c866c16930bd278006e8ff2cbe3d520d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1de5f6798be3bacc4eaf018d10924d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb688b443ac454b7dd4f03b283525e6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1c2484cd46b5858cb058a793654922d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="IOCC-Aligning-Semantic-and-Cluster-Centers-for-Few-shot-Short-Text-Clustering"><a href="#IOCC-Aligning-Semantic-and-Cluster-Centers-for-Few-shot-Short-Text-Clustering" class="headerlink" title="IOCC: Aligning Semantic and Cluster Centers for Few-shot Short Text   Clustering"></a>IOCC: Aligning Semantic and Cluster Centers for Few-shot Short Text   Clustering</h2><p><strong>Authors:Jixuan Yin, Zhihao Yao, Wenshuai Huo, Xinmiao Yu, Xiaocheng Feng, Bo Li</strong></p>
<p>In clustering tasks, it is essential to structure the feature space into clear, well-separated distributions. However, because short text representations have limited expressiveness, conventional methods struggle to identify cluster centers that truly capture each categoryâ€™s underlying semantics, causing the representations to be optimized in suboptimal directions. To address this issue, we propose IOCC, a novel few-shot contrastive learning method that achieves alignment between the cluster centers and the semantic centers. IOCC consists of two key modules: Interaction-enhanced Optimal Transport (IEOT) and Center-aware Contrastive Learning (CACL). Specifically, IEOT incorporates semantic interactions between individual samples into the conventional optimal transport problem, and generate pseudo-labels. Based on these pseudo-labels, we aggregate high-confidence samples to construct pseudo-centers that approximate the semantic centers. Next, CACL optimizes text representations toward their corresponding pseudo-centers. As training progresses, the collaboration between the two modules gradually reduces the gap between cluster centers and semantic centers. Therefore, the model will learn a high-quality distribution, improving clustering performance. Extensive experiments on eight benchmark datasets show that IOCC outperforms previous methods, achieving up to 7.34% improvement on challenging Biomedical dataset and also excelling in clustering stability and efficiency. The code is available at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/IOCC-C438">https://anonymous.4open.science/r/IOCC-C438</a>. </p>
<blockquote>
<p>åœ¨èšç±»ä»»åŠ¡ä¸­ï¼Œå°†ç‰¹å¾ç©ºé—´ç»“æ„åŒ–ä¸ºæ¸…æ™°ã€åˆ†ç¦»è‰¯å¥½çš„åˆ†å¸ƒè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºçŸ­æ–‡æœ¬è¡¨ç¤ºçš„è¡¨è¾¾èƒ½åŠ›æœ‰é™ï¼Œä¼ ç»Ÿæ–¹æ³•éš¾ä»¥è¯†åˆ«çœŸæ­£æ•æ‰æ¯ä¸ªç±»åˆ«åº•å±‚è¯­ä¹‰çš„èšç±»ä¸­å¿ƒï¼Œå¯¼è‡´è¡¨ç¤ºçš„ä¼˜åŒ–æ–¹å‘ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†IOCCï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å°‘æ ·æœ¬å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œå®ç°äº†èšç±»ä¸­å¿ƒä¸è¯­ä¹‰ä¸­å¿ƒä¹‹é—´çš„å¯¹é½ã€‚IOCCç”±ä¸¤ä¸ªå…³é”®æ¨¡å—ç»„æˆï¼šäº¤äº’å¢å¼ºæœ€ä¼˜ä¼ è¾“ï¼ˆIEOTï¼‰å’Œä¸­å¿ƒæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ ï¼ˆCACLï¼‰ã€‚å…·ä½“æ¥è¯´ï¼ŒIEOTå°†ä¸ªä½“æ ·æœ¬ä¹‹é—´çš„è¯­ä¹‰äº¤äº’çº³å…¥ä¼ ç»Ÿæœ€ä¼˜ä¼ è¾“é—®é¢˜ä¸­ï¼Œå¹¶ç”Ÿæˆä¼ªæ ‡ç­¾ã€‚åŸºäºè¿™äº›ä¼ªæ ‡ç­¾ï¼Œæˆ‘ä»¬èšåˆé«˜ç½®ä¿¡åº¦æ ·æœ¬æ¥æ„å»ºè¿‘ä¼¼è¯­ä¹‰ä¸­å¿ƒçš„ä¼ªä¸­å¿ƒã€‚æ¥ä¸‹æ¥ï¼ŒCACLå°†æ–‡æœ¬è¡¨ç¤ºä¼˜åŒ–æœå‘å…¶å¯¹åº”çš„ä¼ªä¸­å¿ƒã€‚éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œä¸¤ä¸ªæ¨¡å—ä¹‹é—´çš„åä½œé€æ¸ç¼©å°äº†èšç±»ä¸­å¿ƒå’Œè¯­ä¹‰ä¸­å¿ƒä¹‹é—´çš„å·®è·ã€‚å› æ­¤ï¼Œæ¨¡å‹å°†å­¦ä¹ é«˜è´¨é‡åˆ†å¸ƒï¼Œæé«˜èšç±»æ€§èƒ½ã€‚åœ¨å…«ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒIOCCä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šå®ç°äº†é«˜è¾¾7.34%çš„æ”¹è¿›ï¼ŒåŒæ—¶åœ¨èšç±»ç¨³å®šæ€§å’Œæ•ˆç‡æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚ä»£ç å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/IOCC-C438%E3%80%82">https://anonymous.4open.science/r/IOCC-C438ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06126v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§åŸºäºå°‘æ•°æ ·æœ¬å¯¹æ¯”å­¦ä¹ çš„èšç±»æ–¹æ³•IOCCï¼Œæ—¨åœ¨è§£å†³çŸ­æ–‡æœ¬è¡¨ç¤ºçš„é—®é¢˜ã€‚é€šè¿‡æ„å»ºäº¤äº’å¢å¼ºçš„æœ€ä¼˜ä¼ è¾“æ¨¡å—ï¼ˆIEOTï¼‰å’Œä¸­å¿ƒæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ æ¨¡å—ï¼ˆCACLï¼‰ï¼ŒIOCCå®ç°äº†å¯¹èšç±»ä¸­å¿ƒå’Œè¯­ä¹‰ä¸­å¿ƒçš„åŒ¹é…ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒIOCCåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šï¼Œå®ç°äº†é«˜è¾¾7.34%çš„æ€§èƒ½æå‡ï¼Œä¸”åœ¨èšç±»ç¨³å®šæ€§å’Œæ•ˆç‡æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IOCCæ˜¯ä¸€ç§ç”¨äºèšç±»ä»»åŠ¡çš„å°‘æ•°æ ·æœ¬å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³çŸ­æ–‡æœ¬è¡¨ç¤ºçš„é—®é¢˜ã€‚</li>
<li>IOCCé€šè¿‡æ„å»ºIEOTæ¨¡å—ï¼Œå°†æ ·æœ¬é—´çš„è¯­ä¹‰äº¤äº’çº³å…¥ä¼ ç»Ÿæœ€ä¼˜ä¼ è¾“é—®é¢˜ä¸­ï¼Œç”Ÿæˆä¼ªæ ‡ç­¾ã€‚</li>
<li>åŸºäºä¼ªæ ‡ç­¾ï¼ŒIOCCèšåˆé«˜ç½®ä¿¡åº¦æ ·æœ¬æ„å»ºä¼ªä¸­å¿ƒï¼Œè¿‘ä¼¼è¯­ä¹‰ä¸­å¿ƒã€‚</li>
<li>CACLæ¨¡å—åˆ™è‡´åŠ›äºä¼˜åŒ–æ–‡æœ¬è¡¨ç¤ºï¼Œä½¿å…¶æœå‘å¯¹åº”çš„ä¼ªä¸­å¿ƒã€‚</li>
<li>éšç€è®­ç»ƒçš„è¿›è¡Œï¼ŒIEOTå’ŒCACLä¸¤ä¸ªæ¨¡å—çš„åä½œé€æ¸ç¼©å°äº†èšç±»ä¸­å¿ƒå’Œè¯­ä¹‰ä¸­å¿ƒçš„å·®è·ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒIOCCåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06126">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-88bbbed53dfce0b2040c7c1fd345d5a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d769d5af080da0ad6e6ea441610e2e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a7d9b4f0ade5f3a3ef00d30466fc247.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec37e894812c8598909a2946ac8c7899.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Prompting-for-Extractive-Quranic-QA-with-Instruction-Tuned-LLMs"><a href="#Few-Shot-Prompting-for-Extractive-Quranic-QA-with-Instruction-Tuned-LLMs" class="headerlink" title="Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs"></a>Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs</h2><p><strong>Authors:Mohamed Basem, Islam Oshallah, Ali Hamdi, Ammar Mohammed</strong></p>
<p>This paper presents two effective approaches for Extractive Question Answering (QA) on the Quran. It addresses challenges related to complex language, unique terminology, and deep meaning in the text. The second uses few-shot prompting with instruction-tuned large language models such as Gemini and DeepSeek. A specialized Arabic prompt framework is developed for span extraction. A strong post-processing system integrates subword alignment, overlap suppression, and semantic filtering. This improves precision and reduces hallucinations. Evaluations show that large language models with Arabic instructions outperform traditional fine-tuned models. The best configuration achieves a pAP10 score of 0.637. The results confirm that prompt-based instruction tuning is effective for low-resource, semantically rich QA tasks. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸¤ç§é’ˆå¯¹ã€Šå¤å…°ç»ã€‹çš„æå–å¼é—®ç­”ï¼ˆQAï¼‰çš„æœ‰æ•ˆæ–¹æ³•ã€‚å®ƒè§£å†³äº†ä¸å¤æ‚è¯­è¨€ã€ç‹¬ç‰¹æœ¯è¯­å’Œæ–‡æœ¬æ·±å±‚æ„ä¹‰ç›¸å…³çš„æŒ‘æˆ˜ã€‚ç¬¬äºŒç§æ–¹æ³•ä½¿ç”¨å°‘é‡æç¤ºä¸ç»è¿‡æŒ‡ä»¤è°ƒæ•´çš„ å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚Geminiå’ŒDeepSeekã€‚å¼€å‘äº†ä¸€ä¸ªä¸“é—¨çš„é˜¿æ‹‰ä¼¯è¯­æç¤ºæ¡†æ¶ï¼Œç”¨äºè·¨åº¦æå–ã€‚å¼ºå¤§çš„åå¤„ç†ç³»ç»Ÿé›†æˆäº†å­è¯å¯¹é½ã€é‡å æŠ‘åˆ¶å’Œè¯­ä¹‰è¿‡æ»¤ã€‚è¿™æé«˜äº†ç²¾åº¦å¹¶å‡å°‘äº†å¹»è§‰ã€‚è¯„ä¼°è¡¨æ˜ï¼Œå¸¦æœ‰é˜¿æ‹‰ä¼¯è¯­æŒ‡ä»¤çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¼˜äºä¼ ç»Ÿå¾®è°ƒæ¨¡å‹ã€‚æœ€ä½³é…ç½®è¾¾åˆ°pAP10å¾—åˆ†ä¸º0.637ã€‚ç»“æœè¯å®ï¼ŒåŸºäºæç¤ºçš„æŒ‡ä»¤è°ƒæ•´å¯¹äºä½èµ„æºã€è¯­ä¹‰ä¸°å¯Œçš„é—®ç­”ä»»åŠ¡æ˜¯æœ‰æ•ˆçš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06103v1">PDF</a> 6 pages , 2 figures , Accepted in IMSA 2025,Egypt ,   <a target="_blank" rel="noopener" href="https://imsa.msa.edu.eg/">https://imsa.msa.edu.eg/</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸¤ç§é’ˆå¯¹ã€Šå¤å…°ç»ã€‹çš„æœ‰æ•ˆæå–å¼é—®ç­”æ–¹æ³•ã€‚ç¬¬äºŒç§æ–¹æ³•é‡‡ç”¨åŸºäºæŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Geminiå’ŒDeepSeekï¼‰è¿›è¡Œå°‘é‡æç¤ºï¼Œå¼€å‘äº†ä¸€ä¸ªä¸“ç”¨çš„é˜¿æ‹‰ä¼¯æ–‡æç¤ºæ¡†æ¶ç”¨äºè·¨æå–ã€‚é€šè¿‡æ•´åˆå­è¯å¯¹é½ã€æŠ‘åˆ¶é‡å å’Œè¯­ä¹‰è¿‡æ»¤çš„å¼ºå¤§çš„åå¤„ç†ç³»ç»Ÿæé«˜äº†ç²¾åº¦å¹¶å‡å°‘äº†è¯¯ç”Ÿæˆã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œå¸¦æœ‰é˜¿æ‹‰ä¼¯æŒ‡ä»¤çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¼˜äºä¼ ç»Ÿå¾®è°ƒæ¨¡å‹ï¼Œæœ€ä½³é…ç½®è¾¾åˆ°pAP10åˆ†æ•°ä¸º0.637ã€‚è¯æ˜åŸºäºæŒ‡ä»¤è°ƒæ•´çš„æç¤ºå¯¹äºèµ„æºåŒ®ä¹ä½†è¯­ä¹‰ä¸°å¯Œçš„é—®ç­”ä»»åŠ¡æ˜¯æœ‰æ•ˆçš„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸¤ç§é’ˆå¯¹ã€Šå¤å…°ç»ã€‹çš„æå–å¼é—®ç­”çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>ç¬¬äºŒç§æ–¹æ³•ä½¿ç”¨åŸºäºæŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå°‘é‡æç¤ºï¼Œå¼€å‘äº†é˜¿æ‹‰ä¼¯æ–‡æç¤ºæ¡†æ¶ç”¨äºè·¨æå–ã€‚</li>
<li>è®ºæ–‡å¼ºè°ƒäº†æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¤æ‚è¯­è¨€ã€ç‹¬ç‰¹æœ¯è¯­å’Œæ–‡æœ¬ä¸­çš„æ·±å±‚å«ä¹‰ã€‚</li>
<li>é€šè¿‡å¼ºå¤§çš„åå¤„ç†ç³»ç»Ÿæé«˜äº†ç²¾åº¦å¹¶å‡å°‘äº†è¯¯ç”Ÿæˆã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼Œå¸¦æœ‰é˜¿æ‹‰ä¼¯æŒ‡ä»¤çš„å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½ä¼˜äºä¼ ç»Ÿå¾®è°ƒæ¨¡å‹ã€‚</li>
<li>æœ€ä½³é…ç½®çš„pAP10åˆ†æ•°ä¸º0.637ï¼Œè¡¨æ˜å…¶æ€§èƒ½è¾ƒé«˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06103">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2c94999826fc14ecfc31b63e060ccaf9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62b07b47f75c32fd720fe3f5de63ad39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d643a662c122ed8f40465c8e194d007.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c435702f1ea57c62176f82b140290901.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-63b7e700cfd18f3a9d68fc5c8a2bd642.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17501311f9e37f3128bfbfd7acf183b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f408f58a717874407744f415a2f9bbe.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Dean-of-LLM-Tutors-Exploring-Comprehensive-and-Automated-Evaluation-of-LLM-generated-Educational-Feedback-via-LLM-Feedback-Evaluators"><a href="#Dean-of-LLM-Tutors-Exploring-Comprehensive-and-Automated-Evaluation-of-LLM-generated-Educational-Feedback-via-LLM-Feedback-Evaluators" class="headerlink" title="Dean of LLM Tutors: Exploring Comprehensive and Automated Evaluation of   LLM-generated Educational Feedback via LLM Feedback Evaluators"></a>Dean of LLM Tutors: Exploring Comprehensive and Automated Evaluation of   LLM-generated Educational Feedback via LLM Feedback Evaluators</h2><p><strong>Authors:Keyang Qian, Yixin Cheng, Rui Guan, Wei Dai, Flora Jin, Kaixun Yang, Sadia Nawaz, Zachari Swiecki, Guanliang Chen, Lixiang Yan, Dragan GaÅ¡eviÄ‡</strong></p>
<p>The use of LLM tutors to provide automated educational feedback to students on student assignment submissions has received much attention in the AI in Education field. However, the stochastic nature and tendency for hallucinations in LLMs can undermine both quality of learning experience and adherence to ethical standards. To address this concern, we propose a method that uses LLM feedback evaluators (DeanLLMs) to automatically and comprehensively evaluate feedback generated by LLM tutor for submissions on university assignments before it is delivered to students. This allows low-quality feedback to be rejected and enables LLM tutors to improve the feedback they generated based on the evaluation results. We first proposed a comprehensive evaluation framework for LLM-generated educational feedback, comprising six dimensions for feedback content, seven for feedback effectiveness, and three for hallucination types. Next, we generated a virtual assignment submission dataset covering 85 university assignments from 43 computer science courses using eight commonly used commercial LLMs. We labelled and open-sourced the assignment dataset to support the fine-tuning and evaluation of LLM feedback evaluators. Our findings show that o3-pro demonstrated the best performance in zero-shot labelling of feedback while o4-mini demonstrated the best performance in few-shot labelling of feedback. Moreover, GPT-4.1 achieved human expert level performance after fine-tuning (Accuracy 79.8%, F1-score 79.4%; human average Accuracy 78.3%, F1-score 82.6%). Finally, we used our best-performance model to evaluate 2,000 assignment feedback instances generated by 10 common commercial LLMs, 200 each, to compare the quality of feedback generated by different LLMs. Our LLM feedback evaluator method advances our ability to automatically provide high-quality and reliable educational feedback to students. </p>
<blockquote>
<p>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¾…å¯¼å·¥å…·ä¸ºå­¦ç”Ÿæä¾›è‡ªåŠ¨åŒ–æ•™è‚²åé¦ˆåœ¨äººå·¥æ™ºèƒ½æ•™è‚²é¢†åŸŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„éšæœºæ€§å’Œå‡ºç°å¹»è§‰çš„å€¾å‘å¯èƒ½ä¼šç ´åå­¦ä¹ ä½“éªŒçš„è´¨é‡å’Œéµå®ˆé“å¾·æ ‡å‡†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æ‹…å¿§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨LLMåé¦ˆè¯„ä¼°å™¨ï¼ˆDeanLLMsï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥è‡ªåŠ¨å’Œå…¨é¢åœ°è¯„ä¼°LLMè¾…å¯¼å·¥å…·é’ˆå¯¹å¤§å­¦ä½œä¸šæäº¤çš„åé¦ˆï¼Œç„¶åå†å°†å…¶é€’é€ç»™å­¦ç”Ÿã€‚è¿™å…è®¸æ‹’ç»ä½è´¨é‡çš„åé¦ˆï¼Œå¹¶ä½¿å¾—LLMè¾…å¯¼å·¥å…·å¯ä»¥æ ¹æ®è¯„ä¼°ç»“æœæ”¹è¿›å…¶ç”Ÿæˆçš„åé¦ˆã€‚æˆ‘ä»¬é¦–æ¬¡ä¸ºLLMç”Ÿæˆçš„æ•™è‚²åé¦ˆæå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬å…­ä¸ªå…³äºåé¦ˆå†…å®¹çš„ç»´åº¦ï¼Œä¸ƒä¸ªå…³äºåé¦ˆæ•ˆæœçš„ç»´åº¦å’Œä¸‰ä¸ªå…³äºå¹»è§‰ç±»å‹çš„ç»´åº¦ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨å…«ç§å¸¸è§å•†ä¸šLLMç”Ÿæˆäº†ä¸€ä¸ªè™šæ‹Ÿçš„ä½œä¸šæäº¤æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ¶µç›–äº†æ¥è‡ª43é—¨è®¡ç®—æœºç§‘å­¦è¯¾ç¨‹çš„85é¡¹ä½œä¸šã€‚æˆ‘ä»¬å¯¹ä½œä¸šæ•°æ®é›†è¿›è¡Œäº†æ ‡æ³¨å¹¶å¼€æºï¼Œä»¥æ”¯æŒLLMåé¦ˆè¯„ä¼°å™¨çš„å¾®è°ƒä¸è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨é›¶æ ·æœ¬æ ‡è®°åé¦ˆæ–¹é¢ï¼Œo3-proè¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œè€Œåœ¨å°‘æ ·æœ¬æ ‡è®°åé¦ˆæ–¹é¢ï¼Œo4-miniè¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒGPT-4.1åœ¨ç»è¿‡å¾®è°ƒåè¾¾åˆ°äº†äººç±»ä¸“å®¶çš„æ°´å¹³ï¼ˆå‡†ç¡®ç‡79.8%ï¼ŒF1åˆ†æ•°79.4%ï¼›äººç±»å¹³å‡å‡†ç¡®ç‡78.3%ï¼ŒF1åˆ†æ•°82.6%ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨æ€§èƒ½æœ€ä½³çš„æ¨¡å‹è¯„ä¼°äº†ç”±åç§å¸¸è§å•†ä¸šLLMç”Ÿæˆçš„2000ä¸ªä½œä¸šåé¦ˆå®ä¾‹ï¼ˆæ¯ç§LLM 200ä¸ªï¼‰ï¼Œä»¥æ¯”è¾ƒä¸åŒLLMç”Ÿæˆçš„åé¦ˆè´¨é‡ã€‚æˆ‘ä»¬çš„LLMåé¦ˆè¯„ä¼°å™¨æ–¹æ³•æé«˜äº†æˆ‘ä»¬ä¸ºå­¦ç”Ÿæä¾›é«˜è´¨é‡å’Œå¯é æ•™è‚²åé¦ˆçš„è‡ªåŠ¨åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05952v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMè¾…å¯¼å·¥å…·ä¸ºå­¦ç”Ÿä½œä¸šæä¾›è‡ªåŠ¨åŒ–åé¦ˆåœ¨æ•™è‚²ç•Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼ŒLLMçš„éšæœºæ€§å’Œå¹»è§‰å€¾å‘å¯èƒ½å½±å“å­¦ä¹ ä½“éªŒå’Œéµå®ˆé“å¾·æ ‡å‡†ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä½¿ç”¨LLMåé¦ˆè¯„ä¼°ç³»ç»Ÿï¼ˆDeanLLMsï¼‰è‡ªåŠ¨å…¨é¢è¯„ä¼°LLMè¾…å¯¼å·¥å…·æä¾›çš„åé¦ˆçš„æ–¹æ³•ã€‚æ­¤æ–¹æ³•èƒ½åœ¨å­¦ç”Ÿä½œä¸šæäº¤åå¯¹å­¦ç”Ÿçš„ä½œä¸šåé¦ˆè¿›è¡Œè‡ªåŠ¨è¯„ä»·ï¼Œå¯¹ä½è´¨é‡çš„åé¦ˆè¿›è¡Œç­›é€‰ã€‚è¯¥ç ”ç©¶å›¢é˜Ÿæå‡ºäº†å…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶ä¸ºè¯„ä¼°åé¦ˆè´¨é‡å¼€å‘äº†ä¸€ä¸ªè™šæ‹Ÿä½œä¸šæäº¤æ•°æ®é›†ã€‚æœ€ç»ˆï¼Œé€šè¿‡è¯„ä¼°æ¨¡å‹ï¼Œå›¢é˜Ÿå‘ç°GPT-4.1åœ¨ç»è¿‡å¾®è°ƒåè¾¾åˆ°äººç±»ä¸“å®¶çº§åˆ«çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜ä½¿ç”¨æœ€ä½³æ€§èƒ½æ¨¡å‹è¯„ä¼°äº†ä¸åŒLLMç”Ÿæˆçš„åé¦ˆè´¨é‡ã€‚æ­¤ç ”ç©¶æé«˜äº†ä¸ºå­¦ç”Ÿè‡ªåŠ¨æä¾›é«˜è´¨é‡å¯é æ•™è‚²åé¦ˆçš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMè¾…å¯¼å·¥å…·æä¾›è‡ªåŠ¨åŒ–æ•™è‚²åé¦ˆå¼•èµ·å…³æ³¨ã€‚</li>
<li>LLMçš„éšæœºæ€§å’Œå¹»è§‰å¯èƒ½å½±å“å­¦ä¹ ä½“éªŒå’Œéµå®ˆé“å¾·æ ‡å‡†ã€‚</li>
<li>ä½¿ç”¨DeanLLMsæ–¹æ³•è‡ªåŠ¨å…¨é¢è¯„ä¼°LLMæä¾›çš„åé¦ˆã€‚</li>
<li>æå‡ºä¸€ä¸ªåŒ…å«å¤šç»´åº¦è¯„ä»·ä½“ç³»çš„ç»¼åˆè¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°LLMç”Ÿæˆçš„åé¦ˆè´¨é‡ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªè™šæ‹Ÿä½œä¸šæäº¤æ•°æ®é›†æ”¯æŒLLMåé¦ˆè¯„ä¼°æ¨¡å‹çš„å¾®è°ƒä¸è¯„ä¼°ã€‚</li>
<li>GPT-4.1ç»è¿‡å¾®è°ƒåæ€§èƒ½æ¥è¿‘äººç±»ä¸“å®¶æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05952">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e9150ad5054987fda80cf103440fa8c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-517867bb87f1821ddb70e1acf73b126b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4fc45a0845b250564b010c4793d59210.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64bed3f9c6e5b220b70231eeb6987589.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VISTA-Vision-Language-Imitation-of-Situational-Thinking-and-Attention-for-Human-Like-Driver-Focus-in-Dynamic-Environments"><a href="#VISTA-Vision-Language-Imitation-of-Situational-Thinking-and-Attention-for-Human-Like-Driver-Focus-in-Dynamic-Environments" class="headerlink" title="VISTA: Vision-Language Imitation of Situational Thinking and Attention   for Human-Like Driver Focus in Dynamic Environments"></a>VISTA: Vision-Language Imitation of Situational Thinking and Attention   for Human-Like Driver Focus in Dynamic Environments</h2><p><strong>Authors:Kaiser Hamid, Khandakar Ashrafi Akbar, Nade Liang</strong></p>
<p>Driver visual attention prediction is a critical task in autonomous driving and human-computer interaction (HCI) research. Most prior studies focus on estimating attention allocation at a single moment in time, typically using static RGB images such as driving scene pictures. In this work, we propose a vision-language framework that models the changing landscape of driversâ€™ gaze through natural language, using few-shot and zero-shot learning on single RGB images. We curate and refine high-quality captions from the BDD-A dataset using human-in-the-loop feedback, then fine-tune LLaVA to align visual perception with attention-centric scene understanding. Our approach integrates both low-level cues and top-down context (e.g., route semantics, risk anticipation), enabling language-based descriptions of gaze behavior. We evaluate performance across training regimes (few shot, and one-shot) and introduce domain-specific metrics for semantic alignment and response diversity. Results show that our fine-tuned model outperforms general-purpose VLMs in attention shift detection and interpretability. To our knowledge, this is among the first attempts to generate driver visual attention allocation and shifting predictions in natural language, offering a new direction for explainable AI in autonomous driving. Our approach provides a foundation for downstream tasks such as behavior forecasting, human-AI teaming, and multi-agent coordination. </p>
<blockquote>
<p>é©¾é©¶è€…è§†è§‰æ³¨æ„åŠ›é¢„æµ‹åœ¨è‡ªåŠ¨é©¾é©¶å’Œäººæœºäº¤äº’ï¼ˆHCIï¼‰ç ”ç©¶ä¸­æ˜¯ä¸€é¡¹è‡³å…³é‡è¦çš„ä»»åŠ¡ã€‚æ—©æœŸçš„å¤§éƒ¨åˆ†ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä¼°è®¡å•ä¸€æ—¶åˆ»çš„æ³¨æ„åŠ›åˆ†é…ï¼Œé€šå¸¸ä½¿ç”¨é™æ€RGBå›¾åƒï¼Œå¦‚é©¾é©¶åœºæ™¯å›¾ç‰‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè§†è§‰è¯­è¨€æ¡†æ¶ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€å¯¹é©¾é©¶è€…è§†çº¿å˜åŒ–è¿›è¡Œå»ºæ¨¡ï¼Œåˆ©ç”¨å•RGBå›¾åƒçš„å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬å­¦ä¹ ã€‚æˆ‘ä»¬é€šè¿‡äººç±»å‚ä¸åé¦ˆæ¥ç²¾ç‚¼å’Œç­›é€‰BDD-Aæ•°æ®é›†çš„é«˜è´¨é‡æ ‡é¢˜ï¼Œç„¶åå¯¹LLaVAè¿›è¡Œå¾®è°ƒï¼Œä½¿è§†è§‰æ„ŸçŸ¥ä¸ä»¥æ³¨æ„åŠ›ä¸ºä¸­å¿ƒçš„åœºæ™¯ç†è§£ä¿æŒä¸€è‡´ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ä½çº§çº¿ç´¢å’Œè‡ªä¸Šè€Œä¸‹ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚ï¼Œè·¯çº¿è¯­ä¹‰ã€é£é™©é¢„æµ‹ï¼‰ï¼Œå®ç°äº†åŸºäºè¯­è¨€çš„è§†çº¿è¡Œä¸ºæè¿°ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸åŒè®­ç»ƒæ¨¡å¼ï¼ˆå°‘æ ·æœ¬å’Œä¸€æ¬¡å­¦ä¹ ï¼‰çš„æ€§èƒ½ï¼Œå¹¶å¼•å…¥äº†ç”¨äºè¯­ä¹‰å¯¹é½å’Œå“åº”å¤šæ ·æ€§çš„é¢†åŸŸç‰¹å®šæŒ‡æ ‡ã€‚ç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„æ¨¡å‹åœ¨æ³¨æ„åŠ›è½¬ç§»æ£€æµ‹å’Œå¯è§£é‡Šæ€§æ–¹é¢ä¼˜äºé€šç”¨VLMsã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•ç”¨è‡ªç„¶è¯­è¨€ç”Ÿæˆé©¾é©¶è€…è§†è§‰æ³¨æ„åŠ›åˆ†é…å’Œè½¬ç§»é¢„æµ‹ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶ä¸­çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½æä¾›äº†æ–°çš„æ–¹å‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚è¡Œä¸ºé¢„æµ‹ã€äººæœºåä½œå’Œå¤šæ™ºèƒ½ä½“åè°ƒï¼‰æä¾›äº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05852v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆè§†è§‰ä¸è¯­è¨€çš„æ¡†æ¶ï¼Œåˆ©ç”¨å°‘é‡æ ·æœ¬å­¦ä¹ ä¸é›¶æ ·æœ¬å­¦ä¹ æŠ€æœ¯ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æè¿°æ¥æ¨¡æ‹Ÿé©¾é©¶è€…è§†çº¿çš„åŠ¨æ€å˜åŒ–ã€‚è¯¥ç ”ç©¶ä½¿ç”¨BDD-Aæ•°æ®é›†çš„é«˜è´¨é‡æè¿°ï¼Œé€šè¿‡äººç±»åé¦ˆè¿›è¡Œå¾®è°ƒï¼Œå¹¶é‡‡ç”¨LLaVAæ¨¡å‹è¿›è¡Œè§†è§‰æ„ŸçŸ¥ä¸æ³¨æ„åŠ›é›†ä¸­åœºæ™¯ç†è§£çš„æ ¡å‡†ã€‚è¯¥æ–¹æ³•ç»“åˆäº†åº•å±‚çº¿ç´¢å’Œé¡¶å±‚ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¦‚è·¯çº¿è¯­ä¹‰å’Œé£é™©è¯„ä¼°ï¼Œä»¥è¯­è¨€æè¿°é©¾é©¶è€…çš„è§†çº¿è¡Œä¸ºã€‚åœ¨å¤šç§è®­ç»ƒæ¨¡å¼ä¸‹è¿›è¡Œè¯„ä¼°ï¼Œå¹¶å¼•å…¥ç‰¹å®šé¢†åŸŸçš„è¯­ä¹‰å¯¹é½å’Œå“åº”å¤šæ ·æ€§æŒ‡æ ‡ã€‚ç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒåçš„æ¨¡å‹åœ¨æ³¨æ„åŠ›è½¬ç§»æ£€æµ‹å’Œå¯è§£é‡Šæ€§æ–¹é¢ä¼˜äºé€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ã€‚è¿™æ˜¯é¦–æ¬¡å°è¯•ç”¨è‡ªç„¶è¯­è¨€ç”Ÿæˆé©¾é©¶è€…è§†è§‰æ³¨æ„åŠ›åˆ†é…å’Œè½¬ç§»é¢„æµ‹çš„ç ”ç©¶ä¹‹ä¸€ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶ä¸­çš„å¯è§£é‡Šæ€§AIæä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰ä¸è¯­è¨€æ¡†æ¶ï¼Œç”¨äºæ¨¡æ‹Ÿé©¾é©¶è€…è§†çº¿çš„åŠ¨æ€å˜åŒ–ã€‚</li>
<li>åˆ©ç”¨å°‘é‡æ ·æœ¬å­¦ä¹ ä¸é›¶æ ·æœ¬å­¦ä¹ æŠ€æœ¯ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æè¿°æ¥é¢„æµ‹é©¾é©¶è€…çš„è§†è§‰æ³¨æ„åŠ›ã€‚</li>
<li>ä½¿ç”¨BDD-Aæ•°æ®é›†çš„é«˜è´¨é‡æè¿°ï¼Œå¹¶ç»“åˆäººç±»åé¦ˆè¿›è¡Œå¾®è°ƒã€‚</li>
<li>é‡‡ç”¨LLaVAæ¨¡å‹æ ¡å‡†è§†è§‰æ„ŸçŸ¥ä¸æ³¨æ„åŠ›é›†ä¸­åœºæ™¯ç†è§£ã€‚</li>
<li>ç»“åˆåº•å±‚çº¿ç´¢å’Œé¡¶å±‚ä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œåˆ†æã€‚</li>
<li>åœ¨å¤šç§è®­ç»ƒæ¨¡å¼ä¸‹è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œå¹¶å¼•å…¥ç‰¹å®šé¢†åŸŸçš„è¯­ä¹‰å¯¹é½å’Œå“åº”å¤šæ ·æ€§æŒ‡æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05852">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7f3335e7b0ada2d0e984e0daa10e4eb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c26850fef748711a180758de7b944429.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c7c5c02bb1c2527ff95768d4daf826da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3581ed0f2b19eb83645e520f02a7f03b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Deployment-of-Pretrained-MRI-Transformers-in-Brain-Imaging-Tasks"><a href="#Few-Shot-Deployment-of-Pretrained-MRI-Transformers-in-Brain-Imaging-Tasks" class="headerlink" title="Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging   Tasks"></a>Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging   Tasks</h2><p><strong>Authors:Mengyu Li, Guoyao Shen, Chad W. Farris, Xin Zhang</strong></p>
<p>Machine learning using transformers has shown great potential in medical imaging, but its real-world applicability remains limited due to the scarcity of annotated data. In this study, we propose a practical framework for the few-shot deployment of pretrained MRI transformers in diverse brain imaging tasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a large-scale, multi-cohort brain MRI dataset comprising over 31 million slices, we obtain highly transferable latent representations that generalize well across tasks and datasets. For high-level tasks such as classification, a frozen MAE encoder combined with a lightweight linear head achieves state-of-the-art accuracy in MRI sequence identification with minimal supervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a hybrid architecture that fuses multiscale CNN features with pretrained MAE embeddings. This model consistently outperforms other strong baselines in both skull stripping and multi-class anatomical segmentation under data-limited conditions. With extensive quantitative and qualitative evaluations, our framework demonstrates efficiency, stability, and scalability, suggesting its suitability for low-resource clinical environments and broader neuroimaging applications. </p>
<blockquote>
<p>åˆ©ç”¨è½¬æ¢å™¨è¿›è¡Œæœºå™¨å­¦ä¹ åœ¨åŒ»å­¦æˆåƒä¸­æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç”±äºç¼ºä¹æ ‡æ³¨æ•°æ®ï¼Œå…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„é€‚ç”¨æ€§ä»ç„¶æœ‰é™ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå®ç”¨çš„æ¡†æ¶ï¼Œç”¨äºåœ¨å¤šç§è„‘æˆåƒä»»åŠ¡ä¸­éƒ¨ç½²å°‘é‡çš„é¢„è®­ç»ƒMRIè½¬æ¢å™¨ã€‚é€šè¿‡åœ¨ä¸€ä¸ªå¤§è§„æ¨¡ã€å¤šé˜Ÿåˆ—çš„è„‘MRIæ•°æ®é›†ï¼ˆåŒ…å«è¶…è¿‡3100ä¸‡å¼ åˆ‡ç‰‡ï¼‰ä¸Šé‡‡ç”¨Masked Autoencoderï¼ˆMAEï¼‰é¢„è®­ç»ƒç­–ç•¥ï¼Œæˆ‘ä»¬è·å¾—äº†é«˜åº¦å¯è¿ç§»çš„æ½œåœ¨è¡¨ç¤ºï¼Œè¿™äº›è¡¨ç¤ºåœ¨ä»»åŠ¡å’Œæ•°æ®é›†ä¹‹é—´å…·æœ‰å¾ˆå¥½çš„é€šç”¨æ€§ã€‚å¯¹äºé«˜çº§ä»»åŠ¡ï¼Œå¦‚åˆ†ç±»ï¼Œä½¿ç”¨å†»ç»“çš„MAEç¼–ç å™¨ç»“åˆè½»é‡çº§çº¿æ€§å¤´ï¼Œå¯ä»¥åœ¨å‡ ä¹æ— ç›‘ç£çš„æƒ…å†µä¸‹å®ç°MRIåºåˆ—è¯†åˆ«çš„æœ€æ–°å‡†ç¡®æ€§ã€‚å¯¹äºä½çº§ä»»åŠ¡ï¼ˆå¦‚åˆ†å‰²ï¼‰ï¼Œæˆ‘ä»¬æå‡ºäº†MAE-FUnetï¼Œè¿™æ˜¯ä¸€ç§æ··åˆæ¶æ„ï¼Œèåˆäº†å¤šå°ºåº¦CNNç‰¹å¾ä¸é¢„è®­ç»ƒçš„MAEåµŒå…¥ã€‚è¯¥æ¨¡å‹åœ¨é¢…éª¨å‰¥ç¦»å’Œå¤šç±»è§£å‰–åˆ†å‰²ä»»åŠ¡ä¸­å‡æŒç»­è¶…è¶Šå…¶ä»–å¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œå±•ç°å‡ºå…¶åœ¨æ•°æ®æœ‰é™æ¡ä»¶ä¸‹çš„ä¼˜è¶Šæ€§ã€‚é€šè¿‡å¹¿æ³›çš„å®šé‡å’Œå®šæ€§è¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ¡†æ¶è¡¨ç°å‡ºé«˜æ•ˆæ€§ã€ç¨³å®šæ€§å’Œå¯æ‰©å±•æ€§ï¼Œè¡¨æ˜å…¶é€‚ç”¨äºèµ„æºæœ‰é™çš„ä¸´åºŠç¯å¢ƒå’Œæ›´å¹¿æ³›çš„ç¥ç»å½±åƒåº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05783v1">PDF</a> 30 pages, 8 figures, 7 tables</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå®ç”¨çš„æ¡†æ¶ï¼Œç”¨äºåœ¨åŒ»å­¦å›¾åƒé¢†åŸŸéƒ¨ç½²é¢„è®­ç»ƒçš„MRIè½¬æ¢å™¨ã€‚é€šè¿‡å¤§è§„æ¨¡å¤šé˜Ÿåˆ—MRIæ•°æ®é›†é‡‡ç”¨Masked Autoencoderï¼ˆMAEï¼‰é¢„è®­ç»ƒç­–ç•¥ï¼Œè·å¾—é«˜åº¦å¯è¿ç§»çš„æ½œåœ¨è¡¨ç¤ºï¼Œå®ç°è·¨ä»»åŠ¡å’Œæ•°æ®é›†çš„è‰¯å¥½æ³›åŒ–ã€‚åœ¨é«˜çº§ä»»åŠ¡ï¼ˆå¦‚åˆ†ç±»ï¼‰ä¸­ï¼Œå†»ç»“çš„MAEç¼–ç å™¨ç»“åˆè½»é‡çº§çº¿æ€§å¤´å®ç°äº†åœ¨MRIåºåˆ—è¯†åˆ«æ–¹é¢çš„æœ€å…ˆè¿›çš„å‡†ç¡®åº¦ï¼Œå¹¶å¯åœ¨æ•°æ®å—é™çš„æ¡ä»¶ä¸‹å–å¾—è¾ƒå¥½çš„æ•ˆæœã€‚åœ¨ä½çº§ä»»åŠ¡ï¼ˆå¦‚åˆ†å‰²ï¼‰ä¸­ï¼Œæœ¬ç ”ç©¶æå‡ºäº†MAE-FUnetæ··åˆæ¶æ„ï¼Œèåˆäº†å¤šå°ºåº¦CNNç‰¹å¾ä¸é¢„è®­ç»ƒçš„MAEåµŒå…¥ã€‚è¯¥æ¨¡å‹åœ¨é¢…éª¨å‰¥ç¦»å’Œå¤šç±»è§£å‰–åˆ†å‰²ä»»åŠ¡ä¸­å‡ä¼˜äºå…¶ä»–å¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œå¹¶é€šè¿‡å¹¿æ³›çš„å®šé‡å’Œå®šæ€§è¯„ä¼°è¯æ˜äº†å…¶æ•ˆç‡ã€ç¨³å®šæ€§å’Œå¯æ‰©å±•æ€§ï¼Œé€‚ç”¨äºä½èµ„æºä¸´åºŠç¯å¢ƒå’Œæ›´å¹¿æ³›çš„ç¥ç»å½±åƒåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨Masked Autoencoder (MAE) é¢„è®­ç»ƒç­–ç•¥å®ç°äº†MRIè½¬æ¢å™¨åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸçš„ä¼˜å¼‚è¡¨ç°ã€‚</li>
<li>é€šè¿‡å¤§è§„æ¨¡MRIæ•°æ®é›†çš„è®­ç»ƒï¼Œè·å¾—äº†é«˜åº¦å¯è¿ç§»çš„æ½œåœ¨è¡¨ç¤ºï¼Œæå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨é«˜çº§ä»»åŠ¡ä¸­ï¼Œç»“åˆå†»ç»“çš„MAEç¼–ç å™¨å’Œè½»é‡çº§çº¿æ€§å¤´å®ç°äº†MRIåºåˆ—è¯†åˆ«çš„æœ€å…ˆè¿›çš„å‡†ç¡®åº¦ã€‚</li>
<li>é’ˆå¯¹ä½çº§ä»»åŠ¡å¦‚åˆ†å‰²ï¼Œæå‡ºäº†MAE-FUnetæ··åˆæ¶æ„ï¼Œèåˆäº†å¤šå°ºåº¦CNNç‰¹å¾ä¸é¢„è®­ç»ƒçš„MAEåµŒå…¥ï¼Œè¡¨ç°å‡ºè‰²ã€‚</li>
<li>æ¨¡å‹åœ¨æ•°æ®å—é™çš„æ¡ä»¶ä¸‹ä»èƒ½ä¿æŒä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>æ¨¡å‹é€šè¿‡äº†å¹¿æ³›çš„å®šé‡å’Œå®šæ€§è¯„ä¼°ï¼Œè¯æ˜äº†å…¶æ•ˆç‡ã€ç¨³å®šæ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05783">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-902914f04553879e8aca889e34e38ebb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de58066bf898e3df5e6f283375875c4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b270e043182bf3326e98bc3533e1cb45.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Generalized-Few-Shot-Out-of-Distribution-Detection"><a href="#Generalized-Few-Shot-Out-of-Distribution-Detection" class="headerlink" title="Generalized Few-Shot Out-of-Distribution Detection"></a>Generalized Few-Shot Out-of-Distribution Detection</h2><p><strong>Authors:Pinxuan Li, Bing Cao, Changqing Zhang, Qinghua Hu</strong></p>
<p>Few-shot Out-of-Distribution (OOD) detection has emerged as a critical research direction in machine learning for practical deployment. Most existing Few-shot OOD detection methods suffer from insufficient generalization capability for the open world. Due to the few-shot learning paradigm, the OOD detection ability is often overfit to the limited training data itself, thus degrading the performance on generalized data and performing inconsistently across different scenarios. To address this challenge, we proposed a Generalized Few-shot OOD Detection (GOOD) framework, which empowers the general knowledge of the OOD detection model with an auxiliary General Knowledge Model (GKM), instead of directly learning from few-shot data. We proceed to reveal the few-shot OOD detection from a generalization perspective and theoretically derive the Generality-Specificity balance (GS-balance) for OOD detection, which provably reduces the upper bound of generalization error with a general knowledge model. Accordingly, we propose a Knowledge Dynamic Embedding (KDE) mechanism to adaptively modulate the guidance of general knowledge. KDE dynamically aligns the output distributions of the OOD detection model to the general knowledge model based on the Generalized Belief (G-Belief) of GKM, thereby boosting the GS-balance. Experiments on real-world OOD benchmarks demonstrate our superiority. Codes will be available. </p>
<blockquote>
<p>å°‘æ ·æœ¬åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ£€æµ‹ä½œä¸ºæœºå™¨å­¦ä¹ åœ¨å®é™…éƒ¨ç½²ä¸­çš„å…³é”®ç ”ç©¶æ–¹å‘å·²ç»å‡ºç°ã€‚å¤§å¤šæ•°ç°æœ‰çš„å°‘æ ·æœ¬OODæ£€æµ‹æ–¹æ³•å¯¹äºå¼€æ”¾ä¸–ç•Œçš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ä»¥åº”å¯¹æŒ‘æˆ˜ã€‚ç”±äºå°‘æ ·æœ¬å­¦ä¹ æ¨¡å¼ï¼ŒOODæ£€æµ‹èƒ½åŠ›å¾€å¾€è¿‡åº¦æ‹Ÿåˆæœ‰é™çš„è®­ç»ƒæ•°æ®æœ¬èº«ï¼Œä»è€Œåœ¨å¹¿ä¹‰æ•°æ®ä¸Šçš„è¡¨ç°ä¸‹é™ï¼Œå¹¶ä¸”åœ¨ä¸åŒåœºæ™¯ä¸­çš„è¡¨ç°ä¸ä¸€è‡´ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¹¿ä¹‰å°‘æ ·æœ¬OODæ£€æµ‹ï¼ˆGOODï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡è¾…åŠ©é€šç”¨çŸ¥è¯†æ¨¡å‹ï¼ˆGKMï¼‰èµ‹äºˆOODæ£€æµ‹æ¨¡å‹çš„é€šç”¨çŸ¥è¯†ï¼Œè€Œä¸æ˜¯ç›´æ¥ä»å°‘é‡æ•°æ®ä¸­å­¦ä¹ ã€‚æˆ‘ä»¬ä»æ³›åŒ–è§’åº¦æ­ç¤ºå°‘æ ·æœ¬OODæ£€æµ‹ï¼Œå¹¶ç†è®ºä¸Šæ¨å¯¼å‡ºOODæ£€æµ‹çš„æ³›åŒ–æ€§-ç‰¹å¼‚æ€§å¹³è¡¡ï¼ˆGS-balanceï¼‰ï¼Œä½¿ç”¨é€šç”¨çŸ¥è¯†æ¨¡å‹å¯ä»¥è¯æ˜é™ä½æ³›åŒ–è¯¯å·®çš„ä¸Šç•Œã€‚ç›¸åº”åœ°ï¼Œæˆ‘ä»¬æå‡ºäº†çŸ¥è¯†åŠ¨æ€åµŒå…¥ï¼ˆKDEï¼‰æœºåˆ¶ï¼Œä»¥è‡ªé€‚åº”åœ°è°ƒèŠ‚é€šç”¨çŸ¥è¯†çš„æŒ‡å¯¼ã€‚KDEåŸºäºGKMçš„å¹¿ä¹‰ä¿¡å¿µï¼ˆG-Beliefï¼‰åŠ¨æ€è°ƒæ•´OODæ£€æµ‹æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒï¼Œä»¥åŒ¹é…é€šç”¨çŸ¥è¯†æ¨¡å‹ï¼Œä»è€Œæé«˜GS-balanceã€‚åœ¨çœŸå®ä¸–ç•Œçš„OODåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚ç›¸å…³ä»£ç å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05732v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é¢å‘å®é™…åº”ç”¨éƒ¨ç½²çš„æœºå™¨å­¦ä¹ é¢†åŸŸä¸­æ–°å…´çš„å…³é”®ç ”ç©¶æ–¹å‘â€”â€”Few-shot Out-of-Distributionï¼ˆOODï¼‰æ£€æµ‹ã€‚ç°æœ‰æ–¹æ³•æ™®éå­˜åœ¨æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸‹è¡¨ç°ä¸ä½³ã€‚é’ˆå¯¹æ­¤æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†Generalized Few-shot OOD Detectionï¼ˆGOODï¼‰æ¡†æ¶ï¼Œå€ŸåŠ©è¾…åŠ©é€šç”¨çŸ¥è¯†æ¨¡å‹ï¼ˆGKMï¼‰ä¸ºOODæ£€æµ‹æ¨¡å‹èµ‹äºˆé€šç”¨çŸ¥è¯†èƒ½åŠ›ï¼Œè€Œä¸æ˜¯ç›´æ¥ä»å°‘é‡æ•°æ®ä¸­å­¦ä¹ ã€‚æœ¬æ–‡è¿˜ä»æ³›åŒ–è§’åº¦æ­ç¤ºäº†few-shot OODæ£€æµ‹ï¼Œå¹¶ç†è®ºä¸Šæ¨å¯¼äº†ç”¨äºå‡å°‘æ³›åŒ–è¯¯å·®ä¸Šç•Œçš„Generality-Specificityå¹³è¡¡ï¼ˆGS-balanceï¼‰ã€‚åŒæ—¶ï¼Œæå‡ºäº†Knowledge Dynamic Embeddingï¼ˆKDEï¼‰æœºåˆ¶ï¼Œæ ¹æ®GKMçš„Generalized Beliefè‡ªé€‚åº”è°ƒæ•´é€šç”¨çŸ¥è¯†çš„æŒ‡å¯¼ï¼Œæå‡GS-balanceã€‚åœ¨çœŸå®ä¸–ç•Œçš„OODåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shot Out-of-Distribution (OOD) æ£€æµ‹æ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„é‡è¦ç ”ç©¶æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨å®é™…åº”ç”¨éƒ¨ç½²ä¸­ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸‹æ€§èƒ½ä¸‹é™ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†Generalized Few-shot OOD Detection (GOOD) æ¡†æ¶ï¼Œå€ŸåŠ©è¾…åŠ©é€šç”¨çŸ¥è¯†æ¨¡å‹ï¼ˆGKMï¼‰å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ­ç¤ºäº†ä»æ³›åŒ–è§’åº¦çœ‹å¾…few-shot OODæ£€æµ‹ï¼Œå¹¶æ¨å¯¼äº†Generality-Specificity balanceï¼ˆGS-balanceï¼‰ä»¥å‡å°‘æ³›åŒ–è¯¯å·®ä¸Šç•Œã€‚</li>
<li>æå‡ºäº†Knowledge Dynamic Embedding (KDE) æœºåˆ¶ï¼Œæ ¹æ®GKMçš„Generalized Beliefè‡ªé€‚åº”è°ƒæ•´é€šç”¨çŸ¥è¯†çš„æŒ‡å¯¼ï¼Œå¢å¼ºæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åœ¨çœŸå®ä¸–ç•Œçš„OODåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œè¯æ˜äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05732">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d45784f0c76d1ed0178171de8bd28015.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a745f490a8e7a3c8287beb0f9144069.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afd68642de3e6293a710630b31358f38.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-356f0196447049779deae95b8197611d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="M-2-IV-Towards-Efficient-and-Fine-grained-Multimodal-In-Context-Learning-via-Representation-Engineering"><a href="#M-2-IV-Towards-Efficient-and-Fine-grained-Multimodal-In-Context-Learning-via-Representation-Engineering" class="headerlink" title="M$^2$IV: Towards Efficient and Fine-grained Multimodal In-Context   Learning via Representation Engineering"></a>M$^2$IV: Towards Efficient and Fine-grained Multimodal In-Context   Learning via Representation Engineering</h2><p><strong>Authors:Yanshu Li, Yi Cao, Hongyang He, Qisen Cheng, Xiang Fu, Xi Xiao, Tianyang Wang, Ruixiang Tang</strong></p>
<p>Multimodal in-context learning (ICL) equips Large Vision-language Models (LVLMs) with the ability to adapt to new tasks via multiple user-provided demonstrations, without requiring any model parameter updates. However, its effectiveness is constrained by the token-intensive nature of multimodal inputs and the complexity of cross-modal few-shot reasoning, which together hinder LVLMs from extracting useful patterns from demonstrations. To address these challenges, we propose \textbf{M$^2$IV}, a novel representation engineering approach that replaces explicit token-level demonstrations with a set of learnable Multimodal In-context Vectors directly injected into the residual streams of LVLMs. By analyzing the distinct roles of multi-head attention (MHA) and multi-layer perceptrons (MLP) in the ICL process, we design a training strategy that enables M$^2$IV to perform fine-grained semantic distillation and robust cross-modal representation learning. M$^2$IV not only improves performance across diverse tasks and LVLMs but also significantly reduces token overhead, enabling graceful scaling to many-shot scenarios. To further enhance usability, we introduce \textbf{VLibrary}, a repository that stores trained M$^2$IVs for flexible retrieval and injection. With VLibrary, users can steer pre-trained LVLMs in a customized manner that meets diverse requirements. Extensive experiments demonstrate that M$^2$IV consistently outperforms vanilla ICL and prior representation engineering baselines, achieving an average accuracy gain of 3.74% with substantial improvements in overall efficiency. </p>
<blockquote>
<p>å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä½¿å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰èƒ½å¤Ÿé€šè¿‡å¤šä¸ªç”¨æˆ·æä¾›çš„æ¼”ç¤ºæ¥é€‚åº”æ–°ä»»åŠ¡ï¼Œè€Œæ— éœ€è¿›è¡Œä»»ä½•æ¨¡å‹å‚æ•°æ›´æ–°ã€‚ç„¶è€Œï¼Œå…¶æœ‰æ•ˆæ€§å—åˆ°å¤šæ¨¡æ€è¾“å…¥ä¸­ä»¤ç‰Œå¯†é›†æ€§è´¨å’Œè·¨æ¨¡æ€å°‘é‡æ¨ç†çš„å¤æ‚æ€§çš„é™åˆ¶ï¼Œè¿™ä¸¤è€…å…±åŒé˜»ç¢LVLMsä»æ¼”ç¤ºä¸­æå–æœ‰ç”¨çš„æ¨¡å¼ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†\textbf{M$^2$IV}ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹è¡¨ç¤ºå·¥ç¨‹æ–¹æ³•ï¼Œå®ƒç”¨ä¸€ç»„å¯å­¦ä¹ çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å‘é‡æ›¿æ¢æ˜¾å¼ä»¤ç‰Œçº§æ¼”ç¤ºï¼Œå¹¶ç›´æ¥æ³¨å…¥LVLMsçš„æ®‹å·®æµä¸­ã€‚é€šè¿‡åˆ†æå¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰å’Œå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰åœ¨ICLè¿‡ç¨‹ä¸­çš„ä¸åŒä½œç”¨ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è®­ç»ƒç­–ç•¥ï¼Œä½¿M$^2$IVèƒ½å¤Ÿæ‰§è¡Œç²¾ç»†çš„è¯­ä¹‰è’¸é¦å’Œç¨³å¥çš„è·¨æ¨¡æ€è¡¨ç¤ºå­¦ä¹ ã€‚M$^2$IVä¸ä»…æé«˜äº†ä¸åŒä»»åŠ¡å’ŒLVLMsçš„æ€§èƒ½ï¼Œè€Œä¸”å¤§å¤§é™ä½äº†ä»¤ç‰Œå¼€é”€ï¼Œèƒ½å¤Ÿçµæ´»åœ°æ‰©å±•åˆ°è®¸å¤šåœºæ™¯ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å¯ç”¨æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†\textbf{VLibrary}ï¼Œä¸€ä¸ªå­˜å‚¨è®­ç»ƒå¥½çš„M$^2$IVçš„ä»“åº“ï¼Œç”¨äºçµæ´»æ£€ç´¢å’Œæ³¨å…¥ã€‚å€ŸåŠ©VLibraryï¼Œç”¨æˆ·å¯ä»¥ä»¥ç¬¦åˆå„ç§éœ€æ±‚çš„æ–¹å¼å¼•å¯¼é¢„è®­ç»ƒçš„LVLMsã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒM$^2$IVå§‹ç»ˆä¼˜äºæ™®é€šICLå’Œå…ˆå‰çš„è¡¨ç¤ºå·¥ç¨‹åŸºçº¿ï¼Œåœ¨å¹³å‡å‡†ç¡®ç‡ä¸Šæé«˜äº†3.74%ï¼Œå¹¶ä¸”åœ¨æ€»ä½“æ•ˆç‡ä¸Šæœ‰æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04633v2">PDF</a> COLM 2025, 30 pages, 10 figures, 16 tables</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰é€šè¿‡ç”¨æˆ·æä¾›çš„å¤šä¸ªç¤ºèŒƒæ¥ä½¿å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰é€‚åº”æ–°ä»»åŠ¡ï¼Œæ— éœ€æ›´æ–°ä»»ä½•æ¨¡å‹å‚æ•°ã€‚ç„¶è€Œï¼Œå…¶æœ‰æ•ˆæ€§å—åˆ°å¤šæ¨¡æ€è¾“å…¥ä»¤ç‰Œå¯†é›†å’Œè·¨æ¨¡æ€å°‘æ ·æœ¬æ¨ç†çš„å¤æ‚æ€§çš„é™åˆ¶ï¼Œé˜»ç¢äº†LVLMsä»æ¼”ç¤ºä¸­æå–æœ‰ç”¨æ¨¡å¼ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†M$^2$IVï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹è¡¨ç¤ºå·¥ç¨‹æ–¹æ³•ï¼Œå®ƒç”¨ä¸€ç»„å¯å­¦ä¹ çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å‘é‡æ›¿æ¢æ˜ç¡®çš„ä»¤ç‰Œçº§æ¼”ç¤ºï¼Œç›´æ¥æ³¨å…¥LVLMsçš„æ®‹å·®æµä¸­ã€‚é€šè¿‡åˆ†æå’ŒICLè¿‡ç¨‹ä¸­å¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰å’Œå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰çš„ä¸åŒä½œç”¨ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è®­ç»ƒç­–ç•¥ï¼Œä½¿M$^2$IVèƒ½å¤Ÿè¿›è¡Œç²¾ç»†çš„è¯­ä¹‰è’¸é¦å’Œç¨³å¥çš„è·¨æ¨¡æ€è¡¨ç¤ºå­¦ä¹ ã€‚M$^2$IVä¸ä»…æé«˜äº†ä¸åŒä»»åŠ¡å’ŒLVLMsçš„æ€§èƒ½ï¼Œè€Œä¸”æ˜¾è‘—å‡å°‘äº†ä»¤ç‰Œå¼€é”€ï¼Œå®ç°å¹³æ»‘æ‰©å±•è‡³å¤šå®ä¾‹åœºæ™¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†VLibraryå­˜å‚¨åº“ï¼Œç”¨äºå­˜å‚¨è®­ç»ƒå¥½çš„M$^2$IVå‘é‡ä»¥ä¾›çµæ´»æ£€ç´¢å’Œæ³¨å…¥ã€‚ç”¨æˆ·å¯ä»¥åˆ©ç”¨VLibraryä»¥ç¬¦åˆå„ç§éœ€æ±‚çš„æ–¹å¼å®šåˆ¶é¢„è®­ç»ƒçš„LVLMsã€‚å®éªŒè¡¨æ˜ï¼ŒM$^2$IVå§‹ç»ˆä¼˜äºåŸºæœ¬çš„ICLå’Œå…ˆå‰çš„è¡¨ç¤ºå·¥ç¨‹åŸºçº¿ï¼Œå¹³å‡å‡†ç¡®åº¦æé«˜äº†3.74%ï¼Œæ•´ä½“æ•ˆç‡ä¹Ÿå¤§å¤§æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å…è®¸å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰é€šè¿‡ç”¨æˆ·æä¾›çš„ç¤ºèŒƒé€‚åº”æ–°ä»»åŠ¡ï¼Œæ— éœ€æ›´æ–°æ¨¡å‹å‚æ•°ã€‚</li>
<li>M$^2$IVæ˜¯ä¸€ç§æ–°å‹è¡¨ç¤ºå·¥ç¨‹æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€è¾“å…¥çš„ä»¤ç‰Œå¯†é›†æ€§å’Œè·¨æ¨¡æ€å°‘æ ·æœ¬æ¨ç†çš„å¤æ‚æ€§æŒ‘æˆ˜ã€‚</li>
<li>M$^2$IVé€šè¿‡å¼•å…¥å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å‘é‡æ¥æ”¹è¿›ICLï¼Œè¿™äº›å‘é‡ç›´æ¥æ³¨å…¥LVLMsçš„æ®‹å·®æµä¸­ï¼Œæé«˜äº†æ€§èƒ½å’Œæ•ˆç‡ã€‚</li>
<li>M$^2$IVç»“åˆäº†å¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰å’Œå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰çš„åˆ†æï¼Œå®ç°äº†ç²¾ç»†çš„è¯­ä¹‰è’¸é¦å’Œè·¨æ¨¡æ€è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>VLibraryæ˜¯ä¸€ä¸ªå­˜å‚¨åº“ï¼Œç”¨äºå­˜å‚¨è®­ç»ƒå¥½çš„M$^2$IVå‘é‡ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥æ ¹æ®å„ç§éœ€æ±‚å®šåˆ¶é¢„è®­ç»ƒçš„LVLMsã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒM$^2$IVåœ¨å¤šä¸ªä»»åŠ¡å’ŒLVLMsä¸Šçš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„ICLæ–¹æ³•å’Œè¡¨ç¤ºå·¥ç¨‹åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04633">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d40c1c3d4b16eb223e26f4618502c499.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-254bbab1f840c76245a32159cdb884d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8e8178acf0e30627e015f9b56298b5e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-12/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-12/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-12/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3a52a6c3190b8d8432f33e8f8ec143a0.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-12  SPARSE Data, Rich Results Few-Shot Semi-Supervised Learning via   Class-Conditioned Image Translation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-12/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1d484d4af7b1b5b3a516d355b31b255a.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-12  GLM-4.5 Agentic, Reasoning, and Coding (ARC) Foundation Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
