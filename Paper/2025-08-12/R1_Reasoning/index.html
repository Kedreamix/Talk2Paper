<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-12  GLM-4.5 Agentic, Reasoning, and Coding (ARC) Foundation Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c05542ab9bce7cd9cf8f9736871817e2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    93 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-12-æ›´æ–°"><a href="#2025-08-12-æ›´æ–°" class="headerlink" title="2025-08-12 æ›´æ–°"></a>2025-08-12 æ›´æ–°</h1><h2 id="GLM-4-5-Agentic-Reasoning-and-Coding-ARC-Foundation-Models"><a href="#GLM-4-5-Agentic-Reasoning-and-Coding-ARC-Foundation-Models" class="headerlink" title="GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models"></a>GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models</h2><p><strong>Authors:GLM-4. 5 Team,  :, Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen Zhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng, Yifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang, Zilin Zhu, Bohan Zhang, Bosi Wen, Bowen Wu, Bowen Xu, Can Huang, Casey Zhao, Changpeng Cai, Chao Yu, Chen Li, Chendi Ge, Chenghua Huang, Chenhui Zhang, Chenxi Xu, Chenzheng Zhu, Chuang Li, Congfeng Yin, Daoyan Lin, Dayong Yang, Dazhi Jiang, Ding Ai, Erle Zhu, Fei Wang, Gengzheng Pan, Guo Wang, Hailong Sun, Haitao Li, Haiyang Li, Haiyi Hu, Hanyu Zhang, Hao Peng, Hao Tai, Haoke Zhang, Haoran Wang, Haoyu Yang, He Liu, He Zhao, Hongwei Liu, Hongxi Yan, Huan Liu, Huilong Chen, Ji Li, Jiajing Zhao, Jiamin Ren, Jian Jiao, Jiani Zhao, Jianyang Yan, Jiaqi Wang, Jiayi Gui, Jiayue Zhao, Jie Liu, Jijie Li, Jing Li, Jing Lu, Jingsen Wang, Jingwei Yuan, Jingxuan Li, Jingzhao Du, Jinhua Du, Jinxin Liu, Junkai Zhi, Junli Gao, Ke Wang, Lekang Yang, Liang Xu, Lin Fan, Lindong Wu, Lintao Ding, Lu Wang, Man Zhang, Minghao Li, Minghuan Xu, Mingming Zhao, Mingshu Zhai, Pengfan Du, Qian Dong, Shangde Lei, Shangqing Tu, Shangtong Yang, Shaoyou Lu, Shijie Li, Shuang Li,  Shuang-Li, Shuxun Yang, Sibo Yi, Tianshu Yu, Wei Tian, Weihan Wang, Wenbo Yu, Weng Lam Tam, Wenjie Liang, Wentao Liu, Xiao Wang, Xiaohan Jia, Xiaotao Gu, Xiaoying Ling, Xin Wang, Xing Fan, Xingru Pan, Xinyuan Zhang, Xinze Zhang, Xiuqing Fu, Xunkai Zhang, Yabo Xu, Yandong Wu, Yida Lu, Yidong Wang, Yilin Zhou, Yiming Pan, Ying Zhang, Yingli Wang, Yingru Li, Yinpei Su, Yipeng Geng, Yitong Zhu, Yongkun Yang, Yuhang Li, Yuhao Wu, Yujiang Li, Yunan Liu, Yunqing Wang, Yuntao Li, Yuxuan Zhang, Zezhen Liu, Zhen Yang, Zhengda Zhou, Zhongpei Qiao, Zhuoer Feng, Zhuorui Liu, Zichen Zhang, Zihan Wang, Zijun Yao, Zikang Wang, Ziqiang Liu, Ziwei Chai, Zixuan Li, Zuodong Zhao, Wenguang Chen, Jidong Zhai, Bin Xu, Minlie Huang, Hongning Wang, Juanzi Li, Yuxiao Dong, Jie Tang</strong></p>
<p>We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at <a target="_blank" rel="noopener" href="https://github.com/zai-org/GLM-4.5">https://github.com/zai-org/GLM-4.5</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºGLM-4.5ï¼Œè¿™æ˜¯ä¸€æ¬¾æ‹¥æœ‰æ··åˆä¸“å®¶ï¼ˆMoEï¼‰çš„å¤§å‹å¼€æºè¯­è¨€æ¨¡å‹ï¼Œæ€»å‚æ•°ä¸º355Bï¼Œæ¿€æ´»å‚æ•°ä¸º32Bï¼Œé‡‡ç”¨æ··åˆæ¨ç†æ–¹æ³•ï¼Œæ”¯æŒæ€è€ƒå’Œç›´æ¥å“åº”ä¸¤ç§æ¨¡å¼ã€‚é€šè¿‡ä»¥å¤šç§é˜¶æ®µå¯¹å¤šè¾¾çš„ä¸‡äº¿ä»£å¸è¿›è¡Œè®­ç»ƒä»¥åŠåœ¨è®­ç»ƒåçš„ä½¿ç”¨ä¸“å®¶æ¨¡å‹è¿­ä»£ä¸å¼ºåŒ–å­¦ä¹ ï¼ŒGLM-4.5åœ¨ä»£ç†æ™ºèƒ½ã€æ¨ç†å’Œç¼–ç ä»»åŠ¡ï¼ˆARCï¼‰ä¸­è¡¨ç°å“è¶Šã€‚å…¶åœ¨TAU-Benchå¾—åˆ†70.1%ï¼ŒAIME 24å¾—åˆ†é«˜è¾¾91%ï¼Œå¹¶åœ¨SWE-bench Verifiedå¾—åˆ†ä¸Šå–å¾—64.2%ã€‚å°½ç®¡å…¶å‚æ•°å°‘äºå‡ ä¸ªç«äº‰å¯¹æ‰‹ï¼Œä½†GLM-4.5åœ¨æ‰€æœ‰è¯„ä¼°æ¨¡å‹ä¸­æ’åç¬¬ä¸‰ï¼Œå¹¶åœ¨ä»£ç†æ™ºèƒ½åŸºå‡†æµ‹è¯•ä¸­æ’åç¬¬äºŒã€‚æˆ‘ä»¬åŒæ—¶æ¨å‡ºGLM-4.5ï¼ˆæ‹¥æœ‰355Bå‚æ•°ï¼‰å’Œç²¾ç®€ç‰ˆGLM-4.5-Airï¼ˆæ‹¥æœ‰106Bå‚æ•°ï¼‰ï¼Œä»¥ä¿ƒè¿›åœ¨æ¨ç†å’Œä»£ç†æ™ºèƒ½AIç³»ç»Ÿä¸­çš„ç ”ç©¶è¿›æ­¥ã€‚æœ‰å…³ä»£ç ã€æ¨¡å‹ä»¥åŠå…¶ä»–è¯¦ç»†ä¿¡æ¯å¯ä»¥åœ¨ä»¥ä¸‹é“¾æ¥ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/zai-org/GLM-4.5">https://github.com/zai-org/GLM-4.5</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06471v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>GLM-4.5æ˜¯ä¸€ä¸ªæ··åˆæ¨ç†æ–¹æ³•çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…·å¤‡æ€è€ƒæ¨¡å¼å’Œç›´æ¥å“åº”æ¨¡å¼ã€‚å®ƒé€šè¿‡å¤šé˜¶æ®µè®­ç»ƒå’Œç»¼åˆè®­ç»ƒåæ¨¡å‹è¿­ä»£ä¸å¼ºåŒ–å­¦ä¹ ï¼Œå®ç°äº†åœ¨ä»£ç†æ™ºèƒ½ã€æ¨ç†å’Œç¼–ç ä»»åŠ¡ä¸Šçš„å¼ºå¤§æ€§èƒ½ã€‚GLM-4.5ä»¥è¾ƒå°‘çš„å‚æ•°åœ¨ä¼—å¤šè¯„ä¼°æ¨¡å‹ä¸­æ’åç¬¬ä¸‰ï¼Œä¸”ä¸ºæ¨è¿›äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ç ”ç©¶æ¨å‡ºäº†å®Œæ•´ç‰ˆæœ¬å’Œç²¾ç®€ç‰ˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GLM-4.5æ˜¯ä¸€ä¸ªåŸºäºä¸“å®¶æ··åˆæ¨¡å‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ‹¥æœ‰åºå¤§çš„å‚æ•°æ•°é‡ä»¥åŠå¤šç§æ™ºèƒ½åŠŸèƒ½ã€‚</li>
<li>å®ƒé‡‡ç”¨äº†æ”¯æŒæ€è€ƒæ¨¡å¼å’Œç›´æ¥å“åº”æ¨¡å¼çš„æ··åˆæ¨ç†æ–¹æ³•ï¼Œå…¼å…·è®¡ç®—æ•ˆç‡ä¸æ·±åº¦æ€è€ƒèƒ½åŠ›ã€‚</li>
<li>GLM-4.5é€šè¿‡å¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®å’Œä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹å–å¾—äº†å‡ºè‰²çš„æ€§èƒ½è¡¨ç°ã€‚åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºè®¸å¤šç«äº‰å¯¹æ‰‹ã€‚</li>
<li>GLM-4.5åœ¨å¤šé˜¶æ®µè®­ç»ƒåé‡‡ç”¨ä¸“å®¶æ¨¡å‹è¿­ä»£å’Œå¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>GLM-4.5åœ¨ä»£ç†æ™ºèƒ½ã€æ¨ç†å’Œç¼–ç ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½è¡¨ç°èƒ½åŠ›ï¼Œå–å¾—äº†ä¼˜ç§€çš„æ’åç»“æœã€‚å¹¶ä¸”å·²åœ¨TAU-Bench, AIME 24ä»¥åŠSWE-bench Verifiedç­‰åŸºå‡†æµ‹è¯•ä¸­è·å¾—äº†éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06471">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1944867326eeaabd7d81a352f6cfae56.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9a407c3a3daea14fb7a45b779106a6b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e8d62789986051bc0b00b2c09d86a32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1b0c84a58ef6965c1995d3af65a64ac.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SIFThinker-Spatially-Aware-Image-Focus-for-Visual-Reasoning"><a href="#SIFThinker-Spatially-Aware-Image-Focus-for-Visual-Reasoning" class="headerlink" title="SIFThinker: Spatially-Aware Image Focus for Visual Reasoning"></a>SIFThinker: Spatially-Aware Image Focus for Visual Reasoning</h2><p><strong>Authors:Zhangquan Chen, Ruihui Zhao, Chuwei Luo, Mingze Sun, Xinlei Yu, Yangyang Kang, Ruqi Huang</strong></p>
<p>Current multimodal large language models (MLLMs) still face significant challenges in complex visual tasks (e.g., spatial understanding, fine-grained perception). Prior methods have tried to incorporate visual reasoning, however, they fail to leverage attention correction with spatial cues to iteratively refine their focus on prompt-relevant regions. In this paper, we introduce SIFThinker, a spatially-aware â€œthink-with-imagesâ€ framework that mimics human visual perception. Specifically, SIFThinker enables attention correcting and image region focusing by interleaving depth-enhanced bounding boxes and natural language. Our contributions are twofold: First, we introduce a reverse-expansion-forward-inference strategy that facilitates the generation of interleaved image-text chains of thought for process-level supervision, which in turn leads to the construction of the SIF-50K dataset. Besides, we propose GRPO-SIF, a reinforced training paradigm that integrates depth-informed visual grounding into a unified reasoning pipeline, teaching the model to dynamically correct and focus on prompt-relevant regions. Extensive experiments demonstrate that SIFThinker outperforms state-of-the-art methods in spatial understanding and fine-grained visual perception, while maintaining strong general capabilities, highlighting the effectiveness of our method. </p>
<blockquote>
<p>å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤æ‚çš„è§†è§‰ä»»åŠ¡ï¼ˆå¦‚ç©ºé—´ç†è§£ã€ç²¾ç»†ç²’åº¦æ„ŸçŸ¥ï¼‰æ–¹é¢ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¹‹å‰çš„æ–¹æ³•å·²ç»å°è¯•èå…¥è§†è§‰æ¨ç†ï¼Œç„¶è€Œï¼Œå®ƒä»¬æœªèƒ½åˆ©ç”¨å¸¦æœ‰ç©ºé—´çº¿ç´¢çš„æ³¨æ„åŠ›ä¿®æ­£æ¥è¿­ä»£åœ°è°ƒæ•´å¯¹æç¤ºç›¸å…³åŒºåŸŸçš„å…³æ³¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†SIFThinkerï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡ä»¿äººç±»è§†è§‰æ„ŸçŸ¥çš„â€œä¸å›¾åƒä¸€èµ·æ€è€ƒâ€çš„ç©ºé—´æ„ŸçŸ¥æ¡†æ¶ã€‚å…·ä½“è€Œè¨€ï¼ŒSIFThinkeré€šè¿‡äº¤æ›¿ä½¿ç”¨æ·±åº¦å¢å¼ºè¾¹ç•Œæ¡†å’Œè‡ªç„¶è¯­è¨€æ¥å®ç°æ³¨æ„åŠ›æ ¡æ­£å’Œå›¾åƒåŒºåŸŸèšç„¦ã€‚æˆ‘ä»¬çš„è´¡çŒ®æœ‰ä¸¤æ–¹é¢ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åå‘æ‰©å±•å‰å‘æ¨ç†ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æœ‰åŠ©äºç”Ÿæˆäº¤æ›¿çš„å›¾åƒæ–‡æœ¬æ€ç»´é“¾ï¼Œç”¨äºè¿‡ç¨‹çº§ç›‘ç£ï¼Œä»è€Œæ„å»ºäº†SIF-50Kæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†GRPO-SIFï¼Œè¿™æ˜¯ä¸€ç§å¼ºåŒ–è®­ç»ƒèŒƒå¼ï¼Œå°†æ·±åº¦ä¿¡æ¯è§†è§‰å®šä½æ•´åˆåˆ°ç»Ÿä¸€æ¨ç†ç®¡é“ä¸­ï¼Œæ•™ä¼šæ¨¡å‹åŠ¨æ€åœ°ä¿®æ­£å’Œå…³æ³¨æç¤ºç›¸å…³åŒºåŸŸã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSIFThinkeråœ¨ç©ºé—´ç†è§£å’Œç²¾ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥æ–¹é¢ä¼˜äºæœ€æ–°æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„é€šç”¨èƒ½åŠ›ï¼Œå‡¸æ˜¾äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06259v1">PDF</a> 15 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SIFThinkerï¼Œä¸€ä¸ªç»“åˆè§†è§‰æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»è§†è§‰æ„ŸçŸ¥ï¼Œé€šè¿‡æ·±åº¦å¢å¼ºè¾¹ç•Œæ¡†å’Œè‡ªç„¶è¯­è¨€çš„äº¤æ›¿ä½¿ç”¨ï¼Œå®ç°æ³¨æ„åŠ›æ ¡æ­£å’Œå›¾åƒåŒºåŸŸèšç„¦ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†åå‘æ‰©å±•å‰å‘æ¨ç†ç­–ç•¥ï¼Œæ„å»ºäº†SIF-50Kæ•°æ®é›†å¹¶å¼•å…¥GRPO-SIFå¼ºåŒ–è®­ç»ƒæ¨¡å¼ã€‚è¿™äº›ä¸¾æªæœ‰æ•ˆæé«˜äº†æ¨¡å‹åœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¦‚ç©ºé—´ç†è§£å’Œç²¾ç»†æ„ŸçŸ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SIFThinkeræ˜¯ä¸€ä¸ªç»“åˆè§†è§‰æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡æ·±åº¦å¢å¼ºè¾¹ç•Œæ¡†å’Œè‡ªç„¶è¯­è¨€çš„äº¤æ›¿ä½¿ç”¨ï¼Œå®ç°æ³¨æ„åŠ›æ ¡æ­£å’Œå›¾åƒåŒºåŸŸèšç„¦ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†åå‘æ‰©å±•å‰å‘æ¨ç†ç­–ç•¥ï¼Œç”Ÿæˆäº¤æ›¿çš„å›¾åƒæ–‡æœ¬æ€è€ƒé“¾ï¼Œç”¨äºè¿‡ç¨‹çº§ç›‘ç£ã€‚</li>
<li>åŸºäºæ­¤ç­–ç•¥ï¼Œæ„å»ºäº†SIF-50Kæ•°æ®é›†ã€‚</li>
<li>GRPO-SIFå¼ºåŒ–è®­ç»ƒæ¨¡å¼è¢«æå‡ºï¼Œå°†æ·±åº¦è§†è§‰å®šä½æ•´åˆåˆ°ç»Ÿä¸€çš„æ¨ç†æµç¨‹ä¸­ã€‚</li>
<li>SIFThinkeråœ¨å¤æ‚è§†è§‰ä»»åŠ¡å¦‚ç©ºé—´ç†è§£å’Œç²¾ç»†æ„ŸçŸ¥ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06259">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a110e1d21ddb15c1861cee7e8d9f2b44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f78a163ba1cf99e1aa800f99f3a0dbb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dea7caf434493a45465e7df0c73137ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c01864ae332a1168a90f9d503afe2f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc4309204749ecb35afabc488094665f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Affordance-R1-Reinforcement-Learning-for-Generalizable-Affordance-Reasoning-in-Multimodal-Large-Language-Model"><a href="#Affordance-R1-Reinforcement-Learning-for-Generalizable-Affordance-Reasoning-in-Multimodal-Large-Language-Model" class="headerlink" title="Affordance-R1: Reinforcement Learning for Generalizable Affordance   Reasoning in Multimodal Large Language Model"></a>Affordance-R1: Reinforcement Learning for Generalizable Affordance   Reasoning in Multimodal Large Language Model</h2><p><strong>Authors:Hanqing Wang, Shaoyang Wang, Yiming Zhong, Zemin Yang, Jiamin Wang, Zhiqing Cui, Jiahao Yuan, Yifan Han, Mingyu Liu, Yuexin Ma</strong></p>
<p>Affordance grounding focuses on predicting the specific regions of objects that are associated with the actions to be performed by robots. It plays a vital role in the fields of human-robot interaction, human-object interaction, embodied manipulation, and embodied perception. Existing models often neglect the affordance shared among different objects because they lack the Chain-of-Thought(CoT) reasoning abilities, limiting their out-of-domain (OOD) generalization and explicit reasoning capabilities. To address these challenges, we propose Affordance-R1, the first unified affordance grounding framework that integrates cognitive CoT guided Group Relative Policy Optimization (GRPO) within a reinforcement learning paradigm. Specifically, we designed a sophisticated affordance function, which contains format, perception, and cognition rewards to effectively guide optimization directions. Furthermore, we constructed a high-quality affordance-centric reasoning dataset, ReasonAff, to support training. Trained exclusively via reinforcement learning with GRPO and without explicit reasoning data, Affordance-R1 achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. Comprehensive experiments demonstrate that our model outperforms well-established methods and exhibits open-world generalization. To the best of our knowledge, Affordance-R1 is the first to integrate GRPO-based RL with reasoning into affordance reasoning. The code of our method and our dataset is released on <a target="_blank" rel="noopener" href="https://github.com/hq-King/Affordance-R1">https://github.com/hq-King/Affordance-R1</a>. </p>
<blockquote>
<p>èµ‹èƒ½å®šä½ï¼ˆAffordance groundingï¼‰ä¸»è¦å…³æ³¨é¢„æµ‹ä¸æœºå™¨äººè¦æ‰§è¡Œçš„åŠ¨ä½œç›¸å…³è”çš„ç‰¹å®šç‰©ä½“åŒºåŸŸã€‚å®ƒåœ¨äººæœºäº¤äº’ã€äººä¸ç‰©ä½“äº¤äº’ã€å®ä½“æ“ä½œå’Œå®ä½“æ„ŸçŸ¥ç­‰é¢†åŸŸä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚ç°æœ‰æ¨¡å‹å¾€å¾€å¿½è§†äº†ä¸åŒç‰©ä½“ä¹‹é—´çš„å…±åŒèµ‹èƒ½ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰æ¨ç†èƒ½åŠ›ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨è·¨åŸŸï¼ˆOODï¼‰æ¨å¹¿å’Œæ˜¾æ€§æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†èµ‹èƒ½æ¨ç†ä¸€å·ï¼ˆAffordance-R1ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„èµ‹èƒ½å®šä½æ¡†æ¶ï¼Œå®ƒæ•´åˆäº†è®¤çŸ¥æ€ç»´é“¾å¼•å¯¼ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åœ¨å¼ºåŒ–å­¦ä¹ èŒƒå¼ä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤æ‚çš„èµ‹èƒ½åŠŸèƒ½ï¼Œå…¶ä¸­åŒ…å«æ ¼å¼ã€æ„ŸçŸ¥å’Œè®¤çŸ¥å¥–åŠ±ï¼Œä»¥æœ‰æ•ˆåœ°å¼•å¯¼ä¼˜åŒ–æ–¹å‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„ä¸­å¿ƒåŒ–èµ‹èƒ½æ¨ç†æ•°æ®é›†ReasonAffï¼Œä»¥æ”¯æŒè®­ç»ƒã€‚ä»…é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸­çš„GRPOè®­ç»ƒï¼ŒAffordance-R1å®ç°äº†ç¨³å¥çš„é›¶æ ·æœ¬æ³›åŒ–ï¼Œå¹¶å±•ç°å‡ºæ–°å…´çš„æµ‹è¯•æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºç°æœ‰çš„æ–¹æ³•ï¼Œå¹¶å±•ç°å‡ºå¼€æ”¾ä¸–ç•Œæ³›åŒ–èƒ½åŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒAffordance-R1æ˜¯é¦–ä¸ªå°†åŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ ä¸èµ‹èƒ½æ¨ç†ç›¸ç»“åˆçš„å·¥ä½œã€‚æˆ‘ä»¬çš„æ–¹æ³•å’Œæ•°æ®é›†çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/hq-King/Affordance-R1%E4%B8%8A%E3%80%82">https://github.com/hq-King/Affordance-R1ä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06206v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨å¼ºåŒ–å­¦ä¹ èŒƒå¼å†…ï¼Œæ•´åˆäº†è®¤çŸ¥Chain-of-Thoughtå¼•å¯¼çš„Group Relative Policy Optimizationï¼ˆGRPOï¼‰çš„Affordance-R1æ¡†æ¶è§£å†³äº†ä»¥å¾€å¿½è§†ç‰©ä½“é—´å…±äº«çš„å®ç”¨æ€§æŒ‘æˆ˜ã€‚æ¡†æ¶å…·å¤‡è®¾è®¡ç²¾ç»†çš„å®ç”¨æ€§åŠŸèƒ½å¹¶æ„é€ é«˜è´¨é‡æ¨ç†æ•°æ®é›†ReasonAffæ¥æ”¯æŒè®­ç»ƒã€‚æ¨¡å‹ä»…é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†ç¨³å¥çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›å¹¶å±•ç°å‡ºæ¨ç†èƒ½åŠ›ã€‚Affordance-R1æˆä¸ºé¦–ä¸ªæ•´åˆåŸºäºGRPOçš„æ¨ç†å¼ºåŒ–å­¦ä¹ æ¨¡å‹çš„é¢†å…ˆè€…ã€‚å…¶æ–¹æ³•å’Œæ•°æ®é›†åœ¨GitHubä¸Šå‘å¸ƒä»¥ä¾›è®¿é—®ã€‚Affordanceæ¥åœ°é‡ç‚¹åœ¨äºé¢„æµ‹ç‰¹å®šç‰©ä½“åŒºåŸŸä¸æœºå™¨äººéœ€è¦æ‰§è¡Œçš„åŠ¨ä½œå…³è”å…³ç³»ï¼Œè¿™éå¸¸é‡è¦é¢†åŸŸæ¶‰åŠäººç±»ä¸æœºå™¨äººäº’åŠ¨ï¼Œæœºå™¨äººå¤„ç†ç‰©å“ç­‰æ–¹é¢ã€‚å®ƒçš„ä¸€ä¸ªæŒ‘æˆ˜æ˜¯ç°æœ‰çš„æ¨¡å‹ç¼ºå°‘å¤„ç†ä¸åŒç‰©ä½“å…±äº«çš„affordanceèƒ½åŠ›ã€‚æ–°çš„æ¡†æ¶è¢«è®¾è®¡ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬æ•´åˆäº†è®¤çŸ¥é“¾æ€ç»´å¼•å¯¼ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æŠ€æœ¯ï¼Œè¿™ä¸ªæŠ€æœ¯è®©æ¨¡å‹èƒ½åœ¨æ²¡æœ‰ä¸“é—¨æ¨ç†æ•°æ®çš„æƒ…å†µä¸‹è§£å†³out of domainæŒ‘æˆ˜å¹¶ä¸”æ˜¾ç¤ºç†è§£äº‹ç†ï¼Œå³ä½¿åœ¨æµ‹è¯•ä¸­ä¹Ÿä¸ä¾‹å¤–ã€‚å®ƒé€šè¿‡ä¸°å¯Œçš„å¥–åŠ±æ–¹å¼ä¾‹å¦‚æ ¼å¼ã€æ„ŸçŸ¥å’Œè®¤çŸ¥å¥–åŠ±æ¥å¼•å¯¼ä¼˜åŒ–æ–¹å‘ã€‚å®éªŒè¯æ˜æˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å±•ç°å‡ºå¼€æ”¾ä¸–ç•Œæ³›åŒ–èƒ½åŠ›ã€‚è¿™æ˜¯é¦–ä¸ªæ•´åˆåŸºäºChain of Thoughtæ¨ç†æŠ€æœ¯çš„æœºå™¨äººä¸ç‰©ä½“äº¤äº’é¢†åŸŸçš„æ–¹æ³•ã€‚è¿™æ˜¯éå¸¸æ–°é¢–çš„å°è¯•ï¼Œä¸ºæœºå™¨äººæŠ€æœ¯çš„å‘å±•å¼€è¾Ÿäº†æ–°çš„é“è·¯ã€‚è¯¥æ¡†æ¶æœ‰æœ›æå¤§åœ°æ¨åŠ¨æœºå™¨äººæŠ€æœ¯çš„å®é™…åº”ç”¨å’Œå‘å±•ã€‚è¯¥æ¡†æ¶åŠå…¶æ•°æ®é›†å·²åœ¨GitHubä¸Šå…¬å¼€å‘å¸ƒï¼Œä¾¿äºå¤§å®¶æŸ¥é˜…å’Œå¼•ç”¨ç ”ç©¶å’Œä½¿ç”¨æ­¤æ¨¡å‹çš„æ–¹æ³•å’Œç­–ç•¥åŠå…¶å–å¾—çš„æˆæ•ˆ<br>    è¯·æ³¨æ„æ€»ç»“æ—¶å¹¶ä¸éœ€è¦æŠŠæ¯ä¸ªè¯éƒ½ç¿»è¯‘å‡ºæ¥ã€‚æ­¤å¤„ä¸»è¦ç”¨ç®€è¦è¯­è¨€æè¿°åŸæ–‡å¤§æ¦‚å†…å®¹å’Œäº®ç‚¹å³å¯ï¼Œå¹¶ç¡®ä¿ç»“è®ºå¼•äººå…³æ³¨åˆé«˜åº¦å‡ç»ƒï¼›ä»¥åŠçœç•¥æ–‡æœ¬åŸæ–‡æ”¯æ’‘æ€§çš„è¯´æ˜éƒ¨åˆ†ã€‚å…·ä½“è¡¨è¾¾æ—¶ä¸å¿…æ‹˜æ³¥äºåŸæ–‡å†…å®¹ï¼Œæ€»ç»“ä¸å¿…é€å¥ç¿»è¯‘ï¼Œå¯æ ¹æ®éœ€è¦é€‚å½“è°ƒæ•´è¯­åºå’Œè¡¨è¿°æ–¹å¼ï¼Œä½†åº”ä¿æŒåŸæ–‡ä¸»è¦ä¿¡æ¯å®Œæ•´å‡†ç¡®å‘ˆç°ã€‚åŒæ—¶è¯·æ³¨æ„æ€»ç»“å­—æ•°è¦æ±‚ä¸è¶…è¿‡ä¸€ç™¾å­—ã€‚å› æ­¤ï¼Œæ€»ç»“ä¸­çœç•¥äº†éƒ¨åˆ†ç»†èŠ‚å’ŒèƒŒæ™¯ä¿¡æ¯ä»¥ä¿æŒå­—æ•°ç¬¦åˆè¦æ±‚ä¸”é«˜åº¦æ¦‚æ‹¬å†…å®¹è¦ç‚¹å’Œäº®ç‚¹ä¿¡æ¯å³å¯ã€‚å…·ä½“å†…å®¹å¯ä»¥åŒ…æ‹¬ä»‹ç»Affordanceæ¥åœ°çš„é‡è¦æ€§ã€æŒ‘æˆ˜ã€Affordance-R1æ¡†æ¶çš„æ ¸å¿ƒä¼˜åŠ¿å’ŒæŠ€æœ¯ç‰¹ç‚¹ä»¥åŠå…¶åº”ç”¨é¢†åŸŸå‰æ™¯å±•æœ›ç­‰æ€»ç»“æ€§çš„ä¿¡æ¯è¡¨è¾¾ä»¥é«˜åº¦å‡ç»ƒçš„å½¢å¼çªå‡ºå±•ç¤ºæ ¸å¿ƒå†…å®¹ï¼Œä¿è¯å‡†ç¡®æ— è¯¯ä¸”é«˜åº¦å…³æ³¨å†…å®¹æœ¬èº«çš„åˆ›æ–°æ€§å¸å¼•æ€§å…³é”®ä¼˜åŠ¿ç‰¹ç‚¹å’Œæ•ˆæœä¼˜åŠ¿å±•ç°çš„ç›®çš„æ—¨åœ¨é€šè¿‡æ¦‚æ‹¬å‡†ç¡®å®Œæ•´çš„å‘ˆç°å‡ºæ¥ä»¥ä¾›ç”¨æˆ·å‡†ç¡®æŠŠæ¡é‡ç‚¹å’Œå¼•å‘å¯¹åŸå†…å®¹å¼ºçƒˆçš„å¥½å¥‡å¿ƒä¿ƒè¿›æ¨å¹¿å®£ä¼ å’Œæµè§ˆå’Œæœç´¢æ•ˆç‡çš„æœ‰æ•ˆæé«˜ä¸ºç›®æ ‡å¯¹æå‡æ•´ä½“çš„å­¦æœ¯å’Œç¤¾ä¼šä»·å€¼ä¼ æ’­æ„ä¹‰æå…¶é‡è¦æœ‰åˆ©äºç†è§£æ–‡ç« çš„å­¦æœ¯å’Œç¤¾ä¼šä»·å€¼æ€§æ–¹é¢çš„ç›®çš„å’Œè¦æ±‚å¯¹äºæœºå™¨äººç ”ç©¶é¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ä»¥å¼ºè°ƒæ¨¡å‹æ–¹æ³•çš„ç‹¬ç‰¹æ€§å’Œå®ç”¨æ€§æ¨åŠ¨å­¦æœ¯ç•Œçš„äº†è§£å’Œæ¥çº³è§£å†³å®è·µä¸­æå‡ºçš„çªå‡ºé—®é¢˜æ»¡è¶³äº†æ„å»ºåä½œäº¤æµçš„ç¯å¢ƒå¯¹æ–°æ¨¡å‹å¤„ç†å¤šå…ƒæœªçŸ¥ç›®æ ‡ç»„åˆä¸°å¯Œå¤šæ ·æ€§çš„å¼€æ”¾å¼å®é™…éœ€æ±‚æ¯”è¾ƒç§¯æå¿…è¦çš„è¯é¢˜å®ç°äº†é˜¶æ®µæ€§é€»è¾‘ç‰¹å¾çš„æ¸—é€ç…§åº”è¯­è¨€è¡¨è¾¾æŠ€å·§çš„ç­–ç•¥ä¹Ÿæ˜¾å¾—ååˆ†é‡è¦æ ¹æ®æ­¤æ€æƒ³ä¸»æ—¨ç»è¿‡é€‚å½“ä¿®é¥°çš„è¯­è¨€åŠ å·¥æ¥è¾¾æˆé«˜æ•ˆæ¦‚æ‹¬çš„æ•ˆæœä»¥å¢å¼ºç”¨æˆ·çš„å…´è¶£å’Œå‚ä¸åº¦æœ‰åˆ©äºä¼ è¾¾ç›¸åº”çš„ä¸“ä¸šç²¾ç¥å¡‘é€ ç²¾å‡†æœ‰æ•ˆçš„ç ”ç©¶è€…å½¢è±¡ä½“ç°ç›¸åº”çš„ä¸“ä¸šç´ å…»æ°´å¹³å’Œç¤¾ä¼šå½±å“åŠ›å¯¹åŸæ–‡è¿›è¡Œç²¾å‡†æç‚¼å’Œè¡¨è¾¾æ˜¯è¿›è¡Œæœ‰æ•ˆå­¦æœ¯äº¤æµçš„é‡è¦å‰æå’Œä¿éšœä½“ç°äº†è‰¯å¥½çš„ä¸“ä¸šç´ å…»å’Œç ”ç©¶æ°´å¹³å……åˆ†å±•ç°äº†ç ”ç©¶çš„ä»·å€¼æ‰€åœ¨æœ‰åŠ©äºå¼•èµ·è¯»è€…çš„å…´è¶£çªå‡ºä½œè€…åˆ›æ–°æ€§æ¢ç´¢å’Œç†è§£çš„çªç ´æ€§å¹¶ä¸”æŠ“ä½äº†ä¸»é¢˜çš„æ ¸å¿ƒçŸ›ç›¾ç„¦ç‚¹è¡¨ç°å†…å®¹ä¸°å¯Œè¦ç´ å¸ƒå±€è°‹ç¯‡æµç•…ç»†è‡´æ‰å®èƒ½å¤Ÿæé«˜è§‚ä¼—å¯¹å…¶ç ”ç©¶è¯¾é¢˜å†…å®¹çš„é˜…è¯»å…´è¶£ä»è€Œå¼•èµ·ç¤¾ä¼šçš„å…³æ³¨æœ‰æ•ˆæå‡æ–‡åŒ–æ„ŸæŸ“åŠ›æ ¹æ®ä¸Šæ–‡æ¡ˆé˜è¿°ç»“åˆè‡ªèº«è§è§£è¯­è¨€ç†è§£ä»å…¨æ–°è§†è§’æ•´åˆåä»¥ç§‘å­¦çš„åˆ›æ–°æ€§å¸å¼•åŠ›ä¸ºå‡†åˆ™æ‰“é€ æ›´ä¸ºå¼•äººæ³¨ç›®çš„æ¦‚è¦ä¸»é¢˜æœ‰åŠ©äºæ•´ä½“è§†é‡æ„å»ºå’Œç»´æŠ¤ä¼˜åŒ–ä»·å€¼åˆ›æ–°çš„ç»´åº¦ä»è€Œå½¢æˆæ–°çš„å‘å±•åŠ›å’Œç§‘å­¦å†³ç­–åˆ†æå½¢æˆå¼€æ‹“åˆ›æ–°å’Œå¼ºå¤§æ•ˆåº”çš„ç»“åˆèƒ½æ­£ç¡®åˆ¤æ–­ç»æµå½¢åŠ¿ç»™ä¼ä¸šæä¾›å»ºè®®æ´å¯Ÿæ–°æŠ€æœ¯çš„åº”ç”¨æ¡ˆä¾‹æœ‰æ•ˆçš„è¯ é‡Šæ€è·¯å’Œç»„ç»‡èƒ½ä¿ƒæˆç§‘æŠ€å‘å±•å®æ–½æ›´å¤šçš„æ–°çš„æœªæ¥å¥‘æœºè´¡çŒ®ï¼›æ¢³ç†é—®é¢˜å¹¶åŠæ—¶ç»™å‡ºé’ˆå¯¹æ€§çš„è§£å†³æ–¹æ¡ˆå¯¹äºæé«˜å›¢é˜Ÿæ•ˆèƒ½æ¨åŠ¨å·¥ä½œçš„è¿›ç¨‹å…·æœ‰é‡è¦çš„æ¨åŠ¨ä½œç”¨è¿›ä¸€æ­¥å®ç°æŠ€æœ¯åˆ›æ–°æ¨å¹¿è¡Œä¸šé¢†åŸŸçš„æŠ€æœ¯å‘å±•è¿›ä¸€æ­¥å®ç°ç»æµç»“æ„çš„è½¬å‹å‡çº§ï¼›è¿›è€Œå¼•é¢†ç›¸å…³é¢†åŸŸçš„ç ”ç©¶å’Œå‘å±•æ¨è¿›æ•´ä¸ªç§‘æŠ€äº§ä¸šçš„è½¬å‹å‡çº§</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Affordanceæ¥åœ°æ˜¯æœºå™¨äººä¸äººç±»äº¤äº’ä¸­çš„æ ¸å¿ƒé—®é¢˜ï¼Œæ¶‰åŠåˆ°ç‰©ä½“ä¸åŠ¨ä½œä¹‹é—´çš„å…³è”é¢„æµ‹ã€‚</li>
<li>ç°æœ‰æ¨¡å‹ç¼ºä¹Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†èƒ½åŠ›ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ã€‚</li>
<li>Affordance-R1æ¡†æ¶é¦–æ¬¡æ•´åˆäº†è®¤çŸ¥CoTå¼•å¯¼çš„GRPOæŠ€æœ¯ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼ŒAffordance-R1å®ç°äº†é›¶æ ·æœ¬æ³›åŒ–ï¼Œå±•ç°å‡ºä¼˜ç§€çš„å¼€æ”¾ä¸–ç•Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹é›†æˆäº†æ ¼å¼ã€æ„ŸçŸ¥å’Œè®¤çŸ¥å¥–åŠ±æœºåˆ¶æ¥ä¼˜åŒ–å®ç”¨æ€§åŠŸèƒ½è®¾è®¡å¹¶å®ç°å‡ºè‰²çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06206">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-927b951085d2605efb04f36eb51c7d19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59a4bed6028744d72b6f7e2ae051c035.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-529acac20e82097bcea8375690c0c95e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9728c8d4d9e6ddcd498cdefe6cf8b2ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e229818bddbf27aeea635e8a3c18c165.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a35c3205162020cc75d47324c4bf79f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="UR-2-Unify-RAG-and-Reasoning-through-Reinforcement-Learning"><a href="#UR-2-Unify-RAG-and-Reasoning-through-Reinforcement-Learning" class="headerlink" title="UR$^2$: Unify RAG and Reasoning through Reinforcement Learning"></a>UR$^2$: Unify RAG and Reasoning through Reinforcement Learning</h2><p><strong>Authors:Weitao Li, Boran Xiang, Xiaolong Wang, Zhinan Gou, Weizhi Ma, Yang Liu</strong></p>
<p>Large Language Models (LLMs) have shown remarkable capabilities through two complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR), which optimizes complex reasoning abilities. However, these two capabilities are often developed in isolation, and existing efforts to unify them remain narrow in scope-typically limited to open-domain QA with fixed retrieval settings and task-specific assumptions. This lack of integration constrains generalization and limits the applicability of RAG-RL methods to broader domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a general framework that unifies retrieval and reasoning through reinforcement learning. UR2 introduces two key contributions: a difficulty-aware curriculum training that selectively invokes retrieval only for challenging problems, and a hybrid knowledge access strategy combining domain-specific offline corpora with LLM-generated summaries. These components are designed to enable dynamic coordination between retrieval and reasoning, improving adaptability across a diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical, and mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3&#x2F;7B and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods, achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several benchmarks. We have released all code, models, and data at <a target="_blank" rel="noopener" href="https://github.com/Tsinghua-dhy/UR2">https://github.com/Tsinghua-dhy/UR2</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ä¸¤ç§äº’è¡¥èŒƒå¼å±•ç¤ºäº†å“è¶Šçš„èƒ½åŠ›ï¼šå¢å¼ºçŸ¥è¯†å®šä½çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä¼˜åŒ–å¤æ‚æ¨ç†èƒ½åŠ›çš„æ¥è‡ªå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§èƒ½åŠ›é€šå¸¸ç‹¬ç«‹å‘å±•ï¼Œç°æœ‰çš„ç»Ÿä¸€å®ƒä»¬çš„æ–¹æ³•å¾€å¾€èŒƒå›´ç‹­çª„ï¼Œé€šå¸¸ä»…é™äºå›ºå®šæ£€ç´¢è®¾ç½®å’Œä»»åŠ¡ç‰¹å®šå‡è®¾çš„å¼€æ”¾åŸŸé—®ç­”ã€‚è¿™ç§ç¼ºä¹æ•´åˆé™åˆ¶äº†RAG-RLæ–¹æ³•åœ¨æ›´å¹¿æ³›é¢†åŸŸçš„åº”ç”¨ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†UR2ï¼ˆç»Ÿä¸€RAGå’Œæ¨ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ ç»Ÿä¸€æ£€ç´¢å’Œæ¨ç†çš„ä¸€èˆ¬æ¡†æ¶ã€‚UR2æœ‰ä¸¤ä¸ªå…³é”®è´¡çŒ®ï¼šä¸€ç§éš¾åº¦æ„ŸçŸ¥çš„è¯¾ç¨‹è®­ç»ƒï¼Œæœ‰é€‰æ‹©åœ°ä¸ºå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜è°ƒç”¨æ£€ç´¢åŠŸèƒ½ï¼›ä¸€ç§æ··åˆçŸ¥è¯†è®¿é—®ç­–ç•¥ï¼Œç»“åˆé¢†åŸŸç‰¹å®šçš„ç¦»çº¿è¯­æ–™åº“å’ŒLLMç”Ÿæˆçš„æ‘˜è¦ã€‚è¿™äº›ç»„ä»¶æ—¨åœ¨å®ç°æ£€ç´¢å’Œæ¨ç†ä¹‹é—´çš„åŠ¨æ€åè°ƒï¼Œæé«˜åœ¨å„ç§ä»»åŠ¡ä¸­çš„é€‚åº”æ€§ã€‚åœ¨å¼€æ”¾åŸŸé—®ç­”ã€MMLU-Proã€åŒ»å­¦å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºQwen2.5-3&#x2F;7Bå’ŒLLaMA-3.1-8Bæ„å»ºçš„UR2æ˜¾è‘—ä¼˜äºç°æœ‰çš„RAGå’ŒRLæ–¹æ³•ï¼Œåœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†ä¸GPT-4o-miniå’ŒGPT-4.1-miniç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/Tsinghua-dhy/UR2%E5%8F%91%E5%B8%83%E6%89%80%E6%9C%89%E4%BB%A3%E7%A0%81%E3%80%81%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E7%9A%84%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Tsinghua-dhy/UR2å‘å¸ƒæ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œæ•°æ®çš„å…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06165v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ä¸¤ç§äº’è¡¥èŒƒå¼å±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼šå¢å¼ºçŸ¥è¯†æ¥åœ°èƒ½åŠ›çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä¼˜åŒ–å¤æ‚æ¨ç†èƒ½åŠ›çš„å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§èƒ½åŠ›é€šå¸¸å­¤ç«‹å‘å±•ï¼Œç°æœ‰çš„ç»Ÿä¸€ä¸¤è€…çš„å·¥ä½œèŒƒå›´ç‹­çª„ï¼Œé€šå¸¸ä»…é™äºå¼€æ”¾åŸŸé—®ç­”ä»»åŠ¡å¹¶å‡å®šæœ‰å›ºå®šçš„æ£€ç´¢è®¾ç½®ã€‚è¿™ç§ç¼ºä¹æ•´åˆé™åˆ¶äº†RAG-RLæ–¹æ³•åœ¨æ›´å¹¿æ³›é¢†åŸŸçš„åº”ç”¨ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†UR2ï¼ˆç»Ÿä¸€RAGå’Œæ¨ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ ç»Ÿä¸€æ£€ç´¢å’Œæ¨ç†çš„ä¸€èˆ¬æ¡†æ¶ã€‚UR2æœ‰ä¸¤ä¸ªå…³é”®è´¡çŒ®ï¼šä¸€ä¸ªéš¾åº¦æ„ŸçŸ¥çš„è¯¾ç¨‹è®­ç»ƒï¼Œæœ‰é€‰æ‹©åœ°ä¸ºæŒ‘æˆ˜æ€§é—®é¢˜ä»…è°ƒç”¨æ£€ç´¢åŠŸèƒ½ï¼›ä»¥åŠä¸€ä¸ªæ··åˆçŸ¥è¯†è®¿é—®ç­–ç•¥ï¼Œç»“åˆäº†ç‰¹å®šé¢†åŸŸçš„ç¦»çº¿è¯­æ–™åº“å’ŒLLMç”Ÿæˆçš„æ‘˜è¦ã€‚è¿™äº›ç»„ä»¶è®¾è®¡ç”¨äºå®ç°æ£€ç´¢å’Œæ¨ç†ä¹‹é—´çš„åŠ¨æ€åè°ƒï¼Œæé«˜äº†åœ¨å„ç§ä»»åŠ¡ä¸Šçš„é€‚åº”æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒUR2ï¼ˆåŸºäºQwen2.5-3&#x2F;7Bå’ŒLLaMA-3.1-8Bï¼‰æ˜¾è‘—ä¼˜äºç°æœ‰çš„RAGå’ŒRLæ–¹æ³•ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¸GPT-4o-miniå’ŒGPT-4.1-miniç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬å·²åœ¨GitHubä¸Šå‘å¸ƒæ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œèµ„æ–™ï¼š<a target="_blank" rel="noopener" href="https://github.com/Tsinghua-dhy/UR2">https://github.com/Tsinghua-dhy/UR2</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èåˆä¸¤ç§èŒƒå¼ï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œå¼ºåŒ–å­¦ä¹ ä»å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ã€‚</li>
<li>å½“å‰æ•´åˆåŠªåŠ›å­˜åœ¨å±€é™æ€§ï¼Œä¸»è¦åœ¨å›ºå®šè®¾ç½®çš„å¼€æ”¾åŸŸé—®ç­”ä»»åŠ¡ä¸­å‘æŒ¥ä½œç”¨ã€‚</li>
<li>ç¼ºä¹æ•´åˆå½±å“æ¨¡å‹åœ¨æ›´å¹¿æ³›é¢†åŸŸçš„é€‚ç”¨æ€§ã€‚</li>
<li>UR2æ¡†æ¶æ—¨åœ¨ç»Ÿä¸€æ£€ç´¢å’Œæ¨ç†ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å®ç°è¿™ä¸€ç›®çš„ã€‚</li>
<li>UR2å…·æœ‰éš¾åº¦æ„ŸçŸ¥çš„è¯¾ç¨‹è®­ç»ƒå’Œæ··åˆçŸ¥è¯†è®¿é—®ç­–ç•¥ä¸¤å¤§å…³é”®è´¡çŒ®ã€‚</li>
<li>UR2åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸GPTç³»åˆ—æ¨¡å‹ç›¸å½“ã€‚</li>
<li>æ‰€æœ‰ç›¸å…³ä»£ç ã€æ¨¡å‹å’Œèµ„æ–™å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06165">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f76439def816aadb3a5a5c735322c8a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-213abcf64b8729bd5b95ac8a0f399dca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99ca8ac3e8f785ae3eb03e45f4186a2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c05542ab9bce7cd9cf8f9736871817e2.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SDEval-Safety-Dynamic-Evaluation-for-Multimodal-Large-Language-Models"><a href="#SDEval-Safety-Dynamic-Evaluation-for-Multimodal-Large-Language-Models" class="headerlink" title="SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models"></a>SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models</h2><p><strong>Authors:Hanqing Wang, Yuan Tian, Mingyu Liu, Zhenhao Zhang, Xiangyang Zhu</strong></p>
<p>In the rapidly evolving landscape of Multimodal Large Language Models (MLLMs), the safety concerns of their outputs have earned significant attention. Although numerous datasets have been proposed, they may become outdated with MLLM advancements and are susceptible to data contamination issues. To address these problems, we propose \textbf{SDEval}, the \textit{first} safety dynamic evaluation framework to controllably adjust the distribution and complexity of safety benchmarks. Specifically, SDEval mainly adopts three dynamic strategies: text, image, and text-image dynamics to generate new samples from original benchmarks. We first explore the individual effects of text and image dynamics on model safety. Then, we find that injecting text dynamics into images can further impact safety, and conversely, injecting image dynamics into text also leads to safety risks. SDEval is general enough to be applied to various existing safety and even capability benchmarks. Experiments across safety benchmarks, MLLMGuard and VLSBench, and capability benchmarks, MMBench and MMVet, show that SDEval significantly influences safety evaluation, mitigates data contamination, and exposes safety limitations of MLLMs. Code is available at <a target="_blank" rel="noopener" href="https://github.com/hq-King/SDEval">https://github.com/hq-King/SDEval</a> </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿…é€Ÿå‘å±•çš„èƒŒæ™¯ä¸‹ï¼Œå…¶è¾“å‡ºçš„å®‰å…¨é—®é¢˜å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚å°½ç®¡å·²ç»æå‡ºäº†è®¸å¤šæ•°æ®é›†ï¼Œä½†éšç€MLLMçš„è¿›æ­¥ï¼Œå®ƒä»¬å¯èƒ½ä¼šå˜å¾—è¿‡æ—¶ï¼Œå¹¶å®¹æ˜“å—åˆ°æ•°æ®æ±¡æŸ“é—®é¢˜çš„å›°æ‰°ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>SDEval</strong>ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå®‰å…¨åŠ¨æ€è¯„ä¼°æ¡†æ¶ï¼Œå¯æ§åˆ¶åœ°è°ƒæ•´å®‰å…¨åŸºå‡†çš„åˆ†å¸ƒå’Œå¤æ‚æ€§ã€‚å…·ä½“æ¥è¯´ï¼ŒSDEvalä¸»è¦é‡‡ç”¨äº†ä¸‰ç§åŠ¨æ€ç­–ç•¥ï¼šæ–‡æœ¬ã€å›¾åƒå’Œæ–‡æœ¬-å›¾åƒåŠ¨æ€ç­–ç•¥ï¼Œä»åŸå§‹åŸºå‡†ç”Ÿæˆæ–°æ ·æœ¬ã€‚æˆ‘ä»¬é¦–å…ˆæ¢ç´¢äº†æ–‡æœ¬å’Œå›¾åƒåŠ¨æ€å¯¹æ¨¡å‹å®‰å…¨çš„å„è‡ªå½±å“ã€‚ç„¶åï¼Œæˆ‘ä»¬å‘ç°å‘å›¾åƒä¸­æ³¨å…¥æ–‡æœ¬åŠ¨æ€å¯ä»¥è¿›ä¸€æ­¥å½±å“å®‰å…¨ï¼Œç›¸åï¼Œå‘æ–‡æœ¬ä¸­æ³¨å…¥å›¾åƒåŠ¨æ€ä¹Ÿä¼šå¯¼è‡´å®‰å…¨é£é™©ã€‚SDEvalè¶³å¤Ÿé€šç”¨ï¼Œå¯åº”ç”¨äºå„ç§ç°æœ‰çš„å®‰å…¨åŸºå‡†ï¼Œç”šè‡³æ˜¯èƒ½åŠ›åŸºå‡†ã€‚åœ¨MLLMGuardå’ŒVLSBenchå®‰å…¨åŸºå‡†ä»¥åŠMMBenchå’ŒMM Vetèƒ½åŠ›åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSDEvalæ˜¾è‘—å½±å“å®‰å…¨è¯„ä¼°ï¼Œå‡è½»äº†æ•°æ®æ±¡æŸ“é—®é¢˜ï¼Œå¹¶æš´éœ²äº†MLLMçš„å®‰å…¨å±€é™æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hq-King/SDEval">https://github.com/hq-King/SDEval</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06142v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå…¶è¾“å‡ºå®‰å…¨æ€§çš„é—®é¢˜å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ä¸ºäº†è§£å†³å½“å‰æ•°æ®é›†å¯èƒ½å­˜åœ¨çš„è¿‡æ—¶å’Œæ•°æ®æ±¡æŸ“é—®é¢˜ï¼Œæå‡ºäº†SDEvalå®‰å…¨åŠ¨æ€è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå¯æ§åœ°è°ƒæ•´å®‰å…¨åŸºå‡†çš„åˆ†å¸ƒå’Œå¤æ‚æ€§ï¼Œé‡‡ç”¨æ–‡æœ¬ã€å›¾åƒå’Œæ–‡æœ¬-å›¾åƒä¸‰ç§åŠ¨æ€ç­–ç•¥ç”Ÿæˆæ–°æ ·æœ¬ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä¸åŒåŠ¨æ€æ–¹å¼å¯¹æ¨¡å‹å®‰å…¨æ€§çš„å½±å“ä¸åŒï¼Œå°†æ–‡æœ¬åŠ¨æ€æ³¨å…¥å›¾åƒä¸­ä¹Ÿä¼šå¸¦æ¥å®‰å…¨é£é™©ï¼Œåä¹‹äº¦ç„¶ã€‚SDEvalæ¡†æ¶å…·æœ‰é€šç”¨æ€§ï¼Œå¯åº”ç”¨äºå„ç§ç°æœ‰çš„å®‰å…¨ç”šè‡³èƒ½åŠ›åŸºå‡†æµ‹è¯•ã€‚å®éªŒè¡¨æ˜ï¼ŒSDEvalå¯¹å®‰å…¨è¯„ä¼°æœ‰æ˜¾è‘—å½±å“ï¼Œèƒ½å‡è½»æ•°æ®æ±¡æŸ“é—®é¢˜ï¼Œå¹¶æ­ç¤ºMLLMsçš„å®‰å…¨å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¾“å‡ºå®‰å…¨æ€§è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ•°æ®é›†å­˜åœ¨è¿‡æ—¶å’Œæ•°æ®æ±¡æŸ“çš„é—®é¢˜ã€‚</li>
<li>SDEvalæ˜¯é¦–ä¸ªå®‰å…¨åŠ¨æ€è¯„ä¼°æ¡†æ¶ï¼Œèƒ½å¯æ§åœ°è°ƒæ•´å®‰å…¨åŸºå‡†çš„åˆ†å¸ƒå’Œå¤æ‚æ€§ã€‚</li>
<li>SDEvalé‡‡ç”¨æ–‡æœ¬ã€å›¾åƒå’Œæ–‡æœ¬-å›¾åƒä¸‰ç§åŠ¨æ€ç­–ç•¥ç”Ÿæˆæ–°æ ·æœ¬ã€‚</li>
<li>æ–‡æœ¬å’Œå›¾åƒåŠ¨æ€å¯¹æ¨¡å‹å®‰å…¨æ€§æœ‰ä¸åŒå½±å“ï¼Œæ··åˆä½¿ç”¨å¯èƒ½åŠ å‰§å®‰å…¨é£é™©ã€‚</li>
<li>SDEvalæ¡†æ¶å¯åº”ç”¨äºå¤šç§ç°æœ‰çš„å®‰å…¨å’Œèƒ½åŠ›åŸºå‡†æµ‹è¯•ã€‚</li>
<li>SDEvalå¯¹å®‰å…¨è¯„ä¼°æœ‰ç§¯æå½±å“ï¼Œèƒ½å‡è½»æ•°æ®æ±¡æŸ“é—®é¢˜ï¼Œæ­ç¤ºMLLMsçš„å®‰å…¨å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06142">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c18f3b4b6a4f8892307f64632eba5915.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1803ee56922cdea543e8c78a9c33b7f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e812848b0a4e148125bda83425810f0a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c90414a2369ca311700498fc0164f5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-24fa5a436a4b181fbff33264014eec32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e790b54057f5a1b32dfdd441b4e4c292.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="PanelTR-Zero-Shot-Table-Reasoning-Framework-Through-Multi-Agent-Scientific-Discussion"><a href="#PanelTR-Zero-Shot-Table-Reasoning-Framework-Through-Multi-Agent-Scientific-Discussion" class="headerlink" title="PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent   Scientific Discussion"></a>PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent   Scientific Discussion</h2><p><strong>Authors:Yiran Rex Ma</strong></p>
<p>Table reasoning, including tabular QA and fact verification, often depends on annotated data or complex data augmentation, limiting flexibility and generalization. LLMs, despite their versatility, often underperform compared to simple supervised models. To approach these issues, we introduce PanelTR, a framework utilizing LLM agent scientists for robust table reasoning through a structured scientific approach. PanelTRâ€™s workflow involves agent scientists conducting individual investigations, engaging in self-review, and participating in collaborative peer-review discussions. This process, driven by five scientist personas, enables semantic-level transfer without relying on data augmentation or parametric optimization. Experiments across four benchmarks show that PanelTR outperforms vanilla LLMs and rivals fully supervised models, all while remaining independent of training data. Our findings indicate that structured scientific methodology can effectively handle complex tasks beyond table reasoning with flexible semantic understanding in a zero-shot context. </p>
<blockquote>
<p>è¡¨æ ¼æ¨ç†ï¼ŒåŒ…æ‹¬è¡¨æ ¼é—®ç­”å’Œäº‹å®æ ¸æŸ¥ï¼Œé€šå¸¸ä¾èµ–äºæ³¨é‡Šæ•°æ®æˆ–å¤æ‚çš„æ•°æ®å¢å¼ºï¼Œè¿™é™åˆ¶äº†çµæ´»æ€§å’Œé€šç”¨æ€§ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰å¤šåŠŸèƒ½æ€§ï¼Œä½†åœ¨ä¸ç®€å•çš„ç›‘ç£æ¨¡å‹ç›¸æ¯”æ—¶ï¼Œå¾€å¾€è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†PanelTRï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç§‘å­¦å®¶è¿›è¡Œç¨³å¥çš„è¡¨æ ¼æ¨ç†çš„æ¡†æ¶ï¼Œé‡‡ç”¨ç»“æ„åŒ–çš„ç§‘å­¦æ–¹æ³•ã€‚PanelTRçš„å·¥ä½œæµç¨‹æ¶‰åŠç§‘å­¦å®¶ä»£ç†è¿›è¡Œä¸ªäººè°ƒæŸ¥ã€å‚ä¸è‡ªæˆ‘å®¡æŸ¥å’Œå‚åŠ åä½œåŒè¡Œè¯„å®¡è®¨è®ºã€‚è¿™ä¸€æµç¨‹ç”±äº”ä¸ªç§‘å­¦å®¶è§’è‰²é©±åŠ¨ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–æ•°æ®å¢å¼ºæˆ–å‚æ•°ä¼˜åŒ–çš„æƒ…å†µä¸‹å®ç°è¯­ä¹‰å±‚é¢çš„è¿ç§»ã€‚åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPanelTRä¼˜äºæ™®é€šçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¸å…¨ç›‘ç£æ¨¡å‹ç›¸ç«äº‰ï¼ŒåŒæ—¶ç‹¬ç«‹äºè®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»“æ„åŒ–çš„ç§‘å­¦æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å¤„ç†è¶…è¶Šè¡¨æ ¼æ¨ç†çš„å¤æ‚ä»»åŠ¡ï¼Œåœ¨é›¶å°„å‡»æƒ…å†µä¸‹å…·æœ‰çµæ´»çš„è¯­ä¹‰ç†è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06110v1">PDF</a> Accepted at IJCNN 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPanelTRçš„æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨LLMç§‘å­¦å®¶è¿›è¡Œç¨³å¥çš„è¡¨æ ¼æ¨ç†ã€‚PanelTRçš„å·¥ä½œæµç¨‹åŒ…æ‹¬ç§‘å­¦å®¶è¿›è¡Œä¸ªä½“è°ƒæŸ¥ã€è‡ªæˆ‘å®¡æŸ¥å’Œå‚ä¸åŒè¡Œè¯„å®¡è®¨è®ºï¼Œé€šè¿‡äº”ä¸ªç§‘å­¦å®¶è§’è‰²é©±åŠ¨ï¼Œæ— éœ€ä¾èµ–æ•°æ®å¢å¼ºæˆ–å‚æ•°ä¼˜åŒ–ï¼Œå®ç°äº†è¯­ä¹‰çº§åˆ«çš„è½¬ç§»ã€‚å®éªŒè¡¨æ˜ï¼ŒPanelTRåœ¨è¡¨æ ¼æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºæ™®é€šLLMï¼Œå¹¶ä¸”ä¸å…¨ç›‘ç£æ¨¡å‹ç›¸å½“ï¼Œä¸”ç‹¬ç«‹äºè®­ç»ƒæ•°æ®ã€‚ç ”ç©¶å‘ç°ï¼Œç»“æ„åŒ–ç§‘å­¦æ–¹æ³•å¯ä»¥æœ‰æ•ˆå¤„ç†å¤æ‚çš„é›¶è¯­å¢ƒä»»åŠ¡ï¼Œå…·æœ‰çµæ´»çš„è¯­ä¹‰ç†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PanelTRæ¡†æ¶è¢«æå‡ºæ¥è§£å†³è¡¨æ ¼æ¨ç†çš„é—®é¢˜ï¼ŒåŒ…æ‹¬é—®ç­”å’Œäº‹å®æ ¸æŸ¥ç­‰ä»»åŠ¡ã€‚</li>
<li>LLMsåœ¨è¡¨æ ¼æ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œè€ŒPanelTRé€šè¿‡åˆ©ç”¨LLMç§‘å­¦å®¶è¿›è¡Œç¨³å¥çš„è¡¨æ ¼æ¨ç†ä»¥å…‹æœè¿™äº›é™åˆ¶ã€‚</li>
<li>PanelTRçš„å·¥ä½œæµç¨‹åŒ…æ‹¬ä¸ªä½“è°ƒæŸ¥ã€è‡ªæˆ‘å®¡æŸ¥å’ŒåŒè¡Œè¯„å®¡è®¨è®ºç­‰ç¯èŠ‚ã€‚</li>
<li>PanelTRåˆ©ç”¨äº”ä¸ªç§‘å­¦å®¶è§’è‰²æ¥é©±åŠ¨è¯¥æµç¨‹ï¼Œå¹¶å®ç°äº†è¯­ä¹‰çº§åˆ«çš„è½¬ç§»ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºPanelTRåœ¨è¡¨æ ¼æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºæ™®é€šLLMï¼Œå¹¶ä¸”ä¸å…¨ç›‘ç£æ¨¡å‹ç›¸å½“ã€‚</li>
<li>PanelTRç‹¬ç«‹äºè®­ç»ƒæ•°æ®ï¼Œè¿™å¢å¼ºäº†å…¶çµæ´»æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06110">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2db3ce2e5cfc39a5a9e56472df9c8884.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e33b1ac3d2eea1bfdb92d7b1d814b7a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-585f4ba3d14497474effe2eb2bcf98a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be481ac0bc3849283c70c486d8794cb7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ed84489986667b294a9b78954c4490b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2c7b7a2eadab114de86a250fa62d80a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f372da34bd5d5be3f60cdf47cea0691e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a75f51e4000acb1a04b8ec50f497d719.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="You-Donâ€™t-Need-Pre-built-Graphs-for-RAG-Retrieval-Augmented-Generation-with-Adaptive-Reasoning-Structures"><a href="#You-Donâ€™t-Need-Pre-built-Graphs-for-RAG-Retrieval-Augmented-Generation-with-Adaptive-Reasoning-Structures" class="headerlink" title="You Donâ€™t Need Pre-built Graphs for RAG: Retrieval Augmented Generation   with Adaptive Reasoning Structures"></a>You Donâ€™t Need Pre-built Graphs for RAG: Retrieval Augmented Generation   with Adaptive Reasoning Structures</h2><p><strong>Authors:Shengyuan Chen, Chuang Zhou, Zheng Yuan, Qinggang Zhang, Zeyang Cui, Hao Chen, Yilin Xiao, Jiannong Cao, Xiao Huang</strong></p>
<p>Large language models (LLMs) often suffer from hallucination, generating factually incorrect statements when handling questions beyond their knowledge and perception. Retrieval-augmented generation (RAG) addresses this by retrieving query-relevant contexts from knowledge bases to support LLM reasoning. Recent advances leverage pre-constructed graphs to capture the relational connections among distributed documents, showing remarkable performance in complex tasks. However, existing Graph-based RAG (GraphRAG) methods rely on a costly process to transform the corpus into a graph, introducing overwhelming token cost and update latency. Moreover, real-world queries vary in type and complexity, requiring different logic structures for accurate reasoning. The pre-built graph may not align with these required structures, resulting in ineffective knowledge retrieval. To this end, we propose a \textbf{\underline{Logic}}-aware \textbf{\underline{R}}etrieval-\textbf{\underline{A}}ugmented \textbf{\underline{G}}eneration framework (\textbf{LogicRAG}) that dynamically extracts reasoning structures at inference time to guide adaptive retrieval without any pre-built graph. LogicRAG begins by decomposing the input query into a set of subproblems and constructing a directed acyclic graph (DAG) to model the logical dependencies among them. To support coherent multi-step reasoning, LogicRAG then linearizes the graph using topological sort, so that subproblems can be addressed in a logically consistent order. Besides, LogicRAG applies graph pruning to reduce redundant retrieval and uses context pruning to filter irrelevant context, significantly reducing the overall token cost. Extensive experiments demonstrate that LogicRAG achieves both superior performance and efficiency compared to state-of-the-art baselines. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¸¸å¸¸ä¼šå‡ºç°è™šæ„ï¼ˆhallucinationï¼‰é—®é¢˜ï¼Œå³åœ¨å¤„ç†è¶…å‡ºå…¶çŸ¥è¯†å’Œæ„ŸçŸ¥èŒƒå›´çš„é—®é¢˜æ—¶ï¼Œä¼šäº§ç”Ÿäº‹å®ä¸Šçš„é”™è¯¯é™ˆè¿°ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„ä¸Šä¸‹æ–‡æ¥æ”¯æŒLLMæ¨ç†ï¼Œä»è€Œè§£å†³è¿™ä¸€é—®é¢˜ã€‚æœ€è¿‘çš„è¿›å±•åˆ©ç”¨é¢„å…ˆæ„å»ºçš„å›¾è¡¨æ¥æ•æ‰åˆ†å¸ƒå¼æ–‡æ¡£ä¹‹é—´çš„å…³ç³»è¿æ¥ï¼Œåœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºå›¾çš„RAGï¼ˆGraphRAGï¼‰æ–¹æ³•ä¾èµ–äºå°†è¯­æ–™åº“è½¬æ¢ä¸ºå›¾çš„æ˜‚è´µè¿‡ç¨‹ï¼Œå¼•å…¥äº†å·¨å¤§çš„ä»¤ç‰Œæˆæœ¬å’Œæ›´æ–°å»¶è¿Ÿã€‚æ­¤å¤–ï¼Œç°å®ä¸–ç•Œçš„æŸ¥è¯¢å…·æœ‰å„ç§ç±»å‹å’Œå¤æ‚æ€§ï¼Œéœ€è¦ä¸åŒçš„é€»è¾‘ç»“æ„æ¥è¿›è¡Œå‡†ç¡®æ¨ç†ã€‚é¢„æ„å»ºçš„å›¾å½¢å¯èƒ½ä¸è¿™äº›æ‰€éœ€çš„ç»“æ„ä¸åŒ¹é…ï¼Œå¯¼è‡´çŸ¥è¯†æ£€ç´¢æ— æ•ˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ª\textbf{\underline{é€»è¾‘}}æ„ŸçŸ¥çš„\textbf{\underline{R}}etrieval-\textbf{\underline{A}}ugmented \textbf{\underline{G}}enerationæ¡†æ¶ï¼ˆLogicRAGï¼‰ï¼Œå®ƒå¯ä»¥åœ¨æ¨ç†æ—¶é—´åŠ¨æ€æå–æ¨ç†ç»“æ„ï¼Œä»¥æŒ‡å¯¼è‡ªé€‚åº”æ£€ç´¢ï¼Œè€Œæ— éœ€ä»»ä½•é¢„æ„å»ºçš„å›¾ã€‚LogicRAGé¦–å…ˆé€šè¿‡å°†è¾“å…¥æŸ¥è¯¢åˆ†è§£ä¸ºä¸€ç³»åˆ—å­é—®é¢˜å¹¶æ„å»ºæœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰æ¥æ¨¡æ‹Ÿå®ƒä»¬ä¹‹é—´çš„é€»è¾‘ä¾èµ–å…³ç³»ã€‚ä¸ºäº†æ”¯æŒè¿è´¯çš„å¤šæ­¥æ¨ç†ï¼ŒLogicRAGç„¶åä½¿ç”¨æ‹“æ‰‘æ’åºå¯¹å›¾è¿›è¡Œçº¿æ€§åŒ–ï¼Œä»¥ä¾¿æŒ‰é€»è¾‘ä¸€è‡´çš„é¡ºåºè§£å†³å­é—®é¢˜ã€‚æ­¤å¤–ï¼ŒLogicRAGåº”ç”¨å›¾å‰ªææ¥å‡å°‘å†—ä½™æ£€ç´¢ï¼Œå¹¶ä½¿ç”¨ä¸Šä¸‹æ–‡å‰ªææ¥è¿‡æ»¤ä¸ç›¸å…³çš„ä¸Šä¸‹æ–‡ï¼Œä»è€Œå¤§å¤§é™ä½äº†æ€»ä½“ä»¤ç‰Œæˆæœ¬ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°åŸºçº¿ç›¸æ¯”ï¼ŒLogicRAGåœ¨æ€§èƒ½å’Œæ•ˆç‡æ–¹é¢éƒ½å®ç°äº†ä¼˜è¶Šçš„è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06105v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†è¶…å‡ºå…¶çŸ¥è¯†å’Œæ„ŸçŸ¥èŒƒå›´çš„é—®é¢˜æ—¶ï¼Œå®¹æ˜“å‡ºç°ç”Ÿæˆäº‹å®æ€§é”™è¯¯é™ˆè¿°çš„â€œhallucinationâ€ç°è±¡ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†åŸºäºæ£€ç´¢çš„ç”Ÿæˆå¢å¼ºæ–¹æ³•ï¼ˆRAGï¼‰ã€‚æœ€è¿‘çš„ç ”ç©¶åˆ©ç”¨é¢„æ„å»ºçš„å›¾è¡¨æ¥æ•æ‰åˆ†å¸ƒå¼æ–‡æ¡£ä¹‹é—´çš„å…³ç³»è¿æ¥ï¼Œå¹¶åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºå›¾çš„RAGï¼ˆGraphRAGï¼‰æ–¹æ³•éœ€è¦å°†è¯­æ–™åº“è½¬åŒ–ä¸ºå›¾å½¢ï¼Œè¿™ä¸€è¿‡ç¨‹æˆæœ¬é«˜æ˜‚ï¼Œå¸¦æ¥äº†å·¨å¤§çš„ä»¤ç‰Œæˆæœ¬å’Œæ›´æ–°å»¶è¿Ÿã€‚æ­¤å¤–ï¼Œç°å®ä¸–ç•Œä¸­çš„æŸ¥è¯¢ç±»å‹å’Œå¤æ‚æ€§å„ä¸ç›¸åŒï¼Œéœ€è¦ä¸åŒçš„é€»è¾‘ç»“æ„æ¥è¿›è¡Œå‡†ç¡®æ¨ç†ã€‚é¢„æ„å»ºçš„å›¾å½¢å¯èƒ½ä¸æ‰€éœ€çš„é€»è¾‘ç»“æ„ä¸åŒ¹é…ï¼Œå¯¼è‡´çŸ¥è¯†æ£€ç´¢æ— æ•ˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŠ¨æ€æå–æ¨ç†ç»“æ„çš„é€»è¾‘æ„ŸçŸ¥æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼ˆLogicRAGï¼‰ã€‚å®ƒèƒ½åœ¨æ¨ç†æ—¶åˆ†è§£è¾“å…¥æŸ¥è¯¢ä¸ºä¸€ç»„å­é—®é¢˜ï¼Œå¹¶æ„å»ºæœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰æ¥æ¨¡æ‹Ÿå®ƒä»¬ä¹‹é—´çš„é€»è¾‘ä¾èµ–å…³ç³»ã€‚ä¸ºäº†æ”¯æŒè¿è´¯çš„å¤šæ­¥éª¤æ¨ç†ï¼ŒLogicRAGä½¿ç”¨æ‹“æ‰‘æ’åºå¯¹å›¾å½¢è¿›è¡Œçº¿æ€§åŒ–ï¼Œä»¥ä¾¿æŒ‰ç…§é€»è¾‘ä¸€è‡´çš„é¡ºåºè§£å†³å­é—®é¢˜ã€‚æ­¤å¤–ï¼Œå®ƒé€šè¿‡å›¾å‰ªæå‡å°‘å†—ä½™æ£€ç´¢ï¼Œå¹¶é€šè¿‡ä¸Šä¸‹æ–‡å‰ªæè¿‡æ»¤æ‰ä¸ç›¸å…³çš„ä¸Šä¸‹æ–‡ï¼Œå¤§å¤§é™ä½äº†æ•´ä½“çš„ä»¤ç‰Œæˆæœ¬ã€‚å®éªŒè¯æ˜ï¼Œä¸æœ€æ–°çš„åŸºçº¿ç›¸æ¯”ï¼ŒLogicRAGåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜“åœ¨è¶…å‡ºå…¶çŸ¥è¯†å’Œæ„ŸçŸ¥èŒƒå›´çš„é—®é¢˜ä¸­ç”Ÿæˆé”™è¯¯çš„é™ˆè¿°ï¼ˆhallucinationï¼‰ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ï¼ˆRAGï¼‰èƒ½å¤Ÿè§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„ä¸Šä¸‹æ–‡æ¥æ”¯æŒLLMæ¨ç†ã€‚</li>
<li>ç°æœ‰åŸºäºå›¾çš„RAGï¼ˆGraphRAGï¼‰æ–¹æ³•è½¬åŒ–è¯­æ–™åº“ä¸ºå›¾å½¢çš„æˆæœ¬é«˜æ˜‚ï¼Œä¸”é¢ä¸´æ›´æ–°å»¶è¿Ÿçš„é—®é¢˜ã€‚</li>
<li>LogicRAGæ¡†æ¶èƒ½åœ¨æ¨ç†æ—¶åŠ¨æ€æå–æ¨ç†ç»“æ„ï¼Œæ— éœ€é¢„æ„å»ºå›¾å½¢ï¼Œæé«˜äº†çŸ¥è¯†æ£€ç´¢çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>LogicRAGé€šè¿‡åˆ†è§£æŸ¥è¯¢ä¸ºå­é—®é¢˜å¹¶å»ºç«‹æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰æ¥å¤„ç†æŸ¥è¯¢ä¸­çš„é€»è¾‘ä¾èµ–å…³ç³»ã€‚</li>
<li>LogicRAGé€šè¿‡å›¾å‰ªæå’Œä¸Šä¸‹æ–‡å‰ªææŠ€æœ¯é™ä½äº†æ•´ä½“çš„ä»¤ç‰Œæˆæœ¬ï¼Œæé«˜äº†æ£€ç´¢å’Œç”Ÿæˆçš„æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06105">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-99744e8a627dbb3bb49d5f9f8e810a9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c128724cf44142ca6da708bc9098a7c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-89e3bf41a40aef955ce97403248e1498.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-504267ca12e27dc66103a8d118cdb31b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38edf2dd42c30afe1b6b13bd3b879eab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28deba671ff7122e54af0d845e4db9e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69b54fc184d3a4eb3e5f6e12f68cda12.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Can-Large-Models-Fool-the-Eye-A-New-Turing-Test-for-Biological-Animation"><a href="#Can-Large-Models-Fool-the-Eye-A-New-Turing-Test-for-Biological-Animation" class="headerlink" title="Can Large Models Fool the Eye? A New Turing Test for Biological   Animation"></a>Can Large Models Fool the Eye? A New Turing Test for Biological   Animation</h2><p><strong>Authors:Zijian Chen, Lirong Deng, Zhengyu Chen, Kaiwei Zhang, Qi Jia, Yuan Tian, Yucheng Zhu, Guangtao Zhai</strong></p>
<p>Evaluating the abilities of large models and manifesting their gaps are challenging. Current benchmarks adopt either ground-truth-based score-form evaluation on static datasets or indistinct textual chatbot-style human preferences collection, which may not provide users with immediate, intuitive, and perceptible feedback on performance differences. In this paper, we introduce BioMotion Arena, a novel framework for evaluating large language models (LLMs) and multimodal large language models (MLLMs) via visual animation. Our methodology draws inspiration from the inherent visual perception of motion patterns characteristic of living organisms that utilizes point-light source imaging to amplify the performance discrepancies between models. Specifically, we employ a pairwise comparison evaluation and collect more than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion variants. Data analyses show that the crowd-sourced human votes are in good agreement with those of expert raters, demonstrating the superiority of our BioMotion Arena in offering discriminative feedback. We also find that over 90% of evaluated models, including the cutting-edge open-source InternVL3 and proprietary Claude-4 series, fail to produce fundamental humanoid point-light groups, much less smooth and biologically plausible motions. This enables BioMotion Arena to serve as a challenging benchmark for performance visualization and a flexible evaluation framework without restrictions on ground-truth. </p>
<blockquote>
<p>è¯„ä¼°å¤§å‹æ¨¡å‹çš„èƒ½åŠ›å¹¶å±•ç¤ºå…¶å·®è·æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚å½“å‰çš„åŸºå‡†æµ‹è¯•é‡‡ç”¨åŸºäºçœŸå®æ•°æ®çš„é™æ€æ•°æ®é›†ä¸Šçš„è¯„åˆ†å½¢å¼è¯„ä¼°æˆ–æ¨¡ç³Šçš„æ–‡æœ¬èŠå¤©æœºå™¨äººå¼çš„äººç±»åå¥½æ”¶é›†ï¼Œè¿™å¯èƒ½æ— æ³•ä¸ºç”¨æˆ·æä¾›å…³äºæ€§èƒ½å·®å¼‚çš„ç›´æ¥ã€ç›´è§‚å’Œå¯æ„ŸçŸ¥çš„åé¦ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†BioMotion Arenaï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡è§†è§‰åŠ¨ç”»è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•çµæ„Ÿæ¥æºäºç”Ÿç‰©è¿åŠ¨æ¨¡å¼çš„å›ºæœ‰è§†è§‰æ„ŸçŸ¥ï¼Œåˆ©ç”¨ç‚¹å…‰æºæˆåƒæ¥æ”¾å¤§æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨æˆå¯¹æ¯”è¾ƒè¯„ä¼°æ³•ï¼Œå¯¹53ç§ä¸»æµçš„LLMå’ŒMLLMåœ¨90ç§ç”Ÿç‰©è¿åŠ¨å˜ä½“ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œæ”¶é›†äº†è¶…è¿‡4.5ä¸‡å¼ æŠ•ç¥¨ã€‚æ•°æ®åˆ†æè¡¨æ˜ï¼Œç¾¤ä¼—æ¥æºçš„äººç±»æŠ•ç¥¨ä¸ä¸“å®¶è¯„å®¡å‘˜çš„æŠ•ç¥¨é«˜åº¦ä¸€è‡´ï¼Œè¯æ˜äº†æˆ‘ä»¬BioMotion Arenaåœ¨æä¾›åŒºåˆ†åé¦ˆæ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œè¶…è¿‡90%çš„å—è¯„ä¼°æ¨¡å‹ï¼ŒåŒ…æ‹¬æœ€æ–°çš„å¼€æºInternVL3å’Œä¸“æœ‰Claude- 4ç³»åˆ—ï¼Œéƒ½æ— æ³•ç”ŸæˆåŸºæœ¬çš„äººå½¢ç‚¹å…‰æºç»„ï¼Œæ›´ä¸ç”¨è¯´æµç•…å’Œç¬¦åˆç”Ÿç‰©ç‰¹æ€§çš„è¿åŠ¨äº†ã€‚è¿™ä½¿å¾—BioMotion Arenaèƒ½å¤Ÿä½œä¸ºæ€§èƒ½å¯è§†åŒ–çš„æŒ‘æˆ˜åŸºå‡†ï¼Œå¹¶ä¸”ä½œä¸ºä¸€ä¸ªçµæ´»çš„è¯„ä¼°æ¡†æ¶ï¼Œä¸å—çœŸå®æ•°æ®é™åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06072v1">PDF</a> 24 pages, 10 figures</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºBioMotion Arenaçš„æ–°å‹è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡è§†è§‰åŠ¨ç”»è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç”Ÿç‰©è¿åŠ¨æ¨¡å¼çš„è§†è§‰æ„ŸçŸ¥ç‰¹ç‚¹ï¼Œé‡‡ç”¨ç‚¹å…‰æºæˆåƒæŠ€æœ¯æ¥çªå‡ºæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæä¾›æœ‰åŒºåˆ†åº¦çš„åé¦ˆï¼Œå¹¶å‘ç°å¤§å¤šæ•°æ¨¡å‹åœ¨ç”ŸæˆåŸºæœ¬çš„äººå½¢ç‚¹å…‰æºç»„åˆä»¥åŠæµç•…ã€ç¬¦åˆç”Ÿç‰©ç‰¹æ€§çš„åŠ¨ä½œæ–¹é¢å­˜åœ¨å›°éš¾ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å½“å‰æ¨¡å‹è¯„ä¼°æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ï¼šæ— æ³•ä¸ºç”¨æˆ·æä¾›å…³äºæ€§èƒ½å·®å¼‚çš„ç›´æ¥ã€ç›´è§‚å’Œå¯æ„ŸçŸ¥çš„åé¦ˆã€‚</li>
<li>BioMotion Arenaçš„å¼•å…¥ï¼šä¸€ç§æ–°å‹è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡è§†è§‰åŠ¨ç”»è¯„ä¼°LLMså’ŒMLLMsçš„èƒ½åŠ›ã€‚</li>
<li>æ¡†æ¶çš„è¿ä½œåŸç†ï¼šåˆ©ç”¨ç”Ÿç‰©è¿åŠ¨æ¨¡å¼çš„è§†è§‰æ„ŸçŸ¥ç‰¹ç‚¹ï¼Œé‡‡ç”¨ç‚¹å…‰æºæˆåƒæŠ€æœ¯æ”¾å¤§æ¨¡å‹é—´æ€§èƒ½å·®å¼‚ã€‚</li>
<li>è¯„ä¼°æ–¹æ³•ï¼šé‡‡ç”¨æˆå¯¹æ¯”è¾ƒè¯„ä¼°ï¼Œæ”¶é›†è¶…è¿‡45,000å¼ é€‰ç¥¨ï¼Œå¯¹53ç§ä¸»æµLLMså’ŒMLLMsè¿›è¡Œ90ç§ç”Ÿç‰©è¿åŠ¨å˜ç§çš„è¯„ä»·ã€‚</li>
<li>æ•°æ®åˆ†æç»“æœï¼šç¾¤ä¼—æŠ•ç¥¨ä¸ä¸“å®¶è¯„åˆ†é«˜åº¦ä¸€è‡´ï¼Œè¯æ˜BioMotion Arenaæä¾›æœ‰åŒºåˆ†åº¦çš„åé¦ˆã€‚</li>
<li>æ¨¡å‹æ€§èƒ½é—®é¢˜ï¼šè¶…è¿‡90%çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬æœ€å…ˆè¿›çš„å¼€æºInternVL3å’Œä¸“æœ‰Claude-4ç³»åˆ—ï¼Œæ— æ³•ç”ŸæˆåŸºæœ¬çš„äººå½¢ç‚¹å…‰æºç»„åˆï¼ŒåŠ¨ä½œä¹Ÿä¸å¤Ÿæµç•…å’Œç¬¦åˆç”Ÿç‰©ç‰¹æ€§ã€‚</li>
<li>BioMotion Arenaçš„ä¼˜åŠ¿ï¼šä½œä¸ºå…·æœ‰æŒ‘æˆ˜æ€§çš„æ€§èƒ½å¯è§†åŒ–è¯„ä¼°æ¡†æ¶ï¼Œæ— éœ€å—é™äºåœ°é¢çœŸå®æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06072">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0e6eedccdecdbe0077b5f9d3405579c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-749d401cef39241cedae29aff6acb454.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e789c4a9ebd629f643e99528d7ed230b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3ed2d57bbc7152430b6141955da2b0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6f621d2869d9cc47cf9167c821ddeee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e09f529dc69b260f9e2973612694f61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4746b80f292e86bea492d8b1274e1096.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="VQAThinker-Exploring-Generalizable-and-Explainable-Video-Quality-Assessment-via-Reinforcement-Learning"><a href="#VQAThinker-Exploring-Generalizable-and-Explainable-Video-Quality-Assessment-via-Reinforcement-Learning" class="headerlink" title="VQAThinker: Exploring Generalizable and Explainable Video Quality   Assessment via Reinforcement Learning"></a>VQAThinker: Exploring Generalizable and Explainable Video Quality   Assessment via Reinforcement Learning</h2><p><strong>Authors:Linhan Cao, Wei Sun, Weixia Zhang, Xiangyang Zhu, Jun Jia, Kaiwei Zhang, Dandan Zhu, Guangtao Zhai, Xiongkuo Min</strong></p>
<p>Video quality assessment (VQA) aims to objectively quantify perceptual quality degradation in alignment with human visual perception. Despite recent advances, existing VQA models still suffer from two critical limitations: \textit{poor generalization to out-of-distribution (OOD) videos} and \textit{limited explainability}, which restrict their applicability in real-world scenarios. To address these challenges, we propose \textbf{VQAThinker}, a reasoning-based VQA framework that leverages large multimodal models (LMMs) with reinforcement learning to jointly model video quality understanding and scoring, emulating human perceptual decision-making. Specifically, we adopt group relative policy optimization (GRPO), a rule-guided reinforcement learning algorithm that enables reasoning over video quality under score-level supervision, and introduce three VQA-specific rewards: (1) a \textbf{bell-shaped regression reward} that increases rapidly as the prediction error decreases and becomes progressively less sensitive near the ground truth; (2) a \textbf{pairwise ranking reward} that guides the model to correctly determine the relative quality between video pairs; and (3) a \textbf{temporal consistency reward} that encourages the model to prefer temporally coherent videos over their perturbed counterparts. Extensive experiments demonstrate that VQAThinker achieves state-of-the-art performance on both in-domain and OOD VQA benchmarks, showing strong generalization for video quality scoring. Furthermore, evaluations on video quality understanding tasks validate its superiority in distortion attribution and quality description compared to existing explainable VQA models and LMMs. These findings demonstrate that reinforcement learning offers an effective pathway toward building generalizable and explainable VQA models solely with score-level supervision. </p>
<blockquote>
<p>è§†é¢‘è´¨é‡è¯„ä¼°ï¼ˆVQAï¼‰æ—¨åœ¨å®¢è§‚åœ°é‡åŒ–æ„ŸçŸ¥è´¨é‡ä¸‹é™ï¼Œä¸äººç±»è§†è§‰æ„ŸçŸ¥ç›¸ä¸€è‡´ã€‚å°½ç®¡æœ€è¿‘æœ‰è¿›å±•ï¼Œä½†ç°æœ‰çš„VQAæ¨¡å‹ä»ç„¶é¢ä¸´ä¸¤ä¸ªå…³é”®å±€é™æ€§ï¼šå¯¹åˆ†å¸ƒå¤–ï¼ˆOODï¼‰è§†é¢‘çš„æ¨å¹¿èƒ½åŠ›è¾ƒå·®å’Œè§£é‡Šæ€§æœ‰é™ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ¨ç†çš„VQAæ¡†æ¶â€”â€”VQAThinkerï¼Œå®ƒåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å’Œå¼ºåŒ–å­¦ä¹ æ¥è”åˆå»ºæ¨¡è§†é¢‘è´¨é‡ç†è§£å’Œè¯„åˆ†ï¼Œæ¨¡æ‹Ÿäººç±»æ„ŸçŸ¥å†³ç­–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä»¥è§„åˆ™ä¸ºæŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨è¯„åˆ†ç›‘ç£ä¸‹å¯¹è§†é¢‘è´¨é‡è¿›è¡Œæ¨ç†ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸‰ç§é’ˆå¯¹VQAçš„å¥–åŠ±ï¼šï¼ˆ1ï¼‰<strong>é’Ÿå½¢å›å½’å¥–åŠ±</strong>ï¼Œéšç€é¢„æµ‹è¯¯å·®çš„å‡å°è€Œè¿…é€Ÿå¢åŠ ï¼Œåœ¨æ¥è¿‘çœŸå®å€¼æ—¶å˜å¾—ä¸é‚£ä¹ˆæ•æ„Ÿï¼›ï¼ˆ2ï¼‰<strong>æˆå¯¹æ’åå¥–åŠ±</strong>ï¼Œå¼•å¯¼æ¨¡å‹æ­£ç¡®åˆ¤æ–­è§†é¢‘å¯¹ä¹‹é—´çš„ç›¸å¯¹è´¨é‡ï¼›ï¼ˆ3ï¼‰<strong>æ—¶é—´ä¸€è‡´æ€§å¥–åŠ±</strong>ï¼Œé¼“åŠ±æ¨¡å‹é€‰æ‹©æ—¶é—´ä¸Šè¿è´¯çš„è§†é¢‘è€Œä¸æ˜¯å…¶å—æ‰°åŠ¨çš„å¯¹åº”ç‰©ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVQAThinkeråœ¨åŸŸå†…å’ŒåŸŸå¤–çš„VQAåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨è§†é¢‘è´¨é‡è¯„åˆ†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œåœ¨è§†é¢‘è´¨é‡ç†è§£ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¯æ˜ï¼Œä¸ç°æœ‰çš„å¯è§£é‡ŠVQAæ¨¡å‹å’ŒLMMsç›¸æ¯”ï¼Œå…¶åœ¨å¤±çœŸå½’å±å’Œè´¨é‡æè¿°æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ æ˜¯æ„å»ºå…·æœ‰æ³›åŒ–å’Œè§£é‡Šæ€§çš„VQAæ¨¡å‹çš„æœ‰æ•ˆé€”å¾„ï¼Œå¹¶ä¸”åªéœ€è¦è¯„åˆ†çº§çš„ç›‘ç£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06051v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†é¢‘è´¨é‡è¯„ä¼°ï¼ˆVQAï¼‰æ—¨åœ¨å®¢è§‚é‡åŒ–æ„ŸçŸ¥è´¨é‡é€€åŒ–ï¼Œä¸äººç±»è§†è§‰æ„ŸçŸ¥ç›¸ä¸€è‡´ã€‚å°½ç®¡è¿‘æœŸæœ‰æ‰€è¿›å±•ï¼Œä½†ç°æœ‰VQAæ¨¡å‹ä»é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šå¯¹åˆ†å¸ƒå¤–ï¼ˆOODï¼‰è§†é¢‘çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®å’Œè§£é‡Šæ€§æœ‰é™ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºåŸºäºæ¨ç†çš„VQAæ¡†æ¶VQAThinkerï¼Œå®ƒåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å’Œå¼ºåŒ–å­¦ä¹ æ¥è”åˆå»ºæ¨¡è§†é¢‘è´¨é‡ç†è§£å’Œè¯„åˆ†ï¼Œæ¨¡æ‹Ÿäººç±»æ„ŸçŸ¥å†³ç­–è¿‡ç¨‹ã€‚é€šè¿‡é‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿™ä¸€è§„åˆ™å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå®ç°åœ¨è¯„åˆ†çº§åˆ«ç›‘ç£ä¸‹çš„è§†é¢‘è´¨é‡æ¨ç†ã€‚å¼•å…¥ä¸‰ç§VQAç‰¹å®šå¥–åŠ±ï¼š1ï¼‰é’Ÿå½¢å›å½’å¥–åŠ±ï¼Œéšç€é¢„æµ‹è¯¯å·®çš„å‡å°è€Œå¿«é€Ÿå¢åŠ ï¼Œåœ¨æ¥è¿‘çœŸå®å€¼æ—¶å˜å¾—é€æ¸ä¸æ•æ„Ÿï¼›2ï¼‰é…å¯¹æ’åå¥–åŠ±ï¼Œå¼•å¯¼æ¨¡å‹æ­£ç¡®åˆ¤æ–­è§†é¢‘å¯¹ä¹‹é—´çš„ç›¸å¯¹è´¨é‡ï¼›3ï¼‰æ—¶é—´ä¸€è‡´æ€§å¥–åŠ±ï¼Œé¼“åŠ±æ¨¡å‹ä¼˜å…ˆé€‰æ‹©æ—¶é—´è¿è´¯çš„è§†é¢‘è€Œä¸æ˜¯å…¶å—æ‰°åŠ¨çš„å¯¹åº”ç‰©ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVQAThinkeråœ¨åŸŸå†…å’ŒåŸŸå¤–çš„VQAåŸºå‡†æµ‹è¯•ä¸­å‡å®ç°äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ï¼Œåœ¨è§†é¢‘è´¨é‡è¯„åˆ†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œåœ¨è§†é¢‘è´¨é‡ç†è§£ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¯å®ï¼Œä¸ç°æœ‰çš„å¯è§£é‡ŠVQAæ¨¡å‹å’ŒLMMsç›¸æ¯”ï¼Œå…¶åœ¨å¤±çœŸå½’å±å’Œè´¨é‡æè¿°æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ ä¸ºä»…é€šè¿‡è¯„åˆ†çº§ç›‘ç£æ„å»ºé€šç”¨å’Œå¯è§£é‡Šçš„VQAæ¨¡å‹æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>VQAæ—¨åœ¨å®¢è§‚é‡åŒ–è§†é¢‘è´¨é‡çš„æ„ŸçŸ¥é€€åŒ–ï¼Œä¸äººç±»çš„è§†è§‰æ„ŸçŸ¥ç›¸ä¸€è‡´ã€‚</li>
<li>å½“å‰VQAæ¨¡å‹é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šå¯¹OODè§†é¢‘çš„æ³›åŒ–èƒ½åŠ›å·®å’Œè§£é‡Šæ€§æœ‰é™ã€‚</li>
<li>VQAThinkeræ¡†æ¶ç»“åˆå¤§å‹å¤šæ¨¡æ€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ ï¼Œæ¨¡æ‹Ÿäººç±»æ„ŸçŸ¥å†³ç­–è¿‡ç¨‹è¿›è¡Œè§†é¢‘è´¨é‡è¯„ä¼°ã€‚</li>
<li>é‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ï¼Œé€šè¿‡è¯„åˆ†çº§ç›‘ç£å®ç°è§†é¢‘è´¨é‡æ¨ç†ã€‚</li>
<li>å¼•å…¥ä¸‰ç§VQAç‰¹å®šå¥–åŠ±æ¥æå‡æ¨¡å‹æ€§èƒ½ï¼šé’Ÿå½¢å›å½’å¥–åŠ±ã€é…å¯¹æ’åå¥–åŠ±å’Œæ—¶é—´ä¸€è‡´æ€§å¥–åŠ±ã€‚</li>
<li>VQAThinkeråœ¨åŸŸå†…å’ŒåŸŸå¤–çš„VQAåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæ³›åŒ–èƒ½åŠ›å¼ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06051">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-81ce40e492d941262dd6f9ab73c71312.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39dec2b109fc9bee56b54d702c4f6f85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e8aef970d4548695ea5e77486a88559.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d59afdb0139f49102573dc5c9018f7d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73faede90b6a28fafa2c00ab7f1f753f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="EvolvR-Self-Evolving-Pairwise-Reasoning-for-Story-Evaluation-to-Enhance-Generation"><a href="#EvolvR-Self-Evolving-Pairwise-Reasoning-for-Story-Evaluation-to-Enhance-Generation" class="headerlink" title="EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance   Generation"></a>EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance   Generation</h2><p><strong>Authors:Xinda Wang, Zhengxu Hou, Yangshijie Zhang, Bingren Yan, Zhibo Yang, Xingsheng Zhang, Luxi Xing, Qiang Zhou, Chen Zhang</strong></p>
<p>Although the effectiveness of Large Language Models (LLMs) as judges (LLM-as-a-judge) has been validated, their performance remains limited in open-ended tasks, particularly in story evaluation. Accurate story evaluation is crucial not only for assisting human quality judgment but also for providing key signals to guide story generation. However, existing methods face a dilemma: prompt engineering for closed-source models suffers from poor adaptability, while fine-tuning approaches for open-source models lack the rigorous reasoning capabilities essential for story evaluation. To address this, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework. Grounded in pairwise comparison, the framework first self-synthesizes score-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To ensure data quality, these raw CoTs undergo a self-filtering process, utilizing multi-agents to guarantee their logical rigor and robustness. Finally, the evaluator trained on the refined data is deployed as a reward model to guide the story generation task. Experimental results demonstrate that our framework achieves state-of-the-art (SOTA) performance on three evaluation benchmarks including StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward model, it significantly enhances the quality of generated stories, thereby fully validating the superiority of our self-evolving approach. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯„åˆ¤è€…ï¼ˆLLM-as-a-judgeï¼‰çš„æœ‰æ•ˆæ€§å·²ç»å¾—åˆ°éªŒè¯ï¼Œä½†å®ƒä»¬åœ¨å¼€æ”¾ä»»åŠ¡ä¸­çš„è¡¨ç°ä»ç„¶æœ‰é™ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•…äº‹è¯„ä¼°æ–¹é¢ã€‚å‡†ç¡®çš„æ•…äº‹è¯„ä¼°å¯¹äºè¾…åŠ©äººç±»è´¨é‡åˆ¤æ–­å’Œæä¾›å…³é”®ä¿¡å·ä»¥å¼•å¯¼æ•…äº‹ç”Ÿæˆéƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´å›°å¢ƒï¼šä¸ºå°é—­æºæ¨¡å‹è¿›è¡Œçš„æç¤ºå·¥ç¨‹é€‚åº”æ€§è¾ƒå·®ï¼Œè€Œä¸ºå¼€æ”¾æºæ¨¡å‹è¿›è¡Œçš„å¾®è°ƒæ–¹æ³•ç¼ºä¹æ•…äº‹è¯„ä¼°æ‰€éœ€çš„å…³é”®æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªè¿›åŒ–é…å¯¹æ¨ç†ï¼ˆEvolvRï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäºé…å¯¹æ¯”è¾ƒï¼Œé¦–å…ˆé€šè¿‡å¤šè§’è‰²ç­–ç•¥è‡ªæˆ‘åˆæˆä¸åˆ†æ•°å¯¹é½çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®ã€‚ä¸ºç¡®ä¿æ•°æ®è´¨é‡ï¼Œè¿™äº›åŸå§‹CoTç»å†è‡ªæˆ‘è¿‡æ»¤è¿‡ç¨‹ï¼Œåˆ©ç”¨å¤šæ™ºèƒ½ä½“ä¿è¯é€»è¾‘ä¸¥è°¨æ€§å’Œç¨³å¥æ€§ã€‚æœ€åï¼Œåœ¨ç²¾ç‚¼æ•°æ®ä¸Šè®­ç»ƒçš„è¯„ä¼°å™¨è¢«éƒ¨ç½²ä¸ºå¥–åŠ±æ¨¡å‹ï¼Œä»¥æŒ‡å¯¼æ•…äº‹ç”Ÿæˆä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨åŒ…æ‹¬StoryERã€HANNAå’ŒOpenMEVAåœ¨å†…çš„ä¸‰ä¸ªè¯„ä¼°åŸºå‡†ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æ­¤å¤–ï¼Œå½“å®ƒä½œä¸ºå¥–åŠ±æ¨¡å‹æ—¶ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆæ•…äº‹çš„è´¨é‡ï¼Œä»è€Œå……åˆ†éªŒè¯äº†æˆ‘ä»¬è‡ªè¿›åŒ–æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06046v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•…äº‹è¯„ä»·ç­‰å¼€æ”¾ä»»åŠ¡ä¸­çš„æ€§èƒ½æœ‰é™ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†åŸºäºè‡ªæˆ‘è¿›åŒ–çš„é…å¯¹æ¨ç†ï¼ˆEvolvRï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤šè§’è‰²ç­–ç•¥è‡ªæˆ‘åˆæˆä¸åˆ†æ•°å¯¹é½çš„æ€è€ƒé“¾ï¼ˆCoTï¼‰æ•°æ®ï¼Œå¹¶ç»è¿‡è‡ªæˆ‘è¿‡æ»¤è¿‡ç¨‹ä¿è¯æ•°æ®è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ•…äº‹è¯„ä»·æ–¹é¢è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œä½œä¸ºå¥–åŠ±æ¨¡å‹æ—¶ï¼Œèƒ½æ˜¾è‘—æé«˜ç”Ÿæˆæ•…äº‹çš„è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¼€æ”¾ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯æ•…äº‹è¯„ä»·æ–¹é¢çš„æ€§èƒ½å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´ä¸¤éš¾é—®é¢˜ï¼šå°é—­æ¨¡å‹çš„æç¤ºå·¥ç¨‹é€‚åº”æ€§å·®ï¼Œè€Œå¼€æºæ¨¡å‹çš„å¾®è°ƒæ–¹æ³•ç¼ºä¹æ•…äº‹è¯„ä»·æ‰€éœ€çš„ä¸¥è°¨æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†EvolvRæ¡†æ¶ï¼ŒåŸºäºé…å¯¹æ¯”è¾ƒï¼Œé€šè¿‡å¤šè§’è‰²ç­–ç•¥è‡ªæˆ‘åˆæˆä¸åˆ†æ•°å¯¹é½çš„CoTæ•°æ®ã€‚</li>
<li>è‡ªæˆ‘è¿‡æ»¤è¿‡ç¨‹ä¿è¯æ•°æ®è´¨é‡ï¼Œåˆ©ç”¨å¤šæ™ºèƒ½ä½“ä¿è¯é€»è¾‘ä¸¥è°¨æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨ä¸‰ä¸ªè¯„ä»·åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>ä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼ŒEvolvRæ¡†æ¶èƒ½æ˜¾è‘—æé«˜ç”Ÿæˆæ•…äº‹çš„è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06046">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fc9ac2903428da2216c5fd97c48eca4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e5671159e7ab396703e9f86820feed7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-871775a292aa374539bbc97c8e222dc6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-58209316787f209dfe1e1ae501685f0e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Society-of-Mind-Meets-Real-Time-Strategy-A-Hierarchical-Multi-Agent-Framework-for-Strategic-Reasoning"><a href="#Society-of-Mind-Meets-Real-Time-Strategy-A-Hierarchical-Multi-Agent-Framework-for-Strategic-Reasoning" class="headerlink" title="Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent   Framework for Strategic Reasoning"></a>Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent   Framework for Strategic Reasoning</h2><p><strong>Authors:Daechul Ahn, San Kim, Jonghyun Choi</strong></p>
<p>Large Language Models (LLMs) have recently demonstrated impressive action sequence prediction capabilities but often struggle with dynamic, long-horizon tasks such as real-time strategic games. In a game such as StarCraftII (SC2), agents need to manage resource constraints and adapt to evolving battlefield situations in a partially observable environment. This often overwhelms exisiting LLM-based approaches. To address these challenges, we propose a hierarchical multi-agent framework that employs specialized imitation learning agents under a meta-controller called Strategic Planner (SP). By expert demonstrations, each specialized agent learns a distinctive strategy, such as aerial support or defensive maneuvers, and produces coherent, structured multistep action sequences. The SP then orchestrates these proposals into a single, environmentally adaptive plan that ensures local decisions aligning with long-term strategies. We call this HIMA (Hierarchical Imitation Multi-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that encompasses all race match combinations in SC2. Our empirical results show that HIMA outperforms state of the arts in strategic clarity, adaptability, and computational efficiency, underscoring the potential of combining specialized imitation modules with meta-level orchestration to develop more robust, general-purpose AI agents. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„åŠ¨ä½œåºåˆ—é¢„æµ‹èƒ½åŠ›ï¼Œä½†åœ¨åŠ¨æ€ã€é•¿æœŸä»»åŠ¡ï¼ˆå¦‚å®æ—¶æˆ˜ç•¥æ¸¸æˆï¼‰æ–¹é¢å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨æ˜Ÿé™…äº‰éœ¸IIï¼ˆSC2ï¼‰ç­‰æ¸¸æˆä¸­ï¼Œæ™ºèƒ½ä½“éœ€è¦åœ¨éƒ¨åˆ†å¯è§‚å¯Ÿçš„ç¯å¢ƒä¸­ç®¡ç†èµ„æºçº¦æŸå¹¶é€‚åº”ä¸æ–­å˜åŒ–çš„æˆ˜åœºæƒ…å†µã€‚è¿™å¸¸å¸¸ä½¿ç°æœ‰çš„LLMæ–¹æ³•æ„Ÿåˆ°éš¾ä»¥åº”å¯¹ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†å±‚å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨åä¸ºæˆ˜ç•¥è§„åˆ’å™¨ï¼ˆSPï¼‰çš„å…ƒæ§åˆ¶å™¨ä¸‹çš„ä¸“ç”¨æ¨¡ä»¿å­¦ä¹ æ™ºèƒ½ä½“ã€‚é€šè¿‡ä¸“å®¶æ¼”ç¤ºï¼Œæ¯ä¸ªä¸“ç”¨æ™ºèƒ½ä½“å­¦ä¹ ç‹¬ç‰¹çš„ç­–ç•¥ï¼Œå¦‚ç©ºä¸­æ”¯æ´æˆ–é˜²å¾¡æœºåŠ¨ï¼Œå¹¶äº§ç”Ÿè¿è´¯ã€ç»“æ„åŒ–çš„å¤šæ­¥åŠ¨ä½œåºåˆ—ã€‚ç„¶åSPå°†è¿™äº›æè®®åè°ƒæˆä¸€ä¸ªå•ä¸€ã€ç¯å¢ƒè‡ªé€‚åº”çš„è®¡åˆ’ï¼Œç¡®ä¿å±€éƒ¨å†³ç­–ä¸é•¿æœŸç­–ç•¥ä¸€è‡´ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºHIMAï¼ˆåˆ†å±‚æ¨¡ä»¿å¤šæ™ºèƒ½ä½“ï¼‰ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†TEXTSCII-ALLï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„SC2æµ‹è¯•å¹³å°ï¼Œæ¶µç›–äº†SC2ä¸­çš„æ‰€æœ‰ç§æ—æ¯”èµ›ç»„åˆã€‚æˆ‘ä»¬çš„å®è¯ç»“æœè¡¨æ˜ï¼ŒHIMAåœ¨æˆ˜ç•¥æ¸…æ™°åº¦ã€é€‚åº”æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œçªæ˜¾äº†å°†ä¸“ç”¨æ¨¡ä»¿æ¨¡å—ä¸å…ƒçº§åˆ«ç¼–æ’ç›¸ç»“åˆä»¥å¼€å‘æ›´ç¨³å¥ã€é€šç”¨äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06042v1">PDF</a> COLM 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŠ¨ä½œåºåˆ—é¢„æµ‹æ–¹é¢å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†åœ¨åŠ¨æ€ã€é•¿æœŸä»»åŠ¡å¦‚å®æ—¶æˆ˜ç•¥æ¸¸æˆä¸Šå¸¸é‡æŒ‘æˆ˜ã€‚é’ˆå¯¹StarCraftIIæ¸¸æˆï¼Œæˆ‘ä»¬æå‡ºåˆ†å±‚å¤šæ™ºèƒ½ä½“æ¡†æ¶HIMAï¼ˆHierarchical Imitation Multi-Agentï¼‰ï¼Œç»“åˆç‰¹å®šæ¨¡ä»¿å­¦ä¹ æ™ºèƒ½ä½“å’Œå…ƒæ§åˆ¶å™¨æˆ˜ç•¥è§„åˆ’å™¨ï¼ˆSPï¼‰ã€‚é€šè¿‡ä¸“å®¶ç¤ºèŒƒï¼Œæ¯ä¸ªæ™ºèƒ½ä½“å­¦ä¹ ç‹¬ç‰¹ç­–ç•¥å¦‚ç©ºä¸­æ”¯æ´æˆ–é˜²å¾¡æœºåŠ¨ç­‰ï¼Œäº§ç”Ÿè¿è´¯ç»“æ„åŒ–å¤šæ­¥åŠ¨ä½œåºåˆ—ã€‚SPè´Ÿè´£åè°ƒè¿™äº›ææ¡ˆï¼Œå½¢æˆé€‚åº”ç¯å¢ƒå•ä¸€è®¡åˆ’ï¼Œç¡®ä¿å±€éƒ¨å†³ç­–ä¸é•¿æœŸç­–ç•¥ä¸€è‡´ã€‚æˆ‘ä»¬å»ºç«‹çš„ç»¼åˆæµ‹è¯•å¹³å°TEXTSCII-ALLæ¶µç›–äº†SC2æ‰€æœ‰ç§æ—åŒ¹é…ç»„åˆã€‚å®éªŒè¡¨æ˜HIMAåœ¨æˆ˜ç•¥æ¸…æ™°åº¦ã€é€‚åº”æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢è¶…è¶Šç°æœ‰æŠ€æœ¯ï¼Œå‡¸æ˜¾ç»“åˆç‰¹å®šæ¨¡ä»¿æ¨¡å—ä¸å…ƒçº§åè°ƒå¼€å‘æ›´ç¨³å¥ã€é€šç”¨AIæ™ºèƒ½ä½“çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŠ¨ä½œåºåˆ—é¢„æµ‹ä¸Šè¡¨ç°çªå‡ºï¼Œä½†åœ¨åŠ¨æ€ã€é•¿æœŸä»»åŠ¡å¦‚å®æ—¶æˆ˜ç•¥æ¸¸æˆä¸Šå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰LLMæ–¹æ³•åœ¨åº”å¯¹å¤æ‚æ¸¸æˆç¯å¢ƒæ—¶å¯èƒ½æ˜¾å¾—åŠ›ä¸ä»å¿ƒï¼Œéœ€è¦æ›´é«˜çº§çš„åº”å¯¹ç­–ç•¥ã€‚</li>
<li>HIMAï¼ˆHierarchical Imitation Multi-Agentï¼‰æ¡†æ¶ç»“åˆäº†ç‰¹å®šæ¨¡ä»¿å­¦ä¹ æ™ºèƒ½ä½“å’Œå…ƒæ§åˆ¶å™¨æˆ˜ç•¥è§„åˆ’å™¨ï¼ˆSPï¼‰ï¼Œä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>HIMAæ¡†æ¶é€šè¿‡ä¸“å®¶ç¤ºèŒƒä½¿æ¯ä¸ªæ™ºèƒ½ä½“å­¦ä¹ ç‹¬ç‰¹ç­–ç•¥ï¼Œå¦‚ç©ºä¸­æ”¯æ´æˆ–é˜²å¾¡æœºåŠ¨ç­‰ã€‚</li>
<li>æˆ˜ç•¥è§„åˆ’å™¨ï¼ˆSPï¼‰è´Ÿè´£åè°ƒæ™ºèƒ½ä½“çš„è¡ŒåŠ¨ï¼Œç¡®ä¿å±€éƒ¨å†³ç­–ä¸é•¿æœŸç­–ç•¥ä¸€è‡´ã€‚</li>
<li>HIMAåœ¨StarCraftIIæ¸¸æˆçš„ç»¼åˆæµ‹è¯•å¹³å°TEXTSCII-ALLä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd61e90ca14f479884b1fdaa8961f910.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17ab5fd7465666c374bb6d1b34a7bb44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00988d1d8053cdb23adf87a6fdb87867.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86a3fc8316619f3855c50fee969555a1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MathReal-We-Keep-It-Real-A-Real-Scene-Benchmark-for-Evaluating-Math-Reasoning-in-Multimodal-Large-Language-Models"><a href="#MathReal-We-Keep-It-Real-A-Real-Scene-Benchmark-for-Evaluating-Math-Reasoning-in-Multimodal-Large-Language-Models" class="headerlink" title="MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math   Reasoning in Multimodal Large Language Models"></a>MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math   Reasoning in Multimodal Large Language Models</h2><p><strong>Authors:Jun Feng, Zixin Wang, Zhentao Zhang, Yue Guo, Zhihan Zhou, Xiuyi Chen, Zhenyang Li, Dawei Yin</strong></p>
<p>Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in visual mathematical reasoning across various existing benchmarks. However, these benchmarks are predominantly based on clean or processed multimodal inputs, without incorporating the images provided by real-world Kindergarten through 12th grade (K-12) educational users. To address this gap, we introduce MathReal, a meticulously curated dataset comprising 2,000 mathematical questions with images captured by handheld mobile devices in authentic scenarios. Each question is an image, containing the question text and visual element. We systematically classify the real images into three primary categories: image quality degradation, perspective variation, and irrelevant content interference, which are further delineated into 14 subcategories. Additionally, MathReal spans five core knowledge and ability categories, which encompass three question types and are divided into three difficulty levels. To comprehensively evaluate the multimodal mathematical reasoning abilities of state-of-the-art MLLMs in real-world scenarios, we design six experimental settings that enable a systematic analysis of their performance. Through extensive experimentation, we find that the problem-solving abilities of existing MLLMs are significantly challenged in realistic educational contexts. Based on this, we conduct a thorough analysis of their performance and error patterns, providing insights into their recognition, comprehension, and reasoning capabilities, and outlining directions for future improvements. Data and code: <a target="_blank" rel="noopener" href="https://github.com/junfeng0288/MathReal">https://github.com/junfeng0288/MathReal</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ç§ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­å·²åœ¨è§†è§‰æ•°å­¦æ¨ç†æ–¹é¢å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›åŸºå‡†æµ‹è¯•ä¸»è¦åŸºäºå¹²å‡€æˆ–å¤„ç†è¿‡çš„å¤šæ¨¡å¼è¾“å…¥ï¼Œå¹¶æ²¡æœ‰èå…¥ç”±ç°å®ä¸–ç•Œå¹¼å„¿å›­è‡³12å¹´çº§ï¼ˆK-12ï¼‰æ•™è‚²ç”¨æˆ·æä¾›çš„å›¾åƒã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MathRealï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾å¿ƒç­–åˆ’çš„æ•°æ®é›†ï¼ŒåŒ…å«2000ä¸ªæ•°å­¦é—®é¢˜åŠç”±æ‰‹æŒç§»åŠ¨è®¾å¤‡åœ¨çœŸå®åœºæ™¯ä¸­æ‹æ‘„çš„å›¾ç‰‡ã€‚æ¯ä¸ªé—®é¢˜éƒ½æ˜¯ä¸€å¼ åŒ…å«é—®é¢˜æ–‡æœ¬å’Œè§†è§‰å…ƒç´ çš„å›¾ç‰‡ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°å°†è¿™äº›çœŸå®å›¾ç‰‡åˆ†ä¸ºä¸‰å¤§ä¸»è¦ç±»åˆ«ï¼šå›¾åƒè´¨é‡é€€åŒ–ã€è§†è§’å˜åŒ–å’Œæ— å…³å†…å®¹å¹²æ‰°ï¼Œè¿™ä¸‰å¤§ç±»åˆ«åˆç»†åˆ†ä¸º1 the subcategoryç­‰åå››ä¸ªå­ç±»åˆ«ã€‚æ­¤å¤–ï¼ŒMathRealæ¶µç›–äº”ä¸ªæ ¸å¿ƒçŸ¥è¯†å’Œèƒ½åŠ›ç±»åˆ«ï¼ŒåŒ…å«ä¸‰ç§é¢˜å‹ï¼Œåˆ†ä¸ºåˆçº§ã€ä¸­çº§å’Œé«˜çº§ä¸‰ä¸ªéš¾åº¦ç­‰çº§ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°æœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ•°å­¦æ¨ç†èƒ½åŠ›åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„è¡¨ç°ï¼Œæˆ‘ä»¬è®¾è®¡äº†å…­ç§å®éªŒè®¾ç½®ï¼Œä»¥ä¾¿å¯¹å…¶æ€§èƒ½è¿›è¡Œç³»ç»Ÿçš„åˆ†æã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°ç°æœ‰MLLMsåœ¨çœŸå®æ•™è‚²ç¯å¢ƒä¸­çš„é—®é¢˜è§£å†³èƒ½åŠ›é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¯¹ä»–ä»¬çš„æ€§èƒ½åŠé”™è¯¯æ¨¡å¼è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œæ·±å…¥äº†è§£ä»–ä»¬åœ¨è¯†åˆ«ã€ç†è§£å’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥æ”¹è¿›çš„æ–¹å‘ã€‚æ•°æ®å’Œä»£ç å¯é€šè¿‡é“¾æ¥ <a target="_blank" rel="noopener" href="https://github.com/junfeng0288/MathReal">https://github.com/junfeng0288/MathReal</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06009v1">PDF</a> 29 pages, 16 figures</p>
<p><strong>Summary</strong><br>å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç°æœ‰çš„å„ç§åŸºå‡†æµ‹è¯•ä¸­å±•ç°äº†å‡ºè‰²çš„è§†è§‰æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚ä½†ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦åŸºäºæ¸…æ´æˆ–å¤„ç†è¿‡çš„å¤šåª’ä½“è¾“å…¥ï¼Œå¹¶æœªçº³å…¥æ¥è‡ªçœŸå®å¹¼å„¿å›­è‡³åäºŒå¹´çº§ï¼ˆK-12ï¼‰æ•™è‚²ç”¨æˆ·çš„å›¾åƒã€‚ä¸ºè§£å†³æ­¤ç¼ºå£ï¼Œæˆ‘ä»¬æ¨å‡ºMathRealæ•°æ®é›†ï¼ŒåŒ…å«ä¸¤åƒä¸ªæ•°å­¦é—®é¢˜å’Œé€šè¿‡æ‰‹æŒç§»åŠ¨è®¾å¤‡æ‹æ‘„çš„çœŸå®åœºæ™¯å›¾åƒã€‚æ¯ä¸ªé—®é¢˜éƒ½æ˜¯ä¸€å¼ åŒ…å«é—®é¢˜æ–‡æœ¬å’Œè§†è§‰å…ƒç´ çš„å›¾ç‰‡ã€‚æˆ‘ä»¬å°†çœŸå®å›¾åƒåˆ†ä¸ºä¸‰ç±»ï¼šå›¾åƒè´¨é‡é€€åŒ–ã€è§†è§’å˜åŒ–å’Œæ— å…³å†…å®¹å¹²æ‰°ï¼Œå¹¶è¿›ä¸€æ­¥ç»†åˆ†ä¸ºåå››ä¸ªå­ç±»åˆ«ã€‚æ­¤å¤–ï¼ŒMathRealæ¶µç›–äº”ä¸ªæ ¸å¿ƒçŸ¥è¯†å’Œèƒ½åŠ›ç±»åˆ«ï¼ŒåŒ…æ‹¬ä¸‰ç§é¢˜å‹å’Œä¸‰ä¸ªéš¾åº¦çº§åˆ«ã€‚ä¸ºå…¨é¢è¯„ä¼°æœ€æ–°MLLMsåœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„å¤šåª’ä½“æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬è®¾è®¡äº†å…­ç§å®éªŒè®¾ç½®ï¼Œå¯¹å…¶æ€§èƒ½è¿›è¡Œç³»ç»Ÿçš„åˆ†æã€‚é€šè¿‡å®éªŒå‘ç°ï¼Œç°æœ‰MLLMsåœ¨çœŸå®æ•™è‚²ç¯å¢ƒä¸‹çš„è§£é¢˜èƒ½åŠ›é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¯¹å®ƒä»¬çš„æ€§èƒ½åŠé”™è¯¯æ¨¡å¼è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå¹¶æä¾›äº†å…³äºè¯†åˆ«ã€ç†è§£å’Œæ¨ç†èƒ½åŠ›çš„è§è§£ï¼Œä»¥åŠæœªæ¥æ”¹è¿›çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨è§†è§‰æ•°å­¦æ¨ç†æ–¹é¢å±•ç°å‡ºæ˜¾è‘—èƒ½åŠ›ï¼Œä½†åœ¨çœŸå®æ•™è‚²ç¯å¢ƒä¸‹çš„æ€§èƒ½ä»éœ€æå‡ã€‚</li>
<li>å¼•å…¥MathRealæ•°æ®é›†ï¼ŒåŒ…å«çœŸå®åœºæ™¯ä¸­çš„æ•°å­¦é—®é¢˜å’Œå›¾åƒï¼Œä»¥å¡«è¡¥ç°æœ‰åŸºå‡†æµ‹è¯•çš„ä¸è¶³ã€‚</li>
<li>MathRealæ•°æ®é›†å¯¹çœŸå®å›¾åƒè¿›è¡Œç»†è‡´åˆ†ç±»ï¼ŒåŒ…æ‹¬å›¾åƒè´¨é‡ã€è§†è§’å’Œæ— å…³å†…å®¹ç­‰å› ç´ ã€‚</li>
<li>MathRealæ¶µç›–å¤šç§æ•°å­¦çŸ¥è¯†å’Œèƒ½åŠ›ç±»åˆ«ï¼Œä»¥åŠä¸åŒéš¾åº¦çº§åˆ«çš„é¢˜ç›®ã€‚</li>
<li>é€šè¿‡å…­ç§å®éªŒè®¾ç½®å…¨é¢è¯„ä¼°MLLMsåœ¨çœŸå®åœºæ™¯ä¸­çš„å¤šåª’ä½“æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>MLLMsåœ¨çœŸå®æ•™è‚²ç¯å¢ƒä¸‹çš„è§£é¢˜èƒ½åŠ›é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«ã€ç†è§£å’Œæ¨ç†æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06009">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c9c926c0b267b181aa6fe803fd924764.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71afb82f94639f13c0928a197ddfd545.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b972739270ed5468c570d25cdc4aa09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78ac4c8735c6630407bd65bd751aa451.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-089830a639293a1f5c7f5e8526764ab6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0113a0cabf8153b1bf90772e5e3fbd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6dfb4a9c3bbee454b92d8bfcc1cb47b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Pruning-the-Unsurprising-Efficient-Code-Reasoning-via-First-Token-Surprisal"><a href="#Pruning-the-Unsurprising-Efficient-Code-Reasoning-via-First-Token-Surprisal" class="headerlink" title="Pruning the Unsurprising: Efficient Code Reasoning via First-Token   Surprisal"></a>Pruning the Unsurprising: Efficient Code Reasoning via First-Token   Surprisal</h2><p><strong>Authors:Wenhao Zeng, Yaoning Wang, Chao Hu, Yuling Shi, Chengcheng Wan, Hongyu Zhang, Xiaodong Gu</strong></p>
<p>Recently, Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in code reasoning by scaling up the length of Chain-of-Thought (CoT). However, excessively long reasoning traces introduce substantial challenges in terms of training cost, inference latency, and deployment feasibility. While various CoT compression approaches have emerged to address this challenge, they face inherent trade-offs: token-level methods often disrupt syntactic and logical coherence, while step-level methods based on perplexity fail to reliably capture the logically critical reasoning steps. In this paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided pruning to preserve the core reasoning structure, which efficiently reduces the search space for subsequent processing. It then enables a logic-aware pruning by selecting logically essential reasoning steps based on a novel first-token surprisal metric. Finally, ASAP teaches models to autonomously generate and leverage these concise CoTs at inference time, enabling efficient reasoning in coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy across multiple code generation benchmarks while substantially reducing training and inference costs. On the challenging LiveCodeBench v4_v5 benchmark, our approach reduces token generation by 23.5% and inference latency by 43.5% compared to the strongest baseline, while achieving a competitive accuracy of 36.19% in Pass@1. Our results highlight a promising direction for building powerful and efficient LRMs. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡æ‰©å¤§æ€ç»´é“¾ï¼ˆCoTï¼‰çš„é•¿åº¦åœ¨ä»£ç æ¨ç†æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿‡é•¿çš„æ¨ç†ç—•è¿¹å¸¦æ¥äº†è®­ç»ƒæˆæœ¬ã€æ¨ç†å»¶è¿Ÿå’Œéƒ¨ç½²å¯è¡Œæ€§æ–¹é¢çš„å·¨å¤§æŒ‘æˆ˜ã€‚è™½ç„¶å‡ºç°äº†å„ç§CoTå‹ç¼©æ–¹æ³•æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œä½†å®ƒä»¬é¢ä¸´ç€å›ºæœ‰çš„æƒè¡¡ï¼šåŸºäºç¬¦å·å±‚çº§çš„æ–¹æ³•ç»å¸¸ç ´åè¯­æ³•å’Œé€»è¾‘è¿è´¯æ€§ï¼Œè€ŒåŸºäºå›°æƒ‘åº¦çš„æ­¥éª¤å±‚çº§æ–¹æ³•æ— æ³•å¯é åœ°æ•è·é€»è¾‘ä¸Šå…³é”®çš„æ¨ç†æ­¥éª¤ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºé”šç‚¹å¼•å¯¼ã€åŸºäºæƒŠå¥‡åº¦çš„ä¿®å‰ªï¼ˆASAPï¼‰è¿™ä¸€æ–°é¢–çš„ç²—åˆ°ç»†æ¡†æ¶æ¥è¿›è¡ŒCoTå‹ç¼©ã€‚ASAPé¦–å…ˆæ‰§è¡ŒåŸºäºé”šç‚¹çš„ä¿®å‰ªæ¥ä¿ç•™æ ¸å¿ƒæ¨ç†ç»“æ„ï¼Œè¿™æœ‰æ•ˆåœ°å‡å°‘äº†åç»­å¤„ç†è¿‡ç¨‹ä¸­çš„æœç´¢ç©ºé—´ã€‚ç„¶åå®ƒé‡‡ç”¨åŸºäºæ–°æå‡ºçš„ç¬¬ä¸€ç¬¦å·æƒŠå¥‡åº¦æŒ‡æ ‡çš„é€»è¾‘æ„ŸçŸ¥ä¿®å‰ªæ–¹æ³•é€‰æ‹©é€»è¾‘ä¸Šå¿…è¦çš„æ¨ç†æ­¥éª¤ã€‚æœ€åï¼ŒASAPè®­ç»ƒæ¨¡å‹åœ¨æ¨ç†æ—¶è‡ªä¸»ç”Ÿæˆå’Œåˆ©ç”¨è¿™äº›ç®€æ´çš„CoTï¼Œä»è€Œåœ¨ç¼–ç ä»»åŠ¡ä¸­å®ç°é«˜æ•ˆæ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒASAPåœ¨å¤šä¸ªä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†è®­ç»ƒå’Œæ¨ç†æˆæœ¬ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„LiveCodeBench v4_v5åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºæœ€å¼ºåŸºçº¿å°†ç¬¦å·ç”Ÿæˆå‡å°‘äº†23.5%ï¼Œæ¨ç†å»¶è¿Ÿå‡å°‘äº†43.5%ï¼ŒåŒæ—¶åœ¨Pass@1çš„å‡†ç¡®ç‡è¾¾åˆ°äº†36.19%ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå±•ç¤ºäº†æ„å»ºå¼ºå¤§é«˜æ•ˆLRMçš„æœ‰å‰é€”çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05988v1">PDF</a> Code and model available at <a target="_blank" rel="noopener" href="https://github.com/Zengwh02/ASAP">https://github.com/Zengwh02/ASAP</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºASAPçš„åŸºäºé”šç‚¹å¼•å¯¼ä¸æ–°å¥‡åº¦åº¦é‡çš„ç²—åˆ°ç»†æ¡†æ¶ï¼Œç”¨äºå¤§å‹æ¨ç†æ¨¡å‹çš„æ¨ç†é“¾å‹ç¼©ã€‚ASAPé¦–å…ˆé€šè¿‡é”šç‚¹å¼•å¯¼ä¿®å‰ªä¿ç•™æ ¸å¿ƒæ¨ç†ç»“æ„ï¼Œç„¶åé€šè¿‡åŸºäºæ–°å¥‡åº¦åº¦é‡é€‰æ‹©é€»è¾‘ä¸Šå…³é”®çš„æ¨ç†æ­¥éª¤å®ç°é€»è¾‘æ„ŸçŸ¥ä¿®å‰ªã€‚å®éªŒè¡¨æ˜ï¼ŒASAPåœ¨å¤šä¸ªä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†è®­ç»ƒå’Œæ¨ç†æˆæœ¬ã€‚åœ¨LiveCodeBench v4_v5åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸æœ€å¼ºåŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†ä»¤ç‰Œç”Ÿæˆå‡å°‘äº†23.5%ï¼Œæ¨ç†å»¶è¿Ÿå‡å°‘äº†43.5%ï¼ŒåŒæ—¶å‡†ç¡®ç‡è¾¾åˆ°äº†å…·æœ‰ç«äº‰åŠ›çš„36.19%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨ä»£ç æ¨ç†ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†è¿‡é•¿çš„æ¨ç†é“¾å¸¦æ¥äº†è®­ç»ƒæˆæœ¬ã€æ¨ç†å»¶è¿Ÿå’Œéƒ¨ç½²å¯è¡Œæ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ¨ç†é“¾å‹ç¼©æ–¹æ³•é¢ä¸´æƒè¡¡é—®é¢˜ï¼šåŸºäºä»¤ç‰Œçš„æ–¹æ³•å¯èƒ½ç ´åè¯­æ³•å’Œé€»è¾‘è¿è´¯æ€§ï¼Œè€ŒåŸºäºå›°æƒ‘åº¦çš„æ­¥éª¤çº§æ–¹æ³•æ— æ³•å¯é åœ°æ•è·é€»è¾‘ä¸Šå…³é”®æ­¥éª¤ã€‚</li>
<li>æå‡ºçš„ASAPæ¡†æ¶ç»“åˆäº†é”šç‚¹å¼•å¯¼çš„ä¿®å‰ªå’ŒåŸºäºæ–°å¥‡åº¦åº¦é‡çš„é€»è¾‘æ„ŸçŸ¥ä¿®å‰ªæ¥ä¼˜åŒ–æ¨ç†é“¾ã€‚</li>
<li>ASAPé€šè¿‡ä¿ç•™æ ¸å¿ƒæ¨ç†ç»“æ„æœ‰æ•ˆåœ°å‡å°‘äº†æœç´¢ç©ºé—´ï¼Œå¹¶é€šè¿‡é€‰æ‹©é€»è¾‘ä¸Šå…³é”®çš„æ¨ç†æ­¥éª¤æ¥å®ç°é«˜æ•ˆæ¨ç†ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒASAPåœ¨å¤šä¸ªä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—é™ä½äº†è®­ç»ƒå’Œæ¨ç†æˆæœ¬ã€‚</li>
<li>åœ¨LiveCodeBench v4_v5åŸºå‡†æµ‹è¯•ä¸­ï¼ŒASAPç›¸å¯¹äºæœ€å¼ºåŸºçº¿æ˜¾è‘—å‡å°‘äº†ä»¤ç‰Œç”Ÿæˆå’Œæ¨ç†å»¶è¿Ÿï¼ŒåŒæ—¶ä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b73f301f9961a7179a03b03211d1697c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5380d98eae762373190024c5607977e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a1d1724a98bcf8d1dc585c261ee70767.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80c4d61bb8c8286dcf3073e98909f7cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a77bdb7fff00b241ad6c4b01cbe6baf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff27a3ec04ace09d4706a309cc060507.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="PASG-A-Closed-Loop-Framework-for-Automated-Geometric-Primitive-Extraction-and-Semantic-Anchoring-in-Robotic-Manipulation"><a href="#PASG-A-Closed-Loop-Framework-for-Automated-Geometric-Primitive-Extraction-and-Semantic-Anchoring-in-Robotic-Manipulation" class="headerlink" title="PASG: A Closed-Loop Framework for Automated Geometric Primitive   Extraction and Semantic Anchoring in Robotic Manipulation"></a>PASG: A Closed-Loop Framework for Automated Geometric Primitive   Extraction and Semantic Anchoring in Robotic Manipulation</h2><p><strong>Authors:Zhihao Zhu, Yifan Zheng, Siyu Pan, Yaohui Jin, Yao Mu</strong></p>
<p>The fragmentation between high-level task semantics and low-level geometric features remains a persistent challenge in robotic manipulation. While vision-language models (VLMs) have shown promise in generating affordance-aware visual representations, the lack of semantic grounding in canonical spaces and reliance on manual annotations severely limit their ability to capture dynamic semantic-affordance relationships. To address these, we propose Primitive-Aware Semantic Grounding (PASG), a closed-loop framework that introduces: (1) Automatic primitive extraction through geometric feature aggregation, enabling cross-category detection of keypoints and axes; (2) VLM-driven semantic anchoring that dynamically couples geometric primitives with functional affordances and task-relevant description; (3) A spatial-semantic reasoning benchmark and a fine-tuned VLM (Qwen2.5VL-PA). We demonstrate PASGâ€™s effectiveness in practical robotic manipulation tasks across diverse scenarios, achieving performance comparable to manual annotations. PASG achieves a finer-grained semantic-affordance understanding of objects, establishing a unified paradigm for bridging geometric primitives with task semantics in robotic manipulation. </p>
<blockquote>
<p>åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œé«˜çº§ä»»åŠ¡è¯­ä¹‰ä¸ä½çº§å‡ ä½•ç‰¹å¾ä¹‹é—´çš„ç¢ç‰‡åŒ–ä»ç„¶æ˜¯ä¸€ä¸ªæŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚è™½ç„¶è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨ç”Ÿæˆå¯è´Ÿæ‹…çš„è§†è§‰è¡¨å¾æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†åœ¨è§„èŒƒç©ºé—´ä¸­ç¼ºä¹è¯­ä¹‰æ¥åœ°ä»¥åŠå¯¹æ‰‹åŠ¨æ³¨é‡Šçš„ä¾èµ–ï¼Œä¸¥é‡é™åˆ¶äº†å…¶æ•è·åŠ¨æ€è¯­ä¹‰å¯è´Ÿæ‹…å…³ç³»çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸå§‹è¯­ä¹‰æ„ŸçŸ¥æ¥åœ°ï¼ˆPASGï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé—­ç¯æ¡†æ¶ï¼Œå¼•å…¥äº†ï¼šï¼ˆ1ï¼‰é€šè¿‡å‡ ä½•ç‰¹å¾èšåˆè‡ªåŠ¨æå–åŸå§‹ç‰¹å¾ï¼Œå®ç°è·¨ç±»åˆ«æ£€æµ‹å…³é”®ç‚¹å’Œè½´ï¼›ï¼ˆ2ï¼‰ç”±VLMé©±åŠ¨çš„è¯­ä¹‰é”šå®šï¼ŒåŠ¨æ€åœ°å°†å‡ ä½•åŸå§‹æ•°æ®ä¸åŠŸèƒ½å¯è´Ÿæ‹…æ€§å’Œä»»åŠ¡ç›¸å…³æè¿°ç›¸ç»“åˆï¼›ï¼ˆ3ï¼‰ç©ºé—´è¯­ä¹‰æ¨ç†åŸºå‡†æµ‹è¯•å’Œå¾®è°ƒVLMï¼ˆQwen2.5VL-PAï¼‰ã€‚æˆ‘ä»¬åœ¨å¤šç§åœºæ™¯çš„å®ç”¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¯æ˜äº†PASGçš„æœ‰æ•ˆæ€§ï¼Œå…¶æ€§èƒ½å¯ä¸æ‰‹åŠ¨æ³¨é‡Šç›¸å½“ã€‚PASGå®ç°äº†å¯¹å¯¹è±¡çš„æ›´ç²¾ç»†çš„è¯­ä¹‰å¯è´Ÿæ‹…ç†è§£ï¼Œä¸ºæœºå™¨äººæ“ä½œä¸­çš„å‡ ä½•åŸå§‹ä¸ä»»åŠ¡è¯­ä¹‰ä¹‹é—´å»ºç«‹äº†ç»Ÿä¸€çš„æ¡¥æ¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05976v1">PDF</a> Accepted to ICCV 2025. 8 pages main paper, 8 figures, plus   supplementary material</p>
<p><strong>Summary</strong></p>
<p>åœ¨æœºå™¨äººæ“ä½œé¢†åŸŸï¼Œé«˜çº§ä»»åŠ¡è¯­ä¹‰ä¸ä½çº§å‡ ä½•ç‰¹å¾ä¹‹é—´çš„ç¢ç‰‡åŒ–ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Primitive-Aware Semantic Groundingï¼ˆPASGï¼‰æ¡†æ¶ï¼Œå¼•å…¥è‡ªåŠ¨æå–å‡ ä½•ç‰¹å¾ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œè¯­ä¹‰é”šå®šï¼Œå¹¶æ„å»ºç©ºé—´è¯­ä¹‰æ¨ç†åŸºå‡†æµ‹è¯•ã€‚PASGå®ç°äº†ç²¾ç»†åŒ–è¯­ä¹‰-é€‚ç”¨æ€§çš„å¯¹è±¡ç†è§£ï¼Œå»ºç«‹äº†è¿æ¥å‡ ä½•åŸå§‹ä»»åŠ¡å’Œè¯­ä¹‰çš„æ¡¥æ¢ï¼Œåœ¨å®é™…æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨äººæ“ä½œé¢†åŸŸå­˜åœ¨é«˜çº§ä»»åŠ¡è¯­ä¹‰ä¸ä½çº§å‡ ä½•ç‰¹å¾ä¹‹é—´çš„ç¢ç‰‡åŒ–æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç”Ÿæˆé€‚ç”¨æ€§æ„ŸçŸ¥è§†è§‰è¡¨ç¤ºæ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ç¼ºä¹åœ¨è§„èŒƒç©ºé—´ä¸­çš„è¯­ä¹‰é”šå®šï¼Œä¸”ä¾èµ–æ‰‹åŠ¨æ³¨é‡Šï¼Œé™åˆ¶äº†å…¶æ•æ‰åŠ¨æ€è¯­ä¹‰-é€‚ç”¨æ€§å…³ç³»çš„èƒ½åŠ›ã€‚</li>
<li>PASGæ¡†æ¶å¼•å…¥è‡ªåŠ¨æå–å‡ ä½•ç‰¹å¾ï¼Œé€šè¿‡å‡ ä½•ç‰¹å¾èšåˆè¿›è¡Œå…³é”®ç‚¹å’Œè½´çš„è·¨ç±»åˆ«æ£€æµ‹ã€‚</li>
<li>PASGåˆ©ç”¨VLMè¿›è¡Œè¯­ä¹‰é”šå®šï¼ŒåŠ¨æ€åœ°å°†å‡ ä½•åŸå§‹ä¸åŠŸèƒ½é€‚ç”¨æ€§å’Œä»»åŠ¡ç›¸å…³æè¿°ç›¸ç»“åˆã€‚</li>
<li>PASGå»ºç«‹äº†ä¸€ä¸ªç©ºé—´è¯­ä¹‰æ¨ç†åŸºå‡†æµ‹è¯•å’Œå¾®è°ƒè¿‡çš„VLMï¼ˆQwen2.5VL-PAï¼‰ã€‚</li>
<li>PASGåœ¨å®é™…æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„å¤šæ ·åŒ–åœºæ™¯åº”ç”¨æœ‰æ•ˆï¼Œå…¶æ€§èƒ½å¯ä¸æ‰‹åŠ¨æ³¨é‡Šç›¸åª²ç¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05976">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-43f89bc00abb17ca915dce821d99e548.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0c6dbbdd48bd4a6e551482246aba3c08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3718d5df33baeb56e173d0cf3037bb3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad94e10fa49db7a03330e44497e6f240.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bb639c2750cf6c03bf74174f2c14b5f6.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Dean-of-LLM-Tutors-Exploring-Comprehensive-and-Automated-Evaluation-of-LLM-generated-Educational-Feedback-via-LLM-Feedback-Evaluators"><a href="#Dean-of-LLM-Tutors-Exploring-Comprehensive-and-Automated-Evaluation-of-LLM-generated-Educational-Feedback-via-LLM-Feedback-Evaluators" class="headerlink" title="Dean of LLM Tutors: Exploring Comprehensive and Automated Evaluation of   LLM-generated Educational Feedback via LLM Feedback Evaluators"></a>Dean of LLM Tutors: Exploring Comprehensive and Automated Evaluation of   LLM-generated Educational Feedback via LLM Feedback Evaluators</h2><p><strong>Authors:Keyang Qian, Yixin Cheng, Rui Guan, Wei Dai, Flora Jin, Kaixun Yang, Sadia Nawaz, Zachari Swiecki, Guanliang Chen, Lixiang Yan, Dragan GaÅ¡eviÄ‡</strong></p>
<p>The use of LLM tutors to provide automated educational feedback to students on student assignment submissions has received much attention in the AI in Education field. However, the stochastic nature and tendency for hallucinations in LLMs can undermine both quality of learning experience and adherence to ethical standards. To address this concern, we propose a method that uses LLM feedback evaluators (DeanLLMs) to automatically and comprehensively evaluate feedback generated by LLM tutor for submissions on university assignments before it is delivered to students. This allows low-quality feedback to be rejected and enables LLM tutors to improve the feedback they generated based on the evaluation results. We first proposed a comprehensive evaluation framework for LLM-generated educational feedback, comprising six dimensions for feedback content, seven for feedback effectiveness, and three for hallucination types. Next, we generated a virtual assignment submission dataset covering 85 university assignments from 43 computer science courses using eight commonly used commercial LLMs. We labelled and open-sourced the assignment dataset to support the fine-tuning and evaluation of LLM feedback evaluators. Our findings show that o3-pro demonstrated the best performance in zero-shot labelling of feedback while o4-mini demonstrated the best performance in few-shot labelling of feedback. Moreover, GPT-4.1 achieved human expert level performance after fine-tuning (Accuracy 79.8%, F1-score 79.4%; human average Accuracy 78.3%, F1-score 82.6%). Finally, we used our best-performance model to evaluate 2,000 assignment feedback instances generated by 10 common commercial LLMs, 200 each, to compare the quality of feedback generated by different LLMs. Our LLM feedback evaluator method advances our ability to automatically provide high-quality and reliable educational feedback to students. </p>
<blockquote>
<p>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¾…å¯¼å‘˜ä¸ºå­¦ç”Ÿä½œä¸šæäº¤æä¾›è‡ªåŠ¨åŒ–æ•™è‚²åé¦ˆåœ¨äººå·¥æ™ºèƒ½æ•™è‚²é¢†åŸŸä¸­å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„éšæœºæ€§å’Œå‡ºç°å¹»è§‰çš„å€¾å‘å¯èƒ½ä¼šç ´åå­¦ä¹ ä½“éªŒçš„è´¨é‡å’Œéµå®ˆé“å¾·æ ‡å‡†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æ‹…å¿§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨LLMåé¦ˆè¯„ä¼°å™¨ï¼ˆDeanLLMsï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥è‡ªåŠ¨å’Œå…¨é¢åœ°è¯„ä¼°LLMè¾…å¯¼å‘˜ç”Ÿæˆçš„å¤§å­¦ä½œä¸šæäº¤åé¦ˆï¼Œç„¶åå†å°†å…¶é€’é€ç»™å­¦ç”Ÿã€‚è¿™å…è®¸æ‹’ç»ä½è´¨é‡çš„åé¦ˆï¼Œå¹¶ä½¿å¾—LLMè¾…å¯¼å‘˜å¯ä»¥æ ¹æ®è¯„ä¼°ç»“æœæ”¹è¿›å…¶ç”Ÿæˆçš„åé¦ˆã€‚æˆ‘ä»¬é¦–å…ˆä¸ºLLMç”Ÿæˆçš„æ•™è‚²åé¦ˆæå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬å…­ä¸ªå…³äºåé¦ˆå†…å®¹çš„ç»´åº¦ï¼Œä¸ƒä¸ªå…³äºåé¦ˆæ•ˆæœçš„ç»´åº¦ï¼Œä»¥åŠä¸‰ä¸ªå…³äºå¹»è§‰ç±»å‹çš„ç»´åº¦ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨å…«ç§å¸¸è§å•†ä¸šLLMç”Ÿæˆäº†ä¸€ä¸ªè™šæ‹Ÿä½œä¸šæäº¤æ•°æ®é›†ï¼Œæ¶µç›–äº†æ¥è‡ª43é—¨è®¡ç®—æœºç§‘å­¦è¯¾ç¨‹çš„85é¡¹ä½œä¸šã€‚æˆ‘ä»¬å¯¹ä½œä¸šæ•°æ®é›†è¿›è¡Œäº†æ ‡æ³¨å¹¶å¼€æºï¼Œä»¥æ”¯æŒLLMåé¦ˆè¯„ä¼°å™¨çš„å¾®è°ƒä¸è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨é›¶æ ·æœ¬æ ‡æ³¨åé¦ˆæ–¹é¢ï¼Œo3-proè¡¨ç°æœ€ä½³ï¼Œè€Œåœ¨å°‘æ ·æœ¬æ ‡æ³¨åé¦ˆæ–¹é¢ï¼Œo4-miniè¡¨ç°æœ€ä½³ã€‚æ­¤å¤–ï¼ŒGPT-4.1åœ¨ç»è¿‡å¾®è°ƒåè¾¾åˆ°äº†äººç±»ä¸“å®¶çš„æ°´å¹³ï¼ˆå‡†ç¡®ç‡79.8%ï¼ŒF1åˆ†æ•°79.4%ï¼›äººç±»å¹³å‡å‡†ç¡®ç‡78.3%ï¼ŒF1åˆ†æ•°82.6%ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨è¡¨ç°æœ€ä½³çš„æ¨¡å‹è¯„ä¼°äº†ç”±åç§å¸¸è§å•†ä¸šLLMç”Ÿæˆçš„2000ä¸ªä½œä¸šåé¦ˆå®ä¾‹ï¼ˆæ¯ç§LLM 200ä¸ªï¼‰ï¼Œä»¥æ¯”è¾ƒä¸åŒLLMç”Ÿæˆçš„åé¦ˆè´¨é‡ã€‚æˆ‘ä»¬çš„LLMåé¦ˆè¯„ä¼°å™¨æ–¹æ³•æé«˜äº†æˆ‘ä»¬ä¸ºå­¦ç”Ÿè‡ªåŠ¨æä¾›é«˜è´¨é‡å’Œå¯é æ•™è‚²åé¦ˆçš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05952v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶å…³æ³¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å­¦ç”Ÿä½œä¸šåé¦ˆä¸­çš„åº”ç”¨ã€‚ç„¶è€Œï¼ŒLLMçš„éšæœºæ€§å’Œäº§ç”Ÿå¹»è§‰çš„å€¾å‘å¯èƒ½ä¼šå½±å“å­¦ç”Ÿçš„å­¦ä¹ ä½“éªŒå’Œé“å¾·æ ‡å‡†çš„éµå®ˆã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§ä½¿ç”¨LLMåé¦ˆè¯„ä¼°å™¨ï¼ˆDeanLLMsï¼‰çš„æ–¹æ³•ï¼Œè‡ªåŠ¨å…¨é¢è¯„ä¼°LLMå¯¼å¸ˆå¯¹å­¦ç”Ÿä½œä¸šçš„åé¦ˆè´¨é‡ã€‚è¯¥æ–¹æ³•èƒ½å‰”é™¤ä½è´¨é‡çš„åé¦ˆï¼Œä½¿LLMå¯¼å¸ˆæ ¹æ®è¯„ä¼°ç»“æœæ”¹è¿›å…¶ç”Ÿæˆçš„åé¦ˆã€‚ç ”ç©¶æ„å»ºäº†LLMç”Ÿæˆæ•™è‚²åé¦ˆçš„å…¨é¢è¯„ä¼°æ¡†æ¶ï¼Œå¹¶ç”Ÿæˆäº†ä¸€ä¸ªè™šæ‹Ÿä½œä¸šæäº¤æ•°æ®é›†ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒæŸäº›æ¨¡å‹åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ ‡æ³¨åé¦ˆæ–¹é¢è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”GPT-4.1åœ¨å¾®è°ƒåè¾¾åˆ°äº†äººç±»ä¸“å®¶çº§çš„æ€§èƒ½ã€‚æœ€åï¼Œç ”ç©¶ä½¿ç”¨æœ€ä½³æ€§èƒ½æ¨¡å‹è¯„ä¼°äº†ä¸åŒLLMç”Ÿæˆçš„ä½œä¸šåé¦ˆè´¨é‡ã€‚æ­¤æ–¹æ³•æé«˜äº†ä¸ºå­¦ç”Ÿè‡ªåŠ¨æä¾›é«˜è´¨é‡å¯é æ•™è‚²åé¦ˆçš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMè¢«ç”¨äºä¸ºå­¦ç”Ÿæä¾›è‡ªåŠ¨åŒ–æ•™è‚²åé¦ˆï¼Œä½†å…¶éšæœºæ€§å’Œäº§ç”Ÿå¹»è§‰çš„å€¾å‘å¯èƒ½å½±å“å­¦ä¹ ä½“éªŒå’Œé“å¾·æ ‡å‡†ã€‚</li>
<li>æå‡ºäº†ä½¿ç”¨LLMåé¦ˆè¯„ä¼°å™¨ï¼ˆDeanLLMsï¼‰çš„æ–¹æ³•ï¼Œè‡ªåŠ¨å…¨é¢è¯„ä¼°LLMç”Ÿæˆçš„åé¦ˆè´¨é‡ã€‚</li>
<li>ç ”ç©¶æ„å»ºäº†å…¨é¢çš„LLMæ•™è‚²åé¦ˆè¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬åé¦ˆå†…å®¹ã€åé¦ˆæœ‰æ•ˆæ€§ã€å¹»è§‰ç±»å‹ç­‰å¤šä¸ªç»´åº¦ã€‚</li>
<li>ç”Ÿæˆäº†ä¸€ä¸ªæ¶µç›–85ä¸ªå¤§å­¦ä½œä¸šçš„è™šæ‹Ÿä½œä¸šæäº¤æ•°æ®é›†ï¼Œå¹¶å¼€æºä»¥æ”¯æŒLLMåé¦ˆè¯„ä¼°å™¨çš„å¾®è°ƒã€‚</li>
<li>æŸäº›æ¨¡å‹åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ ‡æ³¨åé¦ˆæ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
<li>GPT-4.1åœ¨å¾®è°ƒåè¾¾åˆ°äº†äººç±»ä¸“å®¶çº§çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05952">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e9150ad5054987fda80cf103440fa8c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-517867bb87f1821ddb70e1acf73b126b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fc45a0845b250564b010c4793d59210.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64bed3f9c6e5b220b70231eeb6987589.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Mitigating-Think-Answer-Mismatch-in-LLM-Reasoning-Through-Noise-Aware-Advantage-Reweighting"><a href="#Mitigating-Think-Answer-Mismatch-in-LLM-Reasoning-Through-Noise-Aware-Advantage-Reweighting" class="headerlink" title="Mitigating Think-Answer Mismatch in LLM Reasoning Through Noise-Aware   Advantage Reweighting"></a>Mitigating Think-Answer Mismatch in LLM Reasoning Through Noise-Aware   Advantage Reweighting</h2><p><strong>Authors:Si Shen, Peijun Shen, Wenhua Zhao, Danhao Zhu</strong></p>
<p>Group-Relative Policy Optimization (GRPO) is a key technique for training large reasoning models, yet it suffers from a critical vulnerability: the \emph{Think-Answer Mismatch}, where noisy reward signals corrupt the learning process. This problem is most severe in unbalanced response groups, paradoxically degrading the signal precisely when it should be most informative. To address this challenge, we propose Stable Group-Relative Policy Optimization (S-GRPO), a principled enhancement that derives optimal, noise-aware advantage weights to stabilize training. Our comprehensive experiments on mathematical reasoning benchmarks demonstrate S-GRPOâ€™s effectiveness and robustness. On various models, S-GRPO significantly outperforms DR. GRPO, achieving performance gains of +2.5% on Qwen-Math-7B-Base, +2.2% on Llama-3.2-3B-Base, and +2.4% on Qwen-Math-1.5B-Instruct. Most critically, while standard GRPO fails to learn under 20% synthetic reward noise, S-GRPO maintains stable learning progress. These results highlight S-GRPOâ€™s potential for more robust and effective training of large-scale reasoning models. \footnote{Code and data are available at: <a target="_blank" rel="noopener" href="https://github.com/shenpeijun0212/S-GRPO">https://github.com/shenpeijun0212/S-GRPO</a> </p>
<blockquote>
<p>ç›¸å¯¹ç»„ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ˜¯è®­ç»ƒå¤§å‹æ¨ç†æ¨¡å‹çš„å…³é”®æŠ€æœ¯ï¼Œä½†å®ƒå­˜åœ¨ä¸€ä¸ªä¸¥é‡çš„æ¼æ´ï¼Œå³â€œæ€ç»´-ç­”æ¡ˆä¸åŒ¹é…â€ï¼Œå…¶ä¸­å˜ˆæ‚çš„å¥–åŠ±ä¿¡å·ä¼šç ´åå­¦ä¹ è¿‡ç¨‹ã€‚åœ¨ä¸å¹³è¡¡çš„å“åº”ç»„ä¸­ï¼Œè¿™ä¸ªé—®é¢˜æœ€ä¸ºä¸¥é‡ï¼Œå…·æœ‰è®½åˆºæ„å‘³çš„æ˜¯ï¼Œå®ƒä¼šåœ¨ä¿¡å·æœ€åº”è¯¥å…·æœ‰ä¿¡æ¯å«é‡çš„æ—¶åˆ»å¯¼è‡´ä¿¡å·æ¶åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç¨³å®šç›¸å¯¹ç»„ç­–ç•¥ä¼˜åŒ–ï¼ˆS-GRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æœ‰åŸåˆ™çš„å¢å¼ºæ–¹æ³•ï¼Œå®ƒæ¨å¯¼å‡ºæœ€ä¼˜çš„å™ªå£°æ„ŸçŸ¥ä¼˜åŠ¿æƒé‡æ¥ç¨³å®šè®­ç»ƒã€‚æˆ‘ä»¬åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¯æ˜äº†S-GRPOçš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚åœ¨å„ç§æ¨¡å‹ä¸Šï¼ŒS-GRPOæ˜¾è‘—ä¼˜äºDR. GRPOï¼Œåœ¨Qwen-Math-7B-Baseä¸Šæ€§èƒ½æå‡+2.5%ï¼Œåœ¨Llama-3.2-3B-Baseä¸Šæå‡+2.2%ï¼Œåœ¨Qwen-Math-1.5B-Instructä¸Šæå‡+2.4%ã€‚æœ€å…³é”®çš„æ˜¯ï¼Œå½“æ ‡å‡†GRPOåœ¨åˆæˆå¥–åŠ±å™ªå£°ä¸‹æ— æ³•å­¦ä¹ æ—¶ï¼ŒS-GRPOèƒ½å¤Ÿä¿æŒç¨³å®šçš„å­¦ä¹ è¿›åº¦ã€‚è¿™äº›ç»“æœçªå‡ºäº†S-GRPOåœ¨æ›´ç¨³å¥å’Œæœ‰æ•ˆåœ°è®­ç»ƒå¤§è§„æ¨¡æ¨ç†æ¨¡å‹æ–¹é¢çš„æ½œåŠ›ã€‚æ³¨ï¼šä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/shenpeijun0212/S-GRPO%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/shenpeijun0212/S-GRPOæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05928v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å›¢é˜Ÿç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åœ¨è®­ç»ƒå¤§å‹æ¨ç†æ¨¡å‹æ—¶å­˜åœ¨å…³é”®æ¼æ´ï¼Œå³â€œæ€è€ƒ-å›ç­”ä¸åŒ¹é…â€é—®é¢˜ï¼Œå¯¼è‡´å¥–åŠ±ä¿¡å·ä¸­çš„å™ªå£°ä¼šç ´åå­¦ä¹ è¿‡ç¨‹ã€‚åœ¨å“åº”ä¸å¹³è¡¡çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸€é—®é¢˜å°¤ä¸ºä¸¥é‡ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç¨³å®šå›¢é˜Ÿç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆS-GRPOï¼‰ï¼Œé€šè¿‡æ¨å¯¼æœ€ä¼˜çš„å™ªå£°æ„ŸçŸ¥ä¼˜åŠ¿æƒé‡æ¥ç¨³å®šè®­ç»ƒã€‚åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒS-GRPOåœ¨å¤šç§æ¨¡å‹ä¸Šçš„è¡¨ç°ä¼˜äºDR. GRPOï¼Œæ€§èƒ½æå‡åˆ†åˆ«ä¸ºQwen-Math-7B-Baseæå‡2.5%ï¼ŒLlama-3.2-3B-Baseæå‡2.2%ï¼ŒQwen-Math-1.5B-Instructæå‡2.4%ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæ ‡å‡†GRPOåœ¨åˆæˆå¥–åŠ±å™ªå£°è¶…è¿‡20%çš„æƒ…å†µä¸‹æ— æ³•è¿›è¡Œå­¦ä¹ ï¼Œè€ŒS-GRPOåˆ™èƒ½ç»´æŒç¨³å®šçš„å­¦ä¹ è¿›åº¦ã€‚è¿™çªæ˜¾äº†S-GRPOåœ¨æ›´ç¨³å¥ã€æ›´æœ‰æ•ˆåœ°è®­ç»ƒå¤§è§„æ¨¡æ¨ç†æ¨¡å‹æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Group-Relative Policy Optimization (GRPO) åœ¨è®­ç»ƒå¤§å‹æ¨ç†æ¨¡å‹æ—¶é¢ä¸´ â€œThink-Answer Mismatchâ€ é—®é¢˜ã€‚</li>
<li>åœ¨å“åº”ä¸å¹³è¡¡çš„æƒ…å†µä¸‹ï¼Œè¯¥é—®é¢˜æ›´ä¸ºä¸¥é‡ï¼Œå™ªå£°å¥–åŠ±ä¿¡å·ä¼šç ´åå­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæå‡ºäº†Stable Group-Relative Policy Optimization (S-GRPO)ã€‚</li>
<li>S-GRPO é€šè¿‡æ¨å¯¼æœ€ä¼˜çš„å™ªå£°æ„ŸçŸ¥ä¼˜åŠ¿æƒé‡æ¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼ŒS-GRPO æ˜¾è‘—ä¼˜äº DR. GRPOï¼Œæ€§èƒ½æœ‰æ‰€æå‡ã€‚</li>
<li>åœ¨åˆæˆå¥–åŠ±å™ªå£°è¶…è¿‡ 20% çš„æƒ…å†µä¸‹ï¼Œæ ‡å‡† GRPO æ— æ³•å­¦ä¹ ï¼Œè€Œ S-GRPO èƒ½ç»´æŒç¨³å®šå­¦ä¹ è¿›åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05928">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b6f9e7a287583bb77bdcaff847dbff2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-972444568850ee6d0c5216fe2e0554df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba08fe0d2c48b1445048bdc8383fb72c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55a9c3cf99fe54f0c28e748c8981ebdf.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="WebWatcher-Breaking-New-Frontiers-of-Vision-Language-Deep-Research-Agent"><a href="#WebWatcher-Breaking-New-Frontiers-of-Vision-Language-Deep-Research-Agent" class="headerlink" title="WebWatcher: Breaking New Frontiers of Vision-Language Deep Research   Agent"></a>WebWatcher: Breaking New Frontiers of Vision-Language Deep Research   Agent</h2><p><strong>Authors:Xinyu Geng, Peng Xia, Zhen Zhang, Xinyu Wang, Qiuchen Wang, Ruixue Ding, Chenxi Wang, Jialong Wu, Yida Zhao, Kuan Li, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou</strong></p>
<p>Web agents such as Deep Research have demonstrated superhuman cognitive abilities, capable of solving highly challenging information-seeking problems. However, most research remains primarily text-centric, overlooking visual information in the real world. This makes multimodal Deep Research highly challenging, as such agents require much stronger reasoning abilities in perception, logic, knowledge, and the use of more sophisticated tools compared to text-based agents. To address this limitation, we introduce WebWatcher, a multi-modal Agent for Deep Research equipped with enhanced visual-language reasoning capabilities. It leverages high-quality synthetic multimodal trajectories for efficient cold start training, utilizes various tools for deep reasoning, and further enhances generalization through reinforcement learning. To better evaluate the capabilities of multimodal agents, we propose BrowseComp-VL, a benchmark with BrowseComp-style that requires complex information retrieval involving both visual and textual information. Experimental results show that WebWatcher significantly outperforms proprietary baseline, RAG workflow and open-source agents in four challenging VQA benchmarks, which paves the way for solving complex multimodal information-seeking tasks. </p>
<blockquote>
<p>Deep Researchç­‰ç½‘ç»œä»£ç†å·²ç»å±•ç°å‡ºè¶…äººçš„è®¤çŸ¥èƒ½åŠ›ï¼Œèƒ½å¤Ÿè§£å†³é«˜åº¦å…·æœ‰æŒ‘æˆ˜æ€§çš„ä¿¡æ¯æœç´¢é—®é¢˜ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç ”ç©¶ä»ç„¶ä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬ä¸Šï¼Œå¿½è§†äº†ç°å®ä¸–ç•Œä¸­çš„è§†è§‰ä¿¡æ¯ã€‚è¿™ä½¿å¾—å¤šæ¨¡å¼æ·±åº¦ç ”ç©¶æå…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç›¸å¯¹äºåŸºäºæ–‡æœ¬çš„ä»£ç†ï¼Œæ­¤ç±»ä»£ç†éœ€è¦åœ¨æ„ŸçŸ¥ã€é€»è¾‘ã€çŸ¥è¯†æ–¹é¢æ‹¥æœ‰æ›´å¼ºçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶éœ€è¦ä½¿ç”¨æ›´é«˜çº§çš„å·¥å…·ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†WebWatcherï¼Œè¿™æ˜¯ä¸€ä¸ªé…å¤‡å¢å¼ºè§†è§‰è¯­è¨€æ¨ç†èƒ½åŠ›çš„å¤šæ¨¡å¼æ·±åº¦ç ”ç©¶ä»£ç†ã€‚å®ƒåˆ©ç”¨é«˜è´¨é‡åˆæˆå¤šæ¨¡å¼è½¨è¿¹è¿›è¡Œé«˜æ•ˆå†·å¯åŠ¨è®­ç»ƒï¼Œä½¿ç”¨å„ç§å·¥å…·è¿›è¡Œæ·±å…¥æ¨ç†ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥æé«˜æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†æ›´å¥½åœ°è¯„ä¼°å¤šæ¨¡å¼ä»£ç†çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†BrowseComp-VLåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªéœ€è¦æ¶‰åŠè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯çš„å¤æ‚ä¿¡æ¯æ£€ç´¢çš„åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWebWatcheråœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„VQAåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¸“æœ‰åŸºçº¿ã€RAGå·¥ä½œæµç¨‹å’Œå¼€æºä»£ç†ï¼Œè¿™ä¸ºè§£å†³å¤æ‚çš„å¤šåª’ä½“ä¿¡æ¯æœç´¢ä»»åŠ¡å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05748v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ™ºèƒ½ç½‘ç»œä»£ç†å¦‚Deep Researchå±•ç°å‡ºè¶…äººç±»çš„è®¤çŸ¥èƒ½åŠ›ï¼Œèƒ½å¤Ÿè§£å†³æå…·æŒ‘æˆ˜æ€§çš„ä¿¡æ¯æœç´¢é—®é¢˜ï¼Œä½†å¤§å¤šæ•°ç ”ç©¶ä»ç„¶ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒï¼Œå¿½ç•¥äº†ç°å®ä¸–ç•Œä¸­çš„è§†è§‰ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºWebWatcherâ€”â€”é…å¤‡å¢å¼ºè§†è§‰è¯­è¨€æ¨ç†èƒ½åŠ›çš„å¤šæ¨¡æ€æ·±åº¦ç ”ç©¶ä»£ç†ã€‚å®ƒé‡‡ç”¨é«˜è´¨é‡åˆæˆå¤šæ¨¡æ€è½¨è¿¹è¿›è¡Œé«˜æ•ˆçš„å†·å¯åŠ¨è®­ç»ƒï¼Œåˆ©ç”¨å¤šç§å·¥å…·è¿›è¡Œæ·±åº¦æ¨ç†ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ æé«˜æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºè¯„ä¼°å¤šæ¨¡æ€ä»£ç†çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬æ¨å‡ºBrowseComp-VLåŸºå‡†æµ‹è¯•ï¼Œè¦æ±‚æ¶‰åŠè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯çš„å¤æ‚ä¿¡æ¯æ£€ç´¢ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒWebWatcheråœ¨å››ä¸ªæŒ‘æˆ˜æ€§çš„è§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¸“æœ‰åŸºçº¿ã€RAGå·¥ä½œæµç¨‹å’Œå¼€æºä»£ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ™ºèƒ½ç½‘ç»œä»£ç†å¦‚Deep Researchå·²å±•ç°è¶…äººç±»è®¤çŸ¥èƒ½åŠ›ï¼Œåœ¨è§£å†³ä¿¡æ¯æœç´¢é—®é¢˜ä¸Šå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>å¤§å¤šæ•°ç ”ç©¶ä»è¿‡äºä¾èµ–æ–‡æœ¬ä¿¡æ¯ï¼Œå¿½ç•¥äº†è§†è§‰ä¿¡æ¯çš„é‡è¦æ€§ã€‚</li>
<li>å¤šæ¨¡æ€æ·±åº¦ç ”ç©¶é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦æ›´å¼ºçš„æ¨ç†èƒ½åŠ›å’Œæ›´å¤æ‚çš„å·¥å…·ã€‚</li>
<li>WebWatcheræ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ä»£ç†ï¼Œå…·å¤‡å¢å¼ºè§†è§‰è¯­è¨€æ¨ç†èƒ½åŠ›ã€‚</li>
<li>WebWatcheré‡‡ç”¨é«˜æ•ˆå†·å¯åŠ¨è®­ç»ƒï¼Œåˆ©ç”¨å¤šç§å·¥å…·è¿›è¡Œæ·±åº¦æ¨ç†ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ æé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä¸ºè¯„ä¼°å¤šæ¨¡æ€ä»£ç†çš„èƒ½åŠ›ï¼Œæ¨å‡ºäº†BrowseComp-VLåŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05748">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1956dd5fd557688a45dec379ea19cca5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42e172a7439a4d1e6895db200cdacce2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1806ad7a8042101142af4de27f2f34b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-958a8aac9af03c956ddb7532465ee158.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Klear-CodeTest-Scalable-Test-Case-Generation-for-Code-Reinforcement-Learning"><a href="#Klear-CodeTest-Scalable-Test-Case-Generation-for-Code-Reinforcement-Learning" class="headerlink" title="Klear-CodeTest: Scalable Test Case Generation for Code Reinforcement   Learning"></a>Klear-CodeTest: Scalable Test Case Generation for Code Reinforcement   Learning</h2><p><strong>Authors:Jia Fu, Xinyu Yang, Hongzhi Zhang, Yahui Liu, Jingyuan Zhang, Qi Wang, Fuzheng Zhang, Guorui Zhou</strong></p>
<p>Precise, correct feedback is crucial for effectively training large language models (LLMs) in code reinforcement learning. However, synthesizing high-quality test cases remains a profoundly challenging and unsolved problem. In this work, we present Klear-CodeTest, a comprehensive test case synthesis framework featuring rigorous verification to ensure quality and reliability of test cases. Our approach achieves broad coverage of programming problems via a novel Generator-Validation (G-V) framework, ensuring correctness through a consistency validation mechanism that verifies outputs against gold solutions. The proposed G-V framework generates comprehensive test cases including both regular and corner cases, enhancing test coverage and discriminative power for solution correctness assessment in code reinforcement learning. In addition, we design a multi-layered security sandbox system optimized for online verification platforms, guaranteeing safe and reliable code execution. Through comprehensive experiments, we demonstrate the effectiveness of our curated dataset, showing significant improvements in model performance and training stability. The source codes, curated dataset and sandbox system are available at: <a target="_blank" rel="noopener" href="https://github.com/Kwai-Klear/CodeTest">https://github.com/Kwai-Klear/CodeTest</a>. </p>
<blockquote>
<p>ç²¾ç¡®ã€æ­£ç¡®çš„åé¦ˆå¯¹äºåœ¨ä»£ç å¼ºåŒ–å­¦ä¹ ä¸­æœ‰æ•ˆåœ°è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œåˆæˆé«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹ä»ç„¶æ˜¯ä¸€ä¸ªæå…·æŒ‘æˆ˜ä¸”å°šæœªè§£å†³çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Klear-CodeTestï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„æµ‹è¯•ç”¨ä¾‹åˆæˆæ¡†æ¶ï¼Œå…·æœ‰ä¸¥æ ¼éªŒè¯åŠŸèƒ½ï¼Œä»¥ç¡®ä¿æµ‹è¯•ç”¨ä¾‹çš„è´¨é‡å’Œå¯é æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ–°é¢–çš„å‘ç”Ÿå™¨éªŒè¯ï¼ˆG-Vï¼‰æ¡†æ¶å®ç°äº†å¯¹ç¼–ç¨‹é—®é¢˜çš„å¹¿æ³›è¦†ç›–ï¼Œå¹¶é€šè¿‡ä¸€ç§ä¸€è‡´æ€§éªŒè¯æœºåˆ¶æ¥ç¡®ä¿æ­£ç¡®æ€§ï¼Œè¯¥æœºåˆ¶å°†è¾“å‡ºä¸é»„é‡‘è§£å†³æ–¹æ¡ˆè¿›è¡ŒéªŒè¯ã€‚æ‰€æå‡ºçš„G-Væ¡†æ¶ç”Ÿæˆäº†å…¨é¢çš„æµ‹è¯•ç”¨ä¾‹ï¼ŒåŒ…æ‹¬å¸¸è§„å’Œæç«¯æƒ…å†µï¼Œæé«˜äº†æµ‹è¯•è¦†ç›–ç‡ï¼Œå¹¶å¢å¼ºäº†ä»£ç å¼ºåŒ–å­¦ä¹ ä¸­è§£å†³æ–¹æ¡ˆæ­£ç¡®æ€§çš„è¾¨åˆ«åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºåœ¨çº¿éªŒè¯å¹³å°è®¾è®¡äº†ä¸€ä¸ªä¼˜åŒ–çš„å¤šå±‚å®‰å…¨æ²™ç®±ç³»ç»Ÿï¼Œä¿è¯ä»£ç æ‰§è¡Œçš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚é€šè¿‡å…¨é¢çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ç²¾é€‰æ•°æ®é›†çš„æœ‰æ•ˆæ€§ï¼Œåœ¨æ¨¡å‹æ€§èƒ½å’Œè®­ç»ƒç¨³å®šæ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚æºä»£ç ã€ç²¾é€‰æ•°æ®é›†å’Œæ²™ç®±ç³»ç»Ÿå¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/Kwai-Klear/CodeTest%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Kwai-Klear/CodeTestæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05710v1">PDF</a> 21 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Klear-CodeTestæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç”¨äºåˆæˆé«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹ï¼Œä»¥æ”¯æŒä»£ç å¼ºåŒ–å­¦ä¹ ä¸­å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒã€‚è¯¥æ¡†æ¶é‡‡ç”¨ç”Ÿæˆå™¨éªŒè¯ï¼ˆG-Vï¼‰æ¡†æ¶ï¼Œç¡®ä¿æµ‹è¯•æ¡ˆä¾‹çš„å¹¿æ³›è¦†ç›–å’Œæ­£ç¡®æ€§éªŒè¯ï¼ŒåŒæ—¶è®¾è®¡å¤šå±‚å®‰å…¨æ²™ç®±ç³»ç»Ÿï¼Œä¿è¯åœ¨çº¿éªŒè¯å¹³å°çš„å®‰å…¨å¯é ä»£ç æ‰§è¡Œã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å¯æé«˜æ¨¡å‹æ€§èƒ½å’Œè®­ç»ƒç¨³å®šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Klear-CodeTestæ˜¯ä¸€ä¸ªç”¨äºåˆæˆé«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹çš„æ¡†æ¶ï¼Œç”¨äºæ”¯æŒä»£ç å¼ºåŒ–å­¦ä¹ ä¸­å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨ç”Ÿæˆå™¨éªŒè¯ï¼ˆG-Vï¼‰æ¡†æ¶ï¼Œç¡®ä¿æµ‹è¯•æ¡ˆä¾‹çš„å¹¿æ³›è¦†ç›–å’Œæ­£ç¡®æ€§éªŒè¯ã€‚</li>
<li>G-Væ¡†æ¶é€šè¿‡ç”ŸæˆåŒ…æ‹¬å¸¸è§„å’Œè§’è½æ¡ˆä¾‹çš„ç»¼åˆæµ‹è¯•ç”¨ä¾‹ï¼Œæé«˜æµ‹è¯•è¦†ç›–ç‡ï¼Œå¢å¼ºè§£å†³æ–¹æ¡ˆæ­£ç¡®æ€§çš„è¯„ä¼°èƒ½åŠ›ã€‚</li>
<li>è®¾è®¡äº†å¤šå±‚å®‰å…¨æ²™ç®±ç³»ç»Ÿï¼Œä¿è¯åœ¨çº¿éªŒè¯å¹³å°çš„å®‰å…¨å¯é ä»£ç æ‰§è¡Œã€‚</li>
<li>è¯¥æ¡†æ¶å¯æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶çš„è®­ç»ƒç¨³å®šæ€§å¾—åˆ°äº†å®éªŒéªŒè¯ã€‚</li>
<li>Klear-CodeTestçš„æºä»£ç ã€ç²¾é€‰æ•°æ®é›†å’Œæ²™ç®±ç³»ç»Ÿå¯åœ¨GitHubä¸Šè·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ab327f22b00abc900fc1ee36f926aced.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-212530bd3df238e3525dead0b2a3caff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b1c61b97518c7c818d182eace9dd996.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44d0ed7d2c78aea5bc0f387f8ce8f00d.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="G-UBS-Towards-Robust-Understanding-of-Implicit-Feedback-via-Group-Aware-User-Behavior-Simulation"><a href="#G-UBS-Towards-Robust-Understanding-of-Implicit-Feedback-via-Group-Aware-User-Behavior-Simulation" class="headerlink" title="G-UBS: Towards Robust Understanding of Implicit Feedback via Group-Aware   User Behavior Simulation"></a>G-UBS: Towards Robust Understanding of Implicit Feedback via Group-Aware   User Behavior Simulation</h2><p><strong>Authors:Boyu Chen, Siran Chen, Zhengrong Yue, Kainan Yan, Chenyun Yu, Beibei Kong, Cheng Lei, Chengxiang Zhuo, Zang Li, Yali Wang</strong></p>
<p>User feedback is critical for refining recommendation systems, yet explicit feedback (e.g., likes or dislikes) remains scarce in practice. As a more feasible alternative, inferring user preferences from massive implicit feedback has shown great potential (e.g., a user quickly skipping a recommended video usually indicates disinterest). Unfortunately, implicit feedback is often noisy: a user might skip a video due to accidental clicks or other reasons, rather than disliking it. Such noise can easily misjudge user interests, thereby undermining recommendation performance. To address this issue, we propose a novel Group-aware User Behavior Simulation (G-UBS) paradigm, which leverages contextual guidance from relevant user groups, enabling robust and in-depth interpretation of implicit feedback for individual users. Specifically, G-UBS operates via two key agents. First, the User Group Manager (UGM) effectively clusters users to generate group profiles utilizing a &#96;&#96;summarize-cluster-reflectâ€ workflow based on LLMs. Second, the User Feedback Modeler (UFM) employs an innovative group-aware reinforcement learning approach, where each user is guided by the associated group profiles during the reinforcement learning process, allowing UFM to robustly and deeply examine the reasons behind implicit feedback. To assess our G-UBS paradigm, we have constructed a Video Recommendation benchmark with Implicit Feedback (IF-VR). To the best of our knowledge, this is the first multi-modal benchmark for implicit feedback evaluation in video recommendation, encompassing 15k users, 25k videos, and 933k interaction records with implicit feedback. Extensive experiments on IF-VR demonstrate that G-UBS significantly outperforms mainstream LLMs and MLLMs, with a 4.0% higher proportion of videos achieving a play rate &gt; 30% and 14.9% higher reasoning accuracy on IF-VR. </p>
<blockquote>
<p>ç”¨æˆ·åé¦ˆå¯¹äºå®Œå–„æ¨èç³»ç»Ÿè‡³å…³é‡è¦ï¼Œç„¶è€Œåœ¨å®è·µä¸­ï¼Œæ˜ç¡®çš„åé¦ˆï¼ˆä¾‹å¦‚å–œæ¬¢æˆ–ä¸å–œæ¬¢ï¼‰ä»ç„¶å¾ˆç¨€ç¼ºã€‚ä½œä¸ºä¸€ç§æ›´å¯è¡Œçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä»å¤§é‡çš„éšæ€§åé¦ˆä¸­æ¨æ–­ç”¨æˆ·åå¥½å·²æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼ˆä¾‹å¦‚ï¼Œç”¨æˆ·å¿«é€Ÿè·³è¿‡æ¨èçš„è§†é¢‘é€šå¸¸è¡¨ç¤ºä¸æ„Ÿå…´è¶£ï¼‰ã€‚ç„¶è€Œï¼Œéšæ€§åé¦ˆå¾€å¾€å¸¦æœ‰å™ªå£°ï¼šç”¨æˆ·å¯èƒ½ä¼šå› æ„å¤–ç‚¹å‡»æˆ–å…¶ä»–åŸå› è€Œè·³è¿‡è§†é¢‘ï¼Œè€Œä¸æ˜¯å› ä¸ºä¸å–œæ¬¢ã€‚è¿™ç§å™ªéŸ³å¾ˆå®¹æ˜“è¯¯åˆ¤ç”¨æˆ·å…´è¶£ï¼Œä»è€Œç ´åæ¨èæ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å°ç»„æ„ŸçŸ¥ç”¨æˆ·è¡Œä¸ºæ¨¡æ‹Ÿï¼ˆG-UBSï¼‰èŒƒå¼ï¼Œå®ƒåˆ©ç”¨ç›¸å…³ç”¨æˆ·å°ç»„çš„æƒ…æ™¯æŒ‡å¯¼ï¼Œå®ç°å¯¹ä¸ªåˆ«ç”¨æˆ·çš„éšæ€§åé¦ˆçš„ç¨³å¥å’Œæ·±å…¥çš„è§£è¯»ã€‚å…·ä½“æ¥è¯´ï¼ŒG-UBSé€šè¿‡ä¸¤ä¸ªå…³é”®ä»£ç†è¿›è¡Œæ“ä½œã€‚é¦–å…ˆï¼Œç”¨æˆ·ç¾¤ç»„ç®¡ç†å™¨ï¼ˆUGMï¼‰æœ‰æ•ˆåœ°èšé›†ç”¨æˆ·ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„â€œæ€»ç»“-èšç±»-åæ˜ â€å·¥ä½œæµç¨‹ç”Ÿæˆç¾¤ç»„æ¦‚å†µã€‚å…¶æ¬¡ï¼Œç”¨æˆ·åé¦ˆæ¨¡å‹å™¨ï¼ˆUFMï¼‰é‡‡ç”¨åˆ›æ–°çš„å°ç»„æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªç”¨æˆ·éƒ½ç”±ç›¸å…³çš„ç¾¤ç»„æ¦‚å†µå¼•å¯¼ï¼Œä½¿UFMèƒ½å¤Ÿç¨³å¥è€Œæ·±å…¥åœ°ç ”ç©¶éšæ€§åé¦ˆèƒŒåçš„åŸå› ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„G-UBSèŒƒå¼ï¼Œæˆ‘ä»¬å»ºç«‹äº†å¸¦æœ‰éšæ€§åé¦ˆçš„è§†é¢‘æ¨èåŸºå‡†ï¼ˆIF-VRï¼‰ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯è§†é¢‘æ¨èä¸­éšæ€§åé¦ˆè¯„ä¼°çš„é¦–ä¸ªå¤šæ¨¡å¼åŸºå‡†ï¼ŒåŒ…å«1.5ä¸‡ç”¨æˆ·ã€2.5ä¸‡è§†é¢‘å’Œ93.3ä¸‡å¸¦æœ‰éšæ€§åé¦ˆçš„äº’åŠ¨è®°å½•ã€‚åœ¨IF-VRä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒG-UBSæ˜¾è‘—ä¼˜äºä¸»æµçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œåœ¨IF-VRä¸Šï¼Œè§†é¢‘æ’­æ”¾ç‡è¶…è¿‡30%çš„æ¯”ä¾‹æé«˜äº†4.0%ï¼Œæ¨ç†å‡†ç¡®æ€§æé«˜äº†14.9%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05709v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”¨æˆ·å¯¹æ¨èç³»ç»Ÿçš„åé¦ˆå¯¹ä¼˜åŒ–æ¨èçš„é‡è¦æ€§ï¼Œä»¥åŠåœ¨å®é™…æ“ä½œä¸­æ˜¾å¼åé¦ˆçš„ç¨€ç¼ºæ€§ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§ä»å¤§è§„æ¨¡éšæ€§åé¦ˆä¸­æ¨æ–­ç”¨æˆ·åå¥½çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œéšæ€§åé¦ˆå¸¸å¸¸å¸¦æœ‰å™ªå£°ï¼Œå¯èƒ½å¯¼è‡´è¯¯åˆ¤ç”¨æˆ·å…´è¶£ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†å…¨æ–°çš„Group-aware User Behavior Simulationï¼ˆG-UBSï¼‰èŒƒå¼ã€‚è¯¥èŒƒå¼é€šè¿‡ç›¸å…³ç”¨æˆ·ç¾¤ä½“çš„æƒ…å¢ƒæŒ‡å¯¼ï¼Œå®ç°å¯¹ä¸ªä½“ç”¨æˆ·éšæ€§åé¦ˆçš„ç¨³å¥å’Œæ·±å…¥è§£è¯»ã€‚ä¸ºè¯„ä¼°è¯¥èŒƒå¼ï¼Œå»ºç«‹äº†åŒ…å«éšæ€§åé¦ˆçš„è§†é¢‘æ¨èåŸºå‡†æµ‹è¯•å¹³å°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒG-UBSèŒƒå¼åœ¨æ€§èƒ½å’Œå‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¼˜äºä¸»æµæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”¨æˆ·åé¦ˆå¯¹æ¨èç³»ç»Ÿè‡³å…³é‡è¦ï¼Œä½†æ˜¾å¼åé¦ˆåœ¨å®è·µä¸­è¾ƒä¸ºç¨€ç¼ºã€‚</li>
<li>éšæ€§åé¦ˆä¸ºæ¨æ–­ç”¨æˆ·åå¥½æä¾›äº†å¯è¡Œçš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>éšæ€§åé¦ˆå¸¸å¸¸å¸¦æœ‰å™ªå£°ï¼Œå¯èƒ½å¯¼è‡´è¯¯åˆ¤ç”¨æˆ·å…´è¶£ã€‚</li>
<li>G-UBSèŒƒå¼é€šè¿‡ç»“åˆç”¨æˆ·ç¾¤ä½“æƒ…å¢ƒï¼Œå®ç°å¯¹éšæ€§åé¦ˆçš„ç¨³å¥å’Œæ·±å…¥è§£è¯»ã€‚</li>
<li>G-UBSåŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šUser Group Managerï¼ˆUGMï¼‰å’ŒUser Feedback Modelerï¼ˆUFMï¼‰ã€‚</li>
<li>UGMé‡‡ç”¨â€œæ€»ç»“-èšç±»-åæ˜ â€çš„å·¥ä½œæµç¨‹åŸºäºLLMsç”Ÿæˆç”¨æˆ·ç¾¤ä½“æ¦‚å†µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05709">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f5346fd9cbe7084cbbb60bdf37d9c20e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87b3a5e5900acf992be796828c3ee341.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-261a3ec32036073ed6760a7021e70dd2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1578c45f1c1f5fb6258b5c29d116a8bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0200e20824932f7a9972834ef3737911.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba061c2f3ce5069cc926525982828e5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6537fd51c826a7c077c5250ed94dd41a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-232cf308128391ab2a9819da8e1a2a69.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Exploring-Superior-Function-Calls-via-Reinforcement-Learning"><a href="#Exploring-Superior-Function-Calls-via-Reinforcement-Learning" class="headerlink" title="Exploring Superior Function Calls via Reinforcement Learning"></a>Exploring Superior Function Calls via Reinforcement Learning</h2><p><strong>Authors:Bingguang Hao, Maolin Wang, Zengzhuang Xu, Yicheng Chen, Cunyin Peng, Jinjie GU, Chenyi Zhuang</strong></p>
<p>Function calling capabilities are crucial for deploying Large Language Models in real-world applications, yet current training approaches fail to develop robust reasoning strategies. Supervised fine-tuning produces models that rely on superficial pattern matching, while standard reinforcement learning methods struggle with the complex action space of structured function calls. We present a novel reinforcement learning framework designed to enhance group relative policy optimization through strategic entropy based exploration specifically tailored for function calling tasks. Our approach addresses three critical challenges in function calling: insufficient exploration during policy learning, lack of structured reasoning in chain-of-thought generation, and inadequate verification of parameter extraction. Our two-stage data preparation pipeline ensures high-quality training samples through iterative LLM evaluation and abstract syntax tree validation. Extensive experiments on the Berkeley Function Calling Leaderboard demonstrate that this framework achieves state-of-the-art performance among open-source models with 86.02% overall accuracy, outperforming standard GRPO by up to 6% on complex multi-function scenarios. Notably, our method shows particularly strong improvements on code-pretrained models, suggesting that structured language generation capabilities provide an advantageous starting point for reinforcement learning in function calling tasks. We will release all the code, models and dataset to benefit the community. </p>
<blockquote>
<p>å‡½æ•°è°ƒç”¨èƒ½åŠ›å¯¹äºå°†å¤§å‹è¯­è¨€æ¨¡å‹éƒ¨ç½²åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œç„¶è€Œå½“å‰çš„è®­ç»ƒæ–¹æ³•æ— æ³•åˆ¶å®šç¨³å¥çš„æ¨ç†ç­–ç•¥ã€‚ç›‘ç£å¾®è°ƒäº§ç”Ÿçš„æ¨¡å‹ä¾èµ–äºè‚¤æµ…çš„æ¨¡å¼åŒ¹é…ï¼Œè€Œæ ‡å‡†å¼ºåŒ–å­¦ä¹ æ–¹æ³•éš¾ä»¥åº”å¯¹ç»“æ„å‡½æ•°è°ƒç”¨ä¸­çš„å¤æ‚åŠ¨ä½œç©ºé—´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é’ˆå¯¹å‡½æ•°è°ƒç”¨ä»»åŠ¡é‡èº«å®šåˆ¶çš„æˆ˜ç•¥ç†µåŸºæ¢ç´¢å¢å¼ºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•è§£å†³äº†å‡½æ•°è°ƒç”¨ä¸­çš„ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šç­–ç•¥å­¦ä¹ è¿‡ç¨‹ä¸­çš„æ¢ç´¢ä¸è¶³ã€æ€ç»´ç”Ÿæˆä¸­ç»“æ„æ¨ç†çš„ç¼ºä¹ä»¥åŠå‚æ•°æå–éªŒè¯çš„ä¸è¶³ã€‚æˆ‘ä»¬çš„ä¸¤é˜¶æ®µæ•°æ®å‡†å¤‡ç®¡é“é€šè¿‡è¿­ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°å’ŒæŠ½è±¡è¯­æ³•æ ‘éªŒè¯ï¼Œç¡®ä¿é«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬ã€‚åœ¨ä¼¯å…‹åˆ©å‡½æ•°è°ƒç”¨æ’è¡Œæ¦œä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¼€æºæ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ€»ä½“å‡†ç¡®ç‡ä¸º86.02%ï¼Œåœ¨å¤æ‚çš„å¤šåŠŸèƒ½åœºæ™¯ä¸‹å°†æ ‡å‡†GRPOçš„æ€§èƒ½æé«˜äº†é«˜è¾¾6%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»£ç é¢„è®­ç»ƒæ¨¡å‹ä¸Šè¡¨ç°å‡ºç‰¹åˆ«å¼ºåŠ²çš„æ”¹è¿›ï¼Œè¿™è¡¨æ˜ç»“æ„åŒ–è¯­è¨€ç”Ÿæˆèƒ½åŠ›ä¸ºå‡½æ•°è°ƒä»»åŠ¡ä¸­çš„å¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ä¸ªæœ‰åˆ©çš„èµ·ç‚¹ã€‚æˆ‘ä»¬å°†å‘å¸ƒæ‰€æœ‰çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†ä»¥é€ ç¦ç¤¾åŒºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05118v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å‡½æ•°è°ƒç”¨èƒ½åŠ›ç¼ºå¤±é—®é¢˜ï¼Œç°æœ‰è®­ç»ƒç­–ç•¥æ— æ³•åŸ¹å…»ç¨³å¥çš„æ¨ç†ç­–ç•¥ã€‚ç›‘ç£å¾®è°ƒäº§ç”Ÿçš„æ¨¡å‹ä¾èµ–äºè¡¨é¢æ¨¡å¼åŒ¹é…ï¼Œè€Œæ ‡å‡†å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤æ‚çš„å‡½æ•°è°ƒç”¨åŠ¨ä½œç©ºé—´ä¸­è¡¨ç°æŒ£æ‰ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹å‡½æ•°è°ƒç”¨ä»»åŠ¡çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åŸºäºç­–ç•¥ç›¸å¯¹ç†µçš„æ¢ç´¢æ¥è§£å†³ç¾¤ä½“ç­–ç•¥ä¼˜åŒ–é—®é¢˜ã€‚è¯¥æ–¹æ³•è§£å†³äº†å‡½æ•°è°ƒç”¨ä¸­çš„ä¸‰å¤§æŒ‘æˆ˜ï¼šç­–ç•¥å­¦ä¹ è¿‡ç¨‹ä¸­çš„æ¢ç´¢ä¸è¶³ã€æ€ç»´é“¾ç”Ÿæˆä¸­çš„ç»“æ„åŒ–æ¨ç†ç¼ºå¤±ä»¥åŠå‚æ•°æå–çš„éªŒè¯ä¸è¶³ã€‚é€šè¿‡ä¸¤é˜¶æ®µæ•°æ®å‡†å¤‡ç®¡é“ç¡®ä¿é«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬ï¼Œé€šè¿‡è¿­ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°å’ŒæŠ½è±¡è¯­æ³•æ ‘éªŒè¯ã€‚åœ¨Berkeleyå‡½æ•°è°ƒç”¨æ’è¡Œæ¦œä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¼€æºæ¨¡å‹ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œæ€»ä½“å‡†ç¡®ç‡è¾¾åˆ°86.02%ï¼Œåœ¨å¤æ‚çš„å¤šå‡½æ•°åœºæ™¯ä¸Šæ¯”æ ‡å‡†GRPOé«˜å‡º6%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ–¹æ³•åœ¨ä»£ç é¢„è®­ç»ƒæ¨¡å‹ä¸Šè¡¨ç°å‡ºç‰¹åˆ«å¼ºå¤§çš„æ”¹è¿›ï¼Œè¡¨æ˜ç»“æ„åŒ–è¯­è¨€ç”Ÿæˆèƒ½åŠ›ä¸ºå¼ºåŒ–å­¦ä¹ åœ¨å‡½æ•°è°ƒç”¨ä»»åŠ¡ä¸­æä¾›äº†ä¸€ä¸ªæœ‰åˆ©çš„èµ·ç‚¹ã€‚æˆ‘ä»¬å°†å‘å¸ƒæ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†ä»¥é€ ç¦ç¤¾åŒºã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å‡½æ•°è°ƒç”¨èƒ½åŠ›è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰è®­ç»ƒç­–ç•¥å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>ç›‘ç£å¾®è°ƒäº§ç”Ÿçš„æ¨¡å‹ä¾èµ–äºè¡¨é¢æ¨¡å¼åŒ¹é…ï¼Œç¼ºä¹æ·±åº¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤æ‚çš„å‡½æ•°è°ƒç”¨åŠ¨ä½œç©ºé—´ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶é€šè¿‡ç­–ç•¥ç›¸å¯¹ç†µçš„æ¢ç´¢è§£å†³ç¾¤ä½“ç­–ç•¥ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•è§£å†³äº†å‡½æ•°è°ƒç”¨çš„ä¸‰å¤§æŒ‘æˆ˜ï¼šæ¢ç´¢ä¸è¶³ã€ç»“æ„åŒ–æ¨ç†ç¼ºå¤±å’Œå‚æ•°éªŒè¯ä¸è¶³ã€‚</li>
<li>é€šè¿‡ä¸¤é˜¶æ®µæ•°æ®å‡†å¤‡ç®¡é“ç¡®ä¿é«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-17ebdea62d02fdc80785d69d4fc7f5a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1253f26143895775e96cfd8eae0001db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33a410925db6fd9f09c5a0eb4ef94157.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-809d9b45f96bf6d6b740e919b9119531.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-12/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-12/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-12/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0b89eeb7cdff535f3fa984ba1a4592f9.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-12  Effective Training Data Synthesis for Improving MLLM Chart Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-12/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-4eb0c7372993f511c3987360e26ee704.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-12  MoDA Multi-modal Diffusion Architecture for Talking Head Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28791.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
