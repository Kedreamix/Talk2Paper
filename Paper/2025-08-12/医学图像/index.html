<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-12  Text Embedded Swin-UMamba for DeepLesion Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-466e0ed5a483944761b9d81efec4b2fe.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    53 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-12-æ›´æ–°"><a href="#2025-08-12-æ›´æ–°" class="headerlink" title="2025-08-12 æ›´æ–°"></a>2025-08-12 æ›´æ–°</h1><h2 id="Text-Embedded-Swin-UMamba-for-DeepLesion-Segmentation"><a href="#Text-Embedded-Swin-UMamba-for-DeepLesion-Segmentation" class="headerlink" title="Text Embedded Swin-UMamba for DeepLesion Segmentation"></a>Text Embedded Swin-UMamba for DeepLesion Segmentation</h2><p><strong>Authors:Ruida Cheng, Tejas Sudharshan Mathai, Pritam Mukherjee, Benjamin Hou, Qingqing Zhu, Zhiyong Lu, Matthew McAuliffe, Ronald M. Summers</strong></p>
<p>Segmentation of lesions on CT enables automatic measurement for clinical assessment of chronic diseases (e.g., lymphoma). Integrating large language models (LLMs) into the lesion segmentation workflow offers the potential to combine imaging features with descriptions of lesion characteristics from the radiology reports. In this study, we investigate the feasibility of integrating text into the Swin-UMamba architecture for the task of lesion segmentation. The publicly available ULS23 DeepLesion dataset was used along with short-form descriptions of the findings from the reports. On the test dataset, a high Dice Score of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for lesion segmentation. The proposed Text-Swin-UMamba model outperformed prior approaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p &lt; 0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by 1.74% and 0.22%, respectively. The dataset and code can be accessed at <a target="_blank" rel="noopener" href="https://github.com/ruida/LLM-Swin-UMamba">https://github.com/ruida/LLM-Swin-UMamba</a> </p>
<blockquote>
<p>å¯¹CTä¸Šçš„ç—…ç¶è¿›è¡Œåˆ†å‰²ï¼Œå¯å®ç°æ…¢æ€§ç–¾ç—…çš„ä¸´åºŠè¯„ä¼°ï¼ˆä¾‹å¦‚æ·‹å·´ç˜¤ï¼‰çš„è‡ªåŠ¨æµ‹é‡ã€‚å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é›†æˆåˆ°ç—…ç¶åˆ†å‰²å·¥ä½œæµç¨‹ä¸­ï¼Œå¯å°†å›¾åƒç‰¹å¾ä¸æ¥è‡ªæ”¾å°„å­¦æŠ¥å‘Šçš„ç—…ç¶ç‰¹å¾æè¿°ç›¸ç»“åˆã€‚åœ¨ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†å°†æ–‡æœ¬é›†æˆåˆ°Swin-UMambaæ¶æ„ä¸­è¿›è¡Œç—…ç¶åˆ†å‰²ä»»åŠ¡çš„å¯èƒ½æ€§ã€‚ä½¿ç”¨å…¬å¼€å¯ç”¨çš„ULS23 DeepLesionæ•°æ®é›†ä»¥åŠæŠ¥å‘Šä¸­å‘ç°ç»“æœçš„ç®€çŸ­æè¿°ã€‚åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šï¼Œè·å¾—äº†é«˜è¾¾82ï¼…çš„Diceåˆ†æ•°ï¼ŒHausdorffè·ç¦»ä½è‡³6.58åƒç´ è¿›è¡Œç—…ç¶åˆ†å‰²ã€‚æå‡ºçš„Text-Swin-UMambaæ¨¡å‹åœ¨ä»¥å‰çš„æ–¹æ³•ä¸Šå…·æœ‰å‡ºè‰²çš„è¡¨ç°ï¼šä¸LLMé©±åŠ¨çš„LanGuideMedSegæ¨¡å‹ç›¸æ¯”ï¼Œæé«˜äº†37ï¼…ï¼ˆp &lt;0.001ï¼‰ï¼Œå¹¶ä¸”æ¯”çº¯å›¾åƒåŸºç¡€çš„xLSTM-UNetå’ŒnnUNetæ¨¡å‹åˆ†åˆ«é«˜å‡º1.74ï¼…å’Œ0.22ï¼…ã€‚æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/ruida/LLM-Swin-UMamba%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/ruida/LLM-Swin-UMambaè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06453v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶çš„æ‘˜è¦æŒ‡å‡ºï¼Œé€šè¿‡åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œä½¿ç”¨å›¾åƒåˆ†æå¯¹æ…¢æ€§ç—…å˜ï¼ˆå¦‚æ·‹å·´ç˜¤ï¼‰è¿›è¡ŒCTæ‰«æä¸­çš„ç—…ç¶åˆ†å‰²ä¸ä¸´åºŠè¯„ä¼°ç ”ç©¶å…·æœ‰å¯è¡Œæ€§ã€‚æœ¬ç ”ç©¶é‡‡ç”¨äº†Swin-UMambaæ¶æ„å¹¶èå…¥äº†æ–‡æœ¬ä¿¡æ¯ï¼Œä½¿ç”¨å…¬å¼€å¯ç”¨çš„ULS23 DeepLesionæ•°æ®é›†å’ŒæŠ¥å‘Šä¸­çš„ç®€çŸ­æè¿°å‘ç°ç»“æœã€‚åœ¨æµ‹è¯•é›†ä¸Šï¼Œè¯¥æ¨¡å‹å–å¾—äº†è¾ƒé«˜çš„Diceç³»æ•°ï¼ˆ82%ï¼‰å’Œè¾ƒä½çš„Hausdorffè·ç¦»ï¼ˆ6.58åƒç´ ï¼‰ï¼Œå¹¶æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚é€šè¿‡GitHubé“¾æ¥ï¼Œå…¬å¼€äº†æ•°æ®é›†å’Œä»£ç ã€‚è¯¥æ¨¡å‹å°†æœ‰æœ›ä¸ºåŒ»å­¦å½±åƒé¢†åŸŸçš„ç²¾å‡†åˆ†æå¼€è¾Ÿæ–°çš„å¯èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯è¯¥æ–‡æœ¬çš„ä¸»è¦è§‚ç‚¹ä¸æ”¶è·ç‚¹ï¼š</p>
<ul>
<li>ç ”ç©¶æ—¨åœ¨æ¢è®¨é›†æˆæ–‡æœ¬ä¿¡æ¯åˆ°ç—…ç¶åˆ†å‰²å·¥ä½œæµä¸­çš„å¯è¡Œæ€§ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†Swin-UMambaæ¶æ„å¹¶ç»“åˆæ–‡æœ¬ä¿¡æ¯æ¥å¤„ç†CTå›¾åƒä¸­çš„ç—…ç¶åˆ†å‰²ã€‚</li>
<li>ä½¿ç”¨å…¬å¼€å¯ç”¨çš„ULS23 DeepLesionæ•°æ®é›†ä»¥åŠæŠ¥å‘Šä¸­çš„ç®€çŸ­æè¿°ä½œä¸ºè¾…åŠ©ä¿¡æ¯ã€‚</li>
<li>åœ¨æµ‹è¯•é›†ä¸Šå–å¾—äº†è¾ƒé«˜çš„Diceç³»æ•°ï¼ˆ82%ï¼‰å’Œä½Hausdorffè·ç¦»ï¼ˆ6.58åƒç´ ï¼‰ï¼Œè¡¨æ˜æ¨¡å‹åœ¨ç—…ç¶åˆ†å‰²ä»»åŠ¡ä¸Šçš„è‰¯å¥½æ€§èƒ½ã€‚</li>
<li>è¯¥æ¨¡å‹ç›¸è¾ƒäºå…¶ä»–æ–¹æ³•ï¼ˆå¦‚LLMé©±åŠ¨çš„LanGuideMedSegæ¨¡å‹ã€çº¯å›¾åƒåŸºç¡€çš„xLSTM-UNetå’ŒnnUNetæ¨¡å‹ï¼‰å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå®ç°äº†è¾ƒé«˜çš„æ€§èƒ½æå‡ã€‚</li>
<li>æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡GitHubé“¾æ¥å…¬å¼€è®¿é—®ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¾¿åˆ©çš„èµ„æºã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06453">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f5e398d5269d1d8c0df971eea179d700.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-466e0ed5a483944761b9d81efec4b2fe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-63c5f59fc4977dc49644396d7979d19f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-241c7ce5e6bec6151a67d38217501d76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27dd43665e2ea89d83728baf444a8244.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Multi-Omics-Analysis-for-Cancer-Subtype-Inference-via-Unrolling-Graph-Smoothness-Priors"><a href="#Multi-Omics-Analysis-for-Cancer-Subtype-Inference-via-Unrolling-Graph-Smoothness-Priors" class="headerlink" title="Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph   Smoothness Priors"></a>Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph   Smoothness Priors</h2><p><strong>Authors:Jielong Lu, Zhihao Wu, Jiajun Yu, Jiajun Bu, Haishuai Wang</strong></p>
<p>Integrating multi-omics datasets through data-driven analysis offers a comprehensive understanding of the complex biological processes underlying various diseases, particularly cancer. Graph Neural Networks (GNNs) have recently demonstrated remarkable ability to exploit relational structures in biological data, enabling advances in multi-omics integration for cancer subtype classification. Existing approaches often neglect the intricate coupling between heterogeneous omics, limiting their capacity to resolve subtle cancer subtype heterogeneity critical for precision oncology. To address these limitations, we propose a framework named Graph Transformer for Multi-omics Cancer Subtype Classification (GTMancer). This framework builds upon the GNN optimization problem and extends its application to complex multi-omics data. Specifically, our method leverages contrastive learning to embed multi-omics data into a unified semantic space. We unroll the multiplex graph optimization problem in that unified space and introduce dual sets of attention coefficients to capture structural graph priors both within and among multi-omics data. This approach enables global omics information to guide the refining of the representations of individual omics. Empirical experiments on seven real-world cancer datasets demonstrate that GTMancer outperforms existing state-of-the-art algorithms. </p>
<blockquote>
<p>é€šè¿‡æ•°æ®é©±åŠ¨åˆ†ææ•´åˆå¤šç»„å­¦æ•°æ®é›†ï¼Œä¸ºæˆ‘ä»¬å…¨é¢ç†è§£å„ç§ç–¾ç—…ï¼ˆç‰¹åˆ«æ˜¯ç™Œç—‡ï¼‰èƒŒåçš„å¤æ‚ç”Ÿç‰©è¿‡ç¨‹æä¾›äº†é€”å¾„ã€‚å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰æœ€è¿‘æ˜¾ç¤ºå‡ºåœ¨ç”Ÿç‰©æ•°æ®ä¸­åˆ©ç”¨å…³ç³»ç»“æ„çš„æ˜¾è‘—èƒ½åŠ›ï¼Œæ¨åŠ¨äº†å¤šç»„å­¦æ•´åˆåœ¨ç™Œç—‡äºšå‹åˆ†ç±»æ–¹é¢çš„è¿›å±•ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†äº†ä¸åŒç»„å­¦ä¹‹é—´çš„å¤æ‚è€¦åˆï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨è§£å†³å¯¹ç²¾ç¡®è‚¿ç˜¤å­¦è‡³å…³é‡è¦çš„ç™Œç—‡äºšå‹ç»†å¾®å¼‚è´¨æ€§çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œç”¨äºå¤šç»„å­¦ç™Œç—‡äºšå‹åˆ†ç±»çš„å›¾è½¬æ¢å™¨â€ï¼ˆGTMancerï¼‰çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶å»ºç«‹åœ¨GNNä¼˜åŒ–é—®é¢˜çš„åŸºç¡€ä¸Šï¼Œå¹¶å°†å…¶åº”ç”¨æ‰©å±•åˆ°å¤æ‚çš„å¤šç»„å­¦æ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¯¹æ¯”å­¦ä¹ å°†å¤šç»„å­¦æ•°æ®åµŒå…¥åˆ°ç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´ä¸­ã€‚æˆ‘ä»¬åœ¨è¯¥ç»Ÿä¸€ç©ºé—´ä¸­è§£å¼€å¤šé‡å›¾ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶å¼•å…¥åŒé‡æ³¨æ„åŠ›ç³»æ•°é›†ï¼Œä»¥æ•è·ç»„å†…å’Œç»„é—´å¤šç»„å­¦æ•°æ®çš„ç»“æ„å›¾å…ˆéªŒã€‚è¿™ç§æ–¹æ³•ä½¿å¾—å…¨å±€ç»„å­¦ä¿¡æ¯èƒ½å¤ŸæŒ‡å¯¼å•ä¸ªç»„å­¦è¡¨ç¤ºçš„ç²¾åŒ–ã€‚åœ¨ä¸ƒä¸ªçœŸå®ä¸–ç•Œçš„ç™Œç—‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGTMancerä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›ç®—æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06257v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šç»„å­¦æ•°æ®é›†é€šè¿‡æ•°æ®é©±åŠ¨åˆ†ææ•´åˆï¼Œä¸ºç†è§£å„ç§ç–¾ç—…å°¤å…¶æ˜¯ç™Œç—‡çš„å¤æ‚ç”Ÿç‰©è¿‡ç¨‹æä¾›äº†å…¨é¢è§†è§’ã€‚å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰èƒ½å¤ŸæŒ–æ˜ç”Ÿç‰©æ•°æ®ä¸­çš„å…³ç³»ç»“æ„ï¼Œå› è€Œåœ¨ç™Œç—‡äºšå‹åˆ†ç±»çš„å¤šç»„å­¦æ•´åˆä¸­å–å¾—äº†è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¿½è§†äº†ä¸åŒç»„å­¦ä¹‹é—´çš„å¤æ‚è€¦åˆï¼Œéš¾ä»¥è§£å†³ç™Œç—‡äºšå‹å¼‚è´¨æ€§çš„ç»†å¾®å·®å¼‚ï¼Œè¿™å¯¹äºç²¾å‡†è‚¿ç˜¤å­¦è‡³å…³é‡è¦ã€‚ä¸ºåº”å¯¹è¿™äº›å±€é™ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºâ€œå¤šç»„å­¦ç™Œç—‡äºšå‹åˆ†ç±»å›¾è½¬æ¢å™¨ï¼ˆGTMancerï¼‰â€çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶å»ºç«‹åœ¨GNNä¼˜åŒ–é—®é¢˜çš„åŸºç¡€ä¸Šï¼Œæ‰©å±•è‡³å¤æ‚çš„å¤šç»„å­¦æ•°æ®åº”ç”¨ã€‚å®ƒé€šè¿‡å¯¹æ¯”å­¦ä¹ å°†å¤šç»„å­¦æ•°æ®åµŒå…¥ç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´ï¼Œè§£å†³å¤šå±‚å›¾å½¢ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶å¼•å…¥åŒé‡æ³¨æ„åŠ›ç³»æ•°æ¥æ•æ‰ç»„å­¦å†…éƒ¨å’Œä¹‹é—´çš„ç»“æ„å›¾å½¢å…ˆéªŒä¿¡æ¯ã€‚è¿™ä½¿å¾—å…¨å±€ç»„å­¦ä¿¡æ¯èƒ½å¤ŸæŒ‡å¯¼å•ä¸ªç»„å­¦è¡¨å¾çš„ç»†åŒ–ã€‚åœ¨ä¸ƒä¸ªçœŸå®ä¸–ç•Œçš„ç™Œç—‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGTMancerä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„ç®—æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šç»„å­¦æ•°æ®æ•´åˆèƒ½æä¾›å¯¹ç™Œç—‡ç­‰ç–¾ç—…çš„å…¨é¢ç†è§£ã€‚</li>
<li>å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨æŒ–æ˜ç”Ÿç‰©æ•°æ®å…³ç³»ç»“æ„æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä¿ƒè¿›äº†ç™Œç—‡äºšå‹åˆ†ç±»çš„å¤šç»„å­¦æ•´åˆã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½ç•¥ç»„å­¦é—´çš„å¤æ‚è€¦åˆï¼Œéš¾ä»¥æ•æ‰ç™Œç—‡äºšå‹å¼‚è´¨æ€§çš„ç»†å¾®å·®å¼‚ã€‚</li>
<li>GTManceræ¡†æ¶æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡åµŒå…¥ç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´å¤„ç†å¤šç»„å­¦æ•°æ®ã€‚</li>
<li>GTMancerç»“åˆå¯¹æ¯”å­¦ä¹ å’ŒåŒé‡æ³¨æ„åŠ›ç³»æ•°ï¼Œæ•æ‰ç»“æ„å›¾å½¢å…ˆéªŒä¿¡æ¯ã€‚</li>
<li>GTManceræ–¹æ³•ä½¿å¾—å…¨å±€ç»„å­¦ä¿¡æ¯èƒ½æŒ‡å¯¼å•ä¸ªç»„å­¦è¡¨å¾çš„ç»†åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06257">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-90e946029dd085e9bb210398157bd506.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b060aa4fb6e8c54548c3efa72649a56.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3de4dd532a62dcae8cf2ca508929fe05.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Semantic-Segmentation-Algorithm-for-Pleural-Effusion-Based-on-DBIF-AUNet"><a href="#A-Semantic-Segmentation-Algorithm-for-Pleural-Effusion-Based-on-DBIF-AUNet" class="headerlink" title="A Semantic Segmentation Algorithm for Pleural Effusion Based on   DBIF-AUNet"></a>A Semantic Segmentation Algorithm for Pleural Effusion Based on   DBIF-AUNet</h2><p><strong>Authors:Ruixiang Tang, Jianglong Qin, Mingda Zhang, Yan Song, Yi Wu, Wei Wu</strong></p>
<p>Pleural effusion semantic segmentation can significantly enhance the accuracy and timeliness of clinical diagnosis and treatment by precisely identifying disease severity and lesion areas. Currently, semantic segmentation of pleural effusion CT images faces multiple challenges. These include similar gray levels between effusion and surrounding tissues, blurred edges, and variable morphology. Existing methods often struggle with diverse image variations and complex edges, primarily because direct feature concatenation causes semantic gaps. To address these challenges, we propose the Dual-Branch Interactive Fusion Attention model (DBIF-AUNet). This model constructs a densely nested skip-connection network and innovatively refines the Dual-Domain Feature Disentanglement module (DDFD). The DDFD module orthogonally decouples the functions of dual-domain modules to achieve multi-scale feature complementarity and enhance characteristics at different levels. Concurrently, we design a Branch Interaction Attention Fusion module (BIAF) that works synergistically with the DDFD. This module dynamically weights and fuses global, local, and frequency band features, thereby improving segmentation robustness. Furthermore, we implement a nested deep supervision mechanism with hierarchical adaptive hybrid loss to effectively address class imbalance. Through validation on 1,622 pleural effusion CT images from Southwest Hospital, DBIF-AUNet achieved IoU and Dice scores of 80.1% and 89.0% respectively. These results outperform state-of-the-art medical image segmentation models U-Net++ and Swin-UNet by 5.7%&#x2F;2.7% and 2.2%&#x2F;1.5% respectively, demonstrating significant optimization in segmentation accuracy for complex pleural effusion CT images. </p>
<blockquote>
<p>èƒ¸è†œç§¯æ°´çš„è¯­ä¹‰åˆ†å‰²èƒ½å¤Ÿç²¾ç¡®è¯†åˆ«ç–¾ç—…ä¸¥é‡ç¨‹åº¦å’Œç—…å˜åŒºåŸŸï¼Œä»è€Œæ˜¾è‘—æé«˜ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—å‡†ç¡®æ€§å’ŒåŠæ—¶æ€§ã€‚ç›®å‰ï¼Œèƒ¸è†œç§¯æ°´CTå›¾åƒçš„è¯­ä¹‰åˆ†å‰²é¢ä¸´å¤šé‡æŒ‘æˆ˜ã€‚è¿™äº›æŒ‘æˆ˜åŒ…æ‹¬ç§¯æ°´ä¸å‘¨å›´ç»„ç»‡ä¹‹é—´çš„ç°åº¦ç›¸ä¼¼åº¦ã€è¾¹ç¼˜æ¨¡ç³Šä»¥åŠå½¢æ€å˜åŒ–å¤šæ ·ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥åº”å¯¹å¤šæ ·çš„å›¾åƒå˜åŒ–å’Œå¤æ‚çš„è¾¹ç¼˜ï¼Œä¸»è¦æ˜¯å› ä¸ºç›´æ¥ç‰¹å¾æ‹¼æ¥ä¼šå¯¼è‡´è¯­ä¹‰é¸¿æ²Ÿã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŒåˆ†æ”¯äº¤äº’èåˆæ³¨æ„åŠ›æ¨¡å‹ï¼ˆDBIF-AUNetï¼‰ã€‚è¯¥æ¨¡å‹æ„å»ºäº†ä¸€ä¸ªå¯†é›†åµŒå¥—è·³è·ƒè¿æ¥ç½‘ç»œï¼Œå¹¶åˆ›æ–°åœ°æ”¹è¿›äº†åŒåŸŸç‰¹å¾åˆ†è§£æ¨¡å—ï¼ˆDDFDï¼‰ã€‚DDFDæ¨¡å—é€šè¿‡æ­£äº¤è§£è€¦åŒåŸŸæ¨¡å—çš„åŠŸèƒ½ï¼Œå®ç°å¤šå°ºåº¦ç‰¹å¾äº’è¡¥ï¼Œå¹¶å¢å¼ºä¸åŒçº§åˆ«çš„ç‰¹å¾ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåˆ†æ”¯äº¤äº’æ³¨æ„åŠ›èåˆæ¨¡å—ï¼ˆBIAFï¼‰ï¼Œå®ƒä¸DDFDååŒå·¥ä½œã€‚è¯¥æ¨¡å—åŠ¨æ€åŠ æƒå¹¶èåˆå…¨å±€ã€å±€éƒ¨å’Œé¢‘å¸¦ç‰¹å¾ï¼Œä»è€Œæé«˜åˆ†å‰²ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§å…·æœ‰åˆ†å±‚è‡ªé€‚åº”æ··åˆæŸå¤±çš„åµŒå¥—æ·±åº¦ç›‘ç£æœºåˆ¶ï¼Œä»¥æœ‰æ•ˆè§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚é€šè¿‡å¯¹æ¥è‡ªè¥¿å—åŒ»é™¢çš„1622å¼ èƒ¸è†œç§¯æ°´CTå›¾åƒè¿›è¡ŒéªŒè¯ï¼ŒDBIF-AUNetçš„IoUå’ŒDiceå¾—åˆ†åˆ†åˆ«ä¸º80.1%å’Œ89.0%ï¼Œåˆ†åˆ«ä¼˜äºå…ˆè¿›çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹U-Net++å’ŒSwin-UNetçš„5.7%&#x2F;2.7%å’Œ2.2%&#x2F;1.5%ã€‚è¿™è¯æ˜äº†DBIF-AUNetåœ¨å¤æ‚èƒ¸è†œç§¯æ°´CTå›¾åƒåˆ†å‰²ç²¾åº¦ä¸Šçš„æ˜¾è‘—ä¼˜åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06191v1">PDF</a> 12 pages, 6 figures, 2 tables</p>
<p><strong>Summary</strong><br>     åŸºäºåŒåˆ†æ”¯äº¤äº’èåˆæ³¨æ„åŠ›æ¨¡å‹çš„èƒ¸è†œè…”ç§¯æ¶²CTå›¾åƒè¯­ä¹‰åˆ†å‰²æŠ€æœ¯èƒ½æœ‰æ•ˆæå‡ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—å‡†ç¡®æ€§å’Œæ—¶æ•ˆæ€§ï¼Œé€šè¿‡ç²¾ç¡®è¯†åˆ«ç—…å˜ä¸¥é‡ç¨‹åº¦å’ŒåŒºåŸŸã€‚è¯¥æŠ€æœ¯è§£å†³äº†åŒ…æ‹¬ç°åº¦ç›¸ä¼¼ã€è¾¹ç¼˜æ¨¡ç³Šå’Œå½¢æ€å¤šå˜ç­‰æŒ‘æˆ˜ï¼Œé€šè¿‡æ„å»ºå¯†é›†åµŒå¥—è·³è·ƒè¿æ¥ç½‘ç»œå’Œæ”¹è‰¯çš„åŒåŸŸç‰¹å¾åˆ†è§£æ¨¡å—ï¼Œå®ç°äº†å¤šå°ºåº¦ç‰¹å¾äº’è¡¥å’Œå„çº§ç‰¹å¾å¢å¼ºã€‚åŒæ—¶ï¼Œè®¾è®¡åˆ†æ”¯äº¤äº’æ³¨æ„åŠ›èåˆæ¨¡å—ï¼ŒåŠ¨æ€åŠ æƒèåˆå…¨å±€ã€å±€éƒ¨å’Œé¢‘å¸¦ç‰¹å¾ï¼Œæé«˜åˆ†å‰²ç¨³å¥æ€§ã€‚å®æ–½åµŒå¥—æ·±åº¦ç›‘ç£æœºåˆ¶ï¼Œç»“åˆåˆ†å±‚è‡ªé€‚åº”æ··åˆæŸå¤±ï¼Œæœ‰æ•ˆåº”å¯¹ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚åœ¨è¥¿å—åŒ»é™¢çš„1622å¼ èƒ¸è†œè…”ç§¯æ¶²CTå›¾åƒéªŒè¯ä¸­ï¼ŒDBIF-AUNetå–å¾—äº†äº¤å¹¶æ¯”å’ŒDiceç³»æ•°åˆ†åˆ«ä¸º80.1%å’Œ89.0%çš„ç»“æœï¼Œè¾ƒå…¶ä»–å…ˆè¿›åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>èƒ¸è†œè…”ç§¯æ¶²è¯­ä¹‰åˆ†å‰²å¯¹ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—çš„é‡è¦æ€§ï¼šèƒ½ç²¾ç¡®è¯†åˆ«ç—…å˜ç¨‹åº¦å’ŒåŒºåŸŸï¼Œæé«˜è¯Šæ–­å’Œæ²»ç–—çš„å‡†ç¡®æ€§å’Œæ—¶æ•ˆæ€§ã€‚</li>
<li>èƒ¸è†œè…”ç§¯æ¶²CTå›¾åƒè¯­ä¹‰åˆ†å‰²æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼šåŒ…æ‹¬ç°åº¦ç›¸ä¼¼ã€è¾¹ç¼˜æ¨¡ç³Šå’Œå½¢æ€å¤šå˜ç­‰ã€‚</li>
<li>DBIF-AUNetæ¨¡å‹çš„ä¸»è¦åˆ›æ–°ç‚¹ï¼šæ„å»ºå¯†é›†åµŒå¥—ç½‘ç»œï¼Œæ”¹è‰¯åŒåŸŸç‰¹å¾åˆ†è§£æ¨¡å—ï¼Œå®ç°å¤šå°ºåº¦ç‰¹å¾äº’è¡¥ã€‚</li>
<li>åˆ†æ”¯äº¤äº’æ³¨æ„åŠ›èåˆæ¨¡å—çš„ä½œç”¨ï¼šåŠ¨æ€åŠ æƒèåˆå…¨å±€ã€å±€éƒ¨å’Œé¢‘å¸¦ç‰¹å¾ï¼Œæé«˜åˆ†å‰²ç¨³å¥æ€§ã€‚</li>
<li>åµŒå¥—æ·±åº¦ç›‘ç£æœºåˆ¶å’Œåˆ†å±‚è‡ªé€‚åº”æ··åˆæŸå¤±çš„åº”ç”¨ï¼šæœ‰æ•ˆåº”å¯¹ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>DBIF-AUNetæ¨¡å‹åœ¨èƒ¸è†œè…”ç§¯æ¶²CTå›¾åƒåˆ†å‰²ä¸Šçš„ä¼˜ç§€è¡¨ç°ï¼šè¾ƒå…¶ä»–å…ˆè¿›æ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06191">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7f20682ab59d04ea26028a82467be7fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b5b7530826bbdc01d10df4615417012.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-514e42f5db22ff5020e2cdfbe0cee44c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Towards-MR-Based-Trochleoplasty-Planning"><a href="#Towards-MR-Based-Trochleoplasty-Planning" class="headerlink" title="Towards MR-Based Trochleoplasty Planning"></a>Towards MR-Based Trochleoplasty Planning</h2><p><strong>Authors:Michael Wehrli, Alicia Durrer, Paul Friedrich, Sidaty El Hadramy, Edwin Li, Luana Brahaj, Carol C. Hasler, Philippe C. Cattin</strong></p>
<p>To treat Trochlear Dysplasia (TD), current approaches rely mainly on low-resolution clinical Magnetic Resonance (MR) scans and surgical intuition. The surgeries are planned based on surgeons experience, have limited adoption of minimally invasive techniques, and lead to inconsistent outcomes. We propose a pipeline that generates super-resolved, patient-specific 3D pseudo-healthy target morphologies from conventional clinical MR scans. First, we compute an isotropic super-resolved MR volume using an Implicit Neural Representation (INR). Next, we segment femur, tibia, patella, and fibula with a multi-label custom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to generate pseudo-healthy target morphologies of the trochlear region. In contrast to prior work producing pseudo-healthy low-resolution 3D MR images, our approach enables the generation of sub-millimeter resolved 3D shapes compatible for pre- and intraoperative use. These can serve as preoperative blueprints for reshaping the femoral groove while preserving the native patella articulation. Furthermore, and in contrast to other work, we do not require a CT for our pipeline - reducing the amount of radiation. We evaluated our approach on 25 TD patients and could show that our target morphologies significantly improve the sulcus angle (SA) and trochlear groove depth (TGD). The code and interactive visualization are available at <a target="_blank" rel="noopener" href="https://wehrlimi.github.io/sr-3d-planning/">https://wehrlimi.github.io/sr-3d-planning/</a>. </p>
<blockquote>
<p>é’ˆå¯¹Trochlear Dysplasiaï¼ˆTDï¼‰çš„æ²»ç–—ï¼Œç›®å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºä½åˆ†è¾¨ç‡çš„ä¸´åºŠç£å…±æŒ¯ï¼ˆMRï¼‰æ‰«æå’Œæ‰‹æœ¯ç›´è§‰ã€‚æ‰‹æœ¯è®¡åˆ’åŸºäºå¤–ç§‘åŒ»ç”Ÿç»éªŒï¼Œè¾ƒå°‘é‡‡ç”¨å¾®åˆ›æŠ€æœ¯ï¼Œä¸”æ‰‹æœ¯ç»“æœä¸ä¸€è‡´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æµç¨‹ï¼Œè¯¥æµç¨‹å¯ä»å¸¸è§„ä¸´åºŠMRæ‰«æç”Ÿæˆè¶…åˆ†è¾¨ç‡çš„ã€é’ˆå¯¹æ‚£è€…ç‰¹å®šçš„3Dä¼ªå¥åº·ç›®æ ‡å½¢æ€ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰è®¡ç®—å„å‘åŒæ€§è¶…åˆ†è¾¨ç‡MRä½“ç§¯ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šæ ‡ç­¾è‡ªå®šä¹‰è®­ç»ƒç½‘ç»œå¯¹è‚¡éª¨ã€èƒ«éª¨ã€é«Œéª¨å’Œè…“éª¨è¿›è¡Œåˆ†å‰²ã€‚æœ€åï¼Œæˆ‘ä»¬è®­ç»ƒå°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDMï¼‰ï¼Œä»¥ç”Ÿæˆæ§½çŠ¶åŒºåŸŸçš„ä¼ªå¥åº·ç›®æ ‡å½¢æ€ã€‚ä¸ä¹‹å‰ç”Ÿæˆä¼ªå¥åº·ä½åˆ†è¾¨ç‡3D MRå›¾åƒçš„å·¥ä½œç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆç”¨äºæœ¯å‰å’Œæœ¯ä¸­çš„äºšæ¯«ç±³åˆ†è¾¨ç‡çš„3Då½¢æ€ã€‚è¿™äº›å¯ä»¥ä½œä¸ºé‡å¡‘è‚¡éª¨æ§½åŒæ—¶ä¿ç•™åŸç”Ÿé«Œéª¨å…³èŠ‚çš„æœ¯å‰è“å›¾ã€‚æ­¤å¤–ï¼Œä¸å…¶ä»–å·¥ä½œç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æµç¨‹ä¸éœ€è¦CTï¼Œå‡å°‘äº†è¾å°„é‡ã€‚æˆ‘ä»¬åœ¨2 TNDæ‚£è€…ä¸Šå¯¹è¿›è¡Œäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ”¹å–„æ§½è§’ï¼ˆSAï¼‰å’Œæ§½æ·±ï¼ˆTGDï¼‰ã€‚ä»£ç å’Œäº¤äº’å¼å¯è§†åŒ–å†…å®¹å¯åœ¨<a target="_blank" rel="noopener" href="https://wehrlimi.github.io/sr-3d-planning/">https://wehrlimi.github.io/sr-3d-planning/</a>ä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06076v1">PDF</a> Accepted at MICCAI COLAS Workshop 2025. Code:   <a target="_blank" rel="noopener" href="https://wehrlimi.github.io/sr-3d-planning/">https://wehrlimi.github.io/sr-3d-planning/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ²»ç–—Trochlear Dysplasiaï¼ˆTDï¼‰çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆè¶…åˆ†è¾¨ç‡çš„3Dä¼ªå¥åº·ç›®æ ‡å½¢æ€æ¥æé«˜æ‰‹æœ¯æ•ˆæœã€‚è¯¥æ–¹æ³•åˆ©ç”¨éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰è®¡ç®—è¶…åˆ†è¾¨ç‡MRä½“ç§¯ï¼Œå¹¶é€šè¿‡å¤šæ ‡ç­¾å®šåˆ¶ç½‘ç»œå¯¹è‚¡éª¨ã€èƒ«éª¨ã€è†ç›–éª¨å’Œè…“éª¨è¿›è¡Œåˆ†å‰²ã€‚æœ€åé€šè¿‡å°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDMï¼‰ç”Ÿæˆä¼ªå¥åº·çš„ç›®æ¨™å½¢æ€ã€‚è¿™ç§æ–¹æ³•å¯äº§ç”Ÿäºšæ¯«ç±³åˆ†è¾¨ç‡çš„3Då½¢çŠ¶ï¼Œç”¨äºæœ¯å‰å’Œæœ¯ä¸­æŒ‡å¯¼æ‰‹æœ¯æ“ä½œã€‚å¹¶ä¸”ç›¸æ¯”ä»¥å¾€çš„æ–¹æ³•ï¼Œæœ¬æ–¹æ³•ä¸éœ€è¦CTæ‰«æï¼Œå‡å°‘äº†è¾å°„æš´éœ²ã€‚åœ¨25åTDæ‚£è€…ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—æ”¹å–„æ§½è§’ï¼ˆSAï¼‰å’Œè‚¡éª¨æ²Ÿæ·±åº¦ï¼ˆTGDï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ²»ç–—Trochlear Dysplasiaä¸»è¦ä¾èµ–ä½åˆ†è¾¨ç‡MRæ‰«æå’Œæ‰‹æœ¯ç›´è§‰ï¼Œæ‰‹æœ¯æ•ˆæœä¸ä¸€è‡´ã€‚</li>
<li>æå‡ºä¸€ç§ç”Ÿæˆè¶…åˆ†è¾¨ç‡ã€ç—…äººç‰¹å®šçš„3Dä¼ªå¥åº·ç›®æ ‡å½¢æ€çš„ç®¡é“ã€‚</li>
<li>åˆ©ç”¨éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰è®¡ç®—è¶…åˆ†è¾¨ç‡MRä½“ç§¯ã€‚</li>
<li>é€šè¿‡å¤šæ ‡ç­¾å®šåˆ¶ç½‘ç»œå¯¹éª¨éª¼è¿›è¡Œåˆ†å‰²ã€‚</li>
<li>é‡‡ç”¨å°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDMï¼‰ç”Ÿæˆä¼ªå¥åº·çš„ç›®æ ‡å½¢æ€ã€‚</li>
<li>è¯¥æ–¹æ³•å¯äº§ç”Ÿäºšæ¯«ç±³åˆ†è¾¨ç‡çš„3Då½¢çŠ¶ï¼Œé€‚ç”¨äºæœ¯å‰å’Œæœ¯ä¸­æŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06076">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-69e3f1e035b2f539de5cab25b848a2b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14015175721d02ae2435a1d3c421ede5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-75b3de6e3f9c1cb871b063ae94ce5aee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20f42d52504c59b5c4fdff71cf2d8d0b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LV-Net-Anatomy-aware-lateral-ventricle-shape-modeling-with-a-case-study-on-Alzheimerâ€™s-disease-the-Australian-Imaging-Biomarkers-and-Lifestyle-flagship-study-of-ageing"><a href="#LV-Net-Anatomy-aware-lateral-ventricle-shape-modeling-with-a-case-study-on-Alzheimerâ€™s-disease-the-Australian-Imaging-Biomarkers-and-Lifestyle-flagship-study-of-ageing" class="headerlink" title="LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study   on Alzheimerâ€™s disease, the Australian Imaging Biomarkers and Lifestyle   flagship study of ageing"></a>LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study   on Alzheimerâ€™s disease, the Australian Imaging Biomarkers and Lifestyle   flagship study of ageing</h2><p><strong>Authors:Wonjung Park, Suhyun Ahn, Jinah Park</strong></p>
<p>Lateral ventricle (LV) shape analysis holds promise as a biomarker for neurological diseases; however, challenges remain due to substantial shape variability across individuals and segmentation difficulties arising from limited MRI resolution. We introduce LV-Net, a novel framework for producing individualized 3D LV meshes from brain MRI by deforming an anatomy-aware joint LV-hippocampus template mesh. By incorporating anatomical relationships embedded within the joint template, LV-Net reduces boundary segmentation artifacts and improves reconstruction robustness. In addition, by classifying the vertices of the template mesh based on their anatomical adjacency, our method enhances point correspondence across subjects, leading to more accurate LV shape statistics. We demonstrate that LV-Net achieves superior reconstruction accuracy, even in the presence of segmentation imperfections, and delivers more reliable shape descriptors across diverse datasets. Finally, we apply LV-Net to Alzheimerâ€™s disease analysis, identifying LV subregions that show significantly associations with the disease relative to cognitively normal controls. The codes for LV shape modeling are available at <a target="_blank" rel="noopener" href="https://github.com/PWonjung/LV_Shape_Modeling">https://github.com/PWonjung/LV_Shape_Modeling</a>. </p>
<blockquote>
<p>ä¾§è„‘å®¤ï¼ˆLVï¼‰å½¢æ€åˆ†æä½œä¸ºç¥ç»ç–¾ç—…çš„ç”Ÿç‰©æ ‡å¿—ç‰©å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºä¸ªä½“é—´å­˜åœ¨çš„æ˜æ˜¾å½¢æ€å·®å¼‚ä»¥åŠMRIåˆ†è¾¨ç‡é™åˆ¶å¯¼è‡´çš„åˆ†å‰²å›°éš¾ï¼Œä»å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†LV-Netï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œé€šè¿‡å˜å½¢ä¸€ä¸ªäº†è§£ç»“æ„çš„è”åˆLV-æµ·é©¬æ¨¡æ¿ç½‘æ ¼ï¼Œä»è„‘éƒ¨MRIç”Ÿæˆä¸ªæ€§åŒ–çš„3D LVç½‘æ ¼ã€‚é€šè¿‡èå…¥è”åˆæ¨¡æ¿å†…çš„ç»“æ„å…³ç³»ï¼ŒLV-Netå‡å°‘äº†è¾¹ç•Œåˆ†å‰²ä¼ªå½±ï¼Œæé«˜äº†é‡å»ºçš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯¹æ¨¡æ¿ç½‘æ ¼çš„é¡¶ç‚¹æ ¹æ®å…¶ç»“æ„é‚»æ¥è¿›è¡Œåˆ†ç±»ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†è·¨ä¸»ä½“çš„ç‚¹å¯¹åº”å…³ç³»ï¼Œä»è€Œå¾—åˆ°æ›´å‡†ç¡®çš„LVå½¢æ€ç»Ÿè®¡ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå³ä½¿åœ¨å­˜åœ¨åˆ†å‰²ç¼ºé™·çš„æƒ…å†µä¸‹ï¼ŒLV-Netä¹Ÿèƒ½å®ç°æ›´é«˜çš„é‡å»ºç²¾åº¦ï¼Œå¹¶åœ¨å„ç§æ•°æ®é›†ä¸­æä¾›æ›´å¯é çš„å½¢çŠ¶æè¿°ç¬¦ã€‚æœ€åï¼Œæˆ‘ä»¬å°†LV-Netåº”ç”¨äºé˜¿å°”èŒ¨æµ·é»˜ç—…åˆ†æï¼Œè¯†åˆ«å‡ºä¸è®¤çŸ¥æ­£å¸¸å¯¹ç…§ç›¸æ¯”ï¼Œä¸ç–¾ç—…æœ‰æ˜¾è‘—å…³è”çš„LVå­åŒºåŸŸã€‚LVå½¢æ€å»ºæ¨¡çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/PWonjung/LV_Shape_Modeling%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/PWonjung/LV_Shape_Modelingæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06055v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºåŒ»å­¦å›¾åƒçš„ä¸‰ç»´å»ºæ¨¡æŠ€æœ¯ï¼ŒLV-Netæ¡†æ¶é€šè¿‡ä¸ªæ€§åŒ–åœ°æ„å»ºä¾§è„‘å®¤ï¼ˆLVï¼‰çš„3Dç½‘æ ¼ï¼Œä¸ºç¥ç»æ€§ç–¾ç—…ç ”ç©¶æä¾›äº†æ–°çš„è§†è§’ã€‚è¯¥æ¡†æ¶é‡‡ç”¨è§£å‰–ç»“æ„æ„ŸçŸ¥çš„è”åˆLV-æµ·é©¬æ¨¡æ¿ç½‘æ ¼ï¼Œé€šè¿‡èå…¥è§£å‰–å…³ç³»ï¼Œæé«˜äº†é‡å»ºçš„ç¨³å¥æ€§å¹¶é™ä½äº†è¾¹ç•Œåˆ†å‰²çš„è¯¯å·®ã€‚LV-Netæé«˜äº†é¡¶ç‚¹å¯¹åº”çš„å‡†ç¡®æ€§ï¼Œç”Ÿæˆæ›´ç²¾ç¡®çš„LVå½¢çŠ¶ç»Ÿè®¡ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒLV-Netåœ¨åˆ†å‰²ä¸å®Œç¾çš„æƒ…å†µä¸‹ä»å…·æœ‰å‡ºè‰²çš„é‡å»ºç²¾åº¦ï¼Œä¸ºä¸åŒæ•°æ®é›†æä¾›äº†æ›´å¯é çš„å½¢çŠ¶æè¿°ã€‚æœ€åï¼ŒLV-Netåœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…åˆ†æä¸­çš„åº”ç”¨æ­ç¤ºäº†ä¸ç–¾ç—…ç›¸å…³çš„LVå­åŒºåŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LV-Netæ˜¯ä¸€ä¸ªåŸºäºåŒ»å­¦å›¾åƒçš„ä¸‰ç»´å»ºæ¨¡æ¡†æ¶ï¼Œå¯ä»¥ä¸ªæ€§åŒ–åœ°æ„å»ºä¾§è„‘å®¤ï¼ˆLVï¼‰çš„3Dç½‘æ ¼ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡é‡‡ç”¨è§£å‰–ç»“æ„æ„ŸçŸ¥çš„è”åˆLV-æµ·é©¬æ¨¡æ¿ç½‘æ ¼ï¼Œæé«˜äº†é‡å»ºçš„ç¨³å¥æ€§ã€‚</li>
<li>LV-Neté€šè¿‡èå…¥è§£å‰–å…³ç³»ï¼Œé™ä½äº†è¾¹ç•Œåˆ†å‰²çš„è¯¯å·®å’Œå‡å°‘åˆ†å‰²å›°éš¾çš„é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†ç½‘æ ¼é¡¶ç‚¹å¯¹åº”çš„å‡†ç¡®æ€§ï¼Œæœ‰åŠ©äºç”Ÿæˆæ›´ç²¾ç¡®çš„LVå½¢çŠ¶ç»Ÿè®¡ä¿¡æ¯ã€‚</li>
<li>LV-Netå³ä½¿åœ¨åˆ†å‰²ä¸å®Œç¾çš„æƒ…å†µä¸‹ä¹Ÿå…·æœ‰å‡ºè‰²çš„é‡å»ºç²¾åº¦ï¼Œèƒ½å¤Ÿæä¾›å¯é çš„å½¢çŠ¶æè¿°ã€‚</li>
<li>LV-Netåœ¨ä¸åŒæ•°æ®é›†ä¸­è¡¨ç°ä¼˜è¶Šï¼Œèƒ½å¤Ÿä¸ºç¥ç»æ€§ç–¾ç—…ç ”ç©¶æä¾›æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd2b8df93dbbf7ff5a7caa9b077ee705.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbaaee67cc94ff96ddb71af6a305f93e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1210d3c13b4375a4d2010cbd19704150.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77d5acbe19955fed079a0ed47b1db8eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b66ea3cfcfafc6271a4766387036dd0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Deployment-of-Pretrained-MRI-Transformers-in-Brain-Imaging-Tasks"><a href="#Few-Shot-Deployment-of-Pretrained-MRI-Transformers-in-Brain-Imaging-Tasks" class="headerlink" title="Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging   Tasks"></a>Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging   Tasks</h2><p><strong>Authors:Mengyu Li, Guoyao Shen, Chad W. Farris, Xin Zhang</strong></p>
<p>Machine learning using transformers has shown great potential in medical imaging, but its real-world applicability remains limited due to the scarcity of annotated data. In this study, we propose a practical framework for the few-shot deployment of pretrained MRI transformers in diverse brain imaging tasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a large-scale, multi-cohort brain MRI dataset comprising over 31 million slices, we obtain highly transferable latent representations that generalize well across tasks and datasets. For high-level tasks such as classification, a frozen MAE encoder combined with a lightweight linear head achieves state-of-the-art accuracy in MRI sequence identification with minimal supervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a hybrid architecture that fuses multiscale CNN features with pretrained MAE embeddings. This model consistently outperforms other strong baselines in both skull stripping and multi-class anatomical segmentation under data-limited conditions. With extensive quantitative and qualitative evaluations, our framework demonstrates efficiency, stability, and scalability, suggesting its suitability for low-resource clinical environments and broader neuroimaging applications. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œåˆ©ç”¨å˜å‹å™¨è¿›è¡Œæœºå™¨å­¦ä¹ å·²æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç”±äºç¼ºä¹æ ‡æ³¨æ•°æ®ï¼Œå…¶åœ¨å®é™…ä¸–ç•Œä¸­çš„é€‚ç”¨æ€§ä»ç„¶æœ‰é™ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå®ç”¨çš„æ¡†æ¶ï¼Œç”¨äºåœ¨å¤šç§è„‘æˆåƒä»»åŠ¡ä¸­éƒ¨ç½²å°‘é‡çš„é¢„è®­ç»ƒMRIå˜å‹å™¨ã€‚é€šè¿‡åœ¨ä¸€ä¸ªå¤§è§„æ¨¡ã€å¤šé˜Ÿåˆ—çš„è„‘MRIæ•°æ®é›†ï¼ˆåŒ…å«è¶…è¿‡3100ä¸‡å¼ åˆ‡ç‰‡ï¼‰ä¸Šåˆ©ç”¨Masked Autoencoderï¼ˆMAEï¼‰çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œæˆ‘ä»¬è·å¾—äº†é«˜åº¦å¯è¿ç§»çš„æ½œåœ¨è¡¨ç¤ºï¼Œè¿™äº›è¡¨ç¤ºåœ¨ä»»åŠ¡å’Œæ•°æ®é›†ä¹‹é—´å…·æœ‰å¾ˆå¥½çš„é€šç”¨æ€§ã€‚å¯¹äºé«˜çº§ä»»åŠ¡ï¼Œå¦‚åˆ†ç±»ï¼Œä½¿ç”¨å†»ç»“çš„MAEç¼–ç å™¨ç»“åˆè½»é‡çº§çš„çº¿æ€§å¤´ï¼Œå¯ä»¥åœ¨æå°‘çš„ç›‘ç£ä¸‹å®ç°MRIåºåˆ—è¯†åˆ«çš„æœ€æ–°å‡†ç¡®æ€§ã€‚å¯¹äºä½çº§ä»»åŠ¡ï¼Œå¦‚åˆ†å‰²ï¼Œæˆ‘ä»¬æå‡ºäº†MAE-FUnetï¼Œè¿™æ˜¯ä¸€ç§æ··åˆæ¶æ„ï¼Œèåˆäº†å¤šå°ºåº¦CNNç‰¹å¾ä¸é¢„è®­ç»ƒçš„MAEåµŒå…¥ã€‚åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹åœ¨é¢…éª¨å‰¥ç¦»å’Œå¤šç±»è§£å‰–åˆ†å‰²æ–¹é¢éƒ½å§‹ç»ˆä¼˜äºå…¶ä»–å¼ºå¤§çš„åŸºçº¿ã€‚é€šè¿‡å¹¿æ³›çš„å®šé‡å’Œå®šæ€§è¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ¡†æ¶è¯æ˜äº†å…¶æ•ˆç‡ã€ç¨³å®šæ€§å’Œå¯æ‰©å±•æ€§ï¼Œè¡¨æ˜å®ƒé€‚ç”¨äºèµ„æºæœ‰é™çš„ä¸´åºŠç¯å¢ƒå’Œæ›´å¹¿æ³›çš„ç¥ç»å½±åƒåº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05783v1">PDF</a> 30 pages, 8 figures, 7 tables</p>
<p><strong>æ‘˜è¦</strong><br>    åŒ»å­¦æˆåƒä¸­æœºå™¨å­¦ä¹ åˆ©ç”¨è½¬æ¢å™¨å±•ç¤ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç”±äºç¼ºä¹æ ‡æ³¨æ•°æ®ï¼Œå…¶å®ç”¨æ€§å—é™ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§å®ç”¨æ¡†æ¶ï¼Œç”¨äºåœ¨å°‘é‡æ ·æœ¬ä¸‹éƒ¨ç½²é¢„è®­ç»ƒçš„MRIè½¬æ¢å™¨ï¼Œç”¨äºå¤šç§è„‘æˆåƒä»»åŠ¡ã€‚åˆ©ç”¨å¤§è§„æ¨¡å¤šé˜Ÿåˆ—è„‘MRIæ•°æ®é›†ï¼ˆè¶…è¿‡31ç™¾ä¸‡åˆ‡ç‰‡ï¼‰çš„Masked Autoencoderï¼ˆMAEï¼‰é¢„è®­ç»ƒç­–ç•¥ï¼Œæˆ‘ä»¬è·å¾—é«˜åº¦å¯è¿ç§»çš„æ½œåœ¨è¡¨ç¤ºï¼Œå¯åœ¨ä»»åŠ¡å’Œæ•°æ®é›†ä¹‹é—´è‰¯å¥½åœ°é€šç”¨åŒ–ã€‚å¯¹äºé«˜çº§ä»»åŠ¡å¦‚åˆ†ç±»ï¼Œå†»ç»“çš„MAEç¼–ç å™¨ç»“åˆè½»é‡çº§çº¿æ€§å¤´å®ç°äº†MRIåºåˆ—è¯†åˆ«çš„æœ€æ–°å‡†ç¡®æ€§ï¼Œç›‘ç£éœ€æ±‚æå°‘ã€‚å¯¹äºä½çº§ä»»åŠ¡å¦‚åˆ†å‰²ï¼Œæˆ‘ä»¬æå‡ºMAE-FUnetæ··åˆæ¶æ„ï¼Œå®ƒå°†å¤šå°ºåº¦CNNç‰¹å¾ä¸é¢„è®­ç»ƒçš„MAEåµŒå…¥ç›¸ç»“åˆã€‚è¯¥æ¨¡å‹åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œåœ¨é¢…éª¨å‰¥ç¦»å’Œå¤šç±»è§£å‰–åˆ†å‰²æ–¹é¢å‡ä¼˜äºå…¶ä»–å¼ºå¤§çš„åŸºçº¿æ¨¡å‹ã€‚é€šè¿‡å¹¿æ³›çš„å®šé‡å’Œå®šæ€§è¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å±•ç°å‡ºé«˜æ•ˆæ€§ã€ç¨³å®šæ€§å’Œå¯æ‰©å±•æ€§ï¼Œé€‚ç”¨äºèµ„æºæœ‰é™çš„ä¸´åºŠç¯å¢ƒå’Œæ›´å¹¿æ³›çš„ç¥ç»æˆåƒåº”ç”¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ åœ¨åŒ»å­¦æˆåƒä¸­ä½¿ç”¨è½¬æ¢å™¨å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†æ•°æ®ç¼ºä¹æ ‡æ³¨é™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åˆ©ç”¨å°‘é‡æ•°æ®æ ·æœ¬çš„å®ç”¨æ¡†æ¶ï¼Œç”¨äºéƒ¨ç½²é¢„è®­ç»ƒçš„MRIè½¬æ¢å™¨ä»¥æ‰§è¡Œå¤šç§è„‘æˆåƒä»»åŠ¡ã€‚</li>
<li>ä½¿ç”¨Masked Autoencoderï¼ˆMAEï¼‰é¢„è®­ç»ƒç­–ç•¥åœ¨å¤§å‹å¤šé˜Ÿåˆ—è„‘MRIæ•°æ®é›†ä¸Šè·å¾—é«˜åº¦å¯è¿ç§»çš„æ½œåœ¨è¡¨ç¤ºã€‚</li>
<li>åœ¨MRIåºåˆ—åˆ†ç±»ç­‰é«˜çº§ä»»åŠ¡ä¸Šå®ç°äº†æœ€æ–°å‡†ç¡®æ€§æ°´å¹³ï¼Œä½¿ç”¨å†»ç»“çš„MAEç¼–ç å™¨å’Œè½»é‡çº§çº¿æ€§å¤´ï¼Œç›‘ç£éœ€æ±‚æä½ã€‚</li>
<li>å¯¹äºåˆ†å‰²ç­‰ä½çº§ä»»åŠ¡ï¼Œæå‡ºäº†MAE-FUnetæ··åˆæ¶æ„ï¼Œç»“åˆäº†å¤šå°ºåº¦CNNç‰¹å¾å’Œé¢„è®­ç»ƒçš„MAEåµŒå…¥ã€‚</li>
<li>åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹åœ¨é¢…éª¨å‰¥ç¦»å’Œå¤šç±»è§£å‰–åˆ†å‰²æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05783">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-902914f04553879e8aca889e34e38ebb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-de58066bf898e3df5e6f283375875c4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b270e043182bf3326e98bc3533e1cb45.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MAISI-v2-Accelerated-3D-High-Resolution-Medical-Image-Synthesis-with-Rectified-Flow-and-Region-specific-Contrastive-Loss"><a href="#MAISI-v2-Accelerated-3D-High-Resolution-Medical-Image-Synthesis-with-Rectified-Flow-and-Region-specific-Contrastive-Loss" class="headerlink" title="MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with   Rectified Flow and Region-specific Contrastive Loss"></a>MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with   Rectified Flow and Region-specific Contrastive Loss</h2><p><strong>Authors:Can Zhao, Pengfei Guo, Dong Yang, Yucheng Tang, Yufan He, Benjamin Simon, Mason Belue, Stephanie Harmon, Baris Turkbey, Daguang Xu</strong></p>
<p>Medical image synthesis is an important topic for both clinical and research applications. Recently, diffusion models have become a leading approach in this area. Despite their strengths, many existing methods struggle with (1) limited generalizability that only work for specific body regions or voxel spacings, (2) slow inference, which is a common issue for diffusion models, and (3) weak alignment with input conditions, which is a critical issue for medical imaging. MAISI, a previously proposed framework, addresses generalizability issues but still suffers from slow inference and limited condition consistency. In this work, we present MAISI-v2, the first accelerated 3D medical image synthesis framework that integrates rectified flow to enable fast and high quality generation. To further enhance condition fidelity, we introduce a novel region-specific contrastive loss to enhance the sensitivity to region of interest. Our experiments show that MAISI-v2 can achieve SOTA image quality with $33 \times$ acceleration for latent diffusion model. We also conducted a downstream segmentation experiment to show that the synthetic images can be used for data augmentation. We release our code, training details, model weights, and a GUI demo to facilitate reproducibility and promote further development within the community. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆæˆåœ¨ä¸´åºŠå’Œç ”ç©¶åº”ç”¨ä¸­éƒ½æ˜¯ä¸€ä¸ªé‡è¦çš„ä¸»é¢˜ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹å·²æˆä¸ºè¯¥é¢†åŸŸçš„ä¸»æµæ–¹æ³•ã€‚å°½ç®¡å®ƒä»¬å…·æœ‰ä¼˜åŠ¿ï¼Œä½†è®¸å¤šç°æœ‰æ–¹æ³•ä»ç„¶é¢ä¸´ï¼ˆ1ï¼‰é€šç”¨æ€§æœ‰é™ï¼Œä»…é€‚ç”¨äºç‰¹å®šéƒ¨ä½æˆ–ä½“ç´ é—´è·ï¼›ï¼ˆ2ï¼‰æ¨ç†é€Ÿåº¦æ…¢ï¼Œè¿™æ˜¯æ‰©æ•£æ¨¡å‹çš„å¸¸è§é—®é¢˜ï¼›ï¼ˆ3ï¼‰ä¸è¾“å…¥æ¡ä»¶å¯¹é½æ€§å·®ï¼Œè¿™å¯¹åŒ»å­¦å½±åƒæ¥è¯´æ˜¯å…³é”®é—®é¢˜ã€‚ä¹‹å‰æå‡ºçš„MAISIæ¡†æ¶è§£å†³äº†é€šç”¨æ€§é—®é¢˜ï¼Œä½†ä»ç„¶å­˜åœ¨æ¨ç†é€Ÿåº¦æ…¢å’Œæ¡ä»¶ä¸€è‡´æ€§æœ‰é™çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MAISI-v2ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé›†æˆçš„åŠ é€Ÿ3DåŒ»å­¦å›¾åƒåˆæˆæ¡†æ¶ï¼Œé€šè¿‡é‡‡ç”¨ä¿®æ­£æµæ¥å®ç°å¿«é€Ÿé«˜è´¨é‡ç”Ÿæˆã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ¡ä»¶ä¿çœŸåº¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹åŒºåŸŸç‰¹å®šå¯¹æ¯”æŸå¤±ï¼Œä»¥æé«˜å¯¹æ„Ÿå…´è¶£åŒºåŸŸçš„æ•æ„Ÿæ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒMAISI-v2å¯ä»¥åœ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸Šå®ç°$33 \times$çš„åŠ é€Ÿï¼ŒåŒæ—¶è¾¾åˆ°æœ€æ–°å›¾åƒè´¨é‡ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†ä¸‹æ¸¸åˆ†å‰²å®éªŒï¼Œä»¥è¯æ˜åˆæˆå›¾åƒå¯ç”¨äºæ•°æ®å¢å¼ºã€‚æˆ‘ä»¬å…¬å¼€äº†ä»£ç ã€è®­ç»ƒç»†èŠ‚ã€æ¨¡å‹æƒé‡å’ŒGUIæ¼”ç¤ºï¼Œä»¥ä¿ƒè¿›ç¤¾åŒºå†…çš„å¯é‡å¤æ€§å’Œè¿›ä¸€æ­¥å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05772v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ä»‹ç»äº†åŒ»å­¦å›¾åƒåˆæˆçš„æ–°æ–¹æ³•MAISI-v2ï¼Œå®ƒæ˜¯åŠ é€Ÿçš„3DåŒ»å­¦å›¾åƒåˆæˆæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆä¿®æ­£æµå®ç°å¿«é€Ÿå’Œé«˜è´¨ç”Ÿæˆã€‚ä¸ºè§£å†³é€šç”¨æ€§å’Œæ¡ä»¶ä¸€è‡´æ€§é—®é¢˜ï¼Œå¼•å…¥åŒºåŸŸç‰¹å®šå¯¹æ¯”æŸå¤±ä»¥å¢å¼ºæ„Ÿå…´è¶£åŒºåŸŸçš„æ•æ„Ÿæ€§ã€‚å®éªŒæ˜¾ç¤ºï¼ŒMAISI-v2èƒ½æé«˜æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å›¾åƒè´¨é‡ï¼Œè¾¾åˆ°$33 \times$åŠ é€Ÿæ•ˆæœï¼Œä¸”åˆæˆçš„å›¾åƒå¯ç”¨äºæ•°æ®å¢å¼ºã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆæˆåœ¨ä¸´åºŠå’Œç ”ç©¶åº”ç”¨ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆæˆä¸­çš„ä¸»å¯¼åœ°ä½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨çš„å±€é™æ€§ï¼Œå¦‚æœ‰é™é€šç”¨æ€§ã€æ¨ç†é€Ÿåº¦æ…¢å’Œæ¡ä»¶ä¸€è‡´æ€§å·®ã€‚</li>
<li>MAISI-v2æ¡†æ¶çš„æå‡ºï¼Œè§£å†³äº†é€šç”¨æ€§é—®é¢˜ï¼Œå®ç°äº†å¿«é€Ÿé«˜è´¨é‡ç”Ÿæˆã€‚</li>
<li>é€šè¿‡å¼•å…¥åŒºåŸŸç‰¹å®šå¯¹æ¯”æŸå¤±ï¼Œå¢å¼ºå¯¹æ„Ÿå…´è¶£åŒºåŸŸçš„æ•æ„Ÿæ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºMAISI-v2å…·æœ‰ä¼˜å¼‚çš„å›¾åƒè´¨é‡å’ŒåŠ é€Ÿæ•ˆæœã€‚</li>
<li>åˆæˆçš„å›¾åƒå¯ç”¨äºæ•°æ®å¢å¼ºï¼Œå¹¶å…¬å¼€äº†ä»£ç ã€è®­ç»ƒç»†èŠ‚ã€æ¨¡å‹æƒé‡å’ŒGUIæ¼”ç¤ºï¼Œä»¥ä¿ƒè¿›ç¤¾åŒºå†…çš„å¯é‡å¤æ€§å’Œè¿›ä¸€æ­¥å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05772">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-99ccdc6ec79bededbc9904f95a3d7982.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bceeb8c7cb53b53392ffa55dda043101.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f843b263d3f179e3c12ee60cdd285d53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c69c1811a34f158d5341af8c4ed3bac8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e50db7b49953724dc966807c93730ee3.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SiCmiR-Atlas-Single-Cell-miRNA-Landscapes-Reveals-Hub-miRNA-and-Network-Signatures-in-Human-Cancers"><a href="#SiCmiR-Atlas-Single-Cell-miRNA-Landscapes-Reveals-Hub-miRNA-and-Network-Signatures-in-Human-Cancers" class="headerlink" title="SiCmiR Atlas: Single-Cell miRNA Landscapes Reveals Hub-miRNA and Network   Signatures in Human Cancers"></a>SiCmiR Atlas: Single-Cell miRNA Landscapes Reveals Hub-miRNA and Network   Signatures in Human Cancers</h2><p><strong>Authors:Xiao-Xuan Cai, Jing-Shan Liao, Jia-Jun Ma, Yu-Xuan Pang, Yi-Gang Chen, Yang-Chi-Dung Lin, Yi-Dan Chen, Xin Cao, Yi-Cheng Zhang, Tao-Sheng Xu, Tzong-Yi Lee, Hsi-Yuan Huang, Hsien-Da Huang</strong></p>
<p>microRNA are pivotal post-transcriptional regulators whose single-cell behavior has remained largely inaccessible owing to technical barriers in single-cell small-RNA profiling. We present SiCmiR, a two-layer neural network that predicts miRNA expression profile from only 977 LINCS L1000 landmark genes reducing sensitivity to dropout of single-cell RNA-seq data. Proof-of-concept analyses illustrate how SiCmiR can uncover candidate hub-miRNAs in bulk-seq cell lines and hepatocellular carcinoma, scRNA-seq pancreatic ductal carcinoma and ACTH-secreting pituitary adenoma and extracellular-vesicle-mediated crosstalk in glioblastoma. Trained on 6462 TCGA paired miRNA-mRNA samples, SiCmiR attains state-of-the-art accuracy on held-out cancers and generalizes to unseen cancer types, drug perturbations and scRNA-seq. We next constructed SiCmiR-Atlas, containing 632 public datasets, 9.36 million cells, 726 cell types, which is the first dedicated database of single-cell mature miRNA expressionâ€“providing interactive visualization, biomarker identification and cell-type-resolved miRNA-target networks. SiCmiR transforms bulk-derived statistical power into a single-cell view of miRNA biology and provides a community resource SiCmiR Atlas for biomarker discovery. SiCmiR Atlas is avilable at <a target="_blank" rel="noopener" href="https://awi.cuhk.edu.cn/~SiCmiR/">https://awi.cuhk.edu.cn/~SiCmiR/</a>. </p>
<blockquote>
<p>microRNAæ˜¯å…³é”®çš„è½¬å½•åè°ƒæ§å› å­ï¼Œç”±äºå…¶å•ç»†èƒè¡Œä¸ºçš„ç‰¹ç‚¹ï¼Œåœ¨å•ç»†èƒå°RNAå›¾è°±åˆ†æä¸­ä»å­˜åœ¨è¾ƒå¤§çš„æŠ€æœ¯éšœç¢ï¼Œå› æ­¤æˆ‘ä»¬å¯¹å•ç»†èƒRNAæµ‹åºæ•°æ®çš„ä¸¢å¤±å…·æœ‰é«˜åº¦çš„æ•æ„Ÿæ€§ã€‚æˆ‘ä»¬æ¨å‡ºäº†SiCmiRï¼Œè¿™æ˜¯ä¸€ç§ä¸¤å±‚ç¥ç»ç½‘ç»œï¼Œå®ƒä»…ä»977ä¸ªLINCS L1000æ ‡å¿—æ€§åŸºå› é¢„æµ‹miRNAè¡¨è¾¾å›¾è°±ï¼Œé™ä½äº†å¯¹å•ç»†èƒRNAæµ‹åºæ•°æ®ä¸¢å¤±çš„æ•æ„Ÿæ€§ã€‚æ¦‚å¿µéªŒè¯åˆ†æè¯´æ˜äº†SiCmiRå¦‚ä½•èƒ½åœ¨å¤§è§„æ¨¡æµ‹åºç»†èƒæ ªã€è‚ç»†èƒç™Œã€èƒ°è…ºå¯¼ç®¡ç™Œå’ŒACTHåˆ†æ³Œå‚ä½“è…ºç˜¤ä»¥åŠèƒ¶è´¨æ¯ç»†èƒç˜¤çš„èƒå¤–å›Šæ³¡ä»‹å¯¼çš„ä¸²è¯ä¸­æ­ç¤ºå€™é€‰çš„æ¢çº½miRNAã€‚SiCmiRåœ¨6462ä¸ªTCGAé…å¯¹miRNA-mRNAæ ·æœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯¹æœªåŒ…å«çš„ç™Œç—‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œå¹¶å¯æ¨å¹¿åˆ°æœªè§è¿‡çš„ç™Œç—‡ç±»å‹ã€è¯ç‰©å¹²æ‰°å’Œå•ç»†èƒRNAæµ‹åºã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ„å»ºäº†SiCmiR-Atlasï¼ŒåŒ…å«632ä¸ªå…¬å…±æ•°æ®é›†ã€936ä¸‡ä¸ªç»†èƒã€726ç§ç»†èƒç±»å‹ï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹å•ç»†èƒæˆç†ŸmiRNAè¡¨è¾¾çš„ä¸“ä¸šæ•°æ®åº“â€”â€”æä¾›äº¤äº’å¼å¯è§†åŒ–ã€ç”Ÿç‰©æ ‡è®°ç‰©é‰´å®šå’Œç»†èƒç±»å‹ç‰¹å®šçš„miRNA-é¶æ ‡ç½‘ç»œã€‚SiCmiRå°†å¤§è§„æ¨¡ç»Ÿè®¡èƒ½åŠ›è½¬åŒ–ä¸ºå•ç»†èƒmiRNAç”Ÿç‰©å­¦è§†å›¾ï¼Œå¹¶ä¸ºç”Ÿç‰©æ ‡è®°ç‰©å‘ç°æä¾›äº†ç¤¾åŒºèµ„æºSiCmiR Atlasã€‚SiCmiR Atlaså¯åœ¨<a target="_blank" rel="noopener" href="https://awi.cuhk.edu.cn/~SiCmiR/%E8%AE%BF%E9%97%AE%E3%80%82">https://awi.cuhk.edu.cn/~SiCmiR/è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05692v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†æ–°æŠ€æœ¯SiCmiRï¼Œè¯¥æŠ€æœ¯é€šè¿‡ä¸¤å±‚ç¥ç»ç½‘ç»œé¢„æµ‹miRNAè¡¨è¾¾è°±ï¼Œä»…ä½¿ç”¨977ä¸ªLINCS L1000æ ‡å¿—æ€§åŸºå› ï¼Œæé«˜äº†å¯¹å•ç»†èƒRNA-seqæ•°æ®ä¸¢å¤±çš„æ•æ„Ÿæ€§ã€‚SiCmiRå¯ç”¨äºå‘ç°å€™é€‰ä¸­å¿ƒmiRNAï¼Œåœ¨å¤šç§ç™Œç—‡å’Œè¯ç‰©æ‰°åŠ¨ä¸­è¡¨ç°å‡ºå“è¶Šå‡†ç¡®æ€§ï¼Œå¹¶æ„å»ºé¦–ä¸ªä¸“é—¨ç”¨äºå•ç»†èƒæˆç†ŸmiRNAè¡¨è¾¾çš„æ•°æ®åº“SiCmiR-Atlasã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SiCmiRæ˜¯ä¸€ç§åŸºäºä¸¤å±‚ç¥ç»ç½‘ç»œçš„æŠ€æœ¯ï¼Œç”¨äºé¢„æµ‹miRNAè¡¨è¾¾è°±ã€‚</li>
<li>å®ƒä»…ä½¿ç”¨977ä¸ªLINCS L1000æ ‡å¿—æ€§åŸºå› ï¼Œæé«˜äº†å¯¹å•ç»†èƒRNA-seqæ•°æ®ä¸¢å¤±çš„æ•æ„Ÿæ€§ã€‚</li>
<li>SiCmiRå¯ç”¨äºå‘ç°å¤šç§ç™Œç—‡ä¸­çš„å€™é€‰ä¸­å¿ƒmiRNAï¼ŒåŒ…æ‹¬bulk-seqç»†èƒæ ªã€è‚ç»†èƒç™Œã€èƒ°è…ºå¯¼ç®¡ç™Œå’ŒACTHåˆ†æ³Œå‚ä½“è…ºç˜¤ã€‚</li>
<li>SiCmiRåœ¨ç™Œç—‡ç±»å‹ã€è¯ç‰©æ‰°åŠ¨å’Œå•ç»†èƒRNA-seqä¸Šå…·æœ‰å“è¶Šå‡†ç¡®æ€§ã€‚</li>
<li>æ„å»ºé¦–ä¸ªä¸“é—¨ç”¨äºå•ç»†èƒæˆç†ŸmiRNAè¡¨è¾¾çš„æ•°æ®åº“SiCmiR-Atlasï¼ŒåŒ…å«632ä¸ªå…¬å…±æ•°æ®é›†ã€936ä¸‡ä¸ªç»†èƒå’Œ726ç§ç»†èƒç±»å‹ã€‚</li>
<li>SiCmiRå¯å°†å¤§è§„æ¨¡ç»Ÿè®¡æ•°æ®è½¬åŒ–ä¸ºå•ç»†èƒmiRNAç”Ÿç‰©å­¦è§†è§’ï¼Œå¹¶æä¾›ç¤¾åŒºèµ„æºSiCmiR Atlasç”¨äºç”Ÿç‰©æ ‡å¿—ç‰©å‘ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05692">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-30cffa29f6c29bf1ce74bfa92a13f6e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-394be8dc547ed4e721dc4ccb1d42aaaf.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Generative-AI-for-image-reconstruction-in-Intensity-Interferometry-a-first-attempt"><a href="#Generative-AI-for-image-reconstruction-in-Intensity-Interferometry-a-first-attempt" class="headerlink" title="Generative AI for image reconstruction in Intensity Interferometry: a   first attempt"></a>Generative AI for image reconstruction in Intensity Interferometry: a   first attempt</h2><p><strong>Authors:Km Nitu Rai, Yuri van der Burg, Soumen Basak, Prasenjit Saha, Subrata Sarangi</strong></p>
<p>In the last few years Intensity Interferometry (II) has made significant strides in achieving high-precision resolution of stellar objects at optical wavelengths. Despite these advancements, phase retrieval remains a major challenge due to the nature of photon correlation. This paper explores the application of a conditional Generative Adversarial Network (cGAN) to tackle the problem of image reconstruction in Intensity Interferometry. This approach successfully reconstructs the shape, size, and brightness distribution of a fast-rotating star from sparsely sampled, spatial power spectrum of the source, corresponding to II with four telescopes. Although this particular example could also be addressed using parameter fitting, the results suggest that with larger arrays much more complicated systems could be reconstructed by applying machine-learning techniques to II. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¹²æ¶‰æµ‹é‡æ³•ï¼ˆIntensity Interferometryï¼Œç®€ç§°IIï¼‰åœ¨å…‰å­¦æ³¢é•¿ä¸‹å®ç°æ˜Ÿä½“ç›®æ ‡çš„é«˜ç²¾åº¦åˆ†è¾¨ç‡æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç”±äºå…‰å­ç›¸å…³æ€§æœ¬è´¨çš„å½±å“ï¼Œç›¸ä½æ¢å¤ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æ¢è®¨äº†æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆConditional Generative Adversarial Networkï¼Œç®€ç§°cGANï¼‰åœ¨å¹²æ¶‰æµ‹é‡æ³•å›¾åƒé‡å»ºä¸­çš„åº”ç”¨ã€‚æ­¤æ–¹æ³•æˆåŠŸåœ°ä»ç¨€ç–é‡‡æ ·çš„æºç©ºé—´åŠŸç‡è°±é‡å»ºäº†å¿«é€Ÿæ—‹è½¬æ’æ˜Ÿçš„å½¢çŠ¶ã€å¤§å°å’Œäº®åº¦åˆ†å¸ƒï¼Œå¯¹åº”äºä½¿ç”¨å››ä¸ªæœ›è¿œé•œè¿›è¡Œçš„å¹²æ¶‰æµ‹é‡æ³•ã€‚è™½ç„¶è¿™ä¸ªç‰¹å®šä¾‹å­ä¹Ÿå¯ä»¥é€šè¿‡å‚æ•°æ‹Ÿåˆæ¥è§£å†³ï¼Œä½†ç»“æœè¡¨æ˜ï¼Œå¯¹äºæ›´å¤§çš„é˜µåˆ—ç³»ç»Ÿï¼Œé€šè¿‡æœºå™¨å­¦ä¹ æŠ€æœ¯åº”ç”¨äºå¹²æ¶‰æµ‹é‡æ³•ï¼Œå¯ä»¥é‡å»ºæ›´ä¸ºå¤æ‚çš„ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03398v2">PDF</a> 14 pages, 16 figures</p>
<p><strong>Summary</strong><br>     è¿‘å¹´æ¥ï¼Œå¼ºåº¦å¹²æ¶‰æµ‹é‡æ³•åœ¨å…‰å­¦æ³¢é•¿ä¸Šå¯¹æ’æ˜Ÿç‰©ä½“å®ç°é«˜ç²¾åº¦åˆ†è¾¨ç‡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºå…‰å­ç›¸å…³æ€§ï¼Œç›¸ä½æ¢å¤ä»æ˜¯é‡å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æ¢è®¨äº†æœ‰æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆcGANï¼‰åœ¨å¼ºåº¦å¹²æ¶‰æµ‹é‡æ³•å›¾åƒé‡å»ºä¸­çš„åº”ç”¨ï¼ŒæˆåŠŸä»ç¨€ç–é‡‡æ ·çš„æºç©ºé—´åŠŸç‡è°±ä¸­é‡å»ºäº†å¿«é€Ÿæ—‹è½¬æ’æ˜Ÿçš„å½¢çŠ¶ã€å¤§å°å’Œäº®åº¦åˆ†å¸ƒï¼Œå¯¹åº”äºä½¿ç”¨å››ä¸ªæœ›è¿œé•œçš„å¼ºåº¦å¹²æ¶‰æµ‹é‡æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æœºå™¨å­¦ä¹ æŠ€æœ¯åº”ç”¨äºå¼ºåº¦å¹²æ¶‰æµ‹é‡æ³•ï¼Œå¯ä»¥é‡å»ºæ›´å¤æ‚çš„ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåº¦å¹²æ¶‰æµ‹é‡æ³•ï¼ˆIIï¼‰åœ¨å…‰å­¦æ³¢é•¿ä¸Šå¯¹æ’æ˜Ÿç‰©ä½“å®ç°é«˜ç²¾åº¦åˆ†è¾¨ç‡æœ‰é‡å¤§è¿›å±•ã€‚</li>
<li>ç›¸ä½æ¢å¤ä»æ˜¯å¼ºåº¦å¹²æ¶‰æµ‹é‡æ³•çš„ä¸»è¦æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å› ä¸ºå…‰å­ç›¸å…³æ€§ã€‚</li>
<li>æœ‰æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆcGANï¼‰è¢«åº”ç”¨äºå¼ºåº¦å¹²æ¶‰æµ‹é‡æ³•çš„å›¾åƒé‡å»ºã€‚</li>
<li>cGANæˆåŠŸä»ç¨€ç–é‡‡æ ·çš„æºç©ºé—´åŠŸç‡è°±é‡å»ºäº†å¿«é€Ÿæ—‹è½¬æ’æ˜Ÿçš„å½¢çŠ¶ã€å¤§å°å’Œäº®åº¦åˆ†å¸ƒã€‚</li>
<li>å‚æ•°æ‹Ÿåˆä¹Ÿå¯ä»¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†æœºå™¨å­¦ä¹ æŠ€æœ¯å¯èƒ½ä¸ºæ›´å¤§çš„ç³»ç»Ÿæä¾›æ›´å¤æ‚çš„é‡å»ºè§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ­¤æ–¹æ³•å¯¹äºæ›´å¤æ‚çš„ç³»ç»Ÿå…·æœ‰æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨æ›´å¤§æœ›è¿œé•œé˜µåˆ—çš„å¼ºåº¦å¹²æ¶‰æµ‹é‡æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03398">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bac2c2f0bdc396da5e77a4195a686a07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68ab13b81125de2ebe64d95b17e66a65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9810254167defee8f7dcd29a11faff86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ebd4f82b7b362114b112638f40d5fee2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0dcdfbc4d3588410a0fc7dea84266b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5898def4c0992e12feaeec9833298e8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d44a8faa0bf9a54059e5941dfcbb247.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b09dc2d6dc2033692e591659e169152d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0df564bca96caeadb260dfb2067731ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9f3df862398e63773d7386b55155f8d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Your-other-Left-Vision-Language-Models-Fail-to-Identify-Relative-Positions-in-Medical-Images"><a href="#Your-other-Left-Vision-Language-Models-Fail-to-Identify-Relative-Positions-in-Medical-Images" class="headerlink" title="Your other Left! Vision-Language Models Fail to Identify Relative   Positions in Medical Images"></a>Your other Left! Vision-Language Models Fail to Identify Relative   Positions in Medical Images</h2><p><strong>Authors:Daniel Wolf, Heiko Hillenhagen, Billurvan Taskin, Alex BÃ¤uerle, Meinrad Beer, Michael GÃ¶tz, Timo Ropinski</strong></p>
<p>Clinical decision-making relies heavily on understanding relative positions of anatomical structures and anomalies. Therefore, for Vision-Language Models (VLMs) to be applicable in clinical practice, the ability to accurately determine relative positions on medical images is a fundamental prerequisite. Despite its importance, this capability remains highly underexplored. To address this gap, we evaluate the ability of state-of-the-art VLMs, GPT-4o, Llama3.2, Pixtral, and JanusPro, and find that all models fail at this fundamental task. Inspired by successful approaches in computer vision, we investigate whether visual prompts, such as alphanumeric or colored markers placed on anatomical structures, can enhance performance. While these markers provide moderate improvements, results remain significantly lower on medical images compared to observations made on natural images. Our evaluations suggest that, in medical imaging, VLMs rely more on prior anatomical knowledge than on actual image content for answering relative position questions, often leading to incorrect conclusions. To facilitate further research in this area, we introduce the MIRP , Medical Imaging Relative Positioning, benchmark dataset, designed to systematically evaluate the capability to identify relative positions in medical images. </p>
<blockquote>
<p>ä¸´åºŠå†³ç­–åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå¯¹è§£å‰–ç»“æ„å’Œå¼‚å¸¸éƒ¨ä½ç›¸å¯¹ä½ç½®çš„ç†è§£ã€‚å› æ­¤ï¼Œä¸ºäº†ä½¿è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨ä¸´åºŠå®è·µä¸­å¾—åˆ°åº”ç”¨ï¼Œå‡†ç¡®ç¡®å®šåŒ»å­¦å›¾åƒä¸Šç›¸å¯¹ä½ç½®çš„èƒ½åŠ›æ˜¯åŸºæœ¬å‰æã€‚å°½ç®¡è¿™ä¸€èƒ½åŠ›éå¸¸é‡è¦ï¼Œä½†ç›®å‰å´å¾ˆå°‘è¢«æ·±å…¥ç ”ç©¶ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æœ€å…ˆè¿›çš„VLMsçš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬GPT-4oã€Llama3.2ã€Pixtralå’ŒJanusProï¼Œå‘ç°æ‰€æœ‰æ¨¡å‹åœ¨è¿™ä¸ªåŸºæœ¬ä»»åŠ¡ä¸Šéƒ½å¤±è´¥äº†ã€‚å€Ÿé‰´è®¡ç®—æœºè§†è§‰ä¸­çš„æˆåŠŸæ–¹æ³•ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†è§†è§‰æç¤ºï¼ˆå¦‚åœ¨è§£å‰–ç»“æ„ä¸Šæ”¾ç½®å­—æ¯æ•°å­—æˆ–å½©è‰²æ ‡è®°ï¼‰æ˜¯å¦èƒ½æé«˜æ€§èƒ½ã€‚è™½ç„¶è¿™äº›æ ‡è®°æä¾›äº†é€‚åº¦çš„æ”¹è¿›ï¼Œä½†åœ¨åŒ»å­¦å›¾åƒä¸Šçš„ç»“æœä»ç„¶æ˜¾è‘—ä½äºåœ¨è‡ªç„¶å›¾åƒä¸Šè§‚å¯Ÿåˆ°çš„ç»“æœã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œåœ¨åŒ»å­¦æˆåƒä¸­ï¼ŒVLMsæ›´ä¾èµ–äºå…ˆéªŒè§£å‰–çŸ¥è¯†è€Œä¸æ˜¯å®é™…çš„å›¾åƒå†…å®¹æ¥å›ç­”ç›¸å¯¹ä½ç½®é—®é¢˜ï¼Œè¿™å¸¸å¸¸å¯¼è‡´é”™è¯¯çš„ç»“è®ºã€‚ä¸ºäº†ä¿ƒè¿›è¯¥é¢†åŸŸçš„ç ”ç©¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒ»ç–—æˆåƒç›¸å¯¹å®šä½ï¼ˆMIRPï¼‰åŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°åœ¨åŒ»å­¦å›¾åƒä¸Šè¯†åˆ«ç›¸å¯¹ä½ç½®çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00549v2">PDF</a> Accepted at the International Conference on Medical Image Computing   and Computer Assisted Intervention (MICCAI) 2025</p>
<p><strong>Summary</strong></p>
<p>ä¸´åºŠå†³ç­–ä¾èµ–äºå¯¹è§£å‰–ç»“æ„å’Œå¼‚å¸¸æƒ…å†µçš„ç›¸å¯¹ä½ç½®çš„ç†è§£ã€‚ä¸ºäº†å°†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åº”ç”¨äºä¸´åºŠå®è·µï¼Œå‡†ç¡®ç¡®å®šåŒ»å­¦å›¾åƒä¸Šçš„ç›¸å¯¹ä½ç½®æ˜¯åŸºæœ¬å‰æã€‚å°½ç®¡å…¶é‡è¦æ€§å¾ˆé«˜ï¼Œä½†è¿™ç§èƒ½åŠ›ä»ç„¶è¢«å¤§å¤§å¿½è§†ã€‚è¯„ä¼°äº†æœ€å…ˆè¿›çš„VLMsçš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬GPT-4oã€Llama3.2ã€Pixtralå’ŒJanusProï¼Œå‘ç°å®ƒä»¬åœ¨è¿™ä¸ªåŸºæœ¬ä»»åŠ¡ä¸Šéƒ½å¤±è´¥äº†ã€‚å—è®¡ç®—æœºè§†è§‰æˆåŠŸæ–¹æ³•çš„å¯å‘ï¼Œç ”ç©¶æ˜¯å¦å¯ä»¥é€šè¿‡åœ¨è§£å‰–ç»“æ„ä¸Šæ”¾ç½®å­—æ¯æ•°å­—æˆ–å½©è‰²æ ‡è®°ç­‰è§†è§‰æç¤ºæ¥å¢å¼ºæ€§èƒ½ã€‚è™½ç„¶è¿™äº›æ ‡è®°æä¾›äº†é€‚åº¦çš„æ”¹è¿›ï¼Œä½†åœ¨åŒ»å­¦å›¾åƒä¸Šçš„ç»“æœä»ç„¶æ˜¾è‘—ä½äºåœ¨è‡ªç„¶å›¾åƒä¸Šçš„è§‚å¯Ÿç»“æœã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œåœ¨åŒ»å­¦æˆåƒä¸­ï¼ŒVLMsæ›´ä¾èµ–äºå…ˆéªŒè§£å‰–çŸ¥è¯†è€Œéå®é™…å›¾åƒå†…å®¹æ¥å›ç­”ç›¸å¯¹ä½ç½®é—®é¢˜ï¼Œè¿™å¾€å¾€å¯¼è‡´é”™è¯¯ç»“è®ºã€‚ä¸ºäº†ä¿ƒè¿›è¯¥é¢†åŸŸçš„ç ”ç©¶ï¼Œå¼•å…¥äº†MIRPï¼ˆåŒ»å­¦æˆåƒç›¸å¯¹å®šä½ï¼‰åŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°åœ¨åŒ»å­¦å›¾åƒä¸Šè¯†åˆ«ç›¸å¯¹ä½ç½®çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸´åºŠå†³ç­–éœ€è¦ç†è§£è§£å‰–ç»“æ„å’Œå¼‚å¸¸æƒ…å†µçš„ç›¸å¯¹ä½ç½®ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»å­¦å›¾åƒä¸Šçš„ç›¸å¯¹ä½ç½®ç¡®å®šèƒ½åŠ›æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>å…ˆè¿›çš„VLMsåœ¨åŸºæœ¬ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œå³å‡†ç¡®ç¡®å®šåŒ»å­¦å›¾åƒä¸Šçš„ç›¸å¯¹ä½ç½®ã€‚</li>
<li>è§†è§‰æç¤ºï¼ˆå¦‚å­—æ¯æ•°å­—æˆ–å½©è‰²æ ‡è®°ï¼‰å¯ä»¥æä¾›é€‚åº¦çš„æ”¹è¿›ï¼Œä½†æ€§èƒ½ä»ç„¶æœ‰é™ã€‚</li>
<li>VLMsåœ¨åŒ»å­¦å›¾åƒä¸Šçš„è¡¨ç°ä½äºåœ¨è‡ªç„¶å›¾åƒä¸Šçš„è¡¨ç°ã€‚</li>
<li>åœ¨åŒ»å­¦æˆåƒä¸­ï¼ŒVLMsæ›´ä¾èµ–äºå…ˆéªŒè§£å‰–çŸ¥è¯†è€Œéå›¾åƒå†…å®¹æ¥å›ç­”ç›¸å¯¹ä½ç½®é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00549">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae8801774fb96a98c250a4fc1e65cee9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0821276846275441f8e82e1862a0e846.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e04e22995f7fb2dee4fe7a436bfaced.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Vision-Language-Model-Based-Semantic-Guided-Imaging-Biomarker-for-Lung-Nodule-Malignancy-Prediction"><a href="#Vision-Language-Model-Based-Semantic-Guided-Imaging-Biomarker-for-Lung-Nodule-Malignancy-Prediction" class="headerlink" title="Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Lung   Nodule Malignancy Prediction"></a>Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Lung   Nodule Malignancy Prediction</h2><p><strong>Authors:Luoting Zhuang, Seyed Mohammad Hossein Tabatabaei, Ramin Salehi-Rad, Linh M. Tran, Denise R. Aberle, Ashley E. Prosper, William Hsu</strong></p>
<p>Machine learning models have utilized semantic features, deep features, or both to assess lung nodule malignancy. However, their reliance on manual annotation during inference, limited interpretability, and sensitivity to imaging variations hinder their application in real-world clinical settings. Thus, this research aims to integrate semantic features derived from radiologistsâ€™ assessments of nodules, guiding the model to learn clinically relevant, robust, and explainable imaging features for predicting lung cancer. We obtained 938 low-dose CT scans from the National Lung Screening Trial (NLST) with 1,246 nodules and semantic features. Additionally, the Lung Image Database Consortium dataset contains 1,018 CT scans, with 2,625 lesions annotated for nodule characteristics. Three external datasets were obtained from UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We fine-tuned a pretrained Contrastive Language-Image Pretraining (CLIP) model with a parameter-efficient fine-tuning approach to align imaging and semantic text features and predict the one-year lung cancer diagnosis. Our model outperformed state-of-the-art (SOTA) models in the NLST test set with an AUROC of 0.901 and AUPRC of 0.776. It also showed robust results in external datasets. Using CLIP, we also obtained predictions on semantic features through zero-shot inference, such as nodule margin (AUROC: 0.812), nodule consistency (0.812), and pleural attachment (0.840). Our approach surpasses the SOTA models in predicting lung cancer across datasets collected from diverse clinical settings, providing explainable outputs, aiding clinicians in comprehending the underlying meaning of model predictions. This approach also prevents the model from learning shortcuts and generalizes across clinical settings. The code is available at <a target="_blank" rel="noopener" href="https://github.com/luotingzhuang/CLIP_nodule">https://github.com/luotingzhuang/CLIP_nodule</a>. </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ æ¨¡å‹å·²ç»åˆ©ç”¨è¯­ä¹‰ç‰¹å¾ã€æ·±åº¦ç‰¹å¾æˆ–ä¸¤è€…æ¥è¯„ä¼°è‚ºç»“èŠ‚çš„æ¶æ€§ç¨‹åº¦ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯¹æ‰‹åŠ¨æ ‡æ³¨çš„ä¾èµ–ã€è§£é‡Šæ€§æœ‰é™ä»¥åŠå¯¹æˆåƒå˜åŒ–çš„æ•æ„Ÿæ€§ï¼Œé˜»ç¢äº†å®ƒä»¬åœ¨ç°å®ä¸´åºŠç¯å¢ƒä¸­çš„åº”ç”¨ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ•´åˆæ”¾å°„ç§‘åŒ»ç”Ÿå¯¹ç»“èŠ‚çš„è¯„ä¼°æ‰€è¡ç”Ÿçš„è¯­ä¹‰ç‰¹å¾ï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹ ä¸´åºŠç›¸å…³ã€ç¨³å¥å’Œå¯è§£é‡Šçš„æˆåƒç‰¹å¾ï¼Œä»¥é¢„æµ‹è‚ºç™Œã€‚æˆ‘ä»¬ä»å›½å®¶è‚ºç™Œç­›æŸ¥è¯•éªŒ(NLST)è·å¾—äº†938ä¾‹ä½å‰‚é‡CTæ‰«æï¼ŒåŒ…å«1246ä¸ªç»“èŠ‚å’Œè¯­ä¹‰ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè‚ºéƒ¨å›¾åƒæ•°æ®åº“è”ç›Ÿæ•°æ®é›†åŒ…å«1018ä¾‹CTæ‰«æï¼Œå…¶ä¸­2625ä¸ªç—…ç¶è¢«æ ‡æ³¨ä¸ºç»“èŠ‚ç‰¹å¾ã€‚å¦å¤–ä¸‰ä¸ªå¤–éƒ¨æ•°æ®é›†æ¥è‡ªåŠ å·å¤§å­¦æ´›æ‰çŸ¶åŒ»ç–—ä¸­å¿ƒã€LUNGxæŒ‘æˆ˜èµ›å’Œæœå…‹å¤§å­¦è‚ºç™Œç­›æŸ¥ã€‚æˆ‘ä»¬ä½¿ç”¨å¾®è°ƒè¿‡çš„å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒ(CLIP)æ¨¡å‹ï¼Œé‡‡ç”¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œå¯¹é½æˆåƒå’Œè¯­ä¹‰æ–‡æœ¬ç‰¹å¾ï¼Œé¢„æµ‹ä¸€å¹´å†…è‚ºç™Œçš„è¯Šæ–­ç»“æœã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨NLSTæµ‹è¯•é›†ä¸Šçš„è¡¨ç°è¶…è¿‡äº†æœ€å…ˆè¿›æ¨¡å‹çš„æ€§èƒ½ï¼Œæ›²çº¿ä¸‹é¢ç§¯(AUROC)ä¸º0.901ï¼Œç²¾ç¡®æ¥æ”¶è€…æ“ä½œç‰¹å¾æ›²çº¿ä¸‹é¢ç§¯(AUPRC)ä¸º0.776ã€‚åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šä¹Ÿè¡¨ç°å‡ºç¨³å¥çš„ç»“æœã€‚ä½¿ç”¨CLIPï¼Œæˆ‘ä»¬è¿˜é€šè¿‡é›¶æ ·æœ¬æ¨ç†è·å¾—äº†è¯­ä¹‰ç‰¹å¾çš„é¢„æµ‹ï¼Œå¦‚ç»“èŠ‚è¾¹ç¼˜(AUROC: 0.812)ã€ç»“èŠ‚ä¸€è‡´æ€§(0.812)å’Œèƒ¸è†œé™„ç€(0.840)ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨é¢„æµ‹æ¥è‡ªä¸åŒä¸´åºŠç¯å¢ƒçš„æ•°æ®é›†ä¸Šçš„è‚ºç™Œæ—¶ï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œæä¾›äº†å¯è§£é‡Šçš„è¾“å‡ºç»“æœï¼Œå¸®åŠ©ä¸´åºŠåŒ»ç”Ÿç†è§£æ¨¡å‹é¢„æµ‹ç»“æœçš„å†…åœ¨å«ä¹‰ã€‚è¿™ç§æ–¹æ³•è¿˜é˜²æ­¢äº†æ¨¡å‹å­¦ä¹ æ·å¾„ï¼Œå¹¶åœ¨ä¸åŒçš„ä¸´åºŠç¯å¢ƒä¸­è¿›è¡Œäº†æ¨å¹¿ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/luotingzhuang/CLIP_nodule%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/luotingzhuang/CLIP_noduleæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21344v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åˆ©ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•é¢„æµ‹è‚ºç™Œçš„é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿæ•´åˆäº†æ”¾å°„ç§‘åŒ»ç”Ÿå¯¹ç»“èŠ‚çš„è¯­ä¹‰ç‰¹å¾è¯„ä¼°ï¼ŒæŒ‡å¯¼æ¨¡å‹å­¦ä¹ ä¸´åºŠç›¸å…³ã€ç¨³å¥å’Œå¯è§£é‡Šçš„æˆåƒç‰¹å¾ã€‚ä»–ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„Contrastive Language-Image Pretraining (CLIP)æ¨¡å‹ï¼Œé€šè¿‡å‚æ•°æœ‰æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œå¯¹é½å›¾åƒå’Œè¯­ä¹‰æ–‡æœ¬ç‰¹å¾ï¼Œé¢„æµ‹ä¸€å¹´å†…è‚ºç™Œçš„è¯Šæ–­ã€‚è¯¥æ¨¡å‹åœ¨å¤šä¸ªå¤–éƒ¨æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶æä¾›å¯è§£é‡Šçš„è¾“å‡ºç»“æœï¼Œå¸®åŠ©ä¸´åºŠåŒ»ç”Ÿç†è§£æ¨¡å‹é¢„æµ‹çš„èƒŒåå«ä¹‰ã€‚è¯¥æ¨¡å‹èƒ½é˜²æ­¢å­¦ä¹ å¿«æ·æ–¹å¼å¹¶åœ¨ä¸åŒçš„ä¸´åºŠç¯å¢ƒä¸­è¿›è¡Œæ¨å¹¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ä½¿ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•é¢„æµ‹è‚ºç™Œï¼Œç»“åˆäº†æ”¾å°„ç§‘åŒ»ç”Ÿå¯¹ç»“èŠ‚çš„è¯­ä¹‰ç‰¹å¾è¯„ä¼°ã€‚</li>
<li>é‡‡ç”¨é¢„è®­ç»ƒçš„CLIPæ¨¡å‹ï¼Œé€šè¿‡å‚æ•°æœ‰æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œå¯¹é½å›¾åƒå’Œè¯­ä¹‰æ–‡æœ¬ç‰¹å¾ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ŒåŒ…æ‹¬æ¥è‡ªNLSTçš„è‚ºéƒ¨CTæ‰«ææ•°æ®å’Œå¤–éƒ¨æ•°æ®é›†ã€‚</li>
<li>æ¨¡å‹èƒ½è¿›è¡Œé›¶æ ·æœ¬æ¨æ–­ï¼Œé¢„æµ‹è¯­ä¹‰ç‰¹å¾å¦‚ç»“èŠ‚è¾¹ç¼˜ã€ä¸€è‡´æ€§å’Œèƒ¸è†œé™„ç€ç­‰ã€‚</li>
<li>æ¨¡å‹æä¾›å¯è§£é‡Šçš„è¾“å‡ºç»“æœï¼Œæœ‰åŠ©äºä¸´åºŠåŒ»ç”Ÿç†è§£é¢„æµ‹èƒŒåçš„å«ä¹‰ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿé˜²æ­¢å­¦ä¹ å¿«æ·æ–¹å¼ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½åœ¨ä¸åŒçš„ä¸´åºŠç¯å¢ƒä¸­åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21344">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cfd4b3444c04f0d1a17395833246369a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e58df1630e558dc54263d9c2a29fb9be.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Conditional-Diffusion-Models-are-Medical-Image-Classifiers-that-Provide-Explainability-and-Uncertainty-for-Free"><a href="#Conditional-Diffusion-Models-are-Medical-Image-Classifiers-that-Provide-Explainability-and-Uncertainty-for-Free" class="headerlink" title="Conditional Diffusion Models are Medical Image Classifiers that Provide   Explainability and Uncertainty for Free"></a>Conditional Diffusion Models are Medical Image Classifiers that Provide   Explainability and Uncertainty for Free</h2><p><strong>Authors:Gian Mario Favero, Parham Saremi, Emily Kaczmarek, Brennan Nichyporuk, Tal Arbel</strong></p>
<p>Discriminative classifiers have become a foundational tool in deep learning for medical imaging, excelling at learning separable features of complex data distributions. However, these models often need careful design, augmentation, and training techniques to ensure safe and reliable deployment. Recently, diffusion models have become synonymous with generative modeling in 2D. These models showcase robustness across a range of tasks including natural image classification, where classification is performed by comparing reconstruction errors across images generated for each possible conditioning input. This work presents the first exploration of the potential of class conditional diffusion models for 2D medical image classification. First, we develop a novel majority voting scheme shown to improve the performance of medical diffusion classifiers. Next, extensive experiments on the CheXpert and ISIC Melanoma skin cancer datasets demonstrate that foundation and trained-from-scratch diffusion models achieve competitive performance against SOTA discriminative classifiers without the need for explicit supervision. In addition, we show that diffusion classifiers are intrinsically explainable, and can be used to quantify the uncertainty of their predictions, increasing their trustworthiness and reliability in safety-critical, clinical contexts. Further information is available on our project page: <a target="_blank" rel="noopener" href="https://faverogian.github.io/med-diffusion-classifier.github.io/">https://faverogian.github.io/med-diffusion-classifier.github.io/</a>. </p>
<blockquote>
<p>åˆ¤åˆ«åˆ†ç±»å™¨å·²ç»æˆä¸ºæ·±åº¦åŒ»å­¦æˆåƒä¸­çš„åŸºæœ¬å·¥å…·ï¼Œæ“…é•¿å­¦ä¹ å¤æ‚æ•°æ®åˆ†å¸ƒçš„å¯åˆ†ç‰¹å¾ã€‚ç„¶è€Œï¼Œä¸ºäº†ç¡®ä¿å®‰å…¨å’Œå¯é çš„éƒ¨ç½²ï¼Œè¿™äº›æ¨¡å‹é€šå¸¸éœ€è¦ç²¾å¿ƒè®¾è®¡ã€å¢å¼ºå’Œè®­ç»ƒæŠ€æœ¯ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹å·²æˆä¸ºäºŒç»´ç”Ÿæˆæ¨¡å‹çš„ä»£åè¯ã€‚è¿™äº›æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­å±•ç¤ºäº†ç¨³å¥æ€§ï¼ŒåŒ…æ‹¬è‡ªç„¶å›¾åƒåˆ†ç±»ï¼Œåˆ†ç±»æ˜¯é€šè¿‡æ¯”è¾ƒé’ˆå¯¹æ¯ä¸ªå¯èƒ½çš„æ¡ä»¶è¾“å…¥ç”Ÿæˆçš„å›¾åƒä¹‹é—´çš„é‡å»ºè¯¯å·®æ¥å®Œæˆçš„ã€‚è¿™é¡¹å·¥ä½œé¦–æ¬¡æ¢ç´¢äº†äºŒç»´åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­ç±»æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ½œåŠ›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹å¤šæ•°æŠ•ç¥¨æ–¹æ¡ˆï¼Œä»¥æé«˜åŒ»å­¦æ‰©æ•£åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚å…¶æ¬¡ï¼Œåœ¨CheXpertå’ŒISICé»‘è‰²ç´ ç˜¤çš®è‚¤ç™Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒåŸºç¡€æ‰©æ•£æ¨¡å‹å’Œä»å¤´å¼€å§‹è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ— éœ€æ˜¾å¼ç›‘ç£å³å¯å®ç°ä¸æœ€æ–°åˆ¤åˆ«åˆ†ç±»å™¨ç›¸å½“çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†æ‰©æ•£åˆ†ç±»å™¨æœ¬è´¨ä¸Šæ˜¯å¯è§£é‡Šçš„ï¼Œå¯ç”¨äºé‡åŒ–å…¶é¢„æµ‹çš„çš„ä¸ç¡®å®šæ€§ï¼Œè¿™åœ¨å®‰å…¨å…³é”®çš„åŒ»ç–—ç¯å¢ƒä¸­å¢åŠ äº†å…¶å¯ä¿¡åº¦å’Œå¯é æ€§ã€‚æ›´å¤šä¿¡æ¯è¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://faverogian.github.io/med-diffusion-classifier">https://faverogian.github.io/med-diffusion-classifier</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03687v2">PDF</a> Accepted for publication at MIDL 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†ç±»æ¡ä»¶æ‰©æ•£æ¨¡å‹åœ¨äºŒç»´åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­çš„æ½œåŠ›ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ•°æŠ•ç¥¨æ–¹æ¡ˆï¼Œæé«˜äº†åŒ»å­¦æ‰©æ•£åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚åœ¨CheXpertå’ŒISICé»‘è‰²ç´ ç˜¤çš®è‚¤ç™Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒåŸºç¡€æ‰©æ•£æ¨¡å‹å’Œä»å¤´è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹åœ¨ç«äº‰æ€§èƒ½æ–¹é¢è¾¾åˆ°äº†å½“å‰æœ€ä½³é‰´åˆ«åˆ†ç±»å™¨çš„æ°´å¹³ï¼Œè€Œæ— éœ€æ˜¾å¼ç›‘ç£ã€‚æ­¤å¤–ï¼Œæ‰©æ•£åˆ†ç±»å™¨å…·æœ‰å†…åœ¨çš„å¯è§£é‡Šæ€§ï¼Œå¯ç”¨äºé‡åŒ–é¢„æµ‹çš„ç¡®å®šæ€§ï¼Œä»è€Œå¢åŠ å…¶åœ¨å®‰å…¨å…³é”®çš„åŒ»ç–—ç¯å¢ƒä¸­çš„å¯ä¿¡åº¦å’Œå¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨äºŒç»´åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
<li>æ–°çš„å¤šæ•°æŠ•ç¥¨æ–¹æ¡ˆæé«˜äº†åŒ»å­¦æ‰©æ•£åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨CheXpertå’ŒISICæ•°æ®é›†ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹æ— éœ€æ˜¾å¼ç›‘ç£å³å¯è¾¾åˆ°æœ€ä½³é‰´åˆ«åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚</li>
<li>æ‰©æ•£åˆ†ç±»å™¨å…·æœ‰å†…åœ¨çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>æ‰©æ•£åˆ†ç±»å™¨å¯ä»¥é‡åŒ–é¢„æµ‹çš„ç¡®å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-92cda6b36d5bde5a60a272e0b7a16553.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-087249e41ec4d81107ed81dc778583ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61aeffe72a0f6f1a71eb0e2bb2852ef0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MambaEviScrib-Mamba-and-Evidence-Guided-Consistency-Enhance-CNN-Robustness-for-Scribble-Based-Weakly-Supervised-Ultrasound-Image-Segmentation"><a href="#MambaEviScrib-Mamba-and-Evidence-Guided-Consistency-Enhance-CNN-Robustness-for-Scribble-Based-Weakly-Supervised-Ultrasound-Image-Segmentation" class="headerlink" title="MambaEviScrib: Mamba and Evidence-Guided Consistency Enhance CNN   Robustness for Scribble-Based Weakly Supervised Ultrasound Image Segmentation"></a>MambaEviScrib: Mamba and Evidence-Guided Consistency Enhance CNN   Robustness for Scribble-Based Weakly Supervised Ultrasound Image Segmentation</h2><p><strong>Authors:Xiaoxiang Han, Xinyu Li, Jiang Shang, Yiman Liu, Keyan Chen, Shugong Xu, Qiaohong Liu, Qi Zhang</strong></p>
<p>Segmenting anatomical structures and lesions from ultrasound images contributes to disease assessment. Weakly supervised learning (WSL) based on sparse annotation has achieved encouraging performance and demonstrated the potential to reduce annotation costs. This study attempts to introduce scribble-based WSL into ultrasound image segmentation tasks. However, ultrasound images often suffer from poor contrast and unclear edges, coupled with insufficient supervison signals for edges, posing challenges to edge prediction. Uncertainty modeling has been proven to facilitate models in dealing with these issues. Nevertheless, existing uncertainty estimation paradigms are not robust enough and often filter out predictions near decision boundaries, resulting in unstable edge predictions. Therefore, we propose leveraging predictions near decision boundaries effectively. Specifically, we introduce Dempster-Shafer Theory (DST) of evidence to design an Evidence-Guided Consistency strategy. This strategy utilizes high-evidence predictions, which are more likely to occur near high-density regions, to guide the optimization of low-evidence predictions that may appear near decision boundaries. Furthermore, the diverse sizes and locations of lesions in ultrasound images pose a challenge for CNNs with local receptive fields, as they struggle to model global information. Therefore, we introduce Visual Mamba based on structured state space sequence models, which achieves long-range dependency with linear computational complexity, and we construct a novel hybrid CNN-Mamba framework. During training, the collaboration between the CNN branch and the Mamba branch in the proposed framework draws inspiration from each other based on the EGC strategy. Experiments demonstrate the competitiveness of the proposed method. Dataset and code will be available on <a target="_blank" rel="noopener" href="https://github.com/GtLinyer/MambaEviScrib">https://github.com/GtLinyer/MambaEviScrib</a>. </p>
<blockquote>
<p>ä»è¶…å£°å›¾åƒä¸­åˆ†å‰²è§£å‰–ç»“æ„å’Œç—…å˜æœ‰åŠ©äºç–¾ç—…è¯„ä¼°ã€‚åŸºäºç¨€ç–æ ‡æ³¨çš„å¼±ç›‘ç£å­¦ä¹ ï¼ˆWSLï¼‰å·²ç»å–å¾—äº†ä»¤äººé¼“èˆçš„æ€§èƒ½ï¼Œå¹¶æ˜¾ç¤ºå‡ºé™ä½æ ‡æ³¨æˆæœ¬æ½œåŠ›ã€‚æœ¬ç ”ç©¶è¯•å›¾å°†åŸºäºæ¶‚é¸¦çš„WSLå¼•å…¥è¶…å£°å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¶…å£°å›¾åƒå¸¸å¸¸å­˜åœ¨å¯¹æ¯”åº¦å·®ã€è¾¹ç¼˜ä¸æ¸…ç­‰é—®é¢˜ï¼Œå†åŠ ä¸Šè¾¹ç¼˜ç›‘ç£ä¿¡å·ä¸è¶³ï¼Œç»™è¾¹ç¼˜é¢„æµ‹å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸ç¡®å®šæ€§å»ºæ¨¡å·²è¢«è¯æ˜æœ‰åŠ©äºæ¨¡å‹å¤„ç†è¿™äº›é—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä¸ç¡®å®šæ€§ä¼°è®¡èŒƒå¼è¿˜ä¸å¤Ÿç¨³å¥ï¼Œç»å¸¸è¿‡æ»¤æ‰å†³ç­–è¾¹ç•Œé™„è¿‘çš„é¢„æµ‹ï¼Œå¯¼è‡´è¾¹ç¼˜é¢„æµ‹ä¸ç¨³å®šã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºæœ‰æ•ˆåˆ©ç”¨å†³ç­–è¾¹ç•Œé™„è¿‘çš„é¢„æµ‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥Dempster-Shaferç†è®ºï¼ˆDSTï¼‰çš„è¯æ®æ¥è®¾è®¡ä¸€ç§è¯æ®å¼•å¯¼çš„ä¸€è‡´æ€§ç­–ç•¥ã€‚è¯¥ç­–ç•¥åˆ©ç”¨é«˜è¯æ®é¢„æµ‹ï¼ˆå®ƒä»¬æ›´å¯èƒ½å‡ºç°åœ¨é«˜å¯†åº¦åŒºåŸŸï¼‰ï¼Œæ¥æŒ‡å¯¼ä½è¯æ®é¢„æµ‹çš„ä¼˜åŒ–ï¼Œè¿™äº›ä½è¯æ®é¢„æµ‹å¯èƒ½å‡ºç°åœ¨å†³ç­–è¾¹ç•Œé™„è¿‘ã€‚æ­¤å¤–ï¼Œè¶…å£°å›¾åƒä¸­ç—…å˜çš„å¤§å°å’Œä½ç½®çš„å¤šæ ·æ€§å¯¹å…·æœ‰å±€éƒ¨æ„Ÿå—é‡çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æå‡ºäº†æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬å¾ˆéš¾å¯¹å…¨å±€ä¿¡æ¯è¿›è¡Œå»ºæ¨¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºç»“æ„åŒ–çŠ¶æ€ç©ºé—´åºåˆ—æ¨¡å‹çš„è§†è§‰Mambaï¼Œå®ç°äº†çº¿æ€§è®¡ç®—å¤æ‚åº¦çš„é•¿ç¨‹ä¾èµ–æ€§ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªæ–°å‹çš„æ··åˆCNN-Mambaæ¡†æ¶ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ‰€ææ¡†æ¶ä¸­çš„CNNåˆ†æ”¯å’ŒMambaåˆ†æ”¯ä¹‹é—´çš„åä½œæ˜¯åŸºäºEGCç­–ç•¥çš„ç›¸äº’å¯å‘ã€‚å®éªŒè¯æ˜äº†æ‰€ææ–¹æ³•çš„ç«äº‰åŠ›ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/GtLinyer/MambaEviScrib">https://github.com/GtLinyer/MambaEviScrib</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.19370v3">PDF</a> Accepted by Information Fusion</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ¶‚é¸¦çš„å¼±ç›‘ç£å­¦ä¹ åœ¨è¶…å£°å›¾åƒåˆ†å‰²ä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹è¶…å£°å›¾åƒå¯¹æ¯”åº¦å·®ã€è¾¹ç¼˜ä¸æ¸…åŠç›‘ç£ä¿¡å·ä¸è¶³ç­‰é—®é¢˜ï¼Œæå‡ºåˆ©ç”¨Dempster-Shaferè¯æ®ç†è®ºå¼•å¯¼çš„ä¸€è‡´æ€§ç­–ç•¥ï¼Œæœ‰æ•ˆå¤„ç†è¾¹ç¼˜é¢„æµ‹é—®é¢˜ã€‚åŒæ—¶ï¼Œå¼•å…¥åŸºäºç»“æ„åŒ–çŠ¶æ€ç©ºé—´åºåˆ—æ¨¡å‹çš„è§†è§‰Mambaï¼Œè§£å†³CNNå¯¹å…¨å±€ä¿¡æ¯å»ºæ¨¡çš„å±€é™æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¶…å£°å›¾åƒåˆ†å‰²å¯¹ç–¾ç—…è¯„ä¼°æœ‰è´¡çŒ®ï¼Œä½†å­˜åœ¨å¯¹æ¯”åº¦å·®ã€è¾¹ç¼˜ä¸æ¸…å’Œç¼ºä¹ç›‘ç£ä¿¡å·çš„æŒ‘æˆ˜ã€‚</li>
<li>æ¶‚é¸¦å¼å¼±ç›‘ç£å­¦ä¹ è¢«å¼•å…¥è¶…å£°å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ã€‚</li>
<li>ä¸ç¡®å®šæ€§å»ºæ¨¡æœ‰åŠ©äºæ¨¡å‹å¤„ç†è¾¹ç¼˜é¢„æµ‹é—®é¢˜ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸ç¨³å®šã€‚</li>
<li>æå‡ºåˆ©ç”¨Dempster-Shaferè¯æ®ç†è®ºå¼•å¯¼çš„ä¸€è‡´æ€§ç­–ç•¥ï¼Œæœ‰æ•ˆå¤„ç†è¾¹ç¼˜é¢„æµ‹é—®é¢˜ã€‚</li>
<li>è¶…å£°å›¾åƒä¸­ç—…å˜çš„å¤šæ ·æ€§å’Œä½ç½®æŒ‘æˆ˜äº†CNNçš„å±€éƒ¨æ¥æ”¶åœºã€‚</li>
<li>å¼•å…¥åŸºäºç»“æ„åŒ–çŠ¶æ€ç©ºé—´åºåˆ—æ¨¡å‹çš„è§†è§‰Mambaï¼Œå®ç°é•¿æœŸä¾èµ–å…³ç³»ã€‚</li>
<li>æ„å»ºæ··åˆCNN-Mambaæ¡†æ¶ï¼ŒCNNä¸Mambaåˆ†æ”¯ç›¸äº’åä½œä»¥æ”¹å–„æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.19370">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5d83573a1023d6f8257d510fb3e92565.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18c8b4468265770bb2595971ead1d041.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8df34c4ec9290bc0626f2789476027a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba02f21e9e9bd2fa938c672c2e5a0e08.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-12/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-12/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-12/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6292257a658eb6f3d8e94699632eaf6c.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-12  ScamAgents How AI Agents Can Simulate Human-Level Scam Calls
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-12/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ab8e73ed129615d7ae826c3714a0645f.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-12  UGD-IML A Unified Generative Diffusion-based Framework for Constrained   and Unconstrained Image Manipulation Localization
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32102k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
