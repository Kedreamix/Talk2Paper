<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-12  UGD-IML A Unified Generative Diffusion-based Framework for Constrained   and Unconstrained Image Manipulation Localization">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ab8e73ed129615d7ae826c3714a0645f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    48 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-12-æ›´æ–°"><a href="#2025-08-12-æ›´æ–°" class="headerlink" title="2025-08-12 æ›´æ–°"></a>2025-08-12 æ›´æ–°</h1><h2 id="UGD-IML-A-Unified-Generative-Diffusion-based-Framework-for-Constrained-and-Unconstrained-Image-Manipulation-Localization"><a href="#UGD-IML-A-Unified-Generative-Diffusion-based-Framework-for-Constrained-and-Unconstrained-Image-Manipulation-Localization" class="headerlink" title="UGD-IML: A Unified Generative Diffusion-based Framework for Constrained   and Unconstrained Image Manipulation Localization"></a>UGD-IML: A Unified Generative Diffusion-based Framework for Constrained   and Unconstrained Image Manipulation Localization</h2><p><strong>Authors:Yachun Mi, Xingyang He, Shixin Sun, Yu Li, Yanting Li, Zhixuan Li, Jian Jin, Chen Hui, Shaohui Liu</strong></p>
<p>In the digital age, advanced image editing tools pose a serious threat to the integrity of visual content, making image forgery detection and localization a key research focus. Most existing Image Manipulation Localization (IML) methods rely on discriminative learning and require large, high-quality annotated datasets. However, current datasets lack sufficient scale and diversity, limiting model performance in real-world scenarios. To overcome this, recent studies have explored Constrained IML (CIML), which generates pixel-level annotations through algorithmic supervision. However, existing CIML approaches often depend on complex multi-stage pipelines, making the annotation process inefficient. In this work, we propose a novel generative framework based on diffusion models, named UGD-IML, which for the first time unifies both IML and CIML tasks within a single framework. By learning the underlying data distribution, generative diffusion models inherently reduce the reliance on large-scale labeled datasets, allowing our approach to perform effectively even under limited data conditions. In addition, by leveraging a class embedding mechanism and a parameter-sharing design, our model seamlessly switches between IML and CIML modes without extra components or training overhead. Furthermore, the end-to-end design enables our model to avoid cumbersome steps in the data annotation process. Extensive experimental results on multiple datasets demonstrate that UGD-IML outperforms the SOTA methods by an average of 9.66 and 4.36 in terms of F1 metrics for IML and CIML tasks, respectively. Moreover, the proposed method also excels in uncertainty estimation, visualization and robustness. </p>
<blockquote>
<p>åœ¨æ•°å­—åŒ–æ—¶ä»£ï¼Œå…ˆè¿›çš„å›¾åƒç¼–è¾‘å·¥å…·å¯¹è§†è§‰å†…å®¹çš„å®Œæ•´æ€§æ„æˆäº†ä¸¥é‡å¨èƒï¼Œä½¿å¾—å›¾åƒç¯¡æ”¹æ£€æµ‹å’Œå®šä½æˆä¸ºå…³é”®çš„ç ”ç©¶ç„¦ç‚¹ã€‚å¤§å¤šæ•°ç°æœ‰çš„å›¾åƒæ“çºµå®šä½ï¼ˆIMLï¼‰æ–¹æ³•ä¾èµ–äºåˆ¤åˆ«å­¦ä¹ ï¼Œå¹¶éœ€è¦å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„æœ‰æ ‡æ³¨æ•°æ®é›†ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ•°æ®é›†ç¼ºä¹è¶³å¤Ÿçš„è§„æ¨¡å’Œå¤šæ ·æ€§ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„æ€§èƒ½ã€‚ä¸ºäº†å…‹æœè¿™ä¸€éš¾é¢˜ï¼Œè¿‘æœŸçš„ç ”ç©¶æ¢ç´¢äº†çº¦æŸIMLï¼ˆCIMLï¼‰ï¼Œå®ƒé€šè¿‡ç®—æ³•ç›‘ç£ç”Ÿæˆåƒç´ çº§æ ‡æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„CIMLæ–¹æ³•å¾€å¾€ä¾èµ–äºå¤æ‚çš„å¤šé˜¶æ®µæµæ°´çº¿ï¼Œä½¿å¾—æ ‡æ³¨è¿‡ç¨‹æ•ˆç‡ä½ä¸‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06101v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç»Ÿä¸€ç”Ÿæˆæ¡†æ¶UGD-IMLï¼Œå®ƒç»“åˆäº†å›¾åƒæ“çºµæœ¬åœ°åŒ–ï¼ˆIMLï¼‰å’Œçº¦æŸIMLï¼ˆCIMLï¼‰ä»»åŠ¡ã€‚è¯¥æ¡†æ¶é€šè¿‡å‡å°‘å¯¹æ•°æ®é›†çš„ä¾èµ–å¹¶æé«˜æ•°æ®åˆ©ç”¨æ•ˆç‡ï¼Œåœ¨æœ‰é™æ•°æ®æ¡ä»¶ä¸‹è¡¨ç°è‰¯å¥½ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç±»åµŒå…¥æœºåˆ¶å’Œå‚æ•°å…±äº«è®¾è®¡ï¼ŒUGD-IMLå¯åœ¨IMLå’ŒCIMLæ¨¡å¼ä¹‹é—´æ— ç¼åˆ‡æ¢ï¼Œç®€åŒ–äº†æ•°æ®æ ‡æ³¨è¿‡ç¨‹çš„ç¹çæ­¥éª¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUGD-IMLåœ¨IMLå’ŒCIMLä»»åŠ¡ä¸Šçš„F1æŒ‡æ ‡å¹³å‡ä¼˜äºç°æœ‰æ–¹æ³•9.66å’Œ4.36ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ä¸ç¡®å®šæ€§ä¼°è®¡ã€å¯è§†åŒ–å’Œé²æ£’æ€§æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…ˆè¿›çš„å›¾åƒç¼–è¾‘å·¥å…·å¯¹è§†è§‰å†…å®¹çš„å®Œæ•´æ€§æ„æˆä¸¥é‡å¨èƒï¼Œä¿ƒä½¿å›¾åƒä¼ªé€ æ£€æµ‹å’Œå®šä½æˆä¸ºç ”ç©¶é‡ç‚¹ã€‚</li>
<li>å½“å‰å›¾åƒæ“çºµæœ¬åœ°åŒ–ï¼ˆIMLï¼‰æ–¹æ³•ä¾èµ–åˆ¤åˆ«å­¦ä¹ ï¼Œéœ€è¦å¤§é‡é«˜è´¨é‡æ ‡æ³¨æ•°æ®é›†ï¼Œä½†ç°æœ‰æ•°æ®é›†è§„æ¨¡ä¸è¶³ã€å¤šæ ·æ€§æœ‰é™ï¼Œé™åˆ¶äº†åœ¨ç°å®åœºæ™¯ä¸­çš„è¡¨ç°ã€‚</li>
<li>ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„ç»Ÿä¸€ç”Ÿæˆæ¡†æ¶UGD-IMLï¼Œç»“åˆIMLå’Œçº¦æŸIMLï¼ˆCIMLï¼‰ä»»åŠ¡ã€‚</li>
<li>UGD-IMLé€šè¿‡å­¦ä¹ åº•å±‚æ•°æ®åˆ†å¸ƒï¼Œå‡å°‘äº†å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†çš„ä¾èµ–ï¼Œåœ¨æœ‰é™æ•°æ®æ¡ä»¶ä¸‹è¡¨ç°è‰¯å¥½ã€‚</li>
<li>UGD-IMLé€šè¿‡ç±»åµŒå…¥æœºåˆ¶å’Œå‚æ•°å…±äº«è®¾è®¡ï¼Œå®ç°IMLå’ŒCIMLæ¨¡å¼ä¹‹é—´çš„æ— ç¼åˆ‡æ¢ï¼Œç®€åŒ–äº†æ•°æ®æ ‡æ³¨è¿‡ç¨‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒUGD-IMLåœ¨IMLå’ŒCIMLä»»åŠ¡ä¸Šçš„F1æŒ‡æ ‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06101">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-64af4fb759e22ba00c9de860ae8c7996.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a4a6042733e4367ce70ab740a2ba839.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbdff9c043942a2848c3c6cfdd9ec3a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a38906009c948de78630cf3701d219f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bace691f81c31e1d429a5abe5eb6d30.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Towards-MR-Based-Trochleoplasty-Planning"><a href="#Towards-MR-Based-Trochleoplasty-Planning" class="headerlink" title="Towards MR-Based Trochleoplasty Planning"></a>Towards MR-Based Trochleoplasty Planning</h2><p><strong>Authors:Michael Wehrli, Alicia Durrer, Paul Friedrich, Sidaty El Hadramy, Edwin Li, Luana Brahaj, Carol C. Hasler, Philippe C. Cattin</strong></p>
<p>To treat Trochlear Dysplasia (TD), current approaches rely mainly on low-resolution clinical Magnetic Resonance (MR) scans and surgical intuition. The surgeries are planned based on surgeons experience, have limited adoption of minimally invasive techniques, and lead to inconsistent outcomes. We propose a pipeline that generates super-resolved, patient-specific 3D pseudo-healthy target morphologies from conventional clinical MR scans. First, we compute an isotropic super-resolved MR volume using an Implicit Neural Representation (INR). Next, we segment femur, tibia, patella, and fibula with a multi-label custom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to generate pseudo-healthy target morphologies of the trochlear region. In contrast to prior work producing pseudo-healthy low-resolution 3D MR images, our approach enables the generation of sub-millimeter resolved 3D shapes compatible for pre- and intraoperative use. These can serve as preoperative blueprints for reshaping the femoral groove while preserving the native patella articulation. Furthermore, and in contrast to other work, we do not require a CT for our pipeline - reducing the amount of radiation. We evaluated our approach on 25 TD patients and could show that our target morphologies significantly improve the sulcus angle (SA) and trochlear groove depth (TGD). The code and interactive visualization are available at <a target="_blank" rel="noopener" href="https://wehrlimi.github.io/sr-3d-planning/">https://wehrlimi.github.io/sr-3d-planning/</a>. </p>
<blockquote>
<p>åœ¨å¤„ç†Trochlear Dysplasiaï¼ˆTDï¼‰æ—¶ï¼Œå½“å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºä½åˆ†è¾¨ç‡çš„ä¸´åºŠç£å…±æŒ¯ï¼ˆMRï¼‰æ‰«æå’Œæ‰‹æœ¯ç›´è§‰ã€‚æ‰‹æœ¯è®¡åˆ’æ˜¯åŸºäºå¤–ç§‘åŒ»ç”Ÿçš„ç»éªŒåˆ¶å®šçš„ï¼Œè¾ƒå°‘é‡‡ç”¨å¾®åˆ›æŠ€æœ¯ï¼Œä¸”æ‰‹æœ¯ç»“æœä¸ä¸€ã€‚æˆ‘ä»¬æå‡ºä¸€ä¸ªæµç¨‹ï¼Œè¯¥æµç¨‹å¯ä»å¸¸è§„ä¸´åºŠMRæ‰«æç”Ÿæˆè¶…åˆ†è¾¨ç‡ã€æ‚£è€…ä¸“ç”¨çš„3Dä¼ªå¥åº·ç›®æ ‡å½¢æ€ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰è®¡ç®—å„å‘åŒæ€§è¶…åˆ†è¾¨ç‡MRä½“ç§¯ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šæ ‡ç­¾è‡ªå®šä¹‰è®­ç»ƒç½‘ç»œå¯¹è‚¡éª¨ã€èƒ«éª¨ã€é«Œéª¨å’Œè…“éª¨è¿›è¡Œåˆ†å‰²ã€‚æœ€åï¼Œæˆ‘ä»¬è®­ç»ƒå°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDMï¼‰ï¼Œä»¥ç”ŸæˆTrochlearåŒºåŸŸçš„ä¼ªå¥åº·ç›®æ ‡å½¢æ€ã€‚ä¸ä¹‹å‰äº§ç”Ÿä¼ªå¥åº·ä½åˆ†è¾¨ç‡3D MRå›¾åƒçš„å·¥ä½œç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå…¼å®¹æœ¯å‰å’Œæœ¯ä¸­çš„äºšæ¯«ç±³åˆ†è¾¨ç‡çš„3Då½¢æ€ã€‚è¿™å¯ä»¥ä½œä¸ºé‡å¡‘è‚¡éª¨æ²Ÿå¹¶ä¿æŒåŸæœ‰é«Œéª¨å…³èŠ‚çš„æœ¯å‰è“å›¾ã€‚æ­¤å¤–ï¼Œä¸å…¶ä»–å·¥ä½œç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æµç¨‹ä¸éœ€è¦CTæ‰«æï¼Œå‡å°‘äº†è¾å°„é‡ã€‚æˆ‘ä»¬åœ¨25åTDæ‚£è€…ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„ç›®æ ‡å½¢æ€èƒ½æ˜¾è‘—æ”¹å–„çªè§’ï¼ˆSAï¼‰å’Œçªæ²Ÿæ·±åº¦ï¼ˆTGDï¼‰ã€‚ä»£ç å’Œäº¤äº’å¼å¯è§†åŒ–å†…å®¹å¯åœ¨<a target="_blank" rel="noopener" href="https://wehrlimi.github.io/sr-3d-planning/">https://wehrlimi.github.io/sr-3d-planning/</a>ä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06076v1">PDF</a> Accepted at MICCAI COLAS Workshop 2025. Code:   <a target="_blank" rel="noopener" href="https://wehrlimi.github.io/sr-3d-planning/">https://wehrlimi.github.io/sr-3d-planning/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ²»ç–—Trochlear Dysplasiaï¼ˆTDï¼‰çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰ç”Ÿæˆè¶…åˆ†è¾¨ç‡çš„ä¸ªæ€§åŒ–ä¸‰ç»´ä¼ªå¥åº·ç›®æ ‡å½¢æ€ï¼Œé€šè¿‡å¤šæ ‡ç­¾å®šåˆ¶ç½‘ç»œåˆ†å‰²éª¨éª¼ç»“æ„ï¼Œå¹¶ä½¿ç”¨å°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDMï¼‰ç”Ÿæˆä¼ªå¥åº·ç›®æ ‡å½¢æ€ã€‚è¿™ç§æ–¹æ³•å¯ä»¥åœ¨æœ¯å‰å’Œæœ¯ä¸­ç”Ÿæˆäºšæ¯«ç±³çº§åˆ†è¾¨ç‡çš„ä¸‰ç»´å½¢çŠ¶ï¼Œä½œä¸ºé‡å¡‘è‚¡éª¨æ§½çš„æœ¯å‰è“å›¾ï¼ŒåŒæ—¶ä¿ç•™åŸé«Œéª¨å…³èŠ‚ã€‚ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦CTæ‰«æï¼Œå‡å°‘äº†è¾å°„é‡ã€‚åœ¨25åTDæ‚£è€…ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—æ”¹å–„æ²Ÿè§’ï¼ˆSAï¼‰å’Œæ§½æ·±ï¼ˆTGDï¼‰ã€‚ç›¸å…³ä»£ç å’Œäº¤äº’å¯è§†åŒ–åœ¨<a target="_blank" rel="noopener" href="https://wehrlimi.github.io/sr-3d-planning/%E5%8F%AF%E4%BE%9B%E8%8E%B7%E5%8F%96%E3%80%82">https://wehrlimi.github.io/sr-3d-planning/å¯ä¾›è·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ²»ç–—Trochlear Dysplasiaï¼ˆTDï¼‰ä¸»è¦ä¾èµ–äºä½åˆ†è¾¨ç‡çš„MRæ‰«æå’Œæ‰‹æœ¯ç›´è§‰ï¼Œæ‰‹æœ¯ç»“æœä¸ä¸€è‡´ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªç”Ÿæˆè¶…åˆ†è¾¨ç‡ã€ä¸ªæ€§åŒ–ä¸‰ç»´ä¼ªå¥åº·ç›®æ ‡å½¢æ€çš„ç®¡é“ï¼Œè¯¥ç®¡é“åŸºäºéšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰ã€‚</li>
<li>é€šè¿‡å¤šæ ‡ç­¾å®šåˆ¶ç½‘ç»œåˆ†å‰²éª¨éª¼ç»“æ„ï¼ŒåŒ…æ‹¬è‚¡éª¨ã€èƒ«éª¨ã€é«Œéª¨å’Œè…“éª¨ã€‚</li>
<li>ä½¿ç”¨å°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDMï¼‰ç”Ÿæˆä¼ªå¥åº·ç›®æ ‡å½¢æ€ï¼Œé€‚ç”¨äºæœ¯å‰å’Œæœ¯ä¸­çš„é«˜è§£æåº¦ä¸‰ç»´å½¢çŠ¶ç”Ÿæˆã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä½œä¸ºé‡å¡‘è‚¡éª¨æ§½çš„æœ¯å‰è“å›¾ï¼ŒåŒæ—¶ä¿ç•™åŸé«Œéª¨å…³èŠ‚ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç®¡é“ä¸éœ€è¦CTæ‰«æï¼Œå‡å°‘äº†è¾å°„é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06076">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69e3f1e035b2f539de5cab25b848a2b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14015175721d02ae2435a1d3c421ede5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75b3de6e3f9c1cb871b063ae94ce5aee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20f42d52504c59b5c4fdff71cf2d8d0b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Learning-3D-Texture-Aware-Representations-for-Parsing-Diverse-Human-Clothing-and-Body-Parts"><a href="#Learning-3D-Texture-Aware-Representations-for-Parsing-Diverse-Human-Clothing-and-Body-Parts" class="headerlink" title="Learning 3D Texture-Aware Representations for Parsing Diverse Human   Clothing and Body Parts"></a>Learning 3D Texture-Aware Representations for Parsing Diverse Human   Clothing and Body Parts</h2><p><strong>Authors:Kiran Chhatre, Christopher Peters, Srikrishna Karanam</strong></p>
<p>Existing methods for human parsing into body parts and clothing often use fixed mask categories with broad labels that obscure fine-grained clothing types. Recent open-vocabulary segmentation approaches leverage pretrained text-to-image (T2I) diffusion model features for strong zero-shot transfer, but typically group entire humans into a single person category, failing to distinguish diverse clothing or detailed body parts. To address this, we propose Spectrum, a unified network for part-level pixel parsing (body parts and clothing) and instance-level grouping. While diffusion-based open-vocabulary models generalize well across tasks, their internal representations are not specialized for detailed human parsing. We observe that, unlike diffusion models with broad representations, image-driven 3D texture generators maintain faithful correspondence to input images, enabling stronger representations for parsing diverse clothing and body parts. Spectrum introduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model â€“ obtained by fine-tuning a T2I model on 3D human texture maps â€“ for improved alignment with body parts and clothing. From an input image, we extract human-part internal features via the I2Tx diffusion model and generate semantically valid masks aligned to diverse clothing categories through prompt-guided grounding. Once trained, Spectrum produces semantic segmentation maps for every visible body part and clothing category, ignoring standalone garments or irrelevant objects, for any number of humans in the scene. We conduct extensive cross-dataset experiments â€“ separately assessing body parts, clothing parts, unseen clothing categories, and full-body masks â€“ and demonstrate that Spectrum consistently outperforms baseline methods in prompt-based segmentation. </p>
<blockquote>
<p>ç°æœ‰çš„äººç±»èº«ä½“éƒ¨ä½å’Œæœè£…è§£ææ–¹æ³•é€šå¸¸ä½¿ç”¨å…·æœ‰å¹¿æ³›æ ‡ç­¾çš„å›ºå®šæ©è†œç±»åˆ«ï¼Œè¿™æ©ç›–äº†ç²¾ç»†çš„æœè£…ç±»å‹ã€‚æœ€è¿‘çš„å¼€æ”¾è¯æ±‡åˆ†å‰²æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ç‰¹å¾è¿›è¡Œå¼ºå¤§çš„é›¶æ ·æœ¬è¿ç§»ï¼Œä½†é€šå¸¸å°†æ•´ä¸ªäººç±»å½’å…¥å•äººç±»åˆ«ï¼Œæ— æ³•åŒºåˆ†å„ç§æœè£…æˆ–è¯¦ç»†çš„èº«ä½“éƒ¨ä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Spectrumï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºéƒ¨åˆ†çº§åˆ«çš„åƒç´ è§£æï¼ˆèº«ä½“éƒ¨ä½å’Œæœè£…ï¼‰å’Œå®ä¾‹çº§åˆ«çš„åˆ†ç»„çš„ç»Ÿä¸€ç½‘ç»œã€‚è™½ç„¶åŸºäºæ‰©æ•£çš„å¼€æ”¾è¯æ±‡æ¨¡å‹åœ¨å„é¡¹ä»»åŠ¡ä¸­å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ï¼Œä½†å®ƒä»¬çš„å†…éƒ¨è¡¨ç¤ºå¹¶ä¸é€‚ç”¨äºè¯¦ç»†çš„äººç±»è§£æã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œä¸å…·æœ‰å¹¿æ³›è¡¨ç¤ºçš„æ‰©æ•£æ¨¡å‹ä¸åŒï¼Œå›¾åƒé©±åŠ¨çš„3Dçº¹ç†ç”Ÿæˆå™¨ä¿æŒå¯¹è¾“å…¥å›¾åƒçš„å¿ å®å¯¹åº”ï¼Œèƒ½å¤Ÿä¸ºè§£æå„ç§æœè£…å’Œèº«ä½“éƒ¨ä½æä¾›æ›´å¼ºå¤§çš„è¡¨ç¤ºã€‚Spectrumå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„Image-to-Textureï¼ˆI2Txï¼‰æ‰©æ•£æ¨¡å‹çš„é‡æ–°åˆ©ç”¨æ–¹æ³•â€”â€”é€šè¿‡åœ¨å¯¹3Däººç±»çº¹ç†å›¾ä¸Šè¿›è¡Œå¾®è°ƒå¾—åˆ°çš„T2Iæ¨¡å‹â€”â€”ä»¥æ”¹å–„ä¸èº«ä½“éƒ¨ä½å’Œæœè£…çš„å¯¹é½ã€‚æˆ‘ä»¬ä»è¾“å…¥å›¾åƒä¸­æå–äººä½“éƒ¨ä½å†…éƒ¨ç‰¹å¾ï¼Œé€šè¿‡I2Txæ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸å„ç§æœè£…ç±»åˆ«å¯¹é½çš„è¯­ä¹‰æœ‰æ•ˆæ©è†œï¼Œé€šè¿‡æç¤ºå¼•å¯¼å®šä½ã€‚ä¸€æ—¦è®­ç»ƒå®Œæˆï¼ŒSpectrumå°±å¯ä»¥ä¸ºåœºæ™¯ä¸­çš„æ¯ä¸ªäººç”Ÿæˆè¯­ä¹‰åˆ†å‰²å›¾ï¼Œæ˜¾ç¤ºæ¯ä¸ªå¯è§çš„èº«ä½“éƒ¨ä½å’Œæœè£…ç±»åˆ«ï¼ŒåŒæ—¶å¿½ç•¥ç‹¬ç«‹æœè£…æˆ–æ— å…³ç‰©ä½“ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒâ€”â€”åˆ†åˆ«è¯„ä¼°èº«ä½“éƒ¨ä½ã€æœè£…éƒ¨ä½ã€æœªè§è¿‡çš„æœè£…ç±»åˆ«å’Œå…¨èº«æ©è†œâ€”â€”å¹¶è¯æ˜Spectrumåœ¨æç¤ºé©±åŠ¨çš„åˆ†å‰²æ–¹é¢å§‹ç»ˆä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06032v1">PDF</a> 16 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSpectrumçš„ç»Ÿä¸€ç½‘ç»œï¼Œç”¨äºäººä½“éƒ¨ä½å’Œæœè£…çš„åƒç´ çº§è§£æä»¥åŠå®ä¾‹çº§åˆ«çš„åˆ†ç»„ã€‚æ–‡ç« æŒ‡å‡ºï¼Œç°æœ‰çš„æ–¹æ³•åœ¨ä½¿ç”¨å›ºå®šçš„æ©è†œç±»åˆ«æ—¶ï¼Œé€šå¸¸ä½¿ç”¨å®½æ³›çš„æ ‡ç­¾ï¼Œå¿½ç•¥äº†ç²¾ç»†çš„æœè£…ç±»å‹ã€‚è™½ç„¶æœ€è¿‘çš„å¼€æ”¾è¯æ±‡åˆ†å‰²æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ç‰¹æ€§è¿›è¡Œé›¶æ ·æœ¬è¿ç§»ï¼Œä½†å®ƒä»¬é€šå¸¸å°†æ•´ä¸ªäººç±»å½’ä¸ºä¸€ä¸ªå•ä¸€çš„äººç±»åˆ«ï¼Œæ— æ³•åŒºåˆ†å¤šæ ·åŒ–çš„æœè£…æˆ–è¯¦ç»†çš„èº«ä½“éƒ¨ä½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼ŒSpectrumç»“åˆäº†åŸºäºå›¾åƒçš„3Dçº¹ç†ç”Ÿæˆå™¨çš„ä¼˜ç‚¹ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„å›¾åƒåˆ°çº¹ç†ï¼ˆI2Txï¼‰æ‰©æ•£æ¨¡å‹çš„ç”¨é€”ï¼Œè¯¥æ¨¡å‹é€šè¿‡å¾®è°ƒT2Iæ¨¡å‹è·å¾—ï¼Œä»¥æ”¹å–„ä¸èº«ä½“éƒ¨ä½å’Œæœè£…çš„å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼ŒSpectrumåœ¨åŸºäºæç¤ºçš„åˆ†å‰²ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½ç”Ÿæˆä¸å„ç§æœè£…ç±»åˆ«å¯¹é½çš„è¯­ä¹‰åˆ†å‰²å›¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>1.ç°æœ‰æ–¹æ³•åœ¨äººä½“è§£ææ–¹é¢å­˜åœ¨å±€é™ï¼Œå¦‚ä½¿ç”¨å›ºå®šæ©è†œç±»åˆ«å’Œå®½æ³›æ ‡ç­¾å¯¼è‡´çš„ç²¾ç»†æœè£…ç±»å‹è¯†åˆ«ä¸è¶³ã€‚<br>2.æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨é›¶æ ·æœ¬è¿ç§»æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†éš¾ä»¥åŒºåˆ†å¤šæ ·åŒ–çš„æœè£…å’Œè¯¦ç»†çš„èº«ä½“éƒ¨ä½ã€‚<br>3.Spectrumç½‘ç»œç»“åˆäº†T2Iæ‰©æ•£æ¨¡å‹å’Œå›¾åƒåˆ°çº¹ç†ï¼ˆI2Txï¼‰æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œå®ç°äº†äººä½“éƒ¨ä½å’Œæœè£…çš„åƒç´ çº§è§£æä»¥åŠå®ä¾‹çº§åˆ«çš„åˆ†ç»„ã€‚<br>4.I2Txæ‰©æ•£æ¨¡å‹é€šè¿‡å¾®è°ƒè·å¾—ï¼Œèƒ½æ”¹å–„ä¸èº«ä½“éƒ¨ä½å’Œæœè£…çš„å¯¹é½ï¼Œæé«˜è¯­ä¹‰åˆ†å‰²å›¾çš„ç”Ÿæˆè´¨é‡ã€‚<br>5.Spectrumé€šè¿‡æå–è¾“å…¥å›¾åƒçš„äººä½“éƒ¨ä½å†…éƒ¨ç‰¹å¾ï¼Œç”Ÿæˆä¸å„ç§æœè£…ç±»åˆ«å¯¹é½çš„è¯­ä¹‰åˆ†å‰²å›¾ã€‚<br>6.Spectrumåœ¨è·¨æ•°æ®é›†å®éªŒä¸­çš„è¡¨ç°ä¼˜å¼‚ï¼Œæ— è®ºæ˜¯è¯„ä¼°èº«ä½“éƒ¨ä½ã€æœè£…éƒ¨åˆ†ã€æœªè§è¿‡çš„æœè£…ç±»åˆ«è¿˜æ˜¯å…¨èº«æ©è†œã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-07633d033bb3416b4aadce00c3417c91.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c3d5e64edf44569b16ef4a00b4f9a2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-671a9e7673e05b70968edc262643e7a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9098a28361c5ecc71cf7cccb05d1a2df.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5db9770872c6be2c6512710da2a8c7e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de9782c3f3f4a12fcd354c9f08a18a9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab8e73ed129615d7ae826c3714a0645f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ee4ea3f6e337faee4d27f999150d694.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Improved-Sub-Visible-Particle-Classification-in-Flow-Imaging-Microscopy-via-Generative-AI-Based-Image-Synthesis"><a href="#Improved-Sub-Visible-Particle-Classification-in-Flow-Imaging-Microscopy-via-Generative-AI-Based-Image-Synthesis" class="headerlink" title="Improved Sub-Visible Particle Classification in Flow Imaging Microscopy   via Generative AI-Based Image Synthesis"></a>Improved Sub-Visible Particle Classification in Flow Imaging Microscopy   via Generative AI-Based Image Synthesis</h2><p><strong>Authors:Utku Ozbulak, Michaela Cohrs, Hristo L. Svilenov, Joris Vankerschaver, Wesley De Neve</strong></p>
<p>Sub-visible particle analysis using flow imaging microscopy combined with deep learning has proven effective in identifying particle types, enabling the distinction of harmless components such as silicone oil from protein particles. However, the scarcity of available data and severe imbalance between particle types within datasets remain substantial hurdles when applying multi-class classifiers to such problems, often forcing researchers to rely on less effective methods. The aforementioned issue is particularly challenging for particle types that appear unintentionally and in lower numbers, such as silicone oil and air bubbles, as opposed to protein particles, where obtaining large numbers of images through controlled settings is comparatively straightforward. In this work, we develop a state-of-the-art diffusion model to address data imbalance by generating high-fidelity images that can augment training datasets, enabling the effective training of multi-class deep neural networks. We validate this approach by demonstrating that the generated samples closely resemble real particle images in terms of visual quality and structure. To assess the effectiveness of using diffusion-generated images in training datasets, we conduct large-scale experiments on a validation dataset comprising 500,000 protein particle images and demonstrate that this approach improves classification performance with no negligible downside. Finally, to promote open research and reproducibility, we publicly release both our diffusion models and the trained multi-class deep neural network classifiers, along with a straightforward interface for easy integration into future studies, at <a target="_blank" rel="noopener" href="https://github.com/utkuozbulak/svp-generative-ai">https://github.com/utkuozbulak/svp-generative-ai</a>. </p>
<blockquote>
<p>ä½¿ç”¨æµå¼æˆåƒæ˜¾å¾®é•œç»“åˆæ·±åº¦å­¦ä¹ è¿›è¡Œäºšå¯è§ç²’å­åˆ†æï¼Œå·²è¯æ˜åœ¨è¯†åˆ«ç²’å­ç±»å‹æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œèƒ½å¤ŸåŒºåˆ†ç¡…èƒ¶æ²¹ä¸è›‹ç™½è´¨ç²’å­ç­‰æ— å®³æˆåˆ†ã€‚ç„¶è€Œï¼Œåœ¨å°†å¤šç±»åˆ†ç±»å™¨åº”ç”¨äºæ­¤ç±»é—®é¢˜æ—¶ï¼Œå¯ç”¨æ•°æ®çš„ç¨€ç¼ºæ€§å’Œæ•°æ®é›†å†…ç²’å­ç±»å‹çš„ä¸¥é‡ä¸å¹³è¡¡ä»ç„¶æ˜¯å·¨å¤§çš„éšœç¢ï¼Œè¿™å¸¸å¸¸è¿«ä½¿ç ”ç©¶äººå‘˜ä¸å¾—ä¸ä¾èµ–æ•ˆæœè¾ƒå·®çš„æ–¹æ³•ã€‚å¯¹äºç¡…èƒ¶æ²¹å’Œæ°”æ³¡ç­‰éæœ‰æ„å‡ºç°ä¸”æ•°é‡è¾ƒå°‘çš„ç²’å­ç±»å‹ï¼Œè¿™ä¸€é—®é¢˜æ›´å…·æŒ‘æˆ˜æ€§ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼Œé€šè¿‡å—æ§ç¯å¢ƒè·å–å¤§é‡è›‹ç™½è´¨ç²’å­å›¾åƒåˆ™ç›¸å¯¹å®¹æ˜“ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ç”Ÿæˆé«˜ä¿çœŸå›¾åƒæ¥è§£å†³æ•°æ®ä¸å¹³è¡¡é—®é¢˜ï¼Œè¿™äº›å›¾åƒå¯ä»¥æ‰©å……è®­ç»ƒæ•°æ®é›†ï¼Œä½¿å¾—å¤šç±»æ·±åº¦ç¥ç»ç½‘ç»œçš„è®­ç»ƒæ›´åŠ æœ‰æ•ˆã€‚æˆ‘ä»¬é€šè¿‡å±•ç¤ºç”Ÿæˆçš„æ ·æœ¬åœ¨è§†è§‰è´¨é‡å’Œç»“æ„ä¸Šä¸çœŸå®ç²’å­å›¾åƒéå¸¸ç›¸ä¼¼æ¥éªŒè¯è¿™ç§æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è¯„ä¼°åœ¨è®­ç»ƒæ•°æ®é›†ä¸­ä½¿ç”¨æ‰©æ•£ç”Ÿæˆå›¾åƒçš„æ•ˆæœï¼Œæˆ‘ä»¬åœ¨åŒ…å«50ä¸‡å¼ è›‹ç™½è´¨ç²’å­å›¾åƒçš„éªŒè¯æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§è§„æ¨¡å®éªŒï¼Œå¹¶è¯æ˜è¯¥æ–¹æ³•åœ¨åˆ†ç±»æ€§èƒ½ä¸Šæœ‰æ‰€æé«˜ï¼Œä¸”æ²¡æœ‰æ˜æ˜¾çš„ä¸åˆ©å½±å“ã€‚æœ€åï¼Œä¸ºäº†ä¿ƒè¿›å¼€æ”¾ç ”ç©¶å’Œå¯é‡å¤æ€§ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/utkuozbulak/svp-generative-ai">https://github.com/utkuozbulak/svp-generative-ai</a>ä¸Šå…¬å¼€äº†æˆ‘ä»¬çš„æ‰©æ•£æ¨¡å‹å’Œè®­ç»ƒå¥½çš„å¤šç±»æ·±åº¦ç¥ç»ç½‘ç»œåˆ†ç±»å™¨ï¼Œä»¥åŠä¸€ä¸ªæ˜“äºé›†æˆåˆ°æœªæ¥ç ”ç©¶ä¸­çš„ç®€å•æ¥å£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06021v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åˆ©ç”¨æµæˆåƒæ˜¾å¾®é•œç»“åˆæ·±åº¦å­¦ä¹ è¿›è¡Œäºšå¯è§ç²’å­åˆ†æå·²è¢«è¯æ˜åœ¨è¯†åˆ«ç²’å­ç±»å‹æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œèƒ½å¤ŸåŒºåˆ†å¦‚ç¡…æ²¹ä¸è›‹ç™½è´¨é¢—ç²’ç­‰æ— å®³æˆåˆ†ã€‚ç„¶è€Œï¼Œæ•°æ®é›†å¯ç”¨æ•°æ®çš„ç¨€ç¼ºæ€§å’Œç²’å­ç±»å‹ä¹‹é—´çš„ä¸¥é‡ä¸å¹³è¡¡ä»æ˜¯åº”ç”¨å¤šç±»åˆ†ç±»å™¨äºæ­¤ç±»é—®é¢˜çš„é‡å¤§éšœç¢ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å¼€å‘äº†ä¸€ç§å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹æ¥è§£å†³æ•°æ®ä¸å¹³è¡¡é—®é¢˜ï¼Œé€šè¿‡ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ¥æ‰©å……è®­ç»ƒæ•°æ®é›†ï¼Œä¸ºè®­ç»ƒå¤šç±»æ·±åº¦ç¥ç»ç½‘ç»œæä¾›äº†æœ‰æ•ˆæ–¹æ³•ã€‚æœ¬æ–‡éªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨åŒ…å«50ä¸‡è›‹ç™½è´¨é¢—ç²’å›¾åƒçš„å¤§è§„æ¨¡éªŒè¯æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚ç ”ç©¶å…¬å¼€å‘å¸ƒäº†æ‰©æ•£æ¨¡å‹å’Œè®­ç»ƒå¥½çš„å¤šç±»æ·±åº¦ç¥ç»ç½‘ç»œåˆ†ç±»å™¨ï¼Œä¾¿äºæœªæ¥ç ”ç©¶çš„é›†æˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµæˆåƒæ˜¾å¾®é•œç»“åˆæ·±åº¦å­¦ä¹ åœ¨äºšå¯è§ç²’å­åˆ†æä¸­å…·æœ‰é«˜è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>æ•°æ®ç¨€ç¼ºå’Œç²’å­ç±»å‹ä¸å¹³è¡¡æ˜¯åº”ç”¨å¤šç±»åˆ†ç±»å™¨çš„é‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„é«˜è´¨é‡å›¾åƒå¯ä»¥è§£å†³æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>ç”Ÿæˆçš„æ ·æœ¬åœ¨è§†è§‰è´¨é‡å’Œç»“æ„ä¸Šä¸çœŸå®ç²’å­å›¾åƒç›¸ä¼¼ã€‚</li>
<li>åœ¨å¤§è§„æ¨¡éªŒè¯æ•°æ®é›†ä¸ŠéªŒè¯äº†æ‰©æ•£æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œæé«˜äº†åˆ†ç±»æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶å…¬å¼€çš„æ¨¡å‹å’Œåˆ†ç±»å™¨ä¾¿äºæœªæ¥ç ”ç©¶çš„é›†æˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06021">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72d578e8109274065f422737f2ff558d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a32eade1a6923548fcca363531258fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54752fdfda1ebf1c9357ae378fcc1606.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b03bc057841b107c9163fc406f735a3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="KnapFormer-An-Online-Load-Balancer-for-Efficient-Diffusion-Transformers-Training"><a href="#KnapFormer-An-Online-Load-Balancer-for-Efficient-Diffusion-Transformers-Training" class="headerlink" title="KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers   Training"></a>KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers   Training</h2><p><strong>Authors:Kai Zhang, Peng Wang, Sai Bi, Jianming Zhang, Yuanjun Xiong</strong></p>
<p>We present KnapFormer, an efficient and versatile framework to combine workload balancing and sequence parallelism in distributed training of Diffusion Transformers (DiT). KnapFormer builds on the insight that strong synergy exists between sequence parallelism and the need to address the significant token imbalance across ranks. This imbalance arises from variable-length text inputs and varying visual token counts in mixed-resolution and image-video joint training. KnapFormer redistributes tokens by first gathering sequence length metadata across all ranks in a balancing group and solving a global knapsack problem. The solver aims to minimize the variances of total workload per-GPU, while accounting for the effect of sequence parallelism. By integrating DeepSpeed-Ulysees-based sequence parallelism in the load-balancing decision process and utilizing a simple semi-empirical workload model, KnapFormers achieves minimal communication overhead and less than 1% workload discrepancy in real-world training workloads with sequence length varying from a few hundred to tens of thousands. It eliminates straggler effects and achieves 2x to 3x speedup when training state-of-the-art diffusion models like FLUX on mixed-resolution and image-video joint data corpora. We open-source the KnapFormer implementation at <a target="_blank" rel="noopener" href="https://github.com/Kai-46/KnapFormer/">https://github.com/Kai-46/KnapFormer/</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†KnapFormerï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆä¸”é€šç”¨çš„æ¡†æ¶ï¼Œæ—¨åœ¨ç»“åˆå·¥ä½œé‡å¹³è¡¡å’Œåºåˆ—å¹¶è¡Œæ€§ï¼Œç”¨äºåˆ†å¸ƒå¼è®­ç»ƒæ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰ã€‚KnapFormerå»ºç«‹åœ¨åºåˆ—å¹¶è¡Œæ€§ä¸è§£å†³æ’åé—´æ˜¾è‘—ä»¤ç‰Œä¸å¹³è¡¡ä¹‹é—´å¼ºçƒˆååŒä½œç”¨çš„æ´å¯Ÿä¹‹ä¸Šã€‚è¿™ç§ä¸å¹³è¡¡äº§ç”Ÿäºæ··åˆåˆ†è¾¨ç‡å’Œå›¾åƒè§†é¢‘è”åˆè®­ç»ƒä¸­å¯å˜é•¿åº¦çš„æ–‡æœ¬è¾“å…¥å’Œè§†è§‰ä»¤ç‰Œè®¡æ•°ã€‚KnapFormeré€šè¿‡é‡æ–°åˆ†é…ä»¤ç‰Œæ¥åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œé¦–å…ˆæ”¶é›†å¹³è¡¡ç»„ä¸­æ‰€æœ‰æ’åçš„åºåˆ—é•¿åº¦å…ƒæ•°æ®ï¼Œå¹¶è§£å†³å…¨å±€èƒŒåŒ…é—®é¢˜ã€‚æ±‚è§£å™¨çš„ç›®æ ‡æ˜¯ä½¿æ¯GPUçš„æ€»å·¥ä½œé‡æ–¹å·®æœ€å°åŒ–ï¼ŒåŒæ—¶è€ƒè™‘åºåˆ—å¹¶è¡Œæ€§çš„å½±å“ã€‚é€šè¿‡å°†åŸºäºDeepSpeed-Ulyssesçš„åºåˆ—å¹¶è¡Œæ€§é›†æˆåˆ°è´Ÿè½½å‡è¡¡å†³ç­–è¿‡ç¨‹ä¸­ï¼Œå¹¶åˆ©ç”¨ç®€å•çš„åŠç»éªŒå·¥ä½œé‡æ¨¡å‹ï¼ŒKnapFormeråœ¨å®é™…è®­ç»ƒå·¥ä½œä¸­å®ç°äº†æœ€å°çš„é€šä¿¡å¼€é”€å’Œä¸åˆ°1%çš„å·¥ä½œé‡å·®å¼‚ï¼Œåºåˆ—é•¿åº¦ä»å‡ ç™¾åˆ°æ•°ä¸‡ä¸ç­‰ã€‚å®ƒæ¶ˆé™¤äº†æ»åæ•ˆåº”ï¼Œå¹¶åœ¨æ··åˆåˆ†è¾¨ç‡å’Œå›¾åƒè§†é¢‘è”åˆæ•°æ®é›†ä¸Šè®­ç»ƒæœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚FLUXï¼‰æ—¶å®ç°äº†2å€è‡³3å€çš„åŠ é€Ÿæ•ˆæœã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/Kai-46/KnapFormer/%E5%BC%80%E6%BA%90%E4%BA%86KnapFormer%E5%AE%9E%E7%8E%B0%E3%80%82">https://github.com/Kai-46/KnapFormer/å¼€æºäº†KnapFormerå®ç°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06001v1">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/Kai-46/KnapFormer/">https://github.com/Kai-46/KnapFormer/</a></p>
<p><strong>Summary</strong></p>
<p>KnapFormeræ˜¯ä¸€ä¸ªé’ˆå¯¹Diffusion Transformersï¼ˆDiTï¼‰è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒçš„é«˜æ•ˆä¸”é€šç”¨çš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡è§£å†³å…¨å±€èƒŒåŒ…é—®é¢˜å®ç°äº†å·¥ä½œé‡å¹³è¡¡å’Œåºåˆ—å¹¶è¡Œæ€§çš„ç»“åˆã€‚è¯¥æ¡†æ¶è§£å†³äº†ç”±ä¸åŒé•¿åº¦æ–‡æœ¬è¾“å…¥å’Œæ··åˆåˆ†è¾¨ç‡å›¾åƒè§†é¢‘è”åˆè®­ç»ƒä¸­çš„ä¸åŒè§†è§‰ä»¤ç‰Œè®¡æ•°å¯¼è‡´çš„ä»¤ç‰Œä¸å¹³è¡¡é—®é¢˜ã€‚é€šè¿‡é›†æˆDeepSpeed-Ulyseesçš„åºåˆ—å¹¶è¡Œæ€§å’Œè´Ÿè½½å‡è¡¡å†³ç­–è¿‡ç¨‹ï¼Œå¹¶ä½¿ç”¨ç®€å•çš„å·¥ä½œè´Ÿè½½æ¨¡å‹ï¼ŒKnapFormerå®ç°äº†æœ€å°çš„é€šä¿¡å¼€é”€å’ŒçœŸå®è®­ç»ƒåœºæ™¯ä¸­åºåˆ—é•¿åº¦ä»å‡ ç™¾åˆ°æ•°ä¸‡å˜åŒ–æ—¶çš„å·¥ä½œè´Ÿè½½å·®å¼‚å°äº1%ã€‚å®ƒæ¶ˆé™¤äº†æ»åæ•ˆåº”ï¼Œå¹¶åœ¨æ··åˆåˆ†è¾¨ç‡å’Œå›¾åƒè§†é¢‘è”åˆæ•°æ®é›†ä¸Šè®­ç»ƒå…ˆè¿›çš„æ‰©æ•£æ¨¡å‹æ—¶å®ç°äº†2å€è‡³3å€çš„é€Ÿåº¦æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>KnapFormeræ˜¯ä¸€ä¸ªé’ˆå¯¹Diffusion Transformersçš„åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨ç»“åˆå·¥ä½œé‡å¹³è¡¡å’Œåºåˆ—å¹¶è¡Œæ€§ã€‚</li>
<li>è¯¥æ¡†æ¶è§£å†³äº†ç”±ä¸åŒæ–‡æœ¬é•¿åº¦å’Œæ··åˆåˆ†è¾¨ç‡å›¾åƒè§†é¢‘è”åˆè®­ç»ƒä¸­çš„è§†è§‰ä»¤ç‰Œè®¡æ•°ä¸åŒå¯¼è‡´çš„ä»¤ç‰Œä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>é€šè¿‡é›†æˆDeepSpeed-Ulyseesçš„åºåˆ—å¹¶è¡Œæ€§ï¼ŒKnapFormerå®ç°äº†è´Ÿè½½å‡è¡¡å†³ç­–è¿‡ç¨‹ã€‚</li>
<li>KnapFormerä½¿ç”¨ç®€å•çš„å·¥ä½œè´Ÿè½½æ¨¡å‹ï¼Œå®ç°äº†æœ€å°çš„é€šä¿¡å¼€é”€å’Œå·¥ä½œè´Ÿè½½å·®å¼‚å°äº1%ã€‚</li>
<li>KnapFormerèƒ½å¤Ÿæ¶ˆé™¤æ»åæ•ˆåº”ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>åœ¨çœŸå®åœºæ™¯ä¸­ï¼ŒKnapFormerå®ç°äº†åœ¨æ··åˆåˆ†è¾¨ç‡å’Œå›¾åƒè§†é¢‘è”åˆæ•°æ®é›†ä¸Šè®­ç»ƒå…ˆè¿›çš„æ‰©æ•£æ¨¡å‹æ—¶çš„é«˜é€Ÿæ€§èƒ½æå‡ï¼Œè¾¾åˆ°2å€è‡³3å€çš„é€Ÿåº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06001">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0e94f96c898350a82db5dea707760deb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62a9a228cdff3b58f2827d08906bded1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-492bae9da46b11781019b71f4006431b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4feebec0f513c8907ff98efc07c87889.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="A-3DGS-Diffusion-Self-Supervised-Framework-for-Normal-Estimation-from-a-Single-Image"><a href="#A-3DGS-Diffusion-Self-Supervised-Framework-for-Normal-Estimation-from-a-Single-Image" class="headerlink" title="A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a   Single Image"></a>A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a   Single Image</h2><p><strong>Authors:Yanxing Liang, Yinghui Wang, Jinlong Yang, Wei Li</strong></p>
<p>The lack of spatial dimensional information remains a challenge in normal estimation from a single image. Recent diffusion-based methods have demonstrated significant potential in 2D-to-3D implicit mapping, they rely on data-driven statistical priors and miss the explicit modeling of light-surface interaction, leading to multi-view normal direction conflicts. Moreover, the discrete sampling mechanism of diffusion models causes gradient discontinuity in differentiable rendering reconstruction modules, preventing 3D geometric errors from being backpropagated to the normal generation network, thereby forcing existing methods to depend on dense normal annotations. This paper proposes SINGAD, a novel Self-supervised framework from a single Image for Normal estimation via 3D GAussian splatting guided Diffusion. By integrating physics-driven light-interaction modeling and a differentiable rendering-based reprojection strategy, our framework directly converts 3D geometric errors into normal optimization signals, solving the challenges of multi-view geometric inconsistency and data dependency. Specifically, the framework constructs a light-interaction-driven 3DGS reparameterization model to generate multi-scale geometric features consistent with light transport principles, ensuring multi-view normal consistency. A cross-domain feature fusion module is designed within a conditional diffusion model, embedding geometric priors to constrain normal generation while maintaining accurate geometric error propagation. Furthermore, a differentiable 3D reprojection loss strategy is introduced for self-supervised optimization that minimizes geometric error between the reconstructed and input image, eliminating dependence on annotated normal datasets. Quantitative evaluations on the Google Scanned Objects dataset demonstrate that our method outperforms state-of-the-art approaches across multiple metrics. </p>
<blockquote>
<p>ç”±äºç¼ºä¹ç©ºé—´ç»´åº¦ä¿¡æ¯ï¼Œä»å•å¹…å›¾åƒè¿›è¡Œæ­£å¸¸ä¼°ç®—ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ€è¿‘çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•åœ¨äºŒç»´åˆ°ä¸‰ç»´éšå¼æ˜ å°„æ–¹é¢è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬ä¾èµ–äºæ•°æ®é©±åŠ¨çš„ç»Ÿè®¡å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶å¿½ç•¥äº†å…‰é¢äº¤äº’çš„æ˜¾å¼å»ºæ¨¡ï¼Œä»è€Œå¯¼è‡´å¤šè§†è§’æ­£å¸¸æ–¹å‘å†²çªã€‚æ­¤å¤–ï¼Œæ‰©æ•£æ¨¡å‹çš„ç¦»æ•£é‡‡æ ·æœºåˆ¶å¯¼è‡´å¯å¾®åˆ†æ¸²æŸ“é‡å»ºæ¨¡å—ä¸­çš„æ¢¯åº¦ä¸è¿ç»­ï¼Œé˜»ç¢äº†å°†ä¸‰ç»´å‡ ä½•è¯¯å·®åå‘ä¼ æ’­åˆ°æ­£å¸¸ç”Ÿæˆç½‘ç»œï¼Œä»è€Œè¿«ä½¿ç°æœ‰æ–¹æ³•ä¾èµ–äºå¯†é›†çš„æ­£å¸¸æ³¨é‡Šã€‚æœ¬æ–‡æå‡ºäº†SINGADï¼Œä¸€ç§æ–°å‹çš„è‡ªç›‘ç£æ¡†æ¶ï¼Œé€šè¿‡ä»¥ä¸‰ç»´é«˜æ–¯å–·å°„å¼•å¯¼æ‰©æ•£ä»å•å¹…å›¾åƒè¿›è¡Œæ­£å¸¸ä¼°ç®—ã€‚é€šè¿‡æ•´åˆç‰©ç†é©±åŠ¨çš„å…‰äº¤äº’å»ºæ¨¡å’ŒåŸºäºå¯å¾®åˆ†æ¸²æŸ“çš„é‡æŠ•å½±ç­–ç•¥ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ç›´æ¥å°†ä¸‰ç»´å‡ ä½•è¯¯å·®è½¬æ¢ä¸ºæ­£å¸¸ä¼˜åŒ–ä¿¡å·ï¼Œè§£å†³äº†å¤šè§†è§’å‡ ä½•ä¸ä¸€è‡´æ€§å’Œæ•°æ®ä¾èµ–æ€§çš„æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªå…‰äº¤äº’é©±åŠ¨çš„3DGSé‡å‚æ•°åŒ–æ¨¡å‹ï¼Œä»¥ç”Ÿæˆç¬¦åˆå…‰ä¼ è¾“åŸç†çš„å¤šå°ºåº¦å‡ ä½•ç‰¹å¾ï¼Œç¡®ä¿å¤šè§†è§’æ­£å¸¸çš„ä¸€è‡´æ€§ã€‚åœ¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸­è®¾è®¡äº†ä¸€ä¸ªè·¨åŸŸç‰¹å¾èåˆæ¨¡å—ï¼ŒåµŒå…¥å‡ ä½•å…ˆéªŒä»¥çº¦æŸæ­£å¸¸çš„ç”Ÿæˆï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®çš„å‡ ä½•è¯¯å·®ä¼ æ’­ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ç§å¯å¾®åˆ†çš„ä¸‰ç»´é‡æŠ•å½±æŸå¤±ç­–ç•¥è¿›è¡Œè‡ªç›‘ç£ä¼˜åŒ–ï¼Œä»¥æœ€å°åŒ–é‡å»ºå›¾åƒå’Œè¾“å…¥å›¾åƒä¹‹é—´çš„å‡ ä½•è¯¯å·®ï¼Œä»è€Œæ¶ˆé™¤å¯¹æ³¨é‡Šæ­£å¸¸æ•°æ®é›†ä¾èµ–æ€§ã€‚åœ¨Googleæ‰«æå¯¹è±¡æ•°æ®é›†ä¸Šçš„å®šé‡è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šæŒ‡æ ‡ä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05950v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹è‡ªç›‘ç£å•å›¾åƒæ³•æ³•å‘ä¼°è®¡æ¡†æ¶SINGADã€‚é€šè¿‡æ•´åˆç‰©ç†é©±åŠ¨çš„å…‰äº¤äº’å»ºæ¨¡å’Œå¯å¾®åˆ†æ¸²æŸ“çš„é‡æŠ•å½±ç­–ç•¥ï¼Œæ¡†æ¶ç›´æ¥è½¬åŒ–3Då‡ ä½•è¯¯å·®ä¸ºæ³•å‘ä¼˜åŒ–ä¿¡å·ï¼Œè§£å†³å¤šè§†è§’å‡ ä½•ä¸ä¸€è‡´å’Œæ•°æ®ä¾èµ–é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Googleæ‰«æç‰©ä½“æ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨2D-to-3Déšå¼æ˜ å°„ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†ä»é¢ä¸´ç©ºé—´ç»´åº¦ä¿¡æ¯ç¼ºå¤±çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–æ•°æ®é©±åŠ¨çš„ç»Ÿè®¡å…ˆéªŒï¼Œç¼ºä¹æ˜¾å¼å…‰é¢äº¤äº’å»ºæ¨¡ï¼Œå¯¼è‡´å¤šè§†è§’æ³•å‘æ–¹å‘å†²çªã€‚</li>
<li>SINGADæ¡†æ¶é€šè¿‡ç»“åˆç‰©ç†é©±åŠ¨çš„å…‰äº¤äº’å»ºæ¨¡å’Œå¯å¾®åˆ†æ¸²æŸ“ç­–ç•¥ï¼Œè§£å†³å¤šè§†è§’å‡ ä½•ä¸ä¸€è‡´å’Œæ•°æ®ä¾èµ–é—®é¢˜ã€‚</li>
<li>æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªå…‰äº¤äº’é©±åŠ¨çš„3DGSé‡å‚æ•°åŒ–æ¨¡å‹ï¼Œç”Ÿæˆä¸å…‰ä¼ è¾“åŸåˆ™ä¸€è‡´çš„å¤šå°ºåº¦å‡ ä½•ç‰¹å¾ï¼Œç¡®ä¿å¤šè§†è§’æ³•å‘ä¸€è‡´æ€§ã€‚</li>
<li>æ¡†æ¶è®¾è®¡äº†ä¸€ä¸ªè·¨åŸŸç‰¹å¾èåˆæ¨¡å—ï¼ŒåµŒå…¥å‡ ä½•å…ˆéªŒä»¥çº¦æŸæ³•å‘ç”Ÿæˆå¹¶ä¿æŒå‡†ç¡®çš„å‡ ä½•è¯¯å·®ä¼ æ’­ã€‚</li>
<li>å¼•å…¥å¯å¾®åˆ†çš„3Dé‡æŠ•å½±æŸå¤±ç­–ç•¥è¿›è¡Œè‡ªç›‘ç£ä¼˜åŒ–ï¼Œå‡å°‘é‡å»ºå›¾åƒä¸è¾“å…¥å›¾åƒä¹‹é—´çš„å‡ ä½•è¯¯å·®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05950">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7a075fd21a3e5b1aee131743f0a96da3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-004a80adc803ad4d448b863f0992b795.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MAISI-v2-Accelerated-3D-High-Resolution-Medical-Image-Synthesis-with-Rectified-Flow-and-Region-specific-Contrastive-Loss"><a href="#MAISI-v2-Accelerated-3D-High-Resolution-Medical-Image-Synthesis-with-Rectified-Flow-and-Region-specific-Contrastive-Loss" class="headerlink" title="MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with   Rectified Flow and Region-specific Contrastive Loss"></a>MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with   Rectified Flow and Region-specific Contrastive Loss</h2><p><strong>Authors:Can Zhao, Pengfei Guo, Dong Yang, Yucheng Tang, Yufan He, Benjamin Simon, Mason Belue, Stephanie Harmon, Baris Turkbey, Daguang Xu</strong></p>
<p>Medical image synthesis is an important topic for both clinical and research applications. Recently, diffusion models have become a leading approach in this area. Despite their strengths, many existing methods struggle with (1) limited generalizability that only work for specific body regions or voxel spacings, (2) slow inference, which is a common issue for diffusion models, and (3) weak alignment with input conditions, which is a critical issue for medical imaging. MAISI, a previously proposed framework, addresses generalizability issues but still suffers from slow inference and limited condition consistency. In this work, we present MAISI-v2, the first accelerated 3D medical image synthesis framework that integrates rectified flow to enable fast and high quality generation. To further enhance condition fidelity, we introduce a novel region-specific contrastive loss to enhance the sensitivity to region of interest. Our experiments show that MAISI-v2 can achieve SOTA image quality with $33 \times$ acceleration for latent diffusion model. We also conducted a downstream segmentation experiment to show that the synthetic images can be used for data augmentation. We release our code, training details, model weights, and a GUI demo to facilitate reproducibility and promote further development within the community. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆæˆåœ¨ä¸´åºŠå’Œç ”ç©¶åº”ç”¨ä¸­éƒ½æ˜¯ä¸€ä¸ªé‡è¦çš„è¯é¢˜ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹å·²æˆä¸ºè¯¥é¢†åŸŸçš„ä¸»æµæ–¹æ³•ã€‚å°½ç®¡å®ƒä»¬å…·æœ‰ä¼˜åŠ¿ï¼Œä½†è®¸å¤šç°æœ‰æ–¹æ³•ä»ç„¶é¢ä¸´ï¼ˆ1ï¼‰é€šç”¨æ€§æœ‰é™ï¼Œä»…é€‚ç”¨äºç‰¹å®šéƒ¨ä½æˆ–ä½“ç´ é—´è·ï¼›ï¼ˆ2ï¼‰æ¨ç†é€Ÿåº¦æ…¢ï¼Œè¿™æ˜¯æ‰©æ•£æ¨¡å‹çš„å¸¸è§é—®é¢˜ï¼›ï¼ˆ3ï¼‰ä¸è¾“å…¥æ¡ä»¶å¯¹é½ä¸ä½³ï¼Œè¿™å¯¹åŒ»å­¦å½±åƒæ¥è¯´æ˜¯å…³é”®é—®é¢˜ã€‚MAISIæ˜¯ä¸€ä¸ªå…ˆå‰æå‡ºçš„æ¡†æ¶ï¼Œè§£å†³äº†é€šç”¨æ€§é—®é¢˜ï¼Œä½†ä»ç„¶é¢ä¸´æ¨ç†é€Ÿåº¦æ…¢å’Œæ¡ä»¶ä¸€è‡´æ€§æœ‰é™çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MAISI-v2ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé›†æˆäº†çŸ«æ­£æµçš„åŠ é€Ÿ3DåŒ»å­¦å›¾åƒåˆæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°å¿«é€Ÿå’Œé«˜è´¨é‡çš„ç”Ÿæˆã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ¡ä»¶ä¿çœŸåº¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŒºåŸŸç‰¹å¼‚æ€§å¯¹æ¯”æŸå¤±ï¼Œä»¥æé«˜å¯¹æ„Ÿå…´è¶£åŒºåŸŸçš„æ•æ„Ÿæ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒMAISI-v2å¯ä»¥åœ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸Šå®ç°$33 \times$çš„åŠ é€Ÿï¼ŒåŒæ—¶è¾¾åˆ°æœ€ä½³å›¾åƒè´¨é‡ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†ä¸‹æ¸¸åˆ†å‰²å®éªŒï¼Œä»¥è¯æ˜åˆæˆå›¾åƒå¯ç”¨äºæ•°æ®å¢å¼ºã€‚æˆ‘ä»¬å‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ã€è®­ç»ƒç»†èŠ‚ã€æ¨¡å‹æƒé‡å’ŒGUIæ¼”ç¤ºï¼Œä»¥ä¿ƒè¿›ç¤¾åŒºå†…çš„å¯é‡å¤æ€§å’Œè¿›ä¸€æ­¥å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05772v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŒ»ç–—å›¾åƒåˆæˆåœ¨ä¸´åºŠå’Œç ”ç©¶åº”ç”¨ä¸­å‡å…·æœ‰é‡è¦æ„ä¹‰ï¼Œæ‰©æ•£æ¨¡å‹æ˜¯è¿™ä¸€é¢†åŸŸçš„é¢†å…ˆæ–¹æ³•ã€‚è™½ç„¶å¼ºå¤§ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå¦‚é€šç”¨æ€§ä¸è¶³ã€æ¨ç†é€Ÿåº¦æ…¢ä»¥åŠä¸è¾“å…¥æ¡ä»¶å¯¹é½ä¸ä½³ã€‚MAISI-v2æ˜¯è§£å†³è¿™äº›é—®é¢˜çš„é¦–ä¸ªåŠ é€Ÿçš„3DåŒ»ç–—å›¾åƒåˆæˆæ¡†æ¶ï¼Œå®ƒé€šè¿‡æ•´åˆçŸ«æ­£æµå®ç°å¿«é€Ÿã€é«˜è´¨é‡çš„ç”Ÿæˆã€‚ä¸ºè¿›ä¸€æ­¥æé«˜æ¡ä»¶ä¿çœŸåº¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŒºåŸŸç‰¹å¼‚æ€§å¯¹æ¯”æŸå¤±ï¼Œä»¥æé«˜å¯¹æ„Ÿå…´è¶£åŒºåŸŸçš„æ•æ„Ÿæ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒMAISI-v2å¯ä»¥åœ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸Šå®ç°$33 \times$çš„åŠ é€Ÿï¼ŒåŒæ—¶è¾¾åˆ°æœ€å…ˆè¿›çš„å›¾åƒè´¨é‡ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†ä¸‹æ¸¸åˆ†å‰²å®éªŒï¼Œä»¥å±•ç¤ºåˆæˆå›¾åƒå¯ç”¨äºæ•°æ®å¢å¼ºã€‚æˆ‘ä»¬å…¬å¼€äº†ä»£ç ã€è®­ç»ƒç»†èŠ‚ã€æ¨¡å‹æƒé‡å’ŒGUIæ¼”ç¤ºï¼Œä»¥ä¿ƒè¿›ç¤¾åŒºå†…çš„å¯é‡å¤æ€§å’Œè¿›ä¸€æ­¥å‘å±•ã€‚</p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<ol>
<li>åŒ»ç–—å›¾åƒåˆæˆæ˜¯ä¸´åºŠå’Œç ”ç©¶çš„é‡è¦åº”ç”¨ä¹‹ä¸€ï¼Œæ‰©æ•£æ¨¡å‹æ˜¯è¯¥é¢†åŸŸçš„é¢†å…ˆæ–¹æ³•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨é€šç”¨æ€§ä¸è¶³çš„é—®é¢˜ï¼Œä»…é€‚ç”¨äºç‰¹å®šåŒºåŸŸæˆ–ä½“ç´ é—´è·ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹çš„å¸¸è§é—®é¢˜æ˜¯æ¨ç†é€Ÿåº¦æ…¢ã€‚</li>
<li>åŒ»ç–—æˆåƒä¸­ï¼Œä¸è¾“å…¥æ¡ä»¶å¯¹é½ä¸ä½³æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚</li>
<li>MAISI-v2æ˜¯é¦–ä¸ªåŠ é€Ÿçš„3DåŒ»ç–—å›¾åƒåˆæˆæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆçŸ«æ­£æµå®ç°å¿«é€Ÿé«˜è´¨é‡çš„ç”Ÿæˆã€‚</li>
<li>ä¸ºæé«˜æ¡ä»¶ä¿çœŸåº¦ï¼Œå¼•å…¥äº†æ–°å‹åŒºåŸŸç‰¹å¼‚æ€§å¯¹æ¯”æŸå¤±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05772">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-99ccdc6ec79bededbc9904f95a3d7982.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bceeb8c7cb53b53392ffa55dda043101.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f843b263d3f179e3c12ee60cdd285d53.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c69c1811a34f158d5341af8c4ed3bac8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e50db7b49953724dc966807c93730ee3.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Neural-Driven-Image-Editing"><a href="#Neural-Driven-Image-Editing" class="headerlink" title="Neural-Driven Image Editing"></a>Neural-Driven Image Editing</h2><p><strong>Authors:Pengfei Zhou, Jie Xia, Xiaopeng Peng, Wangbo Zhao, Zilong Ye, Zekai Li, Suorong Yang, Jiadong Pan, Yuanxiang Chen, Ziqiao Wang, Kai Wang, Qian Zheng, Xiaojun Chang, Gang Pan, Shurong Dong, Kaipeng Zhang, Yang You</strong></p>
<p>Traditional image editing typically relies on manual prompting, making it labor-intensive and inaccessible to individuals with limited motor control or language abilities. Leveraging recent advances in brain-computer interfaces (BCIs) and generative models, we propose LoongX, a hands-free image editing approach driven by multimodal neurophysiological signals. LoongX utilizes state-of-the-art diffusion models trained on a comprehensive dataset of 23,928 image editing pairs, each paired with synchronized electroencephalography (EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography (PPG), and head motion signals that capture user intent. To effectively address the heterogeneity of these signals, LoongX integrates two key modules. The cross-scale state space (CS3) module encodes informative modality-specific features. The dynamic gated fusion (DGF) module further aggregates these features into a unified latent space, which is then aligned with edit semantics via fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train the encoders using contrastive learning to align cognitive states with semantic intentions from embedded natural language. Extensive experiments demonstrate that LoongX achieves performance comparable to text-driven methods (CLIP-I: 0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural signals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results highlight the promise of neural-driven generative models in enabling accessible, intuitive image editing and open new directions for cognitive-driven creative technologies. Datasets and code will be released to support future work and foster progress in this emerging area. </p>
<blockquote>
<p>ä¼ ç»Ÿå›¾åƒç¼–è¾‘é€šå¸¸ä¾èµ–äºæ‰‹åŠ¨æç¤ºï¼Œè¿™ä½¿å¾—å®ƒåŠ³åŠ¨å¯†é›†ï¼Œå¯¹äºè¿åŠ¨æ§åˆ¶æˆ–è¯­è¨€èƒ½åŠ›æœ‰é™çš„ä¸ªäººæ¥è¯´éš¾ä»¥æ¥è§¦ã€‚æˆ‘ä»¬åˆ©ç”¨æœ€è¿‘è„‘æœºæ¥å£ï¼ˆBCIsï¼‰å’Œç”Ÿæˆæ¨¡å‹çš„è¿›æ­¥ï¼Œæå‡ºäº†LoongXï¼Œä¸€ç§ç”±å¤šæ¨¡æ€ç¥ç»ç”Ÿç†ä¿¡å·é©±åŠ¨çš„æ— æ¥è§¦å›¾åƒç¼–è¾‘æ–¹æ³•ã€‚LoongXåˆ©ç”¨æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ï¼Œåœ¨23928ä¸ªå›¾åƒç¼–è¾‘å¯¹ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œæ¯ä¸ªå›¾åƒç¼–è¾‘å¯¹éƒ½ä¸åŒæ­¥è„‘ç”µå›¾ï¼ˆEEGï¼‰ã€åŠŸèƒ½è¿‘çº¢å¤–å…‰è°±ï¼ˆfNIRSï¼‰ã€å…‰ç”µå®¹ç§¯è„‰ææ³¢ï¼ˆPPGï¼‰å’Œå¤´éƒ¨è¿åŠ¨ä¿¡å·é…å¯¹ï¼Œè¿™äº›ä¿¡å·æ•æ‰äº†ç”¨æˆ·æ„å›¾ã€‚ä¸ºäº†æœ‰æ•ˆè§£å†³è¿™äº›ä¿¡å·çš„å¼‚è´¨æ€§ï¼ŒLoongXé›†æˆäº†ä¸¤ä¸ªå…³é”®æ¨¡å—ã€‚è·¨å°ºåº¦çŠ¶æ€ç©ºé—´ï¼ˆCS3ï¼‰æ¨¡å—ç¼–ç å…·æœ‰ä¿¡æ¯æ€§çš„æ¨¡æ€ç‰¹å®šç‰¹å¾ã€‚åŠ¨æ€é—¨æ§èåˆï¼ˆDGFï¼‰æ¨¡å—è¿›ä¸€æ­¥å°†è¿™äº›ç‰¹å¾èšåˆåˆ°ç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œç„¶åé€šè¿‡å¾®è°ƒæ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰ä¸ç¼–è¾‘è¯­ä¹‰å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨å¯¹æ¯”å­¦ä¹ å¯¹ç¼–ç å™¨è¿›è¡Œé¢„è®­ç»ƒï¼Œå°†è®¤çŸ¥çŠ¶æ€ä¸åµŒå…¥çš„è‡ªç„¶è¯­è¨€çš„è¯­ä¹‰æ„å›¾å¯¹é½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLoongXçš„æ€§èƒ½ä¸æ–‡æœ¬é©±åŠ¨çš„æ–¹æ³•ç›¸å½“ï¼ˆCLIP-Iï¼š0.6605 vs. 0.6558ï¼›DINOï¼š0.4812 vs. 0.4636ï¼‰ï¼Œå¹¶åœ¨ç»“åˆç¥ç»ä¿¡å·æ—¶è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼ˆCLIP-Tï¼š0.2588 vs. 0.2549ï¼‰ã€‚è¿™äº›ç»“æœçªæ˜¾äº†ç¥ç»é©±åŠ¨ç”Ÿæˆæ¨¡å‹åœ¨å›¾åƒç¼–è¾‘æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶ä¸ºè®¤çŸ¥é©±åŠ¨çš„åˆ›é€ æ€§æŠ€æœ¯æ‰“å¼€äº†æ–°çš„å‘å±•æ–¹å‘ã€‚æˆ‘ä»¬å°†å‘å¸ƒæ•°æ®é›†å’Œä»£ç ä»¥æ”¯æŒæœªæ¥çš„å·¥ä½œå¹¶ä¿ƒè¿›è¿™ä¸€æ–°å…´é¢†åŸŸçš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05397v2">PDF</a> 22 pages, 14 figures</p>
<p><strong>Summary</strong><br>     åŸºäºå…ˆè¿›çš„è„‘æœºæ¥å£æŠ€æœ¯å’Œç”Ÿæˆæ¨¡å‹ï¼Œæå‡ºä¸€ç§åä¸ºLoongXçš„æ— æ‰‹å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œé€šè¿‡å¤šæ¨¡æ€ç¥ç»ç”Ÿç†ä¿¡å·é©±åŠ¨ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼Œæœ‰æ•ˆæ•´åˆä¸åŒä¿¡å·ï¼Œå®ç°ç”¨æˆ·æ„å›¾ä¸å›¾åƒç¼–è¾‘çš„ç²¾å‡†å¯¹æ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LoongXåˆ©ç”¨å…ˆè¿›çš„è„‘æœºæ¥å£æŠ€æœ¯ï¼Œå®ç°æ— æ‰‹æ“ä½œçš„å›¾åƒç¼–è¾‘ã€‚</li>
<li>é€šè¿‡ç»“åˆå¤šæ¨¡æ€ç¥ç»ç”Ÿç†ä¿¡å·ï¼ˆEEGã€fNIRSã€PPGç­‰ï¼‰ï¼Œæ•è·ç”¨æˆ·æ„å›¾ã€‚</li>
<li>LoongXé€šè¿‡CS3æ¨¡å—æå–ä¿¡å·ä¸­çš„ç‰¹å®šç‰¹å¾ï¼Œå¹¶é€šè¿‡DGFæ¨¡å—å°†è¿™äº›ç‰¹å¾æ•´åˆåˆ°ç»Ÿä¸€æ½œåœ¨ç©ºé—´ã€‚</li>
<li>åˆ©ç”¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰ä¸ç¼–è¾‘è¯­ä¹‰å¯¹é½ï¼Œé€šè¿‡å¾®è°ƒå®ç°æ½œåœ¨ç©ºé—´çš„ç²¾ç¡®æ“ä½œã€‚</li>
<li>é€šè¿‡å¯¹æ¯”å®éªŒï¼Œæ˜¾ç¤ºLoongXåœ¨æ€§èƒ½ä¸Šå¯ä¸æ–‡æœ¬é©±åŠ¨æ–¹æ³•ç›¸åª²ç¾ï¼Œå¹¶åœ¨ç»“åˆç¥ç»ä¿¡å·å’Œè¯­éŸ³æ—¶è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>LoongXçš„å®ç°è¯æ˜äº†ç¥ç»é©±åŠ¨ç”Ÿæˆæ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä¸­çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05397">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-95161203435d1f72e220956f6ce93237.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f9057763433807b27477d32afdeac12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fba4545ea2ffedf1f7be7d1cc611da2e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DanceGRPO-Unleashing-GRPO-on-Visual-Generation"><a href="#DanceGRPO-Unleashing-GRPO-on-Visual-Generation" class="headerlink" title="DanceGRPO: Unleashing GRPO on Visual Generation"></a>DanceGRPO: Unleashing GRPO on Visual Generation</h2><p><strong>Authors:Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, Ping Luo</strong></p>
<p>Recent advances in generative AI have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. While Reinforcement Learning (RL) has emerged as a promising approach for fine-tuning generative models, existing methods like DDPO and DPOK face fundamental limitations - particularly their inability to maintain stable optimization when scaling to large and diverse prompt sets, severely restricting their practical utility. This paper presents DanceGRPO, a framework that addresses these limitations through an innovative adaptation of Group Relative Policy Optimization (GRPO) for visual generation tasks. Our key insight is that GRPOâ€™s inherent stability mechanisms uniquely position it to overcome the optimization challenges that plague prior RL-based approaches on visual generation. DanceGRPO establishes several significant advances: First, it demonstrates consistent and stable policy optimization across multiple modern generative paradigms, including both diffusion models and rectified flows. Second, it maintains robust performance when scaling to complex, real-world scenarios encompassing three key tasks and four foundation models. Third, it shows remarkable versatility in optimizing for diverse human preferences as captured by five distinct reward models assessing image&#x2F;video aesthetics, text-image alignment, video motion quality, and binary feedback. Our comprehensive experiments reveal that DanceGRPO outperforms baseline methods by up to 181% across multiple established benchmarks, including HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis. </p>
<blockquote>
<p>æœ€è¿‘ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„è¿›æ­¥åœ¨è§†è§‰å†…å®¹åˆ›ä½œæ–¹é¢å¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ï¼Œç„¶è€Œï¼Œå°†æ¨¡å‹è¾“å‡ºä¸äººç±»åå¥½å¯¹é½ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¾®è°ƒç”Ÿæˆæ¨¡å‹æ–¹é¢æ˜¾ç¤ºå‡ºæœ‰å‰é€”çš„åº”ç”¨å‰æ™¯ï¼Œä½†ç°æœ‰çš„æ–¹æ³•å¦‚DDPOå’ŒDPOKé¢ä¸´åŸºç¡€æ€§çš„å±€é™â€”â€”ç‰¹åˆ«æ˜¯åœ¨æ‰©å±•åˆ°å¤§å‹å’Œå¤šæ ·åŒ–çš„æç¤ºé›†æ—¶æ— æ³•ç»´æŒç¨³å®šçš„ä¼˜åŒ–ï¼Œè¿™ä¸¥é‡é™åˆ¶äº†å®ƒä»¬çš„å®ç”¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†DanceGRPOæ¡†æ¶ï¼Œå®ƒé€šè¿‡é’ˆå¯¹è§†è§‰ç”Ÿæˆä»»åŠ¡çš„Group Relative Policy Optimizationï¼ˆGRPOï¼‰çš„åˆ›æ–°æ€§é€‚åº”æ¥è§£å†³è¿™äº›å±€é™æ€§ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯GRPOçš„å†…åœ¨ç¨³å®šæ€§æœºåˆ¶ä»¥å…¶ç‹¬ç‰¹çš„æ–¹å¼å…‹æœäº†ä¼˜åŒ–æŒ‘æˆ˜ï¼Œè¿™äº›æŒ‘æˆ˜ä¸€ç›´å›°æ‰°ç€å…ˆå‰çš„åŸºäºRLçš„è§†è§‰ç”Ÿæˆæ–¹æ³•ã€‚DanceGRPOå–å¾—äº†å‡ ä¸ªé‡è¦è¿›å±•ï¼šé¦–å…ˆï¼Œå®ƒè¯æ˜äº†åœ¨ç°ä»£ç”ŸæˆèŒƒå¼ä¸­çš„ç¨³å®šæ”¿ç­–ä¼˜åŒ–ä¸€è‡´æ€§ï¼ŒåŒ…æ‹¬æ‰©æ•£æ¨¡å‹å’Œæ ¡æ­£æµã€‚å…¶æ¬¡ï¼Œåœ¨æ‰©å±•åˆ°åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ä»»åŠ¡å’Œå››ä¸ªåŸºç¡€æ¨¡å‹åœ¨å†…çš„å¤æ‚ç°å®ä¸–ç•Œåœºæ™¯æ—¶ï¼Œå®ƒä¿æŒäº†ç¨³å¥çš„æ€§èƒ½ã€‚ç¬¬ä¸‰ï¼Œåœ¨ä¼˜åŒ–äº”ç§ä¸åŒå¥–åŠ±æ¨¡å‹æ‰€æ•æ‰åˆ°çš„ä¸åŒäººç±»åå¥½æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„é€šç”¨æ€§ï¼Œè¿™äº›å¥–åŠ±æ¨¡å‹è¯„ä¼°å›¾åƒ&#x2F;è§†é¢‘ç¾å­¦ã€æ–‡æœ¬ä¸å›¾åƒçš„å¯¹é½æ–¹å¼ã€è§†é¢‘è¿åŠ¨è´¨é‡å’ŒäºŒè¿›åˆ¶åé¦ˆã€‚æˆ‘ä»¬çš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œåœ¨å¤šä¸ªæ—¢å®šåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDanceGRPOè¾ƒåŸºçº¿æ–¹æ³•é«˜å‡º181%ï¼ŒåŒ…æ‹¬HPS-v2.1ã€CLIP Scoreã€VideoAlignå’ŒGenEvalã€‚æˆ‘ä»¬çš„ç»“æœè¯å®ï¼ŒDanceGRPOæ˜¯è§†è§‰ç”Ÿæˆé¢†åŸŸä¸­å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä»»åŠ¡æ‰©å±•çš„ç¨³å¥ä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ä¸è§†è§‰åˆæˆçš„å’Œè°ç»Ÿä¸€æä¾›äº†æ–°çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07818v3">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://dancegrpo.github.io/">https://dancegrpo.github.io/</a></p>
<p><strong>Summary</strong><br>     æœ€æ–°è¿›å±•çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨è§†è§‰å†…å®¹åˆ›ä½œé¢†åŸŸæ€èµ·é©å‘½ï¼Œä½†ä»é¢ä¸´æ¨¡å‹è¾“å‡ºä¸äººç±»åå¥½å¯¹é½çš„æŒ‘æˆ˜ã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¾®è°ƒç”Ÿæˆæ¨¡å‹æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•å¦‚DDPOå’ŒDPOKå­˜åœ¨æ ¹æœ¬æ€§å±€é™ï¼Œéš¾ä»¥åœ¨å¤§è§„æ¨¡ä¸”å¤šæ ·åŒ–çš„æç¤ºé›†ä¸Šç»´æŒç¨³å®šçš„ä¼˜åŒ–ï¼Œé™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚æœ¬æ–‡æå‡ºDanceGRPOæ¡†æ¶ï¼Œé€šè¿‡é›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„åˆ›æ–°é€‚åº”ï¼Œè§£å†³è¿™äº›æŒ‘æˆ˜ã€‚DanceGRPOåœ¨è§†è§‰ç”Ÿæˆä»»åŠ¡ä¸Šå®ç°äº†å¤šé¡¹é‡è¦è¿›å±•ï¼ŒåŒ…æ‹¬åœ¨ä¸åŒç°ä»£ç”ŸæˆèŒƒå¼ä¸­æŒç»­ç¨³å®šçš„æ”¿ç­–ä¼˜åŒ–ã€åœ¨å¤æ‚ç°å®åœºæ™¯ä¸­çš„ç¨³å¥æ€§èƒ½ä»¥åŠä¼˜åŒ–å¤šç§äººç±»åå¥½çš„çµæ´»æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒDanceGRPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾ƒåŸºçº¿æ–¹æ³•é«˜å‡º181%ï¼Œæˆä¸ºè§†è§‰ç”Ÿæˆé¢†åŸŸä¸­å¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆï¼ˆRLHFï¼‰ä»»åŠ¡çš„ç¨³å¥ä¸”é€šç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼AIåœ¨è§†è§‰å†…å®¹åˆ›ä½œé¢†åŸŸçš„è¿›å±•åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¾®è°ƒç”Ÿæˆæ¨¡å‹ä¸­çš„æ½œåŠ›ä¸ç°æœ‰æ–¹æ³•çš„å±€é™ã€‚</li>
<li>DanceGRPOæ¡†æ¶é€šè¿‡é›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰è§£å†³ç°æœ‰æŒ‘æˆ˜ã€‚</li>
<li>DanceGRPOå®ç°è·¨å¤šç§ç”ŸæˆèŒƒå¼çš„ç¨³å®šæ”¿ç­–ä¼˜åŒ–ã€‚</li>
<li>DanceGRPOåœ¨å¤æ‚ç°å®åœºæ™¯ä¸­çš„ç¨³å¥æ€§èƒ½ã€‚</li>
<li>DanceGRPOä¼˜åŒ–å¤šç§äººç±»åå¥½çš„çµæ´»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07818">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-13ac5fd83772d9b961ec574be998440e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e9d6931f2e16a40f43b18a7d58d5dd8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17ac967dcc28335cbc82838396dd7a5b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FLUX-Text-A-Simple-and-Advanced-Diffusion-Transformer-Baseline-for-Scene-Text-Editing"><a href="#FLUX-Text-A-Simple-and-Advanced-Diffusion-Transformer-Baseline-for-Scene-Text-Editing" class="headerlink" title="FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for   Scene Text Editing"></a>FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for   Scene Text Editing</h2><p><strong>Authors:Rui Lan, Yancheng Bai, Xu Duan, Mingxing Li, Dongyang Jin, Ryan Xu, Lei Sun, Xiangxiang Chu</strong></p>
<p>Scene text editing aims to modify or add texts on images while ensuring text fidelity and overall visual quality consistent with the background. Recent methods are primarily built on UNet-based diffusion models, which have improved scene text editing results, but still struggle with complex glyph structures, especially for non-Latin ones (\eg, Chinese, Korean, Japanese). To address these issues, we present \textbf{FLUX-Text}, a simple and advanced multilingual scene text editing DiT method. Specifically, our FLUX-Text enhances glyph understanding and generation through lightweight Visual and Text Embedding Modules, while preserving the original generative capability of FLUX. We further propose a Regional Text Perceptual Loss tailored for text regions, along with a matching two-stage training strategy to better balance text editing and overall image quality. Benefiting from the DiT-based architecture and lightweight feature injection modules, FLUX-Text can be trained with only $0.1$M training examples, a \textbf{97%} reduction compared to $2.9$M required by popular methods. Extensive experiments on multiple public datasets, including English and Chinese benchmarks, demonstrate that our method surpasses other methods in visual quality and text fidelity. All the code is available at <a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/FluxText">https://github.com/AMAP-ML/FluxText</a>. </p>
<blockquote>
<p>åœºæ™¯æ–‡æœ¬ç¼–è¾‘æ—¨åœ¨åœ¨å›¾åƒä¸Šä¿®æ”¹æˆ–æ·»åŠ æ–‡æœ¬ï¼ŒåŒæ—¶ç¡®ä¿æ–‡æœ¬å¿ è¯šåº¦å’Œä¸èƒŒæ™¯ä¸€è‡´çš„æ•´ä½“è§†è§‰è´¨é‡ã€‚æœ€è¿‘çš„æ–¹æ³•ä¸»è¦å»ºç«‹åœ¨åŸºäºUNetçš„æ‰©æ•£æ¨¡å‹ä¸Šï¼Œå·²ç»æ”¹å–„äº†åœºæ™¯æ–‡æœ¬ç¼–è¾‘çš„ç»“æœï¼Œä½†åœ¨å¤„ç†å¤æ‚çš„å­—å½¢ç»“æ„æ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯éæ‹‰ä¸è¯­ç³»ï¼ˆä¾‹å¦‚ä¸­æ–‡ã€éŸ©è¯­ã€æ—¥è¯­ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>FLUX-Text</strong>ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•å…ˆè¿›çš„å¤šå…ƒåœºæ™¯æ–‡æœ¬ç¼–è¾‘DiTæ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„FLUX-Texté€šè¿‡è½»é‡çº§çš„è§†è§‰å’Œæ–‡æœ¬åµŒå…¥æ¨¡å—å¢å¼ºäº†å­—å½¢ç†è§£å’Œç”Ÿæˆï¼ŒåŒæ—¶ä¿ç•™äº†FLUXçš„åŸå§‹ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜é’ˆå¯¹æ–‡æœ¬åŒºåŸŸæå‡ºäº†åŒºåŸŸæ–‡æœ¬æ„ŸçŸ¥æŸå¤±ï¼Œä»¥åŠä¸€ä¸ªé…å¥—çš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä»¥æ›´å¥½åœ°å¹³è¡¡æ–‡æœ¬ç¼–è¾‘å’Œæ•´ä½“å›¾åƒè´¨é‡ã€‚å¾—ç›ŠäºDiTæ¶æ„å’Œè½»é‡çº§ç‰¹å¾æ³¨å…¥æ¨¡å—ï¼ŒFLUX-Textä»…éœ€0.1Mè®­ç»ƒæ ·æœ¬å³å¯è¿›è¡Œè®­ç»ƒï¼Œä¸æµè¡Œæ–¹æ³•æ‰€éœ€çš„2.9Mç›¸æ¯”ï¼Œå‡å°‘äº†97%ã€‚åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬è‹±è¯­å’Œä¸­æ–‡åŸºå‡†æµ‹è¯•ï¼Œè¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œæ–‡æœ¬å¿ è¯šåº¦æ–¹é¢è¶…è¿‡äº†å…¶ä»–æ–¹æ³•ã€‚æ‰€æœ‰ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/FluxText%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/AMAP-ML/FluxTextè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03329v3">PDF</a> 10 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºUNetæ‰©æ•£æ¨¡å‹çš„å…ˆè¿›å¤šè¯­è¨€åœºæ™¯æ–‡æœ¬ç¼–è¾‘æ–¹æ³•â€”â€”FLUX-Textã€‚è¯¥æ–¹æ³•é€šè¿‡è½»é‡çº§è§†è§‰å’Œæ–‡æœ¬åµŒå…¥æ¨¡å—ï¼Œå¢å¼ºäº†å­—å½¢ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†FLUXçš„åŸå§‹ç”Ÿæˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§é’ˆå¯¹æ–‡æœ¬åŒºåŸŸçš„åŒºåŸŸæ€§æ–‡æœ¬æ„ŸçŸ¥æŸå¤±å’Œç›¸åº”çš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä»¥æ›´å¥½åœ°å¹³è¡¡æ–‡æœ¬ç¼–è¾‘å’Œæ•´ä½“å›¾åƒè´¨é‡ã€‚FLUX-Textå¯åœ¨ä»…ä½¿ç”¨å°‘é‡è®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹è¾¾åˆ°å‡ºè‰²çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†éæ‹‰ä¸è¯­ç³»ï¼ˆå¦‚ä¸­æ–‡ï¼‰çš„å¤æ‚å­—å½¢ç»“æ„æ—¶è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FLUX-Textæ˜¯åŸºäºUNetæ‰©æ•£æ¨¡å‹çš„å…ˆè¿›å¤šè¯­è¨€åœºæ™¯æ–‡æœ¬ç¼–è¾‘æ–¹æ³•ã€‚</li>
<li>FLUX-Texté€šè¿‡è½»é‡çº§è§†è§‰å’Œæ–‡æœ¬åµŒå…¥æ¨¡å—å¢å¼ºäº†å­—å½¢ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é’ˆå¯¹æ–‡æœ¬åŒºåŸŸçš„åŒºåŸŸæ€§æ–‡æœ¬æ„ŸçŸ¥æŸå¤±ï¼Œä»¥æé«˜æ–‡æœ¬ç¼–è¾‘çš„è´¨é‡ã€‚</li>
<li>é‡‡ç”¨äº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä»¥å¹³è¡¡æ–‡æœ¬ç¼–è¾‘å’Œæ•´ä½“å›¾åƒè´¨é‡çš„ä¼˜åŒ–ã€‚</li>
<li>FLUX-Textå¯åœ¨ä»…ä½¿ç”¨å°‘é‡è®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹è¾¾åˆ°å‡ºè‰²çš„æ€§èƒ½ã€‚</li>
<li>FLUX-Textåœ¨å¤„ç†éæ‹‰ä¸è¯­ç³»ï¼ˆå¦‚ä¸­æ–‡ï¼‰çš„å¤æ‚å­—å½¢ç»“æ„æ—¶è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ‰€æœ‰ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03329">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4ab8fe005ddbce4922111133882179ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c23f81414da9c62ce39738ce4c3e75d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8d5280844e91632a6a4677afec08994.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1368a212715e29a72b28910a429cb3aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46fe98b625663acfea9aa6119d54877a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3ce41b3c4de87e1325f0f800a29a89e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Conditional-Diffusion-Models-are-Medical-Image-Classifiers-that-Provide-Explainability-and-Uncertainty-for-Free"><a href="#Conditional-Diffusion-Models-are-Medical-Image-Classifiers-that-Provide-Explainability-and-Uncertainty-for-Free" class="headerlink" title="Conditional Diffusion Models are Medical Image Classifiers that Provide   Explainability and Uncertainty for Free"></a>Conditional Diffusion Models are Medical Image Classifiers that Provide   Explainability and Uncertainty for Free</h2><p><strong>Authors:Gian Mario Favero, Parham Saremi, Emily Kaczmarek, Brennan Nichyporuk, Tal Arbel</strong></p>
<p>Discriminative classifiers have become a foundational tool in deep learning for medical imaging, excelling at learning separable features of complex data distributions. However, these models often need careful design, augmentation, and training techniques to ensure safe and reliable deployment. Recently, diffusion models have become synonymous with generative modeling in 2D. These models showcase robustness across a range of tasks including natural image classification, where classification is performed by comparing reconstruction errors across images generated for each possible conditioning input. This work presents the first exploration of the potential of class conditional diffusion models for 2D medical image classification. First, we develop a novel majority voting scheme shown to improve the performance of medical diffusion classifiers. Next, extensive experiments on the CheXpert and ISIC Melanoma skin cancer datasets demonstrate that foundation and trained-from-scratch diffusion models achieve competitive performance against SOTA discriminative classifiers without the need for explicit supervision. In addition, we show that diffusion classifiers are intrinsically explainable, and can be used to quantify the uncertainty of their predictions, increasing their trustworthiness and reliability in safety-critical, clinical contexts. Further information is available on our project page: <a target="_blank" rel="noopener" href="https://faverogian.github.io/med-diffusion-classifier.github.io/">https://faverogian.github.io/med-diffusion-classifier.github.io/</a>. </p>
<blockquote>
<p>åˆ¤åˆ«åˆ†ç±»å™¨å·²æˆä¸ºæ·±åº¦å­¦ä¹ åŒ»å­¦å½±åƒåˆ†æçš„åŸºç¡€å·¥å…·ï¼Œæ“…é•¿å­¦ä¹ å¤æ‚æ•°æ®åˆ†å¸ƒçš„åˆ†ç¦»ç‰¹å¾ã€‚ç„¶è€Œï¼Œä¸ºäº†ç¡®ä¿å®‰å…¨å¯é çš„éƒ¨ç½²ï¼Œè¿™äº›æ¨¡å‹é€šå¸¸éœ€è¦ç²¾å¿ƒè®¾è®¡ã€å¢å¼ºå’Œè®­ç»ƒæŠ€æœ¯ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹å·²æˆä¸ºäºŒç»´ç”Ÿæˆæ¨¡å‹çš„ä»£åè¯ã€‚è¿™äº›æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºç¨³å¥æ€§ï¼ŒåŒ…æ‹¬è‡ªç„¶å›¾åƒåˆ†ç±»ï¼Œåˆ†ç±»æ˜¯é€šè¿‡æ¯”è¾ƒæ¯ä¸ªå¯èƒ½çš„æ¡ä»¶è¾“å…¥æ‰€ç”Ÿæˆçš„å›¾åƒçš„é‡æ„è¯¯å·®æ¥å®Œæˆçš„ã€‚æœ¬æ–‡é¦–æ¬¡æ¢ç´¢äº†ç±»æ¡ä»¶æ‰©æ•£æ¨¡å‹åœ¨äºŒç»´åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­çš„æ½œåŠ›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹å¤šæ•°æŠ•ç¥¨æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆè¢«è¯æ˜å¯ä»¥æé«˜åŒ»å­¦æ‰©æ•£åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚æ¥ä¸‹æ¥ï¼Œåœ¨CheXpertå’ŒISICé»‘è‰²ç´ ç˜¤çš®è‚¤ç™Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒåŸºç¡€æ‰©æ•£æ¨¡å‹å’Œä»å¤´è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ä¸æœ€æ–°åˆ¤åˆ«åˆ†ç±»å™¨ç›¸å½“ï¼Œæ— éœ€æ˜¾å¼ç›‘ç£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œæ‰©æ•£åˆ†ç±»å™¨æœ¬è´¨ä¸Šæ˜¯å¯è§£é‡Šçš„ï¼Œå¯ç”¨äºé‡åŒ–å…¶é¢„æµ‹çš„çš„ä¸ç¡®å®šæ€§ï¼Œè¿™åœ¨å®‰å…¨å…³é”®çš„ä¸´åºŠç¯å¢ƒä¸­å¢åŠ äº†å…¶å¯ä¿¡åº¦å’Œå¯é æ€§ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://faverogian.github.io/med-diffusion-classifier.github.io/%E3%80%82">https://faverogian.github.io/med-diffusion-classifier.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03687v2">PDF</a> Accepted for publication at MIDL 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡é¦–æ¬¡æ¢ç´¢äº†ç±»æ¡ä»¶æ‰©æ•£æ¨¡å‹åœ¨äºŒç»´åŒ»å­¦å½±åƒåˆ†ç±»ä¸­çš„æ½œåŠ›ã€‚ç ”ç©¶å›¢é˜Ÿå‘å±•äº†ä¸€ç§æ–°å‹å¤šæ•°æŠ•ç¥¨æ–¹æ¡ˆï¼Œç”¨äºæé«˜åŒ»å­¦å½±åƒæ‰©æ•£åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ— è®ºæ˜¯åœ¨CheXpertè¿˜æ˜¯ISICé»‘è‰²ç´ ç˜¤çš®è‚¤ç™Œæ•°æ®é›†ä¸Šï¼ŒåŸºç¡€æ‰©æ•£æ¨¡å‹å’Œä»å¤´è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹éƒ½èƒ½è¾¾åˆ°ä¸å½“å‰é¡¶å°–åˆ¤åˆ«æ¨¡å‹ç›¸ç«äº‰çš„æ€§èƒ½ï¼Œä¸”æ— éœ€æ˜¾å¼ç›‘ç£ã€‚æ­¤å¤–ï¼Œæ‰©æ•£åˆ†ç±»å™¨å…·æœ‰å†…åœ¨çš„å¯è§£é‡Šæ€§ï¼Œå¯é‡åŒ–é¢„æµ‹çš„ä¸ç¡®å®šæ€§ï¼Œå¢åŠ äº†å…¶åœ¨å®‰å…¨å…³é”®çš„åŒ»ç–—ç¯å¢ƒä¸­çš„å¯ä¿¡åº¦å’Œå¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç±»æ¡ä»¶æ‰©æ•£æ¨¡å‹é¦–æ¬¡è¢«æ¢ç´¢ç”¨äºäºŒç»´åŒ»å­¦å½±åƒåˆ†ç±»ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæå‡ºäº†æ–°å‹å¤šæ•°æŠ•ç¥¨æ–¹æ¡ˆä»¥æé«˜åŒ»å­¦å½±åƒæ‰©æ•£åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨åŒ»ç–—å½±åƒåˆ†ç±»ä¸Šè¡¨ç°å‡ºä¸é¡¶å°–åˆ¤åˆ«æ¨¡å‹ç›¸ç«äº‰çš„æ€§èƒ½ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹æ— éœ€æ˜¾å¼ç›‘ç£ï¼Œå³å¯å®ç°è‰¯å¥½æ€§èƒ½ã€‚</li>
<li>æ‰©æ•£åˆ†ç±»å™¨å…·æœ‰å†…åœ¨çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>æ‰©æ•£åˆ†ç±»å™¨å¯é‡åŒ–é¢„æµ‹çš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>æ‰©æ•£åˆ†ç±»å™¨çš„å¯ä¿¡åº¦å’Œå¯é æ€§åœ¨åŒ»ç–—ç¯å¢ƒä¸­å°¤ä¸ºé‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-92cda6b36d5bde5a60a272e0b7a16553.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-087249e41ec4d81107ed81dc778583ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61aeffe72a0f6f1a71eb0e2bb2852ef0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-12/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-12/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-12/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-466e0ed5a483944761b9d81efec4b2fe.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-12  Text Embedded Swin-UMamba for DeepLesion Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-12/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-944d853beef8f5d8d6e55cbcdaba8dd0.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-12  Roll Your Eyes Gaze Redirection via Explicit 3D Eyeball Rotation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31686.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
