<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-12  Effective Training Data Synthesis for Improving MLLM Chart Understanding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-0b89eeb7cdff535f3fa984ba1a4592f9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    68 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-12-æ›´æ–°"><a href="#2025-08-12-æ›´æ–°" class="headerlink" title="2025-08-12 æ›´æ–°"></a>2025-08-12 æ›´æ–°</h1><h2 id="Effective-Training-Data-Synthesis-for-Improving-MLLM-Chart-Understanding"><a href="#Effective-Training-Data-Synthesis-for-Improving-MLLM-Chart-Understanding" class="headerlink" title="Effective Training Data Synthesis for Improving MLLM Chart Understanding"></a>Effective Training Data Synthesis for Improving MLLM Chart Understanding</h2><p><strong>Authors:Yuwei Yang, Zeyu Zhang, Yunzhong Hou, Zhuowan Li, Gaowen Liu, Ali Payani, Yuan-Sen Ting, Liang Zheng</strong></p>
<p>Being able to effectively read scientific plots, or chart understanding, is a central part toward building effective agents for science. However, existing multimodal large language models (MLLMs), especially open-source ones, are still falling behind with a typical success rate of 30%-50% on challenging benchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are often restricted by their inadequate similarity to the real charts, which could compromise model training and performance on complex real-world charts. In this study, we show that modularizing chart generation and diversifying visual details improves chart understanding capabilities. In particular, we design a five-step data synthesis pipeline, where we separate data and function creation for single plot generation, condition the generation of later subplots on earlier ones for multi-subplot figures, visually diversify the generated figures, filter out low quality data, and finally generate the question-answer (QA) pairs with GPT-4o. This approach allows us to streamline the generation of fine-tuning datasets and introduce the effective chart dataset (ECD), which contains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring 250+ chart type combinations with high visual complexity. We show that ECD consistently improves the performance of various MLLMs on a range of real-world and synthetic test sets. Code, data and models are available at: <a target="_blank" rel="noopener" href="https://github.com/yuweiyang-anu/ECD">https://github.com/yuweiyang-anu/ECD</a>. </p>
<blockquote>
<p>èƒ½å¤Ÿæœ‰æ•ˆåœ°é˜…è¯»ç§‘å­¦å›¾è¡¨æˆ–ç†è§£å›¾è¡¨æ˜¯æ„å»ºç§‘å­¦ä»£ç†äººçš„æ ¸å¿ƒéƒ¨åˆ†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œå°¤å…¶æ˜¯å¼€æºæ¨¡å‹ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šçš„æˆåŠŸç‡é€šå¸¸åœ¨30%-50%ä¹‹é—´ã€‚ä»¥å‰å…³äºç”¨åˆæˆå›¾è¡¨å¾®è°ƒMLLMsçš„ç ”ç©¶å¾€å¾€å—åˆ°å…¶ä¸çœŸå®å›¾è¡¨ç›¸ä¼¼æ€§ä¸è¶³çš„é™åˆ¶ï¼Œè¿™å¯èƒ½ä¼šå½±å“åˆ°æ¨¡å‹åœ¨å¤æ‚ç°å®å›¾è¡¨çš„è®­ç»ƒå’Œæ€§èƒ½ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†æ¨¡å—åŒ–å›¾è¡¨ç”Ÿæˆå’Œå¤šæ ·åŒ–è§†è§‰ç»†èŠ‚å¯ä»¥æé«˜å›¾è¡¨çš„ç†è§£èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªäº”æ­¥æ•°æ®åˆæˆæµç¨‹ï¼Œå°†æ•°æ®å’Œå•ä¸€å›¾è¡¨çš„åˆ›å»ºåŠŸèƒ½åˆ†å¼€ï¼Œå¯¹å¤šå­å›¾å›¾çš„åç»­å­å›¾ç”Ÿæˆè¿›è¡Œæ¡ä»¶åŒ–ï¼Œè§†è§‰ä¸Šå¤šæ ·åŒ–ç”Ÿæˆçš„å›¾è¡¨ï¼Œè¿‡æ»¤æ‰ä½è´¨é‡æ•°æ®ï¼Œå¹¶æœ€ç»ˆä½¿ç”¨GPT-4oç”Ÿæˆé—®ç­”å¯¹ã€‚è¿™ç§æ–¹æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿç²¾ç®€å¾®è°ƒæ•°æ®é›†çš„äº§ç”Ÿï¼Œå¹¶å¼•å…¥æœ‰æ•ˆçš„å›¾è¡¨æ•°æ®é›†ï¼ˆECDï¼‰ï¼Œå…¶ä¸­åŒ…å«1ä¸‡å¤šä¸ªå›¾è¡¨å›¾åƒå’Œè¶…è¿‡3ä¸‡å¤šä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–25ä¸ªä¸»é¢˜ï¼Œå±•ç¤ºè¶…è¿‡25ç§å›¾è¡¨ç±»å‹ç»„åˆçš„é«˜è§†è§‰å¤æ‚æ€§ã€‚æˆ‘ä»¬è¯æ˜äº†ECDåœ¨å„ç§ç°å®ä¸–ç•Œå’Œåˆæˆæµ‹è¯•é›†ä¸Šéƒ½èƒ½æé«˜å„ç§MLLMçš„æ€§èƒ½ã€‚ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/yuweiyang-anu/ECD%E3%80%82">https://github.com/yuweiyang-anu/ECDã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06492v1">PDF</a> Accepted by ICCV 2025 (poster). 26 pages, 17 figures</p>
<p><strong>Summary</strong><br>å¤§è¦æ¨¡èªè¨€æ¨¡å‹åœ¨åœ–è¡¨ç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨è™•ç†è¤‡é›œçš„ç¾å¯¦ä¸–ç•Œåœ–è¡¨æ™‚ã€‚æœ¬ç ”ç©¶é€šéæ¨¡çµ„åŒ–åœ–è¡¨ç”Ÿæˆå’Œå¢åŠ è¦–è¦ºç´°ç¯€ä¾†æ”¹å–„æ¨¡å‹åœ¨åœ–è¡¨ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶è¨­è¨ˆäº†äº”å€‹æ­¥é©Ÿçš„æ•¸æ“šåˆæˆæµç¨‹ï¼Œä¸¦åˆ©ç”¨GPT-4oç”Ÿæˆå•é¡Œå’Œç­”æ¡ˆå°ã€‚è©²æ–¹æ³•æœ‰åŠ©äºå„ªåŒ–å¾®èª¿æ•¸æ“šé›†çš„ç”Ÿæˆï¼Œä¸¦å¼•å…¥é«˜æ•ˆåœ–è¡¨æ•¸æ“šé›†ï¼ˆECDï¼‰ï¼ŒåŒ…å«10,000å¤šå€‹åœ–è¡¨å½±åƒå’Œ300,000å¤šå€‹å•é¡Œç­”æ¡ˆå°ï¼Œæ¶‰åŠ25å€‹ä¸»é¡Œå’Œ250å¤šç¨®è¤‡é›œçš„åœ–è¡¨é¡å‹çµ„åˆã€‚ECDåœ¨ä¸åŒç¾å¯¦ä¸–ç•Œå’Œåˆæˆæ¸¬è©¦é›†ä¸Šçš„è¡¨ç¾å‡æœ‰æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡å¼å¤§å‹èªè¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åœ–è¡¨ç†è§£æ–¹é¢å­˜åœ¨å±€é™ï¼Œå°¤å…¶åœ¨è™•ç†è¤‡é›œç¾å¯¦ä¸–ç•Œåœ–è¡¨æ™‚ã€‚</li>
<li>æ¨¡çµ„åŒ–åœ–è¡¨ç”Ÿæˆå’Œå¢åŠ è¦–è¦ºç´°ç¯€å¯ä»¥æé«˜æ¨¡å‹åœ¨åœ–è¡¨ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>æœ¬ç ”ç©¶è¨­è¨ˆäº†åŒ…å«äº”å€‹æ­¥é©Ÿçš„æ•¸æ“šåˆæˆæµç¨‹ï¼Œä»¥æé«˜åœ–è¡¨ç”Ÿæˆè³ªé‡ã€‚</li>
<li>åˆ©ç”¨GPT-4oç”Ÿæˆå•é¡Œå’Œç­”æ¡ˆå°ï¼Œä»¥å„ªåŒ–å¾®èª¿æ•¸æ“šé›†ã€‚</li>
<li>å¼•å…¥é«˜æ•ˆåœ–è¡¨æ•¸æ“šé›†ï¼ˆECDï¼‰ï¼ŒåŒ…å«å¤§é‡åœ–è¡¨å½±åƒå’Œå•é¡Œç­”æ¡ˆå°ï¼Œæ¶‰åŠå¤šä¸»é¡Œå’Œè¤‡é›œçš„åœ–è¡¨é¡å‹çµ„åˆã€‚</li>
<li>ECDåœ¨ä¸åŒç¾å¯¦ä¸–ç•Œå’Œåˆæˆæ¸¬è©¦é›†ä¸Šçš„è¡¨ç¾å‡æœ‰é¡¯è‘—æå‡ã€‚</li>
<li>è©²ç ”ç©¶çš„ä»£ç¢¼ã€æ•¸æ“šå’Œæ¨¡å‹å¯åœ¨ç¶²ä¸Šæ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06492">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa74c442de33cfa2d42bd39f03b0c60a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fca6a62b44d9886b648158ca690acd7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d989b60a37e91db676c215e136c4ff3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="GLM-4-5-Agentic-Reasoning-and-Coding-ARC-Foundation-Models"><a href="#GLM-4-5-Agentic-Reasoning-and-Coding-ARC-Foundation-Models" class="headerlink" title="GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models"></a>GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models</h2><p><strong>Authors:GLM-4. 5 Team,  :, Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen Zhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng, Yifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang, Zilin Zhu, Bohan Zhang, Bosi Wen, Bowen Wu, Bowen Xu, Can Huang, Casey Zhao, Changpeng Cai, Chao Yu, Chen Li, Chendi Ge, Chenghua Huang, Chenhui Zhang, Chenxi Xu, Chenzheng Zhu, Chuang Li, Congfeng Yin, Daoyan Lin, Dayong Yang, Dazhi Jiang, Ding Ai, Erle Zhu, Fei Wang, Gengzheng Pan, Guo Wang, Hailong Sun, Haitao Li, Haiyang Li, Haiyi Hu, Hanyu Zhang, Hao Peng, Hao Tai, Haoke Zhang, Haoran Wang, Haoyu Yang, He Liu, He Zhao, Hongwei Liu, Hongxi Yan, Huan Liu, Huilong Chen, Ji Li, Jiajing Zhao, Jiamin Ren, Jian Jiao, Jiani Zhao, Jianyang Yan, Jiaqi Wang, Jiayi Gui, Jiayue Zhao, Jie Liu, Jijie Li, Jing Li, Jing Lu, Jingsen Wang, Jingwei Yuan, Jingxuan Li, Jingzhao Du, Jinhua Du, Jinxin Liu, Junkai Zhi, Junli Gao, Ke Wang, Lekang Yang, Liang Xu, Lin Fan, Lindong Wu, Lintao Ding, Lu Wang, Man Zhang, Minghao Li, Minghuan Xu, Mingming Zhao, Mingshu Zhai, Pengfan Du, Qian Dong, Shangde Lei, Shangqing Tu, Shangtong Yang, Shaoyou Lu, Shijie Li, Shuang Li,  Shuang-Li, Shuxun Yang, Sibo Yi, Tianshu Yu, Wei Tian, Weihan Wang, Wenbo Yu, Weng Lam Tam, Wenjie Liang, Wentao Liu, Xiao Wang, Xiaohan Jia, Xiaotao Gu, Xiaoying Ling, Xin Wang, Xing Fan, Xingru Pan, Xinyuan Zhang, Xinze Zhang, Xiuqing Fu, Xunkai Zhang, Yabo Xu, Yandong Wu, Yida Lu, Yidong Wang, Yilin Zhou, Yiming Pan, Ying Zhang, Yingli Wang, Yingru Li, Yinpei Su, Yipeng Geng, Yitong Zhu, Yongkun Yang, Yuhang Li, Yuhao Wu, Yujiang Li, Yunan Liu, Yunqing Wang, Yuntao Li, Yuxuan Zhang, Zezhen Liu, Zhen Yang, Zhengda Zhou, Zhongpei Qiao, Zhuoer Feng, Zhuorui Liu, Zichen Zhang, Zihan Wang, Zijun Yao, Zikang Wang, Ziqiang Liu, Ziwei Chai, Zixuan Li, Zuodong Zhao, Wenguang Chen, Jidong Zhai, Bin Xu, Minlie Huang, Hongning Wang, Juanzi Li, Yuxiao Dong, Jie Tang</strong></p>
<p>We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at <a target="_blank" rel="noopener" href="https://github.com/zai-org/GLM-4.5">https://github.com/zai-org/GLM-4.5</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºGLM-4.5ï¼Œè¿™æ˜¯ä¸€æ¬¾å¼€æºçš„ä¸“å®¶æ··åˆï¼ˆMoEï¼‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ€»å…±æœ‰355Bä¸ªå‚æ•°ï¼Œæ¿€æ´»å‚æ•°ä¸º32Bï¼Œé‡‡ç”¨æ··åˆæ¨ç†æ–¹æ³•ï¼Œæ”¯æŒæ€è€ƒå’Œç›´æ¥å“åº”ä¸¤ç§æ¨¡å¼ã€‚é€šè¿‡23Tæ ‡è®°çš„å¤šé˜¶æ®µè®­ç»ƒå’Œä»¥ä¸“å®¶æ¨¡å‹è¿­ä»£å’Œå¼ºåŒ–å­¦ä¹ è¿›è¡Œçš„å…¨é¢åè®­ç»ƒï¼ŒGLM-4.5åœ¨ä»£ç†ã€æ¨ç†å’Œç¼–ç ï¼ˆARCï¼‰ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œåœ¨TAU-Benchä¸Šå¾—åˆ†ä¸º70.1%ï¼Œåœ¨AIME 24ä¸Šå¾—åˆ†ä¸º91.0%ï¼Œåœ¨SWE-bench Verifiedä¸Šå¾—åˆ†ä¸º64.2%ã€‚å°½ç®¡å‚æ•°å°‘äºå‡ ä¸ªç«äº‰å¯¹æ‰‹ï¼Œä½†GLM-4.5åœ¨æ‰€æœ‰è¯„ä¼°çš„æ¨¡å‹ä¸­æ’åç¬¬ä¸‰ï¼Œåœ¨ä»£ç†åŸºå‡†æµ‹è¯•ä¸­æ’åç¬¬äºŒã€‚æˆ‘ä»¬å‘å¸ƒGLM-4.5ï¼ˆ355Bå‚æ•°ï¼‰å’Œå…¶ç²¾ç®€ç‰ˆGLM-4.5-Airï¼ˆ106Bå‚æ•°ï¼‰ï¼Œä»¥ä¿ƒè¿›å¯¹æ¨ç†å’Œä»£ç†äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ç ”ç©¶ã€‚ä»£ç ã€æ¨¡å‹å’Œæ›´å¤šä¿¡æ¯å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zai-org/GLM-4.5%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zai-org/GLM-4.5ä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06471v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>GLM-4.5æ˜¯ä¸€æ¬¾æ‹¥æœ‰355Bæ€»å‚æ•°å’Œ32Bæ¿€æ´»å‚æ•°çš„å¼€æºæ··åˆä¸“å®¶å¤§å‹è¯­è¨€æ¨¡å‹ã€‚å®ƒé€šè¿‡æ··åˆæ¨ç†æ–¹æ³•æ”¯æŒæ€è€ƒå’Œç›´æ¥å“åº”æ¨¡å¼ï¼Œåœ¨TAU-Benchå¾—åˆ†70.1%ï¼ŒAIME 24å¾—åˆ†91.0%ï¼ŒSWE-bench Verifiedå¾—åˆ†64.2%ã€‚åœ¨å¤šé˜¶æ®µè®­ç»ƒå’Œå…¨é¢çš„è®­ç»ƒåï¼Œå…¶æ€§èƒ½å¼ºåŠ²ï¼Œæ’åç¬¬ä¸‰ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†ç²¾ç®€ç‰ˆGLM-4.5-Airï¼ˆå‚æ•°ä¸º106Bï¼‰ã€‚è¯¦æƒ…å¯è®¿é—®ç›¸å…³GitHubé“¾æ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GLM-4.5æ˜¯ä¸€æ¬¾å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨æ··åˆä¸“å®¶ç»“æ„ã€‚</li>
<li>å®ƒæ‹¥æœ‰ä¸¤ç§æ¨¡å¼ï¼šæ€è€ƒæ¨¡å¼å’Œç›´æ¥å“åº”æ¨¡å¼ã€‚</li>
<li>GLM-4.5åœ¨å¤šé˜¶æ®µè®­ç»ƒåè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>GLM-4.5åœ¨TAU-Benchã€AIME 24å’ŒSWE-bench Verifiedæµ‹è¯•ä¸­åˆ†åˆ«å–å¾—äº†è‰¯å¥½çš„æˆç»©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06471">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1944867326eeaabd7d81a352f6cfae56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a407c3a3daea14fb7a45b779106a6b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e8d62789986051bc0b00b2c09d86a32.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1b0c84a58ef6965c1995d3af65a64ac.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ScamAgents-How-AI-Agents-Can-Simulate-Human-Level-Scam-Calls"><a href="#ScamAgents-How-AI-Agents-Can-Simulate-Human-Level-Scam-Calls" class="headerlink" title="ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls"></a>ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls</h2><p><strong>Authors:Sanket Badhe</strong></p>
<p>Large Language Models (LLMs) have demonstrated impressive fluency and reasoning capabilities, but their potential for misuse has raised growing concern. In this paper, we present ScamAgent, an autonomous multi-turn agent built on top of LLMs, capable of generating highly realistic scam call scripts that simulate real-world fraud scenarios. Unlike prior work focused on single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts dynamically to simulated user responses, and employs deceptive persuasion strategies across conversational turns. We show that current LLM safety guardrails, including refusal mechanisms and content filters, are ineffective against such agent-based threats. Even models with strong prompt-level safeguards can be bypassed when prompts are decomposed, disguised, or delivered incrementally within an agent framework. We further demonstrate the transformation of scam scripts into lifelike voice calls using modern text-to-speech systems, completing a fully automated scam pipeline. Our findings highlight an urgent need for multi-turn safety auditing, agent-level control frameworks, and new methods to detect and disrupt conversational deception powered by generative AI. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æµç•…æ€§å’Œæ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬å¯èƒ½è¢«æ»¥ç”¨çš„æ½œåŠ›ä¹Ÿå¼•å‘äº†æ—¥ç›Šå¢é•¿çš„æ‹…å¿§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ScamAgentï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLMæ„å»ºçš„è‡ªä¸»å¤šè½®å¯¹è¯ä»£ç†ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜åº¦é€¼çœŸçš„è¯ˆéª—ç”µè¯è„šæœ¬ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„æ¬ºè¯ˆåœºæ™¯ã€‚ä¸ä»¥å¾€ä¸“æ³¨äºå•è½®æç¤ºæ»¥ç”¨çš„å·¥ä½œä¸åŒï¼ŒScamAgentä¿ç•™äº†å¯¹è¯è®°å¿†ï¼Œèƒ½å¤ŸåŠ¨æ€é€‚åº”æ¨¡æ‹Ÿç”¨æˆ·å“åº”ï¼Œå¹¶åœ¨å¯¹è¯è¿‡ç¨‹ä¸­é‡‡ç”¨æ¬ºéª—æ€§çš„è¯´æœç­–ç•¥ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå½“å‰çš„LLMå®‰å…¨æŠ¤æ ï¼ŒåŒ…æ‹¬æ‹’ç»æœºåˆ¶å’Œå†…å®¹è¿‡æ»¤å™¨ï¼Œå¯¹äºè¿™ç§åŸºäºä»£ç†çš„å¨èƒéƒ½æ˜¯æ— æ•ˆçš„ã€‚å³ä½¿åœ¨æ¨¡å‹ä¸­å…·æœ‰å¼ºå¤§çš„æç¤ºçº§ä¿éšœæªæ–½ï¼Œå½“æç¤ºè¢«åˆ†è§£ã€ä¼ªè£…æˆ–åœ¨ä»£ç†æ¡†æ¶å†…é€æ­¥ä¼ é€’æ—¶ï¼Œä¹Ÿå¯ä»¥ç»•è¿‡è¿™äº›ä¿éšœæªæ–½ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¼”ç¤ºäº†ä½¿ç”¨ç°ä»£æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿå°†è¯ˆéª—è„šæœ¬è½¬æ¢ä¸ºé€¼çœŸçš„è¯­éŸ³é€šè¯ï¼Œå®Œæˆäº†å…¨è‡ªåŠ¨è¯ˆéª—æµç¨‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†å¤šè½®å®‰å…¨å®¡è®¡ã€ä»£ç†çº§æ§åˆ¶æ¡†æ¶ä»¥åŠæ£€æµ‹å’Œç ´åç”±ç”Ÿæˆå¼äººå·¥æ™ºèƒ½é©±åŠ¨çš„å¯¹è¯æ¬ºè¯ˆçš„æ–°æ–¹æ³•çš„è¿«åˆ‡éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06457v1">PDF</a> Accepted at CAMLIS 25: Conference on Applied Machine Learning for   Information Security. 10 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°å‡ºå¼ºå¤§çš„æµç•…åº¦å’Œæ¨ç†èƒ½åŠ›ï¼Œä½†å…¶æ½œåœ¨æ»¥ç”¨é£é™©å¼•å‘å…³æ³¨ã€‚æœ¬ç ”ç©¶æå‡ºScamAgentï¼Œä¸€æ¬¾åŸºäºLLMsæ„å»ºçš„å¤šè½®è‡ªä¸»ä»£ç†ï¼Œå¯ç”Ÿæˆé«˜åº¦é€¼çœŸçš„è¯ˆéª—ç”µè¯è„šæœ¬ï¼Œæ¨¡æ‹ŸçœŸå®æ¬ºè¯ˆåœºæ™¯ã€‚ScamAgentèƒ½ç»´æŒå¯¹è¯è®°å¿†ï¼ŒåŠ¨æ€é€‚åº”æ¨¡æ‹Ÿç”¨æˆ·å›åº”ï¼Œå¹¶é‡‡ç”¨æ¬ºéª—æ€§è¯´æœç­–ç•¥è¿›è¡Œå¤šè½®å¯¹è¯ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œç°æœ‰çš„LLMå®‰å…¨é˜²æŠ¤æªæ–½ï¼Œå¦‚æ‹’ç»æœºåˆ¶å’Œå†…å®¹è¿‡æ»¤å™¨ï¼Œå¯¹è¿™ç§åŸºäºä»£ç†çš„å¨èƒæ— æ•ˆã€‚å³ä½¿åœ¨å¼ºå¤§çš„æç¤ºå±‚æ¬¡ä¿æŠ¤æªæ–½ä¹Ÿå¯èƒ½åœ¨ä»£ç†æ¡†æ¶ä¸‹è¢«ç»•è¿‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶å±•ç¤ºäº†åˆ©ç”¨ç°ä»£è¯­éŸ³è¯†åˆ«ç³»ç»Ÿå°†è¯ˆéª—è„šæœ¬è½¬æ¢ä¸ºé€¼çœŸè¯­éŸ³é€šè¯çš„è¿‡ç¨‹ï¼Œå®ç°è‡ªåŠ¨åŒ–è¯ˆéª—æµæ°´çº¿ã€‚å› æ­¤è¿«åˆ‡éœ€è¦å¼€å‘å¤šè½®å®‰å…¨é˜²æŠ¤æªæ–½å’Œç”ŸæˆAIæ§åˆ¶æ¡†æ¶ï¼Œä»¥æ£€æµ‹å¹¶é˜»æ­¢å¯¹è¯æ¬ºè¯ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså±•ç°å‡ºå¼ºå¤§çš„è¯­è¨€å¤„ç†èƒ½åŠ›ï¼Œä½†å­˜åœ¨æ»¥ç”¨é£é™©ã€‚</li>
<li>ScamAgentæ˜¯åŸºäºLLMsçš„å¤šè½®å¯¹è¯ä»£ç†ï¼Œèƒ½ç”Ÿæˆé€¼çœŸçš„è¯ˆéª—ç”µè¯è„šæœ¬ã€‚</li>
<li>ScamAgentèƒ½ç»´æŒå¯¹è¯è®°å¿†å¹¶åŠ¨æ€é€‚åº”æ¨¡æ‹Ÿç”¨æˆ·å›åº”ã€‚</li>
<li>å½“å‰LLMçš„å®‰å…¨é˜²æŠ¤æªæ–½å¯¹åŸºäºä»£ç†çš„å¨èƒæ— æ•ˆã€‚</li>
<li>ç”Ÿæˆå¼AIå¯èƒ½ä¼šè¢«ç”¨äºæ„å»ºè‡ªåŠ¨åŒ–è¯ˆéª—æµæ°´çº¿ã€‚</li>
<li>éœ€è¦å¼€å‘å¤šè½®å®‰å…¨é˜²æŠ¤æªæ–½å’Œç”ŸæˆAIæ§åˆ¶æ¡†æ¶æ¥åº”å¯¹æ½œåœ¨å¨èƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06457">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b80b629bae4a38384faf92a457faa752.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07a6123d27751a4cee5ebba3d730848d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6292257a658eb6f3d8e94699632eaf6c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Text-Embedded-Swin-UMamba-for-DeepLesion-Segmentation"><a href="#Text-Embedded-Swin-UMamba-for-DeepLesion-Segmentation" class="headerlink" title="Text Embedded Swin-UMamba for DeepLesion Segmentation"></a>Text Embedded Swin-UMamba for DeepLesion Segmentation</h2><p><strong>Authors:Ruida Cheng, Tejas Sudharshan Mathai, Pritam Mukherjee, Benjamin Hou, Qingqing Zhu, Zhiyong Lu, Matthew McAuliffe, Ronald M. Summers</strong></p>
<p>Segmentation of lesions on CT enables automatic measurement for clinical assessment of chronic diseases (e.g., lymphoma). Integrating large language models (LLMs) into the lesion segmentation workflow offers the potential to combine imaging features with descriptions of lesion characteristics from the radiology reports. In this study, we investigate the feasibility of integrating text into the Swin-UMamba architecture for the task of lesion segmentation. The publicly available ULS23 DeepLesion dataset was used along with short-form descriptions of the findings from the reports. On the test dataset, a high Dice Score of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for lesion segmentation. The proposed Text-Swin-UMamba model outperformed prior approaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p &lt; 0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by 1.74% and 0.22%, respectively. The dataset and code can be accessed at <a target="_blank" rel="noopener" href="https://github.com/ruida/LLM-Swin-UMamba">https://github.com/ruida/LLM-Swin-UMamba</a> </p>
<blockquote>
<p>å¯¹CTä¸Šçš„ç—…ç¶è¿›è¡Œåˆ†å‰²ï¼Œå¯å®ç°æ…¢æ€§ç—…å˜ï¼ˆå¦‚æ·‹å·´ç˜¤ï¼‰çš„ä¸´åºŠè¯„ä¼°çš„è‡ªåŠ¨æµ‹é‡ã€‚å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é›†æˆåˆ°ç—…ç¶åˆ†å‰²å·¥ä½œæµç¨‹ä¸­ï¼Œæœ‰å¯èƒ½å°†å›¾åƒç‰¹å¾ä¸æ”¾å°„å­¦æŠ¥å‘Šä¸­å…³äºç—…ç¶ç‰¹å¾çš„æè¿°ç»“åˆèµ·æ¥ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å°†æ–‡æœ¬é›†æˆåˆ°Swin-UMambaæ¶æ„ä¸­è¿›è¡Œç—…ç¶åˆ†å‰²ä»»åŠ¡çš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨äº†å…¬å¼€å¯ç”¨çš„ULS23 DeepLesionæ•°æ®é›†ä»¥åŠæŠ¥å‘Šä¸­å‘ç°ç»“æœçš„ç®€çŸ­æè¿°ã€‚åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šï¼Œç—…ç¶åˆ†å‰²çš„Diceåˆ†æ•°é«˜è¾¾82%ï¼ŒHausdorffè·ç¦»ä½è‡³6.58ï¼ˆåƒç´ ï¼‰ã€‚æ‰€æå‡ºçš„Text-Swin-UMambaæ¨¡å‹åœ¨å…ˆå‰çš„æ–¹æ³•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼šç›¸è¾ƒäºLLMé©±åŠ¨çš„LanGuideMedSegæ¨¡å‹ï¼Œæ€§èƒ½æé«˜äº†37%ï¼ˆp &lt; 0.001ï¼‰ï¼Œå¹¶ä¸”æ¯”çº¯å›¾åƒåŸºçš„xLSTM-UNetå’ŒnnUNetæ¨¡å‹åˆ†åˆ«é«˜å‡º1.74%å’Œ0.22%ã€‚æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/ruida/LLM-Swin-UMamba%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/ruida/LLM-Swin-UMambaè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06453v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨CTå½±åƒä¸Šè¿›è¡Œç—…å˜åˆ†å‰²ï¼Œå¯è‡ªåŠ¨æµ‹é‡ç”¨äºæ…¢æ€§ç–¾ç—…çš„ä¸´åºŠè¯„ä¼°ï¼ˆå¦‚æ·‹å·´ç˜¤ï¼‰ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†å°†æ–‡æœ¬æ•´åˆåˆ°Swin-UMambaæ¶æ„ä¸­è¿›è¡Œç—…å˜åˆ†å‰²çš„å¯è¡Œæ€§ï¼Œç»“åˆäº†å½±åƒç‰¹å¾ä¸æ¥è‡ªæ”¾å°„æŠ¥å‘Šçš„ç—…å˜ç‰¹å¾æè¿°ã€‚ä½¿ç”¨å…¬å¼€çš„ULS23 DeepLesionæ•°æ®é›†å’ŒæŠ¥å‘Šçš„ç®€çŸ­æè¿°å‘ç°ï¼Œæµ‹è¯•æ•°æ®é›†ä¸Šç—…å˜åˆ†å‰²çš„Diceç³»æ•°é«˜è¾¾82%ï¼ŒHausdorffè·ç¦»ä½è‡³6.58åƒç´ ã€‚æå‡ºçš„Text-Swin-UMambaæ¨¡å‹è¾ƒå…ˆå‰çš„LLMé©±åŠ¨æ¨¡å‹LanGuideMedSegé«˜å‡º37%ï¼ˆp &lt; 0.001ï¼‰ï¼Œå¹¶ä¸”ç›¸è¾ƒäºçº¯å›¾åƒåŸºç¡€çš„xLSTM-UNetå’ŒnnUNetæ¨¡å‹åˆ†åˆ«é«˜å‡º1.74%å’Œ0.22%ã€‚æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡é“¾æ¥è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/ruida/LLM-Swin-UMamba">https://github.com/ruida/LLM-Swin-UMamba</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç—…å˜åˆ†å‰²åœ¨CTå½±åƒä¸Šçš„è‡ªåŠ¨æµ‹é‡å¯ç”¨äºè¯„ä¼°æ…¢æ€§ç–¾ç—…ã€‚</li>
<li>æ•´åˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ‰åŠ©äºç»“åˆå½±åƒç‰¹å¾ä¸æ”¾å°„æŠ¥å‘Šçš„ç—…å˜ç‰¹å¾æè¿°ã€‚</li>
<li>ä½¿ç”¨ULS23 DeepLesionæ•°æ®é›†å’Œç®€çŸ­æŠ¥å‘Šæè¿°è¿›è¡Œç ”ç©¶ã€‚</li>
<li>Text-Swin-UMambaæ¨¡å‹å®ç°äº†è¾ƒé«˜çš„ç—…å˜åˆ†å‰²æ€§èƒ½ï¼ŒDiceç³»æ•°è¾¾82%ï¼ŒHausdorffè·ç¦»ä½è‡³6.58åƒç´ ã€‚</li>
<li>ä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼ŒText-Swin-UMambaæ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸LLMé©±åŠ¨çš„LanGuideMedSegæ¨¡å‹çš„æ¯”è¾ƒä¸­ã€‚</li>
<li>æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡GitHubå…¬å¼€è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06453">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f5e398d5269d1d8c0df971eea179d700.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-466e0ed5a483944761b9d81efec4b2fe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-63c5f59fc4977dc49644396d7979d19f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-241c7ce5e6bec6151a67d38217501d76.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27dd43665e2ea89d83728baf444a8244.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Quantifying-Conversation-Drift-in-MCP-via-Latent-Polytope"><a href="#Quantifying-Conversation-Drift-in-MCP-via-Latent-Polytope" class="headerlink" title="Quantifying Conversation Drift in MCP via Latent Polytope"></a>Quantifying Conversation Drift in MCP via Latent Polytope</h2><p><strong>Authors:Haoran Shi, Hongwei Yao, Shuo Shao, Shaopeng Jiao, Ziqi Peng, Zhan Qin, Cong Wang</strong></p>
<p>The Model Context Protocol (MCP) enhances large language models (LLMs) by integrating external tools, enabling dynamic aggregation of real-time data to improve task execution. However, its non-isolated execution context introduces critical security and privacy risks. In particular, adversarially crafted content can induce tool poisoning or indirect prompt injection, leading to conversation hijacking, misinformation propagation, or data exfiltration. Existing defenses, such as rule-based filters or LLM-driven detection, remain inadequate due to their reliance on static signatures, computational inefficiency, and inability to quantify conversational hijacking. To address these limitations, we propose SecMCP, a secure framework that detects and quantifies conversation drift, deviations in latent space trajectories induced by adversarial external knowledge. By modeling LLM activation vectors within a latent polytope space, SecMCP identifies anomalous shifts in conversational dynamics, enabling proactive detection of hijacking, misleading, and data exfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3, Vicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA), demonstrating robust detection with AUROC scores exceeding 0.915 while maintaining system usability. Our contributions include a systematic categorization of MCP security threats, a novel latent polytope-based methodology for quantifying conversation drift, and empirical validation of SecMCPâ€™s efficacy. </p>
<blockquote>
<p>æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰é€šè¿‡æ•´åˆå¤–éƒ¨å·¥å…·ï¼Œå®ç°å®æ—¶æ•°æ®çš„åŠ¨æ€èšåˆï¼Œä»è€Œæå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»»åŠ¡æ‰§è¡Œæ•ˆèƒ½ã€‚ç„¶è€Œï¼Œå…¶ééš”ç¦»çš„æ‰§è¡Œä¸Šä¸‹æ–‡å¼•å…¥äº†å…³é”®çš„å®‰å…¨å’Œéšç§é£é™©ã€‚ç‰¹åˆ«æ˜¯ï¼Œå¯¹æŠ—æ€§æ„é€ çš„å†…å®¹å¯èƒ½ä¼šå¯¼è‡´å·¥å…·ä¸­æ¯’æˆ–é—´æ¥æç¤ºæ³¨å…¥ï¼Œä»è€Œå¯¼è‡´å¯¹è¯åŠ«æŒã€é”™è¯¯ä¿¡æ¯ä¼ æ’­æˆ–æ•°æ®æ³„éœ²ã€‚ç°æœ‰çš„é˜²å¾¡æ‰‹æ®µï¼Œå¦‚åŸºäºè§„åˆ™çš„è¿‡æ»¤å™¨æˆ–LLMé©±åŠ¨çš„æ£€æµ‹ï¼Œä»ç„¶ä¸è¶³ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºé™æ€ç­¾åã€è®¡ç®—æ•ˆç‡ä½ä¸‹ï¼Œå¹¶ä¸”æ— æ³•é‡åŒ–å¯¹è¯åŠ«æŒã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†SecMCPï¼Œä¸€ä¸ªå®‰å…¨æ¡†æ¶ï¼Œå¯ä»¥æ£€æµ‹å’Œé‡åŒ–å¯¹è¯æ¼‚ç§»ï¼Œå³ç”±å¯¹æŠ—æ€§å¤–éƒ¨çŸ¥è¯†å¼•èµ·çš„æ½œåœ¨ç©ºé—´è½¨è¿¹çš„åå·®ã€‚é€šè¿‡åœ¨æ½œåœ¨å¤šé¢ä½“ç©ºé—´å†…å»ºæ¨¡LLMæ¿€æ´»å‘é‡ï¼ŒSecMCPèƒ½å¤Ÿè¯†åˆ«ä¼šè¯åŠ¨æ€çš„å¼‚å¸¸å˜åŒ–ï¼Œä»è€Œä¸»åŠ¨æ£€æµ‹åŠ«æŒã€è¯¯å¯¼å’Œæ•°æ®æ³„éœ²ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLlama3ã€Vicunaã€Mistralï¼‰ä¸Šè¯„ä¼°äº†SecMCPçš„æ€§èƒ½ï¼Œè¿™äº›æ¨¡å‹åœ¨åŸºå‡†æ•°æ®é›†ï¼ˆMS MARCOã€HotpotQAã€FinQAï¼‰ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ£€æµ‹èƒ½åŠ›ï¼ŒAUROCå¾—åˆ†è¶…è¿‡0.915ï¼ŒåŒæ—¶ä¿æŒäº†ç³»ç»Ÿçš„å¯ç”¨æ€§ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬å¯¹MCPå®‰å…¨å¨èƒçš„ç³»ç»Ÿåˆ†ç±»ã€ä¸€ç§åŸºäºæ½œåœ¨å¤šé¢ä½“çš„é‡åŒ–å¯¹è¯æ¼‚ç§»çš„æ–°æ–¹æ³•ï¼Œä»¥åŠSecMCPæœ‰æ•ˆæ€§çš„å®è¯éªŒè¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06418v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰é€šè¿‡æ•´åˆå¤–éƒ¨å·¥å…·å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠŸèƒ½ï¼Œå®ç°å®æ—¶æ•°æ®çš„åŠ¨æ€èšåˆï¼Œæé«˜ä»»åŠ¡æ‰§è¡Œæ•ˆç‡ã€‚ä½†å…¶ééš”ç¦»çš„æ‰§è¡Œä¸Šä¸‹æ–‡å¼•å…¥äº†å…³é”®çš„å®‰å…¨å’Œéšç§é£é™©ã€‚ç‰¹åˆ«æ˜¯æ•Œå¯¹åŠ¿åŠ›åˆ¶é€ çš„æ¶æ„å†…å®¹å¯èƒ½å¯¼è‡´å·¥å…·ä¸­æ¯’æˆ–é—´æ¥æç¤ºæ³¨å…¥ï¼Œä»è€Œå¯¼è‡´å¯¹è¯åŠ«æŒã€è™šå‡ä¿¡æ¯ä¼ æ’­æˆ–æ•°æ®æ³„éœ²ã€‚æˆ‘ä»¬æå‡ºSecMCPï¼Œä¸€ä¸ªå®‰å…¨æ¡†æ¶ï¼Œç”¨äºæ£€æµ‹å’Œé‡åŒ–å¯¹è¯æ¼‚ç§»ï¼Œå³å¯¹ç”±å¯¹æŠ—æ€§å¤–éƒ¨çŸ¥è¯†å¼•èµ·çš„æ½œåœ¨ç©ºé—´è½¨è¿¹çš„åå·®ã€‚é€šè¿‡æ¨¡æ‹ŸLLMæ¿€æ´»å‘é‡åœ¨æ½œåœ¨å¤šé¢ä½“ç©ºé—´å†…ï¼ŒSecMCPè¯†åˆ«å‡ºä¼šè¯åŠ¨æ€çš„å¼‚å¸¸å˜åŒ–ï¼Œèƒ½å¤Ÿä¸»åŠ¨æ£€æµ‹åŠ«æŒã€è¯¯å¯¼å’Œæ•°æ®æ³„éœ²ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªæœ€å…ˆè¿›çš„LLMä¸Šè¯„ä¼°äº†SecMCPï¼Œç»“æœè¡¨æ˜å…¶åœ¨ä¿æŒç³»ç»Ÿå¯ç”¨æ€§çš„åŒæ—¶ï¼Œå…·æœ‰ç¨³å¥çš„æ£€æµ‹èƒ½åŠ›ï¼ŒAUROCå¾—åˆ†è¶…è¿‡0.915ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MCPé€šè¿‡æ•´åˆå¤–éƒ¨å·¥å…·å¢å¼ºLLMçš„åŠŸèƒ½ï¼Œä½†ééš”ç¦»çš„æ‰§è¡Œä¸Šä¸‹æ–‡å­˜åœ¨å®‰å…¨å’Œéšç§é£é™©ã€‚</li>
<li>å¯¹æŠ—æ€§å†…å®¹å¯èƒ½å¯¼è‡´å·¥å…·ä¸­æ¯’æˆ–æç¤ºæ³¨å…¥ï¼Œå¼•å‘å¯¹è¯åŠ«æŒã€è™šå‡ä¿¡æ¯ä¼ æ’­å’Œæ•°æ®æ³„éœ²ç­‰é£é™©ã€‚</li>
<li>ç°æœ‰çš„é˜²å¾¡æ‰‹æ®µï¼Œå¦‚åŸºäºè§„åˆ™çš„è¿‡æ»¤å™¨æˆ–LLMé©±åŠ¨çš„æ£€æµ‹ï¼Œç”±äºä¾èµ–é™æ€ç­¾åã€è®¡ç®—æ•ˆç‡ä½ä¸‹å’Œæ— æ³•é‡åŒ–å¯¹è¯åŠ«æŒç­‰å±€é™æ€§ï¼Œæ˜¾å¾—æ‰è¥Ÿè§è‚˜ã€‚</li>
<li>SecMCPæ˜¯ä¸€ä¸ªå®‰å…¨æ¡†æ¶ï¼Œèƒ½æ£€æµ‹å’Œé‡åŒ–ç”±å¯¹æŠ—æ€§å¤–éƒ¨çŸ¥è¯†å¼•èµ·çš„å¯¹è¯æ¼‚ç§»ã€‚</li>
<li>SecMCPé€šè¿‡æ¨¡æ‹ŸLLMæ¿€æ´»å‘é‡åœ¨æ½œåœ¨å¤šé¢ä½“ç©ºé—´å†…æ¥è¯†åˆ«å¼‚å¸¸çš„ä¼šè¯åŠ¨æ€å˜åŒ–ã€‚</li>
<li>SecMCPèƒ½ä¸»åŠ¨æ£€æµ‹å¯¹è¯åŠ«æŒã€è¯¯å¯¼å’Œæ•°æ®æ³„éœ²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06418">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f23c91c344aac1ee1df2777f2ace2bc4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f34b4427d831df3e1d6c70fc0ac1a89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-add305b5dc4913d0f3c58a5d35c8313e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb9beacb98f85e290362c07d169d6f13.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LLMs-vs-Chinese-Anime-Enthusiasts-A-Comparative-Study-on-Emotionally-Supportive-Role-Playing"><a href="#LLMs-vs-Chinese-Anime-Enthusiasts-A-Comparative-Study-on-Emotionally-Supportive-Role-Playing" class="headerlink" title="LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally   Supportive Role-Playing"></a>LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally   Supportive Role-Playing</h2><p><strong>Authors:Lanlan Qiu, Xiao Pu, Yeqi Feng, Tianxing He</strong></p>
<p>Large Language Models (LLMs) have demonstrated impressive capabilities in role-playing conversations and providing emotional support as separate research directions. However, there remains a significant research gap in combining these capabilities to enable emotionally supportive interactions with virtual characters. To address this research gap, we focus on anime characters as a case study because of their well-defined personalities and large fan bases. This choice enables us to effectively evaluate how well LLMs can provide emotional support while maintaining specific character traits. We introduce ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We first thoughtfully select 20 top-tier characters from popular anime communities and design 60 emotion-centric real-world scenario questions. Then, we execute a nationwide selection process to identify 40 Chinese anime enthusiasts with profound knowledge of specific characters and extensive experience in role-playing. Next, we systematically collect two rounds of dialogue data from 10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP performance of LLMs, we design a user experience-oriented evaluation system featuring 9 fine-grained metrics across three dimensions: basic dialogue, role-playing and emotional support, along with an overall metric for response diversity. In total, the dataset comprises 2,400 human-written and 24,000 LLM-generated answers, supported by over 132,000 human annotations. Experimental results show that top-performing LLMs surpass human fans in role-playing and emotional support, while humans still lead in response diversity. We hope this work can provide valuable resources and insights for future research on optimizing LLMs in ESRP. Our datasets are available at <a target="_blank" rel="noopener" href="https://github.com/LanlanQiu/ChatAnime">https://github.com/LanlanQiu/ChatAnime</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§’è‰²æ‰®æ¼”å¯¹è¯å’Œæä¾›æƒ…æ„Ÿæ”¯æŒæ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œè¿™äº›èƒ½åŠ›ä½œä¸ºç‹¬ç«‹çš„ç ”ç©¶æ–¹å‘å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œå¦‚ä½•å°†è¿™ä¸¤ç§èƒ½åŠ›ç»“åˆèµ·æ¥ä»¥å®ç°ä¸è™šæ‹Ÿè§’è‰²çš„æƒ…æ„Ÿæ”¯æŒäº¤äº’ä»ç„¶å­˜åœ¨å¾ˆå¤§çš„ç ”ç©¶ç©ºç™½ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œæˆ‘ä»¬ä»¥åŠ¨æ¼«è§’è‰²ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œå› ä¸ºå®ƒä»¬çš„ä¸ªæ€§ç‰¹å¾æ˜ç¡®ï¼Œå¹¶ä¸”æ‹¥æœ‰å¤§é‡çš„ç²‰ä¸ç¾¤ä½“ã€‚è¿™ç§é€‰æ‹©ä½¿æˆ‘ä»¬èƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°LLMåœ¨ä¿æŒç‰¹å®šè§’è‰²ç‰¹å¾çš„åŒæ—¶æä¾›æƒ…æ„Ÿæ”¯æŒçš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä»‹ç»äº†ChatAnimeï¼Œè¿™æ˜¯é¦–ä¸ªæƒ…æ„Ÿæ”¯æŒè§’è‰²æ‰®æ¼”ï¼ˆESRPï¼‰æ•°æ®é›†ã€‚æˆ‘ä»¬é¦–å…ˆä»æµè¡Œçš„åŠ¨æ¼«ç¤¾åŒºç²¾å¿ƒæŒ‘é€‰äº†20ä¸ªé¡¶çº§è§’è‰²ï¼Œå¹¶è®¾è®¡äº†60ä¸ªä»¥æƒ…æ„Ÿä¸ºä¸­å¿ƒçš„ç°å®åœºæ™¯é—®é¢˜ã€‚ç„¶åï¼Œæˆ‘ä»¬æ‰§è¡Œäº†ä¸€é¡¹å…¨å›½èŒƒå›´å†…çš„é€‰æ‹”è¿‡ç¨‹ï¼Œç¡®å®šäº†40åå¯¹ç‰¹å®šè§’è‰²æœ‰æ·±åšçŸ¥è¯†ã€ä¸°å¯Œè§’è‰²æ‰®æ¼”ç»éªŒçš„ä¸­æ–‡åŠ¨æ¼«çˆ±å¥½è€…ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ä»10ä¸ªLLMå’Œè¿™40åä¸­æ–‡åŠ¨æ¼«çˆ±å¥½è€…æ”¶é›†äº†ä¸¤è½®å¯¹è¯æ•°æ®ã€‚ä¸ºäº†è¯„ä¼°LLMçš„ESRPæ€§èƒ½ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä»¥ç”¨æˆ·ä½“éªŒä¸ºå¯¼å‘çš„è¯„ä»·ç³»ç»Ÿï¼ŒåŒ…æ‹¬9ä¸ªç²¾ç»†åº¦é‡çš„æŒ‡æ ‡ï¼Œæ¶µç›–åŸºæœ¬å¯¹è¯ã€è§’è‰²æ‰®æ¼”å’Œæƒ…æ„Ÿæ”¯æŒä¸‰ä¸ªç»´åº¦ï¼Œä»¥åŠä¸€ä¸ªç»¼åˆå“åº”å¤šæ ·æ€§çš„æŒ‡æ ‡ã€‚æ€»å…±åŒ…æ‹¬äººç±»ç¼–å†™çš„2400ä¸ªç­”æ¡ˆå’Œç”±LLMç”Ÿæˆçš„24ä¸‡ä¸ªç­”æ¡ˆï¼Œå¾—åˆ°äº†è¶…è¿‡13ä¸‡2åƒä¸ªäººç±»æ³¨é‡Šçš„æ”¯æŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¡¨ç°æœ€å¥½çš„LLMåœ¨è§’è‰²æ‰®æ¼”å’Œæƒ…æ„Ÿæ”¯æŒæ–¹é¢è¶…è¶Šäº†äººç±»ç²‰ä¸ï¼Œè€Œäººç±»åœ¨å“åº”å¤šæ ·æ€§æ–¹é¢ä»å é¢†å…ˆåœ°ä½ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½ä¸ºæœªæ¥å…³äºä¼˜åŒ–LLMåœ¨ESRPæ–¹é¢çš„ç ”ç©¶çš„æä¾›æœ‰ä»·å€¼çš„èµ„æºå’Œè§è§£ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LanlanQiu/ChatAnime">https://github.com/LanlanQiu/ChatAnime</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06388v1">PDF</a> 21 pages, 17 figures, 3 tables</p>
<p><strong>Summary</strong>ï¼š</p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§’è‰²æ‰®æ¼”å’Œæƒ…æ„Ÿæ”¯æŒæ–¹é¢å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ½œåŠ›ï¼Œä½†åœ¨å°†ä¸¤è€…ç»“åˆä»¥å®ç°ä¸è™šæ‹Ÿè§’è‰²çš„æƒ…æ„Ÿæ”¯æŒäº¤äº’æ–¹é¢ä»å­˜åœ¨ç ”ç©¶ç©ºç™½ã€‚æœ¬ç ”ç©¶ä»¥åŠ¨æ¼«è§’è‰²ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œå»ºç«‹äº†é¦–ä¸ªæƒ…æ„Ÿæ”¯æŒè§’è‰²æ‰®æ¼”ï¼ˆESRPï¼‰æ•°æ®é›†ChatAnimeã€‚é€šè¿‡ç²¾å¿ƒæŒ‘é€‰æµè¡ŒåŠ¨æ¼«ç¤¾åŒºçš„20ä¸ªé¡¶å°–è§’è‰²ï¼Œè®¾è®¡60ä¸ªæƒ…æ„Ÿä¸­å¿ƒåŒ–çš„ç°å®åœºæ™¯é—®é¢˜ï¼Œå¹¶æ”¶é›†ä¸¤è½®å¯¹è¯æ•°æ®ï¼Œè¯„ä¼°LLMåœ¨ESRPæ–¹é¢çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé¡¶å°–LLMåœ¨è§’è‰²æ‰®æ¼”å’Œæƒ…æ„Ÿæ”¯æŒæ–¹é¢è¶…è¶Šäº†äººç±»ç²‰ä¸ï¼Œè€Œäººç±»åœ¨å“åº”å¤šæ ·æ€§æ–¹é¢ä»é¢†å…ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMåœ¨è§’è‰²æ‰®æ¼”å’Œæƒ…æ„Ÿæ”¯æŒæ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œä½†ç»“åˆè¿™ä¸¤æ–¹é¢çš„ç ”ç©¶ä»å­˜åœ¨ç©ºç™½ã€‚</li>
<li>ChatAnimeæ•°æ®é›†ç”¨äºè¯„ä¼°LLMåœ¨æƒ…æ„Ÿæ”¯æŒè§’è‰²æ‰®æ¼”ï¼ˆESRPï¼‰æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>æ•°æ®é›†é€šè¿‡ç²¾å¿ƒæŒ‘é€‰çš„åŠ¨æ¼«è§’è‰²å’Œè®¾è®¡çš„æƒ…æ„Ÿåœºæ™¯é—®é¢˜æ¥æ”¶é›†å¯¹è¯æ•°æ®ã€‚</li>
<li>è¯„ä¼°ç³»ç»ŸåŒ…æ‹¬åŸºæœ¬å¯¹è¯ã€è§’è‰²æ‰®æ¼”å’Œæƒ…æ„Ÿæ”¯æŒä¸‰ä¸ªç»´åº¦çš„9ä¸ªç²¾ç»†æŒ‡æ ‡ä»¥åŠæ•´ä½“å“åº”å¤šæ ·æ€§æŒ‡æ ‡ã€‚</li>
<li>æ•°æ®é›†åŒ…å«2,400ä¸ªäººç±»å†™å…¥çš„ç­”æ¡ˆå’Œ24,000ä¸ªLLMç”Ÿæˆçš„ç­”æ¡ˆï¼Œä»¥åŠè¶…è¿‡132,000ä¸ªäººç±»æ³¨é‡Šã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œé¡¶å°–LLMåœ¨è§’è‰²æ‰®æ¼”å’Œæƒ…æ„Ÿæ”¯æŒæ–¹é¢è¶…è¶Šäº†äººç±»ç²‰ä¸ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06388">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d1f7afe17024b22b8ce51e5c676025a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2b87bd0fbb16ca3d9014a38a1a968fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd8a0fce85ff01843a428ba292e61fad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ad7835384bb077bb882f9926600e857.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92fdafe97e23b6a7735066df3b1214bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6e442c7ddc524e00434d0781e4d3ae8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a614dc8076471443c031d5557553279.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="End-to-End-Text-to-SQL-with-Dataset-Selection-Leveraging-LLMs-for-Adaptive-Query-Generation"><a href="#End-to-End-Text-to-SQL-with-Dataset-Selection-Leveraging-LLMs-for-Adaptive-Query-Generation" class="headerlink" title="End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for   Adaptive Query Generation"></a>End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for   Adaptive Query Generation</h2><p><strong>Authors:Anurag Tripathi, Vaibhav Patle, Abhinav Jain, Ayush Pundir, Sairam Menon, Ajeet Kumar Singh</strong></p>
<p>Text-to-SQL bridges the gap between natural language and structured database language, thus allowing non-technical users to easily query databases. Traditional approaches model text-to-SQL as a direct translation task, where a given Natural Language Query (NLQ) is mapped to an SQL command. Recent advances in large language models (LLMs) have significantly improved translation accuracy, however, these methods all require that the target database is pre-specified. This becomes problematic in scenarios with multiple extensive databases, where identifying the correct database becomes a crucial yet overlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL framework to identify the userâ€™s intended database before generating SQL queries. Our approach leverages LLMs and prompt engineering to extract implicit information from natural language queries (NLQs) in the form of a ruleset. We then train a large db_id prediction model, which includes a RoBERTa-based finetuned encoder, to predict the correct Database identifier (db_id) based on both the NLQ and the LLM-generated rules. Finally, we refine the generated SQL by using critic agents to correct errors. Experimental results demonstrate that our framework outperforms the current state-of-the-art models in both database intent prediction and SQL generation accuracy. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°SQLæŠ€æœ¯å¼¥åˆäº†è‡ªç„¶è¯­è¨€ä¸ç»“æ„åŒ–æ•°æ®åº“è¯­è¨€ä¹‹é—´çš„å·®è·ï¼Œä»è€Œå…è®¸éæŠ€æœ¯ç”¨æˆ·è½»æ¾æŸ¥è¯¢æ•°æ®åº“ã€‚ä¼ ç»Ÿæ–¹æ³•å°†æ–‡æœ¬åˆ°SQLå»ºæ¨¡ä¸ºç›´æ¥ç¿»è¯‘ä»»åŠ¡ï¼Œå…¶ä¸­ç»™å®šçš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼ˆNLQï¼‰è¢«æ˜ å°„åˆ°SQLå‘½ä»¤ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•æ˜¾è‘—æé«˜äº†ç¿»è¯‘çš„å‡†ç¡®æ€§ï¼Œç„¶è€Œï¼Œè¿™äº›æ–¹æ³•éƒ½è¦æ±‚é¢„å…ˆæŒ‡å®šç›®æ ‡æ•°æ®åº“ã€‚åœ¨å…·æœ‰å¤šä¸ªå¤§å‹æ•°æ®åº“çš„åœºæ™¯ä¸­ï¼Œè¿™æˆä¸ºäº†é—®é¢˜ï¼Œç¡®å®šæ­£ç¡®çš„æ•°æ®åº“æˆä¸ºè‡³å…³é‡è¦ä½†è¢«å¿½ç•¥çš„æ­¥éª¤ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„ç«¯åˆ°ç«¯æ–‡æœ¬åˆ°SQLæ¡†æ¶ï¼Œç”¨äºåœ¨ç”ŸæˆSQLæŸ¥è¯¢ä¹‹å‰è¯†åˆ«ç”¨æˆ·æ„å›¾çš„æ•°æ®åº“ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨LLMå’Œæç¤ºå·¥ç¨‹ä»è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼ˆNLQï¼‰ä¸­æå–è§„åˆ™é›†å½¢å¼çš„éšå«ä¿¡æ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå¤§å‹æ•°æ®åº“æ ‡è¯†ç¬¦ï¼ˆdb_idï¼‰é¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŒ…æ‹¬åŸºäºRoBERTaçš„å¾®è°ƒç¼–ç å™¨ï¼Œæ ¹æ®NLQå’ŒLLMç”Ÿæˆçš„è§„åˆ™é¢„æµ‹æ­£ç¡®çš„æ•°æ®åº“æ ‡è¯†ç¬¦ï¼ˆdb_idï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰¹åˆ¤ä»£ç†å®Œå–„ç”Ÿæˆçš„SQLä»¥çº æ­£é”™è¯¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨æ•°æ®åº“æ„å›¾é¢„æµ‹å’ŒSQLç”Ÿæˆå‡†ç¡®æ€§æ–¹é¢éƒ½ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06387v1">PDF</a> Accepted in IJCNN25</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŠ€æœ¯çªç ´ï¼Œæ–‡æœ¬åˆ°SQLçš„è½¬æ¢å˜å¾—æ›´åŠ ç²¾å‡†ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸å°†æ–‡æœ¬åˆ°SQLç›´æ¥å»ºæ¨¡ä¸ºç¿»è¯‘ä»»åŠ¡ï¼Œä½†è¿™ç§æ–¹æ³•éœ€è¦åœ¨é¢„å…ˆæŒ‡å®šçš„æ•°æ®åº“ä¸­è¿›è¡Œã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸‰é˜¶æ®µçš„ç«¯åˆ°ç«¯æ–‡æœ¬åˆ°SQLæ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨LLMå’Œæç¤ºå·¥ç¨‹æ¥æå–éšå«ä¿¡æ¯å½¢æˆè§„åˆ™é›†ï¼Œæ¥è¯†åˆ«ç”¨æˆ·æ„å›¾çš„æ•°æ®åº“å¹¶ç”ŸæˆSQLæŸ¥è¯¢ã€‚è¯¥æ¡†æ¶ä½¿ç”¨åŸºäºRoBERTaçš„å¾®è°ƒç¼–ç å™¨é¢„æµ‹æ•°æ®åº“æ ‡è¯†ç¬¦ï¼ˆdb_idï¼‰ï¼Œå¹¶åˆ©ç”¨æ‰¹åˆ¤ä»£ç†ä¿®æ­£ç”Ÿæˆçš„SQLè¯­å¥ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ•°æ®åº“æ„å›¾é¢„æµ‹å’ŒSQLç”Ÿæˆå‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°SQLæŠ€æœ¯å…è®¸éæŠ€æœ¯ç”¨æˆ·è½»æ¾æŸ¥è¯¢æ•°æ®åº“ï¼Œç¼©å°äº†è‡ªç„¶è¯­è¨€ä¸ç»“æ„åŒ–æ•°æ®åº“è¯­è¨€ä¹‹é—´çš„å·®è·ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å°†æ–‡æœ¬åˆ°SQLå»ºæ¨¡ä¸ºç›´æ¥çš„ç¿»è¯‘ä»»åŠ¡ï¼Œéœ€è¦åœ¨é¢„å…ˆæŒ‡å®šçš„æ•°æ®åº“ä¸­è¿›è¡Œæ“ä½œã€‚ä½†åœ¨é¢å¯¹å¤šä¸ªå¤§å‹æ•°æ®åº“æ—¶ï¼Œè¿™æˆä¸ºäº†é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ä¸‰é˜¶æ®µçš„ç«¯åˆ°ç«¯æ–‡æœ¬åˆ°SQLæ¡†æ¶ï¼Œé¦–å…ˆè¯†åˆ«ç”¨æˆ·æ„å›¾çš„æ•°æ®åº“ï¼Œç„¶åç”ŸæˆSQLæŸ¥è¯¢ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæç¤ºå·¥ç¨‹æå–éšå«ä¿¡æ¯å½¢æˆè§„åˆ™é›†ã€‚</li>
<li>ä½¿ç”¨åŸºäºRoBERTaçš„å¾®è°ƒç¼–ç å™¨é¢„æµ‹æ­£ç¡®çš„æ•°æ®åº“æ ‡è¯†ç¬¦ï¼ˆdb_idï¼‰ã€‚</li>
<li>é€šè¿‡è®­ç»ƒçš„å¤§å‹db_idé¢„æµ‹æ¨¡å‹èƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æŸ¥è¯¢å’ŒLLMç”Ÿæˆçš„è§„åˆ™é¢„æµ‹æ­£ç¡®çš„æ•°æ®åº“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06387">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-974938444d802e17799c3423590ab443.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4baa56d144f7c4013ac954b71a8c3e31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1c6ece092d8f859b6967659b5d184a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-709cea50cb8250f1cbfaee293b2b5255.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96ebe5e29c93bc5f190d7f4b9c8d7c73.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SpeakerLM-End-to-End-Versatile-Speaker-Diarization-and-Recognition-with-Multimodal-Large-Language-Models"><a href="#SpeakerLM-End-to-End-Versatile-Speaker-Diarization-and-Recognition-with-Multimodal-Large-Language-Models" class="headerlink" title="SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with   Multimodal Large Language Models"></a>SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with   Multimodal Large Language Models</h2><p><strong>Authors:Han Yin, Yafeng Chen, Chong Deng, Luyao Cheng, Hui Wang, Chao-Hong Tan, Qian Chen, Wen Wang, Xiangang Li</strong></p>
<p>The Speaker Diarization and Recognition (SDR) task aims to predict â€œwho spoke when and whatâ€ within an audio clip, which is a crucial task in various real-world multi-speaker scenarios such as meeting transcription and dialogue systems. Existing SDR systems typically adopt a cascaded framework, combining multiple modules such as speaker diarization (SD) and automatic speech recognition (ASR). The cascaded systems suffer from several limitations, such as error propagation, difficulty in handling overlapping speech, and lack of joint optimization for exploring the synergy between SD and ASR tasks. To address these limitations, we introduce SpeakerLM, a unified multimodal large language model for SDR that jointly performs SD and ASR in an end-to-end manner. Moreover, to facilitate diverse real-world scenarios, we incorporate a flexible speaker registration mechanism into SpeakerLM, enabling SDR under different speaker registration settings. SpeakerLM is progressively developed with a multi-stage training strategy on large-scale real data. Extensive experiments show that SpeakerLM demonstrates strong data scaling capability and generalizability, outperforming state-of-the-art cascaded baselines on both in-domain and out-of-domain public SDR benchmarks. Furthermore, experimental results show that the proposed speaker registration mechanism effectively ensures robust SDR performance of SpeakerLM across diverse speaker registration conditions and varying numbers of registered speakers. </p>
<blockquote>
<p>è¯­éŸ³è¯†åˆ«ä¸è¯†åˆ«ï¼ˆSDRï¼‰ä»»åŠ¡æ—¨åœ¨é¢„æµ‹éŸ³é¢‘å‰ªè¾‘ä¸­çš„â€œè°ä½•æ—¶è¯´äº†ä»€ä¹ˆâ€ï¼Œè¿™åœ¨ä¼šè®®è½¬å½•å’Œå¯¹è¯ç³»ç»Ÿç­‰å¤šç§çœŸå®ä¸–ç•Œå¤šè¯´è¯äººåœºæ™¯ä¸­æ˜¯ä¸€ä¸ªè‡³å…³é‡è¦çš„ä»»åŠ¡ã€‚ç°æœ‰çš„SDRç³»ç»Ÿé€šå¸¸é‡‡ç”¨çº§è”æ¡†æ¶ï¼Œç»“åˆå¤šä¸ªæ¨¡å—ï¼Œå¦‚è¯´è¯äººè¯†åˆ«ï¼ˆSDï¼‰å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚çº§è”ç³»ç»Ÿå­˜åœ¨å‡ ä¸ªå±€é™æ€§ï¼Œä¾‹å¦‚è¯¯å·®ä¼ æ’­ã€å¤„ç†é‡å è¯­éŸ³çš„å›°éš¾ï¼Œä»¥åŠç¼ºä¹ä¸ºæ¢ç´¢SDå’ŒASRä»»åŠ¡ä¹‹é—´ååŒå·¥ä½œçš„è”åˆä¼˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†SpeakerLMï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šåª’ä½“å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Œç”¨äºSDRï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è”åˆæ‰§è¡ŒSDå’ŒASRã€‚æ­¤å¤–ï¼Œä¸ºäº†åº”å¯¹å„ç§çœŸå®ä¸–ç•Œåœºæ™¯ï¼Œæˆ‘ä»¬å°†çµæ´»çš„è¯´è¯äººæ³¨å†Œæœºåˆ¶çº³å…¥SpeakerLMä¸­ï¼Œä»¥å®ç°åœ¨ä¸åŒçš„è¯´è¯äººæ³¨å†Œè®¾ç½®ä¸‹è¿›è¡ŒSDRã€‚SpeakerLMé‡‡ç”¨å¤§è§„æ¨¡çœŸå®æ•°æ®çš„å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥è¿›è¡Œé€æ­¥å¼€å‘ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSpeakerLMè¡¨ç°å‡ºå¼ºå¤§çš„æ•°æ®æ‰©å±•èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ï¼Œåœ¨åŸŸå†…å’ŒåŸŸå¤–å…¬å…±SDRåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºæœ€æ–°çš„çº§è”åŸºçº¿ã€‚æ­¤å¤–ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è¯´è¯äººæ³¨å†Œæœºåˆ¶æœ‰æ•ˆåœ°ç¡®ä¿äº†SpeakerLMåœ¨ä¸åŒè¯´è¯äººæ³¨å†Œæ¡ä»¶å’Œä¸åŒæ³¨å†Œè¯´è¯äººæ•°é‡çš„æƒ…å†µä¸‹å®ç°ç¨³å¥çš„SDRæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06372v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šSpeakerLMæ˜¯ä¸€æ¬¾ä¸ºè¯´è¯è€…è¯†åˆ«å’Œè¯´è¯æ—¶åºé¢„æµ‹ä»»åŠ¡ï¼ˆSDRï¼‰è®¾è®¡çš„ç»Ÿä¸€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚å®ƒèƒ½åœ¨ä¸€ä¸ªç«¯åˆ°ç«¯çš„æµç¨‹ä¸­åŒæ—¶å®Œæˆè¯´è¯è€…è¯†åˆ«å’Œè¯­éŸ³è‡ªåŠ¨è¯†åˆ«çš„ä»»åŠ¡ï¼Œè§£å†³äº†ä¼ ç»Ÿçº§è”ç³»ç»Ÿçš„è¯¯å·®ä¼ æ’­å’Œéš¾ä»¥å¤„ç†é‡å è¯­éŸ³ç­‰é—®é¢˜ã€‚é€šè¿‡å¼•å…¥çµæ´»çš„è¯´è¯è€…æ³¨å†Œæœºåˆ¶ï¼ŒSpeakerLMå¯ä»¥åœ¨ä¸åŒçš„è¯´è¯è€…æ³¨å†Œè®¾ç½®ä¸‹å®ç°SDRã€‚å…¶é‡‡ç”¨å¤§è§„æ¨¡çœŸå®æ•°æ®çš„å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„æ•°æ®æ‰©å±•èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ï¼Œä¼˜äºçº§è”åŸºçº¿æ¨¡å‹ã€‚å®éªŒè¡¨æ˜è¯¥æ¨¡å‹çš„è¯´è¯è€…æ³¨å†Œæœºåˆ¶å¯ä»¥æœ‰æ•ˆåœ°åœ¨å„ç§æ³¨å†Œæ¡ä»¶å’Œä¸åŒæ•°é‡çš„å·²æ³¨å†Œè¯´è¯è€…çš„æƒ…å†µä¸‹ç¡®ä¿SDRæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>SpeakerLMæ˜¯ä¸€ä¸ªç”¨äºSDRä»»åŠ¡çš„ç»Ÿä¸€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>å®ƒèƒ½å¤Ÿåœ¨ä¸€ä¸ªç«¯åˆ°ç«¯çš„æµç¨‹ä¸­å®Œæˆè¯´è¯è€…è¯†åˆ«å’Œè¯­éŸ³è‡ªåŠ¨è¯†åˆ«ï¼ˆASRï¼‰ã€‚</li>
<li>ä¸ä¼ ç»Ÿçš„çº§è”ç³»ç»Ÿç›¸æ¯”ï¼ŒSpeakerLMè§£å†³äº†è¯¯å·®ä¼ æ’­å’Œé‡å è¯­éŸ³å¤„ç†çš„é—®é¢˜ã€‚</li>
<li>SpeakerLMå¼•å…¥äº†çµæ´»çš„è¯´è¯è€…æ³¨å†Œæœºåˆ¶ä»¥é€‚åº”ä¸åŒçš„è¯´è¯è€…æ³¨å†Œè®¾ç½®ã€‚</li>
<li>SpeakerLMåœ¨å¤§é‡çœŸå®æ•°æ®ä¸Šé‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„æ•°æ®æ‰©å±•èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ã€‚</li>
<li>ä¸ç°æœ‰çš„çº§è”åŸºçº¿ç›¸æ¯”ï¼ŒSpeakerLMåœ¨å…¬å…±SDRåŸºå‡†æµ‹è¯•ä¸­å…·æœ‰å‡ºè‰²çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06372">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a3eb44f283c52a95e0a479ee4000ecd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72f750df5e263d7f232803459ab546ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b89eeb7cdff535f3fa984ba1a4592f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65feea9bd1df6ad8967abcacf2644132.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ed94171998e99e10085fc197d1ba0fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee6a1419be5b9d73d4f1a8494ed8bea2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ff334d95281ff61fc888bc8d66a462c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Cyberbullying-Detection-via-Aggression-Enhanced-Prompting"><a href="#Cyberbullying-Detection-via-Aggression-Enhanced-Prompting" class="headerlink" title="Cyberbullying Detection via Aggression-Enhanced Prompting"></a>Cyberbullying Detection via Aggression-Enhanced Prompting</h2><p><strong>Authors:Aisha Saeid, Anu Sabu, Girish A. Koushik, Ferrante Neri, Diptesh Kanojia</strong></p>
<p>Detecting cyberbullying on social media remains a critical challenge due to its subtle and varied expressions. This study investigates whether integrating aggression detection as an auxiliary task within a unified training framework can enhance the generalisation and performance of large language models (LLMs) in cyberbullying detection. Experiments are conducted on five aggression datasets and one cyberbullying dataset using instruction-tuned LLMs. We evaluated multiple strategies: zero-shot, few-shot, independent LoRA fine-tuning, and multi-task learning (MTL). Given the inconsistent results of MTL, we propose an enriched prompt pipeline approach in which aggression predictions are embedded into cyberbullying detection prompts to provide contextual augmentation. Preliminary results show that the enriched prompt pipeline consistently outperforms standard LoRA fine-tuning, indicating that aggression-informed context significantly boosts cyberbullying detection. This study highlights the potential of auxiliary tasks, such as aggression detection, to improve the generalisation of LLMs for safety-critical applications on social networks. </p>
<blockquote>
<p>æ£€æµ‹ç¤¾äº¤åª’ä½“ä¸Šçš„ç½‘ç»œæ¬ºå‡Œä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå› ä¸ºç½‘ç»œæ¬ºå‡Œçš„è¡¨è¾¾æ–¹å¼å¾ˆå¾®å¦™ä¸”å¤šæ ·ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨åœ¨ç»Ÿä¸€è®­ç»ƒæ¡†æ¶å†…æ•´åˆæ”»å‡»æ£€æµ‹ä½œä¸ºè¾…åŠ©ä»»åŠ¡æ˜¯å¦å¯ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç½‘ç»œæ¬ºå‡Œæ£€æµ‹ä¸­çš„é€šç”¨æ€§å’Œæ€§èƒ½ã€‚ç ”ç©¶é‡‡ç”¨æŒ‡ä»¤è°ƒä¼˜çš„LLMï¼Œåœ¨äº”ä¸ªæ”»å‡»æ•°æ®é›†å’Œä¸€ä¸ªç½‘ç»œæ¬ºå‡Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šç§ç­–ç•¥ï¼šé›¶æ ·æœ¬ã€å°‘æ ·æœ¬ã€ç‹¬ç«‹LoRAå¾®è°ƒä»¥åŠå¤šä»»åŠ¡å­¦ä¹ ï¼ˆMTLï¼‰ã€‚é‰´äºå¤šä»»åŠ¡å­¦ä¹ çš„ç»“æœä¸ä¸€è‡´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸°å¯Œçš„æç¤ºç®¡é“æ–¹æ³•ï¼Œå…¶ä¸­æ”»å‡»é¢„æµ‹è¢«åµŒå…¥åˆ°ç½‘ç»œæ¬ºå‡Œæ£€æµ‹æç¤ºä¸­ä»¥æä¾›ä¸Šä¸‹æ–‡å¢å¼ºã€‚åˆæ­¥ç»“æœè¡¨æ˜ï¼Œä¸°å¯Œçš„æç¤ºç®¡é“å§‹ç»ˆä¼˜äºæ ‡å‡†çš„LoRAå¾®è°ƒï¼Œè¿™è¡¨æ˜æ”»å‡»ä¿¡æ¯ä¸°å¯Œçš„ä¸Šä¸‹æ–‡å¯ä»¥æå¤§åœ°ä¿ƒè¿›ç½‘ç»œæ¬ºå‡Œæ£€æµ‹ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†è¾…åŠ©ä»»åŠ¡ï¼ˆå¦‚æ”»å‡»æ£€æµ‹ï¼‰çš„æ½œåŠ›ï¼Œå¯ä»¥æ”¹è¿›LLMåœ¨ç¤¾äº¤ç½‘ç»œçš„å®‰å…¨å…³é”®åº”ç”¨ä¸­çš„é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06360v1">PDF</a> Accepted to RANLP 2025</p>
<p><strong>æ€»ç»“</strong></p>
<p>ç¤¾äº¤åª’ä½“ä¸Šçš„ç½‘ç»œæ¬ºå‡Œç°è±¡å› å½¢å¼å¾®å¦™å¤šå˜ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„å…³é”®æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢å°†æ”»å‡»æ£€æµ‹ä½œä¸ºè¾…åŠ©ä»»åŠ¡é›†æˆåˆ°ç»Ÿä¸€è®­ç»ƒæ¡†æ¶ä¸­ï¼Œæ˜¯å¦å¯ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç½‘ç»œæ¬ºå‡Œæ£€æµ‹ä¸­çš„æ³›åŒ–èƒ½åŠ›å’Œæ€§èƒ½ã€‚å®éªŒé‡‡ç”¨äº”ä¸ªæ”»å‡»æ•°æ®é›†å’Œä¸€ä¸ªç½‘ç»œæ¬ºå‡Œæ•°æ®é›†ï¼Œå¯¹æŒ‡ä»¤å¾®è°ƒLLMè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬è¯„ä¼°äº†é›¶æ ·æœ¬å­¦ä¹ ã€å°‘æ ·æœ¬å­¦ä¹ ã€ç‹¬ç«‹LoRAå¾®è°ƒä»¥åŠå¤šä»»åŠ¡å­¦ä¹ ï¼ˆMTLï¼‰ç­‰å¤šç§ç­–ç•¥ã€‚è€ƒè™‘åˆ°å¤šä»»åŠ¡å­¦ä¹ çš„ä¸ç¨³å®šç»“æœï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºçš„æç¤ºç®¡é“æ–¹æ³•ï¼Œå…¶ä¸­æ”»å‡»é¢„æµ‹è¢«åµŒå…¥åˆ°ç½‘ç»œæ¬ºå‡Œæ£€æµ‹æç¤ºä¸­ä»¥æä¾›ä¸Šä¸‹æ–‡å¢å¼ºã€‚åˆæ­¥ç»“æœè¡¨æ˜ï¼Œå¢å¼ºçš„æç¤ºç®¡é“å§‹ç»ˆä¼˜äºæ ‡å‡†çš„LoRAå¾®è°ƒï¼Œè¡¨æ˜æ”»å‡»ä¿¡æ¯ä¸Šä¸‹æ–‡æ˜¾è‘—æé«˜äº†ç½‘ç»œæ¬ºå‡Œæ£€æµ‹æ€§èƒ½ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†è¾…åŠ©ä»»åŠ¡ï¼ˆå¦‚æ”»å‡»æ£€æµ‹ï¼‰åœ¨ç¤¾äº¤åª’ä½“å®‰å…¨å…³é”®åº”ç”¨ä¸­å¯¹æé«˜LLMæ³›åŒ–èƒ½åŠ›çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç½‘ç»œæ¬ºå‡Œå› å…¶å½¢å¼çš„å¾®å¦™å¤šå˜ä»æ˜¯ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>åœ¨ç»Ÿä¸€è®­ç»ƒæ¡†æ¶å†…é›†æˆæ”»å‡»æ£€æµ‹æœ‰åŠ©äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹å¯¹ç½‘ç»œæ¬ºå‡Œæ£€æµ‹çš„æ³›åŒ–èƒ½åŠ›å’Œæ€§èƒ½ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†å¤šä¸ªæ•°æ®é›†å¹¶å¯¹ä¸åŒç­–ç•¥è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å­¦ä¹ ã€å°‘æ ·æœ¬å­¦ä¹ å’Œå¤šä»»åŠ¡å­¦ä¹ ç­‰ã€‚</li>
<li>å¤šä»»åŠ¡å­¦ä¹ çš„ç»“æœä¸ä¸€è‡´ï¼Œæå‡ºäº†ä¸€ç§å¢å¼ºçš„æç¤ºç®¡é“æ–¹æ³•ä»¥æé«˜ç½‘ç»œæ¬ºå‡Œæ£€æµ‹æ€§èƒ½ã€‚</li>
<li>å¢å¼ºçš„æç¤ºç®¡é“é€šè¿‡å°†æ”»å‡»é¢„æµ‹åµŒå…¥åˆ°ç½‘ç»œæ¬ºå‡Œæ£€æµ‹æç¤ºä¸­æä¾›ä¸Šä¸‹æ–‡å¢å¼ºï¼Œè¡¨ç°ä¼˜äºæ ‡å‡†LoRAå¾®è°ƒã€‚</li>
<li>æ”»å‡»ä¿¡æ¯çš„ä¸Šä¸‹æ–‡å¯¹æé«˜ç½‘ç»œæ¬ºå‡Œæ£€æµ‹å‡†ç¡®æ€§æœ‰å…³é”®ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06360">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72bfe1cacb6dcea9dee99550405c239a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28d984c26a74d681aa4b4117dfa35a50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d65f73996b250f0e66b84d6ac7b3bf1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Prompting-for-Extractive-Quranic-QA-with-Instruction-Tuned-LLMs"><a href="#Few-Shot-Prompting-for-Extractive-Quranic-QA-with-Instruction-Tuned-LLMs" class="headerlink" title="Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs"></a>Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs</h2><p><strong>Authors:Mohamed Basem, Islam Oshallah, Ali Hamdi, Ammar Mohammed</strong></p>
<p>This paper presents two effective approaches for Extractive Question Answering (QA) on the Quran. It addresses challenges related to complex language, unique terminology, and deep meaning in the text. The second uses few-shot prompting with instruction-tuned large language models such as Gemini and DeepSeek. A specialized Arabic prompt framework is developed for span extraction. A strong post-processing system integrates subword alignment, overlap suppression, and semantic filtering. This improves precision and reduces hallucinations. Evaluations show that large language models with Arabic instructions outperform traditional fine-tuned models. The best configuration achieves a pAP10 score of 0.637. The results confirm that prompt-based instruction tuning is effective for low-resource, semantically rich QA tasks. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸¤ç§é’ˆå¯¹ã€Šå¤å…°ç»ã€‹çš„æŠ½å–å¼é—®ç­”ï¼ˆQAï¼‰çš„æœ‰æ•ˆæ–¹æ³•ã€‚å®ƒè§£å†³äº†ä¸å¤æ‚è¯­è¨€ã€ç‹¬ç‰¹æœ¯è¯­å’Œæ–‡æœ¬æ·±å±‚å«ä¹‰ç›¸å…³çš„æŒ‘æˆ˜ã€‚ç¬¬äºŒç§æ–¹æ³•ä½¿ç”¨å°‘é‡æç¤ºæŒ‡ä»¤è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚Geminiå’ŒDeepSeekã€‚å¼€å‘äº†ä¸€ä¸ªä¸“é—¨çš„é˜¿æ‹‰ä¼¯æç¤ºæ¡†æ¶ç”¨äºè·¨åº¦æå–ã€‚å¼ºå¤§çš„åå¤„ç†ç³»ç»Ÿé›†æˆäº†å­è¯å¯¹é½ã€é‡å æŠ‘åˆ¶å’Œè¯­ä¹‰è¿‡æ»¤ã€‚è¿™æé«˜äº†ç²¾åº¦å¹¶å‡å°‘äº†å¹»è§‰ã€‚è¯„ä¼°è¡¨æ˜ï¼Œå¸¦æœ‰é˜¿æ‹‰ä¼¯æŒ‡ä»¤çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¼˜äºä¼ ç»Ÿå¾®è°ƒæ¨¡å‹ã€‚æœ€ä½³é…ç½®å®ç°äº†pAP10å¾—åˆ†ä¸º0.637ã€‚ç»“æœè¯å®ï¼ŒåŸºäºæç¤ºçš„æŒ‡ä»¤è°ƒæ•´å¯¹äºèµ„æºè¾ƒå°‘ã€è¯­ä¹‰ä¸°å¯Œçš„é—®ç­”ä»»åŠ¡éå¸¸æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06103v1">PDF</a> 6 pages , 2 figures , Accepted in IMSA 2025,Egypt ,   <a target="_blank" rel="noopener" href="https://imsa.msa.edu.eg/">https://imsa.msa.edu.eg/</a></p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡ä»‹ç»äº†ä¸¤ç§é’ˆå¯¹ã€Šå¤å…°ç»ã€‹çš„æœ‰æ•ˆæå–å¼é—®ç­”æ–¹æ³•ã€‚ç¬¬äºŒç§æ–¹æ³•é‡‡ç”¨æŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå°‘æ ·æœ¬æç¤ºï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªä¸“é—¨çš„é˜¿æ‹‰ä¼¯æ–‡æç¤ºæ¡†æ¶ç”¨äºè·¨åº¦æå–ã€‚å¼ºå¤§çš„åå¤„ç†ç³»ç»Ÿæé«˜äº†ç²¾ç¡®åº¦å¹¶å‡å°‘äº†å¹»è§‰ã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œå¸¦æœ‰é˜¿æ‹‰ä¼¯æŒ‡ä»¤çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¼˜äºä¼ ç»Ÿå¾®è°ƒæ¨¡å‹çš„æœ€ä½³é…ç½®ï¼Œè¾¾åˆ°pAP10å¾—åˆ†ä¸º0.637ã€‚è¯æ˜åŸºäºæç¤ºçš„æŒ‡ä»¤å¾®è°ƒå¯¹äºä½èµ„æºã€è¯­ä¹‰ä¸°å¯Œçš„é—®ç­”ä»»åŠ¡æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸¤ç§é’ˆå¯¹ã€Šå¤å…°ç»ã€‹çš„æœ‰æ•ˆæå–å¼é—®ç­”æ–¹æ³•ï¼Œè§£å†³äº†å¤æ‚è¯­è¨€ã€ç‹¬ç‰¹æœ¯è¯­å’Œæ–‡æœ¬æ·±å±‚å«ä¹‰çš„æŒ‘æˆ˜ã€‚</li>
<li>ç¬¬äºŒç§æ–¹æ³•é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œå¹¶é€šè¿‡å°‘æ ·æœ¬æç¤ºè¿›è¡Œå®ç°ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªä¸“é—¨çš„é˜¿æ‹‰ä¼¯æ–‡æç¤ºæ¡†æ¶ç”¨äºè·¨åº¦æå–ï¼Œæé«˜äº†é—®ç­”çš„å‡†ç¡®æ€§ã€‚</li>
<li>å¼ºå¤§çš„åå¤„ç†ç³»ç»ŸåŒ…æ‹¬å­è¯å¯¹é½ã€é‡å æŠ‘åˆ¶å’Œè¯­ä¹‰è¿‡æ»¤ï¼Œæé«˜äº†ç»“æœçš„ç²¾ç¡®åº¦å¹¶é™ä½äº†è¯¯æŠ¥ç‡ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨é˜¿æ‹‰ä¼¯æŒ‡ä»¤çš„å¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°ä¼˜äºä¼ ç»Ÿå¾®è°ƒæ¨¡å‹ã€‚</li>
<li>æœ€ä½³é…ç½®è¾¾åˆ°pAP10å¾—åˆ†ä¸º0.637ï¼Œè¡¨æ˜å…¶è‰¯å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06103">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c94999826fc14ecfc31b63e060ccaf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62b07b47f75c32fd720fe3f5de63ad39.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d643a662c122ed8f40465c8e194d007.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c435702f1ea57c62176f82b140290901.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-63b7e700cfd18f3a9d68fc5c8a2bd642.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17501311f9e37f3128bfbfd7acf183b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f408f58a717874407744f415a2f9bbe.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ART-Adaptive-Relation-Tuning-for-Generalized-Relation-Prediction"><a href="#ART-Adaptive-Relation-Tuning-for-Generalized-Relation-Prediction" class="headerlink" title="ART: Adaptive Relation Tuning for Generalized Relation Prediction"></a>ART: Adaptive Relation Tuning for Generalized Relation Prediction</h2><p><strong>Authors:Gopika Sudhakaran, Hikaru Shindo, Patrick Schramowski, Simone Schaub-Meyer, Kristian Kersting, Stefan Roth</strong></p>
<p>Visual relation detection (VRD) is the task of identifying the relationships between objects in a scene. VRD models trained solely on relation detection data struggle to generalize beyond the relations on which they are trained. While prompt tuning has been used to adapt vision-language models (VLMs) for VRD, it uses handcrafted prompts and struggles with novel or complex relations. We argue that instruction tuning offers a more effective solution by fine-tuning VLMs on diverse instructional data. We thus introduce ART, an Adaptive Relation Tuning framework that adapts VLMs for VRD through instruction tuning and strategic instance selection. By converting VRD datasets into an instruction tuning format and employing an adaptive sampling algorithm, ART directs the VLM to focus on informative relations while maintaining generalizability. Specifically, we focus on the relation classification, where subject-object boxes are given and the model predicts the predicate between them. We tune on a held-in set and evaluate across multiple held-out datasets of varying complexity. Our approach strongly improves over its baselines and can infer unseen relation concepts, a capability absent in mainstream VRD methods. We demonstrate ARTâ€™s practical value by using the predicted relations for segmenting complex scenes. </p>
<blockquote>
<p>è§†è§‰å…³ç³»æ£€æµ‹ï¼ˆVRDï¼‰æ˜¯è¯†åˆ«åœºæ™¯ä¸­ç‰©ä½“ä¹‹é—´å…³ç³»çš„ä»»åŠ¡ã€‚ä»…é€šè¿‡å…³ç³»æ£€æµ‹æ•°æ®è¿›è¡Œè®­ç»ƒçš„VRDæ¨¡å‹å¾ˆéš¾æ¨å¹¿åˆ°å…¶æœªè®­ç»ƒè¿‡çš„å…³ç³»ä¹‹å¤–ã€‚è™½ç„¶æç¤ºè°ƒæ•´å·²è¢«ç”¨äºé€‚åº”è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡ŒVRDï¼Œä½†å®ƒä½¿ç”¨æ‰‹å·¥åˆ¶ä½œçš„æç¤ºï¼Œå¯¹äºæ–°é¢–æˆ–å¤æ‚çš„å…³ç³»å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬è®¤ä¸ºï¼ŒæŒ‡ä»¤è°ƒæ•´é€šè¿‡åœ¨å¯¹å¤šæ ·åŒ–æŒ‡ä»¤æ•°æ®è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹ä¸ºVLMæä¾›æ›´ä¸ºæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ARTï¼Œä¸€ç§è‡ªé€‚åº”å…³ç³»è°ƒæ•´æ¡†æ¶ï¼Œå®ƒé€šè¿‡æŒ‡ä»¤è°ƒæ•´å’Œæˆ˜ç•¥æ€§å®ä¾‹é€‰æ‹©æ¥é€‚åº”VLMè¿›è¡ŒVRDã€‚é€šè¿‡å°†VRDæ•°æ®é›†è½¬æ¢ä¸ºæŒ‡ä»¤è°ƒæ•´æ ¼å¼å¹¶é‡‡ç”¨è‡ªé€‚åº”é‡‡æ ·ç®—æ³•ï¼ŒARTæŒ‡å¯¼VLMä¸“æ³¨äºä¿¡æ¯ä¸°å¯Œçš„å…³ç³»ï¼ŒåŒæ—¶ä¿æŒå…¶æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å…³æ³¨å…³ç³»åˆ†ç±»ï¼Œå…¶ä¸­ç»™å®šä¸»è¯­-å¯¹è±¡æ¡†ï¼Œæ¨¡å‹é¢„æµ‹å®ƒä»¬ä¹‹é—´çš„è°“è¯­ã€‚æˆ‘ä»¬åœ¨ä¿ç•™é›†ä¸Šè¿›è¡Œè°ƒæ•´ï¼Œå¹¶åœ¨å¤šä¸ªä¸åŒå¤æ‚åº¦çš„ä¿ç•™å¤–æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¾ƒå…¶åŸºçº¿æ–¹æ³•æœ‰æ˜¾è‘—æ”¹å–„ï¼Œå¹¶èƒ½æ¨æ–­æœªè§çš„å…³ç³»æ¦‚å¿µï¼Œè¿™æ˜¯ä¸»æµVRDæ–¹æ³•æ‰€ä¸å…·å¤‡çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨é¢„æµ‹çš„ä¼™ä¼´å…³ç³»æ¥åˆ†å‰²å¤æ‚åœºæ™¯ï¼Œå±•ç¤ºäº†ARTçš„å®é™…ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23543v2">PDF</a> Accepted for publication in ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>è§†è§‰å…³ç³»æ£€æµ‹ï¼ˆVRDï¼‰çš„ä»»åŠ¡æ˜¯è¯†åˆ«åœºæ™¯ä¸­ç‰©ä½“ä¹‹é—´çš„å…³ç³»ã€‚ä»…é€šè¿‡å…³ç³»æ£€æµ‹æ•°æ®è®­ç»ƒçš„VRDæ¨¡å‹éš¾ä»¥æ³›åŒ–åˆ°å…¶è®­ç»ƒä»¥å¤–çš„å…³ç³»ã€‚è™½ç„¶æç¤ºè°ƒæ•´å·²è¢«ç”¨äºé€‚åº”è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡ŒVRDï¼Œä½†å®ƒä½¿ç”¨æ‰‹å·¥åˆ¶ä½œçš„æç¤ºï¼Œå¯¹äºæ–°é¢–æˆ–å¤æ‚çš„å…³ç³»å­˜åœ¨å›°éš¾ã€‚æœ¬æ–‡ä¸»å¼ é€šè¿‡åœ¨ä¸åŒæŒ‡ä»¤æ•°æ®ä¸Šå¾®è°ƒVLMæ¥æä¾›æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ARTï¼Œè¿™æ˜¯ä¸€ç§è‡ªé€‚åº”å…³ç³»è°ƒæ•´æ¡†æ¶ï¼Œå®ƒé€šè¿‡æŒ‡ä»¤è°ƒæ•´å’Œç­–ç•¥æ€§å®ä¾‹é€‰æ‹©æ¥é€‚åº”VLMè¿›è¡ŒVRDã€‚é€šè¿‡å°†VRDæ•°æ®é›†è½¬æ¢ä¸ºæŒ‡ä»¤è°ƒæ•´æ ¼å¼å¹¶é‡‡ç”¨è‡ªé€‚åº”é‡‡æ ·ç®—æ³•ï¼ŒARTæŒ‡å¯¼VLMä¸“æ³¨äºä¿¡æ¯ä¸°å¯Œçš„å…³ç³»ï¼ŒåŒæ—¶ä¿æŒå…¶æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬é‡ç‚¹å…³æ³¨å…³ç³»åˆ†ç±»ä»»åŠ¡ï¼Œç»™å‡ºä¸»è¯­å’Œå®¾è¯­æ¡†ï¼Œæ¨¡å‹é¢„æµ‹å®ƒä»¬ä¹‹é—´çš„è°“è¯­ã€‚æˆ‘ä»¬åœ¨ä¿ç•™é›†ä¸Šè¿›è¡Œè°ƒæ•´ï¼Œå¹¶åœ¨å¤šä¸ªå¤æ‚çš„ä¿ç•™å¤–æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æœ‰æ˜æ˜¾çš„æ”¹è¿›ï¼Œå¹¶ä¸”å¯ä»¥æ¨æ–­æœªè§è¿‡çš„å…³ç³»æ¦‚å¿µï¼Œè¿™æ˜¯ä¸»æµVRDæ–¹æ³•æ‰€ç¼ºä¹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨é¢„æµ‹çš„å…³ç³»æ¥åˆ†å‰²å¤æ‚åœºæ™¯æ¥è¯æ˜ARTçš„å®é™…ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VRDä»»åŠ¡çš„é‡ç‚¹æ˜¯è¯†åˆ«åœºæ™¯ä¸­ç‰©ä½“é—´çš„å…³ç³»ã€‚</li>
<li>å•çº¯ä¾èµ–å…³ç³»æ£€æµ‹æ•°æ®è®­ç»ƒçš„VRDæ¨¡å‹æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚</li>
<li>ç›¸æ¯”ä¼ ç»Ÿçš„æç¤ºè°ƒæ•´æ–¹æ³•ï¼ŒæŒ‡ä»¤è°ƒæ•´æ›´ä¸ºæœ‰æ•ˆï¼Œå°¤å…¶å¯¹äºæ–°é¢–çš„æˆ–å¤æ‚çš„å…³ç³»ã€‚</li>
<li>ARTæ¡†æ¶é€šè¿‡æŒ‡ä»¤è°ƒæ•´å’Œç­–ç•¥æ€§å®ä¾‹é€‰æ‹©é€‚åº”äº†VLMè¿›è¡ŒVRDã€‚</li>
<li>ARTèƒ½å°†VRDæ•°æ®é›†è½¬åŒ–ä¸ºæŒ‡ä»¤è°ƒæ•´æ ¼å¼ï¼Œå¹¶é‡‡ç”¨è‡ªé€‚åº”é‡‡æ ·ç®—æ³•ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ARTåœ¨å…³ç³»åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æœªè§è¿‡çš„å…³ç³»æ¦‚å¿µæ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23543">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9a3a064f35e0030a9159f172204d586d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27b51e4484e04f3f27124f5ee809084e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-30cf387408ce3ef9c438258ae244b508.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ccbc0d916ba4e25a2a72ea25c621d6f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="AICrypto-A-Comprehensive-Benchmark-For-Evaluating-Cryptography-Capabilities-of-Large-Language-Models"><a href="#AICrypto-A-Comprehensive-Benchmark-For-Evaluating-Cryptography-Capabilities-of-Large-Language-Models" class="headerlink" title="AICrypto: A Comprehensive Benchmark For Evaluating Cryptography   Capabilities of Large Language Models"></a>AICrypto: A Comprehensive Benchmark For Evaluating Cryptography   Capabilities of Large Language Models</h2><p><strong>Authors:Yu Wang, Yijian Liu, Liheng Ji, Han Luo, Wenjie Li, Xiaofei Zhou, Chiyun Feng, Puji Wang, Yuhan Cao, Geyuan Zhang, Xiaojian Li, Rongwu Xu, Yilei Chen, Tianxing He</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable capabilities across a variety of domains. However, their applications in cryptography, which serves as a foundational pillar of cybersecurity, remain largely unexplored. To address this gap, we propose \textbf{AICrypto}, the first comprehensive benchmark designed to evaluate the cryptographic capabilities of LLMs. The benchmark comprises 135 multiple-choice questions, 150 capture-the-flag (CTF) challenges, and 18 proof problems, covering a broad range of skills from factual memorization to vulnerability exploitation and formal reasoning. All tasks are carefully reviewed or constructed by cryptography experts to ensure correctness and rigor. To support automated evaluation of CTF challenges, we design an agent-based framework. To gain deeper insight into the current state of cryptographic proficiency in LLMs, we introduce human expert performance baselines for comparison across all task types. Our evaluation of 17 leading LLMs reveals that state-of-the-art models match or even surpass human experts in memorizing cryptographic concepts, exploiting common vulnerabilities, and routine proofs. However, they still lack a deep understanding of abstract mathematical concepts and struggle with tasks that require multi-step reasoning and dynamic analysis. We hope this work could provide insights for future research on LLMs in cryptographic applications. Our code and dataset are available at <a target="_blank" rel="noopener" href="https://aicryptobench.github.io/">https://aicryptobench.github.io</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¯†ç å­¦ä¸­çš„åº”ç”¨ï¼Œä½œä¸ºç½‘ç»œå®‰å…¨çš„åŸºç¡€æ”¯æŸ±ï¼Œä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†\textbf{AICrypto}ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°LLMå¯†ç å­¦èƒ½åŠ›çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬135é“é€‰æ‹©é¢˜ã€150é“å¤ºæ——ï¼ˆCTFï¼‰æŒ‘æˆ˜å’Œ18é“è¯æ˜é—®é¢˜ï¼Œæ¶µç›–äº†ä»äº‹å®è®°å¿†åˆ°æ¼æ´åˆ©ç”¨å’Œé€»è¾‘æ¨ç†çš„å¹¿æ³›æŠ€èƒ½ã€‚æ‰€æœ‰ä»»åŠ¡éƒ½ç”±å¯†ç å­¦ä¸“å®¶ä»”ç»†å®¡æŸ¥æˆ–æ„å»ºï¼Œä»¥ç¡®ä¿æ­£ç¡®æ€§å’Œä¸¥è°¨æ€§ã€‚ä¸ºäº†æ”¯æŒCTFæŒ‘æˆ˜çš„è‡ªåŠ¨åŒ–è¯„ä¼°ï¼Œæˆ‘ä»¬è®¾è®¡äº†åŸºäºä»£ç†çš„æ¡†æ¶ã€‚ä¸ºäº†æ·±å…¥äº†è§£LLMå½“å‰çš„å¯†ç å­¦ç†Ÿç»ƒç¨‹åº¦ï¼Œæˆ‘ä»¬ä¸ºæ‰€æœ‰ä»»åŠ¡ç±»å‹å¼•å…¥äº†äººç±»ä¸“å®¶æ€§èƒ½åŸºå‡†ä»¥ä¾›æ¯”è¾ƒã€‚æˆ‘ä»¬å¯¹17æ¬¾é¢†å…ˆçš„LLMè¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºï¼Œæœ€æ–°æ¨¡å‹åœ¨è®°å¿†å¯†ç å­¦æ¦‚å¿µã€åˆ©ç”¨å¸¸è§æ¼æ´å’Œå¸¸è§„è¯æ˜æ–¹é¢ä¸äººç±»ä¸“å®¶ç›¸åŒ¹é…ç”šè‡³è¶…è¶Šäººç±»ä¸“å®¶ã€‚ç„¶è€Œï¼Œä»–ä»¬ä»ç„¶å¯¹æŠ½è±¡æ•°å­¦æ¦‚å¿µçš„ç†è§£ä¸å¤Ÿæ·±å…¥ï¼Œå¹¶ä¸”åœ¨éœ€è¦å¤šæ­¥éª¤æ¨ç†å’ŒåŠ¨æ€åˆ†æçš„ä»»åŠ¡ä¸­è¡¨ç°æŒ£æ‰ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½ä¸ºæœªæ¥LLMåœ¨å¯†ç å­¦åº”ç”¨æ–¹é¢çš„ç ”ç©¶æä¾›è§è§£ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://aicryptobench.github.ioä¸Šæ‰¾åˆ°./">https://aicryptobench.github.ioä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09580v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šé¢†åŸŸè¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨å¯†ç å­¦é¢†åŸŸçš„åº”ç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæå‡ºäº†AICryptoåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•æ—¨åœ¨è¯„ä¼°LLMçš„å¯†ç å­¦èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¤šé¡¹é€‰æ‹©é¢˜ã€æ•æ‰æ ‡å¿—æŒ‘æˆ˜å’Œè¯æ˜é—®é¢˜ã€‚é€šè¿‡å¯¹17æ¬¾é¢†å…ˆLLMçš„è¯„ä¼°ï¼Œå‘ç°å®ƒä»¬åœ¨æŸäº›æ–¹é¢è¾¾åˆ°ç”šè‡³è¶…è¶Šäº†äººç±»ä¸“å®¶çš„æ°´å¹³ï¼Œä½†ä»å­˜åœ¨å¯¹æ•°å­¦æŠ½è±¡æ¦‚å¿µçš„æ·±å±‚æ¬¡ç†è§£ä¸è¶³ä»¥åŠå¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„å›°éš¾ã€‚è¿™ä¸€å·¥ä½œå¯¹æœªæ¥åœ¨å¯†ç å­¦é¢†åŸŸåº”ç”¨LLMçš„ç ”ç©¶å…·æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚æ•°æ®é›†å¯è®¿é—®[ç½‘å€]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMåœ¨å¤šä¸ªé¢†åŸŸå±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†åœ¨å¯†ç å­¦é¢†åŸŸçš„åº”ç”¨ä»å¾…æ¢ç´¢ã€‚</li>
<li>AICryptoåŸºå‡†æµ‹è¯•æ˜¯é¦–ä¸ªè¯„ä¼°LLMå¯†ç å­¦èƒ½åŠ›çš„ç»¼åˆæµ‹è¯•ï¼Œæ¶µç›–å¤šç§é¢˜å‹ã€‚</li>
<li>LLMåœ¨æŸäº›å¯†ç å­¦ä»»åŠ¡ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šäº†äººç±»ä¸“å®¶çš„æ°´å¹³ã€‚</li>
<li>LLMåœ¨ç†è§£æŠ½è±¡æ•°å­¦æ¦‚å¿µå’Œå¤šæ­¥æ¨ç†ä»»åŠ¡æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09580">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-071206a7acf05ea3d022897a5c0bd13e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3eae19e6bbd01a73cd0a08daec674c00.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d91f89ec76aef25b4979806731276bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25da2ac07d0941f0df3e29a8b9cbb67c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c708bb129a416ac5d6850b81b7a8115.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-486d32fe61f0eae20de282d4b9f3edc5.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Floating-Point-Data-Transformation-for-Lossless-Compression"><a href="#Floating-Point-Data-Transformation-for-Lossless-Compression" class="headerlink" title="Floating-Point Data Transformation for Lossless Compression"></a>Floating-Point Data Transformation for Lossless Compression</h2><p><strong>Authors:Samirasadat Jamalidinan, Kazem Cheshmi</strong></p>
<p>Floating-point data is widely used across various domains. Depending on the required precision, each floating-point value can occupy several bytes. Lossless storage of this information is crucial due to its critical accuracy, as seen in applications such as medical imaging and language model weights. In these cases, data size is often significant, making lossless compression essential. Previous approaches either treat this data as raw byte streams for compression or fail to leverage all patterns within the dataset. However, because multiple bytes represent a single value and due to inherent patterns in floating-point representations, some of these bytes are correlated. To leverage this property, we propose a novel data transformation method called Typed Data Transformation (TDT) that groups related bytes together to improve compression. We implemented and tested our approach on various datasets across both CPU and GPU. TDT achieves a geometric mean compression ratio improvement of 1.16$\times$ over state-of-the-art compression tools such as zstd, while also improving both compression and decompression throughput by 1.18â€“3.79$\times$. </p>
<blockquote>
<p>æµ®ç‚¹æ•°æ®å¹¿æ³›åº”ç”¨äºå„ä¸ªé¢†åŸŸã€‚æ ¹æ®æ‰€éœ€çš„ç²¾åº¦ï¼Œæ¯ä¸ªæµ®ç‚¹å€¼å¯èƒ½ä¼šå ç”¨å¤šä¸ªå­—èŠ‚ã€‚ç”±äºå…¶åœ¨åŒ»ç–—æˆåƒå’Œè¯­è¨€æ¨¡å‹æƒé‡ç­‰åº”ç”¨ä¸­çš„å…³é”®å‡†ç¡®æ€§ï¼Œæ— æŸå­˜å‚¨è¿™äº›ä¿¡æ¯è‡³å…³é‡è¦ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œæ•°æ®å¤§å°å¾€å¾€å¾ˆå¤§ï¼Œå› æ­¤æ— æŸå‹ç¼©å˜å¾—è‡³å…³é‡è¦ã€‚ä»¥å‰çš„æ–¹æ³•è¦ä¹ˆå°†è¿™ç§æ•°æ®è§†ä¸ºåŸå§‹å­—èŠ‚æµè¿›è¡Œå‹ç¼©ï¼Œè¦ä¹ˆæœªèƒ½åˆ©ç”¨æ•°æ®é›†ä¸­çš„æ‰€æœ‰æ¨¡å¼ã€‚ç„¶è€Œï¼Œç”±äºå¤šä¸ªå­—èŠ‚ä»£è¡¨ä¸€ä¸ªå€¼ï¼Œå¹¶ä¸”ç”±äºæµ®ç‚¹è¡¨ç¤ºä¸­çš„å›ºæœ‰æ¨¡å¼ï¼Œè¿™äº›å­—èŠ‚ä¸­çš„æŸäº›æ˜¯ç›¸äº’å…³è”çš„ã€‚ä¸ºäº†åˆ©ç”¨è¿™ä¸€ç‰¹æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ•°æ®è½¬æ¢æ–¹æ³•ï¼Œç§°ä¸ºç±»å‹åŒ–æ•°æ®è½¬æ¢ï¼ˆTDTï¼‰ï¼Œå®ƒå°†ç›¸å…³å­—èŠ‚ç»„åˆåœ¨ä¸€èµ·ä»¥æé«˜å‹ç¼©æ•ˆæœã€‚æˆ‘ä»¬åœ¨CPUå’ŒGPUä¸Šçš„å„ç§æ•°æ®é›†ä¸Šå®ç°äº†å¹¶æµ‹è¯•äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚TDTåœ¨zstdç­‰æœ€å…ˆè¿›çš„å‹ç¼©å·¥å…·ä¸Šå®ç°äº†å‡ ä½•å¹³å‡å‹ç¼©ç‡æé«˜1.16å€ï¼ŒåŒæ—¶æé«˜äº†å‹ç¼©å’Œè§£å‹ç¼©ååé‡ï¼Œè¾¾åˆ°1.18-3.79å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18062v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æµ®ç‚¹æ•°æ•°æ®åœ¨å„ä¸ªé¢†åŸŸä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œä»¥åŠå¯¹å…¶è¿›è¡Œæ— æŸå­˜å‚¨å’Œå‹ç¼©çš„é‡è¦æ€§ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°å‹æ•°æ®è½¬æ¢æ–¹æ³•â€”â€”Typed Data Transformationï¼ˆTDTï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå°†ç›¸å…³çš„å­—èŠ‚åˆ†ç»„åœ¨ä¸€èµ·ä»¥æé«˜å‹ç¼©æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTDTç›¸è¾ƒäºç°æœ‰çš„å‹ç¼©å·¥å…·å¦‚zstdï¼Œå®ç°äº†å‡ ä½•å¹³å‡å‹ç¼©æ¯”çš„æé«˜ï¼Œå¹¶ä¸”åœ¨å‹ç¼©å’Œè§£å‹ç¼©é€Ÿåº¦ä¸Šä¹Ÿæœ‰æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ®ç‚¹æ•°æ•°æ®åœ¨å¤šä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ï¼Œå…¶æ— æŸå­˜å‚¨å’Œå‹ç¼©è‡³å…³é‡è¦ã€‚</li>
<li>å‰ç½®æ–¹æ³•åœ¨å¤„ç†æµ®ç‚¹æ•°æ•°æ®çš„å‹ç¼©æ—¶ï¼Œå¾€å¾€å°†å…¶è§†ä¸ºåŸå§‹çš„å­—èŠ‚æµæˆ–è€…æœªèƒ½å……åˆ†åˆ©ç”¨æ•°æ®é›†ä¸­çš„æ‰€æœ‰æ¨¡å¼ã€‚</li>
<li>ç”±äºå¤šä¸ªå­—èŠ‚ä»£è¡¨ä¸€ä¸ªå€¼ï¼Œä¸”æµ®ç‚¹æ•°è¡¨ç¤ºä¸­å­˜åœ¨å›ºæœ‰çš„æ¨¡å¼ï¼Œå› æ­¤æŸäº›å­—èŠ‚ä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ•°æ®è½¬æ¢æ–¹æ³•â€”â€”Typed Data Transformationï¼ˆTDTï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨å­—èŠ‚ä¹‹é—´çš„ç›¸å…³æ€§è¿›è¡Œæ›´å¥½çš„å‹ç¼©ã€‚</li>
<li>TDTæ–¹æ³•é€šè¿‡å°†ç›¸å…³å­—èŠ‚åˆ†ç»„åœ¨ä¸€èµ·ï¼Œæé«˜äº†å‹ç¼©æ•ˆç‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒTDTç›¸è¾ƒäºç°æœ‰å‹ç¼©å·¥å…·ï¼Œå®ç°äº†æ›´é«˜çš„å‡ ä½•å¹³å‡å‹ç¼©æ¯”å’Œæ›´å¿«çš„å‹ç¼©è§£å‹ç¼©é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18062">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fb15bb2d0f9c22363e9bc36aa6ab7d5d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-28b49d9c9367c28c67d9fcfcf1d57169.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bdd9e7f58ded559f2cc670b043a63b9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b10a682e950697d1edd398c191ce6ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c21976891ac445f346a985196f75adc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0fa4d680d19a58c6e43235e459a5015.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MAATS-A-Multi-Agent-Automated-Translation-System-Based-on-MQM-Evaluation"><a href="#MAATS-A-Multi-Agent-Automated-Translation-System-Based-on-MQM-Evaluation" class="headerlink" title="MAATS: A Multi-Agent Automated Translation System Based on MQM   Evaluation"></a>MAATS: A Multi-Agent Automated Translation System Based on MQM   Evaluation</h2><p><strong>Authors:George Wang, Jiaqian Hu, Safinah Ali</strong></p>
<p>We present MAATS, a Multi Agent Automated Translation System that leverages the Multidimensional Quality Metrics (MQM) framework as a fine-grained signal for error detection and refinement. MAATS employs multiple specialized AI agents, each focused on a distinct MQM category (e.g., Accuracy, Fluency, Style, Terminology), followed by a synthesis agent that integrates the annotations to iteratively refine translations. This design contrasts with conventional single-agent methods that rely on self-correction.   Evaluated across diverse language pairs and Large Language Models (LLMs), MAATS outperforms zero-shot and single-agent baselines with statistically significant gains in both automatic metrics and human assessments. It excels particularly in semantic accuracy, locale adaptation, and linguistically distant language pairs. Qualitative analysis highlights its strengths in multi-layered error diagnosis, omission detection across perspectives, and context-aware refinement. By aligning modular agent roles with interpretable MQM dimensions, MAATS narrows the gap between black-box LLMs and human translation workflows, shifting focus from surface fluency to deeper semantic and contextual fidelity. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†MAATSï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“è‡ªåŠ¨åŒ–ç¿»è¯‘ç³»ç»Ÿï¼Œå®ƒåˆ©ç”¨å¤šç»´è´¨é‡æŒ‡æ ‡ï¼ˆMQMï¼‰æ¡†æ¶ä½œä¸ºç²¾ç»†ä¿¡å·è¿›è¡Œé”™è¯¯æ£€æµ‹å’Œä¿®æ­£ã€‚MAATSé‡‡ç”¨å¤šä¸ªä¸“ä¸šAIæ™ºèƒ½ä½“ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“ä¸“æ³¨äºä¸€ä¸ªç‹¬ç‰¹çš„MQMç±»åˆ«ï¼ˆä¾‹å¦‚å‡†ç¡®æ€§ã€æµç•…æ€§ã€é£æ ¼ã€æœ¯è¯­ç­‰ï¼‰ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªç»¼åˆæ™ºèƒ½ä½“æ¥æ•´åˆæ³¨é‡Šï¼Œä»¥è¿­ä»£æ–¹å¼æ”¹è¿›ç¿»è¯‘ã€‚è¿™ç§è®¾è®¡ä¸ä¼ ç»Ÿçš„å•ä¸€æ™ºèƒ½ä½“æ–¹æ³•å½¢æˆå¯¹æ¯”ï¼Œåè€…ä¾èµ–äºè‡ªæˆ‘ä¿®æ­£ã€‚åœ¨å¤šç§è¯­è¨€å¯¹å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMAATSåœ¨è‡ªåŠ¨æŒ‡æ ‡å’Œäººç±»è¯„ä¼°ä¸Šçš„è¡¨ç°éƒ½ä¼˜äºé›¶æ ·æœ¬å’Œå•ä¸€æ™ºèƒ½ä½“åŸºçº¿ã€‚å®ƒåœ¨è¯­ä¹‰å‡†ç¡®æ€§ã€æœ¬åœ°åŒ–é€‚åº”å’Œè¿œè·ç¦»è¯­è¨€å¯¹ä¸Šè¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚å®šæ€§åˆ†æçªå‡ºäº†å…¶åœ¨å¤šå±‚é”™è¯¯è¯Šæ–­ã€ä»ä¸åŒè§’åº¦æ£€æµ‹é—æ¼ä»¥åŠä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¿®æ­£æ–¹é¢çš„ä¼˜åŠ¿ã€‚é€šè¿‡å°†æ¨¡å—åŒ–æ™ºèƒ½ä½“è§’è‰²ä¸å¯è§£é‡Šæ€§MQMç»´åº¦ç›¸ç»“åˆï¼ŒMAATSç¼©å°äº†é»‘ç®±LLMä¸äººç±»ç¿»è¯‘å·¥ä½œæµç¨‹ä¹‹é—´çš„å·®è·ï¼Œå°†é‡ç‚¹ä»è¡¨é¢æµç•…æ€§è½¬ç§»åˆ°æ›´æ·±å±‚æ¬¡çš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡å¿ å®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14848v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MAATSæ˜¯ä¸€ä¸ªå¤šä»£ç†è‡ªåŠ¨åŒ–ç¿»è¯‘ç³»ç»Ÿï¼Œå®ƒåˆ©ç”¨å¤šç»´è´¨é‡åº¦é‡ï¼ˆMQMï¼‰æ¡†æ¶ä½œä¸ºç²¾ç»†ä¿¡å·è¿›è¡Œé”™è¯¯æ£€æµ‹å’Œä¿®æ­£ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨å¤šä¸ªä¸“ä¸šAIä»£ç†ï¼Œæ¯ä¸ªä»£ç†ä¸“æ³¨äºMQMçš„ä¸€ä¸ªç‰¹å®šç±»åˆ«ï¼ˆå¦‚å‡†ç¡®æ€§ã€æµç•…æ€§ã€é£æ ¼å’Œæœ¯è¯­ç­‰ï¼‰ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªç»¼åˆä»£ç†æ¥æ•´åˆæ³¨é‡Šï¼Œä»¥è¿­ä»£æ–¹å¼æ”¹è¿›ç¿»è¯‘ã€‚ä¸ä¼ ç»Ÿä¾èµ–è‡ªæˆ‘ä¿®æ­£çš„å•ä»£ç†æ–¹æ³•ç›¸æ¯”ï¼ŒMAATSåœ¨å¤šç§è¯­è¨€å¯¹å’Œå¤§è¯­è¨€æ¨¡å‹ä¸Šçš„è¡¨ç°æ›´ä¸ºå‡ºè‰²ï¼Œæ— è®ºåœ¨è‡ªåŠ¨æŒ‡æ ‡è¿˜æ˜¯äººå·¥è¯„ä¼°æ–¹é¢éƒ½è¶…è¿‡äº†é›¶æ ·æœ¬å’Œå•ä»£ç†åŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­ä¹‰å‡†ç¡®æ€§ã€åœ°åŸŸé€‚åº”æ€§å’Œè¯­è¨€è·ç¦»è¾ƒå¤§çš„è¯­è¨€å¯¹ä¸Šè¡¨ç°å°¤ä¸ºçªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MAATSæ˜¯ä¸€ä¸ªå¤šä»£ç†è‡ªåŠ¨åŒ–ç¿»è¯‘ç³»ç»Ÿï¼Œåˆ©ç”¨å¤šç»´è´¨é‡åº¦é‡ï¼ˆMQMï¼‰æ¡†æ¶è¿›è¡Œé”™è¯¯æ£€æµ‹å’Œä¿®æ­£ã€‚</li>
<li>MAATSé‡‡ç”¨å¤šä¸ªä¸“ä¸šAIä»£ç†ï¼Œæ¯ä¸ªä»£ç†ä¸“æ³¨äºMQMçš„ä¸€ä¸ªç‰¹å®šç±»åˆ«ã€‚</li>
<li>MAATSé€šè¿‡é›†æˆæ³¨é‡Šè¿›è¡Œè¿­ä»£æ”¹è¿›ç¿»è¯‘ã€‚</li>
<li>MAATSåœ¨å¤šç§è¯­è¨€å¯¹å’Œå¤§è¯­è¨€æ¨¡å‹ä¸Šçš„è¡¨ç°è¶…è¿‡é›¶æ ·æœ¬å’Œå•ä»£ç†åŸºçº¿ã€‚</li>
<li>MAATSåœ¨è¯­ä¹‰å‡†ç¡®æ€§ã€åœ°åŸŸé€‚åº”æ€§å’Œè¯­è¨€è·ç¦»è¾ƒå¤§çš„è¯­è¨€å¯¹ä¸Šè¡¨ç°çªå‡ºã€‚</li>
<li>MAATSå…·æœ‰å¤šå±‚é”™è¯¯è¯Šæ–­ã€å¤šè§’åº¦é—æ¼æ£€æµ‹å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ”¹è¿›çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14848">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bc7d93be698da4704498156887ca6fec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ab63a47c3b092c0ad212db4305b3634.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-999dea03eae714ead18244426d6f7fb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4730060dcae309fdf6a2cfbda26b39fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7fe26c079bcada888e54f012e19fe2c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2a9ce654d11d9ed7f78798d08379059.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CRUST-Bench-A-Comprehensive-Benchmark-for-C-to-safe-Rust-Transpilation"><a href="#CRUST-Bench-A-Comprehensive-Benchmark-for-C-to-safe-Rust-Transpilation" class="headerlink" title="CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation"></a>CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation</h2><p><strong>Authors:Anirudh Khatry, Robert Zhang, Jia Pan, Ziteng Wang, Qiaochu Chen, Greg Durrett, Isil Dillig</strong></p>
<p>C-to-Rust transpilation is essential for modernizing legacy C code while enhancing safety and interoperability with modern Rust ecosystems. However, no dataset currently exists for evaluating whether a system can transpile C into safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset of 100 C repositories, each paired with manually-written interfaces in safe Rust as well as test cases that can be used to validate correctness of the transpilation. By considering entire repositories rather than isolated functions, CRUST-Bench captures the challenges of translating complex projects with dependencies across multiple files. The provided Rust interfaces provide explicit specifications that ensure adherence to idiomatic, memory-safe Rust patterns, while the accompanying test cases enforce functional correctness. We evaluate state-of-the-art large language models (LLMs) on this task and find that safe and idiomatic Rust generation is still a challenging problem for various state-of-the-art methods and techniques. We also provide insights into the errors LLMs usually make in transpiling code from C to safe Rust. The best performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot setting. Improvements on CRUST-Bench would lead to improved transpilation systems that can reason about complex scenarios and help in migrating legacy codebases from C into languages like Rust that ensure memory safety. You can find the dataset and code at <a target="_blank" rel="noopener" href="https://github.com/anirudhkhatry/CRUST-bench">https://github.com/anirudhkhatry/CRUST-bench</a>. </p>
<blockquote>
<p>Cåˆ°Rustçš„è½¬è¯‘å¯¹äºç°ä»£åŒ–é—ç•™Cä»£ç è‡³å…³é‡è¦ï¼ŒåŒæ—¶è¿˜å¯ä»¥æé«˜ä¸ç°ä»£Rustç”Ÿæ€ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œäº’æ“ä½œæ€§ã€‚ç„¶è€Œï¼Œç›®å‰å°šä¸å­˜åœ¨ç”¨äºè¯„ä¼°ç³»ç»Ÿæ˜¯å¦èƒ½å°†Cè½¬è¯‘ä¸ºå®‰å…¨Rustå¹¶èƒ½é€šè¿‡ä¸€ç»„æµ‹è¯•ç”¨ä¾‹çš„æ•°æ®é›†ã€‚æˆ‘ä»¬å¼•å…¥äº†CRUST-Benchæ•°æ®é›†ï¼ŒåŒ…å«100ä¸ªCä»£ç åº“ï¼Œæ¯ä¸ªåº“éƒ½é…æœ‰äººå·¥ç¼–å†™çš„å®‰å…¨Rustæ¥å£ä»¥åŠå¯ç”¨äºéªŒè¯è½¬è¯‘æ­£ç¡®æ€§çš„æµ‹è¯•ç”¨ä¾‹ã€‚é€šè¿‡è€ƒè™‘æ•´ä¸ªä»£ç åº“è€Œä¸æ˜¯å­¤ç«‹çš„å‡½æ•°ï¼ŒCRUST-Benchæ•æ‰åˆ°äº†ç¿»è¯‘å…·æœ‰è·¨å¤šä¸ªæ–‡ä»¶ä¾èµ–å…³ç³»çš„å¤æ‚é¡¹ç›®çš„æŒ‘æˆ˜ã€‚æä¾›çš„Rustæ¥å£æä¾›äº†æ˜ç¡®è§„èŒƒï¼Œç¡®ä¿éµå¾ªä¹ æƒ¯æ€§å’Œå†…å­˜å®‰å…¨çš„Rustæ¨¡å¼ï¼Œè€Œé…å¥—çš„æµ‹è¯•ç”¨ä¾‹åˆ™å¼ºåˆ¶å®è¡ŒåŠŸèƒ½æ­£ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨æ­¤ä»»åŠ¡ä¸Šè¯„ä¼°äº†æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå‘ç°ç”Ÿæˆå®‰å…¨å’Œä¹ æƒ¯æ€§çš„Rustä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œéœ€è¦é‡‡ç”¨å„ç§æœ€å…ˆè¿›çš„æ–¹æ³•å’ŒæŠ€å·§ã€‚æˆ‘ä»¬è¿˜æ·±å…¥äº†è§£äº†LLMåœ¨å°†ä»£ç ä»Cè½¬è¯‘ä¸ºå®‰å…¨Rustæ—¶é€šå¸¸å‡ºç°çš„é”™è¯¯ã€‚è¡¨ç°æœ€ä½³çš„æ¨¡å‹OpenAI o1åªèƒ½åœ¨å•é•œå¤´è®¾ç½®ä¸­è§£å†³15ä¸ªä»»åŠ¡ã€‚åœ¨CRUST-Benchä¸Šçš„æ”¹è¿›å°†å¯¼è‡´æ”¹è¿›è½¬è¯‘ç³»ç»Ÿï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†å¤æ‚åœºæ™¯ï¼Œå¹¶å°†é—ç•™ä»£ç åº“ä»Cè¿ç§»åˆ°åƒRustè¿™æ ·çš„ç¡®ä¿å†…å­˜å®‰å…¨çš„è¯­è¨€ã€‚æ‚¨å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/anirudhkhatry/CRUST-bench">https://github.com/anirudhkhatry/CRUST-bench</a>æ‰¾åˆ°æ•°æ®é›†å’Œä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15254v2">PDF</a> To be published at COLM, 2025</p>
<p><strong>æ‘˜è¦</strong><br>    Cè½¬Rustçš„ç¼–è¯‘å™¨åœ¨å°†é—ç•™Cä»£ç ç°ä»£åŒ–åŒ–æ–¹é¢éå¸¸é‡è¦ï¼Œå¯ä»¥æé«˜å®‰å…¨æ€§å’Œä¸ç°ä»£Rustç”Ÿæ€ç³»ç»Ÿçš„äº’æ“ä½œæ€§ã€‚ç„¶è€Œï¼Œå½“å‰ç¼ºä¹è¯„ä¼°ç³»ç»Ÿèƒ½å¦å®‰å…¨åœ°å°†Cä»£ç ç¼–è¯‘æˆRustå¹¶æˆåŠŸé€šè¿‡æµ‹è¯•ç”¨ä¾‹çš„æ•°æ®é›†ã€‚æˆ‘ä»¬æ¨å‡ºCRUST-Benchæ•°æ®é›†ï¼ŒåŒ…å«ä¸€ç™¾ä¸ªCè¯­è¨€ä»£ç åº“ï¼Œæ¯ä¸ªä»£ç åº“éƒ½æœ‰å®‰å…¨çš„Rustæ‰‹åŠ¨ç¼–å†™çš„æ¥å£åŠæµ‹è¯•ç”¨ä¾‹ã€‚CRUST-Benchè€ƒè™‘äº†æ•´ä¸ªä»£ç åº“è€Œéå­¤ç«‹çš„å‡½æ•°ï¼Œèƒ½åæ˜ ç¿»è¯‘å¤æ‚é¡¹ç›®æ—¶çš„æŒ‘æˆ˜ã€‚æä¾›çš„Rustæ¥å£ç¡®ä¿éµå¾ªæƒ¯ç”¨ä¸”å†…å­˜å®‰å…¨çš„Rustæ¨¡å¼ï¼Œè€Œæµ‹è¯•ç”¨ä¾‹åˆ™ç¡®ä¿åŠŸèƒ½æ­£ç¡®æ€§ã€‚æˆ‘ä»¬è¯„ä¼°äº†æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ­¤ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå‘ç°ç”Ÿæˆå®‰å…¨ä¸”æƒ¯ç”¨çš„Rustä»£ç å¯¹äºå„ç§æœ€å…ˆè¿›çš„æ–¹æ³•å’ŒæŠ€å·§ä»æ˜¯ä¸€ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜ã€‚æˆ‘ä»¬è¿˜æä¾›äº†å…³äºLLMåœ¨å°†ä»£ç ä»Cè½¬æ¢ä¸ºå®‰å…¨Rustæ—¶å¸¸è§é”™è¯¯çš„è§è§£ã€‚è¡¨ç°æœ€ä½³çš„æ¨¡å‹OpenAI o1åœ¨ä¸€æ¬¡æµ‹è¯•ä¸­åªèƒ½è§£å†³15é¡¹ä»»åŠ¡ã€‚å¯¹CRUST-Benchçš„æ”¹è¿›å°†æ¨åŠ¨ç¼–è¯‘ç³»ç»Ÿçš„è¿›æ­¥ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†å¤æ‚åœºæ™¯å¹¶å¸®åŠ©å°†é—ç•™ä»£ç åº“ä»Cè¿ç§»åˆ°å¦‚Rustç­‰è¯­è¨€ã€‚ç›¸å…³æ•°æ®å’Œä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/anirudhkhatry/CRUST-bench%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/anirudhkhatry/CRUST-benchè·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Cä»£ç è¿ç§»åˆ°Rustè¯­è¨€ä¸­å¯æœ‰æ•ˆæå‡å®‰å…¨æ€§å’Œä¸Rustç”Ÿæ€ç³»ç»Ÿçš„å…¼å®¹æ€§ã€‚</li>
<li>ç›®å‰ç¼ºä¹ä¸€ä¸ªè¯„ä¼°Cè½¬Rustç¼–è¯‘å™¨æ€§èƒ½çš„æ•°æ®é›†ã€‚</li>
<li>CRUST-Benchæ•°æ®é›†åŒ…å«ä¸€ç™¾ä¸ªCä»£ç åº“ï¼Œæ¯ä¸ªéƒ½é…å¤‡æ‰‹åŠ¨ç¼–å†™çš„å®‰å…¨Rustæ¥å£å’Œæµ‹è¯•ç”¨ä¾‹ã€‚</li>
<li>CRUST-Benchè€ƒè™‘æ•´ä¸ªä»£ç åº“ï¼Œåæ˜ ç¿»è¯‘å¤æ‚é¡¹ç›®çš„æŒ‘æˆ˜ã€‚</li>
<li>LLMåœ¨ç”Ÿæˆå®‰å…¨ä¸”æƒ¯ç”¨çš„Rustä»£ç æ–¹é¢ä»æœ‰å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨ä¸€æ¬¡å®Œæˆå¤šé¡¹ä»»åŠ¡æ–¹é¢ã€‚</li>
<li>OpenAI o1æ¨¡å‹å°½ç®¡åœ¨æŸäº›ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä¸€æ¬¡æ€§å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›å—é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15254">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de43cf2a1b4941aa1b7dedbba926df07.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d30adebecddc2c2df0568ac628976499.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57ba5f95d1cc7a19e6b78b7c64df44ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b97845c18b472f0dd1ee397910a0a88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2450f17f5ce84f3a89f156add72cbc49.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Contextually-Entangled-Gradient-Mapping-for-Optimized-LLM-Comprehension"><a href="#Contextually-Entangled-Gradient-Mapping-for-Optimized-LLM-Comprehension" class="headerlink" title="Contextually Entangled Gradient Mapping for Optimized LLM Comprehension"></a>Contextually Entangled Gradient Mapping for Optimized LLM Comprehension</h2><p><strong>Authors:Colin Sisate, Alistair Goldfinch, Vincent Waterstone, Sebastian Kingsley, Mariana Blackthorn</strong></p>
<p>Contextually Entangled Gradient Mapping (CEGM) introduces a new approach to gradient optimization, redefining the relationship between contextual embeddings and gradient updates to enhance semantic coherence and reasoning capabilities in neural architectures. By treating gradients as dynamic carriers of contextual dependencies rather than isolated numerical entities, the proposed methodology bridges critical gaps in existing optimization strategies. The integration of entangled gradient dynamics into a loss regularization framework demonstrated significant improvements in tasks involving long-form reasoning, contextual retention, and adaptability to unseen domains. Experimental evaluations showed that the CEGM-enhanced model consistently outperformed baseline approaches, achieving higher accuracy in token-level predictions and greater resilience to noisy inputs. Practical implementations involved modifications to training pipelines, introducing entanglement layers and dynamic coefficient adjustments that seamlessly align with existing architectures. Results further highlighted reductions in semantic drift during sequential transformations and improvements in embedding coherence across paraphrased sentences, showing the robustness and versatility of the proposed methodology. The findings demonstrate the broader implications of gradient entanglement for both theoretical advancements and practical applications in optimization strategies. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡çº ç¼ æ¢¯åº¦æ˜ å°„ï¼ˆCEGMï¼‰å¼•å…¥äº†ä¸€ç§æ–°çš„æ¢¯åº¦ä¼˜åŒ–æ–¹æ³•ï¼Œé‡æ–°å®šä¹‰äº†ä¸Šä¸‹æ–‡åµŒå…¥å’Œæ¢¯åº¦æ›´æ–°ä¹‹é—´çš„å…³ç³»ï¼Œä»¥å¢å¼ºç¥ç»ç½‘ç»œæ¶æ„ä¸­çš„è¯­ä¹‰è¿è´¯æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å°†æ¢¯åº¦è§†ä¸ºåŠ¨æ€æ‰¿è½½ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»çš„è½½ä½“ï¼Œè€Œéå­¤ç«‹çš„æ•°å€¼å®ä½“ï¼Œä»è€Œå¼¥è¡¥äº†ç°æœ‰ä¼˜åŒ–ç­–ç•¥ä¸­çš„å…³é”®ç©ºç™½ã€‚å°†çº ç¼ æ¢¯åº¦åŠ¨åŠ›å­¦é›†æˆåˆ°æŸå¤±æ­£åˆ™åŒ–æ¡†æ¶ä¸­ï¼Œåœ¨æ¶‰åŠé•¿å½¢å¼æ¨ç†ã€ä¸Šä¸‹æ–‡ä¿ç•™å’Œé€‚åº”æœªè§é¢†åŸŸç­‰ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ˜¾ç€æ”¹è¿›ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œä½¿ç”¨CEGMå¢å¼ºçš„æ¨¡å‹å§‹ç»ˆä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œåœ¨ä»¤ç‰Œçº§é¢„æµ‹æ–¹é¢å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸”å¯¹å™ªå£°è¾“å…¥çš„é€‚åº”æ€§æ›´å¼ºã€‚å®é™…åº”ç”¨æ¶‰åŠè®­ç»ƒç®¡é“ä¿®æ”¹ï¼Œå¼•å…¥äº†çº ç¼ å±‚ä»¥åŠåŠ¨æ€ç³»æ•°è°ƒæ•´ï¼Œè¿™äº›è°ƒæ•´ä¸ç°æœ‰æ¶æ„æ— ç¼å¯¹æ¥ã€‚ç»“æœè¿›ä¸€æ­¥çªå‡ºäº†åœ¨è¿ç»­å˜æ¢è¿‡ç¨‹ä¸­è¯­ä¹‰æ¼‚ç§»çš„å‡å°‘ä»¥åŠè·¨åŒä¹‰å¥åµŒå…¥è¿è´¯æ€§çš„æé«˜ï¼Œæ˜¾ç¤ºäº†æ‰€æå‡ºæ–¹æ³•çš„ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ¢¯åº¦çº ç¼ å¯¹ä¼˜åŒ–ç­–ç•¥çš„ç†è®ºè¿›æ­¥å’Œå®é™…åº”ç”¨å…·æœ‰æ›´å¹¿æ³›çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00048v2">PDF</a> arXiv admin note: This paper has been withdrawn by arXiv due to   disputed and unverifiable authorship</p>
<p><strong>Summary</strong></p>
<p>æ¢¯åº¦ä¼˜åŒ–æ–°æ–¹æ³•â€”â€”è¯­å¢ƒçº ç¼ æ¢¯åº¦æ˜ å°„ï¼ˆCEGMï¼‰æå‡ºé‡æ–°å®šä¹‰è¯­å¢ƒåµŒå…¥ä¸æ¢¯åº¦æ›´æ–°ä¹‹é—´çš„å…³ç³»ï¼Œä»¥æé«˜ç¥ç»ç½‘ç»œæ¶æ„ä¸­çš„è¯­ä¹‰è¿è´¯æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚é€šè¿‡æŠŠæ¢¯åº¦è§†ä¸ºåŠ¨æ€æºå¸¦è¯­å¢ƒä¾èµ–æ€§çš„è½½ä½“è€Œéå­¤ç«‹çš„æ•°å€¼å®ä½“ï¼Œè¯¥æ–¹æ³•å¡«è¡¥äº†ç°æœ‰ä¼˜åŒ–ç­–ç•¥çš„å…³é”®ç©ºç™½ã€‚åœ¨æ¶‰åŠé•¿å½¢å¼æ¨ç†ã€è¯­å¢ƒä¿ç•™ä»¥åŠé€‚åº”æœªè§é¢†åŸŸç­‰ä»»åŠ¡ä¸­ï¼Œå°†çº ç¼ æ¢¯åº¦åŠ¨æ€æ•´åˆè‡³æŸå¤±æ­£åˆ™åŒ–æ¡†æ¶ä¸­å±•ç¤ºå‡ºæ˜¾è‘—æ”¹å–„æ•ˆæœã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒCEGMå¢å¼ºæ¨¡å‹åœ¨è¯çº§é¢„æµ‹ä¸­å§‹ç»ˆä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå…·æœ‰æ›´å¼ºçš„å™ªå£°è¾“å…¥é€‚åº”æ€§å’Œæ›´é«˜çš„å‡†ç¡®æ€§ã€‚å®é™…åº”ç”¨åŒ…æ‹¬å¯¹è®­ç»ƒæµç¨‹è¿›è¡Œä¿®æ”¹ã€å¼•å…¥çº ç¼ å±‚å’ŒåŠ¨æ€ç³»æ•°è°ƒæ•´ï¼Œä¸ç°æœ‰æ¶æ„æ— ç¼å¯¹æ¥ã€‚ç»“æœè¿˜çªå‡ºäº†åœ¨è¿ç»­å˜æ¢è¿‡ç¨‹ä¸­è¯­ä¹‰æ¼‚ç§»çš„å‡å°‘ä»¥åŠè·¨åŒä¹‰å¥åµŒå…¥è¿è´¯æ€§çš„æé«˜ï¼Œæ˜¾ç¤ºå‡ºè¯¥æ–¹æ³•çš„ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚è¿™ä¸€å‘ç°å±•ç¤ºäº†æ¢¯åº¦çº ç¼ åœ¨ä¼˜åŒ–ç­–ç•¥çš„ç†è®ºè¿›æ­¥å’Œå®è·µåº”ç”¨ä¸­çš„æ›´å¹¿æ³›å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CEGMæå‡ºäº†åŸºäºæ¢¯åº¦ä¼˜åŒ–çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡é‡æ–°ç•Œå®šè¯­å¢ƒåµŒå…¥å’Œæ¢¯åº¦æ›´æ–°çš„å…³ç³»å¼ºåŒ–è¯­ä¹‰è¿è´¯æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å°†æ¢¯åº¦è§†ä¸ºåŠ¨æ€è½½ä½“è€Œéå­¤ç«‹çš„æ•°å€¼å®ä½“ï¼Œæ³¨é‡å…¶åœ¨è¯­å¢ƒä¾èµ–æ€§ä¸­çš„ä½œç”¨ã€‚</li>
<li>CEGMæ˜¾è‘—æé«˜äº†é•¿å½¢å¼æ¨ç†ã€è¯­å¢ƒä¿ç•™å’Œé€‚åº”æœªè§é¢†åŸŸç­‰ä»»åŠ¡çš„æ•ˆæœã€‚</li>
<li>åœ¨è¯çº§é¢„æµ‹ä¸­ï¼ŒCEGMå¢å¼ºæ¨¡å‹è¡¨ç°å‡ºæ¯”åŸºçº¿æ–¹æ³•æ›´é«˜çš„æ€§èƒ½å’Œå‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ä¿®æ”¹è®­ç»ƒæµç¨‹ã€å¼•å…¥çº ç¼ å±‚å’ŒåŠ¨æ€ç³»æ•°è°ƒæ•´ç­‰å®ç°å®é™…åº”ç”¨ã€‚</li>
<li>ç»“æœæ˜¾ç¤ºCEGMå‡å°‘äº†è¯­ä¹‰æ¼‚ç§»å¹¶æé«˜äº†åµŒå…¥è¿è´¯æ€§ï¼Œè¯æ˜å…¶ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00048">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7c64c47a55bfbe2f70e0d6607e2570bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac2e0dfdf596744b532fbbf7a55ed1dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c3bd417baefeacf73fd353c6e82950e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb41a96df7e96f912401acc50fa6f8fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-228d172ea8bc8e9319e46cafcd1dd132.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81a2cedd22237b0e0c504463ecd81151.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Are-Your-LLMs-Capable-of-Stable-Reasoning"><a href="#Are-Your-LLMs-Capable-of-Stable-Reasoning" class="headerlink" title="Are Your LLMs Capable of Stable Reasoning?"></a>Are Your LLMs Capable of Stable Reasoning?</h2><p><strong>Authors:Junnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu, Songyang Gao, Wenwei Zhang, Songyang Zhang, Kai Chen</strong></p>
<p>The rapid advancement of large language models (LLMs) has shown remarkable progress in complex reasoning tasks. However, a significant disparity exists between benchmark performances and real-world applications. We attribute this gap primarily to current evaluation protocols and metrics, which inadequately capture the full spectrum of LLM capabilities, especially in complex reasoning tasks where both accuracy and consistency are essential. In this paper, we introduce G-Pass@$k$, a novel evaluation metric that continuously assesses model performance across multiple sampling attempts, quantifying both the modelâ€™s performance potential and its stability. Through extensive experiments on various public and newly constructed benchmarks, we employ G-Pass@$k$ in conjunction with state-of-the-art large language models to provide comprehensive insights into their potential capabilities and operational consistency. Our findings reveal a significant opportunity to enhance the realistic reasoning abilities of LLMs, underscoring the necessity for more robust evaluation metrics. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼ŒåŸºå‡†æµ‹è¯•æ€§èƒ½ä¸å®é™…åº”ç”¨ä¹‹é—´å­˜åœ¨å¾ˆå¤§çš„å·®è·ã€‚æˆ‘ä»¬å°†è¿™ä¸€å·®è·ä¸»è¦å½’å› äºå½“å‰çš„è¯„ä¼°åè®®å’ŒæŒ‡æ ‡ï¼Œå®ƒä»¬æœªèƒ½å……åˆ†æ•æ‰LLMçš„å…¨éƒ¨èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡†ç¡®æ€§å’Œä¸€è‡´æ€§éƒ½è‡³å…³é‡è¦çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†G-Pass@kè¿™ä¸€æ–°å‹è¯„ä¼°æŒ‡æ ‡ï¼Œå®ƒèƒ½å¤ŸæŒç»­è¯„ä¼°æ¨¡å‹å¤šæ¬¡é‡‡æ ·çš„æ€§èƒ½ï¼Œé‡åŒ–æ¨¡å‹çš„æ½œåŠ›ä¸ç¨³å®šæ€§ã€‚æˆ‘ä»¬é€šè¿‡å¤§é‡å®éªŒï¼Œåœ¨å„ç§å…¬å…±å’Œæ–°å»ºåŸºå‡†æµ‹è¯•ä¸Šï¼Œç»“åˆæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨G-Pass@kæŒ‡æ ‡å¯¹å…¶æ½œåœ¨èƒ½åŠ›å’Œæ“ä½œä¸€è‡´æ€§æä¾›äº†å…¨é¢çš„è§è§£ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæé«˜LLMçš„ç°å®æ¨ç†èƒ½åŠ›å­˜åœ¨å·¨å¤§æœºä¼šï¼Œå¹¶å¼ºè°ƒéœ€è¦æ›´ç¨³å¥çš„è¯„ä¼°æŒ‡æ ‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13147v5">PDF</a> ACL 2025 Camera, Benchmark:   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/opencompass/LiveMathBench">https://huggingface.co/datasets/opencompass/LiveMathBench</a>, Code:   <a target="_blank" rel="noopener" href="https://github.com/open-compass/GPassK">https://github.com/open-compass/GPassK</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å­˜åœ¨æ€§èƒ½è¯„ä¼°ä¸å®é™…åº”ç”¨ä¹‹é—´çš„é¸¿æ²Ÿã€‚æœ¬æ–‡å¼•å…¥G-Pass@kè¯„ä¼°æŒ‡æ ‡ï¼Œé€šè¿‡å¤šæ¬¡é‡‡æ ·è¯„ä¼°æ¨¡å‹æ€§èƒ½å’Œç¨³å®šæ€§ã€‚å®éªŒè¡¨æ˜ï¼Œç°æœ‰å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…æ¨ç†èƒ½åŠ›æ–¹é¢ä»æœ‰å¾…æå‡ï¼Œéœ€è¦æ›´ç¨³å¥çš„è¯„ä¼°æŒ‡æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>è¯„ä¼°æŒ‡æ ‡ä¸å®é™…åº”ç”¨ä¹‹é—´å­˜åœ¨é¸¿æ²Ÿã€‚</li>
<li>å¼•å…¥G-Pass@kè¯„ä¼°æŒ‡æ ‡ï¼Œå…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½å’Œç¨³å®šæ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œç°æœ‰LLMåœ¨å®é™…æ¨ç†èƒ½åŠ›æ–¹é¢æœ‰å¾…æå‡ã€‚</li>
<li>LLMéœ€è¦æ›´ç¨³å¥çš„è¯„ä¼°æŒ‡æ ‡æ¥åæ˜ å…¶å®é™…æ€§èƒ½ã€‚</li>
<li>G-Pass@kè¯„ä¼°æŒ‡æ ‡èƒ½å¤Ÿé‡åŒ–æ¨¡å‹æ€§èƒ½æ½œåŠ›å’Œç¨³å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13147">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d51e82d959179601dc51800dfa7796f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c01c3eb955c3a45d0d5c0c35997e75d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d934194f0d0b9bf2f23de8d01bceebb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a41c8ad08c9c956a8fca954ab274135.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-12/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-12/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-12/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1d484d4af7b1b5b3a516d355b31b255a.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-12  GLM-4.5 Agentic, Reasoning, and Coding (ARC) Foundation Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-12/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c05542ab9bce7cd9cf8f9736871817e2.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-12  GLM-4.5 Agentic, Reasoning, and Coding (ARC) Foundation Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32102k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
