<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-05  MOTIF Modular Thinking via Reinforcement Fine-tuning in LLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-0a452a52b6d593509c4f9c3aeb6565e5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-05-æ›´æ–°"><a href="#2025-07-05-æ›´æ–°" class="headerlink" title="2025-07-05 æ›´æ–°"></a>2025-07-05 æ›´æ–°</h1><h2 id="MOTIF-Modular-Thinking-via-Reinforcement-Fine-tuning-in-LLMs"><a href="#MOTIF-Modular-Thinking-via-Reinforcement-Fine-tuning-in-LLMs" class="headerlink" title="MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs"></a>MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs</h2><p><strong>Authors:Purbesh Mitra, Sennur Ulukus</strong></p>
<p>Recent advancements in the reasoning capabilities of large language models (LLMs) show that employing group relative policy optimization (GRPO) algorithm for reinforcement learning (RL) training allows the models to use more thinking&#x2F;reasoning tokens for generating better responses. However, LLMs can generate only a finite amount of tokens while maintaining attention to the previously generated tokens. This limit, also known as the context size of an LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens. To think beyond the limit of context size, an LLM must employ a modular thinking strategy to reason over multiple rounds. In this work, we propose $\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ â€“ an RL training method for generating thinking tokens in multiple rounds, effectively allowing the model to think with additional context size. We trained the open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our experiments show 3.8% and 3.3% improvements over vanilla GRPO based training in the respective benchmarks. Furthermore, this improvement was achieved with only 15% of samples, thus demonstrating sample efficiency of MOTIF. Our code and models are available at <a target="_blank" rel="noopener" href="https://github.com/purbeshmitra/MOTIF">https://github.com/purbeshmitra/MOTIF</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/purbeshmitra/MOTIF">https://huggingface.co/purbeshmitra/MOTIF</a>, respectively. </p>
<blockquote>
<p>è¿‘æœŸå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢çš„è¿›å±•è¡¨æ˜ï¼Œé‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒï¼Œå¯ä»¥ä½¿æ¨¡å‹ä½¿ç”¨æ›´å¤šçš„æ€è€ƒ&#x2F;æ¨ç†ä»¤ç‰Œæ¥ç”Ÿæˆæ›´å¥½çš„å›å¤ã€‚ç„¶è€Œï¼ŒLLMåªèƒ½åœ¨ç»´æŒå¯¹å…ˆå‰ç”Ÿæˆä»¤ç‰Œçš„æ³¨æ„åŠ›æ—¶ç”Ÿæˆæœ‰é™æ•°é‡çš„ä»¤ç‰Œã€‚è¿™ä¸ªé™åˆ¶ï¼Œä¹Ÿç§°ä¸ºLLMçš„ä¸Šä¸‹æ–‡å¤§å°ï¼Œæ˜¯LLMåœ¨ä»»æ„å¤§é‡ä»¤ç‰Œä¸Šè¿›è¡Œæ¨ç†çš„ç“¶é¢ˆã€‚ä¸ºäº†è¶…è¶Šä¸Šä¸‹æ–‡å¤§å°çš„é™åˆ¶ï¼ŒLLMå¿…é¡»é‡‡ç”¨æ¨¡å—åŒ–æ€è€ƒç­–ç•¥ï¼Œè¿›è¡Œå¤šè½®æ¨ç†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>MOTIFï¼šé€šè¿‡å¼ºåŒ–å¾®è°ƒå®ç°æ¨¡å—åŒ–æ€è€ƒ</strong>â€”â€”ä¸€ç§ç”¨äºç”Ÿæˆå¤šè½®æ€è€ƒä»¤ç‰Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ³•ï¼Œæœ‰æ•ˆåœ°ä½¿æ¨¡å‹èƒ½å¤Ÿæ€è€ƒé¢å¤–çš„ä¸Šä¸‹æ–‡å¤§å°ã€‚æˆ‘ä»¬å¯¹å¼€æºæ¨¡å‹Qwen2.5-3B-Instructè¿›è¡Œäº†GSM8Kæ•°æ®é›†çš„å‚æ•°æ•ˆç‡å¾®è°ƒè®­ç»ƒï¼Œå¹¶åœ¨MATH500å’ŒAIME2024åŸºå‡†æµ‹è¯•ä¸Šæµ‹è¯•äº†å…¶å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨å„è‡ªçš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸å¯¹äºåŸºäºGRPOçš„æ™®é€šè®­ç»ƒï¼Œå…¶å‡†ç¡®ç‡æé«˜äº†3.8%å’Œ3.3%ã€‚æ­¤å¤–ï¼Œè¿™ä¸€æ”¹è¿›ä»…ä½¿ç”¨äº†15%çš„æ ·æœ¬ï¼Œä»è€Œè¯æ˜äº†MOTIFçš„æ ·æœ¬æ•ˆç‡ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹åˆ†åˆ«ä½äº<a target="_blank" rel="noopener" href="https://github.com/purbeshmitra/MOTIF">https://github.com/purbeshmitra/MOTIF</a>å’Œ<a target="_blank" rel="noopener" href="https://huggingface.co/purbeshmitra/MOTIF%E3%80%82">https://huggingface.co/purbeshmitra/MOTIFã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02851v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿåœ¨ç”Ÿæˆå“åº”æ—¶åˆ©ç”¨æ›´å¤šçš„æ€è€ƒ&#x2F;æ¨ç†æ ‡è®°ã€‚ç„¶è€Œï¼ŒLLMåœ¨ç»´æŒå¯¹å…ˆå‰ç”Ÿæˆæ ‡è®°çš„æ³¨æ„åŠ›æ—¶åªèƒ½ç”Ÿæˆæœ‰é™æ•°é‡çš„æ ‡è®°ã€‚è¿™ç§é™åˆ¶æ˜¯LLMä½¿ç”¨ä»»æ„å¤§é‡æ ‡è®°è¿›è¡Œæ¨ç†çš„ç“¶é¢ˆã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMOTIFçš„å¼ºåŒ–å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡å¤šè½®ç”Ÿæˆæ€è€ƒæ ‡è®°ï¼Œæœ‰æ•ˆåœ°ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨é¢å¤–çš„ä¸Šä¸‹æ–‡å¤§å°ä¸­è¿›è¡Œæ€è€ƒã€‚æˆ‘ä»¬åœ¨GSM8Kæ•°æ®é›†ä¸Šè®­ç»ƒäº†å¼€æºæ¨¡å‹Qwen2.5-3B-Instructï¼Œå¹¶åœ¨MATH500å’ŒAIME2024åŸºå‡†æµ‹è¯•ä¸Šæµ‹è¯•äº†å…¶å‡†ç¡®æ€§ã€‚å®éªŒæ˜¾ç¤ºï¼Œç›¸è¾ƒäºåŸºäºGRPOçš„è®­ç»ƒæ–¹æ³•ï¼ŒMOTIFåœ¨MATH500å’ŒAIME2024çš„åŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«æé«˜äº†3.8ï¼…å’Œ3.3ï¼…ã€‚æ­¤å¤–ï¼ŒMOTIFä»…ä½¿ç”¨15ï¼…çš„æ ·æœ¬å®ç°äº†è¿™ç§æ”¹è¿›ï¼Œè¯æ˜äº†å…¶æ ·æœ¬æ•ˆç‡ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹åˆ†åˆ«ä½äº<a target="_blank" rel="noopener" href="https://github.com/purbeshmitra/MOTIF%E5%92%8Chttps://huggingface.co/purbeshmitra/MOTIF%E3%80%82">https://github.com/purbeshmitra/MOTIFå’Œhttps://huggingface.co/purbeshmitra/MOTIFã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMé‡‡ç”¨GRPOç®—æ³•è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒåï¼Œå¯ä»¥ä½¿ç”¨æ›´å¤šæ€è€ƒ&#x2F;æ¨ç†æ ‡è®°ç”Ÿæˆå“åº”ã€‚</li>
<li>LLMå­˜åœ¨ä¸Šä¸‹æ–‡å¤§å°é™åˆ¶ï¼Œé™åˆ¶äº†å…¶ä½¿ç”¨å¤§é‡æ ‡è®°è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ã€‚</li>
<li>MOTIFæ˜¯ä¸€ç§é€šè¿‡å¼ºåŒ–å¾®è°ƒå®ç°æ¨¡å—åŒ–æ€è€ƒçš„ç­–ç•¥ï¼Œå…è®¸æ¨¡å‹åœ¨å¤šè½®ä¸­è¿›è¡Œæ¨ç†ã€‚</li>
<li>åœ¨MATH500å’ŒAIME2024åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMOTIFç›¸è¾ƒäºåŸºäºGRPOçš„è®­ç»ƒæ–¹æ³•æœ‰æ‰€æé«˜ã€‚</li>
<li>MOTIFä»…ä½¿ç”¨å°‘é‡æ ·æœ¬å³å¯å®ç°æ”¹è¿›ï¼Œæ˜¾ç¤ºå‡ºå…¶æ ·æœ¬æ•ˆç‡ã€‚</li>
<li>ä»£ç å’Œæ¨¡å‹å¯åœ¨æŒ‡å®šé“¾æ¥æ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02851">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eb2362a2a325b43cb6e1d15e9cb70983.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e92e02b18d116892753304798db90cf5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b4d610b4c44b6c3650c4cbe979d66c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d075a7f093a8e9ea0cc4533ba4da973.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89447d94a23b2f75118426fb3ff6e780.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="StepHint-Multi-level-Stepwise-Hints-Enhance-Reinforcement-Learning-to-Reason"><a href="#StepHint-Multi-level-Stepwise-Hints-Enhance-Reinforcement-Learning-to-Reason" class="headerlink" title="StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to   Reason"></a>StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to   Reason</h2><p><strong>Authors:Kaiyi Zhang, Ang Lv, Jinpeng Li, Yongbo Wang, Feng Wang, Haoyuan Hu, Rui Yan</strong></p>
<p>Reinforcement learning with verifiable rewards (RLVR) is a promising approach for improving the complex reasoning abilities of large language models (LLMs). However, current RLVR methods face two significant challenges: the near-miss reward problem, where a small mistake can invalidate an otherwise correct reasoning process, greatly hindering training efficiency; and exploration stagnation, where models tend to focus on solutions within their <code>comfort zone,&#39;&#39; lacking the motivation to explore potentially more effective alternatives. To address these challenges, we propose StepHint, a novel RLVR algorithm that utilizes multi-level stepwise hints to help models explore the solution space more effectively. StepHint generates valid reasoning chains from stronger models and partitions these chains into reasoning steps using our proposed adaptive partitioning method. The initial few steps are used as hints, and simultaneously, multiple-level hints (each comprising a different number of steps) are provided to the model. This approach directs the model&#39;s exploration toward a promising solution subspace while preserving its flexibility for independent exploration. By providing hints, StepHint mitigates the near-miss reward problem, thereby improving training efficiency. Additionally, the external reasoning pathways help the model develop better reasoning abilities, enabling it to move beyond its </code>comfort zoneâ€™â€™ and mitigate exploration stagnation. StepHint outperforms competitive RLVR enhancement methods across six mathematical benchmarks, while also demonstrating superior generalization and excelling over baselines on out-of-domain benchmarks. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¯æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤æ‚æ¨ç†èƒ½åŠ›çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œå½“å‰çš„RLVRæ–¹æ³•é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯æ¥è¿‘å¥–åŠ±é—®é¢˜ï¼Œå³ä½¿æ˜¯å¾ˆå°çš„é”™è¯¯ä¹Ÿä¼šä½¿æ­£ç¡®çš„æ¨ç†è¿‡ç¨‹å¤±æ•ˆï¼Œæå¤§åœ°é˜»ç¢äº†è®­ç»ƒæ•ˆç‡ï¼›äºŒæ˜¯æ¢ç´¢åœæ»é—®é¢˜ï¼Œæ¨¡å‹å¾€å¾€åªå…³æ³¨å…¶â€œèˆ’é€‚åŒºâ€å†…çš„è§£å†³æ–¹æ¡ˆï¼Œç¼ºä¹æ¢ç´¢å¯èƒ½æ›´æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆçš„åŠ¨æœºã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†StepHintè¿™ä¸€æ–°å‹RLVRç®—æ³•ï¼Œå®ƒåˆ©ç”¨å¤šå±‚æ¬¡é€æ­¥æç¤ºæ¥å¸®åŠ©æ¨¡å‹æ›´æœ‰æ•ˆåœ°æ¢ç´¢è§£ç©ºé—´ã€‚StepHintä»æ›´å¼ºå¤§çš„æ¨¡å‹ä¸­ç”Ÿæˆæœ‰æ•ˆçš„æ¨ç†é“¾ï¼Œå¹¶ä½¿ç”¨æˆ‘ä»¬æå‡ºçš„è‡ªé€‚åº”åˆ†åŒºæ–¹æ³•å¯¹è¿™äº›é“¾è¿›è¡Œæ¨ç†æ­¥éª¤åˆ’åˆ†ã€‚æœ€åˆçš„å‡ ä¸ªæ­¥éª¤è¢«ç”¨ä½œæç¤ºï¼ŒåŒæ—¶å‘æ¨¡å‹æä¾›ä¸åŒçº§åˆ«çš„æç¤ºï¼ˆæ¯ä¸ªåŒ…å«ä¸åŒæ•°é‡çš„æ­¥éª¤ï¼‰ã€‚è¿™ç§æ–¹æ³•å¯ä»¥å¼•å¯¼æ¨¡å‹çš„æ¢ç´¢æœç€æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆå­ç©ºé—´è¿›è¡Œï¼ŒåŒæ—¶ä¿æŒå…¶ç‹¬ç«‹æ¢ç´¢çš„çµæ´»æ€§ã€‚é€šè¿‡æä¾›æç¤ºï¼ŒStepHintç¼“è§£äº†æ¥è¿‘å¥–åŠ±é—®é¢˜ï¼Œä»è€Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚æ­¤å¤–ï¼Œå¤–éƒ¨æ¨ç†è·¯å¾„æœ‰åŠ©äºæ¨¡å‹å‘å±•æ›´å¥½çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿè¶…è¶Šå…¶â€œèˆ’é€‚åŒºâ€ï¼Œå¹¶ç¼“è§£æ¢ç´¢åœæ»é—®é¢˜ã€‚åœ¨å…­ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒStepHintçš„è¡¨ç°ä¼˜äºå…¶ä»–ç«äº‰æ€§RLVRå¢å¼ºæ–¹æ³•ï¼ŒåŒæ—¶åœ¨åŸŸå¤–åŸºå‡†æµ‹è¯•ä¸­æ˜¾ç¤ºå‡ºå“è¶Šæ³›åŒ–èƒ½åŠ›å’Œè¶…è¶ŠåŸºçº¿çš„æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02841v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ é…åˆå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰RLVRæ–¹æ³•é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šè¿‘é”™å¥–åŠ±é—®é¢˜å’Œæ¢ç´¢åœæ»é—®é¢˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºStepHintï¼Œä¸€ç§åˆ©ç”¨å¤šå±‚æ¬¡é€æ­¥æç¤ºçš„æ–°å‹RLVRç®—æ³•ï¼Œä»¥æ›´æœ‰æ•ˆåœ°å¸®åŠ©æ¨¡å‹æ¢ç´¢è§£ç©ºé—´ã€‚StepHinté€šè¿‡è‡ªé€‚åº”åˆ†åŒºæ–¹æ³•å°†å¼ºæ¨¡å‹çš„åˆç†é“¾åˆ†å‰²æˆæ¨ç†æ­¥éª¤ï¼Œå¹¶æä¾›å¤šçº§æç¤ºä»¥å¼•å¯¼æ¨¡å‹æ¢ç´¢æœ‰å‰æ™¯çš„è§£ç©ºé—´ã€‚è¿™è§£å†³äº†è¿‘é”™å¥–åŠ±é—®é¢˜ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œå¹¶å¸®åŠ©æ¨¡å‹è¶…è¶Šèˆ’é€‚åŒºè¿›è¡Œæ¢ç´¢ã€‚åœ¨å…­ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒStepHintè¡¨ç°ä¼˜äºå…¶ä»–RLVRå¢å¼ºæ–¹æ³•ï¼Œå¹¶åœ¨åŸŸå¤–åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå“è¶Šæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLVRæ–¹æ³•å¯¹äºæå‡LLMçš„å¤æ‚æ¨ç†èƒ½åŠ›å…·æœ‰æ½œåŠ›ã€‚</li>
<li>å½“å‰RLVRæ–¹æ³•é¢ä¸´è¿‘é”™å¥–åŠ±å’Œæ¢ç´¢åœæ»ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>StepHintæ˜¯ä¸€ç§æ–°å‹RLVRç®—æ³•ï¼Œåˆ©ç”¨å¤šå±‚æ¬¡é€æ­¥æç¤ºå¸®åŠ©æ¨¡å‹æ›´æœ‰æ•ˆåœ°æ¢ç´¢è§£ç©ºé—´ã€‚</li>
<li>StepHinté€šè¿‡è‡ªé€‚åº”åˆ†åŒºæ–¹æ³•ç”Ÿæˆåˆç†é“¾ï¼Œå¹¶æä¾›å¤šçº§æç¤ºä»¥å¼•å¯¼æ¨¡å‹æ¢ç´¢ã€‚</li>
<li>StepHintè§£å†³äº†è¿‘é”™å¥–åŠ±é—®é¢˜ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>StepHintå¸®åŠ©æ¨¡å‹è¶…è¶Šèˆ’é€‚åŒºè¿›è¡Œæ¢ç´¢ï¼Œè¡¨ç°å‡ºä¼˜ç§€çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02841">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-46cfa5cc531ec0a3d795c75389fb58a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc18672d62d0c7dfbbeb2276b6de2764.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af41ed51955f04bc86311b4a6261fff6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Multimodal-Mathematical-Reasoning-with-Diverse-Solving-Perspective"><a href="#Multimodal-Mathematical-Reasoning-with-Diverse-Solving-Perspective" class="headerlink" title="Multimodal Mathematical Reasoning with Diverse Solving Perspective"></a>Multimodal Mathematical Reasoning with Diverse Solving Perspective</h2><p><strong>Authors:Wenhao Shi, Zhiqiang Hu, Yi Bin, Yang Yang, See-Kiong Ng, Heng Tao Shen</strong></p>
<p>Recent progress in large-scale reinforcement learning (RL) has notably enhanced the reasoning capabilities of large language models (LLMs), especially in mathematical domains. However, current multimodal LLMs (MLLMs) for mathematical reasoning often rely on one-to-one image-text pairs and single-solution supervision, overlooking the diversity of valid reasoning perspectives and internal reflections. In this work, we introduce MathV-DP, a novel dataset that captures multiple diverse solution trajectories for each image-question pair, fostering richer reasoning supervision. We further propose Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and enhanced via group relative policy optimization (GRPO), a rule-based RL approach that integrates correctness discrimination and diversity-aware reward functions. Our method emphasizes learning from varied reasoning perspectives and distinguishing between correct yet distinct solutions. Extensive experiments on the MathVistaâ€™s minitest and Math-V benchmarks demonstrate that Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and generative diversity, highlighting the importance of incorporating diverse perspectives and reflective reasoning in multimodal mathematical reasoning. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è¿›æ­¥æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦é¢†åŸŸã€‚ç„¶è€Œï¼Œå½“å‰ç”¨äºæ•°å­¦æ¨ç†çš„å¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰é€šå¸¸ä¾èµ–äºä¸€å¯¹ä¸€çš„å›¾åƒæ–‡æœ¬å¯¹å’Œå•ä¸€è§£å†³æ–¹æ¡ˆçš„ç›‘ç£ï¼Œå¿½è§†äº†æœ‰æ•ˆçš„æ¨ç†è§’åº¦å’Œå†…éƒ¨åæ€çš„å¤šæ ·æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†MathV-DPï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ•°æ®é›†ï¼Œå®ƒæ•æ‰äº†æ¯ä¸ªå›¾åƒé—®é¢˜å¯¹çš„å¤šé‡å¤šæ ·åŒ–è§£å†³æ–¹æ¡ˆè½¨è¿¹ï¼Œä¸ºæ›´ä¸°å¯Œçš„æ¨ç†ç›‘ç£æä¾›äº†æ”¯æŒã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†Qwen-VL-DPæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºQwen-VLæ„å»ºï¼Œé€šè¿‡æœ‰ç›‘ç£å­¦ä¹ è¿›è¡Œå¾®è°ƒï¼Œå¹¶é€šè¿‡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•â€”â€”ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œå¢å¼ºï¼Œè¯¥æ–¹æ³•èåˆäº†æ­£ç¡®æ€§åˆ¤åˆ«å’Œå¤šæ ·æ€§æ„ŸçŸ¥å¥–åŠ±åŠŸèƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼ºè°ƒä»å¤šç§æ¨ç†è§’åº¦å­¦ä¹ ï¼Œå¹¶åŒºåˆ†æ­£ç¡®ä½†ä¸åŒçš„è§£å†³æ–¹æ¡ˆã€‚åœ¨MathVistaçš„å¾®å°æµ‹è¯•å’ŒMath-VåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒQwen-VL-DPåœ¨å‡†ç¡®æ€§å’Œç”Ÿæˆå¤šæ ·æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆå‰çš„åŸºç¡€MLLMï¼Œçªæ˜¾äº†åœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä¸­èå…¥å¤šæ ·æ€§å’Œåæ€æ¨ç†çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02804v1">PDF</a> 8 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦é¢†åŸŸã€‚ç„¶è€Œï¼Œå½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä¸Šå¸¸å¸¸ä¾èµ–äºå•ä¸€å›¾åƒæ–‡æœ¬å¯¹å’Œå•ä¸€è§£å†³æ–¹æ¡ˆçš„ç›‘ç£ï¼Œå¿½è§†äº†æœ‰æ•ˆçš„æ¨ç†è§†è§’çš„å¤šæ ·æ€§ä»¥åŠå†…éƒ¨åæ€ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†MathV-DPæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†èƒ½å¤Ÿæ•æ‰æ¯ä¸ªå›¾åƒé—®é¢˜å¯¹çš„å¤šä¸ªä¸åŒè§£å†³æ–¹æ¡ˆè½¨è¿¹ï¼Œä¸ºæ›´ä¸°å¯Œçš„æ¨ç†ç›‘ç£æä¾›æ”¯æŒã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æå‡ºäº†Qwen-VL-DPæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºQwen-VLæ„å»ºï¼Œé€šè¿‡ç›‘ç£å­¦ä¹ è¿›è¡Œå¾®è°ƒï¼Œå¹¶é€šè¿‡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•â€”â€”ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œå¢å¼ºï¼Œè¯¥æ–¹æ³•èåˆäº†æ­£ç¡®æ€§é‰´åˆ«å’Œå¤šæ ·æ€§æ„ŸçŸ¥å¥–åŠ±å‡½æ•°ã€‚è¯¥æ–¹æ³•é‡è§†ä»å¤šæ ·åŒ–çš„æ¨ç†è§†è§’å­¦ä¹ ï¼Œå¹¶åŒºåˆ†æ­£ç¡®ä½†ä¸åŒçš„è§£å†³æ–¹æ¡ˆã€‚åœ¨MathVistaçš„å¾®å°æµ‹è¯•å’ŒMath-VåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒQwen-VL-DPåœ¨å‡†ç¡®ç‡å’Œç”Ÿæˆå¤šæ ·æ€§ä¸Šæ˜¾è‘—ä¼˜äºå…ˆå‰çš„åŸºç¡€å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œå‡¸æ˜¾äº†èå…¥å¤šæ ·è§†è§’å’Œåæ€æ¨ç†åœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¼ºåŒ–å­¦ä¹ æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä¸Šç¼ºä¹å¤šæ ·æ€§å’Œå†…éƒ¨åæ€ã€‚</li>
<li>ä»‹ç»äº†MathV-DPæ•°æ®é›†ï¼Œæ”¯æŒæ›´ä¸°å¯Œã€å¤šæ ·åŒ–çš„æ•°å­¦æ¨ç†ç›‘ç£ã€‚</li>
<li>æå‡ºäº†Qwen-VL-DPæ¨¡å‹ï¼ŒåŸºäºQwen-VLæ„å»ºï¼Œèåˆäº†ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>Qwen-VL-DPæ¨¡å‹é‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œé›†æˆæ­£ç¡®æ€§é‰´åˆ«å’Œå¤šæ ·æ€§æ„ŸçŸ¥å¥–åŠ±å‡½æ•°ã€‚</li>
<li>Qwen-VL-DPæ¨¡å‹é‡è§†ä»å¤šæ ·åŒ–æ¨ç†è§†è§’å­¦ä¹ ï¼Œå¹¶åŒºåˆ†æ­£ç¡®ä½†ä¸åŒçš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-caf74a71370469e31a06e66ab64769ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c76fd42b8f50b62a7b8a887d7a8d164.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1a1d1d3ece43c265f75e1cd56565b60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13d38d329e91ff4e075dc865670af37a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Is-Reasoning-All-You-Need-Probing-Bias-in-the-Age-of-Reasoning-Language-Models"><a href="#Is-Reasoning-All-You-Need-Probing-Bias-in-the-Age-of-Reasoning-Language-Models" class="headerlink" title="Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language   Models"></a>Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language   Models</h2><p><strong>Authors:Riccardo Cantini, Nicola Gabriele, Alessio Orsino, Domenico Talia</strong></p>
<p>Reasoning Language Models (RLMs) have gained traction for their ability to perform complex, multi-step reasoning tasks through mechanisms such as Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these capabilities promise improved reliability, their impact on robustness to social biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark, originally designed for Large Language Models (LLMs), to investigate the adversarial robustness of RLMs to bias elicitation. We systematically evaluate state-of-the-art RLMs across diverse sociocultural dimensions, using an LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak techniques to assess the strength of built-in safety mechanisms. Our evaluation addresses three key questions: (i) how the introduction of reasoning capabilities affects model fairness and robustness; (ii) whether models fine-tuned for reasoning exhibit greater safety than those relying on CoT prompting at inference time; and (iii) how the success rate of jailbreak attacks targeting bias elicitation varies with the reasoning mechanisms employed. Our findings reveal a nuanced relationship between reasoning capabilities and bias safety. Surprisingly, models with explicit reasoning, whether via CoT prompting or fine-tuned reasoning traces, are generally more vulnerable to bias elicitation than base models without such mechanisms, suggesting reasoning may unintentionally open new pathways for stereotype reinforcement. Reasoning-enabled models appear somewhat safer than those relying on CoT prompting, which are particularly prone to contextual reframing attacks through storytelling prompts, fictional personas, or reward-shaped instructions. These results challenge the assumption that reasoning inherently improves robustness and underscore the need for more bias-aware approaches to reasoning design. </p>
<blockquote>
<p>æ¨ç†è¯­è¨€æ¨¡å‹ï¼ˆRLMsï¼‰é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæˆ–ç²¾ç»†è°ƒæ•´åçš„æ¨ç†è½¨è¿¹ç­‰æœºåˆ¶ï¼Œèƒ½å¤Ÿæ‰§è¡Œå¤æ‚çš„å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ï¼Œä»è€Œè·å¾—äº†å®é™…åº”ç”¨ã€‚å°½ç®¡è¿™äº›èƒ½åŠ›å¸¦æ¥äº†æ›´é«˜çš„å¯é æ€§æ‰¿è¯ºï¼Œä½†å®ƒä»¬å¯¹ç¤¾ä¼šåè§ç¨³å¥æ€§çš„å½±å“ä»ç„¶ä¸æ¸…æ¥šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸“é—¨ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è®¾è®¡çš„CLEAR-BiasåŸºå‡†æµ‹è¯•ï¼Œæ¥æ¢ç©¶RLMså¯¹æŠ—åè§è¯±å¯¼çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†æœ€å…ˆè¿›çš„çŠ¶æ€RLMsåœ¨å¤šç§ç¤¾ä¼šæ–‡åŒ–ç»´åº¦ä¸Šçš„è¡¨ç°ï¼Œé‡‡ç”¨LLMä½œä¸ºæ³•å®˜çš„æ–¹æ³•è¿›è¡Œè‡ªåŠ¨åŒ–å®‰å…¨è¯„åˆ†ï¼Œå¹¶åˆ©ç”¨è¶Šç‹±æŠ€æœ¯æ¥è¯„ä¼°å†…ç½®å®‰å…¨æœºåˆ¶çš„å¼ºåº¦ã€‚æˆ‘ä»¬çš„è¯„ä¼°è§£å†³äº†ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼šï¼ˆiï¼‰æ¨ç†èƒ½åŠ›çš„å¼•å…¥å¦‚ä½•å½±å“æ¨¡å‹çš„å…¬å¹³æ€§å’Œç¨³å¥æ€§ï¼›ï¼ˆiiï¼‰ä¸ºæ¨ç†ç²¾ç»†è°ƒæ•´è¿‡çš„æ¨¡å‹ä¸é‚£äº›åœ¨æ¨ç†æ—¶ä¾èµ–CoTæç¤ºçš„æ¨¡å‹ç›¸æ¯”ï¼Œæ˜¯å¦è¡¨ç°å‡ºæ›´é«˜çš„å®‰å…¨æ€§ï¼›ï¼ˆiiiï¼‰é’ˆå¯¹åè§è¯±å¯¼çš„è¶Šç‹±æ”»å‡»æˆåŠŸç‡å¦‚ä½•éšç€æ‰€ä½¿ç”¨çš„æ¨ç†æœºåˆ¶è€Œå˜åŒ–ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œæ¨ç†èƒ½åŠ›ä¸åè§å®‰å…¨æ€§ä¹‹é—´å­˜åœ¨å¾®å¦™çš„å…³ç³»ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæ— è®ºæ˜¯é€šè¿‡CoTæç¤ºè¿˜æ˜¯ç²¾ç»†è°ƒæ•´çš„æ¨ç†è½¨è¿¹ï¼Œå…·æœ‰æ˜ç¡®æ¨ç†èƒ½åŠ›çš„æ¨¡å‹é€šå¸¸æ›´å®¹æ˜“å—åˆ°åè§è¯±å¯¼çš„å½±å“ï¼Œè¿™è¡¨æ˜æ¨ç†å¯èƒ½ä¼šæ— æ„ä¸­æ‰“å¼€åˆ»æ¿å°è±¡å¼ºåŒ–çš„æ–°é€”å¾„ã€‚å…·æœ‰æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ä¼¼ä¹æ¯”é‚£äº›ä¾èµ–CoTæç¤ºçš„æ¨¡å‹æ›´å®‰å…¨ä¸€äº›ï¼Œåè€…ç‰¹åˆ«å®¹æ˜“å—åˆ°é€šè¿‡è®²æ•…äº‹æç¤ºã€è™šæ„äººç‰©æˆ–å¥–åŠ±å½¢çŠ¶æŒ‡ä»¤è¿›è¡Œä¸Šä¸‹æ–‡é‡æ„æ”»å‡»ã€‚è¿™äº›ç»“æœæŒ‘æˆ˜äº†æ¨ç†å¿…ç„¶æé«˜ç¨³å¥æ€§çš„å‡è®¾ï¼Œå¹¶å¼ºè°ƒéœ€è¦æ›´å¤šå¯¹åè§æœ‰æ„è¯†çš„æ¨ç†è®¾è®¡æ–¹æ³•æ¥æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02799v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡ç ”ç©¶æ¢è®¨äº†Reasoning Language Modelsï¼ˆRLMsï¼‰åœ¨å¤„ç†ç¤¾ä¼šåè§æ–¹é¢çš„ç¨³å¥æ€§é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨CLEAR-Bias benchmarkè¯„ä¼°ä¸åŒç¤¾ä¼šæ–‡åŒ–èƒŒæ™¯ä¸‹çš„å…ˆè¿›RLMsï¼Œå‘ç°å¼•å…¥æ¨ç†èƒ½åŠ›å¯¹æ¨¡å‹çš„å…¬å¹³æ€§å’Œç¨³å¥æ€§äº§ç”Ÿå½±å“ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡CoTæç¤ºæˆ–ç²¾ç»†è°ƒæ•´æ¨ç†è½¨è¿¹çš„æ¨¡å‹ï¼Œåœ¨åè§æ¿€å‘æ–¹é¢é€šå¸¸æ¯”æ²¡æœ‰è¿™äº›æœºåˆ¶çš„åŸºå‡†æ¨¡å‹æ›´å®¹æ˜“å—åˆ°æ”»å‡»ã€‚ç‰¹åˆ«æ˜¯ä¾èµ–CoTæç¤ºçš„æ¨¡å‹æ›´å®¹æ˜“å—åˆ°é€šè¿‡æ•…äº‹å™è¿°ã€è™šæ„è§’è‰²æˆ–å¥–åŠ±æŒ‡å¯¼ç­‰æ–¹å¼è¿›è¡Œçš„ä¸Šä¸‹æ–‡é‡æ„æ”»å‡»ã€‚å› æ­¤ï¼Œå°½ç®¡æ¨ç†èƒ½åŠ›èƒ½æé«˜å¯é æ€§ï¼Œä½†ä¹Ÿéœ€è¦æ›´åŠ å…³æ³¨åå·®æ„ŸçŸ¥çš„æ¨ç†è®¾è®¡ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLMsåœ¨å¤„ç†ç¤¾ä¼šåè§æ–¹é¢çš„ç¨³å¥æ€§å°šå¾…ç ”ç©¶ã€‚</li>
<li>ä½¿ç”¨CLEAR-Bias benchmarkè¯„ä¼°RLMsåœ¨å¤šç§ç¤¾ä¼šæ–‡åŒ–èƒŒæ™¯ä¸‹çš„è¡¨ç°ã€‚</li>
<li>å¼•å…¥æ¨ç†èƒ½åŠ›å½±å“æ¨¡å‹çš„å…¬å¹³æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>é€šè¿‡CoTæç¤ºè¿›è¡Œæ¨ç†çš„æ¨¡å‹æ›´å®¹æ˜“å—åˆ°åè§æ¿€å‘æ”»å‡»ã€‚</li>
<li>ç²¾ç»†è°ƒæ•´æ¨ç†è½¨è¿¹çš„æ¨¡å‹ç›¸è¾ƒäºä¾èµ–CoTæç¤ºçš„æ¨¡å‹æ›´å®‰å…¨ã€‚</li>
<li>ä¸Šä¸‹æ–‡é‡æ„æ”»å‡»ï¼ˆå¦‚é€šè¿‡æ•…äº‹å™è¿°ç­‰ï¼‰é’ˆå¯¹åŸºäºCoTæç¤ºçš„æ¨¡å‹æ›´ä¸ºæœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02799">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fb54283f52b269b43c9f5057ec4a0a42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27cc000c112eab2c0b5273f0afafe001.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AIGI-Holmes-Towards-Explainable-and-Generalizable-AI-Generated-Image-Detection-via-Multimodal-Large-Language-Models"><a href="#AIGI-Holmes-Towards-Explainable-and-Generalizable-AI-Generated-Image-Detection-via-Multimodal-Large-Language-Models" class="headerlink" title="AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image   Detection via Multimodal Large Language Models"></a>AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image   Detection via Multimodal Large Language Models</h2><p><strong>Authors:Ziyin Zhou, Yunpeng Luo, Yuanchen Wu, Ke Sun, Jiayi Ji, Ke Yan, Shouhong Ding, Xiaoshuai Sun, Yunsheng Wu, Rongrong Ji</strong></p>
<p>The rapid development of AI-generated content (AIGC) technology has led to the misuse of highly realistic AI-generated images (AIGI) in spreading misinformation, posing a threat to public information security. Although existing AIGI detection techniques are generally effective, they face two issues: 1) a lack of human-verifiable explanations, and 2) a lack of generalization in the latest generation technology. To address these issues, we introduce a large-scale and comprehensive dataset, Holmes-Set, which includes the Holmes-SFTSet, an instruction-tuning dataset with explanations on whether images are AI-generated, and the Holmes-DPOSet, a human-aligned preference dataset. Our work introduces an efficient data annotation method called the Multi-Expert Jury, enhancing data generation through structured MLLM explanations and quality control via cross-model evaluation, expert defect filtering, and human preference modification. In addition, we propose Holmes Pipeline, a meticulously designed three-stage training framework comprising visual expert pre-training, supervised fine-tuning, and direct preference optimization. Holmes Pipeline adapts multimodal large language models (MLLMs) for AIGI detection while generating human-verifiable and human-aligned explanations, ultimately yielding our model AIGI-Holmes. During the inference stage, we introduce a collaborative decoding strategy that integrates the model perception of the visual expert with the semantic reasoning of MLLMs, further enhancing the generalization capabilities. Extensive experiments on three benchmarks validate the effectiveness of our AIGI-Holmes. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•å¯¼è‡´äº†é«˜åº¦é€¼çœŸçš„AIç”Ÿæˆå›¾åƒï¼ˆAIGIï¼‰è¢«æ»¥ç”¨ï¼Œä»è€Œä¼ æ’­é”™è¯¯ä¿¡æ¯ï¼Œå¯¹å…¬ä¼—ä¿¡æ¯å®‰å…¨æ„æˆå¨èƒã€‚å°½ç®¡ç°æœ‰çš„AIGIæ£€æµ‹æŠ€æœ¯é€šå¸¸æœ‰æ•ˆï¼Œä½†å®ƒä»¬é¢ä¸´ä¸¤ä¸ªé—®é¢˜ï¼šä¸€æ˜¯ç¼ºä¹å¯éªŒè¯çš„äººä¸ºè§£é‡Šï¼ŒäºŒæ˜¯ç¼ºä¹åœ¨æœ€æ–°æŠ€æœ¯ä¸­çš„é€šç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤§è§„æ¨¡ä¸”ç»¼åˆçš„æ•°æ®é›†Holmes-Setï¼Œå®ƒåŒ…æ‹¬Holmes-SFTSetï¼ˆä¸€ä¸ªåŒ…å«å›¾åƒæ˜¯å¦ç”±AIç”Ÿæˆè§£é‡Šçš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼‰å’ŒHolmes-DPOSetï¼ˆä¸€ä¸ªäººä¸ºå¯¹é½çš„åå¥½æ•°æ®é›†ï¼‰ã€‚æˆ‘ä»¬çš„å·¥ä½œå¼•å…¥äº†ä¸€ç§é«˜æ•ˆçš„æ•°æ®æ³¨é‡Šæ–¹æ³•ï¼Œç§°ä¸ºå¤šä¸“å®¶é™ªå®¡å›¢ï¼Œé€šè¿‡ç»“æ„åŒ–çš„MLLMè§£é‡Šå’Œè´¨é‡æ§åˆ¶ï¼ˆåŒ…æ‹¬è·¨æ¨¡å‹è¯„ä¼°ã€ä¸“å®¶ç¼ºé™·è¿‡æ»¤å’Œäººç±»åå¥½ä¿®æ”¹ï¼‰å¢å¼ºæ•°æ®ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ç²¾å¿ƒè®¾è®¡çš„ä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶Holmes Pipelineï¼ŒåŒ…æ‹¬è§†è§‰ä¸“å®¶é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒä»¥åŠç›´æ¥åå¥½ä¼˜åŒ–ã€‚Holmes Pipelineä½¿å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€‚åº”AIGIæ£€æµ‹ï¼ŒåŒæ—¶ç”Ÿæˆå¯éªŒè¯ä¸”ç¬¦åˆäººç±»åå¥½çš„è§£é‡Šï¼Œæœ€ç»ˆç”Ÿæˆæˆ‘ä»¬çš„æ¨¡å‹AIGI-Holmesã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨ååŒè§£ç ç­–ç•¥ï¼Œå°†è§†è§‰ä¸“å®¶çš„æ¨¡å‹æ„ŸçŸ¥ä¸MLLMsçš„è¯­ä¹‰æ¨ç†ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„AIGI-Holmesçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02664v1">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong><br>     äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•å¯¼è‡´AIç”Ÿæˆå›¾åƒï¼ˆAIGIï¼‰è¢«ç”¨äºä¼ æ’­é”™è¯¯ä¿¡æ¯ï¼Œå¨èƒå…¬ä¼—ä¿¡æ¯å®‰å…¨ã€‚é’ˆå¯¹ç°æœ‰AIGIæ£€æµ‹æŠ€æœ¯åœ¨è§£é‡Šæ€§å’Œé€šç”¨æ€§æ–¹é¢çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Holmes-Setå¤§è§„æ¨¡ç»¼åˆæ•°æ®é›†å’ŒMulti-Expert Juryæ•°æ®æ ‡æ³¨æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†Holmes Pipelineä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶å’ŒAIGI-Holmesæ¨¡å‹ï¼Œç”¨äºç”Ÿæˆäººç±»å¯éªŒè¯çš„ã€ä¸äººç±»å¯¹é½çš„è§£é‡Šï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µé‡‡ç”¨ååŒè§£ç ç­–ç•¥ï¼Œæé«˜æ¨¡å‹çš„é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰æŠ€æœ¯å¿«é€Ÿå‘å±•ï¼ŒAIç”Ÿæˆå›¾åƒï¼ˆAIGIï¼‰è¢«è¯¯ç”¨äºä¼ æ’­é”™è¯¯ä¿¡æ¯ï¼Œå¯¹å…¬ä¼—ä¿¡æ¯å®‰å…¨æ„æˆå¨èƒã€‚</li>
<li>ç°æœ‰AIGIæ£€æµ‹æŠ€æœ¯åœ¨è§£é‡Šæ€§å’Œé€šç”¨æ€§æ–¹é¢å­˜åœ¨é—®é¢˜ã€‚</li>
<li>Holmes-Setæ•°æ®é›†çš„å¼•å…¥ï¼ŒåŒ…æ‹¬Holmes-SFTSetå’ŒHolmes-DPOSetï¼Œæœ‰åŠ©äºè§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨Multi-Expert Juryæ•°æ®æ ‡æ³¨æ–¹æ³•ï¼Œé€šè¿‡ç»“æ„åŒ–çš„MLLMè§£é‡Šå’Œè´¨é‡æ§åˆ¶æ¥æé«˜æ•°æ®ç”Ÿæˆè´¨é‡ã€‚</li>
<li>Holmes Pipelineè®­ç»ƒæ¡†æ¶å’ŒAIGI-Holmesæ¨¡å‹çš„è®¾è®¡ï¼Œæ—¨åœ¨ç”Ÿæˆäººç±»å¯éªŒè¯çš„ã€ä¸äººç±»å¯¹é½çš„è§£é‡Šã€‚</li>
<li>ååŒè§£ç ç­–ç•¥åœ¨æ¨ç†é˜¶æ®µçš„åº”ç”¨ï¼Œå¢å¼ºäº†æ¨¡å‹çš„é€šç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02664">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ff882394e8ece8fd1136685570a4caf2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36a2d26239472c644959493de853681e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-555ab8b2cdca9007b241b61c16d388c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-42a0f3ca8b6587a61ff2f5bad2640090.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd7ea3ea1f82521b7ce150fefa86af9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1eb76b23e2f25f705108e13a066f2f86.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Decoupled-Planning-and-Execution-A-Hierarchical-Reasoning-Framework-for-Deep-Search"><a href="#Decoupled-Planning-and-Execution-A-Hierarchical-Reasoning-Framework-for-Deep-Search" class="headerlink" title="Decoupled Planning and Execution: A Hierarchical Reasoning Framework for   Deep Search"></a>Decoupled Planning and Execution: A Hierarchical Reasoning Framework for   Deep Search</h2><p><strong>Authors:Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yang Zhao, Hongjin Qian, Zhicheng Dou</strong></p>
<p>Complex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: they use a single model to handle both high-level planning and detailed execution, leading to inefficient reasoning and limited scalability. In this paper, we introduce HiRA, a hierarchical framework that separates strategic planning from specialized execution. Our approach decomposes complex search tasks into focused subtasks, assigns each subtask to domain-specific agents equipped with external tools and reasoning capabilities, and coordinates the results through a structured integration mechanism. This separation prevents execution details from disrupting high-level reasoning while enabling the system to leverage specialized expertise for different types of information processing. Experiments on four complex, cross-modal deep search benchmarks demonstrate that HiRA significantly outperforms state-of-the-art RAG and agent-based systems. Our results show improvements in both answer quality and system efficiency, highlighting the effectiveness of decoupled planning and execution for multi-step information seeking tasks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ignorejjj/HiRA">https://github.com/ignorejjj/HiRA</a>. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œæœç´¢åœºæ™¯ä¸­çš„å¤æ‚ä¿¡æ¯éœ€æ±‚éœ€è¦è·¨ä¸åŒæºè¿›è¡Œæ·±åº¦æ¨ç†å’ŒçŸ¥è¯†ç»¼åˆï¼Œè€Œä¼ ç»Ÿçš„å¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰ç®¡é“åœ¨æœ‰æ•ˆè§£å†³è¿™æ–¹é¢å­˜åœ¨å›°éš¾ã€‚å½“å‰åŸºäºæ¨ç†çš„æ–¹æ³•å­˜åœ¨æ ¹æœ¬æ€§çš„å±€é™ï¼šå®ƒä»¬ä½¿ç”¨å•ä¸ªæ¨¡å‹åŒæ—¶å¤„ç†é«˜çº§è§„åˆ’å’Œè¯¦ç»†æ‰§è¡Œï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡ä½ä¸‹å’Œå¯æ‰©å±•æ€§æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†HiRAï¼Œä¸€ä¸ªå±‚æ¬¡æ¡†æ¶ï¼Œå°†æˆ˜ç•¥è§„åˆ’ä¸ä¸“ä¸šæ‰§è¡Œç›¸åˆ†ç¦»ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†å¤æ‚çš„æœç´¢ä»»åŠ¡åˆ†è§£ä¸ºæœ‰é’ˆå¯¹æ€§çš„å­ä»»åŠ¡ï¼Œå°†æ¯ä¸ªå­ä»»åŠ¡åˆ†é…ç»™é…å¤‡å¤–éƒ¨å·¥å…·å’Œæ¨ç†èƒ½åŠ›çš„é¢†åŸŸç‰¹å®šä»£ç†ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–æ•´åˆæœºåˆ¶åè°ƒç»“æœã€‚è¿™ç§åˆ†ç¦»é˜²æ­¢æ‰§è¡Œç»†èŠ‚ç ´åé«˜çº§æ¨ç†ï¼ŒåŒæ—¶ä½¿ç³»ç»Ÿèƒ½å¤Ÿåˆ©ç”¨ä¸åŒç±»å‹çš„ä¿¡æ¯å¤„ç†çš„ä¸“ä¸šçŸ¥è¯†ã€‚åœ¨å››ä¸ªå¤æ‚ã€è·¨æ¨¡æ€æ·±åº¦æœç´¢åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHiRAæ˜¾è‘—ä¼˜äºæœ€æ–°çš„RAGå’ŒåŸºäºä»£ç†çš„ç³»ç»Ÿã€‚æˆ‘ä»¬çš„ç»“æœæé«˜äº†ç­”æ¡ˆè´¨é‡å’Œç³»ç»Ÿæ•ˆç‡ï¼Œçªå‡ºäº†é’ˆå¯¹å¤šæ­¥éª¤ä¿¡æ¯æœç´¢ä»»åŠ¡çš„è§£è€¦è§„åˆ’å’Œæ‰§è¡Œçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ignorejjj/HiRA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ignorejjj/HiRAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02652v1">PDF</a> 9 pages</p>
<p><strong>Summary</strong></p>
<p>é¢å¯¹çœŸå®ä¸–ç•Œæœç´¢åœºæ™¯ä¸­å¤æ‚çš„æ·±åº¦æ¨ç†å’ŒçŸ¥è¯†åˆæˆéœ€æ±‚ï¼Œä¼ ç»ŸåŸºäºæ£€ç´¢çš„ç”Ÿæˆï¼ˆRAGï¼‰ç®¡é“å’Œå½“å‰åŸºäºæ¨ç†çš„æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡æå‡ºHiRAï¼Œä¸€ä¸ªå±‚æ¬¡æ¡†æ¶ï¼Œå°†æˆ˜ç•¥è§„åˆ’ä¸ä¸“é¡¹æ‰§è¡Œåˆ†ç¦»ï¼Œä»¥å¤„ç†å¤æ‚çš„æœç´¢ä»»åŠ¡ã€‚HiRAå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºä¸“é¡¹å­ä»»åŠ¡ï¼Œå¹¶ä¸ºæ¯ä¸ªå­ä»»åŠ¡åˆ†é…å…·å¤‡å¤–éƒ¨å·¥å…·å’Œæ¨ç†èƒ½åŠ›çš„ä¸“é¡¹ä»£ç†ï¼Œé€šè¿‡ç»“æ„åŒ–æ•´åˆæœºåˆ¶åè°ƒç»“æœã€‚è¿™ç§æ–¹æ³•é¿å…äº†æ‰§è¡Œç»†èŠ‚å¹²æ‰°é«˜çº§æ¨ç†ï¼ŒåŒæ—¶ä½¿ç³»ç»Ÿèƒ½å¤Ÿé’ˆå¯¹ä¸åŒç±»å‹çš„ä¿¡æ¯å¤„ç†åˆ©ç”¨ä¸“ä¸šä¸“é•¿ã€‚åœ¨å››ä¸ªå¤æ‚ã€è·¨æ¨¡æ€æ·±åº¦æœç´¢åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHiRAæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„RAGå’ŒåŸºäºä»£ç†çš„ç³»ç»Ÿï¼Œåœ¨ç­”æ¡ˆè´¨é‡å’Œç³»ç»Ÿæ•ˆç‡æ–¹é¢éƒ½æœ‰æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çœŸå®ä¸–ç•Œçš„æœç´¢éœ€æ±‚éœ€è¦æ·±åº¦æ¨ç†å’ŒçŸ¥è¯†åˆæˆã€‚</li>
<li>ä¼ ç»ŸRAGç®¡é“å’Œå½“å‰åŸºäºæ¨ç†çš„æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>HiRAæ˜¯ä¸€ä¸ªå±‚æ¬¡æ¡†æ¶ï¼Œå°†æˆ˜ç•¥è§„åˆ’ä¸ä¸“é¡¹æ‰§è¡Œåˆ†ç¦»ï¼Œä»¥å¤„ç†å¤æ‚çš„æœç´¢ä»»åŠ¡ã€‚</li>
<li>HiRAå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºä¸“é¡¹å­ä»»åŠ¡ï¼Œå¹¶ä¸ºæ¯ä¸ªå­ä»»åŠ¡åˆ†é…ä¸“é¡¹ä»£ç†ã€‚</li>
<li>HiRAé€šè¿‡ç»“æ„åŒ–æ•´åˆæœºåˆ¶åè°ƒä»£ç†çš„ç»“æœã€‚</li>
<li>HiRAé¿å…äº†æ‰§è¡Œç»†èŠ‚å¹²æ‰°é«˜çº§æ¨ç†ã€‚</li>
<li>HiRAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–ç³»ç»Ÿï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ­¥ä¿¡æ¯å¯»æ‰¾ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cdbe86af5b7a56ebf9b82aa172e86c51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b8a612723e286c9564377217a6ed15a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de011114c3658339bdf81cf4a784697c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-814a208671c75af68bb64b15b2072d21.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VRAgent-R1-Boosting-Video-Recommendation-with-MLLM-based-Agents-via-Reinforcement-Learning"><a href="#VRAgent-R1-Boosting-Video-Recommendation-with-MLLM-based-Agents-via-Reinforcement-Learning" class="headerlink" title="VRAgent-R1: Boosting Video Recommendation with MLLM-based Agents via   Reinforcement Learning"></a>VRAgent-R1: Boosting Video Recommendation with MLLM-based Agents via   Reinforcement Learning</h2><p><strong>Authors:Siran Chen, Boyu Chen, Chenyun Yu, Yuxiao Luo, Ouyang Yi, Lei Cheng, Chengxiang Zhuo, Zang Li, Yali Wang</strong></p>
<p>Owing to powerful natural language processing and generative capabilities, large language model (LLM) agents have emerged as a promising solution for enhancing recommendation systems via user simulation. However, in the realm of video recommendation, existing studies predominantly resort to prompt-based simulation using frozen LLMs and encounter the intricate challenge of multimodal content understanding. This frequently results in suboptimal item modeling and user preference learning, thereby ultimately constraining recommendation performance. To address these challenges, we introduce VRAgent-R1, a novel agent-based paradigm that incorporates human-like intelligence in user simulation. Specifically, VRAgent-R1 comprises two distinct agents: the Item Perception (IP) Agent and the User Simulation (US) Agent, designed for interactive user-item modeling. Firstly, the IP Agent emulates human-like progressive thinking based on MLLMs, effectively capturing hidden recommendation semantics in videos. With a more comprehensive multimodal content understanding provided by the IP Agent, the video recommendation system is equipped to provide higher-quality candidate items. Subsequently, the US Agent refines the recommended video sets based on in-depth chain-of-thought (CoT) reasoning and achieves better alignment with real user preferences through reinforcement learning. Experimental results on a large-scale video recommendation benchmark have demonstrated the effectiveness of our proposed VRAgent-R1 method, e.g., the IP Agent achieves a 6.0% improvement in NDCG@10 on the MicroLens-100k dataset, while the US Agent shows approximately 45.0% higher accuracy in user decision simulation compared to state-of-the-art baselines. </p>
<blockquote>
<p>ç”±äºå¼ºå¤§çš„è‡ªç„¶è¯­è¨€å¤„ç†å’Œç”Ÿæˆèƒ½åŠ›ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†ç”¨æˆ·æ¨¡æ‹Ÿå·²ç»æˆä¸ºå¢å¼ºæ¨èç³»ç»Ÿçš„ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œåœ¨è§†é¢‘æ¨èé¢†åŸŸï¼Œç°æœ‰ç ”ç©¶ä¸»è¦ä¾èµ–äºåŸºäºæç¤ºçš„æ¨¡æ‹Ÿä½¿ç”¨å†»ç»“çš„LLMï¼Œå¹¶é¢ä¸´å¤æ‚çš„å¤šåª’ä½“å†…å®¹ç†è§£æŒ‘æˆ˜ã€‚è¿™ç»å¸¸å¯¼è‡´é¡¹ç›®å»ºæ¨¡å’Œç”¨æˆ·åå¥½å­¦ä¹ ä¸ç†æƒ³ï¼Œä»è€Œæœ€ç»ˆé™åˆ¶æ¨èæ€§èƒ½ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VRAgent-R1ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹åŸºäºä»£ç†çš„èŒƒå¼ï¼Œå°†äººç±»æ™ºèƒ½èå…¥ç”¨æˆ·æ¨¡æ‹Ÿä¸­ã€‚å…·ä½“æ¥è¯´ï¼ŒVRAgent-R1åŒ…æ‹¬ä¸¤ä¸ªç‹¬ç‰¹çš„ä»£ç†ï¼šé¡¹ç›®æ„ŸçŸ¥ï¼ˆIPï¼‰ä»£ç†å’Œç”¨æˆ·æ¨¡æ‹Ÿï¼ˆUSï¼‰ä»£ç†ï¼Œç”¨äºäº¤äº’å¼ç”¨æˆ·é¡¹ç›®å»ºæ¨¡ã€‚é¦–å…ˆï¼ŒIPä»£ç†åŸºäºMLLMsæ¨¡æ‹Ÿäººç±»æ¸è¿›æ€è€ƒçš„æ–¹å¼ï¼Œæœ‰æ•ˆåœ°æ•æ‰è§†é¢‘ä¸­çš„éšè—æ¨èè¯­ä¹‰ã€‚é€šè¿‡IPä»£ç†æä¾›çš„æ›´å…¨é¢çš„å¤šåª’ä½“å†…å®¹ç†è§£ï¼Œè§†é¢‘æ¨èç³»ç»Ÿèƒ½å¤Ÿæä¾›æ›´é«˜è´¨é‡çš„å€™é€‰é¡¹ç›®ã€‚å…¶æ¬¡ï¼ŒUSä»£ç†åŸºäºæ·±å…¥çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æ¥å®Œå–„æ¨èçš„è§†é¢‘é›†ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ æ›´å¥½åœ°ç¬¦åˆçœŸå®ç”¨æˆ·åå¥½ã€‚åœ¨å¤§è§„æ¨¡è§†é¢‘æ¨èåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜äº†æˆ‘ä»¬æå‡ºçš„VRAgent-R1æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¾‹å¦‚IPä»£ç†åœ¨MicroLens-100kæ•°æ®é›†ä¸Šå®ç°äº†NDCG@10çš„6.0%æ”¹è¿›ï¼Œè€ŒUSä»£ç†åœ¨ç”¨æˆ·å†³ç­–æ¨¡æ‹Ÿæ–¹é¢çš„å‡†ç¡®æ€§æ¯”æœ€æ–°åŸºçº¿é«˜å‡ºçº¦45.0%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02626v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†é¢‘æ¨èæ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³ç°æœ‰ç ”ç©¶ä¸­å­˜åœ¨çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„åŸºäºä»£ç†çš„æ–¹æ³•VRAgent-R1ï¼Œå…¶ä¸­åŒ…æ‹¬ç”¨äºäº¤äº’å¼ç”¨æˆ·å»ºæ¨¡çš„ç‰©å“æ„ŸçŸ¥ï¼ˆIPï¼‰ä»£ç†å’Œç”¨æˆ·æ¨¡æ‹Ÿï¼ˆUSï¼‰ä»£ç†ã€‚IPä»£ç†åŸºäºMLLMæ¨¡æ‹Ÿäººç±»çš„æ¸è¿›æ€ç»´ï¼Œæ›´æœ‰æ•ˆåœ°æ•æ‰è§†é¢‘ä¸­çš„æ¨èè¯­ä¹‰ï¼›è€ŒUSä»£ç†é€šè¿‡æ·±åº¦é“¾æ€ç»´æ¨ç†æ¥ç²¾ç‚¼æ¨èçš„è§†é¢‘é›†å¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ æ›´å¥½åœ°ç¬¦åˆçœŸå®ç”¨æˆ·åå¥½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVRAgent-R1æ–¹æ³•æœ‰æ•ˆæé«˜äº†è§†é¢‘æ¨èçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†è¢«ç”¨äºå¢å¼ºæ¨èç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨è§†é¢‘æ¨èæ–¹é¢ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦ä½¿ç”¨åŸºäºæç¤ºçš„æ¨¡æ‹Ÿæ–¹æ³•ï¼Œé¢ä¸´å¤šæ¨¡æ€å†…å®¹ç†è§£çš„æŒ‘æˆ˜ã€‚</li>
<li>VRAgent-R1åŒ…æ‹¬ç‰©å“æ„ŸçŸ¥ï¼ˆIPï¼‰ä»£ç†å’Œç”¨æˆ·æ¨¡æ‹Ÿï¼ˆUSï¼‰ä»£ç†ï¼Œç”¨äºäº¤äº’å¼ç”¨æˆ·å»ºæ¨¡ã€‚</li>
<li>IPä»£ç†æ¨¡æ‹Ÿäººç±»çš„æ¸è¿›æ€ç»´ï¼Œæ›´æœ‰æ•ˆåœ°æ•æ‰è§†é¢‘ä¸­çš„æ¨èè¯­ä¹‰ã€‚</li>
<li>USä»£ç†é€šè¿‡æ·±åº¦é“¾æ€ç»´æ¨ç†æ¥ç²¾ç‚¼æ¨èçš„è§†é¢‘é›†ï¼Œå¹¶æ›´å¥½åœ°ç¬¦åˆçœŸå®ç”¨æˆ·åå¥½ã€‚</li>
<li>VRAgent-R1æ–¹æ³•åœ¨å®éªŒä¸Šè¢«è¯æ˜èƒ½æœ‰æ•ˆæé«˜è§†é¢‘æ¨èæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02626">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-40934976f2b680fbb2f9a1a176ea5e3d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a70bb736c160c30d6ef975b026e1bd2c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf1a9ddd370f5e35c7f1991119ec9147.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3c9a511e7e696932b398fcb9baff0f2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="WebSailor-Navigating-Super-human-Reasoning-for-Web-Agent"><a href="#WebSailor-Navigating-Super-human-Reasoning-for-Web-Agent" class="headerlink" title="WebSailor: Navigating Super-human Reasoning for Web Agent"></a>WebSailor: Navigating Super-human Reasoning for Web Agent</h2><p><strong>Authors:Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, Weizhou Shen, Junkai Zhang, Dingchu Zhang, Xixi Wu, Yong Jiang, Ming Yan, Pengjun Xie, Fei Huang, Jingren Zhou</strong></p>
<p>Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all opensource agents in complex information-seeking tasks, matching proprietary agentsâ€™ performance and closing the capability gap. </p>
<blockquote>
<p>çªç ´äººç±»è®¤çŸ¥å±€é™æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­çš„ä¸€é¡¹å…³é”®å‰æ²¿æŠ€æœ¯ã€‚DeepResearchç­‰ä¸“æœ‰æ™ºèƒ½ç³»ç»Ÿå·²åœ¨BrowseCompç­‰æä¸ºå¤æ‚çš„ä¿¡æ¯æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºè¶…äººç±»çš„èƒ½åŠ›ï¼Œè¿™æ˜¯ä»¥å‰æ— æ³•å®ç°çš„ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå®ƒä»¬çš„æˆåŠŸä¾èµ–äºä¸€ç§å¼€æºæ¨¡å‹ä¸­ä¸å­˜åœ¨çš„å¤æ‚æ¨ç†æ¨¡å¼ï¼šåœ¨æµè§ˆå¹¿é˜”çš„ä¿¡æ¯æ™¯è§‚æ—¶ï¼Œç³»ç»Ÿåœ°å‡å°‘æç«¯ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æ¨å‡ºäº†WebSailorï¼Œè¿™æ˜¯ä¸€ç§å®Œæ•´çš„è®­ç»ƒåæ–¹æ³•ï¼Œæ—¨åœ¨åŸ¹å…»è¿™ç§è‡³å…³é‡è¦çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯é€šè¿‡ç»“æ„åŒ–é‡‡æ ·ã€ä¿¡æ¯æ¨¡ç³Šå¤„ç†ã€RFTå†·å¯åŠ¨å’Œé«˜æ•ˆçš„æ™ºèƒ½å¼ºåŒ–å­¦ä¹ è®­ç»ƒç®—æ³•DUPOï¼ˆå¤åˆ¶é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œç”Ÿæˆå…·æœ‰æ–°é¢–æ€§ã€é«˜ä¸ç¡®å®šæ€§çš„ä»»åŠ¡ã€‚é€šè¿‡è¿™ä¸€ç»¼åˆç®¡é“ï¼ŒWebSailoråœ¨å¤æ‚çš„ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­å¤§å¤§è¶…è¶Šäº†æ‰€æœ‰å¼€æºæ™ºèƒ½ä½“ï¼Œè¾¾åˆ°äº†ä¸“æœ‰æ™ºèƒ½ä½“çš„æ€§èƒ½æ°´å¹³ï¼Œå¹¶å¼¥è¡¥äº†èƒ½åŠ›å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02592v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ·±ç ”çš„ä¸“æœ‰ä»£ç†ç³»ç»Ÿåœ¨å¤æ‚ä¿¡æ¯æ£€ç´¢åŸºå‡†æµ‹è¯•ï¼ˆå¦‚BrowseCompï¼‰ä¸­å±•ç°å‡ºè¶…è¶Šäººç±»çš„æ€§èƒ½ã€‚å…¶æˆåŠŸæºäºå¤„ç†é«˜ä¸ç¡®å®šæ€§ä¿¡æ¯æ—¶çš„ç²¾ç»†æ¨ç†æ¨¡å¼ã€‚ä¸ºèµ‹äºˆå¼€æºæ¨¡å‹è¿™ç§èƒ½åŠ›ï¼Œæˆ‘ä»¬æ¨å‡ºWebSailorï¼Œä¸€ç§å…¨æ–°çš„è®­ç»ƒåæ–¹æ³•ã€‚å®ƒé€šè¿‡ç»“æ„åŒ–é‡‡æ ·ã€ä¿¡æ¯æ¨¡ç³Šå¤„ç†ã€RFTå†·å¯åŠ¨ä»¥åŠé«˜æ•ˆçš„ä»£ç†å¼ºåŒ–å­¦ä¹ è®­ç»ƒç®—æ³•DUPOï¼Œæ˜¾è‘—æå‡äº†åœ¨å¤æ‚ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä¸ä¸“æœ‰ä»£ç†æ€§èƒ½ç›¸å½“ï¼Œç¼©å°äº†èƒ½åŠ›å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸“æœ‰ä»£ç†ç³»ç»Ÿåœ¨å¤æ‚ä¿¡æ¯æ£€ç´¢ä¸­è¶…è¶Šäººç±»è¡¨ç°ã€‚</li>
<li>æ·±ç ”çš„ä»£ç†ç³»ç»ŸæˆåŠŸæºäºå¤„ç†é«˜ä¸ç¡®å®šæ€§ä¿¡æ¯æ—¶çš„ç²¾ç»†æ¨ç†ã€‚</li>
<li>WebSailoræ˜¯ä¸€ç§æ–°çš„è®­ç»ƒåæ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¼€æºæ¨¡å‹åœ¨ä¿¡æ¯æ£€ç´¢ä¸­çš„æ€§èƒ½ã€‚</li>
<li>WebSailoré€šè¿‡ç»“æ„åŒ–é‡‡æ ·ã€ä¿¡æ¯æ¨¡ç³Šç­‰æ‰‹æ®µç”Ÿæˆé«˜ä¸ç¡®å®šæ€§ä»»åŠ¡ã€‚</li>
<li>RFTå†·å¯åŠ¨æŠ€æœ¯æé«˜äº†ä»£ç†ç³»ç»Ÿçš„æ•ˆç‡ã€‚</li>
<li>DUPOç®—æ³•æ˜¯WebSailorçš„æ ¸å¿ƒï¼Œæœ‰æ•ˆæå‡äº†ä»£ç†åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02592">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-840bdbdf3b114734fd0d7349bbace748.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f066ffc2f04ce6076ad534beaaa0f84.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Reconstructing-Close-Human-Interaction-with-Appearance-and-Proxemics-Reasoning"><a href="#Reconstructing-Close-Human-Interaction-with-Appearance-and-Proxemics-Reasoning" class="headerlink" title="Reconstructing Close Human Interaction with Appearance and Proxemics   Reasoning"></a>Reconstructing Close Human Interaction with Appearance and Proxemics   Reasoning</h2><p><strong>Authors:Buzhen Huang, Chen Li, Chongyang Xu, Dongyue Lu, Jinnan Chen, Yangang Wang, Gim Hee Lee</strong></p>
<p>Due to visual ambiguities and inter-person occlusions, existing human pose estimation methods cannot recover plausible close interactions from in-the-wild videos. Even state-of-the-art large foundation models~(\eg, SAM) cannot accurately distinguish human semantics in such challenging scenarios. In this work, we find that human appearance can provide a straightforward cue to address these obstacles. Based on this observation, we propose a dual-branch optimization framework to reconstruct accurate interactive motions with plausible body contacts constrained by human appearances, social proxemics, and physical laws. Specifically, we first train a diffusion model to learn the human proxemic behavior and pose prior knowledge. The trained network and two optimizable tensors are then incorporated into a dual-branch optimization framework to reconstruct human motions and appearances. Several constraints based on 3D Gaussians, 2D keypoints, and mesh penetrations are also designed to assist the optimization. With the proxemics prior and diverse constraints, our method is capable of estimating accurate interactions from in-the-wild videos captured in complex environments. We further build a dataset with pseudo ground-truth interaction annotations, which may promote future research on pose estimation and human behavior understanding. Experimental results on several benchmarks demonstrate that our method outperforms existing approaches. The code and data are available at <a target="_blank" rel="noopener" href="https://www.buzhenhuang.com/works/CloseApp.html">https://www.buzhenhuang.com/works/CloseApp.html</a>. </p>
<blockquote>
<p>ç”±äºè§†è§‰æ¨¡ç³Šå’Œäººä¸äººä¹‹é—´çš„é®æŒ¡ï¼Œç°æœ‰çš„äººä½“å§¿æ€ä¼°è®¡æ–¹æ³•æ— æ³•ä»é‡ç”Ÿè§†é¢‘ä¸­æ¢å¤å‡ºåˆç†çš„ç´§å¯†äº¤äº’ã€‚å³ä½¿æ˜¯æœ€å…ˆè¿›çš„å¤§å‹åŸºç¡€æ¨¡å‹ï¼ˆä¾‹å¦‚SAMï¼‰ä¹Ÿæ— æ³•åœ¨è¿™ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­å‡†ç¡®åŒºåˆ†äººç±»è¯­ä¹‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘ç°äººç±»å¤–è§‚å¯ä»¥æä¾›ä¸€ç§ç›´æ¥çš„çº¿ç´¢æ¥è§£å†³è¿™äº›éšœç¢ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŒåˆ†æ”¯ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡äººç±»å¤–è§‚ã€ç¤¾äº¤ç©ºé—´å­¦ã€ç‰©ç†å®šå¾‹çš„çº¦æŸæ¥é‡å»ºå‡†ç¡®çš„äº¤äº’åŠ¨ä½œå’Œåˆç†çš„èº«ä½“æ¥è§¦ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆè®­ç»ƒä¸€ä¸ªæ‰©æ•£æ¨¡å‹æ¥å­¦ä¹ äººç±»çš„ç©ºé—´è¡Œä¸ºå§¿æ€å…ˆéªŒçŸ¥è¯†ã€‚ç„¶åï¼Œå°†è®­ç»ƒå¥½çš„ç½‘ç»œå’Œä¸¤ä¸ªå¯ä¼˜åŒ–çš„å¼ é‡çº³å…¥åŒåˆ†æ”¯ä¼˜åŒ–æ¡†æ¶ä¸­ï¼Œä»¥é‡å»ºäººç±»åŠ¨ä½œå’Œå¤–è§‚ã€‚è¿˜è®¾è®¡äº†åŸºäº3Dé«˜æ–¯ã€2Då…³é”®ç‚¹ä»¥åŠç½‘æ ¼ç©¿é€çš„å¤šä¸ªçº¦æŸæ¥å¸®åŠ©ä¼˜åŒ–ã€‚é€šè¿‡ç©ºé—´å­¦å…ˆéªŒçŸ¥è¯†å’Œå¤šæ ·çš„çº¦æŸï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿä»å¤æ‚ç¯å¢ƒä¸­æ•æ‰åˆ°çš„é‡ç”Ÿè§†é¢‘ä¸­ä¼°è®¡å‡ºå‡†ç¡®çš„äººä½“äº¤äº’ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å»ºç«‹äº†ä¸€ä¸ªå¸¦æœ‰ä¼ªçœŸå®äº¤äº’æ³¨é‡Šçš„æ•°æ®é›†ï¼Œè¿™å¯èƒ½ä¼šä¿ƒè¿›æœªæ¥å¯¹äººä½“å§¿æ€ä¼°è®¡å’Œè¡Œä¸ºç†è§£çš„ç ”ç©¶ã€‚åœ¨å‡ ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://www.buzhenhuang.com/works/CloseApp.html%E8%AE%BF%E9%98%B5%E3%80%82">https://www.buzhenhuang.com/works/CloseApp.htmlè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02565v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºäººç±»å¤–è§‚çš„åŒåˆ†æ”¯ä¼˜åŒ–æ¡†æ¶ï¼Œç”¨äºä»é‡å¤–è§†é¢‘ä¸­é‡å»ºå‡†ç¡®çš„äººä½“äº¤äº’åŠ¨ä½œã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆäººç±»è¡Œä¸ºå­¦å’Œç‰©ç†è§„å¾‹ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹å­¦ä¹ äººç±»è¡Œä¸ºæ¨¡å¼å’Œå§¿åŠ¿å…ˆéªŒçŸ¥è¯†ï¼Œå†é€šè¿‡ä¼˜åŒ–ç®—æ³•é‡å»ºäººä½“åŠ¨ä½œå’Œå¤–è§‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰çš„äººä½“å§¿æ€ä¼°è®¡æ–¹æ³•æ— æ³•ä»é‡å¤–è§†é¢‘æ¢å¤å‡ºå¯ä¿¡çš„è¿‘è·ç¦»äº¤äº’åŠ¨ä½œï¼ŒåŸå› åœ¨äºè§†è§‰æ¨¡ç³Šå’Œäººä¸äººä¹‹é—´çš„ç›¸äº’é®æŒ¡ã€‚</li>
<li>äººç±»å¤–è§‚å¯ä»¥ä½œä¸ºä¸€ç§ç›´è§‚çº¿ç´¢æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªåŒåˆ†æ”¯ä¼˜åŒ–æ¡†æ¶ï¼Œèƒ½å¤Ÿç»“åˆäººç±»å¤–è§‚ã€ç¤¾äº¤ç©ºé—´å­¦ï¼ˆproxemicsï¼‰å’Œç‰©ç†å®šå¾‹æ¥é‡å»ºå‡†ç¡®çš„äººä½“äº¤äº’åŠ¨ä½œã€‚</li>
<li>é€šè¿‡è®­ç»ƒæ‰©æ•£æ¨¡å‹æ¥å­¦ä¹ äººç±»çš„è¡Œä¸ºæ¨¡å¼å’Œå§¿åŠ¿å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>é€šè¿‡å¤šç§çº¦æŸï¼ˆå¦‚åŸºäº3Dé«˜æ–¯åˆ†å¸ƒã€2Då…³é”®ç‚¹ã€ç½‘æ ¼ç©¿é€çš„çº¦æŸï¼‰æ¥è¾…åŠ©ä¼˜åŒ–è¿‡ç¨‹ã€‚</li>
<li>å»ºç«‹äº†ä¸€ä¸ªå¸¦æœ‰ä¼ªåœ°é¢çœŸå®äº¤äº’æ³¨é‡Šçš„æ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›æœªæ¥å¯¹äººä½“å§¿æ€ä¼°è®¡å’Œäººç±»è¡Œä¸ºç†è§£çš„ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02565">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-42bc17fa39ad06cd6cb26dab7c9fad37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e13336ff5e3373bf1c2748676e1565ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d19eb1174d90cea03009525b6f1425fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77be3bfc211df6a2b17d27843b78e17d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Clarifying-Before-Reasoning-A-Coq-Prover-with-Structural-Context"><a href="#Clarifying-Before-Reasoning-A-Coq-Prover-with-Structural-Context" class="headerlink" title="Clarifying Before Reasoning: A Coq Prover with Structural Context"></a>Clarifying Before Reasoning: A Coq Prover with Structural Context</h2><p><strong>Authors:Yanzhen Lu, Hanbin Yang, Xiaodie Wang, Ge Zhang, Biao Li, Chenxu Fu, Chao Li, Yang Yuan, Andrew Chi-Chih Yao</strong></p>
<p>In this work, we investigate whether improving task clarity can enhance reasoning ability of large language models, focusing on theorem proving in Coq. We introduce a concept-level metric to evaluate task clarity and show that adding structured semantic context to the standard input used by modern LLMs, leads to a 1.85$\times$ improvement in clarity score (44.5%<del>$\rightarrow$</del>82.3%). Using the general-purpose model \texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof success (21.8%<del>$\rightarrow$</del>45.8%) and outperforms the previous state-of-the-art \texttt{Graph2Tac} (33.2%). We evaluate this on 1,386 theorems randomly sampled from 15 standard Coq packages, following the same evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller models on our structured data can achieve even higher performance (48.6%). Our method uses selective concept unfolding to enrich task descriptions, and employs a Plannerâ€“Executor architecture. These findings highlight the value of structured task representations in bridging the gap between understanding and reasoning. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æé«˜ä»»åŠ¡æ¸…æ™°åº¦æ˜¯å¦èƒ½å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œé‡ç‚¹å…³æ³¨Coqä¸­çš„å®šç†è¯æ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ¦‚å¿µå±‚é¢çš„æŒ‡æ ‡æ¥è¯„ä¼°ä»»åŠ¡æ¸…æ™°åº¦ï¼Œå¹¶è¡¨æ˜å‘ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹çš„æ ‡å‡†è¾“å…¥æ·»åŠ ç»“æ„åŒ–è¯­ä¹‰ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥æé«˜æ¸…æ™°åº¦è¯„åˆ†ï¼ˆä»44.5%æé«˜åˆ°82.3%ï¼‰ï¼Œæé«˜å¹…åº¦ä¸º1.85å€ã€‚ä½¿ç”¨é€šç”¨æ¨¡å‹DeepSeek-V3ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿è¯æ˜æˆåŠŸç‡æé«˜äº†2.1å€ï¼ˆä»21.8%æé«˜åˆ°45.8%ï¼‰ï¼Œå¹¶è¶…è¶Šäº†ä¹‹å‰çš„æœ€ä½³æ¨¡å‹Graph2Tacï¼ˆ33.2%ï¼‰ã€‚æˆ‘ä»¬åœ¨ä»15ä¸ªæ ‡å‡†Coqè½¯ä»¶åŒ…ä¸­éšæœºæŠ½å–çš„1386ä¸ªå®šç†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œéµå¾ªä¸Graph2Tacç›¸åŒçš„è¯„ä¼°åè®®ã€‚æ­¤å¤–ï¼Œåœ¨ç»“æ„åŒ–æ•°æ®ä¸Šå¾®è°ƒè¾ƒå°çš„æ¨¡å‹å¯ä»¥è·å¾—æ›´é«˜çš„æ€§èƒ½ï¼ˆ48.6%ï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨é€‰æ‹©æ€§æ¦‚å¿µå±•å¼€æ¥ä¸°å¯Œä»»åŠ¡æè¿°ï¼Œå¹¶é‡‡ç”¨Planner-Executoræ¶æ„ã€‚è¿™äº›å‘ç°çªæ˜¾äº†ç»“æ„åŒ–ä»»åŠ¡è¡¨ç¤ºåœ¨ç†è§£å’Œæ¨ç†ä¹‹é—´æ¶èµ·æ¡¥æ¢çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02541v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬å·¥ä½œç ”ç©¶æé«˜ä»»åŠ¡æ¸…æ™°åº¦æ˜¯å¦èƒ½æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œé‡ç‚¹ç ”ç©¶Coqä¸­çš„å®šç†è¯æ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ¦‚å¿µå±‚é¢çš„æŒ‡æ ‡æ¥è¯„ä¼°ä»»åŠ¡æ¸…æ™°åº¦ï¼Œå¹¶æ˜¾ç¤ºå‘ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹çš„æ ‡å‡†è¾“å…¥æ·»åŠ ç»“æ„åŒ–è¯­ä¹‰ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥æé«˜æ¸…æ™°åº¦è¯„åˆ†ï¼ˆä»44.5%æé«˜åˆ°82.3%ï¼‰ï¼Œæé«˜äº†çº¦1.85å€ã€‚ä½¿ç”¨é€šç”¨æ¨¡å‹DeepSeek-V3ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿å¾—è¯æ˜æˆåŠŸçš„æ¦‚ç‡æé«˜äº†çº¦ä¸¤å€ï¼ˆä»21.8%æé«˜åˆ°45.8%ï¼‰ï¼Œå¹¶ä¼˜äºå…ˆå‰çš„æœ€ä½³æ¨¡å‹Graph2Tacï¼ˆæˆåŠŸç‡ä¸º33.2%ï¼‰ã€‚æˆ‘ä»¬åœ¨ä»æ ‡å‡†Coqè½¯ä»¶åŒ…ä¸­éšæœºæŠ½å–çš„1,386ä¸ªå®šç†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œéµå¾ªGraph2Tacç›¸åŒçš„è¯„ä¼°åè®®ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç»†åŒ–æˆ‘ä»¬çš„ç»“æ„åŒ–æ•°æ®å¯¹å°æ¨¡å‹è¿›è¡Œå¾®è°ƒå¯ä»¥å®ç°æ›´é«˜çš„æ€§èƒ½ï¼ˆæˆåŠŸç‡ä¸º48.6%ï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä½¿ç”¨é€‰æ‹©æ€§æ¦‚å¿µå±•å¼€æ¥ä¸°å¯Œä»»åŠ¡æè¿°ï¼Œå¹¶é‡‡ç”¨Planner-Executoræ¶æ„å®ç°ä¸Šè¿°æˆæœã€‚è¿™äº›å‘ç°çªæ˜¾äº†ç»“æ„åŒ–ä»»åŠ¡è¡¨ç¤ºåœ¨ç†è§£å’Œæ¨ç†ä¹‹é—´æ¶èµ·æ¡¥æ¢çš„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äº†æé«˜ä»»åŠ¡æ¸…æ™°åº¦å¯¹å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨Coqä¸­çš„å®šç†è¯æ˜ã€‚</li>
<li>å¼•å…¥æ¦‚å¿µå±‚é¢çš„æŒ‡æ ‡æ¥è¯„ä¼°ä»»åŠ¡æ¸…æ™°åº¦ï¼Œå¹¶å‘ç°æ·»åŠ ç»“æ„åŒ–è¯­ä¹‰ä¸Šä¸‹æ–‡å¯æ˜¾è‘—æé«˜æ¸…æ™°åº¦è¯„åˆ†ã€‚</li>
<li>ä½¿ç”¨DeepSeek-V3æ¨¡å‹ï¼Œåœ¨å®šç†è¯æ˜æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸç‡æå‡ï¼Œä¼˜äºå…ˆå‰çš„æœ€ä½³æ¨¡å‹Graph2Tacã€‚</li>
<li>åœ¨å¤§é‡éšæœºæŠ½å–çš„Coqå®šç†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œéµå¾ªäº†Graph2Tacçš„è¯„ä¼°åè®®ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨é€‰æ‹©æ€§æ¦‚å¿µå±•å¼€å’ŒPlanner-Executoræ¶æ„ä¸°å¯Œä»»åŠ¡æè¿°æ¥å®ç°æˆæœã€‚</li>
<li>ç»†è°ƒå°æ¨¡å‹åœ¨ç»“æ„åŒ–æ•°æ®ä¸Šå¯ä»¥è¾¾åˆ°æ›´é«˜çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02541">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-14b6f4843b1d0431ee6bd49785897a02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f8342172f7a22ad7aaac74621532036.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Are-Synthetic-Videos-Useful-A-Benchmark-for-Retrieval-Centric-Evaluation-of-Synthetic-Videos"><a href="#Are-Synthetic-Videos-Useful-A-Benchmark-for-Retrieval-Centric-Evaluation-of-Synthetic-Videos" class="headerlink" title="Are Synthetic Videos Useful? A Benchmark for Retrieval-Centric   Evaluation of Synthetic Videos"></a>Are Synthetic Videos Useful? A Benchmark for Retrieval-Centric   Evaluation of Synthetic Videos</h2><p><strong>Authors:Zecheng Zhao, Selena Song, Tong Chen, Zhi Chen, Shazia Sadiq, Yadan Luo</strong></p>
<p>Text-to-video (T2V) synthesis has advanced rapidly, yet current evaluation metrics primarily capture visual quality and temporal consistency, offering limited insight into how synthetic videos perform in downstream tasks such as text-to-video retrieval (TVR). In this work, we introduce SynTVA, a new dataset and benchmark designed to evaluate the utility of synthetic videos for building retrieval models. Based on 800 diverse user queries derived from MSRVTT training split, we generate synthetic videos using state-of-the-art T2V models and annotate each video-text pair along four key semantic alignment dimensions: Object &amp; Scene, Action, Attribute, and Prompt Fidelity. Our evaluation framework correlates general video quality assessment (VQA) metrics with these alignment scores, and examines their predictive power for downstream TVR performance. To explore pathways of scaling up, we further develop an Auto-Evaluator to estimate alignment quality from existing metrics. Beyond benchmarking, our results show that SynTVA is a valuable asset for dataset augmentation, enabling the selection of high-utility synthetic samples that measurably improve TVR outcomes. Project page and dataset can be found at <a target="_blank" rel="noopener" href="https://jasoncodemaker.github.io/SynTVA/">https://jasoncodemaker.github.io/SynTVA/</a>. </p>
<blockquote>
<p>æ–‡æœ¬è½¬è§†é¢‘ï¼ˆT2Vï¼‰åˆæˆæŠ€æœ¯å·²ç»è¿…é€Ÿå‘å±•ï¼Œç„¶è€Œå½“å‰çš„è¯„ä¼°æŒ‡æ ‡ä¸»è¦æ•æ‰è§†è§‰è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§ï¼Œå¯¹äºåˆæˆè§†é¢‘åœ¨ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬è½¬è§†é¢‘æ£€ç´¢ï¼ˆTVRï¼‰ï¼‰ä¸­çš„è¡¨ç°æä¾›äº†æœ‰é™çš„æ´å¯Ÿã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†SynTVAï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°åˆæˆè§†é¢‘åœ¨æ„å»ºæ£€ç´¢æ¨¡å‹æ—¶çš„å®ç”¨æ€§ã€‚åŸºäºä»MSRVTTè®­ç»ƒé›†ä¸­å¾—å‡ºçš„800ä¸ªä¸åŒç”¨æˆ·æŸ¥è¯¢ï¼Œæˆ‘ä»¬ä½¿ç”¨æœ€å…ˆè¿›çš„T2Væ¨¡å‹ç”Ÿæˆåˆæˆè§†é¢‘ï¼Œå¹¶æ²¿å››ä¸ªå…³é”®è¯­ä¹‰å¯¹é½ç»´åº¦å¯¹æ¯ä¸ªè§†é¢‘æ–‡æœ¬å¯¹è¿›è¡Œæ³¨é‡Šï¼šå¯¹è±¡ä¸åœºæ™¯ã€åŠ¨ä½œã€å±æ€§å’Œæç¤ºä¿çœŸåº¦ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶å°†è¿™äº›å¯¹é½åˆ†æ•°ä¸é€šç”¨è§†é¢‘è´¨é‡è¯„ä¼°ï¼ˆVQAï¼‰æŒ‡æ ‡ç›¸å…³è”ï¼Œå¹¶æ£€æŸ¥å®ƒä»¬å¯¹ä¸‹æ¸¸TVRæ€§èƒ½çš„é¢„æµ‹èƒ½åŠ›ã€‚ä¸ºäº†æ¢ç´¢æ‰©å¤§è§„æ¨¡çš„æ–¹æ³•ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°å™¨ï¼Œä»¥æ ¹æ®ç°æœ‰æŒ‡æ ‡ä¼°è®¡å¯¹é½è´¨é‡ã€‚é™¤äº†åŸºå‡†æµ‹è¯•å¤–ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒSynTVAæ˜¯æ•°æ®é›†å¢å¼ºçš„å®è´µèµ„äº§ï¼Œèƒ½å¤Ÿé€‰æ‹©é«˜å®ç”¨æ€§çš„åˆæˆæ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬å¯ä»¥æ˜¾è‘—æé«˜TVRçš„ç»“æœã€‚é¡¹ç›®é¡µé¢å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://jasoncodemaker.github.io/SynTVA/%E6%89%BE%E5%88%B0%E3%80%82">https://jasoncodemaker.github.io/SynTVA/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02316v1">PDF</a> 7 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ–‡æœ¬è½¬è§†é¢‘ï¼ˆT2Vï¼‰åˆæˆçš„æ–°æ•°æ®é›†SynTVAï¼Œè¯¥æ•°æ®é›†æ—¨åœ¨è¯„ä¼°åˆæˆè§†é¢‘å¯¹äºæ„å»ºæ£€ç´¢æ¨¡å‹çš„æœ‰ç”¨æ€§ã€‚æ•°æ®é›†åŸºäºMSRVTTè®­ç»ƒé›†åˆ†å‰²çš„800ä¸ªä¸åŒç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆåˆæˆè§†é¢‘ï¼Œå¹¶æ²¿å››ä¸ªå…³é”®è¯­ä¹‰å¯¹é½ç»´åº¦å¯¹è§†é¢‘æ–‡æœ¬å¯¹è¿›è¡Œæ ‡æ³¨ï¼šå¯¹è±¡ä¸åœºæ™¯ã€åŠ¨ä½œã€å±æ€§å’Œæç¤ºå¿ å®åº¦ã€‚è¯„ä¼°æ¡†æ¶å°†ä¸€èˆ¬è§†é¢‘è´¨é‡è¯„ä¼°ï¼ˆVQAï¼‰æŒ‡æ ‡ä¸è¿™äº›å¯¹é½åˆ†æ•°ç›¸å…³è”ï¼Œå¹¶æ£€æŸ¥å®ƒä»¬å¯¹ä¸‹æ¸¸TVRæ€§èƒ½çš„é¢„æµ‹èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ‰©å¤§è§„æ¨¡ï¼Œè¿˜å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°å™¨ï¼Œç”¨äºæ ¹æ®ç°æœ‰æŒ‡æ ‡ä¼°è®¡å¯¹é½è´¨é‡ã€‚é™¤äº†ä½œä¸ºåŸºå‡†æµ‹è¯•ï¼ŒSynTVAå¯¹äºæ•°æ®é›†å¢å¼ºä¹Ÿæ˜¾ç¤ºå‡ºå…¶ä»·å€¼ï¼Œèƒ½å¤Ÿé€‰æ‹©é«˜è´¨é‡åˆæˆæ ·æœ¬ï¼Œæ˜¾è‘—æé«˜TVRç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SynTVAæ˜¯ä¸€ä¸ªæ–°çš„æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ–‡æœ¬è½¬è§†é¢‘åˆæˆè§†é¢‘å¯¹äºæ„å»ºæ£€ç´¢æ¨¡å‹çš„æœ‰ç”¨æ€§ã€‚</li>
<li>æ•°æ®é›†åŸºäºMSRVTTè®­ç»ƒé›†åˆ†å‰²ç”Ÿæˆåˆæˆè§†é¢‘ï¼Œå¹¶æ²¿å››ä¸ªå…³é”®è¯­ä¹‰å¯¹é½ç»´åº¦è¿›è¡Œæ ‡æ³¨ã€‚</li>
<li>è¯„ä¼°æ¡†æ¶å…³è”äº†ä¸€èˆ¬è§†é¢‘è´¨é‡è¯„ä¼°ï¼ˆVQAï¼‰æŒ‡æ ‡ä¸è¯­ä¹‰å¯¹é½åˆ†æ•°ã€‚</li>
<li>æ¡†æ¶æ£€æŸ¥äº†è¿™äº›æŒ‡æ ‡å¯¹ä¸‹æ¸¸æ–‡æœ¬è½¬è§†é¢‘æ£€ç´¢ï¼ˆTVRï¼‰æ€§èƒ½çš„é¢„æµ‹èƒ½åŠ›ã€‚</li>
<li>ä¸ºäº†æ‰©å¤§è§„æ¨¡ï¼Œå¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°å™¨æ¥ä¼°è®¡å¯¹é½è´¨é‡ã€‚</li>
<li>SynTVAä¸ä»…ç”¨äºåŸºå‡†æµ‹è¯•ï¼Œè¿˜æ˜¾ç¤ºå‡ºåœ¨æ•°æ®é›†å¢å¼ºæ–¹é¢çš„ä»·å€¼ï¼Œèƒ½å¤Ÿé€‰æ‹©æé«˜æ–‡æœ¬è½¬è§†é¢‘æ£€ç´¢ï¼ˆTVRï¼‰ç»“æœçš„é«˜è´¨é‡åˆæˆæ ·æœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02316">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b9a81b92ce24407da5d9a433cbd18502.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-052491a28bb0676d239c073f57379e31.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-65b909c264f69964b217b5b91508a252.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-52c872b90857606760bc1857880e0a7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac94ef6994f1a3500fc9eccded1adbff.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Uncertainty-aware-Reward-Design-Process"><a href="#Uncertainty-aware-Reward-Design-Process" class="headerlink" title="Uncertainty-aware Reward Design Process"></a>Uncertainty-aware Reward Design Process</h2><p><strong>Authors:Yang Yang, Xiaolu Zhou, Bosong Ding, Miao Xin</strong></p>
<p>Designing effective reward functions is a cornerstone of reinforcement learning (RL), yet it remains a challenging process due to the inefficiencies and inconsistencies inherent in conventional reward engineering methodologies. Recent advances have explored leveraging large language models (LLMs) to automate reward function design. However, their suboptimal performance in numerical optimization often yields unsatisfactory reward quality, while the evolutionary search paradigm demonstrates inefficient utilization of simulation resources, resulting in prohibitively lengthy design cycles with disproportionate computational overhead. To address these challenges, we propose the Uncertainty-aware Reward Design Process (URDP), a novel framework that integrates large language models to streamline reward function design and evaluation in RL environments. URDP quantifies candidate reward function uncertainty based on self-consistency analysis, enabling simulation-free identification of ineffective reward components while discovering novel reward components. Furthermore, we introduce uncertainty-aware Bayesian optimization (UABO), which incorporates uncertainty estimation to significantly enhance hyperparameter configuration efficiency. Finally, we construct a bi-level optimization architecture by decoupling the reward component optimization and the hyperparameter tuning. URDP orchestrates synergistic collaboration between the reward logic reasoning of the LLMs and the numerical optimization strengths of the Bayesian Optimization. We conduct a comprehensive evaluation of URDP across 35 diverse tasks spanning three benchmark environments. Our experimental results demonstrate that URDP not only generates higher-quality reward functions but also achieves significant improvements in the efficiency of automated reward design compared to existing approaches. </p>
<blockquote>
<p>è®¾è®¡æœ‰æ•ˆçš„å¥–åŠ±å‡½æ•°æ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ ¸å¿ƒï¼Œä½†ç”±äºä¼ ç»Ÿå¥–åŠ±å·¥ç¨‹æ–¹æ³•ä¸­çš„æ•ˆç‡ä½å’Œä¸ä¸€è‡´æ€§ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è¿‡ç¨‹ã€‚æœ€è¿‘çš„è¿›å±•å·²ç»æ¢ç´¢äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è‡ªåŠ¨è®¾è®¡å¥–åŠ±å‡½æ•°ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ•°å€¼ä¼˜åŒ–ä¸­çš„æ¬¡ä¼˜æ€§èƒ½å¯¼è‡´çš„å¥–åŠ±è´¨é‡ä»¤äººä¸æ»¡æ„ï¼Œè€Œè¿›åŒ–æœç´¢æ¨¡å¼è¡¨ç°å‡ºæ¨¡æ‹Ÿèµ„æºåˆ©ç”¨ä¸è¶³ï¼Œå¯¼è‡´è®¾è®¡å‘¨æœŸè¿‡é•¿ä¸”è®¡ç®—å¼€é”€ä¸æˆæ¯”ä¾‹ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºä¸ç¡®å®šæ€§çš„å¥–åŠ±è®¾è®¡è¿‡ç¨‹ï¼ˆURDPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨æ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹æ¥ç®€åŒ–RLç¯å¢ƒä¸­çš„å¥–åŠ±å‡½æ•°è®¾è®¡å’Œè¯„ä¼°ã€‚URDPåŸºäºè‡ªæˆ‘ä¸€è‡´æ€§åˆ†æé‡åŒ–å€™é€‰å¥–åŠ±å‡½æ•°çš„ä¸ç¡®å®šæ€§ï¼Œå®ç°åœ¨æ— éœ€æ¨¡æ‹Ÿçš„æƒ…å†µä¸‹è¯†åˆ«æ— æ•ˆçš„å¥–åŠ±ç»„ä»¶ï¼ŒåŒæ—¶å‘ç°æ–°é¢–çš„å¥–åŠ±ç»„ä»¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºä¸ç¡®å®šæ€§çš„è´å¶æ–¯ä¼˜åŒ–ï¼ˆUABOï¼‰ï¼Œå®ƒé€šè¿‡ç»“åˆä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œæå¤§åœ°æé«˜äº†è¶…å‚æ•°é…ç½®çš„æ•ˆç‡ã€‚æœ€åï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªåŒå±‚ä¼˜åŒ–æ¶æ„ï¼Œé€šè¿‡è§£è€¦å¥–åŠ±ç»„ä»¶ä¼˜åŒ–å’Œè¶…å‚æ•°è°ƒæ•´ã€‚URDPååŒè°ƒåº¦å¤§å‹è¯­è¨€æ¨¡å‹çš„å¥–åŠ±é€»è¾‘æ¨ç†å’Œè´å¶æ–¯ä¼˜åŒ–çš„æ•°å€¼ä¼˜åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†ç¯å¢ƒä¸­çš„35ä¸ªä¸åŒä»»åŠ¡ä¸Šå…¨é¢è¯„ä¼°äº†URDPã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒURDPä¸ä»…ç”Ÿæˆæ›´é«˜è´¨é‡çš„å¥–åŠ±å‡½æ•°ï¼Œè€Œä¸”åœ¨è‡ªåŠ¨åŒ–å¥–åŠ±è®¾è®¡çš„æ•ˆç‡æ–¹é¢å®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02256v1">PDF</a> 34 pages, 9 figures</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä¸­è®¾è®¡æœ‰æ•ˆçš„å¥–åŠ±å‡½æ•°æ˜¯æ ¸å¿ƒç¯èŠ‚ï¼Œä½†ä¼ ç»Ÿæ–¹æ³•å­˜åœ¨æ•ˆç‡ä½ä¸‹å’Œç»“æœä¸ä¸€è‡´çš„é—®é¢˜ã€‚æœ€è¿‘å¼€å§‹å°è¯•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨åŒ–è®¾è®¡å¥–åŠ±å‡½æ•°ï¼Œä½†æ•°å€¼ä¼˜åŒ–ä¸­å­˜åœ¨æ€§èƒ½ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„å¥–åŠ±è®¾è®¡è¿‡ç¨‹ï¼ˆURDPï¼‰ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥ä¼˜åŒ–å¥–åŠ±å‡½æ•°çš„è®¾è®¡å’Œè¯„ä¼°ã€‚URDPé€šè¿‡è‡ªæˆ‘ä¸€è‡´æ€§åˆ†æé‡åŒ–å€™é€‰å¥–åŠ±å‡½æ•°çš„ä¸ç¡®å®šæ€§ï¼Œæ— éœ€æ¨¡æ‹Ÿå³å¯è¯†åˆ«æ— æ•ˆçš„å¥–åŠ±ç»„ä»¶å¹¶å‘ç°æ–°çš„å¥–åŠ±ç»„ä»¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†åŸºäºä¸ç¡®å®šæ€§çš„è´å¶æ–¯ä¼˜åŒ–ï¼ˆUABOï¼‰ï¼Œä»¥æé«˜è¶…å‚æ•°é…ç½®çš„è°ƒæ•´æ•ˆç‡ã€‚é€šè¿‡æ„å»ºä¸¤çº§ä¼˜åŒ–æ¶æ„ï¼Œå®ç°å¥–åŠ±ç»„ä»¶ä¼˜åŒ–å’Œè¶…å‚æ•°è°ƒæ•´çš„è§£è€¦ã€‚åœ¨å¤šä¸ªåŸºå‡†ç¯å¢ƒå’Œä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒURDPä¸ä»…ç”Ÿæˆäº†æ›´é«˜è´¨é‡çš„å¥–åŠ±å‡½æ•°ï¼Œè€Œä¸”åœ¨è‡ªåŠ¨åŒ–å¥–åŠ±è®¾è®¡æ–¹é¢ä¹Ÿæœ‰æ˜¾è‘—æ•ˆç‡æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä¸­å¥–åŠ±å‡½æ•°è®¾è®¡æ˜¯æ ¸å¿ƒæŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>ä¼ ç»Ÿå¥–åŠ±å‡½æ•°è®¾è®¡æ–¹æ³•çš„æ•ˆç‡å’Œä¸€è‡´æ€§å­˜åœ¨é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨åŒ–å¥–åŠ±å‡½æ•°è®¾è®¡æ˜¯ä¸€ç§æ–°å…´è¶‹åŠ¿ã€‚</li>
<li>æ•°å€¼ä¼˜åŒ–åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨ä¸­å­˜åœ¨æ€§èƒ½ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>URDPæ¡†æ¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œè´å¶æ–¯ä¼˜åŒ–ï¼Œæé«˜äº†å¥–åŠ±å‡½æ•°è®¾è®¡çš„æ•ˆç‡å’Œè´¨é‡ã€‚</li>
<li>URDPé€šè¿‡è‡ªæˆ‘ä¸€è‡´æ€§åˆ†æé‡åŒ–å¥–åŠ±å‡½æ•°çš„ä¸ç¡®å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-31c3a951fbf182c89190869f33a31f1a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7064c5dca7c88d130c7b0bc1bbc14af9.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Scaling-LLM-Planning-NL2FLOW-for-Parametric-Problem-Generation-and-Rigorous-Evaluation"><a href="#Scaling-LLM-Planning-NL2FLOW-for-Parametric-Problem-Generation-and-Rigorous-Evaluation" class="headerlink" title="Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and   Rigorous Evaluation"></a>Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and   Rigorous Evaluation</h2><p><strong>Authors:Jungkoo Kang</strong></p>
<p>Progress in enhancing large language model (LLM) planning and reasoning capabilities is significantly hampered by the bottleneck of scalable, reliable data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully automated system for parametrically generating planning problems - expressed in natural language, a structured intermediate representation, and formal PDDL - and rigorously evaluating the quality of generated plans. I demonstrate NL2FLOWâ€™s capabilities by generating a dataset of 2296 problems in the automated workflow generation domain and evaluating multiple open-sourced, instruct-tuned LLMs. My results reveal that the highest performing models achieved 86% success in generating valid plans and 69% in generating optimal plans, specifically for problems with feasible solutions. Regression analysis shows that the influence of problem characteristics on plan generation is contingent on both model and prompt design. Notably, I observed that the highest success rate for translating natural language into a JSON representation of a plan was lower than the highest rate of generating a valid plan directly. This suggests that unnecessarily decomposing the reasoning task - introducing intermediate translation steps - may actually degrade performance, implying a benefit to models capable of reasoning directly from natural language to action. As I scale LLM reasoning to increasingly complex problems, the bottlenecks and sources of error within these systems will inevitably shift. Therefore, a dynamic understanding of these limitations - and the tools to systematically reveal them - will be crucial for unlocking the full potential of LLMs as intelligent problem solvers. </p>
<blockquote>
<p>åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§„åˆ’å’Œæ¨ç†èƒ½åŠ›æ–¹é¢ï¼Œå¯æ‰©å±•çš„å¯é æ•°æ®ç”Ÿæˆå’Œè¯„ä¼°ç“¶é¢ˆæ˜¾è‘—é˜»ç¢äº†å…¶è¿›å±•ã€‚ä¸ºäº†å…‹æœè¿™ä¸€éš¾é¢˜ï¼Œæˆ‘æ¨å‡ºäº†NL2FLOWç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨çš„å‚æ•°åŒ–ç”Ÿæˆè§„åˆ’é—®é¢˜çš„ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿä»¥è‡ªç„¶è¯­è¨€ã€ç»“æ„åŒ–ä¸­é—´è¡¨ç¤ºå’Œæ­£å¼PDDLè¡¨è¾¾é—®é¢˜ï¼Œå¹¶ä¸¥æ ¼è¯„ä¼°ç”Ÿæˆè®¡åˆ’çš„è´¨é‡ã€‚æˆ‘é€šè¿‡ç”Ÿæˆè‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ç”Ÿæˆé¢†åŸŸä¸­çš„2296ä¸ªé—®é¢˜æ•°æ®é›†æ¥å±•ç¤ºNL2FLOWçš„åŠŸèƒ½ï¼Œå¹¶è¯„ä¼°å¤šä¸ªå¼€æºçš„ã€ç»è¿‡æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æˆ‘çš„ç»“æœè¡¨æ˜ï¼Œè¡¨ç°æœ€ä½³çš„æ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ•ˆè®¡åˆ’æ–¹é¢è¾¾åˆ°äº†86%çš„æˆåŠŸç‡ï¼Œåœ¨ç”Ÿæˆæœ€ä¼˜è®¡åˆ’æ–¹é¢è¾¾åˆ°äº†69%ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰å¯è¡Œè§£çš„é—®é¢˜ä¸Šã€‚å›å½’åˆ†æè¡¨æ˜ï¼Œé—®é¢˜ç‰¹æ€§å¯¹è®¡åˆ’ç”Ÿæˆçš„å½±å“å–å†³äºæ¨¡å‹å’Œæç¤ºè®¾è®¡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘è§‚å¯Ÿåˆ°å°†è‡ªç„¶è¯­è¨€ç¿»è¯‘æˆè®¡åˆ’JSONè¡¨ç¤ºå½¢å¼çš„æœ€é«˜æˆåŠŸç‡ä½äºç›´æ¥ç”Ÿæˆæœ‰æ•ˆè®¡åˆ’çš„æœ€é«˜æˆåŠŸç‡ã€‚è¿™è¡¨æ˜ï¼Œä¸å¿…è¦åœ°åˆ†è§£æ¨ç†ä»»åŠ¡ï¼ˆå¼•å…¥ä¸­é—´ç¿»è¯‘æ­¥éª¤ï¼‰å®é™…ä¸Šå¯èƒ½ä¼šé™ä½æ€§èƒ½ï¼Œæš—ç¤ºé‚£äº›èƒ½å¤Ÿç›´æ¥ä»è‡ªç„¶è¯­è¨€è¿›è¡Œæ¨ç†çš„æ¨¡å‹å…·æœ‰ä¼˜åŠ¿ã€‚éšç€æˆ‘å°†å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†æ‰©å±•åˆ°è¶Šæ¥è¶Šå¤æ‚çš„é—®é¢˜ï¼Œè¿™äº›ç³»ç»Ÿä¸­çš„ç“¶é¢ˆå’Œé”™è¯¯æ¥æºå°†ä¸å¯é¿å…åœ°å‘ç”Ÿå˜åŒ–ã€‚å› æ­¤ï¼Œå¯¹è¿™äº›é™åˆ¶çš„åŠ¨æ€ç†è§£ä»¥åŠç³»ç»Ÿåœ°æ­ç¤ºå®ƒä»¬çš„å·¥å…·ï¼Œå¯¹äºè§£é”å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ™ºèƒ½é—®é¢˜æ±‚è§£å™¨çš„æ½œåŠ›è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02253v1">PDF</a> 20 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>åœ¨è‡ªç„¶è¯­è¨€æ¨¡å‹è§„åˆ’èƒ½åŠ›ç”Ÿæˆå’Œè¯„ä¼°ç“¶é¢ˆçš„é—®é¢˜ä¸­ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„è§£å†³æ–¹æ¡ˆNL2FLOWã€‚å®ƒå…¨è‡ªåŠ¨å‚æ•°åŒ–ç”Ÿæˆè§„åˆ’é—®é¢˜ï¼Œå¹¶èƒ½å¯¹ç”Ÿæˆçš„è§„åˆ’è´¨é‡è¿›è¡Œä¸¥æ ¼è¯„ä¼°ã€‚é€šè¿‡å®éªŒç”Ÿæˆäº†è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹é¢†åŸŸçš„2296ä¸ªé—®é¢˜æ•°æ®é›†ï¼Œå¹¶è¯„ä¼°äº†å¤šä¸ªå¼€æºçš„æŒ‡ä»¤è°ƒä¼˜å¤§å‹è¯­è¨€æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³æ¨¡å‹ç”Ÿæˆæœ‰æ•ˆè®¡åˆ’çš„æˆåŠŸç‡è¾¾åˆ°86%ï¼Œç”Ÿæˆæœ€ä¼˜è®¡åˆ’çš„æˆåŠŸç‡è¾¾åˆ°69%ã€‚åŒæ—¶å‘ç°ï¼Œå°†æ¨ç†ä»»åŠ¡åˆ†è§£æˆä¸å¿…è¦çš„ä¸­é—´ç¿»è¯‘æ­¥éª¤å¯èƒ½ä¼šé™ä½æ€§èƒ½ï¼Œç›´æ¥æ ¹æ®è‡ªç„¶è¯­è¨€è¿›è¡Œæ¨ç†çš„æ¨¡å‹å¯èƒ½å…·æœ‰ä¼˜åŠ¿ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹è§£å†³è¶Šæ¥è¶Šå¤æ‚çš„é—®é¢˜ï¼Œå…¶ç“¶é¢ˆå’Œé”™è¯¯æ¥æºä¹Ÿä¼šå‘ç”Ÿå˜åŒ–ï¼Œå› æ­¤éœ€è¦åŠ¨æ€ç†è§£è¿™äº›é™åˆ¶å’Œç³»ç»Ÿæ­ç¤ºå·¥å…·ï¼Œä»¥è§£é”å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ™ºèƒ½é—®é¢˜æ±‚è§£å™¨çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NL2FLOWç³»ç»Ÿè§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§„åˆ’èƒ½åŠ›ç”Ÿæˆå’Œè¯„ä¼°æ–¹é¢çš„ç“¶é¢ˆé—®é¢˜ã€‚</li>
<li>NL2FLOWèƒ½å…¨è‡ªåŠ¨å‚æ•°åŒ–ç”Ÿæˆè§„åˆ’é—®é¢˜ï¼Œå¹¶ä¸¥æ ¼è¯„ä¼°ç”Ÿæˆçš„è§„åˆ’è´¨é‡ã€‚</li>
<li>é€šè¿‡å®éªŒç”Ÿæˆäº†è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹é¢†åŸŸçš„æ•°æ®é›†ï¼Œå¹¶è¯„ä¼°äº†å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æœ€ä½³æ¨¡å‹ç”Ÿæˆæœ‰æ•ˆè®¡åˆ’çš„æˆåŠŸç‡ä¸º86%ï¼Œç”Ÿæˆæœ€ä¼˜è®¡åˆ’çš„æˆåŠŸç‡ä¸º69%ã€‚</li>
<li>ä¸å¿…è¦çš„ä¸­é—´ç¿»è¯‘æ­¥éª¤å¯èƒ½ä¼šé™ä½å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>ç›´æ¥ä»è‡ªç„¶è¯­è¨€è¿›è¡Œæ¨ç†çš„æ¨¡å‹å¯èƒ½å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02253">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3fc3690bb11e52aad851ae8aaf547400.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-217c0ded8e473a3c992ef18af7f01162.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00bc40f065f7d76ea30ca91896bf1a9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b92f62b147474cdf44f9627ba3d6dd4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7fa44fcd6163eea0c5b8fe4671c915a6.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ESTR-CoT-Towards-Explainable-and-Accurate-Event-Stream-based-Scene-Text-Recognition-with-Chain-of-Thought-Reasoning"><a href="#ESTR-CoT-Towards-Explainable-and-Accurate-Event-Stream-based-Scene-Text-Recognition-with-Chain-of-Thought-Reasoning" class="headerlink" title="ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text   Recognition with Chain-of-Thought Reasoning"></a>ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text   Recognition with Chain-of-Thought Reasoning</h2><p><strong>Authors:Xiao Wang, Jingtao Jiang, Qiang Chen, Lan Chen, Lin Zhu, Yaowei Wang, Yonghong Tian, Jin Tang</strong></p>
<p>Event stream based scene text recognition is a newly arising research topic in recent years which performs better than the widely used RGB cameras in extremely challenging scenarios, especially the low illumination, fast motion. Existing works either adopt end-to-end encoder-decoder framework or large language models for enhanced recognition, however, they are still limited by the challenges of insufficient interpretability and weak contextual logical reasoning. In this work, we propose a novel chain-of-thought reasoning based event stream scene text recognition framework, termed ESTR-CoT. Specifically, we first adopt the vision encoder EVA-CLIP (ViT-G&#x2F;14) to transform the input event stream into tokens and utilize a Llama tokenizer to encode the given generation prompt. A Q-former is used to align the vision token to the pre-trained large language model Vicuna-7B and output both the answer and chain-of-thought (CoT) reasoning process simultaneously. Our framework can be optimized using supervised fine-tuning in an end-to-end manner. In addition, we also propose a large-scale CoT dataset to train our framework via a three stage processing (i.e., generation, polish, and expert verification). This dataset provides a solid data foundation for the development of subsequent reasoning-based large models. Extensive experiments on three event stream STR benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the effectiveness and interpretability of our proposed framework. The source code and pre-trained models will be released on <a target="_blank" rel="noopener" href="https://github.com/Event-AHU/ESTR-CoT">https://github.com/Event-AHU/ESTR-CoT</a>. </p>
<blockquote>
<p>åŸºäºäº‹ä»¶æµçš„åœºæ™¯æ–‡æœ¬è¯†åˆ«æ˜¯è¿‘å¹´æ¥æ–°å…´çš„ç ”ç©¶è¯¾é¢˜ï¼Œå®ƒåœ¨æç«¯æŒ‘æˆ˜åœºæ™¯ï¼ˆå°¤å…¶æ˜¯ä½å…‰ç…§ã€å¿«é€Ÿè¿åŠ¨ï¼‰ä¸­çš„è¡¨ç°ä¼˜äºå¹¿æ³›ä½¿ç”¨çš„RGBç›¸æœºã€‚ç°æœ‰çš„å·¥ä½œè¦ä¹ˆé‡‡ç”¨ç«¯åˆ°ç«¯çš„ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ï¼Œè¦ä¹ˆé‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¢å¼ºè¯†åˆ«ï¼Œç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶é¢ä¸´ç€è§£é‡Šæ€§ä¸è¶³å’Œä¸Šä¸‹æ–‡é€»è¾‘æ¨ç†èƒ½åŠ›å¼±çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ€ç»´é“¾æ¨ç†çš„äº‹ä»¶æµåœºæ™¯æ–‡æœ¬è¯†åˆ«æ–°æ¡†æ¶ï¼Œç§°ä¸ºESTR-CoTã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆé‡‡ç”¨è§†è§‰ç¼–ç å™¨EVA-CLIPï¼ˆViT-G&#x2F;14ï¼‰å°†è¾“å…¥çš„äº‹ä»¶æµè½¬æ¢ä¸ºä»¤ç‰Œï¼Œå¹¶ä½¿ç”¨Llamaä»¤ç‰Œç”Ÿæˆå™¨å¯¹ç»™å®šçš„ç”Ÿæˆæç¤ºè¿›è¡Œç¼–ç ã€‚Q-formerç”¨äºå°†è§†è§‰ä»¤ç‰Œä¸é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹Vicuna-7Bå¯¹é½ï¼Œå¹¶åŒæ—¶è¾“å‡ºç­”æ¡ˆå’Œæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥é€šè¿‡ç«¯åˆ°ç«¯çš„ç›‘ç£å¾®è°ƒæ¥è¿›è¡Œä¼˜åŒ–ã€‚</p>
<p>æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªå¤§è§„æ¨¡CoTæ•°æ®é›†ï¼Œé€šè¿‡ä¸‰ä¸ªé˜¶æ®µï¼ˆå³ç”Ÿæˆã€æ¶¦è‰²å’Œä¸“å®¶éªŒè¯ï¼‰æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¡†æ¶ã€‚è¯¥æ•°æ®é›†ä¸ºéšååŸºäºæ¨ç†çš„å¤§å‹æ¨¡å‹çš„å‘å±•æä¾›äº†åšå®çš„æ•°æ®åŸºç¡€ã€‚åœ¨ä¸‰ä¸ªäº‹ä»¶æµSTRåŸºå‡†æ•°æ®é›†ï¼ˆå³EventSTRã€WordArt<em>å’ŒIC15</em>ï¼‰ä¸Šçš„å¤§é‡å®éªŒå……åˆ†éªŒè¯äº†æˆ‘ä»¬æ‰€æå‡ºæ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œå¯è§£é‡Šæ€§ã€‚æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Event-AHU/ESTR-CoT">https://github.com/Event-AHU/ESTR-CoT</a>ä¸Šå‘å¸ƒã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02200v1">PDF</a> A Strong Baseline for Reasoning based Event Stream Scene Text   Recognition</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åŸºäºäº‹ä»¶æµåœºæ™¯æ–‡æœ¬è¯†åˆ«çš„æ–°æ¡†æ¶ï¼Œç§°ä¸ºESTR-CoTã€‚å®ƒé‡‡ç”¨é“¾å¼æ€ç»´æ¨ç†ï¼Œèƒ½åº”å¯¹ä½å…‰ç…§ã€å¿«é€Ÿè¿åŠ¨ç­‰æŒ‘æˆ˜åœºæ™¯ä¸‹çš„æ–‡æœ¬è¯†åˆ«ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è§†è§‰ç¼–ç å™¨EVA-CLIPå’ŒQ-formerï¼Œå°†äº‹ä»¶æµè½¬æ¢ä¸ºä»¤ç‰Œå¹¶ä¸é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹Vicuna-7Bå¯¹é½ï¼ŒåŒæ—¶è¾“å‡ºç­”æ¡ˆå’Œæ¨ç†è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œè¿˜é€šè¿‡ä¸‰é˜¶æ®µå¤„ç†æµç¨‹æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„CoTæ•°æ®é›†ç”¨äºè®­ç»ƒã€‚åœ¨å¤šä¸ªäº‹ä»¶æµSTRåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œå¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ESTR-CoTæ˜¯ä¸€ä¸ªåŸºäºäº‹ä»¶æµåœºæ™¯æ–‡æœ¬è¯†åˆ«çš„æ–°æ¡†æ¶ï¼Œé€‚ç”¨äºä½å…‰ç…§å’Œå¿«é€Ÿè¿åŠ¨ç­‰æŒ‘æˆ˜åœºæ™¯ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†è§†è§‰ç¼–ç å™¨EVA-CLIPå’ŒQ-formeræŠ€æœ¯ï¼Œç”¨äºå°†äº‹ä»¶æµè½¬æ¢ä¸ºä»¤ç‰Œå¹¶ä¸å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½ã€‚</li>
<li>æ¡†æ¶èƒ½åŒæ—¶è¾“å‡ºç­”æ¡ˆå’Œé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†è¿‡ç¨‹ï¼Œå¢å¼ºäº†å¯è§£é‡Šæ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤§è§„æ¨¡CoTæ•°æ®é›†çš„æ„å»ºæ–¹æ³•ï¼Œé€šè¿‡ä¸‰é˜¶æ®µå¤„ç†æµç¨‹ç”¨äºæ¡†æ¶è®­ç»ƒã€‚</li>
<li>æ¡†æ¶é€šè¿‡ç«¯åˆ°ç«¯çš„ç›‘ç£å¾®è°ƒæ–¹å¼å¯ä»¥è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02200">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-93d285ce0be96ddadb83ba01c5153c7d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1264f79e24c9965faff379e055ba979b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e9912729df5a557d389b9d104e6e8ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c2c5a9c4bb2e9d55bdb12e97c50ef83.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Data-Diversification-Methods-In-Alignment-Enhance-Math-Performance-In-LLMs"><a href="#Data-Diversification-Methods-In-Alignment-Enhance-Math-Performance-In-LLMs" class="headerlink" title="Data Diversification Methods In Alignment Enhance Math Performance In   LLMs"></a>Data Diversification Methods In Alignment Enhance Math Performance In   LLMs</h2><p><strong>Authors:Berkan Dokmeci, Qingyang Wu, Ben Athiwaratkun, Ce Zhang, Shuaiwen Leon Song, James Zou</strong></p>
<p>While recent advances in preference learning have enhanced alignment in human feedback, mathematical reasoning remains a persistent challenge. We investigate how data diversification strategies in preference optimization can improve the mathematical reasoning abilities of large language models (LLMs). We evaluate three common data generation methods: temperature sampling, Chain-of-Thought prompting, and Monte Carlo Tree Search (MCTS), and introduce Diversified-ThinkSolve (DTS), a novel structured approach that systematically decomposes problems into diverse reasoning paths. Our results show that with strategically diversified preference data, models can substantially improve mathematical reasoning performance, with the best approach yielding gains of 7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong performance, DTS incurs only a marginal computational overhead (1.03x) compared to the baseline, while MCTS is nearly five times more costly with lower returns. These findings demonstrate that structured exploration of diverse problem-solving methods creates more effective preference data for mathematical alignment than traditional approaches. </p>
<blockquote>
<p>è™½ç„¶åå¥½å­¦ä¹ æ–¹é¢çš„æœ€æ–°è¿›å±•å¢å¼ºäº†äººç±»åé¦ˆçš„å¯¹é½æ€§ï¼Œä½†æ•°å­¦æ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªæŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬ç ”ç©¶äº†åå¥½ä¼˜åŒ–ä¸­çš„æ•°æ®å¤šæ ·åŒ–ç­–ç•¥å¦‚ä½•æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸‰ç§å¸¸è§çš„æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼šæ¸©åº¦é‡‡æ ·ã€æ€ç»´é“¾æç¤ºå’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼Œå¹¶å¼•å…¥äº†å¤šæ ·åŒ–æ€è€ƒæ±‚è§£ï¼ˆDTSï¼‰è¿™ä¸€æ–°å‹ç»“æ„åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç³»ç»Ÿåœ°å°†é—®é¢˜åˆ†è§£ä¸ºå¤šæ ·åŒ–çš„æ¨ç†è·¯å¾„ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œé€šè¿‡é‡‡ç”¨ç­–ç•¥æ€§çš„å¤šæ ·åŒ–åå¥½æ•°æ®ï¼Œæ¨¡å‹èƒ½å¤Ÿæ˜¾è‘—æé«˜æ•°å­¦æ¨ç†æ€§èƒ½ï¼Œå…¶ä¸­æœ€ä½³æ–¹æ³•èƒ½å¤Ÿåœ¨GSM8Kä¸Šæå‡7.1%ï¼Œåœ¨MATHä¸Šæå‡4.2%ï¼Œè¶…è¿‡åŸºå‡†æ¨¡å‹ã€‚å°½ç®¡DTSè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å…¶ç›¸å¯¹äºåŸºå‡†çš„è®¡ç®—å¼€é”€ä»…ç•¥æœ‰å¢åŠ ï¼ˆ1.03å€ï¼‰ï¼Œè€ŒMCTSçš„è®¡ç®—æˆæœ¬åˆ™é«˜å‡ºè¿‘äº”å€ä¸”å›æŠ¥è¾ƒä½ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå¯¹å¤šæ ·åŒ–é—®é¢˜è§£å†³æ–¹æ³•è¿›è¡Œç»“æ„åŒ–æ¢ç´¢èƒ½å¤Ÿåˆ›å»ºæ›´æœ‰æ•ˆçš„åå¥½æ•°æ®ï¼Œä»¥å®ç°æ•°å­¦å¯¹é½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02173v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸåå¥½å­¦ä¹ é¢†åŸŸçš„è¿›å±•å·²æé«˜äº†äººç±»åé¦ˆçš„å¥‘åˆåº¦ï¼Œä½†æ•°å­¦æ¨ç†ä»æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†åå¥½ä¼˜åŒ–ä¸­çš„æ•°æ®å¤šæ ·åŒ–ç­–ç•¥å¦‚ä½•æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸‰ç§å¸¸è§çš„æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼šæ¸©åº¦é‡‡æ ·ã€Chain-of-Thoughtæç¤ºå’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼Œå¹¶å¼•å…¥äº†Diversified-ThinkSolveï¼ˆDTSï¼‰è¿™ä¸€æ–°å‹ç»“æ„åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½ç³»ç»ŸåŒ–åœ°å°†é—®é¢˜åˆ†è§£ä¸ºå¤šæ ·åŒ–çš„æ¨ç†è·¯å¾„ã€‚ç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡ç­–ç•¥æ€§åœ°é‡‡ç”¨å¤šæ ·åŒ–åå¥½æ•°æ®ï¼Œæ¨¡å‹èƒ½åœ¨GSM8Kä¸Šæé«˜7.1%ã€MATHä¸Šæé«˜4.2%çš„æ•°å­¦æ¨ç†æ€§èƒ½ã€‚å°½ç®¡DTSè¡¨ç°å¼ºåŠ²ï¼Œä½†ç›¸æ¯”åŸºç¡€æ¨¡å‹åªäº§ç”Ÿäº†è½»å¾®çš„è®¡ç®—å¼€é”€ï¼ˆ1.03å€ï¼‰ï¼Œè€ŒMCTSæˆæœ¬å‡ ä¹æ˜¯äº”å€ä¸”å›æŠ¥è¾ƒä½ã€‚è¿™è¡¨æ˜ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå¤šæ ·åŒ–é—®é¢˜è§£å†³æ–¹æ³•çš„ç»“æ„åŒ–æ¢ç´¢èƒ½ä¸ºæ•°å­¦å¯¹é½åˆ›é€ æ›´æœ‰æ•ˆçš„åå¥½æ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸåå¥½å­¦ä¹ è¿›å±•æé«˜äº†äººç±»åé¦ˆå¥‘åˆåº¦ï¼Œä½†æ•°å­¦æ¨ç†ä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶äº†æ•°æ®å¤šæ ·åŒ–ç­–ç•¥åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ•°å­¦æ¨ç†èƒ½åŠ›ä¸­çš„åº”ç”¨ã€‚</li>
<li>è¯„ä¼°äº†ä¸‰ç§æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼šæ¸©åº¦é‡‡æ ·ã€Chain-of-Thoughtæç¤ºå’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ã€‚</li>
<li>å¼•å…¥äº†Diversified-ThinkSolveï¼ˆDTSï¼‰è¿™ä¸€æ–°å‹ç»“æ„åŒ–æ–¹æ³•ï¼Œèƒ½ç³»ç»ŸåŒ–åˆ†è§£é—®é¢˜ä¸ºå¤šæ ·åŒ–æ¨ç†è·¯å¾„ã€‚</li>
<li>é€šè¿‡ç­–ç•¥æ€§é‡‡ç”¨å¤šæ ·åŒ–åå¥½æ•°æ®ï¼Œæ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ã€‚</li>
<li>DTSç›¸æ¯”åŸºç¡€æ¨¡å‹ä»…äº§ç”Ÿè½»å¾®è®¡ç®—å¼€é”€ï¼Œè€ŒMCTSæˆæœ¬é«˜æ˜‚ä¸”å›æŠ¥è¾ƒä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02173">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-34235c398e9ca5bc691598f788611db0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92c257f9e9a8703e5f66d1cfb77eac24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-762243fa62e52a8aaf58e970f96f8a6b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Reasoning-or-Not-A-Comprehensive-Evaluation-of-Reasoning-LLMs-for-Dialogue-Summarization"><a href="#Reasoning-or-Not-A-Comprehensive-Evaluation-of-Reasoning-LLMs-for-Dialogue-Summarization" class="headerlink" title="Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for   Dialogue Summarization"></a>Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for   Dialogue Summarization</h2><p><strong>Authors:Keyan Jin, Yapeng Wang, Leonel Santos, Tao Fang, Xu Yang, Sio Kei Im, Hugo GonÃ§alo Oliveira</strong></p>
<p>Dialogue summarization is a challenging task with significant practical value in customer service, meeting analysis, and conversational AI. Although large language models (LLMs) have achieved substantial progress in summarization tasks, the performance of step-by-step reasoning architectures-specifically Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent abstraction and conciseness. In this work, we present the first comprehensive and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning LLMs across three major paradigms-generic, role-oriented, and query-oriented dialogue summarization. Our study spans diverse languages, domains, and summary lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and advanced evaluation protocols that include both LLM-based automatic metrics and human-inspired criteria. Contrary to trends in other reasoning-intensive tasks, our findings show that explicit stepwise reasoning does not consistently improve dialogue summarization quality. Instead, reasoning LLMs are often prone to verbosity, factual inconsistencies, and less concise summaries compared to their non-reasoning counterparts. Through scenario-specific analyses and detailed case studies, we further identify when and why explicit reasoning may fail to benefit-or even hinder-summarization in complex dialogue contexts. Our work provides new insights into the limitations of current reasoning LLMs and highlights the need for targeted modeling and evaluation strategies for real-world dialogue summarization. </p>
<blockquote>
<p>å¯¹è¯æ‘˜è¦æ˜¯ä¸€é¡¹å…·æœ‰å®¢æˆ·æœåŠ¡çš„å®é™…åº”ç”¨ä»·å€¼çš„æŒ‘æˆ˜ä»»åŠ¡ï¼Œåœ¨ä¼šè®®åˆ†æå’Œå¯¹è¯äººå·¥æ™ºèƒ½ä¸­ä¹Ÿæœ‰é‡è¦ä»·å€¼ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ‘˜è¦ä»»åŠ¡ä¸­å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†åœ¨éœ€è¦å¹¶å‘æŠ½è±¡å’Œç®€æ´çš„å¯¹è¯åœºæ™¯ä¸­ï¼Œé€æ­¥æ¨ç†æ¶æ„å°¤å…¶æ˜¯é•¿æ€è€ƒé“¾ï¼ˆCoTï¼‰çš„å®æ–½æ–¹æ¡ˆï¼ˆå¦‚OpenAI-o1å’ŒDeepSeek-R1ï¼‰çš„è¡¨ç°å°šæœªè¢«æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„æ¨ç†LLMå’Œéæ¨ç†LLMè¿›è¡Œäº†é¦–æ¬¡å…¨é¢ç³»ç»Ÿçš„è¯„ä¼°ï¼Œæ¶µç›–äº†ä¸‰ç§ä¸»è¦çš„èŒƒå¼ï¼šé€šç”¨ã€é¢å‘è§’è‰²å’Œé¢å‘æŸ¥è¯¢çš„å¯¹è¯æ‘˜è¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ¶µç›–äº†å¤šç§è¯­è¨€ã€é¢†åŸŸå’Œæ‘˜è¦é•¿åº¦ï¼Œåˆ©ç”¨å¼ºå¤§çš„åŸºå‡†æµ‹è¯•ï¼ˆSAMSumã€DialogSumã€CSDSå’ŒQMSumï¼‰å’Œé«˜çº§è¯„ä¼°åè®®ï¼ŒåŒ…æ‹¬åŸºäºLLMçš„è‡ªåŠ¨æŒ‡æ ‡å’Œäººç±»å¯å‘æ ‡å‡†ã€‚ä¸å…¶ä»–æ¨ç†å¯†é›†å‹ä»»åŠ¡çš„è¶‹åŠ¿ç›¸åï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ˜ç¡®çš„é€æ­¥æ¨ç†å¹¶ä¸ä¸€å®šèƒ½æé«˜å¯¹è¯æ‘˜è¦çš„è´¨é‡ã€‚ç›¸åï¼Œæ¨ç†LLMå¾€å¾€å®¹æ˜“äº§ç”Ÿå†—é•¿ã€äº‹å®ä¸ä¸€è‡´å’Œä¸å¤Ÿç®€æ´çš„æ‘˜è¦ï¼Œä¸éæ¨ç†çš„åŒè¡Œç›¸æ¯”è¡¨ç°è¾ƒå·®ã€‚é€šè¿‡é’ˆå¯¹ç‰¹å®šåœºæ™¯çš„åˆ†æå’Œè¯¦ç»†çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ç¡®å®šäº†ä½•æ—¶ä»¥åŠä¸ºä»€ä¹ˆæ˜ç¡®çš„æ¨ç†å¯èƒ½æ— æ³•åœ¨ç»™å¤æ‚çš„å¯¹è¯ç¯å¢ƒå¸¦æ¥å¥½å¤„ï¼Œç”šè‡³å¯èƒ½äº§ç”Ÿé˜»ç¢ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†å¯¹å½“å‰æ¨ç†LLMå±€é™æ€§çš„æ–°è§è§£ï¼Œå¹¶å¼ºè°ƒäº†ä¸ºç°å®ä¸–ç•Œå¯¹è¯æ‘˜è¦è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å»ºæ¨¡å’Œè¯„ä¼°ç­–ç•¥çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02145v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¯¹è¯æ‘˜è¦æ˜¯ä¸€é¡¹å…·æœ‰å®é™…ä»·å€¼çš„æŒ‘æˆ˜ä»»åŠ¡ï¼Œæ¶‰åŠå®¢æˆ·æœåŠ¡ã€ä¼šè®®åˆ†æå’Œå¯¹è¯äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ‘˜è¦ä»»åŠ¡ä¸­å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†åœ¨éœ€è¦å¹¶å‘æŠ½è±¡å’Œç®€æ´æ€§çš„å¯¹è¯åœºæ™¯ä¸­ï¼Œé€æ­¥æ¨ç†æ¶æ„ï¼ˆå¦‚é•¿é“¾æ€ç»´ï¼‰çš„æ€§èƒ½ä»æœªæ¢ç´¢ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å…¨é¢ç³»ç»Ÿåœ°è¯„ä¼°äº†æœ€å…ˆè¿›çš„æ¨ç†LLMå’Œéæ¨ç†LLMåœ¨ä¸‰ç§ä¸»è¦èŒƒå¼ä¸‹çš„è¡¨ç°ï¼ŒåŒ…æ‹¬é€šç”¨ã€é¢å‘è§’è‰²å’Œé¢å‘æŸ¥è¯¢çš„å¯¹è¯æ‘˜è¦ã€‚ç ”ç©¶æ¶‰åŠå¤šç§è¯­è¨€ã€é¢†åŸŸå’Œæ‘˜è¦é•¿åº¦ï¼Œåˆ©ç”¨å¼ºå¤§çš„åŸºå‡†æµ‹è¯•ï¼ˆSAMSumã€DialogSumã€CSDSå’ŒQMSumï¼‰å’Œå…ˆè¿›çš„è¯„ä¼°åè®®ï¼ŒåŒ…æ‹¬åŸºäºLLMçš„è‡ªåŠ¨æŒ‡æ ‡å’Œäººç±»å¯å‘æ ‡å‡†ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨å¯¹è¯æ‘˜è¦ä¸­ï¼Œæ˜ç¡®çš„é€æ­¥æ¨ç†å¹¶ä¸æ€»æ˜¯èƒ½æé«˜è´¨é‡ã€‚ç›¸åï¼Œæ¨ç†LLMå¾€å¾€å®¹æ˜“äº§ç”Ÿå†—é•¿ã€äº‹å®ä¸ä¸€è‡´å’Œä¸å¤Ÿç®€æ´çš„æ‘˜è¦ï¼Œç›¸æ¯”éæ¨ç†æ¨¡å‹è¡¨ç°ä¸ä½³ã€‚é€šè¿‡æƒ…æ™¯ç‰¹å®šåˆ†æå’Œè¯¦ç»†æ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ç¡®å®šäº†ä½•æ—¶ä»¥åŠä¸ºä»€ä¹ˆæ˜¾å¼æ¨ç†å¯èƒ½ä¸åˆ©äºå¤æ‚å¯¹è¯ä¸Šä¸‹æ–‡ä¸­çš„æ‘˜è¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶æä¾›äº†å¯¹å½“å‰æ¨ç†LLMå±€é™æ€§çš„æ–°è§è§£ï¼Œå¹¶å¼ºè°ƒäº†é’ˆå¯¹çœŸå®ä¸–ç•Œå¯¹è¯æ‘˜è¦è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å»ºæ¨¡å’Œè¯„ä¼°ç­–ç•¥çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¯¹è¯æ‘˜è¦æ˜¯ä¸€é¡¹å…·æœ‰å®é™…ä»·å€¼çš„æŒ‘æˆ˜ä»»åŠ¡ï¼Œå°¤å…¶åœ¨å®¢æˆ·æœåŠ¡ã€ä¼šè®®åˆ†æå’Œå¯¹è¯äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸã€‚</li>
<li>ç›®å‰å¯¹å¯¹è¯åœºæ™¯ä¸­é€æ­¥æ¨ç†æ¶æ„çš„æ€§èƒ½ä»ä¸äº†è§£ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹é•¿é“¾æ€ç»´ç­‰å®ç°çš„ç ”ç©¶ç¼ºå¤±ã€‚</li>
<li>ç ”ç©¶é¦–æ¬¡å…¨é¢è¯„ä¼°äº†æ¨ç†LLMå’Œéæ¨ç†LLMåœ¨å¯¹è¯æ‘˜è¦ä¸­çš„è¡¨ç°å·®å¼‚ã€‚</li>
<li>ç ”ç©¶æ¶‰åŠå¤šç§è¯­è¨€ã€é¢†åŸŸå’Œæ‘˜è¦é•¿åº¦ï¼Œå¹¶é‡‡ç”¨äº†å¼ºå¤§çš„åŸºå‡†æµ‹è¯•å’Œå…ˆè¿›çš„è¯„ä¼°åè®®æ¥ç¡®ä¿ç ”ç©¶çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>é€æ­¥æ¨ç†å¹¶ä¸æ€»æ˜¯èƒ½æé«˜å¯¹è¯æ‘˜è¦çš„è´¨é‡ã€‚åè€Œå¯èƒ½å¯¼è‡´å†—é•¿ã€äº‹å®ä¸ä¸€è‡´çš„æ‘˜è¦ã€‚</li>
<li>æ¨ç†LLMåœ¨å¤æ‚å¯¹è¯ä¸Šä¸‹æ–‡ä¸­çš„è¡¨ç°ä¸ä½³ï¼Œéœ€è¦è¿›ä¸€æ­¥çš„å»ºæ¨¡å’Œè¯„ä¼°ç­–ç•¥æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02145">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f0eab1f186e6ec576c4dea4d2df2bb80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1f30253007c41bb6075bb6f1009e005.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-934cd39491f51042690617ce72127ac1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa89976c4cc5deb4aa7bd904b878103c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da69929f0c2602ecf23a1e4fd124f95d.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Driven-Closed-Loop-UAV-Operation-with-Semantic-Observations"><a href="#Large-Language-Model-Driven-Closed-Loop-UAV-Operation-with-Semantic-Observations" class="headerlink" title="Large Language Model-Driven Closed-Loop UAV Operation with Semantic   Observations"></a>Large Language Model-Driven Closed-Loop UAV Operation with Semantic   Observations</h2><p><strong>Authors:Wenhao Wang, Yanyan Li, Long Jiao, Jiawei Yuan</strong></p>
<p>Recent advances in large Language Models (LLMs) have revolutionized mobile robots, including unmanned aerial vehicles (UAVs), enabling their intelligent operation within Internet of Things (IoT) ecosystems. However, LLMs still face challenges from logical reasoning and complex decision-making, leading to concerns about the reliability of LLM-driven UAV operations in IoT applications. In this paper, we propose a LLM-driven closed-loop control framework that enables reliable UAV operations powered by effective feedback and refinement using two LLM modules, i.e., a Code Generator and an Evaluator. Our framework transforms numerical state observations from UAV operations into natural language trajectory descriptions to enhance the evaluator LLMâ€™s understanding of UAV dynamics for precise feedback generation. Our framework also enables a simulation-based refinement process, and hence eliminates the risks to physical UAVs caused by incorrect code execution during the refinement. Extensive experiments on UAV control tasks with different complexities are conducted. The experimental results show that our framework can achieve reliable UAV operations using LLMs, which significantly outperforms baseline approaches in terms of success rate and completeness with the increase of task complexity. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•åœ¨ç§»åŠ¨æœºå™¨äººé¢†åŸŸå¼•èµ·äº†é©å‘½æ€§çš„å˜é©ï¼ŒåŒ…æ‹¬æ— äººé©¾é©¶èˆªç©ºå™¨ï¼ˆUAVsï¼‰ã€‚è¿™ä½¿å¾—å®ƒä»¬åœ¨ç‰©è”ç½‘ï¼ˆIoTï¼‰ç”Ÿæ€ç³»ç»Ÿä¸­çš„æ™ºèƒ½æ“ä½œæˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼ŒLLMä»ç„¶é¢ä¸´é€»è¾‘æ¨ç†å’Œå¤æ‚å†³ç­–æ–¹é¢çš„æŒ‘æˆ˜ï¼Œè¿™å¼•å‘äº†äººä»¬å¯¹ç‰©è”ç½‘åº”ç”¨ä¸­LLMé©±åŠ¨çš„æ— äººæœºæ“ä½œå¯é æ€§çš„æ‹…å¿§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§LLMé©±åŠ¨çš„é—­ç¯æ§åˆ¶æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸¤ä¸ªLLMæ¨¡å—å³ä»£ç ç”Ÿæˆå™¨å’Œè¯„ä¼°å™¨ï¼Œåˆ©ç”¨æœ‰æ•ˆçš„åé¦ˆå’Œæ”¹è¿›ï¼Œå®ç°äº†å¯é çš„æ— äººæœºæ“ä½œã€‚æˆ‘ä»¬çš„æ¡†æ¶å°†æ— äººæœºæ“ä½œçš„æ•°å€¼çŠ¶æ€è§‚å¯Ÿè½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€è½¨è¿¹æè¿°ï¼Œä»¥å¢å¼ºè¯„ä¼°å™¨LLMå¯¹æ— äººæœºåŠ¨åŠ›å­¦çš„ç†è§£ï¼Œä»è€Œå®ç°ç²¾ç¡®åé¦ˆç”Ÿæˆã€‚æˆ‘ä»¬çš„æ¡†æ¶è¿˜æ”¯æŒåŸºäºæ¨¡æ‹Ÿçš„æ”¹è¿›è¿‡ç¨‹ï¼Œä»è€Œæ¶ˆé™¤äº†æ”¹è¿›è¿‡ç¨‹ä¸­é”™è¯¯ä»£ç æ‰§è¡Œå¯¹å®é™…æ— äººæœºé€ æˆçš„é£é™©ã€‚æˆ‘ä»¬å¯¹å…·æœ‰ä¸åŒå¤æ‚ç¨‹åº¦çš„æ— äººæœºæ§åˆ¶ä»»åŠ¡è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿä½¿ç”¨LLMå®ç°å¯é çš„æ— äººæœºæ“ä½œï¼Œåœ¨æˆåŠŸç‡å’Œå®Œæ•´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå¹¶éšç€ä»»åŠ¡å¤æ‚æ€§çš„å¢åŠ è€Œè¡¨ç°å‡ºä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01930v2">PDF</a> 9 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç§»åŠ¨æœºå™¨äººé¢†åŸŸï¼ˆåŒ…æ‹¬æ— äººæœºï¼‰çš„åº”ç”¨å®ç°äº†å…¶åœ¨ç‰©è”ç½‘ç”Ÿæ€ç³»ç»Ÿä¸­çš„æ™ºèƒ½æ“ä½œï¼Œä½†ä»ç„¶å­˜åœ¨é€»è¾‘æ¨ç†å’Œå¤æ‚å†³ç­–çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§LLMé©±åŠ¨çš„é—­ç¯æ§åˆ¶æ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªLLMæ¨¡å—å³ä»£ç ç”Ÿæˆå™¨å’Œè¯„ä¼°å™¨ï¼Œå®ç°å¯é çš„æ— äººæœºæ“ä½œã€‚è¯¥æ¡†æ¶å°†æ— äººæœºçš„æ•°å€¼çŠ¶æ€è§‚å¯Ÿè½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€è½¨è¿¹æè¿°ï¼Œå¢å¼ºè¯„ä¼°å™¨å¯¹æ— äººæœºåŠ¨æ€çš„ç†è§£ï¼Œä»¥ç”Ÿæˆç²¾ç¡®åé¦ˆã€‚æ¡†æ¶è¿˜é€šè¿‡æ¨¡æ‹Ÿä¼˜åŒ–è¿‡ç¨‹ï¼Œå‡å°‘å› ä»£ç æ‰§è¡Œé”™è¯¯å¯¼è‡´çš„ç‰©ç†æ— äººæœºçš„é£é™©ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨å¤æ‚ä»»åŠ¡ä¸­å®ç°äº†å¯é çš„æ— äººæœºæ“ä½œï¼Œå¹¶åœ¨æˆåŠŸç‡å’Œå®Œæ•´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²åº”ç”¨äºç§»åŠ¨æœºå™¨äººé¢†åŸŸï¼Œæ¨åŠ¨æ— äººæœºåœ¨ç‰©è”ç½‘ç”Ÿæ€ç³»ç»Ÿä¸­çš„æ™ºèƒ½æ“ä½œã€‚</li>
<li>LLMåœ¨é€»è¾‘æ¨ç†å’Œå¤æ‚å†³ç­–æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå½±å“æ— äººæœºæ“ä½œçš„å¯é æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§LLMé©±åŠ¨çš„é—­ç¯æ§åˆ¶æ¡†æ¶ï¼ŒåŒ…å«ä»£ç ç”Ÿæˆå™¨å’Œè¯„ä¼°å™¨ä¸¤ä¸ªæ¨¡å—ã€‚</li>
<li>æ¡†æ¶èƒ½å°†æ— äººæœºçš„æ•°å€¼çŠ¶æ€è§‚å¯Ÿè½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€è½¨è¿¹æè¿°ï¼Œå¢å¼ºè¯„ä¼°å™¨å¯¹æ— äººæœºåŠ¨æ€çš„ç†è§£ã€‚</li>
<li>æ¡†æ¶æ”¯æŒæ¨¡æ‹Ÿä¼˜åŒ–è¿‡ç¨‹ï¼Œå‡å°‘ç‰©ç†æ— äººæœºå› ä»£ç æ‰§è¡Œé”™è¯¯è€Œäº§ç”Ÿçš„é£é™©ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ¡†æ¶åœ¨å¤æ‚ä»»åŠ¡ä¸­å®ç°äº†å¯é çš„æ— äººæœºæ“ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f42d97d9cda874b90a7377fe5ca9cc8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b4fab06cc3db47da849f4fd44147abe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c383a0d150447f7e3fdbfe68eb73bbe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a691b73db0c36312c4a697f6aec9130.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f682c04b456367edb962cd4817e6762.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a452a52b6d593509c4f9c3aeb6565e5.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Self-Guided-Process-Reward-Optimization-with-Redefined-Step-wise-Advantage-for-Process-Reinforcement-Learning"><a href="#Self-Guided-Process-Reward-Optimization-with-Redefined-Step-wise-Advantage-for-Process-Reinforcement-Learning" class="headerlink" title="Self-Guided Process Reward Optimization with Redefined Step-wise   Advantage for Process Reinforcement Learning"></a>Self-Guided Process Reward Optimization with Redefined Step-wise   Advantage for Process Reinforcement Learning</h2><p><strong>Authors:Wu Fei, Hao Kong, Shuxian Liang, Yang Lin, Yibo Yang, Jing Tang, Lei Chen, Xiansheng Hua</strong></p>
<p>Process Reinforcement Learning<del>(PRL) has demonstrated considerable potential in enhancing the reasoning capabilities of Large Language Models</del>(LLMs). However, introducing additional process reward models incurs substantial computational overhead, and there is no unified theoretical framework for process-level advantage estimation. To bridge this gap, we propose \textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward \textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables process-aware RL through two key innovations: (1) we first theoretically demonstrate that process rewards can be derived intrinsically from the policy model itself, and (2) we introduce well-defined cumulative process rewards and \textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which facilitates rigorous step-wise action advantage estimation within shared-prompt sampling groups. Our experimental results demonstrate that SPRO outperforms vaniila GRPO with 3.4x higher training efficiency and a 17.5% test accuracy improvement. Furthermore, SPRO maintains a stable and elevated policy entropy throughout training while reducing the average response length by approximately $1&#x2F;3$, evidencing sufficient exploration and prevention of reward hacking. Notably, SPRO incurs no additional computational overhead compared to outcome-supervised RL methods such as GRPO, which benefit industrial implementation. </p>
<blockquote>
<p>è¿‡ç¨‹å¼ºåŒ–å­¦ä¹ ï¼ˆPRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¼•å…¥é¢å¤–çš„æµç¨‹å¥–åŠ±æ¨¡å‹ä¼šäº§ç”Ÿå¤§é‡çš„è®¡ç®—å¼€é”€ï¼Œå¹¶ä¸”æ²¡æœ‰ç»Ÿä¸€çš„ç†è®ºæ¡†æ¶æ¥è¿›è¡Œæµç¨‹çº§åˆ«çš„ä¼˜åŠ¿ä¼°è®¡ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªå¼•å¯¼æµç¨‹å¥–åŠ±ä¼˜åŒ–ï¼ˆSPROï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹å®ç°äº†æµç¨‹æ„ŸçŸ¥çš„RLï¼šï¼ˆ1ï¼‰æˆ‘ä»¬é¦–å…ˆä»ç†è®ºä¸Šè¯æ˜ï¼Œæµç¨‹å¥–åŠ±å¯ä»¥å†…åœ¨åœ°ä»ç­–ç•¥æ¨¡å‹æœ¬èº«æ¨å¯¼å‡ºæ¥ï¼›ï¼ˆ2ï¼‰æˆ‘ä»¬å¼•å…¥äº†å®šä¹‰è‰¯å¥½çš„ç´¯ç§¯æµç¨‹å¥–åŠ±å’Œæ©ç æ­¥éª¤ä¼˜åŠ¿ï¼ˆMSAï¼‰ï¼Œè¿™æœ‰åŠ©äºåœ¨å…±äº«æç¤ºé‡‡æ ·ç»„å†…è¿›è¡Œä¸¥æ ¼çš„é€æ­¥è¡ŒåŠ¨ä¼˜åŠ¿ä¼°è®¡ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSPROç›¸è¾ƒäºæ ‡å‡†çš„GRPOæ–¹æ³•ï¼Œè®­ç»ƒæ•ˆç‡æé«˜äº†3.4å€ï¼Œæµ‹è¯•ç²¾åº¦æé«˜äº†17.5%ã€‚æ­¤å¤–ï¼ŒSPROåœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒäº†ç¨³å®šå’Œè¾ƒé«˜çš„ç­–ç•¥ç†µï¼ŒåŒæ—¶å¹³å‡å“åº”é•¿åº¦å‡å°‘äº†å¤§çº¦ä¸‰åˆ†ä¹‹ä¸€ï¼Œè¯æ˜äº†å…¶å……åˆ†çš„æ¢ç´¢èƒ½åŠ›å’Œé˜²æ­¢å¥–åŠ±æ“çºµçš„æ•ˆæœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç›¸è¾ƒäºç»“æœç›‘ç£çš„RLæ–¹æ³•ï¼ˆå¦‚GRPOï¼‰ï¼ŒSPROæ²¡æœ‰é¢å¤–çš„è®¡ç®—å¼€é”€ï¼Œæœ‰åˆ©äºå·¥ä¸šå®æ–½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01551v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‡ç¨‹å¼ºåŒ–å­¦ä¹ ï¼ˆPRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¼•å…¥é¢å¤–çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ä¼šå¸¦æ¥å¾ˆå¤§çš„è®¡ç®—å¼€é”€ï¼Œå¹¶ä¸”ç¼ºä¹ç»Ÿä¸€çš„è¿‡ç¨‹çº§ä¼˜åŠ¿ä¼°è®¡ç†è®ºæ¡†æ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºè‡ªæˆ‘å¯¼å‘çš„è¿‡ç¨‹å¥–åŠ±ä¼˜åŒ–ï¼ˆSPROï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªå…³é”®åˆ›æ–°å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼šï¼ˆ1ï¼‰ä»ç†è®ºä¸Šè¯æ˜è¿‡ç¨‹å¥–åŠ±å¯ä»¥å†…åœ¨åœ°ä»ç­–ç•¥æ¨¡å‹æœ¬èº«è¡ç”Ÿå‡ºæ¥ï¼›ï¼ˆ2ï¼‰å¼•å…¥å®šä¹‰æ˜ç¡®çš„è¿‡ç¨‹ç´¯ç§¯å¥–åŠ±å’Œé®ç½©æ­¥éª¤ä¼˜åŠ¿ï¼ˆMSAï¼‰ï¼Œæœ‰åŠ©äºåœ¨å…±äº«æç¤ºé‡‡æ ·ç»„å†…è¿›è¡Œä¸¥æ ¼çš„æ­¥éª¤çº§è¡ŒåŠ¨ä¼˜åŠ¿ä¼°è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPROåœ¨è®­ç»ƒæ•ˆç‡ä¸Šä¼˜äºåŸºç¡€GRPOæ–¹æ³•ï¼Œå®ç°äº†3.4å€çš„è®­ç»ƒæ•ˆç‡æå‡å’Œ17.5%çš„æµ‹è¯•ç²¾åº¦æ”¹è¿›ã€‚åŒæ—¶ï¼ŒSPROåœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒäº†ç¨³å®šçš„ç­–ç•¥ç†µæå‡ï¼Œå¹¶æˆåŠŸå°†å¹³å‡å“åº”é•¿åº¦å‡å°‘äº†çº¦ä¸‰åˆ†ä¹‹ä¸€ï¼Œè¯æ˜äº†å…¶åœ¨å……åˆ†æ¢ç´¢å’Œé¢„é˜²å¥–åŠ±ç ´è§£æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç›¸è¾ƒäºç»“æœç›‘ç£çš„RLæ–¹æ³•ï¼ˆå¦‚GRPOï¼‰ï¼ŒSPROæ²¡æœ‰å¼•å…¥é¢å¤–çš„è®¡ç®—å¼€é”€ï¼Œæ›´é€‚åˆå·¥ä¸šåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‡ç¨‹å¼ºåŒ–å­¦ä¹ ï¼ˆPRLï¼‰èƒ½æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥SPROæ¡†æ¶æ¥ä¼˜åŒ–è¿‡ç¨‹å¥–åŠ±ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šä»ç­–ç•¥æ¨¡å‹ä¸­æ¨å¯¼è¿‡ç¨‹å¥–åŠ±å’Œå®šä¹‰ç´¯ç§¯è¿‡ç¨‹å¥–åŠ±åŠé®ç½©æ­¥éª¤ä¼˜åŠ¿ï¼ˆMSAï¼‰ã€‚</li>
<li>SPROæ¡†æ¶èƒ½æé«˜è®­ç»ƒæ•ˆç‡ï¼Œå®ç°æ›´é«˜çš„æµ‹è¯•ç²¾åº¦ã€‚</li>
<li>SPROä¿æŒç¨³å®šçš„ç­–ç•¥ç†µæå‡ï¼Œå¹¶å‡å°‘å¹³å‡å“åº”é•¿åº¦ã€‚</li>
<li>SPROç›¸è¾ƒäºå…¶ä»–æ–¹æ³•æ²¡æœ‰é¢å¤–çš„è®¡ç®—å¼€é”€ï¼Œé€‚åˆå·¥ä¸šåº”ç”¨ã€‚</li>
<li>SPROæ¡†æ¶åœ¨ç†è®ºä¸Šæœ‰å……åˆ†çš„éªŒè¯ï¼Œå¹¶å·²ç»è¿‡å®éªŒéªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-22e596d759e058e989fa168663f10189.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb1177a01394a39624f5b83253ef8759.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9330702e867e207dfe155eece83868e1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Symbolic-or-Numerical-Understanding-Physics-Problem-Solving-in-Reasoning-LLMs"><a href="#Symbolic-or-Numerical-Understanding-Physics-Problem-Solving-in-Reasoning-LLMs" class="headerlink" title="Symbolic or Numerical? Understanding Physics Problem Solving in   Reasoning LLMs"></a>Symbolic or Numerical? Understanding Physics Problem Solving in   Reasoning LLMs</h2><p><strong>Authors:Nifu Dan, Yujun Cai, Yiwei Wang</strong></p>
<p>Navigating the complexities of physics reasoning has long been a difficult task for Large Language Models (LLMs), requiring a synthesis of profound conceptual understanding and adept problem-solving techniques. In this study, we investigate the application of advanced instruction-tuned reasoning models, such as Deepseek-R1, to address a diverse spectrum of physics problems curated from the challenging SciBench benchmark. Our comprehensive experimental evaluation reveals the remarkable capabilities of reasoning models. Not only do they achieve state-of-the-art accuracy in answering intricate physics questions, but they also generate distinctive reasoning patterns that emphasize on symbolic derivation. Furthermore, our findings indicate that even for these highly sophisticated reasoning models, the strategic incorporation of few-shot prompting can still yield measurable improvements in overall accuracy, highlighting the potential for continued performance gains. </p>
<blockquote>
<p>é©¾é©­ç‰©ç†æ¨ç†çš„å¤æ‚æ€§å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´ä¸€ç›´æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ï¼Œéœ€è¦æ·±åšçš„æ¦‚å¿µç†è§£å’Œç†Ÿç»ƒçš„é—®é¢˜è§£å†³æŠ€å·§çš„ç»“åˆã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å…ˆè¿›æŒ‡ä»¤è°ƒæ•´æ¨ç†æ¨¡å‹çš„åº”ç”¨ï¼Œå¦‚Deepseek-R1ï¼Œä»¥è§£å†³ä»å…·æœ‰æŒ‘æˆ˜æ€§çš„SciBenchåŸºå‡†æµ‹è¯•ä¸­ç²¾é€‰çš„å„ç§ç‰©ç†é—®é¢˜ã€‚æˆ‘ä»¬çš„ç»¼åˆå®éªŒè¯„ä¼°æ˜¾ç¤ºäº†æ¨ç†æ¨¡å‹çš„å“è¶Šèƒ½åŠ›ã€‚å®ƒä»¬ä¸ä»…åœ¨å›ç­”å¤æ‚ç‰©ç†é—®é¢˜æ—¶è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œè€Œä¸”äº§ç”Ÿäº†å¼ºè°ƒç¬¦å·æ¨å¯¼çš„ç‹¬ç‰¹æ¨ç†æ¨¡å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯å¯¹äºè¿™äº›é«˜åº¦å¤æ‚çš„æ¨ç†æ¨¡å‹ï¼Œé€šè¿‡ç­–ç•¥æ€§åœ°å¼•å…¥å°‘é‡æç¤ºï¼Œä»ç„¶å¯ä»¥æé«˜æ€»ä½“å‡†ç¡®æ€§ï¼Œè¿™çªæ˜¾äº†æŒç»­æé«˜æ€§èƒ½çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01334v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å…ˆè¿›çš„æ•™å­¦æ¨ç†æ¨¡å‹ï¼Œå¦‚Deepseek-R1ï¼Œåœ¨åº”å¯¹æ¥è‡ªSciBenchåŸºå‡†æµ‹è¯•çš„æŒ‘æˆ˜æ€§ç‰©ç†é—®é¢˜æ—¶çš„åº”ç”¨ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¿™äº›æ¨ç†æ¨¡å‹ä¸ä»…èƒ½å¤Ÿåœ¨å›ç­”å¤æ‚ç‰©ç†é—®é¢˜æ–¹é¢è¾¾åˆ°æœ€æ–°æŠ€æœ¯çš„å‡†ç¡®æ€§ï¼Œè€Œä¸”è¿˜è¡¨ç°å‡ºç‹¬ç‰¹çš„å¼ºè°ƒç¬¦å·æ¨å¯¼çš„æ¨ç†æ¨¡å¼ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°ï¼Œå³ä½¿æ˜¯å¯¹äºè¿™äº›é«˜åº¦å¤æ‚çš„æ¨ç†æ¨¡å‹ï¼Œé€šè¿‡æˆ˜ç•¥æ€§åœ°èå…¥å°‘é‡æç¤ºï¼Œä»å¯ä»¥æé«˜æ•´ä½“å‡†ç¡®æ€§ï¼Œæ˜¾ç¤ºå‡ºæ½œåœ¨çš„æ€§èƒ½æå‡ç©ºé—´ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å…ˆè¿›çš„æ•™å­¦æ¨ç†æ¨¡å‹ï¼Œå¦‚Deepseek-R1ï¼Œèƒ½å¤Ÿåº”å¯¹å¤šæ ·åŒ–çš„ç‰©ç†é—®é¢˜ï¼Œè¾¾åˆ°å‰æ²¿çš„å‡†ç¡®ç‡ã€‚</li>
<li>è¿™äº›æ¨ç†æ¨¡å‹å±•ç°å‡ºç‹¬ç‰¹çš„æ¨ç†æ¨¡å¼ï¼Œé‡è§†ç¬¦å·æ¨å¯¼ã€‚</li>
<li>å³ä½¿æ˜¯é«˜åº¦å¤æ‚çš„æ¨ç†æ¨¡å‹ï¼Œèå…¥å°‘é‡æç¤ºä»å¯æå‡æ•´ä½“å‡†ç¡®æ€§ã€‚</li>
<li>LLMåœ¨ç‰©ç†æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦æ·±åˆ»çš„æ¦‚å¿µç†è§£å’Œç†Ÿç»ƒçš„é—®é¢˜è§£å†³æŠ€å·§ã€‚</li>
<li>SciBenchåŸºå‡†æµ‹è¯•çš„ç‰©ç†é—®é¢˜å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æˆ˜ç•¥æ€§åœ°ä½¿ç”¨æç¤ºæ–¹æ³•å¯èƒ½æœ‰åŠ©äºè¿›ä¸€æ­¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01334">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aab02320fa33366802e367f7a5388eef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43ee0b3af7d989529f628567f903df25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-372b906d2927d0b416cf3711220ee157.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bce1c83d5c44a0ac7c57117747866cef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39a106965c2a43254c57b5354cd250ca.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="HumanoidGen-Data-Generation-for-Bimanual-Dexterous-Manipulation-via-LLM-Reasoning"><a href="#HumanoidGen-Data-Generation-for-Bimanual-Dexterous-Manipulation-via-LLM-Reasoning" class="headerlink" title="HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM   Reasoning"></a>HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM   Reasoning</h2><p><strong>Authors:Zhi Jing, Siyuan Yang, Jicong Ao, Ting Xiao, Yugang Jiang, Chenjia Bai</strong></p>
<p>For robotic manipulation, existing robotics datasets and simulation benchmarks predominantly cater to robot-arm platforms. However, for humanoid robots equipped with dual arms and dexterous hands, simulation tasks and high-quality demonstrations are notably lacking. Bimanual dexterous manipulation is inherently more complex, as it requires coordinated arm movements and hand operations, making autonomous data collection challenging. This paper presents HumanoidGen, an automated task creation and demonstration collection framework that leverages atomic dexterous operations and LLM reasoning to generate relational constraints. Specifically, we provide spatial annotations for both assets and dexterous hands based on the atomic operations, and perform an LLM planner to generate a chain of actionable spatial constraints for arm movements based on object affordances and scenes. To further improve planning ability, we employ a variant of Monte Carlo tree search to enhance LLM reasoning for long-horizon tasks and insufficient annotation. In experiments, we create a novel benchmark with augmented scenarios to evaluate the quality of the collected data. The results show that the performance of the 2D and 3D diffusion policies can scale with the generated dataset. Project page is <a target="_blank" rel="noopener" href="https://openhumanoidgen.github.io/">https://openhumanoidgen.github.io</a>. </p>
<blockquote>
<p>å¯¹äºæœºå™¨äººæ“ä½œè€Œè¨€ï¼Œç°æœ‰çš„æœºå™¨äººæ•°æ®é›†å’Œæ¨¡æ‹ŸåŸºå‡†æµ‹è¯•ä¸»è¦é¢å‘æœºå™¨äººæ‰‹è‡‚å¹³å°ã€‚ç„¶è€Œï¼Œå¯¹äºé…å¤‡æœ‰åŒè‡‚å’Œçµå·§åŒæ‰‹çš„äººå½¢æœºå™¨äººï¼Œæ¨¡æ‹Ÿä»»åŠ¡å’Œé«˜è´¨é‡çš„æ¼”ç¤ºæ˜æ˜¾ç¼ºä¹ã€‚åŒæ‰‹åŠ¨ä½œæ“ä½œæœ¬è´¨ä¸Šæ›´ä¸ºå¤æ‚ï¼Œå› ä¸ºå®ƒéœ€è¦åè°ƒæ‰‹è‡‚è¿åŠ¨å’Œæ‰‹éƒ¨æ“ä½œï¼Œä½¿å¾—è‡ªä¸»çš„æ•°æ®æ”¶é›†å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºäº†HumanoidGenï¼Œä¸€ä¸ªè‡ªåŠ¨åŒ–ä»»åŠ¡åˆ›å»ºå’Œæ¼”ç¤ºæ”¶é›†æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨åŸå­çµå·§æ“ä½œå’Œå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†æ¥ç”Ÿæˆå…³ç³»çº¦æŸã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åŸºäºåŸå­æ“ä½œä¸ºèµ„äº§å’Œçµå·§åŒæ‰‹æä¾›ç©ºé—´æ³¨é‡Šï¼Œå¹¶ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è§„åˆ’å™¨æ ¹æ®ç‰©ä½“åŠŸèƒ½å’Œåœºæ™¯ç”Ÿæˆä¸€ç³»åˆ—å¯æ“ä½œçš„ç©ºé—´çº¦æŸé“¾ï¼Œç”¨äºæ‰‹è‡‚è¿åŠ¨ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜è§„åˆ’èƒ½åŠ›ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§è’™ç‰¹å¡æ´›æ ‘æœç´¢çš„å˜ç§æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹å¯¹é•¿æœŸä»»åŠ¡å’Œä¸è¶³æ³¨é‡Šçš„æ¨ç†ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå¸¦æœ‰å¢å¼ºåœºæ™¯çš„æ–°åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°æ”¶é›†æ•°æ®çš„è´¨é‡ã€‚ç»“æœè¡¨æ˜ï¼ŒäºŒç»´å’Œä¸‰ç»´æ‰©æ•£ç­–ç•¥çš„æ€§èƒ½å¯ä»¥éšç€ç”Ÿæˆçš„æ•°æ®é›†è€Œæ‰©å±•ã€‚é¡¹ç›®é¡µé¢ä¸º<a target="_blank" rel="noopener" href="https://openhumanoidgen.github.io./">https://openhumanoidgen.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00833v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://openhumanoidgen.github.io/">https://openhumanoidgen.github.io</a></p>
<p><strong>Summary</strong><br>åŸºäºåŸå­æ“ä½œçš„äººç±»çµå·§æ¨¡æ‹Ÿæ•°æ®æ”¶é›†æ¡†æ¶HumanoidGenç”¨äºæœºå™¨äººåŒè‡‚æ“æ§ã€‚æ­¤æ¡†æ¶é‡‡ç”¨LLMè§„åˆ’å™¨ç”ŸæˆåŠ¨ä½œçº¦æŸé“¾ï¼Œè§£å†³äººå½¢æœºå™¨äººåŒè‡‚çµå·§æ“æ§çš„å¤æ‚æ€§æŒ‘æˆ˜ã€‚é€šè¿‡ç©ºé—´æ ‡æ³¨å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢å¢å¼ºLLMæ¨ç†èƒ½åŠ›ï¼Œæé«˜é•¿å‘¨æœŸä»»åŠ¡çš„è§„åˆ’èƒ½åŠ›ã€‚æ–°å‹æ•°æ®é›†ä¸æ¨¡æ‹Ÿåœºæ™¯çš„èåˆå±•ç¤ºå¯¹æ”¶é›†çš„å®æ—¶æ€§äº§ç”Ÿå½±å“ã€‚<strong>Key Takeaways</strong>:</p>
<ul>
<li>ç°æœ‰æœºå™¨äººæ•°æ®é›†ä¸»è¦é’ˆå¯¹æœºå™¨äººæ‰‹è‡‚å¹³å°ï¼Œä½†äººå½¢æœºå™¨äººåŒè‡‚çµå·§æ“æ§æ¨¡æ‹Ÿä»»åŠ¡å’Œæ•°æ®é›†ç¼ºä¹ã€‚</li>
<li>HumanoidGenæ¡†æ¶åˆ©ç”¨åŸå­çµå·§æ“ä½œå’ŒLLMè§„åˆ’å™¨ç”Ÿæˆç©ºé—´çº¦æŸåŠ¨ä½œé“¾æ¥è§£å†³åŒè‡‚æ“æ§å¤æ‚æ€§ã€‚</li>
<li>åˆ©ç”¨ç©ºé—´æ ‡æ³¨è¿›è¡Œèµ„äº§å’Œæ‰‹éƒ¨æ•°æ®è®°å½•ã€‚</li>
<li>é‡‡ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢å¢å¼ºLLMæ¨ç†èƒ½åŠ›ï¼Œæé«˜é•¿å‘¨æœŸä»»åŠ¡å’Œä¸è¶³æ ‡æ³¨çš„åº”å¯¹èƒ½åŠ›ã€‚</li>
<li>åˆ›å»ºæ–°å‹æ•°æ®é›†ä¸å¢å¼ºåœºæ™¯èåˆçš„è¯„ä»·æ ‡å‡†æ¥è¯„ä¼°æ•°æ®è´¨é‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00833">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-da917aaff50528fb7976d7e49f239782.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-511ab14a974b610fc36bbce2657099b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-344188027b3e6b2e0f3c362be8291c32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a73115cfe38e9cdc422fdbccdf303012.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ee75fe5d636e6595422ebc460361f7f.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-05/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-05/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-05/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-215e91a93cb251b46cce4a9432d2d75f.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-05  Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for   Data-Efficient Model Adaptation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-04/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0cdf499a44bb68c011dfaafa802e11c2.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-04  ADAptation Reconstruction-based Unsupervised Active Learning for Breast   Ultrasound Diagnosis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25370.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
