<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-05  AnyI2V Animating Any Conditional Image with Motion Control">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-d0c2a7cc29fee5319b749d83188c03e7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-05-æ›´æ–°"><a href="#2025-07-05-æ›´æ–°" class="headerlink" title="2025-07-05 æ›´æ–°"></a>2025-07-05 æ›´æ–°</h1><h2 id="AnyI2V-Animating-Any-Conditional-Image-with-Motion-Control"><a href="#AnyI2V-Animating-Any-Conditional-Image-with-Motion-Control" class="headerlink" title="AnyI2V: Animating Any Conditional Image with Motion Control"></a>AnyI2V: Animating Any Conditional Image with Motion Control</h2><p><strong>Authors:Ziye Li, Hao Luo, Xincheng Shuai, Henghui Ding</strong></p>
<p>Recent advancements in video generation, particularly in diffusion models, have driven notable progress in text-to-video (T2V) and image-to-video (I2V) synthesis. However, challenges remain in effectively integrating dynamic motion signals and flexible spatial constraints. Existing T2V methods typically rely on text prompts, which inherently lack precise control over the spatial layout of generated content. In contrast, I2V methods are limited by their dependence on real images, which restricts the editability of the synthesized content. Although some methods incorporate ControlNet to introduce image-based conditioning, they often lack explicit motion control and require computationally expensive training. To address these limitations, we propose AnyI2V, a training-free framework that animates any conditional images with user-defined motion trajectories. AnyI2V supports a broader range of modalities as the conditional image, including data types such as meshes and point clouds that are not supported by ControlNet, enabling more flexible and versatile video generation. Additionally, it supports mixed conditional inputs and enables style transfer and editing via LoRA and text prompts. Extensive experiments demonstrate that the proposed AnyI2V achieves superior performance and provides a new perspective in spatial- and motion-controlled video generation. Code is available at <a target="_blank" rel="noopener" href="https://henghuiding.com/AnyI2V/">https://henghuiding.com/AnyI2V/</a>. </p>
<blockquote>
<p>è§†é¢‘ç”Ÿæˆé¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œå°¤å…¶æ˜¯æ‰©æ•£æ¨¡å‹çš„åº”ç”¨ï¼Œæ¨åŠ¨äº†æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰å’Œå›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰åˆæˆçš„æ˜¾è‘—è¿›æ­¥ã€‚ç„¶è€Œï¼Œåœ¨æœ‰æ•ˆé›†æˆåŠ¨æ€è¿åŠ¨ä¿¡å·å’Œçµæ´»ç©ºé—´çº¦æŸæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ç°æœ‰çš„T2Væ–¹æ³•é€šå¸¸ä¾èµ–äºæ–‡æœ¬æç¤ºï¼Œè¿™å›ºæœ‰åœ°ç¼ºä¹å¯¹ç”Ÿæˆå†…å®¹ç©ºé—´å¸ƒå±€çš„ç²¾ç¡®æ§åˆ¶ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒI2Væ–¹æ³•å—é™äºå¯¹çœŸå®å›¾åƒçš„ä¾èµ–ï¼Œè¿™é™åˆ¶äº†åˆæˆå†…å®¹çš„å¯ç¼–è¾‘æ€§ã€‚è™½ç„¶ä¸€äº›æ–¹æ³•ç»“åˆäº†ControlNetè¿›è¡ŒåŸºäºå›¾åƒçš„è°ƒèŠ‚ï¼Œä½†å®ƒä»¬é€šå¸¸ç¼ºä¹æ˜ç¡®çš„è¿åŠ¨æ§åˆ¶ï¼Œå¹¶ä¸”éœ€è¦è®¡ç®—æ˜‚è´µçš„è®­ç»ƒã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†AnyI2Vï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·å®šä¹‰çš„è¿åŠ¨è½¨è¿¹å¯¹ä»»ä½•æ¡ä»¶å›¾åƒè¿›è¡ŒåŠ¨ç”»å¤„ç†ã€‚AnyI2Væ”¯æŒæ›´å¹¿æ³›çš„æ¨¡æ€ä½œä¸ºæ¡ä»¶å›¾åƒï¼ŒåŒ…æ‹¬ControlNetä¸æ”¯æŒçš„ç½‘æ ¼å’Œç‚¹äº‘ç­‰æ•°æ®ç±»å‹ï¼Œä»è€Œå®ç°æ›´çµæ´»å’Œå¤šåŠŸèƒ½åŒ–çš„è§†é¢‘ç”Ÿæˆã€‚æ­¤å¤–ï¼Œå®ƒæ”¯æŒæ··åˆæ¡ä»¶è¾“å…¥ï¼Œå¹¶é€šè¿‡LoRAå’Œæ–‡æœ¬æç¤ºå®ç°é£æ ¼è½¬æ¢å’Œç¼–è¾‘ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„AnyI2Vè¾¾åˆ°äº†å“è¶Šçš„æ€§èƒ½ï¼Œä¸ºç©ºé—´å’Œè¿åŠ¨æ§åˆ¶è§†é¢‘ç”Ÿæˆæä¾›äº†æ–°çš„è§†è§’ã€‚ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://henghuiding.com/AnyI2V/]%E8%8E%B7%E5%BE%97%E3%80%82">https://henghuiding.com/AnyI2V/]è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02857v1">PDF</a> ICCV 2025, Project Page: <a target="_blank" rel="noopener" href="https://henghuiding.com/AnyI2V/">https://henghuiding.com/AnyI2V/</a></p>
<p><strong>Summary</strong><br>     é’ˆå¯¹æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰å’Œå›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰çš„åˆæˆï¼Œæœ€æ–°è¿›å±•åœ¨æ‰©æ•£æ¨¡å‹ä¸Šå–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼Œä½†ä»å­˜åœ¨åŠ¨æ€è¿åŠ¨ä¿¡å·å’Œçµæ´»ç©ºé—´çº¦æŸçš„æ•´åˆé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹ç²¾ç¡®çš„ç©ºé—´å¸ƒå±€æ§åˆ¶æˆ–ç¼–è¾‘æ€§ã€‚ä¸ºæ­¤ï¼Œæå‡ºAnyI2Vï¼Œä¸€ä¸ªæ— éœ€è®­ç»ƒå³å¯æ ¹æ®ç”¨æˆ·å®šä¹‰çš„è¿åŠ¨è½¨è¿¹å¯¹æ¡ä»¶å›¾åƒè¿›è¡ŒåŠ¨ç”»åŒ–çš„æ¡†æ¶ã€‚AnyI2Væ”¯æŒæ›´å¹¿æ³›çš„æ¨¡æ€ä½œä¸ºæ¡ä»¶å›¾åƒï¼Œå¹¶å®ç°æ··åˆæ¡ä»¶è¾“å…¥ï¼Œé€šè¿‡LoRAå’Œæ–‡æœ¬æç¤ºè¿›è¡Œé£æ ¼è½¬æ¢å’Œç¼–è¾‘ã€‚å®éªŒè¯æ˜ï¼ŒAnyI2Vå®ç°å“è¶Šæ€§èƒ½ï¼Œä¸ºç©ºé—´å’Œè¿åŠ¨æ§åˆ¶çš„è§†é¢‘ç”Ÿæˆæä¾›äº†æ–°çš„è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆé¢†åŸŸå–å¾—æœ€æ–°è¿›å±•ï¼Œæ¨åŠ¨äº†æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰å’Œå›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰åˆæˆçš„æ˜¾è‘—å‘å±•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨ç©ºé—´å¸ƒå±€æ§åˆ¶å’Œç¼–è¾‘æ€§çš„é™åˆ¶ã€‚</li>
<li>AnyI2Væ¡†æ¶è§£å†³äº†è¿™äº›é—®é¢˜ï¼Œæ— éœ€è®­ç»ƒå³å¯æ ¹æ®ç”¨æˆ·å®šä¹‰çš„è¿åŠ¨è½¨è¿¹å¯¹æ¡ä»¶å›¾åƒè¿›è¡ŒåŠ¨ç”»åŒ–ã€‚</li>
<li>AnyI2Væ”¯æŒå¤šç§æ¨¡æ€ä½œä¸ºæ¡ä»¶å›¾åƒï¼ŒåŒ…æ‹¬ç½‘æ ¼å’Œç‚¹äº‘ç­‰æ•°æ®ç±»å‹ã€‚</li>
<li>AnyI2Væ”¯æŒæ··åˆæ¡ä»¶è¾“å…¥ï¼Œå¹¶èƒ½å¤Ÿé€šè¿‡LoRAå’Œæ–‡æœ¬æç¤ºè¿›è¡Œé£æ ¼è½¬æ¢å’Œç¼–è¾‘ã€‚</li>
<li>AnyI2Vå®ç°å“è¶Šæ€§èƒ½ï¼Œä¸ºè§†é¢‘ç”Ÿæˆæä¾›äº†æ–°çš„è§†è§’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d0c2a7cc29fee5319b749d83188c03e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a570dc1efa096a93e81a092bc59f2b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb40801b10ea204e4d34dc3559de36d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5056103b928af1bcc26025179393d76.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bc3164d29b7b5b1aba445fa6c26db43d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LangScene-X-Reconstruct-Generalizable-3D-Language-Embedded-Scenes-with-TriMap-Video-Diffusion"><a href="#LangScene-X-Reconstruct-Generalizable-3D-Language-Embedded-Scenes-with-TriMap-Video-Diffusion" class="headerlink" title="LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with   TriMap Video Diffusion"></a>LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with   TriMap Video Diffusion</h2><p><strong>Authors:Fangfu Liu, Hao Li, Jiawei Chi, Hanyang Wang, Minghui Yang, Fudong Wang, Yueqi Duan</strong></p>
<p>Recovering 3D structures with open-vocabulary scene understanding from 2D images is a fundamental but daunting task. Recent developments have achieved this by performing per-scene optimization with embedded language information. However, they heavily rely on the calibrated dense-view reconstruction paradigm, thereby suffering from severe rendering artifacts and implausible semantic synthesis when limited views are available. In this paper, we introduce a novel generative framework, coined LangScene-X, to unify and generate 3D consistent multi-modality information for reconstruction and understanding. Powered by the generative capability of creating more consistent novel observations, we can build generalizable 3D language-embedded scenes from only sparse views. Specifically, we first train a TriMap video diffusion model that can generate appearance (RGBs), geometry (normals), and semantics (segmentation maps) from sparse inputs through progressive knowledge integration. Furthermore, we propose a Language Quantized Compressor (LQC), trained on large-scale image datasets, to efficiently encode language embeddings, enabling cross-scene generalization without per-scene retraining. Finally, we reconstruct the language surface fields by aligning language information onto the surface of 3D scenes, enabling open-ended language queries. Extensive experiments on real-world data demonstrate the superiority of our LangScene-X over state-of-the-art methods in terms of quality and generalizability. Project Page: <a target="_blank" rel="noopener" href="https://liuff19.github.io/LangScene-X">https://liuff19.github.io/LangScene-X</a>. </p>
<blockquote>
<p>ä»2Då›¾åƒä¸­æ¢å¤3Dç»“æ„å¹¶å®ç°å¼€æ”¾è¯æ±‡åœºæ™¯ç†è§£æ˜¯ä¸€é¡¹åŸºæœ¬ä½†è‰°å·¨çš„ä»»åŠ¡ã€‚æœ€è¿‘çš„å‘å±•é€šè¿‡æ‰§è¡Œåœºæ™¯ä¼˜åŒ–å¹¶åµŒå…¥è¯­è¨€ä¿¡æ¯æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¸¥é‡ä¾èµ–äºæ ¡å‡†çš„å¯†é›†è§†å›¾é‡å»ºèŒƒå¼ï¼Œå› æ­¤åœ¨å¯ç”¨è§†å›¾æœ‰é™çš„æƒ…å†µä¸‹ä¼šå‡ºç°ä¸¥é‡çš„æ¸²æŸ“ä¼ªå½±å’Œä¸å¯ä¿¡çš„è¯­ä¹‰åˆæˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹ç”Ÿæˆæ¡†æ¶ï¼Œç§°ä¸ºLangScene-Xï¼Œä»¥ç»Ÿä¸€å¹¶ç”Ÿæˆç”¨äºé‡å»ºå’Œç†è§£çš„3Dä¸€è‡´çš„å¤šæ¨¡æ€ä¿¡æ¯ã€‚å‡­å€Ÿåˆ›é€ æ›´ä¸€è‡´çš„æ–°è§‚æµ‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œæˆ‘ä»¬ä»…ä»ç¨€ç–è§†å›¾å°±å¯ä»¥æ„å»ºå¯æ³›åŒ–çš„3Dè¯­è¨€åµŒå…¥åœºæ™¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆè®­ç»ƒäº†ä¸€ä¸ªTriMapè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥é€šè¿‡æ¸è¿›çš„çŸ¥è¯†æ•´åˆä»ç¨€ç–è¾“å…¥ç”Ÿæˆå¤–è§‚ï¼ˆRGBï¼‰ã€å‡ ä½•ï¼ˆæ³•çº¿ï¼‰å’Œè¯­ä¹‰ï¼ˆåˆ†å‰²å›¾ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåœ¨å¤§å‹å›¾åƒæ•°æ®é›†ä¸Šè®­ç»ƒçš„Language Quantized Compressorï¼ˆLQCï¼‰ï¼Œä»¥æœ‰æ•ˆåœ°ç¼–ç è¯­è¨€åµŒå…¥ï¼Œä»è€Œå®ç°è·¨åœºæ™¯çš„æ³›åŒ–ï¼Œæ— éœ€å¯¹æ¯ä¸€ä¸ªåœºæ™¯è¿›è¡Œå†è®­ç»ƒã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å°†è¯­è¨€ä¿¡æ¯å¯¹é½åˆ°3Dåœºæ™¯çš„è¡¨é¢ä¸Šï¼Œé‡å»ºäº†è¯­è¨€è¡¨é¢åœºï¼Œä»è€Œå®ç°äº†å¼€æ”¾çš„è¯­è¨€æŸ¥è¯¢ã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„LangScene-Xåœ¨è´¨é‡å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://liuff19.github.io/LangScene-X%E3%80%82">https://liuff19.github.io/LangScene-Xã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02813v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://liuff19.github.io/LangScene-X">https://liuff19.github.io/LangScene-X</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ç”Ÿæˆæ¡†æ¶LangScene-Xï¼Œç”¨äºç»Ÿä¸€å’Œç”Ÿæˆç”¨äºé‡å»ºå’Œç†è§£çš„3Dä¸€è‡´çš„å¤šæ¨¡æ€ä¿¡æ¯ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿä»ç¨€ç–è§†è§’æ„å»ºå¯æ³›åŒ–çš„3Dè¯­è¨€åµŒå…¥åœºæ™¯ï¼Œé€šè¿‡TriMapè§†é¢‘æ‰©æ•£æ¨¡å‹é€æ­¥é›†æˆçŸ¥è¯†ç”Ÿæˆå¤–è§‚ã€å‡ ä½•å’Œè¯­ä¹‰ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†Language Quantized Compressorï¼ˆLQCï¼‰ä»¥é«˜æ•ˆç¼–ç è¯­è¨€åµŒå…¥ï¼Œå®ç°è·¨åœºæ™¯æ³›åŒ–è€Œæ— éœ€é’ˆå¯¹æ¯ä¸ªåœºæ™¯è¿›è¡Œå†è®­ç»ƒã€‚æœ€åï¼Œé€šè¿‡å¯¹é½è¯­è¨€ä¿¡æ¯åˆ°3Dåœºæ™¯è¡¨é¢ï¼Œé‡å»ºäº†è¯­è¨€è¡¨é¢åœºï¼Œæ”¯æŒå¼€æ”¾å¼è¯­è¨€æŸ¥è¯¢ã€‚å®éªŒè¯æ˜ï¼ŒLangScene-Xåœ¨è´¨é‡å’Œæ³›åŒ–æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LangScene-Xæ˜¯ä¸€ä¸ªæ–°å‹ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿç»Ÿä¸€ç”Ÿæˆ3Dä¸€è‡´çš„å¤šæ¨¡æ€ä¿¡æ¯ï¼Œç”¨äºä»2Då›¾åƒç†è§£åœºæ™¯å¹¶è¿›è¡Œ3Dç»“æ„æ¢å¤ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿä»ç¨€ç–è§†è§’æ„å»º3Dè¯­è¨€åµŒå…¥åœºæ™¯ï¼Œå…·æœ‰å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¯ä»¥åˆ›å»ºæ›´ä¸€è‡´çš„å…¨æ–°è§‚å¯Ÿç»“æœã€‚</li>
<li>TriMapè§†é¢‘æ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆå¤–è§‚ã€å‡ ä½•å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œé€šè¿‡é€æ­¥é›†æˆçŸ¥è¯†æ¥å®ç°é‡å»ºã€‚</li>
<li>Language Quantized Compressorï¼ˆLQCï¼‰èƒ½å¤Ÿé«˜æ•ˆç¼–ç è¯­è¨€åµŒå…¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒåœºæ™¯é—´æ³›åŒ–ï¼Œè€Œæ— éœ€é’ˆå¯¹æ¯ä¸ªåœºæ™¯è¿›è¡Œå†è®­ç»ƒã€‚</li>
<li>é€šè¿‡å°†è¯­è¨€ä¿¡æ¯å¯¹é½åˆ°3Dåœºæ™¯è¡¨é¢ï¼ŒLangScene-Xå®ç°äº†è¯­è¨€è¡¨é¢åœºçš„é‡å»ºï¼Œæ”¯æŒå¼€æ”¾å¼è¯­è¨€æŸ¥è¯¢ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒLangScene-Xåœ¨è´¨é‡å’Œæ³›åŒ–æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™è§†è§’ä¸‹çš„æ¸²æŸ“å’Œè¯­ä¹‰åˆæˆæ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02813">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-23d06951f50213e4441d4458ec69ed68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f12b8956e6b3bb97dd86a97356ea54a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfa57a17fe3fdfde3bdbd5e34f95d38d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02c6f92058dfd8753e7ac6a84c8f8a29.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RichControl-Structure-and-Appearance-Rich-Training-Free-Spatial-Control-for-Text-to-Image-Generation"><a href="#RichControl-Structure-and-Appearance-Rich-Training-Free-Spatial-Control-for-Text-to-Image-Generation" class="headerlink" title="RichControl: Structure- and Appearance-Rich Training-Free Spatial   Control for Text-to-Image Generation"></a>RichControl: Structure- and Appearance-Rich Training-Free Spatial   Control for Text-to-Image Generation</h2><p><strong>Authors:Liheng Zhang, Lexi Pang, Hang Ye, Xiaoxuan Ma, Yizhou Wang</strong></p>
<p>Text-to-image (T2I) diffusion models have shown remarkable success in generating high-quality images from text prompts. Recent efforts extend these models to incorporate conditional images (e.g., depth or pose maps) for fine-grained spatial control. Among them, feature injection methods have emerged as a training-free alternative to traditional fine-tuning approaches. However, they often suffer from structural misalignment, condition leakage, and visual artifacts, especially when the condition image diverges significantly from natural RGB distributions. By revisiting existing methods, we identify a core limitation: the synchronous injection of condition features fails to account for the trade-off between domain alignment and structural preservation during denoising. Inspired by this observation, we propose a flexible feature injection framework that decouples the injection timestep from the denoising process. At its core is a structure-rich injection module, which enables the model to better adapt to the evolving interplay between alignment and structure preservation throughout the diffusion steps, resulting in more faithful structural generation. In addition, we introduce appearance-rich prompting and a restart refinement strategy to further enhance appearance control and visual quality. Together, these designs enable training-free generation that is both structure-rich and appearance-rich. Extensive experiments show that our approach achieves state-of-the-art performance across diverse zero-shot conditioning scenarios. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚æœ€è¿‘çš„ç ”ç©¶åŠªåŠ›å°†è¿™äº›æ¨¡å‹æ‰©å±•åˆ°ç»“åˆæ¡ä»¶å›¾åƒï¼ˆä¾‹å¦‚æ·±åº¦æˆ–å§¿æ€å›¾ï¼‰ä»¥å®ç°ç²¾ç»†çš„ç©ºé—´æ§åˆ¶ã€‚å…¶ä¸­ï¼Œç‰¹å¾æ³¨å…¥æ–¹æ³•ä½œä¸ºä¼ ç»Ÿå¾®è°ƒæ–¹æ³•çš„æ— è®­ç»ƒæ›¿ä»£æ–¹æ³•åº”è¿è€Œç”Ÿã€‚ç„¶è€Œï¼Œå®ƒä»¬ç»å¸¸é­å—ç»“æ„é”™ä½ã€æ¡ä»¶æ³„æ¼å’Œè§†è§‰ä¼ªå½±ç­‰é—®é¢˜ï¼Œå°¤å…¶æ˜¯å½“æ¡ä»¶å›¾åƒä¸è‡ªç„¶çš„RGBåˆ†å¸ƒç›¸å·®è¾ƒå¤§æ—¶ã€‚é€šè¿‡å¯¹ç°æœ‰æ–¹æ³•è¿›è¡Œé‡æ–°å®¡è§†ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªæ ¸å¿ƒå±€é™ï¼šæ¡ä»¶ç‰¹å¾çš„åŒæ­¥æ³¨å…¥æœªèƒ½è€ƒè™‘åˆ°å»å™ªè¿‡ç¨‹ä¸­çš„åŸŸå¯¹é½ä¸ç»“æ„ä¿æŒä¹‹é—´çš„æƒè¡¡ã€‚å—æ­¤è§‚å¯Ÿå¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§çµæ´»çš„ç‰¹å¾æ³¨å…¥æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†æ³¨å…¥æ—¶é—´ä¸å»å™ªè¿‡ç¨‹è§£è€¦ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªç»“æ„ä¸°å¯Œçš„æ³¨å…¥æ¨¡å—ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”æ‰©æ•£æ­¥éª¤ä¸­å¯¹é½ä¸ç»“æ„ä¿æŒä¹‹é—´çš„ä¸æ–­æ¼”å˜ï¼Œä»è€Œç”Ÿæˆæ›´çœŸå®ç²¾ç»†çš„ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤–è§‚ä¸°å¯Œçš„æç¤ºå’Œé‡å¯ä¼˜åŒ–ç­–ç•¥ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºå¤–è§‚æ§åˆ¶å’Œè§†è§‰è´¨é‡ã€‚è¿™äº›è®¾è®¡çš„ç»“åˆå®ç°äº†æ— è®­ç»ƒç”Ÿæˆï¼Œæ—¢ä¸°å¯Œç»“æ„åˆä¸°å¯Œå¤–è§‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§é›¶æ ·æœ¬æ¡ä»¶åœºæ™¯ä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02792v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹å·²æˆåŠŸå®ç°æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚è¿‘æœŸçš„ç ”ç©¶å°è¯•å°†è¿™ç§æ¨¡å‹æ‰©å±•åˆ°ç»“åˆæ¡ä»¶å›¾åƒï¼ˆä¾‹å¦‚æ·±åº¦æˆ–å§¿æ€å›¾ï¼‰ä»¥å®ç°ç²¾ç»†çš„ç©ºé—´æ§åˆ¶ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç‰¹å¾æ³¨å…¥æ–¹æ³•å¾€å¾€é¢ä¸´ç»“æ„ä¸å¯¹å‡†ã€æ¡ä»¶æ³„æ¼å’Œè§†è§‰ä¼ªå½±ç­‰é—®é¢˜ï¼Œå°¤å…¶åœ¨æ¡ä»¶å›¾åƒä¸è‡ªç„¶RGBåˆ†å¸ƒç›¸å·®è¾ƒå¤§æ—¶ã€‚æœ¬ç ”ç©¶è¯†åˆ«äº†ç°æœ‰æ–¹æ³•çš„æ ¸å¿ƒå±€é™ï¼šåŒæ­¥æ³¨å…¥æ¡ä»¶ç‰¹å¾æœªèƒ½è€ƒè™‘åŸŸå¯¹é½ä¸ç»“æ„ä¿ç•™ä¹‹é—´çš„æƒè¡¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªçµæ´»çš„ç‰¹å¾æ³¨å…¥æ¡†æ¶ï¼Œå°†æ³¨å…¥æ—¶é—´ä¸å»å™ªè¿‡ç¨‹åˆ†ç¦»ã€‚å…¶æ ¸å¿ƒæ˜¯ç»“æ„ä¸°å¯Œçš„æ³¨å…¥æ¨¡å—ï¼Œä½¿æ¨¡å‹èƒ½æ›´å¥½åœ°é€‚åº”æ‰©æ•£æ­¥éª¤ä¸­å¯¹é½ä¸ç»“æ„ä¿ç•™ä¹‹é—´çš„å¹³è¡¡ï¼Œä»è€Œå®ç°æ›´çœŸå®çš„ç»“æ„ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†å¤–è§‚ä¸°å¯Œçš„æç¤ºå’Œé‡å¯ç»†åŒ–ç­–ç•¥ï¼Œä»¥è¿›ä¸€æ­¥æé«˜å¤–è§‚æ§åˆ¶å’Œè§†è§‰è´¨é‡ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æ— éœ€è®­ç»ƒçš„ç»“æ„ä¸°å¯Œå’Œå¤–è§‚ä¸°å¯Œçš„ç”Ÿæˆï¼Œå¹¶åœ¨å¤šç§é›¶æ ·æœ¬æ¡ä»¶åœºæ™¯ä¸‹å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹èƒ½å¤ŸåŸºäºæ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚</li>
<li>ç°æœ‰ç‰¹å¾æ³¨å…¥æ–¹æ³•å­˜åœ¨ç»“æ„ä¸å¯¹å‡†ã€æ¡ä»¶æ³„æ¼å’Œè§†è§‰ä¼ªå½±ç­‰é—®é¢˜ã€‚</li>
<li>åŒæ­¥æ³¨å…¥æ¡ä»¶ç‰¹å¾çš„æ–¹æ³•æœªå……åˆ†è€ƒè™‘åŸŸå¯¹é½ä¸ç»“æ„ä¿ç•™ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>çµæ´»çš„ç‰¹å¾æ³¨å…¥æ¡†æ¶é€šè¿‡è§£è€¦æ³¨å…¥æ—¶é—´ä¸å»å™ªè¿‡ç¨‹æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>ç»“æ„ä¸°å¯Œçš„æ³¨å…¥æ¨¡å—èƒ½å¹³è¡¡å¯¹é½ä¸ç»“æ„ä¿ç•™ï¼Œå®ç°æ›´çœŸå®çš„ç»“æ„ç”Ÿæˆã€‚</li>
<li>å¼•å…¥å¤–è§‚ä¸°å¯Œçš„æç¤ºå’Œé‡å¯ç»†åŒ–ç­–ç•¥ï¼Œæé«˜å¤–è§‚æ§åˆ¶å’Œè§†è§‰è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02792">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-992a44d76813f28a28a7fdaa65763383.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-65688f8a4471283b4ddc27621fa25110.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e027642ba4dc78cc06f07534d38dbb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9680d97bd7ad50a53bbbc66a97534a0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="APT-Adaptive-Personalized-Training-for-Diffusion-Models-with-Limited-Data"><a href="#APT-Adaptive-Personalized-Training-for-Diffusion-Models-with-Limited-Data" class="headerlink" title="APT: Adaptive Personalized Training for Diffusion Models with Limited   Data"></a>APT: Adaptive Personalized Training for Diffusion Models with Limited   Data</h2><p><strong>Authors:JungWoo Chae, Jiyoon Kim, JaeWoong Choi, Kyungyul Kim, Sangheum Hwang</strong></p>
<p>Personalizing diffusion models using limited data presents significant challenges, including overfitting, loss of prior knowledge, and degradation of text alignment. Overfitting leads to shifts in the noise prediction distribution, disrupting the denoising trajectory and causing the model to lose semantic coherence. In this paper, we propose Adaptive Personalized Training (APT), a novel framework that mitigates overfitting by employing adaptive training strategies and regularizing the modelâ€™s internal representations during fine-tuning. APT consists of three key components: (1) Adaptive Training Adjustment, which introduces an overfitting indicator to detect the degree of overfitting at each time step bin and applies adaptive data augmentation and adaptive loss weighting based on this indicator; (2)Representation Stabilization, which regularizes the mean and variance of intermediate feature maps to prevent excessive shifts in noise prediction; and (3) Attention Alignment for Prior Knowledge Preservation, which aligns the cross-attention maps of the fine-tuned model with those of the pretrained model to maintain prior knowledge and semantic coherence. Through extensive experiments, we demonstrate that APT effectively mitigates overfitting, preserves prior knowledge, and outperforms existing methods in generating high-quality, diverse images with limited reference data. </p>
<blockquote>
<p>ä½¿ç”¨æœ‰é™æ•°æ®ä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹é¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¿‡æ‹Ÿåˆã€ä¸¢å¤±å…ˆéªŒçŸ¥è¯†å’Œæ–‡æœ¬å¯¹é½çš„é€€åŒ–ã€‚è¿‡æ‹Ÿåˆä¼šå¯¼è‡´å™ªå£°é¢„æµ‹åˆ†å¸ƒå‘ç”Ÿåç§»ï¼Œç ´åå»å™ªè½¨è¿¹ï¼Œå¯¼è‡´æ¨¡å‹å¤±å»è¯­ä¹‰è¿è´¯æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”ä¸ªæ€§åŒ–è®­ç»ƒï¼ˆAPTï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡é‡‡ç”¨è‡ªé€‚åº”è®­ç»ƒç­–ç•¥å’Œæ­£åˆ™åŒ–æ¨¡å‹åœ¨å¾®è°ƒæœŸé—´çš„å†…éƒ¨è¡¨å¾æ¥ç¼“è§£è¿‡æ‹Ÿåˆé—®é¢˜ã€‚APTåŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰è‡ªé€‚åº”è®­ç»ƒè°ƒæ•´ï¼Œå®ƒå¼•å…¥è¿‡æ‹ŸåˆæŒ‡æ ‡æ¥æ£€æµ‹æ¯ä¸ªæ—¶é—´æ­¥é•¿çš„è¿‡æ‹Ÿåˆç¨‹åº¦ï¼Œå¹¶åŸºäºæ­¤æŒ‡æ ‡åº”ç”¨è‡ªé€‚åº”æ•°æ®å¢å¼ºå’Œè‡ªé€‚åº”æŸå¤±åŠ æƒï¼›ï¼ˆ2ï¼‰è¡¨å¾ç¨³å®šï¼Œå®ƒæ­£åˆ™åŒ–ä¸­é—´ç‰¹å¾å›¾çš„å‡å€¼å’Œæ–¹å·®ï¼Œä»¥é˜²æ­¢å™ªå£°é¢„æµ‹è¿‡åº¦åç§»ï¼›ï¼ˆ3ï¼‰ä¿ç•™å…ˆéªŒçŸ¥è¯†çš„æ³¨æ„åŠ›å¯¹é½ï¼Œå®ƒå°†å¾®è°ƒæ¨¡å‹çš„äº¤å‰æ³¨æ„åŠ›å›¾ä¸é¢„è®­ç»ƒæ¨¡å‹çš„äº¤å‰æ³¨æ„åŠ›å›¾å¯¹é½ï¼Œä»¥ç»´æŒå…ˆéªŒçŸ¥è¯†å’Œè¯­ä¹‰è¿è´¯æ€§ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜APTèƒ½å¤Ÿæœ‰æ•ˆåœ°ç¼“è§£è¿‡æ‹Ÿåˆï¼Œä¿ç•™å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶åœ¨ä½¿ç”¨æœ‰é™å‚è€ƒæ•°æ®æ—¶ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02687v1">PDF</a> CVPR 2025 camera ready. Project page: <a target="_blank" rel="noopener" href="https://lgcnsai.github.io/apt">https://lgcnsai.github.io/apt</a></p>
<p><strong>Summary</strong>ï¼š<br>ä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹åœ¨ä½¿ç”¨æœ‰é™æ•°æ®æ—¶é¢ä¸´è¿‡æ‹Ÿåˆã€ä¸¢å¤±å…ˆéªŒçŸ¥è¯†å’Œæ–‡æœ¬å¯¹é½é€€åŒ–ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºè‡ªé€‚åº”ä¸ªæ€§åŒ–è®­ç»ƒï¼ˆAPTï¼‰æ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”è®­ç»ƒç­–ç•¥å’Œå¾®è°ƒæ—¶çš„æ¨¡å‹å†…éƒ¨è¡¨ç¤ºæ­£åˆ™åŒ–æ¥ç¼“è§£è¿‡æ‹Ÿåˆé—®é¢˜ã€‚APTåŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šè‡ªé€‚åº”è®­ç»ƒè°ƒæ•´ã€è¡¨ç¤ºç¨³å®šåŒ–å’Œæ³¨æ„åŠ›å¯¹é½ä»¥ä¿ç•™å…ˆéªŒçŸ¥è¯†ã€‚å®éªŒè¡¨æ˜ï¼ŒAPTæœ‰æ•ˆç¼“è§£è¿‡æ‹Ÿåˆï¼Œä¿ç•™å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶åœ¨æœ‰é™å‚è€ƒæ•°æ®ä¸‹ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æœ‰é™æ•°æ®çš„ä¸ªæ€§åŒ–åº”ç”¨é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¿‡æ‹Ÿåˆã€ä¸¢å¤±å…ˆéªŒçŸ¥è¯†å’Œæ–‡æœ¬å¯¹é½é—®é¢˜ã€‚</li>
<li>è¿‡æ‹Ÿåˆä¼šå¯¼è‡´å™ªå£°é¢„æµ‹åˆ†å¸ƒåç§»ï¼Œç ´åå»å™ªè½¨è¿¹å¹¶å¯¼è‡´æ¨¡å‹è¯­ä¹‰è¿è´¯æ€§ä¸§å¤±ã€‚</li>
<li>APTæ¡†æ¶é€šè¿‡è‡ªé€‚åº”è®­ç»ƒç­–ç•¥å’Œæ¨¡å‹å†…éƒ¨è¡¨ç¤ºæ­£åˆ™åŒ–æ¥ç¼“è§£è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>APTåŒ…æ‹¬è‡ªé€‚åº”è®­ç»ƒè°ƒæ•´ã€è¡¨ç¤ºç¨³å®šå’Œæ³¨æ„åŠ›å¯¹é½ä»¥ä¿ç•™å…ˆéªŒçŸ¥è¯†çš„ä¸‰ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>è‡ªé€‚åº”è®­ç»ƒè°ƒæ•´é€šè¿‡å¼•å…¥è¿‡æ‹ŸåˆæŒ‡æ ‡æ¥æ£€æµ‹æ¯ä¸ªæ—¶é—´æ­¥é•¿çš„è¿‡æ‹Ÿåˆç¨‹åº¦ï¼Œå¹¶åŸºäºæ­¤è¿›è¡Œè‡ªé€‚åº”æ•°æ®å¢å¼ºå’ŒæŸå¤±æƒé‡è°ƒæ•´ã€‚</li>
<li>è¡¨ç¤ºç¨³å®šåŒ–é€šè¿‡æ­£åˆ™åŒ–ä¸­é—´ç‰¹å¾å›¾çš„å‡å€¼å’Œæ–¹å·®æ¥é˜²æ­¢å™ªå£°é¢„æµ‹è¿‡åº¦åç§»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cf229fe99c5ffd9f570bf08b3f445739.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6133c83a4ea91839bbb2831c8e1175d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-648b329d8b609460f99fa4846b21988c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72043f92f526d573855f5e66fe80aa38.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Learning-few-step-posterior-samplers-by-unfolding-and-distillation-of-diffusion-models"><a href="#Learning-few-step-posterior-samplers-by-unfolding-and-distillation-of-diffusion-models" class="headerlink" title="Learning few-step posterior samplers by unfolding and distillation of   diffusion models"></a>Learning few-step posterior samplers by unfolding and distillation of   diffusion models</h2><p><strong>Authors:Charlesquin Kemajou Mbakam, Jonathan Spence, Marcelo Pereyra</strong></p>
<p>Diffusion models (DMs) have emerged as powerful image priors in Bayesian computational imaging. Two primary strategies have been proposed for leveraging DMs in this context: Plug-and-Play methods, which are zero-shot and highly flexible but rely on approximations; and specialized conditional DMs, which achieve higher accuracy and faster inference for specific tasks through supervised training. In this work, we introduce a novel framework that integrates deep unfolding and model distillation to transform a DM image prior into a few-step conditional model for posterior sampling. A central innovation of our approach is the unfolding of a Markov chain Monte Carlo (MCMC) algorithm - specifically, the recently proposed LATINO Langevin sampler (Spagnoletti et al., 2025) - representing the first known instance of deep unfolding applied to a Monte Carlo sampling scheme. We demonstrate our proposed unfolded and distilled samplers through extensive experiments and comparisons with the state of the art, where they achieve excellent accuracy and computational efficiency, while retaining the flexibility to adapt to variations in the forward model at inference time. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ä½œä¸ºè´å¶æ–¯è®¡ç®—æˆåƒä¸­çš„å¼ºå¤§å›¾åƒå…ˆéªŒå·²ç»å´­éœ²å¤´è§’ã€‚é’ˆå¯¹æ­¤èƒŒæ™¯ï¼Œæå‡ºäº†ä¸¤ç§ä¸»è¦çš„åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼šå³æ’å³ç”¨æ–¹æ³•ï¼Œå®ƒä»¬æ˜¯é›¶æ ·æœ¬ä¸”é«˜åº¦çµæ´»ä½†ä¾èµ–äºè¿‘ä¼¼ï¼›ä»¥åŠä¸“ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ç›‘ç£è®­ç»ƒé’ˆå¯¹ç‰¹å®šä»»åŠ¡å®ç°æ›´é«˜çš„ç²¾åº¦å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°é¢–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æ·±åº¦å±•å¼€å’Œæ¨¡å‹è’¸é¦æŠ€æœ¯ï¼Œå°†æ‰©æ•£æ¨¡å‹å›¾åƒå…ˆéªŒè½¬åŒ–ä¸ºç”¨äºåé‡‡æ ·çš„ä¸€æ­¥æ¡ä»¶æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„ä¸€ä¸ªæ ¸å¿ƒåˆ›æ–°æ˜¯å±•å¼€é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—ï¼ˆMCMCï¼‰ç®—æ³•â€”â€”å…·ä½“æ¥è¯´ï¼Œæ˜¯æœ€è¿‘æå‡ºçš„LATINO Langeviné‡‡æ ·å™¨ï¼ˆSpagnolettiç­‰äººï¼Œ2025ï¼‰â€”â€”è¿™ä»£è¡¨äº†é¦–æ¬¡å°†æ·±åº¦å±•å¼€åº”ç”¨äºè’™ç‰¹å¡ç½—é‡‡æ ·æ–¹æ¡ˆçš„å®ä¾‹ã€‚æˆ‘ä»¬é€šè¿‡å¹¿æ³›çš„å®éªŒå’Œä¸æœ€æ–°æŠ€æœ¯çš„æ¯”è¾ƒæ¥å±•ç¤ºæˆ‘ä»¬æå‡ºçš„å±•å¼€å’Œè’¸é¦é‡‡æ ·å™¨ï¼Œå®ƒä»¬åœ¨ä¿æŒæ¨ç†æ—¶å‰å‘æ¨¡å‹å˜åŒ–é€‚åº”æ€§çš„åŒæ—¶ï¼Œå®ç°äº†å‡ºè‰²çš„å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02686v1">PDF</a> 28 pages, 16 figures, 10 tables</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨è´å¶æ–¯è®¡ç®—æˆåƒä¸­ä½œä¸ºå›¾åƒå…ˆéªŒå±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ã€‚æœ¬ç ”ç©¶å¼•å…¥ä¸€ä¸ªç»“åˆæ·±åº¦å±•å¼€å’Œæ¨¡å‹è’¸é¦çš„æ–°æ¡†æ¶ï¼Œå°†æ‰©æ•£æ¨¡å‹å›¾åƒå…ˆéªŒè½¬åŒ–ä¸ºç”¨äºåéªŒé‡‡æ ·çš„å¤šæ­¥æ¡ä»¶æ¨¡å‹ã€‚å…³é”®åˆ›æ–°åœ¨äºå°†é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—ç®—æ³•ï¼ˆMCMCï¼‰è¿›è¡Œå±•å¼€ï¼Œå°¤å…¶æ˜¯Spagnolettiç­‰äººäºè¿‘æœŸæå‡ºçš„LATINO Langeviné‡‡æ ·å™¨ã€‚è¿™æ˜¯é¦–æ¬¡å°†æ·±åº¦å±•å¼€åº”ç”¨äºè’™ç‰¹å¡ç½—é‡‡æ ·æ–¹æ¡ˆã€‚å®éªŒè¯æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å’Œè’¸é¦é‡‡æ ·å™¨åœ¨å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿ç•™äº†é€‚åº”å‰å‘æ¨¡å‹å˜åŒ–çš„çµæ´»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨è´å¶æ–¯è®¡ç®—æˆåƒä¸­ä½œä¸ºå¼ºå¤§çš„å›¾åƒå…ˆéªŒã€‚</li>
<li>æå‡ºäº†ç»“åˆæ·±åº¦å±•å¼€å’Œæ¨¡å‹è’¸é¦çš„æ–°æ¡†æ¶ï¼Œç”¨äºå°†æ‰©æ•£æ¨¡å‹è½¬åŒ–ä¸ºæ¡ä»¶æ¨¡å‹è¿›è¡ŒåéªŒé‡‡æ ·ã€‚</li>
<li>åˆ›æ–°æ€§åœ°å±•å¼€é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—ç®—æ³•ï¼ˆMCMCï¼‰ï¼Œå…·ä½“æ˜¯Spagnolettiç­‰äººæå‡ºçš„LATINO Langeviné‡‡æ ·å™¨ã€‚</li>
<li>æ­¤ä¸ºæ·±åº¦å±•å¼€é¦–æ¬¡åº”ç”¨äºè’™ç‰¹å¡ç½—é‡‡æ ·æ–¹æ¡ˆã€‚</li>
<li>é€šè¿‡å¹¿æ³›å®éªŒå’Œä¸æœ€æ–°æŠ€æœ¯çš„æ¯”è¾ƒï¼Œè¯æ˜æ‰€æå‡ºçš„æ–¹æ³•å’Œè’¸é¦é‡‡æ ·å™¨å…·æœ‰é«˜å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
<li>æ‰€æå‡ºæ–¹æ³•èƒ½å¤Ÿé€‚åº”ä¸åŒçš„å‰å‘æ¨¡å‹å˜åŒ–ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæ‰©æ•£æ¨¡å‹åœ¨è´å¶æ–¯è®¡ç®—æˆåƒé¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„è§†è§’å’Œå·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ece5f8def27a63ef1b6b4e3face95067.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b18d1fd0cf7fadde1d207267175b8e80.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AC-Refiner-Efficient-Arithmetic-Circuit-Optimization-Using-Conditional-Diffusion-Models"><a href="#AC-Refiner-Efficient-Arithmetic-Circuit-Optimization-Using-Conditional-Diffusion-Models" class="headerlink" title="AC-Refiner: Efficient Arithmetic Circuit Optimization Using Conditional   Diffusion Models"></a>AC-Refiner: Efficient Arithmetic Circuit Optimization Using Conditional   Diffusion Models</h2><p><strong>Authors:Chenhao Xue, Kezhi Li, Jiaxing Zhang, Yi Ren, Zhengyuan Shi, Chen Zhang, Yibo Lin, Lining Zhang, Qiang Xu, Guangyu Sun</strong></p>
<p>Arithmetic circuits, such as adders and multipliers, are fundamental components of digital systems, directly impacting the performance, power efficiency, and area footprint. However, optimizing these circuits remains challenging due to the vast design space and complex physical constraints. While recent deep learning-based approaches have shown promise, they struggle to consistently explore high-potential design variants, limiting their optimization efficiency. To address this challenge, we propose AC-Refiner, a novel arithmetic circuit optimization framework leveraging conditional diffusion models. Our key insight is to reframe arithmetic circuit synthesis as a conditional image generation task. By carefully conditioning the denoising diffusion process on target quality-of-results (QoRs), AC-Refiner consistently produces high-quality circuit designs. Furthermore, the explored designs are used to fine-tune the diffusion model, which focuses the exploration near the Pareto frontier. Experimental results demonstrate that AC-Refiner generates designs with superior Pareto optimality, outperforming state-of-the-art baselines. The performance gain is further validated by integrating AC-Refiner into practical applications. </p>
<blockquote>
<p>ç®—æœ¯ç”µè·¯ï¼Œå¦‚åŠ æ³•å™¨å’Œä¹˜æ³•å™¨ï¼Œæ˜¯æ•°å­—ç³»ç»Ÿçš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ï¼Œç›´æ¥å½±å“æ€§èƒ½ã€åŠŸè€—å’Œé¢ç§¯å ç”¨ã€‚ç„¶è€Œï¼Œç”±äºå·¨å¤§çš„è®¾è®¡ç©ºé—´å’Œå¤æ‚çš„ç‰©ç†çº¦æŸï¼Œä¼˜åŒ–è¿™äº›ç”µè·¯ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚è™½ç„¶æœ€è¿‘çš„åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•æ˜¾ç¤ºå‡ºäº†ä¸€å®šçš„æ½œåŠ›ï¼Œä½†å®ƒä»¬éš¾ä»¥æŒç»­æ¢ç´¢é«˜æ½œåŠ›çš„è®¾è®¡å˜ä½“ï¼Œä»è€Œé™åˆ¶äº†ä¼˜åŒ–æ•ˆç‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†AC-Refinerï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ–°å‹ç®—æœ¯ç”µè·¯ä¼˜åŒ–æ¡†æ¶ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯å°†ç®—æœ¯ç”µè·¯åˆæˆé‡æ–°æ„å»ºä¸ºæ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚é€šè¿‡ä»”ç»†å°†å»å™ªæ‰©æ•£è¿‡ç¨‹è°ƒèŠ‚åœ¨ç›®æ ‡ç»“æœè´¨é‡ï¼ˆQoRï¼‰ä¸Šï¼ŒAC-Refinerèƒ½å¤ŸæŒç»­äº§ç”Ÿé«˜è´¨é‡çš„ç”µè·¯è®¾è®¡ã€‚æ­¤å¤–ï¼Œæ‰€æ¢ç´¢çš„è®¾è®¡ç”¨äºå¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼Œä½¿æ¢ç´¢é›†ä¸­åœ¨å¸•ç´¯æ‰˜å‰æ²¿é™„è¿‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAC-Refinerç”Ÿæˆçš„è®¾è®¡å…·æœ‰ä¼˜è¶Šçš„å¸•ç´¯æ‰˜æœ€ä¼˜æ€§ï¼Œè¶…è¿‡äº†æœ€æ–°çš„åŸºçº¿æ ‡å‡†ã€‚é€šè¿‡å°†AC-Refineré›†æˆåˆ°å®é™…åº”ç”¨ä¸­ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02598v1">PDF</a> 8 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç®—æœ¯ç”µè·¯å¦‚åŠ å™¨å’Œä¹˜å™¨åœ¨æ•°å­—ç³»ç»Ÿä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œå…¶æ€§èƒ½ä¼˜åŒ–è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºè®¾è®¡ç©ºé—´çš„åºå¤§å’Œç‰©ç†çº¦æŸçš„å¤æ‚æ€§ï¼Œä¼˜åŒ–é¢ä¸´æŒ‘æˆ˜ã€‚è¿‘æœŸæ·±åº¦å­¦ä¹ æ–¹æ³•è™½å±•ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨æ¢ç´¢é«˜æ•ˆè®¾è®¡å˜ä½“æ—¶å­˜åœ¨å±€é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„ç®—æœ¯ç”µè·¯ä¼˜åŒ–æ¡†æ¶AC-Refinerã€‚é€šè¿‡æŠŠç®—æœ¯ç”µè·¯åˆæˆé‡æ–°å®šä½ä¸ºæ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ï¼ŒAC-Refinerèƒ½åœ¨ç›®æ ‡ç»“æœè´¨é‡æŒ‡å¯¼ä¸‹æŒç»­ç”Ÿæˆé«˜è´¨é‡ç”µè·¯è®¾è®¡ã€‚æ­¤å¤–ï¼Œæ‰€æ¢ç´¢çš„è®¾è®¡ç”¨äºå¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼Œä½¿å…¶èšç„¦äºParetoå‰æ²¿çš„æ¢ç´¢ã€‚å®éªŒè¯å®AC-Refinerç”Ÿæˆçš„ç”µè·¯è®¾è®¡å…·æœ‰æ›´ä½³çš„Paretoæœ€ä¼˜æ€§ï¼Œè¶…è¶Šäº†ç°æœ‰åŸºçº¿æŠ€æœ¯ï¼Œå¹¶åœ¨å®é™…åº”ç”¨ä¸­å¾—åˆ°äº†æ€§èƒ½æå‡éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç®—æœ¯ç”µè·¯æ˜¯æ•°å­—ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œå…¶æ€§èƒ½ä¼˜åŒ–è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨ä¼˜åŒ–ç®—æœ¯ç”µè·¯æ—¶å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥æŒç»­æ¢ç´¢é«˜æ•ˆè®¾è®¡å˜ä½“ã€‚</li>
<li>AC-Refineræ¡†æ¶åˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹è¿›è¡Œç®—æœ¯ç”µè·¯ä¼˜åŒ–ã€‚</li>
<li>AC-Refinerå°†ç®—æœ¯ç”µè·¯åˆæˆé‡æ–°å®šä½ä¸ºæ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>AC-Refineré€šè¿‡ç›®æ ‡ç»“æœè´¨é‡æŒ‡å¯¼ç”Ÿæˆé«˜è´¨é‡çš„ç”µè·¯è®¾è®¡ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å¯é€šè¿‡ä½¿ç”¨æ¢ç´¢æ€§è®¾è®¡è¿›è¡Œå¾®è°ƒï¼Œèšç„¦äºParetoå‰æ²¿çš„æ¢ç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02598">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7da2b2423f0cf4292137ddd836e1fd61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7f052f6621431fd6e5abeab844f01e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-12cafd84df24996a260e6cef1e16cf89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d8cc8adb26a63e6d028bbba2038e4f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f976c4b10ee41c7dbc0a7811c3727a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4b4e47c3c4000a85f450bc1d6473f56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fd1f8cd2c0605afd98d921029ca2109.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f206297e7d6c8a764a8ec9cd42a4a51.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="AvatarMakeup-Realistic-Makeup-Transfer-for-3D-Animatable-Head-Avatars"><a href="#AvatarMakeup-Realistic-Makeup-Transfer-for-3D-Animatable-Head-Avatars" class="headerlink" title="AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars"></a>AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars</h2><p><strong>Authors:Yiming Zhong, Xiaolin Zhang, Ligang Liu, Yao Zhao, Yunchao Wei</strong></p>
<p>Similar to facial beautification in real life, 3D virtual avatars require personalized customization to enhance their visual appeal, yet this area remains insufficiently explored. Although current 3D Gaussian editing methods can be adapted for facial makeup purposes, these methods fail to meet the fundamental requirements for achieving realistic makeup effects: 1) ensuring a consistent appearance during drivable expressions, 2) preserving the identity throughout the makeup process, and 3) enabling precise control over fine details. To address these, we propose a specialized 3D makeup method named AvatarMakeup, leveraging a pretrained diffusion model to transfer makeup patterns from a single reference photo of any individual. We adopt a coarse-to-fine idea to first maintain the consistent appearance and identity, and then to refine the details. In particular, the diffusion model is employed to generate makeup images as supervision. Due to the uncertainties in diffusion process, the generated images are inconsistent across different viewpoints and expressions. Therefore, we propose a Coherent Duplication method to coarsely apply makeup to the target while ensuring consistency across dynamic and multiview effects. Coherent Duplication optimizes a global UV map by recoding the averaged facial attributes among the generated makeup images. By querying the global UV map, it easily synthesizes coherent makeup guidance from arbitrary views and expressions to optimize the target avatar. Given the coarse makeup avatar, we further enhance the makeup by incorporating a Refinement Module into the diffusion model to achieve high makeup quality. Experiments demonstrate that AvatarMakeup achieves state-of-the-art makeup transfer quality and consistency throughout animation. </p>
<blockquote>
<p>ä¸ç°å®ç”Ÿæ´»ä¸­çš„é¢éƒ¨ç¾å®¹ç±»ä¼¼ï¼Œ3Dè™šæ‹Ÿè§’è‰²éœ€è¦ä¸ªæ€§åŒ–å®šåˆ¶ä»¥å¢å¼ºå…¶è§†è§‰å¸å¼•åŠ›ï¼Œä½†è¿™ä¸ªé¢†åŸŸä»ç„¶æ²¡æœ‰å¾—åˆ°è¶³å¤Ÿçš„æ¢ç´¢ã€‚è™½ç„¶å½“å‰çš„3Dé«˜æ–¯ç¼–è¾‘æ–¹æ³•å¯ä»¥é€‚åº”é¢éƒ¨åŒ–å¦†çš„ç›®çš„ï¼Œä½†è¿™äº›æ–¹æ³•æ— æ³•æ»¡è¶³å®ç°çœŸå®åŒ–å¦†æ•ˆæœçš„åŸºæœ¬éœ€æ±‚ï¼š1ï¼‰åœ¨å¯é©±åŠ¨çš„è¡¨æƒ…ä¸­ä¿æŒå¤–è§‚çš„ä¸€è‡´æ€§ï¼Œ2ï¼‰åœ¨åŒ–å¦†è¿‡ç¨‹ä¸­ä¿æŒèº«ä»½ç‰¹å¾ï¼Œä»¥åŠ3ï¼‰å¯¹ç»†èŠ‚è¿›è¡Œç²¾ç¡®æ§åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸“é—¨çš„3DåŒ–å¦†æ–¹æ³•ï¼Œåä¸ºAvatarMakeupï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä»ä»»ä½•ä¸ªäººçš„å•å¼ å‚è€ƒç…§ç‰‡ä¸­è½¬ç§»åŒ–å¦†æ¨¡å¼ã€‚æˆ‘ä»¬é‡‡ç”¨ç”±ç²—åˆ°ç»†çš„ç†å¿µï¼Œé¦–å…ˆä¿æŒå¤–è§‚å’Œèº«ä»½çš„ä¸€è‡´æ€§ï¼Œç„¶åå¯¹ç»†èŠ‚è¿›è¡Œå®Œå–„ã€‚ç‰¹åˆ«æ˜¯ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”ŸæˆåŒ–å¦†å›¾åƒä½œä¸ºç›‘ç£ã€‚ç”±äºæ‰©æ•£è¿‡ç¨‹ä¸­çš„ä¸ç¡®å®šæ€§ï¼Œç”Ÿæˆçš„å›¾åƒåœ¨ä¸åŒçš„è§†è§’å’Œè¡¨æƒ…ä¸Šæ˜¯ä¸ä¸€è‡´çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¿è´¯å¤åˆ¶æ–¹æ³•ï¼Œå°†åŒ–å¦†ç²—ç•¥åœ°åº”ç”¨åˆ°ç›®æ ‡ä¸Šï¼ŒåŒæ—¶ç¡®ä¿åŠ¨æ€å’Œå¤šè§†è§’æ•ˆæœçš„ä¸€è‡´æ€§ã€‚è¿è´¯å¤åˆ¶é€šè¿‡é‡æ–°ç¼–ç ç”ŸæˆåŒ–å¦†å›¾åƒä¹‹é—´çš„å¹³å‡é¢éƒ¨å±æ€§æ¥ä¼˜åŒ–å…¨å±€UVæ˜ å°„ã€‚é€šè¿‡æŸ¥è¯¢å…¨å±€UVæ˜ å°„ï¼Œå®ƒå¾ˆå®¹æ˜“åˆæˆæ¥è‡ªä»»æ„è§†è§’å’Œè¡¨æƒ…çš„è¿è´¯åŒ–å¦†æŒ‡å¯¼ï¼Œä»¥ä¼˜åŒ–ç›®æ ‡è§’è‰²ã€‚ç»™å®šç²—ç•¥çš„åŒ–å¦†è§’è‰²ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡å°†ç»†åŒ–æ¨¡å—èå…¥æ‰©æ•£æ¨¡å‹æ¥æé«˜åŒ–å¦†å“è´¨ï¼Œä»è€Œå®ç°é«˜è´¨é‡çš„åŒ–å¦†æ•ˆæœã€‚å®éªŒè¡¨æ˜ï¼ŒAvatarMakeupåœ¨åŠ¨ç”»ä¸­å®ç°äº†æœ€å…ˆè¿›çš„åŒ–å¦†è½¬ç§»è´¨é‡å’Œä¸€è‡´æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02419v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºAvatarMakeupçš„3Dè™šæ‹Ÿè§’è‰²åŒ–å¦†æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä»å•ä¸€å‚è€ƒç…§ç‰‡è½¬ç§»å¦†å®¹æ¨¡å¼ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä»ç²—åˆ°ç»†çš„ç­–ç•¥ï¼Œå…ˆä¿æŒå¤–è§‚å’Œèº«ä»½çš„è¿ç»­æ€§ï¼Œå†ç»†åŒ–ç»†èŠ‚ã€‚ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¦†å®¹å›¾åƒä½œä¸ºç›‘ç£ï¼Œå¹¶æå‡ºä¸€ç§Coherent Duplicationæ–¹æ³•æ¥ç¡®ä¿åŠ¨æ€å’Œå¤šè§†è§’ä¸‹çš„å¦†å®¹ä¸€è‡´æ€§ã€‚æœ€åï¼Œé€šè¿‡å¼•å…¥ç»†åŒ–æ¨¡å—æ¥è¿›ä¸€æ­¥æé«˜å¦†å®¹è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰3Dè™šæ‹Ÿè§’è‰²çš„åŒ–å¦†ä¸ªæ€§åŒ–å®šåˆ¶éœ€æ±‚å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>ç°æœ‰çš„3Dé«˜æ–¯ç¼–è¾‘æ–¹æ³•åœ¨è™šæ‹Ÿè§’è‰²åŒ–å¦†æ–¹é¢å­˜åœ¨ç¼ºé™·ï¼Œä¸èƒ½æ»¡è¶³å®ç°çœŸå®å¦†å®¹æ•ˆæœçš„åŸºæœ¬è¦æ±‚ã€‚</li>
<li>AvatarMakeupæ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä»å•ä¸€å‚è€ƒç…§ç‰‡è½¬ç§»å¦†å®¹æ¨¡å¼ï¼Œå¼¥è¡¥äº†ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨ä»ç²—åˆ°ç»†çš„ç­–ç•¥ï¼Œå…ˆä¿æŒå¤–è§‚å’Œèº«ä»½çš„è¿ç»­æ€§ï¼Œå†ç»†åŒ–ç»†èŠ‚ï¼Œç¡®ä¿å¦†å®¹çš„çœŸå®æ€§å’Œç²¾ç»†åº¦ã€‚</li>
<li>ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¦†å®¹å›¾åƒä½œä¸ºç›‘ç£ï¼Œå¹¶å€ŸåŠ©Coherent Duplicationæ–¹æ³•æ¥ç¡®ä¿åŠ¨æ€å’Œå¤šè§†è§’ä¸‹çš„å¦†å®¹ä¸€è‡´æ€§ã€‚</li>
<li>é’ˆå¯¹ç”Ÿæˆçš„å¦†å®¹å›¾åƒåœ¨ä¸åŒè§†è§’å’Œè¡¨æƒ…ä¸‹çš„ä¸ä¸€è‡´æ€§ï¼Œæå‡ºäº†ä¼˜åŒ–å…¨å±€UVåœ°å›¾çš„Coherent Duplicationæ–¹æ³•ã€‚</li>
<li>å¼•å…¥ç»†åŒ–æ¨¡å—æ¥è¿›ä¸€æ­¥æé«˜å¦†å®¹è´¨é‡ï¼Œå®ç°æ›´è‡ªç„¶ã€çœŸå®çš„è™šæ‹Ÿè§’è‰²åŒ–å¦†æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc87e0924f6be6ebe0fd48bebca87ff2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7aa1481255c0157df1d6ec2b1ba125f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66fda79a028a957be39888e65417c685.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e8a6e4afb0f824163399f06eff68027c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8163a2160dd5abd48450c8e3796778c3.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Heeding-the-Inner-Voice-Aligning-ControlNet-Training-via-Intermediate-Features-Feedback"><a href="#Heeding-the-Inner-Voice-Aligning-ControlNet-Training-via-Intermediate-Features-Feedback" class="headerlink" title="Heeding the Inner Voice: Aligning ControlNet Training via Intermediate   Features Feedback"></a>Heeding the Inner Voice: Aligning ControlNet Training via Intermediate   Features Feedback</h2><p><strong>Authors:Nina Konovalova, Maxim Nikolaev, Andrey Kuznetsov, Aibek Alanov</strong></p>
<p>Despite significant progress in text-to-image diffusion models, achieving precise spatial control over generated outputs remains challenging. ControlNet addresses this by introducing an auxiliary conditioning module, while ControlNet++ further refines alignment through a cycle consistency loss applied only to the final denoising steps. However, this approach neglects intermediate generation stages, limiting its effectiveness. We propose InnerControl, a training strategy that enforces spatial consistency across all diffusion steps. Our method trains lightweight convolutional probes to reconstruct input control signals (e.g., edges, depth) from intermediate UNet features at every denoising step. These probes efficiently extract signals even from highly noisy latents, enabling pseudo ground truth controls for training. By minimizing the discrepancy between predicted and target conditions throughout the entire diffusion process, our alignment loss improves both control fidelity and generation quality. Combined with established techniques like ControlNet++, InnerControl achieves state-of-the-art performance across diverse conditioning methods (e.g., edges, depth). </p>
<blockquote>
<p>å°½ç®¡æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ç°å¯¹ç”Ÿæˆè¾“å‡ºçš„ç²¾ç¡®ç©ºé—´æ§åˆ¶ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ControlNeté€šè¿‡å¼•å…¥è¾…åŠ©æ¡ä»¶æ¨¡å—æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè€ŒControlNet++åˆ™é€šè¿‡ä»…åº”ç”¨äºæœ€ç»ˆå»å™ªæ­¥éª¤çš„å¾ªç¯ä¸€è‡´æ€§æŸå¤±æ¥è¿›ä¸€æ­¥æ”¹è¿›å¯¹é½ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¿½ç•¥äº†ä¸­é—´ç”Ÿæˆé˜¶æ®µï¼Œé™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æå‡ºäº†InnerControlï¼Œè¿™æ˜¯ä¸€ç§è®­ç»ƒç­–ç•¥ï¼Œå¼ºåˆ¶æ‰€æœ‰æ‰©æ•£æ­¥éª¤ä¸­çš„ç©ºé—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•è®­ç»ƒè½»é‡çº§çš„å·ç§¯æ¢é’ˆï¼Œä»æ¯ä¸€æ­¥å»å™ªè¿‡ç¨‹ä¸­çš„ä¸­é—´UNetç‰¹å¾é‡å»ºè¾“å…¥æ§åˆ¶ä¿¡å·ï¼ˆä¾‹å¦‚ï¼Œè¾¹ç¼˜ã€æ·±åº¦ï¼‰ã€‚è¿™äº›æ¢é’ˆå³ä½¿ä»é«˜åº¦å˜ˆæ‚çš„æ½œåœ¨ç‰¹å¾ä¸­ä¹Ÿèƒ½æœ‰æ•ˆåœ°æå–ä¿¡å·ï¼Œä¸ºè®­ç»ƒæä¾›ä¼ªçœŸå®æ§åˆ¶ã€‚é€šè¿‡æœ€å°åŒ–æ•´ä¸ªæ‰©æ•£è¿‡ç¨‹ä¸­é¢„æµ‹æ¡ä»¶ä¸ç›®æ ‡æ¡ä»¶ä¹‹é—´çš„å·®å¼‚ï¼Œæˆ‘ä»¬çš„å¯¹é½æŸå¤±æé«˜äº†æ§åˆ¶ä¿çœŸåº¦å’Œç”Ÿæˆè´¨é‡ã€‚ç»“åˆControlNet++ç­‰ç°æœ‰æŠ€æœ¯ï¼ŒInnerControlåœ¨ä¸åŒæ¡ä»¶æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œè¾¹ç¼˜ã€æ·±åº¦ï¼‰ä¸‹å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02321v1">PDF</a> code available at <a target="_blank" rel="noopener" href="https://github.com/ControlGenAI/InnerControl">https://github.com/ControlGenAI/InnerControl</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†InnerControlï¼Œä¸€ç§é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç©ºé—´æ§åˆ¶è®­ç»ƒç­–ç•¥ã€‚é€šè¿‡è®­ç»ƒè½»é‡çº§å·ç§¯æ¢é’ˆä»æ¯ä¸ªå»å™ªæ­¥éª¤çš„ä¸­é—´UNetç‰¹å¾é‡å»ºè¾“å…¥æ§åˆ¶ä¿¡å·ï¼Œå¦‚è¾¹ç¼˜å’Œæ·±åº¦ï¼Œä»¥åœ¨æ•´ä¸ªæ‰©æ•£è¿‡ç¨‹ä¸­æé«˜æ§åˆ¶å’Œç”Ÿæˆè´¨é‡ã€‚ç»“åˆControlNet++ç­‰æŠ€æœ¯ï¼ŒInnerControlå®ç°äº†åœ¨å„ç§æ¡ä»¶ä¸‹çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ§åˆ¶æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºç‰©çš„ç©ºé—´ä½ç½®æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>ControlNeté€šè¿‡å¼•å…¥è¾…åŠ©æ¡ä»¶æ¨¡å—æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè€ŒControlNet++åˆ™é€šè¿‡ä»…åœ¨æœ€ç»ˆå»å™ªæ­¥éª¤ä¸­åº”ç”¨å¾ªç¯ä¸€è‡´æ€§æŸå¤±æ¥è¿›ä¸€æ­¥ä¼˜åŒ–å¯¹é½ã€‚</li>
<li>ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¿½ç•¥äº†ä¸­é—´ç”Ÿæˆé˜¶æ®µï¼Œé™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>InnerControlæ˜¯ä¸€ç§æ–°çš„è®­ç»ƒç­–ç•¥ï¼Œå®ƒé€šè¿‡å¼ºåˆ¶æ‰€æœ‰æ‰©æ•£æ­¥éª¤çš„ç©ºé—´ä¸€è‡´æ€§æ¥æé«˜æ§åˆ¶å’Œç”Ÿæˆè´¨é‡ã€‚</li>
<li>InnerControlä½¿ç”¨è½»é‡çº§å·ç§¯æ¢é’ˆä»æ¯ä¸ªå»å™ªæ­¥éª¤çš„ä¸­é—´UNetç‰¹å¾é‡å»ºè¾“å…¥æ§åˆ¶ä¿¡å·ï¼ˆå¦‚è¾¹ç¼˜å’Œæ·±åº¦ï¼‰ã€‚</li>
<li>è¿™äº›æ¢é’ˆèƒ½å¤Ÿä»é«˜åº¦å™ªå£°çš„æ½œåœ¨ç‰¹å¾ä¸­æå–ä¿¡å·ï¼Œä¸ºè®­ç»ƒæä¾›ä¼ªçœŸå®æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02321">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-21341642793c62c9f1b781af7d8f2471.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0640cf7fc39787410a02d0671c920936.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e29832df27fbb23f0dd887468fea94c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14f4351a8a86f8077e714d4e2ef1e276.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DreamComposer-Empowering-Diffusion-Models-with-Multi-View-Conditions-for-3D-Content-Generation"><a href="#DreamComposer-Empowering-Diffusion-Models-with-Multi-View-Conditions-for-3D-Content-Generation" class="headerlink" title="DreamComposer++: Empowering Diffusion Models with Multi-View Conditions   for 3D Content Generation"></a>DreamComposer++: Empowering Diffusion Models with Multi-View Conditions   for 3D Content Generation</h2><p><strong>Authors:Yunhan Yang, Shuo Chen, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Edmund Y. Lam, Hengshuang Zhao, Tong He, Xihui Liu</strong></p>
<p>Recent advancements in leveraging pre-trained 2D diffusion models achieve the generation of high-quality novel views from a single in-the-wild image. However, existing works face challenges in producing controllable novel views due to the lack of information from multiple views. In this paper, we present DreamComposer++, a flexible and scalable framework designed to improve current view-aware diffusion models by incorporating multi-view conditions. Specifically, DreamComposer++ utilizes a view-aware 3D lifting module to extract 3D representations of an object from various views. These representations are then aggregated and rendered into the latent features of target view through the multi-view feature fusion module. Finally, the obtained features of target view are integrated into pre-trained image or video diffusion models for novel view synthesis. Experimental results demonstrate that DreamComposer++ seamlessly integrates with cutting-edge view-aware diffusion models and enhances their abilities to generate controllable novel views from multi-view conditions. This advancement facilitates controllable 3D object reconstruction and enables a wide range of applications. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„2Dæ‰©æ•£æ¨¡å‹çš„è¿›å±•å®ç°äº†ä»å•ä¸€é‡å¤–å›¾åƒç”Ÿæˆé«˜è´¨é‡æ–°é¢–è§†è§’çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰å·¥ä½œç”±äºç¼ºå°‘å¤šè§†è§’ä¿¡æ¯ï¼Œé¢ä¸´ç€ç”Ÿæˆå¯æ§æ–°é¢–è§†è§’çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DreamComposer++ï¼Œè¿™æ˜¯ä¸€ä¸ªçµæ´»ä¸”å¯æ‰©å±•çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥å¤šè§†è§’æ¡ä»¶æ¥æ”¹å–„å½“å‰çš„è§†è§’æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼ŒDreamComposer++åˆ©ç”¨è§†è§’æ„ŸçŸ¥çš„3Dæå‡æ¨¡å—ä»å„ç§è§†è§’æå–å¯¹è±¡çš„3Dè¡¨ç¤ºã€‚ç„¶åï¼Œè¿™äº›è¡¨ç¤ºè¢«èšåˆå¹¶é€šè¿‡å¤šè§†è§’ç‰¹å¾èåˆæ¨¡å—å‘ˆç°ä¸ºç›®æ ‡è§†è§’çš„æ½œåœ¨ç‰¹å¾ã€‚æœ€åï¼Œå°†ç›®æ ‡è§†è§’çš„ç‰¹å¾é›†æˆåˆ°é¢„è®­ç»ƒçš„å›¾åƒæˆ–è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ï¼Œç”¨äºåˆæˆæ–°é¢–è§†è§’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDreamComposer++æ— ç¼é›†æˆäºå‰æ²¿çš„è§†è§’æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼Œå¢å¼ºäº†å…¶ä»å¤šè§†è§’æ¡ä»¶ç”Ÿæˆå¯æ§æ–°é¢–è§†è§’çš„èƒ½åŠ›ã€‚è¿™ä¸€è¿›å±•ä¿ƒè¿›äº†å¯æ§çš„3Då¯¹è±¡é‡å»ºï¼Œå¹¶å¯ç”¨äº†å¹¿æ³›çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02299v1">PDF</a> Accepted by TPAMI, extension of CVPR 2024 paper DreamComposer</p>
<p><strong>Summary</strong><br>æœ¬æ–‡ä»‹ç»äº†DreamComposer++æ¡†æ¶ï¼Œå®ƒé€šè¿‡å¼•å…¥å¤šè§†å›¾æ¡ä»¶ï¼Œæ”¹è¿›äº†å½“å‰çš„è§†å›¾æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼Œæé«˜äº†ä»å•ä¸€å›¾åƒç”Ÿæˆé«˜è´¨é‡æ–°é¢–è§†å›¾çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è§†å›¾æ„ŸçŸ¥çš„3Dæå‡æ¨¡å—ä»å„ä¸ªè§†è§’æå–å¯¹è±¡çš„3Dè¡¨ç¤ºï¼Œç„¶åé€šè¿‡å¤šè§†å›¾ç‰¹å¾èåˆæ¨¡å—è¿›è¡Œèšåˆå’Œæ¸²æŸ“ï¼Œæœ€åé›†æˆåˆ°é¢„è®­ç»ƒçš„å›¾åƒæˆ–è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ï¼Œç”¨äºåˆæˆæ–°é¢–è§†å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDreamComposer++æ— ç¼é›†æˆäºå‰æ²¿çš„è§†å›¾æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼Œå¢å¼ºäº†å…¶åœ¨å¤šè§†å›¾æ¡ä»¶ä¸‹çš„å¯æ§æ–°é¢–è§†å›¾ç”Ÿæˆèƒ½åŠ›ï¼Œä¿ƒè¿›äº†å¯æ§çš„3Då¯¹è±¡é‡å»ºï¼Œå¹¶å¯ç”¨äº†å¹¿æ³›çš„åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DreamComposer++æ˜¯ä¸€ä¸ªçµæ´»ä¸”å¯æ‰©å±•çš„æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹è¿›ç°æœ‰çš„è§†å›¾æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥å¤šè§†å›¾æ¡ä»¶ï¼Œæé«˜äº†ä»å•ä¸€å›¾åƒç”Ÿæˆé«˜è´¨é‡æ–°é¢–è§†å›¾çš„èƒ½åŠ›ã€‚</li>
<li>DreamComposer++åˆ©ç”¨è§†å›¾æ„ŸçŸ¥çš„3Dæå‡æ¨¡å—æå–å¯¹è±¡çš„3Dè¡¨ç¤ºã€‚</li>
<li>å¤šè§†å›¾ç‰¹å¾èåˆæ¨¡å—ç”¨äºèšåˆå’Œæ¸²æŸ“ä»ä¸åŒè§†è§’æå–çš„ç‰¹å¾ã€‚</li>
<li>é›†æˆåˆ°é¢„è®­ç»ƒçš„å›¾åƒæˆ–è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ï¼Œç”¨äºåˆæˆæ–°é¢–è§†å›¾ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒDreamComposer++èƒ½å¤Ÿæ— ç¼é›†æˆäºå‰æ²¿çš„è§†å›¾æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼Œå¹¶å¢å¼ºå…¶æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-43ea709f3af0bff9fcd955725c42e2fe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b4c7ba89429d425b64439a7204c1f330.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a46a9f0df083c4abd760e8ca735413cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-25c0a0515c61c6bb19e8ee3f5cdb37c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e2fbd3b7e34ab97889fc793f373f7a7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FreeMorph-Tuning-Free-Generalized-Image-Morphing-with-Diffusion-Model"><a href="#FreeMorph-Tuning-Free-Generalized-Image-Morphing-with-Diffusion-Model" class="headerlink" title="FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model"></a>FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model</h2><p><strong>Authors:Yukang Cao, Chenyang Si, Jinghao Wang, Ziwei Liu</strong></p>
<p>We present FreeMorph, the first tuning-free method for image morphing that accommodates inputs with different semantics or layouts. Unlike existing methods that rely on finetuning pre-trained diffusion models and are limited by time constraints and semantic&#x2F;layout discrepancies, FreeMorph delivers high-fidelity image morphing without requiring per-instance training. Despite their efficiency and potential, tuning-free methods face challenges in maintaining high-quality results due to the non-linear nature of the multi-step denoising process and biases inherited from the pre-trained diffusion model. In this paper, we introduce FreeMorph to address these challenges by integrating two key innovations. 1) We first propose a guidance-aware spherical interpolation design that incorporates explicit guidance from the input images by modifying the self-attention modules, thereby addressing identity loss and ensuring directional transitions throughout the generated sequence. 2) We further introduce a step-oriented variation trend that blends self-attention modules derived from each input image to achieve controlled and consistent transitions that respect both inputs. Our extensive evaluations demonstrate that FreeMorph outperforms existing methods, being 10x ~ 50x faster and establishing a new state-of-the-art for image morphing. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†FreeMorphï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è°ƒæ•´çš„å›¾ç‰‡æ¸å˜æ–¹æ³•ï¼Œèƒ½å¤Ÿé€‚åº”å…·æœ‰ä¸åŒè¯­ä¹‰æˆ–å¸ƒå±€çš„å›¾åƒè¾“å…¥ã€‚ä¸ç°æœ‰çš„ä¾èµ–äºå¾®è°ƒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ä¸åŒï¼Œè¿™äº›æ–¹æ³•å—åˆ°æ—¶é—´é™åˆ¶å’Œè¯­ä¹‰&#x2F;å¸ƒå±€å·®å¼‚çš„é™åˆ¶ï¼ŒFreeMorphæ— éœ€å¯¹æ¯ä¸ªå®ä¾‹è¿›è¡Œè®­ç»ƒå³å¯å®ç°é«˜ä¿çœŸåº¦çš„å›¾åƒæ¸å˜ã€‚å°½ç®¡æ— éœ€è°ƒæ•´çš„æ–¹æ³•å…·æœ‰é«˜æ•ˆæ€§å’Œæ½œåŠ›ï¼Œä½†ç”±äºå¤šæ­¥å»å™ªè¿‡ç¨‹çš„éçº¿æ€§æ€§è´¨å’Œä»é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç»§æ‰¿çš„åè§ï¼Œå®ƒä»¬åœ¨ç»´æŒé«˜è´¨é‡ç»“æœæ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥FreeMorphæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œé€šè¿‡æ•´åˆä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æŒ‡å¯¼æ„ŸçŸ¥çƒå½¢æ’å€¼è®¾è®¡ï¼Œé€šè¿‡ä¿®æ”¹è‡ªæ³¨æ„åŠ›æ¨¡å—æ¥èå…¥è¾“å…¥å›¾åƒçš„æ˜ç¡®æŒ‡å¯¼ï¼Œä»è€Œè§£å†³èº«ä»½ä¸¢å¤±é—®é¢˜ï¼Œç¡®ä¿ç”Ÿæˆåºåˆ—ä¸­çš„å®šå‘è¿‡æ¸¡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§é¢å‘æ­¥éª¤çš„å˜å¼‚è¶‹åŠ¿ï¼Œé€šè¿‡èåˆæ¥è‡ªæ¯ä¸ªè¾“å…¥å›¾åƒçš„è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œå®ç°å—æ§ä¸”ä¸€è‡´çš„è¿‡æ¸¡ï¼ŒåŒæ—¶å°Šé‡ä¸¤ä¸ªè¾“å…¥ã€‚æˆ‘ä»¬çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒFreeMorphä¼˜äºç°æœ‰æ–¹æ³•ï¼Œé€Ÿåº¦æé«˜äº†10å€è‡³50å€ï¼Œä¸ºå›¾åƒæ¸å˜å»ºç«‹äº†æ–°çš„æŠ€æœ¯æ ‡æ†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01953v1">PDF</a> ICCV 2025. Project page: <a target="_blank" rel="noopener" href="https://yukangcao.github.io/FreeMorph/">https://yukangcao.github.io/FreeMorph/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†FreeMorphï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è°ƒæ•´çš„å›¾åƒèåˆæ–¹æ³•ï¼Œå¯é€‚åº”å…·æœ‰ä¸åŒè¯­ä¹‰æˆ–å¸ƒå±€çš„å›¾åƒè¾“å…¥ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒFreeMorphæ— éœ€å¯¹é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä¸”ä¸å—æ—¶é—´é™åˆ¶å’Œè¯­ä¹‰&#x2F;å¸ƒå±€å·®å¼‚çš„é™åˆ¶ï¼Œå¯åœ¨æ— éœ€æ¯æ¬¡å®ä¾‹è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°é«˜ä¿çœŸå›¾åƒèåˆã€‚ä¸ºäº†è§£å†³æŒ‘æˆ˜å¹¶ç»´æŒé«˜è´¨é‡çš„ç»“æœï¼Œè¯¥è®ºæ–‡æå‡ºäº†FreeMorphçš„ä¸¤é¡¹åˆ›æ–°æ–¹æ³•ã€‚é¦–å…ˆæå‡ºäº†ä¸€ç§å¸¦æœ‰å¼•å¯¼çƒé¢çš„æ’å€¼è®¾è®¡ï¼Œè¯¥è®¾è®¡é€šè¿‡ä¿®æ”¹è‡ªæ³¨æ„åŠ›æ¨¡å—èå…¥äº†æ¥è‡ªè¾“å…¥å›¾åƒçš„æ˜¾å¼å¼•å¯¼ï¼Œä»è€Œè§£å†³èº«ä»½æŸå¤±å¹¶ç¡®ä¿ç”Ÿæˆçš„åºåˆ—ä¸­å§‹ç»ˆä¿æŒæ–¹å‘è¿‡æ¸¡ã€‚å…¶æ¬¡ï¼Œå¼•å…¥äº†åŸºäºæ­¥éª¤çš„å˜åŒ–è¶‹åŠ¿ï¼Œé€šè¿‡å°†æ¯ä¸ªè¾“å…¥å›¾åƒäº§ç”Ÿçš„è‡ªæ³¨æ„åŠ›æ¨¡å—æ··åˆï¼Œå®ç°æ§åˆ¶å¹¶ä¿æŒä¸€è‡´è¿‡æ¸¡ï¼ŒåŒæ—¶å°Šé‡ä¸¤ä¸ªè¾“å…¥å›¾åƒã€‚è¯„ä»·ç»“æœæ˜¾ç¤ºï¼ŒFreeMorphç›¸è¾ƒäºç°æœ‰æ–¹æ³•æ€§èƒ½æ›´ä½³ï¼Œé€Ÿåº¦æå‡è¾¾10å€è‡³50å€ï¼Œæˆä¸ºæ–°çš„å›¾åƒèåˆæŠ€æœ¯å‰æ²¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FreeMorphæ˜¯é¦–ä¸ªæ— éœ€è°ƒæ•´å³å¯å®ç°å›¾åƒèåˆçš„æ–¹æ³•ï¼Œé€‚ç”¨äºä¸åŒè¯­ä¹‰æˆ–å¸ƒå±€çš„è¾“å…¥å›¾åƒã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒFreeMorphæ— éœ€å¯¹é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå…·æœ‰é«˜æ•ˆæ€§å’Œæ½œåŠ›ã€‚</li>
<li>FreeMorphè§£å†³äº†èº«ä»½æŸå¤±é—®é¢˜ï¼Œé€šè¿‡å¼•å¯¼çƒé¢æ’å€¼è®¾è®¡ç¡®ä¿ç”Ÿæˆçš„åºåˆ—ä¸­æ–¹å‘è¿‡æ¸¡çš„è‡ªç„¶æ€§ã€‚</li>
<li>é€šè¿‡å¼•å…¥åŸºäºæ­¥éª¤çš„å˜åŒ–è¶‹åŠ¿ï¼Œå®ç°äº†å¯¹è¾“å…¥å›¾åƒçš„è‡ªæ³¨æ„åŠ›æ¨¡å—çš„æ··åˆï¼Œå®ç°äº†æ§åˆ¶å¹¶ä¿æŒä¸€è‡´è¿‡æ¸¡ã€‚</li>
<li>FreeMorphåœ¨ä¿æŒé«˜è´¨é‡ç»“æœçš„åŒæ—¶ï¼Œè§£å†³äº†å¤šæ­¥å»å™ªè¿‡ç¨‹çš„éçº¿æ€§æ€§è´¨å’Œé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å›ºæœ‰åè§æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li>
<li>è¯„ä¼°è¡¨æ˜ï¼ŒFreeMorphçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œé€Ÿåº¦æå‡æ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01953">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6397ca490c618bb266bd576096b62eba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b95da3dd4fcc057967c3c937beae951.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-276d774f3d342f85076c77842b12fb10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97494490a58721717f83d04475c47875.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Reasoning-to-Edit-Hypothetical-Instruction-Based-Image-Editing-with-Visual-Reasoning"><a href="#Reasoning-to-Edit-Hypothetical-Instruction-Based-Image-Editing-with-Visual-Reasoning" class="headerlink" title="Reasoning to Edit: Hypothetical Instruction-Based Image Editing with   Visual Reasoning"></a>Reasoning to Edit: Hypothetical Instruction-Based Image Editing with   Visual Reasoning</h2><p><strong>Authors:Qingdong He, Xueqin Chen, Chaoyi Wang, Yanjie Pan, Xiaobin Hu, Zhenye Gan, Yabiao Wang, Chengjie Wang, Xiangtai Li, Jiangning Zhang</strong></p>
<p>Instruction-based image editing (IIE) has advanced rapidly with the success of diffusion models. However, existing efforts primarily focus on simple and explicit instructions to execute editing operations such as adding, deleting, moving, or swapping objects. They struggle to handle more complex implicit hypothetical instructions that require deeper reasoning to infer plausible visual changes and user intent. Additionally, current datasets provide limited support for training and evaluating reasoning-aware editing capabilities. Architecturally, these methods also lack mechanisms for fine-grained detail extraction that support such reasoning. To address these limitations, we propose Reason50K, a large-scale dataset specifically curated for training and evaluating hypothetical instruction reasoning image editing, along with ReasonBrain, a novel framework designed to reason over and execute implicit hypothetical instructions across diverse scenarios. Reason50K includes over 50K samples spanning four key reasoning scenarios: Physical, Temporal, Causal, and Story reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs) for editing guidance generation and a diffusion model for image synthesis, incorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture detailed visual and textual semantics essential for supporting instruction reasoning. To mitigate the semantic loss, we further introduce a Cross-Modal Enhancer (CME) that enables rich interactions between the fine-grained cues and MLLM-derived features. Extensive experiments demonstrate that ReasonBrain consistently outperforms state-of-the-art baselines on reasoning scenarios while exhibiting strong zero-shot generalization to conventional IIE tasks. Our dataset and code will be released publicly. </p>
<blockquote>
<p>åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ï¼ˆIIEï¼‰éšç€æ‰©æ•£æ¨¡å‹çš„æˆåŠŸè€Œè¿…é€Ÿå‘å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŠªåŠ›ä¸»è¦é›†ä¸­åœ¨æ‰§è¡Œæ·»åŠ ã€åˆ é™¤ã€ç§»åŠ¨æˆ–äº¤æ¢å¯¹è±¡ç­‰ç®€å•æ˜ç¡®çš„ç¼–è¾‘æŒ‡ä»¤ä¸Šã€‚ä»–ä»¬éš¾ä»¥å¤„ç†éœ€è¦æ›´æ·±åº¦æ¨ç†æ¥æ¨æ–­å¯èƒ½è§†è§‰å˜åŒ–å’Œç”¨æˆ·æ„å›¾çš„æ›´å¤æ‚éšå«å‡è®¾æŒ‡ä»¤ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ•°æ®é›†åœ¨æ”¯æŒè®­ç»ƒè¯„ä¼°æ¨ç†æ„ŸçŸ¥ç¼–è¾‘èƒ½åŠ›æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä»æ¶æ„ä¸Šçœ‹ï¼Œè¿™äº›æ–¹æ³•è¿˜ç¼ºä¹æ”¯æŒæ­¤ç±»æ¨ç†çš„ç²¾ç»†ç»†èŠ‚æå–æœºåˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Reason50Kï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºè®­ç»ƒå’Œè¯„ä¼°å‡è®¾æŒ‡ä»¤æ¨ç†å›¾åƒç¼–è¾‘çš„å¤§å‹æ•°æ®é›†ï¼Œä»¥åŠReasonBrainï¼Œä¸€ä¸ªä¸ºæ‰§è¡Œå„ç§åœºæ™¯ä¸­çš„éšå«å‡è®¾æŒ‡ä»¤è€Œè®¾è®¡çš„æ–°å‹æ¡†æ¶ã€‚Reason50KåŒ…å«è¶…è¿‡5ä¸‡æ ·æœ¬ï¼Œæ¶µç›–å››ç§å…³é”®æ¨ç†åœºæ™¯ï¼šç‰©ç†æ¨ç†ã€æ—¶é—´æ¨ç†ã€å› æœæ¨ç†å’Œæ•…äº‹æ¨ç†ã€‚ReasonBrainåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œç¼–è¾‘æŒ‡å¯¼ç”Ÿæˆå’Œæ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒåˆæˆï¼Œå¹¶èå…¥ç²¾ç»†æ¨ç†çº¿ç´¢æå–ï¼ˆFRCEï¼‰æ¨¡å—ï¼Œä»¥æ•æ‰æ”¯æŒæŒ‡ä»¤æ¨ç†çš„è¯¦ç»†è§†è§‰å’Œæ–‡æœ¬è¯­ä¹‰ã€‚ä¸ºäº†å‡å°‘è¯­ä¹‰æŸå¤±ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†è·¨æ¨¡æ€å¢å¼ºå™¨ï¼ˆCMEï¼‰ï¼Œå®ƒå®ç°äº†ç²¾ç»†çº¿ç´¢å’ŒMLLMæ´¾ç”Ÿç‰¹å¾ä¹‹é—´çš„ä¸°å¯Œäº¤äº’ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒReasonBrainåœ¨æ¨ç†åœºæ™¯ä¸Šå§‹ç»ˆä¼˜äºæœ€æ–°åŸºçº¿ï¼ŒåŒæ—¶åœ¨å¸¸è§„IIEä»»åŠ¡ä¸Šå±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01908v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ï¼ˆIIEï¼‰å€ŸåŠ©æ‰©æ•£æ¨¡å‹å–å¾—äº†å¿«é€Ÿè¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰åŠªåŠ›ä¸»è¦é›†ä¸­åœ¨æ‰§è¡Œæ·»åŠ ã€åˆ é™¤ã€ç§»åŠ¨æˆ–æ›¿æ¢ç‰©ä½“ç­‰ç®€å•æ˜ç¡®çš„ç¼–è¾‘æŒ‡ä»¤ä¸Šï¼Œéš¾ä»¥å¤„ç†éœ€è¦æ›´æ·±åº¦æ¨ç†çš„å¤æ‚éšæ€§å‡è®¾æŒ‡ä»¤ï¼Œä»¥æ¨æ–­å‡ºå¯è¡Œçš„è§†è§‰å˜åŒ–å’Œç”¨æˆ·æ„å›¾ã€‚æ­¤å¤–ï¼Œå½“å‰æ•°æ®é›†å¯¹è®­ç»ƒè¯„ä¼°æ¨ç†æ„ŸçŸ¥ç¼–è¾‘èƒ½åŠ›æ”¯æŒæœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Reason50Kæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä¸“ä¸ºå‡è®¾æŒ‡ä»¤æ¨ç†å›¾åƒç¼–è¾‘çš„è®­ç»ƒå’Œè¯„ä¼°è€Œç­–åˆ’ï¼Œä»¥åŠReasonBrainæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨æ‰§è¡Œè·¨ä¸åŒåœºæ™¯çš„éšæ€§å‡è®¾æŒ‡ä»¤ã€‚ReasonBrainåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç¼–è¾‘æŒ‡å¯¼ç”Ÿæˆå’Œæ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒåˆæˆï¼Œå¹¶å¼•å…¥ç²¾ç»†æ¨ç†çº¿ç´¢æå–æ¨¡å—æ¥æ•æ‰è¯¦ç»†çš„è§†è§‰å’Œæ–‡æœ¬è¯­ä¹‰ã€‚ä¸ºäº†å‡è½»è¯­ä¹‰æŸå¤±ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†è·¨æ¨¡æ€å¢å¼ºå™¨ï¼Œä»¥ä¸°å¯Œç²¾ç»†çº¿ç´¢ä¸MLLMè¡ç”Ÿç‰¹å¾ä¹‹é—´çš„äº¤äº’ã€‚å®éªŒè¡¨æ˜ï¼ŒReasonBrainåœ¨æ¨ç†åœºæ™¯ä¸Šå§‹ç»ˆä¼˜äºæœ€æ–°åŸºçº¿æŠ€æœ¯ï¼Œå¹¶åœ¨å¸¸è§„IIEä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç°æœ‰å›¾åƒç¼–è¾‘æŠ€æœ¯ä¸»è¦å¤„ç†ç®€å•æ˜ç¡®çš„ç¼–è¾‘æŒ‡ä»¤ï¼Œéš¾ä»¥å¤„ç†å¤æ‚çš„éšæ€§å‡è®¾æŒ‡ä»¤ã€‚</li>
<li>ç¼ºä¹é’ˆå¯¹å‡è®¾æŒ‡ä»¤æ¨ç†å›¾åƒç¼–è¾‘çš„è®­ç»ƒå’Œè¯„ä¼°æ•°æ®é›†ã€‚</li>
<li>æå‡ºäº†Reason50Kæ•°æ®é›†ï¼Œä¸“ä¸ºå‡è®¾æŒ‡ä»¤æ¨ç†å›¾åƒç¼–è¾‘çš„è®­ç»ƒå’Œè¯„ä¼°è®¾è®¡ã€‚</li>
<li>å¼•å…¥äº†ReasonBrainæ¡†æ¶ï¼Œç»“åˆäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ï¼Œä»¥æ‰§è¡Œå¤æ‚çš„éšæ€§å‡è®¾æŒ‡ä»¤ã€‚</li>
<li>ReasonBrainåŒ…å«ç²¾ç»†æ¨ç†çº¿ç´¢æå–æ¨¡å—ï¼Œç”¨äºæ•æ‰è¯¦ç»†çš„è§†è§‰å’Œæ–‡æœ¬è¯­ä¹‰ã€‚</li>
<li>è·¨æ¨¡æ€å¢å¼ºå™¨çš„å¼•å…¥å¢å¼ºäº†ç²¾ç»†çº¿ç´¢ä¸è¯­è¨€æ¨¡å‹ç‰¹å¾ä¹‹é—´çš„äº¤äº’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01908">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-02ecc7f203b5dcd1da51e1f4896ead5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03c5afdb1b2171a1d4793ff58685c324.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a174114b9bab6f90864463b72687a5b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae1aba70778f478c908b0454c5407f2c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Vision-Aided-ISAC-in-Low-Altitude-Economy-Networks-via-De-Diffused-Visual-Priors"><a href="#Vision-Aided-ISAC-in-Low-Altitude-Economy-Networks-via-De-Diffused-Visual-Priors" class="headerlink" title="Vision-Aided ISAC in Low-Altitude Economy Networks via De-Diffused   Visual Priors"></a>Vision-Aided ISAC in Low-Altitude Economy Networks via De-Diffused   Visual Priors</h2><p><strong>Authors:Yulan Gao, Ziqiang Ye, Zhonghao Lyu, Ming Xiao, Yue Xiao, Ping Yang, Agata Manolova</strong></p>
<p>Emerging low-altitude economy networks (LAENets) require agile and privacy-preserving resource control under dynamic agent mobility and limited infrastructure support. To meet these challenges, we propose a vision-aided integrated sensing and communication (ISAC) framework for UAV-assisted access systems, where onboard masked De-Diffusion models extract compact semantic tokens, including agent type, activity class, and heading orientation, while explicitly suppressing sensitive visual content. These tokens are fused with mmWave radar measurements to construct a semantic risk heatmap reflecting motion density, occlusion, and scene complexity, which guides access technology selection and resource scheduling. We formulate a multi-objective optimization problem to jointly maximize weighted energy and perception efficiency via radio access technology (RAT) assignment, power control, and beamforming, subject to agent-specific QoS constraints. To solve this, we develop De-Diffusion-driven vision-aided risk-aware resource optimization algorithm DeDiff-VARARO, a novel two-stage cross-modal control algorithm: the first stage reconstructs visual scenes from tokens via De-Diffusion model for semantic parsing, while the second stage employs a deep deterministic policy gradient (DDPG)-based policy to adapt RAT selection, power control, and beam assignment based on fused radar-visual states. Simulation results show that DeDiff-VARARO consistently outperforms baselines in reward convergence, link robustness, and semantic fidelity, achieving within $4%$ of the performance of a raw-image upper bound while preserving user privacy and scalability in dense environments. </p>
<blockquote>
<p>æ–°å…´çš„ä½ç©ºç»æµç½‘ç»œï¼ˆLAENetsï¼‰éœ€è¦åœ¨åŠ¨æ€ä»£ç†ç§»åŠ¨å’Œæœ‰é™çš„åŸºç¡€è®¾æ–½æ”¯æŒä¸‹ï¼Œå®ç°æ•æ·ä¸”ä¿æŠ¤éšç§çš„èµ„æºæ§åˆ¶ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä¸ºæ— äººæœºè¾…åŠ©è®¿é—®ç³»ç»Ÿæå‡ºäº†ä¸€ä¸ªåŸºäºè§†è§‰è¾…åŠ©é›†æˆæ„ŸçŸ¥å’Œé€šä¿¡ï¼ˆISACï¼‰çš„æ¡†æ¶ã€‚åœ¨è¯¥æ¡†æ¶ä¸­ï¼Œæœºè½½çš„å»æ‰©æ•£æ¨¡å‹æå–ç´§å‡‘çš„è¯­ä¹‰ä»¤ç‰Œï¼ŒåŒ…æ‹¬ä»£ç†ç±»å‹ã€æ´»åŠ¨ç±»åˆ«å’Œæœå‘ï¼ŒåŒæ—¶æ˜¾å¼æŠ‘åˆ¶æ•æ„Ÿè§†è§‰å†…å®¹ã€‚è¿™äº›ä»¤ç‰Œä¸æ¯«ç±³æ³¢é›·è¾¾æµ‹é‡å€¼ç›¸èåˆï¼Œæ„å»ºåæ˜ è¿åŠ¨å¯†åº¦ã€é®æŒ¡æƒ…å†µå’Œåœºæ™¯å¤æ‚æ€§çš„è¯­ä¹‰é£é™©çƒ­å›¾ï¼Œä»è€ŒæŒ‡å¯¼è®¿é—®æŠ€æœ¯é€‰æ‹©å’Œèµ„æºè°ƒåº¦ã€‚æˆ‘ä»¬åˆ¶å®šäº†ä¸€ä¸ªå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡æ— çº¿ç”µè®¿é—®æŠ€æœ¯ï¼ˆRATï¼‰åˆ†é…ã€åŠŸç‡æ§åˆ¶å’Œæ³¢æŸå½¢æˆï¼Œè”åˆæœ€å¤§åŒ–åŠ æƒèƒ½é‡å’Œæ„ŸçŸ¥æ•ˆç‡ï¼ŒåŒæ—¶å—åˆ°ç‰¹å®šä»£ç†çš„QoS çº¦æŸã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†å»æ‰©æ•£é©±åŠ¨çš„è§†è§‰è¾…åŠ©é£é™©æ„ŸçŸ¥èµ„æºä¼˜åŒ–ç®—æ³• DeDiff-VARAROï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µè·¨æ¨¡æ€æ§åˆ¶ç®—æ³•ï¼šç¬¬ä¸€é˜¶æ®µé€šè¿‡å»æ‰©æ•£æ¨¡å‹ä»ä»¤ç‰Œé‡å»ºè§†è§‰åœºæ™¯è¿›è¡Œè¯­ä¹‰è§£æï¼›ç¬¬äºŒé˜¶æ®µé‡‡ç”¨åŸºäºæ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ï¼ˆDDPGï¼‰çš„ç­–ç•¥ï¼Œæ ¹æ®èåˆçš„é›·è¾¾-è§†è§‰çŠ¶æ€æ¥é€‚åº”RATé€‰æ‹©ã€åŠŸç‡æ§åˆ¶å’Œæ³¢æŸåˆ†é…ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼ŒDeDiff-VARARO åœ¨å¥–åŠ±æ”¶æ•›ã€é“¾è·¯é²æ£’æ€§å’Œè¯­ä¹‰ä¿çœŸåº¦æ–¹é¢å§‹ç»ˆä¼˜äºåŸºå‡†çº¿ï¼Œåœ¨å¯†é›†ç¯å¢ƒä¸­å®ç°äº†å¯¹åŸå§‹å›¾åƒä¸Šé™æ€§èƒ½çš„96%çš„åŒæ—¶ï¼Œä¿æŠ¤äº†ç”¨æˆ·éšç§å’Œå¯æ‰©å±•æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01574v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ä½ç©ºç»æµç½‘ç»œï¼ˆLAENetsï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚åŠ¨æ€ä»£ç†ç§»åŠ¨æ€§ã€æœ‰é™çš„åŸºç¡€è®¾æ–½æ”¯æŒå’Œéšç§ä¿æŠ¤éœ€æ±‚ï¼Œæå‡ºä¸€ç§åŸºäºè§†è§‰è¾…åŠ©çš„é›†æˆæ„ŸçŸ¥å’Œé€šä¿¡ï¼ˆISACï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è½¦è½½å»æ‰©æ•£æ¨¡å‹æå–ç´§å‡‘è¯­ä¹‰ä»¤ç‰Œï¼Œå¹¶ä¸æ¯«ç±³æ³¢é›·è¾¾æµ‹é‡å€¼èåˆï¼Œæ„å»ºåæ˜ è¿åŠ¨å¯†åº¦ã€é®æŒ¡å’Œåœºæ™¯å¤æ‚æ€§çš„è¯­ä¹‰é£é™©çƒ­å›¾ï¼Œä»è€Œå®ç°è®¿é—®æŠ€æœ¯é€‰æ‹©å’Œèµ„æºè°ƒåº¦ã€‚æå‡ºä¸€ç§åŸºäºå¤šç›®æ ‡ä¼˜åŒ–çš„å»æ‰©æ•£é©±åŠ¨è§†è§‰è¾…åŠ©é£é™©æ„ŸçŸ¥èµ„æºä¼˜åŒ–ç®—æ³•ï¼ˆDeDiff-VARAROï¼‰ï¼Œæ¨¡æ‹Ÿç»“æœæ˜¾ç¤ºå…¶åœ¨å¥–åŠ±æ”¶æ•›ã€é“¾è·¯é²æ£’æ€§å’Œè¯­ä¹‰ä¿çœŸåº¦æ–¹é¢å‡ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶åœ¨ä¿æŠ¤ç”¨æˆ·éšç§å’Œå¯†é›†ç¯å¢ƒä¸­çš„å¯æ‰©å±•æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½ç©ºç»æµç½‘ç»œï¼ˆLAENetsï¼‰é¢ä¸´åŠ¨æ€ä»£ç†ç§»åŠ¨æ€§ã€æœ‰é™åŸºç¡€è®¾æ–½æ”¯æŒå’Œéšç§ä¿æŠ¤çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†åŸºäºè§†è§‰è¾…åŠ©çš„é›†æˆæ„ŸçŸ¥å’Œé€šä¿¡ï¼ˆISACï¼‰æ¡†æ¶ï¼Œç”¨äºUAVè¾…åŠ©è®¿é—®ç³»ç»Ÿã€‚</li>
<li>é€šè¿‡è½¦è½½å»æ‰©æ•£æ¨¡å‹æå–ç´§å‡‘è¯­ä¹‰ä»¤ç‰Œï¼ŒåŒ…æ‹¬ä»£ç†ç±»å‹ã€æ´»åŠ¨ç±»åˆ«å’Œæœå‘ç­‰ï¼ŒåŒæ—¶æ˜¾å¼æŠ‘åˆ¶æ•æ„Ÿè§†è§‰å†…å®¹ã€‚</li>
<li>è¯­ä¹‰ä»¤ç‰Œä¸æ¯«ç±³æ³¢é›·è¾¾æµ‹é‡å€¼èåˆï¼Œæ„å»ºåæ˜ è¿åŠ¨å¯†åº¦ã€é®æŒ¡å’Œåœºæ™¯å¤æ‚æ€§çš„è¯­ä¹‰é£é™©çƒ­å›¾ã€‚</li>
<li>æå‡ºä¸€ç§å¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡æ— çº¿ç”µè®¿é—®æŠ€æœ¯ï¼ˆRATï¼‰åˆ†é…ã€åŠŸç‡æ§åˆ¶å’Œæ³¢æŸå½¢æˆæ¥è”åˆæœ€å¤§åŒ–åŠ æƒèƒ½é‡å’Œæ„ŸçŸ¥æ•ˆç‡ï¼ŒåŒæ—¶è€ƒè™‘ä»£ç†ç‰¹å®šçš„QoS çº¦æŸã€‚</li>
<li>å¼€å‘äº†å»æ‰©æ•£é©±åŠ¨çš„è§†è§‰è¾…åŠ©é£é™©æ„ŸçŸ¥èµ„æºä¼˜åŒ–ç®—æ³•ï¼ˆDeDiff-VARAROï¼‰ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè·¨æ¨¡æ€æ§åˆ¶ç®—æ³•å®ç°èµ„æºä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4a08c73fad53f925efb68d990ec7edea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bb13543815212570ffbde373d32cf35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b350a615395b7d438af8be0acd62dd39.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-910ddd550f471c9f25f2e3f4fa8580d6.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="ReFlex-Text-Guided-Editing-of-Real-Images-in-Rectified-Flow-via-Mid-Step-Feature-Extraction-and-Attention-Adaptation"><a href="#ReFlex-Text-Guided-Editing-of-Real-Images-in-Rectified-Flow-via-Mid-Step-Feature-Extraction-and-Attention-Adaptation" class="headerlink" title="ReFlex: Text-Guided Editing of Real Images in Rectified Flow via   Mid-Step Feature Extraction and Attention Adaptation"></a>ReFlex: Text-Guided Editing of Real Images in Rectified Flow via   Mid-Step Feature Extraction and Attention Adaptation</h2><p><strong>Authors:Jimyeong Kim, Jungwon Park, Yeji Song, Nojun Kwak, Wonjong Rhee</strong></p>
<p>Rectified Flow text-to-image models surpass diffusion models in image quality and text alignment, but adapting ReFlow for real-image editing remains challenging. We propose a new real-image editing method for ReFlow by analyzing the intermediate representations of multimodal transformer blocks and identifying three key features. To extract these features from real images with sufficient structural preservation, we leverage mid-step latent, which is inverted only up to the mid-step. We then adapt attention during injection to improve editability and enhance alignment to the target text. Our method is training-free, requires no user-provided mask, and can be applied even without a source prompt. Extensive experiments on two benchmarks with nine baselines demonstrate its superior performance over prior methods, further validated by human evaluations confirming a strong user preference for our approach. </p>
<blockquote>
<p>ä¿®æ­£æµï¼ˆReFlowï¼‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å›¾åƒè´¨é‡å’Œæ–‡æœ¬å¯¹é½è¶…è¶Šäº†æ‰©æ•£æ¨¡å‹ï¼Œä½†åœ¨å®é™…å›¾åƒç¼–è¾‘ä¸­åº”ç”¨ReFlowä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬é’ˆå¯¹ReFlowæå‡ºäº†ä¸€ç§æ–°çš„å®é™…å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œé€šè¿‡åˆ†æå¤šæ¨¡æ€transformerå—çš„ä¸­é—´è¡¨ç¤ºå¹¶è¯†åˆ«ä¸‰ä¸ªå…³é”®ç‰¹å¾ã€‚ä¸ºäº†ä»å®é™…å›¾åƒä¸­æå–è¿™äº›ç‰¹å¾å¹¶ä¿æŒè¶³å¤Ÿçš„ç»“æ„å®Œæ•´æ€§ï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸­æœŸæ½œåœ¨å˜é‡ï¼Œè¯¥å˜é‡åªåè½¬åˆ°ä¸­æœŸã€‚ç„¶åæˆ‘ä»¬åœ¨æ³¨å…¥æ—¶è°ƒæ•´æ³¨æ„åŠ›ä»¥æé«˜å¯ç¼–è¾‘æ€§å’Œæé«˜ä¸ç›®æ ‡æ–‡æœ¬çš„åŒ¹é…åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œæ— éœ€ç”¨æˆ·æä¾›é®ç½©ï¼Œç”šè‡³åœ¨ä¸æä¾›æºæç¤ºçš„æƒ…å†µä¸‹ä¹Ÿå¯ä»¥åº”ç”¨ã€‚åœ¨ä¸¤ä¸ªåŸºå‡†ä¸Šçš„å¤§é‡å®éªŒä¸ä¹ä¸ªåŸºçº¿ç›¸æ¯”ï¼Œè¯æ˜äº†å…¶è¶…è¶Šå…ˆå‰æ–¹æ³•çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œäººç±»è¯„ä¼°è¿›ä¸€æ­¥è¯å®äº†ç”¨æˆ·å¯¹æˆ‘ä»¬çš„æ–¹æ³•æœ‰å¼ºçƒˆçš„åå¥½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01496v1">PDF</a> Published at ICCV 2025. Project page:   <a target="_blank" rel="noopener" href="https://wlaud1001.github.io/ReFlex/">https://wlaud1001.github.io/ReFlex/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºRectified Flowï¼ˆReFlowï¼‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„çœŸå®å›¾åƒç¼–è¾‘æ–¹æ³•ã€‚é€šè¿‡åˆ†æå¤šæ¨¡æ€transformerå—çš„ä¸­é—´è¡¨ç¤ºï¼Œè¯†åˆ«äº†ä¸‰ä¸ªå…³é”®ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨ä¸­æœŸæ½œåœ¨åè½¬æŠ€æœ¯æå–è¿™äº›ç‰¹å¾ï¼Œæ”¹è¿›äº†æ³¨å…¥æ—¶çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜äº†ç¼–è¾‘èƒ½åŠ›å’Œæ–‡æœ¬å¯¹é½åº¦ã€‚è¯¥æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œæ— éœ€ç”¨æˆ·æä¾›é®ç½©ï¼Œå³ä½¿åœ¨æ²¡æœ‰æºæç¤ºçš„æƒ…å†µä¸‹ä¹Ÿå¯åº”ç”¨ã€‚å®éªŒå’Œäººå·¥è¯„ä¼°å‡è¯å®å…¶æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œç”¨æˆ·åå¥½æ˜æ˜¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºåŸºäºReFlowçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„çœŸå®å›¾åƒç¼–è¾‘æ–¹æ³•ã€‚</li>
<li>åˆ†æå¤šæ¨¡æ€transformerå—çš„ä¸­é—´è¡¨ç¤ºï¼Œè¯†åˆ«å…³é”®ç‰¹å¾ã€‚</li>
<li>åˆ©ç”¨ä¸­æœŸæ½œåœ¨åè½¬æŠ€æœ¯æå–å›¾åƒç‰¹å¾ï¼Œä¿è¯è¶³å¤Ÿçš„ç»“æ„ä¿ç•™ã€‚</li>
<li>æ”¹è¿›æ³¨å…¥æ—¶çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜ç¼–è¾‘èƒ½åŠ›å’Œæ–‡æœ¬å¯¹é½åº¦ã€‚</li>
<li>æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œæ— éœ€ç”¨æˆ·æä¾›é®ç½©ï¼Œåº”ç”¨å¹¿æ³›ã€‚</li>
<li>å®éªŒè¯æ˜å…¶æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01496">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1212142b7f3a69e96806bae79a112f4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d75e7005be7f9bbb1c9039b4970b617.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-445a0cf0861e9aa50598c365a70eac41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61b1beb14e4e2ec24e7d623bf144721a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23c8f2f32b6dd218c038e2e23a3db2d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-545877ca284d99e7477594b9518b0c27.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Representation-Entanglement-for-Generation-Training-Diffusion-Transformers-Is-Much-Easier-Than-You-Think"><a href="#Representation-Entanglement-for-Generation-Training-Diffusion-Transformers-Is-Much-Easier-Than-You-Think" class="headerlink" title="Representation Entanglement for Generation:Training Diffusion   Transformers Is Much Easier Than You Think"></a>Representation Entanglement for Generation:Training Diffusion   Transformers Is Much Easier Than You Think</h2><p><strong>Authors:Ge Wu, Shen Zhang, Ruijing Shi, Shanghua Gao, Zhenyuan Chen, Lei Wang, Zhaowei Chen, Hongcheng Gao, Yao Tang, Jian Yang, Ming-Ming Cheng, Xiang Li</strong></p>
<p>REPA and its variants effectively mitigate training challenges in diffusion models by incorporating external visual representations from pretrained models, through alignment between the noisy hidden projections of denoising networks and foundational clean image representations. We argue that the external alignment, which is absent during the entire denoising inference process, falls short of fully harnessing the potential of discriminative representations. In this work, we propose a straightforward method called Representation Entanglement for Generation (REG), which entangles low-level image latents with a single high-level class token from pretrained foundation models for denoising. REG acquires the capability to produce coherent image-class pairs directly from pure noise, substantially improving both generation quality and training efficiency. This is accomplished with negligible additional inference overhead, requiring only one single additional token for denoising (&lt;0.5% increase in FLOPs and latency). The inference process concurrently reconstructs both image latents and their corresponding global semantics, where the acquired semantic knowledge actively guides and enhances the image generation process. On ImageNet 256$\times$256, SiT-XL&#x2F;2 + REG demonstrates remarkable convergence acceleration, achieving $\textbf{63}\times$ and $\textbf{23}\times$ faster training than SiT-XL&#x2F;2 and SiT-XL&#x2F;2 + REPA, respectively. More impressively, SiT-L&#x2F;2 + REG trained for merely 400K iterations outperforms SiT-XL&#x2F;2 + REPA trained for 4M iterations ($\textbf{10}\times$ longer). Code is available at: <a target="_blank" rel="noopener" href="https://github.com/Martinser/REG">https://github.com/Martinser/REG</a>. </p>
<blockquote>
<p>REPAåŠå…¶å˜ä½“é€šè¿‡ç»“åˆé¢„è®­ç»ƒæ¨¡å‹çš„å¤–éƒ¨è§†è§‰è¡¨å¾ï¼Œé€šè¿‡å¯¹å»å™ªç½‘ç»œçš„å™ªå£°éšè—æŠ•å½±å’ŒåŸºæœ¬çš„å¹²å‡€å›¾åƒè¡¨å¾ä¹‹é—´çš„å¯¹é½ï¼Œæœ‰æ•ˆç¼“è§£äº†æ‰©æ•£æ¨¡å‹ä¸­çš„è®­ç»ƒæŒ‘æˆ˜ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œåœ¨æ•´ä¸ªå»å™ªæ¨ç†è¿‡ç¨‹ä¸­ç¼ºå¤±çš„å¤–éƒ¨å¯¹é½æ— æ³•å……åˆ†åˆ©ç”¨åˆ¤åˆ«è¡¨å¾çš„æ½œåŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºç”Ÿæˆè¡¨ç¤ºçº ç¼ ï¼ˆREGï¼‰çš„ç›´è§‚æ–¹æ³•ï¼Œå®ƒå°†ä½çº§åˆ«å›¾åƒæ½œèƒ½ä¸é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹çš„å•ä¸ªé«˜çº§ç±»åˆ«æ ‡è®°çº ç¼ åœ¨ä¸€èµ·è¿›è¡Œå»å™ªã€‚REGèƒ½å¤Ÿç›´æ¥ä»çº¯å™ªå£°ä¸­äº§ç”Ÿè¿è´¯çš„å›¾åƒç±»åˆ«å¯¹ï¼Œä»è€Œå¤§å¤§æé«˜äº†ç”Ÿæˆè´¨é‡å’Œè®­ç»ƒæ•ˆç‡ã€‚è¿™å‡ ä¹ä¸éœ€è¦é¢å¤–çš„æ¨ç†å¼€é”€ï¼Œä»…éœ€è¦ä¸€ä¸ªé¢å¤–çš„æ ‡è®°è¿›è¡Œå»å™ªï¼ˆFLOPså’Œå»¶è¿Ÿå¢åŠ ä¸åˆ°0.5%ï¼‰ã€‚æ¨ç†è¿‡ç¨‹åŒæ—¶é‡å»ºå›¾åƒæ½œåŠ›å’Œå…¶å¯¹åº”çš„å…¨å±€è¯­ä¹‰ï¼Œæ‰€è·å¾—çš„è¯­ä¹‰çŸ¥è¯†ç§¯æå¼•å¯¼å’Œå¢å¼ºå›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚åœ¨ImageNet 256Ã—256ä¸Šï¼ŒSiT-XL&#x2F;2 + REGæ˜¾ç¤ºäº†ä»¤äººç©ç›®çš„æ”¶æ•›åŠ é€Ÿï¼Œä¸SiT-XL&#x2F;2ç›¸æ¯”åŠ é€Ÿäº†63å€ï¼Œä¸SiT-XL&#x2F;2 + REPAç›¸æ¯”åŠ é€Ÿäº†23å€ã€‚æ›´ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯ï¼Œä»…è®­ç»ƒ40ä¸‡æ¬¡çš„SiT-L&#x2F;2 + REGè¶…è¶Šäº†è®­ç»ƒ4ç™¾ä¸‡æ¬¡ï¼ˆæ—¶é—´é•¿10å€ï¼‰çš„SiT-XL&#x2F;2 + REPAã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Martinser/REG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Martinser/REGæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01467v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>REPAåŠå…¶å˜ä½“é€šè¿‡èå…¥é¢„è®­ç»ƒæ¨¡å‹çš„å¤–éƒ¨è§†è§‰è¡¨å¾ï¼Œæœ‰æ•ˆç¼“è§£äº†æ‰©æ•£æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºREGï¼ˆç”Ÿæˆè¡¨ç¤ºçº ç¼ ï¼‰çš„ç›´è§‚æ–¹æ³•ï¼Œé€šè¿‡å°†ä½çº§åˆ«å›¾åƒæ½œèƒ½ä¸é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹çš„å•ä¸ªé«˜çº§ç±»åˆ«æ ‡è®°çº ç¼ ï¼Œå®ç°äº†å¯¹REPAçš„è¶…è¶Šã€‚REGèƒ½å¤Ÿåœ¨çº¯å™ªå£°ä¸­ç›´æ¥ç”Ÿæˆè¿è´¯çš„å›¾åƒç±»åˆ«å¯¹ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆè´¨é‡å’Œè®­ç»ƒæ•ˆç‡ã€‚è¯¥è¿‡ç¨‹ä»…éœ€ä¸€ä¸ªé¢å¤–çš„æ ‡è®°ç”¨äºå»å™ªï¼Œæ— éœ€å¢åŠ é¢å¤–çš„æ¨ç†å¼€é”€ï¼ˆFLOPså»¶è¿Ÿå¢åŠ ä¸åˆ°0.5%ï¼‰ã€‚åœ¨ImageNet 256Ã—256ä¸Šï¼ŒSiT-XL&#x2F;2 + REGå±•ç°äº†å‡ºè‰²çš„æ”¶æ•›åŠ é€Ÿèƒ½åŠ›ï¼Œç›¸è¾ƒäºSiT-XL&#x2F;2å’ŒSiT-XL&#x2F;2 + REPAåˆ†åˆ«è¾¾åˆ°äº†63å€å’Œ23å€çš„åŠ é€Ÿæ•ˆæœã€‚æ›´ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯ï¼Œä»…è®­ç»ƒ40ä¸‡æ¬¡çš„SiT-L&#x2F;2 + REGè¶…è¶Šäº†è®­ç»ƒ4ç™¾ä¸‡æ¬¡çš„SiT-XL&#x2F;2 + REPAï¼ˆæ—¶é—´é•¿è¾¾å…¶åå€ï¼‰ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/Martinser/REG%E3%80%82">https://github.com/Martinser/REGã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>REPAåŠå…¶ç›¸å…³æ–¹æ³•é€šè¿‡å¼•å…¥å¤–éƒ¨è§†è§‰è¡¨å¾ç¼“è§£äº†æ‰©æ•£æ¨¡å‹åœ¨è®­ç»ƒä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„REGæ–¹æ³•é€šè¿‡çº ç¼ å›¾åƒä½çº§åˆ«æ½œèƒ½ä¸é¢„è®­ç»ƒæ¨¡å‹çš„é«˜çº§ç±»åˆ«æ ‡è®°ï¼Œæ”¹å–„äº†ç”Ÿæˆè´¨é‡å’Œè®­ç»ƒæ•ˆç‡ã€‚</li>
<li>REGåœ¨çº¯å™ªå£°ä¸­ç›´æ¥ç”Ÿæˆè¿è´¯çš„å›¾åƒç±»åˆ«å¯¹ï¼Œæ˜¾ç¤ºäº†å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>REGçš„å»å™ªè¿‡ç¨‹ä»…éœ€è¦ä¸€ä¸ªé¢å¤–çš„æ ‡è®°ï¼Œæœªæ˜¾è‘—å¢åŠ æ¨ç†å¼€é”€ã€‚</li>
<li>åœ¨ImageNetä¸Šï¼ŒREGæ˜¾è‘—åŠ é€Ÿäº†æ¨¡å‹çš„è®­ç»ƒæ”¶æ•›ï¼Œç›¸è¾ƒäºå¯¹æ¯”æ–¹æ³•æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚</li>
<li>REGåœ¨è¾ƒçŸ­çš„è®­ç»ƒæ—¶é—´å†…å³è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºå…¶é«˜æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-58a31e66fd2775830a38ec4f08efee9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-608467cae6de5aa680096a6fa27f489b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22314795a22049a38b6cec5b240e1175.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DiffMark-Diffusion-based-Robust-Watermark-Against-Deepfakes"><a href="#DiffMark-Diffusion-based-Robust-Watermark-Against-Deepfakes" class="headerlink" title="DiffMark: Diffusion-based Robust Watermark Against Deepfakes"></a>DiffMark: Diffusion-based Robust Watermark Against Deepfakes</h2><p><strong>Authors:Chen Sun, Haiyang Sun, Zhiqing Guo, Yunfeng Diao, Liejun Wang, Dan Ma, Gaobo Yang, Keqin Li</strong></p>
<p>Deepfakes pose significant security and privacy threats through malicious facial manipulations. While robust watermarking can aid in authenticity verification and source tracking, existing methods often lack the sufficient robustness against Deepfake manipulations. Diffusion models have demonstrated remarkable performance in image generation, enabling the seamless fusion of watermark with image during generation. In this study, we propose a novel robust watermarking framework based on diffusion model, called DiffMark. By modifying the training and sampling scheme, we take the facial image and watermark as conditions to guide the diffusion model to progressively denoise and generate corresponding watermarked image. In the construction of facial condition, we weight the facial image by a timestep-dependent factor that gradually reduces the guidance intensity with the decrease of noise, thus better adapting to the sampling process of diffusion model. To achieve the fusion of watermark condition, we introduce a cross information fusion (CIF) module that leverages a learnable embedding table to adaptively extract watermark features and integrates them with image features via cross-attention. To enhance the robustness of the watermark against Deepfake manipulations, we integrate a frozen autoencoder during training phase to simulate Deepfake manipulations. Additionally, we introduce Deepfake-resistant guidance that employs specific Deepfake model to adversarially guide the diffusion sampling process to generate more robust watermarked images. Experimental results demonstrate the effectiveness of the proposed DiffMark on typical Deepfakes. Our code will be available at <a target="_blank" rel="noopener" href="https://github.com/vpsg-research/DiffMark">https://github.com/vpsg-research/DiffMark</a>. </p>
<blockquote>
<p>æ·±åº¦ä¼ªé€ æŠ€æœ¯é€šè¿‡æ¶æ„é¢éƒ¨æ“ä½œå¯¹å®‰å…¨æ€§å’Œéšç§æ„æˆé‡å¤§å¨èƒã€‚è™½ç„¶é²æ£’æ€§æ°´å°æœ‰åŠ©äºéªŒè¯çœŸå®æ€§å’Œè¿½è¸ªæ¥æºï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸ç¼ºä¹å¯¹æ·±åº¦ä¼ªé€ æ“ä½œçš„è¶³å¤Ÿé²æ£’æ€§ã€‚æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºäº†æƒŠäººçš„æ€§èƒ½ï¼Œèƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ— ç¼èåˆæ°´å°å’Œå›¾åƒã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹é²æ£’æ°´å°æ¡†æ¶ï¼Œç§°ä¸ºDiffMarkã€‚é€šè¿‡ä¿®æ”¹è®­ç»ƒå’Œé‡‡æ ·æ–¹æ¡ˆï¼Œæˆ‘ä»¬å°†é¢éƒ¨å›¾åƒå’Œæ°´å°ä½œä¸ºæ¡ä»¶æ¥æŒ‡å¯¼æ‰©æ•£æ¨¡å‹é€æ­¥å»å™ªå¹¶ç”Ÿæˆç›¸åº”çš„æ°´å°å›¾åƒã€‚åœ¨æ„å»ºé¢éƒ¨æ¡ä»¶æ—¶ï¼Œæˆ‘ä»¬æ ¹æ®æ—¶é—´æ­¥é•¿å› ç´ å¯¹é¢éƒ¨å›¾åƒè¿›è¡ŒåŠ æƒï¼Œéšç€å™ªå£°çš„å‡å°‘é€æ¸é™ä½æŒ‡å¯¼å¼ºåº¦ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è¿‡ç¨‹ã€‚ä¸ºäº†å®ç°æ°´å°æ¡ä»¶çš„èåˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªäº¤å‰ä¿¡æ¯èåˆï¼ˆCIFï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨å¯å­¦ä¹ çš„åµŒå…¥è¡¨è‡ªé€‚åº”åœ°æå–æ°´å°ç‰¹å¾ï¼Œå¹¶é€šè¿‡äº¤å‰æ³¨æ„åŠ›å°†å®ƒä»¬ä¸å›¾åƒç‰¹å¾ç»“åˆåœ¨ä¸€èµ·ã€‚ä¸ºäº†æé«˜æ°´å°å¯¹æ·±åº¦ä¼ªé€ æ“ä½œçš„é²æ£’æ€§ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒé˜¶æ®µæ•´åˆäº†ä¸€ä¸ªå†»ç»“çš„è‡ªç¼–ç å™¨æ¥æ¨¡æ‹Ÿæ·±åº¦ä¼ªé€ æ“ä½œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†æ·±åº¦ä¼ªé€ æŠµæŠ—æŒ‡å¯¼ï¼Œé‡‡ç”¨ç‰¹å®šçš„æ·±åº¦ä¼ªé€ æ¨¡å‹å¯¹æŠ—æ€§åœ°æŒ‡å¯¼æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ï¼Œç”Ÿæˆæ›´é²æ£’çš„æ°´å°å›¾åƒã€‚å®éªŒç»“æœè¯æ˜äº†DiffMarkåœ¨å…¸å‹æ·±åº¦ä¼ªé€ ä¸Šçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/vpsg-research/DiffMark%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/vpsg-research/DiffMarkä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01428v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹ç¨³å¥æ°´å°æ¡†æ¶DiffMarkï¼Œç”¨äºåº”å¯¹æ·±åº¦ä¼ªé€ æŠ€æœ¯å¸¦æ¥çš„å®‰å…¨å’Œéšç§å¨èƒã€‚é€šè¿‡ä¿®æ”¹è®­ç»ƒä¸é‡‡æ ·æ–¹æ¡ˆï¼Œå°†é¢éƒ¨å›¾åƒä¸æ°´å°ä½œä¸ºæ¡ä»¶å¼•å¯¼æ‰©æ•£æ¨¡å‹é€æ­¥å»å™ªå¹¶ç”Ÿæˆç›¸åº”çš„æ°´å°å›¾åƒã€‚é‡‡ç”¨è·¨ä¿¡æ¯èåˆæ¨¡å—å®ç°æ°´å°ä¸å›¾åƒç‰¹å¾çš„èåˆï¼Œå¹¶é€šè¿‡å†»ç»“çš„è‡ªç¼–ç å™¨å¢å¼ºæ°´å°å¯¹æ·±åº¦ä¼ªé€ æ“ä½œçš„é²æ£’æ€§ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/vpsg-research/DiffMark">é“¾æ¥</a>æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ä¼ªé€ æŠ€æœ¯å¸¦æ¥äº†ä¸¥é‡çš„å®‰å…¨ä¸éšç§å¨èƒï¼Œéœ€è¦æ–°çš„æ–¹æ³•åº”å¯¹ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ°´å°åº”ç”¨ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¯ä»¥æ— ç¼èåˆæ°´å°ä¸å›¾åƒã€‚</li>
<li>æå‡ºåŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹ç¨³å¥æ°´å°æ¡†æ¶DiffMarkã€‚</li>
<li>é€šè¿‡ä¿®æ”¹è®­ç»ƒä¸é‡‡æ ·æ–¹æ¡ˆï¼Œå°†é¢éƒ¨å›¾åƒä¸æ°´å°ä½œä¸ºæ¡ä»¶å¼•å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ°´å°å›¾åƒã€‚</li>
<li>é‡‡ç”¨è·¨ä¿¡æ¯èåˆæ¨¡å—å®ç°æ°´å°ä¸å›¾åƒç‰¹å¾çš„èåˆã€‚</li>
<li>é€šè¿‡å†»ç»“çš„è‡ªç¼–ç å™¨æ¨¡æ‹Ÿæ·±åº¦ä¼ªé€ æ“ä½œï¼Œå¢å¼ºæ°´å°çš„é²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01428">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-99b94c77c6102c5e9a90d786dc100c12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-799380a1af97d5e1bbc59771f10310f0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="DocShaDiffusion-Diffusion-Model-in-Latent-Space-for-Document-Image-Shadow-Removal"><a href="#DocShaDiffusion-Diffusion-Model-in-Latent-Space-for-Document-Image-Shadow-Removal" class="headerlink" title="DocShaDiffusion: Diffusion Model in Latent Space for Document Image   Shadow Removal"></a>DocShaDiffusion: Diffusion Model in Latent Space for Document Image   Shadow Removal</h2><p><strong>Authors:Wenjie Liu, Bingshu Wang, Ze Wang, C. L. Philip Chen</strong></p>
<p>Document shadow removal is a crucial task in the field of document image enhancement. However, existing methods tend to remove shadows with constant color background and ignore color shadows. In this paper, we first design a diffusion model in latent space for document image shadow removal, called DocShaDiffusion. It translates shadow images from pixel space to latent space, enabling the model to more easily capture essential features. To address the issue of color shadows, we design a shadow soft-mask generation module (SSGM). It is able to produce accurate shadow mask and add noise into shadow regions specially. Guided by the shadow mask, a shadow mask-aware guided diffusion module (SMGDM) is proposed to remove shadows from document images by supervising the diffusion and denoising process. We also propose a shadow-robust perceptual feature loss to preserve details and structures in document images. Moreover, we develop a large-scale synthetic document color shadow removal dataset (SDCSRD). It simulates the distribution of realistic color shadows and provides powerful supports for the training of models. Experiments on three public datasets validate the proposed methodâ€™s superiority over state-of-the-art. Our code and dataset will be publicly available. </p>
<blockquote>
<p>æ–‡æ¡£é˜´å½±å»é™¤æ˜¯æ–‡æ¡£å›¾åƒå¢å¼ºé¢†åŸŸä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€åªèƒ½å»é™¤å…·æœ‰æ’å®šèƒŒæ™¯è‰²çš„é˜´å½±ï¼Œè€Œå¿½ç•¥äº†å½©è‰²é˜´å½±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨æ½œåœ¨ç©ºé—´è®¾è®¡ä¸€ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œç”¨äºæ–‡æ¡£å›¾åƒé˜´å½±å»é™¤ï¼Œç§°ä¸ºDocShaDiffusionã€‚å®ƒå°†é˜´å½±å›¾åƒä»åƒç´ ç©ºé—´ç¿»è¯‘åˆ°æ½œåœ¨ç©ºé—´ï¼Œä½¿æ¨¡å‹æ›´å®¹æ˜“æ•æ‰å…³é”®ç‰¹å¾ã€‚ä¸ºäº†è§£å†³å½©è‰²é˜´å½±çš„é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé˜´å½±è½¯æ©è†œç”Ÿæˆæ¨¡å—ï¼ˆSSGMï¼‰ã€‚å®ƒèƒ½å¤Ÿäº§ç”Ÿå‡†ç¡®çš„é˜´å½±æ©è†œï¼Œå¹¶åœ¨é˜´å½±åŒºåŸŸæ·»åŠ ç‰¹å®šå™ªå£°ã€‚åœ¨é˜´å½±æ©è†œçš„æŒ‡å¯¼ä¸‹ï¼Œæå‡ºäº†é˜´å½±æ©è†œæ„ŸçŸ¥å¼•å¯¼æ‰©æ•£æ¨¡å—ï¼ˆSMGDMï¼‰ï¼Œé€šè¿‡ç›‘ç£æ‰©æ•£å’Œå»å™ªè¿‡ç¨‹æ¥å»é™¤æ–‡æ¡£å›¾åƒä¸­çš„é˜´å½±ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§é˜´å½±é²æ£’æ„ŸçŸ¥ç‰¹å¾æŸå¤±ï¼Œä»¥ä¿ç•™æ–‡æ¡£å›¾åƒä¸­çš„ç»†èŠ‚å’Œç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¤§è§„æ¨¡åˆæˆæ–‡æ¡£å½©è‰²é˜´å½±å»é™¤æ•°æ®é›†ï¼ˆSDCSRDï¼‰ã€‚å®ƒæ¨¡æ‹Ÿäº†ç°å®å½©è‰²é˜´å½±çš„åˆ†å¸ƒï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›äº†æœ‰åŠ›æ”¯æŒã€‚åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†æ‰€ææ–¹æ³•ç›¸è¾ƒäºæœ€æ–°æŠ€æœ¯çš„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01422v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–‡æ¡£å›¾åƒé˜´å½±å»é™¤æ–¹æ³•ã€‚é€šè¿‡è®¾è®¡æ½œä¼ç©ºé—´çš„æ‰©æ•£æ¨¡å‹ï¼ˆDocShaDiffusionï¼‰ï¼Œå°†é˜´å½±å›¾åƒä»åƒç´ ç©ºé—´ç¿»è¯‘åˆ°æ½œä¼ç©ºé—´ï¼Œæ›´æ˜“äºæ•æ‰å…³é”®ç‰¹å¾ã€‚ä¸ºè§£å†³å½©è‰²é˜´å½±é—®é¢˜ï¼Œè®¾è®¡äº†é˜´å½±è½¯æ©è†œç”Ÿæˆæ¨¡å—ï¼ˆSSGMï¼‰ï¼Œèƒ½å‡†ç¡®ç”Ÿæˆé˜´å½±æ©è†œï¼Œå¹¶åœ¨é˜´å½±åŒºåŸŸæ·»åŠ å™ªå£°ã€‚åœ¨é˜´å½±æ©è†œçš„å¼•å¯¼ä¸‹ï¼Œæå‡ºäº†é˜´å½±æ©è†œæ„ŸçŸ¥çš„æ‰©æ•£æ¨¡å—ï¼ˆSMGDMï¼‰ï¼Œé€šè¿‡ç›‘ç£æ‰©æ•£å’Œå»å™ªè¿‡ç¨‹æ¥å»é™¤æ–‡æ¡£å›¾åƒä¸­çš„é˜´å½±ã€‚åŒæ—¶ï¼Œæå‡ºäº†é˜´å½±é²æ£’æ„ŸçŸ¥ç‰¹å¾æŸå¤±ï¼Œä»¥ä¿ç•™æ–‡æ¡£å›¾åƒä¸­çš„ç»†èŠ‚å’Œç»“æ„ã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„æ–‡æ¡£å½©è‰²é˜´å½±å»é™¤åˆæˆæ•°æ®é›†ï¼ˆSDCSRDï¼‰ï¼Œæ¨¡æ‹Ÿç°å®å½©è‰²é˜´å½±çš„åˆ†å¸ƒï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›æœ‰åŠ›æ”¯æŒã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–‡æ¡£å›¾åƒé˜´å½±å»é™¤æ–¹æ³•ã€‚</li>
<li>è®¾è®¡äº†æ½œä¼ç©ºé—´çš„æ‰©æ•£æ¨¡å‹ï¼ˆDocShaDiffusionï¼‰ï¼Œå®ç°ä»åƒç´ ç©ºé—´åˆ°æ½œä¼ç©ºé—´çš„ç¿»è¯‘ã€‚</li>
<li>å¼•å…¥é˜´å½±è½¯æ©è†œç”Ÿæˆæ¨¡å—ï¼ˆSSGMï¼‰ä»¥å¤„ç†å½©è‰²é˜´å½±ã€‚</li>
<li>æå‡ºäº†é˜´å½±æ©è†œæ„ŸçŸ¥çš„æ‰©æ•£æ¨¡å—ï¼ˆSMGDMï¼‰ï¼Œé€šè¿‡ç›‘ç£æ‰©æ•£å’Œå»å™ªè¿‡ç¨‹è¿›è¡Œé˜´å½±å»é™¤ã€‚</li>
<li>å¼•å…¥äº†é˜´å½±é²æ£’æ„ŸçŸ¥ç‰¹å¾æŸå¤±ï¼Œä»¥ä¿ç•™æ–‡æ¡£å›¾åƒçš„ç»†èŠ‚å’Œç»“æ„ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªæ–‡æ¡£å½©è‰²é˜´å½±å»é™¤åˆæˆæ•°æ®é›†ï¼ˆSDCSRDï¼‰ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01422">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-594493b390b27ef59c3ead738d823257.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28c2626c4811173bed143809ee3ef8be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c835280ab45e764f433df60338c7882e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ce0bf599f729ce27759772d22267b44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-018e3460bb1cfe57c06c32cd26656cc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c362a795c3738046415578e6eee9ffc.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ODE-t-ODE-l-Shortcutting-the-Time-and-Length-in-Diffusion-and-Flow-Models-for-Faster-Sampling"><a href="#ODE-t-ODE-l-Shortcutting-the-Time-and-Length-in-Diffusion-and-Flow-Models-for-Faster-Sampling" class="headerlink" title="ODE$_t$(ODE$_l$): Shortcutting the Time and Length in Diffusion and Flow   Models for Faster Sampling"></a>ODE$_t$(ODE$_l$): Shortcutting the Time and Length in Diffusion and Flow   Models for Faster Sampling</h2><p><strong>Authors:Denis Gudovskiy, Wenzhao Zheng, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer</strong></p>
<p>Recently, continuous normalizing flows (CNFs) and diffusion models (DMs) have been studied using the unified theoretical framework. Although such models can generate high-quality data points from a noise distribution, the sampling demands multiple iterations to solve an ordinary differential equation (ODE) with high computational complexity. Most existing methods focus on reducing the number of time steps during the sampling process to improve efficiency. In this work, we explore a complementary direction in which the quality-complexity tradeoff can be dynamically controlled in terms of time steps and in the length of the neural network. We achieve this by rewiring the blocks in the transformer-based architecture to solve an inner discretized ODE w.r.t. its length. Then, we employ time- and length-wise consistency terms during flow matching training, and as a result, the sampling can be performed with an arbitrary number of time steps and transformer blocks. Unlike others, our ODE$_t$(ODE$_l$) approach is solver-agnostic in time dimension and decreases both latency and memory usage. Compared to the previous state of the art, image generation experiments on CelebA-HQ and ImageNet show a latency reduction of up to 3$\times$ in the most efficient sampling mode, and a FID score improvement of up to 3.5 points for high-quality sampling. We release our code and model weights with fully reproducible experiments. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè¿ç»­å½’ä¸€åŒ–æµï¼ˆCNFsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å·²ç»ä½¿ç”¨ç»Ÿä¸€çš„ç†è®ºæ¡†æ¶è¿›è¡Œäº†ç ”ç©¶ã€‚å°½ç®¡è¿™äº›æ¨¡å‹èƒ½å¤Ÿä»å™ªå£°åˆ†å¸ƒä¸­ç”Ÿæˆé«˜è´¨é‡çš„æ•°æ®ç‚¹ï¼Œä½†é‡‡æ ·éœ€è¦å¤šæ¬¡è¿­ä»£æ¥è§£å†³å…·æœ‰é«˜è¶…ç®—å¤æ‚åº¦çš„å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰ã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸“æ³¨äºå‡å°‘é‡‡æ ·è¿‡ç¨‹ä¸­çš„æ—¶é—´æ­¥é•¿æ•°é‡ä»¥æé«˜æ•ˆç‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸€ä¸ªå¯ä»¥åŠ¨æ€æ§åˆ¶è´¨é‡å¤æ‚æ€§æƒè¡¡çš„æ–¹å‘ï¼Œæ¶‰åŠæ—¶é—´æ­¥é•¿å’Œç¥ç»ç½‘ç»œé•¿åº¦ã€‚æˆ‘ä»¬é€šè¿‡åœ¨åŸºäºå˜å‹å™¨çš„æ¶æ„ä¸­é‡æ–°è¿æ¥å—æ¥è§£å†³é—®é¢˜ï¼Œè§£å†³äº†ä¸€ä¸ªå…³äºå…¶é•¿åº¦çš„å†…éƒ¨ç¦»æ•£åŒ–ODEã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨æµåŒ¹é…è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥äº†æ—¶é—´å’Œé•¿åº¦ä¸€è‡´æ€§é¡¹ï¼Œå› æ­¤ï¼Œé‡‡æ ·å¯ä»¥ä½¿ç”¨ä»»æ„æ•°é‡çš„æ—¶é—´æ­¥é•¿å’Œå˜å‹å™¨å—è¿›è¡Œã€‚ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„ODEtï¼ˆODElï¼‰æ–¹æ³•æ˜¯æ—¶é—´ç»´åº¦ä¸Šçš„æ±‚è§£å™¨æ— å…³ï¼Œå‡å°‘äº†å»¶è¿Ÿå’Œå†…å­˜ä½¿ç”¨ã€‚ä¸å…ˆå‰çš„ç ”ç©¶ç›¸æ¯”ï¼Œåœ¨CelebA-HQå’ŒImageNetä¸Šçš„å›¾åƒç”Ÿæˆå®éªŒæ˜¾ç¤ºï¼Œåœ¨æœ€æœ‰æ•ˆçš„é‡‡æ ·æ¨¡å¼ä¸‹å»¶è¿Ÿé™ä½äº†é«˜è¾¾ä¸‰å€ï¼Œé«˜è´¨é‡é‡‡æ ·çš„FIDåˆ†æ•°æé«˜äº†é«˜è¾¾3.5åˆ†ã€‚æˆ‘ä»¬å‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹æƒé‡ï¼Œå¹¶æä¾›äº†å¯å®Œå…¨é‡å¤çš„å®éªŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21714v2">PDF</a> Preprint. Github page: github.com&#x2F;gudovskiy&#x2F;odelt</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†è¿‘æœŸå¯¹è¿ç»­å½’ä¸€åŒ–æµï¼ˆCNFsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„ç ”ç©¶ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿä»å™ªå£°åˆ†å¸ƒç”Ÿæˆé«˜è´¨é‡æ•°æ®ç‚¹ï¼Œä½†é‡‡æ ·éœ€è¦è§£å†³å…·æœ‰è®¡ç®—å¤æ‚æ€§çš„å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰ã€‚æœ¬æ–‡æ¢ç´¢äº†ä¸€ä¸ªæ–°æ–¹å‘ï¼Œå¯ä»¥é€šè¿‡åŠ¨æ€æ§åˆ¶æ—¶é—´æ­¥é•¿å’Œç¥ç»ç½‘ç»œé•¿åº¦æ¥è¿›è¡Œè´¨é‡å¤æ‚åº¦çš„æƒè¡¡ã€‚é€šè¿‡é‡æ–°è¿æ¥å˜å‹å™¨æ¶æ„çš„å—æ¥è§£å†³å†…éƒ¨ç¦»æ•£åŒ–çš„ODEï¼Œå¹¶å¼•å…¥æ—¶é—´å’Œé•¿åº¦ä¸€è‡´æ€§æœ¯è¯­è¿›è¡ŒæµåŒ¹é…è®­ç»ƒï¼Œå®ç°äº†å¯åœ¨ä»»æ„æ•°é‡çš„æ—¶é—´æ­¥é•¿å’Œå˜å‹å™¨å—ä¸Šè¿›è¡Œé‡‡æ ·çš„èƒ½åŠ›ã€‚æœ¬æ–‡çš„æ–¹æ³•åœ¨æ—¶é—´å’Œé•¿åº¦ç»´åº¦ä¸Šæ˜¯æ±‚è§£å™¨æ— å…³çš„ï¼Œå¯ä»¥é™ä½å»¶è¿Ÿå¹¶å‡å°‘å†…å­˜ä½¿ç”¨ã€‚åœ¨CelebA-HQå’ŒImageNetä¸Šçš„å›¾åƒç”Ÿæˆå®éªŒè¡¨æ˜ï¼Œåœ¨æœ€æœ‰æ•ˆçš„é‡‡æ ·æ¨¡å¼ä¸‹ï¼Œå»¶è¿Ÿé™ä½äº†ä¸‰å€ï¼Œé«˜è´¨é‡é‡‡æ ·çš„FIDåˆ†æ•°æé«˜äº†æœ€å¤š3.5åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CNFså’ŒDMsä½¿ç”¨ç»Ÿä¸€çš„ç†è®ºæ¡†æ¶è¿›è¡Œç ”ç©¶ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡æ•°æ®ç‚¹ï¼Œä½†é‡‡æ ·æ¶‰åŠè§£å†³ODEï¼Œè®¡ç®—å¤æ‚åº¦é«˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å‡å°‘é‡‡æ ·è¿‡ç¨‹ä¸­çš„æ—¶é—´æ­¥é•¿ä»¥æé«˜æ•ˆç‡ã€‚</li>
<li>æœ¬æ–‡æ¢ç´¢äº†ä¸€ä¸ªæ–°æ–¹å‘ï¼Œé€šè¿‡åŠ¨æ€æ§åˆ¶æ—¶é—´æ­¥é•¿å’Œç¥ç»ç½‘ç»œé•¿åº¦æ¥æƒè¡¡è´¨é‡å¤æ‚åº¦ã€‚</li>
<li>é€šè¿‡é‡å†™åŸºäºå˜å‹å™¨çš„æ¶æ„æ¥è§£å†³å†…éƒ¨ç¦»æ•£åŒ–çš„ODEï¼Œå¹¶å¼•å…¥æ—¶é—´å’Œé•¿åº¦ä¸€è‡´æ€§æœ¯è¯­è¿›è¡Œè®­ç»ƒã€‚</li>
<li>é‡‡æ ·å¯ä»¥åœ¨ä»»æ„æ•°é‡çš„æ—¶é—´æ­¥é•¿å’Œå˜å‹å™¨å—ä¸Šè¿›è¡Œï¼Œæ–¹æ³•åœ¨æ—¶é—´ç»´åº¦ä¸Šæ˜¯æ±‚è§£å™¨æ— å…³çš„ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œåœ¨CelebA-HQå’ŒImageNetä¸Šçš„å›¾åƒç”Ÿæˆå®éªŒä¸­ï¼Œå»¶è¿Ÿé™ä½äº†ä¸‰å€ï¼ŒFIDåˆ†æ•°æœ‰æ‰€æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21714">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d3ff2633e9a88e3fd7e632a19a08085e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74524288940c2e8c8dc7ac8af13f2d23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccea217a820801d014e303e1909315ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5150c18868376dd6b7d1a2f9395931fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fc2c43e432d22ab694af8d04b77b0db.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="FramePrompt-In-context-Controllable-Animation-with-Zero-Structural-Changes"><a href="#FramePrompt-In-context-Controllable-Animation-with-Zero-Structural-Changes" class="headerlink" title="FramePrompt: In-context Controllable Animation with Zero Structural   Changes"></a>FramePrompt: In-context Controllable Animation with Zero Structural   Changes</h2><p><strong>Authors:Guian Fang, Yuchao Gu, Mike Zheng Shou</strong></p>
<p>Generating controllable character animation from a reference image and motion guidance remains a challenging task due to the inherent difficulty of injecting appearance and motion cues into video diffusion models. Prior works often rely on complex architectures, explicit guider modules, or multi-stage processing pipelines, which increase structural overhead and hinder deployment. Inspired by the strong visual context modeling capacity of pre-trained video diffusion transformers, we propose FramePrompt, a minimalist yet powerful framework that treats reference images, skeleton-guided motion, and target video clips as a unified visual sequence. By reformulating animation as a conditional future prediction task, we bypass the need for guider networks and structural modifications. Experiments demonstrate that our method significantly outperforms representative baselines across various evaluation metrics while also simplifying training. Our findings highlight the effectiveness of sequence-level visual conditioning and demonstrate the potential of pre-trained models for controllable animation without architectural changes. </p>
<blockquote>
<p>ä»å‚è€ƒå›¾åƒå’Œè¿åŠ¨æŒ‡å¯¼ç”Ÿæˆå¯æ§è§’è‰²åŠ¨ç”»ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå°†å¤–è§‚å’Œè¿åŠ¨çº¿ç´¢æ³¨å…¥è§†é¢‘æ‰©æ•£æ¨¡å‹å›ºæœ‰çš„éš¾åº¦ã€‚ä»¥å‰çš„å·¥ä½œç»å¸¸ä¾èµ–äºå¤æ‚çš„æ¶æ„ã€æ˜ç¡®çš„æŒ‡å¯¼æ¨¡å—æˆ–å¤šé˜¶æ®µå¤„ç†ç®¡é“ï¼Œè¿™å¢åŠ äº†ç»“æ„ä¸Šçš„é¢å¤–è´Ÿæ‹…å¹¶é˜»ç¢äº†éƒ¨ç½²ã€‚å—åˆ°é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£å˜å‹å™¨å¼ºå¤§è§†è§‰ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†FramePromptï¼Œè¿™æ˜¯ä¸€ä¸ªæç®€è€Œå¼ºå¤§çš„æ¡†æ¶ï¼Œå°†å‚è€ƒå›¾åƒã€éª¨æ¶å¼•å¯¼çš„è¿åŠ¨å’Œç›®æ ‡è§†é¢‘ç‰‡æ®µè§†ä¸ºç»Ÿä¸€è§†è§‰åºåˆ—ã€‚é€šè¿‡é‡æ–°åˆ¶å®šåŠ¨ç”»ä½œä¸ºæ¡ä»¶æœªæ¥é¢„æµ‹ä»»åŠ¡ï¼Œæˆ‘ä»¬ä¸éœ€è¦æŒ‡å¯¼ç½‘ç»œå’Œç»“æ„ä¿®æ”¹ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§è¯„ä¼°æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºä»£è¡¨æ€§åŸºçº¿ï¼ŒåŒæ—¶ç®€åŒ–äº†è®­ç»ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜åºåˆ—çº§è§†è§‰è°ƒèŠ‚çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†é¢„è®­ç»ƒæ¨¡å‹åœ¨æ— éœ€æ¶æ„æ›´æ”¹çš„æƒ…å†µä¸‹è¿›è¡Œå¯æ§åŠ¨ç”»çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17301v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://frameprompt.github.io/">https://frameprompt.github.io/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹ä»å‚è€ƒå›¾åƒå’Œè¿åŠ¨æŒ‡å¯¼ç”Ÿæˆå¯æ§è§’è‰²åŠ¨ç”»çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†FramePromptæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£å˜å‹å™¨å¼ºå¤§çš„è§†è§‰ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ï¼Œå°†å‚è€ƒå›¾åƒã€éª¨æ¶å¼•å¯¼çš„è¿åŠ¨å’Œç›®æ ‡è§†é¢‘ç‰‡æ®µè§†ä¸ºç»Ÿä¸€è§†è§‰åºåˆ—ã€‚é€šè¿‡å°†å…¶é‡æ–°è¡¨è¿°ä¸ºæ¡ä»¶æœªæ¥é¢„æµ‹ä»»åŠ¡ï¼Œé¿å¼€äº†å¯¹æŒ‡å¯¼ç½‘ç»œçš„ç»“æ„ä¿®æ”¹éœ€æ±‚ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§è¯„ä¼°æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºä»£è¡¨æ€§åŸºçº¿ï¼ŒåŒæ—¶ç®€åŒ–äº†è®­ç»ƒã€‚ç ”ç©¶å¼ºè°ƒäº†åºåˆ—çº§è§†è§‰æ¡ä»¶çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†é¢„è®­ç»ƒæ¨¡å‹åœ¨æ— éœ€æ¶æ„æ›´æ”¹çš„æƒ…å†µä¸‹è¿›è¡Œå¯æ§åŠ¨ç”»çš„æ½œåŠ›ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>ç”Ÿæˆä»å‚è€ƒå›¾åƒå’Œè¿åŠ¨æŒ‡å¯¼çš„å¯æ§è§’è‰²åŠ¨ç”»æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚</li>
<li>æ­¤å‰çš„æ–¹æ³•å¸¸å¸¸ä¾èµ–å¤æ‚çš„æ¶æ„ã€æ˜ç¡®çš„å¼•å¯¼æ¨¡å—æˆ–å¤šé˜¶æ®µå¤„ç†ç®¡é“ï¼Œè¿™å¢åŠ äº†ç»“æ„ä¸Šçš„è´Ÿæ‹…å¹¶é˜»ç¢äº†éƒ¨ç½²ã€‚</li>
<li>FramePromptæ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£å˜å‹å™¨çš„è§†è§‰ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ï¼Œå°†å‚è€ƒå›¾åƒã€éª¨æ¶å¼•å¯¼çš„è¿åŠ¨å’Œç›®æ ‡è§†é¢‘ç‰‡æ®µè§†ä¸ºç»Ÿä¸€è§†è§‰åºåˆ—ã€‚</li>
<li>é€šè¿‡å°†åŠ¨ç”»é‡æ–°è¡¨è¿°ä¸ºæ¡ä»¶æœªæ¥é¢„æµ‹ä»»åŠ¡ï¼Œé¿å¼€äº†å¯¹æŒ‡å¯¼ç½‘ç»œå’Œç»“æ„ä¿®æ”¹çš„éœ€æ±‚ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒFramePromptåœ¨å¤šç§è¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†åºåˆ—çº§è§†è§‰æ¡ä»¶çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>é¢„è®­ç»ƒæ¨¡å‹åœ¨å¯æ§åŠ¨ç”»æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œä¸”æ— éœ€è¿›è¡Œæ¶æ„æ›´æ”¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17301">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8d31d2ec0dc906c9bda057e9109cfc07.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-98210cc46c90dd6bf9a6e32cca88f62b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12ab27eda95dce9f859b8d867ab983be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99364389594c1da319807121ba06c90a.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Text-Aware-Image-Restoration-with-Diffusion-Models"><a href="#Text-Aware-Image-Restoration-with-Diffusion-Models" class="headerlink" title="Text-Aware Image Restoration with Diffusion Models"></a>Text-Aware Image Restoration with Diffusion Models</h2><p><strong>Authors:Jaewon Min, Jin Hyeon Kim, Paul Hyunbin Cho, Jaeeun Lee, Jihye Park, Minkyu Park, Sangpil Kim, Hyunhee Park, Seungryong Kim</strong></p>
<p>Image restoration aims to recover degraded images. However, existing diffusion-based restoration methods, despite great success in natural image restoration, often struggle to faithfully reconstruct textual regions in degraded images. Those methods frequently generate plausible but incorrect text-like patterns, a phenomenon we refer to as text-image hallucination. In this paper, we introduce Text-Aware Image Restoration (TAIR), a novel restoration task that requires the simultaneous recovery of visual contents and textual fidelity. To tackle this task, we present SA-Text, a large-scale benchmark of 100K high-quality scene images densely annotated with diverse and complex text instances. Furthermore, we propose a multi-task diffusion framework, called TeReDiff, that integrates internal features from diffusion models into a text-spotting module, enabling both components to benefit from joint training. This allows for the extraction of rich text representations, which are utilized as prompts in subsequent denoising steps. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art restoration methods, achieving significant gains in text recognition accuracy. See our project page: <a target="_blank" rel="noopener" href="https://cvlab-kaist.github.io/TAIR/">https://cvlab-kaist.github.io/TAIR/</a> </p>
<blockquote>
<p>å›¾åƒä¿®å¤æ—¨åœ¨æ¢å¤é€€åŒ–å›¾åƒã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºæ‰©æ•£çš„ä¿®å¤æ–¹æ³•è™½ç„¶åœ¨è‡ªç„¶å›¾åƒä¿®å¤ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨é€€åŒ–å›¾åƒçš„æ–‡æœ¬åŒºåŸŸé‡å»ºä¸­å´å¸¸å¸¸éš¾ä»¥å¿ å®è¿˜åŸã€‚è¿™äº›æ–¹æ³•ç»å¸¸ç”Ÿæˆçœ‹ä¼¼åˆç†ä½†é”™è¯¯çš„æ–‡æœ¬æ¨¡å¼ï¼Œæˆ‘ä»¬å°†è¿™ç§ç°è±¡ç§°ä¸ºæ–‡æœ¬å›¾åƒå¹»è§‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–‡æœ¬æ„ŸçŸ¥å›¾åƒä¿®å¤ï¼ˆTAIRï¼‰è¿™ä¸€æ–°çš„ä¿®å¤ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡è¦æ±‚åŒæ—¶æ¢å¤è§†è§‰å†…å®¹å’Œæ–‡æœ¬å¿ å®åº¦ã€‚ä¸ºè§£å†³æ­¤ä»»åŠ¡ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SA-Textï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«10ä¸‡å¼ é«˜è´¨é‡åœºæ™¯å›¾åƒï¼Œå¯†é›†æ ‡æ³¨äº†å¤šæ ·ä¸”å¤æ‚çš„æ–‡æœ¬å®ä¾‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šä»»åŠ¡æ‰©æ•£æ¡†æ¶ï¼Œç§°ä¸ºTeReDiffï¼Œå®ƒå°†æ‰©æ•£æ¨¡å‹çš„å†…éƒ¨ç‰¹å¾é›†æˆåˆ°æ–‡æœ¬ç‚¹æ¨¡å—ä¸­ï¼Œä½¿ä¸¤ä¸ªç»„ä»¶éƒ½èƒ½ä»è”åˆè®­ç»ƒä¸­å—ç›Šã€‚è¿™å…è®¸æå–ä¸°å¯Œçš„æ–‡æœ¬è¡¨ç¤ºï¼Œç”¨ä½œåç»­å»å™ªæ­¥éª¤ä¸­çš„æç¤ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„ä¿®å¤æ–¹æ³•ï¼Œåœ¨æ–‡æœ¬è¯†åˆ«å‡†ç¡®æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚è¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://cvlab-kaist.github.io/TAIR/">https://cvlab-kaist.github.io/TAIR/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09993v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://cvlab-kaist.github.io/TAIR/">https://cvlab-kaist.github.io/TAIR/</a></p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†å›¾åƒä¿®å¤çš„ç›®æ ‡å’Œç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†æ–‡æœ¬åŒºåŸŸæ—¶çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†æ–‡æœ¬æ„ŸçŸ¥å›¾åƒä¿®å¤ï¼ˆTAIRï¼‰ä»»åŠ¡ï¼Œå¹¶ä»‹ç»äº†é’ˆå¯¹è¯¥ä»»åŠ¡çš„å¤§å‹åŸºå‡†æ•°æ®é›†SA-Textã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§å¤šä»»åŠ¡æ‰©æ•£æ¡†æ¶TeReDiffï¼Œå®ƒé€šè¿‡æ•´åˆæ‰©æ•£æ¨¡å‹çš„å†…éƒ¨ç‰¹å¾ä¸æ–‡æœ¬è¯†åˆ«æ¨¡å—ï¼Œæé«˜äº†æ–‡æœ¬è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ–‡æœ¬è¯†åˆ«å‡†ç¡®ç‡ä¸Šæ˜æ˜¾ä¼˜äºç°æœ‰ä¿®å¤æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒä¿®å¤çš„ç›®æ ‡ï¼šæ¢å¤å—æŸå›¾åƒã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ¨¡å‹æŒ‘æˆ˜ï¼šåœ¨è‡ªç„¶å›¾åƒæ¢å¤æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ–‡æœ¬åŒºåŸŸé‡å»ºæ—¶å¾€å¾€å‡ºç°é—®é¢˜ã€‚</li>
<li>æ–‡æœ¬æ„ŸçŸ¥å›¾åƒä¿®å¤ä»»åŠ¡ä»‹ç»ï¼šéœ€è¦åŒæ—¶æ¢å¤è§†è§‰å†…å®¹å’Œæ–‡æœ¬ä¿çœŸåº¦ã€‚</li>
<li>å¤§å‹åŸºå‡†æ•°æ®é›†SA-Textä»‹ç»ï¼šåŒ…å«10ä¸‡å¼ é«˜è´¨é‡åœºæ™¯å›¾åƒï¼Œå¯†é›†æ ‡æ³¨äº†å¤šæ ·ä¸”å¤æ‚çš„æ–‡æœ¬å®ä¾‹ã€‚</li>
<li>å¤šä»»åŠ¡æ‰©æ•£æ¡†æ¶TeReDiffï¼šæ•´åˆæ‰©æ•£æ¨¡å‹çš„å†…éƒ¨ç‰¹å¾ä¸æ–‡æœ¬è¯†åˆ«æ¨¡å—è¿›è¡Œè”åˆè®­ç»ƒã€‚</li>
<li>TeReDiffçš„ä¼˜åŠ¿ï¼šæå–ä¸°å¯Œçš„æ–‡æœ¬è¡¨ç¤ºï¼Œç”¨äºåç»­å»å™ªæ­¥éª¤ä¸­çš„æç¤ºã€‚</li>
<li>å®éªŒç»“æœï¼šè¯¥æ–¹æ³•åœ¨æ–‡æœ¬è¯†åˆ«å‡†ç¡®ç‡ä¸Šä¼˜äºç°æœ‰ä¿®å¤æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09993">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a042fcc9d227f97bb1ed6a3eff0b0b9c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7eb60e3380b277cb38aa7f9ee229ff23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-511bb8bf09e45804c3a98364bdb10570.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c5eec05910710dbada7c22db87db7aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f57900cb98032e547d883d08dea1d2d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4250366fe6a6b383eff719e28a13adb.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Non-rigid-Motion-Correction-for-MRI-Reconstruction-via-Coarse-To-Fine-Diffusion-Models"><a href="#Non-rigid-Motion-Correction-for-MRI-Reconstruction-via-Coarse-To-Fine-Diffusion-Models" class="headerlink" title="Non-rigid Motion Correction for MRI Reconstruction via Coarse-To-Fine   Diffusion Models"></a>Non-rigid Motion Correction for MRI Reconstruction via Coarse-To-Fine   Diffusion Models</h2><p><strong>Authors:Frederic Wang, Jonathan I. Tamir</strong></p>
<p>Magnetic Resonance Imaging (MRI) is highly susceptible to motion artifacts due to the extended acquisition times required for k-space sampling. These artifacts can compromise diagnostic utility, particularly for dynamic imaging. We propose a novel alternating minimization framework that leverages a bespoke diffusion model to jointly reconstruct and correct non-rigid motion-corrupted k-space data. The diffusion model uses a coarse-to-fine denoising strategy to capture large overall motion and reconstruct the lower frequencies of the image first, providing a better inductive bias for motion estimation than that of standard diffusion models. We demonstrate the performance of our approach on both real-world cine cardiac MRI datasets and complex simulated rigid and non-rigid deformations, even when each motion state is undersampled by a factor of 64x. Additionally, our method is agnostic to sampling patterns, anatomical variations, and MRI scanning protocols, as long as some low frequency components are sampled during each motion state. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ç”±äºkç©ºé—´é‡‡æ ·æ‰€éœ€çš„é•¿é‡‡é›†æ—¶é—´ï¼Œå¾ˆå®¹æ˜“å—åˆ°è¿åŠ¨ä¼ªå½±çš„å½±å“ã€‚è¿™äº›ä¼ªå½±å¯èƒ½ä¼šé™ä½è¯Šæ–­çš„æ•ˆç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€æˆåƒä¸­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹äº¤æ›¿æœ€å°åŒ–æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ä¸“ç”¨æ‰©æ•£æ¨¡å‹è”åˆé‡å»ºå’Œçº æ­£éåˆšæ€§è¿åŠ¨æŸåçš„kç©ºé—´æ•°æ®ã€‚æ‰©æ•£æ¨¡å‹é‡‡ç”¨ç”±ç²—åˆ°ç»†çš„é™å™ªç­–ç•¥ï¼Œé¦–å…ˆæ•æ‰æ•´ä½“å¤§è¿åŠ¨å¹¶é‡å»ºå›¾åƒçš„ä½é¢‘éƒ¨åˆ†ï¼Œä¸ºè¿åŠ¨ä¼°è®¡æä¾›æ¯”æ ‡å‡†æ‰©æ•£æ¨¡å‹æ›´å¥½çš„å½’çº³åè§ã€‚æˆ‘ä»¬åœ¨ç°å®ä¸–ç•Œçš„å¿ƒè„ç”µå½±MRIæ•°æ®é›†å’Œå¤æ‚çš„æ¨¡æ‹Ÿåˆšæ€§å’Œéåˆšæ€§å˜å½¢ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•æ€§èƒ½ï¼Œå³ä½¿åœ¨æ¯ä¸ªè¿åŠ¨çŠ¶æ€ä¸‹ä»¥64å€æ¬ é‡‡æ ·çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯¹é‡‡æ ·æ¨¡å¼ã€è§£å‰–å˜åŒ–å’ŒMRIæ‰«æåè®®æ²¡æœ‰ç‰¹å®šçš„è¦æ±‚ï¼Œåªè¦åœ¨æ¯ä¸ªè¿åŠ¨çŠ¶æ€ä¸‹é‡‡æ ·ä¸€äº›ä½é¢‘æˆåˆ†å³å¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15057v3">PDF</a> ICIP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å®šåˆ¶çš„æ‰©æ•£æ¨¡å‹çš„æ–°äº¤æ›¿æœ€å°åŒ–æ¡†æ¶ï¼Œè”åˆé‡å»ºå’Œæ ¡æ­£éåˆšæ€§è¿åŠ¨å—å¹²æ‰°çš„k-ç©ºé—´æ•°æ®ã€‚æ‰©æ•£æ¨¡å‹é‡‡ç”¨ä»ç²—åˆ°ç»†çš„é™å™ªç­–ç•¥ï¼Œé¦–å…ˆæ•æ‰æ•´ä½“å¤§è¿åŠ¨å¹¶é‡å»ºå›¾åƒçš„ä½é¢‘éƒ¨åˆ†ï¼Œä¸ºè¿åŠ¨ä¼°è®¡æä¾›æ›´å¥½çš„å½’çº³åç½®ã€‚è¯¥æ¨¡å‹å¯¹çœŸå®ä¸–ç•Œçš„ç”µå½±å¿ƒè„MRIæ•°æ®é›†å’Œå¤æ‚çš„æ¨¡æ‹Ÿåˆšæ€§åŠéåˆšæ€§å˜å½¢éƒ½æœ‰è‰¯å¥½è¡¨ç°ï¼Œå³ä½¿åœ¨æ¯ä¸ªè¿åŠ¨çŠ¶æ€æ¬ é‡‡æ ·64å€çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯¹é‡‡æ ·æ¨¡å¼ã€è§£å‰–å˜åŒ–å’ŒMRIæ‰«æåè®®éƒ½å…·æœ‰é²æ£’æ€§ï¼Œåªè¦æ¯ä¸ªè¿åŠ¨çŠ¶æ€æœŸé—´éƒ½é‡‡æ ·äº†ä¸€äº›ä½é¢‘æˆåˆ†å³å¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ç”¨äºMRIä¸­çš„éåˆšæ€§è¿åŠ¨æ ¡æ­£ã€‚</li>
<li>æ¨¡å‹ç»“åˆäº†é‡å»ºå’Œæ ¡æ­£è¿åŠ¨å—å¹²æ‰°çš„k-ç©ºé—´æ•°æ®ã€‚</li>
<li>é‡‡ç”¨ä»ç²—åˆ°ç»†çš„é™å™ªç­–ç•¥æ¥å¤„ç†æ•´ä½“å¤§è¿åŠ¨å’Œä½é¢‘å›¾åƒé‡å»ºã€‚</li>
<li>æ¨¡å‹åœ¨çœŸå®å’Œæ¨¡æ‹Ÿçš„MRIæ•°æ®é›†ä¸Šéƒ½è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹å¯¹å¤šç§è¿åŠ¨çŠ¶æ€å…·æœ‰è‰¯å¥½çš„é€‚åº”æ€§ï¼Œå³ä½¿åœ¨é«˜åº¦æ¬ é‡‡æ ·çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å·¥ä½œã€‚</li>
<li>æ–¹æ³•å¯¹å„ç§é‡‡æ ·æ¨¡å¼ã€è§£å‰–å˜åŒ–å’ŒMRIæ‰«æåè®®å…·æœ‰é²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15057">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dcb73323e1357571f95576cf827ff061.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40c450a3001a4b039f9561669912de35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7a89deb44f02ea190b093675187992e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1879d5afbf5f04752a5b3d36a1b7585.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1b84582bd7f57ab6c58ed143a97ad8e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-57052dc9379f13a30206187b3a442480.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4118c0aa3cf8ed7d5566417769df99db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d2a90d028abcb96a0153659c4c1beca.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-05/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-05/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-06/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c5bd301c013f4c6653fb04aefd7c77ab.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-06  Large Reasoning Models are not thinking straight on the unreliability   of thinking trajectories
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-05/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-16256fb252544fb37bdf6def48db78c7.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-05  LocalDyGS Multi-view Global Dynamic Scene Modeling via Adaptive Local   Implicit Feature Decoupling
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23154.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
