<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-05  Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for   Data-Efficient Model Adaptation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-215e91a93cb251b46cce4a9432d2d75f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-05-æ›´æ–°"><a href="#2025-07-05-æ›´æ–°" class="headerlink" title="2025-07-05 æ›´æ–°"></a>2025-07-05 æ›´æ–°</h1><h2 id="Bootstrapping-Grounded-Chain-of-Thought-in-Multimodal-LLMs-for-Data-Efficient-Model-Adaptation"><a href="#Bootstrapping-Grounded-Chain-of-Thought-in-Multimodal-LLMs-for-Data-Efficient-Model-Adaptation" class="headerlink" title="Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for   Data-Efficient Model Adaptation"></a>Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for   Data-Efficient Model Adaptation</h2><p><strong>Authors:Jiaer Xia, Bingkui Tong, Yuhang Zang, Rui Shao, Kaiyang Zhou</strong></p>
<p>Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in interpreting images using natural language. However, without using large-scale datasets for retraining, these models are difficult to adapt to specialized vision tasks, e.g., chart understanding. This problem is caused by a mismatch between pre-training and downstream datasets: pre-training datasets primarily concentrate on scenes and objects but contain limited information about specialized, non-object images, such as charts and tables. In this paper, we share an interesting finding that training an MLLM with Chain-of-Thought (CoT) reasoning data can facilitate model adaptation in specialized vision tasks, especially under data-limited regimes. However, we identify a critical issue within CoT data distilled from pre-trained MLLMs, i.e., the data often contains multiple factual errors in the reasoning steps. To address the problem, we propose Grounded Chain-of-Thought (GCoT), a simple bootstrapping-based approach that aims to inject grounding information (i.e., bounding boxes) into CoT data, essentially making the reasoning steps more faithful to input images. We evaluate our approach on five specialized vision tasks, which cover a variety of visual formats including charts, tables, receipts, and reports. The results demonstrate that under data-limited regimes our approach significantly improves upon fine-tuning and distillation. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åˆ©ç”¨è‡ªç„¶è¯­è¨€è§£é‡Šå›¾åƒæ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¦‚æœä¸ä½¿ç”¨å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œé‡æ–°è®­ç»ƒï¼Œè¿™äº›æ¨¡å‹å¾ˆéš¾é€‚åº”ç‰¹å®šçš„è§†è§‰ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾è¡¨ç†è§£ã€‚è¿™ä¸ªé—®é¢˜æ˜¯ç”±é¢„è®­ç»ƒä¸ä¸‹æ¸¸æ•°æ®é›†ä¹‹é—´çš„ä¸åŒ¹é…é€ æˆçš„ï¼šé¢„è®­ç»ƒæ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨åœºæ™¯å’Œå¯¹è±¡ä¸Šï¼Œä½†å…³äºç‰¹å®šã€éå¯¹è±¡å›¾åƒï¼ˆå¦‚å›¾è¡¨å’Œè¡¨æ ¼ï¼‰çš„ä¿¡æ¯æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ†äº«äº†ä¸€ä¸ªæœ‰è¶£çš„å‘ç°ï¼Œå³é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æ•°æ®è®­ç»ƒMLLMå¯ä»¥ä¿ƒè¿›æ¨¡å‹åœ¨ç‰¹å®šè§†è§‰ä»»åŠ¡ä¸­çš„é€‚åº”ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°äº†ä»é¢„è®­ç»ƒçš„MLLMä¸­æç‚¼å‡ºçš„CoTæ•°æ®å­˜åœ¨çš„ä¸€ä¸ªå…³é”®é—®é¢˜ï¼Œå³æ•°æ®ä¸­çš„æ¨ç†æ­¥éª¤å¾€å¾€å­˜åœ¨å¤šä¸ªäº‹å®é”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ¥åœ°é“¾æ€ç»´ï¼ˆGCoTï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ—¨åœ¨æ³¨å…¥æ¥åœ°ä¿¡æ¯ï¼ˆå³è¾¹ç•Œæ¡†ï¼‰åˆ°CoTæ•°æ®ä¸­ï¼Œæœ¬è´¨ä¸Šä½¿æ¨ç†æ­¥éª¤æ›´åŠ å¿ äºè¾“å…¥å›¾åƒã€‚æˆ‘ä»¬åœ¨äº”ä¸ªç‰¹å®šçš„è§†è§‰ä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¿™äº›ä»»åŠ¡æ¶µç›–äº†å„ç§è§†è§‰æ ¼å¼ï¼ŒåŒ…æ‹¬å›¾è¡¨ã€è¡¨æ ¼ã€æ”¶æ®å’ŒæŠ¥å‘Šã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æ”¹è¿›äº†å¾®è°ƒæ–¹æ³•å’Œæç‚¼æ–¹æ³•çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02859v1">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong><br>    å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åˆ©ç”¨è‡ªç„¶è¯­è¨€è§£è¯»å›¾åƒæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨ä¸ä½¿ç”¨å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œå†è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œéš¾ä»¥é€‚åº”ç‰¹å®šçš„è§†è§‰ä»»åŠ¡ï¼Œå¦‚å›¾è¡¨ç†è§£ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†æ•°æ®è®­ç»ƒMLLMçš„æ–¹æ³•ï¼Œæœ‰åŠ©äºæ¨¡å‹åœ¨ç‰¹å®šè§†è§‰ä»»åŠ¡ä¸­çš„é€‚åº”ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œå‘ç°CoTæ•°æ®å­˜åœ¨å…³é”®æ€§é—®é¢˜ï¼Œå³æ¨ç†æ­¥éª¤ä¸­å¸¸å«æœ‰å¤šå¤„äº‹å®é”™è¯¯ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºåŸºäºæ¥åœ°æ€ç»´é“¾ï¼ˆGCoTï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨å°†æ¥åœ°ä¿¡æ¯ï¼ˆå³è¾¹ç•Œæ¡†ï¼‰æ³¨å…¥CoTæ•°æ®ï¼Œä½¿æ¨ç†æ­¥éª¤æ›´å¿ å®äºè¾“å…¥å›¾åƒã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œåœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æ”¹è¿›äº†å¾®è°ƒä¸è’¸é¦çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨è§£è¯»å›¾åƒæ–¹é¢è¡¨ç°å‡ºå“è¶Šèƒ½åŠ›ï¼Œä½†é€‚åº”ç‰¹å®šè§†è§‰ä»»åŠ¡å­˜åœ¨å›°éš¾ã€‚</li>
<li>CoTæ•°æ®æœ‰åŠ©äºMLLMåœ¨ç‰¹å®šè§†è§‰ä»»åŠ¡ä¸­çš„é€‚åº”ï¼Œä½†åœ¨æ¨ç†æ­¥éª¤ä¸­å­˜åœ¨äº‹å®é”™è¯¯ã€‚</li>
<li>GCoTæ–¹æ³•æ—¨åœ¨è§£å†³CoTæ•°æ®ä¸­çš„é—®é¢˜ï¼Œé€šè¿‡æ³¨å…¥æ¥åœ°ä¿¡æ¯æé«˜æ¨ç†å‡†ç¡®æ€§ã€‚</li>
<li>GCoTæ–¹æ³•åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹æ˜¾è‘—æ”¹è¿›äº†MLLMçš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚</li>
<li>è¯„ä¼°ç»“æœè¡¨æ˜GCoTåœ¨å¤šç§ç‰¹å®šè§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ¥åœ°ä¿¡æ¯çš„å¼•å…¥å¢å¼ºäº†MLLMå¯¹è¾“å…¥å›¾åƒçš„ç†è§£èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02859">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-63268b48713e5e16c38ec2cd4cc11363.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-210052bd53d25e168868eadd428f352b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5c7e6c060d36cd7bfb618a9fe9cd61e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Requirements-Elicitation-Follow-Up-Question-Generation"><a href="#Requirements-Elicitation-Follow-Up-Question-Generation" class="headerlink" title="Requirements Elicitation Follow-Up Question Generation"></a>Requirements Elicitation Follow-Up Question Generation</h2><p><strong>Authors:Yuchen Shen, Anmol Singhal, Travis Breaux</strong></p>
<p>Interviews are a widely used technique in eliciting requirements to gather stakeholder needs, preferences, and expectations for a software system. Effective interviewing requires skilled interviewers to formulate appropriate interview questions in real time while facing multiple challenges, including lack of familiarity with the domain, excessive cognitive load, and information overload that hinders how humans process stakeholdersâ€™ speech. Recently, large language models (LLMs) have exhibited state-of-the-art performance in multiple natural language processing tasks, including text summarization and entailment. To support interviewers, we investigate the application of GPT-4o to generate follow-up interview questions during requirements elicitation by building on a framework of common interviewer mistake types. In addition, we describe methods to generate questions based on interviewee speech. We report a controlled experiment to evaluate LLM-generated and human-authored questions with minimal guidance, and a second controlled experiment to evaluate the LLM-generated questions when generation is guided by interviewer mistake types. Our findings demonstrate that, for both experiments, the LLM-generated questions are no worse than the human-authored questions with respect to clarity, relevancy, and informativeness. In addition, LLM-generated questions outperform human-authored questions when guided by common mistakes types. This highlights the potential of using LLMs to help interviewers improve the quality and ease of requirements elicitation interviews in real time. </p>
<blockquote>
<p>è®¿è°ˆæ˜¯å¹¿æ³›ä½¿ç”¨çš„æŠ€æœ¯ï¼Œç”¨äºæ¿€å‘éœ€æ±‚ä»¥æ”¶é›†è½¯ä»¶ç³»ç»Ÿçš„åˆ©ç›Šç›¸å…³è€…çš„éœ€æ±‚ã€åå¥½å’ŒæœŸæœ›ã€‚æœ‰æ•ˆçš„è®¿è°ˆéœ€è¦æŠ€èƒ½å¨´ç†Ÿçš„è®¿è°ˆè€…åœ¨é¢å¯¹å¤šä¸ªæŒ‘æˆ˜æ—¶ï¼Œå®æ—¶åˆ¶å®šé€‚å½“çš„è®¿è°ˆé—®é¢˜ï¼ŒåŒ…æ‹¬å¯¹é¢†åŸŸçš„é™Œç”Ÿã€è¿‡åº¦çš„è®¤çŸ¥è´Ÿè·å’Œä¿¡æ¯è¿‡è½½ï¼Œè¿™å¦¨ç¢äº†äººç±»å¤„ç†åˆ©ç›Šç›¸å…³è€…çš„è¨€è¯­ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æ–‡æœ¬æ‘˜è¦å’Œå†…æ¶µæ¨ç†ã€‚ä¸ºäº†æ”¯æŒè®¿è°ˆè€…ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†GPT-4oåœ¨éœ€æ±‚æ¿€å‘è¿‡ç¨‹ä¸­ç”Ÿæˆåç»­è®¿è°ˆé—®é¢˜çš„åº”ç”¨ï¼Œè¿™å»ºç«‹åœ¨å¸¸è§çš„è®¿è°ˆè€…é”™è¯¯ç±»å‹æ¡†æ¶ä¹‹ä¸Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†åŸºäºå—è®¿è€…è¨€è¯­ç”Ÿæˆé—®é¢˜çš„æ–¹æ³•ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†ä¸€é¡¹æ§åˆ¶å®éªŒï¼Œä»¥è¯„ä¼°LLMç”Ÿæˆçš„é—®é¢˜å’Œå‡ ä¹æ— æŒ‡å¯¼çš„äººç±»åˆ›ä½œçš„é—®é¢˜ï¼Œä»¥åŠç¬¬äºŒé¡¹æ§åˆ¶å®éªŒï¼Œä»¥è¯„ä¼°åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å—åˆ°è®¿è°ˆè€…é”™è¯¯ç±»å‹å¼•å¯¼ä¸‹çš„LLMç”Ÿæˆçš„é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯¹äºè¿™ä¸¤é¡¹å®éªŒï¼ŒLLMç”Ÿæˆçš„é—®é¢˜åœ¨æ¸…æ™°åº¦ã€ç›¸å…³æ€§å’Œä¿¡æ¯é‡æ–¹é¢å¹¶ä¸äºšäºäººç±»åˆ›ä½œçš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œå½“å—åˆ°å¸¸è§é”™è¯¯ç±»å‹çš„å¼•å¯¼æ—¶ï¼ŒLLMç”Ÿæˆçš„é—®é¢˜è¡¨ç°ä¼˜äºäººç±»åˆ›ä½œçš„é—®é¢˜ã€‚è¿™çªå‡ºäº†ä½¿ç”¨LLMå¸®åŠ©è®¿è°ˆè€…åœ¨å®æ—¶ä¸­æé«˜éœ€æ±‚æ¿€å‘è®¿è°ˆçš„è´¨é‡å’Œä¾¿æ·æ€§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02858v1">PDF</a> 13 pages, 2 figures, accepted at the 33rd IEEE International   Requirements Engineering 2025</p>
<p><strong>Summary</strong><br>     è®¿è°ˆæ˜¯å¹¿æ³›ä½¿ç”¨çš„æŠ€æœ¯ï¼Œç”¨äºæ¿€å‘éœ€æ±‚ä»¥æ”¶é›†è½¯ä»¶ç³»ç»Ÿçš„åˆ©ç›Šç›¸å…³è€…éœ€æ±‚ã€åå¥½å’ŒæœŸæœ›ã€‚æœ‰æ•ˆè®¿è°ˆéœ€è¦ç†Ÿç»ƒçš„é—®ç­”å¸ˆå®æ—¶åˆ¶å®šé€‚å½“çš„é—®é¢˜ï¼Œå¹¶é¢ä¸´å¤šé‡æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¸ç†Ÿæ‚‰é¢†åŸŸã€è¿‡åº¦è®¤çŸ¥è´Ÿè·å’Œä¿¡æ¯è¿‡è½½ï¼Œé˜»ç¢äº†äººä»¬å¯¹åˆ©ç›Šç›¸å…³è€…è¯è¯­çš„å¤„ç†ã€‚æˆ‘ä»¬æ¢è®¨äº†åŸºäºGPT-4oçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨éœ€æ±‚æ¿€å‘è®¿è°ˆä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œæ—¨åœ¨æ”¯æŒé—®ç­”å¸ˆç”Ÿæˆè·Ÿè¿›é—®é¢˜ã€‚æœ¬æ–‡é€šè¿‡å»ºç«‹å¸¸è§çš„é—®ç­”å¸ˆé”™è¯¯ç±»å‹æ¡†æ¶æ¥æ„å»ºç”Ÿæˆé—®é¢˜çš„æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡å—æ§å®éªŒè¯„ä¼°äº†åœ¨æœ€å°æŒ‡å¯¼ä¸‹LLMç”Ÿæˆçš„é—®é¢˜ä¸äººç±»åˆ›ä½œçš„é—®é¢˜ä¹‹é—´çš„è¡¨ç°å·®å¼‚ï¼Œä»¥åŠæ ¹æ®é—®ç­”å¸ˆé”™è¯¯ç±»å‹æŒ‡å¯¼é—®é¢˜ç”Ÿæˆæ—¶çš„è¡¨ç°å·®å¼‚ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨ä½•ç§å®éªŒä¸­ï¼ŒLLMç”Ÿæˆçš„é—®é¢˜åœ¨æ¸…æ™°åº¦ã€ç›¸å…³æ€§å’Œä¿¡æ¯é‡æ–¹é¢éƒ½ä¸äºšäºäººç±»åˆ›ä½œçš„é—®é¢˜ã€‚åœ¨æŒ‡å¯¼å¸¸è§é”™è¯¯ç±»å‹çš„æƒ…å†µä¸‹ï¼ŒLLMç”Ÿæˆçš„é—®é¢˜è¡¨ç°ä¼˜äºäººç±»åˆ›ä½œçš„é—®é¢˜ã€‚è¿™çªæ˜¾äº†ä½¿ç”¨LLMå®æ—¶å¸®åŠ©é—®ç­”å¸ˆæé«˜éœ€æ±‚æ¿€å‘è®¿è°ˆè´¨é‡å’Œä¾¿æ·æ€§çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¿è°ˆæ˜¯è½¯ä»¶éœ€æ±‚æ”¶é›†çš„å…³é”®æŠ€æœ¯ï¼Œç”¨äºè·å–åˆ©ç›Šç›¸å…³è€…çš„éœ€æ±‚ã€åå¥½å’ŒæœŸæœ›ã€‚</li>
<li>æœ‰æ•ˆè®¿è°ˆéœ€è¦é¢å¯¹å¤šé‡æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é¢†åŸŸä¸ç†Ÿæ‚‰ã€è®¤çŸ¥è´Ÿè·å’Œä¿¡æ¯è¿‡è½½ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚GPT-4oåœ¨ç”Ÿæˆè®¿è°ˆé—®é¢˜æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>é€šè¿‡å»ºç«‹å¸¸è§é—®ç­”å¸ˆé”™è¯¯ç±»å‹æ¡†æ¶æ¥å¢å¼ºLLMç”Ÿæˆé—®é¢˜çš„æ•ˆæœã€‚</li>
<li>LLMç”Ÿæˆçš„é—®é¢˜åœ¨æ¸…æ™°åº¦ã€ç›¸å…³æ€§å’Œä¿¡æ¯é‡æ–¹é¢ä¸äººç±»åˆ›ä½œçš„é—®é¢˜ç›¸å½“ã€‚</li>
<li>åœ¨æŒ‡å¯¼å¸¸è§é”™è¯¯ç±»å‹çš„æƒ…å†µä¸‹ï¼ŒLLMç”Ÿæˆçš„é—®é¢˜è¡¨ç°ä¼˜äºäººç±»åˆ›ä½œçš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02858">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-77b8a5b77527480752fdffebc72be461.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MOTIF-Modular-Thinking-via-Reinforcement-Fine-tuning-in-LLMs"><a href="#MOTIF-Modular-Thinking-via-Reinforcement-Fine-tuning-in-LLMs" class="headerlink" title="MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs"></a>MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs</h2><p><strong>Authors:Purbesh Mitra, Sennur Ulukus</strong></p>
<p>Recent advancements in the reasoning capabilities of large language models (LLMs) show that employing group relative policy optimization (GRPO) algorithm for reinforcement learning (RL) training allows the models to use more thinking&#x2F;reasoning tokens for generating better responses. However, LLMs can generate only a finite amount of tokens while maintaining attention to the previously generated tokens. This limit, also known as the context size of an LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens. To think beyond the limit of context size, an LLM must employ a modular thinking strategy to reason over multiple rounds. In this work, we propose $\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ â€“ an RL training method for generating thinking tokens in multiple rounds, effectively allowing the model to think with additional context size. We trained the open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our experiments show 3.8% and 3.3% improvements over vanilla GRPO based training in the respective benchmarks. Furthermore, this improvement was achieved with only 15% of samples, thus demonstrating sample efficiency of MOTIF. Our code and models are available at <a target="_blank" rel="noopener" href="https://github.com/purbeshmitra/MOTIF">https://github.com/purbeshmitra/MOTIF</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/purbeshmitra/MOTIF">https://huggingface.co/purbeshmitra/MOTIF</a>, respectively. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›å–å¾—äº†è¿›å±•ï¼Œç ”ç©¶è¡¨æ˜ï¼Œé‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒï¼Œå¯ä»¥ä½¿æ¨¡å‹åˆ©ç”¨æ›´å¤šçš„æ€è€ƒ&#x2F;æ¨ç†æ ‡è®°æ¥ç”Ÿæˆæ›´å¥½çš„å›åº”ã€‚ç„¶è€Œï¼ŒLLMåªèƒ½åœ¨ä¿æŒå¯¹å…ˆå‰ç”Ÿæˆæ ‡è®°çš„æ³¨æ„çš„åŒæ—¶ç”Ÿæˆæœ‰é™æ•°é‡çš„æ ‡è®°ã€‚è¿™ä¸ªé™åˆ¶ä¹Ÿè¢«ç§°ä¸ºLLMçš„ä¸Šä¸‹æ–‡å¤§å°ï¼Œæ˜¯åœ¨ä½¿ç”¨ä»»æ„æ•°é‡çš„æ ‡è®°è¿›è¡ŒLLMæ¨ç†æ—¶çš„ç“¶é¢ˆã€‚ä¸ºäº†è¶…è¶Šä¸Šä¸‹æ–‡å¤§å°çš„é™åˆ¶ï¼ŒLLMå¿…é¡»é‡‡ç”¨æ¨¡å—åŒ–æ€è€ƒç­–ç•¥æ¥è¿›è¡Œå¤šè½®æ¨ç†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>MOTIFï¼šé€šè¿‡å¼ºåŒ–å¾®è°ƒå®ç°æ¨¡å—åŒ–æ€è€ƒ</strong>â€”â€”ä¸€ç§ç”¨äºç”Ÿæˆå¤šè½®æ€è€ƒæ ‡è®°çš„RLè®­ç»ƒæ–¹æ³•ï¼Œæœ‰æ•ˆåœ°ä½¿æ¨¡å‹èƒ½å¤Ÿç”¨é¢å¤–çš„ä¸Šä¸‹æ–‡å¤§å°è¿›è¡Œæ€è€ƒã€‚æˆ‘ä»¬åœ¨GSM8Kæ•°æ®é›†ä¸Šå¯¹å¼€æºæ¨¡å‹Qwen2.5-3B-Instructè¿›è¡Œäº†å‚æ•°é«˜æ•ˆçš„å¾®è°ƒè®­ç»ƒï¼Œå¹¶åœ¨MATH500å’ŒAIME2024åŸºå‡†æµ‹è¯•ä¸Šå¯¹å…¶å‡†ç¡®æ€§è¿›è¡Œäº†æµ‹è¯•ã€‚å®éªŒè¡¨æ˜ï¼Œä¸åŸºäºGRPOçš„å¸¸è§„è®­ç»ƒç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„è‡ªåŸºå‡†æµ‹è¯•ä¸­çš„å‡†ç¡®ç‡æé«˜äº†3.8%å’Œ3.3%ã€‚æ­¤å¤–ï¼Œè¿™ä¸€æ”¹è¿›ä»…ä½¿ç”¨äº†15%çš„æ ·æœ¬ï¼Œä»è€Œè¯æ˜äº†MOTIFçš„æ ·æœ¬æ•ˆç‡ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹åˆ†åˆ«å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/purbeshmitra/MOTIF%E5%92%8Chttps://huggingface.co/purbeshmitra/MOTIF%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/purbeshmitra/MOTIFå’Œhttps://huggingface.co/purbeshmitra/MOTIFæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02851v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æœ€æ–°è¿›å±•ï¼Œç ”ç©¶é‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨æ›´å¤šæ€è€ƒ&#x2F;æ¨ç†ä»¤ç‰Œç”Ÿæˆæ›´å¥½çš„å“åº”ã€‚ç„¶è€Œï¼ŒLLMåœ¨ç»´æŒå¯¹å…ˆå‰ç”Ÿæˆä»¤ç‰Œçš„æ³¨æ„æ—¶ï¼Œåªèƒ½ç”Ÿæˆæœ‰é™æ•°é‡çš„ä»¤ç‰Œã€‚è¿™ä¸€é™åˆ¶æ˜¯LLMè¿›è¡Œä»»æ„å¤§é‡ä»¤ç‰Œæ¨ç†çš„ç“¶é¢ˆã€‚æœ¬ç ”ç©¶æå‡ºäº†é€šè¿‡å¼ºåŒ–å¾®è°ƒå®ç°æ¨¡å—åŒ–æ€ç»´çš„MOTIFæ–¹æ³•â€”â€”ä¸€ç§RLè®­ç»ƒç”Ÿæˆå¤šè½®æ€è€ƒä»¤ç‰Œçš„æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°ä½¿æ¨¡å‹èƒ½å¤Ÿæ€è€ƒé¢å¤–çš„ä¸Šä¸‹æ–‡å¤§å°ã€‚æˆ‘ä»¬åœ¨GSM8Kæ•°æ®é›†ä¸Šè®­ç»ƒäº†å¼€æºæ¨¡å‹Qwen2.5-3B-Instructï¼Œå¹¶åœ¨MATH500å’ŒAIME2024åŸºå‡†æµ‹è¯•ä¸Šæµ‹è¯•äº†å…¶å‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä¸åŸºäºGRPOçš„å¸¸è§„è®­ç»ƒç›¸æ¯”ï¼ŒMOTIFåœ¨ç›¸åº”åŸºå‡†æµ‹è¯•ä¸­çš„å‡†ç¡®ç‡æé«˜äº†3.8%å’Œ3.3%ã€‚æ­¤å¤–ï¼Œè¿™ä¸€æ”¹è¿›ä»…ä½¿ç”¨äº†15%çš„æ ·æœ¬ï¼Œæ˜¾ç¤ºäº†MOTIFçš„é«˜æ ·æœ¬æ•ˆç‡ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹åˆ†åˆ«ä½äº<a target="_blank" rel="noopener" href="https://github.com/purbeshmitra/MOTIF%E5%92%8Chttps://huggingface.co/purbeshmitra/MOTIF%E3%80%82">https://github.com/purbeshmitra/MOTIFå’Œhttps://huggingface.co/purbeshmitra/MOTIFã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMé‡‡ç”¨GRPOç®—æ³•å¼ºåŒ–è®­ç»ƒå¯ä»¥æå‡ç”Ÿæˆå“åº”è´¨é‡ã€‚</li>
<li>LLMé¢ä¸´ç”Ÿæˆä»¤ç‰Œæ•°é‡æœ‰é™çš„ç“¶é¢ˆã€‚</li>
<li>MOTIFæ˜¯ä¸€ç§å¤šè½®æ¨ç†ä»¤ç‰Œç”Ÿæˆçš„RLè®­ç»ƒæ–¹æ³•ï¼Œå¢åŠ æ¨¡å‹çš„ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨MATH500å’ŒAIME2024åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMOTIFè¾ƒå¸¸è§„GRPOè®­ç»ƒå‡†ç¡®ç‡æœ‰æ‰€æå‡ã€‚</li>
<li>MOTIFåœ¨æ ·æœ¬æ•ˆç‡æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œä»…ä½¿ç”¨15%æ ·æœ¬å³å–å¾—æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†å¼€æºæ¨¡å‹Qwen2.5-3B-Instructè¿›è¡Œè®­ç»ƒï¼Œå¹¶å…¬å¼€äº†ä»£ç å’Œæ¨¡å‹ä¾›ä»–äººä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02851">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eb2362a2a325b43cb6e1d15e9cb70983.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e92e02b18d116892753304798db90cf5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b4d610b4c44b6c3650c4cbe979d66c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d075a7f093a8e9ea0cc4533ba4da973.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89447d94a23b2f75118426fb3ff6e780.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Visual-Contextual-Attack-Jailbreaking-MLLMs-with-Image-Driven-Context-Injection"><a href="#Visual-Contextual-Attack-Jailbreaking-MLLMs-with-Image-Driven-Context-Injection" class="headerlink" title="Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context   Injection"></a>Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context   Injection</h2><p><strong>Authors:Ziqi Miao, Yi Ding, Lijun Li, Jing Shao</strong></p>
<p>With the emergence of strong visual-language capabilities, multimodal large language models (MLLMs) have demonstrated tremendous potential for real-world applications. However, the security vulnerabilities exhibited by the visual modality pose significant challenges to deploying such models in open-world environments. Recent studies have successfully induced harmful responses from target MLLMs by encoding harmful textual semantics directly into visual inputs. However, in these approaches, the visual modality primarily serves as a trigger for unsafe behavior, often exhibiting semantic ambiguity and lacking grounding in realistic scenarios. In this work, we define a novel setting: visual-centric jailbreak, where visual information serves as a necessary component in constructing a complete and realistic jailbreak context. Building on this setting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates contextual dialogue using four distinct visual-focused strategies, dynamically generating auxiliary images when necessary to construct a visual-centric jailbreak scenario. To maximize attack effectiveness, it incorporates automatic toxicity obfuscation and semantic refinement to produce a final attack prompt that reliably triggers harmful responses from the target black-box MLLMs. Specifically, VisCo achieves a toxicity score of 4.78 and an Attack Success Rate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming the baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Dtc7w3PQ/Visco-Attack">https://github.com/Dtc7w3PQ/Visco-Attack</a>. </p>
<blockquote>
<p>éšç€å¼ºå¤§çš„è§†è§‰è¯­è¨€èƒ½åŠ›çš„å‡ºç°ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œè§†è§‰æ¨¡å¼æ‰€å±•ç°çš„å®‰å…¨æ¼æ´ä¸ºåœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­éƒ¨ç½²æ­¤ç±»æ¨¡å‹å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚æœ€è¿‘çš„ç ”ç©¶å·²ç»æˆåŠŸè¯±å¯¼ç›®æ ‡MLLMsäº§ç”Ÿæœ‰å®³å“åº”ï¼Œé€šè¿‡å°†æœ‰å®³æ–‡æœ¬è¯­ä¹‰ç›´æ¥ç¼–ç åˆ°è§†è§‰è¾“å…¥ä¸­ã€‚ç„¶è€Œï¼Œåœ¨è¿™äº›æ–¹æ³•ä¸­ï¼Œè§†è§‰æ¨¡å¼ä¸»è¦å……å½“ä¸å®‰å…¨è¡Œä¸ºçš„è§¦å‘å› ç´ ï¼Œé€šå¸¸è¡¨ç°å‡ºè¯­ä¹‰æ¨¡ç³Šï¼Œå¹¶ä¸”åœ¨ç°å®åœºæ™¯ä¸­ç¼ºä¹ä¾æ®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ç§æ–°å‹åœºæ™¯ï¼šè§†è§‰ä¸ºä¸­å¿ƒçš„è¶Šç‹±ï¼Œå…¶ä¸­è§†è§‰ä¿¡æ¯åœ¨æ„å»ºå®Œæ•´ä¸”ç°å®çš„è¶Šç‹±ä¸Šä¸‹æ–‡ä¸­å……å½“å¿…è¦ç»„æˆéƒ¨åˆ†ã€‚åŸºäºè¿™ä¸€åœºæ™¯ï¼Œæˆ‘ä»¬æå‡ºäº†VisCoï¼ˆè§†è§‰ä¸Šä¸‹æ–‡ï¼‰æ”»å‡»ã€‚VisCoä½¿ç”¨å››ç§ä¸åŒçš„ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ç­–ç•¥æ¥åˆ¶é€ ä¸Šä¸‹æ–‡å¯¹è¯ï¼Œåœ¨å¿…è¦æ—¶åŠ¨æ€ç”Ÿæˆè¾…åŠ©å›¾åƒï¼Œä»¥æ„å»ºè§†è§‰ä¸ºä¸­å¿ƒçš„è¶Šç‹±åœºæ™¯ã€‚ä¸ºäº†æœ€å¤§åŒ–æ”»å‡»æ•ˆæœï¼Œå®ƒç»“åˆäº†è‡ªåŠ¨çš„æ¯’æ€§æ¨¡ç³Šå¤„ç†å’Œè¯­ä¹‰ç»†åŒ–ï¼Œä»¥äº§ç”Ÿæœ€ç»ˆçš„æ”»å‡»æç¤ºï¼Œä»è€Œå¯é åœ°è§¦å‘ç›®æ ‡é»‘ç›’MLLMsçš„æœ‰å®³å“åº”ã€‚å…·ä½“æ¥è¯´ï¼ŒVisCoåœ¨MM-SafetyBenchä¸Šå¯¹GPT-4oå®ç°çš„æ¯’æ€§å¾—åˆ†ä¸º4.78ï¼Œæ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ä¸º85%ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿ï¼ŒåŸºçº¿çš„æ¯’æ€§å¾—åˆ†ä¸º2.48ï¼ŒASRä¸º22.2%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Dtc7w3PQ/Visco-Attack">https://github.com/Dtc7w3PQ/Visco-Attack</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02844v1">PDF</a> 16 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†è§‰ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å·¨å¤§æ½œåŠ›åŠå…¶å®‰å…¨æ¼æ´æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰ç ”ç©¶ä¸­è§†è§‰æ¨¡æ€ä¸»è¦ç”¨äºè§¦å‘ä¸å®‰å…¨è¡Œä¸ºçš„é—®é¢˜ï¼Œæœ¬æ–‡å®šä¹‰äº†ä¸€ç§æ–°çš„æ”»å‡»åœºæ™¯ï¼šè§†è§‰ä¸ºä¸­å¿ƒè¶Šç‹±ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰ä¸Šä¸‹æ–‡çš„æ”»å‡»æ–¹æ³•VisCoã€‚VisCoåˆ©ç”¨å››ç§ç‹¬ç‰¹çš„è§†è§‰ç­–ç•¥æ„å»ºä¸Šä¸‹æ–‡å¯¹è¯ï¼Œå¹¶åœ¨å¿…è¦æ—¶åŠ¨æ€ç”Ÿæˆè¾…åŠ©å›¾åƒæ¥æ„å»ºçœŸå®çš„è¶Šç‹±åœºæ™¯ã€‚é€šè¿‡è‡ªåŠ¨çš„æ¯’æ€§æ··æ·†å’Œè¯­ä¹‰ä¼˜åŒ–ï¼ŒVisCoèƒ½å¤Ÿæœ‰æ•ˆåœ°è§¦å‘ç›®æ ‡é»‘ç®±MLLMsçš„æ¶æ„å“åº”ã€‚å®ƒåœ¨MM-SafetyBenchä¸Šçš„æ¯’æ€§å¾—åˆ†ä¸º4.78ï¼Œæ”»å‡»æˆåŠŸç‡ä¸º85%ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†å­˜åœ¨å®‰å…¨æ¼æ´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦é€šè¿‡è§†è§‰æ¨¡æ€è§¦å‘MLLMsçš„ä¸å®‰å…¨è¡Œä¸ºï¼Œä½†è¿™ç§æ–¹æ³•å­˜åœ¨è¯­ä¹‰æ¨¡ç³Šå’Œç¼ºä¹å®é™…åœºæ™¯åŸºç¡€çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡å®šä¹‰äº†æ–°çš„æ”»å‡»åœºæ™¯ï¼šè§†è§‰ä¸ºä¸­å¿ƒè¶Šç‹±ï¼Œå¹¶æå‡ºVisCoæ”»å‡»æ–¹æ³•ï¼Œåˆ©ç”¨è§†è§‰ä¸Šä¸‹æ–‡æ„å»ºå¯¹è¯ã€‚</li>
<li>VisCoé€šè¿‡åŠ¨æ€ç”Ÿæˆè¾…åŠ©å›¾åƒæ„å»ºçœŸå®çš„è¶Šç‹±åœºæ™¯ï¼Œå¹¶é‡‡ç”¨è‡ªåŠ¨æ¯’æ€§æ··æ·†å’Œè¯­ä¹‰ä¼˜åŒ–æŠ€æœ¯ã€‚</li>
<li>VisCoåœ¨MM-SafetyBenchä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºå…¶æ¯’æ€§å¾—åˆ†ä¸º4.78ï¼Œæ”»å‡»æˆåŠŸç‡ä¸º85%ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
<li>VisCoæ”»å‡»æ–¹æ³•å…·æœ‰é‡è¦çš„å®é™…æ„ä¹‰ï¼Œæé†’æˆ‘ä»¬éœ€è¦æ³¨æ„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02844">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a4c1896d381528fcfa09e1be9ed5aeb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4701ed76a65b7645287d6492d5a70c31.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="StepHint-Multi-level-Stepwise-Hints-Enhance-Reinforcement-Learning-to-Reason"><a href="#StepHint-Multi-level-Stepwise-Hints-Enhance-Reinforcement-Learning-to-Reason" class="headerlink" title="StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to   Reason"></a>StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to   Reason</h2><p><strong>Authors:Kaiyi Zhang, Ang Lv, Jinpeng Li, Yongbo Wang, Feng Wang, Haoyuan Hu, Rui Yan</strong></p>
<p>Reinforcement learning with verifiable rewards (RLVR) is a promising approach for improving the complex reasoning abilities of large language models (LLMs). However, current RLVR methods face two significant challenges: the near-miss reward problem, where a small mistake can invalidate an otherwise correct reasoning process, greatly hindering training efficiency; and exploration stagnation, where models tend to focus on solutions within their <code>comfort zone,&#39;&#39; lacking the motivation to explore potentially more effective alternatives. To address these challenges, we propose StepHint, a novel RLVR algorithm that utilizes multi-level stepwise hints to help models explore the solution space more effectively. StepHint generates valid reasoning chains from stronger models and partitions these chains into reasoning steps using our proposed adaptive partitioning method. The initial few steps are used as hints, and simultaneously, multiple-level hints (each comprising a different number of steps) are provided to the model. This approach directs the model&#39;s exploration toward a promising solution subspace while preserving its flexibility for independent exploration. By providing hints, StepHint mitigates the near-miss reward problem, thereby improving training efficiency. Additionally, the external reasoning pathways help the model develop better reasoning abilities, enabling it to move beyond its </code>comfort zoneâ€™â€™ and mitigate exploration stagnation. StepHint outperforms competitive RLVR enhancement methods across six mathematical benchmarks, while also demonstrating superior generalization and excelling over baselines on out-of-domain benchmarks. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¯æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤æ‚æ¨ç†èƒ½åŠ›çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œå½“å‰çš„RLVRæ–¹æ³•é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯è¿‘é”™å¥–åŠ±é—®é¢˜ï¼Œå³ä¸€ä¸ªå°é”™è¯¯å¯èƒ½ä¼šä½¿æ­£ç¡®çš„æ¨ç†è¿‡ç¨‹æ— æ•ˆï¼Œæå¤§åœ°é˜»ç¢è®­ç»ƒæ•ˆç‡ï¼›äºŒæ˜¯æ¢ç´¢åœæ»é—®é¢˜ï¼Œæ¨¡å‹å¾€å¾€å±€é™äºå…¶â€œèˆ’é€‚åŒºâ€å†…çš„è§£å†³æ–¹æ¡ˆï¼Œç¼ºä¹æ¢ç´¢å¯èƒ½æ›´æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆçš„åŠ¨æœºã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†StepHintè¿™ä¸€æ–°å‹RLVRç®—æ³•ï¼Œå®ƒåˆ©ç”¨å¤šå±‚æ¬¡é€æ­¥æç¤ºæ¥å¸®åŠ©æ¨¡å‹æ›´æœ‰æ•ˆåœ°æ¢ç´¢è§£ç©ºé—´ã€‚StepHintä»æ›´å¼ºå¤§çš„æ¨¡å‹ä¸­ç”Ÿæˆæœ‰æ•ˆçš„æ¨ç†é“¾ï¼Œå¹¶ä½¿ç”¨æˆ‘ä»¬æå‡ºçš„è‡ªé€‚åº”åˆ†åŒºæ–¹æ³•å°†è¿™äº›é“¾åˆ†å‰²æˆæ¨ç†æ­¥éª¤ã€‚æœ€åˆçš„å‡ ä¸ªæ­¥éª¤è¢«ç”¨ä½œæç¤ºï¼ŒåŒæ—¶å‘æ¨¡å‹æä¾›å¤šä¸ªçº§åˆ«çš„æç¤ºï¼ˆæ¯ä¸ªçº§åˆ«åŒ…å«ä¸åŒæ•°é‡çš„æ­¥éª¤ï¼‰ã€‚è¿™ç§æ–¹æ³•å¯ä»¥å¼•å¯¼æ¨¡å‹çš„æ¢ç´¢æœç€æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆå­ç©ºé—´è¿›è¡Œï¼ŒåŒæ—¶ä¿æŒå…¶ç‹¬ç«‹æ¢ç´¢çš„çµæ´»æ€§ã€‚é€šè¿‡æä¾›æç¤ºï¼ŒStepHintç¼“è§£äº†è¿‘é”™å¥–åŠ±é—®é¢˜ï¼Œä»è€Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚æ­¤å¤–ï¼Œå¤–éƒ¨æ¨ç†è·¯å¾„æœ‰åŠ©äºæ¨¡å‹å‘å±•æ›´å¥½çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿè¶…è¶Šå…¶â€œèˆ’é€‚åŒºâ€ï¼Œå¹¶ç¼“è§£æ¢ç´¢åœæ»é—®é¢˜ã€‚åœ¨å…­ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒStepHintçš„è¡¨ç°ä¼˜äºå…¶ä»–ç«äº‰æ€§RLVRå¢å¼ºæ–¹æ³•ï¼ŒåŒæ—¶æ˜¾ç¤ºå‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨åŸŸå¤–åŸºå‡†æµ‹è¯•ä¸Šè¶…è¶ŠåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02841v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰å¹¿é˜”å‰æ™¯ã€‚ç„¶è€Œï¼Œç°æœ‰RLVRæ–¹æ³•é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šè¿‘é‚»å¥–åŠ±é—®é¢˜ï¼Œå³ä¸€ä¸ªå¾®å°é”™è¯¯å¯èƒ½ç ´åæ­£ç¡®çš„æ¨ç†è¿‡ç¨‹ï¼Œæå¤§å½±å“è®­ç»ƒæ•ˆç‡ï¼›ä»¥åŠæ¢ç´¢åœæ»é—®é¢˜ï¼Œæ¨¡å‹å€¾å‘äºä¸“æ³¨äºå…¶â€œèˆ’é€‚åŒºâ€å†…çš„è§£å†³æ–¹æ¡ˆï¼Œç¼ºä¹æ¢ç´¢æ›´å¤šæœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆçš„åŠ¨æœºã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºStepHintè¿™ä¸€æ–°å‹RLVRç®—æ³•ï¼Œåˆ©ç”¨å¤šå±‚æ¬¡é€æ­¥æç¤ºå¸®åŠ©æ¨¡å‹æ›´æœ‰æ•ˆåœ°æ¢ç´¢è§£ç©ºé—´ã€‚StepHinté€šè¿‡è‡ªé€‚åº”åˆ†åŒºæ–¹æ³•å°†æ›´å¼ºæ¨¡å‹çš„åˆç†é“¾åˆ†å‰²æˆæ¨ç†æ­¥éª¤ã€‚åˆå§‹çš„å‡ ä¸ªæ­¥éª¤ä½œä¸ºæç¤ºï¼ŒåŒæ—¶æä¾›å¤šçº§æç¤ºï¼ˆæ¯ä¸ªåŒ…å«ä¸åŒæ•°é‡çš„æ­¥éª¤ï¼‰ç»™æ¨¡å‹ã€‚è¿™ä¸€æ–¹æ³•å¼•å¯¼æ¨¡å‹æœç€æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆå­ç©ºé—´æ¢ç´¢ï¼ŒåŒæ—¶ä¿æŒå…¶ç‹¬ç«‹æ¢ç´¢çš„çµæ´»æ€§ã€‚é€šè¿‡æä¾›æç¤ºï¼ŒStepHintç¼“è§£äº†è¿‘é‚»å¥–åŠ±é—®é¢˜ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚æ­¤å¤–ï¼Œå¤–éƒ¨æ¨ç†é€”å¾„å¸®åŠ©æ¨¡å‹å‘å±•æ›´å¥½çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿè¶…è¶Šå…¶â€œèˆ’é€‚åŒºâ€ï¼Œå¹¶ç¼“è§£æ¢ç´¢åœæ»é—®é¢˜ã€‚StepHintåœ¨å…­ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜äºå…¶ä»–RLVRå¢å¼ºæ–¹æ³•çš„æ•ˆæœï¼ŒåŒæ—¶åœ¨åŸŸå¤–åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºè‰²çš„æ³›åŒ–æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLVRå¯¹äºæé«˜LLMçš„å¤æ‚æ¨ç†èƒ½åŠ›å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>å½“å‰RLVRæ–¹æ³•é¢ä¸´è¿‘é‚»å¥–åŠ±é—®é¢˜å’Œæ¢ç´¢åœæ»é—®é¢˜ã€‚</li>
<li>StepHintæ˜¯ä¸€ç§æ–°å‹çš„RLVRç®—æ³•ï¼Œé€šè¿‡å¤šå±‚æ¬¡é€æ­¥æç¤ºå¸®åŠ©æ¨¡å‹æ›´æœ‰æ•ˆåœ°æ¢ç´¢è§£ç©ºé—´ã€‚</li>
<li>StepHintåˆ©ç”¨è‡ªé€‚åº”åˆ†åŒºæ–¹æ³•å°†æ¨ç†æ­¥éª¤åˆ†å‰²ï¼Œå¹¶æä¾›åˆå§‹æç¤ºæ¥å¼•å¯¼æ¨¡å‹è®­ç»ƒã€‚</li>
<li>StepHintç¼“è§£äº†è¿‘é‚»å¥–åŠ±é—®é¢˜å¹¶æé«˜è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>StepHintæœ‰åŠ©äºæ¨¡å‹è¶…è¶Šå…¶â€œèˆ’é€‚åŒºâ€ï¼Œå‘å±•æ›´å¥½çš„æ¨ç†èƒ½åŠ›å¹¶ç¼“è§£æ¢ç´¢åœæ»é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02841">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-46cfa5cc531ec0a3d795c75389fb58a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc18672d62d0c7dfbbeb2276b6de2764.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af41ed51955f04bc86311b4a6261fff6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Multimodal-Mathematical-Reasoning-with-Diverse-Solving-Perspective"><a href="#Multimodal-Mathematical-Reasoning-with-Diverse-Solving-Perspective" class="headerlink" title="Multimodal Mathematical Reasoning with Diverse Solving Perspective"></a>Multimodal Mathematical Reasoning with Diverse Solving Perspective</h2><p><strong>Authors:Wenhao Shi, Zhiqiang Hu, Yi Bin, Yang Yang, See-Kiong Ng, Heng Tao Shen</strong></p>
<p>Recent progress in large-scale reinforcement learning (RL) has notably enhanced the reasoning capabilities of large language models (LLMs), especially in mathematical domains. However, current multimodal LLMs (MLLMs) for mathematical reasoning often rely on one-to-one image-text pairs and single-solution supervision, overlooking the diversity of valid reasoning perspectives and internal reflections. In this work, we introduce MathV-DP, a novel dataset that captures multiple diverse solution trajectories for each image-question pair, fostering richer reasoning supervision. We further propose Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and enhanced via group relative policy optimization (GRPO), a rule-based RL approach that integrates correctness discrimination and diversity-aware reward functions. Our method emphasizes learning from varied reasoning perspectives and distinguishing between correct yet distinct solutions. Extensive experiments on the MathVistaâ€™s minitest and Math-V benchmarks demonstrate that Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and generative diversity, highlighting the importance of incorporating diverse perspectives and reflective reasoning in multimodal mathematical reasoning. </p>
<blockquote>
<p>è¿‘æœŸå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„è¿›å±•æ˜¾è‘—æé«˜äº†å…¶æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦é¢†åŸŸã€‚ç„¶è€Œï¼Œå½“å‰ç”¨äºæ•°å­¦æ¨ç†çš„å¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰å¸¸å¸¸ä¾èµ–äºä¸€å¯¹ä¸€çš„å›¾åƒæ–‡æœ¬å¯¹å’Œå•ä¸€è§£å†³æ–¹æ¡ˆçš„ç›‘ç£ï¼Œå¿½ç•¥äº†æœ‰æ•ˆçš„æ¨ç†è§†è§’çš„å¤šæ ·æ€§å’Œå†…éƒ¨åæ€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†MathV-DPï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ•°æ®é›†ï¼Œèƒ½å¤Ÿæ•æ‰æ¯ä¸ªå›¾åƒé—®é¢˜å¯¹çš„å¤šä¸ªå¤šæ ·åŒ–è§£å†³æ–¹æ¡ˆè½¨è¿¹ï¼Œä¸ºæ›´ä¸°å¯Œçš„æ¨ç†ç›‘ç£æä¾›æ”¯æŒã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†Qwen-VL-DPæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºQwen-VLæ„å»ºï¼Œé€šè¿‡ç›‘ç£å­¦ä¹ è¿›è¡Œå¾®è°ƒï¼Œå¹¶é€šè¿‡åŸºäºç»„çš„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å¢å¼ºã€‚GRPOæ˜¯ä¸€ç§åŸºäºè§„åˆ™çš„RLæ–¹æ³•ï¼Œé›†æˆäº†æ­£ç¡®æ€§åˆ¤åˆ«å’Œå¤šæ ·æ€§æ„ŸçŸ¥å¥–åŠ±å‡½æ•°ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼ºè°ƒä»å¤šæ ·åŒ–çš„æ¨ç†è§†è§’å­¦ä¹ ï¼Œå¹¶åŒºåˆ†æ­£ç¡®ä½†ä¸åŒçš„è§£å†³æ–¹æ¡ˆã€‚åœ¨MathVistaçš„å¾®å°æµ‹è¯•å’ŒMath-VåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒQwen-VL-DPåœ¨å‡†ç¡®æ€§å’Œç”Ÿæˆå¤šæ ·æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆå‰çš„åŸºç¡€MLLMï¼Œçªæ˜¾äº†åœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä¸­èå…¥å¤šæ ·è§†è§’å’Œåæ€æ¨ç†çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02804v1">PDF</a> 8 pages</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„è¿›æ­¥æ˜¾è‘—æé«˜äº†æ•°å­¦é¢†åŸŸä¸­çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä¸Šå¾€å¾€ä¾èµ–äºä¸€å¯¹ä¸€çš„å›¾åƒæ–‡æœ¬å¯¹å’Œå•ä¸€è§£å†³æ–¹æ¡ˆçš„ç›‘ç£å­¦ä¹ ï¼Œå¿½è§†äº†æœ‰æ•ˆæ¨ç†è§†è§’çš„å¤šæ ·æ€§å’Œå†…éƒ¨åæ€ã€‚æœ¬ç ”ç©¶å¼•å…¥MathV-DPæ•°æ®é›†ï¼Œé’ˆå¯¹æ¯å¯¹å›¾åƒé—®é¢˜æ•æ‰å¤šä¸ªå¤šæ ·çš„è§£å†³æ–¹æ¡ˆè½¨è¿¹ï¼Œæä¾›ä¸°å¯Œçš„æ¨ç†ç›‘ç£ã€‚è¿›ä¸€æ­¥æå‡ºQwen-VL-DPæ¨¡å‹ï¼ŒåŸºäºQwen-VLæ„å»ºï¼Œé€šè¿‡ç›‘ç£å­¦ä¹ å¾®è°ƒï¼Œå¹¶é‡‡ç”¨åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•â€”â€”ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œå¢å¼ºã€‚è¯¥æ–¹æ³•é‡è§†ä»ä¸åŒæ¨ç†è§†è§’å­¦ä¹ å¹¶åŒºåˆ†æ­£ç¡®ä½†ä¸åŒçš„è§£å†³æ–¹æ¡ˆã€‚åœ¨MathVistaçš„Miniæµ‹è¯•å’ŒMath-VåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒQwen-VL-DPåœ¨å‡†ç¡®æ€§å’Œç”Ÿæˆå¤šæ ·æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆå‰çš„MLLMåŸºç¡€æ¨¡å‹ï¼Œå¼ºè°ƒåœ¨å¤šåª’ä½“æ•°å­¦æ¨ç†ä¸­èå…¥å¤šæ ·è§†è§’å’Œåæ€æ¨ç†çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦é¢†åŸŸçš„æ¨ç†èƒ½åŠ›å¾—åˆ°å¢å¼ºã€‚</li>
<li>å½“å‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä¸Šç¼ºä¹å¤šæ ·æ€§è§†è§’å’Œå†…éƒ¨åæ€ã€‚</li>
<li>å¼•å…¥MathV-DPæ•°æ®é›†ï¼ŒåŒ…å«å¤šæ ·åŒ–çš„è§£å†³æ–¹æ¡ˆè½¨è¿¹ï¼Œä»¥ä¿ƒè¿›ä¸°å¯Œçš„æ¨ç†ç›‘ç£ã€‚</li>
<li>æå‡ºQwen-VL-DPæ¨¡å‹ï¼Œç»“åˆç›‘ç£å­¦ä¹ å’ŒåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆGRPOï¼‰è¿›è¡Œå¢å¼ºã€‚</li>
<li>Qwen-VL-DPæ¨¡å‹é‡è§†ä»ä¸åŒæ¨ç†è§†è§’å­¦ä¹ å¹¶åŒºåˆ†æ­£ç¡®ä½†ä¸åŒçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>åœ¨åŸºå‡†æµ‹è¯•ä¸Šï¼ŒQwen-VL-DPæ¨¡å‹è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œç”Ÿæˆå¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-caf74a71370469e31a06e66ab64769ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c76fd42b8f50b62a7b8a887d7a8d164.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1a1d1d3ece43c265f75e1cd56565b60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13d38d329e91ff4e075dc865670af37a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Is-Reasoning-All-You-Need-Probing-Bias-in-the-Age-of-Reasoning-Language-Models"><a href="#Is-Reasoning-All-You-Need-Probing-Bias-in-the-Age-of-Reasoning-Language-Models" class="headerlink" title="Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language   Models"></a>Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language   Models</h2><p><strong>Authors:Riccardo Cantini, Nicola Gabriele, Alessio Orsino, Domenico Talia</strong></p>
<p>Reasoning Language Models (RLMs) have gained traction for their ability to perform complex, multi-step reasoning tasks through mechanisms such as Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these capabilities promise improved reliability, their impact on robustness to social biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark, originally designed for Large Language Models (LLMs), to investigate the adversarial robustness of RLMs to bias elicitation. We systematically evaluate state-of-the-art RLMs across diverse sociocultural dimensions, using an LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak techniques to assess the strength of built-in safety mechanisms. Our evaluation addresses three key questions: (i) how the introduction of reasoning capabilities affects model fairness and robustness; (ii) whether models fine-tuned for reasoning exhibit greater safety than those relying on CoT prompting at inference time; and (iii) how the success rate of jailbreak attacks targeting bias elicitation varies with the reasoning mechanisms employed. Our findings reveal a nuanced relationship between reasoning capabilities and bias safety. Surprisingly, models with explicit reasoning, whether via CoT prompting or fine-tuned reasoning traces, are generally more vulnerable to bias elicitation than base models without such mechanisms, suggesting reasoning may unintentionally open new pathways for stereotype reinforcement. Reasoning-enabled models appear somewhat safer than those relying on CoT prompting, which are particularly prone to contextual reframing attacks through storytelling prompts, fictional personas, or reward-shaped instructions. These results challenge the assumption that reasoning inherently improves robustness and underscore the need for more bias-aware approaches to reasoning design. </p>
<blockquote>
<p>æ¨ç†è¯­è¨€æ¨¡å‹ï¼ˆRLMsï¼‰é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæˆ–ç²¾ç»†è°ƒæ•´åçš„æ¨ç†è½¨è¿¹ç­‰æœºåˆ¶ï¼Œèƒ½å¤Ÿæ‰§è¡Œå¤æ‚çš„å¤šæ­¥æ¨ç†ä»»åŠ¡ï¼Œå› æ­¤å—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚å°½ç®¡è¿™äº›åŠŸèƒ½æé«˜äº†å¯é æ€§ï¼Œä½†å®ƒä»¬å¯¹ç¤¾ä¼šåè§ç¨³å¥æ€§çš„å½±å“ä»ç„¶ä¸æ¸…æ¥šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨åŸæœ¬ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®¾è®¡çš„CLEAR-BiasåŸºå‡†æµ‹è¯•ï¼Œæ¥ç ”ç©¶RLMså¯¹æŠ—åè§å¼•å‘çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†æœ€å…ˆè¿›çš„RLMsåœ¨å¤šç§ç¤¾ä¼šæ–‡åŒ–ç»´åº¦ä¸Šçš„è¡¨ç°ï¼Œé‡‡ç”¨LLMä½œä¸ºæ³•å®˜çš„æ–¹æ³•è¿›è¡Œè‡ªåŠ¨åŒ–å®‰å…¨è¯„åˆ†ï¼Œå¹¶åˆ©ç”¨è¶Šç‹±æŠ€æœ¯æ¥è¯„ä¼°å†…ç½®å®‰å…¨æœºåˆ¶çš„å¼ºåº¦ã€‚æˆ‘ä»¬çš„è¯„ä¼°è§£å†³äº†ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼šï¼ˆiï¼‰æ¨ç†èƒ½åŠ›çš„å¼•å…¥å¦‚ä½•å½±å“æ¨¡å‹çš„å…¬å¹³æ€§å’Œç¨³å¥æ€§ï¼›ï¼ˆiiï¼‰ä¸ä¾èµ–CoTæç¤ºè¿›è¡Œæ¨æ–­çš„æ¨¡å‹ç›¸æ¯”ï¼Œç»è¿‡ç²¾ç»†è°ƒæ•´çš„æ¨ç†æ¨¡å‹æ˜¯å¦è¡¨ç°å‡ºæ›´é«˜çš„å®‰å…¨æ€§ï¼›ä»¥åŠï¼ˆiiiï¼‰é’ˆå¯¹åè§å¼•å‘çš„æ”»å‡»ï¼Œé‡‡ç”¨ä¸åŒæ¨ç†æœºåˆ¶çš„è¶Šç‹±æ”»å‡»æˆåŠŸç‡å¦‚ä½•å˜åŒ–ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°æ¨ç†èƒ½åŠ›ä¸åè§å®‰å…¨ä¹‹é—´æœ‰ç€å¾®å¦™çš„å…³ç³»ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæ— è®ºæ˜¯é€šè¿‡CoTæç¤ºè¿˜æ˜¯ç²¾ç»†è°ƒæ•´çš„æ¨ç†è½¨è¿¹ï¼Œå…·æœ‰æ˜ç¡®æ¨ç†èƒ½åŠ›çš„æ¨¡å‹é€šå¸¸æ›´å®¹æ˜“å—åˆ°åè§çš„å½±å“ï¼Œè¿™è¡¨æ˜æ¨ç†å¯èƒ½ä¼šæ— æ„ä¸­æ‰“å¼€åˆ»æ¿å°è±¡å¼ºåŒ–çš„æ–°é€”å¾„ã€‚ä¸ä¾èµ–CoTæç¤ºçš„æ¨¡å‹ç›¸æ¯”ï¼Œå…·å¤‡æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ä¼¼ä¹æ›´å®‰å…¨ä¸€äº›ï¼Œå°¤å…¶æ˜¯åœ¨é¢å¯¹é€šè¿‡æ•…äº‹æç¤ºã€è™šæ„è§’è‰²æˆ–å¥–åŠ±å¯¼å‘æŒ‡ä»¤è¿›è¡Œä¸Šä¸‹æ–‡é‡æ„æ”»å‡»æ—¶è¡¨ç°å‡ºè¾ƒé«˜çš„è„†å¼±æ€§ã€‚è¿™äº›ç»“æœæŒ‘æˆ˜äº†æ¨ç†å›ºæœ‰æé«˜ç¨³å¥æ€§çš„å‡è®¾ï¼Œå¹¶å¼ºè°ƒéœ€è¦æ›´å¤šåå‘ç†æ€§çš„æ–¹æ³•æ¥è®¾è®¡æ¨ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02799v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ‘˜è¦ä¸»è¦æ¢è®¨äº†Reasoning Language Modelsï¼ˆRLMsï¼‰å¯¹ç¤¾ä¼šåè§å½±å“çš„ç¨³å¥æ€§ã€‚ç ”ç©¶åˆ©ç”¨ä¸“ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹è®¾è®¡çš„CLEAR-Bias benchmarkæ¥è¯„ä¼°RLMså¯¹æŠ—åè§æ¿€å‘çš„ç¨³å¥æ€§ã€‚ç ”ç©¶ç³»ç»Ÿåœ°è¯„ä¼°äº†æœ€å…ˆè¿›çš„RLMsåœ¨ä¸åŒç¤¾ä¼šæ–‡åŒ–ç»´åº¦ä¸Šçš„è¡¨ç°ï¼Œåˆ©ç”¨LLMä½œä¸ºæ³•å®˜çš„æ–¹æ³•æ¥è¿›è¡Œè‡ªåŠ¨åŒ–å®‰å…¨è¯„åˆ†å¹¶åˆ©ç”¨è¶Šç‹±æŠ€æœ¯æ¥è¯„ä¼°å†…ç½®å®‰å…¨æœºåˆ¶çš„å¼ºåº¦ã€‚ç ”ç©¶å‘ç°ï¼Œå¼•å…¥æ¨ç†èƒ½åŠ›å¦‚ä½•å½±å“æ¨¡å‹çš„å…¬å¹³æ€§å’Œç¨³å¥æ€§ï¼›ç»è¿‡æ¨ç†å¾®è°ƒåçš„æ¨¡å‹æ˜¯å¦æ¯”åœ¨æ¨ç†æ—¶ä¾èµ–Chain-of-Thoughtï¼ˆCoTï¼‰æç¤ºå±•ç°å‡ºæ›´é«˜çš„å®‰å…¨æ€§ï¼›é’ˆå¯¹åè§æ¿€å‘çš„è¶Šç‹±æ”»å‡»æˆåŠŸç‡ä¸æ‰€ä½¿ç”¨çš„æ¨ç†æœºåˆ¶çš„å…³ç³»å¦‚ä½•ã€‚ç ”ç©¶å‘ç°æ¨ç†èƒ½åŠ›ä¸åè§å®‰å…¨ä¹‹é—´å­˜åœ¨å¾®å¦™å…³ç³»ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œé€šè¿‡CoTæç¤ºæˆ–ç»è¿‡ç²¾ç»†è°ƒæ•´çš„æ¨ç†è½¨è¿¹æ˜¾ç¤ºå‡ºçš„æ˜ç¡®æ¨ç†æ¨¡å‹é€šå¸¸æ¯”æ²¡æœ‰è¿™äº›æœºåˆ¶çš„åŸºå‡†æ¨¡å‹æ›´å®¹æ˜“å—åˆ°åè§æ¿€å‘çš„å½±å“ï¼Œè¿™è¡¨æ˜æ¨ç†å¯èƒ½ä¼šæ— æ„ä¸­æ‰“å¼€åˆ»æ¿å°è±¡å¼ºåŒ–çš„æ–°é€”å¾„ã€‚ç›¸è¾ƒäºä¾èµ–CoTæç¤ºçš„æ¨¡å‹ï¼Œä½¿ç”¨æ¨ç†åŠŸèƒ½çš„æ¨¡å‹ç›¸å¯¹æ›´å®‰å…¨ä¸€äº›ï¼Œåè€…ç‰¹åˆ«å®¹æ˜“å—åˆ°é€šè¿‡æ•…äº‹æç¤ºã€è™šæ„è§’è‰²æˆ–å¥–åŠ±å¯¼å‘æŒ‡ä»¤è¿›è¡Œä¸Šä¸‹æ–‡é‡æ„æ”»å‡»çš„å½±å“ã€‚è¿™äº›ç»“æœæŒ‘æˆ˜äº†æ¨ç†å›ºæœ‰æé«˜ç¨³å¥æ€§çš„å‡è®¾ï¼Œå¹¶å¼ºè°ƒäº†éœ€è¦æ›´å¤šåè§æ„è¯†çš„æ¨ç†è®¾è®¡ç­–ç•¥ã€‚</p>
<p><strong>å…³é”®å‘ç°</strong></p>
<ol>
<li>å¼•å…¥æ¨ç†èƒ½åŠ›ä¼šå½±å“æ¨¡å‹çš„å…¬å¹³æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>é€šè¿‡Chain-of-Thoughtï¼ˆCoTï¼‰æç¤ºæˆ–ç²¾ç»†è°ƒæ•´çš„æ¨ç†è½¨è¿¹æ˜¾ç¤ºçš„æ˜ç¡®æ¨ç†æ¨¡å‹æ›´å®¹æ˜“å—åˆ°åè§æ¿€å‘çš„å½±å“ã€‚</li>
<li>ä¸ä¾èµ–CoTæç¤ºçš„æ¨¡å‹ç›¸æ¯”ï¼Œä½¿ç”¨æ¨ç†åŠŸèƒ½çš„æ¨¡å‹ç›¸å¯¹æ›´å®‰å…¨ã€‚</li>
<li>ä¸Šä¸‹æ–‡é‡æ„æ”»å‡»ï¼ˆå¦‚é€šè¿‡æ•…äº‹æç¤ºã€è™šæ„è§’è‰²æˆ–å¥–åŠ±å¯¼å‘æŒ‡ä»¤ï¼‰å¯¹æ¨¡å‹çš„ç¨³å¥æ€§æ„æˆå¨èƒã€‚</li>
<li>æ¨ç†æœºåˆ¶å¯èƒ½æ— æ„ä¸­å¼ºåŒ–åˆ»æ¿å°è±¡ã€‚</li>
<li>éœ€è¦æ›´å¤šçš„ç ”ç©¶æ¥æ·±å…¥äº†è§£å¦‚ä½•ç»“åˆæ¨ç†èƒ½åŠ›å’Œæ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02799">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fb54283f52b269b43c9f5057ec4a0a42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27cc000c112eab2c0b5273f0afafe001.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="From-Long-Videos-to-Engaging-Clips-A-Human-Inspired-Video-Editing-Framework-with-Multimodal-Narrative-Understanding"><a href="#From-Long-Videos-to-Engaging-Clips-A-Human-Inspired-Video-Editing-Framework-with-Multimodal-Narrative-Understanding" class="headerlink" title="From Long Videos to Engaging Clips: A Human-Inspired Video Editing   Framework with Multimodal Narrative Understanding"></a>From Long Videos to Engaging Clips: A Human-Inspired Video Editing   Framework with Multimodal Narrative Understanding</h2><p><strong>Authors:Xiangfeng Wang, Xiao Li, Yadong Wei, Xueyu Song, Yang Song, Xiaoqiang Xia, Fangrui Zeng, Zaiyi Chen, Liu Liu, Gu Xu, Tong Xu</strong></p>
<p>The rapid growth of online video content, especially on short video platforms, has created a growing demand for efficient video editing techniques that can condense long-form videos into concise and engaging clips. Existing automatic editing methods predominantly rely on textual cues from ASR transcripts and end-to-end segment selection, often neglecting the rich visual context and leading to incoherent outputs. In this paper, we propose a human-inspired automatic video editing framework (HIVE) that leverages multimodal narrative understanding to address these limitations. Our approach incorporates character extraction, dialogue analysis, and narrative summarization through multimodal large language models, enabling a holistic understanding of the video content. To further enhance coherence, we apply scene-level segmentation and decompose the editing process into three subtasks: highlight detection, opening&#x2F;ending selection, and pruning of irrelevant content. To facilitate research in this area, we introduce DramaAD, a novel benchmark dataset comprising over 800 short drama episodes and 500 professionally edited advertisement clips. Experimental results demonstrate that our framework consistently outperforms existing baselines across both general and advertisement-oriented editing tasks, significantly narrowing the quality gap between automatic and human-edited videos. </p>
<blockquote>
<p>åœ¨çº¿è§†é¢‘å†…å®¹çš„å¿«é€Ÿå¢é•¿ï¼Œç‰¹åˆ«æ˜¯åœ¨çŸ­è§†é¢‘å¹³å°ä¸Šï¼Œå·²ç»äº§ç”Ÿäº†å¯¹é«˜æ•ˆè§†é¢‘ç¼–è¾‘æŠ€æœ¯çš„æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ï¼Œè¿™äº›æŠ€æœ¯èƒ½å¤Ÿå°†é•¿è§†é¢‘æµ“ç¼©æˆç®€æ´å¸å¼•äººçš„ç‰‡æ®µã€‚ç°æœ‰çš„è‡ªåŠ¨ç¼–è¾‘æ–¹æ³•ä¸»è¦ä¾èµ–äºASRè½¬å½•çš„æ–‡æœ¬çº¿ç´¢å’Œç«¯åˆ°ç«¯çš„ç‰‡æ®µé€‰æ‹©ï¼Œå¾€å¾€å¿½è§†äº†ä¸°å¯Œçš„è§†è§‰ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´è¾“å‡ºå†…å®¹ç¼ºä¹è¿è´¯æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå—äººç±»å¯å‘çš„è‡ªåŠ¨è§†é¢‘ç¼–è¾‘æ¡†æ¶ï¼ˆHIVEï¼‰ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤šæ¨¡å¼å™äº‹ç†è§£æ¥è§£å†³è¿™äº›é™åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†å­—ç¬¦æå–ã€å¯¹è¯åˆ†æå’Œå™äº‹æ‘˜è¦ï¼Œé€šè¿‡å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ç°å¯¹è§†é¢‘å†…å®¹çš„æ•´ä½“ç†è§£ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜è¿è´¯æ€§ï¼Œæˆ‘ä»¬åº”ç”¨åœºæ™¯çº§åˆ†å‰²ï¼Œå¹¶å°†ç¼–è¾‘è¿‡ç¨‹åˆ†è§£ä¸ºä¸‰ä¸ªå­ä»»åŠ¡ï¼šé«˜å…‰æ£€æµ‹ã€å¼€å¤´&#x2F;ç»“å°¾é€‰æ‹©ï¼Œä»¥åŠåˆ é™¤æ— å…³å†…å®¹ã€‚ä¸ºäº†ä¿ƒè¿›è¯¥é¢†åŸŸçš„ç ”ç©¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†DramaADï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡800ä¸ªçŸ­å‰§ç‰‡æ®µå’Œ500ä¸ªä¸“ä¸šç¼–è¾‘çš„å¹¿å‘Šç‰‡æ®µã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨é€šç”¨å’Œé¢å‘å¹¿å‘Šçš„ç¼–è¾‘ä»»åŠ¡ä¸Šéƒ½ä¼˜äºç°æœ‰åŸºçº¿ï¼Œæ˜¾è‘—ç¼©å°äº†è‡ªåŠ¨ç¼–è¾‘è§†é¢‘å’Œäººç±»ç¼–è¾‘è§†é¢‘ä¹‹é—´çš„è´¨é‡å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02790v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨çº¿è§†é¢‘å†…å®¹çš„è¿…é€Ÿå¢é•¿ï¼Œå°¤å…¶æ˜¯çŸ­è§†é¢‘å¹³å°ä¸Šçš„å†…å®¹ï¼Œå¯¹é«˜æ•ˆçš„è§†é¢‘ç¼–è¾‘æŠ€æœ¯æå‡ºäº†æ›´é«˜çš„è¦æ±‚ï¼Œéœ€è¦å°†é•¿è§†é¢‘æµ“ç¼©æˆç®€æ´ã€å¼•äººå…¥èƒœçš„ç‰‡æ®µã€‚ç°æœ‰è‡ªåŠ¨ç¼–è¾‘æ–¹æ³•ä¸»è¦ä¾èµ–ASRæ–‡æœ¬çš„æ–‡æœ¬çº¿ç´¢å’Œç«¯åˆ°ç«¯ç‰‡æ®µé€‰æ‹©ï¼Œå¿½è§†äº†ä¸°å¯Œçš„è§†è§‰è¯­å¢ƒï¼Œå¯¼è‡´è¾“å‡ºå†…å®¹ä¸è¿è´¯ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å€Ÿé‰´äººç±»ç¼–è¾‘æ€è·¯çš„è‡ªåŠ¨è§†é¢‘ç¼–è¾‘æ¡†æ¶ï¼ˆHIVEï¼‰ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å¤šæ¨¡æ€å™äº‹ç†è§£æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚ç»“åˆäººç‰©æå–ã€å¯¹è¯åˆ†æå’Œå™äº‹æ‘˜è¦ï¼Œé€šè¿‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ç°å¯¹è§†é¢‘å†…å®¹çš„æ•´ä½“ç†è§£ã€‚ä¸ºæé«˜è¿è´¯æ€§ï¼Œåº”ç”¨åœºæ™¯çº§åˆ†å‰²ï¼Œå°†ç¼–è¾‘è¿‡ç¨‹åˆ†è§£ä¸ºä¸‰ä¸ªå­ä»»åŠ¡ï¼šé«˜å…‰æ£€æµ‹ã€å¼€å¤´&#x2F;ç»“å°¾é€‰æ‹©å’Œä¸ç›¸å…³å†…å®¹çš„ä¿®å‰ªã€‚ä¸ºæ¨è¿›è¯¥é¢†åŸŸç ”ç©¶ï¼Œæœ¬æ–‡å¼•å…¥äº†DramaADæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡800ä¸ªçŸ­ç‰‡æˆå‰§å’Œ500ä¸ªä¸“ä¸šç¼–è¾‘çš„å¹¿å‘Šç‰‡æ®µã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸€èˆ¬å’Œå¹¿å‘Šå¯¼å‘çš„ç¼–è¾‘ä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå¤§å¹…ç¼©å°äº†è‡ªåŠ¨ç¼–è¾‘å’Œäººç±»ç¼–è¾‘è§†é¢‘çš„è´¨é‡å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨çº¿è§†é¢‘å†…å®¹çš„å¢é•¿æ¨åŠ¨äº†é«˜æ•ˆè§†é¢‘ç¼–è¾‘æŠ€æœ¯çš„éœ€æ±‚ã€‚</li>
<li>ç°æœ‰è‡ªåŠ¨ç¼–è¾‘æ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬çº¿ç´¢å’Œç«¯åˆ°ç«¯ç‰‡æ®µé€‰æ‹©ï¼Œå­˜åœ¨è§†è§‰è¯­å¢ƒå¿½è§†çš„é—®é¢˜ã€‚</li>
<li>æå‡ºçš„HIVEæ¡†æ¶ç»“åˆå¤šæ¨¡æ€å™äº‹ç†è§£ï¼Œè§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>HIVEæ¡†æ¶ç»“åˆäººç‰©æå–ã€å¯¹è¯åˆ†æå’Œå™äº‹æ‘˜è¦ï¼Œå®ç°è§†é¢‘å†…å®¹çš„å…¨é¢ç†è§£ã€‚</li>
<li>ä¸ºæé«˜è¿è´¯æ€§ï¼ŒHIVEæ¡†æ¶é‡‡ç”¨åœºæ™¯çº§åˆ†å‰²ï¼Œåˆ†è§£ç¼–è¾‘ä»»åŠ¡ä¸ºä¸‰ä¸ªå­ä»»åŠ¡ã€‚</li>
<li>å¼•å…¥DramaADæ•°æ®é›†ï¼Œä¿ƒè¿›è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02790">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a5acacebabf89c699c80f97bd98637ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8edf50be34f76ad5c99d49852d09615.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43a86a7a01143ee8b105350c746c210f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91ce1cffcab23913634b944fdcda72b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04d1e8dae9e9f79e689ddec4ca7aa6d5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DeSTA2-5-Audio-Toward-General-Purpose-Large-Audio-Language-Model-with-Self-Generated-Cross-Modal-Alignment"><a href="#DeSTA2-5-Audio-Toward-General-Purpose-Large-Audio-Language-Model-with-Self-Generated-Cross-Modal-Alignment" class="headerlink" title="DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with   Self-Generated Cross-Modal Alignment"></a>DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with   Self-Generated Cross-Modal Alignment</h2><p><strong>Authors:Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, Chao-Han Huck Yang, Sung-Feng Huang, Chih-Kai Yang, Chee-En Yu, Chun-Wei Chen, Wei-Chih Chen, Chien-yu Huang, Yi-Cheng Lin, Yu-Xiang Lin, Chi-An Fu, Chun-Yi Kuan, Wenze Ren, Xuanjun Chen, Wei-Ping Huang, En-Pei Hu, Tzu-Quan Lin, Yuan-Kuei Wu, Kuan-Po Huang, Hsiao-Ying Huang, Huang-Cheng Chou, Kai-Wei Chang, Cheng-Han Chiang, Boris Ginsburg, Yu-Chiang Frank Wang, Hung-yi Lee</strong></p>
<p>We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model (LALM) designed for robust auditory perception and instruction-following, without requiring task-specific audio instruction-tuning. Recent LALMs typically augment Large Language Models (LLMs) with auditory capabilities by training on large-scale, manually curated or LLM-synthesized audio-instruction datasets. However, these approaches have often suffered from the catastrophic forgetting of the LLMâ€™s original language abilities. To address this, we revisit the data construction pipeline and propose DeSTA, a self-generated cross-modal alignment strategy in which the backbone LLM generates its own training targets. This approach preserves the LLMâ€™s native language proficiency while establishing effective audio-text alignment, thereby enabling zero-shot generalization without task-specific tuning. Using DeSTA, we construct DeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training samples derived from 7,000 hours of audio spanning 50 diverse datasets, including speech, environmental sounds, and music. DeSTA2.5-Audio achieves state-of-the-art or competitive performance across a wide range of audio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA, Speech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate that our self-generated strategy outperforms widely adopted data construction and training strategies in both auditory perception and instruction-following capabilities. Our findings underscore the importance of carefully designed data construction in LALM development and offer practical insights for building robust, general-purpose LALMs. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†DeSTA2.5-Audioï¼Œè¿™æ˜¯ä¸€æ¬¾é€šç”¨çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰ï¼Œæ—¨åœ¨å®ç°ç¨³å¥çš„å¬è§‰æ„ŸçŸ¥å’ŒæŒ‡ä»¤éµå¾ªï¼Œè€Œæ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„éŸ³é¢‘æŒ‡ä»¤è¿›è¡Œè°ƒæ•´ã€‚æœ€è¿‘çš„LALMé€šå¸¸é€šè¿‡åœ¨å¤§è§„æ¨¡ã€æ‰‹å·¥æ•´ç†æˆ–LLMåˆæˆçš„éŸ³é¢‘æŒ‡ä»¤æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¬è§‰èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€ä¼šå¯¼è‡´LLMçš„åŸå§‹è¯­è¨€èƒ½åŠ›çš„ç¾éš¾æ€§é—å¿˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†æ•°æ®æ„å»ºæµç¨‹ï¼Œæå‡ºäº†DeSTAï¼Œè¿™æ˜¯ä¸€ç§è‡ªæˆ‘ç”Ÿæˆçš„è·¨æ¨¡æ€å¯¹é½ç­–ç•¥ï¼Œå…¶ä¸­éª¨å¹²LLMç”Ÿæˆè‡ªå·±çš„è®­ç»ƒç›®æ ‡ã€‚è¿™ç§æ–¹æ³•ä¿ç•™äº†LLMçš„æ¯è¯­ç†Ÿç»ƒç¨‹åº¦ï¼ŒåŒæ—¶å®ç°äº†æœ‰æ•ˆçš„éŸ³é¢‘æ–‡æœ¬å¯¹é½ï¼Œä»è€Œå®ç°äº†é›¶æ ·æœ¬æ³›åŒ–ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡è°ƒæ•´ã€‚ä½¿ç”¨DeSTAï¼Œæˆ‘ä»¬æ„å»ºäº†DeSTA-AQA5Mï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€ä»»åŠ¡é€šç”¨çš„æ•°æ®é›†ï¼ŒåŒ…å«500ä¸‡è®­ç»ƒæ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬æ¥æºäº7000å°æ—¶éŸ³é¢‘ï¼Œè·¨è¶Š50ä¸ªä¸åŒæ•°æ®é›†ï¼ŒåŒ…æ‹¬è¯­éŸ³ã€ç¯å¢ƒå£°éŸ³å’ŒéŸ³ä¹ã€‚DeSTA2.5-Audioåœ¨å¹¿æ³›çš„éŸ³é¢‘è¯­è¨€åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³æˆ–å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒ…æ‹¬Dynamic-SUPERBã€MMAUã€SAKURAã€Speech-IFEvalå’ŒVoiceBenchã€‚ç»¼åˆå¯¹æ¯”ç ”ç©¶è¡¨æ˜ï¼Œæˆ‘ä»¬çš„è‡ªæˆ‘ç”Ÿæˆç­–ç•¥åœ¨å¬è§‰æ„ŸçŸ¥å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›æ–¹é¢ä¼˜äºå¹¿æ³›é‡‡ç”¨çš„æ•°æ®æ„å»ºå’ŒåŸ¹è®­ç­–ç•¥ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†ç²¾å¿ƒè®¾è®¡çš„æ•°æ®æ„å»ºåœ¨LALMå‘å±•ä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæ„å»ºç¨³å¥ã€é€šç”¨çš„LALMæä¾›äº†å®è·µè§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02768v1">PDF</a> Model and code available at:   <a target="_blank" rel="noopener" href="https://github.com/kehanlu/DeSTA2.5-Audio">https://github.com/kehanlu/DeSTA2.5-Audio</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>DeSTA2.5-Audioæ˜¯ä¸€ç§é€šç”¨çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰ï¼Œæ—¨åœ¨å®ç°ç¨³å¥çš„å¬è§‰æ„ŸçŸ¥å’ŒæŒ‡ä»¤éµå¾ªï¼Œè€Œæ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„éŸ³é¢‘æŒ‡ä»¤è¿›è¡Œè°ƒæ•´ã€‚è¯¥æ¨¡å‹é€šè¿‡é‡æ–°å®¡è§†æ•°æ®æ„å»ºç®¡é“å¹¶æå‡ºDeSTAï¼ˆä¸€ç§è‡ªæˆ‘ç”Ÿæˆçš„è·¨æ¨¡æ€å¯¹é½ç­–ç•¥ï¼‰ï¼Œå…¶ä¸­éª¨å¹²LLMç”Ÿæˆè‡ªå·±çš„è®­ç»ƒç›®æ ‡ï¼Œä»è€Œè§£å†³äº†LLMåŸæœ‰è¯­è¨€èƒ½åŠ›çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚ä½¿ç”¨DeSTAæ„å»ºçš„å¤§å‹ã€ä»»åŠ¡æ— å…³æ•°æ®é›†DeSTA-AQA5Mï¼ŒåŒ…å«ä»7000å°æ—¶éŸ³é¢‘ä¸­æ´¾ç”Ÿçš„500ä¸‡è®­ç»ƒæ ·æœ¬ï¼Œè·¨è¶Š50ä¸ªä¸åŒçš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬è¯­éŸ³ã€ç¯å¢ƒå£°éŸ³å’ŒéŸ³ä¹ã€‚DeSTA2.5-Audioåœ¨å¤šç§éŸ³é¢‘è¯­è¨€åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æˆ–å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒ…æ‹¬Dynamic-SUPERBã€MMAUã€SAKURAã€Speech-IFEvalå’ŒVoiceBenchã€‚ç»¼åˆå¯¹æ¯”ç ”ç©¶è¡¨æ˜ï¼Œè‡ªæˆ‘ç”Ÿæˆç­–ç•¥åœ¨å¬è§‰æ„ŸçŸ¥å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›æ–¹é¢ä¼˜äºå¹¿æ³›é‡‡ç”¨çš„æ•°æ®æ„å»ºå’ŒåŸ¹è®­ç­–ç•¥ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†ç²¾å¿ƒè®¾è®¡çš„æ•°æ®æ„å»ºåœ¨LALMå‘å±•ä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæ„å»ºç¨³å¥ã€é€šç”¨çš„LALMæä¾›äº†å®é™…è§è§£ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>DeSTA2.5-Audioæ˜¯ä¸€ç§é€šç”¨çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰ï¼Œèƒ½è¿›è¡Œç¨³å¥çš„å¬è§‰æ„ŸçŸ¥å’ŒæŒ‡ä»¤éµå¾ªã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡è‡ªæˆ‘ç”Ÿæˆçš„è·¨æ¨¡æ€å¯¹é½ç­–ç•¥ï¼ˆDeSTAï¼‰è§£å†³LLMåŸæœ‰è¯­è¨€èƒ½åŠ›çš„é—å¿˜é—®é¢˜ã€‚</li>
<li>DeSTA-AQA5Mæ•°æ®é›†ç”¨äºè®­ç»ƒDeSTA2.5-Audioæ¨¡å‹ï¼ŒåŒ…å«ä»7000å°æ—¶éŸ³é¢‘æ´¾ç”Ÿçš„500ä¸‡è®­ç»ƒæ ·æœ¬ã€‚</li>
<li>DeSTA2.5-Audioåœ¨å¤šä¸ªéŸ³é¢‘è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç»¼åˆå¯¹æ¯”ç ”ç©¶è¡¨æ˜ï¼Œè‡ªæˆ‘ç”Ÿæˆç­–ç•¥åœ¨å¬è§‰æ„ŸçŸ¥å’ŒæŒ‡ä»¤éµå¾ªæ–¹é¢ä¼˜äºå…¶ä»–ç­–ç•¥ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†ç²¾å¿ƒè®¾è®¡çš„æ•°æ®æ„å»ºåœ¨LALMå‘å±•ä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02768">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0c4bac782af11c12b6789d286e30c9e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a0f4d6df48b5e4186469cbd78ba28ab.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Meta-SecAlign-A-Secure-Foundation-LLM-Against-Prompt-Injection-Attacks"><a href="#Meta-SecAlign-A-Secure-Foundation-LLM-Against-Prompt-Injection-Attacks" class="headerlink" title="Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks"></a>Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks</h2><p><strong>Authors:Sizhe Chen, Arman Zharmagambetov, David Wagner, Chuan Guo</strong></p>
<p>Prompt injection attacks pose a significant security threat to LLM-integrated applications. Model-level defenses have shown strong effectiveness, but are currently deployed into commercial-grade models in a closed-source manner. We believe open-source models are needed by the AI security community, where co-development of attacks and defenses through open research drives scientific progress in mitigation against prompt injection attacks. To this end, we develop Meta SecAlign, the first open-source and open-weight LLM with built-in model-level defense that achieves commercial-grade model performance. We provide complete details of our training recipe, which utilizes an improved version of the SOTA SecAlign defense. Evaluations on 9 utility benchmarks and 7 security benchmarks show that Meta SecAlign, despite being trained on a generic instruction-tuning dataset, confers security in unseen downstream tasks, including tool-calling and agentic web navigation, in addition general instruction-following. Our best model â€“ Meta-SecAlign-70B â€“ achieves state-of-the-art robustness against prompt injection attacks and comparable utility to closed-source commercial LLM with model-level defense. </p>
<blockquote>
<p>æŒ‡ä»¤æ³¨å…¥æ”»å‡»å¯¹é›†æˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åº”ç”¨æ„æˆé‡å¤§å®‰å…¨å¨èƒã€‚æ¨¡å‹å±‚é¢çš„é˜²å¾¡å·²ç»æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æœ‰æ•ˆæ€§ï¼Œä½†ç›®å‰æ˜¯ä»¥é—­æºçš„æ–¹å¼éƒ¨ç½²åœ¨å•†ä¸šçº§æ¨¡å‹ä¸­ã€‚æˆ‘ä»¬ç›¸ä¿¡AIå®‰å…¨ç¤¾åŒºéœ€è¦å¼€æºæ¨¡å‹ï¼Œé€šè¿‡å¼€æ”¾ç ”ç©¶å…±åŒå¼€å‘æ”»å‡»å’Œé˜²å¾¡ï¼Œæ¨åŠ¨ç¼“è§£æŒ‡ä»¤æ³¨å…¥æ”»å‡»çš„ç§‘ç ”è¿›å±•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†Meta SecAlignï¼Œè¿™æ˜¯ä¸€ä¸ªé¦–ä¸ªå¼€æºä¸”å…¬å¼€æƒé‡çš„å…·æœ‰å†…ç½®æ¨¡å‹çº§é˜²å¾¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ç°äº†å•†ä¸šçº§æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬æä¾›äº†è®­ç»ƒé£Ÿè°±çš„å®Œæ•´ç»†èŠ‚ï¼Œåˆ©ç”¨æ”¹è¿›çš„ç‰ˆæœ¬SOTA SecAligné˜²å¾¡ç­–ç•¥ã€‚åœ¨9ä¸ªå®ç”¨åŸºå‡†æµ‹è¯•å’Œ7ä¸ªå®‰å…¨åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMeta SecAlignå³ä½¿åœ¨é€šç”¨æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä¹Ÿèƒ½åœ¨æœªè§è¿‡çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­æä¾›å®‰å…¨æ€§ï¼ŒåŒ…æ‹¬å·¥å…·è°ƒç”¨å’Œä»£ç†ç½‘é¡µå¯¼èˆªï¼Œä»¥åŠä¸€èˆ¬çš„æŒ‡ä»¤éµå¾ªã€‚æˆ‘ä»¬æœ€å¥½çš„æ¨¡å‹â€”â€”Meta-SecAlign-70Bâ€”â€”å®ç°äº†é’ˆå¯¹æŒ‡ä»¤æ³¨å…¥æ”»å‡»çš„ä¸šç•Œæœ€å‰æ²¿ç¨³å¥æ€§ï¼Œä¸å…·æœ‰æ¨¡å‹çº§é˜²å¾¡çš„é—­æºå•†ä¸šå¤§å‹è¯­è¨€æ¨¡å‹çš„å®ç”¨æ€§ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02735v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ¨¡å‹çº§åˆ«çš„é˜²å¾¡å¯¹äºLLMé›†æˆåº”ç”¨çš„å®‰å…¨è‡³å…³é‡è¦ã€‚ç›®å‰ï¼Œæ¨¡å‹çº§åˆ«çš„é˜²å¾¡æªæ–½ä»¥é—­æºæ–¹å¼éƒ¨ç½²åœ¨å•†ä¸šçº§æ¨¡å‹ä¸­ï¼Œé˜»ç¢äº†AIå®‰å…¨ç¤¾åŒºçš„å‘å±•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåä¸ºMeta SecAlignçš„å¼€æºã€å…¬å¼€æƒé‡LLMæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·æœ‰å†…ç½®çš„æ¨¡å‹çº§åˆ«é˜²å¾¡åŠŸèƒ½ï¼Œå¹¶å®ç°äº†å•†ä¸šçº§æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„è®­ç»ƒé…æ–¹ç»è¿‡æ”¹è¿›ï¼Œé‡‡ç”¨äº†æœ€å…ˆè¿›çš„SecAligné˜²å¾¡æŠ€æœ¯ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒMeta SecAlignåœ¨æœªè§è¿‡çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­å…·æœ‰å®‰å…¨æ€§ï¼ŒåŒ…æ‹¬å·¥å…·è°ƒç”¨å’Œè‡ªä¸»ç½‘é¡µå¯¼èˆªï¼Œä»¥åŠä¸€èˆ¬çš„æŒ‡ä»¤éµå¾ªã€‚æœ€å¥½çš„æ¨¡å‹Meta-SecAlign-70Båœ¨æŠµå¾¡æç¤ºæ³¨å…¥æ”»å‡»æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œå¹¶ä¸”åœ¨å…·æœ‰æ¨¡å‹çº§åˆ«é˜²å¾¡çš„é—­æºå•†ä¸šLLMä¸­å…·æœ‰ç›¸å½“çš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡å‹çº§åˆ«çš„é˜²å¾¡å¯¹LLMé›†æˆåº”ç”¨çš„å®‰å…¨è‡³å…³é‡è¦ã€‚</li>
<li>ç›®å‰å¤§å¤šæ•°æ¨¡å‹çº§åˆ«çš„é˜²å¾¡æªæ–½æ˜¯é—­æºçš„ï¼Œé˜»ç¢äº†AIå®‰å…¨ç¤¾åŒºçš„å¼€æ”¾ç ”ç©¶å’Œå‘å±•ã€‚</li>
<li>Meta SecAlignæ˜¯ä¸€ä¸ªå…·æœ‰å†…ç½®æ¨¡å‹çº§åˆ«é˜²å¾¡åŠŸèƒ½çš„å¼€æºLLMæ¨¡å‹ï¼Œå…·æœ‰å•†ä¸šçº§æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>Meta SecAligné‡‡ç”¨äº†æœ€å…ˆè¿›çš„SecAligné˜²å¾¡æŠ€æœ¯çš„æ”¹è¿›ç‰ˆè®­ç»ƒé…æ–¹ã€‚</li>
<li>Meta SecAlignåœ¨æœªè§è¿‡çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­å…·æœ‰å®‰å…¨æ€§ï¼ŒåŒ…æ‹¬å·¥å…·è°ƒç”¨å’Œè‡ªä¸»ç½‘é¡µå¯¼èˆªç­‰ã€‚</li>
<li>Meta-SecAlign-70Bæ¨¡å‹åœ¨æŠµå¾¡æç¤ºæ³¨å…¥æ”»å‡»æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02735">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0b992be8eea6a003113d7ae8b392771f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e55f95cb14394f1055b9551c375779a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92c8f7165051b401b4d521a567cfc09f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6409894c548a87f78763a87f337aefe.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Bourbaki-Self-Generated-and-Goal-Conditioned-MDPs-for-Theorem-Proving"><a href="#Bourbaki-Self-Generated-and-Goal-Conditioned-MDPs-for-Theorem-Proving" class="headerlink" title="Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving"></a>Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving</h2><p><strong>Authors:Matthieu Zimmer, Xiaotong Ji, Rasul Tutunov, Anthony Bordg, Jun Wang, Haitham Bou Ammar</strong></p>
<p>Reasoning remains a challenging task for large language models (LLMs), especially within the logically constrained environment of automated theorem proving (ATP), due to sparse rewards and the vast scale of proofs. These challenges are amplified in benchmarks like PutnamBench, which contains university-level problems requiring complex, multi-step reasoning. To address this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new framework in which agents generate and pursue their subgoals based on the evolving proof state. Given this more structured generation of goals, the resulting problem becomes more amenable to search. We then apply Monte Carlo Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B) solves 26 problems, achieving new state-of-the-art results with models at this scale. </p>
<blockquote>
<p>æ¨ç†å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨é€»è¾‘çº¦æŸç¯å¢ƒä¸‹çš„è‡ªåŠ¨å®šç†è¯æ˜ï¼ˆATPï¼‰ï¼Œè¿™æ˜¯ç”±äºå¥–åŠ±ç¨€ç–å’Œè¯æ˜çš„å¤§è§„æ¨¡æ€§ã€‚åœ¨åƒPutnamBenchè¿™æ ·çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¿™äº›æŒ‘æˆ˜æ›´åŠ æ”¾å¤§ï¼Œå…¶ä¸­åŒ…å«éœ€è¦å¤æ‚å¤šæ­¥éª¤æ¨ç†çš„å¤§å­¦çº§åˆ«é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªç”Ÿæˆç›®æ ‡æ¡ä»¶MDPï¼ˆsG-MDPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œå…¶ä¸­ä»£ç†æ ¹æ®ä¸æ–­å˜åŒ–çš„è¯æ˜çŠ¶æ€ç”Ÿæˆå’Œè¿½æ±‚å…¶å­ç›®æ ‡ã€‚é‰´äºè¿™ç§æ›´æœ‰ç»“æ„çš„ç”Ÿæˆç›®æ ‡ï¼Œäº§ç”Ÿçš„é—®é¢˜å˜å¾—æ›´é€‚åˆæœç´¢ã€‚ç„¶åï¼Œæˆ‘ä»¬åº”ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ç­‰ç®—æ³•æ¥è§£å†³sG-MDPé—®é¢˜ï¼Œåœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­åº”ç”¨Bourbakiï¼ˆ7Bï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–ç³»ç»Ÿï¼Œå¯ä»¥ç”¨äºå­ç›®æ ‡ç”Ÿæˆå’Œæˆ˜æœ¯åˆæˆï¼Œå¹¶èƒ½é›†æˆå¤šä¸ª7B LLMã€‚åœ¨PutnamBenchä¸Šï¼ŒBourbakiï¼ˆ7Bï¼‰è§£å†³äº†26ä¸ªé—®é¢˜ï¼Œåœ¨å¦‚æ­¤è§„æ¨¡çš„æ¨¡å‹ä¸­å–å¾—äº†æœ€æ–°çš„æœ€å…ˆè¿›çš„æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02726v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€»è¾‘æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå°¤å…¶åœ¨è‡ªåŠ¨åŒ–å®šç†è¯æ˜ï¼ˆATPï¼‰çš„é€»è¾‘çº¦æŸç¯å¢ƒä¸­ï¼Œå› ä¸ºè¯æ˜çš„å¥–åŠ±ç¨€ç–ä¸”è§„æ¨¡åºå¤§ã€‚é’ˆå¯¹æ­¤ç±»æŒ‘æˆ˜ï¼Œåœ¨åƒPutnamBenchè¿™æ ·çš„åŸºå‡†æµ‹è¯•ä¸­å°¤å…¶çªå‡ºï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªæˆ‘ç”Ÿæˆçš„ç›®æ ‡æ¡ä»¶é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆsG-MDPï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œä»£ç†å¯ä»¥æ ¹æ®ä¸æ–­å˜åŒ–çš„è¯æ˜çŠ¶æ€ç”Ÿæˆå’Œè¿½æ±‚å…¶å­ç›®æ ‡ã€‚è¿™ä½¿å¾—é—®é¢˜æ›´é€‚åˆæœç´¢è§£å†³ã€‚ç„¶åï¼Œæˆ‘ä»¬åº”ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ç­‰ç®—æ³•æ¥è§£å†³sG-MDPé—®é¢˜ï¼Œåœ¨Bourbakiï¼ˆ7Bï¼‰ç³»ç»Ÿä¸­å®ç°äº†è¿™ä¸€æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–ç³»ç»Ÿï¼Œå¯ä»¥ç”¨äºç”Ÿæˆå­ç›®æ ‡å’Œæˆ˜æœ¯åˆæˆã€‚åœ¨PutnamBenchä¸Šï¼ŒBourbakiï¼ˆ7Bï¼‰è§£å†³äº†26ä¸ªé—®é¢˜ï¼Œå®ç°äº†æ­¤è§„æ¨¡æ¨¡å‹çš„æ–°ä¸–ç•Œçºªå½•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–å®šç†è¯æ˜ï¼ˆATPï¼‰ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºè¯æ˜çš„å¥–åŠ±ç¨€ç–å’Œè§„æ¨¡åºå¤§ã€‚</li>
<li>sG-MDPæ¡†æ¶è¢«å¼•å…¥ï¼Œå…è®¸ä»£ç†æ ¹æ®è¯æ˜çŠ¶æ€çš„æ¼”å˜ç”Ÿæˆå’Œè¿½æ±‚å­ç›®æ ‡ã€‚</li>
<li>sG-MDPä½¿å¾—é—®é¢˜æ›´æ˜“äºæœç´¢è§£å†³ã€‚</li>
<li>é‡‡ç”¨äº†ç±»ä¼¼äºè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰çš„ç®—æ³•æ¥è§£å†³sG-MDPé—®é¢˜ã€‚</li>
<li>Bourbakiï¼ˆ7Bï¼‰ç³»ç»Ÿå®ç°äº†è¿™ä¸€æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–ç³»ç»Ÿï¼Œç”¨äºç”Ÿæˆå­ç›®æ ‡å’Œæˆ˜æœ¯åˆæˆã€‚</li>
<li>Bourbakiï¼ˆ7Bï¼‰åœ¨PutnamBenchåŸºå‡†æµ‹è¯•ä¸­è§£å†³äº†26ä¸ªé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02726">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8aceb35d8729930e15e118fd2f244952.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e683bf8113e8c870f9f8539129151d3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="AIGI-Holmes-Towards-Explainable-and-Generalizable-AI-Generated-Image-Detection-via-Multimodal-Large-Language-Models"><a href="#AIGI-Holmes-Towards-Explainable-and-Generalizable-AI-Generated-Image-Detection-via-Multimodal-Large-Language-Models" class="headerlink" title="AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image   Detection via Multimodal Large Language Models"></a>AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image   Detection via Multimodal Large Language Models</h2><p><strong>Authors:Ziyin Zhou, Yunpeng Luo, Yuanchen Wu, Ke Sun, Jiayi Ji, Ke Yan, Shouhong Ding, Xiaoshuai Sun, Yunsheng Wu, Rongrong Ji</strong></p>
<p>The rapid development of AI-generated content (AIGC) technology has led to the misuse of highly realistic AI-generated images (AIGI) in spreading misinformation, posing a threat to public information security. Although existing AIGI detection techniques are generally effective, they face two issues: 1) a lack of human-verifiable explanations, and 2) a lack of generalization in the latest generation technology. To address these issues, we introduce a large-scale and comprehensive dataset, Holmes-Set, which includes the Holmes-SFTSet, an instruction-tuning dataset with explanations on whether images are AI-generated, and the Holmes-DPOSet, a human-aligned preference dataset. Our work introduces an efficient data annotation method called the Multi-Expert Jury, enhancing data generation through structured MLLM explanations and quality control via cross-model evaluation, expert defect filtering, and human preference modification. In addition, we propose Holmes Pipeline, a meticulously designed three-stage training framework comprising visual expert pre-training, supervised fine-tuning, and direct preference optimization. Holmes Pipeline adapts multimodal large language models (MLLMs) for AIGI detection while generating human-verifiable and human-aligned explanations, ultimately yielding our model AIGI-Holmes. During the inference stage, we introduce a collaborative decoding strategy that integrates the model perception of the visual expert with the semantic reasoning of MLLMs, further enhancing the generalization capabilities. Extensive experiments on three benchmarks validate the effectiveness of our AIGI-Holmes. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•å¯¼è‡´é«˜åº¦é€¼çœŸçš„AIç”Ÿæˆå›¾åƒï¼ˆAIGIï¼‰è¢«æ»¥ç”¨ï¼Œä¼ æ’­é”™è¯¯ä¿¡æ¯ï¼Œå¯¹å…¬ä¼—ä¿¡æ¯å®‰å…¨æ„æˆå¨èƒã€‚å°½ç®¡ç°æœ‰çš„AIGIæ£€æµ‹æŠ€æœ¯é€šå¸¸æœ‰æ•ˆï¼Œä½†å®ƒä»¬é¢ä¸´ä¸¤ä¸ªé—®é¢˜ï¼š1ï¼‰ç¼ºä¹å¯éªŒè¯çš„äººä¸ºè§£é‡Šï¼›2ï¼‰æœ€æ–°æŠ€æœ¯ä¸­ç¼ºä¹é€šç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤§è§„æ¨¡ä¸”ç»¼åˆçš„æ•°æ®é›†Holmes-Setï¼Œå…¶ä¸­åŒ…æ‹¬Holmes-SFTSetï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¦æœ‰è§£é‡ŠæŒ‡ä»¤çš„æ•°æ®é›†ï¼Œè§£é‡Šå›¾åƒæ˜¯å¦æ˜¯AIç”Ÿæˆçš„ï¼Œä»¥åŠHolmes-DPOSetï¼Œè¿™æ˜¯ä¸€ä¸ªä¸äººç±»å¯¹é½çš„åå¥½æ•°æ®é›†ã€‚æˆ‘ä»¬çš„å·¥ä½œå¼•å…¥äº†ä¸€ç§é«˜æ•ˆçš„æ•°æ®æ³¨é‡Šæ–¹æ³•ï¼Œç§°ä¸ºå¤šä¸“å®¶é™ªå®¡å›¢ï¼Œé€šè¿‡ç»“æ„åŒ–çš„MLLMè§£é‡Šå’Œè´¨é‡æ§åˆ¶ï¼ˆåŒ…æ‹¬è·¨æ¨¡å‹è¯„ä¼°ã€ä¸“å®¶ç¼ºé™·è¿‡æ»¤å’Œäººç±»åå¥½ä¿®æ”¹ï¼‰å¢å¼ºæ•°æ®ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ç²¾å¿ƒè®¾è®¡çš„ä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶Holmes Pipelineï¼ŒåŒ…æ‹¬è§†è§‰ä¸“å®¶é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒä»¥åŠç›´æ¥åå¥½ä¼˜åŒ–ã€‚Holmes Pipelineä½¿å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€‚åº”AIGIæ£€æµ‹ï¼ŒåŒæ—¶ç”Ÿæˆå¯éªŒè¯çš„ã€ä¸äººç±»å¯¹é½çš„è§£é‡Šï¼Œæœ€ç»ˆäº§ç”Ÿæˆ‘ä»¬çš„æ¨¡å‹AIGI-Holmesã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ååŒè§£ç ç­–ç•¥ï¼Œå°†è§†è§‰ä¸“å®¶çš„æ¨¡å‹æ„ŸçŸ¥ä¸MLLMsçš„è¯­ä¹‰æ¨ç†ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„é€šç”¨åŒ–èƒ½åŠ›ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„AIGI-Holmesçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02664v1">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong><br>     äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•å¯¼è‡´äººå·¥æ™ºèƒ½ç”Ÿæˆå›¾åƒï¼ˆAIGIï¼‰è¢«æ¶æ„ç”¨äºä¼ æ’­è™šå‡ä¿¡æ¯ï¼Œå¨èƒå…¬ä¼—ä¿¡æ¯å®‰å…¨ã€‚ä¸ºè§£å†³ç°æœ‰AIGIæ£€æµ‹æŠ€æœ¯åœ¨äººç±»å¯éªŒè¯è§£é‡Šå’Œæœ€æ–°æŠ€æœ¯é€šç”¨æ€§æ–¹é¢çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤§è§„æ¨¡ç»¼åˆæ•°æ®é›†Holmes-Setï¼ŒåŒ…æ‹¬å¸¦æœ‰å›¾åƒæ˜¯å¦AIç”Ÿæˆè§£é‡Šçš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†Holmes-SFTSetå’Œäººç±»å¯¹é½åå¥½æ•°æ®é›†Holmes-DPOSetã€‚é€šè¿‡æå‡ºæœ‰æ•ˆçš„æ•°æ®æ³¨é‡Šæ–¹æ³•â€”â€”å¤šä¸“å®¶é™ªå®¡å›¢ï¼Œæˆ‘ä»¬å¢å¼ºäº†æ•°æ®çš„ç”Ÿæˆï¼Œé€šè¿‡ç»“æ„åŒ–MLLMè§£é‡Šå’Œè´¨é‡æ§åˆ¶ï¼ˆåŒ…æ‹¬è·¨æ¨¡å‹è¯„ä¼°ã€ä¸“å®¶ç¼ºé™·è¿‡æ»¤å’Œäººç±»åå¥½ä¿®æ”¹ï¼‰ï¼Œæˆ‘ä»¬æ¨è¿›äº†æ•°æ®ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç²¾å¿ƒè®¾è®¡äº†Holmesç®¡é“ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è§†è§‰ä¸“å®¶é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒä»¥åŠç›´æ¥åå¥½ä¼˜åŒ–çš„ä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¨¡å‹AIGI-Holmesåˆ©ç”¨è¯¥ç®¡é“é€‚åº”äº†å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”¨äºAIGIæ£€æµ‹ï¼Œç”Ÿæˆäº†äººç±»å¯éªŒè¯ä¸”ä¸äººç±»å¯¹é½çš„è§£é‡Šã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†åä½œè§£ç ç­–ç•¥ï¼Œå°†æ¨¡å‹çš„è§†è§‰ä¸“å®¶æ„ŸçŸ¥ä¸MLLMsçš„è¯­ä¹‰æ¨ç†ç›¸ç»“åˆï¼Œå¢å¼ºäº†é€šç”¨æ€§ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†AIGI-Holmesçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI-generated content (AIGC)æŠ€æœ¯çš„å‘å±•å¯¼è‡´AIç”Ÿæˆçš„å›¾åƒï¼ˆAIGIï¼‰è¢«ç”¨äºä¼ æ’­è¯¯å¯¼æ€§ä¿¡æ¯ï¼Œå¸¦æ¥å…¬å…±å®‰å…¨é£é™©ã€‚</li>
<li>ç°æœ‰AIGIæ£€æµ‹æŠ€æœ¯ç¼ºä¹äººç±»å¯éªŒè¯çš„è§£é‡Šå’Œæœ€æ–°æŠ€æœ¯çš„é€šç”¨æ€§ã€‚</li>
<li>å¼•å…¥å¤§è§„æ¨¡ç»¼åˆæ•°æ®é›†Holmes-Setä»¥è§£å†³è¿™äº›é—®é¢˜ï¼ŒåŒ…æ‹¬æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†å’Œäººç±»å¯¹é½åå¥½æ•°æ®é›†ã€‚</li>
<li>æå‡ºå¤šä¸“å®¶é™ªå®¡å›¢æ•°æ®æ³¨é‡Šæ–¹æ³•ï¼Œå¢å¼ºæ•°æ®ç”Ÿæˆå’Œè´¨é‡æ§åˆ¶ã€‚</li>
<li>ç²¾å¿ƒè®¾è®¡çš„Holmesç®¡é“åŒ…å«è§†è§‰ä¸“å®¶é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒå’Œç›´æ¥åå¥½ä¼˜åŒ–ç­‰é˜¶æ®µã€‚</li>
<li>æ¨¡å‹AIGI-Holmesåˆ©ç”¨Holmesç®¡é“é€‚åº”äº†å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”¨äºAIGIæ£€æµ‹ï¼Œå¯ä»¥ç”Ÿæˆäººç±»å¯éªŒè¯ä¸”ä¸äººç±»å¯¹é½çš„è§£é‡Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02664">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ff882394e8ece8fd1136685570a4caf2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36a2d26239472c644959493de853681e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-555ab8b2cdca9007b241b61c16d388c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42a0f3ca8b6587a61ff2f5bad2640090.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd7ea3ea1f82521b7ce150fefa86af9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1eb76b23e2f25f705108e13a066f2f86.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Scaling-LLM-Planning-NL2FLOW-for-Parametric-Problem-Generation-and-Rigorous-Evaluation"><a href="#Scaling-LLM-Planning-NL2FLOW-for-Parametric-Problem-Generation-and-Rigorous-Evaluation" class="headerlink" title="Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and   Rigorous Evaluation"></a>Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and   Rigorous Evaluation</h2><p><strong>Authors:Jungkoo Kang</strong></p>
<p>Progress in enhancing large language model (LLM) planning and reasoning capabilities is significantly hampered by the bottleneck of scalable, reliable data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully automated system for parametrically generating planning problems - expressed in natural language, a structured intermediate representation, and formal PDDL - and rigorously evaluating the quality of generated plans. I demonstrate NL2FLOWâ€™s capabilities by generating a dataset of 2296 problems in the automated workflow generation domain and evaluating multiple open-sourced, instruct-tuned LLMs. My results reveal that the highest performing models achieved 86% success in generating valid plans and 69% in generating optimal plans, specifically for problems with feasible solutions. Regression analysis shows that the influence of problem characteristics on plan generation is contingent on both model and prompt design. Notably, I observed that the highest success rate for translating natural language into a JSON representation of a plan was lower than the highest rate of generating a valid plan directly. This suggests that unnecessarily decomposing the reasoning task - introducing intermediate translation steps - may actually degrade performance, implying a benefit to models capable of reasoning directly from natural language to action. As I scale LLM reasoning to increasingly complex problems, the bottlenecks and sources of error within these systems will inevitably shift. Therefore, a dynamic understanding of these limitations - and the tools to systematically reveal them - will be crucial for unlocking the full potential of LLMs as intelligent problem solvers. </p>
<blockquote>
<p>åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§„åˆ’å’Œæ¨ç†èƒ½åŠ›æ–¹é¢ï¼Œå¯æ‰©å±•ä¸”å¯é çš„æ•°æ®ç”Ÿæˆä¸è¯„ä¼°çš„ç“¶é¢ˆå¯¹å…¶é€ æˆäº†é‡å¤§é˜»ç¢ã€‚ä¸ºäº†å…‹æœè¿™ä¸€éš¾é¢˜ï¼Œæˆ‘å¼•å…¥äº†NL2FLOWï¼Œè¿™æ˜¯ä¸€ä¸ªå‚æ•°åŒ–ç”Ÿæˆè§„åˆ’é—®é¢˜çš„å…¨è‡ªåŠ¨ç³»ç»Ÿï¼Œèƒ½å¤Ÿä»¥è‡ªç„¶è¯­è¨€ã€ç»“æ„åŒ–ä¸­é—´è¡¨ç¤ºå’Œæ­£å¼PDDLè¡¨è¾¾é—®é¢˜ï¼Œå¹¶ä¸¥æ ¼è¯„ä¼°ç”Ÿæˆè®¡åˆ’çš„è´¨é‡ã€‚æˆ‘é€šè¿‡ç”ŸæˆåŒ…å«2296ä¸ªé—®é¢˜çš„æ•°æ®é›†ï¼Œåœ¨è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ç”Ÿæˆé¢†åŸŸå±•ç¤ºäº†NL2FLOWçš„èƒ½åŠ›ï¼Œå¹¶è¯„ä¼°äº†å¤šä¸ªå¼€æºçš„ã€ç»è¿‡æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æˆ‘çš„ç»“æœæ˜¾ç¤ºï¼Œè¡¨ç°æœ€ä½³çš„æ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ•ˆè®¡åˆ’æ–¹é¢è¾¾åˆ°äº†86%çš„æˆåŠŸç‡ï¼Œåœ¨ç”Ÿæˆæœ€ä¼˜è®¡åˆ’æ–¹é¢è¾¾åˆ°äº†69%ï¼Œè¿™ä¸»è¦æ˜¯é’ˆå¯¹æœ‰å¯è¡Œè§£å†³æ–¹æ¡ˆçš„é—®é¢˜ã€‚å›å½’åˆ†æè¡¨æ˜ï¼Œé—®é¢˜ç‰¹æ€§å¯¹è®¡åˆ’ç”Ÿæˆçš„å½±å“å–å†³äºæ¨¡å‹å’Œæç¤ºè®¾è®¡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘è§‚å¯Ÿåˆ°å°†è‡ªç„¶è¯­è¨€è½¬åŒ–ä¸ºè®¡åˆ’JSONè¡¨ç¤ºå½¢å¼çš„æœ€é«˜æˆåŠŸç‡ä½äºç›´æ¥ç”Ÿæˆæœ‰æ•ˆè®¡åˆ’çš„æœ€é«˜æˆåŠŸç‡ã€‚è¿™è¡¨æ˜ï¼Œä¸å¿…è¦åœ°åˆ†è§£æ¨ç†ä»»åŠ¡ï¼ˆå¼•å…¥ä¸­é—´ç¿»è¯‘æ­¥éª¤ï¼‰å¯èƒ½ä¼šé™ä½æ€§èƒ½ï¼Œæš—ç¤ºé‚£äº›èƒ½å¤Ÿç›´æ¥ä»è‡ªç„¶è¯­è¨€è¿›è¡Œæ¨ç†çš„æ¨¡å‹å…·æœ‰ä¼˜åŠ¿ã€‚éšç€æˆ‘å°†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ‰©å±•åˆ°è¶Šæ¥è¶Šå¤æ‚çš„é—®é¢˜ï¼Œè¿™äº›ç³»ç»Ÿä¸­çš„ç“¶é¢ˆå’Œé”™è¯¯æ¥æºä¹Ÿå¿…å°†å‘ç”Ÿå˜åŒ–ã€‚å› æ­¤ï¼Œå¯¹è¿™äº›é™åˆ¶çš„åŠ¨æ€ç†è§£ä»¥åŠæ­ç¤ºå®ƒä»¬çš„å·¥å…·ï¼Œå¯¹äºè§£é”å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ™ºèƒ½é—®é¢˜æ±‚è§£å™¨çš„æ½œåŠ›è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02253v1">PDF</a> 20 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†NL2FLOWç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿæ—¨åœ¨é€šè¿‡å‚æ•°åŒ–ç”Ÿæˆè§„åˆ’é—®é¢˜ï¼Œå…‹æœå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§„åˆ’å’Œæ¨ç†èƒ½åŠ›æ–¹é¢çš„æå‡ç“¶é¢ˆã€‚ä½œè€…é€šè¿‡ç”ŸæˆåŒ…å«è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ç”ŸæˆåŸŸçš„æ•°æ®é›†å¹¶å¯¹å¤šä¸ªå¼€æºçš„ã€ç»è¿‡æŒ‡ä»¤è°ƒä¼˜çš„LLMè¿›è¡Œè¯„ä¼°ï¼Œå±•ç¤ºäº†NL2FLOWçš„èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæœ€ä¼˜æ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ•ˆè®¡åˆ’å’Œæœ€ä¼˜è®¡åˆ’æ–¹é¢çš„æˆåŠŸç‡åˆ†åˆ«ä¸º86%å’Œ69%ï¼Œä¸”å›å½’åˆ†ææ˜¾ç¤ºé—®é¢˜ç‰¹æ€§å¯¹è®¡åˆ’ç”Ÿæˆçš„å½±å“å–å†³äºæ¨¡å‹å’Œæç¤ºè®¾è®¡ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œä»è‡ªç„¶è¯­è¨€ç›´æ¥è¿›è¡Œæ¨ç†çš„èƒ½åŠ›å¯èƒ½ä¼˜äºéœ€è¦ç»è¿‡ä¸­é—´ç¿»è¯‘æ­¥éª¤çš„æ–¹æ³•ã€‚éšç€LLMæ¨ç†åº”ç”¨äºè¶Šæ¥è¶Šå¤æ‚çš„é—®é¢˜ï¼Œå¯¹å…¶å±€é™æ€§çš„åŠ¨æ€ç†è§£å’Œæ­ç¤ºè¿™äº›å±€é™æ€§çš„å·¥å…·å°†æ˜¯è§£é”LLMä½œä¸ºæ™ºèƒ½é—®é¢˜æ±‚è§£å™¨æ½œåŠ›çš„é‡è¦å…³é”®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NL2FLOWç³»ç»Ÿèƒ½å¤Ÿè‡ªåŠ¨åŒ–ç”Ÿæˆè§„åˆ’é—®é¢˜ï¼Œå¹¶ä»¥è‡ªç„¶è¯­è¨€å’Œæ­£å¼PDDLè¡¨ç¤ºè¡¨è¾¾ï¼Œä¸ºLLMè§„åˆ’å’Œæ¨ç†èƒ½åŠ›çš„æå‡æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚</li>
<li>é€šè¿‡ç”ŸæˆåŒ…å«è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ç”ŸæˆåŸŸçš„æ•°æ®é›†ï¼Œå¯¹LLMè¿›è¡Œè¯„ä¼°ï¼Œå±•ç¤ºäº†NL2FLOWçš„å®é™…æ•ˆæœã€‚</li>
<li>æœ€ä¼˜æ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ•ˆè®¡åˆ’å’Œæœ€ä¼˜è®¡åˆ’æ–¹é¢çš„æˆåŠŸç‡åˆ†åˆ«ä¸º86%å’Œ69%ï¼Œè¡¨æ˜LLMåœ¨è§„åˆ’ä»»åŠ¡ä¸­çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>å›å½’åˆ†ææ­ç¤ºäº†é—®é¢˜ç‰¹æ€§ã€æ¨¡å‹å’Œæç¤ºè®¾è®¡å¯¹è®¡åˆ’ç”Ÿæˆçš„å½±å“ã€‚</li>
<li>ç ”ç©¶å‘ç°ç›´æ¥ä»è‡ªç„¶è¯­è¨€è¿›è¡Œæ¨ç†çš„èƒ½åŠ›å¯èƒ½ä¼˜äºéœ€è¦ç»è¿‡ä¸­é—´ç¿»è¯‘æ­¥éª¤çš„æ–¹æ³•ï¼Œè¿™æš—ç¤ºäº†æœªæ¥æ¨¡å‹å¼€å‘çš„æ–¹å‘ã€‚</li>
<li>éšç€é—®é¢˜çš„å¤æ‚æ€§å¢åŠ ï¼Œäº†è§£LLMçš„å±€é™æ€§ä»¥åŠæ­ç¤ºè¿™äº›å±€é™æ€§çš„å·¥å…·å°†å˜å¾—è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02253">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3fc3690bb11e52aad851ae8aaf547400.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-217c0ded8e473a3c992ef18af7f01162.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00bc40f065f7d76ea30ca91896bf1a9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b92f62b147474cdf44f9627ba3d6dd4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fa44fcd6163eea0c5b8fe4671c915a6.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Latent-Chain-of-Thought-Decoding-the-Depth-Recurrent-Transformer"><a href="#Latent-Chain-of-Thought-Decoding-the-Depth-Recurrent-Transformer" class="headerlink" title="Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer"></a>Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer</h2><p><strong>Authors:Wenquan Lu, Yuechuan Yang, Kyle Lee, Yanshu Li, Enqi Liu</strong></p>
<p>Chain-of-thought (CoT) reasoning has enabled transformer-based language models to excel at complex mathematics and multi-step planning. However, in standard decoder-only architectures, these reasoning steps are externalized in natural language, improving interpretability at the cost of efficiency. To capture reasoning that is not easily represented in words, many works have explored recurrent architectures that aim to internalize reasoning in latent space, potentially supporting latent CoT. In this paper, we investigate whether such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer that reuses layers at inference time without increasing parameter count. We examine the modelâ€™s internal behavior on arithmetic tasks using a suite of probing techniques including the Logit Lens and Coda Lens. Our findings reveal limited evidence of interpretable latent CoT by tracking rank trajectories of final and intermediate result tokens. Furthermore, we uncover significant probing inconsistencies across recurrent blocks, where the interpretability of hidden states depends heavily on both the layer index and the decoding method. Finally, we empirically show that increasing recurrence depth yields only marginal gains and falls well short of models that explicitly externalize reasoning steps. The code is available at <a target="_blank" rel="noopener" href="https://github.com/wenquanlu/huginn-latent-cot">https://github.com/wenquanlu/huginn-latent-cot</a>. </p>
<blockquote>
<p>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ä½¿åŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ•°å­¦å’Œå¤šæ­¥è§„åˆ’æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œåœ¨æ ‡å‡†çš„ä»…è§£ç å™¨æ¶æ„ä¸­ï¼Œè¿™äº›æ¨ç†æ­¥éª¤ä»¥è‡ªç„¶è¯­è¨€å½¢å¼å¤–åœ¨åŒ–ï¼Œä»¥æé«˜å¯è§£é‡Šæ€§ï¼Œä½†ç‰ºç‰²äº†æ•ˆç‡ã€‚ä¸ºäº†æ•æ‰ä¸å®¹æ˜“ç”¨æ–‡å­—è¡¨ç¤ºçš„ç†ç”±ï¼Œè®¸å¤šå·¥ä½œå·²ç»æ¢ç´¢äº†å¾ªç¯æ¶æ„ï¼Œæ—¨åœ¨å°†æ¨ç†å†…åœ¨åŒ–äºæ½œåœ¨ç©ºé—´ï¼Œå¯èƒ½æ”¯æŒæ½œåœ¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†è¿™ç§æ¨ç†ç»“æ„æ˜¯å¦å‡ºç°åœ¨Huginn-3.5Bè¿™ä¸€æ·±åº¦å¾ªç¯è½¬æ¢å™¨ä¸­ï¼Œè¯¥è½¬æ¢å™¨åœ¨æ¨ç†æ—¶é—´é‡æ–°ä½¿ç”¨å±‚è€Œä¸ä¼šå¢åŠ å‚æ•°è®¡æ•°ã€‚æˆ‘ä»¬ä½¿ç”¨åŒ…æ‹¬Logit Lenså’ŒCoda Lensç­‰ä¸€ç³»åˆ—æ¢æµ‹æŠ€æœ¯å¯¹æ¨¡å‹åœ¨ç®—æœ¯ä»»åŠ¡ä¸Šçš„å†…éƒ¨è¡Œä¸ºè¿›è¡Œæ£€æŸ¥ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ˜¾ç¤ºæœ‰é™çš„è¯æ®è¡¨æ˜å­˜åœ¨å¯è§£é‡Šçš„æ½œåœ¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ï¼Œé€šè¿‡è·Ÿè¸ªæœ€ç»ˆç»“æœä»¤ç‰Œå’Œä¸­é—´ç»“æœä»¤ç‰Œçš„æ’åè½¨è¿¹æ¥ä½“ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ä¸åŒå¾ªç¯å—ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ¢æµ‹ä¸ä¸€è‡´æ€§ï¼Œéšè—çŠ¶æ€çš„è§£é‡Šæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå±‚ç´¢å¼•å’Œè§£ç æ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬ä»å®è¯ä¸Šè¯æ˜ï¼Œå¢åŠ å¾ªç¯æ·±åº¦åªä¼šå¸¦æ¥å¾®å°çš„æ”¶ç›Šï¼Œè¿œè¿œè½åäºé‚£äº›æ˜ç¡®å¤–åœ¨åŒ–æ¨ç†æ­¥éª¤çš„æ¨¡å‹ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wenquanlu/huginn-latent-cot%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wenquanlu/huginn-latent-cotæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02199v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Huginn-3.5Bæ·±åº¦å¾ªç¯Transformeræ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„å†…éƒ¨è¡Œä¸ºã€‚é€šè¿‡ä¸€ç³»åˆ—æ¢æµ‹æŠ€æœ¯ï¼Œç ”ç©¶å‘ç°åœ¨è¯¥æ¨¡å‹ä¸­æœ‰é™çš„è¯æ®è¡¨æ˜å…¶å…·å¤‡å¯è§£é‡Šçš„æ½œåœ¨é“¾å¼æ€ç»´ï¼ˆlatent CoTï¼‰ã€‚ç„¶è€Œï¼Œä¸åŒå¾ªç¯å—é—´çš„æ¢æµ‹ç»“æœå­˜åœ¨ä¸ä¸€è‡´æ€§ï¼Œéšè—çŠ¶æ€çš„è§£é‡Šæ€§å–å†³äºå±‚ç´¢å¼•å’Œè§£ç æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå¢åŠ å¾ªç¯æ·±åº¦å¸¦æ¥çš„æ”¶ç›Šç”šå¾®ï¼Œä»è¿œä½äºæ˜¾å¼å¤–éƒ¨åŒ–æ¨ç†æ­¥éª¤çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Huginn-3.5Bæ¨¡å‹ç»“åˆäº†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ï¼Œä»¥æ·±åº¦å¾ªç¯Transformeræ¶æ„æ‰§è¡Œå¤æ‚ä»»åŠ¡å’Œå¤šæ­¥éª¤è§„åˆ’ã€‚</li>
<li>è¯¥æ¨¡å‹ä½¿ç”¨æ¢æµ‹æŠ€æœ¯æ¥ç ”ç©¶å…¶åœ¨ç®—æœ¯ä»»åŠ¡ä¸­çš„å†…éƒ¨è¡Œä¸ºã€‚</li>
<li>ç ”ç©¶å‘ç°æœ‰é™è¯æ®è¡¨æ˜Huginn-3.5Bå…·å¤‡å¯è§£é‡Šçš„æ½œåœ¨é“¾å¼æ€ç»´ï¼ˆlatent CoTï¼‰ã€‚</li>
<li>åœ¨ä¸åŒå¾ªç¯å—ä¸­ï¼Œéšè—çŠ¶æ€çš„è§£é‡Šæ€§å­˜åœ¨ä¸ä¸€è‡´æ€§ã€‚</li>
<li>å¢åŠ æ¨¡å‹çš„å¾ªç¯æ·±åº¦å¸¦æ¥çš„æ€§èƒ½æå‡æœ‰é™ã€‚</li>
<li>ä¸æ˜¾å¼å¤–éƒ¨åŒ–æ¨ç†æ­¥éª¤çš„æ¨¡å‹ç›¸æ¯”ï¼ŒHuginn-3.5Bçš„è¡¨ç°ä»æœ‰å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02199">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c872c13c57adab5f026002ffa9ba0dfc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2b20ea4bf60eeec37cc45dea4d777e3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31876c0b37b73616383b8465a8a75688.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06af2c5ec909d162e7aa578b4e25500e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0bd00a4f426dd22f1731dd78ecc3910.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="FinAI-BERT-A-Transformer-Based-Model-for-Sentence-Level-Detection-of-AI-Disclosures-in-Financial-Reports"><a href="#FinAI-BERT-A-Transformer-Based-Model-for-Sentence-Level-Detection-of-AI-Disclosures-in-Financial-Reports" class="headerlink" title="FinAI-BERT: A Transformer-Based Model for Sentence-Level Detection of AI   Disclosures in Financial Reports"></a>FinAI-BERT: A Transformer-Based Model for Sentence-Level Detection of AI   Disclosures in Financial Reports</h2><p><strong>Authors:Muhammad Bilal Zafar</strong></p>
<p>The proliferation of artificial intelligence (AI) in financial services has prompted growing demand for tools that can systematically detect AI-related disclosures in corporate filings. While prior approaches often rely on keyword expansion or document-level classification, they fall short in granularity, interpretability, and robustness. This study introduces FinAI-BERT, a domain-adapted transformer-based language model designed to classify AI-related content at the sentence level within financial texts. The model was fine-tuned on a manually curated and balanced dataset of 1,586 sentences drawn from 669 annual reports of U.S. banks (2015 to 2023). FinAI-BERT achieved near-perfect classification performance (accuracy of 99.37 percent, F1 score of 0.993), outperforming traditional baselines such as Logistic Regression, Naive Bayes, Random Forest, and XGBoost. Interpretability was ensured through SHAP-based token attribution, while bias analysis and robustness checks confirmed the modelâ€™s stability across sentence lengths, adversarial inputs, and temporal samples. Theoretically, the study advances financial NLP by operationalizing fine-grained, theme-specific classification using transformer architectures. Practically, it offers a scalable, transparent solution for analysts, regulators, and scholars seeking to monitor the diffusion and framing of AI across financial institutions. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨é‡‘èæœåŠ¡çš„æ™®åŠå¼•å‘äº†å¯¹ä¼ä¸šæ–‡ä»¶ç³»ç»Ÿä¸­AIç›¸å…³æŠ«éœ²çš„ç³»ç»Ÿæ£€æµ‹å·¥å…·çš„éœ€æ±‚å¢é•¿ã€‚å°½ç®¡å…ˆå‰çš„æ–¹æ³•ç»å¸¸ä¾èµ–äºå…³é”®è¯æ‰©å±•æˆ–æ–‡æ¡£çº§åˆ«çš„åˆ†ç±»ï¼Œä½†åœ¨ç²’åº¦ã€å¯è§£é‡Šæ€§å’Œç¨³å¥æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†FinAI-BERTï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºåŸŸçš„è½¬æ¢å™¨è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨åœ¨ç¾å›½é“¶è¡Œï¼ˆ2015å¹´è‡³2023å¹´ï¼‰çš„å¹´åº¦æŠ¥å‘Šä¸­æŠ½å–çš„669ä»½æŠ¥å‘Šçš„æ–‡æœ¬æ•°æ®åŸºç¡€ä¸Šå¯¹é‡‘èæ–‡æœ¬ä¸­çš„å¥å­çº§åˆ«è¿›è¡ŒAIç›¸å…³å†…å®¹çš„åˆ†ç±»ã€‚è¯¥æ¨¡å‹ç»è¿‡ç²¾ç»†è°ƒæ•´ï¼Œé‡‡ç”¨äººå·¥ç¼–è¾‘ä¸”å¹³è¡¡çš„åŒ…å«ä¸€ä¸‡äº”åƒå…«åå…­å¥çš„æ•°æ®é›†ã€‚FinAI-BERTå®ç°äº†è¿‘ä¹å®Œç¾çš„åˆ†ç±»æ€§èƒ½ï¼ˆå‡†ç¡®ç‡ä¸ºç™¾åˆ†ä¹‹ä¹åä¹ç‚¹ä¸‰ä¸ƒï¼ŒF1åˆ†æ•°ä¸ºç™¾åˆ†ä¹‹ä¹åä¹ç‚¹ä¸‰ï¼‰ï¼Œè¶…è¿‡äº†é€»è¾‘å›å½’ã€æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ã€éšæœºæ£®æ—å’ŒXGBoostç­‰ä¼ ç»ŸåŸºçº¿æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡åŸºäºSHAPçš„ä»¤ç‰Œå½’å±ç¡®ä¿äº†å¯è§£é‡Šæ€§ï¼Œè€Œåè§åˆ†æå’Œç¨³å¥æ€§æ£€æŸ¥åˆ™è¯å®äº†è¯¥æ¨¡å‹åœ¨å¥å­é•¿åº¦ã€å¯¹æŠ—æ€§è¾“å…¥å’Œæ—¶é—´æ ·æœ¬æ–¹é¢çš„ç¨³å®šæ€§ã€‚åœ¨ç†è®ºä¸Šï¼Œè¯¥ç ”ç©¶é€šè¿‡åˆ©ç”¨è½¬æ¢å™¨æ¶æ„è¿›è¡Œç²¾ç»†ç²’åº¦çš„ä¸»é¢˜ç‰¹å®šåˆ†ç±»ï¼Œæ¨åŠ¨äº†é‡‘èNLPçš„å‘å±•ã€‚åœ¨å®è·µä¸­ï¼Œå®ƒä¸ºåˆ†æå¸ˆã€ç›‘ç®¡æœºæ„å’Œå­¦è€…æä¾›äº†ä¸€ç§å¯ä¼¸ç¼©çš„é€æ˜è§£å†³æ–¹æ¡ˆï¼Œç”¨äºç›‘æ§é‡‘èæœºæ„ä¸­äººå·¥æ™ºèƒ½çš„æ‰©æ•£å’Œæ¡†æ¶æ„å»ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01991v1">PDF</a> The FinAI-BERT model can be directly loaded via Hugging Face   Transformers (<a target="_blank" rel="noopener" href="https://huggingface.co/bilalzafar/FinAI-BERT">https://huggingface.co/bilalzafar/FinAI-BERT</a>) for   sentence-level AI disclosure classification</p>
<p><strong>Summary</strong>ï¼š</p>
<p>éšç€äººå·¥æ™ºèƒ½åœ¨é‡‘èæœåŠ¡çš„æ™®åŠï¼Œå¯¹ä¼ä¸šå¹´æŠ¥ä¸­AIç›¸å…³æŠ«éœ²å†…å®¹çš„ç³»ç»Ÿæ£€æµ‹å·¥å…·çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†FinAI-BERTæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ˜¯åŸºäºåŸŸé€‚åº”çš„è½¬æ¢å™¨è¯­è¨€æ¨¡å‹ï¼Œå¯åœ¨é‡‘èæ–‡æœ¬ä¸­çš„å¥å­çº§åˆ«å¯¹AIç›¸å…³å†…å®¹è¿›è¡Œåˆ†ç±»ã€‚è¯¥æ¨¡å‹åœ¨æ‰‹åŠ¨ç¼–çº‚å’Œå¹³è¡¡çš„1586ä¸ªå¥å­æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¿™äº›å¥å­æ¥è‡ª2015å¹´è‡³2023å¹´ç¾å›½é“¶è¡Œçš„669ä»½å¹´æŠ¥ã€‚FinAI-BERTåˆ†ç±»æ€§èƒ½ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡ä¸º99.37%ï¼ŒF1åˆ†æ•°ä¸º0.993ï¼Œä¼˜äºé€»è¾‘å›å½’ã€æœ´ç´ è´å¶æ–¯ã€éšæœºæ£®æ—å’ŒXGBoostç­‰ä¼ ç»ŸåŸºçº¿æ¨¡å‹ã€‚åŒæ—¶ï¼Œé€šè¿‡SHAPåŸºäºæ ‡è®°çš„å½’å› ç¡®ä¿äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œå¹¶é€šè¿‡åå·®åˆ†æå’Œç¨³å¥æ€§æ£€æŸ¥éªŒè¯äº†æ¨¡å‹åœ¨ä¸åŒå¥å­é•¿åº¦ã€å¯¹æŠ—æ€§è¾“å…¥å’Œæ—¶é—´æ ·æœ¬ä¸Šçš„ç¨³å®šæ€§ã€‚ç ”ç©¶ä¸ä»…åœ¨é‡‘èNLPé¢†åŸŸæ¨åŠ¨äº†ä»¥è½¬æ¢å™¨æ¶æ„è¿›è¡Œç²¾ç»†ç²’åº¦ã€ä¸»é¢˜ç‰¹å®šçš„åˆ†ç±»æ–¹æ³•çš„å‘å±•ï¼Œè€Œä¸”ä¸ºåˆ†æå¸ˆã€ç›‘ç®¡æœºæ„å’Œå­¦è€…æä¾›äº†ä¸€ä¸ªå¯è§„æ¨¡åŒ–ã€é€æ˜çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥ç›‘æµ‹é‡‘èæœºæ„ä¸­AIçš„æ‰©æ•£å’Œæ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>FinAI-BERTæ˜¯ä¸€ä¸ªåŸºäºè½¬æ¢å™¨è¯­è¨€æ¨¡å‹çš„é‡‘èæ–‡æœ¬ä¸­AIç›¸å…³å†…å®¹åˆ†ç±»å·¥å…·ã€‚</li>
<li>æ¨¡å‹åœ¨æ‰‹åŠ¨ç¼–çº‚å’Œå¹³è¡¡çš„å¥å­æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>FinAI-BERTåˆ†ç±»æ€§èƒ½ä¼˜äºå¤šç§ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹å…·æœ‰é«˜åº¦çš„å¯è§£é‡Šæ€§ï¼Œé€šè¿‡SHAPæ ‡è®°å½’å› è¿›è¡ŒéªŒè¯ã€‚</li>
<li>æ¨¡å‹ç¨³å®šæ€§ç»è¿‡åå·®åˆ†æå’Œå¤šç§åœºæ™¯çš„ç¨³å¥æ€§æ£€æŸ¥ã€‚</li>
<li>ç ”ç©¶æ¨åŠ¨äº†é‡‘èNLPé¢†åŸŸä¸­ç²¾ç»†ç²’åº¦ä¸»é¢˜ç‰¹å®šåˆ†ç±»æ–¹æ³•çš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01991">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-47d1b8f60918eb480f69d4cbb70a7efa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f70416dbcd621274d15fd2086f03b5d6.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Driven-Closed-Loop-UAV-Operation-with-Semantic-Observations"><a href="#Large-Language-Model-Driven-Closed-Loop-UAV-Operation-with-Semantic-Observations" class="headerlink" title="Large Language Model-Driven Closed-Loop UAV Operation with Semantic   Observations"></a>Large Language Model-Driven Closed-Loop UAV Operation with Semantic   Observations</h2><p><strong>Authors:Wenhao Wang, Yanyan Li, Long Jiao, Jiawei Yuan</strong></p>
<p>Recent advances in large Language Models (LLMs) have revolutionized mobile robots, including unmanned aerial vehicles (UAVs), enabling their intelligent operation within Internet of Things (IoT) ecosystems. However, LLMs still face challenges from logical reasoning and complex decision-making, leading to concerns about the reliability of LLM-driven UAV operations in IoT applications. In this paper, we propose a LLM-driven closed-loop control framework that enables reliable UAV operations powered by effective feedback and refinement using two LLM modules, i.e., a Code Generator and an Evaluator. Our framework transforms numerical state observations from UAV operations into natural language trajectory descriptions to enhance the evaluator LLMâ€™s understanding of UAV dynamics for precise feedback generation. Our framework also enables a simulation-based refinement process, and hence eliminates the risks to physical UAVs caused by incorrect code execution during the refinement. Extensive experiments on UAV control tasks with different complexities are conducted. The experimental results show that our framework can achieve reliable UAV operations using LLMs, which significantly outperforms baseline approaches in terms of success rate and completeness with the increase of task complexity. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥å·²ç»å½»åº•æ”¹å˜äº†ç§»åŠ¨æœºå™¨äººï¼ŒåŒ…æ‹¬æ— äººæœºï¼ˆUAVsï¼‰ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ç‰©è”ç½‘ï¼ˆIoTï¼‰ç”Ÿæ€ç³»ç»Ÿå†…è¿›è¡Œæ™ºèƒ½æ“ä½œã€‚ç„¶è€Œï¼ŒLLMä»é¢ä¸´é€»è¾‘æ¨ç†å’Œå¤æ‚å†³ç­–çš„æŒ‘æˆ˜ï¼Œè¿™å¼•å‘äº†äººä»¬å¯¹LLMé©±åŠ¨çš„æ— äººæœºåœ¨IoTåº”ç”¨ç¨‹åºä¸­è¿è¡Œçš„å¯é æ€§çš„æ‹…å¿§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§LLMé©±åŠ¨çš„é—­ç¯æ§åˆ¶æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸¤ä¸ªLLMæ¨¡å—å³ä»£ç ç”Ÿæˆå™¨å’Œè¯„ä¼°å™¨çš„æœ‰æ•ˆåé¦ˆå’Œæ”¹è¿›ï¼Œä½¿æ— äººæœºèƒ½å¤Ÿå¯é è¿è¡Œã€‚æˆ‘ä»¬çš„æ¡†æ¶å°†æ— äººæœºæ“ä½œä¸­çš„æ•°å€¼çŠ¶æ€è§‚å¯Ÿè½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€è½¨è¿¹æè¿°ï¼Œä»¥æé«˜è¯„ä¼°LLMå¯¹æ— äººæœºåŠ¨åŠ›å­¦çš„ç†è§£ï¼Œä»è€Œç”Ÿæˆç²¾ç¡®çš„åé¦ˆã€‚æˆ‘ä»¬çš„æ¡†æ¶è¿˜æ”¯æŒåŸºäºæ¨¡æ‹Ÿçš„æ”¹è¿›è¿‡ç¨‹ï¼Œå› æ­¤æ¶ˆé™¤äº†æ”¹è¿›è¿‡ç¨‹ä¸­ç”±äºä»£ç æ‰§è¡Œä¸æ­£ç¡®è€Œå¯¹å®ä½“æ— äººæœºé€ æˆçš„é£é™©ã€‚åœ¨å…·æœ‰ä¸åŒå¤æ‚æ€§çš„æ— äººæœºæ§åˆ¶ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿåˆ©ç”¨LLMå®ç°å¯é çš„æ— äººæœºæ“ä½œï¼Œåœ¨ä»»åŠ¡å¤æ‚æ€§å¢åŠ çš„æƒ…å†µä¸‹ï¼Œç›¸è¾ƒäºåŸºå‡†æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨æˆåŠŸç‡å’Œå®Œæ•´æ€§æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01930v2">PDF</a> 9 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç§»åŠ¨æœºå™¨äººé¢†åŸŸçš„åº”ç”¨å·²å–å¾—çªç ´æ€§è¿›å±•ï¼Œå°¤å…¶æ˜¯åœ¨æ— äººæœºï¼ˆUAVï¼‰æ–¹é¢ï¼Œä½¿æ— äººæœºåœ¨ç‰©è”ç½‘ï¼ˆIoTï¼‰ç”Ÿæ€ç³»ç»Ÿä¸­çš„æ™ºèƒ½æ“ä½œæˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼ŒLLMåœ¨é€»è¾‘æ¨ç†å’Œå¤æ‚å†³ç­–åˆ¶å®šæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œè¿™å¼•å‘äº†äººä»¬å¯¹LLMé©±åŠ¨çš„æ— äººæœºåœ¨IoTåº”ç”¨ä¸­å¯é æ€§çš„æ‹…å¿§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§LLMé©±åŠ¨çš„é—­ç¯æ§åˆ¶æ¡†æ¶ï¼Œé€šè¿‡æœ‰æ•ˆçš„åé¦ˆå’Œæ”¹è¿›ï¼Œä½¿ç”¨ä¸¤ä¸ªLLMæ¨¡å—ï¼ˆå³ä»£ç ç”Ÿæˆå™¨å’Œè¯„ä¼°å™¨ï¼‰å®ç°å¯é çš„æ— äººæœºæ“ä½œã€‚è¯¥æ¡†æ¶å°†æ— äººæœºçš„æ•°å€¼çŠ¶æ€è§‚æµ‹è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€è½¨è¿¹æè¿°ï¼Œå¢å¼ºäº†è¯„ä¼°LLMå¯¹æ— äººæœºåŠ¨åŠ›å­¦çš„ç†è§£ï¼Œä»¥äº§ç”Ÿç²¾ç¡®çš„åé¦ˆã€‚è¯¥æ¡†æ¶è¿˜å¯ç”¨äº†åŸºäºæ¨¡æ‹Ÿçš„æ”¹è¿›è¿‡ç¨‹ï¼Œä»è€Œæ¶ˆé™¤äº†æ”¹è¿›è¿‡ç¨‹ä¸­å› ä»£ç æ‰§è¡Œé”™è¯¯è€Œå¯¹å®ä½“æ— äººæœºé€ æˆçš„é£é™©ã€‚é€šè¿‡å¯¹ä¸åŒå¤æ‚åº¦çš„æ— äººæœºæ§åˆ¶ä»»åŠ¡è¿›è¡Œå¤§é‡å®éªŒï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å¯å®ç°å¯é çš„æ— äººæœºLLMæ“ä½œï¼Œåœ¨æˆåŠŸç‡å’Œå®Œæ•´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå¹¶éšç€ä»»åŠ¡å¤æ‚æ€§çš„å¢åŠ è€Œè¡¨ç°å‡ºä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ç§»åŠ¨æœºå™¨äººé¢†åŸŸï¼Œç‰¹åˆ«æ˜¯æ— äººæœºæ“ä½œä¸­å®ç°äº†é‡å¤§è¿›å±•ï¼Œæ¨åŠ¨äº†ç‰©è”ç½‘ç”Ÿæ€ç³»ç»Ÿä¸­çš„æ™ºèƒ½æ“ä½œã€‚</li>
<li>LLMåœ¨é€»è¾‘æ¨ç†å’Œå¤æ‚å†³ç­–åˆ¶å®šæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œå½±å“äº†æ— äººæœºæ“ä½œçš„å¯é æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§LLMé©±åŠ¨çš„é—­ç¯æ§åˆ¶æ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªæ¨¡å—â€”â€”ä»£ç ç”Ÿæˆå™¨å’Œè¯„ä¼°å™¨å®ç°å¯é æ— äººæœºæ“ä½œã€‚</li>
<li>æ¡†æ¶å°†æ— äººæœºçš„æ•°å€¼çŠ¶æ€è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€è½¨è¿¹æè¿°ï¼Œå¢å¼ºäº†è¯„ä¼°å™¨å¯¹æ— äººæœºåŠ¨åŠ›å­¦çš„ç†è§£ã€‚</li>
<li>æ¡†æ¶æ”¯æŒåŸºäºæ¨¡æ‹Ÿçš„æ”¹è¿›è¿‡ç¨‹ï¼Œå‡å°‘äº†å¯¹å®ä½“æ— äººæœºçš„é£é™©ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ¡†æ¶åœ¨å¤æ‚ä»»åŠ¡ä¸­å®ç°äº†å¯é çš„æ— äººæœºæ“ä½œï¼Œå¹¶åœ¨æˆåŠŸç‡å’Œå®Œæ•´æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f42d97d9cda874b90a7377fe5ca9cc8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b4fab06cc3db47da849f4fd44147abe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c383a0d150447f7e3fdbfe68eb73bbe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a691b73db0c36312c4a697f6aec9130.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7f682c04b456367edb962cd4817e6762.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a452a52b6d593509c4f9c3aeb6565e5.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Symbolic-or-Numerical-Understanding-Physics-Problem-Solving-in-Reasoning-LLMs"><a href="#Symbolic-or-Numerical-Understanding-Physics-Problem-Solving-in-Reasoning-LLMs" class="headerlink" title="Symbolic or Numerical? Understanding Physics Problem Solving in   Reasoning LLMs"></a>Symbolic or Numerical? Understanding Physics Problem Solving in   Reasoning LLMs</h2><p><strong>Authors:Nifu Dan, Yujun Cai, Yiwei Wang</strong></p>
<p>Navigating the complexities of physics reasoning has long been a difficult task for Large Language Models (LLMs), requiring a synthesis of profound conceptual understanding and adept problem-solving techniques. In this study, we investigate the application of advanced instruction-tuned reasoning models, such as Deepseek-R1, to address a diverse spectrum of physics problems curated from the challenging SciBench benchmark. Our comprehensive experimental evaluation reveals the remarkable capabilities of reasoning models. Not only do they achieve state-of-the-art accuracy in answering intricate physics questions, but they also generate distinctive reasoning patterns that emphasize on symbolic derivation. Furthermore, our findings indicate that even for these highly sophisticated reasoning models, the strategic incorporation of few-shot prompting can still yield measurable improvements in overall accuracy, highlighting the potential for continued performance gains. </p>
<blockquote>
<p>å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´ï¼Œé•¿æœŸä»¥æ¥ï¼Œé©¾é©­å¤æ‚çš„ç‰©ç†æ¨ç†ä¸€ç›´æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ï¼Œéœ€è¦æ·±åˆ»çš„æ¦‚å¿µç†è§£å’Œç†Ÿç»ƒçš„é—®é¢˜è§£å†³æŠ€æœ¯çš„ç»“åˆã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†é«˜çº§æŒ‡ä»¤è°ƒæ•´æ¨ç†æ¨¡å‹ï¼ˆå¦‚Deepseek-R1ï¼‰åœ¨åº”å¯¹æ¥è‡ªå…·æœ‰æŒ‘æˆ˜æ€§çš„SciBenchåŸºå‡†çš„ä¸€ç³»åˆ—å¤šæ ·åŒ–ç‰©ç†é—®é¢˜æ—¶çš„åº”ç”¨æƒ…å†µã€‚æˆ‘ä»¬çš„å…¨é¢å®éªŒè¯„ä¼°è¡¨æ˜æ¨ç†æ¨¡å‹çš„å“è¶Šèƒ½åŠ›ã€‚è¿™äº›æ¨¡å‹ä¸ä»…èƒ½å¤Ÿåœ¨å›ç­”å¤æ‚çš„ç‰©ç†é—®é¢˜æ—¶è¾¾åˆ°æœ€æ–°çš„ç²¾ç¡®åº¦ï¼Œè€Œä¸”ä¼šäº§ç”Ÿå¼ºè°ƒç¬¦å·æ¨å¯¼çš„ç‹¬ç‰¹æ¨ç†æ¨¡å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¿˜å‘ç°ï¼Œå³ä½¿æ˜¯è¿™äº›é«˜åº¦å¤æ‚çš„æ¨ç†æ¨¡å‹ï¼Œé€šè¿‡ç­–ç•¥æ€§åœ°é‡‡ç”¨å°‘é‡æç¤ºä»ç„¶èƒ½å¤Ÿæé«˜æ•´ä½“ç²¾åº¦ï¼Œè¿™çªæ˜¾äº†æœªæ¥æ€§èƒ½å¢é•¿çš„å¯èƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01334v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶æ¢è®¨äº†å…ˆè¿›çš„æŒ‡ä»¤è°ƒæ•´æ¨ç†æ¨¡å‹ï¼Œå¦‚Deepseek-R1ï¼Œåœ¨è§£å†³å¤šæ ·åŒ–ç‰©ç†é—®é¢˜ä¸­çš„åº”ç”¨ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹ä¸ä»…è¾¾åˆ°å›ç­”å¤æ‚ç‰©ç†é—®é¢˜çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œè¿˜èƒ½äº§ç”Ÿç‹¬ç‰¹çš„å¼ºè°ƒç¬¦å·æ¨å¯¼çš„æ¨ç†æ¨¡å¼ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿åœ¨é«˜åº¦å…ˆè¿›çš„æ¨ç†æ¨¡å‹ä¸­ï¼Œç­–ç•¥æ€§ä½¿ç”¨å°æ ·æœ¬æç¤ºä»ç„¶å¯ä»¥è¿›ä¸€æ­¥æé«˜æ€»ä½“å‡†ç¡®æ€§ï¼Œå±•ç¤ºäº†æ€§èƒ½æ”¹è¿›çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å…ˆè¿›çš„æŒ‡ä»¤è°ƒæ•´æ¨ç†æ¨¡å‹ï¼Œå¦‚Deepseek-R1ï¼Œåœ¨è§£å†³ç‰©ç†é—®é¢˜æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ã€‚</li>
<li>è¿™äº›æ¨¡å‹èƒ½å¤Ÿåœ¨å›ç­”å¤æ‚ç‰©ç†é—®é¢˜æ—¶è¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ã€‚</li>
<li>æ¨ç†æ¨¡å‹èƒ½å¤Ÿäº§ç”Ÿç‹¬ç‰¹çš„å¼ºè°ƒç¬¦å·æ¨å¯¼çš„æ¨ç†æ¨¡å¼ã€‚</li>
<li>åœ¨é«˜åº¦å…ˆè¿›çš„æ¨ç†æ¨¡å‹ä¸­ï¼Œä½¿ç”¨å°æ ·æœ¬æç¤ºå¯ä»¥æé«˜æ€»ä½“å‡†ç¡®æ€§ã€‚</li>
<li>è¿™ç§ç­–ç•¥æ€§ä½¿ç”¨å°æ ·æœ¬æç¤ºçš„æ–¹æ³•å±•ç¤ºäº†æ€§èƒ½æ”¹è¿›çš„æ½œåŠ›ã€‚</li>
<li>å®éªŒä¸­ä½¿ç”¨äº†SciBenchåŸºå‡†æµ‹è¯•ä¸­çš„å¤šæ ·åŒ–ç‰©ç†é—®é¢˜æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01334">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aab02320fa33366802e367f7a5388eef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-43ee0b3af7d989529f628567f903df25.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-372b906d2927d0b416cf3711220ee157.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bce1c83d5c44a0ac7c57117747866cef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39a106965c2a43254c57b5354cd250ca.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="GPAS-Accelerating-Convergence-of-LLM-Pretraining-via-Gradient-Preserving-Activation-Scaling"><a href="#GPAS-Accelerating-Convergence-of-LLM-Pretraining-via-Gradient-Preserving-Activation-Scaling" class="headerlink" title="GPAS: Accelerating Convergence of LLM Pretraining via   Gradient-Preserving Activation Scaling"></a>GPAS: Accelerating Convergence of LLM Pretraining via   Gradient-Preserving Activation Scaling</h2><p><strong>Authors:Tianhao Chen, Xin Xu, Zijing Liu, Pengxiang Li, Xinyuan Song, Ajay Kumar Jaiswal, Fan Zhang, Jishan Hu, Yang Wang, Hao Chen, Shizhe Diao, Shiwei Liu, Yu Li, Lu Yin, Can Yang</strong></p>
<p>Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series, predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While being stable during pretraining and scalable to large model sizes, Pre-LN suffers from an exponential growth in activation variance across layers, causing the shortcut to dominate over sub-layer outputs in the residual connection and limiting the learning capacity of deeper layers. To mitigate this issue, we propose Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be used in combination with existing approaches. GPAS works by scaling down the intermediate activations while keeping their gradients unchanged. This leaves information in the activations intact, and avoids the gradient vanishing problem associated with gradient downscaling. Extensive experiments across various model sizes from 71M to 1B show that GPAS achieves consistent performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows promise in improving alternative architectures such as Sandwich-LN and DeepNorm, demonstrating its versatility and potential for improving training dynamics in a wide range of settings. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/dandingsky/GPAS">https://github.com/dandingsky/GPAS</a>. </p>
<blockquote>
<p>ç°ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚LLaMAã€Qwenå’ŒDeepSeekç³»åˆ—ï¼Œä¸»è¦é‡‡ç”¨äº†Pre-LayerNormï¼ˆPre-LNï¼‰Transformeræ¶æ„ã€‚è™½ç„¶åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­è¡¨ç°ç¨³å®šï¼Œä¸”èƒ½å¤Ÿæ‰©å±•åˆ°å¤§å‹æ¨¡å‹ï¼Œä½†Pre-LNåœ¨å±‚é—´æ¿€æ´»æ–¹å·®ä¸Šå‘ˆç°æŒ‡æ•°å¢é•¿ï¼Œå¯¼è‡´æ®‹å·®è¿æ¥ä¸­çš„å¿«æ·æ–¹å¼ä¸»å¯¼å­å±‚è¾“å‡ºï¼Œå¹¶é™åˆ¶æ·±å±‚å±‚çš„å­¦ä¹ èƒ½åŠ›ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ¢¯åº¦ä¿æŒæ¿€æ´»ç¼©æ”¾ï¼ˆGPASï¼‰æŠ€æœ¯ï¼Œå®ƒå¯ä»¥ä¸ç°æœ‰æ–¹æ³•ç›¸ç»“åˆä½¿ç”¨ã€‚GPASé€šè¿‡ç¼©å°ä¸­é—´æ¿€æ´»å€¼åŒæ—¶ä¿æŒå…¶æ¢¯åº¦ä¸å˜æ¥å‘æŒ¥ä½œç”¨ã€‚è¿™ä¿ç•™äº†æ¿€æ´»å€¼ä¸­çš„ä¿¡æ¯ï¼Œé¿å…äº†ä¸æ¢¯åº¦ç¼©å°ç›¸å…³çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚åœ¨71Måˆ°1Bçš„å„ç§æ¨¡å‹å¤§å°ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGPASå®ç°äº†æ€§èƒ½çš„ç¨³å®šæå‡ã€‚é™¤äº†å¢å¼ºPre-LN Transformerçš„æ€§èƒ½å¤–ï¼ŒGPASåœ¨æ”¹è¿›æ›¿ä»£æ¶æ„ï¼ˆå¦‚Sandwich-LNå’ŒDeepNormï¼‰æ–¹é¢ä¹Ÿæ˜¾ç¤ºå‡ºå¸Œæœ›ï¼Œè¯æ˜äº†å…¶åœ¨æ”¹è¿›å„ç§è®¾ç½®ä¸­çš„è®­ç»ƒåŠ¨åŠ›å­¦çš„é€šç”¨æ€§å’Œæ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/dandingsky/GPAS%E3%80%82">https://github.com/dandingsky/GPASã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22049v2">PDF</a> </p>
<p><strong>Summary</strong><br>ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹å¤šé‡‡ç”¨Pre-LayerNormï¼ˆPre-LNï¼‰Transformeræ¶æ„ï¼Œå®ƒåœ¨é¢„è®­ç»ƒå’Œå¤§è§„æ¨¡æ¨¡å‹åº”ç”¨æ–¹é¢è¡¨ç°ç¨³å®šã€‚ç„¶è€Œï¼ŒPre-LNé¢ä¸´æ¿€æ´»æ–¹å·®éšå±‚æ•°å‘ˆæŒ‡æ•°å¢é•¿çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´æ®‹å·®è¿æ¥ä¸­çš„å¿«æ·æ–¹å¼ä¸»å¯¼å­å±‚è¾“å‡ºï¼Œé™åˆ¶äº†æ·±å±‚çš„å­¦ä¹ èƒ½åŠ›ã€‚ä¸ºç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åä¸ºGradient-Preserving Activation Scalingï¼ˆGPASï¼‰çš„æŠ€æœ¯ã€‚å®ƒé€šè¿‡å‡å°ä¸­é—´æ¿€æ´»å€¼çš„åŒæ—¶ä¿æŒæ¢¯åº¦ä¸å˜ï¼Œæ—¢ä¿ç•™äº†æ¿€æ´»ä¿¡æ¯ï¼Œåˆé¿å…äº†æ¢¯åº¦ä¸‹é™å¯¼è‡´çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚å®éªŒè¯æ˜ï¼ŒGPASåœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸­éƒ½èƒ½å®ç°æ€§èƒ½çš„æå‡ï¼Œå¹¶æœ‰æœ›æ”¹è¿›å…¶ä»–æ¶æ„å¦‚Sandwich-LNå’ŒDeepNormã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/dandingsky/GPAS%E4%B8%8A%E3%80%82">https://github.com/dandingsky/GPASä¸Šã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹å€¾å‘äºé‡‡ç”¨Pre-LayerNormï¼ˆPre-LNï¼‰Transformeræ¶æ„ã€‚</li>
<li>Pre-LNåœ¨é¢„è®­ç»ƒå’Œå¤§è§„æ¨¡åº”ç”¨æ–¹é¢è¡¨ç°ç¨³å®šï¼Œä½†å­˜åœ¨æ¿€æ´»æ–¹å·®éšå±‚æ•°å¢é•¿çš„é—®é¢˜ã€‚</li>
<li>GPASæŠ€æœ¯èƒ½æœ‰æ•ˆç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡å‡å°ä¸­é—´æ¿€æ´»å€¼åŒæ—¶ä¿æŒæ¢¯åº¦ä¸å˜ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚</li>
<li>GPASåœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸­éƒ½èƒ½å®ç°æ€§èƒ½æå‡ã€‚</li>
<li>GPASä¸ä»…é€‚ç”¨äºPre-LNæ¶æ„ï¼Œå¯¹å…¶ä»–æ¨¡å‹æ¶æ„å¦‚Sandwich-LNå’ŒDeepNormä¹Ÿæœ‰æ”¹è¿›æ½œåŠ›ã€‚</li>
<li>è¯¥æŠ€æœ¯å·²ç»å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22049">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d8e4dd4abb9a100c9861b535af6a442.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1a6a560142553c0a364d3bc72fbcc11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-668f25a8c54a4af0db7c4b0a307817bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2db0eef633ec8e52b284f2951bfa4c8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c41b64f0b098ca4d0e0bf2f008bf765b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b97a25e529fb0f806bb346863a38911a.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="From-Web-Search-towards-Agentic-Deep-Research-Incentivizing-Search-with-Reasoning-Agents"><a href="#From-Web-Search-towards-Agentic-Deep-Research-Incentivizing-Search-with-Reasoning-Agents" class="headerlink" title="From Web Search towards Agentic Deep Research: Incentivizing Search with   Reasoning Agents"></a>From Web Search towards Agentic Deep Research: Incentivizing Search with   Reasoning Agents</h2><p><strong>Authors:Weizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo, Guancheng Wan, Liangwei Yang, Chenxuan Xie, Yuyao Yang, Wei-Chieh Huang, Chunyu Miao, Henry Peng Zou, Xiao Luo, Yusheng Zhao, Yankai Chen, Chunkit Chan, Peilin Zhou, Xinyang Zhang, Chenwei Zhang, Jingbo Shang, Ming Zhang, Yangqiu Song, Irwin King, Philip S. Yu</strong></p>
<p>Information retrieval is a cornerstone of modern knowledge acquisition, enabling billions of queries each day across diverse domains. However, traditional keyword-based search engines are increasingly inadequate for handling complex, multi-step information needs. Our position is that Large Language Models (LLMs), endowed with reasoning and agentic capabilities, are ushering in a new paradigm termed Agentic Deep Research. These systems transcend conventional information search techniques by tightly integrating autonomous reasoning, iterative retrieval, and information synthesis into a dynamic feedback loop. We trace the evolution from static web search to interactive, agent-based systems that plan, explore, and learn. We also introduce a test-time scaling law to formalize the impact of computational depth on reasoning and search. Supported by benchmark results and the rise of open-source implementations, we demonstrate that Agentic Deep Research not only significantly outperforms existing approaches, but is also poised to become the dominant paradigm for future information seeking. All the related resources, including industry products, research papers, benchmark datasets, and open-source implementations, are collected for the community in <a target="_blank" rel="noopener" href="https://github.com/DavidZWZ/Awesome-Deep-Research">https://github.com/DavidZWZ/Awesome-Deep-Research</a>. </p>
<blockquote>
<p>ä¿¡æ¯æ£€ç´¢æ˜¯ç°ä»£çŸ¥è¯†è·å–çš„æ ¸å¿ƒåŸºçŸ³ï¼Œæ¯å¤©å¯åœ¨å¤šä¸ªé¢†åŸŸå¤„ç†æ•°åäº¿æ¬¡æŸ¥è¯¢ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„åŸºäºå…³é”®è¯çš„æœç´¢å¼•æ“è¶Šæ¥è¶Šéš¾ä»¥æ»¡è¶³å¤æ‚çš„ã€å¤šæ­¥éª¤çš„ä¿¡æ¯éœ€æ±‚ã€‚æˆ‘ä»¬çš„è§‚ç‚¹æ˜¯ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡æ¨ç†å’Œæ™ºèƒ½èƒ½åŠ›ï¼Œæ­£åœ¨å‚¬ç”Ÿä¸€ç§åä¸ºæ™ºèƒ½æ·±åº¦ç ”ç©¶çš„æ–°èŒƒå¼ã€‚è¿™äº›ç³»ç»Ÿé€šè¿‡ç´§å¯†é›†æˆè‡ªä¸»æ¨ç†ã€è¿­ä»£æ£€ç´¢å’Œä¿¡æ¯åˆæˆåˆ°ä¸€ä¸ªåŠ¨æ€åé¦ˆå¾ªç¯ä¸­ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿä¿¡æ¯æœç´¢æŠ€æœ¯ã€‚æˆ‘ä»¬è¿½æº¯äº†ä»é™æ€ç½‘é¡µæœç´¢åˆ°äº¤äº’å¼ã€åŸºäºä»£ç†çš„ç³»ç»Ÿçš„æ¼”å˜ï¼Œè¿™äº›ç³»ç»Ÿå¯ä»¥è®¡åˆ’ã€æ¢ç´¢å’Œå­¦ä¹ çš„å†ç¨‹ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§æµ‹è¯•æ—¶ç¼©æ”¾å®šå¾‹ï¼Œä»¥æ­£å¼è®¡ç®—æ·±åº¦å¯¹æ¨ç†å’Œæœç´¢çš„å½±å“ã€‚åœ¨åŸºå‡†æµ‹è¯•ç»“æœå’Œå¼€æºå®ç°å…´èµ·çš„æ”¯æŒä¸‹ï¼Œæˆ‘ä»¬è¯æ˜æ™ºèƒ½æ·±åº¦ç ”ç©¶ä¸ä»…æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè€Œä¸”è¿˜å°†æˆä¸ºæœªæ¥ä¿¡æ¯æœç´¢çš„ä¸»å¯¼èŒƒå¼ã€‚æ‰€æœ‰ç›¸å…³èµ„æºï¼ŒåŒ…æ‹¬å·¥ä¸šäº§å“ã€ç ”ç©¶è®ºæ–‡ã€åŸºå‡†æ•°æ®é›†å’Œå¼€æºå®ç°ï¼Œéƒ½æ”¶é›†åœ¨äº†<a target="_blank" rel="noopener" href="https://github.com/DavidZWZ/Awesome-Deep-Research%EF%BC%8C%E4%BB%A5%E4%BE%9B%E7%A4%BE%E5%8C%BA%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/DavidZWZ/Awesome-Deep-Researchï¼Œä»¥ä¾›ç¤¾åŒºä½¿ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18959v3">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨ä¿¡æ¯æ£€ç´¢é¢†åŸŸï¼Œä¼ ç»ŸåŸºäºå…³é”®è¯çš„æœç´¢å¼•æ“åœ¨å¤„ç†å¤æ‚ã€å¤šæ­¥éª¤çš„ä¿¡æ¯éœ€æ±‚æ—¶æ—¥ç›Šæ˜¾å¾—ä¸è¶³ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡ç»“åˆè‡ªä¸»æ¨ç†ã€è¿­ä»£æ£€ç´¢å’Œä¿¡æ¯åˆæˆï¼Œå¼€åˆ›äº†åä¸ºAgenticæ·±åº¦ç ”ç©¶çš„æ–°èŒƒå¼ã€‚Agenticæ·±åº¦ç ”ç©¶ä¸ä»…æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè€Œä¸”å°†æˆä¸ºæœªæ¥ä¿¡æ¯æœç´¢çš„ä¸»å¯¼èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¿¡æ¯æ£€ç´¢æ˜¯ç°ä»£çŸ¥è¯†è·å–çš„æ ¸å¿ƒï¼Œæ¯å¤©å¤„ç†æ•°åäº¿æ¬¡æŸ¥è¯¢ã€‚</li>
<li>ä¼ ç»Ÿæœç´¢å¼•æ“åœ¨å¤„ç†å¤æ‚ä¿¡æ¯éœ€æ±‚æ—¶å­˜åœ¨ä¸è¶³ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·æœ‰æ¨ç†å’Œä»£ç†èƒ½åŠ›ï¼Œå¼€å¯äº†åä¸ºAgenticæ·±åº¦ç ”ç©¶çš„æ–°èŒƒå¼ã€‚</li>
<li>Agenticæ·±åº¦ç ”ç©¶é›†æˆäº†è‡ªä¸»æ¨ç†ã€è¿­ä»£æ£€ç´¢å’Œä¿¡æ¯åˆæˆã€‚</li>
<li>Agenticç³»ç»Ÿä»é™æ€ç½‘é¡µæœç´¢è¿›åŒ–åˆ°äº¤äº’å¼çš„ã€åŸºäºä»£ç†çš„ç³»ç»Ÿï¼Œèƒ½å¤Ÿè¿›è¡Œè§„åˆ’ã€æ¢ç´¢å’Œå­¦ä¹ ã€‚</li>
<li>å­˜åœ¨ä¸€ä¸ªæµ‹è¯•æ—¶çš„å°ºåº¦å®šå¾‹ï¼Œç”¨äºå½¢å¼åŒ–è®¡ç®—æ·±åº¦å¯¹æ¨ç†å’Œæœç´¢çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f9a57a6730e21115b5e3be79c0c73400.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76a584b347a5f8a48347111db296ccc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-155280869ad2bd126b54c3da5ebc7cd6.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Privacy-Preserving-in-Connected-and-Autonomous-Vehicles-Through-Vision-to-Text-Transformation"><a href="#Privacy-Preserving-in-Connected-and-Autonomous-Vehicles-Through-Vision-to-Text-Transformation" class="headerlink" title="Privacy-Preserving in Connected and Autonomous Vehicles Through Vision   to Text Transformation"></a>Privacy-Preserving in Connected and Autonomous Vehicles Through Vision   to Text Transformation</h2><p><strong>Authors:Abdolazim Rezaei, Mehdi Sookhak, Ahmad Patooghy</strong></p>
<p>Connected and Autonomous Vehicles (CAVs) rely on a range of devices that often process privacy-sensitive data. Among these, roadside units play a critical role particularly through the use of AI-equipped (AIE) cameras for applications such as violation detection. However, the privacy risks associated with captured imagery remain a major concern, as such data can be misused for identity theft, profiling, or unauthorized commercial purposes. While traditional techniques such as face blurring and obfuscation have been applied to mitigate privacy risks, individual privacy remains at risk, as individuals can still be tracked using other features such as their clothing. This paper introduces a novel privacy-preserving framework that leverages feedback-based reinforcement learning (RL) and vision-language models (VLMs) to protect sensitive visual information captured by AIE cameras. The main idea is to convert images into semantically equivalent textual descriptions, ensuring that scene-relevant information is retained while visual privacy is preserved. A hierarchical RL strategy is employed to iteratively refine the generated text, enhancing both semantic accuracy and privacy. Evaluation results demonstrate significant improvements in both privacy protection and textual quality, with the Unique Word Count increasing by approximately 77% and Detail Density by around 50% compared to existing approaches. </p>
<blockquote>
<p>è‡ªåŠ¨é©¾é©¶å’Œè”ç½‘è½¦è¾†ï¼ˆCAVsï¼‰ä¾èµ–äºä¸€ç³»åˆ—ç»å¸¸å¤„ç†æ•æ„Ÿéšç§æ•°æ®çš„è®¾å¤‡ã€‚å…¶ä¸­ï¼Œè·¯è¾¹å•å…ƒå‘æŒ¥ç€å…³é”®ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨é…å¤‡äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„æ‘„åƒæœºè¿›è¡Œè¿ç« æ£€æµ‹ç­‰åº”ç”¨æ–¹é¢ã€‚ç„¶è€Œï¼Œæ•è·å›¾åƒæ‰€å¸¦æ¥çš„éšç§é£é™©ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦çš„æ‹…å¿§ï¼Œå› ä¸ºè¿™ç§æ•°æ®å¯èƒ½ä¼šè¢«ç”¨äºèº«ä»½ç›—çªƒã€ä¸ªäººæ¡£æ¡ˆåˆ¶ä½œæˆ–æœªç»æˆæƒçš„å•†ä¸šç›®çš„ç­‰ä¸å½“ç”¨é€”ã€‚è™½ç„¶ä¼ ç»Ÿçš„æŠ€æœ¯å¦‚é¢éƒ¨æ¨¡ç³Šå’Œæ¨¡ç³Šå¤„ç†å·²è¢«åº”ç”¨äºå‡è½»éšç§é£é™©ï¼Œä½†ä¸ªäººéšç§é—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œå› ä¸ºä¸ªä½“ä»ç„¶å¯ä»¥é€šè¿‡ä»–ä»¬çš„æœè£…ç­‰å…¶ä»–ç‰¹å¾è¿›è¡Œè·Ÿè¸ªã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„éšç§ä¿æŠ¤æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨åŸºäºåé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ¥ä¿æŠ¤è¢«äººå·¥æ™ºèƒ½æ‘„åƒæœºæ•è·çš„æ•æ„Ÿè§†è§‰ä¿¡æ¯ã€‚ä¸»è¦æ€æƒ³æ˜¯å°†å›¾åƒè½¬æ¢ä¸ºè¯­ä¹‰ç­‰æ•ˆçš„æ–‡æœ¬æè¿°ï¼Œç¡®ä¿åœºæ™¯ç›¸å…³ä¿¡æ¯è¢«ä¿ç•™çš„åŒæ—¶ä¿æŠ¤è§†è§‰éšç§ã€‚é‡‡ç”¨åˆ†å±‚RLç­–ç•¥æ¥è¿­ä»£ä¼˜åŒ–ç”Ÿæˆçš„æ–‡æœ¬ï¼Œæé«˜è¯­ä¹‰å‡†ç¡®æ€§å’Œéšç§ä¿æŠ¤èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œåœ¨éšç§ä¿æŠ¤å’Œæ–‡æœ¬è´¨é‡æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå”¯ä¸€å­—æ•°å¢åŠ äº†çº¦77%ï¼Œç»†èŠ‚å¯†åº¦å¢åŠ äº†çº¦50%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15854v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ–°å‹çš„éšç§ä¿æŠ¤æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆåé¦ˆå¼ºåŒ–å­¦ä¹ å’Œè§†è§‰è¯­è¨€æ¨¡å‹æŠ€æœ¯ï¼Œå°†AIEç›¸æœºæ•æ‰åˆ°çš„æ•æ„Ÿè§†è§‰ä¿¡æ¯è½¬æ¢ä¸ºè¯­ä¹‰ç­‰æ•ˆçš„æ–‡æœ¬æè¿°ï¼Œä»¥ä¿ç•™åœºæ™¯ç›¸å…³ä¿¡æ¯å¹¶ä¿æŠ¤è§†è§‰éšç§ã€‚é‡‡ç”¨åˆ†å±‚å¼ºåŒ–å­¦ä¹ ç­–ç•¥å¯¹ç”Ÿæˆçš„æ–‡æœ¬è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œæé«˜è¯­ä¹‰å‡†ç¡®æ€§å’Œéšç§ä¿æŠ¤æ•ˆæœã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨éšç§ä¿æŠ¤å’Œæ–‡æœ¬è´¨é‡æ–¹é¢å‡æœ‰æ˜¾è‘—æ”¹å–„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CAVsä¸­ä½¿ç”¨çš„è·¯è¾¹å•å…ƒé…å¤‡AIç›¸æœºè¿›è¡Œè¿ç« æ£€æµ‹ç­‰åº”ç”¨ï¼Œä½†æ•æ‰çš„å›¾åƒå­˜åœ¨éšç§æ³„éœ²é£é™©ã€‚</li>
<li>ä¼ ç»ŸæŠ€æœ¯å¦‚é¢éƒ¨æ¨¡ç³Šå’Œä¼ªè£…ä¸èƒ½å®Œå…¨ä¿æŠ¤ä¸ªäººéšç§ï¼Œå› ä¸ºä»å¯é€šè¿‡å…¶ä»–ç‰¹å¾å¦‚æœè£…è¿½è¸ªä¸ªä½“ã€‚</li>
<li>æ–°æ¡†æ¶åˆ©ç”¨åé¦ˆå¼ºåŒ–å­¦ä¹ å’Œè§†è§‰è¯­è¨€æ¨¡å‹ä¿æŠ¤æ•æ„Ÿè§†è§‰ä¿¡æ¯ï¼Œå°†å›¾åƒè½¬æ¢ä¸ºæ–‡æœ¬æè¿°ã€‚</li>
<li>æ¡†æ¶èƒ½ä¿ç•™åœºæ™¯ç›¸å…³ä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŠ¤è§†è§‰éšç§ã€‚</li>
<li>é‡‡ç”¨åˆ†å±‚å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¼˜åŒ–ç”Ÿæˆçš„æ–‡æœ¬ï¼Œæé«˜è¯­ä¹‰å‡†ç¡®æ€§å’Œéšç§ä¿æŠ¤æ•ˆæœã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæ–°æ¡†æ¶åœ¨éšç§ä¿æŠ¤å’Œæ–‡æœ¬è´¨é‡æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15854">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-215e91a93cb251b46cce4a9432d2d75f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50076c15e1073e6d839d1ec0fbc40a66.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-07ea7e4d0a8e41f24ccbf7fd182782a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bcf25e67928c811e9ca3d48bc8c0826.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d79e799d3798b87f80b0f8390d20732.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1974e79666e9cdfbaeb431bb5f19f8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bdd78b6504c87378b9cd62a6ccee40af.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-05/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-05/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-05/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-a70bb736c160c30d6ef975b026e1bd2c.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-05  Establishing Best Practices for Building Rigorous Agentic Benchmarks
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-05/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0a452a52b6d593509c4f9c3aeb6565e5.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-05  MOTIF Modular Thinking via Reinforcement Fine-tuning in LLMs
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23542.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
