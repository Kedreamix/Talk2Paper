<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-05  A Cookbook for Community-driven Data Collection of Impaired Speech in   LowResource Languages">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-27433933ca222f56889973b3cbd8992e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    36 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-05-æ›´æ–°"><a href="#2025-07-05-æ›´æ–°" class="headerlink" title="2025-07-05 æ›´æ–°"></a>2025-07-05 æ›´æ–°</h1><h2 id="A-Cookbook-for-Community-driven-Data-Collection-of-Impaired-Speech-in-LowResource-Languages"><a href="#A-Cookbook-for-Community-driven-Data-Collection-of-Impaired-Speech-in-LowResource-Languages" class="headerlink" title="A Cookbook for Community-driven Data Collection of Impaired Speech in   LowResource Languages"></a>A Cookbook for Community-driven Data Collection of Impaired Speech in   LowResource Languages</h2><p><strong>Authors:Sumaya Ahmed Salihs, Isaac Wiafe, Jamal-Deen Abdulai, Elikem Doe Atsakpo, Gifty Ayoka, Richard Cave, Akon Obu Ekpezu, Catherine Holloway, Katrin Tomanek, Fiifi Baffoe Payin Winful</strong></p>
<p>This study presents an approach for collecting speech samples to build Automatic Speech Recognition (ASR) models for impaired speech, particularly, low-resource languages. It aims to democratize ASR technology and data collection by developing a â€œcookbookâ€ of best practices and training for community-driven data collection and ASR model building. As a proof-of-concept, this study curated the first open-source dataset of impaired speech in Akan: a widely spoken indigenous language in Ghana. The study involved participants from diverse backgrounds with speech impairments. The resulting dataset, along with the cookbook and open-source tools, are publicly available to enable researchers and practitioners to create inclusive ASR technologies tailored to the unique needs of speech impaired individuals. In addition, this study presents the initial results of fine-tuning open-source ASR models to better recognize impaired speech in Akan. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ”¶é›†è¯­éŸ³æ ·æœ¬çš„æ–¹æ³•ï¼Œç”¨äºæ„å»ºé’ˆå¯¹å—æŸè¯­éŸ³ï¼ˆå°¤å…¶æ˜¯èµ„æºåŒ®ä¹çš„è¯­è¨€ï¼‰çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡å¼€å‘ç¤¾åŒºé©±åŠ¨çš„æ•°æ®æ”¶é›†å’ŒASRæ¨¡å‹æ„å»ºçš„â€œæœ€ä½³å®è·µæ‰‹å†Œâ€å’ŒåŸ¹è®­ææ–™ï¼Œä½¿ASRæŠ€æœ¯å’Œæ•°æ®æ”¶é›†æ°‘ä¸»åŒ–ã€‚ä½œä¸ºæ¦‚å¿µéªŒè¯ï¼Œæœ¬ç ”ç©¶ç²¾å¿ƒåˆ¶ä½œäº†åŠ çº³å¹¿æ³›ä½¿ç”¨çš„æœ¬åœŸè¯­è¨€é˜¿åè¯­çš„é¦–ä¸ªå¼€æºå—æŸè¯­éŸ³æ•°æ®é›†ã€‚è¯¥ç ”ç©¶æ¶‰åŠæ¥è‡ªä¸åŒèƒŒæ™¯ä¸”å­˜åœ¨è¨€è¯­éšœç¢çš„å‚ä¸è€…ã€‚æ‰€å¾—æ•°æ®é›†ä»¥åŠä¸æ‰‹å†Œå’Œå¼€æºå·¥å…·ä¸€èµ·ï¼Œå¯ä¾›ç ”ç©¶äººå‘˜å’Œå®è·µäººå‘˜ä½¿ç”¨ï¼Œä»¥åˆ›å»ºé€‚åˆç‰¹å®šå—æŸè¯­éŸ³ä¸ªä½“éœ€æ±‚çš„åŒ…å®¹æ€§ASRæŠ€æœ¯ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜å±•ç¤ºäº†å¾®è°ƒå¼€æºASRæ¨¡å‹ä»¥æ›´å¥½åœ°è¯†åˆ«é˜¿åè¯­ä¸­çš„å—æŸè¯­éŸ³çš„åˆæ­¥ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02428v1">PDF</a> This version has been reviewed and accepted for presentation at the   InterSpeech 2025 conference to be held in Rotterdam from 17 to 21 August. 5   pages and 3 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ”¶é›†è¯­éŸ³æ ·æœ¬çš„æ–¹æ³•ï¼Œç”¨äºæ„å»ºé’ˆå¯¹å—æŸè¯­éŸ³ï¼Œå°¤å…¶æ˜¯ä½èµ„æºè¯­è¨€çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ã€‚ç ”ç©¶æ—¨åœ¨é€šè¿‡å¼€å‘æœ€ä½³å®è·µâ€œæ‰‹å†Œâ€å’ŒåŸ¹è®­ææ–™ï¼Œå®ç°ç¤¾åŒºé©±åŠ¨çš„æ•°æ®æ”¶é›†å’ŒASRæ¨¡å‹æ„å»ºï¼Œä»è€Œæ™®åŠASRæŠ€æœ¯å’Œæ•°æ®é‡‡é›†ã€‚ä½œä¸ºæ¦‚å¿µè¯æ˜ï¼Œæœ¬ç ”ç©¶æ•´ç†äº†é¦–ä¸ªå¼€æºçš„å—æŸè¯­éŸ³æ•°æ®é›†â€”â€”é˜¿åè¯­ï¼šä¸€ç§åœ¨åŠ çº³å¹¿æ³›ä½¿ç”¨çš„åœŸè‘—è¯­è¨€ã€‚ç ”ç©¶æ¶‰åŠæ¥è‡ªä¸åŒèƒŒæ™¯ä¸”å­˜åœ¨è¯­éŸ³ç¼ºé™·çš„å‚ä¸è€…ã€‚æ‰€å¾—æ•°æ®é›†ä»¥åŠæ‰‹å†Œå’Œå¼€æºå·¥å…·å‡å…¬å¼€å¯ç”¨ï¼Œä»¥ä¾¿ç ”ç©¶è€…å’Œä»ä¸šè€…æ ¹æ®è¯­éŸ³éšœç¢è€…çš„ç‹¬ç‰¹éœ€æ±‚åˆ›å»ºåŒ…å®¹æ€§çš„ASRæŠ€æœ¯ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜å±•ç¤ºäº†å¾®è°ƒå¼€æºASRæ¨¡å‹ä»¥æ›´å¥½åœ°è¯†åˆ«å—æŸçš„é˜¿åè¯­çš„åˆæ­¥ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹å—æŸè¯­éŸ³ï¼Œç‰¹åˆ«æ˜¯ä½èµ„æºè¯­è¨€çš„ASRæ¨¡å‹çš„æ•°æ®æ”¶é›†æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶ç›®æ ‡æ˜¯æ™®åŠASRæŠ€æœ¯å’Œæ•°æ®é‡‡é›†ï¼Œé€šè¿‡å¼€å‘æœ€ä½³å®è·µæ‰‹å†Œå’ŒåŸ¹è®­ææ–™æ¥å®ç°ã€‚</li>
<li>ä½œä¸ºæ¦‚å¿µè¯æ˜ï¼Œæœ¬ç ”ç©¶åˆ›å»ºäº†é¦–ä¸ªå¼€æºçš„é˜¿åè¯­å—æŸè¯­éŸ³æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†åŒ…æ‹¬äº†æ¥è‡ªä¸åŒèƒŒæ™¯ä¸”å­˜åœ¨è¯­éŸ³éšœç¢çš„å‚ä¸è€…ã€‚</li>
<li>æ•°æ®é›†ã€æ‰‹å†Œå’Œå¼€æºå·¥å…·å‡å…¬å¼€å¯ç”¨ï¼Œä¾¿äºç ”ç©¶å’Œå¼€å‘é€‚åº”è¯­éŸ³éšœç¢è€…éœ€æ±‚çš„ASRæŠ€æœ¯ã€‚</li>
<li>ç ”ç©¶å±•ç¤ºäº†å¾®è°ƒå¼€æºASRæ¨¡å‹ä»¥ä¼˜åŒ–è¯†åˆ«å—æŸé˜¿åè¯­çš„èƒ½åŠ›çš„åˆæ­¥æˆæœã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºæ„å»ºæ›´å…·åŒ…å®¹æ€§çš„ASRæŠ€æœ¯åšå‡ºäº†è´¡çŒ®ï¼Œç‰¹åˆ«å…³æ³¨è¯­éŸ³éšœç¢è€…çš„éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02428">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2323360bf798a37040ca104aeb85a049.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af35a69688ad5d749875ff80ed2284c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7eadc60290288f49a81787d38438b48c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2784f7d2430dea1d94f6057ba9b48edc.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Adaptability-of-ASR-Models-on-Low-Resource-Language-A-Comparative-Study-of-Whisper-and-Wav2Vec-BERT-on-Bangla"><a href="#Adaptability-of-ASR-Models-on-Low-Resource-Language-A-Comparative-Study-of-Whisper-and-Wav2Vec-BERT-on-Bangla" class="headerlink" title="Adaptability of ASR Models on Low-Resource Language: A Comparative Study   of Whisper and Wav2Vec-BERT on Bangla"></a>Adaptability of ASR Models on Low-Resource Language: A Comparative Study   of Whisper and Wav2Vec-BERT on Bangla</h2><p><strong>Authors:Md Sazzadul Islam Ridoy, Sumi Akter, Md. Aminur Rahman</strong></p>
<p>In recent years, neural models trained on large multilingual text and speech datasets have shown great potential for supporting low-resource languages. This study investigates the performances of two state-of-the-art Automatic Speech Recognition (ASR) models, OpenAIâ€™s Whisper (Small &amp; Large-V2) and Facebookâ€™s Wav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments using two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to evaluate model performances. Through systematic fine-tuning and hyperparameter optimization, including learning rate, epochs, and model checkpoint selection, we have compared the models based on Word Error Rate (WER), Character Error Rate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model outperformed Whisper across all key evaluation metrics, demonstrated superior performance while requiring fewer computational resources, and offered valuable insights to develop robust speech recognition systems in low-resource linguistic settings. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œåœ¨å¤§å‹å¤šè¯­è¨€æ–‡æœ¬å’Œè¯­éŸ³æ•°æ®é›†ä¸Šè®­ç»ƒçš„ç¥ç»ç½‘ç»œæ¨¡å‹å·²æ˜¾ç¤ºå‡ºæ”¯æŒä½èµ„æºè¯­è¨€çš„å·¨å¤§æ½œåŠ›ã€‚æœ¬ç ”ç©¶è°ƒæŸ¥äº†ä¸¤ç§æœ€æ–°è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬OpenAIçš„whisperï¼ˆå°å‹ä¸å¤§å‹-V2ï¼‰å’ŒFacebookçš„Wav2Vec-BERTåœ¨ä½èµ„æºè¯­è¨€é‚¦æ‹‰è¯­ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸¤ä¸ªå…¬å¼€å¯ç”¨çš„æ•°æ®é›†Mozilla Common Voice-17å’ŒOpenSLRæ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡ç³»ç»Ÿå¾®è°ƒã€è¶…å‚æ•°ä¼˜åŒ–ï¼ŒåŒ…æ‹¬å­¦ä¹ ç‡ã€å‘¨æœŸå’Œæ¨¡å‹æ£€æŸ¥ç‚¹é€‰æ‹©ï¼Œæˆ‘ä»¬æ ¹æ®å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ã€è®­ç»ƒæ—¶é—´å’Œè®¡ç®—æ•ˆç‡æ¯”è¾ƒäº†è¿™äº›æ¨¡å‹ã€‚Wav2Vec-BERTæ¨¡å‹åœ¨å„é¡¹å…³é”®è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜äºwhisperï¼Œè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼ŒåŒæ—¶éœ€è¦è¾ƒå°‘çš„è®¡ç®—èµ„æºï¼Œä¸ºåœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­å¼€å‘ç¨³å¥çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿæä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01931v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è¿‘å¹´æ¥ï¼ŒåŸºäºå¤§å‹å¤šè¯­ç§æ–‡æœ¬å’Œè¯­éŸ³æ•°æ®é›†çš„ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€æ”¯æŒæ–¹é¢è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚æœ¬ç ”ç©¶é‡‡ç”¨æœ€å…ˆè¿›çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹OpenAIçš„Whisperï¼ˆå°å‹å’Œå¤§å‹V2ç‰ˆæœ¬ï¼‰å’ŒFacebookçš„Wav2Vec-BERTå¯¹ä½èµ„æºè¯­è¨€Banglaè¿›è¡Œæ€§èƒ½è¯„ä¼°ã€‚é€šè¿‡å®éªŒè¯„ä¼°ï¼ŒWav2Vec-BERTæ¨¡å‹åœ¨å„é¡¹æŒ‡æ ‡ä¸Šä¼˜äºWhisperï¼Œè¡¨ç°æ›´ä¼˜ç§€ä¸”è®¡ç®—èµ„æºéœ€æ±‚æ›´å°‘ï¼Œä¸ºåœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­å¼€å‘ç¨³å¥çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿæä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€æ”¯æŒæ–¹é¢å±•ç°æ½œåŠ›ã€‚</li>
<li>è¯„ä¼°äº†ä¸¤ç§å…ˆè¿›ASRæ¨¡å‹ï¼šOpenAIçš„Whisperå’ŒFacebookçš„Wav2Vec-BERTåœ¨Banglaè¯­è¨€ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨Mozilla Common Voice-17å’ŒOpenSLRä¸¤ä¸ªå…¬å¼€æ•°æ®é›†è¿›è¡Œå®éªŒè¯„ä¼°ã€‚</li>
<li>é€šè¿‡ç³»ç»Ÿå¾®è°ƒã€è¶…å‚æ•°ä¼˜åŒ–ï¼ˆåŒ…æ‹¬å­¦ä¹ ç‡ã€å‘¨æœŸå’Œæ¨¡å‹æ£€æŸ¥ç‚¹é€‰æ‹©ï¼‰æ¥æ¯”è¾ƒæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>Wav2Vec-BERTæ¨¡å‹åœ¨å…³é”®è¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºWhisperã€‚</li>
<li>Wav2Vec-BERTæ¨¡å‹è¡¨ç°æ›´ä¼˜ç§€ä¸”éœ€è¦è¾ƒå°‘çš„è®¡ç®—èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01931">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ee8b167dbcb81e11a529484c8efa843.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7b527a3b7e4536a666cac8de77e84d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f67259ea5580a1ff1cdfebcb3efc7119.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ca33b236e5244858a623fdc84001582.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc52abb08df05f9a47efc2827258ce16.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a281e290c39f368cea08077acd214cdc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a575fdd2c735e499a8decc920f3da9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffa784d09e1bd8e1d2c67ac293d4aa4d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PERTINENCE-Input-based-Opportunistic-Neural-Network-Dynamic-Execution"><a href="#PERTINENCE-Input-based-Opportunistic-Neural-Network-Dynamic-Execution" class="headerlink" title="PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution"></a>PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution</h2><p><strong>Authors:Omkar Shende, Gayathri Ananthanarayanan, Marcello Traiola</strong></p>
<p>Deep neural networks (DNNs) have become ubiquitous thanks to their remarkable ability to model complex patterns across various domains such as computer vision, speech recognition, robotics, etc. While large DNN models are often more accurate than simpler, lightweight models, they are also resource- and energy-hungry. Hence, it is imperative to design methods to reduce reliance on such large models without significant degradation in output accuracy. The high computational cost of these models is often necessary only for a reduced set of challenging inputs, while lighter models can handle most simple ones. Thus, carefully combining properties of existing DNN models in a dynamic, input-based way opens opportunities to improve efficiency without impacting accuracy.   In this work, we introduce PERTINENCE, a novel online method designed to analyze the complexity of input features and dynamically select the most suitable model from a pre-trained set to process a given input effectively. To achieve this, we employ a genetic algorithm to explore the training space of an ML-based input dispatcher, enabling convergence towards the Pareto front in the solution space that balances overall accuracy and computational efficiency.   We showcase our approach on state-of-the-art Convolutional Neural Networks (CNNs) trained on the CIFAR-10 and CIFAR-100, as well as Vision Transformers (ViTs) trained on TinyImageNet dataset. We report results showing PERTINENCEâ€™s ability to provide alternative solutions to existing state-of-the-art models in terms of trade-offs between accuracy and number of operations. By opportunistically selecting among models trained for the same task, PERTINENCE achieves better or comparable accuracy with up to 36% fewer operations. </p>
<blockquote>
<p>æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰ç”±äºå…¶è·¨å¤šä¸ªé¢†åŸŸï¼ˆå¦‚è®¡ç®—æœºè§†è§‰ã€è¯­éŸ³è¯†åˆ«ã€æœºå™¨äººç­‰ï¼‰å»ºç«‹å¤æ‚æ¨¡å¼çš„å“è¶Šèƒ½åŠ›è€Œå˜å¾—æ— å¤„ä¸åœ¨ã€‚è™½ç„¶å¤§å‹DNNæ¨¡å‹é€šå¸¸æ¯”ç®€å•ã€è½»é‡çº§çš„æ¨¡å‹æ›´å‡†ç¡®ï¼Œä½†å®ƒä»¬ä¹Ÿéœ€è¦æ›´å¤šçš„èµ„æºå’Œèƒ½æºã€‚å› æ­¤ï¼Œè®¾è®¡æ–¹æ³•æ¥å‡å°‘å¯¹è¿™æ ·çš„å¤§æ¨¡å‹çš„ä¾èµ–ï¼ŒåŒæ—¶ä¸ä¼šæ˜¾è‘—é™ä½è¾“å‡ºç²¾åº¦ï¼Œè¿™æ˜¯è‡³å…³é‡è¦çš„ã€‚è¿™äº›æ¨¡å‹çš„é«˜è®¡ç®—æˆæœ¬é€šå¸¸åªæ˜¯ä¸ºäº†å‡å°‘ä¸€äº›å…·æœ‰æŒ‘æˆ˜æ€§çš„è¾“å…¥è€Œå¿…éœ€çš„ï¼Œè€Œè¾ƒè½»çš„æ¨¡å‹å¯ä»¥å¤„ç†å¤§å¤šæ•°ç®€å•çš„è¾“å…¥ã€‚å› æ­¤ï¼Œä»¥ä¸€ç§åŠ¨æ€ã€åŸºäºè¾“å…¥çš„æ–¹å¼è°¨æ…åœ°ç»“åˆç°æœ‰DNNæ¨¡å‹çš„å±æ€§ï¼Œä¸ºæé«˜æ•ˆç‡æä¾›äº†æœºä¼šï¼Œè€Œä¸ä¼šå½±å“å‡†ç¡®æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†PERTINENCEï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„åœ¨çº¿æ–¹æ³•ï¼Œæ—¨åœ¨åˆ†æè¾“å…¥ç‰¹å¾å¤æ‚æ€§ï¼Œå¹¶ä»é¢„è®­ç»ƒæ¨¡å‹é›†ä¸­åŠ¨æ€é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹æ¥å¤„ç†ç»™å®šè¾“å…¥ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬é‡‡ç”¨é—ä¼ ç®—æ³•æ¥æ¢ç´¢åŸºäºæœºå™¨å­¦ä¹ çš„è¾“å…¥è°ƒåº¦å™¨çš„è®­ç»ƒç©ºé—´ï¼Œä½¿è§£å†³æ–¹æ¡ˆåœ¨æ•´ä½“å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æˆ‘ä»¬åœ¨CIFAR-10å’ŒCIFAR-100ä¸Šè®­ç»ƒçš„æœ€æ–°å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä»¥åŠTinyImageNetæ•°æ®é›†ä¸Šè®­ç»ƒçš„è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚æŠ¥å‘Šç»“æœæ˜¾ç¤ºï¼ŒPERTINENCEèƒ½å¤Ÿåœ¨å‡†ç¡®æ€§å’Œæ“ä½œæ¬¡æ•°ä¹‹é—´æä¾›å¯¹ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹çš„æ›¿ä»£è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡åœ¨æœ‰ç›¸åŒä»»åŠ¡çš„æ¨¡å‹ä¸­åšå‡ºé€‰æ‹©ï¼ŒPERTINENCEåœ¨å…·æœ‰é«˜è¾¾36%æ›´å°‘æ“ä½œçš„æƒ…å†µä¸‹å®ç°äº†æ›´å¥½çš„æˆ–ç›¸å½“çš„å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01695v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨å¤šä¸ªé¢†åŸŸå¦‚è®¡ç®—æœºè§†è§‰ã€è¯­éŸ³è¯†åˆ«ã€æœºå™¨äººç­‰ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨¡å¼è¯†åˆ«èƒ½åŠ›ã€‚è™½ç„¶å¤§å‹DNNæ¨¡å‹é€šå¸¸æ¯”è½»é‡çº§æ¨¡å‹æ›´å‡†ç¡®ï¼Œä½†å®ƒä»¬æ¶ˆè€—å¤§é‡èµ„æºå’Œèƒ½æºã€‚å› æ­¤ï¼Œè®¾è®¡æ–¹æ³•æ¥å‡å°‘å¯¹å¤§å‹æ¨¡å‹çš„ä¾èµ–ï¼ŒåŒæ—¶ä¸æ˜¾è‘—é™ä½è¾“å‡ºå‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPERTINENCEçš„æ–°å‹åœ¨çº¿æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ—¨åœ¨åˆ†æè¾“å…¥ç‰¹å¾å¤æ‚æ€§ï¼Œå¹¶ä»é¢„è®­ç»ƒæ¨¡å‹é›†ä¸­åŠ¨æ€é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹æ¥å¤„ç†ç»™å®šè¾“å…¥ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬é‡‡ç”¨é—ä¼ ç®—æ³•æ¥æ¢ç´¢MLè¾“å…¥è°ƒåº¦å™¨çš„è®­ç»ƒç©ºé—´ï¼Œä»¥åœ¨è§£å†³æ–¹æ¡ˆç©ºé—´ä¸­å®ç°å¹³è¡¡æ€»ä½“å‡†ç¡®åº¦å’Œè®¡ç®—æ•ˆç‡çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚æˆ‘ä»¬åœ¨CIFAR-10å’ŒCIFAR-100ä¸Šè®­ç»ƒçš„æœ€æ–°å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä»¥åŠTinyImageNetæ•°æ®é›†ä¸Šè®­ç»ƒçš„è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼ŒPERTINENCEèƒ½å¤Ÿåœ¨å‡†ç¡®åº¦ä¸è¿ç®—æ¬¡æ•°ä¹‹é—´æä¾›å¯¹ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹çš„æ›¿ä»£è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡åœ¨ç›¸åŒä»»åŠ¡è®­ç»ƒçš„æ¨¡å‹ä¹‹é—´çµæ´»é€‰æ‹©ï¼ŒPERTINENCEå®ç°äº†æ›´å¥½çš„æˆ–ç›¸å½“çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶å‡å°‘äº†é«˜è¾¾36%çš„è¿ç®—æ¬¡æ•°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¼ºå¤§çš„æ¨¡å¼è¯†åˆ«èƒ½åŠ›ï¼Œä½†å¤§å‹æ¨¡å‹èµ„æºæ¶ˆè€—å¤§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹åœ¨çº¿æ–¹æ³•PERTINENCEï¼Œå¯æ ¹æ®è¾“å…¥ç‰¹å¾å¤æ‚æ€§åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹ã€‚</li>
<li>PERTINENCEé‡‡ç”¨é—ä¼ ç®—æ³•æ¢ç´¢MLè¾“å…¥è°ƒåº¦å™¨çš„è®­ç»ƒç©ºé—´ï¼Œå®ç°å¸•ç´¯æ‰˜æœ€ä¼˜ï¼Œå¹³è¡¡å‡†ç¡®åº¦å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPERTINENCEæä¾›äº†ä¸ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ç›¸å½“çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†è¿ç®—æ¬¡æ•°ã€‚</li>
<li>PERTINENCEæ–¹æ³•å…·æœ‰æ½œåŠ›åœ¨æé«˜è®¡ç®—æ•ˆç‡çš„åŒæ—¶ä¸æŸå¤±å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡çµæ´»é€‰æ‹©é’ˆå¯¹åŒä¸€ä»»åŠ¡è®­ç»ƒçš„æ¨¡å‹ï¼ŒPERTINENCEå®ç°äº†æ•ˆç‡ä¸æ€§èƒ½çš„å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01695">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-da39ef3aaf347829c0668814c74454b3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f1c61c65e4ed146392678685727d899.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aada67893ff072ed115ca7ae4e91048e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6725ec9df6cc18c91bb1949a0e6bd33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e31bb16184536bfcdd48b4c784374b2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d8bd734b7c41f1b376db7107be3c1bd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edba3624b33be438bc8d430a00ed97c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07ec59fb84d7701dbfbaf7cdcd0e1b8e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Learning-from-Random-Subspace-Exploration-Generalized-Test-Time-Augmentation-with-Self-supervised-Distillation"><a href="#Learning-from-Random-Subspace-Exploration-Generalized-Test-Time-Augmentation-with-Self-supervised-Distillation" class="headerlink" title="Learning from Random Subspace Exploration: Generalized Test-Time   Augmentation with Self-supervised Distillation"></a>Learning from Random Subspace Exploration: Generalized Test-Time   Augmentation with Self-supervised Distillation</h2><p><strong>Authors:Andrei Jelea, Ahmed Nabil Belbachir, Marius Leordeanu</strong></p>
<p>We introduce Generalized Test-Time Augmentation (GTTA), a highly effective method for improving the performance of a trained model, which unlike other existing Test-Time Augmentation approaches from the literature is general enough to be used off-the-shelf for many vision and non-vision tasks, such as classification, regression, image segmentation and object detection. By applying a new general data transformation, that randomly perturbs multiple times the PCA subspace projection of a test input, GTTA forms robust ensembles at test time in which, due to sound statistical properties, the structural and systematic noises in the initial input data is filtered out and final estimator errors are reduced. Different from other existing methods, we also propose a final self-supervised learning stage in which the ensemble output, acting as an unsupervised teacher, is used to train the initial single student model, thus reducing significantly the test time computational cost, at no loss in accuracy. Our tests and comparisons to strong TTA approaches and SoTA models on various vision and non-vision well-known datasets and tasks, such as image classification and segmentation, speech recognition and house price prediction, validate the generality of the proposed GTTA. Furthermore, we also prove its effectiveness on the more specific real-world task of salmon segmentation and detection in low-visibility underwater videos, for which we introduce DeepSalmon, the largest dataset of its kind in the literature. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åä¸ºå¹¿ä¹‰æµ‹è¯•æ—¶é—´å¢å¼ºï¼ˆGTTAï¼‰çš„æœ‰æ•ˆæ–¹æ³•ï¼Œç”¨äºæé«˜è®­ç»ƒæ¨¡å‹æ€§èƒ½ã€‚ä¸æ–‡çŒ®ä¸­çš„å…¶ä»–ç°æœ‰æµ‹è¯•æ—¶é—´å¢å¼ºæ–¹æ³•ä¸åŒï¼ŒGTTAå…·æœ‰è¶³å¤Ÿçš„é€šç”¨æ€§ï¼Œå¯ä»¥åº”ç”¨äºè®¸å¤šè§†è§‰å’Œéè§†è§‰ä»»åŠ¡ï¼Œä¾‹å¦‚åˆ†ç±»ã€å›å½’ã€å›¾åƒåˆ†å‰²å’Œå¯¹è±¡æ£€æµ‹ã€‚é€šè¿‡åº”ç”¨æ–°çš„é€šç”¨æ•°æ®è½¬æ¢ï¼Œå¤šæ¬¡éšæœºæ‰°åŠ¨æµ‹è¯•è¾“å…¥çš„PCAå­ç©ºé—´æŠ•å½±ï¼ŒGTTAåœ¨æµ‹è¯•æ—¶å½¢æˆç¨³å¥çš„é›†åˆã€‚ç”±äºå…·æœ‰è‰¯å¥½çš„ç»Ÿè®¡å±æ€§ï¼Œåˆå§‹è¾“å…¥æ•°æ®ä¸­çš„ç»“æ„å’Œç³»ç»Ÿå™ªå£°è¢«è¿‡æ»¤æ‰ï¼Œæœ€ç»ˆä¼°è®¡å™¨è¯¯å·®å‡å°ã€‚ä¸å…¶ä»–ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªæœ€ç»ˆçš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ é˜¶æ®µï¼Œå…¶ä¸­é›†åˆè¾“å‡ºä½œä¸ºæ— ç›‘ç£æ•™å¸ˆï¼Œç”¨äºè®­ç»ƒåˆå§‹å•ä¸€å­¦ç”Ÿæ¨¡å‹ï¼Œä»è€Œåœ¨ä¸å½±å“ç²¾åº¦çš„æƒ…å†µä¸‹æ˜¾è‘—é™ä½æµ‹è¯•æ—¶é—´çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬çš„æµ‹è¯•å’Œå¯¹å„ç§è§†è§‰å’Œéè§†è§‰çŸ¥åæ•°æ®é›†å’Œä»»åŠ¡ä¸Šçš„å¼ºå¤§TTAæ–¹æ³•å’Œæœ€æ–°æ¨¡å‹çš„æ¯”è¾ƒï¼Œå¦‚å›¾åƒåˆ†ç±»å’Œåˆ†å‰²ã€è¯­éŸ³è¯†åˆ«å’Œæˆ¿å±‹ä»·æ ¼é¢„æµ‹ç­‰ï¼ŒéªŒè¯äº†æ‰€æå‡ºGTTAçš„é€šç”¨æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†å…¶åœ¨æ›´å…·ä½“çš„ç°å®ä¸–ç•Œä»»åŠ¡â€”â€”ä½å¯è§åº¦æ°´ä¸‹è§†é¢‘ä¸­çš„é²‘é±¼åˆ†å‰²å’Œæ£€æµ‹â€”â€”çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ­¤æˆ‘ä»¬å¼•å…¥äº†DeepSalmonï¼Œè¿™æ˜¯æ–‡çŒ®ä¸­åŒç±»æœ€å¤§çš„æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01347v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†å¹¿ä¹‰æµ‹è¯•æ—¶é—´å¢å¼ºï¼ˆGTTAï¼‰æ–¹æ³•ï¼Œç”¨äºæé«˜è®­ç»ƒæ¨¡å‹æ€§èƒ½ã€‚ä¸å…¶ä»–ç°æœ‰æµ‹è¯•æ—¶é—´å¢å¼ºæ–¹æ³•ä¸åŒï¼ŒGTTAå…·æœ‰è¶³å¤Ÿçš„é€šç”¨æ€§ï¼Œå¯å¹¿æ³›åº”ç”¨äºè®¸å¤šè§†è§‰å’Œéè§†è§‰ä»»åŠ¡ï¼Œå¦‚åˆ†ç±»ã€å›å½’ã€å›¾åƒåˆ†å‰²å’Œå¯¹è±¡æ£€æµ‹ã€‚å®ƒé€šè¿‡æ–°çš„é€šç”¨æ•°æ®è½¬æ¢ï¼Œéšæœºæ‰°åŠ¨æµ‹è¯•è¾“å…¥çš„PCAå­ç©ºé—´æŠ•å½±ï¼Œåœ¨æµ‹è¯•æ—¶å½¢æˆç¨³å¥çš„é›†åˆï¼Œè¿‡æ»¤æ‰åˆå§‹è¾“å…¥æ•°æ®ä¸­çš„ç»“æ„å’Œç³»ç»Ÿæ€§å™ªå£°ï¼Œå‡å°‘æœ€ç»ˆä¼°è®¡å™¨è¯¯å·®ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æœ€ç»ˆçš„è‡ªç›‘ç£å­¦ä¹ é˜¶æ®µï¼Œåˆ©ç”¨é›†åˆè¾“å‡ºä½œä¸ºæ— ç›‘ç£æ•™å¸ˆæ¥è®­ç»ƒåˆå§‹å•ä¸€å­¦ç”Ÿæ¨¡å‹ï¼Œä»è€Œæ˜¾è‘—å‡å°‘æµ‹è¯•æ—¶é—´è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¸æŸå¤±å‡†ç¡®æ€§ã€‚åœ¨å¤šç§è§†è§‰å’Œéè§†è§‰çš„çŸ¥åæ•°æ®é›†å’Œä»»åŠ¡ä¸Šçš„æµ‹è¯•ï¼ŒéªŒè¯äº†GTTAçš„é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†Generalized Test-Time Augmentationï¼ˆGTTAï¼‰æ–¹æ³•ï¼Œé€‚ç”¨äºå¤šç§ä»»åŠ¡ï¼Œå¦‚åˆ†ç±»ã€å›å½’ã€å›¾åƒåˆ†å‰²å’Œå¯¹è±¡æ£€æµ‹ã€‚</li>
<li>é€šè¿‡æ–°çš„é€šç”¨æ•°æ®è½¬æ¢ï¼Œéšæœºæ‰°åŠ¨æµ‹è¯•è¾“å…¥çš„PCAå­ç©ºé—´æŠ•å½±ï¼Œå½¢æˆç¨³å¥é›†åˆï¼Œå‡å°‘ä¼°è®¡å™¨è¯¯å·®ã€‚</li>
<li>å¼•å…¥è‡ªç›‘ç£å­¦ä¹ é˜¶æ®µï¼Œåˆ©ç”¨é›†åˆè¾“å‡ºä½œä¸ºæ— ç›‘ç£æ•™å¸ˆæ¥è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ï¼Œé™ä½æµ‹è¯•æ—¶é—´æˆæœ¬ã€‚</li>
<li>GTTAæ–¹æ³•å…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ï¼Œå¯ä»¥åœ¨å„ç§è§†è§‰å’Œéè§†è§‰æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚</li>
<li>ç›¸æ¯”å…¶ä»–å¼ºTTAæ–¹æ³•å’Œæœ€æ–°æ¨¡å‹ï¼ŒGTTAåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚</li>
<li>é¦–æ¬¡åœ¨æ–‡çŒ®ä¸­å¼•å…¥äº†DeepSalmonæ•°æ®é›†ï¼Œç”¨äºä½å¯è§åº¦æ°´ä¸‹è§†é¢‘çš„é²‘é±¼åˆ†å‰²å’Œæ£€æµ‹ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01347">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1d94aac69cf1c9d4bf58495b2408c0b5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MambAttention-Mamba-with-Multi-Head-Attention-for-Generalizable-Single-Channel-Speech-Enhancement"><a href="#MambAttention-Mamba-with-Multi-Head-Attention-for-Generalizable-Single-Channel-Speech-Enhancement" class="headerlink" title="MambAttention: Mamba with Multi-Head Attention for Generalizable   Single-Channel Speech Enhancement"></a>MambAttention: Mamba with Multi-Head Attention for Generalizable   Single-Channel Speech Enhancement</h2><p><strong>Authors:Nikolai Lund KÃ¼hne, Jesper Jensen, Jan Ã˜stergaard, Zheng-Hua Tan</strong></p>
<p>With the advent of new sequence models like Mamba and xLSTM, several studies have shown that these models match or outperform state-of-the-art models in single-channel speech enhancement, automatic speech recognition, and self-supervised audio representation learning. However, prior research has demonstrated that sequence models like LSTM and Mamba tend to overfit to the training set. To address this issue, previous works have shown that adding self-attention to LSTMs substantially improves generalization performance for single-channel speech enhancement. Nevertheless, neither the concept of hybrid Mamba and time-frequency attention models nor their generalization performance have been explored for speech enhancement. In this paper, we propose a novel hybrid architecture, MambAttention, which combines Mamba and shared time- and frequency-multi-head attention modules for generalizable single-channel speech enhancement. To train our model, we introduce VoiceBank+Demand Extended (VB-DemandEx), a dataset inspired by VoiceBank+Demand but with more challenging noise types and lower signal-to-noise ratios. Trained on VB-DemandEx, our proposed MambAttention model significantly outperforms existing state-of-the-art LSTM-, xLSTM-, Mamba-, and Conformer-based systems of similar complexity across all reported metrics on two out-of-domain datasets: DNS 2020 and EARS-WHAM_v2, while matching their performance on the in-domain dataset VB-DemandEx. Ablation studies highlight the role of weight sharing between the time- and frequency-multi-head attention modules for generalization performance. Finally, we explore integrating the shared time- and frequency-multi-head attention modules with LSTM and xLSTM, which yields a notable performance improvement on the out-of-domain datasets. However, our MambAttention model remains superior on both out-of-domain datasets across all reported evaluation metrics. </p>
<blockquote>
<p>éšç€Mambaå’ŒxLSTMç­‰æ–°åºåˆ—æ¨¡å‹çš„å‡ºç°ï¼Œå¤šé¡¹ç ”ç©¶è¡¨æ˜è¿™äº›æ¨¡å‹åœ¨å•é€šé“è¯­éŸ³å¢å¼ºã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œè‡ªæˆ‘ç›‘ç£éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ æ–¹é¢è¾¾åˆ°äº†æˆ–è¶…è¶Šäº†ç°æœ‰æ¨¡å‹çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼ŒåƒLSTMå’ŒMambaè¿™æ ·çš„åºåˆ—æ¨¡å‹å®¹æ˜“è¿‡åº¦æ‹Ÿåˆè®­ç»ƒé›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä»¥å‰çš„ç ”ç©¶è¡¨æ˜ï¼Œåœ¨LSTMä¸­æ·»åŠ è‡ªæ³¨æ„åŠ›å¯ä»¥å¤§å¤§æé«˜å•é€šé“è¯­éŸ³å¢å¼ºçš„æ³›åŒ–æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…³äºæ··åˆMambaå’Œæ—¶é—´-é¢‘ç‡æ³¨æ„åŠ›æ¨¡å‹çš„æ¦‚å¿µåŠå…¶æ³›åŒ–æ€§èƒ½åœ¨è¯­éŸ³å¢å¼ºæ–¹é¢çš„æ¢ç´¢å°šæœªå‡ºç°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ··åˆæ¶æ„MambAttentionï¼Œå®ƒå°†Mambaå’Œå…±äº«çš„æ—¶é—´ä¸é¢‘ç‡å¤šå¤´æ³¨æ„åŠ›æ¨¡å—ç›¸ç»“åˆï¼Œç”¨äºé€šç”¨çš„å•é€šé“è¯­éŸ³å¢å¼ºã€‚ä¸ºäº†è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†VoiceBank+Demand Extendedï¼ˆVB-DemandExï¼‰æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä»¥VoiceBank+Demandä¸ºçµæ„Ÿï¼Œä½†å…·æœ‰æ›´å…·æŒ‘æˆ˜æ€§çš„å™ªå£°ç±»å‹å’Œæ›´ä½çš„ä¿¡å™ªæ¯”ã€‚åœ¨VB-DemandExä¸Šè®­ç»ƒçš„MambAttentionæ¨¡å‹åœ¨ä¸¤ä¸ªåŸŸå¤–æ•°æ®é›†DNS 2020å’ŒEARS-WHAM_v2ä¸Šæ‰€æœ‰æŠ¥å‘Šçš„æŒ‡æ ‡ä¸­éƒ½æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰å¤æ‚çš„åŸºäºLSTMã€xLSTMã€Mambaå’ŒConformerçš„ç³»ç»Ÿï¼ŒåŒæ—¶åœ¨åŸŸå†…æ•°æ®é›†VB-DemandExä¸Šçš„æ€§èƒ½ä¸ä¹‹ç›¸åŒ¹é…ã€‚æ¶ˆèç ”ç©¶çªå‡ºäº†æ—¶é—´ä¸é¢‘ç‡å¤šå¤´æ³¨æ„åŠ›æ¨¡å—æƒé‡å…±äº«åœ¨æ³›åŒ–æ€§èƒ½ä¸­çš„ä½œç”¨ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢ç´¢äº†å°†å…±äº«çš„æ—¶é—´ä¸é¢‘ç‡å¤šå¤´æ³¨æ„åŠ›æ¨¡å—ä¸LSTMå’ŒxLSTMé›†æˆï¼Œè¿™åœ¨åŸŸå¤–æ•°æ®é›†ä¸Šäº§ç”Ÿäº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„MambAttentionæ¨¡å‹åœ¨ä¸¤ä¸ªåŸŸå¤–æ•°æ®é›†ä¸Šçš„æ‰€æœ‰æŠ¥å‘Šçš„è¯„ä»·æŒ‡æ ‡ä¸Šä»ç„¶è¡¨ç°æœ€ä½³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00966v1">PDF</a> Submitted to IEEE&#x2F;ACM Transactions on Audio, Speech, and Language   Processing for possible publication</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸ºMambAttentionçš„æ··åˆæ¶æ„ï¼Œç»“åˆäº†Mambaå’Œå…±äº«æ—¶é—´ã€é¢‘ç‡å¤šå¤´æ³¨æ„åŠ›æ¨¡å—ï¼Œç”¨äºé€šç”¨çš„å•é€šé“è¯­éŸ³å¢å¼ºã€‚ä¸ºè®­ç»ƒæ¨¡å‹ï¼Œç ”ç©¶å¼•å…¥äº†æ›´å…·æŒ‘æˆ˜æ€§çš„å™ªå£°ç±»å‹å’Œè¾ƒä½ä¿¡å™ªæ¯”çš„VoiceBank+Demandæ‰©å±•æ•°æ®é›†ã€‚åœ¨è·¨åŸŸæ•°æ®é›†DNS 2020å’ŒEARS-WHAM_v2ä¸Šï¼ŒMambAttentionæ¨¡å‹æ˜¾è‘—ä¼˜äºå…¶ä»–å…ˆè¿›çš„LSTMã€xLSTMã€Mambaå’ŒConformeræ¨¡å‹ï¼ŒåŒæ—¶åœ¨é¢†åŸŸå†…æ•°æ®é›†VB-DemandExä¸Šçš„æ€§èƒ½ç›¸åŒ¹é…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°åºåˆ—æ¨¡å‹ï¼ˆå¦‚Mambaå’ŒxLSTMï¼‰åœ¨è¯­éŸ³å¢å¼ºã€è¯­éŸ³è¯†åˆ«å’ŒéŸ³é¢‘è¡¨ç¤ºå­¦ä¹ æ–¹é¢è¡¨ç°å‡ºä¸ç°æœ‰å…ˆè¿›æŠ€æœ¯ç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ã€‚</li>
<li>LSTMå’ŒMambaæ¨¡å‹å®¹æ˜“è¿‡åº¦æ‹Ÿåˆè®­ç»ƒé›†ï¼Œæ·»åŠ è‡ªæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥æ”¹å–„å…¶æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ··åˆæ¶æ„MambAttentionï¼Œç»“åˆäº†Mambaå’Œå…±äº«æ—¶é—´ã€é¢‘ç‡å¤šå¤´æ³¨æ„åŠ›æ¨¡å—ï¼Œç”¨äºå•é€šé“è¯­éŸ³å¢å¼ºã€‚</li>
<li>å¼•å…¥äº†æ›´å…·æŒ‘æˆ˜æ€§çš„VoiceBank+Demandæ‰©å±•æ•°æ®é›†ï¼ˆVB-DemandExï¼‰ç”¨äºæ¨¡å‹è®­ç»ƒã€‚</li>
<li>MambAttentionæ¨¡å‹åœ¨è·¨åŸŸæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–å…ˆè¿›æ¨¡å‹ã€‚</li>
<li>æƒé‡å…±äº«åœ¨æ—¶é—´å’Œé¢‘ç‡å¤šå¤´æ³¨æ„åŠ›æ¨¡å—é—´å¯¹äºæ¨¡å‹çš„æ³›åŒ–æ€§èƒ½èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00966">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ae99ab8740a31542de7caf8a8b58dd7e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27433933ca222f56889973b3cbd8992e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Rectifying-Magnitude-Neglect-in-Linear-Attention"><a href="#Rectifying-Magnitude-Neglect-in-Linear-Attention" class="headerlink" title="Rectifying Magnitude Neglect in Linear Attention"></a>Rectifying Magnitude Neglect in Linear Attention</h2><p><strong>Authors:Qihang Fan, Huaibo Huang, Yuang Ai, ran He</strong></p>
<p>As the core operator of Transformers, Softmax Attention exhibits excellent global modeling capabilities. However, its quadratic complexity limits its applicability to vision tasks. In contrast, Linear Attention shares a similar formulation with Softmax Attention while achieving linear complexity, enabling efficient global information modeling. Nevertheless, Linear Attention suffers from a significant performance degradation compared to standard Softmax Attention. In this paper, we analyze the underlying causes of this issue based on the formulation of Linear Attention. We find that, unlike Softmax Attention, Linear Attention entirely disregards the magnitude information of the Query. This prevents the attention score distribution from dynamically adapting as the Query scales. As a result, despite its structural similarity to Softmax Attention, Linear Attention exhibits a significantly different attention score distribution. Based on this observation, we propose Magnitude-Aware Linear Attention (MALA), which modifies the computation of Linear Attention to fully incorporate the Queryâ€™s magnitude. This adjustment allows MALA to generate an attention score distribution that closely resembles Softmax Attention while exhibiting a more well-balanced structure. We evaluate the effectiveness of MALA on multiple tasks, including image classification, object detection, instance segmentation, semantic segmentation, natural language processing, speech recognition, and image generation. Our MALA achieves strong results on all of these tasks. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/qhfan/MALA">https://github.com/qhfan/MALA</a> </p>
<blockquote>
<p>ä½œä¸ºTransformerçš„æ ¸å¿ƒæ“ä½œå™¨ï¼ŒSoftmax Attentionå±•ç°å‡ºå‡ºè‰²çš„å…¨å±€å»ºæ¨¡èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶äºŒæ¬¡å¤æ‚åº¦é™åˆ¶äº†å…¶åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒLinear Attentionä¸Softmax Attentionå…·æœ‰ç›¸ä¼¼çš„å…¬å¼ï¼Œä½†å®ç°äº†çº¿æ€§å¤æ‚åº¦ï¼Œèƒ½å¤Ÿå®ç°æœ‰æ•ˆçš„å…¨å±€ä¿¡æ¯å»ºæ¨¡ã€‚ç„¶è€Œï¼Œä¸æ ‡å‡†çš„Softmax Attentionç›¸æ¯”ï¼ŒLinear Attentionçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åŸºäºLinear Attentionçš„å…¬å¼åˆ†ææ­¤é—®é¢˜çš„æ ¹æœ¬åŸå› ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸Softmax Attentionä¸åŒï¼ŒLinear Attentionå®Œå…¨å¿½ç•¥äº†Queryçš„å¹…åº¦ä¿¡æ¯ã€‚è¿™é˜»æ­¢äº†æ³¨æ„åŠ›å¾—åˆ†åˆ†å¸ƒåœ¨Queryè§„æ¨¡æ‰©å¤§æ—¶åŠ¨æ€é€‚åº”ã€‚å› æ­¤ï¼Œå°½ç®¡å…¶ä¸Softmax Attentionç»“æ„ç›¸ä¼¼ï¼Œä½†Linear Attentionçš„æ³¨æ„åŠ›å¾—åˆ†åˆ†å¸ƒå´å¤§ä¸ç›¸åŒã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†å¹…åº¦æ„ŸçŸ¥Linear Attentionï¼ˆMALAï¼‰ï¼Œå®ƒä¿®æ”¹äº†Linear Attentionçš„è®¡ç®—ä»¥å……åˆ†èå…¥Queryçš„å¹…åº¦ã€‚è¿™ä¸€è°ƒæ•´ä½¿MALAèƒ½å¤Ÿç”Ÿæˆä¸Softmax Attentionç›¸ä¼¼çš„æ³¨æ„åŠ›å¾—åˆ†åˆ†å¸ƒï¼ŒåŒæ—¶å±•ç°å‡ºæ›´å¹³è¡¡çš„ç»“æ„ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¯„ä¼°äº†MALAçš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å®ä¾‹åˆ†å‰²ã€è¯­ä¹‰åˆ†å‰²ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«å’Œå›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬çš„MALAåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†å¼ºå¤§çš„ç»“æœã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/qhfan/MALA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/qhfan/MALAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00698v1">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Transformerä¸­çš„æ ¸å¿ƒæ“ä½œï¼ŒåŒ…æ‹¬Softmax Attentionå’ŒLinear Attentionã€‚Softmax Attentionå…·æœ‰å‡ºè‰²çš„å…¨å±€å»ºæ¨¡èƒ½åŠ›ï¼Œä½†å…¶äºŒæ¬¡å¤æ‚æ€§é™åˆ¶äº†å…¶åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚Linear Attentionä¸Softmax Attentionå…·æœ‰ç›¸ä¼¼çš„å…¬å¼å½¢å¼ä½†å…·æœ‰çº¿æ€§å¤æ‚æ€§ï¼Œèƒ½å¤Ÿå®ç°é«˜æ•ˆçš„å…¨å±€ä¿¡æ¯å»ºæ¨¡ã€‚ç„¶è€Œï¼ŒLinear Attentionç›¸è¾ƒäºæ ‡å‡†Softmax Attentionå­˜åœ¨æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚æœ¬æ–‡åˆ†æäº†è¿™ä¸€é—®é¢˜èƒŒåçš„åŸå› ï¼Œå¹¶æå‡ºäº†Magnitude-Aware Linear Attentionï¼ˆMALAï¼‰ã€‚MALAè°ƒæ•´äº†Linear Attentionçš„è®¡ç®—æ–¹å¼ï¼Œå……åˆ†èå…¥äº†Queryçš„å¹…åº¦ä¿¡æ¯ï¼Œç”Ÿæˆäº†ä¸Softmax Attentionç›¸ä¼¼çš„æ³¨æ„åŠ›å¾—åˆ†åˆ†å¸ƒï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Softmax Attentionå…·æœ‰ä¼˜ç§€çš„å…¨å±€å»ºæ¨¡èƒ½åŠ›ï¼Œä½†å…¶äºŒæ¬¡å¤æ‚æ€§é™åˆ¶äº†å…¶åœ¨è§†è§‰ä»»åŠ¡çš„åº”ç”¨ã€‚</li>
<li>Linear Attentionå®ç°äº†é«˜æ•ˆçš„å…¨å±€ä¿¡æ¯å»ºæ¨¡ï¼Œä½†ä¸Softmax Attentionç›¸æ¯”å­˜åœ¨æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</li>
<li>Linear Attentionæ€§èƒ½ä¸‹é™çš„åŸå› åœ¨äºå…¶å¿½è§†äº†Queryçš„å¹…åº¦ä¿¡æ¯ï¼Œå¯¼è‡´æ³¨æ„åŠ›å¾—åˆ†åˆ†å¸ƒæ— æ³•åŠ¨æ€é€‚åº”Queryçš„å˜åŒ–ã€‚</li>
<li>MALAè°ƒæ•´äº†Linear Attentionçš„è®¡ç®—æ–¹å¼ï¼Œèå…¥äº†Queryçš„å¹…åº¦ä¿¡æ¯ï¼Œç”Ÿæˆäº†ä¸Softmax Attentionç›¸ä¼¼çš„æ³¨æ„åŠ›å¾—åˆ†åˆ†å¸ƒã€‚</li>
<li>MALAåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½æ•ˆæœï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å®ä¾‹åˆ†å‰²ã€è¯­ä¹‰åˆ†å‰²ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«å’Œå›¾åƒç”Ÿæˆã€‚</li>
<li>MALAçš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/qhfan/MALA%E4%B8%8A%E5%85%AC%E5%BC%8F%E3%80%82">https://github.com/qhfan/MALAä¸Šå…¬å¼€ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00698">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-712bd7ce05fc97e3f0db51790a2599a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9ae945a2ce34db84f97b69f636a7ad2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c67263aea0a851211e4b263070b438e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7d0fbfade3a2fae4267e44252804f06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb9855ff5403bcef39fb12ca47885498.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Audio-3DVG-Unified-Audio-Point-Cloud-Fusion-for-3D-Visual-Grounding"><a href="#Audio-3DVG-Unified-Audio-Point-Cloud-Fusion-for-3D-Visual-Grounding" class="headerlink" title="Audio-3DVG: Unified Audio - Point Cloud Fusion for 3D Visual Grounding"></a>Audio-3DVG: Unified Audio - Point Cloud Fusion for 3D Visual Grounding</h2><p><strong>Authors:Duc Cao-Dinh, Khai Le-Duc, Anh Dao, Bach Phan Tat, Chris Ngo, Duy M. H. Nguyen, Nguyen X. Khanh, Thanh Nguyen-Tang</strong></p>
<p>3D Visual Grounding (3DVG) involves localizing target objects in 3D point clouds based on natural language. While prior work has made strides using textual descriptions, leveraging spoken language-known as Audio-based 3D Visual Grounding-remains underexplored and challenging. Motivated by advances in automatic speech recognition (ASR) and speech representation learning, we propose Audio-3DVG, a simple yet effective framework that integrates audio and spatial information for enhanced grounding. Rather than treating speech as a monolithic input, we decompose the task into two complementary components. First, we introduce Object Mention Detection, a multi-label classification task that explicitly identifies which objects are referred to in the audio, enabling more structured audio-scene reasoning. Second, we propose an Audio-Guided Attention module that captures interactions between candidate objects and relational speech cues, improving target discrimination in cluttered scenes. To support benchmarking, we synthesize audio descriptions for standard 3DVG datasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate that Audio-3DVG not only achieves new state-of-the-art performance in audio-based grounding, but also competes with text-based methods-highlighting the promise of integrating spoken language into 3D vision tasks. </p>
<blockquote>
<p>3Dè§†è§‰å®šä½ï¼ˆ3DVGï¼‰æ¶‰åŠåœ¨è‡ªç„¶è¯­è¨€çš„æŒ‡å¯¼ä¸‹åœ¨3Dç‚¹äº‘ä¸­å®šä½ç›®æ ‡å¯¹è±¡ã€‚è™½ç„¶å…ˆå‰çš„å·¥ä½œåœ¨æ–‡æœ¬æè¿°æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†åˆ©ç”¨å£è¯­çš„åŸºäºéŸ³é¢‘çš„3Dè§†è§‰å®šä½ï¼ˆAudio-based 3D Visual Groundingï¼‰ä»ç„¶è¢«å¿½è§†ä¸”å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å—è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè¯­éŸ³è¡¨ç¤ºå­¦ä¹ çš„è¿›æ­¥çš„æ¨åŠ¨ï¼Œæˆ‘ä»¬æå‡ºäº†Audio-3DVGï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œå®ƒé›†æˆäº†éŸ³é¢‘å’Œç©ºé—´ä¿¡æ¯ä»¥å¢å¼ºå®šä½ã€‚æˆ‘ä»¬å¹¶ä¸å°†è¯­éŸ³è§†ä¸ºå•ä¸€è¾“å…¥ï¼Œè€Œæ˜¯å°†ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªäº’è¡¥çš„ç»„æˆéƒ¨åˆ†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯¹è±¡æåŠæ£€æµ‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ï¼Œæ˜ç¡®è¯†åˆ«éŸ³é¢‘ä¸­æåˆ°çš„å¯¹è±¡ï¼Œä»è€Œå®ç°æ›´ç»“æ„åŒ–çš„éŸ³é¢‘åœºæ™¯æ¨ç†ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†Audio-Guided Attentionæ¨¡å—ï¼Œè¯¥æ¨¡å—æ•è·å€™é€‰å¯¹è±¡å’Œå…³ç³»è¯­éŸ³çº¿ç´¢ä¹‹é—´çš„äº¤äº’ï¼Œæé«˜äº†æ‚ä¹±åœºæ™¯ä¸­ç›®æ ‡ç‰©ä½“çš„è¾¨åˆ«èƒ½åŠ›ã€‚ä¸ºäº†æ”¯æŒåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¯¹æ ‡å‡†çš„3DVGæ•°æ®é›†ï¼ˆåŒ…æ‹¬ScanReferã€Sr3Då’ŒNr3Dï¼‰è¿›è¡Œäº†éŸ³é¢‘æè¿°åˆæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAudio-3DVGä¸ä»…åœ¨åŸºäºéŸ³é¢‘çš„å®šä½æ–¹é¢è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œè€Œä¸”åœ¨åŸºäºæ–‡æœ¬çš„æ–¹æ³•ä¸­ä¹Ÿå…·æœ‰ç«äº‰åŠ›ï¼Œè¿™çªæ˜¾äº†å°†å£è¯­èå…¥3Dè§†è§‰ä»»åŠ¡çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00669v1">PDF</a> Work in progress, 42 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>Audio-3DVGæ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œæ•´åˆéŸ³é¢‘å’Œç©ºé—´ä¿¡æ¯ï¼Œç”¨äºå¢å¼º3Dç‚¹äº‘ä¸­ç›®æ ‡å¯¹è±¡çš„å®šä½ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³è¡¨ç¤ºå­¦ä¹ æŠ€æœ¯ï¼Œå°†è¯­éŸ³åˆ†è§£ä¸ºå¯¹è±¡æåŠæ£€æµ‹å’Œè¯­éŸ³å¼•å¯¼æ³¨æ„åŠ›ä¸¤ä¸ªäº’è¡¥ä»»åŠ¡ï¼Œæé«˜äº†ç›®æ ‡è¯†åˆ«å’Œåœºæ™¯æ¨ç†èƒ½åŠ›ã€‚åˆæˆéŸ³é¢‘æè¿°æ”¯æŒåŸºå‡†æµ‹è¯•ï¼Œå®éªŒç»“æœè¯æ˜Audio-3DVGåœ¨éŸ³é¢‘å®šä½æ–¹é¢è¾¾åˆ°æ–°çš„å…ˆè¿›æ°´å¹³ï¼Œå¹¶åœ¨ä¸æ–‡æœ¬å®šä½æ–¹æ³•çš„ç«äº‰ä¸­å±•ç°å‡ºæ½œåŠ›ã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>Audio-3DVGæ¡†æ¶æ•´åˆéŸ³é¢‘å’Œç©ºé—´ä¿¡æ¯ï¼Œå¢å¼º3Dç‚¹äº‘ä¸­ç›®æ ‡å¯¹è±¡çš„å®šä½ã€‚</li>
<li>å€ŸåŠ©è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³è¡¨ç¤ºå­¦ä¹ æŠ€æœ¯ï¼Œå¤„ç†è¯­éŸ³ä¿¡æ¯ã€‚</li>
<li>æ¡†æ¶åŒ…å«ä¸¤ä¸ªäº’è¡¥ä»»åŠ¡ï¼šå¯¹è±¡æåŠæ£€æµ‹å’Œè¯­éŸ³å¼•å¯¼æ³¨æ„åŠ›ã€‚</li>
<li>å¯¹è±¡æåŠæ£€æµ‹æ˜¯ä¸€ä¸ªå¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ï¼Œæ˜ç¡®è¯†åˆ«éŸ³é¢‘ä¸­æåˆ°çš„å¯¹è±¡ï¼Œä½¿åœºæ™¯æ¨ç†æ›´åŠ ç»“æ„åŒ–ã€‚</li>
<li>è¯­éŸ³å¼•å¯¼æ³¨æ„åŠ›æ¨¡å—æ•æ‰å€™é€‰å¯¹è±¡å’Œå…³ç³»è¯­éŸ³çº¿ç´¢ä¹‹é—´çš„äº¤äº’ï¼Œæé«˜æ‚ä¹±åœºæ™¯ä¸­ç›®æ ‡è¾¨åˆ«èƒ½åŠ›ã€‚</li>
<li>ä¸ºæ”¯æŒåŸºå‡†æµ‹è¯•ï¼ŒåˆæˆéŸ³é¢‘æè¿°ç”¨äºæ ‡å‡†3DVGæ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00669">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6813babde84e5282260929fd4003897d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6a886d25ee8adfcd0fe79efaff0ca38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14a2a72e577fc1e41f3aac3087743620.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f7b725cf2e3786d22874f9683926232.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-High-Fidelity-Speech-Super-Resolution-Network-using-a-Complex-Global-Attention-Module-with-Spectro-Temporal-Loss"><a href="#A-High-Fidelity-Speech-Super-Resolution-Network-using-a-Complex-Global-Attention-Module-with-Spectro-Temporal-Loss" class="headerlink" title="A High-Fidelity Speech Super Resolution Network using a Complex Global   Attention Module with Spectro-Temporal Loss"></a>A High-Fidelity Speech Super Resolution Network using a Complex Global   Attention Module with Spectro-Temporal Loss</h2><p><strong>Authors:Tarikul Islam Tamiti, Biraj Joshi, Rida Hasan, Rashedul Hasan, Taieba Athay, Nursad Mamun, Anomadarshi Barua</strong></p>
<p>Speech super-resolution (SSR) enhances low-resolution speech by increasing the sampling rate. While most SSR methods focus on magnitude reconstruction, recent research highlights the importance of phase reconstruction for improved perceptual quality. Therefore, we introduce CTFT-Net, a Complex Time-Frequency Transformation Network that reconstructs both magnitude and phase in complex domains for improved SSR tasks. It incorporates a complex global attention block to model inter-phoneme and inter-frequency dependencies and a complex conformer to capture long-range and local features, improving frequency reconstruction and noise robustness. CTFT-Net employs time-domain and multi-resolution frequency-domain loss functions for better generalization. Experiments show CTFT-Net outperforms state-of-the-art models (NU-Wave, WSRGlow, NVSR, AERO) on the VCTK dataset, particularly for extreme upsampling (2 kHz to 48 kHz), reconstructing high frequencies effectively without noisy artifacts. </p>
<blockquote>
<p>è¯­éŸ³è¶…åˆ†è¾¨ç‡ï¼ˆSSRï¼‰é€šè¿‡æé«˜é‡‡æ ·ç‡æ¥å¢å¼ºä½åˆ†è¾¨ç‡è¯­éŸ³ã€‚è™½ç„¶å¤§å¤šæ•°SSRæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å¹…åº¦é‡å»ºä¸Šï¼Œä½†æœ€è¿‘çš„ç ”ç©¶å¼ºè°ƒäº†ç›¸ä½é‡å»ºå¯¹æé«˜æ„ŸçŸ¥è´¨é‡çš„é‡è¦æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†CTFT-Netï¼Œè¿™æ˜¯ä¸€ç§å¤æ‚çš„æ—¶é—´-é¢‘ç‡è½¬æ¢ç½‘ç»œï¼Œå®ƒåœ¨å¤æ‚åŸŸä¸­é‡å»ºå¹…åº¦å’Œç›¸ä½ï¼Œä»¥æ”¹è¿›SSRä»»åŠ¡ã€‚å®ƒé‡‡ç”¨å¤æ‚çš„å…¨å±€æ³¨æ„åŠ›å—æ¥å»ºæ¨¡éŸ³ç´ é—´å’Œé¢‘ç‡é—´çš„ä¾èµ–æ€§ï¼Œå¹¶é‡‡ç”¨å¤æ‚çš„è½¬æ¢å™¨æ¥æ•è·é•¿ç¨‹å’Œå±€éƒ¨ç‰¹å¾ï¼Œä»è€Œæé«˜é¢‘ç‡é‡å»ºå’ŒæŠ—å™ªæ€§ã€‚CTFT-Neté‡‡ç”¨æ—¶åŸŸå’Œå¤šåˆ†è¾¨ç‡é¢‘åŸŸæŸå¤±å‡½æ•°ï¼Œä»¥å®ç°æ›´å¥½çš„æ³›åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒCTFT-Netåœ¨VCTKæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„æ¨¡å‹ï¼ˆNU-Waveã€WSRGlowã€NVSRã€AEROï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨æç«¯ä¸Šé‡‡æ ·ï¼ˆ2 kHzåˆ°48 kHzï¼‰æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°é‡å»ºé«˜é¢‘è€Œæ— å™ªå£°ä¼ªå½±ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00229v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è¯­éŸ³è¶…åˆ†è¾¨ç‡ï¼ˆSSRï¼‰é€šè¿‡æé«˜é‡‡æ ·ç‡æ¥æå‡ä½åˆ†è¾¨ç‡è¯­éŸ³ã€‚å½“å‰å¤§å¤šæ•°SSRæ–¹æ³•ä¸»è¦å…³æ³¨å¹…åº¦é‡å»ºï¼Œä½†æœ€æ–°ç ”ç©¶è¡¨æ˜ç›¸ä½é‡å»ºå¯¹äºæé«˜æ„ŸçŸ¥è´¨é‡ä¹Ÿå¾ˆé‡è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†CTFT-Netï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨å¤æ‚åŸŸè¿›è¡Œå¹…åº¦å’Œç›¸ä½é‡å»ºçš„å¤æ‚æ—¶é¢‘è½¬æ¢ç½‘ç»œï¼Œä»¥æé«˜SSRä»»åŠ¡çš„æ•ˆæœã€‚å®ƒç»“åˆäº†å¤æ‚å…¨å±€æ³¨æ„åŠ›å—æ¥å»ºæ¨¡éŸ³ç´ é—´å’Œé¢‘ç‡é—´çš„ä¾èµ–æ€§ï¼Œä»¥åŠå¤æ‚å˜å‹å™¨æ¥æ•æ‰é•¿ç¨‹å’Œå±€éƒ¨ç‰¹å¾ï¼Œæé«˜äº†é¢‘ç‡é‡å»ºå’Œå™ªå£°é²æ£’æ€§ã€‚CTFT-Neté‡‡ç”¨æ—¶åŸŸå’Œå¤šåˆ†è¾¨ç‡é¢‘åŸŸæŸå¤±å‡½æ•°ï¼Œä»¥æ›´å¥½åœ°æ¨å¹¿ã€‚å®éªŒè¡¨æ˜ï¼ŒCTFT-Netåœ¨VCTKæ•°æ®é›†ä¸Šä¼˜äºå…¶ä»–æœ€å…ˆè¿›æ¨¡å‹ï¼ˆNU-Waveã€WSRGlowã€NVSRã€AEROï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨æç«¯ä¸Šé‡‡æ ·ï¼ˆ2 kHzè‡³48 kHzï¼‰çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°é‡å»ºé«˜é¢‘è€Œæ²¡æœ‰å™ªå£°ä¼ªå½±ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯­éŸ³è¶…åˆ†è¾¨ç‡ï¼ˆSSRï¼‰æŠ€æœ¯èƒ½é€šè¿‡æé«˜é‡‡æ ·ç‡æ”¹å–„ä½åˆ†è¾¨ç‡è¯­éŸ³ã€‚</li>
<li>ç›®å‰çš„SSRæ–¹æ³•å¤šå…³æ³¨å¹…åº¦é‡å»ºï¼Œä½†ç›¸ä½é‡å»ºå¯¹äºæé«˜è¯­éŸ³æ„ŸçŸ¥è´¨é‡ä¹Ÿå¾ˆé‡è¦ã€‚</li>
<li>CTFT-Netæ˜¯ä¸€ä¸ªåœ¨å¤æ‚åŸŸè¿›è¡Œå¹…åº¦å’Œç›¸ä½é‡å»ºçš„ç½‘ç»œï¼Œæ—¨åœ¨æé«˜SSRæ•ˆæœã€‚</li>
<li>CTFT-Netç»“åˆäº†å¤æ‚å…¨å±€æ³¨æ„åŠ›å—å’Œå¤æ‚å˜å‹å™¨ï¼Œä»¥æé«˜é¢‘ç‡é‡å»ºå’Œå™ªå£°é²æ£’æ€§ã€‚</li>
<li>CTFT-Neté‡‡ç”¨æ—¶åŸŸå’Œå¤šåˆ†è¾¨ç‡é¢‘åŸŸæŸå¤±å‡½æ•°ä»¥å®ç°æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜CTFT-Netåœ¨VCTKæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–å…ˆè¿›æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00229">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9605ec5b2145687be5e33514bdd02336.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d865b239602a454755d8bfd711dbea1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a669c8a365a38bdbc9cb73de05d76de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d759f2d3258596752fdd83594566998.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Unifying-Global-and-Near-Context-Biasing-in-a-Single-Trie-Pass"><a href="#Unifying-Global-and-Near-Context-Biasing-in-a-Single-Trie-Pass" class="headerlink" title="Unifying Global and Near-Context Biasing in a Single Trie Pass"></a>Unifying Global and Near-Context Biasing in a Single Trie Pass</h2><p><strong>Authors:Iuliia Thorbecke, EsaÃº Villatoro-Tello, Juan Zuluaga-Gomez, Shashi Kumar, Sergio Burdisso, Pradeep Rangappa, AndrÃ©s Carofilis, Srikanth Madikeri, Petr Motlicek, Karthik Pandia, Kadri HacioÄŸlu, Andreas Stolcke</strong></p>
<p>Despite the success of end-to-end automatic speech recognition (ASR) models, challenges persist in recognizing rare, out-of-vocabulary words - including named entities (NE) - and in adapting to new domains using only text data. This work presents a practical approach to address these challenges through an unexplored combination of an NE bias list and a word-level n-gram language model (LM). This solution balances simplicity and effectiveness, improving entitiesâ€™ recognition while maintaining or even enhancing overall ASR performance. We efficiently integrate this enriched biasing method into a transducer-based ASR system, enabling context adaptation with almost no computational overhead. We present our results on three datasets spanning four languages and compare them to state-of-the-art biasing strategies. We demonstrate that the proposed combination of keyword biasing and n-gram LM improves entity recognition by up to 32% relative and reduces overall WER by up to a 12% relative. </p>
<blockquote>
<p>å°½ç®¡ç«¯åˆ°ç«¯çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨è¯†åˆ«ç¨€æœ‰ã€è¶…å‡ºè¯æ±‡è¡¨çš„å•è¯ï¼ˆåŒ…æ‹¬å‘½åå®ä½“ï¼ˆNEï¼‰ï¼‰ä»¥åŠä»…ä½¿ç”¨æ–‡æœ¬æ•°æ®é€‚åº”æ–°é¢†åŸŸæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é€šè¿‡NEåç½®åˆ—è¡¨å’Œè¯çº§nå…ƒè¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰çš„æœªæ¢ç´¢ç»„åˆæ¥è§£å†³è¿™äº›æŒ‘æˆ˜çš„å®é™…æ–¹æ³•ã€‚æ­¤è§£å†³æ–¹æ¡ˆå¹³è¡¡äº†ç®€å•æ€§å’Œæœ‰æ•ˆæ€§ï¼Œåœ¨ä¿æŒæˆ–ç”šè‡³æé«˜æ€»ä½“ASRæ€§èƒ½çš„åŒæ—¶ï¼Œæé«˜äº†å®ä½“çš„è¯†åˆ«èƒ½åŠ›ã€‚æˆ‘ä»¬æœ‰æ•ˆåœ°å°†è¿™ç§ä¸°å¯Œçš„åå‘æ–¹æ³•é›†æˆåˆ°åŸºäºè½¬æ¢å™¨çš„ASRç³»ç»Ÿä¸­ï¼Œå®ç°äº†ä¸Šä¸‹æ–‡é€‚åº”ï¼Œå‡ ä¹ä¸éœ€è¦è®¡ç®—å¼€é”€ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªè·¨è¶Šå››ç§è¯­è¨€çš„æ•°æ®é›†ä¸Šå±•ç¤ºæˆ‘ä»¬çš„ç»“æœï¼Œå¹¶å°†å®ƒä»¬ä¸æœ€æ–°çš„åå‘ç­–ç•¥è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬è¯æ˜äº†å…³é”®è¯åå‘å’Œnå…ƒLMçš„ç»„åˆå¯ä»¥æé«˜å®ä½“è¯†åˆ«ç‡ï¼Œç›¸å¯¹æé«˜å¹…åº¦é«˜è¾¾32%ï¼Œå¹¶å°†æ€»ä½“WERé™ä½é«˜è¾¾12%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.13514v2">PDF</a> Accepted to TSD2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ç« ä»‹ç»äº†ä¸€ç§é’ˆå¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹åœ¨è¯†åˆ«ç¨€æœ‰è¯å’Œé€‚åº”æ–°é¢†åŸŸæ–¹é¢çš„æŒ‘æˆ˜çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚é€šè¿‡ç»“åˆå‘½åå®ä½“ï¼ˆNEï¼‰ååˆ—è¡¨å’Œè¯çº§n-gramè¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ï¼Œå®ç°äº†ç®€å•æœ‰æ•ˆçš„å¹³è¡¡ï¼Œæé«˜äº†å®ä½“è¯†åˆ«èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†æ•´ä½“ASRæ€§èƒ½ç”šè‡³æœ‰æ‰€æå‡ã€‚è¯¥æ–¹æ¡ˆè¢«é«˜æ•ˆé›†æˆåˆ°åŸºäºè½¬æ¢å™¨çš„ASRç³»ç»Ÿä¸­ï¼Œå®ç°äº†ä¸Šä¸‹æ–‡é€‚åº”ï¼Œå‡ ä¹ä¸å¢åŠ è®¡ç®—å¼€é”€ã€‚åœ¨è·¨è¶Šå››ç§è¯­è¨€çš„ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå…³é”®è¯åå‘ä¸n-gram LMçš„ç»„åˆæé«˜äº†å®ä½“è¯†åˆ«ç‡è¾¾32%ï¼Œå¹¶é™ä½äº†æ•´ä½“å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰è¾¾12%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« ä¸­æå‡ºäº†ä¸€ä¸ªè§£å†³è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹è¯†åˆ«ç¨€æœ‰è¯å’Œé€‚åº”æ–°é¢†åŸŸçš„æ–°æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•ç»“åˆäº†å‘½åå®ä½“ï¼ˆNEï¼‰ååˆ—è¡¨å’Œè¯çº§n-gramè¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ï¼Œä»¥å®ç°ç®€å•æ€§å’Œæœ‰æ•ˆæ€§çš„å¹³è¡¡ã€‚</li>
<li>è¯¥æ–¹æ¡ˆå¯é«˜æ•ˆé›†æˆåˆ°åŸºäºè½¬æ¢å™¨çš„ASRç³»ç»Ÿä¸­ï¼Œå®ç°ä¸Šä¸‹æ–‡é€‚åº”ï¼Œä¸”å‡ ä¹ä¸å¢åŠ è®¡ç®—å¼€é”€ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æé«˜äº†å®ä½“è¯†åˆ«ç‡ï¼Œå¹¶é™ä½äº†æ•´ä½“å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚</li>
<li>ä¸ç°æœ‰æœ€å…ˆè¿›çš„åå‘ç­–ç•¥ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨è·¨è¶Šå››ç§è¯­è¨€çš„ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>æ–‡ç« å¼ºè°ƒè¯¥æ–¹æ¡ˆåœ¨æé«˜å®ä½“è¯†åˆ«èƒ½åŠ›çš„åŒæ—¶ï¼Œä¿æŒäº†æˆ–ç”šè‡³æé«˜äº†æ•´ä½“çš„ASRæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.13514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-29b29f49238adfd828ba366e14147abc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2fc29bc5edea1c1d803e194ec2bb601.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17f91e213606fa6742dfaae41151a071.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-05/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-05/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-05/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bfd583945d9bafba1e23acf6d9241e65.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-05  CanonSwap High-Fidelity and Consistent Video Face Swapping via   Canonical Space Modulation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-05/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7ba1342b022dfe9799067191d670b916.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-05  Weakly-supervised Contrastive Learning with Quantity Prompts for Moving   Infrared Small Target Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23154.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
