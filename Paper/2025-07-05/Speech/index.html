<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-07-05  A Cookbook for Community-driven Data Collection of Impaired Speech in   LowResource Languages">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-27433933ca222f56889973b3cbd8992e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    36 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-05-更新"><a href="#2025-07-05-更新" class="headerlink" title="2025-07-05 更新"></a>2025-07-05 更新</h1><h2 id="A-Cookbook-for-Community-driven-Data-Collection-of-Impaired-Speech-in-LowResource-Languages"><a href="#A-Cookbook-for-Community-driven-Data-Collection-of-Impaired-Speech-in-LowResource-Languages" class="headerlink" title="A Cookbook for Community-driven Data Collection of Impaired Speech in   LowResource Languages"></a>A Cookbook for Community-driven Data Collection of Impaired Speech in   LowResource Languages</h2><p><strong>Authors:Sumaya Ahmed Salihs, Isaac Wiafe, Jamal-Deen Abdulai, Elikem Doe Atsakpo, Gifty Ayoka, Richard Cave, Akon Obu Ekpezu, Catherine Holloway, Katrin Tomanek, Fiifi Baffoe Payin Winful</strong></p>
<p>This study presents an approach for collecting speech samples to build Automatic Speech Recognition (ASR) models for impaired speech, particularly, low-resource languages. It aims to democratize ASR technology and data collection by developing a “cookbook” of best practices and training for community-driven data collection and ASR model building. As a proof-of-concept, this study curated the first open-source dataset of impaired speech in Akan: a widely spoken indigenous language in Ghana. The study involved participants from diverse backgrounds with speech impairments. The resulting dataset, along with the cookbook and open-source tools, are publicly available to enable researchers and practitioners to create inclusive ASR technologies tailored to the unique needs of speech impaired individuals. In addition, this study presents the initial results of fine-tuning open-source ASR models to better recognize impaired speech in Akan. </p>
<blockquote>
<p>本研究提出了一种收集语音样本的方法，用于构建针对受损语音（尤其是资源匮乏的语言）的自动语音识别（ASR）模型。本研究旨在通过开发社区驱动的数据收集和ASR模型构建的“最佳实践手册”和培训材料，使ASR技术和数据收集民主化。作为概念验证，本研究精心制作了加纳广泛使用的本土语言阿坎语的首个开源受损语音数据集。该研究涉及来自不同背景且存在言语障碍的参与者。所得数据集以及与手册和开源工具一起，可供研究人员和实践人员使用，以创建适合特定受损语音个体需求的包容性ASR技术。此外，本研究还展示了微调开源ASR模型以更好地识别阿坎语中的受损语音的初步结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02428v1">PDF</a> This version has been reviewed and accepted for presentation at the   InterSpeech 2025 conference to be held in Rotterdam from 17 to 21 August. 5   pages and 3 tables</p>
<p><strong>Summary</strong></p>
<p>本研究提出了一种收集语音样本的方法，用于构建针对受损语音，尤其是低资源语言的自动语音识别（ASR）模型。研究旨在通过开发最佳实践“手册”和培训材料，实现社区驱动的数据收集和ASR模型构建，从而普及ASR技术和数据采集。作为概念证明，本研究整理了首个开源的受损语音数据集——阿坎语：一种在加纳广泛使用的土著语言。研究涉及来自不同背景且存在语音缺陷的参与者。所得数据集以及手册和开源工具均公开可用，以便研究者和从业者根据语音障碍者的独特需求创建包容性的ASR技术。此外，本研究还展示了微调开源ASR模型以更好地识别受损的阿坎语的初步结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本研究提出了一种针对受损语音，特别是低资源语言的ASR模型的数据收集方法。</li>
<li>研究目标是普及ASR技术和数据采集，通过开发最佳实践手册和培训材料来实现。</li>
<li>作为概念证明，本研究创建了首个开源的阿坎语受损语音数据集。</li>
<li>数据集包括了来自不同背景且存在语音障碍的参与者。</li>
<li>数据集、手册和开源工具均公开可用，便于研究和开发适应语音障碍者需求的ASR技术。</li>
<li>研究展示了微调开源ASR模型以优化识别受损阿坎语的能力的初步成果。</li>
<li>此研究为构建更具包容性的ASR技术做出了贡献，特别关注语音障碍者的需求。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02428">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2323360bf798a37040ca104aeb85a049.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af35a69688ad5d749875ff80ed2284c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7eadc60290288f49a81787d38438b48c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2784f7d2430dea1d94f6057ba9b48edc.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Adaptability-of-ASR-Models-on-Low-Resource-Language-A-Comparative-Study-of-Whisper-and-Wav2Vec-BERT-on-Bangla"><a href="#Adaptability-of-ASR-Models-on-Low-Resource-Language-A-Comparative-Study-of-Whisper-and-Wav2Vec-BERT-on-Bangla" class="headerlink" title="Adaptability of ASR Models on Low-Resource Language: A Comparative Study   of Whisper and Wav2Vec-BERT on Bangla"></a>Adaptability of ASR Models on Low-Resource Language: A Comparative Study   of Whisper and Wav2Vec-BERT on Bangla</h2><p><strong>Authors:Md Sazzadul Islam Ridoy, Sumi Akter, Md. Aminur Rahman</strong></p>
<p>In recent years, neural models trained on large multilingual text and speech datasets have shown great potential for supporting low-resource languages. This study investigates the performances of two state-of-the-art Automatic Speech Recognition (ASR) models, OpenAI’s Whisper (Small &amp; Large-V2) and Facebook’s Wav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments using two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to evaluate model performances. Through systematic fine-tuning and hyperparameter optimization, including learning rate, epochs, and model checkpoint selection, we have compared the models based on Word Error Rate (WER), Character Error Rate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model outperformed Whisper across all key evaluation metrics, demonstrated superior performance while requiring fewer computational resources, and offered valuable insights to develop robust speech recognition systems in low-resource linguistic settings. </p>
<blockquote>
<p>近年来，在大型多语言文本和语音数据集上训练的神经网络模型已显示出支持低资源语言的巨大潜力。本研究调查了两种最新自动语音识别（ASR）模型的性能，包括OpenAI的whisper（小型与大型-V2）和Facebook的Wav2Vec-BERT在低资源语言邦拉语上的表现。我们使用了两个公开可用的数据集Mozilla Common Voice-17和OpenSLR来评估模型性能。通过系统微调、超参数优化，包括学习率、周期和模型检查点选择，我们根据单词错误率（WER）、字符错误率（CER）、训练时间和计算效率比较了这些模型。Wav2Vec-BERT模型在各项关键评估指标上表现优于whisper，表现出优异的性能，同时需要较少的计算资源，为在低资源语言环境中开发稳健的语音识别系统提供了有价值的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01931v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>近年来，基于大型多语种文本和语音数据集的神经网络模型在低资源语言支持方面表现出巨大潜力。本研究采用最先进的自动语音识别（ASR）模型OpenAI的Whisper（小型和大型V2版本）和Facebook的Wav2Vec-BERT对低资源语言Bangla进行性能评估。通过实验评估，Wav2Vec-BERT模型在各项指标上优于Whisper，表现更优秀且计算资源需求更少，为在低资源语言环境中开发稳健的语音识别系统提供了有价值的见解。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>神经网络模型在低资源语言支持方面展现潜力。</li>
<li>评估了两种先进ASR模型：OpenAI的Whisper和Facebook的Wav2Vec-BERT在Bangla语言上的性能。</li>
<li>使用Mozilla Common Voice-17和OpenSLR两个公开数据集进行实验评估。</li>
<li>通过系统微调、超参数优化（包括学习率、周期和模型检查点选择）来比较模型性能。</li>
<li>Wav2Vec-BERT模型在关键评估指标上优于Whisper。</li>
<li>Wav2Vec-BERT模型表现更优秀且需要较少的计算资源。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01931">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6ee8b167dbcb81e11a529484c8efa843.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7b527a3b7e4536a666cac8de77e84d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f67259ea5580a1ff1cdfebcb3efc7119.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ca33b236e5244858a623fdc84001582.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc52abb08df05f9a47efc2827258ce16.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a281e290c39f368cea08077acd214cdc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a575fdd2c735e499a8decc920f3da9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffa784d09e1bd8e1d2c67ac293d4aa4d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PERTINENCE-Input-based-Opportunistic-Neural-Network-Dynamic-Execution"><a href="#PERTINENCE-Input-based-Opportunistic-Neural-Network-Dynamic-Execution" class="headerlink" title="PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution"></a>PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution</h2><p><strong>Authors:Omkar Shende, Gayathri Ananthanarayanan, Marcello Traiola</strong></p>
<p>Deep neural networks (DNNs) have become ubiquitous thanks to their remarkable ability to model complex patterns across various domains such as computer vision, speech recognition, robotics, etc. While large DNN models are often more accurate than simpler, lightweight models, they are also resource- and energy-hungry. Hence, it is imperative to design methods to reduce reliance on such large models without significant degradation in output accuracy. The high computational cost of these models is often necessary only for a reduced set of challenging inputs, while lighter models can handle most simple ones. Thus, carefully combining properties of existing DNN models in a dynamic, input-based way opens opportunities to improve efficiency without impacting accuracy.   In this work, we introduce PERTINENCE, a novel online method designed to analyze the complexity of input features and dynamically select the most suitable model from a pre-trained set to process a given input effectively. To achieve this, we employ a genetic algorithm to explore the training space of an ML-based input dispatcher, enabling convergence towards the Pareto front in the solution space that balances overall accuracy and computational efficiency.   We showcase our approach on state-of-the-art Convolutional Neural Networks (CNNs) trained on the CIFAR-10 and CIFAR-100, as well as Vision Transformers (ViTs) trained on TinyImageNet dataset. We report results showing PERTINENCE’s ability to provide alternative solutions to existing state-of-the-art models in terms of trade-offs between accuracy and number of operations. By opportunistically selecting among models trained for the same task, PERTINENCE achieves better or comparable accuracy with up to 36% fewer operations. </p>
<blockquote>
<p>深度神经网络（DNNs）由于其跨多个领域（如计算机视觉、语音识别、机器人等）建立复杂模式的卓越能力而变得无处不在。虽然大型DNN模型通常比简单、轻量级的模型更准确，但它们也需要更多的资源和能源。因此，设计方法来减少对这样的大模型的依赖，同时不会显著降低输出精度，这是至关重要的。这些模型的高计算成本通常只是为了减少一些具有挑战性的输入而必需的，而较轻的模型可以处理大多数简单的输入。因此，以一种动态、基于输入的方式谨慎地结合现有DNN模型的属性，为提高效率提供了机会，而不会影响准确性。在这项工作中，我们引入了PERTINENCE，这是一种新的在线方法，旨在分析输入特征复杂性，并从预训练模型集中动态选择最适合的模型来处理给定输入。为实现这一目标，我们采用遗传算法来探索基于机器学习的输入调度器的训练空间，使解决方案在整体准确性和计算效率之间取得平衡。我们在CIFAR-10和CIFAR-100上训练的最新卷积神经网络（CNN）以及TinyImageNet数据集上训练的视觉转换器（ViT）上展示了我们的方法。报告结果显示，PERTINENCE能够在准确性和操作次数之间提供对现有最先进模型的替代解决方案。通过在有相同任务的模型中做出选择，PERTINENCE在具有高达36%更少操作的情况下实现了更好的或相当的准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01695v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>深度神经网络（DNN）在多个领域如计算机视觉、语音识别、机器人等中展现出强大的模式识别能力。虽然大型DNN模型通常比轻量级模型更准确，但它们消耗大量资源和能源。因此，设计方法来减少对大型模型的依赖，同时不显著降低输出准确性至关重要。本文介绍了一种名为PERTINENCE的新型在线方法，该方法旨在分析输入特征复杂性，并从预训练模型集中动态选择最适合的模型来处理给定输入。为实现这一目标，我们采用遗传算法来探索ML输入调度器的训练空间，以在解决方案空间中实现平衡总体准确度和计算效率的帕累托前沿。我们在CIFAR-10和CIFAR-100上训练的最新卷积神经网络（CNN）以及TinyImageNet数据集上训练的视觉转换器（ViT）上展示了我们的方法。结果表明，PERTINENCE能够在准确度与运算次数之间提供对现有最先进模型的替代解决方案。通过在相同任务训练的模型之间灵活选择，PERTINENCE实现了更好的或相当的准确性，同时减少了高达36%的运算次数。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>深度神经网络（DNN）在多个领域具有强大的模式识别能力，但大型模型资源消耗大。</li>
<li>提出了一种新型在线方法PERTINENCE，可根据输入特征复杂性动态选择最合适的模型。</li>
<li>PERTINENCE采用遗传算法探索ML输入调度器的训练空间，实现帕累托最优，平衡准确度和计算效率。</li>
<li>在多个数据集上的实验表明，PERTINENCE提供了与现有最先进模型相当的准确性，同时显著减少了运算次数。</li>
<li>PERTINENCE方法具有潜力在提高计算效率的同时不损失准确性。</li>
<li>通过灵活选择针对同一任务训练的模型，PERTINENCE实现了效率与性能的平衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01695">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-da39ef3aaf347829c0668814c74454b3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f1c61c65e4ed146392678685727d899.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aada67893ff072ed115ca7ae4e91048e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6725ec9df6cc18c91bb1949a0e6bd33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e31bb16184536bfcdd48b4c784374b2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d8bd734b7c41f1b376db7107be3c1bd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edba3624b33be438bc8d430a00ed97c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07ec59fb84d7701dbfbaf7cdcd0e1b8e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Learning-from-Random-Subspace-Exploration-Generalized-Test-Time-Augmentation-with-Self-supervised-Distillation"><a href="#Learning-from-Random-Subspace-Exploration-Generalized-Test-Time-Augmentation-with-Self-supervised-Distillation" class="headerlink" title="Learning from Random Subspace Exploration: Generalized Test-Time   Augmentation with Self-supervised Distillation"></a>Learning from Random Subspace Exploration: Generalized Test-Time   Augmentation with Self-supervised Distillation</h2><p><strong>Authors:Andrei Jelea, Ahmed Nabil Belbachir, Marius Leordeanu</strong></p>
<p>We introduce Generalized Test-Time Augmentation (GTTA), a highly effective method for improving the performance of a trained model, which unlike other existing Test-Time Augmentation approaches from the literature is general enough to be used off-the-shelf for many vision and non-vision tasks, such as classification, regression, image segmentation and object detection. By applying a new general data transformation, that randomly perturbs multiple times the PCA subspace projection of a test input, GTTA forms robust ensembles at test time in which, due to sound statistical properties, the structural and systematic noises in the initial input data is filtered out and final estimator errors are reduced. Different from other existing methods, we also propose a final self-supervised learning stage in which the ensemble output, acting as an unsupervised teacher, is used to train the initial single student model, thus reducing significantly the test time computational cost, at no loss in accuracy. Our tests and comparisons to strong TTA approaches and SoTA models on various vision and non-vision well-known datasets and tasks, such as image classification and segmentation, speech recognition and house price prediction, validate the generality of the proposed GTTA. Furthermore, we also prove its effectiveness on the more specific real-world task of salmon segmentation and detection in low-visibility underwater videos, for which we introduce DeepSalmon, the largest dataset of its kind in the literature. </p>
<blockquote>
<p>我们介绍了一种名为广义测试时间增强（GTTA）的有效方法，用于提高训练模型性能。与文献中的其他现有测试时间增强方法不同，GTTA具有足够的通用性，可以应用于许多视觉和非视觉任务，例如分类、回归、图像分割和对象检测。通过应用新的通用数据转换，多次随机扰动测试输入的PCA子空间投影，GTTA在测试时形成稳健的集合。由于具有良好的统计属性，初始输入数据中的结构和系统噪声被过滤掉，最终估计器误差减小。与其他现有方法不同，我们还提出了一个最终的自我监督学习阶段，其中集合输出作为无监督教师，用于训练初始单一学生模型，从而在不影响精度的情况下显著降低测试时间的计算成本。我们的测试和对各种视觉和非视觉知名数据集和任务上的强大TTA方法和最新模型的比较，如图像分类和分割、语音识别和房屋价格预测等，验证了所提出GTTA的通用性。此外，我们还证明了其在更具体的现实世界任务——低可见度水下视频中的鲑鱼分割和检测——的有效性，为此我们引入了DeepSalmon，这是文献中同类最大的数据集。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01347v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了广义测试时间增强（GTTA）方法，用于提高训练模型性能。与其他现有测试时间增强方法不同，GTTA具有足够的通用性，可广泛应用于许多视觉和非视觉任务，如分类、回归、图像分割和对象检测。它通过新的通用数据转换，随机扰动测试输入的PCA子空间投影，在测试时形成稳健的集合，过滤掉初始输入数据中的结构和系统性噪声，减少最终估计器误差。此外，还提出了一种最终的自监督学习阶段，利用集合输出作为无监督教师来训练初始单一学生模型，从而显著减少测试时间计算成本，同时不损失准确性。在多种视觉和非视觉的知名数据集和任务上的测试，验证了GTTA的通用性和有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了Generalized Test-Time Augmentation（GTTA）方法，适用于多种任务，如分类、回归、图像分割和对象检测。</li>
<li>通过新的通用数据转换，随机扰动测试输入的PCA子空间投影，形成稳健集合，减少估计器误差。</li>
<li>引入自监督学习阶段，利用集合输出作为无监督教师来训练学生模型，降低测试时间成本。</li>
<li>GTTA方法具有广泛的适用性，可以在各种视觉和非视觉数据集上进行测试。</li>
<li>相比其他强TTA方法和最新模型，GTTA在多个任务上表现出优越的性能。</li>
<li>首次在文献中引入了DeepSalmon数据集，用于低可见度水下视频的鲑鱼分割和检测任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01347">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1d94aac69cf1c9d4bf58495b2408c0b5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MambAttention-Mamba-with-Multi-Head-Attention-for-Generalizable-Single-Channel-Speech-Enhancement"><a href="#MambAttention-Mamba-with-Multi-Head-Attention-for-Generalizable-Single-Channel-Speech-Enhancement" class="headerlink" title="MambAttention: Mamba with Multi-Head Attention for Generalizable   Single-Channel Speech Enhancement"></a>MambAttention: Mamba with Multi-Head Attention for Generalizable   Single-Channel Speech Enhancement</h2><p><strong>Authors:Nikolai Lund Kühne, Jesper Jensen, Jan Østergaard, Zheng-Hua Tan</strong></p>
<p>With the advent of new sequence models like Mamba and xLSTM, several studies have shown that these models match or outperform state-of-the-art models in single-channel speech enhancement, automatic speech recognition, and self-supervised audio representation learning. However, prior research has demonstrated that sequence models like LSTM and Mamba tend to overfit to the training set. To address this issue, previous works have shown that adding self-attention to LSTMs substantially improves generalization performance for single-channel speech enhancement. Nevertheless, neither the concept of hybrid Mamba and time-frequency attention models nor their generalization performance have been explored for speech enhancement. In this paper, we propose a novel hybrid architecture, MambAttention, which combines Mamba and shared time- and frequency-multi-head attention modules for generalizable single-channel speech enhancement. To train our model, we introduce VoiceBank+Demand Extended (VB-DemandEx), a dataset inspired by VoiceBank+Demand but with more challenging noise types and lower signal-to-noise ratios. Trained on VB-DemandEx, our proposed MambAttention model significantly outperforms existing state-of-the-art LSTM-, xLSTM-, Mamba-, and Conformer-based systems of similar complexity across all reported metrics on two out-of-domain datasets: DNS 2020 and EARS-WHAM_v2, while matching their performance on the in-domain dataset VB-DemandEx. Ablation studies highlight the role of weight sharing between the time- and frequency-multi-head attention modules for generalization performance. Finally, we explore integrating the shared time- and frequency-multi-head attention modules with LSTM and xLSTM, which yields a notable performance improvement on the out-of-domain datasets. However, our MambAttention model remains superior on both out-of-domain datasets across all reported evaluation metrics. </p>
<blockquote>
<p>随着Mamba和xLSTM等新序列模型的出现，多项研究表明这些模型在单通道语音增强、自动语音识别和自我监督音频表示学习方面达到了或超越了现有模型的性能。然而，先前的研究表明，像LSTM和Mamba这样的序列模型容易过度拟合训练集。为了解决这一问题，以前的研究表明，在LSTM中添加自注意力可以大大提高单通道语音增强的泛化性能。然而，关于混合Mamba和时间-频率注意力模型的概念及其泛化性能在语音增强方面的探索尚未出现。在本文中，我们提出了一种新型的混合架构MambAttention，它将Mamba和共享的时间与频率多头注意力模块相结合，用于通用的单通道语音增强。为了训练我们的模型，我们引入了VoiceBank+Demand Extended（VB-DemandEx）数据集，该数据集以VoiceBank+Demand为灵感，但具有更具挑战性的噪声类型和更低的信噪比。在VB-DemandEx上训练的MambAttention模型在两个域外数据集DNS 2020和EARS-WHAM_v2上所有报告的指标中都显著超越了现有复杂的基于LSTM、xLSTM、Mamba和Conformer的系统，同时在域内数据集VB-DemandEx上的性能与之相匹配。消融研究突出了时间与频率多头注意力模块权重共享在泛化性能中的作用。最后，我们探索了将共享的时间与频率多头注意力模块与LSTM和xLSTM集成，这在域外数据集上产生了显著的性能改进。然而，我们的MambAttention模型在两个域外数据集上的所有报告的评价指标上仍然表现最佳。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00966v1">PDF</a> Submitted to IEEE&#x2F;ACM Transactions on Audio, Speech, and Language   Processing for possible publication</p>
<p><strong>Summary</strong></p>
<p>本研究提出了一个名为MambAttention的混合架构，结合了Mamba和共享时间、频率多头注意力模块，用于通用的单通道语音增强。为训练模型，研究引入了更具挑战性的噪声类型和较低信噪比的VoiceBank+Demand扩展数据集。在跨域数据集DNS 2020和EARS-WHAM_v2上，MambAttention模型显著优于其他先进的LSTM、xLSTM、Mamba和Conformer模型，同时在领域内数据集VB-DemandEx上的性能相匹配。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>新序列模型（如Mamba和xLSTM）在语音增强、语音识别和音频表示学习方面表现出与现有先进技术相当或更优的性能。</li>
<li>LSTM和Mamba模型容易过度拟合训练集，添加自注意力机制可以改善其泛化性能。</li>
<li>引入了一种新型的混合架构MambAttention，结合了Mamba和共享时间、频率多头注意力模块，用于单通道语音增强。</li>
<li>引入了更具挑战性的VoiceBank+Demand扩展数据集（VB-DemandEx）用于模型训练。</li>
<li>MambAttention模型在跨域数据集上显著优于其他先进模型。</li>
<li>权重共享在时间和频率多头注意力模块间对于模型的泛化性能起到关键作用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00966">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ae99ab8740a31542de7caf8a8b58dd7e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27433933ca222f56889973b3cbd8992e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Rectifying-Magnitude-Neglect-in-Linear-Attention"><a href="#Rectifying-Magnitude-Neglect-in-Linear-Attention" class="headerlink" title="Rectifying Magnitude Neglect in Linear Attention"></a>Rectifying Magnitude Neglect in Linear Attention</h2><p><strong>Authors:Qihang Fan, Huaibo Huang, Yuang Ai, ran He</strong></p>
<p>As the core operator of Transformers, Softmax Attention exhibits excellent global modeling capabilities. However, its quadratic complexity limits its applicability to vision tasks. In contrast, Linear Attention shares a similar formulation with Softmax Attention while achieving linear complexity, enabling efficient global information modeling. Nevertheless, Linear Attention suffers from a significant performance degradation compared to standard Softmax Attention. In this paper, we analyze the underlying causes of this issue based on the formulation of Linear Attention. We find that, unlike Softmax Attention, Linear Attention entirely disregards the magnitude information of the Query. This prevents the attention score distribution from dynamically adapting as the Query scales. As a result, despite its structural similarity to Softmax Attention, Linear Attention exhibits a significantly different attention score distribution. Based on this observation, we propose Magnitude-Aware Linear Attention (MALA), which modifies the computation of Linear Attention to fully incorporate the Query’s magnitude. This adjustment allows MALA to generate an attention score distribution that closely resembles Softmax Attention while exhibiting a more well-balanced structure. We evaluate the effectiveness of MALA on multiple tasks, including image classification, object detection, instance segmentation, semantic segmentation, natural language processing, speech recognition, and image generation. Our MALA achieves strong results on all of these tasks. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/qhfan/MALA">https://github.com/qhfan/MALA</a> </p>
<blockquote>
<p>作为Transformer的核心操作器，Softmax Attention展现出出色的全局建模能力。然而，其二次复杂度限制了其在视觉任务中的应用。相比之下，Linear Attention与Softmax Attention具有相似的公式，但实现了线性复杂度，能够实现有效的全局信息建模。然而，与标准的Softmax Attention相比，Linear Attention的性能显著下降。在本文中，我们基于Linear Attention的公式分析此问题的根本原因。我们发现，与Softmax Attention不同，Linear Attention完全忽略了Query的幅度信息。这阻止了注意力得分分布在Query规模扩大时动态适应。因此，尽管其与Softmax Attention结构相似，但Linear Attention的注意力得分分布却大不相同。基于此观察，我们提出了幅度感知Linear Attention（MALA），它修改了Linear Attention的计算以充分融入Query的幅度。这一调整使MALA能够生成与Softmax Attention相似的注意力得分分布，同时展现出更平衡的结构。我们在多个任务上评估了MALA的有效性，包括图像分类、目标检测、实例分割、语义分割、自然语言处理、语音识别和图像生成。我们的MALA在所有任务上都取得了强大的结果。代码将在<a target="_blank" rel="noopener" href="https://github.com/qhfan/MALA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/qhfan/MALA找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00698v1">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了Transformer中的核心操作，包括Softmax Attention和Linear Attention。Softmax Attention具有出色的全局建模能力，但其二次复杂性限制了其在视觉任务中的应用。Linear Attention与Softmax Attention具有相似的公式形式但具有线性复杂性，能够实现高效的全局信息建模。然而，Linear Attention相较于标准Softmax Attention存在性能下降的问题。本文分析了这一问题背后的原因，并提出了Magnitude-Aware Linear Attention（MALA）。MALA调整了Linear Attention的计算方式，充分融入了Query的幅度信息，生成了与Softmax Attention相似的注意力得分分布，并在多个任务上取得了良好效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Softmax Attention具有优秀的全局建模能力，但其二次复杂性限制了其在视觉任务的应用。</li>
<li>Linear Attention实现了高效的全局信息建模，但与Softmax Attention相比存在性能下降问题。</li>
<li>Linear Attention性能下降的原因在于其忽视了Query的幅度信息，导致注意力得分分布无法动态适应Query的变化。</li>
<li>MALA调整了Linear Attention的计算方式，融入了Query的幅度信息，生成了与Softmax Attention相似的注意力得分分布。</li>
<li>MALA在多个任务上取得了良好效果，包括图像分类、目标检测、实例分割、语义分割、自然语言处理、语音识别和图像生成。</li>
<li>MALA的代码将在<a target="_blank" rel="noopener" href="https://github.com/qhfan/MALA%E4%B8%8A%E5%85%AC%E5%BC%8F%E3%80%82">https://github.com/qhfan/MALA上公开。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00698">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-712bd7ce05fc97e3f0db51790a2599a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9ae945a2ce34db84f97b69f636a7ad2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c67263aea0a851211e4b263070b438e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7d0fbfade3a2fae4267e44252804f06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb9855ff5403bcef39fb12ca47885498.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Audio-3DVG-Unified-Audio-Point-Cloud-Fusion-for-3D-Visual-Grounding"><a href="#Audio-3DVG-Unified-Audio-Point-Cloud-Fusion-for-3D-Visual-Grounding" class="headerlink" title="Audio-3DVG: Unified Audio - Point Cloud Fusion for 3D Visual Grounding"></a>Audio-3DVG: Unified Audio - Point Cloud Fusion for 3D Visual Grounding</h2><p><strong>Authors:Duc Cao-Dinh, Khai Le-Duc, Anh Dao, Bach Phan Tat, Chris Ngo, Duy M. H. Nguyen, Nguyen X. Khanh, Thanh Nguyen-Tang</strong></p>
<p>3D Visual Grounding (3DVG) involves localizing target objects in 3D point clouds based on natural language. While prior work has made strides using textual descriptions, leveraging spoken language-known as Audio-based 3D Visual Grounding-remains underexplored and challenging. Motivated by advances in automatic speech recognition (ASR) and speech representation learning, we propose Audio-3DVG, a simple yet effective framework that integrates audio and spatial information for enhanced grounding. Rather than treating speech as a monolithic input, we decompose the task into two complementary components. First, we introduce Object Mention Detection, a multi-label classification task that explicitly identifies which objects are referred to in the audio, enabling more structured audio-scene reasoning. Second, we propose an Audio-Guided Attention module that captures interactions between candidate objects and relational speech cues, improving target discrimination in cluttered scenes. To support benchmarking, we synthesize audio descriptions for standard 3DVG datasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate that Audio-3DVG not only achieves new state-of-the-art performance in audio-based grounding, but also competes with text-based methods-highlighting the promise of integrating spoken language into 3D vision tasks. </p>
<blockquote>
<p>3D视觉定位（3DVG）涉及在自然语言的指导下在3D点云中定位目标对象。虽然先前的工作在文本描述方面取得了进展，但利用口语的基于音频的3D视觉定位（Audio-based 3D Visual Grounding）仍然被忽视且具有挑战性。受自动语音识别（ASR）和语音表示学习的进步的推动，我们提出了Audio-3DVG，这是一个简单有效的框架，它集成了音频和空间信息以增强定位。我们并不将语音视为单一输入，而是将任务分解为两个互补的组成部分。首先，我们引入了对象提及检测，这是一个多标签分类任务，明确识别音频中提到的对象，从而实现更结构化的音频场景推理。其次，我们提出了Audio-Guided Attention模块，该模块捕获候选对象和关系语音线索之间的交互，提高了杂乱场景中目标物体的辨别能力。为了支持基准测试，我们对标准的3DVG数据集（包括ScanRefer、Sr3D和Nr3D）进行了音频描述合成。实验结果表明，Audio-3DVG不仅在基于音频的定位方面达到了新的最先进的性能水平，而且在基于文本的方法中也具有竞争力，这突显了将口语融入3D视觉任务的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00669v1">PDF</a> Work in progress, 42 pages</p>
<p><strong>摘要</strong></p>
<p>Audio-3DVG是一个简单有效的框架，整合音频和空间信息，用于增强3D点云中目标对象的定位。该框架利用自动语音识别和语音表示学习技术，将语音分解为对象提及检测和语音引导注意力两个互补任务，提高了目标识别和场景推理能力。合成音频描述支持基准测试，实验结果证明Audio-3DVG在音频定位方面达到新的先进水平，并在与文本定位方法的竞争中展现出潜力。</p>
<p><strong>要点掌握</strong></p>
<ol>
<li>Audio-3DVG框架整合音频和空间信息，增强3D点云中目标对象的定位。</li>
<li>借助自动语音识别和语音表示学习技术，处理语音信息。</li>
<li>框架包含两个互补任务：对象提及检测和语音引导注意力。</li>
<li>对象提及检测是一个多标签分类任务，明确识别音频中提到的对象，使场景推理更加结构化。</li>
<li>语音引导注意力模块捕捉候选对象和关系语音线索之间的交互，提高杂乱场景中目标辨别能力。</li>
<li>为支持基准测试，合成音频描述用于标准3DVG数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00669">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6813babde84e5282260929fd4003897d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6a886d25ee8adfcd0fe79efaff0ca38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14a2a72e577fc1e41f3aac3087743620.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f7b725cf2e3786d22874f9683926232.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-High-Fidelity-Speech-Super-Resolution-Network-using-a-Complex-Global-Attention-Module-with-Spectro-Temporal-Loss"><a href="#A-High-Fidelity-Speech-Super-Resolution-Network-using-a-Complex-Global-Attention-Module-with-Spectro-Temporal-Loss" class="headerlink" title="A High-Fidelity Speech Super Resolution Network using a Complex Global   Attention Module with Spectro-Temporal Loss"></a>A High-Fidelity Speech Super Resolution Network using a Complex Global   Attention Module with Spectro-Temporal Loss</h2><p><strong>Authors:Tarikul Islam Tamiti, Biraj Joshi, Rida Hasan, Rashedul Hasan, Taieba Athay, Nursad Mamun, Anomadarshi Barua</strong></p>
<p>Speech super-resolution (SSR) enhances low-resolution speech by increasing the sampling rate. While most SSR methods focus on magnitude reconstruction, recent research highlights the importance of phase reconstruction for improved perceptual quality. Therefore, we introduce CTFT-Net, a Complex Time-Frequency Transformation Network that reconstructs both magnitude and phase in complex domains for improved SSR tasks. It incorporates a complex global attention block to model inter-phoneme and inter-frequency dependencies and a complex conformer to capture long-range and local features, improving frequency reconstruction and noise robustness. CTFT-Net employs time-domain and multi-resolution frequency-domain loss functions for better generalization. Experiments show CTFT-Net outperforms state-of-the-art models (NU-Wave, WSRGlow, NVSR, AERO) on the VCTK dataset, particularly for extreme upsampling (2 kHz to 48 kHz), reconstructing high frequencies effectively without noisy artifacts. </p>
<blockquote>
<p>语音超分辨率（SSR）通过提高采样率来增强低分辨率语音。虽然大多数SSR方法主要集中在幅度重建上，但最近的研究强调了相位重建对提高感知质量的重要性。因此，我们引入了CTFT-Net，这是一种复杂的时间-频率转换网络，它在复杂域中重建幅度和相位，以改进SSR任务。它采用复杂的全局注意力块来建模音素间和频率间的依赖性，并采用复杂的转换器来捕获长程和局部特征，从而提高频率重建和抗噪性。CTFT-Net采用时域和多分辨率频域损失函数，以实现更好的泛化。实验表明，CTFT-Net在VCTK数据集上的表现优于最先进的模型（NU-Wave、WSRGlow、NVSR、AERO），特别是在极端上采样（2 kHz到48 kHz）情况下，能够有效地重建高频而无噪声伪影。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00229v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>语音超分辨率（SSR）通过提高采样率来提升低分辨率语音。当前大多数SSR方法主要关注幅度重建，但最新研究表明相位重建对于提高感知质量也很重要。因此，我们引入了CTFT-Net，这是一个在复杂域进行幅度和相位重建的复杂时频转换网络，以提高SSR任务的效果。它结合了复杂全局注意力块来建模音素间和频率间的依赖性，以及复杂变压器来捕捉长程和局部特征，提高了频率重建和噪声鲁棒性。CTFT-Net采用时域和多分辨率频域损失函数，以更好地推广。实验表明，CTFT-Net在VCTK数据集上优于其他最先进模型（NU-Wave、WSRGlow、NVSR、AERO），特别是在极端上采样（2 kHz至48 kHz）的情况下，能够有效地重建高频而没有噪声伪影。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>语音超分辨率（SSR）技术能通过提高采样率改善低分辨率语音。</li>
<li>目前的SSR方法多关注幅度重建，但相位重建对于提高语音感知质量也很重要。</li>
<li>CTFT-Net是一个在复杂域进行幅度和相位重建的网络，旨在提高SSR效果。</li>
<li>CTFT-Net结合了复杂全局注意力块和复杂变压器，以提高频率重建和噪声鲁棒性。</li>
<li>CTFT-Net采用时域和多分辨率频域损失函数以实现更好的泛化能力。</li>
<li>实验表明CTFT-Net在VCTK数据集上的性能优于其他先进模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00229">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9605ec5b2145687be5e33514bdd02336.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d865b239602a454755d8bfd711dbea1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a669c8a365a38bdbc9cb73de05d76de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d759f2d3258596752fdd83594566998.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Unifying-Global-and-Near-Context-Biasing-in-a-Single-Trie-Pass"><a href="#Unifying-Global-and-Near-Context-Biasing-in-a-Single-Trie-Pass" class="headerlink" title="Unifying Global and Near-Context Biasing in a Single Trie Pass"></a>Unifying Global and Near-Context Biasing in a Single Trie Pass</h2><p><strong>Authors:Iuliia Thorbecke, Esaú Villatoro-Tello, Juan Zuluaga-Gomez, Shashi Kumar, Sergio Burdisso, Pradeep Rangappa, Andrés Carofilis, Srikanth Madikeri, Petr Motlicek, Karthik Pandia, Kadri Hacioğlu, Andreas Stolcke</strong></p>
<p>Despite the success of end-to-end automatic speech recognition (ASR) models, challenges persist in recognizing rare, out-of-vocabulary words - including named entities (NE) - and in adapting to new domains using only text data. This work presents a practical approach to address these challenges through an unexplored combination of an NE bias list and a word-level n-gram language model (LM). This solution balances simplicity and effectiveness, improving entities’ recognition while maintaining or even enhancing overall ASR performance. We efficiently integrate this enriched biasing method into a transducer-based ASR system, enabling context adaptation with almost no computational overhead. We present our results on three datasets spanning four languages and compare them to state-of-the-art biasing strategies. We demonstrate that the proposed combination of keyword biasing and n-gram LM improves entity recognition by up to 32% relative and reduces overall WER by up to a 12% relative. </p>
<blockquote>
<p>尽管端到端的自动语音识别（ASR）模型取得了成功，但在识别稀有、超出词汇表的单词（包括命名实体（NE））以及仅使用文本数据适应新领域方面仍存在挑战。本研究提出了一种通过NE偏置列表和词级n元语言模型（LM）的未探索组合来解决这些挑战的实际方法。此解决方案平衡了简单性和有效性，在保持或甚至提高总体ASR性能的同时，提高了实体的识别能力。我们有效地将这种丰富的偏向方法集成到基于转换器的ASR系统中，实现了上下文适应，几乎不需要计算开销。我们在三个跨越四种语言的数据集上展示我们的结果，并将它们与最新的偏向策略进行比较。我们证明了关键词偏向和n元LM的组合可以提高实体识别率，相对提高幅度高达32%，并将总体WER降低高达12%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.13514v2">PDF</a> Accepted to TSD2025</p>
<p><strong>Summary</strong></p>
<p>该文章介绍了一种针对自动语音识别（ASR）模型在识别稀有词和适应新领域方面的挑战的有效解决方案。通过结合命名实体（NE）偏列表和词级n-gram语言模型（LM），实现了简单有效的平衡，提高了实体识别能力，同时保持了整体ASR性能甚至有所提升。该方案被高效集成到基于转换器的ASR系统中，实现了上下文适应，几乎不增加计算开销。在跨越四种语言的三个数据集上的实验结果表明，关键词偏向与n-gram LM的组合提高了实体识别率达32%，并降低了整体字词错误率（WER）达12%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章中提出了一个解决自动语音识别（ASR）模型识别稀有词和适应新领域的新方法。</li>
<li>方法结合了命名实体（NE）偏列表和词级n-gram语言模型（LM），以实现简单性和有效性的平衡。</li>
<li>该方案可高效集成到基于转换器的ASR系统中，实现上下文适应，且几乎不增加计算开销。</li>
<li>实验结果表明，该方法提高了实体识别率，并降低了整体字词错误率（WER）。</li>
<li>与现有最先进的偏向策略相比，该方法在跨越四种语言的三个数据集上表现出更好的性能。</li>
<li>文章强调该方案在提高实体识别能力的同时，保持了或甚至提高了整体的ASR性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.13514">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-29b29f49238adfd828ba366e14147abc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2fc29bc5edea1c1d803e194ec2bb601.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17f91e213606fa6742dfaae41151a071.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-05/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-05/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-05/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bfd583945d9bafba1e23acf6d9241e65.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2025-07-05  CanonSwap High-Fidelity and Consistent Video Face Swapping via   Canonical Space Modulation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-05/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7ba1342b022dfe9799067191d670b916.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-07-05  Weakly-supervised Contrastive Learning with Quantity Prompts for Moving   Infrared Small Target Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23154.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
