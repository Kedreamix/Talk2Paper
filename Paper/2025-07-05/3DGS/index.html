<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2025-07-05  HyperGaussians High-Dimensional Gaussian Splatting for High-Fidelity   Animatable Face Avatars">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-42bc17fa39ad06cd6cb26dab7c9fad37.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    15.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    63 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-05-更新"><a href="#2025-07-05-更新" class="headerlink" title="2025-07-05 更新"></a>2025-07-05 更新</h1><h2 id="HyperGaussians-High-Dimensional-Gaussian-Splatting-for-High-Fidelity-Animatable-Face-Avatars"><a href="#HyperGaussians-High-Dimensional-Gaussian-Splatting-for-High-Fidelity-Animatable-Face-Avatars" class="headerlink" title="HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity   Animatable Face Avatars"></a>HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity   Animatable Face Avatars</h2><p><strong>Authors:Gent Serifi, Marcel C. Bühler</strong></p>
<p>We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for high-quality animatable face avatars. Creating such detailed face avatars from videos is a challenging problem and has numerous applications in augmented and virtual reality. While tremendous successes have been achieved for static faces, animatable avatars from monocular videos still fall in the uncanny valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face through a collection of 3D Gaussian primitives. 3DGS excels at rendering static faces, but the state-of-the-art still struggles with nonlinear deformations, complex lighting effects, and fine details. While most related works focus on predicting better Gaussian parameters from expression codes, we rethink the 3D Gaussian representation itself and how to make it more expressive. Our insights lead to a novel extension of 3D Gaussians to high-dimensional multivariate Gaussians, dubbed ‘HyperGaussians’. The higher dimensionality increases expressivity through conditioning on a learnable local embedding. However, splatting HyperGaussians is computationally expensive because it requires inverting a high-dimensional covariance matrix. We solve this by reparameterizing the covariance matrix, dubbed the ‘inverse covariance trick’. This trick boosts the efficiency so that HyperGaussians can be seamlessly integrated into existing models. To demonstrate this, we plug in HyperGaussians into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our evaluation on 19 subjects from 4 face datasets shows that HyperGaussians outperform 3DGS numerically and visually, particularly for high-frequency details like eyeglass frames, teeth, complex facial movements, and specular reflections. </p>
<blockquote>
<p>我们介绍了HyperGaussians，这是3D高斯贴图的一种新型扩展，用于创建高质量的可动画面部化身。从视频中创建如此详细的面部化身是一个具有挑战性的问题，并且在增强和虚拟现实中有许多应用。虽然静态面孔已经取得了巨大的成功，但从单目视频中创建的可动画化身仍然处于不真实与真实之间的境地。现行标准3D高斯贴图（3DGS）通过一组3D高斯原始数据表示面部。3DGS在呈现静态面孔方面非常出色，但最新技术仍然难以处理非线性变形、复杂的灯光效果和精细细节。大多数相关作品都专注于从表情代码中预测更好的高斯参数，而我们重新思考了3D高斯表示本身以及如何使其更具表现力。我们的见解导致将3D高斯扩展到了高维多元高斯，称为“HyperGaussians”。更高的维度通过以可学习局部嵌入为条件来增加表现力。然而，绘制HyperGaussians的计算成本很高，因为它需要反转高维协方差矩阵。我们通过重新参数化协方差矩阵解决了这个问题，称为“逆协方差技巧”。这种技巧提高了效率，使得HyperGaussians可以无缝地集成到现有模型中。为了证明这一点，我们将HyperGaussians插入到现有快速单目面部化身技术的最新成果中：FlashAvatar。我们对来自四个面部数据集的19个主题进行的评估表明，HyperGaussians在数值和视觉上均优于3DGS，尤其是在眼镜框、牙齿、复杂面部运动和镜面反射等高频细节方面。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02803v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://gserifi.github.io/HyperGaussians">https://gserifi.github.io/HyperGaussians</a></p>
<p><strong>摘要</strong></p>
<p>本文介绍了HyperGaussians，这是一种基于三维高斯混色的新型扩展技术，用于创建高质量的可动画面部化身。该技术对于从视频创建详细的面部化身具有挑战性，但在增强和虚拟现实等领域具有广泛应用。尽管静态面部渲染已经取得了巨大成功，但从单目视频中创建的可动画化身仍然在“尴尬之谷”阶段。本文提出的HyperGaussians通过引入高维多元高斯分布，扩展了三维高斯表示法，提高了其表现力。同时，为解决混色HyperGaussians所需的高维协方差矩阵计算成本较高的问题，采用协方差矩阵重新参数化的方法（“逆协方差技巧”），提高了计算效率，使HyperGaussians能够无缝集成到现有模型中。将HyperGaussians应用于当前先进的快速单目面部化身技术FlashAvatar中，对四个面部数据集的19名主体的评估显示，HyperGaussians在数值和视觉上均优于三维高斯混色技术，特别是在眼镜框、牙齿、复杂面部动作和镜面反射等高频细节方面表现更出色。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>介绍了一种新型技术HyperGaussians，该技术扩展了三维高斯混色技术以创建更详细的可动画面部化身。</li>
<li>HyperGaussians通过引入高维多元高斯分布提高了表现力。</li>
<li>提出了一种解决HyperGaussians混色计算成本高的方法——“逆协方差技巧”，提高了计算效率。</li>
<li>将HyperGaussians集成到现有模型（如FlashAvatar）中，以改进面部化身的质量。</li>
<li>HyperGaussians在复杂面部动作和高频细节（如眼镜框和牙齿）的渲染上表现优异。</li>
<li>对多个数据集进行的评估表明，HyperGaussians在数值和视觉上均优于现有技术（如三维高斯混色）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02803">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9e733f4b7900d962afc9d76566455a9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d08312a1c6545c4754165ab6ae10e45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49e2aeb6534ccdd9e43a4397170f7846.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6729f068a39236f40a3778df8c91343.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-239841382d5b91846d0380efed18d723.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d9fbe9edf3b53ba2b1d486dd22d7fa0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ArtGS-3D-Gaussian-Splatting-for-Interactive-Visual-Physical-Modeling-and-Manipulation-of-Articulated-Objects"><a href="#ArtGS-3D-Gaussian-Splatting-for-Interactive-Visual-Physical-Modeling-and-Manipulation-of-Articulated-Objects" class="headerlink" title="ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and   Manipulation of Articulated Objects"></a>ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and   Manipulation of Articulated Objects</h2><p><strong>Authors:Qiaojun Yu, Xibin Yuan, Yu jiang, Junting Chen, Dongzhe Zheng, Ce Hao, Yang You, Yixing Chen, Yao Mu, Liu Liu, Cewu Lu</strong></p>
<p>Articulated object manipulation remains a critical challenge in robotics due to the complex kinematic constraints and the limited physical reasoning of existing methods. In this work, we introduce ArtGS, a novel framework that extends 3D Gaussian Splatting (3DGS) by integrating visual-physical modeling for articulated object understanding and interaction. ArtGS begins with multi-view RGB-D reconstruction, followed by reasoning with a vision-language model (VLM) to extract semantic and structural information, particularly the articulated bones. Through dynamic, differentiable 3DGS-based rendering, ArtGS optimizes the parameters of the articulated bones, ensuring physically consistent motion constraints and enhancing the manipulation policy. By leveraging dynamic Gaussian splatting, cross-embodiment adaptability, and closed-loop optimization, ArtGS establishes a new framework for efficient, scalable, and generalizable articulated object modeling and manipulation. Experiments conducted in both simulation and real-world environments demonstrate that ArtGS significantly outperforms previous methods in joint estimation accuracy and manipulation success rates across a variety of articulated objects. Additional images and videos are available on the project website: <a target="_blank" rel="noopener" href="https://sites.google.com/view/artgs/home">https://sites.google.com/view/artgs/home</a> </p>
<blockquote>
<p>关节式对象操作仍然是机器人技术中的一项关键挑战，因为存在复杂的运动学约束和现有方法的物理推理有限。在这项工作中，我们引入了ArtGS，这是一个通过整合视觉物理建模来扩展三维高斯贴片技术（3DGS）的新型框架，用于关节式对象的理解和交互。ArtGS始于多视角RGB-D重建，随后利用视觉语言模型（VLM）进行推理，以提取语义和结构信息，特别是关节骨骼。通过基于动态、可区分3DGS的渲染，ArtGS优化了关节骨骼的参数，确保物理一致的动态约束，并增强了操作策略。通过利用动态高斯贴片技术、跨体态适应性和闭环优化，ArtGS建立了一个高效、可扩展和通用的关节式对象建模和操作的新框架。在模拟和真实环境中所进行的实验表明，ArtGS在关节估计准确性和操作成功率方面大大优于以前的方法，适用于各种关节式对象。更多图片和视频可在项目网站查看：<a target="_blank" rel="noopener" href="https://sites.google.com/view/artgs/home">https://sites.google.com/view/artgs/home</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02600v1">PDF</a> Accepted by IROS 2025</p>
<p><strong>Summary</strong><br>本文提出了一个名为ArtGS的新框架，通过集成视觉物理建模来实现关节可动对象的操控和交互，使用视觉-语言模型提取语义和结构信息，并利用动态可微分的3DGS渲染技术优化关节参数，确保物理一致性运动约束，提高了操控策略。在模拟和真实环境中进行的实验表明，ArtGS在关节估计准确性和操控成功率方面显著优于先前的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ArtGS是一个基于视觉物理建模的框架，用于关节可动对象的操控和交互。</li>
<li>利用多视角RGB-D重建进行初步对象理解。</li>
<li>通过结合视觉语言模型（VLM）提取语义和结构信息，特别是关节骨信息。</li>
<li>使用动态、可微分的3DGS渲染技术进行关节参数优化。</li>
<li>引入动态高斯蒙版、跨嵌入适应性和闭环优化机制提升框架效率。</li>
<li>ArtGS在关节估计准确性和操控成功率上超越先前方法。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02600">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-19306d84afdf2ad525578ded6641a000.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82149cf648e10c65999d8b82245f0e70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c09f202400fa5ea8ef8ff84ed9a0c7f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8522b2f228986f325a3f851f05808ac2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d503646dc1cadc42402b7d3d52481021.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Reconstructing-Close-Human-Interaction-with-Appearance-and-Proxemics-Reasoning"><a href="#Reconstructing-Close-Human-Interaction-with-Appearance-and-Proxemics-Reasoning" class="headerlink" title="Reconstructing Close Human Interaction with Appearance and Proxemics   Reasoning"></a>Reconstructing Close Human Interaction with Appearance and Proxemics   Reasoning</h2><p><strong>Authors:Buzhen Huang, Chen Li, Chongyang Xu, Dongyue Lu, Jinnan Chen, Yangang Wang, Gim Hee Lee</strong></p>
<p>Due to visual ambiguities and inter-person occlusions, existing human pose estimation methods cannot recover plausible close interactions from in-the-wild videos. Even state-of-the-art large foundation models~(\eg, SAM) cannot accurately distinguish human semantics in such challenging scenarios. In this work, we find that human appearance can provide a straightforward cue to address these obstacles. Based on this observation, we propose a dual-branch optimization framework to reconstruct accurate interactive motions with plausible body contacts constrained by human appearances, social proxemics, and physical laws. Specifically, we first train a diffusion model to learn the human proxemic behavior and pose prior knowledge. The trained network and two optimizable tensors are then incorporated into a dual-branch optimization framework to reconstruct human motions and appearances. Several constraints based on 3D Gaussians, 2D keypoints, and mesh penetrations are also designed to assist the optimization. With the proxemics prior and diverse constraints, our method is capable of estimating accurate interactions from in-the-wild videos captured in complex environments. We further build a dataset with pseudo ground-truth interaction annotations, which may promote future research on pose estimation and human behavior understanding. Experimental results on several benchmarks demonstrate that our method outperforms existing approaches. The code and data are available at <a target="_blank" rel="noopener" href="https://www.buzhenhuang.com/works/CloseApp.html">https://www.buzhenhuang.com/works/CloseApp.html</a>. </p>
<blockquote>
<p>由于视觉模糊和人与人之间的相互遮挡，现有的人体姿态估计方法无法从野生视频中恢复出合理的紧密交互。即使是最先进的大型基础模型（例如SAM）也无法在这种具有挑战性的场景中准确区分人体语义。在这项工作中，我们发现人的外观可以提供一种直接的线索来解决这些障碍。基于这一观察，我们提出了一种双分支优化框架，通过人体外观、社交空间学（proxemics）和物理定律的约束来重建准确的交互动作和合理的身体接触。具体来说，我们首先对扩散模型进行训练，学习人类的空间行为学和姿态先验知识。然后将训练好的网络和两个可优化的张量纳入双分支优化框架中，以重建人体动作和外观。还设计了基于3D高斯分布、2D关键点以及网格穿透的多种约束来帮助优化。凭借空间学先验知识和多样化的约束条件，我们的方法能够从复杂环境中捕获的野生视频中估计出准确的人体交互动作。我们还建立了一个带有伪地面真实交互注释的数据集，这可能有助于推动未来在姿态估计和人类行为理解方面的研究。在几个基准测试上的实验结果证明，我们的方法优于现有方法。代码和数据集可在<a target="_blank" rel="noopener" href="https://www.buzhenhuang.com/works/CloseApp.html%E6%89%BE%E5%88%B0%E3%80%82">https://www.buzhenhuang.com/works/CloseApp.html找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02565v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文针对人类姿态估计在复杂环境中的难点进行了深入研究，发现人类外观是突破这一问题的关键线索。因此提出一个基于人体外观、社交距离和人类行为约束的双分支优化框架来准确重建人类运动。结合扩散模型进行学习和训练网络，在模拟不同复杂环境下的场景，可以有效地估计人类交互动作。该研究还建立了一个包含伪真实交互注释的数据集，促进了姿态估计和人类行为理解的研究。实验结果表明，该方法优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有的人类姿态估计方法在处理复杂环境中的紧密交互时存在困难。</li>
<li>人类外观信息是解决这一问题的关键线索。</li>
<li>提出了一种基于人体外观、社交距离和人类行为约束的双分支优化框架来重建准确的人类运动。</li>
<li>利用扩散模型学习人类行为模式和姿态先验知识。</li>
<li>通过多种约束（如3D高斯分布、2D关键点、网格穿透等）辅助优化过程。</li>
<li>建立了一个包含伪真实交互注释的数据集，有助于推动姿态估计和人类行为理解的研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02565">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-42bc17fa39ad06cd6cb26dab7c9fad37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e13336ff5e3373bf1c2748676e1565ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d19eb1174d90cea03009525b6f1425fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-77be3bfc211df6a2b17d27843b78e17d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AvatarMakeup-Realistic-Makeup-Transfer-for-3D-Animatable-Head-Avatars"><a href="#AvatarMakeup-Realistic-Makeup-Transfer-for-3D-Animatable-Head-Avatars" class="headerlink" title="AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars"></a>AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars</h2><p><strong>Authors:Yiming Zhong, Xiaolin Zhang, Ligang Liu, Yao Zhao, Yunchao Wei</strong></p>
<p>Similar to facial beautification in real life, 3D virtual avatars require personalized customization to enhance their visual appeal, yet this area remains insufficiently explored. Although current 3D Gaussian editing methods can be adapted for facial makeup purposes, these methods fail to meet the fundamental requirements for achieving realistic makeup effects: 1) ensuring a consistent appearance during drivable expressions, 2) preserving the identity throughout the makeup process, and 3) enabling precise control over fine details. To address these, we propose a specialized 3D makeup method named AvatarMakeup, leveraging a pretrained diffusion model to transfer makeup patterns from a single reference photo of any individual. We adopt a coarse-to-fine idea to first maintain the consistent appearance and identity, and then to refine the details. In particular, the diffusion model is employed to generate makeup images as supervision. Due to the uncertainties in diffusion process, the generated images are inconsistent across different viewpoints and expressions. Therefore, we propose a Coherent Duplication method to coarsely apply makeup to the target while ensuring consistency across dynamic and multiview effects. Coherent Duplication optimizes a global UV map by recoding the averaged facial attributes among the generated makeup images. By querying the global UV map, it easily synthesizes coherent makeup guidance from arbitrary views and expressions to optimize the target avatar. Given the coarse makeup avatar, we further enhance the makeup by incorporating a Refinement Module into the diffusion model to achieve high makeup quality. Experiments demonstrate that AvatarMakeup achieves state-of-the-art makeup transfer quality and consistency throughout animation. </p>
<blockquote>
<p>类似于现实生活中的面部美容，3D虚拟角色需要个性化定制以增强其视觉吸引力，但这个领域仍然没有得到足够的探索。尽管当前的3D高斯编辑方法可以适应面部化妆的目的，但这些方法未能满足实现真实化妆效果的基本需求：1）在可驱动的表情中确保一致的外观，2）在化妆过程中保留身份特征，以及3）对细节进行精确控制。为了解决这些问题，我们提出了一种专门的3D化妆方法，名为AvatarMakeup，它利用预训练的扩散模型从任何个人的单张参考照片中转移化妆模式。我们采用由粗到细的理念，首先保持外观和身份的一致性，然后细化细节。特别是，利用扩散模型生成化妆图像作为监督。由于扩散过程中的不确定性，生成的图像在不同的视角和表情上存在差异。因此，我们提出了一种连贯复制方法，将化妆粗略地应用到目标上，同时确保动态和多视角效果的一致性。连贯复制通过重新编码生成化妆图像之间的平均面部属性来优化全局UV映射。通过查询全局UV映射，它很容易合成来自任意视角和表情的连贯化妆指导，以优化目标角色。给定粗略的化妆角色，我们进一步通过将细化模块融入到扩散模型中，以提高化妆品质量。实验表明，AvatarMakeup在动画过程中实现了先进的化妆转移质量和一致性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02419v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为AvatarMakeup的3D虚拟角色化妆方法，该方法利用预训练的扩散模型从单一参考照片转移妆容。采用由粗到细的策略，先保持外观和身份的连续性，再细化细节。使用扩散模型生成妆容图像作为监督，并提出Coherent Duplication方法确保动态和多视角下的妆容一致性。通过优化全局UV映射，合成任意视角和表情下的连贯妆容指导。进一步通过引入细化模块提升妆容质量。实验表明，AvatarMakeup在化妆转移质量和动画一致性方面达到领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3D虚拟角色需要个性化定制以增强其视觉吸引力，但当前方法无法满足真实妆容效果的要求。</li>
<li>提出了AvatarMakeup方法，利用预训练的扩散模型从单一参考照片转移妆容。</li>
<li>采用由粗到细的策略，先保证外观和身份的连续性，再细化细节。</li>
<li>使用扩散模型生成妆容图像作为监督，但存在不确定性问题。</li>
<li>提出Coherent Duplication方法，优化全局UV映射，确保动态和多视角下的妆容一致性。</li>
<li>引入细化模块提升妆容质量。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02419">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bc87e0924f6be6ebe0fd48bebca87ff2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7aa1481255c0157df1d6ec2b1ba125f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66fda79a028a957be39888e65417c685.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8a6e4afb0f824163399f06eff68027c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8163a2160dd5abd48450c8e3796778c3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LocalDyGS-Multi-view-Global-Dynamic-Scene-Modeling-via-Adaptive-Local-Implicit-Feature-Decoupling"><a href="#LocalDyGS-Multi-view-Global-Dynamic-Scene-Modeling-via-Adaptive-Local-Implicit-Feature-Decoupling" class="headerlink" title="LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local   Implicit Feature Decoupling"></a>LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local   Implicit Feature Decoupling</h2><p><strong>Authors:Jiahao Wu, Rui Peng, Jianbo Jiao, Jiayu Yang, Luyang Tang, Kaiqiang Xiong, Jie Liang, Jinbo Yan, Runling Liu, Ronggang Wang</strong></p>
<p>Due to the complex and highly dynamic motions in the real world, synthesizing dynamic videos from multi-view inputs for arbitrary viewpoints is challenging. Previous works based on neural radiance field or 3D Gaussian splatting are limited to modeling fine-scale motion, greatly restricting their application. In this paper, we introduce LocalDyGS, which consists of two parts to adapt our method to both large-scale and fine-scale motion scenes: 1) We decompose a complex dynamic scene into streamlined local spaces defined by seeds, enabling global modeling by capturing motion within each local space. 2) We decouple static and dynamic features for local space motion modeling. A static feature shared across time steps captures static information, while a dynamic residual field provides time-specific features. These are combined and decoded to generate Temporal Gaussians, modeling motion within each local space. As a result, we propose a novel dynamic scene reconstruction framework to model highly dynamic real-world scenes more realistically. Our method not only demonstrates competitive performance on various fine-scale datasets compared to state-of-the-art (SOTA) methods, but also represents the first attempt to model larger and more complex highly dynamic scenes. Project page: <a target="_blank" rel="noopener" href="https://wujh2001.github.io/LocalDyGS/">https://wujh2001.github.io/LocalDyGS/</a>. </p>
<blockquote>
<p>由于现实世界中复杂且高度动态的动态，从多视角输入合成任意视角的动态视频是一项挑战。之前基于神经辐射场或3D高斯贴图的工作仅限于对精细动作的建模，极大地限制了其应用。在本文中，我们介绍了LocalDyGS，它由两部分组成，使我们的方法能够适应大规模和精细动作的场景：1）我们将复杂的动态场景分解为由种子定义的流线型局部空间，通过捕捉每个局部空间内的运动来进行全局建模。2）我们将静态和动态特征解耦，用于局部空间运动建模。一个跨时间步共享的静态特征捕捉静态信息，而一个动态残差场提供特定时间的特征。这些特征相结合并解码，生成时序高斯，模拟每个局部空间内的运动。因此，我们提出了一种新的动态场景重建框架，以更真实的方式对高度动态的现实世界场景进行建模。我们的方法不仅在各种精细数据集上展现出与最新技术相竞争的性能，而且还首次尝试对更大、更复杂的动态场景进行建模。项目页面：<a target="_blank" rel="noopener" href="https://wujh2001.github.io/LocalDyGS/%E3%80%82">https://wujh2001.github.io/LocalDyGS/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02363v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为LocalDyGS的新方法，用于合成动态视频。该方法通过分解复杂动态场景、解构静态与动态特征，建立局部空间运动模型，以应对真实世界中复杂且高度动态的运动。此框架不仅在精细尺度数据集上表现卓越，更是首次尝试模拟更大、更复杂的场景运动。详情请参见项目网页。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LocalDyGS方法能处理真实世界中的复杂且高度动态的运动场景。</li>
<li>方法通过分解复杂动态场景到局部空间进行建模，以应对大规模运动场景。</li>
<li>静态和动态特征被解耦，以便更准确地捕捉局部空间的运动特性。</li>
<li>通过结合静态特征和动态残差场，生成时间高斯模型，以模拟局部空间的运动。</li>
<li>LocalDyGS不仅在精细尺度数据集上的性能具有竞争力，而且能够模拟更大、更复杂的场景运动。</li>
<li>项目网页提供了更多关于LocalDyGS方法的详细信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02363">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3081ecf13671adcdfd22f22fbcf9e856.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2c08eecf660c741dd5021cf74a99305.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1e303bb91bdef2ea5454b3c8f4a05bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b89ee60a50eaa45e72e6e5ed7faf2fd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d44c5724d9804024e315f6126094b53.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Gbake-Baking-3D-Gaussian-Splats-into-Reflection-Probes"><a href="#Gbake-Baking-3D-Gaussian-Splats-into-Reflection-Probes" class="headerlink" title="Gbake: Baking 3D Gaussian Splats into Reflection Probes"></a>Gbake: Baking 3D Gaussian Splats into Reflection Probes</h2><p><strong>Authors:Stephen Pasch, Joel K. Salzman, Changxi Zheng</strong></p>
<p>The growing popularity of 3D Gaussian Splatting has created the need to integrate traditional computer graphics techniques and assets in splatted environments. Since 3D Gaussian primitives encode lighting and geometry jointly as appearance, meshes are relit improperly when inserted directly in a mixture of 3D Gaussians and thus appear noticeably out of place. We introduce GBake, a specialized tool for baking reflection probes from Gaussian-splatted scenes that enables realistic reflection mapping of traditional 3D meshes in the Unity game engine. </p>
<blockquote>
<p>随着三维高斯混合技术的日益普及，需要在混合环境中集成传统的计算机图形技术和资源。由于三维高斯基本体将光照和几何结构共同编码为外观，因此当直接插入混合的三维高斯中时，网格会重新照明不当，从而显得格格不入。我们引入了GBake，这是一个用于从高斯混合场景中烘焙反射探针的专用工具，它能够在Unity游戏引擎中实现传统三维网格的真实反射映射。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02257v1">PDF</a> SIGGRAPH 2025 Posters</p>
<p><strong>Summary</strong></p>
<p>随着三维高斯混合技术的普及，需要将传统计算机图形技术资产融入混合环境。由于高斯原始数据同时编码光照和几何外观，直接插入混合高斯中的网格会重新照明不当，导致突兀感。为此，我们引入了GBake工具，它能从高斯混合场景中烘焙反射探针，实现在Unity游戏引擎中传统三维网格的真实反射映射。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>三维高斯混合技术的普及带来了对传统计算机图形技术资产整合的需求。</li>
<li>3D Gaussian primitives联合编码光照和几何外观，导致直接插入混合高斯环境中的网格照明出现问题。</li>
<li>高斯环境下的网格需要重新照明以避免突兀感。</li>
<li>GBake工具是为了解决在Unity游戏引擎中传统三维网格在高斯混合场景中的真实反射映射问题而开发的。</li>
<li>GBake工具能从高斯混合场景中烘焙反射探针。</li>
<li>通过使用GBake工具，可以实现在Unity游戏引擎中更真实的渲染效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02257">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e5ef49662025aa95e91853d5fdf5b47c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-639829e00245204fa509b3bf0d8fb577.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6928f7d99de7cd57cf64e1a706c43885.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9fc9297b3f94e4d810a3dca992d529cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fba6fdd60c213dcc1c15cdc97bedfba0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-928213e960387f486e50224b7f68fcdc.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="3D-Gaussian-Splatting-Driven-Multi-View-Robust-Physical-Adversarial-Camouflage-Generation"><a href="#3D-Gaussian-Splatting-Driven-Multi-View-Robust-Physical-Adversarial-Camouflage-Generation" class="headerlink" title="3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial   Camouflage Generation"></a>3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial   Camouflage Generation</h2><p><strong>Authors:Tianrui Lou, Xiaojun Jia, Siyuan Liang, Jiawei Liang, Ming Zhang, Yanjun Xiao, Xiaochun Cao</strong></p>
<p>Physical adversarial attack methods expose the vulnerabilities of deep neural networks and pose a significant threat to safety-critical scenarios such as autonomous driving. Camouflage-based physical attack is a more promising approach compared to the patch-based attack, offering stronger adversarial effectiveness in complex physical environments. However, most prior work relies on mesh priors of the target object and virtual environments constructed by simulators, which are time-consuming to obtain and inevitably differ from the real world. Moreover, due to the limitations of the backgrounds in training images, previous methods often fail to produce multi-view robust adversarial camouflage and tend to fall into sub-optimal solutions. Due to these reasons, prior work lacks adversarial effectiveness and robustness across diverse viewpoints and physical environments. We propose a physical attack framework based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and precise reconstruction with few images, along with photo-realistic rendering capabilities. Our framework further enhances cross-view robustness and adversarial effectiveness by preventing mutual and self-occlusion among Gaussians and employing a min-max optimization approach that adjusts the imaging background of each viewpoint, helping the algorithm filter out non-robust adversarial features. Extensive experiments validate the effectiveness and superiority of PGA. Our code is available at:<a target="_blank" rel="noopener" href="https://github.com/TRLou/PGA">https://github.com/TRLou/PGA</a>. </p>
<blockquote>
<p>物理对抗攻击方法揭示了深度神经网络的脆弱性，并对自动驾驶等安全关键场景构成了重大威胁。与基于补丁的攻击相比，基于伪装攻击的物攻击方法是一种更有前途的方法，在复杂的物理环境中具有更强的对抗效果。然而，大多数早期的研究依赖于目标物体的网格先验和模拟器构建的虚拟环境，这些都需要耗费大量时间且不可避免地与真实世界存在差异。此外，由于训练图像背景的限制，之前的方法往往无法生成多视角稳健的对抗伪装，并倾向于陷入次优解。由于这些原因，早期的研究在跨越不同视角和物理环境方面的对抗效果和稳健性方面存在不足。我们提出了一种基于三维高斯拼贴（3DGS）的物理攻击框架，名为PGA。该框架具有快速精确重建少量图像的能力以及逼真的渲染能力。我们的框架通过防止高斯之间的相互和自遮挡并采用最小最大优化方法调整每个视角的成像背景，进一步提高了跨视角的稳健性和对抗效果，帮助算法过滤掉非稳健的对抗特征。大量实验验证了PGA的有效性和优越性。我们的代码可在：<a target="_blank" rel="noopener" href="https://github.com/TRLou/PGA%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/TRLou/PGA访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01367v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>基于物理的攻击方法揭示了深度神经网络的安全漏洞，特别是在自动驾驶等安全关键场景中构成了重大威胁。相比于基于补丁的攻击方法，伪装攻击是一种更有前景的方法，能够在复杂的物理环境中实现更强的对抗性效果。然而，大多数先前的研究依赖于目标物体的网格先验和由模拟器构建的虚拟环境，这些环境耗时且不可避免地与真实世界存在差异。此外，由于训练图像背景的限制，先前的方法往往难以生成多视角的鲁棒对抗伪装，并容易陷入次优解决方案。为了解决这些问题，我们提出了基于三维高斯拼贴（3DGS）的物理攻击框架——PGA。该框架具有快速精确重建和逼真的渲染能力。通过防止高斯之间的互相遮挡和自我遮挡以及采用调整每个视点成像背景的minmax优化方法，该框架增强了跨视角的鲁棒性和对抗性效果，使算法能够过滤掉非鲁棒的对抗特征。实验证明PGA的有效性优于其他方法。我们的代码可通过链接访问：[链接地址]。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>物理对抗攻击方法揭示了深度神经网络的安全漏洞，特别是在自动驾驶等安全关键场景中。</li>
<li>伪装攻击是一种在复杂物理环境中实现更强对抗性效果的有前景的方法。</li>
<li>大多数先前研究依赖于虚拟环境，这些环境与真实世界存在差异。</li>
<li>训练图像背景的限制导致先前方法难以生成多视角的鲁棒对抗伪装。</li>
<li>提出的基于三维高斯拼贴（3DGS）的物理攻击框架PGA具有快速精确重建和逼真的渲染能力。</li>
<li>PGA通过防止高斯之间的互相遮挡和自我遮挡以及采用minmax优化方法增强了跨视角的鲁棒性和对抗性效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01367">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-44deda9972e1204140a4e1abbf956124.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcbcc4d220811732b4e339d428bda9db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f74e4e5477d3a8509967dea6f0e494f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cde9389c287b2f88fc1e844fd1f0e301.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="VISTA-Open-Vocabulary-Task-Relevant-Robot-Exploration-with-Online-Semantic-Gaussian-Splatting"><a href="#VISTA-Open-Vocabulary-Task-Relevant-Robot-Exploration-with-Online-Semantic-Gaussian-Splatting" class="headerlink" title="VISTA: Open-Vocabulary, Task-Relevant Robot Exploration with Online   Semantic Gaussian Splatting"></a>VISTA: Open-Vocabulary, Task-Relevant Robot Exploration with Online   Semantic Gaussian Splatting</h2><p><strong>Authors:Keiko Nagami, Timothy Chen, Javier Yu, Ola Shorinwa, Maximilian Adang, Carlyn Dougherty, Eric Cristofalo, Mac Schwager</strong></p>
<p>We present VISTA (Viewpoint-based Image selection with Semantic Task Awareness), an active exploration method for robots to plan informative trajectories that improve 3D map quality in areas most relevant for task completion. Given an open-vocabulary search instruction (e.g., “find a person”), VISTA enables a robot to explore its environment to search for the object of interest, while simultaneously building a real-time semantic 3D Gaussian Splatting reconstruction of the scene. The robot navigates its environment by planning receding-horizon trajectories that prioritize semantic similarity to the query and exploration of unseen regions of the environment. To evaluate trajectories, VISTA introduces a novel, efficient viewpoint-semantic coverage metric that quantifies both the geometric view diversity and task relevance in the 3D scene. On static datasets, our coverage metric outperforms state-of-the-art baselines, FisherRF and Bayes’ Rays, in computation speed and reconstruction quality. In quadrotor hardware experiments, VISTA achieves 6x higher success rates in challenging maps, compared to baseline methods, while matching baseline performance in less challenging maps. Lastly, we show that VISTA is platform-agnostic by deploying it on a quadrotor drone and a Spot quadruped robot. Open-source code will be released upon acceptance of the paper. </p>
<blockquote>
<p>我们提出VISTA（基于视点图像选择并具有语义任务感知能力），这是一种机器人主动探索方法，用于规划能够改善任务完成相关区域的三维地图质量的轨迹。给定开放词汇搜索指令（例如，“寻找一个人”），VISTA允许机器人在探索环境以寻找感兴趣对象的同时，实时构建场景的语义三维高斯喷溅重建。机器人通过规划具有优先语义相似性和探索未知区域的未来地平线轨迹来导航环境。为了评估轨迹，VISTA引入了一种新的高效视点语义覆盖度量标准，该标准可以量化三维场景中的几何视图多样性和任务相关性。在静态数据集上，我们的覆盖度量标准在计算速度和重建质量方面优于最新基线FisherRF和贝叶斯射线。在四旋翼硬件实验中，与基线方法相比，VISTA在具有挑战性的地图上实现了高达六倍的成功率提升，同时在不太具有挑战性的地图上保持基线性能。最后，我们通过将其部署在四旋翼无人机和Spot四足机器人上证明了VISTA具有平台中立性。论文被接受后将发布开源代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01125v1">PDF</a> 9 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了一个名为VISTA的机器人主动探索方法，用于规划信息轨迹以提高3D地图质量，重点关注与任务完成相关的区域。VISTA可处理开放式词汇搜索指令，使机器人在探索环境寻找目标对象的同时，实时构建场景的语义3D高斯混合重建模型。通过规划不断接近地平线的轨迹，机器人能够优先考虑到查询的语义相似性和对未观测区域的探索。VISTA引入了一种新的、高效的视点语义覆盖度量标准，以量化三维场景中的几何视图多样性和任务相关性。在静态数据集上，其覆盖度量在计算速度和重建质量上优于最新基线FisherRF和Bayes射线。在无人机的硬件实验中，与基线方法相比，VISTA在具有挑战性的地图上成功率提高了六倍，同时在不太具有挑战性的地图上保持了基线性能。此外，我们还展示了VISTA的平台无关性，可以在无人机和Spot四足机器人上部署。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VISTA是一种机器人主动探索方法，用于规划信息轨迹以提高3D地图质量。</li>
<li>VISTA可处理开放式词汇搜索指令，结合语义探索和实时3D重建。</li>
<li>机器人通过规划不断接近地平线的轨迹来平衡查询的语义相似性和对未观测区域的探索。</li>
<li>VISTA引入了一种新的视点语义覆盖度量标准，以量化三维场景中的几何视图多样性和任务相关性。</li>
<li>在静态数据集上，VISTA的覆盖度量在计算速度和重建质量方面优于其他方法。</li>
<li>在硬件实验中，VISTA在挑战性地图上表现出更高的成功率，同时在不太具有挑战性的地图上保持了基线性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01125">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-913e2b8e8f37562e40265d2df28d783a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac3b8829ffe4dceab36baf5db547448c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d28fdf275388d246e0c1df4a250a1df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-473b4dda75125b31cbbe75a2ac05788d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Masks-make-discriminative-models-great-again"><a href="#Masks-make-discriminative-models-great-again" class="headerlink" title="Masks make discriminative models great again!"></a>Masks make discriminative models great again!</h2><p><strong>Authors:Tianshi Cao, Marie-Julie Rakotosaona, Ben Poole, Federico Tombari, Michael Niemeyer</strong></p>
<p>We present Image2GS, a novel approach that addresses the challenging problem of reconstructing photorealistic 3D scenes from a single image by focusing specifically on the image-to-3D lifting component of the reconstruction process. By decoupling the lifting problem (converting an image to a 3D model representing what is visible) from the completion problem (hallucinating content not present in the input), we create a more deterministic task suitable for discriminative models. Our method employs visibility masks derived from optimized 3D Gaussian splats to exclude areas not visible from the source view during training. This masked training strategy significantly improves reconstruction quality in visible regions compared to strong baselines. Notably, despite being trained only on masked regions, Image2GS remains competitive with state-of-the-art discriminative models trained on full target images when evaluated on complete scenes. Our findings highlight the fundamental struggle discriminative models face when fitting unseen regions and demonstrate the advantages of addressing image-to-3D lifting as a distinct problem with specialized techniques. </p>
<blockquote>
<p>我们提出了Image2GS这一新方法，专注于解决从单一图像重建写实风格的3D场景这一难题。我们的方法重点研究重建过程中的图像到3D转换环节。通过解除提升问题（将图像转换为表示可见内容的3D模型）与补全问题（凭空想象输入中不存在的部分）之间的耦合关系，我们创造了一个更确定性的任务，更适合判别模型来处理。我们的方法使用由优化后的3D高斯展布派生的可见性掩码，在训练过程中排除源视角不可见的区域。与强大的基线相比，这种掩码训练策略在可见区域的重建质量方面显著提高。值得注意的是，尽管仅在掩码区域上进行训练，但当在完整场景上评估时，Image2GS仍与在完整目标图像上训练的最新判别模型竞争。我们的研究突出了判别模型在面对未知区域拟合时的基本挑战，并展示了将图像到3D转换作为一个具有专门技术的问题来解决的优点。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00916v1">PDF</a> </p>
<p><strong>Summary</strong><br>图像重建中的单幅图像到三维场景的转换问题，可以通过采用专门的图像到三维提升技术来解决。本文提出一种名为Image2GS的新方法，通过将提升问题和完成问题分离，实现了从图像到三维模型的转换。采用基于优化的三维高斯掩模进行训练，显著提高可见区域的重建质量。Image2GS方法表现出独特的优势，在解决特定问题时展现了较高的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Image2GS专注于从单幅图像重建三维场景的提升问题，分离了提升问题和完成问题。</li>
<li>使用优化的三维高斯掩模在训练过程中排除源视图不可见区域，从而提高可见区域的重建质量。</li>
<li>Image2GS在训练时仅针对掩模区域进行训练，但在完整场景评估时仍与全目标图像训练的最新判别模型竞争。</li>
<li>该方法强调了判别模型在拟合未见区域时的基本挑战，并展示了采用专门技术的图像到三维提升问题的优势。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00916">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-23332aba3a4f6d7924df6f9cf74231c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc2dbf7781b29f8d30fffee64395f30b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7581ba4d4a5aa83439c5c5cc67dd4db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-023a5cb73bc8f41d465996d91b5318d0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LOD-GS-Level-of-Detail-Sensitive-3D-Gaussian-Splatting-for-Detail-Conserved-Anti-Aliasing"><a href="#LOD-GS-Level-of-Detail-Sensitive-3D-Gaussian-Splatting-for-Detail-Conserved-Anti-Aliasing" class="headerlink" title="LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail   Conserved Anti-Aliasing"></a>LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail   Conserved Anti-Aliasing</h2><p><strong>Authors:Zhenya Yang, Bingchen Gong, Kai Chen, Qi Dou</strong></p>
<p>Despite the advancements in quality and efficiency achieved by 3D Gaussian Splatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent challenge. Existing approaches primarily rely on low-pass filtering to mitigate aliasing. However, these methods are not sensitive to the sampling rate, often resulting in under-filtering and over-smoothing renderings. To address this limitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework for Gaussian Splatting, which dynamically predicts the optimal filtering strength for each 3D Gaussian primitive. Specifically, we introduce a set of basis functions to each Gaussian, which take the sampling rate as input to model appearance variations, enabling sampling-rate-sensitive filtering. These basis function parameters are jointly optimized with the 3D Gaussian in an end-to-end manner. The sampling rate is influenced by both focal length and camera distance. However, existing methods and datasets rely solely on down-sampling to simulate focal length changes for anti-aliasing evaluation, overlooking the impact of camera distance. To enable a more comprehensive assessment, we introduce a new synthetic dataset featuring objects rendered at varying camera distances. Extensive experiments on both public datasets and our newly collected dataset demonstrate that our method achieves SOTA rendering quality while effectively eliminating aliasing. The code and dataset have been open-sourced. </p>
<blockquote>
<p>尽管三维高斯拼贴（3DGS）在三维场景渲染中取得了质量和效率的提升，但混叠伪影仍然是一个持续存在的挑战。现有方法主要依赖低通滤波来减轻混叠。然而，这些方法对采样率不够敏感，通常会导致滤波不足和过度平滑的渲染结果。为了解决这一局限性，我们提出了LOD-GS，这是一个用于高斯拼贴的细节层次敏感滤波框架，能够动态预测每个三维高斯基元的最佳滤波强度。具体来说，我们为每个高斯引入了一组基函数，以采样率作为输入来模拟外观变化，从而实现采样率敏感滤波。这些基函数的参数与三维高斯以端到端的方式进行联合优化。采样率受到焦距和相机距离的影响。然而，现有方法和数据集仅通过下采样来模拟焦距变化以进行抗混叠评估，忽略了相机距离的影响。为了进行更全面的评估，我们引入了一个新的合成数据集，该数据集包含在不同相机距离下呈现的对象。在公共数据集和我们新收集的数据集上的大量实验表明，我们的方法达到了先进的渲染质量，同时有效地消除了混叠。相关代码和数据集已开源。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00554v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对3D场景渲染中的抗锯齿问题，提出了一种基于Level-of-Detail（LOD）敏感性的高斯Splatting（LOD-GS）滤波框架。该框架通过动态预测每个3D高斯基元的最佳滤波强度来解决现有方法的局限性。同时引入了一套基础函数，用于对采样率进行建模，实现采样率敏感滤波。通过优化基础函数参数和3D高斯参数，达到先进的渲染质量，有效消除锯齿效应。此外，本文还引入了一个新的合成数据集，用于全面评估不同相机距离下的抗锯齿效果。代码和数据集已开源。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>现有的基于低通滤波的抗锯齿方法因缺乏采样率的敏感性，可能导致过滤不足和过度平滑的渲染结果。</li>
<li>LOD-GS滤波框架通过动态预测每个3D高斯基元的最佳滤波强度来解决这一问题。</li>
<li>LOD-GS引入了基础函数来建模采样率的变化，包括相机焦距和距离的影响。</li>
<li>仅依赖降采样模拟焦距变化的现有方法和数据集无法全面评估抗锯齿效果，忽视了相机距离的影响。为此引入了新的合成数据集用于全面评估性能。实验表明LOD-GS可有效消除锯齿并达到领先的渲染质量。数据和代码已经开源以供研究使用。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00554">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ca616831399626a0e1c9de66930a1785.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f70979b4f369c36529216b28aa50d70.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-62141399150fea7a898457057d6d3446.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89fec2c82b2a96e79ecf2f0fd70bdff0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-759d9d60017bcf11345b45d5b9d5ee26.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GDGS-3D-Gaussian-Splatting-Via-Geometry-Guided-Initialization-And-Dynamic-Density-Control"><a href="#GDGS-3D-Gaussian-Splatting-Via-Geometry-Guided-Initialization-And-Dynamic-Density-Control" class="headerlink" title="GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And   Dynamic Density Control"></a>GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And   Dynamic Density Control</h2><p><strong>Authors:Xingjun Wang, Lianlei Shan</strong></p>
<p>We propose a method to enhance 3D Gaussian Splatting (3DGS)~\cite{Kerbl2023}, addressing challenges in initialization, optimization, and density control. Gaussian Splatting is an alternative for rendering realistic images while supporting real-time performance, and it has gained popularity due to its explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate initialization and faces difficulties in optimizing unstructured Gaussian distributions into ordered surfaces, with limited adaptive density control mechanism proposed so far. Our first key contribution is a geometry-guided initialization to predict Gaussian parameters, ensuring precise placement and faster convergence. We then introduce a surface-aligned optimization strategy to refine Gaussian placement, improving geometric accuracy and aligning with the surface normals of the scene. Finally, we present a dynamic adaptive density control mechanism that adjusts Gaussian density based on regional complexity, for visual fidelity. These innovations enable our method to achieve high-fidelity real-time rendering and significant improvements in visual quality, even in complex scenes. Our method demonstrates comparable or superior results to state-of-the-art methods, rendering high-fidelity images in real time. </p>
<blockquote>
<p>我们提出了一种改进3D高斯拼贴（3DGS）的方法\cite{Kerbl2023}，以解决初始化、优化和密度控制方面的挑战。高斯拼贴是一种用于呈现真实图像的技术，支持实时性能，由于其明确的3D高斯表示而广受欢迎。然而，3DGS严重依赖于准确的初始化，并且在将无序的高斯分布优化为有序的曲面时面临困难，目前提出的自适应密度控制机制有限。我们的第一项关键贡献是通过几何引导初始化来预测高斯参数，确保精确放置和更快的收敛速度。然后，我们引入了一种与曲面对齐的优化策略，以细化高斯放置，提高几何精度并与场景的表面法线对齐。最后，我们提出了一种动态自适应密度控制机制，根据区域复杂性调整高斯密度，以提高视觉保真度。这些创新使我们的方法能够实现高保真实时渲染和视觉质量的显着提高，即使在复杂的场景中也是如此。我们的方法与最新技术相比，呈现出相当或更好的结果，能够实时呈现高保真图像。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00363v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>增强3D高斯绘制（3DGS）方法，解决初始化、优化和密度控制方面的挑战。该方法通过几何引导初始化预测高斯参数，确保精确放置和更快收敛；引入表面对齐优化策略，提高几何精度和场景表面法线对齐；提出动态自适应密度控制机制，根据区域复杂性调整高斯密度，以提高视觉保真度。这些创新使该方法实现高保真实时渲染，在复杂场景中显著提高视觉质量，与最新方法相比具有相当或更优的结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出增强3D高斯绘制（3DGS）方法，解决初始化、优化和密度控制的挑战。</li>
<li>通过几何引导初始化预测高斯参数，确保精确放置和更快收敛。</li>
<li>引入表面对齐优化策略，提高几何精度和场景表面法线对齐。</li>
<li>提出动态自适应密度控制机制，根据区域复杂性调整高斯密度。</li>
<li>实现高保真实时渲染，显著提高视觉质量。</li>
<li>方法在复杂场景中表现良好。</li>
<li>与最新方法相比，具有相当或更优的结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00363">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-71d2e084295bd7363e8238f5fafc542a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4802c1343006395686514ba2ab72a576.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04816cb8dce23a39750384813c8b46b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec69e8b559229bea4ed3fb2557c37be7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d70951868f492301eb96e086644332f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1264527f138ee53e31f14d8c8ceaf1a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dde21d871e3e5d15aaff6c7c5782e978.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="RGE-GS-Reward-Guided-Expansive-Driving-Scene-Reconstruction-via-Diffusion-Priors"><a href="#RGE-GS-Reward-Guided-Expansive-Driving-Scene-Reconstruction-via-Diffusion-Priors" class="headerlink" title="RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via   Diffusion Priors"></a>RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via   Diffusion Priors</h2><p><strong>Authors:Sicong Du, Jiarun Liu, Qifeng Chen, Hao-Xiang Chen, Tai-Jiang Mu, Sheng Yang</strong></p>
<p>A single-pass driving clip frequently results in incomplete scanning of the road structure, making reconstructed scene expanding a critical requirement for sensor simulators to effectively regress driving actions. Although contemporary 3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction quality, their direct extension through the integration of diffusion priors often introduces cumulative physical inconsistencies and compromises training efficiency. To address these limitations, we present RGE-GS, a novel expansive reconstruction framework that synergizes diffusion-based generation with reward-guided Gaussian integration. The RGE-GS framework incorporates two key innovations: First, we propose a reward network that learns to identify and prioritize consistently generated patterns prior to reconstruction phases, thereby enabling selective retention of diffusion outputs for spatial stability. Second, during the reconstruction process, we devise a differentiated training strategy that automatically adjust Gaussian optimization progress according to scene converge metrics, which achieving better convergence than baseline methods. Extensive evaluations of publicly available datasets demonstrate that RGE-GS achieves state-of-the-art performance in reconstruction quality. Our source-code will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/CN-ADLab/RGE-GS">https://github.com/CN-ADLab/RGE-GS</a>. </p>
<blockquote>
<p>单通道驾驶视频剪辑经常导致对道路结构的不完全扫描，这使得重建场景成为传感器模拟器有效回归驾驶动作的关键要求。尽管当前的3D高斯拼贴（3DGS）技术实现了令人印象深刻的重建质量，但其通过集成扩散先验值的直接扩展常常会引入累积的物理不一致性并影响训练效率。为了解决这些局限性，我们提出了RGE-GS，这是一种新型扩展重建框架，它协同基于扩散的生成与奖励引导的高斯积分。RGE-GS框架包含两个关键创新点：首先，我们提出了一种奖励网络，该网络在重建阶段之前学习识别和优先生成一致的模式，从而能够有选择地保留扩散输出以实现空间稳定性。其次，在重建过程中，我们制定了一种差异化的训练策略，该策略可根据场景收敛指标自动调整高斯优化进度，从而实现比基线方法更好的收敛效果。对公开可用数据集的广泛评估表明，RGE-GS在重建质量方面达到了最新技术水平。我们的源代码将在<a target="_blank" rel="noopener" href="https://github.com/CN-ADLab/RGE-GS%E4%B8%8A%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/CN-ADLab/RGE-GS上公开。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22800v2">PDF</a> </p>
<p><strong>Summary</strong><br>     基于单通道驾驶片段常常导致道路结构扫描不完整的问题，研究者提出一种新型重建框架RGE-GS，该框架结合了扩散生成与奖励引导的高斯积分技术。其主要创新点包括奖励网络的引入和差异化训练策略的设计。奖励网络能够识别并优先保留重建阶段的一致性生成模式，从而实现扩散输出的选择性保留，提高空间稳定性；差异化训练策略则根据场景收敛指标自动调整高斯优化进程，以实现更高的收敛效果。RGE-GS框架在公开数据集上的评估结果展现出其重建质量优于现有方法。相关源代码将在GitHub上公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>单通道驾驶片段导致道路结构扫描不完整的问题。</li>
<li>RGE-GS是一种新型重建框架，结合了扩散生成与奖励引导的高斯积分技术。</li>
<li>奖励网络能够识别并优先保留一致性生成模式，提高空间稳定性。</li>
<li>差异化训练策略能够根据场景收敛指标自动调整高斯优化进程。</li>
<li>RGE-GS框架在公开数据集上的重建质量优于现有方法。</li>
<li>该研究的源代码将在GitHub上公开，便于他人参考和使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22800">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-511bdf6d28b5f4925c6376f0a24d4003.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1dc5887293733c727875ceffbd312327.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a61f77cef6f2fad62c6db945607a6710.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cab6026585490bb0e4c0e44ab4c64ed8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f97898b913f2623f1c2e953da890eb8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-331841ae066036511fc1fb6939f94a70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3aa49ebc0ed0f569c5990a35c8a6d8d2.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DeGauss-Dynamic-Static-Decomposition-with-Gaussian-Splatting-for-Distractor-free-3D-Reconstruction"><a href="#DeGauss-Dynamic-Static-Decomposition-with-Gaussian-Splatting-for-Distractor-free-3D-Reconstruction" class="headerlink" title="DeGauss: Dynamic-Static Decomposition with Gaussian Splatting for   Distractor-free 3D Reconstruction"></a>DeGauss: Dynamic-Static Decomposition with Gaussian Splatting for   Distractor-free 3D Reconstruction</h2><p><strong>Authors:Rui Wang, Quentin Lohmeyer, Mirko Meboldt, Siyu Tang</strong></p>
<p>Reconstructing clean, distractor-free 3D scenes from real-world captures remains a significant challenge, particularly in highly dynamic and cluttered settings such as egocentric videos. To tackle this problem, we introduce DeGauss, a simple and robust self-supervised framework for dynamic scene reconstruction based on a decoupled dynamic-static Gaussian Splatting design. DeGauss models dynamic elements with foreground Gaussians and static content with background Gaussians, using a probabilistic mask to coordinate their composition and enable independent yet complementary optimization. DeGauss generalizes robustly across a wide range of real-world scenarios, from casual image collections to long, dynamic egocentric videos, without relying on complex heuristics or extensive supervision. Experiments on benchmarks including NeRF-on-the-go, ADT, AEA, Hot3D, and EPIC-Fields demonstrate that DeGauss consistently outperforms existing methods, establishing a strong baseline for generalizable, distractor-free 3D reconstructionin highly dynamic, interaction-rich environments. Project page: <a target="_blank" rel="noopener" href="https://batfacewayne.github.io/DeGauss.io/">https://batfacewayne.github.io/DeGauss.io/</a> </p>
<blockquote>
<p>从现实世界捕捉中重建干净、无干扰物的3D场景仍然是一个重大挑战，特别是在高度动态和杂乱的环境中，如第一人称视频。为了解决这个问题，我们引入了DeGauss，这是一个简单而稳健的基于解耦动态静态高斯涂布设计的动态场景重建自监督框架。DeGauss使用前景高斯对动态元素进行建模，使用背景高斯对静态内容进行建模，并使用概率掩膜来协调它们的组合，实现独立但互补的优化。DeGauss在多种真实场景中具有稳健的泛化能力，从随意的图像集合到冗长、动态的第一人称视频，无需依赖复杂的启发式方法或广泛的监督。在包括NeRF-on-the-go、ADT、AEA、Hot3D和EPIC-Fields等基准测试上的实验表明，DeGauss始终优于现有方法，为在高度动态、交互丰富的环境中的通用无干扰物3D重建建立了强大的基准线。项目页面：<a target="_blank" rel="noopener" href="https://batfacewayne.github.io/DeGauss.io/">https://batfacewayne.github.io/DeGauss.io/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13176v2">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为DeGauss的简洁、稳健的自监督动态场景重建框架，用于从真实世界捕捉中重建无干扰物的3D场景。该框架基于动态静态高斯斑点设计的解耦设计，使用概率掩膜协调动态元素和静态内容的组合，使它们能够独立但互补地进行优化。DeGauss能够广泛应用于各种真实场景，从随机图像集合到长的动态第一人称视频，且无需依赖复杂的启发式方法或广泛的监督。在多个基准测试上的实验结果表明，DeGauss在高度动态、交互丰富的环境中，在无干扰物的3D重建方面始终优于现有方法，为通用化的3D重建建立了强大的基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeGauss是一个自监督的动态场景重建框架，能够处理真实世界捕捉的无干扰物3D场景重建。</li>
<li>框架采用动态静态高斯斑点设计的解耦设计，分别建模动态元素和静态内容。</li>
<li>使用概率掩膜协调动态和静态内容的组合，实现独立且互补的优化。</li>
<li>DeGauss适用于多种真实场景，从随机图像集合到长动态第一人称视频。</li>
<li>该框架无需复杂的启发式方法或广泛的监督即可实现良好的性能。</li>
<li>在多个基准测试上，DeGauss的表现优于现有方法，尤其在高度动态、交互丰富的环境中的表现更为出色。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13176">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-29ea926948ae864003e61fd5d9ac3d28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52abc34ee50baeec82082d5a0b0a39dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e7ca9349c5c917b61361a49ba3a2b90.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd25e5aad263e01ba35748e113a06ea1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Grounding-Creativity-in-Physics-A-Brief-Survey-of-Physical-Priors-in-AIGC"><a href="#Grounding-Creativity-in-Physics-A-Brief-Survey-of-Physical-Priors-in-AIGC" class="headerlink" title="Grounding Creativity in Physics: A Brief Survey of Physical Priors in   AIGC"></a>Grounding Creativity in Physics: A Brief Survey of Physical Priors in   AIGC</h2><p><strong>Authors:Siwei Meng, Yawei Luo, Ping Liu</strong></p>
<p>Recent advancements in AI-generated content have significantly improved the realism of 3D and 4D generation. However, most existing methods prioritize appearance consistency while neglecting underlying physical principles, leading to artifacts such as unrealistic deformations, unstable dynamics, and implausible objects interactions. Incorporating physics priors into generative models has become a crucial research direction to enhance structural integrity and motion realism. This survey provides a review of physics-aware generative methods, systematically analyzing how physical constraints are integrated into 3D and 4D generation. First, we examine recent works in incorporating physical priors into static and dynamic 3D generation, categorizing methods based on representation types, including vision-based, NeRF-based, and Gaussian Splatting-based approaches. Second, we explore emerging techniques in 4D generation, focusing on methods that model temporal dynamics with physical simulations. Finally, we conduct a comparative analysis of major methods, highlighting their strengths, limitations, and suitability for different materials and motion dynamics. By presenting an in-depth analysis of physics-grounded AIGC, this survey aims to bridge the gap between generative models and physical realism, providing insights that inspire future research in physically consistent content generation. </p>
<blockquote>
<p>近期人工智能生成内容的进展极大地提高了3D和4D生成的逼真度。然而，大多数现有方法优先考虑外观的一致性，却忽略了基本的物理原理，导致出现不真实的变形、不稳定的动态以及不合理的物体交互等伪影。将物理先验知识融入生成模型已成为增强结构完整性和运动逼真度的重要研究方向。本文综述了物理感知的生成方法，系统分析了如何将物理约束融入3D和4D生成。首先，我们研究了将物理先验知识融入静态和动态3D生成的最新工作，按表示类型对方法进行分类，包括基于视觉的、基于NeRF的和基于高斯拼贴的方法。其次，我们探索了4D生成的新兴技术，重点关注使用物理模拟对时间动态进行建模的方法。最后，我们对主要方法进行了比较分析，突出了它们各自的优势、局限性和在不同材料和运动动力学方面的适用性。通过对基于物理原理的人工智能生成内容的深入分析，本文旨在弥合生成模型和物理逼真之间的鸿沟，提供能激发未来在物理一致性内容生成方面研究的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07007v3">PDF</a> Accepted by IJCAI 2025 Survey Track</p>
<p><strong>Summary</strong><br>     近期人工智能生成内容的技术进步已显著提高3D和4D生成的逼真度。然而，现有方法多注重外观一致性，忽视底层物理原理，导致生成内容出现不真实变形、动态不稳定、物体交互不合理等问题。融入物理先验知识到生成模型已成为增强结构完整性和运动逼真度的关键研究方向。本文综述了物理感知生成方法，系统分析如何将物理约束融入3D和4D生成。首先，我们研究了将物理先验知识融入静态和动态3D生成的最新工作，按表现形式分类，包括基于视觉、NeRF和Gaussian Splatting的方法。其次，我们探讨了4D生成的新兴技术，重点关注通过物理模拟对时间动态进行建模的方法。最后，我们对主要方法进行了对比分析，突出其优势、局限性和在不同材料和运动动力学方面的适用性。本文旨在填补生成模型与物理现实主义之间的鸿沟，为物理一致的内容生成提供洞察，激发未来研究灵感。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI生成的3D和4D内容在逼真度上有了显著提升，但存在不真实变形、动态不稳定等问题。</li>
<li>融入物理先验知识到生成模型是增强结构完整性和运动逼真度的关键。</li>
<li>静态和动态3D生成的最新工作按表现形式分类，包括基于视觉、NeRF和Gaussian Splatting的方法。</li>
<li>4D生成技术正关注通过物理模拟对时间动态进行建模的方法。</li>
<li>物理感知生成方法在不同材料和运动动力学方面有不同的适用性和局限性。</li>
<li>本文旨在填补生成模型与物理现实主义之间的鸿沟。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07007">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8316847a61ca9ce8a873eb8dcbaa3c8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc02149aac6500706268cff41d86f7d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcb9ba7838cf34365a0566c3c008fea1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Instruct-4DGS-Efficient-Dynamic-Scene-Editing-via-4D-Gaussian-based-Static-Dynamic-Separation"><a href="#Instruct-4DGS-Efficient-Dynamic-Scene-Editing-via-4D-Gaussian-based-Static-Dynamic-Separation" class="headerlink" title="Instruct-4DGS: Efficient Dynamic Scene Editing via 4D Gaussian-based   Static-Dynamic Separation"></a>Instruct-4DGS: Efficient Dynamic Scene Editing via 4D Gaussian-based   Static-Dynamic Separation</h2><p><strong>Authors:Joohyun Kwon, Hanbyel Cho, Junmo Kim</strong></p>
<p>Recent 4D dynamic scene editing methods require editing thousands of 2D images used for dynamic scene synthesis and updating the entire scene with additional training loops, resulting in several hours of processing to edit a single dynamic scene. Therefore, these methods are not scalable with respect to the temporal dimension of the dynamic scene (i.e., the number of timesteps). In this work, we propose Instruct-4DGS, an efficient dynamic scene editing method that is more scalable in terms of temporal dimension. To achieve computational efficiency, we leverage a 4D Gaussian representation that models a 4D dynamic scene by combining static 3D Gaussians with a Hexplane-based deformation field, which captures dynamic information. We then perform editing solely on the static 3D Gaussians, which is the minimal but sufficient component required for visual editing. To resolve the misalignment between the edited 3D Gaussians and the deformation field, which may arise from the editing process, we introduce a refinement stage using a score distillation mechanism. Extensive editing results demonstrate that Instruct-4DGS is efficient, reducing editing time by more than half compared to existing methods while achieving high-quality edits that better follow user instructions. Code and results: <a target="_blank" rel="noopener" href="https://hanbyelcho.info/instruct-4dgs/">https://hanbyelcho.info/instruct-4dgs/</a> </p>
<blockquote>
<p>最近的4D动态场景编辑方法需要对用于动态场景合成的数千张2D图像进行编辑，并通过额外的训练循环更新整个场景，导致编辑单个动态场景需要数小时的处理时间。因此，这些方法在动态场景的时空维度上（即时序数量）并不具备可扩展性。在这项工作中，我们提出了Instruct-4DGS，一种高效的动态场景编辑方法，在时空维度上更具可扩展性。为了实现计算效率，我们采用了一种4D高斯表示法，通过结合静态3D高斯和基于Hexplane的变形场来模拟一个4D动态场景，其中变形场捕捉动态信息。然后，我们仅在静态的3D高斯上进行编辑，这是视觉编辑所需的最小但足够的组件。为了解决编辑后的3D高斯和变形场之间可能出现的错位问题，我们引入了一个使用评分蒸馏机制的优化阶段。大量的编辑结果表明，Instruct-4DGS是高效的，与现有方法相比，编辑时间减少了超过一半，同时实现了高质量的编辑，更好地遵循了用户的指令。相关代码和结果可通过<a target="_blank" rel="noopener" href="https://hanbyelcho.info/instruct-4dgs/%E6%9F%A5%E7%9C%8B%E3%80%82">https://hanbyelcho.info/instruct-4dgs/查看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02091v3">PDF</a> Accepted to CVPR 2025. The first two authors contributed equally</p>
<p><strong>Summary</strong></p>
<p>基于当前四维动态场景编辑方法需要处理数千张二维图像用于动态场景合成，并且需要额外的训练循环来更新整个场景，编辑单个动态场景需要数小时的处理时间。因此，这些方法在动态场景的时空维度上并不具备可扩展性。本研究提出了一种高效的四维动态场景编辑方法——Instruct-4DGS，该方法在时空维度上更具可扩展性。为提升计算效率，本研究采用四维高斯表示法，结合静态三维高斯与基于六平面的变形场来模拟四维动态场景。编辑过程仅针对静态三维高斯进行，这是视觉编辑所需的最小且充分的组件。为解决编辑过程中可能出现的编辑后的三维高斯与变形场之间的不匹配问题，引入了一种基于分数蒸馏的细化阶段。大量的编辑结果证明，Instruct-4DGS方法高效，与现有方法相比，编辑时间减少了一半以上，同时实现了高质量的编辑，更好地遵循了用户的指令。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前四维动态场景编辑方法存在处理时间长、不便于编辑大量动态场景的问题。</li>
<li>Instruct-4DGS方法被提出，以提高四维动态场景编辑的效率与时空维度的可扩展性。</li>
<li>Instruct-4DGS采用四维高斯表示法，结合静态三维高斯与基于六平面的变形场来模拟和编辑四维动态场景。</li>
<li>编辑过程主要针对静态三维高斯进行，这是视觉编辑的最小且充分组件。</li>
<li>为解决编辑后可能出现的三维高斯与变形场不匹配问题，引入了基于分数蒸馏的细化阶段。</li>
<li>Instruct-4DGS方法大大减少了编辑时间，与现有方法相比效率提高一倍以上。</li>
<li>Instruct-4DGS实现了高质量的编辑，更好地遵循了用户的指令。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02091">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e1dd46c4fa181424ce7db384ecef1e94.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d06d97a6a06d81f3261bfb33b2ed4644.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d373dba2b2e0eaced75c9e267c6117fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d0a47a1eecb653cfabe528e0b375132.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="GLS-Geometry-aware-3D-Language-Gaussian-Splatting"><a href="#GLS-Geometry-aware-3D-Language-Gaussian-Splatting" class="headerlink" title="GLS: Geometry-aware 3D Language Gaussian Splatting"></a>GLS: Geometry-aware 3D Language Gaussian Splatting</h2><p><strong>Authors:Jiaxiong Qiu, Liu Liu, Xinjie Wang, Tianwei Lin, Wei Sui, Zhizhong Su</strong></p>
<p>Recently, 3D Gaussian Splatting (3DGS) has achieved impressive performance on indoor surface reconstruction and 3D open-vocabulary segmentation. This paper presents GLS, a unified framework of 3D surface reconstruction and open-vocabulary segmentation based on 3DGS. GLS extends two fields by improving their sharpness and smoothness. For indoor surface reconstruction, we introduce surface normal prior as a geometric cue to guide the rendered normal, and use the normal error to optimize the rendered depth. For 3D open-vocabulary segmentation, we employ 2D CLIP features to guide instance features and enhance the surface smoothness, then utilize DEVA masks to maintain their view consistency. Extensive experiments demonstrate the effectiveness of jointly optimizing surface reconstruction and 3D open-vocabulary segmentation, where GLS surpasses state-of-the-art approaches of each task on MuSHRoom, ScanNet++ and LERF-OVS datasets. Project webpage: <a target="_blank" rel="noopener" href="https://jiaxiongq.github.io/GLS_ProjectPage">https://jiaxiongq.github.io/GLS_ProjectPage</a>. </p>
<blockquote>
<p>最近，3D高斯贴图（3DGS）在室内表面重建和3D开放词汇分割方面取得了令人印象深刻的性能。本文提出了基于3DGS的GLS，这是一个统一的3D表面重建和开放词汇分割框架。GLS通过提高锐度和平滑度来扩展这两个领域。对于室内表面重建，我们引入表面法线先验作为几何线索来引导渲染法线，并使用法线误差来优化渲染深度。对于3D开放词汇分割，我们采用2D CLIP特征来引导实例特征并增强表面平滑度，然后使用DEVA遮罩来保持视图一致性。大量实验表明，联合优化表面重建和3D开放词汇分割是有效的，其中GLS在MuSHRoom、ScanNet++和LERF-OVS数据集上超越了每个任务的最先进方法。项目网页：<a target="_blank" rel="noopener" href="https://jiaxiongq.github.io/GLS_ProjectPage%E3%80%82">https://jiaxiongq.github.io/GLS_ProjectPage。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18066v2">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>基于三维高斯拼贴技术（3DGS），本文提出了GLS统一框架，用于室内表面重建和三维开放词汇分割。在室内表面重建方面，引入法线先验作为几何线索来指导渲染法线，并利用法线误差优化渲染深度。在三维开放词汇分割方面，采用二维CLIP特征引导实例特征并增强表面平滑度，利用DEVA掩膜保持视图一致性。通过联合优化表面重建和三维开放词汇分割，GLS在MuSHRoom、ScanNet++和LERF-OVS数据集上的性能超越了各任务的最先进方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>介绍了基于三维高斯拼贴技术（3DGS）的GLS统一框架，用于室内表面重建和三维开放词汇分割。</li>
<li>在室内表面重建中，引入法线先验作为几何线索指导渲染法线，并通过法线误差优化渲染深度。</li>
<li>采用二维CLIP特征引导实例特征，增强表面平滑度，并利用DEVA掩膜保持视图一致性。</li>
<li>通过联合优化表面重建和三维开放词汇分割任务，GLS在多个数据集上的性能超越现有方法。</li>
<li>GLS框架提高了两个领域的清晰度与平滑度。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18066">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cae79a3f8826f4d9890ae02490f0f798.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-59c8f6e20226ebc1114941036502cd3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e6d5e781e9fea9eaa5b7add8b69187c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a52dfcace7c2e4032d8f76e6ecd4f54d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf52afaa2da9ba4f54dcec6e785977b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a7a81ad9d62cddff4435fb8258b824c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-05/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-05/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-05/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-16256fb252544fb37bdf6def48db78c7.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-07-05  LocalDyGS Multi-view Global Dynamic Scene Modeling via Adaptive Local   Implicit Feature Decoupling
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-05/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9e733f4b7900d962afc9d76566455a9d.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-07-05  HyperGaussians High-Dimensional Gaussian Splatting for High-Fidelity   Animatable Face Avatars
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26024.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
