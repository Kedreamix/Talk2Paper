<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-23  Grasp Any Region Towards Precise, Contextual Pixel Understanding for   Multimodal LLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-82ae12e967f2b361e411f5374cbf505d~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159933&auth_key=1761159933-0-0-283aaeebfab969dc266190d2f42cf7cb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    69 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-23-æ›´æ–°"><a href="#2025-10-23-æ›´æ–°" class="headerlink" title="2025-10-23 æ›´æ–°"></a>2025-10-23 æ›´æ–°</h1><h2 id="Grasp-Any-Region-Towards-Precise-Contextual-Pixel-Understanding-for-Multimodal-LLMs"><a href="#Grasp-Any-Region-Towards-Precise-Contextual-Pixel-Understanding-for-Multimodal-LLMs" class="headerlink" title="Grasp Any Region: Towards Precise, Contextual Pixel Understanding for   Multimodal LLMs"></a>Grasp Any Region: Towards Precise, Contextual Pixel Understanding for   Multimodal LLMs</h2><p><strong>Authors:Haochen Wang, Yuhao Wang, Tao Zhang, Yikang Zhou, Yanwei Li, Jiacong Wang, Ye Tian, Jiahao Meng, Zilong Huang, Guangcan Mai, Anran Wang, Yunhai Tong, Zhuochen Wang, Xiangtai Li, Zhaoxiang Zhang</strong></p>
<p>While Multimodal Large Language Models (MLLMs) excel at holistic understanding, they struggle in capturing the dense world with complex scenes, requiring fine-grained analysis of intricate details and object inter-relationships. Region-level MLLMs have been a promising step. However, previous attempts are generally optimized to understand given regions in isolation, neglecting crucial global contexts. To address this, we introduce Grasp Any Region (GAR) for comprehen- sive region-level visual understanding. Empowered by an effective RoI-aligned feature replay technique, GAR supports (1) precise perception by leveraging necessary global contexts, and (2) modeling interactions between multiple prompts. Together, it then naturally achieves (3) advanced compositional reasoning to answer specific free-form questions about any region, shifting the paradigm from passive description to active dialogue. Moreover, we construct GAR-Bench, which not only provides a more accurate evaluation of single-region comprehension, but also, more importantly, measures interactions and complex reasoning across multiple regions. Extensive experiments have demonstrated that GAR-1B not only maintains the state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5 on DLC-Bench, but also excels at modeling relationships between multiple prompts with advanced comprehension capabilities, even surpassing InternVL3-78B on GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms in-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong capabilities can be easily transferred to videos. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ•´ä½“ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ•æ‰å…·æœ‰å¤æ‚åœºæ™¯çš„å¯†é›†ä¸–ç•Œæ—¶é‡åˆ°å›°éš¾ï¼Œéœ€è¦ç²¾ç»†åˆ†æå¤æ‚ç»†èŠ‚å’Œå¯¹è±¡ä¹‹é—´çš„ç›¸äº’å…³ç³»ã€‚åŒºåŸŸçº§MLLMsæ˜¯ä¸€ä¸ªæœ‰å¸Œæœ›çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„å°è¯•é€šå¸¸è¢«ä¼˜åŒ–ä¸ºç†è§£ç»™å®šçš„å­¤ç«‹åŒºåŸŸï¼Œå¿½ç•¥äº†é‡è¦çš„å…¨å±€ä¸Šä¸‹æ–‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç”¨äºå…¨é¢åŒºåŸŸçº§è§†è§‰ç†è§£çš„Grasp Any Regionï¼ˆGARï¼‰ã€‚é€šè¿‡æœ‰æ•ˆçš„RoIå¯¹é½ç‰¹å¾å›æ”¾æŠ€æœ¯ï¼ŒGARæ”¯æŒï¼ˆ1ï¼‰åˆ©ç”¨å¿…è¦å…¨å±€ä¸Šä¸‹æ–‡çš„ç²¾ç¡®æ„ŸçŸ¥ï¼Œï¼ˆ2ï¼‰å¯¹å¤šä¸ªæç¤ºä¹‹é—´çš„äº¤äº’è¿›è¡Œå»ºæ¨¡ã€‚å› æ­¤ï¼Œå®ƒè‡ªç„¶åœ°å®ç°äº†ï¼ˆ3ï¼‰å…ˆè¿›çš„ç»„åˆæ¨ç†ï¼Œå¯ä»¥å›ç­”æœ‰å…³ä»»ä½•åŒºåŸŸçš„ç‰¹å®šè‡ªç”±å½¢å¼é—®é¢˜ï¼Œä»è¢«åŠ¨æè¿°è½¬å‘ä¸»åŠ¨å¯¹è¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†GAR-Benchï¼Œå®ƒä¸ä»…å¯ä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°å•ä¸€åŒºåŸŸçš„ç†è§£èƒ½åŠ›ï¼Œè€Œä¸”æ›´é‡è¦çš„æ˜¯ï¼Œè¿˜å¯ä»¥è¡¡é‡å¤šä¸ªåŒºåŸŸä¹‹é—´çš„äº¤äº’å’Œå¤æ‚æ¨ç†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGAR-1Bä¸ä»…ä¿æŒäº†æœ€å…ˆè¿›çš„æè¿°èƒ½åŠ›ï¼Œä¾‹å¦‚åœ¨DLC-Benchä¸Šæ¯”DAM-3Bé«˜å‡º4.5åˆ†ï¼Œè€Œä¸”åœ¨å»ºæ¨¡å¤šä¸ªæç¤ºä¹‹é—´çš„å…³ç³»æ–¹é¢ä¹Ÿè¡¨ç°å‡ºå“è¶Šçš„é«˜çº§ç†è§£èƒ½åŠ›ï¼Œç”šè‡³åœ¨GAR-Bench-VQAä¸Šè¶…è¿‡äº†InternVL3-78Bã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„é›¶å¯åŠ¨GAR-8Bç”šè‡³åœ¨VideoRefer-BenchQä¸Šçš„è¡¨ç°ä¼˜äºé¢†åŸŸå†…çš„VideoRefer-7Bï¼Œè¿™è¡¨æ˜å…¶å¼ºå¤§èƒ½åŠ›å¯ä»¥è½»æ¾è½¬ç§»åˆ°è§†é¢‘é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18876v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MLLMsåœ¨å¤„ç†å¤æ‚åœºæ™¯æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶åœ¨æ•æ‰ç²¾ç»†ç²’åº¦çš„ç»†èŠ‚å’Œå¯¹è±¡é—´å…³ç³»æ–¹é¢ã€‚ä¸ºæ”¹å–„æ­¤é—®é¢˜ï¼Œæå‡ºäº†Grasp Any Regionï¼ˆGARï¼‰æ¨¡å‹ï¼Œé€šè¿‡æœ‰æ•ˆç»“åˆå…¨å±€ä¸Šä¸‹æ–‡å’Œå¤šæç¤ºäº¤äº’ï¼Œå®ç°äº†é«˜çº§ç»„åˆæ¨ç†ï¼Œèƒ½å›ç­”å…³äºä»»ä½•åŒºåŸŸçš„ç‰¹å®šè‡ªç”±å½¢å¼é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜æ„å»ºäº†GAR-Benchï¼Œä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°å•åŒºåŸŸç†è§£èƒ½åŠ›ï¼Œå¹¶æµ‹é‡è·¨å¤šä¸ªåŒºåŸŸçš„äº¤äº’å’Œå¤æ‚æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒGAR-1Bä¸ä»…ä¿æŒäº†æœ€å…ˆè¿›çš„æè¿°èƒ½åŠ›ï¼Œè€Œä¸”åœ¨å»ºæ¨¡å¤šæç¤ºå…³ç³»æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„ç†è§£èƒ½åŠ›ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œé›¶æ ·æœ¬çš„GAR-8Båœ¨VideoRefer-BenchQä¸Šçš„è¡¨ç°ç”šè‡³è¶…è¿‡äº†VideoRefer-7Bï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„è·¨åŸŸèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsè™½ç„¶åœ¨æ•´ä½“ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ•æ‰å¤æ‚åœºæ™¯çš„å¯†é›†ä¸–ç•Œæ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦ç²¾ç»†ç²’åº¦çš„åˆ†æå’Œå¯¹è±¡é—´å…³ç³»çš„ç†è§£ã€‚</li>
<li>Grasp Any Region (GAR)æ¨¡å‹æ—¨åœ¨å®ç°å…¨é¢çš„åŒºåŸŸçº§è§†è§‰ç†è§£ï¼Œé€šè¿‡ç»“åˆå…¨å±€ä¸Šä¸‹æ–‡å’Œå¤šæç¤ºäº¤äº’æ¥å…‹æœMLLMsçš„å±€é™æ€§ã€‚</li>
<li>GARæ¨¡å‹æ”¯æŒç²¾ç¡®æ„ŸçŸ¥ï¼Œé€šè¿‡åˆ©ç”¨å¿…è¦çš„å…¨å±€ä¸Šä¸‹æ–‡å’Œå»ºæ¨¡å¤šä¸ªæç¤ºä¹‹é—´çš„äº¤äº’æ¥å®ç°é«˜çº§ç»„åˆæ¨ç†ã€‚</li>
<li>GAR-Benchçš„æ„å»ºä¸ä»…æä¾›äº†å¯¹å•åŒºåŸŸç†è§£çš„æ›´ç²¾ç¡®è¯„ä¼°ï¼Œè¿˜è¡¡é‡äº†è·¨å¤šä¸ªåŒºåŸŸçš„äº¤äº’å’Œå¤æ‚æ¨ç†ã€‚</li>
<li>GAR-1Bæ¨¡å‹åœ¨ä¿æŒæœ€å…ˆè¿›çš„æè¿°èƒ½åŠ›çš„åŒæ—¶ï¼Œä¹Ÿè¡¨ç°å‡ºå“è¶Šçš„å¤šæç¤ºå…³ç³»å»ºæ¨¡èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒGAR-1Båœ¨æŸäº›ä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¿‡äº†å…¶ä»–å¤§å‹æ¨¡å‹ï¼Œå¦‚DAM-3Bå’ŒInternVL3-78Bã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18876">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8f6633b05389fac30789de6b90120cea~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159555&auth_key=1761159555-0-0-a2b2a9f8c7cb2a1107e237fc614c06b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3fe226aee0a16858a35f17042d064d20~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159563&auth_key=1761159563-0-0-2368122dc2977626366b746dae5fe15b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-59d515b9822eefa3db46c1a0ec8f9eac~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159570&auth_key=1761159570-0-0-7c57662ae2b8c1b86eb14664e5ddf9b2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-88e7488cfe41e5998caa911a143213bc~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159576&auth_key=1761159576-0-0-cc646f303a43b75aaa28a6a58c714ef3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LightMem-Lightweight-and-Efficient-Memory-Augmented-Generation"><a href="#LightMem-Lightweight-and-Efficient-Memory-Augmented-Generation" class="headerlink" title="LightMem: Lightweight and Efficient Memory-Augmented Generation"></a>LightMem: Lightweight and Efficient Memory-Augmented Generation</h2><p><strong>Authors:Jizhan Fang, Xinle Deng, Haoming Xu, Ziyan Jiang, Yuqi Tang, Ziwen Xu, Shumin Deng, Yunzhi Yao, Mengru Wang, Shuofei Qiao, Huajun Chen, Ningyu Zhang</strong></p>
<p>Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments. Memory systems enable LLMs to move beyond stateless interactions by introducing persistent information storage, retrieval, and utilization mechanisms. However, existing memory systems often introduce substantial time and computational overhead. To this end, we introduce a new memory system called LightMem, which strikes a balance between the performance and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of human memory, LightMem organizes memory into three complementary stages. First, cognition-inspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics. Next, topic-aware short-term memory consolidates these topic-based groups, organizing and summarizing content for more structured access. Finally, long-term memory with sleep-time update employs an offline procedure that decouples consolidation from online inference. Experiments on LongMemEval with GPT and Qwen backbones show that LightMem outperforms strong baselines in accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API calls by up to 159x, and runtime by over 12x. The code is available at <a target="_blank" rel="noopener" href="https://github.com/zjunlp/LightMem">https://github.com/zjunlp/LightMem</a>. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰å‡ºè‰²çš„èƒ½åŠ›ï¼Œä½†åœ¨åŠ¨æ€å’Œå¤æ‚ç¯å¢ƒä¸­æœ‰æ•ˆåˆ©ç”¨å†å²äº¤äº’ä¿¡æ¯æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚è®°å¿†ç³»ç»Ÿé€šè¿‡å¼•å…¥æŒä¹…ä¿¡æ¯å­˜å‚¨ã€æ£€ç´¢å’Œåˆ©ç”¨æœºåˆ¶ï¼Œä½¿LLMèƒ½å¤Ÿè¶…è¶Šæ— çŠ¶æ€äº¤äº’ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è®°å¿†ç³»ç»Ÿå¾€å¾€å¸¦æ¥å¤§é‡çš„æ—¶é—´å’Œè®¡ç®—å¼€é”€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„è®°å¿†ç³»ç»Ÿï¼Œç§°ä¸ºLightMemï¼Œå®ƒåœ¨è®°å¿†ç³»ç»Ÿçš„æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†å¹³è¡¡ã€‚LightMemå—åˆ°äººç±»è®°å¿†çš„Atkinson-Shiffrinæ¨¡å‹çš„å¯å‘ï¼Œå°†è®°å¿†åˆ†ä¸ºä¸‰ä¸ªäº’è¡¥é˜¶æ®µã€‚é¦–å…ˆï¼Œå—è®¤çŸ¥å¯å‘çš„æ„Ÿè§‰è®°å¿†é€šè¿‡è½»é‡çº§å‹ç¼©å¿«é€Ÿè¿‡æ»¤æ‰æ— å…³ä¿¡æ¯ï¼Œå¹¶æ ¹æ®å…¶ä¸»é¢˜å¯¹ä¿¡æ¯è¿›è¡Œåˆ†ç»„ã€‚æ¥ä¸‹æ¥ï¼Œä¸»é¢˜æ„ŸçŸ¥çš„çŸ­æœŸè®°å¿†å·©å›ºè¿™äº›åŸºäºä¸»é¢˜çš„åˆ†ç»„ï¼Œç»„ç»‡å’Œæ€»ç»“å†…å®¹ï¼Œä»¥ä¾¿è¿›è¡Œæ›´ç»“æ„åŒ–çš„è®¿é—®ã€‚æœ€åï¼Œå…·æœ‰ç¡çœ æ—¶æ›´æ–°çš„é•¿æœŸè®°å¿†é‡‡ç”¨ç¦»çº¿ç¨‹åºï¼Œå°†å·©å›ºä¸åœ¨çº¿æ¨ç†è§£è€¦ã€‚åœ¨LongMemEvalä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨GPTå’ŒQwenéª¨æ¶çš„LightMemåœ¨å‡†ç¡®æ€§ä¸Šè¶…è¿‡äº†å¼ºå¤§çš„åŸºçº¿ï¼ˆæé«˜äº†é«˜è¾¾10.9ï¼…çš„å‡†ç¡®ç‡ï¼‰ï¼ŒåŒæ—¶å‡å°‘äº†é«˜è¾¾117å€çš„ä»¤ç‰Œä½¿ç”¨ã€é«˜è¾¾159å€çš„APIè°ƒç”¨ä»¥åŠè¶…è¿‡12å€çš„è¿è¡Œæ—¶ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zjunlp/LightMem%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/zjunlp/LightMemè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18866v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŠ¨æ€å¤æ‚ç¯å¢ƒä¸­éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨å†å²äº¤äº’ä¿¡æ¯ã€‚å†…å­˜ç³»ç»Ÿçš„å¼•å…¥ä½¿LLMsèƒ½å¤Ÿé€šè¿‡æŒä¹…æ€§ä¿¡æ¯å­˜å‚¨ã€æ£€ç´¢å’Œåˆ©ç”¨æœºåˆ¶æ¥è¶…è¶Šæ— çŠ¶æ€äº¤äº’ã€‚ç„¶è€Œï¼Œç°æœ‰å†…å­˜ç³»ç»Ÿå¾€å¾€å¸¦æ¥å·¨å¤§çš„æ—¶é—´å’Œè®¡ç®—å¼€é”€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„å†…å­˜ç³»ç»ŸLightMemï¼Œå®ƒåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†å¹³è¡¡ã€‚LightMemå—åˆ°äººç±»è®°å¿†Atkinson-Shiffrinæ¨¡å‹çš„å¯å‘ï¼Œå°†è®°å¿†åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µã€‚é¦–å…ˆï¼Œè®¤çŸ¥å¯å‘çš„æ„Ÿè§‰è®°å¿†é€šè¿‡è½»é‡çº§å‹ç¼©å¿«é€Ÿè¿‡æ»¤æ— å…³ä¿¡æ¯å¹¶æŒ‰ä¸»é¢˜åˆ†ç»„ä¿¡æ¯ã€‚å…¶æ¬¡ï¼Œä¸»é¢˜æ„ŸçŸ¥çš„çŸ­æœŸè®°å¿†å·©å›ºè¿™äº›åŸºäºä¸»é¢˜çš„åˆ†ç»„ï¼Œç»„ç»‡å’Œæ€»ç»“å†…å®¹ä»¥è¿›è¡Œæ›´ç»“æ„åŒ–çš„è®¿é—®ã€‚æœ€åï¼Œå…·æœ‰ç¡çœ æ—¶æ›´æ–°çš„é•¿æœŸè®°å¿†é‡‡ç”¨ç¦»çº¿ç¨‹åºï¼Œå°†å·©å›ºä¸åœ¨çº¿æ¨ç†è§£è€¦ã€‚åœ¨LongMemEvalä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLightMemåœ¨å‡†ç¡®æ€§æ–¹é¢ä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼ˆæé«˜äº†é«˜è¾¾10.9ï¼…ï¼‰ï¼ŒåŒæ—¶å‡å°‘äº†ä»¤ç‰Œä½¿ç”¨ï¼ˆé«˜è¾¾117å€ï¼‰ã€APIè°ƒç”¨ï¼ˆé«˜è¾¾159å€ï¼‰ï¼Œå¹¶é™ä½äº†è¿è¡Œæ—¶é—´ï¼ˆè¶…è¿‡12å€ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨å¤æ‚ç¯å¢ƒä¸­éš¾ä»¥åˆ©ç”¨å†å²äº¤äº’ä¿¡æ¯ã€‚</li>
<li>å†…å­˜ç³»ç»Ÿçš„å¼•å…¥å¸®åŠ©LLMså¤„ç†æŒä¹…æ€§ä¿¡æ¯ã€‚</li>
<li>ç°æœ‰å†…å­˜ç³»ç»Ÿå­˜åœ¨æ—¶é—´å’Œè®¡ç®—å¼€é”€ã€‚</li>
<li>LightMemæ˜¯ä¸€ä¸ªæ–°çš„å†…å­˜ç³»ç»Ÿï¼Œåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´è¾¾åˆ°å¹³è¡¡ã€‚</li>
<li>LightMemå—åˆ°äººç±»è®°å¿†æ¨¡å‹çš„å¯å‘ï¼Œåˆ†ä¸ºæ„Ÿè§‰è®°å¿†ã€çŸ­æœŸè®°å¿†å’Œé•¿æœŸè®°å¿†ä¸‰ä¸ªé˜¶æ®µã€‚</li>
<li>LightMemé€šè¿‡å‡å°‘ä»¤ç‰Œä½¿ç”¨ã€APIè°ƒç”¨å’Œè¿è¡Œæ—¶ï¼Œæé«˜äº†æ€§èƒ½ã€‚</li>
<li>LightMemåœ¨å‡†ç¡®æ€§æ–¹é¢ä¼˜äºå…¶ä»–å¼ºå¤§çš„åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18866">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e0fee29d8ecf31eaaadb8e348cb9f46f~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159584&auth_key=1761159584-0-0-71d88896c3c0129e35fa0abcf01863cc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-afe6072340ad8831f60a537bf6a71227~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159591&auth_key=1761159591-0-0-b55987d4f7669fb9ae8c99b48404315f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cb7ce29fcaa1a9442235a0e3058be492~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159598&auth_key=1761159598-0-0-dc259b481bd0288b858c06f25eb47f2c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ddf17fbdb94439eec1725e15cc97a54d~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159605&auth_key=1761159605-0-0-3e361ae33b1f368c03f2f26cd0dbfd52&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-960298b222f532b2c65e7d69c3b1581e~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159612&auth_key=1761159612-0-0-24a77d0a8a06bf1f3165cc3813a6362e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="EffiReasonTrans-RL-Optimized-Reasoning-for-Code-Translation"><a href="#EffiReasonTrans-RL-Optimized-Reasoning-for-Code-Translation" class="headerlink" title="EffiReasonTrans: RL-Optimized Reasoning for Code Translation"></a>EffiReasonTrans: RL-Optimized Reasoning for Code Translation</h2><p><strong>Authors:Yanlin Wang, Rongyi Ou, Yanli Wang, Mingwei Liu, Jiachi Chen, Ensheng Shi, Xilin Liu, Yuchi Ma, Zibin Zheng</strong></p>
<p>Code translation is a crucial task in software development and maintenance. While recent advancements in large language models (LLMs) have improved automated code translation accuracy, these gains often come at the cost of increased inference latency, hindering real-world development workflows that involve human-in-the-loop inspection. To address this trade-off, we propose EffiReasonTrans, a training framework designed to improve translation accuracy while balancing inference latency. We first construct a high-quality reasoning-augmented dataset by prompting a stronger language model, DeepSeek-R1, to generate intermediate reasoning and target translations. Each (source code, reasoning, target code) triplet undergoes automated syntax and functionality checks to ensure reliability. Based on this dataset, we employ a two-stage training strategy: supervised fine-tuning on reasoning-augmented samples, followed by reinforcement learning to further enhance accuracy and balance inference latency. We evaluate EffiReasonTrans on six translation pairs. Experimental results show that it consistently improves translation accuracy (up to +49.2% CA and +27.8% CodeBLEU compared to the base model) while reducing the number of generated tokens (up to -19.3%) and lowering inference latency in most cases (up to -29.0%). Ablation studies further confirm the complementary benefits of the two-stage training framework. Additionally, EffiReasonTrans demonstrates improved translation accuracy when integrated into agent-based frameworks. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/DeepSoftwareAnalytics/EffiReasonTrans">https://github.com/DeepSoftwareAnalytics/EffiReasonTrans</a>. </p>
<blockquote>
<p>ä»£ç ç¿»è¯‘æ˜¯è½¯ä»¶å¼€å‘å’Œç»´æŠ¤ä¸­çš„ä¸€é¡¹é‡è¦ä»»åŠ¡ã€‚è™½ç„¶æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æé«˜äº†è‡ªåŠ¨ä»£ç ç¿»è¯‘çš„å‡†ç¡®åº¦ï¼Œä½†è¿™äº›è¿›æ­¥å¾€å¾€ä¼šå¯¼è‡´æ¨ç†å»¶è¿Ÿå¢åŠ ï¼Œé˜»ç¢äº†æ¶‰åŠäººç±»å¾ªç¯æ£€æŸ¥çš„å®é™…å¼€å‘å·¥ä½œæµç¨‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æƒè¡¡é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†EffiReasonTransï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æé«˜ç¿»è¯‘å‡†ç¡®åº¦åŒæ—¶å¹³è¡¡æ¨ç†å»¶è¿Ÿçš„è®­ç»ƒæ¡†æ¶ã€‚æˆ‘ä»¬é¦–å…ˆé€šè¿‡æç¤ºæ›´å¼ºå¤§çš„è¯­è¨€æ¨¡å‹DeepSeek-R1æ¥æ„å»ºé«˜è´¨é‡æ¨ç†å¢å¼ºæ•°æ®é›†ï¼Œç”Ÿæˆä¸­é—´æ¨ç†å’Œç›®æ ‡ç¿»è¯‘ã€‚æ¯ä¸ªï¼ˆæºä»£ç ã€æ¨ç†ã€ç›®æ ‡ä»£ç ï¼‰ä¸‰å…ƒç»„éƒ½ç»è¿‡è‡ªåŠ¨åŒ–çš„è¯­æ³•å’ŒåŠŸèƒ½æ€§æ£€æŸ¥ï¼Œä»¥ç¡®ä¿å…¶å¯é æ€§ã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šé¦–å…ˆåœ¨æ¨ç†å¢å¼ºæ ·æœ¬ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œç„¶åé‡‡ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥æé«˜å‡†ç¡®æ€§å’Œå¹³è¡¡æ¨ç†å»¶è¿Ÿã€‚æˆ‘ä»¬åœ¨å…­ä¸ªç¿»è¯‘å¯¹ä¸Šè¯„ä¼°äº†EffiReasonTransã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå®ƒæŒç»­æé«˜äº†ç¿»è¯‘å‡†ç¡®åº¦ï¼ˆä¸åŸºç¡€æ¨¡å‹ç›¸æ¯”ï¼Œæœ€é«˜æå‡äº†+49.2% CAå’Œ+27.8% CodeBLEUï¼‰ï¼ŒåŒæ—¶å‡å°‘äº†ç”Ÿæˆçš„ä»¤ç‰Œæ•°é‡ï¼ˆæœ€å¤šå‡å°‘-19.3%ï¼‰ï¼Œå¹¶åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹é™ä½äº†æ¨ç†å»¶è¿Ÿï¼ˆæœ€å¤šé™ä½-29.0%ï¼‰ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶çš„äº’è¡¥ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œå½“é›†æˆåˆ°åŸºäºä»£ç†çš„æ¡†æ¶ä¸­æ—¶ï¼ŒEffiReasonTransæ˜¾ç¤ºå‡ºæ›´é«˜çš„ç¿»è¯‘å‡†ç¡®åº¦ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/DeepSoftwareAnalytics/EffiReasonTrans%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/DeepSoftwareAnalytics/EffiReasonTransä¸Šè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18863v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä»£ç ç¿»è¯‘åœ¨è½¯ä»¶å¼€å‘å’Œç»´æŠ¤ä¸­çš„é‡è¦æ€§ï¼Œä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨ä»£ç ç¿»è¯‘æ–¹é¢çš„æœ€æ–°è¿›å±•ã€‚é’ˆå¯¹ç°æœ‰æŠ€æœ¯ä¸­æ¨ç†ç¿»è¯‘å‡†ç¡®æ€§å’Œæ¨ç†å»¶è¿Ÿä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ¡†æ¶EffiReasonTransã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºé«˜è´¨é‡æ¨ç†å¢å¼ºæ•°æ®é›†å’Œé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨æé«˜ç¿»è¯‘å‡†ç¡®æ€§å¹¶å¹³è¡¡æ¨ç†å»¶è¿Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEffiReasonTransåœ¨ç¿»è¯‘å‡†ç¡®æ€§æ–¹é¢æœ‰æ˜¾è‘—æ”¹è¿›ï¼ŒåŒæ—¶å‡å°‘äº†ç”Ÿæˆçš„ä»¤ç‰Œæ•°é‡å’Œé™ä½äº†æ¨ç†å»¶è¿Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç ç¿»è¯‘åœ¨è½¯ä»¶å¼€å‘å’Œç»´æŠ¤ä¸­æ‰®æ¼”å…³é”®è§’è‰²ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨ä»£ç ç¿»è¯‘æ–¹é¢çš„è¿›å±•å¸¦æ¥äº†å‡†ç¡®æ€§çš„æé«˜ï¼Œä½†æ¨ç†å»¶è¿Ÿé—®é¢˜å½±å“äº†å®é™…åº”ç”¨ã€‚</li>
<li>EffiReasonTransæ˜¯ä¸€ä¸ªæ—¨åœ¨æé«˜ç¿»è¯‘å‡†ç¡®æ€§å¹¶å¹³è¡¡æ¨ç†å»¶è¿Ÿçš„è®­ç»ƒæ¡†æ¶ã€‚</li>
<li>EffiReasonTransé€šè¿‡æ„å»ºé«˜è´¨é‡æ¨ç†å¢å¼ºæ•°æ®é›†å’Œé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥å®ç°æ€§èƒ½æå‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒEffiReasonTransåœ¨ç¿»è¯‘å‡†ç¡®æ€§æ–¹é¢æœ‰æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶é™ä½äº†ç”Ÿæˆçš„ä»¤ç‰Œæ•°é‡å’Œæ¨ç†å»¶è¿Ÿã€‚</li>
<li>æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†EffiReasonTransä¸­ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18863">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d5235231abdbeddc9a82831af05ca48a~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159619&auth_key=1761159619-0-0-24af06b29f191c7e73aa4a09b66e0783&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1d61a9341027fb536d47a5b680d0cf7e~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159626&auth_key=1761159626-0-0-7888db28fd8d816ed72ad884792b0d99&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6240e050e5096a9aaebb15a4d20aedd5~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159632&auth_key=1761159632-0-0-7f685cd6be5a11239cc2a42a900b1be3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-37d2e3e2372a453f6d00993dccea94d6~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159639&auth_key=1761159639-0-0-c659285c152e4598d2b8fa093db54aa0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-89cd0dfe8112f58520d1992d1c6b1105~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159646&auth_key=1761159646-0-0-eb7b2b4b2ff1d56d7843a83e58b3bd6c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e99db2ca6546fd07fb763f7ca0d794ae~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159652&auth_key=1761159652-0-0-509e8bfe21fe02c042a391124f89871d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-55fbdf2ad4c0d7623a08559f2d95789d~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159659&auth_key=1761159659-0-0-c478016150f10e3c74474dc2e906b984&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Streamlining-Acceptance-Test-Generation-for-Mobile-Applications-Through-Large-Language-Models-An-Industrial-Case-Study"><a href="#Streamlining-Acceptance-Test-Generation-for-Mobile-Applications-Through-Large-Language-Models-An-Industrial-Case-Study" class="headerlink" title="Streamlining Acceptance Test Generation for Mobile Applications Through   Large Language Models: An Industrial Case Study"></a>Streamlining Acceptance Test Generation for Mobile Applications Through   Large Language Models: An Industrial Case Study</h2><p><strong>Authors:Pedro LuÃ­s Fonseca, Bruno Lima, JoÃ£o Pascoal Faria</strong></p>
<p>Mobile acceptance testing remains a bottleneck in modern software development, particularly for cross-platform mobile development using frameworks like Flutter. While developers increasingly rely on automated testing tools, creating and maintaining acceptance test artifacts still demands significant manual effort. To help tackle this issue, we introduce AToMIC, an automated framework leveraging specialized Large Language Models to generate Gherkin scenarios, Page Objects, and executable UI test scripts directly from requirements (JIRA tickets) and recent code changes. Applied to BMWâ€™s MyBMW app, covering 13 real-world issues in a 170+ screen codebase, AToMIC produced executable test artifacts in under five minutes per feature on standard hardware. The generated artifacts were of high quality: 93.3% of Gherkin scenarios were syntactically correct upon generation, 78.8% of PageObjects ran without manual edits, and 100% of generated UI tests executed successfully. In a survey, all practitioners reported time savings (often a full developer-day per feature) and strong confidence in adopting the approach. These results confirm AToMIC as a scalable, practical solution for streamlining acceptance test creation and maintenance in industrial mobile projects. </p>
<blockquote>
<p>ç§»åŠ¨éªŒæ”¶æµ‹è¯•ä»æ˜¯ç°ä»£è½¯ä»¶å¼€å‘ä¸­çš„ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨Flutterç­‰æ¡†æ¶è¿›è¡Œè·¨å¹³å°ç§»åŠ¨å¼€å‘æ—¶ã€‚è™½ç„¶å¼€å‘è€…è¶Šæ¥è¶Šä¾èµ–è‡ªåŠ¨åŒ–æµ‹è¯•å·¥å…·ï¼Œä½†åˆ›å»ºå’Œç»´æŠ¤éªŒæ”¶æµ‹è¯•å·¥ä»¶ä»ç„¶éœ€è¦å·¨å¤§çš„æ‰‹åŠ¨åŠªåŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†AToMICï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨ä¸“ä¸šçš„å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨åŒ–ç”ŸæˆGherkinåœºæ™¯ã€é¡µé¢å¯¹è±¡å’Œå¯æ‰§è¡ŒUIæµ‹è¯•è„šæœ¬çš„æ¡†æ¶ï¼Œè¿™äº›è„šæœ¬å¯ä»¥ç›´æ¥ä»éœ€æ±‚ï¼ˆJIRAç¥¨æ®ï¼‰å’Œæœ€è¿‘çš„ä»£ç æ›´æ”¹ä¸­ç”Ÿæˆã€‚åœ¨å®é©¬çš„MyBMWåº”ç”¨ä¸Šåº”ç”¨æ­¤æ¡†æ¶ï¼Œè¦†ç›–äº†ä¸€ä¸ªæ‹¥æœ‰è¶…è¿‡170ä¸ªå±å¹•ä»£ç åº“çš„13ä¸ªçœŸå®é—®é¢˜ï¼ŒAToMICåœ¨æ ‡å‡†ç¡¬ä»¶ä¸Šæ¯ä¸ªåŠŸèƒ½ç”Ÿæˆå¯æ‰§è¡Œæµ‹è¯•å·¥ä»¶çš„æ—¶é—´ä¸åˆ°äº”åˆ†é’Ÿã€‚ç”Ÿæˆçš„å·¥ä»¶è´¨é‡å¾ˆé«˜ï¼šç”Ÿæˆçš„Gherkinåœºæ™¯ä¸­è¯­æ³•æ­£ç¡®ç‡é«˜è¾¾93.3%ï¼Œæ— éœ€æ‰‹åŠ¨ç¼–è¾‘å³å¯è¿è¡Œçš„é¡µé¢å¯¹è±¡å 78.8%ï¼Œç”Ÿæˆçš„UIæµ‹è¯•å…¨éƒ¨æˆåŠŸæ‰§è¡Œã€‚åœ¨è°ƒæŸ¥ä¸­ï¼Œæ‰€æœ‰ä»ä¸šè€…éƒ½æŠ¥å‘Šäº†æ—¶é—´èŠ‚çœï¼ˆé€šå¸¸æ¯ä¸ªåŠŸèƒ½å¯ä»¥èŠ‚çœä¸€æ•´å¤©çš„å¼€å‘æ—¶é—´ï¼‰ä»¥åŠå¯¹é‡‡ç”¨è¿™ç§æ–¹æ³•çš„å¼ºçƒˆä¿¡å¿ƒã€‚è¿™äº›ç»“æœè¯å®AToMICæ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„å®é™…è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥ç®€åŒ–å·¥ä¸šç§»åŠ¨é¡¹ç›®çš„éªŒæ”¶æµ‹è¯•åˆ›å»ºå’Œç»´æŠ¤å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18861v1">PDF</a> </p>
<p><strong>Summary</strong><br>è‡ªåŠ¨åŒ–æ¡†æ¶AToMICï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä»éœ€æ±‚ï¼ˆå¦‚JIRAç¥¨æ®ï¼‰å’Œæœ€æ–°ä»£ç æ›´æ”¹ä¸­ç›´æ¥ç”ŸæˆGherkinåœºæ™¯ã€é¡µé¢å¯¹è±¡å’Œå¯æ‰§è¡Œçš„UIæµ‹è¯•è„šæœ¬ï¼Œè§£å†³äº†ç§»åŠ¨éªŒæ”¶æµ‹è¯•åœ¨ç°ä»£è½¯ä»¶å¼€å‘ä¸­çš„ç“¶é¢ˆé—®é¢˜ã€‚åœ¨BMWçš„MyBMWåº”ç”¨ç¨‹åºä¸Šåº”ç”¨ï¼Œè¦†ç›–170å¤šä¸ªå±å¹•ä»£ç åº“ä¸­çš„13ä¸ªç°å®é—®é¢˜ï¼ŒAToMICåœ¨æ ‡å‡†ç¡¬ä»¶ä¸Šäºäº”åˆ†é’Ÿå†…ç”Ÿæˆå¯æ‰§è¡Œæµ‹è¯•æˆæœã€‚ç”Ÿæˆç‰©è´¨é‡é«˜ï¼šç”Ÿæˆçš„Gherkinåœºæ™¯è¯­æ³•æ­£ç¡®ç‡é«˜è¾¾ç™¾åˆ†ä¹‹ä¹åä¸‰ç‚¹ä¸‰ï¼Œç™¾åˆ†ä¹‹ä¸ƒåå…«ç‚¹å…«çš„é¡µé¢å¯¹è±¡æ— éœ€æ‰‹åŠ¨ç¼–è¾‘å³å¯è¿è¡Œï¼Œç™¾åˆ†ä¹‹ç™¾çš„UIæµ‹è¯•å¯æˆåŠŸæ‰§è¡Œã€‚è°ƒæŸ¥ç»“æœè¯å®ï¼Œä½¿ç”¨AToMICå¯èŠ‚çœæ—¶é—´ï¼ˆæ¯ä¸ªåŠŸèƒ½é€šå¸¸å¯èŠ‚çœå¼€å‘äººå‘˜ä¸€å¤©çš„æ—¶é—´ï¼‰ï¼Œå¹¶ä¸”å®è·µè€…å¯¹è¯¥æ–¹æ³•å……æ»¡ä¿¡å¿ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AToMICæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç§»åŠ¨éªŒæ”¶æµ‹è¯•åœ¨ç°ä»£è½¯ä»¶å¼€å‘ä¸­çš„ç“¶é¢ˆé—®é¢˜ã€‚</li>
<li>AToMICåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä»éœ€æ±‚å’Œæœ€æ–°ä»£ç æ›´æ”¹ä¸­ç”Ÿæˆæµ‹è¯•è„šæœ¬ã€‚</li>
<li>åœ¨BMWçš„MyBMWåº”ç”¨ç¨‹åºä¸Šåº”ç”¨ï¼ŒAToMICèƒ½å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡çš„å¯æ‰§è¡Œæµ‹è¯•æˆæœã€‚</li>
<li>AToMICç”Ÿæˆçš„æµ‹è¯•æˆæœåŒ…æ‹¬Gherkinåœºæ™¯ã€é¡µé¢å¯¹è±¡å’ŒUIæµ‹è¯•è„šæœ¬ã€‚</li>
<li>AToMICç”Ÿæˆçš„æµ‹è¯•æˆæœè´¨é‡é«˜ï¼Œå…¶ä¸­Gherkinåœºæ™¯è¯­æ³•æ­£ç¡®ç‡é«˜è¾¾ç™¾åˆ†ä¹‹ä¹åä¸‰ç‚¹ä¸‰ã€‚</li>
<li>å®è·µè€…é€šè¿‡ä½¿ç”¨AToMICèŠ‚çœäº†å¤§é‡æ—¶é—´ï¼ˆæ¯ä¸ªåŠŸèƒ½é€šå¸¸å¯èŠ‚çœå¼€å‘äººå‘˜ä¸€å¤©çš„æ—¶é—´ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18861">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1ef543b761b22e6741b1ef682bc618e9~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159666&auth_key=1761159666-0-0-2e82669b02197904b822307dce96875b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bf752872b1fe7604058ebeee1cd74971~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159673&auth_key=1761159673-0-0-a8ee483cb65f0050ca08a243c89111c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-182155589ef267ecbf08358826bdd01b~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159679&auth_key=1761159679-0-0-b63a544c74244f6e0c4bec1fd54ce795&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1643f5a7e7b5b0c9f2ad9dfacdc9c151~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159685&auth_key=1761159685-0-0-e76dfe9165a0cf9a87b613beabb8c350&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6739280474e3bf1c98aad5958724f1c2~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159692&auth_key=1761159692-0-0-376957804decccba21f8c85c3063f3d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2f42126cca162800eff7b594d396808~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159698&auth_key=1761159698-0-0-754916c843a8c2667ea3c4cb28eb824e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5695de331f649907001415a79f6885f1~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159705&auth_key=1761159705-0-0-767f7e8aff60887d44519821ba2f3770&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Towards-Faithful-and-Controllable-Personalization-via-Critique-Post-Edit-Reinforcement-Learning"><a href="#Towards-Faithful-and-Controllable-Personalization-via-Critique-Post-Edit-Reinforcement-Learning" class="headerlink" title="Towards Faithful and Controllable Personalization via Critique-Post-Edit   Reinforcement Learning"></a>Towards Faithful and Controllable Personalization via Critique-Post-Edit   Reinforcement Learning</h2><p><strong>Authors:Chenghao Zhu, Meiling Tao, Tiannan Wang, Dongyi Ding, Yuchen Eleanor Jiang, Wangchunshu Zhou</strong></p>
<p>Faithfully personalizing large language models (LLMs) to align with individual user preferences is a critical but challenging task. While supervised fine-tuning (SFT) quickly reaches a performance plateau, standard reinforcement learning from human feedback (RLHF) also struggles with the nuances of personalization. Scalar-based reward models are prone to reward hacking which leads to verbose and superficially personalized responses. To address these limitations, we propose Critique-Post-Edit, a robust reinforcement learning framework that enables more faithful and controllable personalization. Our framework integrates two key components: (1) a Personalized Generative Reward Model (GRM) that provides multi-dimensional scores and textual critiques to resist reward hacking, and (2) a Critique-Post-Edit mechanism where the policy model revises its own outputs based on these critiques for more targeted and efficient learning. Under a rigorous length-controlled evaluation, our method substantially outperforms standard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an average 11% win-rate improvement, and personalized Qwen2.5-14B model surpasses the performance of GPT-4.1. These results demonstrate a practical path to faithful, efficient, and controllable personalization. </p>
<blockquote>
<p>å¯¹ç”¨æˆ·åå¥½è¿›è¡Œå¿ å®çš„ä¸ªæ€§åŒ–è°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯ä¸€é¡¹è‡³å…³é‡è¦çš„ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚è™½ç„¶ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¯ä»¥å¿«é€Ÿè¾¾åˆ°æ€§èƒ½ç“¶é¢ˆï¼Œä½†æ ‡å‡†çš„äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰åœ¨ä¸ªæ€§åŒ–çš„ç»†å¾®å·®åˆ«æ–¹é¢ä¹Ÿé¢ä¸´ç€å›°éš¾ã€‚åŸºäºæ ‡é‡çš„å¥–åŠ±æ¨¡å‹å®¹æ˜“å¥–åŠ±ä½œå¼Šï¼Œå¯¼è‡´å†—é•¿å’Œè¡¨é¢ä¸Šçš„ä¸ªæ€§åŒ–å›åº”ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†æ‰¹åˆ¤åç¼–è¾‘ï¼ˆCritique-Post-Editï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç¨³å¥çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°æ›´å¿ å®å’Œå¯æ§çš„ä¸ªæ€§åŒ–ã€‚æˆ‘ä»¬çš„æ¡†æ¶é›†æˆäº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰ä¸ªæ€§åŒ–ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆGRMï¼‰ï¼Œæä¾›å¤šç»´åˆ†æ•°å’Œæ–‡æœ¬è¯„ä»·æ¥æŠµæŠ—å¥–åŠ±ä½œå¼Šï¼›ï¼ˆ2ï¼‰æ‰¹åˆ¤åç¼–è¾‘æœºåˆ¶ï¼Œç­–ç•¥æ¨¡å‹æ ¹æ®è¿™äº›è¯„ä»·ä¿®è®¢è‡ªå·±çš„è¾“å‡ºï¼Œä»¥æ›´æœ‰é’ˆå¯¹æ€§å’Œé«˜æ•ˆçš„å­¦ä¹ ã€‚åœ¨ä¸¥æ ¼æ§åˆ¶çš„é•¿åº¦è¯„ä¼°ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸ªæ€§åŒ–åŸºå‡†æµ‹è¯•ä¸Šå¤§å¤§ä¼˜äºæ ‡å‡†PPOã€‚ä¸ªæ€§åŒ–Qwen2.5-7Bçš„å¹³å‡èƒœç‡æé«˜äº†11%ï¼Œä¸ªæ€§åŒ–Qwen2.5-14Bæ¨¡å‹è¶…è¶Šäº†GPT-4.1çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœå±•ç¤ºäº†ä¸€æ¡å®ç°å¿ å®ã€é«˜æ•ˆå’Œå¯æ§ä¸ªæ€§åŒ–çš„å®ç”¨é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18849v1">PDF</a> work in progress</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸ªäººåŒ–å¯¹é½ç”¨æˆ·åå¥½æ˜¯ä¸€é¡¹é‡è¦ä¸”å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¾ˆå¿«è¾¾åˆ°æ€§èƒ½ç“¶é¢ˆï¼Œè€Œæ ‡å‡†çš„äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰åœ¨ä¸ªæ€§åŒ–ç»†èŠ‚æ–¹é¢ä¹Ÿå­˜åœ¨å›°éš¾ã€‚åŸºäºæ ‡é‡çš„å¥–åŠ±æ¨¡å‹å®¹æ˜“å¥–åŠ±ä½œå¼Šï¼Œå¯¼è‡´å†—é•¿å’Œè¡¨é¢ä¸Šçš„ä¸ªæ€§åŒ–å›åº”ã€‚ä¸ºè§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†Critique-Post-Editè¿™ä¸€ç¨³å¥çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ç°æ›´å¿ å®å’Œå¯æ§çš„ä¸ªäººåŒ–ã€‚è¯¥æ¡†æ¶æ•´åˆäº†ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰ä¸ªæ€§åŒ–ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆGRMï¼‰ï¼Œæä¾›å¤šç»´åˆ†æ•°å’Œæ–‡æœ¬è¯„è®ºä»¥æŠµæŠ—å¥–åŠ±ä½œå¼Šï¼›ï¼ˆ2ï¼‰Critique-Post-Editæœºåˆ¶ï¼Œæ”¿ç­–æ¨¡å‹æ ¹æ®è¿™äº›è¯„è®ºä¿®è®¢å…¶è‡ªå·±çš„è¾“å‡ºï¼Œä»¥å®ç°æ›´æœ‰é’ˆå¯¹æ€§å’Œé«˜æ•ˆçš„å­¦ä¹ ã€‚åœ¨ä¸¥æ ¼æ§åˆ¶çš„è¯„ä¼°ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸ªäººåŒ–åŸºå‡†æµ‹è¯•ä¸Šå¤§å¤§ä¼˜äºæ ‡å‡†PPOã€‚ä¸ªæ€§åŒ–Qwen2.5-7Bçš„å¹³å‡èƒœç‡æé«˜11%ï¼Œä¸ªæ€§åŒ–Qwen2.5-14Bæ¨¡å‹è¶…è¶ŠGPT-4.1çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœå±•ç¤ºäº†ä¸€æ¡å®ç°å¿ å®ã€é«˜æ•ˆå’Œå¯æ§ä¸ªäººåŒ–çš„å®ç”¨é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸ªäººåŒ–å¯¹äºå¯¹é½ç”¨æˆ·åå¥½è‡³å…³é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œæ ‡å‡†çš„å¼ºåŒ–å­¦ä¹ åœ¨ä¸ªäººåŒ–æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>åŸºäºæ ‡é‡çš„å¥–åŠ±æ¨¡å‹å®¹æ˜“å¥–åŠ±ä½œå¼Šï¼Œå¯¼è‡´å›åº”è¡¨é¢åŒ–å’Œå†—é•¿ã€‚</li>
<li>Critique-Post-Editæ¡†æ¶é€šè¿‡æ•´åˆä¸ªæ€§åŒ–ç”Ÿæˆå¥–åŠ±æ¨¡å‹å’ŒCritique-Post-Editæœºåˆ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>ä¸ªæ€§åŒ–ç”Ÿæˆå¥–åŠ±æ¨¡å‹æä¾›å¤šç»´åˆ†æ•°å’Œæ–‡æœ¬è¯„è®ºï¼ŒæŠµæŠ—å¥–åŠ±ä½œå¼Šã€‚</li>
<li>Critique-Post-Editæœºåˆ¶å…è®¸æ”¿ç­–æ¨¡å‹æ ¹æ®è¯„è®ºä¿®è®¢è¾“å‡ºï¼Œå®ç°æ›´é«˜æ•ˆå’Œæœ‰é’ˆå¯¹æ€§çš„å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18849">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ece372b6b609f8488d302aa7935e483a~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159712&auth_key=1761159712-0-0-76c141dbf5848945c0476e056d45de05&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a29656658d78e111642e1e989643ddb3~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159719&auth_key=1761159719-0-0-2107e1fc8edeba1f8999a26596c78e22&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cbe70530880fd980e5e16caaaeb1b738~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159726&auth_key=1761159726-0-0-4a97b6ad240a291c964b3c359bac3234&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-94502518bda6eaf33bc6c8ee9bb1b2bd~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159732&auth_key=1761159732-0-0-d9b2532132e9440ceb0ff2d4d00c362e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MTraining-Distributed-Dynamic-Sparse-Attention-for-Efficient-Ultra-Long-Context-Training"><a href="#MTraining-Distributed-Dynamic-Sparse-Attention-for-Efficient-Ultra-Long-Context-Training" class="headerlink" title="MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long   Context Training"></a>MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long   Context Training</h2><p><strong>Authors:Wenxuan Li, Chengruidong Zhang, Huiqiang Jiang, Yucheng Li, Yuqing Yang, Lili Qiu</strong></p>
<p>The adoption of long context windows has become a standard feature in Large Language Models (LLMs), as extended contexts significantly enhance their capacity for complex reasoning and broaden their applicability across diverse scenarios. Dynamic sparse attention is a promising approach for reducing the computational cost of long-context. However, efficiently training LLMs with dynamic sparse attention on ultra-long contexts-especially in distributed settings-remains a significant challenge, due in large part to worker- and step-level imbalance. This paper introduces MTraining, a novel distributed methodology leveraging dynamic sparse attention to enable efficient training for LLMs with ultra-long contexts. Specifically, MTraining integrates three key components: a dynamic sparse training pattern, balanced sparse ring attention, and hierarchical sparse ring attention. These components are designed to synergistically address the computational imbalance and communication overheads inherent in dynamic sparse attention mechanisms during the training of models with extensive context lengths. We demonstrate the efficacy of MTraining by training Qwen2.5-3B, successfully expanding its context window from 32K to 512K tokens on a cluster of 32 A100 GPUs. Our evaluations on a comprehensive suite of downstream tasks, including RULER, PG-19, InfiniteBench, and Needle In A Haystack, reveal that MTraining achieves up to a 6x higher training throughput while preserving model accuracy. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/microsoft/MInference/tree/main/MTraining">https://github.com/microsoft/MInference/tree/main/MTraining</a>. </p>
<blockquote>
<p>é•¿ä¸Šä¸‹æ–‡çª—å£å·²ç»æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ‡å‡†åŠŸèƒ½ï¼Œæ‰©å±•çš„ä¸Šä¸‹æ–‡æ˜¾è‘—å¢å¼ºäº†å…¶å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œå¹¶æ‰©å¤§äº†å…¶åœ¨ä¸åŒåœºæ™¯ä¸­çš„åº”ç”¨èŒƒå›´ã€‚åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›æ˜¯ä¸€ç§å¾ˆæœ‰å‰é€”çš„æ–¹æ³•ï¼Œå¯ä»¥é™ä½é•¿ä¸Šä¸‹æ–‡çš„è®¡ç®—æˆæœ¬ã€‚ç„¶è€Œï¼Œåœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­ï¼Œä½¿ç”¨åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›æœ‰æ•ˆåœ°è®­ç»ƒå…·æœ‰è¶…é•¿ä¸Šä¸‹æ–‡çš„LLMä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå·¥ä½œçº§åˆ«å’Œæ­¥éª¤çº§åˆ«çš„å¤±è¡¡ã€‚æœ¬æ–‡ä»‹ç»äº†MTrainingï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åˆ†å¸ƒå¼æ–¹æ³•ï¼Œåˆ©ç”¨åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›å®ç°å…·æœ‰è¶…é•¿ä¸Šä¸‹æ–‡çš„LLMçš„é«˜æ•ˆè®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼ŒMTrainingé›†æˆäº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šåŠ¨æ€ç¨€ç–è®­ç»ƒæ¨¡å¼ã€å¹³è¡¡ç¨€ç–ç¯å½¢æ³¨æ„åŠ›å’Œåˆ†å±‚ç¨€ç–ç¯å½¢æ³¨æ„åŠ›ã€‚è¿™äº›ç»„ä»¶è¢«ååŒè®¾è®¡ä»¥è§£å†³åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶åœ¨å…·æœ‰å¹¿æ³›ä¸Šä¸‹æ–‡é•¿åº¦çš„æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å›ºæœ‰çš„è®¡ç®—ä¸å¹³è¡¡å’Œé€šä¿¡å¼€é”€é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡è®­ç»ƒQwen2.5-3Bæ¥å±•ç¤ºMTrainingçš„æœ‰æ•ˆæ€§ï¼ŒæˆåŠŸå°†å…¶ä¸Šä¸‹æ–‡çª—å£ä»32Kæ‰©å±•åˆ°512Kä»¤ç‰Œé›†ç¾¤çš„32ä¸ªA100 GPUä¸Šã€‚æˆ‘ä»¬åœ¨ä¸€ç³»åˆ—ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¯„ä¼°ï¼ŒåŒ…æ‹¬RULERã€PG-19ã€InfiniteBenchå’ŒHaystackä¸­çš„é’ˆå¯»æ‰¾ä»»åŠ¡ï¼Œæ­ç¤ºäº†MTrainingåœ¨æé«˜è®­ç»ƒæ•ˆç‡çš„åŒæ—¶ä¿æŒæ¨¡å‹å‡†ç¡®æ€§ï¼Œå®ç°äº†é«˜è¾¾6å€çš„åŸ¹è®­ååé‡æå‡ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/microsoft/MInference/tree/main/MTraining">https://github.com/microsoft/MInference/tree/main/MTraining</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18830v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­é‡‡ç”¨é•¿ä¸Šä¸‹æ–‡çª—å£å·²æˆä¸ºæ ‡å‡†ç‰¹æ€§ï¼Œæ‰©å±•çš„ä¸Šä¸‹æ–‡æ˜¾è‘—æé«˜äº†å…¶å¤æ‚æ¨ç†èƒ½åŠ›å¹¶æ‹“å®½äº†åº”ç”¨åœºæ™¯èŒƒå›´ã€‚åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›æ˜¯ä¸€ç§é™ä½é•¿ä¸Šä¸‹æ–‡è®¡ç®—æˆæœ¬çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œåœ¨åˆ†å¸ƒå¼è®¾ç½®ä¸­ï¼Œä½¿ç”¨åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›æœ‰æ•ˆåœ°è®­ç»ƒLLMå¤„ç†è¶…é•¿ä¸Šä¸‹æ–‡ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸»è¦ä½“ç°åœ¨å·¥ä½œçº§åˆ«å’Œæ­¥éª¤çº§åˆ«çš„å¤±è¡¡ã€‚æœ¬æ–‡æå‡ºMTrainingï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹åˆ†å¸ƒå¼æ–¹æ³•ï¼Œåˆ©ç”¨åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›å®ç°LLMçš„è¶…é•¿ä¸Šä¸‹æ–‡é«˜æ•ˆè®­ç»ƒã€‚MTrainingåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šåŠ¨æ€ç¨€ç–è®­ç»ƒæ¨¡å¼ã€å¹³è¡¡ç¨€ç–ç¯å½¢æ³¨æ„åŠ›å’Œåˆ†å±‚ç¨€ç–ç¯å½¢æ³¨æ„åŠ›ã€‚è¿™äº›ç»„ä»¶ååŒè§£å†³åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„è®¡ç®—ä¸å¹³è¡¡å’Œé€šä¿¡å¼€é”€é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹é•¿ä¸Šä¸‹æ–‡é•¿åº¦çš„æƒ…å†µã€‚é€šè¿‡åœ¨ä¸€ä¸ªåŒ…å«32ä¸ªA100 GPUçš„é›†ç¾¤ä¸Šå¯¹Qwen2.5-3Bè¿›è¡Œè®­ç»ƒï¼ŒæˆåŠŸå°†å…¶ä¸Šä¸‹æ–‡çª—å£ä»32Kæ‰©å±•åˆ°512Kä»¤ç‰Œï¼Œå±•ç¤ºäº†MTrainingçš„æœ‰æ•ˆæ€§ã€‚åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMTrainingå®ç°äº†é«˜è¾¾6å€çš„åŸ¹è®­ååé‡ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­é•¿ä¸Šä¸‹æ–‡çª—å£çš„åº”ç”¨å¢å¼ºäº†å…¶å¤æ‚æ¨ç†èƒ½åŠ›å’Œåº”ç”¨åœºæ™¯èŒƒå›´ã€‚</li>
<li>åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›æ˜¯é™ä½é•¿ä¸Šä¸‹æ–‡è®¡ç®—æˆæœ¬çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è®­ç»ƒè¶…é•¿ä¸Šä¸‹æ–‡çš„LLMé¢ä¸´å·¥ä½œçº§åˆ«å’Œæ­¥éª¤çº§åˆ«çš„æŒ‘æˆ˜ã€‚</li>
<li>MTrainingæ˜¯ä¸€ç§æ–°å‹åˆ†å¸ƒå¼æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›å®ç°LLMçš„è¶…é•¿ä¸Šä¸‹æ–‡é«˜æ•ˆè®­ç»ƒã€‚</li>
<li>MTrainingåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ä»¥ååŒè§£å†³è®¡ç®—ä¸å¹³è¡¡å’Œé€šä¿¡å¼€é”€é—®é¢˜ã€‚</li>
<li>MTrainingæˆåŠŸæ‰©å±•äº†æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£å¹¶æé«˜äº†è®­ç»ƒååé‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18830">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8f3be4a7eea520d231630c70f50b1ce1~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159740&auth_key=1761159740-0-0-1fb9f90f72a98d10be0a4bb825c7df07&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-111522dafcc22c2993782472035c8a6b~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159747&auth_key=1761159747-0-0-6d7a91cb76c496fa9ef16d9f6ec618f4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c040a440df206e074f7803a32cc562a5~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159753&auth_key=1761159753-0-0-4658d8659310d2c73aef25089a8259d1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8321e6da2ddf840ab856578742eda61a~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159760&auth_key=1761159760-0-0-fdad0d01e79f9cd355ca4059d127b79c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5bbd3c99c23dea4d80444821b05f93a4~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159766&auth_key=1761159766-0-0-10d53edd52a52f97e6d6855cf0716ae1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f6fea0c79989eac0abc1536efe6926fa~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159773&auth_key=1761159773-0-0-d8151bf4bb6969c3a61037678fac0523&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eff1c1d62e2dca8c09ddc0d4eb224aab~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159779&auth_key=1761159779-0-0-150b785a1b23dc3fae4741fca5435b2b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Fine-Tuned-Thoughts-Leveraging-Chain-of-Thought-Reasoning-for-Industrial-Asset-Health-Monitoring"><a href="#Fine-Tuned-Thoughts-Leveraging-Chain-of-Thought-Reasoning-for-Industrial-Asset-Health-Monitoring" class="headerlink" title="Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for   Industrial Asset Health Monitoring"></a>Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for   Industrial Asset Health Monitoring</h2><p><strong>Authors:Shuxin Lin, Dhaval Patel, Christodoulos Constantinides</strong></p>
<p>Small Language Models (SLMs) are becoming increasingly popular in specialized fields, such as industrial applications, due to their efficiency, lower computational requirements, and ability to be fine-tuned for domain-specific tasks, enabling accurate and cost-effective solutions. However, performing complex reasoning using SLMs in specialized fields such as Industry 4.0 remains challenging. In this paper, we propose a knowledge distillation framework for industrial asset health, which transfers reasoning capabilities via Chain-of-Thought (CoT) distillation from Large Language Models (LLMs) to smaller, more efficient models (SLMs). We discuss the advantages and the process of distilling LLMs using multi-choice question answering (MCQA) prompts to enhance reasoning and refine decision-making. We also perform in-context learning to verify the quality of the generated knowledge and benchmark the performance of fine-tuned SLMs with generated knowledge against widely used LLMs. The results show that the fine-tuned SLMs with CoT reasoning outperform the base models by a significant margin, narrowing the gap to their LLM counterparts. Our code is open-sourced at: <a target="_blank" rel="noopener" href="https://github.com/IBM/FailureSensorIQ">https://github.com/IBM/FailureSensorIQ</a>. </p>
<blockquote>
<p>å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰åœ¨å·¥ä¸šåº”ç”¨ç­‰ä¸“ä¸šé¢†åŸŸæ—¥ç›Šæµè¡Œï¼Œå› å…¶æ•ˆç‡é«˜ã€è®¡ç®—éœ€æ±‚ä½ï¼Œå¹¶ä¸”èƒ½é’ˆå¯¹ç‰¹å®šé¢†åŸŸä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œä»è€Œå®ç°ç²¾ç¡®ä¸”ç»æµå®æƒ çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œåœ¨äº§ä¸š4.0ç­‰ä¸“ä¸šé¢†åŸŸä½¿ç”¨SLMè¿›è¡Œå¤æ‚æ¨ç†ä»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé¢å‘å·¥ä¸šèµ„äº§å¥åº·çš„çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰è’¸é¦çš„æ–¹å¼ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°æ›´å°ã€æ›´æœ‰æ•ˆç‡çš„å°å‹è¯­è¨€æ¨¡å‹ä¸Šã€‚æˆ‘ä»¬è®¨è®ºäº†åœ¨å¤šé€‰é—®ç­”ï¼ˆMCQAï¼‰æç¤ºä¸‹ä½¿ç”¨LLMè¿›è¡Œè’¸é¦çš„ä¼˜åŠ¿å’Œæµç¨‹ï¼Œä»¥å¢å¼ºæ¨ç†å¹¶æ”¹è¿›å†³ç­–åˆ¶å®šã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œä»¥éªŒè¯ç”ŸæˆçŸ¥è¯†çš„è´¨é‡ï¼Œå¹¶å¯¹æ¯”ç»è¿‡å¾®è°ƒçš„å°å‹è¯­è¨€æ¨¡å‹å’Œå¹¿æ³›ä½¿ç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œç»è¿‡æ€ç»´é“¾æ¨ç†è°ƒæ•´çš„å°å‹è¯­è¨€æ¨¡å‹åœ¨æ€§èƒ½ä¸Šå¤§å¤§è¶…è¿‡äº†åŸºç¡€æ¨¡å‹ï¼Œå¹¶ç¼©å°äº†ä¸å…¶å¤§å‹è¯­è¨€æ¨¡å‹å¯¹åº”ç‰©çš„å·®è·ã€‚æˆ‘ä»¬çš„ä»£ç å·²å¼€æºåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/IBM/FailureSensorIQ%E3%80%82">https://github.com/IBM/FailureSensorIQã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18817v1">PDF</a> Accepted at EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå°è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰åœ¨ä¸“ä¸šåŒ–é¢†åŸŸå¦‚å·¥ä¸šåº”ç”¨çš„æµè¡Œè¶‹åŠ¿ï¼Œæœ¬æ–‡å°†æ¢è®¨å¦‚ä½•é€šè¿‡çŸ¥è¯†è’¸é¦æŠ€æœ¯æå‡SLMçš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé¢å‘å·¥ä¸šèµ„äº§å¥åº·çš„è’¸é¦æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›é€šè¿‡æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰è’¸é¦æŠ€æœ¯è½¬ç§»åˆ°å°å‹ã€é«˜æ•ˆçš„SLMä¸Šã€‚è¯¥ç ”ç©¶è¿˜é€šè¿‡å¤šé€‰é—®ç­”ï¼ˆMCQAï¼‰æç¤ºè¿›è¡ŒLLMè’¸é¦çš„ä¼˜åŠ¿å’Œè¿‡ç¨‹ï¼Œä»¥æå‡æ¨ç†èƒ½åŠ›å’Œä¼˜åŒ–å†³ç­–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡CoTæ¨ç†æŠ€æœ¯ç²¾ç»†è°ƒæ•´çš„SLMæ€§èƒ½æ˜¾è‘—æå‡ï¼Œæ˜¾è‘—ç¼©å°äº†ä¸LLMçš„å·®è·ã€‚ç›¸å…³ä»£ç å·²å¼€æºäºGitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å°è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰åœ¨å·¥ä¸šåº”ç”¨ç­‰ç‰¹å®šé¢†åŸŸè¡¨ç°å‡ºæ•ˆç‡ã€è®¡ç®—éœ€æ±‚ä½å’Œå¯é’ˆå¯¹ç‰¹å®šä»»åŠ¡å¾®è°ƒçš„ä¼˜åŠ¿ã€‚</li>
<li>çŸ¥è¯†è’¸é¦æ˜¯ä¸€ç§å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æŠ€æœ¯ã€‚</li>
<li>æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰è’¸é¦æ˜¯çŸ¥è¯†è’¸é¦çš„ä¸€ç§å½¢å¼ï¼Œæœ‰åŠ©äºæé«˜SLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¤šé€‰é—®ç­”ï¼ˆMCQAï¼‰æç¤ºè¿›è¡ŒLLMè’¸é¦ï¼Œæå‡æ¨ç†å’Œä¼˜åŒ–å†³ç­–è¿‡ç¨‹ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºç²¾ç»†è°ƒæ•´çš„SLMé€šè¿‡CoTæ¨ç†æŠ€æœ¯æ€§èƒ½æ˜¾è‘—æå‡ï¼Œæ˜¾è‘—ç¼©å°ä¸LLMçš„å·®è·ã€‚</li>
<li>å¼€æ”¾æºä»£ç ä»¥ä¾¿å…¶ä»–ç ”ç©¶äººå‘˜ä½¿ç”¨å’Œè¿›ä¸€æ­¥å¼€å‘ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2cdec7f605ab6b07bc1a3db005f265df~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159787&auth_key=1761159787-0-0-7ed432d314e6a5d41da198bca04f93de&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7ba01eb0b4981d54530921ccbacf8627~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159794&auth_key=1761159794-0-0-ccedc384b53a76980accdb04909968c4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-35dd2678d1ba08da42903f0a19f5b40c~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159800&auth_key=1761159800-0-0-1f6c4ecdca86f0675d11926c69edd949&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cad8a4d33c9bfeb9ab18ee7b3d858755~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159806&auth_key=1761159806-0-0-0ba3d5218fce9f29f627239b4bd29feb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-13325004a093a7ec4dc6790bcebbabae~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159812&auth_key=1761159812-0-0-ab2044807d91a8c2855e33f344b726d0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0689919410ea96bf2e1d3e9418ab2e39~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159818&auth_key=1761159818-0-0-c83dc5a45913c2f0e76faf4e492f47e4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2197c609a2b9f8f010b885e80861ae76~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159824&auth_key=1761159824-0-0-ecdefdfbd7d6d62ccab6a42e0c4367fd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Online-SFT-for-LLM-Reasoning-Surprising-Effectiveness-of-Self-Tuning-without-Rewards"><a href="#Online-SFT-for-LLM-Reasoning-Surprising-Effectiveness-of-Self-Tuning-without-Rewards" class="headerlink" title="Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning   without Rewards"></a>Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning   without Rewards</h2><p><strong>Authors:Mengqi Li, Lei Zhao, Anthony Man-Cho So, Ruoyu Sun, Xiao Li</strong></p>
<p>We present a simple, self-help online supervised finetuning (OSFT) paradigm for LLM reasoning. In this paradigm, the model generates its own responses and is immediately finetuned on this self-generated data. OSFT is a highly efficient training strategy for LLM reasoning, as it is reward-free and uses just one rollout by default. Experiment results show that OSFT achieves downstream performance on challenging mathematical reasoning tasks comparable to strong reinforcement learning with verifiable rewards (RLVR) methods such as GRPO. Our ablation study further demonstrates the efficiency and robustness of OSFT. The major mechanism of OSFT lies in facilitating the modelâ€™s own existing preference (latent knowledge) learned from pretraining, which leads to reasoning ability improvement. We believe that OSFT offers an efficient and promising alternative to more complex, reward-based training paradigms. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ElementQi/OnlineSFT">https://github.com/ElementQi/OnlineSFT</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•ã€è‡ªåŠ©çš„åœ¨çº¿ç›‘ç£å¾®è°ƒï¼ˆOSFTï¼‰èŒƒå¼ï¼Œç”¨äºLLMæ¨ç†ã€‚åœ¨æ­¤èŒƒå¼ä¸­ï¼Œæ¨¡å‹è‡ªè¡Œç”Ÿæˆç­”æ¡ˆï¼Œå¹¶ç«‹å³åœ¨æ­¤è‡ªç”Ÿæˆçš„æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚OSFTæ˜¯ä¸€ç§é’ˆå¯¹LLMæ¨ç†çš„é«˜æ•ˆè®­ç»ƒç­–ç•¥ï¼Œå› ä¸ºå®ƒä¸éœ€è¦å¥–åŠ±ï¼Œå¹¶ä¸”é»˜è®¤åªä½¿ç”¨ä¸€æ¬¡è¿è¡Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOSFTåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„ä¸‹æ¸¸æ€§èƒ½ä¸å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ–¹æ³•ï¼ˆå¦‚GRPOï¼‰ç›¸å½“ã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº†OSFTçš„æ•ˆç‡å’Œç¨³å¥æ€§ã€‚OSFTçš„ä¸»è¦æœºåˆ¶åœ¨äºä¿ƒè¿›æ¨¡å‹ä»é¢„è®­ç»ƒä¸­å­¦åˆ°çš„è‡ªèº«åå¥½ï¼ˆæ½œåœ¨çŸ¥è¯†ï¼‰ï¼Œä»è€Œæé«˜æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬ç›¸ä¿¡OSFTä¸ºæ›´å¤æ‚çš„åŸºäºå¥–åŠ±çš„è®­ç»ƒèŒƒå¼æä¾›äº†é«˜æ•ˆä¸”å‰æ™¯å¹¿é˜”çš„æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ElementQi/OnlineSFT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ElementQi/OnlineSFTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18814v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ç®€å•ã€è‡ªæˆ‘ç›‘ç£çš„åœ¨çº¿å¾®è°ƒï¼ˆOSFTï¼‰èŒƒå¼ï¼Œç”¨äºLLMæ¨ç†ã€‚è¯¥æ¨¡å‹é€šè¿‡ç”Ÿæˆè‡ªå·±çš„å“åº”å¹¶ç«‹å³åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå®ç°é«˜æ•ˆè®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOSFTåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸åŸºäºå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ–¹æ³•å¦‚GRPOç›¸å½“ã€‚å…¶æœºåˆ¶åœ¨äºä¿ƒè¿›æ¨¡å‹ä»é¢„è®­ç»ƒä¸­å­¦åˆ°çš„åå¥½ï¼ˆæ½œåœ¨çŸ¥è¯†ï¼‰ï¼Œä»è€Œæé«˜æ¨ç†èƒ½åŠ›ã€‚OSFTä¸ºå¤æ‚çš„å¥–åŠ±è®­ç»ƒèŒƒå¼æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åä¸ºOSFTçš„è‡ªæˆ‘ç›‘ç£åœ¨çº¿å¾®è°ƒèŒƒå¼ï¼Œç”¨äºLLMæ¨ç†ã€‚</li>
<li>OSFTé€šè¿‡æ¨¡å‹è‡ªæˆ‘ç”Ÿæˆå“åº”å¹¶ç«‹å³è¿›è¡Œå¾®è°ƒï¼Œå®ç°é«˜æ•ˆè®­ç»ƒã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºOSFTåœ¨æŒ‘æˆ˜æ€§æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸åŸºäºå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸å½“ã€‚</li>
<li>OSFTçš„ä¸»è¦æœºåˆ¶åœ¨äºä¿ƒè¿›æ¨¡å‹ä»é¢„è®­ç»ƒä¸­å­¦åˆ°çš„åå¥½ï¼ˆæ½œåœ¨çŸ¥è¯†ï¼‰ã€‚</li>
<li>OSFTä¸ºå¤æ‚çš„å¥–åŠ±è®­ç»ƒèŒƒå¼æä¾›äº†ä¸€ä¸ªå¯è¡Œçš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>OSFTèŒƒå¼çš„ä¼˜ç‚¹æ˜¯å¥–åŠ±å…è´¹å’Œé»˜è®¤åªéœ€ä¸€æ¬¡è¿è¡Œï¼Œæé«˜äº†æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18814">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b6d2cdf62e7ec4a17e10438e4f39f10d~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159831&auth_key=1761159831-0-0-53814ee88dd8b8ff297b4bb53eedf001&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-681c290ff58035730a47ad3a263987d0~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159838&auth_key=1761159838-0-0-25b7f8af95430c33fcbd5ff4a6a03846&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-89e7377cde71df207064c52fcbcfae84~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159845&auth_key=1761159845-0-0-99b05e8830e4521875055c68bb0e9a46&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ProCLIP-Progressive-Vision-Language-Alignment-via-LLM-based-Embedder"><a href="#ProCLIP-Progressive-Vision-Language-Alignment-via-LLM-based-Embedder" class="headerlink" title="ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder"></a>ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder</h2><p><strong>Authors:Xiaoxing Hu, Kaicheng Yang, Ziyong Feng, Qi Ming, Zonghao Guo, Xiang An, Ziyong Feng, Junchi Yan, Xue Yang</strong></p>
<p>The original CLIP text encoder is limited by a maximum input length of 77 tokens, which hampers its ability to effectively process long texts and perform fine-grained semantic understanding. In addition, the CLIP text encoder lacks support for multilingual inputs. All these limitations significantly restrict its applicability across a broader range of tasks. Recent studies have attempted to replace the CLIP text encoder with an LLM-based embedder to enhance its ability in processing long texts, multilingual understanding, and fine-grained semantic comprehension. However, because the representation spaces of LLMs and the vision-language space of CLIP are pretrained independently without alignment priors, direct alignment using contrastive learning can disrupt the intrinsic vision-language alignment in the CLIP image encoder, leading to an underutilization of the knowledge acquired during pre-training. To address this challenge, we propose ProCLIP, a curriculum learning-based progressive vision-language alignment framework to effectively align the CLIP image encoder with an LLM-based embedder. Specifically, ProCLIP first distills knowledge from CLIPâ€™s text encoder into the LLM-based embedder to leverage CLIPâ€™s rich pretrained knowledge while establishing initial alignment between the LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns the CLIP image encoder with the LLM-based embedder through image-text contrastive tuning, employing self-distillation regularization to avoid overfitting. To achieve a more effective alignment, instance semantic alignment loss and embedding structure alignment loss are employed during representation inheritance and contrastive tuning. The Code is available at <a target="_blank" rel="noopener" href="https://github.com/VisionXLab/ProCLIP">https://github.com/VisionXLab/ProCLIP</a> </p>
<blockquote>
<p>åŸCLIPæ–‡æœ¬ç¼–ç å™¨çš„æœ€å¤§è¾“å…¥é•¿åº¦é™åˆ¶ä¸º77ä¸ªæ ‡è®°ï¼Œè¿™å¦¨ç¢äº†å…¶æœ‰æ•ˆå¤„ç†é•¿æ–‡æœ¬å’Œè¿›è¡Œç²¾ç»†è¯­ä¹‰ç†è§£çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒCLIPæ–‡æœ¬ç¼–ç å™¨ä¸æ”¯æŒå¤šè¯­è¨€è¾“å…¥ã€‚æ‰€æœ‰è¿™äº›é™åˆ¶æå¤§åœ°é™åˆ¶äº†å…¶åœ¨æ›´å¹¿æ³›ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚æœ€è¿‘çš„ç ”ç©¶è¯•å›¾ç”¨åŸºäºLLMçš„åµŒå…¥å™¨æ›¿æ¢CLIPæ–‡æœ¬ç¼–ç å™¨ï¼Œä»¥å¢å¼ºå…¶å¤„ç†é•¿æ–‡æœ¬ã€å¤šè¯­è¨€ç†è§£å’Œç²¾ç»†è¯­ä¹‰ç†è§£çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºLLMçš„è¡¨ç¤ºç©ºé—´ä¸CLIPçš„è§†è§‰è¯­è¨€ç©ºé—´æ˜¯ç‹¬ç«‹è¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œæ²¡æœ‰å¯¹é½å…ˆéªŒï¼Œç›´æ¥ä½¿ç”¨å¯¹æ¯”å­¦ä¹ è¿›è¡Œç›´æ¥å¯¹é½å¯èƒ½ä¼šç ´åCLIPå›¾åƒç¼–ç å™¨ä¸­çš„å†…åœ¨è§†è§‰è¯­è¨€å¯¹é½ï¼Œå¯¼è‡´åœ¨é¢„è®­ç»ƒæœŸé—´è·å¾—çš„çŸ¥è¯†æœªèƒ½å……åˆ†åˆ©ç”¨ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ProCLIPï¼Œä¸€ç§åŸºäºè¯¾ç¨‹å­¦ä¹ çš„æ¸è¿›è§†è§‰è¯­è¨€å¯¹é½æ¡†æ¶ï¼Œæœ‰æ•ˆåœ°å°†CLIPå›¾åƒç¼–ç å™¨ä¸åŸºäºLLMçš„åµŒå…¥å™¨å¯¹é½ã€‚å…·ä½“æ¥è¯´ï¼ŒProCLIPé¦–å…ˆé€šè¿‡è’¸é¦ä»CLIPæ–‡æœ¬ç¼–ç å™¨ä¸­æå–çŸ¥è¯†å¹¶æ³¨å…¥åˆ°åŸºäºLLMçš„åµŒå…¥å™¨ä¸­ï¼Œä»¥åˆ©ç”¨CLIPä¸°å¯Œçš„é¢„è®­ç»ƒçŸ¥è¯†å¹¶åœ¨LLMåµŒå…¥å™¨å’ŒCLIPå›¾åƒç¼–ç å™¨ä¹‹é—´å»ºç«‹åˆæ­¥å¯¹é½ã€‚éšåï¼ŒProCLIPé€šè¿‡å›¾åƒæ–‡æœ¬å¯¹æ¯”è°ƒæ•´è¿›ä¸€æ­¥å¯¹é½CLIPå›¾åƒç¼–ç å™¨å’ŒåŸºäºLLMçš„åµŒå…¥å™¨ï¼Œå¹¶é‡‡ç”¨è‡ªæˆ‘è’¸é¦æ­£åˆ™åŒ–æ¥é¿å…è¿‡æ‹Ÿåˆã€‚ä¸ºäº†å®ç°æ›´æœ‰æ•ˆçš„å¯¹é½ï¼Œåœ¨è¡¨ç¤ºç»§æ‰¿å¯¹æ¯”è°ƒæ•´è¿‡ç¨‹ä¸­é‡‡ç”¨äº†å®ä¾‹è¯­ä¹‰å¯¹é½æŸå¤±å’ŒåµŒå…¥ç»“æ„å¯¹é½æŸå¤±ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/VisionXLab/ProCLIP%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/VisionXLab/ProCLIPä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18795v1">PDF</a> 17 pages, 5 fiugres</p>
<p><strong>Summary</strong></p>
<p>åœ¨CLIPæ–‡æœ¬ç¼–ç å™¨å—é™äºæœ€å¤§è¾“å…¥é•¿åº¦å’Œå¤šè¯­ç§è¾“å…¥ç¼ºå¤±çš„é™åˆ¶ä¸‹ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†é•¿æ–‡æœ¬å’Œè¿›è¡Œç²¾ç»†è¯­ä¹‰ç†è§£ã€‚è¿‘æœŸç ”ç©¶å°è¯•ç”¨LLMåµŒå…¥å™¨æ›¿æ¢CLIPæ–‡æœ¬ç¼–ç å™¨ä»¥æå‡å…¶å¤„ç†é•¿æ–‡æœ¬å’Œå¤šè¯­ç§ç†è§£çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºLLMå’ŒCLIPçš„è§†è§‰è¯­è¨€ç©ºé—´æ˜¯ç‹¬ç«‹é¢„è®­ç»ƒçš„ï¼Œç›´æ¥å¯¹æ¯”å­¦ä¹ å¯èƒ½ç ´åCLIPå›¾åƒç¼–ç å™¨ä¸­çš„è§†è§‰è¯­è¨€å¯¹é½ï¼Œå¯¼è‡´é¢„è®­ç»ƒæœŸé—´å­¦åˆ°çš„çŸ¥è¯†æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºProCLIPï¼Œä¸€ç§åŸºäºè¯¾ç¨‹å­¦ä¹ çš„æ¸è¿›è§†è§‰è¯­è¨€å¯¹é½æ¡†æ¶ï¼Œæœ‰æ•ˆå¯¹é½CLIPå›¾åƒç¼–ç å™¨å’ŒLLMåµŒå…¥å™¨ã€‚ProCLIPé¦–å…ˆè’¸é¦CLIPæ–‡æœ¬ç¼–ç å™¨çš„çŸ¥è¯†åˆ°LLMåµŒå…¥å™¨ä¸­ï¼Œå¹¶åˆ©ç”¨CLIPä¸°å¯Œçš„é¢„è®­ç»ƒçŸ¥è¯†å»ºç«‹LLMåµŒå…¥å™¨å’ŒCLIPå›¾åƒç¼–ç å™¨ä¹‹é—´çš„åˆæ­¥å¯¹é½ã€‚éšåï¼Œé€šè¿‡å›¾åƒæ–‡æœ¬å¯¹æ¯”è°ƒæ•´è¿›ä¸€æ­¥å¯¹é½CLIPå›¾åƒç¼–ç å™¨å’ŒLLMåµŒå…¥å™¨ï¼Œå¹¶é‡‡ç”¨è‡ªæˆ‘è’¸é¦æ­£åˆ™åŒ–é¿å…è¿‡æ‹Ÿåˆã€‚ä¸ºå®ç°æ›´æœ‰æ•ˆçš„å¯¹é½ï¼Œå®ä¾‹è¯­ä¹‰å¯¹é½æŸå¤±å’ŒåµŒå…¥ç»“æ„å¯¹é½æŸå¤±åœ¨è¡¨ç¤ºç»§æ‰¿å¯¹æ¯”è°ƒæ•´è¿‡ç¨‹ä¸­å¾—ä»¥åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPæ–‡æœ¬ç¼–ç å™¨å—é™äºæœ€å¤§è¾“å…¥é•¿åº¦ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†é•¿æ–‡æœ¬å’Œè¿›è¡Œç²¾ç»†è¯­ä¹‰ç†è§£ã€‚</li>
<li>LLMåµŒå…¥å™¨èƒ½å¤Ÿæé«˜CLIPåœ¨å¤„ç†é•¿æ–‡æœ¬å’Œå¤šè¯­ç§æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>ç›´æ¥å¯¹æ¯”å­¦ä¹ å¯èƒ½å¯¼è‡´é¢„è®­ç»ƒçŸ¥è¯†çš„æµªè´¹å’Œè§†è§‰è¯­è¨€å¯¹é½çš„ç ´åã€‚</li>
<li>ProCLIPé€šè¿‡è¯¾ç¨‹å­¦ä¹ æ¡†æ¶å®ç°CLIPå›¾åƒç¼–ç å™¨å’ŒLLMåµŒå…¥å™¨çš„æœ‰æ•ˆå¯¹é½ã€‚</li>
<li>ProCLIPåˆ©ç”¨è’¸é¦æŠ€æœ¯å°†CLIPæ–‡æœ¬ç¼–ç å™¨çš„çŸ¥è¯†è½¬ç§»åˆ°LLMåµŒå…¥å™¨ä¸­ã€‚</li>
<li>è‡ªæˆ‘è’¸é¦æ­£åˆ™åŒ–ç”¨äºé¿å…è¿‡æ‹Ÿåˆï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ProCLIPåˆ©ç”¨å®ä¾‹è¯­ä¹‰å¯¹é½æŸå¤±å’ŒåµŒå…¥ç»“æ„å¯¹é½æŸå¤±ä»¥å®ç°æ›´æœ‰æ•ˆçš„æ¨¡å‹å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18795">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c2138adca8b81c809f6bdbc0b9b8f7f4~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159852&auth_key=1761159852-0-0-64a54bf698fee20a168ec34b02c626a1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e2c3d1e6005f5e55c17390dd591c5882~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159859&auth_key=1761159859-0-0-242631b5ff5622f5f8653fc394da0d12&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e76d52b00b6143a61a71a6a3de48580b~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159866&auth_key=1761159866-0-0-50844158d3cac019188dbf80def8d6c5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-243592d5d2d688dec34b48e030d5172d~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159889&auth_key=1761159889-0-0-7e692c48b869d2a417e624fdcefee1e7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="KAT-Coder-Technical-Report"><a href="#KAT-Coder-Technical-Report" class="headerlink" title="KAT-Coder Technical Report"></a>KAT-Coder Technical Report</h2><p><strong>Authors:Zizheng Zhan, Ken Deng, Xiaojiang Zhang, Jinghui Wang, Huaixi Tang, Zhiyi Lai, Haoyang Huang, Wen Xiang, Kun Wu, Wenhao Zhuang, Minglei Zhang, Shaojie Wang, Shangpeng Yan, Kepeng Lei, Zongxian Feng, Huiming Wang, Zheng Lin, Mengtong Li, Mengfei Xie, Yinghan Cui, Xuxing Chen, Chao Wang, Weihao Li, Wenqiang Zhu, Jiarong Zhang, Jingxuan Xu, Songwei Yu, Yifan Yao, Xinping Lei, Han Li, Junqi Xiong, Zuchen Gao, Dailin Li, Haimo Li, Jiaheng Liu, Yuqun Zhang, Junyi Peng, Haotian Zhang, Bin Chen</strong></p>
<p>Recent advances in large language models (LLMs) have enabled progress in agentic coding, where models autonomously reason, plan, and act within interactive software development workflows. However, bridging the gap between static text-based training and dynamic real-world agentic execution remains a core challenge. In this technical report, we present KAT-Coder, a large-scale agentic code model trained through a multi-stage curriculum encompassing Mid-Term Training, Supervised Fine-Tuning (SFT), Reinforcement Fine-Tuning (RFT), and Reinforcement-to-Deployment Adaptation. The Mid-Term stage enhances reasoning, planning, and reflection capabilities through a corpus of real software engineering data and synthetic agentic interactions. The SFT stage constructs a million-sample dataset balancing twenty programming languages, ten development contexts, and ten task archetypes. The RFT stage introduces a novel multi-ground-truth reward formulation for stable and sample-efficient policy optimization. Finally, the Reinforcement-to-Deployment phase adapts the model to production-grade IDE environments using Error-Masked SFT and Tree-Structured Trajectory Training. In summary, these stages enable KAT-Coder to achieve robust tool-use reliability, instruction alignment, and long-context reasoning, forming a deployable foundation for real-world intelligent coding agents. Our KAT series 32B model, KAT-Dev, has been open-sourced on <a target="_blank" rel="noopener" href="https://huggingface.co/Kwaipilot/KAT-Dev">https://huggingface.co/Kwaipilot/KAT-Dev</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ¨åŠ¨äº†ä»£ç†ç¼–ç çš„å‘å±•ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿåœ¨äº¤äº’å¼è½¯ä»¶å¼€å‘å·¥ä½œæµä¸­è‡ªä¸»æ¨ç†ã€è§„åˆ’å’Œè¡ŒåŠ¨ã€‚ç„¶è€Œï¼Œå¼¥åˆåŸºäºé™æ€æ–‡æœ¬çš„åŸ¹è®­å’ŒåŠ¨æ€ç°å®ä¸–ç•Œä»£ç†æ‰§è¡Œä¹‹é—´çš„å·®è·ä»ç„¶æ˜¯ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†KAT-Coderï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡åŒ…æ‹¬ä¸­æœŸåŸ¹è®­ã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰å’Œå¼ºåŒ–åˆ°éƒ¨ç½²é€‚åº”çš„å¤šé˜¶æ®µè¯¾ç¨‹è®­ç»ƒçš„å¤§è§„æ¨¡ä»£ç†ç¼–ç æ¨¡å‹ã€‚ä¸­æœŸé˜¶æ®µé€šè¿‡çœŸå®è½¯ä»¶å·¥ç¨‹æ•°æ®å’Œåˆæˆä»£ç†äº¤äº’çš„è¯­æ–™åº“å¢å¼ºäº†æ¨ç†ã€è§„åˆ’å’Œåæ€èƒ½åŠ›ã€‚SFTé˜¶æ®µæ„å»ºäº†ä¸€ä¸ªç™¾ä¸‡æ ·æœ¬æ•°æ®é›†ï¼Œå¹³è¡¡äº†äºŒåç§ç¼–ç¨‹è¯­è¨€ã€åç§å¼€å‘ç¯å¢ƒå’Œåç§ä»»åŠ¡åŸå‹ã€‚RFTé˜¶æ®µå¼•å…¥äº†ä¸€ç§æ–°å‹çš„å¤šåœ°é¢çœŸå®å¥–åŠ±å…¬å¼ï¼Œç”¨äºç¨³å®šå’Œé«˜æ•ˆæ ·æœ¬çš„ç­–ç•¥ä¼˜åŒ–ã€‚æœ€åï¼Œå¼ºåŒ–åˆ°éƒ¨ç½²é˜¶æ®µä½¿ç”¨é”™è¯¯å±è”½SFTå’Œæ ‘ç»“æ„è½¨è¿¹è®­ç»ƒä½¿æ¨¡å‹é€‚åº”ç”Ÿäº§çº§IDEç¯å¢ƒã€‚æ€»ä¹‹ï¼Œè¿™äº›é˜¶æ®µä½¿KAT-Coderå®ç°äº†ç¨³å¥çš„å·¥å…·ä½¿ç”¨å¯é æ€§ã€æŒ‡ä»¤å¯¹é½å’Œé•¿ä¸Šä¸‹æ–‡æ¨ç†ï¼Œä¸ºç°å®ä¸–ç•Œçš„æ™ºèƒ½ç¼–ç ä»£ç†å¥ å®šäº†å¯éƒ¨ç½²çš„åŸºç¡€ã€‚æˆ‘ä»¬çš„KATç³»åˆ—32Bæ¨¡å‹KAT-Devå·²åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/Kwaipilot/KAT-Dev">https://huggingface.co/Kwaipilot/KAT-Dev</a>ä¸Šå¼€æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18779v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç†ç¼–ç æ–¹é¢çš„è¿›æ­¥ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨äº¤äº’å¼è½¯ä»¶å¼€å‘å·¥ä½œæµä¸­è‡ªä¸»æ¨ç†ã€è§„åˆ’å’Œè¡ŒåŠ¨ã€‚æœ¬æŠ¥å‘Šä»‹ç»äº†KAT-Coderï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ä»£ç†ç¼–ç æ¨¡å‹ï¼Œç»å†äº†åŒ…æ‹¬ä¸­æœŸè®­ç»ƒã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰å’Œå¼ºåŒ–åˆ°éƒ¨ç½²é€‚åº”ç­‰å¤šä¸ªé˜¶æ®µçš„è®­ç»ƒã€‚è¿™äº›é˜¶æ®µä½¿KAT-Coderå®ç°äº†å·¥å…·ä½¿ç”¨å¯é æ€§ã€æŒ‡ä»¤å¯¹é½å’Œé•¿ä¸Šä¸‹æ–‡æ¨ç†ï¼Œä¸ºç°å®ä¸–ç•Œæ™ºèƒ½ç¼–ç ä»£ç†çš„éƒ¨ç½²å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„è¿›æ­¥æ¨åŠ¨äº†ä»£ç†ç¼–ç çš„å‘å±•ï¼Œä½¿æ¨¡å‹èƒ½è‡ªä¸»å‚ä¸è½¯ä»¶å¼€å‘æµç¨‹ã€‚<br>2.KAT-Coderæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ä»£ç†ç¼–ç æ¨¡å‹ï¼Œç»å†äº†å¤šé˜¶æ®µè®­ç»ƒã€‚</li>
<li>ä¸­æœŸè®­ç»ƒé˜¶æ®µæå‡äº†KAT-Coderçš„æ¨ç†ã€è§„åˆ’å’Œåæ€èƒ½åŠ›ã€‚</li>
<li>ç›‘ç£å¾®è°ƒé˜¶æ®µæ„å»ºäº†å¹³è¡¡å¤šç§ç¼–ç¨‹è¯­è¨€ã€å¼€å‘ç¯å¢ƒå’Œä»»åŠ¡åŸå‹çš„å¤§æ ·æœ¬æ•°æ®é›†ã€‚</li>
<li>å¼ºåŒ–å¾®è°ƒé˜¶æ®µå¼•å…¥äº†å¤šåœ°é¢çœŸå®å¥–åŠ±å…¬å¼ï¼Œå®ç°ç¨³å®šä¸”é«˜æ•ˆçš„ç­–ç•¥ä¼˜åŒ–ã€‚</li>
<li>ä»å¼ºåŒ–åˆ°éƒ¨ç½²é˜¶æ®µä½¿KAT-Coderé€‚åº”ç”Ÿäº§çº§IDEç¯å¢ƒã€‚</li>
<li>KATç³»åˆ—32Bæ¨¡å‹KAT-Devå·²å¼€æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18779">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-760cffb2dc68addd1a7e5f5de789d0a7~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159898&auth_key=1761159898-0-0-c01f2d684940b45a7f146ef5c2e3e92e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0b032abf160d33516216963a9b7d0dc7~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159905&auth_key=1761159905-0-0-2c10cc1eb4dffd23972353eefd2d5019&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-da2a93e57df6f05a0bf4ad41ab5fcdf1~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159912&auth_key=1761159912-0-0-bfca2ce87d4d1681b6b1129da0ca9eac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Topoformer-brain-like-topographic-organization-in-Transformer-language-models-through-spatial-querying-and-reweighting"><a href="#Topoformer-brain-like-topographic-organization-in-Transformer-language-models-through-spatial-querying-and-reweighting" class="headerlink" title="Topoformer: brain-like topographic organization in Transformer language   models through spatial querying and reweighting"></a>Topoformer: brain-like topographic organization in Transformer language   models through spatial querying and reweighting</h2><p><strong>Authors:Taha Binhuraib, Greta Tuckute, Nicholas Blauch</strong></p>
<p>Spatial functional organization is a hallmark of biological brains: neurons are arranged topographically according to their response properties, at multiple scales. In contrast, representations within most machine learning models lack spatial biases, instead manifesting as disorganized vector spaces that are difficult to visualize and interpret. Here, we propose a novel form of self-attention that turns Transformers into â€œTopoformersâ€ with topographic organization. We introduce spatial querying - where keys and queries are arranged on 2D grids, and local pools of queries are associated with a given key - and spatial reweighting, where we convert the standard fully connected layer of self-attention into a locally connected layer. We first demonstrate the feasibility of our approach by training a 1-layer Topoformer on a sentiment classification task. Training with spatial querying encourages topographic organization in the queries and keys, and spatial reweighting separately encourages topographic organization in the values and self-attention outputs. We then apply the Topoformer motifs at scale, training a BERT architecture with a masked language modeling objective. We find that the topographic variant performs on par with a non-topographic control model on NLP benchmarks, yet produces interpretable topographic organization as evaluated via eight linguistic test suites. Finally, analyzing an fMRI dataset of human brain responses to a large set of naturalistic sentences, we demonstrate alignment between low-dimensional topographic variability in the Topoformer model and human brain language network. Scaling up Topoformers further holds promise for greater interpretability in NLP research, and for more accurate models of the organization of linguistic information in the human brain. </p>
<blockquote>
<p>ç©ºé—´åŠŸèƒ½ç»„ç»‡æ˜¯ç”Ÿç‰©å¤§è„‘çš„ç‰¹å¾ï¼šç¥ç»å…ƒæ ¹æ®å…¶ååº”å±æ€§åœ¨å¤šä¸ªå°ºåº¦ä¸Šè¿›è¡Œæ‹“æ‰‘æ’åˆ—ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤§å¤šæ•°æœºå™¨å­¦ä¹ æ¨¡å‹å†…çš„è¡¨ç¤ºç¼ºä¹ç©ºé—´åè§ï¼Œè€Œæ˜¯è¡¨ç°ä¸ºéš¾ä»¥å¯è§†åŒ–å’Œè§£é‡Šçš„æ‚ä¹±æ— ç« çš„å‘é‡ç©ºé—´ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è‡ªæ³¨æ„åŠ›å½¢å¼ï¼Œå°†Transformerè½¬å˜ä¸ºå…·æœ‰æ‹“æ‰‘ç»„ç»‡çš„â€œTopoformerâ€ã€‚æˆ‘ä»¬å¼•å…¥äº†ç©ºé—´æŸ¥è¯¢ - å…¶ä¸­é”®å’ŒæŸ¥è¯¢æŒ‰äºŒç»´ç½‘æ ¼æ’åˆ—ï¼Œå±€éƒ¨æŸ¥è¯¢ä¸ç»™å®šçš„é”®ç›¸å…³è” - ä»¥åŠç©ºé—´é‡æ–°åŠ æƒï¼Œå…¶ä¸­æˆ‘ä»¬å°†è‡ªæ³¨æ„åŠ›çš„æ ‡å‡†å…¨è¿æ¥å±‚è½¬æ¢ä¸ºå±€éƒ¨è¿æ¥å±‚ã€‚æˆ‘ä»¬é¦–å…ˆé€šè¿‡åœ¨ä¸€ä¸ªæƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸Šè®­ç»ƒä¸€å±‚Topoformeræ¥è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯è¡Œæ€§ã€‚ä½¿ç”¨ç©ºé—´æŸ¥è¯¢è¿›è¡Œè®­ç»ƒé¼“åŠ±æŸ¥è¯¢å’Œé”®çš„æ‹“æ‰‘ç»„ç»‡ï¼Œè€Œç©ºé—´é‡æ–°åŠ æƒåˆ™åˆ†åˆ«é¼“åŠ±å€¼å’Œè‡ªæ³¨æ„åŠ›è¾“å‡ºçš„æ‹“æ‰‘ç»„ç»‡ã€‚ç„¶åï¼Œæˆ‘ä»¬åº”ç”¨Topoformerä¸»é¢˜å¹¶æ‰©å¤§è§„æ¨¡ï¼Œä½¿ç”¨æ©ç›–è¯­è¨€å»ºæ¨¡ç›®æ ‡æ¥è®­ç»ƒBERTæ¶æ„ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨NLPåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ‹“æ‰‘å˜ä½“ä¸éæ‹“æ‰‘å¯¹ç…§æ¨¡å‹è¡¨ç°ç›¸å½“ï¼Œä½†ä¼šäº§ç”Ÿå¯è§£é‡Šçš„æ‹“æ‰‘ç»„ç»‡ï¼Œè¿™å·²é€šè¿‡å…«ä¸ªè¯­è¨€æµ‹è¯•å¥—ä»¶è¿›è¡Œè¯„ä¼°ã€‚æœ€åï¼Œé€šè¿‡åˆ†æäººç±»å¤§è„‘å¯¹å¤§é‡è‡ªç„¶å¥å­çš„å“åº”çš„fMRIæ•°æ®é›†ï¼Œæˆ‘ä»¬å±•ç¤ºäº†Topoformeræ¨¡å‹çš„ä½ç»´æ‹“æ‰‘å˜åŒ–ä¸äººç±»å¤§è„‘è¯­è¨€ç½‘ç»œä¹‹é—´çš„å¯¹é½ã€‚è¿›ä¸€æ­¥æ‰©å±•Topoformersæœ‰æœ›ä¸ºNLPç ”ç©¶ä¸­æä¾›æ›´å¤šçš„å¯è§£é‡Šæ€§ï¼Œå¹¶åˆ›å»ºæ›´å‡†ç¡®çš„äººç±»å¤§è„‘è¯­è¨€ä¿¡æ¯ç»„ç»‡æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18745v1">PDF</a> ICLR 2024 Workshop on Representational Alignment (Re-Align) Camera   Ready</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†å¸¦æœ‰åœ°å½¢ç»„ç»‡ç»“æ„çš„â€œTopoformerâ€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡å¼•å…¥ç©ºé—´æŸ¥è¯¢å’Œç©ºé—´é‡åŠ æƒæœºåˆ¶ï¼Œä½¿Transformeræ¨¡å‹å…·å¤‡åœ°å½¢ç»„ç»‡ç‰¹æ€§ã€‚åœ¨æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜è¯¥æ–¹æ³•çš„å¯è¡Œæ€§ã€‚å°†å…¶åº”ç”¨äºBERTæ¶æ„å¹¶è¿›è¡Œè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œç»“æœæ˜¾ç¤ºTopoformeræ¨¡å‹æ€§èƒ½ä¸å¸¸è§„æ¨¡å‹ç›¸å½“ï¼Œä½†å…·å¤‡å¯è§£é‡Šçš„åœ°å½¢ç»„ç»‡ç»“æ„ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä¸äººç±»å¤§è„‘è¯­è¨€ç½‘ç»œçš„å¯¹æ¯”ç ”ç©¶ï¼Œå‘ç°Topoformeræ¨¡å‹çš„ä½ç»´åœ°å½¢å˜åŒ–ä¸äººç±»å¤§è„‘å“åº”ä¹‹é—´å­˜åœ¨ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Topoformeræ¨¡å‹ç»“åˆäº†ç©ºé—´æŸ¥è¯¢å’Œç©ºé—´é‡åŠ æƒæœºåˆ¶ï¼Œä½¿Transformerå…·å¤‡åœ°å½¢ç»„ç»‡ç»“æ„ç‰¹æ€§ã€‚</li>
<li>é€šè¿‡æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡éªŒè¯äº†Topoformeræ¨¡å‹çš„å¯è¡Œæ€§ã€‚</li>
<li>å°†Topoformeræ¨¡å‹åº”ç”¨äºBERTæ¶æ„è¿›è¡Œè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œæ€§èƒ½ä¸å¸¸è§„æ¨¡å‹ç›¸å½“ã€‚</li>
<li>Topoformeræ¨¡å‹å±•ç°å‡ºå¯è§£é‡Šçš„åœ°å½¢ç»„ç»‡ç»“æ„ã€‚</li>
<li>Topoformeræ¨¡å‹çš„ä½ç»´åœ°å½¢å˜åŒ–ä¸äººç±»å¤§è„‘å“åº”ä¹‹é—´å­˜åœ¨ä¸€è‡´æ€§ã€‚</li>
<li>Topoformeræ¨¡å‹çš„æå‡ºæœ‰åŠ©äºæé«˜NLPç ”ç©¶çš„å¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18745">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-abe616709dbaa1ef1dc4fcd8a65966d4~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159919&auth_key=1761159919-0-0-cd95ac32ed4a86748ed3496847f5ac8b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2dbc69b91a2633e973040d749e18b5bc~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159926&auth_key=1761159926-0-0-c28c2d07328efc3573cc3e8144bf9707&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-82ae12e967f2b361e411f5374cbf505d~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159933&auth_key=1761159933-0-0-283aaeebfab969dc266190d2f42cf7cb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-42e08f919e12421796e96493321a05f0~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159940&auth_key=1761159940-0-0-6773cc5f0b16b9cc33686d4d129971cb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-953bbee6ed2e92d24c1ba062ae03b796~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159946&auth_key=1761159946-0-0-93dbf68146cdccde3d5e3b70d17aa1a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DART-A-Structured-Dataset-of-Regulatory-Drug-Documents-in-Italian-for-Clinical-NLP"><a href="#DART-A-Structured-Dataset-of-Regulatory-Drug-Documents-in-Italian-for-Clinical-NLP" class="headerlink" title="DART: A Structured Dataset of Regulatory Drug Documents in Italian for   Clinical NLP"></a>DART: A Structured Dataset of Regulatory Drug Documents in Italian for   Clinical NLP</h2><p><strong>Authors:Mariano Barone, Antonio Laudante, Giuseppe Riccio, Antonio Romano, Marco Postiglione, Vincenzo Moscato</strong></p>
<p>The extraction of pharmacological knowledge from regulatory documents has become a key focus in biomedical natural language processing, with applications ranging from adverse event monitoring to AI-assisted clinical decision support. However, research in this field has predominantly relied on English-language corpora such as DrugBank, leaving a significant gap in resources tailored to other healthcare systems. To address this limitation, we introduce DART (Drug Annotation from Regulatory Texts), the first structured corpus of Italian Summaries of Product Characteristics derived from the official repository of the Italian Medicines Agency (AIFA). The dataset was built through a reproducible pipeline encompassing web-scale document retrieval, semantic segmentation of regulatory sections, and clinical summarization using a few-shot-tuned large language model with low-temperature decoding. DART provides structured information on key pharmacological domains such as indications, adverse drug reactions, and drug-drug interactions. To validate its utility, we implemented an LLM-based drug interaction checker that leverages the dataset to infer clinically meaningful interactions. Experimental results show that instruction-tuned LLMs can accurately infer potential interactions and their clinical implications when grounded in the structured textual fields of DART. We publicly release our code on GitHub: <a target="_blank" rel="noopener" href="https://github.com/PRAISELab-PicusLab/DART">https://github.com/PRAISELab-PicusLab/DART</a>. </p>
<blockquote>
<p>ä»ç›‘ç®¡æ–‡çŒ®ä¸­æå–è¯ç†å­¦çŸ¥è¯†å·²æˆä¸ºç”Ÿç‰©åŒ»å­¦è‡ªç„¶è¯­è¨€å¤„ç†çš„å…³é”®ç„¦ç‚¹ï¼Œå…¶åº”ç”¨ä»ä¸è‰¯äº‹ä»¶ç›‘æµ‹åˆ°äººå·¥æ™ºèƒ½è¾…åŠ©ä¸´åºŠå†³ç­–æ”¯æŒä¸ç­‰ã€‚ç„¶è€Œï¼Œè¯¥é¢†åŸŸçš„ç ”ç©¶ä¸»è¦ä¾èµ–äºå¦‚DrugBankç­‰çš„è‹±è¯­è¯­æ–™åº“ï¼Œé’ˆå¯¹å…¶ä»–åŒ»ç–—ç³»ç»Ÿçš„èµ„æºå­˜åœ¨å·¨å¤§å·®è·ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DARTï¼ˆæ¥è‡ªç›‘ç®¡æ–‡æœ¬çš„è¯å“æ³¨é‡Šï¼‰ï¼Œè¿™æ˜¯ä»æ„å¤§åˆ©è¯å“ç®¡ç†å±€ï¼ˆAIFAï¼‰çš„å®˜æ–¹å­˜å‚¨åº“ä¸­æå–çš„ç¬¬ä¸€ä¸ªç»“æ„åŒ–æ„å¤§åˆ©è¯­è¯å“ç‰¹æ€§æ‘˜è¦è¯­æ–™åº“ã€‚è¯¥æ•°æ®é›†æ˜¯é€šè¿‡å¯é‡å¤çš„ç®¡é“æ„å»ºçš„ï¼Œè¯¥ç®¡é“åŒ…æ‹¬ç½‘é¡µè§„æ¨¡æ–‡æ¡£æ£€ç´¢ã€ç›‘ç®¡éƒ¨åˆ†çš„è¯­ä¹‰åˆ†å‰²ä»¥åŠä½¿ç”¨ç»è¿‡å‡ æ¬¡è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä¸´åºŠæ‘˜è¦ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨ä½æ¸©è§£ç æŠ€æœ¯ã€‚DARTæä¾›äº†å…³äºå…³é”®è¯ç†å­¦é¢†åŸŸçš„ç»“æ„åŒ–ä¿¡æ¯ï¼Œå¦‚é€‚åº”ç—‡ã€è¯ç‰©ä¸è‰¯ååº”å’Œè¯ç‰©ç›¸äº’ä½œç”¨ç­‰ã€‚ä¸ºäº†éªŒè¯å…¶å®ç”¨æ€§ï¼Œæˆ‘ä»¬å®æ–½äº†ä¸€ä¸ªåŸºäºLLMçš„è¯ç‰©ç›¸äº’ä½œç”¨æ£€æŸ¥å™¨ï¼Œè¯¥æ£€æŸ¥å™¨åˆ©ç”¨æ•°æ®é›†æ¨æ–­å…·æœ‰ä¸´åºŠæ„ä¹‰çš„ç›¸äº’ä½œç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨DARTçš„ç»“æ„åŒ–æ–‡æœ¬å­—æ®µçš„åŸºç¡€ä¸Šï¼Œç»è¿‡æŒ‡ä»¤è®­ç»ƒçš„LLMå¯ä»¥å‡†ç¡®åœ°æ¨æ–­å‡ºæ½œåœ¨çš„ç›¸äº’ä½œç”¨åŠå…¶ä¸´åºŠæ„ä¹‰ã€‚æˆ‘ä»¬åœ¨GitHubä¸Šå…¬å¼€å‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/PRAISELab-PicusLab/DART%E3%80%82">https://github.com/PRAISELab-PicusLab/DARTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18475v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä»ç›‘ç®¡æ–‡ä»¶ä¸­æå–è¯ç†å­¦çŸ¥è¯†çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºå½“å‰ç ”ç©¶ä¸»è¦ä¾èµ–è‹±è¯­è¯­æ–™åº“ï¼Œå¦‚DrugBankï¼Œå¿½è§†äº†å…¶ä»–åŒ»ç–—ç³»ç»Ÿçš„èµ„æºéœ€æ±‚ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†DARTï¼ˆæ¥è‡ªæ„å¤§åˆ©è¯ç‰©ç®¡ç†å±€ï¼ˆAIFAï¼‰å®˜æ–¹å­˜å‚¨åº“çš„æ„å¤§åˆ©äº§å“ç‰¹æ€§æ‘˜è¦çš„ç»“æ„åŒ–è¯­æ–™åº“ï¼‰ã€‚è¯¥æ•°æ®é›†é€šè¿‡å¯é‡å¤çš„ç®¡é“æ„å»ºï¼ŒåŒ…æ‹¬ç½‘é¡µè§„æ¨¡æ–‡æ¡£æ£€ç´¢ã€ç›‘ç®¡éƒ¨åˆ†çš„è¯­ä¹‰åˆ†å‰²ä»¥åŠä½¿ç”¨ç»è¿‡å°‘é‡æ ·æœ¬è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä¸´åºŠæ€»ç»“ã€‚DARTæä¾›å…³äºå…³é”®è¯ç†å­¦é¢†åŸŸçš„ç»“æ„åŒ–ä¿¡æ¯ï¼Œå¦‚é€‚åº”ç—‡ã€è¯ç‰©ä¸è‰¯ååº”å’Œè¯ç‰©ç›¸äº’ä½œç”¨ã€‚ä¸ºéªŒè¯å…¶å®ç”¨æ€§ï¼Œæœ¬æ–‡å®æ–½äº†ä¸€ä¸ªåŸºäºLLMçš„è¯ç‰©ç›¸äº’ä½œç”¨æ£€æŸ¥å™¨ï¼Œåˆ©ç”¨æ•°æ®é›†æ¨æ–­å…·æœ‰ä¸´åºŠæ„ä¹‰çš„ç›¸äº’ä½œç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºæŒ‡ä»¤è®­ç»ƒçš„LLMåœ¨DARTçš„ç»“æ„åŒ–æ–‡æœ¬å­—æ®µä¸­ï¼Œèƒ½å‡†ç¡®æ¨æ–­æ½œåœ¨çš„ç›¸äº’ä½œç”¨åŠå…¶ä¸´åºŠæ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå–è¯ç†å­¦çŸ¥è¯†ä»ç›‘ç®¡æ–‡ä»¶åœ¨ç”Ÿç‰©åŒ»å­¦è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æ˜¯å…³é”®ç„¦ç‚¹ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦ä¾èµ–è‹±è¯­è¯­æ–™åº“ï¼Œå¿½è§†äº†å…¶ä»–åŒ»ç–—ç³»ç»Ÿçš„èµ„æºéœ€æ±‚ï¼Œå­˜åœ¨èµ„æºå·®è·ã€‚</li>
<li>DARTæ˜¯é¦–ä¸ªåŸºäºæ„å¤§åˆ©è¯å“ç‰¹æ€§æ‘˜è¦çš„ç»“æ„åŒ–è¯­æ–™åº“ã€‚</li>
<li>DARTæ•°æ®é›†é€šè¿‡å¯é‡å¤çš„ç®¡é“æ„å»ºï¼ŒåŒ…æ‹¬æ–‡æ¡£æ£€ç´¢ã€è¯­ä¹‰åˆ†å‰²å’Œä¸´åºŠæ€»ç»“ã€‚</li>
<li>DARTæä¾›å…³äºå…³é”®è¯ç†å­¦é¢†åŸŸçš„ç»“æ„åŒ–ä¿¡æ¯ï¼Œå¦‚é€‚åº”ç—‡ã€ä¸è‰¯ååº”å’Œè¯ç‰©ç›¸äº’ä½œç”¨ã€‚</li>
<li>åˆ©ç”¨DARTæ•°æ®é›†å¼€å‘äº†ä¸€ä¸ªåŸºäºLLMçš„è¯ç‰©ç›¸äº’ä½œç”¨æ£€æŸ¥å™¨ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMèƒ½å‡†ç¡®åŸºäºDARTçš„ç»“æ„åŒ–æ–‡æœ¬å­—æ®µæ¨æ–­è¯ç‰©é—´çš„æ½œåœ¨ç›¸äº’ä½œç”¨åŠå…¶ä¸´åºŠæ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18475">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2d9157fdb84ace347acf0b11583e22ff~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159954&auth_key=1761159954-0-0-c9e947fb3f2b1a6cf69471e9f5590f19&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6d68b493f2a9810a91afa84e7967bf11~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159961&auth_key=1761159961-0-0-8313cd0f75f8ffeac3e69df276f9b09c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-19c80e3377172e6a5dff67c0aaf063c5~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159968&auth_key=1761159968-0-0-a9adff1bd0062b4b55b4abf3eb99d792&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-581618a43cfa94343c2c55b7126754d7~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159974&auth_key=1761159974-0-0-91e5a7657b5b4b724b5b0a9ef7ca8ef8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-23f391a8ed782b66eead3417612f4224~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159980&auth_key=1761159980-0-0-fcbcade8e20f8ba738f5ff60ab77deb4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fb25160bf97127900a938e98a9cc1f2a~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159987&auth_key=1761159987-0-0-bfa42293f398290a419d8c3f64cb86f3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SimBench-Benchmarking-the-Ability-of-Large-Language-Models-to-Simulate-Human-Behaviors"><a href="#SimBench-Benchmarking-the-Ability-of-Large-Language-Models-to-Simulate-Human-Behaviors" class="headerlink" title="SimBench: Benchmarking the Ability of Large Language Models to Simulate   Human Behaviors"></a>SimBench: Benchmarking the Ability of Large Language Models to Simulate   Human Behaviors</h2><p><strong>Authors:Tiancheng Hu, Joachim Baumann, Lorenzo Lupo, Nigel Collier, Dirk Hovy, Paul RÃ¶ttger</strong></p>
<p>Large language model (LLM) simulations of human behavior have the potential to revolutionize the social and behavioral sciences, if and only if they faithfully reflect real human behaviors. Current evaluations are fragmented, based on bespoke tasks and metrics, creating a patchwork of incomparable results. To address this, we introduce SimBench, the first large-scale, standardized benchmark for a robust, reproducible science of LLM simulation. By unifying 20 diverse datasets covering tasks from moral decision-making to economic choice across a large global participant pool, SimBench provides the necessary foundation to ask fundamental questions about when, how, and why LLM simulations succeed or fail. We show that, while even the best LLMs today have limited simulation ability (score: 40.80&#x2F;100), performance scales log-linearly with model size. Simulation performance is not improved by increased inference-time compute. We demonstrate an alignment-simulation trade-off: instruction-tuning improves performance on low-entropy (consensus) questions but degrades it on high-entropy (diverse) ones. Models particularly struggle when simulating specific demographic groups. Finally, we demonstrate that simulation ability correlates most strongly with deep, knowledge-intensive reasoning (MMLU-Pro, r&#x3D;0.939). By making progress measurable, we aim to accelerate the development of more faithful LLM simulators. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹äººç±»è¡Œä¸ºçš„æ¨¡æ‹Ÿå…·æœ‰é¢ è¦†ç¤¾ä¼šå’Œè¡Œä¸ºç§‘å­¦çš„æ½œåŠ›ï¼Œä½†è¿™ç§æ½œåŠ›åªæœ‰åœ¨å®ƒä»¬çœŸå®åæ˜ äººç±»è¡Œä¸ºçš„æƒ…å†µä¸‹æ‰èƒ½å®ç°ã€‚å½“å‰çš„è¯„ä¼°æ˜¯é›¶ç¢çš„ï¼ŒåŸºäºç‰¹å®šä»»åŠ¡å’ŒæŒ‡æ ‡ï¼Œå¯¼è‡´å‡ºç°ç»“æœä¸å¯æ¯”è¾ƒçš„æƒ…å†µã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SimBenchï¼Œè¿™æ˜¯é’ˆå¯¹ç¨³å¥ã€å¯é‡å¤çš„LLMæ¨¡æ‹Ÿç§‘å­¦çš„ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•ã€‚SimBenchç»Ÿä¸€äº†20ä¸ªæ¶µç›–ä»é“å¾·å†³ç­–åˆ°ç»æµé€‰æ‹©ç­‰ä»»åŠ¡çš„å¤šæ ·åŒ–æ•°æ®é›†ï¼Œè¦†ç›–äº†å¤§é‡å…¨çƒå‚ä¸è€…ç¾¤ä½“ï¼Œä¸ºå…³äºLLMæ¨¡æ‹Ÿä½•æ—¶ã€å¦‚ä½•ä»¥åŠä¸ºä½•æˆåŠŸæˆ–å¤±è´¥ç­‰æ ¹æœ¬é—®é¢˜æä¾›äº†å¿…è¦çš„åŸºç¡€ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ˜¾ç¤ºï¼Œå°½ç®¡ä»Šå¤©å³ä½¿æ˜¯æœ€å¥½çš„LLMæ¨¡æ‹Ÿèƒ½åŠ›ä¹Ÿæœ‰é™ï¼ˆå¾—åˆ†ï¼š40.80&#x2F;100ï¼‰ï¼Œä½†éšç€æ¨¡å‹å¤§å°çš„å¢åŠ ï¼Œæ€§èƒ½å‘ˆå¯¹æ•°çº¿æ€§å¢é•¿ã€‚å¢åŠ æ¨ç†æ—¶é—´çš„è®¡ç®—å¹¶ä¸ä¼šæé«˜æ¨¡æ‹Ÿæ€§èƒ½ã€‚æˆ‘ä»¬è¯æ˜äº†å­˜åœ¨ä¸€ç§å¯¹é½æ¨¡æ‹Ÿçš„æƒè¡¡ï¼šæŒ‡ä»¤è°ƒæ•´æ”¹è¿›äº†ä½ç†µï¼ˆå…±è¯†ï¼‰é—®é¢˜ä¸Šçš„æ€§èƒ½ï¼Œä½†é™ä½äº†é«˜ç†µï¼ˆå¤šæ ·åŒ–ï¼‰é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚åœ¨æ¨¡æ‹Ÿç‰¹å®šäººå£ç¾¤ä½“æ—¶ï¼Œæ¨¡å‹å°¤å…¶é¢ä¸´å›°éš¾ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜æ¨¡æ‹Ÿèƒ½åŠ›ä¸æ·±å…¥çš„çŸ¥è¯†å¯†é›†å‹æ¨ç†ï¼ˆMMLU-Proï¼Œr&#x3D;0.939ï¼‰å…³è”åº¦æœ€é«˜ã€‚é€šè¿‡ä½¿è¿›æ­¥å¯è¡¡é‡ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åŠ é€Ÿæ›´çœŸå®å¯é çš„LLMæ¨¡æ‹Ÿå™¨çš„å¼€å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17516v2">PDF</a> Project Website: <a target="_blank" rel="noopener" href="http://simbench.tiancheng.hu/">http://simbench.tiancheng.hu/</a> Data:   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/pitehu/SimBench">https://huggingface.co/datasets/pitehu/SimBench</a></p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹äººç±»è¡Œä¸ºçš„æ¨¡æ‹Ÿå…·æœ‰é©å‘½æ€§æ½œåŠ›ï¼Œå‰ææ˜¯å¿…é¡»çœŸå®åæ˜ äººç±»è¡Œä¸ºã€‚å½“å‰è¯„ä¼°æ–¹å¼é›¶æ•£ï¼Œç¼ºä¹ç»Ÿä¸€æ ‡å‡†ï¼Œå¯¼è‡´ç»“æœéš¾ä»¥æ¯”è¾ƒã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æ¨å‡ºSimBenchï¼Œé¦–ä¸ªå¤§è§„æ¨¡æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å»ºç«‹ç¨³å¥ã€å¯é‡å¤éªŒè¯çš„LLMæ¨¡æ‹Ÿç§‘å­¦ã€‚SimBenchç»Ÿä¸€äº†æ¶µç›–é“å¾·å†³ç­–ã€ç»æµé€‰æ‹©ç­‰ä»»åŠ¡çš„20ä¸ªä¸åŒæ•°æ®é›†ï¼Œå¹¶é¢å‘å…¨çƒå¤§è§„æ¨¡å‚ä¸è€…ç¾¤ä½“è¿›è¡Œæµ‹è¯•ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰æœ€ä½³LLMçš„æ¨¡æ‹Ÿèƒ½åŠ›æœ‰é™ï¼Œæ€§èƒ½éšæ¨¡å‹è§„æ¨¡å‘ˆå¯¹æ•°çº¿æ€§å¢é•¿ï¼Œæ¨ç†èƒ½åŠ›å¯¹æ¨¡æ‹Ÿæ€§èƒ½å½±å“æ˜¾è‘—ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡åˆ¶å®šå¯åº¦é‡çš„æ ‡å‡†æ¥åŠ é€Ÿæ›´çœŸå®çš„äººç±»è¡Œä¸ºLLMæ¨¡æ‹Ÿå™¨çš„å¼€å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹äººç±»è¡Œä¸ºçš„æ¨¡æ‹Ÿæœ‰æ½œåŠ›æ”¹å˜ç¤¾äº¤å’Œè¡Œä¸ºç§‘å­¦ã€‚</li>
<li>å½“å‰LLMæ¨¡æ‹Ÿçš„è¯„ä¼°æ–¹æ³•å­˜åœ¨ç¢ç‰‡åŒ–é—®é¢˜ï¼Œç¼ºä¹ç»Ÿä¸€æ ‡å‡†ã€‚</li>
<li>SimBenchä½œä¸ºé¦–ä¸ªå¤§è§„æ¨¡æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•ï¼Œä¸ºå»ºç«‹ç¨³å¥ã€å¯é‡å¤çš„LLMæ¨¡æ‹Ÿç§‘å­¦æä¾›äº†å¿…è¦åŸºç¡€ã€‚</li>
<li>SimBenchç»Ÿä¸€äº†æ¶µç›–å¤šç§ä»»åŠ¡çš„å¤šæ ·åŒ–æ•°æ®é›†ï¼Œå¹¶é¢å‘å…¨çƒå¤§è§„æ¨¡å‚ä¸è€…è¿›è¡Œæµ‹è¯•ã€‚</li>
<li>å½“å‰LLMæ¨¡æ‹Ÿèƒ½åŠ›æœ‰é™ï¼Œæ€§èƒ½éšæ¨¡å‹è§„æ¨¡å¢é•¿ï¼Œä¸æ·±åº¦çŸ¥è¯†æ¨ç†èƒ½åŠ›å…³è”æœ€å¼ºã€‚</li>
<li>LLMæ¨¡æ‹Ÿæ€§èƒ½ä¸ä¼šå› æ¨ç†æ—¶é—´è®¡ç®—å¢åŠ è€Œæé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17516">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c4998d1ae4aeda59c674ec360145a4aa~resize:0:q75.jpg?source=1f5c5e47&expiration=1761159994&auth_key=1761159994-0-0-e86b8ba884d517932bbca3520104465c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d4c1ed777d490906f144590f6ceafc1~resize:0:q75.jpg?source=1f5c5e47&expiration=1761160001&auth_key=1761160001-0-0-a610e5fe36133eec7cb8fef8b8b75248&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Retrieval-in-the-Chain-Bootstrapping-Large-Language-Models-for-Generative-Retrieval"><a href="#Retrieval-in-the-Chain-Bootstrapping-Large-Language-Models-for-Generative-Retrieval" class="headerlink" title="Retrieval-in-the-Chain: Bootstrapping Large Language Models for   Generative Retrieval"></a>Retrieval-in-the-Chain: Bootstrapping Large Language Models for   Generative Retrieval</h2><p><strong>Authors:Yingchen Zhang, Ruqing Zhang, Jiafeng Guo, Wenjun Peng, Sen Li, Fuyu Lv</strong></p>
<p>Generative retrieval (GR) is an emerging paradigm that leverages large language models (LLMs) to autoregressively generate document identifiers (docids) relevant to a given query. Prior works have focused on leveraging the generative capabilities of LLMs to improve GR, while overlooking that their reasoning capabilities could likewise help. This raises a key question: Can explicit reasoning benefit GR? To investigate, we first conduct a preliminary study where an LLM is prompted to generate free-form chain-of-thought (CoT) reasoning before performing constrained docid decoding. Although this method outperforms standard GR, the generated reasoning tends to be verbose and poorly aligned with the docid space. These limitations motivate the development of a reasoning mechanism better tailored to GR.   Therefore, we propose Reason-for-Retrieval (R4R), a reasoning-augmented framework for GR that converts free-form CoT reasoning into a compact, structured format, and iteratively refines the reasoning during the retrieval process. R4R augments an existing GR method by leveraging a reasoning-capable LLM that has been instruction-tuned for GR. At inference time, R4R first uses the LLM to generate an initial structured reasoning; then the same LLM alternates between (i) constrained decoding with the chosen GR method to produce candidate docids and (ii) updating the reasoning based on retrieval results to improve the next round. R4R does not require additional models or training, and instead a single LLM serves as both the reasoning generator and the retriever. Extensive experiments on Natural Questions, MS MARCO, and a real-world item-search benchmark validate the effectiveness of R4R. </p>
<blockquote>
<p>ç”Ÿæˆå¼æ£€ç´¢ï¼ˆGRï¼‰æ˜¯ä¸€ç§æ–°å…´èŒƒå¼ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥è‡ªå›å½’çš„æ–¹å¼ç”Ÿæˆä¸ç»™å®šæŸ¥è¯¢ç›¸å…³çš„æ–‡æ¡£æ ‡è¯†ç¬¦ï¼ˆdocidsï¼‰ã€‚æ—©æœŸçš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨åˆ©ç”¨LLMçš„ç”Ÿæˆèƒ½åŠ›æ¥æ”¹å–„GRï¼Œå´å¿½è§†äº†å®ƒä»¬çš„æ¨ç†èƒ½åŠ›åŒæ ·å¯ä»¥æä¾›å¸®åŠ©ã€‚è¿™å°±å¼•å‘äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šæ˜ç¡®çš„æ¨ç†èƒ½å¦æœ‰ç›ŠäºGRï¼Ÿä¸ºäº†æ¢ç©¶è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆè¿›è¡Œäº†ä¸€é¡¹åˆæ­¥ç ”ç©¶ï¼Œåœ¨æ­¤ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æç¤ºLLMåœ¨è¿›è¡Œçº¦æŸdocidè§£ç ä¹‹å‰å…ˆç”Ÿæˆè‡ªç”±å½¢å¼çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ã€‚å°½ç®¡è¿™ç§æ–¹æ³•ä¼˜äºæ ‡å‡†çš„GRï¼Œä½†ç”Ÿæˆçš„æ¨ç†å¾€å¾€è¿‡äºå†—é•¿ï¼Œä¸”ä¸docidç©ºé—´ä¸å¤ªå¯¹é½ã€‚è¿™äº›å±€é™æ€§ä¿ƒä½¿æˆ‘ä»¬å¼€å‘ä¸€ç§æ›´é€‚åˆGRçš„æ¨ç†æœºåˆ¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘æ£€ç´¢çš„æ¨ç†ï¼ˆR4Rï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé¢å‘GRçš„å¢å¼ºå‹æ¨ç†æ¡†æ¶ï¼Œå®ƒå°†è‡ªç”±å½¢å¼çš„CoTæ¨ç†è½¬æ¢ä¸ºç´§å‡‘ã€ç»“æ„åŒ–çš„æ ¼å¼ï¼Œå¹¶åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­è¿­ä»£åœ°ä¼˜åŒ–æ¨ç†ã€‚R4Rå¢å¼ºäº†ä¸€ç§ç°æœ‰çš„GRæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å…·å¤‡GRæŒ‡ä»¤çš„æ¨ç†LLMã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒR4Ré¦–å…ˆä½¿ç”¨LLMç”Ÿæˆåˆå§‹çš„ç»“æ„åŒ–æ¨ç†ï¼›ç„¶åï¼ŒåŒä¸€LLMäº¤æ›¿è¿›è¡Œï¼ˆiï¼‰ä½¿ç”¨é€‰æ‹©çš„GRæ–¹æ³•è¿›è¡Œçº¦æŸè§£ç ä»¥äº§ç”Ÿå€™é€‰docidsï¼Œï¼ˆiiï¼‰åŸºäºæ£€ç´¢ç»“æœæ›´æ–°æ¨ç†ä»¥æé«˜ä¸‹ä¸€è½®çš„å‡†ç¡®æ€§ã€‚R4Rä¸éœ€è¦é¢å¤–çš„æ¨¡å‹æˆ–è®­ç»ƒï¼Œè€Œæ˜¯ä½¿ç”¨ä¸€ä¸ªLLMåŒæ—¶ä½œä¸ºæ¨ç†ç”Ÿæˆå™¨å’Œæ£€ç´¢å™¨ã€‚åœ¨è‡ªç„¶é—®é¢˜ã€MS MARCOå’Œç°å®ä¸–ç•Œä¸­çš„å•†å“æœç´¢åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†R4Rçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13095v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†ç”Ÿæˆå¼æ£€ç´¢ï¼ˆGRï¼‰é¢†åŸŸçš„ä¸€ä¸ªæ–°å…´é—®é¢˜ï¼Œå³å¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ¨ç†èƒ½åŠ›æ¥æå‡GRçš„æ•ˆæœã€‚æ–‡ç« é¦–å…ˆå›é¡¾äº†ä»¥å¾€ç ”ç©¶ä¸­åªå…³æ³¨LLMçš„ç”Ÿæˆèƒ½åŠ›è€Œå¿½è§†å…¶æ¨ç†èƒ½åŠ›çš„å±€é™æ€§ï¼Œå¹¶æå‡ºä¸€ä¸ªé‡è¦é—®é¢˜ï¼šæ˜ç¡®æ¨ç†èƒ½å¦æœ‰ç›ŠäºGRã€‚ä¸ºæ¢ç©¶è¿™ä¸€é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºReason-for-Retrievalï¼ˆR4Rï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†è‡ªç”±å½¢å¼çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†è½¬åŒ–ä¸ºç´§å‡‘çš„ç»“æ„åŒ–æ ¼å¼ï¼Œå¹¶åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­è¿­ä»£ä¼˜åŒ–æ¨ç†ã€‚R4Ré€šè¿‡åˆ©ç”¨ç»è¿‡æŒ‡å¯¼è®­ç»ƒçš„å…·å¤‡æ¨ç†èƒ½åŠ›çš„LLMæ¥å¢å¼ºç°æœ‰çš„GRæ–¹æ³•ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒR4Ré¦–å…ˆä½¿ç”¨LLMç”Ÿæˆåˆå§‹ç»“æ„åŒ–æ¨ç†ï¼Œç„¶åäº¤æ›¿è¿›è¡Œçº¦æŸè§£ç ä¸åŸºäºæ£€ç´¢ç»“æœçš„æ¨ç†æ›´æ–°ï¼Œä»¥æ”¹è¿›ä¸‹ä¸€è½®çš„æ£€ç´¢ã€‚R4Råªéœ€ä¸€ä¸ªLLMå³å¯åŒæ—¶ä½œä¸ºæ¨ç†ç”Ÿæˆå™¨å’Œæ£€ç´¢å™¨ï¼Œæ— éœ€é¢å¤–çš„æ¨¡å‹æˆ–è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR4Råœ¨è‡ªç„¶é—®é¢˜ã€MS MARCOä»¥åŠçœŸå®ç‰©å“æœç´¢ç­‰å¤šä¸ªæ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼æ£€ç´¢ï¼ˆGRï¼‰å¼€å§‹æ¢ç´¢åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œä¸ä»…ä»…æ˜¯å…¶ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>ç°æœ‰ç ”ç©¶åœ¨GRä¸­å¿½è§†äº†æ¨ç†çš„é‡è¦æ€§ã€‚</li>
<li>æå‡ºReason-for-Retrievalï¼ˆR4Rï¼‰æ¡†æ¶ï¼Œå°†è‡ªç”±å½¢å¼çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†è½¬åŒ–ä¸ºç»“æ„åŒ–æ ¼å¼ã€‚</li>
<li>R4Ræ¡†æ¶åˆ©ç”¨å•ä¸€çš„LLMè¿›è¡Œæ¨ç†ç”Ÿæˆå’Œæ£€ç´¢ï¼Œå¢å¼ºäº†ç°æœ‰çš„GRæ–¹æ³•ã€‚</li>
<li>R4Ré€šè¿‡è¿­ä»£æ›´æ–°æ¨ç†æ¥æå‡æ£€ç´¢æ•ˆæœã€‚</li>
<li>R4Råœ¨è‡ªç„¶é—®é¢˜ã€MS MARCOå’ŒçœŸå®ç‰©å“æœç´¢ç­‰å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9f65b0defe59ec3f4474392d16b1409a~resize:0:q75.jpg?source=1f5c5e47&expiration=1761160008&auth_key=1761160008-0-0-8df7b93288708a1a6e78d7d371e6ac72&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8f7d3a2d2e9b59fa6c5809598e151579~resize:0:q75.jpg?source=1f5c5e47&expiration=1761160015&auth_key=1761160015-0-0-2fe788416723d08327e62ca01d7137b1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-54909d62486394003d7ba95199ba7376~resize:0:q75.jpg?source=1f5c5e47&expiration=1761160021&auth_key=1761160021-0-0-f98c0899533738e8d4a277e775bb8591&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-338aaccc421e2acb56f970c6db9620dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1761160028&auth_key=1761160028-0-0-529c81159f2a7c44bda651c603bff964&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-33162bd3fe6dd5fa8c9a9eea6f9c80a6~resize:0:q75.jpg?source=1f5c5e47&expiration=1761160035&auth_key=1761160035-0-0-97cf10575a8ab39ffa82b16329b23b2d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="UniVideo-Unified-Understanding-Generation-and-Editing-for-Videos"><a href="#UniVideo-Unified-Understanding-Generation-and-Editing-for-Videos" class="headerlink" title="UniVideo: Unified Understanding, Generation, and Editing for Videos"></a>UniVideo: Unified Understanding, Generation, and Editing for Videos</h2><p><strong>Authors:Cong Wei, Quande Liu, Zixuan Ye, Qiulin Wang, Xintao Wang, Pengfei Wan, Kun Gai, Wenhu Chen</strong></p>
<p>Unified multimodal models have shown promising results in multimodal content generation and editing but remain largely limited to the image domain. In this work, we present UniVideo, a versatile framework that extends unified modeling to the video domain. UniVideo adopts a dual-stream design, combining a Multimodal Large Language Model (MLLM) for instruction understanding with a Multimodal DiT (MMDiT) for video generation. This design enables accurate interpretation of complex multimodal instructions while preserving visual consistency. Built on this architecture, UniVideo unifies diverse video generation and editing tasks under a single multimodal instruction paradigm and is jointly trained across them. Extensive experiments demonstrate that UniVideo matches or surpasses state-of-the-art task-specific baselines in text&#x2F;image-to-video generation, in-context video generation and in-context video editing. Notably, the unified design of UniVideo enables two forms of generalization. First, UniVideo supports task composition, such as combining editing with style transfer, by integrating multiple capabilities within a single instruction. Second, even without explicit training on free-form video editing, UniVideo transfers its editing capability from large-scale image editing data to this setting, handling unseen instructions such as green-screening characters or changing materials within a video. Beyond these core capabilities, UniVideo also supports visual-prompt-based video generation, where the MLLM interprets visual prompts and guides the MMDiT during synthesis. To foster future research, we will release our model and code. </p>
<blockquote>
<p>ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤šæ¨¡æ€å†…å®¹ç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢å·²æ˜¾ç¤ºå‡ºæœ‰å‰é€”çš„ç»“æœï¼Œä½†ä¸»è¦å±€é™äºå›¾åƒé¢†åŸŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†UniVideoï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå°†ç»Ÿä¸€å»ºæ¨¡æ‰©å±•åˆ°è§†é¢‘é¢†åŸŸã€‚UniVideoé‡‡ç”¨åŒæµè®¾è®¡ï¼Œç»“åˆç”¨äºæŒ‡ä»¤ç†è§£çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å’Œç”¨äºè§†é¢‘ç”Ÿæˆçš„å¤šæ¨¡æ€DiTï¼ˆMMDiTï¼‰ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿå‡†ç¡®è§£é‡Šå¤æ‚çš„å¤šæ¨¡æ€æŒ‡ä»¤ï¼ŒåŒæ—¶ä¿æŒè§†è§‰ä¸€è‡´æ€§ã€‚åŸºäºè¿™ç§æ¶æ„ï¼ŒUniVideoå°†å„ç§è§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ç»Ÿä¸€åœ¨å•ä¸€çš„å¤šæ¨¡æ€æŒ‡ä»¤èŒƒå¼ä¸‹ï¼Œå¹¶åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¿›è¡Œè”åˆè®­ç»ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒUniVideoåœ¨æ–‡æœ¬&#x2F;å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆã€ä¸Šä¸‹æ–‡è§†é¢‘ç”Ÿæˆå’Œä¸Šä¸‹æ–‡è§†é¢‘ç¼–è¾‘æ–¹é¢è¾¾åˆ°äº†æˆ–è¶…è¶Šäº†æœ€å…ˆè¿›çš„ä»»åŠ¡ç‰¹å®šåŸºå‡†ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒUniVideoçš„ç»Ÿä¸€è®¾è®¡å®ç°äº†ä¸¤ç§å½¢å¼çš„æ³›åŒ–ã€‚é¦–å…ˆï¼ŒUniVideoæ”¯æŒä»»åŠ¡ç»„åˆï¼Œä¾‹å¦‚é€šè¿‡å•ä¸ªæŒ‡ä»¤é›†æˆå¤šç§åŠŸèƒ½æ¥ç»“åˆç¼–è¾‘ä¸é£æ ¼è½¬æ¢ã€‚å…¶æ¬¡ï¼Œå³ä½¿åœ¨æ²¡æœ‰é’ˆå¯¹è‡ªç”±å½¢å¼è§†é¢‘ç¼–è¾‘çš„æ˜ç¡®è®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒUniVideoä¹Ÿèƒ½å°†å…¶ç¼–è¾‘èƒ½åŠ›ä»å¤§è§„æ¨¡å›¾åƒç¼–è¾‘æ•°æ®è½¬ç§»åˆ°è¿™ä¸€ç¯å¢ƒä¸­ï¼Œå¤„ç†æœªè§è¿‡çš„æŒ‡ä»¤ï¼Œå¦‚åœ¨è§†é¢‘ä¸­æŠ åƒæˆ–æ›´æ”¹æè´¨ã€‚é™¤äº†è¿™äº›æ ¸å¿ƒèƒ½åŠ›ä¹‹å¤–ï¼ŒUniVideoè¿˜æ”¯æŒåŸºäºè§†è§‰æç¤ºçš„è§†é¢‘ç”Ÿæˆï¼Œå…¶ä¸­MLLMè§£é‡Šè§†è§‰æç¤ºå¹¶åœ¨åˆæˆè¿‡ç¨‹ä¸­å¼•å¯¼MMDiTã€‚ä¸ºäº†ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„æ¨¡å‹å’Œä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08377v2">PDF</a> Project Website <a target="_blank" rel="noopener" href="https://congwei1230.github.io/UniVideo/">https://congwei1230.github.io/UniVideo/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºUniVideoçš„é€šç”¨æ¡†æ¶ï¼Œå®ƒå°†ç»Ÿä¸€å»ºæ¨¡æ‰©å±•åˆ°è§†é¢‘é¢†åŸŸã€‚UniVideoé‡‡ç”¨åŒæµè®¾è®¡ï¼Œç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¿›è¡ŒæŒ‡ä»¤ç†è§£ä¸å¤šæ¨¡æ€DiTï¼ˆMMDiTï¼‰è¿›è¡Œè§†é¢‘ç”Ÿæˆã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿå‡†ç¡®è§£é‡Šå¤æ‚çš„å¤šåª’ä½“æŒ‡ä»¤ï¼ŒåŒæ—¶ä¿æŒè§†è§‰ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜ï¼ŒUniVideoåœ¨æ–‡æœ¬&#x2F;å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆã€ä¸Šä¸‹æ–‡å†…è§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘ç­‰æ–¹é¢çš„æ€§èƒ½è¾¾åˆ°äº†æˆ–è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚æ­¤å¤–ï¼ŒUniVideoè¿˜æ”¯æŒä»»åŠ¡ç»„åˆå’Œä»å¤§è§„æ¨¡å›¾åƒç¼–è¾‘æ•°æ®è¿ç§»ç¼–è¾‘èƒ½åŠ›ï¼Œèƒ½å¤Ÿå®ç°æœªè§æŒ‡ä»¤çš„å¤„ç†ï¼Œå¦‚ç»¿å¹•æ›¿æ¢è§’è‰²æˆ–æ›´æ”¹è§†é¢‘ä¸­çš„æè´¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniVideoæ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå°†ç»Ÿä¸€å»ºæ¨¡æ‰©å±•åˆ°è§†é¢‘é¢†åŸŸã€‚</li>
<li>é‡‡ç”¨åŒæµè¿è®¾è®¡ï¼Œç»“åˆäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸å¤šæ¨¡æ€DiTã€‚</li>
<li>UniVideoèƒ½å¤Ÿå‡†ç¡®è§£é‡Šå¤æ‚çš„å¤šåª’ä½“æŒ‡ä»¤å¹¶ä¿æŒè§†è§‰ä¸€è‡´æ€§ã€‚</li>
<li>UniVideoåœ¨è§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</li>
<li>UniVideoæ”¯æŒä»»åŠ¡ç»„åˆï¼Œå¦‚ç¼–è¾‘ä¸é£æ ¼è½¬æ¢çš„é›†æˆã€‚</li>
<li>UniVideoèƒ½å¤Ÿä»å¤§è§„æ¨¡å›¾åƒç¼–è¾‘æ•°æ®è¿ç§»ç¼–è¾‘èƒ½åŠ›ï¼Œå¤„ç†æœªè§æŒ‡ä»¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08377">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-de87d4373fc21299d6732cffa381443a~resize:0:q75.jpg?source=1f5c5e47&expiration=1761160042&auth_key=1761160042-0-0-a0ed3d87686f1d066acd38ebd3d1d071&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-feb6de2a3eee823b4a8bdd5c3f39190b~resize:0:q75.jpg?source=1f5c5e47&expiration=1761160050&auth_key=1761160050-0-0-3d4e184fc1d4df3f7911015bce490303&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cf7102d2905b146986a84dd4fe48b5ed~resize:0:q75.jpg?source=1f5c5e47&expiration=1761160057&auth_key=1761160057-0-0-e44c76c8c8f5906fec8dcce3412c75e4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ef7a65e9e26bead6c751633af897d169~resize:0:q75.jpg?source=1f5c5e47&expiration=1761160064&auth_key=1761160064-0-0-f8638ae756919d2c58664115bf0eaf98&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a0435b5b742a232d3b778c6bbbfb804e~resize:0:q75.jpg?source=1f5c5e47&expiration=1761160071&auth_key=1761160071-0-0-ef824d532755dfa97cb989c7d7a40024&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-71861922691bb7c959766abb9ebbe4cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1761160077&auth_key=1761160077-0-0-117a17c4fb74201c60e61a37fd897c77&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="NEXUS-Network-Exploration-for-eXploiting-Unsafe-Sequences-in-Multi-Turn-LLM-Jailbreaks"><a href="#NEXUS-Network-Exploration-for-eXploiting-Unsafe-Sequences-in-Multi-Turn-LLM-Jailbreaks" class="headerlink" title="NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn   LLM Jailbreaks"></a>NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn   LLM Jailbreaks</h2><p><strong>Authors:Javad Rafiei Asl, Sidhant Narula, Mohammad Ghasemigol, Eduardo Blanco, Daniel Takabi</strong></p>
<p>Large Language Models (LLMs) have revolutionized natural language processing but remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks that distribute malicious intent across benign exchanges and bypass alignment mechanisms. Existing approaches often explore the adversarial space poorly, rely on hand-crafted heuristics, or lack systematic query refinement. We present NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular framework for constructing, refining, and executing optimized multi-turn attacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a harmful intent into a structured semantic network of topics, entities, and query chains; (2) a feedback-driven Simulator that iteratively refines and prunes these chains through attacker-victim-judge LLM collaboration using harmfulness and semantic-similarity benchmarks; and (3) a Network Traverser that adaptively navigates the refined query space for real-time attacks. This pipeline uncovers stealthy, high-success adversarial paths across LLMs. On several closed-source and open-source LLMs, NEXUS increases attack success rate by 2.1% to 19.4% over prior methods. Code: <a target="_blank" rel="noopener" href="https://github.com/inspire-lab/NEXUS">https://github.com/inspire-lab/NEXUS</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œä½†ä»ç„¶å®¹æ˜“å—åˆ°è¶Šç‹±æ”»å‡»çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯é‚£äº›å°†æ¶æ„æ„å›¾åˆ†æ•£åœ¨è‰¯æ€§å¯¹è¯ä¸­å¹¶ç»•è¿‡å¯¹é½æœºåˆ¶çš„å¤šè½®è¶Šç‹±æ”»å‡»ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å¯¹å¯¹æŠ—ç©ºé—´æ¢ç´¢ä¸è¶³ï¼Œä¾èµ–æ‰‹å·¥å¯å‘å¼æ–¹æ³•ï¼Œæˆ–ç¼ºä¹ç³»ç»ŸæŸ¥è¯¢ä¼˜åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†NEXUSï¼ˆç”¨äºå¼€å‘ä¸å®‰å…¨åºåˆ—çš„ç½‘ç»œæ¢ç´¢ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºã€ä¼˜åŒ–å’Œæ‰§è¡Œä¼˜åŒ–å¤šè½®æ”»å‡»çš„æ¨¡å—åŒ–æ¡†æ¶ã€‚NEXUSåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰ThoughtNetï¼Œå®ƒæŒ‰å±‚æ¬¡ç»“æ„æ‰©å±•æœ‰å®³æ„å›¾ï¼Œå½¢æˆä¸»é¢˜ã€å®ä½“å’ŒæŸ¥è¯¢é“¾çš„ç»“æ„åŒ–è¯­ä¹‰ç½‘ç»œï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªåé¦ˆé©±åŠ¨çš„æ¨¡æ‹Ÿå™¨ï¼Œé€šè¿‡æ”»å‡»è€…-å—å®³è€…-æ³•å®˜LLMåˆä½œï¼Œä½¿ç”¨æœ‰å®³æ€§å’Œè¯­ä¹‰ç›¸ä¼¼æ€§åŸºå‡†æ¥è¿­ä»£ä¼˜åŒ–å’Œä¿®å‰ªè¿™äº›é“¾ï¼›ä»¥åŠï¼ˆ3ï¼‰ä¸€ä¸ªç½‘ç»œéå†å™¨ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°å¯¼èˆªä¼˜åŒ–åçš„æŸ¥è¯¢ç©ºé—´ä»¥è¿›è¡Œå®æ—¶æ”»å‡»ã€‚æ­¤ç®¡é“åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ­ç¤ºäº†éšè”½ä¸”é«˜åº¦æˆåŠŸçš„å¯¹æŠ—æ€§è·¯å¾„ã€‚åœ¨å¤šä¸ªé—­æºå’Œå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šï¼ŒNEXUSç›¸è¾ƒäºå…ˆå‰æ–¹æ³•æé«˜äº†2.1%è‡³19.4%çš„æ”»å‡»æˆåŠŸç‡ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/inspire-lab/NEXUS">https://github.com/inspire-lab/NEXUS</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03417v2">PDF</a> This paper has been accepted in the main conference proceedings of   the 2025 Conference on Empirical Methods in Natural Language Processing   (EMNLP 2025). Javad Rafiei Asl and Sidhant Narula are co-first authors</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†é©å‘½æ€§çš„è¿›å±•ï¼Œä½†æ˜“å—è¶Šç‹±æ”»å‡»å½±å“ï¼Œå°¤å…¶æ˜¯åˆ†å¸ƒå¼æ¶æ„æ„å›¾çš„å¤šè½®è¶Šç‹±æ”»å‡»ï¼Œèƒ½å¤Ÿç»•è¿‡å¯¹é½æœºåˆ¶ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†NEXUSï¼ˆç½‘ç»œæ¢ç´¢ä¸å®‰å…¨åºåˆ—ï¼‰æ¡†æ¶ï¼ŒåŒ…å«æ„å»ºã€ä¼˜åŒ–å’Œæ‰§è¡Œå¤šè½®æ”»å‡»çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚NEXUSæ¡†æ¶åŒ…æ‹¬ï¼šThoughtNetï¼ˆå±‚æ¬¡æ‰©å±•æœ‰å®³æ„å›¾ä¸ºç»“æ„åŒ–è¯­ä¹‰ç½‘ç»œï¼‰ã€åé¦ˆé©±åŠ¨æ¨¡æ‹Ÿå™¨ï¼ˆé€šè¿‡æ”»å‡»è€…-å—å®³è€…-æ³•å®˜LLMåˆä½œè¿›è¡Œç²¾ç‚¼å’Œä¿®å‰ªï¼‰å’Œç½‘ç»œéå†å™¨ï¼ˆè‡ªé€‚åº”å¯¼èˆªç²¾ç‚¼æŸ¥è¯¢ç©ºé—´è¿›è¡Œå®æ—¶æ”»å‡»ï¼‰ã€‚è¯¥ç®¡é“åœ¨å¤šä¸ªé—­æºå’Œå¼€æºLLMä¸Šæé«˜äº†æ”»å‡»æˆåŠŸç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMè™½ç„¶æ¨åŠ¨äº†NLPçš„è¿›æ­¥ï¼Œä½†ä»é¢ä¸´è¶Šç‹±æ”»å‡»çš„é£é™©ï¼Œå°¤å…¶æ˜¯å¤šè½®è¶Šç‹±æ”»å‡»ã€‚</li>
<li>NEXUSæ¡†æ¶æ—¨åœ¨æ„å»ºã€ä¼˜åŒ–å’Œæ‰§è¡Œå¤šè½®æ”»å‡»ï¼Œä»¥åº”å¯¹è¿™ä¸€é—®é¢˜ã€‚</li>
<li>ThoughtNetæ¨¡å—èƒ½å¤Ÿå°†æœ‰å®³æ„å›¾å±‚æ¬¡åŒ–ä¸ºç»“æ„åŒ–è¯­ä¹‰ç½‘ç»œã€‚</li>
<li>åé¦ˆé©±åŠ¨æ¨¡æ‹Ÿå™¨é€šè¿‡æ”»å‡»è€…-å—å®³è€…-æ³•å®˜LLMåˆä½œè¿›è¡Œæ„å›¾é“¾çš„ç²¾ç‚¼å’Œä¿®å‰ªã€‚</li>
<li>ç½‘ç»œéå†å™¨æ¨¡å—èƒ½å¤Ÿè‡ªé€‚åº”åœ°å¯¼èˆªç²¾ç‚¼æŸ¥è¯¢ç©ºé—´ï¼Œå®ç°å®æ—¶æ”»å‡»ã€‚</li>
<li>NEXUSæ¡†æ¶æé«˜äº†åœ¨å¤šä¸ªLLMä¸Šçš„æ”»å‡»æˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03417">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7bf8a9213f0c968ecd96d89c26190960~resize:0:q75.jpg?source=1f5c5e47&expiration=1761160085&auth_key=1761160085-0-0-8390a3f312b8a668b3c4e29d061ece5c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bd3b2e5dec3f6903007a2fab38390bdd~resize:0:q75.jpg?source=1f5c5e47&expiration=1761160092&auth_key=1761160092-0-0-07955fa13921e8b7ae9473913a61b035&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-adb5b21f4d7c257e2bfbde6c96442c14~resize:0:q75.jpg?source=1f5c5e47&expiration=1761160099&auth_key=1761160099-0-0-d507a689c9b8e555b36c36a4f7e687e1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-67f39784f4df5a9769697ac1287bb377~resize:0:q75.jpg?source=1f5c5e47&expiration=1761160106&auth_key=1761160106-0-0-48769231bda28df8a5c39152a9f35b65&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-23/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-23/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-23/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-dc053db36ab6b97965999356848168cd~resize:0:q75.jpg?source=1f5c5e47&expiration=1761161810&auth_key=1761161810-0-0-b5c97185ab511972594075d3eb8dc980&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-23  Search Self-play Pushing the Frontier of Agent Capability without   Supervision
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-23/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-8bedb78797f5c0f238db7ac71b82ed53~resize:0:q75.jpg?source=1f5c5e47&expiration=1761156894&auth_key=1761156894-0-0-b17864af552523973c2b7d8fa023aff5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-23  Towards Faithful and Controllable Personalization via Critique-Post-Edit   Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31373.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
