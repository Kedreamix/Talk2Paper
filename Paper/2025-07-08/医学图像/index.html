<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-08  VHU-Net Variational Hadamard U-Net for Body MRI Bias Field Correction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-37f628b13b37a070dce6d6ea394c4542.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    49 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-08-æ›´æ–°"><a href="#2025-07-08-æ›´æ–°" class="headerlink" title="2025-07-08 æ›´æ–°"></a>2025-07-08 æ›´æ–°</h1><h2 id="VHU-Net-Variational-Hadamard-U-Net-for-Body-MRI-Bias-Field-Correction"><a href="#VHU-Net-Variational-Hadamard-U-Net-for-Body-MRI-Bias-Field-Correction" class="headerlink" title="VHU-Net: Variational Hadamard U-Net for Body MRI Bias Field Correction"></a>VHU-Net: Variational Hadamard U-Net for Body MRI Bias Field Correction</h2><p><strong>Authors:Xin Zhu, Ahmet Enis Cetin, Gorkem Durak, Batuhan Gundogdu, Ziliang Hong, Hongyi Pan, Ertugrul Aktas, Elif Keles, Hatice Savas, Aytekin Oto, Hiten Patel, Adam B. Murphy, Ashley Ross, Frank Miller, Baris Turkbey, Ulas Bagci</strong></p>
<p>Bias field artifacts in magnetic resonance imaging (MRI) scans introduce spatially smooth intensity inhomogeneities that degrade image quality and hinder downstream analysis. To address this challenge, we propose a novel variational Hadamard U-Net (VHU-Net) for effective body MRI bias field correction. The encoder comprises multiple convolutional Hadamard transform blocks (ConvHTBlocks), each integrating convolutional layers with a Hadamard transform (HT) layer. Specifically, the HT layer performs channel-wise frequency decomposition to isolate low-frequency components, while a subsequent scaling layer and semi-soft thresholding mechanism suppress redundant high-frequency noise. To compensate for the HT layerâ€™s inability to model inter-channel dependencies, the decoder incorporates an inverse HT-reconstructed transformer block, enabling global, frequency-aware attention for the recovery of spatially consistent bias fields. The stacked decoder ConvHTBlocks further enhance the capacity to reconstruct the underlying ground-truth bias field. Building on the principles of variational inference, we formulate a new evidence lower bound (ELBO) as the training objective, promoting sparsity in the latent space while ensuring accurate bias field estimation. Comprehensive experiments on abdominal and prostate MRI datasets demonstrate the superiority of VHU-Net over existing state-of-the-art methods in terms of intensity uniformity, signal fidelity, and tissue contrast. Moreover, the corrected images yield substantial downstream improvements in segmentation accuracy. Our framework offers computational efficiency, interpretability, and robust performance across multi-center datasets, making it suitable for clinical deployment. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ‰«æä¸­çš„åç½®åœºä¼ªå½±å¼•å…¥äº†ç©ºé—´å¹³æ»‘å¼ºåº¦ä¸å‡åŒ€æ€§ï¼Œé™ä½äº†å›¾åƒè´¨é‡ï¼Œé˜»ç¢äº†ä¸‹æ¸¸åˆ†æã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å˜åˆ†å“ˆè¾¾ç›Uå½¢ç½‘ç»œï¼ˆVHU-Netï¼‰ï¼Œç”¨äºæœ‰æ•ˆçš„ä½“MRIåç½®åœºæ ¡æ­£ã€‚ç¼–ç å™¨ç”±å¤šä¸ªå·ç§¯å“ˆè¾¾ç›å˜æ¢å—ï¼ˆConvHTBlocksï¼‰ç»„æˆï¼Œæ¯ä¸ªå—éƒ½ç»“åˆäº†å·ç§¯å±‚å’Œå“ˆè¾¾ç›å˜æ¢ï¼ˆHTï¼‰å±‚ã€‚å…·ä½“æ¥è¯´ï¼ŒHTå±‚æ‰§è¡Œé€šé“é¢‘ç‡åˆ†è§£ï¼Œä»¥éš”ç¦»ä½é¢‘åˆ†é‡ï¼Œè€Œéšåçš„ç¼©æ”¾å±‚å’ŒåŠè½¯é˜ˆå€¼æœºåˆ¶æŠ‘åˆ¶å†—ä½™çš„é«˜é¢‘å™ªå£°ã€‚ä¸ºäº†å¼¥è¡¥HTå±‚æ— æ³•å»ºæ¨¡é€šé“é—´ä¾èµ–æ€§çš„ä¸è¶³ï¼Œè§£ç å™¨é‡‡ç”¨äº†ä¸€ä¸ªé€†HTé‡å»ºçš„å˜æ¢å—ï¼Œèƒ½å¤Ÿå®ç°å…¨å±€é¢‘ç‡æ„ŸçŸ¥æ³¨æ„åŠ›ä»¥æ¢å¤ç©ºé—´ä¸€è‡´çš„åç½®åœºã€‚å †å çš„è§£ç å™¨ConvHTå—è¿›ä¸€æ­¥å¢å¼ºäº†é‡å»ºæ½œåœ¨çœŸå®åç½®åœºçš„èƒ½åŠ›ã€‚åŸºäºå˜åˆ†æ¨ç†çš„åŸç†ï¼Œæˆ‘ä»¬å°†æ–°çš„è¯æ®ä¸‹é™ï¼ˆELBOï¼‰ä½œä¸ºè®­ç»ƒç›®æ ‡ï¼Œä»¥ä¿ƒè¿›æ½œåœ¨ç©ºé—´çš„ç¨€ç–æ€§ï¼ŒåŒæ—¶ç¡®ä¿å‡†ç¡®çš„åç½®åœºä¼°è®¡ã€‚åœ¨è…¹éƒ¨å’Œå‰åˆ—è…ºMRIæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒVHU-Netåœ¨å¼ºåº¦å‡åŒ€æ€§ã€ä¿¡å·ä¿çœŸåº¦å’Œç»„ç»‡å¯¹æ¯”åº¦æ–¹é¢ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæ ¡æ­£åçš„å›¾åƒåœ¨åˆ†å‰²ç²¾åº¦ä¸Šäº§ç”Ÿäº†æ˜¾è‘—çš„ä¸‹æ¸¸æ”¹è¿›ã€‚æˆ‘ä»¬çš„æ¡†æ¶å…·æœ‰è®¡ç®—æ•ˆç‡é«˜ã€å¯è§£é‡Šæ€§å¼ºã€å¤šä¸­å¿ƒæ•°æ®é›†æ€§èƒ½ç¨³å¥ç­‰ä¼˜ç‚¹ï¼Œé€‚åˆä¸´åºŠéƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19181v2">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ‰«æä¸­çš„åç½®åœºä¼ªå½±é—®é¢˜ï¼Œæå‡ºä¸€ç§æ–°å‹çš„å˜åˆ†Hadamard U-Netï¼ˆVHU-Netï¼‰è¿›è¡Œæœ‰æ•ˆçš„ä½“MRIåç½®åœºæ ¡æ­£ã€‚è¯¥ç½‘ç»œé€šè¿‡ç»“åˆå·ç§¯å±‚å’ŒHadamardå˜æ¢å±‚ï¼Œå®ç°äº†å¯¹ä½é¢‘é¢‘è°±çš„åˆ†è§£ä»¥åŠå¯¹å†—ä½™é«˜é¢‘å™ªå£°çš„æŠ‘åˆ¶ã€‚åˆ©ç”¨é€†HTé‡å»ºçš„è§£ç å™¨å—å®ç°å…¨å±€é¢‘ç‡æ„ŸçŸ¥æ³¨æ„åŠ›ï¼Œæ¢å¤ç©ºé—´ä¸€è‡´çš„åç½®åœºã€‚é€šè¿‡å˜åˆ†æ¨æ–­åŸç†åˆ¶å®šæ–°çš„è¯æ®ä¸‹é™ï¼ˆELBOï¼‰ä½œä¸ºè®­ç»ƒç›®æ ‡ï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸­ä¿ƒè¿›ç¨€ç–æ€§å¹¶ç¡®ä¿å‡†ç¡®çš„åç½®åœºä¼°è®¡ã€‚åœ¨è…¹éƒ¨å’Œå‰åˆ—è…ºMRIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVHU-Netåœ¨å¼ºåº¦å‡åŒ€æ€§ã€ä¿¡å·ä¿çœŸåº¦å’Œç»„ç»‡å¯¹æ¯”åº¦æ–¹é¢ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚æ ¡æ­£åçš„å›¾åƒåœ¨åˆ†å‰²ç²¾åº¦ä¸Šæœ‰æ˜¾è‘—æé«˜ï¼Œä¸”è¯¥æ¡†æ¶å…·æœ‰è®¡ç®—æ•ˆç‡é«˜ã€å¯è§£é‡Šæ€§å¼ºå’Œå¤šä¸­å¿ƒæ•°æ®é›†è¡¨ç°ç¨³å¥ç­‰ä¼˜ç‚¹ï¼Œé€‚åˆä¸´åºŠéƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­çš„åç½®åœºä¼ªå½±ä¼šå¼•å…¥ç©ºé—´å¹³æ»‘å¼ºåº¦ä¸å‡åŒ€æ€§ï¼Œå½±å“å›¾åƒè´¨é‡å’Œåç»­åˆ†æã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹çš„å˜åˆ†Hadamard U-Netï¼ˆVHU-Netï¼‰è¿›è¡ŒMRIåç½®åœºæ ¡æ­£ã€‚</li>
<li>VHU-Netç»“åˆå·ç§¯å±‚å’ŒHadamardå˜æ¢å±‚ï¼Œæœ‰æ•ˆåˆ†è§£ä½é¢‘é¢‘è°±å¹¶æŠ‘åˆ¶å†—ä½™é«˜é¢‘å™ªå£°ã€‚</li>
<li>é€†HTé‡å»ºçš„è§£ç å™¨å—å®ç°å…¨å±€é¢‘ç‡æ„ŸçŸ¥æ³¨æ„åŠ›ï¼Œæœ‰åŠ©äºæ¢å¤ç©ºé—´ä¸€è‡´çš„åç½®åœºã€‚</li>
<li>é€šè¿‡å˜åˆ†æ¨æ–­åŸç†åˆ¶å®šæ–°çš„è¯æ®ä¸‹é™ï¼ˆELBOï¼‰ä½œä¸ºè®­ç»ƒç›®æ ‡ï¼Œç¡®ä¿å‡†ç¡®ä¼°è®¡åç½®åœºã€‚</li>
<li>åœ¨è…¹éƒ¨å’Œå‰åˆ—è…ºMRIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVHU-Netåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19181">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b051859f4f8b03f9169c93836dc80702.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-735c0028a47fa260ff2fc9eeb7c147ae.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AI-Flow-Perspectives-Scenarios-and-Approaches"><a href="#AI-Flow-Perspectives-Scenarios-and-Approaches" class="headerlink" title="AI Flow: Perspectives, Scenarios, and Approaches"></a>AI Flow: Perspectives, Scenarios, and Approaches</h2><p><strong>Authors:Hongjun An, Wenhan Hu, Sida Huang, Siqi Huang, Ruanjun Li, Yuanzhi Liang, Jiawei Shao, Yiliang Song, Zihan Wang, Cheng Yuan, Chi Zhang, Hongyuan Zhang, Wenhao Zhuang, Xuelong Li</strong></p>
<p>Pioneered by the foundational information theory by Claude Shannon and the visionary framework of machine intelligence by Alan Turing, the convergent evolution of information and communication technologies (IT&#x2F;CT) has created an unbroken wave of connectivity and computation. This synergy has sparked a technological revolution, now reaching its peak with large artificial intelligence (AI) models that are reshaping industries and redefining human-machine collaboration. However, the realization of ubiquitous intelligence faces considerable challenges due to substantial resource consumption in large models and high communication bandwidth demands. To address these challenges, AI Flow has been introduced as a multidisciplinary framework that integrates cutting-edge IT and CT advancements, with a particular emphasis on the following three key points. First, device-edge-cloud framework serves as the foundation, which integrates end devices, edge servers, and cloud clusters to optimize scalability and efficiency for low-latency model inference. Second, we introduce the concept of familial models, which refers to a series of different-sized models with aligned hidden features, enabling effective collaboration and the flexibility to adapt to varying resource constraints and dynamic scenarios. Third, connectivity- and interaction-based intelligence emergence is a novel paradigm of AI Flow. By leveraging communication networks to enhance connectivity, the collaboration among AI models across heterogeneous nodes achieves emergent intelligence that surpasses the capability of any single model. The innovations of AI Flow provide enhanced intelligence, timely responsiveness, and ubiquitous accessibility to AI services, paving the way for the tighter fusion of AI techniques and communication systems. </p>
<blockquote>
<p>åœ¨ä¿¡æ¯ç†è®ºå…ˆé©±å…‹åŠ³å¾·Â·é¦™å†œå’Œäººå·¥æ™ºèƒ½å…ˆé©±è‰¾ä¼¦Â·å›¾çµå¼€åˆ›æ€§ç†è®ºæ¡†æ¶ä¸‹ï¼Œä¿¡æ¯å’Œé€šä¿¡æŠ€æœ¯çš„èåˆï¼ˆIT&#x2F;CTï¼‰ä¸æ–­æ¼”åŒ–ï¼Œæ¨åŠ¨äº†è¿æ¥å’Œè®¡ç®—çš„ä¸é—´æ–­æµªæ½®ã€‚è¿™ç§ååŒä½œç”¨å¼•å‘äº†ä¸€åœºæŠ€æœ¯é©å‘½ï¼Œç°åœ¨éšç€å¤§å‹äººå·¥æ™ºèƒ½æ¨¡å‹çš„å´›èµ·è¾¾åˆ°é¡¶å³°ï¼Œæ­£åœ¨é‡å¡‘äº§ä¸šå¹¶é‡æ–°å®šä¹‰äººæœºåä½œã€‚ç„¶è€Œï¼Œå®ç°æ— å¤„ä¸åœ¨çš„æ™ºèƒ½é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå¤§å‹æ¨¡å‹éœ€è¦å¤§é‡çš„èµ„æºæ¶ˆè€—å’Œé«˜é€šä¿¡å¸¦å®½éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå¼•å…¥äº†AI Flowè¿™ä¸€è·¨å­¦ç§‘æ¡†æ¶ï¼Œæ•´åˆäº†å‰æ²¿çš„ITå’ŒCTæŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯ä»¥ä¸‹ä¸‰ä¸ªå…³é”®ç‚¹ã€‚é¦–å…ˆï¼Œè®¾å¤‡è¾¹ç¼˜äº‘æ¡†æ¶æ˜¯æ ¸å¿ƒåŸºç¡€ï¼Œå®ƒå°†ç»ˆç«¯è®¾å¤‡ã€è¾¹ç¼˜æœåŠ¡å™¨å’Œäº‘é›†ç¾¤æ•´åˆåœ¨ä¸€èµ·ï¼Œä¼˜åŒ–å¯æ‰©å±•æ€§ï¼Œæé«˜ä½å»¶è¿Ÿæ¨¡å‹æ¨ç†çš„æ•ˆç‡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†å®¶æ—æ¨¡å‹çš„æ¦‚å¿µï¼Œè¿™æ˜¯æŒ‡å…·æœ‰å¯¹é½éšè—ç‰¹å¾çš„ä¸€ç³»åˆ—ä¸åŒå¤§å°çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°æœ‰æ•ˆçš„åä½œå’Œé€‚åº”ä¸åŒçš„èµ„æºçº¦æŸå’ŒåŠ¨æ€åœºæ™¯ã€‚ç¬¬ä¸‰ï¼ŒåŸºäºè¿æ¥å’Œäº¤äº’çš„æ™ºèƒ½æ¶Œç°æ˜¯AI Flowçš„æ–°èŒƒå¼ã€‚é€šè¿‡åˆ©ç”¨é€šä¿¡ç½‘ç»œå¢å¼ºè¿æ¥æ€§ï¼Œä¸åŒèŠ‚ç‚¹ä¸Šçš„äººå·¥æ™ºèƒ½æ¨¡å‹ä¹‹é—´çš„åä½œå®ç°äº†æ¶Œç°æ™ºèƒ½ï¼Œè¶…è¶Šäº†ä»»ä½•å•ä¸€æ¨¡å‹çš„èƒ½åŠ›ã€‚AI Flowçš„åˆ›æ–°æä¾›äº†å¢å¼ºçš„æ™ºèƒ½ã€åŠæ—¶å“åº”å’Œæ— å¤„ä¸åœ¨çš„äººå·¥æ™ºèƒ½æœåŠ¡è®¿é—®èƒ½åŠ›ï¼Œä¸ºäººå·¥æ™ºèƒ½æŠ€æœ¯å’Œé€šä¿¡ç³»ç»Ÿä¹‹é—´çš„ç´§å¯†èåˆé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12479v2">PDF</a> Authors are with Institute of Artificial Intelligence (TeleAI), China   Telecom, China. Author names are listed alphabetically by surname. This work   was conducted at TeleAI, facilitated by Dr. Jiawei Shao (e-mail:   <a href="mailto:&#x73;&#x68;&#97;&#111;&#106;&#x77;&#x32;&#64;&#x63;&#104;&#x69;&#x6e;&#97;&#116;&#101;&#x6c;&#x65;&#x63;&#111;&#x6d;&#46;&#99;&#110;">&#x73;&#x68;&#97;&#111;&#106;&#x77;&#x32;&#64;&#x63;&#104;&#x69;&#x6e;&#97;&#116;&#101;&#x6c;&#x65;&#x63;&#111;&#x6d;&#46;&#99;&#110;</a>) under the leadership of Prof. Xuelong Li. The   corresponding author is Prof. Xuelong Li (e-mail: xuelong <a href="mailto:&#108;&#x69;&#64;&#105;&#101;&#101;&#x65;&#46;&#x6f;&#x72;&#x67;">&#108;&#x69;&#64;&#105;&#101;&#101;&#x65;&#46;&#x6f;&#x72;&#x67;</a>), the   CTO and Chief Scientist of China Telecom</p>
<p><strong>Summary</strong><br>     ä¿¡æ¯è®ºåˆ›å§‹äººå…‹åŠ³å¾·Â·é¦™å†œä¸äººå·¥æ™ºèƒ½å…ˆé©±è‰¾ä¼¦Â·å›¾çµå¼€åˆ›æ€§çš„ç†è®ºä¸ºä¿¡æ¯ä¸é€šä¿¡æŠ€æœ¯ï¼ˆIT&#x2F;CTï¼‰çš„èåˆæ¼”å˜å¥ å®šäº†åŸºç¡€ã€‚è¿™ä¸€ååŒè¿›åŒ–åˆ›é€ äº†è¿æ¥å’Œè®¡ç®—çš„æŒç»­æµªæ½®ï¼Œå¼•å‘äº†ä¸€åœºæŠ€æœ¯é©å‘½ã€‚å¦‚ä»Šï¼Œå¤§å‹äººå·¥æ™ºèƒ½æ¨¡å‹æ­£é‡å¡‘äº§ä¸šå¹¶é‡æ–°å®šä¹‰äººæœºåä½œã€‚ç„¶è€Œï¼Œå®ç°æ™®éæ™ºèƒ½é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå¦‚å¤§å‹æ¨¡å‹çš„é«˜èµ„æºæ¶ˆè€—å’Œé€šä¿¡å¸¦å®½çš„é«˜éœ€æ±‚ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼ŒAI Flowä½œä¸ºä¸€ä¸ªè·¨å­¦ç§‘æ¡†æ¶åº”è¿è€Œç”Ÿï¼Œé›†æˆäº†æœ€å‰æ²¿çš„ITå’ŒCTæŠ€æœ¯ï¼Œé‡ç‚¹åŒ…æ‹¬ï¼šè®¾å¤‡è¾¹ç¼˜äº‘æ¡†æ¶ã€å®¶æ—æ¨¡å‹çš„æ¦‚å¿µä»¥åŠåŸºäºè¿æ¥å’Œäº¤äº’çš„æ™ºåŠ›æ¶Œç°æ–°æ¨¡å¼ã€‚è¿™äº›åˆ›æ–°ä¸ºå¢å¼ºæ™ºèƒ½ã€åŠæ—¶å“åº”å’Œæ— å¤„ä¸åœ¨çš„AIæœåŠ¡è®¿é—®æä¾›äº†æ¡ä»¶ï¼Œä¸ºäººå·¥æ™ºèƒ½æŠ€æœ¯å’Œé€šä¿¡ç³»ç»Ÿçš„ç´§å¯†èåˆé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä¿¡æ¯ä¸é€šä¿¡æŠ€æœ¯èåˆæ¼”å˜åŸºäºé¦™å†œçš„ä¿¡æ¯è®ºå’Œå›¾çµçš„äººå·¥æ™ºèƒ½æ¡†æ¶ã€‚</li>
<li>å¤§å‹äººå·¥æ™ºèƒ½æ¨¡å‹æ­£é‡å¡‘äº§ä¸šå¹¶é‡æ–°å®šä¹‰äººæœºåä½œï¼Œä½†å®ç°æ™®éæ™ºèƒ½é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>AI Flowæ¡†æ¶é›†æˆITå’ŒCTæŠ€æœ¯ï¼Œä»¥åº”å¯¹èµ„æºæ¶ˆè€—å’Œé€šä¿¡å¸¦å®½çš„æŒ‘æˆ˜ã€‚</li>
<li>è®¾å¤‡è¾¹ç¼˜äº‘æ¡†æ¶æ˜¯AI Flowçš„åŸºç¡€ï¼Œæ•´åˆç»ˆç«¯ã€è¾¹ç¼˜æœåŠ¡å™¨å’Œäº‘é›†ç¾¤ï¼Œä¼˜åŒ–å¯æ‰©å±•æ€§å’Œæ•ˆç‡ã€‚</li>
<li>å®¶æ—æ¨¡å‹æ¦‚å¿µåŒ…å«ä¸åŒå¤§å°çš„æ¨¡å‹ï¼Œå…·æœ‰å¯¹é½çš„éšè—ç‰¹å¾ï¼Œé€‚åº”èµ„æºçº¦æŸå’ŒåŠ¨æ€åœºæ™¯ã€‚</li>
<li>åŸºäºè¿æ¥å’Œäº¤äº’çš„æ™ºåŠ›æ¶Œç°æ˜¯AI Flowçš„æ–°å‹æ¨¡å¼ï¼Œé€šè¿‡é€šä¿¡ç½‘ç»œå¢å¼ºè¿æ¥ï¼Œå®ç°æ¨¡å‹é—´åä½œæ¶Œç°æ™ºèƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12479">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-16e02722b74a8c83e18f1b9751914ad7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b71919176b1263eda39eef8c48b37f9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80d185e19cb77bc915d71b3e6863804e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Non-rigid-Motion-Correction-for-MRI-Reconstruction-via-Coarse-To-Fine-Diffusion-Models"><a href="#Non-rigid-Motion-Correction-for-MRI-Reconstruction-via-Coarse-To-Fine-Diffusion-Models" class="headerlink" title="Non-rigid Motion Correction for MRI Reconstruction via Coarse-To-Fine   Diffusion Models"></a>Non-rigid Motion Correction for MRI Reconstruction via Coarse-To-Fine   Diffusion Models</h2><p><strong>Authors:Frederic Wang, Jonathan I. Tamir</strong></p>
<p>Magnetic Resonance Imaging (MRI) is highly susceptible to motion artifacts due to the extended acquisition times required for k-space sampling. These artifacts can compromise diagnostic utility, particularly for dynamic imaging. We propose a novel alternating minimization framework that leverages a bespoke diffusion model to jointly reconstruct and correct non-rigid motion-corrupted k-space data. The diffusion model uses a coarse-to-fine denoising strategy to capture large overall motion and reconstruct the lower frequencies of the image first, providing a better inductive bias for motion estimation than that of standard diffusion models. We demonstrate the performance of our approach on both real-world cine cardiac MRI datasets and complex simulated rigid and non-rigid deformations, even when each motion state is undersampled by a factor of 64x. Additionally, our method is agnostic to sampling patterns, anatomical variations, and MRI scanning protocols, as long as some low frequency components are sampled during each motion state. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ç”±äºkç©ºé—´é‡‡æ ·æ‰€éœ€çš„é•¿æ—¶é—´é‡‡é›†è€Œæ˜“å—åˆ°è¿åŠ¨ä¼ªå½±çš„å½±å“ã€‚è¿™äº›ä¼ªå½±å¯èƒ½æŸå®³è¯Šæ–­çš„æ•ˆç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€æˆåƒä¸­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„äº¤æ›¿æœ€å°åŒ–æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ä¸“é—¨çš„æ‰©æ•£æ¨¡å‹æ¥è”åˆé‡å»ºå’Œçº æ­£éåˆšæ€§è¿åŠ¨å—æŸçš„kç©ºé—´æ•°æ®ã€‚è¯¥æ‰©æ•£æ¨¡å‹é‡‡ç”¨ç”±ç²—åˆ°ç»†çš„é™å™ªç­–ç•¥ï¼Œé¦–å…ˆæ•è·æ•´ä½“å¤§è¿åŠ¨å¹¶é‡å»ºå›¾åƒçš„ä½é¢‘éƒ¨åˆ†ï¼Œä¸ºè¿åŠ¨ä¼°è®¡æä¾›äº†æ¯”æ ‡å‡†æ‰©æ•£æ¨¡å‹æ›´å¥½çš„å½’çº³åç½®ã€‚æˆ‘ä»¬åœ¨ç°å®ä¸–ç•Œçš„ç”µå½±å¿ƒè„MRIæ•°æ®é›†å’Œå¤æ‚çš„æ¨¡æ‹Ÿåˆšæ€§å’Œéåˆšæ€§å˜å½¢ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•æ€§èƒ½ï¼Œå³ä½¿åœ¨æ¯ç§è¿åŠ¨çŠ¶æ€æŒ‰64å€æ¬ é‡‡æ ·çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯¹äºé‡‡æ ·æ¨¡å¼ã€è§£å‰–å·®å¼‚å’ŒMRIæ‰«æåè®®æŒå¼€æ”¾æ€§æ€åº¦ï¼Œåªè¦åœ¨æ¯ä¸ªè¿åŠ¨çŠ¶æ€ä¸‹é‡‡æ ·ä¸€äº›ä½é¢‘æˆåˆ†å³å¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15057v3">PDF</a> ICIP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºäº¤æ›¿æœ€å°åŒ–æ¡†æ¶å’Œä¸“ç”¨æ‰©æ•£æ¨¡å‹çš„éåˆšæ€§è¿åŠ¨è…èš€kç©ºé—´æ•°æ®è”åˆé‡å»ºå’Œæ ¡æ­£æ–¹æ³•ã€‚è¯¥æ‰©æ•£æ¨¡å‹é‡‡ç”¨ç”±ç²—åˆ°ç»†çš„é™å™ªç­–ç•¥ï¼Œå…ˆæ•æ‰æ•´ä½“å¤§è¿åŠ¨å¹¶é‡å»ºå›¾åƒçš„ä½é¢‘éƒ¨åˆ†ï¼Œä¸ºè¿åŠ¨ä¼°è®¡æä¾›æ›´å¥½çš„å½’çº³åç½®ã€‚è¯¥æ–¹æ³•åœ¨çœŸå®ç”µå½±å¿ƒè„MRIæ•°æ®é›†å’Œå¤æ‚çš„æ¨¡æ‹Ÿåˆšæ€§å’Œéåˆšæ€§å˜å½¢ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œå³ä½¿åœ¨æ¯ä¸ªè¿åŠ¨çŠ¶æ€æ¬ é‡‡æ ·64å€çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ­¤æ–¹æ³•å¯¹äºé‡‡æ ·æ¨¡å¼ã€è§£å‰–å˜å¼‚å’ŒMRIæ‰«æåè®®å…·æœ‰é€šç”¨æ€§ï¼Œåªè¦åœ¨æ¯ä¸ªè¿åŠ¨çŠ¶æ€ä¸‹é‡‡æ ·ä¸€äº›ä½é¢‘æˆåˆ†å³å¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MRIå¯¹è¿åŠ¨ä¼ªå½±é«˜åº¦æ•æ„Ÿï¼Œç”±äºkç©ºé—´é‡‡æ ·çš„æ‰©å±•é‡‡é›†æ—¶é—´ï¼Œè¿åŠ¨ä¼ªå½±ä¼šå½±å“è¯Šæ–­æ•ˆç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€æˆåƒä¸­ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„äº¤æ›¿æœ€å°åŒ–æ¡†æ¶ï¼Œç»“åˆäº†ä¸“ç”¨çš„æ‰©æ•£æ¨¡å‹æ¥è”åˆé‡å»ºå’Œæ ¡æ­£éåˆšæ€§è¿åŠ¨è…èš€çš„kç©ºé—´æ•°æ®ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹é‡‡ç”¨ç”±ç²—åˆ°ç»†çš„é™å™ªç­–ç•¥ï¼Œé¦–å…ˆé‡å»ºå›¾åƒçš„ä½é¢‘éƒ¨åˆ†ï¼Œä¸ºè¿åŠ¨ä¼°è®¡æä¾›æ›´å¥½çš„åŸºç¡€ã€‚</li>
<li>æ–¹æ³•åœ¨çœŸå®ç”µå½±å¿ƒè„MRIæ•°æ®é›†å’Œæ¨¡æ‹Ÿçš„åˆšæ€§å’Œéåˆšæ€§å˜å½¢ä¸Šè¿›è¡Œäº†æ¼”ç¤ºï¼Œæ€§èƒ½è‰¯å¥½ã€‚</li>
<li>å³ä½¿åœ¨ä¸¥é‡çš„æ¬ é‡‡æ ·æƒ…å†µä¸‹ï¼ˆæ¯ä¸ªè¿åŠ¨çŠ¶æ€æ¬ é‡‡æ ·64å€ï¼‰ï¼Œè¯¥æ–¹æ³•ä»ç„¶æœ‰æ•ˆã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹é‡‡æ ·æ¨¡å¼ã€è§£å‰–å˜å¼‚å’ŒMRIæ‰«æåè®®å…·æœ‰é€šç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15057">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dcb73323e1357571f95576cf827ff061.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40c450a3001a4b039f9561669912de35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7a89deb44f02ea190b093675187992e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1879d5afbf5f04752a5b3d36a1b7585.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1b84582bd7f57ab6c58ed143a97ad8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57052dc9379f13a30206187b3a442480.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4118c0aa3cf8ed7d5566417769df99db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d2a90d028abcb96a0153659c4c1beca.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="XGeM-A-Multi-Prompt-Foundation-Model-for-Multimodal-Medical-Data-Generation"><a href="#XGeM-A-Multi-Prompt-Foundation-Model-for-Multimodal-Medical-Data-Generation" class="headerlink" title="XGeM: A Multi-Prompt Foundation Model for Multimodal Medical Data   Generation"></a>XGeM: A Multi-Prompt Foundation Model for Multimodal Medical Data   Generation</h2><p><strong>Authors:Daniele Molino, Francesco Di Feola, Eliodoro Faiella, Deborah Fazzini, Domiziana Santucci, Linlin Shen, Valerio Guarrasi, Paolo Soda</strong></p>
<p>The adoption of Artificial Intelligence in medical imaging holds great promise, yet it remains hindered by challenges such as data scarcity, privacy concerns, and the need for robust multimodal integration. While recent advances in generative modeling have enabled high-quality synthetic data generation, existing approaches are often limited to unimodal, unidirectional synthesis and therefore lack the ability to jointly synthesize multiple modalities while preserving clinical consistency. To address this challenge, we introduce XGeM, a 6.77-billion-parameter multimodal generative model designed to support flexible, any-to-any synthesis between medical data modalities. XGeM constructs a shared latent space via contrastive learning and introduces a novel Multi-Prompt Training strategy, enabling conditioning on arbitrary subsets of input modalities. This design allows the model to adapt to heterogeneous clinical inputs and generate multiple outputs jointly, preserving both semantic and structural coherence. We extensively validate XGeM: first we benchmark it against five competitors on the MIMIC-CXR dataset, a state-of-the-art dataset for multi-view Chest X-ray and radiological report generation. Secondly, we perform a Visual Turing Test with expert radiologists to assess the realism and clinical relevance of the generated data, ensuring alignment with real-world scenarios. Finally, we show how XGeM can support key medical data challenges such as anonymization, class imbalance, and data scarcity, underscoring its utility as a foundation model for medical data synthesis. Project page is at <a target="_blank" rel="noopener" href="https://cosbidev.github.io/XGeM/">https://cosbidev.github.io/XGeM/</a>. </p>
<blockquote>
<p>å°†äººå·¥æ™ºèƒ½åº”ç”¨äºåŒ»å­¦æˆåƒé¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œä½†ä»é¢ä¸´ç€æ•°æ®ç¨€ç¼ºã€éšç§æ‹…å¿§ä»¥åŠéœ€è¦ç¨³å¥çš„å¤šæ¨¡å¼é›†æˆç­‰æŒ‘æˆ˜ã€‚è™½ç„¶æœ€è¿‘ç”Ÿæˆå»ºæ¨¡æŠ€æœ¯çš„è¿›æ­¥å·²å®ç°äº†é«˜è´¨é‡åˆæˆæ•°æ®çš„ç”Ÿæˆï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸ä»…é™äºå•æ¨¡å¼ã€å•å‘çš„åˆæˆï¼Œå› æ­¤ç¼ºä¹åŒæ—¶åˆæˆå¤šç§æ¨¡å¼å¹¶ä¿æŒä¸´åºŠä¸€è‡´æ€§çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†XGeMï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰67.7äº¿å‚æ•°çš„å¤šæ¨¡å¼ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨æ”¯æŒåŒ»å­¦æ•°æ®æ¨¡å¼ä¹‹é—´çš„çµæ´»ã€ä»»æ„åˆ°ä»»æ„çš„åˆæˆã€‚XGeMé€šè¿‡å¯¹æ¯”å­¦ä¹ æ„å»ºäº†ä¸€ä¸ªå…±äº«æ½œåœ¨ç©ºé—´ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„å¤šæç¤ºè®­ç»ƒç­–ç•¥ï¼Œèƒ½å¤Ÿå®ç°ä»¥è¾“å…¥æ¨¡å¼çš„ä»»æ„å­é›†ä¸ºæ¡ä»¶ã€‚è¿™ç§è®¾è®¡ä½¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”å¤šæ ·åŒ–çš„ä¸´åºŠè¾“å…¥ï¼Œå¹¶è”åˆç”Ÿæˆå¤šä¸ªè¾“å‡ºï¼ŒåŒæ—¶ä¿æŒè¯­ä¹‰å’Œç»“æ„çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å¯¹XGeMè¿›è¡Œäº†å¹¿æ³›éªŒè¯ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šä¸äº”ç§ç«äº‰å¯¹æ‰‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤šè§†å›¾èƒ¸éƒ¨Xå°„çº¿å’Œæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„æœ€å…ˆè¿›æ•°æ®é›†ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä¸ä¸“å®¶æ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡Œäº†ä¸€åœºè§†è§‰å›¾çµæµ‹è¯•ï¼Œä»¥è¯„ä¼°ç”Ÿæˆæ•°æ®çš„ç°å®ä¸»ä¹‰å’Œä¸´åºŠç›¸å…³æ€§ï¼Œç¡®ä¿å…¶ä¸çœŸå®ä¸–ç•Œåœºæ™¯çš„ä¸€è‡´æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†XGeMå¦‚ä½•æ”¯æŒåŒ»å­¦æ•°æ®çš„å…³é”®æŒ‘æˆ˜ï¼Œå¦‚åŒ¿ååŒ–ã€ç±»åˆ«ä¸å¹³è¡¡å’Œæ•°æ®ç¨€ç¼ºï¼Œå¼ºè°ƒå…¶åœ¨åŒ»å­¦æ•°æ®åˆæˆåŸºç¡€æ¨¡å‹ä¸­çš„å®ç”¨æ€§ã€‚é¡¹ç›®é¡µé¢ä¸ºï¼š[<a target="_blank" rel="noopener" href="https://cosbidev.github.io/XGeM/]">https://cosbidev.github.io/XGeM/]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04614v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>äººå·¥æ™ºèƒ½åœ¨åŒ»å­¦æˆåƒé¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†ä»é¢ä¸´æ•°æ®ç¨€ç¼ºã€éšç§é¡¾è™‘å’Œå¤šæ¨¡æ€èåˆç­‰æŒ‘æˆ˜ã€‚è¿‘æœŸç”Ÿæˆå»ºæ¨¡çš„è¿›æ­¥æ¨åŠ¨äº†é«˜è´¨é‡åˆæˆæ•°æ®çš„ç”Ÿæˆï¼Œä½†ç°æœ‰æ–¹æ³•å¤§å¤šå±€é™äºå•æ¨¡æ€ã€å•å‘åˆæˆï¼Œæ— æ³•åœ¨å¤šæ¨¡æ€é—´è¿›è¡Œè”åˆåˆæˆå¹¶ä¿æŒä¸´åºŠä¸€è‡´æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºXGeMï¼Œä¸€ä¸ª6.77äº¿å‚æ•°çš„å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ï¼Œæ”¯æŒåŒ»å­¦æ•°æ®æ¨¡æ€é—´çš„çµæ´»ä»»æ„åˆæˆã€‚XGeMé€šè¿‡å¯¹æ¯”å­¦ä¹ æ„å»ºå…±äº«æ½œåœ¨ç©ºé—´ï¼Œå¹¶å¼•å…¥å…¨æ–°å¤šæç¤ºè®­ç»ƒç­–ç•¥ï¼Œå¯åŸºäºä»»æ„è¾“å…¥æ¨¡æ€å­é›†è¿›è¡Œæ¡ä»¶è®¾ç½®ã€‚æ­¤è®¾è®¡ä½¿æ¨¡å‹èƒ½é€‚åº”å¼‚è´¨ä¸´åºŠè¾“å…¥ï¼Œè”åˆç”Ÿæˆå¤šä¸ªè¾“å‡ºï¼ŒåŒæ—¶ä¿æŒè¯­ä¹‰å’Œç»“æ„çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å…¨é¢éªŒè¯äº†XGeMçš„æ€§èƒ½ï¼šé¦–å…ˆåœ¨MIMIC-CXRæ•°æ®é›†ä¸Šä¸äº”ç§ç«å“è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œè¯¥æ•°æ®é›†ç”¨äºå¤šè§†è§’èƒ¸éƒ¨Xå…‰ä¸æ”¾å°„æŠ¥å‘Šç”Ÿæˆï¼Œå¤„äºä¸šç•Œé¢†å…ˆåœ°ä½ï¼›å…¶æ¬¡ï¼Œæˆ‘ä»¬è¿›è¡Œè§†è§‰å›¾çµæµ‹è¯•ï¼Œé‚€è¯·ä¸“å®¶æ”¾å°„åŒ»å¸ˆè¯„ä¼°ç”Ÿæˆæ•°æ®çš„çœŸå®æ€§å’Œä¸´åºŠç›¸å…³æ€§ï¼Œç¡®ä¿ä¸çœŸå®åœºæ™¯å¯¹é½ï¼›æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†XGeMå¦‚ä½•æ”¯æŒåŒ»å­¦æ•°æ®çš„å…³é”®æŒ‘æˆ˜ï¼Œå¦‚åŒ¿ååŒ–ã€ç±»åˆ«ä¸å¹³è¡¡å’Œæ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå‡¸æ˜¾å…¶ä½œä¸ºåŒ»å­¦æ•°æ®åˆæˆåŸºç¡€æ¨¡å‹çš„å®ç”¨æ€§ã€‚é¡¹ç›®é¡µé¢åœ°å€ä¸º<a target="_blank" rel="noopener" href="https://cosbidev.github.io/XGeM/">é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨åŒ»å­¦æˆåƒä¸­é¢ä¸´æ•°æ®ç¨€ç¼ºã€éšç§é¡¾è™‘å’Œå¤šæ¨¡æ€èåˆçš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰ç”Ÿæˆå»ºæ¨¡æ–¹æ³•å¤§å¤šå±€é™äºå•æ¨¡æ€æ•°æ®åˆæˆï¼Œç¼ºä¹å¤šæ¨¡æ€é—´çš„è”åˆåˆæˆèƒ½åŠ›ã€‚</li>
<li>XGeMæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ï¼Œæ”¯æŒåŒ»å­¦æ•°æ®æ¨¡æ€é—´çš„ä»»æ„åˆæˆï¼Œæ„å»ºå…±äº«æ½œåœ¨ç©ºé—´å¹¶å¼•å…¥å¤šæç¤ºè®­ç»ƒç­–ç•¥ã€‚</li>
<li>XGeMå¯ä»¥åœ¨å¼‚è´¨ä¸´åºŠè¾“å…¥æ¡ä»¶ä¸‹å·¥ä½œï¼Œè”åˆç”Ÿæˆå¤šä¸ªè¾“å‡ºå¹¶ä¿æŒä¸´åºŠä¸€è‡´æ€§ã€‚</li>
<li>XGeMåœ¨MIMIC-CXRæ•°æ®é›†ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶ä¸å¤šç§ç«å“å¯¹æ¯”è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>é€šè¿‡è§†è§‰å›¾çµæµ‹è¯•ï¼ŒXGeMç”Ÿæˆçš„æ•°æ®å…·æœ‰çœŸå®æ€§å’Œä¸´åºŠç›¸å…³æ€§ã€‚</li>
<li>XGeMæœ‰åŠ©äºè§£å†³åŒ»å­¦æ•°æ®çš„å…³é”®æŒ‘æˆ˜ï¼Œå¦‚åŒ¿ååŒ–ã€ç±»åˆ«ä¸å¹³è¡¡å’Œæ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04614">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3b2ebf115139e72e8e0a87be1e7f335a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Bi-modality-medical-images-synthesis-by-a-bi-directional-discrete-process-matching-method"><a href="#Bi-modality-medical-images-synthesis-by-a-bi-directional-discrete-process-matching-method" class="headerlink" title="Bi-modality medical images synthesis by a bi-directional discrete   process matching method"></a>Bi-modality medical images synthesis by a bi-directional discrete   process matching method</h2><p><strong>Authors:Zhe Xiong, Qiaoqiao Ding, Xiaoqun Zhang</strong></p>
<p>Recently, medical image synthesis gains more and more popularity, along with the rapid development of generative models. Medical image synthesis aims to generate an unacquired image modality, often from other observed data modalities. Synthesized images can be used for clinical diagnostic assistance, data augmentation for model training and validation or image quality improving. In the meanwhile, the flow-based models are among the successful generative models for the ability of generating realistic and high-quality synthetic images. However, most flow-based models require to calculate flow ordinary different equation (ODE) evolution steps in synthesis process, for which the performances are significantly limited by heavy computation time due to a large number of time iterations. In this paper, we propose a novel flow-based model, namely bi-directional Discrete Process Matching (Bi-DPM) to accomplish the bi-modality image synthesis tasks. Different to other flow matching based models, we propose to utilize both forward and backward ODE flows and enhance the consistency on the intermediate images over a few discrete time steps, resulting in a synthesis process maintaining high-quality generations for both modalities under the guidance of paired data. Our experiments on three datasets of MRI T1&#x2F;T2 and CT&#x2F;MRI demonstrate that Bi-DPM outperforms other state-of-the-art flow-based methods for bi-modality image synthesis, delivering higher image quality with accurate anatomical regions. </p>
<blockquote>
<p>éšç€ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼ŒåŒ»å­¦å›¾åƒåˆæˆè¶Šæ¥è¶Šå—åˆ°æ¬¢è¿ã€‚åŒ»å­¦å›¾åƒåˆæˆçš„ç›®æ ‡æ˜¯ä»å…¶ä»–å·²è§‚å¯Ÿåˆ°çš„æ•°æ®æ¨¡æ€ç”Ÿæˆæœªè·å¾—çš„å›¾åƒæ¨¡æ€ã€‚åˆæˆçš„å›¾åƒå¯ç”¨äºä¸´åºŠè¯Šæ–­è¾…åŠ©ã€æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯çš„æ•°æ®å¢å¼ºæˆ–å›¾åƒè´¨é‡æ”¹è¿›ã€‚ä¸æ­¤åŒæ—¶ï¼ŒåŸºäºæµçš„æ¨¡å‹æ˜¯ç”Ÿæˆç°å®å’Œé«˜è´¨é‡åˆæˆå›¾åƒçš„æˆåŠŸçš„ç”Ÿæˆæ¨¡å‹ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°åŸºäºæµçš„æ¨¡å‹éœ€è¦åœ¨åˆæˆè¿‡ç¨‹ä¸­è®¡ç®—æµæ™®é€šå¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰çš„æ¼”åŒ–æ­¥éª¤ï¼Œç”±äºå¤§é‡æ—¶é—´è¿­ä»£ï¼Œå…¶æ€§èƒ½å—åˆ°è®¡ç®—æ—¶é—´é•¿çš„é™åˆ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæµçš„æ¨¡å‹ï¼Œå³åŒå‘ç¦»æ•£è¿‡ç¨‹åŒ¹é…ï¼ˆBi-DPMï¼‰ï¼Œä»¥å®ŒæˆåŒå‘å›¾åƒåˆæˆä»»åŠ¡ã€‚ä¸å…¶ä»–åŸºäºæµåŒ¹é…çš„æ¨¡å‹ä¸åŒï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨æ­£å‘å’Œåå‘ODEæµï¼Œå¹¶å¢å¼ºå‡ ä¸ªç¦»æ•£æ—¶é—´æ­¥éª¤ä¸­ä¸­é—´å›¾åƒçš„ä¸€è‡´æ€§ï¼Œä»è€Œåœ¨ä¿è¯é…å¯¹æ•°æ®å¼•å¯¼çš„æƒ…å†µä¸‹ï¼Œä½¿ä¸¤ç§æ¨¡æ€çš„åˆæˆè¿‡ç¨‹ä¿æŒé«˜è´¨é‡ç”Ÿæˆã€‚æˆ‘ä»¬åœ¨MRI T1&#x2F;T2å’ŒCT&#x2F;MRIçš„ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒBi-DPMåœ¨åŒå‘å›¾åƒåˆæˆæ–¹é¢ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„åŸºäºæµçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰å‡†ç¡®è§£å‰–åŒºåŸŸçš„é«˜è´¨é‡å›¾åƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.03977v3">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹åŒå‘ç¦»æ•£è¿‡ç¨‹åŒ¹é…ï¼ˆBi-DPMï¼‰æµç¨‹æ¨¡å‹ï¼Œç”¨äºå®ŒæˆåŒæ¨¡æ€å›¾åƒåˆæˆä»»åŠ¡ã€‚è¯¥æ¨¡å‹åˆ©ç”¨æ­£å‘å’Œåå‘ODEæµï¼Œæé«˜ä¸­é—´å›¾åƒåœ¨å‡ ä¸ªç¦»æ•£æ—¶é—´æ­¥éª¤ä¸Šçš„ä¸€è‡´æ€§ï¼Œåœ¨é…å¯¹æ•°æ®çš„æŒ‡å¯¼ä¸‹ï¼Œä¿æŒä¸¤ç§æ¨¡æ€çš„é«˜è´¨é‡ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼ŒBi-DPMåœ¨åŒæ¨¡æ€å›¾åƒåˆæˆä¸Šä¼˜äºå…¶ä»–å…ˆè¿›æµç¨‹æ¨¡å‹ï¼Œæä¾›å‡†ç¡®è§£å‰–åŒºåŸŸçš„æ›´é«˜è´¨é‡å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—å›¾åƒåˆæˆéšç€ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•è€Œè¶Šæ¥è¶Šå—æ¬¢è¿ã€‚</li>
<li>åŒ»ç–—å›¾åƒåˆæˆçš„ç›®æ ‡æ˜¯ä»å…¶ä»–è§‚å¯Ÿåˆ°çš„æ•°æ®æ¨¡æ€ç”Ÿæˆæœªè·å¾—çš„å›¾åƒæ¨¡æ€ã€‚</li>
<li>åˆæˆå›¾åƒå¯ç”¨äºä¸´åºŠè¯Šæ–­è¾…åŠ©ã€æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯çš„æ•°æ®å¢å¼ºä»¥åŠå›¾åƒè´¨é‡æ”¹è¿›ã€‚</li>
<li>æµæ¨¡å‹åœ¨ç”ŸæˆçœŸå®é«˜è´¨é‡çš„åˆæˆå›¾åƒæ–¹é¢è¡¨ç°å‡ºæˆåŠŸã€‚</li>
<li>å¤§å¤šæ•°æµç¨‹æ¨¡å‹éœ€è¦åœ¨åˆæˆè¿‡ç¨‹ä¸­è®¡ç®—æµå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰è¿›åŒ–æ­¥éª¤ï¼Œä½†ç”±äºå¤§é‡æ—¶é—´è¿­ä»£ï¼Œæ€§èƒ½å—åˆ°è®¡ç®—æ—¶é—´é•¿çš„é™åˆ¶ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹åŒå‘ç¦»æ•£è¿‡ç¨‹åŒ¹é…ï¼ˆBi-DPMï¼‰æµç¨‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨æ­£å‘å’Œåå‘ODEæµï¼Œæé«˜ä¸­é—´å›¾åƒä¸€è‡´æ€§ï¼Œå¹¶åœ¨é…å¯¹æ•°æ®çš„æŒ‡å¯¼ä¸‹ä¿æŒé«˜è´¨é‡ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.03977">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2be9bbad66dbe4c8ecb0859470b0849f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-855c1d198747c5cb423a68534d0404b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-728a8e1fb0a350849f2ceab254792d71.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e9fd1cc9166d4660b1441ee3669c6e9.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="NuSegDG-Integration-of-Heterogeneous-Space-and-Gaussian-Kernel-for-Domain-Generalized-Nuclei-Segmentation"><a href="#NuSegDG-Integration-of-Heterogeneous-Space-and-Gaussian-Kernel-for-Domain-Generalized-Nuclei-Segmentation" class="headerlink" title="NuSegDG: Integration of Heterogeneous Space and Gaussian Kernel for   Domain-Generalized Nuclei Segmentation"></a>NuSegDG: Integration of Heterogeneous Space and Gaussian Kernel for   Domain-Generalized Nuclei Segmentation</h2><p><strong>Authors:Zhenye Lou, Qing Xu, Zekun Jiang, Xiangjian He, Zhen Chen, Yi Wang, Chenxin Li, Maggie M. He, Wenting Duan</strong></p>
<p>Domain-generalized nuclei segmentation refers to the generalizability of models to unseen domains based on knowledge learned from source domains and is challenged by various image conditions, cell types, and stain strategies. Recently, the Segment Anything Model (SAM) has made great success in universal image segmentation by interactive prompt modes (e.g., point and box). Despite its strengths, the original SAM presents limited adaptation to medical images. Moreover, SAM requires providing manual bounding box prompts for each object to produce satisfactory segmentation masks, so it is laborious in nuclei segmentation scenarios. To address these limitations, we propose a domain-generalizable framework for nuclei image segmentation, abbreviated to NuSegDG. Specifically, we first devise a Heterogeneous Space Adapter (HS-Adapter) to learn multi-dimensional feature representations of different nuclei domains by injecting a small number of trainable parameters into the image encoder of SAM. To alleviate the labor-intensive requirement of manual prompts, we introduce a Gaussian-Kernel Prompt Encoder (GKP-Encoder) to generate density maps driven by a single point, which guides segmentation predictions by mixing position prompts and semantic prompts. Furthermore, we present a Two-Stage Mask Decoder (TSM-Decoder) to effectively convert semantic masks to instance maps without the manual demand for morphological shape refinement. Based on our experimental evaluations, the proposed NuSegDG demonstrates state-of-the-art performance in nuclei instance segmentation, exhibiting superior domain generalization capabilities. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/xq141839/NuSegDG">https://github.com/xq141839/NuSegDG</a>. </p>
<blockquote>
<p>é¢†åŸŸæ³›åŒ–æ ¸åˆ†å‰²ï¼ˆDomain-Generalized Nuclei Segmentationï¼‰æŒ‡çš„æ˜¯æ¨¡å‹åœ¨æœªè§é¢†åŸŸä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŸºäºä»æºé¢†åŸŸå­¦åˆ°çš„çŸ¥è¯†ï¼Œå®ƒé¢ä¸´ç€å„ç§å›¾åƒæ¡ä»¶ã€ç»†èƒç±»å‹å’ŒæŸ“è‰²ç­–ç•¥çš„æŒ‘æˆ˜ã€‚æœ€è¿‘ï¼Œé€šè¿‡äº¤äº’å¼æç¤ºæ¨¡å¼ï¼ˆå¦‚ç‚¹å’Œæ¡†ï¼‰ï¼ŒSegment Anything Modelï¼ˆSAMï¼‰åœ¨é€šç”¨å›¾åƒåˆ†å‰²æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸã€‚å°½ç®¡å…¶å¼ºå¤§ï¼Œä½†åŸå§‹SAMå¯¹åŒ»å­¦å›¾åƒçš„é€‚åº”æ€§æœ‰é™ã€‚æ­¤å¤–ï¼ŒSAMéœ€è¦ä¸ºæ¯ä¸ªå¯¹è±¡æä¾›æ‰‹åŠ¨è¾¹ç•Œæ¡†æç¤ºä»¥äº§ç”Ÿä»¤äººæ»¡æ„çš„åˆ†å‰²æ©ç ï¼Œå› æ­¤åœ¨ç»†èƒæ ¸åˆ†å‰²åœºæ™¯ä¸­å¾ˆç¹çã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºç»†èƒæ ¸å›¾åƒåˆ†å‰²çš„é¢†åŸŸæ³›åŒ–æ¡†æ¶ï¼Œç®€ç§°ä¸ºNuSegDGã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ä¸ªHeterogeneous Space Adapterï¼ˆHS-Adapterï¼‰ï¼Œé€šè¿‡å‘SAMçš„å›¾åƒç¼–ç å™¨æ³¨å…¥å°‘é‡å¯è®­ç»ƒå‚æ•°ï¼Œå­¦ä¹ ä¸åŒæ ¸é¢†åŸŸçš„å¤šç»´ç‰¹å¾è¡¨ç¤ºã€‚ä¸ºäº†å‡è½»ç¹ççš„æ‰‹åŠ¨æç¤ºè¦æ±‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†Gaussian-Kernel Prompt Encoderï¼ˆGKP-Encoderï¼‰æ¥ç”Ÿæˆç”±å•ç‚¹é©±åŠ¨çš„æ¦‚ç‡å¯†åº¦å›¾ï¼Œé€šè¿‡æ··åˆä½ç½®æç¤ºå’Œè¯­ä¹‰æç¤ºæ¥æŒ‡å¯¼åˆ†å‰²é¢„æµ‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§Two-Stage Mask Decoderï¼ˆTSM-Decoderï¼‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†è¯­ä¹‰æ©ç è½¬æ¢ä¸ºå®ä¾‹å›¾ï¼Œæ— éœ€å¯¹å½¢æ€è¿›è¡Œæ‰‹åŠ¨ç»†åŒ–ã€‚æˆ‘ä»¬çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæ‰€æå‡ºçš„NuSegDGåœ¨ç»†èƒæ ¸å®ä¾‹åˆ†å‰²æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œè¡¨ç°å‡ºå“è¶Šé¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xq141839/NuSegDG">https://github.com/xq141839/NuSegDG</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11787v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é¢†åŸŸæ³›åŒ–ç»†èƒæ ¸åˆ†å‰²æŒ‡çš„æ˜¯æ¨¡å‹åœ¨æœªè§é¢†åŸŸä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŸºäºä»æºé¢†åŸŸå­¦åˆ°çš„çŸ¥è¯†ã€‚é¢ä¸´å„ç§å›¾åƒæ¡ä»¶ã€ç»†èƒç±»å‹å’ŒæŸ“è‰²ç­–ç•¥çš„æŒ‘æˆ˜ã€‚æœ€è¿‘ï¼ŒSegment Anything Modelï¼ˆSAMï¼‰åœ¨é€šç”¨å›¾åƒåˆ†å‰²ä¸­é€šè¿‡äº¤äº’å¼æç¤ºæ¨¡å¼ï¼ˆå¦‚ç‚¹å’Œæ¡†ï¼‰å–å¾—äº†å·¨å¤§æˆåŠŸã€‚ç„¶è€Œï¼ŒåŸå§‹SAMåœ¨åŒ»å­¦å›¾åƒä¸Šçš„é€‚åº”æ€§æœ‰é™ã€‚æ­¤å¤–ï¼ŒSAMéœ€è¦ä¸ºæ¯ä¸ªå¯¹è±¡æä¾›æ‰‹åŠ¨è¾¹ç•Œæ¡†æç¤ºä»¥äº§ç”Ÿæ»¡æ„çš„åˆ†å‰²æ©è†œï¼Œå› æ­¤åœ¨ç»†èƒæ ¸åˆ†å‰²åœºæ™¯ä¸­å¾ˆç¹çã€‚é’ˆå¯¹è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºç»†èƒæ ¸å›¾åƒåˆ†å‰²çš„é¢†åŸŸæ³›åŒ–æ¡†æ¶ï¼Œç®€ç§°ä¸ºNuSegDGã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªHeterogeneous Space Adapterï¼ˆHS-Adapterï¼‰æ¥å­¦ä¹ ä¸åŒç»†èƒæ ¸é¢†åŸŸçš„å¤šç»´ç‰¹å¾è¡¨ç¤ºï¼Œæ–¹æ³•æ˜¯é€šè¿‡åœ¨SAMçš„å›¾åƒç¼–ç å™¨ä¸­æ³¨å…¥å°‘é‡å¯è®­ç»ƒå‚æ•°ã€‚ä¸ºäº†å‡è½»å¯¹æ‰‹åŠ¨æç¤ºçš„åŠ³åŠ¨å¼ºåº¦è¦æ±‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†Gaussian-Kernel Prompt Encoderï¼ˆGKP-Encoderï¼‰æ¥ç”Ÿæˆç”±å•ç‚¹é©±åŠ¨çš„æ¦‚ç‡å¯†åº¦å›¾ï¼Œé€šè¿‡æ··åˆä½ç½®æç¤ºå’Œè¯­ä¹‰æç¤ºæ¥æŒ‡å¯¼åˆ†å‰²é¢„æµ‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªTwo-Stage Mask Decoderï¼ˆTSM-Decoderï¼‰ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å°†è¯­ä¹‰æ©è†œè½¬æ¢ä¸ºå®ä¾‹å›¾ï¼Œæ— éœ€è¿›è¡Œå½¢æ€å­¦å½¢çŠ¶ç²¾ä¿®çš„æ‰‹åŠ¨éœ€æ±‚ã€‚æˆ‘ä»¬çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæ‰€æå‡ºçš„NuSegDGåœ¨ç»†èƒæ ¸å®ä¾‹åˆ†å‰²æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œé¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é¢†åŸŸæ³›åŒ–ç»†èƒæ ¸åˆ†å‰²æ˜¯åŒ»å­¦å›¾åƒåˆ†æçš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ï¼Œæ¶‰åŠæ¨¡å‹åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„é€‚åº”æ€§ã€‚</li>
<li>Segment Anything Modelï¼ˆSAMï¼‰åœ¨é€šç”¨å›¾åƒåˆ†å‰²ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨åŒ»å­¦å›¾åƒä¸­çš„é€‚åº”æ€§æœ‰é™ã€‚</li>
<li>æå‡ºçš„NuSegDGæ¡†æ¶é€šè¿‡å¼•å…¥Heterogeneous Space Adapterï¼ˆHS-Adapterï¼‰å¢å¼ºäº†æ¨¡å‹å¯¹ç»†èƒæ ¸é¢†åŸŸçš„é€‚åº”æ€§ã€‚</li>
<li>Gaussian-Kernel Prompt Encoderï¼ˆGKP-Encoderï¼‰å‡è½»äº†æ‰‹åŠ¨æç¤ºçš„éœ€æ±‚ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå¯†åº¦å›¾è¿›è¡Œåˆ†å‰²é¢„æµ‹ã€‚</li>
<li>Two-Stage Mask Decoderï¼ˆTSM-Decoderï¼‰æœ‰æ•ˆå°†è¯­ä¹‰æ©è†œè½¬æ¢ä¸ºå®ä¾‹å›¾ï¼Œå‡å°‘äº†å½¢æ€å­¦å½¢çŠ¶ç²¾ä¿®çš„éœ€è¦ã€‚</li>
<li>NuSegDGæ¡†æ¶åœ¨ç»†èƒæ ¸å®ä¾‹åˆ†å‰²æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œé¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.11787">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-deb4f6b430c005484586b0e6b7c13ce6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e85e45bc2b3de3d2c25f60becee13d8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Anatomical-Foundation-Models-for-Brain-MRIs"><a href="#Anatomical-Foundation-Models-for-Brain-MRIs" class="headerlink" title="Anatomical Foundation Models for Brain MRIs"></a>Anatomical Foundation Models for Brain MRIs</h2><p><strong>Authors:Carlo Alberto Barbano, Matteo Brunello, Benoit Dufumier, Marco Grangetto</strong></p>
<p>Deep Learning (DL) in neuroimaging has become increasingly relevant for detecting neurological conditions and neurodegenerative disorders. One of the most predominant biomarkers in neuroimaging is represented by brain age, which has been shown to be a good indicator for different conditions, such as Alzheimerâ€™s Disease. Using brain age for weakly supervised pre-training of DL models in transfer learning settings has also recently shown promising results, especially when dealing with data scarcity of different conditions. On the other hand, anatomical information of brain MRIs (e.g. cortical thickness) can provide important information for learning good representations that can be transferred to many downstream tasks. In this work, we propose AnatCL, an anatomical foundation model for brain MRIs that i.) leverages anatomical information in a weakly contrastive learning approach, and ii.) achieves state-of-the-art performances across many different downstream tasks. To validate our approach we consider 12 different downstream tasks for the diagnosis of different conditions such as Alzheimerâ€™s Disease, autism spectrum disorder, and schizophrenia. Furthermore, we also target the prediction of 10 different clinical assessment scores using structural MRI data. Our findings show that incorporating anatomical information during pre-training leads to more robust and generalizable representations. Pre-trained models can be found at: <a target="_blank" rel="noopener" href="https://github.com/EIDOSLAB/AnatCL">https://github.com/EIDOSLAB/AnatCL</a>. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰åœ¨ç¥ç»æˆåƒä¸­å¯¹äºæ£€æµ‹ç¥ç»ç³»ç»Ÿç–¾ç—…å’Œç¥ç»é€€è¡Œæ€§ç–¾ç—…å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç¥ç»æˆåƒä¸­æœ€ä¸»è¦çš„ç”Ÿç‰©æ ‡å¿—ç‰©ä¹‹ä¸€å°±æ˜¯è„‘å¹´é¾„ï¼Œå®ƒå·²è¢«è¯æ˜æ˜¯ä¸åŒç–¾ç—…ï¼ˆå¦‚é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼‰çš„è‰¯å¥½æŒ‡æ ‡ã€‚åœ¨è¿ç§»å­¦ä¹ ç¯å¢ƒä¸­ï¼Œä½¿ç”¨è„‘å¹´é¾„å¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œå¼±ç›‘ç£é¢„è®­ç»ƒä¹Ÿæ˜¾ç¤ºå‡ºæœ‰å‰é€”çš„ç»“æœï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ä¸åŒç–¾ç—…çš„ç¨€ç¼ºæ•°æ®æ—¶ã€‚å¦ä¸€æ–¹é¢ï¼Œå¤§è„‘MRIçš„è§£å‰–å­¦ä¿¡æ¯ï¼ˆä¾‹å¦‚çš®å±‚åšåº¦ï¼‰å¯ä»¥ä¸ºå­¦ä¹ è‰¯å¥½çš„è¡¨ç¤ºæä¾›é‡è¦ä¿¡æ¯ï¼Œè¿™äº›è¡¨ç¤ºå¯ä»¥è½¬ç§»åˆ°è®¸å¤šä¸‹æ¸¸ä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AnatCLï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤§è„‘MRIçš„è§£å‰–å­¦åŸºç¡€æ¨¡å‹ï¼Œå®ƒä¸€ï¼‰ä»¥å¼±å¯¹æ¯”å­¦ä¹ çš„æ–¹å¼åˆ©ç”¨è§£å‰–å­¦ä¿¡æ¯ï¼ŒäºŒï¼‰åœ¨è®¸å¤šä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬è€ƒè™‘äº†12ä¸ªä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œç”¨äºè¯Šæ–­ä¸åŒçš„ç–¾ç—…ï¼Œå¦‚é˜¿å°”èŒ¨æµ·é»˜ç—…ã€è‡ªé—­ç—‡è°±ç³»éšœç¢å’Œç²¾ç¥åˆ†è£‚ç—‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è‡´åŠ›äºä½¿ç”¨ç»“æ„MRIæ•°æ®é¢„æµ‹10ç§ä¸åŒçš„ä¸´åºŠè¯„ä¼°åˆ†æ•°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­èå…¥è§£å‰–å­¦ä¿¡æ¯ä¼šå¯¼è‡´æ›´ç¨³å¥å’Œå¯æ¨å¹¿çš„è¡¨ç¤ºã€‚é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/EIDOSLAB/AnatCL%E3%80%82">https://github.com/EIDOSLAB/AnatCLã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.07079v4">PDF</a> Updated version; added ablation study</p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ åœ¨ç¥ç»å½±åƒä¸­çš„åº”ç”¨å¯¹äºæ£€æµ‹ç¥ç»æ€§ç–¾ç—…å’Œç¥ç»é€€è¡Œæ€§ç–¾ç—…è¶Šæ¥è¶Šé‡è¦ã€‚è„‘å¹´é¾„æ˜¯ç¥ç»å½±åƒä¸­æœ€é‡è¦çš„ç”Ÿç‰©æ ‡å¿—ç‰©ä¹‹ä¸€ï¼Œå¯ç”¨äºé¢„æµ‹å¤šç§ç–¾ç—…ï¼Œå¦‚é˜¿å°”èŒ¨æµ·é»˜ç—‡ã€‚åˆ©ç”¨è„‘å¹´é¾„å¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œå¼±ç›‘ç£é¢„è®­ç»ƒï¼Œåœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹å±•ç°å‡ºè‰¯å¥½çš„åº”ç”¨å‰æ™¯ã€‚å¦ä¸€æ–¹é¢ï¼Œè„‘éƒ¨MRIçš„ç»“æ„ä¿¡æ¯ï¼ˆå¦‚çš®å±‚åšåº¦ï¼‰å¯ä»¥ä¸ºå­¦ä¹ è‰¯å¥½è¡¨ç¤ºæä¾›é‡è¦ä¿¡æ¯ï¼Œè¿™äº›è¡¨ç¤ºå¯åº”ç”¨äºè®¸å¤šä¸‹æ¸¸ä»»åŠ¡ã€‚æœ¬å·¥ä½œæå‡ºAnatCLæ¨¡å‹ï¼Œé€šè¿‡å¼±å¯¹æ¯”å­¦ä¹ åˆ©ç”¨ç»“æ„ä¿¡æ¯ï¼Œå¹¶åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚è¯¥æ¨¡å‹å¯¹é˜¿å°”èŒ¨æµ·é»˜ç—‡ã€è‡ªé—­ç—‡è°±ç³»éšœç¢å’Œç²¾ç¥åˆ†è£‚ç—‡ç­‰ç–¾ç—…çš„è¯Šæ–­ä»¥åŠåŸºäºç»“æ„MRIæ•°æ®çš„ä¸´åºŠè¯„åˆ†é¢„æµ‹ç­‰12é¡¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œäº†éªŒè¯ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨é¢„è®­ç»ƒé˜¶æ®µèå…¥ç»“æ„ä¿¡æ¯æœ‰åŠ©äºå½¢æˆæ›´ç¨³å¥å’Œé€šç”¨çš„è¡¨ç¤ºã€‚é¢„è®­ç»ƒæ¨¡å‹å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/EIDOSLAB/AnatCL%E3%80%82">https://github.com/EIDOSLAB/AnatCLã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨ç¥ç»å½±åƒä¸­çš„åº”ç”¨åœ¨æ£€æµ‹ç¥ç»æ€§ç–¾ç—…å’Œç¥ç»é€€è¡Œæ€§ç–¾ç—…æ–¹é¢çš„é‡è¦æ€§ã€‚</li>
<li>è„‘å¹´é¾„ä½œä¸ºç¥ç»å½±åƒä¸­çš„å…³é”®ç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œå¯¹é¢„æµ‹ä¸åŒç–¾ç—…æœ‰è‰¯å¥½è¡¨ç°ã€‚</li>
<li>åˆ©ç”¨è„‘å¹´é¾„å¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œå¼±ç›‘ç£é¢„è®­ç»ƒåœ¨æ•°æ®ç¨€ç¼ºæƒ…å†µä¸‹æ˜¾ç¤ºå‡ºè‰¯å¥½çš„åº”ç”¨å‰æ™¯ã€‚</li>
<li>è„‘éƒ¨MRIçš„ç»“æ„ä¿¡æ¯ï¼ˆå¦‚çš®å±‚åšåº¦ï¼‰ä¸ºå­¦ä¹ è‰¯å¥½è¡¨ç¤ºæä¾›é‡è¦ä¿¡æ¯ï¼Œé€‚ç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li>AnatCLæ¨¡å‹é€šè¿‡å¼±å¯¹æ¯”å­¦ä¹ åˆ©ç”¨ç»“æ„ä¿¡æ¯ï¼Œå¹¶åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</li>
<li>AnatCLæ¨¡å‹åœ¨å¤šç§ç–¾ç—…çš„è¯Šæ–­å’Œä¸´åºŠè¯„åˆ†é¢„æµ‹ç­‰ä»»åŠ¡è¿›è¡Œäº†éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.07079">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-80a8b07b4a8975041f8e8ef1bd1c9578.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7f1cc02193fa8e25cb3e7975ebf0e86a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7a0fc5689e3647a89727f031a3d01bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ee4d03a5a95fb13af1874a268564efe.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="De-LightSAM-Modality-Decoupled-Lightweight-SAM-for-Generalizable-Medical-Segmentation"><a href="#De-LightSAM-Modality-Decoupled-Lightweight-SAM-for-Generalizable-Medical-Segmentation" class="headerlink" title="De-LightSAM: Modality-Decoupled Lightweight SAM for Generalizable   Medical Segmentation"></a>De-LightSAM: Modality-Decoupled Lightweight SAM for Generalizable   Medical Segmentation</h2><p><strong>Authors:Qing Xu, Jiaxuan Li, Xiangjian He, Chenxin Li, Fiseha B. Tesem, Wenting Duan, Zhen Chen, Rong Qu, Jonathan M. Garibaldi, Chang Wen Chen</strong></p>
<p>The universality of deep neural networks across different modalities and their generalization capabilities to unseen domains play an essential role in medical image segmentation. The recent segment anything model (SAM) has demonstrated strong adaptability across diverse natural scenarios. However, the huge computational costs, demand for manual annotations as prompts and conflict-prone decoding process of SAM degrade its generalization capabilities in medical scenarios. To address these limitations, we propose a modality-decoupled lightweight SAM for domain-generalized medical image segmentation, named De-LightSAM. Specifically, we first devise a lightweight domain-controllable image encoder (DC-Encoder) that produces discriminative visual features for diverse modalities. Further, we introduce the self-patch prompt generator (SP-Generator) to automatically generate high-quality dense prompt embeddings for guiding segmentation decoding. Finally, we design the query-decoupled modality decoder (QM-Decoder) that leverages a one-to-one strategy to provide an independent decoding channel for every modality, preventing mutual knowledge interference of different modalities. Moreover, we design a multi-modal decoupled knowledge distillation (MDKD) strategy to leverage robust common knowledge to complement domain-specific medical feature representations. Extensive experiments indicate that De-LightSAM outperforms state-of-the-arts in diverse medical imaging segmentation tasks, displaying superior modality universality and generalization capabilities. Especially, De-LightSAM uses only 2.0% parameters compared to SAM-H. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/xq141839/De-LightSAM">https://github.com/xq141839/De-LightSAM</a>. </p>
<blockquote>
<p>æ·±åº¦ç¥ç»ç½‘ç»œåœ¨ä¸åŒæ¨¡æ€ä¹‹é—´çš„é€šç”¨æ€§ä»¥åŠå®ƒä»¬å¯¹æœªè§é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚æœ€è¿‘çš„â€œä»»ä½•äº‹ç‰©åˆ†å‰²æ¨¡å‹â€(SAM)å·²åœ¨å¤šç§è‡ªç„¶åœºæ™¯ä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„é€‚åº”æ€§ã€‚ç„¶è€Œï¼ŒSAMçš„å·¨å¤§è®¡ç®—æˆæœ¬ã€å¯¹æ‰‹åŠ¨æ³¨é‡Šçš„æç¤ºéœ€æ±‚ä»¥åŠæ˜“å†²çªè§£ç è¿‡ç¨‹ï¼Œä½¿å…¶åœ¨åŒ»å­¦åœºæ™¯ä¸­æ³›åŒ–èƒ½åŠ›å—é™ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºé¢†åŸŸæ³›åŒ–åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ¨¡æ€è§£è€¦è½»é‡åŒ–SAMï¼Œåä¸ºDe-LightSAMã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ç§è½»é‡çº§åŸŸå¯æ§å›¾åƒç¼–ç å™¨ï¼ˆDC-Encoderï¼‰ï¼Œç”¨äºç”Ÿæˆä¸åŒæ¨¡æ€çš„åˆ¤åˆ«æ€§è§†è§‰ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªä¿®è¡¥æç¤ºç”Ÿæˆå™¨ï¼ˆSP-Generatorï¼‰ï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡å¯†é›†æç¤ºåµŒå…¥ï¼Œä»¥æŒ‡å¯¼åˆ†å‰²è§£ç ã€‚æœ€åï¼Œæˆ‘ä»¬è®¾è®¡äº†æŸ¥è¯¢è§£è€¦æ¨¡æ€è§£ç å™¨ï¼ˆQM-Decoderï¼‰ï¼Œå®ƒé‡‡ç”¨ä¸€å¯¹ä¸€ç­–ç•¥ï¼Œä¸ºæ¯ç§æ¨¡æ€æä¾›ç‹¬ç«‹çš„è§£ç é€šé“ï¼Œé˜²æ­¢ä¸åŒæ¨¡æ€ä¹‹é—´çš„çŸ¥è¯†ç›¸äº’å¹²æ‰°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¤šæ¨¡æ€è§£è€¦çŸ¥è¯†è’¸é¦ï¼ˆMDKDï¼‰ç­–ç•¥ï¼Œåˆ©ç”¨ç¨³å¥çš„é€šç”¨çŸ¥è¯†æ¥è¡¥å……é¢†åŸŸç‰¹å®šçš„åŒ»å­¦ç‰¹å¾è¡¨ç¤ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨å¤šç§åŒ»å­¦æˆåƒåˆ†å‰²ä»»åŠ¡ä¸­ï¼ŒDe-LightSAMçš„æ€§èƒ½ä¼˜äºå…¶ä»–æœ€æ–°æŠ€æœ¯ï¼Œæ˜¾ç¤ºå‡ºå“è¶Šçš„æ¨¡æ€é€šç”¨æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œç›¸è¾ƒäºSAM-Hï¼ŒDe-LightSAMä»…ä½¿ç”¨å…¶2.0%çš„å‚æ•°ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xq141839/De-LightSAM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/xq141839/De-LightSAMè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.14153v5">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ·±åº¦å­¦ä¹ æ¨¡å‹De-LightSAMã€‚è¯¥æ¨¡å‹è§£å†³äº†SAMæ¨¡å‹åœ¨åŒ»å­¦åœºæ™¯ä¸­çš„è®¡ç®—æˆæœ¬é«˜ã€éœ€è¦æ‰‹åŠ¨æ ‡æ³¨æç¤ºå’Œæ˜“å†²çªè§£ç ç­‰é—®é¢˜ã€‚De-LightSAMé€šè¿‡è®¾è®¡è½»é‡çº§åŸŸå¯æ§å›¾åƒç¼–ç å™¨ã€è‡ªè¡¥ä¸æç¤ºç”Ÿæˆå™¨å’ŒæŸ¥è¯¢è§£è€¦æ¨¡æ€è§£ç å™¨ï¼Œæé«˜äº†æ¨¡å‹çš„è·¨æ¨¡æ€é€šç”¨æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†å¤šæ¨¡æ€è§£è€¦çŸ¥è¯†è’¸é¦ç­–ç•¥ï¼Œä»¥åˆ©ç”¨ç¨³å¥çš„é€šç”¨çŸ¥è¯†æ¥è¡¥å……ç‰¹å®šé¢†åŸŸçš„åŒ»å­¦ç‰¹å¾è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼ŒDe-LightSAMåœ¨å¤šç§åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå…·æœ‰å‡ºè‰²çš„è·¨æ¨¡æ€é€šç”¨æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>De-LightSAMè§£å†³äº†SAMæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„è®¡ç®—æˆæœ¬é«˜ã€éœ€è¦æ‰‹åŠ¨æ ‡æ³¨æç¤ºå’Œæ˜“å†²çªè§£ç çš„é—®é¢˜ã€‚</li>
<li>De-LightSAMé€šè¿‡è®¾è®¡è½»é‡çº§åŸŸå¯æ§å›¾åƒç¼–ç å™¨ï¼ˆDC-Encoderï¼‰æé«˜æ¨¡å‹çš„è·¨æ¨¡æ€é€šç”¨æ€§ã€‚</li>
<li>è‡ªè¡¥ä¸æç¤ºç”Ÿæˆå™¨ï¼ˆSP-Generatorï¼‰èƒ½å¤Ÿè‡ªåŠ¨äº§ç”Ÿé«˜è´¨é‡å¯†é›†æç¤ºåµŒå…¥ï¼Œå¼•å¯¼åˆ†å‰²è§£ç ã€‚</li>
<li>æŸ¥è¯¢è§£è€¦æ¨¡æ€è§£ç å™¨ï¼ˆQM-Decoderï¼‰é‡‡ç”¨ä¸€å¯¹ä¸€ç­–ç•¥ï¼Œä¸ºæ¯ç§æ¨¡æ€æä¾›ç‹¬ç«‹è§£ç é€šé“ï¼Œé˜²æ­¢ä¸åŒæ¨¡æ€ä¹‹é—´çš„çŸ¥è¯†å¹²æ‰°ã€‚</li>
<li>å¤šæ¨¡æ€è§£è€¦çŸ¥è¯†è’¸é¦ï¼ˆMDKDï¼‰ç­–ç•¥åˆ©ç”¨ç¨³å¥çš„é€šç”¨çŸ¥è¯†æ¥è¡¥å……ç‰¹å®šé¢†åŸŸçš„åŒ»å­¦ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>De-LightSAMåœ¨å¤šç§åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>De-LightSAMçš„å‚æ•°ä½¿ç”¨é‡ä»…ä¸ºSAM-Hçš„2.0%ï¼Œæ›´åŠ è½»é‡çº§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.14153">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-927d11647088fd6517a67367dc027896.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dcad1a905ce55ccc70a56032bd6777a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75afd551ed64a7c7f08b369661ff25bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37f628b13b37a070dce6d6ea394c4542.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-155d3d53c3e3300421298a1f4ddf9c16.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Comparing-Lasso-and-Adaptive-Lasso-in-High-Dimensional-Data-A-Genetic-Survival-Analysis-in-Triple-Negative-Breast-Cancer"><a href="#Comparing-Lasso-and-Adaptive-Lasso-in-High-Dimensional-Data-A-Genetic-Survival-Analysis-in-Triple-Negative-Breast-Cancer" class="headerlink" title="Comparing Lasso and Adaptive Lasso in High-Dimensional Data: A Genetic   Survival Analysis in Triple-Negative Breast Cancer"></a>Comparing Lasso and Adaptive Lasso in High-Dimensional Data: A Genetic   Survival Analysis in Triple-Negative Breast Cancer</h2><p><strong>Authors:Pilar GonzÃ¡lez-Barquero, Rosa E. Lillo, Ãlvaro MÃ©ndez-Civieta</strong></p>
<p>In high-dimensional survival analysis, effective variable selection is crucial for both model interpretation and predictive performance. This paper investigates Cox regression with lasso and adaptive lasso penalties in genomic datasets where covariates far outnumber observations. We propose and evaluate four weight calculation strategies for adaptive lasso specifically designed for high-dimensional settings: ridge regression, principal component analysis (PCA), univariate Cox regression, and random survival forest (RSF) based weights. To address the inherent variability in high dimensional model selection, we develop a robust procedure that evaluates performance across multiple data partitions and selects variables based on a novel importance index. Extensive simulation studies demonstrate that adaptive lasso with ridge and PCA weights significantly outperforms standard lasso in variable selection accuracy while maintaining similar or better predictive performance across various correlation structures, censoring proportions (0-80%), and dimensionality settings. These improvements are particularly pronounced in highly-censored scenarios, making our approach valuable for real-world genetic studies with limited observed events. We apply our methodology to triple-negative breast cancer data with 234 patients, over 19500 variables and 82% censoring, identifying key genetic and clinical prognostic factors. Our findings demonstrate that adaptive lasso with appropriate weight calculation provides more stable and interpretable models for high-dimensional survival analysis. </p>
<blockquote>
<p>åœ¨é«˜ç»´ç”Ÿå­˜åˆ†æä¸­ï¼Œæœ‰æ•ˆçš„å˜é‡é€‰æ‹©å¯¹äºæ¨¡å‹è§£é‡Šå’Œé¢„æµ‹æ€§èƒ½éƒ½è‡³å…³é‡è¦ã€‚æœ¬æ–‡ç ”ç©¶äº†åœ¨åŸºå› ç»„æ•°æ®é›†ä¸­ä½¿ç”¨lassoå’Œè‡ªé€‚åº”lassoæƒ©ç½šçš„Coxå›å½’ï¼Œå…¶ä¸­åå˜é‡è¿œè¿œè¶…è¿‡è§‚æµ‹å€¼ã€‚æˆ‘ä»¬é’ˆå¯¹é«˜ç»´ç¯å¢ƒä¸“é—¨æå‡ºäº†å››ç§è‡ªé€‚åº”lassoæƒé‡è®¡ç®—ç­–ç•¥ï¼Œå¹¶å¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ï¼šå²­å›å½’ã€ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ã€å•å˜é‡Coxå›å½’å’ŒåŸºäºéšæœºç”Ÿå­˜æ£®æ—ï¼ˆRSFï¼‰çš„æƒé‡ã€‚ä¸ºäº†è§£å†³é«˜ç»´æ¨¡å‹é€‰æ‹©ä¸­çš„å›ºæœ‰å˜åŒ–æ€§ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç¨³å¥çš„ç¨‹åºï¼Œè¯¥ç¨‹åºå¯ä»¥åœ¨å¤šä¸ªæ•°æ®åˆ†åŒºä¸­è¯„ä¼°æ€§èƒ½ï¼Œå¹¶æ ¹æ®æ–°çš„é‡è¦æ€§æŒ‡æ•°é€‰æ‹©å˜é‡ã€‚å¤§é‡çš„æ¨¡æ‹Ÿç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨å²­å’ŒPCAæƒé‡çš„è‡ªé€‚åº”lassoåœ¨å˜é‡é€‰æ‹©å‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¼˜äºæ ‡å‡†lassoï¼ŒåŒæ—¶åœ¨å„ç§å…³è”ç»“æ„ã€å®¡æŸ¥æ¯”ä¾‹ï¼ˆ0-80%ï¼‰å’Œé«˜ç»´è®¾ç½®ä¸‹ä¿æŒç›¸ä¼¼æˆ–æ›´å¥½çš„é¢„æµ‹æ€§èƒ½ã€‚è¿™äº›æ”¹è¿›åœ¨é«˜åº¦å®¡æŸ¥çš„åœºæ™¯ä¸­å°¤ä¸ºçªå‡ºï¼Œä½¿æˆ‘ä»¬çš„æ–¹æ³•å¯¹äºå…·æœ‰æœ‰é™è§‚å¯Ÿäº‹ä»¶çš„çœŸå®ä¸–ç•Œé—ä¼ ç ”ç©¶å…·æœ‰ä»·å€¼ã€‚æˆ‘ä»¬å°†è¯¥æ–¹æ³•åº”ç”¨äº234ä¾‹ä¸‰é˜´æ€§ä¹³è…ºç™Œæ•°æ®ï¼Œè¶…è¿‡19500ä¸ªå˜é‡å’Œ82%çš„å®¡æŸ¥ï¼Œä»¥ç¡®å®šå…³é”®çš„é—ä¼ å’Œä¸´åºŠé¢„åå› ç´ ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨é€‚å½“æƒé‡è®¡ç®—çš„è‡ªé€‚åº”lassoä¸ºé«˜ç»´ç”Ÿå­˜åˆ†ææä¾›äº†æ›´ç¨³å®šå’Œå¯è§£é‡Šçš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.19213v2">PDF</a> 20 pages, 3 figures, 6 tables</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†é«˜ç»´ç”Ÿå­˜åˆ†æä¸­ï¼Œä½¿ç”¨å²­å›å½’ã€ä¸»æˆåˆ†åˆ†æã€å•å˜é‡Coxå›å½’å’Œéšæœºç”Ÿå­˜æ£®æ—æƒé‡è®¡ç®—ç­–ç•¥çš„è‡ªé€‚åº”Lassoæ–¹æ³•åœ¨åŸºå› ç»„æ•°æ®é›†ä¸Šçš„å˜é‡é€‰æ‹©æ•ˆæœã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè‡ªé€‚åº”Lassoæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆé€‰æ‹©é‡è¦å˜é‡ï¼Œåœ¨é¢„æµ‹æ€§èƒ½å’Œæ¨¡å‹è§£é‡Šæ€§ä¸Šè¡¨ç°ä¼˜äºæ ‡å‡†Lassoæ–¹æ³•ã€‚åœ¨é«˜åº¦æˆªæ–­çš„æƒ…å†µä¸‹è¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œé€‚åˆåº”ç”¨äºçœŸå®ä¸–ç•Œçš„é—ä¼ ç ”ç©¶ã€‚ç ”ç©¶æœ€åé€šè¿‡ä¸‰é‡é˜´æ€§ä¹³è…ºç™Œæ•°æ®é›†éªŒè¯äº†æ–¹æ³•çš„å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜ç»´ç”Ÿå­˜åˆ†æä¸­ï¼Œæœ‰æ•ˆçš„å˜é‡é€‰æ‹©å¯¹äºæ¨¡å‹è§£é‡Šå’Œé¢„æµ‹æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>è‡ªé€‚åº”Lassoæ–¹æ³•ç»“åˆäº†Coxå›å½’ä¸æƒ©ç½šé¡¹ï¼Œç‰¹åˆ«é€‚ç”¨äºé«˜ç»´æ•°æ®é›†ã€‚</li>
<li>æå‡ºäº†å››ç§é’ˆå¯¹è‡ªé€‚åº”Lassoçš„é‡é‡è®¡ç®—ç­–ç•¥ï¼ŒåŒ…æ‹¬å²­å›å½’ã€ä¸»æˆåˆ†åˆ†æã€å•å˜é‡Coxå›å½’å’Œéšæœºç”Ÿå­˜æ£®æ—ã€‚</li>
<li>è‡ªé€‚åº”Lassoæ–¹æ³•åœ¨å¤šæ•°æ®åˆ†åŒºä¸Šçš„è¡¨ç°ç¨³å¥ï¼Œé€šè¿‡æ–°å‹é‡è¦æ€§æŒ‡æ•°é€‰æ‹©å˜é‡ã€‚</li>
<li>æ¨¡æ‹Ÿç ”ç©¶è¡¨æ˜ï¼Œè‡ªé€‚åº”Lassoåœ¨å˜é‡é€‰æ‹©å‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¼˜äºæ ‡å‡†Lassoï¼Œå¹¶ä¸”åœ¨å„ç§ç›¸å…³æ€§ç»“æ„ã€æˆªæ–­æ¯”ä¾‹å’Œç»´åº¦è®¾ç½®ä¸Šç»´æŒç›¸ä¼¼çš„é¢„æµ‹æ€§èƒ½æˆ–æœ‰æ‰€æå‡ã€‚</li>
<li>åœ¨é«˜åº¦æˆªæ–­çš„åœºæ™¯ä¸‹ï¼Œè‡ªé€‚åº”Lassoçš„æ”¹è¿›å°¤ä¸ºæ˜¾è‘—ï¼Œè¿™å¯¹çœŸå®ä¸–ç•Œçš„é—ä¼ ç ”ç©¶å…·æœ‰å®ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.19213">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-470e2dd00c25ae52dacfd5b9f2311316.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccd5bebb35cb0defb403f3de166e4603.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Average-Calibration-Error-A-Differentiable-Loss-for-Improved-Reliability-in-Image-Segmentation"><a href="#Average-Calibration-Error-A-Differentiable-Loss-for-Improved-Reliability-in-Image-Segmentation" class="headerlink" title="Average Calibration Error: A Differentiable Loss for Improved   Reliability in Image Segmentation"></a>Average Calibration Error: A Differentiable Loss for Improved   Reliability in Image Segmentation</h2><p><strong>Authors:Theodore Barfoot, Luis Garcia-Peraza-Herrera, Ben Glocker, Tom Vercauteren</strong></p>
<p>Deep neural networks for medical image segmentation often produce overconfident results misaligned with empirical observations. Such miscalibration, challenges their clinical translation. We propose to use marginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss function to improve pixel-wise calibration without compromising segmentation quality. We show that this loss, despite using hard binning, is directly differentiable, bypassing the need for approximate but differentiable surrogate or soft binning approaches. Our work also introduces the concept of dataset reliability histograms which generalises standard reliability diagrams for refined visual assessment of calibration in semantic segmentation aggregated at the dataset level. Using mL1-ACE, we reduce average and maximum calibration error by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS 2021 dataset. We share our code here: <a target="_blank" rel="noopener" href="https://github.com/cai4cai/ACE-DLIRIS">https://github.com/cai4cai/ACE-DLIRIS</a> </p>
<blockquote>
<p>é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ·±åº¦å­¦ä¹ ç½‘ç»œå¸¸å¸¸äº§ç”Ÿè¿‡äºè‡ªä¿¡çš„ç»“æœï¼Œè¿™äº›ç»“æœä¸å®è¯è§‚å¯Ÿä¸ç¬¦ã€‚è¿™ç§è¯¯æ ¡å‡†å¯¹å…¶ä¸´åºŠç¿»è¯‘æ„æˆäº†æŒ‘æˆ˜ã€‚æˆ‘ä»¬å»ºè®®ä½¿ç”¨è¾¹é™…L1å¹³å‡æ ¡å‡†è¯¯å·®ï¼ˆmL1-ACEï¼‰ä½œä¸ºä¸€ç§æ–°çš„è¾…åŠ©æŸå¤±å‡½æ•°ï¼Œä»¥æé«˜åƒç´ çº§çš„æ ¡å‡†ï¼ŒåŒæ—¶ä¸æŸå®³åˆ†å‰²è´¨é‡ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå°½ç®¡ä½¿ç”¨äº†ç¡¬åˆ†ç®±ï¼Œä½†è¿™ç§æŸå¤±æ˜¯ç›´æ¥å¯å¾®åˆ†çš„ï¼Œä»è€Œç»•è¿‡äº†éœ€è¦ä½¿ç”¨è¿‘ä¼¼ä½†å¯å¾®åˆ†çš„æ›¿ä»£æˆ–è½¯åˆ†ç®±æ–¹æ³•çš„éœ€æ±‚ã€‚æˆ‘ä»¬çš„å·¥ä½œè¿˜å¼•å…¥äº†æ•°æ®é›†å¯é æ€§ç›´æ–¹å›¾çš„æ¦‚å¿µï¼Œå®ƒæ¨å¹¿äº†æ ‡å‡†å¯é æ€§å›¾ï¼Œç”¨äºå¯¹æ•°æ®é›†çº§åˆ«çš„è¯­ä¹‰åˆ†å‰²æ ¡å‡†è¿›è¡Œç²¾ç»†çš„è§†è§‰è¯„ä¼°ã€‚ä½¿ç”¨mL1-ACEï¼Œæˆ‘ä»¬å°†å¹³å‡å’Œæœ€å¤§æ ¡å‡†è¯¯å·®åˆ†åˆ«é™ä½äº†45%å’Œ55%ï¼ŒåŒæ—¶åœ¨BraTS 2021æ•°æ®é›†ä¸Šä¿æŒäº†87%çš„Diceå¾—åˆ†ã€‚æˆ‘ä»¬çš„ä»£ç åˆ†äº«åœ¨è¿™é‡Œï¼š<a target="_blank" rel="noopener" href="https://github.com/cai4cai/ACE-DLIRIS">https://github.com/cai4cai/ACE-DLIRIS</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.06759v2">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æ·±åº¦ç¥ç»ç½‘ç»œäº§ç”Ÿçš„è¿‡åº¦è‡ªä¿¡ç»“æœé—®é¢˜ï¼Œæå‡ºä½¿ç”¨è¾¹é™…L1å¹³å‡æ ¡å‡†è¯¯å·®ï¼ˆmL1-ACEï¼‰ä½œä¸ºæ–°å‹è¾…åŠ©æŸå¤±å‡½æ•°ï¼Œæ”¹å–„åƒç´ çº§æ ¡å‡†è€Œä¸å½±å“åˆ†å‰²è´¨é‡ã€‚å¼•å…¥æ•°æ®é›†å¯é æ€§ç›´æ–¹å›¾ï¼Œä¾¿äºç²¾ç»†å¯è§†åŒ–è¯„ä¼°è¯­ä¹‰åˆ†å‰²çš„æ ¡å‡†æƒ…å†µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ç¥ç»ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¸¸äº§ç”Ÿè¿‡åº¦è‡ªä¿¡çš„ç»“æœï¼Œä¸å®é™…æƒ…å†µä¸ç¬¦ã€‚</li>
<li>å¼•å…¥è¾¹é™…L1å¹³å‡æ ¡å‡†è¯¯å·®ï¼ˆmL1-ACEï¼‰ä½œä¸ºæ–°çš„è¾…åŠ©æŸå¤±å‡½æ•°ï¼Œæ”¹å–„åƒç´ çº§æ ¡å‡†ã€‚</li>
<li>mL1-ACEæŸå¤±å‡½æ•°ä½¿ç”¨ç¡¬åˆ†ç®±ï¼Œä½†å…·å¤‡ç›´æ¥å¯å¾®æ€§ï¼Œæ— éœ€è¿‘ä¼¼ä½†å¯å¾®çš„æ›¿ä»£æˆ–è½¯åˆ†ç®±æ–¹æ³•ã€‚</li>
<li>å¼•å…¥æ•°æ®é›†å¯é æ€§ç›´æ–¹å›¾ï¼Œç”¨äºç²¾ç»†å¯è§†åŒ–è¯„ä¼°è¯­ä¹‰åˆ†å‰²çš„æ ¡å‡†æƒ…å†µï¼Œä¾¿äºåœ¨æ•°æ®é›†å±‚é¢è¿›è¡Œæ ¡å‡†è¯„ä¼°ã€‚</li>
<li>ä½¿ç”¨mL1-ACEï¼Œå¹³å‡æ ¡å‡†è¯¯å·®å’Œæœ€å¤§æ ¡å‡†è¯¯å·®åˆ†åˆ«é™ä½äº†45%å’Œ55%ã€‚</li>
<li>åœ¨BraTS 2021æ•°æ®é›†ä¸Šï¼Œç»´æŒ87%çš„Diceå¾—åˆ†ã€‚</li>
<li>ç ”ç©¶æˆæœå·²å…±äº«äºï¼š<a target="_blank" rel="noopener" href="https://github.com/cai4cai/ACE-DLIRIS">https://github.com/cai4cai/ACE-DLIRIS</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.06759">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e8263ab8c92ec21703b6965c65f954cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9d6ad7c0dd2036cb14bf4af38aa6af2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b92c85046bac61be54c371272062d92f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9427cce65b7878cf6ea8e534a2d40b53.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Improving-Robustness-and-Reliability-in-Medical-Image-Classification-with-Latent-Guided-Diffusion-and-Nested-Ensembles"><a href="#Improving-Robustness-and-Reliability-in-Medical-Image-Classification-with-Latent-Guided-Diffusion-and-Nested-Ensembles" class="headerlink" title="Improving Robustness and Reliability in Medical Image Classification   with Latent-Guided Diffusion and Nested-Ensembles"></a>Improving Robustness and Reliability in Medical Image Classification   with Latent-Guided Diffusion and Nested-Ensembles</h2><p><strong>Authors:Xing Shen, Hengguan Huang, Brennan Nichyporuk, Tal Arbel</strong></p>
<p>Once deployed, medical image analysis methods are often faced with unexpected image corruptions and noise perturbations. These unknown covariate shifts present significant challenges to deep learning based methods trained on â€œcleanâ€ images. This often results in unreliable predictions and poorly calibrated confidence, hence hindering clinical applicability. While recent methods have been developed to address specific issues such as confidence calibration or adversarial robustness, no single framework effectively tackles all these challenges simultaneously. To bridge this gap, we propose LaDiNE, a novel ensemble learning method combining the robustness of Vision Transformers with diffusion-based generative models for improved reliability in medical image classification. Specifically, transformer encoder blocks are used as hierarchical feature extractors that learn invariant features from images for each ensemble member, resulting in features that are robust to input perturbations. In addition, diffusion models are used as flexible density estimators to estimate member densities conditioned on the invariant features, leading to improved modeling of complex data distributions while retaining properly calibrated confidence. Extensive experiments on tuberculosis chest X-rays and melanoma skin cancer datasets demonstrate that LaDiNE achieves superior performance compared to a wide range of state-of-the-art methods by simultaneously improving prediction accuracy and confidence calibration under unseen noise, adversarial perturbations, and resolution degradation. </p>
<blockquote>
<p>éƒ¨ç½²åŒ»ç–—å›¾åƒåˆ†ææ–¹æ³•åï¼Œå®ƒä»¬ç»å¸¸é¢ä¸´æ„å¤–çš„å›¾åƒæŸåå’Œå™ªå£°å¹²æ‰°ã€‚è¿™äº›æœªçŸ¥çš„åå˜é‡å˜åŒ–ç»™åŸºäºâ€œå¹²å‡€â€å›¾åƒè®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ–¹æ³•å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚è¿™é€šå¸¸ä¼šå¯¼è‡´é¢„æµ‹ç»“æœä¸å¯é å’Œç½®ä¿¡åº¦æ ¡å‡†ä¸ä½³ï¼Œä»è€Œé˜»ç¢äº†å…¶åœ¨ä¸´åºŠä¸Šçš„é€‚ç”¨æ€§ã€‚è™½ç„¶æœ€è¿‘å·²ç»å¼€å‘äº†ä¸€äº›æ–¹æ³•æ¥è§£å†³ä¿¡å¿ƒæ ¡å‡†æˆ–å¯¹æŠ—ç¨³å¥æ€§ç­‰é—®é¢˜ï¼Œä½†æ²¡æœ‰å•ä¸€æ¡†æ¶èƒ½æœ‰æ•ˆåœ°åŒæ—¶è§£å†³æ‰€æœ‰è¿™äº›æŒ‘æˆ˜ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†LaDiNEï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œå°†è§†è§‰å˜å‹å™¨çš„ç¨³å¥æ€§ä¸åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹çš„å¯é æ€§ç›¸ç»“åˆï¼Œä»¥æé«˜åŒ»ç–—å›¾åƒåˆ†ç±»çš„å¯é æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨å˜å‹å™¨ç¼–ç å™¨å—ä½œä¸ºåˆ†å±‚ç‰¹å¾æå–å™¨ï¼Œä»æ¯ä¸ªé›†æˆæˆå‘˜å­¦ä¹ å›¾åƒçš„æ’å®šç‰¹å¾ï¼Œä»è€Œäº§ç”Ÿå¯¹è¾“å…¥æ‰°åŠ¨å…·æœ‰é²æ£’æ€§çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæ‰©æ•£æ¨¡å‹è¢«ç”¨ä½œçµæ´»çš„å¯†åº¦ä¼°è®¡å™¨ï¼Œæ ¹æ®æ’å®šç‰¹å¾ä¼°è®¡æˆå‘˜å¯†åº¦ï¼Œä»è€Œåœ¨ä¿ç•™é€‚å½“æ ¡å‡†çš„ç½®ä¿¡åº¦çš„åŒæ—¶ï¼Œå®ç°å¯¹å¤æ‚æ•°æ®åˆ†å¸ƒçš„æ”¹è¿›å»ºæ¨¡ã€‚åœ¨è‚ºç»“æ ¸èƒ¸éƒ¨Xå°„çº¿å’Œé»‘è‰²ç´ ç˜¤çš®è‚¤ç™Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLaDiNEé€šè¿‡åŒæ—¶æé«˜é¢„æµ‹ç²¾åº¦å’Œç½®ä¿¡åº¦æ ¡å‡†ï¼Œåœ¨æœªè§çš„å™ªå£°ã€å¯¹æŠ—æ€§æ‰°åŠ¨å’Œåˆ†è¾¨ç‡ä¸‹é™çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†ä¸ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”çš„å“è¶Šæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15952v5">PDF</a> Accepted to IEEE Transactions on Medical Imaging, 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†ææ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­å¸¸é­é‡å›¾åƒæŸåå’Œå™ªå£°å¹²æ‰°ç­‰æŒ‘æˆ˜ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ–°å‹é›†æˆå­¦ä¹ æ–¹æ³•LaDiNEï¼Œç»“åˆVision Transformerå’Œæ‰©æ•£ç”Ÿæˆæ¨¡å‹çš„ç¨³å¥æ€§ï¼Œæé«˜åŒ»å­¦å›¾åƒåˆ†ç±»çš„å¯é æ€§ã€‚å®éªŒè¯æ˜ï¼ŒLaDiNEåœ¨ä¸åŒå™ªå£°ã€å¯¹æŠ—æ€§å¹²æ‰°å’Œåˆ†è¾¨ç‡ä¸‹é™çš„æƒ…å†µä¸‹ï¼Œé¢„æµ‹å‡†ç¡®ç‡å’Œç½®ä¿¡åº¦æ ¡å‡†å‡ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†ææ–¹æ³•é¢ä¸´æœªçŸ¥åå˜é‡åç§»çš„æŒ‘æˆ˜ï¼Œå½±å“æ·±åº¦å­¦ä¹ çš„é¢„æµ‹å¯é æ€§å’Œä¸´åºŠé€‚ç”¨æ€§ã€‚</li>
<li>LaDiNEæ˜¯ä¸€ç§æ–°å‹é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œç»“åˆVision Transformerå’Œæ‰©æ•£ç”Ÿæˆæ¨¡å‹çš„ä¼˜ç‚¹ã€‚</li>
<li>LaDiNEé€šè¿‡å˜å‹å™¨ç¼–ç å™¨å—æå–å›¾åƒçš„ä¸å˜ç‰¹å¾ï¼Œå¢å¼ºå¯¹è¾“å…¥æ‰°åŠ¨çš„ç¨³å¥æ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ä½œä¸ºçµæ´»çš„å¯†åº¦ä¼°è®¡å™¨ï¼Œä¼°è®¡æˆå‘˜å¯†åº¦å¹¶åŸºäºä¸å˜ç‰¹å¾è¿›è¡Œæ¡ä»¶å»ºæ¨¡ã€‚</li>
<li>LaDiNEèƒ½æé«˜é¢„æµ‹å‡†ç¡®æ€§å’Œç½®ä¿¡åº¦æ ¡å‡†ï¼Œåœ¨æœªè§å™ªå£°ã€å¯¹æŠ—æ€§å¹²æ‰°å’Œåˆ†è¾¨ç‡é™ä½çš„æƒ…å†µä¸‹è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å®éªŒåœ¨è‚ºç»“æ ¸èƒ¸éƒ¨Xå°„çº¿å’Œé»‘è‰²ç´ ç˜¤çš®è‚¤ç™Œæ•°æ®é›†ä¸ŠéªŒè¯äº†LaDiNEçš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.15952">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5801b261fc3288b77c742be779e566cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69469cc40b7d67cc7929e4497c22279a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d7d358148a8bd51be061c817c1c5b42e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-09/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5079c80e2fc378a16c514f6a221b54e7.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-09  Spatio-Temporal LLM Reasoning about Environments and Actions
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-08/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-913e399ca3527dd47d6feb49ff889d86.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-08  TTRL Test-Time Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24474.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
