<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-07-08  VHU-Net Variational Hadamard U-Net for Body MRI Bias Field Correction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-37f628b13b37a070dce6d6ea394c4542.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    49 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-08-更新"><a href="#2025-07-08-更新" class="headerlink" title="2025-07-08 更新"></a>2025-07-08 更新</h1><h2 id="VHU-Net-Variational-Hadamard-U-Net-for-Body-MRI-Bias-Field-Correction"><a href="#VHU-Net-Variational-Hadamard-U-Net-for-Body-MRI-Bias-Field-Correction" class="headerlink" title="VHU-Net: Variational Hadamard U-Net for Body MRI Bias Field Correction"></a>VHU-Net: Variational Hadamard U-Net for Body MRI Bias Field Correction</h2><p><strong>Authors:Xin Zhu, Ahmet Enis Cetin, Gorkem Durak, Batuhan Gundogdu, Ziliang Hong, Hongyi Pan, Ertugrul Aktas, Elif Keles, Hatice Savas, Aytekin Oto, Hiten Patel, Adam B. Murphy, Ashley Ross, Frank Miller, Baris Turkbey, Ulas Bagci</strong></p>
<p>Bias field artifacts in magnetic resonance imaging (MRI) scans introduce spatially smooth intensity inhomogeneities that degrade image quality and hinder downstream analysis. To address this challenge, we propose a novel variational Hadamard U-Net (VHU-Net) for effective body MRI bias field correction. The encoder comprises multiple convolutional Hadamard transform blocks (ConvHTBlocks), each integrating convolutional layers with a Hadamard transform (HT) layer. Specifically, the HT layer performs channel-wise frequency decomposition to isolate low-frequency components, while a subsequent scaling layer and semi-soft thresholding mechanism suppress redundant high-frequency noise. To compensate for the HT layer’s inability to model inter-channel dependencies, the decoder incorporates an inverse HT-reconstructed transformer block, enabling global, frequency-aware attention for the recovery of spatially consistent bias fields. The stacked decoder ConvHTBlocks further enhance the capacity to reconstruct the underlying ground-truth bias field. Building on the principles of variational inference, we formulate a new evidence lower bound (ELBO) as the training objective, promoting sparsity in the latent space while ensuring accurate bias field estimation. Comprehensive experiments on abdominal and prostate MRI datasets demonstrate the superiority of VHU-Net over existing state-of-the-art methods in terms of intensity uniformity, signal fidelity, and tissue contrast. Moreover, the corrected images yield substantial downstream improvements in segmentation accuracy. Our framework offers computational efficiency, interpretability, and robust performance across multi-center datasets, making it suitable for clinical deployment. </p>
<blockquote>
<p>磁共振成像（MRI）扫描中的偏置场伪影引入了空间平滑强度不均匀性，降低了图像质量，阻碍了下游分析。为了应对这一挑战，我们提出了一种新型的变分哈达玛U形网络（VHU-Net），用于有效的体MRI偏置场校正。编码器由多个卷积哈达玛变换块（ConvHTBlocks）组成，每个块都结合了卷积层和哈达玛变换（HT）层。具体来说，HT层执行通道频率分解，以隔离低频分量，而随后的缩放层和半软阈值机制抑制冗余的高频噪声。为了弥补HT层无法建模通道间依赖性的不足，解码器采用了一个逆HT重建的变换块，能够实现全局频率感知注意力以恢复空间一致的偏置场。堆叠的解码器ConvHT块进一步增强了重建潜在真实偏置场的能力。基于变分推理的原理，我们将新的证据下限（ELBO）作为训练目标，以促进潜在空间的稀疏性，同时确保准确的偏置场估计。在腹部和前列腺MRI数据集上的综合实验表明，VHU-Net在强度均匀性、信号保真度和组织对比度方面优于现有最先进的方法。此外，校正后的图像在分割精度上产生了显著的下游改进。我们的框架具有计算效率高、可解释性强、多中心数据集性能稳健等优点，适合临床部署。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19181v2">PDF</a> </p>
<p><strong>Summary</strong><br>     针对磁共振成像（MRI）扫描中的偏置场伪影问题，提出一种新型的变分Hadamard U-Net（VHU-Net）进行有效的体MRI偏置场校正。该网络通过结合卷积层和Hadamard变换层，实现了对低频频谱的分解以及对冗余高频噪声的抑制。利用逆HT重建的解码器块实现全局频率感知注意力，恢复空间一致的偏置场。通过变分推断原理制定新的证据下限（ELBO）作为训练目标，在潜在空间中促进稀疏性并确保准确的偏置场估计。在腹部和前列腺MRI数据集上的实验表明，VHU-Net在强度均匀性、信号保真度和组织对比度方面优于现有先进技术。校正后的图像在分割精度上有显著提高，且该框架具有计算效率高、可解释性强和多中心数据集表现稳健等优点，适合临床部署。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>磁共振成像（MRI）中的偏置场伪影会引入空间平滑强度不均匀性，影响图像质量和后续分析。</li>
<li>提出一种新型的变分Hadamard U-Net（VHU-Net）进行MRI偏置场校正。</li>
<li>VHU-Net结合卷积层和Hadamard变换层，有效分解低频频谱并抑制冗余高频噪声。</li>
<li>逆HT重建的解码器块实现全局频率感知注意力，有助于恢复空间一致的偏置场。</li>
<li>通过变分推断原理制定新的证据下限（ELBO）作为训练目标，确保准确估计偏置场。</li>
<li>在腹部和前列腺MRI数据集上的实验表明，VHU-Net在多个评估指标上优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19181">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b051859f4f8b03f9169c93836dc80702.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-735c0028a47fa260ff2fc9eeb7c147ae.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AI-Flow-Perspectives-Scenarios-and-Approaches"><a href="#AI-Flow-Perspectives-Scenarios-and-Approaches" class="headerlink" title="AI Flow: Perspectives, Scenarios, and Approaches"></a>AI Flow: Perspectives, Scenarios, and Approaches</h2><p><strong>Authors:Hongjun An, Wenhan Hu, Sida Huang, Siqi Huang, Ruanjun Li, Yuanzhi Liang, Jiawei Shao, Yiliang Song, Zihan Wang, Cheng Yuan, Chi Zhang, Hongyuan Zhang, Wenhao Zhuang, Xuelong Li</strong></p>
<p>Pioneered by the foundational information theory by Claude Shannon and the visionary framework of machine intelligence by Alan Turing, the convergent evolution of information and communication technologies (IT&#x2F;CT) has created an unbroken wave of connectivity and computation. This synergy has sparked a technological revolution, now reaching its peak with large artificial intelligence (AI) models that are reshaping industries and redefining human-machine collaboration. However, the realization of ubiquitous intelligence faces considerable challenges due to substantial resource consumption in large models and high communication bandwidth demands. To address these challenges, AI Flow has been introduced as a multidisciplinary framework that integrates cutting-edge IT and CT advancements, with a particular emphasis on the following three key points. First, device-edge-cloud framework serves as the foundation, which integrates end devices, edge servers, and cloud clusters to optimize scalability and efficiency for low-latency model inference. Second, we introduce the concept of familial models, which refers to a series of different-sized models with aligned hidden features, enabling effective collaboration and the flexibility to adapt to varying resource constraints and dynamic scenarios. Third, connectivity- and interaction-based intelligence emergence is a novel paradigm of AI Flow. By leveraging communication networks to enhance connectivity, the collaboration among AI models across heterogeneous nodes achieves emergent intelligence that surpasses the capability of any single model. The innovations of AI Flow provide enhanced intelligence, timely responsiveness, and ubiquitous accessibility to AI services, paving the way for the tighter fusion of AI techniques and communication systems. </p>
<blockquote>
<p>在信息理论先驱克劳德·香农和人工智能先驱艾伦·图灵开创性理论框架下，信息和通信技术的融合（IT&#x2F;CT）不断演化，推动了连接和计算的不间断浪潮。这种协同作用引发了一场技术革命，现在随着大型人工智能模型的崛起达到顶峰，正在重塑产业并重新定义人机协作。然而，实现无处不在的智能面临着巨大的挑战，因为大型模型需要大量的资源消耗和高通信带宽需求。为了解决这些挑战，引入了AI Flow这一跨学科框架，整合了前沿的IT和CT技术，特别是以下三个关键点。首先，设备边缘云框架是核心基础，它将终端设备、边缘服务器和云集群整合在一起，优化可扩展性，提高低延迟模型推理的效率。其次，我们引入了家族模型的概念，这是指具有对齐隐藏特征的一系列不同大小的模型，能够实现有效的协作和适应不同的资源约束和动态场景。第三，基于连接和交互的智能涌现是AI Flow的新范式。通过利用通信网络增强连接性，不同节点上的人工智能模型之间的协作实现了涌现智能，超越了任何单一模型的能力。AI Flow的创新提供了增强的智能、及时响应和无处不在的人工智能服务访问能力，为人工智能技术和通信系统之间的紧密融合铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12479v2">PDF</a> Authors are with Institute of Artificial Intelligence (TeleAI), China   Telecom, China. Author names are listed alphabetically by surname. This work   was conducted at TeleAI, facilitated by Dr. Jiawei Shao (e-mail:   <a href="mailto:&#x73;&#x68;&#97;&#111;&#106;&#x77;&#x32;&#64;&#x63;&#104;&#x69;&#x6e;&#97;&#116;&#101;&#x6c;&#x65;&#x63;&#111;&#x6d;&#46;&#99;&#110;">&#x73;&#x68;&#97;&#111;&#106;&#x77;&#x32;&#64;&#x63;&#104;&#x69;&#x6e;&#97;&#116;&#101;&#x6c;&#x65;&#x63;&#111;&#x6d;&#46;&#99;&#110;</a>) under the leadership of Prof. Xuelong Li. The   corresponding author is Prof. Xuelong Li (e-mail: xuelong <a href="mailto:&#108;&#x69;&#64;&#105;&#101;&#101;&#x65;&#46;&#x6f;&#x72;&#x67;">&#108;&#x69;&#64;&#105;&#101;&#101;&#x65;&#46;&#x6f;&#x72;&#x67;</a>), the   CTO and Chief Scientist of China Telecom</p>
<p><strong>Summary</strong><br>     信息论创始人克劳德·香农与人工智能先驱艾伦·图灵开创性的理论为信息与通信技术（IT&#x2F;CT）的融合演变奠定了基础。这一协同进化创造了连接和计算的持续浪潮，引发了一场技术革命。如今，大型人工智能模型正重塑产业并重新定义人机协作。然而，实现普遍智能面临着巨大的挑战，如大型模型的高资源消耗和通信带宽的高需求。为解决这些挑战，AI Flow作为一个跨学科框架应运而生，集成了最前沿的IT和CT技术，重点包括：设备边缘云框架、家族模型的概念以及基于连接和交互的智力涌现新模式。这些创新为增强智能、及时响应和无处不在的AI服务访问提供了条件，为人工智能技术和通信系统的紧密融合铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>信息与通信技术融合演变基于香农的信息论和图灵的人工智能框架。</li>
<li>大型人工智能模型正重塑产业并重新定义人机协作，但实现普遍智能面临挑战。</li>
<li>AI Flow框架集成IT和CT技术，以应对资源消耗和通信带宽的挑战。</li>
<li>设备边缘云框架是AI Flow的基础，整合终端、边缘服务器和云集群，优化可扩展性和效率。</li>
<li>家族模型概念包含不同大小的模型，具有对齐的隐藏特征，适应资源约束和动态场景。</li>
<li>基于连接和交互的智力涌现是AI Flow的新型模式，通过通信网络增强连接，实现模型间协作涌现智能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12479">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-16e02722b74a8c83e18f1b9751914ad7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b71919176b1263eda39eef8c48b37f9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80d185e19cb77bc915d71b3e6863804e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Non-rigid-Motion-Correction-for-MRI-Reconstruction-via-Coarse-To-Fine-Diffusion-Models"><a href="#Non-rigid-Motion-Correction-for-MRI-Reconstruction-via-Coarse-To-Fine-Diffusion-Models" class="headerlink" title="Non-rigid Motion Correction for MRI Reconstruction via Coarse-To-Fine   Diffusion Models"></a>Non-rigid Motion Correction for MRI Reconstruction via Coarse-To-Fine   Diffusion Models</h2><p><strong>Authors:Frederic Wang, Jonathan I. Tamir</strong></p>
<p>Magnetic Resonance Imaging (MRI) is highly susceptible to motion artifacts due to the extended acquisition times required for k-space sampling. These artifacts can compromise diagnostic utility, particularly for dynamic imaging. We propose a novel alternating minimization framework that leverages a bespoke diffusion model to jointly reconstruct and correct non-rigid motion-corrupted k-space data. The diffusion model uses a coarse-to-fine denoising strategy to capture large overall motion and reconstruct the lower frequencies of the image first, providing a better inductive bias for motion estimation than that of standard diffusion models. We demonstrate the performance of our approach on both real-world cine cardiac MRI datasets and complex simulated rigid and non-rigid deformations, even when each motion state is undersampled by a factor of 64x. Additionally, our method is agnostic to sampling patterns, anatomical variations, and MRI scanning protocols, as long as some low frequency components are sampled during each motion state. </p>
<blockquote>
<p>磁共振成像（MRI）由于k空间采样所需的长时间采集而易受到运动伪影的影响。这些伪影可能损害诊断的效用，特别是在动态成像中。我们提出了一种新型的交替最小化框架，它利用专门的扩散模型来联合重建和纠正非刚性运动受损的k空间数据。该扩散模型采用由粗到细的降噪策略，首先捕获整体大运动并重建图像的低频部分，为运动估计提供了比标准扩散模型更好的归纳偏置。我们在现实世界的电影心脏MRI数据集和复杂的模拟刚性和非刚性变形上展示了我们的方法性能，即使在每种运动状态按64倍欠采样的情况下也是如此。此外，我们的方法对于采样模式、解剖差异和MRI扫描协议持开放性态度，只要在每个运动状态下采样一些低频成分即可。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15057v3">PDF</a> ICIP 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于交替最小化框架和专用扩散模型的非刚性运动腐蚀k空间数据联合重建和校正方法。该扩散模型采用由粗到细的降噪策略，先捕捉整体大运动并重建图像的低频部分，为运动估计提供更好的归纳偏置。该方法在真实电影心脏MRI数据集和复杂的模拟刚性和非刚性变形上表现出良好的性能，即使在每个运动状态欠采样64倍的情况下也是如此。此方法对于采样模式、解剖变异和MRI扫描协议具有通用性，只要在每个运动状态下采样一些低频成分即可。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MRI对运动伪影高度敏感，由于k空间采样的扩展采集时间，运动伪影会影响诊断效用，特别是在动态成像中。</li>
<li>提出了一种新颖的交替最小化框架，结合了专用的扩散模型来联合重建和校正非刚性运动腐蚀的k空间数据。</li>
<li>扩散模型采用由粗到细的降噪策略，首先重建图像的低频部分，为运动估计提供更好的基础。</li>
<li>方法在真实电影心脏MRI数据集和模拟的刚性和非刚性变形上进行了演示，性能良好。</li>
<li>即使在严重的欠采样情况下（每个运动状态欠采样64倍），该方法仍然有效。</li>
<li>该方法对采样模式、解剖变异和MRI扫描协议具有通用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15057">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dcb73323e1357571f95576cf827ff061.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40c450a3001a4b039f9561669912de35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7a89deb44f02ea190b093675187992e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1879d5afbf5f04752a5b3d36a1b7585.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1b84582bd7f57ab6c58ed143a97ad8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57052dc9379f13a30206187b3a442480.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4118c0aa3cf8ed7d5566417769df99db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d2a90d028abcb96a0153659c4c1beca.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="XGeM-A-Multi-Prompt-Foundation-Model-for-Multimodal-Medical-Data-Generation"><a href="#XGeM-A-Multi-Prompt-Foundation-Model-for-Multimodal-Medical-Data-Generation" class="headerlink" title="XGeM: A Multi-Prompt Foundation Model for Multimodal Medical Data   Generation"></a>XGeM: A Multi-Prompt Foundation Model for Multimodal Medical Data   Generation</h2><p><strong>Authors:Daniele Molino, Francesco Di Feola, Eliodoro Faiella, Deborah Fazzini, Domiziana Santucci, Linlin Shen, Valerio Guarrasi, Paolo Soda</strong></p>
<p>The adoption of Artificial Intelligence in medical imaging holds great promise, yet it remains hindered by challenges such as data scarcity, privacy concerns, and the need for robust multimodal integration. While recent advances in generative modeling have enabled high-quality synthetic data generation, existing approaches are often limited to unimodal, unidirectional synthesis and therefore lack the ability to jointly synthesize multiple modalities while preserving clinical consistency. To address this challenge, we introduce XGeM, a 6.77-billion-parameter multimodal generative model designed to support flexible, any-to-any synthesis between medical data modalities. XGeM constructs a shared latent space via contrastive learning and introduces a novel Multi-Prompt Training strategy, enabling conditioning on arbitrary subsets of input modalities. This design allows the model to adapt to heterogeneous clinical inputs and generate multiple outputs jointly, preserving both semantic and structural coherence. We extensively validate XGeM: first we benchmark it against five competitors on the MIMIC-CXR dataset, a state-of-the-art dataset for multi-view Chest X-ray and radiological report generation. Secondly, we perform a Visual Turing Test with expert radiologists to assess the realism and clinical relevance of the generated data, ensuring alignment with real-world scenarios. Finally, we show how XGeM can support key medical data challenges such as anonymization, class imbalance, and data scarcity, underscoring its utility as a foundation model for medical data synthesis. Project page is at <a target="_blank" rel="noopener" href="https://cosbidev.github.io/XGeM/">https://cosbidev.github.io/XGeM/</a>. </p>
<blockquote>
<p>将人工智能应用于医学成像领域具有巨大的潜力，但仍面临着数据稀缺、隐私担忧以及需要稳健的多模式集成等挑战。虽然最近生成建模技术的进步已实现了高质量合成数据的生成，但现有方法通常仅限于单模式、单向的合成，因此缺乏同时合成多种模式并保持临床一致性的能力。为了解决这一挑战，我们引入了XGeM，这是一个拥有67.7亿参数的多模式生成模型，旨在支持医学数据模式之间的灵活、任意到任意的合成。XGeM通过对比学习构建了一个共享潜在空间，并引入了一种新颖的多提示训练策略，能够实现以输入模式的任意子集为条件。这种设计使模型能够适应多样化的临床输入，并联合生成多个输出，同时保持语义和结构的一致性。我们对XGeM进行了广泛验证：首先，我们在MIMIC-CXR数据集上与五种竞争对手进行了基准测试，这是一个用于多视图胸部X射线和放射学报告生成的最先进数据集。其次，我们与专家放射科医生进行了一场视觉图灵测试，以评估生成数据的现实主义和临床相关性，确保其与真实世界场景的一致性。最后，我们展示了XGeM如何支持医学数据的关键挑战，如匿名化、类别不平衡和数据稀缺，强调其在医学数据合成基础模型中的实用性。项目页面为：[<a target="_blank" rel="noopener" href="https://cosbidev.github.io/XGeM/]">https://cosbidev.github.io/XGeM/]</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04614v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>人工智能在医学成像领域具有巨大潜力，但仍面临数据稀缺、隐私顾虑和多模态融合等挑战。近期生成建模的进步推动了高质量合成数据的生成，但现有方法大多局限于单模态、单向合成，无法在多模态间进行联合合成并保持临床一致性。为解决此问题，我们推出XGeM，一个6.77亿参数的多模态生成模型，支持医学数据模态间的灵活任意合成。XGeM通过对比学习构建共享潜在空间，并引入全新多提示训练策略，可基于任意输入模态子集进行条件设置。此设计使模型能适应异质临床输入，联合生成多个输出，同时保持语义和结构的一致性。我们全面验证了XGeM的性能：首先在MIMIC-CXR数据集上与五种竞品进行基准测试，该数据集用于多视角胸部X光与放射报告生成，处于业界领先地位；其次，我们进行视觉图灵测试，邀请专家放射医师评估生成数据的真实性和临床相关性，确保与真实场景对齐；最后，我们展示了XGeM如何支持医学数据的关键挑战，如匿名化、类别不平衡和数据稀缺问题，凸显其作为医学数据合成基础模型的实用性。项目页面地址为<a target="_blank" rel="noopener" href="https://cosbidev.github.io/XGeM/">链接</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人工智能在医学成像中面临数据稀缺、隐私顾虑和多模态融合的挑战。</li>
<li>现有生成建模方法大多局限于单模态数据合成，缺乏多模态间的联合合成能力。</li>
<li>XGeM是一个多模态生成模型，支持医学数据模态间的任意合成，构建共享潜在空间并引入多提示训练策略。</li>
<li>XGeM可以在异质临床输入条件下工作，联合生成多个输出并保持临床一致性。</li>
<li>XGeM在MIMIC-CXR数据集上进行了基准测试，并与多种竞品对比表现出优势。</li>
<li>通过视觉图灵测试，XGeM生成的数据具有真实性和临床相关性。</li>
<li>XGeM有助于解决医学数据的关键挑战，如匿名化、类别不平衡和数据稀缺问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04614">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3b2ebf115139e72e8e0a87be1e7f335a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Bi-modality-medical-images-synthesis-by-a-bi-directional-discrete-process-matching-method"><a href="#Bi-modality-medical-images-synthesis-by-a-bi-directional-discrete-process-matching-method" class="headerlink" title="Bi-modality medical images synthesis by a bi-directional discrete   process matching method"></a>Bi-modality medical images synthesis by a bi-directional discrete   process matching method</h2><p><strong>Authors:Zhe Xiong, Qiaoqiao Ding, Xiaoqun Zhang</strong></p>
<p>Recently, medical image synthesis gains more and more popularity, along with the rapid development of generative models. Medical image synthesis aims to generate an unacquired image modality, often from other observed data modalities. Synthesized images can be used for clinical diagnostic assistance, data augmentation for model training and validation or image quality improving. In the meanwhile, the flow-based models are among the successful generative models for the ability of generating realistic and high-quality synthetic images. However, most flow-based models require to calculate flow ordinary different equation (ODE) evolution steps in synthesis process, for which the performances are significantly limited by heavy computation time due to a large number of time iterations. In this paper, we propose a novel flow-based model, namely bi-directional Discrete Process Matching (Bi-DPM) to accomplish the bi-modality image synthesis tasks. Different to other flow matching based models, we propose to utilize both forward and backward ODE flows and enhance the consistency on the intermediate images over a few discrete time steps, resulting in a synthesis process maintaining high-quality generations for both modalities under the guidance of paired data. Our experiments on three datasets of MRI T1&#x2F;T2 and CT&#x2F;MRI demonstrate that Bi-DPM outperforms other state-of-the-art flow-based methods for bi-modality image synthesis, delivering higher image quality with accurate anatomical regions. </p>
<blockquote>
<p>随着生成模型的快速发展，医学图像合成越来越受到欢迎。医学图像合成的目标是从其他已观察到的数据模态生成未获得的图像模态。合成的图像可用于临床诊断辅助、模型训练和验证的数据增强或图像质量改进。与此同时，基于流的模型是生成现实和高质量合成图像的成功的生成模型之一。然而，大多数基于流的模型需要在合成过程中计算流普通微分方程（ODE）的演化步骤，由于大量时间迭代，其性能受到计算时间长的限制。在本文中，我们提出了一种新型的基于流的模型，即双向离散过程匹配（Bi-DPM），以完成双向图像合成任务。与其他基于流匹配的模型不同，我们提出利用正向和反向ODE流，并增强几个离散时间步骤中中间图像的一致性，从而在保证配对数据引导的情况下，使两种模态的合成过程保持高质量生成。我们在MRI T1&#x2F;T2和CT&#x2F;MRI的三个数据集上的实验表明，Bi-DPM在双向图像合成方面优于其他最先进的基于流的方法，能够生成具有准确解剖区域的高质量图像。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.03977v3">PDF</a> </p>
<p><strong>Summary</strong><br>     本文提出一种新型双向离散过程匹配（Bi-DPM）流程模型，用于完成双模态图像合成任务。该模型利用正向和反向ODE流，提高中间图像在几个离散时间步骤上的一致性，在配对数据的指导下，保持两种模态的高质量生成。实验表明，Bi-DPM在双模态图像合成上优于其他先进流程模型，提供准确解剖区域的更高质量图像。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医疗图像合成随着生成模型的快速发展而越来越受欢迎。</li>
<li>医疗图像合成的目标是从其他观察到的数据模态生成未获得的图像模态。</li>
<li>合成图像可用于临床诊断辅助、模型训练和验证的数据增强以及图像质量改进。</li>
<li>流模型在生成真实高质量的合成图像方面表现出成功。</li>
<li>大多数流程模型需要在合成过程中计算流常微分方程（ODE）进化步骤，但由于大量时间迭代，性能受到计算时间长的限制。</li>
<li>本文提出了一种新型双向离散过程匹配（Bi-DPM）流程模型，该模型利用正向和反向ODE流，提高中间图像一致性，并在配对数据的指导下保持高质量生成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.03977">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2be9bbad66dbe4c8ecb0859470b0849f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-855c1d198747c5cb423a68534d0404b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-728a8e1fb0a350849f2ceab254792d71.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e9fd1cc9166d4660b1441ee3669c6e9.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="NuSegDG-Integration-of-Heterogeneous-Space-and-Gaussian-Kernel-for-Domain-Generalized-Nuclei-Segmentation"><a href="#NuSegDG-Integration-of-Heterogeneous-Space-and-Gaussian-Kernel-for-Domain-Generalized-Nuclei-Segmentation" class="headerlink" title="NuSegDG: Integration of Heterogeneous Space and Gaussian Kernel for   Domain-Generalized Nuclei Segmentation"></a>NuSegDG: Integration of Heterogeneous Space and Gaussian Kernel for   Domain-Generalized Nuclei Segmentation</h2><p><strong>Authors:Zhenye Lou, Qing Xu, Zekun Jiang, Xiangjian He, Zhen Chen, Yi Wang, Chenxin Li, Maggie M. He, Wenting Duan</strong></p>
<p>Domain-generalized nuclei segmentation refers to the generalizability of models to unseen domains based on knowledge learned from source domains and is challenged by various image conditions, cell types, and stain strategies. Recently, the Segment Anything Model (SAM) has made great success in universal image segmentation by interactive prompt modes (e.g., point and box). Despite its strengths, the original SAM presents limited adaptation to medical images. Moreover, SAM requires providing manual bounding box prompts for each object to produce satisfactory segmentation masks, so it is laborious in nuclei segmentation scenarios. To address these limitations, we propose a domain-generalizable framework for nuclei image segmentation, abbreviated to NuSegDG. Specifically, we first devise a Heterogeneous Space Adapter (HS-Adapter) to learn multi-dimensional feature representations of different nuclei domains by injecting a small number of trainable parameters into the image encoder of SAM. To alleviate the labor-intensive requirement of manual prompts, we introduce a Gaussian-Kernel Prompt Encoder (GKP-Encoder) to generate density maps driven by a single point, which guides segmentation predictions by mixing position prompts and semantic prompts. Furthermore, we present a Two-Stage Mask Decoder (TSM-Decoder) to effectively convert semantic masks to instance maps without the manual demand for morphological shape refinement. Based on our experimental evaluations, the proposed NuSegDG demonstrates state-of-the-art performance in nuclei instance segmentation, exhibiting superior domain generalization capabilities. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/xq141839/NuSegDG">https://github.com/xq141839/NuSegDG</a>. </p>
<blockquote>
<p>领域泛化核分割（Domain-Generalized Nuclei Segmentation）指的是模型在未见领域中的泛化能力，基于从源领域学到的知识，它面临着各种图像条件、细胞类型和染色策略的挑战。最近，通过交互式提示模式（如点和框），Segment Anything Model（SAM）在通用图像分割方面取得了巨大成功。尽管其强大，但原始SAM对医学图像的适应性有限。此外，SAM需要为每个对象提供手动边界框提示以产生令人满意的分割掩码，因此在细胞核分割场景中很繁琐。为了解决这些局限性，我们提出了一个用于细胞核图像分割的领域泛化框架，简称为NuSegDG。具体来说，我们首先设计了一个Heterogeneous Space Adapter（HS-Adapter），通过向SAM的图像编码器注入少量可训练参数，学习不同核领域的多维特征表示。为了减轻繁琐的手动提示要求，我们引入了Gaussian-Kernel Prompt Encoder（GKP-Encoder）来生成由单点驱动的概率密度图，通过混合位置提示和语义提示来指导分割预测。此外，我们提出了一种Two-Stage Mask Decoder（TSM-Decoder），能够有效地将语义掩码转换为实例图，无需对形态进行手动细化。我们的实验评估表明，所提出的NuSegDG在细胞核实例分割方面达到了最新性能水平，表现出卓越领域泛化能力。源代码可在<a target="_blank" rel="noopener" href="https://github.com/xq141839/NuSegDG">https://github.com/xq141839/NuSegDG</a> 获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11787v3">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>领域泛化细胞核分割指的是模型在未见领域中的泛化能力，基于从源领域学到的知识。面临各种图像条件、细胞类型和染色策略的挑战。最近，Segment Anything Model（SAM）在通用图像分割中通过交互式提示模式（如点和框）取得了巨大成功。然而，原始SAM在医学图像上的适应性有限。此外，SAM需要为每个对象提供手动边界框提示以产生满意的分割掩膜，因此在细胞核分割场景中很繁琐。针对这些局限性，我们提出了一个用于细胞核图像分割的领域泛化框架，简称为NuSegDG。具体而言，我们设计了一个Heterogeneous Space Adapter（HS-Adapter）来学习不同细胞核领域的多维特征表示，方法是通过在SAM的图像编码器中注入少量可训练参数。为了减轻对手动提示的劳动强度要求，我们引入了Gaussian-Kernel Prompt Encoder（GKP-Encoder）来生成由单点驱动的概率密度图，通过混合位置提示和语义提示来指导分割预测。此外，我们还提出了一个Two-Stage Mask Decoder（TSM-Decoder），可以有效地将语义掩膜转换为实例图，无需进行形态学形状精修的手动需求。我们的实验评估表明，所提出的NuSegDG在细胞核实例分割方面表现出卓越的性能和领域泛化能力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>领域泛化细胞核分割是医学图像分析的一个重要挑战，涉及模型在不同条件下的适应性。</li>
<li>Segment Anything Model（SAM）在通用图像分割中取得了成功，但在医学图像中的适应性有限。</li>
<li>提出的NuSegDG框架通过引入Heterogeneous Space Adapter（HS-Adapter）增强了模型对细胞核领域的适应性。</li>
<li>Gaussian-Kernel Prompt Encoder（GKP-Encoder）减轻了手动提示的需求，能够自动生成密度图进行分割预测。</li>
<li>Two-Stage Mask Decoder（TSM-Decoder）有效将语义掩膜转换为实例图，减少了形态学形状精修的需要。</li>
<li>NuSegDG框架在细胞核实例分割方面表现出卓越的性能和领域泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.11787">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-deb4f6b430c005484586b0e6b7c13ce6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e85e45bc2b3de3d2c25f60becee13d8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Anatomical-Foundation-Models-for-Brain-MRIs"><a href="#Anatomical-Foundation-Models-for-Brain-MRIs" class="headerlink" title="Anatomical Foundation Models for Brain MRIs"></a>Anatomical Foundation Models for Brain MRIs</h2><p><strong>Authors:Carlo Alberto Barbano, Matteo Brunello, Benoit Dufumier, Marco Grangetto</strong></p>
<p>Deep Learning (DL) in neuroimaging has become increasingly relevant for detecting neurological conditions and neurodegenerative disorders. One of the most predominant biomarkers in neuroimaging is represented by brain age, which has been shown to be a good indicator for different conditions, such as Alzheimer’s Disease. Using brain age for weakly supervised pre-training of DL models in transfer learning settings has also recently shown promising results, especially when dealing with data scarcity of different conditions. On the other hand, anatomical information of brain MRIs (e.g. cortical thickness) can provide important information for learning good representations that can be transferred to many downstream tasks. In this work, we propose AnatCL, an anatomical foundation model for brain MRIs that i.) leverages anatomical information in a weakly contrastive learning approach, and ii.) achieves state-of-the-art performances across many different downstream tasks. To validate our approach we consider 12 different downstream tasks for the diagnosis of different conditions such as Alzheimer’s Disease, autism spectrum disorder, and schizophrenia. Furthermore, we also target the prediction of 10 different clinical assessment scores using structural MRI data. Our findings show that incorporating anatomical information during pre-training leads to more robust and generalizable representations. Pre-trained models can be found at: <a target="_blank" rel="noopener" href="https://github.com/EIDOSLAB/AnatCL">https://github.com/EIDOSLAB/AnatCL</a>. </p>
<blockquote>
<p>深度学习（DL）在神经成像中对于检测神经系统疾病和神经退行性疾病变得越来越重要。神经成像中最主要的生物标志物之一就是脑年龄，它已被证明是不同疾病（如阿尔茨海默病）的良好指标。在迁移学习环境中，使用脑年龄对深度学习模型进行弱监督预训练也显示出有前途的结果，尤其是在处理不同疾病的稀缺数据时。另一方面，大脑MRI的解剖学信息（例如皮层厚度）可以为学习良好的表示提供重要信息，这些表示可以转移到许多下游任务。在这项工作中，我们提出了AnatCL，这是一个用于大脑MRI的解剖学基础模型，它一）以弱对比学习的方式利用解剖学信息，二）在许多不同的下游任务上实现了最先进的性能。为了验证我们的方法，我们考虑了12个不同的下游任务，用于诊断不同的疾病，如阿尔茨海默病、自闭症谱系障碍和精神分裂症。此外，我们还致力于使用结构MRI数据预测10种不同的临床评估分数。我们的研究结果表明，在预训练过程中融入解剖学信息会导致更稳健和可推广的表示。预训练模型可以在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/EIDOSLAB/AnatCL%E3%80%82">https://github.com/EIDOSLAB/AnatCL。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.07079v4">PDF</a> Updated version; added ablation study</p>
<p><strong>Summary</strong><br>     深度学习在神经影像中的应用对于检测神经性疾病和神经退行性疾病越来越重要。脑年龄是神经影像中最重要的生物标志物之一，可用于预测多种疾病，如阿尔茨海默症。利用脑年龄对深度学习模型进行弱监督预训练，在数据稀缺的情况下展现出良好的应用前景。另一方面，脑部MRI的结构信息（如皮层厚度）可以为学习良好表示提供重要信息，这些表示可应用于许多下游任务。本工作提出AnatCL模型，通过弱对比学习利用结构信息，并在多种下游任务上达到最新水平。该模型对阿尔茨海默症、自闭症谱系障碍和精神分裂症等疾病的诊断以及基于结构MRI数据的临床评分预测等12项下游任务进行了验证。研究发现，在预训练阶段融入结构信息有助于形成更稳健和通用的表示。预训练模型可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/EIDOSLAB/AnatCL%E3%80%82">https://github.com/EIDOSLAB/AnatCL。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习在神经影像中的应用在检测神经性疾病和神经退行性疾病方面的重要性。</li>
<li>脑年龄作为神经影像中的关键生物标志物，对预测不同疾病有良好表现。</li>
<li>利用脑年龄对深度学习模型进行弱监督预训练在数据稀缺情况下显示出良好的应用前景。</li>
<li>脑部MRI的结构信息（如皮层厚度）为学习良好表示提供重要信息，适用于多种下游任务。</li>
<li>AnatCL模型通过弱对比学习利用结构信息，并在多种下游任务上达到最新性能水平。</li>
<li>AnatCL模型在多种疾病的诊断和临床评分预测等任务进行了验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.07079">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-80a8b07b4a8975041f8e8ef1bd1c9578.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7f1cc02193fa8e25cb3e7975ebf0e86a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7a0fc5689e3647a89727f031a3d01bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ee4d03a5a95fb13af1874a268564efe.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="De-LightSAM-Modality-Decoupled-Lightweight-SAM-for-Generalizable-Medical-Segmentation"><a href="#De-LightSAM-Modality-Decoupled-Lightweight-SAM-for-Generalizable-Medical-Segmentation" class="headerlink" title="De-LightSAM: Modality-Decoupled Lightweight SAM for Generalizable   Medical Segmentation"></a>De-LightSAM: Modality-Decoupled Lightweight SAM for Generalizable   Medical Segmentation</h2><p><strong>Authors:Qing Xu, Jiaxuan Li, Xiangjian He, Chenxin Li, Fiseha B. Tesem, Wenting Duan, Zhen Chen, Rong Qu, Jonathan M. Garibaldi, Chang Wen Chen</strong></p>
<p>The universality of deep neural networks across different modalities and their generalization capabilities to unseen domains play an essential role in medical image segmentation. The recent segment anything model (SAM) has demonstrated strong adaptability across diverse natural scenarios. However, the huge computational costs, demand for manual annotations as prompts and conflict-prone decoding process of SAM degrade its generalization capabilities in medical scenarios. To address these limitations, we propose a modality-decoupled lightweight SAM for domain-generalized medical image segmentation, named De-LightSAM. Specifically, we first devise a lightweight domain-controllable image encoder (DC-Encoder) that produces discriminative visual features for diverse modalities. Further, we introduce the self-patch prompt generator (SP-Generator) to automatically generate high-quality dense prompt embeddings for guiding segmentation decoding. Finally, we design the query-decoupled modality decoder (QM-Decoder) that leverages a one-to-one strategy to provide an independent decoding channel for every modality, preventing mutual knowledge interference of different modalities. Moreover, we design a multi-modal decoupled knowledge distillation (MDKD) strategy to leverage robust common knowledge to complement domain-specific medical feature representations. Extensive experiments indicate that De-LightSAM outperforms state-of-the-arts in diverse medical imaging segmentation tasks, displaying superior modality universality and generalization capabilities. Especially, De-LightSAM uses only 2.0% parameters compared to SAM-H. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/xq141839/De-LightSAM">https://github.com/xq141839/De-LightSAM</a>. </p>
<blockquote>
<p>深度神经网络在不同模态之间的通用性以及它们对未见领域的泛化能力在医学图像分割中扮演着至关重要的角色。最近的“任何事物分割模型”(SAM)已在多种自然场景中表现出了强大的适应性。然而，SAM的巨大计算成本、对手动注释的提示需求以及易冲突解码过程，使其在医学场景中泛化能力受限。为了克服这些局限性，我们提出了一种用于领域泛化医学图像分割的模态解耦轻量化SAM，名为De-LightSAM。具体来说，我们首先设计了一种轻量级域可控图像编码器（DC-Encoder），用于生成不同模态的判别性视觉特征。此外，我们引入了自修补提示生成器（SP-Generator），用于自动生成高质量密集提示嵌入，以指导分割解码。最后，我们设计了查询解耦模态解码器（QM-Decoder），它采用一对一策略，为每种模态提供独立的解码通道，防止不同模态之间的知识相互干扰。此外，我们设计了一种多模态解耦知识蒸馏（MDKD）策略，利用稳健的通用知识来补充领域特定的医学特征表示。大量实验表明，在多种医学成像分割任务中，De-LightSAM的性能优于其他最新技术，显示出卓越的模态通用性和泛化能力。值得一提的是，相较于SAM-H，De-LightSAM仅使用其2.0%的参数。源代码可在<a target="_blank" rel="noopener" href="https://github.com/xq141839/De-LightSAM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/xq141839/De-LightSAM获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.14153v5">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对医学图像分割的深度学习模型De-LightSAM。该模型解决了SAM模型在医学场景中的计算成本高、需要手动标注提示和易冲突解码等问题。De-LightSAM通过设计轻量级域可控图像编码器、自补丁提示生成器和查询解耦模态解码器，提高了模型的跨模态通用性和计算效率。此外，还设计了多模态解耦知识蒸馏策略，以利用稳健的通用知识来补充特定领域的医学特征表示。实验表明，De-LightSAM在多种医学图像分割任务上优于其他模型，具有出色的跨模态通用性和泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>De-LightSAM解决了SAM模型在医学图像分割中的计算成本高、需要手动标注提示和易冲突解码的问题。</li>
<li>De-LightSAM通过设计轻量级域可控图像编码器（DC-Encoder）提高模型的跨模态通用性。</li>
<li>自补丁提示生成器（SP-Generator）能够自动产生高质量密集提示嵌入，引导分割解码。</li>
<li>查询解耦模态解码器（QM-Decoder）采用一对一策略，为每种模态提供独立解码通道，防止不同模态之间的知识干扰。</li>
<li>多模态解耦知识蒸馏（MDKD）策略利用稳健的通用知识来补充特定领域的医学特征表示。</li>
<li>De-LightSAM在多种医学图像分割任务上表现出优异的性能，优于其他模型。</li>
<li>De-LightSAM的参数使用量仅为SAM-H的2.0%，更加轻量级。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.14153">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-927d11647088fd6517a67367dc027896.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dcad1a905ce55ccc70a56032bd6777a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75afd551ed64a7c7f08b369661ff25bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37f628b13b37a070dce6d6ea394c4542.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-155d3d53c3e3300421298a1f4ddf9c16.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Comparing-Lasso-and-Adaptive-Lasso-in-High-Dimensional-Data-A-Genetic-Survival-Analysis-in-Triple-Negative-Breast-Cancer"><a href="#Comparing-Lasso-and-Adaptive-Lasso-in-High-Dimensional-Data-A-Genetic-Survival-Analysis-in-Triple-Negative-Breast-Cancer" class="headerlink" title="Comparing Lasso and Adaptive Lasso in High-Dimensional Data: A Genetic   Survival Analysis in Triple-Negative Breast Cancer"></a>Comparing Lasso and Adaptive Lasso in High-Dimensional Data: A Genetic   Survival Analysis in Triple-Negative Breast Cancer</h2><p><strong>Authors:Pilar González-Barquero, Rosa E. Lillo, Álvaro Méndez-Civieta</strong></p>
<p>In high-dimensional survival analysis, effective variable selection is crucial for both model interpretation and predictive performance. This paper investigates Cox regression with lasso and adaptive lasso penalties in genomic datasets where covariates far outnumber observations. We propose and evaluate four weight calculation strategies for adaptive lasso specifically designed for high-dimensional settings: ridge regression, principal component analysis (PCA), univariate Cox regression, and random survival forest (RSF) based weights. To address the inherent variability in high dimensional model selection, we develop a robust procedure that evaluates performance across multiple data partitions and selects variables based on a novel importance index. Extensive simulation studies demonstrate that adaptive lasso with ridge and PCA weights significantly outperforms standard lasso in variable selection accuracy while maintaining similar or better predictive performance across various correlation structures, censoring proportions (0-80%), and dimensionality settings. These improvements are particularly pronounced in highly-censored scenarios, making our approach valuable for real-world genetic studies with limited observed events. We apply our methodology to triple-negative breast cancer data with 234 patients, over 19500 variables and 82% censoring, identifying key genetic and clinical prognostic factors. Our findings demonstrate that adaptive lasso with appropriate weight calculation provides more stable and interpretable models for high-dimensional survival analysis. </p>
<blockquote>
<p>在高维生存分析中，有效的变量选择对于模型解释和预测性能都至关重要。本文研究了在基因组数据集中使用lasso和自适应lasso惩罚的Cox回归，其中协变量远远超过观测值。我们针对高维环境专门提出了四种自适应lasso权重计算策略，并对其进行了评估：岭回归、主成分分析（PCA）、单变量Cox回归和基于随机生存森林（RSF）的权重。为了解决高维模型选择中的固有变化性，我们开发了一种稳健的程序，该程序可以在多个数据分区中评估性能，并根据新的重要性指数选择变量。大量的模拟研究表明，使用岭和PCA权重的自适应lasso在变量选择准确性上显著优于标准lasso，同时在各种关联结构、审查比例（0-80%）和高维设置下保持相似或更好的预测性能。这些改进在高度审查的场景中尤为突出，使我们的方法对于具有有限观察事件的真实世界遗传研究具有价值。我们将该方法应用于234例三阴性乳腺癌数据，超过19500个变量和82%的审查，以确定关键的遗传和临床预后因素。我们的研究结果表明，使用适当权重计算的自适应lasso为高维生存分析提供了更稳定和可解释的模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.19213v2">PDF</a> 20 pages, 3 figures, 6 tables</p>
<p><strong>Summary</strong><br>     本文探讨了高维生存分析中，使用岭回归、主成分分析、单变量Cox回归和随机生存森林权重计算策略的自适应Lasso方法在基因组数据集上的变量选择效果。研究结果表明，自适应Lasso方法能够有效选择重要变量，在预测性能和模型解释性上表现优于标准Lasso方法。在高度截断的情况下表现尤为出色，适合应用于真实世界的遗传研究。研究最后通过三重阴性乳腺癌数据集验证了方法的实用性和有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高维生存分析中，有效的变量选择对于模型解释和预测性能至关重要。</li>
<li>自适应Lasso方法结合了Cox回归与惩罚项，特别适用于高维数据集。</li>
<li>提出了四种针对自适应Lasso的重量计算策略，包括岭回归、主成分分析、单变量Cox回归和随机生存森林。</li>
<li>自适应Lasso方法在多数据分区上的表现稳健，通过新型重要性指数选择变量。</li>
<li>模拟研究表明，自适应Lasso在变量选择准确性上显著优于标准Lasso，并且在各种相关性结构、截断比例和维度设置上维持相似的预测性能或有所提升。</li>
<li>在高度截断的场景下，自适应Lasso的改进尤为显著，这对真实世界的遗传研究具有实用价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.19213">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-470e2dd00c25ae52dacfd5b9f2311316.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccd5bebb35cb0defb403f3de166e4603.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Average-Calibration-Error-A-Differentiable-Loss-for-Improved-Reliability-in-Image-Segmentation"><a href="#Average-Calibration-Error-A-Differentiable-Loss-for-Improved-Reliability-in-Image-Segmentation" class="headerlink" title="Average Calibration Error: A Differentiable Loss for Improved   Reliability in Image Segmentation"></a>Average Calibration Error: A Differentiable Loss for Improved   Reliability in Image Segmentation</h2><p><strong>Authors:Theodore Barfoot, Luis Garcia-Peraza-Herrera, Ben Glocker, Tom Vercauteren</strong></p>
<p>Deep neural networks for medical image segmentation often produce overconfident results misaligned with empirical observations. Such miscalibration, challenges their clinical translation. We propose to use marginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss function to improve pixel-wise calibration without compromising segmentation quality. We show that this loss, despite using hard binning, is directly differentiable, bypassing the need for approximate but differentiable surrogate or soft binning approaches. Our work also introduces the concept of dataset reliability histograms which generalises standard reliability diagrams for refined visual assessment of calibration in semantic segmentation aggregated at the dataset level. Using mL1-ACE, we reduce average and maximum calibration error by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS 2021 dataset. We share our code here: <a target="_blank" rel="noopener" href="https://github.com/cai4cai/ACE-DLIRIS">https://github.com/cai4cai/ACE-DLIRIS</a> </p>
<blockquote>
<p>针对医学图像分割的深度学习网络常常产生过于自信的结果，这些结果与实证观察不符。这种误校准对其临床翻译构成了挑战。我们建议使用边际L1平均校准误差（mL1-ACE）作为一种新的辅助损失函数，以提高像素级的校准，同时不损害分割质量。我们表明，尽管使用了硬分箱，但这种损失是直接可微分的，从而绕过了需要使用近似但可微分的替代或软分箱方法的需求。我们的工作还引入了数据集可靠性直方图的概念，它推广了标准可靠性图，用于对数据集级别的语义分割校准进行精细的视觉评估。使用mL1-ACE，我们将平均和最大校准误差分别降低了45%和55%，同时在BraTS 2021数据集上保持了87%的Dice得分。我们的代码分享在这里：<a target="_blank" rel="noopener" href="https://github.com/cai4cai/ACE-DLIRIS">https://github.com/cai4cai/ACE-DLIRIS</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.06759v2">PDF</a> </p>
<p><strong>Summary</strong><br>     针对医学图像分割中深度神经网络产生的过度自信结果问题，提出使用边际L1平均校准误差（mL1-ACE）作为新型辅助损失函数，改善像素级校准而不影响分割质量。引入数据集可靠性直方图，便于精细可视化评估语义分割的校准情况。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度神经网络在医学图像分割中常产生过度自信的结果，与实际情况不符。</li>
<li>引入边际L1平均校准误差（mL1-ACE）作为新的辅助损失函数，改善像素级校准。</li>
<li>mL1-ACE损失函数使用硬分箱，但具备直接可微性，无需近似但可微的替代或软分箱方法。</li>
<li>引入数据集可靠性直方图，用于精细可视化评估语义分割的校准情况，便于在数据集层面进行校准评估。</li>
<li>使用mL1-ACE，平均校准误差和最大校准误差分别降低了45%和55%。</li>
<li>在BraTS 2021数据集上，维持87%的Dice得分。</li>
<li>研究成果已共享于：<a target="_blank" rel="noopener" href="https://github.com/cai4cai/ACE-DLIRIS">https://github.com/cai4cai/ACE-DLIRIS</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.06759">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e8263ab8c92ec21703b6965c65f954cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9d6ad7c0dd2036cb14bf4af38aa6af2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b92c85046bac61be54c371272062d92f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9427cce65b7878cf6ea8e534a2d40b53.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Improving-Robustness-and-Reliability-in-Medical-Image-Classification-with-Latent-Guided-Diffusion-and-Nested-Ensembles"><a href="#Improving-Robustness-and-Reliability-in-Medical-Image-Classification-with-Latent-Guided-Diffusion-and-Nested-Ensembles" class="headerlink" title="Improving Robustness and Reliability in Medical Image Classification   with Latent-Guided Diffusion and Nested-Ensembles"></a>Improving Robustness and Reliability in Medical Image Classification   with Latent-Guided Diffusion and Nested-Ensembles</h2><p><strong>Authors:Xing Shen, Hengguan Huang, Brennan Nichyporuk, Tal Arbel</strong></p>
<p>Once deployed, medical image analysis methods are often faced with unexpected image corruptions and noise perturbations. These unknown covariate shifts present significant challenges to deep learning based methods trained on “clean” images. This often results in unreliable predictions and poorly calibrated confidence, hence hindering clinical applicability. While recent methods have been developed to address specific issues such as confidence calibration or adversarial robustness, no single framework effectively tackles all these challenges simultaneously. To bridge this gap, we propose LaDiNE, a novel ensemble learning method combining the robustness of Vision Transformers with diffusion-based generative models for improved reliability in medical image classification. Specifically, transformer encoder blocks are used as hierarchical feature extractors that learn invariant features from images for each ensemble member, resulting in features that are robust to input perturbations. In addition, diffusion models are used as flexible density estimators to estimate member densities conditioned on the invariant features, leading to improved modeling of complex data distributions while retaining properly calibrated confidence. Extensive experiments on tuberculosis chest X-rays and melanoma skin cancer datasets demonstrate that LaDiNE achieves superior performance compared to a wide range of state-of-the-art methods by simultaneously improving prediction accuracy and confidence calibration under unseen noise, adversarial perturbations, and resolution degradation. </p>
<blockquote>
<p>部署医疗图像分析方法后，它们经常面临意外的图像损坏和噪声干扰。这些未知的协变量变化给基于“干净”图像训练的深度学习方法带来了巨大的挑战。这通常会导致预测结果不可靠和置信度校准不佳，从而阻碍了其在临床上的适用性。虽然最近已经开发了一些方法来解决信心校准或对抗稳健性等问题，但没有单一框架能有效地同时解决所有这些挑战。为了填补这一空白，我们提出了LaDiNE，这是一种新型集成学习方法，将视觉变压器的稳健性与基于扩散的生成模型的可靠性相结合，以提高医疗图像分类的可靠性。具体来说，我们使用变压器编码器块作为分层特征提取器，从每个集成成员学习图像的恒定特征，从而产生对输入扰动具有鲁棒性的特征。此外，扩散模型被用作灵活的密度估计器，根据恒定特征估计成员密度，从而在保留适当校准的置信度的同时，实现对复杂数据分布的改进建模。在肺结核胸部X射线和黑色素瘤皮肤癌数据集上的大量实验表明，LaDiNE通过同时提高预测精度和置信度校准，在未见的噪声、对抗性扰动和分辨率下降的情况下，实现了与一系列最先进的方法相比的卓越性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15952v5">PDF</a> Accepted to IEEE Transactions on Medical Imaging, 2025</p>
<p><strong>Summary</strong><br>医学图像分析方法在实际应用中常遭遇图像损坏和噪声干扰等挑战。针对此问题，我们提出一种新型集成学习方法LaDiNE，结合Vision Transformer和扩散生成模型的稳健性，提高医学图像分类的可靠性。实验证明，LaDiNE在不同噪声、对抗性干扰和分辨率下降的情况下，预测准确率和置信度校准均优于其他先进方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分析方法面临未知协变量偏移的挑战，影响深度学习的预测可靠性和临床适用性。</li>
<li>LaDiNE是一种新型集成学习方法，结合Vision Transformer和扩散生成模型的优点。</li>
<li>LaDiNE通过变压器编码器块提取图像的不变特征，增强对输入扰动的稳健性。</li>
<li>扩散模型作为灵活的密度估计器，估计成员密度并基于不变特征进行条件建模。</li>
<li>LaDiNE能提高预测准确性和置信度校准，在未见噪声、对抗性干扰和分辨率降低的情况下表现优异。</li>
<li>实验在肺结核胸部X射线和黑色素瘤皮肤癌数据集上验证了LaDiNE的优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.15952">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5801b261fc3288b77c742be779e566cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69469cc40b7d67cc7929e4497c22279a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d7d358148a8bd51be061c817c1c5b42e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-09/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5079c80e2fc378a16c514f6a221b54e7.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-07-09  Spatio-Temporal LLM Reasoning about Environments and Actions
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-08/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-913e399ca3527dd47d6feb49ff889d86.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-07-08  TTRL Test-Time Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24474.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
