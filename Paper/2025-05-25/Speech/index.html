<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-25  In-Context Learning Boosts Speech Recognition via Human-like Adaptation   to Speakers and Language Varieties">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-5aa9a4d98f38fb6ecbd409bf9c96945f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    7.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    29 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-25-æ›´æ–°"><a href="#2025-05-25-æ›´æ–°" class="headerlink" title="2025-05-25 æ›´æ–°"></a>2025-05-25 æ›´æ–°</h1><h2 id="In-Context-Learning-Boosts-Speech-Recognition-via-Human-like-Adaptation-to-Speakers-and-Language-Varieties"><a href="#In-Context-Learning-Boosts-Speech-Recognition-via-Human-like-Adaptation-to-Speakers-and-Language-Varieties" class="headerlink" title="In-Context Learning Boosts Speech Recognition via Human-like Adaptation   to Speakers and Language Varieties"></a>In-Context Learning Boosts Speech Recognition via Human-like Adaptation   to Speakers and Language Varieties</h2><p><strong>Authors:Nathan Roll, Calbert Graham, Yuka Tatsumi, Kim Tien Nguyen, Meghan Sumner, Dan Jurafsky</strong></p>
<p>Human listeners readily adjust to unfamiliar speakers and language varieties through exposure, but do these adaptation benefits extend to state-of-the-art spoken language models? We introduce a scalable framework that allows for in-context learning (ICL) in Phi-4 Multimodal using interleaved task prompts and audio-text pairs, and find that as few as 12 example utterances (~50 seconds) at inference time reduce word error rates by a relative 19.7% (1.2 pp.) on average across diverse English corpora. These improvements are most pronounced in low-resource varieties, when the context and target speaker match, and when more examples are providedâ€“though scaling our procedure yields diminishing marginal returns to context length. Overall, we find that our novel ICL adaptation scheme (1) reveals a similar performance profile to human listeners, and (2) demonstrates consistent improvements to automatic speech recognition (ASR) robustness across diverse speakers and language backgrounds. While adaptation succeeds broadly, significant gaps remain for certain varieties, revealing where current models still fall short of human flexibility. We release our prompts and code on GitHub. </p>
<blockquote>
<p>äººç±»å¬ä¼—é€šè¿‡æ¥è§¦å¾ˆå®¹æ˜“é€‚åº”ä¸ç†Ÿæ‚‰çš„è¯´è¯è€…å’Œè¯­è¨€å˜ä½“ï¼Œä½†è¿™äº›é€‚åº”å¥½å¤„æ˜¯å¦å»¶ä¼¸åˆ°æœ€å…ˆè¿›çš„å£è¯­æ¨¡å‹å‘¢ï¼Ÿæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œå…è®¸åœ¨Phi-4å¤šæ¨¡å¼ä¸­ä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ï¼Œé€šè¿‡äº¤æ›¿çš„ä»»åŠ¡æç¤ºå’ŒéŸ³é¢‘æ–‡æœ¬å¯¹è¿›è¡Œå­¦ä¹ ï¼Œæˆ‘ä»¬å‘ç°ï¼Œåœ¨æ¨ç†æ—¶é—´åªéœ€12ä¸ªç¤ºä¾‹è¯è¯­ï¼ˆçº¦50ç§’ï¼‰ï¼Œå°±å¯ä»¥åœ¨å¤šç§è‹±è¯­è¯­æ–™åº“ä¸­å¹³å‡é™ä½ç›¸å¯¹å­—é”™è¯¯ç‡19.7%ï¼ˆ1.2ä¸ªç™¾åˆ†ç‚¹ï¼‰ã€‚è¿™äº›æ”¹è¿›åœ¨ä½èµ„æºå˜ä½“ä¸­æœ€ä¸ºæ˜æ˜¾ï¼Œå½“ä¸Šä¸‹æ–‡å’Œç›®æ ‡è¯´è¯è€…åŒ¹é…æ—¶ï¼Œä»¥åŠæä¾›æ›´å¤šç¤ºä¾‹æ—¶â€”â€”å°½ç®¡æ‰©å¤§æˆ‘ä»¬çš„ç¨‹åºä¼šç»™ä¸Šä¸‹æ–‡é•¿åº¦å¸¦æ¥è¾¹é™…é€’å‡çš„æ”¶ç›Šã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬æ–°é¢–çš„ICLé€‚åº”æ–¹æ¡ˆï¼ˆ1ï¼‰å±•ç°äº†ä¸äººç±»å¬ä¼—ç›¸ä¼¼çš„æ€§èƒ½ç‰¹å¾ï¼Œï¼ˆ2ï¼‰åœ¨å¤šç§è¯´è¯è€…å’Œè¯­è¨€èƒŒæ™¯ä¸‹ï¼Œå¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç¨³å¥æ€§è¡¨ç°å‡ºäº†æŒç»­çš„æ”¹è¿›ã€‚è™½ç„¶é€‚åº”å¤§ä½“ä¸Šæ˜¯æˆåŠŸçš„ï¼Œä½†å¯¹äºæŸäº›å˜ä½“ä»å­˜åœ¨è¾ƒå¤§å·®è·ï¼Œè¿™æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨çµæ´»æ€§æ–¹é¢ä»ç„¶ä¸åŠäººç±»ã€‚æˆ‘ä»¬åœ¨GitHubä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„æç¤ºå’Œä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14887v1">PDF</a> 15 pages; 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…è®¸åœ¨Phi-4å¤šæ¨¡å¼ä¸­ä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ¥é€‚åº”ä¸ç†Ÿæ‚‰çš„è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡é—´éš”çš„ä»»åŠ¡æç¤ºå’ŒéŸ³é¢‘æ–‡æœ¬å¯¹ï¼Œå‘ç°åªéœ€åœ¨æ¨ç†æ—¶é—´æä¾›å°‘é‡çš„ç¤ºä¾‹è¯è¯­ï¼ˆçº¦50ç§’ï¼‰ï¼Œå¹³å‡é™ä½ç›¸å¯¹è¯é”™è¯¯ç‡ä¸ºé™ä½è¯é”™è¯¯ç‡çš„ç™¾åˆ†æ¯”æ¯”æœªæ”¹è¿›çš„åŸºçº¿é™ä½äº†19.7%ï¼ˆå‡å°‘ç™¾åˆ†ä¹‹ä¸€ï¼‰ï¼Œå‡å°‘äº†åœ¨å¤šæ ·åŒ–çš„è‹±è¯­è¯­æ–™åº“ä¸Šçš„è¯¯å·®ç‡ã€‚æ”¹å–„æœ€å¤§çš„åœºåˆæ˜¯åœ¨èµ„æºå’Œç›®æ ‡æ¼”è®²è€…ç›¸åŒ¹é…æ—¶æä¾›æ›´å¤šç¤ºä¾‹çš„åœºåˆï¼Œè€Œå¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦åˆ™äº§ç”Ÿè¾¹é™…æ”¶ç›Šé€’å‡æ•ˆåº”ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬æ–‡æå‡ºçš„ICLé€‚åº”æ–¹æ¡ˆï¼ˆ1ï¼‰è¡¨ç°å‡ºä¸äººç±»å¬ä¼—ç›¸ä¼¼çš„æ€§èƒ½ç‰¹å¾ï¼Œï¼ˆ2ï¼‰è¯æ˜äº†åœ¨å¤šæ ·åŒ–å’ŒèƒŒæ™¯å„å¼‚çš„æ¼”è®²è€…ä¸­è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç¨³å¥æ€§çš„æŒç»­æ”¹è¿›ã€‚å°½ç®¡é€‚åº”æ˜¯æˆåŠŸçš„ï¼Œä½†å¯¹äºæŸäº›è¯­è¨€å˜ä½“ä»ç„¶å­˜åœ¨æ˜¾è‘—çš„å·®è·ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹ä»ç„¶æ— æ³•ä¸äººç±»çµæ´»æ€§å®Œå…¨åŒ¹é…çš„åœ°æ–¹ã€‚æˆ‘ä»¬å·²åœ¨GitHubä¸Šå‘å¸ƒäº†æç¤ºå’Œä»£ç ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œå…è®¸ä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ¥å¢å¼ºè¯­è¨€æ¨¡å‹çš„é€‚åº”æ€§ã€‚</li>
<li>é€šè¿‡é—´éš”çš„ä»»åŠ¡æç¤ºå’ŒéŸ³é¢‘æ–‡æœ¬å¯¹ï¼Œå°‘é‡ç¤ºä¾‹è¯è¯­å³å¯æ”¹å–„è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å¹³å‡é™ä½è¯é”™è¯¯ç‡æ˜¾è‘—ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå’Œç›®æ ‡æ¼”è®²è€…ç›¸åŒ¹é…çš„æƒ…å†µä¸‹æä¾›æ›´å¤šç¤ºä¾‹æ—¶æ”¹å–„æ›´æ˜æ˜¾ã€‚ä½†ä¸Šä¸‹æ–‡é•¿åº¦çš„è¾¹é™…æ•ˆç›Šæœ‰æ‰€é€’å‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14887">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-87ef2ef2d4772091c289e3d0795a3bc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d720ef224c58fc3de55cc6c07c53eb69.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-49c06159b3b279027b2f3ae2196ebf2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-426db9cc572c13a427bcde7c4d5ec2f0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Towards-Inclusive-ASR-Investigating-Voice-Conversion-for-Dysarthric-Speech-Recognition-in-Low-Resource-Languages"><a href="#Towards-Inclusive-ASR-Investigating-Voice-Conversion-for-Dysarthric-Speech-Recognition-in-Low-Resource-Languages" class="headerlink" title="Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric   Speech Recognition in Low-Resource Languages"></a>Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric   Speech Recognition in Low-Resource Languages</h2><p><strong>Authors:Chin-Jou Li, Eunjung Yeo, Kwanghee Choi, Paula Andrea PÃ©rez-Toro, Masao Someki, Rohan Kumar Das, Zhengjun Yue, Juan Rafael Orozco-Arroyave, Elmar NÃ¶th, David R. Mortensen</strong></p>
<p>Automatic speech recognition (ASR) for dysarthric speech remains challenging due to data scarcity, particularly in non-English languages. To address this, we fine-tune a voice conversion model on English dysarthric speech (UASpeech) to encode both speaker characteristics and prosodic distortions, then apply it to convert healthy non-English speech (FLEURS) into non-English dysarthric-like speech. The generated data is then used to fine-tune a multilingual ASR model, Massively Multilingual Speech (MMS), for improved dysarthric speech recognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE (Tamil) demonstrates that VC with both speaker and prosody conversion significantly outperforms the off-the-shelf MMS performance and conventional augmentation techniques such as speed and tempo perturbation. Objective and subjective analyses of the generated data further confirm that the generated speech simulates dysarthric characteristics. </p>
<blockquote>
<p>é’ˆå¯¹å‘éŸ³å›°éš¾ï¼ˆå‘éŸ³éšœç¢ï¼‰è¯­éŸ³çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç”±äºå…¶æ•°æ®ç¨€ç¼ºæ€§ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨éè‹±è¯­ç¯å¢ƒä¸­æ›´æ˜¯å¦‚æ­¤ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨è‹±è¯­å‘éŸ³å›°éš¾è¯­éŸ³ï¼ˆUASpeechï¼‰ä¸Šå¯¹è¯­éŸ³è½¬æ¢æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥ç¼–ç è¯´è¯äººçš„ç‰¹æ€§å’ŒéŸµå¾‹å¤±çœŸï¼Œç„¶åå°†å…¶åº”ç”¨äºå°†å¥åº·çš„éè‹±è¯­è¯­éŸ³ï¼ˆFLEURSï¼‰è½¬æ¢ä¸ºéè‹±è¯­å‘éŸ³éšœç¢ç±»è¯­éŸ³ã€‚ç„¶åä½¿ç”¨ç”Ÿæˆçš„æ•°æ®å¯¹å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹Massively Multilingual Speechï¼ˆMMSï¼‰è¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜å¯¹å‘éŸ³éšœç¢è¯­éŸ³çš„è¯†åˆ«èƒ½åŠ›ã€‚åœ¨PC-GITAï¼ˆè¥¿ç­ç‰™è¯­ï¼‰ã€EasyCallï¼ˆæ„å¤§åˆ©è¯­ï¼‰å’ŒSSNCEï¼ˆæ³°ç±³å°”è¯­ï¼‰ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒåŒæ—¶å®ç°è¯´è¯äººå’ŒéŸµå¾‹è½¬æ¢çš„è¯­éŸ³è½¬æ¢æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æˆçš„MMSæ€§èƒ½å’Œä¼ ç»Ÿçš„å¢å¼ºæŠ€æœ¯ï¼Œå¦‚é€Ÿåº¦å’ŒèŠ‚å¥æ‰°åŠ¨ã€‚å¯¹ç”Ÿæˆæ•°æ®çš„å®¢è§‚å’Œä¸»è§‚åˆ†æè¿›ä¸€æ­¥è¯å®ï¼Œç”Ÿæˆçš„è¯­éŸ³æ¨¡æ‹Ÿäº†å‘éŸ³éšœç¢çš„ç‰¹å¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14874v1">PDF</a> 5 pages, 1 figure, Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†å¦‚ä½•åˆ©ç”¨è‹±è¯­å‘éŸ³éšœç¢è¯­éŸ³ï¼ˆUASpeechï¼‰è®­ç»ƒè¯­éŸ³è½¬æ¢æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥ç¼–ç è¯´è¯äººçš„ç‰¹å¾å’ŒéŸµå¾‹æ‰­æ›²ã€‚ç„¶åï¼Œå°†æ­¤æ¨¡å‹åº”ç”¨äºå°†å¥åº·çš„éè‹±è¯­è¯­éŸ³ï¼ˆFLEURSï¼‰è½¬æ¢ä¸ºéè‹±è¯­çš„å‘éŸ³éšœç¢è¯­éŸ³ã€‚ç”Ÿæˆçš„è¯­éŸ³æ•°æ®ç”¨äºå¾®è°ƒå¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹Massively Multilingual Speechï¼ˆMMSï¼‰ï¼Œä»¥æé«˜å¯¹å‘éŸ³éšœç¢è¯­éŸ³çš„è¯†åˆ«èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒåŒæ—¶è½¬æ¢è¯´è¯äººå’ŒéŸµå¾‹çš„è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰æ˜¾è‘—ä¼˜äºç°æˆçš„MMSæ€§èƒ½å’Œä¼ ç»Ÿçš„å¢å¼ºæŠ€æœ¯ï¼Œå¦‚é€Ÿåº¦å’ŒèŠ‚å¥æ‰°åŠ¨ã€‚å¯¹ç”Ÿæˆæ•°æ®çš„å®¢è§‚å’Œä¸»è§‚åˆ†æè¿›ä¸€æ­¥è¯å®ï¼Œç”Ÿæˆçš„è¯­éŸ³æ¨¡æ‹Ÿäº†å‘éŸ³éšœç¢çš„ç‰¹å¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®ç¨€ç¼ºæ˜¯éè‹±è¯­å‘éŸ³éšœç¢è¯­éŸ³è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡è‹±è¯­å‘éŸ³éšœç¢è¯­éŸ³ï¼ˆUASpeechï¼‰è®­ç»ƒè¯­éŸ³è½¬æ¢æ¨¡å‹ä»¥ç¼–ç è¯´è¯äººçš„ç‰¹å¾å’ŒéŸµå¾‹æ‰­æ›²ã€‚</li>
<li>æ¨¡å‹åº”ç”¨äºå°†å¥åº·éè‹±è¯­è¯­éŸ³è½¬æ¢ä¸ºéè‹±è¯­çš„å‘éŸ³éšœç¢è¯­éŸ³ã€‚</li>
<li>ç”Ÿæˆçš„è¯­éŸ³æ•°æ®ç”¨äºå¾®è°ƒå¤šè¯­è¨€ASRæ¨¡å‹Massively Multilingual Speechï¼ˆMMSï¼‰ã€‚</li>
<li>è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰åœ¨è½¬æ¢è¯´è¯äººå’ŒéŸµå¾‹æ–¹é¢æ˜¾è‘—æé«˜äº†ASRæ€§èƒ½ã€‚</li>
<li>è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒVCæ–¹æ³•ä¼˜äºä¼ ç»Ÿçš„å¢å¼ºæŠ€æœ¯å’Œç°æˆçš„ASRæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14874">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7405e8a441d5f6012f0f152aff55ba4f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-100f58f32e4ff2a6eb1693e7a33a951d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78056c740a7209de2a0bdbc167f264df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-569360d203bc2186acb89f7d6ccd725e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b8274cfe195b12f29332b0779ceb8c6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Mitigating-Subgroup-Disparities-in-Multi-Label-Speech-Emotion-Recognition-A-Pseudo-Labeling-and-Unsupervised-Learning-Approach"><a href="#Mitigating-Subgroup-Disparities-in-Multi-Label-Speech-Emotion-Recognition-A-Pseudo-Labeling-and-Unsupervised-Learning-Approach" class="headerlink" title="Mitigating Subgroup Disparities in Multi-Label Speech Emotion   Recognition: A Pseudo-Labeling and Unsupervised Learning Approach"></a>Mitigating Subgroup Disparities in Multi-Label Speech Emotion   Recognition: A Pseudo-Labeling and Unsupervised Learning Approach</h2><p><strong>Authors:Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee</strong></p>
<p>While subgroup disparities and performance bias are increasingly studied in computational research, fairness in categorical Speech Emotion Recognition (SER) remains underexplored. Existing methods often rely on explicit demographic labels, which are difficult to obtain due to privacy concerns. To address this limitation, we introduce an Implicit Demography Inference (IDI) module that leverages pseudo-labeling from a pre-trained model and unsupervised learning using k-means clustering to mitigate bias in SER. Our experiments show that pseudo-labeling IDI reduces subgroup disparities, improving fairness metrics by over 33% with less than a 3% decrease in SER accuracy. Also, the unsupervised IDI yields more than a 26% improvement in fairness metrics with a drop of less than 4% in SER performance. Further analyses reveal that the unsupervised IDI consistently mitigates race and age disparities, demonstrating its potential in scenarios where explicit demographic information is unavailable. </p>
<blockquote>
<p>åœ¨è®¡ç®—ç ”ç©¶ä¸­ï¼Œè™½ç„¶å¯¹å­ç¾¤ä½“å·®å¼‚å’Œæ€§èƒ½åè§çš„ç ”ç©¶è¶Šæ¥è¶Šå¤šï¼Œä½†åœ¨åˆ†ç±»è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ä¸­çš„å…¬å¹³æ€§ä»ç„¶è¢«å¿½è§†ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ˜ç¡®çš„äººå£ç»Ÿè®¡æ ‡ç­¾ï¼Œä½†ç”±äºéšç§æ‹…å¿§ï¼Œè¿™äº›æ ‡ç­¾å¾ˆéš¾è·å¾—ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªéšå¼äººå£ç»Ÿè®¡æ¨æ–­ï¼ˆIDIï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ä¼ªæ ‡ç­¾å’Œé€šè¿‡k-meansèšç±»è¿›è¡Œæ— ç›‘ç£å­¦ä¹ ï¼Œä»¥å‡è½»SERä¸­çš„åè§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¼ªæ ‡ç­¾IDIå‡å°‘äº†å­ç¾¤ä½“å·®å¼‚ï¼Œé€šè¿‡æé«˜è¶…è¿‡33%çš„å…¬å¹³æ€§æŒ‡æ ‡ï¼ŒåŒæ—¶SERå‡†ç¡®ç‡ä¸‹é™ä¸åˆ°3%ã€‚æ­¤å¤–ï¼Œæ— ç›‘ç£çš„IDIåœ¨å…¬å¹³æ€§æŒ‡æ ‡ä¸Šæé«˜äº†è¶…è¿‡26%ï¼ŒåŒæ—¶SERæ€§èƒ½ä¸‹é™ä¸åˆ°4%ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œæ— ç›‘ç£çš„IDIæŒç»­ç¼“è§£äº†ç§æ—å’Œå¹´é¾„å·®å¼‚ï¼Œè¯æ˜äº†åœ¨ç¼ºä¹æ˜ç¡®äººå£ç»Ÿè®¡ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œå…¶æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14449v2">PDF</a> Accepted by InterSpeech 2025. 7 pages including 2 pages of appendix</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥æ–‡æœ¬ä¸»è¦æ¢è®¨äº†åœ¨è®¡ç®—ç ”ç©¶ä¸­è¶Šæ¥è¶Šå—å…³æ³¨çš„å­ç¾¤å·®å¼‚å’Œæ€§èƒ½åå·®é—®é¢˜ï¼Œå¹¶æŒ‡å‡ºåˆ†ç±»è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ä¸­çš„å…¬å¹³æ€§å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºéš¾ä»¥è·å¾—çš„æ˜ç¡®äººå£ç»Ÿè®¡æ ‡ç­¾ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†éšå¼äººå£ç»Ÿè®¡æ¨æ–­ï¼ˆIDIï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ä¼ªæ ‡ç­¾å’ŒKå‡å€¼èšç±»ç­‰æ— ç›‘ç£å­¦ä¹ æŠ€æœ¯æ¥ç¼“è§£SERä¸­çš„åè§ã€‚å®éªŒè¡¨æ˜ï¼Œä¼ªæ ‡ç­¾åŒ–çš„IDIèƒ½å¤Ÿå‡å°‘å­ç¾¤å·®å¼‚ï¼Œåœ¨å…¬å¹³åº¦æŒ‡æ ‡ä¸Šæé«˜è¶…è¿‡ç™¾åˆ†ä¹‹ä¸‰åä¸‰çš„åŒæ—¶ä»…é™ä½SERå‡†ç¡®ç‡ä¸åˆ°ç™¾åˆ†ä¹‹ä¸‰ã€‚æ­¤å¤–ï¼Œæ— ç›‘ç£çš„IDIåœ¨å…¬å¹³åº¦æŒ‡æ ‡ä¸Šæé«˜äº†è¶…è¿‡ç™¾åˆ†ä¹‹äºŒåå…­ï¼Œä¸”åªé™ä½äº†ä¸åˆ°ç™¾åˆ†ä¹‹å››çš„SERæ€§èƒ½ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œæ— ç›‘ç£çš„IDIåœ¨ç¼“è§£ç§æ—å’Œå¹´é¾„å·®å¼‚æ–¹é¢è¡¨ç°ç¨³å®šï¼Œæ˜¾ç¤ºå‡ºåœ¨ç¼ºä¹æ˜ç¡®äººå£ç»Ÿè®¡ä¿¡æ¯çš„æƒ…å†µä¸‹å…·æœ‰æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰ç ”ç©¶çš„å…³æ³¨ç‚¹æ˜¯è¯†åˆ«å’Œåˆ†æè®¡ç®—ç ”ç©¶ä¸­æ—¥ç›Šå‡¸æ˜¾çš„å­ç¾¤å·®å¼‚å’Œæ€§èƒ½åå·®é—®é¢˜ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰çš„åˆ†ç±»è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ–¹æ³•åœ¨å¤„ç†å…¬å¹³æ€§æ–¹é¢çš„ä¸è¶³ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è¿‡äºä¾èµ–éš¾ä»¥è·å–çš„äººå£ç»Ÿè®¡æ ‡ç­¾ä½œä¸ºè¾“å…¥ï¼Œå› æ­¤å­˜åœ¨å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ç§åä¸ºéšå¼äººå£ç»Ÿè®¡æ¨æ–­ï¼ˆIDIï¼‰çš„æ–°æ¨¡å—ã€‚è¯¥æ¨¡å—ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ä¼ªæ ‡ç­¾å’Œæ— ç›‘ç£å­¦ä¹ æŠ€æœ¯æ¥ç¼“è§£åè§é—®é¢˜ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯äº†ä¼ªæ ‡ç­¾åŒ–çš„IDIåœ¨å‡å°‘å­ç¾¤å·®å¼‚å’Œæé«˜å…¬å¹³æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå…¶æ”¹è¿›æ•ˆæœæ˜¾è‘—ä¸”å¯¹SERå‡†ç¡®ç‡çš„å½±å“è¾ƒå°ã€‚</li>
<li>æ— ç›‘ç£çš„IDIåœ¨å…¬å¹³åº¦æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºæ›´å¥½çš„æ•ˆæœï¼Œå¹¶ä¸”åœ¨ç»´æŒç›¸å¯¹è¾ƒé«˜çš„SERæ€§èƒ½çš„åŒæ—¶è¿›è¡Œæ”¹è¿›ã€‚è¿™è¡¨æ˜æ— ç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†ç¼ºä¹æ˜ç¡®äººå£ç»Ÿè®¡ä¿¡æ¯çš„æƒ…å†µæ—¶å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c19382ab92a3ef83332b2059c94327f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-902a56032e981efba948702f7456c8a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1446e47e2efdff41f6751858208bc760.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d6de9cd829211815b2e3bfe41807c93.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7faca6b6383e210de16348d3a3c75829.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5aa9a4d98f38fb6ecbd409bf9c96945f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Improving-Noise-Robustness-of-LLM-based-Zero-shot-TTS-via-Discrete-Acoustic-Token-Denoising"><a href="#Improving-Noise-Robustness-of-LLM-based-Zero-shot-TTS-via-Discrete-Acoustic-Token-Denoising" class="headerlink" title="Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete   Acoustic Token Denoising"></a>Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete   Acoustic Token Denoising</h2><p><strong>Authors:Ye-Xin Lu, Hui-Peng Du, Fei Liu, Yang Ai, Zhen-Hua Ling</strong></p>
<p>Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ–¹æ³•å€¾å‘äºä¿ç•™éŸ³é¢‘æç¤ºçš„å£°å­¦ç¯å¢ƒï¼Œä½†å½“éŸ³é¢‘æç¤ºåŒ…å«å™ªå£°æ—¶ï¼Œä¼šå¯¼è‡´åˆæˆè¯­éŸ³è´¨é‡ä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºç¥ç»ç½‘ç»œç¼–è§£ç å™¨çš„è¯­éŸ³å»å™ªå™¨ï¼Œå¹¶å°†å…¶ä¸å…ˆè¿›çš„LLM-based TTSæ¨¡å‹LauraTTSç›¸ç»“åˆï¼Œå®ç°äº†å™ªå£°é²æ£’çš„é›¶æ ·æœ¬TTSã€‚æ‰€æå‡ºçš„ç¼–è§£ç å™¨å»å™ªå™¨ç”±éŸ³é¢‘ç¼–è§£ç å™¨ã€ä»¤ç‰Œå»å™ªå™¨å’ŒåµŒå…¥ç²¾ç‚¼å™¨ç»„æˆã€‚ä»¤ç‰Œå»å™ªå™¨ä»å˜ˆæ‚çš„ä»¤ç‰Œä¸­é¢„æµ‹å‰ä¸¤ä¸ªç»„çš„å¹²å‡€å£°å­¦ä»¤ç‰Œï¼Œè¿™å¯ä»¥ä½œä¸ºLauraTTSåˆæˆé«˜è´¨é‡ä¸ªæ€§åŒ–è¯­éŸ³çš„å£°å­¦æç¤ºï¼Œæˆ–é€šè¿‡åµŒå…¥ç²¾ç‚¼å™¨å’Œç¼–è§£ç å™¨è§£ç å™¨è½¬æ¢ä¸ºå¹²å‡€çš„è¯­éŸ³æ³¢å½¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„ç¼–è§£ç å™¨å»å™ªå™¨ä¼˜äºæœ€æ–°çš„è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ–¹æ³•ï¼Œå¹¶ä¸”æ‰€æå‡ºçš„å™ªå£°é²æ£’çš„LauraTTSè¶…è¿‡äº†ä½¿ç”¨é™„åŠ SEæ¨¡å‹çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13830v2">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œç¼–è§£ç å™¨çš„è¯­éŸ³å»å™ªå™¨ï¼Œå¹¶å°†å…¶ä¸å…ˆè¿›çš„LLM-based TTSæ¨¡å‹LauraTTSç›¸ç»“åˆï¼Œå®ç°äº†å™ªå£°é²æ£’çš„é›¶æ ·æœ¬TTSã€‚è¯¥ç¼–è§£ç å™¨å»å™ªå™¨åŒ…æ‹¬éŸ³é¢‘ç¼–è§£ç å™¨ã€ä»¤ç‰Œå»å™ªå™¨å’ŒåµŒå…¥ç²¾ç‚¼å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ç¼–è§£ç å™¨å»å™ªå™¨ä¼˜äºç°æœ‰çš„è¯­éŸ³å¢å¼ºæ–¹æ³•ï¼Œè€Œæå‡ºçš„å™ªå£°é²æ£’çš„LauraTTSåˆ™è¶…è¶Šäº†ä½¿ç”¨é™„åŠ SEæ¨¡å‹çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based TTSæ–¹æ³•ä¼šä¿ç•™éŸ³é¢‘æç¤ºçš„å£°å­¦ç¯å¢ƒï¼Œå¯¼è‡´åˆæˆè¯­éŸ³è´¨é‡ä¸‹é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºç¥ç»ç½‘ç»œç¼–è§£ç å™¨çš„è¯­éŸ³å»å™ªå™¨ï¼ŒåŒ…æ‹¬éŸ³é¢‘ç¼–è§£ç å™¨ã€ä»¤ç‰Œå»å™ªå™¨å’ŒåµŒå…¥ç²¾ç‚¼å™¨ã€‚</li>
<li>ä»¤ç‰Œå»å™ªå™¨èƒ½ä»å«å™ªéŸ³é¢‘ä¸­é¢„æµ‹å‡ºæ¸…æ´çš„å£°å­¦ä»¤ç‰Œï¼Œå¯ä½œä¸ºLauraTTSçš„é«˜è´¨è¯­éŸ³åˆæˆæˆ–æ¸…æ´è¯­éŸ³æ³¢å½¢çš„è½¬æ¢ä¾æ®ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„ç¼–è§£ç å™¨å»å™ªå™¨åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„è¯­éŸ³å¢å¼ºæ–¹æ³•ã€‚</li>
<li>æ•´åˆç¼–è§£ç å™¨å»å™ªå™¨å’ŒLauraTTSæ¨¡å‹åï¼Œå®ç°äº†å™ªå£°é²æ£’çš„é›¶æ ·æœ¬TTSã€‚</li>
<li>å™ªå£°é²æ£’çš„LauraTTSæ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ä½¿ç”¨é™„åŠ è¯­éŸ³å¢å¼ºæ¨¡å‹çš„æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºæé«˜è¯­éŸ³åˆæˆè´¨é‡ï¼Œå°¤å…¶åœ¨å«å™ªéŸ³é¢‘æç¤ºçš„æƒ…å†µä¸‹å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13830">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4aa79b6eb807d311ebdea7380b9395ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba58973ea83532ce32150a761118e124.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-582a9fe21eabd309d2c2f7fd1e78776e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02c233800f084b2074fac3697b931790.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Granary-Speech-Recognition-and-Translation-Dataset-in-25-European-Languages"><a href="#Granary-Speech-Recognition-and-Translation-Dataset-in-25-European-Languages" class="headerlink" title="Granary: Speech Recognition and Translation Dataset in 25 European   Languages"></a>Granary: Speech Recognition and Translation Dataset in 25 European   Languages</h2><p><strong>Authors:Nithin Rao Koluguri, Monica Sekoyan, George Zelenfroynd, Sasha Meister, Shuoyang Ding, Sofia Kostandian, He Huang, Nikolay Karpov, Jagadeesh Balam, Vitaly Lavrukhin, Yifan Peng, Sara Papi, Marco Gaido, Alessio Brutti, Boris Ginsburg</strong></p>
<p>Multi-task and multilingual approaches benefit large models, yet speech processing for low-resource languages remains underexplored due to data scarcity. To address this, we present Granary, a large-scale collection of speech datasets for recognition and translation across 25 European languages. This is the first open-source effort at this scale for both transcription and translation. We enhance data quality using a pseudo-labeling pipeline with segmentation, two-pass inference, hallucination filtering, and punctuation restoration. We further generate translation pairs from pseudo-labeled transcriptions using EuroLLM, followed by a data filtration pipeline. Designed for efficiency, our pipeline processes vast amount of data within hours. We assess models trained on processed data by comparing their performance on previously curated datasets for both high- and low-resource languages. Our findings show that these models achieve similar performance using approx. 50% less data. Dataset will be made available at <a target="_blank" rel="noopener" href="https://hf.co/datasets/nvidia/Granary">https://hf.co/datasets/nvidia/Granary</a> </p>
<blockquote>
<p>å¤šä»»åŠ¡å’Œå¤šè¯­è¨€æ–¹æ³•å¯¹äºå¤§å‹æ¨¡å‹æœ‰ç›Šï¼Œä½†ç”±äºæ•°æ®ç¨€ç¼ºï¼Œé’ˆå¯¹ä½èµ„æºè¯­è¨€çš„è¯­éŸ³å¤„ç†ä»ç„¶è¢«å¿½è§†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Granaryï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–25ç§æ¬§æ´²è¯­è¨€çš„è¯­éŸ³æ•°æ®é›†çš„å¤§è§„æ¨¡é›†åˆï¼Œç”¨äºè¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘ã€‚è¿™æ˜¯è½¬å½•å’Œç¿»è¯‘é¢†åŸŸè¿™ä¸€è§„æ¨¡çš„é¦–æ¬¡å¼€æºåŠªåŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨åŒ…å«åˆ†å‰²ã€äºŒæ¬¡æ¨æ–­ã€è™šæ„è¿‡æ»¤å’Œæ ‡ç‚¹æ¢å¤çš„ä¼ªæ ‡ç­¾ç®¡é“æ¥æé«˜æ•°æ®è´¨é‡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä½¿ç”¨EuroLLMå¯¹ä¼ªæ ‡ç­¾è½¬å½•ç”Ÿæˆç¿»è¯‘é…å¯¹ï¼Œéšåè¿›è¡Œæ•°æ®è¿‡æ»¤ç®¡é“å¤„ç†ã€‚æˆ‘ä»¬çš„ç®¡é“è®¾è®¡é«˜æ•ˆï¼Œèƒ½åœ¨æ•°å°æ—¶å†…å¤„ç†å¤§é‡æ•°æ®ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ä¸ºé«˜èµ„æºå’Œä½èµ„æºè¯­è¨€å…ˆå‰æ•´ç†çš„æ•°æ®é›†ä¸Šè¯„ä¼°ç»è¿‡å¤„ç†çš„æ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼Œå¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè¿™äº›æ¨¡å‹ä½¿ç”¨çº¦å°‘ä¸€åŠçš„æ•°æ®ä¾¿å®ç°äº†ç›¸ä¼¼æ€§èƒ½ã€‚æ•°æ®é›†å°†å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://hf.co/datasets/nvidia/Granary%E3%80%82">https://hf.co/datasets/nvidia/Granaryã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13404v2">PDF</a> Accepted at Interspeech 2025 v2: Added links</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘çš„å¼€æºå¤§è§„æ¨¡è¯­éŸ³æ•°æ®é›†Granaryï¼Œè¦†ç›–25ç§æ¬§æ´²è¯­è¨€ã€‚é€šè¿‡ä½¿ç”¨ä¼ªæ ‡ç­¾ç®¡é“å¢å¼ºæ•°æ®è´¨é‡ï¼Œå¹¶ä½¿ç”¨EuroLLMç”Ÿæˆç¿»è¯‘é…å¯¹ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨å¤„ç†åçš„æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨é«˜ä½èµ„æºè¯­è¨€ä¸Šéƒ½èƒ½å®ç°ç›¸ä¼¼çš„æ€§èƒ½ï¼Œè€Œä¸”åªéœ€è¦ä½¿ç”¨çº¦50%çš„æ•°æ®ã€‚æ•°æ®é›†å¯ç”¨äºè®­ç»ƒæ›´é«˜æ•ˆçš„è¯­è¨€æ¨¡å‹ï¼Œåœ°å€å¤šè¯­ç§åœºæ™¯ä¸‹èµ„æºåŒ®ä¹çš„é—®é¢˜ã€‚æ•°æ®å¯ç”¨åœ¨è¯¥ç½‘å€ï¼š<a target="_blank" rel="noopener" href="https://hf.co/datasets/nvidia/Granary">https://hf.co/datasets/nvidia/Granary</a>ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§åä¸ºGranaryçš„å¤§è§„æ¨¡è¯­éŸ³æ•°æ®é›†ï¼Œæ”¯æŒè·¨25ç§æ¬§æ´²è¯­è¨€çš„è¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘ã€‚</li>
<li>æ•°æ®é›†é€šè¿‡ä¼ªæ ‡ç­¾ç®¡é“å¢å¼ºæ•°æ®è´¨é‡ï¼ŒåŒ…æ‹¬åˆ†å‰²ã€äºŒæ¬¡æ¨æ–­ã€å¹»è§‰è¿‡æ»¤å’Œæ ‡ç‚¹æ¢å¤ç­‰æ­¥éª¤ã€‚</li>
<li>åˆ©ç”¨EuroLLMç”Ÿæˆä¼ªæ ‡ç­¾è½¬å½•çš„ç¿»è¯‘é…å¯¹ã€‚</li>
<li>é€šè¿‡æ•ˆç‡é«˜çš„ç®¡é“å¤„ç†å¤§é‡æ•°æ®ï¼ŒçŸ­æ—¶é—´å†…å®Œæˆã€‚</li>
<li>åœ¨é«˜ã€ä½èµ„æºè¯­è¨€æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä½¿ç”¨å¤„ç†åçš„æ•°æ®è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½è‰¯å¥½ï¼Œä»…éœ€è¦çº¦50%çš„æ•°æ®å³å¯è¾¾åˆ°ç›¸ä¼¼æ€§èƒ½ã€‚ </li>
<li>æ•°æ®é›†è§£å†³äº†å¤šè¯­è¨€åœºæ™¯ä¸‹èµ„æºåŒ®ä¹çš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13404">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8d03e392dd69e6b595da6c1e1fbc41d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1351ee0dd08e8d9946525486538385c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a31acfa0ff8b0f5423755fcea7040ad8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f55deb30f8415f6c3755638f6ccd8ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4101ea0665a3dd64b2bf784bb87a5bb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-744a25304eeda920133272d6511fc538.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MoHAVE-Mixture-of-Hierarchical-Audio-Visual-Experts-for-Robust-Speech-Recognition"><a href="#MoHAVE-Mixture-of-Hierarchical-Audio-Visual-Experts-for-Robust-Speech-Recognition" class="headerlink" title="MoHAVE: Mixture of Hierarchical Audio-Visual Experts for Robust Speech   Recognition"></a>MoHAVE: Mixture of Hierarchical Audio-Visual Experts for Robust Speech   Recognition</h2><p><strong>Authors:Sungnyun Kim, Kangwook Jang, Sangmin Bae, Sungwoo Cho, Se-Young Yun</strong></p>
<p>Audio-visual speech recognition (AVSR) has become critical for enhancing speech recognition in noisy environments by integrating both auditory and visual modalities. However, existing AVSR systems struggle to scale up without compromising computational efficiency. In this study, we introduce MoHAVE (Mixture of Hierarchical Audio-Visual Experts), a novel robust AVSR framework designed to address these scalability constraints. By leveraging a Mixture-of-Experts (MoE) architecture, MoHAVE activates modality-specific expert groups, ensuring dynamic adaptation to various audio-visual inputs with minimal computational overhead. Key contributions of MoHAVE include: (1) a sparse MoE framework that efficiently scales AVSR model capacity, (2) a hierarchical gating mechanism that dynamically utilizes the expert groups based on input context, enhancing adaptability and robustness, and (3) remarkable performance across robust AVSR benchmarks, including LRS3 and MuAViC transcription and translation tasks, setting a new standard for scalable speech recognition systems. </p>
<blockquote>
<p>è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰é€šè¿‡ç»“åˆå¬è§‰å’Œè§†è§‰æ¨¡å¼ï¼Œåœ¨å¢å¼ºå™ªå£°ç¯å¢ƒä¸­çš„è¯­éŸ³è¯†åˆ«æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„AVSRç³»ç»Ÿåœ¨æ‰©å¤§è§„æ¨¡æ—¶å¾€å¾€éš¾ä»¥ä¿è¯è®¡ç®—æ•ˆç‡ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†MoHAVEï¼ˆåˆ†å±‚è§†å¬ä¸“å®¶æ··åˆç‰©ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç¨³å¥çš„AVSRæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™äº›å¯æ‰©å±•æ€§çº¦æŸã€‚é€šè¿‡åˆ©ç”¨ä¸“å®¶æ··åˆç‰©ï¼ˆMoEï¼‰æ¶æ„ï¼ŒMoHAVEæ¿€æ´»äº†ç‰¹å®šçš„ä¸“å®¶å°ç»„ï¼Œç¡®ä¿ä»¥æœ€å°çš„è®¡ç®—å¼€é”€åŠ¨æ€é€‚åº”å„ç§è§†å¬è¾“å…¥ã€‚MoHAVEçš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªç¨€ç–çš„MoEæ¡†æ¶ï¼Œæœ‰æ•ˆåœ°æ‰©å±•äº†AVSRæ¨¡å‹çš„å®¹é‡ï¼›ï¼ˆ2ï¼‰ä¸€ç§åŸºäºè¾“å…¥ä¸Šä¸‹æ–‡çš„åˆ†å±‚é—¨æ§æœºåˆ¶ï¼ŒåŠ¨æ€åˆ©ç”¨ä¸“å®¶å°ç»„ï¼Œæé«˜é€‚åº”æ€§å’Œç¨³å¥æ€§ï¼›ï¼ˆ3ï¼‰åœ¨åŒ…æ‹¬LRS3å’ŒMuAViCè½¬å½•å’Œç¿»è¯‘ä»»åŠ¡åœ¨å†…çš„ç¨³å¥AVSRåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä¸ºå¯æ‰©å±•çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿè®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10447v2">PDF</a> Accepted to ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MoHAVEï¼ˆåŸºäºå±‚æ¬¡éŸ³é¢‘è§†è§‰ä¸“å®¶çš„æ··åˆæ¨¡å‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ç¨³å¥çš„è§†å¬è¯­éŸ³è¯†åˆ«æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¯æ‰©å±•æ€§æ–¹é¢çš„é™åˆ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ï¼Œæ¿€æ´»æ¨¡æ€ç‰¹å®šçš„ä¸“å®¶ç»„ï¼Œç¡®ä¿å¯¹å„ç§è§†å¬è¾“å…¥çš„åŠ¨æ€é€‚åº”å¹¶å…·æœ‰è¾ƒä½çš„è®¡ç®—å¼€é”€ã€‚å…¶æ ¸å¿ƒè´¡çŒ®åŒ…æ‹¬æ„å»ºé«˜æ•ˆçš„ç¨€ç–MoEæ¡†æ¶ï¼Œé‡‡ç”¨åŸºäºè¾“å…¥ä¸Šä¸‹æ–‡çš„åˆ†å±‚é—¨æ§æœºåˆ¶åŠ¨æ€åˆ©ç”¨ä¸“å®¶ç»„ï¼Œä»¥åŠåœ¨å¤šä¸ªç¨³å¥çš„è§†å¬è¯­éŸ³è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šã€‚è¯¥æ¨¡å‹åœ¨è¯­éŸ³è¯†åˆ«é¢†åŸŸè®¾å®šäº†æ–°çš„å¯æ‰©å±•æ€§æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MoHAVEæ˜¯ä¸€ä¸ªæ–°é¢–çš„è§†å¬è¯­éŸ³è¯†åˆ«æ¡†æ¶ï¼Œç»“åˆäº†éŸ³é¢‘å’Œè§†è§‰æ¨¡æ€ä»¥å¢å¼ºåœ¨å˜ˆæ‚ç¯å¢ƒä¸­çš„è¯­éŸ³è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>MoHAVEåˆ©ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„æ¥è§£å†³ç°æœ‰AVSRç³»ç»Ÿçš„å¯æ‰©å±•æ€§é—®é¢˜ã€‚</li>
<li>MoHAVEé€šè¿‡æ¿€æ´»æ¨¡æ€ç‰¹å®šçš„ä¸“å®¶ç»„ï¼Œç¡®ä¿å¯¹ä¸åŒçš„è§†å¬è¾“å…¥è¿›è¡ŒåŠ¨æ€é€‚åº”ï¼ŒåŒæ—¶å…·æœ‰è¾ƒä½çš„è®¡ç®—å¼€é”€ã€‚</li>
<li>MoHAVEåŒ…å«ä¸€ä¸ªç¨€ç–MoEæ¡†æ¶ï¼Œå¯ä»¥é«˜æ•ˆåœ°æ‰©å±•AVSRæ¨¡å‹çš„å®¹é‡ã€‚</li>
<li>MoHAVEé‡‡ç”¨åˆ†å±‚é—¨æ§æœºåˆ¶ï¼Œæ ¹æ®è¾“å…¥ä¸Šä¸‹æ–‡åŠ¨æ€åˆ©ç”¨ä¸“å®¶ç»„ï¼Œå¢å¼ºäº†é€‚åº”æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>MoHAVEåœ¨å¤šä¸ªè§†å¬è¯­éŸ³è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°çªå‡ºï¼ŒåŒ…æ‹¬LRS3å’ŒMuAViCè½¬å½•å’Œç¿»è¯‘ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10447">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aac8f2fe1758b00221a40f5404c432b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8b5c153f57ee0479e81362daa99a64a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1c2b3407f424af69440ae2d7549537d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e370eb8d55af7a7a49355570adc14158.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Enhancing-Intelligibility-for-Generative-Target-Speech-Extraction-via-Joint-Optimization-with-Target-Speaker-ASR"><a href="#Enhancing-Intelligibility-for-Generative-Target-Speech-Extraction-via-Joint-Optimization-with-Target-Speaker-ASR" class="headerlink" title="Enhancing Intelligibility for Generative Target Speech Extraction via   Joint Optimization with Target Speaker ASR"></a>Enhancing Intelligibility for Generative Target Speech Extraction via   Joint Optimization with Target Speaker ASR</h2><p><strong>Authors:Hao Ma, Rujin Chen, Xiao-Lei Zhang, Ju Liu, Xuelong Li</strong></p>
<p>Target speech extraction (TSE) isolates the speech of a specific speaker from a multi-talker overlapped speech mixture. Most existing TSE models rely on discriminative methods, typically predicting a time-frequency spectrogram mask for the target speech. However, imperfections in these masks often result in over-&#x2F;under-suppression of target&#x2F;non-target speech, degrading perceptual quality. Generative methods, by contrast, re-synthesize target speech based on the mixture and target speaker cues, achieving superior perceptual quality. Nevertheless, these methods often overlook speech intelligibility, leading to alterations or loss of semantic content in the re-synthesized speech. Inspired by the Whisper modelâ€™s success in target speaker ASR, we propose a generative TSE framework based on the pre-trained Whisper model to address the above issues. This framework integrates semantic modeling with flow-based acoustic modeling to achieve both high intelligibility and perceptual quality. Results from multiple benchmarks demonstrate that the proposed method outperforms existing generative and discriminative baselines. We present speech samples on <a target="_blank" rel="noopener" href="https://aisaka0v0.github.io/GenerativeTSE_demo/">https://aisaka0v0.github.io/GenerativeTSE_demo/</a>. </p>
<blockquote>
<p>ç›®æ ‡è¯­éŸ³æå–ï¼ˆTSEï¼‰ä»å¤šè¯´è¯äººé‡å çš„è¯­éŸ³æ··åˆä¸­åˆ†ç¦»å‡ºç‰¹å®šè¯´è¯äººçš„è¯­éŸ³ã€‚ç°æœ‰çš„å¤§å¤šæ•°TSEæ¨¡å‹ä¾èµ–äºåˆ¤åˆ«æ–¹æ³•ï¼Œé€šå¸¸é¢„æµ‹ç›®æ ‡è¯­éŸ³çš„æ—¶é—´-é¢‘ç‡è°±å›¾æ©è†œã€‚ç„¶è€Œï¼Œè¿™äº›æ©è†œçš„ä¸å®Œç¾å¾€å¾€å¯¼è‡´ç›®æ ‡è¯­éŸ³æˆ–éç›®æ ‡è¯­éŸ³çš„è¿‡æŠ‘åˆ¶&#x2F;æ¬ æŠ‘åˆ¶ï¼Œé™ä½æ„ŸçŸ¥è´¨é‡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç”Ÿæˆæ–¹æ³•åŸºäºæ··åˆå’Œç›®æ ‡è¯´è¯äººçº¿ç´¢é‡æ–°åˆæˆç›®æ ‡è¯­éŸ³ï¼Œå®ç°ä¼˜è¶Šçš„æ„ŸçŸ¥è´¨é‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€å¿½è§†äº†è¯­éŸ³çš„å¯æ‡‚åº¦ï¼Œå¯¼è‡´é‡æ–°åˆæˆçš„è¯­éŸ³ä¸­çš„è¯­ä¹‰å†…å®¹å‘ç”Ÿæ”¹å˜æˆ–ä¸¢å¤±ã€‚å—Whisperæ¨¡å‹åœ¨ç›®æ ‡è¯´è¯äººè¯­éŸ³è¯†åˆ«ä¸­çš„æˆåŠŸå¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºé¢„è®­ç»ƒWhisperæ¨¡å‹çš„ç”ŸæˆTSEæ¡†æ¶ï¼Œä»¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†è¯­ä¹‰å»ºæ¨¡ä¸åŸºäºæµçš„å£°å­¦å»ºæ¨¡ç›¸ç»“åˆï¼Œå®ç°é«˜å¯æ‡‚åº¦å’Œæ„ŸçŸ¥è´¨é‡ã€‚æ¥è‡ªå¤šä¸ªåŸºå‡†æµ‹è¯•çš„ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºç°æœ‰çš„ç”Ÿæˆå‹å’Œåˆ¤åˆ«å‹åŸºå‡†æ–¹æ³•ã€‚è¯­éŸ³æ ·æœ¬è¯·è®¿é—®ï¼š[<a target="_blank" rel="noopener" href="https://aisaka0v0.github.io/GenerativeTSE_demo/%E3%80%82]">https://aisaka0v0.github.io/GenerativeTSE_demo/ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.14477v2">PDF</a> Submitted to IEEE Signal Processing Letters</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºé¢„è®­ç»ƒWhisperæ¨¡å‹çš„ç”Ÿæˆå¼ç›®æ ‡è¯­éŸ³æå–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†è¯­ä¹‰å»ºæ¨¡å’ŒåŸºäºæµçš„å£°å­¦å»ºæ¨¡ï¼Œæ—¨åœ¨æé«˜ç›®æ ‡è¯­éŸ³æå–çš„æ™ºå¬æ€§å’Œæ„ŸçŸ¥è´¨é‡ã€‚è¯¥æ–¹æ³•åœ¨å¤šåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰çš„ç”Ÿæˆå¼å’Œåˆ¤åˆ«å¼åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›®æ ‡è¯­éŸ³æå–ï¼ˆTSEï¼‰æ˜¯ä»å¤šè¯´è¯äººé‡å è¯­éŸ³ä¸­åˆ†ç¦»ç‰¹å®šè¯´è¯äººçš„è¯­éŸ³ã€‚</li>
<li>ç°æœ‰TSEæ¨¡å‹å¤§å¤šä¾èµ–äºåˆ¤åˆ«å¼æ–¹æ³•ï¼Œé€šè¿‡é¢„æµ‹æ—¶é—´-é¢‘ç‡è°±å›¾æ©è†œæ¥æå–ç›®æ ‡è¯­éŸ³ã€‚</li>
<li>åˆ¤åˆ«å¼æ–¹æ³•å­˜åœ¨çš„ç¼ºé™·æ˜¯æ©è†œä¸å®Œç¾ï¼Œå¯èƒ½å¯¼è‡´ç›®æ ‡è¯­éŸ³æˆ–éç›®æ ‡è¯­éŸ³çš„è¿‡åº¦&#x2F;ä¸è¶³æŠ‘åˆ¶ï¼Œå½±å“æ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>ç”Ÿæˆå¼æ–¹æ³•é€šè¿‡æ··åˆè¯­éŸ³å’Œç›®æ ‡è¯´è¯äººçº¿ç´¢é‡æ–°åˆæˆç›®æ ‡è¯­éŸ³ï¼Œå¯ä»¥è¾¾åˆ°è¾ƒé«˜çš„æ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>ç„¶è€Œï¼Œç”Ÿæˆå¼æ–¹æ³•å¸¸å¸¸å¿½è§†è¯­éŸ³çš„æ¸…æ™°åº¦ï¼Œå¯¼è‡´é‡æ–°åˆæˆçš„è¯­éŸ³ä¸­è¯­ä¹‰å†…å®¹çš„æ”¹å˜æˆ–ä¸¢å¤±ã€‚</li>
<li>æœ¬æ–‡å—Whisperæ¨¡å‹åœ¨ç›®æ ‡è¯´è¯äººè¯­éŸ³è¯†åˆ«ä¸­çš„æˆåŠŸå¯å‘ï¼Œæå‡ºä¸€ç§åŸºäºé¢„è®­ç»ƒWhisperæ¨¡å‹çš„ç”Ÿæˆå¼TSEæ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.14477">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-037497f069e0dc738365e2c4c336663d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5ccc52813c3cff0c01d6c583b8c2208.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e3d7fb31204902805bdd12eeed15326.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="From-KAN-to-GR-KAN-Advancing-Speech-Enhancement-with-KAN-Based-Methodology"><a href="#From-KAN-to-GR-KAN-Advancing-Speech-Enhancement-with-KAN-Based-Methodology" class="headerlink" title="From KAN to GR-KAN: Advancing Speech Enhancement with KAN-Based   Methodology"></a>From KAN to GR-KAN: Advancing Speech Enhancement with KAN-Based   Methodology</h2><p><strong>Authors:Haoyang Li, Yuchen Hu, Chen Chen, Sabato Marco Siniscalchi, Songting Liu, Eng Siong Chng</strong></p>
<p>Deep neural network (DNN)-based speech enhancement (SE) usually uses conventional activation functions, which lack the expressiveness to capture complex multiscale structures needed for high-fidelity SE. Group-Rational KAN (GR-KAN), a variant of Kolmogorov-Arnold Networks (KAN), retains KANâ€™s expressiveness while improving scalability on complex tasks. We adapt GR-KAN to existing DNN-based SE by replacing dense layers with GR-KAN layers in the time-frequency (T-F) domain MP-SENet and adapting GR-KANâ€™s activations into the 1D CNN layers in the time-domain Demucs. Results on Voicebank-DEMAND show that GR-KAN requires up to 4x fewer parameters while improving PESQ by up to 0.1. In contrast, KAN, facing scalability issues, outperforms MLP on a small-scale signal modeling task but fails to improve MP-SENet. We demonstrate the first successful use of KAN-based methods for consistent improvement in both time- and SoTA TF-domain SE, establishing GR-KAN as a promising alternative for SE. </p>
<blockquote>
<p>åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰çš„è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰é€šå¸¸ä½¿ç”¨ä¼ ç»Ÿçš„æ¿€æ´»å‡½æ•°ï¼Œè¿™äº›å‡½æ•°ç¼ºä¹è¡¨è¾¾å¤æ‚å¤šå°ºåº¦ç»“æ„æ‰€éœ€çš„è¡¨è¾¾èƒ½åŠ›ï¼Œæ— æ³•ç”¨äºé«˜ä¿çœŸSEã€‚Group-Rational KANï¼ˆGR-KANï¼‰æ˜¯Kolmogorov-Arnold Networksï¼ˆKANï¼‰çš„ä¸€ç§å˜ä½“ï¼Œåœ¨å¤æ‚ä»»åŠ¡ä¸Šä¿ç•™äº†KANçš„è¡¨è¾¾èƒ½åŠ›å¹¶æé«˜äº†å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬é€šè¿‡å°†æ—¶é—´-é¢‘ç‡ï¼ˆT-Fï¼‰åŸŸMP-SENetä¸­çš„å¯†é›†å±‚æ›¿æ¢ä¸ºGR-KANå±‚ï¼Œå¹¶å°†GR-KANçš„æ¿€æ´»é€‚åº”åˆ°æ—¶é—´åŸŸDemucsçš„1D CNNå±‚ï¼Œå°†GR-KANé€‚åº”åˆ°ç°æœ‰çš„åŸºäºDNNçš„SEã€‚åœ¨Voicebank-DEMANDä¸Šçš„ç»“æœè¡¨æ˜ï¼ŒGR-KANåœ¨æ”¹è¿›PESQé«˜è¾¾0.1çš„åŒæ—¶ï¼Œå‚æ•°å‡å°‘äº†é«˜è¾¾4å€ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒKANé¢ä¸´å¯æ‰©å±•æ€§é—®é¢˜ï¼Œåœ¨å°è§„æ¨¡ä¿¡å·å»ºæ¨¡ä»»åŠ¡ä¸Šä¼˜äºMLPï¼Œä½†æœªèƒ½æ”¹è¿›MP-SENetã€‚æˆ‘ä»¬å±•ç¤ºäº†åŸºäºKANçš„æ–¹æ³•åœ¨æ—¶é—´å’Œå½“å‰TFåŸŸSEä¸­çš„æŒç»­æ”¹è¿›çš„é¦–ä¾‹æˆåŠŸåº”ç”¨ï¼Œè¯æ˜äº†GR-KANåœ¨SEä¸­çš„æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17778v2">PDF</a> Accepted to Interspeech2025</p>
<p><strong>Summary</strong><br>     åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰çš„è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰é€šå¸¸ä½¿ç”¨ä¼ ç»Ÿçš„æ¿€æ´»å‡½æ•°ï¼Œéš¾ä»¥æ•æ‰å¤æ‚çš„å¤šå°ºåº¦ç»“æ„ä»¥å®ç°é«˜ä¿çœŸSEã€‚ Group-Rational KANï¼ˆGR-KANï¼‰ä½œä¸ºKolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰çš„å˜ä½“ï¼Œåœ¨ä¿ç•™KANè¡¨è¾¾åŠ›çš„åŒæ—¶ï¼Œæé«˜äº†åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬é€šè¿‡å°†ç°æœ‰DNN-based SEä¸­çš„å¯†é›†å±‚æ›¿æ¢ä¸ºGR-KANå±‚ï¼Œå¹¶é€‚åº”GR-KANæ¿€æ´»åˆ°æ—¶é—´åŸŸDemucsçš„1D CNNå±‚å’Œæ—¶é—´-é¢‘ç‡ï¼ˆT-Fï¼‰åŸŸMP-SENetï¼Œå®ç°äº†GR-KANåœ¨SEä¸­çš„åº”ç”¨ã€‚åœ¨Voicebank-DEMANDä¸Šçš„ç»“æœæ˜¾ç¤ºï¼ŒGR-KANçš„å‚æ•°éœ€æ±‚å°‘è¾¾4å€ï¼ŒåŒæ—¶PESQæé«˜è¾¾0.1ã€‚å°½ç®¡KANåœ¨å°è§„æ¨¡ä¿¡å·å»ºæ¨¡ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºMLPï¼Œä½†åœ¨æ”¹å–„MP-SENetæ–¹é¢å¹¶æœªæˆåŠŸã€‚æˆ‘ä»¬é¦–æ¬¡æˆåŠŸä½¿ç”¨åŸºäºKANçš„æ–¹æ³•ï¼Œåœ¨æ—¶é—´å’Œå½“å‰TFåŸŸSEä¸­éƒ½å®ç°äº†æ”¹è¿›ï¼Œç¡®ç«‹äº†GR-KANåœ¨SEä¸­çš„æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆåœ°ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰ä¸­ä½¿ç”¨çš„ä¼ ç»Ÿæ¿€æ´»å‡½æ•°éš¾ä»¥æ•æ‰å¤æ‚çš„å¤šå°ºåº¦ç»“æ„ã€‚</li>
<li>Group-Rational KANï¼ˆGR-KANï¼‰ä¿ç•™äº†Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰çš„è¡¨è¾¾èƒ½åŠ›å¹¶æé«˜äº†å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„å¯æ‰©å±•æ€§ã€‚</li>
<li>é€šè¿‡å°†GR-KANå±‚æ›¿æ¢å¯†é›†å±‚å¹¶é€‚åº”GR-KANæ¿€æ´»åˆ°ç°æœ‰DNN-based SEçš„ä¸åŒæ¶æ„ä¸­ï¼Œå®ç°äº†GR-KANåœ¨SEä¸­çš„é¦–æ¬¡åº”ç”¨ã€‚</li>
<li>åœ¨Voicebank-DEMANDä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGR-KANçš„å‚æ•°éœ€æ±‚æ›´å°‘ä¸”æ€§èƒ½æœ‰æ‰€æå‡ã€‚</li>
<li>è™½ç„¶KANåœ¨å°è§„æ¨¡ä¿¡å·å»ºæ¨¡ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ”¹å–„MP-SENetæ–¹é¢å¹¶æœªè¾¾åˆ°é¢„æœŸæ•ˆæœã€‚</li>
<li>åŸºäºKANçš„æ–¹æ³•åœ¨æ—¶é—´å’Œå½“å‰TFåŸŸSEä¸­éƒ½å®ç°äº†æ”¹è¿›ï¼Œä¸ºSEé¢†åŸŸæä¾›äº†æ–°çš„è§†è§’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17778">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-564b3a08f757e21f42c482b074c7171d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c137bb0c6ba2570a2835c6792ac445a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8677f5f3a70a079045c236b6f10dcac6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee360f6c9d850a0bf680726c7086f08e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1346ff85c2384b8ad1d10ee705e6196.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-541b50ca816e74a26e50c4612546d5e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-baf1e95a2ab2ace3f2b9ea5420ad0007.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-25/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-25/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-25/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-10c20e5ebcfcbbe0109d82b703a8120c.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-25  Evaluation and optimization of deep learning models for enhanced   detection of brain cancer using transmission optical microscopy of thin brain   tissue samples
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-25/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-70c7f4cd00d993f756664c1de8e0ada4.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-25  Boosting Few-Shot Open-Set Object Detection via Prompt Learning and   Robust Decision Boundary
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27663.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
