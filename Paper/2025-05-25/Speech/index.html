<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-05-25  In-Context Learning Boosts Speech Recognition via Human-like Adaptation   to Speakers and Language Varieties">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-5aa9a4d98f38fb6ecbd409bf9c96945f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    29 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-25-更新"><a href="#2025-05-25-更新" class="headerlink" title="2025-05-25 更新"></a>2025-05-25 更新</h1><h2 id="In-Context-Learning-Boosts-Speech-Recognition-via-Human-like-Adaptation-to-Speakers-and-Language-Varieties"><a href="#In-Context-Learning-Boosts-Speech-Recognition-via-Human-like-Adaptation-to-Speakers-and-Language-Varieties" class="headerlink" title="In-Context Learning Boosts Speech Recognition via Human-like Adaptation   to Speakers and Language Varieties"></a>In-Context Learning Boosts Speech Recognition via Human-like Adaptation   to Speakers and Language Varieties</h2><p><strong>Authors:Nathan Roll, Calbert Graham, Yuka Tatsumi, Kim Tien Nguyen, Meghan Sumner, Dan Jurafsky</strong></p>
<p>Human listeners readily adjust to unfamiliar speakers and language varieties through exposure, but do these adaptation benefits extend to state-of-the-art spoken language models? We introduce a scalable framework that allows for in-context learning (ICL) in Phi-4 Multimodal using interleaved task prompts and audio-text pairs, and find that as few as 12 example utterances (~50 seconds) at inference time reduce word error rates by a relative 19.7% (1.2 pp.) on average across diverse English corpora. These improvements are most pronounced in low-resource varieties, when the context and target speaker match, and when more examples are provided–though scaling our procedure yields diminishing marginal returns to context length. Overall, we find that our novel ICL adaptation scheme (1) reveals a similar performance profile to human listeners, and (2) demonstrates consistent improvements to automatic speech recognition (ASR) robustness across diverse speakers and language backgrounds. While adaptation succeeds broadly, significant gaps remain for certain varieties, revealing where current models still fall short of human flexibility. We release our prompts and code on GitHub. </p>
<blockquote>
<p>人类听众通过接触很容易适应不熟悉的说话者和语言变体，但这些适应好处是否延伸到最先进的口语模型呢？我们引入了一个可扩展的框架，允许在Phi-4多模式中使用上下文学习（ICL），通过交替的任务提示和音频文本对进行学习，我们发现，在推理时间只需12个示例话语（约50秒），就可以在多种英语语料库中平均降低相对字错误率19.7%（1.2个百分点）。这些改进在低资源变体中最为明显，当上下文和目标说话者匹配时，以及提供更多示例时——尽管扩大我们的程序会给上下文长度带来边际递减的收益。总的来说，我们发现我们新颖的ICL适应方案（1）展现了与人类听众相似的性能特征，（2）在多种说话者和语言背景下，对自动语音识别（ASR）稳健性表现出了持续的改进。虽然适应大体上是成功的，但对于某些变体仍存在较大差距，这揭示了当前模型在灵活性方面仍然不及人类。我们在GitHub上发布了我们的提示和代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14887v1">PDF</a> 15 pages; 3 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个可扩展的框架，该框架允许在Phi-4多模式中使用上下文学习（ICL）来适应不熟悉的语言模型。通过间隔的任务提示和音频文本对，发现只需在推理时间提供少量的示例话语（约50秒），平均降低相对词错误率为降低词错误率的百分比比未改进的基线降低了19.7%（减少百分之一），减少了在多样化的英语语料库上的误差率。改善最大的场合是在资源和目标演讲者相匹配时提供更多示例的场合，而增加上下文长度则产生边际收益递减效应。总体而言，本文提出的ICL适应方案（1）表现出与人类听众相似的性能特征，（2）证明了在多样化和背景各异的演讲者中自动语音识别（ASR）稳健性的持续改进。尽管适应是成功的，但对于某些语言变体仍然存在显著的差距，揭示了当前模型仍然无法与人类灵活性完全匹配的地方。我们已在GitHub上发布了提示和代码。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了一个可扩展的框架，允许使用上下文学习（ICL）来增强语言模型的适应性。</li>
<li>通过间隔的任务提示和音频文本对，少量示例话语即可改善语言模型的性能。</li>
<li>平均降低词错误率显著，尤其是在资源和目标演讲者相匹配的情况下提供更多示例时改善更明显。但上下文长度的边际效益有所递减。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14887">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-87ef2ef2d4772091c289e3d0795a3bc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d720ef224c58fc3de55cc6c07c53eb69.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-49c06159b3b279027b2f3ae2196ebf2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-426db9cc572c13a427bcde7c4d5ec2f0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Towards-Inclusive-ASR-Investigating-Voice-Conversion-for-Dysarthric-Speech-Recognition-in-Low-Resource-Languages"><a href="#Towards-Inclusive-ASR-Investigating-Voice-Conversion-for-Dysarthric-Speech-Recognition-in-Low-Resource-Languages" class="headerlink" title="Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric   Speech Recognition in Low-Resource Languages"></a>Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric   Speech Recognition in Low-Resource Languages</h2><p><strong>Authors:Chin-Jou Li, Eunjung Yeo, Kwanghee Choi, Paula Andrea Pérez-Toro, Masao Someki, Rohan Kumar Das, Zhengjun Yue, Juan Rafael Orozco-Arroyave, Elmar Nöth, David R. Mortensen</strong></p>
<p>Automatic speech recognition (ASR) for dysarthric speech remains challenging due to data scarcity, particularly in non-English languages. To address this, we fine-tune a voice conversion model on English dysarthric speech (UASpeech) to encode both speaker characteristics and prosodic distortions, then apply it to convert healthy non-English speech (FLEURS) into non-English dysarthric-like speech. The generated data is then used to fine-tune a multilingual ASR model, Massively Multilingual Speech (MMS), for improved dysarthric speech recognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE (Tamil) demonstrates that VC with both speaker and prosody conversion significantly outperforms the off-the-shelf MMS performance and conventional augmentation techniques such as speed and tempo perturbation. Objective and subjective analyses of the generated data further confirm that the generated speech simulates dysarthric characteristics. </p>
<blockquote>
<p>针对发音困难（发音障碍）语音的自动语音识别（ASR）由于其数据稀缺性仍然具有挑战性，特别是在非英语环境中更是如此。为解决这一问题，我们在英语发音困难语音（UASpeech）上对语音转换模型进行微调，以编码说话人的特性和韵律失真，然后将其应用于将健康的非英语语音（FLEURS）转换为非英语发音障碍类语音。然后使用生成的数据对多语种自动语音识别模型Massively Multilingual Speech（MMS）进行微调，以提高对发音障碍语音的识别能力。在PC-GITA（西班牙语）、EasyCall（意大利语）和SSNCE（泰米尔语）上的评估表明，同时实现说话人和韵律转换的语音转换方法显著优于现成的MMS性能和传统的增强技术，如速度和节奏扰动。对生成数据的客观和主观分析进一步证实，生成的语音模拟了发音障碍的特征。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14874v1">PDF</a> 5 pages, 1 figure, Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了如何利用英语发音障碍语音（UASpeech）训练语音转换模型，该模型可以编码说话人的特征和韵律扭曲。然后，将此模型应用于将健康的非英语语音（FLEURS）转换为非英语的发音障碍语音。生成的语音数据用于微调多语言自动语音识别（ASR）模型Massively Multilingual Speech（MMS），以提高对发音障碍语音的识别能力。评估结果表明，同时转换说话人和韵律的语音转换（VC）显著优于现成的MMS性能和传统的增强技术，如速度和节奏扰动。对生成数据的客观和主观分析进一步证实，生成的语音模拟了发音障碍的特征。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据稀缺是非英语发音障碍语音自动语音识别（ASR）的主要挑战。</li>
<li>通过英语发音障碍语音（UASpeech）训练语音转换模型以编码说话人的特征和韵律扭曲。</li>
<li>模型应用于将健康非英语语音转换为非英语的发音障碍语音。</li>
<li>生成的语音数据用于微调多语言ASR模型Massively Multilingual Speech（MMS）。</li>
<li>语音转换（VC）在转换说话人和韵律方面显著提高了ASR性能。</li>
<li>评估结果表明，VC方法优于传统的增强技术和现成的ASR模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14874">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7405e8a441d5f6012f0f152aff55ba4f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-100f58f32e4ff2a6eb1693e7a33a951d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78056c740a7209de2a0bdbc167f264df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-569360d203bc2186acb89f7d6ccd725e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b8274cfe195b12f29332b0779ceb8c6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Mitigating-Subgroup-Disparities-in-Multi-Label-Speech-Emotion-Recognition-A-Pseudo-Labeling-and-Unsupervised-Learning-Approach"><a href="#Mitigating-Subgroup-Disparities-in-Multi-Label-Speech-Emotion-Recognition-A-Pseudo-Labeling-and-Unsupervised-Learning-Approach" class="headerlink" title="Mitigating Subgroup Disparities in Multi-Label Speech Emotion   Recognition: A Pseudo-Labeling and Unsupervised Learning Approach"></a>Mitigating Subgroup Disparities in Multi-Label Speech Emotion   Recognition: A Pseudo-Labeling and Unsupervised Learning Approach</h2><p><strong>Authors:Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee</strong></p>
<p>While subgroup disparities and performance bias are increasingly studied in computational research, fairness in categorical Speech Emotion Recognition (SER) remains underexplored. Existing methods often rely on explicit demographic labels, which are difficult to obtain due to privacy concerns. To address this limitation, we introduce an Implicit Demography Inference (IDI) module that leverages pseudo-labeling from a pre-trained model and unsupervised learning using k-means clustering to mitigate bias in SER. Our experiments show that pseudo-labeling IDI reduces subgroup disparities, improving fairness metrics by over 33% with less than a 3% decrease in SER accuracy. Also, the unsupervised IDI yields more than a 26% improvement in fairness metrics with a drop of less than 4% in SER performance. Further analyses reveal that the unsupervised IDI consistently mitigates race and age disparities, demonstrating its potential in scenarios where explicit demographic information is unavailable. </p>
<blockquote>
<p>在计算研究中，虽然对子群体差异和性能偏见的研究越来越多，但在分类语音情感识别（SER）中的公平性仍然被忽视。现有方法通常依赖于明确的人口统计标签，但由于隐私担忧，这些标签很难获得。为了解决这一局限性，我们引入了一个隐式人口统计推断（IDI）模块，该模块利用预训练模型的伪标签和通过k-means聚类进行无监督学习，以减轻SER中的偏见。我们的实验表明，伪标签IDI减少了子群体差异，通过提高超过33%的公平性指标，同时SER准确率下降不到3%。此外，无监督的IDI在公平性指标上提高了超过26%，同时SER性能下降不到4%。进一步的分析表明，无监督的IDI持续缓解了种族和年龄差异，证明了在缺乏明确人口统计信息的情况下，其潜在的应用价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14449v2">PDF</a> Accepted by InterSpeech 2025. 7 pages including 2 pages of appendix</p>
<p><strong>摘要</strong></p>
<p>该文本主要探讨了在计算研究中越来越受关注的子群差异和性能偏差问题，并指出分类语音情感识别（SER）中的公平性尚未得到充分探索。现有方法往往依赖于难以获得的明确人口统计标签。为解决这一局限性，我们引入了隐式人口统计推断（IDI）模块，该模块利用预训练模型的伪标签和K均值聚类等无监督学习技术来缓解SER中的偏见。实验表明，伪标签化的IDI能够减少子群差异，在公平度指标上提高超过百分之三十三的同时仅降低SER准确率不到百分之三。此外，无监督的IDI在公平度指标上提高了超过百分之二十六，且只降低了不到百分之四的SER性能。进一步分析表明，无监督的IDI在缓解种族和年龄差异方面表现稳定，显示出在缺乏明确人口统计信息的情况下具有潜力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>当前研究的关注点是识别和分析计算研究中日益凸显的子群差异和性能偏差问题，并指出了现有的分类语音情感识别（SER）方法在处理公平性方面的不足。</li>
<li>现有方法过于依赖难以获取的人口统计标签作为输入，因此存在局限性。为解决这一问题，引入了一种名为隐式人口统计推断（IDI）的新模块。该模块使用预训练模型的伪标签和无监督学习技术来缓解偏见问题。</li>
<li>通过实验验证了伪标签化的IDI在减少子群差异和提高公平性方面的有效性，其改进效果显著且对SER准确率的影响较小。</li>
<li>无监督的IDI在公平度指标上表现出更好的效果，并且在维持相对较高的SER性能的同时进行改进。这表明无监督学习方法在处理缺乏明确人口统计信息的情况时具有优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14449">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c19382ab92a3ef83332b2059c94327f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-902a56032e981efba948702f7456c8a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1446e47e2efdff41f6751858208bc760.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d6de9cd829211815b2e3bfe41807c93.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7faca6b6383e210de16348d3a3c75829.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5aa9a4d98f38fb6ecbd409bf9c96945f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Improving-Noise-Robustness-of-LLM-based-Zero-shot-TTS-via-Discrete-Acoustic-Token-Denoising"><a href="#Improving-Noise-Robustness-of-LLM-based-Zero-shot-TTS-via-Discrete-Acoustic-Token-Denoising" class="headerlink" title="Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete   Acoustic Token Denoising"></a>Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete   Acoustic Token Denoising</h2><p><strong>Authors:Ye-Xin Lu, Hui-Peng Du, Fei Liu, Yang Ai, Zhen-Hua Ling</strong></p>
<p>Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models. </p>
<blockquote>
<p>基于大型语言模型（LLM）的零样本文本到语音（TTS）方法倾向于保留音频提示的声学环境，但当音频提示包含噪声时，会导致合成语音质量下降。在本文中，我们提出了一种新型的基于神经网络编解码器的语音去噪器，并将其与先进的LLM-based TTS模型LauraTTS相结合，实现了噪声鲁棒的零样本TTS。所提出的编解码器去噪器由音频编解码器、令牌去噪器和嵌入精炼器组成。令牌去噪器从嘈杂的令牌中预测前两个组的干净声学令牌，这可以作为LauraTTS合成高质量个性化语音的声学提示，或通过嵌入精炼器和编解码器解码器转换为干净的语音波形。实验结果表明，我们提出的编解码器去噪器优于最新的语音增强（SE）方法，并且所提出的噪声鲁棒的LauraTTS超过了使用附加SE模型的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13830v2">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于神经网络编解码器的语音去噪器，并将其与先进的LLM-based TTS模型LauraTTS相结合，实现了噪声鲁棒的零样本TTS。该编解码器去噪器包括音频编解码器、令牌去噪器和嵌入精炼器。实验结果表明，所提出的编解码器去噪器优于现有的语音增强方法，而提出的噪声鲁棒的LauraTTS则超越了使用附加SE模型的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based TTS方法会保留音频提示的声学环境，导致合成语音质量下降。</li>
<li>提出了一种新型的基于神经网络编解码器的语音去噪器，包括音频编解码器、令牌去噪器和嵌入精炼器。</li>
<li>令牌去噪器能从含噪音频中预测出清洁的声学令牌，可作为LauraTTS的高质语音合成或清洁语音波形的转换依据。</li>
<li>实验结果显示，所提出的编解码器去噪器在性能上超越了现有的语音增强方法。</li>
<li>整合编解码器去噪器和LauraTTS模型后，实现了噪声鲁棒的零样本TTS。</li>
<li>噪声鲁棒的LauraTTS模型在性能上超越了使用附加语音增强模型的方法。</li>
<li>该方法对于提高语音合成质量，尤其在含噪音频提示的情况下具有重要意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13830">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4aa79b6eb807d311ebdea7380b9395ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba58973ea83532ce32150a761118e124.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-582a9fe21eabd309d2c2f7fd1e78776e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02c233800f084b2074fac3697b931790.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Granary-Speech-Recognition-and-Translation-Dataset-in-25-European-Languages"><a href="#Granary-Speech-Recognition-and-Translation-Dataset-in-25-European-Languages" class="headerlink" title="Granary: Speech Recognition and Translation Dataset in 25 European   Languages"></a>Granary: Speech Recognition and Translation Dataset in 25 European   Languages</h2><p><strong>Authors:Nithin Rao Koluguri, Monica Sekoyan, George Zelenfroynd, Sasha Meister, Shuoyang Ding, Sofia Kostandian, He Huang, Nikolay Karpov, Jagadeesh Balam, Vitaly Lavrukhin, Yifan Peng, Sara Papi, Marco Gaido, Alessio Brutti, Boris Ginsburg</strong></p>
<p>Multi-task and multilingual approaches benefit large models, yet speech processing for low-resource languages remains underexplored due to data scarcity. To address this, we present Granary, a large-scale collection of speech datasets for recognition and translation across 25 European languages. This is the first open-source effort at this scale for both transcription and translation. We enhance data quality using a pseudo-labeling pipeline with segmentation, two-pass inference, hallucination filtering, and punctuation restoration. We further generate translation pairs from pseudo-labeled transcriptions using EuroLLM, followed by a data filtration pipeline. Designed for efficiency, our pipeline processes vast amount of data within hours. We assess models trained on processed data by comparing their performance on previously curated datasets for both high- and low-resource languages. Our findings show that these models achieve similar performance using approx. 50% less data. Dataset will be made available at <a target="_blank" rel="noopener" href="https://hf.co/datasets/nvidia/Granary">https://hf.co/datasets/nvidia/Granary</a> </p>
<blockquote>
<p>多任务和多语言方法对于大型模型有益，但由于数据稀缺，针对低资源语言的语音处理仍然被忽视。为了解决这个问题，我们推出了Granary，这是一个涵盖25种欧洲语言的语音数据集的大规模集合，用于语音识别和翻译。这是转录和翻译领域这一规模的首次开源努力。我们使用包含分割、二次推断、虚构过滤和标点恢复的伪标签管道来提高数据质量。我们进一步使用EuroLLM对伪标签转录生成翻译配对，随后进行数据过滤管道处理。我们的管道设计高效，能在数小时内处理大量数据。我们通过在为高资源和低资源语言先前整理的数据集上评估经过处理的数据训练的模型，对模型进行评估。我们的研究发现，这些模型使用约少一半的数据便实现了相似性能。数据集将发布在：<a target="_blank" rel="noopener" href="https://hf.co/datasets/nvidia/Granary%E3%80%82">https://hf.co/datasets/nvidia/Granary。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13404v2">PDF</a> Accepted at Interspeech 2025 v2: Added links</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对语音识别和翻译的开源大规模语音数据集Granary，覆盖25种欧洲语言。通过使用伪标签管道增强数据质量，并使用EuroLLM生成翻译配对。研究发现，在处理后的数据上训练的模型在高低资源语言上都能实现相似的性能，而且只需要使用约50%的数据。数据集可用于训练更高效的语言模型，地址多语种场景下资源匮乏的问题。数据可用在该网址：<a target="_blank" rel="noopener" href="https://hf.co/datasets/nvidia/Granary">https://hf.co/datasets/nvidia/Granary</a>。 </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了一种名为Granary的大规模语音数据集，支持跨25种欧洲语言的语音识别和翻译。</li>
<li>数据集通过伪标签管道增强数据质量，包括分割、二次推断、幻觉过滤和标点恢复等步骤。</li>
<li>利用EuroLLM生成伪标签转录的翻译配对。</li>
<li>通过效率高的管道处理大量数据，短时间内完成。</li>
<li>在高、低资源语言数据集上的评估显示，使用处理后的数据训练的模型性能良好，仅需要约50%的数据即可达到相似性能。 </li>
<li>数据集解决了多语言场景下资源匮乏的问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13404">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8d03e392dd69e6b595da6c1e1fbc41d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1351ee0dd08e8d9946525486538385c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a31acfa0ff8b0f5423755fcea7040ad8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f55deb30f8415f6c3755638f6ccd8ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4101ea0665a3dd64b2bf784bb87a5bb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-744a25304eeda920133272d6511fc538.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MoHAVE-Mixture-of-Hierarchical-Audio-Visual-Experts-for-Robust-Speech-Recognition"><a href="#MoHAVE-Mixture-of-Hierarchical-Audio-Visual-Experts-for-Robust-Speech-Recognition" class="headerlink" title="MoHAVE: Mixture of Hierarchical Audio-Visual Experts for Robust Speech   Recognition"></a>MoHAVE: Mixture of Hierarchical Audio-Visual Experts for Robust Speech   Recognition</h2><p><strong>Authors:Sungnyun Kim, Kangwook Jang, Sangmin Bae, Sungwoo Cho, Se-Young Yun</strong></p>
<p>Audio-visual speech recognition (AVSR) has become critical for enhancing speech recognition in noisy environments by integrating both auditory and visual modalities. However, existing AVSR systems struggle to scale up without compromising computational efficiency. In this study, we introduce MoHAVE (Mixture of Hierarchical Audio-Visual Experts), a novel robust AVSR framework designed to address these scalability constraints. By leveraging a Mixture-of-Experts (MoE) architecture, MoHAVE activates modality-specific expert groups, ensuring dynamic adaptation to various audio-visual inputs with minimal computational overhead. Key contributions of MoHAVE include: (1) a sparse MoE framework that efficiently scales AVSR model capacity, (2) a hierarchical gating mechanism that dynamically utilizes the expert groups based on input context, enhancing adaptability and robustness, and (3) remarkable performance across robust AVSR benchmarks, including LRS3 and MuAViC transcription and translation tasks, setting a new standard for scalable speech recognition systems. </p>
<blockquote>
<p>视听语音识别（AVSR）通过结合听觉和视觉模式，在增强噪声环境中的语音识别方面发挥着至关重要的作用。然而，现有的AVSR系统在扩大规模时往往难以保证计算效率。在这项研究中，我们引入了MoHAVE（分层视听专家混合物），这是一种新型稳健的AVSR框架，旨在解决这些可扩展性约束。通过利用专家混合物（MoE）架构，MoHAVE激活了特定的专家小组，确保以最小的计算开销动态适应各种视听输入。MoHAVE的主要贡献包括：（1）一个稀疏的MoE框架，有效地扩展了AVSR模型的容量；（2）一种基于输入上下文的分层门控机制，动态利用专家小组，提高适应性和稳健性；（3）在包括LRS3和MuAViC转录和翻译任务在内的稳健AVSR基准测试上表现出卓越的性能，为可扩展的语音识别系统设定了新的标准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10447v2">PDF</a> Accepted to ICML 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了MoHAVE（基于层次音频视觉专家的混合模型），这是一种新型的稳健的视听语音识别框架，旨在解决可扩展性方面的限制。该框架利用混合专家（MoE）架构，激活模态特定的专家组，确保对各种视听输入的动态适应并具有较低的计算开销。其核心贡献包括构建高效的稀疏MoE框架，采用基于输入上下文的分层门控机制动态利用专家组，以及在多个稳健的视听语音识别基准测试中表现卓越。该模型在语音识别领域设定了新的可扩展性标准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MoHAVE是一个新颖的视听语音识别框架，结合了音频和视觉模态以增强在嘈杂环境中的语音识别能力。</li>
<li>MoHAVE利用混合专家（MoE）架构来解决现有AVSR系统的可扩展性问题。</li>
<li>MoHAVE通过激活模态特定的专家组，确保对不同的视听输入进行动态适应，同时具有较低的计算开销。</li>
<li>MoHAVE包含一个稀疏MoE框架，可以高效地扩展AVSR模型的容量。</li>
<li>MoHAVE采用分层门控机制，根据输入上下文动态利用专家组，增强了适应性和稳健性。</li>
<li>MoHAVE在多个视听语音识别基准测试中表现突出，包括LRS3和MuAViC转录和翻译任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10447">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-aac8f2fe1758b00221a40f5404c432b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8b5c153f57ee0479e81362daa99a64a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1c2b3407f424af69440ae2d7549537d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e370eb8d55af7a7a49355570adc14158.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Enhancing-Intelligibility-for-Generative-Target-Speech-Extraction-via-Joint-Optimization-with-Target-Speaker-ASR"><a href="#Enhancing-Intelligibility-for-Generative-Target-Speech-Extraction-via-Joint-Optimization-with-Target-Speaker-ASR" class="headerlink" title="Enhancing Intelligibility for Generative Target Speech Extraction via   Joint Optimization with Target Speaker ASR"></a>Enhancing Intelligibility for Generative Target Speech Extraction via   Joint Optimization with Target Speaker ASR</h2><p><strong>Authors:Hao Ma, Rujin Chen, Xiao-Lei Zhang, Ju Liu, Xuelong Li</strong></p>
<p>Target speech extraction (TSE) isolates the speech of a specific speaker from a multi-talker overlapped speech mixture. Most existing TSE models rely on discriminative methods, typically predicting a time-frequency spectrogram mask for the target speech. However, imperfections in these masks often result in over-&#x2F;under-suppression of target&#x2F;non-target speech, degrading perceptual quality. Generative methods, by contrast, re-synthesize target speech based on the mixture and target speaker cues, achieving superior perceptual quality. Nevertheless, these methods often overlook speech intelligibility, leading to alterations or loss of semantic content in the re-synthesized speech. Inspired by the Whisper model’s success in target speaker ASR, we propose a generative TSE framework based on the pre-trained Whisper model to address the above issues. This framework integrates semantic modeling with flow-based acoustic modeling to achieve both high intelligibility and perceptual quality. Results from multiple benchmarks demonstrate that the proposed method outperforms existing generative and discriminative baselines. We present speech samples on <a target="_blank" rel="noopener" href="https://aisaka0v0.github.io/GenerativeTSE_demo/">https://aisaka0v0.github.io/GenerativeTSE_demo/</a>. </p>
<blockquote>
<p>目标语音提取（TSE）从多说话人重叠的语音混合中分离出特定说话人的语音。现有的大多数TSE模型依赖于判别方法，通常预测目标语音的时间-频率谱图掩膜。然而，这些掩膜的不完美往往导致目标语音或非目标语音的过抑制&#x2F;欠抑制，降低感知质量。相比之下，生成方法基于混合和目标说话人线索重新合成目标语音，实现优越的感知质量。然而，这些方法往往忽视了语音的可懂度，导致重新合成的语音中的语义内容发生改变或丢失。受Whisper模型在目标说话人语音识别中的成功启发，我们提出了基于预训练Whisper模型的生成TSE框架，以解决上述问题。该框架将语义建模与基于流的声学建模相结合，实现高可懂度和感知质量。来自多个基准测试的结果表明，所提出的方法优于现有的生成型和判别型基准方法。语音样本请访问：[<a target="_blank" rel="noopener" href="https://aisaka0v0.github.io/GenerativeTSE_demo/%E3%80%82]">https://aisaka0v0.github.io/GenerativeTSE_demo/。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.14477v2">PDF</a> Submitted to IEEE Signal Processing Letters</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于预训练Whisper模型的生成式目标语音提取框架，该框架结合了语义建模和基于流的声学建模，旨在提高目标语音提取的智听性和感知质量。该方法在多基准测试中的表现优于现有的生成式和判别式基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>目标语音提取（TSE）是从多说话人重叠语音中分离特定说话人的语音。</li>
<li>现有TSE模型大多依赖于判别式方法，通过预测时间-频率谱图掩膜来提取目标语音。</li>
<li>判别式方法存在的缺陷是掩膜不完美，可能导致目标语音或非目标语音的过度&#x2F;不足抑制，影响感知质量。</li>
<li>生成式方法通过混合语音和目标说话人线索重新合成目标语音，可以达到较高的感知质量。</li>
<li>然而，生成式方法常常忽视语音的清晰度，导致重新合成的语音中语义内容的改变或丢失。</li>
<li>本文受Whisper模型在目标说话人语音识别中的成功启发，提出一种基于预训练Whisper模型的生成式TSE框架。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.14477">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-037497f069e0dc738365e2c4c336663d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5ccc52813c3cff0c01d6c583b8c2208.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e3d7fb31204902805bdd12eeed15326.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="From-KAN-to-GR-KAN-Advancing-Speech-Enhancement-with-KAN-Based-Methodology"><a href="#From-KAN-to-GR-KAN-Advancing-Speech-Enhancement-with-KAN-Based-Methodology" class="headerlink" title="From KAN to GR-KAN: Advancing Speech Enhancement with KAN-Based   Methodology"></a>From KAN to GR-KAN: Advancing Speech Enhancement with KAN-Based   Methodology</h2><p><strong>Authors:Haoyang Li, Yuchen Hu, Chen Chen, Sabato Marco Siniscalchi, Songting Liu, Eng Siong Chng</strong></p>
<p>Deep neural network (DNN)-based speech enhancement (SE) usually uses conventional activation functions, which lack the expressiveness to capture complex multiscale structures needed for high-fidelity SE. Group-Rational KAN (GR-KAN), a variant of Kolmogorov-Arnold Networks (KAN), retains KAN’s expressiveness while improving scalability on complex tasks. We adapt GR-KAN to existing DNN-based SE by replacing dense layers with GR-KAN layers in the time-frequency (T-F) domain MP-SENet and adapting GR-KAN’s activations into the 1D CNN layers in the time-domain Demucs. Results on Voicebank-DEMAND show that GR-KAN requires up to 4x fewer parameters while improving PESQ by up to 0.1. In contrast, KAN, facing scalability issues, outperforms MLP on a small-scale signal modeling task but fails to improve MP-SENet. We demonstrate the first successful use of KAN-based methods for consistent improvement in both time- and SoTA TF-domain SE, establishing GR-KAN as a promising alternative for SE. </p>
<blockquote>
<p>基于深度神经网络（DNN）的语音增强（SE）通常使用传统的激活函数，这些函数缺乏表达复杂多尺度结构所需的表达能力，无法用于高保真SE。Group-Rational KAN（GR-KAN）是Kolmogorov-Arnold Networks（KAN）的一种变体，在复杂任务上保留了KAN的表达能力并提高了可扩展性。我们通过将时间-频率（T-F）域MP-SENet中的密集层替换为GR-KAN层，并将GR-KAN的激活适应到时间域Demucs的1D CNN层，将GR-KAN适应到现有的基于DNN的SE。在Voicebank-DEMAND上的结果表明，GR-KAN在改进PESQ高达0.1的同时，参数减少了高达4倍。相比之下，KAN面临可扩展性问题，在小规模信号建模任务上优于MLP，但未能改进MP-SENet。我们展示了基于KAN的方法在时间和当前TF域SE中的持续改进的首例成功应用，证明了GR-KAN在SE中的有前途的替代方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17778v2">PDF</a> Accepted to Interspeech2025</p>
<p><strong>Summary</strong><br>     基于深度神经网络（DNN）的语音增强（SE）通常使用传统的激活函数，难以捕捉复杂的多尺度结构以实现高保真SE。 Group-Rational KAN（GR-KAN）作为Kolmogorov-Arnold网络（KAN）的变体，在保留KAN表达力的同时，提高了在复杂任务上的可扩展性。我们通过将现有DNN-based SE中的密集层替换为GR-KAN层，并适应GR-KAN激活到时间域Demucs的1D CNN层和时间-频率（T-F）域MP-SENet，实现了GR-KAN在SE中的应用。在Voicebank-DEMAND上的结果显示，GR-KAN的参数需求少达4倍，同时PESQ提高达0.1。尽管KAN在小规模信号建模任务中表现优于MLP，但在改善MP-SENet方面并未成功。我们首次成功使用基于KAN的方法，在时间和当前TF域SE中都实现了改进，确立了GR-KAN在SE中的有前途的替代方案地位。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度神经网络（DNN）在语音增强（SE）中使用的传统激活函数难以捕捉复杂的多尺度结构。</li>
<li>Group-Rational KAN（GR-KAN）保留了Kolmogorov-Arnold网络（KAN）的表达能力并提高了其在复杂任务上的可扩展性。</li>
<li>通过将GR-KAN层替换密集层并适应GR-KAN激活到现有DNN-based SE的不同架构中，实现了GR-KAN在SE中的首次应用。</li>
<li>在Voicebank-DEMAND上的实验表明，GR-KAN的参数需求更少且性能有所提升。</li>
<li>虽然KAN在小规模信号建模任务中表现良好，但在改善MP-SENet方面并未达到预期效果。</li>
<li>基于KAN的方法在时间和当前TF域SE中都实现了改进，为SE领域提供了新的视角。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17778">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-564b3a08f757e21f42c482b074c7171d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c137bb0c6ba2570a2835c6792ac445a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8677f5f3a70a079045c236b6f10dcac6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee360f6c9d850a0bf680726c7086f08e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1346ff85c2384b8ad1d10ee705e6196.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-541b50ca816e74a26e50c4612546d5e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-baf1e95a2ab2ace3f2b9ea5420ad0007.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-25/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-25/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-25/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-10c20e5ebcfcbbe0109d82b703a8120c.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-05-25  Evaluation and optimization of deep learning models for enhanced   detection of brain cancer using transmission optical microscopy of thin brain   tissue samples
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-25/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-70c7f4cd00d993f756664c1de8e0ada4.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-05-25  Boosting Few-Shot Open-Set Object Detection via Prompt Learning and   Robust Decision Boundary
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27663.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
