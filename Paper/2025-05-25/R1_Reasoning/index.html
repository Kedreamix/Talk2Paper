<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-25  R1-ShareVL Incentivizing Reasoning Capability of Multimodal Large   Language Models via Share-GRPO">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e0ddb9172526b07885c64909614a637b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-25-æ›´æ–°"><a href="#2025-05-25-æ›´æ–°" class="headerlink" title="2025-05-25 æ›´æ–°"></a>2025-05-25 æ›´æ–°</h1><h2 id="R1-ShareVL-Incentivizing-Reasoning-Capability-of-Multimodal-Large-Language-Models-via-Share-GRPO"><a href="#R1-ShareVL-Incentivizing-Reasoning-Capability-of-Multimodal-Large-Language-Models-via-Share-GRPO" class="headerlink" title="R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large   Language Models via Share-GRPO"></a>R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large   Language Models via Share-GRPO</h2><p><strong>Authors:Huanjin Yao, Qixiang Yin, Jingyi Zhang, Min Yang, Yibo Wang, Wenhao Wu, Fei Su, Li Shen, Minghui Qiu, Dacheng Tao, Jiaxing Huang</strong></p>
<p>In this work, we aim to incentivize the reasoning ability of Multimodal Large Language Models (MLLMs) via reinforcement learning (RL) and develop an effective approach that mitigates the sparse reward and advantage vanishing issues during RL. To this end, we propose Share-GRPO, a novel RL approach that tackle these issues by exploring and sharing diverse reasoning trajectories over expanded question space. Specifically, Share-GRPO first expands the question space for a given question via data transformation techniques, and then encourages MLLM to effectively explore diverse reasoning trajectories over the expanded question space and shares the discovered reasoning trajectories across the expanded questions during RL. In addition, Share-GRPO also shares reward information during advantage computation, which estimates solution advantages hierarchically across and within question variants, allowing more accurate estimation of relative advantages and improving the stability of policy training. Extensive evaluations over six widely-used reasoning benchmarks showcase the superior performance of our method. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/HJYao00/R1-ShareVL">https://github.com/HJYao00/R1-ShareVL</a>. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¿€åŠ±å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶å¼€å‘ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•æ¥è§£å†³RLè¿‡ç¨‹ä¸­çš„ç¨€ç–å¥–åŠ±å’Œä¼˜åŠ¿æ¶ˆå¤±é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Share-GRPOï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„RLæ–¹æ³•ï¼Œé€šè¿‡åœ¨æ‰©å±•çš„é—®é¢˜ç©ºé—´ä¸Šæ¢ç´¢å’Œå…±äº«å¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼ŒShare-GRPOé¦–å…ˆé€šè¿‡æ•°æ®è½¬æ¢æŠ€æœ¯ä¸ºç»™å®šé—®é¢˜æ‰©å±•é—®é¢˜ç©ºé—´ï¼Œç„¶åé¼“åŠ±MLLMåœ¨æ‰©å±•çš„é—®é¢˜ç©ºé—´ä¸Šæœ‰æ•ˆåœ°æ¢ç´¢å¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹ï¼Œå¹¶åœ¨RLè¿‡ç¨‹ä¸­å…±äº«å‘ç°çš„æ¨ç†è½¨è¿¹ã€‚æ­¤å¤–ï¼ŒShare-GRPOè¿˜åœ¨è®¡ç®—ä¼˜åŠ¿æ—¶å…±äº«å¥–åŠ±ä¿¡æ¯ï¼Œè¯¥ä¿¡æ¯åˆ†å±‚ä¼°è®¡é—®é¢˜å˜ä½“ä¹‹é—´çš„è§£å†³æ–¹æ¡ˆä¼˜åŠ¿ï¼Œä»è€Œå…è®¸æ›´å‡†ç¡®åœ°ä¼°è®¡ç›¸å¯¹ä¼˜åŠ¿å¹¶æ”¹è¿›ç­–ç•¥è®­ç»ƒçš„ç¨³å®šæ€§ã€‚åœ¨å…­ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„å“è¶Šæ€§èƒ½ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/HJYao00/R1-ShareVL">https://github.com/HJYao00/R1-ShareVL</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16673v1">PDF</a> Technical report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¿€åŠ±å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶æå‡ºä¸€ç§æœ‰æ•ˆæ–¹æ³•æ¥è§£å†³RLä¸­çš„ç¨€ç–å¥–åŠ±å’Œä¼˜åŠ¿æ¶ˆå¤±é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„RLæ–¹æ³•Share-GRPOï¼Œå®ƒé€šè¿‡æ‰©å±•é—®é¢˜ç©ºé—´å¹¶æ¢ç´¢å’Œå…±äº«å¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚Share-GRPOé€šè¿‡æ•°æ®è½¬æ¢æŠ€æœ¯æ‰©å±•ç»™å®šé—®é¢˜çš„é—®é¢˜ç©ºé—´ï¼Œé¼“åŠ±MLLMåœ¨æ‰©å±•çš„é—®é¢˜ç©ºé—´ä¸Šæœ‰æ•ˆæ¢ç´¢å¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹ï¼Œå¹¶åœ¨RLè¿‡ç¨‹ä¸­åˆ†äº«å‘ç°çš„æ¨ç†è½¨è¿¹ã€‚æ­¤å¤–ï¼ŒShare-GRPOåœ¨ä¼˜åŠ¿è®¡ç®—è¿‡ç¨‹ä¸­å…±äº«å¥–åŠ±ä¿¡æ¯ï¼Œè¿™å¯ä»¥åœ¨é—®é¢˜å˜ä½“ä¹‹é—´å’Œå†…éƒ¨è¿›è¡Œåˆ†å±‚è§£å†³æ–¹æ¡ˆä¼˜åŠ¿ä¼°è®¡ï¼Œä»è€Œæ›´å‡†ç¡®åœ°ä¼°è®¡ç›¸å¯¹ä¼˜åŠ¿å¹¶æé«˜ç­–ç•¥è®­ç»ƒçš„ç¨³å®šæ€§ã€‚åœ¨å…­ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºäº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¿€åŠ±å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•Share-GRPOï¼Œè§£å†³ç¨€ç–å¥–åŠ±å’Œä¼˜åŠ¿æ¶ˆå¤±é—®é¢˜ã€‚</li>
<li>Share-GRPOé€šè¿‡æ•°æ®è½¬æ¢æŠ€æœ¯æ‰©å±•é—®é¢˜ç©ºé—´ã€‚</li>
<li>Share-GRPOé¼“åŠ±æ¨¡å‹åœ¨æ‰©å±•çš„é—®é¢˜ç©ºé—´ä¸Šæ¢ç´¢å¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹ã€‚</li>
<li>Share-GRPOåœ¨RLè¿‡ç¨‹ä¸­åˆ†äº«å‘ç°çš„æ¨ç†è½¨è¿¹ã€‚</li>
<li>Share-GRPOåœ¨ä¼˜åŠ¿è®¡ç®—è¿‡ç¨‹ä¸­å…±äº«å¥–åŠ±ä¿¡æ¯ï¼Œæé«˜ç­–ç•¥è®­ç»ƒçš„ç¨³å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16673">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-80a74954a103c729953c949eacf01a2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90306ea103bd8d16b087c0e10564ae46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06aaf3dbc1f88f4271a45c5c3366f036.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CoNav-Collaborative-Cross-Modal-Reasoning-for-Embodied-Navigation"><a href="#CoNav-Collaborative-Cross-Modal-Reasoning-for-Embodied-Navigation" class="headerlink" title="CoNav: Collaborative Cross-Modal Reasoning for Embodied Navigation"></a>CoNav: Collaborative Cross-Modal Reasoning for Embodied Navigation</h2><p><strong>Authors:Haihong Hao, Mingfei Han, Changlin Li, Zhihui Li, Xiaojun Chang</strong></p>
<p>Embodied navigation demands comprehensive scene understanding and precise spatial reasoning. While image-text models excel at interpreting pixel-level color and lighting cues, 3D-text models capture volumetric structure and spatial relationships. However, unified fusion approaches that jointly fuse 2D images, 3D point clouds, and textual instructions face challenges in limited availability of triple-modality data and difficulty resolving conflicting beliefs among modalities. In this work, we introduce CoNav, a collaborative cross-modal reasoning framework where a pretrained 3D-text model explicitly guides an image-text navigation agent by providing structured spatial-semantic knowledge to resolve ambiguities during navigation. Specifically, we introduce Cross-Modal Belief Alignment, which operationalizes this cross-modal guidance by simply sharing textual hypotheses from the 3D-text model to the navigation agent. Through lightweight fine-tuning on a small 2D-3D-text corpus, the navigation agent learns to integrate visual cues with spatial-semantic knowledge derived from the 3D-text model, enabling effective reasoning in embodied navigation. CoNav achieves significant improvements on four standard embodied navigation benchmarks (R2R, CVDN, REVERIE, SOON) and two spatial reasoning benchmarks (ScanQA, SQA3D). Moreover, under close navigation Success Rate, CoNav often generates shorter paths compared to other methods (as measured by SPL), showcasing the potential and challenges of fusing data from different modalities in embodied navigation. Project Page: <a target="_blank" rel="noopener" href="https://oceanhao.github.io/CoNav/">https://oceanhao.github.io/CoNav/</a> </p>
<blockquote>
<p>èº«ä½“åŒ–å¯¼èˆªè¦æ±‚å…¨é¢çš„åœºæ™¯ç†è§£å’Œç²¾ç¡®çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚è™½ç„¶å›¾åƒæ–‡æœ¬æ¨¡å‹æ“…é•¿è§£è¯»åƒç´ çº§çš„é¢œè‰²å’Œå…‰ç…§çº¿ç´¢ï¼Œä½†3Dæ–‡æœ¬æ¨¡å‹èƒ½å¤Ÿæ•æ‰ä½“ç§¯ç»“æ„å’Œç©ºé—´å…³ç³»ã€‚ç„¶è€Œï¼Œè”åˆèåˆ2Då›¾åƒã€3Dç‚¹äº‘å’Œæ–‡æœ¬æŒ‡ä»¤çš„ç»Ÿä¸€èåˆæ–¹æ³•é¢ä¸´ç€ä¸‰é‡æ¨¡æ€æ•°æ®æœ‰é™ä»¥åŠè§£å†³å„æ¨¡æ€ä¹‹é—´å†²çªä¿¡å¿µçš„å›°éš¾çš„æŒ‘æˆ˜ã€‚åœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†CoNavï¼Œè¿™æ˜¯ä¸€ä¸ªåä½œå¼è·¨æ¨¡æ€æ¨ç†æ¡†æ¶ï¼Œå…¶ä¸­é¢„è®­ç»ƒçš„3Dæ–‡æœ¬æ¨¡å‹é€šè¿‡æä¾›ç»“æ„åŒ–ç©ºé—´è¯­ä¹‰çŸ¥è¯†æ¥æ˜ç¡®æŒ‡å¯¼å›¾åƒæ–‡æœ¬å¯¼èˆªä»£ç†ï¼Œä»¥è§£å†³å¯¼èˆªè¿‡ç¨‹ä¸­çš„æ­§ä¹‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†è·¨æ¨¡æ€ä¿¡å¿µå¯¹é½ï¼Œå®ƒé€šè¿‡ç®€å•åœ°å…±äº«3Dæ–‡æœ¬æ¨¡å‹çš„æ–‡æœ¬å‡è®¾æ¥å®ç°è·¨æ¨¡æ€æŒ‡å¯¼å¯¼èˆªä»£ç†ã€‚é€šè¿‡åœ¨ä¸€ä¸ªå°çš„2D-3D-æ–‡æœ¬è¯­æ–™åº“ä¸Šè¿›è¡Œè½»é‡çº§å¾®è°ƒï¼Œå¯¼èˆªä»£ç†å­¦ä¼šäº†å°†è§†è§‰çº¿ç´¢ä¸ä»3Dæ–‡æœ¬æ¨¡å‹ä¸­æ´¾ç”Ÿçš„ç©ºé—´è¯­ä¹‰çŸ¥è¯†ç›¸ç»“åˆï¼Œä»è€Œåœ¨èº«ä½“åŒ–å¯¼èˆªä¸­å®ç°æœ‰æ•ˆçš„æ¨ç†ã€‚CoNavåœ¨å››ä¸ªæ ‡å‡†çš„èº«ä½“åŒ–å¯¼èˆªåŸºå‡†æµ‹è¯•ï¼ˆR2Rã€CVDNã€REVERIEã€SOONï¼‰å’Œä¸¤ä¸ªç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆScanQAã€SQA3Dï¼‰ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚æ­¤å¤–ï¼Œåœ¨ç´§å¯†å¯¼èˆªæˆåŠŸç‡ï¼ˆSuccess Rateï¼‰æ–¹é¢ï¼ŒCoNavç”Ÿæˆçš„è·¯å¾„é€šå¸¸æ¯”å…¶ä»–æ–¹æ³•æ›´çŸ­ï¼ˆä»¥SPLè¡¡é‡ï¼‰ï¼Œå±•ç¤ºäº†èåˆä¸åŒæ¨¡æ€æ•°æ®çš„æ½œåŠ›ä¸æŒ‘æˆ˜ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://oceanhao.github.io/CoNav/">https://oceanhao.github.io/CoNav/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16663v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è·¨æ¨¡æ€ååŒæ¨ç†æ¡†æ¶CoNavï¼Œç”¨äºè§£å†³èº«ä¸´å…¶å¢ƒçš„å¯¼èˆªä¸­çš„éš¾é¢˜ã€‚è¯¥æ¡†æ¶èåˆäº†é¢„è®­ç»ƒçš„3Dæ–‡æœ¬æ¨¡å‹ä¸å›¾åƒæ–‡æœ¬å¯¼èˆªæ¨¡å‹ï¼Œé€šè¿‡æä¾›ç»“æ„åŒ–ç©ºé—´è¯­ä¹‰çŸ¥è¯†è§£å†³å¯¼èˆªè¿‡ç¨‹ä¸­çš„æ­§ä¹‰é—®é¢˜ã€‚é€šè¿‡è·¨æ¨¡æ€ä¿¡å¿µå¯¹é½æœºåˆ¶ï¼ŒCoNavå®ç°äº†ä¸åŒæ¨¡æ€é—´çš„æœ‰æ•ˆæ²Ÿé€šï¼Œæé«˜äº†å¯¼èˆªä»£ç†åœ¨èº«ä¸´å…¶å¢ƒçš„å¯¼èˆªç¯å¢ƒä¸­çš„æ¨ç†èƒ½åŠ›ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCoNavå‡å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoNavæ¡†æ¶ç»“åˆäº†é¢„è®­ç»ƒçš„3Dæ–‡æœ¬æ¨¡å‹ä¸å›¾åƒæ–‡æœ¬å¯¼èˆªæ¨¡å‹ï¼Œæé«˜äº†èº«ä¸´å…¶å¢ƒçš„å¯¼èˆªä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è·¨æ¨¡æ€ä¿¡å¿µå¯¹é½æœºåˆ¶å®ç°äº†ä¸åŒæ¨¡æ€é—´çš„æœ‰æ•ˆæ²Ÿé€šï¼Œè§£å†³äº†å¯¼èˆªè¿‡ç¨‹ä¸­çš„æ­§ä¹‰é—®é¢˜ã€‚</li>
<li>CoNavé€šè¿‡èåˆè§†è§‰çº¿ç´¢å’Œç”±3Dæ–‡æœ¬æ¨¡å‹æ´¾ç”Ÿçš„ç©ºé—´è¯­ä¹‰çŸ¥è¯†ï¼Œä½¿å¯¼èˆªä»£ç†èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•´åˆä¸åŒæ¨¡æ€çš„æ•°æ®ã€‚</li>
<li>CoNavåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œè¯æ˜äº†èåˆä¸åŒæ¨¡æ€æ•°æ®çš„æ½œåŠ›ã€‚</li>
<li>CoNavç”Ÿæˆçš„è·¯å¾„é€šå¸¸æ¯”å…¶ä»–æ–¹æ³•æ›´çŸ­ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜åŠ¿ã€‚</li>
<li>æœ‰é™çš„ä¸‰ç§æ¨¡æ€æ•°æ®å¯ç”¨æ€§å’Œè§£å†³ä¸åŒæ¨¡æ€é—´å†²çªä¿¡å¿µçš„å›°éš¾ä»æ˜¯å½“å‰é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16663">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d278392fd0482073489b495619782726.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11e6fd3f44dc517450b6155c223503cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0d6dad9b070963a58163beac7c193c8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Point-Detect-Count-Multi-Task-Medical-Image-Understanding-with-Instruction-Tuned-Vision-Language-Models"><a href="#Point-Detect-Count-Multi-Task-Medical-Image-Understanding-with-Instruction-Tuned-Vision-Language-Models" class="headerlink" title="Point, Detect, Count: Multi-Task Medical Image Understanding with   Instruction-Tuned Vision-Language Models"></a>Point, Detect, Count: Multi-Task Medical Image Understanding with   Instruction-Tuned Vision-Language Models</h2><p><strong>Authors:Sushant Gautam, Michael A. Riegler, PÃ¥l Halvorsen</strong></p>
<p>We investigate fine-tuning Vision-Language Models (VLMs) for multi-task medical image understanding, focusing on detection, localization, and counting of findings in medical images. Our objective is to evaluate whether instruction-tuned VLMs can simultaneously improve these tasks, with the goal of enhancing diagnostic accuracy and efficiency. Using MedMultiPoints, a multimodal dataset with annotations from endoscopy (polyps and instruments) and microscopy (sperm cells), we reformulate each task into instruction-based prompts suitable for vision-language reasoning. We fine-tune Qwen2.5-VL-7B-Instruct using Low-Rank Adaptation (LoRA) across multiple task combinations. Results show that multi-task training improves robustness and accuracy. For example, it reduces the Count Mean Absolute Error (MAE) and increases Matching Accuracy in the Counting + Pointing task. However, trade-offs emerge, such as more zero-case point predictions, indicating reduced reliability in edge cases despite overall performance gains. Our study highlights the potential of adapting general-purpose VLMs to specialized medical tasks via prompt-driven fine-tuning. This approach mirrors clinical workflows, where radiologists simultaneously localize, count, and describe findings - demonstrating how VLMs can learn composite diagnostic reasoning patterns. The model produces interpretable, structured outputs, offering a promising step toward explainable and versatile medical AI. Code, model weights, and scripts will be released for reproducibility at <a target="_blank" rel="noopener" href="https://github.com/simula/PointDetectCount">https://github.com/simula/PointDetectCount</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†é’ˆå¯¹å¤šä»»åŠ¡åŒ»å­¦å½±åƒç†è§£çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¾®è°ƒæŠ€æœ¯ï¼Œé‡ç‚¹èšç„¦äºåŒ»å­¦å½±åƒä¸­çš„æ£€æµ‹ç»“æœã€å®šä½ä»¥åŠè®¡æ•°ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è¯„ä¼°æŒ‡ä»¤ä¼˜åŒ–å‹VLMsæ˜¯å¦èƒ½å¤ŸåŒæ—¶æ”¹è¿›è¿™äº›ä»»åŠ¡ï¼Œä»¥æé«˜è¯Šæ–­å’Œæ•ˆç‡ã€‚æˆ‘ä»¬ä½¿ç”¨MedMultiPointsæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç»“åˆäº†å†…çª¥é•œï¼ˆæ¯è‚‰å’Œä»ªå™¨ï¼‰å’Œæ˜¾å¾®é•œï¼ˆç²¾å­ç»†èƒï¼‰çš„æ³¨é‡Šï¼Œå½¢æˆä¸€ä¸ªå¤šæ¨¡å¼æ•°æ®é›†ã€‚æˆ‘ä»¬å°†æ¯ä¸ªä»»åŠ¡é‡æ–°åˆ¶å®šä¸ºåŸºäºæŒ‡ä»¤çš„æç¤ºï¼Œé€‚ç”¨äºè§†è§‰è¯­è¨€æ¨ç†ã€‚æˆ‘ä»¬ä½¿ç”¨LoRAæŠ€æœ¯å¯¹Qwen2.5-VL-7B-Instructè¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”å¤šç§ä»»åŠ¡ç»„åˆã€‚ç»“æœæ˜¾ç¤ºï¼Œå¤šä»»åŠ¡è®­ç»ƒæé«˜äº†ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚ä¾‹å¦‚ï¼Œå®ƒå‡å°‘äº†è®¡æ•°å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ï¼Œå¹¶æé«˜äº†è®¡æ•°+å®šä½ä»»åŠ¡çš„åŒ¹é…å‡†ç¡®ç‡ã€‚ç„¶è€Œï¼Œä¹Ÿå‡ºç°äº†ä¸€äº›æƒè¡¡ï¼Œä¾‹å¦‚æ›´å¤šçš„é›¶æ¡ˆä¾‹ç‚¹é¢„æµ‹ï¼Œè¿™è¡¨æ˜å°½ç®¡æ€»ä½“æ€§èƒ½æœ‰æ‰€æå‡ï¼Œä½†åœ¨è¾¹ç¼˜æƒ…å†µä¸‹å¯é æ€§æœ‰æ‰€é™ä½ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†é€šè¿‡æç¤ºé©±åŠ¨å¾®è°ƒå°†é€šç”¨VLMsé€‚åº”ä¸“ä¸šåŒ»ç–—ä»»åŠ¡çš„æ½œåŠ›ã€‚è¿™ç§æ–¹æ³•åæ˜ äº†ä¸´åºŠå·¥ä½œæµç¨‹ï¼Œæ”¾å°„ç§‘åŒ»ç”Ÿå¯ä»¥åŒæ—¶å®šä½ã€è®¡æ•°å’Œæè¿°å‘ç°ç»“æœï¼Œå±•ç¤ºäº†VLMså¦‚ä½•å­¦ä¹ å¤åˆè¯Šæ–­æ¨ç†æ¨¡å¼ã€‚è¯¥æ¨¡å‹äº§ç”Ÿå¯è§£é‡Šã€ç»“æ„åŒ–çš„è¾“å‡ºï¼Œä¸ºå‘å¯è§£é‡Šå’Œå¤šåŠŸèƒ½åŒ»ç–—AIè¿ˆå‡ºäº†ä¸€æ­¥ï¼Œå…·æœ‰å¹¿é˜”çš„å‘å±•å‰æ™¯ã€‚ç›¸å…³ä»£ç ã€æ¨¡å‹æƒé‡å’Œè„šæœ¬å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/simula/PointDetectCount%E4%B8%8A%E5%8F%91%E5%B8%83%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E9%87%8D%E7%8E%B0%E6%80%A7%E3%80%82">https://github.com/simula/PointDetectCountä¸Šå‘å¸ƒï¼Œä»¥ä¿ƒè¿›é‡ç°æ€§ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16647v1">PDF</a> Accepted as a full paper at the 38th IEEE International Symposium on   Computer-Based Medical Systems (CBMS) 2025</p>
<p><strong>Summary</strong>ï¼š<br>ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¤šä»»åŠ¡è®­ç»ƒå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œå¾®è°ƒï¼Œä»¥æå‡åŒ»å­¦å›¾åƒç†è§£çš„æ€§èƒ½ï¼Œé‡ç‚¹æ¢ç´¢æ£€æµ‹ã€å®šä½å’Œè®¡æ•°ä»»åŠ¡ã€‚ä½¿ç”¨MedMultiPointsæ•°æ®é›†å’Œå¤šæ¨¡æ€æ ‡æ³¨ï¼Œç ”ç©¶å›¢é˜Ÿå°†ä»»åŠ¡è½¬åŒ–ä¸ºåŸºäºæŒ‡ä»¤çš„æç¤ºï¼Œä»¥é€‚åº”è§†è§‰è¯­è¨€æ¨ç†ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¤šä»»åŠ¡è®­ç»ƒèƒ½æé«˜æ¨¡å‹çš„ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ï¼Œå‡å°‘è®¡æ•°ä»»åŠ¡çš„å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ï¼Œæé«˜åŒ¹é…å‡†ç¡®ç‡ã€‚å°½ç®¡åœ¨ç‰¹å®šæƒ…å†µä¸‹å‡ºç°ä¸€äº›æƒè¡¡ï¼Œå¦‚æ›´å¤šé›¶ç‚¹é¢„æµ‹ç»“æœçš„å‡ºç°ï¼Œä½†è¯¥ç ”ç©¶çªå‡ºäº†æŒ‡ä»¤é©±åŠ¨å¾®è°ƒå¯¹äºé€‚åº”ä¸“ä¸šåŒ»å­¦ä»»åŠ¡çš„æ½œåŠ›ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†VLMså­¦ä¹ ç»„åˆè¯Šæ–­æ¨ç†æ¨¡å¼çš„æ½œåŠ›ï¼Œä¸ºåŒ»å­¦AIçš„å¯è§£é‡Šæ€§å’Œé€šç”¨æ€§è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚ç›¸å…³ç ”ç©¶èµ„æ–™å¯åœ¨<a href="#https://github.com/simula/PointDetectCount">é“¾æ¥åœ°å€</a>è·å–ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>ç ”ç©¶å›¢é˜Ÿé’ˆå¯¹åŒ»å­¦å›¾åƒç†è§£çš„å¤šä»»åŠ¡è®­ç»ƒè¿›è¡Œäº†è§†è§‰è¯­è¨€æ¨¡å‹çš„å¾®è°ƒç ”ç©¶ã€‚</li>
<li>ç ”ç©¶é‡ç‚¹åŒ…æ‹¬æ£€æµ‹ã€å®šä½å’Œè®¡æ•°ä»»åŠ¡ï¼Œæ—¨åœ¨æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>ä½¿ç”¨MedMultiPointsæ•°æ®é›†å’Œå¤šæ¨¡æ€æ ‡æ³¨ä¿¡æ¯æ¥é€‚åº”è§†è§‰è¯­è¨€æ¨ç†ä»»åŠ¡ã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜å¤šä»»åŠ¡è®­ç»ƒèƒ½å¤Ÿæé«˜æ¨¡å‹çš„ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡æŒ‡ä»¤é©±åŠ¨çš„å¾®è°ƒç­–ç•¥å¯ä»¥è°ƒæ•´é€šç”¨æ¨¡å‹ä»¥é€‚åº”ç‰¹å®šåŒ»å­¦ä»»åŠ¡éœ€æ±‚ã€‚</li>
<li>ç‰¹å®šæƒ…å†µä¸‹çš„æƒè¡¡å¯èƒ½å‡ºç°å¦‚å¢åŠ é›¶ç‚¹é¢„æµ‹çš„æƒ…å†µã€‚ç„¶è€Œå°½ç®¡å¦‚æ­¤å…¶ä»ç„¶æ˜¯é€šç”¨å¯è§£é‡Šçš„ï¼Œæ˜¯åŒ»å­¦AIçš„ä¸€ä¸ªæœ‰å‰æ™¯çš„æ­¥éª¤ã€‚ç ”ç©¶ææ–™å…¬å¼€åœ¨æŒ‡å®šé“¾æ¥ä¾›æŸ¥é˜…å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16647">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb2c99815c192704a3f055f58582fd18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3aec9cd89c3d9eff3027bd6f9c0b4113.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4147ce1c6eedaad57f24d207c7349a2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d18b3daedcd78a2d97fcf57b8b6107a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SSR-Zero-Simple-Self-Rewarding-Reinforcement-Learning-for-Machine-Translation"><a href="#SSR-Zero-Simple-Self-Rewarding-Reinforcement-Learning-for-Machine-Translation" class="headerlink" title="SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine   Translation"></a>SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine   Translation</h2><p><strong>Authors:Wenjie Yang, Mao Zheng, Mingyang Song, Zheng Li</strong></p>
<p>Large language models (LLMs) have recently demonstrated remarkable capabilities in machine translation (MT). However, most advanced MT-specific LLMs heavily rely on external supervision signals during training, such as human-annotated reference data or trained reward models (RMs), which are often expensive to obtain and challenging to scale. To overcome this limitation, we propose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for MT that is reference-free, fully online, and relies solely on self-judging rewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as the backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs, e.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like Qwen2.5-32B-Instruct in English $\leftrightarrow$ Chinese translation tasks from WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR with external supervision from COMET, our strongest model, SSR-X-Zero-7B, achieves state-of-the-art performance in English $\leftrightarrow$ Chinese translation, surpassing all existing open-source models under 72B parameters and even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro. Our analysis highlights the effectiveness of the self-rewarding mechanism compared to the external LLM-as-a-judge approach in MT and demonstrates its complementary benefits when combined with trained RMs. Our findings provide valuable insight into the potential of self-improving RL methods. We have publicly released our code, data and models. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å…ˆè¿›çš„é’ˆå¯¹æœºå™¨ç¿»è¯‘çš„LLMåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸¥é‡ä¾èµ–äºå¤–éƒ¨ç›‘ç£ä¿¡å·ï¼Œå¦‚äººå·¥æ ‡æ³¨çš„å‚è€ƒæ•°æ®æˆ–å·²è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ï¼Œè¿™äº›ä¿¡å·çš„è·å–å¾€å¾€æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥æ‰©å±•ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºæœºå™¨ç¿»è¯‘çš„æ— éœ€å‚è€ƒçš„ç®€å•è‡ªæˆ‘å¥–åŠ±ï¼ˆSSRï¼‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å®Œå…¨åœ¨çº¿ï¼Œä»…ä¾èµ–äºè‡ªæˆ‘åˆ¤æ–­å¥–åŠ±ã€‚ä½¿ç”¨SSRè¿›è¡Œè®­ç»ƒï¼Œä»¥13Kå•è¯­ä¾‹å¥å’ŒQwen-2.5-7Bä½œä¸ºéª¨å¹²ï¼Œæˆ‘ä»¬çš„SSR-Zero-7Bæ¨¡å‹åœ¨WMT23ã€WMT24å’ŒFlores200åŸºå‡†æµ‹è¯•ä¸­çš„è‹±è¯­â†’ä¸­æ–‡ç¿»è¯‘ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„é’ˆå¯¹æœºå™¨ç¿»è¯‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚TowerInstruct-13Bå’ŒGemmaX-28-9Bï¼Œä»¥åŠåœ¨é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è¡¨ç°æ›´å¥½çš„Qwen2.5-32B-Instructã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ©ç”¨æ¥è‡ªCOMETçš„å¤–éƒ¨ç›‘ç£å¢å¼ºSSRï¼Œæˆ‘ä»¬æœ€å¼ºçš„æ¨¡å‹SSR-X-Zero-7Båœ¨è‹±è¯­â†’ä¸­æ–‡ç¿»è¯‘æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œä¸ä»…åœ¨å‚æ•°å°‘äº72Bçš„ç°æœ‰å¼€æºæ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œç”šè‡³è¶…è¶Šäº†å°é—­æºæ¨¡å‹ï¼Œå¦‚GPT-4oå’ŒGemini 1.5 Proã€‚æˆ‘ä»¬çš„åˆ†æçªå‡ºäº†è‡ªæˆ‘å¥–åŠ±æœºåˆ¶åœ¨æœºå™¨ç¿»è¯‘ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸å¤–éƒ¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„åˆ¤è€…çš„æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼Œè¯æ˜äº†å…¶ç»“åˆè®­ç»ƒåçš„å¥–åŠ±æ¨¡å‹çš„äº’è¡¥ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºè‡ªæˆ‘æ”¹è¿›å¼ºåŒ–å­¦ä¹ æ–¹æ³•æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚æˆ‘ä»¬å·²ç»å…¬å¼€å‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16637v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å¤§å¤šæ•°å…ˆè¿›çš„æœºå™¨ç¿»è¯‘ç‰¹å®šçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸¥é‡ä¾èµ–å¤–éƒ¨ç›‘ç£ä¿¡å·ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€å‚è€ƒçš„ç®€æ˜“è‡ªæˆ‘å¥–åŠ±å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å®Œå…¨åœ¨çº¿ï¼Œå¹¶ä»…ä¾èµ–äºè‡ªæˆ‘åˆ¤æ–­å¥–åŠ±ã€‚ä½¿ç”¨SSRè®­ç»ƒå’ŒQwen-2.5-7Bä½œä¸ºéª¨å¹²ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨WMT23ã€WMT24å’ŒFlores200åŸºå‡†æµ‹è¯•ä¸­çš„è‹±æ±‰ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœºå™¨ç¿»è¯‘ç‰¹å®šçš„å¤§å‹è¯­è¨€æ¨¡å‹ä»¥åŠæ›´å¤§çš„é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¢å¼ºSSRçš„å¤–éƒ¨ç›‘ç£æ¥è‡ªCOMETï¼Œæˆ‘ä»¬æœ€å¼ºçš„æ¨¡å‹SSR-X-Zero-7Bå®ç°äº†è‹±æ±‰ç¿»è¯‘çš„æœ€æ–°æ€§èƒ½ï¼Œè¶…è¶Šäº†æ‰€æœ‰ç°æœ‰çš„å¼€æºæ¨¡å‹ï¼Œç”šè‡³è¶…è¿‡äº†é—­æºæ¨¡å‹ï¼Œå¦‚GPT-4oå’ŒGemini 1.5 Proã€‚æˆ‘ä»¬çš„åˆ†æçªå‡ºäº†è‡ªæˆ‘å¥–åŠ±æœºåˆ¶åœ¨æœºå™¨ç¿»è¯‘ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶ä¸è®­ç»ƒåçš„å¥–åŠ±æ¨¡å‹çš„äº’è¡¥ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºè‡ªæˆ‘æ”¹è¿›çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚æˆ‘ä»¬å·²ç»å…¬å¼€å‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ— éœ€å‚è€ƒçš„ç®€æ˜“è‡ªæˆ‘å¥–åŠ±å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæœºå™¨ç¿»è¯‘ï¼Œè¯¥æ¡†æ¶å®Œå…¨åœ¨çº¿å¹¶ä»…ä¾èµ–äºè‡ªæˆ‘åˆ¤æ–­å¥–åŠ±ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨SSRè®­ç»ƒå’ŒQwen-2.5-7Bä½œä¸ºéª¨å¹²ï¼Œæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„è‹±æ±‰ç¿»è¯‘ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚</li>
<li>é€šè¿‡ç»“åˆå¤–éƒ¨ç›‘ç£ï¼Œæ¨¡å‹æ€§èƒ½å¾—åˆ°äº†è¿›ä¸€æ­¥æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›æ¨¡å‹ã€‚</li>
<li>è‡ªæˆ‘å¥–åŠ±æœºåˆ¶åœ¨æœºå™¨ç¿»è¯‘ä¸­çš„æœ‰æ•ˆæ€§å¾—åˆ°è¯å®ï¼Œå¹¶ä¸è®­ç»ƒåçš„å¥–åŠ±æ¨¡å‹è¡¨ç°å‡ºäº’è¡¥ä¼˜åŠ¿ã€‚</li>
<li>ç ”ç©¶ä¸ºè‡ªæˆ‘æ”¹è¿›çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•æä¾›äº†æ–°çš„è§è§£ã€‚</li>
<li>ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å·²å…¬å¼€åˆ†äº«ã€‚</li>
<li>ç ”ç©¶ä¸ºæœºå™¨ç¿»è¯‘é¢†åŸŸçš„å‘å±•åšå‡ºäº†é‡è¦è´¡çŒ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16637">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8037d086bff3d9553f46e20db3b8e6e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80b40c47663b1d9cd9214cf814f66e0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-698c2080d5cbad153a57a7005d564776.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ed2fa163c0d7703c08d364aec916979.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="O-2-Searcher-A-Searching-based-Agent-Model-for-Open-Domain-Open-Ended-Question-Answering"><a href="#O-2-Searcher-A-Searching-based-Agent-Model-for-Open-Domain-Open-Ended-Question-Answering" class="headerlink" title="O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended   Question Answering"></a>O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended   Question Answering</h2><p><strong>Authors:Jianbiao Mei, Tao Hu, Daocheng Fu, Licheng Wen, Xuemeng Yang, Rong Wu, Pinlong Cai, Xing Gao, Yu Yang, Chengjun Xie, Botian Shi, Yong Liu, Yu Qiao</strong></p>
<p>Large Language Models (LLMs), despite their advancements, are fundamentally limited by their static parametric knowledge, hindering performance on tasks requiring open-domain up-to-date information. While enabling LLMs to interact with external knowledge environments is a promising solution, current efforts primarily address closed-end problems. Open-ended questions, which characterized by lacking a standard answer or providing non-unique and diverse answers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a novel search agent leveraging reinforcement learning to effectively tackle both open-ended and closed-ended questions in the open domain. O$^2$-Searcher leverages an efficient, locally simulated search environment for dynamic knowledge acquisition, effectively decoupling the external world knowledge from modelâ€™s sophisticated reasoning processes. It employs a unified training mechanism with meticulously designed reward functions, enabling the agent to identify problem types and adapt different answer generation strategies. Furthermore, to evaluate performance on complex open-ended tasks, we construct O$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain open-ended questions with associated web page caches. Extensive experiments show that O$^2$-Searcher, using only a 3B model, significantly surpasses leading LLM agents on O$^2$-QA. It also achieves SOTA results on various closed-ended QA benchmarks against similarly-sized models, while performing on par with much larger ones. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å–å¾—äº†è¿›å±•ï¼Œä½†å®ƒä»¬ä»ç„¶å—åˆ°é™æ€å‚æ•°çŸ¥è¯†çš„æ ¹æœ¬é™åˆ¶ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬åœ¨éœ€è¦å¼€æ”¾é¢†åŸŸæœ€æ–°ä¿¡æ¯çš„ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è™½ç„¶è®©LLMsä¸å¤–éƒ¨ç¯å¢ƒè¿›è¡Œäº¤äº’æ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç›®å‰çš„åŠªåŠ›ä¸»è¦ä¾§é‡äºè§£å†³å°é—­å¼é—®é¢˜ã€‚å¼€æ”¾å¼é—®é¢˜ç¼ºä¹æ ‡å‡†ç­”æ¡ˆæˆ–æä¾›éå”¯ä¸€å’Œå¤šæ ·åŒ–çš„ç­”æ¡ˆï¼Œä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†O$^2$-Searcherï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¼ºåŒ–å­¦ä¹ çš„æ–°æœç´¢ä»£ç†ï¼Œæœ‰æ•ˆåœ°è§£å†³å¼€æ”¾å¼å’Œå°é—­å¼é—®é¢˜ã€‚O$^2$-Searcheråˆ©ç”¨é«˜æ•ˆçš„å±€éƒ¨æ¨¡æ‹Ÿæœç´¢ç¯å¢ƒè¿›è¡ŒåŠ¨æ€çŸ¥è¯†è·å–ï¼Œæœ‰æ•ˆåœ°å°†å¤–éƒ¨ä¸–ç•Œçš„çŸ¥è¯†ä¸æ¨¡å‹çš„å¤æ‚æ¨ç†è¿‡ç¨‹è§£è€¦ã€‚å®ƒé‡‡ç”¨ç»Ÿä¸€çš„åŸ¹è®­æœºåˆ¶å’Œç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼Œä½¿ä»£ç†èƒ½å¤Ÿè¯†åˆ«é—®é¢˜ç±»å‹å¹¶é€‚åº”ä¸åŒçš„ç­”æ¡ˆç”Ÿæˆç­–ç•¥ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¯„ä¼°åœ¨å¤æ‚å¼€æ”¾å¼ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œæˆ‘ä»¬æ„å»ºäº†O$^2$-QAï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«300ä¸ªæ‰‹åŠ¨æ•´ç†çš„å¤šé¢†åŸŸå¼€æ”¾å¼é—®é¢˜åŠå…¶ç›¸å…³ç½‘é¡µç¼“å­˜ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä»…ä½¿ç”¨3Bæ¨¡å‹çš„O$^2$-Searcheråœ¨O$^2$-QAä¸Šæ˜¾è‘—è¶…è¶Šäº†é¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ã€‚å®ƒåœ¨å„ç§å°é—­å¼é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šä¹Ÿå–å¾—äº†æœ€ä½³ç»“æœï¼Œä¸ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ç›¸æ¯”è¡¨ç°è‰¯å¥½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16582v1">PDF</a> 25 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å—é™äºé™æ€å‚æ•°çŸ¥è¯†ï¼Œéš¾ä»¥åº”å¯¹éœ€è¦å¼€æ”¾é¢†åŸŸå³æ—¶ä¿¡æ¯çš„ä»»åŠ¡ã€‚å°½ç®¡ä¸å¤–éƒ¨çŸ¥è¯†ç¯å¢ƒäº’åŠ¨æ˜¯è§£å†³æ–¹æ¡ˆï¼Œä½†å½“å‰ç ”ç©¶ä¸»è¦å…³æ³¨å°é—­å¼é—®é¢˜ï¼Œå¯¹å¼€æ”¾å¼é—®é¢˜ç¼ºä¹æ¢ç´¢ã€‚ä¸ºè§£å†³æ­¤å·®è·ï¼Œæœ¬æ–‡æå‡ºO$^2$-Searcherï¼Œä¸€ä¸ªåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è§£å†³å¼€æ”¾å¼å’Œå°é—­å¼é—®é¢˜çš„æœç´¢ä»£ç†ã€‚å®ƒåœ¨æœ¬åœ°æ¨¡æ‹Ÿæœç´¢ç¯å¢ƒå®ç°åŠ¨æ€çŸ¥è¯†è·å–ï¼Œå°†å¤–éƒ¨ä¸–ç•ŒçŸ¥è¯†ä¸æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹åˆ†ç¦»ã€‚é€šè¿‡ç»Ÿä¸€è®­ç»ƒæœºåˆ¶å’Œç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼Œä»£ç†èƒ½è¯†åˆ«é—®é¢˜ç±»å‹å¹¶é‡‡ç”¨ä¸åŒç­”æ¡ˆç”Ÿæˆç­–ç•¥ã€‚åŒæ—¶ï¼Œæ„å»ºO$^2$-QAä½œä¸ºé«˜è´¨é‡åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«300ä¸ªæ‰‹åŠ¨æ•´ç†çš„å¤šé¢†åŸŸå¼€æ”¾å¼é—®é¢˜åŠå…¶ç½‘é¡µç¼“å­˜ã€‚å®éªŒæ˜¾ç¤ºï¼Œä»…ä½¿ç”¨3Bæ¨¡å‹çš„O$^2$-Searcheråœ¨O$^2$-QAä¸Šæ˜¾è‘—è¶…è¶Šé¢†å…ˆLLMä»£ç†ï¼Œå¹¶åœ¨å„ç§å°é—­å¼é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä½³æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å—é™äºé™æ€å‚æ•°çŸ¥è¯†ï¼Œéš¾ä»¥åº”å¯¹éœ€è¦å¼€æ”¾é¢†åŸŸå³æ—¶ä¿¡æ¯çš„ä»»åŠ¡ã€‚</li>
<li>å¼€æ”¾å¼é—®é¢˜åœ¨LLMç ”ç©¶ä¸­è¢«å¿½è§†ã€‚</li>
<li>O$^2$-Searcheråˆ©ç”¨å¼ºåŒ–å­¦ä¹ è§£å†³å¼€æ”¾å¼å’Œå°é—­å¼é—®é¢˜ï¼Œèƒ½åœ¨æœ¬åœ°æ¨¡æ‹Ÿæœç´¢ç¯å¢ƒå®ç°åŠ¨æ€çŸ¥è¯†è·å–ã€‚</li>
<li>O$^2$-Searcheré€šè¿‡ç»Ÿä¸€è®­ç»ƒæœºåˆ¶å’Œå¥–åŠ±å‡½æ•°è¯†åˆ«é—®é¢˜ç±»å‹å¹¶ç”Ÿæˆç­”æ¡ˆã€‚</li>
<li>æ„å»ºäº†O$^2$-QAä½œä¸ºé«˜è´¨é‡åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°åœ¨å¤æ‚å¼€æ”¾å¼ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>O$^2$-Searcheråœ¨O$^2$-QAä¸Šè¡¨ç°ä¼˜è¶Šï¼Œæ˜¾è‘—è¶…è¶Šå…¶ä»–LLMä»£ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16582">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a79122ab1a26aa98808275a60d0d0a34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b8a2a3bec0c619d0bd283c919a14b9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d14ba5cf04f2b87a3c074368134e999.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8577460e43a987c0b9ce0ecd0bcf8e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-36c9a2ec9ca7826a16f52102bac095ff.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Bridging-the-Dynamic-Perception-Gap-Training-Free-Draft-Chain-of-Thought-for-Dynamic-Multimodal-Spatial-Reasoning"><a href="#Bridging-the-Dynamic-Perception-Gap-Training-Free-Draft-Chain-of-Thought-for-Dynamic-Multimodal-Spatial-Reasoning" class="headerlink" title="Bridging the Dynamic Perception Gap: Training-Free Draft   Chain-of-Thought for Dynamic Multimodal Spatial Reasoning"></a>Bridging the Dynamic Perception Gap: Training-Free Draft   Chain-of-Thought for Dynamic Multimodal Spatial Reasoning</h2><p><strong>Authors:Siqu Ou, Hongcheng Liu, Pingjie Wang, Yusheng Liao, Chuan Xuan, Yanfeng Wang, Yu Wang</strong></p>
<p>While chains-of-thought (CoT) have advanced complex reasoning in multimodal large language models (MLLMs), existing methods remain confined to text or static visual domains, often faltering in dynamic spatial reasoning tasks. To bridge this gap, we present GRASSLAND, a novel maze navigation benchmark designed to evaluate dynamic spatial reasoning. Our experiments show that augmenting textual reasoning chains with dynamic visual drafts, overlaid on input images, significantly outperforms conventional approaches, offering new insights into spatial reasoning in evolving environments. To generalize this capability, we propose D2R (Dynamic Draft-Augmented Reasoning), a training-free framework that seamlessly integrates textual CoT with corresponding visual drafts into MLLMs. Extensive evaluations demonstrate that D2R consistently enhances performance across diverse tasks, establishing a robust baseline for dynamic spatial reasoning without requiring model fine-tuning. Project is open at <a target="_blank" rel="noopener" href="https://github.com/Cratileo/D2R">https://github.com/Cratileo/D2R</a>. </p>
<blockquote>
<p>è™½ç„¶æ€ç»´é“¾ï¼ˆCoTï¼‰å·²ç»åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­æ¨åŠ¨äº†å¤æ‚çš„æ¨ç†è¿‡ç¨‹ï¼Œä½†ç°æœ‰æ–¹æ³•ä»ç„¶å±€é™äºæ–‡æœ¬æˆ–é™æ€è§†è§‰é¢†åŸŸï¼Œåœ¨åŠ¨æ€ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­ç»å¸¸è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†GRASSLANDï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹è¿·å®«å¯¼èˆªåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°åŠ¨æ€ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œé€šè¿‡åœ¨è¾“å…¥å›¾åƒä¸Šå åŠ åŠ¨æ€è§†è§‰è‰å›¾æ¥å¢å¼ºæ–‡æœ¬æ¨ç†é“¾ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä¸ºåŠ¨æ€ç¯å¢ƒä¸­çš„ç©ºé—´æ¨ç†æä¾›äº†æ–°çš„è§è§£ã€‚ä¸ºäº†æ¨å¹¿è¿™ç§èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†D2Rï¼ˆåŠ¨æ€è‰å›¾å¢å¼ºæ¨ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒå³å¯æ— ç¼åœ°å°†æ–‡æœ¬æ€ç»´é“¾ä¸ç›¸åº”çš„è§†è§‰è‰å›¾é›†æˆåˆ°MLLMsä¸­çš„æ¡†æ¶ã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒD2Råœ¨å„ç§ä»»åŠ¡ä¸­éƒ½èƒ½æé«˜æ€§èƒ½ï¼Œåœ¨æ— éœ€å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹å»ºç«‹äº†åŠ¨æ€ç©ºé—´æ¨ç†çš„ç¨³å¥åŸºå‡†ã€‚é¡¹ç›®å·²å¼€æ”¾è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/Cratileo/D2R%E3%80%82">https://github.com/Cratileo/D2Rã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16579v1">PDF</a> 19 pages, 8 figures</p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä¸»è¦æ¢è®¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€ç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šçš„å±€é™æ€§ï¼Œä¸ºæ­¤æå‡ºäº†GRASSLANDå¯¼èˆªè¿·å®«åŸºå‡†æµ‹è¯•æ–¹æ³•ä»¥åŠåŠ¨æ€è‰æ¡ˆå¢å¼ºæ¨ç†ï¼ˆD2Rï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½æ— ç¼é›†æˆæ–‡æœ¬æ€è€ƒé“¾å’Œå¯¹åº”çš„è§†è§‰è‰æ¡ˆï¼Œåœ¨ä¸éœ€è¦æ¨¡å‹å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œå³å¯å®ç°å¯¹åŠ¨æ€ç©ºé—´æ¨ç†ä»»åŠ¡çš„ç¨³å¥åŸºå‡†æµ‹è¯•ã€‚ç‚¹å‡»é“¾æ¥<a target="_blank" rel="noopener" href="https://github.com/Cratileo/D2R%E5%8F%AF%E8%8E%B7%E5%8F%96%E6%9B%B4%E5%A4%9A%E4%BF%A1%E6%81%AF%E3%80%82">https://github.com/Cratileo/D2Rå¯è·å–æ›´å¤šä¿¡æ¯ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€ç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å—é™ã€‚</li>
<li>æå‡ºGRASSLANDå¯¼èˆªè¿·å®«åŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°åŠ¨æ€ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡åœ¨æ–‡æœ¬æ¨ç†é“¾ä¸­å¢åŠ åŠ¨æ€è§†è§‰è‰æ¡ˆæ¥ä¼˜åŒ–æ€§èƒ½ã€‚</li>
<li>D2Ræ¡†æ¶å¯å®ç°æ— è®­ç»ƒé›†æˆæ–‡æœ¬ä¸è§†è§‰è‰æ¡ˆçš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æ¨ç†ã€‚</li>
<li>D2Ræ¡†æ¶èƒ½æé«˜æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>D2Ræ¡†æ¶æ— éœ€æ¨¡å‹å¾®è°ƒå³å¯ä¸ºåŠ¨æ€ç©ºé—´æ¨ç†ä»»åŠ¡æä¾›ç¨³å¥åŸºå‡†æµ‹è¯•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16579">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2fa523a93f584eb0510fb82d35bb40b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe4af3a2b280f323093a506053625c30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74a0207b1df44723392041920e674211.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58b48926e1f2a37db4e07081847be70a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6557100a8a90979e4d93ed3ec83e9842.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21e6ea854a5de2abc015a94bef327393.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ScholarBench-A-Bilingual-Benchmark-for-Abstraction-Comprehension-and-Reasoning-Evaluation-in-Academic-Contexts"><a href="#ScholarBench-A-Bilingual-Benchmark-for-Abstraction-Comprehension-and-Reasoning-Evaluation-in-Academic-Contexts" class="headerlink" title="ScholarBench: A Bilingual Benchmark for Abstraction, Comprehension, and   Reasoning Evaluation in Academic Contexts"></a>ScholarBench: A Bilingual Benchmark for Abstraction, Comprehension, and   Reasoning Evaluation in Academic Contexts</h2><p><strong>Authors:Dongwon Noh, Donghyeok Koh, Junghun Yuk, Gyuwan Kim, Jaeyong Lee, Kyungtae Lim, Cheoneum Park</strong></p>
<p>Prior benchmarks for evaluating the domain-specific knowledge of large language models (LLMs) lack the scalability to handle complex academic tasks. To address this, we introduce \texttt{ScholarBench}, a benchmark centered on deep expert knowledge and complex academic problem-solving, which evaluates the academic reasoning ability of LLMs and is constructed through a three-step process. \texttt{ScholarBench} targets more specialized and logically complex contexts derived from academic literature, encompassing five distinct problem types. Unlike prior benchmarks, \texttt{ScholarBench} evaluates the abstraction, comprehension, and reasoning capabilities of LLMs across eight distinct research domains. To ensure high-quality evaluation data, we define category-specific example attributes and design questions that are aligned with the characteristic research methodologies and discourse structures of each domain. Additionally, this benchmark operates as an English-Korean bilingual dataset, facilitating simultaneous evaluation for linguistic capabilities of LLMs in both languages. The benchmark comprises 5,031 examples in Korean and 5,309 in English, with even state-of-the-art models like o3-mini achieving an average evaluation score of only 0.543, demonstrating the challenging nature of this benchmark. </p>
<blockquote>
<p>å…ˆå‰è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢†åŸŸç‰¹å®šçŸ¥è¯†çš„åŸºå‡†æµ‹è¯•ç¼ºä¹å¤„ç†å¤æ‚å­¦æœ¯ä»»åŠ¡çš„å¯æ‰©å±•æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†<code>ScholarBench</code>ï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥æ·±åº¦ä¸“ä¸šçŸ¥è¯†å’Œå¤æ‚å­¦æœ¯é—®é¢˜è§£å†³ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMçš„å­¦æœ¯æ¨ç†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡ä¸‰æ­¥è¿‡ç¨‹æ„å»ºã€‚<code>ScholarBench</code>é’ˆå¯¹ä»å­¦æœ¯æ–‡çŒ®ä¸­æ´¾ç”Ÿçš„æ›´ä¸“ä¸šå’Œé€»è¾‘æ›´å¤æ‚çš„æƒ…å¢ƒï¼ŒåŒ…å«äº”ç§ä¸åŒçš„é—®é¢˜ç±»å‹ã€‚ä¸å…ˆå‰çš„åŸºå‡†æµ‹è¯•ä¸åŒï¼Œ<code>ScholarBench</code>è¯„ä¼°LLMåœ¨å…«ä¸ªä¸åŒç ”ç©¶é¢†åŸŸçš„æŠ½è±¡ã€ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†ç¡®ä¿é«˜è´¨é‡çš„è¯„ä»·æ•°æ®ï¼Œæˆ‘ä»¬å®šä¹‰äº†ç‰¹å®šç±»åˆ«çš„ç¤ºä¾‹å±æ€§ï¼Œå¹¶è®¾è®¡äº†ä¸æ¯ä¸ªé¢†åŸŸç‰¹å¾çš„ç ”ç©¶æ–¹æ³•å’Œè¯è¯­ç»“æ„ç›¸ç¬¦çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿™ä¸ªåŸºå‡†æµ‹è¯•æ˜¯ä¸€ä¸ªè‹±è¯­-éŸ©è¯­åŒè¯­æ•°æ®é›†ï¼Œä¾¿äºåŒæ—¶è¯„ä¼°LLMåœ¨è¿™ä¸¤ç§è¯­è¨€ä¸­çš„è¯­è¨€èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«5031ä¸ªéŸ©è¯­ç¤ºä¾‹å’Œ5309ä¸ªè‹±è¯­ç¤ºä¾‹ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚o3-miniï¼Œå¹³å‡è¯„ä¼°å¾—åˆ†ä¹Ÿåªæœ‰0.543ï¼Œè¿™è¡¨æ˜è¯¥åŸºå‡†æµ‹è¯•å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16566v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºScholarBenchçš„æ–°åŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•æ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚å­¦æœ¯ä»»åŠ¡ä¸­çš„ä¸“ä¸šçŸ¥è¯†ã€‚ScholarBenchåŒ…å«äº”ç§ä¸åŒçš„é—®é¢˜ç±»å‹ï¼Œå¹¶æ¶‰åŠå…«ä¸ªä¸åŒçš„ç ”ç©¶é¢†åŸŸçš„å­¦æœ¯çŸ¥è¯†ã€‚è¿™ä¸€åŸºå‡†æµ‹è¯•ä»¥è‹±æ–‡å’ŒéŸ©æ–‡åŒè¯­è¿›è¡Œï¼Œä»¥è¯„ä¼°LLMåœ¨è¿™ä¸¤ç§è¯­è¨€ä¸­çš„è¯­è¨€èƒ½åŠ›ã€‚è¯¥æµ‹è¯•å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ä¹Ÿéš¾ä»¥å–å¾—é«˜åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li><code>ScholarBench</code>æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºå‡†æµ‹è¯•ï¼Œä¸“æ³¨äºè¯„ä¼°å…¶åœ¨å¤æ‚å­¦æœ¯ä»»åŠ¡ä¸­çš„ä¸“ä¸šçŸ¥è¯†ã€‚</li>
<li>è¯¥æµ‹è¯•åŒ…å«äº”ä¸ªä¸åŒçš„é—®é¢˜ç±»å‹ï¼Œæ—¨åœ¨è¯„ä¼°LLMåœ¨å­¦æœ¯æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li><code>ScholarBench</code>è¦†ç›–äº†å…«ä¸ªä¸åŒçš„ç ”ç©¶é¢†åŸŸï¼Œä½¿æµ‹è¯•æ›´åŠ å…¨é¢å’Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•æ˜¯ä¸€ä¸ªè‹±è¯­å’ŒéŸ©æ–‡åŒè¯­æ•°æ®é›†ï¼Œå¯ç”¨äºè¯„ä¼°LLMåœ¨è¿™ä¸¤ç§è¯­è¨€ä¸­çš„è¯­è¨€èƒ½åŠ›ã€‚</li>
<li>æµ‹è¯•æ•°æ®è´¨é‡é«˜ï¼Œé€šè¿‡å®šä¹‰ç‰¹å®šç±»åˆ«çš„å±æ€§æ¥è®¾è®¡é—®é¢˜ï¼Œä¸æ¯ä¸ªé¢†åŸŸçš„ç ”ç©¶æ–¹æ³•å’Œè¯è¯­ç»“æ„ç›¸ç¬¦ã€‚</li>
<li>ç›®å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨ScholarBenchä¸Šçš„è¡¨ç°å¹¶ä¸ç†æƒ³ï¼Œæ˜¾ç¤ºå‡ºè¯¥æµ‹è¯•çš„æŒ‘æˆ˜æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae3859eb21c9d338b5d53bd370656c0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d71b23fade58022b4b963a76b2e9ae2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b6a214c9e1c928b244ab501474ce01f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05525590af82d764ff8a3c2f2a3cea84.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e5ae2a9ec14597d8fae88f7dbc98bd93.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ALTo-Adaptive-Length-Tokenizer-for-Autoregressive-Mask-Generation"><a href="#ALTo-Adaptive-Length-Tokenizer-for-Autoregressive-Mask-Generation" class="headerlink" title="ALTo: Adaptive-Length Tokenizer for Autoregressive Mask Generation"></a>ALTo: Adaptive-Length Tokenizer for Autoregressive Mask Generation</h2><p><strong>Authors:Lingfeng Wang, Hualing Lin, Senda Chen, Tao Wang, Changxu Cheng, Yangyang Zhong, Dong Zheng, Wuyue Zhao</strong></p>
<p>While humans effortlessly draw visual objects and shapes by adaptively allocating attention based on their complexity, existing multimodal large language models (MLLMs) remain constrained by rigid token representations. Bridging this gap, we propose ALTo, an adaptive length tokenizer for autoregressive mask generation. To achieve this, a novel token length predictor is designed, along with a length regularization term and a differentiable token chunking strategy. We further build ALToLLM that seamlessly integrates ALTo into MLLM. Preferences on the trade-offs between mask quality and efficiency is implemented by group relative policy optimization (GRPO). Experiments demonstrate that ALToLLM achieves state-of-the-art performance with adaptive token cost on popular segmentation benchmarks. Code and models are released at <a target="_blank" rel="noopener" href="https://github.com/yayafengzi/ALToLLM">https://github.com/yayafengzi/ALToLLM</a>. </p>
<blockquote>
<p>äººç±»åœ¨ç»˜åˆ¶è§†è§‰å¯¹è±¡å’Œå½¢çŠ¶æ—¶ï¼Œå¯ä»¥è½»æ¾åœ°æ ¹æ®å®ƒä»¬çš„å¤æ‚æ€§è‡ªé€‚åº”åœ°åˆ†é…æ³¨æ„åŠ›ï¼Œè€Œç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä»å—åˆ°åˆšæ€§ä»¤ç‰Œè¡¨ç¤ºçš„çº¦æŸã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ALToï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè‡ªå›å½’æ©ç ç”Ÿæˆçš„è‡ªé€‚åº”é•¿åº¦ä»¤ç‰Œå™¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ–°å‹ä»¤ç‰Œé•¿åº¦é¢„æµ‹å™¨ï¼Œä»¥åŠä¸€ä¸ªé•¿åº¦æ­£åˆ™åŒ–é¡¹å’Œä¸€ä¸ªå¯å¾®åˆ†çš„ä»¤ç‰Œåˆ†å—ç­–ç•¥ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ„å»ºäº†æ— ç¼é›†æˆALToåˆ°MLLMçš„ALToLLMã€‚é€šè¿‡å¯¹ç¾¤ä½“ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å®æ–½ï¼Œå®ç°äº†æ©ç è´¨é‡å’Œæ•ˆç‡ä¹‹é—´çš„æƒè¡¡åå¥½ã€‚å®éªŒè¡¨æ˜ï¼ŒALToLLMåœ¨æµè¡Œåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å…·æœ‰è‡ªé€‚åº”ä»¤ç‰Œæˆæœ¬çš„æœ€ä½³æ€§èƒ½ã€‚ä»£ç å’Œæ¨¡å‹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/yayafengzi/ALToLLM%E3%80%82">https://github.com/yayafengzi/ALToLLMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16495v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”é•¿åº¦åˆ†è¯å™¨ALToï¼Œç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è‡ªé€‚åº”ç”Ÿæˆæ©ç ã€‚é€šè¿‡è®¾è®¡æ–°å‹ä»¤ç‰Œé•¿åº¦é¢„æµ‹å™¨ã€é•¿åº¦æ­£åˆ™åŒ–é¡¹å’Œå¯å¾®åˆ†ä»¤ç‰Œåˆ†å—ç­–ç•¥ï¼Œå®ç°äº†ALToã€‚è¿›ä¸€æ­¥æ„å»ºäº†å°†ALToæ— ç¼é›†æˆåˆ°MLLMä¸­çš„ALToLLMã€‚é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å®ç°æ©ç è´¨é‡å’Œæ•ˆç‡ä¹‹é—´çš„æƒè¡¡åå¥½ã€‚å®éªŒè¯æ˜ï¼ŒALToLLMåœ¨æµè¡Œåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å…·æœ‰è‡ªé€‚åº”ä»¤ç‰Œæˆæœ¬çš„æœ€æ–°æ€§èƒ½ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/yayafengzi/ALToLLM%E3%80%82">https://github.com/yayafengzi/ALToLLMã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ALToæ˜¯ä¸€ç§è‡ªé€‚åº”é•¿åº¦åˆ†è¯å™¨ï¼Œç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è‡ªé€‚åº”ç”Ÿæˆæ©ç ã€‚</li>
<li>ALToé€šè¿‡è®¾è®¡ä»¤ç‰Œé•¿åº¦é¢„æµ‹å™¨ã€é•¿åº¦æ­£åˆ™åŒ–é¡¹å’Œå¯å¾®åˆ†ä»¤ç‰Œåˆ†å—ç­–ç•¥æ¥å®ç°ã€‚</li>
<li>ALToLLMå°†ALToé›†æˆåˆ°MLLMä¸­ï¼Œå®ç°äº†æ©ç è´¨é‡å’Œæ•ˆç‡ä¹‹é—´çš„çµæ´»æƒè¡¡ã€‚</li>
<li>é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å®æ–½è¿™ç§æƒè¡¡ã€‚</li>
<li>ALToLLMåœ¨æµè¡Œåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è‡ªé€‚åº”ä»¤ç‰Œæˆæœ¬çš„æœ€æ–°æ€§èƒ½ã€‚</li>
<li>å‘å¸ƒäº†ç›¸å…³ä»£ç å’Œæ¨¡å‹ï¼Œä»¥ä¾¿å…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16495">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e24dc70a348946a12f6b86a34ffd6663.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90bdd9f87bfd3aa4e6cea780c4dea606.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f048e9c384410c742abe73e5e46274b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed5c45351c646b2fc0721f082e643098.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="WebAgent-R1-Training-Web-Agents-via-End-to-End-Multi-Turn-Reinforcement-Learning"><a href="#WebAgent-R1-Training-Web-Agents-via-End-to-End-Multi-Turn-Reinforcement-Learning" class="headerlink" title="WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement   Learning"></a>WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement   Learning</h2><p><strong>Authors:Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, Hyokun Yun, Lihong Li</strong></p>
<p>While reinforcement learning (RL) has demonstrated remarkable success in enhancing large language models (LLMs), it has primarily focused on single-turn tasks such as solving math problems. Training effective web agents for multi-turn interactions remains challenging due to the complexity of long-horizon decision-making across dynamic web interfaces. In this work, we present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework for training web agents. It learns directly from online interactions with web environments by asynchronously generating diverse trajectories, entirely guided by binary rewards depending on task success. Experiments on the WebArena-Lite benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to 44.8%, significantly outperforming existing state-of-the-art methods and strong proprietary models such as OpenAI o3. In-depth analyses reveal the effectiveness of the thinking-based prompting strategy and test-time scaling through increased interactions for web tasks. We further investigate different RL initialization policies by introducing two variants, namely WebAgent-R1-Zero and WebAgent-R1-CoT, which highlight the importance of the warm-up training stage (i.e., behavior cloning) and provide insights on incorporating long chain-of-thought (CoT) reasoning in web agents. </p>
<blockquote>
<p>è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆæ•ˆï¼Œä½†å®ƒä¸»è¦é›†ä¸­åœ¨å¦‚è§£å†³æ•°å­¦é—®é¢˜ç­‰å•å›åˆä»»åŠ¡ä¸Šã€‚ç”±äºè·¨åŠ¨æ€ç½‘é¡µç•Œé¢çš„é•¿å‘¨æœŸå†³ç­–å¤æ‚æ€§ï¼Œè®­ç»ƒç”¨äºå¤šå›åˆäº¤äº’çš„æœ‰æ•ˆç½‘ç»œä»£ç†ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†WebAgent-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„ç«¯åˆ°ç«¯å¤šå›åˆå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè®­ç»ƒç½‘ç»œä»£ç†ã€‚å®ƒç›´æ¥ä»ä¸åœ¨çº¿ç½‘ç»œç¯å¢ƒçš„äº¤äº’ä¸­å­¦ä¹ ï¼Œé€šè¿‡å¼‚æ­¥ç”Ÿæˆå¤šæ ·çš„è½¨è¿¹ï¼Œå®Œå…¨ç”±æ ¹æ®ä»»åŠ¡æˆåŠŸä¸å¦çš„äºŒå€¼å¥–åŠ±å¼•å¯¼ã€‚åœ¨WebArena-LiteåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº†WebAgent-R1çš„æœ‰æ•ˆæ€§ï¼Œå°†Qwen-2.5-3Bçš„ä»»åŠ¡æˆåŠŸç‡ä»6.1%æé«˜åˆ°33.9%ï¼Œå°†Llama-3.1-8Bçš„ä»»åŠ¡æˆåŠŸç‡ä»8.5%æé«˜åˆ°44.8%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•å’Œå¼ºå¤§çš„ä¸“æœ‰æ¨¡å‹ï¼Œå¦‚OpenAI o3ã€‚æ·±å…¥åˆ†ææ­ç¤ºäº†åŸºäºæ€è€ƒçš„æç¤ºç­–ç•¥å’Œé€šè¿‡å¢åŠ äº¤äº’æ¥æé«˜æµ‹è¯•æ—¶é—´ç¼©æ”¾å¯¹ç½‘é¡µä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡å¼•å…¥ä¸¤ç§å˜ä½“ï¼Œå³WebAgent-R1-Zeroå’ŒWebAgent-R1-CoTï¼Œæ¢è®¨äº†ä¸åŒçš„å¼ºåŒ–å­¦ä¹ åˆå§‹åŒ–ç­–ç•¥ï¼Œçªå‡ºäº†çƒ­èº«è®­ç»ƒé˜¶æ®µï¼ˆå³è¡Œä¸ºå…‹éš†ï¼‰çš„é‡è¦æ€§ï¼Œå¹¶æä¾›äº†å°†é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰æ¨ç†èå…¥ç½‘ç»œä»£ç†çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16421v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºWebAgent-R1çš„ç®€å•æœ‰æ•ˆçš„ç«¯åˆ°ç«¯å¤šè½®å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè®­ç»ƒç½‘é¡µä»£ç†ã€‚è¯¥æ¡†æ¶ç›´æ¥ä»åœ¨çº¿çš„ç½‘é¡µç¯å¢ƒäº¤äº’ä¸­å­¦ä¹ ï¼Œé€šè¿‡å¼‚æ­¥ç”Ÿæˆå¤šæ ·çš„è½¨è¿¹ï¼Œå®Œå…¨ç”±ä»»åŠ¡æˆåŠŸçš„äºŒå…ƒå¥–åŠ±å¼•å¯¼ã€‚å®éªŒè¡¨æ˜ï¼ŒWebAgent-R1åœ¨WebArena-LiteåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—æé«˜ä»»åŠ¡æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•å’Œå¼ºå¤§çš„ä¸“æœ‰æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæ·±åº¦åˆ†ææ­ç¤ºäº†åŸºäºæ€è€ƒæç¤ºç­–ç•¥çš„æœ‰æ•ˆæ€§ä»¥åŠæµ‹è¯•æ—¶é€šè¿‡å¢åŠ äº¤äº’æ¥æé«˜ä»»åŠ¡æ€§èƒ½çš„ç¼©æ”¾ç­–ç•¥ã€‚è¿˜é€šè¿‡å¼•å…¥ä¸¤ç§å˜ä½“WebAgent-R1-Zeroå’ŒWebAgent-R1-CoTï¼Œæ¢è®¨äº†ä¸åŒçš„å¼ºåŒ–å­¦ä¹ åˆå§‹åŒ–ç­–ç•¥ï¼Œçªæ˜¾äº†çƒ­èº«è®­ç»ƒé˜¶æ®µï¼ˆå³è¡Œä¸ºå…‹éš†ï¼‰çš„é‡è¦æ€§ï¼Œå¹¶æä¾›äº†å°†é•¿é“¾æ€ç»´èå…¥ç½‘é¡µä»£ç†çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WebAgent-R1æ˜¯ä¸€ç§ç”¨äºè®­ç»ƒç½‘é¡µä»£ç†çš„å¤šè½®å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„é•¿æœŸå†³ç­–é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶ç›´æ¥ä»åœ¨çº¿çš„ç½‘é¡µç¯å¢ƒäº¤äº’ä¸­å­¦ä¹ ï¼Œé€šè¿‡å¼‚æ­¥ç”Ÿæˆå¤šæ ·çš„è½¨è¿¹æ¥å®Œæˆä»»åŠ¡ã€‚</li>
<li>WebAgent-R1é€šè¿‡äºŒå…ƒå¥–åŠ±æœºåˆ¶è¿›è¡Œå¼•å¯¼ï¼Œæ ¹æ®ä»»åŠ¡æˆåŠŸä¸å¦è¿›è¡Œåé¦ˆã€‚</li>
<li>åœ¨WebArena-LiteåŸºå‡†æµ‹è¯•ä¸Šï¼ŒWebAgent-R1æ˜¾è‘—æé«˜ä»»åŠ¡æˆåŠŸç‡ï¼Œä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•å’Œä¸“æœ‰æ¨¡å‹ã€‚</li>
<li>æ·±åº¦åˆ†æè¡¨æ˜ï¼ŒåŸºäºæ€è€ƒæç¤ºç­–ç•¥å’Œæµ‹è¯•æ—¶é€šè¿‡å¢åŠ äº¤äº’æ¥æé«˜ä»»åŠ¡æ€§èƒ½çš„ç¼©æ”¾ç­–ç•¥æ˜¯æœ‰æ•ˆçš„ã€‚</li>
<li>WebAgent-R1çš„å˜ä½“WebAgent-R1-Zeroå’ŒWebAgent-R1-CoTå¼ºè°ƒäº†çƒ­èº«è®­ç»ƒé˜¶æ®µçš„é‡è¦æ€§ï¼Œå¹¶æ¢è®¨äº†èå…¥é•¿é“¾æ€ç»´åœ¨ç½‘é¡µä»£ç†ä¸­çš„ä»·å€¼ã€‚</li>
<li>å®éªŒç»“æœæ­ç¤ºäº†å¼ºåŒ–å­¦ä¹ çš„åˆå§‹åŒ–ç­–ç•¥å’Œçƒ­èº«è®­ç»ƒé˜¶æ®µå¯¹ç½‘é¡µä»£ç†æ€§èƒ½çš„é‡è¦å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16421">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5b644a27521daa3e125cf52c31b8ad4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4742907be9c02c3f5eddd2c23676c50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-980744fa5f8b8bd8f97c52ebdbe2ff96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2f96938cbbf52620e1896460df54ea3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0ddb9172526b07885c64909614a637b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f85ab7cab094195f36a92b0579a0b449.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Tool-Star-Empowering-LLM-Brained-Multi-Tool-Reasoner-via-Reinforcement-Learning"><a href="#Tool-Star-Empowering-LLM-Brained-Multi-Tool-Reasoner-via-Reinforcement-Learning" class="headerlink" title="Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement   Learning"></a>Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement   Learning</h2><p><strong>Authors:Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, Ji-Rong Wen</strong></p>
<p>Recently, large language models (LLMs) have shown remarkable reasoning capabilities via large-scale reinforcement learning (RL). However, leveraging the RL algorithm to empower effective multi-tool collaborative reasoning in LLMs remains an open challenge. In this paper, we introduce Tool-Star, an RL-based framework designed to empower LLMs to autonomously invoke multiple external tools during stepwise reasoning. Tool-Star integrates six types of tools and incorporates systematic designs in both data synthesis and training. To address the scarcity of tool-use data, we propose a general tool-integrated reasoning data synthesis pipeline, which combines tool-integrated prompting with hint-based sampling to automatically and scalably generate tool-use trajectories. A subsequent quality normalization and difficulty-aware classification process filters out low-quality samples and organizes the dataset from easy to hard. Furthermore, we propose a two-stage training framework to enhance multi-tool collaborative reasoning by: (1) cold-start fine-tuning, which guides LLMs to explore reasoning patterns via tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with hierarchical reward design, which reinforces reward understanding and promotes effective tool collaboration. Experimental analyses on over 10 challenging reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star. The code is available at <a target="_blank" rel="noopener" href="https://github.com/dongguanting/Tool-Star">https://github.com/dongguanting/Tool-Star</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¡¨ç°å‡ºäº†ä»¤äººç©ç›®çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¦‚ä½•åˆ©ç”¨RLç®—æ³•èµ‹èƒ½LLMä¸­çš„æœ‰æ•ˆå¤šå·¥å…·ååŒæ¨ç†ä»æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Tool-Starï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºRLçš„æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿LLMèƒ½å¤Ÿåœ¨é€æ­¥æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»è°ƒç”¨å¤šç§å¤–éƒ¨å·¥å…·ã€‚Tool-Staré›†æˆäº†å…­ç§ç±»å‹çš„å·¥å…·ï¼Œå¹¶åœ¨æ•°æ®åˆæˆå’ŒåŸ¹è®­æ–¹é¢éƒ½è¿›è¡Œäº†ç³»ç»Ÿè®¾è®¡ã€‚ä¸ºäº†è§£å†³å·¥å…·ä½¿ç”¨æ•°æ®çš„ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„å·¥å…·é›†æˆæ¨ç†æ•°æ®åˆæˆç®¡é“ï¼Œå®ƒç»“åˆäº†å·¥å…·é›†æˆæç¤ºå’ŒåŸºäºæç¤ºçš„é‡‡æ ·ï¼Œä»è€Œå¯ä»¥è‡ªåŠ¨å’Œå¯æ‰©å±•åœ°ç”Ÿæˆå·¥å…·ä½¿ç”¨è½¨è¿¹ã€‚éšåçš„è´¨é‡å½’ä¸€åŒ–å’Œéš¾åº¦æ„ŸçŸ¥åˆ†ç±»è¿‡ç¨‹è¿‡æ»¤æ‰ä½è´¨é‡æ ·æœ¬ï¼Œå¹¶ä»æ˜“åˆ°éš¾ç»„ç»‡æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡ä»¥ä¸‹ä¸¤ä¸ªæ–¹é¢æ¥å¢å¼ºå¤šå·¥å…·ååŒæ¨ç†ï¼šï¼ˆ1ï¼‰å†·å¯åŠ¨å¾®è°ƒï¼Œå¼•å¯¼LLMé€šè¿‡å·¥å…·è°ƒç”¨åé¦ˆæ¢ç´¢æ¨ç†æ¨¡å¼ï¼›ï¼ˆ2ï¼‰å…·æœ‰åˆ†å±‚å¥–åŠ±è®¾è®¡çš„å¤šå·¥å…·è‡ªæˆ‘æ‰¹åˆ¤RLç®—æ³•ï¼ŒåŠ å¼ºå¥–åŠ±ç†è§£å¹¶ä¿ƒè¿›æœ‰æ•ˆå·¥å…·åä½œã€‚åœ¨è¶…è¿‡10ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒåˆ†æçªå‡ºäº†Tool-Starçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚ä»£ç å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://github.com/dongguanting/Tool-Star%E3%80%82">https://github.com/dongguanting/Tool-Starã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16410v1">PDF</a> Working in progress</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¡†æ¶Tool-Starï¼Œæ—¨åœ¨è®©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€æ­¥æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»è°ƒç”¨å¤šç§å¤–éƒ¨å·¥å…·ã€‚é€šè¿‡å¼•å…¥å·¥å…·é›†æˆæ¨ç†æ•°æ®åˆæˆç®¡é“å’Œä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼ŒTool-Starè§£å†³äº†å·¥å…·ä½¿ç”¨æ•°æ®çš„ç¨€ç¼ºæ€§é—®é¢˜ï¼Œå¹¶æé«˜äº†å¤šå·¥å…·åä½œæ¨ç†çš„æ•ˆæœã€‚è¯¥æ¡†æ¶åœ¨è¶…è¿‡10ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå±•ç°å‡ºæœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Tool-Staræ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œç”¨äºèµ‹èƒ½å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»è°ƒç”¨å¤šç§å¤–éƒ¨å·¥å…·ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªé€šç”¨çš„å·¥å…·é›†æˆæ¨ç†æ•°æ®åˆæˆç®¡é“ï¼Œé€šè¿‡å·¥å…·é›†æˆæç¤ºå’ŒåŸºäºæç¤ºçš„é‡‡æ ·ï¼Œè‡ªåŠ¨ä¸”å¯æ‰©å±•åœ°ç”Ÿæˆå·¥å…·ä½¿ç”¨è½¨è¿¹ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡å†·å¯åŠ¨å¾®è°ƒå’Œå¤šå·¥å…·è‡ªæˆ‘æ‰¹åˆ¤å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¢å¼ºå¤šå·¥å…·åä½œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>è§£å†³äº†å·¥å…·ä½¿ç”¨æ•°æ®çš„ç¨€ç¼ºæ€§é—®é¢˜ï¼Œé€šè¿‡è´¨é‡å½’ä¸€åŒ–å’Œéš¾åº¦æ„ŸçŸ¥åˆ†ç±»è¿‡ç¨‹ç­›é€‰å‡ºä½è´¨é‡æ ·æœ¬ï¼Œå¹¶æŒ‰éš¾åº¦ç»„ç»‡æ•°æ®é›†ã€‚</li>
<li>Tool-Staræ¡†æ¶é€šè¿‡å¼ºåŒ–å¥–åŠ±ç†è§£å’Œä¿ƒè¿›æœ‰æ•ˆå·¥å…·åä½œï¼Œå®ç°äº†æ¨ç†èƒ½åŠ›çš„æ˜¾è‘—æé«˜ã€‚</li>
<li>åœ¨è¶…è¿‡10ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å®éªŒåˆ†æï¼Œè¯æ˜äº†Tool-Starçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16410">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0245332e700bdbf2489d2ec76caae982.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a0d785c57bd62f23778927bed8f7906.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="AceReason-Nemotron-Advancing-Math-and-Code-Reasoning-through-Reinforcement-Learning"><a href="#AceReason-Nemotron-Advancing-Math-and-Code-Reasoning-through-Reinforcement-Learning" class="headerlink" title="AceReason-Nemotron: Advancing Math and Code Reasoning through   Reinforcement Learning"></a>AceReason-Nemotron: Advancing Math and Code Reasoning through   Reinforcement Learning</h2><p><strong>Authors:Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping</strong></p>
<p>Despite recent progress in large-scale reinforcement learning (RL) for reasoning, the training recipe for building high-performing reasoning models remains elusive. Key implementation details of frontier models, such as DeepSeek-R1, including data curation strategies and RL training recipe, are often omitted. Moreover, recent research indicates distillation remains more effective than RL for smaller models. In this work, we demonstrate that large-scale RL can significantly enhance the reasoning capabilities of strong, small- and mid-sized models, achieving results that surpass those of state-of-the-art distillation-based models. We systematically study the RL training process through extensive ablations and propose a simple yet effective approach: first training on math-only prompts, then on code-only prompts. Notably, we find that math-only RL not only significantly enhances the performance of strong distilled models on math benchmarks (e.g., +14.6% &#x2F; +17.2% on AIME 2025 for the 7B &#x2F; 14B models), but also code reasoning tasks (e.g., +6.8% &#x2F; +5.8% on LiveCodeBench for the 7B &#x2F; 14B models). In addition, extended code-only RL iterations further improve performance on code benchmarks with minimal or no degradation in math results. We develop a robust data curation pipeline to collect challenging prompts with high-quality, verifiable answers and test cases to enable verification-based RL across both domains. Finally, we identify key experimental insights, including curriculum learning with progressively increasing response lengths and the stabilizing effect of on-policy parameter updates. We find that RL not only elicits the foundational reasoning capabilities acquired during pretraining and supervised fine-tuning (e.g., distillation), but also pushes the limits of the modelâ€™s reasoning ability, enabling it to solve problems that were previously unsolvable. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘åœ¨å¤§å‹å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¨ç†æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†æ„å»ºé«˜æ€§èƒ½æ¨ç†æ¨¡å‹çš„è®­ç»ƒé…æ–¹ä»ç„¶ä¸æ˜ç¡®ã€‚å‰æ²¿æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1ï¼‰çš„å…³é”®å®ç°ç»†èŠ‚ï¼ŒåŒ…æ‹¬æ•°æ®æ”¶é›†ç­–ç•¥å’ŒRLè®­ç»ƒé…æ–¹ï¼Œé€šå¸¸è¢«çœç•¥ã€‚è€Œä¸”ï¼Œæœ€è¿‘æœ‰ç ”ç©¶è¡¨æ˜ï¼Œå¯¹äºå°å‹æ¨¡å‹ï¼Œè’¸é¦ä»ç„¶æ¯”RLæ›´æœ‰æ•ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯æ˜å¤§è§„æ¨¡RLå¯ä»¥æ˜¾ç€å¢å¼ºå¼ºå¤§ã€ä¸­å°å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå®ç°è¶…è¶ŠåŸºäºè’¸é¦çš„æœ€å…ˆè¿›æ¨¡å‹çš„ç»“æœã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†RLè®­ç»ƒè¿‡ç¨‹ï¼Œé€šè¿‡å¹¿æ³›çš„å®éªŒè¿›è¡Œæ¶ˆèç ”ç©¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼šé¦–å…ˆåœ¨åªæœ‰æ•°å­¦çš„æç¤ºä¸Šè¿›è¡Œè®­ç»ƒï¼Œç„¶ååœ¨åªæœ‰ä»£ç æç¤ºä¸Šè¿›è¡Œè®­ç»ƒã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°ä»…æ•°å­¦RLä¸ä»…æ˜¾ç€æé«˜äº†è’¸é¦æ¨¡å‹åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚AIME 2025çš„åŸºå‡†æµ‹è¯•æé«˜+14.6%&#x2F;+17.2%ï¼ˆå¯¹äºå¤§å‹æ¨¡å‹å’Œå·¨å¤§è§„æ¨¡æ¨¡å‹ï¼‰ï¼‰ä¸Šçš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨ä»£ç æ¨ç†ä»»åŠ¡ä¸Šä¹Ÿæœ‰è‰¯å¥½è¡¨ç°ï¼ˆä¾‹å¦‚LiveCodeBenchçš„åŸºå‡†æµ‹è¯•æé«˜+6.8%&#x2F; +5.8%ï¼‰ã€‚æ­¤å¤–ï¼Œé¢å¤–çš„ä»…ä»£ç RLè¿­ä»£è¿›ä¸€æ­¥æé«˜äº†ä»£ç åŸºå‡†æµ‹è¯•çš„æ€§èƒ½ï¼ŒåŒæ—¶å‡ ä¹ä¸ä¼šå¯¹æ•°å­¦ç»“æœé€ æˆä»»ä½•æŸå¤±ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¼ºå¤§çš„æ•°æ®æ”¶é›†ç®¡é“ï¼Œç”¨äºæ”¶é›†å…·æœ‰é«˜è´¨é‡å’Œå¯éªŒè¯ç­”æ¡ˆä»¥åŠæµ‹è¯•ç”¨ä¾‹çš„æŒ‘æˆ˜æ€§æç¤ºï¼Œä»¥å®ç°è·¨ä¸¤ä¸ªé¢†åŸŸçš„åŸºäºéªŒè¯çš„RLã€‚æœ€åï¼Œæˆ‘ä»¬è·å¾—äº†å…³é”®çš„å®éªŒè§è§£ï¼ŒåŒ…æ‹¬æ¸è¿›å¢åŠ å“åº”é•¿åº¦çš„è¯¾ç¨‹å­¦ä¹ å’Œåœ¨çº¿ç­–ç•¥å‚æ•°æ›´æ–°çš„ç¨³å®šæ•ˆæœã€‚æˆ‘ä»¬å‘ç°RLä¸ä»…æ¿€å‘äº†åœ¨é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒæœŸé—´è·å¾—çš„åŸºç¡€æ¨ç†èƒ½åŠ›ï¼ˆä¾‹å¦‚è’¸é¦ï¼‰ï¼Œè€Œä¸”è¿˜çªç ´äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æé™ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè§£å†³ä»¥å‰æ— æ³•è§£å†³çš„é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16400v1">PDF</a> We release the model at:   <a target="_blank" rel="noopener" href="https://huggingface.co/nvidia/AceReason-Nemotron-14B">https://huggingface.co/nvidia/AceReason-Nemotron-14B</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡æ¨ç†æ¨¡å‹æ€§èƒ½æ–¹é¢çš„åº”ç”¨ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå°½ç®¡æœ€è¿‘æœ‰æ‰€è¿›å±•ï¼Œä½†æ„å»ºé«˜æ€§èƒ½æ¨ç†æ¨¡å‹çš„è®­ç»ƒé…æ–¹ä»ç„¶ä¸æ˜ç¡®ã€‚é€šè¿‡å¯¹å‰æ²¿æ¨¡å‹å¦‚DeepSeek-R1çš„æ•°æ®æ”¶é›†å’ŒRLè®­ç»ƒç­–ç•¥çš„ç ”ç©¶ï¼Œå‘ç°å¤§è§„æ¨¡RLèƒ½æ˜¾è‘—æå‡å°å‹å’Œä¸­ç­‰è§„æ¨¡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶è¶…è¶Šç°æœ‰çš„åŸºäºè’¸é¦çš„æ¨¡å‹ã€‚æ–‡ç« è¿˜é€šè¿‡å¹¿æ³›çš„ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæœ‰æ•ˆçš„RLè®­ç»ƒè¿‡ç¨‹ï¼šé¦–å…ˆè¿›è¡Œæ•°å­¦æç¤ºè®­ç»ƒï¼Œç„¶åè¿›è¡Œä»£ç æç¤ºè®­ç»ƒã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†æ•°æ®æ”¶é›†å’Œå¤„ç†æµç¨‹ï¼Œä»¥åŠå…³é”®çš„å®éªŒæ´å¯Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ å¯ä»¥æ˜¾è‘—æå‡å°å‹å’Œä¸­ç­‰è§„æ¨¡æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>é€šè¿‡ç ”ç©¶å‰æ²¿æ¨¡å‹çš„å®æ–½ç»†èŠ‚ï¼Œå‘ç°è’¸é¦ç›¸æ¯”å¼ºåŒ–å­¦ä¹ å¯¹å°å‹æ¨¡å‹æ›´æœ‰æ•ˆã€‚</li>
<li>åœ¨RLè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå…ˆè¿›è¡Œæ•°å­¦æç¤ºè®­ç»ƒï¼Œå†è¿›è¡Œä»£ç æç¤ºè®­ç»ƒçš„æ–¹æ³•è¢«è¯æ˜æ˜¯ç®€å•è€Œæœ‰æ•ˆçš„ã€‚</li>
<li>æ•°å­¦æç¤ºè®­ç»ƒä¸ä»…èƒ½æå‡æ•°å­¦åŸºå‡†æµ‹è¯•æ€§èƒ½ï¼Œè¿˜èƒ½æé«˜ä»£ç æ¨ç†ä»»åŠ¡çš„è¡¨ç°ã€‚</li>
<li>é€šè¿‡æ‰©å±•ä»£ç æç¤ºè®­ç»ƒçš„RLè¿­ä»£ï¼Œå¯ä»¥åœ¨ä»£ç åŸºå‡†æµ‹è¯•ä¸Šè¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒæ•°å­¦ç»“æœçš„ç¨³å®šæ€§ã€‚</li>
<li>å¼€å‘äº†ç”¨äºæ”¶é›†å…·æœ‰æŒ‘æˆ˜æ€§æç¤ºå’Œé«˜è´¨é‡ç­”æ¡ˆçš„æ•°æ®æ”¶é›†å’Œå¤„ç†æµç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16400">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e014f0232f93fd71687c04484ed164b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b35960c6f525a755adc85f8c2562ee26.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SATURN-SAT-based-Reinforcement-Learning-to-Unleash-Language-Model-Reasoning"><a href="#SATURN-SAT-based-Reinforcement-Learning-to-Unleash-Language-Model-Reasoning" class="headerlink" title="SATURN: SAT-based Reinforcement Learning to Unleash Language Model   Reasoning"></a>SATURN: SAT-based Reinforcement Learning to Unleash Language Model   Reasoning</h2><p><strong>Authors:Huanyu Liu, Jia Li, Hao Zhu, Kechi Zhang, Yihong Dong, Ge Li</strong></p>
<p>How to design reinforcement learning (RL) tasks that effectively unleash the reasoning capability of large language models (LLMs) remains an open question. Existing RL tasks (e.g., math, programming, and constructing reasoning tasks) suffer from three key limitations: (1) Scalability. They rely heavily on human annotation or expensive LLM synthesis to generate sufficient training data. (2) Verifiability. LLMsâ€™ outputs are hard to verify automatically and reliably. (3) Controllable Difficulty. Most tasks lack fine-grained difficulty control, making it hard to train LLMs to develop reasoning ability from easy to hard.   To address these limitations, we propose Saturn, a SAT-based RL framework that uses Boolean Satisfiability (SAT) problems to train and evaluate LLM reasoning. Saturn enables scalable task construction, rule-based verification, and precise difficulty control. Saturn designs a curriculum learning pipeline that continuously improves LLMsâ€™ reasoning capability by constructing SAT tasks of increasing difficulty and training LLMs from easy to hard. To ensure stable training, we design a principled mechanism to control difficulty transitions.   We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying difficulty. It supports the evaluation of how LLM reasoning changes with problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of +14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g., AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in constructing RL tasks, Saturn achieves further improvements of +8.8%. We release the source code, data, and models to support future research. </p>
<blockquote>
<p>å¦‚ä½•è®¾è®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»»åŠ¡ï¼Œä»¥æœ‰æ•ˆåœ°é‡Šæ”¾å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚ç°æœ‰çš„RLä»»åŠ¡ï¼ˆä¾‹å¦‚æ•°å­¦ã€ç¼–ç¨‹å’Œæ„å»ºæ¨ç†ä»»åŠ¡ï¼‰å­˜åœ¨ä¸‰ä¸ªä¸»è¦å±€é™æ€§ï¼šï¼ˆ1ï¼‰å¯æ‰©å±•æ€§ã€‚å®ƒä»¬ä¸¥é‡ä¾èµ–äºäººå·¥æ³¨é‡Šæˆ–æ˜‚è´µçš„LLMåˆæˆæ¥ç”Ÿæˆè¶³å¤Ÿçš„è®­ç»ƒæ•°æ®ã€‚ï¼ˆ2ï¼‰å¯éªŒè¯æ€§ã€‚LLMçš„è¾“å‡ºéš¾ä»¥è‡ªåŠ¨å’Œå¯é åœ°éªŒè¯ã€‚ï¼ˆ3ï¼‰å¯æ§çš„å›°éš¾åº¦ã€‚å¤§å¤šæ•°ä»»åŠ¡ç¼ºä¹ç²¾ç»†çš„éš¾åº¦æ§åˆ¶ï¼Œå¾ˆéš¾è®­ç»ƒLLMä»ç®€å•åˆ°å¤æ‚åœ°å‘å±•æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºSATçš„RLæ¡†æ¶Saturnï¼Œå®ƒä½¿ç”¨å¸ƒå°”å¯æ»¡è¶³æ€§é—®é¢˜æ¥è®­ç»ƒå’Œè¯„ä¼°LLMæ¨ç†ã€‚Saturnæ”¯æŒå¯æ‰©å±•çš„ä»»åŠ¡æ„å»ºã€åŸºäºè§„åˆ™çš„éªŒè¯å’Œç²¾ç¡®çš„éš¾åº¦æ§åˆ¶ã€‚Saturnè®¾è®¡äº†ä¸€ä¸ªæŒç»­æé«˜LLMæ¨ç†èƒ½åŠ›çš„è¯¾ç¨‹å­¦ä¹ ç®¡é“ï¼Œé€šè¿‡æ„å»ºéš¾åº¦ä¸æ–­å¢åŠ çš„SATä»»åŠ¡ï¼Œä»ç®€å•åˆ°éš¾è®­ç»ƒLLMã€‚ä¸ºäº†ç¡®ä¿ç¨³å®šçš„è®­ç»ƒï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æœ‰åŸåˆ™çš„æœºåˆ¶æ¥æ§åˆ¶éš¾åº¦è¿‡æ¸¡ã€‚æˆ‘ä»¬ä»‹ç»äº†Saturn-2.6kæ•°æ®é›†ï¼ŒåŒ…å«2660ä¸ªéš¾åº¦ä¸åŒçš„SATé—®é¢˜ã€‚å®ƒæ”¯æŒè¯„ä¼°LLMæ¨ç†èƒ½åŠ›éšé—®é¢˜éš¾åº¦å˜åŒ–çš„æƒ…å†µã€‚æˆ‘ä»¬å°†Saturnåº”ç”¨äºDeepSeek-R1-Distill-Qwenï¼Œå¹¶è·å¾—äº†Saturn-1.5Bå’ŒSaturn-7Bã€‚æˆ‘ä»¬å–å¾—äº†å‡ ä¸ªæ˜¾è‘—çš„ç»“æœï¼šï¼ˆ1ï¼‰åœ¨SATé—®é¢˜ä¸Šï¼ŒSaturn-1.5Bå’ŒSaturn-7Bçš„å¹³å‡pass@3åˆ†åˆ«æé«˜äº†+14.0å’Œ+28.1ã€‚ï¼ˆ2ï¼‰åœ¨æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ä¸Šï¼ŒSaturn-1.5Bå’ŒSaturn-7Båœ¨åŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚AIMEã€LiveCodeBenchï¼‰ä¸Šçš„å¹³å‡åˆ†æ•°åˆ†åˆ«æé«˜äº†+4.9å’Œ+1.8ã€‚ï¼ˆ3ï¼‰ä¸æ„å»ºRLä»»åŠ¡çš„æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒSaturnå–å¾—äº†+8.8%çš„è¿›ä¸€æ­¥æ”¹è¿›ã€‚æˆ‘ä»¬å‘å¸ƒäº†æºä»£ç ã€æ•°æ®å’Œæ¨¡å‹ï¼Œä»¥æ”¯æŒæœªæ¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16368v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨å¦‚ä½•è®¾è®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»»åŠ¡ä»¥æœ‰æ•ˆé‡Šæ”¾å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚é’ˆå¯¹ç°æœ‰RLä»»åŠ¡å­˜åœ¨çš„å¯æ‰©å±•æ€§ã€å¯éªŒè¯æ€§å’Œå¯æ§éš¾åº¦ç­‰ä¸‰ä¸ªå…³é”®é™åˆ¶ï¼Œæå‡ºäº†åŸºäºSATçš„RLæ¡†æ¶Saturnã€‚Saturnåˆ©ç”¨å¸ƒå°”å¯æ»¡è¶³æ€§é—®é¢˜æ¥è®­ç»ƒå’Œè¯„ä¼°LLMæ¨ç†ï¼Œå®ç°äº†å¯æ‰©å±•çš„ä»»åŠ¡æ„å»ºã€åŸºäºè§„åˆ™çš„éªŒè¯å’Œç²¾ç¡®çš„éš¾åº¦æ§åˆ¶ã€‚Saturnè®¾è®¡äº†ä¸€æ¡æŒç»­æ”¹è¿›LLMæ¨ç†èƒ½åŠ›çš„è¯¾ç¨‹å­¦ä¹ ç®¡é“ï¼Œé€šè¿‡æ„å»ºéš¾åº¦é€’å¢çš„SATä»»åŠ¡ï¼Œä»æ˜“åˆ°éš¾è®­ç»ƒLLMã€‚æœ¬æ–‡è¿˜ä»‹ç»äº†Saturn-2.6kæ•°æ®é›†ï¼ŒåŒ…å«2660ä¸ªéš¾åº¦ä¸åŒçš„SATé—®é¢˜ï¼Œç”¨äºè¯„ä¼°LLMæ¨ç†èƒ½åŠ›éšé—®é¢˜éš¾åº¦çš„å˜åŒ–ã€‚åº”ç”¨Saturnæ¡†æ¶äºDeepSeek-R1-Distill-Qwenä»»åŠ¡ï¼Œå–å¾—äº†æ˜¾è‘—æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰RLä»»åŠ¡å­˜åœ¨å¯æ‰©å±•æ€§ã€å¯éªŒè¯æ€§å’Œå¯æ§éš¾åº¦ç­‰é™åˆ¶ã€‚</li>
<li>Saturnæ¡†æ¶åˆ©ç”¨å¸ƒå°”å¯æ»¡è¶³æ€§é—®é¢˜æ¥è®­ç»ƒå’Œè¯„ä¼°LLMæ¨ç†èƒ½åŠ›ã€‚</li>
<li>Saturnå®ç°äº†å¯æ‰©å±•çš„ä»»åŠ¡æ„å»ºã€åŸºäºè§„åˆ™çš„è‡ªåŠ¨éªŒè¯å’Œç²¾ç¡®çš„éš¾åº¦æ§åˆ¶ã€‚</li>
<li>Saturnè®¾è®¡äº†ä¸€æ¡æŒç»­æ”¹è¿›LLMæ¨ç†èƒ½åŠ›çš„è¯¾ç¨‹å­¦ä¹ ç®¡é“ã€‚</li>
<li>Saturn-2.6kæ•°æ®é›†ç”¨äºè¯„ä¼°LLMæ¨ç†èƒ½åŠ›éšé—®é¢˜éš¾åº¦çš„å˜åŒ–ã€‚</li>
<li>åº”ç”¨Saturnæ¡†æ¶äºç‰¹å®šä»»åŠ¡å–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
<li>Saturnæ¡†æ¶çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å·²å…¬å¼€ï¼Œä»¥æ”¯æŒæœªæ¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fe313a9fc3985833310ee882c6184530.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71dd3dcfaede2b855c4223894f74e3b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a457e4bff8b71b86738c58a40fea0a66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f308fdfd9dc7723916d5430b5acfcb4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6f89ef39d7d59842aa3cbcd10a07e76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc4d9573a86f33b61a1c858e608e811f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Embodied-Agents-Meet-Personalization-Exploring-Memory-Utilization-for-Personalized-Assistance"><a href="#Embodied-Agents-Meet-Personalization-Exploring-Memory-Utilization-for-Personalized-Assistance" class="headerlink" title="Embodied Agents Meet Personalization: Exploring Memory Utilization for   Personalized Assistance"></a>Embodied Agents Meet Personalization: Exploring Memory Utilization for   Personalized Assistance</h2><p><strong>Authors:Taeyoon Kwon, Dongwook Choi, Sunghwan Kim, Hyojun Kim, Seungjun Moon, Beong-woo Kwak, Kuan-Hao Huang, Jinyoung Yeo</strong></p>
<p>Embodied agents empowered by large language models (LLMs) have shown strong performance in household object rearrangement tasks. However, these tasks primarily focus on single-turn interactions with simplified instructions, which do not truly reflect the challenges of providing meaningful assistance to users. To provide personalized assistance, embodied agents must understand the unique semantics that users assign to the physical world (e.g., favorite cup, breakfast routine) by leveraging prior interaction history to interpret dynamic, real-world instructions. Yet, the effectiveness of embodied agents in utilizing memory for personalized assistance remains largely underexplored. To address this gap, we present MEMENTO, a personalized embodied agent evaluation framework designed to comprehensively assess memory utilization capabilities to provide personalized assistance. Our framework consists of a two-stage memory evaluation process design that enables quantifying the impact of memory utilization on task performance. This process enables the evaluation of agentsâ€™ understanding of personalized knowledge in object rearrangement tasks by focusing on its role in goal interpretation: (1) the ability to identify target objects based on personal meaning (object semantics), and (2) the ability to infer object-location configurations from consistent user patterns, such as routines (user patterns). Our experiments across various LLMs reveal significant limitations in memory utilization, with even frontier models like GPT-4o experiencing a 30.5% performance drop when required to reference multiple memories, particularly in tasks involving user patterns. These findings, along with our detailed analyses and case studies, provide valuable insights for future research in developing more effective personalized embodied agents. Project website: <a target="_blank" rel="noopener" href="https://connoriginal.github.io/MEMENTO">https://connoriginal.github.io/MEMENTO</a> </p>
<blockquote>
<p>ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èµ‹èƒ½çš„å®ä½“ä»£ç†äººåœ¨å®¶åº­ç‰©å“æ•´ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›ä»»åŠ¡ä¸»è¦å…³æ³¨å¸¦æœ‰ç®€åŒ–æŒ‡ä»¤çš„å•è½®äº¤äº’ï¼Œå¹¶ä¸èƒ½çœŸæ­£åæ˜ ä¸ºç”¨æˆ·æä¾›æœ‰æ„ä¹‰å¸®åŠ©çš„æŒ‘æˆ˜ã€‚ä¸ºäº†æä¾›ä¸ªæ€§åŒ–çš„å¸®åŠ©ï¼Œå®ä½“ä»£ç†äººå¿…é¡»åˆ©ç”¨ç”¨æˆ·èµ‹äºˆç‰©ç†ä¸–ç•Œçš„ç‹¬ç‰¹è¯­ä¹‰ï¼ˆå¦‚æœ€å–œæ¬¢çš„æ¯å­ã€æ—©é¤ä¾‹è¡Œç¨‹åºï¼‰ï¼Œå¹¶åˆ©ç”¨ä»¥å¾€äº¤äº’å†å²æ¥è§£è¯»åŠ¨æ€ã€ç°å®ä¸–ç•Œçš„æŒ‡ä»¤ã€‚ç„¶è€Œï¼Œå®ä½“ä»£ç†äººåœ¨åˆ©ç”¨è®°å¿†æä¾›ä¸ªæ€§åŒ–å¸®åŠ©æ–¹é¢çš„æœ‰æ•ˆæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå°šæœªè¢«æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MEMENTOï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ªæ€§åŒ–çš„å®ä½“ä»£ç†äººè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°åˆ©ç”¨è®°å¿†æä¾›ä¸ªæ€§åŒ–å¸®åŠ©çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç”±ä¸¤é˜¶æ®µè®°å¿†è¯„ä¼°æµç¨‹è®¾è®¡ç»„æˆï¼Œèƒ½å¤Ÿé‡åŒ–è®°å¿†åˆ©ç”¨å¯¹ä»»åŠ¡æ€§èƒ½çš„å½±å“ã€‚è¿™ä¸ªè¿‡ç¨‹é€šè¿‡å…³æ³¨è®°å¿†åœ¨ç›®æ ‡è§£è¯»ä¸­çš„ä½œç”¨ï¼Œæ¥è¯„ä¼°ä»£ç†äººåœ¨ç‰©å“æ•´ç†ä»»åŠ¡ä¸­å¯¹ä¸ªæ€§åŒ–çŸ¥è¯†çš„ç†è§£èƒ½åŠ›ï¼šï¼ˆ1ï¼‰åŸºäºä¸ªäººæ„ä¹‰ï¼ˆå¯¹è±¡è¯­ä¹‰ï¼‰è¯†åˆ«ç›®æ ‡å¯¹è±¡çš„èƒ½åŠ›ï¼›ï¼ˆ2ï¼‰ä»ä¸€è‡´çš„ç”¨æˆ·æ¨¡å¼ä¸­æ¨æ–­å¯¹è±¡ä½ç½®é…ç½®çš„èƒ½åŠ›ï¼Œä¾‹å¦‚ä¾‹è¡Œç¨‹åºï¼ˆç”¨æˆ·æ¨¡å¼ï¼‰ã€‚æˆ‘ä»¬åœ¨å„ç§LLMä¸Šçš„å®éªŒæ­ç¤ºäº†è®°å¿†åˆ©ç”¨çš„æ˜¾è‘—å±€é™æ€§ï¼Œç”šè‡³åƒGPT-4oè¿™æ ·çš„å‰æ²¿æ¨¡å‹åœ¨éœ€è¦å‚è€ƒå¤šæ¡è®°å¿†æ—¶æ€§èƒ½ä¸‹é™30.5%ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠç”¨æˆ·æ¨¡å¼çš„ä»»åŠ¡ä¸­ã€‚è¿™äº›å‘ç°ï¼Œä»¥åŠæˆ‘ä»¬çš„è¯¦ç»†åˆ†æå’Œæ¡ˆä¾‹ç ”ç©¶ï¼Œä¸ºæœªæ¥ç ”ç©¶å¼€å‘æ›´æœ‰æ•ˆçš„ä¸ªæ€§åŒ–å®ä½“ä»£ç†äººæä¾›äº†å®è´µçš„è§è§£ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://connoriginal.github.io/MEMENTO">https://connoriginal.github.io/MEMENTO</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16348v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†èµ‹èƒ½å¤§å‹è¯­è¨€æ¨¡å‹çš„å®ä½“ä»£ç†åœ¨å®¶åº­ç‰©ä½“æ’åˆ—ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œå½“å‰çš„ä»»åŠ¡ä¸»è¦å…³æ³¨ç®€åŒ–æŒ‡ä»¤ä¸‹çš„å•å›åˆäº¤äº’ï¼Œå¹¶ä¸èƒ½çœŸæ­£åæ˜ ä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–æœåŠ¡çš„æŒ‘æˆ˜ã€‚å®ä½“ä»£ç†éœ€è¦åˆ©ç”¨ç”¨æˆ·èµ‹äºˆçš„ç‰©ç†ä¸–ç•Œç‹¬ç‰¹è¯­ä¹‰ï¼Œé€šè¿‡ä»¥å¾€äº¤äº’å†å²ç†è§£ä¸ªæ€§åŒ–çŸ¥è¯†ï¼Œä»¥è§£é‡ŠåŠ¨æ€ã€ç°å®ä¸–ç•Œä¸­çš„æŒ‡ä»¤ã€‚ä½†å®ä½“ä»£ç†åœ¨åˆ©ç”¨è®°å¿†æä¾›ä¸ªæ€§åŒ–æœåŠ¡æ–¹é¢çš„æœ‰æ•ˆæ€§å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€ä¸è¶³ï¼Œæœ¬æ–‡æå‡ºäº†MEMENTOè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°å®ä½“ä»£ç†åˆ©ç”¨è®°å¿†æä¾›ä¸ªæ€§åŒ–æœåŠ¡çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªä¸¤é˜¶æ®µè®°å¿†è¯„ä¼°è¿‡ç¨‹ï¼Œèƒ½å¤Ÿé‡åŒ–è®°å¿†åˆ©ç”¨å¯¹ä»»åŠ¡æ€§èƒ½çš„å½±å“ï¼Œå¹¶é€šè¿‡ç›®æ ‡è§£è¯»è¯„ä¼°ä»£ç†å¯¹ä¸ªæ€§åŒ–çŸ¥è¯†çš„ç†è§£èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œä¸åŒå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®°å¿†åˆ©ç”¨æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™ï¼Œå³ä½¿æ˜¯å‰æ²¿æ¨¡å‹å¦‚GPT-4oåœ¨æ¶‰åŠç”¨æˆ·æ¨¡å¼çš„å¤šè®°å¿†å‚è€ƒä»»åŠ¡ä¸­ï¼Œæ€§èƒ½ä¹Ÿä¼šä¸‹é™30.5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®ä½“ä»£ç†åœ¨å®¶åº­ç‰©ä½“æ’åˆ—ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å½“å‰ä»»åŠ¡æœªèƒ½çœŸæ­£åæ˜ ä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–æœåŠ¡çš„æŒ‘æˆ˜ã€‚</li>
<li>å®ä½“ä»£ç†éœ€è¦åˆ©ç”¨ç”¨æˆ·ä¸ç‰©ç†ä¸–ç•Œçš„ç‹¬ç‰¹è¯­ä¹‰äº¤äº’ä»¥åŠè¿‡å»çš„äº¤äº’å†å²æ¥ç†è§£ä¸ªæ€§åŒ–çŸ¥è¯†ã€‚</li>
<li>MEMENTOè¯„ä¼°æ¡†æ¶ç”¨äºå…¨é¢è¯„ä¼°å®ä½“ä»£ç†åœ¨åˆ©ç”¨è®°å¿†æä¾›ä¸ªæ€§åŒ–æœåŠ¡æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>ä¸¤é˜¶æ®µè®°å¿†è¯„ä¼°è¿‡ç¨‹èƒ½å¤Ÿé‡åŒ–è®°å¿†åˆ©ç”¨å¯¹ä»»åŠ¡æ€§èƒ½çš„å½±å“ï¼Œå¹¶è¯„ä¼°ä»£ç†å¯¹ä¸ªæ€§åŒ–çŸ¥è¯†çš„ç†è§£èƒ½åŠ›ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®°å¿†åˆ©ç”¨æ–¹é¢å­˜åœ¨å±€é™ï¼Œå°¤å…¶æ˜¯æ¶‰åŠç”¨æˆ·æ¨¡å¼çš„å¤šè®°å¿†å‚è€ƒä»»åŠ¡ã€‚</li>
<li>å‰æ²¿æ¨¡å‹å¦‚GPT-4oåœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸‹é™æ˜¾è‘—ã€‚</li>
<li>ç ”ç©¶ä¸ºå¼€å‘æ›´æœ‰æ•ˆçš„ä¸ªæ€§åŒ–å®ä½“ä»£ç†æä¾›äº†å®è´µè§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16348">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-595e052b9d053cd433f64a1c60b8dbea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4af9c0e019ec8eeb68c49ceb235fa58a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-834ed23c654082c86b0f628e48681b1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b9428e2d3e9374c2f1b0aeea4bc13ea.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ARPO-End-to-End-Policy-Optimization-for-GUI-Agents-with-Experience-Replay"><a href="#ARPO-End-to-End-Policy-Optimization-for-GUI-Agents-with-Experience-Replay" class="headerlink" title="ARPO:End-to-End Policy Optimization for GUI Agents with Experience   Replay"></a>ARPO:End-to-End Policy Optimization for GUI Agents with Experience   Replay</h2><p><strong>Authors:Fanbin Lu, Zhisheng Zhong, Shu Liu, Chi-Wing Fu, Jiaya Jia</strong></p>
<p>Training large language models (LLMs) as interactive agents for controlling graphical user interfaces (GUIs) presents a unique challenge to optimize long-horizon action sequences with multimodal feedback from complex environments. While recent works have advanced multi-turn reinforcement learning (RL) for reasoning and tool-using capabilities in LLMs, their application to GUI-based agents remains relatively underexplored due to the difficulty of sparse rewards, delayed feedback, and high rollout costs. In this paper, we investigate end-to-end policy optimization for vision-language-based GUI agents with the aim of improving performance on complex, long-horizon computer tasks. We propose Agentic Replay Policy Optimization (ARPO), an end-to-end RL approach that augments Group Relative Policy Optimization (GRPO) with a replay buffer to reuse the successful experience across training iterations. To further stabilize the training process, we propose a task selection strategy that filters tasks based on baseline agent performance, allowing the agent to focus on learning from informative interactions. Additionally, we compare ARPO with offline preference optimization approaches, highlighting the advantages of policy-based methods in GUI environments. Experiments on the OSWorld benchmark demonstrate that ARPO achieves competitive results, establishing a new performance baseline for LLM-based GUI agents trained via reinforcement learning. Our findings underscore the effectiveness of reinforcement learning for training multi-turn, vision-language GUI agents capable of managing complex real-world UI interactions. Codes and models:<a target="_blank" rel="noopener" href="https://github.com/dvlab-research/ARPO.git">https://github.com/dvlab-research/ARPO.git</a>. </p>
<blockquote>
<p>è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ§åˆ¶å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰çš„äº’åŠ¨ä»£ç†ï¼Œé¢ä¸´ç€ä¼˜åŒ–æ¥è‡ªå¤æ‚ç¯å¢ƒçš„å¤šæ¨¡å¼åé¦ˆçš„é•¿æœŸè¡ŒåŠ¨åºåˆ—çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚å°½ç®¡æœ€è¿‘çš„ç ”ç©¶å·²ç»æ¨åŠ¨äº†LLMä¸­çš„å¤šå›åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç”¨äºæ¨ç†å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œä½†å…¶åœ¨GUIä»£ç†ä¸­çš„åº”ç”¨ç”±äºç¨€ç–å¥–åŠ±ã€å»¶è¿Ÿåé¦ˆå’Œé«˜æ»šåŠ¨æˆæœ¬è€Œç›¸å¯¹æœªè¢«å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†é¢å‘è§†è§‰è¯­è¨€åŸºç¡€çš„GUIä»£ç†ç«¯åˆ°ç«¯ç­–ç•¥ä¼˜åŒ–ï¼Œæ—¨åœ¨æé«˜åœ¨å¤æ‚çš„é•¿æœŸè®¡ç®—æœºä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†Agentic Replay Policy Optimizationï¼ˆARPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„RLæ–¹æ³•ï¼Œå®ƒé€šè¿‡å¢åŠ ä¸€ä¸ªå›æ”¾ç¼“å†²åŒºæ¥å¢å¼ºGroup Relative Policy Optimizationï¼ˆGRPOï¼‰ï¼Œä»¥é‡æ–°ä½¿ç”¨æˆåŠŸçš„ç»éªŒè¿›è¡Œè·¨è®­ç»ƒè¿­ä»£çš„å­¦ä¹ ã€‚ä¸ºäº†è¿›ä¸€æ­¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»»åŠ¡é€‰æ‹©ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æ ¹æ®åŸºçº¿ä»£ç†æ€§èƒ½è¿‡æ»¤ä»»åŠ¡ï¼Œä½¿ä»£ç†èƒ½å¤Ÿä¸“æ³¨äºä»ä¿¡æ¯ä¸°å¯Œçš„äº¤äº’ä¸­å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†ARPOä¸ç¦»çº¿åå¥½ä¼˜åŒ–æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼Œçªå‡ºäº†åŸºäºç­–ç•¥çš„æ–¹æ³•åœ¨GUIç¯å¢ƒä¸­çš„ä¼˜åŠ¿ã€‚åœ¨OSWorldåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒARPOå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œä¸ºé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„LLM GUIä»£ç†å»ºç«‹äº†æ–°çš„æ€§èƒ½åŸºå‡†ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†å¼ºåŒ–å­¦ä¹ åœ¨è®­ç»ƒèƒ½å¤Ÿè¿›è¡Œå¤æ‚ç°å®ä¸–ç•ŒUIäº¤äº’çš„å¤šå›åˆè§†è§‰è¯­è¨€GUIä»£ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å’Œæ¨¡å‹åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/dvlab-research/ARPO.git%E3%80%82">https://github.com/dvlab-research/ARPO.gitã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16282v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰çš„äº’åŠ¨ä»£ç†é¢ä¸´ä¼˜åŒ–é•¿å‘¨æœŸåŠ¨ä½œåºåˆ—çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºAgentic Replay Policy Optimization (ARPO)æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„åœ¨å¤æ‚é•¿å‘¨æœŸè®¡ç®—æœºä»»åŠ¡ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚é‡‡ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰å¹¶ç»“åˆå›æ”¾ç¼“å­˜ï¼Œä½¿æˆåŠŸç»éªŒèƒ½å¤Ÿè·¨è®­ç»ƒè¿­ä»£è¿›è¡Œé‡ç”¨ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†åŸºäºåŸºçº¿ä»£ç†æ€§èƒ½çš„ä»»åŠ¡ç­›é€‰ç­–ç•¥ï¼Œè®©ä»£ç†èšç„¦äºå­¦ä¹ æœ‰ç”¨çš„äº¤äº’å†…å®¹ã€‚å®éªŒè¯æ˜ï¼ŒARPOåœ¨OSWorldåŸºå‡†æµ‹è¯•ä¸Šå–å¾—ç«äº‰ä¼˜åŠ¿ï¼Œå»ºç«‹äº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹GUIä»£ç†çš„æ–°æ€§èƒ½åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰çš„äº’åŠ¨ä»£ç†é¢ä¸´ä¼˜åŒ–é•¿å‘¨æœŸåŠ¨ä½œåºåˆ—çš„æŒ‘æˆ˜ã€‚</li>
<li>ARPOæ˜¯ä¸€ç§ç”¨äºç«¯åˆ°ç«¯ç­–ç•¥ä¼˜åŒ–çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜LLMåœ¨å¤æ‚è®¡ç®—æœºä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>ARPOç»“åˆäº†Group Relative Policy Optimizationï¼ˆGRPOï¼‰å’Œå›æ”¾ç¼“å­˜æŠ€æœ¯ï¼Œä»¥é‡ç”¨æˆåŠŸç»éªŒå¹¶ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡åŸºäºåŸºçº¿ä»£ç†æ€§èƒ½çš„ä»»åŠ¡ç­›é€‰ç­–ç•¥ï¼ŒARPOä½¿ä»£ç†èšç„¦äºå­¦ä¹ æœ‰ç”¨çš„äº¤äº’å†…å®¹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b129e7a82cda4a2dacb1b8c95f8df89f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6accefc586d553af290c23fd2560b8b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Think-RM-Enabling-Long-Horizon-Reasoning-in-Generative-Reward-Models"><a href="#Think-RM-Enabling-Long-Horizon-Reasoning-in-Generative-Reward-Models" class="headerlink" title="Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models"></a>Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models</h2><p><strong>Authors:Ilgee Hong, Changlong Yu, Liang Qiu, Weixiang Yan, Zhenghao Xu, Haoming Jiang, Qingru Zhang, Qin Lu, Xin Liu, Chao Zhang, Tuo Zhao</strong></p>
<p>Reinforcement learning from human feedback (RLHF) has become a powerful post-training paradigm for aligning large language models with human preferences. A core challenge in RLHF is constructing accurate reward signals, where the conventional Bradley-Terry reward models (BT RMs) often suffer from sensitivity to data size and coverage, as well as vulnerability to reward hacking. Generative reward models (GenRMs) offer a more robust alternative by generating chain-of-thought (CoT) rationales followed by a final reward. However, existing GenRMs rely on shallow, vertically scaled reasoning, limiting their capacity to handle nuanced or complex (e.g., reasoning-intensive) tasks. Moreover, their pairwise preference outputs are incompatible with standard RLHF algorithms that require pointwise reward signals. In this work, we introduce Think-RM, a training framework that enables long-horizon reasoning in GenRMs by modeling an internal thinking process. Rather than producing structured, externally provided rationales, Think-RM generates flexible, self-guided reasoning traces that support advanced capabilities such as self-reflection, hypothetical reasoning, and divergent reasoning. To elicit these reasoning abilities, we first warm-up the models by supervised fine-tuning (SFT) over long CoT data. We then further improve the modelâ€™s long-horizon abilities by rule-based reinforcement learning (RL). In addition, we propose a novel pairwise RLHF pipeline that directly optimizes policies using pairwise preference rewards, eliminating the need for pointwise reward conversion and enabling more effective use of Think-RM outputs. Experiments show that Think-RM achieves state-of-the-art results on RM-Bench, outperforming both BT RM and vertically scaled GenRM by 8%. When combined with our pairwise RLHF pipeline, it demonstrates superior end-policy performance compared to traditional approaches. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²ç»æˆä¸ºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½çš„å¼ºå¤§åè®­ç»ƒèŒƒå¼ã€‚RLHFçš„æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºæ„å»ºå‡†ç¡®çš„å¥–åŠ±ä¿¡å·ï¼Œä¼ ç»Ÿçš„Bradley-Terryå¥–åŠ±æ¨¡å‹ï¼ˆBT RMï¼‰é€šå¸¸å—åˆ°æ•°æ®å¤§å°å’Œè¦†ç›–èŒƒå›´çš„æ•æ„Ÿæ€§å½±å“ï¼ŒåŒæ—¶ä¹Ÿå®¹æ˜“å—åˆ°å¥–åŠ±é»‘å®¢çš„æ”»å‡»ã€‚ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆGenRMï¼‰é€šè¿‡ç”Ÿæˆæ€è€ƒé“¾ï¼ˆCoTï¼‰ç†ç”±å’Œæœ€ç»ˆå¥–åŠ±ï¼Œæä¾›äº†ä¸€ç§æ›´ç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„GenRMsä¾èµ–äºæµ…å±‚ã€å‚ç›´æ‰©å±•çš„æ¨ç†ï¼Œé™åˆ¶äº†å®ƒä»¬å¤„ç†å¾®å¦™æˆ–å¤æ‚ï¼ˆä¾‹å¦‚ï¼Œæ¨ç†å¯†é›†å‹ï¼‰ä»»åŠ¡çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå®ƒä»¬çš„æˆå¯¹åå¥½è¾“å‡ºä¸éœ€è¦ç‚¹å¯¹ç‚¹å¥–åŠ±ä¿¡å·çš„æ ‡å‡†RLHFç®—æ³•ä¸å…¼å®¹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†Think-RMï¼Œä¸€ä¸ªè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡åœ¨ç”Ÿæˆå¥–åŠ±æ¨¡å‹ä¸­å»ºç«‹å†…éƒ¨æ€è€ƒè¿‡ç¨‹ï¼Œå®ç°é•¿æœŸæ¨ç†èƒ½åŠ›ã€‚Think-RMä¸æ˜¯äº§ç”Ÿç»“æ„åŒ–çš„ã€å¤–éƒ¨æä¾›çš„ç†ç”±ï¼Œè€Œæ˜¯ç”Ÿæˆçµæ´»ã€è‡ªæˆ‘å¼•å¯¼çš„æ¨ç†è½¨è¿¹ï¼Œæ”¯æŒé«˜çº§èƒ½åŠ›ï¼Œå¦‚è‡ªæˆ‘åæ€ã€å‡è®¾æ¨ç†å’Œå‘æ•£æ¨ç†ã€‚ä¸ºäº†æ¿€å‘è¿™äº›æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨é•¿CoTæ•°æ®ä¸Šè¿›è¡Œæ¨¡å‹é¢„çƒ­ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›ä¸€æ­¥æ”¹å–„æ¨¡å‹çš„é•¿è¿œèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„é…å¯¹RLHFç®¡é“ï¼Œè¯¥ç®¡é“ç›´æ¥ä½¿ç”¨é…å¯¹åå¥½å¥–åŠ±æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œæ¶ˆé™¤äº†å¯¹ç‚¹å¯¹ç‚¹å¥–åŠ±è½¬æ¢çš„éœ€æ±‚ï¼Œå¹¶èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨Think-RMè¾“å‡ºã€‚å®éªŒè¡¨æ˜ï¼ŒThink-RMåœ¨RM-Benchä¸Šå–å¾—äº†æœ€æ–°æˆæœï¼Œä¸ä¼ ç»Ÿçš„BT RMå’Œå‚ç›´æ‰©å±•çš„GenRMç›¸æ¯”ï¼Œæ€§èƒ½æé«˜äº†8%ã€‚å½“ä¸æˆ‘ä»¬çš„é…å¯¹RLHFç®¡é“ç»“åˆæ—¶ï¼Œå®ƒæœ€ç»ˆç­–ç•¥çš„ç»©æ•ˆè¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16265v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé’ˆå¯¹æ„å»ºç²¾å‡†å¥–åŠ±ä¿¡å·çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå‡ºç°äº†ä¸€ä¸ªæ–°çš„é—®é¢˜ï¼šç°æœ‰æ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼Œå¯¹æ•°æ®å’Œå¥–åŠ±æ“çºµæ•æ„Ÿä¸”å¤„ç†å¤æ‚ä»»åŠ¡èƒ½åŠ›æœ‰é™ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†Think-RMè®­ç»ƒæ¡†æ¶ï¼Œå®ƒé€šè¿‡æ¨¡æ‹Ÿå†…éƒ¨æ€è€ƒè¿‡ç¨‹å®ç°é•¿æœŸè§„åˆ’æ¨ç†ã€‚ç»è¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åï¼ŒThink-RMç”Ÿæˆçµæ´»çš„è‡ªæˆ‘å¼•å¯¼æ¨ç†è½¨è¿¹ï¼Œæ”¯æŒé«˜çº§èƒ½åŠ›å¦‚è‡ªæˆ‘åæ€ã€å‡è®¾æ¨ç†å’Œå‘æ•£æ¨ç†ç­‰ã€‚æœ¬ç ”ç©¶è¿˜æå‡ºäº†æ–°å‹RLHFç®¡é“ï¼Œç›´æ¥ä½¿ç”¨é…å¯¹åå¥½å¥–åŠ±ä¼˜åŒ–ç­–ç•¥ï¼Œæ— éœ€è½¬æ¢ç‚¹å¯¹å¥–åŠ±ï¼Œä½¿Think-RMè¾“å‡ºæ›´åŠ é«˜æ•ˆã€‚å®éªŒè¡¨æ˜ï¼ŒThink-RMåœ¨RM-Benchä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ä»¥ç¬¦åˆäººç±»åå¥½çš„é‡è¦æ–¹æ³•ã€‚</li>
<li>æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºæ„å»ºå‡†ç¡®çš„å¥–åŠ±ä¿¡å·æ¨¡å‹ï¼Œä¼ ç»Ÿçš„Bradley-Terryå¥–åŠ±æ¨¡å‹å­˜åœ¨æ•°æ®è§„æ¨¡æ•æ„Ÿæ€§å’Œå¥–åŠ±æ“çºµçš„è„†å¼±æ€§ç­‰é—®é¢˜ã€‚</li>
<li>ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆGenRMsï¼‰æä¾›äº†æ›´ç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å®ƒä»¬å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›æœ‰é™ä¸”ä¾èµ–å‚ç›´æ¯”ä¾‹æ¨ç†ã€‚</li>
<li>Think-RMè®­ç»ƒæ¡†æ¶å¼•å…¥å»ºæ¨¡å†…éƒ¨æ€è€ƒè¿‡ç¨‹çš„æ¦‚å¿µï¼Œæ”¯æŒé«˜çº§èƒ½åŠ›å¦‚è‡ªæˆ‘åæ€ã€å‡è®¾æ¨ç†å’Œå‘æ•£æ¨ç†ç­‰ã€‚</li>
<li>é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œæé«˜äº†æ¨¡å‹çš„é•¿æœŸè§„åˆ’èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„RLHFç®¡é“ï¼Œå¯ç›´æ¥ä¼˜åŒ–ä½¿ç”¨é…å¯¹åå¥½å¥–åŠ±çš„ç­–ç•¥ï¼Œä¸Think-RMè¾“å‡ºå…¼å®¹ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16265">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e8c01ffae928a26ce157a10407d35795.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1639a2f0201b16668871556d9ac79e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d4234c8c085f891b51a4932232441c8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Realistic-Evaluation-of-TabPFN-v2-in-Open-Environments"><a href="#Realistic-Evaluation-of-TabPFN-v2-in-Open-Environments" class="headerlink" title="Realistic Evaluation of TabPFN v2 in Open Environments"></a>Realistic Evaluation of TabPFN v2 in Open Environments</h2><p><strong>Authors:Zi-Jian Cheng, Zi-Yi Jia, Zhi Zhou, Yu-Feng Li, Lan-Zhe Guo</strong></p>
<p>Tabular data, owing to its ubiquitous presence in real-world domains, has garnered significant attention in machine learning research. While tree-based models have long dominated tabular machine learning tasks, the recently proposed deep learning model TabPFN v2 has emerged, demonstrating unparalleled performance and scalability potential. Although extensive research has been conducted on TabPFN v2 to further improve performance, the majority of this research remains confined to closed environments, neglecting the challenges that frequently arise in open environments. This raises the question: Can TabPFN v2 maintain good performance in open environments? To this end, we conduct the first comprehensive evaluation of TabPFN v2â€™s adaptability in open environments. We construct a unified evaluation framework covering various real-world challenges and assess the robustness of TabPFN v2 under open environments scenarios using this framework. Empirical results demonstrate that TabPFN v2 shows significant limitations in open environments but is suitable for small-scale, covariate-shifted, and class-balanced tasks. Tree-based models remain the optimal choice for general tabular tasks in open environments. To facilitate future research on open environments challenges, we advocate for open environments tabular benchmarks, multi-metric evaluation, and universal modules to strengthen model robustness. We publicly release our evaluation framework at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/tabpfn-ood-4E65">https://anonymous.4open.science/r/tabpfn-ood-4E65</a>. </p>
<blockquote>
<p>ç”±äºè¡¨æ ¼æ•°æ®åœ¨ç°å®ä¸–ç•Œä¸­æ— å¤„ä¸åœ¨ï¼Œå› æ­¤åœ¨æœºå™¨å­¦ä¹ ç ”ç©¶ä¸­å¼•èµ·äº†æå¤§çš„å…³æ³¨ã€‚å°½ç®¡åŸºäºæ ‘çš„æ¨¡å‹é•¿æœŸä»¥æ¥ä¸€ç›´ä¸»å¯¼ç€è¡¨æ ¼æœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œä½†æœ€è¿‘æå‡ºçš„æ·±åº¦å­¦ä¹ æ¨¡å‹TabPFN v2å·²ç»å‡ºç°ï¼Œè¡¨ç°å‡ºäº†æ— ä¸ä¼¦æ¯”çš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§æ½œåŠ›ã€‚å°½ç®¡å·²ç»å¯¹TabPFN v2è¿›è¡Œäº†å¤§é‡ç ”ç©¶ä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œä½†å¤§å¤šæ•°ç ”ç©¶ä»ç„¶å±€é™äºå°é—­ç¯å¢ƒï¼Œå¿½è§†äº†åœ¨å¼€æ”¾ç¯å¢ƒä¸­ç»å¸¸å‡ºç°çš„æŒ‘æˆ˜ã€‚è¿™å¼•å‘äº†ä»¥ä¸‹é—®é¢˜ï¼šTabPFN v2èƒ½åœ¨å¼€æ”¾ç¯å¢ƒä¸­ä¿æŒè‰¯å¥½çš„æ€§èƒ½å—ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯¹TabPFN v2åœ¨å¼€æ”¾ç¯å¢ƒä¸­çš„é€‚åº”æ€§è¿›è¡Œäº†é¦–æ¬¡å…¨é¢è¯„ä¼°ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–äº†å„ç§ç°å®ä¸–ç•Œçš„æŒ‘æˆ˜ï¼Œå¹¶ä½¿ç”¨è¯¥æ¡†æ¶è¯„ä¼°äº†TabPFN v2åœ¨å¼€æ”¾ç¯å¢ƒåœºæ™¯ä¸‹çš„ç¨³å¥æ€§ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒTabPFN v2åœ¨å¼€æ”¾ç¯å¢ƒä¸­å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œä½†é€‚ç”¨äºå°è§„æ¨¡ã€åå˜é‡åç§»å’Œç±»åˆ«å¹³è¡¡çš„ä»»åŠ¡ã€‚åœ¨å¼€æ”¾ç¯å¢ƒçš„é€šç”¨è¡¨æ ¼ä»»åŠ¡ä¸­ï¼ŒåŸºäºæ ‘çš„æ¨¡å‹ä»ç„¶æ˜¯æœ€ä½³é€‰æ‹©ã€‚ä¸ºäº†ä¿ƒè¿›æœªæ¥å¯¹å¼€æ”¾ç¯å¢ƒæŒ‘æˆ˜çš„ç ”ç©¶ï¼Œæˆ‘ä»¬æå€¡å»ºç«‹å¼€æ”¾ç¯å¢ƒè¡¨æ ¼åŸºå‡†æµ‹è¯•ã€å¤šæŒ‡æ ‡è¯„ä¼°å’Œé€šç”¨æ¨¡å—ä»¥å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒæˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/tabpfn-ood-4E65%E3%80%82">https://anonymous.4open.science/r/tabpfn-ood-4E65ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16226v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨æ·±åº¦å­¦ä¹ æ¨¡å‹TabPFN v2åœ¨å¼€æ”¾ç¯å¢ƒä¸‹çš„é€‚åº”æ€§ç ”ç©¶ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå°½ç®¡TabPFN v2åœ¨å°é—­ç¯å¢ƒä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨çœŸå®ä¸–ç•Œçš„å¼€æ”¾ç¯å¢ƒä¸‹å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚é€šè¿‡æ„å»ºç»Ÿä¸€è¯„ä¼°æ¡†æ¶å¯¹å„ç§ç°å®æŒ‘æˆ˜è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°æ ‘æ¨¡å‹ä»æ˜¯å¼€æ”¾ç¯å¢ƒä¸‹ä¸€èˆ¬è¡¨æ ¼ä»»åŠ¡çš„æœ€ä½³é€‰æ‹©ã€‚ä¸ºä¿ƒè¿›å¯¹å¼€æ”¾ç¯å¢ƒæŒ‘æˆ˜çš„ç ”ç©¶ï¼Œæ–‡ç« æå€¡å»ºç«‹å¼€æ”¾ç¯å¢ƒè¡¨æ ¼åŸºå‡†æµ‹è¯•ã€å¤šæŒ‡æ ‡è¯„ä¼°å’Œé€šç”¨æ¨¡å—ä»¥å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TabPFN v2åœ¨æœºå™¨å­¦ä¹ ä¸­ä¸»è¦ç”¨äºå¤„ç†è¡¨æ ¼æ•°æ®ï¼Œå°½ç®¡åœ¨å°é—­ç¯å¢ƒä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¼€æ”¾ç¯å¢ƒä¸‹å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å¼€æ”¾ç¯å¢ƒå¯¹TabPFN v2çš„æŒ‘æˆ˜åŒ…æ‹¬å„ç§ç°å®ä¸–ç•Œçš„å¤æ‚æ€§å’Œå˜åŒ–æ€§ã€‚</li>
<li>é€šè¿‡æ„å»ºç»Ÿä¸€è¯„ä¼°æ¡†æ¶å¯¹TabPFN v2åœ¨å¼€æ”¾ç¯å¢ƒä¸‹çš„ç¨³å¥æ€§è¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>å®è¯ç»“æœè¡¨æ˜ï¼ŒTabPFN v2åœ¨å°è§„æ¨¡ã€åå˜é‡åç§»å’Œç±»åˆ«å¹³è¡¡çš„ä»»åŠ¡ä¸­è¡¨ç°è¾ƒå¥½ã€‚</li>
<li>åœ¨å¼€æ”¾ç¯å¢ƒä¸‹ï¼Œæ ‘æ¨¡å‹ä»ç„¶æ˜¯å¤„ç†ä¸€èˆ¬è¡¨æ ¼ä»»åŠ¡çš„æœ€ä½³é€‰æ‹©ã€‚</li>
<li>ä¸ºä¿ƒè¿›å¯¹å¼€æ”¾ç¯å¢ƒæŒ‘æˆ˜çš„ç ”ç©¶ï¼Œå»ºè®®å»ºç«‹å¼€æ”¾ç¯å¢ƒè¡¨æ ¼åŸºå‡†æµ‹è¯•ã€å¤šæŒ‡æ ‡è¯„ä»·å’Œé€šç”¨æ¨¡å—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16226">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6f2eb83456954c2e8d7fbe0a2b077ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cd7918785eafecd31de8f187072e422.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9766c224b18741b8081e21f9b9674550.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="AudioTrust-Benchmarking-the-Multifaceted-Trustworthiness-of-Audio-Large-Language-Models"><a href="#AudioTrust-Benchmarking-the-Multifaceted-Trustworthiness-of-Audio-Large-Language-Models" class="headerlink" title="AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large   Language Models"></a>AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large   Language Models</h2><p><strong>Authors:Kai Li, Can Shen, Yile Liu, Jirui Han, Kelong Zheng, Xuechao Zou, Zhe Wang, Xingjian Du, Shun Zhang, Hanjun Luo, Yingbin Jin, Xinxin Xing, Ziyang Ma, Yue Liu, Xiaojun Jia, Yifan Zhang, Junfeng Fang, Kun Wang, Yibo Yan, Haoyang Li, Yiming Li, Xiaobin Zhuang, Yang Liu, Haibo Hu, Zhuo Chen, Zhizheng Wu, Xiaolin Hu, Eng-Siong Chng, XiaoFeng Wang, Wenyuan Xu, Wei Dong, Xinfeng Li</strong></p>
<p>The rapid advancement and expanding applications of Audio Large Language Models (ALLMs) demand a rigorous understanding of their trustworthiness. However, systematic research on evaluating these models, particularly concerning risks unique to the audio modality, remains largely unexplored. Existing evaluation frameworks primarily focus on the text modality or address only a restricted set of safety dimensions, failing to adequately account for the unique characteristics and application scenarios inherent to the audio modality. We introduce AudioTrust-the first multifaceted trustworthiness evaluation framework and benchmark specifically designed for ALLMs. AudioTrust facilitates assessments across six key dimensions: fairness, hallucination, safety, privacy, robustness, and authentication. To comprehensively evaluate these dimensions, AudioTrust is structured around 18 distinct experimental setups. Its core is a meticulously constructed dataset of over 4,420 audio&#x2F;text samples, drawn from real-world scenarios (e.g., daily conversations, emergency calls, voice assistant interactions), specifically designed to probe the multifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully designs 9 audio-specific evaluation metrics, and we employ a large-scale automated pipeline for objective and scalable scoring of model outputs. Experimental results reveal the trustworthiness boundaries and limitations of current state-of-the-art open-source and closed-source ALLMs when confronted with various high-risk audio scenarios, offering valuable insights for the secure and trustworthy deployment of future audio models. Our platform and benchmark are available at <a target="_blank" rel="noopener" href="https://github.com/JusperLee/AudioTrust">https://github.com/JusperLee/AudioTrust</a>. </p>
<blockquote>
<p>éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆALLMï¼‰çš„å¿«é€Ÿå‘å±•å’Œå¹¿æ³›åº”ç”¨è¦æ±‚æˆ‘ä»¬å¯¹å®ƒä»¬çš„å¯ä¿¡åº¦æœ‰ä¸¥æ ¼çš„ç†è§£ã€‚ç„¶è€Œï¼Œå…³äºè¯„ä¼°è¿™äº›æ¨¡å‹çš„ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹éŸ³é¢‘æ¨¡æ€ç‰¹æœ‰é£é™©çš„ç ”ç©¶ï¼Œä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚ç°æœ‰çš„è¯„ä¼°æ¡†æ¶ä¸»è¦å…³æ³¨æ–‡æœ¬æ¨¡æ€æˆ–åªæ¶‰åŠæœ‰é™çš„å®‰å…¨ç»´åº¦ï¼Œæœªèƒ½å……åˆ†è€ƒè™‘åˆ°éŸ³é¢‘æ¨¡æ€çš„å›ºæœ‰ç‰¹æ€§å’Œåº”ç”¨åœºæ™¯ã€‚æˆ‘ä»¬å¼•å…¥äº†AudioTrustâ€”â€”é¦–ä¸ªä¸“ä¸ºALLMè®¾è®¡çš„å¤šæ–¹é¢å¯ä¿¡åº¦è¯„ä¼°æ¡†æ¶å’ŒåŸºå‡†æµ‹è¯•ã€‚AudioTrustä¾¿äºåœ¨å…­ä¸ªå…³é”®ç»´åº¦è¿›è¡Œè¯„ä¼°ï¼šå…¬å¹³æ€§ã€å¹»è§‰ã€å®‰å…¨ã€éšç§ã€ç¨³å¥æ€§å’Œèº«ä»½éªŒè¯ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°è¿™äº›ç»´åº¦ï¼ŒAudioTrustå›´ç»•18ä¸ªç‹¬ç‰¹çš„å®éªŒè®¾ç½®æ„å»ºã€‚å…¶æ ¸å¿ƒæ˜¯ç²¾å¿ƒæ„å»ºçš„æ•°æ®é›†ï¼ŒåŒ…å«4420ä¸ªéŸ³é¢‘&#x2F;æ–‡æœ¬æ ·æœ¬ï¼Œæ¥è‡ªç°å®ä¸–ç•Œåœºæ™¯ï¼ˆä¾‹å¦‚æ—¥å¸¸å¯¹è¯ã€ç´§æ€¥å‘¼å«ã€è¯­éŸ³åŠ©æ‰‹äº¤äº’ï¼‰ï¼Œä¸“é—¨è®¾è®¡ç”¨äºæ¢æµ‹ALLMçš„å¤šæ–¹é¢å¯ä¿¡åº¦ã€‚ä¸ºäº†è¿›è¡Œè¯„ä¼°ï¼Œè¯¥åŸºå‡†æµ‹è¯•ç²¾å¿ƒè®¾è®¡äº†9ä¸ªéŸ³é¢‘ç‰¹å®šè¯„ä¼°æŒ‡æ ‡ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤§è§„æ¨¡è‡ªåŠ¨åŒ–ç®¡é“å¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œå®¢è§‚å’Œå¯æ‰©å±•çš„è¯„åˆ†ã€‚å®éªŒç»“æœæ­ç¤ºäº†å½“å‰æœ€æ–°å¼€æºå’Œä¸“æœ‰ALLMåœ¨é¢å¯¹å„ç§é«˜é£é™©éŸ³é¢‘åœºæ™¯æ—¶çš„å¯ä¿¡åº¦ç•Œé™å’Œå±€é™æ€§ï¼Œä¸ºæœªæ¥éŸ³é¢‘æ¨¡å‹çš„å®‰å…¨å’Œå¯ä¿¡éƒ¨ç½²æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚æˆ‘ä»¬çš„å¹³å°å’ŒåŸºå‡†æµ‹è¯•å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JusperLee/AudioTrust">https://github.com/JusperLee/AudioTrust</a>è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16211v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆALLMï¼‰çš„å¿«é€Ÿå‘å±•å’Œåº”ç”¨å¯¹æ¨¡å‹å¯ä¿¡åº¦è¯„ä¼°çš„è¿«åˆ‡éœ€æ±‚ã€‚ç„¶è€Œï¼Œé’ˆå¯¹éŸ³é¢‘æ¨¡æ€ç‰¹æœ‰é£é™©çš„è¯„ä¼°ç ”ç©¶ä»ç„¶ç›¸å¯¹ç¼ºä¹ã€‚ç°æœ‰çš„è¯„ä¼°æ¡†æ¶ä¸»è¦å…³æ³¨æ–‡æœ¬æ¨¡æ€æˆ–ä»…æ¶‰åŠæœ‰é™çš„å®‰å…¨ç»´åº¦ï¼Œæœªèƒ½å……åˆ†è€ƒè™‘éŸ³é¢‘æ¨¡æ€çš„ç‹¬ç‰¹ç‰¹æ€§å’Œåº”ç”¨åœºæ™¯ã€‚å› æ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†AudioTrustâ€”â€”é¦–ä¸ªä¸“ä¸ºALLMè®¾è®¡çš„å¤šæ–¹é¢å¯ä¿¡åº¦è¯„ä¼°æ¡†æ¶å’ŒåŸºå‡†æµ‹è¯•ã€‚AudioTrustèƒ½å¤Ÿè·¨å…­ä¸ªå…³é”®ç»´åº¦è¿›è¡Œè¯„ä¼°ï¼šå…¬å¹³æ€§ã€å¹»è§‰ã€å®‰å…¨ã€éšç§ã€ç¨³å¥æ€§å’Œè®¤è¯ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°è¿™äº›ç»´åº¦ï¼ŒAudioTrustå›´ç»•18ä¸ªä¸åŒçš„å®éªŒè®¾ç½®æ„å»ºã€‚å…¶æ ¸å¿ƒæ˜¯ä½¿ç”¨æ¥è‡ªçœŸå®åœºæ™¯ï¼ˆå¦‚æ—¥å¸¸å¯¹è¯ã€ç´§æ€¥å‘¼å«ã€è¯­éŸ³åŠ©æ‰‹äº¤äº’ï¼‰çš„4420ä¸ªéŸ³é¢‘&#x2F;æ–‡æœ¬æ ·æœ¬ç²¾å¿ƒæ„å»ºçš„æ•°æ®é›†ï¼Œæ—¨åœ¨æ¢æµ‹ALLMçš„å¤šæ–¹é¢å¯ä¿¡åº¦ã€‚è¯„ä¼°é‡‡ç”¨9ä¸ªéŸ³é¢‘ç‰¹å®šè¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶é‡‡ç”¨å¤§è§„æ¨¡è‡ªåŠ¨åŒ–ç®¡é“å¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œå®¢è§‚å’Œå¯æ‰©å±•çš„è¯„åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰å…ˆè¿›å¼€æºå’Œé—­æºALLMåœ¨é¢å¯¹å„ç§é«˜é£é™©éŸ³é¢‘åœºæ™¯æ—¶çš„å¯ä¿¡åº¦ç•Œé™å’Œå±€é™æ€§ï¼Œä¸ºæœªæ¥éŸ³é¢‘æ¨¡å‹çš„å®‰å…¨å¯ä¿¡éƒ¨ç½²æä¾›äº†å®è´µè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆALLMï¼‰çš„å¯ä¿¡æ€§è¯„ä¼°éœ€æ±‚è¿«åˆ‡ï¼Œä½†é’ˆå¯¹éŸ³é¢‘æ¨¡æ€ç‰¹æœ‰é£é™©çš„è¯„ä¼°ç ”ç©¶ç›¸å¯¹ä¸è¶³ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ¡†æ¶ä¸»è¦å…³æ³¨æ–‡æœ¬æ¨¡æ€æˆ–æœ‰é™çš„å®‰å…¨ç»´åº¦ï¼Œç¼ºä¹é’ˆå¯¹éŸ³é¢‘æ¨¡æ€çš„å…¨é¢è¯„ä¼°ã€‚</li>
<li>AudioTrustæ˜¯é¦–ä¸ªä¸“ä¸ºALLMè®¾è®¡çš„å¤šæ–¹é¢å¯ä¿¡æ€§è¯„ä¼°æ¡†æ¶å’ŒåŸºå‡†æµ‹è¯•ï¼Œèƒ½å¤Ÿè·¨å…­ä¸ªå…³é”®ç»´åº¦è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>AudioTrustä½¿ç”¨æ¥è‡ªçœŸå®åœºæ™¯çš„éŸ³é¢‘&#x2F;æ–‡æœ¬æ ·æœ¬æ•°æ®é›†ï¼Œæ—¨åœ¨æ¢æµ‹ALLMçš„å¤šæ–¹é¢å¯ä¿¡åº¦ã€‚</li>
<li>è¯„ä¼°é‡‡ç”¨9ä¸ªéŸ³é¢‘ç‰¹å®šè¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶é‡‡ç”¨å¤§è§„æ¨¡è‡ªåŠ¨åŒ–ç®¡é“è¿›è¡Œè¯„åˆ†ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰å…ˆè¿›çš„ALLMåœ¨é¢å¯¹é«˜é£é™©éŸ³é¢‘åœºæ™¯æ—¶å­˜åœ¨å¯ä¿¡åº¦ç•Œé™å’Œå±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16211">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-436e64ee010beb359a126c701bd8ffef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-54dda451559d9c58e799f8a262e64784.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d346566c2faa744dfd923d0964abbe3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63c7c018d38cdd0a2c9fd1f3d8429317.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59513d5791662d028a87b020cb84cc40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-628cedea84d9c16b05cd0b4e5b33f78e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="VLM-R-3-Region-Recognition-Reasoning-and-Refinement-for-Enhanced-Multimodal-Chain-of-Thought"><a href="#VLM-R-3-Region-Recognition-Reasoning-and-Refinement-for-Enhanced-Multimodal-Chain-of-Thought" class="headerlink" title="VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced   Multimodal Chain-of-Thought"></a>VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced   Multimodal Chain-of-Thought</h2><p><strong>Authors:Chaoya Jiang, Yongrui Heng, Wei Ye, Han Yang, Haiyang Xu, Ming Yan, Ji Zhang, Fei Huang, Shikun Zhang</strong></p>
<p>Recently, reasoning-based MLLMs have achieved a degree of success in generating long-form textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on and revisiting of visual regions to achieve precise grounding of textual reasoning in visual evidence. We introduce \textbf{VLM-R$^3$} (\textbf{V}isual \textbf{L}anguage \textbf{M}odel with \textbf{R}egion \textbf{R}ecognition and \textbf{R}easoning), a framework that equips an MLLM with the ability to (i) decide \emph{when} additional visual evidence is needed, (ii) determine \emph{where} to ground within the image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved chain-of-thought. The core of our method is \textbf{Region-Conditioned Reinforcement Policy Optimization (R-GRPO)}, a training paradigm that rewards the model for selecting informative regions, formulating appropriate transformations (e.g.\ crop, zoom), and integrating the resulting visual context into subsequent reasoning steps. To bootstrap this policy, we compile a modest but carefully curated Visuo-Lingual Interleaved Rationale (VLIR) corpus that provides step-level supervision on region selection and textual justification. Extensive experiments on MathVista, ScienceQA, and other benchmarks show that VLM-R$^3$ sets a new state of the art in zero-shot and few-shot settings, with the largest gains appearing on questions demanding subtle spatial reasoning or fine-grained visual cue extraction. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºæ¨ç†çš„MLLMåœ¨ç”Ÿæˆé•¿æ–‡æœ¬æ¨ç†é“¾æ–¹é¢å–å¾—äº†ä¸€å®šçš„æˆåŠŸã€‚ç„¶è€Œï¼Œåœ¨é¢å¯¹éœ€è¦åŠ¨æ€å’Œè¿­ä»£åœ°å…³æ³¨å¹¶é‡æ–°å®¡è§†è§†è§‰åŒºåŸŸä»¥å®ç°æ–‡æœ¬æ¨ç†åœ¨è§†è§‰è¯æ®ä¸­çš„ç²¾ç¡®å®šä½ç­‰å¤æ‚ä»»åŠ¡æ—¶ï¼Œå®ƒä»¬ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†VLM-R$^3$ï¼ˆå¸¦åŒºåŸŸè¯†åˆ«å’Œæ¨ç†çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºMLLMæä¾›èƒ½åŠ›çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿï¼ˆiï¼‰å†³å®šä½•æ—¶éœ€è¦é¢å¤–çš„è§†è§‰è¯æ®ï¼Œï¼ˆiiï¼‰ç¡®å®šåœ¨å›¾åƒä¸­çš„å®šä½ä½ç½®ï¼Œä»¥åŠï¼ˆiiiï¼‰æ— ç¼åœ°å°†ç›¸å…³çš„å­å›¾åƒå†…å®¹é‡æ–°ç¼–ç»‡æˆè¿è´¯çš„æ¨ç†é“¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯åŒºåŸŸæ¡ä»¶å¼ºåŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆR-GRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è®­ç»ƒèŒƒå¼ï¼Œå¥–åŠ±æ¨¡å‹é€‰æ‹©ä¿¡æ¯åŒºåŸŸã€åˆ¶å®šé€‚å½“çš„è½¬æ¢ï¼ˆä¾‹å¦‚è£å‰ªã€æ”¾å¤§ï¼‰ï¼Œå¹¶å°†å¾—åˆ°çš„è§†è§‰ä¸Šä¸‹æ–‡æ•´åˆåˆ°éšåçš„æ¨ç†æ­¥éª¤ä¸­ã€‚ä¸ºäº†å¼•å¯¼è¿™ä¸ªç­–ç•¥ï¼Œæˆ‘ä»¬ç²¾å¿ƒæ•´ç†äº†ä¸€ä¸ªè§„æ¨¡è™½å°ä½†ç»è¿‡ç²¾å¿ƒç­–åˆ’çš„è§†å¬è¯­è¨€äº¤é”™è§£é‡Šè¯­æ–™åº“ï¼ˆVLIRï¼‰ï¼Œè¯¥è¯­æ–™åº“å¯¹åŒºåŸŸé€‰æ‹©å’Œæ–‡æœ¬è§£é‡Šæ­¥éª¤çš„ç›‘ç£æä¾›çº§åˆ«æ˜ç¡®çš„æŒ‡å¯¼ã€‚åœ¨MathVistaã€ScienceQAå’Œå…¶ä»–åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVLM-R$^3$åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸­è¾¾åˆ°äº†æ–°çš„æŠ€æœ¯æ°´å¹³ï¼Œå°¤å…¶æ˜¯åœ¨è¦æ±‚ç²¾ç»†ç©ºé—´æ¨ç†æˆ–ç²¾ç»†è§†è§‰çº¿ç´¢æå–çš„é—®é¢˜ä¸Šï¼Œå‡ºç°äº†æœ€å¤§çš„æ”¶ç›Šæå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16192v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†VLM-RÂ³æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸ºMLLMæä¾›äº†åœ¨è§†è§‰è¯æ®ä¸­è¿›è¡Œç²¾ç¡®æ–‡æœ¬æ¨ç†çš„èƒ½åŠ›ã€‚å…¶æ ¸å¿ƒæ˜¯Region-Conditioned Reinforcement Policy Optimization (R-GRPO)è®­ç»ƒèŒƒå¼ï¼Œå¥–åŠ±æ¨¡å‹é€‰æ‹©ä¿¡æ¯åŒºåŸŸã€åˆ¶å®šé€‚å½“çš„è½¬æ¢ï¼ˆå¦‚è£å‰ªã€æ”¾å¤§ï¼‰å¹¶å°†å¾—åˆ°çš„è§†è§‰ä¸Šä¸‹æ–‡é›†æˆåˆ°éšåçš„æ¨ç†æ­¥éª¤ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVLM-RÂ³åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸Šè¾¾åˆ°äº†æ–°çš„æ°´å¹³ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç²¾ç»†ç©ºé—´æ¨ç†æˆ–ç»†å¾®è§†è§‰çº¿ç´¢æå–çš„é—®é¢˜ä¸Šå–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLM-RÂ³æ¡†æ¶ä¸ºMLLMé…å¤‡äº†åœ¨è§†è§‰è¯æ®ä¸­è¿›è¡Œç²¾ç¡®æ–‡æœ¬æ¨ç†çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…æ‹¬å†³å®šä½•æ—¶éœ€è¦é¢å¤–çš„è§†è§‰è¯æ®ã€ç¡®å®šåœ¨å›¾åƒä¸­çš„å®šä½ä»¥åŠå¦‚ä½•æ— ç¼åœ°å°†ç›¸å…³å­å›¾åƒå†…å®¹èå…¥æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>æ ¸å¿ƒæ–¹æ³•ä¸ºRegion-Conditioned Reinforcement Policy Optimization (R-GRPO)è®­ç»ƒèŒƒå¼ï¼Œå¥–åŠ±æ¨¡å‹é€‰æ‹©ä¿¡æ¯åŒºåŸŸå¹¶è¿›è¡Œé€‚å½“çš„è½¬æ¢ã€‚</li>
<li>VLM-RÂ³é€šè¿‡ç¼–è¯‘ä¸€ä¸ªé€‚åº¦ä½†ç²¾å¿ƒç­–åˆ’çš„Visuo-Lingual Interleaved Rationale (VLIR)è¯­æ–™åº“æ¥å¼•å¯¼ç­–ç•¥ã€‚</li>
<li>VLM-RÂ³åœ¨MathVistaã€ScienceQAç­‰åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç²¾ç»†ç©ºé—´æ¨ç†æˆ–ç»†å¾®è§†è§‰çº¿ç´¢æå–çš„é—®é¢˜ä¸Šã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸Šå®ç°äº†æ–°çš„æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16192">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e58c89e9347a05f0805fa5c6ec2b876.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-422aef326d89b2525c634234f6fae7df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcae11ec4f5131ec71033ae813583463.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Sudoku-Bench-Evaluating-creative-reasoning-with-Sudoku-variants"><a href="#Sudoku-Bench-Evaluating-creative-reasoning-with-Sudoku-variants" class="headerlink" title="Sudoku-Bench: Evaluating creative reasoning with Sudoku variants"></a>Sudoku-Bench: Evaluating creative reasoning with Sudoku variants</h2><p><strong>Authors:Jeffrey Seely, Yuki Imajuku, Tianyu Zhao, Edoardo Cetin, Llion Jones</strong></p>
<p>Existing reasoning benchmarks for large language models (LLMs) frequently fail to capture authentic creativity, often rewarding memorization of previously observed patterns. We address this shortcoming with Sudoku-Bench, a curated benchmark of challenging and unconventional Sudoku variants specifically selected to evaluate creative, multi-step logical reasoning. Sudoku variants form an unusually effective domain for reasoning research: each puzzle introduces unique or subtly interacting constraints, making memorization infeasible and requiring solvers to identify novel logical breakthroughs (&#96;&#96;break-insâ€™â€™). Despite their diversity, Sudoku variants maintain a common and compact structure, enabling clear and consistent evaluation. Sudoku-Bench includes a carefully chosen puzzle set, a standardized text-based puzzle representation, and flexible tools compatible with thousands of publicly available puzzles â€“ making it easy to extend into a general research environment. Baseline experiments show that state-of-the-art LLMs solve fewer than 15% of puzzles unaided, highlighting significant opportunities to advance long-horizon, strategic reasoning capabilities. </p>
<blockquote>
<p>ç°æœ‰çš„é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†åŸºå‡†æµ‹è¯•é€šå¸¸æ— æ³•æ•æ‰çœŸå®çš„åˆ›é€ åŠ›ï¼Œé€šå¸¸åªæ˜¯å¥–åŠ±å¯¹å…ˆå‰è§‚å¯Ÿæ¨¡å¼çš„è®°å¿†ã€‚æˆ‘ä»¬é€šè¿‡Sudoku-Benchæ¥è§£å†³è¿™ä¸€ç¼ºé™·ï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾å¿ƒç­–åˆ’çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«å…·æœ‰æŒ‘æˆ˜æ€§å’Œéä¼ ç»Ÿçš„æ•°ç‹¬å˜ä½“ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°åˆ›é€ æ€§å¤šæ­¥éª¤é€»è¾‘æ¨ç†ã€‚æ•°ç‹¬å˜ä½“åœ¨æ¨ç†ç ”ç©¶ä¸­è¡¨ç°å‡ºäº†å¼‚å¸¸çš„æœ‰æ•ˆæ€§ï¼šæ¯ä¸ªè°œé¢˜éƒ½ä¼šå¼•å…¥ç‹¬ç‰¹æˆ–å¾®å¦™äº¤äº’çš„çº¦æŸï¼Œä½¿è®°å¿†å˜å¾—ä¸å¯èƒ½ï¼Œå¹¶è¦æ±‚è§£è°œè€…æ‰¾å‡ºæ–°çš„é€»è¾‘çªç ´ï¼ˆâ€œçªç ´â€ï¼‰ã€‚å°½ç®¡å®ƒä»¬å…·æœ‰å¤šæ ·æ€§ï¼Œä½†æ•°ç‹¬å˜ä½“ä¿æŒäº†å…±åŒä¸”ç´§å‡‘çš„ç»“æ„ï¼Œä»è€Œå¯ä»¥è¿›è¡Œæ¸…æ™°å’Œä¸€è‡´çš„è¯„ä»·ã€‚Sudoku-BenchåŒ…æ‹¬ç²¾å¿ƒæŒ‘é€‰çš„è°œé¢˜é›†ã€æ ‡å‡†åŒ–çš„æ–‡æœ¬è°œé¢˜è¡¨ç¤ºä»¥åŠå…¼å®¹æ•°åƒä¸ªå…¬å¼€è°œé¢˜çš„çµæ´»å·¥å…·â€”â€”ä½¿å…¶æ˜“äºæ‰©å±•ä¸ºä¸€èˆ¬ç ”ç©¶ç¯å¢ƒã€‚åŸºå‡†å®éªŒè¡¨æ˜ï¼Œæœ€æ–°å‰æ²¿çš„LLMåœ¨æ²¡æœ‰å¸®åŠ©çš„æƒ…å†µä¸‹åªèƒ½è§£å†³å°‘äº15%çš„è°œé¢˜ï¼Œè¿™å‡¸æ˜¾äº†æé«˜é•¿æœŸè§†é‡å’Œæˆ˜ç•¥æ¨ç†èƒ½åŠ›çš„é‡å¤§æœºä¼šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16135v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¸¸å¸¸åå‘äºæ£€æµ‹æ¨¡å‹çš„è®°å¿†èƒ½åŠ›è€ŒéçœŸæ­£çš„åˆ›é€ åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Sudoku-Benchï¼Œè¿™æ˜¯ä¸€å¥—ç»è¿‡ç²¾å¿ƒæŒ‘é€‰çš„æå…·æŒ‘æˆ˜æ€§å’Œéä¼ ç»Ÿçš„æ•°ç‹¬å˜ç§ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹çš„åˆ›é€ æ€§å¤šæ­¥éª¤é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚æ•°ç‹¬å˜ç§å¯¹äºæ¨ç†ç ”ç©¶æ¥è¯´æ˜¯ä¸€ä¸ªç‹¬ç‰¹çš„é¢†åŸŸï¼Œæ¯ä¸ªè°œé¢˜éƒ½æœ‰ç‹¬ç‰¹çš„çº¦æŸæ¡ä»¶ï¼Œè¦æ±‚è§£å†³è€…è¯†åˆ«å‡ºæ–°çš„é€»è¾‘çªç ´ç‚¹ï¼ˆâ€œçªç ´ç‚¹â€ï¼‰ï¼Œè€Œéä¾èµ–è®°å¿†ã€‚å°½ç®¡æ•°ç‹¬è°œé¢˜å¤šæ ·ï¼Œä½†å®ƒä»¬ä¿æŒäº†æ¸…æ™°ä¸€è‡´çš„è¯„ä»·æ ‡å‡†ã€‚Sudoku-BenchåŒ…æ‹¬ä¸€å¥—ç²¾å¿ƒæŒ‘é€‰çš„è°œé¢˜ã€æ ‡å‡†åŒ–çš„æ–‡æœ¬è°œé¢˜è¡¨ç¤ºä»¥åŠçµæ´»çš„å·¥å…·ï¼Œä¸æ•°åƒä¸ªå…¬å¼€å¯ç”¨çš„è°œé¢˜å…¼å®¹ï¼Œæ˜“äºæ‰©å±•ä¸ºä¸€èˆ¬ç ”ç©¶ç¯å¢ƒã€‚åŸºå‡†å®éªŒè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„LLMåœ¨æ²¡æœ‰è¾…åŠ©çš„æƒ…å†µä¸‹åªèƒ½è§£å†³ä¸åˆ°15%çš„è°œé¢˜ï¼Œè¿™çªæ˜¾äº†æé«˜é•¿æœŸæˆ˜ç•¥æ¨ç†èƒ½åŠ›çš„å·¨å¤§æœºä¼šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç°æœ‰æ¨ç†åŸºå‡†æµ‹è¯•å¸¸å¸¸ä¸èƒ½å‡†ç¡®è¯„ä¼°æ¨¡å‹çš„åˆ›é€ åŠ›ï¼Œæ›´å¤šåœ°æ˜¯æ£€æµ‹æ¨¡å‹çš„è®°å¿†èƒ½åŠ›ã€‚</li>
<li>Sudoku-Benchæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹çš„åˆ›é€ æ€§å¤šæ­¥éª¤é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ•°ç‹¬å˜ç§æ˜¯è¯„ä¼°é€»è¾‘æ¨ç†èƒ½åŠ›çš„ç†æƒ³é¢†åŸŸï¼Œå› ä¸ºå®ƒä»¬è¦æ±‚è¯†åˆ«é€»è¾‘çªç ´ç‚¹è€Œéä¾èµ–è®°å¿†ã€‚</li>
<li>Sudoku-BenchåŒ…æ‹¬ç²¾é€‰çš„è°œé¢˜é›†ã€æ ‡å‡†åŒ–çš„æ–‡æœ¬è¡¨ç¤ºå·¥å…·å’Œçµæ´»çš„è¯„ä»·ç³»ç»Ÿã€‚</li>
<li>åŸºå‡†å®éªŒè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„LLMè§£å†³æ•°ç‹¬è°œé¢˜çš„èƒ½åŠ›æœ‰é™ï¼Œçªæ˜¾äº†æé«˜é•¿æœŸæˆ˜ç•¥æ¨ç†èƒ½åŠ›çš„éœ€æ±‚ã€‚</li>
<li>Sudoku-Benchæ˜“äºæ‰©å±•ä¸ºä¸€èˆ¬ç ”ç©¶ç¯å¢ƒï¼Œå¹¶å¯ä¸æ•°åƒä¸ªå…¬å¼€å¯ç”¨çš„è°œé¢˜å…¼å®¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16135">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c729f65c3f77e5ab0a62a401d8c54a22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aeb21c2a20704ce89a199741f71a360b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96fd9d19f693ac4b3b1f4704a6a5f813.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-25/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-25/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-25/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-6159bc24c2e0e6f729f8be17cf4d5d87.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-25  Short-Range Dependency Effects on Transformer Instability and a   Decomposed Attention Solution
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-24/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-66fbd04d15f2a4b4c738ef5ca205bb1e.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-24  MM-MovieDubber Towards Multi-Modal Learning for Multi-Modal Movie   Dubbing
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32251.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
