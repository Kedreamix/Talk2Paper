<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-25  Short-Range Dependency Effects on Transformer Instability and a   Decomposed Attention Solution">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-6159bc24c2e0e6f729f8be17cf4d5d87.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-29
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-25-æ›´æ–°"><a href="#2025-05-25-æ›´æ–°" class="headerlink" title="2025-05-25 æ›´æ–°"></a>2025-05-25 æ›´æ–°</h1><h2 id="Short-Range-Dependency-Effects-on-Transformer-Instability-and-a-Decomposed-Attention-Solution"><a href="#Short-Range-Dependency-Effects-on-Transformer-Instability-and-a-Decomposed-Attention-Solution" class="headerlink" title="Short-Range Dependency Effects on Transformer Instability and a   Decomposed Attention Solution"></a>Short-Range Dependency Effects on Transformer Instability and a   Decomposed Attention Solution</h2><p><strong>Authors:Suvadeep Hajra</strong></p>
<p>Transformer language models have driven significant progress across various fields, including natural language processing and computer vision. A central component of these models is the self-attention (SA) mechanism, which learns rich vector representations of tokens by modeling their relationships with others in a sequence. However, despite extensive research, transformers continue to suffer from training instability â€“ often manifesting as spikes or divergence in the training loss during a run.   In this work, we identify one source of this instability: SAâ€™s limited ability to capture short-range dependencies, especially in tasks like language modeling, where almost every token heavily relies on its nearby neighbors. This limitation causes the pre-softmax logits of SA to grow rapidly, destabilizing training. To address this, we propose decomposing the SA into local (short-range) and global (long-range) attention heads. This decomposed attention, referred to as Long Short-attention (LS-attention), mitigates logit explosion and results in more stable training compared to an equivalent multi-head self-attention (MHSA). Empirical comparisons with two alternative training stabilization methods show that LS-attention reduces the validation perplexity to nearly 2&#x2F;5 of that achieved by one method and reaches a similar perplexity as the other method using only 1&#x2F;20 of the GPU hours. Additionally, our experiments demonstrate that LS-attention reduces inference latency by up to 36% compared to a state-of-the-art implementation of equivalent MHSA. </p>
<blockquote>
<p>Transformerè¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ã€‚è¿™äº›æ¨¡å‹çš„æ ¸å¿ƒç»„ä»¶æ˜¯è‡ªæˆ‘å…³æ³¨ï¼ˆSAï¼‰æœºåˆ¶ï¼Œå®ƒé€šè¿‡å»ºæ¨¡åºåˆ—ä¸­ä»¤ç‰Œä¹‹é—´çš„å…³ç³»æ¥å­¦ä¹ ä¸°å¯Œçš„å‘é‡è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå°½ç®¡è¿›è¡Œäº†å¤§é‡ç ”ç©¶ï¼ŒTransformerä»å—åˆ°è®­ç»ƒä¸ç¨³å®šçš„å½±å“â€”â€”åœ¨è¿è¡Œè¿‡ç¨‹ä¸­ï¼Œè®­ç»ƒæŸå¤±ä¼šå‡ºç°é£™å‡æˆ–å‘æ•£ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç¡®å®šäº†è¿™ç§ä¸ç¨³å®šæ€§çš„ä¸€ä¸ªæ¥æºï¼šSAåœ¨æ•æ‰çŸ­è·ç¦»ç›¸å…³æ€§æ–¹é¢çš„æœ‰é™èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­è¨€å»ºæ¨¡ç­‰ä»»åŠ¡ä¸­ï¼Œå‡ ä¹æ¯ä¸ªä»¤ç‰Œéƒ½ä¸¥é‡ä¾èµ–äºå…¶é™„è¿‘çš„é‚»å±…ã€‚è¿™ç§å±€é™æ€§å¯¼è‡´SAçš„é¢„softmaxå¯¹æ•°å¿«é€Ÿå¢åŠ ï¼Œä»è€Œä½¿è®­ç»ƒä¸ç¨³å®šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†SAåˆ†è§£ä¸ºå±€éƒ¨ï¼ˆçŸ­è·ç¦»ï¼‰å’Œå…¨å±€ï¼ˆé•¿è·ç¦»ï¼‰æ³¨æ„åŠ›å¤´ã€‚è¿™ç§åˆ†è§£çš„æ³¨æ„åŠ›è¢«ç§°ä¸ºé•¿çŸ­æœŸæ³¨æ„åŠ›ï¼ˆLS-attentionï¼‰ï¼Œå®ƒç¼“è§£äº†å¯¹æ•°çˆ†ç‚¸é—®é¢˜ï¼Œä¸ç­‰æ•ˆçš„å¤šå¤´è‡ªå…³æ³¨ï¼ˆMHSAï¼‰ç›¸æ¯”ï¼Œå®ƒå®ç°äº†æ›´ç¨³å®šçš„è®­ç»ƒã€‚ä¸ä¸¤ç§æ›¿ä»£çš„è®­ç»ƒç¨³å®šåŒ–æ–¹æ³•çš„ç»éªŒæ¯”è¾ƒè¡¨æ˜ï¼ŒLS-attentionå°†éªŒè¯å›°æƒ‘åº¦é™ä½åˆ°ä¸€ç§æ–¹æ³•çš„è¿‘äº”åˆ†ä¹‹äºŒï¼Œå¹¶ä½¿ç”¨ä»…ç›¸å½“äºå¦ä¸€ç§æ–¹æ³•äº”ååˆ†ä¹‹ä¸€çš„æ—¶é—´è¾¾åˆ°äº†ç±»ä¼¼çš„å›°æƒ‘åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸ç­‰æ•ˆçš„MHSAçš„æœ€æ–°å®ç°ç›¸æ¯”ï¼ŒLS-attentionå°†æ¨ç†å»¶è¿Ÿæ—¶é—´å‡å°‘äº†é«˜è¾¾36%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15548v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    Transformerè¯­è¨€æ¨¡å‹åœ¨å„é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ã€‚å…¶æ ¸å¿ƒç»„ä»¶è‡ªæ³¨æ„åŠ›ï¼ˆSAï¼‰æœºåˆ¶é€šè¿‡å»ºæ¨¡åºåˆ—ä¸­ä»¤ç‰Œä¹‹é—´çš„å…³ç³»æ¥å­¦ä¹ ä¸°å¯Œçš„å‘é‡è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå°½ç®¡è¿›è¡Œäº†å¤§é‡ç ”ç©¶ï¼ŒTransformerä»é¢ä¸´è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œè¡¨ç°ä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±æ³¢åŠ¨æˆ–å‘æ•£ã€‚æœ¬æ–‡è¯†åˆ«äº†è¿™ç§ä¸ç¨³å®šæ€§çš„ä¸€ä¸ªæ¥æºï¼šè‡ªæ³¨æ„åŠ›åœ¨æ•æ‰çŸ­è·ç¦»ä¾èµ–æ–¹é¢çš„èƒ½åŠ›æœ‰é™ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­è¨€å»ºæ¨¡ç­‰ä»»åŠ¡ä¸­ï¼Œå‡ ä¹æ¯ä¸ªä»¤ç‰Œéƒ½ä¸¥é‡ä¾èµ–äºå…¶é‚»è¿‘çš„ä»¤ç‰Œã€‚è¿™ç§å±€é™æ€§å¯¼è‡´SAçš„é¢„softmaxå¯¹æ•°å‡ ç‡è¿…é€Ÿå¢é•¿ï¼Œä½¿è®­ç»ƒä¸ç¨³å®šã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å°†SAåˆ†è§£ä¸ºå±€éƒ¨ï¼ˆçŸ­è·ç¦»ï¼‰å’Œå…¨å±€ï¼ˆé•¿è·ç¦»ï¼‰æ³¨æ„åŠ›å¤´ã€‚è¿™ç§åˆ†è§£æ³¨æ„åŠ›è¢«ç§°ä¸ºé•¿çŸ­æœŸæ³¨æ„åŠ›ï¼ˆLS-attentionï¼‰ï¼Œå®ƒç¼“è§£äº†logitçˆ†ç‚¸é—®é¢˜ï¼Œä¸ç­‰æ•ˆçš„å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMHSAï¼‰ç›¸æ¯”ï¼Œå®ç°äº†æ›´ç¨³å®šçš„è®­ç»ƒã€‚ä¸ä¸¤ç§æ›¿ä»£çš„è®­ç»ƒç¨³å®šæ–¹æ³•è¿›è¡Œæ¯”è¾ƒè¡¨æ˜ï¼ŒLS-attentionå°†éªŒè¯å›°æƒ‘åº¦é™ä½åˆ°ä¸€ç§æ–¹æ³•çš„è¿‘äº”åˆ†ä¹‹äºŒï¼Œå¹¶ä»¥ä»…ä½¿ç”¨äº”ååˆ†ä¹‹ä¸€GPUå°æ—¶çš„æ—¶é—´è¾¾åˆ°äº†å¦ä¸€ç§æ–¹æ³•çš„å›°æƒ‘åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸ç­‰æ•ˆçš„MHSAçš„ç°æœ‰å…ˆè¿›å®ç°ç›¸æ¯”ï¼ŒLS-attentionæœ€å¤šå¯å°†æ¨ç†å»¶è¿Ÿæ—¶é—´å‡å°‘36%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Transformerè¯­è¨€æ¨¡å‹åœ¨å„é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ï¼Œä½†ä»é¢ä¸´è®­ç»ƒä¸ç¨³å®šçš„æŒ‘æˆ˜ã€‚</li>
<li>è®­ç»ƒä¸ç¨³å®šè¡¨ç°ä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±æ³¢åŠ¨æˆ–å‘æ•£ã€‚</li>
<li>è‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨æ•æ‰çŸ­è·ç¦»ä¾èµ–æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œè¿™æ˜¯å¯¼è‡´è®­ç»ƒä¸ç¨³å®šçš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚</li>
<li>æå‡ºçš„LS-attentioné€šè¿‡åˆ†è§£è‡ªæ³¨æ„åŠ›ä¸ºå±€éƒ¨å’Œå…¨å±€æ³¨æ„åŠ›å¤´æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>LS-attentionå®ç°äº†æ›´ç¨³å®šçš„è®­ç»ƒï¼Œä¸ç­‰æ•ˆçš„MHSAç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>LS-attentionåœ¨éªŒè¯å›°æƒ‘åº¦å’ŒGPUå°æ—¶ä½¿ç”¨æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15548">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed7de421e67428342fa9c0628bf9e738.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6db8171bcad142a4d4b56f176191780b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49d846d532be06dd1fba071a59647bd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fa0934b4401ff90b442649ab41789cd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="An-Efficient-Private-GPT-Never-Autoregressively-Decodes"><a href="#An-Efficient-Private-GPT-Never-Autoregressively-Decodes" class="headerlink" title="An Efficient Private GPT Never Autoregressively Decodes"></a>An Efficient Private GPT Never Autoregressively Decodes</h2><p><strong>Authors:Zhengyi Li, Yue Guan, Kang Yang, Yu Feng, Ning Liu, Yu Yu, Jingwen Leng, Minyi Guo</strong></p>
<p>The wide deployment of the generative pre-trained transformer (GPT) has raised privacy concerns for both clients and servers. While cryptographic primitives can be employed for secure GPT inference to protect the privacy of both parties, they introduce considerable performance overhead.To accelerate secure inference, this study proposes a public decoding and secure verification approach that utilizes public GPT models, motivated by the observation that securely decoding one and multiple tokens takes a similar latency. The client uses the public model to generate a set of tokens, which are then securely verified by the private model for acceptance. The efficiency of our approach depends on the acceptance ratio of tokens proposed by the public model, which we improve from two aspects: (1) a private sampling protocol optimized for cryptographic primitives and (2) model alignment using knowledge distillation. Our approach improves the efficiency of secure decoding while maintaining the same level of privacy and generation quality as standard secure decoding. Experiments demonstrate a $2.1\times \sim 6.0\times$ speedup compared to standard decoding across three pairs of public-private models and different network conditions. </p>
<blockquote>
<p>åŸºäºç”Ÿæˆå¼é¢„è®­ç»ƒè½¬æ¢å™¨ï¼ˆGPTï¼‰çš„å¹¿æ³›åº”ç”¨ï¼Œå¼•å‘äº†å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨çš„éšç§æ‹…å¿§ã€‚è™½ç„¶å¯ä»¥ä½¿ç”¨åŠ å¯†æŠ€æœ¯æ¥ä¿æŠ¤GPTæ¨ç†è¿‡ç¨‹ï¼Œä»¥ä¿æŠ¤åŒæ–¹çš„éšç§ï¼Œä½†å®ƒä»¬ä¼šå¼•å…¥ç›¸å½“å¤§çš„æ€§èƒ½å¼€é”€ã€‚ä¸ºäº†åŠ é€Ÿå®‰å…¨æ¨ç†ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å…¬å¼€GPTæ¨¡å‹çš„å…¬å¼€è§£ç å’Œå®‰å…¨éªŒè¯æ–¹æ³•ã€‚è¯¥æ–¹æ³•å—åˆ°ä»¥ä¸‹è§‚å¯Ÿç»“æœçš„å¯å‘ï¼šå®‰å…¨è§£ç å•ä¸ªå’Œå¤šä¸ªä»¤ç‰Œæ‰€éœ€çš„æ—¶é—´å»¶è¿Ÿç›¸ä¼¼ã€‚å®¢æˆ·ç«¯ä½¿ç”¨å…¬å…±æ¨¡å‹ç”Ÿæˆä¸€ç»„ä»¤ç‰Œï¼Œç„¶åç”±ç§æœ‰æ¨¡å‹å¯¹è¿™äº›ä»¤ç‰Œè¿›è¡Œå®‰å…¨éªŒè¯ä»¥ç¡®å®šæ˜¯å¦æ¥å—ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ•ˆç‡å–å†³äºå…¬å…±æ¨¡å‹æå‡ºçš„ä»¤ç‰Œçš„æ¥å—ç‡ï¼Œæˆ‘ä»¬ä»ä¸¤ä¸ªæ–¹é¢è¿›è¡Œäº†æ”¹è¿›ï¼šï¼ˆ1ï¼‰é’ˆå¯¹åŠ å¯†æŠ€æœ¯ä¼˜åŒ–çš„ç§æœ‰é‡‡æ ·åè®®ï¼›ï¼ˆ2ï¼‰ä½¿ç”¨çŸ¥è¯†è’¸é¦è¿›è¡Œæ¨¡å‹å¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æé«˜å®‰å…¨è§£ç æ•ˆç‡çš„åŒæ—¶ï¼Œä¿æŒäº†ä¸æ ‡å‡†å®‰å…¨è§£ç ç›¸åŒçš„éšç§å’Œç”Ÿæˆè´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼Œä¸æ ‡å‡†è§£ç ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨ä¸‰å¯¹å…¬å…±-ç§æœ‰æ¨¡å‹çš„ä¸åŒç½‘ç»œæ¡ä»¶ä¸‹å®ç°äº†$ 2.1\timesè‡³ 6.0\times$ çš„é€Ÿåº¦æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15252v1">PDF</a> Accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>GPTçš„å¹¿æ³›åº”ç”¨å¼•å‘äº†å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨çš„éšç§æ‹…å¿§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼ŒåŒæ—¶ä¿æŒé«˜æ€§èƒ½ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå…¬å¼€è§£ç å’Œå®‰å…¨éªŒè¯çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å…¬å¼€GPTæ¨¡å‹è¿›è¡Œè§£ç ï¼Œå¹¶é€šè¿‡ç§æœ‰æ¨¡å‹è¿›è¡Œå®‰å…¨éªŒè¯ã€‚é€šè¿‡ä¼˜åŒ–ç§æœ‰é‡‡æ ·åè®®å’Œæ¨¡å‹å¯¹é½ï¼Œæé«˜äº†è¯¥æ–¹æ³•çš„æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼Œä¸æ ‡å‡†è§£ç ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒç›¸åŒéšç§å’Œç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†$2.1\times \sim 6.0\times$çš„åŠ é€Ÿæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPTçš„å¹¿æ³›åº”ç”¨å¼•å‘äº†å…³äºå®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨éšç§çš„æ‹…å¿§ã€‚</li>
<li>åŠ å¯†åŸå§‹æŠ€æœ¯è™½ç„¶å¯ä»¥ä¿æŠ¤GPTæ¨ç†è¿‡ç¨‹ä¸­çš„éšç§ï¼Œä½†ä¼šå¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½å¼€é”€ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå…¬å¼€è§£ç å’Œå®‰å…¨éªŒè¯çš„æ–¹æ³•ï¼Œåˆ©ç”¨å…¬å¼€GPTæ¨¡å‹è¿›è¡Œè§£ç ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–ç§æœ‰é‡‡æ ·åè®®å’Œæ¨¡å‹å¯¹é½ï¼Œæé«˜äº†å®‰å…¨è§£ç çš„æ•ˆç‡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒç›¸åŒéšç§å’Œç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>è¯¥æ–¹æ³•é€‚ç”¨äºå¤šç§å…¬å¼€-ç§æœ‰æ¨¡å‹ç»„åˆå’Œä¸åŒç½‘ç»œæ¡ä»¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15252">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5578ed944719a9bbaa2880938e30a10e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73abfde2d32900c7a64ca00cddfbb14d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcfb97b7b78bdb52dfe58346815bb933.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Towards-Explainable-Temporal-Reasoning-in-Large-Language-Models-A-Structure-Aware-Generative-Framework"><a href="#Towards-Explainable-Temporal-Reasoning-in-Large-Language-Models-A-Structure-Aware-Generative-Framework" class="headerlink" title="Towards Explainable Temporal Reasoning in Large Language Models: A   Structure-Aware Generative Framework"></a>Towards Explainable Temporal Reasoning in Large Language Models: A   Structure-Aware Generative Framework</h2><p><strong>Authors:Zihao Jiang, Ben Liu, Miao Peng, Wenjie Xu, Yao Xiao, Zhenyan Shan, Min Peng</strong></p>
<p>While large language models (LLMs) show great potential in temporal reasoning, most existing work focuses heavily on enhancing performance, often neglecting the explainable reasoning processes underlying the results. To address this gap, we introduce a comprehensive benchmark covering a wide range of temporal granularities, designed to systematically evaluate LLMsâ€™ capabilities in explainable temporal reasoning. Furthermore, our findings reveal that LLMs struggle to deliver convincing explanations when relying solely on textual information. To address challenge, we propose GETER, a novel structure-aware generative framework that integrates Graph structures with text for Explainable TEmporal Reasoning. Specifically, we first leverage temporal knowledge graphs to develop a temporal encoder that captures structural information for the query. Subsequently, we introduce a structure-text prefix adapter to map graph structure features into the text embedding space. Finally, LLMs generate explanation text by seamlessly integrating the soft graph token with instruction-tuning prompt tokens. Experimental results indicate that GETER achieves state-of-the-art performance while also demonstrating its effectiveness as well as strong generalization capabilities. Our dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/carryTatum/GETER">https://github.com/carryTatum/GETER</a>. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¶åºæ¨ç†æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å¤§å¤šæ•°ç°æœ‰å·¥ä½œéƒ½é›†ä¸­åœ¨æé«˜æ€§èƒ½ä¸Šï¼Œå¾€å¾€å¿½è§†äº†ç»“æœèƒŒåå¯è§£é‡Šæ¨ç†è¿‡ç¨‹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•æ¶µç›–äº†å¹¿æ³›çš„æ—¶é—´ç²’åº¦ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°LLMåœ¨å¯è§£é‡Šæ—¶åºæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒLLMåœ¨ä»…ä¾èµ–æ–‡æœ¬ä¿¡æ¯æ—¶ï¼Œå¾ˆéš¾æä¾›ä»¤äººä¿¡æœçš„è§£é‡Šã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†GETERï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ç»“æ„æ„ŸçŸ¥ç”Ÿæˆæ¡†æ¶ï¼Œå®ƒå°†å›¾ç»“æ„ä¸æ–‡æœ¬ç›¸ç»“åˆï¼Œç”¨äºå¯è§£é‡Šçš„æ—¶åºæ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ©ç”¨æ—¶åºçŸ¥è¯†å›¾è°±å¼€å‘äº†ä¸€ä¸ªæ—¶åºç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨å¯ä»¥æ•è·æŸ¥è¯¢çš„ç»“æ„ä¿¡æ¯ã€‚éšåï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç»“æ„æ–‡æœ¬å‰ç¼€é€‚é…å™¨ï¼Œå°†å›¾ç»“æ„ç‰¹å¾æ˜ å°„åˆ°æ–‡æœ¬åµŒå…¥ç©ºé—´ã€‚æœ€åï¼ŒLLMé€šè¿‡æ— ç¼èåˆè½¯å›¾ä»¤ç‰Œä¸æŒ‡ä»¤è°ƒæ•´æç¤ºä»¤ç‰Œæ¥ç”Ÿæˆè§£é‡Šæ–‡æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGETERè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ŒåŒæ—¶åœ¨å±•ç¤ºå…¶æœ‰æ•ˆæ€§æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/carryTatum/GETER%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/carryTatum/GETERæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15245v1">PDF</a> In Findings of the Association for Computational Linguistics: ACL   2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¶åºæ¨ç†ä¸Šå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†ç°æœ‰ç ”ç©¶å¤šä¾§é‡äºæ€§èƒ½æå‡ï¼Œå¿½è§†äº†ç»“æœèƒŒåçš„å¯è§£é‡Šæ¨ç†è¿‡ç¨‹ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å¹¿æ³›çš„æ—¶é—´ç²’åº¦ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°LLMåœ¨å¯è§£é‡Šæ—¶åºæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚å‘ç°LLMåœ¨ä»…ä¾èµ–æ–‡æœ¬ä¿¡æ¯æ—¶ï¼Œéš¾ä»¥ç»™å‡ºä»¤äººä¿¡æœçš„è§£é‡Šã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†GETERï¼Œä¸€ä¸ªç»“åˆå›¾ç»“æ„å’Œæ–‡æœ¬çš„æ–°å‹ç»“æ„æ„ŸçŸ¥ç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºå¯è§£é‡Šçš„æ—¶åºæ¨ç†ã€‚é€šè¿‡åˆ©ç”¨æ—¶åºçŸ¥è¯†å›¾è°±å¼€å‘æ—¶åºç¼–ç å™¨æ•æ‰æŸ¥è¯¢çš„ç»“æ„ä¿¡æ¯ï¼Œå¹¶å¼•å…¥ç»“æ„æ–‡æœ¬å‰ç¼€é€‚é…å™¨å°†å›¾ç»“æ„ç‰¹å¾æ˜ å°„åˆ°æ–‡æœ¬åµŒå…¥ç©ºé—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGETERåœ¨è¾¾åˆ°æœ€æ–°æ€§èƒ½çš„åŒæ—¶ï¼Œä¹Ÿå±•ç°å‡ºå…¶æœ‰æ•ˆæ€§å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨æ—¶åºæ¨ç†æ–¹é¢æœ‰å¾ˆå¤§æ½œåŠ›ï¼Œä½†ç°æœ‰çš„ç ”ç©¶æ›´å¤šå…³æ³¨æ€§èƒ½æå‡ï¼Œå¿½è§†äº†ç»“æœçš„è§£é‡Šæ€§ã€‚</li>
<li>æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºå‡†æµ‹è¯•æ¥è¯„ä¼°LLMåœ¨å¯è§£é‡Šæ—¶åºæ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œè¦†ç›–å¹¿æ³›çš„æ—¶é—´ç²’åº¦ã€‚</li>
<li>LLMåœ¨ä»…ä¾èµ–æ–‡æœ¬ä¿¡æ¯æ—¶ï¼Œè§£é‡Šèƒ½åŠ›æœ‰å¾…æé«˜ã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GETERæ¡†æ¶ï¼Œç»“åˆäº†å›¾ç»“æ„å’Œæ–‡æœ¬ä¿¡æ¯ã€‚</li>
<li>GETERåˆ©ç”¨æ—¶åºçŸ¥è¯†å›¾è°±å¼€å‘æ—¶åºç¼–ç å™¨ï¼Œæ•æ‰æŸ¥è¯¢çš„ç»“æ„ä¿¡æ¯ã€‚</li>
<li>GETERé€šè¿‡ç»“æ„æ–‡æœ¬å‰ç¼€é€‚é…å™¨å°†å›¾ç»“æ„ç‰¹å¾èå…¥æ–‡æœ¬åµŒå…¥ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15245">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a0a5ac2a83a24bafee10563ac86628f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05f06549f2567246aba0e43a751b75f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c411766391cbf938f458d0c475146de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78bb4170457466c8c5677e316ca90456.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ConspEmoLLM-v2-A-robust-and-stable-model-to-detect-sentiment-transformed-conspiracy-theories"><a href="#ConspEmoLLM-v2-A-robust-and-stable-model-to-detect-sentiment-transformed-conspiracy-theories" class="headerlink" title="ConspEmoLLM-v2: A robust and stable model to detect   sentiment-transformed conspiracy theories"></a>ConspEmoLLM-v2: A robust and stable model to detect   sentiment-transformed conspiracy theories</h2><p><strong>Authors:Zhiwei Liu, Paul Thompson, Jiaqi Rong, Sophia Ananiadou</strong></p>
<p>Despite the many benefits of large language models (LLMs), they can also cause harm, e.g., through automatic generation of misinformation, including conspiracy theories. Moreover, LLMs can also â€˜â€™disguiseâ€™â€™ conspiracy theories by altering characteristic textual features, e.g., by transforming their typically strong negative emotions into a more positive tone. Although several studies have proposed automated conspiracy theory detection methods, they are usually trained using human-authored text, whose features can vary from LLM-generated text. Furthermore, several conspiracy detection models, including the previously proposed ConspEmoLLM, rely heavily on the typical emotional features of human-authored conspiracy content. As such, intentionally disguised content may evade detection. To combat such issues, we firstly developed an augmented version of the ConDID conspiracy detection dataset, ConDID-v2, which supplements human-authored conspiracy tweets with versions rewritten by an LLM to reduce the negativity of their original sentiment. The quality of the rewritten tweets was verified by combining human and LLM-based assessment. We subsequently used ConDID-v2 to train ConspEmoLLM-v2, an enhanced version of ConspEmoLLM. Experimental results demonstrate that ConspEmoLLM-v2 retains or exceeds the performance of ConspEmoLLM on the original human-authored content in ConDID, and considerably outperforms both ConspEmoLLM and several other baselines when applied to sentiment-transformed tweets in ConDID-v2. The project will be available at <a target="_blank" rel="noopener" href="https://github.com/lzw108/ConspEmoLLM">https://github.com/lzw108/ConspEmoLLM</a>. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ‰å¾ˆå¤šå¥½å¤„ï¼Œä½†å®ƒä»¬ä¹Ÿå¯èƒ½é€ æˆå±å®³ï¼Œä¾‹å¦‚é€šè¿‡è‡ªåŠ¨ç”Ÿæˆé”™è¯¯ä¿¡æ¯ï¼ŒåŒ…æ‹¬é˜´è°‹è®ºã€‚æ­¤å¤–ï¼ŒLLMè¿˜å¯ä»¥é€šè¿‡æ”¹å˜ç‰¹å¾æ–‡æœ¬ç‰¹å¾â€œä¼ªè£…â€é˜´è°‹è®ºï¼Œä¾‹å¦‚ï¼Œå°†é€šå¸¸å¼ºçƒˆçš„è´Ÿé¢æƒ…ç»ªè½¬å˜ä¸ºæ›´ç§¯æçš„è¯­æ°”ã€‚å°½ç®¡å·²æœ‰å‡ é¡¹ç ”ç©¶æå‡ºäº†è‡ªåŠ¨åŒ–çš„é˜´è°‹è®ºæ£€æµ‹æ–¹æ³•ï¼Œä½†å®ƒä»¬é€šå¸¸ä½¿ç”¨äººç±»åˆ›ä½œçš„æ–‡æœ¬è¿›è¡Œè®­ç»ƒï¼Œå…¶ç‰¹å¾å¯èƒ½ä¸LLMç”Ÿæˆçš„æ–‡æœ¬æœ‰æ‰€ä¸åŒã€‚æ­¤å¤–ï¼ŒåŒ…æ‹¬å…ˆå‰æå‡ºçš„ConspEmoLLMåœ¨å†…çš„å‡ ä¸ªé˜´è°‹æ£€æµ‹æ¨¡å‹ï¼Œéƒ½ä¸¥é‡ä¾èµ–äºäººç±»åˆ›ä½œé˜´è°‹å†…å®¹çš„å…¸å‹æƒ…æ„Ÿç‰¹å¾ã€‚å› æ­¤ï¼Œæ•…æ„ä¼ªè£…çš„å†…å®¹å¯èƒ½ä¼šé€ƒé¿æ£€æµ‹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹ConDIDé˜´è°‹æ£€æµ‹æ•°æ®é›†è¿›è¡Œäº†å¢å¼ºç‰ˆå¼€å‘ï¼Œå³ConDID-v2ç‰ˆæœ¬ï¼Œè¯¥ç‰ˆæœ¬è¡¥å……äº†ç”±LLMæ”¹å†™çš„é˜´è°‹æ¨ç‰¹ç‰ˆæœ¬ï¼Œå‡è½»äº†åŸå§‹æƒ…ç»ªçš„æ¶ˆææ€§ã€‚æ”¹å†™æ¨æ–‡çš„å“è´¨æ˜¯é€šè¿‡ç»“åˆäººç±»å’ŒLLMè¯„ä¼°æ¥éªŒè¯çš„ã€‚éšåï¼Œæˆ‘ä»¬ä½¿ç”¨ConDID-v2æ¥è®­ç»ƒå¢å¼ºç‰ˆçš„ConspEmoLLMï¼Œå³ConspEmoLLM-v2ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒConspEmoLLM-v2åœ¨ConDIDä¸­çš„åŸå§‹äººç±»åˆ›ä½œå†…å®¹ä¸Šä¿æŒäº†ä¸ConspEmoLLMç›¸å½“æˆ–æ›´é«˜çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ConDID-v2çš„æƒ…æ„Ÿè½¬æ¢æ¨ç‰¹ä¸Šæ˜æ˜¾ä¼˜äºConspEmoLLMå’Œå…¶ä»–å‡ ä¸ªåŸºå‡†çº¿ã€‚è¯¥é¡¹ç›®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/lzw108/ConspEmoLLM%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/lzw108/ConspEmoLLMä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14917v1">PDF</a> work in progress</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶æœ‰å¾ˆå¤šä¼˜ç‚¹ï¼Œä½†ä¹Ÿå¯èƒ½äº§ç”Ÿå±å®³ï¼Œä¾‹å¦‚è‡ªåŠ¨ç”Ÿæˆé”™è¯¯ä¿¡æ¯ï¼ŒåŒ…æ‹¬é˜´è°‹è®ºã€‚æ­¤å¤–ï¼ŒLLMè¿˜èƒ½é€šè¿‡æ”¹å˜æ–‡æœ¬ç‰¹å¾ï¼ˆå¦‚æ”¹å˜åŸæœ¬å¼ºçƒˆçš„è´Ÿé¢æƒ…ç»ªä¸ºæ›´ç§¯æçš„è¯­è°ƒï¼‰æ¥â€œä¼ªè£…â€é˜´è°‹è®ºã€‚è™½ç„¶å·²æœ‰é’ˆå¯¹é˜´è°‹è®ºè‡ªåŠ¨æ£€æµ‹çš„æ–¹æ³•ï¼Œä½†å®ƒä»¬é€šå¸¸ä½¿ç”¨äººç±»æ’°å†™çš„æ–‡æœ¬è¿›è¡Œè®­ç»ƒï¼Œä¸LLMç”Ÿæˆçš„æ–‡æœ¬ç‰¹å¾å¯èƒ½å­˜åœ¨å·®å¼‚ã€‚å› æ­¤ï¼Œä¼ªè£…è¿‡çš„å†…å®¹å¯èƒ½é€ƒé¿æ£€æµ‹ã€‚ä¸ºåº”å¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ConDIDé˜´è°‹æ£€æµ‹æ•°æ®é›†çš„å¢å¼ºç‰ˆConDID-v2ï¼Œè¯¥æ•°æ®é›†è¡¥å……äº†ç”±LLMæ”¹å†™ä»¥é™ä½åŸå§‹æƒ…ç»ªè´Ÿé¢æ€§çš„é˜´è°‹è®ºæ¨ç‰¹å†…å®¹ã€‚é€šè¿‡ç»“åˆäººç±»å’ŒLLMè¯„ä¼°ï¼Œæˆ‘ä»¬éªŒè¯äº†æ”¹å†™æ¨æ–‡çš„å“è´¨ã€‚éšåä½¿ç”¨ConDID-v2è®­ç»ƒäº†å¢å¼ºç‰ˆConspEmoLLMã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒConspEmoLLM-v2åœ¨ConDIDä¸Šçš„æ€§èƒ½ä¸ConspEmoLLMç›¸å½“æˆ–æ›´ä½³ï¼Œåœ¨æƒ…æ„Ÿè½¬å˜çš„æ¨ç‰¹å†…å®¹ConDID-v2ä¸Šåˆ™æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹å’ŒåŸºçº¿ã€‚è¯¥é¡¹ç›®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/lzw108/ConspEmoLLM">https://github.com/lzw108/ConspEmoLLM</a>å‘å¸ƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯èƒ½è‡ªåŠ¨ç”ŸæˆåŒ…æ‹¬é˜´è°‹è®ºåœ¨å†…çš„é”™è¯¯ä¿¡æ¯ï¼Œé€ æˆå±å®³ã€‚</li>
<li>LLMèƒ½å¤Ÿæ”¹å˜æ–‡æœ¬ç‰¹å¾ï¼Œå¦‚æƒ…ç»ªè¯­è°ƒï¼Œä»¥â€œä¼ªè£…â€é˜´è°‹è®ºå†…å®¹ã€‚</li>
<li>ç°æœ‰çš„é˜´è°‹è®ºæ£€æµ‹æ¨¡å‹ä¸»è¦åŸºäºäººç±»æ’°å†™çš„æ–‡æœ¬è¿›è¡Œè®­ç»ƒï¼Œå¯èƒ½æ— æ³•æœ‰æ•ˆæ£€æµ‹LLMç”Ÿæˆçš„ä¼ªè£…å†…å®¹ã€‚</li>
<li>å¼€å‘äº†ConDID-v2æ•°æ®é›†ï¼ŒåŒ…å«ç”±LLMæ”¹å†™çš„é˜´è°‹è®ºæ¨ç‰¹ï¼Œä»¥æ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­çš„ä¼ªè£…å†…å®¹ã€‚</li>
<li>é€šè¿‡ç»“åˆäººç±»å’ŒLLMè¯„ä¼°ï¼ŒéªŒè¯äº†ConDID-v2æ•°æ®é›†çš„è´¨é‡ã€‚</li>
<li>ä½¿ç”¨ConDID-v2æ•°æ®é›†è®­ç»ƒçš„ConspEmoLLM-v2æ¨¡å‹åœ¨æ£€æµ‹é˜´è°‹è®ºæ–¹é¢æ€§èƒ½ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æƒ…æ„Ÿè½¬å˜çš„æ–‡æœ¬å†…å®¹æ—¶ã€‚</li>
<li>è¯¥é¡¹ç›®å°†å…¬å¼€å‘å¸ƒï¼Œä¾›ç ”ç©¶äººå‘˜ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14917">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eb81f94c53c3b07b485670cb3f14b33d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a4ea4c00f7eb17d3863f6eebf7f82bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d382754298cf3ea5eec6841a591c24c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1849fa62eaf0ed7be36ac0e799879f7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3b9386c732e407c45ab571894bb5a32.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b972a54b71952ec4017468fa68cd17fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13ce09ae9cc2392b00c8ecd4cde3b6c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0743f6b99d94756b7b050c3a7c3bfa2.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TinyV-Reducing-False-Negatives-in-Verification-Improves-RL-for-LLM-Reasoning"><a href="#TinyV-Reducing-False-Negatives-in-Verification-Improves-RL-for-LLM-Reasoning" class="headerlink" title="TinyV: Reducing False Negatives in Verification Improves RL for LLM   Reasoning"></a>TinyV: Reducing False Negatives in Verification Improves RL for LLM   Reasoning</h2><p><strong>Authors:Zhangchen Xu, Yuetai Li, Fengqing Jiang, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Radha Poovendran</strong></p>
<p>Reinforcement Learning (RL) has become a powerful tool for enhancing the reasoning abilities of large language models (LLMs) by optimizing their policies with reward signals. Yet, RLâ€™s success relies on the reliability of rewards, which are provided by verifiers. In this paper, we expose and analyze a widespread problemâ€“false negativesâ€“where verifiers wrongly reject correct model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals that over 38% of model-generated responses suffer from false negatives, where the verifier fails to recognize correct answers. We show, both empirically and theoretically, that these false negatives severely impair RL training by depriving the model of informative gradient signals and slowing convergence. To mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments existing rule-based methods, which dynamically identifies potential false negatives and recovers valid responses to produce more accurate reward estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts pass rates by up to 10% and accelerates convergence relative to the baseline. Our findings highlight the critical importance of addressing verifier false negatives and offer a practical approach to improve RL-based fine-tuning of LLMs. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/uw-nsl/TinyV">https://github.com/uw-nsl/TinyV</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºé€šè¿‡ä¼˜åŒ–ç­–ç•¥å¹¶ä½¿ç”¨å¥–åŠ±ä¿¡å·å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„ä¸€ç§å¼ºå¤§å·¥å…·ã€‚ç„¶è€Œï¼ŒRLçš„æˆåŠŸå–å†³äºå¥–åŠ±çš„å¯é æ€§ï¼Œè¿™äº›å¥–åŠ±ç”±éªŒè¯å™¨æä¾›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ­ç¤ºå¹¶åˆ†æäº†æ™®éå­˜åœ¨çš„é—®é¢˜â€”â€”å‡é˜´æ€§ï¼ˆfalse negativesï¼‰â€”â€”å…¶ä¸­éªŒè¯å™¨é”™è¯¯åœ°æ‹’ç»äº†æ­£ç¡®çš„æ¨¡å‹è¾“å‡ºã€‚æˆ‘ä»¬å¯¹Big-Math-RL-Verifiedæ•°æ®é›†çš„æ·±å…¥ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œè¶…è¿‡38%çš„æ¨¡å‹ç”Ÿæˆå“åº”å­˜åœ¨å‡é˜´æ€§é—®é¢˜ï¼Œå³éªŒè¯å™¨æ— æ³•è¯†åˆ«æ­£ç¡®ç­”æ¡ˆã€‚æˆ‘ä»¬å®è¯å’Œç†è®ºä¸Šéƒ½è¡¨æ˜ï¼Œè¿™äº›å‡é˜´æ€§ä¼šä¸¥é‡æŸå®³RLè®­ç»ƒï¼Œå› ä¸ºä¼šå‰¥å¤ºæ¨¡å‹çš„ä¿¡æ¯åŒ–æ¢¯åº¦ä¿¡å·å¹¶å‡æ…¢æ”¶æ•›é€Ÿåº¦ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†tinyVï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLMçš„è½»é‡çº§éªŒè¯å™¨ï¼Œå®ƒå¢å¼ºäº†ç°æœ‰çš„åŸºäºè§„åˆ™çš„æ–¹æ³•ï¼Œèƒ½å¤ŸåŠ¨æ€è¯†åˆ«æ½œåœ¨çš„å‡é˜´æ€§å¹¶æ¢å¤æœ‰æ•ˆå“åº”ï¼Œä»¥äº§ç”Ÿæ›´å‡†ç¡®çš„å¥–åŠ±ä¼°è®¡ã€‚åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œé›†æˆTinyVçš„é€šè¿‡ç‡æé«˜äº†é«˜è¾¾10%ï¼Œç›¸å¯¹äºåŸºçº¿åŠ é€Ÿäº†æ”¶æ•›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜è§£å†³éªŒè¯å™¨å‡é˜´æ€§çš„å…³é”®é‡è¦æ€§ï¼Œå¹¶æä¾›äº†ä¸€ç§å®ç”¨çš„æ–¹æ³•æ¥æ”¹è¿›åŸºäºRLçš„LLMå¾®è°ƒã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/uw-nsl/TinyV%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/uw-nsl/TinyVè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14625v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€šè¿‡ä¼˜åŒ–ç­–ç•¥å¹¶å€ŸåŠ©å¥–åŠ±ä¿¡å·æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒRLçš„æˆåŠŸä¾èµ–äºå¥–åŠ±çš„å¯é æ€§ï¼Œå¥–åŠ±ç”±éªŒè¯å™¨æä¾›ã€‚æœ¬æ–‡æ­ç¤ºäº†ä¸€ä¸ªæ™®éå­˜åœ¨çš„é—®é¢˜â€”â€”å‡é˜´æ€§é”™è¯¯ï¼Œå³éªŒè¯å™¨é”™è¯¯åœ°æ‹’ç»æ­£ç¡®çš„æ¨¡å‹è¾“å‡ºã€‚é€šè¿‡å¯¹Big-Math-RL-Verifiedæ•°æ®é›†çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°è¶…è¿‡38%çš„æ¨¡å‹ç”Ÿæˆå“åº”å—åˆ°å‡é˜´æ€§é”™è¯¯çš„å½±å“ã€‚è¿™äº›å‡é˜´æ€§é”™è¯¯ä¸¥é‡é˜»ç¢äº†RLè®­ç»ƒï¼Œå‰¥å¤ºäº†æ¨¡å‹çš„ä¿¡æ¯æ¢¯åº¦ä¿¡å·å¹¶å‡ç¼“äº†æ”¶æ•›é€Ÿåº¦ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TinyVï¼Œä¸€ç§åŸºäºLLMçš„è½»é‡çº§éªŒè¯å™¨ï¼Œå®ƒèƒ½å¤Ÿå¢å¼ºç°æœ‰çš„åŸºäºè§„åˆ™çš„æ–¹æ³•ï¼ŒåŠ¨æ€è¯†åˆ«æ½œåœ¨çš„å‡é˜´æ€§é”™è¯¯å¹¶æ¢å¤æœ‰æ•ˆçš„å“åº”ï¼Œä»è€Œæä¾›æ›´å‡†ç¡®çš„å¥–åŠ±ä¼°è®¡ã€‚åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œé›†æˆTinyVçš„é€šè¿‡ç‡æé«˜äº†10%ï¼Œå¹¶ä¸”ç›¸å¯¹äºåŸºçº¿åŠ é€Ÿäº†æ”¶æ•›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç”¨äºæå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>éªŒè¯å™¨åœ¨RLè®­ç»ƒä¸­è‡³å…³é‡è¦ï¼Œä½†å…¶å­˜åœ¨å‡é˜´æ€§é”™è¯¯çš„é—®é¢˜ã€‚</li>
<li>å‡é˜´æ€§é”™è¯¯æŒ‡çš„æ˜¯éªŒè¯å™¨é”™è¯¯åœ°æ‹’ç»æ­£ç¡®çš„æ¨¡å‹è¾“å‡ºã€‚</li>
<li>åœ¨Big-Math-RL-Verifiedæ•°æ®é›†çš„ç ”ç©¶ä¸­å‘ç°ï¼Œè¶…è¿‡38%çš„æ¨¡å‹ç”Ÿæˆå“åº”å—åˆ°å‡é˜´æ€§é”™è¯¯çš„å½±å“ã€‚</li>
<li>å‡é˜´æ€§é”™è¯¯ä¼šä¸¥é‡é˜»ç¢RLè®­ç»ƒï¼Œå¯¼è‡´æ¨¡å‹æ”¶æ•›é€Ÿåº¦å‡æ…¢ã€‚</li>
<li>TinyVæ˜¯ä¸€ç§åŸºäºLLMçš„è½»é‡çº§éªŒè¯å™¨ï¼Œèƒ½å¤Ÿå¢å¼ºç°æœ‰æ–¹æ³•å¹¶åŠ¨æ€è¯†åˆ«å‡é˜´æ€§é”™è¯¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14625">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-30e78e92622af442a94c03aa27a1c2ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45315ad641603139bd5ff244f95ac9c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a08c30a5d241b44320c26435bfbb837.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b98fa5864234d9a3f6c406da5a995e97.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Video-GPT-via-Next-Clip-Diffusion"><a href="#Video-GPT-via-Next-Clip-Diffusion" class="headerlink" title="Video-GPT via Next Clip Diffusion"></a>Video-GPT via Next Clip Diffusion</h2><p><strong>Authors:Shaobin Zhuang, Zhipeng Huang, Ying Zhang, Fangyikang Wang, Canmiao Fu, Binxin Yang, Chong Sun, Chen Li, Yali Wang</strong></p>
<p>GPT has shown its remarkable success in natural language processing. However, the language sequence is not sufficient to describe spatial-temporal details in the visual world. Alternatively, the video sequence is good at capturing such details. Motivated by this fact, we propose a concise Video-GPT in this paper by treating video as new language for visual world modeling. By analogy to next token prediction in GPT, we introduce a novel next clip diffusion paradigm for pretraining Video-GPT. Different from the previous works, this distinct paradigm allows Video-GPT to tackle both short-term generation and long-term prediction, by autoregressively denoising the noisy clip according to the clean clips in the history. Extensive experiments show our Video-GPT achieves the state-of-the-art performance on video prediction, which is the key factor towards world modeling (Physics-IQ Benchmark: Video-GPT 34.97 vs. Kling 23.64 vs. Wan 20.89). Moreover, it can be well adapted on 6 mainstream video tasks in both video generation and understanding, showing its great generalization capacity in downstream. The project page is at <a target="_blank" rel="noopener" href="https://zhuangshaobin.github.io/Video-GPT.github.io/">https://zhuangshaobin.github.io/Video-GPT.github.io/</a>. </p>
<blockquote>
<p>GPTåœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œè¯­è¨€åºåˆ—ä¸è¶³ä»¥æè¿°è§†è§‰ä¸–ç•Œä¸­çš„æ—¶ç©ºç»†èŠ‚ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè§†é¢‘åºåˆ—æ›´æ“…é•¿æ•æ‰è¿™äº›ç»†èŠ‚ã€‚åŸºäºè¿™ä¸€äº‹å®ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æå‡ºäº†ä¸€ç§ç®€æ´çš„è§†é¢‘GPTï¼Œå°†è§†é¢‘è§†ä¸ºè§†è§‰ä¸–ç•Œå»ºæ¨¡çš„æ–°è¯­è¨€ã€‚é€šè¿‡æ¨¡æ‹ŸGPTä¸­çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„ä¸‹ä¸€ä¸ªå‰ªè¾‘æ‰©æ•£æ¨¡å¼æ¥é¢„è®­ç»ƒè§†é¢‘GPTã€‚ä¸ä¹‹å‰çš„å·¥ä½œä¸åŒï¼Œè¿™ç§ç‹¬ç‰¹çš„æ¨¡å¼å…è®¸è§†é¢‘GPTè§£å†³çŸ­æœŸç”Ÿæˆå’Œé•¿æœŸé¢„æµ‹é—®é¢˜ï¼Œæ ¹æ®å†å²ä¸­çš„å¹²å‡€å‰ªè¾‘å¯¹å™ªå£°å‰ªè¾‘è¿›è¡Œè‡ªå›å½’å»å™ªã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è§†é¢‘GPTåœ¨è§†é¢‘é¢„æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ï¼Œè¿™æ˜¯å®ç°ä¸–ç•Œå»ºæ¨¡çš„å…³é”®å› ç´ ï¼ˆPhysics-IQ Benchmarkï¼šè§†é¢‘GPT 34.97 vs. Kling 23.64 vs. Wan 20.89ï¼‰ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥å¾ˆå¥½åœ°é€‚åº”è§†é¢‘ç”Ÿæˆå’Œç†è§£æ–¹é¢çš„6ç§ä¸»æµä»»åŠ¡ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å·¨å¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚é¡¹ç›®é¡µé¢ä½äº<a target="_blank" rel="noopener" href="https://zhuangshaobin.github.io/Video%E2%80%9CGPT.github.io/">https://zhuangshaobin.github.io/Video-GPT.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12489v2">PDF</a> 22 pages, 12 figures, 18 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„Video-GPTæ¨¡å‹ï¼Œå®ƒå°†è§†é¢‘è§†ä¸ºä¸€ç§æ–°çš„è¯­è¨€æ¥è¿›è¡Œè§†è§‰ä¸–ç•Œå»ºæ¨¡ã€‚é€šè¿‡å¼•å…¥ä¸‹ä¸€ä¸ªå‰ªè¾‘æ‰©æ•£èŒƒå¼è¿›è¡Œé¢„è®­ç»ƒï¼ŒVideo-GPTä¸ä»…èƒ½å¤Ÿå¤„ç†çŸ­æœŸç”Ÿæˆï¼Œè¿˜èƒ½è¿›è¡Œé•¿æœŸé¢„æµ‹ã€‚å®éªŒè¡¨æ˜ï¼ŒVideo-GPTåœ¨è§†é¢‘é¢„æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œå¹¶ä¸”åœ¨å…­å¤§ä¸»æµè§†é¢‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„é€‚åº”æ€§ï¼Œå±•ç°å‡ºå…¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Video-GPTæ¨¡å‹å°†è§†é¢‘è§†ä¸ºä¸€ç§æ–°è¯­è¨€è¿›è¡Œè§†è§‰ä¸–ç•Œå»ºæ¨¡ã€‚</li>
<li>å¼•å…¥ä¸‹ä¸€ä¸ªå‰ªè¾‘æ‰©æ•£èŒƒå¼è¿›è¡Œé¢„è®­ç»ƒï¼Œå…è®¸å¤„ç†çŸ­æœŸç”Ÿæˆå’Œé•¿æœŸé¢„æµ‹ã€‚</li>
<li>Video-GPTåœ¨è§†é¢‘é¢„æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒVideo-GPTè¡¨ç°æ›´ä¼˜ï¼Œå¦‚Physics-IQ Benchmarkæµ‹è¯•ä¸­å¾—åˆ†é«˜äºKlingå’ŒWanã€‚</li>
<li>Video-GPTå¯é€‚åº”å…­å¤§ä¸»æµè§†é¢‘ä»»åŠ¡ï¼ŒåŒ…æ‹¬è§†é¢‘ç”Ÿæˆå’Œç†è§£ã€‚</li>
<li>Video-GPTå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12489">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f74727eed7e469edcaa5b23e2696af09.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6159bc24c2e0e6f729f8be17cf4d5d87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-522041452fd6400da1ad1c1ec2eba23b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edc96a36afedd6103677e14942e896ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35976ec655681789cdc85f7756300682.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89c59c7ad8a05fc995714b1b3586f696.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1cb8d940f386ab1b8d348eec2067304d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Instruction-Tuning-Data-Synthesis-from-Scratch-via-Web-Reconstruction"><a href="#Instruction-Tuning-Data-Synthesis-from-Scratch-via-Web-Reconstruction" class="headerlink" title="Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction"></a>Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction</h2><p><strong>Authors:Yuxin Jiang, Yufei Wang, Chuhan Wu, Xinyi Dai, Yan Xu, Weinan Gan, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Wei Wang</strong></p>
<p>The improvement of LLMsâ€™ instruction-following capabilities depends critically on the availability of high-quality instruction-response pairs. While existing automatic data synthetic methods alleviate the burden of manual curation, they often rely heavily on either the quality of seed data or strong assumptions about the structure and content of web documents. To tackle these challenges, we propose Web Reconstruction (WebR), a fully automated framework for synthesizing high-quality instruction-tuning (IT) data directly from raw web documents with minimal assumptions. Leveraging the inherent diversity of raw web content, we conceptualize web reconstruction as an instruction-tuning data synthesis task via a novel dual-perspective paradigmâ€“Web as Instruction and Web as Responseâ€“where each web document is designated as either an instruction or a response to trigger the reconstruction process. Comprehensive experiments show that datasets generated by WebR outperform state-of-the-art baselines by up to 16.65% across four instruction-following benchmarks. Notably, WebR demonstrates superior compatibility, data efficiency, and scalability, enabling enhanced domain adaptation with minimal effort. The data and code are publicly available at <a target="_blank" rel="noopener" href="https://github.com/YJiangcm/WebR">https://github.com/YJiangcm/WebR</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ‰§è¡ŒæŒ‡ä»¤èƒ½åŠ›çš„æ”¹è¿›å…³é”®åœ¨äºé«˜è´¨é‡æŒ‡ä»¤å“åº”å¯¹çš„å¯ç”¨æ€§ã€‚ç°æœ‰çš„è‡ªåŠ¨æ•°æ®åˆæˆæ–¹æ³•å‡è½»äº†æ‰‹åŠ¨æ•´ç†çš„è´Ÿæ‹…ï¼Œä½†å®ƒä»¬é€šå¸¸è¦ä¹ˆä¾èµ–äºç§å­æ•°æ®çš„è´¨é‡ï¼Œè¦ä¹ˆä¾èµ–äºå¯¹ç½‘é¡µæ–‡æ¡£ç»“æ„å’Œå†…å®¹çš„å¼ºçƒˆå‡è®¾ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Web Reconstructionï¼ˆWebRï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨çš„æ¡†æ¶ï¼Œå¯ä»¥ä»åŸå§‹ç½‘é¡µæ–‡æ¡£ç›´æ¥åˆæˆé«˜è´¨é‡æŒ‡ä»¤è°ƒæ•´ï¼ˆITï¼‰æ•°æ®ï¼Œå¹¶ä¸”å‡è®¾æœ€å°‘ã€‚æˆ‘ä»¬åˆ©ç”¨åŸå§‹ç½‘é¡µå†…å®¹çš„å›ºæœ‰å¤šæ ·æ€§ï¼Œé€šè¿‡ä¸€ç§æ–°é¢–çš„åŒè§†è§’æ¨¡å¼ï¼Œå°†ç½‘ç»œé‡å»ºæ¦‚å¿µåŒ–ä¸ºæŒ‡ä»¤è°ƒæ•´æ•°æ®åˆæˆä»»åŠ¡â€”â€”â€œç½‘ç»œä½œä¸ºæŒ‡ä»¤â€å’Œâ€œç½‘ç»œä½œä¸ºå“åº”â€ï¼Œæ¯ä¸ªç½‘ç»œæ–‡æ¡£éƒ½è¢«æŒ‡å®šä¸ºæŒ‡ä»¤æˆ–å“åº”ä»¥è§¦å‘é‡å»ºè¿‡ç¨‹ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒWebRç”Ÿæˆçš„æ•°æ®é›†åœ¨å››ä¸ªæ‰§è¡ŒæŒ‡ä»¤åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºæœ€æ–°åŸºçº¿ï¼Œæœ€é«˜æå‡è¾¾16.65%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒWebRè¡¨ç°å‡ºå‡ºè‰²çš„å…¼å®¹æ€§ã€æ•°æ®æ•ˆç‡å’Œå¯æ‰©å±•æ€§ï¼Œå¯å®ç°ä½æˆæœ¬çš„é¢†åŸŸé€‚åº”ã€‚ç›¸å…³æ•°æ®å’Œä»£ç å…¬å¼€å¯ç”¨ï¼Œåœ°å€æ˜¯<a target="_blank" rel="noopener" href="https://github.com/YJiangcm/WebR%E3%80%82">https://github.com/YJiangcm/WebRã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15573v2">PDF</a> 16 pages, 11 figures, 9 tables. ACL 2025 camera-ready version</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„æŒ‡å¯¼æ€§å“åº”èƒ½åŠ›æå‡ä¾èµ–äºé«˜è´¨é‡æŒ‡å¯¼æŒ‡ä»¤å’Œå“åº”å¯¹çš„æ„å»ºã€‚å°½ç®¡ç°æœ‰çš„è‡ªåŠ¨åŒ–æ•°æ®åˆæˆæ–¹æ³•èƒ½å¤Ÿå‡è½»äººå·¥æ•´ç†çš„è´Ÿæ‹…ï¼Œä½†å®ƒä»¬å¸¸å¸¸ä¾èµ–ç§å­æ•°æ®çš„è´¨é‡å’Œå‡è®¾æ–‡æ¡£ç»“æ„ç­‰å±€é™æ€§è¾ƒå¤§ã€‚æœ¬ç ”ç©¶æå‡ºäº†Webé‡å»ºï¼ˆWebRï¼‰æ–¹æ³•ï¼Œå®ƒèƒ½å¤Ÿåœ¨æœ€å°åŒ–å‡è®¾çš„åŸºç¡€ä¸Šç›´æ¥ç”±åŸå§‹æ–‡æ¡£è‡ªåŠ¨åˆæˆé«˜è´¨é‡æŒ‡å¯¼æ€§è°ƒä¼˜æ•°æ®ã€‚é€šè¿‡åˆ©ç”¨åŸå§‹ç½‘é¡µå†…å®¹çš„å›ºæœ‰å¤šæ ·æ€§ï¼Œæœ¬ç ”ç©¶å°†é‡å»ºè¿‡ç¨‹æ¦‚å¿µåŒ–ä¸ºä¸€ä¸ªæŒ‡ä»¤æ€§è°ƒä¼˜æ•°æ®åˆæˆä»»åŠ¡ï¼Œé‡‡ç”¨æ–°é¢–çš„â€œåŒé‡è§†è§’â€æ¨¡å¼â€”â€”å°†ç½‘é¡µè§†ä¸ºæŒ‡ä»¤å’Œå“åº”â€”â€”æ¯ä¸ªç½‘é¡µæ–‡æ¡£éƒ½è¢«æŒ‡å®šä¸ºæŒ‡ä»¤æˆ–å“åº”ä»¥è§¦å‘é‡å»ºè¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼ŒWebRç”Ÿæˆçš„æ•°æ®åº“åœ¨å››é¡¹æŒ‡ä»¤è·Ÿè¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œé«˜å‡ºçº¦è¾¾ç™¾åˆ†ä¹‹åå…­ç‚¹å…­äº”ï¼Œæ˜¾è‘—åœ°å±•ç¤ºäº†WebRçš„é«˜æ•ˆé€‚ç”¨æ€§ã€æ•°æ®æ•ˆç›ŠåŠæ‰©å±•ä¼˜åŠ¿ã€‚æˆ‘ä»¬å›¢é˜Ÿå·²ç»åœ¨ç½‘å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/YJiangcm/WebR%E5%85%AC%E5%BC%80%E4%BA%86%E6%95%B0%E%E6%8D%AE%E4%BB%A3%E7%A0%81%E4%BB%A5%E4%BE%BF%E7%A0%B9%E5%AD%A6%E7%94%A8%E3%80%82">https://github.com/YJiangcm/WebRå…¬å¼€äº†æ•°æ®å’Œä»£ç ä»¥ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</a> </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMsçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›æ”¹å–„ä¾èµ–äºé«˜è´¨é‡æŒ‡ä»¤å“åº”å¯¹çš„å¯ç”¨æ€§ã€‚</li>
<li>ç°æœ‰è‡ªåŠ¨æ•°æ®åˆæˆæ–¹æ³•è™½ç„¶å‡è½»æ‰‹åŠ¨æ•´ç†è´Ÿæ‹…ï¼Œä½†ä»å­˜åœ¨ä¾èµ–ç§å­æ•°æ®å’Œæ–‡æ¡£ç»“æ„å‡è®¾çš„é—®é¢˜ã€‚</li>
<li>æå‡ºWebé‡å»ºï¼ˆWebRï¼‰æ¡†æ¶ï¼Œå¯ç›´æ¥ä»åŸå§‹ç½‘é¡µå†…å®¹è‡ªåŠ¨åŒ–åˆæˆé«˜è´¨é‡æŒ‡ä»¤è°ƒä¼˜æ•°æ®ã€‚</li>
<li>WebRåˆ©ç”¨åŸå§‹ç½‘é¡µå†…å®¹çš„å¤šæ ·æ€§ï¼Œé€šè¿‡åŒé‡è§†è§’æ¨¡å¼ï¼ˆç½‘é¡µä½œä¸ºæŒ‡ä»¤å’Œå“åº”ï¼‰è¿›è¡Œé‡å»ºã€‚</li>
<li>WebRç”Ÿæˆçš„æ•°æ®åº“åœ¨å››é¡¹æŒ‡ä»¤è·Ÿè¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œæœ€é«˜æå‡è¾¾ç™¾åˆ†ä¹‹åå…­ç‚¹å…­äº”ã€‚</li>
<li>WebRå…·æœ‰é«˜æ•ˆé€‚ç”¨æ€§ã€æ•°æ®æ•ˆç›ŠåŠæ‰©å±•ä¼˜åŠ¿ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15573">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-38c0e4a8b1d2b2bf91bbbf44f9788669.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-32c7e3f1818793be1552b8586766c76d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-717b518733059218efd7ba560fbb6e7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-330cc88266b2f6728a350daa25b9534b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b242c5a86175517cab8d0ec3a9d69461.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Unifying-Search-and-Recommendation-A-Generative-Paradigm-Inspired-by-Information-Theory"><a href="#Unifying-Search-and-Recommendation-A-Generative-Paradigm-Inspired-by-Information-Theory" class="headerlink" title="Unifying Search and Recommendation: A Generative Paradigm Inspired by   Information Theory"></a>Unifying Search and Recommendation: A Generative Paradigm Inspired by   Information Theory</h2><p><strong>Authors:Jujia Zhao, Wenjie Wang, Chen Xu, Xiuying Chen, Zhaochun Ren, Suzan Verberne</strong></p>
<p>Recommender systems and search engines serve as foundational elements of online platforms, with the former delivering information proactively and the latter enabling users to seek information actively. Unifying both tasks in a shared model is promising since it can enhance user modeling and item understanding. Previous approaches mainly follow a discriminative paradigm, utilizing shared encoders to process input features and task-specific heads to perform each task. However, this paradigm encounters two key challenges: gradient conflict and manual design complexity. From the information theory perspective, these challenges potentially both stem from the same issue â€“ low mutual information between the input features and task-specific outputs during the optimization process.   To tackle these issues, we propose GenSR, a novel generative paradigm for unifying search and recommendation (S&amp;R), which leverages task-specific prompts to partition the modelâ€™s parameter space into subspaces, thereby enhancing mutual information. To construct effective subspaces for each task, GenSR first prepares informative representations for each subspace and then optimizes both subspaces in one unified model. Specifically, GenSR consists of two main modules: (1) Dual Representation Learning, which independently models collaborative and semantic historical information to derive expressive item representations; and (2) S&amp;R Task Unifying, which utilizes contrastive learning together with instruction tuning to generate task-specific outputs effectively. Extensive experiments on two public datasets show GenSR outperforms state-of-the-art methods across S&amp;R tasks. Our work introduces a new generative paradigm compared with previous discriminative methods and establishes its superiority from the mutual information perspective. </p>
<blockquote>
<p>æ¨èç³»ç»Ÿå’Œæœç´¢å¼•æ“ä½œä¸ºåœ¨çº¿å¹³å°çš„åŸºç¡€å…ƒç´ ï¼Œå‰è€…ä¸»åŠ¨æä¾›ä¿¡æ¯ï¼Œåè€…ä½¿ç”¨æˆ·èƒ½å¤Ÿä¸»åŠ¨å¯»æ‰¾ä¿¡æ¯ã€‚åœ¨å…±äº«æ¨¡å‹ä¸­ç»Ÿä¸€è¿™ä¸¤é¡¹ä»»åŠ¡æ˜¯æœ‰å‰æ™¯çš„ï¼Œå› ä¸ºå®ƒå¯ä»¥å¢å¼ºç”¨æˆ·å»ºæ¨¡å’Œç‰©å“ç†è§£ã€‚ä¹‹å‰çš„æ–¹æ³•ä¸»è¦éµå¾ªåˆ¤åˆ«èŒƒå¼ï¼Œä½¿ç”¨å…±äº«ç¼–ç å™¨å¤„ç†è¾“å…¥ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨ä»»åŠ¡ç‰¹å®šå¤´æ‰§è¡Œæ¯ä¸ªä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™ç§èŒƒå¼é¢ä¸´ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šæ¢¯åº¦å†²çªå’Œæ‰‹åŠ¨è®¾è®¡å¤æ‚æ€§ã€‚ä»ä¿¡æ¯ç†è®ºçš„è§’åº¦æ¥çœ‹ï¼Œè¿™äº›æŒ‘æˆ˜éƒ½æºäºåŒæ ·çš„é—®é¢˜ï¼Œå³åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­è¾“å…¥ç‰¹å¾å’Œä»»åŠ¡ç‰¹å®šè¾“å‡ºä¹‹é—´çš„äº’ä¿¡æ¯è¾ƒä½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GenSRï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€æœç´¢å’Œæ¨èï¼ˆS&amp;Rï¼‰çš„æ–°å‹ç”ŸæˆèŒƒå¼ã€‚å®ƒé€šè¿‡ç‰¹å®šä»»åŠ¡çš„æç¤ºæ¥å°†æ¨¡å‹å‚æ•°ç©ºé—´åˆ’åˆ†ä¸ºå­ç©ºé—´ï¼Œä»è€Œæé«˜äº’ä¿¡æ¯ã€‚ä¸ºäº†ä¸ºæ¯ä¸ªä»»åŠ¡æ„å»ºæœ‰æ•ˆçš„å­ç©ºé—´ï¼ŒGenSRé¦–å…ˆä¸ºæ¯ä¸ªå­ç©ºé—´å‡†å¤‡ä¿¡æ¯è¡¨ç¤ºï¼Œç„¶ååœ¨ç»Ÿä¸€æ¨¡å‹ä¸­ä¼˜åŒ–è¿™äº›å­ç©ºé—´ã€‚å…·ä½“æ¥è¯´ï¼ŒGenSRç”±ä¸¤ä¸ªä¸»è¦æ¨¡å—ç»„æˆï¼šï¼ˆ1ï¼‰åŒè¡¨ç¤ºå­¦ä¹ ï¼Œå®ƒç‹¬ç«‹åœ°å»ºæ¨¡ååŒå’Œè¯­ä¹‰å†å²ä¿¡æ¯ï¼Œä»¥å¾—å‡ºè¡¨è¾¾æ€§çš„ç‰©å“è¡¨ç¤ºï¼›ï¼ˆ2ï¼‰S&amp;Rä»»åŠ¡ç»Ÿä¸€ï¼Œå®ƒåˆ©ç”¨å¯¹æ¯”å­¦ä¹ ä¸æŒ‡ä»¤å¾®è°ƒæ¥æœ‰æ•ˆåœ°ç”Ÿæˆä»»åŠ¡ç‰¹å®šè¾“å‡ºã€‚åœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGenSRåœ¨S&amp;Rä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ä¹‹å‰çš„åˆ¤åˆ«æ–¹æ³•ç›¸æ¯”å¼•å…¥äº†ä¸€ç§æ–°çš„ç”ŸæˆèŒƒå¼ï¼Œå¹¶ä»äº’ä¿¡æ¯è§’åº¦è¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06714v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†æ¨èç³»ç»Ÿå’Œæœç´¢å¼•æ“çš„ç»Ÿä¸€æ¨¡å‹ï¼ŒæŒ‡å‡ºä¼ ç»Ÿæ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºä¸€ç§åŸºäºç”Ÿæˆå¼çš„æ–°èŒƒå¼GenSRã€‚GenSRé€šè¿‡ä»»åŠ¡ç‰¹å®šæç¤ºå°†æ¨¡å‹å‚æ•°ç©ºé—´åˆ’åˆ†ä¸ºå­ç©ºé—´ï¼Œæé«˜äº’ä¿¡æ¯æ¥è§£å†³æŒ‘æˆ˜ã€‚GenSRåŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šDual Representation Learning å’Œ S&amp;R Task Unifyingã€‚å®éªŒè¡¨æ˜ï¼ŒGenSRåœ¨ç»Ÿä¸€æ¨¡å‹ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨èç³»ç»Ÿå’Œæœç´¢å¼•æ“æ˜¯ä¿¡æ¯å¹³å°çš„åŸºç¡€ç»„æˆéƒ¨åˆ†ï¼Œå‰è€…ä¸»åŠ¨æä¾›ä¿¡æ¯ï¼Œåè€…ä½¿ç”¨æˆ·èƒ½å¤Ÿä¸»åŠ¨å¯»æ‰¾ä¿¡æ¯ã€‚</li>
<li>å°†ä¸¤è€…ç»Ÿä¸€åœ¨æ¨¡å‹ä¸­å¯ä»¥æé«˜ç”¨æˆ·å»ºæ¨¡å’Œç‰©å“ç†è§£çš„èƒ½åŠ›ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦é‡‡ç”¨åˆ¤åˆ«å¼èŒƒå¼ï¼Œå­˜åœ¨æ¢¯åº¦å†²çªå’Œæ‰‹åŠ¨è®¾è®¡å¤æ‚æ€§ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>è¿™äº›æŒ‘æˆ˜å¯èƒ½æºäºä¼˜åŒ–è¿‡ç¨‹ä¸­çš„ä½äº’ä¿¡æ¯é—®é¢˜ã€‚</li>
<li>GenSRæ˜¯ä¸€ç§åŸºäºç”Ÿæˆå¼çš„æ–°èŒƒå¼ï¼Œé€šè¿‡ä»»åŠ¡ç‰¹å®šæç¤ºåˆ’åˆ†æ¨¡å‹å‚æ•°ç©ºé—´ï¼Œæé«˜äº’ä¿¡æ¯æ¥è§£å†³æŒ‘æˆ˜ã€‚</li>
<li>GenSRåŒ…æ‹¬Dual Representation Learningå’ŒS&amp;R Task Unifyingä¸¤ä¸ªä¸»è¦æ¨¡å—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06714">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ecfcaa21cff6713b275d67b138abbce0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-964e592eb0aa59da1889cff8d0d32957.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a1e4445c7e2b20190079b37a3ee9434.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5b9fa667f693c21c9a9b333e2ca3a5ec.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ASMA-Tune-Unlocking-LLMsâ€™-Assembly-Code-Comprehension-via-Structural-Semantic-Instruction-Tuning"><a href="#ASMA-Tune-Unlocking-LLMsâ€™-Assembly-Code-Comprehension-via-Structural-Semantic-Instruction-Tuning" class="headerlink" title="ASMA-Tune: Unlocking LLMsâ€™ Assembly Code Comprehension via   Structural-Semantic Instruction Tuning"></a>ASMA-Tune: Unlocking LLMsâ€™ Assembly Code Comprehension via   Structural-Semantic Instruction Tuning</h2><p><strong>Authors:Xinyi Wang, Jiashui Wang, Jinbo Su, Ke Wang, Peng Chen, Yanming Liu, Long Liu, Xiang Li, Yangdong Wang, Qiyuan Chen, Rongze Chen, Chunfu Jia</strong></p>
<p>Assembly code analysis and comprehension play critical roles in applications like reverse engineering, yet they face substantial challenges due to low information density and a lack of explicit syntactic structures. While traditional masked language modeling (MLM) approaches do not explicitly focus on natural language interaction, emerging decoder-focused large language models (LLMs) demonstrate partial success in binary analysis yet remain underexplored for holistic comprehension. We present Assembly Augmented Tuning, an end-to-end structural-semantic instruction tuning framework that synergizes encoder architecture with decoder-based LLMs through a projector module, where the assembly encoder extracts hardware-level structural features, the projector bridges representations with the semantic space, and the instruction-tuned LLM preserves natural language capabilities. Experimental results demonstrate three key advantages: (1) State-of-the-art performance in assembly comprehension with +39.7% Recall@1 and +17.8% MRR improvements over GPT-4-Turbo, (2) Consistent enhancements across base models (24.6-107.4% Recall@1 and 15.2-106.3% MRR on Qwen2.5-Coder, Deepseek-Coder and CodeLlama variants), and (3) Superior instruction-following capabilities (41.5%-118% improvements) with controlled code generation degradation (-8.9% to -35% across architectures). </p>
<blockquote>
<p>æ±‡ç¼–ä»£ç åˆ†æå’Œç†è§£åœ¨é€†å‘å·¥ç¨‹ç­‰åº”ç”¨ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œä½†ç”±äºä¿¡æ¯å¯†åº¦ä½å’Œç¼ºä¹æ˜ç¡®çš„è¯­æ³•ç»“æ„ï¼Œå®ƒä»¬é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ã€‚è™½ç„¶ä¼ ç»Ÿçš„æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰æ–¹æ³•å¹¶ä¸ä¸“æ³¨äºè‡ªç„¶è¯­è¨€äº¤äº’ï¼Œä½†æ–°å…´çš„ä»¥è§£ç å™¨ä¸ºé‡ç‚¹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨äºŒè¿›åˆ¶åˆ†ææ–¹é¢å–å¾—äº†ä¸€å®šçš„æˆåŠŸï¼Œä½†åœ¨æ•´ä½“ç†è§£æ–¹é¢ä»è¢«æ¢ç´¢å¾—ä¸å¤Ÿå……åˆ†ã€‚æˆ‘ä»¬æå‡ºäº†Assembly Augmented Tuningï¼ˆæ±‡ç¼–å¢å¼ºè°ƒæ•´ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„ç»“æ„è¯­ä¹‰æŒ‡ä»¤è°ƒæ•´æ¡†æ¶ï¼Œå®ƒé€šè¿‡æŠ•å½±ä»ªæ¨¡å—ååŒç¼–ç å™¨æ¶æ„å’ŒåŸºäºè§£ç å™¨çš„LLMã€‚å…¶ä¸­ï¼Œæ±‡ç¼–ç¼–ç å™¨æå–ç¡¬ä»¶å±‚é¢çš„ç»“æ„ç‰¹å¾ï¼ŒæŠ•å½±ä»ªå°†è¡¨ç¤ºä¸è¯­ä¹‰ç©ºé—´è”ç³»èµ·æ¥ï¼ŒæŒ‡ä»¤è°ƒæ•´çš„LLMä¿ç•™äº†è‡ªç„¶è¯­è¨€åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜äº†ä¸‰ä¸ªä¸»è¦ä¼˜åŠ¿ï¼šï¼ˆ1ï¼‰åœ¨æ±‡ç¼–ç†è§£æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ï¼Œä¸GPT-4 Turboç›¸æ¯”ï¼ŒRecall@1æé«˜äº†+39.7%ï¼ŒMRRæé«˜äº†+17.8%ï¼›ï¼ˆ2ï¼‰åœ¨åŸºç¡€æ¨¡å‹ä¸Šå®ç°äº†ä¸€è‡´çš„æ”¹è¿›ï¼ŒQwen2.5-Coderã€Deepseek-Coderå’ŒCodeLlamaå˜ç§ä¸Šçš„Recall@1æé«˜äº†24.6%~107.4%ï¼ŒMRRæé«˜äº†15.2%~106.3%ï¼›ï¼ˆ3ï¼‰åœ¨æŒ‡ä»¤éµå¾ªèƒ½åŠ›ä¸Šè¡¨ç°å“è¶Šï¼Œæ§åˆ¶ä»£ç ç”Ÿæˆé€€åŒ–çš„æƒ…å†µä¸‹ï¼Œæ”¹è¿›èŒƒå›´åœ¨41.5%~118%ä¹‹é—´ï¼ˆä¸åŒæ¶æ„ä¸‹çš„é€€åŒ–ç‡ä¸º-8.9%~-35\ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11617v2">PDF</a> 9 pages, multiple figures</p>
<p><strong>Summary</strong><br>æ–°ä¸€ä»£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ±‡ç¼–ä»£ç åˆ†æå’Œç†è§£æ–¹é¢çš„åº”ç”¨å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æå‡ºäº†ä¸€ç§åä¸ºâ€œAssembly Augmented Tuningâ€çš„ç«¯åˆ°ç«¯ç»“æ„è¯­ä¹‰æŒ‡ä»¤è°ƒæ•´æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æŠ•å½±ä»ªæ¨¡å—å°†ç¼–ç å™¨æ¶æ„ä¸åŸºäºè§£ç å™¨çš„è¯­è¨€æ¨¡å‹ååŒå·¥ä½œï¼Œå®ç°äº†æ±‡ç¼–ç¼–ç å™¨çš„ç¡¬ä»¶çº§ç»“æ„ç‰¹å¾æå–ã€æŠ•å½±ä»ªçš„è·¨è¡¨å¾è¯­ä¹‰ç©ºé—´æ¡¥æ¢ä½œç”¨ä»¥åŠæŒ‡ä»¤è°ƒæ•´çš„è¯­è¨€æ¨¡å‹çš„è‡ªç„¶è¯­è¨€èƒ½åŠ›ä¿ç•™ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ¡†æ¶åœ¨æ±‡ç¼–ç†è§£æ–¹é¢çš„å“è¶Šæ€§èƒ½ï¼Œå…·æœ‰æ˜¾è‘—çš„æå‡æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ±‡ç¼–ä»£ç åˆ†æä¸ç†è§£åœ¨é€†å‘å·¥ç¨‹ç­‰é¢†åŸŸå…·æœ‰å…³é”®ä½œç”¨ï¼Œä½†é¢ä¸´ä¿¡æ¯å¯†åº¦ä½å’Œç¼ºä¹æ˜ç¡®å¥æ³•ç»“æ„çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿè¯­è¨€æ¨¡å‹åœ¨æ±‡ç¼–ä»£ç æ–¹é¢çš„è¡¨ç°æœ‰é™ï¼Œè€Œæ–°å…´çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨äºŒè¿›åˆ¶åˆ†ææ–¹é¢å–å¾—éƒ¨åˆ†æˆåŠŸã€‚</li>
<li>Assembly Augmented Tuningæ¡†æ¶ç»“åˆäº†ç¼–ç å™¨æ¶æ„å’ŒåŸºäºè§£ç å™¨çš„è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æŠ•å½±ä»ªæ¨¡å—å®ç°ç»“æ„è¯­ä¹‰æŒ‡ä»¤è°ƒæ•´ã€‚</li>
<li>æ±‡ç¼–ç¼–ç å™¨èƒ½å¤Ÿæå–ç¡¬ä»¶çº§ç»“æ„ç‰¹å¾ã€‚</li>
<li>æŠ•å½±ä»ªæ¨¡å—åœ¨è·¨è¡¨å¾è¯­ä¹‰ç©ºé—´æ–¹é¢èµ·åˆ°æ¡¥æ¢ä½œç”¨ã€‚</li>
<li>æŒ‡ä»¤è°ƒæ•´çš„è¯­è¨€æ¨¡å‹èƒ½å¤Ÿä¿ç•™è‡ªç„¶è¯­è¨€èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6e424e2a41e06172c7402b4ada62e020.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71d7ec911673be0f4c91aa88b108172d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b21040300fc78dab608b8bb6f75f2ece.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20da68903ffb0b22f7f0874c5e780fbd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05c658c6ce89594d1ec5ab449e472314.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Capacity-Aware-Inference-Mitigating-the-Straggler-Effect-in-Mixture-of-Experts"><a href="#Capacity-Aware-Inference-Mitigating-the-Straggler-Effect-in-Mixture-of-Experts" class="headerlink" title="Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of   Experts"></a>Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of   Experts</h2><p><strong>Authors:Shwai He, Weilin Cai, Jiayi Huang, Ang Li</strong></p>
<p>The Mixture of Experts (MoE) is an effective architecture for scaling large language models by leveraging sparse expert activation, optimizing the trade-off between performance and efficiency. However, under expert parallelism, MoE suffers from inference inefficiencies due to imbalanced token-to-expert assignment, where some experts are overloaded while others remain underutilized. This imbalance leads to poor resource utilization and increased latency, as the most burdened expert dictates the overall delay, a phenomenon we define as the \textbf{\textit{Straggler Effect}}. To mitigate this, we propose Capacity-Aware Inference, including two key techniques: (1) \textbf{\textit{Capacity-Aware Token Drop}}, which discards overloaded tokens to regulate the maximum latency of MoE, and (2) \textbf{\textit{Capacity-Aware Token Reroute}}, which reallocates overflowed tokens to underutilized experts, balancing the token distribution. These techniques collectively optimize both high-load and low-load expert utilization, leading to a more efficient MoE inference pipeline. Extensive experiments demonstrate the effectiveness of our methods, showing significant improvements in inference efficiency, e.g., 0.2% average performance increase and a 1.94$\times$ inference speedup on Mixtral-8$\times$7B-Instruct. </p>
<blockquote>
<p>æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ˜¯ä¸€ç§é€šè¿‡åˆ©ç”¨ç¨€ç–çš„ä¸“å®¶æ¿€æ´»æ¥æ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆæ¶æ„ï¼Œä¼˜åŒ–äº†æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚ç„¶è€Œï¼Œåœ¨ä¸“å®¶å¹¶è¡Œæ€§ä¸‹ï¼ŒMoEç”±äºä¸å‡è¡¡çš„ä»¤ç‰Œåˆ°ä¸“å®¶åˆ†é…è€Œé­å—æ¨ç†æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå…¶ä¸­ä¸€äº›ä¸“å®¶è¿‡è½½ï¼Œè€Œå…¶ä»–ä¸“å®¶åˆ™åˆ©ç”¨ç‡ä¸è¶³ã€‚è¿™ç§ä¸å¹³è¡¡å¯¼è‡´èµ„æºåˆ©ç”¨ä¸è¶³å’Œå»¶è¿Ÿå¢åŠ ï¼Œå› ä¸ºæœ€ç¹é‡çš„ä¸“å®¶å†³å®šäº†æ€»ä½“å»¶è¿Ÿï¼Œæˆ‘ä»¬å°†è¿™ç§ç°è±¡å®šä¹‰ä¸ºâ€œæ»åè€…æ•ˆåº”â€ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å®¹é‡æ„ŸçŸ¥æ¨ç†ï¼ŒåŒ…æ‹¬ä¸¤ç§å…³é”®æŠ€æœ¯ï¼šï¼ˆ1ï¼‰â€œå®¹é‡æ„ŸçŸ¥ä»¤ç‰Œä¸¢å¼ƒâ€ï¼Œä¸¢å¼ƒè¿‡è½½çš„ä»¤ç‰Œä»¥æ§åˆ¶MoEçš„æœ€å¤§å»¶è¿Ÿï¼›ï¼ˆ2ï¼‰â€œå®¹é‡æ„ŸçŸ¥ä»¤ç‰Œé‡æ–°è·¯ç”±â€ï¼Œé‡æ–°åˆ†é…æº¢å‡ºçš„ä»¤ç‰Œç»™åˆ©ç”¨ç‡ä½çš„ä¸“å®¶ï¼Œå¹³è¡¡ä»¤ç‰Œåˆ†å¸ƒã€‚è¿™äº›æŠ€æœ¯å…±åŒä¼˜åŒ–äº†é«˜è´Ÿè½½å’Œä½è´Ÿè½½çš„ä¸“å®¶åˆ©ç”¨ç‡ï¼Œä»è€Œå®ç°äº†æ›´æœ‰æ•ˆçš„MoEæ¨ç†ç®¡é“ã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨æ¨ç†æ•ˆç‡æ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾ç€æé«˜ï¼Œä¾‹å¦‚åœ¨Mixtral-8Ã—7B-Instructä¸Šå¹³å‡æ€§èƒ½æé«˜0.2ï¼…ï¼Œæ¨ç†é€Ÿåº¦æé«˜1.94å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05066v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„é€šè¿‡åˆ©ç”¨ç¨€ç–ä¸“å®¶æ¿€æ´»æ¥æ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¼˜åŒ–æ€§èƒ½ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚ç„¶è€Œï¼Œåœ¨ä¸“å®¶å¹¶è¡Œæ–¹é¢ï¼ŒMoEåœ¨æ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨æ•ˆç‡é—®é¢˜ï¼Œä¸»è¦æ˜¯ç”±äºä¸å‡è¡¡çš„ä»¤ç‰Œåˆ°ä¸“å®¶åˆ†é…é€ æˆçš„ã€‚ä¸€äº›ä¸“å®¶è¿‡è½½è€Œå…¶ä»–ä¸“å®¶åˆ©ç”¨ç‡ä¸è¶³ï¼Œè¿™ç§ä¸å¹³è¡¡å¯¼è‡´èµ„æºåˆ©ç”¨ç‡ä½å’Œå»¶è¿Ÿå¢åŠ ï¼Œæˆ‘ä»¬å°†è¿™ç§ç°è±¡ç§°ä¸ºâ€œæ‹–åè…¿æ•ˆåº”â€ã€‚ä¸ºç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºå®¹é‡æ„ŸçŸ¥æ¨ç†æŠ€æœ¯ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®æŠ€æœ¯ï¼šï¼ˆ1ï¼‰å®¹é‡æ„ŸçŸ¥ä»¤ç‰Œä¸¢å¼ƒæŠ€æœ¯ç”¨äºä¸¢å¼ƒè¿‡è½½ä»¤ç‰Œä»¥æ§åˆ¶MoEçš„æœ€å¤§å»¶è¿Ÿï¼›ï¼ˆ2ï¼‰å®¹é‡æ„ŸçŸ¥ä»¤ç‰Œé‡æ–°è·¯ç”±æŠ€æœ¯ç”¨äºå°†æº¢å‡ºçš„ä»¤ç‰Œé‡æ–°åˆ†é…ç»™åˆ©ç”¨ç‡ä½çš„ä¸“å®¶ï¼Œå¹³è¡¡ä»¤ç‰Œåˆ†å¸ƒã€‚è¿™äº›æŠ€æœ¯å…±åŒä¼˜åŒ–äº†é«˜è´Ÿè½½å’Œä½è´Ÿè½½ä¸“å®¶çš„åˆ©ç”¨ç‡ï¼Œæé«˜äº†MoEæ¨ç†æµç¨‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚å®éªŒè¯æ˜æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆï¼Œåœ¨Mixtral-8Ã—7B-Instructä¸Šå®ç°äº†å¹³å‡æ€§èƒ½æå‡0.2%ï¼Œæ¨ç†é€Ÿåº¦æå‡1.94å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MoEæ¶æ„é€šè¿‡ç¨€ç–ä¸“å®¶æ¿€æ´»æ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>ä¸“å®¶å¹¶è¡Œä¸­çš„MoEå­˜åœ¨æ¨ç†æ•ˆç‡é—®é¢˜ï¼Œä¸»è¦ç”±äºä¸å‡è¡¡çš„ä»¤ç‰Œåˆ°ä¸“å®¶åˆ†é…å¯¼è‡´èµ„æºåˆ©ç”¨ç‡ä½å’Œå»¶è¿Ÿå¢åŠ ã€‚</li>
<li>æå‡ºå®¹é‡æ„ŸçŸ¥æ¨ç†æŠ€æœ¯ä»¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼ŒåŒ…æ‹¬å®¹é‡æ„ŸçŸ¥ä»¤ç‰Œä¸¢å¼ƒå’Œå®¹é‡æ„ŸçŸ¥ä»¤ç‰Œé‡æ–°è·¯ç”±æŠ€æœ¯ã€‚</li>
<li>ä»¤ç‰Œä¸¢å¼ƒæŠ€æœ¯æ§åˆ¶MoEçš„æœ€å¤§å»¶è¿Ÿï¼Œä»¤ç‰Œé‡æ–°è·¯ç”±æŠ€æœ¯å¹³è¡¡ä»¤ç‰Œåˆ†å¸ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05066">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8057c7e0e0a3c04020ca02c1d4be2813.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85f164b3d8b125012b87307558141965.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31a14157b6102f845089f2368ed1dae9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8be693b8aa2e9b31df36e42d90d2ad7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a672af56ff94125d01fdc60085a7d9e9.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="HybridNorm-Towards-Stable-and-Efficient-Transformer-Training-via-Hybrid-Normalization"><a href="#HybridNorm-Towards-Stable-and-Efficient-Transformer-Training-via-Hybrid-Normalization" class="headerlink" title="HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid   Normalization"></a>HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid   Normalization</h2><p><strong>Authors:Zhijian Zhuo, Yutao Zeng, Ya Wang, Sijun Zhang, Jian Yang, Xiaoqing Li, Xun Zhou, Jinwen Ma</strong></p>
<p>Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, challenges remain in training deep transformer networks, especially regarding the position of layer normalization. While Pre-Norm structures facilitate more stable training owing to their stronger identity path, they often lead to suboptimal performance compared to Post-Norm. In this paper, we propose $\textbf{HybridNorm}$, a simple yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. We provide both theoretical insights and empirical evidence demonstrating that HybridNorm improves gradient flow and model robustness. Extensive experiments on large-scale transformer models, including both dense and sparse variants, show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches across multiple benchmarks. These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/BryceZhuo/HybridNorm">https://github.com/BryceZhuo/HybridNorm</a>. </p>
<blockquote>
<p>å˜å‹å™¨æ¶æ„å·²ç»å¹¿æ³›åº”ç”¨äºå„ç§æœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ã€‚å°½ç®¡å…¶è¡¨ç°å“è¶Šï¼Œä½†åœ¨è®­ç»ƒæ·±åº¦å˜å‹å™¨ç½‘ç»œæ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å…³äºå±‚å½’ä¸€åŒ–çš„ä½ç½®é—®é¢˜ã€‚è™½ç„¶Pre-Normç»“æ„ç”±äºå…¶æ›´å¼ºçš„èº«ä»½è·¯å¾„è€Œä¿ƒè¿›æ›´ç¨³å®šçš„è®­ç»ƒï¼Œä½†å®ƒä»¬é€šå¸¸å¯¼è‡´ä¸Post-Normç›¸æ¯”æ€§èƒ½ä¸ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†$\textbf{HybridNorm}$ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ··åˆå½’ä¸€åŒ–ç­–ç•¥ï¼Œå®ƒç»“åˆäº†Pre-Normå’ŒPost-Normçš„ä¼˜ç‚¹ã€‚å…·ä½“æ¥è¯´ï¼ŒHybridNormåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­é‡‡ç”¨QKVå½’ä¸€åŒ–ï¼Œå¹¶åœ¨æ¯ä¸ªå˜å‹å™¨å—çš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ä¸­é‡‡ç”¨Post-Normã€‚æˆ‘ä»¬æä¾›ç†è®ºè§è§£å’Œå®è¯è¯æ®è¡¨æ˜ï¼ŒHybridNormæ”¹å–„äº†æ¢¯åº¦æµå’Œæ¨¡å‹ç¨³å¥æ€§ã€‚åœ¨å¤§å‹å˜å‹å™¨æ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬å¯†é›†å’Œç¨€ç–å˜ä½“ï¼Œè¡¨æ˜HybridNormåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå§‹ç»ˆä¼˜äºPre-Normå’ŒPost-Normæ–¹æ³•ã€‚è¿™äº›å‘ç°çªå‡ºäº†HybridNormä½œä¸ºæ›´ç¨³å®šã€æ›´æœ‰æ•ˆçš„æŠ€æœ¯ï¼Œå¯æ”¹å–„æ·±åº¦å˜å‹å™¨æ¨¡å‹çš„è®­ç»ƒæ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BryceZhuo/HybridNorm">https://github.com/BryceZhuo/HybridNorm</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04598v3">PDF</a> </p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­çš„Transformeræ¶æ„å¹¿æ³›åº”ç”¨äºæœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ã€‚æœ¬æ–‡ä»‹ç»äº†HybridNormï¼Œä¸€ç§ç»“åˆPre-Normå’ŒPost-Normä¼˜åŠ¿çš„æ··åˆå½’ä¸€åŒ–ç­–ç•¥ã€‚å®ƒåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­é‡‡ç”¨QKVå½’ä¸€åŒ–ï¼Œå¹¶åœ¨æ¯ä¸ªTransformerå—çš„é¦ˆé€å‰ç½‘ç»œï¼ˆFFNï¼‰ä¸­ä½¿ç”¨Post-Normã€‚HybridNormæ”¹å–„äº†æ¢¯åº¦æµå¹¶å¢å¼ºäº†æ¨¡å‹çš„ç¨³å¥æ€§ã€‚å®éªŒè¯æ˜ï¼Œå®ƒåœ¨å¤§å‹Transformeræ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡äº†Pre-Normå’ŒPost-Normæ–¹æ³•ã€‚ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¶æ„æ˜¯æœºå™¨å­¦ä¹ ä»»åŠ¡çš„é»˜è®¤é€‰æ‹©ï¼Œå°¤å…¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ã€‚</li>
<li>Pre-Normæœ‰åŠ©äºç¨³å®šè®­ç»ƒï¼Œä½†å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚</li>
<li>Post-Normåœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°è¾ƒå¥½ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´ã€‚</li>
<li>HybridNormç»“åˆäº†Pre-Normå’ŒPost-Normçš„ä¼˜ç‚¹ã€‚</li>
<li>HybridNormåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­ä½¿ç”¨QKVå½’ä¸€åŒ–ï¼Œå¹¶åœ¨é¦ˆé€å‰ç½‘ç»œï¼ˆFFNï¼‰ä¸­ä½¿ç”¨Post-Normã€‚</li>
<li>HybridNormé€šè¿‡æ”¹å–„æ¢¯åº¦æµå’Œå¢å¼ºæ¨¡å‹ç¨³å¥æ€§æ¥æå‡æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04598">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-47b827a54795ecc5ed20177cd0e4a6fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8764f2897c4d9bd4fcb6280a585b96b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d9731aec10ab6c8ab062a79da8ad5a4.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Scaling-Text-Rich-Image-Understanding-via-Code-Guided-Synthetic-Multimodal-Data-Generation"><a href="#Scaling-Text-Rich-Image-Understanding-via-Code-Guided-Synthetic-Multimodal-Data-Generation" class="headerlink" title="Scaling Text-Rich Image Understanding via Code-Guided Synthetic   Multimodal Data Generation"></a>Scaling Text-Rich Image Understanding via Code-Guided Synthetic   Multimodal Data Generation</h2><p><strong>Authors:Yue Yang, Ajay Patel, Matt Deitke, Tanmay Gupta, Luca Weihs, Andrew Head, Mark Yatskar, Chris Callison-Burch, Ranjay Krishna, Aniruddha Kembhavi, Christopher Clark</strong></p>
<p>Reasoning about images with rich text, such as charts and documents, is a critical application of vision-language models (VLMs). However, VLMs often struggle in these domains due to the scarcity of diverse text-rich vision-language data. To address this challenge, we present CoSyn, a framework that leverages the coding capabilities of text-only large language models (LLMs) to automatically create synthetic text-rich multimodal data. Given input text describing a target domain (e.g., â€œnutrition fact labelsâ€), CoSyn prompts an LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic images. With the underlying code as textual representations of the synthetic images, CoSyn can generate high-quality instruction-tuning data, again relying on a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K images and 2.7M rows of vision-language instruction-tuning data. Comprehensive experiments on seven benchmarks demonstrate that models trained on our synthetic data achieve state-of-the-art performance among competitive open-source models, including Llama 3.2, and surpass proprietary models such as GPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing data, enabling VLMs to ground information within input images, showcasing its potential for developing multimodal agents capable of acting in real-world environments. </p>
<blockquote>
<p>å¯¹äºåŒ…å«å›¾è¡¨å’Œæ–‡æ¡£ç­‰ä¸°å¯Œæ–‡æœ¬çš„å›¾ç‰‡æ¨ç†æ˜¯è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å…³é”®åº”ç”¨ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¤šæ ·åŒ–çš„ä¸°å¯Œæ–‡æœ¬è§†è§‰è¯­è¨€æ•°æ®ï¼ŒVLMåœ¨è¿™äº›é¢†åŸŸå¾€å¾€é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CoSynæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨çº¯æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¼–ç èƒ½åŠ›æ¥è‡ªåŠ¨åˆ›å»ºåˆæˆä¸°å¯Œæ–‡æœ¬çš„å¤šæ¨¡æ€æ•°æ®ã€‚ç»™å®šæè¿°ç›®æ ‡åŸŸçš„è¾“å…¥æ–‡æœ¬ï¼ˆä¾‹å¦‚ï¼Œâ€œè¥å…»äº‹å®æ ‡ç­¾â€ï¼‰ï¼ŒCoSynä¼šæç¤ºLLMç”Ÿæˆç”¨äºæ¸²æŸ“åˆæˆå›¾åƒçš„ä»£ç ï¼ˆPythonã€HTMLã€LaTeXç­‰ï¼‰ã€‚CoSynä»¥åº•å±‚ä»£ç ä½œä¸ºåˆæˆå›¾åƒçš„æ–‡æœ¬è¡¨ç¤ºï¼Œå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„æ•™å­¦è°ƒæ•´æ•°æ®ï¼Œè¿™åŒæ ·ä¾èµ–äºçº¯æ–‡æœ¬LLMã€‚ä½¿ç”¨CoSynï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«40ä¸‡å¼ å›¾åƒå’Œ270ä¸‡è¡Œè§†è§‰è¯­è¨€æ•™å­¦è°ƒæ•´æ•°æ®çš„æ•°æ®é›†ã€‚åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œåœ¨æˆ‘ä»¬åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨ç«äº‰å¼€æºæ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬Llama 3.2ï¼Œå¹¶è¶…è¶Šäº†ä¸“æœ‰æ¨¡å‹ï¼Œå¦‚GPT-4Vå’ŒGemini 1.5 Flashã€‚æ­¤å¤–ï¼ŒCoSynå¯ä»¥ç”ŸæˆåˆæˆæŒ‡å‘æ•°æ®ï¼Œä½¿VLMèƒ½å¤Ÿåœ¨è¾“å…¥å›¾åƒä¸­å®šä½ä¿¡æ¯ï¼Œå±•ç¤ºäº†å…¶å¼€å‘èƒ½å¤Ÿåœ¨ç°å®ä¸–ç•Œä¸­è¡ŒåŠ¨çš„å¤šåª’ä½“ä»£äººçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14846v2">PDF</a> Published in ACL 2025, project page:   <a target="_blank" rel="noopener" href="https://yueyang1996.github.io/cosyn/">https://yueyang1996.github.io/cosyn/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºCoSynçš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¼–ç èƒ½åŠ›ï¼Œè‡ªåŠ¨ç”Ÿæˆä¸°å¯Œçš„æ–‡æœ¬è§†è§‰æ•°æ®ï¼Œä»¥è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å¤„ç†å¯Œæ–‡æœ¬å›¾åƒï¼ˆå¦‚å›¾è¡¨å’Œæ–‡æ¡£ï¼‰æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚CoSynèƒ½å¤Ÿä»æè¿°ç›®æ ‡é¢†åŸŸçš„æ–‡æœ¬è¾“å…¥ç”Ÿæˆä»£ç ï¼Œè¿›è€Œæ¸²æŸ“åˆæˆå›¾åƒã€‚å®ƒè¿˜å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„æ•™å­¦è°ƒæ•´æ•°æ®ï¼Œå¹¶ç”¨äºè®­ç»ƒVLMã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨CoSynåˆæˆçš„æ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šäº†å¼€æºæ¨¡å‹ä»¥åŠä¸“æœ‰æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒCoSynè¿˜èƒ½äº§ç”ŸåˆæˆæŒ‡å‘æ•°æ®ï¼Œä½¿VLMèƒ½å¤Ÿåœ¨è¾“å…¥å›¾åƒä¸­å®šä½ä¿¡æ¯ï¼Œå±•ç¤ºå…¶åœ¨å¼€å‘èƒ½å¤Ÿåœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­è¡ŒåŠ¨çš„å¤šåª’ä½“ä»£ç†æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoSynæ¡†æ¶åˆ©ç”¨LLMè‡ªåŠ¨ç”Ÿæˆåˆæˆæ–‡æœ¬ä¸°å¯Œè§†è§‰æ•°æ®ï¼Œè§£å†³VLMå¤„ç†å¯Œæ–‡æœ¬å›¾åƒçš„æŒ‘æˆ˜ã€‚</li>
<li>CoSynèƒ½å¤Ÿä»æè¿°ç›®æ ‡é¢†åŸŸçš„æ–‡æœ¬ç”Ÿæˆä»£ç ï¼Œè¿›è€Œæ¸²æŸ“åˆæˆå›¾åƒã€‚</li>
<li>CoSynèƒ½ç”Ÿæˆé«˜è´¨é‡çš„æ•™å­¦è°ƒæ•´æ•°æ®ï¼Œç”¨äºè®­ç»ƒVLMã€‚</li>
<li>ä½¿ç”¨CoSynåˆæˆçš„æ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œè¶…è¶Šäº†ä¸€äº›å¼€æºå’Œä¸“æœ‰æ¨¡å‹ã€‚</li>
<li>CoSynå¯ä»¥äº§ç”ŸåˆæˆæŒ‡å‘æ•°æ®ï¼Œå¸®åŠ©VLMåœ¨å›¾åƒä¸­å®šä½ä¿¡æ¯ã€‚</li>
<li>CoSynåœ¨å¼€å‘èƒ½åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­è¡ŒåŠ¨çš„å¤šåª’ä½“ä»£ç†æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚</li>
<li>CoSynçš„åº”ç”¨æœ‰åŠ©äºæ¨è¿›è§†è§‰è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†åŒ…å«ä¸°å¯Œæ–‡æœ¬çš„å›¾åƒæ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14846">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d3ef096e065011ee49be1bcc286789b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ce670ea39635a2e0df5696a484b4eac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a37a89b7f9ad0a829171e6de69194509.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0451c5901ef9853e0469c0bc6b24374f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-edbccba1fd3cfb8f2475ee822bac013d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GLTW-Joint-Improved-Graph-Transformer-and-LLM-via-Three-Word-Language-for-Knowledge-Graph-Completion"><a href="#GLTW-Joint-Improved-Graph-Transformer-and-LLM-via-Three-Word-Language-for-Knowledge-Graph-Completion" class="headerlink" title="GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language   for Knowledge Graph Completion"></a>GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language   for Knowledge Graph Completion</h2><p><strong>Authors:Kangyang Luo, Yuzhuo Bai, Cheng Gao, Shuzheng Si, Yingli Shen, Zhu Liu, Zhitong Wang, Cunliang Kong, Wenhao Li, Yufei Huang, Ye Tian, Xuantang Xiong, Lei Han, Maosong Sun</strong></p>
<p>Knowledge Graph Completion (KGC), which aims to infer missing or incomplete facts, is a crucial task for KGs. However, integrating the vital structural information of KGs into Large Language Models (LLMs) and outputting predictions deterministically remains challenging. To address this, we propose a new method called GLTW, which encodes the structural information of KGs and merges it with LLMs to enhance KGC performance. Specifically, we introduce an improved Graph Transformer (iGT) that effectively encodes subgraphs with both local and global structural information and inherits the characteristics of language model, bypassing training from scratch. Also, we develop a subgraph-based multi-classification training objective, using all entities within KG as classification objects, to boost learning efficiency.Importantly, we combine iGT with an LLM that takes KG language prompts as input.Our extensive experiments on various KG datasets show that GLTW achieves significant performance gains compared to SOTA baselines. </p>
<blockquote>
<p>çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ—¨åœ¨æ¨æ–­ç¼ºå¤±æˆ–ä¸å®Œæ•´çš„äº‹å®ï¼Œæ˜¯çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå°†çŸ¥è¯†å›¾è°±ä¸­çš„é‡è¦ç»“æ„ä¿¡æ¯æ•´åˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼Œå¹¶ç¡®å®šæ€§åœ°è¾“å‡ºé¢„æµ‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºGLTWã€‚è¯¥æ–¹æ³•å¯¹çŸ¥è¯†å›¾è°±çš„ç»“æ„ä¿¡æ¯è¿›è¡Œç¼–ç ï¼Œå¹¶ä¸LLMåˆå¹¶ï¼Œä»¥æé«˜KGCçš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ”¹è¿›çš„å›¾è½¬æ¢å™¨ï¼ˆiGTï¼‰ï¼Œå®ƒèƒ½æœ‰æ•ˆåœ°ç¼–ç å…·æœ‰å±€éƒ¨å’Œå…¨å±€ç»“æ„ä¿¡æ¯çš„å­å›¾ï¼Œå¹¶ç»§æ‰¿è¯­è¨€æ¨¡å‹çš„ç‰¹æ€§ï¼Œæ— éœ€ä»é›¶å¼€å§‹è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºå­å›¾çš„å¤šå…ƒåˆ†ç±»è®­ç»ƒç›®æ ‡ï¼Œåˆ©ç”¨çŸ¥è¯†å›¾è°±ä¸­çš„æ‰€æœ‰å®ä½“ä½œä¸ºåˆ†ç±»å¯¹è±¡ï¼Œä»¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å°†iGTä¸ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ç›¸ç»“åˆï¼Œè¯¥æ¨¡å‹ä»¥çŸ¥è¯†å›¾è°±è¯­è¨€æç¤ºä½œä¸ºè¾“å…¥ã€‚æˆ‘ä»¬åœ¨å„ç§çŸ¥è¯†å›¾è°±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°åŸºçº¿ç›¸æ¯”ï¼ŒGLTWå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11471v3">PDF</a> Accepted by ACL2025(Findings)</p>
<p><strong>Summary</strong></p>
<p>çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ—¨åœ¨æ¨æ–­ç¼ºå¤±æˆ–ä¸å®Œæ•´çš„äº‹å®ï¼Œæ˜¯çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä¸­çš„å…³é”®ä»»åŠ¡ã€‚å°†çŸ¥è¯†å›¾è°±çš„é‡è¦ç»“æ„ä¿¡æ¯èå…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¹¶ç¡®å®šæ€§åœ°è¾“å‡ºé¢„æµ‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•GLTWï¼Œè¯¥æ–¹æ³•ç¼–ç çŸ¥è¯†å›¾è°±çš„ç»“æ„ä¿¡æ¯å¹¶å°†å…¶ä¸LLMåˆå¹¶ï¼Œä»¥æé«˜KGCæ€§èƒ½ã€‚å…·ä½“åœ°ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ”¹è¿›çš„å›¾è½¬æ¢å™¨ï¼ˆiGTï¼‰ï¼Œå®ƒæœ‰æ•ˆåœ°ç¼–ç å…·æœ‰å±€éƒ¨å’Œå…¨å±€ç»“æ„ä¿¡æ¯çš„å­å›¾ï¼Œå¹¶ç»§æ‰¿äº†è¯­è¨€æ¨¡å‹çš„ç‰¹æ€§ï¼Œæ— éœ€ä»é›¶å¼€å§‹è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºå­å›¾çš„å¤šå…ƒåˆ†ç±»è®­ç»ƒç›®æ ‡ï¼Œä»¥çŸ¥è¯†å›¾è°±ä¸­çš„æ‰€æœ‰å®ä½“ä½œä¸ºåˆ†ç±»å¯¹è±¡ï¼Œä»¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å°†iGTä¸æ¥å—çŸ¥è¯†å›¾è°±è¯­è¨€æç¤ºä½œä¸ºè¾“å…¥çš„LLMç›¸ç»“åˆã€‚åœ¨å¤šä¸ªçŸ¥è¯†å›¾è°±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒGLTWä¸æœ€æ–°åŸºçº¿ç›¸æ¯”å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ˜¯çŸ¥è¯†å›¾è°±ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œæ—¨åœ¨æ¨æ–­ç¼ºå¤±æˆ–ä¸å®Œæ•´çš„äº‹å®ã€‚</li>
<li>æ•´åˆçŸ¥è¯†å›¾è°±çš„ç»“æ„ä¿¡æ¯åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¹¶ç¡®å®šæ€§åœ°è¾“å‡ºé¢„æµ‹å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•GLTWï¼Œé€šè¿‡ç¼–ç çŸ¥è¯†å›¾è°±çš„ç»“æ„ä¿¡æ¯å¹¶ä¸LLMç»“åˆï¼Œæé«˜KGCæ€§èƒ½ã€‚</li>
<li>å¼•å…¥æ”¹è¿›çš„å›¾è½¬æ¢å™¨ï¼ˆiGTï¼‰ï¼Œæœ‰æ•ˆç¼–ç åŒ…å«å±€éƒ¨å’Œå…¨å±€ç»“æ„ä¿¡æ¯çš„å­å›¾ã€‚</li>
<li>iGTç»§æ‰¿äº†è¯­è¨€æ¨¡å‹çš„ç‰¹æ€§ï¼Œæ— éœ€ä»é›¶å¼€å§‹è®­ç»ƒã€‚</li>
<li>å¼€å‘åŸºäºå­å›¾çš„å¤šå…ƒåˆ†ç±»è®­ç»ƒç›®æ ‡ï¼Œåˆ©ç”¨çŸ¥è¯†å›¾è°±ä¸­çš„æ‰€æœ‰å®ä½“ä½œä¸ºåˆ†ç±»å¯¹è±¡ï¼Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11471">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-791515823b9f1ddd471f55903498292f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-437ab6684952ff6223861883bd39da1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6ea70906a6fb25c839bcbba5eb55d68.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6effeaeebf7e65c62181eb46bc72370c.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Memory-Is-Not-the-Bottleneck-Cost-Efficient-Continual-Learning-via-Weight-Space-Consolidation"><a href="#Memory-Is-Not-the-Bottleneck-Cost-Efficient-Continual-Learning-via-Weight-Space-Consolidation" class="headerlink" title="Memory Is Not the Bottleneck: Cost-Efficient Continual Learning via   Weight Space Consolidation"></a>Memory Is Not the Bottleneck: Cost-Efficient Continual Learning via   Weight Space Consolidation</h2><p><strong>Authors:Dongkyu Cho, Taesup Moon, Rumi Chunara, Kyunghyun Cho, Sungmin Cha</strong></p>
<p>Continual learning (CL) has traditionally emphasized minimizing exemplar memory usage, assuming that memory is the primary bottleneck. However, in modern computing environments-particularly those involving large foundation models-memory is inexpensive and abundant, while GPU time constitutes the main cost. This paper re-examines CL under a more realistic setting with sufficient exemplar memory, where the system can retain a representative portion of past data. We find that, under this regime, stability improves due to reduced forgetting, but plasticity diminishes as the model becomes biased toward prior tasks and struggles to adapt to new ones. Notably, even simple baselines like naive replay can match or exceed the performance of state-of-the-art methods at a fraction of the computational cost. Building on this insight, we propose a lightweight yet effective method called Weight Space Consolidation, which directly operates in the modelâ€™s weight space via two core mechanisms: (1) rank-based parameter resets to recover plasticity, and (2) weight averaging to enhance stability. Our approach outperforms strong baselines across class-incremental learning with image classifiers and continual instruction tuning with large language models, while requiring only one-third to one-fourth of the training cost. These findings challenge long-standing CL assumptions and establish a new, cost-efficient baseline for real-world continual learning systems where exemplar memory is no longer the limiting factor. </p>
<blockquote>
<p>æŒç»­å­¦ä¹ ï¼ˆCLï¼‰ä¼ ç»Ÿä¸Šå¼ºè°ƒæœ€å°åŒ–ç¤ºä¾‹å†…å­˜çš„ä½¿ç”¨ï¼Œå‡è®¾å†…å­˜æ˜¯ä¸»è¦ç“¶é¢ˆã€‚ç„¶è€Œï¼Œåœ¨ç°ä»£è®¡ç®—ç¯å¢ƒä¸­â€”â€”ç‰¹åˆ«æ˜¯æ¶‰åŠå¤§å‹åŸºç¡€æ¨¡å‹çš„æƒ…å¢ƒâ€”â€”å†…å­˜æ˜¯ä¾¿å®œä¸”ä¸°å¯Œçš„ï¼Œè€ŒGPUæ—¶é—´æ„æˆäº†ä¸»è¦æˆæœ¬ã€‚æœ¬æ–‡åœ¨ä¸€ä¸ªæ›´ç°å®çš„ã€æ‹¥æœ‰å……è¶³ç¤ºä¾‹å†…å­˜çš„è®¾å®šä¸‹é‡æ–°è€ƒå¯Ÿäº†CLï¼Œåœ¨è¿™ç§è®¾å®šä¸‹ï¼Œç³»ç»Ÿå¯ä»¥ä¿ç•™ä¸€éƒ¨åˆ†è¿‡å»çš„ä»£è¡¨æ€§æ•°æ®ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨è¿™ç§åˆ¶åº¦ä¸‹ï¼Œç”±äºå‡å°‘äº†é—å¿˜ï¼Œç¨³å®šæ€§å¾—åˆ°äº†æé«˜ï¼Œä½†å¯å¡‘æ€§é™ä½äº†ï¼Œå› ä¸ºæ¨¡å‹åå‘äºå…ˆå‰çš„ä»»åŠ¡ï¼Œéš¾ä»¥é€‚åº”æ–°ä»»åŠ¡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿ç®€å•çš„åŸºçº¿æ–¹æ³•ï¼Œå¦‚å¤©çœŸçš„å¤è¿°ï¼Œä¹Ÿå¯ä»¥åœ¨è®¡ç®—æˆæœ¬çš„ä¸€å°éƒ¨åˆ†å†…åŒ¹é…æˆ–è¶…è¿‡æœ€æ–°æŠ€æœ¯æ–¹æ³•çš„æ€§èƒ½ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§ä½†æœ‰æ•ˆçš„æ–¹æ³•ï¼Œç§°ä¸ºæƒé‡ç©ºé—´æ•´åˆï¼ˆWeight Space Consolidationï¼‰ï¼Œå®ƒé€šè¿‡ä¸¤ç§æ ¸å¿ƒæœºåˆ¶ç›´æ¥åœ¨æ¨¡å‹çš„æƒé‡ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼šï¼ˆ1ï¼‰åŸºäºæ’åçš„å‚æ•°é‡ç½®ä»¥æ¢å¤å¯å¡‘æ€§ï¼Œä»¥åŠï¼ˆ2ï¼‰æƒé‡å¹³å‡ä»¥å¢å¼ºç¨³å®šæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç±»å¢é‡å­¦ä¹ ä¸å›¾åƒåˆ†ç±»å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æŒç»­æŒ‡ä»¤è°ƒæ•´æ–¹é¢ä¼˜äºå¼ºå¤§çš„åŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶åªéœ€ä¸‰åˆ†ä¹‹ä¸€åˆ°å››åˆ†ä¹‹ä¸€çš„è®­ç»ƒæˆæœ¬ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†é•¿æœŸå­˜åœ¨çš„CLå‡è®¾ï¼Œå¹¶ä¸ºç°å®ä¸–ç•Œçš„æŒç»­å­¦ä¹ ç³»ç»Ÿå»ºç«‹äº†æ–°çš„ã€æˆæœ¬æ•ˆç›Šé«˜çš„åŸºå‡†çº¿ï¼Œå…¶ä¸­ç¤ºä¾‹å†…å­˜ä¸å†æ˜¯é™åˆ¶å› ç´ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07274v3">PDF</a> 23 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡é‡æ–°å®¡è§†äº†åœ¨å…·æœ‰å……è¶³æ ·æœ¬å†…å­˜çš„æ›´ç°å®ç¯å¢ƒä¸‹æŒç»­å­¦ä¹ ï¼ˆCLï¼‰çš„é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨å……è¶³å†…å­˜ä¸‹ç¨³å®šæ€§å› å‡å°‘é—å¿˜è€Œæé«˜ï¼Œä½†å¯å¡‘æ€§å› æ¨¡å‹åå‘å…ˆå‰ä»»åŠ¡è€Œé™ä½ï¼Œéš¾ä»¥é€‚åº”æ–°ä»»åŠ¡ã€‚ç®€å•åŸºçº¿å¦‚æœ´ç´ å›æ”¾èƒ½åœ¨è¾ƒä½è®¡ç®—æˆæœ¬ä¸‹åŒ¹é…æˆ–è¶…è¶Šå…ˆè¿›æ–¹æ³•æ€§èƒ½ã€‚åŸºäºæ­¤ï¼Œæå‡ºä¸€ç§è½»é‡çº§æœ‰æ•ˆçš„Weight Space Consolidationæ–¹æ³•ï¼Œé€šè¿‡å‚æ•°é‡ç½®å’Œæƒé‡å¹³å‡æœºåˆ¶æé«˜æ¨¡å‹å¯å¡‘æ€§å’Œç¨³å®šæ€§ã€‚è¯¥æ–¹æ³•åœ¨ç±»å¢é‡å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æŒç»­æŒ‡ä»¤è°ƒæ•´æ–¹é¢è¡¨ç°ä¼˜äºå¼ºåŸºçº¿ï¼Œä¸”ä»…éœ€ä¸‰åˆ†ä¹‹ä¸€è‡³å››åˆ†ä¹‹ä¸€çš„è®­ç»ƒæˆæœ¬ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†é•¿æœŸå­˜åœ¨çš„CLå‡è®¾ï¼Œä¸ºç°å®ä¸–ç•Œä¸­æ ·æœ¬å†…å­˜ä¸å†æ˜¯é™åˆ¶å› ç´ çš„æŒç»­å­¦ä¹ ç³»ç»Ÿå»ºç«‹äº†æ–°çš„æˆæœ¬æ•ˆç›ŠåŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»ŸæŒç»­å­¦ä¹ ï¼ˆCLï¼‰ä¾§é‡äºæœ€å°åŒ–æ ·æœ¬å†…å­˜ä½¿ç”¨ï¼Œä½†åœ¨ç°ä»£è®¡ç®—ç¯å¢ƒä¸­ï¼Œå†…å­˜å……è¶³ä¸”æˆæœ¬ä½å»‰ï¼ŒGPUæ—¶é—´æˆä¸ºä¸»è¦æˆæœ¬ã€‚</li>
<li>åœ¨å……è¶³å†…å­˜ç¯å¢ƒä¸‹ï¼Œç¨³å®šæ€§å› å‡å°‘é—å¿˜è€Œæé«˜ï¼Œä½†æ¨¡å‹å¯å¡‘æ€§é™ä½ï¼Œéš¾ä»¥é€‚åº”æ–°ä»»åŠ¡ã€‚</li>
<li>ç®€å•å›æ”¾æ–¹æ³•èƒ½åœ¨è¾ƒä½è®¡ç®—æˆæœ¬ä¸‹è¾¾åˆ°æˆ–è¶…è¶Šå¤æ‚æ–¹æ³•æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è½»é‡çº§æ–¹æ³•Weight Space Consolidationï¼Œé€šè¿‡å‚æ•°é‡ç½®å’Œæƒé‡å¹³å‡æé«˜æ¨¡å‹å¯å¡‘æ€§å’Œç¨³å®šæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ç±»å¢é‡å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹æŒç»­æŒ‡ä»¤è°ƒæ•´æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä¸å…¶ä»–å¼ºåŸºçº¿ç›¸æ¯”ï¼ŒWeight Space Consolidationæ–¹æ³•çš„è®­ç»ƒæˆæœ¬æ›´ä½ï¼Œä»…éœ€ä¸‰åˆ†ä¹‹ä¸€è‡³å››åˆ†ä¹‹ä¸€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07274">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-73b3ff753ca3891336324fd84d0d3b85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-94372edec620d884a601e09f338177d1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-db36b2fa4c51e6de0058c3a846dfadc2.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="BARE-Leveraging-Base-Language-Models-for-Few-Shot-Synthetic-Data-Generation"><a href="#BARE-Leveraging-Base-Language-Models-for-Few-Shot-Synthetic-Data-Generation" class="headerlink" title="BARE: Leveraging Base Language Models for Few-Shot Synthetic Data   Generation"></a>BARE: Leveraging Base Language Models for Few-Shot Synthetic Data   Generation</h2><p><strong>Authors:Alan Zhu, Parth Asawa, Jared Quincy Davis, Lingjiao Chen, Boris Hanin, Ion Stoica, Joseph E. Gonzalez, Matei Zaharia</strong></p>
<p>As the demand for high-quality data in model training grows, researchers and developers are increasingly generating synthetic data to tune and train LLMs. However, current data generation methods rely on seed sets containing tens of thousands of examples to prompt instruction-tuned models. This reliance can be especially problematic when the curation of high-quality examples is expensive or difficult. In this paper we explore the novel few-shot synthetic data generation setting â€“ generating a high-quality dataset from a few examples. We show that when working with only a few seed examples, instruction-tuned models used in current synthetic data methods produce insufficient diversity for downstream tasks. In contrast, we show that base models without post-training, largely untapped for synthetic data generation, offer substantially greater output diversity, albeit with lower instruction following abilities. Leveraging this insight, we propose Base-Refine (BARE), a novel two-stage method that combines the diversity of base models with the quality assurance of instruction-tuned models. BARE excels in few-shot synthetic data generation: using only 3 seed examples it generates diverse, high-quality datasets that significantly improve downstream task performance. We show that fine-tuning Llama 3.1 8B with 1,000 BARE-generated samples achieves performance comparable to state-of-the-art similarly sized models on LiveCodeBench tasks. Furthermore, data generated with BARE enables a 101% improvement for a fine-tuned Llama 3.2 1B on GSM8K over data generated by only instruction-models, and an 18.4% improvement for a fine-tuned Llama 3.1 8B over the state-of-the-art RAFT method for RAG data generation. </p>
<blockquote>
<p>éšç€æ¨¡å‹è®­ç»ƒä¸­å¯¹é«˜è´¨é‡æ•°æ®çš„éœ€æ±‚ä¸æ–­å¢é•¿ï¼Œç ”ç©¶äººå‘˜å’Œå¼€å‘äººå‘˜æ­£åœ¨è¶Šæ¥è¶Šå¤šåœ°ç”Ÿæˆåˆæˆæ•°æ®æ¥è°ƒæ•´å’Œè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ•°æ®ç”Ÿæˆæ–¹æ³•ä¾èµ–äºåŒ…å«æ•°åä¸‡ä¸ªç¤ºä¾‹çš„ç§å­é›†æ¥æç¤ºæŒ‡ä»¤è°ƒæ•´æ¨¡å‹ã€‚å½“é«˜è´¨é‡ç¤ºä¾‹çš„æ”¶é›†æ—¢æ˜‚è´µåˆå›°éš¾æ—¶ï¼Œè¿™ç§ä¾èµ–å¯èƒ½æ˜¯ç‰¹åˆ«æˆé—®é¢˜çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†æ–°é¢–çš„å°æ ·æœ¬åˆæˆæ•°æ®ç”Ÿæˆåœºæ™¯â€”â€”ä»å°‘é‡ç¤ºä¾‹ç”Ÿæˆé«˜è´¨é‡æ•°æ®é›†ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨ä¸ä»…å‡ ä¸ªç§å­ç¤ºä¾‹ä¸€èµ·å·¥ä½œæ—¶ï¼Œå½“å‰åˆæˆæ•°æ®æ–¹æ³•ä¸­æ‰€ä½¿ç”¨çš„æ•™å­¦å‹æ¨¡å‹äº§ç”Ÿçš„ä¸‹æ¸¸ä»»åŠ¡å¤šæ ·æ€§ä¸è¶³ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬æ˜¾ç¤ºï¼Œå°šæœªç”¨äºåˆæˆæ•°æ®ç”Ÿæˆçš„åŸºå‡†æ¨¡å‹æä¾›äº†å¤§é‡æ›´å¤§çš„è¾“å‡ºå¤šæ ·æ€§ï¼Œå°½ç®¡å…¶éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›è¾ƒä½ã€‚åˆ©ç”¨è¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†Base-Refineï¼ˆBAREï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œå®ƒå°†åŸºå‡†æ¨¡å‹çš„å¤šæ ·æ€§ä¸æŒ‡ä»¤è°ƒæ•´æ¨¡å‹çš„è´¨é‡ä¿è¯ç›¸ç»“åˆã€‚BAREåœ¨å°‘æ ·æœ¬åˆæˆæ•°æ®ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼šä»…ä½¿ç”¨ä¸‰ä¸ªç§å­ç¤ºä¾‹å³å¯ç”Ÿæˆå¤šæ ·åŒ–ä¸”é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œè¿™æ˜¾ç€æé«˜äº†ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä½¿ç”¨ç”±BAREç”Ÿæˆçš„æ ·æœ¬å¾®è°ƒåçš„Llama 3.1 8Bæ¨¡å‹åœ¨LiveCodeBenchä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸æœ€æ–°æŠ€æœ¯æ°´å¹³çš„ç±»ä¼¼è§„æ¨¡æ¨¡å‹ç›¸å½“ã€‚æ­¤å¤–ï¼Œä½¿ç”¨BAREç”Ÿæˆçš„æ•°æ®å¯ä½¿å¾®è°ƒåçš„Llama 3.2 1Båœ¨GSM8Kä¸Šçš„æ€§èƒ½æé«˜101%ï¼Œç›¸è¾ƒäºä»…ä½¿ç”¨æŒ‡ä»¤æ¨¡å‹ç”Ÿæˆçš„æ•°æ®ï¼›å¯¹äºå¾®è°ƒåçš„Llama 3.1 8Bæ¨¡å‹åœ¨RAGæ•°æ®ç”Ÿæˆæ–¹é¢çš„æ€§èƒ½æé«˜18.4%ï¼Œè¶…è¿‡äº†å½“å‰æœ€å…ˆè¿›çš„RAFTæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01697v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€æ¨¡å‹è®­ç»ƒå¯¹æ•°æ®è´¨é‡éœ€æ±‚çš„æé«˜ï¼Œç ”ç©¶äººå‘˜è¶Šæ¥è¶Šä¾èµ–ç”Ÿæˆåˆæˆæ•°æ®æ¥è°ƒæ•´å’Œè®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ä½†å½“å‰çš„æ•°æ®ç”Ÿæˆæ–¹æ³•ä¾èµ–æ•°åä¸‡ä¸ªæ ·æœ¬ç»„æˆçš„ç§å­é›†æ¥æç¤ºæŒ‡ä»¤è°ƒæ•´æ¨¡å‹ï¼Œè¿™åœ¨é«˜è´¨é‡æ ·æœ¬çš„æ”¶é›†æˆæœ¬é«˜æ˜‚æˆ–å›°éš¾æ—¶å°¤ä¸ºæ£˜æ‰‹ã€‚æœ¬æ–‡æ¢ç´¢äº†æ–°å‹å°‘æ ·æœ¬åˆæˆæ•°æ®ç”Ÿæˆè®¾ç½®ï¼Œå³ä»å°‘é‡æ ·æœ¬ç”Ÿæˆé«˜è´¨é‡æ•°æ®é›†ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨ä»…æœ‰çš„å‡ ä¸ªç§å­æ ·æœ¬ï¼Œå½“å‰åˆæˆæ•°æ®æ–¹æ³•ä¸­çš„æŒ‡ä»¤è°ƒæ•´æ¨¡å‹äº§ç”Ÿçš„å¤šæ ·æ€§ä¸è¶³ä»¥åº”å¯¹ä¸‹æ¸¸ä»»åŠ¡ã€‚ç›¸åï¼Œå°½ç®¡æŒ‡ä»¤éµå¾ªèƒ½åŠ›è¾ƒä½ï¼Œä½†æœªè¢«å……åˆ†å¼€å‘çš„åŸºå‡†æ¨¡å‹åœ¨åˆæˆæ•°æ®ç”Ÿæˆæ–¹é¢æä¾›äº†æ›´ä¸°å¯Œçš„è¾“å‡ºå¤šæ ·æ€§ã€‚åˆ©ç”¨è¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†Base-Refineï¼ˆBAREï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œç»“åˆäº†åŸºå‡†æ¨¡å‹çš„å¤šæ ·æ€§å’ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹çš„è´¨é‡ä¿è¯ã€‚BAREåœ¨å°‘æ ·æœ¬åˆæˆæ•°æ®ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼šä»…ä½¿ç”¨ä¸‰ä¸ªç§å­æ ·æœ¬å°±èƒ½ç”Ÿæˆå¤šæ ·åŒ–ã€é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œæ˜¾è‘—æé«˜äº†ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚å®éªŒæ˜¾ç¤ºï¼Œä½¿ç”¨BAREç”Ÿæˆçš„1000ä¸ªæ ·æœ¬å¾®è°ƒLlama 3.1 8Bæ¨¡å‹ï¼Œåœ¨LiveCodeBenchä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸æœ€æ–°æŠ€æœ¯æ°´å¹³çš„ç±»ä¼¼è§„æ¨¡æ¨¡å‹ç›¸å½“ã€‚æ­¤å¤–ï¼Œä½¿ç”¨BAREç”Ÿæˆçš„æ•°æ®å¯¹Llama 3.2 1Bè¿›è¡Œå¾®è°ƒï¼Œåœ¨GSM8Kä¸Šçš„æ”¹è¿›ç‡ä¸º101%ï¼Œå¯¹Llama 3.1 8Bçš„æ”¹è¿›ç‡ä¸ºæœ€æ–°RAFTæ–¹æ³•çš„18.4%ã€‚è¿™äº›æ•°æ®è¡¨æ˜ï¼Œåœ¨å¤§å‹è¯­è¨€æ¨¡å‹é¢†åŸŸä½¿ç”¨æ­¤æ–¹æ³•æœ‰å·¨å¤§æ½œåŠ›ã€‚åŒæ—¶æ¢ç´¢å°†æ­¤æ¡†æ¶æ‰©å±•åˆ°æ›´å¹¿æ³›çš„åº”ç”¨åœºæ™¯çš„å¯èƒ½æ€§ã€‚åŒæ—¶æŒ‡å‡ºæœªæ¥çš„ç ”ç©¶æ–¹å‘åŒ…æ‹¬è¿›ä¸€æ­¥ä¼˜åŒ–åˆæˆæ•°æ®çš„å¤šæ ·æ€§å’Œè´¨é‡ç­‰ã€‚æ€»ä½“æ¥è¯´ï¼Œè¯¥ç ”ç©¶ä¸ºè§£å†³é«˜è´¨é‡æ•°æ®éœ€æ±‚æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–°æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶å…³æ³¨äºä»å°‘é‡æ ·æœ¬ä¸­ç”Ÿæˆé«˜è´¨é‡æ•°æ®é›†çš„æ–°æ–¹æ³•ã€‚</li>
<li>å½“å‰åˆæˆæ•°æ®æ–¹æ³•ä¾èµ–å¤§é‡ç§å­ç¤ºä¾‹ï¼Œè¿™åœ¨é«˜æˆæœ¬æˆ–éš¾ä»¥è·å–é«˜è´¨é‡æ ·æœ¬æ—¶æˆä¸ºé—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µæ–¹æ³•Base-Refineï¼ˆBAREï¼‰ï¼Œç»“åˆäº†åŸºå‡†æ¨¡å‹çš„å¤šæ ·æ€§å’ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹çš„è´¨é‡ä¿è¯ã€‚</li>
<li>BAREæ–¹æ³•åœ¨å°‘æ ·æœ¬åˆæˆæ•°æ®ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä»…ä½¿ç”¨å°‘æ•°ç§å­æ ·æœ¬å°±èƒ½ç”Ÿæˆå¤šæ ·åŒ–ã€é«˜è´¨é‡çš„æ•°æ®é›†ã€‚</li>
<li>BAREç”Ÿæˆçš„åˆæˆæ•°æ®å¯ä»¥æ˜¾è‘—æé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼Œä¾‹å¦‚åœ¨å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶çš„ä»»åŠ¡è¡¨ç°ã€‚å…·ä½“æ¡ˆä¾‹åŒ…æ‹¬Llamaæ¨¡å‹çš„å¤šä¸ªç‰ˆæœ¬å’Œä¸åŒçš„ä»»åŠ¡åŸºå‡†æµ‹è¯•ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚</li>
<li>æ­¤æ–¹æ³•çš„æ½œåœ¨åº”ç”¨é¢†åŸŸå¹¿æ³›ï¼Œæœªæ¥å¯ä»¥æ¢ç´¢æ‰©å±•åˆ°æ›´å¤šåœºæ™¯çš„å¯èƒ½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01697">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-732eabf3cca18f43125c1a1133e4979a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-489d9e2aca63db868c028afa7cc4688b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a05c40204bac0d3a17b8b400573df7a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce53d8eff2ade9b6ac5ba91bff8be431.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d01e1697a47b259e1829f35caba44291.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc89248735488e2af125e4416f350632.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Property-Enhanced-Instruction-Tuning-for-Multi-task-Molecule-Generation-with-Large-Language-Models"><a href="#Property-Enhanced-Instruction-Tuning-for-Multi-task-Molecule-Generation-with-Large-Language-Models" class="headerlink" title="Property Enhanced Instruction Tuning for Multi-task Molecule Generation   with Large Language Models"></a>Property Enhanced Instruction Tuning for Multi-task Molecule Generation   with Large Language Models</h2><p><strong>Authors:Xuan Lin, Long Chen, Yile Wang, Xiangxiang Zeng, Philip S. Yu</strong></p>
<p>Large language models (LLMs) are widely applied in various natural language processing tasks such as question answering and machine translation. However, due to the lack of labeled data and the difficulty of manual annotation for biochemical properties, the performance for molecule generation tasks is still limited, especially for tasks involving multi-properties constraints. In this work, we present a two-step framework PEIT (Property Enhanced Instruction Tuning) to improve LLMs for molecular-related tasks. In the first step, we use textual descriptions, SMILES, and biochemical properties as multimodal inputs to pre-train a model called PEIT-GEN, by aligning multi-modal representations to synthesize instruction data. In the second step, we fine-tune existing open-source LLMs with the synthesized data, the resulting PEIT-LLM can handle molecule captioning, text-based molecule generation, molecular property prediction, and our newly proposed multi-constraint molecule generation tasks. Experimental results show that our pre-trained PEIT-GEN outperforms MolT5 and BioT5 in molecule captioning, demonstrating modalities align well between textual descriptions, structures, and biochemical properties. Furthermore, PEIT-LLM shows promising improvements in multi-task molecule generation, proving the scalability of the PEIT framework for various molecular tasks. We release the code, constructed instruction data, and model checkpoints in <a target="_blank" rel="noopener" href="https://github.com/chenlong164/PEIT">https://github.com/chenlong164/PEIT</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¹¿æ³›åº”ç”¨äºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚é—®ç­”å’Œæœºå™¨ç¿»è¯‘ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ ‡ç­¾æ•°æ®å’Œç”Ÿç‰©åŒ–å­¦å±æ€§æ‰‹åŠ¨æ³¨é‡Šçš„å›°éš¾ï¼Œåˆ†å­ç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ä»ç„¶æœ‰é™ï¼Œå°¤å…¶æ˜¯æ¶‰åŠå¤šå±æ€§çº¦æŸçš„ä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„PEITï¼ˆå±æ€§å¢å¼ºæŒ‡ä»¤è°ƒæ•´ï¼‰æ¡†æ¶ï¼Œä»¥æé«˜LLMåœ¨åˆ†å­ç›¸å…³ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨æ–‡æœ¬æè¿°ã€SMILESå’Œç”Ÿç‰©åŒ–å­¦å±æ€§ä½œä¸ºå¤šæ¨¡å¼è¾“å…¥ï¼Œé€šè¿‡å¯¹é½å¤šæ¨¡å¼è¡¨ç¤ºæ¥åˆæˆæŒ‡ä»¤æ•°æ®ï¼Œä»è€Œé¢„è®­ç»ƒä¸€ä¸ªåä¸ºPEIT-GENçš„æ¨¡å‹ã€‚åœ¨ç¬¬äºŒæ­¥ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨åˆæˆæ•°æ®å¯¹ç°æœ‰çš„å¼€æºLLMè¿›è¡Œå¾®è°ƒï¼Œå¾—åˆ°çš„PEIT-LLMå¯ä»¥å¤„ç†åˆ†å­æè¿°ã€æ–‡æœ¬åŸºç¡€ä¸Šçš„åˆ†å­ç”Ÿæˆã€åˆ†å­å±æ€§é¢„æµ‹ä»¥åŠæˆ‘ä»¬æ–°æå‡ºçš„å¤šçº¦æŸåˆ†å­ç”Ÿæˆä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬é¢„è®­ç»ƒçš„PEIT-GENåœ¨åˆ†å­æè¿°æ–¹é¢ä¼˜äºMolT5å’ŒBioT5ï¼Œè¡¨æ˜æ–‡æœ¬æè¿°ã€ç»“æ„å’Œç”Ÿç‰©åŒ–å­¦å±æ€§ä¹‹é—´çš„æ¨¡å¼å¯¹é½è‰¯å¥½ã€‚æ­¤å¤–ï¼ŒPEIT-LLMåœ¨å¤šä»»åŠ¡åˆ†å­ç”Ÿæˆæ–¹é¢æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„æ”¹è¿›ï¼Œè¯æ˜äº†PEITæ¡†æ¶åœ¨å„ç§åˆ†å­ä»»åŠ¡ä¸Šçš„å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/chenlong164/PEIT">https://github.com/chenlong164/PEIT</a>ä¸Šå‘å¸ƒäº†ä»£ç ã€æ„å»ºæŒ‡ä»¤æ•°æ®å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18084v4">PDF</a> 9</p>
<p><strong>Summary</strong></p>
<p>LLMsåœ¨åˆ†å­ç”Ÿæˆä»»åŠ¡ä¸­çš„æ€§èƒ½å—é™äºç¼ºä¹æ ‡ç­¾æ•°æ®å’Œæ‰‹åŠ¨æ ‡æ³¨çš„å›°éš¾ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸¤æ­¥æ¡†æ¶PEITï¼Œä»¥æé«˜LLMsåœ¨åˆ†å­ç›¸å…³ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚é¦–å…ˆï¼Œä½¿ç”¨æ–‡æœ¬æè¿°ã€SMILESå’Œç”Ÿç‰©åŒ–å­¦å±æ€§ç­‰å¤šæ¨¡å¼è¾“å…¥é¢„è®­ç»ƒPEIT-GENæ¨¡å‹ã€‚å…¶æ¬¡ï¼Œç”¨åˆæˆæ•°æ®å¾®è°ƒç°æœ‰å¼€æºLLMsï¼Œå¾—åˆ°çš„PEIT-LLMèƒ½å¤„ç†åˆ†å­æè¿°ã€æ–‡æœ¬åŸºç¡€åˆ†å­ç”Ÿæˆã€åˆ†å­å±æ€§é¢„æµ‹ä»¥åŠæ–°æå‡ºçš„å¤šçº¦æŸåˆ†å­ç”Ÿæˆä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPEIT-GENåœ¨åˆ†å­æè¿°ä¸Šä¼˜äºMolT5å’ŒBioT5ï¼Œè¯æ˜æ–‡æœ¬æè¿°ã€ç»“æ„å’Œç”Ÿç‰©åŒ–å­¦å±æ€§ä¹‹é—´çš„æ¨¡å¼å¯¹é½è‰¯å¥½ã€‚åŒæ—¶ï¼ŒPEIT-LLMåœ¨å¤šä»»åŠ¡åˆ†å­ç”Ÿæˆæ–¹é¢å±•ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼Œè¯æ˜PEITæ¡†æ¶å¯¹å„ç§åˆ†å­ä»»åŠ¡çš„å¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨åˆ†å­ç”Ÿæˆä»»åŠ¡ä¸­é¢ä¸´æ€§èƒ½æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯æ¶‰åŠå¤šå±æ€§çº¦æŸçš„ä»»åŠ¡ã€‚</li>
<li>PEITæ¡†æ¶åˆ†ä¸ºä¸¤æ­¥ï¼šé¦–å…ˆä½¿ç”¨å¤šæ¨¡å¼è¾“å…¥é¢„è®­ç»ƒPEIT-GENæ¨¡å‹ï¼›ç„¶åä½¿ç”¨åˆæˆæ•°æ®å¾®è°ƒç°æœ‰å¼€æºLLMsã€‚</li>
<li>PEIT-GENåœ¨åˆ†å­æè¿°æ–¹é¢çš„æ€§èƒ½ä¼˜äºMolT5å’ŒBioT5ï¼Œè¯æ˜äº†ä¸åŒæ¨¡å¼ä¹‹é—´çš„è‰¯å¥½å¯¹é½ã€‚</li>
<li>PEIT-LLMèƒ½å¤„ç†å¤šç§åˆ†å­ç›¸å…³ä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ†å­æè¿°ã€æ–‡æœ¬åŸºç¡€åˆ†å­ç”Ÿæˆã€åˆ†å­å±æ€§é¢„æµ‹ä»¥åŠå¤šçº¦æŸåˆ†å­ç”Ÿæˆã€‚</li>
<li>æ¡†æ¶çš„æ‰©å±•æ€§å¥½ï¼Œèƒ½é€‚åº”ä¸åŒçš„åˆ†å­ä»»åŠ¡ã€‚</li>
<li>ä»£ç ã€æ„å»ºæŒ‡ä»¤æ•°æ®å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæ”¹å–„LLMsåœ¨åˆ†å­ç›¸å…³ä»»åŠ¡ä¸Šçš„æ€§èƒ½æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18084">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d830d6aebab5305d9cebc64e0000224f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31f76ba4483727f61820faca18ea1443.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a471b11fa3d7bdbc3e5febfcdba29895.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba199ef9aa2dc39f1d4d5e08effd8c6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbbd765fafcd09f21f4b2375d4b1e5ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6978b5b6f6f7d804a553b89244f67278.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Performance-of-ChatGPT-on-tasks-involving-physics-visual-representations-the-case-of-the-Brief-Electricity-and-Magnetism-Assessment"><a href="#Performance-of-ChatGPT-on-tasks-involving-physics-visual-representations-the-case-of-the-Brief-Electricity-and-Magnetism-Assessment" class="headerlink" title="Performance of ChatGPT on tasks involving physics visual   representations: the case of the Brief Electricity and Magnetism Assessment"></a>Performance of ChatGPT on tasks involving physics visual   representations: the case of the Brief Electricity and Magnetism Assessment</h2><p><strong>Authors:Giulia Polverini, Jakob Melin, Elias Onerud, Bor Gregorcic</strong></p>
<p>Artificial intelligence-based chatbots are increasingly influencing physics education due to their ability to interpret and respond to textual and visual inputs. This study evaluates the performance of two large multimodal model-based chatbots, ChatGPT-4 and ChatGPT-4o on the Brief Electricity and Magnetism Assessment (BEMA), a conceptual physics inventory rich in visual representations such as vector fields, circuit diagrams, and graphs. Quantitative analysis shows that ChatGPT-4o outperforms both ChatGPT-4 and a large sample of university students, and demonstrates improvements in ChatGPT-4oâ€™s vision interpretation ability over its predecessor ChatGPT-4. However, qualitative analysis of ChatGPT-4oâ€™s responses reveals persistent challenges. We identified three types of difficulties in the chatbotâ€™s responses to tasks on BEMA: (1) difficulties with visual interpretation, (2) difficulties in providing correct physics laws or rules, and (3) difficulties with spatial coordination and application of physics representations. Spatial reasoning tasks, particularly those requiring the use of the right-hand rule, proved especially problematic. These findings highlight that the most broadly used large multimodal model-based chatbot, ChatGPT-4o, still exhibits significant difficulties in engaging with physics tasks involving visual representations. While the chatbot shows potential for educational applications, including personalized tutoring and accessibility support for students who are blind or have low vision, its limitations necessitate caution. On the other hand, our findings can also be leveraged to design assessments that are difficult for chatbots to solve. </p>
<blockquote>
<p>åŸºäºäººå·¥æ™ºèƒ½çš„èŠå¤©æœºå™¨äººç”±äºå…¶è§£é‡Šå’Œå“åº”æ–‡æœ¬å’Œè§†è§‰è¾“å…¥çš„èƒ½åŠ›ï¼Œæ­£åœ¨è¶Šæ¥è¶Šå¤šåœ°å½±å“ç‰©ç†æ•™è‚²ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸¤ä¸ªå¤§å‹å¤šæ¨¡å¼æ¨¡å‹åŸºç¡€èŠå¤©æœºå™¨äººChatGPT-4å’ŒChatGPT-4oåœ¨â€œç®€çŸ­çš„ç”µä¸ç£è¯„ä¼°(BEMA)â€ä¸Šçš„è¡¨ç°ã€‚BEMAæ˜¯ä¸€ä¸ªæ¦‚å¿µä¸°å¯Œçš„ç‰©ç†é¢˜åº“ï¼ŒåŒ…å«çŸ¢é‡åœºã€ç”µè·¯å›¾å’Œå›¾è¡¨ç­‰è§†è§‰è¡¨ç¤ºã€‚å®šé‡åˆ†æè¡¨æ˜ï¼ŒChatGPT-4oåœ¨ChatGPT-4å’Œå¤§é‡å¤§å­¦ç”Ÿæ ·æœ¬ä¸­çš„è¡¨ç°æ›´ä¸ºå‡ºè‰²ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨è§†è§‰è§£é‡Šèƒ½åŠ›æ–¹é¢å¯¹å‰èº«ChatGPT-4çš„æ”¹è¿›ã€‚ç„¶è€Œï¼Œå¯¹ChatGPT-4oçš„å›åº”çš„å®šæ€§åˆ†ææ­ç¤ºäº†æŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬ç¡®å®šäº†èŠå¤©æœºå™¨äººåœ¨BEMAä»»åŠ¡ä¸­çš„ä¸‰ç§å›°éš¾ï¼šä¸€æ˜¯è§†è§‰è§£é‡Šçš„å›°éš¾ï¼ŒäºŒæ˜¯æä¾›æ­£ç¡®çš„ç‰©ç†å®šå¾‹æˆ–è§„åˆ™çš„å›°éš¾ï¼Œä¸‰æ˜¯ç©ºé—´åè°ƒå’Œç‰©ç†è¡¨ç¤ºåº”ç”¨çš„å›°éš¾ã€‚ç©ºé—´æ¨ç†ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯é‚£äº›éœ€è¦ä½¿ç”¨å³æ‰‹å®šåˆ™çš„ä»»åŠ¡ï¼Œè¯æ˜ç‰¹åˆ«æ£˜æ‰‹ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæœ€å¹¿æ³›ä½¿ç”¨çš„åŸºäºå¤§å‹å¤šæ¨¡å¼æ¨¡å‹çš„èŠå¤©æœºå™¨äººChatGPT-4oåœ¨æ¶‰åŠè§†è§‰è¡¨ç¤ºçš„ç‰©ç†å­¦ä»»åŠ¡ä¸­ä»å­˜åœ¨é‡å¤§å›°éš¾ã€‚è™½ç„¶èŠå¤©æœºå™¨äººåœ¨æ•™è‚²åº”ç”¨æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼ŒåŒ…æ‹¬ä¸ªæ€§åŒ–è¾…å¯¼å’Œæ”¯æŒç›²äººæˆ–è§†åŠ›ä¸ä½³çš„å­¦ç”Ÿï¼Œä½†å…¶å±€é™æ€§éœ€è¦è°¨æ…å¯¹å¾…ã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬çš„å‘ç°ä¹Ÿå¯ä»¥ç”¨æ¥è®¾è®¡å¯¹èŠå¤©æœºå™¨äººæ¥è¯´éš¾ä»¥è§£å†³çš„è¯„ä¼°é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10019v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>äººå·¥æ™ºèƒ½èŠå¤©æœºå™¨äººå¯¹ç‰©ç†æ•™è‚²çš„å½±å“æ—¥ç›Šæ˜¾è‘—ï¼Œå®ƒä»¬èƒ½å¤Ÿè§£é‡Šå’Œå›åº”æ–‡æœ¬å’Œè§†è§‰è¾“å…¥ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸¤ä¸ªåŸºäºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„èŠå¤©æœºå™¨äººChatGPT-4å’ŒChatGPT-4oåœ¨åŒ…å«çŸ¢é‡åœºã€ç”µè·¯å›¾å’Œå›¾è¡¨ç­‰è§†è§‰å‘ˆç°ä¸°å¯Œçš„æ¦‚å¿µæ€§ç‰©ç†æµ‹è¯•â€”â€”ç®€çŸ­ç”µåŠ›ä¸ç£å­¦è¯„ä¼°ï¼ˆBEMAï¼‰ä¸Šçš„è¡¨ç°ã€‚å®šé‡åˆ†ææ˜¾ç¤ºï¼ŒChatGPT-4oåœ¨ChatGPT-4å’Œå¤§é‡å¤§å­¦ç”Ÿæ ·æœ¬ä¸­çš„è¡¨ç°æ›´ä¼˜ç§€ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨è§†è§‰è§£é‡Šèƒ½åŠ›æ–¹é¢çš„æ”¹è¿›ã€‚ç„¶è€Œï¼Œå¯¹ChatGPT-4oçš„å“åº”çš„å®šæ€§åˆ†ææ­ç¤ºäº†æŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚å…¶å›°éš¾åŒ…æ‹¬ä¸‰ä¸ªæ–¹é¢ï¼šè§†è§‰è§£é‡Šæ–¹é¢çš„å›°éš¾ã€æä¾›æ­£ç¡®çš„ç‰©ç†å®šå¾‹æˆ–è§„åˆ™çš„å›°éš¾ä»¥åŠåœ¨ç©ºé—´åè°ƒå’Œç‰©ç†è¡¨ç¤ºåº”ç”¨æ–¹é¢çš„å›°éš¾ã€‚ç©ºé—´æ¨ç†ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯é‚£äº›éœ€è¦ä½¿ç”¨å³æ‰‹è§„åˆ™çš„ä»»åŠ¡ï¼Œç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæœ€å¹¿æ³›ä½¿ç”¨çš„å¤§å‹åŸºäºå¤šæ¨¡æ€æ¨¡å‹çš„èŠå¤©æœºå™¨äººChatGPT-4oåœ¨æ¶‰åŠè§†è§‰å‘ˆç°çš„ç‰©ç†ä»»åŠ¡ä¸­ä»å­˜åœ¨æ˜¾è‘—å›°éš¾ã€‚è™½ç„¶è¯¥èŠå¤©æœºå™¨äººåœ¨ä¸ªæ€§åŒ–è¾…å¯¼å’Œè§†åŠ›éšœç¢å­¦ç”Ÿæ”¯æŒæ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œä½†å…¶å±€é™æ€§éœ€è¦è°¨æ…å¯¹å¾…ã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬çš„å‘ç°ä¹Ÿå¯ç”¨äºè®¾è®¡éš¾ä»¥è¢«èŠå¤©æœºå™¨äººè§£å†³çš„è¯„ä¼°æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½èŠå¤©æœºå™¨äººåœ¨ç‰©ç†æ•™è‚²ä¸­å½±å“æ˜¾è‘—ï¼Œå¾—ç›Šäºå…¶è§£é‡Šå’Œå›åº”æ–‡æœ¬å’Œè§†è§‰è¾“å…¥çš„èƒ½åŠ›ã€‚</li>
<li>ChatGPT-4oåœ¨è§†è§‰è§£é‡Šèƒ½åŠ›ä¸Šç›¸è¾ƒäºChatGPT-4æœ‰æ‰€æå‡ï¼Œå¹¶åœ¨ç®€çŸ­ç”µåŠ›ä¸ç£å­¦è¯„ä¼°ä¸­è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>èŠå¤©æœºå™¨äººåœ¨è§†è§‰è§£é‡Šã€æä¾›æ­£ç¡®ç‰©ç†å®šå¾‹æˆ–è§„åˆ™ä»¥åŠç©ºé—´åè°ƒå’Œç‰©ç†è¡¨ç¤ºåº”ç”¨æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>ç©ºé—´æ¨ç†ä»»åŠ¡å¯¹èŠå¤©æœºå™¨äººæ¥è¯´å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ChatGPT-4oè™½ç„¶å…·æœ‰æ•™è‚²åº”ç”¨æ½œåŠ›ï¼Œå¦‚ä¸ªæ€§åŒ–è¾…å¯¼å’Œè§†åŠ›éšœç¢å­¦ç”Ÿæ”¯æŒï¼Œä½†å­˜åœ¨å±€é™æ€§éœ€è°¨æ…å¯¹å¾…ã€‚</li>
<li>ç ”ç©¶ç»“æœå¯ç”¨äºè®¾è®¡éš¾ä»¥è¢«èŠå¤©æœºå™¨äººè§£å†³çš„è¯„ä¼°æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10019">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e1a2fada1806b793aff4867313337a0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Evaluating-LLM-based-Approaches-to-Legal-Citation-Prediction-Domain-specific-Pre-training-Fine-tuning-or-RAG-A-Benchmark-and-an-Australian-Law-Case-Study"><a href="#Evaluating-LLM-based-Approaches-to-Legal-Citation-Prediction-Domain-specific-Pre-training-Fine-tuning-or-RAG-A-Benchmark-and-an-Australian-Law-Case-Study" class="headerlink" title="Evaluating LLM-based Approaches to Legal Citation Prediction:   Domain-specific Pre-training, Fine-tuning, or RAG? A Benchmark and an   Australian Law Case Study"></a>Evaluating LLM-based Approaches to Legal Citation Prediction:   Domain-specific Pre-training, Fine-tuning, or RAG? A Benchmark and an   Australian Law Case Study</h2><p><strong>Authors:Jiuzhou Han, Paul Burgess, Ehsan Shareghi</strong></p>
<p>Large Language Models (LLMs) have demonstrated strong potential across legal tasks, yet the problem of legal citation prediction remains under-explored. At its core, this task demands fine-grained contextual understanding and precise identification of relevant legislation or precedent. We introduce the AusLaw Citation Benchmark, a real-world dataset comprising 55k Australian legal instances and 18,677 unique citations which to the best of our knowledge is the first of its scale and scope. We then conduct a systematic benchmarking across a range of solutions: (i) standard prompting of both general and law-specialised LLMs, (ii) retrieval-only pipelines with both generic and domain-specific embeddings, (iii) supervised fine-tuning, and (iv) several hybrid strategies that combine LLMs with retrieval augmentation through query expansion, voting ensembles, or re-ranking. Results show that neither general nor law-specific LLMs suffice as stand-alone solutions, with performance near zero. Instruction tuning (of even a generic open-source LLM) on task-specific dataset is among the best performing solutions. We highlight that database granularity along with the type of embeddings play a critical role in retrieval-based approaches, with hybrid methods which utilise a trained re-ranker delivering the best results. Despite this, a performance gap of nearly 50% remains, underscoring the value of this challenging benchmark as a rigorous test-bed for future research in legal-domain. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ³•å¾‹ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œä½†æ³•å¾‹å¼•æ–‡é¢„æµ‹çš„é—®é¢˜ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚è¯¥ä»»åŠ¡çš„æ ¸å¿ƒéœ€æ±‚æ˜¯ç²¾ç»†çš„ä¸Šä¸‹æ–‡ç†è§£å’Œå¯¹ç›¸å…³ç«‹æ³•æˆ–å…ˆä¾‹çš„ç²¾ç¡®è¯†åˆ«ã€‚æˆ‘ä»¬å¼•å…¥äº†AusLawå¼•æ–‡åŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«5.5ä¸‡æ¾³å¤§åˆ©äºšæ³•å¾‹å®ä¾‹å’Œ18677æ¡ç‹¬ç‰¹å¼•æ–‡çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¦‚æ­¤è§„æ¨¡å’ŒèŒƒå›´çš„æ•°æ®é›†ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹ä¸€ç³»åˆ—è§£å†³æ–¹æ¡ˆè¿›è¡Œäº†ç³»ç»Ÿçš„åŸºå‡†æµ‹è¯•ï¼šï¼ˆiï¼‰é€šç”¨å’Œæ³•å¾‹ä¸“ä¸šLLMçš„æ ‡å‡†æç¤ºï¼Œï¼ˆiiï¼‰ä½¿ç”¨é€šç”¨å’Œé¢†åŸŸç‰¹å®šåµŒå…¥çš„ä»…æ£€ç´¢ç®¡é“ï¼Œï¼ˆiiiï¼‰ç›‘ç£å¾®è°ƒï¼Œä»¥åŠï¼ˆivï¼‰å‡ ç§å°†LLMä¸é€šè¿‡æŸ¥è¯¢æ‰©å±•ã€æŠ•ç¥¨é›†åˆæˆ–é‡æ–°æ’åºè¿›è¡Œæ£€ç´¢å¢å¼ºçš„æ··åˆç­–ç•¥ã€‚ç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯é€šç”¨è¿˜æ˜¯æ³•å¾‹ä¸“ä¸šçš„LLMéƒ½ä¸è¶³ä»¥ä½œä¸ºç‹¬ç«‹çš„è§£å†³æ–¹æ¡ˆï¼Œæ€§èƒ½æ¥è¿‘ä¸ºé›¶ã€‚åœ¨ç‰¹å®šä»»åŠ¡æ•°æ®é›†ä¸Šå¯¹ï¼ˆå³ä½¿æ˜¯é€šç”¨çš„å¼€æºLLMï¼‰è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒæ˜¯è¡¨ç°æœ€å¥½çš„è§£å†³æ–¹æ¡ˆä¹‹ä¸€ã€‚æˆ‘ä»¬å¼ºè°ƒï¼Œæ•°æ®åº“ç²’åº¦ä»¥åŠåµŒå…¥ç±»å‹åœ¨åŸºäºæ£€ç´¢çš„æ–¹æ³•ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œåˆ©ç”¨ç»è¿‡è®­ç»ƒçš„é‡æ–°æ’åºå™¨çš„æ··åˆæ–¹æ³•å–å¾—äº†æœ€å¥½çš„ç»“æœã€‚å°½ç®¡å¦‚æ­¤ï¼Œä»å­˜åœ¨è¿‘50%çš„æ€§èƒ½å·®è·ï¼Œè¿™å‡¸æ˜¾äº†åœ¨è¿™ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä½œä¸ºæ³•å¾‹é¢†åŸŸæœªæ¥ç ”ç©¶çš„ä¸¥æ ¼æµ‹è¯•å¹³å°çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06272v2">PDF</a> For code, data, and models see <a target="_blank" rel="noopener" href="https://auslawbench.github.io/">https://auslawbench.github.io</a></p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ³•å¾‹ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†æ³•å¾‹å¼•æ–‡é¢„æµ‹é—®é¢˜ä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚æœ¬æ–‡å¼•å…¥äº†AusLaw Citation Benchmarkæ•°æ®é›†ï¼ŒåŒ…å«5.5ä¸‡ä»½æ¾³å¤§åˆ©äºšæ³•å¾‹æ¡ˆä¾‹å’Œ1.8ä¸‡å¤šä¸ªç‹¬ç‰¹å¼•æ–‡ï¼Œæ˜¯è¿„ä»Šä¸ºæ­¢è§„æ¨¡æœ€å¤§ã€èŒƒå›´æœ€å¹¿çš„æ•°æ®é›†ã€‚æ–‡ç« å¯¹ä¸€ç³»åˆ—è§£å†³æ–¹æ¡ˆè¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼ŒåŒ…æ‹¬æ ‡å‡†æç¤ºé€šç”¨å’Œæ³•å¾‹ä¸“ä¸šåŒ–çš„LLMæ¨¡å‹ç­‰ã€‚ç»“æœæŒ‡å‡ºï¼Œé€šç”¨å’Œæ³•å¾‹ä¸“ä¸šLLMæ¨¡å‹å•ç‹¬ä½¿ç”¨æ•ˆæœæœ‰é™ï¼Œæ¥è¿‘é›¶æ€§èƒ½ã€‚é€šè¿‡ä»»åŠ¡ç‰¹å®šæ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒæ˜¯è¡¨ç°æœ€å¥½çš„è§£å†³æ–¹æ¡ˆä¹‹ä¸€ã€‚æ–‡ç« å¼ºè°ƒæ•°æ®åº“ç²’åº¦å’ŒåµŒå…¥ç±»å‹åœ¨åŸºäºæ£€ç´¢çš„æ–¹æ³•ä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œåˆ©ç”¨è®­ç»ƒåçš„é‡æ–°æ’åå™¨è¿›è¡Œæ··åˆæ–¹æ³•å–å¾—æœ€ä½³ç»“æœã€‚å°½ç®¡å¦‚æ­¤ï¼Œä»å­˜åœ¨è¿‘50%çš„æ€§èƒ½å·®è·ï¼Œçªæ˜¾å‡ºè¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•å¯¹äºæœªæ¥æ³•å¾‹é¢†åŸŸç ”ç©¶çš„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMsåœ¨æ³•å¾‹ä»»åŠ¡ä¸­æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†æ³•å¾‹å¼•æ–‡é¢„æµ‹ä»æ˜¯æœªå……åˆ†æ¢ç´¢çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†AusLaw Citation Benchmarkæ•°æ®é›†ï¼Œè§„æ¨¡å¹¿æ³›ã€‚</li>
<li>è¯„ä¼°äº†å¤šç§è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬LLMæ¨¡å‹çš„æ ‡å‡†æç¤ºã€æ£€ç´¢ç®¡é“ç­‰ã€‚</li>
<li>é€šç”¨å’Œæ³•å¾‹ä¸“ä¸šLLMæ¨¡å‹å•ç‹¬ä½¿ç”¨æ•ˆæœæœ‰é™ã€‚</li>
<li>ä»»åŠ¡ç‰¹å®šæ•°æ®é›†çš„æŒ‡ä»¤å¾®è°ƒæ˜¯æœ€ä½³è§£å†³æ–¹æ¡ˆä¹‹ä¸€ã€‚</li>
<li>æ•°æ®åº“ç²’åº¦å’ŒåµŒå…¥ç±»å‹åœ¨åŸºäºæ£€ç´¢çš„æ–¹æ³•ä¸­è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06272">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e2578dfe470a350fea900246f4feeb8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d29450757528427670da9dc87b46c11e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="How-Out-of-Distribution-Detection-Learning-Theory-Enhances-Transformer-Learnability-and-Reliability"><a href="#How-Out-of-Distribution-Detection-Learning-Theory-Enhances-Transformer-Learnability-and-Reliability" class="headerlink" title="How Out-of-Distribution Detection Learning Theory Enhances Transformer:   Learnability and Reliability"></a>How Out-of-Distribution Detection Learning Theory Enhances Transformer:   Learnability and Reliability</h2><p><strong>Authors:Yijin Zhou, Yutang Ge, Xiaowen Dong, Yuguang Wang</strong></p>
<p>Transformers excel in natural language processing and computer vision tasks. However, they still face challenges in generalizing to Out-of-Distribution (OOD) datasets, i.e. data whose distribution differs from that seen during training. OOD detection aims to distinguish outliers while preserving in-distribution (ID) data performance. This paper introduces the OOD detection Probably Approximately Correct (PAC) Theory for transformers, which establishes the conditions for data distribution and model configurations for the OOD detection learnability of transformers. It shows that outliers can be accurately represented and distinguished with sufficient data under conditions. The theoretical implications highlight the trade-off between theoretical principles and practical training paradigms. By examining this trade-off, we naturally derived the rationale for leveraging auxiliary outliers to enhance OOD detection. Our theory suggests that by penalizing the misclassification of outliers within the loss function and strategically generating soft synthetic outliers, one can robustly bolster the reliability of transformer networks. This approach yields a novel algorithm that ensures learnability and refines the decision boundaries between inliers and outliers. In practice, the algorithm consistently achieves state-of-the-art (SOTA) performance across various data formats. </p>
<blockquote>
<p>Transformeråœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ³›åŒ–åˆ°åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ•°æ®é›†æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå³é‚£äº›ä¸è®­ç»ƒæœŸé—´æ‰€è§åˆ†å¸ƒä¸åŒçš„æ•°æ®ã€‚OODæ£€æµ‹æ—¨åœ¨åŒºåˆ†å¼‚å¸¸å€¼ï¼ŒåŒæ—¶ä¿æŒå†…éƒ¨åˆ†å¸ƒï¼ˆIDï¼‰æ•°æ®æ€§èƒ½ã€‚æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹Transformerçš„OODæ£€æµ‹å¯èƒ½è¿‘ä¼¼æ­£ç¡®ï¼ˆPACï¼‰ç†è®ºï¼Œè¯¥ç†è®ºç¡®å®šäº†æ•°æ®åˆ†å¸ƒå’Œæ¨¡å‹é…ç½®çš„æ¡ä»¶ï¼Œä¸ºTransformerçš„OODæ£€æµ‹å­¦ä¹ èƒ½åŠ›å¥ å®šäº†åŸºç¡€ã€‚å®ƒè¡¨æ˜ï¼Œåœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼Œé€šè¿‡è¶³å¤Ÿçš„æ•°æ®ï¼Œå¼‚å¸¸å€¼å¯ä»¥ç²¾ç¡®åœ°è¡¨ç¤ºå’ŒåŒºåˆ†ã€‚ç†è®ºå½±å“çªå‡ºäº†ç†è®ºåŸåˆ™å’Œå®è·µè®­ç»ƒèŒƒå¼ä¹‹é—´çš„æƒè¡¡ã€‚é€šè¿‡è€ƒå¯Ÿè¿™ç§æƒè¡¡ï¼Œæˆ‘ä»¬è‡ªç„¶åœ°å¾—å‡ºäº†åˆ©ç”¨è¾…åŠ©å¼‚å¸¸å€¼å¢å¼ºOODæ£€æµ‹çš„ç†ç”±ã€‚æˆ‘ä»¬çš„ç†è®ºè®¤ä¸ºï¼Œé€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­æƒ©ç½šå¼‚å¸¸å€¼çš„è¯¯åˆ†ç±»ï¼Œå¹¶æˆ˜ç•¥æ€§åœ°ç”Ÿæˆè½¯åˆæˆå¼‚å¸¸å€¼ï¼Œå¯ä»¥ç¨³å¥åœ°æé«˜Transformerç½‘ç»œçš„å¯é æ€§ã€‚è¿™ç§æ–¹æ³•äº§ç”Ÿäº†ä¸€ç§æ–°ç®—æ³•ï¼Œè¯¥ç®—æ³•ç¡®ä¿äº†å­¦ä¹ èƒ½åŠ›å¹¶ç»†åŒ–äº†å†…å€¼ï¼ˆinliersï¼‰å’Œå¼‚å¸¸å€¼ä¹‹é—´çš„å†³ç­–è¾¹ç•Œã€‚åœ¨å®è·µä¸­ï¼Œè¯¥ç®—æ³•åœ¨å„ç§æ•°æ®æ ¼å¼ä¸Šå§‹ç»ˆå®ç°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³çš„è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.12915v5">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹Transformeråœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­é¢ä¸´çš„Out-of-Distributionï¼ˆOODï¼‰æ•°æ®é›†æŒ‘æˆ˜ï¼Œå¼•å…¥äº†OODæ£€æµ‹çš„PACç†è®ºã€‚è¯¥ç†è®ºä¸ºTransformeråœ¨OODæ£€æµ‹ä¸­çš„å­¦ä¹ æ€§è®¾å®šäº†æ•°æ®åˆ†å¸ƒå’Œæ¨¡å‹é…ç½®çš„æ¡ä»¶ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼Œå¼‚å¸¸å€¼å¯ä»¥å‡†ç¡®è¡¨ç¤ºå’ŒåŒºåˆ†ã€‚ç†è®ºä¸Šçš„æƒè¡¡ä¸ºåˆ©ç”¨è¾…åŠ©å¼‚å¸¸å€¼å¢å¼ºOODæ£€æµ‹æä¾›äº†ä¾æ®ã€‚é€šè¿‡æƒ©ç½šæŸå¤±å‡½æ•°ä¸­å¼‚å¸¸å€¼çš„è¯¯åˆ†ç±»ï¼Œå¹¶æˆ˜ç•¥æ€§åœ°ç”Ÿæˆè½¯åˆæˆå¼‚å¸¸å€¼ï¼Œå¯ä»¥ç¨³å¥åœ°æé«˜Transformerç½‘ç»œçš„å¯é æ€§ã€‚è¿™ç§æ–¹æ³•å½¢æˆäº†ä¸€ä¸ªæ–°ç®—æ³•ï¼Œè¯¥ç®—æ³•ä¿è¯äº†å­¦ä¹ æ€§å¹¶ä¼˜åŒ–äº†æ­£å¸¸å€¼å’Œå¼‚å¸¸å€¼ä¹‹é—´çš„å†³ç­–è¾¹ç•Œï¼Œåœ¨å®é™…åº”ç”¨ä¸­å®ç°äº†è·¨å„ç§æ•°æ®æ ¼å¼çš„æœ€æ–°æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformersé¢ä¸´åœ¨OODæ•°æ®é›†ä¸Šæ³›åŒ–çš„æŒ‘æˆ˜ã€‚</li>
<li>PACç†è®ºä¸ºTransformeråœ¨OODæ£€æµ‹ä¸­çš„å­¦ä¹ æ€§æä¾›äº†ç†è®ºåŸºç¡€ã€‚</li>
<li>åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼Œå¼‚å¸¸å€¼å¯ä»¥å‡†ç¡®è¡¨ç¤ºå’ŒåŒºåˆ†ã€‚</li>
<li>ç†è®ºä¸Šçš„æƒè¡¡å¯¹äºå¢å¼ºOODæ£€æµ‹å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨è¾…åŠ©å¼‚å¸¸å€¼å’Œè°ƒæ•´æŸå¤±å‡½æ•°ï¼Œå¯ä»¥æé«˜Transformerç½‘ç»œçš„å¯é æ€§ã€‚</li>
<li>æ–°ç®—æ³•ä¼˜åŒ–äº†æ­£å¸¸å€¼å’Œå¼‚å¸¸å€¼ä¹‹é—´çš„å†³ç­–è¾¹ç•Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.12915">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-84f379e2da105414a2929b296d349c0a.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="EntGPT-Entity-Linking-with-Generative-Large-Language-Models"><a href="#EntGPT-Entity-Linking-with-Generative-Large-Language-Models" class="headerlink" title="EntGPT: Entity Linking with Generative Large Language Models"></a>EntGPT: Entity Linking with Generative Large Language Models</h2><p><strong>Authors:Yifan Ding, Amrit Poudel, Qingkai Zeng, Tim Weninger, Balaji Veeramani, Sanmitra Bhattacharya</strong></p>
<p>Entity Linking in natural language processing seeks to match text entities to their corresponding entries in a dictionary or knowledge base. Traditional approaches rely on contextual models, which can be complex, hard to train, and have limited transferability across different domains. Generative large language models like GPT offer a promising alternative but often underperform with naive prompts. In this study, we introduce EntGPT, employing advanced prompt engineering to enhance EL tasks. Our three-step hard-prompting method (EntGPT-P) significantly boosts the micro-F_1 score by up to 36% over vanilla prompts, achieving competitive performance across 10 datasets without supervised fine-tuning. Additionally, our instruction tuning method (EntGPT-I) improves micro-F_1 scores by 2.1% on average in supervised EL tasks and outperforms several baseline models in six Question Answering tasks. Our methods are compatible with both open-source and proprietary LLMs. All data and code are available on GitHub at <a target="_blank" rel="noopener" href="https://github.com/yifding/In_Context_EL">https://github.com/yifding/In_Context_EL</a>. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å®ä½“é“¾æ¥æ—¨åœ¨å°†æ–‡æœ¬å®ä½“åŒ¹é…åˆ°è¯å…¸æˆ–çŸ¥è¯†åº“ä¸­çš„ç›¸åº”æ¡ç›®ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ä¾èµ–äºä¸Šä¸‹æ–‡æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å¯èƒ½å¾ˆå¤æ‚ã€éš¾ä»¥è®­ç»ƒï¼Œå¹¶ä¸”åœ¨ä¸åŒé¢†åŸŸçš„è¿ç§»èƒ½åŠ›æœ‰é™ã€‚åƒGPTè¿™æ ·çš„ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†é€šå¸¸åœ¨æ²¡æœ‰æç¤ºçš„æƒ…å†µä¸‹è¡¨ç°ä¸ä½³ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†EntGPTï¼Œé‡‡ç”¨å…ˆè¿›çš„æç¤ºæŠ€æœ¯æ¥å¢å¼ºå®ä½“é“¾æ¥ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ä¸‰æ­¥ç¡¬æç¤ºæ–¹æ³•ï¼ˆEntGPT-Pï¼‰é€šè¿‡å…ˆè¿›çš„æç¤ºæŠ€æœ¯æ˜¾è‘—æé«˜äº†å¾®è°ƒå‰çš„å¾®F_1åˆ†æ•°ï¼Œç›¸è¾ƒäºå¸¸è§„æç¤ºæœ€é«˜æå‡äº†36%ï¼Œåœ¨åä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æŒ‡ä»¤è°ƒæ•´æ–¹æ³•ï¼ˆEntGPT-Iï¼‰åœ¨ç›‘ç£çš„å®ä½“é“¾æ¥ä»»åŠ¡ä¸­å¹³å‡æé«˜äº†å¾®F_1åˆ†æ•°è¾¾2.1%ï¼Œå¹¶ä¸”åœ¨å…­ä¸ªé—®ç­”ä»»åŠ¡ä¸­ä¼˜äºå‡ ä¸ªåŸºçº¿æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸å¼€æºå’Œä¸“æœ‰å¤§å‹è¯­è¨€æ¨¡å‹å…¼å®¹ã€‚æ‰€æœ‰æ•°æ®ä»£ç å‡å¯åœ¨GitHubä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/yifding/In_Context_EL%E3%80%82">https://github.com/yifding/In_Context_ELã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.06738v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å®ä½“é“¾æ¥ï¼ˆEntity Linkingï¼ŒELï¼‰æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸€é¡¹ä»»åŠ¡ï¼Œæ—¨åœ¨å°†æ–‡æœ¬ä¸­çš„å®ä½“ä¸å­—å…¸æˆ–çŸ¥è¯†åº“ä¸­çš„ç›¸åº”æ¡ç›®åŒ¹é…ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºä¸Šä¸‹æ–‡æ¨¡å‹ï¼Œå¯èƒ½å¤æ‚ã€è®­ç»ƒå›°éš¾ï¼Œä¸”åœ¨ä¸åŒé¢†åŸŸçš„è¿ç§»èƒ½åŠ›æœ‰é™ã€‚æœ¬ç ”ç©¶å¼•å…¥EntGPTï¼Œé‡‡ç”¨å…ˆè¿›çš„æç¤ºå·¥ç¨‹æŠ€æœ¯æ¥æå‡ELä»»åŠ¡æ€§èƒ½ã€‚EntGPT-Pçš„ä¸‰æ­¥ç¡¬æç¤ºæ–¹æ³•èƒ½æ˜¾è‘—æå‡å¾®F_1åˆ†æ•°ï¼Œæœ€é«˜æå‡è¾¾36%ï¼Œä¸”åœ¨æ— éœ€ç›‘ç£å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨10ä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å…·æœ‰ç«äº‰åŠ›ã€‚EntGPT-Içš„æŒ‡ä»¤è°ƒæ•´æ–¹æ³•åˆ™åœ¨ç›‘ç£ELä»»åŠ¡ä¸­å¹³å‡æé«˜å¾®F_1åˆ†æ•°2.1%ï¼Œå¹¶åœ¨å…­ä¸ªé—®ç­”ä»»åŠ¡ä¸­ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚æœ¬ç ”ç©¶çš„æ–¹æ³•ä¸å¼€æºå’Œä¸“æœ‰å¤§å‹è¯­è¨€æ¨¡å‹å‡å…¼å®¹ï¼Œç›¸å…³æ•°æ®å’Œä»£ç å·²ä¸Šä¼ è‡³GitHubï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/yifding/In_Context_EL%EF%BC%89%E3%80%82">https://github.com/yifding/In_Context_ELï¼‰ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®ä½“é“¾æ¥ï¼ˆELï¼‰æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸€é¡¹é‡è¦ä»»åŠ¡ï¼Œæ—¨åœ¨åŒ¹é…æ–‡æœ¬å®ä½“ä¸å­—å…¸æˆ–çŸ¥è¯†åº“ä¸­çš„æ¡ç›®ã€‚</li>
<li>ä¼ ç»ŸELæ–¹æ³•ä¾èµ–äºå¤æ‚çš„ä¸Šä¸‹æ–‡æ¨¡å‹ï¼Œä¸”åœ¨ä¸åŒé¢†åŸŸçš„è¿ç§»èƒ½åŠ›æœ‰é™ã€‚</li>
<li>æœ¬ç ”ç©¶å¼•å…¥EntGPTï¼Œé€šè¿‡å…ˆè¿›çš„æç¤ºå·¥ç¨‹æŠ€æœ¯æå‡ELä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>EntGPT-Pçš„ä¸‰æ­¥ç¡¬æç¤ºæ–¹æ³•èƒ½æ˜¾è‘—æå‡å¾®F_1åˆ†æ•°ï¼Œæœ€é«˜æå‡è¾¾36%ï¼Œä¸”åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜ç§€ã€‚</li>
<li>EntGPT-Içš„æŒ‡ä»¤è°ƒæ•´æ–¹æ³•åœ¨ç›‘ç£ELä»»åŠ¡å’Œé—®ç­”ä»»åŠ¡ä¸­å‡æœ‰ä¼˜å¼‚è¡¨ç°ã€‚</li>
<li>æœ¬ç ”ç©¶çš„æ–¹æ³•ä¸å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹å…¼å®¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.06738">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a0b538f1e6502c3cd772b686a02d0be1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4ecb95a25c5c6465be47fca0131a32f.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-25/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-25/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-25/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-70c7f4cd00d993f756664c1de8e0ada4.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-25  Boosting Few-Shot Open-Set Object Detection via Prompt Learning and   Robust Decision Boundary
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-25/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e0ddb9172526b07885c64909614a637b.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-25  R1-ShareVL Incentivizing Reasoning Capability of Multimodal Large   Language Models via Share-GRPO
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19380.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
