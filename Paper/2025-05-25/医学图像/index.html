<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-25  Evaluation and optimization of deep learning models for enhanced   detection of brain cancer using transmission optical microscopy of thin brain   tissue samples">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-10c20e5ebcfcbbe0109d82b703a8120c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    58 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-25-æ›´æ–°"><a href="#2025-05-25-æ›´æ–°" class="headerlink" title="2025-05-25 æ›´æ–°"></a>2025-05-25 æ›´æ–°</h1><h2 id="Evaluation-and-optimization-of-deep-learning-models-for-enhanced-detection-of-brain-cancer-using-transmission-optical-microscopy-of-thin-brain-tissue-samples"><a href="#Evaluation-and-optimization-of-deep-learning-models-for-enhanced-detection-of-brain-cancer-using-transmission-optical-microscopy-of-thin-brain-tissue-samples" class="headerlink" title="Evaluation and optimization of deep learning models for enhanced   detection of brain cancer using transmission optical microscopy of thin brain   tissue samples"></a>Evaluation and optimization of deep learning models for enhanced   detection of brain cancer using transmission optical microscopy of thin brain   tissue samples</h2><p><strong>Authors:Mohnish Sao, Mousa Alrubayan, Prabhakar Pradhan</strong></p>
<p>Optical transmission spectroscopy is one method to understand brain tissue structural properties from brain tissue biopsy samples, yet manual interpretation is resource intensive and prone to inter observer variability. Deep convolutional neural networks (CNNs) offer automated feature learning directly from raw brightfield images. Here, we evaluate ResNet50 and DenseNet121 on a curated dataset of 2,931 bright-field transmission optical microscopy images of thin brain tissue, split into 1,996 for training, 437 for validation, and 498 for testing. Our two stage transfer learning protocol involves initial training of a classifier head on frozen pretrained feature extractors, followed by fine tuning of deeper convolutional blocks with extensive data augmentation (rotations, flips, intensity jitter) and early stopping. DenseNet121 achieves 88.35 percent test accuracy, 0.9614 precision, 0.8667 recall, and 0.9116 F1 score the best performance compared to ResNet50 (82.12 percent, 0.9035, 0.8142, 0.8563). Detailed analysis of confusion matrices, training and validation curves, and classwise prediction distributions illustrates robust convergence and minimal bias. These findings demonstrate the superior generalization of dense connectivity on limited medical datasets and outline future directions for multi-class tumor grading and clinical translation. </p>
<blockquote>
<p>å…‰å­¦ä¼ è¾“å…‰è°±æ³•æ˜¯ä¸€ç§é€šè¿‡è„‘ç»„ç»‡æ´»æ£€æ ·æœ¬äº†è§£è„‘ç»„ç»‡ç»“æ„æ€§è´¨çš„æ–¹æ³•ï¼Œä½†äººå·¥è§£è¯»éœ€è¦å¤§é‡èµ„æºï¼Œä¸”æ˜“å‡ºç°è§‚å¯Ÿè€…é—´å·®å¼‚ã€‚æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å¯ä»¥ç›´æ¥ä»åŸå§‹æ˜åœºå›¾åƒä¸­å®ç°è‡ªåŠ¨åŒ–ç‰¹å¾å­¦ä¹ ã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬å¯¹åŒ…å«2931å¼ è„‘ç»„ç»‡è–„ç‰‡æ˜åœºé€å°„å…‰å­¦æ˜¾å¾®é•œå›¾åƒçš„å®šåˆ¶æ•°æ®é›†è¿›è¡Œäº†ResNet50å’ŒDenseNet121çš„è¯„ä¼°ï¼Œå…¶ä¸­1996å¼ ç”¨äºè®­ç»ƒï¼Œ437å¼ ç”¨äºéªŒè¯ï¼Œ498å¼ ç”¨äºæµ‹è¯•ã€‚æˆ‘ä»¬çš„ä¸¤é˜¶æ®µè¿ç§»å­¦ä¹ åè®®æ¶‰åŠé¦–å…ˆåœ¨å†»ç»“çš„é¢„è®­ç»ƒç‰¹å¾æå–å™¨ä¸Šè®­ç»ƒåˆ†ç±»å™¨å¤´éƒ¨ï¼Œéšåé€šè¿‡ä¸°å¯Œçš„æ•°æ®å¢å¼ºï¼ˆæ—‹è½¬ã€ç¿»è½¬ã€å¼ºåº¦æŠ–åŠ¨ï¼‰è¿›è¡Œæ·±å±‚å·ç§¯å—çš„å¾®è°ƒï¼Œå¹¶æ—©æœŸåœæ­¢è®­ç»ƒã€‚DenseNet121è¾¾åˆ°äº†88.35%çš„æµ‹è¯•å‡†ç¡®ç‡ã€0.9614çš„ç²¾ç¡®åº¦ã€0.8667çš„å¬å›ç‡å’Œ0.9116çš„F1åˆ†æ•°ï¼Œä¸ResNet50ç›¸æ¯”è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼ˆåˆ†åˆ«ä¸º82.12%ã€0.9035ã€0.8142å’Œ0.8563ï¼‰ã€‚å¯¹æ··æ·†çŸ©é˜µã€è®­ç»ƒå’ŒéªŒè¯æ›²çº¿ä»¥åŠç±»åˆ«é¢„æµ‹åˆ†å¸ƒçš„è¯¦ç»†åˆ†æï¼Œè¯æ˜äº†å…¶ç¨³å¥çš„æ”¶æ•›æ€§å’Œè¾ƒä½çš„åè§ã€‚è¿™äº›å‘ç°è¡¨æ˜å¯†é›†è¿æ¥åœ¨æœ‰é™åŒ»ç–—æ•°æ®é›†ä¸Šçš„ä¼˜è¶Šæ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æŒ‡å‡ºäº†å¤šç±»è‚¿ç˜¤åˆ†çº§å’Œä¸´åºŠè½¬åŒ–çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11735v1">PDF</a> 10 pages, 5 figures</p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ ä¸­çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å¯è‡ªåŠ¨å­¦ä¹ è„‘ç»„ç»‡æ´»æ£€æ ·æœ¬çš„ç»“æ„ç‰¹æ€§ï¼Œå‡è½»äººå·¥è§£è¯»çš„è´Ÿæ‹…å¹¶å‡å°‘è§‚å¯Ÿè€…é—´çš„å·®å¼‚ã€‚æœ¬ç ”ç©¶å¯¹ResNet50å’ŒDenseNet121ç½‘ç»œåœ¨è„‘ç»„ç»‡è–„åˆ‡ç‰‡é€å°„å…‰å­¦æ˜¾å¾®é•œå›¾åƒä¸Šè¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºDenseNet121åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°å‡é«˜äºResNet50ã€‚è¿™è¡¨æ˜åœ¨æœ‰é™çš„åŒ»å­¦æ•°æ®é›†ä¸­ï¼Œå¯†é›†è¿æ¥ç½‘ç»œå…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸ºå¤šç±»è‚¿ç˜¤åˆ†çº§å’Œä¸´åºŠè½¬åŒ–æä¾›äº†æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Optical transmission spectroscopyå¯ç”¨äºé€šè¿‡è„‘ç»„ç»‡æ´»æ£€æ ·æœ¬äº†è§£è„‘ç»„ç»‡ç»“æ„æ€§ç‰¹å¾ã€‚</li>
<li>æ‰‹åŠ¨è§£è¯»è¿™äº›ç‰¹å¾èµ„æºæ¶ˆè€—å¤§ä¸”å­˜åœ¨è§‚å¯Ÿè€…é—´å·®å¼‚ã€‚</li>
<li>Deep convolutional neural networks (CNNs)èƒ½è‡ªåŠ¨ä»åŸå§‹æ˜åœºå›¾åƒä¸­å­¦ä¹ ç‰¹å¾ã€‚</li>
<li>æœ¬ç ”ç©¶å¯¹æ¯”äº†ResNet50å’ŒDenseNet121ç½‘ç»œåœ¨è„‘ç»„ç»‡è–„åˆ‡ç‰‡å›¾åƒæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚</li>
<li>DenseNet121åœ¨å‡†ç¡®ç‡ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°ç­‰è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°æ›´ä½³ã€‚</li>
<li>è¯¦ç»†åˆ†æè¡¨æ˜DenseNet121åœ¨æœ‰é™åŒ»å­¦æ•°æ®é›†ä¸Šå…·æœ‰è¾ƒå¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11735">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-736b31df4f8ccfb8023b1def31398409.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f41c0c47b5fff1a002e3bf704b07e69.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-945dc21300ba24d5b54a51a28e85f3b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2a48cd6105529db4575d10a05a59f91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1d6409e5eb1888eee6f00e5afd70392.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HWA-UNETR-Hierarchical-Window-Aggregate-UNETR-for-3D-Multimodal-Gastric-Lesion-Segmentation"><a href="#HWA-UNETR-Hierarchical-Window-Aggregate-UNETR-for-3D-Multimodal-Gastric-Lesion-Segmentation" class="headerlink" title="HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric   Lesion Segmentation"></a>HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric   Lesion Segmentation</h2><p><strong>Authors:Jiaming Liang, Lihuan Dai, Xiaoqi Sheng, Xiangguang Chen, Chun Yao, Guihua Tao, Qibin Leng, Hongmin Cai, Xi Zhong</strong></p>
<p>Multimodal medical image segmentation faces significant challenges in the context of gastric cancer lesion analysis. This clinical context is defined by the scarcity of independent multimodal datasets and the imperative to amalgamate inherently misaligned modalities. As a result, algorithms are constrained to train on approximate data and depend on application migration, leading to substantial resource expenditure and a potential decline in analysis accuracy. To address those challenges, we have made two major contributions: First, we publicly disseminate the GCM 2025 dataset, which serves as the first large-scale, open-source collection of gastric cancer multimodal MRI scans, featuring professionally annotated FS-T2W, CE-T1W, and ADC images from 500 patients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework that employs an original HWA block with learnable window aggregation layers to establish dynamic feature correspondences between different modalitiesâ€™ anatomical structures, and leverages the innovative tri-orientated fusion mamba mechanism for context modeling and capturing long-range spatial dependencies. Extensive experiments on our GCM 2025 dataset and the publicly BraTS 2021 dataset validate the performance of our framework, demonstrating that the new approach surpasses existing methods by up to 1.68% in the Dice score while maintaining solid robustness. The dataset and code are public via <a target="_blank" rel="noopener" href="https://github.com/JeMing-creater/HWA-UNETR">https://github.com/JeMing-creater/HWA-UNETR</a>. </p>
<blockquote>
<p>åœ¨èƒƒç™Œç—…ç¶åˆ†æçš„èƒŒæ™¯ä¸‹ï¼Œå¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ã€‚è¿™ç§ä¸´åºŠèƒŒæ™¯çš„ç‰¹ç‚¹æ˜¯ç¼ºä¹ç‹¬ç«‹çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œä»¥åŠè¿«åˆ‡éœ€è¦èåˆå†…åœ¨ä¸ä¸€è‡´çš„æ¨¡æ€ã€‚å› æ­¤ï¼Œç®—æ³•å—åˆ°è®­ç»ƒæ•°æ®è¿‘ä¼¼æ€§çš„é™åˆ¶ï¼Œå¹¶ä¾èµ–äºåº”ç”¨è¿ç§»ï¼Œå¯¼è‡´èµ„æºæ¶ˆè€—å·¨å¤§ï¼Œåˆ†æç²¾åº¦å¯èƒ½ä¸‹é™ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åšå‡ºäº†ä¸¤å¤§è´¡çŒ®ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å…¬å¼€ä¼ æ’­äº†GCM 2025æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ˜¯é¦–ä¸ªå¤§è§„æ¨¡çš„ã€å¼€æºçš„èƒƒç™Œå¤šæ¨¡æ€MRIæ‰«æé›†åˆï¼ŒåŒ…å«æ¥è‡ª500åæ‚£è€…çš„ä¸“ä¸šæ³¨é‡ŠFS-T2Wã€CE-T1Wå’ŒADCå›¾åƒã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä»‹ç»äº†HWA-UNETRï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„3Dåˆ†å‰²æ¡†æ¶ï¼Œé‡‡ç”¨åŸå§‹HWAå—å’Œå¯å­¦ä¹ çš„çª—å£èšåˆå±‚æ¥å»ºç«‹ä¸åŒæ¨¡æ€è§£å‰–ç»“æ„ä¹‹é—´çš„åŠ¨æ€ç‰¹å¾å¯¹åº”å…³ç³»ï¼Œå¹¶åˆ©ç”¨åˆ›æ–°çš„ä¸‰è§’å®šå‘èåˆå¦ˆå¦ˆæœºåˆ¶è¿›è¡Œä¸Šä¸‹æ–‡å»ºæ¨¡å’Œæ•è·é•¿ç¨‹ç©ºé—´ä¾èµ–æ€§ã€‚åœ¨æˆ‘ä»¬çš„GCM 2025æ•°æ®é›†å’Œå…¬å¼€çš„BraTS 2021æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶çš„æ€§èƒ½ï¼Œç»“æœè¡¨æ˜ï¼Œæ–°æ–¹æ³•åœ¨Diceå¾—åˆ†ä¸Šæ¯”ç°æœ‰æ–¹æ³•é«˜å‡º1.68%ï¼ŒåŒæ—¶ä¿æŒäº†ç¨³å¥æ€§ã€‚æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/JeMing-creater/HWA-UNETR%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/JeMing-creater/HWA-UNETRå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10464v2">PDF</a> This work has been provisionally accepted for MICCAI 2025</p>
<p><strong>æ‘˜è¦</strong><br>    èƒƒç™Œç—…ç¶åˆ†æçš„å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚ç¼ºä¹ç‹¬ç«‹å¤šæ¨¡æ€æ•°æ®é›†ä»¥åŠéœ€è¦å¯¹å›ºæœ‰é”™é…æ¨¡æ€è¿›è¡Œèåˆçš„ä¸´åºŠç¯å¢ƒï¼Œé™åˆ¶äº†ç®—æ³•çš„è®­ç»ƒæ•°æ®å¹¶ä¾èµ–äºåº”ç”¨è¿ç§»ï¼Œå¯¼è‡´äº†èµ„æºæ¶ˆè€—å¤§ä¸”åˆ†æç²¾åº¦å¯èƒ½ä¸‹é™ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åšå‡ºäº†ä¸¤å¤§è´¡çŒ®ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å…¬å¼€å‘å¸ƒäº†GCM 2025æ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡å…¬å¼€çš„èƒƒç™Œå¤šæ¨¡æ€MRIæ‰«ææ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª500åæ‚£è€…çš„ä¸“ä¸šæ³¨é‡ŠFS-T2Wã€CE-T1Wå’ŒADCå›¾åƒï¼›å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†HWA-UNETRï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„3Dåˆ†å‰²æ¡†æ¶ï¼Œé‡‡ç”¨å¯å­¦ä¹ çš„çª—å£èšåˆå±‚å»ºç«‹ä¸åŒæ¨¡æ€è§£å‰–ç»“æ„ä¹‹é—´çš„åŠ¨æ€ç‰¹å¾å¯¹åº”å…³ç³»ï¼Œå¹¶åˆ©ç”¨åˆ›æ–°çš„ä¸‰è§’èåˆmamaæœºåˆ¶è¿›è¡Œä¸Šä¸‹æ–‡å»ºæ¨¡å’Œæ•è·é•¿æœŸç©ºé—´ä¾èµ–æ€§ã€‚åœ¨æˆ‘ä»¬çš„GCM 2025æ•°æ®é›†å’Œå…¬å¼€çš„BraTS 2021æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ¡†æ¶çš„æ€§èƒ½ï¼Œæ–°æ–¹æ³•çš„Diceå¾—åˆ†é«˜å‡ºç°æœ‰æ–¹æ³•é«˜è¾¾1.68%ï¼ŒåŒæ—¶ä¿æŒäº†ç¨³å¥æ€§ã€‚æ•°æ®é›†å’Œä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/JeMing-creater/HWA-UNETR%E3%80%82">https://github.com/JeMing-creater/HWA-UNETRã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨èƒƒç™Œç—…ç¶åˆ†æä¸­å­˜åœ¨æŒ‘æˆ˜ï¼Œç¼ºä¹ç‹¬ç«‹å¤šæ¨¡æ€æ•°æ®é›†å’Œæ¨¡æ€èåˆçš„éœ€æ±‚æ˜¯ä¸»è¦åŸå› ã€‚</li>
<li>GCM 2025æ•°æ®é›†çš„å…¬å¼€è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œå®ƒåŒ…å«äº†å¤§é‡ç»è¿‡ä¸“ä¸šæ³¨é‡Šçš„èƒƒç™Œå¤šæ¨¡æ€MRIæ‰«ææ•°æ®ã€‚</li>
<li>HWA-UNETRæ¡†æ¶è¢«å¼•å…¥ä»¥è§£å†³å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²çš„æŒ‘æˆ˜ï¼Œå®ƒé‡‡ç”¨å¯å­¦ä¹ çš„çª—å£èšåˆå±‚å»ºç«‹ä¸åŒæ¨¡æ€ä¹‹é—´çš„åŠ¨æ€ç‰¹å¾å¯¹åº”å…³ç³»ã€‚</li>
<li>HWA-UNETRæ¡†æ¶åœ¨GCM 2025æ•°æ®é›†å’ŒBraTS 2021æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜å¼‚ï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•Diceå¾—åˆ†æé«˜1.68%ã€‚</li>
<li>è¯¥æ¡†æ¶å…·å¤‡å¼ºå¤§çš„ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ•è·é•¿æœŸç©ºé—´ä¾èµ–æ€§ã€‚</li>
<li>HWA-UNETRæ¡†æ¶å…·å¤‡ç¨³å¥æ€§ï¼Œèƒ½å¤Ÿåº”å¯¹ä¸åŒæ•°æ®é›†çš„æŒ‘æˆ˜ã€‚</li>
<li>æ•°æ®é›†å’Œä»£ç å·²ç»å…¬å¼€ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fd8aba369a458b3135b43866e1660010.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85a41fcd5f13056645a134422a164310.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1044cfd5bc27eab37db31e98e9f8ceb7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ChestX-Reasoner-Advancing-Radiology-Foundation-Models-with-Reasoning-through-Step-by-Step-Verification"><a href="#ChestX-Reasoner-Advancing-Radiology-Foundation-Models-with-Reasoning-through-Step-by-Step-Verification" class="headerlink" title="ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning   through Step-by-Step Verification"></a>ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning   through Step-by-Step Verification</h2><p><strong>Authors:Ziqing Fan, Cheng Liang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie</strong></p>
<p>Recent advances in reasoning-enhanced large language models (LLMs) and multimodal LLMs (MLLMs) have significantly improved performance in complex tasks, yet medical AI models often overlook the structured reasoning processes inherent in clinical practice. In this work, we present ChestX-Reasoner, a radiology diagnosis MLLM designed to leverage process supervision mined directly from clinical reports, reflecting the step-by-step reasoning followed by radiologists. We construct a large dataset by extracting and refining reasoning chains from routine radiology reports. Our two-stage training framework combines supervised fine-tuning and reinforcement learning guided by process rewards to better align model reasoning with clinical standards. We introduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual question answering samples with 301K clinically validated reasoning steps, and propose RadRScore, a metric evaluating reasoning factuality, completeness, and effectiveness. ChestX-Reasoner outperforms existing medical and general-domain MLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%, and 18% improvements in reasoning ability compared to the best medical MLLM, the best general MLLM, and its base model, respectively, as well as 3.3%, 24%, and 27% improvements in outcome accuracy. All resources are open-sourced to facilitate further research in medical reasoning MLLMs. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œåœ¨æ¨ç†å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€LLMï¼ˆMLLMsï¼‰æ–¹é¢çš„è¿›å±•ï¼Œå·²ç»åœ¨å¤æ‚ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜æ€§èƒ½ã€‚ç„¶è€Œï¼ŒåŒ»ç–—AIæ¨¡å‹å¾€å¾€å¿½ç•¥äº†ä¸´åºŠå®è·µä¸­çš„ç»“æ„åŒ–æ¨ç†è¿‡ç¨‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ChestX-Reasonerï¼Œè¿™æ˜¯ä¸€ç§è®¾è®¡ç”¨äºåˆ©ç”¨ç›´æ¥ä»ä¸´åºŠæŠ¥å‘Šä¸­æŒ–æ˜çš„è¿‡ç¨‹ç›‘ç£çš„æ”¾å°„å­¦è¯Šæ–­MLLMï¼Œåæ˜ äº†æ”¾å°„åŒ»å¸ˆéµå¾ªçš„é€æ­¥æ¨ç†ã€‚æˆ‘ä»¬é€šè¿‡ä»å¸¸è§„æ”¾å°„æŠ¥å‘Šä¸­æå–å’Œç²¾ç‚¼æ¨ç†é“¾æ¥æ„å»ºå¤§å‹æ•°æ®é›†ã€‚æˆ‘ä»¬çš„ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ç»“åˆäº†ç›‘ç£å¾®è°ƒä¸ç”±è¿‡ç¨‹å¥–åŠ±å¼•å¯¼çš„è‡ªæˆ‘å¼ºåŒ–å­¦ä¹ ï¼Œä»¥æ›´å¥½åœ°ä½¿æ¨¡å‹æ¨ç†ä¸ä¸´åºŠæ ‡å‡†ä¿æŒä¸€è‡´ã€‚æˆ‘ä»¬ä»‹ç»äº†RadRBench-CXRï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«59Kè§†è§‰é—®ç­”æ ·æœ¬å’Œ301Kä¸´åºŠéªŒè¯æ¨ç†æ­¥éª¤çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œå¹¶æå‡ºäº†RadRScoreï¼Œä¸€ä¸ªè¯„ä¼°æ¨ç†çœŸå®æ€§ã€å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§çš„æŒ‡æ ‡ã€‚ChestX-Reasoneråœ¨è¯Šæ–­å’Œæ²»ç–—å‡†ç¡®åº¦ä»¥åŠæ¨ç†èƒ½åŠ›æ–¹é¢å‡ä¼˜äºç°æœ‰çš„åŒ»ç–—å’Œé€šç”¨é¢†åŸŸMLLMsã€‚ç›¸è¾ƒäºæœ€ä½³çš„åŒ»ç–—MLLMã€é€šç”¨MLLMåŠå…¶åŸºç¡€æ¨¡å‹ï¼Œåœ¨æ¨ç†èƒ½åŠ›æ–¹é¢åˆ†åˆ«æé«˜äº†16%ã€5.9%å’Œ18%ï¼›åœ¨ç»“æœå‡†ç¡®æ€§æ–¹é¢åˆ†åˆ«æé«˜äº†3.3%ã€24%å’Œ27%ã€‚æ‰€æœ‰èµ„æºå‡å¼€æºï¼Œä»¥ä¿ƒè¿›åœ¨åŒ»ç–—æ¨ç†MLLMæ–¹é¢çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20930v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬å·¥ä½œæå‡ºäº†ChestX-Reasonerï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆä¸´åºŠæŠ¥å‘Šä¸­çš„æ¨ç†è¿‡ç¨‹ç›‘ç£è®¾è®¡çš„æ”¾å°„å­¦è¯Šæ–­å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚é€šè¿‡ä»å¸¸è§„æ”¾å°„æŠ¥å‘Šä¸­æå–å’Œç²¾ç‚¼æ¨ç†é“¾ï¼Œæ„å»ºå¤§å‹æ•°æ®é›†ã€‚é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œç»“åˆç›‘ç£å¾®è°ƒä¸ä»¥è¿‡ç¨‹å¥–åŠ±å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ ï¼Œä½¿æ¨¡å‹æ¨ç†æ›´ç¬¦åˆä¸´åºŠæ ‡å‡†ã€‚åŒæ—¶å¼•å…¥äº†RadRBench-CXRåŸºå‡†æµ‹è¯•å’ŒRadRScoreè¯„ä¼°æŒ‡æ ‡ï¼Œä»¥è¯„ä¼°æ¨ç†çš„çœŸå®æ€§ã€å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§ã€‚ChestX-Reasoneråœ¨è¯Šæ–­å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„åŒ»å­¦å’Œé€šç”¨MLLMsæ¨¡å‹ï¼Œå®ç°äº†æ˜¾è‘—çš„æå‡å¹¶å…¬å¼€æ‰€æœ‰èµ„æºä»¥ä¿ƒè¿›åŒ»å­¦æ¨ç†MLLMçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ChestX-Reasoneræ˜¯ä¸€ä¸ªç»“åˆä¸´åºŠæ¨ç†è¿‡ç¨‹ç›‘ç£è®¾è®¡çš„æ”¾å°„å­¦è¯Šæ–­å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚</li>
<li>é€šè¿‡ä»å¸¸è§„æ”¾å°„æŠ¥å‘Šä¸­æå–å’Œç²¾ç‚¼æ¨ç†é“¾æ„å»ºå¤§å‹æ•°æ®é›†ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œç»“åˆç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ï¼Œä½¿æ¨¡å‹æ¨ç†æ›´ç¬¦åˆä¸´åºŠæ ‡å‡†ã€‚</li>
<li>å¼•å…¥äº†RadRBench-CXRåŸºå‡†æµ‹è¯•å’ŒRadRScoreè¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>ChestX-Reasoneråœ¨è¯Šæ–­å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„æå‡ã€‚</li>
<li>ä¸ç°æœ‰åŒ»å­¦å’Œé€šç”¨MLLMsç›¸æ¯”ï¼ŒChestX-Reasoneræœ‰æ›´é«˜çš„è¯Šæ–­å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-37673b58a3def46386d5ead91762052e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-befb15e5263924b957ac4458cec0898e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de9651e1ccd442e2cdf58484bea1033e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf8cc1e29a8a6525ad2c40b98341e38d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Multiobjective-optimization-for-scattering-mitigation-and-scattering-screen-reconstruction-in-VLBI-observations-of-the-Galactic-Center"><a href="#Multiobjective-optimization-for-scattering-mitigation-and-scattering-screen-reconstruction-in-VLBI-observations-of-the-Galactic-Center" class="headerlink" title="Multiobjective optimization for scattering mitigation and scattering   screen reconstruction in VLBI observations of the Galactic Center"></a>Multiobjective optimization for scattering mitigation and scattering   screen reconstruction in VLBI observations of the Galactic Center</h2><p><strong>Authors:Alejandro Mus, Teresa Toscano, Hendrik MÃ¼ller, Guang-Yao Zhao, Andrei Lobanov, Ciriaco Goddi</strong></p>
<p>Imaging reconstruction of interferometric data is a hard ill-posed inverse problem. Its difficulty is increased when observing the Galactic Center, which is obscured by a scattering screen. This is because the scattering breaks the one-to-one correspondence between images and visibilities. Solving the scattering problem is one of the biggest challenges in radio imaging of the Galactic Center. In this work we present a novel strategy to mitigate its effect and constrain the screen itself using multiobjective optimization. We exploit the potential of evolutionary algorithms to describe the optimization landscape to recover the intrinsic source structure and the scattering screen affecting the data. We successfully recover both the screen and the source in a wide range of simulated cases, including the speed of a moving screen at 230 GHz. Particularly, we can recover a ring structure in scattered data at 86 GHz. Our analysis demonstrates the huge potential that recent advancements in imaging and optimization algorithms offer to recover image structures, even in weakly constrained and degenerated, possibly multi-modal settings. The successful reconstruction of the scattering screen opens the window to event horizon scale works on the Galactic Center at 86G Hz up to 116 GHz, and the study of the scattering screen itself. </p>
<blockquote>
<p>æˆåƒé‡å»ºå¹²æ¶‰æ•°æ®æ˜¯ä¸€ä¸ªéš¾åº¦è¾ƒå¤§çš„åé—®é¢˜ã€‚åœ¨è§‚æµ‹é“¶æ²³ç³»ä¸­å¿ƒæ—¶éš¾åº¦æ›´å¤§ï¼Œå› ä¸ºé“¶æ²³ç³»ä¸­å¿ƒè¢«ä¸€ä¸ªæ•£å°„å±æ‰€é®è”½ã€‚æ•£å°„ç ´åäº†å›¾åƒå’Œå¯è§åº¦ä¹‹é—´çš„ä¸€ä¸€å¯¹åº”å…³ç³»ã€‚è§£å†³æ•£å°„é—®é¢˜æ˜¯é“¶æ²³ç³»ä¸­å¿ƒå°„ç”µæˆåƒé¢ä¸´çš„æœ€å¤§æŒ‘æˆ˜ä¹‹ä¸€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç­–ç•¥ï¼Œåˆ©ç”¨å¤šç›®æ ‡ä¼˜åŒ–æ¥ç¼“è§£æ•£å°„æ•ˆåº”å¹¶çº¦æŸæ•£å°„å±æœ¬èº«ã€‚æˆ‘ä»¬åˆ©ç”¨è¿›åŒ–ç®—æ³•æè¿°ä¼˜åŒ–æ™¯è§‚çš„æ½œåŠ›ï¼Œä»¥æ¢å¤å½±å“æ•°æ®çš„å›ºæœ‰æºç»“æ„å’Œæ•£å°„å±ã€‚æˆ‘ä»¬åœ¨å¹¿æ³›çš„æ¨¡æ‹Ÿæ¡ˆä¾‹ä¸­æˆåŠŸåœ°æ¢å¤äº†å±å¹•å’Œæºï¼ŒåŒ…æ‹¬ä»¥230 GHzç§»åŠ¨çš„å±å¹•é€Ÿåº¦ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨æ•£å°„æ•°æ®ä¸­æ¢å¤ç¯çŠ¶ç»“æ„ï¼ˆå¦‚åœ¨86 GHzçš„æ•°æ®ï¼‰ã€‚æˆ‘ä»¬çš„åˆ†æè¯æ˜äº†æˆåƒå’Œä¼˜åŒ–ç®—æ³•çš„æœ€æ–°è¿›å±•åœ¨æ¢å¤å›¾åƒç»“æ„æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œå³ä½¿åœ¨å¼±çº¦æŸå’Œé€€åŒ–ã€å¯èƒ½æ˜¯å¤šæ¨¡æ€çš„ç¯å¢ƒä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆåŠŸé‡å»ºæ•£å°„å±ä¸ºåœ¨86 GHzè‡³116 GHzä¸Šå¯¹é“¶æ²³ç³»ä¸­å¿ƒçš„äº‹ä»¶è§†ç•Œè§„æ¨¡å·¥ä½œæ‰“å¼€äº†çª—å£ï¼Œä»¥åŠå¯¹æ•£å°„å±æœ¬èº«çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16257v2">PDF</a> To appear in A&amp;A</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å¹²æ¶‰æ•°æ®çš„æˆåƒé‡å»ºæ˜¯ä¸€ä¸ªç—…æ€çš„é€†é—®é¢˜ï¼Œå°¤å…¶åœ¨å¯¹é“¶æ²³ç³»ä¸­å¿ƒè¿›è¡Œè§‚æµ‹æ—¶éš¾åº¦æ›´å¤§ï¼Œå› ä¸ºæ•£å°„ç ´åäº†å›¾åƒä¸å¯è§åº¦ä¹‹é—´çš„ä¸€ä¸€å¯¹åº”å…³ç³»ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„ç­–ç•¥ï¼Œåˆ©ç”¨å¤šç›®æ ‡ä¼˜åŒ–æ¥å‡è½»æ•£å°„çš„å½±å“å¹¶çº¦æŸæ•£å°„å±æœ¬èº«ã€‚æˆ‘ä»¬åˆ©ç”¨è¿›åŒ–ç®—æ³•æè¿°ä¼˜åŒ–æ™¯è§‚ï¼Œä»¥æ¢å¤å½±å“æ•°æ®çš„å†…åœ¨æºç»“æ„å’Œæ•£å°„å±ã€‚åœ¨å¤šç§æ¨¡æ‹Ÿæƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æˆåŠŸæ¢å¤äº†å±å¹•å’Œæºï¼ŒåŒ…æ‹¬ç§»åŠ¨å±å¹•çš„é€Ÿåº¦ï¼ˆåœ¨230 GHzä¸‹ï¼‰ã€‚ç‰¹åˆ«æ˜¯åœ¨86 GHzçš„æ•£å°„æ•°æ®ä¸­ï¼Œæˆ‘ä»¬æˆåŠŸæ¢å¤äº†ç¯å½¢ç»“æ„ã€‚æˆ‘ä»¬çš„åˆ†æå±•ç¤ºäº†æœ€æ–°çš„æˆåƒå’Œä¼˜åŒ–ç®—æ³•åœ¨æ¢å¤å›¾åƒç»“æ„æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œå³ä½¿åœ¨å¼±çº¦æŸå’Œé€€åŒ–çš„å¯èƒ½å¤šæ¨¡æ€ç¯å¢ƒä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆåŠŸé‡å»ºæ•£å°„å±ä¸ºåœ¨86 GHzè‡³116 GHzä¸Šå¯¹é“¶æ²³ç³»ä¸­å¿ƒçš„äº‹ä»¶è§†ç•Œè§„æ¨¡å·¥ä½œæ‰“å¼€äº†çª—å£ï¼Œå¹¶å¯¹æ•£å°„å±æœ¬èº«è¿›è¡Œäº†ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¹²æ¶‰æ•°æ®æˆåƒé‡å»ºæ˜¯ä¸€ä¸ªç—…æ€çš„é€†é—®é¢˜ï¼Œå°¤å…¶åœ¨è§‚æµ‹é“¶æ²³ç³»ä¸­å¿ƒæ—¶éš¾åº¦æ›´å¤§ã€‚</li>
<li>æ•£å°„ç ´åäº†å›¾åƒä¸å¯è§åº¦ä¹‹é—´çš„ä¸€ä¸€å¯¹åº”å…³ç³»ï¼Œæ˜¯å°„ç”µæˆåƒä¸­é¢ä¸´çš„æœ€å¤§æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„ç­–ç•¥ï¼Œåˆ©ç”¨å¤šç›®æ ‡ä¼˜åŒ–å‡è½»æ•£å°„å½±å“å¹¶çº¦æŸæ•£å°„å±ã€‚</li>
<li>é€šè¿‡è¿›åŒ–ç®—æ³•æè¿°ä¼˜åŒ–æ™¯è§‚ï¼Œå¯ä»¥æ¢å¤å†…åœ¨æºç»“æ„å’Œå½±å“æ•°æ®çš„æ•£å°„å±ã€‚</li>
<li>åœ¨å¤šç§æ¨¡æ‹Ÿæƒ…å†µä¸‹æˆåŠŸæ¢å¤äº†å±å¹•å’Œæºï¼ŒåŒ…æ‹¬ç§»åŠ¨å±å¹•çš„é€Ÿåº¦ã€‚</li>
<li>æˆåŠŸåœ¨86 GHzçš„æ•£å°„æ•°æ®ä¸­æ¢å¤ç¯å½¢ç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16257">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eb100ad97f6e77a28396e7ac4809c25f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Explicit-and-Implicit-Representations-in-AI-based-3D-Reconstruction-for-Radiology-A-Systematic-Review"><a href="#Explicit-and-Implicit-Representations-in-AI-based-3D-Reconstruction-for-Radiology-A-Systematic-Review" class="headerlink" title="Explicit and Implicit Representations in AI-based 3D Reconstruction for   Radiology: A Systematic Review"></a>Explicit and Implicit Representations in AI-based 3D Reconstruction for   Radiology: A Systematic Review</h2><p><strong>Authors:Yuezhe Yang, Boyu Yang, Yaqian Wang, Yang He, Xingbo Dong, Zhe Jin</strong></p>
<p>The demand for high-quality medical imaging in clinical practice and assisted diagnosis has made 3D reconstruction in radiological imaging a key research focus. Artificial intelligence (AI) has emerged as a promising approach to enhancing reconstruction accuracy while reducing acquisition and processing time, thereby minimizing patient radiation exposure and discomfort and ultimately benefiting clinical diagnosis. This review explores state-of-the-art AI-based 3D reconstruction algorithms in radiological imaging, categorizing them into explicit and implicit approaches based on their underlying principles. Explicit methods include point-based, volume-based, and Gaussian representations, while implicit methods encompass implicit prior embedding and neural radiance fields. Additionally, we examine commonly used evaluation metrics and benchmark datasets. Finally, we discuss the current state of development, key challenges, and future research directions in this evolving field. Our project available on: <a target="_blank" rel="noopener" href="https://github.com/Bean-Young/AI4Radiology">https://github.com/Bean-Young/AI4Radiology</a>. </p>
<blockquote>
<p>ä¸´åºŠå®è·µå’Œå¯¹è¾…åŠ©è¯Šæ–­çš„é«˜è´¨é‡åŒ»å­¦æˆåƒçš„éœ€æ±‚ä½¿å¾—æ”¾å°„æˆåƒä¸­çš„3Dé‡å»ºæˆä¸ºå…³é”®çš„ç ”ç©¶é‡ç‚¹ã€‚äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å·²æˆä¸ºä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œå¯ä»¥æé«˜é‡å»ºç²¾åº¦ï¼ŒåŒæ—¶å‡å°‘é‡‡é›†å’Œå¤„ç†æ—¶é—´ï¼Œä»è€Œæœ€å°åŒ–æ‚£è€…çš„è¾å°„æš´éœ²å’Œä¸é€‚æ„Ÿï¼Œå¹¶æœ€ç»ˆæœ‰ç›Šäºä¸´åºŠè¯Šæ–­ã€‚æœ¬æ–‡ç»¼è¿°äº†åŸºäºäººå·¥æ™ºèƒ½çš„æ”¾å°„æˆåƒ3Dé‡å»ºç®—æ³•çš„æœ€æ–°è¿›å±•ï¼Œæ ¹æ®å®ƒä»¬çš„åŸºæœ¬åŸç†å°†å®ƒä»¬åˆ†ä¸ºæ˜¾å¼æ–¹æ³•å’Œéšå¼æ–¹æ³•ã€‚æ˜¾å¼æ–¹æ³•åŒ…æ‹¬ç‚¹åŸºã€ä½“ç§¯åŸºå’Œé«˜æ–¯è¡¨ç¤ºæ³•ï¼Œè€Œéšå¼æ–¹æ³•åŒ…æ‹¬éšå¼å…ˆéªŒåµŒå…¥å’Œç¥ç»è¾å°„åœºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†å¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡å’ŒåŸºå‡†æ•°æ®é›†ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†è¯¥é¢†åŸŸçš„å½“å‰å¼€å‘çŠ¶æ€ã€å…³é”®æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚æˆ‘ä»¬çš„é¡¹ç›®å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/Bean-Young/AI4Radiology">https://github.com/Bean-Young/AI4Radiology</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11349v2">PDF</a> 20 pages, 5 figures, submit to Medical Image Analysis</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†äººå·¥æ™ºèƒ½åœ¨åŒ»å­¦å½±åƒå­¦ä¸‰ç»´é‡å»ºä¸­çš„é‡è¦ä½œç”¨å’Œåº”ç”¨ã€‚ä»‹ç»äº†åŸºäºäººå·¥æ™ºèƒ½çš„ä¸‰ç»´é‡å»ºç®—æ³•çš„å‰æ²¿è¿›å±•ï¼ŒåŒ…æ‹¬æ˜¾å¼æ–¹æ³•å’Œéšå¼æ–¹æ³•ã€‚æ˜¾å¼æ–¹æ³•ä¸»è¦åŒ…æ‹¬ç‚¹åŸºã€ä½“ç§¯åŸºå’Œé«˜æ–¯è¡¨ç¤ºæ³•ï¼Œéšå¼æ–¹æ³•åŒ…æ‹¬éšå¼å…ˆéªŒåµŒå…¥å’Œç¥ç»è¾å°„åœºã€‚æ–‡ç« è¿˜ä»‹ç»äº†å¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡å’ŒåŸºå‡†æ•°æ®é›†ï¼Œå¹¶è®¨è®ºäº†å½“å‰çš„å‘å±•çŠ¶å†µã€å…³é”®æŒ‘æˆ˜ä»¥åŠæœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚é¡¹ç›®ä»£ç å¯é€šè¿‡é“¾æ¥è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/Bean-Young/AI4Radiology">https://github.com/Bean-Young/AI4Radiology</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å½±åƒå­¦ä¸­ä¸‰ç»´é‡å»ºçš„é‡è¦æ€§åŠå…¶åœ¨ä¸´åºŠå®è·µå’Œè¾…åŠ©è¯Šæ–­ä¸­çš„éœ€æ±‚ã€‚</li>
<li>äººå·¥æ™ºèƒ½åœ¨æå‡ä¸‰ç»´é‡å»ºå‡†ç¡®æ€§ã€å‡å°‘é‡‡é›†å’Œå¤„ç†æ—¶é—´æ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>äººå·¥æ™ºèƒ½åœ¨åŒ»å­¦å½±åƒå­¦ä¸‰ç»´é‡å»ºä¸­çš„æ˜¾å¼æ–¹æ³•å’Œéšå¼æ–¹æ³•ï¼ŒåŒ…æ‹¬å„è‡ªçš„ä¼˜ç‚¹å’ŒæŒ‘æˆ˜ã€‚</li>
<li>å¸¸ç”¨çš„è¯„ä¼°ä¸‰ç»´é‡å»ºæ•ˆæœçš„è¯„ä»·æŒ‡æ ‡å’ŒåŸºå‡†æ•°æ®é›†ã€‚</li>
<li>å½“å‰åŒ»å­¦å½±åƒå­¦ä¸‰ç»´é‡å»ºé¢†åŸŸçš„å‘å±•çŠ¶å†µã€‚</li>
<li>é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ï¼Œå¦‚ç®—æ³•å¤æ‚æ€§ã€æ•°æ®è·å–å’Œå¤„ç†éš¾åº¦ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11349">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b16f5fede7861ca900b1641bf2a5e72d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6da8a985ca41a23cad9da5757857cc5b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4e5e11e6f7e0c7e1876c1ef77c65cbd6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa4301441efc27c77d72e3c85818681b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-63ffd54979e1de6491aa82731437c742.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a709bd7d1025d630719384e375a1ff43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa53d811d79f6f89c0799ffa6b9f040b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Customized-SAM-2-for-Referring-Remote-Sensing-Image-Segmentation"><a href="#Customized-SAM-2-for-Referring-Remote-Sensing-Image-Segmentation" class="headerlink" title="Customized SAM 2 for Referring Remote Sensing Image Segmentation"></a>Customized SAM 2 for Referring Remote Sensing Image Segmentation</h2><p><strong>Authors:Fu Rong, Meng Lan, Qian Zhang, Lefei Zhang</strong></p>
<p>Referring Remote Sensing Image Segmentation (RRSIS) aims to segment target objects in remote sensing (RS) images based on textual descriptions. Although Segment Anything Model 2 (SAM 2) has shown remarkable performance in various segmentation tasks, its application to RRSIS presents several challenges, including understanding the text-described RS scenes and generating effective prompts from text descriptions. To address these issues, we propose RS2-SAM 2, a novel framework that adapts SAM 2 to RRSIS by aligning the adapted RS features and textual features, providing pseudo-mask-based dense prompts, and enforcing boundary constraints. Specifically, we first employ a union encoder to jointly encode the visual and textual inputs, generating aligned visual and text embeddings as well as multimodal class tokens. Then, we design a bidirectional hierarchical fusion module to adapt SAM 2 to RS scenes and align adapted visual features with the visually enhanced text embeddings, improving the modelâ€™s interpretation of text-described RS scenes. Additionally, a mask prompt generator is introduced to take the visual embeddings and class tokens as input and produce a pseudo-mask as the dense prompt of SAM 2. To further refine segmentation, we introduce a text-guided boundary loss to optimize segmentation boundaries by computing text-weighted gradient differences. Experimental results on several RRSIS benchmarks demonstrate that RS2-SAM 2 achieves state-of-the-art performance. </p>
<blockquote>
<p>è¿œç¨‹é¥æ„Ÿå›¾åƒåˆ†å‰²ï¼ˆRRSISï¼‰æ—¨åœ¨æ ¹æ®æ–‡æœ¬æè¿°å¯¹é¥æ„Ÿï¼ˆRSï¼‰å›¾åƒä¸­çš„ç›®æ ‡å¯¹è±¡è¿›è¡Œåˆ†å‰²ã€‚å°½ç®¡Segment Anything Model 2ï¼ˆSAM 2ï¼‰åœ¨å„ç§åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å°†å…¶åº”ç”¨äºRRSISé¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç†è§£æ–‡æœ¬æè¿°çš„RSåœºæ™¯å’Œä»æ–‡æœ¬æè¿°ä¸­äº§ç”Ÿæœ‰æ•ˆçš„æç¤ºã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RS2-SAM 2ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œé€šè¿‡é€‚åº”SAM 2åˆ°RRSISï¼Œé€šè¿‡å¯¹é½é€‚åº”çš„RSç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾ã€æä¾›åŸºäºä¼ªæ©ç çš„å¯†é›†æç¤ºå’Œæ‰§è¡Œè¾¹ç•Œçº¦æŸæ¥é€‚åº”RRSISã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆé‡‡ç”¨è”åˆç¼–ç å™¨å¯¹è§†è§‰å’Œæ–‡æœ¬è¾“å…¥è¿›è¡Œç¼–ç ï¼Œç”Ÿæˆå¯¹é½çš„è§†è§‰å’Œæ–‡æœ¬åµŒå…¥ä»¥åŠå¤šæ¨¡å¼ç±»åˆ«æ ‡è®°ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŒå‘å±‚æ¬¡èåˆæ¨¡å—ï¼Œä½¿SAM 2é€‚åº”RSåœºæ™¯ï¼Œå¹¶ä½¿é€‚åº”åçš„è§†è§‰ç‰¹å¾ä¸å¢å¼ºçš„æ–‡æœ¬åµŒå…¥å¯¹é½ï¼Œæé«˜æ¨¡å‹å¯¹æ–‡æœ¬æè¿°çš„RSåœºæ™¯çš„è§£é‡Šèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæ©è†œæç¤ºç”Ÿæˆå™¨ï¼Œä»¥è§†è§‰åµŒå…¥å’Œç±»åˆ«æ ‡è®°ä¸ºè¾“å…¥ï¼Œç”Ÿæˆä¸€ä¸ªä¼ªæ©è†œä½œä¸ºSAM 2çš„å¯†é›†æç¤ºã€‚ä¸ºäº†è¿›ä¸€æ­¥ç»†åŒ–åˆ†å‰²ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–‡æœ¬å¼•å¯¼è¾¹ç•ŒæŸå¤±ï¼Œé€šè¿‡è®¡ç®—æ–‡æœ¬åŠ æƒçš„æ¢¯åº¦å·®å¼‚æ¥ä¼˜åŒ–åˆ†å‰²è¾¹ç•Œã€‚åœ¨å¤šä¸ªRRSISåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRS2-SAM 2è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07266v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ–‡æœ¬æè¿°çš„é¥æ„Ÿå›¾åƒåˆ†å‰²ï¼ˆRRSISï¼‰æ—¨åœ¨å®ç°å¯¹é¥æ„Ÿå›¾åƒä¸­ç›®æ ‡å¯¹è±¡çš„åˆ†å‰²ã€‚å°½ç®¡SAM 2æ¨¡å‹åœ¨å„ç§åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å°†å…¶åº”ç”¨äºRRSISé¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼Œå¦‚ç†è§£æ–‡æœ¬æè¿°çš„é¥æ„Ÿåœºæ™¯å’Œä»æ–‡æœ¬æè¿°ä¸­äº§ç”Ÿæœ‰æ•ˆçš„æç¤ºã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RS2-SAM 2æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¯¹é½é¥æ„Ÿç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾ã€æä¾›åŸºäºä¼ªæ©ç çš„å¯†é›†æç¤ºå’Œæ‰§è¡Œè¾¹ç•Œçº¦æŸï¼Œä½¿SAM 2é€‚åº”RRSISã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRS2-SAM 2åœ¨å¤šä¸ªRRSISåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RS2-SAM 2æ¡†æ¶æ—¨åœ¨ä½¿SAM 2æ¨¡å‹é€‚åº”åŸºäºæ–‡æœ¬æè¿°çš„é¥æ„Ÿå›¾åƒåˆ†å‰²ï¼ˆRRSISï¼‰ã€‚</li>
<li>RS2-SAM 2é€šè¿‡è”åˆç¼–ç è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ï¼Œç”Ÿæˆå¯¹é½çš„è§†è§‰å’Œæ–‡æœ¬åµŒå…¥ä»¥åŠå¤šæ¨¡æ€ç±»ä»¤ç‰Œæ¥è§£å†³ç†è§£æ–‡æœ¬æè¿°çš„é¥æ„Ÿåœºæ™¯å’Œç”Ÿæˆæœ‰æ•ˆæç¤ºçš„æŒ‘æˆ˜ã€‚</li>
<li>æ¡†æ¶è®¾è®¡äº†ä¸€ä¸ªåŒå‘å±‚æ¬¡èåˆæ¨¡å—ï¼Œä½¿SAM 2é€‚åº”é¥æ„Ÿåœºæ™¯ï¼Œå¹¶å°†é€‚åº”åçš„è§†è§‰ç‰¹å¾ä¸å¢å¼ºæ–‡æœ¬åµŒå…¥å¯¹é½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ©è†œæç¤ºç”Ÿæˆå™¨ï¼Œä»¥è§†è§‰åµŒå…¥å’Œç±»ä»¤ç‰Œä¸ºè¾“å…¥ï¼Œç”Ÿæˆä¼ªæ©è†œä½œä¸ºSAM 2çš„å¯†é›†æç¤ºã€‚</li>
<li>ä¸ºè¿›ä¸€æ­¥ä¼˜åŒ–åˆ†å‰²è¾¹ç•Œï¼Œå¼•å…¥äº†æ–‡æœ¬å¼•å¯¼è¾¹ç•ŒæŸå¤±ï¼Œé€šè¿‡è®¡ç®—æ–‡æœ¬åŠ æƒçš„æ¢¯åº¦å·®å¼‚æ¥ä¼˜åŒ–åˆ†å‰²è¾¹ç•Œã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒRS2-SAM 2åœ¨å¤šä¸ªRRSISåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>RS2-SAM 2æ¡†æ¶çš„åº”ç”¨ä¸ºé¥æ„Ÿå›¾åƒåˆ†å‰²æä¾›äº†ä¸€ç§æ–°çš„ã€æœ‰æ•ˆçš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07266">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d62e34b19803616f11e79d2e9427a398.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f130e21aea2c96b1328072c325532d11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-adce5752d05efa43e67cb3b0d4e419ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c422967dac52dde7539b963283463f58.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Liver-Cirrhosis-Stage-Estimation-from-MRI-with-Deep-Learning"><a href="#Liver-Cirrhosis-Stage-Estimation-from-MRI-with-Deep-Learning" class="headerlink" title="Liver Cirrhosis Stage Estimation from MRI with Deep Learning"></a>Liver Cirrhosis Stage Estimation from MRI with Deep Learning</h2><p><strong>Authors:Jun Zeng, Debesh Jha, Ertugrul Aktas, Elif Keles, Alpay Medetalibeyoglu, Matthew Antalek, Federica Proietto Salanitri, Amir A. Borhani, Daniela P. Ladner, Gorkem Durak, Ulas Bagci</strong></p>
<p>We present an end-to-end deep learning framework for automated liver cirrhosis stage estimation from multi-sequence MRI. Cirrhosis is the severe scarring (fibrosis) of the liver and a common endpoint of various chronic liver diseases. Early diagnosis is vital to prevent complications such as decompensation and cancer, which significantly decreases life expectancy. However, diagnosing cirrhosis in its early stages is challenging, and patients often present with life-threatening complications. Our approach integrates multi-scale feature learning with sequence-specific attention mechanisms to capture subtle tissue variations across cirrhosis progression stages. Using CirrMRI600+, a large-scale publicly available dataset of 628 high-resolution MRI scans from 339 patients, we demonstrate state-of-the-art performance in three-stage cirrhosis classification. Our best model achieves 72.8% accuracy on T1W and 63.8% on T2W sequences, significantly outperforming traditional radiomics-based approaches. Through extensive ablation studies, we show that our architecture effectively learns stage-specific imaging biomarkers. We establish new benchmarks for automated cirrhosis staging and provide insights for developing clinically applicable deep learning systems. The source code will be available at <a target="_blank" rel="noopener" href="https://github.com/JunZengz/CirrhosisStage">https://github.com/JunZengz/CirrhosisStage</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºä»å¤šåºåˆ—MRIè‡ªåŠ¨ä¼°è®¡è‚ç¡¬åŒ–åˆ†æœŸã€‚è‚ç¡¬åŒ–æ˜¯è‚è„çš„ä¸¥é‡ç˜¢ç—•ï¼ˆçº¤ç»´åŒ–ï¼‰å½¢æˆï¼Œæ˜¯å¤šç§æ…¢æ€§è‚ç—…çš„å¸¸è§ç»ˆç‚¹ã€‚æ—©æœŸè¯Šæ–­å¯¹äºé¢„é˜²å¹¶å‘ç—‡ï¼ˆå¦‚å¤±ä»£å¿å’Œç™Œç—‡ï¼‰è‡³å…³é‡è¦ï¼Œè¿™äº›å¹¶å‘ç—‡ä¼šæ˜¾è‘—é™ä½é¢„æœŸå¯¿å‘½ã€‚ç„¶è€Œï¼Œåœ¨è‚ç¡¬åŒ–æ—©æœŸè¿›è¡Œè¯Šæ–­å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œæ‚£è€…é€šå¸¸ä¼šå‡ºç°å±åŠç”Ÿå‘½çš„å¹¶å‘ç—‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†å¤šå°ºåº¦ç‰¹å¾å­¦ä¹ ä¸åºåˆ—ç‰¹å®šçš„æ³¨æ„åŠ›æœºåˆ¶ç›¸ç»“åˆï¼Œä»¥æ•æ‰è‚ç¡¬åŒ–è¿›å±•é˜¶æ®µä¸­ç»†å¾®çš„ç»„ç»‡å˜åŒ–ã€‚æˆ‘ä»¬ä½¿ç”¨CirrMRI600+æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¥è‡ª339åæ‚£è€…çš„628ä¸ªé«˜åˆ†è¾¨ç‡MRIæ‰«æï¼Œå±•ç¤ºäº†åœ¨ä¸‰æœŸè‚ç¡¬åŒ–åˆ†ç±»ä¸­çš„æœ€æ–°æ€§èƒ½ã€‚æˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹åœ¨T1Wä¸Šè¾¾åˆ°äº†72.8%çš„å‡†ç¡®ç‡ï¼Œåœ¨T2Wåºåˆ—ä¸Šè¾¾åˆ°äº†63.8%çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„åŸºäºæ”¾å°„ç»„å­¦çš„æ–¹æ³•ã€‚é€šè¿‡å¹¿æ³›çš„æ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ¶æ„æœ‰æ•ˆåœ°å­¦ä¹ äº†ä¸é˜¶æ®µç›¸å…³çš„æˆåƒç”Ÿç‰©æ ‡å¿—ç‰©ã€‚æˆ‘ä»¬ä¸ºè‡ªåŠ¨åŒ–è‚ç¡¬åŒ–åˆ†æœŸè®¾å®šäº†æ–°çš„åŸºå‡†ï¼Œå¹¶ä¸ºå¼€å‘ä¸´åºŠé€‚ç”¨çš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿæä¾›äº†è§è§£ã€‚æºä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/JunZengz/CirrhosisStage%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/JunZengz/CirrhosisStageä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18225v3">PDF</a> 7 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œç”¨äºä»å¤šåºåˆ—MRIä¸­è‡ªåŠ¨ä¼°è®¡è‚ç¡¬åŒ–åˆ†æœŸã€‚è¯¥ç ”ç©¶åˆ©ç”¨å¤§è§„æ¨¡å…¬å¼€æ•°æ®é›†CirrMRI600+è¿›è¡Œä¸‰é˜¶æ®µè‚ç¡¬åŒ–åˆ†ç±»ï¼Œå¹¶å®ç°äº†å…ˆè¿›æ€§èƒ½ï¼Œæœ€ä½³æ¨¡å‹åœ¨T1Wå’ŒT2Wåºåˆ—ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º72.8%å’Œ63.8%ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºäºæ”¾å°„å­¦çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é‡‡ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•è¿›è¡Œè‚ç¡¬åŒ–åˆ†æœŸçš„è‡ªåŠ¨ä¼°è®¡ã€‚</li>
<li>åˆ©ç”¨å¤šåºåˆ—MRIæ•°æ®è¿›è¡Œè‚ç¡¬åŒ–åˆ†æœŸã€‚</li>
<li>ä½¿ç”¨å¤§è§„æ¨¡å…¬å¼€æ•°æ®é›†CirrMRI600+è¿›è¡Œå®éªŒç ”ç©¶ã€‚</li>
<li>æœ€ä½³æ¨¡å‹åœ¨T1Wå’ŒT2Wåºåˆ—ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º72.8%å’Œ63.8%ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºäºæ”¾å°„å­¦çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡æ¶ˆèç ”ç©¶ï¼Œè¯æ˜è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆå­¦ä¹ é˜¶æ®µç‰¹å®šçš„æˆåƒç”Ÿç‰©æ ‡å¿—ç‰©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18225">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-08ae028fd11d989facee11595f1cdfa6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cbb318ce984e5c89aa445cbf732de46.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23545de6d1078c7074ace886ff439475.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9967fd34377a496082d0c076488b76ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbee84e5642f0b2987439026f043e195.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7916a88300ab2b9a596a938f06e9a4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0055112c5d88633b2521b61d989402fb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b312c7c9c90e02dc480d0442ecd69015.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Topology-Design-of-Reconfigurable-Intelligent-Surfaces-Based-on-Current-Distribution-and-Otsu-Image-Segmentation"><a href="#Topology-Design-of-Reconfigurable-Intelligent-Surfaces-Based-on-Current-Distribution-and-Otsu-Image-Segmentation" class="headerlink" title="Topology Design of Reconfigurable Intelligent Surfaces Based on Current   Distribution and Otsu Image Segmentation"></a>Topology Design of Reconfigurable Intelligent Surfaces Based on Current   Distribution and Otsu Image Segmentation</h2><p><strong>Authors:Zhen Zhang, Jun Wei Zhang, Hui Dong Li, Junhui Qiu, Lijie Wu, Wan Wan Cao, Ren Wang, Jia Nan Zhang, Qiang Cheng</strong></p>
<p>Miniaturization of reconffgurable intelligent surface RIS) elements is a crucial trend in the development of RISs. It not only facilitates the attainment of multifunctional integration but also promotes seamless amalgamation with other elements. The current on the RIS element plays a crucial role in determining the characteristics of the induced electromagnetic ffeld components. Segments with high current intensity determine the performance of RIS elements. Carving the parts with strong current distribution density into the metal patch of RIS element structure can achieve miniaturization. Based on this insight, this work proposes a topology design method that leverages current distribution and image processing techniques to achieve efffcient miniaturization of the RIS elements. In this proposed method, we ffrst obtain the current distribution across different operational states and the period of the working frequency. Next, we employ the Otsu image segmentation method to extract relevant image information from the current distribution images of the RIS elements. Subsequently, we utilize linear mapping techniques to convert this image information into the structure of RIS elements. Then, based on the structure of the RIS elements, the Quasi-Newton optimization algorithm is utilized to obtain the parameters of the tunable device that correspond to various operational states. As a result, we successfully construct the structural topology of the RIS elements based on their current distribution, designing areas with strong current distribution as metal patches. To validate the performance of the proposed method, a 16 by 16 3-bit RIS was developed, fabricated and measured. Compared with existing RIS designs, the proportion of the top-layer metal patches is smaller, which provides the possibility for integrating other functions and devices. </p>
<blockquote>
<p>å¯é‡æ„æ™ºèƒ½è¡¨é¢ï¼ˆRISï¼‰å…ƒç´ çš„å¾®å‹åŒ–æ˜¯RISå‘å±•çš„é‡è¦è¶‹åŠ¿ã€‚å®ƒä¸ä»…æœ‰åˆ©äºå®ç°å¤šåŠŸèƒ½é›†æˆï¼Œè€Œä¸”ä¿ƒè¿›äº†ä¸å…¶ä»–å…ƒç´ çš„æ— ç¼èåˆã€‚RISå…ƒä»¶ä¸Šçš„ç”µæµåœ¨å†³å®šæ„Ÿåº”ç”µç£åœºç»„ä»¶çš„ç‰¹æ€§æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚é«˜ç”µæµå¼ºåº¦çš„åŒºåŸŸå†³å®šäº†RISå…ƒä»¶çš„æ€§èƒ½ã€‚é€šè¿‡å°†ç”µæµåˆ†å¸ƒå¯†åº¦è¾ƒå¤§çš„éƒ¨åˆ†é›•åˆ»æˆRISå…ƒä»¶ç»“æ„çš„é‡‘å±è´´ç‰‡ï¼Œå¯ä»¥å®ç°å¾®å‹åŒ–ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œè¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§åˆ©ç”¨ç”µæµåˆ†å¸ƒå’Œå›¾åƒå¤„ç†æŠ€æœ¯æ¥å®ç°RISå…ƒç´ é«˜æ•ˆå¾®å‹åŒ–çš„æ‹“æ‰‘è®¾è®¡æ–¹æ³•ã€‚åœ¨è¯¥æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè·å¾—ä¸åŒå·¥ä½œçŠ¶æ€ä¸‹å’Œå·¥ä½œé¢‘ç‡å‘¨æœŸçš„ç”µæµåˆ†å¸ƒã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é‡‡ç”¨Otsuå›¾åƒåˆ†å‰²æ–¹æ³•ä»RISå…ƒä»¶çš„ç”µæµåˆ†å¸ƒå›¾åƒä¸­æå–ç›¸å…³å›¾åƒä¿¡æ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨çº¿æ€§æ˜ å°„æŠ€æœ¯å°†å›¾åƒä¿¡æ¯è½¬æ¢ä¸ºRISå…ƒä»¶çš„ç»“æ„ã€‚éšåï¼ŒåŸºäºRISå…ƒä»¶çš„ç»“æ„ï¼Œåˆ©ç”¨æ‹Ÿç‰›é¡¿ä¼˜åŒ–ç®—æ³•è·å¾—å¯¹åº”äºå„ç§å·¥ä½œçŠ¶æ€çš„å¯è°ƒè®¾å¤‡çš„å‚æ•°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æˆåŠŸæ„å»ºäº†åŸºäºç”µæµåˆ†å¸ƒçš„RISå…ƒç´ ç»“æ„æ‹“æ‰‘ï¼Œè®¾è®¡ç”µæµåˆ†å¸ƒå¼ºçƒˆçš„åŒºåŸŸä½œä¸ºé‡‘å±è´´ç‰‡ã€‚ä¸ºäº†éªŒè¯æ‰€ææ–¹æ³•çš„æ€§èƒ½ï¼Œå¼€å‘ã€åˆ¶ä½œå¹¶æµ‹é‡äº†ä¸€ä¸ª16x16 3ä½RISã€‚ä¸ç°æœ‰çš„RISè®¾è®¡ç›¸æ¯”ï¼Œé¡¶å±‚é‡‘å±è´´ç‰‡çš„æ¯”ä¾‹è¾ƒå°ï¼Œè¿™ä¸ºé›†æˆå…¶ä»–åŠŸèƒ½å’Œè®¾å¤‡æä¾›äº†å¯èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18067v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¯é‡æ„æ™ºèƒ½è¡¨é¢ï¼ˆRISï¼‰å…ƒç´ çš„å¾®å‹åŒ–è¶‹åŠ¿ï¼ŒæŒ‡å‡ºå¾®å‹åŒ–æœ‰åŠ©äºå®ç°å¤šåŠŸèƒ½é›†æˆå’Œä¸å…¶ä»–å…ƒç´ çš„æ— ç¼èåˆã€‚æ–‡ç« æå‡ºäº†åŸºäºç”µæµåˆ†å¸ƒå’Œå›¾åƒå¤„ç†æŠ€æœ¯çš„æ‹“æ‰‘è®¾è®¡æ–¹æ³•ï¼Œä»¥å®ç°RISå…ƒç´ çš„é«˜æ•ˆå¾®å‹åŒ–ã€‚è¯¥æ–¹æ³•é¦–å…ˆè·å–ä¸åŒå·¥ä½œçŠ¶æ€ä¸‹å’Œå·¥ä½œé¢‘ç‡å‘¨æœŸçš„ç”µæµåˆ†å¸ƒï¼Œç„¶åé‡‡ç”¨Otsuå›¾åƒåˆ†å‰²æ–¹æ³•æå–RISå…ƒç´ çš„ç”µæµåˆ†å¸ƒå›¾åƒçš„ç›¸å…³ä¿¡æ¯ï¼Œå†åˆ©ç”¨çº¿æ€§æ˜ å°„æŠ€æœ¯å°†å›¾åƒä¿¡æ¯è½¬æ¢ä¸ºRISå…ƒç´ çš„ç»“æ„ã€‚æœ€åï¼ŒåŸºäºQuasi-Newtonä¼˜åŒ–ç®—æ³•è·å–ä¸å„ç§å·¥ä½œçŠ¶æ€ç›¸å¯¹åº”çš„å¯è°ƒè®¾å¤‡çš„å‚æ•°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æˆåŠŸæ„å»ºäº†åŸºäºç”µæµåˆ†å¸ƒçš„RISå…ƒç´ ç»“æ„æ‹“æ‰‘ï¼Œå¹¶éªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯é‡æ„æ™ºèƒ½è¡¨é¢ï¼ˆRISï¼‰å…ƒç´ çš„å¾®å‹åŒ–æ˜¯å‘å±•çš„å…³é”®è¶‹åŠ¿ï¼Œæœ‰åŠ©äºå®ç°å¤šåŠŸèƒ½é›†æˆå’Œä¸å…¶ä»–å…ƒç´ çš„èåˆã€‚</li>
<li>ç”µæµåˆ†å¸ƒåœ¨å†³å®šRISå…ƒç´ ç‰¹æ€§æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ï¼Œé«˜ç”µæµå¼ºåº¦åŒºåŸŸå½±å“RISå…ƒç´ æ€§èƒ½ã€‚</li>
<li>é€šè¿‡åœ¨é‡‘å±è´´ç‰‡ç»“æ„ä¸­ç²¾ç»†é›•åˆ»å¼ºç”µæµåˆ†å¸ƒçš„éƒ¨åˆ†æ¥å®ç°å¾®å‹åŒ–ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºç”µæµåˆ†å¸ƒå’Œå›¾åƒå¤„ç†æŠ€æœ¯çš„æ‹“æ‰‘è®¾è®¡æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•æ¶‰åŠè·å–ä¸åŒå·¥ä½œçŠ¶æ€å’Œå·¥ä½œé¢‘ç‡å‘¨æœŸçš„ç”µæµåˆ†å¸ƒï¼Œå¹¶é‡‡ç”¨Otsuå›¾åƒåˆ†å‰²æ³•æå–ç›¸å…³ä¿¡æ¯ã€‚</li>
<li>ä½¿ç”¨çº¿æ€§æ˜ å°„æŠ€æœ¯å’ŒQuasi-Newtonä¼˜åŒ–ç®—æ³•å°†å›¾åƒä¿¡æ¯è½¬æ¢ä¸ºRISå…ƒç´ çš„ç»“æ„å¹¶è·å–å¯è°ƒè®¾å¤‡å‚æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18067">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6287be051a8a68044b266607ca52378f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af7ae939d10cfe76eaee41d44a5df3f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a911e98c8c02930310a6ab4da2dec363.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a57277aef6f63581fca74598d9ea9d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10c20e5ebcfcbbe0109d82b703a8120c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Mask-of-truth-model-sensitivity-to-unexpected-regions-of-medical-images"><a href="#Mask-of-truth-model-sensitivity-to-unexpected-regions-of-medical-images" class="headerlink" title="Mask of truth: model sensitivity to unexpected regions of medical images"></a>Mask of truth: model sensitivity to unexpected regions of medical images</h2><p><strong>Authors:ThÃ©o Sourget, Michelle Hestbek-MÃ¸ller, Amelia JimÃ©nez-SÃ¡nchez, Jack Junchi Xu, Veronika Cheplygina</strong></p>
<p>The development of larger models for medical image analysis has led to increased performance. However, it also affected our ability to explain and validate model decisions. Models can use non-relevant parts of images, also called spurious correlations or shortcuts, to obtain high performance on benchmark datasets but fail in real-world scenarios. In this work, we challenge the capacity of convolutional neural networks (CNN) to classify chest X-rays and eye fundus images while masking out clinically relevant parts of the image. We show that all models trained on the PadChest dataset, irrespective of the masking strategy, are able to obtain an Area Under the Curve (AUC) above random. Moreover, the models trained on full images obtain good performance on images without the region of interest (ROI), even superior to the one obtained on images only containing the ROI. We also reveal a possible spurious correlation in the Chaksu dataset while the performances are more aligned with the expectation of an unbiased model. We go beyond the performance analysis with the usage of the explainability method SHAP and the analysis of embeddings. We asked a radiology resident to interpret chest X-rays under different masking to complement our findings with clinical knowledge. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/TheoSourget/MMC_Masking">https://github.com/TheoSourget/MMC_Masking</a> and <a target="_blank" rel="noopener" href="https://github.com/TheoSourget/MMC_Masking_EyeFundus">https://github.com/TheoSourget/MMC_Masking_EyeFundus</a> </p>
<blockquote>
<p>å¼€å‘ç”¨äºåŒ»å­¦å›¾åƒåˆ†æçš„å¤§å‹æ¨¡å‹å·²ç»æé«˜äº†æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ä¹Ÿå½±å“äº†æˆ‘ä»¬è§£é‡Šå’ŒéªŒè¯æ¨¡å‹å†³ç­–çš„èƒ½åŠ›ã€‚æ¨¡å‹å¯èƒ½ä¼šä½¿ç”¨ä¸å›¾åƒä¸ç›¸å…³çš„éƒ¨åˆ†ï¼ˆä¹Ÿç§°ä¸ºå¶ç„¶å…³è”æˆ–æ·å¾„ï¼‰æ¥è·å¾—åŸºå‡†æ•°æ®é›†ä¸Šçš„é«˜æ€§èƒ½ï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„åœºæ™¯ä¸­å´ä¼šå¤±è´¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡é®æŒ¡åŒ»å­¦ä¸Šé‡è¦çš„å›¾åƒéƒ¨åˆ†æ¥æŒ‘æˆ˜å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å¯¹èƒ¸éƒ¨Xå°„çº¿å’Œçœ¼åº•å›¾åƒçš„åˆ†ç±»èƒ½åŠ›ã€‚æˆ‘ä»¬æ˜¾ç¤ºï¼Œåœ¨PadChestæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ‰€æœ‰æ¨¡å‹ï¼Œæ— è®ºé‡‡ç”¨ä½•ç§é®æŒ¡ç­–ç•¥ï¼Œéƒ½èƒ½è·å¾—é«˜äºéšæœºçš„æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰ã€‚æ­¤å¤–ï¼Œåœ¨å®Œæ•´å›¾åƒä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨æ²¡æœ‰æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰çš„å›¾åƒä¸Šä¹Ÿèƒ½è·å¾—è‰¯å¥½çš„æ€§èƒ½ï¼Œç”šè‡³ä¼˜äºä»…åœ¨åŒ…å«ROIçš„å›¾åƒä¸Šè·å¾—çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜æ­ç¤ºäº†Chaksuæ•°æ®é›†ä¸­å¯èƒ½å­˜åœ¨çš„å¶ç„¶å…³è”ï¼ŒåŒæ—¶å…¶æ€§èƒ½æ›´ç¬¦åˆæ— åè§æ¨¡å‹çš„é¢„æœŸã€‚é™¤äº†æ€§èƒ½åˆ†æï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨äº†SHAPè§£é‡Šæ–¹æ³•å’ŒåµŒå…¥åˆ†æã€‚æˆ‘ä»¬é‚€è¯·äº†ä¸€ä½æ”¾å°„ç§‘ä½é™¢åŒ»å¸ˆåœ¨ä¸åŒé®æŒ¡æ¡ä»¶ä¸‹è§£è¯»èƒ¸éƒ¨Xå°„çº¿ï¼Œä»¥ç»“åˆæˆ‘ä»¬çš„ä¸´åºŠçŸ¥è¯†å‘ç°ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/TheoSourget/MMC_Masking">https://github.com/TheoSourget/MMC_Masking</a> å’Œ <a target="_blank" rel="noopener" href="https://github.com/TheoSourget/MMC_Masking_EyeFundus">https://github.com/TheoSourget/MMC_Masking_EyeFundus</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04030v3">PDF</a> Updated after publication in the Journal of Imaging Informatics in   Medicine</p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒåˆ†ææ¨¡å‹å‘å±•æå‡æ€§èƒ½ï¼Œä½†éš¾ä»¥è§£é‡Šå’ŒéªŒè¯å†³ç­–ã€‚æ¨¡å‹å¯èƒ½åˆ©ç”¨å›¾åƒçš„éå…³é”®éƒ¨åˆ†è·å¾—é«˜åŸºå‡†æ•°æ®é›†æ€§èƒ½ï¼Œä½†åœ¨çœŸå®åœºæ™¯ä¸­å¤±è´¥ã€‚ç ”ç©¶æŒ‘æˆ˜å·ç§¯ç¥ç»ç½‘ç»œå¯¹èƒ¸éƒ¨Xå…‰å’Œçœ¼åº•å›¾åƒçš„åˆ†ç±»èƒ½åŠ›ï¼ŒåŒæ—¶æ©ç›–ä¸´åºŠä¸Šå…³é”®éƒ¨åˆ†ã€‚æ¨¡å‹åœ¨æ— å…³åŒºåŸŸä¸Šè¡¨ç°è‰¯å¥½ï¼Œç”šè‡³ä¼˜äºä»…åŒ…å«ROIçš„å›¾åƒã€‚ä»£ç å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†ææ¨¡å‹æ€§èƒ½æå‡åŒæ—¶ï¼Œå­˜åœ¨è§£é‡Šå’ŒéªŒè¯å†³ç­–å›°éš¾çš„é—®é¢˜ã€‚</li>
<li>æ¨¡å‹å¯èƒ½åˆ©ç”¨å›¾åƒçš„éç›¸å…³éƒ¨åˆ†ï¼ˆå³å¶ç„¶å…³è”æˆ–æ·å¾„ï¼‰è·å¾—é«˜åŸºå‡†æ•°æ®é›†æ€§èƒ½ï¼Œä½†åœ¨çœŸå®åœºæ™¯ä¸­åº”ç”¨æ—¶å¯èƒ½å¤±è´¥ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†å·ç§¯ç¥ç»ç½‘ç»œå¯¹èƒ¸éƒ¨Xå…‰å’Œçœ¼åº•å›¾åƒåˆ†ç±»çš„èƒ½åŠ›ï¼Œå¹¶åœ¨æ©ç›–ä¸´åºŠå…³é”®éƒ¨åˆ†çš„æƒ…å†µä¸‹è¿›è¡Œäº†æµ‹è¯•ã€‚</li>
<li>æ‰€æœ‰åœ¨PadChestæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œæ— è®ºé‡‡ç”¨ä½•ç§æ©ç›–ç­–ç•¥ï¼Œéƒ½èƒ½è·å¾—è¶…è¿‡éšæœºçš„AUCæ€§èƒ½ã€‚</li>
<li>åœ¨æ— å…³åŒºåŸŸä¸Šè®­ç»ƒçš„æ¨¡å‹è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹ä¼˜äºä»…åŒ…å«ROIçš„å›¾åƒã€‚</li>
<li>ç ”ç©¶å‘ç°äº†Chaksuæ•°æ®é›†ä¸­çš„ä¸€ç§å¯èƒ½çš„å¶ç„¶å…³è”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04030">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b29058033cf118f62d77ad1561b3b218.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9fc2600e79b34d06b4f7fccc7b884b3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-50b0ac347bfbe4d0abc9fe8d4bcc7a2a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="KAN-Mamba-FusionNet-Redefining-Medical-Image-Segmentation-with-Non-Linear-Modeling"><a href="#KAN-Mamba-FusionNet-Redefining-Medical-Image-Segmentation-with-Non-Linear-Modeling" class="headerlink" title="KAN-Mamba FusionNet: Redefining Medical Image Segmentation with   Non-Linear Modeling"></a>KAN-Mamba FusionNet: Redefining Medical Image Segmentation with   Non-Linear Modeling</h2><p><strong>Authors:Akansh Agrawal, Akshan Agrawal, Shashwat Gupta, Priyanka Bagade</strong></p>
<p>Medical image segmentation is essential for applications like robotic surgeries, disease diagnosis, and treatment planning. Recently, various deep-learning models have been proposed to enhance medical image segmentation. One promising approach utilizes Kolmogorov-Arnold Networks (KANs), which better capture non-linearity in input data. However, they are unable to effectively capture long-range dependencies, which are required to accurately segment complex medical images and, by that, improve diagnostic accuracy in clinical settings. Neural networks such as Mamba can handle long-range dependencies. However, they have a limited ability to accurately capture non-linearities in the images as compared to KANs. Thus, we propose a novel architecture, the KAN-Mamba FusionNet, which improves segmentation accuracy by effectively capturing the non-linearities from input and handling long-range dependencies with the newly proposed KAMBA block. We evaluated the proposed KAN-Mamba FusionNet on three distinct medical image segmentation datasets: BUSI, Kvasir-Seg, and GlaS - and found it consistently outperforms state-of-the-art methods in IoU and F1 scores. Further, we examined the effects of various components and assessed their contributions to the overall model performance via ablation studies. The findings highlight the effectiveness of this methodology for reliable medical image segmentation, providing a unique approach to address intricate visual data issues in healthcare. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹äºæœºå™¨äººæ‰‹æœ¯ã€ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼Œä¸ºäº†æå‡åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ•ˆæœï¼Œå·²ç»æå‡ºäº†å„ç§æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•æ˜¯åˆ©ç”¨Kolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰ï¼Œå®ƒèƒ½æ›´å¥½åœ°æ•æ‰è¾“å…¥æ•°æ®ä¸­çš„éçº¿æ€§ã€‚ç„¶è€Œï¼Œå®ƒä»¬æ— æ³•æœ‰æ•ˆåœ°æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œè¿™å¯¹äºå‡†ç¡®åˆ†å‰²å¤æ‚çš„åŒ»å­¦å›¾åƒä»¥åŠåœ¨ä¸´åºŠç¯å¢ƒä¸­æé«˜è¯Šæ–­å‡†ç¡®æ€§æ˜¯å¿…è¦çš„ã€‚ç¥ç»ç½‘ç»œï¼ˆå¦‚Mambaï¼‰èƒ½å¤Ÿå¤„ç†é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œä½†ä¸KANsç›¸æ¯”ï¼Œå®ƒä»¬åœ¨å‡†ç¡®æ•æ‰å›¾åƒéçº¿æ€§æ–¹é¢çš„èƒ½åŠ›æœ‰é™ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¶æ„ï¼Œå³KAN-Mamba FusionNetï¼Œå®ƒé€šè¿‡æœ‰æ•ˆæ•æ‰è¾“å…¥çš„éçº¿æ€§å’Œä½¿ç”¨æ–°æå‡ºçš„KAMBAå—æ¥å¤„ç†é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œæé«˜äº†åˆ†å‰²å‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªä¸åŒçš„åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„KAN-Mamba FusionNetï¼šBUSIã€Kvasir-Segå’ŒGlaSï¼Œå‘ç°å®ƒåœ¨IoUå’ŒF1åˆ†æ•°ä¸Šå‡ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å„ç§ç»„ä»¶çš„å½±å“ï¼Œå¹¶é€šè¿‡æ¶ˆèç ”ç©¶è¯„ä¼°äº†å®ƒä»¬å¯¹æ•´ä½“æ¨¡å‹æ€§èƒ½çš„è´¡çŒ®ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¯é çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºåŒ»ç–—ä¿å¥ä¸­å¤æ‚çš„è§†è§‰æ•°æ®é—®é¢˜æä¾›äº†ç‹¬ç‰¹çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11926v2">PDF</a> 11 pages, 2 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>åŸºäºKolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰å’ŒMambaç¥ç»ç½‘ç»œçš„ä¼˜åŠ¿ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„åŒ»å­¦å›¾åƒåˆ†å‰²ç½‘ç»œæ¶æ„â€”â€”KAN-Mamba FusionNetã€‚è¯¥æ¶æ„èåˆäº†KANså¯¹éçº¿æ€§çš„æ•æ‰èƒ½åŠ›å’ŒMambaå¯¹é•¿è·ç¦»ä¾èµ–çš„å¤„ç†èƒ½åŠ›ï¼Œé€šè¿‡æ–°æå‡ºçš„KAMBAæ¨¡å—ï¼Œæé«˜äº†åˆ†å‰²å‡†ç¡®åº¦å’Œè¯Šæ–­å‡†ç¡®æ€§ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†å…¶è¡¨ç°ä¼˜äºå…¶ä»–å‰æ²¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨æœºå™¨äººæ‰‹æœ¯ã€ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ç­‰åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>æœ€è¿‘æå‡ºçš„æ·±åº¦å­¦ä¹ æ¨¡å‹å¦‚Kolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰å’ŒMambaç¥ç»ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æœ‰æ½œåŠ›ã€‚</li>
<li>KANsèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è¾“å…¥æ•°æ®çš„éçº¿æ€§ï¼Œä½†æ— æ³•æœ‰æ•ˆæ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚</li>
<li>Mambaç¥ç»ç½‘ç»œèƒ½å¤Ÿå¤„ç†é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œä½†åœ¨æ•æ‰å›¾åƒéçº¿æ€§æ–¹é¢èƒ½åŠ›æœ‰é™ã€‚</li>
<li>æå‡ºçš„KAN-Mamba FusionNetç»“åˆäº†KANså’ŒMambaçš„ä¼˜åŠ¿ï¼Œé€šè¿‡KAMBAæ¨¡å—æé«˜äº†åˆ†å‰²å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨ä¸‰ä¸ªä¸åŒçš„åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒKAN-Mamba FusionNetçš„è¡¨ç°ä¼˜äºå…¶ä»–æœ€æ–°æ–¹æ³•ã€‚</li>
<li>é€šè¿‡æ¶ˆèç ”ç©¶ï¼Œè¯å®äº†æ¶æ„ä¸­å„ä¸ªç»„ä»¶çš„æœ‰æ•ˆæ€§åŠå…¶å¯¹æ¨¡å‹æ€§èƒ½çš„è´¡çŒ®ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.11926">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d4112361f3a638fd902c354b976ddd5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f1e39e8e5033e0b4334778b14531e91.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Local-Clustering-for-Lung-Cancer-Image-Classification-via-Sparse-Solution-Technique"><a href="#Local-Clustering-for-Lung-Cancer-Image-Classification-via-Sparse-Solution-Technique" class="headerlink" title="Local Clustering for Lung Cancer Image Classification via Sparse   Solution Technique"></a>Local Clustering for Lung Cancer Image Classification via Sparse   Solution Technique</h2><p><strong>Authors:Jackson Hamel, Ming-Jun Lai, Zhaiming Shen, Ye Tian</strong></p>
<p>In this work, we propose to use a local clustering approach based on the sparse solution technique to study the medical image, especially the lung cancer image classification task. We view images as the vertices in a weighted graph and the similarity between a pair of images as the edges in the graph. The vertices within the same cluster can be assumed to share similar features and properties, thus making the applications of graph clustering techniques very useful for image classification. Recently, the approach based on the sparse solutions of linear systems for graph clustering has been found to identify clusters more efficiently than traditional clustering methods such as spectral clustering. We propose to use the two newly developed local clustering methods based on sparse solution of linear system for image classification. In addition, we employ a box spline-based tight-wavelet-framelet method to clean these images and help build a better adjacency matrix before clustering. The performance of our methods is shown to be very effective in classifying images. Our approach is significantly more efficient and either favorable or equally effective compared with other state-of-the-art approaches. Finally, we shall make a remark by pointing out two image deformation methods to build up more artificial image data to increase the number of labeled images. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç¨€ç–è§£æŠ€æœ¯çš„å±€éƒ¨èšç±»æ–¹æ³•ï¼Œç”¨äºåŒ»å­¦å›¾åƒçš„ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯åœ¨è‚ºç™Œå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬å°†å›¾åƒè§†ä¸ºåŠ æƒå›¾ä¸­çš„é¡¶ç‚¹ï¼Œå°†å›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§è§†ä¸ºå›¾ä¸­çš„è¾¹ã€‚åŒä¸€èšç±»ä¸­çš„é¡¶ç‚¹å¯ä»¥å‡å®šå…·æœ‰ç›¸ä¼¼çš„ç‰¹å¾å’Œå±æ€§ï¼Œè¿™ä½¿å¾—å›¾èšç±»æŠ€æœ¯åœ¨å›¾åƒåˆ†ç±»ä¸­çš„åº”ç”¨éå¸¸æœ‰ç”¨ã€‚æœ€è¿‘å‘ç°ï¼ŒåŸºäºçº¿æ€§ç³»ç»Ÿç¨€ç–è§£çš„èšç±»æ–¹æ³•æ¯”ä¼ ç»Ÿçš„èšç±»æ–¹æ³•ï¼ˆå¦‚è°±èšç±»ï¼‰æ›´èƒ½æœ‰æ•ˆåœ°è¯†åˆ«èšç±»ã€‚æˆ‘ä»¬å»ºè®®ä½¿ç”¨ä¸¤ç§æ–°å¼€å‘çš„åŸºäºçº¿æ€§ç³»ç»Ÿç¨€ç–è§£çš„å±€éƒ¨èšç±»æ–¹æ³•è¿›è¡Œå›¾åƒåˆ†ç±»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨åŸºäºç®±çº¿ç´§å°æ³¢æ¡†æ¶çš„æ–¹æ³•å¯¹å›¾åƒè¿›è¡Œæ¸…ç†ï¼Œå¸®åŠ©åœ¨èšç±»ä¹‹å‰å»ºç«‹æ›´å¥½çš„é‚»æ¥çŸ©é˜µã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒåˆ†ç±»æ–¹é¢çš„è¡¨ç°éå¸¸æœ‰æ•ˆï¼Œä¸å…¶ä»–æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ›´æœ‰æ•ˆç‡ï¼Œè¦ä¹ˆå…·æœ‰ä¼˜åŠ¿ï¼Œè¦ä¹ˆåŒæ ·æœ‰æ•ˆã€‚æœ€åï¼Œæˆ‘ä»¬åº”è¯¥æŒ‡å‡ºä¸¤ç§å›¾åƒå˜å½¢æ–¹æ³•æ¥å»ºç«‹æ›´å¤šçš„äººå·¥å›¾åƒæ•°æ®ï¼Œä»¥å¢åŠ æ ‡è®°å›¾åƒçš„æ•°é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.08800v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºåˆ©ç”¨åŸºäºç¨€ç–è§£æŠ€æœ¯çš„å±€éƒ¨èšç±»æ–¹æ³•ï¼Œé’ˆå¯¹åŒ»å­¦å›¾åƒå°¤å…¶æ˜¯è‚ºç™Œå›¾åƒåˆ†ç±»ä»»åŠ¡è¿›è¡Œç ”ç©¶ã€‚å°†å›¾åƒè§†ä¸ºåŠ æƒå›¾ä¸­çš„é¡¶ç‚¹ï¼Œå›¾åƒé—´çš„ç›¸ä¼¼æ€§è§†ä¸ºå›¾ä¸­çš„è¾¹ã€‚åŒä¸€èšç±»ä¸­çš„é¡¶ç‚¹å¯è§†ä¸ºå…·æœ‰ç›¸ä¼¼ç‰¹å¾å’Œå±æ€§ï¼Œå› æ­¤å›¾èšç±»æŠ€æœ¯å¯¹äºå›¾åƒåˆ†ç±»éå¸¸æœ‰ç”¨ã€‚åŸºäºçº¿æ€§ç³»ç»Ÿç¨€ç–è§£çš„å›¾èšç±»æ–¹æ³•èƒ½æ›´æœ‰æ•ˆåœ°è¯†åˆ«èšç±»ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†åŸºäºç›’çŠ¶æ ·æ¡çš„ç´§å°æ³¢æ¡†æ¶æ–¹æ³•å¯¹å›¾åƒè¿›è¡Œæ¸…æ´—ï¼Œå¸®åŠ©å»ºç«‹æ›´å¥½çš„é‚»æ¥çŸ©é˜µè¿›è¡Œèšç±»ã€‚è¯¥æ–¹æ³•åœ¨å›¾åƒåˆ†ç±»æ–¹é¢è¡¨ç°å‡ºéå¸¸æœ‰æ•ˆçš„æ€§èƒ½ï¼Œä¸å…¶ä»–æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„æ•ˆç‡ï¼Œè¦ä¹ˆä¼˜è¶Šï¼Œè¦ä¹ˆç›¸å½“ã€‚æœ€åæå‡ºäº†ä¸¤ç§ç”Ÿæˆæ›´å¤šäººå·¥å›¾åƒæ•°æ®çš„æ–¹æ³•ä»¥å¢åŠ æ ‡æ³¨å›¾åƒçš„æ•°é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é‡‡ç”¨åŸºäºç¨€ç–è§£æŠ€æœ¯çš„å±€éƒ¨èšç±»æ–¹æ³•ç”¨äºåŒ»å­¦å›¾åƒåˆ†ç±»ï¼Œç‰¹åˆ«æ˜¯è‚ºç™Œå›¾åƒåˆ†ç±»ã€‚</li>
<li>å°†å›¾åƒè§†ä¸ºåŠ æƒå›¾ä¸­çš„é¡¶ç‚¹ï¼Œå›¾åƒé—´çš„ç›¸ä¼¼æ€§è§†ä¸ºå›¾ä¸­çš„è¾¹ï¼Œåº”ç”¨å›¾èšç±»æŠ€æœ¯è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>åŸºäºçº¿æ€§ç³»ç»Ÿç¨€ç–è§£çš„å›¾èšç±»æ–¹æ³•èƒ½æ›´æœ‰æ•ˆåœ°è¯†åˆ«èšç±»ï¼Œæé«˜å›¾åƒåˆ†ç±»æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨ç›’çŠ¶æ ·æ¡çš„ç´§å°æ³¢æ¡†æ¶æ–¹æ³•å¯¹å›¾åƒè¿›è¡Œæ¸…æ´—ï¼Œä¼˜åŒ–é‚»æ¥çŸ©é˜µçš„å»ºç«‹ã€‚</li>
<li>è¯¥æ–¹æ³•ç›¸è¾ƒäºå…¶ä»–æœ€æ–°æ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œè¡¨ç°ä¸ºé«˜æ•ˆç‡ã€ä¼˜è¶Šæˆ–ç›¸å½“æœ‰æ•ˆã€‚</li>
<li>æå‡ºäº†é€šè¿‡å›¾åƒå˜å½¢æ–¹æ³•ç”Ÿæˆæ›´å¤šäººå·¥å›¾åƒæ•°æ®ä»¥å¢åŠ æ ‡æ³¨å›¾åƒæ•°é‡çš„ç­–ç•¥ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºåŒ»å­¦å›¾åƒåˆ†ç±»æä¾›äº†æ–°çš„è§†è§’å’ŒæŠ€æœ¯æ‰‹æ®µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.08800">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eb02e43f099b1ec0b4ebd27881430a00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a271262fb3731e13b6499f827844169.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07a5dd90390bb86a27ba8ca0e9a9379b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-458f7cd770c6b753c5e38b60d44de7b1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DEFT-Efficient-Fine-Tuning-of-Diffusion-Models-by-Learning-the-Generalised-h-transform"><a href="#DEFT-Efficient-Fine-Tuning-of-Diffusion-Models-by-Learning-the-Generalised-h-transform" class="headerlink" title="DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the   Generalised $h$-transform"></a>DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the   Generalised $h$-transform</h2><p><strong>Authors:Alexander Denker, Francisco Vargas, Shreyas Padhy, Kieran Didi, Simon Mathis, Vincent Dutordoir, Riccardo Barbano, Emile Mathieu, Urszula Julia Komorowska, Pietro Lio</strong></p>
<p>Generative modelling paradigms based on denoising diffusion processes have emerged as a leading candidate for conditional sampling in inverse problems. In many real-world applications, we often have access to large, expensively trained unconditional diffusion models, which we aim to exploit for improving conditional sampling. Most recent approaches are motivated heuristically and lack a unifying framework, obscuring connections between them. Further, they often suffer from issues such as being very sensitive to hyperparameters, being expensive to train or needing access to weights hidden behind a closed API. In this work, we unify conditional training and sampling using the mathematically well-understood Doobâ€™s h-transform. This new perspective allows us to unify many existing methods under a common umbrella. Under this framework, we propose DEFT (Doobâ€™s h-transform Efficient FineTuning), a new approach for conditional generation that simply fine-tunes a very small network to quickly learn the conditional $h$-transform, while keeping the larger unconditional network unchanged. DEFT is much faster than existing baselines while achieving state-of-the-art performance across a variety of linear and non-linear benchmarks. On image reconstruction tasks, we achieve speedups of up to 1.6$\times$, while having the best perceptual quality on natural images and reconstruction performance on medical images. Further, we also provide initial experiments on protein motif scaffolding and outperform reconstruction guidance methods. </p>
<blockquote>
<p>åŸºäºå»å™ªæ‰©æ•£è¿‡ç¨‹çš„ç”Ÿæˆå»ºæ¨¡èŒƒå¼å·²æˆä¸ºé€†é—®é¢˜ä¸­æ¡ä»¶é‡‡æ ·çš„é¢†å…ˆå€™é€‰æ–¹æ³•ã€‚åœ¨è®¸å¤šå®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å¯ä»¥è®¿é—®å¤§å‹ä¸”ç»è¿‡æ˜‚è´µè®­ç»ƒçš„æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬æ—¨åœ¨åˆ©ç”¨è¿™äº›æ¨¡å‹æ¥æ”¹å–„æ¡ä»¶é‡‡æ ·ã€‚æœ€è¿‘çš„æ–¹æ³•å¤§å¤šæ˜¯å¯å‘å¼ä¸”ç¼ºä¹ç»Ÿä¸€æ¡†æ¶ï¼Œå¯¼è‡´å®ƒä»¬ä¹‹é—´çš„è”ç³»æ¨¡ç³Šã€‚æ­¤å¤–ï¼Œå®ƒä»¬ç»å¸¸é¢ä¸´è¯¸å¦‚å¯¹è¶…å‚æ•°éå¸¸æ•æ„Ÿã€è®­ç»ƒæˆæœ¬é«˜æ˜‚æˆ–éœ€è¦è®¿é—®å°é—­APIåé¢çš„æƒé‡ç­‰é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ•°å­¦ä¸Šç†è§£è‰¯å¥½çš„Doobçš„hè½¬æ¢æ¥ç»Ÿä¸€æ¡ä»¶è®­ç»ƒå’Œé‡‡æ ·ã€‚è¿™ä¸ªæ–°è§†è§’å…è®¸æˆ‘ä»¬å°†è®¸å¤šç°æœ‰æ–¹æ³•çº³å…¥åŒä¸€æ¡†æ¶ä¸‹ã€‚åœ¨è¿™ä¸ªæ¡†æ¶ä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†DEFTï¼ˆDoobçš„hè½¬æ¢é«˜æ•ˆå¾®è°ƒï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¡ä»¶ç”Ÿæˆæ–¹æ³•ï¼Œå®ƒåªéœ€å¾®è°ƒä¸€ä¸ªéå¸¸å°çš„ç½‘ç»œï¼Œå³å¯å¿«é€Ÿå­¦ä¹ æ¡ä»¶hè½¬æ¢ï¼ŒåŒæ—¶ä¿æŒè¾ƒå¤§çš„æ— æ¡ä»¶ç½‘ç»œä¸å˜ã€‚DEFTç›¸æ¯”ç°æœ‰åŸºå‡†æµ‹è¯•é€Ÿåº¦æ›´å¿«ï¼ŒåŒæ—¶åœ¨å„ç§çº¿æ€§å’Œéçº¿æ€§åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸Šï¼Œæˆ‘ä»¬å®ç°äº†æœ€é«˜è¾¾1.6å€çš„åŠ é€Ÿï¼ŒåŒæ—¶åœ¨è‡ªç„¶å›¾åƒä¸Šè·å¾—æœ€ä½³çš„æ„ŸçŸ¥è´¨é‡å’Œåœ¨åŒ»å­¦å›¾åƒä¸Šçš„é‡å»ºæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†åˆæ­¥çš„è›‹ç™½è´¨åŸºåºæ”¯æ¶å®éªŒï¼Œå¹¶åœ¨é‡å»ºæŒ‡å¯¼æ–¹æ³•ä¸Šè¡¨ç°å‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01781v5">PDF</a> arXiv admin note: text overlap with arXiv:2312.09236</p>
<p><strong>Summary</strong><br>     åŸºäºå»å™ªæ‰©æ•£è¿‡ç¨‹çš„ç”Ÿæˆå»ºæ¨¡èŒƒå¼å·²æˆä¸ºè§£å†³åé—®é¢˜ä¸­æ¡ä»¶é‡‡æ ·çš„é¢†å…ˆå€™é€‰æ–¹æ³•ã€‚æœ¬æ–‡åˆ©ç”¨æ•°å­¦ä¸Šç†è§£è‰¯å¥½çš„Doobçš„h-å˜æ¢ç»Ÿä¸€äº†æ¡ä»¶è®­ç»ƒå’Œé‡‡æ ·ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºäº†DEFTï¼ˆDoobçš„h-å˜æ¢é«˜æ•ˆå¾®è°ƒï¼‰è¿™ä¸€æ–°çš„æ¡ä»¶ç”Ÿæˆæ–¹æ³•ï¼Œåªéœ€å¾®è°ƒä¸€ä¸ªéå¸¸å°çš„ç½‘ç»œå³å¯å¿«é€Ÿå­¦ä¹ æ¡ä»¶h-å˜æ¢ï¼ŒåŒæ—¶ä¿æŒè¾ƒå¤§çš„æ— æ¡ä»¶ç½‘ç»œä¸å˜ã€‚DEFTåœ¨å¤šç§çº¿æ€§å’Œéçº¿æ€§åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸Šå®ç°äº†æœ€é«˜è¾¾1.6å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶åœ¨è‡ªç„¶å›¾åƒä¸Šå…·æœ‰æœ€ä½³çš„æ„ŸçŸ¥è´¨é‡å’ŒåŒ»ç–—å›¾åƒçš„é‡å»ºæ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜åœ¨è›‹ç™½è´¨åŸºåºæ”¯æ¶æ–¹é¢è¿›è¡Œäº†åˆæ­¥å®éªŒï¼Œå¹¶è¶…è¶Šäº†é‡å»ºæŒ‡å¯¼æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå»ºæ¨¡èŒƒå¼åœ¨è§£å†³åé—®é¢˜çš„æ¡ä»¶é‡‡æ ·ä¸­å æ®é¢†å…ˆåœ°ä½ï¼Œå…¶ä¸­åŸºäºå»å™ªæ‰©æ•£è¿‡ç¨‹çš„æ–¹æ³•å—åˆ°å…³æ³¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç¼ºä¹ç»Ÿä¸€æ¡†æ¶ï¼Œä¸”å­˜åœ¨è¶…å‚æ•°æ•æ„Ÿã€è®­ç»ƒæˆæœ¬é«˜æ˜‚ã€ä¾èµ–å°é—­APIç­‰é—®é¢˜ã€‚</li>
<li>å¼•å…¥Doobçš„h-å˜æ¢æ¥ç»Ÿä¸€æ¡ä»¶è®­ç»ƒå’Œé‡‡æ ·ï¼Œæä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’ã€‚</li>
<li>æå‡ºDEFTæ–¹æ³•ï¼Œé€šè¿‡å¾®è°ƒå°ç½‘ç»œå¿«é€Ÿå­¦ä¹ æ¡ä»¶h-å˜æ¢ï¼Œä¿æŒå¤§ç½‘ç»œä¸å˜ï¼Œå®ç°å¿«é€Ÿä¸”é«˜è´¨é‡çš„æ¡ä»¶ç”Ÿæˆã€‚</li>
<li>DEFTåœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡å’Œæ„ŸçŸ¥è´¨é‡æ”¹è¿›ã€‚</li>
<li>DEFTåœ¨åŒ»ç–—å›¾åƒé‡å»ºæ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.01781">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3bcfe49d7ffed4a06f10a0a8dd981ec2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-089cd1947b9623c22b4d8ad4cb1c4339.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Breast-Cancer-Classification-Using-Gradient-Boosting-Algorithms-Focusing-on-Reducing-the-False-Negative-and-SHAP-for-Explainability"><a href="#Breast-Cancer-Classification-Using-Gradient-Boosting-Algorithms-Focusing-on-Reducing-the-False-Negative-and-SHAP-for-Explainability" class="headerlink" title="Breast Cancer Classification Using Gradient Boosting Algorithms Focusing   on Reducing the False Negative and SHAP for Explainability"></a>Breast Cancer Classification Using Gradient Boosting Algorithms Focusing   on Reducing the False Negative and SHAP for Explainability</h2><p><strong>Authors:JoÃ£o Manoel Herrera Pinheiro, Marcelo Becker</strong></p>
<p>Cancer is one of the diseases that kill the most women in the world, with breast cancer being responsible for the highest number of cancer cases and consequently deaths. However, it can be prevented by early detection and, consequently, early treatment. Any development for detection or perdition this kind of cancer is important for a better healthy life. Many studies focus on a model with high accuracy in cancer prediction, but sometimes accuracy alone may not always be a reliable metric. This study implies an investigative approach to studying the performance of different machine learning algorithms based on boosting to predict breast cancer focusing on the recall metric. Boosting machine learning algorithms has been proven to be an effective tool for detecting medical diseases. The dataset of the University of California, Irvine (UCI) repository has been utilized to train and test the model classifier that contains their attributes. The main objective of this study is to use state-of-the-art boosting algorithms such as AdaBoost, XGBoost, CatBoost and LightGBM to predict and diagnose breast cancer and to find the most effective metric regarding recall, ROC-AUC, and confusion matrix. Furthermore, our study is the first to use these four boosting algorithms with Optuna, a library for hyperparameter optimization, and the SHAP method to improve the interpretability of our model, which can be used as a support to identify and predict breast cancer. We were able to improve AUC or recall for all the models and reduce the False Negative for AdaBoost and LigthGBM the final AUC were more than 99.41% for all models. </p>
<blockquote>
<p>ç™Œç—‡æ˜¯ä¸–ç•Œä¸Šå¯¼è‡´å¥³æ€§æ­»äº¡çš„ä¸»è¦ç–¾ç—…ä¹‹ä¸€ï¼Œå…¶ä¸­ä¹³è…ºç™Œç—…ä¾‹æ•°å’Œæ­»äº¡äººæ•°å‡å±…é¦–ä½ã€‚ç„¶è€Œï¼Œé€šè¿‡æ—©æœŸå‘ç°å’Œæ—©æœŸæ²»ç–—ï¼Œç™Œç—‡æ˜¯å¯ä»¥é¢„é˜²çš„ã€‚ä»»ä½•æœ‰åŠ©äºæ£€æµ‹æˆ–é¢„é˜²è¿™ç±»ç™Œç—‡çš„å‘å±•å¯¹äºæ›´å¥½åœ°ç”Ÿæ´»å…·æœ‰é‡è¦æ„ä¹‰ã€‚è®¸å¤šç ”ç©¶é›†ä¸­åœ¨å¼€å‘å…·æœ‰é«˜ç²¾åº¦é¢„æµ‹ç™Œç—‡çš„æ¨¡å‹ä¸Šï¼Œä½†æœ‰æ—¶ä»…ä¾é å‡†ç¡®æ€§å¯èƒ½å¹¶ä¸æ˜¯ä¸€ä¸ªå¯é çš„æŒ‡æ ‡ã€‚æœ¬ç ”ç©¶é‡‡ç”¨ä¸€ç§åŸºäºboostingçš„æœºå™¨å­¦ä¹ æ–¹æ³•æ¥ç ”ç©¶é¢„æµ‹ä¹³è…ºç™Œçš„æ€§èƒ½ï¼Œé‡ç‚¹ç ”ç©¶å¬å›ç‡æŒ‡æ ‡ã€‚å·²æœ‰ç ”ç©¶è¯æ˜ï¼Œä½¿ç”¨boostingç®—æ³•çš„æœºå™¨å­¦ä¹ æ˜¯æ£€æµ‹åŒ»å­¦ç–¾ç—…çš„æœ‰æ•ˆå·¥å…·ã€‚æœ¬ç ”ç©¶ä½¿ç”¨äº†åŠ åˆ©ç¦å°¼äºšå¤§å­¦æ¬§æ–‡åˆ†æ ¡ï¼ˆUCIï¼‰å­˜å‚¨åº“çš„æ•°æ®é›†æ¥è®­ç»ƒå’Œæµ‹è¯•åˆ†ç±»æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŒ…å«äº†å„ç§å±æ€§ã€‚æœ¬ç ”ç©¶çš„ä¸»è¦ç›®æ ‡æ˜¯ä½¿ç”¨æœ€æ–°çš„boostingç®—æ³•ï¼ˆå¦‚AdaBoostã€XGBoostã€CatBoostå’ŒLightGBMï¼‰æ¥é¢„æµ‹å’Œè¯Šæ–­ä¹³è…ºç™Œï¼Œå¹¶æ‰¾åˆ°å…³äºå¬å›ç‡ã€ROC-AUCå’Œæ··æ·†çŸ©é˜µçš„æœ€æœ‰æ•ˆæŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ˜¯é¦–æ¬¡å°†è¿™å››ä¸ªboostingç®—æ³•ä¸ç”¨äºè¶…å‚æ•°ä¼˜åŒ–çš„Optunaåº“ä»¥åŠSHAPæ–¹æ³•ç›¸ç»“åˆï¼Œä»¥æé«˜æˆ‘ä»¬æ¨¡å‹çš„è§£é‡Šæ€§ï¼Œå¯ä½œä¸ºæ£€æµ‹å’Œé¢„æµ‹ä¹³è…ºç™Œçš„è¾…åŠ©å·¥å…·ã€‚æˆ‘ä»¬æˆåŠŸæé«˜äº†æ‰€æœ‰æ¨¡å‹çš„AUCæˆ–å¬å›ç‡ï¼Œå¹¶é™ä½äº†AdaBoostå’ŒLightGBMçš„å‡é˜´æ€§ç»“æœã€‚æ‰€æœ‰æ¨¡å‹çš„æœ€ç»ˆAUCå‡è¶…è¿‡99.41%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.09548v3">PDF</a> 9 pages, 16 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åˆ©ç”¨æœºå™¨å­¦ä¹ ç®—æ³•é¢„æµ‹ä¹³è…ºç™Œçš„é—®é¢˜ã€‚ç ”ç©¶é€šè¿‡ä½¿ç”¨AdaBoostã€XGBoostã€CatBoostå’ŒLightGBMç­‰å…ˆè¿›ç®—æ³•è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•æ¨¡å‹åˆ†ç±»å™¨ï¼Œå¹¶åˆ©ç”¨æ•°æ®é›†æ‰¾å‡ºæœ€æœ‰æ•ˆçš„è¯„ä»·æŒ‡æ ‡å…³äºå¬å›ç‡ã€ROC-AUCå’Œæ··æ·†çŸ©é˜µã€‚è¯¥ç ”ç©¶é¦–æ¬¡ç»“åˆOptunaåº“è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–å’ŒSHAPæ–¹æ³•æ”¹å–„æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œä»è€Œæœ‰æ•ˆæ”¹å–„ä¹³è…ºç™Œçš„è¯Šæ–­ä¸é¢„æµ‹æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä¹³è…ºç™Œæ˜¯ä¸–ç•ŒèŒƒå›´å†…å¯¼è‡´å¥³æ€§æ­»äº¡çš„ä¸»è¦ç–¾ç—…ä¹‹ä¸€ï¼Œæ—©æœŸæ£€æµ‹å’Œé¢„é˜²æ˜¯å…³é”®ã€‚</li>
<li>ä»…ä¾èµ–å‡†ç¡®æ€§ä½œä¸ºè¯„ä¼°æŒ‡æ ‡å¯èƒ½ä¸è¶³å¤Ÿå¯é ï¼Œå› æ­¤æœ¬ç ”ç©¶å…³æ³¨å¬å›ç‡ç­‰è¯„ä»·æŒ‡æ ‡ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†åŠ å·å¤§å­¦æ¬§æ–‡åˆ†æ ¡ï¼ˆUCIï¼‰çš„æ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•ã€‚</li>
<li>AdaBoostã€XGBoostã€CatBoostå’ŒLightGBMç­‰ç®—æ³•è¢«åº”ç”¨äºé¢„æµ‹å’Œè¯Šæ–­ä¹³è…ºç™Œã€‚</li>
<li>ç»“åˆOptunaåº“è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–ï¼Œå¹¶ä½¿ç”¨SHAPæ–¹æ³•æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>æ‰€æœ‰æ¨¡å‹çš„AUCæˆ–å¬å›ç‡å‡æœ‰æ‰€æå‡ï¼ŒAdaBoostå’ŒLightGBMçš„å‡é˜´æ€§ç»“æœæœ‰æ‰€é™ä½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.09548">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3b50b0b930d513f3420df80c4a766ca9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4e19406f0596642cb77bdff72a220c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5a54d231a3ec331cbcf7c471bf5f7d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4770fc034555293478c5f5e5161d5388.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73b8f5e503d8bacb43ae6c2a2d74079e.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Generalizing-Medical-Image-Representations-via-Quaternion-Wavelet-Networks"><a href="#Generalizing-Medical-Image-Representations-via-Quaternion-Wavelet-Networks" class="headerlink" title="Generalizing Medical Image Representations via Quaternion Wavelet   Networks"></a>Generalizing Medical Image Representations via Quaternion Wavelet   Networks</h2><p><strong>Authors:Luigi Sigillo, Eleonora Grassucci, Aurelio Uncini, Danilo Comminiello</strong></p>
<p>Neural network generalizability is becoming a broad research field due to the increasing availability of datasets from different sources and for various tasks. This issue is even wider when processing medical data, where a lack of methodological standards causes large variations being provided by different imaging centers or acquired with various devices and cofactors. To overcome these limitations, we introduce a novel, generalizable, data- and task-agnostic framework able to extract salient features from medical images. The proposed quaternion wavelet network (QUAVE) can be easily integrated with any pre-existing medical image analysis or synthesis task, and it can be involved with real, quaternion, or hypercomplex-valued models, generalizing their adoption to single-channel data. QUAVE first extracts different sub-bands through the quaternion wavelet transform, resulting in both low-frequency&#x2F;approximation bands and high-frequency&#x2F;fine-grained features. Then, it weighs the most representative set of sub-bands to be involved as input to any other neural model for image processing, replacing standard data samples. We conduct an extensive experimental evaluation comprising different datasets, diverse image analysis, and synthesis tasks including reconstruction, segmentation, and modality translation. We also evaluate QUAVE in combination with both real and quaternion-valued models. Results demonstrate the effectiveness and the generalizability of the proposed framework that improves network performance while being flexible to be adopted in manifold scenarios and robust to domain shifts. The full code is available at: <a target="_blank" rel="noopener" href="https://github.com/ispamm/QWT">https://github.com/ispamm/QWT</a>. </p>
<blockquote>
<p>ç¥ç»ç½‘ç»œæ³›åŒ–æ€§å› ä¸åŒæ¥æºå’Œä»»åŠ¡çš„æ•°æ®é›†æ—¥ç›Šå¢å¤šè€Œæˆä¸ºä¸€ä¸ªå¹¿æ³›çš„ç ”ç©¶é¢†åŸŸã€‚åœ¨å¤„ç†åŒ»ç–—æ•°æ®æ—¶ï¼Œè¿™ä¸ªé—®é¢˜æ›´ä¸ºçªå‡ºï¼Œå› ä¸ºç¼ºä¹æ–¹æ³•è®ºæ ‡å‡†å¯¼è‡´ç”±ä¸åŒæˆåƒä¸­å¿ƒæˆ–é€šè¿‡å„ç§è®¾å¤‡å’Œè¾…åŠ©å› ç´ è·å¾—çš„æ•°æ®å­˜åœ¨å¾ˆå¤§å·®å¼‚ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹ã€å¯æ³›åŒ–çš„ã€ç‹¬ç«‹äºæ•°æ®å’Œä»»åŠ¡çš„åˆ†ææ¡†æ¶ï¼Œèƒ½å¤Ÿä»åŒ»å­¦å›¾åƒä¸­æå–é‡è¦ç‰¹å¾ã€‚æ‰€æå‡ºçš„å››å…ƒå°æ³¢ç½‘ç»œï¼ˆQUAVEï¼‰å¯ä»¥è½»æ¾åœ°ä¸ä»»ä½•ç°æœ‰çš„åŒ»å­¦å›¾åƒåˆ†ææˆ–åˆæˆä»»åŠ¡é›†æˆï¼Œå¹¶ä¸”å¯ä»¥æ¶‰åŠå®æ•°ã€å››å…ƒæ•°æˆ–è¶…å¤æ•°æ¨¡å‹ï¼Œå°†å…¶æ¨å¹¿åˆ°å•é€šé“æ•°æ®ã€‚QUAVEé¦–å…ˆé€šè¿‡å››å…ƒå°æ³¢å˜æ¢æå–ä¸åŒçš„å­å¸¦ï¼Œå¾—åˆ°ä½é¢‘&#x2F;è¿‘ä¼¼å¸¦å’Œé«˜é¢‘&#x2F;ç²¾ç»†ç‰¹å¾ã€‚ç„¶åï¼Œå®ƒæƒè¡¡æœ€å…·ä»£è¡¨æ€§çš„å­å¸¦é›†ä½œä¸ºå…¶ä»–ç¥ç»ç½‘ç»œæ¨¡å‹å¤„ç†å›¾åƒçš„è¾“å…¥ï¼Œä»£æ›¿æ ‡å‡†æ•°æ®æ ·æœ¬ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒè¯„ä¼°ï¼ŒåŒ…æ‹¬ä¸åŒçš„æ•°æ®é›†ã€å¤šæ ·çš„å›¾åƒåˆ†æå’Œåˆæˆä»»åŠ¡ï¼Œå¦‚é‡å»ºã€åˆ†å‰²å’Œæ¨¡æ€è½¬æ¢ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†QUAVEä¸å®æ•°å’Œå››å…ƒæ•°å€¼æ¨¡å‹çš„ç»„åˆã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºæ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œåœ¨æé«˜ç½‘ç»œæ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½å¤Ÿåœ¨å¤šç§åœºæ™¯ä¸­è¢«çµæ´»é‡‡ç”¨ï¼Œå¹¶å¯¹é¢†åŸŸåç§»å…·æœ‰é²æ£’æ€§ã€‚å®Œæ•´ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/ispamm/QWT">https://github.com/ispamm/QWT</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10224v5">PDF</a> Paper accepted to Neurocomputing Journal</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹ã€é€šç”¨ã€é’ˆå¯¹æ•°æ®å’Œä»»åŠ¡çš„æ— åè§çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿä»åŒ»å­¦å›¾åƒä¸­æå–é‡è¦ç‰¹å¾ã€‚æå‡ºçš„å››å…ƒå°æ³¢ç½‘ç»œï¼ˆQUAVEï¼‰å¯è½»æ¾é›†æˆåˆ°ä»»ä½•ç°æœ‰çš„åŒ»å­¦å›¾åƒåˆ†ææˆ–åˆæˆä»»åŠ¡ä¸­ï¼Œå¹¶å¯ä¸å®æ•°ã€å››å…ƒæ•°æˆ–è¶…å¤æ•°æ¨¡å‹ç»“åˆï¼Œå°†å…¶æ¨å¹¿åˆ°å•é€šé“æ•°æ®ã€‚QUAVEé¦–å…ˆé€šè¿‡å››å…ƒå°æ³¢å˜æ¢æå–ä¸åŒçš„å­å¸¦ï¼Œå¾—åˆ°ä½é¢‘&#x2F;è¿‘ä¼¼å¸¦å’Œé«˜é¢‘&#x2F;ç²¾ç»†ç‰¹å¾ï¼Œç„¶ååŠ æƒæœ€å…·ä»£è¡¨æ€§çš„å­å¸¦é›†ä½œä¸ºå…¶ä»–ç¥ç»ç½‘ç»œæ¨¡å‹å¤„ç†å›¾åƒçš„è¾“å…¥ï¼Œæ›¿ä»£æ ‡å‡†æ•°æ®æ ·æœ¬ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆä¸”é€šç”¨ï¼Œå¯æé«˜ç½‘ç»œæ€§èƒ½ï¼Œå¯çµæ´»åº”ç”¨äºå¤šç§åœºæ™¯ï¼Œå¯¹é¢†åŸŸå˜åŒ–å…·æœ‰é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œçš„é€šç”¨æ€§åœ¨åŒ»å­¦å›¾åƒå¤„ç†é¢†åŸŸå˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œç”±äºä¸åŒæ¥æºå’Œä»»åŠ¡çš„æ•°æ®é›†æ—¥ç›Šå¢å¤šã€‚</li>
<li>å¤„ç†åŒ»å­¦æ•°æ®æ—¶ç¼ºä¹æ–¹æ³•è®ºæ ‡å‡†å¯¼è‡´å¤§å˜åŒ–ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹é€šç”¨æ¡†æ¶â€”â€”å››å…ƒå°æ³¢ç½‘ç»œï¼ˆQUAVEï¼‰ï¼Œèƒ½å¤Ÿæå–åŒ»å­¦å›¾åƒçš„é‡è¦ç‰¹å¾ï¼Œå¯è½»æ¾é›†æˆåˆ°ç°æœ‰ä»»åŠ¡ä¸­ã€‚</li>
<li>QUAVEå¯è¿›è¡Œå››å…ƒå°æ³¢å˜æ¢ä»¥æå–å­å¸¦ï¼ŒåŒ…æ‹¬ä½é¢‘å’Œé«˜é¢‘ç‰¹å¾ã€‚</li>
<li>QUAVEå¯ä¸å…¶ä»–ç¥ç»ç½‘ç»œæ¨¡å‹ç»“åˆï¼Œç”¨äºå›¾åƒå¤„ç†å’Œæ›¿ä»£æ ‡å‡†æ•°æ®æ ·æœ¬ã€‚</li>
<li>é€šè¿‡å¹¿æ³›å®éªŒè¯„ä¼°ï¼Œè¯æ˜QUAVEæ¡†æ¶æœ‰æ•ˆä¸”é€šç”¨ï¼Œå¯æé«˜ç½‘ç»œæ€§èƒ½ï¼Œçµæ´»åº”ç”¨äºå¤šç§åœºæ™¯ï¼Œå¹¶å¯¹é¢†åŸŸå˜åŒ–å…·æœ‰é²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.10224">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-28e0b6c9113f4cca570fb2ed6e34e71d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12890453b76a9be8c8241dbe6a1e816f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8c36f47c381d93c7567615f0704074e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-673f0c3a64c7c432fe4edced0ddbf772.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dfae4cbfdbfd3364931dd18d7a9e0313.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-737428947398d503c531a7f872d3575e.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-25/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-25/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-26/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c61c772c5fd765f8c1f3570e98718fc8.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-26  NOVER Incentive Training for Language Models via Verifier-Free   Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-25/Speech/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5aa9a4d98f38fb6ecbd409bf9c96945f.jpg" class="responsive-img" alt="Speech">
                        
                        <span class="card-title">Speech</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-25  In-Context Learning Boosts Speech Recognition via Human-like Adaptation   to Speakers and Language Varieties
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                    Speech
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Speech/">
                        <span class="chip bg-color">Speech</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28879.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
