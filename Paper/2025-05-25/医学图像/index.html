<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-05-25  Evaluation and optimization of deep learning models for enhanced   detection of brain cancer using transmission optical microscopy of thin brain   tissue samples">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-10c20e5ebcfcbbe0109d82b703a8120c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    14.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    58 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-25-更新"><a href="#2025-05-25-更新" class="headerlink" title="2025-05-25 更新"></a>2025-05-25 更新</h1><h2 id="Evaluation-and-optimization-of-deep-learning-models-for-enhanced-detection-of-brain-cancer-using-transmission-optical-microscopy-of-thin-brain-tissue-samples"><a href="#Evaluation-and-optimization-of-deep-learning-models-for-enhanced-detection-of-brain-cancer-using-transmission-optical-microscopy-of-thin-brain-tissue-samples" class="headerlink" title="Evaluation and optimization of deep learning models for enhanced   detection of brain cancer using transmission optical microscopy of thin brain   tissue samples"></a>Evaluation and optimization of deep learning models for enhanced   detection of brain cancer using transmission optical microscopy of thin brain   tissue samples</h2><p><strong>Authors:Mohnish Sao, Mousa Alrubayan, Prabhakar Pradhan</strong></p>
<p>Optical transmission spectroscopy is one method to understand brain tissue structural properties from brain tissue biopsy samples, yet manual interpretation is resource intensive and prone to inter observer variability. Deep convolutional neural networks (CNNs) offer automated feature learning directly from raw brightfield images. Here, we evaluate ResNet50 and DenseNet121 on a curated dataset of 2,931 bright-field transmission optical microscopy images of thin brain tissue, split into 1,996 for training, 437 for validation, and 498 for testing. Our two stage transfer learning protocol involves initial training of a classifier head on frozen pretrained feature extractors, followed by fine tuning of deeper convolutional blocks with extensive data augmentation (rotations, flips, intensity jitter) and early stopping. DenseNet121 achieves 88.35 percent test accuracy, 0.9614 precision, 0.8667 recall, and 0.9116 F1 score the best performance compared to ResNet50 (82.12 percent, 0.9035, 0.8142, 0.8563). Detailed analysis of confusion matrices, training and validation curves, and classwise prediction distributions illustrates robust convergence and minimal bias. These findings demonstrate the superior generalization of dense connectivity on limited medical datasets and outline future directions for multi-class tumor grading and clinical translation. </p>
<blockquote>
<p>光学传输光谱法是一种通过脑组织活检样本了解脑组织结构性质的方法，但人工解读需要大量资源，且易出现观察者间差异。深度卷积神经网络（CNN）可以直接从原始明场图像中实现自动化特征学习。在此，我们对包含2931张脑组织薄片明场透射光学显微镜图像的定制数据集进行了ResNet50和DenseNet121的评估，其中1996张用于训练，437张用于验证，498张用于测试。我们的两阶段迁移学习协议涉及首先在冻结的预训练特征提取器上训练分类器头部，随后通过丰富的数据增强（旋转、翻转、强度抖动）进行深层卷积块的微调，并早期停止训练。DenseNet121达到了88.35%的测试准确率、0.9614的精确度、0.8667的召回率和0.9116的F1分数，与ResNet50相比表现出最佳性能（分别为82.12%、0.9035、0.8142和0.8563）。对混淆矩阵、训练和验证曲线以及类别预测分布的详细分析，证明了其稳健的收敛性和较低的偏见。这些发现表明密集连接在有限医疗数据集上的优越泛化能力，并指出了多类肿瘤分级和临床转化的未来研究方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11735v1">PDF</a> 10 pages, 5 figures</p>
<p><strong>Summary</strong><br>     深度学习中的卷积神经网络（CNN）可自动学习脑组织活检样本的结构特性，减轻人工解读的负担并减少观察者间的差异。本研究对ResNet50和DenseNet121网络在脑组织薄切片透射光学显微镜图像上进行评估，结果显示DenseNet121在测试集上的准确率、精确度、召回率和F1分数均高于ResNet50。这表明在有限的医学数据集中，密集连接网络具有更好的泛化能力，并为多类肿瘤分级和临床转化提供了方向。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Optical transmission spectroscopy可用于通过脑组织活检样本了解脑组织结构性特征。</li>
<li>手动解读这些特征资源消耗大且存在观察者间差异。</li>
<li>Deep convolutional neural networks (CNNs)能自动从原始明场图像中学习特征。</li>
<li>本研究对比了ResNet50和DenseNet121网络在脑组织薄切片图像数据集上的表现。</li>
<li>DenseNet121在准确率、精确度、召回率和F1分数等评估指标上表现更佳。</li>
<li>详细分析表明DenseNet121在有限医学数据集上具有较好的泛化能力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11735">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-736b31df4f8ccfb8023b1def31398409.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f41c0c47b5fff1a002e3bf704b07e69.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-945dc21300ba24d5b54a51a28e85f3b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2a48cd6105529db4575d10a05a59f91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1d6409e5eb1888eee6f00e5afd70392.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HWA-UNETR-Hierarchical-Window-Aggregate-UNETR-for-3D-Multimodal-Gastric-Lesion-Segmentation"><a href="#HWA-UNETR-Hierarchical-Window-Aggregate-UNETR-for-3D-Multimodal-Gastric-Lesion-Segmentation" class="headerlink" title="HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric   Lesion Segmentation"></a>HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric   Lesion Segmentation</h2><p><strong>Authors:Jiaming Liang, Lihuan Dai, Xiaoqi Sheng, Xiangguang Chen, Chun Yao, Guihua Tao, Qibin Leng, Hongmin Cai, Xi Zhong</strong></p>
<p>Multimodal medical image segmentation faces significant challenges in the context of gastric cancer lesion analysis. This clinical context is defined by the scarcity of independent multimodal datasets and the imperative to amalgamate inherently misaligned modalities. As a result, algorithms are constrained to train on approximate data and depend on application migration, leading to substantial resource expenditure and a potential decline in analysis accuracy. To address those challenges, we have made two major contributions: First, we publicly disseminate the GCM 2025 dataset, which serves as the first large-scale, open-source collection of gastric cancer multimodal MRI scans, featuring professionally annotated FS-T2W, CE-T1W, and ADC images from 500 patients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework that employs an original HWA block with learnable window aggregation layers to establish dynamic feature correspondences between different modalities’ anatomical structures, and leverages the innovative tri-orientated fusion mamba mechanism for context modeling and capturing long-range spatial dependencies. Extensive experiments on our GCM 2025 dataset and the publicly BraTS 2021 dataset validate the performance of our framework, demonstrating that the new approach surpasses existing methods by up to 1.68% in the Dice score while maintaining solid robustness. The dataset and code are public via <a target="_blank" rel="noopener" href="https://github.com/JeMing-creater/HWA-UNETR">https://github.com/JeMing-creater/HWA-UNETR</a>. </p>
<blockquote>
<p>在胃癌病灶分析的背景下，多模态医学图像分割面临着巨大的挑战。这种临床背景的特点是缺乏独立的多模态数据集，以及迫切需要融合内在不一致的模态。因此，算法受到训练数据近似性的限制，并依赖于应用迁移，导致资源消耗巨大，分析精度可能下降。为了应对这些挑战，我们做出了两大贡献：首先，我们公开传播了GCM 2025数据集，该数据集是首个大规模的、开源的胃癌多模态MRI扫描集合，包含来自500名患者的专业注释FS-T2W、CE-T1W和ADC图像。其次，我们介绍了HWA-UNETR，这是一种新型的3D分割框架，采用原始HWA块和可学习的窗口聚合层来建立不同模态解剖结构之间的动态特征对应关系，并利用创新的三角定向融合妈妈机制进行上下文建模和捕获长程空间依赖性。在我们的GCM 2025数据集和公开的BraTS 2021数据集上的广泛实验验证了我们的框架的性能，结果表明，新方法在Dice得分上比现有方法高出1.68%，同时保持了稳健性。数据集和代码可通过<a target="_blank" rel="noopener" href="https://github.com/JeMing-creater/HWA-UNETR%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/JeMing-creater/HWA-UNETR公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10464v2">PDF</a> This work has been provisionally accepted for MICCAI 2025</p>
<p><strong>摘要</strong><br>    胃癌病灶分析的多模态医学图像分割面临诸多挑战。缺乏独立多模态数据集以及需要对固有错配模态进行融合的临床环境，限制了算法的训练数据并依赖于应用迁移，导致了资源消耗大且分析精度可能下降。为应对这些挑战，我们做出了两大贡献：首先，我们公开发布了GCM 2025数据集，这是首个大规模公开的胃癌多模态MRI扫描数据集，包含来自500名患者的专业注释FS-T2W、CE-T1W和ADC图像；其次，我们引入了HWA-UNETR，这是一种新型的3D分割框架，采用可学习的窗口聚合层建立不同模态解剖结构之间的动态特征对应关系，并利用创新的三角融合mama机制进行上下文建模和捕获长期空间依赖性。在我们的GCM 2025数据集和公开的BraTS 2021数据集上的实验验证了该框架的性能，新方法的Dice得分高出现有方法高达1.68%，同时保持了稳健性。数据集和代码公开在<a target="_blank" rel="noopener" href="https://github.com/JeMing-creater/HWA-UNETR%E3%80%82">https://github.com/JeMing-creater/HWA-UNETR。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>多模态医学图像分割在胃癌病灶分析中存在挑战，缺乏独立多模态数据集和模态融合的需求是主要原因。</li>
<li>GCM 2025数据集的公开解决了这一问题，它包含了大量经过专业注释的胃癌多模态MRI扫描数据。</li>
<li>HWA-UNETR框架被引入以解决多模态医学图像分割的挑战，它采用可学习的窗口聚合层建立不同模态之间的动态特征对应关系。</li>
<li>HWA-UNETR框架在GCM 2025数据集和BraTS 2021数据集上的实验表现优异，相比现有方法Dice得分提高1.68%。</li>
<li>该框架具备强大的上下文建模能力，能够捕获长期空间依赖性。</li>
<li>HWA-UNETR框架具备稳健性，能够应对不同数据集的挑战。</li>
<li>数据集和代码已经公开，便于其他研究者使用和改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10464">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-fd8aba369a458b3135b43866e1660010.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85a41fcd5f13056645a134422a164310.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1044cfd5bc27eab37db31e98e9f8ceb7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ChestX-Reasoner-Advancing-Radiology-Foundation-Models-with-Reasoning-through-Step-by-Step-Verification"><a href="#ChestX-Reasoner-Advancing-Radiology-Foundation-Models-with-Reasoning-through-Step-by-Step-Verification" class="headerlink" title="ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning   through Step-by-Step Verification"></a>ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning   through Step-by-Step Verification</h2><p><strong>Authors:Ziqing Fan, Cheng Liang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie</strong></p>
<p>Recent advances in reasoning-enhanced large language models (LLMs) and multimodal LLMs (MLLMs) have significantly improved performance in complex tasks, yet medical AI models often overlook the structured reasoning processes inherent in clinical practice. In this work, we present ChestX-Reasoner, a radiology diagnosis MLLM designed to leverage process supervision mined directly from clinical reports, reflecting the step-by-step reasoning followed by radiologists. We construct a large dataset by extracting and refining reasoning chains from routine radiology reports. Our two-stage training framework combines supervised fine-tuning and reinforcement learning guided by process rewards to better align model reasoning with clinical standards. We introduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual question answering samples with 301K clinically validated reasoning steps, and propose RadRScore, a metric evaluating reasoning factuality, completeness, and effectiveness. ChestX-Reasoner outperforms existing medical and general-domain MLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%, and 18% improvements in reasoning ability compared to the best medical MLLM, the best general MLLM, and its base model, respectively, as well as 3.3%, 24%, and 27% improvements in outcome accuracy. All resources are open-sourced to facilitate further research in medical reasoning MLLMs. </p>
<blockquote>
<p>最近，在推理增强大型语言模型（LLMs）和多模态LLM（MLLMs）方面的进展，已经在复杂任务中显著提高性能。然而，医疗AI模型往往忽略了临床实践中的结构化推理过程。在这项工作中，我们提出了ChestX-Reasoner，这是一种设计用于利用直接从临床报告中挖掘的过程监督的放射学诊断MLLM，反映了放射医师遵循的逐步推理。我们通过从常规放射报告中提取和精炼推理链来构建大型数据集。我们的两阶段训练框架结合了监督微调与由过程奖励引导的自我强化学习，以更好地使模型推理与临床标准保持一致。我们介绍了RadRBench-CXR，这是一个包含59K视觉问答样本和301K临床验证推理步骤的综合基准测试，并提出了RadRScore，一个评估推理真实性、完整性和有效性的指标。ChestX-Reasoner在诊断和治疗准确度以及推理能力方面均优于现有的医疗和通用领域MLLMs。相较于最佳的医疗MLLM、通用MLLM及其基础模型，在推理能力方面分别提高了16%、5.9%和18%；在结果准确性方面分别提高了3.3%、24%和27%。所有资源均开源，以促进在医疗推理MLLM方面的进一步研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20930v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本工作提出了ChestX-Reasoner，这是一种结合临床报告中的推理过程监督设计的放射学诊断多模态大型语言模型（MLLM）。通过从常规放射报告中提取和精炼推理链，构建大型数据集。采用两阶段训练框架，结合监督微调与以过程奖励引导的强化学习，使模型推理更符合临床标准。同时引入了RadRBench-CXR基准测试和RadRScore评估指标，以评估推理的真实性、完整性和有效性。ChestX-Reasoner在诊断准确性和推理能力方面超越了现有的医学和通用MLLMs模型，实现了显著的提升并公开所有资源以促进医学推理MLLM的进一步研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ChestX-Reasoner是一个结合临床推理过程监督设计的放射学诊断多模态大型语言模型（MLLM）。</li>
<li>通过从常规放射报告中提取和精炼推理链构建大型数据集。</li>
<li>采用两阶段训练框架，结合监督微调与强化学习，使模型推理更符合临床标准。</li>
<li>引入了RadRBench-CXR基准测试和RadRScore评估指标。</li>
<li>ChestX-Reasoner在诊断准确性和推理能力方面表现出显著的提升。</li>
<li>与现有医学和通用MLLMs相比，ChestX-Reasoner有更高的诊断准确性和推理能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20930">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-37673b58a3def46386d5ead91762052e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-befb15e5263924b957ac4458cec0898e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de9651e1ccd442e2cdf58484bea1033e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf8cc1e29a8a6525ad2c40b98341e38d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Multiobjective-optimization-for-scattering-mitigation-and-scattering-screen-reconstruction-in-VLBI-observations-of-the-Galactic-Center"><a href="#Multiobjective-optimization-for-scattering-mitigation-and-scattering-screen-reconstruction-in-VLBI-observations-of-the-Galactic-Center" class="headerlink" title="Multiobjective optimization for scattering mitigation and scattering   screen reconstruction in VLBI observations of the Galactic Center"></a>Multiobjective optimization for scattering mitigation and scattering   screen reconstruction in VLBI observations of the Galactic Center</h2><p><strong>Authors:Alejandro Mus, Teresa Toscano, Hendrik Müller, Guang-Yao Zhao, Andrei Lobanov, Ciriaco Goddi</strong></p>
<p>Imaging reconstruction of interferometric data is a hard ill-posed inverse problem. Its difficulty is increased when observing the Galactic Center, which is obscured by a scattering screen. This is because the scattering breaks the one-to-one correspondence between images and visibilities. Solving the scattering problem is one of the biggest challenges in radio imaging of the Galactic Center. In this work we present a novel strategy to mitigate its effect and constrain the screen itself using multiobjective optimization. We exploit the potential of evolutionary algorithms to describe the optimization landscape to recover the intrinsic source structure and the scattering screen affecting the data. We successfully recover both the screen and the source in a wide range of simulated cases, including the speed of a moving screen at 230 GHz. Particularly, we can recover a ring structure in scattered data at 86 GHz. Our analysis demonstrates the huge potential that recent advancements in imaging and optimization algorithms offer to recover image structures, even in weakly constrained and degenerated, possibly multi-modal settings. The successful reconstruction of the scattering screen opens the window to event horizon scale works on the Galactic Center at 86G Hz up to 116 GHz, and the study of the scattering screen itself. </p>
<blockquote>
<p>成像重建干涉数据是一个难度较大的反问题。在观测银河系中心时难度更大，因为银河系中心被一个散射屏所遮蔽。散射破坏了图像和可见度之间的一一对应关系。解决散射问题是银河系中心射电成像面临的最大挑战之一。在这项工作中，我们提出了一种新的策略，利用多目标优化来缓解散射效应并约束散射屏本身。我们利用进化算法描述优化景观的潜力，以恢复影响数据的固有源结构和散射屏。我们在广泛的模拟案例中成功地恢复了屏幕和源，包括以230 GHz移动的屏幕速度。尤其值得一提的是，我们能够在散射数据中恢复环状结构（如在86 GHz的数据）。我们的分析证明了成像和优化算法的最新进展在恢复图像结构方面的巨大潜力，即使在弱约束和退化、可能是多模态的环境中也是如此。成功重建散射屏为在86 GHz至116 GHz上对银河系中心的事件视界规模工作打开了窗口，以及对散射屏本身的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16257v2">PDF</a> To appear in A&amp;A</p>
<p><strong>Summary</strong><br>     针对干涉数据的成像重建是一个病态的逆问题，尤其在对银河系中心进行观测时难度更大，因为散射破坏了图像与可见度之间的一一对应关系。本研究提出了一种新的策略，利用多目标优化来减轻散射的影响并约束散射屏本身。我们利用进化算法描述优化景观，以恢复影响数据的内在源结构和散射屏。在多种模拟情况下，我们成功恢复了屏幕和源，包括移动屏幕的速度（在230 GHz下）。特别是在86 GHz的散射数据中，我们成功恢复了环形结构。我们的分析展示了最新的成像和优化算法在恢复图像结构方面的巨大潜力，即使在弱约束和退化的可能多模态环境中也是如此。成功重建散射屏为在86 GHz至116 GHz上对银河系中心的事件视界规模工作打开了窗口，并对散射屏本身进行了研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>干涉数据成像重建是一个病态的逆问题，尤其在观测银河系中心时难度更大。</li>
<li>散射破坏了图像与可见度之间的一一对应关系，是射电成像中面临的最大挑战之一。</li>
<li>研究提出了一种新的策略，利用多目标优化减轻散射影响并约束散射屏。</li>
<li>通过进化算法描述优化景观，可以恢复内在源结构和影响数据的散射屏。</li>
<li>在多种模拟情况下成功恢复了屏幕和源，包括移动屏幕的速度。</li>
<li>成功在86 GHz的散射数据中恢复环形结构。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16257">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-eb100ad97f6e77a28396e7ac4809c25f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Explicit-and-Implicit-Representations-in-AI-based-3D-Reconstruction-for-Radiology-A-Systematic-Review"><a href="#Explicit-and-Implicit-Representations-in-AI-based-3D-Reconstruction-for-Radiology-A-Systematic-Review" class="headerlink" title="Explicit and Implicit Representations in AI-based 3D Reconstruction for   Radiology: A Systematic Review"></a>Explicit and Implicit Representations in AI-based 3D Reconstruction for   Radiology: A Systematic Review</h2><p><strong>Authors:Yuezhe Yang, Boyu Yang, Yaqian Wang, Yang He, Xingbo Dong, Zhe Jin</strong></p>
<p>The demand for high-quality medical imaging in clinical practice and assisted diagnosis has made 3D reconstruction in radiological imaging a key research focus. Artificial intelligence (AI) has emerged as a promising approach to enhancing reconstruction accuracy while reducing acquisition and processing time, thereby minimizing patient radiation exposure and discomfort and ultimately benefiting clinical diagnosis. This review explores state-of-the-art AI-based 3D reconstruction algorithms in radiological imaging, categorizing them into explicit and implicit approaches based on their underlying principles. Explicit methods include point-based, volume-based, and Gaussian representations, while implicit methods encompass implicit prior embedding and neural radiance fields. Additionally, we examine commonly used evaluation metrics and benchmark datasets. Finally, we discuss the current state of development, key challenges, and future research directions in this evolving field. Our project available on: <a target="_blank" rel="noopener" href="https://github.com/Bean-Young/AI4Radiology">https://github.com/Bean-Young/AI4Radiology</a>. </p>
<blockquote>
<p>临床实践和对辅助诊断的高质量医学成像的需求使得放射成像中的3D重建成为关键的研究重点。人工智能（AI）已成为一种有前途的方法，可以提高重建精度，同时减少采集和处理时间，从而最小化患者的辐射暴露和不适感，并最终有益于临床诊断。本文综述了基于人工智能的放射成像3D重建算法的最新进展，根据它们的基本原理将它们分为显式方法和隐式方法。显式方法包括点基、体积基和高斯表示法，而隐式方法包括隐式先验嵌入和神经辐射场。此外，我们还介绍了常用的评估指标和基准数据集。最后，我们讨论了该领域的当前开发状态、关键挑战和未来研究方向。我们的项目可用在：<a target="_blank" rel="noopener" href="https://github.com/Bean-Young/AI4Radiology">https://github.com/Bean-Young/AI4Radiology</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11349v2">PDF</a> 20 pages, 5 figures, submit to Medical Image Analysis</p>
<p><strong>Summary</strong></p>
<p>本文主要探讨了人工智能在医学影像学三维重建中的重要作用和应用。介绍了基于人工智能的三维重建算法的前沿进展，包括显式方法和隐式方法。显式方法主要包括点基、体积基和高斯表示法，隐式方法包括隐式先验嵌入和神经辐射场。文章还介绍了常用的评估指标和基准数据集，并讨论了当前的发展状况、关键挑战以及未来的研究方向。项目代码可通过链接访问：<a target="_blank" rel="noopener" href="https://github.com/Bean-Young/AI4Radiology">https://github.com/Bean-Young/AI4Radiology</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学影像学中三维重建的重要性及其在临床实践和辅助诊断中的需求。</li>
<li>人工智能在提升三维重建准确性、减少采集和处理时间方面的潜力。</li>
<li>人工智能在医学影像学三维重建中的显式方法和隐式方法，包括各自的优点和挑战。</li>
<li>常用的评估三维重建效果的评价指标和基准数据集。</li>
<li>当前医学影像学三维重建领域的发展状况。</li>
<li>面临的关键挑战，如算法复杂性、数据获取和处理难度等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11349">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b16f5fede7861ca900b1641bf2a5e72d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6da8a985ca41a23cad9da5757857cc5b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4e5e11e6f7e0c7e1876c1ef77c65cbd6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa4301441efc27c77d72e3c85818681b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-63ffd54979e1de6491aa82731437c742.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a709bd7d1025d630719384e375a1ff43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa53d811d79f6f89c0799ffa6b9f040b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Customized-SAM-2-for-Referring-Remote-Sensing-Image-Segmentation"><a href="#Customized-SAM-2-for-Referring-Remote-Sensing-Image-Segmentation" class="headerlink" title="Customized SAM 2 for Referring Remote Sensing Image Segmentation"></a>Customized SAM 2 for Referring Remote Sensing Image Segmentation</h2><p><strong>Authors:Fu Rong, Meng Lan, Qian Zhang, Lefei Zhang</strong></p>
<p>Referring Remote Sensing Image Segmentation (RRSIS) aims to segment target objects in remote sensing (RS) images based on textual descriptions. Although Segment Anything Model 2 (SAM 2) has shown remarkable performance in various segmentation tasks, its application to RRSIS presents several challenges, including understanding the text-described RS scenes and generating effective prompts from text descriptions. To address these issues, we propose RS2-SAM 2, a novel framework that adapts SAM 2 to RRSIS by aligning the adapted RS features and textual features, providing pseudo-mask-based dense prompts, and enforcing boundary constraints. Specifically, we first employ a union encoder to jointly encode the visual and textual inputs, generating aligned visual and text embeddings as well as multimodal class tokens. Then, we design a bidirectional hierarchical fusion module to adapt SAM 2 to RS scenes and align adapted visual features with the visually enhanced text embeddings, improving the model’s interpretation of text-described RS scenes. Additionally, a mask prompt generator is introduced to take the visual embeddings and class tokens as input and produce a pseudo-mask as the dense prompt of SAM 2. To further refine segmentation, we introduce a text-guided boundary loss to optimize segmentation boundaries by computing text-weighted gradient differences. Experimental results on several RRSIS benchmarks demonstrate that RS2-SAM 2 achieves state-of-the-art performance. </p>
<blockquote>
<p>远程遥感图像分割（RRSIS）旨在根据文本描述对遥感（RS）图像中的目标对象进行分割。尽管Segment Anything Model 2（SAM 2）在各种分割任务中表现出卓越的性能，但将其应用于RRSIS面临一些挑战，包括理解文本描述的RS场景和从文本描述中产生有效的提示。为了解决这些问题，我们提出了RS2-SAM 2，这是一个新型框架，通过适应SAM 2到RRSIS，通过对齐适应的RS特征和文本特征、提供基于伪掩码的密集提示和执行边界约束来适应RRSIS。具体来说，我们首先采用联合编码器对视觉和文本输入进行编码，生成对齐的视觉和文本嵌入以及多模式类别标记。然后，我们设计了一个双向层次融合模块，使SAM 2适应RS场景，并使适应后的视觉特征与增强的文本嵌入对齐，提高模型对文本描述的RS场景的解释能力。此外，我们还引入了一个掩膜提示生成器，以视觉嵌入和类别标记为输入，生成一个伪掩膜作为SAM 2的密集提示。为了进一步细化分割，我们引入了一种文本引导边界损失，通过计算文本加权的梯度差异来优化分割边界。在多个RRSIS基准测试上的实验结果表明，RS2-SAM 2达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07266v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于文本描述的遥感图像分割（RRSIS）旨在实现对遥感图像中目标对象的分割。尽管SAM 2模型在各种分割任务中表现出卓越的性能，但将其应用于RRSIS面临一些挑战，如理解文本描述的遥感场景和从文本描述中产生有效的提示。为解决这些问题，我们提出了RS2-SAM 2框架，该框架通过对齐遥感特征和文本特征、提供基于伪掩码的密集提示和执行边界约束，使SAM 2适应RRSIS。实验结果表明，RS2-SAM 2在多个RRSIS基准测试上达到了最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RS2-SAM 2框架旨在使SAM 2模型适应基于文本描述的遥感图像分割（RRSIS）。</li>
<li>RS2-SAM 2通过联合编码视觉和文本输入，生成对齐的视觉和文本嵌入以及多模态类令牌来解决理解文本描述的遥感场景和生成有效提示的挑战。</li>
<li>框架设计了一个双向层次融合模块，使SAM 2适应遥感场景，并将适应后的视觉特征与增强文本嵌入对齐。</li>
<li>引入了一个掩膜提示生成器，以视觉嵌入和类令牌为输入，生成伪掩膜作为SAM 2的密集提示。</li>
<li>为进一步优化分割边界，引入了文本引导边界损失，通过计算文本加权的梯度差异来优化分割边界。</li>
<li>实验结果表明，RS2-SAM 2在多个RRSIS基准测试上实现了最先进的性能。</li>
<li>RS2-SAM 2框架的应用为遥感图像分割提供了一种新的、有效的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07266">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d62e34b19803616f11e79d2e9427a398.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f130e21aea2c96b1328072c325532d11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-adce5752d05efa43e67cb3b0d4e419ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c422967dac52dde7539b963283463f58.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Liver-Cirrhosis-Stage-Estimation-from-MRI-with-Deep-Learning"><a href="#Liver-Cirrhosis-Stage-Estimation-from-MRI-with-Deep-Learning" class="headerlink" title="Liver Cirrhosis Stage Estimation from MRI with Deep Learning"></a>Liver Cirrhosis Stage Estimation from MRI with Deep Learning</h2><p><strong>Authors:Jun Zeng, Debesh Jha, Ertugrul Aktas, Elif Keles, Alpay Medetalibeyoglu, Matthew Antalek, Federica Proietto Salanitri, Amir A. Borhani, Daniela P. Ladner, Gorkem Durak, Ulas Bagci</strong></p>
<p>We present an end-to-end deep learning framework for automated liver cirrhosis stage estimation from multi-sequence MRI. Cirrhosis is the severe scarring (fibrosis) of the liver and a common endpoint of various chronic liver diseases. Early diagnosis is vital to prevent complications such as decompensation and cancer, which significantly decreases life expectancy. However, diagnosing cirrhosis in its early stages is challenging, and patients often present with life-threatening complications. Our approach integrates multi-scale feature learning with sequence-specific attention mechanisms to capture subtle tissue variations across cirrhosis progression stages. Using CirrMRI600+, a large-scale publicly available dataset of 628 high-resolution MRI scans from 339 patients, we demonstrate state-of-the-art performance in three-stage cirrhosis classification. Our best model achieves 72.8% accuracy on T1W and 63.8% on T2W sequences, significantly outperforming traditional radiomics-based approaches. Through extensive ablation studies, we show that our architecture effectively learns stage-specific imaging biomarkers. We establish new benchmarks for automated cirrhosis staging and provide insights for developing clinically applicable deep learning systems. The source code will be available at <a target="_blank" rel="noopener" href="https://github.com/JunZengz/CirrhosisStage">https://github.com/JunZengz/CirrhosisStage</a>. </p>
<blockquote>
<p>我们提出了一种端到端的深度学习框架，用于从多序列MRI自动估计肝硬化分期。肝硬化是肝脏的严重瘢痕（纤维化）形成，是多种慢性肝病的常见终点。早期诊断对于预防并发症（如失代偿和癌症）至关重要，这些并发症会显著降低预期寿命。然而，在肝硬化早期进行诊断具有挑战性，患者通常会出现危及生命的并发症。我们的方法将多尺度特征学习与序列特定的注意力机制相结合，以捕捉肝硬化进展阶段中细微的组织变化。我们使用CirrMRI600+数据集，该数据集包含来自339名患者的628个高分辨率MRI扫描，展示了在三期肝硬化分类中的最新性能。我们的最佳模型在T1W上达到了72.8%的准确率，在T2W序列上达到了63.8%的准确率，显著优于传统的基于放射组学的方法。通过广泛的消融研究，我们证明了我们的架构有效地学习了与阶段相关的成像生物标志物。我们为自动化肝硬化分期设定了新的基准，并为开发临床适用的深度学习系统提供了见解。源代码将在<a target="_blank" rel="noopener" href="https://github.com/JunZengz/CirrhosisStage%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/JunZengz/CirrhosisStage上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18225v3">PDF</a> 7 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个基于深度学习的端到端框架，用于从多序列MRI中自动估计肝硬化分期。该研究利用大规模公开数据集CirrMRI600+进行三阶段肝硬化分类，并实现了先进性能，最佳模型在T1W和T2W序列上的准确率分别为72.8%和63.8%，显著优于传统基于放射学的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究采用深度学习方法进行肝硬化分期的自动估计。</li>
<li>利用多序列MRI数据进行肝硬化分期。</li>
<li>使用大规模公开数据集CirrMRI600+进行实验研究。</li>
<li>最佳模型在T1W和T2W序列上的准确率分别为72.8%和63.8%。</li>
<li>深度学习方法显著优于传统基于放射学的方法。</li>
<li>通过消融研究，证明该方法能有效学习阶段特定的成像生物标志物。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18225">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-08ae028fd11d989facee11595f1cdfa6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cbb318ce984e5c89aa445cbf732de46.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23545de6d1078c7074ace886ff439475.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9967fd34377a496082d0c076488b76ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbee84e5642f0b2987439026f043e195.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7916a88300ab2b9a596a938f06e9a4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0055112c5d88633b2521b61d989402fb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b312c7c9c90e02dc480d0442ecd69015.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Topology-Design-of-Reconfigurable-Intelligent-Surfaces-Based-on-Current-Distribution-and-Otsu-Image-Segmentation"><a href="#Topology-Design-of-Reconfigurable-Intelligent-Surfaces-Based-on-Current-Distribution-and-Otsu-Image-Segmentation" class="headerlink" title="Topology Design of Reconfigurable Intelligent Surfaces Based on Current   Distribution and Otsu Image Segmentation"></a>Topology Design of Reconfigurable Intelligent Surfaces Based on Current   Distribution and Otsu Image Segmentation</h2><p><strong>Authors:Zhen Zhang, Jun Wei Zhang, Hui Dong Li, Junhui Qiu, Lijie Wu, Wan Wan Cao, Ren Wang, Jia Nan Zhang, Qiang Cheng</strong></p>
<p>Miniaturization of reconffgurable intelligent surface RIS) elements is a crucial trend in the development of RISs. It not only facilitates the attainment of multifunctional integration but also promotes seamless amalgamation with other elements. The current on the RIS element plays a crucial role in determining the characteristics of the induced electromagnetic ffeld components. Segments with high current intensity determine the performance of RIS elements. Carving the parts with strong current distribution density into the metal patch of RIS element structure can achieve miniaturization. Based on this insight, this work proposes a topology design method that leverages current distribution and image processing techniques to achieve efffcient miniaturization of the RIS elements. In this proposed method, we ffrst obtain the current distribution across different operational states and the period of the working frequency. Next, we employ the Otsu image segmentation method to extract relevant image information from the current distribution images of the RIS elements. Subsequently, we utilize linear mapping techniques to convert this image information into the structure of RIS elements. Then, based on the structure of the RIS elements, the Quasi-Newton optimization algorithm is utilized to obtain the parameters of the tunable device that correspond to various operational states. As a result, we successfully construct the structural topology of the RIS elements based on their current distribution, designing areas with strong current distribution as metal patches. To validate the performance of the proposed method, a 16 by 16 3-bit RIS was developed, fabricated and measured. Compared with existing RIS designs, the proportion of the top-layer metal patches is smaller, which provides the possibility for integrating other functions and devices. </p>
<blockquote>
<p>可重构智能表面（RIS）元素的微型化是RIS发展的重要趋势。它不仅有利于实现多功能集成，而且促进了与其他元素的无缝融合。RIS元件上的电流在决定感应电磁场组件的特性方面起着至关重要的作用。高电流强度的区域决定了RIS元件的性能。通过将电流分布密度较大的部分雕刻成RIS元件结构的金属贴片，可以实现微型化。基于这一见解，这项工作提出了一种利用电流分布和图像处理技术来实现RIS元素高效微型化的拓扑设计方法。在该方法中，我们首先获得不同工作状态下和工作频率周期的电流分布。接下来，我们采用Otsu图像分割方法从RIS元件的电流分布图像中提取相关图像信息。然后，我们利用线性映射技术将图像信息转换为RIS元件的结构。随后，基于RIS元件的结构，利用拟牛顿优化算法获得对应于各种工作状态的可调设备的参数。因此，我们成功构建了基于电流分布的RIS元素结构拓扑，设计电流分布强烈的区域作为金属贴片。为了验证所提方法的性能，开发、制作并测量了一个16x16 3位RIS。与现有的RIS设计相比，顶层金属贴片的比例较小，这为集成其他功能和设备提供了可能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18067v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了可重构智能表面（RIS）元素的微型化趋势，指出微型化有助于实现多功能集成和与其他元素的无缝融合。文章提出了基于电流分布和图像处理技术的拓扑设计方法，以实现RIS元素的高效微型化。该方法首先获取不同工作状态下和工作频率周期的电流分布，然后采用Otsu图像分割方法提取RIS元素的电流分布图像的相关信息，再利用线性映射技术将图像信息转换为RIS元素的结构。最后，基于Quasi-Newton优化算法获取与各种工作状态相对应的可调设备的参数。研究结果表明，该方法成功构建了基于电流分布的RIS元素结构拓扑，并验证了所提出方法的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>可重构智能表面（RIS）元素的微型化是发展的关键趋势，有助于实现多功能集成和与其他元素的融合。</li>
<li>电流分布在决定RIS元素特性方面起着关键作用，高电流强度区域影响RIS元素性能。</li>
<li>通过在金属贴片结构中精细雕刻强电流分布的部分来实现微型化。</li>
<li>提出了一种基于电流分布和图像处理技术的拓扑设计方法。</li>
<li>该方法涉及获取不同工作状态和工作频率周期的电流分布，并采用Otsu图像分割法提取相关信息。</li>
<li>使用线性映射技术和Quasi-Newton优化算法将图像信息转换为RIS元素的结构并获取可调设备参数。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18067">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6287be051a8a68044b266607ca52378f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af7ae939d10cfe76eaee41d44a5df3f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a911e98c8c02930310a6ab4da2dec363.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a57277aef6f63581fca74598d9ea9d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10c20e5ebcfcbbe0109d82b703a8120c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Mask-of-truth-model-sensitivity-to-unexpected-regions-of-medical-images"><a href="#Mask-of-truth-model-sensitivity-to-unexpected-regions-of-medical-images" class="headerlink" title="Mask of truth: model sensitivity to unexpected regions of medical images"></a>Mask of truth: model sensitivity to unexpected regions of medical images</h2><p><strong>Authors:Théo Sourget, Michelle Hestbek-Møller, Amelia Jiménez-Sánchez, Jack Junchi Xu, Veronika Cheplygina</strong></p>
<p>The development of larger models for medical image analysis has led to increased performance. However, it also affected our ability to explain and validate model decisions. Models can use non-relevant parts of images, also called spurious correlations or shortcuts, to obtain high performance on benchmark datasets but fail in real-world scenarios. In this work, we challenge the capacity of convolutional neural networks (CNN) to classify chest X-rays and eye fundus images while masking out clinically relevant parts of the image. We show that all models trained on the PadChest dataset, irrespective of the masking strategy, are able to obtain an Area Under the Curve (AUC) above random. Moreover, the models trained on full images obtain good performance on images without the region of interest (ROI), even superior to the one obtained on images only containing the ROI. We also reveal a possible spurious correlation in the Chaksu dataset while the performances are more aligned with the expectation of an unbiased model. We go beyond the performance analysis with the usage of the explainability method SHAP and the analysis of embeddings. We asked a radiology resident to interpret chest X-rays under different masking to complement our findings with clinical knowledge. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/TheoSourget/MMC_Masking">https://github.com/TheoSourget/MMC_Masking</a> and <a target="_blank" rel="noopener" href="https://github.com/TheoSourget/MMC_Masking_EyeFundus">https://github.com/TheoSourget/MMC_Masking_EyeFundus</a> </p>
<blockquote>
<p>开发用于医学图像分析的大型模型已经提高了性能。然而，这也影响了我们解释和验证模型决策的能力。模型可能会使用与图像不相关的部分（也称为偶然关联或捷径）来获得基准数据集上的高性能，但在现实世界的场景中却会失败。在这项工作中，我们通过遮挡医学上重要的图像部分来挑战卷积神经网络（CNN）对胸部X射线和眼底图像的分类能力。我们显示，在PadChest数据集上训练的所有模型，无论采用何种遮挡策略，都能获得高于随机的曲线下面积（AUC）。此外，在完整图像上训练的模型在没有感兴趣区域（ROI）的图像上也能获得良好的性能，甚至优于仅在包含ROI的图像上获得的性能。我们还揭示了Chaksu数据集中可能存在的偶然关联，同时其性能更符合无偏见模型的预期。除了性能分析，我们还使用了SHAP解释方法和嵌入分析。我们邀请了一位放射科住院医师在不同遮挡条件下解读胸部X射线，以结合我们的临床知识发现。我们的代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/TheoSourget/MMC_Masking">https://github.com/TheoSourget/MMC_Masking</a> 和 <a target="_blank" rel="noopener" href="https://github.com/TheoSourget/MMC_Masking_EyeFundus">https://github.com/TheoSourget/MMC_Masking_EyeFundus</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04030v3">PDF</a> Updated after publication in the Journal of Imaging Informatics in   Medicine</p>
<p><strong>Summary</strong><br>     医学图像分析模型发展提升性能，但难以解释和验证决策。模型可能利用图像的非关键部分获得高基准数据集性能，但在真实场景中失败。研究挑战卷积神经网络对胸部X光和眼底图像的分类能力，同时掩盖临床上关键部分。模型在无关区域上表现良好，甚至优于仅包含ROI的图像。代码公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分析模型性能提升同时，存在解释和验证决策困难的问题。</li>
<li>模型可能利用图像的非相关部分（即偶然关联或捷径）获得高基准数据集性能，但在真实场景中应用时可能失败。</li>
<li>研究评估了卷积神经网络对胸部X光和眼底图像分类的能力，并在掩盖临床关键部分的情况下进行了测试。</li>
<li>所有在PadChest数据集上训练的模型，无论采用何种掩盖策略，都能获得超过随机的AUC性能。</li>
<li>在无关区域上训练的模型表现出良好的性能，甚至在某些情况下优于仅包含ROI的图像。</li>
<li>研究发现了Chaksu数据集中的一种可能的偶然关联。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04030">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b29058033cf118f62d77ad1561b3b218.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9fc2600e79b34d06b4f7fccc7b884b3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-50b0ac347bfbe4d0abc9fe8d4bcc7a2a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="KAN-Mamba-FusionNet-Redefining-Medical-Image-Segmentation-with-Non-Linear-Modeling"><a href="#KAN-Mamba-FusionNet-Redefining-Medical-Image-Segmentation-with-Non-Linear-Modeling" class="headerlink" title="KAN-Mamba FusionNet: Redefining Medical Image Segmentation with   Non-Linear Modeling"></a>KAN-Mamba FusionNet: Redefining Medical Image Segmentation with   Non-Linear Modeling</h2><p><strong>Authors:Akansh Agrawal, Akshan Agrawal, Shashwat Gupta, Priyanka Bagade</strong></p>
<p>Medical image segmentation is essential for applications like robotic surgeries, disease diagnosis, and treatment planning. Recently, various deep-learning models have been proposed to enhance medical image segmentation. One promising approach utilizes Kolmogorov-Arnold Networks (KANs), which better capture non-linearity in input data. However, they are unable to effectively capture long-range dependencies, which are required to accurately segment complex medical images and, by that, improve diagnostic accuracy in clinical settings. Neural networks such as Mamba can handle long-range dependencies. However, they have a limited ability to accurately capture non-linearities in the images as compared to KANs. Thus, we propose a novel architecture, the KAN-Mamba FusionNet, which improves segmentation accuracy by effectively capturing the non-linearities from input and handling long-range dependencies with the newly proposed KAMBA block. We evaluated the proposed KAN-Mamba FusionNet on three distinct medical image segmentation datasets: BUSI, Kvasir-Seg, and GlaS - and found it consistently outperforms state-of-the-art methods in IoU and F1 scores. Further, we examined the effects of various components and assessed their contributions to the overall model performance via ablation studies. The findings highlight the effectiveness of this methodology for reliable medical image segmentation, providing a unique approach to address intricate visual data issues in healthcare. </p>
<blockquote>
<p>医学图像分割对于机器人手术、疾病诊断和治疗计划等应用至关重要。最近，为了提升医学图像分割的效果，已经提出了各种深度学习模型。一种有前途的方法是利用Kolmogorov-Arnold网络（KANs），它能更好地捕捉输入数据中的非线性。然而，它们无法有效地捕捉长距离依赖关系，这对于准确分割复杂的医学图像以及在临床环境中提高诊断准确性是必要的。神经网络（如Mamba）能够处理长距离依赖关系，但与KANs相比，它们在准确捕捉图像非线性方面的能力有限。因此，我们提出了一种新型架构，即KAN-Mamba FusionNet，它通过有效捕捉输入的非线性和使用新提出的KAMBA块来处理长距离依赖关系，提高了分割准确性。我们在三个不同的医学图像分割数据集上评估了所提出的KAN-Mamba FusionNet：BUSI、Kvasir-Seg和GlaS，发现它在IoU和F1分数上均优于最先进的方法。此外，我们研究了各种组件的影响，并通过消融研究评估了它们对整体模型性能的贡献。研究结果表明，该方法在可靠的医学图像分割方面的有效性，为医疗保健中复杂的视觉数据问题提供了独特的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11926v2">PDF</a> 11 pages, 2 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>基于Kolmogorov-Arnold网络（KANs）和Mamba神经网络的优势，提出了一种新的医学图像分割网络架构——KAN-Mamba FusionNet。该架构融合了KANs对非线性的捕捉能力和Mamba对长距离依赖的处理能力，通过新提出的KAMBA模块，提高了分割准确度和诊断准确性。在多个数据集上的评估证明了其表现优于其他前沿方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>医学图像分割在机器人手术、疾病诊断和治疗计划等应用中至关重要。</li>
<li>最近提出的深度学习模型如Kolmogorov-Arnold网络（KANs）和Mamba神经网络在医学图像分割中有潜力。</li>
<li>KANs能够更好地捕捉输入数据的非线性，但无法有效捕捉长距离依赖关系。</li>
<li>Mamba神经网络能够处理长距离依赖关系，但在捕捉图像非线性方面能力有限。</li>
<li>提出的KAN-Mamba FusionNet结合了KANs和Mamba的优势，通过KAMBA模块提高了分割准确性。</li>
<li>在三个不同的医学图像分割数据集上评估，KAN-Mamba FusionNet的表现优于其他最新方法。</li>
<li>通过消融研究，证实了架构中各个组件的有效性及其对模型性能的贡献。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.11926">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d4112361f3a638fd902c354b976ddd5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f1e39e8e5033e0b4334778b14531e91.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Local-Clustering-for-Lung-Cancer-Image-Classification-via-Sparse-Solution-Technique"><a href="#Local-Clustering-for-Lung-Cancer-Image-Classification-via-Sparse-Solution-Technique" class="headerlink" title="Local Clustering for Lung Cancer Image Classification via Sparse   Solution Technique"></a>Local Clustering for Lung Cancer Image Classification via Sparse   Solution Technique</h2><p><strong>Authors:Jackson Hamel, Ming-Jun Lai, Zhaiming Shen, Ye Tian</strong></p>
<p>In this work, we propose to use a local clustering approach based on the sparse solution technique to study the medical image, especially the lung cancer image classification task. We view images as the vertices in a weighted graph and the similarity between a pair of images as the edges in the graph. The vertices within the same cluster can be assumed to share similar features and properties, thus making the applications of graph clustering techniques very useful for image classification. Recently, the approach based on the sparse solutions of linear systems for graph clustering has been found to identify clusters more efficiently than traditional clustering methods such as spectral clustering. We propose to use the two newly developed local clustering methods based on sparse solution of linear system for image classification. In addition, we employ a box spline-based tight-wavelet-framelet method to clean these images and help build a better adjacency matrix before clustering. The performance of our methods is shown to be very effective in classifying images. Our approach is significantly more efficient and either favorable or equally effective compared with other state-of-the-art approaches. Finally, we shall make a remark by pointing out two image deformation methods to build up more artificial image data to increase the number of labeled images. </p>
<blockquote>
<p>在这项工作中，我们提出了一种基于稀疏解技术的局部聚类方法，用于医学图像的研究，特别是在肺癌图像分类任务中的应用。我们将图像视为加权图中的顶点，将图像之间的相似性视为图中的边。同一聚类中的顶点可以假定具有相似的特征和属性，这使得图聚类技术在图像分类中的应用非常有用。最近发现，基于线性系统稀疏解的聚类方法比传统的聚类方法（如谱聚类）更能有效地识别聚类。我们建议使用两种新开发的基于线性系统稀疏解的局部聚类方法进行图像分类。此外，我们还采用基于箱线紧小波框架的方法对图像进行清理，帮助在聚类之前建立更好的邻接矩阵。我们的方法在图像分类方面的表现非常有效，与其他最新方法相比，我们的方法更有效率，要么具有优势，要么同样有效。最后，我们应该指出两种图像变形方法来建立更多的人工图像数据，以增加标记图像的数量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.08800v2">PDF</a> </p>
<p><strong>Summary</strong><br>     本研究提出利用基于稀疏解技术的局部聚类方法，针对医学图像尤其是肺癌图像分类任务进行研究。将图像视为加权图中的顶点，图像间的相似性视为图中的边。同一聚类中的顶点可视为具有相似特征和属性，因此图聚类技术对于图像分类非常有用。基于线性系统稀疏解的图聚类方法能更有效地识别聚类。此外，还采用了基于盒状样条的紧小波框架方法对图像进行清洗，帮助建立更好的邻接矩阵进行聚类。该方法在图像分类方面表现出非常有效的性能，与其他最新方法相比，具有更高的效率，要么优越，要么相当。最后提出了两种生成更多人工图像数据的方法以增加标注图像的数量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究采用基于稀疏解技术的局部聚类方法用于医学图像分类，特别是肺癌图像分类。</li>
<li>将图像视为加权图中的顶点，图像间的相似性视为图中的边，应用图聚类技术进行分类。</li>
<li>基于线性系统稀疏解的图聚类方法能更有效地识别聚类，提高图像分类性能。</li>
<li>采用盒状样条的紧小波框架方法对图像进行清洗，优化邻接矩阵的建立。</li>
<li>该方法相较于其他最新方法具有显著优势，表现为高效率、优越或相当有效。</li>
<li>提出了通过图像变形方法生成更多人工图像数据以增加标注图像数量的策略。</li>
<li>该研究为医学图像分类提供了新的视角和技术手段。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.08800">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-eb02e43f099b1ec0b4ebd27881430a00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a271262fb3731e13b6499f827844169.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07a5dd90390bb86a27ba8ca0e9a9379b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-458f7cd770c6b753c5e38b60d44de7b1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DEFT-Efficient-Fine-Tuning-of-Diffusion-Models-by-Learning-the-Generalised-h-transform"><a href="#DEFT-Efficient-Fine-Tuning-of-Diffusion-Models-by-Learning-the-Generalised-h-transform" class="headerlink" title="DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the   Generalised $h$-transform"></a>DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the   Generalised $h$-transform</h2><p><strong>Authors:Alexander Denker, Francisco Vargas, Shreyas Padhy, Kieran Didi, Simon Mathis, Vincent Dutordoir, Riccardo Barbano, Emile Mathieu, Urszula Julia Komorowska, Pietro Lio</strong></p>
<p>Generative modelling paradigms based on denoising diffusion processes have emerged as a leading candidate for conditional sampling in inverse problems. In many real-world applications, we often have access to large, expensively trained unconditional diffusion models, which we aim to exploit for improving conditional sampling. Most recent approaches are motivated heuristically and lack a unifying framework, obscuring connections between them. Further, they often suffer from issues such as being very sensitive to hyperparameters, being expensive to train or needing access to weights hidden behind a closed API. In this work, we unify conditional training and sampling using the mathematically well-understood Doob’s h-transform. This new perspective allows us to unify many existing methods under a common umbrella. Under this framework, we propose DEFT (Doob’s h-transform Efficient FineTuning), a new approach for conditional generation that simply fine-tunes a very small network to quickly learn the conditional $h$-transform, while keeping the larger unconditional network unchanged. DEFT is much faster than existing baselines while achieving state-of-the-art performance across a variety of linear and non-linear benchmarks. On image reconstruction tasks, we achieve speedups of up to 1.6$\times$, while having the best perceptual quality on natural images and reconstruction performance on medical images. Further, we also provide initial experiments on protein motif scaffolding and outperform reconstruction guidance methods. </p>
<blockquote>
<p>基于去噪扩散过程的生成建模范式已成为逆问题中条件采样的领先候选方法。在许多实际应用中，我们通常可以访问大型且经过昂贵训练的无条件扩散模型，我们旨在利用这些模型来改善条件采样。最近的方法大多是启发式且缺乏统一框架，导致它们之间的联系模糊。此外，它们经常面临诸如对超参数非常敏感、训练成本高昂或需要访问封闭API后面的权重等问题。在这项工作中，我们使用数学上理解良好的Doob的h转换来统一条件训练和采样。这个新视角允许我们将许多现有方法纳入同一框架下。在这个框架下，我们提出了DEFT（Doob的h转换高效微调），这是一种新的条件生成方法，它只需微调一个非常小的网络，即可快速学习条件h转换，同时保持较大的无条件网络不变。DEFT相比现有基准测试速度更快，同时在各种线性和非线性基准测试中实现了最先进的性能。在图像重建任务上，我们实现了最高达1.6倍的加速，同时在自然图像上获得最佳的感知质量和在医学图像上的重建性能。此外，我们还进行了初步的蛋白质基序支架实验，并在重建指导方法上表现出色。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01781v5">PDF</a> arXiv admin note: text overlap with arXiv:2312.09236</p>
<p><strong>Summary</strong><br>     基于去噪扩散过程的生成建模范式已成为解决反问题中条件采样的领先候选方法。本文利用数学上理解良好的Doob的h-变换统一了条件训练和采样。在此基础上，提出了DEFT（Doob的h-变换高效微调）这一新的条件生成方法，只需微调一个非常小的网络即可快速学习条件h-变换，同时保持较大的无条件网络不变。DEFT在多种线性和非线性基准测试上实现了最先进的性能，在图像重建任务上实现了最高达1.6倍的速度提升，同时在自然图像上具有最佳的感知质量和医疗图像的重建性能。此外，还在蛋白质基序支架方面进行了初步实验，并超越了重建指导方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成建模范式在解决反问题的条件采样中占据领先地位，其中基于去噪扩散过程的方法受到关注。</li>
<li>现有方法缺乏统一框架，且存在超参数敏感、训练成本高昂、依赖封闭API等问题。</li>
<li>引入Doob的h-变换来统一条件训练和采样，提供了一个新的视角。</li>
<li>提出DEFT方法，通过微调小网络快速学习条件h-变换，保持大网络不变，实现快速且高质量的条件生成。</li>
<li>DEFT在图像重建任务上实现了显著的速度提升和感知质量改进。</li>
<li>DEFT在医疗图像重建方面表现出优异的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.01781">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3bcfe49d7ffed4a06f10a0a8dd981ec2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-089cd1947b9623c22b4d8ad4cb1c4339.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Breast-Cancer-Classification-Using-Gradient-Boosting-Algorithms-Focusing-on-Reducing-the-False-Negative-and-SHAP-for-Explainability"><a href="#Breast-Cancer-Classification-Using-Gradient-Boosting-Algorithms-Focusing-on-Reducing-the-False-Negative-and-SHAP-for-Explainability" class="headerlink" title="Breast Cancer Classification Using Gradient Boosting Algorithms Focusing   on Reducing the False Negative and SHAP for Explainability"></a>Breast Cancer Classification Using Gradient Boosting Algorithms Focusing   on Reducing the False Negative and SHAP for Explainability</h2><p><strong>Authors:João Manoel Herrera Pinheiro, Marcelo Becker</strong></p>
<p>Cancer is one of the diseases that kill the most women in the world, with breast cancer being responsible for the highest number of cancer cases and consequently deaths. However, it can be prevented by early detection and, consequently, early treatment. Any development for detection or perdition this kind of cancer is important for a better healthy life. Many studies focus on a model with high accuracy in cancer prediction, but sometimes accuracy alone may not always be a reliable metric. This study implies an investigative approach to studying the performance of different machine learning algorithms based on boosting to predict breast cancer focusing on the recall metric. Boosting machine learning algorithms has been proven to be an effective tool for detecting medical diseases. The dataset of the University of California, Irvine (UCI) repository has been utilized to train and test the model classifier that contains their attributes. The main objective of this study is to use state-of-the-art boosting algorithms such as AdaBoost, XGBoost, CatBoost and LightGBM to predict and diagnose breast cancer and to find the most effective metric regarding recall, ROC-AUC, and confusion matrix. Furthermore, our study is the first to use these four boosting algorithms with Optuna, a library for hyperparameter optimization, and the SHAP method to improve the interpretability of our model, which can be used as a support to identify and predict breast cancer. We were able to improve AUC or recall for all the models and reduce the False Negative for AdaBoost and LigthGBM the final AUC were more than 99.41% for all models. </p>
<blockquote>
<p>癌症是世界上导致女性死亡的主要疾病之一，其中乳腺癌病例数和死亡人数均居首位。然而，通过早期发现和早期治疗，癌症是可以预防的。任何有助于检测或预防这类癌症的发展对于更好地生活具有重要意义。许多研究集中在开发具有高精度预测癌症的模型上，但有时仅依靠准确性可能并不是一个可靠的指标。本研究采用一种基于boosting的机器学习方法来研究预测乳腺癌的性能，重点研究召回率指标。已有研究证明，使用boosting算法的机器学习是检测医学疾病的有效工具。本研究使用了加利福尼亚大学欧文分校（UCI）存储库的数据集来训练和测试分类模型，该模型包含了各种属性。本研究的主要目标是使用最新的boosting算法（如AdaBoost、XGBoost、CatBoost和LightGBM）来预测和诊断乳腺癌，并找到关于召回率、ROC-AUC和混淆矩阵的最有效指标。此外，我们的研究是首次将这四个boosting算法与用于超参数优化的Optuna库以及SHAP方法相结合，以提高我们模型的解释性，可作为检测和预测乳腺癌的辅助工具。我们成功提高了所有模型的AUC或召回率，并降低了AdaBoost和LightGBM的假阴性结果。所有模型的最终AUC均超过99.41%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.09548v3">PDF</a> 9 pages, 16 figures</p>
<p><strong>Summary</strong></p>
<p>本文研究了利用机器学习算法预测乳腺癌的问题。研究通过使用AdaBoost、XGBoost、CatBoost和LightGBM等先进算法进行训练和测试模型分类器，并利用数据集找出最有效的评价指标关于召回率、ROC-AUC和混淆矩阵。该研究首次结合Optuna库进行超参数优化和SHAP方法改善模型的可解释性，从而有效改善乳腺癌的诊断与预测效果。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>乳腺癌是世界范围内导致女性死亡的主要疾病之一，早期检测和预防是关键。</li>
<li>仅依赖准确性作为评估指标可能不足够可靠，因此本研究关注召回率等评价指标。</li>
<li>研究使用了加州大学欧文分校（UCI）的数据集进行模型训练和测试。</li>
<li>AdaBoost、XGBoost、CatBoost和LightGBM等算法被应用于预测和诊断乳腺癌。</li>
<li>结合Optuna库进行超参数优化，并使用SHAP方法提高模型的可解释性。</li>
<li>所有模型的AUC或召回率均有所提升，AdaBoost和LightGBM的假阴性结果有所降低。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.09548">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3b50b0b930d513f3420df80c4a766ca9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4e19406f0596642cb77bdff72a220c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5a54d231a3ec331cbcf7c471bf5f7d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4770fc034555293478c5f5e5161d5388.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73b8f5e503d8bacb43ae6c2a2d74079e.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Generalizing-Medical-Image-Representations-via-Quaternion-Wavelet-Networks"><a href="#Generalizing-Medical-Image-Representations-via-Quaternion-Wavelet-Networks" class="headerlink" title="Generalizing Medical Image Representations via Quaternion Wavelet   Networks"></a>Generalizing Medical Image Representations via Quaternion Wavelet   Networks</h2><p><strong>Authors:Luigi Sigillo, Eleonora Grassucci, Aurelio Uncini, Danilo Comminiello</strong></p>
<p>Neural network generalizability is becoming a broad research field due to the increasing availability of datasets from different sources and for various tasks. This issue is even wider when processing medical data, where a lack of methodological standards causes large variations being provided by different imaging centers or acquired with various devices and cofactors. To overcome these limitations, we introduce a novel, generalizable, data- and task-agnostic framework able to extract salient features from medical images. The proposed quaternion wavelet network (QUAVE) can be easily integrated with any pre-existing medical image analysis or synthesis task, and it can be involved with real, quaternion, or hypercomplex-valued models, generalizing their adoption to single-channel data. QUAVE first extracts different sub-bands through the quaternion wavelet transform, resulting in both low-frequency&#x2F;approximation bands and high-frequency&#x2F;fine-grained features. Then, it weighs the most representative set of sub-bands to be involved as input to any other neural model for image processing, replacing standard data samples. We conduct an extensive experimental evaluation comprising different datasets, diverse image analysis, and synthesis tasks including reconstruction, segmentation, and modality translation. We also evaluate QUAVE in combination with both real and quaternion-valued models. Results demonstrate the effectiveness and the generalizability of the proposed framework that improves network performance while being flexible to be adopted in manifold scenarios and robust to domain shifts. The full code is available at: <a target="_blank" rel="noopener" href="https://github.com/ispamm/QWT">https://github.com/ispamm/QWT</a>. </p>
<blockquote>
<p>神经网络泛化性因不同来源和任务的数据集日益增多而成为一个广泛的研究领域。在处理医疗数据时，这个问题更为突出，因为缺乏方法论标准导致由不同成像中心或通过各种设备和辅助因素获得的数据存在很大差异。为了克服这些局限性，我们引入了一种新型、可泛化的、独立于数据和任务的分析框架，能够从医学图像中提取重要特征。所提出的四元小波网络（QUAVE）可以轻松地与任何现有的医学图像分析或合成任务集成，并且可以涉及实数、四元数或超复数模型，将其推广到单通道数据。QUAVE首先通过四元小波变换提取不同的子带，得到低频&#x2F;近似带和高频&#x2F;精细特征。然后，它权衡最具代表性的子带集作为其他神经网络模型处理图像的输入，代替标准数据样本。我们进行了广泛的实验评估，包括不同的数据集、多样的图像分析和合成任务，如重建、分割和模态转换。我们还评估了QUAVE与实数和四元数值模型的组合。结果表明，所提出框架的有效性和泛化能力，在提高网络性能的同时，能够在多种场景中被灵活采用，并对领域偏移具有鲁棒性。完整代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/ispamm/QWT">https://github.com/ispamm/QWT</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10224v5">PDF</a> Paper accepted to Neurocomputing Journal</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新型、通用、针对数据和任务的无偏见的框架，能够从医学图像中提取重要特征。提出的四元小波网络（QUAVE）可轻松集成到任何现有的医学图像分析或合成任务中，并可与实数、四元数或超复数模型结合，将其推广到单通道数据。QUAVE首先通过四元小波变换提取不同的子带，得到低频&#x2F;近似带和高频&#x2F;精细特征，然后加权最具代表性的子带集作为其他神经网络模型处理图像的输入，替代标准数据样本。实验评估表明，该框架有效且通用，可提高网络性能，可灵活应用于多种场景，对领域变化具有鲁棒性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经网络的通用性在医学图像处理领域变得越来越重要，由于不同来源和任务的数据集日益增多。</li>
<li>处理医学数据时缺乏方法论标准导致大变化。</li>
<li>提出了一种新型通用框架——四元小波网络（QUAVE），能够提取医学图像的重要特征，可轻松集成到现有任务中。</li>
<li>QUAVE可进行四元小波变换以提取子带，包括低频和高频特征。</li>
<li>QUAVE可与其他神经网络模型结合，用于图像处理和替代标准数据样本。</li>
<li>通过广泛实验评估，证明QUAVE框架有效且通用，可提高网络性能，灵活应用于多种场景，并对领域变化具有鲁棒性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.10224">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-28e0b6c9113f4cca570fb2ed6e34e71d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12890453b76a9be8c8241dbe6a1e816f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8c36f47c381d93c7567615f0704074e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-673f0c3a64c7c432fe4edced0ddbf772.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dfae4cbfdbfd3364931dd18d7a9e0313.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-737428947398d503c531a7f872d3575e.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-25/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-25/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-26/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c61c772c5fd765f8c1f3570e98718fc8.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-05-26  NOVER Incentive Training for Language Models via Verifier-Free   Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-25/Speech/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5aa9a4d98f38fb6ecbd409bf9c96945f.jpg" class="responsive-img" alt="Speech">
                        
                        <span class="card-title">Speech</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Speech 方向最新论文已更新，请持续关注 Update in 2025-05-25  In-Context Learning Boosts Speech Recognition via Human-like Adaptation   to Speakers and Language Varieties
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                    Speech
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Speech/">
                        <span class="chip bg-color">Speech</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28879.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
