<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-02-05  Multi-agent Multi-armed Bandit with Fully Heavy-tailed Dynamics">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-7f7e8b593b1b5e007bf2ad4307af4d54.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    80 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-05-更新"><a href="#2025-02-05-更新" class="headerlink" title="2025-02-05 更新"></a>2025-02-05 更新</h1><h2 id="Multi-agent-Multi-armed-Bandit-with-Fully-Heavy-tailed-Dynamics"><a href="#Multi-agent-Multi-armed-Bandit-with-Fully-Heavy-tailed-Dynamics" class="headerlink" title="Multi-agent Multi-armed Bandit with Fully Heavy-tailed Dynamics"></a>Multi-agent Multi-armed Bandit with Fully Heavy-tailed Dynamics</h2><p><strong>Authors:Xingyu Wang, Mengfan Xu</strong></p>
<p>We study decentralized multi-agent multi-armed bandits in fully heavy-tailed settings, where clients communicate over sparse random graphs with heavy-tailed degree distributions and observe heavy-tailed (homogeneous or heterogeneous) reward distributions with potentially infinite variance. The objective is to maximize system performance by pulling the globally optimal arm with the highest global reward mean across all clients. We are the first to address such fully heavy-tailed scenarios, which capture the dynamics and challenges in communication and inference among multiple clients in real-world systems. In homogeneous settings, our algorithmic framework exploits hub-like structures unique to heavy-tailed graphs, allowing clients to aggregate rewards and reduce noises via hub estimators when constructing UCB indices; under $M$ clients and degree distributions with power-law index $\alpha &gt; 1$, our algorithm attains a regret bound (almost) of order $O(M^{1 -\frac{1}{\alpha}} \log{T})$. Under heterogeneous rewards, clients synchronize by communicating with neighbors, aggregating exchanged estimators in UCB indices; With our newly established information delay bounds on sparse random graphs, we prove a regret bound of $O(M \log{T})$. Our results improve upon existing work, which only address time-invariant connected graphs, or light-tailed dynamics in dense graphs and rewards. </p>
<blockquote>
<p>我们研究在完全重尾环境中分散的多智能体多臂赌博问题。在此环境中，客户端通过稀疏随机图进行通信，这些图的度分布具有重尾特性，并且观察到重尾（同质或异质）的奖励分布，可能存在无限方差。我们的目标是最大化系统性能，通过拉动所有客户中具有最高全局奖励均值的最佳全局手臂。我们是第一个解决这种完全重尾场景的研究团队，这捕捉了现实世界中多个客户端之间的通信和推断的动力学和挑战。在同质环境中，我们的算法框架利用重尾图中独有的中心结构，允许客户端在构建UCB指数时通过中心估计器来聚合奖励并减少噪声；在拥有幂律指数α&gt; 1的M个客户端和度分布的情况下，我们的算法达到了几乎为O(M^{1 -\frac{1}{\alpha}} log{T})的后悔界。在异质奖励下，客户端通过与邻居通信进行同步，在UCB指数中聚合交换的估计器；根据我们在稀疏随机图上新建立的信息延迟界限，我们证明了O(M log{T})的后悔界。我们的研究结果改进了现有工作，这些工作仅针对时间不变的连通图或密集图中的轻尾动态和奖励进行研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19239v1">PDF</a> 40 pages</p>
<p><strong>Summary</strong>：<br>在完全重尾分布的环境中，研究了分散式多智能体多臂老虎机问题。在稀疏随机图中通信时，通信客户之间的节点度数分布和奖励分布呈现重尾特性。目标是最大化系统性能，通过拉动全局最优臂获得最高全局平均奖励。首次解决此类完全重尾场景问题，捕捉了现实系统中多个客户端之间的通信和推断动态和挑战。在奖励分布均匀的环境下，算法框架利用重尾图的中心结构特点，构建UCB指数时通过中心估计器聚合奖励并减少噪声；在M个客户端和幂律指数α&gt;1的情况下，算法达到几乎为O(M^{1-\frac{1}{\alpha}} log{T})的后悔界。在奖励分布不均的环境下，客户端通过与邻居同步通信并在UCB指数中聚合估计器；根据新建立的信息延迟边界在稀疏随机图上，我们证明了后悔界为O(M log{T})。结果优于现有只处理时间不变连通图或密集图中轻尾动力学的解决方案。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>研究了分散式多智能体多臂老虎机在完全重尾分布环境下的表现。</li>
<li>在稀疏随机图中通信时，节点度数分布和奖励分布呈现重尾特性。</li>
<li>目标是通过最大化系统性能来拉动全局最优臂以获取最高全局平均奖励。</li>
<li>该研究首次解决此类完全重尾场景问题，反映现实系统中多个客户端间的通信和推断动态和挑战。</li>
<li>在均匀奖励环境下，算法利用重尾图的中心结构特点来优化性能。</li>
<li>在不同环境下，算法有不同的后悔界表现。在M个客户端和特定幂律指数下，算法达到特定的后悔界。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19239">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-17b0c921e9e81a98b9d8447b95408cbf.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="An-Empirical-Game-Theoretic-Analysis-of-Autonomous-Cyber-Defence-Agents"><a href="#An-Empirical-Game-Theoretic-Analysis-of-Autonomous-Cyber-Defence-Agents" class="headerlink" title="An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents"></a>An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents</h2><p><strong>Authors:Gregory Palmer, Luke Swaby, Daniel J. B. Harrold, Matthew Stewart, Alex Hiles, Chris Willis, Ian Miles, Sara Farmer</strong></p>
<p>The recent rise in increasingly sophisticated cyber-attacks raises the need for robust and resilient autonomous cyber-defence (ACD) agents. Given the variety of cyber-attack tactics, techniques and procedures (TTPs) employed, learning approaches that can return generalisable policies are desirable. Meanwhile, the assurance of ACD agents remains an open challenge. We address both challenges via an empirical game-theoretic analysis of deep reinforcement learning (DRL) approaches for ACD using the principled double oracle (DO) algorithm. This algorithm relies on adversaries iteratively learning (approximate) best responses against each others’ policies; a computationally expensive endeavour for autonomous cyber operations agents. In this work we introduce and evaluate a theoretically-sound, potential-based reward shaping approach to expedite this process. In addition, given the increasing number of open-source ACD-DRL approaches, we extend the DO formulation to allow for multiple response oracles (MRO), providing a framework for a holistic evaluation of ACD approaches. </p>
<blockquote>
<p>近期日益复杂的网络攻击的增加，使得对强大且坚韧的自主网络安全防御（ACD）代理的需求愈发迫切。考虑到所使用的网络攻击策略、技术和程序（TTPs）的多样性，学习能够回归通用策略的方法是非常理想的。同时，ACD代理的保证仍然是一个开放性的挑战。我们通过深度强化学习（DRL）方法来解决这两个挑战，并对ACD进行实证博弈论分析，采用有原则的Double Oracle（DO）算法。该算法依赖于对手之间针对彼此的决策进行迭代学习（近似）最佳响应，对于自主网络操作代理来说是一项计算成本高昂的工作。在这项工作中，我们引入并评估了一种基于理论的奖励塑形方法，以加快这一过程。此外，鉴于开源ACD-DRL方法的数量不断增加，我们将DO公式扩展到允许多重响应Oracle（MRO），为全面评估ACD方法提供了一个框架。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19206v1">PDF</a> 21 pages, 17 figures, 10 tables</p>
<p><strong>Summary</strong></p>
<p>随着网络攻击手段日益复杂，对自主网络安全防御（ACD）智能体的需求愈发迫切。由于网络攻击战术、技术和程序（TTPs）的种类多样，学习通用化策略至关重要。针对ACD智能体的可信度问题以及学习策略的泛化问题，本研究采用基于博弈论深度强化学习（DRL）的方法，利用原则性双盲算法（DO算法）进行分析。为提高计算效率，本研究引入了一种基于理论的奖励塑造方法。此外，考虑到开源ACD-DRL方法的数量不断增加，本研究将DO算法扩展到多重响应双盲（MRO），为全面评估ACD方法提供框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自主网络安全防御（ACD）智能体因网络攻击复杂性而越发重要。</li>
<li>需要开发可学习通用化策略的网络安全防御策略。</li>
<li>研究通过深度强化学习的博弈论分析来解决策略泛化问题。</li>
<li>采用原则性双盲算法（DO算法）进行分析。</li>
<li>为提高计算效率，引入基于理论的奖励塑造方法。</li>
<li>针对日益增长的开源ACD方法，扩展了多重响应双盲算法框架。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19206">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f5cea58dd7c10a1b00bc350c4f3678bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f7e8b593b1b5e007bf2ad4307af4d54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d69826a6b4a05e8f6e7674604344e01.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Autonomous-Legacy-Web-Application-Upgrades-Using-a-Multi-Agent-System"><a href="#Autonomous-Legacy-Web-Application-Upgrades-Using-a-Multi-Agent-System" class="headerlink" title="Autonomous Legacy Web Application Upgrades Using a Multi-Agent System"></a>Autonomous Legacy Web Application Upgrades Using a Multi-Agent System</h2><p><strong>Authors:Valtteri Ala-Salmi, Zeeshan Rasheed, Abdul Malik Sami, Zheying Zhang, Kai-Kristian Kemell, Jussi Rasku, Shahbaz Siddeeq, Mika Saari, Pekka Abrahamsson</strong></p>
<p>The use of Large Language Models (LLMs) for autonomous code generation is gaining attention in emerging technologies. As LLM capabilities expand, they offer new possibilities such as code refactoring, security enhancements, and legacy application upgrades. Many outdated web applications pose security and reliability challenges, yet companies continue using them due to the complexity and cost of upgrades. To address this, we propose an LLM-based multi-agent system that autonomously upgrades legacy web applications to the latest versions. The system distributes tasks across multiple phases, updating all relevant files. To evaluate its effectiveness, we employed Zero-Shot Learning (ZSL) and One-Shot Learning (OSL) prompts, applying identical instructions in both cases. The evaluation involved updating view files and measuring the number and types of errors in the output. For complex tasks, we counted the successfully met requirements. The experiments compared the proposed system with standalone LLM execution, repeated multiple times to account for stochastic behavior. Results indicate that our system maintains context across tasks and agents, improving solution quality over the base model in some cases. This study provides a foundation for future model implementations in legacy code updates. Additionally, findings highlight LLMs’ ability to update small outdated files with high precision, even with basic prompts. The source code is publicly available on GitHub: <a target="_blank" rel="noopener" href="https://github.com/alasalm1/Multi-agent-pipeline">https://github.com/alasalm1/Multi-agent-pipeline</a>. </p>
<blockquote>
<p>使用大型语言模型（LLM）进行自主代码生成正受到新兴技术的关注。随着LLM能力的扩展，它们提供了新的可能性，例如代码重构、安全增强和遗留应用程序升级。许多过时的web应用程序带来了安全和可靠性挑战，但由于升级复杂性和成本，公司仍继续使用它们。为了解决这一问题，我们提出了一种基于LLM的多智能体系统，该系统可自主将遗留web应用程序升级到最新版本。该系统将任务分布到多个阶段，并更新所有相关文件。为了评估其有效性，我们采用了零样本学习（ZSL）和单样本学习（OSL）提示，在两种情况下均应用相同的指令。评估过程包括更新视图文件并测量输出中的错误数量和类型。对于复杂任务，我们计算成功满足的要求数量。实验将所提出系统与独立LLM执行进行了比较，多次重复以考虑随机行为。结果表明，我们的系统在任务和智能体之间保持了上下文，在某些情况下提高了解决方案的质量。这项研究为未来模型在遗留代码更新中的实现提供了基础。此外，研究结果还强调了LLM更新小过时文件的高精度能力，即使使用基本的提示也能实现。源代码已在GitHub上公开可用：<a target="_blank" rel="noopener" href="https://github.com/alasalm1/Multi-agent-pipeline%E3%80%82">https://github.com/alasalm1/Multi-agent-pipeline。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19204v1">PDF</a> 13 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的自主代码生成技术正受到新兴技术的关注。随着LLM能力的扩展，它们为代码重构、安全增强和遗留应用升级等提供了新的可能性。为应对遗留web应用存在的安全和可靠性挑战，提出了一种基于LLM的多智能体系统，可自主将遗留web应用升级到最新版本。通过分布任务在多阶段更新所有相关文件。实验表明，该系统在维持语境跨任务和智能体的情况下，在某些情况下提高了解决方案的质量。此研究为未来模型在遗留代码更新中的应用提供了基础。LLM即使使用基本提示，也能高精度地更新小型过时文件。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）正被用于自主代码生成，为代码重构、安全增强和遗留应用升级提供新可能性。</li>
<li>遗留web应用存在安全和可靠性挑战，公司因升级复杂性和成本而继续使用。</li>
<li>提出一种基于LLM的多智能体系统，可自主升级遗留web应用到最新版本，通过分布任务在多阶段更新文件。</li>
<li>通过Zero-Shot Learning（ZSL）和One-Shot Learning（OSL）提示评估系统有效性。</li>
<li>实验表明，该系统在维持语境跨任务和智能体的情况下，提高解决方案质量。</li>
<li>LLMs能够更新小型过时文件，且精度高，即使使用基本提示也能实现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19204">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-893be34ad8d104c4b041f0725b4a295f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5104042522cf6da29d53beb73e4cfc2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae6b0157d7a32c04f34839a372f5d852.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="O-MAPL-Offline-Multi-agent-Preference-Learning"><a href="#O-MAPL-Offline-Multi-agent-Preference-Learning" class="headerlink" title="O-MAPL: Offline Multi-agent Preference Learning"></a>O-MAPL: Offline Multi-agent Preference Learning</h2><p><strong>Authors:The Viet Bui, Tien Mai, Hong Thanh Nguyen</strong></p>
<p>Inferring reward functions from demonstrations is a key challenge in reinforcement learning (RL), particularly in multi-agent RL (MARL), where large joint state-action spaces and complex inter-agent interactions complicate the task. While prior single-agent studies have explored recovering reward functions and policies from human preferences, similar work in MARL is limited. Existing methods often involve separate stages of supervised reward learning and MARL algorithms, leading to unstable training. In this work, we introduce a novel end-to-end preference-based learning framework for cooperative MARL, leveraging the underlying connection between reward functions and soft Q-functions. Our approach uses a carefully-designed multi-agent value decomposition strategy to improve training efficiency. Extensive experiments on SMAC and MAMuJoCo benchmarks show that our algorithm outperforms existing methods across various tasks. </p>
<blockquote>
<p>从演示中推断奖励函数是强化学习（RL）中的一项关键挑战，特别是在多智能体强化学习（MARL）中，联合状态动作空间的大规模和智能体间的复杂交互使任务复杂化。虽然之前的单智能体研究已经探索了从人类偏好中恢复奖励函数和政策，但类似的多智能体研究受到限制。现有方法通常涉及监督奖励学习和MARL算法的独立阶段，导致训练不稳定。在这项工作中，我们为合作型多智能体强化学习引入了一种新型端到端的基于偏好的学习框架，利用奖励函数和软Q函数之间的内在联系。我们的方法采用精心设计的多智能体价值分解策略来提高训练效率。在SMAC和MAMuJoCo基准测试上的广泛实验表明，我们的算法在各种任务上的表现优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18944v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了强化学习（RL）中的关键挑战——从演示中推断奖励函数，特别是在多智能体强化学习（MARL）中。由于联合状态动作空间庞大和智能体间复杂交互，这项工作更具挑战性。尽管先前单智能体研究已探索从人类偏好中恢复奖励函数和策略，但多智能体强化学习中的类似工作仍有限。现有方法通常涉及监督奖励学习和MARL算法的单独阶段，导致训练不稳定。本研究介绍了一种新型基于偏好的端到端学习框架，用于合作型MARL，利用奖励函数和软Q函数之间的内在联系。通过精心设计的多智能体值分解策略，提高了训练效率。在SMAC和MAMuJoCo基准测试上的广泛实验表明，我们的算法在各项任务中的表现均优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多智能体强化学习（MARL）在推断奖励函数方面面临挑战，主要由于联合状态动作空间庞大和智能体间的复杂交互。</li>
<li>现有方法在监督奖励学习和MARL算法分离阶段存在训练不稳定的问题。</li>
<li>研究提出了一种新型的基于偏好的端到端学习框架，该框架针对合作型MARL，并融合了奖励函数和软Q函数之间的联系。</li>
<li>通过精心设计的多智能体值分解策略，提高了训练效率。</li>
<li>在SMAC和MAMuJoCo基准测试上进行的实验表明，该算法在多种任务中的表现均优于现有方法。</li>
<li>该框架可能为解决多智能体系统中的奖励函数推断问题提供新的思路和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18944">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-bf8ef703ef3f4490b3c085182b0fa0d7.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="KBQA-o1-Agentic-Knowledge-Base-Question-Answering-with-Monte-Carlo-Tree-Search"><a href="#KBQA-o1-Agentic-Knowledge-Base-Question-Answering-with-Monte-Carlo-Tree-Search" class="headerlink" title="KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree   Search"></a>KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree   Search</h2><p><strong>Authors:Haoran Luo, Haihong E, Yikai Guo, Qika Lin, Xiaobao Wu, Xinyu Mu, Wenhao Liu, Meina Song, Yifan Zhu, Luu Anh Tuan</strong></p>
<p>Knowledge Base Question Answering (KBQA) aims to answer natural language questions with a large-scale structured knowledge base (KB). Despite advancements with large language models (LLMs), KBQA still faces challenges in weak KB awareness, imbalance between effectiveness and efficiency, and high reliance on annotated data. To address these challenges, we propose KBQA-o1, a novel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a ReAct-based agent process for stepwise logical form generation with KB environment exploration. Moreover, it employs MCTS, a heuristic search method driven by policy and reward models, to balance agentic exploration’s performance and search space. With heuristic exploration, KBQA-o1 generates high-quality annotations for further improvement by incremental fine-tuning. Experimental results show that KBQA-o1 outperforms previous low-resource KBQA methods with limited annotated data, boosting Llama-3.1-8B model’s GrailQA F1 performance to 78.5% compared to 48.5% of the previous sota method with GPT-3.5-turbo. </p>
<blockquote>
<p>知识库问答（KBQA）旨在利用大规模结构化知识库（KB）回答自然语言问题。尽管大型语言模型（LLM）有所进步，但KBQA仍然面临对知识库了解不足、效率和效果之间的不平衡以及高度依赖标注数据的挑战。为了应对这些挑战，我们提出了KBQA-o1，这是一种新型的基于蒙特卡洛树搜索（MCTS）的智能KBQA方法。它引入了一种基于ReAct的代理过程，用于逐步生成逻辑形式并进行知识库环境探索。此外，它采用MCTS（一种受策略和奖励模型驱动的自适应搜索方法）来平衡智能探索的性能和搜索空间。通过启发式探索，KBQA-o1可以生成高质量标注，通过增量微调进一步改进。实验结果表明，KBQA-o1优于之前低资源KBQA方法，这些方法依赖于有限的标注数据，它使得Llama-3.1-8B模型的GrailQA F1性能提升到78.5%，而之前最佳方法GPT-3.5-turbo的该指标为48.5%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18922v1">PDF</a> Preprint</p>
<p><strong>Summary</strong><br>KBQA系统旨在通过大规模结构化知识库回答自然语言问题。为应对KBQA的挑战，如弱知识库意识、效率和有效性之间的不平衡以及高度依赖标注数据的问题，我们提出了KBQA-o1这一新型智能体知识库问答方法，结合蒙特卡洛树搜索（MCTS）。它引入基于ReAct的智能体过程，进行逐步逻辑形式生成和知识库环境探索。此外，利用MCTS这一受政策和奖励模型驱动的策略性搜索方法，实现智能体探索性能与搜索空间的平衡。通过启发式探索，KBQA-o1可生成高质量标注用于进一步增量微调。实验结果显示，KBQA-o1在有限标注数据的情况下，优于先前低资源KBQA方法，将Llama-3.1-8B模型的GrailQA F1性能提升至78.5%，远超先前最佳方法GPT-3.5-turbo的48.5%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>KBQA系统通过大规模结构化知识库回答自然语言问题。</li>
<li>KBQA面临弱知识库意识、效率和有效性之间的不平衡以及依赖标注数据等挑战。</li>
<li>KBQA-o1是应对这些挑战的新型智能体知识库问答方法，结合了蒙特卡洛树搜索（MCTS）。</li>
<li>KBQA-o1通过基于ReAct的智能体过程进行逐步逻辑形式生成和知识库环境探索。</li>
<li>MCTS平衡了智能体探索的性能和搜索空间。</li>
<li>KBQA-o1能生成高质量标注用于增量微调。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18922">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f3d588ec96b1eb5ee38958dfa48a9b98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eaf997481d34e0a06d1d6109110f86fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f613419b707245de6415f437be40c24c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94d85a678797b37aa1bc87a4bae3f0e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69ba8d082e5d4040e1aa68da06ed3009.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="UP-VLA-A-Unified-Understanding-and-Prediction-Model-for-Embodied-Agent"><a href="#UP-VLA-A-Unified-Understanding-and-Prediction-Model-for-Embodied-Agent" class="headerlink" title="UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent"></a>UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent</h2><p><strong>Authors:Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, Jianyu Chen</strong></p>
<p>Recent advancements in Vision-Language-Action (VLA) models have leveraged pre-trained Vision-Language Models (VLMs) to improve the generalization capabilities. VLMs, typically pre-trained on vision-language understanding tasks, provide rich semantic knowledge and reasoning abilities. However, prior research has shown that VLMs often focus on high-level semantic content and neglect low-level features, limiting their ability to capture detailed spatial information and understand physical dynamics. These aspects, which are crucial for embodied control tasks, remain underexplored in existing pre-training paradigms. In this paper, we investigate the training paradigm for VLAs, and introduce \textbf{UP-VLA}, a \textbf{U}nified VLA model training with both multi-modal \textbf{U}nderstanding and future \textbf{P}rediction objectives, enhancing both high-level semantic comprehension and low-level spatial understanding. Experimental results show that UP-VLA achieves a 33% improvement on the Calvin ABC-D benchmark compared to the previous state-of-the-art method. Additionally, UP-VLA demonstrates improved success rates in real-world manipulation tasks, particularly those requiring precise spatial information. </p>
<blockquote>
<p>最近，Vision-Language-Action（VLA）模型的进步利用了预训练的Vision-Language Models（VLMs）来提高模型的泛化能力。通常，VLMs在视觉语言理解任务上进行预训练，提供了丰富的语义知识和推理能力。然而，先前的研究表明，VLMs往往关注高级语义内容而忽视低级特征，限制了它们在捕捉详细的空间信息和理解物理动态方面的能力。对于实体控制任务来说，这些方面至关重要，但在现有的预训练范式中仍然探索不足。在本文中，我们研究了VLA的训练范式，并引入了UP-VLA，这是一个统一的VLA模型训练，具有多模式理解（multi-modal understanding）和未来预测（future prediction）目标，增强了高级语义理解和低级空间理解。实验结果表明，与先前最先进的方法相比，UP-VLA在Calvin ABC-D基准测试上实现了33%的改进。此外，UP-VLA在现实世界操作任务上的成功率也有所提高，尤其是那些需要精确空间信息的任务。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18867v2">PDF</a> </p>
<p><strong>Summary</strong><br>基于最新视觉语言动作（VLA）模型的进展，研究引入了预训练视觉语言模型（VLM）以提升泛化能力。然而，现有研究指出VLM模型常常忽略低层次特征，限制了捕捉详细空间信息和理解物理动态的能力。本研究调查了VLA的训练模式，并提出了统一VLA模型训练范例UP-VLA，旨在提高多模态理解与未来预测目标，以增强高层次语义理解和低层次空间理解。在Calvin ABC-D基准测试中，UP-VLA相比之前的方法取得了33%的改进，并在现实操作任务中显示出更高的成功率，特别是在需要精确空间信息的任务中。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLMs虽能提供丰富的语义知识和推理能力，但常常忽略低层次特征，限制了空间信息和物理动态的理解。</li>
<li>本研究调查了VLA模型的训练范式，并引入了UP-VLA模型，该模型结合了多模态理解和未来预测目标。</li>
<li>UP-VLA模型旨在增强高层次语义理解和低层次空间理解。</li>
<li>在Calvin ABC-D基准测试中，UP-VLA模型相比之前的方法有显著改进。</li>
<li>UP-VLA模型在现实操作任务中表现出更高的成功率，特别是在需要精确空间信息的任务中。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18867">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d68736e9356b84d4e88f96afab4c48af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c3018d1b33a2c60a22bd069feede1bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48559e3e0d07c1a74fef1c1185645ecb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2707d558f22a4073b49954cfd764bc26.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="RepoAudit-An-Autonomous-LLM-Agent-for-Repository-Level-Code-Auditing"><a href="#RepoAudit-An-Autonomous-LLM-Agent-for-Repository-Level-Code-Auditing" class="headerlink" title="RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing"></a>RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing</h2><p><strong>Authors:Jinyao Guo, Chengpeng Wang, Xiangzhe Xu, Zian Su, Xiangyu Zhang</strong></p>
<p>Code auditing is a code review process with the goal of finding bugs. Large Language Models (LLMs) have shown substantial potential in this task, offering the ability to analyze programs without compilation and enabling customized bug detection following specified prompts. However, applying LLMs to repository-level code auditing presents notable challenges. The inherent context limits and hallucinations of LLMs can lead to the low quality of bug reports. Meanwhile, the large size of software repositories introduces substantial time and token costs, hindering efficiency and scalability in real-world scenarios. This work introduces an autonomous LLM-agent, RepoAudit, designed to enable precise and efficient repository-level code auditing. Equipped with the agent memory, RepoAudit explores the code repository on demand, analyzing data-flow facts along different feasible program paths in individual functions. It also introduces the validator to check the data-flow facts for hallucination mitigation and examine the satisfiability of path conditions of potential buggy paths, which enables RepoAudit to discard false positives in the code auditing. Our experiment shows that RepoAudit powered by Claude 3.5 Sonnet successfully finds 38 true bugs in 15 real-world systems, consuming 0.44 hours and $2.54 per project on average. </p>
<blockquote>
<p>代码审计是一种寻找错误的代码审查过程。大型语言模型（LLM）在这一任务中表现出了巨大的潜力，它们能够在不编译的情况下分析程序，并根据特定提示进行定制化的错误检测。然而，将LLM应用于仓库级别的代码审计存在显著的挑战。LLM的固有上下文限制和幻觉可能导致错误报告的质量低下。同时，软件仓库的大规模性引入了大量的时间和令牌成本，阻碍了现实世界场景中的效率和可扩展性。这项工作引入了一个自主的语言模型代理RepoAudit，旨在实现精确高效的仓库级代码审计。配备有代理内存的RepoAudit按需探索代码仓库，分析单个函数中不同可行程序路径的数据流事实。它还引入了验证器，以减轻幻觉并检查数据流事实，并检查潜在错误路径的路径条件的可满足性，这使得RepoAudit能够在代码审计中剔除误报。我们的实验表明，由Claude 3.5 Sonnet驱动的RepoAudit成功在15个真实系统中找到了38个真实错误，平均每个项目消耗0.44小时和2.54美元。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18160v2">PDF</a> 19 pages, 8 tables, 5 figures, 3 listings</p>
<p><strong>Summary</strong></p>
<p>代码审计旨在发现错误，大型语言模型（LLM）在此任务中显示出巨大潜力，可分析程序而无需编译，并能根据特定提示进行定制化的错误检测。然而，将LLM应用于仓库级代码审计存在挑战。LLM的固有上下文限制和幻想可能导致错误报告质量低下。同时，软件仓库的大规模引入了大量时间和令牌成本，阻碍了现实世界场景中的效率和可扩展性。本研究引入了一种自主LLM代理RepoAudit，旨在实现精确高效的仓库级代码审计。RepoAudit配备了代理内存，可按需探索代码仓库，分析单个函数中不同可行程序路径的数据流事实。它还引入了验证器，以减轻幻想并检查数据流事实，并检查潜在错误路径的满足性，使RepoAudit能够在代码审计中剔除误报。实验表明，由Claude 3.5 Sonnet驱动的RepoAudit成功在15个真实系统中找到38个真实错误，平均每个项目消耗0.44小时和2.54美元。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>代码审计目标是发现错误，大型语言模型在此任务中显示出潜力。</li>
<li>LLM应用于仓库级代码审计存在挑战，如上下文限制和幻想问题。</li>
<li>RepoAudit是一个自主LLM代理，可实现精确高效的仓库级代码审计。</li>
<li>RepoAudit配备了代理内存，可按需探索代码仓库，并分析数据流事实。</li>
<li>验证器的引入用于检查数据流事实并减少幻想，同时检查潜在错误路径的满足性。</li>
<li>实验显示RepoAudit成功在多个真实系统中发现真实错误，且效率较高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18160">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-19b0901ae3271e79208026b5578aa8aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3c5cf8733e2ffc7645a20994d7c9034.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b444555ee30ab25234be1035ab2fb62e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cba0fe6becab38b01c669d770b1bedb9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="B3C-A-Minimalist-Approach-to-Offline-Multi-Agent-Reinforcement-Learning"><a href="#B3C-A-Minimalist-Approach-to-Offline-Multi-Agent-Reinforcement-Learning" class="headerlink" title="B3C: A Minimalist Approach to Offline Multi-Agent Reinforcement Learning"></a>B3C: A Minimalist Approach to Offline Multi-Agent Reinforcement Learning</h2><p><strong>Authors:Woojun Kim, Katia Sycara</strong></p>
<p>Overestimation arising from selecting unseen actions during policy evaluation is a major challenge in offline reinforcement learning (RL). A minimalist approach in the single-agent setting – adding behavior cloning (BC) regularization to existing online RL algorithms – has been shown to be effective; however, this approach is understudied in multi-agent settings. In particular, overestimation becomes worse in multi-agent settings due to the presence of multiple actions, resulting in the BC regularization-based approach easily suffering from either over-regularization or critic divergence. To address this, we propose a simple yet effective method, Behavior Cloning regularization with Critic Clipping (B3C), which clips the target critic value in policy evaluation based on the maximum return in the dataset and pushes the limit of the weight on the RL objective over BC regularization, thereby improving performance. Additionally, we leverage existing value factorization techniques, particularly non-linear factorization, which is understudied in offline settings. Integrated with non-linear value factorization, B3C outperforms state-of-the-art algorithms on various offline multi-agent benchmarks. </p>
<blockquote>
<p>在离线强化学习（RL）中，由于在策略评估时选择未观察到的动作而导致的过度估计是主要挑战。在单智能体环境中，一种极简主义的方法——向现有的在线RL算法添加行为克隆（BC）正则化已被证明是有效的；然而，在多智能体环境中，这种方法的研究还不够。特别是在多智能体环境中，由于存在多种动作，过度估计的情况变得更糟，导致基于BC正则化的方法很容易受到过度正则化或评论家发散的影响。为了解决这一问题，我们提出了一种简单有效的方法，即带有评论家裁剪的行为克隆正则化（B3C），该方法基于数据集的最大回报来裁剪目标评论家值，并提高了在策略评估中RL目标上权重的限制，从而提高了性能。此外，我们利用现有的值分解技术，特别是在离线环境中尚未充分研究的非线性分解技术。与非线性值分解相结合，B3C在各种离线多智能体基准测试上优于最新算法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18138v2">PDF</a> </p>
<p><strong>总结</strong></p>
<p>离线强化学习中的主要挑战在于选择未知动作时产生的过估计问题。在单智能体环境中，通过向现有在线强化学习算法添加行为克隆（BC）正则化这种极简方案已被证明是有效的。然而，在多智能体环境中，该方案的研究相对较少。特别是由于存在多个动作，过估计问题在多智能体环境中变得更加严重，导致基于BC正则化的方法容易遭受过度正则化或评论家发散的问题。为解决这一问题，我们提出了一种简单有效的方法——行为克隆正则化与评论家裁剪（B3C）。该方法基于数据集的最大回报来裁剪目标评论家值，并在政策评估中改进性能。此外，我们利用现有的价值分解技术，特别是离线环境中研究较少的非线性分解技术。结合非线性价值分解，B3C在多种离线多智能体基准测试上超越了现有算法。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>离线强化学习面临的主要挑战是选择未知动作时导致的过估计问题。</li>
<li>在多智能体环境中，由于多个动作的存在，过估计问题变得更加严重。</li>
<li>行为克隆（BC）正则化是一种有效的单智能体强化学习优化方法，但在多智能体环境中其效果有待提高。</li>
<li>行为克隆正则化与评论家裁剪（B3C）方法旨在解决多智能体环境中的过估计问题，通过裁剪目标评论家值来改善性能。</li>
<li>B3C方法结合了非线性价值分解技术，以提高在离线多智能体环境中的性能。</li>
<li>B3C在各种离线多智能体基准测试上的表现超越了现有算法。</li>
<li>该研究为多智能体离线强化学习提供了一个新的、有效的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18138">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0faaace700df26cb9d78f3117ac40ad9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5805cf636a2823ed774b902936a2fad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ef52ff26a04fe0ec706b837184daf39.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76f0c5ef7e22dbb2d45ce72e6a1ce532.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Fortran2CPP-Automating-Fortran-to-C-Translation-using-LLMs-via-Multi-Turn-Dialogue-and-Dual-Agent-Integration"><a href="#Fortran2CPP-Automating-Fortran-to-C-Translation-using-LLMs-via-Multi-Turn-Dialogue-and-Dual-Agent-Integration" class="headerlink" title="Fortran2CPP: Automating Fortran-to-C++ Translation using LLMs via   Multi-Turn Dialogue and Dual-Agent Integration"></a>Fortran2CPP: Automating Fortran-to-C++ Translation using LLMs via   Multi-Turn Dialogue and Dual-Agent Integration</h2><p><strong>Authors:Le Chen, Bin Lei, Dunzhi Zhou, Pei-Hung Lin, Chunhua Liao, Caiwen Ding, Ali Jannesari</strong></p>
<p>Translating legacy Fortran code into C++ is a crucial step in modernizing high-performance computing (HPC) applications. However, the scarcity of high-quality, parallel Fortran-to-C++ datasets and the limited domain-specific expertise in large language models (LLMs) present significant challenges for automated translation. In this paper, we introduce Fortran2CPP, a multi-turn dialogue dataset generated by a novel LLM agent-based approach that integrates a dual-LLM Questioner-Solver module to enhance translation accuracy. Our dataset comprises 11.7k dialogues capturing iterative feedback-decision workflows including code translation, compilation, execution, unit testing, and error-fixing. Using this dataset, we fine-tune several open-weight LLMs and achieve up to a 3.31x improvement in CodeBLEU scores and a 92% increase in compilation success rate, demonstrating enhanced syntactic accuracy and functional reliability. Our findings highlight the value of dialogue-based LLM training for complex code translation tasks. The dataset and model have been open-sourced and are available on our public GitHub repository\footnote{\url{<a target="_blank" rel="noopener" href="https://github.com/HPC-Fortran2CPP/Fortran2Cpp%7D%7D">https://github.com/HPC-Fortran2CPP/Fortran2Cpp}}</a>. </p>
<blockquote>
<p>将传统Fortran代码转换为C++代码是现代高性能计算（HPC）应用现代化过程中的关键步骤。然而，高质量并行Fortran到C++数据集缺乏以及大型语言模型（LLM）中特定领域的专业知识有限，给自动化翻译带来了重大挑战。在本文中，我们介绍了Fortran2CPP，这是一个通过基于新型LLM代理的方法生成的多轮对话数据集，集成了双LLM问答解决模块，以提高翻译准确性。我们的数据集包含11.7k个对话，捕捉了包括代码翻译、编译、执行、单元测试和错误修复在内的迭代反馈决策工作流程。使用该数据集，我们对几个开源LLM进行了微调，实现了高达3.31倍的CodeBLEU得分提升和92%的编译成功率提升，证明了其在语法准确性和功能可靠性方面的提升。我们的研究突出了基于对话的LLM训练在复杂代码翻译任务中的价值。数据集和模型已开源，可在我们的GitHub公共存储库中找到（<a target="_blank" rel="noopener" href="https://github.com/HPC-Fortran2CPP/Fortran2Cpp%EF%BC%89%E3%80%82">https://github.com/HPC-Fortran2CPP/Fortran2Cpp）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19770v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了将旧版Fortran代码翻译成C++的重要性，并指出由于缺乏高质量并行Fortran到C++的数据集以及大型语言模型（LLM）的特定领域专业知识所带来的挑战。为此，本文提出了一种基于多轮对话的新方法Fortran2CPP，该方法集成了双LLM问答解决模块以提高翻译准确性。数据集包含捕获迭代反馈决策工作流的对话，包括代码翻译、编译、执行、单元测试和错误修复。使用此数据集微调开放权重LLM，可提高代码BLEU得分并增加编译成功率，显示出增强的语法准确性和功能可靠性。研究结果表明对话式LLM训练对于复杂的代码翻译任务的价值。数据集和模型已开源，可在公共GitHub存储库中找到。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Fortran代码向C++的转换是高性能计算应用现代化的重要步骤。</li>
<li>缺乏高质量并行Fortran到C++的数据集以及大型语言模型的特定领域专业知识给自动化翻译带来了挑战。</li>
<li>介绍了Fortran2CPP方法，这是一种基于多轮对话的方法，集成了双LLM问答解决模块以提高翻译准确性。</li>
<li>Fortran2CPP数据集包含用于代码翻译任务的迭代反馈决策工作流对话。</li>
<li>使用此数据集对开放权重LLM进行微调，提高了语法准确性和功能可靠性。</li>
<li>Fortran2CPP方法实现了高达3.31倍的代码BLEU得分改进和92%的编译成功率增加。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19770">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5f4ca5ae820d1405e3f7f4ca095f44ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae68d02bd8067997f177146114fe19f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8227e17589f566c7495106653c0af8f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bebac2fa21a7f930132a5aa05ebb3bb.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Multi-modal-Agent-Tuning-Building-a-VLM-Driven-Agent-for-Efficient-Tool-Usage"><a href="#Multi-modal-Agent-Tuning-Building-a-VLM-Driven-Agent-for-Efficient-Tool-Usage" class="headerlink" title="Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool   Usage"></a>Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool   Usage</h2><p><strong>Authors:Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaojian Ma, Tao Yuan, Yue Fan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, Qing Li</strong></p>
<p>The advancement of large language models (LLMs) prompts the development of multi-modal agents, which are used as a controller to call external tools, providing a feasible way to solve practical tasks. In this paper, we propose a multi-modal agent tuning method that automatically generates multi-modal tool-usage data and tunes a vision-language model (VLM) as the controller for powerful tool-usage reasoning. To preserve the data quality, we prompt the GPT-4o mini model to generate queries, files, and trajectories, followed by query-file and trajectory verifiers. Based on the data synthesis pipeline, we collect the MM-Traj dataset that contains 20K tasks with trajectories of tool usage. Then, we develop the T3-Agent via \underline{T}rajectory \underline{T}uning on VLMs for \underline{T}ool usage using MM-Traj. Evaluations on the GTA and GAIA benchmarks show that the T3-Agent consistently achieves improvements on two popular VLMs: MiniCPM-V-8.5B and {Qwen2-VL-7B}, which outperforms untrained VLMs by $20%$, showing the effectiveness of the proposed data synthesis pipeline, leading to high-quality data for tool-usage capabilities. </p>
<blockquote>
<p>随着大型语言模型（LLM）的进展，多模态代理的发展得到了推动，这些代理被用作控制器来调用外部工具，为解决实际任务提供了可行的方法。在本文中，我们提出了一种多模态代理调整方法，该方法可自动生成多模态工具使用数据，并调整视觉语言模型（VLM）作为控制器，以实现强大的工具使用推理。为了保持数据质量，我们提示GPT-4o小型模型生成查询、文件和轨迹，随后进行查询文件验证器和轨迹验证器。基于数据合成管道，我们收集了包含2万个任务使用轨迹的MM-Traj数据集。然后，我们通过使用MM-Traj的轨迹调整VLM来开发T3代理。在GTA和GAIA基准测试上的评估表明，T3代理在两款流行的VLM（MiniCPM-V-8.5B和Qwen2-VL-7B）上均实现了持续改进，其性能比未训练的VLM高出20%，证明了所提出的数据合成管道的有效性，为工具使用能力提供了高质量的数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15606v2">PDF</a> ICLR 2025, <a target="_blank" rel="noopener" href="https://mat-agent.github.io/">https://mat-agent.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>大型语言模型的进步推动了多模态代理的发展，本文提出了一种多模态代理调节方法，该方法可自动生成多模态工具使用数据，并调节视觉语言模型作为控制器来进行工具使用推理。通过数据合成管道，我们收集了MM-Traj数据集，并在此基础上开发了T3-Agent。评估表明，T3-Agent在两种流行的VLM上实现了改进，并优于未训练的VLM，证明了数据合成管道的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型的进步促进了多模态代理的发展，多模态代理能够作为控制器调用外部工具，为解决实际任务提供了可行方法。</li>
<li>本文提出了一种多模态代理调节方法，可以自动生成多模态工具使用数据。</li>
<li>采用了数据合成管道来收集MM-Traj数据集，该数据集包含2万项任务及工具使用轨迹。</li>
<li>开发了一种基于MM-Traj的T3-Agent，通过轨迹调节视觉语言模型进行工具使用。</li>
<li>评估结果显示T3-Agent在两种流行的VLM上表现优异，优于未训练的VLM。</li>
<li>数据合成管道的有效性得到了验证，能够生成高质量的工具使用数据。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15606">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-963b18445f42c73720c6ca81034c8c5c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-167386209c58998672452da9d496a130.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-037e35ae237c29563089361fce834dbd.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Brained-GUI-Agents-A-Survey"><a href="#Large-Language-Model-Brained-GUI-Agents-A-Survey" class="headerlink" title="Large Language Model-Brained GUI Agents: A Survey"></a>Large Language Model-Brained GUI Agents: A Survey</h2><p><strong>Authors:Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang</strong></p>
<p>GUIs have long been central to human-computer interaction, providing an intuitive and visually-driven way to access and interact with digital systems. The advent of LLMs, particularly multimodal models, has ushered in a new era of GUI automation. They have demonstrated exceptional capabilities in natural language understanding, code generation, and visual processing. This has paved the way for a new generation of LLM-brained GUI agents capable of interpreting complex GUI elements and autonomously executing actions based on natural language instructions. These agents represent a paradigm shift, enabling users to perform intricate, multi-step tasks through simple conversational commands. Their applications span across web navigation, mobile app interactions, and desktop automation, offering a transformative user experience that revolutionizes how individuals interact with software. This emerging field is rapidly advancing, with significant progress in both research and industry.   To provide a structured understanding of this trend, this paper presents a comprehensive survey of LLM-brained GUI agents, exploring their historical evolution, core components, and advanced techniques. We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness. Additionally, we examine emerging applications powered by these agents. Through a detailed analysis, this survey identifies key research gaps and outlines a roadmap for future advancements in the field. By consolidating foundational knowledge and state-of-the-art developments, this work aims to guide both researchers and practitioners in overcoming challenges and unlocking the full potential of LLM-brained GUI agents. </p>
<blockquote>
<p>图形用户界面（GUIs）长期以来一直是人机交互的核心，提供了一种直观且视觉驱动的方式来访问和与数字系统交互。大语言模型（LLMs）的出现，特别是多模态模型，已经开启了GUI自动化的新时代。它们在自然语言理解、代码生成和视觉处理方面表现出了卓越的能力。这为新一代基于LLM的GUI代理铺平了道路，这些代理能够解释复杂的GUI元素，并基于自然语言指令自主执行操作。这些代理代表了范式转变，使用户能够通过简单的命令执行复杂的多步骤任务。它们的应用范围涵盖网页导航、移动应用交互和桌面自动化，提供变革性的用户体验，彻底改变个人与软件的交互方式。这个新兴领域正在迅速发展，在研究和工业方面都取得了重大进展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18279v8">PDF</a> The collection of papers reviewed in this survey will be hosted and   regularly updated on the GitHub repository:   <a target="_blank" rel="noopener" href="https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey">https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey</a> Additionally, a   searchable webpage is available at <a target="_blank" rel="noopener" href="https://aka.ms/gui-agent">https://aka.ms/gui-agent</a> for easier access   and exploration</p>
<p><strong>Summary</strong></p>
<p>本文介绍了GUI在人机交互中的长期重要地位，以及大型语言模型（LLM）特别是多模态模型的出现所带来的GUI自动化新时代。LLM展现出在理解自然语言、生成代码和视觉处理方面的卓越能力，为新一代基于LLM的GUI代理的发展铺平了道路。这些代理可以解释复杂的GUI元素并根据自然语言指令自主执行操作。它们的应用范围广泛，包括网页导航、移动应用交互和桌面自动化，为用户提供了一种变革性的体验，彻底改变了个人与软件的交互方式。本文全面概述了基于LLM的GUI代理的历史演变、核心组件和先进技术，并探讨了该领域的关键研究问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GUI在人机交互中占据重要地位，大型语言模型（LLM）的引入为GUI自动化带来了新的发展机会。</li>
<li>LLM在理解自然语言、生成代码和视觉处理方面表现出卓越能力。</li>
<li>基于LLM的GUI代理能够解释复杂的GUI元素并根据自然语言指令自主执行操作。</li>
<li>这些代理的应用范围广泛，包括网页导航、移动应用交互和桌面自动化。</li>
<li>基于LLM的GUI代理代表了用户与软件交互方式的重大转变。</li>
<li>当前研究问题包括GUI代理框架、数据收集和利用、针对GUI任务的特殊动作模型开发等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18279">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ac9fb2875712cb27583503f3f8465f38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6a4be3fb98a0b63af42b7b053140fa6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-58397a39ca33d56b26bd204d67f82c65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d89366ba73b064043f371d52faf01a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fea2d287475388eff773cf68ce228c9.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LLM-Consensus-Multi-Agent-Debate-for-Visual-Misinformation-Detection"><a href="#LLM-Consensus-Multi-Agent-Debate-for-Visual-Misinformation-Detection" class="headerlink" title="LLM-Consensus: Multi-Agent Debate for Visual Misinformation Detection"></a>LLM-Consensus: Multi-Agent Debate for Visual Misinformation Detection</h2><p><strong>Authors:Kumud Lakara, Georgia Channing, Juil Sock, Christian Rupprecht, Philip Torr, John Collomosse, Christian Schroeder de Witt</strong></p>
<p>One of the most challenging forms of misinformation involves the out-of-context (OOC) use of images paired with misleading text, creating false narratives. Existing AI-driven detection systems lack explainability and require expensive finetuning. We address these issues with LLM-Consensus, a multi-agent debate system for OOC misinformation detection. LLM-Consensus introduces a novel multi-agent debate framework where multimodal agents collaborate to assess contextual consistency and request external information to enhance cross-context reasoning and decision-making. Our framework enables explainable detection with state-of-the-art accuracy even without domain-specific fine-tuning. Extensive ablation studies confirm that external retrieval significantly improves detection accuracy, and user studies demonstrate that LLM-Consensus boosts performance for both experts and non-experts. These results position LLM-Consensus as a powerful tool for autonomous and citizen intelligence applications. </p>
<blockquote>
<p>误解信息最具挑战性的形式之一涉及脱离上下文（OOC）使用与误导性文本配对的图像，从而创造错误的叙事。现有的AI驱动的检测系统缺乏解释性，并需要昂贵的微调。我们通过LLM-Consensus解决这些问题，这是一种用于OOC误解信息检测的基于多智能体的辩论系统。LLM-Consensus引入了一种新型的多智能体辩论框架，其中多模态智能体相互协作以评估上下文一致性并请求外部信息以增强跨上下文推理和决策制定。我们的框架在无需特定领域微调的情况下，即可实现具有前沿准确性的可解释检测。广泛的消融研究证实，外部检索显着提高了检测准确性，用户研究也表明LLM-Consensus可提升专家和非专家的性能。这些结果将LLM-Consensus定位为自主和公民智能应用的强大工具。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20140v2">PDF</a> </p>
<p><strong>Summary</strong><br>文本提出了一种基于多模态对话框架的OOC虚假信息检测模型LLM-Consensus。该模型通过引入外部信息提高跨上下文推理和决策制定能力，支持可解释性检测并实现了较高准确度，无需特定领域微调。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM-Consensus解决了现有AI驱动检测系统中存在的解释性不足和昂贵微调成本的问题。</li>
<li>LLM-Consensus采用多模态对话框架，实现跨上下文推理和决策制定。</li>
<li>该模型引入外部信息检索功能，显著提高检测准确性。</li>
<li>LLM-Consensus的检测具有可解释性，并达到业界领先水平。</li>
<li>用户研究表明，LLM-Consensus对于专家和非专家用户都有性能提升。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.20140">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e36c5a16c8ef7de074086def01e1443d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a88743f99f8cdacc999bb3b5fcabaf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bdc559403dfe18679a699ec261cd043.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e9b2946381f99885a86058580f6e4cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e36047926c7227ed84d8c665397a4b5.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GraphTeam-Facilitating-Large-Language-Model-based-Graph-Analysis-via-Multi-Agent-Collaboration"><a href="#GraphTeam-Facilitating-Large-Language-Model-based-Graph-Analysis-via-Multi-Agent-Collaboration" class="headerlink" title="GraphTeam: Facilitating Large Language Model-based Graph Analysis via   Multi-Agent Collaboration"></a>GraphTeam: Facilitating Large Language Model-based Graph Analysis via   Multi-Agent Collaboration</h2><p><strong>Authors:Xin Li, Qizhi Chu, Yubin Chen, Yang Liu, Yaoqi Liu, Zekai Yu, Weize Chen, Chen Qian, Chuan Shi, Cheng Yang</strong></p>
<p>Graphs are widely used for modeling relational data in real-world scenarios, such as social networks and urban computing. Existing LLM-based graph analysis approaches either integrate graph neural networks (GNNs) for specific machine learning tasks, limiting their transferability, or rely solely on LLMs’ internal reasoning ability, resulting in suboptimal performance. To address these limitations, we take advantage of recent advances in LLM-based agents, which have shown capabilities of utilizing external knowledge or tools for problem solving. By simulating human problem-solving strategies such as analogy and collaboration, we propose a multi-agent system based on LLMs named GraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from three modules, and the agents with different specialities can collaborate with each other to address complex problems. Specifically, (1) input-output normalization module: the question agent extracts and refines four key arguments from the original question, facilitating the problem understanding, and the answer agent organizes the results to meet the output requirement; (2) external knowledge retrieval module: we first build a knowledge base consisting of relevant documentation and experience information, and then the search agent retrieves the most relevant entries for each question. (3) problem-solving module: given the retrieved information from search agent, the coding agent uses established algorithms via programming to generate solutions, and in case the coding agent does not work, the reasoning agent will directly compute the results without programming. Extensive experiments on six graph analysis benchmarks demonstrate that GraphTeam achieves state-of-the-art performance with an average 25.85% improvement over the best baseline in terms of accuracy. The code and data are available at <a target="_blank" rel="noopener" href="https://github.com/BUPT-GAMMA/GraphTeam">https://github.com/BUPT-GAMMA/GraphTeam</a>. </p>
<blockquote>
<p>图被广泛用于现实场景中的关系数据建模，如社交网络和城市计算。现有的基于大型语言模型（LLM）的图分析方法的不足之处在于，它们要么将图神经网络（GNNs）集成到特定的机器学习任务中，限制了其可迁移性，要么仅依赖于LLMs的内部推理能力，导致性能不佳。为了解决这些局限性，我们利用基于LLM的代理的最新进展，这些代理已显示出利用外部知识或工具解决问题的能力。通过模拟人类的解决问题策略，如类比和协作，我们提出了一种基于LLM的多代理系统，名为GraphTeam，用于图分析。GraphTeam由三个模块中的五个基于LLM的代理组成，不同专业的代理可以相互协作来解决复杂问题。具体来说，（1）输入输出归一化模块：问题代理从原始问题中提取并优化四个关键参数，促进对问题的理解，答案代理则负责按输出要求组织结果；（2）外部知识检索模块：我们首先建立一个包含相关文档和经验信息的知识库，然后搜索代理根据每个问题检索最相关的条目。（3）问题解决模块：给定来自搜索代理的检索信息，编码代理通过编程使用既定算法来生成解决方案；如果编码代理无法工作，推理代理将直接进行计算以得出结果。在六个图分析基准测试上的广泛实验表明，GraphTeam在准确度方面达到了最先进的性能，与最佳基线相比平均提高了25.85%。相关代码和数据可以在<a target="_blank" rel="noopener" href="https://github.com/BUPT-GAMMA/GraphTeam%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/BUPT-GAMMA/GraphTeam获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.18032v3">PDF</a> </p>
<p><strong>Summary</strong><br>    利用大型语言模型（LLM）构建名为GraphTeam的多智能体系统，用于图分析。该系统由三个模块组成，包含输入输出的规范处理、外部知识的检索、问题求解。该系统的智能体能通过协同合作解决复杂问题，对六种图分析基准测试的实验结果表明，GraphTeam实现了卓越的性能，平均准确率相较于最佳基线提高了25.85%。相关代码和数据在GitHub上公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GraphTeam是一个基于大型语言模型（LLM）的多智能体系统，用于图分析。</li>
<li>GraphTeam解决了现有LLM-based图分析方法的局限性，如缺乏转移性和性能不足。</li>
<li>GraphTeam包含三个模块：输入输出的规范处理、外部知识的检索和问题求解。</li>
<li>输入输出规范处理模块包含问题理解和结果组织。</li>
<li>外部知识检索模块建立了包含相关文档和经验信息的知识库，并能够通过搜索智能体检索最相关的信息。</li>
<li>问题求解模块能够通过编程智能体利用已建立的算法生成解决方案，如果不适用编程方式，推理智能体会直接计算结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.18032">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a3604c982b8bed75f44f10e4023e70f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6174aaec6b371137dfce026a2e408e6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd6e32e390d7e1a502518465cfd21472.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Improving-Parallel-Program-Performance-with-LLM-Optimizers-via-Agent-System-Interface"><a href="#Improving-Parallel-Program-Performance-with-LLM-Optimizers-via-Agent-System-Interface" class="headerlink" title="Improving Parallel Program Performance with LLM Optimizers via   Agent-System Interface"></a>Improving Parallel Program Performance with LLM Optimizers via   Agent-System Interface</h2><p><strong>Authors:Anjiang Wei, Allen Nie, Thiago S. F. X. Teixeira, Rohan Yadav, Wonchan Lee, Ke Wang, Alex Aiken</strong></p>
<p>Modern scientific discovery increasingly relies on high-performance computing for complex modeling and simulation. A key challenge in improving parallel program performance is efficiently mapping tasks to processors and data to memory, a process dictated by intricate, low-level system code known as mappers. Developing high-performance mappers demands days of manual tuning, posing a significant barrier for domain scientists without systems expertise. We introduce a framework that automates mapper development with generative optimization, leveraging richer feedback beyond scalar performance metrics. Our approach features the Agent-System Interface, which includes a Domain-Specific Language (DSL) to abstract away low-level complexity of system code and define a structured search space, as well as AutoGuide, a mechanism that interprets raw execution output into actionable feedback. Unlike traditional reinforcement learning methods such as OpenTuner, which rely solely on scalar feedback, our method finds superior mappers in far fewer iterations. With just 10 iterations, it outperforms OpenTuner even after 1000 iterations, achieving 3.8X faster performance. Our approach finds mappers that surpass expert-written mappers by up to 1.34X speedup across nine benchmarks while reducing tuning time from days to minutes. </p>
<blockquote>
<p>现代科学发现越来越依赖于高性能计算进行复杂的建模和模拟。提高并行程序性能的关键挑战在于有效地将任务映射到处理器并将数据映射到内存，这一过程由复杂的低级系统代码（称为映射器）决定。开发高性能映射器需要数天的手动调整，这对没有系统专业知识的领域科学家来说是一个重大障碍。我们引入了一个框架，该框架使用生成优化来自动进行映射器开发，并利用标量性能指标之外的更丰富反馈。我们的方法具有Agent-System接口，它包括一个领域特定语言（DSL），以消除系统代码的底层复杂性并定义结构化的搜索空间，以及AutoGuide，它是一种解释原始执行输出为可操作反馈的机制。与传统的强化学习方法（如OpenTuner）不同，后者仅依赖于标量反馈，我们的方法在较少的迭代次数中找到更优的映射器。仅需10次迭代，即使在1000次迭代后，它的性能也优于OpenTuner，达到了3.8倍的速度提升。我们的方法找到的映射器在九个基准测试中实现了高达1.34倍的速度提升，并将调整时间从数天缩短到数分钟。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15625v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了现代科学发现中高性能计算在复杂建模和仿真中的重要作用。为提高并行程序性能的关键挑战是有效映射任务到处理器和数据到内存，这一过程由称为映射器的低级系统代码控制。开发高性能映射器需要数天的手动调整，对没有系统专业知识的领域科学家构成重大障碍。本文引入了一个使用生成优化自动映射器开发的框架，该框架利用丰富的反馈超越了标量性能度量。其特点包括Agent-System接口和AutoGuide机制，分别通过领域特定语言（DSL）抽象系统代码的低级复杂性并定义结构化搜索空间，以及将原始执行输出解释为可操作的反馈。该方法在较少的迭代次数中找到高性能映射器，优于仅依赖标量反馈的传统强化学习方法（如OpenTuner），并在仅10次迭代中实现了高达3.8倍的性能提升。此外，该方法找到了超过专家编写的映射器在九个基准测试中的速度提升高达1.34倍，并将调整时间从数天缩短到数分钟。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现代科学发现依赖于高性能计算进行复杂建模和仿真。</li>
<li>开发高性能映射器是一项挑战，需要数天的手动调整，对没有系统专业知识的领域科学家构成挑战。</li>
<li>介绍了一种使用生成优化自动映射器开发的框架。</li>
<li>该框架利用丰富的反馈超越了标量性能度量，通过Agent-System接口和AutoGuide机制实现了高效映射器的开发。</li>
<li>Agent-System接口包括一个领域特定语言（DSL），可以抽象系统代码的低级复杂性并定义结构化搜索空间。</li>
<li>该方法在较少的迭代次数中找到高性能映射器，优于传统强化学习方法（如OpenTuner）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15625">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-57a203442bb2fbcfbbcdcd1b81d1cea8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b31d30a3c2bf87ecd0b09ed66571f077.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fcb1d601ab0cab8c85e9b54146ee0e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f8fa1ec0b0ec3855774e11d92c766a0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Breaking-the-Curse-of-Multiagency-in-Robust-Multi-Agent-Reinforcement-Learning"><a href="#Breaking-the-Curse-of-Multiagency-in-Robust-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Breaking the Curse of Multiagency in Robust Multi-Agent Reinforcement   Learning"></a>Breaking the Curse of Multiagency in Robust Multi-Agent Reinforcement   Learning</h2><p><strong>Authors:Laixi Shi, Jingchu Gai, Eric Mazumdar, Yuejie Chi, Adam Wierman</strong></p>
<p>Standard multi-agent reinforcement learning (MARL) algorithms are vulnerable to sim-to-real gaps. To address this, distributionally robust Markov games (RMGs) have been proposed to enhance robustness in MARL by optimizing the worst-case performance when game dynamics shift within a prescribed uncertainty set. RMGs remains under-explored, from reasonable problem formulation to the development of sample-efficient algorithms. Two notorious and open challenges are the formulation of the uncertainty set and whether the corresponding RMGs can overcome the curse of multiagency, where the sample complexity scales exponentially with the number of agents. In this work, we propose a natural class of RMGs inspired by behavioral economics, where each agent’s uncertainty set is shaped by both the environment and the integrated behavior of other agents. We first establish the well-posedness of this class of RMGs by proving the existence of game-theoretic solutions such as robust Nash equilibria and coarse correlated equilibria (CCE). Assuming access to a generative model, we then introduce a sample-efficient algorithm for learning the CCE whose sample complexity scales polynomially with all relevant parameters. To the best of our knowledge, this is the first algorithm to break the curse of multiagency for RMGs, regardless of the uncertainty set formulation. </p>
<blockquote>
<p>标准的多智能体强化学习（MARL）算法容易受到仿真到现实的差距影响。为解决这一问题，提出了分布鲁棒马尔可夫博弈（RMGs），通过优化在规定的不确定性集内游戏动态变化时的最坏情况性能，增强MARL中的稳健性。RMGs的研究仍然不足，从合理的问题公式化到样本高效算法的开发都是如此。两个著名且公开的挑战是不确定性集合的公式化以及相应的RMGs是否能够克服多智能体的诅咒，即样本复杂度随智能体数量的增加而呈指数级增长。在这项工作中，我们提出了一种受行为经济学启发的自然RMGs类别，其中每个智能体的不确定性集是由环境和其它智能体的综合行为共同塑造的。我们首先通过建立这类RMGs的适定性来证明博弈论解的存在性，如鲁棒纳什均衡和粗糙相关均衡（CCE）。假设能够访问生成模型，我们然后介绍了一种学习CCE的样本高效算法，其样本复杂度与所有相关参数呈多项式增长。据我们所知，这是第一个打破了RMGs的多智能体诅咒的算法，无论不确定性集的公式化如何。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.20067v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一类受行为经济学启发的分布稳健性马尔可夫游戏（RMGs），旨在增强多智能体强化学习（MARL）的稳健性。通过考虑环境和其他智能体的综合行为来构建每个智能体的不确定性集，进而优化最坏情况下的性能。本文证明了这类RMGs的游戏理论解的存在性，如稳健的纳什均衡和粗糙相关均衡。在假设有生成模型的情况下，提出了一种样本效率高的学习CCE的算法，其样本复杂度与所有相关参数呈多项式增长，打破了RMGs中的多智能体诅咒，无论不确定性集的形式如何。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RMGs旨在增强MARL的稳健性，对抗模拟到实际的差距。</li>
<li>本文提出了受行为经济学启发的RMGs，构建智能体的不确定性集时考虑了环境和其他智能体的行为。</li>
<li>证明了这类RMGs的游戏理论解的存在性，如稳健的纳什均衡和CCE。</li>
<li>引入了一种样本效率高的算法来学习CCE，其样本复杂度与所有相关参数呈多项式增长。</li>
<li>该算法打破了RMGs中的多智能体诅咒，即样本复杂度不会随着智能体数量的增加而指数增长。</li>
<li>该算法对不确定性集的具体形式具有鲁棒性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.20067">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-612b85517245a1b4f9a3cb0445f2e829.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4d72ece8302284ea9bd3fbb5c026b138.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="InvAgent-A-Large-Language-Model-based-Multi-Agent-System-for-Inventory-Management-in-Supply-Chains"><a href="#InvAgent-A-Large-Language-Model-based-Multi-Agent-System-for-Inventory-Management-in-Supply-Chains" class="headerlink" title="InvAgent: A Large Language Model based Multi-Agent System for Inventory   Management in Supply Chains"></a>InvAgent: A Large Language Model based Multi-Agent System for Inventory   Management in Supply Chains</h2><p><strong>Authors:Yinzhu Quan, Zefang Liu</strong></p>
<p>Supply chain management (SCM) involves coordinating the flow of goods, information, and finances across various entities to deliver products efficiently. Effective inventory management is crucial in today’s volatile and uncertain world. Previous research has demonstrated the superiority of heuristic methods and reinforcement learning applications in inventory management. However, the application of large language models (LLMs) as autonomous agents in multi-agent systems for inventory management remains underexplored. This study introduces a novel approach using LLMs to manage multi-agent inventory systems. Leveraging their zero-shot learning capabilities, our model, InvAgent, enhances resilience and improves efficiency across the supply chain network. Our contributions include utilizing LLMs for zero-shot learning to enable adaptive and informed decision-making without prior training, providing explainability and clarity through chain-of-thought, and demonstrating dynamic adaptability to varying demand scenarios while reducing costs and preventing stockouts. Extensive evaluations across different scenarios highlight the efficiency of our model in SCM. </p>
<blockquote>
<p>供应链管理（SCM）涉及协调各实体之间的货物、信息和财务流动，以实现高效的产品交付。在如今这个波动和不确定的世界中，有效的库存管理至关重要。先前的研究已经证明了启发式方法和强化学习应用在库存管理中的优越性。然而，将大型语言模型（LLM）作为多智能体系统中的自主智能体应用于库存管理仍然未被充分探索。本研究介绍了一种使用LLM管理多智能体库存系统的新方法。利用他们的零样本学习能力，我们的模型InvAgent增强了供应链的弹性并提高了效率。我们的贡献包括利用LLM进行零样本学习，以实现未经训练的自适应和基于信息的决策制定，通过思维链提供解释性和清晰度，并展示对不断变化的需求场景的动态适应性，同时降低成本并防止缺货。在不同场景下的广泛评估突出了我们模型在供应链管理中的效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.11384v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于供应链管理的有效库存管理在现代动荡和不确定的世界中尤为关键。过去的研究已显示启发式方法和强化学习在库存管理中的应用优势。然而，关于在多智能体系统中使用大型语言模型（LLM）进行库存管理的自主性智能体的应用仍然鲜有研究。本研究采用了一种新型方法，即利用LLM管理多智能体库存系统。借助其零样本学习能力，我们的模型InvAgent提高了供应链网络的弹性和效率。其贡献包括利用LLM进行零样本学习以实现无需预先训练的适应性和智能决策制定，通过思维链提供解释性和清晰度，并展示对不断变化的需求场景的适应力，同时降低成本并避免缺货情况。在不同场景下的广泛评估凸显了我们的模型在供应链管理中的效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>供应链管理中有效的库存管理对于现代不稳定环境中的企业至关重要。</li>
<li>大型语言模型（LLM）在多智能体系统中的库存管理应用尚未得到充分探索。</li>
<li>本研究提出了一种使用LLM管理多智能体库存系统的创新方法。</li>
<li>LLM的零样本学习能力增强了供应链网络的弹性和效率。</li>
<li>模型InvAgent实现了无需预先训练的适应性和智能决策制定。</li>
<li>模型提供了清晰的解释性，能够展示对不断变化需求的适应力，并降低成本和避免缺货情况。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.11384">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1d426e836ceab4d1d7d5a77a6f03633e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1ca28b525f78fe1e7d063b9b7eabf00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e0679e6f0d387fde193dd1d29d476ec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-458798ffb30144fb0ce48310c0744a57.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="LNS2-RL-Combining-Multi-Agent-Reinforcement-Learning-with-Large-Neighborhood-Search-in-Multi-Agent-Path-Finding"><a href="#LNS2-RL-Combining-Multi-Agent-Reinforcement-Learning-with-Large-Neighborhood-Search-in-Multi-Agent-Path-Finding" class="headerlink" title="LNS2+RL: Combining Multi-Agent Reinforcement Learning with Large   Neighborhood Search in Multi-Agent Path Finding"></a>LNS2+RL: Combining Multi-Agent Reinforcement Learning with Large   Neighborhood Search in Multi-Agent Path Finding</h2><p><strong>Authors:Yutong Wang, Tanishq Duhan, Jiaoyang Li, Guillaume Sartoretti</strong></p>
<p>Multi-Agent Path Finding (MAPF) is a critical component of logistics and warehouse management, which focuses on planning collision-free paths for a team of robots in a known environment. Recent work introduced a novel MAPF approach, LNS2, which proposed to repair a quickly obtained set of infeasible paths via iterative replanning, by relying on a fast, yet lower-quality, prioritized planning (PP) algorithm. At the same time, there has been a recent push for Multi-Agent Reinforcement Learning (MARL) based MAPF algorithms, which exhibit improved cooperation over such PP algorithms, although inevitably remaining slower. In this paper, we introduce a new MAPF algorithm, LNS2+RL, which combines the distinct yet complementary characteristics of LNS2 and MARL to effectively balance their individual limitations and get the best from both worlds. During early iterations, LNS2+RL relies on MARL for low-level replanning, which we show eliminates collisions much more than a PP algorithm. There, our MARL-based planner allows agents to reason about past and future information to gradually learn cooperative decision-making through a finely designed curriculum learning. At later stages of planning, LNS2+RL adaptively switches to PP algorithm to quickly resolve the remaining collisions, naturally trading off solution quality (number of collisions in the solution) and computational efficiency. Our comprehensive experiments on high-agent-density tasks across various team sizes, world sizes, and map structures consistently demonstrate the superior performance of LNS2+RL compared to many MAPF algorithms, including LNS2, LaCAM, EECBS, and SCRIMP. In maps with complex structures, the advantages of LNS2+RL are particularly pronounced, with LNS2+RL achieving a success rate of over 50% in nearly half of the tested tasks, while that of LaCAM, EECBS and SCRIMP falls to 0%. </p>
<blockquote>
<p>多智能体路径查找（MAPF）是物流管理和仓库管理的重要组成部分，主要关注在已知环境中为机器人团队规划无碰撞路径。最近的工作引入了一种新的MAPF方法LNS2，它依靠一种快速但质量较低的优先规划（PP）算法，通过迭代重新规划来修复一组快速获得的不可行路径。与此同时，基于多智能体强化学习（MARL）的MAPF算法也备受关注，它们在合作方面表现出比PP算法更好的性能，但不可避免地速度较慢。在本文中，我们介绍了一种新的MAPF算法LNS2+RL，它结合了LNS2和MARL的独特且互补的特性，有效地平衡了各自的局限性，并融合了两者最好的部分。在早期迭代中，LNS2+RL依赖于MARL进行低级重新规划，我们证明这消除了比PP算法更多的碰撞。我们的基于MARL的规划器允许智能体根据过去和未来信息进行推理，并通过精心设计的学习课程逐步学习合作决策。在规划后期阶段，LNS2+RL自适应地切换到PP算法，快速解决剩余的碰撞问题，自然地平衡解决方案质量（解决方案中的碰撞次数）和计算效率。我们在各种任务规模、世界规模和地图结构上的全面实验显示，在高密度智能体任务中，LNS2+RL的性能始终优于许多MAPF算法，包括LNS2、LaCAM、EECBS和SCRIMP等。在具有复杂结构的地图上，LNS2+RL的优势尤为突出，在几乎一半的实验任务中成功率超过50%，而LaCAM、EECBS和SCRIMP等的成功率则在测试任务中降为0%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.17794v3">PDF</a> Accepted for presentation at AAAI 2025</p>
<p><strong>Summary</strong><br>     多智能体路径查找（MAPF）是物流及仓库管理的关键部分，涉及在已知环境中为机器人团队规划无碰撞路径。本文提出一种新的MAPF算法LNS2+RL，结合了LNS2和基于多智能体强化学习（MARL）的特性，有效平衡两者的局限，实现优势互补。早期迭代使用MARL进行低级重规划，后期则采用优先级规划（PP）快速解决剩余碰撞问题。相比其他MAPF算法，LNS2+RL在复杂地图结构中表现尤为出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LNS2+RL结合了LNS2和MARL的特点，旨在平衡解决方案质量和计算效率。</li>
<li>在早期迭代中，LNS2+RL使用MARL进行低级重规划，能有效减少碰撞。</li>
<li>MARL允许智能体考虑过去和未来信息，通过精心设计的课程学习逐步学习合作决策。</li>
<li>在后期规划阶段，LNS2+RL自适应切换到PP算法，以快速解决剩余碰撞问题。</li>
<li>LNS2+RL在高性能智能体密度的任务中表现出卓越的性能，尤其在复杂地图结构中表现更突出。</li>
<li>LNS2+RL成功率为超过一半的任务中超过一半以上，而其他算法如LaCAM、EECBS和SCRIMP在复杂地图中的成功率降低至零。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.17794">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e2191bfe4d188619fdf162eb24f9319c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8726e7b1fefd956d60fa7d8d3e5a4694.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-806a9c35ea08fec7040786d8ee2670cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47613049f32db06580aefc8791dcbf36.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Exploring-Prosocial-Irrationality-for-LLM-Agents-A-Social-Cognition-View"><a href="#Exploring-Prosocial-Irrationality-for-LLM-Agents-A-Social-Cognition-View" class="headerlink" title="Exploring Prosocial Irrationality for LLM Agents: A Social Cognition   View"></a>Exploring Prosocial Irrationality for LLM Agents: A Social Cognition   View</h2><p><strong>Authors:Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu</strong></p>
<p>Large language models (LLMs) have been shown to face hallucination issues due to the data they trained on often containing human bias; whether this is reflected in the decision-making process of LLM Agents remains under-explored. As LLM Agents are increasingly employed in intricate social environments, a pressing and natural question emerges: Can we utilize LLM Agents’ systematic hallucinations to mirror human cognitive biases, thus exhibiting irrational social intelligence? In this paper, we probe the irrational behavior among contemporary LLM Agents by melding practical social science experiments with theoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM Agents framework that utilizes hallucination properties to assess and enhance LLM Agents’ social intelligence through cognitive biases. Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities and highlighting the significance of hallucination properties. Additionally, the CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents. </p>
<blockquote>
<p>大型语言模型（LLM）由于训练数据常含有人类偏见，已出现幻觉问题；然而，这一问题是否反映在LLM代理的决策过程中尚未得到充分探索。随着LLM代理越来越多地被应用于复杂的社会环境，一个紧迫而自然的问题出现了：我们能否利用LLM代理的系统性幻觉来反映人类认知偏见，从而表现出非理性的社会智能？在本文中，我们通过结合实用的社会科学实验和理论见解，探究了当代LLM代理的非理性行为。具体来说，我们提出了CogMir，这是一个开放的多LLM代理框架，利用幻觉属性通过认知偏见来评估和增强LLM代理的社会智能。在CogMir子集上的实验结果表明，LLM代理和人类在不确定条件下的非理性亲社会决策表现出高度一致性，这强调了LLM代理作为社会实体的亲社会性，并突出了幻觉属性的重要性。此外，CogMir框架展示了一个有价值的平台潜力，可以鼓励更多关于LLM代理社会智能的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14744v3">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）面临的幻觉问题反映在人类偏见上。本研究探讨LLM代理人在社会环境中展现的不理性行为，并提出CogMir框架，利用幻觉属性评估并提升LLM代理人的社会智能。实验结果显示，LLM代理人在不确定条件下的决策与人类行为高度一致，凸显了幻觉属性的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在决策过程中会展现出幻觉问题，这源于训练数据中包含的人类偏见。</li>
<li>LLM代理人在社会环境中展现出不理性行为，这与人有一定的相似性。</li>
<li>CogMir框架旨在利用LLM代理人的幻觉属性来评估并提升其社会智能。</li>
<li>实验证明，LLM代理人在不确定条件下的决策与人类行为高度一致。</li>
<li>幻觉属性对于展示LLM代理人的社会智能具有重要意义。</li>
<li>CogMir框架有助于鼓励更多关于LLM代理人社会智能的研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14744">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d363e0a98f98af4693fa77361c76f1f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32b294058b6c442104e7806b28cc8b84.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="RH20T-P-A-Primitive-Level-Robotic-Dataset-Towards-Composable-Generalization-Agents"><a href="#RH20T-P-A-Primitive-Level-Robotic-Dataset-Towards-Composable-Generalization-Agents" class="headerlink" title="RH20T-P: A Primitive-Level Robotic Dataset Towards Composable   Generalization Agents"></a>RH20T-P: A Primitive-Level Robotic Dataset Towards Composable   Generalization Agents</h2><p><strong>Authors:Zeren Chen, Zhelun Shi, Xiaoya Lu, Lehan He, Sucheng Qian, Zhenfei Yin, Wanli Ouyang, Jing Shao, Yu Qiao, Cewu Lu, Lu Sheng</strong></p>
<p>Achieving generalizability in solving out-of-distribution tasks is one of the ultimate goals of learning robotic manipulation. Recent progress of Vision-Language Models (VLMs) has shown that VLM-based task planners can alleviate the difficulty of solving novel tasks, by decomposing the compounded tasks as a plan of sequentially executing primitive-level skills that have been already mastered. It is also promising for robotic manipulation to adapt such composable generalization ability, in the form of composable generalization agents (CGAs). However, the community lacks of reliable design of primitive skills and a sufficient amount of primitive-level data annotations. Therefore, we propose RH20T-P, a primitive-level robotic manipulation dataset, which contains about 38k video clips covering 67 diverse manipulation tasks in real-world scenarios. Each clip is manually annotated according to a set of meticulously designed primitive skills that are common in robotic manipulation. Furthermore, we standardize a plan-execute CGA paradigm and implement an exemplar baseline called RA-P on our RH20T-P, whose positive performance on solving unseen tasks validates that the proposed dataset can offer composable generalization ability to robotic manipulation agents. </p>
<blockquote>
<p>实现通用性来解决分布式外的任务是学习机器人操作的一个终极目标。最近的视觉语言模型（VLM）进展表明，基于VLM的任务规划器可以通过将复杂的任务分解为一系列已掌握的原始技能来减轻解决新任务的难度。对于机器人操作来说，采用可组合通用代理（CGA）的形式适应这种可组合通用能力也很有前景。然而，社区缺乏可靠的原始技能设计和足够的原始级别数据注释。因此，我们提出了RH20T-P，这是一个原始级别的机器人操作数据集，包含约3.8万个视频剪辑，涵盖现实场景中67种多样的操作任务。每个剪辑都根据精心设计的在机器人操作中常见的原始技能集进行手动注释。此外，我们对CGA范式进行了标准化，并在我们的RH20T-P上实施了RA-P示例基线，其在解决未见任务上的积极表现证明了我们提出的数据集可以为机器人操作代理提供可组合通用能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.19622v2">PDF</a> 18 pages, 11 figures, 7 tables. Accepted by NeurIPS 2024 Workshop</p>
<p><strong>Summary</strong></p>
<p>视觉语言模型（VLM）具备分解复杂任务并依次执行已掌握的基础技能的能力，这为解决新型任务提供了一种新的策略。这种策略在机器人操作领域展现出巨大潜力，有助于构建具有组合泛化能力的代理（CGAs）。当前领域面临基础技能设计和数据标注不足的问题，为此我们提出RH20T-P数据集，包含约3.8万条涵盖现实场景中67种不同操作任务的视频片段。每个片段都依据一套经过精心设计的通用基础技能进行手动标注。我们确定了CGA的标准范例并实现RA-P范例用于基准测试。它在解决未知任务上的积极表现验证了数据集可以为机器人操作代理提供组合泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLM能够分解复杂任务为一系列基础技能执行计划，为解决新型任务提供新思路。</li>
<li>VLM在机器人操作领域具备巨大潜力，有助于构建具有组合泛化能力的代理（CGAs）。但现实中仍缺乏可靠的基础技能设计和充足的基础级数据标注。为解决这一问题，我们提出了RH20T-P数据集。该数据集包含涵盖多种现实操作任务的手动标注视频片段，依据一套精心设计的通用基础技能进行标注。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.19622">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-22da279baaeac5b7526923b58ee5d8b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7bbb48395518e438b8205bf48f7c2aea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3025a78173f4d85ac3ca65b5af389e04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be69d1d56815b2109105be440761f9f6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a5c8a6e22aaf5f04a11526f9484ecc91.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Replication-proof-Bandit-Mechanism-Design-with-Bayesian-Agents"><a href="#Replication-proof-Bandit-Mechanism-Design-with-Bayesian-Agents" class="headerlink" title="Replication-proof Bandit Mechanism Design with Bayesian Agents"></a>Replication-proof Bandit Mechanism Design with Bayesian Agents</h2><p><strong>Authors:Suho Shin, Seyed A. Esmaeili, MohammadTaghi Hajiaghayi</strong></p>
<p>We study the problem of designing replication-proof bandit mechanisms when agents strategically register or replicate their own arms to maximize their payoff. Specifically, we consider Bayesian agents who only know the distribution from which their own arms’ mean rewards are sampled, unlike the original setting of by Shin et al. 2022. Interestingly, with Bayesian agents in stark contrast to the previous work, analyzing the replication-proofness of an algorithm becomes significantly complicated even in a single-agent setting. We provide sufficient and necessary conditions for an algorithm to be replication-proof in the single-agent setting, and present an algorithm that satisfies these properties. These results center around several analytical theorems that focus on \emph{comparing the expected regret of multiple bandit instances}, and therefore might be of independent interest since they have not been studied before to the best of our knowledge. We expand this result to the multi-agent setting, and provide a replication-proof algorithm for any problem instance. We finalize our result by proving its sublinear regret upper bound which matches that of Shin et al. 2022. </p>
<blockquote>
<p>我们研究了在设计复制证明的多臂赌博机制时，代理人为了最大化收益而策略性地注册或复制自己的手臂的问题。具体来说，我们考虑的是贝叶斯代理人，他们只知道自己的手臂平均奖励的采样分布，这与Shin等人2022年的原始设置不同。有趣的是，与之前的作品相比，带有贝叶斯代理人的分析，即使在单代理设置中，分析算法的防复制性也变得非常复杂。我们为算法在单代理设置中提供防复制的充分必要条件，并展示满足这些属性的算法。这些结果围绕几个分析定理展开，这些定理侧重于比较多个赌博实例的预期遗憾，因此它们可能是独立感兴趣的，因为据我们所知，之前没有人研究过。我们将结果扩展到多代理设置，并为任何问题实例提供防复制算法。最后，我们通过证明其匹配的次线性遗憾上限来完善我们的结果，这与Shin等人2022年的结果相符。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.16896v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究探讨了设计防止策略性复制的多臂赌博机机制的问题，特别考虑了战略性代理如何注册或复制自己的臂以最大化收益。与Shin et al. 2022的原始设置不同，我们考虑了仅知道自身手臂奖励均值分布采样的贝叶斯代理的情况。有趣的是，即使在单代理设置中，与之前的分析相比，分析算法的防复制性也变得更加复杂。我们提供了算法在单代理设置中具备防复制性的充分必要条件，并给出了满足这些属性的算法。这些结果集中在几个分析定理上，重点在于比较多个赌博机的预期遗憾，这可能是独立兴趣的研究主题，因为在现有知识范围内尚未进行过类似研究。我们将结果扩展到多代理设置，并为任何问题实例提供了防复制的算法。最后，我们证明了其匹配的遗憾上界是次线性的，与Shin et al. 2022的结果相符。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究了设计防止代理战略性复制的多臂赌博机机制的问题。</li>
<li>分析了贝叶斯代理在单代理设置中的防复制性，与之前的分析相比更加复杂。</li>
<li>提供了算法在单代理设置中具备防复制性的充分必要条件。</li>
<li>提出了一种满足这些属性的算法。</li>
<li>研究结果集中在比较多个赌博机的预期遗憾的分析定理上，这可能是独立兴趣的研究主题。</li>
<li>将研究结果扩展到多代理设置，并为任何问题实例提供了防复制的算法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.16896">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c1133166c532d7158345071fbc4d9ff3.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-05/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-05/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-05/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6e5552a211b287412ea0a65dabcbc44a.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-02-05  A Survey on Class-Agnostic Counting Advancements from Reference-Based   to Open-World Text-Guided Approaches
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-05/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1857492ad3c81d4f22d20216aa887010.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-02-05  SELMA A Speech-Enabled Language Model for Virtual Assistant   Interactions
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">11538.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
