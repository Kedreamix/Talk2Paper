<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-02-05  A Survey on Class-Agnostic Counting Advancements from Reference-Based   to Open-World Text-Guided Approaches">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6e5552a211b287412ea0a65dabcbc44a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-02-05
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    46 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-05-更新"><a href="#2025-02-05-更新" class="headerlink" title="2025-02-05 更新"></a>2025-02-05 更新</h1><h2 id="A-Survey-on-Class-Agnostic-Counting-Advancements-from-Reference-Based-to-Open-World-Text-Guided-Approaches"><a href="#A-Survey-on-Class-Agnostic-Counting-Advancements-from-Reference-Based-to-Open-World-Text-Guided-Approaches" class="headerlink" title="A Survey on Class-Agnostic Counting: Advancements from Reference-Based   to Open-World Text-Guided Approaches"></a>A Survey on Class-Agnostic Counting: Advancements from Reference-Based   to Open-World Text-Guided Approaches</h2><p><strong>Authors:Luca Ciampi, Ali Azmoudeh, Elif Ecem Akbaba, Erdi Sarıtaş, Ziya Ata Yazıcı, Hazım Kemal Ekenel, Giuseppe Amato, Fabrizio Falchi</strong></p>
<p>Object counting has recently shifted towards class-agnostic counting (CAC), which addresses the challenge of counting objects across arbitrary categories, tackling a critical need in versatile counting systems. While humans effortlessly identify and count objects from diverse categories without prior knowledge, most counting methods remain restricted to enumerating instances of known classes, requiring extensive labeled datasets for training, and struggling under open-vocabulary settings. Conversely, CAC aims to count objects belonging to classes never seen during training, typically operating in a few-shot setting. In this paper, for the first time, we review advancements in CAC methodologies, categorizing them into three paradigms based on how target object classes can be specified: reference-based, reference-less, and open-world text-guided. Reference-based approaches have set performance benchmarks using exemplar-guided mechanisms. Reference-less methods eliminate exemplar dependency by leveraging inherent image patterns. Finally, open-world text-guided methods utilize vision-language models, enabling object class descriptions through textual prompts, representing a flexible and appealing solution. We analyze state-of-the-art techniques and we report their results on existing gold standard benchmarks, comparing their performance and identifying and discussing their strengths and limitations. Persistent challenges – such as annotation dependency, scalability, and generalization – are discussed, alongside future directions. We believe this survey serves as a valuable resource for researchers to understand the progressive developments and contributions over time and the current state-of-the-art of CAC, suggesting insights for future directions and challenges to be addressed. </p>
<blockquote>
<p>对象计数最近已经转向类别未知计数（CAC），这解决了跨任意类别计数对象的挑战，满足了通用计数系统的迫切需求。虽然人类能够毫不费力地识别并计算来自不同类别的对象数量，而无需事先知识，但大多数计数方法仍然仅限于计算已知类别的实例数量，需要广泛的有标签数据集进行训练，并且在开放词汇设置下遇到挑战。相反，CAC旨在计算属于在训练期间未见过的类别的对象数量，通常运行在少量样本设置下。在本文中，我们首次回顾了CAC方法的发展，根据目标对象类别的指定方式将它们分为三种范式：基于参考、无参考和开放世界文本引导。基于参考的方法使用示例引导机制设定了性能基准。无参考方法通过利用固有的图像模式来消除对示例的依赖。最后，开放世界文本引导的方法利用视觉语言模型，通过文本提示实现对象类别描述，代表了一种灵活且有吸引力的解决方案。我们分析了最新技术，并在现有的黄金标准基准测试上报告了它们的成果，比较了它们的性能，并讨论和分析了它们的优缺点。还讨论了持续的挑战，如注释依赖性、可扩展性和泛化性，以及未来的研究方向。我们相信这份综述对于研究人员了解CAC的渐进发展、贡献和当前最新状态非常有价值，并为未来的研究方向和挑战提供了见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19184v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文综述了类未知计数（CAC）方法的最新进展，将其分为三种基于目标类别指定方式的范式：基于参考的、无参考的和开放世界文本引导的方法。基于参考的方法使用范例引导机制设定性能基准，无参考方法则利用图像内在模式消除范例依赖性，而开放世界文本引导的方法则利用视觉语言模型，通过文本提示实现目标类别描述，为CAC提供了一个灵活且具吸引力的解决方案。文章分析了最新技术并在现有标准基准上报告了结果，比较了性能，同时讨论和识别了各自的优缺点。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>类未知计数（CAC）是对象计数领域的新趋势，旨在解决跨任意类别计数对象的挑战。</li>
<li>CAC方法分为三个基于目标类别指定方式的范式：基于参考、无参考和开放世界文本引导。</li>
<li>基于参考的方法使用范例引导机制，设定了性能基准。</li>
<li>无参考方法利用图像内在模式，消除对范例的依赖。</li>
<li>开放世界文本引导的方法使用视觉语言模型，通过文本提示实现目标类别描述，为CAC提供灵活解决方案。</li>
<li>现有技术面临持续挑战，如注释依赖性、可扩展性和泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19184">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-145f23e1925289060febbd5617b9acf3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-491a1286959ee4ea224240df29efa28b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e267e207e324da9d3db85497c61affba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0932c7d84192b291dc3282be33cf9edc.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Enhancing-Code-Generation-for-Low-Resource-Languages-No-Silver-Bullet"><a href="#Enhancing-Code-Generation-for-Low-Resource-Languages-No-Silver-Bullet" class="headerlink" title="Enhancing Code Generation for Low-Resource Languages: No Silver Bullet"></a>Enhancing Code Generation for Low-Resource Languages: No Silver Bullet</h2><p><strong>Authors:Alessandro Giagnorio, Alberto Martin-Lopez, Gabriele Bavota</strong></p>
<p>The advent of Large Language Models (LLMs) has significantly advanced the field of automated code generation. LLMs rely on large and diverse datasets to learn syntax, semantics, and usage patterns of programming languages. For low-resource languages (i.e., niche programming languages characterized by the scarcity of training data), the limited availability of such data hampers the models’ ability to generalize effectively, resulting in poorer code generation performance as compared to high-resource languages. For this reason, there is a quest for techniques able to close this performance gap. We present an empirical study investigating the effectiveness of several approaches for boosting LLMs’ performance on low-resource languages, namely: (i) a classic fine-tuning, which is however capped in size by the scarcity of training data; (ii) three variants of in-context learning, with prompts crafted to provide the LLM with additional information about the low-resource language (e.g., few-shot examples showcasing features of the targeted language); and (iii) a pre-training objective teaching the model how to translate between high- and low-resource languages. The context of our study are two low-resource languages (R and Racket) and six LLMs having different architectures and sizes. Our findings reveal that a fine-tuning is usually the best choice for smaller LLMs, possibly due to the fact that even a small dataset is sufficient to train their limited number of parameters. With the increase in size of the models, in-context learning becomes more and more effective, representing a safe and cheap bet (i.e., it always helps, but with different magnitudes). Differently, very large LLMs may deteriorate their performance on low-resource languages when fine-tuning is performed, possibly due to the lack of enough data needed to effectively update their weights. </p>
<blockquote>
<p>大型语言模型（LLM）的出现，极大地推动了自动化代码生成领域的发展。LLM依赖于大型和多样化的数据集来学习编程语言的语法、语义和使用模式。对于资源匮乏的语言（即训练数据稀缺的专有编程语言），此类数据的有限可用性阻碍了模型有效地推广，与资源丰富的语言相比，导致较差的代码生成性能。因此，人们正在寻找能够缩小这一性能差距的技术。我们进行了一项实证研究，探讨了提高LLM在资源匮乏的语言上的性能的几种方法的有效性，即：（i）经典的微调，但受限于训练数据的稀缺性；（ii）三种上下文学习变体，通过提示为LLM提供有关低资源语言的额外信息（例如，展示目标语言特征的几个示例）；（iii）一个预训练目标，教会模型如何在高资源和低资源语言之间进行翻译。我们研究的背景是两种资源匮乏的语言（R和Racket）和六种具有不同架构和规模的LLM。我们的研究结果表明，对于较小的LLM，微调通常是最佳选择，这可能是由于即使小数据集也足以训练其有限数量的参数。随着模型规模的增加，上下文学习变得越来越有效，是一个安全且经济的选择（即它总是有帮助，但程度不同）。不同的是，对于非常大的LLM，当进行微调时，其在资源匮乏的语言上的性能可能会下降，这可能是由于缺乏足够的数据来有效地更新其权重。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19085v1">PDF</a> Accepted at ICPC’25</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在自动代码生成领域取得了显著进展，但在处理低资源语言时面临挑战。针对这一问题，本文探讨了提升LLM在低资源语言上性能的方法，包括微调、上下文学习和预训练目标等策略。研究发现，对于小型LLM，微调通常是最佳选择；随着模型规模的增加，上下文学习变得更加有效；而对于非常大的LLM，过度微调可能导致性能下降。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在自动代码生成上表现卓越，但在处理低资源语言时面临挑战。</li>
<li>有限的训练数据限制了LLM在低资源语言上的性能。</li>
<li>经典微调受限于训练数据的稀缺性，但对于小型LLM可能仍是最优选择。</li>
<li>随着模型规模的增加，上下文学习变得更加有效。</li>
<li>预训练目标可以帮助模型在低资源语言上进行翻译。</li>
<li>对于非常大的LLM，过度微调可能导致性能下降。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19085">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a9297ff527b475a1b96824d4e1b89c76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb7c113e531204aad160fc9d5c3dc5c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c7b484fc65ea24d2fa2634289945db1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Memory-Efficient-Fine-Tuning-of-Transformers-via-Token-Selection"><a href="#Memory-Efficient-Fine-Tuning-of-Transformers-via-Token-Selection" class="headerlink" title="Memory-Efficient Fine-Tuning of Transformers via Token Selection"></a>Memory-Efficient Fine-Tuning of Transformers via Token Selection</h2><p><strong>Authors:Antoine Simoulin, Namyong Park, Xiaoyi Liu, Grey Yang</strong></p>
<p>Fine-tuning provides an effective means to specialize pre-trained models for various downstream tasks. However, fine-tuning often incurs high memory overhead, especially for large transformer-based models, such as LLMs. While existing methods may reduce certain parts of the memory required for fine-tuning, they still require caching all intermediate activations computed in the forward pass to update weights during the backward pass. In this work, we develop TokenTune, a method to reduce memory usage, specifically the memory to store intermediate activations, in the fine-tuning of transformer-based models. During the backward pass, TokenTune approximates the gradient computation by backpropagating through just a subset of input tokens. Thus, with TokenTune, only a subset of intermediate activations are cached during the forward pass. Also, TokenTune can be easily combined with existing methods like LoRA, further reducing the memory cost. We evaluate our approach on pre-trained transformer models with up to billions of parameters, considering the performance on multiple downstream tasks such as text classification and question answering in a few-shot learning setup. Overall, TokenTune achieves performance on par with full fine-tuning or representative memory-efficient fine-tuning methods, while greatly reducing the memory footprint, especially when combined with other methods with complementary memory reduction mechanisms. We hope that our approach will facilitate the fine-tuning of large transformers, in specializing them for specific domains or co-training them with other neural components from a larger system. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/tokentune">https://github.com/facebookresearch/tokentune</a>. </p>
<blockquote>
<p>微调（Fine-tuning）为针对各种下游任务的专业化预训练模型提供了有效的手段。然而，微调通常会引起较高的内存开销，尤其是对于基于大型变换模型的预训练模型（如大型语言模型）。尽管现有方法可能减少了微调所需的某些内存部分，但它们仍然需要在前向传播过程中缓存所有中间激活的计算结果，以在反向传播过程中更新权重。在这项工作中，我们开发了TokenTune，一种旨在减少基于变换模型的微调过程中的内存使用量的方法，特别是用于存储中间激活值的内存。在反向传播过程中，TokenTune通过仅通过输入令牌的一个子集进行反向传播来近似梯度计算。因此，使用TokenTune时，仅在正向传播过程中缓存一小部分中间激活值。此外，TokenTune还可以轻松地与LoRA等现有方法相结合，进一步降低内存成本。我们在具有数十亿参数的预训练变换模型上评估了我们的方法，并考虑了其在多个下游任务（如文本分类和问题回答）上的性能表现。总体而言，TokenTune的性能与完全微调或代表性内存高效的微调方法相当，同时大大降低了内存占用空间，尤其是与其他具有互补内存减少机制的方法相结合时。我们希望我们的方法能够促进大型转换器的微调工作，在特定领域对其进行专业化处理或与其他神经网络组件从更大的系统进行协同训练。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/tokentune%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/facebookresearch/tokentune获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18824v1">PDF</a> EMNLP 2024</p>
<p><strong>Summary</strong></p>
<p>本文提出了TokenTune方法，用于减少微调基于转换器模型时的内存使用。通过仅缓存输入令牌子集的部分中间激活值，TokenTune能够在微调过程中降低内存消耗。此方法可与现有方法（如LoRA）结合使用，进一步降低内存成本。在多项下游任务（如文本分类和问答）的少量学习设置中，TokenTune实现了与完全微调或代表性内存高效微调方法相当的性能，同时大大减少了内存占用。我们的代码已在GitHub上公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TokenTune方法专注于减少微调基于转换器模型时的内存使用。</li>
<li>通过仅缓存输入令牌子集的部分中间激活值，降低了内存消耗。</li>
<li>TokenTune可以与现有方法（如LoRA）结合使用，以进一步提高内存效率。</li>
<li>在多项下游任务中，TokenTune实现了与完全微调或内存高效微调方法相当的性能。</li>
<li>TokenTune特别适用于大型转换模型的微调。</li>
<li>TokenTune有助于在少量学习设置中进行模型微调。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18824">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1e1c487c55b8a6062c9ffb1e4484fe20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b6cb4b39d2d1e275deb8d539c58d398.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cbde7ae77eaef35e3dbb34bee1ab0457.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c416cc18ef2326782798ac2f2b0649f7.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Divergent-Emotional-Patterns-in-Disinformation-on-Social-Media-An-Analysis-of-Tweets-and-TikToks-about-the-DANA-in-Valencia"><a href="#Divergent-Emotional-Patterns-in-Disinformation-on-Social-Media-An-Analysis-of-Tweets-and-TikToks-about-the-DANA-in-Valencia" class="headerlink" title="Divergent Emotional Patterns in Disinformation on Social Media? An   Analysis of Tweets and TikToks about the DANA in Valencia"></a>Divergent Emotional Patterns in Disinformation on Social Media? An   Analysis of Tweets and TikToks about the DANA in Valencia</h2><p><strong>Authors:Iván Arcos, Paolo Rosso, Ramón Salaverría</strong></p>
<p>This study investigates the dissemination of disinformation on social media platforms during the DANA event (DANA is a Spanish acronym for Depresion Aislada en Niveles Altos, translating to high-altitude isolated depression) that resulted in extremely heavy rainfall and devastating floods in Valencia, Spain, on October 29, 2024. We created a novel dataset of 650 TikTok and X posts, which was manually annotated to differentiate between disinformation and trustworthy content. Additionally, a Few-Shot annotation approach with GPT-4o achieved substantial agreement (Cohen’s kappa of 0.684) with manual labels. Emotion analysis revealed that disinformation on X is mainly associated with increased sadness and fear, while on TikTok, it correlates with higher levels of anger and disgust. Linguistic analysis using the LIWC dictionary showed that trustworthy content utilizes more articulate and factual language, whereas disinformation employs negations, perceptual words, and personal anecdotes to appear credible. Audio analysis of TikTok posts highlighted distinct patterns: trustworthy audios featured brighter tones and robotic or monotone narration, promoting clarity and credibility, while disinformation audios leveraged tonal variation, emotional depth, and manipulative musical elements to amplify engagement. In detection models, SVM+TF-IDF achieved the highest F1-Score, excelling with limited data. Incorporating audio features into roberta-large-bne improved both Accuracy and F1-Score, surpassing its text-only counterpart and SVM in Accuracy. GPT-4o Few-Shot also performed well, showcasing the potential of large language models for automated disinformation detection. These findings demonstrate the importance of leveraging both textual and audio features for improved disinformation detection on multimodal platforms like TikTok. </p>
<blockquote>
<p>本研究调查了在DANA事件（DANA是西班牙语中Depresion Aislada en Niveles Altos的缩写，意为高海拔孤立性抑郁症）期间社交媒体平台上传播的错误信息。该事件导致2024年10月29日西班牙瓦伦西亚出现极端强降雨和破坏性洪水。我们创建了包含650个TikTok和X帖子的新型数据集，并进行了手动注释，以区分错误信息和可靠内容。另外，使用GPT-4o的Few-Shot标注方法达到了与手动标签的实质一致性（Cohen的kappa值为0.684）。情绪分析显示，X上的错误信息主要与悲伤和恐惧感增加有关，而在TikTok上，它与愤怒和厌恶程度较高相关。使用LIWC词典进行的语言分析表明，可靠的内容使用更艺术和事实性的语言，而错误信息则使用否定词、感知词汇和个人轶事来显得可信。对TikTok帖子的音频分析突显了明显的模式：可靠音频的音调更明亮，采用机器人或单调的叙述，以促进清晰度和可信度；而错误信息的音频则利用音调变化、情感深度和操纵性的音乐元素来增强参与度。在检测模型中，SVM+TF-IDF获得了最高的F1分数，在有限的数据下表现卓越。将音频特性融入roberta-large-bne中提高了准确率和F1分数，超越了其纯文本对应模型和SVM的准确率。GPT-4o的Few-Shot表现也相当不错，展示了大型语言模型在自动错误信息检测中的潜力。这些发现表明，利用文本和音频特征对于改进TikTok等多模式平台上的错误信息检测至关重要。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18640v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该研究探讨了DANA事件（高海拔孤立性抑郁症的缩写，该事件导致西班牙瓦伦西亚于2024年10月29日发生极端强降雨和破坏性洪水）期间社交媒体平台上假消息的扩散。研究者创建了一个包含650篇TikTok和X平台的帖子的数据集，并进行手动标注以区分假消息和可靠内容。情感分析发现，X平台上的假消息主要与悲伤和恐惧感增加有关，而TikTok上的假消息则与愤怒和厌恶情绪更高相关。语言分析显示，可靠内容使用更艺术和事实性的语言，而假消息则使用否定词、感知词汇和个人轶事来显得可信。音频分析表明，可信的音频具有更明亮的音调和机器人或单调的叙述，而假消息的音频则利用音调变化和操纵性音乐元素来增加参与度。检测模型中，SVM+TF-IDF达到最高F1分数，结合音频特征的roberta-large-bne在准确率和F1分数上有所提升。GPT-4o的Few-Shot表现也良好，突显大型语言模型在自动假消息检测中的潜力。研究结果表明，利用文本和音频特征对于改进TikTok等多模式平台上的假消息检测至关重要。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究聚焦于DANA事件期间社交媒体上的假消息传播。</li>
<li>创建了包含TikTok和X平台帖子的数据集，并进行手动标注。</li>
<li>情感分析显示不同平台上假消息与不同情绪的相关性。</li>
<li>语言分析揭示可靠内容和假消息在语言使用上的差异。</li>
<li>音频分析显示，假消息的音频具有特定的特征和模式。</li>
<li>SVM+TF-IDF和结合音频特征的roberta-large-bne在检测模型中表现最佳。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18640">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-477b838eafa0b4ac2b6e6d4c5037b922.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f396db46548f0da08690d8e3c93c6ef0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e5552a211b287412ea0a65dabcbc44a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-64cee2bcb1496e62434fe9b45128e222.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7aa006c3c0eea4f753016d7a3abdb734.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0395d2e25df8372c64803ef96f5d25e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ca162cc9c5c6f210867d310788dcaf1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-adbc311e336567736e20ce409dd8a3ee.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DWTNeRF-Boosting-Few-shot-Neural-Radiance-Fields-via-Discrete-Wavelet-Transform"><a href="#DWTNeRF-Boosting-Few-shot-Neural-Radiance-Fields-via-Discrete-Wavelet-Transform" class="headerlink" title="DWTNeRF: Boosting Few-shot Neural Radiance Fields via Discrete Wavelet   Transform"></a>DWTNeRF: Boosting Few-shot Neural Radiance Fields via Discrete Wavelet   Transform</h2><p><strong>Authors:Hung Nguyen, Blark Runfa Li, Truong Nguyen</strong></p>
<p>Neural Radiance Fields (NeRF) has achieved superior performance in novel view synthesis and 3D scene representation, but its practical applications are hindered by slow convergence and reliance on dense training views. To this end, we present DWTNeRF, a unified framework based on Instant-NGP’s fast-training hash encoding. It is coupled with regularization terms designed for few-shot NeRF, which operates on sparse training views. Our DWTNeRF additionally includes a novel Discrete Wavelet loss that allows explicit prioritization of low frequencies directly in the training objective, reducing few-shot NeRF’s overfitting on high frequencies in earlier training stages. We also introduce a model-based approach, based on multi-head attention, that is compatible with INGP, which are sensitive to architectural changes. On the 3-shot LLFF benchmark, DWTNeRF outperforms Vanilla INGP by 15.07% in PSNR, 24.45% in SSIM and 36.30% in LPIPS. Our approach encourages a re-thinking of current few-shot approaches for fast-converging implicit representations like INGP or 3DGS. </p>
<blockquote>
<p>神经辐射场（NeRF）在新型视图合成和3D场景表示方面取得了卓越的性能，但其实际应用受到了收敛速度慢和依赖密集训练视图的影响。为此，我们提出了DWTNeRF，这是一个基于Instant-NGP快速训练哈希编码的统一框架。它与针对少量射击NeRF设计的正则化术语相结合，在稀疏训练视图上运行。我们的DWTNeRF还包括一种新型离散小波损失，允许在训练目标中直接明确优先处理低频，从而减少早期训练阶段中少量射击NeRF对高频的过拟合。我们还介绍了一种基于多头注意力的模型方法，该方法与INGP兼容，对架构变化敏感。在3次拍摄的LLFF基准测试中，DWTNeRF在PSNR上比Vanilla INGP高出15.07%，在SSIM上高出24.45%，在LPIPS上高出36.30%。我们的方法鼓励重新思考当前的少量射击方法，以快速收敛隐式表示形式，如INGP或3DGS。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12637v2">PDF</a> 17 pages, 13 figures, 8 tables</p>
<p><strong>Summary</strong></p>
<p>NeRF技术在新视角合成和三维场景表示方面表现出卓越性能，但其实际应用受到缓慢收敛和依赖密集训练视图的限制。为解决这些问题，提出了DWTNeRF统一框架，结合Instant-NGP的快速训练哈希编码，设计用于少量射击NeRF的正则化术语，并在稀疏训练视图上运行。DWTNeRF还包括一种新的离散小波损失，允许在训练目标中显式优先考虑低频信息，减少早期训练阶段对高频的过拟合。在3次拍摄的LLFF基准测试中，DWTNeRF在PSNR上较Vanilla INGP高出15.07%，在SSIM上高出24.45%，在LPIPS上高出36.30%。DWTNeRF为当前少量拍摄隐式表示方法的快速收敛提供了一个新的思考方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRF技术在处理和展示三维场景上具有优越性能，但存在收敛慢和依赖密集训练视图的问题。</li>
<li>DWTNeRF是一个基于Instant-NGP的快速训练哈希编码的统一框架，解决了NeRF技术的收敛问题。它能在稀疏的训练视图上运行。</li>
<li>DWTNeRF框架包括一种新颖的离散小波损失技术，能够在训练目标中显式优先考虑低频信息，减少了早期训练阶段对高频的过拟合。</li>
<li>在基准测试中，DWTNeRF的性能显著优于传统的NeRF技术。具体来说，它在PSNR、SSIM和LPIPS等关键指标上取得了显著的提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12637">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-322630a2fc4fa86cd69f3f8b10e5c9b4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1944b6685006aa1cd5980d9714859fa7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc499b0fb467fba02fd43e2a14a45bb3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-776a01eb9971f1504f3874ddaffc5062.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1448d40ba7d1e67623aadd084d03038.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Self-Instruct-Few-Shot-Jailbreaking-Decompose-the-Attack-into-Pattern-and-Behavior-Learning"><a href="#Self-Instruct-Few-Shot-Jailbreaking-Decompose-the-Attack-into-Pattern-and-Behavior-Learning" class="headerlink" title="Self-Instruct Few-Shot Jailbreaking: Decompose the Attack into Pattern   and Behavior Learning"></a>Self-Instruct Few-Shot Jailbreaking: Decompose the Attack into Pattern   and Behavior Learning</h2><p><strong>Authors:Jiaqi Hua, Wanxu Wei</strong></p>
<p>Recently, several works have been conducted on jailbreaking Large Language Models (LLMs) with few-shot malicious demos. In particular, Zheng et al. focus on improving the efficiency of Few-Shot Jailbreaking (FSJ) by injecting special tokens into the demos and employing demo-level random search, known as Improved Few-Shot Jailbreaking (I-FSJ). Nevertheless, we notice that this method may still require a long context to jailbreak advanced models e.g. 32 shots of demos for Meta-Llama-3-8B-Instruct (Llama-3) \cite{llama3modelcard}. In this paper, we discuss the limitations of I-FSJ and propose Self-Instruct Few-Shot Jailbreaking (Self-Instruct-FSJ) facilitated with the demo-level greedy search. This framework decomposes the FSJ attack into pattern and behavior learning to exploit the model’s vulnerabilities in a more generalized and efficient way. We conduct elaborate experiments to evaluate our method on common open-source models and compare it with baseline algorithms. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/iphosi/Self-Instruct-FSJ">https://github.com/iphosi/Self-Instruct-FSJ</a>. </p>
<blockquote>
<p>最近，有若干研究利用少量恶意演示来破解大型语言模型（LLM）。特别是，Zheng等人专注于通过向演示中注入特殊令牌并采用演示级随机搜索来提高少量演示破解（FSJ）的效率，这被称为改进型少量演示破解（I-FSJ）。然而，我们注意到此方法可能仍需要较长的上下文来破解高级模型，例如在Meta-Llama-3-8B-Instruct（Llama-3）中需要32个演示镜头 \cite{llama3modelcard}。在本文中，我们讨论了I-FSJ的局限性，并提出了借助演示级贪心搜索的辅助自我指令少量演示破解（Self-Instruct-FSJ）框架。此框架将FSJ攻击分解为模式学习和行为学习，以更通用和高效的方式利用模型的漏洞。我们进行了精心设计的实验，以评估我们的方法在常用开源模型上的表现，并与基线算法进行了比较。我们的代码位于 <a target="_blank" rel="noopener" href="https://github.com/iphosi/Self-Instruct-FSJ%E3%80%82">https://github.com/iphosi/Self-Instruct-FSJ。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07959v2">PDF</a> </p>
<p><strong>Summary</strong><br>文本聚焦于如何通过破解大型语言模型来提升模型效率的问题。文章讨论了现有的Improved Few-Shot Jailbreaking方法在某些高级模型上的局限性，并介绍了Self-Instruct Few-Shot Jailbreaking框架，该框架通过分解FSJ攻击的模式和行为学习，以更通用和高效的方式利用模型的漏洞。通过实验验证该方法的性能并公开了源代码。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Large Language Models (LLMs)的破解方法一直是研究热点，但现有的Improved Few-Shot Jailbreaking方法在破解高级模型时仍有局限性。</li>
<li>Self-Instruct Few-Shot Jailbreaking框架被提出，通过分解FSJ攻击的模式和行为学习，以更通用和高效的方式利用模型的漏洞。</li>
<li>该框架包括在演示中注入特殊令牌并采用演示级别的贪婪搜索来提高效率。</li>
<li>实验验证了该框架在公共开源模型上的性能，并与基线算法进行了比较。</li>
<li>文章的源代码已公开，可供进一步研究和参考。</li>
<li>Self-Instruct Few-Shot Jailbreaking可能有助于提升大型语言模型的效率和安全性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07959">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9aff4786b83449cfffcb835e599c1838.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-239d4c4889fb269a2d731a757422bfaf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ad6362e32a52bd94f6aebe45ef7a9bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b6f252c9c6ff95b63182db19deb1791.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c03cfcdff2bcbaf223bacefc80cb6654.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Transforming-Role-Classification-in-Scientific-Teams-Using-LLMs-and-Advanced-Predictive-Analytics"><a href="#Transforming-Role-Classification-in-Scientific-Teams-Using-LLMs-and-Advanced-Predictive-Analytics" class="headerlink" title="Transforming Role Classification in Scientific Teams Using LLMs and   Advanced Predictive Analytics"></a>Transforming Role Classification in Scientific Teams Using LLMs and   Advanced Predictive Analytics</h2><p><strong>Authors:Wonduk Seo, Yi Bu</strong></p>
<p>Scientific team dynamics are critical in determining the nature and impact of research outputs. However, existing methods for classifying author roles based on self-reports and clustering lack comprehensive contextual analysis of contributions. Thus, we present a transformative approach to classifying author roles in scientific teams using advanced large language models (LLMs), which offers a more refined analysis compared to traditional clustering methods. Specifically, we seek to complement and enhance these traditional methods by utilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting, we categorize author roles and demonstrate that GPT-4 outperforms other models across multiple categories, surpassing traditional approaches such as XGBoost and BERT. Our methodology also includes building a predictive deep learning model using 10 features. By training this model on a dataset derived from the OpenAlex database, which provides detailed metadata on academic publications – such as author-publication history, author affiliation, research topics, and citation counts – we achieve an F1 score of 0.76, demonstrating robust classification of author roles. </p>
<blockquote>
<p>科研团队的动态在决定研究成果的性质和影响方面至关重要。然而，现有的基于自我报告和聚类的作者角色分类方法缺乏对贡献的全面上下文分析。因此，我们提出了一种利用先进的大型语言模型（LLMs）对科研团队中的作者角色进行分类的变革性方法，与传统聚类方法相比，它提供了更精细的分析。具体来说，我们希望通过利用开源和专有的大型语言模型（如GPT-4、Llama 3 70B、Llama 2 70B和Mistral 7x8B）来分类角色，以补充和增强这些方法。通过少量的提示，我们对作者角色进行了分类，并证明GPT-4在多个类别中优于其他模型，超越了传统的XGBoost和BERT等方法。我们的方法还包括建立一个使用10个特征的预测深度学习模型。通过在OpenAlex数据库衍生的数据集上训练模型（该数据库提供有关学术出版物（如作者出版历史、作者归属关系、研究主题和引用计数）的详细元数据），我们实现了F1分数为0.76，证明了作者角色分类的稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07267v2">PDF</a> 16 pages, 5 figures, 3 tables</p>
<p><strong>摘要</strong></p>
<p>科研团队内部的动态对研究产出的性质和影响力起关键作用。现有的基于自我报告和聚类的作者角色分类方法缺乏全面的贡献上下文分析。因此，我们提出了一种利用先进的大型语言模型（LLM）对科研团队中的作者角色进行分类的变革性方法，相比传统的聚类方法，它提供了更为精细的分析。我们旨在结合并优化这些传统方法，利用开源和专有LLM（如GPT-4、Llama3 70B、Llama2 70B和Mistral 7x8B）进行角色分类。通过小样本提示的方法，我们对作者角色进行了分类，并证明GPT-4在多个类别中表现优于其他模型，超越了传统的如XGBoost和BERT的方法。我们的方法还包括利用OpenAlex数据库提供的详细元数据构建预测深度学习模型，对作者角色进行分类，实现了F1分数为0.76。该模型训练数据集包含作者出版历史、作者隶属关系、研究主题和引用计数等特征。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>科研团队动态对研究输出的性质和影响力至关重要。</li>
<li>现有作者角色分类方法主要基于自我报告和聚类，缺乏全面的贡献上下文分析。</li>
<li>利用先进的大型语言模型（LLM）进行作者角色分类是一种更精细的方法。</li>
<li>GPT-4在多个类别中表现优于其他模型，用于角色分类。</li>
<li>GPT-4的优越性能超越了传统的如XGBoost和BERT的方法。</li>
<li>利用OpenAlex数据库的详细元数据构建了预测深度学习模型，实现了F1分数为0.76的作者角色分类。</li>
<li>模型训练数据集包含丰富的特征，如作者出版历史、作者隶属关系、研究主题和引用计数等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07267">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e8a299164449d30c742d0b21507a37ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a6688da5e0d419e2d4d3d259c337d2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b02efb3fc36694be89eef2cb2d49fd4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SEED4D-A-Synthetic-Ego–Exo-Dynamic-4D-Data-Generator-Driving-Dataset-and-Benchmark"><a href="#SEED4D-A-Synthetic-Ego–Exo-Dynamic-4D-Data-Generator-Driving-Dataset-and-Benchmark" class="headerlink" title="SEED4D: A Synthetic Ego–Exo Dynamic 4D Data Generator, Driving Dataset   and Benchmark"></a>SEED4D: A Synthetic Ego–Exo Dynamic 4D Data Generator, Driving Dataset   and Benchmark</h2><p><strong>Authors:Marius Kästingschäfer, Théo Gieruc, Sebastian Bernhard, Dylan Campbell, Eldar Insafutdinov, Eyvaz Najafli, Thomas Brox</strong></p>
<p>Models for egocentric 3D and 4D reconstruction, including few-shot interpolation and extrapolation settings, can benefit from having images from exocentric viewpoints as supervision signals. No existing dataset provides the necessary mixture of complex, dynamic, and multi-view data. To facilitate the development of 3D and 4D reconstruction methods in the autonomous driving context, we propose a Synthetic Ego–Exo Dynamic 4D (SEED4D) data generator and dataset. We present a customizable, easy-to-use data generator for spatio-temporal multi-view data creation. Our open-source data generator allows the creation of synthetic data for camera setups commonly used in the NuScenes, KITTI360, and Waymo datasets. Additionally, SEED4D encompasses two large-scale multi-view synthetic urban scene datasets. Our static (3D) dataset encompasses 212k inward- and outward-facing vehicle images from 2k scenes, while our dynamic (4D) dataset contains 16.8M images from 10k trajectories, each sampled at 100 points in time with egocentric images, exocentric images, and LiDAR data. The datasets and the data generator can be found at <a target="_blank" rel="noopener" href="https://seed4d.github.io/">https://seed4d.github.io/</a>. </p>
<blockquote>
<p>对于以自我为中心（egocentric）的3D和4D重建模型，包括小样本插值和外推设置，可以从异位视角的图像中获得监督信号，从而受益。目前没有数据集能够提供复杂、动态和多视角数据所必需的混合。为了促进在自动驾驶背景下3D和4D重建方法的发展，我们提出了合成自我异位动态四维（SEED4D）数据生成器和数据集。我们展示了一个可定制的、易于使用的多视角时空数据生成器。我们的开源数据生成器可以创建常用于NuScenes、KITTI360和Waymo数据集的相机设置合成数据。此外，SEED4D包含两个大规模的多视角合成城市场景数据集。我们的静态（3D）数据集包含来自2k场景的21.2万张车内和车外车辆图像，而我们的动态（4D）数据集包含来自10k轨迹的1680万张图像，每条轨迹在时间上采样100个点，包括以自我为中心的图像、以异位为中心的图像和激光雷达数据。数据集和数据生成器可在<a target="_blank" rel="noopener" href="https://seed4d.github.io/%E6%89%BE%E5%88%B0%E3%80%82">https://seed4d.github.io/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00730v2">PDF</a> WACV 2025. Project page: <a target="_blank" rel="noopener" href="https://seed4d.github.io/">https://seed4d.github.io/</a>. Code:   <a target="_blank" rel="noopener" href="https://github.com/continental/seed4d">https://github.com/continental/seed4d</a></p>
<p><strong>Summary</strong></p>
<p>该文本介绍了一个名为SEED4D的合成数据集生成器，它能生成适用于自动驾驶场景的egocentric 3D和4D重建模型所需的合成数据。该生成器可以为复杂的动态多视角数据创建混合体，从而促进模型的训练和发展。SEED4D包含两个大规模的多视角合成城市场景数据集，涵盖了静态的3D数据集和动态的4D数据集。此外，数据生成器易于使用且可定制，适用于NuScenes、KITTI360和Waymo等常用的相机设置。数据集和生成器可在SEED4D官方网站下载和使用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SEED4D是一个合成数据集生成器，用于生成适用于自动驾驶场景的egocentric 3D和4D重建模型的数据。</li>
<li>生成的数据包括复杂的动态多视角数据，有助于提高模型的训练效果。</li>
<li>SEED4D包含静态的3D数据集和动态的4D数据集，涵盖大规模的多视角合成城市场景。</li>
<li>数据生成器适用于常见的相机设置，如NuScenes、KITTI360和Waymo。</li>
<li>数据生成器具有可定制性和易用性。</li>
<li>SEED4D官网提供数据集和生成器的下载和使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00730">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2b5d474f6113ca437186996097352132.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31bc860111de28b14927ccbc01ca476a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62c4109857afd76eb0c52894f945bd89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61c0f73e3087fc9adf3f9e09868913d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-113f6947b4114b0c044331657c5c1594.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-427617f414311408ffd5e945e1ab662c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Reinforced-Prompt-Personalization-for-Recommendation-with-Large-Language-Models"><a href="#Reinforced-Prompt-Personalization-for-Recommendation-with-Large-Language-Models" class="headerlink" title="Reinforced Prompt Personalization for Recommendation with Large Language   Models"></a>Reinforced Prompt Personalization for Recommendation with Large Language   Models</h2><p><strong>Authors:Wenyu Mao, Jiancan Wu, Weijian Chen, Chongming Gao, Xiang Wang, Xiangnan He</strong></p>
<p>Designing effective prompts can empower LLMs to understand user preferences and provide recommendations with intent comprehension and knowledge utilization capabilities. Nevertheless, recent studies predominantly concentrate on task-wise prompting, developing fixed prompt templates shared across all users in a given recommendation task (e.g., rating or ranking). Although convenient, task-wise prompting overlooks individual user differences, leading to inaccurate analysis of user interests. In this work, we introduce the concept of instance-wise prompting, aiming at personalizing discrete prompts for individual users. Toward this end, we propose Reinforced Prompt Personalization (RPP) to realize it automatically. To improve efficiency and quality, RPP personalizes prompts at the sentence level rather than searching in the vast vocabulary word-by-word. Specifically, RPP breaks down the prompt into four patterns, tailoring patterns based on multi-agent and combining them. Then the personalized prompts interact with LLMs (environment) iteratively, to boost LLMs’ recommending performance (reward). In addition to RPP, to improve the scalability of action space, our proposal of RPP+ dynamically refines the selected actions with LLMs throughout the iterative process. Extensive experiments on various datasets demonstrate the superiority of RPP&#x2F;RPP+ over traditional recommender models, few-shot methods, and other prompt-based methods, underscoring the significance of instance-wise prompting in LLMs for recommendation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/maowenyu-11/RPP">https://github.com/maowenyu-11/RPP</a>. </p>
<blockquote>
<p>设计有效的提示可以赋能大型语言模型（LLMs），使其能够理解用户偏好，并提供具有意图理解和知识利用能力的推荐。然而，最近的研究主要集中在任务式的提示上，即在给定的推荐任务（如评分或排名）中，为所有用户开发固定的提示模板。虽然方便，但任务式的提示忽视了单个用户之间的差异，导致对用户兴趣的不准确分析。在这项工作中，我们引入了实例级提示的概念，旨在针对单个用户进行个性化的离散提示。为此，我们提出了强化提示个性化（RPP）的方法来实现其自动化。为了提高效率和质量，RPP在句子级别个性化提示，而不是在庞大的词汇表中逐字搜索。具体来说，RPP将提示分解为四种模式，基于多代理定制模式并将其组合。然后，个性化的提示与LLMs（环境）进行迭代交互，以提高LLMs的推荐性能（奖励）。除了RPP之外，为了提高动作空间的可扩展性，我们提出的RPP+在迭代过程中动态优化所选动作与LLMs的配合。在多种数据集上的大量实验表明，RPP&#x2F;RPP+优于传统推荐模型、少样本方法和其他基于提示的方法，突显了实例级提示在LLMs推荐中的重要性。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/maowenyu-11/RPP">https://github.com/maowenyu-11/RPP</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.17115v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了设计有效提示语可以赋能大型语言模型（LLMs）以理解用户偏好并提供个性化推荐的能力。针对当前主要研究的任务式提示方法忽略用户个体差异的问题，本文提出了实例式提示的概念，旨在针对每个用户个性化定制提示语。为此，本文提出了强化提示个性化（RPP）方法，可以在句子级别个性化提示语，提高效率和质量。此外，为了改善行动空间的扩展性，还提出了RPP+方法，在迭代过程中动态优化所选行动。实验证明，RPP和RPP+在推荐性能上优于传统推荐模型、少样本方法和其它提示语方法，突显实例式提示在LLMs推荐中的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>设计有效提示语能增强LLMs理解用户偏好并提供个性化推荐的能力。</li>
<li>当前任务式提示方法忽略用户个体差异，导致对用户兴趣分析不准确。</li>
<li>实例式提示旨在针对每个用户个性化定制提示语。</li>
<li>强化提示个性化（RPP）方法能在句子级别个性化提示语，提高效率。</li>
<li>RPP+方法动态优化所选行动，改善行动空间的扩展性。</li>
<li>实验证明RPP和RPP+在推荐性能上优于传统和其它方法。</li>
<li>实例式提示在LLMs推荐中具有重要意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.17115">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b4b945cbd194ba89aad4e56a170614b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26b7a5473d333b70f1280cc06d3c51a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e53e3e838f19baf73d39755b645389e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="VL-ICL-Bench-The-Devil-in-the-Details-of-Multimodal-In-Context-Learning"><a href="#VL-ICL-Bench-The-Devil-in-the-Details-of-Multimodal-In-Context-Learning" class="headerlink" title="VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning"></a>VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning</h2><p><strong>Authors:Yongshuo Zong, Ondrej Bohdal, Timothy Hospedales</strong></p>
<p>Large language models (LLMs) famously exhibit emergent in-context learning (ICL) – the ability to rapidly adapt to new tasks using few-shot examples provided as a prompt, without updating the model’s weights. Built on top of LLMs, vision large language models (VLLMs) have advanced significantly in areas such as recognition, reasoning, and grounding. However, investigations into \emph{multimodal ICL} have predominantly focused on few-shot visual question answering (VQA), and image captioning, which we will show neither exploit the strengths of ICL, nor test its limitations. The broader capabilities and limitations of multimodal ICL remain under-explored. In this study, we introduce a comprehensive benchmark VL-ICL Bench for multimodal in-context learning, encompassing a broad spectrum of tasks that involve both images and text as inputs and outputs, and different types of challenges, from {perception to reasoning and long context length}. We evaluate the abilities of state-of-the-art VLLMs against this benchmark suite, revealing their diverse strengths and weaknesses, and showing that even the most advanced models, such as GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks, and the associated strengths and limitations of existing models, we hope that our dataset will inspire future work on enhancing the in-context learning capabilities of VLLMs, as well as inspire new applications that leverage VLLM ICL. The code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/ys-zong/VL-ICL">https://github.com/ys-zong/VL-ICL</a>. </p>
<blockquote>
<p>大型语言模型（LLM）展现出了突出的上下文突发学习（ICL）能力——即使用少量示例进行提示即可快速适应新任务，而无需更新模型权重。建立在大型语言模型之上的视觉大型语言模型（VLLM）在识别、推理和接地等领域取得了显著进展。然而，对于多模式ICL的研究主要集中在基于视觉的少量问答（VQA）和图像描述上，我们将展示它们都没有充分利用ICL的优势，也没有测试其局限性。多模式ICL的更广泛的能力和局限性尚未得到充分探索。本研究中，我们引入了全面的基准测试VL-ICL Bench，用于多模式上下文学习，涵盖了一系列涉及图像和文本作为输入和输出的任务，涵盖了从感知到推理和长上下文长度的不同类型挑战。我们针对这个基准测试套件评估了最先进的大型视觉语言模型的能力，揭示了它们的各种优势和劣势，并表明即使是最先进的模型，如GPT-4，也发现这些任务具有挑战性。通过突出一系列新的ICL任务以及现有模型的关联优势和局限性，我们希望我们的数据集能够激发未来关于增强大型视觉语言模型的上下文学习能力的相关工作，并激发利用大型视觉语言模型ICL的新应用。代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/ys-zong/VL-ICL%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ys-zong/VL-ICL上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.13164v3">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型展现出基于上下文的快速学习能力，而视觉大型语言模型在识别、推理和定位等领域有了显著进展。然而，对于多模态的基于上下文的学习的研究主要集中在视觉问答和图像描述生成上，其未能充分利用和测试这种学习的优势。本研究引入了一个全面的多模态基于上下文学习的基准测试VL-ICL Bench，涵盖一系列任务和挑战，从感知到推理和长文本上下文。我们评估了最先进的视觉大型语言模型在该基准测试上的表现，揭示了其多样化的优势和弱点，并显示即使是最先进的模型如GPT-4也面临挑战。我们希望这一数据集能激励未来增强视觉大型语言模型的基于上下文的学习能力的研究，并激发利用这种学习能力的新应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型展现出基于上下文的快速学习能力。</li>
<li>视觉大型语言模型在识别、推理和定位等领域有显著进展。</li>
<li>当前多模态基于上下文的学习研究主要集中在视觉问答和图像描述生成，未能充分利用和测试这种学习的优势。</li>
<li>引入了一个全面的多模态基于上下文学习的基准测试VL-ICL Bench。</li>
<li>评估了最先进的视觉大型语言模型在该基准测试上的表现。</li>
<li>揭示了视觉大型语言模型在基于上下文学习上的多样化优势和弱点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.13164">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c3f4ce0658459c27973abf3167e64210.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-615437abc2988c94e3bfd80abd3eedc5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75bfc5198c7b62621bde4c7858123edc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02ef11c2143c9259c5c08e36eac8cbab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e860e2d71c7734fe3e5ee796c28356f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Implicit-Shape-and-Appearance-Priors-for-Few-Shot-Full-Head-Reconstruction"><a href="#Implicit-Shape-and-Appearance-Priors-for-Few-Shot-Full-Head-Reconstruction" class="headerlink" title="Implicit Shape and Appearance Priors for Few-Shot Full Head   Reconstruction"></a>Implicit Shape and Appearance Priors for Few-Shot Full Head   Reconstruction</h2><p><strong>Authors:Pol Caselles, Eduard Ramon, Jaime Garcia, Gil Triginer, Francesc Moreno-Noguer</strong></p>
<p>Recent advancements in learning techniques that employ coordinate-based neural representations have yielded remarkable results in multi-view 3D reconstruction tasks. However, these approaches often require a substantial number of input views (typically several tens) and computationally intensive optimization procedures to achieve their effectiveness. In this paper, we address these limitations specifically for the problem of few-shot full 3D head reconstruction. We accomplish this by incorporating a probabilistic shape and appearance prior into coordinate-based representations, enabling faster convergence and improved generalization when working with only a few input images (even as low as a single image). During testing, we leverage this prior to guide the fitting process of a signed distance function using a differentiable renderer. By incorporating the statistical prior alongside parallelizable ray tracing and dynamic caching strategies, we achieve an efficient and accurate approach to few-shot full 3D head reconstruction. Moreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D full head scans and their corresponding posed images and masks, which we use for evaluation purposes. By leveraging this dataset, we demonstrate the remarkable capabilities of our approach in achieving state-of-the-art results in geometry reconstruction while being an order of magnitude faster than previous approaches. </p>
<blockquote>
<p>近期，采用坐标基神经网络表示的学习技术在多视角3D重建任务中取得了显著成果。然而，这些方法通常需要大量的输入视角（通常是数十个）和计算密集型的优化程序才能发挥其作用。在本文中，我们针对少样本全3D头部重建问题解决了这些限制。我们通过将概率形状和外观先验知识融入坐标基表示，仅使用少量输入图像（甚至低至单张图像）即可实现更快的收敛和更好的泛化能力。在测试过程中，我们利用此先验知识，通过可微分渲染器引导有符号距离函数的拟合过程。通过将统计先验知识与可并行化的光线追踪和动态缓存策略相结合，我们实现了高效且精准的少样本全3D头部重建方法。此外，我们扩展了H3DS数据集，现在包含60个高分辨率的3D全头扫描及其相应的姿态图像和蒙版，用于评估目的。我们利用此数据集，展示了我们的方法在几何重建方面达到最新技术成果，同时比以前的方法快一个数量级。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08784v2">PDF</a> Accepted at IEEE Transactions on Pattern Analysis and Machine   Intelligence (TPAMI) 2025</p>
<p><strong>Summary</strong></p>
<p>近期基于坐标的神经网络表示学习方法在多视角3D重建任务中取得了显著成果，但在处理少量输入图像时面临挑战。本文提出了一种针对少视角头部重建的方法，通过引入概率形状和外观先验信息，提高了收敛速度和泛化能力。此外，利用并行光线追踪和动态缓存策略实现了高效准确的重建方法。本文还扩展了H3DS数据集，用于评估重建效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入概率形状和外观先验信息，提高少视角头部重建的收敛速度和泛化能力。</li>
<li>利用可微分的渲染器将先验信息应用于符号距离函数的拟合过程。</li>
<li>通过并行光线追踪和动态缓存策略实现高效准确的重建方法。</li>
<li>扩展了H3DS数据集，包含60个高分辨率的3D头部扫描数据及其对应的姿态图像和掩码。</li>
<li>方法在几何重建方面取得了最新成果，且运行速度比先前的方法快一个数量级。</li>
<li>方法在少视角或单视角的输入条件下仍然表现出良好的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.08784">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-05\./crop_Few-Shot/2310.08784v2/page_0_0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c524a2ed3d0ef17a09c22544f82dfb30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14661a0a06f2f66a4fa5b20080b01f8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea1820582e340a1fe4438a51866dc5ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f867cb1e9782e78db8ed398656876695.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-05/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-05/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-05/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-847b0980c97dd5e32d48301827133346.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-02-05  BLens Contrastive Captioning of Binary Functions using Ensemble   Embedding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-05/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-7f7e8b593b1b5e007bf2ad4307af4d54.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-02-05  Multi-agent Multi-armed Bandit with Fully Heavy-tailed Dynamics
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">10254.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
