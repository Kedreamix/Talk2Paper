<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-02-05  SELMA A Speech-Enabled Language Model for Virtual Assistant   Interactions">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-021237d826480c29139f3dc0970fc579.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    29 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-05-更新"><a href="#2025-02-05-更新" class="headerlink" title="2025-02-05 更新"></a>2025-02-05 更新</h1><h2 id="SELMA-A-Speech-Enabled-Language-Model-for-Virtual-Assistant-Interactions"><a href="#SELMA-A-Speech-Enabled-Language-Model-for-Virtual-Assistant-Interactions" class="headerlink" title="SELMA: A Speech-Enabled Language Model for Virtual Assistant   Interactions"></a>SELMA: A Speech-Enabled Language Model for Virtual Assistant   Interactions</h2><p><strong>Authors:Dominik Wagner, Alexander Churchill, Siddharth Sigtia, Erik Marchi</strong></p>
<p>In this work, we present and evaluate SELMA, a Speech-Enabled Language Model for virtual Assistant interactions that integrates audio and text as inputs to a Large Language Model (LLM). SELMA is designed to handle three primary and two auxiliary tasks related to interactions with virtual assistants simultaneously within a single end-to-end model. We employ low-rank adaptation modules for parameter-efficient training of both the audio encoder and the LLM. Additionally, we implement a feature pooling strategy enabling the system to recognize global patterns and improve accuracy on tasks less reliant on individual sequence elements. Experimental results on Voice Trigger (VT) detection, Device-Directed Speech Detection (DDSD), and Automatic Speech Recognition (ASR), demonstrate that our approach both simplifies the typical input processing pipeline of virtual assistants significantly and also improves performance compared to dedicated models for each individual task. SELMA yields relative Equal-Error Rate improvements of 64% on the VT detection task, and 22% on DDSD, while also achieving word error rates close to the baseline. </p>
<blockquote>
<p>在这项工作中，我们介绍并评估了SELMA，这是一个用于虚拟助理交互的语音赋能语言模型，它将音频和文本作为输入集成到大型语言模型（LLM）中。SELMA被设计用于在一个端到端的模型中同时处理与虚拟助理交互相关的三个主要任务和两个辅助任务。我们采用低阶适应模块，实现音频编码器和大语言模型的参数高效训练。此外，我们实施了一种特征池策略，使系统能够识别全局模式，提高对不依赖个别序列元素的任务的准确性。在语音触发（VT）检测、定向设备语音检测（DDSD）和自动语音识别（ASR）方面的实验结果表明，我们的方法大大简化了虚拟助理的典型输入处理管道，与为每个单独任务设计的模型相比，还提高了性能。在VT检测任务上，SELMA的相对等错误率提高了64%，在DDSD上提高了22%，同时实现了接近基准值的词错误率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19377v2">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了SELMA，一个用于虚拟助理交互的语音驱动语言模型。它通过整合音频和文本输入到大型语言模型（LLM）中，可同时处理三项主要任务和两项辅助任务。使用低秩自适应模块实现音频编码器和LLM的参数有效训练。采用特征池策略提高系统对全局模式的识别能力，并在依赖单个序列元素较少的任务上提高准确性。实验结果表明，在语音触发检测、设备定向语音识别和自动语音识别任务上，与为每个单独任务设计的模型相比，此方法显著简化了虚拟助理的输入处理流程并提高了性能。SELMA在语音触发检测任务上的等价误差率相对提高了64%，在设备定向语音识别任务上提高了22%，同时字词错误率接近基线水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SELMA是一个用于虚拟助理交互的语音驱动语言模型，整合音频和文本作为输入。</li>
<li>SELMA设计用于同时处理三项主要任务和两项辅助任务。</li>
<li>采用低秩自适应模块实现参数高效训练，包括音频编码器和LLM。</li>
<li>特征池策略用于识别全局模式，提高在依赖单个序列元素较少的任务上的准确性。</li>
<li>实验结果显示，SELMA在语音触发检测、设备定向语音识别和自动语音识别任务上表现出优越性能。</li>
<li>与传统为每个任务单独设计的模型相比，SELMA简化了虚拟助理的输入处理流程。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19377">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-196e96ad81d0532652e3493cf0628bc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0db57738db4b74506d952e632b10ea7d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa31cdf212845cb639ed901b1b96b390.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd9a5f5ac7e58c0b88df01738e094dab.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="VisualSpeech-Enhance-Prosody-with-Visual-Context-in-TTS"><a href="#VisualSpeech-Enhance-Prosody-with-Visual-Context-in-TTS" class="headerlink" title="VisualSpeech: Enhance Prosody with Visual Context in TTS"></a>VisualSpeech: Enhance Prosody with Visual Context in TTS</h2><p><strong>Authors:Shumin Que, Anton Ragni</strong></p>
<p>Text-to-Speech (TTS) synthesis faces the inherent challenge of producing multiple speech outputs with varying prosody from a single text input. While previous research has addressed this by predicting prosodic information from both text and speech, additional contextual information, such as visual features, remains underutilized. This paper investigates the potential of integrating visual context to enhance prosody prediction. We propose a novel model, VisualSpeech, which incorporates both visual and textual information for improved prosody generation. Empirical results demonstrate that visual features provide valuable prosodic cues beyond the textual input, significantly enhancing the naturalness and accuracy of the synthesized speech. Audio samples are available at <a target="_blank" rel="noopener" href="https://ariameetgit.github.io/VISUALSPEECH-SAMPLES/">https://ariameetgit.github.io/VISUALSPEECH-SAMPLES/</a>. </p>
<blockquote>
<p>文本转语音（TTS）合成面临着一个固有挑战，即如何从单个文本输入生成具有不同韵律的多个语音输出。虽然之前的研究已经通过从文本和语音预测韵律信息来解决这个问题，但额外的上下文信息，如视觉特征，仍然被低估和忽略。本文探讨了整合视觉上下文以增强韵律预测的可能性。我们提出了一种新型模型VisualSpeech，该模型结合了视觉和文本信息，以改进韵律生成。经验结果表明，视觉特征提供了超越文本输入的宝贵韵律线索，显著增强了合成语音的自然度和准确性。音频样本可在<a target="_blank" rel="noopener" href="https://ariameetgit.github.io/VISUALSPEECH-SAMPLES%E6%9%E6%9F%A5%E6%89%BE%E3%80%82">https://ariameetgit.github.io/VISUALSPEECH-SAMPLES/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19258v1">PDF</a> </p>
<p><strong>总结</strong></p>
<p>本文研究了将视觉上下文融入语音合成中的潜力，以增强语调预测。提出了一种新的模型VisualSpeech，该模型结合了视觉和文本信息，以改进语调生成。实验结果表明，视觉特征提供了文本输入之外的宝贵语调线索，显著增强了合成语音的自然度和准确性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>文本转语音（TTS）合成面临从单一文本输入产生多种语音输出的固有挑战。</li>
<li>尽管先前的研究通过预测文本和语音的语调信息来解决这个问题，但视觉特征等额外上下文信息仍未得到充分利用。</li>
<li>本文探索了整合视觉上下文以增强语调预测的可能性。</li>
<li>提出了一个新的模型VisualSpeech，该模型结合了视觉和文本信息，以改进语调生成。</li>
<li>实证结果表明，视觉特征在提供语调线索方面具有重要价值，超越了文本输入。</li>
<li>VisualSpeech模型能显著增强合成语音的自然度和准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19258">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0888f2df01bc18733ce5757dd39538df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ae00a9f374f7f4f37e9c1a314c6196f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f87fa7e9e6eff76e3521125eedfccb16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b0b41a015816f98a2beabba7447e4f8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1b8a2afa9dcbb93ecfdc3af58bded0a6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DyPCL-Dynamic-Phoneme-level-Contrastive-Learning-for-Dysarthric-Speech-Recognition"><a href="#DyPCL-Dynamic-Phoneme-level-Contrastive-Learning-for-Dysarthric-Speech-Recognition" class="headerlink" title="DyPCL: Dynamic Phoneme-level Contrastive Learning for Dysarthric Speech   Recognition"></a>DyPCL: Dynamic Phoneme-level Contrastive Learning for Dysarthric Speech   Recognition</h2><p><strong>Authors:Wonjun Lee, Solee Im, Heejin Do, Yunsu Kim, Jungseul Ok, Gary Geunbae Lee</strong></p>
<p>Dysarthric speech recognition often suffers from performance degradation due to the intrinsic diversity of dysarthric severity and extrinsic disparity from normal speech. To bridge these gaps, we propose a Dynamic Phoneme-level Contrastive Learning (DyPCL) method, which leads to obtaining invariant representations across diverse speakers. We decompose the speech utterance into phoneme segments for phoneme-level contrastive learning, leveraging dynamic connectionist temporal classification alignment. Unlike prior studies focusing on utterance-level embeddings, our granular learning allows discrimination of subtle parts of speech. In addition, we introduce dynamic curriculum learning, which progressively transitions from easy negative samples to difficult-to-distinguishable negative samples based on phonetic similarity of phoneme. Our approach to training by difficulty levels alleviates the inherent variability of speakers, better identifying challenging speeches. Evaluated on the UASpeech dataset, DyPCL outperforms baseline models, achieving an average 22.10% relative reduction in word error rate (WER) across the overall dysarthria group. </p>
<blockquote>
<p>构音障碍语音识别常常因为构音障碍的固有多样性和与正常语音的外在差异而性能下降。为了弥补这些差距，我们提出了一种动态音素级对比学习（DyPCL）方法，该方法可以获得不同说话者的不变表示。我们将语音发音分解成音素片段进行音素级对比学习，利用动态连接时态分类对齐。与以往研究专注于句子级嵌入不同，我们的粒度学习可以区分语音的细微部分。此外，我们引入了动态课程学习，它根据音素的语音相似性，从容易区分的负样本逐渐过渡到难以区分的负样本。我们按难度级别进行培训的方法减轻了说话者的内在变化，更好地识别了具有挑战性的语音。在UASpeech数据集上评估，DyPCL优于基线模型，在整个构音障碍组中平均降低了22.10%的字错误率（WER）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19010v2">PDF</a> NAACL 2025 main conference, 9pages, 1 page appendix</p>
<p><strong>总结</strong></p>
<p>本文提出一种动态音素级对比学习（DyPCL）方法，用于改善发音障碍语音识别性能。针对发音障碍多样性和与正常语音的差异性，DyPCL通过音素级对比学习获得跨不同说话者的不变表示。该方法采用动态连接时间分类对齐技术，将语音片段分解为音素片段进行学习。与传统的基于句子级别的嵌入方法不同，DyPCL的精细学习能够区分语音的细微部分。此外，还引入了动态课程学习，根据音素的语音相似性，从易于区分的负样本逐渐过渡到难以区分的负样本。该方法通过按难度级别进行培训，减轻了说话人的内在变化，更好地识别了具有挑战性的语音。在UASpeech数据集上的评估结果表明，DyPCL相较于基线模型表现出色，整体发音障碍组的平均词错误率（WER）降低了22.10%。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>DyPCL方法旨在解决发音障碍语音识别的性能下降问题，面对发音障碍的多样性和与正常语音的差异。</li>
<li>通过音素级对比学习获得跨不同说话者的不变表示。</li>
<li>采用动态连接时间分类对齐技术分解语音片段进行更精细的学习。</li>
<li>引入动态课程学习，按语音相似性逐渐过渡学习难度。</li>
<li>按难度级别培训的方法减轻了说话人的内在变化，更好地识别具有挑战性的语音。</li>
<li>在UASpeech数据集上的评估显示，DyPCL相较于基线模型显著降低了词错误率（WER）。</li>
<li>DyPCL方法实现了平均22.10%的相对减少词错误率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19010">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b3bba646d4843209eaf58de3d61e4001.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f9219882919c39fe5bbb9aa0373bb23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cbb6babce0998bebad625e9b29115da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2445f2bfebc11200cd36af1ca203f3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f88283304830805ff996dd7df017fc82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cfbf0a1a3a9d6d338f2ec314a55f6bd.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Image-Text-and-Speech-Data-Augmentation-using-Multimodal-LLMs-for-Deep-Learning-A-Survey"><a href="#Image-Text-and-Speech-Data-Augmentation-using-Multimodal-LLMs-for-Deep-Learning-A-Survey" class="headerlink" title="Image, Text, and Speech Data Augmentation using Multimodal LLMs for Deep   Learning: A Survey"></a>Image, Text, and Speech Data Augmentation using Multimodal LLMs for Deep   Learning: A Survey</h2><p><strong>Authors:Ranjan Sapkota, Shaina Raza, Maged Shoman, Achyut Paudel, Manoj Karkee</strong></p>
<p>In the past five years, research has shifted from traditional Machine Learning (ML) and Deep Learning (DL) approaches to leveraging Large Language Models (LLMs) , including multimodality, for data augmentation to enhance generalization, and combat overfitting in training deep convolutional neural networks. However, while existing surveys predominantly focus on ML and DL techniques or limited modalities (text or images), a gap remains in addressing the latest advancements and multi-modal applications of LLM-based methods. This survey fills that gap by exploring recent literature utilizing multimodal LLMs to augment image, text, and audio data, offering a comprehensive understanding of these processes. We outlined various methods employed in the LLM-based image, text and speech augmentation, and discussed the limitations identified in current approaches. Additionally, we identified potential solutions to these limitations from the literature to enhance the efficacy of data augmentation practices using multimodal LLMs. This survey serves as a foundation for future research, aiming to refine and expand the use of multimodal LLMs in enhancing dataset quality and diversity for deep learning applications. (Surveyed Paper GitHub Repo: <a target="_blank" rel="noopener" href="https://github.com/WSUAgRobotics/data-aug-multi-modal-llm">https://github.com/WSUAgRobotics/data-aug-multi-modal-llm</a>. Keywords: LLM data augmentation, LLM text data augmentation, LLM image data augmentation, LLM speech data augmentation, audio augmentation, voice augmentation, chatGPT for data augmentation, DeepSeek R1 text data augmentation, DeepSeek R1 image augmentation, Image Augmentation using LLM, Text Augmentation using LLM, LLM data augmentation for deep learning applications) </p>
<blockquote>
<p>过去五年，研究已从传统的机器学习和深度学习方法转向利用大型语言模型（LLM），包括多模态，进行数据增强以提高泛化能力，并应对训练深度卷积神经网络时的过拟合问题。然而，尽管现有的调查主要集中在ML和DL技术或有限模式（文本或图像）上，但在解决基于LLM的方法的最新进展和多模态应用方面仍存在差距。本调查通过探索利用多模态LLM增强图像、文本和音频数据的最新文献来填补这一空白，为这些过程提供全面的理解。我们概述了基于LLM的图像、文本和语音增强所采用的各种方法，并讨论了当前方法中所识别的局限性。此外，我们从文献中确定了解决这些局限性的潜在解决方案，以提高使用多模态LLM的数据增强实践的有效性。本调查为未来研究奠定了基础，旨在完善和发展多模态LLM在深度学习应用中的使用和增强数据集质量和多样性的方法。（所调查论文GitHub仓库：<a target="_blank" rel="noopener" href="https://github.com/WSUAgRobotics/data-aug-multi-modal-llm%E3%80%82%E5%85%B3%E9%94%AE%E8%AF%8D%EF%BC%9ALLM%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%81LLM%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%81LLM%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%81LLM%E8%AF%AD%E9%9F%B3%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%81%E9%9F%B3%E9%A2%91%E5%A2%9E%E5%BC%BA%E3%80%81%E8%AF%AD%E9%9F%B3%E5%A2%9E%E5%BC%BA%E3%80%81ChatGPT%E7%94%A8%E4%BA%8E%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%81DeepSeek">https://github.com/WSUAgRobotics/data-aug-multi-modal-llm。关键词：LLM数据增强、LLM文本数据增强、LLM图像数据增强、LLM语音数据增强、音频增强、语音增强、ChatGPT用于数据增强、DeepSeek</a> R1文本数据增强、DeepSeek R1图像增强、使用LLM的图像增强、使用LLM的文本增强、LLM数据增强在深度学习应用）</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18648v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>这篇调研论文填补了关于多模态大型语言模型（LLM）数据增强的研究空白。文章深入探讨了使用多模态LLM增强图像、文本和音频数据的最新文献，并概述了LLM在图像、文本和语音增强中的应用方法。此外，论文讨论了当前方法的局限性，并从文献中提出了针对这些局限性的潜在解决方案，以提高使用多模态LLM的数据增强实践效果。此调研为未来的研究奠定了基础，旨在完善并扩展多模态LLM在深度学习应用中的使用，以提高数据集的质量和多样性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>调研论文概述了多模态大型语言模型（LLM）在数据增强方面的最新进展。</li>
<li>文章详细探讨了LLM在图像、文本和语音增强中的应用方法。</li>
<li>论文指出了现有LLM数据增强方法的局限性。</li>
<li>从文献中提出了针对这些局限性的潜在解决方案，以提高数据增强实践的效果。</li>
<li>调研强调了多模态LLM在提高数据集质量和多样性方面的作用。</li>
<li>文章提到的关键词包括LLM数据增强、音频增强、语音增强以及特定工具如ChatGPT用于数据增强等。</li>
<li>此调研为未来的研究提供了基础，旨在扩展和完善多模态LLM在深度学习领域的应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18648">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cf7688673a137114d94fa9ad19f96200.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10f6c3e5f7df1eb8112a3060693596f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb37708e4ce94b0baf260c789a28432d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Efficient-Event-based-Delay-Learning-in-Spiking-Neural-Networks"><a href="#Efficient-Event-based-Delay-Learning-in-Spiking-Neural-Networks" class="headerlink" title="Efficient Event-based Delay Learning in Spiking Neural Networks"></a>Efficient Event-based Delay Learning in Spiking Neural Networks</h2><p><strong>Authors:Balázs Mészáros, James C. Knight, Thomas Nowotny</strong></p>
<p>Spiking Neural Networks (SNNs) are attracting increased attention as a more energy-efficient alternative to traditional Artificial Neural Networks. Spiking neurons are stateful and intrinsically recurrent, making them well-suited for spatio-temporal tasks. However, this intrinsic memory is limited by synaptic and membrane time constants. A powerful additional mechanism are delays. In this paper, we propose a novel event-based training method for SNNs with delays, grounded in the EventProp formalism and enabling the calculation of exact gradients with respect to weights and delays. Our method supports multiple spikes per neuron and, to our best knowledge, is the first delay learning algorithm to be applied to recurrent SNNs. We evaluate our method on a simple sequence detection task, and the Yin-Yang, Spiking Heidelberg Digits and Spiking Speech Commands datasets, demonstrating that our algorithm can optimize delays from suboptimal initial conditions and enhance classification accuracy compared to architectures without delays. Finally, we show that our approach uses less than half the memory of the current state-of-the-art delay-learning method and is up to 26x faster. </p>
<blockquote>
<p>脉冲神经网络（Spiking Neural Networks，简称SNNs）作为一种比传统人工神经网络更节能的替代方案，正越来越受到关注。脉冲神经元具有状态和内在复发的特性，使其非常适合时空任务。然而，这种内在的记忆受到突触和膜时间常数的限制。一种强大的附加机制是延迟。在本文中，我们提出了一种基于事件的新型脉冲神经网络训练方法，该方法以EventProp形式为基础，能够计算关于权重和延迟的精确梯度。我们的方法支持每个神经元的多个脉冲，并且据我们所知，是第一个应用于复发脉冲神经网络的延迟学习算法。我们在简单的序列检测任务以及阴阳、脉冲海德堡数字和脉冲语音命令数据集上评估了我们的方法，证明我们的算法可以从不佳的初始条件优化延迟并提高分类准确率，与没有延迟的架构相比具有优势。最后，我们证明我们的方法使用的内存不到当前最先进的延迟学习方法的一半，并且速度最快可提高26倍。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07331v2">PDF</a> </p>
<p><strong>Summary</strong><br>     脉冲神经网络（SNNs）作为更节能的传统人工神经网络替代品而备受关注。脉冲神经元具有状态和内在复发性，使其适合时空任务。然而，这种内在记忆受到突触和膜时间常数的限制。延迟是一种强大的附加机制。本文提出一种基于EventProp形式主义的带有延迟的SNNs的新型事件驱动训练方法，能够计算关于权重和延迟的确切梯度。我们的方法支持每个神经元的多个脉冲，据我们所知，是首个应用于递归SNNs的延迟学习算法。我们在简单的序列检测任务以及阴阳、脉冲海德堡数字和脉冲语音命令数据集上评估了我们的方法，证明我们的算法可以优化从次优初始条件开始的延迟，并提高分类精度，与没有延迟的架构相比。最后，我们证明我们的方法使用的内存不到当前最先进的延迟学习方法的一半，并且速度最快可达26倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Spiking Neural Networks (SNNs) 作为一种更节能的神经网络形式受到关注。</li>
<li>脉冲神经元具有状态和内在复发性，适合处理时空任务。</li>
<li>延迟是SNNs中强大的附加机制，用于优化性能。</li>
<li>提出了一种新型事件驱动训练方法，结合EventProp形式主义，可计算权重和延迟的确切梯度。</li>
<li>该方法支持多个脉冲，并首次应用于递归SNNs的延迟学习。</li>
<li>在序列检测任务及多个数据集上评估，证明算法能优化延迟并提升分类精度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07331">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fb5487d1b523d19084184b1e596c7e73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-985974598a650db595f2e82aee117459.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eb9b02ac2bbea6ac68b241cdf29c6b21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9dd79f1fc8a708d9639de21b95b99de.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="On-Creating-A-Brain-To-Text-Decoder"><a href="#On-Creating-A-Brain-To-Text-Decoder" class="headerlink" title="On Creating A Brain-To-Text Decoder"></a>On Creating A Brain-To-Text Decoder</h2><p><strong>Authors:Zenon Lamprou, Yashar Moshfeghi</strong></p>
<p>Brain decoding has emerged as a rapidly advancing and extensively utilized technique within neuroscience. This paper centers on the application of raw electroencephalogram (EEG) signals for decoding human brain activity, offering a more expedited and efficient methodology for enhancing our understanding of the human brain. The investigation specifically scrutinizes the efficacy of brain-computer interfaces (BCI) in deciphering neural signals associated with speech production, with particular emphasis on the impact of vocabulary size, electrode density, and training data on the framework’s performance. The study reveals the competitive word error rates (WERs) achievable on the Librispeech benchmark through pre-training on unlabelled data for speech processing. Furthermore, the study evaluates the efficacy of voice recognition under configurations with limited labeled data, surpassing previous state-of-the-art techniques while utilizing significantly fewer labels. Additionally, the research provides a comprehensive analysis of error patterns in voice recognition and the influence of model size and unlabelled training data. It underscores the significance of factors such as vocabulary size and electrode density in enhancing BCI performance, advocating for an increase in microelectrodes and refinement of language models. </p>
<blockquote>
<p>脑解码作为神经科学领域的一种快速进步且广泛应用的技术已经崭露头角。本文重点研究原始脑电图（EEG）信号在解码人脑活动中的应用，为增强我们对人脑的理解提供了一种更快、更高效的方法。研究特别细致地审视了脑机接口（BCI）在解析与言语产生相关的神经信号方面的有效性，特别强调了词汇量、电极密度和训练数据对框架性能的影响。该研究通过在无标签数据上进行预训练，揭示了Librispeech基准测试上可实现的竞争词错误率（WERs）。此外，该研究在有限标记数据配置下对语音识别的有效性进行了评估，在利用明显更少的标签的情况下，超越了以前最先进的技术。此外，该研究还全面分析了语音识别中的错误模式以及模型大小和未标记训练数据的影响。它强调了增加词汇量和提高电极密度等要素在提高BCI性能中的重要性，提倡增加微电极数量和完善语言模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06326v2">PDF</a> </p>
<p><strong>Summary</strong><br>脑解码是神经科学中发展迅速且广泛应用的技术。本文关注原始脑电图信号在解码人类脑活动中的应用，提供一种更快、更高效的方法，以提高对大脑的理解。研究重点探讨了脑机接口在解析与言语产生相关的神经信号方面的有效性，特别关注词汇量、电极密度和训练数据对系统性能的影响。研究还通过预训练未标记数据，揭示了Librispeech基准测试上可达到的竞争词错误率，并在有限标记数据配置下评估了语音识别的有效性，同时分析了语音识别的错误模式以及模型大小和未标记训练数据的影响。强调了增加微电极和改进语言模型的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>脑解码技术已成为神经科学中的核心方法，利用原始脑电图信号解码人类脑活动，提高我们对大脑的理解。</li>
<li>研究关注脑机接口在解析与言语产生相关的神经信号方面的有效性。</li>
<li>词汇量、电极密度和训练数据是影响脑机接口性能的关键因素。</li>
<li>通过预训练未标记数据，可以在Librispeech基准测试上实现较低的词错误率。</li>
<li>在有限标记数据配置下，研究提出的语音识别方法优于现有技术。</li>
<li>研究分析了语音识别的错误模式，并探讨了模型大小和未标记训练数据的影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06326">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8df5733bc61e92075cd497fedf3aa89a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-021237d826480c29139f3dc0970fc579.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Wave-U-Mamba-An-End-To-End-Framework-For-High-Quality-And-Efficient-Speech-Super-Resolution"><a href="#Wave-U-Mamba-An-End-To-End-Framework-For-High-Quality-And-Efficient-Speech-Super-Resolution" class="headerlink" title="Wave-U-Mamba: An End-To-End Framework For High-Quality And Efficient   Speech Super Resolution"></a>Wave-U-Mamba: An End-To-End Framework For High-Quality And Efficient   Speech Super Resolution</h2><p><strong>Authors:Yongjoon Lee, Chanwoo Kim</strong></p>
<p>Speech Super-Resolution (SSR) is a task of enhancing low-resolution speech signals by restoring missing high-frequency components. Conventional approaches typically reconstruct log-mel features, followed by a vocoder that generates high-resolution speech in the waveform domain. However, as mel features lack phase information, this can result in performance degradation during the reconstruction phase. Motivated by recent advances with Selective State Spaces Models (SSMs), we propose a method, referred to as Wave-U-Mamba that directly performs SSR in time domain. In our comparative study, including models such as WSRGlow, NU-Wave 2, and AudioSR, Wave-U-Mamba demonstrates superior performance, achieving the lowest Log-Spectral Distance (LSD) across various low-resolution sampling rates, ranging from 8 to 24 kHz. Additionally, subjective human evaluations, scored using Mean Opinion Score (MOS) reveal that our method produces SSR with natural and human-like quality. Furthermore, Wave-U-Mamba achieves these results while generating high-resolution speech over nine times faster than baseline models on a single A100 GPU, with parameter sizes less than 2% of those in the baseline models. </p>
<blockquote>
<p>语音超分辨率（SSR）是通过恢复缺失的高频成分来提高低分辨率语音信号的任务。传统方法通常重建对数梅尔特征，然后通过声码器在波形域生成高分辨率语音。然而，由于梅尔特征缺乏相位信息，这可能导致重建阶段的性能下降。受最近选择性状态空间模型（SSM）进展的启发，我们提出了一种称为Wave-U-Mamba的方法，直接在时间域执行SSR。在我们的比较研究中，包括WSRGlow、NU-Wave 2和AudioSR等模型，Wave-U-Mamba展示了出色的性能，在各种低分辨率采样率（从8到24 kHz）下实现了最低的对数谱距离（LSD）。此外，使用平均意见得分（MOS）进行的主观人类评估表明，我们的方法产生的SSR具有自然和人性化质量。而且，Wave-U-Mamba在单个A100 GPU上生成高分辨率语音的速度是基线模型的9倍，参数大小不到基线模型的2%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09337v3">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了语音超分辨率（SSR）的任务，即通过对缺失的高频成分进行恢复来提高低分辨率语音信号的质量。文章提出了一种基于选择性状态空间模型（SSMs）的方法——Wave-U-Mamba，直接在时间域进行SSR。相较于其他模型，如WSRGlow、NU-Wave 2和AudioSR，Wave-U-Mamba在各项低分辨率采样率下的Log-Spectral Distance（LSD）指标表现更优，且生成的语音质量自然、逼真。同时，Wave-U-Mamba在单A100 GPU上的运行速度是基线模型的九倍，参数大小仅为基线模型的不到2%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音超分辨率（SSR）旨在提高低分辨率语音信号的质量，通过恢复缺失的高频成分来实现。</li>
<li>传统方法通常通过重建log-mel特征，然后利用vocoder生成高分辨率语音波形。</li>
<li>log-mel特征缺乏相位信息，可能导致重建阶段性能下降。</li>
<li>基于选择性状态空间模型（SSMs）的Wave-U-Mamba方法直接在时间域进行SSR。</li>
<li>在各种低分辨率采样率下，Wave-U-Mamba的Log-Spectral Distance（LSD）表现优于其他模型。</li>
<li>人类主观评价显示，Wave-U-Mamba生成的语音质量自然、逼真。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09337">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0a1db6dfe62eb8580a65a6a5bb57ecfa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c605b7b85e62a59426ea1e89f147081.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3110bda44b5dc88da52cef986e8a6d2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c55e38e14f89ca1e3009420a35d10a96.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MS-HuBERT-Mitigating-Pre-training-and-Inference-Mismatch-in-Masked-Language-Modelling-methods-for-learning-Speech-Representations"><a href="#MS-HuBERT-Mitigating-Pre-training-and-Inference-Mismatch-in-Masked-Language-Modelling-methods-for-learning-Speech-Representations" class="headerlink" title="MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked   Language Modelling methods for learning Speech Representations"></a>MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked   Language Modelling methods for learning Speech Representations</h2><p><strong>Authors:Hemant Yadav, Sunayana Sitaram, Rajiv Ratn Shah</strong></p>
<p>In recent years, self-supervised pre-training methods have gained significant traction in learning high-level information from raw speech. Among these methods, HuBERT has demonstrated SOTA performance in automatic speech recognition (ASR). However, HuBERT’s performance lags behind data2vec due to disparities in pre-training strategies. In this paper, we propose (i) a Swap method to address pre-training and inference mismatch observed in HuBERT and (ii) incorporates Multicluster masked prediction loss for more effective utilization of the models capacity. The resulting method is, MS-HuBERT, an end-to-end self-supervised pre-training method for learning robust speech representations. It beats vanilla HuBERT on the ASR Librispeech benchmark on average by a 5% margin when evaluated on different finetuning splits. Additionally, we demonstrate that the learned embeddings obtained during pre-training encode essential information for improving performance of content based tasks such as ASR. </p>
<blockquote>
<p>近年来，自监督预训练方法在从原始语音中学习高级信息方面获得了很大的关注。在这些方法中，HuBERT在自动语音识别（ASR）方面表现出了卓越的性能。然而，由于预训练策略的差异，HuBERT的性能落后于data2vec。在本文中，我们提出了（i）一种Swap方法来解决HuBERT中观察到的预训练和推理不匹配的问题，（ii）并融入了多集群掩码预测损失，以更有效地利用模型容量。由此产生的方法为MS-HuBERT，是一种端到端的自监督预训练方法，用于学习稳健的语音表示。在平均意义上，当在不同微调分割上评估时，它在ASR Librispeech基准测试上的表现优于原版HuBERT，平均提高了5%的准确率。此外，我们证明了在预训练过程中获得的学习嵌入编码了对改进基于内容任务的性能至关重要的信息，例如自动语音识别等。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.05661v3">PDF</a> 4 pages, submitted to interspeech2024</p>
<p><strong>总结</strong></p>
<p>近年来，自监督预训练方法已从原始语音中学习高级信息获得了显著进展。HuBERT等方法在自动语音识别（ASR）方面表现出卓越性能。然而，由于预训练策略的差异，HuBERT的性能落后于data2vec。本文提出了（i）解决HuBERT在预训练和推理过程中不匹配问题的Swap方法，以及（ii）结合多集群掩码预测损失，更有效地利用模型容量。由此产生的方法称为MS-HuBERT，是一种端到端的自监督预训练方法，用于学习稳健的语音表示。在ASR Librispeech基准测试中，与vanilla HuBERT相比，MS-HuBERT在微调的不同分割点上平均提高了5%的性能。此外，我们还证明，在预训练过程中获得的嵌入编码对于改进基于内容的任务（如ASR）的性能至关重要。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>自监督预训练方法已用于从原始语音中学习高级信息。</li>
<li>HuBERT在自动语音识别（ASR）方面表现出卓越性能，但预训练策略的差异导致其在某些方面落后于data2vec。</li>
<li>本文提出了Swap方法来解决HuBERT在预训练和推理过程中的不匹配问题。</li>
<li>多集群掩码预测损失被引入以提高模型效率。</li>
<li>MS-HuBERT是一种新的自监督预训练方法，旨在学习稳健的语音表示。</li>
<li>MS-HuBERT在ASR Librispeech基准测试中优于原版HuBERT。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.05661">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e73eac2fa02ccc1cb25e101ccb52550c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6ed15cb4238f8dee78bec52cc55256c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd85e2c2e729e486d20bdae5b3635c76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2857e337797ec8a87ce31155d7353a94.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8daf29d02907ddf910f53299020b37ce.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-05/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-05/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-05/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-4f141bc5650c59804d3f9a2085de1663.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-02-05  Ambient Denoising Diffusion Generative Adversarial Networks for   Establishing Stochastic Object Models from Noisy Image Data
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-05/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-a9bb2a5ae7ed8d00afc2eefdb3258681.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-02-05  Improving Multi-Label Contrastive Learning by Leveraging Label   Distribution
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18588k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
