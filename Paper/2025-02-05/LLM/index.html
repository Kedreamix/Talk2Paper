<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-02-05  SELMA A Speech-Enabled Language Model for Virtual Assistant   Interactions">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1857492ad3c81d4f22d20216aa887010.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    79 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-05-更新"><a href="#2025-02-05-更新" class="headerlink" title="2025-02-05 更新"></a>2025-02-05 更新</h1><h2 id="SELMA-A-Speech-Enabled-Language-Model-for-Virtual-Assistant-Interactions"><a href="#SELMA-A-Speech-Enabled-Language-Model-for-Virtual-Assistant-Interactions" class="headerlink" title="SELMA: A Speech-Enabled Language Model for Virtual Assistant   Interactions"></a>SELMA: A Speech-Enabled Language Model for Virtual Assistant   Interactions</h2><p><strong>Authors:Dominik Wagner, Alexander Churchill, Siddharth Sigtia, Erik Marchi</strong></p>
<p>In this work, we present and evaluate SELMA, a Speech-Enabled Language Model for virtual Assistant interactions that integrates audio and text as inputs to a Large Language Model (LLM). SELMA is designed to handle three primary and two auxiliary tasks related to interactions with virtual assistants simultaneously within a single end-to-end model. We employ low-rank adaptation modules for parameter-efficient training of both the audio encoder and the LLM. Additionally, we implement a feature pooling strategy enabling the system to recognize global patterns and improve accuracy on tasks less reliant on individual sequence elements. Experimental results on Voice Trigger (VT) detection, Device-Directed Speech Detection (DDSD), and Automatic Speech Recognition (ASR), demonstrate that our approach both simplifies the typical input processing pipeline of virtual assistants significantly and also improves performance compared to dedicated models for each individual task. SELMA yields relative Equal-Error Rate improvements of 64% on the VT detection task, and 22% on DDSD, while also achieving word error rates close to the baseline. </p>
<blockquote>
<p>在这项工作中，我们介绍并评估了SELMA，这是一个用于虚拟助手交互的语音赋能语言模型，它将音频和文本作为输入集成到大型语言模型（LLM）中。SELMA设计用于在单个端到端模型中同时处理与虚拟助手交互相关的三个主要任务和两个辅助任务。我们采用低阶适应模块，对音频编码器和LLM进行参数有效的训练。此外，我们实现了特征池策略，使系统能够识别全局模式，并在对单个序列元素依赖较少的任务上提高准确性。在语音触发（VT）检测、定向设备语音检测（DDSD）和自动语音识别（ASR）方面的实验结果表明，我们的方法大大简化了虚拟助理的典型输入处理管道，并且在与每个单独任务的专用模型相比时提高了性能。在VT检测任务上，SELMA的相对等错误率提高了64%，在DDSD上的提高了2 2%，同时其词错误率接近基线水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19377v2">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>摘要</strong></p>
<p>本工作介绍了SELMA，一种面向虚拟助手交互的语音驱动语言模型，它整合了音频和文本作为大型语言模型（LLM）的输入。SELMA旨在在一个单一端到端模型中同时处理与虚拟助手交互的三个主要任务和两个辅助任务。我们采用低阶适应模块，实现音频编码器与LLM的参数高效训练。此外，我们实施了一项特性池策略，使系统能够识别全局模式，提高不太依赖个别序列元素的准确性。在语音触发（VT）检测、定向设备语音检测（DDSD）和自动语音识别（ASR）方面的实验结果表明，我们的方法大大简化了虚拟助手的典型输入处理管道，并且在与每个单独任务的专用模型相比时提高了性能。在VT检测任务上，SELMA实现了相对等误率（Equal-Error Rate）提高64%，在DDSD上提高22%，同时达到接近基准的单词错误率。</p>
<p><strong>要点分析</strong></p>
<ol>
<li>SELMA是一个面向虚拟助手交互的语音驱动语言模型，集成音频和文本作为大型语言模型的输入。</li>
<li>SELMA旨在在一个单一端到端模型中同时处理虚拟助手交互的三个主要任务和两个辅助任务。</li>
<li>采用低阶适应模块实现参数高效训练，适用于音频编码器和LLM。</li>
<li>特性池策略使系统能够识别全局模式，提高任务准确性，尤其在不依赖个别序列元素的场景下。</li>
<li>在VT检测、DDSD和ASR等实验任务上，SELMA表现出显著的性能改进。</li>
<li>SELMA在VT检测任务上的等误率相对改进达到64%，在DDSD上达到22%。</li>
<li>SELMA在单词错误率方面接近基线性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19377">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-196e96ad81d0532652e3493cf0628bc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0db57738db4b74506d952e632b10ea7d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa31cdf212845cb639ed901b1b96b390.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd9a5f5ac7e58c0b88df01738e094dab.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Improving-the-Robustness-of-Representation-Misdirection-for-Large-Language-Model-Unlearning"><a href="#Improving-the-Robustness-of-Representation-Misdirection-for-Large-Language-Model-Unlearning" class="headerlink" title="Improving the Robustness of Representation Misdirection for Large   Language Model Unlearning"></a>Improving the Robustness of Representation Misdirection for Large   Language Model Unlearning</h2><p><strong>Authors:Dang Huu-Tien, Hoang Thanh-Tung, Le-Minh Nguyen, Naoya Inoue</strong></p>
<p>Representation Misdirection (RM) and variants are established large language model (LLM) unlearning methods with state-of-the-art performance. In this paper, we show that RM methods inherently reduce models’ robustness, causing them to misbehave even when a single non-adversarial forget-token is in the retain-query. Toward understanding underlying causes, we reframe the unlearning process as backdoor attacks and defenses: forget-tokens act as backdoor triggers that, when activated in retain-queries, cause disruptions in RM models’ behaviors, similar to successful backdoor attacks. To mitigate this vulnerability, we propose Random Noise Augmentation – a model and method agnostic approach with theoretical guarantees for improving the robustness of RM methods. Extensive experiments demonstrate that RNA significantly improves the robustness of RM models while enhancing the unlearning performances. </p>
<blockquote>
<p>表示误导（RM）及其变体是建立在大规模语言模型（LLM）上的遗忘方法，具有最先进的性能。在本文中，我们表明RM方法本质上会降低模型的稳健性，甚至在保留查询中存在单个非对抗性遗忘令牌时也会导致模型表现异常。为了了解潜在原因，我们将遗忘过程重新构建为后门攻击和防御：遗忘令牌充当后门触发器，在保留查询中激活时，会对RM模型的行为造成干扰，类似于成功的后门攻击。为了缓解这种漏洞，我们提出了随机噪声增强法（RNA）——这是一种模型和通用的方法，理论上能保证提高RM方法的稳健性。大量实验表明，RNA显著提高了RM模型的稳健性，同时提高了遗忘性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19202v2">PDF</a> 12 pages, 4 figures, 1 table</p>
<p><strong>Summary</strong></p>
<p>本文探讨了大型语言模型（LLM）中的表示误导（RM）方法及其变种的问题。研究发现RM方法会降低模型的稳健性，即使在非对抗性的遗忘令牌存在于保留查询中时也会导致模型行为失常。为此，本文将遗忘过程重新构建为后门攻击和防御，并提出了随机噪声增强法，以提高RM方法的稳健性，且通过实验证明了其显著效果和理论保证。此方法不仅对特定模型适用，而是对多种模型和方法的泛化性能较强。通过引入随机噪声增强法，可有效缓解RM模型的脆弱性并提升其性能。同时提高了模型遗忘和泛化能力。总结起来，该研究对于改进大型语言模型的稳健性和性能具有重要意义。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RM方法会降低LLM模型的稳健性，导致其面对特定情况时容易出错。通过将该问题重构为后门攻击与防御的问题进行分析，可以直观地展示其脆弱性所在。因此需增强模型的健壮性以避免其可能导致的误判和性能下降等问题。其中发现当在保留查询中存在单个非对抗遗忘令牌时会出现此种情况，显示出该问题具有一定威胁性且普遍存在性较高。通过对现有的无针对性的攻击和防御策略进行改进或创新是提升模型稳健性的关键途径之一。同时发现遗忘令牌类似于后门攻击中的触发器，激活后会干扰模型行为。因此，对遗忘令牌的管理和识别是提升模型稳健性的重要手段之一。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19202">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1857492ad3c81d4f22d20216aa887010.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Brain-inspired-sparse-training-enables-Transformers-and-LLMs-to-perform-as-fully-connected"><a href="#Brain-inspired-sparse-training-enables-Transformers-and-LLMs-to-perform-as-fully-connected" class="headerlink" title="Brain-inspired sparse training enables Transformers and LLMs to perform   as fully connected"></a>Brain-inspired sparse training enables Transformers and LLMs to perform   as fully connected</h2><p><strong>Authors:Yingtao Zhang, Jialin Zhao, Wenjing Wu, Ziheng Liao, Umberto Michieli, Carlo Vittorio Cannistraci</strong></p>
<p>This study aims to enlarge our current knowledge on application of brain-inspired network science principles for training artificial neural networks (ANNs) with sparse connectivity. Dynamic sparse training (DST) can reduce the computational demands in ANNs, but faces difficulties to keep peak performance at high sparsity levels. The Cannistraci-Hebb training (CHT) is a brain-inspired method for growing connectivity in DST. CHT leverages a gradient-free, topology-driven link regrowth, which has shown ultra-sparse (1% connectivity or lower) advantage across various tasks compared to fully connected networks. Yet, CHT suffers two main drawbacks: (i) its time complexity is O(Nd^3) - N node network size, d node degree - hence it can apply only to ultra-sparse networks. (ii) it selects top link prediction scores, which is inappropriate for the early training epochs, when the network presents unreliable connections. We propose a GPU-friendly approximation of the CH link predictor, which reduces the computational complexity to O(N^3), enabling a fast implementation of CHT in large-scale models. We introduce the Cannistraci-Hebb training soft rule (CHTs), which adopts a strategy for sampling connections in both link removal and regrowth, balancing the exploration and exploitation of network topology. To improve performance, we integrate CHTs with a sigmoid gradual density decay (CHTss). Empirical results show that, using 1% of connections, CHTs outperforms fully connected networks in MLP on visual classification tasks, compressing some networks to &lt; 30% nodes. Using 5% of the connections, CHTss outperforms fully connected networks in two Transformer-based machine translation tasks. Using 30% of the connections, CHTss achieves superior performance compared to other dynamic sparse training methods in language modeling, and it surpasses the fully connected counterpart in zero-shot evaluations. </p>
<blockquote>
<p>本研究旨在扩大我们目前关于应用脑启发网络科学原理训练具有稀疏连接的人工神经网络（ANN）的知识。动态稀疏训练（DST）可以降低人工神经网络中的计算需求，但在高稀疏级别上保持峰值性能面临困难。Cannistraci-Hebb训练（CHT）是一种受大脑启发的在DST中增加连接性的方法。CHT利用无梯度、拓扑驱动的连接再生，与全连接网络相比，在各种任务中显示了超稀疏（1%的连接度或更低）的优势。然而，CHT存在两个主要缺点：（i）其时间复杂度为O（Nd^3）——N为网络节点大小，d为节点度——因此只能应用于超稀疏网络。（ii）它选择顶级链接预测分数，这在网络呈现不可靠连接的早期训练周期中是不恰当的。我们提出了CH链接预测器的GPU友好近似，将计算复杂度降低到O（N^3），使CHT在大规模模型中的快速实现成为可能。我们引入了Cannistraci-Hebb训练软规则（CHTs），该规则采用在连接删除和再生中都进行采样连接的策略，平衡网络拓扑的探索与利用。为了提高性能，我们将CHTs与sigmoid逐渐密度衰减（CHTss）相结合。经验结果表明，使用1%的连接，CHTs在多层感知器视觉分类任务上的表现优于全连接网络，可以将某些网络压缩到小于30%的节点。使用5%的连接，CHTss在基于Transformer的机器翻译任务中的表现优于全连接网络。使用30%的连接，CHTss在语言建模方面的性能优于其他动态稀疏训练方法，并且在零样本评估中超越了全连接模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19107v1">PDF</a> </p>
<p><strong>Summary</strong><br>基于脑启发的网络科学原则来训练具有稀疏连接的人工神经网络（ANN），有望提高我们的认知。文章研究了动态稀疏训练（DST）及其在训练过程中面临的挑战，特别是保持高峰性能的问题。Cannistraci-Hebb训练（CHT）是一种脑启发的方法，用于在DST中增加连接性。然而，CHT存在两个主要缺点：计算复杂度高且仅适用于超稀疏网络；早期训练时存在网络连接的可靠性问题。为克服这些问题，提出了对CH链接预测器的GPU友好近似方案及采样连接策略的改良版CHTs方法。结合sigmoid逐渐密度衰减技术后，新方法在视觉分类任务上的多层感知器网络压缩至不足三十分之一的情况下依然超越全连接网络性能，并且提高了翻译任务和语言建模的表现。实验结果表明其在稀疏网络中展现优异性能，值得进一步研究推广。 </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>研究的重点是通过引入基于脑启发的网络科学原则训练具有稀疏连接的人工神经网络。 </li>
<li>动态稀疏训练（DST）可以减少人工神经网络中的计算需求，但在保持高峰性能方面面临挑战。 </li>
<li>Cannistraci-Hebb训练（CHT）通过梯度无关的拓扑驱动链接重生成法促进网络连接增长，并在各种任务中显示出优势。 </li>
<li>CHT存在计算复杂度高和对早期训练不可靠连接敏感的问题。 </li>
<li>提出了一种GPU友好的CH链接预测器近似方案，以降低计算复杂度并适用于大规模模型。 </li>
<li>引入采样连接策略的改良版CHTs方法，结合sigmoid逐渐密度衰减技术改善性能。 </li>
<li>实验结果表明新方法在视觉分类任务上表现优越，同时在翻译任务和语言建模中表现优异，值得进一步研究推广。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19107">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-192d64c730cf0c1624b7cd5707fbdf70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-760bfe3a31aac62e81bbce04a7bad768.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c669be030823453bf33177083259ced.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-143b1bd40a6bcdb3eb2b278bb96ac5ca.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="OT-Transformer-A-Continuous-time-Transformer-Architecture-with-Optimal-Transport-Regularization"><a href="#OT-Transformer-A-Continuous-time-Transformer-Architecture-with-Optimal-Transport-Regularization" class="headerlink" title="OT-Transformer: A Continuous-time Transformer Architecture with Optimal   Transport Regularization"></a>OT-Transformer: A Continuous-time Transformer Architecture with Optimal   Transport Regularization</h2><p><strong>Authors:Kelvin Kan, Xingjian Li, Stanley Osher</strong></p>
<p>Transformers have achieved state-of-the-art performance in numerous tasks. In this paper, we propose a continuous-time formulation of transformers. Specifically, we consider a dynamical system whose governing equation is parametrized by transformer blocks. We leverage optimal transport theory to regularize the training problem, which enhances stability in training and improves generalization of the resulting model. Moreover, we demonstrate in theory that this regularization is necessary as it promotes uniqueness and regularity of solutions. Our model is flexible in that almost any existing transformer architectures can be adopted to construct the dynamical system with only slight modifications to the existing code. We perform extensive numerical experiments on tasks motivated by natural language processing, image classification, and point cloud classification. Our experimental results show that the proposed method improves the performance of its discrete counterpart and outperforms relevant comparing models. </p>
<blockquote>
<p>Transformer在许多任务中都取得了最先进的性能。在本文中，我们提出了一个连续时间形式的Transformer模型。具体来说，我们考虑一个动态系统，其控制方程由Transformer块参数化。我们利用最优传输理论来规范训练问题，这增强了训练的稳定性并提高了模型的泛化能力。此外，我们从理论上证明这种规范化是必要的，因为它可以促进解的唯一性和规律性。我们的模型很灵活，几乎可以采用任何现有的Transformer架构来构建动态系统，只需对现有的代码进行微小的修改。我们在受自然语言处理、图像分类和点云分类任务启发的任务上进行了大量的数值实验。我们的实验结果表明，该方法改进了其离散对应模型的性能，并优于相关的对比模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18793v1">PDF</a> </p>
<p><strong>Summary</strong>：本文提出了基于连续时间形式的Transformer模型，通过动态系统方程参数化Transformer块。利用最优传输理论对训练问题进行正则化，提高了模型的稳定性和泛化性能。模型灵活，几乎可采用任何现有Transformer架构构建动态系统，只需对现有代码进行微小修改。实验结果表明，该方法提高了离散模型的性能，并优于相关对比模型。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>本文提出了基于连续时间形式的Transformer模型。</li>
<li>通过动态系统方程参数化Transformer块。</li>
<li>利用最优传输理论对训练进行正则化，提高模型稳定性和泛化性能。</li>
<li>模型具有灵活性，可几乎采用任何现有Transformer架构构建动态系统。</li>
<li>正则化有助于促进解的唯一性和规律性。</li>
<li>广泛实验表明，该方法提高了离散模型的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18793">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d9ce4ab9999a428a45c62bf5eb9bd420.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbb07f1405eab7cafd48a0d2ee04eaca.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="The-TIP-of-the-Iceberg-Revealing-a-Hidden-Class-of-Task-In-Prompt-Adversarial-Attacks-on-LLMs"><a href="#The-TIP-of-the-Iceberg-Revealing-a-Hidden-Class-of-Task-In-Prompt-Adversarial-Attacks-on-LLMs" class="headerlink" title="The TIP of the Iceberg: Revealing a Hidden Class of Task-In-Prompt   Adversarial Attacks on LLMs"></a>The TIP of the Iceberg: Revealing a Hidden Class of Task-In-Prompt   Adversarial Attacks on LLMs</h2><p><strong>Authors:Sergey Berezin, Reza Farahbakhsh, Noel Crespi</strong></p>
<p>We present a novel class of jailbreak adversarial attacks on LLMs, termed Task-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks (e.g., cipher decoding, riddles, code execution) into the model’s prompt to indirectly generate prohibited inputs. To systematically assess the effectiveness of these attacks, we introduce the PHRYGE benchmark. We demonstrate that our techniques successfully circumvent safeguards in six state-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings highlight critical weaknesses in current LLM safety alignments and underscore the urgent need for more sophisticated defence strategies.   Warning: this paper contains examples of unethical inquiries used solely for research purposes. </p>
<blockquote>
<p>我们提出一种新型的大语言模型越狱对抗攻击，称为任务提示（TIP）攻击。我们的方法将序列到序列的任务（如密码解码、谜语、代码执行）嵌入到模型的提示中，间接生成禁止的输入。为了系统地评估这些攻击的有效性，我们引入了PHRYGE基准测试。我们证明我们的技术成功地绕过了六种最先进语言模型的保障措施，包括GPT-4o和LLaMA 3.2。我们的研究突出了当前大型语言模型安全对齐中的关键弱点，并强调了更复杂的防御策略的紧迫需求。警告：本论文包含的询问示例仅用于研究目的，可能存在不道德的内容。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18626v2">PDF</a> </p>
<p><strong>Summary</strong><br>任务在提示中的攻击（TIP攻击）是一种新型的大语言模型越狱对抗攻击方式。该方法通过将序列到序列任务嵌入模型提示中，间接生成禁止输入，并对六个最新语言模型进行了演示，包括GPT-4o和LLaMA 3.2。研究表明，当前大语言模型的安全对齐存在重大缺陷，急需更先进的防御策略。请注意，本文仅供参考的例子仅为研究目的。大语言模型攻防研究的目的是深入剖析攻击特点与安全性能分析进行不断改进和避免对现实世界造成的潜在风险。本研究展示的攻击案例并不代表研究者的道德立场和推荐行为方式。通过持续研究改进安全性能保障人类生产生活和社会秩序的正常运行。因此本摘要致力于保持客观公正的研究态度并倡导使用技术的正面影响造福人类社会。本文总结了关于大语言模型安全性的重要研究成果，揭示了新的攻击方式及其潜在威胁，并呼吁业界关注防御策略的发展。旨在推动大语言模型的安全性和可靠性不断提高，从而更好地服务于人类社会。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是关于该文本的关键见解：</p>
<ul>
<li>提出了一种新型的大语言模型攻击方式——任务在提示中的攻击（TIP攻击），该方法通过嵌入特定任务间接生成禁止输入。</li>
<li>研究展示了TIP攻击如何绕过六个先进语言模型的保护措施，包括GPT-4o和LLaMA 3.2。</li>
<li>研究强调了当前大语言模型在安全对齐方面的重大缺陷，指出需要更先进的防御策略。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18626">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5ca7753f1824329dc66d3d5da14c3784.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a91c465259dd08ba012f8747c79b476.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e279029ef890ffe7c442201f993b1e3f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Transforming-Role-Classification-in-Scientific-Teams-Using-LLMs-and-Advanced-Predictive-Analytics"><a href="#Transforming-Role-Classification-in-Scientific-Teams-Using-LLMs-and-Advanced-Predictive-Analytics" class="headerlink" title="Transforming Role Classification in Scientific Teams Using LLMs and   Advanced Predictive Analytics"></a>Transforming Role Classification in Scientific Teams Using LLMs and   Advanced Predictive Analytics</h2><p><strong>Authors:Wonduk Seo, Yi Bu</strong></p>
<p>Scientific team dynamics are critical in determining the nature and impact of research outputs. However, existing methods for classifying author roles based on self-reports and clustering lack comprehensive contextual analysis of contributions. Thus, we present a transformative approach to classifying author roles in scientific teams using advanced large language models (LLMs), which offers a more refined analysis compared to traditional clustering methods. Specifically, we seek to complement and enhance these traditional methods by utilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting, we categorize author roles and demonstrate that GPT-4 outperforms other models across multiple categories, surpassing traditional approaches such as XGBoost and BERT. Our methodology also includes building a predictive deep learning model using 10 features. By training this model on a dataset derived from the OpenAlex database, which provides detailed metadata on academic publications – such as author-publication history, author affiliation, research topics, and citation counts – we achieve an F1 score of 0.76, demonstrating robust classification of author roles. </p>
<blockquote>
<p>科研团队的动态在决定研究成果的性质和影响方面至关重要。然而，现有的基于自我报告和聚类的作者角色分类方法缺乏对贡献的全面上下文分析。因此，我们提出了一种利用先进的大型语言模型（LLM）对科研团队中的作者角色进行分类的变革性方法，与传统聚类方法相比，它提供了更精细的分析。具体来说，我们希望通过利用开源和专有LLM（如GPT-4、Llama3 70B、Llama2 70B和Mistral 7x8B）来补充和增强这些传统方法，进行角色分类。我们采用少量提示的方法对作者角色进行分类，并证明GPT-4在多类别中表现优于其他模型，超越了传统的XGBoost和BERT等方法。我们的方法还包括建立一个预测深度学习模型，使用10个特征。通过在OpenAlex数据库衍生的数据集上训练该模型，该数据库提供了关于学术出版物的详细元数据，如作者出版历史、作者隶属关系、研究主题和引用计数等，我们获得了0.76的F1分数，证明了作者角色分类的稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07267v2">PDF</a> 16 pages, 5 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>科研团队中的动态对研究结果有着重要影响。现有的作者角色分类方法主要基于自我报告和聚类，缺乏对贡献的全面上下文分析。为此，本文提出使用大型语言模型（LLM）来精细分类作者角色的创新方法。通过利用开源和专有LLM（如GPT-4、Llama3 70B等）进行角色分类，我们的方法超越了传统的聚类方法。使用少样本提示技术进行分类，结果显示GPT-4在多个类别中的表现超过其他模型和传统方法，如XGBoost和BERT。通过训练深度学习模型对来自OpenAlex数据库的包含作者出版历史、作者隶属关系等详细信息的数据集进行预测，我们实现了F1分数为0.76的分类效果。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>作者角色分类对于评估科研团队动态和研究成果至关重要。</li>
<li>传统基于自我报告和聚类的作者角色分类方法缺乏全面的上下文分析。</li>
<li>大型语言模型（LLM）提供了一种更精细的分类方法，能更准确地分析作者的贡献。</li>
<li>GPT-4在作者角色分类中表现优异，超过了其他模型和传统方法。</li>
<li>使用少样本提示技术进行分类是一种有效的策略。</li>
<li>利用OpenAlex数据库的详细信息训练深度学习模型，实现了较高的分类效果（F1分数为0.76）。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07267">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e8a299164449d30c742d0b21507a37ff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a6688da5e0d419e2d4d3d259c337d2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b02efb3fc36694be89eef2cb2d49fd4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ConSim-Measuring-Concept-Based-Explanations’-Effectiveness-with-Automated-Simulatability"><a href="#ConSim-Measuring-Concept-Based-Explanations’-Effectiveness-with-Automated-Simulatability" class="headerlink" title="ConSim: Measuring Concept-Based Explanations’ Effectiveness with   Automated Simulatability"></a>ConSim: Measuring Concept-Based Explanations’ Effectiveness with   Automated Simulatability</h2><p><strong>Authors:Antonin Poché, Alon Jacovi, Agustin Martin Picard, Victor Boutin, Fanny Jourdan</strong></p>
<p>Concept-based explanations work by mapping complex model computations to human-understandable concepts. Evaluating such explanations is very difficult, as it includes not only the quality of the induced space of possible concepts but also how effectively the chosen concepts are communicated to users. Existing evaluation metrics often focus solely on the former, neglecting the latter. We introduce an evaluation framework for measuring concept explanations via automated simulatability: a simulator’s ability to predict the explained model’s outputs based on the provided explanations. This approach accounts for both the concept space and its interpretation in an end-to-end evaluation. Human studies for simulatability are notoriously difficult to enact, particularly at the scale of a wide, comprehensive empirical evaluation (which is the subject of this work). We propose using large language models (LLMs) as simulators to approximate the evaluation and report various analyses to make such approximations reliable. Our method allows for scalable and consistent evaluation across various models and datasets. We report a comprehensive empirical evaluation using this framework and show that LLMs provide consistent rankings of explanation methods. Code available at <a target="_blank" rel="noopener" href="https://github.com/AnonymousConSim/ConSim">https://github.com/AnonymousConSim/ConSim</a>. </p>
<blockquote>
<p>基于概念的解释是通过将复杂的模型计算映射到人类可理解的概念来工作的。评估这样的解释是非常困难的，因为它不仅包括可能概念空间的诱导质量，还包括所选概念如何有效地传达给用户。现有的评估指标往往只关注前者，而忽视了后者。我们引入了一个评估框架，通过自动化模拟性来衡量概念解释：模拟器根据提供的解释预测解释模型的输出的能力。这种方法既考虑了概念空间，又考虑了其端到端的最终解释。模拟性的人体研究实施起来尤为困难，尤其是在广泛而全面的经验评估的尺度上（这是本文的主题）。我们建议使用大型语言模型（LLM）作为模拟器来进行近似评估，并报告各种分析以使这些近似值可靠。我们的方法允许在各种模型和数据集上进行可扩展和一致性的评估。我们报告了使用此框架进行的全面经验评估，并展示了LLM对解释方法的一致排名。代码可在 <a target="_blank" rel="noopener" href="https://github.com/AnonymousConSim/ConSim">https://github.com/AnonymousConSim/ConSim</a> 找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05855v3">PDF</a> </p>
<p><strong>Summary</strong><br>概念型解释的评估难度较高，因为它既涉及可能概念的质量，也涉及如何有效地向用户传达所选概念。现有的评估指标往往只关注前者而忽略了后者。本研究引入了一个评估框架，通过模拟能力来衡量概念解释，即模拟器根据提供的解释预测解释模型的输出的能力。此方法考虑了概念空间及其解释两个方面，进行全面评估。鉴于对人类研究的模拟性评估存在困难，特别是在广泛的实证评估中，我们提出使用大型语言模型（LLM）作为模拟器进行近似评估，并报告了各种分析以确保这种近似可靠。我们的方法允许对不同的模型和数据集进行可扩展和一致的评估。我们使用此框架进行了全面的实证评估，并展示了LLM对解释方法的一致排名。相关代码可在<a target="_blank" rel="noopener" href="https://github.com/AnonymousConSim/ConSim">匿名链接</a>获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>概念型解释通过将复杂的模型计算映射到人类可理解的概念来工作。</li>
<li>概念型解释的评估涉及两个主要方面：可能概念的质量和如何有效地向用户传达这些概念。</li>
<li>现有评估指标主要关注概念的质量，而忽视了用户沟通的有效性。</li>
<li>引入了一个基于模拟能力的评估框架来衡量概念解释，同时考虑了概念空间和其解释。</li>
<li>使用大型语言模型（LLM）作为模拟器来近似评估人类研究的难度和挑战性进行了讨论。</li>
<li>提出了一种使用LLM进行可靠近似评估的方法，并展示了其在不同模型和数据集上的可扩展性和一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05855">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2717bc60520b87375e48d4b843d51155.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-569c4b60057d7b21e9193d6b30bec592.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4a4cc152c4baba105b0b5080b7a2650.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c4b1a27b46fa592775fcf45364fed70.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Multi-modal-Agent-Tuning-Building-a-VLM-Driven-Agent-for-Efficient-Tool-Usage"><a href="#Multi-modal-Agent-Tuning-Building-a-VLM-Driven-Agent-for-Efficient-Tool-Usage" class="headerlink" title="Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool   Usage"></a>Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool   Usage</h2><p><strong>Authors:Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaojian Ma, Tao Yuan, Yue Fan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, Qing Li</strong></p>
<p>The advancement of large language models (LLMs) prompts the development of multi-modal agents, which are used as a controller to call external tools, providing a feasible way to solve practical tasks. In this paper, we propose a multi-modal agent tuning method that automatically generates multi-modal tool-usage data and tunes a vision-language model (VLM) as the controller for powerful tool-usage reasoning. To preserve the data quality, we prompt the GPT-4o mini model to generate queries, files, and trajectories, followed by query-file and trajectory verifiers. Based on the data synthesis pipeline, we collect the MM-Traj dataset that contains 20K tasks with trajectories of tool usage. Then, we develop the T3-Agent via \underline{T}rajectory \underline{T}uning on VLMs for \underline{T}ool usage using MM-Traj. Evaluations on the GTA and GAIA benchmarks show that the T3-Agent consistently achieves improvements on two popular VLMs: MiniCPM-V-8.5B and {Qwen2-VL-7B}, which outperforms untrained VLMs by $20%$, showing the effectiveness of the proposed data synthesis pipeline, leading to high-quality data for tool-usage capabilities. </p>
<blockquote>
<p>大型语言模型（LLM）的进步促进了多模态代理的发展，这些代理用作控制器来调用外部工具，为解决实际任务提供了可行的方法。在本文中，我们提出了一种多模态代理调整方法，该方法可自动生成多模态工具使用数据，并调整视觉语言模型（VLM）作为控制器，以进行强大的工具使用推理。为了保持数据质量，我们指示GPT-4o小型模型生成查询、文件和轨迹，随后通过查询文件验证器和轨迹验证器进行验证。基于数据合成管道，我们收集了MM-Traj数据集，其中包含包含工具使用轨迹的2万个任务。然后，我们通过MM-Traj上基于轨迹调整的T3代理开发方法开发出了T3-Agent。在GTA和GAIA基准测试上的评估表明，T3-Agent在两种流行的VLM上取得了持续的改进：MiniCPM-V-8.5B和Qwen2-VL-7B，其性能比未训练的VLM高出20%，证明了所提出的数据合成管道的有效性，为工具使用能力提供了高质量的数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15606v2">PDF</a> ICLR 2025, <a target="_blank" rel="noopener" href="https://mat-agent.github.io/">https://mat-agent.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的发展促进了多模态代理的开发，该代理可作为控制器调用外部工具，为解决实际任务提供了可行途径。本文提出了一种多模态代理调整方法，该方法可自动生成多模态工具使用数据，并调整视觉语言模型（VLM）作为控制器进行工具使用推理。为保持数据质量，使用GPT-4o小型模型生成查询、文件和轨迹，随后通过查询文件及轨迹验证器进行验证。基于数据合成管道，我们收集了MM-Traj数据集，包含2万个任务及工具使用轨迹。然后，我们通过MM-Traj在VLM上进行轨迹调整，开发出T3-Agent。在GTA和GAIA基准测试上的评估表明，T3-Agent在两种流行的VLM上实现了持续的改进，即MiniCPM-V-8.5B和Qwen2-VL-7B，其性能比未训练的VLM提高了20%，显示了数据合成管道的有效性，为工具使用能力生成了高质量数据。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）推动多模态代理发展，多模态代理能够作为控制器调用外部工具以解决实用任务。</li>
<li>提出了一种多模态代理调整方法，自动生成多模态工具使用数据并调整视觉语言模型（VLM）。</li>
<li>采用GPT-4o小型模型生成查询、文件和轨迹，以确保数据质量。</li>
<li>引入了查询文件及轨迹验证机制以确保数据的准确性和有效性。</li>
<li>基于数据合成管道，收集了包含2万个任务及工具使用轨迹的MM-Traj数据集。</li>
<li>开发出了T3-Agent，通过MM-Traj在VLM上进行轨迹调整。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15606">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-963b18445f42c73720c6ca81034c8c5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-167386209c58998672452da9d496a130.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-037e35ae237c29563089361fce834dbd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CATSplat-Context-Aware-Transformer-with-Spatial-Guidance-for-Generalizable-3D-Gaussian-Splatting-from-A-Single-View-Image"><a href="#CATSplat-Context-Aware-Transformer-with-Spatial-Guidance-for-Generalizable-3D-Gaussian-Splatting-from-A-Single-View-Image" class="headerlink" title="CATSplat: Context-Aware Transformer with Spatial Guidance for   Generalizable 3D Gaussian Splatting from A Single-View Image"></a>CATSplat: Context-Aware Transformer with Spatial Guidance for   Generalizable 3D Gaussian Splatting from A Single-View Image</h2><p><strong>Authors:Wonseok Roh, Hwanhee Jung, Jong Wook Kim, Seunggwan Lee, Innfarn Yoo, Andreas Lugmayr, Seunggeun Chi, Karthik Ramani, Sangpil Kim</strong></p>
<p>Recently, generalizable feed-forward methods based on 3D Gaussian Splatting have gained significant attention for their potential to reconstruct 3D scenes using finite resources. These approaches create a 3D radiance field, parameterized by per-pixel 3D Gaussian primitives, from just a few images in a single forward pass. However, unlike multi-view methods that benefit from cross-view correspondences, 3D scene reconstruction with a single-view image remains an underexplored area. In this work, we introduce CATSplat, a novel generalizable transformer-based framework designed to break through the inherent constraints in monocular settings. First, we propose leveraging textual guidance from a visual-language model to complement insufficient information from a single image. By incorporating scene-specific contextual details from text embeddings through cross-attention, we pave the way for context-aware 3D scene reconstruction beyond relying solely on visual cues. Moreover, we advocate utilizing spatial guidance from 3D point features toward comprehensive geometric understanding under single-view settings. With 3D priors, image features can capture rich structural insights for predicting 3D Gaussians without multi-view techniques. Extensive experiments on large-scale datasets demonstrate the state-of-the-art performance of CATSplat in single-view 3D scene reconstruction with high-quality novel view synthesis. </p>
<blockquote>
<p>最近，基于三维高斯Splatting的可泛化前馈方法因其利用有限资源重建三维场景的潜力而受到广泛关注。这些方法在单次前向传递中仅从少量图像中创建由像素级三维高斯基元参数化的三维辐射场。然而，不同于受益于跨视图对应关系的多视图方法，使用单视图图像进行三维场景重建仍然是一个未被充分研究的领域。在这项工作中，我们引入了CATSplat，这是一种新型的可泛化基于transformer的框架，旨在突破单目设置中的固有约束。首先，我们提出利用视觉语言模型的文本指导来补充单幅图像中的信息不足。通过结合文本嵌入的场景特定上下文细节进行交叉注意力，我们为基于文本指导的上下文感知三维场景重建铺平了道路，不再仅仅依赖于视觉线索。此外，我们主张利用三维点特征的空间指导来实现单视图设置下的全面几何理解。借助三维先验知识，图像特征可以捕捉丰富的结构信息，以预测三维高斯分布，无需使用多视图技术。在大规模数据集上的广泛实验表明，CATSplat在单视图三维场景重建中具有最先进的性能，并能进行高质量的新视角合成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12906v2">PDF</a> </p>
<p><strong>Summary</strong>:<br>基于文本指导与空间引导的单视角三维场景重建研究提出了一种新型的可泛化的变压器框架CATSplat。它结合了视觉语言模型的文本指导与三维点特征的空间指导，通过单张图像实现丰富的几何理解，突破单视角重建的内在限制。该框架在单视角三维场景重建方面表现出卓越性能，具有高质量的新视角合成能力。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>CATSplat是基于单张图像实现三维场景重建的新型框架。</li>
<li>利用视觉语言模型的文本指导补充单一图像信息不足的问题。</li>
<li>通过跨注意力机制引入场景特定的上下文细节，实现了基于文本的上下文感知三维场景重建。</li>
<li>引入了空间指导机制，利用三维点特征进行几何理解。</li>
<li>利用三维先验知识，图像特征可以捕捉丰富的结构信息，预测三维高斯分布，无需多视角技术。</li>
<li>在大规模数据集上的实验证明了CATSplat在单视角三维场景重建领域的先进性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12906">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-26625a8cd9fd0477e93c92a31f275e0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fcee7c7d3e153b917e17f05ceba3e90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68e3c41ed5ea4d5050fc93e670dcd597.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6b21a98b2a22c9cf98787dfb154b118.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56b1f79adf6fa9c78178ba24b46572b1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="The-Open-Source-Advantage-in-Large-Language-Models-LLMs"><a href="#The-Open-Source-Advantage-in-Large-Language-Models-LLMs" class="headerlink" title="The Open Source Advantage in Large Language Models (LLMs)"></a>The Open Source Advantage in Large Language Models (LLMs)</h2><p><strong>Authors:Jiya Manchanda, Laura Boettcher, Matheus Westphalen, Jasser Jasser</strong></p>
<p>Large language models (LLMs) have rapidly advanced natural language processing, driving significant breakthroughs in tasks such as text generation, machine translation, and domain-specific reasoning. The field now faces a critical dilemma in its approach: closed-source models like GPT-4 deliver state-of-the-art performance but restrict reproducibility, accessibility, and external oversight, while open-source frameworks like LLaMA and Mixtral democratize access, foster collaboration, and support diverse applications, achieving competitive results through techniques like instruction tuning and LoRA. Hybrid approaches address challenges like bias mitigation and resource accessibility by combining the scalability of closed-source systems with the transparency and inclusivity of open-source framework. However, in this position paper, we argue that open-source remains the most robust path for advancing LLM research and ethical deployment. </p>
<blockquote>
<p>大型语言模型（LLM）在自然语言处理领域取得了快速进展，推动了文本生成、机器翻译和领域特定推理等任务的重大突破。然而，该领域的方法面临一个关键的困境：像GPT-4这样的封闭源模型虽然提供了最先进的性能，但限制了可重复性、可访问性和外部监督；而像LLaMA和Mixtral这样的开源框架则实现了民主化的访问，促进了协作并支持多样化的应用，通过指令微调LoRA等技术取得了具有竞争力的结果。混合方法通过结合封闭系统的可扩展性和开源框架的透明度和包容性来解决偏见缓解和资源可及性等挑战。然而，在这篇立场论文中，我们认为开源仍然是推进LLM研究和道德部署的最稳健途径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12004v2">PDF</a> 9 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>大规模语言模型（LLM）在自然语言处理领域取得了快速进展，已在文本生成、机器翻译和领域特定推理等任务中取得了显著突破。当前，该领域面临着一个关键困境：封闭源模型如GPT-4虽然性能卓越，但限制了可重复性、可访问性和外部监督；而开源框架如LLaMA和Mixtral实现了民主化的访问和协作，并支持各种应用，通过指令微调等技术取得具有竞争力的结果。这篇立场论文认为，结合封闭源系统的可扩展性和开源框架的透明性与包容性，通过混合方法解决偏见缓解和资源可及性等挑战，但开源仍是推进LLM研究和道德部署的最稳健途径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs已在多个NLP任务中取得显著进展。</li>
<li>封闭源模型与开源模型各有优势和局限。</li>
<li>封闭源模型如GPT-4性能卓越，但限制可重复性和外部监督。</li>
<li>开源框架如LLaMA和Mixtral支持多样化应用，并实现民主化的访问和协作。</li>
<li>混合方法结合封闭源和开源的优势，解决挑战如偏见缓解和资源可及性。</li>
<li>立场论文强调开源是推进LLM研究和道德部署的最稳健途径。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12004">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-64a8efe4e74dff2a5d3ba44afebbc30a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-360bef31b822bdaf7d94c785ecaf45d7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Iris-Breaking-GUI-Complexity-with-Adaptive-Focus-and-Self-Refining"><a href="#Iris-Breaking-GUI-Complexity-with-Adaptive-Focus-and-Self-Refining" class="headerlink" title="Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining"></a>Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining</h2><p><strong>Authors:Zhiqi Ge, Juncheng Li, Xinglei Pang, Minghe Gao, Kaihang Pan, Wang Lin, Hao Fei, Wenqiao Zhang, Siliang Tang, Yueting Zhuang</strong></p>
<p>Digital agents are increasingly employed to automate tasks in interactive digital environments such as web pages, software applications, and operating systems. While text-based agents built on Large Language Models (LLMs) often require frequent updates due to platform-specific APIs, visual agents leveraging Multimodal Large Language Models (MLLMs) offer enhanced adaptability by interacting directly with Graphical User Interfaces (GUIs). However, these agents face significant challenges in visual perception, particularly when handling high-resolution, visually complex digital environments. This paper introduces Iris, a foundational visual agent that addresses these challenges through two key innovations: Information-Sensitive Cropping (ISC) and Self-Refining Dual Learning (SRDL). ISC dynamically identifies and prioritizes visually dense regions using a edge detection algorithm, enabling efficient processing by allocating more computational resources to areas with higher information density. SRDL enhances the agent’s ability to handle complex tasks by leveraging a dual-learning loop, where improvements in referring (describing UI elements) reinforce grounding (locating elements) and vice versa, all without requiring additional annotated data. Empirical evaluations demonstrate that Iris achieves state-of-the-art performance across multiple benchmarks with only 850K GUI annotations, outperforming methods using 10x more training data. These improvements further translate to significant gains in both web and OS agent downstream tasks. </p>
<blockquote>
<p>数字代理越来越多地被用于自动化网页、软件应用程序和操作系统等交互式数字环境中的任务。基于大型语言模型（LLM）的文本代理通常需要频繁更新以适应平台特定的API，而利用多模态大型语言模型（MLLM）的视觉代理通过直接与图形用户界面（GUI）交互，提供了更高的适应性。然而，这些代理在视觉感知方面面临重大挑战，尤其是在处理高分辨率、视觉复杂的数字环境时。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10342v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>数字代理在网页、软件应用和操作系统等交互式数字环境中被广泛应用于自动化任务。基于大型语言模型的文本代理通常需要频繁更新以适应平台特定的API，而利用多模态大型语言模型的视觉代理通过直接与用户界面交互提供了更高的适应性。然而，这些代理在视觉感知方面面临重大挑战，特别是在处理高分辨率和视觉复杂的数字环境时。本文介绍了Iris这一基础视觉代理，它通过两项关键创新技术解决了这些挑战：信息敏感裁剪（ISC）和自我完善双重学习（SRDL）。ISC使用边缘检测算法动态识别和优先处理视觉密集区域，SRDL则通过双重学习循环提高代理处理复杂任务的能力。实证评估显示，Iris在多个基准测试中实现了最先进的性能，仅使用85万GUI注释就优于使用10倍训练数据的方法。这些改进进一步转化为网页和操作系统代理下游任务的显著收益。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数字代理在交互式数字环境中广泛应用于自动化任务。</li>
<li>基于大型语言模型的文本代理需要频繁更新以适应平台特定的API。</li>
<li>视觉代理利用多模态大型语言模型直接与用户界面交互以提高适应性。</li>
<li>视觉代理在视觉感知方面面临处理高分辨率和视觉复杂数字环境的挑战。</li>
<li>Iris通过信息敏感裁剪（ISC）和自我完善双重学习（SRDL）解决这些挑战。</li>
<li>ISC使用边缘检测算法动态识别和处理视觉密集区域。</li>
<li>SRDL通过双重学习循环提高代理处理复杂任务的能力。Iris在多个基准测试中表现优秀，仅使用少量注释数据就实现了卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10342">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ceb7d61ef26d6501596f192f83f98f16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95e90b6ee0a3fc5c51ff5571700e50ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73833221afdf83a38c7c79589af7dd31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d45644094312b06caf231347068713d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-112d1182da6faa4d5b92d32d1d3827cc.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Frontiers-in-Intelligent-Colonoscopy"><a href="#Frontiers-in-Intelligent-Colonoscopy" class="headerlink" title="Frontiers in Intelligent Colonoscopy"></a>Frontiers in Intelligent Colonoscopy</h2><p><strong>Authors:Ge-Peng Ji, Jingyi Liu, Peng Xu, Nick Barnes, Fahad Shahbaz Khan, Salman Khan, Deng-Ping Fan</strong></p>
<p>Colonoscopy is currently one of the most sensitive screening methods for colorectal cancer. This study investigates the frontiers of intelligent colonoscopy techniques and their prospective implications for multimodal medical applications. With this goal, we begin by assessing the current data-centric and model-centric landscapes through four tasks for colonoscopic scene perception, including classification, detection, segmentation, and vision-language understanding. This assessment enables us to identify domain-specific challenges and reveals that multimodal research in colonoscopy remains open for further exploration. To embrace the coming multimodal era, we establish three foundational initiatives: a large-scale multimodal instruction tuning dataset ColonINST, a colonoscopy-designed multimodal language model ColonGPT, and a multimodal benchmark. To facilitate ongoing monitoring of this rapidly evolving field, we provide a public website for the latest updates: <a target="_blank" rel="noopener" href="https://github.com/ai4colonoscopy/IntelliScope">https://github.com/ai4colonoscopy/IntelliScope</a>. </p>
<blockquote>
<p>结肠镜检查目前是结直肠癌最敏感的检测方法之一。本研究旨在探讨智能结肠镜检查技术的最新进展及其对多模式医学应用的潜在影响。为此，我们首先通过四项结肠镜场景感知任务来评估当前以数据为中心和以模型为中心的情况，包括分类、检测、分割和视觉语言理解。这一评估使我们能够确定特定领域的挑战，并表明结肠镜检查中的多模式研究仍有待进一步探索。为了迎接即将到来的多模式时代，我们建立了三个基本项目：大规模多模式指令调整数据集ColonINST、针对结肠镜检查设计的多模式语言模型ColonGPT以及多模式基准测试。为了促进这一快速发展领域的持续监测，我们提供了最新更新的公共网站：<a target="_blank" rel="noopener" href="https://github.com/ai4colonoscopy/IntelliScope%E3%80%82">https://github.com/ai4colonoscopy/IntelliScope。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17241v2">PDF</a> [Work in progress] A comprehensive survey of intelligent colonoscopy   in the multimodal era. [Updated Version V2] New training strategy for   colonoscopy-specific multimodal language model</p>
<p><strong>总结</strong></p>
<p>本文探讨了智能结肠镜检查技术的最新进展及其在多模态医疗应用中的潜在影响。文章通过评估结肠镜检查的当前数据为中心和模型为中心的现状，包括分类、检测、分割和视觉语言理解等任务，确定了特定领域的挑战，并指出结肠镜检中的多模态研究仍有待进一步探索。为迎接即将到来的多模态时代，本文建立了三个基础项目：大规模多模态指令调整数据集ColonINST、结肠镜检查设计的多模态语言模型ColonGPT和多模态基准测试。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>智能结肠镜检查技术是结肠癌筛查中最敏感的方法之一。</li>
<li>本文通过四项任务评估了结肠镜检查的当前状况，包括分类、检测、分割和视觉语言理解。</li>
<li>文章指出了当前领域存在的特定挑战，如数据收集和模型性能优化等。</li>
<li>多模态研究在结肠镜检查中仍有待进一步探索和发展。</li>
<li>为推动这一领域的发展，建立了三个基础项目：ColonINST数据集、ColonGPT模型和多模态基准测试。</li>
<li>这些项目旨在提高智能结肠镜检查技术的性能和效率，以及推动其在多模态医疗应用中的广泛应用。</li>
<li>最后，文章提供了一个公共网站以提供最新更新的信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.17241">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5326b71b9dc5824ce9ad4f28c6d12e8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b53b24981eaa4ba63c757e6b8c26e90e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bfa2469b1553cd7f4c56a1663ac7c9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94389db38ff181a7c35cc9770aa2cc86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e8bf3b00154cab5b4bcd52e8b3d483d7.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Efficient-Annotator-Reliability-Assessment-and-Sample-Weighting-for-Knowledge-Based-Misinformation-Detection-on-Social-Media"><a href="#Efficient-Annotator-Reliability-Assessment-and-Sample-Weighting-for-Knowledge-Based-Misinformation-Detection-on-Social-Media" class="headerlink" title="Efficient Annotator Reliability Assessment and Sample Weighting for   Knowledge-Based Misinformation Detection on Social Media"></a>Efficient Annotator Reliability Assessment and Sample Weighting for   Knowledge-Based Misinformation Detection on Social Media</h2><p><strong>Authors:Owen Cook, Charlie Grimshaw, Ben Wu, Sophie Dillon, Jack Hicks, Luke Jones, Thomas Smith, Matyas Szert, Xingyi Song</strong></p>
<p>Misinformation spreads rapidly on social media, confusing the truth and targeting potentially vulnerable people. To effectively mitigate the negative impact of misinformation, it must first be accurately detected before applying a mitigation strategy, such as X’s community notes, which is currently a manual process. This study takes a knowledge-based approach to misinformation detection, modelling the problem similarly to one of natural language inference. The EffiARA annotation framework is introduced, aiming to utilise inter- and intra-annotator agreement to understand the reliability of each annotator and influence the training of large language models for classification based on annotator reliability. In assessing the EffiARA annotation framework, the Russo-Ukrainian Conflict Knowledge-Based Misinformation Classification Dataset (RUC-MCD) was developed and made publicly available. This study finds that sample weighting using annotator reliability performs the best, utilising both inter- and intra-annotator agreement and soft-label training. The highest classification performance achieved using Llama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large. </p>
<blockquote>
<p>社交媒体上的错误信息迅速传播，混淆真相，针对潜在弱势群体。为了有效减轻错误信息带来的负面影响，必须在应用缓解策略之前准确检测错误信息，例如X的社区笔记，但目前这是一个手动过程。本研究采用基于知识的方法检测错误信息，将问题模拟为自然语言推理问题之一。介绍了EffiARA注释框架，旨在利用标注者之间的内部和外部共识来理解每个标注者的可靠性，并基于标注者可靠性训练大规模语言模型进行分类。在评估EffiARA注释框架时，开发了基于知识的关于俄乌冲突的信息分类数据集（RUC-MCD），并已公开发布。本研究发现，使用标注者可靠性进行样本加权的表现最好，它结合了标注者之间的内部和外部共识和软标签训练。使用Llama-3.2-1B实现的最高分类性能是宏F1值为0.757，使用TwHIN-BERT-large的宏F1值为0.740。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14515v2">PDF</a> 8 pages, 3 figures, 3 tables. Code available here:   <a target="_blank" rel="noopener" href="https://github.com/MiniEggz/ruc-misinfo">https://github.com/MiniEggz/ruc-misinfo</a>; annotation framework available here:   <a target="_blank" rel="noopener" href="https://github.com/MiniEggz/EffiARA">https://github.com/MiniEggz/EffiARA</a></p>
<p><strong>摘要</strong><br>社交媒体上错误信息传播迅速，混淆真相，针对潜在易受害群体。为有效缓解错误信息带来的负面影响，必须在采取缓解策略之前准确检测错误信息，如X的社区注释，目前这一过程是手动进行的。本研究采用基于知识的方法检测错误信息，将问题模拟为自然语言推理问题。引入EffiARA注释框架，旨在利用注释员之间的内部和外部共识来理解每个注释员的可靠性，并影响基于注释员可靠性分类的大型语言模型的训练。在评估EffiARA注释框架时，开发了基于俄乌冲突知识错误的分类数据集（RUC-MCD），并已公开供公众使用。本研究发现，使用注释员可靠性进行样本加权的表现最好，并同时使用注释员之间的内部和外部共识和软标签训练。使用Llama-3.2-1B时最高分类性能达到的宏观F值为0.757，使用TwHIN-BERT-large时为0.740。</p>
<p><strong>要点提炼</strong></p>
<ol>
<li>社交媒体上的错误信息能迅速传播，混淆真相，并对潜在易受害群体产生影响。</li>
<li>为缓解错误信息的影响，需先进行准确检测，再采取相应缓解策略。</li>
<li>本研究采用基于知识的方法检测错误信息，模拟为自然语言推理问题。</li>
<li>引入EffiARA注释框架，利用注释员之间的内外共识理解注释员可靠性，并影响语言模型的训练。</li>
<li>开发并公开了RUC-MCD数据集用于评估。</li>
<li>研究发现样本加权结合内外共识和软标签训练的方式在错误信息检测中表现最佳。</li>
<li>使用Llama-3.2-1B和TwHIN-BERT-large模型时，最高分类性能的宏观F值分别为0.757和0.740。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.14515">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c18f91a4bc70964df99d6b371ad8bd8f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3577bb927cecc6d2ece3df74e7531c6d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e211047149cd91e7e9f3b2002317966.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56683aa62c076a99ad005d656eabde86.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CollabEdit-Towards-Non-destructive-Collaborative-Knowledge-Editing"><a href="#CollabEdit-Towards-Non-destructive-Collaborative-Knowledge-Editing" class="headerlink" title="CollabEdit: Towards Non-destructive Collaborative Knowledge Editing"></a>CollabEdit: Towards Non-destructive Collaborative Knowledge Editing</h2><p><strong>Authors:Jiamu Zheng, Jinghuai Zhang, Tianyu Du, Xuhong Zhang, Jianwei Yin, Tao Lin</strong></p>
<p>Collaborative learning of large language models (LLMs) has emerged as a new paradigm for utilizing private data from different parties to guarantee efficiency and privacy. Meanwhile, Knowledge Editing (KE) for LLMs has also garnered increased attention due to its ability to manipulate the behaviors of LLMs explicitly, yet leaves the collaborative KE case (in which knowledge edits of multiple parties are aggregated in a privacy-preserving and continual manner) unexamined. To this end, this manuscript dives into the first investigation of collaborative KE, in which we start by carefully identifying the unique three challenges therein, including knowledge overlap, knowledge conflict, and knowledge forgetting. We then propose a non-destructive collaborative KE framework, COLLABEDIT, which employs a novel model merging mechanism to mimic the global KE behavior while preventing the severe performance drop. Extensive experiments on two canonical datasets demonstrate the superiority of COLLABEDIT compared to other destructive baselines, and results shed light on addressing three collaborative KE challenges and future applications. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/LINs-lab/CollabEdit">https://github.com/LINs-lab/CollabEdit</a>. </p>
<blockquote>
<p>协同学习大型语言模型（LLM）已经成为利用不同方的私有数据以保证效率和隐私的新范式。与此同时，由于能够明确地操作LLM的行为，LLM的知识编辑（KE）也引起了越来越多的关注，但协同KE的情况（即多方知识编辑以隐私保护和持续的方式进行聚合）尚未得到研究。为此，本文深入研究了协同KE，首先仔细确定了其中的三个独特挑战，包括知识重叠、知识冲突和知识遗忘。然后，我们提出了非破坏性的协同KE框架COLLABEDIT，该框架采用了一种新型模型合并机制来模拟全局KE行为，同时防止性能严重下降。在两个典型数据集上的大量实验证明了COLLABEDIT相较于其他破坏性基准线的优越性，实验结果也为我们解决了三个协同KE挑战以及未来的应用提供了启示。我们的代码位于<a target="_blank" rel="noopener" href="https://github.com/LINs-lab/CollabEdit%E3%80%82">https://github.com/LINs-lab/CollabEdit。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.09508v2">PDF</a> 20 pages, 11 figures. Published as a conference paper at ICLR 2025.   Code at <a target="_blank" rel="noopener" href="https://github.com/LINs-lab/CollabEdit">https://github.com/LINs-lab/CollabEdit</a></p>
<p><strong>Summary</strong></p>
<p>本文探索了基于大型语言模型（LLM）的协同知识编辑（Knowledge Editing, KE）。文章首先确定了协同KE的三个独特挑战，包括知识重叠、知识冲突和知识遗忘。然后提出了一种非破坏性的协同KE框架COLLABEDIT，采用新颖的模型合并机制来模拟全局KE行为，防止性能严重下降。实验证明，COLLABEDIT在解决协同KE的三个挑战方面表现优越。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>协同学习大型语言模型（LLM）已成为利用各方私有数据的新模式，兼顾效率和隐私。</li>
<li>知识编辑（KE）技术可以明确操控LLM的行为，但协同知识编辑（协同KE）尚未得到充分研究。</li>
<li>协同KE面临三个独特挑战：知识重叠、知识冲突和知识遗忘。</li>
<li>COLLABEDIT框架采用非破坏性方法处理协同KE问题，通过模拟全局KE行为来合并模型。</li>
<li>COLLABEDIT框架在典型数据集上的实验表现优于其他破坏性基线方法。</li>
<li>COLLABEDIT框架为解决协同KE的三个挑战提供了有效手段。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.09508">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0d0a2d317d27849c4319b6748ac95e12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3668c21ab9c13cd23995f8c27ef61296.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="COMPL-AI-Framework-A-Technical-Interpretation-and-LLM-Benchmarking-Suite-for-the-EU-Artificial-Intelligence-Act"><a href="#COMPL-AI-Framework-A-Technical-Interpretation-and-LLM-Benchmarking-Suite-for-the-EU-Artificial-Intelligence-Act" class="headerlink" title="COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking   Suite for the EU Artificial Intelligence Act"></a>COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking   Suite for the EU Artificial Intelligence Act</h2><p><strong>Authors:Philipp Guldimann, Alexander Spiridonov, Robin Staab, Nikola Jovanović, Mark Vero, Velko Vechev, Anna-Maria Gueorguieva, Mislav Balunović, Nikola Konstantinov, Pavol Bielik, Petar Tsankov, Martin Vechev</strong></p>
<p>The EU’s Artificial Intelligence Act (AI Act) is a significant step towards responsible AI development, but lacks clear technical interpretation, making it difficult to assess models’ compliance. This work presents COMPL-AI, a comprehensive framework consisting of (i) the first technical interpretation of the EU AI Act, translating its broad regulatory requirements into measurable technical requirements, with the focus on large language models (LLMs), and (ii) an open-source Act-centered benchmarking suite, based on thorough surveying and implementation of state-of-the-art LLM benchmarks. By evaluating 12 prominent LLMs in the context of COMPL-AI, we reveal shortcomings in existing models and benchmarks, particularly in areas like robustness, safety, diversity, and fairness. This work highlights the need for a shift in focus towards these aspects, encouraging balanced development of LLMs and more comprehensive regulation-aligned benchmarks. Simultaneously, COMPL-AI for the first time demonstrates the possibilities and difficulties of bringing the Act’s obligations to a more concrete, technical level. As such, our work can serve as a useful first step towards having actionable recommendations for model providers, and contributes to ongoing efforts of the EU to enable application of the Act, such as the drafting of the GPAI Code of Practice. </p>
<blockquote>
<p>欧盟的《人工智能法案》（AI法案）是朝着负责任的人工智能发展迈出的重要一步，但它缺乏明确的技术解读，使得难以评估模型是否符合法规要求。本文介绍了COMPL-AI，这是一个全面的框架，包括（i）对欧盟AI法案的首份技术解读，将宽泛的监管要求转化为可衡量的技术要求，重点关注大型语言模型（LLM）；（ii）一个以法案为中心的开源基准测试套件，基于对当前最前沿LLM基准测试的深入调查和实施。通过COMPL-AI评估了12个突出的大型语言模型，我们揭示了现有模型和基准测试在稳健性、安全性、多样性和公平性等方面的不足。这项工作强调了需要关注这些方面，鼓励大型语言模型的平衡发展，以及更全面的符合法规要求的基准测试。同时，COMPLI-AI首次展示了将法案义务转化为更具体的技术层面所面临的可能性和困难。因此，我们的工作可以为模型提供商提供可行的建议，为欧盟实施该法案的努力做出贡献，如起草GPAI实践守则等。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.07959v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该论文介绍了EU的AI法案（AI Act）在推动人工智能发展方面的积极作用，但法案缺乏明确的技术解读，难以评估模型的合规性。为此，论文提出了COMPL-AI框架，该框架首次对EU AI法案进行了技术解读，将广泛的监管要求转化为可度量的技术要求，并重点关注大型语言模型（LLM）。通过评估12个突出的大型语言模型，论文揭示了现有模型和基准测试的不足，特别是在稳健性、安全性、多样性和公平性方面。这项工作强调了需要关注这些方面，并鼓励大型语言模型的平衡发展以及更全面的法规基准测试。同时，COMPL-AI首次展示了将法案义务转化为更具体技术层面可能性和困难。因此，这项工作可以为模型提供商提供可操作的建议，并为欧盟实施该法案的努力做出贡献。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EU的AI法案是朝着负责任的AI发展迈出的重要一步，但缺乏明确的技术解读。</li>
<li>COMPL-AI框架首次对EU AI法案进行了技术解读，并特别关注大型语言模型。</li>
<li>通过评估发现，现有大型语言模型在稳健性、安全性、多样性和公平性方面存在不足。</li>
<li>需要更多关注大型语言模型的平衡发展以及更全面的法规基准测试。</li>
<li>COMPL-AI展示了将法案义务转化为具体技术层面的可能性和困难。</li>
<li>该工作为模型提供商提供了可操作的建议。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.07959">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ff5a64c67eda1ff49e21b90e501d5158.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fee550a034d38534f0684b670ccdc2c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="TIS-DPO-Token-level-Importance-Sampling-for-Direct-Preference-Optimization-With-Estimated-Weights"><a href="#TIS-DPO-Token-level-Importance-Sampling-for-Direct-Preference-Optimization-With-Estimated-Weights" class="headerlink" title="TIS-DPO: Token-level Importance Sampling for Direct Preference   Optimization With Estimated Weights"></a>TIS-DPO: Token-level Importance Sampling for Direct Preference   Optimization With Estimated Weights</h2><p><strong>Authors:Aiwei Liu, Haoping Bai, Zhiyun Lu, Yanchao Sun, Xiang Kong, Simon Wang, Jiulong Shan, Albin Madappally Jose, Xiaojiang Liu, Lijie Wen, Philip S. Yu, Meng Cao</strong></p>
<p>Direct Preference Optimization (DPO) has been widely adopted for preference alignment of Large Language Models (LLMs) due to its simplicity and effectiveness. However, DPO is derived as a bandit problem in which the whole response is treated as a single arm, ignoring the importance differences between tokens, which may affect optimization efficiency and make it difficult to achieve optimal results. In this work, we propose that the optimal data for DPO has equal expected rewards for each token in winning and losing responses, as there is no difference in token importance. However, since the optimal dataset is unavailable in practice, we propose using the original dataset for importance sampling to achieve unbiased optimization. Accordingly, we propose a token-level importance sampling DPO objective named TIS-DPO that assigns importance weights to each token based on its reward. Inspired by previous works, we estimate the token importance weights using the difference in prediction probabilities from a pair of contrastive LLMs. We explore three methods to construct these contrastive LLMs: (1) guiding the original LLM with contrastive prompts, (2) training two separate LLMs using winning and losing responses, and (3) performing forward and reverse DPO training with winning and losing responses. Experiments show that TIS-DPO significantly outperforms various baseline methods on harmlessness and helpfulness alignment and summarization tasks. We also visualize the estimated weights, demonstrating their ability to identify key token positions. </p>
<blockquote>
<p>直接偏好优化（DPO）因简单有效而广泛应用于大型语言模型（LLM）的偏好对齐。然而，DPO是作为一种强盗问题而推导出来的，它将整个响应视为一个单一的臂，忽略了标记之间的重要性差异，这可能会影响优化效率，并难以达到最佳结果。在我们的工作中，我们提出DPO的最优数据在获胜和失败响应中为每个标记提供相等的预期奖励，因为标记的重要性没有差异。然而，由于在实际操作中无法获得最优数据集，我们建议使用原始数据集进行重要性采样以实现无偏优化。因此，我们提出了基于标记级别重要性采样的DPO目标，命名为TIS-DPO，它根据每个标记的奖励来分配重要性权重。受以前工作的启发，我们估计标记重要性权重是使用一对对比LLM的预测概率差异。我们探索了构建这些对比LLM的三种方法：1）用对比提示引导原始LLM；2）使用获胜和失败响应训练两个单独的LLM；以及3）使用获胜和失败响应进行正向和反向DPO训练。实验表明，在无害性、有益性对齐和摘要任务方面，TIS-DPO显著优于各种基线方法。我们还可视化了估计的权重，展示了它们识别关键标记位置的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.04350v2">PDF</a> 30 pages, 8 figures, 8 tables, Published in ICLR 2025</p>
<p><strong>Summary</strong><br>大语言模型（LLM）偏好对齐中广泛采用了直接偏好优化（DPO）方法，因其简单有效。但DPO方法在处理响应时将整个响应视为单个臂，忽略了标记间的差异重要性，可能影响优化效率并难以达到最优结果。本研究提出在DPO的最优数据中，获胜和失败响应中的每个标记应有相等的预期奖励。为实践中的最优数据集缺失问题，我们提出了重要性采样策略并使用原数据集进行无偏优化。据此，我们提出了名为TIS-DPO的标记级别重要性采样DPO目标，根据奖励为每个标记分配重要性权重。受先前工作的启发，我们利用一对对比LLM的预测概率差异来估计标记重要性权重。本研究探讨了构建对比LLM的三种方法，并在无害性、有益性对齐和摘要任务上进行了实验验证，证明TIS-DPO显著优于各种基线方法。我们还展示了估计权重的可视化能力，能识别关键标记位置。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>直接偏好优化（DPO）在LLM偏好对齐中的广泛应用及其简洁有效性。</li>
<li>DPO方法存在的问题：忽视标记间的重要性差异，可能影响优化效率和结果优化。</li>
<li>提出在最优数据集中每个标记应有相等预期奖励的观点。</li>
<li>为解决实践中最优数据集的缺失问题，采用重要性采样策略并利用原数据集进行无偏优化。</li>
<li>提出名为TIS-DPO的标记级别重要性采样DPO目标，根据奖励为每个标记分配权重。</li>
<li>利用对比LLM的预测概率差异来估计标记重要性权重的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.04350">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-79b664213186147549a75840b6856c16.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-64337a5b9f1356a91ed984bee71a71f7.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GPT-4o-as-the-Gold-Standard-A-Scalable-and-General-Purpose-Approach-to-Filter-Language-Model-Pretraining-Data"><a href="#GPT-4o-as-the-Gold-Standard-A-Scalable-and-General-Purpose-Approach-to-Filter-Language-Model-Pretraining-Data" class="headerlink" title="GPT-4o as the Gold Standard: A Scalable and General Purpose Approach to   Filter Language Model Pretraining Data"></a>GPT-4o as the Gold Standard: A Scalable and General Purpose Approach to   Filter Language Model Pretraining Data</h2><p><strong>Authors:Jifan Zhang, Ziyue Luo, Jia Liu, Ness Shroff, Robert Nowak</strong></p>
<p>Large language models require vast amounts of high-quality training data, but effective filtering of web-scale datasets remains a significant challenge. This paper demonstrates that GPT-4o is remarkably effective at identifying high-quality training data, but its prohibitive cost makes it impractical at web-scale. We propose SIEVE, a lightweight alternative that matches GPT-4o accuracy at less than 1% of the cost. SIEVE can perform up to 500 filtering operations for the cost of one GPT-4o filtering call. The key to SIEVE is a seamless integration of GPT-4o and lightweight text classification models, using active learning to fine-tune these models in the background with a small number of calls to GPT-4o. Once trained, it performs as well as GPT-4o at a tiny fraction of the cost. Through different filtering prompts, SIEVE can efficiently curate high quality data for general or specialized domains from web-scale corpora – a valuable capability given the current scarcity of high-quality domain-specific datasets. Extensive experiments using automatic and human evaluation metrics show that SIEVE and GPT-4o achieve similar performance on five highly specific filtering prompts. In addition, when performing quality filtering on web crawl datasets, we demonstrate SIEVE can further improve over state-of-the-art quality filtering methods in the DataComp-LM challenge for selecting LLM pretraining data. </p>
<blockquote>
<p>大型语言模型需要大量的高质量训练数据，但如何在网络规模的数据集中进行有效的过滤仍然是一个巨大的挑战。本文证明了GPT-4o在识别高质量训练数据方面非常有效，但其高昂的成本使得在网络规模上难以实现。我们提出了SIEVE，一个轻量级的替代品，其准确性可与GPT-4o相媲美，但成本不到其百分之一。SIEVE可以在一次GPT-4o过滤调用的成本下执行高达500次过滤操作。SIEVE的关键在于无缝集成了GPT-4o和轻量级文本分类模型，利用主动学习在后台微调这些模型，只需少量调用GPT-4o即可。一旦训练完成，它的表现与GPT-4o一样出色，但成本却很小。通过不同的过滤提示，SIEVE可以有效地从网络规模语料库中为通用或特定领域筛选高质量数据——在当前高质量特定领域数据集稀缺的情况下，这是一项非常有价值的能力。使用自动和人类评估指标的广泛实验表明，SIEVE和GPT-4o在五个高度特定的过滤提示上取得了相似的性能。此外，在对网络爬虫数据集进行质量过滤时，我们在DataComp-LM挑战中进一步证明了SIEVE在选取LLM预训练数据方面优于当前先进的质量过滤方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02755v3">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本论文针对大型语言模型需要大量高质量训练数据的问题，提出一种名为SIEVE的轻量级筛选方法。该方法结合了GPT-4o和轻量级文本分类模型，通过主动学习进行微调，能够在成本较低的情况下实现与GPT-4o相近的筛选效果。SIEVE可以高效地从网络规模语料库中筛选高质量数据，特别是在特定领域表现突出。实验表明，其在五个高度特定的筛选提示以及网络爬虫数据集的质量筛选任务中表现出优越性能，并在DataComp-LM挑战中进一步改进了现有质量筛选方法。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型需要大量高质量训练数据，但筛选这些数据是一大挑战。</li>
<li>GPT-4o在识别高质量训练数据方面表现出色，但成本高昂，难以实现大规模应用。</li>
<li>SIEVE是一种轻量级筛选方法，能够在成本较低的情况下实现与GPT-4o相近的筛选效果。</li>
<li>SIEVE通过结合GPT-4o和轻量级文本分类模型，使用主动学习进行微调。</li>
<li>SIEVE可以高效地从网络规模语料库中筛选高质量数据，特别是在特定领域表现突出。</li>
<li>实验证明，SIEVE在多个特定筛选任务及网络爬虫数据集的质量筛选中表现优越。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02755">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-14a5b8621d516759f1fbe0f6ff3dd45c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a198f7c21cd624bb6ce782253f07a07c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31cd2e7b5fce530e5c7be18b08c44287.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fcbe3a10a82ad41f59cad5af04b29c8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="How-to-Make-LLMs-Strong-Node-Classifiers"><a href="#How-to-Make-LLMs-Strong-Node-Classifiers" class="headerlink" title="How to Make LLMs Strong Node Classifiers?"></a>How to Make LLMs Strong Node Classifiers?</h2><p><strong>Authors:Zhe Xu, Kaveh Hassani, Si Zhang, Hanqing Zeng, Michihiro Yasunaga, Limei Wang, Dongqi Fu, Ning Yao, Bo Long, Hanghang Tong</strong></p>
<p>Language Models (LMs) are increasingly challenging the dominance of domain-specific models, such as Graph Neural Networks (GNNs) and Graph Transformers (GTs), in graph learning tasks. Following this trend, we propose a novel approach that empowers off-the-shelf LMs to achieve performance comparable to state-of-the-art (SOTA) GNNs on node classification tasks, without requiring any architectural modification. By preserving the LM’s original architecture, our approach retains a key benefit of LM instruction tuning: the ability to jointly train on diverse datasets, fostering greater flexibility and efficiency. To achieve this, we introduce two key augmentation strategies: (1) Enriching LMs’ input using topological and semantic retrieval methods, which provide richer contextual information, and (2) guiding the LMs’ classification process through a lightweight GNN classifier that effectively prunes class candidates. Our experiments on real-world datasets show that backbone Flan-T5 LMs equipped with these augmentation strategies outperform SOTA text-output node classifiers and are comparable to top-performing vector-output node classifiers. By bridging the gap between specialized node classifiers and general LMs, this work paves the way for more versatile and widely applicable graph learning models. We will open-source the code upon publication. </p>
<blockquote>
<p>语言模型（LMs）正在挑战图神经网络（GNNs）和图转换器（GTs）等特定领域模型在图学习任务中的主导地位。随着这一趋势，我们提出了一种新方法，使现成的语言模型能够在节点分类任务上实现与最新技术（SOTA）GNN相比的性能，而无需进行任何架构修改。通过保留LM的原始架构，我们的方法保留了LM指令调整的关键优势：在多样数据集上进行联合训练的能力，提高了灵活性和效率。为了实现这一点，我们引入了两种关键的增强策略：（1）使用拓扑和语义检索方法丰富LM的输入，提供更丰富的上下文信息；（2）通过一个轻量级的GNN分类器引导LM的分类过程，有效缩减类别候选。我们在真实数据集上的实验表明，配备这些增强策略的后端Flan-T5 LMs超越了最先进的文本输出节点分类器，并与顶级向量输出节点分类器的性能相当。通过缩小专业节点分类器和通用LM之间的差距，这项工作为更通用和更广泛适用的图学习模型铺平了道路。代码将在发布时开源。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02296v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>语言模型（LMs）正在挑战图神经网络（GNNs）和图转换器（GTs）等域特定模型在图学习任务中的主导地位。本文提出了一种新型方法，在不改变架构的前提下，使现成的LMs在节点分类任务上的性能可与最先进的GNNs相媲美。通过保留LM的原始架构，我们的方法保留了LM指令调整的的关键优势，即能够在各种数据集上进行联合训练，提高了灵活性和效率。我们引入了两个关键的增强策略：一是利用拓扑和语义检索方法丰富LM的输入，提供更丰富的上下文信息；二是通过一个轻量级的GNN分类器引导LM的分类过程，有效地减少类候选。在真实数据集上的实验表明，配备这些增强策略的Flan-T5 LMs超越了最先进文本输出节点分类器，并与顶级向量输出节点分类器性能相当。本文缩小了专用节点分类器与一般LMs之间的差距，为更通用和广泛适用的图学习模型铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LMs正在挑战图学习领域中的特定模型（如GNNs和GTs）的主导地位。</li>
<li>提出了一种新型方法，使LMs在节点分类任务上的性能可与最先进的GNNs相媲美，无需进行任何架构修改。</li>
<li>通过保留LM的原始架构，保留了其联合训练的能力，提高了灵活性和效率。</li>
<li>引入两个关键增强策略：利用拓扑和语义检索丰富LM输入，以及使用轻量级GNN分类器引导LM的分类过程。</li>
<li>在真实数据集上的实验表明，增强的LMs性能超越了许多最先进文本输出节点分类器。</li>
<li>增强的LMs性能与顶级向量输出节点分类器相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02296">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-cd85519e2fc1e51061e4026e47a63ab5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b79bc6286ae4432e6f0244306e2368bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-127c00fe4fa6cf5d6213185c4b9e0065.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="LLaVA-3D-A-Simple-yet-Effective-Pathway-to-Empowering-LMMs-with-3D-awareness"><a href="#LLaVA-3D-A-Simple-yet-Effective-Pathway-to-Empowering-LMMs-with-3D-awareness" class="headerlink" title="LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with   3D-awareness"></a>LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with   3D-awareness</h2><p><strong>Authors:Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, Xihui Liu</strong></p>
<p>Recent advancements in Large Multimodal Models (LMMs) have greatly enhanced their proficiency in 2D visual understanding tasks, enabling them to effectively process and understand images and videos. However, the development of LMMs with 3D-awareness for 3D scene understanding has been hindered by the lack of large-scale 3D vision-language datasets and powerful 3D encoders. In this paper, we introduce a simple yet effective framework called LLaVA-3D. Leveraging the strong 2D understanding priors from LLaVA, our LLaVA-3D efficiently adapts LLaVA for 3D scene understanding without compromising 2D understanding capabilities. To achieve this, we utilize the 3D position embeddings to bring the 2D CLIP patches within a 3D spatial context. By integrating the 3D position embeddings into 2D LMMs and employing joint 2D and 3D vision-language instruction tuning, we establish a unified architecture for both 2D image understanding and 3D scene understanding. Experimental results show that LLaVA-3D converges 3.5x faster than existing 3D LMMs when trained on 3D vision-language datasets. Moreover, LLaVA-3D not only achieves state-of-the-art performance across various 3D tasks but also maintains comparable 2D image understanding and vision-language conversation capabilities with LLaVA. </p>
<blockquote>
<p>最近，大型多模态模型（LMMs）在二维视觉理解任务方面的能力得到了极大的提升，能够有效地处理和理解图像和视频。然而，对于三维场景理解的三维感知型LMMs的发展受到了大规模三维视觉语言数据集和强大三维编码器的缺乏的阻碍。在本文中，我们介绍了一个简单有效的框架，称为LLaVA-3D。我们利用LLaVA强大的二维理解先验知识，使LLaVA-3D能够高效地进行三维场景理解，同时不妥协二维理解能力。为了实现这一点，我们利用三维位置嵌入，将二维CLIP补丁置于三维空间上下文中。通过将三维位置嵌入集成到二维LMMs中，并采用联合二维和三维视觉语言指令调整，我们建立了用于二维图像理解和三维场景理解的统一架构。实验结果表明，在三维视觉语言数据集上进行训练时，LLaVA-3D的收敛速度是现有三维LMMs的3.5倍。此外，LLaVA-3D不仅在各种三维任务上达到了最先进的性能，而且与LLaVA保持了相当的二维图像理解和视觉语言对话能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18125v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://zcmax.github.io/projects/LLaVA-3D/">https://zcmax.github.io/projects/LLaVA-3D/</a></p>
<p><strong>Summary</strong></p>
<p>近期大型多模态模型（LMMs）在二维视觉理解任务上的能力显著提升，但在三维场景理解方面的发展受到大型三维视觉语言数据集和强大三维编码器的缺乏的阻碍。本文介绍了一个简单有效的框架LLaVA-3D，它利用LLaVA的强大的二维理解先验知识，在不损害二维理解能力的情况下，有效地适应三维场景理解。通过整合三维位置嵌入，将二维CLIP补丁置于三维空间上下文中，并联合采用二维和三维视觉语言指令调整，建立了一个统一的架构，既可用于二维图像理解，也可用于三维场景理解。实验结果表明，LLaVA-3D在三维视觉语言数据集上进行训练时，收敛速度是现有三维LMMs的3.5倍。LLaVA-3D不仅在各种三维任务上达到最新性能水平，而且与LLaVA相比，还能保持相当的二维图像理解和视觉语言对话能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LMMs在二维视觉理解任务上取得了显著进展。</li>
<li>三维场景理解的发展受到大型三维视觉语言数据集的缺乏和强大三维编码器的限制。</li>
<li>LLaVA-3D框架结合了LLaVA的二维理解先验知识和三维位置嵌入技术。</li>
<li>LLaVA-3D实现了二维和三维视觉理解的统一架构。</li>
<li>LLaVA-3D在训练时收敛速度快，达到现有三维LMMs的3.5倍。</li>
<li>LLaVA-3D在各种三维任务上表现优异，达到最新性能水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.18125">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dd7cc6c33475428ac4017f233d31a098.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f99fa733bdb184d551b733c3979ac5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09ef05626e0748491f31f07bebcc066a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f423fc593572bdaaba9a65a6c077d4c.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="FMDLlama-Financial-Misinformation-Detection-based-on-Large-Language-Models"><a href="#FMDLlama-Financial-Misinformation-Detection-based-on-Large-Language-Models" class="headerlink" title="FMDLlama: Financial Misinformation Detection based on Large Language   Models"></a>FMDLlama: Financial Misinformation Detection based on Large Language   Models</h2><p><strong>Authors:Zhiwei Liu, Xin Zhang, Kailai Yang, Qianqian Xie, Jimin Huang, Sophia Ananiadou</strong></p>
<p>The emergence of social media has made the spread of misinformation easier. In the financial domain, the accuracy of information is crucial for various aspects of financial market, which has made financial misinformation detection (FMD) an urgent problem that needs to be addressed. Large language models (LLMs) have demonstrated outstanding performance in various fields. However, current studies mostly rely on traditional methods and have not explored the application of LLMs in the field of FMD. The main reason is the lack of FMD instruction tuning datasets and evaluation benchmarks. In this paper, we propose FMDLlama, the first open-sourced instruction-following LLMs for FMD task based on fine-tuning Llama3.1 with instruction data, the first multi-task FMD instruction dataset (FMDID) to support LLM instruction tuning, and a comprehensive FMD evaluation benchmark (FMD-B) with classification and explanation generation tasks to test the FMD ability of LLMs. We compare our models with a variety of LLMs on FMD-B, where our model outperforms other open-sourced LLMs as well as OpenAI’s products. This project is available at <a target="_blank" rel="noopener" href="https://github.com/lzw108/FMD">https://github.com/lzw108/FMD</a>. </p>
<blockquote>
<p>社交媒体的兴起使得错误信息的传播更加容易。在金融领域，信息的准确性对金融市场的各个方面都至关重要，这使得金融虚假信息检测（FMD）成为一个亟待解决的问题。大型语言模型（LLM）在各个领域都表现出了卓越的性能。然而，当前的研究主要依赖于传统方法，尚未探索LLM在金融虚假信息检测（FMD）领域的应用。主要原因是缺乏FMD指令调整数据集和评估基准。在本文中，我们提出了FMDLlama，这是基于指令数据微调Llama3.1的金融虚假信息检测的首个开源指令遵循的大型语言模型。我们还推出了首个多任务金融虚假信息检测指令数据集（FMDID），以支持LLM指令调整，以及全面的FMD评估基准（FMD-B），包括分类和解释生成任务，以测试LLM的金融虚假信息检测能力。我们在FMD-B上比较了我们的模型与多种大型语言模型，结果显示我们的模型在性能上超越了其他开源的大型语言模型以及OpenAI的产品。此项目可在<a target="_blank" rel="noopener" href="https://github.com/lzw108/FMD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lzw108/FMD找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16452v2">PDF</a> Accepted by The Web Conference (WWW) 2025 Short Paper Track</p>
<p><strong>Summary</strong><br>金融领域中信息的准确性对于市场的各个方面都至关重要，因此金融虚假信息检测（FMD）是一个亟待解决的问题。大型语言模型（LLM）在各个领域表现出卓越的性能，但现有研究大多依赖于传统方法，尚未探索LLM在FMD领域的应用。缺乏FMD指令微调数据集和评估基准是主要原因。本文提出FMDLlama，基于指令数据微调Llama3.1，推出首款支持LLM指令调整的多任务FMD指令数据集FMDID和全面的FMD评估基准FMD-B，包括分类和解释生成任务，以测试LLM的FMD能力。在FMD-B上与其他模型相比，FMDLlama表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>金融信息的准确性对于金融市场至关重要，金融虚假信息检测（FMD）是亟待解决的问题。</li>
<li>大型语言模型（LLM）在各个领域表现出卓越性能，但尚未在FMD领域得到应用。</li>
<li>当前研究缺乏FMD指令微调数据集和评估基准。</li>
<li>本文提出了FMDLlama，基于指令数据微调Llama3.1，并推出首款多任务FMD指令数据集FMDID。</li>
<li>FMDLlama还推出了全面的FMD评估基准FMD-B，包括分类和解释生成任务。</li>
<li>FMDLlama模型在FMD-B上的性能优于其他开源LLMs以及OpenAI的产品。</li>
<li>该项目已在GitHub上开源，可供公众访问和使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.16452">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-62b411ccb75ed9de9c3dfa18f513b565.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-216638f86e3ace78d9a8650f98674dba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a5df60212804cba427811be5750904c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7aa2ae25ad42c696c982a275b18a0304.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-05/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-05/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-05/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-7f7e8b593b1b5e007bf2ad4307af4d54.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-02-05  Multi-agent Multi-armed Bandit with Fully Heavy-tailed Dynamics
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-01/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1e0af654c11e71fe45c47b56f4848cb0.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion 方向最新论文已更新，请持续关注 Update in 2025-02-01  Free-T2M Frequency Enhanced Text-to-Motion Diffusion Model With   Consistency Loss
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">16905.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
