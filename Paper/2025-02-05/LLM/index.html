<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-05  SELMA A Speech-Enabled Language Model for Virtual Assistant   Interactions">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1857492ad3c81d4f22d20216aa887010.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-05-æ›´æ–°"><a href="#2025-02-05-æ›´æ–°" class="headerlink" title="2025-02-05 æ›´æ–°"></a>2025-02-05 æ›´æ–°</h1><h2 id="SELMA-A-Speech-Enabled-Language-Model-for-Virtual-Assistant-Interactions"><a href="#SELMA-A-Speech-Enabled-Language-Model-for-Virtual-Assistant-Interactions" class="headerlink" title="SELMA: A Speech-Enabled Language Model for Virtual Assistant   Interactions"></a>SELMA: A Speech-Enabled Language Model for Virtual Assistant   Interactions</h2><p><strong>Authors:Dominik Wagner, Alexander Churchill, Siddharth Sigtia, Erik Marchi</strong></p>
<p>In this work, we present and evaluate SELMA, a Speech-Enabled Language Model for virtual Assistant interactions that integrates audio and text as inputs to a Large Language Model (LLM). SELMA is designed to handle three primary and two auxiliary tasks related to interactions with virtual assistants simultaneously within a single end-to-end model. We employ low-rank adaptation modules for parameter-efficient training of both the audio encoder and the LLM. Additionally, we implement a feature pooling strategy enabling the system to recognize global patterns and improve accuracy on tasks less reliant on individual sequence elements. Experimental results on Voice Trigger (VT) detection, Device-Directed Speech Detection (DDSD), and Automatic Speech Recognition (ASR), demonstrate that our approach both simplifies the typical input processing pipeline of virtual assistants significantly and also improves performance compared to dedicated models for each individual task. SELMA yields relative Equal-Error Rate improvements of 64% on the VT detection task, and 22% on DDSD, while also achieving word error rates close to the baseline. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»å¹¶è¯„ä¼°äº†SELMAï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè™šæ‹ŸåŠ©æ‰‹äº¤äº’çš„è¯­éŸ³èµ‹èƒ½è¯­è¨€æ¨¡å‹ï¼Œå®ƒå°†éŸ³é¢‘å’Œæ–‡æœ¬ä½œä¸ºè¾“å…¥é›†æˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ã€‚SELMAè®¾è®¡ç”¨äºåœ¨å•ä¸ªç«¯åˆ°ç«¯æ¨¡å‹ä¸­åŒæ—¶å¤„ç†ä¸è™šæ‹ŸåŠ©æ‰‹äº¤äº’ç›¸å…³çš„ä¸‰ä¸ªä¸»è¦ä»»åŠ¡å’Œä¸¤ä¸ªè¾…åŠ©ä»»åŠ¡ã€‚æˆ‘ä»¬é‡‡ç”¨ä½é˜¶é€‚åº”æ¨¡å—ï¼Œå¯¹éŸ³é¢‘ç¼–ç å™¨å’ŒLLMè¿›è¡Œå‚æ•°æœ‰æ•ˆçš„è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®ç°äº†ç‰¹å¾æ± ç­–ç•¥ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿè¯†åˆ«å…¨å±€æ¨¡å¼ï¼Œå¹¶åœ¨å¯¹å•ä¸ªåºåˆ—å…ƒç´ ä¾èµ–è¾ƒå°‘çš„ä»»åŠ¡ä¸Šæé«˜å‡†ç¡®æ€§ã€‚åœ¨è¯­éŸ³è§¦å‘ï¼ˆVTï¼‰æ£€æµ‹ã€å®šå‘è®¾å¤‡è¯­éŸ³æ£€æµ‹ï¼ˆDDSDï¼‰å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ–¹é¢çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¤§ç®€åŒ–äº†è™šæ‹ŸåŠ©ç†çš„å…¸å‹è¾“å…¥å¤„ç†ç®¡é“ï¼Œå¹¶ä¸”åœ¨ä¸æ¯ä¸ªå•ç‹¬ä»»åŠ¡çš„ä¸“ç”¨æ¨¡å‹ç›¸æ¯”æ—¶æé«˜äº†æ€§èƒ½ã€‚åœ¨VTæ£€æµ‹ä»»åŠ¡ä¸Šï¼ŒSELMAçš„ç›¸å¯¹ç­‰é”™è¯¯ç‡æé«˜äº†64%ï¼Œåœ¨DDSDä¸Šçš„æé«˜äº†2 2%ï¼ŒåŒæ—¶å…¶è¯é”™è¯¯ç‡æ¥è¿‘åŸºçº¿æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19377v2">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬å·¥ä½œä»‹ç»äº†SELMAï¼Œä¸€ç§é¢å‘è™šæ‹ŸåŠ©æ‰‹äº¤äº’çš„è¯­éŸ³é©±åŠ¨è¯­è¨€æ¨¡å‹ï¼Œå®ƒæ•´åˆäº†éŸ³é¢‘å’Œæ–‡æœ¬ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å…¥ã€‚SELMAæ—¨åœ¨åœ¨ä¸€ä¸ªå•ä¸€ç«¯åˆ°ç«¯æ¨¡å‹ä¸­åŒæ—¶å¤„ç†ä¸è™šæ‹ŸåŠ©æ‰‹äº¤äº’çš„ä¸‰ä¸ªä¸»è¦ä»»åŠ¡å’Œä¸¤ä¸ªè¾…åŠ©ä»»åŠ¡ã€‚æˆ‘ä»¬é‡‡ç”¨ä½é˜¶é€‚åº”æ¨¡å—ï¼Œå®ç°éŸ³é¢‘ç¼–ç å™¨ä¸LLMçš„å‚æ•°é«˜æ•ˆè®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®æ–½äº†ä¸€é¡¹ç‰¹æ€§æ± ç­–ç•¥ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿè¯†åˆ«å…¨å±€æ¨¡å¼ï¼Œæé«˜ä¸å¤ªä¾èµ–ä¸ªåˆ«åºåˆ—å…ƒç´ çš„å‡†ç¡®æ€§ã€‚åœ¨è¯­éŸ³è§¦å‘ï¼ˆVTï¼‰æ£€æµ‹ã€å®šå‘è®¾å¤‡è¯­éŸ³æ£€æµ‹ï¼ˆDDSDï¼‰å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ–¹é¢çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¤§ç®€åŒ–äº†è™šæ‹ŸåŠ©æ‰‹çš„å…¸å‹è¾“å…¥å¤„ç†ç®¡é“ï¼Œå¹¶ä¸”åœ¨ä¸æ¯ä¸ªå•ç‹¬ä»»åŠ¡çš„ä¸“ç”¨æ¨¡å‹ç›¸æ¯”æ—¶æé«˜äº†æ€§èƒ½ã€‚åœ¨VTæ£€æµ‹ä»»åŠ¡ä¸Šï¼ŒSELMAå®ç°äº†ç›¸å¯¹ç­‰è¯¯ç‡ï¼ˆEqual-Error Rateï¼‰æé«˜64%ï¼Œåœ¨DDSDä¸Šæé«˜22%ï¼ŒåŒæ—¶è¾¾åˆ°æ¥è¿‘åŸºå‡†çš„å•è¯é”™è¯¯ç‡ã€‚</p>
<p><strong>è¦ç‚¹åˆ†æ</strong></p>
<ol>
<li>SELMAæ˜¯ä¸€ä¸ªé¢å‘è™šæ‹ŸåŠ©æ‰‹äº¤äº’çš„è¯­éŸ³é©±åŠ¨è¯­è¨€æ¨¡å‹ï¼Œé›†æˆéŸ³é¢‘å’Œæ–‡æœ¬ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¾“å…¥ã€‚</li>
<li>SELMAæ—¨åœ¨åœ¨ä¸€ä¸ªå•ä¸€ç«¯åˆ°ç«¯æ¨¡å‹ä¸­åŒæ—¶å¤„ç†è™šæ‹ŸåŠ©æ‰‹äº¤äº’çš„ä¸‰ä¸ªä¸»è¦ä»»åŠ¡å’Œä¸¤ä¸ªè¾…åŠ©ä»»åŠ¡ã€‚</li>
<li>é‡‡ç”¨ä½é˜¶é€‚åº”æ¨¡å—å®ç°å‚æ•°é«˜æ•ˆè®­ç»ƒï¼Œé€‚ç”¨äºéŸ³é¢‘ç¼–ç å™¨å’ŒLLMã€‚</li>
<li>ç‰¹æ€§æ± ç­–ç•¥ä½¿ç³»ç»Ÿèƒ½å¤Ÿè¯†åˆ«å…¨å±€æ¨¡å¼ï¼Œæé«˜ä»»åŠ¡å‡†ç¡®æ€§ï¼Œå°¤å…¶åœ¨ä¸ä¾èµ–ä¸ªåˆ«åºåˆ—å…ƒç´ çš„åœºæ™¯ä¸‹ã€‚</li>
<li>åœ¨VTæ£€æµ‹ã€DDSDå’ŒASRç­‰å®éªŒä»»åŠ¡ä¸Šï¼ŒSELMAè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚</li>
<li>SELMAåœ¨VTæ£€æµ‹ä»»åŠ¡ä¸Šçš„ç­‰è¯¯ç‡ç›¸å¯¹æ”¹è¿›è¾¾åˆ°64%ï¼Œåœ¨DDSDä¸Šè¾¾åˆ°22%ã€‚</li>
<li>SELMAåœ¨å•è¯é”™è¯¯ç‡æ–¹é¢æ¥è¿‘åŸºçº¿æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19377">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-196e96ad81d0532652e3493cf0628bc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0db57738db4b74506d952e632b10ea7d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa31cdf212845cb639ed901b1b96b390.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd9a5f5ac7e58c0b88df01738e094dab.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Improving-the-Robustness-of-Representation-Misdirection-for-Large-Language-Model-Unlearning"><a href="#Improving-the-Robustness-of-Representation-Misdirection-for-Large-Language-Model-Unlearning" class="headerlink" title="Improving the Robustness of Representation Misdirection for Large   Language Model Unlearning"></a>Improving the Robustness of Representation Misdirection for Large   Language Model Unlearning</h2><p><strong>Authors:Dang Huu-Tien, Hoang Thanh-Tung, Le-Minh Nguyen, Naoya Inoue</strong></p>
<p>Representation Misdirection (RM) and variants are established large language model (LLM) unlearning methods with state-of-the-art performance. In this paper, we show that RM methods inherently reduce modelsâ€™ robustness, causing them to misbehave even when a single non-adversarial forget-token is in the retain-query. Toward understanding underlying causes, we reframe the unlearning process as backdoor attacks and defenses: forget-tokens act as backdoor triggers that, when activated in retain-queries, cause disruptions in RM modelsâ€™ behaviors, similar to successful backdoor attacks. To mitigate this vulnerability, we propose Random Noise Augmentation â€“ a model and method agnostic approach with theoretical guarantees for improving the robustness of RM methods. Extensive experiments demonstrate that RNA significantly improves the robustness of RM models while enhancing the unlearning performances. </p>
<blockquote>
<p>è¡¨ç¤ºè¯¯å¯¼ï¼ˆRMï¼‰åŠå…¶å˜ä½“æ˜¯å»ºç«‹åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸Šçš„é—å¿˜æ–¹æ³•ï¼Œå…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜RMæ–¹æ³•æœ¬è´¨ä¸Šä¼šé™ä½æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œç”šè‡³åœ¨ä¿ç•™æŸ¥è¯¢ä¸­å­˜åœ¨å•ä¸ªéå¯¹æŠ—æ€§é—å¿˜ä»¤ç‰Œæ—¶ä¹Ÿä¼šå¯¼è‡´æ¨¡å‹è¡¨ç°å¼‚å¸¸ã€‚ä¸ºäº†äº†è§£æ½œåœ¨åŸå› ï¼Œæˆ‘ä»¬å°†é—å¿˜è¿‡ç¨‹é‡æ–°æ„å»ºä¸ºåé—¨æ”»å‡»å’Œé˜²å¾¡ï¼šé—å¿˜ä»¤ç‰Œå……å½“åé—¨è§¦å‘å™¨ï¼Œåœ¨ä¿ç•™æŸ¥è¯¢ä¸­æ¿€æ´»æ—¶ï¼Œä¼šå¯¹RMæ¨¡å‹çš„è¡Œä¸ºé€ æˆå¹²æ‰°ï¼Œç±»ä¼¼äºæˆåŠŸçš„åé—¨æ”»å‡»ã€‚ä¸ºäº†ç¼“è§£è¿™ç§æ¼æ´ï¼Œæˆ‘ä»¬æå‡ºäº†éšæœºå™ªå£°å¢å¼ºæ³•ï¼ˆRNAï¼‰â€”â€”è¿™æ˜¯ä¸€ç§æ¨¡å‹å’Œé€šç”¨çš„æ–¹æ³•ï¼Œç†è®ºä¸Šèƒ½ä¿è¯æé«˜RMæ–¹æ³•çš„ç¨³å¥æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRNAæ˜¾è‘—æé«˜äº†RMæ¨¡å‹çš„ç¨³å¥æ€§ï¼ŒåŒæ—¶æé«˜äº†é—å¿˜æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19202v2">PDF</a> 12 pages, 4 figures, 1 table</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„è¡¨ç¤ºè¯¯å¯¼ï¼ˆRMï¼‰æ–¹æ³•åŠå…¶å˜ç§çš„é—®é¢˜ã€‚ç ”ç©¶å‘ç°RMæ–¹æ³•ä¼šé™ä½æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œå³ä½¿åœ¨éå¯¹æŠ—æ€§çš„é—å¿˜ä»¤ç‰Œå­˜åœ¨äºä¿ç•™æŸ¥è¯¢ä¸­æ—¶ä¹Ÿä¼šå¯¼è‡´æ¨¡å‹è¡Œä¸ºå¤±å¸¸ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å°†é—å¿˜è¿‡ç¨‹é‡æ–°æ„å»ºä¸ºåé—¨æ”»å‡»å’Œé˜²å¾¡ï¼Œå¹¶æå‡ºäº†éšæœºå™ªå£°å¢å¼ºæ³•ï¼Œä»¥æé«˜RMæ–¹æ³•çš„ç¨³å¥æ€§ï¼Œä¸”é€šè¿‡å®éªŒè¯æ˜äº†å…¶æ˜¾è‘—æ•ˆæœå’Œç†è®ºä¿è¯ã€‚æ­¤æ–¹æ³•ä¸ä»…å¯¹ç‰¹å®šæ¨¡å‹é€‚ç”¨ï¼Œè€Œæ˜¯å¯¹å¤šç§æ¨¡å‹å’Œæ–¹æ³•çš„æ³›åŒ–æ€§èƒ½è¾ƒå¼ºã€‚é€šè¿‡å¼•å…¥éšæœºå™ªå£°å¢å¼ºæ³•ï¼Œå¯æœ‰æ•ˆç¼“è§£RMæ¨¡å‹çš„è„†å¼±æ€§å¹¶æå‡å…¶æ€§èƒ½ã€‚åŒæ—¶æé«˜äº†æ¨¡å‹é—å¿˜å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ€»ç»“èµ·æ¥ï¼Œè¯¥ç ”ç©¶å¯¹äºæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¨³å¥æ€§å’Œæ€§èƒ½å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RMæ–¹æ³•ä¼šé™ä½LLMæ¨¡å‹çš„ç¨³å¥æ€§ï¼Œå¯¼è‡´å…¶é¢å¯¹ç‰¹å®šæƒ…å†µæ—¶å®¹æ˜“å‡ºé”™ã€‚é€šè¿‡å°†è¯¥é—®é¢˜é‡æ„ä¸ºåé—¨æ”»å‡»ä¸é˜²å¾¡çš„é—®é¢˜è¿›è¡Œåˆ†æï¼Œå¯ä»¥ç›´è§‚åœ°å±•ç¤ºå…¶è„†å¼±æ€§æ‰€åœ¨ã€‚å› æ­¤éœ€å¢å¼ºæ¨¡å‹çš„å¥å£®æ€§ä»¥é¿å…å…¶å¯èƒ½å¯¼è‡´çš„è¯¯åˆ¤å’Œæ€§èƒ½ä¸‹é™ç­‰é—®é¢˜ã€‚å…¶ä¸­å‘ç°å½“åœ¨ä¿ç•™æŸ¥è¯¢ä¸­å­˜åœ¨å•ä¸ªéå¯¹æŠ—é—å¿˜ä»¤ç‰Œæ—¶ä¼šå‡ºç°æ­¤ç§æƒ…å†µï¼Œæ˜¾ç¤ºå‡ºè¯¥é—®é¢˜å…·æœ‰ä¸€å®šå¨èƒæ€§ä¸”æ™®éå­˜åœ¨æ€§è¾ƒé«˜ã€‚é€šè¿‡å¯¹ç°æœ‰çš„æ— é’ˆå¯¹æ€§çš„æ”»å‡»å’Œé˜²å¾¡ç­–ç•¥è¿›è¡Œæ”¹è¿›æˆ–åˆ›æ–°æ˜¯æå‡æ¨¡å‹ç¨³å¥æ€§çš„å…³é”®é€”å¾„ä¹‹ä¸€ã€‚åŒæ—¶å‘ç°é—å¿˜ä»¤ç‰Œç±»ä¼¼äºåé—¨æ”»å‡»ä¸­çš„è§¦å‘å™¨ï¼Œæ¿€æ´»åä¼šå¹²æ‰°æ¨¡å‹è¡Œä¸ºã€‚å› æ­¤ï¼Œå¯¹é—å¿˜ä»¤ç‰Œçš„ç®¡ç†å’Œè¯†åˆ«æ˜¯æå‡æ¨¡å‹ç¨³å¥æ€§çš„é‡è¦æ‰‹æ®µä¹‹ä¸€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19202">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1857492ad3c81d4f22d20216aa887010.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Brain-inspired-sparse-training-enables-Transformers-and-LLMs-to-perform-as-fully-connected"><a href="#Brain-inspired-sparse-training-enables-Transformers-and-LLMs-to-perform-as-fully-connected" class="headerlink" title="Brain-inspired sparse training enables Transformers and LLMs to perform   as fully connected"></a>Brain-inspired sparse training enables Transformers and LLMs to perform   as fully connected</h2><p><strong>Authors:Yingtao Zhang, Jialin Zhao, Wenjing Wu, Ziheng Liao, Umberto Michieli, Carlo Vittorio Cannistraci</strong></p>
<p>This study aims to enlarge our current knowledge on application of brain-inspired network science principles for training artificial neural networks (ANNs) with sparse connectivity. Dynamic sparse training (DST) can reduce the computational demands in ANNs, but faces difficulties to keep peak performance at high sparsity levels. The Cannistraci-Hebb training (CHT) is a brain-inspired method for growing connectivity in DST. CHT leverages a gradient-free, topology-driven link regrowth, which has shown ultra-sparse (1% connectivity or lower) advantage across various tasks compared to fully connected networks. Yet, CHT suffers two main drawbacks: (i) its time complexity is O(Nd^3) - N node network size, d node degree - hence it can apply only to ultra-sparse networks. (ii) it selects top link prediction scores, which is inappropriate for the early training epochs, when the network presents unreliable connections. We propose a GPU-friendly approximation of the CH link predictor, which reduces the computational complexity to O(N^3), enabling a fast implementation of CHT in large-scale models. We introduce the Cannistraci-Hebb training soft rule (CHTs), which adopts a strategy for sampling connections in both link removal and regrowth, balancing the exploration and exploitation of network topology. To improve performance, we integrate CHTs with a sigmoid gradual density decay (CHTss). Empirical results show that, using 1% of connections, CHTs outperforms fully connected networks in MLP on visual classification tasks, compressing some networks to &lt; 30% nodes. Using 5% of the connections, CHTss outperforms fully connected networks in two Transformer-based machine translation tasks. Using 30% of the connections, CHTss achieves superior performance compared to other dynamic sparse training methods in language modeling, and it surpasses the fully connected counterpart in zero-shot evaluations. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ—¨åœ¨æ‰©å¤§æˆ‘ä»¬ç›®å‰å…³äºåº”ç”¨è„‘å¯å‘ç½‘ç»œç§‘å­¦åŸç†è®­ç»ƒå…·æœ‰ç¨€ç–è¿æ¥çš„äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰çš„çŸ¥è¯†ã€‚åŠ¨æ€ç¨€ç–è®­ç»ƒï¼ˆDSTï¼‰å¯ä»¥é™ä½äººå·¥ç¥ç»ç½‘ç»œä¸­çš„è®¡ç®—éœ€æ±‚ï¼Œä½†åœ¨é«˜ç¨€ç–çº§åˆ«ä¸Šä¿æŒå³°å€¼æ€§èƒ½é¢ä¸´å›°éš¾ã€‚Cannistraci-Hebbè®­ç»ƒï¼ˆCHTï¼‰æ˜¯ä¸€ç§å—å¤§è„‘å¯å‘çš„åœ¨DSTä¸­å¢åŠ è¿æ¥æ€§çš„æ–¹æ³•ã€‚CHTåˆ©ç”¨æ— æ¢¯åº¦ã€æ‹“æ‰‘é©±åŠ¨çš„è¿æ¥å†ç”Ÿï¼Œä¸å…¨è¿æ¥ç½‘ç»œç›¸æ¯”ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­æ˜¾ç¤ºäº†è¶…ç¨€ç–ï¼ˆ1%çš„è¿æ¥åº¦æˆ–æ›´ä½ï¼‰çš„ä¼˜åŠ¿ã€‚ç„¶è€Œï¼ŒCHTå­˜åœ¨ä¸¤ä¸ªä¸»è¦ç¼ºç‚¹ï¼šï¼ˆiï¼‰å…¶æ—¶é—´å¤æ‚åº¦ä¸ºOï¼ˆNd^3ï¼‰â€”â€”Nä¸ºç½‘ç»œèŠ‚ç‚¹å¤§å°ï¼Œdä¸ºèŠ‚ç‚¹åº¦â€”â€”å› æ­¤åªèƒ½åº”ç”¨äºè¶…ç¨€ç–ç½‘ç»œã€‚ï¼ˆiiï¼‰å®ƒé€‰æ‹©é¡¶çº§é“¾æ¥é¢„æµ‹åˆ†æ•°ï¼Œè¿™åœ¨ç½‘ç»œå‘ˆç°ä¸å¯é è¿æ¥çš„æ—©æœŸè®­ç»ƒå‘¨æœŸä¸­æ˜¯ä¸æ°å½“çš„ã€‚æˆ‘ä»¬æå‡ºäº†CHé“¾æ¥é¢„æµ‹å™¨çš„GPUå‹å¥½è¿‘ä¼¼ï¼Œå°†è®¡ç®—å¤æ‚åº¦é™ä½åˆ°Oï¼ˆN^3ï¼‰ï¼Œä½¿CHTåœ¨å¤§è§„æ¨¡æ¨¡å‹ä¸­çš„å¿«é€Ÿå®ç°æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬å¼•å…¥äº†Cannistraci-Hebbè®­ç»ƒè½¯è§„åˆ™ï¼ˆCHTsï¼‰ï¼Œè¯¥è§„åˆ™é‡‡ç”¨åœ¨è¿æ¥åˆ é™¤å’Œå†ç”Ÿä¸­éƒ½è¿›è¡Œé‡‡æ ·è¿æ¥çš„ç­–ç•¥ï¼Œå¹³è¡¡ç½‘ç»œæ‹“æ‰‘çš„æ¢ç´¢ä¸åˆ©ç”¨ã€‚ä¸ºäº†æé«˜æ€§èƒ½ï¼Œæˆ‘ä»¬å°†CHTsä¸sigmoidé€æ¸å¯†åº¦è¡°å‡ï¼ˆCHTssï¼‰ç›¸ç»“åˆã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨1%çš„è¿æ¥ï¼ŒCHTsåœ¨å¤šå±‚æ„ŸçŸ¥å™¨è§†è§‰åˆ†ç±»ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå…¨è¿æ¥ç½‘ç»œï¼Œå¯ä»¥å°†æŸäº›ç½‘ç»œå‹ç¼©åˆ°å°äº30%çš„èŠ‚ç‚¹ã€‚ä½¿ç”¨5%çš„è¿æ¥ï¼ŒCHTssåœ¨åŸºäºTransformerçš„æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºå…¨è¿æ¥ç½‘ç»œã€‚ä½¿ç”¨30%çš„è¿æ¥ï¼ŒCHTssåœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢çš„æ€§èƒ½ä¼˜äºå…¶ä»–åŠ¨æ€ç¨€ç–è®­ç»ƒæ–¹æ³•ï¼Œå¹¶ä¸”åœ¨é›¶æ ·æœ¬è¯„ä¼°ä¸­è¶…è¶Šäº†å…¨è¿æ¥æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19107v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºè„‘å¯å‘çš„ç½‘ç»œç§‘å­¦åŸåˆ™æ¥è®­ç»ƒå…·æœ‰ç¨€ç–è¿æ¥çš„äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰ï¼Œæœ‰æœ›æé«˜æˆ‘ä»¬çš„è®¤çŸ¥ã€‚æ–‡ç« ç ”ç©¶äº†åŠ¨æ€ç¨€ç–è®­ç»ƒï¼ˆDSTï¼‰åŠå…¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ä¿æŒé«˜å³°æ€§èƒ½çš„é—®é¢˜ã€‚Cannistraci-Hebbè®­ç»ƒï¼ˆCHTï¼‰æ˜¯ä¸€ç§è„‘å¯å‘çš„æ–¹æ³•ï¼Œç”¨äºåœ¨DSTä¸­å¢åŠ è¿æ¥æ€§ã€‚ç„¶è€Œï¼ŒCHTå­˜åœ¨ä¸¤ä¸ªä¸»è¦ç¼ºç‚¹ï¼šè®¡ç®—å¤æ‚åº¦é«˜ä¸”ä»…é€‚ç”¨äºè¶…ç¨€ç–ç½‘ç»œï¼›æ—©æœŸè®­ç»ƒæ—¶å­˜åœ¨ç½‘ç»œè¿æ¥çš„å¯é æ€§é—®é¢˜ã€‚ä¸ºå…‹æœè¿™äº›é—®é¢˜ï¼Œæå‡ºäº†å¯¹CHé“¾æ¥é¢„æµ‹å™¨çš„GPUå‹å¥½è¿‘ä¼¼æ–¹æ¡ˆåŠé‡‡æ ·è¿æ¥ç­–ç•¥çš„æ”¹è‰¯ç‰ˆCHTsæ–¹æ³•ã€‚ç»“åˆsigmoidé€æ¸å¯†åº¦è¡°å‡æŠ€æœ¯åï¼Œæ–°æ–¹æ³•åœ¨è§†è§‰åˆ†ç±»ä»»åŠ¡ä¸Šçš„å¤šå±‚æ„ŸçŸ¥å™¨ç½‘ç»œå‹ç¼©è‡³ä¸è¶³ä¸‰ååˆ†ä¹‹ä¸€çš„æƒ…å†µä¸‹ä¾ç„¶è¶…è¶Šå…¨è¿æ¥ç½‘ç»œæ€§èƒ½ï¼Œå¹¶ä¸”æé«˜äº†ç¿»è¯‘ä»»åŠ¡å’Œè¯­è¨€å»ºæ¨¡çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜å…¶åœ¨ç¨€ç–ç½‘ç»œä¸­å±•ç°ä¼˜å¼‚æ€§èƒ½ï¼Œå€¼å¾—è¿›ä¸€æ­¥ç ”ç©¶æ¨å¹¿ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶çš„é‡ç‚¹æ˜¯é€šè¿‡å¼•å…¥åŸºäºè„‘å¯å‘çš„ç½‘ç»œç§‘å­¦åŸåˆ™è®­ç»ƒå…·æœ‰ç¨€ç–è¿æ¥çš„äººå·¥ç¥ç»ç½‘ç»œã€‚ </li>
<li>åŠ¨æ€ç¨€ç–è®­ç»ƒï¼ˆDSTï¼‰å¯ä»¥å‡å°‘äººå·¥ç¥ç»ç½‘ç»œä¸­çš„è®¡ç®—éœ€æ±‚ï¼Œä½†åœ¨ä¿æŒé«˜å³°æ€§èƒ½æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚ </li>
<li>Cannistraci-Hebbè®­ç»ƒï¼ˆCHTï¼‰é€šè¿‡æ¢¯åº¦æ— å…³çš„æ‹“æ‰‘é©±åŠ¨é“¾æ¥é‡ç”Ÿæˆæ³•ä¿ƒè¿›ç½‘ç»œè¿æ¥å¢é•¿ï¼Œå¹¶åœ¨å„ç§ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºä¼˜åŠ¿ã€‚ </li>
<li>CHTå­˜åœ¨è®¡ç®—å¤æ‚åº¦é«˜å’Œå¯¹æ—©æœŸè®­ç»ƒä¸å¯é è¿æ¥æ•æ„Ÿçš„é—®é¢˜ã€‚ </li>
<li>æå‡ºäº†ä¸€ç§GPUå‹å¥½çš„CHé“¾æ¥é¢„æµ‹å™¨è¿‘ä¼¼æ–¹æ¡ˆï¼Œä»¥é™ä½è®¡ç®—å¤æ‚åº¦å¹¶é€‚ç”¨äºå¤§è§„æ¨¡æ¨¡å‹ã€‚ </li>
<li>å¼•å…¥é‡‡æ ·è¿æ¥ç­–ç•¥çš„æ”¹è‰¯ç‰ˆCHTsæ–¹æ³•ï¼Œç»“åˆsigmoidé€æ¸å¯†åº¦è¡°å‡æŠ€æœ¯æ”¹å–„æ€§èƒ½ã€‚ </li>
<li>å®éªŒç»“æœè¡¨æ˜æ–°æ–¹æ³•åœ¨è§†è§‰åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼ŒåŒæ—¶åœ¨ç¿»è¯‘ä»»åŠ¡å’Œè¯­è¨€å»ºæ¨¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå€¼å¾—è¿›ä¸€æ­¥ç ”ç©¶æ¨å¹¿ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19107">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-192d64c730cf0c1624b7cd5707fbdf70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-760bfe3a31aac62e81bbce04a7bad768.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c669be030823453bf33177083259ced.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-143b1bd40a6bcdb3eb2b278bb96ac5ca.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="OT-Transformer-A-Continuous-time-Transformer-Architecture-with-Optimal-Transport-Regularization"><a href="#OT-Transformer-A-Continuous-time-Transformer-Architecture-with-Optimal-Transport-Regularization" class="headerlink" title="OT-Transformer: A Continuous-time Transformer Architecture with Optimal   Transport Regularization"></a>OT-Transformer: A Continuous-time Transformer Architecture with Optimal   Transport Regularization</h2><p><strong>Authors:Kelvin Kan, Xingjian Li, Stanley Osher</strong></p>
<p>Transformers have achieved state-of-the-art performance in numerous tasks. In this paper, we propose a continuous-time formulation of transformers. Specifically, we consider a dynamical system whose governing equation is parametrized by transformer blocks. We leverage optimal transport theory to regularize the training problem, which enhances stability in training and improves generalization of the resulting model. Moreover, we demonstrate in theory that this regularization is necessary as it promotes uniqueness and regularity of solutions. Our model is flexible in that almost any existing transformer architectures can be adopted to construct the dynamical system with only slight modifications to the existing code. We perform extensive numerical experiments on tasks motivated by natural language processing, image classification, and point cloud classification. Our experimental results show that the proposed method improves the performance of its discrete counterpart and outperforms relevant comparing models. </p>
<blockquote>
<p>Transformeråœ¨è®¸å¤šä»»åŠ¡ä¸­éƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè¿ç»­æ—¶é—´å½¢å¼çš„Transformeræ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªåŠ¨æ€ç³»ç»Ÿï¼Œå…¶æ§åˆ¶æ–¹ç¨‹ç”±Transformerå—å‚æ•°åŒ–ã€‚æˆ‘ä»¬åˆ©ç”¨æœ€ä¼˜ä¼ è¾“ç†è®ºæ¥è§„èŒƒè®­ç»ƒé—®é¢˜ï¼Œè¿™å¢å¼ºäº†è®­ç»ƒçš„ç¨³å®šæ€§å¹¶æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜è¿™ç§è§„èŒƒåŒ–æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºå®ƒå¯ä»¥ä¿ƒè¿›è§£çš„å”¯ä¸€æ€§å’Œè§„å¾‹æ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹å¾ˆçµæ´»ï¼Œå‡ ä¹å¯ä»¥é‡‡ç”¨ä»»ä½•ç°æœ‰çš„Transformeræ¶æ„æ¥æ„å»ºåŠ¨æ€ç³»ç»Ÿï¼Œåªéœ€å¯¹ç°æœ‰çš„ä»£ç è¿›è¡Œå¾®å°çš„ä¿®æ”¹ã€‚æˆ‘ä»¬åœ¨å—è‡ªç„¶è¯­è¨€å¤„ç†ã€å›¾åƒåˆ†ç±»å’Œç‚¹äº‘åˆ†ç±»ä»»åŠ¡å¯å‘çš„ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¤§é‡çš„æ•°å€¼å®éªŒã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ”¹è¿›äº†å…¶ç¦»æ•£å¯¹åº”æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä¼˜äºç›¸å…³çš„å¯¹æ¯”æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18793v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡æå‡ºäº†åŸºäºè¿ç»­æ—¶é—´å½¢å¼çš„Transformeræ¨¡å‹ï¼Œé€šè¿‡åŠ¨æ€ç³»ç»Ÿæ–¹ç¨‹å‚æ•°åŒ–Transformerå—ã€‚åˆ©ç”¨æœ€ä¼˜ä¼ è¾“ç†è®ºå¯¹è®­ç»ƒé—®é¢˜è¿›è¡Œæ­£åˆ™åŒ–ï¼Œæé«˜äº†æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ³›åŒ–æ€§èƒ½ã€‚æ¨¡å‹çµæ´»ï¼Œå‡ ä¹å¯é‡‡ç”¨ä»»ä½•ç°æœ‰Transformeræ¶æ„æ„å»ºåŠ¨æ€ç³»ç»Ÿï¼Œåªéœ€å¯¹ç°æœ‰ä»£ç è¿›è¡Œå¾®å°ä¿®æ”¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æé«˜äº†ç¦»æ•£æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä¼˜äºç›¸å…³å¯¹æ¯”æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†åŸºäºè¿ç»­æ—¶é—´å½¢å¼çš„Transformeræ¨¡å‹ã€‚</li>
<li>é€šè¿‡åŠ¨æ€ç³»ç»Ÿæ–¹ç¨‹å‚æ•°åŒ–Transformerå—ã€‚</li>
<li>åˆ©ç”¨æœ€ä¼˜ä¼ è¾“ç†è®ºå¯¹è®­ç»ƒè¿›è¡Œæ­£åˆ™åŒ–ï¼Œæé«˜æ¨¡å‹ç¨³å®šæ€§å’Œæ³›åŒ–æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹å…·æœ‰çµæ´»æ€§ï¼Œå¯å‡ ä¹é‡‡ç”¨ä»»ä½•ç°æœ‰Transformeræ¶æ„æ„å»ºåŠ¨æ€ç³»ç»Ÿã€‚</li>
<li>æ­£åˆ™åŒ–æœ‰åŠ©äºä¿ƒè¿›è§£çš„å”¯ä¸€æ€§å’Œè§„å¾‹æ€§ã€‚</li>
<li>å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æé«˜äº†ç¦»æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18793">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d9ce4ab9999a428a45c62bf5eb9bd420.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbb07f1405eab7cafd48a0d2ee04eaca.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="The-TIP-of-the-Iceberg-Revealing-a-Hidden-Class-of-Task-In-Prompt-Adversarial-Attacks-on-LLMs"><a href="#The-TIP-of-the-Iceberg-Revealing-a-Hidden-Class-of-Task-In-Prompt-Adversarial-Attacks-on-LLMs" class="headerlink" title="The TIP of the Iceberg: Revealing a Hidden Class of Task-In-Prompt   Adversarial Attacks on LLMs"></a>The TIP of the Iceberg: Revealing a Hidden Class of Task-In-Prompt   Adversarial Attacks on LLMs</h2><p><strong>Authors:Sergey Berezin, Reza Farahbakhsh, Noel Crespi</strong></p>
<p>We present a novel class of jailbreak adversarial attacks on LLMs, termed Task-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks (e.g., cipher decoding, riddles, code execution) into the modelâ€™s prompt to indirectly generate prohibited inputs. To systematically assess the effectiveness of these attacks, we introduce the PHRYGE benchmark. We demonstrate that our techniques successfully circumvent safeguards in six state-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings highlight critical weaknesses in current LLM safety alignments and underscore the urgent need for more sophisticated defence strategies.   Warning: this paper contains examples of unethical inquiries used solely for research purposes. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºä¸€ç§æ–°å‹çš„å¤§è¯­è¨€æ¨¡å‹è¶Šç‹±å¯¹æŠ—æ”»å‡»ï¼Œç§°ä¸ºä»»åŠ¡æç¤ºï¼ˆTIPï¼‰æ”»å‡»ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†åºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼ˆå¦‚å¯†ç è§£ç ã€è°œè¯­ã€ä»£ç æ‰§è¡Œï¼‰åµŒå…¥åˆ°æ¨¡å‹çš„æç¤ºä¸­ï¼Œé—´æ¥ç”Ÿæˆç¦æ­¢çš„è¾“å…¥ã€‚ä¸ºäº†ç³»ç»Ÿåœ°è¯„ä¼°è¿™äº›æ”»å‡»çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†PHRYGEåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æŠ€æœ¯æˆåŠŸåœ°ç»•è¿‡äº†å…­ç§æœ€å…ˆè¿›è¯­è¨€æ¨¡å‹çš„ä¿éšœæªæ–½ï¼ŒåŒ…æ‹¬GPT-4oå’ŒLLaMA 3.2ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨å¯¹é½ä¸­çš„å…³é”®å¼±ç‚¹ï¼Œå¹¶å¼ºè°ƒäº†æ›´å¤æ‚çš„é˜²å¾¡ç­–ç•¥çš„ç´§è¿«éœ€æ±‚ã€‚è­¦å‘Šï¼šæœ¬è®ºæ–‡åŒ…å«çš„è¯¢é—®ç¤ºä¾‹ä»…ç”¨äºç ”ç©¶ç›®çš„ï¼Œå¯èƒ½å­˜åœ¨ä¸é“å¾·çš„å†…å®¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18626v2">PDF</a> </p>
<p><strong>Summary</strong><br>ä»»åŠ¡åœ¨æç¤ºä¸­çš„æ”»å‡»ï¼ˆTIPæ”»å‡»ï¼‰æ˜¯ä¸€ç§æ–°å‹çš„å¤§è¯­è¨€æ¨¡å‹è¶Šç‹±å¯¹æŠ—æ”»å‡»æ–¹å¼ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†åºåˆ—åˆ°åºåˆ—ä»»åŠ¡åµŒå…¥æ¨¡å‹æç¤ºä¸­ï¼Œé—´æ¥ç”Ÿæˆç¦æ­¢è¾“å…¥ï¼Œå¹¶å¯¹å…­ä¸ªæœ€æ–°è¯­è¨€æ¨¡å‹è¿›è¡Œäº†æ¼”ç¤ºï¼ŒåŒ…æ‹¬GPT-4oå’ŒLLaMA 3.2ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨å¯¹é½å­˜åœ¨é‡å¤§ç¼ºé™·ï¼Œæ€¥éœ€æ›´å…ˆè¿›çš„é˜²å¾¡ç­–ç•¥ã€‚è¯·æ³¨æ„ï¼Œæœ¬æ–‡ä»…ä¾›å‚è€ƒçš„ä¾‹å­ä»…ä¸ºç ”ç©¶ç›®çš„ã€‚å¤§è¯­è¨€æ¨¡å‹æ”»é˜²ç ”ç©¶çš„ç›®çš„æ˜¯æ·±å…¥å‰–ææ”»å‡»ç‰¹ç‚¹ä¸å®‰å…¨æ€§èƒ½åˆ†æè¿›è¡Œä¸æ–­æ”¹è¿›å’Œé¿å…å¯¹ç°å®ä¸–ç•Œé€ æˆçš„æ½œåœ¨é£é™©ã€‚æœ¬ç ”ç©¶å±•ç¤ºçš„æ”»å‡»æ¡ˆä¾‹å¹¶ä¸ä»£è¡¨ç ”ç©¶è€…çš„é“å¾·ç«‹åœºå’Œæ¨èè¡Œä¸ºæ–¹å¼ã€‚é€šè¿‡æŒç»­ç ”ç©¶æ”¹è¿›å®‰å…¨æ€§èƒ½ä¿éšœäººç±»ç”Ÿäº§ç”Ÿæ´»å’Œç¤¾ä¼šç§©åºçš„æ­£å¸¸è¿è¡Œã€‚å› æ­¤æœ¬æ‘˜è¦è‡´åŠ›äºä¿æŒå®¢è§‚å…¬æ­£çš„ç ”ç©¶æ€åº¦å¹¶å€¡å¯¼ä½¿ç”¨æŠ€æœ¯çš„æ­£é¢å½±å“é€ ç¦äººç±»ç¤¾ä¼šã€‚æœ¬æ–‡æ€»ç»“äº†å…³äºå¤§è¯­è¨€æ¨¡å‹å®‰å…¨æ€§çš„é‡è¦ç ”ç©¶æˆæœï¼Œæ­ç¤ºäº†æ–°çš„æ”»å‡»æ–¹å¼åŠå…¶æ½œåœ¨å¨èƒï¼Œå¹¶å‘¼åä¸šç•Œå…³æ³¨é˜²å¾¡ç­–ç•¥çš„å‘å±•ã€‚æ—¨åœ¨æ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§å’Œå¯é æ€§ä¸æ–­æé«˜ï¼Œä»è€Œæ›´å¥½åœ°æœåŠ¡äºäººç±»ç¤¾ä¼šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³äºè¯¥æ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ul>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤§è¯­è¨€æ¨¡å‹æ”»å‡»æ–¹å¼â€”â€”ä»»åŠ¡åœ¨æç¤ºä¸­çš„æ”»å‡»ï¼ˆTIPæ”»å‡»ï¼‰ï¼Œè¯¥æ–¹æ³•é€šè¿‡åµŒå…¥ç‰¹å®šä»»åŠ¡é—´æ¥ç”Ÿæˆç¦æ­¢è¾“å…¥ã€‚</li>
<li>ç ”ç©¶å±•ç¤ºäº†TIPæ”»å‡»å¦‚ä½•ç»•è¿‡å…­ä¸ªå…ˆè¿›è¯­è¨€æ¨¡å‹çš„ä¿æŠ¤æªæ–½ï¼ŒåŒ…æ‹¬GPT-4oå’ŒLLaMA 3.2ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†å½“å‰å¤§è¯­è¨€æ¨¡å‹åœ¨å®‰å…¨å¯¹é½æ–¹é¢çš„é‡å¤§ç¼ºé™·ï¼ŒæŒ‡å‡ºéœ€è¦æ›´å…ˆè¿›çš„é˜²å¾¡ç­–ç•¥ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18626">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ca7753f1824329dc66d3d5da14c3784.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a91c465259dd08ba012f8747c79b476.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e279029ef890ffe7c442201f993b1e3f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Transforming-Role-Classification-in-Scientific-Teams-Using-LLMs-and-Advanced-Predictive-Analytics"><a href="#Transforming-Role-Classification-in-Scientific-Teams-Using-LLMs-and-Advanced-Predictive-Analytics" class="headerlink" title="Transforming Role Classification in Scientific Teams Using LLMs and   Advanced Predictive Analytics"></a>Transforming Role Classification in Scientific Teams Using LLMs and   Advanced Predictive Analytics</h2><p><strong>Authors:Wonduk Seo, Yi Bu</strong></p>
<p>Scientific team dynamics are critical in determining the nature and impact of research outputs. However, existing methods for classifying author roles based on self-reports and clustering lack comprehensive contextual analysis of contributions. Thus, we present a transformative approach to classifying author roles in scientific teams using advanced large language models (LLMs), which offers a more refined analysis compared to traditional clustering methods. Specifically, we seek to complement and enhance these traditional methods by utilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting, we categorize author roles and demonstrate that GPT-4 outperforms other models across multiple categories, surpassing traditional approaches such as XGBoost and BERT. Our methodology also includes building a predictive deep learning model using 10 features. By training this model on a dataset derived from the OpenAlex database, which provides detailed metadata on academic publications â€“ such as author-publication history, author affiliation, research topics, and citation counts â€“ we achieve an F1 score of 0.76, demonstrating robust classification of author roles. </p>
<blockquote>
<p>ç§‘ç ”å›¢é˜Ÿçš„åŠ¨æ€åœ¨å†³å®šç ”ç©¶æˆæœçš„æ€§è´¨å’Œå½±å“æ–¹é¢è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºè‡ªæˆ‘æŠ¥å‘Šå’Œèšç±»çš„ä½œè€…è§’è‰²åˆ†ç±»æ–¹æ³•ç¼ºä¹å¯¹è´¡çŒ®çš„å…¨é¢ä¸Šä¸‹æ–‡åˆ†æã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹ç§‘ç ”å›¢é˜Ÿä¸­çš„ä½œè€…è§’è‰²è¿›è¡Œåˆ†ç±»çš„å˜é©æ€§æ–¹æ³•ï¼Œä¸ä¼ ç»Ÿèšç±»æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒæä¾›äº†æ›´ç²¾ç»†çš„åˆ†æã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¸Œæœ›é€šè¿‡åˆ©ç”¨å¼€æºå’Œä¸“æœ‰LLMï¼ˆå¦‚GPT-4ã€Llama3 70Bã€Llama2 70Bå’ŒMistral 7x8Bï¼‰æ¥è¡¥å……å’Œå¢å¼ºè¿™äº›ä¼ ç»Ÿæ–¹æ³•ï¼Œè¿›è¡Œè§’è‰²åˆ†ç±»ã€‚æˆ‘ä»¬é‡‡ç”¨å°‘é‡æç¤ºçš„æ–¹æ³•å¯¹ä½œè€…è§’è‰²è¿›è¡Œåˆ†ç±»ï¼Œå¹¶è¯æ˜GPT-4åœ¨å¤šç±»åˆ«ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„XGBoostå’ŒBERTç­‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜åŒ…æ‹¬å»ºç«‹ä¸€ä¸ªé¢„æµ‹æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä½¿ç”¨10ä¸ªç‰¹å¾ã€‚é€šè¿‡åœ¨OpenAlexæ•°æ®åº“è¡ç”Ÿçš„æ•°æ®é›†ä¸Šè®­ç»ƒè¯¥æ¨¡å‹ï¼Œè¯¥æ•°æ®åº“æä¾›äº†å…³äºå­¦æœ¯å‡ºç‰ˆç‰©çš„è¯¦ç»†å…ƒæ•°æ®ï¼Œå¦‚ä½œè€…å‡ºç‰ˆå†å²ã€ä½œè€…éš¶å±å…³ç³»ã€ç ”ç©¶ä¸»é¢˜å’Œå¼•ç”¨è®¡æ•°ç­‰ï¼Œæˆ‘ä»¬è·å¾—äº†0.76çš„F1åˆ†æ•°ï¼Œè¯æ˜äº†ä½œè€…è§’è‰²åˆ†ç±»çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07267v2">PDF</a> 16 pages, 5 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>ç§‘ç ”å›¢é˜Ÿä¸­çš„åŠ¨æ€å¯¹ç ”ç©¶ç»“æœæœ‰ç€é‡è¦å½±å“ã€‚ç°æœ‰çš„ä½œè€…è§’è‰²åˆ†ç±»æ–¹æ³•ä¸»è¦åŸºäºè‡ªæˆ‘æŠ¥å‘Šå’Œèšç±»ï¼Œç¼ºä¹å¯¹è´¡çŒ®çš„å…¨é¢ä¸Šä¸‹æ–‡åˆ†æã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ç²¾ç»†åˆ†ç±»ä½œè€…è§’è‰²çš„åˆ›æ–°æ–¹æ³•ã€‚é€šè¿‡åˆ©ç”¨å¼€æºå’Œä¸“æœ‰LLMï¼ˆå¦‚GPT-4ã€Llama3 70Bç­‰ï¼‰è¿›è¡Œè§’è‰²åˆ†ç±»ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†ä¼ ç»Ÿçš„èšç±»æ–¹æ³•ã€‚ä½¿ç”¨å°‘æ ·æœ¬æç¤ºæŠ€æœ¯è¿›è¡Œåˆ†ç±»ï¼Œç»“æœæ˜¾ç¤ºGPT-4åœ¨å¤šä¸ªç±»åˆ«ä¸­çš„è¡¨ç°è¶…è¿‡å…¶ä»–æ¨¡å‹å’Œä¼ ç»Ÿæ–¹æ³•ï¼Œå¦‚XGBoostå’ŒBERTã€‚é€šè¿‡è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹æ¥è‡ªOpenAlexæ•°æ®åº“çš„åŒ…å«ä½œè€…å‡ºç‰ˆå†å²ã€ä½œè€…éš¶å±å…³ç³»ç­‰è¯¦ç»†ä¿¡æ¯çš„æ•°æ®é›†è¿›è¡Œé¢„æµ‹ï¼Œæˆ‘ä»¬å®ç°äº†F1åˆ†æ•°ä¸º0.76çš„åˆ†ç±»æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä½œè€…è§’è‰²åˆ†ç±»å¯¹äºè¯„ä¼°ç§‘ç ”å›¢é˜ŸåŠ¨æ€å’Œç ”ç©¶æˆæœè‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»ŸåŸºäºè‡ªæˆ‘æŠ¥å‘Šå’Œèšç±»çš„ä½œè€…è§’è‰²åˆ†ç±»æ–¹æ³•ç¼ºä¹å…¨é¢çš„ä¸Šä¸‹æ–‡åˆ†æã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›äº†ä¸€ç§æ›´ç²¾ç»†çš„åˆ†ç±»æ–¹æ³•ï¼Œèƒ½æ›´å‡†ç¡®åœ°åˆ†æä½œè€…çš„è´¡çŒ®ã€‚</li>
<li>GPT-4åœ¨ä½œè€…è§’è‰²åˆ†ç±»ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡äº†å…¶ä»–æ¨¡å‹å’Œä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>ä½¿ç”¨å°‘æ ·æœ¬æç¤ºæŠ€æœ¯è¿›è¡Œåˆ†ç±»æ˜¯ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥ã€‚</li>
<li>åˆ©ç”¨OpenAlexæ•°æ®åº“çš„è¯¦ç»†ä¿¡æ¯è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå®ç°äº†è¾ƒé«˜çš„åˆ†ç±»æ•ˆæœï¼ˆF1åˆ†æ•°ä¸º0.76ï¼‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07267">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e8a299164449d30c742d0b21507a37ff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a6688da5e0d419e2d4d3d259c337d2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b02efb3fc36694be89eef2cb2d49fd4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ConSim-Measuring-Concept-Based-Explanationsâ€™-Effectiveness-with-Automated-Simulatability"><a href="#ConSim-Measuring-Concept-Based-Explanationsâ€™-Effectiveness-with-Automated-Simulatability" class="headerlink" title="ConSim: Measuring Concept-Based Explanationsâ€™ Effectiveness with   Automated Simulatability"></a>ConSim: Measuring Concept-Based Explanationsâ€™ Effectiveness with   Automated Simulatability</h2><p><strong>Authors:Antonin PochÃ©, Alon Jacovi, Agustin Martin Picard, Victor Boutin, Fanny Jourdan</strong></p>
<p>Concept-based explanations work by mapping complex model computations to human-understandable concepts. Evaluating such explanations is very difficult, as it includes not only the quality of the induced space of possible concepts but also how effectively the chosen concepts are communicated to users. Existing evaluation metrics often focus solely on the former, neglecting the latter. We introduce an evaluation framework for measuring concept explanations via automated simulatability: a simulatorâ€™s ability to predict the explained modelâ€™s outputs based on the provided explanations. This approach accounts for both the concept space and its interpretation in an end-to-end evaluation. Human studies for simulatability are notoriously difficult to enact, particularly at the scale of a wide, comprehensive empirical evaluation (which is the subject of this work). We propose using large language models (LLMs) as simulators to approximate the evaluation and report various analyses to make such approximations reliable. Our method allows for scalable and consistent evaluation across various models and datasets. We report a comprehensive empirical evaluation using this framework and show that LLMs provide consistent rankings of explanation methods. Code available at <a target="_blank" rel="noopener" href="https://github.com/AnonymousConSim/ConSim">https://github.com/AnonymousConSim/ConSim</a>. </p>
<blockquote>
<p>åŸºäºæ¦‚å¿µçš„è§£é‡Šæ˜¯é€šè¿‡å°†å¤æ‚çš„æ¨¡å‹è®¡ç®—æ˜ å°„åˆ°äººç±»å¯ç†è§£çš„æ¦‚å¿µæ¥å·¥ä½œçš„ã€‚è¯„ä¼°è¿™æ ·çš„è§£é‡Šæ˜¯éå¸¸å›°éš¾çš„ï¼Œå› ä¸ºå®ƒä¸ä»…åŒ…æ‹¬å¯èƒ½æ¦‚å¿µç©ºé—´çš„è¯±å¯¼è´¨é‡ï¼Œè¿˜åŒ…æ‹¬æ‰€é€‰æ¦‚å¿µå¦‚ä½•æœ‰æ•ˆåœ°ä¼ è¾¾ç»™ç”¨æˆ·ã€‚ç°æœ‰çš„è¯„ä¼°æŒ‡æ ‡å¾€å¾€åªå…³æ³¨å‰è€…ï¼Œè€Œå¿½è§†äº†åè€…ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æ¨¡æ‹Ÿæ€§æ¥è¡¡é‡æ¦‚å¿µè§£é‡Šï¼šæ¨¡æ‹Ÿå™¨æ ¹æ®æä¾›çš„è§£é‡Šé¢„æµ‹è§£é‡Šæ¨¡å‹çš„è¾“å‡ºçš„èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•æ—¢è€ƒè™‘äº†æ¦‚å¿µç©ºé—´ï¼Œåˆè€ƒè™‘äº†å…¶ç«¯åˆ°ç«¯çš„æœ€ç»ˆè§£é‡Šã€‚æ¨¡æ‹Ÿæ€§çš„äººä½“ç ”ç©¶å®æ–½èµ·æ¥å°¤ä¸ºå›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨å¹¿æ³›è€Œå…¨é¢çš„ç»éªŒè¯„ä¼°çš„å°ºåº¦ä¸Šï¼ˆè¿™æ˜¯æœ¬æ–‡çš„ä¸»é¢˜ï¼‰ã€‚æˆ‘ä»¬å»ºè®®ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ¨¡æ‹Ÿå™¨æ¥è¿›è¡Œè¿‘ä¼¼è¯„ä¼°ï¼Œå¹¶æŠ¥å‘Šå„ç§åˆ†æä»¥ä½¿è¿™äº›è¿‘ä¼¼å€¼å¯é ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸åœ¨å„ç§æ¨¡å‹å’Œæ•°æ®é›†ä¸Šè¿›è¡Œå¯æ‰©å±•å’Œä¸€è‡´æ€§çš„è¯„ä¼°ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†ä½¿ç”¨æ­¤æ¡†æ¶è¿›è¡Œçš„å…¨é¢ç»éªŒè¯„ä¼°ï¼Œå¹¶å±•ç¤ºäº†LLMå¯¹è§£é‡Šæ–¹æ³•çš„ä¸€è‡´æ’åã€‚ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/AnonymousConSim/ConSim">https://github.com/AnonymousConSim/ConSim</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05855v3">PDF</a> </p>
<p><strong>Summary</strong><br>æ¦‚å¿µå‹è§£é‡Šçš„è¯„ä¼°éš¾åº¦è¾ƒé«˜ï¼Œå› ä¸ºå®ƒæ—¢æ¶‰åŠå¯èƒ½æ¦‚å¿µçš„è´¨é‡ï¼Œä¹Ÿæ¶‰åŠå¦‚ä½•æœ‰æ•ˆåœ°å‘ç”¨æˆ·ä¼ è¾¾æ‰€é€‰æ¦‚å¿µã€‚ç°æœ‰çš„è¯„ä¼°æŒ‡æ ‡å¾€å¾€åªå…³æ³¨å‰è€…è€Œå¿½ç•¥äº†åè€…ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿèƒ½åŠ›æ¥è¡¡é‡æ¦‚å¿µè§£é‡Šï¼Œå³æ¨¡æ‹Ÿå™¨æ ¹æ®æä¾›çš„è§£é‡Šé¢„æµ‹è§£é‡Šæ¨¡å‹çš„è¾“å‡ºçš„èƒ½åŠ›ã€‚æ­¤æ–¹æ³•è€ƒè™‘äº†æ¦‚å¿µç©ºé—´åŠå…¶è§£é‡Šä¸¤ä¸ªæ–¹é¢ï¼Œè¿›è¡Œå…¨é¢è¯„ä¼°ã€‚é‰´äºå¯¹äººç±»ç ”ç©¶çš„æ¨¡æ‹Ÿæ€§è¯„ä¼°å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨å¹¿æ³›çš„å®è¯è¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ¨¡æ‹Ÿå™¨è¿›è¡Œè¿‘ä¼¼è¯„ä¼°ï¼Œå¹¶æŠ¥å‘Šäº†å„ç§åˆ†æä»¥ç¡®ä¿è¿™ç§è¿‘ä¼¼å¯é ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸å¯¹ä¸åŒçš„æ¨¡å‹å’Œæ•°æ®é›†è¿›è¡Œå¯æ‰©å±•å’Œä¸€è‡´çš„è¯„ä¼°ã€‚æˆ‘ä»¬ä½¿ç”¨æ­¤æ¡†æ¶è¿›è¡Œäº†å…¨é¢çš„å®è¯è¯„ä¼°ï¼Œå¹¶å±•ç¤ºäº†LLMå¯¹è§£é‡Šæ–¹æ³•çš„ä¸€è‡´æ’åã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AnonymousConSim/ConSim">åŒ¿åé“¾æ¥</a>è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¦‚å¿µå‹è§£é‡Šé€šè¿‡å°†å¤æ‚çš„æ¨¡å‹è®¡ç®—æ˜ å°„åˆ°äººç±»å¯ç†è§£çš„æ¦‚å¿µæ¥å·¥ä½œã€‚</li>
<li>æ¦‚å¿µå‹è§£é‡Šçš„è¯„ä¼°æ¶‰åŠä¸¤ä¸ªä¸»è¦æ–¹é¢ï¼šå¯èƒ½æ¦‚å¿µçš„è´¨é‡å’Œå¦‚ä½•æœ‰æ•ˆåœ°å‘ç”¨æˆ·ä¼ è¾¾è¿™äº›æ¦‚å¿µã€‚</li>
<li>ç°æœ‰è¯„ä¼°æŒ‡æ ‡ä¸»è¦å…³æ³¨æ¦‚å¿µçš„è´¨é‡ï¼Œè€Œå¿½è§†äº†ç”¨æˆ·æ²Ÿé€šçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªåŸºäºæ¨¡æ‹Ÿèƒ½åŠ›çš„è¯„ä¼°æ¡†æ¶æ¥è¡¡é‡æ¦‚å¿µè§£é‡Šï¼ŒåŒæ—¶è€ƒè™‘äº†æ¦‚å¿µç©ºé—´å’Œå…¶è§£é‡Šã€‚</li>
<li>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ¨¡æ‹Ÿå™¨æ¥è¿‘ä¼¼è¯„ä¼°äººç±»ç ”ç©¶çš„éš¾åº¦å’ŒæŒ‘æˆ˜æ€§è¿›è¡Œäº†è®¨è®ºã€‚</li>
<li>æå‡ºäº†ä¸€ç§ä½¿ç”¨LLMè¿›è¡Œå¯é è¿‘ä¼¼è¯„ä¼°çš„æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„å¯æ‰©å±•æ€§å’Œä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05855">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2717bc60520b87375e48d4b843d51155.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-569c4b60057d7b21e9193d6b30bec592.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4a4cc152c4baba105b0b5080b7a2650.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c4b1a27b46fa592775fcf45364fed70.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Multi-modal-Agent-Tuning-Building-a-VLM-Driven-Agent-for-Efficient-Tool-Usage"><a href="#Multi-modal-Agent-Tuning-Building-a-VLM-Driven-Agent-for-Efficient-Tool-Usage" class="headerlink" title="Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool   Usage"></a>Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool   Usage</h2><p><strong>Authors:Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaojian Ma, Tao Yuan, Yue Fan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, Qing Li</strong></p>
<p>The advancement of large language models (LLMs) prompts the development of multi-modal agents, which are used as a controller to call external tools, providing a feasible way to solve practical tasks. In this paper, we propose a multi-modal agent tuning method that automatically generates multi-modal tool-usage data and tunes a vision-language model (VLM) as the controller for powerful tool-usage reasoning. To preserve the data quality, we prompt the GPT-4o mini model to generate queries, files, and trajectories, followed by query-file and trajectory verifiers. Based on the data synthesis pipeline, we collect the MM-Traj dataset that contains 20K tasks with trajectories of tool usage. Then, we develop the T3-Agent via \underline{T}rajectory \underline{T}uning on VLMs for \underline{T}ool usage using MM-Traj. Evaluations on the GTA and GAIA benchmarks show that the T3-Agent consistently achieves improvements on two popular VLMs: MiniCPM-V-8.5B and {Qwen2-VL-7B}, which outperforms untrained VLMs by $20%$, showing the effectiveness of the proposed data synthesis pipeline, leading to high-quality data for tool-usage capabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä¿ƒè¿›äº†å¤šæ¨¡æ€ä»£ç†çš„å‘å±•ï¼Œè¿™äº›ä»£ç†ç”¨ä½œæ§åˆ¶å™¨æ¥è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼Œä¸ºè§£å†³å®é™…ä»»åŠ¡æä¾›äº†å¯è¡Œçš„æ–¹æ³•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€ä»£ç†è°ƒæ•´æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯è‡ªåŠ¨ç”Ÿæˆå¤šæ¨¡æ€å·¥å…·ä½¿ç”¨æ•°æ®ï¼Œå¹¶è°ƒæ•´è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä½œä¸ºæ§åˆ¶å™¨ï¼Œä»¥è¿›è¡Œå¼ºå¤§çš„å·¥å…·ä½¿ç”¨æ¨ç†ã€‚ä¸ºäº†ä¿æŒæ•°æ®è´¨é‡ï¼Œæˆ‘ä»¬æŒ‡ç¤ºGPT-4oå°å‹æ¨¡å‹ç”ŸæˆæŸ¥è¯¢ã€æ–‡ä»¶å’Œè½¨è¿¹ï¼Œéšåé€šè¿‡æŸ¥è¯¢æ–‡ä»¶éªŒè¯å™¨å’Œè½¨è¿¹éªŒè¯å™¨è¿›è¡ŒéªŒè¯ã€‚åŸºäºæ•°æ®åˆæˆç®¡é“ï¼Œæˆ‘ä»¬æ”¶é›†äº†MM-Trajæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«åŒ…å«å·¥å…·ä½¿ç”¨è½¨è¿¹çš„2ä¸‡ä¸ªä»»åŠ¡ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡MM-Trajä¸ŠåŸºäºè½¨è¿¹è°ƒæ•´çš„T3ä»£ç†å¼€å‘æ–¹æ³•å¼€å‘å‡ºäº†T3-Agentã€‚åœ¨GTAå’ŒGAIAåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒT3-Agentåœ¨ä¸¤ç§æµè¡Œçš„VLMä¸Šå–å¾—äº†æŒç»­çš„æ”¹è¿›ï¼šMiniCPM-V-8.5Bå’ŒQwen2-VL-7Bï¼Œå…¶æ€§èƒ½æ¯”æœªè®­ç»ƒçš„VLMé«˜å‡º20%ï¼Œè¯æ˜äº†æ‰€æå‡ºçš„æ•°æ®åˆæˆç®¡é“çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå·¥å…·ä½¿ç”¨èƒ½åŠ›æä¾›äº†é«˜è´¨é‡çš„æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15606v2">PDF</a> ICLR 2025, <a target="_blank" rel="noopener" href="https://mat-agent.github.io/">https://mat-agent.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ä¿ƒè¿›äº†å¤šæ¨¡æ€ä»£ç†çš„å¼€å‘ï¼Œè¯¥ä»£ç†å¯ä½œä¸ºæ§åˆ¶å™¨è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼Œä¸ºè§£å†³å®é™…ä»»åŠ¡æä¾›äº†å¯è¡Œé€”å¾„ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€ä»£ç†è°ƒæ•´æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯è‡ªåŠ¨ç”Ÿæˆå¤šæ¨¡æ€å·¥å…·ä½¿ç”¨æ•°æ®ï¼Œå¹¶è°ƒæ•´è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä½œä¸ºæ§åˆ¶å™¨è¿›è¡Œå·¥å…·ä½¿ç”¨æ¨ç†ã€‚ä¸ºä¿æŒæ•°æ®è´¨é‡ï¼Œä½¿ç”¨GPT-4oå°å‹æ¨¡å‹ç”ŸæˆæŸ¥è¯¢ã€æ–‡ä»¶å’Œè½¨è¿¹ï¼Œéšåé€šè¿‡æŸ¥è¯¢æ–‡ä»¶åŠè½¨è¿¹éªŒè¯å™¨è¿›è¡ŒéªŒè¯ã€‚åŸºäºæ•°æ®åˆæˆç®¡é“ï¼Œæˆ‘ä»¬æ”¶é›†äº†MM-Trajæ•°æ®é›†ï¼ŒåŒ…å«2ä¸‡ä¸ªä»»åŠ¡åŠå·¥å…·ä½¿ç”¨è½¨è¿¹ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡MM-Trajåœ¨VLMä¸Šè¿›è¡Œè½¨è¿¹è°ƒæ•´ï¼Œå¼€å‘å‡ºT3-Agentã€‚åœ¨GTAå’ŒGAIAåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒT3-Agentåœ¨ä¸¤ç§æµè¡Œçš„VLMä¸Šå®ç°äº†æŒç»­çš„æ”¹è¿›ï¼Œå³MiniCPM-V-8.5Bå’ŒQwen2-VL-7Bï¼Œå…¶æ€§èƒ½æ¯”æœªè®­ç»ƒçš„VLMæé«˜äº†20%ï¼Œæ˜¾ç¤ºäº†æ•°æ®åˆæˆç®¡é“çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå·¥å…·ä½¿ç”¨èƒ½åŠ›ç”Ÿæˆäº†é«˜è´¨é‡æ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨åŠ¨å¤šæ¨¡æ€ä»£ç†å‘å±•ï¼Œå¤šæ¨¡æ€ä»£ç†èƒ½å¤Ÿä½œä¸ºæ§åˆ¶å™¨è°ƒç”¨å¤–éƒ¨å·¥å…·ä»¥è§£å†³å®ç”¨ä»»åŠ¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€ä»£ç†è°ƒæ•´æ–¹æ³•ï¼Œè‡ªåŠ¨ç”Ÿæˆå¤šæ¨¡æ€å·¥å…·ä½¿ç”¨æ•°æ®å¹¶è°ƒæ•´è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚</li>
<li>é‡‡ç”¨GPT-4oå°å‹æ¨¡å‹ç”ŸæˆæŸ¥è¯¢ã€æ–‡ä»¶å’Œè½¨è¿¹ï¼Œä»¥ç¡®ä¿æ•°æ®è´¨é‡ã€‚</li>
<li>å¼•å…¥äº†æŸ¥è¯¢æ–‡ä»¶åŠè½¨è¿¹éªŒè¯æœºåˆ¶ä»¥ç¡®ä¿æ•°æ®çš„å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>åŸºäºæ•°æ®åˆæˆç®¡é“ï¼Œæ”¶é›†äº†åŒ…å«2ä¸‡ä¸ªä»»åŠ¡åŠå·¥å…·ä½¿ç”¨è½¨è¿¹çš„MM-Trajæ•°æ®é›†ã€‚</li>
<li>å¼€å‘å‡ºäº†T3-Agentï¼Œé€šè¿‡MM-Trajåœ¨VLMä¸Šè¿›è¡Œè½¨è¿¹è°ƒæ•´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15606">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-963b18445f42c73720c6ca81034c8c5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-167386209c58998672452da9d496a130.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-037e35ae237c29563089361fce834dbd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CATSplat-Context-Aware-Transformer-with-Spatial-Guidance-for-Generalizable-3D-Gaussian-Splatting-from-A-Single-View-Image"><a href="#CATSplat-Context-Aware-Transformer-with-Spatial-Guidance-for-Generalizable-3D-Gaussian-Splatting-from-A-Single-View-Image" class="headerlink" title="CATSplat: Context-Aware Transformer with Spatial Guidance for   Generalizable 3D Gaussian Splatting from A Single-View Image"></a>CATSplat: Context-Aware Transformer with Spatial Guidance for   Generalizable 3D Gaussian Splatting from A Single-View Image</h2><p><strong>Authors:Wonseok Roh, Hwanhee Jung, Jong Wook Kim, Seunggwan Lee, Innfarn Yoo, Andreas Lugmayr, Seunggeun Chi, Karthik Ramani, Sangpil Kim</strong></p>
<p>Recently, generalizable feed-forward methods based on 3D Gaussian Splatting have gained significant attention for their potential to reconstruct 3D scenes using finite resources. These approaches create a 3D radiance field, parameterized by per-pixel 3D Gaussian primitives, from just a few images in a single forward pass. However, unlike multi-view methods that benefit from cross-view correspondences, 3D scene reconstruction with a single-view image remains an underexplored area. In this work, we introduce CATSplat, a novel generalizable transformer-based framework designed to break through the inherent constraints in monocular settings. First, we propose leveraging textual guidance from a visual-language model to complement insufficient information from a single image. By incorporating scene-specific contextual details from text embeddings through cross-attention, we pave the way for context-aware 3D scene reconstruction beyond relying solely on visual cues. Moreover, we advocate utilizing spatial guidance from 3D point features toward comprehensive geometric understanding under single-view settings. With 3D priors, image features can capture rich structural insights for predicting 3D Gaussians without multi-view techniques. Extensive experiments on large-scale datasets demonstrate the state-of-the-art performance of CATSplat in single-view 3D scene reconstruction with high-quality novel view synthesis. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºä¸‰ç»´é«˜æ–¯Splattingçš„å¯æ³›åŒ–å‰é¦ˆæ–¹æ³•å› å…¶åˆ©ç”¨æœ‰é™èµ„æºé‡å»ºä¸‰ç»´åœºæ™¯çš„æ½œåŠ›è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚è¿™äº›æ–¹æ³•åœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­ä»…ä»å°‘é‡å›¾åƒä¸­åˆ›å»ºç”±åƒç´ çº§ä¸‰ç»´é«˜æ–¯åŸºå…ƒå‚æ•°åŒ–çš„ä¸‰ç»´è¾å°„åœºã€‚ç„¶è€Œï¼Œä¸åŒäºå—ç›Šäºè·¨è§†å›¾å¯¹åº”å…³ç³»çš„å¤šè§†å›¾æ–¹æ³•ï¼Œä½¿ç”¨å•è§†å›¾å›¾åƒè¿›è¡Œä¸‰ç»´åœºæ™¯é‡å»ºä»ç„¶æ˜¯ä¸€ä¸ªæœªè¢«å……åˆ†ç ”ç©¶çš„é¢†åŸŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†CATSplatï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¯æ³›åŒ–åŸºäºtransformerçš„æ¡†æ¶ï¼Œæ—¨åœ¨çªç ´å•ç›®è®¾ç½®ä¸­çš„å›ºæœ‰çº¦æŸã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬æŒ‡å¯¼æ¥è¡¥å……å•å¹…å›¾åƒä¸­çš„ä¿¡æ¯ä¸è¶³ã€‚é€šè¿‡ç»“åˆæ–‡æœ¬åµŒå…¥çš„åœºæ™¯ç‰¹å®šä¸Šä¸‹æ–‡ç»†èŠ‚è¿›è¡Œäº¤å‰æ³¨æ„åŠ›ï¼Œæˆ‘ä»¬ä¸ºåŸºäºæ–‡æœ¬æŒ‡å¯¼çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸‰ç»´åœºæ™¯é‡å»ºé“ºå¹³äº†é“è·¯ï¼Œä¸å†ä»…ä»…ä¾èµ–äºè§†è§‰çº¿ç´¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸»å¼ åˆ©ç”¨ä¸‰ç»´ç‚¹ç‰¹å¾çš„ç©ºé—´æŒ‡å¯¼æ¥å®ç°å•è§†å›¾è®¾ç½®ä¸‹çš„å…¨é¢å‡ ä½•ç†è§£ã€‚å€ŸåŠ©ä¸‰ç»´å…ˆéªŒçŸ¥è¯†ï¼Œå›¾åƒç‰¹å¾å¯ä»¥æ•æ‰ä¸°å¯Œçš„ç»“æ„ä¿¡æ¯ï¼Œä»¥é¢„æµ‹ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒï¼Œæ— éœ€ä½¿ç”¨å¤šè§†å›¾æŠ€æœ¯ã€‚åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCATSplatåœ¨å•è§†å›¾ä¸‰ç»´åœºæ™¯é‡å»ºä¸­å…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶èƒ½è¿›è¡Œé«˜è´¨é‡çš„æ–°è§†è§’åˆæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12906v2">PDF</a> </p>
<p><strong>Summary</strong>:<br>åŸºäºæ–‡æœ¬æŒ‡å¯¼ä¸ç©ºé—´å¼•å¯¼çš„å•è§†è§’ä¸‰ç»´åœºæ™¯é‡å»ºç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„å¯æ³›åŒ–çš„å˜å‹å™¨æ¡†æ¶CATSplatã€‚å®ƒç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬æŒ‡å¯¼ä¸ä¸‰ç»´ç‚¹ç‰¹å¾çš„ç©ºé—´æŒ‡å¯¼ï¼Œé€šè¿‡å•å¼ å›¾åƒå®ç°ä¸°å¯Œçš„å‡ ä½•ç†è§£ï¼Œçªç ´å•è§†è§’é‡å»ºçš„å†…åœ¨é™åˆ¶ã€‚è¯¥æ¡†æ¶åœ¨å•è§†è§’ä¸‰ç»´åœºæ™¯é‡å»ºæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå…·æœ‰é«˜è´¨é‡çš„æ–°è§†è§’åˆæˆèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>CATSplatæ˜¯åŸºäºå•å¼ å›¾åƒå®ç°ä¸‰ç»´åœºæ™¯é‡å»ºçš„æ–°å‹æ¡†æ¶ã€‚</li>
<li>åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬æŒ‡å¯¼è¡¥å……å•ä¸€å›¾åƒä¿¡æ¯ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡è·¨æ³¨æ„åŠ›æœºåˆ¶å¼•å…¥åœºæ™¯ç‰¹å®šçš„ä¸Šä¸‹æ–‡ç»†èŠ‚ï¼Œå®ç°äº†åŸºäºæ–‡æœ¬çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸‰ç»´åœºæ™¯é‡å»ºã€‚</li>
<li>å¼•å…¥äº†ç©ºé—´æŒ‡å¯¼æœºåˆ¶ï¼Œåˆ©ç”¨ä¸‰ç»´ç‚¹ç‰¹å¾è¿›è¡Œå‡ ä½•ç†è§£ã€‚</li>
<li>åˆ©ç”¨ä¸‰ç»´å…ˆéªŒçŸ¥è¯†ï¼Œå›¾åƒç‰¹å¾å¯ä»¥æ•æ‰ä¸°å¯Œçš„ç»“æ„ä¿¡æ¯ï¼Œé¢„æµ‹ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒï¼Œæ— éœ€å¤šè§†è§’æŠ€æœ¯ã€‚</li>
<li>åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†CATSplatåœ¨å•è§†è§’ä¸‰ç»´åœºæ™¯é‡å»ºé¢†åŸŸçš„å…ˆè¿›æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12906">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-26625a8cd9fd0477e93c92a31f275e0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fcee7c7d3e153b917e17f05ceba3e90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68e3c41ed5ea4d5050fc93e670dcd597.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6b21a98b2a22c9cf98787dfb154b118.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56b1f79adf6fa9c78178ba24b46572b1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="The-Open-Source-Advantage-in-Large-Language-Models-LLMs"><a href="#The-Open-Source-Advantage-in-Large-Language-Models-LLMs" class="headerlink" title="The Open Source Advantage in Large Language Models (LLMs)"></a>The Open Source Advantage in Large Language Models (LLMs)</h2><p><strong>Authors:Jiya Manchanda, Laura Boettcher, Matheus Westphalen, Jasser Jasser</strong></p>
<p>Large language models (LLMs) have rapidly advanced natural language processing, driving significant breakthroughs in tasks such as text generation, machine translation, and domain-specific reasoning. The field now faces a critical dilemma in its approach: closed-source models like GPT-4 deliver state-of-the-art performance but restrict reproducibility, accessibility, and external oversight, while open-source frameworks like LLaMA and Mixtral democratize access, foster collaboration, and support diverse applications, achieving competitive results through techniques like instruction tuning and LoRA. Hybrid approaches address challenges like bias mitigation and resource accessibility by combining the scalability of closed-source systems with the transparency and inclusivity of open-source framework. However, in this position paper, we argue that open-source remains the most robust path for advancing LLM research and ethical deployment. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œæ¨åŠ¨äº†æ–‡æœ¬ç”Ÿæˆã€æœºå™¨ç¿»è¯‘å’Œé¢†åŸŸç‰¹å®šæ¨ç†ç­‰ä»»åŠ¡çš„é‡å¤§çªç ´ã€‚ç„¶è€Œï¼Œè¯¥é¢†åŸŸçš„æ–¹æ³•é¢ä¸´ä¸€ä¸ªå…³é”®çš„å›°å¢ƒï¼šåƒGPT-4è¿™æ ·çš„å°é—­æºæ¨¡å‹è™½ç„¶æä¾›äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†é™åˆ¶äº†å¯é‡å¤æ€§ã€å¯è®¿é—®æ€§å’Œå¤–éƒ¨ç›‘ç£ï¼›è€ŒåƒLLaMAå’ŒMixtralè¿™æ ·çš„å¼€æºæ¡†æ¶åˆ™å®ç°äº†æ°‘ä¸»åŒ–çš„è®¿é—®ï¼Œä¿ƒè¿›äº†åä½œå¹¶æ”¯æŒå¤šæ ·åŒ–çš„åº”ç”¨ï¼Œé€šè¿‡æŒ‡ä»¤å¾®è°ƒLoRAç­‰æŠ€æœ¯å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚æ··åˆæ–¹æ³•é€šè¿‡ç»“åˆå°é—­ç³»ç»Ÿçš„å¯æ‰©å±•æ€§å’Œå¼€æºæ¡†æ¶çš„é€æ˜åº¦å’ŒåŒ…å®¹æ€§æ¥è§£å†³åè§ç¼“è§£å’Œèµ„æºå¯åŠæ€§ç­‰æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œåœ¨è¿™ç¯‡ç«‹åœºè®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºå¼€æºä»ç„¶æ˜¯æ¨è¿›LLMç ”ç©¶å’Œé“å¾·éƒ¨ç½²çš„æœ€ç¨³å¥é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12004v2">PDF</a> 9 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œå·²åœ¨æ–‡æœ¬ç”Ÿæˆã€æœºå™¨ç¿»è¯‘å’Œé¢†åŸŸç‰¹å®šæ¨ç†ç­‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çªç ´ã€‚å½“å‰ï¼Œè¯¥é¢†åŸŸé¢ä¸´ç€ä¸€ä¸ªå…³é”®å›°å¢ƒï¼šå°é—­æºæ¨¡å‹å¦‚GPT-4è™½ç„¶æ€§èƒ½å“è¶Šï¼Œä½†é™åˆ¶äº†å¯é‡å¤æ€§ã€å¯è®¿é—®æ€§å’Œå¤–éƒ¨ç›‘ç£ï¼›è€Œå¼€æºæ¡†æ¶å¦‚LLaMAå’ŒMixtralå®ç°äº†æ°‘ä¸»åŒ–çš„è®¿é—®å’Œåä½œï¼Œå¹¶æ”¯æŒå„ç§åº”ç”¨ï¼Œé€šè¿‡æŒ‡ä»¤å¾®è°ƒç­‰æŠ€æœ¯å–å¾—å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚è¿™ç¯‡ç«‹åœºè®ºæ–‡è®¤ä¸ºï¼Œç»“åˆå°é—­æºç³»ç»Ÿçš„å¯æ‰©å±•æ€§å’Œå¼€æºæ¡†æ¶çš„é€æ˜æ€§ä¸åŒ…å®¹æ€§ï¼Œé€šè¿‡æ··åˆæ–¹æ³•è§£å†³åè§ç¼“è§£å’Œèµ„æºå¯åŠæ€§ç­‰æŒ‘æˆ˜ï¼Œä½†å¼€æºä»æ˜¯æ¨è¿›LLMç ”ç©¶å’Œé“å¾·éƒ¨ç½²çš„æœ€ç¨³å¥é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså·²åœ¨å¤šä¸ªNLPä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>å°é—­æºæ¨¡å‹ä¸å¼€æºæ¨¡å‹å„æœ‰ä¼˜åŠ¿å’Œå±€é™ã€‚</li>
<li>å°é—­æºæ¨¡å‹å¦‚GPT-4æ€§èƒ½å“è¶Šï¼Œä½†é™åˆ¶å¯é‡å¤æ€§å’Œå¤–éƒ¨ç›‘ç£ã€‚</li>
<li>å¼€æºæ¡†æ¶å¦‚LLaMAå’ŒMixtralæ”¯æŒå¤šæ ·åŒ–åº”ç”¨ï¼Œå¹¶å®ç°æ°‘ä¸»åŒ–çš„è®¿é—®å’Œåä½œã€‚</li>
<li>æ··åˆæ–¹æ³•ç»“åˆå°é—­æºå’Œå¼€æºçš„ä¼˜åŠ¿ï¼Œè§£å†³æŒ‘æˆ˜å¦‚åè§ç¼“è§£å’Œèµ„æºå¯åŠæ€§ã€‚</li>
<li>ç«‹åœºè®ºæ–‡å¼ºè°ƒå¼€æºæ˜¯æ¨è¿›LLMç ”ç©¶å’Œé“å¾·éƒ¨ç½²çš„æœ€ç¨³å¥é€”å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12004">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-64a8efe4e74dff2a5d3ba44afebbc30a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-360bef31b822bdaf7d94c785ecaf45d7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Iris-Breaking-GUI-Complexity-with-Adaptive-Focus-and-Self-Refining"><a href="#Iris-Breaking-GUI-Complexity-with-Adaptive-Focus-and-Self-Refining" class="headerlink" title="Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining"></a>Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining</h2><p><strong>Authors:Zhiqi Ge, Juncheng Li, Xinglei Pang, Minghe Gao, Kaihang Pan, Wang Lin, Hao Fei, Wenqiao Zhang, Siliang Tang, Yueting Zhuang</strong></p>
<p>Digital agents are increasingly employed to automate tasks in interactive digital environments such as web pages, software applications, and operating systems. While text-based agents built on Large Language Models (LLMs) often require frequent updates due to platform-specific APIs, visual agents leveraging Multimodal Large Language Models (MLLMs) offer enhanced adaptability by interacting directly with Graphical User Interfaces (GUIs). However, these agents face significant challenges in visual perception, particularly when handling high-resolution, visually complex digital environments. This paper introduces Iris, a foundational visual agent that addresses these challenges through two key innovations: Information-Sensitive Cropping (ISC) and Self-Refining Dual Learning (SRDL). ISC dynamically identifies and prioritizes visually dense regions using a edge detection algorithm, enabling efficient processing by allocating more computational resources to areas with higher information density. SRDL enhances the agentâ€™s ability to handle complex tasks by leveraging a dual-learning loop, where improvements in referring (describing UI elements) reinforce grounding (locating elements) and vice versa, all without requiring additional annotated data. Empirical evaluations demonstrate that Iris achieves state-of-the-art performance across multiple benchmarks with only 850K GUI annotations, outperforming methods using 10x more training data. These improvements further translate to significant gains in both web and OS agent downstream tasks. </p>
<blockquote>
<p>æ•°å­—ä»£ç†è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨äºè‡ªåŠ¨åŒ–ç½‘é¡µã€è½¯ä»¶åº”ç”¨ç¨‹åºå’Œæ“ä½œç³»ç»Ÿç­‰äº¤äº’å¼æ•°å­—ç¯å¢ƒä¸­çš„ä»»åŠ¡ã€‚åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬ä»£ç†é€šå¸¸éœ€è¦é¢‘ç¹æ›´æ–°ä»¥é€‚åº”å¹³å°ç‰¹å®šçš„APIï¼Œè€Œåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„è§†è§‰ä»£ç†é€šè¿‡ç›´æ¥ä¸å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰äº¤äº’ï¼Œæä¾›äº†æ›´é«˜çš„é€‚åº”æ€§ã€‚ç„¶è€Œï¼Œè¿™äº›ä»£ç†åœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡ã€è§†è§‰å¤æ‚çš„æ•°å­—ç¯å¢ƒæ—¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10342v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ•°å­—ä»£ç†åœ¨ç½‘é¡µã€è½¯ä»¶åº”ç”¨å’Œæ“ä½œç³»ç»Ÿç­‰äº¤äº’å¼æ•°å­—ç¯å¢ƒä¸­è¢«å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨åŒ–ä»»åŠ¡ã€‚åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ä»£ç†é€šå¸¸éœ€è¦é¢‘ç¹æ›´æ–°ä»¥é€‚åº”å¹³å°ç‰¹å®šçš„APIï¼Œè€Œåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è§†è§‰ä»£ç†é€šè¿‡ç›´æ¥ä¸ç”¨æˆ·ç•Œé¢äº¤äº’æä¾›äº†æ›´é«˜çš„é€‚åº”æ€§ã€‚ç„¶è€Œï¼Œè¿™äº›ä»£ç†åœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å’Œè§†è§‰å¤æ‚çš„æ•°å­—ç¯å¢ƒæ—¶ã€‚æœ¬æ–‡ä»‹ç»äº†Irisè¿™ä¸€åŸºç¡€è§†è§‰ä»£ç†ï¼Œå®ƒé€šè¿‡ä¸¤é¡¹å…³é”®åˆ›æ–°æŠ€æœ¯è§£å†³äº†è¿™äº›æŒ‘æˆ˜ï¼šä¿¡æ¯æ•æ„Ÿè£å‰ªï¼ˆISCï¼‰å’Œè‡ªæˆ‘å®Œå–„åŒé‡å­¦ä¹ ï¼ˆSRDLï¼‰ã€‚ISCä½¿ç”¨è¾¹ç¼˜æ£€æµ‹ç®—æ³•åŠ¨æ€è¯†åˆ«å’Œä¼˜å…ˆå¤„ç†è§†è§‰å¯†é›†åŒºåŸŸï¼ŒSRDLåˆ™é€šè¿‡åŒé‡å­¦ä¹ å¾ªç¯æé«˜ä»£ç†å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚å®è¯è¯„ä¼°æ˜¾ç¤ºï¼ŒIrisåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä»…ä½¿ç”¨85ä¸‡GUIæ³¨é‡Šå°±ä¼˜äºä½¿ç”¨10å€è®­ç»ƒæ•°æ®çš„æ–¹æ³•ã€‚è¿™äº›æ”¹è¿›è¿›ä¸€æ­¥è½¬åŒ–ä¸ºç½‘é¡µå’Œæ“ä½œç³»ç»Ÿä»£ç†ä¸‹æ¸¸ä»»åŠ¡çš„æ˜¾è‘—æ”¶ç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°å­—ä»£ç†åœ¨äº¤äº’å¼æ•°å­—ç¯å¢ƒä¸­å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨åŒ–ä»»åŠ¡ã€‚</li>
<li>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ä»£ç†éœ€è¦é¢‘ç¹æ›´æ–°ä»¥é€‚åº”å¹³å°ç‰¹å®šçš„APIã€‚</li>
<li>è§†è§‰ä»£ç†åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç›´æ¥ä¸ç”¨æˆ·ç•Œé¢äº¤äº’ä»¥æé«˜é€‚åº”æ€§ã€‚</li>
<li>è§†è§‰ä»£ç†åœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢é¢ä¸´å¤„ç†é«˜åˆ†è¾¨ç‡å’Œè§†è§‰å¤æ‚æ•°å­—ç¯å¢ƒçš„æŒ‘æˆ˜ã€‚</li>
<li>Irisé€šè¿‡ä¿¡æ¯æ•æ„Ÿè£å‰ªï¼ˆISCï¼‰å’Œè‡ªæˆ‘å®Œå–„åŒé‡å­¦ä¹ ï¼ˆSRDLï¼‰è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>ISCä½¿ç”¨è¾¹ç¼˜æ£€æµ‹ç®—æ³•åŠ¨æ€è¯†åˆ«å’Œå¤„ç†è§†è§‰å¯†é›†åŒºåŸŸã€‚</li>
<li>SRDLé€šè¿‡åŒé‡å­¦ä¹ å¾ªç¯æé«˜ä»£ç†å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚Irisåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜ç§€ï¼Œä»…ä½¿ç”¨å°‘é‡æ³¨é‡Šæ•°æ®å°±å®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10342">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ceb7d61ef26d6501596f192f83f98f16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95e90b6ee0a3fc5c51ff5571700e50ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73833221afdf83a38c7c79589af7dd31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d45644094312b06caf231347068713d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-112d1182da6faa4d5b92d32d1d3827cc.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Frontiers-in-Intelligent-Colonoscopy"><a href="#Frontiers-in-Intelligent-Colonoscopy" class="headerlink" title="Frontiers in Intelligent Colonoscopy"></a>Frontiers in Intelligent Colonoscopy</h2><p><strong>Authors:Ge-Peng Ji, Jingyi Liu, Peng Xu, Nick Barnes, Fahad Shahbaz Khan, Salman Khan, Deng-Ping Fan</strong></p>
<p>Colonoscopy is currently one of the most sensitive screening methods for colorectal cancer. This study investigates the frontiers of intelligent colonoscopy techniques and their prospective implications for multimodal medical applications. With this goal, we begin by assessing the current data-centric and model-centric landscapes through four tasks for colonoscopic scene perception, including classification, detection, segmentation, and vision-language understanding. This assessment enables us to identify domain-specific challenges and reveals that multimodal research in colonoscopy remains open for further exploration. To embrace the coming multimodal era, we establish three foundational initiatives: a large-scale multimodal instruction tuning dataset ColonINST, a colonoscopy-designed multimodal language model ColonGPT, and a multimodal benchmark. To facilitate ongoing monitoring of this rapidly evolving field, we provide a public website for the latest updates: <a target="_blank" rel="noopener" href="https://github.com/ai4colonoscopy/IntelliScope">https://github.com/ai4colonoscopy/IntelliScope</a>. </p>
<blockquote>
<p>ç»“è‚ é•œæ£€æŸ¥ç›®å‰æ˜¯ç»“ç›´è‚ ç™Œæœ€æ•æ„Ÿçš„æ£€æµ‹æ–¹æ³•ä¹‹ä¸€ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨æ™ºèƒ½ç»“è‚ é•œæ£€æŸ¥æŠ€æœ¯çš„æœ€æ–°è¿›å±•åŠå…¶å¯¹å¤šæ¨¡å¼åŒ»å­¦åº”ç”¨çš„æ½œåœ¨å½±å“ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡å››é¡¹ç»“è‚ é•œåœºæ™¯æ„ŸçŸ¥ä»»åŠ¡æ¥è¯„ä¼°å½“å‰ä»¥æ•°æ®ä¸ºä¸­å¿ƒå’Œä»¥æ¨¡å‹ä¸ºä¸­å¿ƒçš„æƒ…å†µï¼ŒåŒ…æ‹¬åˆ†ç±»ã€æ£€æµ‹ã€åˆ†å‰²å’Œè§†è§‰è¯­è¨€ç†è§£ã€‚è¿™ä¸€è¯„ä¼°ä½¿æˆ‘ä»¬èƒ½å¤Ÿç¡®å®šç‰¹å®šé¢†åŸŸçš„æŒ‘æˆ˜ï¼Œå¹¶è¡¨æ˜ç»“è‚ é•œæ£€æŸ¥ä¸­çš„å¤šæ¨¡å¼ç ”ç©¶ä»æœ‰å¾…è¿›ä¸€æ­¥æ¢ç´¢ã€‚ä¸ºäº†è¿æ¥å³å°†åˆ°æ¥çš„å¤šæ¨¡å¼æ—¶ä»£ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸‰ä¸ªåŸºæœ¬é¡¹ç›®ï¼šå¤§è§„æ¨¡å¤šæ¨¡å¼æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ColonINSTã€é’ˆå¯¹ç»“è‚ é•œæ£€æŸ¥è®¾è®¡çš„å¤šæ¨¡å¼è¯­è¨€æ¨¡å‹ColonGPTä»¥åŠå¤šæ¨¡å¼åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†ä¿ƒè¿›è¿™ä¸€å¿«é€Ÿå‘å±•é¢†åŸŸçš„æŒç»­ç›‘æµ‹ï¼Œæˆ‘ä»¬æä¾›äº†æœ€æ–°æ›´æ–°çš„å…¬å…±ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://github.com/ai4colonoscopy/IntelliScope%E3%80%82">https://github.com/ai4colonoscopy/IntelliScopeã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17241v2">PDF</a> [Work in progress] A comprehensive survey of intelligent colonoscopy   in the multimodal era. [Updated Version V2] New training strategy for   colonoscopy-specific multimodal language model</p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ™ºèƒ½ç»“è‚ é•œæ£€æŸ¥æŠ€æœ¯çš„æœ€æ–°è¿›å±•åŠå…¶åœ¨å¤šæ¨¡æ€åŒ»ç–—åº”ç”¨ä¸­çš„æ½œåœ¨å½±å“ã€‚æ–‡ç« é€šè¿‡è¯„ä¼°ç»“è‚ é•œæ£€æŸ¥çš„å½“å‰æ•°æ®ä¸ºä¸­å¿ƒå’Œæ¨¡å‹ä¸ºä¸­å¿ƒçš„ç°çŠ¶ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€æ£€æµ‹ã€åˆ†å‰²å’Œè§†è§‰è¯­è¨€ç†è§£ç­‰ä»»åŠ¡ï¼Œç¡®å®šäº†ç‰¹å®šé¢†åŸŸçš„æŒ‘æˆ˜ï¼Œå¹¶æŒ‡å‡ºç»“è‚ é•œæ£€ä¸­çš„å¤šæ¨¡æ€ç ”ç©¶ä»æœ‰å¾…è¿›ä¸€æ­¥æ¢ç´¢ã€‚ä¸ºè¿æ¥å³å°†åˆ°æ¥çš„å¤šæ¨¡æ€æ—¶ä»£ï¼Œæœ¬æ–‡å»ºç«‹äº†ä¸‰ä¸ªåŸºç¡€é¡¹ç›®ï¼šå¤§è§„æ¨¡å¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ColonINSTã€ç»“è‚ é•œæ£€æŸ¥è®¾è®¡çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ColonGPTå’Œå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ™ºèƒ½ç»“è‚ é•œæ£€æŸ¥æŠ€æœ¯æ˜¯ç»“è‚ ç™Œç­›æŸ¥ä¸­æœ€æ•æ„Ÿçš„æ–¹æ³•ä¹‹ä¸€ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡å››é¡¹ä»»åŠ¡è¯„ä¼°äº†ç»“è‚ é•œæ£€æŸ¥çš„å½“å‰çŠ¶å†µï¼ŒåŒ…æ‹¬åˆ†ç±»ã€æ£€æµ‹ã€åˆ†å‰²å’Œè§†è§‰è¯­è¨€ç†è§£ã€‚</li>
<li>æ–‡ç« æŒ‡å‡ºäº†å½“å‰é¢†åŸŸå­˜åœ¨çš„ç‰¹å®šæŒ‘æˆ˜ï¼Œå¦‚æ•°æ®æ”¶é›†å’Œæ¨¡å‹æ€§èƒ½ä¼˜åŒ–ç­‰ã€‚</li>
<li>å¤šæ¨¡æ€ç ”ç©¶åœ¨ç»“è‚ é•œæ£€æŸ¥ä¸­ä»æœ‰å¾…è¿›ä¸€æ­¥æ¢ç´¢å’Œå‘å±•ã€‚</li>
<li>ä¸ºæ¨åŠ¨è¿™ä¸€é¢†åŸŸçš„å‘å±•ï¼Œå»ºç«‹äº†ä¸‰ä¸ªåŸºç¡€é¡¹ç›®ï¼šColonINSTæ•°æ®é›†ã€ColonGPTæ¨¡å‹å’Œå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ã€‚</li>
<li>è¿™äº›é¡¹ç›®æ—¨åœ¨æé«˜æ™ºèƒ½ç»“è‚ é•œæ£€æŸ¥æŠ€æœ¯çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œä»¥åŠæ¨åŠ¨å…¶åœ¨å¤šæ¨¡æ€åŒ»ç–—åº”ç”¨ä¸­çš„å¹¿æ³›åº”ç”¨ã€‚</li>
<li>æœ€åï¼Œæ–‡ç« æä¾›äº†ä¸€ä¸ªå…¬å…±ç½‘ç«™ä»¥æä¾›æœ€æ–°æ›´æ–°çš„ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.17241">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5326b71b9dc5824ce9ad4f28c6d12e8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b53b24981eaa4ba63c757e6b8c26e90e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bfa2469b1553cd7f4c56a1663ac7c9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94389db38ff181a7c35cc9770aa2cc86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e8bf3b00154cab5b4bcd52e8b3d483d7.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Efficient-Annotator-Reliability-Assessment-and-Sample-Weighting-for-Knowledge-Based-Misinformation-Detection-on-Social-Media"><a href="#Efficient-Annotator-Reliability-Assessment-and-Sample-Weighting-for-Knowledge-Based-Misinformation-Detection-on-Social-Media" class="headerlink" title="Efficient Annotator Reliability Assessment and Sample Weighting for   Knowledge-Based Misinformation Detection on Social Media"></a>Efficient Annotator Reliability Assessment and Sample Weighting for   Knowledge-Based Misinformation Detection on Social Media</h2><p><strong>Authors:Owen Cook, Charlie Grimshaw, Ben Wu, Sophie Dillon, Jack Hicks, Luke Jones, Thomas Smith, Matyas Szert, Xingyi Song</strong></p>
<p>Misinformation spreads rapidly on social media, confusing the truth and targeting potentially vulnerable people. To effectively mitigate the negative impact of misinformation, it must first be accurately detected before applying a mitigation strategy, such as Xâ€™s community notes, which is currently a manual process. This study takes a knowledge-based approach to misinformation detection, modelling the problem similarly to one of natural language inference. The EffiARA annotation framework is introduced, aiming to utilise inter- and intra-annotator agreement to understand the reliability of each annotator and influence the training of large language models for classification based on annotator reliability. In assessing the EffiARA annotation framework, the Russo-Ukrainian Conflict Knowledge-Based Misinformation Classification Dataset (RUC-MCD) was developed and made publicly available. This study finds that sample weighting using annotator reliability performs the best, utilising both inter- and intra-annotator agreement and soft-label training. The highest classification performance achieved using Llama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large. </p>
<blockquote>
<p>ç¤¾äº¤åª’ä½“ä¸Šçš„é”™è¯¯ä¿¡æ¯è¿…é€Ÿä¼ æ’­ï¼Œæ··æ·†çœŸç›¸ï¼Œé’ˆå¯¹æ½œåœ¨å¼±åŠ¿ç¾¤ä½“ã€‚ä¸ºäº†æœ‰æ•ˆå‡è½»é”™è¯¯ä¿¡æ¯å¸¦æ¥çš„è´Ÿé¢å½±å“ï¼Œå¿…é¡»åœ¨åº”ç”¨ç¼“è§£ç­–ç•¥ä¹‹å‰å‡†ç¡®æ£€æµ‹é”™è¯¯ä¿¡æ¯ï¼Œä¾‹å¦‚Xçš„ç¤¾åŒºç¬”è®°ï¼Œä½†ç›®å‰è¿™æ˜¯ä¸€ä¸ªæ‰‹åŠ¨è¿‡ç¨‹ã€‚æœ¬ç ”ç©¶é‡‡ç”¨åŸºäºçŸ¥è¯†çš„æ–¹æ³•æ£€æµ‹é”™è¯¯ä¿¡æ¯ï¼Œå°†é—®é¢˜æ¨¡æ‹Ÿä¸ºè‡ªç„¶è¯­è¨€æ¨ç†é—®é¢˜ä¹‹ä¸€ã€‚ä»‹ç»äº†EffiARAæ³¨é‡Šæ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨æ ‡æ³¨è€…ä¹‹é—´çš„å†…éƒ¨å’Œå¤–éƒ¨å…±è¯†æ¥ç†è§£æ¯ä¸ªæ ‡æ³¨è€…çš„å¯é æ€§ï¼Œå¹¶åŸºäºæ ‡æ³¨è€…å¯é æ€§è®­ç»ƒå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è¿›è¡Œåˆ†ç±»ã€‚åœ¨è¯„ä¼°EffiARAæ³¨é‡Šæ¡†æ¶æ—¶ï¼Œå¼€å‘äº†åŸºäºçŸ¥è¯†çš„å…³äºä¿„ä¹Œå†²çªçš„ä¿¡æ¯åˆ†ç±»æ•°æ®é›†ï¼ˆRUC-MCDï¼‰ï¼Œå¹¶å·²å…¬å¼€å‘å¸ƒã€‚æœ¬ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨æ ‡æ³¨è€…å¯é æ€§è¿›è¡Œæ ·æœ¬åŠ æƒçš„è¡¨ç°æœ€å¥½ï¼Œå®ƒç»“åˆäº†æ ‡æ³¨è€…ä¹‹é—´çš„å†…éƒ¨å’Œå¤–éƒ¨å…±è¯†å’Œè½¯æ ‡ç­¾è®­ç»ƒã€‚ä½¿ç”¨Llama-3.2-1Bå®ç°çš„æœ€é«˜åˆ†ç±»æ€§èƒ½æ˜¯å®F1å€¼ä¸º0.757ï¼Œä½¿ç”¨TwHIN-BERT-largeçš„å®F1å€¼ä¸º0.740ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14515v2">PDF</a> 8 pages, 3 figures, 3 tables. Code available here:   <a target="_blank" rel="noopener" href="https://github.com/MiniEggz/ruc-misinfo">https://github.com/MiniEggz/ruc-misinfo</a>; annotation framework available here:   <a target="_blank" rel="noopener" href="https://github.com/MiniEggz/EffiARA">https://github.com/MiniEggz/EffiARA</a></p>
<p><strong>æ‘˜è¦</strong><br>ç¤¾äº¤åª’ä½“ä¸Šé”™è¯¯ä¿¡æ¯ä¼ æ’­è¿…é€Ÿï¼Œæ··æ·†çœŸç›¸ï¼Œé’ˆå¯¹æ½œåœ¨æ˜“å—å®³ç¾¤ä½“ã€‚ä¸ºæœ‰æ•ˆç¼“è§£é”™è¯¯ä¿¡æ¯å¸¦æ¥çš„è´Ÿé¢å½±å“ï¼Œå¿…é¡»åœ¨é‡‡å–ç¼“è§£ç­–ç•¥ä¹‹å‰å‡†ç¡®æ£€æµ‹é”™è¯¯ä¿¡æ¯ï¼Œå¦‚Xçš„ç¤¾åŒºæ³¨é‡Šï¼Œç›®å‰è¿™ä¸€è¿‡ç¨‹æ˜¯æ‰‹åŠ¨è¿›è¡Œçš„ã€‚æœ¬ç ”ç©¶é‡‡ç”¨åŸºäºçŸ¥è¯†çš„æ–¹æ³•æ£€æµ‹é”™è¯¯ä¿¡æ¯ï¼Œå°†é—®é¢˜æ¨¡æ‹Ÿä¸ºè‡ªç„¶è¯­è¨€æ¨ç†é—®é¢˜ã€‚å¼•å…¥EffiARAæ³¨é‡Šæ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨æ³¨é‡Šå‘˜ä¹‹é—´çš„å†…éƒ¨å’Œå¤–éƒ¨å…±è¯†æ¥ç†è§£æ¯ä¸ªæ³¨é‡Šå‘˜çš„å¯é æ€§ï¼Œå¹¶å½±å“åŸºäºæ³¨é‡Šå‘˜å¯é æ€§åˆ†ç±»çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒã€‚åœ¨è¯„ä¼°EffiARAæ³¨é‡Šæ¡†æ¶æ—¶ï¼Œå¼€å‘äº†åŸºäºä¿„ä¹Œå†²çªçŸ¥è¯†é”™è¯¯çš„åˆ†ç±»æ•°æ®é›†ï¼ˆRUC-MCDï¼‰ï¼Œå¹¶å·²å…¬å¼€ä¾›å…¬ä¼—ä½¿ç”¨ã€‚æœ¬ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨æ³¨é‡Šå‘˜å¯é æ€§è¿›è¡Œæ ·æœ¬åŠ æƒçš„è¡¨ç°æœ€å¥½ï¼Œå¹¶åŒæ—¶ä½¿ç”¨æ³¨é‡Šå‘˜ä¹‹é—´çš„å†…éƒ¨å’Œå¤–éƒ¨å…±è¯†å’Œè½¯æ ‡ç­¾è®­ç»ƒã€‚ä½¿ç”¨Llama-3.2-1Bæ—¶æœ€é«˜åˆ†ç±»æ€§èƒ½è¾¾åˆ°çš„å®è§‚Få€¼ä¸º0.757ï¼Œä½¿ç”¨TwHIN-BERT-largeæ—¶ä¸º0.740ã€‚</p>
<p><strong>è¦ç‚¹æç‚¼</strong></p>
<ol>
<li>ç¤¾äº¤åª’ä½“ä¸Šçš„é”™è¯¯ä¿¡æ¯èƒ½è¿…é€Ÿä¼ æ’­ï¼Œæ··æ·†çœŸç›¸ï¼Œå¹¶å¯¹æ½œåœ¨æ˜“å—å®³ç¾¤ä½“äº§ç”Ÿå½±å“ã€‚</li>
<li>ä¸ºç¼“è§£é”™è¯¯ä¿¡æ¯çš„å½±å“ï¼Œéœ€å…ˆè¿›è¡Œå‡†ç¡®æ£€æµ‹ï¼Œå†é‡‡å–ç›¸åº”ç¼“è§£ç­–ç•¥ã€‚</li>
<li>æœ¬ç ”ç©¶é‡‡ç”¨åŸºäºçŸ¥è¯†çš„æ–¹æ³•æ£€æµ‹é”™è¯¯ä¿¡æ¯ï¼Œæ¨¡æ‹Ÿä¸ºè‡ªç„¶è¯­è¨€æ¨ç†é—®é¢˜ã€‚</li>
<li>å¼•å…¥EffiARAæ³¨é‡Šæ¡†æ¶ï¼Œåˆ©ç”¨æ³¨é‡Šå‘˜ä¹‹é—´çš„å†…å¤–å…±è¯†ç†è§£æ³¨é‡Šå‘˜å¯é æ€§ï¼Œå¹¶å½±å“è¯­è¨€æ¨¡å‹çš„è®­ç»ƒã€‚</li>
<li>å¼€å‘å¹¶å…¬å¼€äº†RUC-MCDæ•°æ®é›†ç”¨äºè¯„ä¼°ã€‚</li>
<li>ç ”ç©¶å‘ç°æ ·æœ¬åŠ æƒç»“åˆå†…å¤–å…±è¯†å’Œè½¯æ ‡ç­¾è®­ç»ƒçš„æ–¹å¼åœ¨é”™è¯¯ä¿¡æ¯æ£€æµ‹ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>ä½¿ç”¨Llama-3.2-1Bå’ŒTwHIN-BERT-largeæ¨¡å‹æ—¶ï¼Œæœ€é«˜åˆ†ç±»æ€§èƒ½çš„å®è§‚Få€¼åˆ†åˆ«ä¸º0.757å’Œ0.740ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.14515">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c18f91a4bc70964df99d6b371ad8bd8f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3577bb927cecc6d2ece3df74e7531c6d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e211047149cd91e7e9f3b2002317966.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56683aa62c076a99ad005d656eabde86.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CollabEdit-Towards-Non-destructive-Collaborative-Knowledge-Editing"><a href="#CollabEdit-Towards-Non-destructive-Collaborative-Knowledge-Editing" class="headerlink" title="CollabEdit: Towards Non-destructive Collaborative Knowledge Editing"></a>CollabEdit: Towards Non-destructive Collaborative Knowledge Editing</h2><p><strong>Authors:Jiamu Zheng, Jinghuai Zhang, Tianyu Du, Xuhong Zhang, Jianwei Yin, Tao Lin</strong></p>
<p>Collaborative learning of large language models (LLMs) has emerged as a new paradigm for utilizing private data from different parties to guarantee efficiency and privacy. Meanwhile, Knowledge Editing (KE) for LLMs has also garnered increased attention due to its ability to manipulate the behaviors of LLMs explicitly, yet leaves the collaborative KE case (in which knowledge edits of multiple parties are aggregated in a privacy-preserving and continual manner) unexamined. To this end, this manuscript dives into the first investigation of collaborative KE, in which we start by carefully identifying the unique three challenges therein, including knowledge overlap, knowledge conflict, and knowledge forgetting. We then propose a non-destructive collaborative KE framework, COLLABEDIT, which employs a novel model merging mechanism to mimic the global KE behavior while preventing the severe performance drop. Extensive experiments on two canonical datasets demonstrate the superiority of COLLABEDIT compared to other destructive baselines, and results shed light on addressing three collaborative KE challenges and future applications. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/LINs-lab/CollabEdit">https://github.com/LINs-lab/CollabEdit</a>. </p>
<blockquote>
<p>ååŒå­¦ä¹ å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»æˆä¸ºåˆ©ç”¨ä¸åŒæ–¹çš„ç§æœ‰æ•°æ®ä»¥ä¿è¯æ•ˆç‡å’Œéšç§çš„æ–°èŒƒå¼ã€‚ä¸æ­¤åŒæ—¶ï¼Œç”±äºèƒ½å¤Ÿæ˜ç¡®åœ°æ“ä½œLLMçš„è¡Œä¸ºï¼ŒLLMçš„çŸ¥è¯†ç¼–è¾‘ï¼ˆKEï¼‰ä¹Ÿå¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œä½†ååŒKEçš„æƒ…å†µï¼ˆå³å¤šæ–¹çŸ¥è¯†ç¼–è¾‘ä»¥éšç§ä¿æŠ¤å’ŒæŒç»­çš„æ–¹å¼è¿›è¡Œèšåˆï¼‰å°šæœªå¾—åˆ°ç ”ç©¶ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æ·±å…¥ç ”ç©¶äº†ååŒKEï¼Œé¦–å…ˆä»”ç»†ç¡®å®šäº†å…¶ä¸­çš„ä¸‰ä¸ªç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬çŸ¥è¯†é‡å ã€çŸ¥è¯†å†²çªå’ŒçŸ¥è¯†é—å¿˜ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†éç ´åæ€§çš„ååŒKEæ¡†æ¶COLLABEDITï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†ä¸€ç§æ–°å‹æ¨¡å‹åˆå¹¶æœºåˆ¶æ¥æ¨¡æ‹Ÿå…¨å±€KEè¡Œä¸ºï¼ŒåŒæ—¶é˜²æ­¢æ€§èƒ½ä¸¥é‡ä¸‹é™ã€‚åœ¨ä¸¤ä¸ªå…¸å‹æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†COLLABEDITç›¸è¾ƒäºå…¶ä»–ç ´åæ€§åŸºå‡†çº¿çš„ä¼˜è¶Šæ€§ï¼Œå®éªŒç»“æœä¹Ÿä¸ºæˆ‘ä»¬è§£å†³äº†ä¸‰ä¸ªååŒKEæŒ‘æˆ˜ä»¥åŠæœªæ¥çš„åº”ç”¨æä¾›äº†å¯ç¤ºã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/LINs-lab/CollabEdit%E3%80%82">https://github.com/LINs-lab/CollabEditã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.09508v2">PDF</a> 20 pages, 11 figures. Published as a conference paper at ICLR 2025.   Code at <a target="_blank" rel="noopener" href="https://github.com/LINs-lab/CollabEdit">https://github.com/LINs-lab/CollabEdit</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ååŒçŸ¥è¯†ç¼–è¾‘ï¼ˆKnowledge Editing, KEï¼‰ã€‚æ–‡ç« é¦–å…ˆç¡®å®šäº†ååŒKEçš„ä¸‰ä¸ªç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬çŸ¥è¯†é‡å ã€çŸ¥è¯†å†²çªå’ŒçŸ¥è¯†é—å¿˜ã€‚ç„¶åæå‡ºäº†ä¸€ç§éç ´åæ€§çš„ååŒKEæ¡†æ¶COLLABEDITï¼Œé‡‡ç”¨æ–°é¢–çš„æ¨¡å‹åˆå¹¶æœºåˆ¶æ¥æ¨¡æ‹Ÿå…¨å±€KEè¡Œä¸ºï¼Œé˜²æ­¢æ€§èƒ½ä¸¥é‡ä¸‹é™ã€‚å®éªŒè¯æ˜ï¼ŒCOLLABEDITåœ¨è§£å†³ååŒKEçš„ä¸‰ä¸ªæŒ‘æˆ˜æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ååŒå­¦ä¹ å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²æˆä¸ºåˆ©ç”¨å„æ–¹ç§æœ‰æ•°æ®çš„æ–°æ¨¡å¼ï¼Œå…¼é¡¾æ•ˆç‡å’Œéšç§ã€‚</li>
<li>çŸ¥è¯†ç¼–è¾‘ï¼ˆKEï¼‰æŠ€æœ¯å¯ä»¥æ˜ç¡®æ“æ§LLMçš„è¡Œä¸ºï¼Œä½†ååŒçŸ¥è¯†ç¼–è¾‘ï¼ˆååŒKEï¼‰å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚</li>
<li>ååŒKEé¢ä¸´ä¸‰ä¸ªç‹¬ç‰¹æŒ‘æˆ˜ï¼šçŸ¥è¯†é‡å ã€çŸ¥è¯†å†²çªå’ŒçŸ¥è¯†é—å¿˜ã€‚</li>
<li>COLLABEDITæ¡†æ¶é‡‡ç”¨éç ´åæ€§æ–¹æ³•å¤„ç†ååŒKEé—®é¢˜ï¼Œé€šè¿‡æ¨¡æ‹Ÿå…¨å±€KEè¡Œä¸ºæ¥åˆå¹¶æ¨¡å‹ã€‚</li>
<li>COLLABEDITæ¡†æ¶åœ¨å…¸å‹æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºå…¶ä»–ç ´åæ€§åŸºçº¿æ–¹æ³•ã€‚</li>
<li>COLLABEDITæ¡†æ¶ä¸ºè§£å†³ååŒKEçš„ä¸‰ä¸ªæŒ‘æˆ˜æä¾›äº†æœ‰æ•ˆæ‰‹æ®µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.09508">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d0a2d317d27849c4319b6748ac95e12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3668c21ab9c13cd23995f8c27ef61296.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="COMPL-AI-Framework-A-Technical-Interpretation-and-LLM-Benchmarking-Suite-for-the-EU-Artificial-Intelligence-Act"><a href="#COMPL-AI-Framework-A-Technical-Interpretation-and-LLM-Benchmarking-Suite-for-the-EU-Artificial-Intelligence-Act" class="headerlink" title="COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking   Suite for the EU Artificial Intelligence Act"></a>COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking   Suite for the EU Artificial Intelligence Act</h2><p><strong>Authors:Philipp Guldimann, Alexander Spiridonov, Robin Staab, Nikola JovanoviÄ‡, Mark Vero, Velko Vechev, Anna-Maria Gueorguieva, Mislav BalunoviÄ‡, Nikola Konstantinov, Pavol Bielik, Petar Tsankov, Martin Vechev</strong></p>
<p>The EUâ€™s Artificial Intelligence Act (AI Act) is a significant step towards responsible AI development, but lacks clear technical interpretation, making it difficult to assess modelsâ€™ compliance. This work presents COMPL-AI, a comprehensive framework consisting of (i) the first technical interpretation of the EU AI Act, translating its broad regulatory requirements into measurable technical requirements, with the focus on large language models (LLMs), and (ii) an open-source Act-centered benchmarking suite, based on thorough surveying and implementation of state-of-the-art LLM benchmarks. By evaluating 12 prominent LLMs in the context of COMPL-AI, we reveal shortcomings in existing models and benchmarks, particularly in areas like robustness, safety, diversity, and fairness. This work highlights the need for a shift in focus towards these aspects, encouraging balanced development of LLMs and more comprehensive regulation-aligned benchmarks. Simultaneously, COMPL-AI for the first time demonstrates the possibilities and difficulties of bringing the Actâ€™s obligations to a more concrete, technical level. As such, our work can serve as a useful first step towards having actionable recommendations for model providers, and contributes to ongoing efforts of the EU to enable application of the Act, such as the drafting of the GPAI Code of Practice. </p>
<blockquote>
<p>æ¬§ç›Ÿçš„ã€Šäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹ï¼ˆAIæ³•æ¡ˆï¼‰æ˜¯æœç€è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½å‘å±•è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ï¼Œä½†å®ƒç¼ºä¹æ˜ç¡®çš„æŠ€æœ¯è§£è¯»ï¼Œä½¿å¾—éš¾ä»¥è¯„ä¼°æ¨¡å‹æ˜¯å¦ç¬¦åˆæ³•è§„è¦æ±‚ã€‚æœ¬æ–‡ä»‹ç»äº†COMPL-AIï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼ŒåŒ…æ‹¬ï¼ˆiï¼‰å¯¹æ¬§ç›ŸAIæ³•æ¡ˆçš„é¦–ä»½æŠ€æœ¯è§£è¯»ï¼Œå°†å®½æ³›çš„ç›‘ç®¡è¦æ±‚è½¬åŒ–ä¸ºå¯è¡¡é‡çš„æŠ€æœ¯è¦æ±‚ï¼Œé‡ç‚¹å…³æ³¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼›ï¼ˆiiï¼‰ä¸€ä¸ªä»¥æ³•æ¡ˆä¸ºä¸­å¿ƒçš„å¼€æºåŸºå‡†æµ‹è¯•å¥—ä»¶ï¼ŒåŸºäºå¯¹å½“å‰æœ€å‰æ²¿LLMåŸºå‡†æµ‹è¯•çš„æ·±å…¥è°ƒæŸ¥å’Œå®æ–½ã€‚é€šè¿‡COMPL-AIè¯„ä¼°äº†12ä¸ªçªå‡ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬æ­ç¤ºäº†ç°æœ‰æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•åœ¨ç¨³å¥æ€§ã€å®‰å…¨æ€§ã€å¤šæ ·æ€§å’Œå…¬å¹³æ€§ç­‰æ–¹é¢çš„ä¸è¶³ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†éœ€è¦å…³æ³¨è¿™äº›æ–¹é¢ï¼Œé¼“åŠ±å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹³è¡¡å‘å±•ï¼Œä»¥åŠæ›´å…¨é¢çš„ç¬¦åˆæ³•è§„è¦æ±‚çš„åŸºå‡†æµ‹è¯•ã€‚åŒæ—¶ï¼ŒCOMPLI-AIé¦–æ¬¡å±•ç¤ºäº†å°†æ³•æ¡ˆä¹‰åŠ¡è½¬åŒ–ä¸ºæ›´å…·ä½“çš„æŠ€æœ¯å±‚é¢æ‰€é¢ä¸´çš„å¯èƒ½æ€§å’Œå›°éš¾ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„å·¥ä½œå¯ä»¥ä¸ºæ¨¡å‹æä¾›å•†æä¾›å¯è¡Œçš„å»ºè®®ï¼Œä¸ºæ¬§ç›Ÿå®æ–½è¯¥æ³•æ¡ˆçš„åŠªåŠ›åšå‡ºè´¡çŒ®ï¼Œå¦‚èµ·è‰GPAIå®è·µå®ˆåˆ™ç­‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.07959v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡ä»‹ç»äº†EUçš„AIæ³•æ¡ˆï¼ˆAI Actï¼‰åœ¨æ¨åŠ¨äººå·¥æ™ºèƒ½å‘å±•æ–¹é¢çš„ç§¯æä½œç”¨ï¼Œä½†æ³•æ¡ˆç¼ºä¹æ˜ç¡®çš„æŠ€æœ¯è§£è¯»ï¼Œéš¾ä»¥è¯„ä¼°æ¨¡å‹çš„åˆè§„æ€§ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†COMPL-AIæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é¦–æ¬¡å¯¹EU AIæ³•æ¡ˆè¿›è¡Œäº†æŠ€æœ¯è§£è¯»ï¼Œå°†å¹¿æ³›çš„ç›‘ç®¡è¦æ±‚è½¬åŒ–ä¸ºå¯åº¦é‡çš„æŠ€æœ¯è¦æ±‚ï¼Œå¹¶é‡ç‚¹å…³æ³¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚é€šè¿‡è¯„ä¼°12ä¸ªçªå‡ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè®ºæ–‡æ­ç¤ºäº†ç°æœ‰æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨³å¥æ€§ã€å®‰å…¨æ€§ã€å¤šæ ·æ€§å’Œå…¬å¹³æ€§æ–¹é¢ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†éœ€è¦å…³æ³¨è¿™äº›æ–¹é¢ï¼Œå¹¶é¼“åŠ±å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹³è¡¡å‘å±•ä»¥åŠæ›´å…¨é¢çš„æ³•è§„åŸºå‡†æµ‹è¯•ã€‚åŒæ—¶ï¼ŒCOMPL-AIé¦–æ¬¡å±•ç¤ºäº†å°†æ³•æ¡ˆä¹‰åŠ¡è½¬åŒ–ä¸ºæ›´å…·ä½“æŠ€æœ¯å±‚é¢å¯èƒ½æ€§å’Œå›°éš¾ã€‚å› æ­¤ï¼Œè¿™é¡¹å·¥ä½œå¯ä»¥ä¸ºæ¨¡å‹æä¾›å•†æä¾›å¯æ“ä½œçš„å»ºè®®ï¼Œå¹¶ä¸ºæ¬§ç›Ÿå®æ–½è¯¥æ³•æ¡ˆçš„åŠªåŠ›åšå‡ºè´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EUçš„AIæ³•æ¡ˆæ˜¯æœç€è´Ÿè´£ä»»çš„AIå‘å±•è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ï¼Œä½†ç¼ºä¹æ˜ç¡®çš„æŠ€æœ¯è§£è¯»ã€‚</li>
<li>COMPL-AIæ¡†æ¶é¦–æ¬¡å¯¹EU AIæ³•æ¡ˆè¿›è¡Œäº†æŠ€æœ¯è§£è¯»ï¼Œå¹¶ç‰¹åˆ«å…³æ³¨å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>é€šè¿‡è¯„ä¼°å‘ç°ï¼Œç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¨³å¥æ€§ã€å®‰å…¨æ€§ã€å¤šæ ·æ€§å’Œå…¬å¹³æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>éœ€è¦æ›´å¤šå…³æ³¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹³è¡¡å‘å±•ä»¥åŠæ›´å…¨é¢çš„æ³•è§„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>COMPL-AIå±•ç¤ºäº†å°†æ³•æ¡ˆä¹‰åŠ¡è½¬åŒ–ä¸ºå…·ä½“æŠ€æœ¯å±‚é¢çš„å¯èƒ½æ€§å’Œå›°éš¾ã€‚</li>
<li>è¯¥å·¥ä½œä¸ºæ¨¡å‹æä¾›å•†æä¾›äº†å¯æ“ä½œçš„å»ºè®®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.07959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ff5a64c67eda1ff49e21b90e501d5158.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fee550a034d38534f0684b670ccdc2c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="TIS-DPO-Token-level-Importance-Sampling-for-Direct-Preference-Optimization-With-Estimated-Weights"><a href="#TIS-DPO-Token-level-Importance-Sampling-for-Direct-Preference-Optimization-With-Estimated-Weights" class="headerlink" title="TIS-DPO: Token-level Importance Sampling for Direct Preference   Optimization With Estimated Weights"></a>TIS-DPO: Token-level Importance Sampling for Direct Preference   Optimization With Estimated Weights</h2><p><strong>Authors:Aiwei Liu, Haoping Bai, Zhiyun Lu, Yanchao Sun, Xiang Kong, Simon Wang, Jiulong Shan, Albin Madappally Jose, Xiaojiang Liu, Lijie Wen, Philip S. Yu, Meng Cao</strong></p>
<p>Direct Preference Optimization (DPO) has been widely adopted for preference alignment of Large Language Models (LLMs) due to its simplicity and effectiveness. However, DPO is derived as a bandit problem in which the whole response is treated as a single arm, ignoring the importance differences between tokens, which may affect optimization efficiency and make it difficult to achieve optimal results. In this work, we propose that the optimal data for DPO has equal expected rewards for each token in winning and losing responses, as there is no difference in token importance. However, since the optimal dataset is unavailable in practice, we propose using the original dataset for importance sampling to achieve unbiased optimization. Accordingly, we propose a token-level importance sampling DPO objective named TIS-DPO that assigns importance weights to each token based on its reward. Inspired by previous works, we estimate the token importance weights using the difference in prediction probabilities from a pair of contrastive LLMs. We explore three methods to construct these contrastive LLMs: (1) guiding the original LLM with contrastive prompts, (2) training two separate LLMs using winning and losing responses, and (3) performing forward and reverse DPO training with winning and losing responses. Experiments show that TIS-DPO significantly outperforms various baseline methods on harmlessness and helpfulness alignment and summarization tasks. We also visualize the estimated weights, demonstrating their ability to identify key token positions. </p>
<blockquote>
<p>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å› ç®€å•æœ‰æ•ˆè€Œå¹¿æ³›åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åå¥½å¯¹é½ã€‚ç„¶è€Œï¼ŒDPOæ˜¯ä½œä¸ºä¸€ç§å¼ºç›—é—®é¢˜è€Œæ¨å¯¼å‡ºæ¥çš„ï¼Œå®ƒå°†æ•´ä¸ªå“åº”è§†ä¸ºä¸€ä¸ªå•ä¸€çš„è‡‚ï¼Œå¿½ç•¥äº†æ ‡è®°ä¹‹é—´çš„é‡è¦æ€§å·®å¼‚ï¼Œè¿™å¯èƒ½ä¼šå½±å“ä¼˜åŒ–æ•ˆç‡ï¼Œå¹¶éš¾ä»¥è¾¾åˆ°æœ€ä½³ç»“æœã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºDPOçš„æœ€ä¼˜æ•°æ®åœ¨è·èƒœå’Œå¤±è´¥å“åº”ä¸­ä¸ºæ¯ä¸ªæ ‡è®°æä¾›ç›¸ç­‰çš„é¢„æœŸå¥–åŠ±ï¼Œå› ä¸ºæ ‡è®°çš„é‡è¦æ€§æ²¡æœ‰å·®å¼‚ã€‚ç„¶è€Œï¼Œç”±äºåœ¨å®é™…æ“ä½œä¸­æ— æ³•è·å¾—æœ€ä¼˜æ•°æ®é›†ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨åŸå§‹æ•°æ®é›†è¿›è¡Œé‡è¦æ€§é‡‡æ ·ä»¥å®ç°æ— åä¼˜åŒ–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ ‡è®°çº§åˆ«é‡è¦æ€§é‡‡æ ·çš„DPOç›®æ ‡ï¼Œå‘½åä¸ºTIS-DPOï¼Œå®ƒæ ¹æ®æ¯ä¸ªæ ‡è®°çš„å¥–åŠ±æ¥åˆ†é…é‡è¦æ€§æƒé‡ã€‚å—ä»¥å‰å·¥ä½œçš„å¯å‘ï¼Œæˆ‘ä»¬ä¼°è®¡æ ‡è®°é‡è¦æ€§æƒé‡æ˜¯ä½¿ç”¨ä¸€å¯¹å¯¹æ¯”LLMçš„é¢„æµ‹æ¦‚ç‡å·®å¼‚ã€‚æˆ‘ä»¬æ¢ç´¢äº†æ„å»ºè¿™äº›å¯¹æ¯”LLMçš„ä¸‰ç§æ–¹æ³•ï¼š1ï¼‰ç”¨å¯¹æ¯”æç¤ºå¼•å¯¼åŸå§‹LLMï¼›2ï¼‰ä½¿ç”¨è·èƒœå’Œå¤±è´¥å“åº”è®­ç»ƒä¸¤ä¸ªå•ç‹¬çš„LLMï¼›ä»¥åŠ3ï¼‰ä½¿ç”¨è·èƒœå’Œå¤±è´¥å“åº”è¿›è¡Œæ­£å‘å’Œåå‘DPOè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æ— å®³æ€§ã€æœ‰ç›Šæ€§å¯¹é½å’Œæ‘˜è¦ä»»åŠ¡æ–¹é¢ï¼ŒTIS-DPOæ˜¾è‘—ä¼˜äºå„ç§åŸºçº¿æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜å¯è§†åŒ–äº†ä¼°è®¡çš„æƒé‡ï¼Œå±•ç¤ºäº†å®ƒä»¬è¯†åˆ«å…³é”®æ ‡è®°ä½ç½®çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.04350v2">PDF</a> 30 pages, 8 figures, 8 tables, Published in ICLR 2025</p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åå¥½å¯¹é½ä¸­å¹¿æ³›é‡‡ç”¨äº†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ–¹æ³•ï¼Œå› å…¶ç®€å•æœ‰æ•ˆã€‚ä½†DPOæ–¹æ³•åœ¨å¤„ç†å“åº”æ—¶å°†æ•´ä¸ªå“åº”è§†ä¸ºå•ä¸ªè‡‚ï¼Œå¿½ç•¥äº†æ ‡è®°é—´çš„å·®å¼‚é‡è¦æ€§ï¼Œå¯èƒ½å½±å“ä¼˜åŒ–æ•ˆç‡å¹¶éš¾ä»¥è¾¾åˆ°æœ€ä¼˜ç»“æœã€‚æœ¬ç ”ç©¶æå‡ºåœ¨DPOçš„æœ€ä¼˜æ•°æ®ä¸­ï¼Œè·èƒœå’Œå¤±è´¥å“åº”ä¸­çš„æ¯ä¸ªæ ‡è®°åº”æœ‰ç›¸ç­‰çš„é¢„æœŸå¥–åŠ±ã€‚ä¸ºå®è·µä¸­çš„æœ€ä¼˜æ•°æ®é›†ç¼ºå¤±é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é‡è¦æ€§é‡‡æ ·ç­–ç•¥å¹¶ä½¿ç”¨åŸæ•°æ®é›†è¿›è¡Œæ— åä¼˜åŒ–ã€‚æ®æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºTIS-DPOçš„æ ‡è®°çº§åˆ«é‡è¦æ€§é‡‡æ ·DPOç›®æ ‡ï¼Œæ ¹æ®å¥–åŠ±ä¸ºæ¯ä¸ªæ ‡è®°åˆ†é…é‡è¦æ€§æƒé‡ã€‚å—å…ˆå‰å·¥ä½œçš„å¯å‘ï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸€å¯¹å¯¹æ¯”LLMçš„é¢„æµ‹æ¦‚ç‡å·®å¼‚æ¥ä¼°è®¡æ ‡è®°é‡è¦æ€§æƒé‡ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†æ„å»ºå¯¹æ¯”LLMçš„ä¸‰ç§æ–¹æ³•ï¼Œå¹¶åœ¨æ— å®³æ€§ã€æœ‰ç›Šæ€§å¯¹é½å’Œæ‘˜è¦ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œè¯æ˜TIS-DPOæ˜¾è‘—ä¼˜äºå„ç§åŸºçº¿æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†ä¼°è®¡æƒé‡çš„å¯è§†åŒ–èƒ½åŠ›ï¼Œèƒ½è¯†åˆ«å…³é”®æ ‡è®°ä½ç½®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰åœ¨LLMåå¥½å¯¹é½ä¸­çš„å¹¿æ³›åº”ç”¨åŠå…¶ç®€æ´æœ‰æ•ˆæ€§ã€‚</li>
<li>DPOæ–¹æ³•å­˜åœ¨çš„é—®é¢˜ï¼šå¿½è§†æ ‡è®°é—´çš„é‡è¦æ€§å·®å¼‚ï¼Œå¯èƒ½å½±å“ä¼˜åŒ–æ•ˆç‡å’Œç»“æœä¼˜åŒ–ã€‚</li>
<li>æå‡ºåœ¨æœ€ä¼˜æ•°æ®é›†ä¸­æ¯ä¸ªæ ‡è®°åº”æœ‰ç›¸ç­‰é¢„æœŸå¥–åŠ±çš„è§‚ç‚¹ã€‚</li>
<li>ä¸ºè§£å†³å®è·µä¸­æœ€ä¼˜æ•°æ®é›†çš„ç¼ºå¤±é—®é¢˜ï¼Œé‡‡ç”¨é‡è¦æ€§é‡‡æ ·ç­–ç•¥å¹¶åˆ©ç”¨åŸæ•°æ®é›†è¿›è¡Œæ— åä¼˜åŒ–ã€‚</li>
<li>æå‡ºåä¸ºTIS-DPOçš„æ ‡è®°çº§åˆ«é‡è¦æ€§é‡‡æ ·DPOç›®æ ‡ï¼Œæ ¹æ®å¥–åŠ±ä¸ºæ¯ä¸ªæ ‡è®°åˆ†é…æƒé‡ã€‚</li>
<li>åˆ©ç”¨å¯¹æ¯”LLMçš„é¢„æµ‹æ¦‚ç‡å·®å¼‚æ¥ä¼°è®¡æ ‡è®°é‡è¦æ€§æƒé‡çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.04350">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-79b664213186147549a75840b6856c16.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-64337a5b9f1356a91ed984bee71a71f7.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GPT-4o-as-the-Gold-Standard-A-Scalable-and-General-Purpose-Approach-to-Filter-Language-Model-Pretraining-Data"><a href="#GPT-4o-as-the-Gold-Standard-A-Scalable-and-General-Purpose-Approach-to-Filter-Language-Model-Pretraining-Data" class="headerlink" title="GPT-4o as the Gold Standard: A Scalable and General Purpose Approach to   Filter Language Model Pretraining Data"></a>GPT-4o as the Gold Standard: A Scalable and General Purpose Approach to   Filter Language Model Pretraining Data</h2><p><strong>Authors:Jifan Zhang, Ziyue Luo, Jia Liu, Ness Shroff, Robert Nowak</strong></p>
<p>Large language models require vast amounts of high-quality training data, but effective filtering of web-scale datasets remains a significant challenge. This paper demonstrates that GPT-4o is remarkably effective at identifying high-quality training data, but its prohibitive cost makes it impractical at web-scale. We propose SIEVE, a lightweight alternative that matches GPT-4o accuracy at less than 1% of the cost. SIEVE can perform up to 500 filtering operations for the cost of one GPT-4o filtering call. The key to SIEVE is a seamless integration of GPT-4o and lightweight text classification models, using active learning to fine-tune these models in the background with a small number of calls to GPT-4o. Once trained, it performs as well as GPT-4o at a tiny fraction of the cost. Through different filtering prompts, SIEVE can efficiently curate high quality data for general or specialized domains from web-scale corpora â€“ a valuable capability given the current scarcity of high-quality domain-specific datasets. Extensive experiments using automatic and human evaluation metrics show that SIEVE and GPT-4o achieve similar performance on five highly specific filtering prompts. In addition, when performing quality filtering on web crawl datasets, we demonstrate SIEVE can further improve over state-of-the-art quality filtering methods in the DataComp-LM challenge for selecting LLM pretraining data. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹éœ€è¦å¤§é‡çš„é«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼Œä½†å¦‚ä½•åœ¨ç½‘ç»œè§„æ¨¡çš„æ•°æ®é›†ä¸­è¿›è¡Œæœ‰æ•ˆçš„è¿‡æ»¤ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡è¯æ˜äº†GPT-4oåœ¨è¯†åˆ«é«˜è´¨é‡è®­ç»ƒæ•°æ®æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†å…¶é«˜æ˜‚çš„æˆæœ¬ä½¿å¾—åœ¨ç½‘ç»œè§„æ¨¡ä¸Šéš¾ä»¥å®ç°ã€‚æˆ‘ä»¬æå‡ºäº†SIEVEï¼Œä¸€ä¸ªè½»é‡çº§çš„æ›¿ä»£å“ï¼Œå…¶å‡†ç¡®æ€§å¯ä¸GPT-4oç›¸åª²ç¾ï¼Œä½†æˆæœ¬ä¸åˆ°å…¶ç™¾åˆ†ä¹‹ä¸€ã€‚SIEVEå¯ä»¥åœ¨ä¸€æ¬¡GPT-4oè¿‡æ»¤è°ƒç”¨çš„æˆæœ¬ä¸‹æ‰§è¡Œé«˜è¾¾500æ¬¡è¿‡æ»¤æ“ä½œã€‚SIEVEçš„å…³é”®åœ¨äºæ— ç¼é›†æˆäº†GPT-4oå’Œè½»é‡çº§æ–‡æœ¬åˆ†ç±»æ¨¡å‹ï¼Œåˆ©ç”¨ä¸»åŠ¨å­¦ä¹ åœ¨åå°å¾®è°ƒè¿™äº›æ¨¡å‹ï¼Œåªéœ€å°‘é‡è°ƒç”¨GPT-4oå³å¯ã€‚ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œå®ƒçš„è¡¨ç°ä¸GPT-4oä¸€æ ·å‡ºè‰²ï¼Œä½†æˆæœ¬å´å¾ˆå°ã€‚é€šè¿‡ä¸åŒçš„è¿‡æ»¤æç¤ºï¼ŒSIEVEå¯ä»¥æœ‰æ•ˆåœ°ä»ç½‘ç»œè§„æ¨¡è¯­æ–™åº“ä¸­ä¸ºé€šç”¨æˆ–ç‰¹å®šé¢†åŸŸç­›é€‰é«˜è´¨é‡æ•°æ®â€”â€”åœ¨å½“å‰é«˜è´¨é‡ç‰¹å®šé¢†åŸŸæ•°æ®é›†ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œè¿™æ˜¯ä¸€é¡¹éå¸¸æœ‰ä»·å€¼çš„èƒ½åŠ›ã€‚ä½¿ç”¨è‡ªåŠ¨å’Œäººç±»è¯„ä¼°æŒ‡æ ‡çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSIEVEå’ŒGPT-4oåœ¨äº”ä¸ªé«˜åº¦ç‰¹å®šçš„è¿‡æ»¤æç¤ºä¸Šå–å¾—äº†ç›¸ä¼¼çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåœ¨å¯¹ç½‘ç»œçˆ¬è™«æ•°æ®é›†è¿›è¡Œè´¨é‡è¿‡æ»¤æ—¶ï¼Œæˆ‘ä»¬åœ¨DataComp-LMæŒ‘æˆ˜ä¸­è¿›ä¸€æ­¥è¯æ˜äº†SIEVEåœ¨é€‰å–LLMé¢„è®­ç»ƒæ•°æ®æ–¹é¢ä¼˜äºå½“å‰å…ˆè¿›çš„è´¨é‡è¿‡æ»¤æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02755v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬è®ºæ–‡é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹éœ€è¦å¤§é‡é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„é—®é¢˜ï¼Œæå‡ºä¸€ç§åä¸ºSIEVEçš„è½»é‡çº§ç­›é€‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç»“åˆäº†GPT-4oå’Œè½»é‡çº§æ–‡æœ¬åˆ†ç±»æ¨¡å‹ï¼Œé€šè¿‡ä¸»åŠ¨å­¦ä¹ è¿›è¡Œå¾®è°ƒï¼Œèƒ½å¤Ÿåœ¨æˆæœ¬è¾ƒä½çš„æƒ…å†µä¸‹å®ç°ä¸GPT-4oç›¸è¿‘çš„ç­›é€‰æ•ˆæœã€‚SIEVEå¯ä»¥é«˜æ•ˆåœ°ä»ç½‘ç»œè§„æ¨¡è¯­æ–™åº“ä¸­ç­›é€‰é«˜è´¨é‡æ•°æ®ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å®šé¢†åŸŸè¡¨ç°çªå‡ºã€‚å®éªŒè¡¨æ˜ï¼Œå…¶åœ¨äº”ä¸ªé«˜åº¦ç‰¹å®šçš„ç­›é€‰æç¤ºä»¥åŠç½‘ç»œçˆ¬è™«æ•°æ®é›†çš„è´¨é‡ç­›é€‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨DataComp-LMæŒ‘æˆ˜ä¸­è¿›ä¸€æ­¥æ”¹è¿›äº†ç°æœ‰è´¨é‡ç­›é€‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹éœ€è¦å¤§é‡é«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼Œä½†ç­›é€‰è¿™äº›æ•°æ®æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>GPT-4oåœ¨è¯†åˆ«é«˜è´¨é‡è®­ç»ƒæ•°æ®æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥å®ç°å¤§è§„æ¨¡åº”ç”¨ã€‚</li>
<li>SIEVEæ˜¯ä¸€ç§è½»é‡çº§ç­›é€‰æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æˆæœ¬è¾ƒä½çš„æƒ…å†µä¸‹å®ç°ä¸GPT-4oç›¸è¿‘çš„ç­›é€‰æ•ˆæœã€‚</li>
<li>SIEVEé€šè¿‡ç»“åˆGPT-4oå’Œè½»é‡çº§æ–‡æœ¬åˆ†ç±»æ¨¡å‹ï¼Œä½¿ç”¨ä¸»åŠ¨å­¦ä¹ è¿›è¡Œå¾®è°ƒã€‚</li>
<li>SIEVEå¯ä»¥é«˜æ•ˆåœ°ä»ç½‘ç»œè§„æ¨¡è¯­æ–™åº“ä¸­ç­›é€‰é«˜è´¨é‡æ•°æ®ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å®šé¢†åŸŸè¡¨ç°çªå‡ºã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒSIEVEåœ¨å¤šä¸ªç‰¹å®šç­›é€‰ä»»åŠ¡åŠç½‘ç»œçˆ¬è™«æ•°æ®é›†çš„è´¨é‡ç­›é€‰ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02755">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-14a5b8621d516759f1fbe0f6ff3dd45c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a198f7c21cd624bb6ce782253f07a07c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31cd2e7b5fce530e5c7be18b08c44287.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fcbe3a10a82ad41f59cad5af04b29c8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="How-to-Make-LLMs-Strong-Node-Classifiers"><a href="#How-to-Make-LLMs-Strong-Node-Classifiers" class="headerlink" title="How to Make LLMs Strong Node Classifiers?"></a>How to Make LLMs Strong Node Classifiers?</h2><p><strong>Authors:Zhe Xu, Kaveh Hassani, Si Zhang, Hanqing Zeng, Michihiro Yasunaga, Limei Wang, Dongqi Fu, Ning Yao, Bo Long, Hanghang Tong</strong></p>
<p>Language Models (LMs) are increasingly challenging the dominance of domain-specific models, such as Graph Neural Networks (GNNs) and Graph Transformers (GTs), in graph learning tasks. Following this trend, we propose a novel approach that empowers off-the-shelf LMs to achieve performance comparable to state-of-the-art (SOTA) GNNs on node classification tasks, without requiring any architectural modification. By preserving the LMâ€™s original architecture, our approach retains a key benefit of LM instruction tuning: the ability to jointly train on diverse datasets, fostering greater flexibility and efficiency. To achieve this, we introduce two key augmentation strategies: (1) Enriching LMsâ€™ input using topological and semantic retrieval methods, which provide richer contextual information, and (2) guiding the LMsâ€™ classification process through a lightweight GNN classifier that effectively prunes class candidates. Our experiments on real-world datasets show that backbone Flan-T5 LMs equipped with these augmentation strategies outperform SOTA text-output node classifiers and are comparable to top-performing vector-output node classifiers. By bridging the gap between specialized node classifiers and general LMs, this work paves the way for more versatile and widely applicable graph learning models. We will open-source the code upon publication. </p>
<blockquote>
<p>è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰æ­£åœ¨æŒ‘æˆ˜å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰å’Œå›¾è½¬æ¢å™¨ï¼ˆGTsï¼‰ç­‰ç‰¹å®šé¢†åŸŸæ¨¡å‹åœ¨å›¾å­¦ä¹ ä»»åŠ¡ä¸­çš„ä¸»å¯¼åœ°ä½ã€‚éšç€è¿™ä¸€è¶‹åŠ¿ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œä½¿ç°æˆçš„è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸Šå®ç°ä¸æœ€æ–°æŠ€æœ¯ï¼ˆSOTAï¼‰GNNç›¸æ¯”çš„æ€§èƒ½ï¼Œè€Œæ— éœ€è¿›è¡Œä»»ä½•æ¶æ„ä¿®æ”¹ã€‚é€šè¿‡ä¿ç•™LMçš„åŸå§‹æ¶æ„ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¿ç•™äº†LMæŒ‡ä»¤è°ƒæ•´çš„å…³é”®ä¼˜åŠ¿ï¼šåœ¨å¤šæ ·æ•°æ®é›†ä¸Šè¿›è¡Œè”åˆè®­ç»ƒçš„èƒ½åŠ›ï¼Œæé«˜äº†çµæ´»æ€§å’Œæ•ˆç‡ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§å…³é”®çš„å¢å¼ºç­–ç•¥ï¼šï¼ˆ1ï¼‰ä½¿ç”¨æ‹“æ‰‘å’Œè¯­ä¹‰æ£€ç´¢æ–¹æ³•ä¸°å¯ŒLMçš„è¾“å…¥ï¼Œæä¾›æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼›ï¼ˆ2ï¼‰é€šè¿‡ä¸€ä¸ªè½»é‡çº§çš„GNNåˆ†ç±»å™¨å¼•å¯¼LMçš„åˆ†ç±»è¿‡ç¨‹ï¼Œæœ‰æ•ˆç¼©å‡ç±»åˆ«å€™é€‰ã€‚æˆ‘ä»¬åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œé…å¤‡è¿™äº›å¢å¼ºç­–ç•¥çš„åç«¯Flan-T5 LMsè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–‡æœ¬è¾“å‡ºèŠ‚ç‚¹åˆ†ç±»å™¨ï¼Œå¹¶ä¸é¡¶çº§å‘é‡è¾“å‡ºèŠ‚ç‚¹åˆ†ç±»å™¨çš„æ€§èƒ½ç›¸å½“ã€‚é€šè¿‡ç¼©å°ä¸“ä¸šèŠ‚ç‚¹åˆ†ç±»å™¨å’Œé€šç”¨LMä¹‹é—´çš„å·®è·ï¼Œè¿™é¡¹å·¥ä½œä¸ºæ›´é€šç”¨å’Œæ›´å¹¿æ³›é€‚ç”¨çš„å›¾å­¦ä¹ æ¨¡å‹é“ºå¹³äº†é“è·¯ã€‚ä»£ç å°†åœ¨å‘å¸ƒæ—¶å¼€æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02296v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰æ­£åœ¨æŒ‘æˆ˜å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰å’Œå›¾è½¬æ¢å™¨ï¼ˆGTsï¼‰ç­‰åŸŸç‰¹å®šæ¨¡å‹åœ¨å›¾å­¦ä¹ ä»»åŠ¡ä¸­çš„ä¸»å¯¼åœ°ä½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œåœ¨ä¸æ”¹å˜æ¶æ„çš„å‰æä¸‹ï¼Œä½¿ç°æˆçš„LMsåœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½å¯ä¸æœ€å…ˆè¿›çš„GNNsç›¸åª²ç¾ã€‚é€šè¿‡ä¿ç•™LMçš„åŸå§‹æ¶æ„ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¿ç•™äº†LMæŒ‡ä»¤è°ƒæ•´çš„çš„å…³é”®ä¼˜åŠ¿ï¼Œå³èƒ½å¤Ÿåœ¨å„ç§æ•°æ®é›†ä¸Šè¿›è¡Œè”åˆè®­ç»ƒï¼Œæé«˜äº†çµæ´»æ€§å’Œæ•ˆç‡ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªå…³é”®çš„å¢å¼ºç­–ç•¥ï¼šä¸€æ˜¯åˆ©ç”¨æ‹“æ‰‘å’Œè¯­ä¹‰æ£€ç´¢æ–¹æ³•ä¸°å¯ŒLMçš„è¾“å…¥ï¼Œæä¾›æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼›äºŒæ˜¯é€šè¿‡ä¸€ä¸ªè½»é‡çº§çš„GNNåˆ†ç±»å™¨å¼•å¯¼LMçš„åˆ†ç±»è¿‡ç¨‹ï¼Œæœ‰æ•ˆåœ°å‡å°‘ç±»å€™é€‰ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œé…å¤‡è¿™äº›å¢å¼ºç­–ç•¥çš„Flan-T5 LMsè¶…è¶Šäº†æœ€å…ˆè¿›æ–‡æœ¬è¾“å‡ºèŠ‚ç‚¹åˆ†ç±»å™¨ï¼Œå¹¶ä¸é¡¶çº§å‘é‡è¾“å‡ºèŠ‚ç‚¹åˆ†ç±»å™¨æ€§èƒ½ç›¸å½“ã€‚æœ¬æ–‡ç¼©å°äº†ä¸“ç”¨èŠ‚ç‚¹åˆ†ç±»å™¨ä¸ä¸€èˆ¬LMsä¹‹é—´çš„å·®è·ï¼Œä¸ºæ›´é€šç”¨å’Œå¹¿æ³›é€‚ç”¨çš„å›¾å­¦ä¹ æ¨¡å‹é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LMsæ­£åœ¨æŒ‘æˆ˜å›¾å­¦ä¹ é¢†åŸŸä¸­çš„ç‰¹å®šæ¨¡å‹ï¼ˆå¦‚GNNså’ŒGTsï¼‰çš„ä¸»å¯¼åœ°ä½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œä½¿LMsåœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½å¯ä¸æœ€å…ˆè¿›çš„GNNsç›¸åª²ç¾ï¼Œæ— éœ€è¿›è¡Œä»»ä½•æ¶æ„ä¿®æ”¹ã€‚</li>
<li>é€šè¿‡ä¿ç•™LMçš„åŸå§‹æ¶æ„ï¼Œä¿ç•™äº†å…¶è”åˆè®­ç»ƒçš„èƒ½åŠ›ï¼Œæé«˜äº†çµæ´»æ€§å’Œæ•ˆç‡ã€‚</li>
<li>å¼•å…¥ä¸¤ä¸ªå…³é”®å¢å¼ºç­–ç•¥ï¼šåˆ©ç”¨æ‹“æ‰‘å’Œè¯­ä¹‰æ£€ç´¢ä¸°å¯ŒLMè¾“å…¥ï¼Œä»¥åŠä½¿ç”¨è½»é‡çº§GNNåˆ†ç±»å™¨å¼•å¯¼LMçš„åˆ†ç±»è¿‡ç¨‹ã€‚</li>
<li>åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå¢å¼ºçš„LMsæ€§èƒ½è¶…è¶Šäº†è®¸å¤šæœ€å…ˆè¿›æ–‡æœ¬è¾“å‡ºèŠ‚ç‚¹åˆ†ç±»å™¨ã€‚</li>
<li>å¢å¼ºçš„LMsæ€§èƒ½ä¸é¡¶çº§å‘é‡è¾“å‡ºèŠ‚ç‚¹åˆ†ç±»å™¨ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02296">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cd85519e2fc1e51061e4026e47a63ab5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b79bc6286ae4432e6f0244306e2368bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-127c00fe4fa6cf5d6213185c4b9e0065.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="LLaVA-3D-A-Simple-yet-Effective-Pathway-to-Empowering-LMMs-with-3D-awareness"><a href="#LLaVA-3D-A-Simple-yet-Effective-Pathway-to-Empowering-LMMs-with-3D-awareness" class="headerlink" title="LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with   3D-awareness"></a>LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with   3D-awareness</h2><p><strong>Authors:Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, Xihui Liu</strong></p>
<p>Recent advancements in Large Multimodal Models (LMMs) have greatly enhanced their proficiency in 2D visual understanding tasks, enabling them to effectively process and understand images and videos. However, the development of LMMs with 3D-awareness for 3D scene understanding has been hindered by the lack of large-scale 3D vision-language datasets and powerful 3D encoders. In this paper, we introduce a simple yet effective framework called LLaVA-3D. Leveraging the strong 2D understanding priors from LLaVA, our LLaVA-3D efficiently adapts LLaVA for 3D scene understanding without compromising 2D understanding capabilities. To achieve this, we utilize the 3D position embeddings to bring the 2D CLIP patches within a 3D spatial context. By integrating the 3D position embeddings into 2D LMMs and employing joint 2D and 3D vision-language instruction tuning, we establish a unified architecture for both 2D image understanding and 3D scene understanding. Experimental results show that LLaVA-3D converges 3.5x faster than existing 3D LMMs when trained on 3D vision-language datasets. Moreover, LLaVA-3D not only achieves state-of-the-art performance across various 3D tasks but also maintains comparable 2D image understanding and vision-language conversation capabilities with LLaVA. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨äºŒç»´è§†è§‰ç†è§£ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›å¾—åˆ°äº†æå¤§çš„æå‡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å’Œç†è§£å›¾åƒå’Œè§†é¢‘ã€‚ç„¶è€Œï¼Œå¯¹äºä¸‰ç»´åœºæ™¯ç†è§£çš„ä¸‰ç»´æ„ŸçŸ¥å‹LMMsçš„å‘å±•å—åˆ°äº†å¤§è§„æ¨¡ä¸‰ç»´è§†è§‰è¯­è¨€æ•°æ®é›†å’Œå¼ºå¤§ä¸‰ç»´ç¼–ç å™¨çš„ç¼ºä¹çš„é˜»ç¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œç§°ä¸ºLLaVA-3Dã€‚æˆ‘ä»¬åˆ©ç”¨LLaVAå¼ºå¤§çš„äºŒç»´ç†è§£å…ˆéªŒçŸ¥è¯†ï¼Œä½¿LLaVA-3Dèƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œä¸‰ç»´åœºæ™¯ç†è§£ï¼ŒåŒæ—¶ä¸å¦¥åäºŒç»´ç†è§£èƒ½åŠ›ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸‰ç»´ä½ç½®åµŒå…¥ï¼Œå°†äºŒç»´CLIPè¡¥ä¸ç½®äºä¸‰ç»´ç©ºé—´ä¸Šä¸‹æ–‡ä¸­ã€‚é€šè¿‡å°†ä¸‰ç»´ä½ç½®åµŒå…¥é›†æˆåˆ°äºŒç»´LMMsä¸­ï¼Œå¹¶é‡‡ç”¨è”åˆäºŒç»´å’Œä¸‰ç»´è§†è§‰è¯­è¨€æŒ‡ä»¤è°ƒæ•´ï¼Œæˆ‘ä»¬å»ºç«‹äº†ç”¨äºäºŒç»´å›¾åƒç†è§£å’Œä¸‰ç»´åœºæ™¯ç†è§£çš„ç»Ÿä¸€æ¶æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¸‰ç»´è§†è§‰è¯­è¨€æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼ŒLLaVA-3Dçš„æ”¶æ•›é€Ÿåº¦æ˜¯ç°æœ‰ä¸‰ç»´LMMsçš„3.5å€ã€‚æ­¤å¤–ï¼ŒLLaVA-3Dä¸ä»…åœ¨å„ç§ä¸‰ç»´ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè€Œä¸”ä¸LLaVAä¿æŒäº†ç›¸å½“çš„äºŒç»´å›¾åƒç†è§£å’Œè§†è§‰è¯­è¨€å¯¹è¯èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18125v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://zcmax.github.io/projects/LLaVA-3D/">https://zcmax.github.io/projects/LLaVA-3D/</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨äºŒç»´è§†è§‰ç†è§£ä»»åŠ¡ä¸Šçš„èƒ½åŠ›æ˜¾è‘—æå‡ï¼Œä½†åœ¨ä¸‰ç»´åœºæ™¯ç†è§£æ–¹é¢çš„å‘å±•å—åˆ°å¤§å‹ä¸‰ç»´è§†è§‰è¯­è¨€æ•°æ®é›†å’Œå¼ºå¤§ä¸‰ç»´ç¼–ç å™¨çš„ç¼ºä¹çš„é˜»ç¢ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶LLaVA-3Dï¼Œå®ƒåˆ©ç”¨LLaVAçš„å¼ºå¤§çš„äºŒç»´ç†è§£å…ˆéªŒçŸ¥è¯†ï¼Œåœ¨ä¸æŸå®³äºŒç»´ç†è§£èƒ½åŠ›çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆåœ°é€‚åº”ä¸‰ç»´åœºæ™¯ç†è§£ã€‚é€šè¿‡æ•´åˆä¸‰ç»´ä½ç½®åµŒå…¥ï¼Œå°†äºŒç»´CLIPè¡¥ä¸ç½®äºä¸‰ç»´ç©ºé—´ä¸Šä¸‹æ–‡ä¸­ï¼Œå¹¶è”åˆé‡‡ç”¨äºŒç»´å’Œä¸‰ç»´è§†è§‰è¯­è¨€æŒ‡ä»¤è°ƒæ•´ï¼Œå»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¶æ„ï¼Œæ—¢å¯ç”¨äºäºŒç»´å›¾åƒç†è§£ï¼Œä¹Ÿå¯ç”¨äºä¸‰ç»´åœºæ™¯ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLaVA-3Dåœ¨ä¸‰ç»´è§†è§‰è¯­è¨€æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œæ”¶æ•›é€Ÿåº¦æ˜¯ç°æœ‰ä¸‰ç»´LMMsçš„3.5å€ã€‚LLaVA-3Dä¸ä»…åœ¨å„ç§ä¸‰ç»´ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œè€Œä¸”ä¸LLaVAç›¸æ¯”ï¼Œè¿˜èƒ½ä¿æŒç›¸å½“çš„äºŒç»´å›¾åƒç†è§£å’Œè§†è§‰è¯­è¨€å¯¹è¯èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LMMsåœ¨äºŒç»´è§†è§‰ç†è§£ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ä¸‰ç»´åœºæ™¯ç†è§£çš„å‘å±•å—åˆ°å¤§å‹ä¸‰ç»´è§†è§‰è¯­è¨€æ•°æ®é›†çš„ç¼ºä¹å’Œå¼ºå¤§ä¸‰ç»´ç¼–ç å™¨çš„é™åˆ¶ã€‚</li>
<li>LLaVA-3Dæ¡†æ¶ç»“åˆäº†LLaVAçš„äºŒç»´ç†è§£å…ˆéªŒçŸ¥è¯†å’Œä¸‰ç»´ä½ç½®åµŒå…¥æŠ€æœ¯ã€‚</li>
<li>LLaVA-3Då®ç°äº†äºŒç»´å’Œä¸‰ç»´è§†è§‰ç†è§£çš„ç»Ÿä¸€æ¶æ„ã€‚</li>
<li>LLaVA-3Dåœ¨è®­ç»ƒæ—¶æ”¶æ•›é€Ÿåº¦å¿«ï¼Œè¾¾åˆ°ç°æœ‰ä¸‰ç»´LMMsçš„3.5å€ã€‚</li>
<li>LLaVA-3Dåœ¨å„ç§ä¸‰ç»´ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.18125">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd7cc6c33475428ac4017f233d31a098.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f99fa733bdb184d551b733c3979ac5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09ef05626e0748491f31f07bebcc066a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f423fc593572bdaaba9a65a6c077d4c.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="FMDLlama-Financial-Misinformation-Detection-based-on-Large-Language-Models"><a href="#FMDLlama-Financial-Misinformation-Detection-based-on-Large-Language-Models" class="headerlink" title="FMDLlama: Financial Misinformation Detection based on Large Language   Models"></a>FMDLlama: Financial Misinformation Detection based on Large Language   Models</h2><p><strong>Authors:Zhiwei Liu, Xin Zhang, Kailai Yang, Qianqian Xie, Jimin Huang, Sophia Ananiadou</strong></p>
<p>The emergence of social media has made the spread of misinformation easier. In the financial domain, the accuracy of information is crucial for various aspects of financial market, which has made financial misinformation detection (FMD) an urgent problem that needs to be addressed. Large language models (LLMs) have demonstrated outstanding performance in various fields. However, current studies mostly rely on traditional methods and have not explored the application of LLMs in the field of FMD. The main reason is the lack of FMD instruction tuning datasets and evaluation benchmarks. In this paper, we propose FMDLlama, the first open-sourced instruction-following LLMs for FMD task based on fine-tuning Llama3.1 with instruction data, the first multi-task FMD instruction dataset (FMDID) to support LLM instruction tuning, and a comprehensive FMD evaluation benchmark (FMD-B) with classification and explanation generation tasks to test the FMD ability of LLMs. We compare our models with a variety of LLMs on FMD-B, where our model outperforms other open-sourced LLMs as well as OpenAIâ€™s products. This project is available at <a target="_blank" rel="noopener" href="https://github.com/lzw108/FMD">https://github.com/lzw108/FMD</a>. </p>
<blockquote>
<p>ç¤¾äº¤åª’ä½“çš„å…´èµ·ä½¿å¾—é”™è¯¯ä¿¡æ¯çš„ä¼ æ’­æ›´åŠ å®¹æ˜“ã€‚åœ¨é‡‘èé¢†åŸŸï¼Œä¿¡æ¯çš„å‡†ç¡®æ€§å¯¹é‡‘èå¸‚åœºçš„å„ä¸ªæ–¹é¢éƒ½è‡³å…³é‡è¦ï¼Œè¿™ä½¿å¾—é‡‘èè™šå‡ä¿¡æ¯æ£€æµ‹ï¼ˆFMDï¼‰æˆä¸ºä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ä¸ªé¢†åŸŸéƒ½è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç ”ç©¶ä¸»è¦ä¾èµ–äºä¼ ç»Ÿæ–¹æ³•ï¼Œå°šæœªæ¢ç´¢LLMåœ¨é‡‘èè™šå‡ä¿¡æ¯æ£€æµ‹ï¼ˆFMDï¼‰é¢†åŸŸçš„åº”ç”¨ã€‚ä¸»è¦åŸå› æ˜¯ç¼ºä¹FMDæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†å’Œè¯„ä¼°åŸºå‡†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†FMDLlamaï¼Œè¿™æ˜¯åŸºäºæŒ‡ä»¤æ•°æ®å¾®è°ƒLlama3.1çš„é‡‘èè™šå‡ä¿¡æ¯æ£€æµ‹çš„é¦–ä¸ªå¼€æºæŒ‡ä»¤éµå¾ªçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†é¦–ä¸ªå¤šä»»åŠ¡é‡‘èè™šå‡ä¿¡æ¯æ£€æµ‹æŒ‡ä»¤æ•°æ®é›†ï¼ˆFMDIDï¼‰ï¼Œä»¥æ”¯æŒLLMæŒ‡ä»¤è°ƒæ•´ï¼Œä»¥åŠå…¨é¢çš„FMDè¯„ä¼°åŸºå‡†ï¼ˆFMD-Bï¼‰ï¼ŒåŒ…æ‹¬åˆ†ç±»å’Œè§£é‡Šç”Ÿæˆä»»åŠ¡ï¼Œä»¥æµ‹è¯•LLMçš„é‡‘èè™šå‡ä¿¡æ¯æ£€æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨FMD-Bä¸Šæ¯”è¾ƒäº†æˆ‘ä»¬çš„æ¨¡å‹ä¸å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç»“æœæ˜¾ç¤ºæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å…¶ä»–å¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ä»¥åŠOpenAIçš„äº§å“ã€‚æ­¤é¡¹ç›®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lzw108/FMD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lzw108/FMDæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16452v2">PDF</a> Accepted by The Web Conference (WWW) 2025 Short Paper Track</p>
<p><strong>Summary</strong><br>é‡‘èé¢†åŸŸä¸­ä¿¡æ¯çš„å‡†ç¡®æ€§å¯¹äºå¸‚åœºçš„å„ä¸ªæ–¹é¢éƒ½è‡³å…³é‡è¦ï¼Œå› æ­¤é‡‘èè™šå‡ä¿¡æ¯æ£€æµ‹ï¼ˆFMDï¼‰æ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é—®é¢˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ä¸ªé¢†åŸŸè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†ç°æœ‰ç ”ç©¶å¤§å¤šä¾èµ–äºä¼ ç»Ÿæ–¹æ³•ï¼Œå°šæœªæ¢ç´¢LLMåœ¨FMDé¢†åŸŸçš„åº”ç”¨ã€‚ç¼ºä¹FMDæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†å’Œè¯„ä¼°åŸºå‡†æ˜¯ä¸»è¦åŸå› ã€‚æœ¬æ–‡æå‡ºFMDLlamaï¼ŒåŸºäºæŒ‡ä»¤æ•°æ®å¾®è°ƒLlama3.1ï¼Œæ¨å‡ºé¦–æ¬¾æ”¯æŒLLMæŒ‡ä»¤è°ƒæ•´çš„å¤šä»»åŠ¡FMDæŒ‡ä»¤æ•°æ®é›†FMDIDå’Œå…¨é¢çš„FMDè¯„ä¼°åŸºå‡†FMD-Bï¼ŒåŒ…æ‹¬åˆ†ç±»å’Œè§£é‡Šç”Ÿæˆä»»åŠ¡ï¼Œä»¥æµ‹è¯•LLMçš„FMDèƒ½åŠ›ã€‚åœ¨FMD-Bä¸Šä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼ŒFMDLlamaè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡‘èä¿¡æ¯çš„å‡†ç¡®æ€§å¯¹äºé‡‘èå¸‚åœºè‡³å…³é‡è¦ï¼Œé‡‘èè™šå‡ä¿¡æ¯æ£€æµ‹ï¼ˆFMDï¼‰æ˜¯äºŸå¾…è§£å†³çš„é—®é¢˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ä¸ªé¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å°šæœªåœ¨FMDé¢†åŸŸå¾—åˆ°åº”ç”¨ã€‚</li>
<li>å½“å‰ç ”ç©¶ç¼ºä¹FMDæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†å’Œè¯„ä¼°åŸºå‡†ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†FMDLlamaï¼ŒåŸºäºæŒ‡ä»¤æ•°æ®å¾®è°ƒLlama3.1ï¼Œå¹¶æ¨å‡ºé¦–æ¬¾å¤šä»»åŠ¡FMDæŒ‡ä»¤æ•°æ®é›†FMDIDã€‚</li>
<li>FMDLlamaè¿˜æ¨å‡ºäº†å…¨é¢çš„FMDè¯„ä¼°åŸºå‡†FMD-Bï¼ŒåŒ…æ‹¬åˆ†ç±»å’Œè§£é‡Šç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>FMDLlamaæ¨¡å‹åœ¨FMD-Bä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–å¼€æºLLMsä»¥åŠOpenAIçš„äº§å“ã€‚</li>
<li>è¯¥é¡¹ç›®å·²åœ¨GitHubä¸Šå¼€æºï¼Œå¯ä¾›å…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.16452">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-62b411ccb75ed9de9c3dfa18f513b565.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-216638f86e3ace78d9a8650f98674dba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a5df60212804cba427811be5750904c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7aa2ae25ad42c696c982a275b18a0304.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-05/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-05/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-05/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-7f7e8b593b1b5e007bf2ad4307af4d54.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-05  Multi-agent Multi-armed Bandit with Fully Heavy-tailed Dynamics
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-01/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1e0af654c11e71fe45c47b56f4848cb0.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-01  Free-T2M Frequency Enhanced Text-to-Motion Diffusion Model With   Consistency Loss
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">16905.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
