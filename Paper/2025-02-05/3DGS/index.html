<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2025-02-05  Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven   Surface Normal-aware Tracking and Mapping">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1944b6685006aa1cd5980d9714859fa7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    54 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-05-更新"><a href="#2025-02-05-更新" class="headerlink" title="2025-02-05 更新"></a>2025-02-05 更新</h1><h2 id="Advancing-Dense-Endoscopic-Reconstruction-with-Gaussian-Splatting-driven-Surface-Normal-aware-Tracking-and-Mapping"><a href="#Advancing-Dense-Endoscopic-Reconstruction-with-Gaussian-Splatting-driven-Surface-Normal-aware-Tracking-and-Mapping" class="headerlink" title="Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven   Surface Normal-aware Tracking and Mapping"></a>Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven   Surface Normal-aware Tracking and Mapping</h2><p><strong>Authors:Yiming Huang, Beilei Cui, Long Bai, Zhen Chen, Jinlin Wu, Zhen Li, Hongbin Liu, Hongliang Ren</strong></p>
<p>Simultaneous Localization and Mapping (SLAM) is essential for precise surgical interventions and robotic tasks in minimally invasive procedures. While recent advancements in 3D Gaussian Splatting (3DGS) have improved SLAM with high-quality novel view synthesis and fast rendering, these systems struggle with accurate depth and surface reconstruction due to multi-view inconsistencies. Simply incorporating SLAM and 3DGS leads to mismatches between the reconstructed frames. In this work, we present Endo-2DTAM, a real-time endoscopic SLAM system with 2D Gaussian Splatting (2DGS) to address these challenges. Endo-2DTAM incorporates a surface normal-aware pipeline, which consists of tracking, mapping, and bundle adjustment modules for geometrically accurate reconstruction. Our robust tracking module combines point-to-point and point-to-plane distance metrics, while the mapping module utilizes normal consistency and depth distortion to enhance surface reconstruction quality. We also introduce a pose-consistent strategy for efficient and geometrically coherent keyframe sampling. Extensive experiments on public endoscopic datasets demonstrate that Endo-2DTAM achieves an RMSE of $1.87\pm 0.63$ mm for depth reconstruction of surgical scenes while maintaining computationally efficient tracking, high-quality visual appearance, and real-time rendering. Our code will be released at github.com&#x2F;lastbasket&#x2F;Endo-2DTAM. </p>
<blockquote>
<p>同步定位与地图构建（SLAM）在微创手术中的精确手术干预和机器人任务中至关重要。虽然最近在3D高斯喷射（3DGS）方面的最新进展已通过高质量的新型视图合成和快速渲染功能改进了SLAM，但这些系统在深度与表面重建方面仍存在准确性问题，这是由于多视图的不一致性所导致的。仅仅将SLAM和3DGS相结合会导致重建帧之间的不匹配。在这项工作中，我们提出了Endo-2DTAM，这是一种具有实时功能的内窥镜SLAM系统，结合了二维高斯喷射（2DGS）来解决这些挑战。Endo-2DTAM采用了一种表面法线感知管道，该管道包含跟踪、映射和捆绑调整模块，用于实现几何准确的重建。我们的稳健跟踪模块结合了点到点和点到平面的距离度量，而映射模块则利用法线一致性和深度失真来增强表面重建质量。我们还介绍了一种姿态一致的策略，以实现高效且几何连贯的关键帧采样。在公共内窥镜数据集上的大量实验表明，Endo-2DTAM实现了手术场景深度重建的RMSE为$ 1.87±0.63$毫米，同时保持计算高效的跟踪、高质量视觉效果和实时渲染。我们的代码将在github.com&#x2F;lastbasket&#x2F;Endo-2DTAM上发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19319v1">PDF</a> Accepted by ICRA 2025</p>
<p><strong>Summary</strong></p>
<p>基于实时内镜SLAM系统的Endo-2DTAM，结合了三维高斯分裂（3DGS）技术，解决了在微创手术中的精确定位和映射问题。通过引入表面正常感知管道，实现了几何准确的重建。通过稳健的跟踪模块和映射模块的结合，提高了表面重建质量，同时引入姿态一致性策略以实现高效且几何连贯的关键帧采样。在公共内镜数据集上的实验表明，Endo-2DTAM实现了深度重建的均方根误差（RMSE）为±mm 1.87，同时具备高效计算跟踪、高质量视觉外观和实时渲染功能。代码将在github.com&#x2F;lastbasket&#x2F;Endo-2DTAM发布。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是七个关键要点：</p>
<ol>
<li>Endo-2DTAM解决了Simultaneous Localization and Mapping (SLAM)在微创手术中的精准定位和地图构建问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19319">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f48fce362fed91ed1414e54fd62a95b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a1598d698c4e33baa6e79559d6b74e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-394d5571ecd0a9abe03d7ec5ff7d0372.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-949608df5f5967fa2e3ecd8c98ea1e3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9b3032c5dc928784b6b4de2d2be7cdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e266669db15fb9a9c050fb3bbe7441c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="JGHand-Joint-Driven-Animatable-Hand-Avater-via-3D-Gaussian-Splatting"><a href="#JGHand-Joint-Driven-Animatable-Hand-Avater-via-3D-Gaussian-Splatting" class="headerlink" title="JGHand: Joint-Driven Animatable Hand Avater via 3D Gaussian Splatting"></a>JGHand: Joint-Driven Animatable Hand Avater via 3D Gaussian Splatting</h2><p><strong>Authors:Zhoutao Sun, Xukun Shen, Yong Hu, Yuyou Zhong, Xueyang Zhou</strong></p>
<p>Since hands are the primary interface in daily interactions, modeling high-quality digital human hands and rendering realistic images is a critical research problem. Furthermore, considering the requirements of interactive and rendering applications, it is essential to achieve real-time rendering and driveability of the digital model without compromising rendering quality. Thus, we propose Jointly 3D Gaussian Hand (JGHand), a novel joint-driven 3D Gaussian Splatting (3DGS)-based hand representation that renders high-fidelity hand images in real-time for various poses and characters. Distinct from existing articulated neural rendering techniques, we introduce a differentiable process for spatial transformations based on 3D key points. This process supports deformations from the canonical template to a mesh with arbitrary bone lengths and poses. Additionally, we propose a real-time shadow simulation method based on per-pixel depth to simulate self-occlusion shadows caused by finger movements. Finally, we embed the hand prior and propose an animatable 3DGS representation of the hand driven solely by 3D key points. We validate the effectiveness of each component of our approach through comprehensive ablation studies. Experimental results on public datasets demonstrate that JGHand achieves real-time rendering speeds with enhanced quality, surpassing state-of-the-art methods. </p>
<blockquote>
<p>由于手是日常交互中的主要界面，因此建立高质量的数字人手模型并呈现逼真的图像是一个关键的研究问题。此外，考虑到交互和渲染应用的要求，必须在不损害渲染质量的前提下实现数字模型的实时渲染和可驱动性。因此，我们提出了Jointly 3D Gaussian Hand（JGHand），这是一种基于关节驱动的新型3D高斯展平（3DGS）手部表示方法，可实时呈现各种姿势和角色的高保真手部图像。与现有的关节神经渲染技术不同，我们引入了一种基于3D关键点的空间转换的可微过程。这个过程支持从标准模板到具有任意骨骼长度和姿势的网格的变形。此外，我们提出了一种基于像素深度的实时阴影模拟方法，以模拟由手指移动引起的自遮挡阴影。最后，我们嵌入手部先验知识，并提出一种仅由3D关键点驱动的可动画3DGS手部表示方法。我们通过全面的消融研究验证了我们的方法每个组件的有效性。在公共数据集上的实验结果表明，JGHand实现了高质量的实时渲染速度，超越了最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19088v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于3D高斯拼贴（3DGS）的联合3D手部模型（JGHand），可实时渲染高质量的手部图像。该模型支持多种姿势和角色的手部模拟，具有可驱动性和实时渲染能力。通过引入基于3D关键点的可微分空间变换过程，支持从标准模板到具有任意骨长度和姿势的网格的变形。同时，还提出了基于像素深度的实时阴影模拟方法，模拟手指移动产生的自遮挡阴影。最后，通过嵌入手部先验知识，提出一种仅由3D关键点驱动的可动画3DGS手部表示。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了基于3D高斯拼贴（3DGS）的联合3D手部模型（JGHand），用于高质量手部图像的实时渲染。</li>
<li>支持多种姿势和角色的手部模拟，具有可驱动性。</li>
<li>引入基于3D关键点的可微分空间变换过程，实现从标准模板到任意骨长度和姿势的网格的变形。</li>
<li>提出了实时阴影模拟方法，基于像素深度模拟手指移动产生的自遮挡阴影。</li>
<li>嵌入手部先验知识，提出一种仅由3D关键点驱动的可动画3D手部表示。</li>
<li>通过综合的消融研究验证了方法的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19088">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d4d07a02a25428bfd660d4d657a554a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bffd2aca5d32563b99760ca1f054a2bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b63e751f2bd7a466dfba397fb0e8898.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c43df6073c10bd3ace1216fde0169e1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09a525592829c3099c327d561bc10e5d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="OmniPhysGS-3D-Constitutive-Gaussians-for-General-Physics-Based-Dynamics-Generation"><a href="#OmniPhysGS-3D-Constitutive-Gaussians-for-General-Physics-Based-Dynamics-Generation" class="headerlink" title="OmniPhysGS: 3D Constitutive Gaussians for General Physics-Based Dynamics   Generation"></a>OmniPhysGS: 3D Constitutive Gaussians for General Physics-Based Dynamics   Generation</h2><p><strong>Authors:Yuchen Lin, Chenguo Lin, Jianjin Xu, Yadong Mu</strong></p>
<p>Recently, significant advancements have been made in the reconstruction and generation of 3D assets, including static cases and those with physical interactions. To recover the physical properties of 3D assets, existing methods typically assume that all materials belong to a specific predefined category (e.g., elasticity). However, such assumptions ignore the complex composition of multiple heterogeneous objects in real scenarios and tend to render less physically plausible animation given a wider range of objects. We propose OmniPhysGS for synthesizing a physics-based 3D dynamic scene composed of more general objects. A key design of OmniPhysGS is treating each 3D asset as a collection of constitutive 3D Gaussians. For each Gaussian, its physical material is represented by an ensemble of 12 physical domain-expert sub-models (rubber, metal, honey, water, etc.), which greatly enhances the flexibility of the proposed model. In the implementation, we define a scene by user-specified prompts and supervise the estimation of material weighting factors via a pretrained video diffusion model. Comprehensive experiments demonstrate that OmniPhysGS achieves more general and realistic physical dynamics across a broader spectrum of materials, including elastic, viscoelastic, plastic, and fluid substances, as well as interactions between different materials. Our method surpasses existing methods by approximately 3% to 16% in metrics of visual quality and text alignment. </p>
<blockquote>
<p>近期，3D资产的重建和生成方面取得了重大进展，包括静态案例和具有物理交互的案例。为了恢复3D资产的物理特性，现有方法通常假设所有材料都属于特定的预定义类别（例如，弹性）。然而，这样的假设忽略了真实场景中多个异质对象的复杂组合，并且在面对更广泛的对象时，往往呈现出物理上不太可能的动画。我们提出OmniPhysGS来合成由更一般对象组成的基于物理的3D动态场景。OmniPhysGS的一个关键设计是将每个3D资产视为一组构成的三维高斯分布。对于每个高斯分布，其物理材料由一组由12个物理领域专家子模型（橡胶、金属、蜂蜜、水等）组成，这极大地增强了所提出模型的灵活性。在实现中，我们通过用户指定的提示来定义场景，并通过预训练的视频扩散模型来监督材料权重因子的估计。综合实验表明，OmniPhysGS在更广泛的材料范围内实现了更通用和更现实的物理动态，包括弹性、粘弹性、塑料和流体物质，以及不同材料之间的交互。我们的方法在视觉质量和文本对齐的指标上比现有方法高出约3%到16%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18982v1">PDF</a> Accepted to ICLR 2025; Project page:   <a target="_blank" rel="noopener" href="https://wgsxm.github.io/projects/omniphysgs/">https://wgsxm.github.io/projects/omniphysgs/</a></p>
<p><strong>Summary</strong></p>
<p>该研究提出一种名为OmniPhysGS的基于物理的3D动态场景合成方法，用于生成包含更通用对象的物理场景。该方法将每个3D资产视为一系列三维高斯分布的集合，并为每个高斯分布的物理材质采用一组由物理领域专家提供的子模型表示。通过用户指定的提示和监督预训练视频扩散模型来估计材料权重因子。OmniPhysGS能够生成更为广泛且逼真的物理动态效果，涵盖弹性、粘性弹性、塑性以及流体等多种材质，并实现不同材质间的交互。相较于现有方法，OmniPhysGS在视觉质量和文本对齐方面的指标有所提升，约提升3%至16%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该研究解决了现有方法在重建和生成具有物理交互的3D资产时的局限性，尤其是关于材料类别假设的问题。不同于现有的依赖于预先定义的材料类别的方法，它更注重物体的复杂性和异质性。</li>
<li>研究提出了OmniPhysGS方法，通过用户指定的提示来定义场景，并使用预训练的视频扩散模型来估计材料权重因子。这使得场景的合成更为灵活且高效。</li>
<li>OmniPhysGS能够实现更广泛和逼真的物理动态效果，包括弹性、粘性弹性、塑性以及流体等不同材质的交互效果。这意味着它能够模拟更真实的物理世界场景。</li>
<li>该方法能够在多种材质之间实现交互效果，这在现有的研究中是较为罕见的。这种能力使得生成的场景更加多样化和复杂。</li>
<li>与现有方法相比，OmniPhysGS在视觉质量和文本对齐方面的性能有所提升。这表明它在生成高质量和真实感的场景方面表现出优势。</li>
<li>OmniPhysGS的设计将每个3D资产视为一系列三维高斯分布的集合，这为模型的灵活性提供了重要的支持。此外，它为每个高斯分布的物理材质采用一组由物理领域专家提供的子模型表示，这增强了模型的物理准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18982">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-effbef9d36c428b3e651ca975df2757d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea89d9851371fc71e38c9215f85ff859.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f86ef81e87d5a7cd2be7ae12669aff0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-51baf255595840e042c00bceaf303574.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Novel-View-and-Depth-Synthesis-with-Multi-View-Geometric-Diffusion"><a href="#Zero-Shot-Novel-View-and-Depth-Synthesis-with-Multi-View-Geometric-Diffusion" class="headerlink" title="Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric   Diffusion"></a>Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric   Diffusion</h2><p><strong>Authors:Vitor Guizilini, Muhammad Zubair Irshad, Dian Chen, Greg Shakhnarovich, Rares Ambrus</strong></p>
<p>Current methods for 3D scene reconstruction from sparse posed images employ intermediate 3D representations such as neural fields, voxel grids, or 3D Gaussians, to achieve multi-view consistent scene appearance and geometry. In this paper we introduce MVGD, a diffusion-based architecture capable of direct pixel-level generation of images and depth maps from novel viewpoints, given an arbitrary number of input views. Our method uses raymap conditioning to both augment visual features with spatial information from different viewpoints, as well as to guide the generation of images and depth maps from novel views. A key aspect of our approach is the multi-task generation of images and depth maps, using learnable task embeddings to guide the diffusion process towards specific modalities. We train this model on a collection of more than 60 million multi-view samples from publicly available datasets, and propose techniques to enable efficient and consistent learning in such diverse conditions. We also propose a novel strategy that enables the efficient training of larger models by incrementally fine-tuning smaller ones, with promising scaling behavior. Through extensive experiments, we report state-of-the-art results in multiple novel view synthesis benchmarks, as well as multi-view stereo and video depth estimation. </p>
<blockquote>
<p>当前用于从稀疏定位图像重建3D场景的方法采用中间3D表示，如神经场、体素网格或3D高斯，以实现多视角一致的场景外观和几何。在本文中，我们介绍了MVGD，这是一种基于扩散的架构，能够从任意数量的输入视角直接生成像素级的图像和深度图。我们的方法使用射线图条件来增强视觉特征，并加入从不同视角获取的空间信息，以指导从新颖视角生成图像和深度图。我们方法的关键方面在于图像和深度图的多任务生成，利用可学习的任务嵌入来引导扩散过程朝着特定的模式进行。我们在来自公开数据集的超过6000万个多视角样本上训练此模型，并提出了一些技术，以实现在这种多样条件下的高效且一致的学习。我们还提出了一种新的策略，通过逐步微调较小的模型来有效地训练更大的模型，具有良好的扩展性。通过广泛的实验，我们在多个新颖视图合成基准测试中取得了最新结果，以及在多视角立体和视频深度估计方面也取得了成果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18804v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://mvgd.github.io/">https://mvgd.github.io</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于扩散架构的MVGD方法，能够直接从多个视角的图像和深度图中生成像素级的多视角一致场景。该方法使用射线映射条件增强视觉特征，并引导从新颖视角生成图像和深度图。通过多任务生成图像和深度图，利用可学习的任务嵌入引导扩散过程针对特定模态。实验结果表明，该方法在多视角合成、立体视频和多视角深度估计等多个基准测试中达到最佳状态。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MVGD是一种基于扩散架构的方法，可直接从稀疏定位的图像生成像素级的多视角一致场景。</li>
<li>使用射线映射条件同时增强视觉特征和空间信息，以生成多视角图像和深度图。</li>
<li>多任务生成图像和深度图，利用可学习的任务嵌入引导扩散过程针对特定模态，实现更精细的生成。</li>
<li>在超过6000万多个多视角样本上进行训练，提出在多样条件下实现高效一致学习的技术。</li>
<li>提出一种有效策略，通过逐步微调较小的模型来训练更大的模型，具有良好的扩展性。</li>
<li>在多个基准测试中达到最佳性能，包括多视角合成、立体视频和多视角深度估计等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18804">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5b57ed5796d8a2c5f8dfb70d60bced88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3da6044c0d0b33ac16b98b1803d185d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b7fd0de60e361343da9df3d17decc7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-acb4d07c8e881f7b853da13e46efa796.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Drag-Your-Gaussian-Effective-Drag-Based-Editing-with-Score-Distillation-for-3D-Gaussian-Splatting"><a href="#Drag-Your-Gaussian-Effective-Drag-Based-Editing-with-Score-Distillation-for-3D-Gaussian-Splatting" class="headerlink" title="Drag Your Gaussian: Effective Drag-Based Editing with Score Distillation   for 3D Gaussian Splatting"></a>Drag Your Gaussian: Effective Drag-Based Editing with Score Distillation   for 3D Gaussian Splatting</h2><p><strong>Authors:Yansong Qu, Dian Chen, Xinyang Li, Xiaofan Li, Shengchuan Zhang, Liujuan Cao, Rongrong Ji</strong></p>
<p>Recent advancements in 3D scene editing have been propelled by the rapid development of generative models. Existing methods typically utilize generative models to perform text-guided editing on 3D representations, such as 3D Gaussian Splatting (3DGS). However, these methods are often limited to texture modifications and fail when addressing geometric changes, such as editing a character’s head to turn around. Moreover, such methods lack accurate control over the spatial position of editing results, as language struggles to precisely describe the extent of edits. To overcome these limitations, we introduce DYG, an effective 3D drag-based editing method for 3D Gaussian Splatting. It enables users to conveniently specify the desired editing region and the desired dragging direction through the input of 3D masks and pairs of control points, thereby enabling precise control over the extent of editing. DYG integrates the strengths of the implicit triplane representation to establish the geometric scaffold of the editing results, effectively overcoming suboptimal editing outcomes caused by the sparsity of 3DGS in the desired editing regions. Additionally, we incorporate a drag-based Latent Diffusion Model into our method through the proposed Drag-SDS loss function, enabling flexible, multi-view consistent, and fine-grained editing. Extensive experiments demonstrate that DYG conducts effective drag-based editing guided by control point prompts, surpassing other baselines in terms of editing effect and quality, both qualitatively and quantitatively. Visit our project page at <a target="_blank" rel="noopener" href="https://quyans.github.io/Drag-Your-Gaussian">https://quyans.github.io/Drag-Your-Gaussian</a>. </p>
<blockquote>
<p>最近的三维场景编辑进展得益于生成模型的快速发展。现有方法通常利用生成模型对三维表示进行文本引导编辑，例如三维高斯贴图（3DGS）。然而，这些方法通常仅限于纹理修改，在应对几何变化时往往会失效，例如编辑角色头部以使其旋转。此外，这些方法在控制编辑结果的空间位置方面不够精确，因为语言很难精确描述编辑的程度。为了克服这些限制，我们引入了DYG，这是一种针对三维高斯贴图的有效基于拖拽的三维编辑方法。它使用户可以通过输入三维遮罩和控制点对，方便地指定所需的编辑区域和拖拽方向，从而实现对编辑程度的精确控制。DYG结合了隐式triplane表示的优点，建立编辑结果的三维骨架，有效克服了在所需编辑区域中3DGS稀疏导致的次优编辑结果。此外，我们通过提出的Drag-SDS损失函数，将基于拖拽的潜在扩散模型融入我们的方法，实现灵活、多视角一致、精细的编辑。大量实验表明，DYG通过控制点提示进行有效的基于拖拽的编辑，在编辑效果和品质方面超越其他基线方法，定性和定量评估均如此。请访问我们的项目页面：<a target="_blank" rel="noopener" href="https://quyans.github.io/Drag-Your-Gaussian">https://quyans.github.io/Drag-Your-Gaussian</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18672v1">PDF</a> Visit our project page at <a target="_blank" rel="noopener" href="https://quyans.github.io/Drag-Your-Gaussian">https://quyans.github.io/Drag-Your-Gaussian</a></p>
<p><strong>摘要</strong></p>
<p>最近，三维场景编辑技术的进展得益于生成模型的快速发展。现有方法通常利用生成模型对三维表示进行文本引导编辑，如三维高斯喷溅（3DGS）。然而，这些方法主要局限于纹理修改，在几何变化方面表现不足，如角色头部的旋转编辑。此外，由于缺乏精确的空间位置控制，语言难以精确描述编辑的程度。为了克服这些限制，我们提出了基于拖放的3D编辑方法DYG，用于3D高斯喷溅。它使用户可以通过输入3D遮罩和控制点对来方便地指定所需的编辑区域和拖动方向，从而实现精确的编辑程度控制。DYG结合隐式triplane表示建立编辑结果几何框架的优势，有效克服了3DGS在所需编辑区域稀疏导致的次优编辑结果。此外，我们通过在提出的Drag-SDS损失函数中融入基于拖放的潜在扩散模型，实现了灵活、多视角一致、精细的编辑。大量实验表明，DYG通过控制点提示进行有效的拖放编辑，在编辑效果和品质上超越了其他基线方法。请访问我们的项目页面<a target="_blank" rel="noopener" href="https://quyans.github.io/Drag-Your-Gaussian%E4%BA%86%E8%A7%A3%E6%9B%B4%E5%A4%9A%E3%80%82">https://quyans.github.io/Drag-Your-Gaussian了解更多。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>生成模型推动3D场景编辑技术的新进展。</li>
<li>现有方法主要局限于纹理修改，在几何变化方面存在局限。</li>
<li>提出基于拖放的3D编辑方法DYG，用于3D高斯喷溅。</li>
<li>DYG允许用户通过3D遮罩和控制点方便指定编辑区域和拖动方向。</li>
<li>DYG结合隐式triplane表示实现精确几何编辑结果。</li>
<li>融入基于拖放的潜在扩散模型，实现灵活、多视角一致、精细的编辑。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18672">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9ca5d4a8403e81b29eddf6a9fb0bd4f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32ff45e272d6e24c63eeb2e76f5ba032.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8417e6fbf6e3b5bf38b6c11633406bd2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c3cf313e5a992e9cedcf7def299ceaa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8ab2954a178f94aad6981a71eb7d85a6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="3D-Reconstruction-of-Shoes-for-Augmented-Reality"><a href="#3D-Reconstruction-of-Shoes-for-Augmented-Reality" class="headerlink" title="3D Reconstruction of Shoes for Augmented Reality"></a>3D Reconstruction of Shoes for Augmented Reality</h2><p><strong>Authors:Pratik Shrestha, Sujan Kapali, Swikar Gautam, Vishal Pokharel, Santosh Giri</strong></p>
<p>This paper introduces a mobile-based solution that enhances online shoe shopping through 3D modeling and Augmented Reality (AR), leveraging the efficiency of 3D Gaussian Splatting. Addressing the limitations of static 2D images, the framework generates realistic 3D shoe models from 2D images, achieving an average Peak Signal-to-Noise Ratio (PSNR) of 0.32, and enables immersive AR interactions via smartphones. A custom shoe segmentation dataset of 3120 images was created, with the best-performing segmentation model achieving an Intersection over Union (IoU) score of 0.95. This paper demonstrates the potential of 3D modeling and AR to revolutionize online shopping by offering realistic virtual interactions, with applicability across broader fashion categories. </p>
<blockquote>
<p>本文介绍了一种基于移动设备的解决方案，通过3D建模和增强现实（AR）技术提高网上购鞋体验，利用3D高斯拼贴技术的效率。该框架解决了静态2D图像的限制，能够从2D图像生成逼真的3D鞋模，平均峰值信噪比（PSNR）达到0.32，并通过智能手机实现沉浸式AR交互。创建了一个包含3120张图片的自定义鞋类分割数据集，最佳分割模型达到交并比（IoU）分数为0.95。本文展示了3D建模和AR技术在在线购物中的潜力，通过提供逼真的虚拟交互，可广泛应用于更广泛的时尚类别。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18643v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于移动设备的解决方案，通过3D建模和增强现实（AR）技术提升在线购鞋体验。该方案利用3D高斯喷绘技术，克服静态2D图像的局限性，通过2D图像生成逼真的3D鞋模，平均峰值信噪比（PSNR）达到0.32，并通过智能手机实现沉浸式AR交互。研究创建了一个包含3120张图片的自定义鞋类分割数据集，最佳分割模型的交并比（IoU）分数达到0.95。本文展示了3D建模和AR技术在在线购物中的潜力，通过提供逼真的虚拟交互，有望改变消费者的购物体验，并且具有更广泛的应用于时尚领域。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用移动设备和3D建模技术提升在线购鞋体验。</li>
<li>通过3D高斯喷绘技术从2D图像生成3D鞋模。</li>
<li>平均峰值信噪比（PSNR）达到0.32，保证模型的真实性。</li>
<li>实现沉浸式AR交互，增强购物体验。</li>
<li>创建了一个自定义鞋类分割数据集，用于训练和测试模型。</li>
<li>最佳分割模型的交并比（IoU）分数达到0.95，显示模型的良好性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18643">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7f80aff134a12258b6af3660047ef3d8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3fd98da88995ca83a94a251521fc189d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c3e047902492058b7a0e262e2ff6b60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c0c3ecf1c36e2188806bd20b23a7b13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59db83c7f971b04f64bf2a17564fa41b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-59aa11ff5e186429c7303d86208fc15f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb369b522b3ff20e02bdc1d2721f7468.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ea5c7edae7ca812e1e3fb50e2da9b21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6a001b367af19504e9e565d64e94188.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-510e875372ebff529268b43f0de5f496.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Deformable-Beta-Splatting"><a href="#Deformable-Beta-Splatting" class="headerlink" title="Deformable Beta Splatting"></a>Deformable Beta Splatting</h2><p><strong>Authors:Rong Liu, Dylan Sun, Meida Chen, Yue Wang, Andrew Feng</strong></p>
<p>3D Gaussian Splatting (3DGS) has advanced radiance field reconstruction by enabling real-time rendering. However, its reliance on Gaussian kernels for geometry and low-order Spherical Harmonics (SH) for color encoding limits its ability to capture complex geometries and diverse colors. We introduce Deformable Beta Splatting (DBS), a deformable and compact approach that enhances both geometry and color representation. DBS replaces Gaussian kernels with deformable Beta Kernels, which offer bounded support and adaptive frequency control to capture fine geometric details with higher fidelity while achieving better memory efficiency. In addition, we extended the Beta Kernel to color encoding, which facilitates improved representation of diffuse and specular components, yielding superior results compared to SH-based methods. Furthermore, Unlike prior densification techniques that depend on Gaussian properties, we mathematically prove that adjusting regularized opacity alone ensures distribution-preserved Markov chain Monte Carlo (MCMC), independent of the splatting kernel type. Experimental results demonstrate that DBS achieves state-of-the-art visual quality while utilizing only 45% of the parameters and rendering 1.5x faster than 3DGS-based methods. Notably, for the first time, splatting-based methods outperform state-of-the-art Neural Radiance Fields, highlighting the superior performance and efficiency of DBS for real-time radiance field rendering. </p>
<blockquote>
<p>3D Gaussian Splatting（3DGS）通过实现实时渲染，推动了辐射场重建的发展。然而，它依赖于几何高斯核和低阶球面谐波（SH）进行颜色编码，这限制了其捕捉复杂几何和多样颜色的能力。我们引入了可变形Beta Splatting（DBS），这是一种可变形且紧凑的方法，能够增强几何和颜色的表示。DBS用可变形的Beta核替换高斯核，提供有界支持和自适应频率控制，以更高的保真度捕捉精细的几何细节，同时实现更好的内存效率。此外，我们将Beta核扩展到颜色编码，这有助于改进漫反射和镜面成分的表示，与基于SH的方法相比，产生更优越的结果。此外，与以往依赖高斯属性的密集化技术不同，我们从数学上证明，仅调整正则化不透明度就能确保分布保留的马尔可夫链蒙特卡罗（MCMC）方法独立于Splatting核类型。实验结果表明，DBS达到了最先进的视觉质量，同时仅使用3DGS方法的45%参数，并且渲染速度提高了1.5倍。值得注意的是，基于Splatting的方法首次超越了最先进的神经辐射场技术，突显了DBS在实时辐射场渲染方面的卓越性能和效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18630v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>3DGS在实时渲染中推动了辐射场重建的发展，但受限于高斯核和低阶球面谐波的颜色编码。现在引入可变形Beta Splatting（DBS），提高了几何和颜色表示的精度和效率。DBS使用可变形Beta核替代高斯核，支持有界支持和自适应频率控制，更准确地捕捉精细几何细节，同时实现更好的内存效率。此外，将Beta核扩展到颜色编码，改进了漫反射和镜面成分表示，获得优于基于SH的方法的结果。实验证明，DBS实现了卓越的可视化质量，仅使用45%的参数，渲染速度比基于3DGS的方法快1.5倍。重要的是，基于劈绘的方法首次超越了最先进的神经辐射场，突显了DBS在实时辐射场渲染中的卓越性能和效率。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3DGS推动了实时渲染中的辐射场重建，但存在对复杂几何和多彩色的捕捉限制。</li>
<li>引入DBS，一种可变形和紧凑的方法，增强几何和颜色表示。</li>
<li>DBS使用可变形Beta核替代高斯核，更好地捕捉精细几何细节，提高内存效率。</li>
<li>扩展Beta核用于颜色编码，改进漫反射和镜面成分表示，优于球面谐波方法。</li>
<li>实验证明DBS在可视化质量、参数使用和渲染速度上优于3DGS。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18630">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6880b8f0f58d66ce069be7c92ea604da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f094f9e65c2f344f91bc7eaeffa13ccf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0b565469839fdd8b78fa199d6fa4893c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ec2c6c2feb197d05515bc4e19ad8cab.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="VoD-3DGS-View-opacity-Dependent-3D-Gaussian-Splatting"><a href="#VoD-3DGS-View-opacity-Dependent-3D-Gaussian-Splatting" class="headerlink" title="VoD-3DGS: View-opacity-Dependent 3D Gaussian Splatting"></a>VoD-3DGS: View-opacity-Dependent 3D Gaussian Splatting</h2><p><strong>Authors:Mateusz Nowak, Wojciech Jarosz, Peter Chin</strong></p>
<p>Reconstructing a 3D scene from images is challenging due to the different ways light interacts with surfaces depending on the viewer’s position and the surface’s material. In classical computer graphics, materials can be classified as diffuse or specular, interacting with light differently. The standard 3D Gaussian Splatting model struggles to represent view-dependent content, since it cannot differentiate an object within the scene from the light interacting with its specular surfaces, which produce highlights or reflections. In this paper, we propose to extend the 3D Gaussian Splatting model by introducing an additional symmetric matrix to enhance the opacity representation of each 3D Gaussian. This improvement allows certain Gaussians to be suppressed based on the viewer’s perspective, resulting in a more accurate representation of view-dependent reflections and specular highlights without compromising the scene’s integrity. By allowing the opacity to be view dependent, our enhanced model achieves state-of-the-art performance on Mip-Nerf, Tanks&amp;Temples, Deep Blending, and Nerf-Synthetic datasets without a significant loss in rendering speed, achieving &gt;60FPS, and only incurring a minimal increase in memory used. </p>
<blockquote>
<p>从图像重建3D场景是一个具有挑战性的任务，因为光线与表面互动的方式取决于观察者的位置和表面材质。在经典计算机图形学中，材料可分为漫反射或镜面反射，与光线的互动方式不同。标准的3D高斯拼贴模型在表示视角相关内容时遇到困难，因为它无法区分场景中的物体与其与镜面表面交互的光线，从而产生高光或反射。在本文中，我们提出通过引入一个额外的对称矩阵来扩展3D高斯拼贴模型，以增强每个3D高斯的不透明度表示。这一改进允许根据观察者的角度抑制某些高斯，从而更准确地表示视角相关的反射和高光，同时不损害场景的完整性。通过允许不透明度依赖于视角，我们增强的模型在Mip-Nerf、Tanks&amp;Temples、Deep Blending和Nerf-Synthetic数据集上实现了最先进的性能，渲染速度没有明显损失，达到&gt;60FPS，并且只增加了很少的内存使用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.17978v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出扩展3D高斯散斑模型，通过引入额外的对称矩阵以增强每个3D高斯的不透明度表示。改进允许根据观察者的视角抑制某些高斯，从而更准确地表示视差相关的反射和高光，同时保持场景的完整性。增强模型在不显著降低渲染速度（达到&gt;60FPS）和仅增加极小内存使用的情况下，实现了Mip-Nerf、Tanks&amp;Temples、Deep Blending和Nerf-Synthetic数据集上的业界领先性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入对称矩阵扩展了3D高斯散斑模型。</li>
<li>增强模型能更准确地表示视差相关的反射和高光。</li>
<li>改进允许根据观察者的视角抑制某些高斯。</li>
<li>增强模型在不牺牲场景完整性的情况下实现上述效果。</li>
<li>渲染速度保持&gt;60FPS，且内存使用仅略有增加。</li>
<li>扩展模型在多个数据集上实现了业界领先的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.17978">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-80725c323b58b19c5a35247f2f868652.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d4ac030b4db062cb0412a2ae14f03d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a28ed0a5f6f1565c7952763c1610bb2.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DWTNeRF-Boosting-Few-shot-Neural-Radiance-Fields-via-Discrete-Wavelet-Transform"><a href="#DWTNeRF-Boosting-Few-shot-Neural-Radiance-Fields-via-Discrete-Wavelet-Transform" class="headerlink" title="DWTNeRF: Boosting Few-shot Neural Radiance Fields via Discrete Wavelet   Transform"></a>DWTNeRF: Boosting Few-shot Neural Radiance Fields via Discrete Wavelet   Transform</h2><p><strong>Authors:Hung Nguyen, Blark Runfa Li, Truong Nguyen</strong></p>
<p>Neural Radiance Fields (NeRF) has achieved superior performance in novel view synthesis and 3D scene representation, but its practical applications are hindered by slow convergence and reliance on dense training views. To this end, we present DWTNeRF, a unified framework based on Instant-NGP’s fast-training hash encoding. It is coupled with regularization terms designed for few-shot NeRF, which operates on sparse training views. Our DWTNeRF additionally includes a novel Discrete Wavelet loss that allows explicit prioritization of low frequencies directly in the training objective, reducing few-shot NeRF’s overfitting on high frequencies in earlier training stages. We also introduce a model-based approach, based on multi-head attention, that is compatible with INGP, which are sensitive to architectural changes. On the 3-shot LLFF benchmark, DWTNeRF outperforms Vanilla INGP by 15.07% in PSNR, 24.45% in SSIM and 36.30% in LPIPS. Our approach encourages a re-thinking of current few-shot approaches for fast-converging implicit representations like INGP or 3DGS. </p>
<blockquote>
<p>神经辐射场（NeRF）在新型视图合成和3D场景表示方面取得了卓越的性能，但其实际应用受到了收敛速度慢和依赖密集训练视图的影响。为此，我们提出了DWTNeRF，这是一个基于Instant-NGP快速训练哈希编码的统一框架。它与针对少射NeRF设计的正则化术语相结合，在稀疏训练视图上运行。我们的DWTNeRF还包括一种新型离散小波损失，允许在训练目标中直接明确优先处理低频，从而减少早期训练阶段中少射NeRF对高频的过拟合。我们还介绍了一种基于多头注意力机制的模型方法，该方法与INGP兼容，对架构变化敏感。在3次拍摄的LLFF基准测试中，DWTNeRF在PSNR上较Vanilla INGP高出15.07%，在SSIM上高出24.45%，在LPIPS上高出36.30%。我们的方法鼓励对现有的少射方法进行重新思考，以寻找如INGP或3DGS这样的快速收敛隐式表示方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12637v2">PDF</a> 17 pages, 13 figures, 8 tables</p>
<p><strong>Summary</strong></p>
<p>NeRF技术在新型视图合成和3D场景表示方面表现出卓越性能，但其实际应用中存在收敛速度慢和依赖密集训练视图的问题。为此，提出了DWTNeRF统一框架，它基于Instant-NGP的快速训练哈希编码，并设计了针对少量射击NeRF的正则化项，可在稀疏训练视图上运行。DWTNeRF还包括一种新型离散小波损失，允许直接在训练目标中明确优先处理低频信息，从而减少早期训练阶段中少量射击NeRF对高频信息的过度拟合。在3次射击LLFF基准测试上，DWTNeRF较Vanilla INGP在PSNR上提高了15.07%，在SSIM上提高了24.45%，在LPIPS上提高了36.30%。我们的方法鼓励对当前少量射击的快速收敛隐式表示方法（如INGP或3DGS）进行重新审视。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRF技术在视图合成和3D场景表示方面表现出卓越性能，但存在收敛慢和依赖密集训练视图的问题。</li>
<li>DWTNeRF框架基于Instant-NGP的快速训练哈希编码，能在稀疏训练视图上运行。</li>
<li>DWTNeRF引入了一种新型离散小波损失，能优先处理低频信息，减少过度拟合。</li>
<li>在基准测试中，DWTNeRF较原有方法有明显性能提升。</li>
<li>DWTNeRF方法对于隐式表示的快速收敛具有启示意义。</li>
<li>DWTNeRF方法为少量射击情况下的NeRF技术提供了新的思考方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12637">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-322630a2fc4fa86cd69f3f8b10e5c9b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1944b6685006aa1cd5980d9714859fa7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc499b0fb467fba02fd43e2a14a45bb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-776a01eb9971f1504f3874ddaffc5062.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1448d40ba7d1e67623aadd084d03038.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CityLoc-6DoF-Pose-Distributional-Localization-for-Text-Descriptions-in-Large-Scale-Scenes-with-Gaussian-Representation"><a href="#CityLoc-6DoF-Pose-Distributional-Localization-for-Text-Descriptions-in-Large-Scale-Scenes-with-Gaussian-Representation" class="headerlink" title="CityLoc: 6DoF Pose Distributional Localization for Text Descriptions in   Large-Scale Scenes with Gaussian Representation"></a>CityLoc: 6DoF Pose Distributional Localization for Text Descriptions in   Large-Scale Scenes with Gaussian Representation</h2><p><strong>Authors:Qi Ma, Runyi Yang, Bin Ren, Nicu Sebe, Ender Konukoglu, Luc Van Gool, Danda Pani Paudel</strong></p>
<p>Localizing textual descriptions within large-scale 3D scenes presents inherent ambiguities, such as identifying all traffic lights in a city. Addressing this, we introduce a method to generate distributions of camera poses conditioned on textual descriptions, facilitating robust reasoning for broadly defined concepts.   Our approach employs a diffusion-based architecture to refine noisy 6DoF camera poses towards plausible locations, with conditional signals derived from pre-trained text encoders. Integration with the pretrained Vision-Language Model, CLIP, establishes a strong linkage between text descriptions and pose distributions. Enhancement of localization accuracy is achieved by rendering candidate poses using 3D Gaussian splatting, which corrects misaligned samples through visual reasoning.   We validate our method’s superiority by comparing it against standard distribution estimation methods across five large-scale datasets, demonstrating consistent outperformance. Code, datasets and more information will be publicly available at our project page. </p>
<blockquote>
<p>在大规模三维场景中对文本描述进行定位存在固有的模糊性，例如识别城市中的所有交通灯。为解决这一问题，我们引入了一种方法，根据文本描述生成相机姿态分布，促进对广义概念的稳健推理。我们的方法采用基于扩散的架构，将嘈杂的6DoF相机姿态细化到合理位置，条件信号来源于预训练的文本编码器。与预训练的视觉语言模型CLIP的结合，在文本描述和姿态分布之间建立了强有力的联系。通过采用三维高斯摊铺渲染候选姿态，实现了定位精度的提升，通过视觉推理纠正了错位样本。我们在五个大规模数据集上将我们的方法与标准分布估计方法进行了比较，展示了持续超越对手的性能。我们的项目页面将公开提供代码、数据集和更多信息。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08982v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种方法，通过生成基于文本描述的相机姿态分布来解决大规模三维场景中本地化文本描述的内生歧义问题。该方法利用扩散架构优化噪声6DoF相机姿态，使其朝向合理位置，并使用预训练文本编码器生成条件信号。结合预训练的视觉语言模型CLIP，在文本描述和姿态分布之间建立强链接。通过3D高斯喷绘技术渲染候选姿态，提高定位精度，并可通过视觉推理修正错位样本。本文方法在五个大规模数据集上与其他分布估计方法进行比较，展现了优越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出一种解决大规模三维场景中本地化文本描述的内生歧义问题的方法。</li>
<li>通过生成基于文本描述的相机姿态分布，促进对广泛定义概念的理解。</li>
<li>利用扩散架构优化相机姿态，使其朝向合理位置。</li>
<li>结合预训练的文本编码器和视觉语言模型CLIP，建立文本描述与姿态分布之间的强链接。</li>
<li>采用3D高斯喷绘技术渲染候选姿态，增强定位准确性。</li>
<li>通过视觉推理修正错位样本。</li>
<li>在五个大规模数据集上验证了方法的优越性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08982">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cf2226f2e53dd1dc268de3efb1fbf85a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ad7dcb8d9410b67d2c2dc55f935454c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-178a714fb7846b95af7cb992255655e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35e518f6ffc1aaf445bc1a652e29c924.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8a174b40829c15f697d98c7b2bc3fff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-704d95646558cc9a4cc174cf5b9dc987.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Reflective-Gaussian-Splatting"><a href="#Reflective-Gaussian-Splatting" class="headerlink" title="Reflective Gaussian Splatting"></a>Reflective Gaussian Splatting</h2><p><strong>Authors:Yuxuan Yao, Zixuan Zeng, Chun Gu, Xiatian Zhu, Li Zhang</strong></p>
<p>Novel view synthesis has experienced significant advancements owing to increasingly capable NeRF- and 3DGS-based methods. However, reflective object reconstruction remains challenging, lacking a proper solution to achieve real-time, high-quality rendering while accommodating inter-reflection. To fill this gap, we introduce a Reflective Gaussian splatting (Ref-Gaussian) framework characterized with two components: (I) Physically based deferred rendering that empowers the rendering equation with pixel-level material properties via formulating split-sum approximation; (II) Gaussian-grounded inter-reflection that realizes the desired inter-reflection function within a Gaussian splatting paradigm for the first time. To enhance geometry modeling, we further introduce material-aware normal propagation and an initial per-Gaussian shading stage, along with 2D Gaussian primitives. Extensive experiments on standard datasets demonstrate that Ref-Gaussian surpasses existing approaches in terms of quantitative metrics, visual quality, and compute efficiency. Further, we show that our method serves as a unified solution for both reflective and non-reflective scenes, going beyond the previous alternatives focusing on only reflective scenes. Also, we illustrate that Ref-Gaussian supports more applications such as relighting and editing. </p>
<blockquote>
<p>基于新型视图合成方法，尤其是日益强大的NeRF和3DGS方法，已经取得了重大进展。然而，反射物体的重建仍然是一个挑战，缺乏一种能在实时实现高质量渲染的同时处理相互反射的适当解决方案。为了填补这一空白，我们引入了基于反射高斯点涂（Ref-Gaussian）的框架，该框架由两个组件构成：（I）基于物理的延迟渲染技术，它通过公式化分割求和近似法为渲染方程赋予像素级材料属性；（II）基于高斯理论的相互反射技术，首次在高斯点涂范式内实现了所需的相互反射功能。为了提高几何建模能力，我们还引入了感知材料的法线传播和初始的高斯阴影阶段，以及二维高斯基本体。在标准数据集上的大量实验表明，Ref-Gaussian在定量指标、视觉质量和计算效率方面超过了现有方法。此外，我们证明我们的方法可以作为反射和非反射场景的统一解决方案，超越了之前只关注反射场景的替代方案。我们还说明Ref-Gaussian支持更多应用，如重新照明和编辑。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19282v2">PDF</a> Accepted for ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>基于NeRF和3DGS的方法，新型视图合成技术取得了显著进展，但反射物体重建仍面临挑战，缺乏实现实时高质量渲染同时兼顾互反射的适当解决方案。为解决此问题，我们提出了基于反射高斯拼贴（Ref-Gaussian）的框架，包含两个关键组成部分：（I）基于物理的延迟渲染，通过公式化分割和求和近似，为渲染方程提供像素级材料属性；（II）基于高斯拼贴的互反射实现。为提高几何建模能力，我们还引入了感知材料的正常传播、初始的高斯着色阶段以及二维高斯基本元素。在标准数据集上的广泛实验表明，Ref-Gaussian在定量指标、视觉质量和计算效率方面均超越了现有方法。此外，我们的方法成为反射和非反射场景的统一解决方案，超越了之前仅关注反射场景的替代方案。同时，Ref-Gaussian还支持重新照明和编辑等更多应用。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>新型视图合成技术取得显著进展，反射物体重建仍是挑战。</li>
<li>引入Reflective Gaussian splatting（Ref-Gaussian）框架，包含基于物理的延迟渲染和Gaussian拼贴实现互反射两个关键部分。</li>
<li>Ref-Gaussian提升几何建模能力，引入材料感知的正常传播和初始高斯着色阶段。</li>
<li>在标准数据集上，Ref-Gaussian在定量指标、视觉质量和计算效率方面超越现有方法。</li>
<li>Ref-Gaussian成为反射和非反射场景的统一解决方案。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19282">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-235230d72bde052b54a5ffb80f3de731.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80b0c681a46443a245df2b7e928e2c20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19cf17b2a79284df75c62499ae3358ac.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Topology-Aware-3D-Gaussian-Splatting-Leveraging-Persistent-Homology-for-Optimized-Structural-Integrity"><a href="#Topology-Aware-3D-Gaussian-Splatting-Leveraging-Persistent-Homology-for-Optimized-Structural-Integrity" class="headerlink" title="Topology-Aware 3D Gaussian Splatting: Leveraging Persistent Homology for   Optimized Structural Integrity"></a>Topology-Aware 3D Gaussian Splatting: Leveraging Persistent Homology for   Optimized Structural Integrity</h2><p><strong>Authors:Tianqi Shen, Shaohua Liu, Jiaqi Feng, Ziye Ma, Ning An</strong></p>
<p>Gaussian Splatting (GS) has emerged as a crucial technique for representing discrete volumetric radiance fields. It leverages unique parametrization to mitigate computational demands in scene optimization. This work introduces Topology-Aware 3D Gaussian Splatting (Topology-GS), which addresses two key limitations in current approaches: compromised pixel-level structural integrity due to incomplete initial geometric coverage, and inadequate feature-level integrity from insufficient topological constraints during optimization. To overcome these limitations, Topology-GS incorporates a novel interpolation strategy, Local Persistent Voronoi Interpolation (LPVI), and a topology-focused regularization term based on persistent barcodes, named PersLoss. LPVI utilizes persistent homology to guide adaptive interpolation, enhancing point coverage in low-curvature areas while preserving topological structure. PersLoss aligns the visual perceptual similarity of rendered images with ground truth by constraining distances between their topological features. Comprehensive experiments on three novel-view synthesis benchmarks demonstrate that Topology-GS outperforms existing methods in terms of PSNR, SSIM, and LPIPS metrics, while maintaining efficient memory usage. This study pioneers the integration of topology with 3D-GS, laying the groundwork for future research in this area. </p>
<blockquote>
<p>高斯贴图（GS）已成为表示离散体积辐射场的关键技术。它利用独特的参数化方法，以减轻场景优化中的计算需求。本文介绍了拓扑感知三维高斯贴图（Topology-GS），解决了当前方法中的两个关键局限性：由于初始几何覆盖不完整而损害像素级结构完整性，以及在优化过程中由于拓扑约束不足而导致特征级完整性不足。为了克服这些局限性，Topology-GS融入了一种新型插值策略——局部持久Voronoi插值（LPVI）和一种基于持久条码的专注于拓扑的正则化项，称为PersLoss。LPVI利用持久同源性来引导自适应插值，在低曲率区域增强点覆盖，同时保留拓扑结构。PersLoss通过对拓扑特征之间的距离进行约束，使渲染图像的视觉感知相似性符合真实情况。在三个全新视图合成基准测试上的综合实验表明，在PSNR、SSIM和LPIPS指标方面，Topology-GS优于现有方法，同时保持高效的内存使用。本研究首创了拓扑与3D-GS的集成，为这一领域的未来研究奠定了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16619v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了高斯采样（GS）在表示离散体积辐射场中的关键作用，并通过独特的参数化方法减轻了场景优化中的计算负担。为此工作引入的拓扑感知三维高斯采样（Topology-GS）解决了当前方法中的两个关键局限：由于初始几何覆盖不完整而导致的像素级结构完整性受损，以及在优化过程中因拓扑约束不足而导致的特征级完整性不足。为了克服这些局限性，Topology-GS采用了一种新颖的内插策略——局部持久Voronoi内插（LPVI）和基于持久条码的拓扑重点正则化术语——PersLoss。LPVI利用持久同源性引导自适应内插，增强了低曲率区域的点覆盖，同时保持拓扑结构。PersLoss通过对拓扑特征之间的距离进行约束，使渲染图像的视觉感知相似性与地面真实情况保持一致。在三个全新视图合成基准测试上的综合实验表明，Topology-GS在PSNR、SSIM和LPIPS指标上优于现有方法，同时保持高效的内存使用。该研究开创了拓扑与3D-GS的集成，为未来该领域的研究奠定了基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高斯采样（GS）是表示离散体积辐射场的重要技术，通过参数化减轻场景优化计算负担。</li>
<li>拓扑感知三维高斯采样（Topology-GS）解决了当前方法的两个关键局限：像素级结构完整性的妥协和特征级完整性的不足。</li>
<li>Topology-GS采用局部持久Voronoi内插（LPVI）和基于持久条码的拓扑重点正则化术语——PersLoss来克服这些局限性。</li>
<li>LPVI利用持久同源性增强低曲率区域的点覆盖，并保持拓扑结构。</li>
<li>PersLoss约束渲染图像与地面真实情况之间的拓扑特征距离，提高视觉感知相似性。</li>
<li>在全新视图合成基准测试上，Topology-GS在PSNR、SSIM和LPIPS指标上表现优越，且内存使用高效。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16619">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a02f8686e34c6863c53aaffe736039f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7056d16d36cd369d87fe8d52d05c4b40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6336e82444c90f3e423b235b9468df0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d91aa38b10bab55707a1d4cecf64ffd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5126ec456f8faccef17946a7462a38f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3898879becbe4124485b60552e6a2c9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b78ef0ec11cd26ba13bcf4e26a8bf85e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f99d376e15640d0f41139b9352e1f5f0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CATSplat-Context-Aware-Transformer-with-Spatial-Guidance-for-Generalizable-3D-Gaussian-Splatting-from-A-Single-View-Image"><a href="#CATSplat-Context-Aware-Transformer-with-Spatial-Guidance-for-Generalizable-3D-Gaussian-Splatting-from-A-Single-View-Image" class="headerlink" title="CATSplat: Context-Aware Transformer with Spatial Guidance for   Generalizable 3D Gaussian Splatting from A Single-View Image"></a>CATSplat: Context-Aware Transformer with Spatial Guidance for   Generalizable 3D Gaussian Splatting from A Single-View Image</h2><p><strong>Authors:Wonseok Roh, Hwanhee Jung, Jong Wook Kim, Seunggwan Lee, Innfarn Yoo, Andreas Lugmayr, Seunggeun Chi, Karthik Ramani, Sangpil Kim</strong></p>
<p>Recently, generalizable feed-forward methods based on 3D Gaussian Splatting have gained significant attention for their potential to reconstruct 3D scenes using finite resources. These approaches create a 3D radiance field, parameterized by per-pixel 3D Gaussian primitives, from just a few images in a single forward pass. However, unlike multi-view methods that benefit from cross-view correspondences, 3D scene reconstruction with a single-view image remains an underexplored area. In this work, we introduce CATSplat, a novel generalizable transformer-based framework designed to break through the inherent constraints in monocular settings. First, we propose leveraging textual guidance from a visual-language model to complement insufficient information from a single image. By incorporating scene-specific contextual details from text embeddings through cross-attention, we pave the way for context-aware 3D scene reconstruction beyond relying solely on visual cues. Moreover, we advocate utilizing spatial guidance from 3D point features toward comprehensive geometric understanding under single-view settings. With 3D priors, image features can capture rich structural insights for predicting 3D Gaussians without multi-view techniques. Extensive experiments on large-scale datasets demonstrate the state-of-the-art performance of CATSplat in single-view 3D scene reconstruction with high-quality novel view synthesis. </p>
<blockquote>
<p>最近，基于三维高斯拼贴（3D Gaussian Splatting）的通用前馈方法因其利用有限资源重建三维场景的潜力而受到广泛关注。这些方法在单次前向传递中仅使用少数图像即可创建一个三维辐射场，该辐射场由像素级三维高斯基元参数化。然而，与受益于跨视图对应的多视角方法不同，基于单视图图像的三维场景重建仍然是一个尚未得到充分探索的领域。在这项工作中，我们引入了CATSplat，这是一个新型的可推广的基于transformer的框架，旨在突破单目视觉设置中的固有约束。首先，我们提出利用视觉语言模型的文本指导来补充单一图像中的不足信息。通过跨注意力融入场景特定的上下文细节和文本嵌入，我们为仅在视觉线索之外进行上下文感知的三维场景重建铺平了道路。此外，我们主张利用三维点特征的空间指导，以实现单视角设置下的全面几何理解。借助三维先验知识，图像特征可以捕捉丰富的结构洞察力，无需多视角技术即可预测三维高斯分布。在大型数据集上的广泛实验表明，CATSplat在单视角三维场景重建中具有最先进的性能，并能实现高质量的新视角合成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12906v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于文本描述的辅助和单视角图像特征的空间指导，CATSplat框架实现了突破单目场景固有约束的通用可变形态建模，完成了超越视觉线索的单视角上下文感知的三维场景重建任务。该方法展示了卓越的单视角三维场景重建性能，实现了高质量的新视角合成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CATSplat是一种基于通用可变形态的框架，旨在突破单目场景重建的固有约束。</li>
<li>利用文本指导来补充单一图像的信息不足，实现上下文感知的三维场景重建。</li>
<li>通过结合视觉语言模型的文本嵌入和跨注意力机制，将场景特定的上下文细节融入重建过程。</li>
<li>利用空间指导进行单视角下的全面几何理解，通过三维先验图像特征捕捉丰富的结构信息。</li>
<li>在大规模数据集上的实验证明了CATSplat在单视角三维场景重建领域的优越性。</li>
<li>该方法可实现高质量的新视角合成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12906">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-26625a8cd9fd0477e93c92a31f275e0a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7fcee7c7d3e153b917e17f05ceba3e90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68e3c41ed5ea4d5050fc93e670dcd597.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6b21a98b2a22c9cf98787dfb154b118.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56b1f79adf6fa9c78178ba24b46572b1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Gaussians-on-their-Way-Wasserstein-Constrained-4D-Gaussian-Splatting-with-State-Space-Modeling"><a href="#Gaussians-on-their-Way-Wasserstein-Constrained-4D-Gaussian-Splatting-with-State-Space-Modeling" class="headerlink" title="Gaussians on their Way: Wasserstein-Constrained 4D Gaussian Splatting   with State-Space Modeling"></a>Gaussians on their Way: Wasserstein-Constrained 4D Gaussian Splatting   with State-Space Modeling</h2><p><strong>Authors:Junli Deng, Yihao Luo</strong></p>
<p>Dynamic scene rendering has taken a leap forward with the rise of 4D Gaussian Splatting, but there’s still one elusive challenge: how to make 3D Gaussians move through time as naturally as they would in the real world, all while keeping the motion smooth and consistent. In this paper, we unveil a fresh approach that blends state-space modeling with Wasserstein geometry, paving the way for a more fluid and coherent representation of dynamic scenes. We introduce a State Consistency Filter that merges prior predictions with the current observations, enabling Gaussians to stay true to their way over time. We also employ Wasserstein distance regularization to ensure smooth, consistent updates of Gaussian parameters, reducing motion artifacts. Lastly, we leverage Wasserstein geometry to capture both translational motion and shape deformations, creating a more physically plausible model for dynamic scenes. Our approach guides Gaussians along their natural way in the Wasserstein space, achieving smoother, more realistic motion and stronger temporal coherence. Experimental results show significant improvements in rendering quality and efficiency, outperforming current state-of-the-art techniques. </p>
<blockquote>
<p>动态场景渲染随着四维高斯贴图技术的兴起取得了长足的发展，但仍面临一个难以捉摸的挑战：如何在保持运动平滑一致的同时，让三维高斯随时间变化像真实世界那样自然移动。在本文中，我们提出了一种将状态空间建模与Wasserstein几何相结合的新方法，为动态场景的表达开辟了更加流畅和连贯的道路。我们引入了一种状态一致性滤波器，它将先前的预测与当前观测相结合，使高斯随着时间的推移保持其真实性。我们还采用Wasserstein距离正则化，以确保高斯参数的平滑一致更新，减少运动伪影。最后，我们利用Wasserstein几何来捕捉平移运动和形状变形，为动态场景创建了一个更物理上合理的模型。我们的方法引导高斯在Wasserstein空间内沿着自然路径移动，实现更平滑、更逼真的运动以及更强的时间连贯性。实验结果表明，在渲染质量和效率方面都有显著提高，超越了当前最先进的技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00333v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>这篇论文提出了一种将状态空间模型与Wasserstein几何相结合的新方法，以更流畅、连贯地表现动态场景。新方法通过使用状态一致性滤波器来融合先前的预测和当前观察结果，确保Gaussians随时间推移保持一致性。此外，采用Wasserstein距离正则化确保Gaussian参数平滑、一致地更新，减少运动伪影。最后，利用Wasserstein几何捕捉平移运动和形状变形，建立更贴近物理现实的动态场景模型。此方法在Wasserstein空间中引导Gaussians沿自然路径运动，实现了更平滑、更逼真的运动效果，并提高了渲染质量和效率，优于当前先进技术水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入状态空间模型与Wasserstein几何结合的新方法，以改善动态场景的渲染质量。</li>
<li>通过状态一致性滤波器融合先前预测和当前观察结果，使Gaussians保持一致性。</li>
<li>采用Wasserstein距离正则化确保Gaussian参数平滑、一致更新，减少运动伪影。</li>
<li>利用Wasserstein几何捕捉运动和形状变形，建立更物理现实的动态场景模型。</li>
<li>方法实现了更平滑、更逼真的运动效果。</li>
<li>提高了渲染效率和质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00333">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0c12607095defd9a632ae9305e6bb924.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3170b95c1229f310499cbbc42c639da7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc407821c8747c3958d093ec6b1aa5ba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0eb966a5d8eb7c56f5a04341943078dc.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-05/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-05/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-05/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6880b8f0f58d66ce069be7c92ea604da.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-02-05  Laser Efficient Language-Guided Segmentation in Neural Radiance Fields
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-05/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-4f141bc5650c59804d3f9a2085de1663.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-02-05  Ambient Denoising Diffusion Generative Adversarial Networks for   Establishing Stochastic Object Models from Noisy Image Data
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23154.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
