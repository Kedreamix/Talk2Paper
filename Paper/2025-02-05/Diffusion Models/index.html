<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-05  Pathological MRI Segmentation by Synthetic Pathological Data Generation   in Fetuses and Neonates">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-ac07eb33e68fc2d771f0cfcb33758122.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    64 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-05-æ›´æ–°"><a href="#2025-02-05-æ›´æ–°" class="headerlink" title="2025-02-05 æ›´æ–°"></a>2025-02-05 æ›´æ–°</h1><h2 id="Pathological-MRI-Segmentation-by-Synthetic-Pathological-Data-Generation-in-Fetuses-and-Neonates"><a href="#Pathological-MRI-Segmentation-by-Synthetic-Pathological-Data-Generation-in-Fetuses-and-Neonates" class="headerlink" title="Pathological MRI Segmentation by Synthetic Pathological Data Generation   in Fetuses and Neonates"></a>Pathological MRI Segmentation by Synthetic Pathological Data Generation   in Fetuses and Neonates</h2><p><strong>Authors:Misha P. T Kaandorp, Damola Agbelese, Hosna Asma-ull, Hyun-Gi Kim, Kelly Payette, Patrice Grehten, Gennari Antonio Giulio, Levente IstvÃ¡n LÃ¡nczi, Andras Jakab</strong></p>
<p>Developing new methods for the automated analysis of clinical fetal and neonatal MRI data is limited by the scarcity of annotated pathological datasets and privacy concerns that often restrict data sharing, hindering the effectiveness of deep learning models. We address this in two ways. First, we introduce Fetal&amp;Neonatal-DDPM, a novel diffusion model framework designed to generate high-quality synthetic pathological fetal and neonatal MRIs from semantic label images. Second, we enhance training data by modifying healthy label images through morphological alterations to simulate conditions such as ventriculomegaly, cerebellar and pontocerebellar hypoplasia, and microcephaly. By leveraging Fetal&amp;Neonatal-DDPM, we synthesize realistic pathological MRIs from these modified pathological label images. Radiologists rated the synthetic MRIs as significantly (p &lt; 0.05) superior in quality and diagnostic value compared to real MRIs, demonstrating features such as blood vessels and choroid plexus, and improved alignment with label annotations. Synthetic pathological data enhanced state-of-the-art nnUNet segmentation performance, particularly for severe ventriculomegaly cases, with the greatest improvements achieved in ventricle segmentation (Dice scores: 0.9253 vs. 0.7317). This study underscores the potential of generative AI as transformative tool for data augmentation, offering improved segmentation performance in pathological cases. This development represents a significant step towards improving analysis and segmentation accuracy in prenatal imaging, and also offers new ways for data anonymization through the generation of pathologic image data. </p>
<blockquote>
<p>é’ˆå¯¹ä¸´åºŠèƒå„¿å’Œæ–°ç”Ÿå„¿MRIæ•°æ®çš„è‡ªåŠ¨åŒ–åˆ†ææ–°æ–¹æ³•çš„å¼€å‘å—é™äºæ ‡æ³¨ç—…ç†æ•°æ®é›†çš„ç¨€ç¼ºæ€§ä»¥åŠéšç§æ‹…å¿§ï¼Œåè€…ç»å¸¸é™åˆ¶æ•°æ®å…±äº«ï¼Œé˜»ç¢äº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤ç§æ–¹å¼æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†Fetalï¼†Neonatal-DDPMè¿™ä¸€æ–°å‹æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨ä»è¯­ä¹‰æ ‡ç­¾å›¾åƒç”Ÿæˆé«˜è´¨é‡åˆæˆç—…ç†èƒå„¿å’Œæ–°ç”Ÿå„¿MRIsã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡å½¢æ€æ”¹å˜ä¿®æ”¹å¥åº·æ ‡ç­¾å›¾åƒæ¥å¢å¼ºè®­ç»ƒæ•°æ®ï¼Œä»¥æ¨¡æ‹Ÿè„‘å®¤æ‰©å¤§ã€å°è„‘å’Œæ¡¥å°è„‘èç¼©ä»¥åŠå°å¤´ç•¸å½¢ç­‰çŠ¶å†µã€‚é€šè¿‡åˆ©ç”¨Fetalï¼†Neonatal-DDPMï¼Œæˆ‘ä»¬ä»è¿™äº›ä¿®æ”¹åçš„ç—…ç†æ ‡ç­¾å›¾åƒä¸­åˆæˆå‡ºé€¼çœŸçš„ç—…ç†æ€§MRIã€‚æ”¾å°„ç§‘åŒ»ç”Ÿè¯„ä»·åˆæˆçš„MRIåœ¨è´¨é‡å’Œè¯Šæ–­ä»·å€¼ä¸Šæ˜¾è‘—é«˜äºçœŸå®MRIï¼ˆp&lt;0.05ï¼‰ï¼Œæ˜¾ç¤ºå‡ºè¡€ç®¡å’Œè„‰ç»œä¸›ç­‰ç‰¹å¾ï¼Œä¸æ ‡ç­¾æ³¨é‡Šå¯¹é½åº¦æ›´é«˜ã€‚åˆæˆç—…ç†æ•°æ®æé«˜äº†æœ€å…ˆè¿›åˆ†å‰²ç½‘ç»œçš„åˆ†å‰²æ€§èƒ½ï¼ˆnnUNetï¼‰ï¼Œç‰¹åˆ«æ˜¯å¯¹äºä¸¥é‡çš„è„‘å®¤æ‰©å¤§ç—…ä¾‹ï¼Œåœ¨è„‘å®¤åˆ†å‰²æ–¹é¢å–å¾—äº†æœ€å¤§çš„æ”¹è¿›ï¼ˆDiceåˆ†æ•°ï¼š0.9253 vs. 0.7317ï¼‰ã€‚è¿™é¡¹ç ”ç©¶å¼ºè°ƒäº†ç”Ÿæˆäººå·¥æ™ºèƒ½ä½œä¸ºæ•°æ®å¢å¼ºå·¥å…·çš„æ½œåŠ›ï¼Œåœ¨ç—…ç†æƒ…å†µä¸‹æé«˜äº†åˆ†å‰²æ€§èƒ½ã€‚è¿™ä¸€å‘å±•ä»£è¡¨äº†äº§å‰æˆåƒåˆ†æå’Œåˆ†å‰²ç²¾åº¦æ”¹è¿›çš„é‡è¦ä¸€æ­¥ï¼ŒåŒæ—¶ä¹Ÿæä¾›äº†æ–°çš„æ–¹æ³•æ¥é€šè¿‡ç”Ÿæˆç—…ç†å›¾åƒæ•°æ®è¿›è¡Œæ•°æ®åŒ¿åå¤„ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19338v1">PDF</a> 30 pages, 4 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è§£å†³ä¸´åºŠèƒå„¿å’Œæ–°ç”Ÿå„¿MRIæ•°æ®è‡ªåŠ¨åŒ–åˆ†æéš¾é¢˜çš„æ–°æ–¹æ³•ã€‚é’ˆå¯¹ç—…ç†æ€§æ•°æ®é›†ç¨€ç¼ºå’Œéšç§æ‹…å¿§é™åˆ¶æ•°æ®å…±äº«çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿé‡‡å–äº†ä¸¤ç§ç­–ç•¥ã€‚ä»–ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ‰©æ•£æ¨¡å‹æ¡†æ¶Fetal&amp;Neonatal-DDPMï¼Œèƒ½å¤Ÿä»è¯­ä¹‰æ ‡ç­¾å›¾åƒç”Ÿæˆé«˜è´¨é‡åˆæˆç—…ç†æ€§èƒå„¿å’Œæ–°ç”Ÿå„¿MRIã€‚åŒæ—¶ï¼Œé€šè¿‡å¯¹å¥åº·æ ‡ç­¾å›¾åƒè¿›è¡Œå½¢æ€å­¦æ”¹å˜ä»¥æ¨¡æ‹Ÿè¯¸å¦‚è„‘ç§¯æ°´ã€å°è„‘å’Œè„‘æ¡¥å°è„‘å‘è‚²ä¸è‰¯å’Œå°å¤´ç•¸å½¢ç­‰çŠ¶å†µï¼Œå¢å¼ºè®­ç»ƒæ•°æ®ã€‚åˆæˆçš„MRIå›¾åƒè¢«æ”¾å°„ç§‘åŒ»ç”Ÿè¯„ä»·ä¸ºå…·æœ‰è¾ƒé«˜çš„è´¨é‡å’Œè¯Šæ–­ä»·å€¼ã€‚æ­¤å¤–ï¼Œåˆæˆç—…ç†æ€§æ•°æ®æé«˜äº†nnUNetåˆ†å‰²çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸¥é‡è„‘ç§¯æ°´ç—…ä¾‹ä¸­ï¼Œè„‘å®¤åˆ†å‰²çš„Diceå¾—åˆ†æ˜¾è‘—æé«˜ã€‚è¯¥ç ”ç©¶å‡¸æ˜¾äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨æ•°æ®å¢å¼ºæ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºæé«˜ç—…ç†æƒ…å†µä¸‹çš„åˆ†æå’Œåˆ†å‰²ç²¾åº¦æä¾›äº†æ–°çš„é€”å¾„ï¼Œå¹¶å¯é€šè¿‡ç”Ÿæˆç—…ç†æ€§å›¾åƒæ•°æ®å®ç°æ•°æ®åŒ¿ååŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>èƒå„¿å’Œæ–°ç”Ÿå„¿MRIæ•°æ®è‡ªåŠ¨åŒ–åˆ†æé¢ä¸´ç—…ç†æ€§æ•°æ®é›†ç¨€ç¼ºå’Œéšç§æ‹…å¿§çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥Fetal&amp;Neonatal-DDPMæ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œä»è¯­ä¹‰æ ‡ç­¾å›¾åƒç”Ÿæˆé«˜è´¨é‡åˆæˆç—…ç†æ€§MRIã€‚</li>
<li>é€šè¿‡å½¢æ€å­¦æ”¹å˜å¥åº·æ ‡ç­¾å›¾åƒä»¥æ¨¡æ‹Ÿç—…ç†æ€§çŠ¶å†µï¼Œå¢å¼ºè®­ç»ƒæ•°æ®ã€‚</li>
<li>åˆæˆçš„MRIå›¾åƒè¢«æ”¾å°„ç§‘åŒ»ç”Ÿè¯„ä»·ä¸ºå…·æœ‰è¾ƒé«˜çš„è´¨é‡å’Œè¯Šæ–­ä»·å€¼ã€‚</li>
<li>åˆæˆç—…ç†æ€§æ•°æ®æé«˜äº†nnUNetåˆ†å‰²æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è„‘å®¤åˆ†å‰²æ–¹é¢ã€‚</li>
<li>ç ”ç©¶å‡¸æ˜¾äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨æ•°æ®å¢å¼ºæ–¹é¢çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19338">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-28ed8e03cfc09a1d5824886f9c8ce7c8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Medical-Semantic-Segmentation-with-Diffusion-Pretrain"><a href="#Medical-Semantic-Segmentation-with-Diffusion-Pretrain" class="headerlink" title="Medical Semantic Segmentation with Diffusion Pretrain"></a>Medical Semantic Segmentation with Diffusion Pretrain</h2><p><strong>Authors:David Li, Anvar Kurmukov, Mikhail Goncharov, Roman Sokolov, Mikhail Belyaev</strong></p>
<p>Recent advances in deep learning have shown that learning robust feature representations is critical for the success of many computer vision tasks, including medical image segmentation. In particular, both transformer and convolutional-based architectures have benefit from leveraging pretext tasks for pretraining. However, the adoption of pretext tasks in 3D medical imaging has been less explored and remains a challenge, especially in the context of learning generalizable feature representations.   We propose a novel pretraining strategy using diffusion models with anatomical guidance, tailored to the intricacies of 3D medical image data. We introduce an auxiliary diffusion process to pretrain a model that produce generalizable feature representations, useful for a variety of downstream segmentation tasks. We employ an additional model that predicts 3D universal body-part coordinates, providing guidance during the diffusion process and improving spatial awareness in generated representations. This approach not only aids in resolving localization inaccuracies but also enriches the modelâ€™s ability to understand complex anatomical structures.   Empirical validation on a 13-class organ segmentation task demonstrate the effectiveness of our pretraining technique. It surpasses existing restorative pretraining methods in 3D medical image segmentation by $7.5%$, and is competitive with the state-of-the-art contrastive pretraining approach, achieving an average Dice coefficient of 67.8 in a non-linear evaluation scenario. </p>
<blockquote>
<p>æœ€è¿‘çš„æ·±åº¦å­¦ä¹ è¿›å±•è¡¨æ˜ï¼Œå­¦ä¹ ç¨³å¥çš„ç‰¹å¾è¡¨ç¤ºå¯¹äºè®¸å¤šè®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼ˆåŒ…æ‹¬åŒ»å­¦å›¾åƒåˆ†å‰²ï¼‰çš„æˆåŠŸè‡³å…³é‡è¦ã€‚ç‰¹åˆ«æ˜¯åŸºäºtransformerå’Œå·ç§¯çš„æ¶æ„éƒ½ä»åˆ©ç”¨é¢„è®­ç»ƒä»»åŠ¡ä¸­è·ç›Šã€‚ç„¶è€Œï¼Œåœ¨3DåŒ»å­¦å½±åƒä¸­é‡‡ç”¨é¢„è®­ç»ƒä»»åŠ¡çš„ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å­¦ä¹ é€šç”¨ç‰¹å¾è¡¨ç¤ºæ–¹é¢ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒçš„æ–°ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å…·æœ‰è§£å‰–å¯¼å‘æ€§ï¼Œé’ˆå¯¹3DåŒ»å­¦å›¾åƒæ•°æ®çš„å¤æ‚æ€§è¿›è¡Œäº†å®šåˆ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¾…åŠ©æ‰©æ•£è¿‡ç¨‹æ¥é¢„è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥äº§ç”Ÿå¯ç”¨äºå„ç§ä¸‹æ¸¸åˆ†å‰²ä»»åŠ¡çš„é€šç”¨ç‰¹å¾è¡¨ç¤ºã€‚æˆ‘ä»¬è¿˜é‡‡ç”¨å¦ä¸€ä¸ªæ¨¡å‹é¢„æµ‹3Dé€šç”¨èº«ä½“éƒ¨ä½åæ ‡ï¼Œåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­æä¾›æŒ‡å¯¼ï¼Œæé«˜äº†ç”Ÿæˆè¡¨ç¤ºçš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æœ‰åŠ©äºè§£å†³å®šä½ä¸å‡†ç¡®çš„é—®é¢˜ï¼Œè¿˜ä¸°å¯Œäº†æ¨¡å‹ç†è§£å¤æ‚è§£å‰–ç»“æ„çš„èƒ½åŠ›ã€‚åœ¨13ç±»å™¨å®˜åˆ†å‰²ä»»åŠ¡ä¸Šçš„ç»éªŒéªŒè¯è¡¨æ˜æˆ‘ä»¬é¢„è®­ç»ƒæŠ€æœ¯çš„æœ‰æ•ˆæ€§ã€‚ä¸ç°æœ‰çš„3DåŒ»å­¦å›¾åƒåˆ†å‰²æ¢å¤é¢„è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æŠ€æœ¯æé«˜äº†7.5%ï¼Œå¹¶ä¸”åœ¨éçº¿æ€§è¯„ä¼°åœºæ™¯ä¸­ä¸æœ€å…ˆè¿›çš„å¯¹æ¯”é¢„è®­ç»ƒæ–¹æ³•ç›¸ç«äº‰ï¼Œå¹³å‡Diceç³»æ•°ä¸º67.8ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19265v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹é¢„è®­ç»ƒç­–ç•¥ï¼Œç»“åˆè§£å‰–å­¦æŒ‡å¯¼ï¼Œé’ˆå¯¹3DåŒ»å­¦å›¾åƒæ•°æ®çš„ç‰¹æ€§è¿›è¡Œå®šåˆ¶ã€‚é€šè¿‡å¼•å…¥è¾…åŠ©æ‰©æ•£è¿‡ç¨‹å’Œé¢„æµ‹3Dé€šç”¨èº«ä½“éƒ¨ä½åæ ‡çš„é™„åŠ æ¨¡å‹ï¼Œæé«˜æ¨¡å‹åœ¨å¤šç§ä¸‹æ¸¸åˆ†å‰²ä»»åŠ¡ä¸­çš„é€šç”¨ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ã€‚å®éªŒéªŒè¯æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä¸ä»…è§£å†³äº†å®šä½ä¸å‡†ç¡®çš„é—®é¢˜ï¼Œè¿˜æé«˜äº†æ¨¡å‹ç†è§£å¤æ‚è§£å‰–ç»“æ„çš„èƒ½åŠ›ã€‚åœ¨13ç±»å™¨å®˜åˆ†å‰²ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„æ¢å¤æ€§é¢„è®­ç»ƒæ–¹æ³•ï¼Œå¹¶ä¸æœ€æ–°çš„å¯¹æ¯”é¢„è®­ç»ƒæ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ ä¸­çš„å…ˆè¿›è¿›å±•è¡¨æ˜ï¼Œå­¦ä¹ é²æ£’çš„ç‰¹å¾è¡¨ç¤ºå¯¹äºåŒ…æ‹¬åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨å†…çš„è®¸å¤šè®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„æˆåŠŸè‡³å…³é‡è¦ã€‚</li>
<li>å°½ç®¡é¢„è®­ç»ƒåœ¨å›¾åƒè¯†åˆ«å’Œå…¶ä»–NLPä»»åŠ¡ä¸­å·²ç»è¯æ˜äº†å…¶ä»·å€¼ï¼Œä½†åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œå°¤å…¶æ˜¯å­¦ä¹ é€šç”¨ç‰¹å¾è¡¨ç¤ºæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹é¢„è®­ç»ƒç­–ç•¥ï¼Œç‰¹åˆ«é€‚ç”¨äº3DåŒ»å­¦å›¾åƒæ•°æ®ã€‚</li>
<li>é€šè¿‡å¼•å…¥è¾…åŠ©æ‰©æ•£è¿‡ç¨‹å’Œé¢„æµ‹èº«ä½“éƒ¨ä½åæ ‡çš„æ¨¡å‹ï¼Œæé«˜äº†æ¨¡å‹çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›å’Œé€šç”¨ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç­–ç•¥åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¡¨ç°å‡æœ‰æ˜¾è‘—æå‡ï¼Œè§£å†³äº†å®šä½ä¸å‡†ç¡®çš„é—®é¢˜ã€‚</li>
<li>åœ¨å¤æ‚çš„è§£å‰–ç»“æ„ç†è§£æ–¹é¢ï¼Œè¯¥ç­–ç•¥è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ”¹è¿›å’Œä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19265">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-486cdd9cc6777c026aacad383e23143a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-203bc45a0197ca1e9f3680dacd825b59.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-12e1b7cc52e8c8e501a1aa25f49942b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d24f8df96f1388426d36736c95f7b0d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-251c7f1d4827f2fa2f8d8819129e0de5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Ambient-Denoising-Diffusion-Generative-Adversarial-Networks-for-Establishing-Stochastic-Object-Models-from-Noisy-Image-Data"><a href="#Ambient-Denoising-Diffusion-Generative-Adversarial-Networks-for-Establishing-Stochastic-Object-Models-from-Noisy-Image-Data" class="headerlink" title="Ambient Denoising Diffusion Generative Adversarial Networks for   Establishing Stochastic Object Models from Noisy Image Data"></a>Ambient Denoising Diffusion Generative Adversarial Networks for   Establishing Stochastic Object Models from Noisy Image Data</h2><p><strong>Authors:Xichen Xu, Wentao Chen, Weimin Zhou</strong></p>
<p>It is widely accepted that medical imaging systems should be objectively assessed via task-based image quality (IQ) measures that ideally account for all sources of randomness in the measured image data, including the variation in the ensemble of objects to be imaged. Stochastic object models (SOMs) that can randomly draw samples from the object distribution can be employed to characterize object variability. To establish realistic SOMs for task-based IQ analysis, it is desirable to employ experimental image data. However, experimental image data acquired from medical imaging systems are subject to measurement noise. Previous work investigated the ability of deep generative models (DGMs) that employ an augmented generative adversarial network (GAN), AmbientGAN, for establishing SOMs from noisy measured image data. Recently, denoising diffusion models (DDMs) have emerged as a leading DGM for image synthesis and can produce superior image quality than GANs. However, original DDMs possess a slow image-generation process because of the Gaussian assumption in the denoising steps. More recently, denoising diffusion GAN (DDGAN) was proposed to permit fast image generation while maintain high generated image quality that is comparable to the original DDMs. In this work, we propose an augmented DDGAN architecture, Ambient DDGAN (ADDGAN), for learning SOMs from noisy image data. Numerical studies that consider clinical computed tomography (CT) images and digital breast tomosynthesis (DBT) images are conducted. The ability of the proposed ADDGAN to learn realistic SOMs from noisy image data is demonstrated. It has been shown that the ADDGAN significantly outperforms the advanced AmbientGAN models for synthesizing high resolution medical images with complex textures. </p>
<blockquote>
<p>æ™®éè®¤åŒåŒ»å­¦å½±åƒç³»ç»Ÿåº”é€šè¿‡åŸºäºä»»åŠ¡çš„å›¾åƒè´¨é‡ï¼ˆIQï¼‰æªæ–½è¿›è¡Œå®¢è§‚è¯„ä¼°ï¼Œè¿™äº›æªæ–½ç†æƒ³æƒ…å†µä¸‹åº”è€ƒè™‘åˆ°æµ‹é‡å›¾åƒæ•°æ®ä¸­æ‰€æœ‰éšæœºæ€§çš„æ¥æºï¼ŒåŒ…æ‹¬å¾…æˆåƒç‰©ä½“é›†åˆçš„å˜å¼‚ã€‚å¯ä»¥ä½¿ç”¨éšæœºå¯¹è±¡æ¨¡å‹ï¼ˆSOMsï¼‰ä»ç‰©ä½“åˆ†å¸ƒä¸­éšæœºæŠ½å–æ ·æœ¬ä»¥è¡¨å¾ç‰©ä½“å˜å¼‚æ€§ã€‚ä¸ºäº†å»ºç«‹åŸºäºä»»åŠ¡çš„IQåˆ†æçš„ç°å®SOMsï¼Œä½¿ç”¨å®éªŒå›¾åƒæ•°æ®æ˜¯ç†æƒ³çš„ã€‚ç„¶è€Œï¼Œä»åŒ»å­¦å½±åƒç³»ç»Ÿè·å¾—çš„å®éªŒå›¾åƒæ•°æ®ä¼šå—åˆ°æµ‹é‡å™ªå£°çš„å½±å“ã€‚æ—©æœŸçš„å·¥ä½œç ”ç©¶äº†ä½¿ç”¨å¢å¼ºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ·±ç”Ÿæˆæ¨¡å‹ï¼ˆDGMï¼‰å»ºç«‹å™ªå£°æµ‹é‡å›¾åƒæ•°æ®çš„SOMsçš„èƒ½åŠ›ã€‚æœ€è¿‘ï¼Œå»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆDDMï¼‰ä½œä¸ºé¢†å…ˆçš„å›¾åƒåˆæˆDGMè€Œå‡ºç°ï¼Œå¯ä»¥äº§ç”Ÿæ¯”GANæ›´é«˜çš„å›¾åƒè´¨é‡ã€‚ç„¶è€Œï¼ŒåŸå§‹DDMsç”±äºå»å™ªæ­¥éª¤ä¸­çš„é«˜æ–¯å‡è®¾è€Œå…·æœ‰ç¼“æ…¢çš„å›¾åƒå¤„ç†è¿‡ç¨‹ã€‚æœ€è¿‘ï¼Œæå‡ºäº†å»å™ªæ‰©æ•£GANï¼ˆDDGANï¼‰ä»¥å…è®¸å¿«é€Ÿå›¾åƒç”Ÿæˆå¹¶ä¿æŒä¸åŸå§‹DDMsç›¸å½“çš„é«˜è´¨é‡å›¾åƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºçš„DDGANæ¶æ„ï¼Œå³ç¯å¢ƒDDGANï¼ˆADDGANï¼‰ï¼Œç”¨äºä»å™ªå£°å›¾åƒæ•°æ®ä¸­å­¦ä¹ SOMsã€‚æˆ‘ä»¬è¿›è¡Œäº†è€ƒè™‘ä¸´åºŠè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒå’Œæ•°å­—ä¹³è…ºæ–­å±‚åˆæˆï¼ˆDBTï¼‰å›¾åƒçš„æ•°å€¼ç ”ç©¶ã€‚è¯æ˜äº†æ‰€æå‡ºçš„ADDGANä»å™ªå£°å›¾åƒæ•°æ®ä¸­å­¦ä¹ ç°å®SOMsçš„èƒ½åŠ›ã€‚å·²ç»æ˜¾ç¤ºï¼ŒADDGANåœ¨åˆæˆå…·æœ‰å¤æ‚çº¹ç†çš„é«˜åˆ†è¾¨ç‡åŒ»å­¦å›¾åƒæ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆè¿›çš„AmbientGANæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19094v1">PDF</a> SPIE Medical Imaging 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¢å¼ºå‹å»å™ªæ‰©æ•£ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆDDGANï¼‰ä»å«å™ªåŒ»å­¦å›¾åƒæ•°æ®ä¸­å»ºç«‹éšæœºå¯¹è±¡æ¨¡å‹ï¼ˆSOMsï¼‰çš„æ–¹æ³•ã€‚é€šè¿‡æ•°å€¼ç ”ç©¶ï¼Œè¯æ˜æ‰€æå‡ºçš„ADDGANï¼ˆAmbient DDGANï¼‰èƒ½ä»å«å™ªçš„åŒ»å­¦å›¾åƒæ•°æ®ä¸­å­¦ä¹ çœŸå®çš„SOMsï¼Œå¹¶åœ¨åˆæˆå…·æœ‰å¤æ‚çº¹ç†çš„é«˜åˆ†è¾¨ç‡åŒ»å­¦å›¾åƒæ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆè¿›çš„AmbientGANæ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶å¼ºè°ƒäº†é€šè¿‡ä»»åŠ¡åŸºç¡€å›¾åƒè´¨é‡ï¼ˆIQï¼‰æªæ–½å¯¹åŒ»å­¦æˆåƒç³»ç»Ÿè¿›è¡Œå®¢è§‚è¯„ä¼°çš„é‡è¦æ€§ï¼Œè€ƒè™‘äº†å›¾åƒæ•°æ®ä¸­çš„æ‰€æœ‰éšæœºæ€§æ¥æºï¼ŒåŒ…æ‹¬è¦æˆåƒçš„å¯¹è±¡é›†åˆçš„å˜å¼‚æ€§ã€‚</li>
<li>éšæœºå¯¹è±¡æ¨¡å‹ï¼ˆSOMsï¼‰å¯ç”¨äºè¡¨å¾å¯¹è±¡å˜å¼‚æ€§ï¼Œå¯ä»å¯¹è±¡åˆ†å¸ƒä¸­éšæœºæŠ½å–æ ·æœ¬ã€‚</li>
<li>ä½¿ç”¨å®éªŒå›¾åƒæ•°æ®è¿›è¡Œä»»åŠ¡åŸºç¡€IQåˆ†ææ—¶ï¼Œéœ€è¦æ³¨æ„æ•°æ®å—æµ‹é‡å™ªå£°å½±å“çš„é—®é¢˜ã€‚</li>
<li>ä¹‹å‰çš„ç ”ç©¶å·²è°ƒæŸ¥äº†ä½¿ç”¨æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼ˆDGMï¼‰å¦‚å¢å¼ºå‹ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„AmbientGANä»å«å™ªæµ‹é‡å›¾åƒæ•°æ®ä¸­å»ºç«‹SOMsçš„èƒ½åŠ›ã€‚</li>
<li>å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆDDMï¼‰ä½œä¸ºé¢†å…ˆçš„å›¾åƒåˆæˆDGMï¼Œèƒ½äº§ç”Ÿæ¯”GANæ›´ä¼˜è´¨çš„å›¾åƒã€‚ç„¶è€Œï¼ŒåŸå§‹DDMsç”±äºå»å™ªæ­¥éª¤ä¸­çš„é«˜æ–¯å‡è®¾ï¼Œå›¾åƒç”Ÿæˆè¿‡ç¨‹è¾ƒæ…¢ã€‚</li>
<li>æ–°æå‡ºçš„å»å™ªæ‰©æ•£ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆDDGANï¼‰å…è®¸å¿«é€Ÿå›¾åƒç”Ÿæˆï¼ŒåŒæ—¶ä¿æŒä¸åŸå§‹DDMsç›¸å½“çš„å›¾åƒè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19094">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f6779a722948ca836eb04348c7e02fbc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72ea7f2caaced0fc1a2553e53adf9499.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bbec2dc4d2b19b867c16214a9579421.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f141bc5650c59804d3f9a2085de1663.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-770238047ef76dab46461ea22f983730.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cedd86632b8bfe03933c6c1429d22e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f606185b13845e181b63e9500963a7c1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Concept-Steerers-Leveraging-K-Sparse-Autoencoders-for-Controllable-Generations"><a href="#Concept-Steerers-Leveraging-K-Sparse-Autoencoders-for-Controllable-Generations" class="headerlink" title="Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable   Generations"></a>Concept Steerers: Leveraging K-Sparse Autoencoders for Controllable   Generations</h2><p><strong>Authors:Dahye Kim, Deepti Ghadiyaram</strong></p>
<p>Despite the remarkable progress in text-to-image generative models, they are prone to adversarial attacks and inadvertently generate unsafe, unethical content. Existing approaches often rely on fine-tuning models to remove specific concepts, which is computationally expensive, lack scalability, and&#x2F;or compromise generation quality. In this work, we propose a novel framework leveraging k-sparse autoencoders (k-SAEs) to enable efficient and interpretable concept manipulation in diffusion models. Specifically, we first identify interpretable monosemantic concepts in the latent space of text embeddings and leverage them to precisely steer the generation away or towards a given concept (e.g., nudity) or to introduce a new concept (e.g., photographic style). Through extensive experiments, we demonstrate that our approach is very simple, requires no retraining of the base model nor LoRA adapters, does not compromise the generation quality, and is robust to adversarial prompt manipulations. Our method yields an improvement of $\mathbf{20.01%}$ in unsafe concept removal, is effective in style manipulation, and is $\mathbf{\sim5}$x faster than current state-of-the-art. </p>
<blockquote>
<p>å°½ç®¡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å®ƒä»¬å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»ï¼Œå¹¶å¯èƒ½æ— æ„ä¸­ç”Ÿæˆä¸å®‰å…¨ã€ä¸ç¬¦åˆä¼¦ç†çš„å†…å®¹ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¾®è°ƒæ¨¡å‹æ¥æ¶ˆé™¤ç‰¹å®šæ¦‚å¿µï¼Œè¿™è®¡ç®—æˆæœ¬é«˜ã€ç¼ºä¹å¯æ‰©å±•æ€§ï¼Œå¹¶ä¸”&#x2F;æˆ–æŸå®³ç”Ÿæˆè´¨é‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨kç¨€ç–è‡ªç¼–ç å™¨ï¼ˆk-SAEsï¼‰çš„æ–°æ¡†æ¶ï¼Œä»¥å®ç°æ‰©æ•£æ¨¡å‹ä¸­é«˜æ•ˆä¸”å¯è§£é‡Šçš„æ¦‚å¿µæ“ä½œã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨æ–‡æœ¬åµŒå…¥çš„æ½œåœ¨ç©ºé—´ä¸­è¯†åˆ«å¯è§£é‡Šçš„å•è¯­ä¹‰æ¦‚å¿µï¼Œå¹¶åˆ©ç”¨å®ƒä»¬æ¥ç²¾ç¡®æ§åˆ¶ç”Ÿæˆè¿œç¦»æˆ–æœå‘ç»™å®šæ¦‚å¿µï¼ˆä¾‹å¦‚ï¼Œè£¸ä½“ï¼‰ï¼Œæˆ–å¼•å…¥æ–°æ¦‚å¿µï¼ˆä¾‹å¦‚ï¼Œæ‘„å½±é£æ ¼ï¼‰ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•éå¸¸ç®€å•ï¼Œæ— éœ€é‡æ–°è®­ç»ƒåŸºç¡€æ¨¡å‹æˆ–ä½¿ç”¨LoRAé€‚é…å™¨ï¼Œä¸ä¼šæŸå®³ç”Ÿæˆè´¨é‡ï¼Œå¹¶ä¸”å¯¹å¯¹æŠ—æ€§æç¤ºæ“ä½œå…·æœ‰é²æ£’æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸å®‰å…¨æ¦‚å¿µåˆ é™¤æ–¹é¢æé«˜äº†20.01%ï¼Œåœ¨é£æ ¼æ“ä½œæ–¹é¢éå¸¸æœ‰æ•ˆï¼Œå¹¶ä¸”æ˜¯å½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯çš„çº¦5å€é€Ÿåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19066v1">PDF</a> 15 pages, 16 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºåˆ©ç”¨k-ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆk-SAEsï¼‰æ„å»ºæ–°å‹æ¡†æ¶ï¼Œå®ç°åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„é«˜æ•ˆä¸”å¯è§£é‡Šçš„æ¦‚å¿µæ“æ§ã€‚é€šè¿‡è¯†åˆ«æ–‡æœ¬åµŒå…¥æ½œåœ¨ç©ºé—´ä¸­çš„å¯è§£é‡Šå•è¯­ä¹‰æ¦‚å¿µï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç²¾å‡†åœ°å¼•å¯¼ç”Ÿæˆç»“æœè¿œç¦»æˆ–é è¿‘ç‰¹å®šæ¦‚å¿µï¼Œæˆ–å¼•å…¥æ–°æ¦‚å¿µï¼Œå¦‚é¿å…ä¸å®‰å…¨çš„å†…å®¹ç”Ÿæˆã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ç®€å•é«˜æ•ˆï¼Œæ— éœ€é‡æ–°è®­ç»ƒåŸºç¡€æ¨¡å‹æˆ–ä½¿ç”¨LoRAé€‚é…å™¨ï¼Œä¸é™ä½ç”Ÿæˆè´¨é‡ï¼Œä¸”å¯¹å¯¹æŠ—æ€§æç¤ºæ“æ§å…·æœ‰é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹è™½ç„¶å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»ï¼Œä¼šæ— æ„ä¸­ç”Ÿæˆä¸å®‰å…¨ã€ä¸é“å¾·çš„å†…å®¹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥å»é™¤ç‰¹å®šæ¦‚å¿µï¼Œä½†è¿™ç§æ–¹æ³•è®¡ç®—æˆæœ¬é«˜ã€ç¼ºä¹å¯æ‰©å±•æ€§ä¸”å¯èƒ½å½±å“ç”Ÿæˆè´¨é‡ã€‚</li>
<li>æœ¬æ–‡æå‡ºåˆ©ç”¨k-ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆk-SAEsï¼‰çš„æ–°æ¡†æ¶ï¼Œå®ç°åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„æ¦‚å¿µæ“æ§ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿè¯†åˆ«æ–‡æœ¬åµŒå…¥æ½œåœ¨ç©ºé—´ä¸­çš„å¯è§£é‡Šå•è¯­ä¹‰æ¦‚å¿µï¼Œå¹¶ç²¾å‡†åœ°å¼•å¯¼ç”Ÿæˆç»“æœè¿œç¦»æˆ–é è¿‘ç‰¹å®šæ¦‚å¿µæˆ–å¼•å…¥æ–°æ¦‚å¿µã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸å¦¥åç”Ÿæˆè´¨é‡çš„å‰æä¸‹ï¼Œæé«˜äº†ä¸å®‰å…¨æ¦‚å¿µç§»é™¤çš„æ•ˆæœï¼Œè¾¾åˆ°20.01%çš„æ”¹è¿›ï¼Œä¸”åœ¨é£æ ¼æ“æ§æ–¹é¢æœ‰æ•ˆã€‚</li>
<li>ä¸å½“å‰å…ˆè¿›æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•é€Ÿåº¦æé«˜äº†çº¦5å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19066">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ac07eb33e68fc2d771f0cfcb33758122.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57fccec82cfe2db0cc22401ee44efc87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c6856205eddf6b99320e1aad68ffbbf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d43187c9e1c4a6644919964b9562ff1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Rethinking-Diffusion-Posterior-Sampling-From-Conditional-Score-Estimator-to-Maximizing-a-Posterior"><a href="#Rethinking-Diffusion-Posterior-Sampling-From-Conditional-Score-Estimator-to-Maximizing-a-Posterior" class="headerlink" title="Rethinking Diffusion Posterior Sampling: From Conditional Score   Estimator to Maximizing a Posterior"></a>Rethinking Diffusion Posterior Sampling: From Conditional Score   Estimator to Maximizing a Posterior</h2><p><strong>Authors:Tongda Xu, Xiyan Cai, Xinjie Zhang, Xingtong Ge, Dailan He, Ming Sun, Jingjing Liu, Ya-Qin Zhang, Jian Li, Yan Wang</strong></p>
<p>Recent advancements in diffusion models have been leveraged to address inverse problems without additional training, and Diffusion Posterior Sampling (DPS) (Chung et al., 2022a) is among the most popular approaches. Previous analyses suggest that DPS accomplishes posterior sampling by approximating the conditional score. While in this paper, we demonstrate that the conditional score approximation employed by DPS is not as effective as previously assumed, but rather aligns more closely with the principle of maximizing a posterior (MAP). This assertion is substantiated through an examination of DPS on 512x512 ImageNet images, revealing that: 1) DPSâ€™s conditional score estimation significantly diverges from the score of a well-trained conditional diffusion model and is even inferior to the unconditional score; 2) The mean of DPSâ€™s conditional score estimation deviates significantly from zero, rendering it an invalid score estimation; 3) DPS generates high-quality samples with significantly lower diversity. In light of the above findings, we posit that DPS more closely resembles MAP than a conditional score estimator, and accordingly propose the following enhancements to DPS: 1) we explicitly maximize the posterior through multi-step gradient ascent and projection; 2) we utilize a light-weighted conditional score estimator trained with only 100 images and 8 GPU hours. Extensive experimental results indicate that these proposed improvements significantly enhance DPSâ€™s performance. The source code for these improvements is provided in <a target="_blank" rel="noopener" href="https://github.com/tongdaxu/Rethinking-Diffusion-Posterior-Sampling-From-Conditional-Score-Estimator-to-Maximizing-a-Posterior">https://github.com/tongdaxu/Rethinking-Diffusion-Posterior-Sampling-From-Conditional-Score-Estimator-to-Maximizing-a-Posterior</a>. </p>
<blockquote>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹çš„æ–°è¿›å±•å·²è¢«ç”¨äºè§£å†³æ— éœ€é¢å¤–è®­ç»ƒçš„åé—®é¢˜ï¼Œå…¶ä¸­Diffusion Posterior Samplingï¼ˆDPSï¼‰ï¼ˆChungç­‰äººï¼Œ2022aï¼‰æ˜¯æœ€å—æ¬¢è¿çš„æ–¹æ³•ä¹‹ä¸€ã€‚ä¹‹å‰çš„åˆ†æè¡¨æ˜ï¼ŒDPSé€šè¿‡è¿‘ä¼¼æ¡ä»¶åˆ†æ•°æ¥å®ç°åéªŒé‡‡æ ·ã€‚ç„¶è€Œï¼Œæœ¬æ–‡ä¸­æˆ‘ä»¬è¯æ˜DPSæ‰€é‡‡ç”¨çš„æ¡ä»¶åˆ†æ•°è¿‘ä¼¼å¹¶ä¸åƒä¹‹å‰å‡è®¾çš„é‚£æ ·æœ‰æ•ˆï¼Œè€Œæ›´æ¥è¿‘äºæœ€å¤§åéªŒï¼ˆMAPï¼‰åŸåˆ™ã€‚è¿™ä¸€ä¸»å¼ é€šè¿‡å¯¹DPSåœ¨512x512 ImageNetå›¾åƒä¸Šçš„æ£€æŸ¥å¾—åˆ°äº†è¯å®ï¼Œç»“æœæ˜¾ç¤ºï¼š1ï¼‰DPSçš„æ¡ä»¶åˆ†æ•°ä¼°è®¡ä¸è®­ç»ƒè‰¯å¥½çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„åˆ†æ•°å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œç”šè‡³ä¸å¦‚æ— æ¡ä»¶åˆ†æ•°ï¼›2ï¼‰DPSçš„æ¡ä»¶åˆ†æ•°ä¼°è®¡å‡å€¼åç¦»é›¶ï¼Œä½¿å…¶æˆä¸ºæ— æ•ˆçš„åˆ†æ•°ä¼°è®¡ï¼›3ï¼‰DPSç”Ÿæˆçš„æ ·æœ¬è´¨é‡è™½é«˜ï¼Œä½†å¤šæ ·æ€§æ˜¾è‘—é™ä½ã€‚é‰´äºä¸Šè¿°å‘ç°ï¼Œæˆ‘ä»¬è®¤ä¸ºDPSæ›´ç±»ä¼¼äºMAPè€Œéæ¡ä»¶åˆ†æ•°ä¼°è®¡å™¨ï¼Œå¹¶æ®æ­¤å¯¹DPSæå‡ºä»¥ä¸‹æ”¹è¿›ï¼š1ï¼‰æˆ‘ä»¬é€šè¿‡å¤šæ­¥æ¢¯åº¦ä¸Šå‡å’ŒæŠ•å½±æ˜¾å¼åœ°æœ€å¤§åŒ–åéªŒï¼›2ï¼‰æˆ‘ä»¬ä½¿ç”¨ä»…ä½¿ç”¨100å¼ å›¾åƒå’Œ8ä¸ªGPUå°æ—¶è®­ç»ƒçš„è½»é‡çº§æ¡ä»¶åˆ†æ•°ä¼°è®¡å™¨ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ”¹è¿›æ˜¾è‘—æé«˜äº†DPSçš„æ€§èƒ½ã€‚è¿™äº›æ”¹è¿›çš„ä»£ç æºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tongdaxu/Rethinking-Diffusion-Posterior-Sampling-From-Conditional-Score-Estimator-to-Maximizing-a-Posterior%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tongdaxu/Rethinking-Diffusion-Posterior-Sampling-From-Conditional-Score-Estimator-to-Maximizing-a-Posterioræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18913v1">PDF</a> ICLR 2025</p>
<p><strong>æ‘˜è¦</strong><br>    è¿‘æœŸæ‰©æ•£æ¨¡å‹çš„æ–°è¿›å±•è¢«ç”¨æ¥è§£å†³é€†å‘é—®é¢˜è€Œæ— éœ€é¢å¤–è®­ç»ƒï¼Œå…¶ä¸­Diffusion Posterior Samplingï¼ˆDPSï¼‰æ˜¯æœ€å—æ¬¢è¿çš„æ–¹æ³•ä¹‹ä¸€ã€‚æœ¬ç ”ç©¶å‘ç°ï¼ŒDPSæ‰€é‡‡ç”¨çš„æ¡ä»¶åˆ†æ•°è¿‘ä¼¼å¹¶éå¦‚å…ˆå‰å‡è®¾èˆ¬æœ‰æ•ˆï¼Œè€Œæ˜¯æ›´è´´è¿‘æœ€å¤§åŒ–åéªŒï¼ˆMAPï¼‰åŸç†ã€‚é€šè¿‡å¯¹DPSåœ¨512x512 ImageNetå›¾åƒä¸Šçš„ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°ï¼š1ï¼‰DPSçš„æ¡ä»¶åˆ†æ•°ä¼°è®¡ä¸è®­ç»ƒè‰¯å¥½çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„åˆ†æ•°å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œç”šè‡³ä¸åŠæ— æ¡ä»¶åˆ†æ•°ï¼›2ï¼‰DPSçš„æ¡ä»¶åˆ†æ•°ä¼°è®¡å‡å€¼åç¦»é›¶ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªæ— æ•ˆçš„åˆ†æ•°ä¼°è®¡ï¼›3ï¼‰DPSç”Ÿæˆçš„æ ·æœ¬è´¨é‡é«˜ï¼Œä½†å¤šæ ·æ€§æ˜¾è‘—é™ä½ã€‚é‰´äºä»¥ä¸Šå‘ç°ï¼Œæˆ‘ä»¬è®¤ä¸ºDPSæ›´ç±»ä¼¼äºMAPè€Œéæ¡ä»¶åˆ†æ•°ä¼°è®¡å™¨ï¼Œå¹¶æ®æ­¤æå‡ºä»¥ä¸‹å¢å¼ºæªæ–½ï¼š1ï¼‰é€šè¿‡å¤šæ­¥æ¢¯åº¦ä¸Šå‡å’ŒæŠ•å½±æ˜¾å¼æœ€å¤§åŒ–åéªŒï¼›2ï¼‰ä½¿ç”¨è½»é‡çº§çš„æ¡ä»¶åˆ†æ•°ä¼°è®¡å™¨ï¼Œä»…ä½¿ç”¨100å¼ å›¾åƒå’Œ8 GPUå°æ—¶è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ”¹è¿›æ˜¾è‘—æé«˜äº†DPSçš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Diffusion Posterior Sampling (DPS) åœ¨å¤„ç†é€†å‘é—®é¢˜æ—¶ï¼Œå…¶æ¡ä»¶åˆ†æ•°ä¼°è®¡å¹¶ä¸å¦‚å…ˆå‰ç ”ç©¶æ‰€ç¤ºæœ‰æ•ˆã€‚</li>
<li>DPSçš„æ¡ä»¶åˆ†æ•°ä¼°è®¡ä¸è‰¯å¥½çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„åˆ†æ•°å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œç”šè‡³ä¸å¦‚æ— æ¡ä»¶åˆ†æ•°ä¼°è®¡ã€‚</li>
<li>DPSçš„æ¡ä»¶åˆ†æ•°ä¼°è®¡å‡å€¼æ˜¾è‘—åç¦»é›¶ï¼Œè¡¨æ˜å…¶åˆ†æ•°ä¼°è®¡æ— æ•ˆã€‚</li>
<li>DPSè™½ç„¶èƒ½ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ï¼Œä½†æ ·æœ¬å¤šæ ·æ€§æ˜¾è‘—é™ä½ã€‚</li>
<li>DPSæ›´æ¥è¿‘æœ€å¤§åŒ–åéªŒï¼ˆMAPï¼‰åŸç†è€Œéæ¡ä»¶åˆ†æ•°ä¼°è®¡ã€‚</li>
<li>æå‡ºäº†é€šè¿‡å¤šæ­¥æ¢¯åº¦ä¸Šå‡å’ŒæŠ•å½±æ¥æ˜¾å¼æœ€å¤§åŒ–åéªŒçš„æ–¹æ³•ï¼Œä»¥æé«˜DPSæ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨è½»é‡çº§çš„æ¡ä»¶åˆ†æ•°ä¼°è®¡å™¨ï¼Œä»…éœ€è¦å°‘é‡å›¾åƒå’Œè®¡ç®—èµ„æºå°±èƒ½è¿›è¡Œæœ‰æ•ˆè®­ç»ƒï¼Œè¿›ä¸€æ­¥æå‡äº†DPSçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18913">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bb2ed6b9e6c96a1f7709ffe8198a78ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5944881582eabf7fd3cbb4db25c18a20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af5b50c9224852ea448ebb3ae92d60c4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f12ae6b4695ab9cd0f7f00125209987.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Distorting-Embedding-Space-for-Safety-A-Defense-Mechanism-for-Adversarially-Robust-Diffusion-Models"><a href="#Distorting-Embedding-Space-for-Safety-A-Defense-Mechanism-for-Adversarially-Robust-Diffusion-Models" class="headerlink" title="Distorting Embedding Space for Safety: A Defense Mechanism for   Adversarially Robust Diffusion Models"></a>Distorting Embedding Space for Safety: A Defense Mechanism for   Adversarially Robust Diffusion Models</h2><p><strong>Authors:Jaesin Ahn, Heechul Jung</strong></p>
<p>Text-to-image diffusion models show remarkable generation performance following text prompts, but risk generating Not Safe For Work (NSFW) contents from unsafe prompts. Existing approaches, such as prompt filtering or concept unlearning, fail to defend against adversarial attacks while maintaining benign image quality. In this paper, we propose a novel approach called Distorting Embedding Space (DES), a text encoder-based defense mechanism that effectively tackles these issues through innovative embedding space control. DES transforms unsafe embeddings, extracted from a text encoder using unsafe prompts, toward carefully calculated safe embedding regions to prevent unsafe contents generation, while reproducing the original safe embeddings. DES also neutralizes the nudity embedding, extracted using prompt &#96;&#96;nudityâ€, by aligning it with neutral embedding to enhance robustness against adversarial attacks. These methods ensure both robust defense and high-quality image generation. Additionally, DES can be adopted in a plug-and-play manner and requires zero inference overhead, facilitating its deployment. Extensive experiments on diverse attack types, including black-box and white-box scenarios, demonstrate DESâ€™s state-of-the-art performance in both defense capability and benign image generation quality. Our model is available at <a target="_blank" rel="noopener" href="https://github.com/aei13/DES">https://github.com/aei13/DES</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬æç¤ºä¸‹æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ç”Ÿæˆæ€§èƒ½ï¼Œä½†å­˜åœ¨ä»ä¸å®‰å…¨æç¤ºç”Ÿæˆä¸é€‚åˆå·¥ä½œåœºæ‰€ï¼ˆNSFWï¼‰å†…å®¹çš„é£é™©ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚æç¤ºè¿‡æ»¤æˆ–æ¦‚å¿µé—å¿˜ï¼Œæœªèƒ½æœ‰æ•ˆå¯¹æŠ—æ”»å‡»åŒæ—¶ä¿æŒè‰¯æ€§å›¾åƒè´¨é‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºâ€œæ‰­æ›²åµŒå…¥ç©ºé—´â€ï¼ˆDESï¼‰çš„æ–°æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ–‡æœ¬ç¼–ç å™¨çš„é˜²å¾¡æœºåˆ¶ï¼Œé€šè¿‡åˆ›æ–°çš„åµŒå…¥ç©ºé—´æ§åˆ¶æœ‰æ•ˆåœ°è§£å†³äº†è¿™äº›é—®é¢˜ã€‚DESå°†ä¸å®‰å…¨åµŒå…¥ï¼ˆé€šè¿‡æ–‡æœ¬ç¼–ç å™¨ä½¿ç”¨ä¸å®‰å…¨æç¤ºæå–ï¼‰è½¬æ¢ä¸ºç²¾å¿ƒè®¡ç®—çš„å®‰å…¨åµŒå…¥åŒºåŸŸï¼Œä»¥é˜²æ­¢ä¸å®‰å…¨å†…å®¹çš„ç”Ÿæˆï¼ŒåŒæ—¶é‡ç°åŸå§‹çš„å®‰å…¨åµŒå…¥ã€‚DESé€šè¿‡å¯¹é½ä½¿ç”¨â€œè£¸éœ²â€æç¤ºæå–çš„è£¸éœ²åµŒå…¥åˆ°ä¸­æ€§åµŒå…¥ä¸­ï¼Œå¢å¼ºäº†å¯¹å¯¹æŠ—æ€§æ”»å‡»çš„ç¨³å¥æ€§ã€‚è¿™äº›æ–¹æ³•ç¡®ä¿äº†å¼ºå¤§çš„é˜²å¾¡å’Œé«˜è´¨é‡çš„å›¾åƒç”Ÿæˆã€‚æ­¤å¤–ï¼ŒDESé‡‡ç”¨å³æ’å³ç”¨æ–¹å¼ï¼Œæ— éœ€ä»»ä½•æ¨ç†å¼€é”€ï¼Œä¾¿äºéƒ¨ç½²ã€‚å¯¹å„ç§æ”»å‡»ç±»å‹è¿›è¡Œçš„å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬é»‘ç®±å’Œç™½ç®±åœºæ™¯ï¼Œè¯æ˜äº†DESåœ¨é˜²å¾¡èƒ½åŠ›å’Œè‰¯æ€§å›¾åƒç”Ÿæˆè´¨é‡æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/aei13/DES%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/aei13/DESæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18877v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå‡ºè‰²çš„å›¾åƒï¼Œä½†å­˜åœ¨ä»ä¸å®‰å…¨æç¤ºç”Ÿæˆä¸é€‚åˆå·¥ä½œåœºåˆï¼ˆNSFWï¼‰å†…å®¹çš„é£é™©ã€‚ç°æœ‰æ–¹æ³•å¦‚æç¤ºè¿‡æ»¤æˆ–æ¦‚å¿µé—å¿˜ï¼Œæ— æ³•åœ¨å¯¹æŠ—æ”»å‡»ä¸­é˜²å¾¡åŒæ—¶ä¿æŒè‰¯æ€§å›¾åƒè´¨é‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œæ‰­æ›²åµŒå…¥ç©ºé—´ï¼ˆDESï¼‰â€çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡åˆ›æ–°çš„åµŒå…¥ç©ºé—´æ§åˆ¶æŠ€æœ¯æœ‰æ•ˆè§£å†³è¿™äº›é—®é¢˜ã€‚DESé€šè¿‡æ–‡æœ¬ç¼–ç å™¨æå–ä¸å®‰å…¨åµŒå…¥ï¼Œå¹¶å°†å…¶å‘ç²¾å¿ƒè®¡ç®—çš„å®‰å…¨åµŒå…¥åŒºåŸŸè½¬å˜ï¼Œé˜²æ­¢ä¸å®‰å…¨å†…å®¹çš„ç”Ÿæˆï¼ŒåŒæ—¶å†ç°åŸå§‹å®‰å…¨åµŒå…¥ã€‚DESè¿˜é€šè¿‡å°†â€œè£¸éœ²â€åµŒå…¥ä¸ä¸­æ€§åµŒå…¥å¯¹é½ï¼Œå¢å¼ºäº†å¯¹å¯¹æŠ—æ”»å‡»çš„ç¨³å¥æ€§ã€‚è¿™äº›æ–¹æ³•ç¡®ä¿äº†å¼ºå¤§çš„é˜²å¾¡èƒ½åŠ›å’Œé«˜è´¨é‡å›¾åƒç”Ÿæˆã€‚æ­¤å¤–ï¼ŒDESé‡‡ç”¨å³æ’å³ç”¨æ–¹å¼ï¼Œæ— éœ€æ¨ç†å»¶è¿Ÿï¼Œä¾¿äºéƒ¨ç½²ã€‚åœ¨ä¸åŒç±»å‹çš„æ”»å‡»åœºæ™¯ï¼ŒåŒ…æ‹¬é»‘ç®±å’Œç™½ç®±åœºæ™¯ä¸­ï¼ŒDESåœ¨é˜²å¾¡èƒ½åŠ›å’Œè‰¯æ€§å›¾åƒç”Ÿæˆè´¨é‡æ–¹é¢éƒ½è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå›¾åƒï¼Œä½†å­˜åœ¨ç”ŸæˆNSFWå†…å®¹çš„é£é™©ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¯¹æŠ—æ”»å‡»ä¸­çš„é˜²å¾¡èƒ½åŠ›æœ‰é™ï¼Œæ— æ³•åŒæ—¶ä¿æŒè‰¯æ€§å›¾åƒè´¨é‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•â€œæ‰­æ›²åµŒå…¥ç©ºé—´ï¼ˆDESï¼‰â€ï¼Œé€šè¿‡æ§åˆ¶åµŒå…¥ç©ºé—´æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>DESé€šè¿‡æ–‡æœ¬ç¼–ç å™¨æå–ä¸å®‰å…¨åµŒå…¥ï¼Œç„¶åå°†å…¶è½¬å˜å‘å®‰å…¨åµŒå…¥åŒºåŸŸï¼Œé˜²æ­¢ä¸å®‰å…¨å†…å®¹çš„ç”Ÿæˆã€‚</li>
<li>DESèƒ½å¤Ÿå¢å¼ºå¯¹å¯¹æŠ—æ”»å‡»çš„ç¨³å¥æ€§ï¼Œé€šè¿‡å¯¹é½â€œè£¸éœ²â€åµŒå…¥ä¸ä¸­æ€§åµŒå…¥å®ç°ã€‚</li>
<li>DESæ–¹æ³•æ—¢å…·æœ‰å¼ºå¤§çš„é˜²å¾¡èƒ½åŠ›ï¼Œåˆèƒ½ä¿æŒé«˜è´¨é‡å›¾åƒç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18877">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3831516e9b05fae1d5cd3cfbff494e08.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a45cb1acdfa060332f9e7c796db1ca0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5933278d0d79c13601451902e456df9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a16c84e03776408fdcd063bcf37ebaf5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54b0c7aa7533363904d363801c4c6f48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-427b87233c84d6b3857c2acef604ce2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c845c6bc35fb58ca6e1b6b9c0c6a43e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SAeUron-Interpretable-Concept-Unlearning-in-Diffusion-Models-with-Sparse-Autoencoders"><a href="#SAeUron-Interpretable-Concept-Unlearning-in-Diffusion-Models-with-Sparse-Autoencoders" class="headerlink" title="SAeUron: Interpretable Concept Unlearning in Diffusion Models with   Sparse Autoencoders"></a>SAeUron: Interpretable Concept Unlearning in Diffusion Models with   Sparse Autoencoders</h2><p><strong>Authors:Bartosz CywiÅ„ski, Kamil Deja</strong></p>
<p>Diffusion models, while powerful, can inadvertently generate harmful or undesirable content, raising significant ethical and safety concerns. Recent machine unlearning approaches offer potential solutions but often lack transparency, making it difficult to understand the changes they introduce to the base model. In this work, we introduce SAeUron, a novel method leveraging features learned by sparse autoencoders (SAEs) to remove unwanted concepts in text-to-image diffusion models. First, we demonstrate that SAEs, trained in an unsupervised manner on activations from multiple denoising timesteps of the diffusion model, capture sparse and interpretable features corresponding to specific concepts. Building on this, we propose a feature selection method that enables precise interventions on model activations to block targeted content while preserving overall performance. Evaluation with the competitive UnlearnCanvas benchmark on object and style unlearning highlights SAeUronâ€™s state-of-the-art performance. Moreover, we show that with a single SAE, we can remove multiple concepts simultaneously and that in contrast to other methods, SAeUron mitigates the possibility of generating unwanted content, even under adversarial attack. Code and checkpoints are available at: <a target="_blank" rel="noopener" href="https://github.com/cywinski/SAeUron">https://github.com/cywinski/SAeUron</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹è™½ç„¶åŠŸèƒ½å¼ºå¤§ï¼Œä½†å¯èƒ½ä¼šä¸ç»æ„åœ°ç”Ÿæˆæœ‰å®³æˆ–ä¸è‰¯å†…å®¹ï¼Œå¼•å‘ä¸¥é‡çš„ä¼¦ç†å’Œå®‰å…¨æ‹…å¿§ã€‚æœ€è¿‘çš„æœºå™¨éå­¦ä¹ ï¼ˆunlearningï¼‰æ–¹æ³•æä¾›äº†æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼Œä½†é€šå¸¸ç¼ºä¹é€æ˜åº¦ï¼Œä½¿å¾—éš¾ä»¥äº†è§£å®ƒä»¬å¯¹åŸºç¡€æ¨¡å‹æ‰€åšçš„æ”¹å˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†SAeUronï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSparse Autoencodersï¼ŒSAEï¼‰å­¦ä¹ åˆ°çš„ç‰¹å¾æ¥å»é™¤æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ä¸éœ€è¦çš„æ¦‚å¿µçš„æ–°æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯æ˜äº†åœ¨æ‰©æ•£æ¨¡å‹çš„å¤šä¸ªå»å™ªæ—¶é—´æ­¥é•¿çš„æ¿€æ´»ä¸Šé‡‡ç”¨æ— ç›‘ç£æ–¹å¼è®­ç»ƒçš„SAEèƒ½å¤Ÿæ•è·å¯¹åº”äºç‰¹å®šæ¦‚å¿µçš„ç¨€ç–å’Œå¯è§£é‡Šçš„ç‰¹å¾ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¨¡å‹æ¿€æ´»è¿›è¡Œç²¾ç¡®å¹²é¢„æ¥é˜»æ­¢ç›®æ ‡å†…å®¹ï¼ŒåŒæ—¶ä¿æŒæ•´ä½“æ€§èƒ½ã€‚ä½¿ç”¨é’ˆå¯¹å¯¹è±¡å’Œé£æ ¼éå­¦ä¹ çš„ç«äº‰æ€§UnlearnCanvasåŸºå‡†æµ‹è¯•ï¼Œçªå‡ºäº†SAeUronçš„é¡¶å°–æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸€ä¸ªSAEå¯ä»¥åŒæ—¶å»é™¤å¤šä¸ªæ¦‚å¿µï¼Œè€Œä¸”ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒSAeUronå‡è½»äº†ç”Ÿæˆä¸éœ€è¦å†…å®¹çš„é£é™©ï¼Œå³ä½¿åœ¨å¯¹æŠ—æ€§æ”»å‡»ä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä»£ç å’Œæ£€æŸ¥ç‚¹å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/cywinski/SAeUron%E3%80%82">https://github.com/cywinski/SAeUronã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18052v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹è™½ç„¶å¼ºå¤§ï¼Œä½†å¯èƒ½æ— æ„ä¸­ç”Ÿæˆæœ‰å®³æˆ–ä¸å¯å–çš„å†…å®¹ï¼Œå¼•å‘ä¸¥é‡çš„ä¼¦ç†å’Œå®‰å…¨æ‹…å¿§ã€‚è™½ç„¶æœ€è¿‘çš„æœºå™¨é—å¿˜æ–¹æ³•æä¾›äº†æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å®ƒä»¬é€šå¸¸ç¼ºä¹é€æ˜åº¦ï¼Œéš¾ä»¥äº†è§£å®ƒä»¬å¯¹åŸºç¡€æ¨¡å‹æ‰€åšçš„æ”¹å˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•SAeUronï¼Œå®ƒåˆ©ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰çš„ç‰¹å¾ï¼Œå»é™¤æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„ä¸æƒ³è¦çš„æ¦‚å¿µã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯æ˜ï¼Œåœ¨æ‰©æ•£æ¨¡å‹çš„å¤šæ­¥å»å™ªæ—¶é—´çš„æ¿€æ´»ä¸Šï¼Œä»¥æ— ç›‘ç£çš„æ–¹å¼è®­ç»ƒçš„SAEå¯ä»¥æ•è·å¯¹åº”ç‰¹å®šæ¦‚å¿µç¨€ç–ä¸”å¯è§£é‡Šçš„ç‰¹å¾ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œå¯ä»¥å¯¹æ¨¡å‹æ¿€æ´»è¿›è¡Œç²¾ç¡®å¹²é¢„ï¼Œä»¥é˜»æ­¢ç›®æ ‡å†…å®¹ï¼ŒåŒæ—¶ä¿æŒæ•´ä½“æ€§èƒ½ã€‚åœ¨å¯¹è±¡å’Œé£æ ¼é—å¿˜æ–¹é¢ï¼Œä½¿ç”¨ç«äº‰æ€§çš„UnlearnCanvasåŸºå‡†æµ‹è¯•çªå‡ºäº†SAeUronçš„å…ˆè¿›æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸€ä¸ªSAEå¯ä»¥åŒæ—¶å»é™¤å¤šä¸ªæ¦‚å¿µï¼Œè€Œä¸”ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒSAeUronå‡è½»äº†ç”Ÿæˆä¸æƒ³è¦å†…å®¹çš„å¯èƒ½æ€§ï¼Œå³ä½¿åœ¨æ•Œå¯¹æ”»å‡»ä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä»£ç å’Œæ£€æŸ¥ç‚¹å¯ç”¨äºï¼š<a target="_blank" rel="noopener" href="https://github.com/cywinski/SAeUron%E3%80%82">https://github.com/cywinski/SAeUronã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å¯èƒ½ç”Ÿæˆæœ‰å®³æˆ–ä¸å¯å–çš„å†…å®¹ï¼Œå¼•å‘ä¼¦ç†å’Œå®‰å…¨æ‹…å¿§ã€‚</li>
<li>æœ€è¿‘çš„æœºå™¨é—å¿˜æ–¹æ³•è™½ä¸ºè§£å†³æ­¤é—®é¢˜æä¾›æ–¹æ¡ˆï¼Œä½†æ™®éç¼ºä¹é€æ˜åº¦ã€‚</li>
<li>SAeUronæ–¹æ³•åˆ©ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨çš„ç‰¹å¾æ¥å»é™¤æ‰©æ•£æ¨¡å‹ä¸­çš„ä¸æƒ³è¦çš„æ¦‚å¿µã€‚</li>
<li>SAEèƒ½å¤Ÿåœ¨æ— ç›‘ç£æ–¹å¼ä¸‹æ•è·å¯¹åº”ç‰¹å®šæ¦‚å¿µçš„ç¨€ç–ä¸”å¯è§£é‡Šçš„ç‰¹å¾ã€‚</li>
<li>SAeUroné€šè¿‡ç‰¹å¾é€‰æ‹©æ–¹æ³•è¿›è¡Œç²¾ç¡®å¹²é¢„ï¼Œèƒ½é˜»æ­¢ç›®æ ‡å†…å®¹ç”ŸæˆåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>SAeUronåœ¨UnlearnCanvasåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºå…¶å…ˆè¿›æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-481413b16d88f9895fba3e76623b62bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f5cf15733e87581e167240ce41bc731.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6018e35370be15bff4ff36e8b0e26b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca8e205864c0df34a603b7f68d369ac7.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="IC-Portrait-In-Context-Matching-for-View-Consistent-Personalized-Portrait"><a href="#IC-Portrait-In-Context-Matching-for-View-Consistent-Personalized-Portrait" class="headerlink" title="IC-Portrait: In-Context Matching for View-Consistent Personalized   Portrait"></a>IC-Portrait: In-Context Matching for View-Consistent Personalized   Portrait</h2><p><strong>Authors:Han Yang, Enis Simsar, Sotiris Anagnostidis, Yanlong Zang, Thomas Hofmann, Ziwei Liu</strong></p>
<p>Existing diffusion models show great potential for identity-preserving generation. However, personalized portrait generation remains challenging due to the diversity in user profiles, including variations in appearance and lighting conditions. To address these challenges, we propose IC-Portrait, a novel framework designed to accurately encode individual identities for personalized portrait generation. Our key insight is that pre-trained diffusion models are fast learners (e.g.,100 ~ 200 steps) for in-context dense correspondence matching, which motivates the two major designs of our IC-Portrait framework. Specifically, we reformulate portrait generation into two sub-tasks: 1) Lighting-Aware Stitching: we find that masking a high proportion of the input image, e.g., 80%, yields a highly effective self-supervisory representation learning of reference image lighting. 2) View-Consistent Adaptation: we leverage a synthetic view-consistent profile dataset to learn the in-context correspondence. The reference profile can then be warped into arbitrary poses for strong spatial-aligned view conditioning. Coupling these two designs by simply concatenating latents to form ControlNet-like supervision and modeling, enables us to significantly enhance the identity preservation fidelity and stability. Extensive evaluations demonstrate that IC-Portrait consistently outperforms existing state-of-the-art methods both quantitatively and qualitatively, with particularly notable improvements in visual qualities. Furthermore, IC-Portrait even demonstrates 3D-aware relighting capabilities. </p>
<blockquote>
<p>ç°æœ‰çš„æ‰©æ•£æ¨¡å‹åœ¨èº«ä»½ä¿ç•™ç”Ÿæˆæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºç”¨æˆ·èµ„æ–™çš„å¤šæ ·æ€§ï¼ŒåŒ…æ‹¬å¤–è§‚å’Œå…‰ç…§æ¡ä»¶çš„å·®å¼‚ï¼Œä¸ªæ€§åŒ–è‚–åƒç”Ÿæˆä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†IC-Portraitï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å‡†ç¡®ç¼–ç ä¸ªäººèº«ä»½ç”¨äºä¸ªæ€§åŒ–è‚–åƒç”Ÿæˆçš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å¯¹äºä¸Šä¸‹æ–‡ä¸­çš„å¯†é›†å¯¹åº”åŒ¹é…æ˜¯å¿«é€Ÿå­¦ä¹ è€…ï¼ˆä¾‹å¦‚ï¼Œ100ã€œ200æ­¥ï¼‰ï¼Œè¿™æ¿€å‘äº†æˆ‘ä»¬IC-Portraitæ¡†æ¶çš„ä¸¤ä¸ªä¸»è¦è®¾è®¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†è‚–åƒç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºä¸¤ä¸ªå­ä»»åŠ¡ï¼š1ï¼‰å…‰ç…§æ„ŸçŸ¥æ‹¼æ¥ï¼šæˆ‘ä»¬å‘ç°ï¼Œé®æŒ¡è¾“å…¥å›¾åƒçš„é«˜æ¯”ä¾‹éƒ¨åˆ†ï¼ˆä¾‹å¦‚80ï¼…ï¼‰ï¼Œå¯ä»¥æœ‰æ•ˆåœ°è¿›è¡Œå‚è€ƒå›¾åƒå…‰ç…§çš„è‡ªæˆ‘ç›‘ç£è¡¨ç¤ºå­¦ä¹ ã€‚2ï¼‰è§†å›¾ä¸€è‡´é€‚åº”ï¼šæˆ‘ä»¬åˆ©ç”¨åˆæˆè§†å›¾ä¸€è‡´çš„è½®å»“æ•°æ®é›†æ¥å­¦ä¹ ä¸Šä¸‹æ–‡ä¸­çš„å¯¹åº”å…³ç³»ã€‚ç„¶åå¯ä»¥å°†å‚è€ƒè½®å»“å˜å½¢ä¸ºä»»æ„å§¿åŠ¿ï¼Œä»¥å®ç°å¼ºå¤§çš„ç©ºé—´å¯¹é½è§†å›¾æ¡ä»¶ã€‚é€šè¿‡ç®€å•åœ°è¿æ¥æ½œåœ¨ç©ºé—´ä»¥å½¢æˆControlNetç±»ä¼¼çš„ç›‘ç£å’Œå»ºæ¨¡ï¼Œå¯ä»¥å¤§å¤§å¢å¼ºèº«ä»½ä¿ç•™çš„ä¿çœŸåº¦å’Œç¨³å®šæ€§ã€‚å¤§é‡è¯„ä¼°è¡¨æ˜ï¼ŒIC-Portraitåœ¨å®šé‡å’Œå®šæ€§ä¸Šå‡ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œåœ¨è§†è§‰å“è´¨ä¸Šæœ‰ç‰¹åˆ«æ˜¾è‘—çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒIC-Portraitç”šè‡³å±•ç¤ºäº†3Dæ„ŸçŸ¥çš„é‡ç…§æ˜èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.17159v2">PDF</a> technical report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºIC-Portraitçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºè§£å†³ä¸ªæ€§åŒ–è‚–åƒç”Ÿæˆä¸­çš„èº«ä»½ä¿ç•™æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ä¸¤ä¸ªå­ä»»åŠ¡ï¼šå…‰ç…§æ„ŸçŸ¥æ‹¼æ¥å’Œè§†å›¾ä¸€è‡´é€‚åº”ï¼Œå®ç°å‡†ç¡®ç¼–ç ä¸ªä½“èº«ä»½ã€‚è¿™ç§æ–¹æ³•é€šè¿‡é«˜åº¦æœ‰æ•ˆçš„è‡ªç›‘ç£å­¦ä¹ ï¼Œä»¥åŠå¯¹å‚è€ƒå›¾åƒå…‰ç…§å’Œè§†è§’ä¸€è‡´æ€§çš„é€‚åº”ï¼Œæ˜¾è‘—æé«˜äº†èº«ä»½ä¿ç•™çš„ä¿çœŸåº¦å’Œç¨³å®šæ€§ã€‚IC-Portraitåœ¨è§†è§‰è´¨é‡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æå‡ï¼Œå¹¶ä¸”å…·æœ‰3Dæ„ŸçŸ¥çš„é‡æ–°ç…§æ˜èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>IC-Portraitæ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œé’ˆå¯¹ä¸ªæ€§åŒ–è‚–åƒç”Ÿæˆä¸­çš„èº«ä»½ä¿ç•™æŒ‘æˆ˜ã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªå­ä»»åŠ¡ï¼šå…‰ç…§æ„ŸçŸ¥æ‹¼æ¥å’Œè§†å›¾ä¸€è‡´é€‚åº”ï¼Œä»¥å‡†ç¡®ç¼–ç ä¸ªä½“èº«ä»½ã€‚</li>
<li>é€šè¿‡é«˜åº¦æœ‰æ•ˆçš„è‡ªç›‘ç£å­¦ä¹ ï¼Œå®ç°å‚è€ƒå›¾åƒå…‰ç…§çš„è‡ªæˆ‘å­¦ä¹ ã€‚</li>
<li>åˆ©ç”¨åˆæˆè§†å›¾ä¸€è‡´çš„è½®å»“æ•°æ®é›†è¿›è¡Œä¸Šä¸‹æ–‡å¯¹åº”å­¦ä¹ ï¼Œä½¿å‚è€ƒè½®å»“å¯ä»¥å˜å½¢ä¸ºä»»æ„å§¿æ€ï¼Œå®ç°ç©ºé—´å¯¹é½è§†å›¾æ¡ä»¶ã€‚</li>
<li>ç®€å•çš„æ½œåœ¨è¿æ¥æ–¹æ³•ï¼ˆå¦‚ControlNetï¼‰æ˜¾è‘—æé«˜èº«ä»½ä¿ç•™çš„ä¿çœŸåº¦å’Œç¨³å®šæ€§ã€‚</li>
<li>IC-Portraitåœ¨è§†è§‰è´¨é‡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æå‡ï¼Œå¹¶å…·å¤‡3Dæ„ŸçŸ¥çš„é‡æ–°ç…§æ˜èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.17159">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-759a6f19606d270b41c19371eac83029.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fae033f722c11754e250050fab37ce49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-181c421d2e16684151e8b7a5e915cf66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3effc876bcfaae1951eea6ed552d3b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad8455a2766e6494eabe2810c31bf337.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Dfilled-Repurposing-Edge-Enhancing-Diffusion-for-Guided-DSM-Void-Filling"><a href="#Dfilled-Repurposing-Edge-Enhancing-Diffusion-for-Guided-DSM-Void-Filling" class="headerlink" title="Dfilled: Repurposing Edge-Enhancing Diffusion for Guided DSM Void   Filling"></a>Dfilled: Repurposing Edge-Enhancing Diffusion for Guided DSM Void   Filling</h2><p><strong>Authors:Daniel Panangian, Ksenia Bittner</strong></p>
<p>Digital Surface Models (DSMs) are essential for accurately representing Earthâ€™s topography in geospatial analyses. DSMs capture detailed elevations of natural and manmade features, crucial for applications like urban planning, vegetation studies, and 3D reconstruction. However, DSMs derived from stereo satellite imagery often contain voids or missing data due to occlusions, shadows, and lowsignal areas. Previous studies have primarily focused on void filling for digital elevation models (DEMs) and Digital Terrain Models (DTMs), employing methods such as inverse distance weighting (IDW), kriging, and spline interpolation. While effective for simpler terrains, these approaches often fail to handle the intricate structures present in DSMs. To overcome these limitations, we introduce Dfilled, a guided DSM void filling method that leverages optical remote sensing images through edge-enhancing diffusion. Dfilled repurposes deep anisotropic diffusion models, which originally designed for super-resolution tasks, to inpaint DSMs. Additionally, we utilize Perlin noise to create inpainting masks that mimic natural void patterns in DSMs. Experimental evaluations demonstrate that Dfilled surpasses traditional interpolation methods and deep learning approaches in DSM void filling tasks. Both quantitative and qualitative assessments highlight the methodâ€™s ability to manage complex features and deliver accurate, visually coherent results. </p>
<blockquote>
<p>æ•°å­—è¡¨é¢æ¨¡å‹ï¼ˆDSMsï¼‰åœ¨åœ°ç†ç©ºé—´åˆ†æä¸­å‡†ç¡®è¡¨ç¤ºåœ°çƒåœ°å½¢æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚DSMæ•æ‰è‡ªç„¶å’Œäººä¸ºç‰¹å¾çš„è¯¦ç»†é«˜ç¨‹ï¼Œå¯¹äºåŸå¸‚è§„åˆ’ã€æ¤è¢«ç ”ç©¶å’Œ3Dé‡å»ºç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä»ç«‹ä½“å«æ˜Ÿå½±åƒæ´¾ç”Ÿçš„DSMsé€šå¸¸ç”±äºé®æŒ¡ã€é˜´å½±å’Œä½ä¿¡å·åŒºåŸŸè€ŒåŒ…å«ç©ºæ´æˆ–ç¼ºå¤±æ•°æ®ã€‚å…ˆå‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ•°å­—é«˜ç¨‹æ¨¡å‹ï¼ˆDEMï¼‰å’Œæ•°å­—åœ°å½¢æ¨¡å‹ï¼ˆDTMï¼‰çš„ç©ºæ´å¡«å……ä¸Šï¼Œé‡‡ç”¨çš„æ–¹æ³•åŒ…æ‹¬åè·ç¦»åŠ æƒï¼ˆIDWï¼‰ã€å…‹é‡Œé‡‘æ³•å’Œæ ·æ¡æ’å€¼ã€‚è¿™äº›æ–¹æ³•åœ¨ç®€å•åœ°å½¢ä¸Šè™½ç„¶æœ‰æ•ˆï¼Œä½†å¾€å¾€éš¾ä»¥å¤„ç†DSMä¸­å­˜åœ¨çš„å¤æ‚ç»“æ„ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†Dfilledï¼Œè¿™æ˜¯ä¸€ç§æœ‰æŒ‡å¯¼çš„DSMç©ºæ´å¡«å……æ–¹æ³•ï¼Œå®ƒé€šè¿‡è¾¹ç¼˜å¢å¼ºæ‰©æ•£åˆ©ç”¨å…‰å­¦é¥æ„Ÿå›¾åƒã€‚Dfilledé‡æ–°è®¾è®¡äº†åŸæœ¬ç”¨äºè¶…åˆ†è¾¨ç‡ä»»åŠ¡çš„æ·±åº¦å„å‘å¼‚æ€§æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºå¡«å……DSMã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨Perlinå™ªå£°ç”Ÿæˆå¡«å……æ©è†œï¼Œä»¥æ¨¡ä»¿DSMä¸­çš„è‡ªç„¶ç©ºæ´æ¨¡å¼ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œåœ¨DSMç©ºæ´å¡«å……ä»»åŠ¡ä¸­ï¼ŒDfilledè¶…è¶Šäº†ä¼ ç»Ÿçš„æ’å€¼æ–¹æ³•å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚å®šé‡å’Œå®šæ€§è¯„ä¼°éƒ½çªå‡ºäº†è¯¥æ–¹æ³•ç®¡ç†å¤æ‚ç‰¹å¾çš„èƒ½åŠ›ï¼Œå¹¶èƒ½æä¾›å‡†ç¡®ã€è§†è§‰è¿è´¯çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15440v2">PDF</a> Accepted to IEEE&#x2F;CVF Winter Conference on Applications of Computer   Vision Workshops (WACVW)</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æ•°å­—è¡¨é¢æ¨¡å‹ï¼ˆDSMsï¼‰åœ¨åœ°ç†ç©ºé—´åˆ†æä¸­å‡†ç¡®è¡¨ç¤ºåœ°çƒåœ°å½¢è‡³å…³é‡è¦ã€‚DSMsæ•æ‰è‡ªç„¶å’Œäººé€ ç‰¹å¾çš„è¯¦ç»†é«˜ç¨‹ï¼Œå¯¹äºåŸå¸‚è§„åˆ’ã€æ¤è¢«ç ”ç©¶å’Œ3Dé‡å»ºç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä»ç«‹ä½“å«æ˜Ÿå½±åƒæ´¾ç”Ÿçš„DSMsé€šå¸¸ç”±äºé®æŒ¡ã€é˜´å½±å’Œä½ä¿¡å·åŒºåŸŸè€ŒåŒ…å«ç©ºæ´æˆ–ç¼ºå¤±æ•°æ®ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ•°å­—é«˜ç¨‹æ¨¡å‹ï¼ˆDEMsï¼‰å’Œæ•°å­—åœ°å½¢æ¨¡å‹ï¼ˆDTMsï¼‰çš„ç©ºæ´å¡«å……ä¸Šï¼Œé‡‡ç”¨åè·ç¦»åŠ æƒï¼ˆIDWï¼‰ã€å…‹é‡Œæ ¼å’Œæ ·æ¡æ’å€¼ç­‰æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•å¯¹äºç®€å•åœ°å½¢æœ‰æ•ˆï¼Œä½†éš¾ä»¥å¤„ç†DSMä¸­çš„å¤æ‚ç»“æ„ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Dfilledï¼Œä¸€ç§åˆ©ç”¨è¾¹ç¼˜å¢å¼ºæ‰©æ•£å¼•å¯¼DSMç©ºæ´å¡«å……çš„æ–¹æ³•ã€‚Dfilledåˆ©ç”¨å…‰å­¦é¥æ„Ÿå›¾åƒï¼Œå¹¶é‡æ–°è®¾è®¡æ·±åº¦å„å‘å¼‚æ€§æ‰©æ•£æ¨¡å‹ä»¥å¡«å……DSMã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨Perlinå™ªå£°åˆ›å»ºæ¨¡ä»¿DSMä¸­è‡ªç„¶ç©ºæ´æ¨¡å¼çš„æ’å€¼æ©æ¨¡ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒDfilledåœ¨DSMç©ºæ´å¡«å……ä»»åŠ¡ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿæ’å€¼æ–¹æ³•å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚å®šé‡å’Œå®šæ€§è¯„ä¼°å‡è¯æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿç®¡ç†å¤æ‚ç‰¹å¾å¹¶æä¾›å‡†ç¡®ã€è§†è§‰è¿è´¯çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ•°å­—è¡¨é¢æ¨¡å‹ï¼ˆDSMsï¼‰åœ¨åœ°ç†ç©ºé—´åˆ†æä¸­éå¸¸é‡è¦ï¼Œèƒ½å‡†ç¡®è¡¨ç¤ºåœ°çƒåœ°å½¢ç‰¹å¾ã€‚</li>
<li>ä»ç«‹ä½“å«æ˜Ÿå½±åƒç”Ÿæˆçš„DSMså¸¸å«æœ‰ç©ºæ´æˆ–ç¼ºå¤±æ•°æ®ï¼Œè¿™ä¼šå½±å“å…¶å‡†ç¡®æ€§ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å¦‚åè·ç¦»åŠ æƒï¼ˆIDWï¼‰ã€å…‹é‡Œæ ¼å’Œæ ·æ¡æ’å€¼åœ¨ç®€å•åœ°å½¢ä¸­æœ‰æ•ˆï¼Œä½†éš¾ä»¥å¤„ç†å¤æ‚çš„DSMç»“æ„ã€‚</li>
<li>å¼•å…¥çš„Dfilledæ–¹æ³•åˆ©ç”¨å…‰å­¦é¥æ„Ÿå›¾åƒå’Œæ·±åº¦å„å‘å¼‚æ€§æ‰©æ•£æ¨¡å‹è¿›è¡ŒDSMç©ºæ´å¡«å……ã€‚</li>
<li>Dfilledä½¿ç”¨Perlinå™ªå£°æ¥æ¨¡ä»¿DSMä¸­çš„è‡ªç„¶ç©ºæ´æ¨¡å¼ï¼Œæé«˜äº†å¡«å……çš„å‡†ç¡®åº¦ã€‚</li>
<li>å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒDfilledåœ¨DSMç©ºæ´å¡«å……ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œæ·±åº¦å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15440">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-51deff1ebc82e9dbac3bfa86c526f035.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-292cef999959e8924af92e7532be1e04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ff2a3261420ce68569adeb580483906.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1be4c41117b0821db6ef49e720e5cc6f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27dfd24eb86d102018e1e43af52852e2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Solving-Blind-Inverse-Problems-Adaptive-Diffusion-Models-for-Motion-corrected-Sparse-view-4DCT"><a href="#Solving-Blind-Inverse-Problems-Adaptive-Diffusion-Models-for-Motion-corrected-Sparse-view-4DCT" class="headerlink" title="Solving Blind Inverse Problems: Adaptive Diffusion Models for   Motion-corrected Sparse-view 4DCT"></a>Solving Blind Inverse Problems: Adaptive Diffusion Models for   Motion-corrected Sparse-view 4DCT</h2><p><strong>Authors:Antoine De Paepe, Alexandre Bousse, ClÃ©mentine Phung-Ngoc, Dimitris Visvikis</strong></p>
<p>Four-dimensional computed tomography (4DCT) is essential for medical imaging applications like radiotherapy, which demand precise respiratory motion representation. Traditional methods for reconstructing 4DCT data suffer from artifacts and noise, especially in sparse-view, low-dose contexts. Motion-corrected (MC) reconstruction is a blind inverse problem that we propose to solve with a novel diffusion model (DM) framework that calibrates an adaptive unknown forward model for motion correction. Furthermore, we used a wavelet diffusion model (WDM) to address computational cost and memory usage. By leveraging the prior probability distribution function (PDF) from the DMs, we enhance the joint reconstruction and motion estimation (JRM) process, improving image quality and preserving resolution. Experiments on extended cardiac-torso (XCAT) phantom data demonstrate that our method outperforms existing techniques, yielding artifact-free, high-resolution reconstructions even under irregular breathing conditions. These results showcase the potential of combining DMs with motion correction to advance sparse-view 4DCT imaging. </p>
<blockquote>
<p>å››ç»´è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆ4DCTï¼‰åœ¨åŒ»å­¦æˆåƒåº”ç”¨ï¼ˆå¦‚æ”¾å°„æ²»ç–—ï¼‰ä¸­è‡³å…³é‡è¦ï¼Œè¿™è¦æ±‚ç²¾ç¡®è¡¨ç¤ºå‘¼å¸è¿åŠ¨ã€‚ä¼ ç»Ÿçš„é‡å»º4DCTæ•°æ®çš„æ–¹æ³•åœ¨ç¨€ç–è§†è§’ã€ä½å‰‚é‡ç¯å¢ƒä¸‹ä¼šå‡ºç°ä¼ªå½±å’Œå™ªå£°ã€‚è¿åŠ¨æ ¡æ­£ï¼ˆMCï¼‰é‡å»ºæ˜¯ä¸€ä¸ªç›²é€†é—®é¢˜ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ä¸€ç§æ–°å‹æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰æ¡†æ¶æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥æ¡†æ¶å¯ä»¥æ ¡å‡†ç”¨äºè¿åŠ¨æ ¡æ­£çš„è‡ªé€‚åº”æœªçŸ¥å‰å‘æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨å°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDMï¼‰æ¥è§£å†³è®¡ç®—æˆæœ¬å’Œå†…å­˜ä½¿ç”¨é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒæ¦‚ç‡åˆ†å¸ƒå‡½æ•°ï¼ˆPDFï¼‰ï¼Œæˆ‘ä»¬å¢å¼ºäº†è”åˆé‡å»ºå’Œè¿åŠ¨ä¼°è®¡ï¼ˆJRMï¼‰è¿‡ç¨‹ï¼Œæé«˜äº†å›¾åƒè´¨é‡å¹¶ä¿æŒäº†åˆ†è¾¨ç‡ã€‚åœ¨æ‰©å±•çš„å¿ƒè„ä½“æ¨¡ï¼ˆXCATï¼‰æ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå³ä½¿åœ¨ä¸è§„åˆ™å‘¼å¸æ¡ä»¶ä¸‹ä¹Ÿèƒ½äº§ç”Ÿæ— ä¼ªå½±ã€é«˜åˆ†è¾¨ç‡çš„é‡å»ºç»“æœã€‚è¿™äº›ç»“æœå±•ç¤ºäº†å°†æ‰©æ•£æ¨¡å‹ä¸è¿åŠ¨æ ¡æ­£ç›¸ç»“åˆåœ¨ç¨€ç–è§†è§’çš„4DCTæˆåƒä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12249v3">PDF</a> 4 pages, 2 figures, 1 table</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å››ç»´è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆ4DCTï¼‰åœ¨åŒ»ç–—æˆåƒåº”ç”¨ä¸­çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ”¾å°„æ²»ç–—é¢†åŸŸä¸­å¯¹ç²¾ç¡®å‘¼å¸è¿åŠ¨è¡¨ç¤ºçš„éœ€æ±‚ã€‚é’ˆå¯¹ä¼ ç»Ÿé‡å»º4DCTæ•°æ®çš„æ–¹æ³•å­˜åœ¨çš„ä¼ªå½±å’Œå™ªå£°é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨€ç–è§†è§’å’Œä½å‰‚é‡æƒ…å†µä¸‹ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰æ¡†æ¶çš„è¿åŠ¨æ ¡æ­£ï¼ˆMCï¼‰é‡å»ºæ–¹æ³•ã€‚åˆ©ç”¨å°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDMï¼‰è§£å†³è®¡ç®—æˆæœ¬å’Œå†…å­˜ä½¿ç”¨é—®é¢˜ï¼Œé€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒæ¦‚ç‡åˆ†å¸ƒå‡½æ•°ï¼ˆPDFï¼‰æ¥å¢å¼ºè”åˆé‡å»ºå’Œè¿åŠ¨ä¼°è®¡ï¼ˆJRMï¼‰è¿‡ç¨‹ï¼Œä»è€Œæé«˜å›¾åƒè´¨é‡å’Œä¿æŒåˆ†è¾¨ç‡ã€‚åœ¨XCAT Phantomæ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå³ä½¿åœ¨ä¸è§„åˆ™å‘¼å¸æ¡ä»¶ä¸‹ä¹Ÿèƒ½å®ç°æ— ä¼ªå½±ã€é«˜åˆ†è¾¨ç‡é‡å»ºã€‚è¿™äº›ç»“æœå±•ç¤ºäº†å°†æ‰©æ•£æ¨¡å‹ä¸è¿åŠ¨æ ¡æ­£ç›¸ç»“åˆåœ¨ç¨€ç–è§†è§’4DCTæˆåƒä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å››ç»´è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆ4DCTï¼‰åœ¨åŒ»ç–—æˆåƒä¸­è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ç²¾ç¡®å‘¼å¸è¿åŠ¨è¡¨ç¤ºçš„æ”¾å°„æ²»ç–—é¢†åŸŸã€‚</li>
<li>ä¼ ç»Ÿé‡å»º4DCTæ•°æ®çš„æ–¹æ³•åœ¨ç¨€ç–è§†è§’å’Œä½å‰‚é‡æƒ…å†µä¸‹å­˜åœ¨ä¼ªå½±å’Œå™ªå£°é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰æ¡†æ¶çš„è¿åŠ¨æ ¡æ­£ï¼ˆMCï¼‰é‡å»ºæ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨å°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDMï¼‰ä»¥æé«˜è®¡ç®—æ•ˆç‡å’Œé™ä½å†…å­˜ä½¿ç”¨ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒæ¦‚ç‡åˆ†å¸ƒå‡½æ•°ï¼ˆPDFï¼‰æ¥å¢å¼ºè”åˆé‡å»ºå’Œè¿åŠ¨ä¼°è®¡ï¼ˆJRMï¼‰è¿‡ç¨‹ã€‚</li>
<li>åœ¨XCAT Phantomæ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå®ç°äº†æ— ä¼ªå½±ã€é«˜åˆ†è¾¨ç‡é‡å»ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12249">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-71bf4d01f439d0c6f34bb7bf95dcfa92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab41b8c1771e184bca1b15ff876f63cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c296c1efcd7322057a0ecc79246b675b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db7ac02aa3519902029d6cdf1d0e59d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0278663466a2b8fc1f0922356d09fabb.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Hidden-in-the-Noise-Two-Stage-Robust-Watermarking-for-Images"><a href="#Hidden-in-the-Noise-Two-Stage-Robust-Watermarking-for-Images" class="headerlink" title="Hidden in the Noise: Two-Stage Robust Watermarking for Images"></a>Hidden in the Noise: Two-Stage Robust Watermarking for Images</h2><p><strong>Authors:Kasra Arabi, Benjamin Feuer, R. Teal Witter, Chinmay Hegde, Niv Cohen</strong></p>
<p>As the quality of image generators continues to improve, deepfakes become a topic of considerable societal debate. Image watermarking allows responsible model owners to detect and label their AI-generated content, which can mitigate the harm. Yet, current state-of-the-art methods in image watermarking remain vulnerable to forgery and removal attacks. This vulnerability occurs in part because watermarks distort the distribution of generated images, unintentionally revealing information about the watermarking techniques.   In this work, we first demonstrate a distortion-free watermarking method for images, based on a diffusion modelâ€™s initial noise. However, detecting the watermark requires comparing the initial noise reconstructed for an image to all previously used initial noises. To mitigate these issues, we propose a two-stage watermarking framework for efficient detection. During generation, we augment the initial noise with generated Fourier patterns to embed information about the group of initial noises we used. For detection, we (i) retrieve the relevant group of noises, and (ii) search within the given group for an initial noise that might match our image. This watermarking approach achieves state-of-the-art robustness to forgery and removal against a large battery of attacks. </p>
<blockquote>
<p>éšç€å›¾åƒç”Ÿæˆå™¨çš„è´¨é‡ä¸æ–­æé«˜ï¼Œæ·±åº¦ä¼ªé€ æˆä¸ºç¤¾ä¼šçƒ­è®®çš„è¯é¢˜ã€‚å›¾åƒæ°´å°å…è®¸è´Ÿè´£ä»»çš„æ¨¡å‹æ‰€æœ‰è€…æ£€æµ‹å’Œæ ‡è®°å…¶AIç”Ÿæˆçš„å†…å®¹ï¼Œè¿™å¯ä»¥å‡è½»æŸå®³ã€‚ç„¶è€Œï¼Œå½“å‰å›¾åƒæ°´å°çš„æœ€å…ˆè¿›æ–¹æ³•ä»ç„¶å®¹æ˜“å—åˆ°ä¼ªé€ å’Œç§»é™¤æ”»å‡»ã€‚è¿™ç§è„†å¼±æ€§éƒ¨åˆ†æ˜¯å› ä¸ºæ°´å°ä¼šæ‰­æ›²ç”Ÿæˆå›¾åƒçš„åˆ†å¸ƒï¼Œä»è€Œæ— æ„é—´æ³„éœ²æœ‰å…³æ°´å°æŠ€æœ¯çš„ä¿¡æ¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå±•ç¤ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„åˆå§‹å™ªå£°çš„æ— æŸæ°´å°æ–¹æ³•ã€‚ç„¶è€Œï¼Œæ£€æµ‹æ°´å°éœ€è¦æ¯”è¾ƒå›¾åƒçš„é‡å»ºåˆå§‹å™ªå£°ä¸æ‰€æœ‰ä¹‹å‰ä½¿ç”¨çš„åˆå§‹å™ªå£°ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºé«˜æ•ˆæ£€æµ‹çš„ä¸¤é˜¶æ®µæ°´å°æ¡†æ¶ã€‚åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨ç”Ÿæˆçš„å‚…ç«‹å¶æ¨¡å¼å¢å¼ºåˆå§‹å™ªå£°ï¼Œä»¥åµŒå…¥æœ‰å…³æˆ‘ä»¬æ‰€ç”¨åˆå§‹å™ªå£°ç»„çš„ä¿¡æ¯ã€‚å¯¹äºæ£€æµ‹ï¼Œæˆ‘ä»¬ï¼ˆiï¼‰æ£€ç´¢ç›¸å…³çš„å™ªå£°ç»„ï¼Œï¼ˆii)åœ¨ç»™å®šç»„å†…æœç´¢å¯èƒ½ä¸æˆ‘ä»¬çš„å›¾åƒåŒ¹é…çš„åˆå§‹å™ªå£°ã€‚è¿™ç§æ°´å°æ–¹æ³•å®ç°äº†å¯¹ä¸€ç³»åˆ—æ”»å‡»çš„é«˜åº¦ç¨³å¥æ€§å’Œé˜²ä¼ªæ€§ï¼Œä¸”åœ¨å»é™¤æ”»å‡»æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04653v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å›¾åƒæ°´å°æŠ€æœ¯åœ¨é˜²æ­¢æ·±åº¦ä¼ªé€ å›¾åƒæ–¹é¢çš„åº”ç”¨åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹å½“å‰å›¾åƒæ°´å°æŠ€æœ¯æ˜“è¢«ç¯¡æ”¹å’Œç§»é™¤çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹åˆå§‹å™ªå£°çš„æ— æŸæ°´å°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸¤ä¸ªé˜¶æ®µå®ç°é«˜æ•ˆæ£€æµ‹ï¼Œç¬¬ä¸€é˜¶æ®µåœ¨ç”Ÿæˆæ—¶å¢å¼ºåˆå§‹å™ªå£°åµŒå…¥ä¿¡æ¯ï¼Œç¬¬äºŒé˜¶æ®µåœ¨æ£€æµ‹æ—¶æœç´¢åŒ¹é…çš„åˆå§‹å™ªå£°ã€‚è¯¥æ–¹æ³•å®ç°äº†å¯¹å¤§é‡æ”»å‡»çš„é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒç”Ÿæˆå™¨è´¨é‡çš„æå‡å¼•å‘äº†æ·±åº¦ä¼ªé€ çš„ç¤¾ä¼šè®®é¢˜ï¼Œå›¾åƒæ°´å°æŠ€æœ¯å…è®¸æ¨¡å‹æ‰€æœ‰è€…å¯¹å…¶AIç”Ÿæˆå†…å®¹è¿›è¡Œæ£€æµ‹å’Œæ ‡è®°ï¼Œå‡è½»æ½œåœ¨å±å®³ã€‚</li>
<li>å½“å‰å›¾åƒæ°´å°æŠ€æœ¯å­˜åœ¨æ˜“è¢«ç¯¡æ”¹å’Œç§»é™¤çš„ç¼ºé™·ï¼Œéƒ¨åˆ†åŸå› æ˜¯æ°´å°ä¼šå¹²æ‰°ç”Ÿæˆå›¾åƒçš„åˆ†å¸ƒï¼Œæ³„éœ²æ°´å°æŠ€æœ¯ä¿¡æ¯ã€‚</li>
<li>åŸºäºæ‰©æ•£æ¨¡å‹çš„åˆå§‹å™ªå£°ï¼Œæå‡ºäº†ä¸€ç§æ— æŸæ°´å°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆå›¾åƒæ—¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç‰¹æ€§åµŒå…¥æ°´å°ä¿¡æ¯ã€‚</li>
<li>æ°´å°æ£€æµ‹éœ€è¦æ¯”è¾ƒå›¾åƒçš„åˆå§‹å™ªå£°ä¸æ‰€æœ‰å·²ä½¿ç”¨çš„åˆå§‹å™ªå£°ï¼Œå› æ­¤æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ°´å°æ£€æµ‹æ¡†æ¶ä»¥æé«˜æ•ˆç‡ã€‚</li>
<li>åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œé€šè¿‡ç”Ÿæˆçš„å‚…é‡Œå¶æ¨¡å¼åµŒå…¥å…³äºä½¿ç”¨çš„åˆå§‹å™ªå£°ç»„çš„ä¿¡æ¯ã€‚</li>
<li>åœ¨ç¬¬äºŒé˜¶æ®µï¼Œé¦–å…ˆæ£€ç´¢ç›¸å…³çš„å™ªå£°ç»„ï¼Œç„¶ååœ¨ç»™å®šçš„å™ªå£°ç»„å†…æœç´¢å¯èƒ½ä¸å›¾åƒåŒ¹é…çš„åˆå§‹å™ªå£°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04653">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-520fc7bf1e378e363acf893fedbd81bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-386598134522fd952f67c0570a8317c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f25e1cc3e8f07f401178a94cd840e207.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-585f38819c156839de00a92b17bbad14.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0d15147d40d07fa34ccfd53c158d89ad.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LANTERN-Accelerating-Visual-Autoregressive-Models-with-Relaxed-Speculative-Decoding"><a href="#LANTERN-Accelerating-Visual-Autoregressive-Models-with-Relaxed-Speculative-Decoding" class="headerlink" title="LANTERN: Accelerating Visual Autoregressive Models with Relaxed   Speculative Decoding"></a>LANTERN: Accelerating Visual Autoregressive Models with Relaxed   Speculative Decoding</h2><p><strong>Authors:Doohyuk Jang, Sihwan Park, June Yong Yang, Yeonsung Jung, Jihun Yun, Souvik Kundu, Sung-Yub Kim, Eunho Yang</strong></p>
<p>Auto-Regressive (AR) models have recently gained prominence in image generation, often matching or even surpassing the performance of diffusion models. However, one major limitation of AR models is their sequential nature, which processes tokens one at a time, slowing down generation compared to models like GANs or diffusion-based methods that operate more efficiently. While speculative decoding has proven effective for accelerating LLMs by generating multiple tokens in a single forward, its application in visual AR models remains largely unexplored. In this work, we identify a challenge in this setting, which we term \textit{token selection ambiguity}, wherein visual AR models frequently assign uniformly low probabilities to tokens, hampering the performance of speculative decoding. To overcome this challenge, we propose a relaxed acceptance condition referred to as LANTERN that leverages the interchangeability of tokens in latent space. This relaxation restores the effectiveness of speculative decoding in visual AR models by enabling more flexible use of candidate tokens that would otherwise be prematurely rejected. Furthermore, by incorporating a total variation distance bound, we ensure that these speed gains are achieved without significantly compromising image quality or semantic coherence. Experimental results demonstrate the efficacy of our method in providing a substantial speed-up over speculative decoding. In specific, compared to a na&quot;ive application of the state-of-the-art speculative decoding, LANTERN increases speed-ups by $\mathbf{1.75}\times$ and $\mathbf{1.82}\times$, as compared to greedy decoding and random sampling, respectively, when applied to LlamaGen, a contemporary visual AR model. </p>
<blockquote>
<p>è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹æœ€è¿‘åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸè·å¾—äº†æ˜¾è‘—çš„é‡è¦æ€§ï¼Œå…¶æ€§èƒ½ç»å¸¸ä¸æ‰©æ•£æ¨¡å‹ç›¸åŒ¹é…ç”šè‡³è¶…è¶Šã€‚ç„¶è€Œï¼ŒARæ¨¡å‹çš„ä¸€ä¸ªä¸»è¦å±€é™æ€§æ˜¯å®ƒä»¬çš„é¡ºåºæ€§ï¼Œå³ä¸€æ¬¡åªå¤„ç†ä¸€ä¸ªæ ‡è®°ï¼Œå¯¼è‡´ä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æˆ–åŸºäºæ‰©æ•£çš„æ–¹æ³•ç­‰æ›´é«˜æ•ˆçš„æ¨¡å‹ç›¸æ¯”ï¼Œç”Ÿæˆé€Ÿåº¦è¾ƒæ…¢ã€‚å°½ç®¡çŒœæµ‹è§£ç å·²ç»è¯æ˜å¯ä»¥é€šè¿‡ä¸€æ¬¡å‰å‘ç”Ÿæˆå¤šä¸ªæ ‡è®°æ¥åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½†å…¶åœ¨è§†è§‰ARæ¨¡å‹ä¸­çš„åº”ç”¨ä»ç„¶æœªè¢«å¹¿æ³›æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç¡®å®šäº†åœ¨è¿™ç§æƒ…å†µä¸‹çš„ä¸€é¡¹æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæ ‡è®°é€‰æ‹©æ­§ä¹‰â€ï¼Œå…¶ä¸­è§†è§‰ARæ¨¡å‹ç»å¸¸ä¸ºæ ‡è®°åˆ†é…å‡åŒ€ä¸”ä½çš„æ¦‚ç‡ï¼Œé˜»ç¢äº†çŒœæµ‹è§£ç çš„æ€§èƒ½ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç§°ä¸ºLANTERNçš„æ”¾å®½æ¥å—æ¡ä»¶çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ½œåœ¨ç©ºé—´ä¸­æ ‡è®°çš„å¯äº’æ¢æ€§ã€‚è¿™ç§æ”¾æ¾æ¢å¤äº†çŒœæµ‹è§£ç åœ¨è§†è§‰ARæ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡æ›´çµæ´»åœ°åˆ©ç”¨å€™é€‰æ ‡è®°ï¼ˆå¦åˆ™ä¼šè¢«è¿‡æ—©æ‹’ç»ï¼‰ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥æ€»å˜å¼‚è·ç¦»ç•Œé™ï¼Œæˆ‘ä»¬ç¡®ä¿è¿™äº›é€Ÿåº¦æå‡æ˜¯åœ¨ä¸æŸå®³å›¾åƒè´¨é‡æˆ–è¯­ä¹‰è¿è´¯æ€§çš„æƒ…å†µä¸‹å®ç°çš„ã€‚å®éªŒç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯¹äºæä¾›çŒœæµ‹è§£ç çš„å®è´¨æ€§åŠ é€Ÿéå¸¸æœ‰æ•ˆã€‚å…·ä½“æ¥è¯´ï¼Œä¸å¯¹æœ€å…ˆè¿›çš„çŒœæµ‹è§£ç çš„æœ´ç´ åº”ç”¨ç›¸æ¯”ï¼ŒLANTERNåœ¨åº”ç”¨äºå½“ä»£è§†è§‰ARæ¨¡å‹LlamaGenæ—¶ï¼Œä¸è´ªå¿ƒè§£ç å’Œéšæœºé‡‡æ ·ç›¸æ¯”ï¼Œåˆ†åˆ«å°†é€Ÿåº¦æé«˜äº†1.75å€å’Œ1.82å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03355v2">PDF</a> 30 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<pre><code>è¿‘æœŸè‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå…¶æ€§èƒ½ç”šè‡³å¯ä¸æ‰©æ•£æ¨¡å‹ç›¸åª²ç¾ã€‚ç„¶è€Œï¼ŒARæ¨¡å‹å­˜åœ¨é¡ºåºå¤„ç†æ ‡è®°çš„å±€é™æ€§ï¼Œå¯¼è‡´ç”Ÿæˆé€Ÿåº¦è¾ƒæ…¢ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é‡åˆ°äº†ä¸€ç§åä¸ºâ€œæ ‡è®°é€‰æ‹©æ¨¡ç³Šæ€§â€çš„æŒ‘æˆ˜ï¼Œè§†è§‰ARæ¨¡å‹ç»å¸¸ä¸ºæ ‡è®°åˆ†é…å‡åŒ€çš„ä½æ¦‚ç‡ï¼Œé˜»ç¢äº†è§£ç é€Ÿåº¦çš„æå‡ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç§°ä¸ºLANTERNçš„å®½æ¾æ¥å—æ¡ä»¶ï¼Œåˆ©ç”¨æ½œåœ¨ç©ºé—´ä¸­æ ‡è®°çš„å¯äº’æ¢æ€§ã€‚é€šè¿‡å¼•å…¥æ€»å˜å¼‚è·ç¦»è¾¹ç•Œï¼Œç¡®ä¿äº†è¿™äº›åŠ é€Ÿå¹¶ä¸ä¼šæŸå®³å›¾åƒè´¨é‡æˆ–è¯­ä¹‰è¿è´¯æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŠ é€Ÿè§£ç æ–¹é¢æ•ˆæœæ˜¾è‘—ã€‚ä¸å½“å‰æœ€ä½³ç­–ç•¥ç›¸æ¯”ï¼ŒLANTERNåœ¨åº”ç”¨äºè§†è§‰ARæ¨¡å‹LlamaGenæ—¶ï¼Œé€Ÿåº¦æå‡åˆ†åˆ«è¾¾åˆ°1.75å€å’Œ1.82å€ã€‚æ­¤æ–¹æ³•çš„å®æ–½æ˜¾è‘—æå‡äº†è§†è§‰ARæ¨¡å‹çš„æ€§èƒ½ä¸æ•ˆç‡ã€‚
 
**Key Takeaways**
 
1. ARæ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºå…¶é¡ºåºå¤„ç†æ ‡è®°çš„ç‰¹æ€§ï¼Œå…¶ç”Ÿæˆé€Ÿåº¦è¾ƒæ…¢ã€‚è¿™é™åˆ¶äº†å…¶åœ¨æŸäº›åº”ç”¨ä¸­çš„ä½¿ç”¨æ•ˆç‡ã€‚
 
2. åœ¨è§†è§‰ARæ¨¡å‹ä¸­å‡ºç°äº†â€œæ ‡è®°é€‰æ‹©æ¨¡ç³Šæ€§â€çš„æŒ‘æˆ˜ï¼Œå³æ¨¡å‹ç»å¸¸ä¸ºæ ‡è®°åˆ†é…å‡åŒ€çš„ä½æ¦‚ç‡ï¼Œå¯¼è‡´è§£ç æ•ˆç‡ä½ä¸‹ã€‚è¿™æˆä¸ºæé«˜è§£ç é€Ÿåº¦çš„å…³é”®éš¾é¢˜ã€‚ä¸ºåº”å¯¹è¿™ä¸€é—®é¢˜éœ€è¦ä¸€ç§æ–°å‹çš„è§£å†³æ–¹æ¡ˆæ¥æé«˜æ¨¡å‹çš„æ•ˆç‡å¹¶é™ä½å¯¹è§£ç®—èµ„æºçš„æ¶ˆè€—éœ€æ±‚ã€‚è¿™ä¸ªé—®é¢˜çš„è§£å†³å°†æˆä¸ºæ¨åŠ¨å›¾åƒç”ŸæˆæŠ€æœ¯å‘å±•çš„é‡è¦ä¸€æ­¥ã€‚å¯¹äºè¯¥é—®é¢˜å°šæœªæœ‰å……åˆ†çš„æ¢ç´¢ä¸ç ”ç©¶ç›®å‰å¯¹äºå¦‚ä½•è§£å†³è¿™ä¸€é—®é¢˜çš„ç ”ç©¶è¿˜å¤„äºåˆçº§é˜¶æ®µå¯¹äºæœªæ¥å¯èƒ½å‡ºç°çš„ç ”ç©¶æ–¹å‘å…·æœ‰æé«˜çš„æ½œåŠ›ä»¥åŠæ¢è®¨ç©ºé—´å·¨å¤§çš„å®ç”¨ä»·å€¼æ„å‘³ç€è¿™é¡¹ç ”ç©¶çš„è¿›æ­¥æœ‰æœ›å¤§å¹…æå‡æ•´ä¸ªå›¾åƒç”Ÿæˆé¢†åŸŸçš„æ€§èƒ½å¹¶å¸¦åŠ¨å…¶å‘å‰å‘å±•å¯¹äºè§†è§‰ARæ¨¡å‹çš„æ€§èƒ½æå‡å…·æœ‰æ·±è¿œå½±å“æœªæ¥çš„ç ”ç©¶éœ€è¦è¿›ä¸€æ­¥æ¢ç´¢å’Œæ”¹è¿›è¯¥é¢†åŸŸçš„ç›¸å…³æŠ€æœ¯ä»¥å®ç°æ›´é«˜æ•ˆçš„å›¾åƒç”Ÿæˆè¿™ä¹Ÿæ˜¯è¯¥æŠ€æœ¯ç ”ç©¶çš„æ·±è¿œæ„ä¹‰æ‰€åœ¨å¸Œæœ›é€šè¿‡è¿›ä¸€æ­¥ç ”ç©¶èƒ½å¤Ÿå¸®åŠ©ä¼˜åŒ–æ ‡è®°é€‰æ‹©å’Œå†³ç­–æœºåˆ¶åŒæ—¶å……åˆ†ç ”ç©¶å„ä¸ªæ€§èƒ½æŒ‡æ ‡ä»¥æœŸä¼˜åŒ–å›¾åƒç”ŸæˆæŠ€æœ¯å¹¶æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03355">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7afbad530fa2b66719ea6f8585e0248b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7dd16a5b23fd3b95a155b78cecb79ac4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b3829000d3ce9a98626169e1ce5ed16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6298cf0b4c5228ab82e116fb9c17a0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d0acc9d9aba839f20cb49b68b2bc385.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Video-Restoration-and-Enhancement-Using-Pre-Trained-Image-Diffusion-Model"><a href="#Zero-Shot-Video-Restoration-and-Enhancement-Using-Pre-Trained-Image-Diffusion-Model" class="headerlink" title="Zero-Shot Video Restoration and Enhancement Using Pre-Trained Image   Diffusion Model"></a>Zero-Shot Video Restoration and Enhancement Using Pre-Trained Image   Diffusion Model</h2><p><strong>Authors:Cong Cao, Huanjing Yue, Xin Liu, Jingyu Yang</strong></p>
<p>Diffusion-based zero-shot image restoration and enhancement models have achieved great success in various tasks of image restoration and enhancement. However, directly applying them to video restoration and enhancement results in severe temporal flickering artifacts. In this paper, we propose the first framework for zero-shot video restoration and enhancement based on the pre-trained image diffusion model. By replacing the spatial self-attention layer with the proposed short-long-range (SLR) temporal attention layer, the pre-trained image diffusion model can take advantage of the temporal correlation between frames. We further propose temporal consistency guidance, spatial-temporal noise sharing, and an early stopping sampling strategy to improve temporally consistent sampling. Our method is a plug-and-play module that can be inserted into any diffusion-based image restoration or enhancement methods to further improve their performance. Experimental results demonstrate the superiority of our proposed method. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/cao-cong/ZVRD">https://github.com/cao-cong/ZVRD</a>. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„é›¶æ ·æœ¬å›¾åƒä¿®å¤å’Œå¢å¼ºæ¨¡å‹åœ¨å›¾åƒä¿®å¤å’Œå¢å¼ºçš„å„ç§ä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå°†å…¶ç›´æ¥åº”ç”¨äºè§†é¢‘ä¿®å¤å’Œå¢å¼ºä¼šå¯¼è‡´ä¸¥é‡çš„ä¸´æ—¶é—ªçƒä¼ªå½±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºé¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹çš„é¦–ä¸ªé›¶æ ·æœ¬è§†é¢‘ä¿®å¤å’Œå¢å¼ºæ¡†æ¶ã€‚é€šè¿‡ç”¨æ‰€æå‡ºçš„çŸ­é•¿ç¨‹ï¼ˆSLRï¼‰ä¸´æ—¶æ³¨æ„åŠ›å±‚æ›¿æ¢ç©ºé—´è‡ªæ³¨æ„åŠ›å±‚ï¼Œé¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹å¯ä»¥åˆ©ç”¨å¸§ä¹‹é—´çš„ä¸´æ—¶ç›¸å…³æ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸´æ—¶ä¸€è‡´æ€§æŒ‡å¯¼ã€æ—¶ç©ºå™ªå£°å…±äº«å’Œæ—©æœŸåœæ­¢é‡‡æ ·ç­–ç•¥ï¼Œä»¥æ”¹å–„æ—¶é—´ä¸€è‡´çš„é‡‡æ ·ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—ï¼Œå¯ä»¥æ’å…¥ä»»ä½•åŸºäºæ‰©æ•£çš„å›¾åƒä¿®å¤æˆ–å¢å¼ºæ–¹æ³•æ¥è¿›ä¸€æ­¥æé«˜å…¶æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/cao-cong/ZVRD%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/cao-cong/ZVRDä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.01960v4">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨é›¶æ ·æœ¬å›¾åƒä¿®å¤å’Œå¢å¼ºä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†ç›´æ¥åº”ç”¨äºè§†é¢‘ä¿®å¤å’Œå¢å¼ºä¼šäº§ç”Ÿä¸¥é‡çš„æ—¶ç©ºé—ªçƒä¼ªå½±ã€‚æœ¬æ–‡é¦–æ¬¡æå‡ºäº†åŸºäºé¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬è§†é¢‘ä¿®å¤å’Œå¢å¼ºæ¡†æ¶ã€‚é€šè¿‡æ›¿æ¢ç©ºé—´è‡ªæ³¨æ„åŠ›å±‚ä¸ºæ‰€æå‡ºçš„é•¿çŸ­ç¨‹ï¼ˆSLRï¼‰æ—¶é—´æ³¨æ„åŠ›å±‚ï¼Œåˆ©ç”¨å¸§é—´çš„æ—¶åºç›¸å…³æ€§ã€‚è¿›ä¸€æ­¥æå‡ºäº†æ—¶é—´ä¸€è‡´æ€§æŒ‡å¯¼ã€æ—¶ç©ºå™ªå£°å…±äº«å’Œæ—©æœŸåœæ­¢é‡‡æ ·ç­–ç•¥æ¥æ”¹å–„æ—¶åºä¸€è‡´æ€§é‡‡æ ·ã€‚è¯¥æ–¹æ³•æ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—ï¼Œå¯æ’å…¥ä»»ä½•åŸºäºæ‰©æ•£çš„å›¾åƒä¿®å¤æˆ–å¢å¼ºæ–¹æ³•ä¸­ï¼Œè¿›ä¸€æ­¥æé«˜å…¶æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨é›¶æ ·æœ¬å›¾åƒä¿®å¤å’Œå¢å¼ºä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†ç›´æ¥åº”ç”¨äºè§†é¢‘ä¼šå‡ºç°æ—¶ç©ºé—ªçƒé—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†é¦–ä¸ªåŸºäºé¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬è§†é¢‘ä¿®å¤å’Œå¢å¼ºæ¡†æ¶ã€‚</li>
<li>é€šè¿‡å¼•å…¥é•¿çŸ­ç¨‹ï¼ˆSLRï¼‰æ—¶é—´æ³¨æ„åŠ›å±‚ï¼Œåˆ©ç”¨å¸§é—´çš„æ—¶åºç›¸å…³æ€§ã€‚</li>
<li>æå‡ºäº†æ—¶é—´ä¸€è‡´æ€§æŒ‡å¯¼ã€æ—¶ç©ºå™ªå£°å…±äº«å’Œæ—©æœŸåœæ­¢é‡‡æ ·ç­–ç•¥æ¥æ”¹å–„æ—¶åºä¸€è‡´æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä½œä¸ºæ’ä»¶æ¨¡å—ï¼Œé€‚ç”¨äºä»»ä½•åŸºäºæ‰©æ•£çš„å›¾åƒä¿®å¤æˆ–å¢å¼ºæ–¹æ³•ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜è¯¥æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.01960">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d2aa55cbf79153a632aa3a62108fc715.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9bd168d030a48c98ce59393965aa883.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6a64643a15995a15fda5df68c86c9e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d213286e9dba67074b0adc4c7cff3db5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c881d379739978dc0b43df2881dabd0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ee5dbd0ba005be8ba772bb8385035e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e031237a2d8a6faa09b672a092b367c7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Invertible-Diffusion-Models-for-Compressed-Sensing"><a href="#Invertible-Diffusion-Models-for-Compressed-Sensing" class="headerlink" title="Invertible Diffusion Models for Compressed Sensing"></a>Invertible Diffusion Models for Compressed Sensing</h2><p><strong>Authors:Bin Chen, Zhenyu Zhang, Weiqi Li, Chen Zhao, Jiwen Yu, Shijie Zhao, Jie Chen, Jian Zhang</strong></p>
<p>While deep neural networks (NN) significantly advance image compressed sensing (CS) by improving reconstruction quality, the necessity of training current CS NNs from scratch constrains their effectiveness and hampers rapid deployment. Although recent methods utilize pre-trained diffusion models for image reconstruction, they struggle with slow inference and restricted adaptability to CS. To tackle these challenges, this paper proposes Invertible Diffusion Models (IDM), a novel efficient, end-to-end diffusion-based CS method. IDM repurposes a large-scale diffusion sampling process as a reconstruction model, and fine-tunes it end-to-end to recover original images directly from CS measurements, moving beyond the traditional paradigm of one-step noise estimation learning. To enable such memory-intensive end-to-end fine-tuning, we propose a novel two-level invertible design to transform both (1) multi-step sampling process and (2) noise estimation U-Net in each step into invertible networks. As a result, most intermediate features are cleared during training to reduce up to 93.8% GPU memory. In addition, we develop a set of lightweight modules to inject measurements into noise estimator to further facilitate reconstruction. Experiments demonstrate that IDM outperforms existing state-of-the-art CS networks by up to 2.64dB in PSNR. Compared to the recent diffusion-based approach DDNM, our IDM achieves up to 10.09dB PSNR gain and 14.54 times faster inference. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Guaishou74851/IDM">https://github.com/Guaishou74851/IDM</a>. </p>
<blockquote>
<p>è™½ç„¶æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆNNï¼‰é€šè¿‡æé«˜é‡å»ºè´¨é‡æ¥æ˜¾è‘—æ¨è¿›å›¾åƒå‹ç¼©æ„ŸçŸ¥ï¼ˆCSï¼‰çš„å‘å±•ï¼Œä½†å½“å‰éœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒCSç¥ç»ç½‘ç»œï¼Œè¿™é™åˆ¶äº†å…¶æœ‰æ•ˆæ€§å¹¶é˜»ç¢äº†å¿«é€Ÿéƒ¨ç½²ã€‚å°½ç®¡æœ€è¿‘çš„æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒé‡å»ºï¼Œä½†å®ƒä»¬é¢ä¸´æ¨ç†é€Ÿåº¦æ…¢å’Œé€‚åº”å‹ç¼©æ„ŸçŸ¥çš„èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†å¯é€†æ‰©æ•£æ¨¡å‹ï¼ˆIDMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„é«˜æ•ˆã€ç«¯åˆ°ç«¯çš„å‹ç¼©æ„ŸçŸ¥æ–°æ–¹æ³•ã€‚IDMå°†å¤§è§„æ¨¡çš„æ‰©æ•£é‡‡æ ·è¿‡ç¨‹é‡æ–°ç”¨ä½œé‡å»ºæ¨¡å‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œç«¯åˆ°ç«¯çš„å¾®è°ƒï¼Œç›´æ¥ä»å‹ç¼©æ„ŸçŸ¥æµ‹é‡ä¸­æ¢å¤åŸå§‹å›¾åƒï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ä¸€æ­¥å™ªå£°ä¼°è®¡å­¦ä¹ èŒƒå¼ã€‚ä¸ºäº†å®ç°è¿™ç§å†…å­˜å¯†é›†å‹çš„ç«¯åˆ°ç«¯å¾®è°ƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸¤çº§å¯é€†è®¾è®¡ï¼Œå°†ï¼ˆ1ï¼‰å¤šæ­¥é‡‡æ ·è¿‡ç¨‹å’Œï¼ˆ2ï¼‰æ¯ä¸€æ­¥ä¸­çš„å™ªå£°ä¼°è®¡U-Netè½¬åŒ–ä¸ºå¯é€†ç½‘ç»œã€‚å› æ­¤ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¸…é™¤äº†å¤§å¤šæ•°ä¸­é—´ç‰¹å¾ï¼Œä»¥å‡å°‘é«˜è¾¾93.8%çš„GPUå†…å­˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç³»åˆ—è½»é‡çº§æ¨¡å—ï¼Œå°†æµ‹é‡å€¼æ³¨å…¥å™ªå£°ä¼°è®¡å™¨ï¼Œä»¥è¿›ä¸€æ­¥ä¿ƒè¿›é‡å»ºã€‚å®éªŒè¡¨æ˜ï¼ŒIDMåœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æ–¹é¢ä¼˜äºç°æœ‰çš„å…ˆè¿›CSç½‘ç»œï¼Œæé«˜äº†é«˜è¾¾2.64dBã€‚ä¸æœ€è¿‘çš„æ‰©æ•£æ¨¡å‹DDNMç›¸æ¯”ï¼Œæˆ‘ä»¬çš„IDMåœ¨PSNRä¸Šå®ç°äº†é«˜è¾¾10.09dBçš„å¢ç›Šï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†14.54å€ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/Guaishou74851/IDM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Guaishou74851/IDMè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.17006v2">PDF</a> Accepted for publication in IEEE Transactions on Pattern Analysis and   Machine Intelligence (TPAMI)</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆNNï¼‰åœ¨å›¾åƒå‹ç¼©æ„ŸçŸ¥ï¼ˆCSï¼‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰CS NNéœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒï¼Œè¿™é™åˆ¶äº†å…¶æœ‰æ•ˆæ€§å¹¶é˜»ç¢äº†å¿«é€Ÿéƒ¨ç½²ã€‚å°½ç®¡å·²æœ‰æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒé‡å»ºï¼Œä½†å®ƒä»¬é¢ä¸´ç€æ¨ç†é€Ÿåº¦æ…¢å’Œé€‚åº”CSèƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºå¯é€†æ‰©æ•£æ¨¡å‹ï¼ˆIDMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„é«˜æ•ˆç«¯åˆ°ç«¯CSæ–¹æ³•ã€‚IDMå°†å¤§è§„æ¨¡çš„æ‰©æ•£é‡‡æ ·è¿‡ç¨‹é‡æ–°ç”¨ä½œé‡å»ºæ¨¡å‹ï¼Œå¹¶å°†å…¶ç«¯åˆ°ç«¯å¾®è°ƒï¼Œç›´æ¥ä»CSæµ‹é‡ä¸­æ¢å¤åŸå§‹å›¾åƒï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ä¸€æ­¥å™ªå£°ä¼°è®¡å­¦ä¹ èŒƒå¼ã€‚ä¸ºå®ç°è¿™ç§å†…å­˜å¯†é›†å‹çš„ç«¯åˆ°ç«¯å¾®è°ƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸¤çº§å¯é€†è®¾è®¡ï¼Œå°†ï¼ˆ1ï¼‰å¤šæ­¥é‡‡æ ·è¿‡ç¨‹å’Œï¼ˆ2ï¼‰æ¯ä¸€æ­¥ä¸­çš„å™ªå£°ä¼°è®¡U-Netè½¬æ¢ä¸ºå¯é€†ç½‘ç»œã€‚ç»“æœï¼Œå¤§å¤šæ•°ä¸­é—´ç‰¹å¾åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¢«æ¸…é™¤ï¼Œä»¥å‡å°‘é«˜è¾¾93.8%çš„GPUå†…å­˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç³»åˆ—è½»é‡çº§æ¨¡å—ï¼Œå°†æµ‹é‡å€¼æ³¨å…¥å™ªå£°ä¼°è®¡å™¨ï¼Œä»¥è¿›ä¸€æ­¥ä¿ƒè¿›é‡å»ºã€‚å®éªŒè¡¨æ˜ï¼ŒIDMåœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æ–¹é¢æ¯”ç°æœ‰å…ˆè¿›çš„CSç½‘ç»œé«˜å‡º2.64dBã€‚ä¸æœ€è¿‘çš„æ‰©æ•£æ–¹æ³•DDNMç›¸æ¯”ï¼Œæˆ‘ä»¬çš„IDMå®ç°äº†é«˜è¾¾10.09dBçš„PSNRå¢ç›Šå’Œ14.54å€çš„æ¨ç†é€Ÿåº¦æå‡ã€‚ç›¸å…³ä»£ç å¯åœ¨XXXä¸­è·å¾—ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒå‹ç¼©æ„ŸçŸ¥æ–¹æ³•â€”â€”å¯é€†æ‰©æ•£æ¨¡å‹ï¼ˆIDMï¼‰ï¼Œè¯¥æ–¹æ³•åŸºäºæ‰©æ•£é‡‡æ ·è¿‡ç¨‹è¿›è¡Œé‡å»ºã€‚</li>
<li>IDMé€šè¿‡ç«¯åˆ°ç«¯å¾®è°ƒç›´æ¥ä»CSæµ‹é‡ä¸­æ¢å¤åŸå§‹å›¾åƒï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ä¸€æ­¥å™ªå£°ä¼°è®¡å­¦ä¹ ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸¤çº§å¯é€†è®¾è®¡ï¼Œä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨å¹¶åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿå‡å°‘GPUå†…å­˜ä½¿ç”¨é«˜è¾¾93.8%ã€‚</li>
<li>IDmé‡‡ç”¨äº†è½»é‡çº§æ¨¡å—æ¥æ³¨å…¥æµ‹é‡å€¼åˆ°å™ªå£°ä¼°è®¡è¿‡ç¨‹ä¸­ä»¥ä¿ƒè¿›é‡å»ºã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒIDMåœ¨PSNRæŒ‡æ ‡ä¸Šæ˜¾è‘—æé«˜ï¼Œç›¸æ¯”ç°æœ‰ç½‘ç»œæœ‰é«˜è¾¾2.64dBçš„æå‡ã€‚</li>
<li>ä¸ç°æœ‰æ‰©æ•£æ¨¡å‹DDNMç›¸æ¯”ï¼ŒIDMåœ¨PSNRä¸Šå®ç°äº†æ›´é«˜çš„å¢ç›Šå¹¶æ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.17006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e647544169bc28be72965b455e42105.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6866aa196008958648b0f477807b33f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a0aa905de9a5f0aa6fbe840dadf3f8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2e76267811ff6d5ebe42bef37cec6d6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6806b5a1f932503e54ec26504f32bb8a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4743078b57f464191e4cf6a6a307af0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Warfare-Breaking-the-Watermark-Protection-of-AI-Generated-Content"><a href="#Warfare-Breaking-the-Watermark-Protection-of-AI-Generated-Content" class="headerlink" title="Warfare:Breaking the Watermark Protection of AI-Generated Content"></a>Warfare:Breaking the Watermark Protection of AI-Generated Content</h2><p><strong>Authors:Guanlin Li, Yifei Chen, Jie Zhang, Shangwei Guo, Han Qiu, Guoyin Wang, Jiwei Li, Tianwei Zhang</strong></p>
<p>AI-Generated Content (AIGC) is rapidly expanding, with services using advanced generative models to create realistic images and fluent text. Regulating such content is crucial to prevent policy violations, such as unauthorized commercialization or unsafe content distribution. Watermarking is a promising solution for content attribution and verification, but we demonstrate its vulnerability to two key attacks: (1) Watermark removal, where adversaries erase embedded marks to evade regulation, and (2) Watermark forging, where they generate illicit content with forged watermarks, leading to misattribution. We propose Warfare, a unified attack framework leveraging a pre-trained diffusion model for content processing and a generative adversarial network for watermark manipulation. Evaluations across datasets and embedding setups show that Warfare achieves high success rates while preserving content quality. We further introduce Warfare-Plus, which enhances efficiency without compromising effectiveness. The code can be found in <a target="_blank" rel="noopener" href="https://github.com/GuanlinLee/warfare">https://github.com/GuanlinLee/warfare</a>. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰æ­£åœ¨è¿…é€Ÿæ‰©å¼ ï¼ŒæœåŠ¡ä¸­ä½¿ç”¨å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹æ¥åˆ›å»ºé€¼çœŸçš„å›¾åƒå’Œæµç•…çš„æ–‡æœ¬ã€‚å¯¹è¿™ç±»å†…å®¹è¿›è¡Œç›‘ç®¡å¯¹äºé˜²æ­¢æ”¿ç­–è¿è§„è¡Œä¸ºè‡³å…³é‡è¦ï¼Œä¾‹å¦‚æœªç»æˆæƒçš„å•†ä¸šåŒ–æˆ–ä¸å®‰å…¨çš„å†…å®¹ä¼ æ’­ã€‚æ°´å°æ˜¯ä¸€ç§å…·æœ‰å‰æ™¯çš„å†…å®¹å½’å±å’ŒéªŒè¯è§£å†³æ–¹æ¡ˆï¼Œä½†æˆ‘ä»¬å±•ç¤ºäº†å…¶é¢ä¸´ä¸¤ç§ä¸»è¦æ”»å‡»çš„è„†å¼±æ€§ï¼šï¼ˆ1ï¼‰æ°´å°ç§»é™¤ï¼Œå…¶ä¸­å¯¹æ‰‹åˆ é™¤åµŒå…¥çš„æ ‡è®°ä»¥é€ƒé¿ç›‘ç®¡ï¼›ï¼ˆ2ï¼‰æ°´å°ä¼ªé€ ï¼Œä»–ä»¬ä½¿ç”¨ä¼ªé€ çš„æ°´å°ç”Ÿæˆéæ³•å†…å®¹ï¼Œå¯¼è‡´å½’å±é”™è¯¯ã€‚æˆ‘ä»¬æå‡ºäº†Warfareï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ”»å‡»æ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå†…å®¹å¤„ç†ï¼Œå¹¶ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œè¿›è¡Œæ°´å°æ“ä½œã€‚åœ¨æ•°æ®é›†å’ŒåµŒå…¥è®¾ç½®ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒWarfareåœ¨ä¿æŒå†…å®¹è´¨é‡çš„åŒæ—¶å®ç°äº†è¾ƒé«˜çš„æˆåŠŸç‡ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†æé«˜æ•ˆç‡è€Œä¸å¦¥åæ•ˆæœçš„Warfare-Plusã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/GuanlinLee/warfare%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/GuanlinLee/warfareä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07726v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>AIGCå†…å®¹ç”ŸæˆæŠ€æœ¯å¿«é€Ÿå‘å±•ï¼Œéœ€è¦ç›‘ç®¡ä»¥é˜²æ­¢æ”¿ç­–è¿è§„ã€‚æ°´å°æ˜¯è§£å†³å†…å®¹å½’å±å’ŒéªŒè¯çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†å­˜åœ¨ä¸¤å¤§æ”»å‡»å¨èƒï¼šä¸€æ˜¯å»é™¤æ°´å°ä»¥é€ƒé¿ç›‘ç®¡ï¼ŒäºŒæ˜¯ä¼ªé€ æ°´å°å¯¼è‡´å†…å®¹è¯¯å½’å±ã€‚æå‡ºåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ„å»ºçš„ Warfare æ”»å‡»æ¡†æ¶æ¥åº”å¯¹ï¼Œå¹¶åœ¨æ•°æ®é›†å’ŒåµŒå…¥è®¾ç½®ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºå…¶é«˜æˆåŠŸç‡ä¸”å†…å®¹è´¨é‡ä¸å—å½±å“ã€‚åŒæ—¶æ¨å‡ºå¢å¼ºç‰ˆ Warfare-Plusï¼Œæé«˜æ•ˆç‡ä¸”ä¸é™ä½æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIGCå†…å®¹ç”ŸæˆæŠ€æœ¯è¿…é€Ÿå‘å±•ï¼Œå¼•å‘ç›‘ç®¡éœ€æ±‚ï¼Œä»¥é˜²æ­¢æ”¿ç­–è¿è§„ã€‚</li>
<li>æ°´å°æ˜¯è§£å†³å†…å®¹å½’å±å’ŒéªŒè¯çš„å¯è¡Œæ–¹æ³•ï¼Œä½†å­˜åœ¨è¢«ç§»é™¤å’Œä¼ªé€ çš„é£é™©ã€‚</li>
<li>æ°´å°ç§»é™¤å’Œä¼ªé€ æ˜¯ä¸¤å¤§æ”»å‡»å¨èƒï¼Œå¯èƒ½å¯¼è‡´é€ƒé¿ç›‘ç®¡å’Œè¯¯å½’å±ã€‚</li>
<li>æå‡ºåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„ Warfare æ”»å‡»æ¡†æ¶ã€‚</li>
<li>Warfareæ¡†æ¶åœ¨æ•°æ®é›†å’ŒåµŒå…¥è®¾ç½®ä¸Šçš„è¯„ä¼°è¡¨ç°å‡ºé«˜æˆåŠŸç‡å’Œå†…å®¹è´¨é‡ä¿ç•™ã€‚</li>
<li>æ¨å‡ºWarfare-Plusï¼Œæé«˜æ•ˆç‡å’Œæ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.07726">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f2bdea2c37a460d1f74da100faaa724f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d6a38b074edbee399912e45bb72c779.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af19da14c367be2ea6cbd71ace7fa6b3.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-05/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-05/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ffc25d1f0f026579682077d55f25eeb2.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-05  Pathological MRI Segmentation by Synthetic Pathological Data Generation   in Fetuses and Neonates
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-05/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6880b8f0f58d66ce069be7c92ea604da.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-05  Laser Efficient Language-Guided Segmentation in Neural Radiance Fields
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">14773.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
