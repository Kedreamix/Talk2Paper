<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-24  Accelerate High-Quality Diffusion Models with Inner Loop Feedback">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-a42362520175822e3979e92166715baa.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    54 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-24-æ›´æ–°"><a href="#2025-01-24-æ›´æ–°" class="headerlink" title="2025-01-24 æ›´æ–°"></a>2025-01-24 æ›´æ–°</h1><h2 id="Accelerate-High-Quality-Diffusion-Models-with-Inner-Loop-Feedback"><a href="#Accelerate-High-Quality-Diffusion-Models-with-Inner-Loop-Feedback" class="headerlink" title="Accelerate High-Quality Diffusion Models with Inner Loop Feedback"></a>Accelerate High-Quality Diffusion Models with Inner Loop Feedback</h2><p><strong>Authors:Matthew Gwilliam, Han Cai, Di Wu, Abhinav Shrivastava, Zhiyu Cheng</strong></p>
<p>We propose Inner Loop Feedback (ILF), a novel approach to accelerate diffusion modelsâ€™ inference. ILF trains a lightweight module to predict future features in the denoising process by leveraging the outputs from a chosen diffusion backbone block at a given time step. This approach exploits two key intuitions; (1) the outputs of a given block at adjacent time steps are similar, and (2) performing partial computations for a step imposes a lower burden on the model than skipping the step entirely. Our method is highly flexible, since we find that the feedback module itself can simply be a block from the diffusion backbone, with all settings copied. Its influence on the diffusion forward can be tempered with a learnable scaling factor from zero initialization. We train this module using distillation losses; however, unlike some prior work where a full diffusion backbone serves as the student, our model freezes the backbone, training only the feedback module. While many efforts to optimize diffusion models focus on achieving acceptable image quality in extremely few steps (1-4 steps), our emphasis is on matching best case results (typically achieved in 20 steps) while significantly reducing runtime. ILF achieves this balance effectively, demonstrating strong performance for both class-to-image generation with diffusion transformer (DiT) and text-to-image generation with DiT-based PixArt-alpha and PixArt-sigma. The quality of ILFâ€™s 1.7x-1.8x speedups are confirmed by FID, CLIP score, CLIP Image Quality Assessment, ImageReward, and qualitative comparisons. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†å†…éƒ¨å¾ªç¯åé¦ˆï¼ˆILFï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŠ é€Ÿæ‰©æ•£æ¨¡å‹æ¨æ–­çš„æ–°å‹æ–¹æ³•ã€‚ILFè®­ç»ƒäº†ä¸€ä¸ªè½»é‡çº§æ¨¡å—ï¼Œé€šè¿‡åˆ©ç”¨åœ¨ç»™å®šæ—¶é—´æ­¥é•¿é€‰æ‹©çš„æ‰©æ•£ä¸»å¹²å—è¾“å‡ºï¼Œæ¥é¢„æµ‹å»å™ªè¿‡ç¨‹ä¸­æœªæ¥çš„ç‰¹å¾ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨äº†ä¸¤ä¸ªå…³é”®ç›´è§‰ï¼šï¼ˆ1ï¼‰ç»™å®šå—åœ¨ç›¸é‚»æ—¶é—´æ­¥çš„è¾“å‡ºæ˜¯ç›¸ä¼¼çš„ï¼Œä»¥åŠï¼ˆ2ï¼‰å¯¹ä¸€æ­¥è¿›è¡Œéƒ¨åˆ†è®¡ç®—æ¯”å®Œå…¨è·³è¿‡è¿™ä¸€æ­¥ç»™æ¨¡å‹å¸¦æ¥çš„è´Ÿæ‹…æ›´ä½ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰å¾ˆé«˜çš„çµæ´»æ€§ï¼Œå› ä¸ºæˆ‘ä»¬å‘ç°åé¦ˆæ¨¡å—æœ¬èº«å¯ä»¥ä»…ä»…æ˜¯æ‰©æ•£ä¸»å¹²çš„ä¸€ä¸ªå—ï¼Œæ‰€æœ‰è®¾ç½®éƒ½è¢«å¤åˆ¶ã€‚å®ƒå¯¹æ‰©æ•£æ­£å‘çš„å½±å“å¯ä»¥é€šè¿‡ä»é›¶åˆå§‹åŒ–å¼€å§‹çš„ä¸€ä¸ªå¯å­¦ä¹ çš„ç¼©æ”¾å› å­æ¥è°ƒèŠ‚ã€‚æˆ‘ä»¬ä½¿ç”¨è’¸é¦æŸå¤±æ¥è®­ç»ƒè¿™ä¸ªæ¨¡å—ï¼›ç„¶è€Œï¼Œä¸ä¸€äº›å…ˆå‰çš„å·¥ä½œä¸åŒï¼Œå…¶ä¸­æ•´ä¸ªæ‰©æ•£ä¸»å¹²ä½œä¸ºå­¦ç”Ÿï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¼šå†»ç»“ä¸»å¹²ï¼Œåªè®­ç»ƒåé¦ˆæ¨¡å—ã€‚è™½ç„¶è®¸å¤šä¼˜åŒ–æ‰©æ•£æ¨¡å‹çš„åŠªåŠ›éƒ½é›†ä¸­åœ¨åœ¨æå°‘çš„æ­¥éª¤ï¼ˆ1-4æ­¥ï¼‰å†…è¾¾åˆ°å¯æ¥å—çš„å›¾åƒè´¨é‡ï¼Œä½†æˆ‘ä»¬çš„é‡ç‚¹æ˜¯åœ¨è¾¾åˆ°æœ€ä½³ç»“æœï¼ˆé€šå¸¸åœ¨20æ­¥å†…å®ç°ï¼‰çš„åŒæ—¶æ˜¾è‘—å‡å°‘è¿è¡Œæ—¶é—´ã€‚ILFæœ‰æ•ˆåœ°å®ç°äº†è¿™ç§å¹³è¡¡ï¼Œåœ¨åˆ©ç”¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰è¿›è¡Œç±»åˆ°å›¾åƒç”Ÿæˆå’ŒåŸºäºDiTçš„PixArt-alphaå’ŒPixArt-sigmaè¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ—¶ï¼Œå‡è¡¨ç°å‡ºå¼ºåŠ²çš„æ€§èƒ½ã€‚ILFçš„1.7å€è‡³1.8å€åŠ é€Ÿè´¨é‡å¾—åˆ°äº†FIDã€CLIPåˆ†æ•°ã€CLIPå›¾åƒè´¨é‡è¯„ä¼°ã€ImageRewardå’Œå®šæ€§æ¯”è¾ƒçš„ç¡®è¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13107v1">PDF</a> submission currently under review; 20 pages, 17 figures, 6 tables</p>
<p><strong>Summary</strong><br>     æˆ‘ä»¬æå‡ºäº†å†…éƒ¨å¾ªç¯åé¦ˆï¼ˆILFï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œæ—¨åœ¨åŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚ILFè®­ç»ƒäº†ä¸€ä¸ªè½»é‡çº§æ¨¡å—ï¼Œé€šè¿‡åˆ©ç”¨ç»™å®šæ—¶é—´æ­¥é•¿ä¸‹é€‰æ‹©çš„æ‰©æ•£ä¸»å¹²å—çš„è¾“å‡ºæ¥é¢„æµ‹å»å™ªè¿‡ç¨‹ä¸­çš„æœªæ¥ç‰¹å¾ã€‚è¯¥æ–¹æ³•åŸºäºä¸¤ä¸ªå…³é”®ç›´è§‰ï¼šï¼ˆ1ï¼‰ç›¸é‚»æ—¶é—´æ­¥é•¿ä¸‹ç»™å®šå—çš„è¾“å‡ºæ˜¯ç›¸ä¼¼çš„ï¼›ï¼ˆ2ï¼‰æ‰§è¡Œéƒ¨åˆ†è®¡ç®—æ­¥éª¤æ¯”å®Œå…¨è·³è¿‡æ­¥éª¤å¯¹æ¨¡å‹çš„è´Ÿæ‹…æ›´å°ã€‚ILFæ–¹æ³•çµæ´»åº¦é«˜ï¼Œåé¦ˆæ¨¡å—æœ¬èº«å¯ä»¥æ˜¯æ‰©æ•£ä¸»å¹²çš„ä¸€ä¸ªå—ï¼Œæ‰€æœ‰è®¾ç½®å‡è¢«å¤åˆ¶ã€‚å®ƒå¯¹æ‰©æ•£æ­£å‘çš„å½±å“å¯ä»¥é€šè¿‡ä»é›¶åˆå§‹åŒ–ä¸­å­¦ä¹ åˆ°çš„ç¼©æ”¾å› å­è¿›è¡Œè°ƒèŠ‚ã€‚æˆ‘ä»¬ä»…è®­ç»ƒè¯¥æ¨¡å—ä½¿ç”¨è’¸é¦æŸå¤±ï¼Œä½†ä¸æŸäº›å…ˆå‰çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹å†»ç»“äº†ä¸»å¹²ï¼Œåªè®­ç»ƒåé¦ˆæ¨¡å—ã€‚è™½ç„¶è®¸å¤šä¼˜åŒ–æ‰©æ•£æ¨¡å‹çš„åŠªåŠ›éƒ½é›†ä¸­åœ¨å¦‚ä½•åœ¨æå°‘çš„æ­¥éª¤ï¼ˆ1-4æ­¥ï¼‰å†…è¾¾åˆ°å¯æ¥å—çš„å›¾åƒè´¨é‡ï¼Œä½†æˆ‘ä»¬çš„é‡ç‚¹æ˜¯åœ¨åŒ¹é…æœ€ä½³ç»“æœï¼ˆé€šå¸¸åœ¨20æ­¥å†…è¾¾åˆ°ï¼‰çš„åŒæ—¶æ˜¾è‘—å‡å°‘è¿è¡Œæ—¶é—´ã€‚ILFæœ‰æ•ˆåœ°å®ç°äº†è¿™ä¸€å¹³è¡¡ï¼Œåœ¨åŸºäºæ‰©æ•£å˜å‹å™¨çš„ç±»åˆ°å›¾åƒç”Ÿæˆå’ŒåŸºäºPixArt-alphaå’ŒPixArt-sigmaçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å‡è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ILFçš„é€Ÿåº¦æå‡1.7å€è‡³1.8å€çš„è´¨é‡å¾—åˆ°äº†FIDã€CLIPåˆ†æ•°ã€CLIPå›¾åƒè´¨é‡è¯„ä¼°ã€ImageRewardå’Œå®šæ€§æ¯”è¾ƒçš„ç¡®è¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åä¸ºå†…éƒ¨å¾ªç¯åé¦ˆï¼ˆILFï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨åŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>ILFåˆ©ç”¨ç»™å®šæ—¶é—´æ­¥é•¿ä¸‹çš„æ‰©æ•£ä¸»å¹²å—è¾“å‡ºæ¥é¢„æµ‹æœªæ¥ç‰¹å¾ã€‚</li>
<li>è¯¥æ–¹æ³•åŸºäºä¸¤ä¸ªå…³é”®ç›´è§‰ï¼šç›¸é‚»æ—¶é—´æ­¥é•¿ä¸‹çš„è¾“å‡ºç›¸ä¼¼æ€§ï¼Œä»¥åŠéƒ¨åˆ†è®¡ç®—æ­¥éª¤çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ILFæ–¹æ³•å…·æœ‰çµæ´»æ€§ï¼Œå…¶åé¦ˆæ¨¡å—å¯ä»¥æ˜¯æ‰©æ•£ä¸»å¹²çš„ä¸€ä¸ªå—ã€‚</li>
<li>ä»…è®­ç»ƒåé¦ˆæ¨¡å—ä½¿ç”¨è’¸é¦æŸå¤±ï¼Œè€Œä¸»å¹²ä¿æŒå†»ç»“ã€‚</li>
<li>ILFåœ¨åŒ¹é…æœ€ä½³ç»“æœçš„åŒæ—¶æ˜¾è‘—å‡å°‘è¿è¡Œæ—¶é—´ï¼Œå®ç°äº†åœ¨æ‰©æ•£æ¨¡å‹ä¼˜åŒ–ä¸­çš„æœ‰æ•ˆå¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13107">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-075c82399f0d786e7e3b030873779e8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-775d25693d9e307fe26284a327bfaf77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d12d8886f4c13a4450794db4460e812.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9562c641b163666b302c482a15e1fdc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-020eb738a1f4fd451bab03088a90743e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Orchid-Image-Latent-Diffusion-for-Joint-Appearance-and-Geometry-Generation"><a href="#Orchid-Image-Latent-Diffusion-for-Joint-Appearance-and-Geometry-Generation" class="headerlink" title="Orchid: Image Latent Diffusion for Joint Appearance and Geometry   Generation"></a>Orchid: Image Latent Diffusion for Joint Appearance and Geometry   Generation</h2><p><strong>Authors:Akshay Krishnan, Xinchen Yan, Vincent Casser, Abhijit Kundu</strong></p>
<p>Diffusion models are state-of-the-art for image generation. Trained on large datasets, they capture expressive image priors that have been used for tasks like inpainting, depth, and (surface) normal prediction. However, these models are typically trained for one specific task, e.g., a separate model for each of color, depth, and normal prediction. Such models do not leverage the intrinsic correlation between appearance and geometry, often leading to inconsistent predictions.   In this paper, we propose using a novel image diffusion prior that jointly encodes appearance and geometry. We introduce a diffusion model Orchid, comprising a Variational Autoencoder (VAE) to encode color, depth, and surface normals to a latent space, and a Latent Diffusion Model (LDM) for generating these joint latents. Orchid directly generates photo-realistic color images, relative depth, and surface normals from user-provided text, and can be used to create image-aligned partial 3D scenes seamlessly. It can also perform image-conditioned tasks like joint monocular depth and normal prediction and is competitive in accuracy to state-of-the-art methods designed for those tasks alone. Lastly, our model learns a joint prior that can be used zero-shot as a regularizer for many inverse problems that entangle appearance and geometry. For example, we demonstrate its effectiveness in color-depth-normal inpainting, showcasing its applicability to problems in 3D generation from sparse views. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹æ˜¯å›¾åƒç”Ÿæˆé¢†åŸŸçš„æœ€å‰æ²¿æŠ€æœ¯ã€‚å®ƒä»¬ç»è¿‡å¤§è§„æ¨¡æ•°æ®é›†è®­ç»ƒï¼Œèƒ½å¤Ÿæ•æ‰å›¾åƒçš„è¡¨è¾¾å…ˆéªŒï¼Œè¢«ç”¨äºå›¾åƒä¿®å¤ã€æ·±åº¦ä¼°è®¡å’Œï¼ˆè¡¨é¢ï¼‰æ³•çº¿é¢„æµ‹ç­‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œè®­ç»ƒï¼Œä¾‹å¦‚é’ˆå¯¹é¢œè‰²ã€æ·±åº¦å’Œæ³•çº¿é¢„æµ‹ç­‰ä»»åŠ¡åˆ†åˆ«ä½¿ç”¨ä¸åŒçš„æ¨¡å‹ã€‚è¿™ç§æ¨¡å‹æ²¡æœ‰åˆ©ç”¨å¤–è§‚å’Œå‡ ä½•ä¹‹é—´çš„å†…åœ¨å…³è”ï¼Œå¸¸å¸¸å¯¼è‡´é¢„æµ‹ç»“æœä¸ä¸€è‡´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒæ‰©æ•£å…ˆéªŒï¼Œèƒ½å¤ŸåŒæ—¶ç¼–ç å¤–è§‚å’Œå‡ ä½•ä¿¡æ¯ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ‰©æ•£æ¨¡å‹â€”â€”Orchidï¼Œå®ƒåŒ…å«ä¸€ä¸ªå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œç”¨äºå°†é¢œè‰²ã€æ·±åº¦å’Œè¡¨é¢æ³•çº¿ç¼–ç åˆ°æ½œåœ¨ç©ºé—´ï¼Œä»¥åŠä¸€ä¸ªæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰æ¥ç”Ÿæˆè¿™äº›è”åˆæ½œåœ¨å˜é‡ã€‚Orchidå¯ä»¥æ ¹æ®ç”¨æˆ·æä¾›çš„æ–‡æœ¬ç›´æ¥ç”Ÿæˆé€¼çœŸçš„å½©è‰²å›¾åƒã€ç›¸å¯¹æ·±åº¦å›¾å’Œè¡¨é¢æ³•çº¿å›¾ï¼Œå¯ç”¨äºåˆ›å»ºæ— ç¼çš„å›¾åƒå¯¹é½éƒ¨åˆ†3Dåœºæ™¯ã€‚å®ƒè¿˜å¯ä»¥æ‰§è¡Œå›¾åƒæ¡ä»¶ä»»åŠ¡ï¼Œå¦‚è”åˆå•ç›®æ·±åº¦ä¼°è®¡å’Œæ³•çº¿é¢„æµ‹ï¼Œå¹¶ä¸”åœ¨å‡†ç¡®æ€§æ–¹é¢ä¸é’ˆå¯¹è¿™äº›ä»»åŠ¡å•ç‹¬è®¾è®¡çš„æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸ç«äº‰ã€‚æœ€åï¼Œæˆ‘ä»¬çš„æ¨¡å‹å­¦ä¹ äº†ä¸€ä¸ªè”åˆå…ˆéªŒï¼Œå¯ä»¥é›¶æ ·æœ¬ä½œä¸ºè®¸å¤šçº ç¼ å¤–è§‚å’Œå‡ ä½•çš„é€†é—®é¢˜çš„æ­£åˆ™åŒ–å™¨ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬é€šè¿‡å½©è‰²æ·±åº¦æ³•çº¿å›¾åƒä¿®å¤ä»»åŠ¡å±•ç¤ºäº†å…¶åº”ç”¨äº3Dç”Ÿæˆçš„ç¨€ç–è§†å›¾é—®é¢˜çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13087v1">PDF</a> Project webpage: <a target="_blank" rel="noopener" href="https://orchid3d.github.io/">https://orchid3d.github.io</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹æ˜¯ç›®å‰å›¾åƒç”Ÿæˆçš„æœ€å…ˆè¿›æ–¹æ³•ã€‚é€šè¿‡å¤§å‹æ•°æ®é›†çš„è®­ç»ƒï¼Œå®ƒä»¬æ•æ‰äº†ç”ŸåŠ¨çš„å›¾åƒå…ˆéªŒçŸ¥è¯†ï¼Œå¯ç”¨äºå›¾åƒè¡¥å…¨ã€æ·±åº¦é¢„æµ‹å’Œè¡¨é¢æ³•çº¿é¢„æµ‹ç­‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸é’ˆå¯¹å•ä¸€ä»»åŠ¡è¿›è¡Œè®­ç»ƒï¼Œå¯¼è‡´é¢œè‰²ã€æ·±åº¦å’Œæ³•çº¿é¢„æµ‹ç­‰ä»»åŠ¡éœ€è¦å•ç‹¬çš„æ¨¡å‹å¤„ç†ã€‚è¿™äº›æ¨¡å‹æœªèƒ½å……åˆ†åˆ©ç”¨å¤–è§‚ä¸å‡ ä½•ä¹‹é—´çš„å†…åœ¨å…³è”ï¼Œå¯¼è‡´é¢„æµ‹ç»“æœçš„ä¸ä¸€è‡´æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹å›¾åƒæ‰©æ•£å…ˆéªŒï¼Œèƒ½è”åˆç¼–ç å¤–è§‚å’Œå‡ ä½•ä¿¡æ¯ã€‚æå‡ºçš„Orchidæ¨¡å‹åŒ…æ‹¬ä¸€ä¸ªç”¨äºç¼–ç é¢œè‰²ã€æ·±åº¦å’Œè¡¨é¢æ³•çº¿çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å’Œä¸€ä¸ªç”¨äºç”Ÿæˆè¿™äº›è”åˆæ½œåœ¨ç‰¹å¾çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ã€‚Orchidå¯æ ¹æ®ç”¨æˆ·æä¾›çš„æ–‡æœ¬ç›´æ¥ç”Ÿæˆé€¼çœŸçš„å½©è‰²å›¾åƒã€ç›¸å¯¹æ·±åº¦åŠè¡¨é¢æ³•çº¿ï¼Œå¹¶å¯ç”¨äºåˆ›å»ºæ— ç¼çš„å›¾åƒå¯¹é½çš„å±€éƒ¨ä¸‰ç»´åœºæ™¯ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜èƒ½æ‰§è¡Œå›¾åƒæ¡ä»¶ä¸‹çš„è”åˆå•çœ¼æ·±åº¦å’Œæ³•çº¿é¢„æµ‹ä»»åŠ¡ï¼Œå¹¶èƒ½åœ¨å‡†ç¡®æ€§æ–¹é¢ä¸é’ˆå¯¹è¿™äº›ä»»åŠ¡å•ç‹¬è®¾è®¡çš„æœ€æ–°æ–¹æ³•ç›¸ç«äº‰ã€‚æœ€åï¼Œè¯¥æ¨¡å‹å­¦ä¹ äº†ä¸€ä¸ªè”åˆå…ˆéªŒï¼Œå¯é›¶æ ·æœ¬ç”¨ä½œè®¸å¤šçº ç¼ å¤–è§‚å’Œå‡ ä½•çš„åé—®é¢˜çš„æ­£åˆ™åŒ–å™¨ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬åœ¨å½©è‰²æ·±åº¦æ³•çº¿è¡¥å…¨ä»»åŠ¡ä¸­å±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨ä»ç¨€ç–è§†å›¾ç”Ÿæˆä¸‰ç»´é—®é¢˜ä¸­çš„é€‚ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ç”¨äºå›¾åƒç”Ÿæˆæ•ˆæœå“è¶Šï¼Œä½†é¢ä¸´ä»»åŠ¡ç‰¹å®šè®­ç»ƒçš„é—®é¢˜ï¼Œç¼ºä¹è·¨ä»»åŠ¡ä¸€è‡´æ€§ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†Orchidæ¨¡å‹ï¼Œç»“åˆå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ï¼Œè”åˆç¼–ç é¢œè‰²å’Œå‡ ä½•ä¿¡æ¯ã€‚</li>
<li>Orchidèƒ½ç›´æ¥ä»æ–‡æœ¬ç”Ÿæˆé€¼çœŸçš„å½©è‰²å›¾åƒã€ç›¸å¯¹æ·±åº¦åŠè¡¨é¢æ³•çº¿ï¼Œæ”¯æŒåˆ›å»ºæ— ç¼çš„å›¾åƒå¯¹é½çš„å±€éƒ¨ä¸‰ç»´åœºæ™¯ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å›¾åƒæ¡ä»¶ä¸‹çš„è”åˆå•çœ¼æ·±åº¦å’Œæ³•çº¿é¢„æµ‹ä»»åŠ¡ä¸­å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>æ¨¡å‹å­¦ä¹ äº†ä¸€ä¸ªè”åˆå…ˆéªŒï¼Œå¯ä½œä¸ºå¤šç§åé—®é¢˜çš„æ­£åˆ™åŒ–å™¨ï¼Œé€‚ç”¨äºå½©è‰²æ·±åº¦æ³•çº¿è¡¥å…¨ç­‰ä»»åŠ¡ã€‚</li>
<li>Orchidæ¨¡å‹å±•ç¤ºäº†åœ¨ä»ç¨€ç–è§†å›¾ç”Ÿæˆä¸‰ç»´é—®é¢˜ä¸­çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13087">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1fcfbd625ad53f6a6d385b2e644fa812.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8126e44a7dfe6093dd2eb0ed6894adf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c34f30ef56fc3391e755438679e6235b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42a2d486ffbc4c4070326e8b6ad6221d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c976e8103f9bcffce9451ed3c2a7c639.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="3D-Object-Manipulation-in-a-Single-Image-using-Generative-Models"><a href="#3D-Object-Manipulation-in-a-Single-Image-using-Generative-Models" class="headerlink" title="3D Object Manipulation in a Single Image using Generative Models"></a>3D Object Manipulation in a Single Image using Generative Models</h2><p><strong>Authors:Ruisi Zhao, Zechuan Zhang, Zongxin Yang, Yi Yang</strong></p>
<p>Object manipulation in images aims to not only edit the objectâ€™s presentation but also gift objects with motion. Previous methods encountered challenges in concurrently handling static editing and dynamic generation, while also struggling to achieve fidelity in object appearance and scene lighting. In this work, we introduce \textbf{OMG3D}, a novel framework that integrates the precise geometric control with the generative power of diffusion models, thus achieving significant enhancements in visual performance. Our framework first converts 2D objects into 3D, enabling user-directed modifications and lifelike motions at the geometric level. To address texture realism, we propose CustomRefiner, a texture refinement module that pre-train a customized diffusion model, aligning the details and style of coarse renderings of 3D rough model with the original image, further refine the texture. Additionally, we introduce IllumiCombiner, a lighting processing module that estimates and corrects background lighting to match human visual perception, resulting in more realistic shadow effects. Extensive experiments demonstrate the outstanding visual performance of our approach in both static and dynamic scenarios. Remarkably, all these steps can be done using one NVIDIA 3090. Project page is at <a target="_blank" rel="noopener" href="https://whalesong-zrs.github.io/OMG3D-projectpage/">https://whalesong-zrs.github.io/OMG3D-projectpage/</a> </p>
<blockquote>
<p>å›¾åƒä¸­çš„ç‰©ä½“æ“ä½œæ—¨åœ¨ä¸ä»…ç¼–è¾‘ç‰©ä½“çš„å‘ˆç°ï¼Œè¿˜èµ‹äºˆç‰©ä½“åŠ¨æ€ã€‚ä¹‹å‰çš„æ–¹æ³•åœ¨å¤„ç†é™æ€ç¼–è¾‘å’ŒåŠ¨æ€ç”Ÿæˆæ—¶é‡åˆ°äº†æŒ‘æˆ˜ï¼ŒåŒæ—¶åœ¨å®ç°ç‰©ä½“å¤–è§‚å’Œåœºæ™¯å…‰ç…§çš„ä¿çœŸåº¦æ–¹é¢ä¹Ÿé‡åˆ°äº†å›°éš¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†\textbf{OMG3D}ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå®ƒå°†ç²¾ç¡®çš„å‡ ä½•æ§åˆ¶ä¸æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ç›¸ç»“åˆï¼Œä»è€Œåœ¨è§†è§‰æ€§èƒ½ä¸Šå®ç°äº†é‡å¤§æ”¹è¿›ã€‚æˆ‘ä»¬çš„æ¡†æ¶é¦–å…ˆå°†2Då¯¹è±¡è½¬æ¢ä¸º3Dï¼Œä»è€Œåœ¨å‡ ä½•çº§åˆ«å®ç°ç”¨æˆ·å¯¼å‘çš„ä¿®æ”¹å’Œé€¼çœŸçš„åŠ¨ä½œã€‚ä¸ºäº†è§£å†³çº¹ç†é€¼çœŸåº¦çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CustomRefinerï¼Œè¿™æ˜¯ä¸€ç§çº¹ç†ä¼˜åŒ–æ¨¡å—ï¼Œå®ƒé¢„å…ˆè®­ç»ƒäº†ä¸€ä¸ªå®šåˆ¶çš„æ‰©æ•£æ¨¡å‹ï¼Œå°†3Dç²—ç³™æ¨¡å‹çš„ç²—ç³™æ¸²æŸ“çš„ç»†èŠ‚å’Œé£æ ¼ä¸åŸå§‹å›¾åƒå¯¹é½ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–çº¹ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†IllumiCombinerï¼Œè¿™æ˜¯ä¸€ç§å…‰ç…§å¤„ç†æ¨¡å—ï¼Œå®ƒä¼°è®¡å¹¶çº æ­£èƒŒæ™¯å…‰ç…§ï¼Œä»¥åŒ¹é…äººç±»è§†è§‰æ„ŸçŸ¥ï¼Œä»è€Œäº§ç”Ÿæ›´é€¼çœŸçš„é˜´å½±æ•ˆæœã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•åœ¨é™æ€å’ŒåŠ¨æ€åœºæ™¯ä¸­çš„å‡ºè‰²è§†è§‰æ€§èƒ½ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œæ‰€æœ‰è¿™äº›æ­¥éª¤éƒ½å¯ä»¥ä½¿ç”¨ä¸€å°NVIDIA 3090å®Œæˆã€‚é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://whalesong-zrs.github.io/OMG3D-projectpage/%E3%80%82">https://whalesong-zrs.github.io/OMG3D-projectpage/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12935v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºOMG3Dçš„æ–°å‹æ¡†æ¶ï¼Œå®ƒå°†ç²¾ç¡®çš„å‡ ä½•æ§åˆ¶ä¸æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ç›¸ç»“åˆï¼Œå®ç°äº†ç‰©ä½“æ“ä½œå›¾åƒä¸­çš„é™æ€ç¼–è¾‘å’ŒåŠ¨æ€ç”Ÿæˆçš„æ˜¾è‘—æ”¹å–„ï¼Œå¹¶åœ¨è§†è§‰æ€§èƒ½ä¸Šå–å¾—äº†é‡å¤§æå‡ã€‚è¯¥æ¡†æ¶é€šè¿‡è½¬æ¢2Då¯¹è±¡ä¸º3Då¯¹è±¡ï¼Œä½¿ç”¨æˆ·å¯ä»¥åœ¨å‡ ä½•çº§åˆ«è¿›è¡ŒæŒ‡å¯¼æ€§ä¿®æ”¹ï¼Œå¹¶èµ‹äºˆç‰©ä½“é€¼çœŸçš„åŠ¨æ€æ•ˆæœã€‚ä¸ºè§£å†³çº¹ç†çœŸå®æ€§é—®é¢˜ï¼Œæå‡ºäº†CustomRefinerçº¹ç†ä¼˜åŒ–æ¨¡å—ï¼›ä¸ºè§£å†³å…‰ç…§é—®é¢˜ï¼Œå¼•å…¥äº†IllumiCombinerå…‰ç…§å¤„ç†æ¨¡å—ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é™æ€å’ŒåŠ¨æ€åœºæ™¯ä¸­å‡è¡¨ç°å‡ºå“è¶Šè§†è§‰æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>1.OMG3Dæ¡†æ¶ç»“åˆäº†ç²¾ç¡®çš„å‡ ä½•æ§åˆ¶ä¸æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚<br>2.OMG3Dèƒ½å®ç°2Då¯¹è±¡åˆ°3Då¯¹è±¡çš„è½¬æ¢ï¼Œå…è®¸ç”¨æˆ·åœ¨å‡ ä½•çº§åˆ«è¿›è¡ŒæŒ‡å¯¼æ€§ä¿®æ”¹ï¼Œå¹¶èµ‹äºˆç‰©ä½“åŠ¨æ€æ•ˆæœã€‚<br>3.CustomRefineræ¨¡å—ç”¨äºä¼˜åŒ–çº¹ç†ï¼Œæé«˜æ¸²æŸ“ç»†èŠ‚å’Œé£æ ¼ä¸åŸå§‹å›¾åƒçš„ä¸€è‡´æ€§ã€‚<br>4.IllumiCombineræ¨¡å—ç”¨äºå¤„ç†å…‰ç…§ï¼Œä¼°è®¡å’Œæ ¡æ­£èƒŒæ™¯å…‰ç…§ï¼Œä»¥åŒ¹é…äººç±»è§†è§‰æ„ŸçŸ¥ï¼Œå®ç°æ›´é€¼çœŸçš„é˜´å½±æ•ˆæœã€‚<br>5.è¯¥æ¡†æ¶åœ¨é™æ€å’ŒåŠ¨æ€åœºæ™¯ä¸­éƒ½è¡¨ç°å‡ºäº†å“è¶Šçš„è§†è§‰æ€§èƒ½ã€‚<br>6.æ‰€æœ‰æ­¥éª¤éƒ½åœ¨ä¸€ä¸ªNVIDIA 3090ä¸Šå®Œæˆã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12935">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0c5b143908b93ff5c595305451d4e0f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59fce56b156d3416a0430688b577707a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-362db1b880ca7c523d5ae9d46099cc9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbb630d96e2fe53b8aa92159be4fa46f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2df41af5ef0492569dc84b98c4f41902.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b044d4bd0a9bfceedabe7d7523c055f0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AMM-Diff-Adaptive-Multi-Modality-Diffusion-Network-for-Missing-Modality-Imputation"><a href="#AMM-Diff-Adaptive-Multi-Modality-Diffusion-Network-for-Missing-Modality-Imputation" class="headerlink" title="AMM-Diff: Adaptive Multi-Modality Diffusion Network for Missing Modality   Imputation"></a>AMM-Diff: Adaptive Multi-Modality Diffusion Network for Missing Modality   Imputation</h2><p><strong>Authors:Aghiles Kebaili, JÃ©rÃ´me Lapuyade-Lahorgue, Pierre Vera, Su Ruan</strong></p>
<p>In clinical practice, full imaging is not always feasible, often due to complex acquisition protocols, stringent privacy regulations, or specific clinical needs. However, missing MR modalities pose significant challenges for tasks like brain tumor segmentation, especially in deep learning-based segmentation, as each modality provides complementary information crucial for improving accuracy. A promising solution is missing data imputation, where absent modalities are generated from available ones. While generative models have been widely used for this purpose, most state-of-the-art approaches are limited to single or dual target translations, lacking the adaptability to generate missing modalities based on varying input configurations. To address this, we propose an Adaptive Multi-Modality Diffusion Network (AMM-Diff), a novel diffusion-based generative model capable of handling any number of input modalities and generating the missing ones. We designed an Image-Frequency Fusion Network (IFFN) that learns a unified feature representation through a self-supervised pretext task across the full input modalities and their selected high-frequency Fourier components. The proposed diffusion model leverages this representation, encapsulating prior knowledge of the complete modalities, and combines it with an adaptive reconstruction strategy to achieve missing modality completion. Experimental results on the BraTS 2021 dataset demonstrate the effectiveness of our approach. </p>
<blockquote>
<p>åœ¨ä¸´åºŠå®è·µä¸­ï¼Œå…¨é¢æˆåƒå¹¶ä¸æ€»æ˜¯å¯è¡Œçš„ï¼Œè¿™å¾€å¾€æ˜¯ç”±äºå¤æ‚çš„é‡‡é›†åè®®ã€ä¸¥æ ¼çš„éšç§è§„å®šæˆ–ç‰¹å®šçš„ä¸´åºŠéœ€æ±‚ã€‚ç„¶è€Œï¼Œç¼ºå¤±çš„MRæ¨¡å¼å¯¹äºå¦‚è„‘è‚¿ç˜¤åˆ†å‰²ç­‰ä»»åŠ¡æå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºäºæ·±åº¦å­¦ä¹ çš„åˆ†å‰²ä¸­ï¼Œå› ä¸ºæ¯ç§æ¨¡å¼éƒ½æä¾›äº†å¯¹æé«˜å‡†ç¡®æ€§è‡³å…³é‡è¦çš„è¡¥å……ä¿¡æ¯ã€‚ä¸€ç§å¯è¡Œçš„è§£å†³æ–¹æ¡ˆæ˜¯æ•°æ®æ’å€¼å¡«è¡¥ç¼ºå¤±éƒ¨åˆ†ï¼Œå…¶ä¸­ç¼ºå¤±çš„æ¨¡å¼ç”±å­˜åœ¨çš„æ¨¡å¼ç”Ÿæˆã€‚è™½ç„¶ç”Ÿæˆæ¨¡å‹å·²è¢«å¹¿æ³›ç”¨äºæ­¤ç›®çš„ï¼Œä½†å¤§å¤šæ•°æœ€å…ˆè¿›çš„æ–¹æ³•ä»…é™äºå•ä¸ªæˆ–åŒç›®æ ‡ç¿»è¯‘ï¼Œç¼ºä¹æ ¹æ®ä¸åŒè¾“å…¥é…ç½®ç”Ÿæˆç¼ºå¤±æ¨¡å¼çš„é€‚åº”æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”å¤šæ¨¡æ€æ‰©æ•£ç½‘ç»œï¼ˆAMM-Diffï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„æ–°å‹ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†ä»»ä½•æ•°é‡çš„è¾“å…¥æ¨¡å¼å¹¶ç”Ÿæˆç¼ºå¤±çš„æ¨¡å¼ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå›¾åƒé¢‘ç‡èåˆç½‘ç»œï¼ˆIFFNï¼‰ï¼Œå®ƒé€šè¿‡å…¨è¾“å…¥æ¨¡å¼åŠå…¶é€‰å®šçš„é«˜é¢‘å‚…é‡Œå¶åˆ†é‡çš„è‡ªç›‘ç£å‰æœŸä»»åŠ¡æ¥å­¦ä¹ ç»Ÿä¸€çš„ç‰¹å¾è¡¨ç¤ºã€‚æ‰€æå‡ºçš„æ‰©æ•£æ¨¡å‹åˆ©ç”¨è¿™ç§è¡¨ç¤ºï¼Œå°è£…äº†å®Œæ•´æ¨¡å¼çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶ç»“åˆè‡ªé€‚åº”é‡å»ºç­–ç•¥æ¥å®ç°ç¼ºå¤±æ¨¡å¼çš„å®Œæˆã€‚åœ¨BraTS 2021æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12840v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†åœ¨ä¸´åºŠå®è·µä¸­ï¼Œç”±äºå¤æ‚çš„é‡‡é›†åè®®ã€ä¸¥æ ¼çš„éšç§è§„å®šæˆ–ç‰¹å®šçš„ä¸´åºŠéœ€æ±‚ï¼Œå…¨é¢æˆåƒå¹¶ä¸æ€»æ˜¯å¯è¡Œçš„ã€‚ç¼ºå¤±çš„MRæ¨¡å¼æ€å¯¹è„‘è‚¿ç˜¤åˆ†å‰²ç­‰ä»»åŠ¡æå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ä¸­ï¼Œå› ä¸ºæ¯ä¸ªæ¨¡æ€éƒ½æä¾›äº†æ”¹å–„å‡†ç¡®æ€§çš„é‡è¦è¡¥å……ä¿¡æ¯ã€‚ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆæ˜¯ç¼ºå¤±æ•°æ®æ’è¡¥ï¼Œå…¶ä¸­ç¼ºå¤±çš„æ¨¡æ€æ˜¯ä»ç°æœ‰çš„æ¨¡æ€ç”Ÿæˆçš„ã€‚è™½ç„¶ç”Ÿæˆæ¨¡å‹å·²è¢«å¹¿æ³›ç”¨äºæ­¤ç›®çš„ï¼Œä½†å¤§å¤šæ•°æœ€å…ˆè¿›çš„æ–¹æ³•ä»…é™äºå•ä¸€æˆ–åŒé‡ç›®æ ‡ç¿»è¯‘ï¼Œç¼ºä¹é€‚åº”ä¸åŒè¾“å…¥é…ç½®ç”Ÿæˆç¼ºå¤±æ¨¡æ€çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”å¤šæ¨¡æ€æ‰©æ•£ç½‘ç»œï¼ˆAMM-Diffï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥å¤„ç†ä»»ä½•æ•°é‡çš„è¾“å…¥æ¨¡æ€å¹¶ç”Ÿæˆç¼ºå¤±çš„æ¨¡æ€ã€‚è®¾è®¡äº†ä¸€ç§å›¾åƒé¢‘ç‡èåˆç½‘ç»œï¼ˆIFFNï¼‰ï¼Œå®ƒé€šè¿‡å…¨è¾“å…¥æ¨¡æ€åŠå…¶é€‰å®šçš„é«˜é¢‘å‚…ç«‹å¶åˆ†é‡çš„è‡ªç›‘ç£é¢„æ–‡æœ¬ä»»åŠ¡å­¦ä¹ ç»Ÿä¸€ç‰¹å¾è¡¨ç¤ºã€‚æ‰€æå‡ºçš„æ‰©æ•£æ¨¡å‹åˆ©ç”¨è¿™ç§è¡¨ç¤ºï¼ŒåŒ…å«å®Œæ•´æ¨¡æ€çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶ç»“åˆè‡ªé€‚åº”é‡å»ºç­–ç•¥æ¥å®ç°ç¼ºå¤±æ¨¡æ€çš„å®Œæˆã€‚åœ¨BraTS 2021æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸´åºŠå®è·µä¸­å…¨é¢æˆåƒä¸æ€»æ˜¯å¯è¡Œï¼Œç¼ºå¤±MRæ¨¡æ€å¯¹ä»»åŠ¡å¦‚è„‘è‚¿ç˜¤åˆ†å‰²å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>ç¼ºå¤±æ•°æ®æ’è¡¥æ˜¯ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œå…¶ä¸­ç¼ºå¤±æ¨¡æ€ä»ç°æœ‰æ¨¡æ€ç”Ÿæˆã€‚</li>
<li>ç°æœ‰ç”Ÿæˆæ¨¡å‹æ–¹æ³•å—é™äºå•ä¸€æˆ–åŒé‡ç›®æ ‡ç¿»è¯‘ï¼Œç¼ºä¹é€‚åº”ä¸åŒè¾“å…¥é…ç½®çš„èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹è‡ªé€‚åº”å¤šæ¨¡æ€æ‰©æ•£ç½‘ç»œï¼ˆAMM-Diffï¼‰ï¼Œèƒ½å¤„ç†ä»»æ„æ•°é‡çš„è¾“å…¥æ¨¡æ€å¹¶ç”Ÿæˆç¼ºå¤±æ¨¡æ€ã€‚</li>
<li>è®¾è®¡äº†å›¾åƒé¢‘ç‡èåˆç½‘ç»œï¼ˆIFFNï¼‰ä»¥å­¦ä¹ ç»Ÿä¸€ç‰¹å¾è¡¨ç¤ºï¼Œé€šè¿‡è‡ªç›‘ç£é¢„æ–‡æœ¬ä»»åŠ¡èåˆå…¨è¾“å…¥æ¨¡æ€åŠå…¶é«˜é¢‘å‚…ç«‹å¶åˆ†é‡ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åˆ©ç”¨åŒ…å«å®Œæ•´æ¨¡æ€å…ˆéªŒçŸ¥è¯†çš„è¡¨ç¤ºï¼Œç»“åˆè‡ªé€‚åº”é‡å»ºç­–ç•¥å®ç°ç¼ºå¤±æ¨¡æ€å®Œæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12840">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-72f00d7dd43f7f068af818cd5f288a35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af2008545ae2b2cfde5896bce7471bd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a25cc65ac3b96c9b4d14bed835eac2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9ac86da308927b41deff2423027d3ac.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="T2ISafety-Benchmark-for-Assessing-Fairness-Toxicity-and-Privacy-in-Image-Generation"><a href="#T2ISafety-Benchmark-for-Assessing-Fairness-Toxicity-and-Privacy-in-Image-Generation" class="headerlink" title="T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in   Image Generation"></a>T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in   Image Generation</h2><p><strong>Authors:Lijun Li, Zhelun Shi, Xuhao Hu, Bowen Dong, Yiran Qin, Xihui Liu, Lu Sheng, Jing Shao</strong></p>
<p>Text-to-image (T2I) models have rapidly advanced, enabling the generation of high-quality images from text prompts across various domains. However, these models present notable safety concerns, including the risk of generating harmful, biased, or private content. Current research on assessing T2I safety remains in its early stages. While some efforts have been made to evaluate models on specific safety dimensions, many critical risks remain unexplored. To address this gap, we introduce T2ISafety, a safety benchmark that evaluates T2I models across three key domains: toxicity, fairness, and bias. We build a detailed hierarchy of 12 tasks and 44 categories based on these three domains, and meticulously collect 70K corresponding prompts. Based on this taxonomy and prompt set, we build a large-scale T2I dataset with 68K manually annotated images and train an evaluator capable of detecting critical risks that previous work has failed to identify, including risks that even ultra-large proprietary models like GPTs cannot correctly detect. We evaluate 12 prominent diffusion models on T2ISafety and reveal several concerns including persistent issues with racial fairness, a tendency to generate toxic content, and significant variation in privacy protection across the models, even with defense methods like concept erasing. Data and evaluator are released under <a target="_blank" rel="noopener" href="https://github.com/adwardlee/t2i_safety">https://github.com/adwardlee/t2i_safety</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹å·²å¾—åˆ°è¿…é€Ÿå‘å±•ï¼Œèƒ½å¤Ÿåœ¨å„ä¸ªé¢†åŸŸæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¼•å‘äº†æ˜¾è‘—çš„å®‰å…¨é—®é¢˜ï¼ŒåŒ…æ‹¬ç”Ÿæˆæœ‰å®³ã€åè§æˆ–ç§äººå†…å®¹çš„æ½œåœ¨é£é™©ã€‚å½“å‰å¯¹äºT2Iå®‰å…¨æ€§çš„è¯„ä¼°ç ”ç©¶ä»å¤„äºåˆçº§é˜¶æ®µã€‚å°½ç®¡å·²ç»æœ‰ä¸€äº›åŠªåŠ›åœ¨æŸäº›ç‰¹å®šçš„å®‰å…¨ç»´åº¦ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œä½†è¿˜æœ‰è®¸å¤šå…³é”®é£é™©å°šæœªæ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†T2ISafetyå®‰å…¨åŸºå‡†ï¼Œç”¨äºåœ¨æ¯’æ€§ã€å…¬å¹³æ€§å’Œåè§è¿™ä¸‰ä¸ªå…³é”®é¢†åŸŸè¯„ä¼°T2Iæ¨¡å‹ã€‚æˆ‘ä»¬åœ¨è¿™ä¸‰ä¸ªé¢†åŸŸçš„åŸºç¡€ä¸Šå»ºç«‹äº†åŒ…å«ä¸‰ä¸ªå±‚çº§ä»»åŠ¡ï¼ˆæ€»è®¡å››çº§ä»»åŠ¡åˆ†ç±»ï¼‰çš„è¯¦ç»†å±‚æ¬¡ç»“æ„ï¼Œå¹¶ç²¾å¿ƒæ”¶é›†äº†ç›¸åº”çš„æç¤ºè¯­å…±ä¸ƒä¸‡æ¡ã€‚åŸºäºè¿™ç§åˆ†ç±»å’Œæç¤ºé›†ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„T2Iæ•°æ®é›†ï¼ˆå…±å…­åå…«ä¸‡å¼ äººå·¥æ³¨é‡Šçš„å›¾åƒï¼‰ï¼Œå¹¶è®­ç»ƒå‡ºä¸€ç§èƒ½å¤Ÿæ£€æµ‹å‡ºä»¥å‰æœªè¯†åˆ«å‡ºé‡å¤§é£é™©çš„è¯„ä¼°å™¨ã€‚ç”šè‡³èƒ½æ£€æµ‹åˆ°å¦‚GPTç­‰å¤§å‹ä¸“æœ‰æ¨¡å‹æœªèƒ½å¯Ÿè§‰çš„é£é™©ã€‚æˆ‘ä»¬å¯¹åäºŒä¸ªä¸»æµæ‰©æ•£æ¨¡å‹è¿›è¡Œäº†T2ISafetyè¯„ä¼°ï¼Œå¹¶æ­ç¤ºäº†åŒ…æ‹¬ç§æ—å…¬å¹³é—®é¢˜æŒç»­å­˜åœ¨ã€å€¾å‘äºç”Ÿæˆæœ‰æ¯’å†…å®¹ä»¥åŠéšç§ä¿æŠ¤åœ¨æ¨¡å‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ç­‰æ‹…å¿§ï¼Œå³ä½¿é‡‡ç”¨æ¦‚å¿µæ¶ˆé™¤ç­‰é˜²å¾¡æ–¹æ³•ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ•°æ®å’Œè¯„ä¼°å™¨å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/adwardlee/t2i_safety%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/adwardlee/t2i_safetyä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12612v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬è½¬å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹åœ¨å®‰å…¨é¢†åŸŸå­˜åœ¨é£é™©éšæ‚£ï¼Œå¦‚å¯èƒ½ç”Ÿæˆæœ‰å®³ã€æœ‰åè§æˆ–æ¶‰åŠéšç§çš„å†…å®¹ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†T2ISafetyå®‰å…¨åŸºå‡†è¯„ä¼°ç³»ç»Ÿï¼Œé’ˆå¯¹æ¯’æ€§ã€å…¬å¹³æ€§å’Œåè§ä¸‰ä¸ªå…³é”®é¢†åŸŸè¿›è¡Œç»†è‡´è¯„ä¼°ã€‚ä»–ä»¬å»ºç«‹äº†åŒ…å«12é¡¹ä»»åŠ¡å’Œ44ä¸ªç±»åˆ«çš„è¯¦ç»†å±‚æ¬¡ç»“æ„ï¼Œå¹¶æ”¶é›†äº†7ä¸‡æ¡ç›¸åº”æç¤ºã€‚åŸºäºè¿™ä¸€åˆ†ç±»å’Œæç¤ºé›†ï¼Œä»–ä»¬å»ºç«‹äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„T2Iæ•°æ®é›†ï¼ŒåŒ…å«6.8ä¸‡å¼ æ‰‹åŠ¨æ ‡æ³¨çš„å›¾åƒï¼Œå¹¶è®­ç»ƒäº†ä¸€ä¸ªèƒ½å¤Ÿæ£€æµ‹å‡ºå…ˆå‰å·¥ä½œæœªèƒ½è¯†åˆ«çš„å…³é”®é£é™©çš„è¯„ä¼°å™¨ã€‚ä»–ä»¬å¯¹12ä¸ªä¸»æµçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œäº†T2ISafetyè¯„ä¼°ï¼Œå¹¶æ­ç¤ºäº†åŒ…æ‹¬ç§æ—å…¬å¹³é—®é¢˜ã€ç”Ÿæˆæœ‰æ¯’å†…å®¹å€¾å‘ä»¥åŠæ¨¡å‹é—´éšç§ä¿æŠ¤æ˜¾è‘—å·®å¼‚ç­‰éšæ‚£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2Iæ¨¡å‹åœ¨å®‰å…¨é¢†åŸŸå­˜åœ¨é£é™©ï¼Œå¯èƒ½ç”Ÿæˆæœ‰å®³ã€æœ‰åè§æˆ–æ¶‰åŠéšç§çš„å†…å®¹ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†T2ISafetyå®‰å…¨åŸºå‡†è¯„ä¼°ç³»ç»Ÿï¼Œé’ˆå¯¹æ¯’æ€§ã€å…¬å¹³æ€§å’Œåè§ä¸‰å¤§é¢†åŸŸè¿›è¡Œè¯„ä¼°ã€‚</li>
<li>T2ISafetyå»ºç«‹äº†åŒ…å«12é¡¹ä»»åŠ¡å’Œ44ä¸ªç±»åˆ«çš„è¯¦ç»†å±‚æ¬¡ç»“æ„ï¼Œå¹¶æ”¶é›†äº†7ä¸‡æ¡æç¤ºç”¨äºè¯„ä¼°ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿå»ºç«‹äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„T2Iæ•°æ®é›†ï¼ŒåŒ…å«6.8ä¸‡å¼ æ‰‹åŠ¨æ ‡æ³¨çš„å›¾åƒã€‚</li>
<li>è®­ç»ƒäº†ä¸€ä¸ªè¯„ä¼°å™¨ï¼Œå¯æ£€æµ‹å‡ºå…ˆå‰æœªè¯†åˆ«çš„å…³é”®é£é™©ã€‚</li>
<li>å¯¹12ä¸ªä¸»æµçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œäº†T2ISafetyè¯„ä¼°ï¼Œå‘ç°å­˜åœ¨ç§æ—å…¬å¹³é—®é¢˜ã€ç”Ÿæˆæœ‰æ¯’å†…å®¹å€¾å‘ç­‰éšæ‚£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12612">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8c33ccf944033a38dc9d8f73ac4f0835.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc52768e37b3a7491f639d5edd6e1a73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32ef989ed17f0a5f8a8fa04b7d0b768c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-089743b87379bcdb99f64fb2413e76dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc0895fdb436f2b410b074ab272ca95c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Image-Motion-Blur-Removal-in-the-Temporal-Dimension-with-Video-Diffusion-Models"><a href="#Image-Motion-Blur-Removal-in-the-Temporal-Dimension-with-Video-Diffusion-Models" class="headerlink" title="Image Motion Blur Removal in the Temporal Dimension with Video Diffusion   Models"></a>Image Motion Blur Removal in the Temporal Dimension with Video Diffusion   Models</h2><p><strong>Authors:Wang Pang, Zhihao Zhan, Xiang Zhu, Yechao Bai</strong></p>
<p>Most motion deblurring algorithms rely on spatial-domain convolution models, which struggle with the complex, non-linear blur arising from camera shake and object motion. In contrast, we propose a novel single-image deblurring approach that treats motion blur as a temporal averaging phenomenon. Our core innovation lies in leveraging a pre-trained video diffusion transformer model to capture diverse motion dynamics within a latent space. It sidesteps explicit kernel estimation and effectively accommodates diverse motion patterns. We implement the algorithm within a diffusion-based inverse problem framework. Empirical results on synthetic and real-world datasets demonstrate that our method outperforms existing techniques in deblurring complex motion blur scenarios. This work paves the way for utilizing powerful video diffusion models to address single-image deblurring challenges. </p>
<blockquote>
<p>å¤§éƒ¨åˆ†çš„è¿åŠ¨å»æ¨¡ç³Šç®—æ³•éƒ½ä¾èµ–äºç©ºé—´åŸŸå·ç§¯æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†ç”±ç›¸æœºæŠ–åŠ¨å’Œç‰©ä½“è¿åŠ¨å¼•èµ·çš„å¤æ‚ã€éçº¿æ€§æ¨¡ç³Šæ—¶æ˜¾å¾—æ‰è¥Ÿè§è‚˜ã€‚ä¸æ­¤ç›¸åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å•å›¾åƒå»æ¨¡ç³Šæ–¹æ³•ï¼Œå®ƒå°†è¿åŠ¨æ¨¡ç³Šè§†ä¸ºä¸€ç§æ—¶é—´å¹³å‡ç°è±¡ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£transformeræ¨¡å‹ï¼Œåœ¨æ½œåœ¨ç©ºé—´å†…æ•æ‰å„ç§è¿åŠ¨åŠ¨æ€ã€‚å®ƒé¿å¼€äº†æ˜ç¡®çš„æ ¸ä¼°è®¡ï¼Œæœ‰æ•ˆåœ°é€‚åº”äº†å„ç§è¿åŠ¨æ¨¡å¼ã€‚æˆ‘ä»¬åœ¨åŸºäºæ‰©æ•£çš„é€†å‘é—®é¢˜æ¡†æ¶å†…å®ç°äº†è¯¥ç®—æ³•ã€‚åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å»é™¤å¤æ‚è¿åŠ¨æ¨¡ç³Šåœºæ™¯æ–¹é¢çš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚è¿™é¡¹å·¥ä½œä¸ºåˆ©ç”¨å¼ºå¤§çš„è§†é¢‘æ‰©æ•£æ¨¡å‹è§£å†³å•å›¾åƒå»æ¨¡ç³ŠæŒ‘æˆ˜é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12604v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ä¼ ç»Ÿè¿åŠ¨å»æ¨¡ç³Šç®—æ³•åœ¨åº”å¯¹ç”±ç›¸æœºæŠ–åŠ¨å’Œç‰©ä½“è¿åŠ¨å¼•èµ·çš„å¤æ‚éçº¿æ€§æ¨¡ç³Šæ—¶çš„å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å•å›¾åƒå»æ¨¡ç³Šæ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†è¿åŠ¨æ¨¡ç³Šè§†ä¸ºæ—¶é—´å¹³å‡ç°è±¡ï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è§†é¢‘æ‰©æ•£è½¬æ¢å™¨æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´å†…æ•æ‰å„ç§è¿åŠ¨åŠ¨æ€ã€‚è¯¥æ–¹æ³•é¿å…äº†æ˜ç¡®çš„æ ¸ä¼°è®¡ï¼Œå¹¶æœ‰æ•ˆåœ°é€‚åº”äº†å„ç§è¿åŠ¨æ¨¡å¼ã€‚å®éªŒç»“æœè¯æ˜ï¼Œåœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤æ‚çš„è¿åŠ¨æ¨¡ç³Šåœºæ™¯çš„å»æ¨¡ç³ŠæŠ€æœ¯ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¤§å¤šæ•°è¿åŠ¨å»æ¨¡ç³Šç®—æ³•ä¸»è¦ä¾èµ–ç©ºé—´åŸŸå·ç§¯æ¨¡å‹ï¼Œéš¾ä»¥å¤„ç†å¤æ‚çš„éçº¿æ€§æ¨¡ç³Šã€‚</li>
<li>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å•å›¾åƒå»æ¨¡ç³Šæ–¹æ³•ï¼Œå°†è¿åŠ¨æ¨¡ç³Šè§†ä¸ºæ—¶é—´å¹³å‡ç°è±¡ã€‚</li>
<li>æ–¹æ³•çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è§†é¢‘æ‰©æ•£è½¬æ¢å™¨æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æ½œåœ¨ç©ºé—´å†…æ•æ‰å„ç§è¿åŠ¨åŠ¨æ€ï¼Œé¿å…äº†æ˜ç¡®çš„æ ¸ä¼°è®¡ã€‚</li>
<li>æ–¹æ³•å¯ä»¥é€‚åº”å¤šç§è¿åŠ¨æ¨¡å¼ï¼Œå¯¹å¤æ‚çš„è¿åŠ¨æ¨¡ç³Šåœºæ™¯æœ‰å¾ˆå¥½çš„å»æ¨¡ç³Šæ•ˆæœã€‚</li>
<li>å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-02b985ac8749884e14d05a4c86a0fdb4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a42362520175822e3979e92166715baa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab76910992cbf120bd358a36287829ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26f9e2554c0be6a989eefd7f3807f081.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6321d26fc9b22b5fb462bc1bcb92192f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4031e5e58508ecd7dd4890b088eb0b90.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ecd41d6ea1199e6a98e2b0c2100e0875.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Ensemble-score-filter-with-image-inpainting-for-data-assimilation-in-tracking-surface-quasi-geostrophic-dynamics-with-partial-observations"><a href="#Ensemble-score-filter-with-image-inpainting-for-data-assimilation-in-tracking-surface-quasi-geostrophic-dynamics-with-partial-observations" class="headerlink" title="Ensemble score filter with image inpainting for data assimilation in   tracking surface quasi-geostrophic dynamics with partial observations"></a>Ensemble score filter with image inpainting for data assimilation in   tracking surface quasi-geostrophic dynamics with partial observations</h2><p><strong>Authors:Siming Liang, Hoang Tran, Feng Bao, Hristo G. Chipilski, Peter Jan van Leeuwen, Guannan Zhang</strong></p>
<p>Data assimilation plays a pivotal role in understanding and predicting turbulent systems within geoscience and weather forecasting, where data assimilation is used to address three fundamental challenges, i.e., high-dimensionality, nonlinearity, and partial observations. Recent advances in machine learning (ML)-based data assimilation methods have demonstrated encouraging results. In this work, we develop an ensemble score filter (EnSF) that integrates image inpainting to solve the data assimilation problems with partial observations. The EnSF method exploits an exclusively designed training-free diffusion models to solve high-dimensional nonlinear data assimilation problems. Its performance has been successfully demonstrated in the context of having full observations, i.e., all the state variables are directly or indirectly observed. However, because the EnSF does not use a covariance matrix to capture the dependence between the observed and unobserved state variables, it is nontrivial to extend the original EnSF method to the partial observation scenario. In this work, we incorporate various image inpainting techniques into the EnSF to predict the unobserved states during data assimilation. At each filtering step, we first use the diffusion model to estimate the observed states by integrating the likelihood information into the score function. Then, we use image inpainting methods to predict the unobserved state variables. We demonstrate the performance of the EnSF with inpainting by tracking the Surface Quasi-Geostrophic (SQG) model dynamics under a variety of scenarios. The successful proof of concept paves the way to more in-depth investigations on exploiting modern image inpainting techniques to advance data assimilation methodology for practical geoscience and weather forecasting problems. </p>
<blockquote>
<p>æ•°æ®åŒåŒ–åœ¨åœ°çƒç§‘å­¦å’Œå¤©æ°”é¢„æŠ¥ä¸­ç†è§£å’Œé¢„æµ‹æ¹æµç³»ç»Ÿæ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œæ•°æ®åŒåŒ–ç”¨äºåº”å¯¹ä¸‰ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼Œå³é«˜ç»´æ€§ã€éçº¿æ€§å’Œéƒ¨åˆ†è§‚æµ‹ã€‚åŸºäºæœºå™¨å­¦ä¹ çš„æ•°æ®åŒåŒ–æ–¹æ³•çš„æœ€æ–°è¿›å±•å·²ç»å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é›†æˆå›¾åƒä¿®å¤æŠ€æœ¯çš„ä¸€ä½“åŒ–è¯„åˆ†è¿‡æ»¤æ–¹æ³•ï¼ˆEnSFï¼‰ï¼Œä»¥è§£å†³éƒ¨åˆ†è§‚æµ‹æ•°æ®åŒåŒ–é—®é¢˜ã€‚EnSFæ–¹æ³•åˆ©ç”¨ä¸“é—¨è®¾è®¡çš„æ— è®­ç»ƒæ‰©æ•£æ¨¡å‹æ¥è§£å†³é«˜ç»´éçº¿æ€§æ•°æ®åŒåŒ–é—®é¢˜ã€‚åœ¨å…·æœ‰å…¨è§‚æµ‹çš„æƒ…å¢ƒä¸­ï¼Œå³æ‰€æœ‰çŠ¶æ€å˜é‡ç›´æ¥æˆ–é—´æ¥è§‚æµ‹çš„æƒ…å†µä¸‹ï¼Œå…¶æ€§èƒ½å·²ç»å¾—åˆ°äº†æˆåŠŸéªŒè¯ã€‚ç„¶è€Œï¼Œç”±äºEnSFæ²¡æœ‰ä½¿ç”¨åæ–¹å·®çŸ©é˜µæ¥æ•æ‰è§‚æµ‹çŠ¶æ€å˜é‡å’Œæœªè§‚æµ‹çŠ¶æ€å˜é‡ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå› æ­¤å°†åŸå§‹EnSFæ–¹æ³•æ‰©å±•åˆ°éƒ¨åˆ†è§‚æµ‹æƒ…æ™¯å¹¶ä¸ç®€å•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†å„ç§å›¾åƒä¿®å¤æŠ€æœ¯èå…¥EnSFä¸­ï¼Œä»¥åœ¨æ•°æ®åŒåŒ–è¿‡ç¨‹ä¸­é¢„æµ‹æœªè§‚æµ‹çŠ¶æ€ã€‚åœ¨æ¯ä¸ªè¿‡æ»¤æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨æ‰©æ•£æ¨¡å‹é€šè¿‡æ•´åˆå¯èƒ½æ€§ä¿¡æ¯æ¥ä¼°è®¡è§‚æµ‹çŠ¶æ€çš„å€¼ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨å›¾åƒä¿®å¤æ–¹æ³•æ¥é¢„æµ‹æœªè§‚æµ‹çŠ¶æ€å˜é‡ã€‚æˆ‘ä»¬é€šè¿‡è·Ÿè¸ªå„ç§åœºæ™¯ä¸‹çš„è¡¨é¢å‡†åœ°è½¬æ¨¡å‹åŠ¨åŠ›å­¦æ¥å±•ç¤ºå¸¦æœ‰ä¿®å¤åŠŸèƒ½çš„EnSFçš„æ€§èƒ½ã€‚æˆåŠŸçš„æ¦‚å¿µéªŒè¯ä¸ºåˆ©ç”¨ç°ä»£å›¾åƒä¿®å¤æŠ€æœ¯è¿›ä¸€æ­¥æ¨åŠ¨æ•°æ®åŒåŒ–æ–¹æ³•åœ¨åœ°çƒç§‘å­¦å’Œå¤©æ°”é¢„æŠ¥é—®é¢˜ä¸­çš„åº”ç”¨å¼€è¾Ÿäº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12419v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ç§é›†æˆå›¾åƒä¿®å¤æŠ€æœ¯çš„é›†åˆè¯„åˆ†è¿‡æ»¤å™¨ï¼ˆEnSFï¼‰ï¼Œä»¥è§£å†³å…·æœ‰éƒ¨åˆ†è§‚æµ‹çš„æ•°æ®åŒåŒ–é—®é¢˜ã€‚EnSFåˆ©ç”¨ä¸“é—¨è®¾è®¡çš„æ— éœ€è®­ç»ƒæ‰©æ•£æ¨¡å‹è§£å†³é«˜ç»´éçº¿æ€§æ•°æ®åŒåŒ–é—®é¢˜ã€‚ç ”ç©¶æˆåŠŸè¯æ˜äº†å…¶åœ¨å…¨è§‚æµ‹ç¯å¢ƒä¸‹çš„æ€§èƒ½ï¼Œä½†é¢å¯¹éƒ¨åˆ†è§‚æµ‹æƒ…æ™¯æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å°†å„ç§å›¾åƒä¿®å¤æŠ€æœ¯èå…¥EnSFä¸­ï¼Œç”¨äºé¢„æµ‹æ•°æ®åŒåŒ–ä¸­çš„æœªè§‚æµ‹çŠ¶æ€ã€‚æ¯ä¸€æ­¥è¿‡æ»¤éƒ½å…ˆåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç»“åˆä¼¼ç„¶ä¿¡æ¯ä¼°è®¡è§‚æµ‹çŠ¶æ€ï¼Œç„¶åä½¿ç”¨å›¾åƒä¿®å¤æ–¹æ³•é¢„æµ‹æœªè§‚æµ‹çŠ¶æ€å˜é‡ã€‚é€šè¿‡å¯¹Surface Quasi-Geostrophicï¼ˆSQGï¼‰æ¨¡å‹åŠ¨æ€è¿›è¡Œè¿½è¸ªï¼Œè¯æ˜äº†EnSFä¸å›¾åƒä¿®å¤ç›¸ç»“åˆçš„æ€§èƒ½ã€‚è¿™ä¸ºåˆ©ç”¨ç°ä»£å›¾åƒä¿®å¤æŠ€æœ¯æ¨åŠ¨æ•°æ®åŒåŒ–æ–¹æ³•åœ¨åœ°çƒç§‘å­¦å’Œå¤©æ°”é¢„æŠ¥é—®é¢˜ä¸­çš„å®é™…åº”ç”¨é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ•°æ®åŒåŒ–åœ¨ç†è§£å’Œé¢„æµ‹åœ°çƒç§‘å­¦å’Œå¤©æ°”é¢„æŠ¥ä¸­çš„æ¹æµç³»ç»Ÿæ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œè§£å†³äº†é«˜ç»´ã€éçº¿æ€§å’Œéƒ¨åˆ†è§‚æµ‹ä¸‰å¤§æŒ‘æˆ˜ã€‚</li>
<li>åŸºäºæœºå™¨å­¦ä¹ çš„æ•°æ®åŒåŒ–æ–¹æ³•å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚</li>
<li>é›†åˆè¯„åˆ†è¿‡æ»¤å™¨ï¼ˆEnSFï¼‰ç»“åˆäº†å›¾åƒä¿®å¤æŠ€æœ¯ä»¥è§£å†³å…·æœ‰éƒ¨åˆ†è§‚æµ‹çš„æ•°æ®åŒåŒ–é—®é¢˜ã€‚</li>
<li>EnSFåˆ©ç”¨æ— éœ€è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è§£å†³é«˜ç»´éçº¿æ€§é—®é¢˜ã€‚</li>
<li>EnSFåœ¨å…¨è§‚æµ‹ç¯å¢ƒä¸‹è¡¨ç°å‡ºæˆåŠŸçš„æ€§èƒ½ã€‚</li>
<li>é¢å¯¹éƒ¨åˆ†è§‚æµ‹æƒ…æ™¯çš„æŒ‘æˆ˜ï¼ŒEnSFé€šè¿‡èå…¥å›¾åƒä¿®å¤æŠ€æœ¯é¢„æµ‹æœªè§‚æµ‹çŠ¶æ€ã€‚</li>
<li>é€šè¿‡è¿½è¸ªSQGæ¨¡å‹åŠ¨æ€è¯æ˜äº†EnSFä¸å›¾åƒä¿®å¤ç»“åˆçš„æ€§èƒ½ï¼Œè¿™ä¸ºæœªæ¥åœ¨åœ°çƒç§‘å­¦å’Œå¤©æ°”é¢„æŠ¥ä¸­å®é™…åº”ç”¨æä¾›äº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d913029468fca6ecd6e21cdfb4fc61ed.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="GPS-as-a-Control-Signal-for-Image-Generation"><a href="#GPS-as-a-Control-Signal-for-Image-Generation" class="headerlink" title="GPS as a Control Signal for Image Generation"></a>GPS as a Control Signal for Image Generation</h2><p><strong>Authors:Chao Feng, Ziyang Chen, Aleksander Holynski, Alexei A. Efros, Andrew Owens</strong></p>
<p>We show that the GPS tags contained in photo metadata provide a useful control signal for image generation. We train GPS-to-image models and use them for tasks that require a fine-grained understanding of how images vary within a city. In particular, we train a diffusion model to generate images conditioned on both GPS and text. The learned model generates images that capture the distinctive appearance of different neighborhoods, parks, and landmarks. We also extract 3D models from 2D GPS-to-image models through score distillation sampling, using GPS conditioning to constrain the appearance of the reconstruction from each viewpoint. Our evaluations suggest that our GPS-conditioned models successfully learn to generate images that vary based on location, and that GPS conditioning improves estimated 3D structure. </p>
<blockquote>
<p>æˆ‘ä»¬è¯æ˜ï¼Œç…§ç‰‡å…ƒæ•°æ®ä¸­åŒ…å«çš„GPSæ ‡ç­¾ä¸ºå›¾åƒç”Ÿæˆæä¾›äº†æœ‰ç”¨çš„æ§åˆ¶ä¿¡å·ã€‚æˆ‘ä»¬è®­ç»ƒGPSåˆ°å›¾åƒçš„æ¨¡å‹ï¼Œå¹¶å°†å®ƒä»¬ç”¨äºéœ€è¦ç²¾ç»†ç†è§£å›¾åƒå¦‚ä½•åœ¨åŸå¸‚å†…éƒ¨å˜åŒ–çš„ä»»åŠ¡ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œä»¥æ ¹æ®GPSå’Œæ–‡æœ¬ç”Ÿæˆå›¾åƒã€‚å­¦ä¹ åˆ°çš„æ¨¡å‹ç”Ÿæˆèƒ½å¤Ÿæ•æ‰ä¸åŒè¡—åŒºã€å…¬å›­å’Œåœ°æ ‡ç‹¬ç‰¹å¤–è§‚çš„å›¾åƒã€‚æˆ‘ä»¬è¿˜é€šè¿‡è¯„åˆ†è’¸é¦é‡‡æ ·ä»äºŒç»´GPSåˆ°å›¾åƒçš„æ¨¡å‹ä¸­æå–ä¸‰ç»´æ¨¡å‹ï¼Œä½¿ç”¨GPSæ¡ä»¶æ¥çº¦æŸæ¯ä¸ªè§†ç‚¹çš„é‡å»ºå¤–è§‚ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„GPSæ¡ä»¶æ¨¡å‹æˆåŠŸå­¦ä¹ æ ¹æ®ä½ç½®ç”Ÿæˆå›¾åƒï¼Œå¹¶ä¸”GPSæ¡ä»¶æ”¹è¿›äº†ä¼°è®¡çš„3Dç»“æ„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12390v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://cfeng16.github.io/gps-gen/">https://cfeng16.github.io/gps-gen/</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡å±•ç¤ºäº†ç…§ç‰‡å…ƒæ•°æ®ä¸­çš„GPSæ ‡ç­¾å¯¹äºå›¾åƒç”Ÿæˆå…·æœ‰æœ‰ç”¨çš„æ§åˆ¶ä¿¡å·ã€‚ç ”ç©¶å›¢é˜Ÿè®­ç»ƒäº†GPSåˆ°å›¾åƒçš„æ¨¡å‹ï¼Œå¹¶ç”¨äºéœ€è¦ç²¾ç»†ç†è§£åŸå¸‚å†…å›¾åƒå¦‚ä½•å˜åŒ–çš„ä»»åŠ¡ã€‚ç‰¹åˆ«æ˜¯ï¼Œä»–ä»¬è®­ç»ƒäº†ä¸€ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥æ ¹æ®GPSå’Œæ–‡æœ¬ç”Ÿæˆå›¾åƒã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæ•æ‰ä¸åŒè¡—åŒºã€å…¬å›­å’Œåœ°æ ‡çš„ç‹¬ç‰¹å¤–è§‚ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿé€šè¿‡è¯„åˆ†è’¸é¦é‡‡æ ·ä»äºŒç»´GPSåˆ°å›¾åƒæ¨¡å‹ä¸­æå–ä¸‰ç»´æ¨¡å‹ï¼Œåˆ©ç”¨GPSæ¡ä»¶çº¦æŸä»æ¯ä¸ªè§†è§’è¿›è¡Œé‡å»ºçš„å¤–è§‚ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒGPSæ¡ä»¶æ¨¡å‹æˆåŠŸå­¦ä¹ æ ¹æ®ä½ç½®ç”Ÿæˆå›¾åƒï¼Œå¹¶ä¸”GPSæ¡ä»¶æ”¹è¿›äº†ä¼°è®¡çš„ä¸‰ç»´ç»“æ„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPSæ ‡ç­¾åœ¨å›¾åƒç”Ÿæˆä¸­æä¾›æœ‰ç”¨çš„æ§åˆ¶ä¿¡å·ã€‚</li>
<li>é€šè¿‡è®­ç»ƒGPSåˆ°å›¾åƒçš„æ¨¡å‹ï¼Œå®ç°äº†å¯¹åŸå¸‚å†…å›¾åƒå˜åŒ–çš„ç²¾ç»†ç†è§£ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å¯ä»¥æ ¹æ®GPSå’Œæ–‡æœ¬ç”Ÿæˆå›¾åƒï¼Œèƒ½æ•æ‰ä¸åŒåœ°ç‚¹çš„ç‹¬ç‰¹å¤–è§‚ã€‚</li>
<li>é€šè¿‡è¯„åˆ†è’¸é¦é‡‡æ ·ä»äºŒç»´GPSåˆ°å›¾åƒæ¨¡å‹ä¸­æå–ä¸‰ç»´æ¨¡å‹ã€‚</li>
<li>GPSæ¡ä»¶ç”¨äºçº¦æŸä»å„ä¸ªè§†è§’çš„é‡å»ºå¤–è§‚ã€‚</li>
<li>GPSæ¡ä»¶æ¨¡å‹èƒ½å¤ŸæˆåŠŸæ ¹æ®ä½ç½®ç”Ÿæˆå›¾åƒã€‚</li>
<li>GPSæ¡ä»¶æ”¹è¿›äº†ä¼°è®¡çš„ä¸‰ç»´ç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12390">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-21f51f2a5a39977be5238d681598a715.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0a76df4bcf6fe913aa566dcc5c18ed7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ecfe5f0da70970b64df689ab69f1b85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0285c3db34c02b3c60e4053cdd434b04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cec582bfbfea2b01e76c1a34c76786e1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Diffusion-Models-for-Anomaly-Detection"><a href="#A-Survey-on-Diffusion-Models-for-Anomaly-Detection" class="headerlink" title="A Survey on Diffusion Models for Anomaly Detection"></a>A Survey on Diffusion Models for Anomaly Detection</h2><p><strong>Authors:Jing Liu, Zhenchao Ma, Zepu Wang, Yang Liu, Zehua Wang, Peng Sun, Liang Song, Bo Hu, Azzedine Boukerche, Victor C. M. Leung</strong></p>
<p>Diffusion models (DMs) have emerged as a powerful class of generative AI models, showing remarkable potential in anomaly detection (AD) tasks across various domains, such as cybersecurity, fraud detection, healthcare, and manufacturing. The intersection of these two fields, termed diffusion models for anomaly detection (DMAD), offers promising solutions for identifying deviations in increasingly complex and high-dimensional data. In this survey, we review recent advances in DMAD research. We begin by presenting the fundamental concepts of AD and DMs, followed by a comprehensive analysis of classic DM architectures including DDPMs, DDIMs, and Score SDEs. We further categorize existing DMAD methods into reconstruction-based, density-based, and hybrid approaches, providing detailed examinations of their methodological innovations. We also explore the diverse tasks across different data modalities, encompassing image, time series, video, and multimodal data analysis. Furthermore, we discuss critical challenges and emerging research directions, including computational efficiency, model interpretability, robustness enhancement, edge-cloud collaboration, and integration with large language models. The collection of DMAD research papers and resources is available at <a target="_blank" rel="noopener" href="https://github.com/fdjingliu/DMAD">https://github.com/fdjingliu/DMAD</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ä½œä¸ºä¸€ç±»å¼ºå¤§çš„ç”Ÿæˆäººå·¥æ™ºèƒ½æ¨¡å‹å·²ç»å´­éœ²å¤´è§’ï¼Œåœ¨å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—æ½œåŠ›ï¼Œå¹¿æ³›åº”ç”¨äºç½‘ç»œå®‰å…¨ã€æ¬ºè¯ˆæ£€æµ‹ã€åŒ»ç–—ä¿å¥å’Œåˆ¶é€ ç­‰å¤šä¸ªé¢†åŸŸã€‚è¿™ä¸¤ä¸ªé¢†åŸŸçš„äº¤é›†ï¼Œè¢«ç§°ä¸ºç”¨äºå¼‚å¸¸æ£€æµ‹çš„æ‰©æ•£æ¨¡å‹ï¼ˆDMADï¼‰ï¼Œä¸ºè¯†åˆ«æ—¥ç›Šå¤æ‚å’Œé«˜ç»´æ•°æ®ä¸­çš„åå·®æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨è¿™ç¯‡ç»¼è¿°ä¸­ï¼Œæˆ‘ä»¬å›é¡¾äº†DMADç ”ç©¶çš„æœ€æ–°è¿›å±•ã€‚é¦–å…ˆä»‹ç»ADå’ŒDMçš„åŸºæœ¬æ¦‚å¿µï¼Œç„¶åå…¨é¢åˆ†æç»å…¸çš„DMæ¶æ„ï¼ŒåŒ…æ‹¬DDPMsã€DDIMSå’ŒScore SDEsã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°†ç°æœ‰çš„DMADæ–¹æ³•åˆ†ä¸ºåŸºäºé‡å»ºçš„ã€åŸºäºå¯†åº¦çš„å’Œæ··åˆæ–¹æ³•ï¼Œå¹¶å¯¹å…¶æ–¹æ³•åˆ›æ–°è¿›è¡Œè¯¦ç»†æ£€æŸ¥ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†ä¸åŒæ•°æ®æ¨¡æ€çš„å¤šæ ·åŒ–ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒã€æ—¶é—´åºåˆ—ã€è§†é¢‘å’Œå¤šæ¨¡æ€æ•°æ®åˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†å…³é”®çš„æŒ‘æˆ˜å’Œæ–°å…´çš„ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬è®¡ç®—æ•ˆç‡ã€æ¨¡å‹å¯è§£é‡Šæ€§ã€é²æ£’æ€§å¢å¼ºã€è¾¹ç¼˜äº‘åä½œä»¥åŠä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„é›†æˆã€‚DMADç ”ç©¶è®ºæ–‡å’Œèµ„æºé›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/fdjingliu/DMAD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/fdjingliu/DMADæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11430v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ä½œä¸ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹çš„æ–°å…´å¼ºå¤§ç±»åˆ«ï¼Œåœ¨å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå¹¿æ³›åº”ç”¨äºç½‘ç»œå®‰å…¨ã€æ¬ºè¯ˆæ£€æµ‹ã€åŒ»ç–—ä¿å¥å’Œåˆ¶é€ ç­‰é¢†åŸŸã€‚æœ¬æ–‡ç»¼è¿°äº†æ‰©æ•£æ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹æ–¹é¢çš„æœ€æ–°ç ”ç©¶è¿›å±•ï¼Œä»‹ç»äº†å¼‚å¸¸æ£€æµ‹å’Œæ‰©æ•£æ¨¡å‹çš„åŸºæœ¬æ¦‚å¿µï¼Œåˆ†æäº†ç»å…¸çš„æ‰©æ•£æ¨¡å‹æ¶æ„ï¼ŒåŒ…æ‹¬DDPMsã€DDIIMså’ŒScore SDEsï¼Œå¹¶å°†ç°æœ‰çš„æ‰©æ•£æ¨¡å‹å¼‚å¸¸æ£€æµ‹æ–¹æ³•åˆ†ä¸ºé‡å»ºå‹ã€å¯†åº¦å‹å’Œæ··åˆå‹æ–¹æ³•ï¼Œè¯¦ç»†æ¢è®¨äº†å…¶æ–¹æ³•åˆ›æ–°ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†ä¸åŒæ•°æ®æ¨¡æ€çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒã€æ—¶é—´åºåˆ—ã€è§†é¢‘å’Œå¤šæ¨¡æ€æ•°æ®åˆ†æã€‚æ–‡ç« è¿˜è®¨è®ºäº†è®¡ç®—æ•ˆç‡ã€æ¨¡å‹å¯è§£é‡Šæ€§ã€é²æ£’æ€§å¢å¼ºã€è¾¹ç¼˜äº‘åä½œä»¥åŠä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„é›†æˆç­‰å…³é”®æŒ‘æˆ˜å’Œæ–°å…´ç ”ç©¶æ–¹å‘ã€‚ç›¸å…³èµ„æºå¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/fdjingliu/DMAD%E3%80%82">https://github.com/fdjingliu/DMADã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ï¼Œåº”ç”¨èŒƒå›´å¹¿æ³›ã€‚</li>
<li>æ–‡ç« ç»¼è¿°äº†DMsåœ¨å¼‚å¸¸æ£€æµ‹æ–¹é¢çš„æœ€æ–°ç ”ç©¶è¿›å±•ã€‚</li>
<li>ä»‹ç»äº†å¼‚å¸¸æ£€æµ‹å’Œæ‰©æ•£æ¨¡å‹çš„åŸºæœ¬æ¦‚å¿µã€‚</li>
<li>åˆ†æäº†ç»å…¸çš„æ‰©æ•£æ¨¡å‹æ¶æ„ï¼ŒåŒ…æ‹¬DDPMsã€DDIIMså’ŒScore SDEsã€‚</li>
<li>å°†ç°æœ‰çš„æ‰©æ•£æ¨¡å‹å¼‚å¸¸æ£€æµ‹æ–¹æ³•åˆ†ä¸ºé‡å»ºå‹ã€å¯†åº¦å‹å’Œæ··åˆå‹æ–¹æ³•ã€‚</li>
<li>ä»‹ç»äº†ä¸åŒæ•°æ®æ¨¡æ€çš„å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11430">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5eb0fba3ba2682755e88f6dc54de251e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad93bf5ca5dae02ada4447cf3e5a1039.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38fa74134c1cbc69610220c57a5573a8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e63f669e407ea30c4cea814c753db54c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe42fc0b2d14a07265587d253d62ba8e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Boosting-Diffusion-Guidance-via-Learning-Degradation-Aware-Models-for-Blind-Super-Resolution"><a href="#Boosting-Diffusion-Guidance-via-Learning-Degradation-Aware-Models-for-Blind-Super-Resolution" class="headerlink" title="Boosting Diffusion Guidance via Learning Degradation-Aware Models for   Blind Super Resolution"></a>Boosting Diffusion Guidance via Learning Degradation-Aware Models for   Blind Super Resolution</h2><p><strong>Authors:Shao-Hao Lu, Ren Wang, Ching-Chun Huang, Wei-Chen Chiu</strong></p>
<p>Recently, diffusion-based blind super-resolution (SR) methods have shown great ability to generate high-resolution images with abundant high-frequency detail, but the detail is often achieved at the expense of fidelity. Meanwhile, another line of research focusing on rectifying the reverse process of diffusion models (i.e., diffusion guidance), has demonstrated the power to generate high-fidelity results for non-blind SR. However, these methods rely on known degradation kernels, making them difficult to apply to blind SR. To address these issues, we present DADiff in this paper. DADiff incorporates degradation-aware models into the diffusion guidance framework, eliminating the need to know degradation kernels. Additionally, we propose two novel techniques â€“ input perturbation and guidance scalar â€“ to further improve our performance. Extensive experimental results show that our proposed method has superior performance over state-of-the-art methods on blind SR benchmarks. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºæ‰©æ•£çš„ç›²è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æ–¹æ³•æ˜¾ç¤ºå‡ºç”Ÿæˆå¯Œå«é«˜é¢‘ç»†èŠ‚çš„é«˜åˆ†è¾¨ç‡å›¾åƒçš„å¼ºå¤§èƒ½åŠ›ï¼Œä½†å¾€å¾€ä»¥ä¿çœŸåº¦çš„æŸå¤±ä¸ºä»£ä»·æ¥å®ç°è¿™äº›ç»†èŠ‚ã€‚ä¸æ­¤åŒæ—¶ï¼Œå¦ä¸€æ¡ç ”ç©¶èšç„¦äºçº æ­£æ‰©æ•£æ¨¡å‹çš„é€†å‘è¿‡ç¨‹ï¼ˆå³æ‰©æ•£å¼•å¯¼ï¼‰ï¼Œå·²æ˜¾ç¤ºå‡ºä¸ºéç›²SRç”Ÿæˆé«˜ä¿çœŸç»“æœçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºå·²çŸ¥çš„é€€åŒ–æ ¸ï¼Œä½¿å¾—å®ƒä»¬éš¾ä»¥åº”ç”¨äºç›²SRã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æå‡ºäº†DADiffã€‚DADiffå°†é€€åŒ–æ„ŸçŸ¥æ¨¡å‹çº³å…¥æ‰©æ•£å¼•å¯¼æ¡†æ¶ï¼Œæ— éœ€äº†è§£é€€åŒ–æ ¸ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–°æŠ€æœ¯â€”â€”è¾“å…¥æ‰°åŠ¨å’Œå¼•å¯¼æ ‡é‡â€”â€”æ¥è¿›ä¸€æ­¥æé«˜æˆ‘ä»¬çš„æ€§èƒ½ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬åœ¨ç›²SRåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¶…è¿‡äº†æœ€æ–°æŠ€æœ¯çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08819v2">PDF</a> To appear in WACV 2025. Code is available at:   <a target="_blank" rel="noopener" href="https://github.com/ryanlu2240/DADiff">https://github.com/ryanlu2240/DADiff</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç›²è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æ–¹æ³•â€”â€”DADiffã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ„ŸçŸ¥é€€åŒ–æ¨¡å‹ä¸æ‰©æ•£å¼•å¯¼æ¡†æ¶ï¼Œæ— éœ€çŸ¥é“é€€åŒ–æ ¸ä¿¡æ¯ã€‚åŒæ—¶ï¼Œé€šè¿‡å¼•å…¥è¾“å…¥æ‰°åŠ¨å’Œå¼•å¯¼æ ‡é‡ä¸¤é¡¹æ–°æŠ€æœ¯ï¼Œè¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ç›²SRåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•æ€§èƒ½å“è¶Šï¼Œè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DADiffç»“åˆæ„ŸçŸ¥é€€åŒ–æ¨¡å‹ä¸æ‰©æ•£å¼•å¯¼æ¡†æ¶ï¼Œè§£å†³æ‰©æ•£æ¨¡å‹åœ¨ç›²è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰ä¸­çš„æ€§èƒ½é—®é¢˜ã€‚</li>
<li>æ— éœ€çŸ¥é“é€€åŒ–æ ¸ä¿¡æ¯ï¼Œæ‰©å¤§äº†åº”ç”¨èŒƒå›´ã€‚</li>
<li>å¼•å…¥è¾“å…¥æ‰°åŠ¨æŠ€æœ¯ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨å¼•å¯¼æ ‡é‡æŠ€æœ¯ï¼Œè¿›ä¸€æ­¥æé«˜æ•ˆæœã€‚</li>
<li>åœ¨ç›²SRåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šã€‚</li>
<li>å¯¹æ¯”ç°æœ‰æŠ€æœ¯ï¼ŒDADiffå…·æœ‰æ›´é«˜çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5fdb8c3bc905a084534dad8213a45fab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5be19807ba797a94e8af474d841ff0ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c0cb7cf103785dca203afc355551137.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfcb5c0167308092bf8db4f5d2801556.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92344895f872238ff4b3e707b02bec12.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Deep-Geometric-Moments-Promote-Shape-Consistency-in-Text-to-3D-Generation"><a href="#Deep-Geometric-Moments-Promote-Shape-Consistency-in-Text-to-3D-Generation" class="headerlink" title="Deep Geometric Moments Promote Shape Consistency in Text-to-3D   Generation"></a>Deep Geometric Moments Promote Shape Consistency in Text-to-3D   Generation</h2><p><strong>Authors:Utkarsh Nath, Rajeev Goel, Eun Som Jeon, Changhoon Kim, Kyle Min, Yezhou Yang, Yingzhen Yang, Pavan Turaga</strong></p>
<p>To address the data scarcity associated with 3D assets, 2D-lifting techniques such as Score Distillation Sampling (SDS) have become a widely adopted practice in text-to-3D generation pipelines. However, the diffusion models used in these techniques are prone to viewpoint bias and thus lead to geometric inconsistencies such as the Janus problem. To counter this, we introduce MT3D, a text-to-3D generative model that leverages a high-fidelity 3D object to overcome viewpoint bias and explicitly infuse geometric understanding into the generation pipeline. Firstly, we employ depth maps derived from a high-quality 3D model as control signals to guarantee that the generated 2D images preserve the fundamental shape and structure, thereby reducing the inherent viewpoint bias. Next, we utilize deep geometric moments to ensure geometric consistency in the 3D representation explicitly. By incorporating geometric details from a 3D asset, MT3D enables the creation of diverse and geometrically consistent objects, thereby improving the quality and usability of our 3D representations. Project page and code: <a target="_blank" rel="noopener" href="https://moment-3d.github.io/">https://moment-3d.github.io/</a> </p>
<blockquote>
<p>é’ˆå¯¹ä¸3Dèµ„äº§ç›¸å…³çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå¦‚åˆ†æ•°è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰çš„2Dæå‡æŠ€æœ¯å·²æˆä¸ºæ–‡æœ¬åˆ°3Dç”Ÿæˆç®¡é“ä¸­å¹¿æ³›é‡‡ç”¨çš„å®è·µã€‚ç„¶è€Œï¼Œè¿™äº›æŠ€æœ¯ä¸­ä½¿ç”¨çš„æ‰©æ•£æ¨¡å‹å®¹æ˜“å‡ºç°è§†ç‚¹åå·®ï¼Œä»è€Œå¯¼è‡´å‡ ä½•ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œä¾‹å¦‚ Janus é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MT3Dï¼Œè¿™æ˜¯ä¸€ä¸ªæ–‡æœ¬åˆ°3Dçš„ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒåˆ©ç”¨é«˜ä¿çœŸ3Då¯¹è±¡æ¥å…‹æœè§†ç‚¹åå·®ï¼Œå¹¶å°†å‡ ä½•ç†è§£æ˜ç¡®èå…¥ç”Ÿæˆç®¡é“ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é‡‡ç”¨ä»é«˜è´¨é‡3Dæ¨¡å‹æ´¾ç”Ÿçš„æ·±åº¦å›¾ä½œä¸ºæ§åˆ¶ä¿¡å·ï¼Œä»¥ä¿è¯ç”Ÿæˆçš„2Då›¾åƒä¿æŒåŸºæœ¬å½¢çŠ¶å’Œç»“æ„ï¼Œä»è€Œå‡å°‘å›ºæœ‰çš„è§†ç‚¹åå·®ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ©ç”¨æ·±å±‚å‡ ä½•çŸ©æ¥ç¡®ä¿3Dè¡¨ç¤ºä¸­çš„å‡ ä½•ä¸€è‡´æ€§ã€‚é€šè¿‡èå…¥3Dèµ„äº§çš„å‡ ä½•ç»†èŠ‚ï¼ŒMT3Dèƒ½å¤Ÿåˆ›å»ºå¤šæ ·ä¸”å‡ ä½•ä¸€è‡´çš„ç‰©ä½“ï¼Œä»è€Œæé«˜æˆ‘ä»¬3Dè¡¨ç¤ºçš„è´¨é‡å’Œå¯ç”¨æ€§ã€‚é¡¹ç›®é¡µé¢å’Œä»£ç ï¼š<a target="_blank" rel="noopener" href="https://moment-3d.github.io/">https://moment-3d.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.05938v2">PDF</a> This paper has been accepted to WACV 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†ä¸ºè§£å†³æ–‡æœ¬åˆ°ä¸‰ç»´ç”Ÿæˆä¸­çš„è§†è§’åå·®é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºMT3Dçš„æ–‡æœ¬åˆ°ä¸‰ç»´ç”Ÿæˆæ¨¡å‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨é«˜è´¨é‡çš„ä¸‰ç»´æ¨¡å‹æ·±åº¦å›¾ä½œä¸ºæ§åˆ¶ä¿¡å·ï¼Œç¡®ä¿äº†ç”Ÿæˆçš„äºŒç»´å›¾åƒä¿æŒäº†åŸºæœ¬çš„å½¢çŠ¶å’Œç»“æ„ï¼Œä»è€Œå‡å°‘äº†å›ºæœ‰çš„è§†è§’åå·®ã€‚åŒæ—¶ï¼Œæ¨¡å‹åˆ©ç”¨æ·±åº¦å‡ ä½•æ—¶åˆ»ç¡®ä¿ä¸‰ç»´è¡¨ç¤ºä¸­çš„å‡ ä½•ä¸€è‡´æ€§ã€‚é€šè¿‡ä»ä¸‰ç»´èµ„äº§ä¸­èå…¥å‡ ä½•ç»†èŠ‚ï¼ŒMT3Dèƒ½å¤Ÿåˆ›å»ºå¤šæ ·ä¸”å‡ ä½•ä¸€è‡´çš„ç‰©ä½“ï¼Œä»è€Œæé«˜ä¸‰ç»´è¡¨ç¤ºçš„è´¨é‡å’Œå¯ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®ç¨€ç¼ºæ˜¯æ–‡æœ¬åˆ°ä¸‰ç»´èµ„äº§ç”Ÿæˆçš„ä¸€å¤§æŒ‘æˆ˜ã€‚å› æ­¤ï¼ŒäºŒç»´æå‡æŠ€æœ¯ï¼ˆå¦‚åˆ†æ•°è’¸é¦é‡‡æ ·ï¼‰å¹¿æ³›åº”ç”¨äºæ–‡æœ¬åˆ°ä¸‰ç»´ç”Ÿæˆçš„æµç¨‹ä¸­ã€‚ä½†æ­¤æ–¹æ³•æ¶‰åŠçš„æ•°æ®è§†è§’åå·®å¯èƒ½å¯¼è‡´å‡ ä½•ä¸ä¸€è‡´é—®é¢˜ã€‚</li>
<li>æ–°æ¨¡å‹MT3Dé‡‡ç”¨é«˜è´¨é‡çš„ä¸‰ç»´æ¨¡å‹æ·±åº¦å›¾ä½œä¸ºæ§åˆ¶ä¿¡å·æ¥ç”ŸæˆäºŒç»´å›¾åƒï¼Œä»¥å‡å°‘è§†è§’åå·®å¹¶ä¿æŒç‰©ä½“çš„åŸºæœ¬å½¢çŠ¶å’Œç»“æ„ã€‚è¿™ä¸€åˆ›æ–°ç­–ç•¥æé«˜äº†ç”Ÿæˆçš„å›¾åƒè´¨é‡ã€‚</li>
<li>MT3Dåˆ©ç”¨æ·±åº¦å‡ ä½•æ—¶åˆ»ç¡®ä¿ä¸‰ç»´è¡¨ç¤ºçš„å‡ ä½•ä¸€è‡´æ€§ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚è¿™ä¸€ç‰¹ç‚¹ä½¿å¾—ç”Ÿæˆçš„ç‰©ä½“æ›´åŠ çœŸå®å’Œå¤šæ ·ã€‚</li>
<li>é€šè¿‡èå…¥ä¸‰ç»´èµ„äº§çš„å‡ ä½•ç»†èŠ‚ï¼ŒMT3Då¯ä»¥åˆ›å»ºæ—¢å¤šæ ·åˆå‡ ä½•ä¸€è‡´çš„ç‰©ä½“ï¼Œå¤§å¤§æé«˜äº†ä¸‰ç»´è¡¨ç¤ºçš„è´¨é‡å’Œå¯ç”¨æ€§ã€‚è¿™æ˜¯è¯¥æ¨¡å‹åœ¨æ–‡æœ¬åˆ°ä¸‰ç»´ç”Ÿæˆé¢†åŸŸçš„é‡å¤§çªç ´ã€‚</li>
<li>æ–°æ¨¡å‹å…‹æœäº†ä¼ ç»Ÿçš„å‡ ä½•ä¸ä¸€è‡´é—®é¢˜ï¼ˆå¦‚Janusé—®é¢˜ï¼‰ï¼Œä¸ºåç»­çš„ä¸‰ç»´ç”Ÿæˆç ”ç©¶æä¾›äº†å®è´µçš„å‚è€ƒå’Œå¯ç¤ºã€‚é¡¹ç›®é¡µé¢å’Œä»£ç å¯ä¾›æŸ¥é˜…ï¼Œä¾¿äºè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
<li>MT3Dæ¨¡å‹å±•ç¤ºäº†å…¶åœ¨æ–‡æœ¬åˆ°ä¸‰ç»´ç”Ÿæˆé¢†åŸŸçš„æ½œåŠ›ï¼Œä¸ºæœªæ¥çš„åº”ç”¨åœºæ™¯æä¾›äº†å¹¿é˜”çš„å¯èƒ½æ€§ã€‚éšç€æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼Œè¯¥æ¨¡å‹æœ‰æœ›åœ¨å„ç§é¢†åŸŸä¸­å‘æŒ¥æ›´å¤§çš„ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.05938">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca7418e970c556779eb187a88b326f70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d17b9178e018c43fea51926710e3f06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-585e0859b3e63ee9a3c8a2c6a42f0e73.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="VisMin-Visual-Minimal-Change-Understanding"><a href="#VisMin-Visual-Minimal-Change-Understanding" class="headerlink" title="VisMin: Visual Minimal-Change Understanding"></a>VisMin: Visual Minimal-Change Understanding</h2><p><strong>Authors:Rabiul Awal, Saba Ahmadi, Le Zhang, Aishwarya Agrawal</strong></p>
<p>Fine-grained understanding of objects, attributes, and relationships between objects is crucial for visual-language models (VLMs). Existing benchmarks primarily focus on evaluating VLMsâ€™ capability to distinguish between two very similar captions given an image. In this paper, we introduce a new, challenging benchmark termed Visual Minimal-Change Understanding (VisMin), which requires models to predict the correct image-caption match given two images and two captions. The image pair and caption pair contain minimal changes, i.e., only one aspect changes at a time from among the following: object, attribute, count, and spatial relation. These changes test the modelsâ€™ understanding of objects, attributes (such as color, material, shape), counts, and spatial relationships between objects. We built an automatic framework using large language models and diffusion models, followed by a rigorous 4-step verification process by human annotators. Empirical experiments reveal that current VLMs exhibit notable deficiencies in understanding spatial relationships and counting abilities. We also generate a large-scale training dataset to finetune CLIP and Idefics2, showing significant improvements in fine-grained understanding across benchmarks and in CLIPâ€™s general image-text alignment. We release all resources, including the benchmark, training data, and finetuned model checkpoints, at <a target="_blank" rel="noopener" href="https://vismin.net/">https://vismin.net/</a>. </p>
<blockquote>
<p>å¯¹äºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è€Œè¨€ï¼Œå¯¹ç‰©ä½“ã€å±æ€§ä»¥åŠç‰©ä½“é—´å…³ç³»çš„ç²¾ç»†ç†è§£æ˜¯è‡³å…³é‡è¦çš„ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦ä¾§é‡äºè¯„ä¼°VLMsåœ¨ç»™å®šå›¾åƒçš„æƒ…å†µä¸‹åŒºåˆ†ä¸¤ä¸ªéå¸¸ç›¸ä¼¼çš„æ ‡é¢˜çš„èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œç§°ä¸ºè§†è§‰æœ€å°å˜åŒ–ç†è§£ï¼ˆVisMinï¼‰ï¼Œè¯¥æµ‹è¯•è¦æ±‚æ¨¡å‹åœ¨ç»™å®šçš„ä¸¤ä¸ªå›¾åƒå’Œä¸¤ä¸ªæ ‡é¢˜ä¸­é¢„æµ‹æ­£ç¡®çš„å›¾åƒ-æ ‡é¢˜åŒ¹é…ã€‚å›¾åƒå¯¹å’Œæ ‡é¢˜å¯¹ä¸­åŒ…å«äº†æœ€å°çš„å˜åŒ–ï¼Œå³æ¯æ¬¡åªæœ‰ä¸€ä¸ªæ–¹é¢çš„å˜åŒ–ï¼ŒåŒ…æ‹¬ï¼šç‰©ä½“ã€å±æ€§ã€æ•°é‡å’Œç©ºé—´å…³ç³»ã€‚è¿™äº›å˜åŒ–æµ‹è¯•äº†æ¨¡å‹å¯¹ç‰©ä½“ã€å±æ€§ï¼ˆå¦‚é¢œè‰²ã€æè´¨ã€å½¢çŠ¶ï¼‰ã€æ•°é‡å’Œç‰©ä½“ä¹‹é—´ç©ºé—´å…³ç³»çš„ç†è§£ã€‚æˆ‘ä»¬ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹å»ºç«‹äº†ä¸€ä¸ªè‡ªåŠ¨æ¡†æ¶ï¼Œéšåç»è¿‡äººå·¥æ³¨é‡Šè€…è¿›è¡Œçš„ä¸¥æ ¼å››æ­¥éªŒè¯è¿‡ç¨‹ã€‚ç»éªŒå®éªŒè¡¨æ˜ï¼Œç›®å‰çš„VLMåœ¨ç†è§£ç©ºé—´å…³ç³»å’Œè®¡æ•°èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜æ˜¾çš„ç¼ºé™·ã€‚æˆ‘ä»¬è¿˜ç”Ÿæˆäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®é›†æ¥å¾®è°ƒCLIPå’ŒIdefics2ï¼Œåœ¨åŸºå‡†æµ‹è¯•å’ŒCLIPçš„é€šç”¨å›¾åƒæ–‡æœ¬å¯¹é½æ–¹é¢æ˜¾ç¤ºå‡ºå¯¹ç²¾ç»†ç†è§£çš„æ˜¾è‘—æ”¹å–„ã€‚æˆ‘ä»¬å‘å¸ƒçš„æ‰€æœ‰èµ„æºï¼ŒåŒ…æ‹¬åŸºå‡†æµ‹è¯•ã€è®­ç»ƒæ•°æ®å’Œå¾®è°ƒåçš„æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œéƒ½å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://vismin.net/%E6%89%BE%E5%88%B0%E3%80%82">https://vismin.net/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.16772v2">PDF</a> Accepted at NeurIPS 2024. Project URL at <a target="_blank" rel="noopener" href="https://vismin.net/">https://vismin.net/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹è¯„ä¼°åŸºå‡†â€”â€”Visual Minimal-Change Understandingï¼ˆVisMinï¼‰ã€‚è¯¥åŸºå‡†è¦æ±‚æ¨¡å‹é¢„æµ‹ç»™å®šä¸¤ä¸ªå›¾åƒå’Œä¸¤ä¸ªä»…åœ¨ä¸€ä¸ªæ–¹é¢å‘ç”Ÿå˜åŒ–çš„æè¿°æ—¶ï¼Œæ­£ç¡®åŒ¹é…å›¾åƒå’Œæè¿°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒVisMinè¯„ä¼°æ¨¡å‹å¯¹å¯¹è±¡ã€å±æ€§ï¼ˆå¦‚é¢œè‰²ã€æè´¨ã€å½¢çŠ¶ï¼‰ã€æ•°é‡å’Œå¯¹è±¡é—´ç©ºé—´å…³ç³»çš„ç†è§£ã€‚é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹æ„å»ºè‡ªåŠ¨æ¡†æ¶ï¼Œå¹¶é€šè¿‡äººç±»æ³¨é‡Šè€…è¿›è¡Œä¸¥æ ¼çš„å››æ­¥éªŒè¯è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œå½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç†è§£å’Œè®¡æ•°èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜æ˜¾ç¼ºé™·ã€‚é€šè¿‡å¾®è°ƒCLIPå’ŒIdefics2çš„è®­ç»ƒæ•°æ®é›†ï¼Œåœ¨åŸºå‡†æµ‹è¯•ä¸­æ˜¾ç¤ºå‡ºç²¾ç»†ç†è§£çš„æ˜¾è‘—æ”¹å–„ã€‚æ‰€æœ‰èµ„æºï¼ŒåŒ…æ‹¬åŸºå‡†æµ‹è¯•ã€è®­ç»ƒæ•°æ®å’Œå¾®è°ƒæ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œå‡å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://vismin.net/%E4%B8%8A%E3%80%82">https://vismin.net/ä¸Šã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰éœ€è¦å¯¹å¯¹è±¡ã€å±æ€§å’Œå¯¹è±¡é—´å…³ç³»çš„ç²¾ç»†ç†è§£ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°åŸºå‡†â€”â€”Visual Minimal-Change Understandingï¼ˆVisMinï¼‰ï¼Œä»¥è¯„ä¼°æ¨¡å‹å¯¹å¾®å°å˜åŒ–çš„æ•æ„Ÿåº¦ã€‚</li>
<li>VisMinè¦æ±‚æ¨¡å‹é¢„æµ‹ç»™å®šä¸¤ä¸ªå›¾åƒå’Œä¸¤ä¸ªæè¿°ä¸­åªæœ‰ä¸€ä¸ªæ–¹é¢å‘ç”Ÿå˜åŒ–æ—¶çš„æ­£ç¡®åŒ¹é…ã€‚</li>
<li>VisMinæµ‹è¯•æ¨¡å‹åœ¨å¯¹è±¡ã€å±æ€§ï¼ˆå¦‚é¢œè‰²ã€æè´¨ã€å½¢çŠ¶ï¼‰ã€æ•°é‡å’Œç©ºé—´å…³ç³»æ–¹é¢çš„ç†è§£ã€‚</li>
<li>é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹æ„å»ºè‡ªåŠ¨è¯„ä¼°æ¡†æ¶ï¼Œå¹¶é€šè¿‡äººå·¥éªŒè¯ç¡®ä¿å‡†ç¡®æ€§ã€‚</li>
<li>ç°æœ‰VLMåœ¨ç†è§£å’Œè®¡æ•°èƒ½åŠ›æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.16772">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7dc8c11625c44d547a1ce363c7ae07c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1f8c65f04c4d5a141d47189b9b82f75d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-804ca11a5a797be77734a321746015ba.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MD-Dose-A-diffusion-model-based-on-the-Mamba-for-radiation-dose-prediction"><a href="#MD-Dose-A-diffusion-model-based-on-the-Mamba-for-radiation-dose-prediction" class="headerlink" title="MD-Dose: A diffusion model based on the Mamba for radiation dose   prediction"></a>MD-Dose: A diffusion model based on the Mamba for radiation dose   prediction</h2><p><strong>Authors:Linjie Fu, Xia Li, Xiuding Cai, Yingkai Wang, Xueyao Wang, Yali Shen, Yu Yao</strong></p>
<p>Radiation therapy is crucial in cancer treatment. Experienced experts typically iteratively generate high-quality dose distribution maps, forming the basis for excellent radiation therapy plans. Therefore, automated prediction of dose distribution maps is significant in expediting the treatment process and providing a better starting point for developing radiation therapy plans. With the remarkable results of diffusion models in predicting high-frequency regions of dose distribution maps, dose prediction methods based on diffusion models have been extensively studied. However, existing methods mainly utilize CNNs or Transformers as denoising networks. CNNs lack the capture of global receptive fields, resulting in suboptimal prediction performance. Transformers excel in global modeling but face quadratic complexity with image size, resulting in significant computational overhead. To tackle these challenges, we introduce a novel diffusion model, MD-Dose, based on the Mamba architecture for predicting radiation therapy dose distribution in thoracic cancer patients. In the forward process, MD-Dose adds Gaussian noise to dose distribution maps to obtain pure noise images. In the backward process, MD-Dose utilizes a noise predictor based on the Mamba to predict the noise, ultimately outputting the dose distribution maps. Furthermore, We develop a Mamba encoder to extract structural information and integrate it into the noise predictor for localizing dose regions in the planning target volume (PTV) and organs at risk (OARs). Through extensive experiments on a dataset of 300 thoracic tumor patients, we showcase the superiority of MD-Dose in various metrics and time consumption. </p>
<blockquote>
<p>æ”¾å°„æ²»ç–—åœ¨ç™Œç—‡æ²»ç–—ä¸­å æœ‰è‡³å…³é‡è¦çš„åœ°ä½ã€‚ç»éªŒä¸°å¯Œçš„ä¸“å®¶é€šå¸¸ä¼šé€šè¿‡è¿­ä»£ç”Ÿæˆé«˜è´¨é‡çš„å‰‚é‡åˆ†å¸ƒå›¾ï¼Œä¸ºä¼˜ç§€çš„æ”¾å°„æ²»ç–—è®¡åˆ’å¥ å®šåŸºç¡€ã€‚å› æ­¤ï¼Œå‰‚é‡åˆ†å¸ƒå›¾çš„è‡ªåŠ¨é¢„æµ‹åœ¨åŠ é€Ÿæ²»ç–—è¿‡ç¨‹ä»¥åŠä¸ºåˆ¶å®šæ”¾å°„æ²»ç–—è®¡åˆ’æä¾›æ›´å¥½çš„èµ·ç‚¹æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰ã€‚æ‰©æ•£æ¨¡å‹åœ¨é¢„æµ‹å‰‚é‡åˆ†å¸ƒå›¾çš„é«˜é¢‘åŒºåŸŸæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœï¼Œå› æ­¤åŸºäºæ‰©æ•£æ¨¡å‹çš„å‰‚é‡é¢„æµ‹æ–¹æ³•å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦åˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æˆ–Transformerä½œä¸ºå»å™ªç½‘ç»œã€‚CNNç¼ºä¹å…¨å±€æ„Ÿå—é‡çš„æ•æ‰ï¼Œå¯¼è‡´é¢„æµ‹æ€§èƒ½ä¸ä½³ã€‚è™½ç„¶Transformeræ“…é•¿å…¨å±€å»ºæ¨¡ï¼Œä½†éšç€å›¾åƒå¤§å°çš„å¢åŠ ï¼Œå…¶é¢ä¸´äºŒæ¬¡å¤æ‚æ€§ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€è¾ƒå¤§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºMambaæ¶æ„çš„æ–°å‹æ‰©æ•£æ¨¡å‹MD-Doseï¼Œç”¨äºé¢„æµ‹èƒ¸éƒ¨ç™Œç—‡æ‚£è€…çš„æ”¾å°„æ²»ç–—å‰‚é‡åˆ†å¸ƒã€‚åœ¨æ­£å‘è¿‡ç¨‹ä¸­ï¼ŒMD-Doseå‘å‰‚é‡åˆ†å¸ƒå›¾æ·»åŠ é«˜æ–¯å™ªå£°ä»¥è·å¾—çº¯å™ªå£°å›¾åƒã€‚åœ¨é€†å‘è¿‡ç¨‹ä¸­ï¼ŒMD-Doseåˆ©ç”¨åŸºäºMambaçš„å™ªå£°é¢„æµ‹å™¨æ¥é¢„æµ‹å™ªå£°ï¼Œå¹¶æœ€ç»ˆè¾“å‡ºå‰‚é‡åˆ†å¸ƒå›¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªMambaç¼–ç å™¨æ¥æå–ç»“æ„ä¿¡æ¯ï¼Œå¹¶å°†å…¶æ•´åˆåˆ°å™ªå£°é¢„æµ‹å™¨ä¸­ï¼Œä»¥å®šä½è®¡åˆ’é¶åŒºï¼ˆPTVï¼‰å’Œé£é™©å™¨å®˜ï¼ˆOARsï¼‰ä¸­çš„å‰‚é‡åŒºåŸŸã€‚é€šè¿‡å¯¹300ä¾‹èƒ¸éƒ¨è‚¿ç˜¤æ‚£è€…æ•°æ®é›†è¿›è¡Œçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†MD-Doseåœ¨å„ç§æŒ‡æ ‡å’Œæ—¶é—´æ¶ˆè€—æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.08479v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè¾å°„æ²»ç–—åœ¨ç™Œç—‡æ²»ç–—ä¸­è‡³å…³é‡è¦ã€‚ä¸“å®¶ä»¬é€šè¿‡è¿­ä»£ç”Ÿæˆé«˜è´¨é‡å‰‚é‡åˆ†å¸ƒå›¾ï¼Œå½¢æˆä¼˜ç§€çš„æ”¾å°„æ²»ç–—è®¡åˆ’çš„åŸºç¡€ã€‚å› æ­¤ï¼Œè‡ªåŠ¨é¢„æµ‹å‰‚é‡åˆ†å¸ƒå›¾å¯¹äºåŠ å¿«æ²»ç–—è¿‡ç¨‹ä»¥åŠä¸ºåˆ¶å®šæ”¾å°„æ²»ç–—è®¡åˆ’æä¾›æ›´å¥½çš„èµ·ç‚¹å…·æœ‰é‡è¦æ„ä¹‰ã€‚åŸºäºæ‰©æ•£æ¨¡å‹åœ¨é¢„æµ‹å‰‚é‡åˆ†å¸ƒå›¾é«˜é¢‘åŒºåŸŸæ–¹é¢çš„å‡ºè‰²è¡¨ç°ï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„å‰‚é‡é¢„æµ‹æ–¹æ³•å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ã€‚æœ¬ç ”ç©¶é’ˆå¯¹ç°æœ‰æ–¹æ³•å­˜åœ¨çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºMambaæ¶æ„çš„æ–°å‹æ‰©æ•£æ¨¡å‹MD-Doseï¼Œç”¨äºé¢„æµ‹èƒ¸éƒ¨ç™Œç—‡æ‚£è€…çš„æ”¾å°„æ²»ç–—å‰‚é‡åˆ†å¸ƒã€‚è¯¥æ¨¡å‹åœ¨æ­£å‘è¿‡ç¨‹ä¸­å‘å‰‚é‡åˆ†å¸ƒå›¾æ·»åŠ é«˜æ–¯å™ªå£°ä»¥è·å¾—çº¯å™ªå£°å›¾åƒï¼Œåœ¨é€†å‘è¿‡ç¨‹ä¸­åˆ©ç”¨åŸºäºMambaçš„å™ªå£°é¢„æµ‹å™¨è¿›è¡Œé¢„æµ‹ï¼Œæœ€ç»ˆè¾“å‡ºå‰‚é‡åˆ†å¸ƒå›¾ã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†Mambaç¼–ç å™¨ä»¥æå–ç»“æ„ä¿¡æ¯å¹¶å°†å…¶æ•´åˆåˆ°å™ªå£°é¢„æµ‹å™¨ä¸­ï¼Œä»¥å®šä½è®¡åˆ’é¶åŒºï¼ˆPTVï¼‰å’Œé£é™©å™¨å®˜ï¼ˆOARsï¼‰çš„å‰‚é‡åŒºåŸŸã€‚åœ¨300ä¾‹èƒ¸éƒ¨è‚¿ç˜¤æ‚£è€…æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMD-Doseåœ¨å„ç§æŒ‡æ ‡å’Œæ—¶é—´æ¶ˆè€—æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¾å°„æ²»ç–—åœ¨ç™Œç—‡æ²»ç–—ä¸­å æ®é‡è¦åœ°ä½ï¼Œä¸“å®¶æ‰‹å·¥ç”Ÿæˆçš„å‰‚é‡åˆ†å¸ƒå›¾æ˜¯é«˜è´¨é‡æ²»ç–—è®¡åˆ’çš„å…³é”®ã€‚</li>
<li>è‡ªåŠ¨åŒ–é¢„æµ‹å‰‚é‡åˆ†å¸ƒå›¾èƒ½åŠ å¿«æ²»ç–—è¿‡ç¨‹ï¼Œä¸ºåˆ¶å®šæ”¾å°„æ²»ç–—è®¡åˆ’æä¾›æ›´å¥½çš„èµ·ç‚¹ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨é¢„æµ‹å‰‚é‡åˆ†å¸ƒå›¾é«˜é¢‘åŒºåŸŸæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå·²è¢«å¹¿æ³›ç ”ç©¶ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä½¿ç”¨CNNæˆ–Transformerä½œä¸ºå»å™ªç½‘ç»œï¼Œä½†å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>MD-Doseæ˜¯ä¸€ç§æ–°å‹æ‰©æ•£æ¨¡å‹ï¼ŒåŸºäºMambaæ¶æ„ï¼Œç”¨äºé¢„æµ‹èƒ¸éƒ¨ç™Œç—‡æ‚£è€…çš„è¾å°„æ²»ç–—å‰‚é‡åˆ†å¸ƒã€‚</li>
<li>MD-Doseé€šè¿‡æ­£å‘å’Œé€†å‘è¿‡ç¨‹é¢„æµ‹å‰‚é‡åˆ†å¸ƒå›¾ï¼Œå¹¶é€šè¿‡Mambaç¼–ç å™¨æå–ç»“æ„ä¿¡æ¯ä»¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.08479">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d4c2d24207e773ddf35dbb0cebe8421.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b29cfef6d11a0d4062826a95f1ae705.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6b7f838ad88733ae54baaa60aca15ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a424dbbfe780c0e2bc8ca0f36545c54.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5944f29fbb656dc9386fa4ef1dede3f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4bbddb0720543f26df1ecfcfd580455.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-24/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-24/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-24/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2402.01034v3/page_3_0.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-24  Learning accurate rigid registration for longitudinal brain MRI from   synthetic data
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-24/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-dc0a07268068ce868603d3c91304793b.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-24  DWTNeRF Boosting Few-shot Neural Radiance Fields via Discrete Wavelet   Transform
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27663.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
