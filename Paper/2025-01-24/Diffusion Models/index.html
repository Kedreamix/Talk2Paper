<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-01-24  Accelerate High-Quality Diffusion Models with Inner Loop Feedback">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-a42362520175822e3979e92166715baa.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    54 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-24-更新"><a href="#2025-01-24-更新" class="headerlink" title="2025-01-24 更新"></a>2025-01-24 更新</h1><h2 id="Accelerate-High-Quality-Diffusion-Models-with-Inner-Loop-Feedback"><a href="#Accelerate-High-Quality-Diffusion-Models-with-Inner-Loop-Feedback" class="headerlink" title="Accelerate High-Quality Diffusion Models with Inner Loop Feedback"></a>Accelerate High-Quality Diffusion Models with Inner Loop Feedback</h2><p><strong>Authors:Matthew Gwilliam, Han Cai, Di Wu, Abhinav Shrivastava, Zhiyu Cheng</strong></p>
<p>We propose Inner Loop Feedback (ILF), a novel approach to accelerate diffusion models’ inference. ILF trains a lightweight module to predict future features in the denoising process by leveraging the outputs from a chosen diffusion backbone block at a given time step. This approach exploits two key intuitions; (1) the outputs of a given block at adjacent time steps are similar, and (2) performing partial computations for a step imposes a lower burden on the model than skipping the step entirely. Our method is highly flexible, since we find that the feedback module itself can simply be a block from the diffusion backbone, with all settings copied. Its influence on the diffusion forward can be tempered with a learnable scaling factor from zero initialization. We train this module using distillation losses; however, unlike some prior work where a full diffusion backbone serves as the student, our model freezes the backbone, training only the feedback module. While many efforts to optimize diffusion models focus on achieving acceptable image quality in extremely few steps (1-4 steps), our emphasis is on matching best case results (typically achieved in 20 steps) while significantly reducing runtime. ILF achieves this balance effectively, demonstrating strong performance for both class-to-image generation with diffusion transformer (DiT) and text-to-image generation with DiT-based PixArt-alpha and PixArt-sigma. The quality of ILF’s 1.7x-1.8x speedups are confirmed by FID, CLIP score, CLIP Image Quality Assessment, ImageReward, and qualitative comparisons. </p>
<blockquote>
<p>我们提出了内部循环反馈（ILF），这是一种加速扩散模型推断的新型方法。ILF训练了一个轻量级模块，通过利用在给定时间步长选择的扩散主干块输出，来预测去噪过程中未来的特征。这种方法利用了两个关键直觉：（1）给定块在相邻时间步的输出是相似的，以及（2）对一步进行部分计算比完全跳过这一步给模型带来的负担更低。我们的方法具有很高的灵活性，因为我们发现反馈模块本身可以仅仅是扩散主干的一个块，所有设置都被复制。它对扩散正向的影响可以通过从零初始化开始的一个可学习的缩放因子来调节。我们使用蒸馏损失来训练这个模块；然而，与一些先前的工作不同，其中整个扩散主干作为学生，我们的模型会冻结主干，只训练反馈模块。虽然许多优化扩散模型的努力都集中在在极少的步骤（1-4步）内达到可接受的图像质量，但我们的重点是在达到最佳结果（通常在20步内实现）的同时显著减少运行时间。ILF有效地实现了这种平衡，在利用扩散变压器（DiT）进行类到图像生成和基于DiT的PixArt-alpha和PixArt-sigma进行文本到图像生成时，均表现出强劲的性能。ILF的1.7倍至1.8倍加速质量得到了FID、CLIP分数、CLIP图像质量评估、ImageReward和定性比较的确证。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13107v1">PDF</a> submission currently under review; 20 pages, 17 figures, 6 tables</p>
<p><strong>Summary</strong><br>     我们提出了内部循环反馈（ILF）这一新方法，旨在加速扩散模型的推理过程。ILF训练了一个轻量级模块，通过利用给定时间步长下选择的扩散主干块的输出来预测去噪过程中的未来特征。该方法基于两个关键直觉：（1）相邻时间步长下给定块的输出是相似的；（2）执行部分计算步骤比完全跳过步骤对模型的负担更小。ILF方法灵活度高，反馈模块本身可以是扩散主干的一个块，所有设置均被复制。它对扩散正向的影响可以通过从零初始化中学习到的缩放因子进行调节。我们仅训练该模块使用蒸馏损失，但与某些先前的工作不同，我们的模型冻结了主干，只训练反馈模块。虽然许多优化扩散模型的努力都集中在如何在极少的步骤（1-4步）内达到可接受的图像质量，但我们的重点是在匹配最佳结果（通常在20步内达到）的同时显著减少运行时间。ILF有效地实现了这一平衡，在基于扩散变压器的类到图像生成和基于PixArt-alpha和PixArt-sigma的文本到图像生成中均表现出强大的性能。ILF的速度提升1.7倍至1.8倍的质量得到了FID、CLIP分数、CLIP图像质量评估、ImageReward和定性比较的确证。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种名为内部循环反馈（ILF）的方法，旨在加速扩散模型的推理过程。</li>
<li>ILF利用给定时间步长下的扩散主干块输出来预测未来特征。</li>
<li>该方法基于两个关键直觉：相邻时间步长下的输出相似性，以及部分计算步骤的有效性。</li>
<li>ILF方法具有灵活性，其反馈模块可以是扩散主干的一个块。</li>
<li>仅训练反馈模块使用蒸馏损失，而主干保持冻结。</li>
<li>ILF在匹配最佳结果的同时显著减少运行时间，实现了在扩散模型优化中的有效平衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13107">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-075c82399f0d786e7e3b030873779e8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-775d25693d9e307fe26284a327bfaf77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d12d8886f4c13a4450794db4460e812.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9562c641b163666b302c482a15e1fdc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-020eb738a1f4fd451bab03088a90743e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Orchid-Image-Latent-Diffusion-for-Joint-Appearance-and-Geometry-Generation"><a href="#Orchid-Image-Latent-Diffusion-for-Joint-Appearance-and-Geometry-Generation" class="headerlink" title="Orchid: Image Latent Diffusion for Joint Appearance and Geometry   Generation"></a>Orchid: Image Latent Diffusion for Joint Appearance and Geometry   Generation</h2><p><strong>Authors:Akshay Krishnan, Xinchen Yan, Vincent Casser, Abhijit Kundu</strong></p>
<p>Diffusion models are state-of-the-art for image generation. Trained on large datasets, they capture expressive image priors that have been used for tasks like inpainting, depth, and (surface) normal prediction. However, these models are typically trained for one specific task, e.g., a separate model for each of color, depth, and normal prediction. Such models do not leverage the intrinsic correlation between appearance and geometry, often leading to inconsistent predictions.   In this paper, we propose using a novel image diffusion prior that jointly encodes appearance and geometry. We introduce a diffusion model Orchid, comprising a Variational Autoencoder (VAE) to encode color, depth, and surface normals to a latent space, and a Latent Diffusion Model (LDM) for generating these joint latents. Orchid directly generates photo-realistic color images, relative depth, and surface normals from user-provided text, and can be used to create image-aligned partial 3D scenes seamlessly. It can also perform image-conditioned tasks like joint monocular depth and normal prediction and is competitive in accuracy to state-of-the-art methods designed for those tasks alone. Lastly, our model learns a joint prior that can be used zero-shot as a regularizer for many inverse problems that entangle appearance and geometry. For example, we demonstrate its effectiveness in color-depth-normal inpainting, showcasing its applicability to problems in 3D generation from sparse views. </p>
<blockquote>
<p>扩散模型是图像生成领域的最前沿技术。它们经过大规模数据集训练，能够捕捉图像的表达先验，被用于图像修复、深度估计和（表面）法线预测等任务。然而，这些模型通常针对特定任务进行训练，例如针对颜色、深度和法线预测等任务分别使用不同的模型。这种模型没有利用外观和几何之间的内在关联，常常导致预测结果不一致。在本文中，我们提出了一种新的图像扩散先验，能够同时编码外观和几何信息。我们引入了一种扩散模型——Orchid，它包含一个变分自编码器（VAE），用于将颜色、深度和表面法线编码到潜在空间，以及一个潜在扩散模型（LDM）来生成这些联合潜在变量。Orchid可以根据用户提供的文本直接生成逼真的彩色图像、相对深度图和表面法线图，可用于创建无缝的图像对齐部分3D场景。它还可以执行图像条件任务，如联合单目深度估计和法线预测，并且在准确性方面与针对这些任务单独设计的最先进的方法相竞争。最后，我们的模型学习了一个联合先验，可以零样本作为许多纠缠外观和几何的逆问题的正则化器。例如，我们通过彩色深度法线图像修复任务展示了其应用于3D生成的稀疏视图问题的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13087v1">PDF</a> Project webpage: <a target="_blank" rel="noopener" href="https://orchid3d.github.io/">https://orchid3d.github.io</a></p>
<p><strong>Summary</strong></p>
<p>扩散模型是目前图像生成的最先进方法。通过大型数据集的训练，它们捕捉了生动的图像先验知识，可用于图像补全、深度预测和表面法线预测等任务。然而，这些模型通常针对单一任务进行训练，导致颜色、深度和法线预测等任务需要单独的模型处理。这些模型未能充分利用外观与几何之间的内在关联，导致预测结果的不一致性。本文提出了一种新型图像扩散先验，能联合编码外观和几何信息。提出的Orchid模型包括一个用于编码颜色、深度和表面法线的变分自编码器（VAE）和一个用于生成这些联合潜在特征的潜在扩散模型（LDM）。Orchid可根据用户提供的文本直接生成逼真的彩色图像、相对深度及表面法线，并可用于创建无缝的图像对齐的局部三维场景。此外，它还能执行图像条件下的联合单眼深度和法线预测任务，并能在准确性方面与针对这些任务单独设计的最新方法相竞争。最后，该模型学习了一个联合先验，可零样本用作许多纠缠外观和几何的反问题的正则化器。例如，我们在彩色深度法线补全任务中展示了其有效性，展示了其在从稀疏视图生成三维问题中的适用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型用于图像生成效果卓越，但面临任务特定训练的问题，缺乏跨任务一致性。</li>
<li>论文提出了Orchid模型，结合变分自编码器（VAE）和潜在扩散模型（LDM），联合编码颜色和几何信息。</li>
<li>Orchid能直接从文本生成逼真的彩色图像、相对深度及表面法线，支持创建无缝的图像对齐的局部三维场景。</li>
<li>该模型在图像条件下的联合单眼深度和法线预测任务中具有竞争力。</li>
<li>模型学习了一个联合先验，可作为多种反问题的正则化器，适用于彩色深度法线补全等任务。</li>
<li>Orchid模型展示了在从稀疏视图生成三维问题中的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13087">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1fcfbd625ad53f6a6d385b2e644fa812.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8126e44a7dfe6093dd2eb0ed6894adf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c34f30ef56fc3391e755438679e6235b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42a2d486ffbc4c4070326e8b6ad6221d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c976e8103f9bcffce9451ed3c2a7c639.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="3D-Object-Manipulation-in-a-Single-Image-using-Generative-Models"><a href="#3D-Object-Manipulation-in-a-Single-Image-using-Generative-Models" class="headerlink" title="3D Object Manipulation in a Single Image using Generative Models"></a>3D Object Manipulation in a Single Image using Generative Models</h2><p><strong>Authors:Ruisi Zhao, Zechuan Zhang, Zongxin Yang, Yi Yang</strong></p>
<p>Object manipulation in images aims to not only edit the object’s presentation but also gift objects with motion. Previous methods encountered challenges in concurrently handling static editing and dynamic generation, while also struggling to achieve fidelity in object appearance and scene lighting. In this work, we introduce \textbf{OMG3D}, a novel framework that integrates the precise geometric control with the generative power of diffusion models, thus achieving significant enhancements in visual performance. Our framework first converts 2D objects into 3D, enabling user-directed modifications and lifelike motions at the geometric level. To address texture realism, we propose CustomRefiner, a texture refinement module that pre-train a customized diffusion model, aligning the details and style of coarse renderings of 3D rough model with the original image, further refine the texture. Additionally, we introduce IllumiCombiner, a lighting processing module that estimates and corrects background lighting to match human visual perception, resulting in more realistic shadow effects. Extensive experiments demonstrate the outstanding visual performance of our approach in both static and dynamic scenarios. Remarkably, all these steps can be done using one NVIDIA 3090. Project page is at <a target="_blank" rel="noopener" href="https://whalesong-zrs.github.io/OMG3D-projectpage/">https://whalesong-zrs.github.io/OMG3D-projectpage/</a> </p>
<blockquote>
<p>图像中的物体操作旨在不仅编辑物体的呈现，还赋予物体动态。之前的方法在处理静态编辑和动态生成时遇到了挑战，同时在实现物体外观和场景光照的保真度方面也遇到了困难。在这项工作中，我们介绍了\textbf{OMG3D}，这是一个新型框架，它将精确的几何控制与扩散模型的生成能力相结合，从而在视觉性能上实现了重大改进。我们的框架首先将2D对象转换为3D，从而在几何级别实现用户导向的修改和逼真的动作。为了解决纹理逼真度的问题，我们提出了CustomRefiner，这是一种纹理优化模块，它预先训练了一个定制的扩散模型，将3D粗糙模型的粗糙渲染的细节和风格与原始图像对齐，进一步优化纹理。此外，我们还介绍了IllumiCombiner，这是一种光照处理模块，它估计并纠正背景光照，以匹配人类视觉感知，从而产生更逼真的阴影效果。大量实验证明了我们方法在静态和动态场景中的出色视觉性能。值得一提的是，所有这些步骤都可以使用一台NVIDIA 3090完成。项目页面是<a target="_blank" rel="noopener" href="https://whalesong-zrs.github.io/OMG3D-projectpage/%E3%80%82">https://whalesong-zrs.github.io/OMG3D-projectpage/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12935v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为OMG3D的新型框架，它将精确的几何控制与扩散模型的生成能力相结合，实现了物体操作图像中的静态编辑和动态生成的显著改善，并在视觉性能上取得了重大提升。该框架通过转换2D对象为3D对象，使用户可以在几何级别进行指导性修改，并赋予物体逼真的动态效果。为解决纹理真实性问题，提出了CustomRefiner纹理优化模块；为解决光照问题，引入了IllumiCombiner光照处理模块。实验证明，该方法在静态和动态场景中均表现出卓越视觉性能。</p>
<p><strong>Key Takeaways</strong></p>
<p>1.OMG3D框架结合了精确的几何控制与扩散模型的生成能力。<br>2.OMG3D能实现2D对象到3D对象的转换，允许用户在几何级别进行指导性修改，并赋予物体动态效果。<br>3.CustomRefiner模块用于优化纹理，提高渲染细节和风格与原始图像的一致性。<br>4.IllumiCombiner模块用于处理光照，估计和校正背景光照，以匹配人类视觉感知，实现更逼真的阴影效果。<br>5.该框架在静态和动态场景中都表现出了卓越的视觉性能。<br>6.所有步骤都在一个NVIDIA 3090上完成。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12935">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0c5b143908b93ff5c595305451d4e0f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59fce56b156d3416a0430688b577707a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-362db1b880ca7c523d5ae9d46099cc9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbb630d96e2fe53b8aa92159be4fa46f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2df41af5ef0492569dc84b98c4f41902.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b044d4bd0a9bfceedabe7d7523c055f0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AMM-Diff-Adaptive-Multi-Modality-Diffusion-Network-for-Missing-Modality-Imputation"><a href="#AMM-Diff-Adaptive-Multi-Modality-Diffusion-Network-for-Missing-Modality-Imputation" class="headerlink" title="AMM-Diff: Adaptive Multi-Modality Diffusion Network for Missing Modality   Imputation"></a>AMM-Diff: Adaptive Multi-Modality Diffusion Network for Missing Modality   Imputation</h2><p><strong>Authors:Aghiles Kebaili, Jérôme Lapuyade-Lahorgue, Pierre Vera, Su Ruan</strong></p>
<p>In clinical practice, full imaging is not always feasible, often due to complex acquisition protocols, stringent privacy regulations, or specific clinical needs. However, missing MR modalities pose significant challenges for tasks like brain tumor segmentation, especially in deep learning-based segmentation, as each modality provides complementary information crucial for improving accuracy. A promising solution is missing data imputation, where absent modalities are generated from available ones. While generative models have been widely used for this purpose, most state-of-the-art approaches are limited to single or dual target translations, lacking the adaptability to generate missing modalities based on varying input configurations. To address this, we propose an Adaptive Multi-Modality Diffusion Network (AMM-Diff), a novel diffusion-based generative model capable of handling any number of input modalities and generating the missing ones. We designed an Image-Frequency Fusion Network (IFFN) that learns a unified feature representation through a self-supervised pretext task across the full input modalities and their selected high-frequency Fourier components. The proposed diffusion model leverages this representation, encapsulating prior knowledge of the complete modalities, and combines it with an adaptive reconstruction strategy to achieve missing modality completion. Experimental results on the BraTS 2021 dataset demonstrate the effectiveness of our approach. </p>
<blockquote>
<p>在临床实践中，全面成像并不总是可行的，这往往是由于复杂的采集协议、严格的隐私规定或特定的临床需求。然而，缺失的MR模式对于如脑肿瘤分割等任务提出了重大挑战，特别是在基于深度学习的分割中，因为每种模式都提供了对提高准确性至关重要的补充信息。一种可行的解决方案是数据插值填补缺失部分，其中缺失的模式由存在的模式生成。虽然生成模型已被广泛用于此目的，但大多数最先进的方法仅限于单个或双目标翻译，缺乏根据不同输入配置生成缺失模式的适应性。为了解决这个问题，我们提出了一种自适应多模态扩散网络（AMM-Diff），这是一种基于扩散的新型生成模型，能够处理任何数量的输入模式并生成缺失的模式。我们设计了一个图像频率融合网络（IFFN），它通过全输入模式及其选定的高频傅里叶分量的自监督前期任务来学习统一的特征表示。所提出的扩散模型利用这种表示，封装了完整模式的先验知识，并结合自适应重建策略来实现缺失模式的完成。在BraTS 2021数据集上的实验结果证明了我们的方法的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12840v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要探讨了在临床实践中，由于复杂的采集协议、严格的隐私规定或特定的临床需求，全面成像并不总是可行的。缺失的MR模式态对脑肿瘤分割等任务提出了重大挑战，特别是在基于深度学习的方法中，因为每个模态都提供了改善准确性的重要补充信息。一种有前途的解决方案是缺失数据插补，其中缺失的模态是从现有的模态生成的。虽然生成模型已被广泛用于此目的，但大多数最先进的方法仅限于单一或双重目标翻译，缺乏适应不同输入配置生成缺失模态的能力。为了解决这个问题，本文提出了一种自适应多模态扩散网络（AMM-Diff），这是一种新型的基于扩散的生成模型，可以处理任何数量的输入模态并生成缺失的模态。设计了一种图像频率融合网络（IFFN），它通过全输入模态及其选定的高频傅立叶分量的自监督预文本任务学习统一特征表示。所提出的扩散模型利用这种表示，包含完整模态的先验知识，并结合自适应重建策略来实现缺失模态的完成。在BraTS 2021数据集上的实验结果证明了该方法的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>临床实践中全面成像不总是可行，缺失MR模态对任务如脑肿瘤分割带来挑战。</li>
<li>缺失数据插补是一种解决方案，其中缺失模态从现有模态生成。</li>
<li>现有生成模型方法受限于单一或双重目标翻译，缺乏适应不同输入配置的能力。</li>
<li>提出了一种新型自适应多模态扩散网络（AMM-Diff），能处理任意数量的输入模态并生成缺失模态。</li>
<li>设计了图像频率融合网络（IFFN）以学习统一特征表示，通过自监督预文本任务融合全输入模态及其高频傅立叶分量。</li>
<li>扩散模型利用包含完整模态先验知识的表示，结合自适应重建策略实现缺失模态完成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12840">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-72f00d7dd43f7f068af818cd5f288a35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af2008545ae2b2cfde5896bce7471bd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a25cc65ac3b96c9b4d14bed835eac2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9ac86da308927b41deff2423027d3ac.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="T2ISafety-Benchmark-for-Assessing-Fairness-Toxicity-and-Privacy-in-Image-Generation"><a href="#T2ISafety-Benchmark-for-Assessing-Fairness-Toxicity-and-Privacy-in-Image-Generation" class="headerlink" title="T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in   Image Generation"></a>T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in   Image Generation</h2><p><strong>Authors:Lijun Li, Zhelun Shi, Xuhao Hu, Bowen Dong, Yiran Qin, Xihui Liu, Lu Sheng, Jing Shao</strong></p>
<p>Text-to-image (T2I) models have rapidly advanced, enabling the generation of high-quality images from text prompts across various domains. However, these models present notable safety concerns, including the risk of generating harmful, biased, or private content. Current research on assessing T2I safety remains in its early stages. While some efforts have been made to evaluate models on specific safety dimensions, many critical risks remain unexplored. To address this gap, we introduce T2ISafety, a safety benchmark that evaluates T2I models across three key domains: toxicity, fairness, and bias. We build a detailed hierarchy of 12 tasks and 44 categories based on these three domains, and meticulously collect 70K corresponding prompts. Based on this taxonomy and prompt set, we build a large-scale T2I dataset with 68K manually annotated images and train an evaluator capable of detecting critical risks that previous work has failed to identify, including risks that even ultra-large proprietary models like GPTs cannot correctly detect. We evaluate 12 prominent diffusion models on T2ISafety and reveal several concerns including persistent issues with racial fairness, a tendency to generate toxic content, and significant variation in privacy protection across the models, even with defense methods like concept erasing. Data and evaluator are released under <a target="_blank" rel="noopener" href="https://github.com/adwardlee/t2i_safety">https://github.com/adwardlee/t2i_safety</a>. </p>
<blockquote>
<p>文本到图像（T2I）模型已得到迅速发展，能够在各个领域根据文本提示生成高质量图像。然而，这些模型引发了显著的安全问题，包括生成有害、偏见或私人内容的潜在风险。当前对于T2I安全性的评估研究仍处于初级阶段。尽管已经有一些努力在某些特定的安全维度上评估模型，但还有许多关键风险尚未探索。为了解决这一差距，我们推出了T2ISafety安全基准，用于在毒性、公平性和偏见这三个关键领域评估T2I模型。我们在这三个领域的基础上建立了包含三个层级任务（总计四级任务分类）的详细层次结构，并精心收集了相应的提示语共七万条。基于这种分类和提示集，我们建立了一个大规模的T2I数据集（共六十八万张人工注释的图像），并训练出一种能够检测出以前未识别出重大风险的评估器。甚至能检测到如GPT等大型专有模型未能察觉的风险。我们对十二个主流扩散模型进行了T2ISafety评估，并揭示了包括种族公平问题持续存在、倾向于生成有毒内容以及隐私保护在模型间存在显著差异等担忧，即使采用概念消除等防御方法也是如此。数据和评估器已在<a target="_blank" rel="noopener" href="https://github.com/adwardlee/t2i_safety%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/adwardlee/t2i_safety上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12612v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本转图像（T2I）模型在安全领域存在风险隐患，如可能生成有害、有偏见或涉及隐私的内容。针对这一问题，研究团队推出了T2ISafety安全基准评估系统，针对毒性、公平性和偏见三个关键领域进行细致评估。他们建立了包含12项任务和44个类别的详细层次结构，并收集了7万条相应提示。基于这一分类和提示集，他们建立了一个大规模的T2I数据集，包含6.8万张手动标注的图像，并训练了一个能够检测出先前工作未能识别的关键风险的评估器。他们对12个主流的扩散模型进行了T2ISafety评估，并揭示了包括种族公平问题、生成有毒内容倾向以及模型间隐私保护显著差异等隐患。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2I模型在安全领域存在风险，可能生成有害、有偏见或涉及隐私的内容。</li>
<li>研究团队推出了T2ISafety安全基准评估系统，针对毒性、公平性和偏见三大领域进行评估。</li>
<li>T2ISafety建立了包含12项任务和44个类别的详细层次结构，并收集了7万条提示用于评估。</li>
<li>研究团队建立了一个大规模的T2I数据集，包含6.8万张手动标注的图像。</li>
<li>训练了一个评估器，可检测出先前未识别的关键风险。</li>
<li>对12个主流的扩散模型进行了T2ISafety评估，发现存在种族公平问题、生成有毒内容倾向等隐患。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12612">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-8c33ccf944033a38dc9d8f73ac4f0835.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc52768e37b3a7491f639d5edd6e1a73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32ef989ed17f0a5f8a8fa04b7d0b768c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-089743b87379bcdb99f64fb2413e76dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc0895fdb436f2b410b074ab272ca95c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Image-Motion-Blur-Removal-in-the-Temporal-Dimension-with-Video-Diffusion-Models"><a href="#Image-Motion-Blur-Removal-in-the-Temporal-Dimension-with-Video-Diffusion-Models" class="headerlink" title="Image Motion Blur Removal in the Temporal Dimension with Video Diffusion   Models"></a>Image Motion Blur Removal in the Temporal Dimension with Video Diffusion   Models</h2><p><strong>Authors:Wang Pang, Zhihao Zhan, Xiang Zhu, Yechao Bai</strong></p>
<p>Most motion deblurring algorithms rely on spatial-domain convolution models, which struggle with the complex, non-linear blur arising from camera shake and object motion. In contrast, we propose a novel single-image deblurring approach that treats motion blur as a temporal averaging phenomenon. Our core innovation lies in leveraging a pre-trained video diffusion transformer model to capture diverse motion dynamics within a latent space. It sidesteps explicit kernel estimation and effectively accommodates diverse motion patterns. We implement the algorithm within a diffusion-based inverse problem framework. Empirical results on synthetic and real-world datasets demonstrate that our method outperforms existing techniques in deblurring complex motion blur scenarios. This work paves the way for utilizing powerful video diffusion models to address single-image deblurring challenges. </p>
<blockquote>
<p>大部分的运动去模糊算法都依赖于空间域卷积模型，这些模型在处理由相机抖动和物体运动引起的复杂、非线性模糊时显得捉襟见肘。与此相反，我们提出了一种新型的单图像去模糊方法，它将运动模糊视为一种时间平均现象。我们的核心创新在于利用预训练的视频扩散transformer模型，在潜在空间内捕捉各种运动动态。它避开了明确的核估计，有效地适应了各种运动模式。我们在基于扩散的逆向问题框架内实现了该算法。在合成和真实世界数据集上的经验结果表明，我们的方法在去除复杂运动模糊场景方面的表现优于现有技术。这项工作为利用强大的视频扩散模型解决单图像去模糊挑战铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12604v1">PDF</a> </p>
<p><strong>Summary</strong><br>     针对传统运动去模糊算法在应对由相机抖动和物体运动引起的复杂非线性模糊时的局限性，我们提出了一种新的单图像去模糊方法。该方法将运动模糊视为时间平均现象，并利用预训练的扩散模型视频扩散转换器模型在潜在空间内捕捉各种运动动态。该方法避免了明确的核估计，并有效地适应了各种运动模式。实验结果证明，在合成和真实数据集上，我们的方法在复杂的运动模糊场景的去模糊技术上优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前大多数运动去模糊算法主要依赖空间域卷积模型，难以处理复杂的非线性模糊。</li>
<li>我们提出了一种新的单图像去模糊方法，将运动模糊视为时间平均现象。</li>
<li>方法的核心创新在于利用预训练的扩散模型视频扩散转换器模型。</li>
<li>该方法能够在潜在空间内捕捉各种运动动态，避免了明确的核估计。</li>
<li>方法可以适应多种运动模式，对复杂的运动模糊场景有很好的去模糊效果。</li>
<li>实验结果证明，该方法在合成和真实数据集上的性能优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12604">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-02b985ac8749884e14d05a4c86a0fdb4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a42362520175822e3979e92166715baa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab76910992cbf120bd358a36287829ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26f9e2554c0be6a989eefd7f3807f081.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6321d26fc9b22b5fb462bc1bcb92192f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4031e5e58508ecd7dd4890b088eb0b90.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ecd41d6ea1199e6a98e2b0c2100e0875.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Ensemble-score-filter-with-image-inpainting-for-data-assimilation-in-tracking-surface-quasi-geostrophic-dynamics-with-partial-observations"><a href="#Ensemble-score-filter-with-image-inpainting-for-data-assimilation-in-tracking-surface-quasi-geostrophic-dynamics-with-partial-observations" class="headerlink" title="Ensemble score filter with image inpainting for data assimilation in   tracking surface quasi-geostrophic dynamics with partial observations"></a>Ensemble score filter with image inpainting for data assimilation in   tracking surface quasi-geostrophic dynamics with partial observations</h2><p><strong>Authors:Siming Liang, Hoang Tran, Feng Bao, Hristo G. Chipilski, Peter Jan van Leeuwen, Guannan Zhang</strong></p>
<p>Data assimilation plays a pivotal role in understanding and predicting turbulent systems within geoscience and weather forecasting, where data assimilation is used to address three fundamental challenges, i.e., high-dimensionality, nonlinearity, and partial observations. Recent advances in machine learning (ML)-based data assimilation methods have demonstrated encouraging results. In this work, we develop an ensemble score filter (EnSF) that integrates image inpainting to solve the data assimilation problems with partial observations. The EnSF method exploits an exclusively designed training-free diffusion models to solve high-dimensional nonlinear data assimilation problems. Its performance has been successfully demonstrated in the context of having full observations, i.e., all the state variables are directly or indirectly observed. However, because the EnSF does not use a covariance matrix to capture the dependence between the observed and unobserved state variables, it is nontrivial to extend the original EnSF method to the partial observation scenario. In this work, we incorporate various image inpainting techniques into the EnSF to predict the unobserved states during data assimilation. At each filtering step, we first use the diffusion model to estimate the observed states by integrating the likelihood information into the score function. Then, we use image inpainting methods to predict the unobserved state variables. We demonstrate the performance of the EnSF with inpainting by tracking the Surface Quasi-Geostrophic (SQG) model dynamics under a variety of scenarios. The successful proof of concept paves the way to more in-depth investigations on exploiting modern image inpainting techniques to advance data assimilation methodology for practical geoscience and weather forecasting problems. </p>
<blockquote>
<p>数据同化在地球科学和天气预报中理解和预测湍流系统方面起着至关重要的作用，数据同化用于应对三个基本挑战，即高维性、非线性和部分观测。基于机器学习的数据同化方法的最新进展已经取得了令人鼓舞的结果。在这项工作中，我们开发了一种集成图像修复技术的一体化评分过滤方法（EnSF），以解决部分观测数据同化问题。EnSF方法利用专门设计的无训练扩散模型来解决高维非线性数据同化问题。在具有全观测的情境中，即所有状态变量直接或间接观测的情况下，其性能已经得到了成功验证。然而，由于EnSF没有使用协方差矩阵来捕捉观测状态变量和未观测状态变量之间的依赖关系，因此将原始EnSF方法扩展到部分观测情景并不简单。在这项工作中，我们将各种图像修复技术融入EnSF中，以在数据同化过程中预测未观测状态。在每个过滤步骤中，我们首先使用扩散模型通过整合可能性信息来估计观测状态的值。然后，我们使用图像修复方法来预测未观测状态变量。我们通过跟踪各种场景下的表面准地转模型动力学来展示带有修复功能的EnSF的性能。成功的概念验证为利用现代图像修复技术进一步推动数据同化方法在地球科学和天气预报问题中的应用开辟了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12419v1">PDF</a> </p>
<p><strong>摘要</strong><br>    本研究开发了一种集成图像修复技术的集合评分过滤器（EnSF），以解决具有部分观测的数据同化问题。EnSF利用专门设计的无需训练扩散模型解决高维非线性数据同化问题。研究成功证明了其在全观测环境下的性能，但面对部分观测情景时面临挑战。本研究将各种图像修复技术融入EnSF中，用于预测数据同化中的未观测状态。每一步过滤都先利用扩散模型结合似然信息估计观测状态，然后使用图像修复方法预测未观测状态变量。通过对Surface Quasi-Geostrophic（SQG）模型动态进行追踪，证明了EnSF与图像修复相结合的性能。这为利用现代图像修复技术推动数据同化方法在地球科学和天气预报问题中的实际应用铺平了道路。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>数据同化在理解和预测地球科学和天气预报中的湍流系统方面起着至关重要的作用，解决了高维、非线性和部分观测三大挑战。</li>
<li>基于机器学习的数据同化方法取得了令人鼓舞的结果。</li>
<li>集合评分过滤器（EnSF）结合了图像修复技术以解决具有部分观测的数据同化问题。</li>
<li>EnSF利用无需训练的扩散模型解决高维非线性问题。</li>
<li>EnSF在全观测环境下表现出成功的性能。</li>
<li>面对部分观测情景的挑战，EnSF通过融入图像修复技术预测未观测状态。</li>
<li>通过追踪SQG模型动态证明了EnSF与图像修复结合的性能，这为未来在地球科学和天气预报中实际应用提供了基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12419">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d913029468fca6ecd6e21cdfb4fc61ed.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="GPS-as-a-Control-Signal-for-Image-Generation"><a href="#GPS-as-a-Control-Signal-for-Image-Generation" class="headerlink" title="GPS as a Control Signal for Image Generation"></a>GPS as a Control Signal for Image Generation</h2><p><strong>Authors:Chao Feng, Ziyang Chen, Aleksander Holynski, Alexei A. Efros, Andrew Owens</strong></p>
<p>We show that the GPS tags contained in photo metadata provide a useful control signal for image generation. We train GPS-to-image models and use them for tasks that require a fine-grained understanding of how images vary within a city. In particular, we train a diffusion model to generate images conditioned on both GPS and text. The learned model generates images that capture the distinctive appearance of different neighborhoods, parks, and landmarks. We also extract 3D models from 2D GPS-to-image models through score distillation sampling, using GPS conditioning to constrain the appearance of the reconstruction from each viewpoint. Our evaluations suggest that our GPS-conditioned models successfully learn to generate images that vary based on location, and that GPS conditioning improves estimated 3D structure. </p>
<blockquote>
<p>我们证明，照片元数据中包含的GPS标签为图像生成提供了有用的控制信号。我们训练GPS到图像的模型，并将它们用于需要精细理解图像如何在城市内部变化的任务。特别是，我们训练了一个扩散模型，以根据GPS和文本生成图像。学习到的模型生成能够捕捉不同街区、公园和地标独特外观的图像。我们还通过评分蒸馏采样从二维GPS到图像的模型中提取三维模型，使用GPS条件来约束每个视点的重建外观。我们的评估表明，我们的GPS条件模型成功学习根据位置生成图像，并且GPS条件改进了估计的3D结构。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12390v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://cfeng16.github.io/gps-gen/">https://cfeng16.github.io/gps-gen/</a></p>
<p><strong>Summary</strong><br>     本文展示了照片元数据中的GPS标签对于图像生成具有有用的控制信号。研究团队训练了GPS到图像的模型，并用于需要精细理解城市内图像如何变化的任务。特别是，他们训练了一个扩散模型，该模型可以根据GPS和文本生成图像。该模型能够捕捉不同街区、公园和地标的独特外观。此外，研究团队通过评分蒸馏采样从二维GPS到图像模型中提取三维模型，利用GPS条件约束从每个视角进行重建的外观。评估表明，GPS条件模型成功学习根据位置生成图像，并且GPS条件改进了估计的三维结构。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPS标签在图像生成中提供有用的控制信号。</li>
<li>通过训练GPS到图像的模型，实现了对城市内图像变化的精细理解。</li>
<li>扩散模型可以根据GPS和文本生成图像，能捕捉不同地点的独特外观。</li>
<li>通过评分蒸馏采样从二维GPS到图像模型中提取三维模型。</li>
<li>GPS条件用于约束从各个视角的重建外观。</li>
<li>GPS条件模型能够成功根据位置生成图像。</li>
<li>GPS条件改进了估计的三维结构。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12390">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-21f51f2a5a39977be5238d681598a715.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0a76df4bcf6fe913aa566dcc5c18ed7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ecfe5f0da70970b64df689ab69f1b85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0285c3db34c02b3c60e4053cdd434b04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cec582bfbfea2b01e76c1a34c76786e1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Diffusion-Models-for-Anomaly-Detection"><a href="#A-Survey-on-Diffusion-Models-for-Anomaly-Detection" class="headerlink" title="A Survey on Diffusion Models for Anomaly Detection"></a>A Survey on Diffusion Models for Anomaly Detection</h2><p><strong>Authors:Jing Liu, Zhenchao Ma, Zepu Wang, Yang Liu, Zehua Wang, Peng Sun, Liang Song, Bo Hu, Azzedine Boukerche, Victor C. M. Leung</strong></p>
<p>Diffusion models (DMs) have emerged as a powerful class of generative AI models, showing remarkable potential in anomaly detection (AD) tasks across various domains, such as cybersecurity, fraud detection, healthcare, and manufacturing. The intersection of these two fields, termed diffusion models for anomaly detection (DMAD), offers promising solutions for identifying deviations in increasingly complex and high-dimensional data. In this survey, we review recent advances in DMAD research. We begin by presenting the fundamental concepts of AD and DMs, followed by a comprehensive analysis of classic DM architectures including DDPMs, DDIMs, and Score SDEs. We further categorize existing DMAD methods into reconstruction-based, density-based, and hybrid approaches, providing detailed examinations of their methodological innovations. We also explore the diverse tasks across different data modalities, encompassing image, time series, video, and multimodal data analysis. Furthermore, we discuss critical challenges and emerging research directions, including computational efficiency, model interpretability, robustness enhancement, edge-cloud collaboration, and integration with large language models. The collection of DMAD research papers and resources is available at <a target="_blank" rel="noopener" href="https://github.com/fdjingliu/DMAD">https://github.com/fdjingliu/DMAD</a>. </p>
<blockquote>
<p>扩散模型（DMs）作为一类强大的生成人工智能模型已经崭露头角，在异常检测（AD）任务中显示出显著潜力，广泛应用于网络安全、欺诈检测、医疗保健和制造等多个领域。这两个领域的交集，被称为用于异常检测的扩散模型（DMAD），为识别日益复杂和高维数据中的偏差提供了有前景的解决方案。在这篇综述中，我们回顾了DMAD研究的最新进展。首先介绍AD和DM的基本概念，然后全面分析经典的DM架构，包括DDPMs、DDIMS和Score SDEs。我们进一步将现有的DMAD方法分为基于重建的、基于密度的和混合方法，并对其方法创新进行详细检查。我们还探讨了不同数据模态的多样化任务，包括图像、时间序列、视频和多模态数据分析。此外，我们还讨论了关键的挑战和新兴的研究方向，包括计算效率、模型可解释性、鲁棒性增强、边缘云协作以及与大型语言模型的集成。DMAD研究论文和资源集可在<a target="_blank" rel="noopener" href="https://github.com/fdjingliu/DMAD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/fdjingliu/DMAD找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11430v2">PDF</a> </p>
<p><strong>Summary</strong><br>     扩散模型（DMs）作为生成式人工智能模型的新兴强大类别，在异常检测（AD）任务中展现出巨大潜力，广泛应用于网络安全、欺诈检测、医疗保健和制造等领域。本文综述了扩散模型在异常检测方面的最新研究进展，介绍了异常检测和扩散模型的基本概念，分析了经典的扩散模型架构，包括DDPMs、DDIIMs和Score SDEs，并将现有的扩散模型异常检测方法分为重建型、密度型和混合型方法，详细探讨了其方法创新。此外，还介绍了不同数据模态的任务，包括图像、时间序列、视频和多模态数据分析。文章还讨论了计算效率、模型可解释性、鲁棒性增强、边缘云协作以及与大型语言模型的集成等关键挑战和新兴研究方向。相关资源可访问：<a target="_blank" rel="noopener" href="https://github.com/fdjingliu/DMAD%E3%80%82">https://github.com/fdjingliu/DMAD。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型（DMs）在异常检测（AD）任务中展现出显著潜力，应用范围广泛。</li>
<li>文章综述了DMs在异常检测方面的最新研究进展。</li>
<li>介绍了异常检测和扩散模型的基本概念。</li>
<li>分析了经典的扩散模型架构，包括DDPMs、DDIIMs和Score SDEs。</li>
<li>将现有的扩散模型异常检测方法分为重建型、密度型和混合型方法。</li>
<li>介绍了不同数据模态的异常检测任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11430">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5eb0fba3ba2682755e88f6dc54de251e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad93bf5ca5dae02ada4447cf3e5a1039.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38fa74134c1cbc69610220c57a5573a8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e63f669e407ea30c4cea814c753db54c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe42fc0b2d14a07265587d253d62ba8e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Boosting-Diffusion-Guidance-via-Learning-Degradation-Aware-Models-for-Blind-Super-Resolution"><a href="#Boosting-Diffusion-Guidance-via-Learning-Degradation-Aware-Models-for-Blind-Super-Resolution" class="headerlink" title="Boosting Diffusion Guidance via Learning Degradation-Aware Models for   Blind Super Resolution"></a>Boosting Diffusion Guidance via Learning Degradation-Aware Models for   Blind Super Resolution</h2><p><strong>Authors:Shao-Hao Lu, Ren Wang, Ching-Chun Huang, Wei-Chen Chiu</strong></p>
<p>Recently, diffusion-based blind super-resolution (SR) methods have shown great ability to generate high-resolution images with abundant high-frequency detail, but the detail is often achieved at the expense of fidelity. Meanwhile, another line of research focusing on rectifying the reverse process of diffusion models (i.e., diffusion guidance), has demonstrated the power to generate high-fidelity results for non-blind SR. However, these methods rely on known degradation kernels, making them difficult to apply to blind SR. To address these issues, we present DADiff in this paper. DADiff incorporates degradation-aware models into the diffusion guidance framework, eliminating the need to know degradation kernels. Additionally, we propose two novel techniques – input perturbation and guidance scalar – to further improve our performance. Extensive experimental results show that our proposed method has superior performance over state-of-the-art methods on blind SR benchmarks. </p>
<blockquote>
<p>最近，基于扩散的盲超分辨率（SR）方法显示出生成富含高频细节的高分辨率图像的强大能力，但往往以保真度的损失为代价来实现这些细节。与此同时，另一条研究聚焦于纠正扩散模型的逆向过程（即扩散引导），已显示出为非盲SR生成高保真结果的能力。然而，这些方法依赖于已知的退化核，使得它们难以应用于盲SR。为了解决这些问题，我们在本文中提出了DADiff。DADiff将退化感知模型纳入扩散引导框架，无需了解退化核。此外，我们提出了两种新技术——输入扰动和引导标量——来进一步提高我们的性能。大量的实验结果表明，我们在盲SR基准测试上的表现超过了最新技术的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08819v2">PDF</a> To appear in WACV 2025. Code is available at:   <a target="_blank" rel="noopener" href="https://github.com/ryanlu2240/DADiff">https://github.com/ryanlu2240/DADiff</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于扩散模型的盲超分辨率（SR）方法——DADiff。该方法结合了感知退化模型与扩散引导框架，无需知道退化核信息。同时，通过引入输入扰动和引导标量两项新技术，进一步提升了性能。实验表明，在盲SR基准测试中，该方法性能卓越，超越了现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DADiff结合感知退化模型与扩散引导框架，解决扩散模型在盲超分辨率（SR）中的性能问题。</li>
<li>无需知道退化核信息，扩大了应用范围。</li>
<li>引入输入扰动技术，提升模型性能。</li>
<li>采用引导标量技术，进一步提高效果。</li>
<li>在盲SR基准测试中表现卓越。</li>
<li>对比现有技术，DADiff具有更高的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08819">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5fdb8c3bc905a084534dad8213a45fab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5be19807ba797a94e8af474d841ff0ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c0cb7cf103785dca203afc355551137.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfcb5c0167308092bf8db4f5d2801556.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92344895f872238ff4b3e707b02bec12.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Deep-Geometric-Moments-Promote-Shape-Consistency-in-Text-to-3D-Generation"><a href="#Deep-Geometric-Moments-Promote-Shape-Consistency-in-Text-to-3D-Generation" class="headerlink" title="Deep Geometric Moments Promote Shape Consistency in Text-to-3D   Generation"></a>Deep Geometric Moments Promote Shape Consistency in Text-to-3D   Generation</h2><p><strong>Authors:Utkarsh Nath, Rajeev Goel, Eun Som Jeon, Changhoon Kim, Kyle Min, Yezhou Yang, Yingzhen Yang, Pavan Turaga</strong></p>
<p>To address the data scarcity associated with 3D assets, 2D-lifting techniques such as Score Distillation Sampling (SDS) have become a widely adopted practice in text-to-3D generation pipelines. However, the diffusion models used in these techniques are prone to viewpoint bias and thus lead to geometric inconsistencies such as the Janus problem. To counter this, we introduce MT3D, a text-to-3D generative model that leverages a high-fidelity 3D object to overcome viewpoint bias and explicitly infuse geometric understanding into the generation pipeline. Firstly, we employ depth maps derived from a high-quality 3D model as control signals to guarantee that the generated 2D images preserve the fundamental shape and structure, thereby reducing the inherent viewpoint bias. Next, we utilize deep geometric moments to ensure geometric consistency in the 3D representation explicitly. By incorporating geometric details from a 3D asset, MT3D enables the creation of diverse and geometrically consistent objects, thereby improving the quality and usability of our 3D representations. Project page and code: <a target="_blank" rel="noopener" href="https://moment-3d.github.io/">https://moment-3d.github.io/</a> </p>
<blockquote>
<p>针对与3D资产相关的数据稀缺问题，如分数蒸馏采样（SDS）的2D提升技术已成为文本到3D生成管道中广泛采用的实践。然而，这些技术中使用的扩散模型容易出现视点偏差，从而导致几何不一致性问题，例如 Janus 问题。为了解决这个问题，我们引入了MT3D，这是一个文本到3D的生成模型，它利用高保真3D对象来克服视点偏差，并将几何理解明确融入生成管道。首先，我们采用从高质量3D模型派生的深度图作为控制信号，以保证生成的2D图像保持基本形状和结构，从而减少固有的视点偏差。其次，我们利用深层几何矩来确保3D表示中的几何一致性。通过融入3D资产的几何细节，MT3D能够创建多样且几何一致的物体，从而提高我们3D表示的质量和可用性。项目页面和代码：<a target="_blank" rel="noopener" href="https://moment-3d.github.io/">https://moment-3d.github.io/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.05938v2">PDF</a> This paper has been accepted to WACV 2025</p>
<p><strong>Summary</strong></p>
<p>文本介绍了为解决文本到三维生成中的视角偏差问题，提出了一个名为MT3D的文本到三维生成模型。该模型采用高质量的三维模型深度图作为控制信号，确保了生成的二维图像保持了基本的形状和结构，从而减少了固有的视角偏差。同时，模型利用深度几何时刻确保三维表示中的几何一致性。通过从三维资产中融入几何细节，MT3D能够创建多样且几何一致的物体，从而提高三维表示的质量和可用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据稀缺是文本到三维资产生成的一大挑战。因此，二维提升技术（如分数蒸馏采样）广泛应用于文本到三维生成的流程中。但此方法涉及的数据视角偏差可能导致几何不一致问题。</li>
<li>新模型MT3D采用高质量的三维模型深度图作为控制信号来生成二维图像，以减少视角偏差并保持物体的基本形状和结构。这一创新策略提高了生成的图像质量。</li>
<li>MT3D利用深度几何时刻确保三维表示的几何一致性，进一步增强了模型的准确性和可靠性。这一特点使得生成的物体更加真实和多样。</li>
<li>通过融入三维资产的几何细节，MT3D可以创建既多样又几何一致的物体，大大提高了三维表示的质量和可用性。这是该模型在文本到三维生成领域的重大突破。</li>
<li>新模型克服了传统的几何不一致问题（如Janus问题），为后续的三维生成研究提供了宝贵的参考和启示。项目页面和代码可供查阅，便于进一步研究和应用。</li>
<li>MT3D模型展示了其在文本到三维生成领域的潜力，为未来的应用场景提供了广阔的可能性。随着技术的进一步发展，该模型有望在各种领域中发挥更大的作用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.05938">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ca7418e970c556779eb187a88b326f70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d17b9178e018c43fea51926710e3f06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-585e0859b3e63ee9a3c8a2c6a42f0e73.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="VisMin-Visual-Minimal-Change-Understanding"><a href="#VisMin-Visual-Minimal-Change-Understanding" class="headerlink" title="VisMin: Visual Minimal-Change Understanding"></a>VisMin: Visual Minimal-Change Understanding</h2><p><strong>Authors:Rabiul Awal, Saba Ahmadi, Le Zhang, Aishwarya Agrawal</strong></p>
<p>Fine-grained understanding of objects, attributes, and relationships between objects is crucial for visual-language models (VLMs). Existing benchmarks primarily focus on evaluating VLMs’ capability to distinguish between two very similar captions given an image. In this paper, we introduce a new, challenging benchmark termed Visual Minimal-Change Understanding (VisMin), which requires models to predict the correct image-caption match given two images and two captions. The image pair and caption pair contain minimal changes, i.e., only one aspect changes at a time from among the following: object, attribute, count, and spatial relation. These changes test the models’ understanding of objects, attributes (such as color, material, shape), counts, and spatial relationships between objects. We built an automatic framework using large language models and diffusion models, followed by a rigorous 4-step verification process by human annotators. Empirical experiments reveal that current VLMs exhibit notable deficiencies in understanding spatial relationships and counting abilities. We also generate a large-scale training dataset to finetune CLIP and Idefics2, showing significant improvements in fine-grained understanding across benchmarks and in CLIP’s general image-text alignment. We release all resources, including the benchmark, training data, and finetuned model checkpoints, at <a target="_blank" rel="noopener" href="https://vismin.net/">https://vismin.net/</a>. </p>
<blockquote>
<p>对于视觉语言模型（VLMs）而言，对物体、属性以及物体间关系的精细理解是至关重要的。现有的基准测试主要侧重于评估VLMs在给定图像的情况下区分两个非常相似的标题的能力。在本文中，我们引入了一个新的具有挑战性的基准测试，称为视觉最小变化理解（VisMin），该测试要求模型在给定的两个图像和两个标题中预测正确的图像-标题匹配。图像对和标题对中包含了最小的变化，即每次只有一个方面的变化，包括：物体、属性、数量和空间关系。这些变化测试了模型对物体、属性（如颜色、材质、形状）、数量和物体之间空间关系的理解。我们使用大型语言模型和扩散模型建立了一个自动框架，随后经过人工注释者进行的严格四步验证过程。经验实验表明，目前的VLM在理解空间关系和计数能力方面存在明显的缺陷。我们还生成了一个大规模的训练数据集来微调CLIP和Idefics2，在基准测试和CLIP的通用图像文本对齐方面显示出对精细理解的显著改善。我们发布的所有资源，包括基准测试、训练数据和微调后的模型检查点，都可以在<a target="_blank" rel="noopener" href="https://vismin.net/%E6%89%BE%E5%88%B0%E3%80%82">https://vismin.net/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.16772v2">PDF</a> Accepted at NeurIPS 2024. Project URL at <a target="_blank" rel="noopener" href="https://vismin.net/">https://vismin.net/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新的视觉语言模型评估基准——Visual Minimal-Change Understanding（VisMin）。该基准要求模型预测给定两个图像和两个仅在一个方面发生变化的描述时，正确匹配图像和描述。通过这种方式，VisMin评估模型对对象、属性（如颜色、材质、形状）、数量和对象间空间关系的理解。采用大型语言模型和扩散模型构建自动框架，并通过人类注释者进行严格的四步验证过程。实验表明，当前视觉语言模型在理解和计数能力方面存在明显缺陷。通过微调CLIP和Idefics2的训练数据集，在基准测试中显示出精细理解的显著改善。所有资源，包括基准测试、训练数据和微调模型检查点，均已发布在<a target="_blank" rel="noopener" href="https://vismin.net/%E4%B8%8A%E3%80%82">https://vismin.net/上。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语言模型（VLMs）需要对对象、属性和对象间关系的精细理解。</li>
<li>提出了一个新的评估基准——Visual Minimal-Change Understanding（VisMin），以评估模型对微小变化的敏感度。</li>
<li>VisMin要求模型预测给定两个图像和两个描述中只有一个方面发生变化时的正确匹配。</li>
<li>VisMin测试模型在对象、属性（如颜色、材质、形状）、数量和空间关系方面的理解。</li>
<li>采用大型语言模型和扩散模型构建自动评估框架，并通过人工验证确保准确性。</li>
<li>现有VLM在理解和计数能力方面存在缺陷。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.16772">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7dc8c11625c44d547a1ce363c7ae07c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1f8c65f04c4d5a141d47189b9b82f75d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-804ca11a5a797be77734a321746015ba.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MD-Dose-A-diffusion-model-based-on-the-Mamba-for-radiation-dose-prediction"><a href="#MD-Dose-A-diffusion-model-based-on-the-Mamba-for-radiation-dose-prediction" class="headerlink" title="MD-Dose: A diffusion model based on the Mamba for radiation dose   prediction"></a>MD-Dose: A diffusion model based on the Mamba for radiation dose   prediction</h2><p><strong>Authors:Linjie Fu, Xia Li, Xiuding Cai, Yingkai Wang, Xueyao Wang, Yali Shen, Yu Yao</strong></p>
<p>Radiation therapy is crucial in cancer treatment. Experienced experts typically iteratively generate high-quality dose distribution maps, forming the basis for excellent radiation therapy plans. Therefore, automated prediction of dose distribution maps is significant in expediting the treatment process and providing a better starting point for developing radiation therapy plans. With the remarkable results of diffusion models in predicting high-frequency regions of dose distribution maps, dose prediction methods based on diffusion models have been extensively studied. However, existing methods mainly utilize CNNs or Transformers as denoising networks. CNNs lack the capture of global receptive fields, resulting in suboptimal prediction performance. Transformers excel in global modeling but face quadratic complexity with image size, resulting in significant computational overhead. To tackle these challenges, we introduce a novel diffusion model, MD-Dose, based on the Mamba architecture for predicting radiation therapy dose distribution in thoracic cancer patients. In the forward process, MD-Dose adds Gaussian noise to dose distribution maps to obtain pure noise images. In the backward process, MD-Dose utilizes a noise predictor based on the Mamba to predict the noise, ultimately outputting the dose distribution maps. Furthermore, We develop a Mamba encoder to extract structural information and integrate it into the noise predictor for localizing dose regions in the planning target volume (PTV) and organs at risk (OARs). Through extensive experiments on a dataset of 300 thoracic tumor patients, we showcase the superiority of MD-Dose in various metrics and time consumption. </p>
<blockquote>
<p>放射治疗在癌症治疗中占有至关重要的地位。经验丰富的专家通常会通过迭代生成高质量的剂量分布图，为优秀的放射治疗计划奠定基础。因此，剂量分布图的自动预测在加速治疗过程以及为制定放射治疗计划提供更好的起点方面具有重要意义。扩散模型在预测剂量分布图的高频区域方面取得了显著成果，因此基于扩散模型的剂量预测方法已经得到了广泛的研究。然而，现有方法主要利用卷积神经网络（CNN）或Transformer作为去噪网络。CNN缺乏全局感受野的捕捉，导致预测性能不佳。虽然Transformer擅长全局建模，但随着图像大小的增加，其面临二次复杂性，导致计算开销较大。为了应对这些挑战，我们提出了一种基于Mamba架构的新型扩散模型MD-Dose，用于预测胸部癌症患者的放射治疗剂量分布。在正向过程中，MD-Dose向剂量分布图添加高斯噪声以获得纯噪声图像。在逆向过程中，MD-Dose利用基于Mamba的噪声预测器来预测噪声，并最终输出剂量分布图。此外，我们开发了一个Mamba编码器来提取结构信息，并将其整合到噪声预测器中，以定位计划靶区（PTV）和风险器官（OARs）中的剂量区域。通过对300例胸部肿瘤患者数据集进行的广泛实验，我们展示了MD-Dose在各种指标和时间消耗方面的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.08479v2">PDF</a> </p>
<p><strong>Summary</strong>：辐射治疗在癌症治疗中至关重要。专家们通过迭代生成高质量剂量分布图，形成优秀的放射治疗计划的基础。因此，自动预测剂量分布图对于加快治疗过程以及为制定放射治疗计划提供更好的起点具有重要意义。基于扩散模型在预测剂量分布图高频区域方面的出色表现，基于扩散模型的剂量预测方法已经得到了广泛的研究。本研究针对现有方法存在的问题，提出了一种基于Mamba架构的新型扩散模型MD-Dose，用于预测胸部癌症患者的放射治疗剂量分布。该模型在正向过程中向剂量分布图添加高斯噪声以获得纯噪声图像，在逆向过程中利用基于Mamba的噪声预测器进行预测，最终输出剂量分布图。此外，还开发了Mamba编码器以提取结构信息并将其整合到噪声预测器中，以定位计划靶区（PTV）和风险器官（OARs）的剂量区域。在300例胸部肿瘤患者数据集上的大量实验表明，MD-Dose在各种指标和时间消耗方面的优越性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>辐射治疗在癌症治疗中占据重要地位，专家手工生成的剂量分布图是高质量治疗计划的关键。</li>
<li>自动化预测剂量分布图能加快治疗过程，为制定放射治疗计划提供更好的起点。</li>
<li>扩散模型在预测剂量分布图高频区域方面表现出色，已被广泛研究。</li>
<li>现有方法主要使用CNN或Transformer作为去噪网络，但存在局限性。</li>
<li>MD-Dose是一种新型扩散模型，基于Mamba架构，用于预测胸部癌症患者的辐射治疗剂量分布。</li>
<li>MD-Dose通过正向和逆向过程预测剂量分布图，并通过Mamba编码器提取结构信息以提高预测准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.08479">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7d4c2d24207e773ddf35dbb0cebe8421.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b29cfef6d11a0d4062826a95f1ae705.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6b7f838ad88733ae54baaa60aca15ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a424dbbfe780c0e2bc8ca0f36545c54.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5944f29fbb656dc9386fa4ef1dede3f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4bbddb0720543f26df1ecfcfd580455.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-24/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-24/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-24/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_医学图像/2402.01034v3/page_3_0.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-01-24  Learning accurate rigid registration for longitudinal brain MRI from   synthetic data
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-24/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-dc0a07268068ce868603d3c91304793b.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-01-24  DWTNeRF Boosting Few-shot Neural Radiance Fields via Discrete Wavelet   Transform
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27663.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
