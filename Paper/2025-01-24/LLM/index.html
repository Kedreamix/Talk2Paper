<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-24  Does Table Source Matter? Benchmarking and Improving Multimodal   Scientific Table Understanding and Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.09957v2/page_1_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-24-æ›´æ–°"><a href="#2025-01-24-æ›´æ–°" class="headerlink" title="2025-01-24 æ›´æ–°"></a>2025-01-24 æ›´æ–°</h1><h2 id="Does-Table-Source-Matter-Benchmarking-and-Improving-Multimodal-Scientific-Table-Understanding-and-Reasoning"><a href="#Does-Table-Source-Matter-Benchmarking-and-Improving-Multimodal-Scientific-Table-Understanding-and-Reasoning" class="headerlink" title="Does Table Source Matter? Benchmarking and Improving Multimodal   Scientific Table Understanding and Reasoning"></a>Does Table Source Matter? Benchmarking and Improving Multimodal   Scientific Table Understanding and Reasoning</h2><p><strong>Authors:Bohao Yang, Yingji Zhang, Dong Liu, AndrÃ© Freitas, Chenghua Lin</strong></p>
<p>Recent large language models (LLMs) have advanced table understanding capabilities but rely on converting tables into text sequences. While multimodal large language models (MLLMs) enable direct visual processing, they face limitations in handling scientific tables due to fixed input image resolutions and insufficient numerical reasoning capabilities. We present a comprehensive framework for multimodal scientific table understanding and reasoning with dynamic input image resolutions. Our framework consists of three key components: (1) MMSci-Pre, a domain-specific table structure learning dataset of 52K scientific table structure recognition samples, (2) MMSci-Ins, an instruction tuning dataset with 12K samples across three table-based tasks, and (3) MMSci-Eval, a benchmark with 3,114 testing samples specifically designed to evaluate numerical reasoning capabilities. Extensive experiments demonstrate that our domain-specific approach with 52K scientific table images achieves superior performance compared to 150K general-domain tables, highlighting the importance of data quality over quantity. Our proposed table-based MLLMs with dynamic input resolutions show significant improvements in both general table understanding and numerical reasoning capabilities, with strong generalisation to held-out datasets. Our code and data are publicly available at <a target="_blank" rel="noopener" href="https://github.com/Bernard-Yang/MMSci_Table">https://github.com/Bernard-Yang/MMSci_Table</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å…·å¤‡äº†å…ˆè¿›çš„è¡¨æ ¼ç†è§£èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä¾èµ–äºå°†è¡¨æ ¼è½¬æ¢ä¸ºæ–‡æœ¬åºåˆ—ã€‚è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰èƒ½å¤Ÿå®ç°ç›´æ¥è§†è§‰å¤„ç†ï¼Œä½†ç”±äºå›ºå®šè¾“å…¥å›¾åƒåˆ†è¾¨ç‡å’Œæ•°å€¼æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œå®ƒä»¬åœ¨å¤„ç†ç§‘å­¦è¡¨æ ¼æ—¶é¢ä¸´å±€é™ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…·æœ‰åŠ¨æ€è¾“å…¥å›¾åƒåˆ†è¾¨ç‡çš„å¤šæ¨¡æ€ç§‘å­¦è¡¨æ ¼ç†è§£ä¸æ¨ç†çš„ç»¼åˆæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰MMSci-Preï¼Œä¸€ä¸ªåŒ…å«5.2ä¸‡ä»½ç§‘å­¦è¡¨æ ¼ç»“æ„è¯†åˆ«æ ·æœ¬çš„ç‰¹å®šé¢†åŸŸè¡¨æ ¼ç»“æ„å­¦ä¹ æ•°æ®é›†ï¼›ï¼ˆ2ï¼‰MMSci-Insï¼Œä¸€ä¸ªåŒ…å«1.2ä¸‡ä»½æ ·æœ¬çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œæ¶µç›–ä¸‰ä¸ªåŸºäºè¡¨æ ¼çš„ä»»åŠ¡ï¼›ï¼ˆ3ï¼‰MMSci-Evalï¼Œä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°æ•°å€¼æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«3,114ä¸ªæµ‹è¯•æ ·æœ¬ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç‰¹å®šé¢†åŸŸæ–¹æ³•åœ¨å¤„ç†5.2ä¸‡ä»½ç§‘å­¦è¡¨æ ¼å›¾åƒæ—¶ï¼Œç›¸è¾ƒäºå¤„ç†æ¶‰åŠ15ä¸‡ä»½æ™®é€šè¡¨æ ¼çš„æ–¹æ³•è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œè¿™çªæ˜¾äº†æ•°æ®è´¨é‡é«˜äºæ•°é‡çš„é‡è¦æ€§ã€‚æˆ‘ä»¬æå‡ºçš„åŸºäºè¡¨æ ¼çš„å¤šæ¨¡æ€LLMå…·æœ‰åŠ¨æ€è¾“å…¥åˆ†è¾¨ç‡ï¼Œåœ¨é€šç”¨è¡¨æ ¼ç†è§£å’Œæ•°å€¼æ¨ç†èƒ½åŠ›æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ï¼Œå¹¶ä¸”å¯¹æœªå‚ä¸æ•°æ®é›†å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Bernard-Yang/MMSci_Table%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Bernard-Yang/MMSci_Tableä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13042v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¡¨æ ¼ç†è§£æ–¹é¢æœ‰æ‰€è¿›æ­¥ï¼Œä½†ä»éœ€å°†è¡¨æ ¼è½¬æ¢ä¸ºæ–‡æœ¬åºåˆ—ã€‚å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¯ç›´æ¥è¿›è¡Œè§†è§‰å¤„ç†ï¼Œä½†åœ¨å¤„ç†ç§‘å­¦è¡¨æ ¼æ—¶ï¼Œç”±äºå›ºå®šè¾“å…¥å›¾åƒåˆ†è¾¨ç‡å’Œæ•°å€¼æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ä¸ªåŒ…å«åŠ¨æ€è¾“å…¥å›¾åƒåˆ†è¾¨ç‡çš„å¤šæ¨¡æ€ç§‘å­¦è¡¨æ ¼ç†è§£ä¸æ¨ç†çš„ç»¼åˆæ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šMMSci-Preæ•°æ®é›†ã€MMSci-InsæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†å’ŒMMSci-EvalåŸºå‡†æµ‹è¯•é›†ã€‚å®éªŒè¡¨æ˜ï¼Œä¸15ä¸‡å¼ é€šç”¨é¢†åŸŸè¡¨æ ¼ç›¸æ¯”ï¼Œä½¿ç”¨5.2ä¸‡å¼ ç§‘å­¦è¡¨æ ¼å›¾åƒçš„é¢†åŸŸç‰¹å®šæ–¹æ³•è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œçªæ˜¾æ•°æ®è´¨é‡æ¯”æ•°é‡æ›´é‡è¦ã€‚æå‡ºçš„åŸºäºè¡¨æ ¼çš„MLLMså…·æœ‰åŠ¨æ€è¾“å…¥åˆ†è¾¨ç‡ï¼Œåœ¨é€šç”¨è¡¨æ ¼ç†è§£å’Œæ•°å€¼æ¨ç†èƒ½åŠ›æ–¹é¢å‡æœ‰æ˜¾è‘—æé«˜ï¼Œå¯¹æœªå…¬å¼€æ•°æ®é›†å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¡¨æ ¼ç†è§£æ–¹é¢å·²å–å¾—è¿›å±•ï¼Œä½†ä»éœ€æ”¹è¿›ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¯ä»¥ç›´æ¥å¤„ç†è§†è§‰ä¿¡æ¯ï¼Œä½†åœ¨å¤„ç†ç§‘å­¦è¡¨æ ¼æ—¶é¢ä¸´å›ºå®šè¾“å…¥å›¾åƒåˆ†è¾¨ç‡å’Œæ•°å€¼æ¨ç†èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸€ä¸ªç»¼åˆæ¡†æ¶ï¼ŒåŒ…å«åŠ¨æ€è¾“å…¥å›¾åƒåˆ†è¾¨ç‡ï¼Œç”¨äºå¤šæ¨¡æ€ç§‘å­¦è¡¨æ ¼ç†è§£ä¸æ¨ç†ã€‚</li>
<li>æ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šMMSci-Preæ•°æ®é›†ç”¨äºè¡¨æ ¼ç»“æ„å­¦ä¹ ï¼ŒMMSci-InsæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ç”¨äºæŒ‡ä»¤è°ƒæ•´ï¼ŒMMSci-EvalåŸºå‡†æµ‹è¯•é›†ç”¨äºè¯„ä¼°æ•°å€¼æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œé¢†åŸŸç‰¹å®šçš„æ–¹æ³•ï¼ˆä½¿ç”¨5.2ä¸‡å¼ ç§‘å­¦è¡¨æ ¼å›¾åƒï¼‰æ€§èƒ½ä¼˜äºé€šç”¨é¢†åŸŸè¡¨æ ¼ï¼ˆä½¿ç”¨15ä¸‡å¼ å›¾åƒï¼‰ï¼Œå¼ºè°ƒæ•°æ®è´¨é‡çš„é‡è¦æ€§ã€‚</li>
<li>åŸºäºè¡¨æ ¼çš„MLLMså…·æœ‰åŠ¨æ€è¾“å…¥åˆ†è¾¨ç‡ï¼Œåœ¨é€šç”¨è¡¨æ ¼ç†è§£å’Œæ•°å€¼æ¨ç†èƒ½åŠ›æ–¹é¢æœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ce8bf44a5e918d3995729f01a183a840.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.13042v1/page_2_0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b41b68cd5eed9e1145b12db934850c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26ec536a383cb36491ef8b00f10b6ba4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-accb2e756f216b4d08217a48783922b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7292fcff74c0e43ad58d4d5a2485f4b3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Pairwise-RM-Perform-Best-of-N-Sampling-with-Knockout-Tournament"><a href="#Pairwise-RM-Perform-Best-of-N-Sampling-with-Knockout-Tournament" class="headerlink" title="Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament"></a>Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament</h2><p><strong>Authors:Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, Juanzi Li</strong></p>
<p>Best-of-N (BoN) sampling, a common strategy for test-time scaling of Large Language Models (LLMs), relies on reward models to select the best candidate solution from multiple generations. However, traditional reward models often assign arbitrary and inconsistent scores, limiting their effectiveness. To address this, we propose a Pairwise Reward Model (Pairwise RM) combined with a knockout tournament for BoN sampling. Instead of assigning absolute scores, given one math problem, Pairwise RM evaluates two candidate solutionsâ€™ correctness simultaneously. This approach eliminates the need for arbitrary scoring and enables cross-validation of solutions through parallel comparison. In the knockout tournament, Pairwise RM conducts pairwise comparisons between candidate solutions and eliminates the incorrect ones iteratively. We construct \ourdataset, a large-scale dataset of 443K pairwise comparisons derived from NumiaMath and annotated using \texttt{gemini-1.5-flash}, and train the Pairwise RM via supervised fine-tuning. Experiments on MATH-500 and the Olympiad Bench demonstrate significant improvements over traditional discriminative reward models. And a 40% to 60% relative improvement is achieved on the top 50% challenging problems. </p>
<blockquote>
<p>â€œBest-of-N (BoN)é‡‡æ ·æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æµ‹è¯•æ—¶é—´ç¼©æ”¾çš„ä¸€ç§å¸¸è§ç­–ç•¥ï¼Œå®ƒä¾èµ–äºå¥–åŠ±æ¨¡å‹ä»å¤šä¸ªç”Ÿæˆä¸­é€‰æ‹©æœ€ä½³å€™é€‰è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä¼ ç»Ÿå¥–åŠ±æ¨¡å‹é€šå¸¸ä¼šåˆ†é…ä»»æ„å’Œä¸ä¸€è‡´çš„åˆ†æ•°ï¼Œé™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆæ·˜æ±°åˆ¶é”¦æ ‡èµ›çš„é…å¯¹å¥–åŠ±æ¨¡å‹ï¼ˆPairwise RMï¼‰ã€‚ä¸ç»™å‡ºç»å¯¹åˆ†æ•°ä¸åŒï¼Œç»™å®šä¸€ä¸ªæ•°å­¦é—®é¢˜ï¼Œé…å¯¹RMåŒæ—¶è¯„ä¼°ä¸¤ä¸ªå€™é€‰è§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§ã€‚è¿™ç§æ–¹æ³•æ¶ˆé™¤äº†å¯¹ä»»æ„è¯„åˆ†çš„éœ€æ±‚ï¼Œå¹¶é€šè¿‡å¹¶è¡Œæ¯”è¾ƒå®ç°äº†è§£å†³æ–¹æ¡ˆçš„äº¤å‰éªŒè¯ã€‚åœ¨æ·˜æ±°é”¦æ ‡èµ›ä¸­ï¼Œé…å¯¹RMè¿›è¡Œå€™é€‰è§£å†³æ–¹æ¡ˆä¹‹é—´çš„é…å¯¹æ¯”è¾ƒï¼Œå¹¶é€šè¿‡è¿­ä»£æ–¹å¼æ·˜æ±°é”™è¯¯çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬æ„å»ºäº†åŸºäºNumiaMathçš„ourdatasetå¤§å‹æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ç”±gemimi-1.5-flashæ ‡æ³¨çš„44.3ä¸‡ä¸ªé…å¯¹æ¯”è¾ƒç»“æœï¼Œå¹¶é€šè¿‡ç›‘ç£å¾®è°ƒè®­ç»ƒé…å¯¹RMã€‚åœ¨MATH-500å’Œå¥¥æ—åŒ¹äºšæ¿ä¸Šçš„å®éªŒè¯æ˜ï¼Œä¸ä¼ ç»Ÿåˆ¤åˆ«å¥–åŠ±æ¨¡å‹ç›¸æ¯”ï¼Œé…å¯¹RMå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„å‰50%é—®é¢˜ä¸Šå–å¾—äº†ç›¸å¯¹æ”¹å–„å¹…åº¦ä¸º40%~60%ã€‚â€</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13007v1">PDF</a> in progress work</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæœ€ä½³Nï¼ˆBoNï¼‰é‡‡æ ·çš„ç­–ç•¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æµ‹è¯•æ—¶çš„ç¼©æ”¾æ–¹æ³•å¸¸ä½¿ç”¨å¥–åŠ±æ¨¡å‹é€‰æ‹©æœ€ä½³çš„å€™é€‰è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹å¸¸å¸¸åˆ†é…ä»»æ„ä¸”ä¸ä¸€è‡´çš„åˆ†æ•°ï¼Œé™åˆ¶äº†å…¶æ•ˆæœã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºæˆå¯¹å¥–åŠ±æ¨¡å‹ï¼ˆPairwise RMï¼‰ä¸æ·˜æ±°åˆ¶é”¦æ ‡èµ›ç»“åˆçš„BoNé‡‡æ ·ç­–ç•¥ã€‚æˆå¯¹å¥–åŠ±æ¨¡å‹æ— éœ€åˆ†é…ç»å¯¹åˆ†æ•°ï¼Œè€Œæ˜¯åŒæ—¶è¯„ä¼°ä¸¤ä¸ªå€™é€‰è§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§ã€‚è¿™ç§æ–¹æ³•æ¶ˆé™¤äº†å¯¹ä»»æ„è¯„åˆ†çš„éœ€æ±‚ï¼Œå¹¶é€šè¿‡å¹¶è¡Œæ¯”è¾ƒå®ç°äº†è§£å†³æ–¹æ¡ˆçš„äº¤å‰éªŒè¯ã€‚åœ¨æ·˜æ±°é”¦æ ‡èµ›ä¸­ï¼Œæˆå¯¹å¥–åŠ±æ¨¡å‹è¿›è¡Œå€™é€‰è§£å†³æ–¹æ¡ˆä¹‹é—´çš„æˆå¯¹æ¯”è¾ƒï¼Œå¹¶é€šè¿‡è¿­ä»£æ¶ˆé™¤é”™è¯¯çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡æ„å»ºäº†åŸºäºNumiaMathçš„å¤§è§„æ¨¡æ•°æ®é›†ourdatasetï¼ŒåŒ…å«44.3ä¸‡ä¸ªæˆå¯¹æ¯”è¾ƒï¼Œå¹¶ä½¿ç”¨gemimi-1.5-flashè¿›è¡Œæ ‡æ³¨ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒè®­ç»ƒæˆå¯¹å¥–åŠ±æ¨¡å‹ã€‚åœ¨MATH-500å’ŒOlympiad Benchä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿåˆ¤åˆ«å¥–åŠ±æ¨¡å‹ç›¸æ¯”æœ‰æ˜¾è‘—æ”¹è¿›ã€‚åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„å‰50%çš„é—®é¢˜ä¸Šå®ç°äº†40%è‡³60%çš„ç›¸å¯¹æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹åœ¨LLMçš„BoNé‡‡æ ·ä¸­å­˜åœ¨ä»»æ„å’Œä¸ä¸€è‡´çš„è¯„åˆ†é—®é¢˜ã€‚</li>
<li>æå‡ºäº†åŸºäºæˆå¯¹å¥–åŠ±æ¨¡å‹ï¼ˆPairwise RMï¼‰çš„BoNé‡‡æ ·ç­–ç•¥ï¼Œé€šè¿‡å¹¶è¡Œæ¯”è¾ƒè¯„ä¼°å€™é€‰è§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§ã€‚</li>
<li>æˆå¯¹å¥–åŠ±æ¨¡å‹æ¶ˆé™¤äº†å¯¹ä»»æ„è¯„åˆ†çš„éœ€æ±‚ï¼Œå¹¶å®ç°äº†è§£å†³æ–¹æ¡ˆçš„äº¤å‰éªŒè¯ã€‚</li>
<li>é‡‡ç”¨äº†æ·˜æ±°åˆ¶é”¦æ ‡èµ›ç­–ç•¥ï¼Œé€šè¿‡è¿­ä»£æ¶ˆé™¤é”™è¯¯çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ„å»ºäº†åŸºäºNumiaMathçš„å¤§è§„æ¨¡æ•°æ®é›†ourdatasetï¼Œç”¨äºè®­ç»ƒæˆå¯¹å¥–åŠ±æ¨¡å‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹ç›¸æ¯”ï¼Œæˆå¯¹å¥–åŠ±æ¨¡å‹åœ¨MATH-500å’ŒOlympiad Benchä¸Šè¡¨ç°æ›´ä¼˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13007">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.13007v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.13007v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.13007v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.13007v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.13007v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LLM4WM-Adapting-LLM-for-Wireless-Multi-Tasking"><a href="#LLM4WM-Adapting-LLM-for-Wireless-Multi-Tasking" class="headerlink" title="LLM4WM: Adapting LLM for Wireless Multi-Tasking"></a>LLM4WM: Adapting LLM for Wireless Multi-Tasking</h2><p><strong>Authors:Xuanyu Liu, Shijian Gao, Boxun Liu, Xiang Cheng, Liuqing Yang</strong></p>
<p>The wireless channel is fundamental to communication, encompassing numerous tasks collectively referred to as channel-associated tasks. These tasks can leverage joint learning based on channel characteristics to share representations and enhance system design. To capitalize on this advantage, LLM4WM is proposedâ€“a large language model (LLM) multi-task fine-tuning framework specifically tailored for channel-associated tasks. This framework utilizes a Mixture of Experts with Low-Rank Adaptation (MoE-LoRA) approach for multi-task fine-tuning, enabling the transfer of the pre-trained LLMâ€™s general knowledge to these tasks. Given the unique characteristics of wireless channel data, preprocessing modules, adapter modules, and multi-task output layers are designed to align the channel data with the LLMâ€™s semantic feature space. Experiments on a channel-associated multi-task dataset demonstrate that LLM4WM outperforms existing methodologies in both full-sample and few-shot evaluations, owing to its robust multi-task joint modeling and transfer learning capabilities. </p>
<blockquote>
<p>æ— çº¿ä¿¡é“æ˜¯é€šä¿¡çš„åŸºç¡€ï¼Œæ¶µç›–äº†è®¸å¤šç»Ÿç§°ä¸ºä¿¡é“ç›¸å…³ä»»åŠ¡çš„å·¥ä½œã€‚è¿™äº›ä»»åŠ¡å¯ä»¥åˆ©ç”¨åŸºäºä¿¡é“ç‰¹æ€§çš„è”åˆå­¦ä¹ æ¥å…±äº«è¡¨ç¤ºå¹¶å¢å¼ºç³»ç»Ÿè®¾è®¡ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨è¿™ä¸€ä¼˜åŠ¿ï¼Œæå‡ºäº†LLM4WMâ€”â€”ä¸€ç§é’ˆå¯¹ä¿¡é“ç›¸å…³ä»»åŠ¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤šä»»åŠ¡å¾®è°ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä½é˜¶é€‚åº”çš„ä¸“å®¶æ··åˆï¼ˆMoE-LoRAï¼‰æ–¹æ³•è¿›è¡Œå¤šä»»åŠ¡å¾®è°ƒï¼Œä½¿é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„é€šç”¨çŸ¥è¯†èƒ½å¤Ÿåº”ç”¨äºè¿™äº›ä»»åŠ¡ã€‚é‰´äºæ— çº¿ä¿¡é“æ•°æ®çš„ç‹¬ç‰¹ç‰¹æ€§ï¼Œè®¾è®¡äº†é¢„å¤„ç†æ¨¡å—ã€é€‚é…å™¨æ¨¡å—å’Œå¤šä»»åŠ¡è¾“å‡ºå±‚ï¼Œä»¥ä½¿ä¿¡é“æ•°æ®ä¸LLMçš„è¯­ä¹‰ç‰¹å¾ç©ºé—´å¯¹é½ã€‚åœ¨ä¿¡é“ç›¸å…³å¤šä»»åŠ¡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLLM4WMåœ¨å®Œå…¨æ ·æœ¬å’Œå°‘é‡æ ·æœ¬è¯„ä¼°ä¸­éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¿™å¾—ç›Šäºå…¶å¼ºå¤§çš„å¤šä»»åŠ¡è”åˆå»ºæ¨¡å’Œè¿ç§»å­¦ä¹ èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12983v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ— çº¿ä¿¡é“é€šä¿¡ä¸­çš„å…³é”®è¦ç´ ä¹‹ä¸€æ˜¯æ— çº¿ä¿¡é“ï¼Œå…¶ä¸­åŒ…å«è®¸å¤šç§°ä¸ºä¿¡é“ç›¸å…³ä»»åŠ¡çš„å·¥ä½œã€‚è¿™äº›ä»»åŠ¡å¯ä»¥åˆ©ç”¨åŸºäºä¿¡é“ç‰¹æ€§çš„è”åˆå­¦ä¹ æ¥å…±äº«è¡¨ç¤ºå¹¶æ”¹è¿›ç³»ç»Ÿè®¾è®¡ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨è¿™ä¸€ä¼˜åŠ¿ï¼Œæå‡ºäº†LLM4WMæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ä¿¡é“ç›¸å…³ä»»åŠ¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤šä»»åŠ¡å¾®è°ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŸºäºä½ç§©é€‚åº”ï¼ˆMoE-LoRAï¼‰çš„ä¸“å®¶æ··åˆï¼ˆMixture of Expertsï¼‰æ–¹æ³•è¿›è¡Œå¤šä»»åŠ¡å¾®è°ƒï¼Œä½¿é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„é€šç”¨çŸ¥è¯†èƒ½å¤Ÿè½¬ç§»åˆ°è¿™äº›ä»»åŠ¡ä¸Šã€‚é’ˆå¯¹æ— çº¿ä¿¡é“æ•°æ®çš„ç‹¬ç‰¹ç‰¹ç‚¹ï¼Œè®¾è®¡äº†é¢„å¤„ç†æ¨¡å—ã€é€‚é…å™¨æ¨¡å—å’Œå¤šä»»åŠ¡è¾“å‡ºå±‚ï¼Œä»¥ä½¿ä¿¡é“æ•°æ®ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ç‰¹å¾ç©ºé—´å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ä¿¡é“ç›¸å…³çš„å¤šä»»åŠ¡æ•°æ®é›†ä¸Šï¼ŒLLM4WMåœ¨å®Œæ•´æ ·æœ¬å’Œå°‘é‡æ ·æœ¬è¯„ä¼°ä¸­éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¿™å½’åŠŸäºå…¶å¼ºå¤§çš„å¤šä»»åŠ¡è”åˆå»ºæ¨¡å’Œè¿ç§»å­¦ä¹ èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ— çº¿ä¿¡é“é€šä¿¡ä¸­æ¶‰åŠè®¸å¤šä¿¡é“ç›¸å…³ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡å¯ä»¥é€šè¿‡è”åˆå­¦ä¹ æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>LLM4WMæ¡†æ¶æ˜¯ä¸€ç§é’ˆå¯¹ä¿¡é“ç›¸å…³ä»»åŠ¡çš„å¤§å‹è¯­è¨€æ¨¡å‹å¤šä»»åŠ¡å¾®è°ƒæ¡†æ¶ã€‚</li>
<li>LLM4WMåˆ©ç”¨MoE-LoRAæ–¹æ³•è¿›è¡Œå¤šä»»åŠ¡å¾®è°ƒï¼Œå®ç°é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„é€šç”¨çŸ¥è¯†è½¬ç§»ã€‚</li>
<li>é’ˆå¯¹æ— çº¿ä¿¡é“æ•°æ®çš„ç‰¹æ€§ï¼ŒLLM4WMè®¾è®¡äº†é¢„å¤„ç†ã€é€‚é…å’Œå¤šä»»åŠ¡è¾“å‡ºå±‚ã€‚</li>
<li>å®éªŒè¯æ˜LLM4WMåœ¨ä¿¡é“ç›¸å…³å¤šä»»åŠ¡æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>LLM4WMçš„å…¨æ ·æœ¬å’Œå°‘é‡æ ·æœ¬è¯„ä¼°è¡¨ç°å‡å¾ˆå‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12983">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12983v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12983v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12983v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12983v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12983v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12983v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="FlanEC-Exploring-Flan-T5-for-Post-ASR-Error-Correction"><a href="#FlanEC-Exploring-Flan-T5-for-Post-ASR-Error-Correction" class="headerlink" title="FlanEC: Exploring Flan-T5 for Post-ASR Error Correction"></a>FlanEC: Exploring Flan-T5 for Post-ASR Error Correction</h2><p><strong>Authors:Moreno La Quatra, Valerio Mario Salerno, Yu Tsao, Sabato Marco Siniscalchi</strong></p>
<p>In this paper, we present an encoder-decoder model leveraging Flan-T5 for post-Automatic Speech Recognition (ASR) Generative Speech Error Correction (GenSEC), and we refer to it as FlanEC. We explore its application within the GenSEC framework to enhance ASR outputs by mapping n-best hypotheses into a single output sentence. By utilizing n-best lists from ASR models, we aim to improve the linguistic correctness, accuracy, and grammaticality of final ASR transcriptions. Specifically, we investigate whether scaling the training data and incorporating diverse datasets can lead to significant improvements in post-ASR error correction. We evaluate FlanEC using the HyPoradise dataset, providing a comprehensive analysis of the modelâ€™s effectiveness in this domain. Furthermore, we assess the proposed approach under different settings to evaluate model scalability and efficiency, offering valuable insights into the potential of instruction-tuned encoder-decoder models for this task. </p>
<blockquote>
<p>æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨Flan-T5è¿›è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åçš„ç”Ÿæˆå¼è¯­éŸ³é”™è¯¯æ ¡æ­£ï¼ˆGenSECï¼‰çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸ºFlanECã€‚æˆ‘ä»¬æ¢ç´¢äº†å…¶åœ¨GenSECæ¡†æ¶å†…çš„åº”ç”¨ï¼Œé€šè¿‡å°†n-bestå‡è®¾æ˜ å°„åˆ°å•ä¸ªè¾“å‡ºå¥å­æ¥å¢å¼ºASRè¾“å‡ºã€‚é€šè¿‡åˆ©ç”¨ASRæ¨¡å‹çš„n-beståˆ—è¡¨ï¼Œæˆ‘ä»¬æ—¨åœ¨æé«˜æœ€ç»ˆASRè½¬å½•çš„è¯­è¨€æ­£ç¡®æ€§ã€å‡†ç¡®æ€§å’Œè¯­æ³•æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ‰©å¤§è®­ç»ƒæ•°æ®å¹¶èå…¥å¤šç§æ•°æ®é›†æ˜¯å¦èƒ½åœ¨ASRä¹‹åçš„é”™è¯¯æ ¡æ­£æ–¹é¢å¸¦æ¥æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬ä½¿ç”¨HyPoradiseæ•°æ®é›†å¯¹FlanECè¿›è¡Œäº†è¯„ä¼°ï¼Œå…¨é¢åˆ†æäº†è¯¥æ¨¡å‹åœ¨æ­¤é¢†åŸŸçš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ä¸åŒçš„è®¾ç½®ä¸‹è¯„ä¼°äº†æ‰€æå‡ºçš„æ–¹æ³•ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡ï¼Œä¸ºæŒ‡ä»¤å¾®è°ƒç¼–ç å™¨-è§£ç å™¨æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸Šçš„æ½œåŠ›æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12979v1">PDF</a> Accepted at the 2024 IEEE Workshop on Spoken Language Technology   (SLT) - GenSEC Challenge</p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨Flan-T5çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¹‹åçš„ç”Ÿæˆå¼è¯­éŸ³é”™è¯¯æ ¡æ­£ï¼ˆGenSECï¼‰ï¼Œç§°ä¸ºFlanECã€‚æ–‡ç« æ¢è®¨äº†å…¶åœ¨GenSECæ¡†æ¶ä¸‹çš„åº”ç”¨ï¼Œæ—¨åœ¨é€šè¿‡å°†ASRæ¨¡å‹çš„n-bestå‡è®¾æ˜ å°„æˆå•ä¸ªè¾“å‡ºå¥å­æ¥æé«˜ASRè¾“å‡ºçš„è¯­è¨€æ­£ç¡®æ€§ã€å‡†ç¡®æ€§å’Œè¯­æ³•æ€§ã€‚æ–‡ç« è¿˜ç ”ç©¶äº†æ‰©å¤§è®­ç»ƒæ•°æ®å’Œå¼•å…¥å¤šæ ·æ•°æ®é›†å¯¹æ”¹å–„ASRé”™è¯¯æ ¡æ­£çš„æ½œåŠ›ï¼Œå¹¶é€šè¿‡HyPoradiseæ•°æ®é›†å¯¹FlanECè¿›è¡Œäº†è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨Flan-T5çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹FlanECï¼Œç”¨äºåè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„ç”Ÿæˆå¼è¯­éŸ³é”™è¯¯æ ¡æ­£ï¼ˆGenSECï¼‰ã€‚</li>
<li>FlanECæ—¨åœ¨é€šè¿‡å°†ASRæ¨¡å‹çš„n-bestå‡è®¾æ˜ å°„æˆå•ä¸ªè¾“å‡ºå¥å­ï¼Œæé«˜ASRè¾“å‡ºçš„è¯­è¨€æ­£ç¡®æ€§ã€å‡†ç¡®æ€§å’Œè¯­æ³•æ€§ã€‚</li>
<li>æ‰©å¤§è®­ç»ƒæ•°æ®å’Œå¼•å…¥å¤šæ ·æ•°æ®é›†å¯¹æ”¹å–„ASRé”™è¯¯æ ¡æ­£æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>é€šè¿‡HyPoradiseæ•°æ®é›†å¯¹FlanECè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>æ–‡ç« è¿˜æ¢è®¨äº†æ¨¡å‹çš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡ï¼Œä¸ºæŒ‡ä»¤è°ƒä¼˜çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸Šçš„æ½œåŠ›æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæ”¹å–„ASRç³»ç»Ÿçš„æ€§èƒ½æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12979v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12979v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12979v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Accessible-Smart-Contracts-Verification-Synthesizing-Formal-Models-with-Tamed-LLMs"><a href="#Accessible-Smart-Contracts-Verification-Synthesizing-Formal-Models-with-Tamed-LLMs" class="headerlink" title="Accessible Smart Contracts Verification: Synthesizing Formal Models with   Tamed LLMs"></a>Accessible Smart Contracts Verification: Synthesizing Formal Models with   Tamed LLMs</h2><p><strong>Authors:Jan Corazza, Ivan Gavran, Gabriela Moreira, Daniel Neider</strong></p>
<p>When blockchain systems are said to be trustless, what this really means is that all the trust is put into software. Thus, there are strong incentives to ensure blockchain software is correct â€“ vulnerabilities here cost millions and break businesses. One of the most powerful ways of establishing software correctness is by using formal methods. Approaches based on formal methods, however, induce a significant overhead in terms of time and expertise required to successfully employ them. Our work addresses this critical disadvantage by automating the creation of a formal model â€“ a mathematical abstraction of the software system â€“ which is often a core task when employing formal methods. We perform model synthesis in three phases: we first transpile the code into model stubs; then we â€œfill in the blanksâ€ using a large language model (LLM); finally, we iteratively repair the generated model, on both syntactical and semantical level. In this way, we significantly reduce the amount of time necessary to create formal models and increase accessibility of valuable software verification methods that rely on them. The practical context of our work was reducing the time-to-value of using formal models for correctness audits of smart contracts. </p>
<blockquote>
<p>å½“æåˆ°åŒºå—é“¾ç³»ç»Ÿæ˜¯æ— ä¿¡ä»»çš„æ—¶ï¼ŒçœŸæ­£çš„æ„æ€æ˜¯æ‰€æœ‰çš„ä¿¡ä»»éƒ½æ”¾åœ¨äº†è½¯ä»¶ä¸Šã€‚å› æ­¤ï¼Œç¡®ä¿åŒºå—é“¾è½¯ä»¶æ­£ç¡®æ€§çš„åŠ¨åŠ›éå¸¸å¼ºçƒˆâ€”â€”è¿™é‡Œçš„æ¼æ´ä¼šé€ æˆæ•°ç™¾ä¸‡çš„æŸå¤±å¹¶ç ´åä¸šåŠ¡ã€‚å»ºç«‹è½¯ä»¶æ­£ç¡®æ€§çš„æœ€å¼ºå¤§æ–¹å¼ä¹‹ä¸€æ˜¯ä½¿ç”¨å½¢å¼åŒ–æ–¹æ³•ã€‚ç„¶è€Œï¼ŒåŸºäºå½¢å¼åŒ–æ–¹æ³•çš„æ–¹æ¡ˆéœ€è¦æ¶ˆè€—å¤§é‡çš„æ—¶é—´å’Œä¸“ä¸šçŸ¥è¯†æ‰èƒ½æˆåŠŸåº”ç”¨ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡è‡ªåŠ¨åŒ–åˆ›å»ºå½¢å¼æ¨¡å‹æ¥è§£å†³è¿™ä¸€å…³é”®åŠ£åŠ¿ï¼Œå½¢å¼æ¨¡å‹æ˜¯è½¯ä»¶ç³»ç»Ÿçš„æ•°å­¦æŠ½è±¡ï¼Œåœ¨ä½¿ç”¨å½¢å¼åŒ–æ–¹æ³•æ—¶é€šå¸¸æ˜¯ä¸€é¡¹æ ¸å¿ƒä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ¨¡å‹åˆæˆåˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å°†ä»£ç è½¬æ¢ä¸ºæ¨¡å‹æ¡†æ¶ï¼›ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥â€œå¡«è¡¥ç©ºç™½â€ï¼›æœ€åï¼Œæˆ‘ä»¬åœ¨è¯­æ³•å’Œè¯­ä¹‰å±‚é¢ä¸Šè¿­ä»£ä¿®å¤ç”Ÿæˆçš„æ¨¡å‹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬æ˜¾è‘—å‡å°‘äº†åˆ›å»ºå½¢å¼æ¨¡å‹æ‰€éœ€çš„æ—¶é—´ï¼Œæé«˜äº†ä¾èµ–äºå®ƒä»¬çš„å®è´µè½¯ä»¶éªŒè¯æ–¹æ³•çš„å¯åŠæ€§ã€‚æˆ‘ä»¬å·¥ä½œçš„å®é™…èƒŒæ™¯æ˜¯å‡å°‘ä½¿ç”¨å½¢å¼æ¨¡å‹å¯¹æ™ºèƒ½åˆçº¦è¿›è¡Œæ­£ç¡®æ€§å®¡è®¡çš„æ—¶é—´æˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12972v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒºå—é“¾ç³»ç»Ÿçš„â€œæ— éœ€ä¿¡ä»»â€ç‰¹æ€§æ„å‘³ç€ä¿¡ä»»ä¸»è¦æ”¾åœ¨è½¯ä»¶ä¸Šï¼Œå› æ­¤ç¡®ä¿åŒºå—é“¾è½¯ä»¶çš„æ­£ç¡®æ€§è‡³å…³é‡è¦ã€‚å½¢å¼åŒ–æ–¹æ³•æ˜¯éªŒè¯è½¯ä»¶æ­£ç¡®æ€§çš„å¼ºå¤§å·¥å…·ï¼Œä½†ä½¿ç”¨å®ƒä»¬éœ€è¦å¤§é‡çš„æ—¶é—´å’Œä¸“ä¸šçŸ¥è¯†ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡è‡ªåŠ¨åŒ–åˆ›å»ºå½¢å¼åŒ–æ¨¡å‹æ¥è§£å†³è¿™ä¸€åŠ£åŠ¿ï¼Œè¯¥æ¨¡å‹æ˜¯è½¯ä»¶ç³»ç»Ÿçš„æ•°å­¦æŠ½è±¡ã€‚æˆ‘ä»¬è¿›è¡Œæ¨¡å‹ç»¼åˆåˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼šé¦–å…ˆï¼Œå°†ä»£ç ç¼–è¯‘æˆæ¨¡å‹å­˜æ ¹ï¼›ç„¶åï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¡«å……ç©ºç™½ï¼›æœ€åï¼Œè¿­ä»£ä¿®å¤ç”Ÿæˆçš„æ¨¡å‹ï¼ŒåŒ…æ‹¬è¯­æ³•å’Œè¯­ä¹‰å±‚é¢ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬æ˜¾è‘—å‡å°‘äº†åˆ›å»ºå½¢å¼åŒ–æ¨¡å‹æ‰€éœ€çš„æ—¶é—´ï¼Œæé«˜äº†ä¾èµ–äºå®ƒä»¬çš„å®è´µè½¯ä»¶éªŒè¯æ–¹æ³•çš„å¯åŠæ€§ã€‚æˆ‘ä»¬å·¥ä½œçš„å®é™…èƒŒæ™¯æ˜¯å‡å°‘å½¢å¼åŒ–æ¨¡å‹åœ¨æ™ºèƒ½åˆçº¦æ­£ç¡®æ€§å®¡è®¡ä¸­ä½¿ç”¨çš„æ—¶é—´æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒºå—é“¾ç³»ç»Ÿçš„â€œä¿¡ä»»â€ä¸»è¦å¯„æ‰˜äºè½¯ä»¶æ­£ç¡®æ€§ã€‚</li>
<li>å½¢å¼åŒ–æ–¹æ³•æ˜¯éªŒè¯è½¯ä»¶æ­£ç¡®æ€§çš„æœ‰æ•ˆæ‰‹æ®µï¼Œä½†å®æ–½è¿‡ç¨‹å¤æ‚ï¼Œéœ€è¦è¾ƒå¤šæ—¶é—´å’Œä¸“ä¸šçŸ¥è¯†ã€‚</li>
<li>æˆ‘ä»¬é€šè¿‡è‡ªåŠ¨åŒ–åˆ›å»ºå½¢å¼åŒ–æ¨¡å‹æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¿™ä¸€æ¨¡å‹æ˜¯è½¯ä»¶ç³»ç»Ÿçš„æ•°å­¦æŠ½è±¡ã€‚</li>
<li>æ¨¡å‹ç»¼åˆåˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šç¼–è¯‘ä»£ç æˆæ¨¡å‹å­˜æ ¹ã€ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¡«å……ç©ºç™½ã€è¿­ä»£ä¿®å¤æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†åˆ›å»ºå½¢å¼åŒ–æ¨¡å‹çš„æ—¶é—´ï¼Œæé«˜äº†è½¯ä»¶éªŒè¯æ–¹æ³•çš„å¯åŠæ€§ã€‚</li>
<li>è¯¥ç ”ç©¶å®é™…åº”ç”¨åœ¨æ™ºèƒ½åˆçº¦æ­£ç¡®æ€§å®¡è®¡ä¸­ï¼Œæ—¨åœ¨é™ä½æ—¶é—´æˆæœ¬ã€‚</li>
<li>ä¿éšœåŒºå—é“¾è½¯ä»¶æ­£ç¡®æ€§å¯¹äºç¡®ä¿æ•´ä¸ªåŒºå—é“¾ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œå®‰å…¨æ€§è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12972">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12972v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12972v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Efficient-Prompt-Compression-with-Evaluator-Heads-for-Long-Context-Transformer-Inference"><a href="#Efficient-Prompt-Compression-with-Evaluator-Heads-for-Long-Context-Transformer-Inference" class="headerlink" title="Efficient Prompt Compression with Evaluator Heads for Long-Context   Transformer Inference"></a>Efficient Prompt Compression with Evaluator Heads for Long-Context   Transformer Inference</h2><p><strong>Authors:Weizhi Fei, Xueyan Niu, Guoqing Xie, Yingqing Liu, Bo Bai, Wei Han</strong></p>
<p>Although applications involving long-context inputs are crucial for the effective utilization of large language models (LLMs), they also result in increased computational costs and reduced performance. To address this challenge, we propose an efficient, training-free prompt compression method that retains key information within compressed prompts. We identify specific attention heads in transformer-based LLMs, which we designate as evaluator heads, that are capable of selecting tokens in long inputs that are most significant for inference. Building on this discovery, we develop EHPC, an Evaluator Head-based Prompt Compression method, which enables LLMs to rapidly â€œskim throughâ€ input prompts by leveraging only the first few layers with evaluator heads during the pre-filling stage, subsequently passing only the important tokens to the model for inference. EHPC achieves state-of-the-art results across two mainstream benchmarks: prompt compression and long-context inference acceleration. Consequently, it effectively reduces the complexity and costs associated with commercial API calls. We further demonstrate that EHPC attains competitive results compared to key-value cache-based acceleration methods, thereby highlighting its potential to enhance the efficiency of LLMs for long-context tasks. </p>
<blockquote>
<p>è™½ç„¶æ¶‰åŠé•¿ä¸Šä¸‹æ–‡è¾“å…¥çš„åº”ç”¨å¯¹äºæœ‰æ•ˆåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡³å…³é‡è¦ï¼Œä½†å®ƒä»¬ä¹Ÿä¼šå¯¼è‡´è®¡ç®—æˆæœ¬å¢åŠ å’Œæ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆã€æ— éœ€è®­ç»ƒæç¤ºå‹ç¼©æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å‹ç¼©æç¤ºä¸­ä¿ç•™å…³é”®ä¿¡æ¯ã€‚æˆ‘ä»¬ç¡®å®šäº†åŸºäºtransformerçš„LLMä¸­çš„ç‰¹å®šæ³¨æ„åŠ›å¤´ï¼Œå°†å…¶æŒ‡å®šä¸ºè¯„ä¼°å¤´ï¼Œè¿™äº›è¯„ä¼°å¤´èƒ½å¤Ÿé€‰æ‹©é•¿è¾“å…¥ä¸­ç”¨äºæ¨æ–­çš„æœ€é‡è¦æ ‡è®°ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬å¼€å‘äº†EHPCï¼Œä¸€ç§åŸºäºè¯„ä¼°å¤´çš„æç¤ºå‹ç¼©æ–¹æ³•ï¼Œå®ƒä½¿LLMèƒ½å¤Ÿåˆ©ç”¨é¢„å¡«å……é˜¶æ®µåªæœ‰å‰å‡ å±‚çš„è¯„ä¼°å¤´å¿«é€Ÿâ€œæµè§ˆâ€è¾“å…¥æç¤ºï¼Œéšååªå°†é‡è¦æ ‡è®°ä¼ é€’ç»™æ¨¡å‹è¿›è¡Œæ¨æ–­ã€‚EHPCåœ¨ä¸¤ä¸ªä¸»æµåŸºå‡†æµ‹è¯•ï¼šæç¤ºå‹ç¼©å’Œé•¿ä¸Šä¸‹æ–‡æ¨ç†åŠ é€Ÿä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœã€‚å› æ­¤ï¼Œå®ƒæœ‰æ•ˆåœ°é™ä½äº†ä¸å•†ä¸šAPIè°ƒç”¨ç›¸å…³çš„å¤æ‚æ€§å’Œæˆæœ¬ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼Œä¸åŸºäºé”®å€¼ç¼“å­˜çš„åŠ é€Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒEHPCå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œä»è€Œçªå‡ºäº†å…¶åœ¨æé«˜LLMå¯¹é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡çš„æ•ˆç‡æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12959v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡è¾“å…¥æ—¶é¢ä¸´è®¡ç®—æˆæœ¬å¢åŠ å’Œæ€§èƒ½ä¸‹é™çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è®­ç»ƒå…è´¹æç¤ºå‹ç¼©æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å‹ç¼©æç¤ºçš„åŒæ—¶ä¿ç•™å…³é”®ä¿¡æ¯ã€‚æˆ‘ä»¬ç¡®å®šäº†åŸºäºè½¬æ¢å™¨çš„LLMä¸­çš„ç‰¹å®šæ³¨æ„åŠ›å¤´ï¼Œç§°ä¸ºè¯„ä¼°å¤´ï¼Œè¿™äº›å¤´èƒ½å¤Ÿé€‰æ‹©é•¿è¾“å…¥ä¸­å¯¹æ¨ç†æœ€é‡è¦çš„æ ‡è®°ã€‚åŸºäºæ­¤å‘ç°ï¼Œæˆ‘ä»¬å¼€å‘äº†EHPCï¼ˆåŸºäºè¯„ä¼°å¤´çš„æç¤ºå‹ç¼©æ–¹æ³•ï¼‰ï¼Œä½¿LLMèƒ½å¤Ÿè¿…é€Ÿæµè§ˆè¾“å…¥æç¤ºï¼Œåœ¨é¢„å¡«å……é˜¶æ®µä»…åˆ©ç”¨å‰å‡ å±‚çš„è¯„ä¼°å¤´ï¼Œå¹¶å°†é‡è¦æ ‡è®°ä¼ é€’ç»™æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚EHPCåœ¨ä¸¤ä¸ªä¸»æµåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„ç»“æœï¼šæç¤ºå‹ç¼©å’Œé•¿ä¸Šä¸‹æ–‡æ¨ç†åŠ é€Ÿã€‚å› æ­¤ï¼Œå®ƒæœ‰æ•ˆåœ°é™ä½äº†ä¸å•†ä¸šAPIè°ƒç”¨ç›¸å…³çš„å¤æ‚æ€§å’Œæˆæœ¬ã€‚EHPCä¸åŸºäºé”®å€¼ç¼“å­˜çš„åŠ é€Ÿæ–¹æ³•ç›¸æ¯”ä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡è¾“å…¥æ—¶é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è®¡ç®—æˆæœ¬å¢åŠ å’Œæ€§èƒ½ä¸‹é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è®­ç»ƒå…è´¹çš„æç¤ºå‹ç¼©æ–¹æ³•EHPCï¼Œèƒ½æœ‰æ•ˆå‹ç¼©é•¿è¾“å…¥å¹¶ä¿æŒå…³é”®ä¿¡æ¯ã€‚</li>
<li>EHPCé€šè¿‡è¯†åˆ«å¹¶ä»…åˆ©ç”¨ç‰¹å®šçš„è¯„ä¼°å¤´è¿›è¡Œé•¿è¾“å…¥çš„ç­›é€‰ï¼Œä»è€Œåœ¨é¢„å¡«å……é˜¶æ®µå¿«é€Ÿæµè§ˆè¾“å…¥æç¤ºã€‚</li>
<li>EHPCå®ç°äº†åœ¨æç¤ºå‹ç¼©å’Œé•¿ä¸Šä¸‹æ–‡æ¨ç†åŠ é€Ÿæ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚</li>
<li>EHPCèƒ½æ˜¾è‘—é™ä½å•†ä¸šAPIè°ƒç”¨çš„å¤æ‚æ€§å¹¶å‡å°‘æˆæœ¬ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12959v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12959v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12959v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12959v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="GANQ-GPU-Adaptive-Non-Uniform-Quantization-for-Large-Language-Models"><a href="#GANQ-GPU-Adaptive-Non-Uniform-Quantization-for-Large-Language-Models" class="headerlink" title="GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models"></a>GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models</h2><p><strong>Authors:Pengxiang Zhao, Xiaoming Yuan</strong></p>
<p>Large Language Models (LLMs) face significant deployment challenges due to their substantial resource requirements. While low-bit quantized weights can reduce memory usage and improve inference efficiency, current hardware lacks native support for mixed-precision General Matrix Multiplication (mpGEMM), resulting in inefficient dequantization-based implementations. Moreover, uniform quantization methods often fail to capture weight distributions adequately, leading to performance degradation. We propose GANQ (GPU-Adaptive Non-Uniform Quantization), a layer-wise post-training non-uniform quantization framework optimized for hardware-efficient lookup table-based mpGEMM. GANQ achieves superior quantization performance by utilizing a training-free, GPU-adaptive optimization algorithm to efficiently reduce layer-wise quantization errors. Extensive experiments demonstrate GANQâ€™s ability to reduce the perplexity gap from the FP16 baseline compared to state-of-the-art methods for both 3-bit and 4-bit quantization. Furthermore, when deployed on a single NVIDIA RTX 4090 GPU, GANQâ€™s quantized models achieve up to 2.57$\times$ speedup over the baseline, advancing memory and inference efficiency in LLM deployment. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”±äºå…¶å·¨å¤§çš„èµ„æºéœ€æ±‚è€Œé¢ä¸´é‡å¤§çš„éƒ¨ç½²æŒ‘æˆ˜ã€‚è™½ç„¶ä½ä½é‡åŒ–æƒé‡å¯ä»¥å‡å°‘å†…å­˜ä½¿ç”¨å¹¶æé«˜æ¨ç†æ•ˆç‡ï¼Œä½†å½“å‰ç¡¬ä»¶ä¸æ”¯æŒæ··åˆç²¾åº¦é€šç”¨çŸ©é˜µä¹˜æ³•ï¼ˆmpGEMMï¼‰ï¼Œå¯¼è‡´åŸºäºåé‡åŒ–çš„å®ç°æ•ˆç‡ä½ä¸‹ã€‚æ­¤å¤–ï¼Œç»Ÿä¸€é‡åŒ–æ–¹æ³•å¾€å¾€ä¸è¶³ä»¥æ•æ‰æƒé‡åˆ†å¸ƒï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬æå‡ºäº†GANQï¼ˆGPUè‡ªé€‚åº”éå‡åŒ€é‡åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹åŸºäºç¡¬ä»¶é«˜æ•ˆæŸ¥æ‰¾è¡¨çš„mpGEMMä¼˜åŒ–çš„é€å±‚åè®­ç»ƒéå‡åŒ€é‡åŒ–æ¡†æ¶ã€‚GANQé€šè¿‡åˆ©ç”¨æ— è®­ç»ƒã€GPUè‡ªé€‚åº”çš„ä¼˜åŒ–ç®—æ³•ï¼Œæœ‰æ•ˆåœ°å‡å°‘äº†é€å±‚é‡åŒ–è¯¯å·®ï¼Œå®ç°äº†å‡ºè‰²çš„é‡åŒ–æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼ŒGANQåœ¨3ä½å’Œ4ä½é‡åŒ–æ–¹é¢èƒ½å¤Ÿå‡å°‘å›°æƒ‘åº¦å·®è·ï¼Œä¸FP16åŸºå‡†ç›¸æ¯”è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œåœ¨å•ä¸ªNVIDIA RTX 4090 GPUä¸Šéƒ¨ç½²æ—¶ï¼ŒGANQçš„é‡åŒ–æ¨¡å‹å®ç°äº†é«˜è¾¾2.57å€äºåŸºå‡†çš„åŠ é€Ÿï¼Œæé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹éƒ¨ç½²ä¸­çš„å†…å­˜å’Œæ¨ç†æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12956v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éƒ¨ç½²é¢ä¸´èµ„æºéœ€æ±‚å¤§çš„æŒ‘æˆ˜ã€‚é‡‡ç”¨ä½æ¯”ç‰¹é‡åŒ–æƒé‡å¯å‡å°‘å†…å­˜ä½¿ç”¨å¹¶æé«˜æ¨ç†æ•ˆç‡ï¼Œä½†å½“å‰ç¡¬ä»¶ä¸æ”¯æŒæ··åˆç²¾åº¦é€šç”¨çŸ©é˜µä¹˜æ³•ï¼ˆmpGEMMï¼‰ï¼Œå¯¼è‡´è§£é‡åŒ–å®æ–½æ•ˆç‡ä½ä¸‹ã€‚å‡åŒ€é‡åŒ–æ–¹æ³•æ— æ³•å……åˆ†æ•æ‰æƒé‡åˆ†å¸ƒï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æœ¬æ–‡æå‡ºGANQï¼ˆGPUè‡ªé€‚åº”éå‡åŒ€é‡åŒ–ï¼‰ï¼Œä¸€ç§é’ˆå¯¹ç¡¬ä»¶é«˜æ•ˆæŸ¥æ‰¾è¡¨mpGEMMä¼˜åŒ–çš„å±‚åéå‡åŒ€é‡åŒ–æ¡†æ¶ã€‚GANQé€šè¿‡æ— è®­ç»ƒã€GPUè‡ªé€‚åº”ä¼˜åŒ–ç®—æ³•ï¼Œæœ‰æ•ˆå‡å°‘å±‚å†…é‡åŒ–è¯¯å·®ï¼Œå®ç°ä¼˜è¶Šçš„é‡æµ‹æ€§èƒ½ã€‚å®éªŒæ˜¾ç¤ºï¼Œä¸å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼ŒGANQç¼©å°äº†ä¸FP16åŸºå‡†çš„å›°æƒ‘åº¦å·®è·ï¼Œåœ¨3ä½å’Œ4ä½é‡åŒ–æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚åœ¨NVIDIA RTX 4090 GPUä¸Šéƒ¨ç½²æ—¶ï¼ŒGANQå®ç°é«˜è¾¾2.57å€çš„é€Ÿåº¦æå‡ï¼Œæ¨è¿›äº†LLMéƒ¨ç½²ä¸­çš„å†…å­˜å’Œæ¨ç†æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éƒ¨ç½²å­˜åœ¨èµ„æºéœ€æ±‚å¤§çš„æŒ‘æˆ˜ã€‚</li>
<li>ä½æ¯”ç‰¹é‡åŒ–æƒé‡èƒ½æé«˜æ¨ç†æ•ˆç‡ï¼Œä½†å½“å‰ç¡¬ä»¶å®æ–½æ•ˆç‡ä½ä¸‹ã€‚</li>
<li>å‡åŒ€é‡åŒ–æ–¹æ³•æ— æ³•å……åˆ†æ•æ‰æƒé‡åˆ†å¸ƒï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>GANQæ¡†æ¶æ˜¯ä¸€ç§é’ˆå¯¹ç¡¬ä»¶é«˜æ•ˆmpGEMMä¼˜åŒ–çš„å±‚åéå‡åŒ€é‡åŒ–æ–¹æ³•ã€‚</li>
<li>GANQé€šè¿‡æ— è®­ç»ƒã€GPUè‡ªé€‚åº”ä¼˜åŒ–ç®—æ³•ï¼Œæœ‰æ•ˆå‡å°‘å±‚å†…é‡åŒ–è¯¯å·®ã€‚</li>
<li>GANQåœ¨é‡åŒ–æ€§èƒ½ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç¼©å°äº†ä¸FP16åŸºå‡†çš„å›°æƒ‘åº¦å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12956">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12956v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12956v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12956v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12956v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Correctness-Assessment-of-Code-Generated-by-Large-Language-Models-Using-Internal-Representations"><a href="#Correctness-Assessment-of-Code-Generated-by-Large-Language-Models-Using-Internal-Representations" class="headerlink" title="Correctness Assessment of Code Generated by Large Language Models Using   Internal Representations"></a>Correctness Assessment of Code Generated by Large Language Models Using   Internal Representations</h2><p><strong>Authors:Tuan-Dung Bui, Thanh Trong Vu, Thu-Trang Nguyen, Son Nguyen, Hieu Dinh Vo</strong></p>
<p>Ensuring the correctness of code generated by Large Language Models (LLMs) presents a significant challenge in AI-driven software development. Existing approaches predominantly rely on black-box (closed-box) approaches that evaluate correctness post-generation, failing to utilize the rich insights embedded in the LLMsâ€™ internal states during code generation. In this paper, we introduce OPENIA, a novel white-box (open-box) framework that leverages these internal representations to assess the correctness of LLM-generated code. OPENIA systematically analyzes the intermediate states of representative open-source LLMs specialized for code, including DeepSeek-Coder, CodeLlama, and MagicCoder, across diverse code generation benchmarks. Our empirical analysis reveals that these internal representations encode latent information, which strongly correlates with the correctness of the generated code. Building on these insights, OPENIA uses a white-box&#x2F;open-box approach to make informed predictions about code correctness, offering significant advantages in adaptability and robustness over traditional classification-based methods and zero-shot approaches. Experimental results demonstrate that OPENIA consistently outperforms baseline models, achieving higher accuracy, precision, recall, and F1-Scores with up to a 2X improvement in standalone code generation and a 46% enhancement in repository-specific scenarios. By unlocking the potential of in-process signals, OPENIA paves the way for more proactive and efficient quality assurance mechanisms in LLM-assisted code generation. </p>
<blockquote>
<p>åœ¨äººå·¥æ™ºèƒ½é©±åŠ¨çš„è½¯ä»¶å¼€å‘ä¸­ï¼Œç¡®ä¿å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„ä»£ç çš„æ­£ç¡®æ€§æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºäº‹åè¯„ä¼°æ­£ç¡®æ€§çš„é»‘ç®±ï¼ˆå°é—­ç®±ï¼‰æ–¹æ³•ï¼Œæœªèƒ½åˆ©ç”¨LLMåœ¨ä»£ç ç”Ÿæˆè¿‡ç¨‹ä¸­å†…éƒ¨çŠ¶æ€çš„ä¸°å¯Œè§è§£ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†OPENIAï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ç™½ç®±ï¼ˆå¼€æ”¾ç®±ï¼‰æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨è¿™äº›å†…éƒ¨è¡¨ç¤ºæ¥è¯„ä¼°LLMç”Ÿæˆçš„ä»£ç çš„æ­£ç¡®æ€§ã€‚OPENIAç³»ç»Ÿåœ°åˆ†æäº†ä¸“é—¨ç”¨äºä»£ç çš„ä»£è¡¨æ€§å¼€æºLLMçš„ä¸­é—´çŠ¶æ€ï¼ŒåŒ…æ‹¬DeepSeek-Coderã€CodeLlamaå’ŒMagicCoderï¼Œä»¥åŠå¤šç§ä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„å®è¯åˆ†æè¡¨æ˜ï¼Œè¿™äº›å†…éƒ¨è¡¨ç¤ºåŒ…å«æ½œåœ¨ä¿¡æ¯ï¼Œä¸ç”Ÿæˆä»£ç çš„æ­£ç¡®æ€§é«˜åº¦ç›¸å…³ã€‚åŸºäºè¿™äº›è§è§£ï¼ŒOPENIAé‡‡ç”¨ç™½ç®±&#x2F;å¼€æ”¾ç®±æ–¹æ³•ï¼Œå¯¹ä»£ç çš„æ­£ç¡®æ€§åšå‡ºæœ‰æ ¹æ®çš„é¢„æµ‹ï¼Œä¸ä¼ ç»Ÿçš„åŸºäºåˆ†ç±»çš„æ–¹æ³•å’Œé›¶æ ·æœ¬æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨é€‚åº”æ€§å’Œç¨³å¥æ€§æ–¹é¢å…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOPENIAå§‹ç»ˆä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œåœ¨ç‹¬ç«‹ä»£ç ç”Ÿæˆæ–¹é¢å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°ï¼Œæœ€é«˜å¯è¾¾2å€çš„æå‡ï¼Œå¹¶åœ¨ç‰¹å®šä»“åº“åœºæ™¯ä¸­æé«˜äº†46%ã€‚é€šè¿‡è§£é”è¿›ç¨‹ä¿¡å·çš„æ½œåŠ›ï¼ŒOPENIAä¸ºLLMè¾…åŠ©çš„ä»£ç ç”Ÿæˆä¸­æ›´ç§¯æã€æ›´æœ‰æ•ˆçš„è´¨é‡ä¿è¯æœºåˆ¶é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12934v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨AIé©±åŠ¨çš„è½¯ä»¶å¼€å‘ä¸­ï¼Œç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„ä»£ç æ­£ç¡®æ€§çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äº‹åè¯„ä¼°çš„é»‘ç®±æ–¹æ³•ï¼Œå¿½ç•¥äº†LLMå†…éƒ¨çŠ¶æ€ä¸­çš„ä¸°å¯Œä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºäº†OPENIAï¼Œä¸€ä¸ªæ–°å‹çš„ç™½ç®±æ¡†æ¶ï¼Œåˆ©ç”¨è¿™äº›å†…éƒ¨è¡¨å¾æ¥è¯„ä¼°LLMç”Ÿæˆçš„ä»£ç çš„æ­£ç¡®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOPENIAåœ¨ä»£ç ç”Ÿæˆè´¨é‡å’Œé¢„æµ‹æ€§èƒ½ä¸Šå‡ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMç”Ÿæˆçš„ä»£ç æ­£ç¡®æ€§ä¿è¯æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–é»‘ç®±äº‹åè¯„ä¼°ï¼Œå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>OPENIAæ˜¯ä¸€ä¸ªæ–°å‹çš„ç™½ç®±æ¡†æ¶ï¼Œåˆ©ç”¨LLMçš„å†…éƒ¨è¡¨å¾æ¥è¯„ä¼°ç”Ÿæˆä»£ç çš„æ­£ç¡®æ€§ã€‚</li>
<li>OPENIAç³»ç»Ÿåœ°åˆ†æäº†DeepSeek-Coderã€CodeLlamaå’ŒMagicCoderç­‰ä»£è¡¨æ€§å¼€æºLLMçš„ä¸­é—´çŠ¶æ€ã€‚</li>
<li>å®è¯ç»“æœæ˜¾ç¤ºï¼ŒLLMçš„å†…éƒ¨è¡¨å¾ç¼–ç äº†ä¸ç”Ÿæˆä»£ç æ­£ç¡®æ€§å¼ºçƒˆç›¸å…³çš„ä¿¡æ¯ã€‚</li>
<li>OPENIAä½¿ç”¨ç™½ç®±æ–¹æ³•åšå‡ºå…³äºä»£ç æ­£ç¡®æ€§çš„é¢„æµ‹ï¼Œç›¸è¾ƒäºä¼ ç»Ÿåˆ†ç±»æ–¹æ³•å’Œé›¶æ ·æœ¬æ–¹æ³•å…·æœ‰æ›´é«˜çš„é€‚åº”æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOPENIAåœ¨å‡†ç¡®æ€§ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°ä¸Šå‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œç‹¬ç«‹ä»£ç ç”Ÿæˆå’Œä»“åº“ç‰¹å®šåœºæ™¯ä¸­çš„æ€§èƒ½æå‡åˆ†åˆ«é«˜è¾¾2å€å’Œ46%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12934">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12934v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12934v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12934v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12934v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Test-Time-Preference-Optimization-On-the-Fly-Alignment-via-Iterative-Textual-Feedback"><a href="#Test-Time-Preference-Optimization-On-the-Fly-Alignment-via-Iterative-Textual-Feedback" class="headerlink" title="Test-Time Preference Optimization: On-the-Fly Alignment via Iterative   Textual Feedback"></a>Test-Time Preference Optimization: On-the-Fly Alignment via Iterative   Textual Feedback</h2><p><strong>Authors:Yafu Li, Xuyang Hu, Xiaoye Qu, Linjie Li, Yu Cheng</strong></p>
<p>Large language models (LLMs) demonstrate impressive performance but lack the flexibility to adapt to human preferences quickly without retraining. In this work, we introduce Test-time Preference Optimization (TPO), a framework that aligns LLM outputs with human preferences during inference, removing the need to update model parameters. Rather than relying on purely numerical rewards, TPO translates reward signals into textual critiques and uses them as textual rewards to iteratively refine its response. Evaluations on benchmarks covering instruction following, preference alignment, safety, and mathematics reveal that TPO progressively improves alignment with human preferences. Notably, after only a few TPO steps, the initially unaligned Llama-3.1-70B-SFT model can surpass the aligned counterpart, Llama-3.1-70B-Instruct. Furthermore, TPO scales efficiently with both the search width and depth during inference. Through case studies, we illustrate how TPO exploits the innate capacity of LLM to interpret and act upon reward signals. Our findings establish TPO as a practical, lightweight alternative for test-time preference optimization, achieving alignment on the fly. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/yafuly/TPO">https://github.com/yafuly/TPO</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†ç¼ºä¹å¿«é€Ÿé€‚åº”äººç±»åå¥½çš„çµæ´»æ€§ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æµ‹è¯•æ—¶åå¥½ä¼˜åŒ–ï¼ˆTPOï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­å°†LLMè¾“å‡ºä¸äººç±»åå¥½å¯¹é½ï¼Œæ— éœ€æ›´æ–°æ¨¡å‹å‚æ•°ã€‚TPOä¸æ˜¯ä¾èµ–çº¯ç²¹çš„æ•°å€¼å¥–åŠ±ï¼Œè€Œæ˜¯å°†å¥–åŠ±ä¿¡å·è½¬åŒ–ä¸ºæ–‡æœ¬è¯„ä»·ï¼Œå¹¶å°†å…¶ç”¨ä½œæ–‡æœ¬å¥–åŠ±æ¥è¿­ä»£ä¼˜åŒ–å…¶å“åº”ã€‚åœ¨æ¶µç›–æŒ‡ä»¤éµå¾ªã€åå¥½å¯¹é½ã€å®‰å…¨å’Œæ•°å­¦ç­‰æ–¹é¢çš„åŸºå‡†æµ‹è¯•è¯„ä¼°ä¸­ï¼ŒTPOé€æ¸æé«˜äº†ä¸äººç±»åå¥½çš„å¯¹é½ç¨‹åº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨ä»…è¿›è¡Œå°‘æ•°å‡ æ­¥TPOåï¼Œæœ€åˆæœªå¯¹é½çš„Llama-3.1-70B-SFTæ¨¡å‹å¯ä»¥è¶…è¶Šå¯¹é½çš„åŒç±»æ¨¡å‹Llama-3.1-70B-Instructã€‚æ­¤å¤–ï¼ŒTPOåœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„æœç´¢å®½åº¦å’Œæ·±åº¦æ–¹é¢éƒ½èƒ½å®ç°é«˜æ•ˆæ‰©å±•ã€‚é€šè¿‡æ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬å±•ç¤ºäº†TPOå¦‚ä½•åˆ©ç”¨LLMè§£é‡Šå’Œå“åº”å¥–åŠ±ä¿¡å·çš„å›ºæœ‰èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒTPOæ˜¯ä¸€ç§å®ç”¨çš„è½»é‡çº§æµ‹è¯•æ—¶åå¥½ä¼˜åŒ–æ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå®ç°å³æ—¶å¯¹é½ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/yafuly/TPO%E3%80%82">https://github.com/yafuly/TPOã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12895v1">PDF</a> 43 pages; work in progress</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç¼ºä¹å¿«é€Ÿé€‚åº”äººç±»åå¥½çš„çµæ´»æ€§ï¼Œéœ€è¦é‡æ–°è®­ç»ƒã€‚æœ¬ç ”ç©¶æå‡ºäº†æµ‹è¯•æ—¶åå¥½ä¼˜åŒ–ï¼ˆTPOï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­å°†LLMè¾“å‡ºä¸äººç±»åå¥½å¯¹é½ï¼Œæ— éœ€æ›´æ–°æ¨¡å‹å‚æ•°ã€‚TPOå°†å¥–åŠ±ä¿¡å·è½¬åŒ–ä¸ºæ–‡æœ¬è¯„ä»·ï¼Œå¹¶å°†å…¶ç”¨ä½œæ–‡æœ¬å¥–åŠ±æ¥è¿­ä»£ä¼˜åŒ–å…¶å“åº”ã€‚åœ¨æ¶µç›–æŒ‡ä»¤éµå¾ªã€åå¥½å¯¹é½ã€å®‰å…¨å’Œæ•°å­¦æ–¹é¢çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒTPOé€æ­¥æé«˜äº†ä¸äººç±»åå¥½çš„å¯¹é½ç¨‹åº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç»è¿‡ä»…å‡ æ­¥çš„TPOä¼˜åŒ–ï¼Œæœ€åˆæœªå¯¹é½çš„Llama-3.1-70B-SFTæ¨¡å‹ç”šè‡³å¯ä»¥è¶…è¶Šå¯¹é½çš„Llama-3.1-70B-Instructã€‚æ­¤å¤–ï¼ŒTPOåœ¨æ¨ç†è¿‡ç¨‹ä¸­èƒ½å¤Ÿé«˜æ•ˆåœ°é€‚åº”æœç´¢å®½åº¦å’Œæ·±åº¦ã€‚æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼ŒTPOèƒ½å¤Ÿåˆ©ç”¨LLMå¯¹å¥–åŠ±ä¿¡å·çš„å†…åœ¨è§£è¯»å’Œè¡ŒåŠ¨èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼ŒTPOæ˜¯ä¸€ç§å®ç”¨çš„è½»é‡çº§æµ‹è¯•æ—¶åå¥½ä¼˜åŒ–æ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå®ç°å³æ—¶å¯¹é½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶æ€§èƒ½å‡ºè‰²ï¼Œä½†åœ¨æ¨ç†è¿‡ç¨‹ä¸­éš¾ä»¥å¿«é€Ÿé€‚åº”äººç±»åå¥½ã€‚</li>
<li>æµ‹è¯•æ—¶åå¥½ä¼˜åŒ–ï¼ˆTPOï¼‰æ¡†æ¶èƒ½å¤Ÿåœ¨ä¸æ›´æ–°æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹ï¼Œå°†LLMè¾“å‡ºä¸äººç±»åå¥½å¯¹é½ã€‚</li>
<li>TPOé€šè¿‡å°†å¥–åŠ±ä¿¡å·è½¬åŒ–ä¸ºæ–‡æœ¬è¯„ä»·ï¼Œç”¨ä½œæ–‡æœ¬å¥–åŠ±æ¥è¿­ä»£ä¼˜åŒ–å“åº”ã€‚</li>
<li>TPOèƒ½å¤Ÿæ˜¾è‘—æé«˜æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æŒ‡ä»¤éµå¾ªã€åå¥½å¯¹é½ã€å®‰å…¨å’Œæ•°å­¦æ–¹é¢ã€‚</li>
<li>ä»…éœ€å°‘é‡TPOæ­¥éª¤ï¼Œæœªå¯¹é½çš„æ¨¡å‹å³å¯è¶…è¶Šå¯¹é½çš„æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>TPOåœ¨æ¨ç†è¿‡ç¨‹ä¸­èƒ½å¤Ÿé«˜æ•ˆåœ°é€‚åº”æœç´¢å®½åº¦å’Œæ·±åº¦ï¼Œåˆ©ç”¨LLMçš„å†…åœ¨èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12895">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12895v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12895v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.12895v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Robust-Hybrid-Classical-Quantum-Transfer-Learning-Model-for-Text-Classification-Using-GPT-Neo-125M-with-LoRA-SMOTE-Enhancement"><a href="#Robust-Hybrid-Classical-Quantum-Transfer-Learning-Model-for-Text-Classification-Using-GPT-Neo-125M-with-LoRA-SMOTE-Enhancement" class="headerlink" title="Robust Hybrid Classical-Quantum Transfer Learning Model for Text   Classification Using GPT-Neo 125M with LoRA &amp; SMOTE Enhancement"></a>Robust Hybrid Classical-Quantum Transfer Learning Model for Text   Classification Using GPT-Neo 125M with LoRA &amp; SMOTE Enhancement</h2><p><strong>Authors:Santanam Wishal</strong></p>
<p>This research introduces a hybrid classical-quantum framework for text classification, integrating GPT-Neo 125M with Low-Rank Adaptation (LoRA) and Synthetic Minority Over-sampling Technique (SMOTE) using quantum computing backends. While the GPT-Neo 125M baseline remains the best-performing model, the implementation of LoRA and SMOTE enhances the hybrid model, resulting in improved accuracy, faster convergence, and better generalization. Experiments on IBMâ€™s 127-qubit quantum backend and Pennylaneâ€™s 32-qubit simulation demonstrate the viability of combining classical neural networks with quantum circuits. This framework underscores the potential of hybrid architectures for advancing natural language processing applications. </p>
<blockquote>
<p>æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§ç”¨äºæ–‡æœ¬åˆ†ç±»çš„æ··åˆç»å…¸-é‡å­æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†GPT-Neo 125Mä¸ä½ç§©é€‚é…ï¼ˆLoRAï¼‰å’Œåˆæˆå°‘æ•°è¿‡é‡‡æ ·æŠ€æœ¯ï¼ˆSMOTEï¼‰ï¼Œå¹¶ä½¿ç”¨é‡å­è®¡ç®—åç«¯ã€‚è™½ç„¶GPT-Neo 125MåŸºçº¿ä»ç„¶æ˜¯è¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼Œä½†LoRAå’ŒSMOTEçš„å®æ–½å¢å¼ºäº†æ··åˆæ¨¡å‹ï¼Œæé«˜äº†å‡†ç¡®æ€§ã€æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨IBMçš„127é‡å­ä½åç«¯å’ŒPennylaneçš„32é‡å­ä½æ¨¡æ‹Ÿå™¨çš„å®éªŒè¯æ˜äº†å°†ç»å…¸ç¥ç»ç½‘ç»œä¸é‡å­ç”µè·¯ç›¸ç»“åˆçš„å¯èƒ½æ€§ã€‚è¯¥æ¡†æ¶çªå‡ºäº†æ··åˆæ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10435v2">PDF</a> 8 pages, 11 figures</p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶æå‡ºä¸€ç§èåˆç»å…¸å’Œé‡å­è®¡ç®—æ¡†æ¶ç”¨äºæ–‡æœ¬åˆ†ç±»ï¼Œç»“åˆäº†GPT-Neo 125Mã€ä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰å’Œåˆæˆå°‘æ•°è¿‡é‡‡æ ·æŠ€æœ¯ï¼ˆSMOTEï¼‰ã€‚ä½¿ç”¨é‡å­è®¡ç®—åç«¯ï¼ŒGPT-Neo 125MåŸºçº¿æ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œç»“åˆLoRAå’ŒSMOTEçš„å®æ–½æé«˜äº†æ··åˆæ¨¡å‹çš„æ€§èƒ½ï¼Œå¸¦æ¥äº†æ›´é«˜çš„å‡†ç¡®æ€§ã€æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨IBMçš„127é‡å­æ¯”ç‰¹åç«¯å’ŒPennylaneçš„32é‡å­æ¯”ç‰¹æ¨¡æ‹Ÿå™¨ä¸Šè¿›è¡Œçš„å®éªŒè¯æ˜äº†ç»“åˆç»å…¸ç¥ç»ç½‘ç»œå’Œé‡å­ç”µè·¯çš„å¯èƒ½æ€§ã€‚è¯¥æ¡†æ¶çªæ˜¾äº†æ··åˆæ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§èåˆç»å…¸å’Œé‡å­è®¡ç®—çš„æ¡†æ¶ç”¨äºæ–‡æœ¬åˆ†ç±»ã€‚</li>
<li>ç»“åˆäº†GPT-Neo 125Mæ¨¡å‹ã€ä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰å’Œåˆæˆå°‘æ•°è¿‡é‡‡æ ·æŠ€æœ¯ï¼ˆSMOTEï¼‰ã€‚</li>
<li>GPT-Neo 125MåŸºçº¿æ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œä½†ç»“åˆLoRAå’ŒSMOTEæå‡äº†æ··åˆæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ¡†æ¶å¢å¼ºäº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€åŠ å¿«äº†æ”¶æ•›é€Ÿåº¦ï¼Œå¹¶æ”¹å–„äº†æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨IBMå’ŒPennylaneçš„å®éªŒå¹³å°ä¸ŠéªŒè¯äº†è¯¥æ¡†æ¶çš„å¯è¡Œæ€§ã€‚</li>
<li>å®éªŒè¯æ˜äº†ç»“åˆç»å…¸ç¥ç»ç½‘ç»œå’Œé‡å­ç”µè·¯çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10435">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.10435v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.10435v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FRAG-A-Flexible-Modular-Framework-for-Retrieval-Augmented-Generation-based-on-Knowledge-Graphs"><a href="#FRAG-A-Flexible-Modular-Framework-for-Retrieval-Augmented-Generation-based-on-Knowledge-Graphs" class="headerlink" title="FRAG: A Flexible Modular Framework for Retrieval-Augmented Generation   based on Knowledge Graphs"></a>FRAG: A Flexible Modular Framework for Retrieval-Augmented Generation   based on Knowledge Graphs</h2><p><strong>Authors:Zengyi Gao, Yukun Cao, Hairu Wang, Ao Ke, Yuan Feng, Xike Xie, S Kevin Zhou</strong></p>
<p>To mitigate the hallucination and knowledge deficiency in large language models (LLMs), Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG) has shown promising potential by utilizing KGs as external resource to enhance LLMs reasoning. However, existing KG-RAG approaches struggle with a trade-off between flexibility and retrieval quality. Modular methods prioritize flexibility by avoiding the use of KG-fine-tuned models during retrieval, leading to fixed retrieval strategies and suboptimal retrieval quality. Conversely, coupled methods embed KG information within models to improve retrieval quality, but at the expense of flexibility. In this paper, we propose a novel flexible modular KG-RAG framework, termed FRAG, which synergizes the advantages of both approaches. FRAG estimates the hop range of reasoning paths based solely on the query and classify it as either simple or complex. To match the complexity of the query, tailored pipelines are applied to ensure efficient and accurate reasoning path retrieval, thus fostering the final reasoning process. By using the query text instead of the KG to infer the structural information of reasoning paths and employing adaptable retrieval strategies, FRAG improves retrieval quality while maintaining flexibility. Moreover, FRAG does not require extra LLMs fine-tuning or calls, significantly boosting efficiency and conserving resources. Extensive experiments show that FRAG achieves state-of-the-art performance with high efficiency and low resource consumption. </p>
<blockquote>
<p>ä¸ºäº†ç¼“è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¹»è§‰å’ŒçŸ¥è¯†ç¼ºé™·é—®é¢˜ï¼ŒåŸºäºçŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•æ˜¾ç¤ºå‡ºåˆ©ç”¨çŸ¥è¯†å›¾è°±ä½œä¸ºå¤–éƒ¨èµ„æºæ¥æå‡LLMæ¨ç†èƒ½åŠ›çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„KG-RAGæ–¹æ³•é¢ä¸´çµæ´»æ€§å’Œæ£€ç´¢è´¨é‡ä¹‹é—´çš„æƒè¡¡ã€‚æ¨¡å—åŒ–æ–¹æ³•ä¼˜å…ˆçµæ´»æ€§ï¼Œé¿å…åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­ä½¿ç”¨é’ˆå¯¹çŸ¥è¯†å›¾è°±å¾®è°ƒè¿‡çš„æ¨¡å‹ï¼Œè¿™å¯¼è‡´å›ºå®šçš„æ£€ç´¢ç­–ç•¥å’Œä½ä¸‹çš„æ£€ç´¢è´¨é‡ã€‚ç›¸åï¼Œè€¦åˆæ–¹æ³•å°†çŸ¥è¯†å›¾è°±ä¿¡æ¯åµŒå…¥æ¨¡å‹ä¸­ä»¥æé«˜æ£€ç´¢è´¨é‡ï¼Œä½†ç‰ºç‰²äº†çµæ´»æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„çµæ´»æ¨¡å—åŒ–KG-RAGæ¡†æ¶ï¼Œåä¸ºFRAGã€‚FRAGæ ¹æ®æŸ¥è¯¢æ¥ä¼°è®¡æ¨ç†è·¯å¾„çš„è·³è½¬èŒƒå›´ï¼Œå¹¶å°†å…¶åˆ†ç±»ä¸ºç®€å•æˆ–å¤æ‚ã€‚ä¸ºäº†ä¸æŸ¥è¯¢çš„å¤æ‚æ€§ç›¸åŒ¹é…ï¼Œæˆ‘ä»¬åº”ç”¨å®šåˆ¶åŒ–çš„æµæ°´çº¿æ¥ç¡®ä¿é«˜æ•ˆä¸”å‡†ç¡®çš„æ¨ç†è·¯å¾„æ£€ç´¢ï¼Œä»è€Œä¿ƒè¿›æœ€ç»ˆçš„æ¨ç†è¿‡ç¨‹ã€‚FRAGä½¿ç”¨æŸ¥è¯¢æ–‡æœ¬è€Œä¸æ˜¯çŸ¥è¯†å›¾è°±æ¥æ¨æ–­æ¨ç†è·¯å¾„çš„ç»“æ„ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨å¯é€‚åº”çš„æ£€ç´¢ç­–ç•¥ï¼Œä»è€Œåœ¨æé«˜æ£€ç´¢è´¨é‡çš„åŒæ—¶ä¿æŒçµæ´»æ€§ã€‚æ­¤å¤–ï¼ŒFRAGä¸éœ€è¦é¢å¤–çš„LLMå¾®è°ƒæˆ–è°ƒç”¨ï¼Œè¿™å¤§å¤§æé«˜äº†æ•ˆç‡å¹¶èŠ‚çœäº†èµ„æºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFRAGä»¥é«˜æ•ˆç‡å’Œä½èµ„æºæ¶ˆè€—å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09957v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºçŸ¥è¯†å›¾è°±çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨çŸ¥è¯†å›¾è°±ä½œä¸ºå¤–éƒ¨èµ„æºï¼Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œç¼“è§£å…¶äº§ç”Ÿçš„å¹»è§‰å’ŒçŸ¥è¯†ç¼ºé™·é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„çµæ´»æ¨¡å—åŒ–KG-RAGæ¡†æ¶ï¼Œå³FRAGï¼Œèåˆäº†ç°æœ‰æ–¹æ³•çš„ä¼˜ç‚¹ã€‚FRAGèƒ½å¤Ÿæ ¹æ®æŸ¥è¯¢ä¼°è®¡æ¨ç†è·¯å¾„çš„è·³è½¬èŒƒå›´ï¼Œå¹¶å¯¹å…¶è¿›è¡Œç®€å•æˆ–å¤æ‚çš„åˆ†ç±»ï¼Œé‡‡ç”¨åŒ¹é…çš„ç®¡é“ç¡®ä¿é«˜æ•ˆå‡†ç¡®çš„æ¨ç†è·¯å¾„æ£€ç´¢ï¼Œä¿ƒè¿›æœ€ç»ˆçš„æ¨ç†è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶ä½¿ç”¨æŸ¥è¯¢æ–‡æœ¬æ¨æ–­æ¨ç†è·¯å¾„çš„ç»“æ„ä¿¡æ¯ï¼Œé‡‡ç”¨çµæ´»çš„æ£€ç´¢ç­–ç•¥ï¼Œæé«˜äº†æ£€ç´¢è´¨é‡ï¼ŒåŒæ—¶ä¿æŒäº†çµæ´»æ€§ã€‚æ­¤å¤–ï¼ŒFRAGä¸éœ€è¦é¢å¤–çš„LLMå¾®è°ƒæˆ–è°ƒç”¨ï¼Œæé«˜äº†æ•ˆç‡å’Œèµ„æºåˆ©ç”¨ç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒFRAGè¾¾åˆ°äº†é«˜æ•ˆä¸”èµ„æºæ¶ˆè€—ä½çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰å¯ä»¥ä½œä¸ºä¸€ç§å¤–éƒ¨èµ„æºï¼Œç”¨äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰KG-RAGæ–¹æ³•é¢ä¸´çµæ´»æ€§å’Œæ£€ç´¢è´¨é‡ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>FRAGæ¡†æ¶èƒ½å¤Ÿæ ¹æ®æŸ¥è¯¢çš„å¤æ‚æ€§è¿›è¡Œçµæ´»çš„æ¨¡å—åŒ–å¤„ç†ï¼Œç¡®ä¿é«˜æ•ˆå‡†ç¡®çš„æ¨ç†è·¯å¾„æ£€ç´¢ã€‚</li>
<li>FRAGä½¿ç”¨æŸ¥è¯¢æ–‡æœ¬æ¨æ–­æ¨ç†è·¯å¾„çš„ç»“æ„ä¿¡æ¯ï¼Œæé«˜äº†æ£€ç´¢è´¨é‡ã€‚</li>
<li>FRAGæ¡†æ¶åœ¨ä¿æŒçµæ´»æ€§çš„åŒæ—¶ï¼Œä¸éœ€è¦é¢å¤–çš„LLMå¾®è°ƒæˆ–è°ƒç”¨ï¼Œæé«˜äº†æ•ˆç‡å’Œèµ„æºåˆ©ç”¨ç‡ã€‚</li>
<li>FRAGå®ç°äº†é«˜æ•ˆä¸”èµ„æºæ¶ˆè€—ä½çš„æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.09957v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.09957v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2501.09957v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Yi-Lightning-Technical-Report"><a href="#Yi-Lightning-Technical-Report" class="headerlink" title="Yi-Lightning Technical Report"></a>Yi-Lightning Technical Report</h2><p><strong>Authors:Alan Wake, Bei Chen, C. X. Lv, Chao Li, Chengen Huang, Chenglin Cai, Chujie Zheng, Daniel Cooper, Fan Zhou, Feng Hu, Ge Zhang, Guoyin Wang, Heng Ji, Howard Qiu, Jiangcheng Zhu, Jun Tian, Katherine Su, Lihuan Zhang, Liying Li, Ming Song, Mou Li, Peng Liu, Qicheng Hu, Shawn Wang, Shijun Zhou, Shiming Yang, Shiyong Li, Tianhang Zhu, Wen Xie, Wenhao Huang, Xiang He, Xiaobo Chen, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Yanpeng Li, Yongke Zhao, Yongzhen Luo, Yuchi Xu, Yuxuan Sha, Zhaodong Yan, Zhiyuan Liu, Zirui Zhang, Zonghong Dai</strong></p>
<p>This technical report presents Yi-Lightning, our latest flagship large language model (LLM). It achieves exceptional performance, ranking 6th overall on Chatbot Arena, with particularly strong results (2nd to 4th place) in specialized categories including Chinese, Math, Coding, and Hard Prompts. Yi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture, featuring advanced expert segmentation and routing mechanisms coupled with optimized KV-caching techniques. Our development process encompasses comprehensive pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF), where we devise deliberate strategies for multi-stage training, synthetic data construction, and reward modeling. Furthermore, we implement RAISE (Responsible AI Safety Engine), a four-component framework to address safety issues across pre-training, post-training, and serving phases. Empowered by our scalable super-computing infrastructure, all these innovations substantially reduce training, deployment and inference costs while maintaining high-performance standards. With further evaluations on public academic benchmarks, Yi-Lightning demonstrates competitive performance against top-tier LLMs, while we observe a notable disparity between traditional, static benchmark results and real-world, dynamic human preferences. This observation prompts a critical reassessment of conventional benchmarksâ€™ utility in guiding the development of more intelligent and powerful AI systems for practical applications. Yi-Lightning is now available through our developer platform at <a target="_blank" rel="noopener" href="https://platform.lingyiwanwu.com/">https://platform.lingyiwanwu.com</a>. </p>
<blockquote>
<p>è¿™ç¯‡æŠ€æœ¯æŠ¥å‘Šä»‹ç»äº†æˆ‘ä»¬æœ€æ–°çš„æ——èˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰â€”â€”Yi-Lightningã€‚å®ƒåœ¨ChatGPTç«æŠ€åœºä¸Šçš„æ€»ä½“æ’åè¾¾åˆ°äº†ç¬¬6åï¼ŒåŒæ—¶åœ¨åŒ…æ‹¬ä¸­æ–‡ã€æ•°å­¦ã€ç¼–ç å’Œç¡¬æç¤ºç­‰ä¸“é¡¹ç±»åˆ«ä¸­å–å¾—äº†ä¼˜å¼‚çš„æˆç»©ï¼ˆç¬¬2è‡³ç¬¬4åï¼‰ã€‚Yi-Lightningé‡‡ç”¨äº†å¢å¼ºçš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ï¼Œé…å¤‡äº†å…ˆè¿›çš„ä¸“å®¶åˆ†å‰²å’Œè·¯ç”±æœºåˆ¶ï¼Œå¹¶ç»“åˆäº†ä¼˜åŒ–çš„KVç¼“å­˜æŠ€æœ¯ã€‚æˆ‘ä»¬çš„å¼€å‘è¿‡ç¨‹åŒ…æ‹¬å…¨é¢çš„é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰ï¼Œåœ¨æ­¤æœŸé—´æˆ‘ä»¬ä¸ºåˆ†é˜¶æ®µè®­ç»ƒã€åˆæˆæ•°æ®æ„å»ºå’Œå¥–åŠ±å»ºæ¨¡åˆ¶å®šäº†ç²¾å¿ƒè®¾è®¡çš„ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®æ–½äº†RAISEï¼ˆè´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½å®‰å…¨å¼•æ“ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±å››ä¸ªç»„ä»¶ç»„æˆçš„æ¡†æ¶ï¼Œç”¨äºè§£å†³é¢„è®­ç»ƒã€åè®­ç»ƒå’Œæä¾›é˜¶æ®µçš„å„ç±»å®‰å…¨é—®é¢˜ã€‚å‡­å€Ÿæˆ‘ä»¬å¯æ‰©å±•çš„è¶…çº§è®¡ç®—åŸºç¡€è®¾æ–½ï¼Œæ‰€æœ‰è¿™äº›åˆ›æ–°åœ¨ç»´æŒé«˜æ€§èƒ½æ ‡å‡†çš„åŒæ—¶ï¼Œå¤§å¤§é™ä½äº†è®­ç»ƒã€éƒ¨ç½²å’Œæ¨ç†æˆæœ¬ã€‚åœ¨å…¬å…±å­¦æœ¯åŸºå‡†ä¸Šçš„è¿›ä¸€æ­¥è¯„ä¼°è¡¨æ˜ï¼ŒYi-Lightningä¸é¡¶çº§LLMç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼›åŒæ—¶ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°ä¼ ç»Ÿé™æ€åŸºå‡†ç»“æœå’Œç°å®ä¸–ç•ŒåŠ¨æ€äººç±»åå¥½ä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„å·®å¼‚ã€‚è¿™ä¸€è§‚å¯Ÿä¿ƒä½¿æˆ‘ä»¬é‡æ–°å®¡è§†ä¼ ç»ŸåŸºå‡†åœ¨æŒ‡å¯¼æ›´å…·æ™ºèƒ½å’Œå¼ºå¤§çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿå®é™…åº”ç”¨å¼€å‘æ–¹é¢çš„å®ç”¨æ€§ã€‚Yi-Lightningç°å·²å¯é€šè¿‡æˆ‘ä»¬çš„å¼€å‘å¹³å°ï¼ˆ<a target="_blank" rel="noopener" href="https://platform.lingyiwanwu.com)ä½¿ç”¨./">https://platform.lingyiwanwu.comï¼‰ä½¿ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01253v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æœ€æ–°æ——èˆ°å¤§å‹è¯­è¨€æ¨¡å‹Yi-Lightningï¼Œå®ƒåœ¨Chatbot Arenaä¸Šæ€»ä½“æ’åç¬¬å…­ï¼Œå¹¶åœ¨åŒ…æ‹¬ä¸­æ–‡ã€æ•°å­¦ã€ç¼–ç å’Œç¡¬æç¤ºç­‰ä¸“é¡¹ç±»åˆ«ä¸­å–å¾—ä¼˜å¼‚ç»“æœã€‚Yi-Lightningé‡‡ç”¨å¢å¼ºçš„æ··åˆä¸“å®¶æ¶æ„ï¼Œå…·æœ‰å…ˆè¿›çš„ä¸“å®¶åˆ†å‰²å’Œè·¯ç”±æœºåˆ¶ä»¥åŠä¼˜åŒ–çš„KVç¼“å­˜æŠ€æœ¯ã€‚å¼€å‘è¿‡ç¨‹åŒ…æ‹¬é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ï¼Œå®æ–½RAISEè´Ÿè´£äººå·¥æ™ºèƒ½å®‰å…¨å¼•æ“è§£å†³å®‰å…¨é—®é¢˜ã€‚Yi-Lightningåˆ©ç”¨å¯æ‰©å±•çš„è¶…çº§è®¡ç®—åŸºç¡€è®¾æ–½å¤§å¹…é™ä½äº†è®­ç»ƒã€éƒ¨ç½²å’Œæ¨ç†æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ€§èƒ½æ ‡å‡†ã€‚è¯¥æ¨¡å‹åœ¨å…¬å…±å­¦æœ¯åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå¹¶è§‚å¯Ÿåˆ°ä¼ ç»Ÿé™æ€åŸºå‡†æµ‹è¯•ç»“æœä¸ç°å®ä¸–ç•Œä¸­åŠ¨æ€äººç±»åå¥½ä¹‹é—´çš„å·®å¼‚ï¼Œè¿™ä¿ƒä½¿é‡æ–°è¯„ä¼°ä¼ ç»ŸåŸºå‡†æµ‹è¯•åœ¨æŒ‡å¯¼å¼€å‘æ›´å…·æ™ºèƒ½å’Œå®ç”¨æ€§çš„AIç³»ç»Ÿæ–¹é¢çš„ä½œç”¨ã€‚Yi-Lightningå·²é€šè¿‡æˆ‘ä»¬çš„å¼€å‘å¹³å°å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Yi-Lightningæ˜¯æœ€æ–°æ——èˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåœ¨Chatbot Arenaä¸Šæ€»ä½“æ’åç¬¬å…­ã€‚</li>
<li>åœ¨ä¸“é¡¹ç±»åˆ«ä¸­ï¼ŒåŒ…æ‹¬ä¸­æ–‡ã€æ•°å­¦ã€ç¼–ç å’Œç¡¬æç¤ºç­‰æ–¹é¢å–å¾—ä¼˜å¼‚æˆæœï¼Œæ’åç¬¬äºŒè‡³ç¬¬å››ã€‚</li>
<li>é‡‡ç”¨å¢å¼ºçš„æ··åˆä¸“å®¶æ¶æ„ï¼Œå…·æœ‰å…ˆè¿›çš„ä¸“å®¶åˆ†å‰²å’Œè·¯ç”±æœºåˆ¶åŠä¼˜åŒ–çš„KVç¼“å­˜æŠ€æœ¯ã€‚</li>
<li>å¼€å‘è¿‡ç¨‹åŒ…æ‹¬é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ã€‚</li>
<li>å®æ–½RAISEè´Ÿè´£äººå·¥æ™ºèƒ½å®‰å…¨å¼•æ“ä»¥è§£å†³å®‰å…¨é—®é¢˜ã€‚</li>
<li>Yi-Lightningåˆ©ç”¨è¶…çº§è®¡ç®—åŸºç¡€è®¾æ–½é™ä½è®­ç»ƒã€éƒ¨ç½²å’Œæ¨ç†æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01253">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2412.01253v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2412.01253v5/page_2_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="VisMin-Visual-Minimal-Change-Understanding"><a href="#VisMin-Visual-Minimal-Change-Understanding" class="headerlink" title="VisMin: Visual Minimal-Change Understanding"></a>VisMin: Visual Minimal-Change Understanding</h2><p><strong>Authors:Rabiul Awal, Saba Ahmadi, Le Zhang, Aishwarya Agrawal</strong></p>
<p>Fine-grained understanding of objects, attributes, and relationships between objects is crucial for visual-language models (VLMs). Existing benchmarks primarily focus on evaluating VLMsâ€™ capability to distinguish between two very similar captions given an image. In this paper, we introduce a new, challenging benchmark termed Visual Minimal-Change Understanding (VisMin), which requires models to predict the correct image-caption match given two images and two captions. The image pair and caption pair contain minimal changes, i.e., only one aspect changes at a time from among the following: object, attribute, count, and spatial relation. These changes test the modelsâ€™ understanding of objects, attributes (such as color, material, shape), counts, and spatial relationships between objects. We built an automatic framework using large language models and diffusion models, followed by a rigorous 4-step verification process by human annotators. Empirical experiments reveal that current VLMs exhibit notable deficiencies in understanding spatial relationships and counting abilities. We also generate a large-scale training dataset to finetune CLIP and Idefics2, showing significant improvements in fine-grained understanding across benchmarks and in CLIPâ€™s general image-text alignment. We release all resources, including the benchmark, training data, and finetuned model checkpoints, at <a target="_blank" rel="noopener" href="https://vismin.net/">https://vismin.net/</a>. </p>
<blockquote>
<p>å¯¹ç‰©ä½“ã€å±æ€§å’Œç‰©ä½“ä¹‹é—´å…³ç³»çš„ç²¾ç»†ç†è§£å¯¹äºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è‡³å…³é‡è¦ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨è¯„ä¼°VLMsåœ¨ç»™å®šå›¾åƒçš„æƒ…å†µä¸‹åŒºåˆ†ä¸¤ä¸ªéå¸¸ç›¸ä¼¼çš„æ ‡é¢˜çš„èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œç§°ä¸ºè§†è§‰æœ€å°å˜åŒ–ç†è§£ï¼ˆVisMinï¼‰ï¼Œè¯¥æµ‹è¯•è¦æ±‚æ¨¡å‹åœ¨ç»™å®šçš„ä¸¤ä¸ªå›¾åƒå’Œä¸¤ä¸ªæ ‡é¢˜ä¸­é¢„æµ‹æ­£ç¡®çš„å›¾åƒ-æ ‡é¢˜åŒ¹é…ã€‚å›¾åƒå¯¹å’Œæ ‡é¢˜å¯¹ä¸­åŒ…å«äº†å¾®å°çš„å˜åŒ–ï¼Œå³æ¯æ¬¡åªæœ‰ä¸€ä¸ªæ–¹é¢çš„å˜åŒ–ï¼ŒåŒ…æ‹¬å¯¹è±¡ã€å±æ€§ã€è®¡æ•°å’Œç©ºé—´å…³ç³»ã€‚è¿™äº›å˜åŒ–æµ‹è¯•äº†æ¨¡å‹å¯¹ç‰©ä½“ã€å±æ€§ï¼ˆå¦‚é¢œè‰²ã€æè´¨ã€å½¢çŠ¶ï¼‰ã€è®¡æ•°ä»¥åŠç‰©ä½“ä¹‹é—´ç©ºé—´å…³ç³»çš„ç†è§£ã€‚æˆ‘ä»¬ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹å»ºç«‹äº†ä¸€ä¸ªè‡ªåŠ¨æ¡†æ¶ï¼Œéšåç»è¿‡äººç±»æ³¨é‡Šè€…ä¸¥æ ¼çš„å››æ­¥éªŒè¯è¿‡ç¨‹ã€‚ç»éªŒå®éªŒè¡¨æ˜ï¼Œç›®å‰çš„VLMåœ¨ç†è§£ç©ºé—´å…³ç³»å’Œè®¡æ•°èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜æ˜¾ç¼ºé™·ã€‚æˆ‘ä»¬è¿˜ç”Ÿæˆäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®é›†æ¥å¾®è°ƒCLIPå’ŒIdefics2ï¼Œåœ¨åŸºå‡†æµ‹è¯•å’ŒCLIPçš„é€šç”¨å›¾åƒæ–‡æœ¬å¯¹é½æ–¹é¢æ˜¾ç¤ºå‡ºå¯¹ç²¾ç»†ç†è§£çš„æ˜¾è‘—æ”¹å–„ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://vismin.net/">https://vismin.net/</a>ä¸Šå‘å¸ƒäº†æ‰€æœ‰èµ„æºï¼ŒåŒ…æ‹¬åŸºå‡†æµ‹è¯•ã€è®­ç»ƒæ•°æ®å’Œå¾®è°ƒæ¨¡å‹æ£€æŸ¥ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.16772v2">PDF</a> Accepted at NeurIPS 2024. Project URL at <a target="_blank" rel="noopener" href="https://vismin.net/">https://vismin.net/</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æŒ‘æˆ˜åŸºå‡†Visual Minimal-Change Understanding (VisMin)ï¼Œè¯¥åŸºå‡†æµ‹è¯•äº†æ¨¡å‹å¯¹äºå›¾ç‰‡ä¸­çš„å¯¹è±¡ã€å±æ€§ã€è®¡æ•°ä»¥åŠå¯¹è±¡é—´ç©ºé—´å…³ç³»çš„ç†è§£ã€‚åœ¨è¯„ä¼°ä¸­å‘ç°äº†ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç†è§£ç©ºé—´å…³ç³»å’Œè®¡æ•°æ–¹é¢çš„ç¼ºé™·ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªå¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›†ä»¥ä¼˜åŒ–CLIPå’ŒIdefics2çš„æ€§èƒ½ï¼Œåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—æ”¹å–„ã€‚åŒæ—¶é‡Šæ”¾äº†æ‰€æœ‰èµ„æºï¼ŒåŒ…æ‹¬åŸºå‡†æµ‹è¯•ã€è®­ç»ƒæ•°æ®å’Œå¾®è°ƒåçš„æ¨¡å‹æ£€æŸ¥ç‚¹ä»¥ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹éœ€è¦ç²¾ç»†ç†è§£å›¾åƒä¸­çš„å¯¹è±¡ã€å±æ€§åŠå¯¹è±¡é—´çš„å…³ç³»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æŒ‘æˆ˜åŸºå‡†Visual Minimal-Change Understanding (VisMin)ã€‚</li>
<li>VisMinåŸºå‡†æµ‹è¯•åŒ…æ‹¬å›¾åƒå’Œæ–‡æœ¬å¯¹ä¸­çš„ç»†å¾®å˜åŒ–ï¼Œå¦‚å¯¹è±¡ã€å±æ€§ã€è®¡æ•°å’Œç©ºé—´å…³ç³»çš„æ”¹å˜ã€‚</li>
<li>å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´å…³ç³»å’Œè®¡æ•°ç†è§£æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚</li>
<li>é‡Šæ”¾äº†ä¸€ä¸ªå¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›†ä»¥ä¼˜åŒ–CLIPå’ŒIdefics2çš„æ€§èƒ½ã€‚</li>
<li>ç»è¿‡è®­ç»ƒæ•°æ®é›†çš„ä¼˜åŒ–ï¼ŒCLIPå’ŒIdefics2åœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.16772">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2407.16772v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2407.16772v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2407.16772v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Panza-Design-and-Analysis-of-a-Fully-Local-Personalized-Text-Writing-Assistant"><a href="#Panza-Design-and-Analysis-of-a-Fully-Local-Personalized-Text-Writing-Assistant" class="headerlink" title="Panza: Design and Analysis of a Fully-Local Personalized Text Writing   Assistant"></a>Panza: Design and Analysis of a Fully-Local Personalized Text Writing   Assistant</h2><p><strong>Authors:Armand Nicolicioiu, Eugenia Iofinova, Eldar Kurtic, Mahdi Nikdan, Andrei Panferov, Ilia Markov, Nir Shavit, Dan Alistarh</strong></p>
<p>The availability of powerful open-source large language models (LLMs) opens exciting use cases, such as automated personal assistants that adapt to the userâ€™s unique data and demands. Two key requirements for such assistants are personalization - in the sense that the assistant should reflect the userâ€™s own writing style - and privacy - users may prefer to always store their personal data locally, on their own computing device. In this application paper, we present a new design and evaluation for such an automated assistant, for the specific use case of email generation, which we call Panza. Specifically, Panza can be trained and deployed locally on commodity hardware, and is personalized to the userâ€™s writing style. Panzaâ€™s personalization features are based on a combination of fine-tuning using a variant of the Reverse Instructions technique together with Retrieval-Augmented Generation (RAG). We demonstrate that this combination allows us to fine-tune an LLM to better reflect a userâ€™s writing style using limited data, while executing on extremely limited resources, e.g. on a free Google Colab instance. Our key methodological contribution is what we believe to be the first detailed study of evaluation metrics for this personalized writing task, and of how different choices of system components - e.g. the use of RAG and of different fine-tuning approaches - impact the systemâ€™s performance. We are releasing the full Panza code as well as a new â€œDavidâ€ personalized email dataset licensed for research use, both available on <a target="_blank" rel="noopener" href="https://github.com/IST-DASLab/PanzaMail">https://github.com/IST-DASLab/PanzaMail</a>. </p>
<blockquote>
<p>å¼ºå¤§çš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯ç”¨æ€§ä¸ºå®é™…åº”ç”¨å¸¦æ¥äº†æ¿€åŠ¨äººå¿ƒçš„ç”¨ä¾‹ï¼Œä¾‹å¦‚é€‚åº”ç”¨æˆ·ç‹¬ç‰¹æ•°æ®å’Œéœ€æ±‚çš„è‡ªåŠ¨åŒ–ä¸ªäººåŠ©ç†ã€‚è¿™ç±»åŠ©ç†çš„ä¸¤ä¸ªå…³é”®è¦æ±‚æ˜¯ä¸ªæ€§åŒ–â€”â€”å³åŠ©ç†åº”è¯¥åæ˜ ç”¨æˆ·è‡ªå·±çš„å†™ä½œé£æ ¼â€”â€”å’Œéšç§â€”â€”ç”¨æˆ·å¯èƒ½æ›´å–œæ¬¢å§‹ç»ˆå°†ä»–ä»¬çš„ä¸ªäººæ•°æ®å­˜å‚¨åœ¨æœ¬åœ°è‡ªå·±çš„è®¡ç®—è®¾å¤‡ä¸Šã€‚åœ¨æœ¬åº”ç”¨è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹ç”µå­é‚®ä»¶ç”Ÿæˆè¿™ä¸€ç‰¹å®šç”¨ä¾‹ï¼Œä¸ºæ­¤ç±»è‡ªåŠ¨åŒ–åŠ©ç†æå‡ºäº†æ–°è®¾è®¡å’Œè¯„ä¼°ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæ½˜æ‰ï¼ˆPanzaï¼‰â€ã€‚å…·ä½“æ¥è¯´ï¼Œæ½˜æ‰å¯ä»¥åœ¨å•†å“ç¡¬ä»¶ä¸Šæœ¬åœ°è¿›è¡Œè®­ç»ƒå’Œéƒ¨ç½²ï¼Œå¹¶å¯å®ç°ä¸ªæ€§åŒ–çš„å†™ä½œé£æ ¼ã€‚æ½˜æ‰çš„ä¸ªæ€§åŒ–åŠŸèƒ½åŸºäºä½¿ç”¨åå‘æŒ‡ä»¤æŠ€æœ¯å˜ä½“è¿›è¡Œå¾®è°ƒï¼Œå¹¶ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€‚æˆ‘ä»¬è¯æ˜ï¼Œè¿™ç§ç»“åˆå…è®¸æˆ‘ä»¬å¾®è°ƒLLMä»¥æ›´å¥½åœ°åæ˜ ç”¨æˆ·çš„å†™ä½œé£æ ¼ï¼ŒåŒæ—¶ä½¿ç”¨æœ‰é™çš„æ•°æ®åœ¨æå…¶æœ‰é™çš„èµ„æºä¸Šè¿è¡Œï¼Œä¾‹å¦‚åœ¨å…è´¹çš„Google Colabå®ä¾‹ä¸Šã€‚æˆ‘ä»¬çš„ä¸»è¦æ–¹æ³•è®ºè´¡çŒ®åœ¨äºæˆ‘ä»¬è®¤ä¸ºè¿™æ˜¯å¯¹ä¸ªæ€§åŒ–å†™ä½œä»»åŠ¡çš„è¯„ä¼°æŒ‡æ ‡çš„é¦–æ¬¡è¯¦ç»†ç ”ç©¶ï¼Œä»¥åŠä¸åŒçš„ç³»ç»Ÿç»„ä»¶é€‰æ‹©â€”â€”ä¾‹å¦‚ä½¿ç”¨RAGå’Œä¸åŒçš„å¾®è°ƒæ–¹æ³•â€”â€”å¦‚ä½•å½±å“ç³»ç»Ÿæ€§èƒ½ã€‚æˆ‘ä»¬å…¬å¼€äº†å®Œæ•´çš„æ½˜æ‰ä»£ç ä»¥åŠç”¨äºç ”ç©¶çš„æ–°â€œå¤§å«â€ä¸ªæ€§åŒ–ç”µå­é‚®ä»¶æ•°æ®é›†ï¼Œä¸¤è€…å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/IST-DASLab/PanzaMail%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/IST-DASLab/PanzaMailä¸Šè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.10994v2">PDF</a> Panza is available at <a target="_blank" rel="noopener" href="https://github.com/IST-DASLab/PanzaMail">https://github.com/IST-DASLab/PanzaMail</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå¼ºå¤§å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨åŒ–ä¸ªäººåŠ©ç†åº”ç”¨ï¼Œä¸“é—¨ç”¨äºç”µå­é‚®ä»¶ç”Ÿæˆï¼Œåä¸ºPanzaã€‚Panzaå¯ä»¥åœ¨æ™®é€šç¡¬ä»¶ä¸Šæœ¬åœ°è¿›è¡Œè®­ç»ƒå’Œéƒ¨ç½²ï¼Œå¹¶èƒ½åæ˜ ç”¨æˆ·çš„å†™ä½œé£æ ¼å®ç°ä¸ªæ€§åŒ–ã€‚å…¶ä¸ªæ€§åŒ–åŠŸèƒ½ç»“åˆäº†åå‘æŒ‡ä»¤æŠ€æœ¯çš„å¾®è°ƒä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§ç»“åˆèƒ½åœ¨æœ‰é™æ•°æ®ä¸‹ä½¿LLMæ›´å¥½åœ°åæ˜ ç”¨æˆ·çš„å†™ä½œé£æ ¼ï¼ŒåŒæ—¶åœ¨æå…¶æœ‰é™çš„èµ„æºä¸‹è¿è¡Œï¼Œå¦‚å…è´¹Google Colabå®ä¾‹ã€‚æœ¬æ–‡çš„å…³é”®æ–¹æ³•è®ºè´¡çŒ®åœ¨äºé¦–æ¬¡å¯¹è¯¥ä¸ªæ€§åŒ–å†™ä½œä»»åŠ¡çš„è¯„ä¼°æŒ‡æ ‡ä»¥åŠä¸åŒç³»ç»Ÿç»„ä»¶çš„é€‰æ‹©å¯¹ç³»ç»Ÿæ€§èƒ½çš„å½±å“è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚åŒæ—¶å…¬å¼€äº†å®Œæ•´çš„Panzaä»£ç å’Œæ–°çš„â€œDavidâ€ä¸ªæ€§åŒ–ç”µå­é‚®ä»¶æ•°æ®é›†ï¼Œä¾›ç ”ç©¶ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Panzaæ˜¯ä¸€ç§åŸºäºå¼ºå¤§å¼€æºLLMçš„è‡ªåŠ¨åŒ–ä¸ªäººåŠ©ç†åº”ç”¨ï¼Œä¸“ä¸ºç”µå­é‚®ä»¶ç”Ÿæˆè®¾è®¡ã€‚</li>
<li>Panzaèƒ½åœ¨æ™®é€šç¡¬ä»¶ä¸Šæœ¬åœ°è¿›è¡Œè®­ç»ƒå’Œéƒ¨ç½²ï¼Œå¹¶èƒ½åæ˜ ç”¨æˆ·çš„å†™ä½œé£æ ¼å®ç°ä¸ªæ€§åŒ–ã€‚</li>
<li>Panzaç»“åˆåå‘æŒ‡ä»¤æŠ€æœ¯çš„å¾®è°ƒä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å®ç°ä¸ªæ€§åŒ–ã€‚</li>
<li>ç»“åˆæœ‰é™æ•°æ®ï¼ŒPanzaèƒ½æ›´å¥½åœ°åæ˜ ç”¨æˆ·çš„å†™ä½œé£æ ¼ï¼Œåœ¨æœ‰é™çš„èµ„æºä¸‹è¿è¡Œã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡è¯¦ç»†ç ”ç©¶äº†ä¸ªæ€§åŒ–å†™ä½œä»»åŠ¡çš„è¯„ä¼°æŒ‡æ ‡åŠç³»ç»Ÿç»„ä»¶å¯¹æ€§èƒ½çš„å½±å“ã€‚</li>
<li>Panzaä»£ç å’Œâ€œDavidâ€ä¸ªæ€§åŒ–ç”µå­é‚®ä»¶æ•°æ®é›†å·²å…¬å¼€ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.10994">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2407.10994v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2407.10994v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2407.10994v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SituationalLLM-Proactive-Language-Models-with-Scene-Awareness-for-Dynamic-Contextual-Task-Guidance"><a href="#SituationalLLM-Proactive-Language-Models-with-Scene-Awareness-for-Dynamic-Contextual-Task-Guidance" class="headerlink" title="SituationalLLM: Proactive Language Models with Scene Awareness for   Dynamic, Contextual Task Guidance"></a>SituationalLLM: Proactive Language Models with Scene Awareness for   Dynamic, Contextual Task Guidance</h2><p><strong>Authors:Muhammad Saif Ullah Khan, Didier Stricker</strong></p>
<p>Large language models (LLMs) have achieved remarkable success in text-based tasks but often struggle to provide actionable guidance in real-world physical environments. This is because of their inability to recognize their limited understanding of the userâ€™s physical context. We present SituationalLLM, a novel approach that integrates structured scene information into an LLM to deliver proactive, context-aware assistance. By encoding objects, attributes, and relationships in a custom Scene Graph Language, SituationalLLM actively identifies gaps in environmental context and seeks clarifications during user interactions. This behavior emerges from training on the Situational Awareness Database for Instruct-Tuning (SAD-Instruct), which combines diverse, scenario-specific scene graphs with iterative, dialogue-based refinements. Experimental results indicate that SituationalLLM outperforms generic LLM baselines in task specificity, reliability, and adaptability, paving the way for environment-aware AI assistants capable of delivering robust, user-centric guidance under real-world constraints. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„ç‰©ç†ç¯å¢ƒä¸­å¾€å¾€éš¾ä»¥æä¾›å¯æ“ä½œçš„æŒ‡å¯¼ã€‚è¿™æ˜¯å› ä¸ºå®ƒä»¬æ— æ³•è¯†åˆ«è‡ªå·±å¯¹ç”¨æˆ·ç‰©ç†ç¯å¢ƒçš„æœ‰é™ç†è§£ã€‚æˆ‘ä»¬æå‡ºäº†SituationalLLMï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œå®ƒå°†ç»“æ„åŒ–åœºæ™¯ä¿¡æ¯é›†æˆåˆ°LLMä¸­ï¼Œä»¥æä¾›ä¸»åŠ¨ã€åŸºäºä¸Šä¸‹æ–‡çš„æ”¯æŒã€‚é€šè¿‡è‡ªå®šä¹‰åœºæ™¯å›¾è¯­è¨€å¯¹ç‰©ä½“ã€å±æ€§å’Œå…³ç³»è¿›è¡Œç¼–ç ï¼ŒSituationalLLMèƒ½å¤Ÿä¸»åŠ¨è¯†åˆ«ç¯å¢ƒä¸Šä¸‹æ–‡ä¸­çš„å·®è·ï¼Œå¹¶åœ¨ç”¨æˆ·äº¤äº’è¿‡ç¨‹ä¸­å¯»æ±‚æ¾„æ¸…ã€‚è¿™ç§è¡Œä¸ºæ¥æºäºå¯¹æŒ‡ä»¤è®­ç»ƒæƒ…å¢ƒæ„è¯†æ•°æ®åº“ï¼ˆSAD-Instructï¼‰çš„è®­ç»ƒï¼Œè¯¥æ•°æ®åº“å°†å¤šæ ·åŒ–ã€ç‰¹å®šåœºæ™¯çš„åœºæ™¯å›¾ä¸è¿­ä»£ã€åŸºäºå¯¹è¯çš„ç»†åŒ–ç›¸ç»“åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä»»åŠ¡ç‰¹å¼‚æ€§ã€å¯é æ€§å’Œé€‚åº”æ€§æ–¹é¢ï¼ŒSituationalLLMè¶…è¶Šäº†é€šç”¨LLMåŸºå‡†ï¼Œä¸ºèƒ½å¤Ÿåœ¨å®é™…çº¦æŸä¸‹æä¾›ç¨³å¥ã€ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒæŒ‡å¯¼çš„ç¯å¢ƒæ„ŸçŸ¥AIåŠ©ç†é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.13302v2">PDF</a> Submitted to Open Research Europe</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„ç‰©ç†ç¯å¢ƒä¸­æä¾›è¡ŒåŠ¨æŒ‡å¯¼æ—¶å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚åŸå› åœ¨äºLLMsæ— æ³•è¯†åˆ«ç”¨æˆ·å¯¹ç‰©ç†ç¯å¢ƒçš„ç†è§£æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSituationalLLMçš„æ–°æ–¹æ³•ï¼Œå®ƒå°†ç»“æ„åŒ–åœºæ™¯ä¿¡æ¯èå…¥LLMä¸­ï¼Œä»¥æä¾›ä¸»åŠ¨ã€æƒ…å¢ƒæ„ŸçŸ¥çš„è¾…åŠ©ã€‚é€šè¿‡è‡ªå®šä¹‰åœºæ™¯å›¾è¯­è¨€ç¼–ç ç‰©ä½“ã€å±æ€§å’Œå…³ç³»ï¼ŒSituationalLLMèƒ½ç§¯æè¯†åˆ«ç¯å¢ƒä¸Šä¸‹æ–‡ä¸­çš„å·®è·ï¼Œåœ¨ç”¨æˆ·äº¤äº’è¿‡ç¨‹ä¸­å¯»æ±‚æ¾„æ¸…ã€‚è¿™ç§è¡Œä¸ºæºäºå¯¹æƒ…å¢ƒæ„ŸçŸ¥æ•°æ®åº“æŒ‡ä»¤è®­ç»ƒï¼ˆSAD-Instructï¼‰çš„è®­ç»ƒï¼Œç»“åˆäº†å¤šç§åœºæ™¯ç‰¹å®šçš„åœºæ™¯å›¾å’ŒåŸºäºè¿­ä»£çš„å¯¹è¯å¼ç»†åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä»»åŠ¡ç‰¹å¼‚æ€§ã€å¯é æ€§å’Œé€‚åº”æ€§æ–¹é¢ï¼ŒSituationalLLMä¼˜äºé€šç”¨LLMåŸºçº¿ï¼Œä¸ºç¯å¢ƒæ„ŸçŸ¥AIåŠ©ç†é“ºå¹³äº†é“è·¯ï¼Œèƒ½å¤Ÿåœ¨ç°å®ä¸–ç•Œçš„çº¦æŸä¸‹æä¾›ç¨³å¥ã€ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨æ–‡æœ¬ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„ç‰©ç†ç¯å¢ƒä¸­æä¾›æŒ‡å¯¼æ—¶å­˜åœ¨å›°éš¾ã€‚</li>
<li>SituationalLLMæ˜¯ä¸€ç§å°†ç»“æ„åŒ–åœºæ™¯ä¿¡æ¯èå…¥LLMçš„æ–°æ–¹æ³•ã€‚</li>
<li>SituationalLLMé€šè¿‡è‡ªå®šä¹‰åœºæ™¯å›¾è¯­è¨€ç¼–ç ç‰©ä½“ã€å±æ€§å’Œå…³ç³»ã€‚</li>
<li>SituationalLLMèƒ½ç§¯æè¯†åˆ«ç¯å¢ƒä¸Šä¸‹æ–‡ä¸­çš„å·®è·å¹¶å¯»æ±‚æ¾„æ¸…ã€‚</li>
<li>SituationalLLMçš„è¡Œä¸ºæºäºå¯¹SAD-Instructçš„è®­ç»ƒï¼Œç»“åˆäº†åœºæ™¯ç‰¹å®šçš„åœºæ™¯å›¾å’ŒåŸºäºå¯¹è¯çš„ç»†åŒ–ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSituationalLLMåœ¨ä»»åŠ¡ç‰¹å¼‚æ€§ã€å¯é æ€§å’Œé€‚åº”æ€§æ–¹é¢ä¼˜äºé€šç”¨LLMåŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.13302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2406.13302v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2406.13302v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2406.13302v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2406.13302v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2406.13302v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2406.13302v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2406.13302v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Judging-the-Judges-Evaluating-Alignment-and-Vulnerabilities-in-LLMs-as-Judges"><a href="#Judging-the-Judges-Evaluating-Alignment-and-Vulnerabilities-in-LLMs-as-Judges" class="headerlink" title="Judging the Judges: Evaluating Alignment and Vulnerabilities in   LLMs-as-Judges"></a>Judging the Judges: Evaluating Alignment and Vulnerabilities in   LLMs-as-Judges</h2><p><strong>Authors:Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, Dieuwke Hupkes</strong></p>
<p>Offering a promising solution to the scalability challenges associated with human evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an approach to evaluating large language models (LLMs). However, there are still many open questions about the strengths and weaknesses of this paradigm, and what potential biases it may hold. In this paper, we present a comprehensive study of the performance of various LLMs acting as judges, focusing on a clean scenario in which inter-human agreement is high. Investigating thirteen judge models of different model sizes and families, judging answers of nine different â€˜examtaker modelsâ€™ - both base and instruction-tuned - we find that only the best (and largest) models achieve reasonable alignment with humans. However, they are still quite far behind inter-human agreement and their assigned scores may still differ with up to 5 points from human-assigned scores. In terms of their ranking of the nine exam-taker models, instead, also smaller models and even the lexical metric contains may provide a reasonable signal. Through error analysis and other studies, we identify vulnerabilities in judge models, such as their sensitivity to prompt complexity and length, and a tendency toward leniency. The fact that even the best judges differ from humans in this comparatively simple setup suggest that caution may be wise when using judges in more complex setups. Lastly, our research rediscovers the importance of using alignment metrics beyond simple percent alignment, showing that judges with high percent agreement can still assign vastly different scores. </p>
<blockquote>
<p>é’ˆå¯¹ä¸äººç±»è¯„ä¼°ç›¸å…³çš„å¯æ‰©å±•æ€§æŒ‘æˆ˜ï¼ŒLLMä½œä¸ºè¯„åˆ¤è€…çš„èŒƒå¼æä¾›äº†ä¸€ç§å‰æ™¯å…‰æ˜çš„è§£å†³æ–¹æ¡ˆï¼Œå®ƒä½œä¸ºä¸€ç§è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•æ­£è¿…é€Ÿè·å¾—æ”¯æŒã€‚ç„¶è€Œï¼Œå…³äºè¯¥èŒƒå¼çš„ä¼˜ç‚¹å’Œç¼ºç‚¹ï¼Œä»¥åŠå®ƒå¯èƒ½å­˜åœ¨çš„æ½œåœ¨åè§ï¼Œä»æœ‰è®¸å¤šæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹ä½œä¸ºè¯„åˆ¤è€…çš„å„ç§LLMçš„æ€§èƒ½è¿›è¡Œäº†å…¨é¢ç ”ç©¶ï¼Œé‡ç‚¹æ˜¯ä¸€ä¸ªäººç±»ä¹‹é—´å…±è¯†ç¨‹åº¦è¾ƒé«˜çš„å¹²å‡€åœºæ™¯ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†13ä¸ªä¸åŒè§„æ¨¡å’Œå®¶æ—çš„è¯„åˆ¤æ¨¡å‹ï¼Œå¯¹ä¹ä¸ªä¸åŒå‚åŠ è€ƒè¯•æ¨¡å‹çš„ç­”æ¡ˆè¿›è¡Œè¯„åˆ¤â€”â€”åŒ…æ‹¬åŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤è°ƒä¼˜æ¨¡å‹â€”â€”æˆ‘ä»¬å‘ç°åªæœ‰æœ€å¥½çš„ï¼ˆä¹Ÿæ˜¯æœ€å¤§çš„ï¼‰æ¨¡å‹æ‰èƒ½å®ç°ä¸äººç±»çš„åˆç†å¯¹é½ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶è¿œè¿œè½åäºäººç±»ä¹‹é—´çš„å…±è¯†ï¼Œå®ƒä»¬ç»™å‡ºçš„åˆ†æ•°ä¸äººç±»ç»™å‡ºçš„åˆ†æ•°å¯èƒ½ç›¸å·®é«˜è¾¾5åˆ†ã€‚è‡³äºå®ƒä»¬å¯¹ä¹ä¸ªå‚åŠ è€ƒè¯•æ¨¡å‹çš„æ’åï¼Œè¾ƒå°æ¨¡å‹ç”šè‡³æ˜¯è¯æ±‡æŒ‡æ ‡ä¹Ÿå¯èƒ½æä¾›åˆç†çš„ä¿¡å·ã€‚é€šè¿‡é”™è¯¯åˆ†æå’Œå…¶ä»–ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°äº†è¯„åˆ¤æ¨¡å‹å­˜åœ¨çš„æ¼æ´ï¼Œå¦‚å®ƒä»¬å¯¹æç¤ºçš„å¤æ‚æ€§å’Œé•¿åº¦çš„æ•æ„Ÿæ€§ï¼Œä»¥åŠå€¾å‘äºå®½å®¹çš„è¶‹åŠ¿ã€‚å³ä½¿åœ¨ç›¸å¯¹ç®€å•çš„è®¾ç½®ä¸­ï¼Œå³ä½¿æœ€å¥½çš„è¯„åˆ¤è€…ä¹Ÿä¸äººç±»å­˜åœ¨å·®å¼‚ï¼Œè¿™è¡¨æ˜åœ¨æ›´å¤æ‚çš„è®¾ç½®ä¸­ä½¿ç”¨è¯„åˆ¤è€…æ—¶å¯èƒ½éœ€è¦è°¨æ…ã€‚æœ€åï¼Œæˆ‘ä»¬çš„ç ”ç©¶é‡æ–°å‘ç°äº†ä½¿ç”¨è¶…è¶Šç®€å•ç™¾åˆ†æ¯”å¯¹é½çš„å¯¹é½æŒ‡æ ‡çš„é‡è¦æ€§ï¼Œè¡¨æ˜å³ä½¿ç™¾åˆ†æ¯”åè®®å¾ˆé«˜çš„è¯„åˆ¤è€…ä»ç„¶å¯ä»¥åˆ†é…æˆªç„¶ä¸åŒçš„åˆ†æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.12624v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMä½œä¸ºè¯„ä¼°è€…çš„æ¨¡å¼åœ¨è§£å†³è¯­è¨€æ¨¡å‹è¯„ä¼°çš„å¯æ‰©å±•æ€§æŒ‘æˆ˜æ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚æœ¬æ–‡å¯¹åœ¨ä¸åŒæ¨¡å‹å¤§å°å’Œå®¶æ—ä¸­çš„LLMè¿›è¡Œè¯„ä¼°ï¼Œå‘ç°åœ¨é«˜äººé—´å…±è¯†çš„åœºæ™¯ä¸‹ï¼Œä»…æœ€å¥½çš„å¤§å‹æ¨¡å‹æ‰èƒ½ä¸äººç±»å®ç°åˆç†å¯¹é½ã€‚ä½†å³ä¾¿å¦‚æ­¤ï¼Œè¿™äº›æ¨¡å‹ä»è½åäºäººé™…å…±è¯†ï¼Œè¯„åˆ†å·®è·æœ€é«˜è¾¾äº”ç‚¹ã€‚æ’åæ–¹é¢ï¼Œè¾ƒå°æ¨¡å‹ç”šè‡³è¯æ±‡åº¦é‡æ ‡å‡†ä¹Ÿå¯æä¾›åˆç†ä¿¡å·ã€‚ç ”ç©¶é€šè¿‡é”™è¯¯åˆ†æç­‰æ–¹å¼å‘ç°LLMè¯„ä¼°æ¨¡å‹çš„è„†å¼±æ€§ï¼Œå¦‚å—æç¤ºå¤æ‚æ€§å’Œé•¿åº¦çš„å½±å“ä»¥åŠå€¾å‘å®½æ¾çš„è¯„åˆ†ç­‰ã€‚å³ä½¿åœ¨ç›¸å¯¹ç®€å•çš„åœºæ™¯ä¸‹ï¼Œæœ€ä½³æ¨¡å‹ä¹Ÿæ— æ³•å®Œå…¨æ¨¡æ‹Ÿäººç±»è¯„ä¼°ï¼Œå› æ­¤åœ¨æ›´å¤æ‚çš„ç¯å¢ƒä¸­ä½¿ç”¨LLMä½œä¸ºè¯„ä¼°è€…éœ€è°¨æ…ã€‚ç ”ç©¶é‡ç”³ä½¿ç”¨è¶…è¶Šç®€å•ç™¾åˆ†æ¯”å¯¹é½çš„æ ¡å‡†æŒ‡æ ‡çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-as-a-judgeæ¨¡å¼åœ¨è§£å†³è¯­è¨€æ¨¡å‹è¯„ä¼°å¯æ‰©å±•æ€§é—®é¢˜ä¸Šå±•ç°æ½œåŠ›ã€‚</li>
<li>åœ¨é«˜äººé—´å…±è¯†åœºæ™¯ä¸‹ï¼Œä»…æœ€å¥½çš„å¤§å‹LLMèƒ½åˆç†å¯¹é½äººç±»è¯„ä»·ã€‚</li>
<li>å³ä½¿æ˜¯æœ€å¥½çš„LLMè¯„ä¼°æ¨¡å‹ä»ä¸äººé™…å…±è¯†å­˜åœ¨å·®è·ï¼Œè¯„åˆ†å·®è·æœ€é«˜è¾¾äº”ç‚¹ã€‚</li>
<li>åœ¨æ’åæ–¹é¢ï¼Œè¾ƒå°æ¨¡å‹ç”šè‡³è¯æ±‡åº¦é‡æ ‡å‡†ä¹Ÿèƒ½æä¾›åˆç†å‚è€ƒã€‚</li>
<li>LLMè¯„ä¼°æ¨¡å‹å­˜åœ¨è„†å¼±æ€§ï¼Œå¦‚å—æç¤ºå¤æ‚æ€§å’Œé•¿åº¦çš„å½±å“ä»¥åŠå€¾å‘å®½æ¾çš„è¯„åˆ†ã€‚</li>
<li>åœ¨å¤æ‚ç¯å¢ƒä¸­ä½¿ç”¨LLMä½œä¸ºè¯„ä¼°è€…éœ€è°¨æ…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.12624">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2406.12624v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2406.12624v5/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2406.12624v5/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2406.12624v5/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="List-Items-One-by-One-A-New-Data-Source-and-Learning-Paradigm-for-Multimodal-LLMs"><a href="#List-Items-One-by-One-A-New-Data-Source-and-Learning-Paradigm-for-Multimodal-LLMs" class="headerlink" title="List Items One by One: A New Data Source and Learning Paradigm for   Multimodal LLMs"></a>List Items One by One: A New Data Source and Learning Paradigm for   Multimodal LLMs</h2><p><strong>Authors:An Yan, Zhengyuan Yang, Junda Wu, Wanrong Zhu, Jianwei Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Julian McAuley, Jianfeng Gao, Lijuan Wang</strong></p>
<p>Set-of-Mark (SoM) Prompting unleashes the visual grounding capability of GPT-4V, by enabling the model to associate visual objects with tags inserted on the image. These tags, marked with alphanumerics, can be indexed via text tokens for easy reference. Despite the extraordinary performance from GPT-4V, we observe that other Multimodal Large Language Models (MLLMs) struggle to understand these visual tags. To promote the learning of SoM prompting for open-source models, we propose a new learning paradigm: â€œlist items one by one,â€ which asks the model to enumerate and describe all visual tags placed on the image following the alphanumeric orders of tags. By integrating our curated dataset with other visual instruction tuning datasets, we are able to equip existing MLLMs with the SoM prompting ability. Furthermore, we evaluate our finetuned SoM models on five MLLM benchmarks. We find that this new dataset, even in a relatively small size (10k-30k images with tags), significantly enhances visual reasoning capabilities and reduces hallucinations for MLLMs. Perhaps surprisingly, these improvements persist even when the visual tags are omitted from input images during inference. This suggests the potential of â€œlist items one by oneâ€ as a new paradigm for training MLLMs, which strengthens the object-text alignment through the use of visual tags in the training stage. Finally, we conduct analyses by probing trained models to understand the working mechanism of SoM. Our code and data are available at \url{<a target="_blank" rel="noopener" href="https://github.com/zzxslp/SoM-LLaVA%7D">https://github.com/zzxslp/SoM-LLaVA}</a>. </p>
<blockquote>
<p>é€šè¿‡å¯ç”¨æ¨¡å‹å°†è§†è§‰å¯¹è±¡ä¸å›¾åƒä¸­æ’å…¥çš„æ ‡ç­¾ç›¸å…³è”ï¼ŒSet-of-Mark (SoM) æç¤ºé‡Šæ”¾äº†GPT-4Vçš„è§†è§‰å®šä½èƒ½åŠ›ã€‚è¿™äº›å¸¦æœ‰å­—æ¯æ•°å­—çš„æ ‡ç­¾å¯ä»¥é€šè¿‡æ–‡æœ¬æ ‡è®°è¿›è¡Œç´¢å¼•ï¼Œä¾¿äºå‚è€ƒã€‚å°½ç®¡GPT-4Vçš„è¡¨ç°éå¸¸å‡ºè‰²ï¼Œä½†æˆ‘ä»¬å‘ç°å…¶ä»–å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¾ˆéš¾ç†è§£è¿™äº›è§†è§‰æ ‡ç­¾ã€‚ä¸ºäº†ä¿ƒè¿›å¼€æºæ¨¡å‹çš„SoMæç¤ºå­¦ä¹ ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ èŒƒå¼ï¼šâ€œé€ä¸€åˆ—å‡ºé¡¹ç›®â€ï¼Œè¦æ±‚æ¨¡å‹æŒ‰ç…§æ ‡ç­¾çš„å­—æ¯æ•°å­—é¡ºåºæšä¸¾å¹¶æè¿°å›¾åƒä¸Šæ”¾ç½®çš„æ‰€æœ‰è§†è§‰æ ‡ç­¾ã€‚é€šè¿‡å°†æˆ‘ä»¬ç²¾é€‰çš„æ•°æ®é›†ä¸å…¶ä»–è§†è§‰æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ç›¸ç»“åˆï¼Œæˆ‘ä»¬èƒ½å¤Ÿèµ‹äºˆç°æœ‰MLLMsä»¥SoMæç¤ºèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨äº”ä¸ªMLLMåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†æˆ‘ä»¬å¾®è°ƒè¿‡çš„SoMæ¨¡å‹ã€‚æˆ‘ä»¬å‘ç°ï¼Œå³ä½¿åœ¨æ–°çš„æ•°æ®é›†ä¸­å›¾åƒæ ‡ç­¾è§„æ¨¡ç›¸å¯¹è¾ƒå°ï¼ˆå¸¦æœ‰æ ‡ç­¾çš„10ä¸‡è‡³30ä¸‡å¼ å›¾åƒï¼‰ï¼Œä¹Ÿèƒ½æ˜¾è‘—æé«˜MLLMçš„è§†è§‰æ¨ç†èƒ½åŠ›å¹¶å‡å°‘å¹»è§‰ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå³ä½¿åœ¨æ¨ç†é˜¶æ®µçœç•¥è¾“å…¥å›¾åƒçš„è§†è§‰æ ‡ç­¾ï¼Œè¿™äº›æ”¹è¿›ä»ç„¶æŒç»­å­˜åœ¨ã€‚è¿™è¡¨æ˜â€œé€ä¸€åˆ—å‡ºé¡¹ç›®â€ä½œä¸ºè®­ç»ƒMLLMçš„æ–°èŒƒå¼å…·æœ‰æ½œåŠ›ï¼Œé€šè¿‡è®­ç»ƒé˜¶æ®µä½¿ç”¨è§†è§‰æ ‡ç­¾æ¥åŠ å¼ºå¯¹è±¡æ–‡æœ¬å¯¹é½ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡æ¢æµ‹è®­ç»ƒæ¨¡å‹æ¥åˆ†æSoMçš„å·¥ä½œåŸç†ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨\url{<a target="_blank" rel="noopener" href="https://github.com/zzxslp/SoM-LLaVA%7D%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/zzxslp/SoM-LLaVA}è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.16375v2">PDF</a> published at COLM-2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Set-of-Markï¼ˆSoMï¼‰æç¤ºå¯¹GPT-4Vçš„è§†è§‰å®šä½èƒ½åŠ›çš„é‡Šæ”¾ä½œç”¨ã€‚é€šè¿‡å›¾åƒä¸­æ’å…¥çš„è§†è§‰å¯¹è±¡æ ‡ç­¾ä¸æ¨¡å‹å…³è”ï¼Œè¿™äº›æ ‡ç­¾ä»¥æ•°å­—æ ‡è®°ï¼Œå¯é€šè¿‡æ–‡æœ¬ä»¤ç‰Œè¿›è¡Œç´¢å¼•ä»¥ä¾¿å‚è€ƒã€‚å°½ç®¡GPT-4Vè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å…¶ä»–å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç†è§£è¿™äº›è§†è§‰æ ‡ç­¾æ—¶é‡åˆ°å›°éš¾ã€‚ä¸ºä¿ƒè¿›å¼€æºæ¨¡å‹çš„SoMæç¤ºå­¦ä¹ ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ æ¨¡å¼ï¼šâ€œé€ä¸€åˆ—å‡ºé¡¹ç›®â€ï¼Œè¦æ±‚æ¨¡å‹æŒ‰æ•°å­—é¡ºåºæšä¸¾å¹¶æè¿°å›¾åƒä¸Šæ”¾ç½®çš„æ‰€æœ‰è§†è§‰æ ‡ç­¾ã€‚é€šè¿‡æ•´åˆæˆ‘ä»¬çš„å®šåˆ¶æ•°æ®é›†ä¸å…¶ä»–è§†è§‰æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä¸ºç°æœ‰çš„MLLMsé…å¤‡SoMæç¤ºèƒ½åŠ›ã€‚è¯„ä¼°å‘ç°ï¼Œå³ä½¿åœ¨æ–°æ•°æ®é›†è§„æ¨¡ç›¸å¯¹è¾ƒå°çš„æƒ…å†µä¸‹ï¼ˆå¸¦æœ‰æ ‡ç­¾çš„1ä¸‡è‡³3ä¸‡å¼ å›¾åƒï¼‰ï¼Œä¹Ÿèƒ½æ˜¾è‘—æé«˜MLLMsçš„è§†è§‰æ¨ç†èƒ½åŠ›å¹¶å‡å°‘å¹»è§‰ã€‚æ›´ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå³ä½¿åœ¨æ¨ç†é˜¶æ®µçœç•¥è§†è§‰æ ‡ç­¾ï¼Œè¿™äº›æ”¹è¿›ä¾ç„¶å­˜åœ¨ã€‚è¿™è¡¨æ˜â€œé€ä¸€åˆ—å‡ºé¡¹ç›®â€ä½œä¸ºè®­ç»ƒMLLMsçš„æ–°æ¨¡å¼çš„æ½œåŠ›ï¼Œé€šè¿‡è®­ç»ƒé˜¶æ®µä½¿ç”¨è§†è§‰æ ‡ç­¾æ¥åŠ å¼ºå¯¹è±¡æ–‡æœ¬å¯¹é½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Set-of-Mark (SoM) æç¤ºèƒ½å¤Ÿé‡Šæ”¾GPT-4Vçš„è§†è§‰å®šä½èƒ½åŠ›ï¼Œé€šè¿‡ä¸å›¾åƒä¸­çš„æ ‡ç­¾å…³è”å®ç°ã€‚</li>
<li>SoMæç¤ºåˆ©ç”¨æ•°å­—æ ‡è®°çš„è§†è§‰æ ‡ç­¾ï¼Œé€šè¿‡æ–‡æœ¬ä»¤ç‰Œè¿›è¡Œç´¢å¼•ï¼Œä¾¿äºæ¨¡å‹è¯†åˆ«ã€‚</li>
<li>å…¶ä»–å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç†è§£è§†è§‰æ ‡ç­¾æ—¶å­˜åœ¨å›°éš¾ã€‚</li>
<li>æå‡ºäº†â€œé€ä¸€åˆ—å‡ºé¡¹ç›®â€çš„æ–°å­¦ä¹ æ¨¡å¼ï¼Œä¿ƒè¿›MLLMså¯¹è§†è§‰æ ‡ç­¾çš„å­¦ä¹ ä¸ç†è§£ã€‚</li>
<li>é€šè¿‡æ•´åˆå®šåˆ¶æ•°æ®é›†ä¸å…¶ä»–è§†è§‰æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œèƒ½èµ‹äºˆç°æœ‰MLLMsä»¥SoMæç¤ºèƒ½åŠ›ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼Œæ–°æ•°æ®é›†ï¼ˆå³ä½¿è§„æ¨¡è¾ƒå°ï¼‰èƒ½æ˜¾è‘—æé«˜MLLMsçš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œå¹¶å‡å°‘å¹»è§‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.16375">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2404.16375v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2404.16375v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2404.16375v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2404.16375v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="UniGraph-Learning-a-Unified-Cross-Domain-Foundation-Model-for-Text-Attributed-Graphs"><a href="#UniGraph-Learning-a-Unified-Cross-Domain-Foundation-Model-for-Text-Attributed-Graphs" class="headerlink" title="UniGraph: Learning a Unified Cross-Domain Foundation Model for   Text-Attributed Graphs"></a>UniGraph: Learning a Unified Cross-Domain Foundation Model for   Text-Attributed Graphs</h2><p><strong>Authors:Yufei He, Yuan Sui, Xiaoxin He, Bryan Hooi</strong></p>
<p>Foundation models like ChatGPT and GPT-4 have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives. However, graph learning has predominantly focused on single-graph models, tailored to specific tasks or datasets, lacking the ability to transfer learned knowledge to different domains. This limitation stems from the inherent complexity and diversity of graph structures, along with the different feature and label spaces specific to graph data. In this paper, we recognize text as an effective unifying medium and employ Text-Attributed Graphs (TAGs) to leverage this potential. We present our UniGraph framework, designed to learn a foundation model for TAGs, which is capable of generalizing to unseen graphs and tasks across diverse domains. Unlike single-graph models that use pre-computed node features of varying dimensions as input, our approach leverages textual features for unifying node representations, even for graphs such as molecular graphs that do not naturally have textual features. We propose a novel cascaded architecture of Language Models (LMs) and Graph Neural Networks (GNNs) as backbone networks. Additionally, we propose the first pre-training algorithm specifically designed for large-scale self-supervised learning on TAGs, based on Masked Graph Modeling. We introduce graph instruction tuning using Large Language Models (LLMs) to enable zero-shot prediction ability. Our comprehensive experiments across various graph learning tasks and domains demonstrate the modelâ€™s effectiveness in self-supervised representation learning on unseen graphs, few-shot in-context transfer, and zero-shot transfer, even surpassing or matching the performance of GNNs that have undergone supervised training on target datasets. </p>
<blockquote>
<p>ç±»ä¼¼ChatGPTå’ŒGPT-4çš„åŸºçŸ³æ¨¡å‹å·²ç»å½»åº•æ”¹å˜äº†äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œå±•ç°å‡ºåœ¨å¹¿æ³›çš„ä»»åŠ¡å’Œåº”ç”¨ä¸­è¶…è¶Šå…¶åˆå§‹è®­ç»ƒç›®æ ‡çš„å“è¶Šæ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå›¾å­¦ä¹ ä¸»è¦å…³æ³¨äºå•ä¸€å›¾æ¨¡å‹ï¼Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡æˆ–æ•°æ®é›†ï¼Œç¼ºä¹å°†æ‰€å­¦çŸ¥è¯†è½¬ç§»åˆ°ä¸åŒé¢†åŸŸçš„èƒ½åŠ›ã€‚è¿™ä¸€é™åˆ¶æºäºå›¾ç»“æ„æœ¬èº«çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ï¼Œä»¥åŠå›¾æ•°æ®ç‰¹æœ‰çš„ç‰¹å¾å’Œæ ‡ç­¾ç©ºé—´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è®¤è¯†åˆ°æ–‡æœ¬æ˜¯ä¸€ç§æœ‰æ•ˆçš„ç»Ÿä¸€åª’ä»‹ï¼Œå¹¶é‡‡ç”¨å¸¦æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGsï¼‰æ¥åˆ©ç”¨è¿™ä¸€æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†UniGraphæ¡†æ¶ï¼Œæ—¨åœ¨å­¦ä¹ ç”¨äºTAGsçš„åŸºçŸ³æ¨¡å‹ï¼Œèƒ½å¤Ÿæ¨å¹¿åˆ°æœªè§è¿‡çš„å›¾å’Œè·¨ä¸åŒé¢†åŸŸçš„ä»»åŠ¡ã€‚ä¸åŒäºä½¿ç”¨é¢„è®¡ç®—èŠ‚ç‚¹ç‰¹å¾ä½œä¸ºè¾“å…¥çš„å•ä¸€å›¾æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æ–‡æœ¬ç‰¹å¾æ¥ç»Ÿä¸€èŠ‚ç‚¹è¡¨ç¤ºï¼Œå³ä½¿å¯¹äºæ²¡æœ‰è‡ªç„¶æ–‡æœ¬ç‰¹å¾çš„å›¾å½¢ï¼ˆå¦‚åˆ†å­å›¾å½¢ï¼‰ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬æå‡ºäº†è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰å’Œå›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰çš„æ–°å‹çº§è”æ¶æ„ä½œä¸ºéª¨å¹²ç½‘ç»œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸“ä¸ºå¤§è§„æ¨¡è‡ªç›‘ç£å­¦ä¹ åœ¨TAGsä¸Šè®¾è®¡çš„ç¬¬ä¸€ä¸ªé¢„è®­ç»ƒç®—æ³•ï¼ŒåŸºäºæ©ç å›¾å»ºæ¨¡ã€‚æˆ‘ä»¬å¼•å…¥ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å›¾æŒ‡ä»¤å¾®è°ƒï¼Œä»¥å®ç°é›¶æ ·æœ¬é¢„æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨å„ç§å›¾å­¦ä¹ ä»»åŠ¡å’Œé¢†åŸŸè¿›è¡Œçš„å…¨é¢å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æœªè§å›¾çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ã€å°‘æ ·æœ¬ä¸Šä¸‹æ–‡è¿ç§»å’Œé›¶æ ·æœ¬è¿ç§»æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œç”šè‡³è¶…è¶Šæˆ–åŒ¹é…äº†åœ¨ç›®æ ‡æ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£è®­ç»ƒçš„GNNsçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.13630v3">PDF</a> KDD 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†Graph AIé¢†åŸŸçš„æœ€æ–°å‘å±•ï¼ŒæŒ‡å‡ºäº†å•å›¾æ¨¡å‹å—é™äºå…¶å›ºå®šçš„åº”ç”¨åœºæ™¯å’ŒåŸŸç‰¹å®šæ€§é—®é¢˜ã€‚é€šè¿‡èåˆæ–‡æœ¬ç‰¹å¾çš„ç»Ÿä¸€è¡¨ç¤ºæ³•æ¥è§£å†³è¯¥é—®é¢˜ï¼Œå¼•å…¥äº†UniGraphæ¡†æ¶ï¼Œé€šè¿‡é‡‡ç”¨å¤§å‹é¢„è®­ç»ƒæ¨¡å‹æŠ€æœ¯æ¥åº”å¯¹å›¾çš„å¤æ‚æ€§ï¼Œè¿›è€Œå®ç°åœ¨å¤šä¸ªå›¾å­¦ä¹ ä»»åŠ¡çš„æœªè§æ•°æ®å’Œä¸åŒåŸŸä¸­çš„é›¶å’Œå°‘é‡æ ·æœ¬è½¬ç§»å­¦ä¹ æ€§èƒ½çš„æé«˜ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè¯¥æ–‡æ‰€é˜è¿°çš„æ–°æ¶æ„å±•ç°äº†ä¸€ç§é¢å‘å›¾æ•°æ®çš„æ ‡å‡†åŒ–ã€è‡ªç›‘ç£çš„è®­ç»ƒèŒƒå¼ï¼Œä»è€Œå®ç°ç»Ÿä¸€çš„è¡¨ç¤ºå­¦ä¹ å’Œè·¨åŸŸè¿ç§»å­¦ä¹ ã€‚è¿™ä¸€çªç ´æ€§çš„æŠ€æœ¯é©æ–°å°†æ¨åŠ¨Graph AIé¢†åŸŸçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å•å›¾æ¨¡å‹å—é™äºç‰¹å®šä»»åŠ¡å’ŒåŸŸï¼Œç¼ºä¹è·¨åŸŸçŸ¥è¯†è¿ç§»èƒ½åŠ›ã€‚</li>
<li>æ–‡æœ¬æ˜¯ä¸€ç§æœ‰æ•ˆçš„ç»Ÿä¸€åª’ä»‹æ¥æ„å»ºå¤šæºæ•°æ®çš„å…³è”ï¼Œå¹¶ä¸”è¯¥æ–‡å°è¯•é‡‡ç”¨æ–‡æœ¬ç‰¹å¾æ¥å¯¹å„ç§èŠ‚ç‚¹çš„è¡¨å¾è¿›è¡Œç»Ÿä¸€è¡¨è¾¾ã€‚é’ˆå¯¹æ²¡æœ‰è‡ªç„¶æ–‡æœ¬ç‰¹å¾çš„å›¾å½¢æ•°æ®ï¼ˆå¦‚åˆ†å­å›¾ï¼‰ï¼Œæå‡ºäº†ä¸€ç§è§£å†³æ–¹æ¡ˆã€‚</li>
<li>UniGraphæ¡†æ¶å¼•å…¥äº†ä¸€ç§æ–°çš„å¤§å‹é¢„è®­ç»ƒæ¨¡å‹æŠ€æœ¯ï¼Œç”¨äºå­¦ä¹ è·¨ä¸åŒåŸŸçš„æœªè§å›¾å½¢çš„é€šç”¨è¡¨ç¤ºã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆè¯­è¨€æ¨¡å‹å’Œå›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰çš„çº§è”æ¶æ„æ¥å®ç°è¿™ä¸€ç‚¹ã€‚è¿™ä¸€æ¶æ„å®ç°äº†è·¨å¤šä¸ªä»»åŠ¡çš„é€šç”¨èƒ½åŠ›ã€‚å…¶åœ¨æœªæ ‡æ•°æ®é›†å’Œå‡ ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„å›¾ä¸Šå±•ç°å‡ºäº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚æ–‡ç« ç‰¹åˆ«å¼ºè°ƒäº†å…¶å¯¹äºåˆ†å­å›¾çš„é€‚ç”¨æ€§ã€‚</li>
<li>è¯¥æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGsï¼‰çš„é¢„è®­ç»ƒç®—æ³•ï¼Œè¯¥ç®—æ³•åŸºäºæ©ç å›¾å»ºæ¨¡è¿›è¡Œå¤§è§„æ¨¡è‡ªç›‘ç£å­¦ä¹ ã€‚å®ƒæå‡ºäº†å›¾æŒ‡ä»¤å¾®è°ƒï¼ˆGITï¼‰çš„æ–¹æ³•æ¥å®ç°é›¶æ ·æœ¬é¢„æµ‹èƒ½åŠ›ã€‚è¿™è¡¨æ˜å®ƒä¸ä»…èƒ½å¤Ÿå¤„ç†å·²æ ‡è®°çš„æ•°æ®ï¼Œè¿˜èƒ½å¤„ç†æœªæ ‡è®°çš„æ•°æ®ã€‚è¿™ä¸€åˆ›æ–°ä¸ºè‡ªç›‘ç£å­¦ä¹ åœ¨å›¾å½¢æ•°æ®ä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•è®ºæŒ‡å¯¼ã€‚è¿™ä¸ä»…å¤§å¤§æé«˜äº†æ¨¡å‹çš„çµæ´»æ€§è€Œä¸”èƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹çš„æ•ˆæœã€‚æ¨¡å‹è¿˜å¯ä»¥è§£å†³æå°‘æ ·æœ¬æƒ…å†µä¸‹çš„ä¸Šä¸‹æ–‡è½¬ç§»é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.13630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2402.13630v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2402.13630v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2402.13630v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2402.13630v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Investigating-Recurrent-Transformers-with-Dynamic-Halt"><a href="#Investigating-Recurrent-Transformers-with-Dynamic-Halt" class="headerlink" title="Investigating Recurrent Transformers with Dynamic Halt"></a>Investigating Recurrent Transformers with Dynamic Halt</h2><p><strong>Authors:Jishnu Ray Chowdhury, Cornelia Caragea</strong></p>
<p>In this paper, we comprehensively study the inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism: (1) the approach of incorporating a depth-wise recurrence similar to Universal Transformers; and (2) the approach of incorporating a chunk-wise temporal recurrence like Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways to extend and combine the above methods - for example, we propose a global mean-based dynamic halting mechanism for Universal Transformers and an augmentation of Temporal Latent Bottleneck with elements from Universal Transformer. We compare the models and probe their inductive biases in several diagnostic tasks, such as Long Range Arena (LRA), flip-flop language modeling, ListOps, and Logical Inference. The code is released in: <a target="_blank" rel="noopener" href="https://github.com/JRC1995/InvestigatingRecurrentTransformers/tree/main">https://github.com/JRC1995/InvestigatingRecurrentTransformers/tree/main</a> </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å…¨é¢ç ”ç©¶äº†å°†Transformerä¸å¾ªç¯æœºåˆ¶ç›¸ç»“åˆçš„ä¸¤ç§ä¸»è¦æ–¹æ³•çš„å½’çº³åè§ï¼šï¼ˆ1ï¼‰é‡‡ç”¨ç±»ä¼¼äºUniversal Transformerçš„æ·±åº¦é€’å½’æ–¹æ³•ï¼›ï¼ˆ2ï¼‰é‡‡ç”¨ç±»ä¼¼äºTemporal Latent Bottleneckçš„åˆ†å—æ—¶é—´é€’å½’æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†æ‰©å±•å’Œç»“åˆä¸Šè¿°æ–¹æ³•çš„æ–°æ–¹å¼â€”â€”ä¾‹å¦‚ï¼Œæˆ‘ä»¬ä¸ºUniversal Transformeræå‡ºäº†åŸºäºå…¨å±€å¹³å‡å€¼çš„åŠ¨æ€åœæ­¢æœºåˆ¶ï¼Œä»¥åŠèåˆäº†Temporal Latent Bottleneckä¸­ä¸€äº›æ¥è‡ªUniversal Transformerçš„å…ƒç´ ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªè¯Šæ–­ä»»åŠ¡ä¸­æ¯”è¾ƒäº†è¿™äº›æ¨¡å‹å¹¶æ¢ç©¶äº†å…¶å½’çº³åè§ï¼ŒåŒ…æ‹¬é•¿èŒƒå›´åŒºåŸŸï¼ˆLRAï¼‰ã€flip-flopè¯­è¨€å»ºæ¨¡ã€ListOpså’Œé€»è¾‘æ¨ç†ç­‰ä»»åŠ¡ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/JRC1995/InvestigatingRecurrentTransformers/tree/main">é“¾æ¥åœ°å€</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.00976v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä¸¤ç§åœ¨Transformerä¸­å¼•å…¥å¾ªç¯æœºåˆ¶çš„ä¸»è¦æ–¹æ³•çš„å½’çº³åè§ï¼ŒåŒ…æ‹¬ç±»ä¼¼äºUniversal Transformerçš„æ·±åº¦é€’å½’æ–¹æ³•å’Œç±»ä¼¼äºTemporal Latent Bottleneckçš„å—çŠ¶æ—¶åºé€’å½’æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæå‡ºäº†æ‰©å±•å’Œç»“åˆä¸Šè¿°æ–¹æ³•çš„æ–°æ–¹å¼ï¼Œå¹¶æ¯”è¾ƒäº†è¿™äº›æ¨¡å‹åœ¨å¤šä¸ªè¯Šæ–­ä»»åŠ¡ä¸­çš„å½’çº³åè§ï¼Œå¦‚Long Range Arenaï¼ˆLRAï¼‰ã€flip-flopè¯­è¨€å»ºæ¨¡ã€ListOpså’Œé€»è¾‘æ¨ç†ç­‰ã€‚ä»£ç å·²å‘å¸ƒåœ¨ç›¸å…³é“¾æ¥ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äº†ä¸¤ç§ä¸»è¦æ–¹æ³•ä»¥åœ¨Transformerä¸­å¼•å…¥å¾ªç¯æœºåˆ¶ã€‚</li>
<li>ç¬¬ä¸€ç§æ–¹æ³•é‡‡ç”¨ç±»ä¼¼äºUniversal Transformerçš„æ·±åº¦é€’å½’æœºåˆ¶ã€‚</li>
<li>ç¬¬äºŒç§æ–¹æ³•é‡‡ç”¨ç±»ä¼¼äºTemporal Latent Bottleneckçš„å—çŠ¶æ—¶åºé€’å½’æœºåˆ¶ã€‚</li>
<li>æå‡ºäº†æ‰©å±•å’Œç»“åˆä¸Šè¿°æ–¹æ³•çš„æ–°æ–¹å¼ï¼ŒåŒ…æ‹¬å…¨çƒå‡å€¼åŠ¨æ€åœæ­¢æœºåˆ¶å’ŒTemporal Latent Bottleneckä¸Universal Transformerå…ƒç´ çš„èåˆã€‚</li>
<li>åœ¨å¤šä¸ªè¯Šæ–­ä»»åŠ¡ä¸­æ¯”è¾ƒäº†è¿™äº›æ¨¡å‹çš„å½’çº³åè§ã€‚</li>
<li>è¿™äº›è¯Šæ–­ä»»åŠ¡åŒ…æ‹¬Long Range Arenaï¼ˆLRAï¼‰ã€flip-flopè¯­è¨€å»ºæ¨¡ã€ListOpså’Œé€»è¾‘æ¨ç†ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.00976">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2402.00976v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2402.00976v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2402.00976v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="BarcodeBERT-Transformers-for-Biodiversity-Analysis"><a href="#BarcodeBERT-Transformers-for-Biodiversity-Analysis" class="headerlink" title="BarcodeBERT: Transformers for Biodiversity Analysis"></a>BarcodeBERT: Transformers for Biodiversity Analysis</h2><p><strong>Authors:Pablo Millan Arias, Niousha Sadjadi, Monireh Safari, ZeMing Gong, Austin T. Wang, Joakim Bruslund Haurum, Iuliia Zarubiieva, Dirk Steinke, Lila Kari, Angel X. Chang, Scott C. Lowe, Graham W. Taylor</strong></p>
<p>In the global challenge of understanding and characterizing biodiversity, short species-specific genomic sequences known as DNA barcodes play a critical role, enabling fine-grained comparisons among organisms within the same kingdom of life. Although machine learning algorithms specifically designed for the analysis of DNA barcodes are becoming more popular, most existing methodologies rely on generic supervised training algorithms. We introduce BarcodeBERT, a family of models tailored to biodiversity analysis and trained exclusively on data from a reference library of 1.5M invertebrate DNA barcodes. We compared the performance of BarcodeBERT on taxonomic identification tasks against a spectrum of machine learning approaches including supervised training of classical neural architectures and fine-tuning of general DNA foundation models. Our self-supervised pretraining strategies on domain-specific data outperform fine-tuned foundation models, especially in identification tasks involving lower taxa such as genera and species. We also compared BarcodeBERT with BLAST, one of the most widely used bioinformatics tools for sequence searching, and found that our method matched BLASTâ€™s performance in species-level classification while being 55 times faster. Our analysis of masking and tokenization strategies also provides practical guidance for building customized DNA language models, emphasizing the importance of aligning model training strategies with dataset characteristics and domain knowledge. The code repository is available at <a target="_blank" rel="noopener" href="https://github.com/bioscan-ml/BarcodeBERT">https://github.com/bioscan-ml/BarcodeBERT</a>. </p>
<blockquote>
<p>åœ¨å…¨çƒæ€§çš„ç”Ÿç‰©å¤šæ ·æ€§å’Œç‰¹æ€§è®¤çŸ¥æŒ‘æˆ˜ä¸­ï¼ŒDNAæ¡å½¢ç å‘æŒ¥äº†è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è¿™æ˜¯ä¸€ç§ç‰¹å®šç‰©ç§çš„ç®€çŸ­åŸºå› ç»„åºåˆ—ï¼Œå¯ä»¥ä½¿å¾—æˆ‘ä»¬åœ¨åŒä¸€ç”Ÿç‰©ç‹å›½ä¸­å¯¹ç”Ÿç‰©è¿›è¡Œç»†è‡´çš„å¯¹æ¯”ç ”ç©¶ã€‚è™½ç„¶ä¸ºDNAæ¡å½¢ç åˆ†æè€Œç‰¹åˆ«è®¾è®¡çš„æœºå™¨å­¦ä¹ ç®—æ³•æ­£è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä»ç„¶ä¾èµ–äºé€šç”¨çš„ç›‘ç£è®­ç»ƒç®—æ³•ã€‚æˆ‘ä»¬æ¨å‡ºäº†BarcodeBERTæ¨¡å‹å®¶æ—ï¼Œè¯¥æ¨¡å‹ä¸“ä¸ºç”Ÿç‰©å¤šæ ·æ€§åˆ†æé‡èº«å®šåˆ¶ï¼Œå¹¶ä»…åŸºäºæ¥è‡ªåŒ…å«150ä¸‡æ— è„Šæ¤åŠ¨ç‰©DNAæ¡å½¢ç å‚è€ƒåº“çš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬å¯¹BarcodeBERTåœ¨åˆ†ç±»é‰´å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸ä¸€ç³»åˆ—æœºå™¨å­¦ä¹ æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒï¼ŒåŒ…æ‹¬ç»å…¸ç¥ç»æ¶æ„çš„ç›‘ç£è®­ç»ƒå’Œé€šç”¨DNAåŸºç¡€æ¨¡å‹çš„å¾®è°ƒã€‚æˆ‘ä»¬åœ¨ç‰¹å®šé¢†åŸŸæ•°æ®ä¸Šçš„è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒç­–ç•¥åœ¨é‰´å®šä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç»è¿‡å¾®è°ƒçš„åŸºç¡€æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠè¾ƒä½åˆ†ç±»ç­‰çº§ï¼ˆå¦‚å±å’Œç§ï¼‰çš„é‰´å®šä»»åŠ¡ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚æˆ‘ä»¬è¿˜æ¯”è¾ƒäº†BarcodeBERTä¸BLASTï¼ˆä¸€ç§å¹¿æ³›ä½¿ç”¨çš„ç”¨äºåºåˆ—æœç´¢çš„ç”Ÿç‰©ä¿¡æ¯å­¦å·¥å…·ï¼‰ï¼Œå‘ç°æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç‰©ç§æ°´å¹³çš„åˆ†ç±»ä¸Šè¾¾åˆ°äº†BLASTçš„æ€§èƒ½æ°´å¹³ï¼ŒåŒæ—¶è¿è¡Œé€Ÿåº¦æ˜¯BLASTçš„55å€ã€‚æˆ‘ä»¬å¯¹æ©ç å’Œåˆ†è¯ç­–ç•¥çš„åˆ†æä¹Ÿä¸ºæ„å»ºå®šåˆ¶çš„DNAè¯­è¨€æ¨¡å‹æä¾›äº†å®é™…æŒ‡å¯¼ï¼Œå¼ºè°ƒäº†å¯¹æ¨¡å‹è®­ç»ƒç­–ç•¥ä¸æ•°æ®é›†ç‰¹å¾å’Œé¢†åŸŸçŸ¥è¯†ç›¸é€‚åº”çš„é‡è¦æ€§ã€‚ç›¸å…³ä»£ç ä»“åº“å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/bioscan-ml/BarcodeBERT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bioscan-ml/BarcodeBERTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02401v2">PDF</a> Main text: 14 pages, Total: 23 pages, 10 figures, formerly accepted   at the 4th Workshop on Self-Supervised Learning: Theory and Practice (NeurIPS   2023)</p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹å…¨çƒç”Ÿç‰©å¤šæ ·æ€§çš„ç†è§£å’Œç‰¹å¾æè¿°æŒ‘æˆ˜ï¼ŒDNAæ¡å½¢ç åœ¨ç‰©ç§é—´çš„ç²¾ç»†æ¯”è¾ƒä¸­æ‰®æ¼”å…³é”®è§’è‰²ã€‚æœ¬ç ”ç©¶å¼•å…¥BarcodeBERTæ¨¡å‹å®¶æ—ï¼Œä¸“é—¨ç”¨äºç”Ÿç‰©å¤šæ ·æ€§åˆ†æï¼Œå¹¶åœ¨åŒ…å«150ä¸‡æ— è„Šæ¤åŠ¨ç‰©DNAæ¡å½¢ç æ•°æ®çš„å‚è€ƒåº“ä¸Šè¿›è¡Œè®­ç»ƒã€‚ç›¸è¾ƒäºå…¶ä»–æœºå™¨å­¦ä¹ æ–¹æ³•ï¼ŒBarcodeBERTåœ¨åˆ†ç±»é‰´å®šä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾ƒä½åˆ†ç±»ç­‰çº§å¦‚å±å’Œç§çš„é‰´å®šä¸Šã€‚æ­¤å¤–ï¼ŒBarcodeBERTä¸å¹¿æ³›ä½¿ç”¨çš„ç”Ÿç‰©ä¿¡æ¯å­¦å·¥å…·BLASTç›¸æ¯”ï¼Œåœ¨ç‰©ç§æ°´å¹³åˆ†ç±»ä¸Šæ€§èƒ½ç›¸å½“ï¼Œä½†è¿è¡Œé€Ÿåº¦å¿«è¾¾55å€ã€‚è¯¥ç ”ç©¶è¿˜åˆ†æäº†æ©ç å’Œåˆ†è¯ç­–ç•¥ï¼Œä¸ºæ„å»ºå®šåˆ¶DNAè¯­è¨€æ¨¡å‹æä¾›äº†å®è·µæŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>DNAæ¡å½¢ç åœ¨ç”Ÿç‰©å¤šæ ·æ€§çš„ç²¾ç»†æ¯”è¾ƒä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>BarcodeBERTæ¨¡å‹æ˜¯ä¸ºç”Ÿç‰©å¤šæ ·æ€§åˆ†æé‡èº«å®šåˆ¶çš„æ¨¡å‹å®¶æ—ã€‚</li>
<li>BarcodeBERTåœ¨é‰´å®šä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å±å’Œç§çš„é‰´å®šä¸Šã€‚</li>
<li>BarcodeBERTä¸BLASTç›¸æ¯”ï¼Œç‰©ç§åˆ†ç±»æ€§èƒ½ç›¸å½“ä½†è¿è¡Œé€Ÿåº¦å¿«ã€‚</li>
<li>BarcodeBERTé‡‡ç”¨è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒç­–ç•¥ï¼Œé€‚ç”¨äºç‰¹å®šé¢†åŸŸçš„DNAæ•°æ®ã€‚</li>
<li>ç ”ç©¶åˆ†æäº†æ©ç å’Œåˆ†è¯ç­–ç•¥ï¼Œä¸ºæ„å»ºå®šåˆ¶DNAè¯­è¨€æ¨¡å‹æä¾›äº†æŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.02401">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2311.02401v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2311.02401v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_LLM/2311.02401v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-24/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-24/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-24/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1b5add3be22696aa2358e1944248651d.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-24  An Offline Multi-Agent Reinforcement Learning Framework for Radio   Resource Management
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-23/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3eb68bd8c8714ad1c6613ca2964d3ba5.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-23  EMO2 End-Effector Guided Audio-Driven Avatar Video Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">9730.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
