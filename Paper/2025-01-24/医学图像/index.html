<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-24  Learning accurate rigid registration for longitudinal brain MRI from   synthetic data">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2402.01034v3/page_3_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    69 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-24-æ›´æ–°"><a href="#2025-01-24-æ›´æ–°" class="headerlink" title="2025-01-24 æ›´æ–°"></a>2025-01-24 æ›´æ–°</h1><h2 id="Learning-accurate-rigid-registration-for-longitudinal-brain-MRI-from-synthetic-data"><a href="#Learning-accurate-rigid-registration-for-longitudinal-brain-MRI-from-synthetic-data" class="headerlink" title="Learning accurate rigid registration for longitudinal brain MRI from   synthetic data"></a>Learning accurate rigid registration for longitudinal brain MRI from   synthetic data</h2><p><strong>Authors:Jingru Fu, Adrian V. Dalca, Bruce Fischl, Rodrigo Moreno, Malte Hoffmann</strong></p>
<p>Rigid registration aims to determine the translations and rotations necessary to align features in a pair of images. While recent machine learning methods have become state-of-the-art for linear and deformable registration across subjects, they have demonstrated limitations when applied to longitudinal (within-subject) registration, where achieving precise alignment is critical. Building on an existing framework for anatomy-aware, acquisition-agnostic affine registration, we propose a model optimized for longitudinal, rigid brain registration. By training the model with synthetic within-subject pairs augmented with rigid and subtle nonlinear transforms, the model estimates more accurate rigid transforms than previous cross-subject networks and performs robustly on longitudinal registration pairs within and across magnetic resonance imaging (MRI) contrasts. </p>
<blockquote>
<p>åˆšæ€§é…å‡†æ—¨åœ¨ç¡®å®šå¯¹é½ä¸€å¯¹å›¾åƒæ‰€éœ€çš„å¹³ç§»å’Œæ—‹è½¬ã€‚è™½ç„¶æœ€è¿‘çš„æœºå™¨å­¦ä¹ æ–¹æ³•å·²æˆä¸ºè·¨ä¸»é¢˜è¿›è¡Œçº¿æ€§å’Œå¯å˜å½¢é…å‡†çš„æœ€æ–°æŠ€æœ¯ï¼Œä½†å®ƒä»¬åœ¨åº”ç”¨äºçºµå‘ï¼ˆå—è¯•è€…å†…éƒ¨ï¼‰é…å‡†æ—¶æ˜¾ç¤ºå‡ºå±€é™æ€§ï¼Œå…¶ä¸­å®ç°ç²¾ç¡®å¯¹é½è‡³å…³é‡è¦ã€‚åŸºäºç°æœ‰çš„è§£å‰–ç»“æ„æ„ŸçŸ¥ã€é‡‡é›†æ— å…³çš„ä»¿å°„é…å‡†æ¡†æ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹çºµå‘åˆšæ€§å¤§è„‘é…å‡†è¿›è¡Œä¼˜åŒ–æ¨¡å‹ã€‚é€šè¿‡å¯¹æ¨¡å‹è¿›è¡Œåˆæˆå—è¯•è€…å†…éƒ¨é…å¯¹è®­ç»ƒï¼Œå¹¶è¾…ä»¥åˆšæ€§å’Œç»†å¾®éçº¿æ€§å˜æ¢å¢å¼ºï¼Œè¯¥æ¨¡å‹ä¼°è®¡çš„åˆšæ€§å˜æ¢æ¯”ä»¥å‰è·¨å—è¯•è€…ç½‘ç»œæ›´å‡†ç¡®ï¼Œå¹¶ä¸”åœ¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å¯¹æ¯”å‰‚çš„çºµå‘é…å‡†å¯¹å†…å’Œè·¨å†…è¡¨ç°ç¨³å¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13010v1">PDF</a> 5 pages, 4 figures, 1 table, rigid image registration, deep learning,   longitudinal analysis, neuroimaging, accepted by the IEEE International   Symposium on Biomedical Imaging</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé…å‡†ä¸­ï¼Œåˆšæ€§é…å‡†æ—¨åœ¨ç¡®å®šå¯¹é½å›¾åƒç‰¹å¾æ‰€éœ€çš„å¹³ç§»å’Œæ—‹è½¬ã€‚å°½ç®¡æœ€æ–°çš„æœºå™¨å­¦ä¹ æ–¹æ³•æ˜¯è·¨ä¸»é¢˜çº¿æ€§å˜å½¢é…å‡†çš„æœ€ä½³é€‰æ‹©ï¼Œä½†åœ¨çºµå‘ï¼ˆå—è¯•è€…å†…éƒ¨ï¼‰é…å‡†ä¸­åº”ç”¨æ—¶è¡¨ç°å‡ºå±€é™æ€§ï¼Œå…¶ä¸­ç²¾ç¡®å¯¹é½è‡³å…³é‡è¦ã€‚åŸºäºè§£å‰–ç»“æ„æ„ŸçŸ¥ã€é‡‡é›†æ— å…³çš„ä»¿å°„é…å‡†ç°æœ‰æ¡†æ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹çºµå‘åˆšæ€§å¤§è„‘é…å‡†çš„ä¼˜åŒ–æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆæˆå—è¯•è€…å†…éƒ¨å¯¹å›¾åƒå¹¶ä½¿ç”¨åˆšæ€§å’Œå¾®å¦™éçº¿æ€§å˜æ¢è¿›è¡Œå¢å¼ºè®­ç»ƒï¼Œä¼°è®¡çš„åˆšæ€§å˜æ¢æ¯”ä»¥å¾€è·¨ä¸»é¢˜ç½‘ç»œçš„æ›´ä¸ºå‡†ç¡®ï¼Œå¹¶åœ¨MRIå¯¹æ¯”åº¦å†…éƒ¨å’Œè·¨å¯¹æ¯”åº¦çš„çºµå‘é…å‡†å¯¹ä¸Šè¡¨ç°ç¨³å¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆšæ€§é…å‡†ç”¨äºç¡®å®šå›¾åƒé—´çš„å¹³ç§»å’Œæ—‹è½¬ä»¥å®ç°å¯¹é½ã€‚</li>
<li>æœºå™¨å­¦ä¹ æ–¹æ³•æ˜¯è·¨ä¸»é¢˜çº¿æ€§å˜å½¢é…å‡†çš„ä¸»æµï¼Œä½†åœ¨çºµå‘é…å‡†ä¸­å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç²¾ç¡®å¯¹é½åœ¨çºµå‘é…å‡†ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>åŸºäºè§£å‰–ç»“æ„æ„ŸçŸ¥çš„ä»¿å°„é…å‡†æ¡†æ¶è¿›è¡Œäº†ä¼˜åŒ–ï¼Œé€‚ç”¨äºçºµå‘åˆšæ€§å¤§è„‘é…å‡†ã€‚</li>
<li>é€šè¿‡åˆæˆå—è¯•è€…å†…éƒ¨å›¾åƒå¯¹å¹¶ä½¿ç”¨åˆšæ€§å’Œéçº¿æ€§å˜æ¢å¢å¼ºè®­ç»ƒï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ä¼˜åŒ–åçš„æ¨¡å‹ä¼°è®¡çš„åˆšæ€§å˜æ¢æ¯”è·¨ä¸»é¢˜ç½‘ç»œæ›´å‡†ç¡®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13010">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-360fb33c739cc5daf556d15bcffc09b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b086455186e89c252055ccf063cf804c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-130412eff396237fb761e14261967262.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.13010v1/page_3_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Deep-Learning-Based-Image-Recovery-and-Pose-Estimation-for-Resident-Space-Objects"><a href="#Deep-Learning-Based-Image-Recovery-and-Pose-Estimation-for-Resident-Space-Objects" class="headerlink" title="Deep Learning-Based Image Recovery and Pose Estimation for Resident   Space Objects"></a>Deep Learning-Based Image Recovery and Pose Estimation for Resident   Space Objects</h2><p><strong>Authors:Louis Aberdeen, Mark Hansen, Melvyn L. Smith, Lyndon Smith</strong></p>
<p>As the density of spacecraft in Earthâ€™s orbit increases, their recognition, pose and trajectory identification becomes crucial for averting potential collisions and executing debris removal operations. However, training models able to identify a spacecraft and its pose presents a significant challenge due to a lack of available image data for model training. This paper puts forth an innovative framework for generating realistic synthetic datasets of Resident Space Object (RSO) imagery. Using the International Space Station (ISS) as a test case, it goes on to combine image regression with image restoration methodologies to estimate pose from blurred images. An analysis of the proposed image recovery and regression techniques was undertaken, providing insights into the performance, potential enhancements and limitations when applied to real imagery of RSOs. The image recovery approach investigated involves first applying image deconvolution using an effective point spread function, followed by detail object extraction with a U-Net. Interestingly, using only U-Net for image reconstruction the best pose performance was attained, reducing the average Mean Squared Error in image recovery by 97.28% and the average angular error by 71.9%. The successful application of U-Net image restoration combined with the Resnet50 regression network for pose estimation of the International Space Station demonstrates the value of a diverse set of evaluation tools for effective solutions to real-world problems such as the analysis of distant objects in Earthâ€™s orbit. </p>
<blockquote>
<p>éšç€åœ°çƒè½¨é“ä¸Šèˆªå¤©å™¨å¯†åº¦çš„å¢åŠ ï¼Œå¯¹å®ƒä»¬çš„è¯†åˆ«ã€å§¿æ€å’Œè½¨è¿¹çš„è¯†åˆ«å¯¹äºé¿å…æ½œåœ¨ç¢°æ’å’Œæ‰§è¡Œç¢ç‰‡æ¸…é™¤æ“ä½œå˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºå¯ç”¨äºæ¨¡å‹è®­ç»ƒçš„å›¾åƒæ•°æ®ç¼ºä¹ï¼Œè®­ç»ƒèƒ½å¤Ÿè¯†åˆ«èˆªå¤©å™¨åŠå…¶å§¿æ€çš„æ¨¡å‹æˆä¸ºäº†ä¸€å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåˆ›æ–°æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå±…æ°‘ç©ºé—´ç‰©ä½“ï¼ˆRSOï¼‰å½±åƒçš„çœŸå®åˆæˆæ•°æ®é›†ã€‚ä»¥å›½é™…ç©ºé—´ç«™ï¼ˆISSï¼‰ä½œä¸ºæµ‹è¯•æ¡ˆä¾‹ï¼Œå®ƒå°†å›¾åƒå›å½’ä¸å›¾åƒæ¢å¤æ–¹æ³•ç›¸ç»“åˆï¼Œä»æ¨¡ç³Šå›¾åƒä¸­ä¼°è®¡å§¿æ€ã€‚å¯¹æ‰€æå‡ºçš„å›¾åƒæ¢å¤å’Œå›å½’æŠ€æœ¯è¿›è¡Œäº†åˆ†æï¼Œä¸ºå°†å…¶åº”ç”¨äºRSOçš„çœŸå®å›¾åƒæ—¶çš„æ€§èƒ½ã€æ½œåœ¨æ”¹è¿›å’Œå±€é™æ€§æä¾›äº†è§è§£ã€‚ç ”ç©¶çš„å›¾åƒæ¢å¤æ–¹æ³•é¦–å…ˆä½¿ç”¨æœ‰æ•ˆçš„ç‚¹æ‰©æ•£å‡½æ•°è¿›è¡Œå›¾åƒåå·ç§¯ï¼Œç„¶åä½¿ç”¨U-Netè¿›è¡Œç»†èŠ‚å¯¹è±¡æå–ã€‚æœ‰è¶£çš„æ˜¯ï¼Œä»…ä½¿ç”¨U-Netè¿›è¡Œå›¾åƒé‡å»ºæ—¶ï¼Œè·å¾—äº†æœ€ä½³çš„å§¿æ€æ€§èƒ½ï¼Œå›¾åƒæ¢å¤çš„å¹³å‡å‡æ–¹è¯¯å·®é™ä½äº†97.28%ï¼Œå¹³å‡è§’åº¦è¯¯å·®é™ä½äº†71.9%ã€‚U-Netå›¾åƒæ¢å¤ä¸Resnet50å›å½’ç½‘ç»œæˆåŠŸåº”ç”¨äºå›½é™…ç©ºé—´ç«™çš„å§¿æ€ä¼°è®¡ï¼Œå±•ç¤ºäº†å¤šç§è¯„ä¼°å·¥å…·åœ¨ç°å®é—®é¢˜è§£å†³ä¸­çš„ä»·å€¼ï¼Œä¾‹å¦‚åˆ†æåœ°çƒè½¨é“ä¸Šçš„è¿œè·ç¦»ç‰©ä½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13009v1">PDF</a> 10 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”Ÿæˆç°å®åˆæˆæ•°æ®é›†çš„æ–¹æ³•ï¼Œç”¨äºè¯†åˆ«åœ°çƒè½¨é“ä¸­çš„èˆªå¤©å™¨åŠå…¶å§¿æ€ã€‚æ–‡ç« åˆ©ç”¨å›½é™…ç©ºé—´ç«™ä½œä¸ºæµ‹è¯•æ¡ˆä¾‹ï¼Œç»“åˆå›¾åƒå›å½’ä¸å›¾åƒæ¢å¤æ–¹æ³•ï¼Œä»æ¨¡ç³Šå›¾åƒä¸­ä¼°è®¡å§¿æ€ã€‚ç ”ç©¶ä¸­åˆ†æäº†å›¾åƒæ¢å¤ä¸å›å½’æŠ€æœ¯çš„æ€§èƒ½ï¼Œå¹¶æå‡ºäº†æ½œåœ¨æ”¹è¿›å’Œå±€é™æ€§ã€‚ä»…ä½¿ç”¨U-Netè¿›è¡Œå›¾åƒé‡å»ºå–å¾—äº†æœ€ä½³å§¿æ€æ€§èƒ½ï¼Œå¹³å‡å›¾åƒæ¢å¤è¯¯å·®å‡å°‘äº†97.28%ï¼Œå¹³å‡è§’åº¦è¯¯å·®å‡å°‘äº†71.9%ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†å¤šç§è¯„ä¼°å·¥å…·åœ¨è§£å†³ç°å®ä¸–ç•Œé—®é¢˜ä¸­çš„ä»·å€¼ï¼Œå¦‚åˆ†æåœ°çƒè½¨é“ä¸­çš„è¿œè·ç¦»ç‰©ä½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>èˆªå¤©å™¨è½¨é“å¯†åº¦å¢åŠ å¯¼è‡´è¯†åˆ«å’Œå§¿æ€è¯†åˆ«çš„é‡è¦æ€§å¢å¼ºï¼Œç”¨äºé¿å…æ½œåœ¨ç¢°æ’å’Œæ‰§è¡Œç¢ç‰‡æ¸…é™¤æ“ä½œã€‚</li>
<li>ç¼ºä¹å¯ç”¨çš„å›¾åƒæ•°æ®æ¥è®­ç»ƒæ¨¡å‹è¿›è¡Œèˆªå¤©å™¨åŠå§¿æ€è¯†åˆ«æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”Ÿæˆåˆæˆæ•°æ®é›†çš„æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒæ¨¡å‹è¯†åˆ«å±…æ°‘ç©ºé—´å¯¹è±¡ï¼ˆRSOï¼‰ã€‚</li>
<li>ä½¿ç”¨å›½é™…ç©ºé—´ç«™ä½œä¸ºæµ‹è¯•æ¡ˆä¾‹ï¼Œç»“åˆäº†å›¾åƒå›å½’å’Œå›¾åƒæ¢å¤æ–¹æ³•æ¥ä¼°è®¡å§¿æ€ã€‚</li>
<li>U-Netåœ¨å›¾åƒé‡å»ºä¸­è¡¨ç°å‡ºæœ€ä½³å§¿æ€æ€§èƒ½ã€‚</li>
<li>ç»¼åˆä½¿ç”¨U-Netå›¾åƒæ¢å¤å’ŒResnet50å›å½’ç½‘ç»œè¿›è¡Œå§¿æ€ä¼°è®¡ï¼Œä¸ºè§£å†³å®é™…é—®é¢˜çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆæä¾›äº†ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13009">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-16ee62ae85f8b5188e2b760eb4987510.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-346f9e4e0bee264f505133227ee27e6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4d7d15a36596ea2b2d644425f391154.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-462cdc99944eee6590205075ae5f1f14.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0181e9f438791a1ff1bcffc69581f506.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1860e8351da549335c2da7363717cbcc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58c7feef746180cebca5970ec2031dde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7090c2eb4a0620b012d142717b331806.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Novel-Tracking-Framework-for-Devices-in-X-ray-Leveraging-Supplementary-Cue-Driven-Self-Supervised-Features"><a href="#A-Novel-Tracking-Framework-for-Devices-in-X-ray-Leveraging-Supplementary-Cue-Driven-Self-Supervised-Features" class="headerlink" title="A Novel Tracking Framework for Devices in X-ray Leveraging Supplementary   Cue-Driven Self-Supervised Features"></a>A Novel Tracking Framework for Devices in X-ray Leveraging Supplementary   Cue-Driven Self-Supervised Features</h2><p><strong>Authors:Saahil Islam, Venkatesh N. Murthy, Dominik Neumann, Serkan Cimen, Puneet Sharma, Andreas Maier, Dorin Comaniciu, Florin C. Ghesu</strong></p>
<p>To restore proper blood flow in blocked coronary arteries via angioplasty procedure, accurate placement of devices such as catheters, balloons, and stents under live fluoroscopy or diagnostic angiography is crucial. Identified balloon markers help in enhancing stent visibility in X-ray sequences, while the catheter tip aids in precise navigation and co-registering vessel structures, reducing the need for contrast in angiography. However, accurate detection of these devices in interventional X-ray sequences faces significant challenges, particularly due to occlusions from contrasted vessels and other devices and distractions from surrounding, resulting in the failure to track such small objects. While most tracking methods rely on spatial correlation of past and current appearance, they often lack strong motion comprehension essential for navigating through these challenging conditions, and fail to effectively detect multiple instances in the scene. To overcome these limitations, we propose a self-supervised learning approach that enhances its spatio-temporal understanding by incorporating supplementary cues and learning across multiple representation spaces on a large dataset. Followed by that, we introduce a generic real-time tracking framework that effectively leverages the pretrained spatio-temporal network and also takes the historical appearance and trajectory data into account. This results in enhanced localization of multiple instances of device landmarks. Our method outperforms state-of-the-art methods in interventional X-ray device tracking, especially stability and robustness, achieving an 87% reduction in max error for balloon marker detection and a 61% reduction in max error for catheter tip detection. </p>
<blockquote>
<p>é€šè¿‡è¡€ç®¡æˆå½¢æœ¯æ¢å¤é˜»å¡çš„å† çŠ¶åŠ¨è„‰ä¸­çš„æ­£å¸¸è¡€æµæ—¶ï¼Œåœ¨å®æ—¶è§å…‰æ£€æŸ¥æˆ–è¯Šæ–­è¡€ç®¡é€ å½±ä¸‹å‡†ç¡®æ”¾ç½®å¯¼ç®¡ã€çƒå›Šå’Œæ”¯æ¶ç­‰è®¾å¤‡è‡³å…³é‡è¦ã€‚æ ‡è¯†çš„çƒå›Šæ ‡è®°æœ‰åŠ©äºå¢å¼ºXå°„çº¿åºåˆ—ä¸­æ”¯æ¶çš„å¯è§æ€§ï¼Œè€Œå¯¼ç®¡å°–ç«¯æœ‰åŠ©äºç²¾ç¡®å¯¼èˆªå’Œå…±æ³¨å†Œè¡€ç®¡ç»“æ„ï¼Œä»è€Œå‡å°‘äº†è¡€ç®¡é€ å½±ä¸­å¯¹æ¯”å‰‚çš„ä½¿ç”¨ã€‚ç„¶è€Œï¼Œåœ¨ä»‹å…¥æ€§Xå°„çº¿åºåˆ—ä¸­å‡†ç¡®æ£€æµ‹è¿™äº›è®¾å¤‡é¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å› ä¸ºå¯¹æ¯”è¡€ç®¡å’Œå…¶ä»–è®¾å¤‡çš„é®æŒ¡ä»¥åŠå‘¨å›´çš„å¹²æ‰°ï¼Œå¯¼è‡´æ— æ³•è·Ÿè¸ªè¿™äº›å°å‹ç‰©ä½“ã€‚å¤§å¤šæ•°è·Ÿè¸ªæ–¹æ³•ä¾èµ–äºè¿‡å»å’Œå½“å‰å¤–è§‚çš„ç©ºé—´ç›¸å…³æ€§ï¼Œä½†å®ƒä»¬é€šå¸¸ç¼ºä¹åœ¨è¿™äº›æ¡ä»¶ä¸‹å¯¼èˆªæ‰€éœ€çš„æœ‰åŠ›è¿åŠ¨ç†è§£ï¼Œå¹¶ä¸”æ— æ³•æœ‰æ•ˆæ£€æµ‹åœºæ™¯ä¸­çš„å¤šä¸ªå®ä¾‹ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªæˆ‘ç›‘ç£çš„å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡èå…¥é¢å¤–çš„çº¿ç´¢å¹¶åœ¨å¤§å‹æ•°æ®é›†ä¸Šå­¦ä¹ å¤šç§è¡¨ç¤ºç©ºé—´ï¼Œå¢å¼ºäº†å…¶æ—¶ç©ºç†è§£ã€‚ä¹‹åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé€šç”¨çš„å®æ—¶è·Ÿè¸ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒçš„æ—¶ç©ºç½‘ç»œï¼ŒåŒæ—¶è€ƒè™‘å†å²å¤–è§‚å’Œè½¨è¿¹æ•°æ®ã€‚è¿™æé«˜äº†å¤šä¸ªè®¾å¤‡åœ°æ ‡å®ä¾‹çš„å®šä½æ•ˆæœã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»‹å…¥æ€§Xå°„çº¿è®¾å¤‡è·Ÿè¸ªæ–¹é¢ä¼˜äºæœ€æ–°æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨³å®šæ€§å’Œé²æ£’æ€§æ–¹é¢ï¼Œæ°”çƒæ ‡è®°æ£€æµ‹çš„æœ€å¤§è¯¯å·®é™ä½äº†87%ï¼Œå¯¼ç®¡å°–ç«¯æ£€æµ‹çš„æœ€å¤§è¯¯å·®é™ä½äº†61%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12958v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨ç»çš®å† çŠ¶åŠ¨è„‰ä»‹å…¥æ²»ç–—è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡é‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•å’Œå®æ—¶è·Ÿè¸ªæ¡†æ¶å®ç°å¯¹å¯¼ç®¡ã€çƒå›Šå’Œæ”¯æ¶ç­‰å™¨æ¢°çš„ç²¾å‡†è¿½è¸ªã€‚è¯¥æ–¹æ³•æé«˜äº†æ—¶ç©ºæ„ŸçŸ¥èƒ½åŠ›ï¼Œæœ‰æ•ˆåº”å¯¹è¡€ç®¡é€ å½±ä¸­çš„é®æŒ¡å’Œå¹²æ‰°é—®é¢˜ï¼Œå®ç°å¯¹å¤šä¸ªå™¨æ¢°å®ä¾‹çš„ç²¾å‡†å®šä½ï¼Œæé«˜äº†æ‰‹æœ¯æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨ç»çš®å† çŠ¶åŠ¨è„‰ä»‹å…¥æ²»ç–—è¿‡ç¨‹ä¸­ï¼Œå‡†ç¡®æ”¾ç½®å¯¼ç®¡ã€çƒå›Šå’Œæ”¯æ¶ç­‰å™¨æ¢°è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰è·Ÿè¸ªæ–¹æ³•å­˜åœ¨ç©ºé—´å…³è”å’ŒåŠ¨æ€ç†è§£ä¸è¶³çš„é—®é¢˜ï¼Œéš¾ä»¥åº”å¯¹è¡€ç®¡é€ å½±ä¸­çš„é®æŒ¡å’Œå¹²æ‰°ã€‚</li>
<li>è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•çš„é‡‡ç”¨æé«˜äº†æ¨¡å‹çš„æ—¶ç©ºæ„ŸçŸ¥èƒ½åŠ›ï¼Œé€šè¿‡å¤šè¡¨ç¤ºç©ºé—´çš„å­¦ä¹ å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¼•å…¥çš„å®æ—¶è·Ÿè¸ªæ¡†æ¶æœ‰æ•ˆç»“åˆäº†é¢„è®­ç»ƒçš„æ—¶ç©ºç½‘ç»œå’Œå†å²å¤–è§‚åŠè½¨è¿¹æ•°æ®ï¼Œæé«˜äº†å¤šä¸ªå™¨æ¢°å®ä¾‹çš„å®šä½ç²¾åº¦ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä»‹å…¥æ€§Xå°„çº¿å™¨æ¢°è·Ÿè¸ªæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨³å®šæ€§å’Œé²æ£’æ€§æ–¹é¢ã€‚</li>
<li>çƒå›Šæ ‡å¿—ç‰©æ£€æµ‹çš„æœ€å¤§è¯¯å·®é™ä½äº†87%ï¼Œå¯¼ç®¡å°–ç«¯æ£€æµ‹çš„æœ€å¤§è¯¯å·®é™ä½äº†61%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12958v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12958v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12958v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-detailed-study-on-spectroscopic-performance-of-SOI-pixel-detector-with-a-pinned-depleted-diode-structure-for-X-ray-astronomy"><a href="#A-detailed-study-on-spectroscopic-performance-of-SOI-pixel-detector-with-a-pinned-depleted-diode-structure-for-X-ray-astronomy" class="headerlink" title="A detailed study on spectroscopic performance of SOI pixel detector with   a pinned depleted diode structure for X-ray astronomy"></a>A detailed study on spectroscopic performance of SOI pixel detector with   a pinned depleted diode structure for X-ray astronomy</h2><p><strong>Authors:Masataka Yukumoto, Koji Mori, Ayaki Takeda, Yusuke Nishioka, Miraku Kimura, Yuta Fuchita, Taiga Yoshida, Takeshi G. Tsuru, Ikuo Kurachi, Kouichi Hagino, Yasuo Arai, Takayoshi Kohmura, Takaaki Tanaka, Kumiko K. Nobukawa</strong></p>
<p>We have been developing silicon-on-insulator (SOI) pixel detectors with a pinned depleted diode (PDD) structure, named â€œXRPIXâ€, for X-ray astronomy. In our previous study, we successfully optimized the design of the PDD structure, achieving both the suppression of large leakage current and satisfactory X-ray spectroscopic performance. Here, we report a detailed study on the X-ray spectroscopic performance of the XRPIX with the optimized PDD structure. The data were obtained at $-60^\circ\mathrm{C}$ with the â€œevent-driven readout modeâ€, in which only a triggering pixel and its surroundings are read out. The energy resolutions in full width at half maximum at 6.4 keV are $178\pm1$ eV and $291\pm1$ eV for single-pixel and all-pixel event spectra, respectively. The all-pixel events include charge-sharing pixel events as well as the single-pixel events. These values are the best achieved in the history of our development. We argue that the gain non-linearity in the low energy side due to excessive charge injection to the charge-sensitive amplifier is a major factor to limit the current spectroscopic performance. Optimization of the amount of the charge injection is expected to lead to further improvement in the spectroscopic performance of XRPIX, especially for the all-pixel event spectrum. </p>
<blockquote>
<p>æˆ‘ä»¬ä¸€ç›´åœ¨å¼€å‘ç”¨äºXå°„çº¿å¤©æ–‡å­¦çš„ç¡…ç»ç¼˜ä½“ä¸Šç¡…ï¼ˆSOIï¼‰åƒç´ æ¢æµ‹å™¨ï¼Œåä¸ºâ€œXRPIXâ€ï¼Œå…¶å…·æœ‰å›ºå®šè€—å°½äºŒæç®¡ï¼ˆPDDï¼‰ç»“æ„ã€‚åœ¨ä¹‹å‰çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æˆåŠŸä¼˜åŒ–äº†PDDç»“æ„çš„è®¾è®¡ï¼Œå®ç°äº†å¤§æ³„æ¼ç”µæµçš„æŠ‘åˆ¶å’Œä»¤äººæ»¡æ„çš„Xå°„çº¿å…‰è°±æ€§èƒ½ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†å…·æœ‰ä¼˜åŒ–PDDç»“æ„çš„XRPIXçš„Xå°„çº¿å…‰è°±æ€§èƒ½çš„è¯¦ç»†ç ”ç©¶ã€‚æ•°æ®æ˜¯åœ¨-60Â°Cçš„æ¸©åº¦ä¸‹ï¼Œé‡‡ç”¨â€œäº‹ä»¶é©±åŠ¨è¯»å–æ¨¡å¼â€è·å¾—çš„ï¼Œåœ¨è¯¥æ¨¡å¼ä¸‹ï¼Œåªæœ‰è§¦å‘åƒç´ åŠå…¶å‘¨å›´åƒç´ è¢«è¯»å‡ºã€‚åœ¨6.4 keVæ—¶ï¼Œå•ç‚¹åƒç´ äº‹ä»¶å…‰è°±çš„å…¨å®½åŠæœ€å¤§å€¼èƒ½é‡åˆ†è¾¨ç‡ä¸º178Â±1 eVï¼Œå…¨åƒç´ äº‹ä»¶å…‰è°±çš„èƒ½é‡åˆ†è¾¨ç‡ä¸º291Â±1 eVã€‚å…¨åƒç´ äº‹ä»¶åŒ…æ‹¬ç”µè·å…±äº«åƒç´ äº‹ä»¶å’Œå•ç‚¹åƒç´ äº‹ä»¶ã€‚è¿™äº›å€¼æ˜¯æˆ‘ä»¬å¼€å‘å†å²ä¸Šæ‰€è¾¾åˆ°çš„æœ€ä½³å€¼ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œç”±äºè¿‡å¤šçš„ç”µè·æ³¨å…¥åˆ°ç”µè·æ•æ„Ÿæ”¾å¤§å™¨ä¸­å¯¼è‡´çš„ä½èƒ½é‡ä¾§çš„å¢ç›Šéçº¿æ€§æ˜¯é™åˆ¶å½“å‰å…‰è°±æ€§èƒ½çš„ä¸»è¦å› ç´ ã€‚ä¼˜åŒ–ç”µè·æ³¨å…¥é‡æœ‰æœ›è¿›ä¸€æ­¥æé«˜XRPIXçš„å…‰è°±æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå…¨åƒç´ äº‹ä»¶å…‰è°±ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12684v1">PDF</a> 11 pages, 14 figures, accepted for publication in NIM A</p>
<p><strong>Summary</strong><br>     é‡‡ç”¨ç¡…ç»ç¼˜ä½“ä¸Šç¡…ï¼ˆSOIï¼‰åƒç´ æ¢æµ‹å™¨ï¼Œé…åˆå›ºå®šè€—å°½äºŒæç®¡ï¼ˆPDDï¼‰ç»“æ„ï¼Œå¼€å‘ç”¨äºXå°„çº¿å¤©æ–‡å­¦çš„XRPIXæ¢æµ‹å™¨ã€‚æˆåŠŸä¼˜åŒ–PDDç»“æ„ï¼Œå‡å°‘å¤§æ³„æ¼ç”µæµï¼Œæé«˜Xå°„çº¿å…‰è°±æ€§èƒ½ã€‚åœ¨-60Â°Cæ¸©åº¦ä¸‹é‡‡ç”¨äº‹ä»¶é©±åŠ¨è¯»å‡ºæ¨¡å¼ï¼Œå•åƒç´ å’Œå…¨åƒç´ äº‹ä»¶å…‰è°±çš„èƒ½é‡åˆ†è¾¨ç‡è¾¾åˆ°æœ€ä½³æ°´å¹³ã€‚æŒ‡å‡ºå¢ç›Šéçº¿æ€§æ˜¯é™åˆ¶å½“å‰å…‰è°±æ€§èƒ½çš„ä¸»è¦å› ç´ ï¼Œä¼˜åŒ–ç”µè·æ³¨å…¥é‡æœ‰æœ›è¿›ä¸€æ­¥æé«˜XRPIXçš„å…‰è°±æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€å‘ç”¨äºXå°„çº¿å¤©æ–‡å­¦çš„ç¡…ç»ç¼˜ä½“ä¸Šç¡…ï¼ˆSOIï¼‰åƒç´ æ¢æµ‹å™¨XRPIXã€‚</li>
<li>æˆåŠŸä¼˜åŒ–å›ºå®šè€—å°½äºŒæç®¡ï¼ˆPDDï¼‰ç»“æ„ï¼ŒæŠ‘åˆ¶å¤§æ³„æ¼ç”µæµï¼Œæé«˜Xå°„çº¿å…‰è°±æ€§èƒ½ã€‚</li>
<li>åœ¨-60Â°Cé‡‡ç”¨äº‹ä»¶é©±åŠ¨è¯»å‡ºæ¨¡å¼ï¼Œè·å¾—å•åƒç´ å’Œå…¨åƒç´ äº‹ä»¶å…‰è°±çš„æœ€ä½³èƒ½é‡åˆ†è¾¨ç‡ã€‚</li>
<li>æŒ‡å‡ºå¢ç›Šéçº¿æ€§æ˜¯é™åˆ¶å½“å‰å…‰è°±æ€§èƒ½çš„ä¸»è¦å› ç´ ã€‚</li>
<li>ä¼˜åŒ–ç”µè·æ³¨å…¥é‡å¯æœ›è¿›ä¸€æ­¥æé«˜XRPIXæ¢æµ‹å™¨çš„å…‰è°±æ€§èƒ½ã€‚</li>
<li>XRPIXæ¢æµ‹å™¨çš„æ€§èƒ½ä¼˜åŒ–å¯¹äºXå°„çº¿å¤©æ–‡å­¦çš„ç ”ç©¶å…·æœ‰é‡è¦å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12684">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12684v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12684v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12684v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12684v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12684v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12684v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12684v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Efficient-Lung-Ultrasound-Severity-Scoring-Using-Dedicated-Feature-Extractor"><a href="#Efficient-Lung-Ultrasound-Severity-Scoring-Using-Dedicated-Feature-Extractor" class="headerlink" title="Efficient Lung Ultrasound Severity Scoring Using Dedicated Feature   Extractor"></a>Efficient Lung Ultrasound Severity Scoring Using Dedicated Feature   Extractor</h2><p><strong>Authors:Jiaqi Guo, Yunnan Wu, Evangelos Kaimakamis, Georgios Petmezas, Vasileios E. Papageorgiou, Nicos Maglaveras, Aggelos K. Katsaggelos</strong></p>
<p>With the advent of the COVID-19 pandemic, ultrasound imaging has emerged as a promising technique for COVID-19 detection, due to its non-invasive nature, affordability, and portability. In response, researchers have focused on developing AI-based scoring systems to provide real-time diagnostic support. However, the limited size and lack of proper annotation in publicly available ultrasound datasets pose significant challenges for training a robust AI model. This paper proposes MeDiVLAD, a novel pipeline to address the above issue for multi-level lung-ultrasound (LUS) severity scoring. In particular, we leverage self-knowledge distillation to pretrain a vision transformer (ViT) without label and aggregate frame-level features via dual-level VLAD aggregation. We show that with minimal finetuning, MeDiVLAD outperforms conventional fully-supervised methods in both frame- and video-level scoring, while offering classification reasoning with exceptional quality. This superior performance enables key applications such as the automatic identification of critical lung pathology areas and provides a robust solution for broader medical video classification tasks. </p>
<blockquote>
<p>éšç€COVID-19å¤§æµè¡Œçš„å‘å±•ï¼Œç”±äºå…¶æ— åˆ›æ€§ã€å¯è´Ÿæ‹…æ€§å’Œä¾¿æºæ€§ï¼Œè¶…å£°æˆåƒå·²æˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„COVID-19æ£€æµ‹æŠ€æœ¯ã€‚å› æ­¤ï¼Œç ”ç©¶äººå‘˜è‡´åŠ›äºå¼€å‘åŸºäºäººå·¥æ™ºèƒ½çš„è¯„åˆ†ç³»ç»Ÿï¼Œä»¥æä¾›å®æ—¶è¯Šæ–­æ”¯æŒã€‚ç„¶è€Œï¼Œå…¬å¼€å¯ç”¨çš„è¶…å£°æ•°æ®é›†è§„æ¨¡æœ‰é™ä¸”ç¼ºä¹é€‚å½“çš„æ³¨é‡Šï¼Œè¿™ç»™è®­ç»ƒç¨³å¥çš„äººå·¥æ™ºèƒ½æ¨¡å‹å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡é’ˆå¯¹ä¸Šè¿°é—®é¢˜æå‡ºäº†MeDiVLADè¿™ä¸€æ–°å‹ç®¡é“ï¼Œç”¨äºè¿›è¡Œå¤šçº§è‚ºè¶…å£°ï¼ˆLUSï¼‰ä¸¥é‡ç¨‹åº¦è¯„åˆ†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨è‡ªæˆ‘çŸ¥è¯†è’¸é¦å¯¹è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰è¿›è¡Œæ— æ ‡ç­¾é¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡åŒçº§VLADèšåˆæ¥æ±‡æ€»å¸§çº§ç‰¹å¾ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¯¹å°‘é‡å¾®è°ƒçš„æƒ…å†µä¸‹ï¼ŒMeDiVLADåœ¨å¸§çº§å’Œè§†é¢‘çº§çš„è¯„åˆ†ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„å®Œå…¨ç›‘ç£æ–¹æ³•ï¼ŒåŒæ—¶æä¾›å‡ºè‰²çš„åˆ†ç±»æ¨ç†è´¨é‡ã€‚è¿™ç§å“è¶Šçš„æ€§èƒ½ä½¿å¾—å…³é”®åº”ç”¨å¦‚è‡ªåŠ¨è¯†åˆ«å…³é”®è‚ºç—…ç†åŒºåŸŸæˆä¸ºå¯èƒ½ï¼Œå¹¶ä¸ºæ›´å¹¿æ³›çš„åŒ»å­¦è§†é¢‘åˆ†ç±»ä»»åŠ¡æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12524v1">PDF</a> Accepted by IEEE ISBI 2025</p>
<p><strong>Summary</strong></p>
<p>è¶…å£°æˆåƒåœ¨COVID-19æ£€æµ‹ä¸­å…·æœ‰æ— åˆ›ã€ç»æµã€ä¾¿æºç­‰ä¼˜ç‚¹ï¼Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚é’ˆå¯¹å…¬å¼€å¯ç”¨çš„è¶…å£°æ•°æ®é›†å¤§å°æœ‰é™ã€ç¼ºä¹é€‚å½“æ ‡æ³¨ç­‰é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºMeDiVLADï¼Œåˆ©ç”¨è‡ªæˆ‘çŸ¥è¯†è’¸é¦é¢„è®­ç»ƒä¸€ä¸ªæ— éœ€æ ‡ç­¾çš„è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ï¼Œå¹¶é€šè¿‡ä¸¤çº§VLADèšåˆè¿›è¡Œå¸§çº§ç‰¹å¾èšåˆã€‚å®éªŒè¡¨æ˜ï¼ŒMeDiVLADåœ¨å¸§çº§å’Œè§†é¢‘çº§è¯„åˆ†ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„å…¨ç›‘ç£æ–¹æ³•ï¼Œä¸”åˆ†ç±»æ¨ç†è´¨é‡ä¼˜å¼‚ï¼Œå¯è‡ªåŠ¨è¯†åˆ«å…³é”®è‚ºéƒ¨ç—…ç†åŒºåŸŸï¼Œä¸ºæ›´å¹¿æ³›çš„åŒ»ç–—è§†é¢‘åˆ†ç±»ä»»åŠ¡æä¾›ç¨³å¥è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°æˆåƒæˆä¸ºCOVID-19æ£€æµ‹çš„æœ‰å‰é€”çš„æŠ€æœ¯ã€‚</li>
<li>å…¬å¼€å¯ç”¨çš„è¶…å£°æ•°æ®é›†å­˜åœ¨å¤§å°æœ‰é™å’Œç¼ºä¹é€‚å½“æ ‡æ³¨çš„é—®é¢˜ã€‚</li>
<li>MeDiVLADåˆ©ç”¨è‡ªæˆ‘çŸ¥è¯†è’¸é¦é¢„è®­ç»ƒè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ã€‚</li>
<li>MeDiVLADé€šè¿‡ä¸¤çº§VLADèšåˆè¿›è¡Œå¸§çº§ç‰¹å¾èšåˆã€‚</li>
<li>MeDiVLADåœ¨å¸§çº§å’Œè§†é¢‘çº§è¯„åˆ†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>MeDiVLADæä¾›é«˜è´¨é‡çš„åˆ†ç±»æ¨ç†ï¼Œå¯è‡ªåŠ¨è¯†åˆ«å…³é”®è‚ºéƒ¨ç—…ç†åŒºåŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12524">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12524v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12524v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12524v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12524v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12524v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12524v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Bidirectional-Brain-Image-Translation-using-Transfer-Learning-from-Generic-Pre-trained-Models"><a href="#Bidirectional-Brain-Image-Translation-using-Transfer-Learning-from-Generic-Pre-trained-Models" class="headerlink" title="Bidirectional Brain Image Translation using Transfer Learning from   Generic Pre-trained Models"></a>Bidirectional Brain Image Translation using Transfer Learning from   Generic Pre-trained Models</h2><p><strong>Authors:Fatima Haimour, Rizik Al-Sayyed, Waleed Mahafza, Omar S. Al-Kadi</strong></p>
<p>Brain imaging plays a crucial role in the diagnosis and treatment of various neurological disorders, providing valuable insights into the structure and function of the brain. Techniques such as magnetic resonance imaging (MRI) and computed tomography (CT) enable non-invasive visualization of the brain, aiding in the understanding of brain anatomy, abnormalities, and functional connectivity. However, cost and radiation dose may limit the acquisition of specific image modalities, so medical image synthesis can be used to generate required medical images without actual addition. In the medical domain, where obtaining labeled medical images is labor-intensive and expensive, addressing data scarcity is a major challenge. Recent studies propose using transfer learning to overcome this issue. This involves adapting pre-trained CycleGAN models, initially trained on non-medical data, to generate realistic medical images. In this work, transfer learning was applied to the task of MR-CT image translation and vice versa using 18 pre-trained non-medical models, and the models were fine-tuned to have the best result. The modelsâ€™ performance was evaluated using four widely used image quality metrics: Peak-signal-to-noise-ratio, Structural Similarity Index, Universal Quality Index, and Visual Information Fidelity. Quantitative evaluation and qualitative perceptual analysis by radiologists demonstrate the potential of transfer learning in medical imaging and the effectiveness of the generic pre-trained model. The results provide compelling evidence of the modelâ€™s exceptional performance, which can be attributed to the high quality and similarity of the training images to actual human brain images. These results underscore the significance of carefully selecting appropriate and representative training images to optimize performance in brain image analysis tasks. </p>
<blockquote>
<p>å¤§è„‘æˆåƒåœ¨è¯Šæ–­å’Œæ²»ç–—å„ç§ç¥ç»ç–¾ç—…ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œå®ƒæä¾›äº†å…³äºå¤§è„‘ç»“æ„å’ŒåŠŸèƒ½çš„å®è´µè§è§£ã€‚è¯¸å¦‚ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å’Œè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ç­‰æŠ€æœ¯èƒ½å¤Ÿå®ç°å¤§è„‘çš„éä¾µå…¥æ€§å¯è§†åŒ–ï¼Œæœ‰åŠ©äºç†è§£å¤§è„‘è§£å‰–ã€å¼‚å¸¸å’ŒåŠŸèƒ½æ€§è¿æ¥ã€‚ç„¶è€Œï¼Œæˆæœ¬å’Œè¾å°„å‰‚é‡å¯èƒ½ä¼šé™åˆ¶ç‰¹å®šå›¾åƒæ¨¡å¼çš„è·å–ï¼Œå› æ­¤åŒ»å­¦å›¾åƒåˆæˆå¯ç”¨äºåœ¨ä¸å®é™…å¢åŠ çš„æƒ…å†µä¸‹ç”Ÿæˆæ‰€éœ€çš„åŒ»å­¦å›¾åƒã€‚åœ¨åŒ»å­¦é¢†åŸŸï¼Œè·å–å¸¦æœ‰æ ‡ç­¾çš„åŒ»å­¦å›¾åƒæ˜¯åŠ³åŠ¨å¯†é›†ä¸”æ˜‚è´µçš„ï¼Œè§£å†³æ•°æ®ç¨€ç¼ºæ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚æœ€è¿‘çš„ç ”ç©¶æå‡ºä½¿ç”¨è¿ç§»å­¦ä¹ æ¥å…‹æœè¿™ä¸ªé—®é¢˜ã€‚è¿™æ¶‰åŠåˆ°é€‚åº”é¢„å…ˆè®­ç»ƒçš„CycleGANæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æœ€åˆåœ¨éåŒ»å­¦æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥ç”Ÿæˆé€¼çœŸçš„åŒ»å­¦å›¾åƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œå°†è¿ç§»å­¦ä¹ åº”ç”¨äºMR-CTå›¾åƒç¿»è¯‘åŠå…¶åå‘ä»»åŠ¡ï¼Œä½¿ç”¨äº†18ä¸ªé¢„å…ˆè®­ç»ƒçš„éåŒ»å­¦æ¨¡å‹ï¼Œå¹¶å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒä»¥å¾—åˆ°æœ€ä½³ç»“æœã€‚æ¨¡å‹çš„æ€§èƒ½æ˜¯ä½¿ç”¨å››ç§å¹¿æ³›ä½¿ç”¨çš„å›¾åƒè´¨é‡æŒ‡æ ‡è¿›è¡Œè¯„ä¼°çš„ï¼šå³°å€¼ä¿¡å™ªæ¯”ã€ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ã€é€šç”¨è´¨é‡æŒ‡æ•°å’Œè§†è§‰ä¿¡æ¯ä¿çœŸåº¦ã€‚æ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡Œçš„å®šé‡è¯„ä¼°å’Œå®šæ€§æ„ŸçŸ¥åˆ†æè¯æ˜äº†è¿ç§»å­¦ä¹ åœ¨åŒ»å­¦æˆåƒä¸­çš„æ½œåŠ›ä»¥åŠé€šç”¨é¢„è®­ç»ƒæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚ç»“æœæä¾›äº†æœ‰åŠ›è¯æ®è¡¨æ˜è¯¥æ¨¡å‹çš„å“è¶Šæ€§èƒ½ï¼Œè¿™å¯ä»¥å½’å› äºè®­ç»ƒå›¾åƒçš„é«˜è´¨é‡å’Œä¸çœŸå®äººç±»å¤§è„‘å›¾åƒçš„ç›¸ä¼¼æ€§ã€‚è¿™äº›ç»“æœå¼ºè°ƒåœ¨å¤§è„‘å›¾åƒåˆ†æä»»åŠ¡ä¸­ä»”ç»†é€‰æ‹©å’Œé€‚å½“ä»£è¡¨æ€§çš„è®­ç»ƒå›¾åƒä»¥ä¼˜åŒ–æ€§èƒ½çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12488v1">PDF</a> 19 pages, 9 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<pre><code>åŒ»å­¦æˆåƒåœ¨è¯Šæ–­ä¸æ²»ç–—å„ç§ç¥ç»æ€§ç–¾ç—…ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ï¼Œæä¾›äº†å¯¹å¤§è„‘ç»“æ„å’ŒåŠŸèƒ½çš„å®è´µè§è§£ã€‚ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å’Œè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ç­‰æŠ€æœ¯å¯å®ç°æ— åˆ›å¯è§†åŒ–ï¼Œæœ‰åŠ©äºäº†è§£å¤§è„‘è§£å‰–ç»“æ„ã€å¼‚å¸¸æƒ…å†µå’ŒåŠŸèƒ½æ€§è¿æ¥ã€‚é’ˆå¯¹è·å–ç‰¹å®šå›¾åƒæ¨¡å¼å¯èƒ½å­˜åœ¨çš„æˆæœ¬å’Œè¾å°„å‰‚é‡é™åˆ¶ï¼Œå¯ä½¿ç”¨åŒ»å­¦å›¾åƒåˆæˆæŠ€æœ¯ç”Ÿæˆæ‰€éœ€çš„åŒ»å­¦å›¾åƒè€Œæ— éœ€å®é™…æ·»åŠ ã€‚åœ¨åŒ»å­¦é¢†åŸŸï¼Œè·å–æ ‡è®°çš„åŒ»å­¦å›¾åƒæ˜¯åŠ³åŠ¨å¯†é›†ä¸”æ˜‚è´µçš„ï¼Œè§£å†³æ•°æ®ç¨€ç¼ºæ€§æ˜¯ä¸»è¦æŒ‘æˆ˜ã€‚æœ€è¿‘çš„ç ”ç©¶æå‡ºäº†åˆ©ç”¨è¿ç§»å­¦ä¹ æ¥åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œè¿™æ¶‰åŠä½¿ç”¨é¢„è®­ç»ƒçš„CycleGANæ¨¡å‹åœ¨éåŒ»å­¦æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç„¶åç”Ÿæˆé€¼çœŸçš„åŒ»å­¦å›¾åƒã€‚æœ¬ç ”ç©¶åº”ç”¨è¿ç§»å­¦ä¹ è¿›è¡ŒMR-CTå›¾åƒè½¬æ¢åŠå…¶åå‘è½¬æ¢ï¼Œä½¿ç”¨18ä¸ªé¢„è®­ç»ƒçš„éåŒ»å­¦æ¨¡å‹ï¼Œå¹¶é€šè¿‡å¾®è°ƒè·å¾—æœ€ä½³ç»“æœã€‚æ¨¡å‹çš„æ€§èƒ½é€šè¿‡ä½¿ç”¨å››ç§å¸¸ç”¨çš„å›¾åƒè´¨é‡æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼šå³°å€¼ä¿¡å™ªæ¯”ã€ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ã€é€šç”¨è´¨é‡æŒ‡æ•°å’Œè§†è§‰ä¿¡æ¯ä¿çœŸåº¦ã€‚å®šé‡è¯„ä¼°å’Œæ”¾å°„ç§‘åŒ»å¸ˆçš„å®šæ€§æ„ŸçŸ¥åˆ†æè¯æ˜äº†è¿ç§»å­¦ä¹ åœ¨åŒ»å­¦æˆåƒä¸­çš„æ½œåŠ›ä»¥åŠé€šç”¨é¢„è®­ç»ƒæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚ç»“æœæä¾›äº†æœ‰åŠ›è¯æ®è¡¨æ˜è¯¥æ¨¡å‹çš„å“è¶Šæ€§èƒ½å¯å½’åŠŸäºè®­ç»ƒå›¾åƒçš„é«˜è´¨é‡ä»¥åŠä¸å®é™…äººç±»å¤§è„‘å›¾åƒçš„ç›¸ä¼¼æ€§ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†ä»”ç»†é€‰æ‹©å’Œä»£è¡¨æ€§å¼ºçš„è®­ç»ƒå›¾åƒå¯¹ä¼˜åŒ–å¤§è„‘å›¾åƒåˆ†æä»»åŠ¡æ€§èƒ½çš„é‡è¦æ€§ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒåœ¨ç¥ç»æ€§ç–¾ç—…çš„è¯Šæ–­å’Œæ²»ç–—ä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œèƒ½æä¾›å…³äºå¤§è„‘ç»“æ„å’ŒåŠŸèƒ½çš„å®è´µä¿¡æ¯ã€‚</li>
<li>MRIå’ŒCTç­‰æŠ€æœ¯èƒ½å¸®åŠ©æ— åˆ›å¯è§†åŒ–å¤§è„‘ï¼Œäº†è§£å¤§è„‘ç»“æ„ã€å¼‚å¸¸æƒ…å†µå’ŒåŠŸèƒ½æ€§è¿æ¥ã€‚</li>
<li>ç”±äºæˆæœ¬å’Œè¾å°„å‰‚é‡çš„é™åˆ¶ï¼ŒåŒ»å­¦å›¾åƒåˆæˆæŠ€æœ¯è¢«ç”¨äºç”Ÿæˆæ‰€éœ€çš„åŒ»å­¦å›¾åƒã€‚</li>
<li>åœ¨åŒ»å­¦é¢†åŸŸï¼Œæ•°æ®ç¨€ç¼ºæ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ï¼Œè·å–æ ‡è®°çš„åŒ»å­¦å›¾åƒæ—¢åŠ³åŠ¨å¯†é›†åˆæ˜‚è´µã€‚</li>
<li>æœ€è¿‘çš„ç ”ç©¶åˆ©ç”¨è¿ç§»å­¦ä¹ æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„æ¨¡å‹åœ¨éåŒ»å­¦æ•°æ®ä¸Šç”Ÿæˆé€¼çœŸçš„åŒ»å­¦å›¾åƒã€‚</li>
<li>ç ”ç©¶ä¸­ä½¿ç”¨è¿ç§»å­¦ä¹ è¿›è¡ŒMR-CTå›¾åƒè½¬æ¢çš„å°è¯•å–å¾—äº†è‰¯å¥½æ•ˆæœï¼Œè¯æ˜äº†è¿ç§»å­¦ä¹ å’Œé€šç”¨é¢„è®­ç»ƒæ¨¡å‹åœ¨åŒ»å­¦æˆåƒä¸­çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12488">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12488v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12488v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12488v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12488v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Multi-stage-intermediate-fusion-for-multimodal-learning-to-classify-non-small-cell-lung-cancer-subtypes-from-CT-and-PET"><a href="#Multi-stage-intermediate-fusion-for-multimodal-learning-to-classify-non-small-cell-lung-cancer-subtypes-from-CT-and-PET" class="headerlink" title="Multi-stage intermediate fusion for multimodal learning to classify   non-small cell lung cancer subtypes from CT and PET"></a>Multi-stage intermediate fusion for multimodal learning to classify   non-small cell lung cancer subtypes from CT and PET</h2><p><strong>Authors:Fatih Aksu, Fabrizia Gelardi, Arturo Chiti, Paolo Soda</strong></p>
<p>Accurate classification of histological subtypes of non-small cell lung cancer (NSCLC) is essential in the era of precision medicine, yet current invasive techniques are not always feasible and may lead to clinical complications. This study presents a multi-stage intermediate fusion approach to classify NSCLC subtypes from CT and PET images. Our method integrates the two modalities at different stages of feature extraction, using voxel-wise fusion to exploit complementary information across varying abstraction levels while preserving spatial correlations. We compare our method against unimodal approaches using only CT or PET images to demonstrate the benefits of modality fusion, and further benchmark it against early and late fusion techniques to highlight the advantages of intermediate fusion during feature extraction. Additionally, we compare our model with the only existing intermediate fusion method for histological subtype classification using PET&#x2F;CT images. Our results demonstrate that the proposed method outperforms all alternatives across key metrics, with an accuracy and AUC equal to 0.724 and 0.681, respectively. This non-invasive approach has the potential to significantly improve diagnostic accuracy, facilitate more informed treatment decisions, and advance personalized care in lung cancer management. </p>
<blockquote>
<p>åœ¨éå°ç»†èƒè‚ºç™Œï¼ˆNSCLCï¼‰çš„ç²¾å‡†åŒ»å­¦æ—¶ä»£ï¼Œå¯¹å…¶ç»„ç»‡å­¦äºšå‹çš„å‡†ç¡®åˆ†ç±»è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„æœ‰åˆ›æŠ€æœ¯å¹¶ä¸æ€»æ˜¯å¯è¡Œï¼Œå¹¶å¯èƒ½å¯¼è‡´ä¸´åºŠå¹¶å‘ç—‡ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µä¸­é—´èåˆæ–¹æ³•æ¥å¯¹NSCLCäºšå‹è¿›è¡ŒCTå’ŒPETå›¾åƒåˆ†ç±»ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒçš„ç‰¹å¾æå–é˜¶æ®µå°†ä¸¤ç§æ¨¡å¼èåˆåœ¨ä¸€èµ·ï¼Œä½¿ç”¨ä½“ç´ çº§çš„èåˆæ¥åˆ©ç”¨ä¸åŒæŠ½è±¡å±‚æ¬¡ä¸Šçš„äº’è¡¥ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™ç©ºé—´ç›¸å…³æ€§ã€‚æˆ‘ä»¬å°†è¯¥æ–¹æ³•ä¸ä»…ä½¿ç”¨CTæˆ–PETå›¾åƒçš„å•ä¸€æ¨¡æ€æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œä»¥è¯æ˜æ¨¡æ€èåˆçš„ä¼˜åŠ¿ï¼Œå¹¶è¿›ä¸€æ­¥å°†å…¶ä¸æ—©æœŸå’Œæ™šæœŸèåˆæŠ€æœ¯è¿›è¡Œæ¯”è¾ƒï¼Œä»¥çªå‡ºç‰¹å¾æå–è¿‡ç¨‹ä¸­ä¸­é—´èåˆçš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸å”¯ä¸€ç°æœ‰çš„ä½¿ç”¨PET&#x2F;CTå›¾åƒè¿›è¡Œç»„ç»‡å­¦äºšå‹åˆ†ç±»çš„ä¸­é—´èåˆæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ‰€æœ‰å…³é”®æŒ‡æ ‡ä¸Šå‡ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå‡†ç¡®ç‡å’ŒAUCåˆ†åˆ«ä¸º0.724å’Œ0.681ã€‚è¿™ç§éä¾µå…¥æ€§æ–¹æ³•æœ‰å¯èƒ½æ˜¾è‘—æé«˜è¯Šæ–­å‡†ç¡®æ€§ï¼Œæœ‰åŠ©äºåšå‡ºæ›´æ˜æ™ºçš„æ²»ç–—å†³ç­–ï¼Œå¹¶æ¨åŠ¨è‚ºç™Œç®¡ç†çš„ä¸ªæ€§åŒ–æŠ¤ç†å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12425v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¤šé˜¶æ®µä¸­é—´èåˆæ–¹æ³•ï¼Œç”¨äºä»CTå’ŒPETå›¾åƒåˆ†ç±»éå°ç»†èƒè‚ºç™Œï¼ˆNSCLCï¼‰äºšå‹ã€‚è¯¥æ–¹æ³•åœ¨ä¸åŒé˜¶æ®µçš„ç‰¹å¾æå–ä¸­èåˆäº†ä¸¤ç§æ¨¡æ€ï¼Œé€šè¿‡ä½“ç´ çº§çš„èåˆæ–¹å¼ï¼Œåˆ©ç”¨ä¸åŒæŠ½è±¡å±‚æ¬¡çš„äº’è¡¥ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™ç©ºé—´ç›¸å…³æ€§ã€‚ä¸åªä½¿ç”¨CTæˆ–PETå›¾åƒçš„å•ä¸€æ¨¡æ€æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰ä¼˜åŠ¿ï¼Œå¹¶ä¸æ—©æœŸå’Œæ™šæœŸèåˆæŠ€æœ¯è¿›è¡Œæ¯”è¾ƒï¼Œçªå‡ºäº†ç‰¹å¾æå–è¿‡ç¨‹ä¸­ä¸­é—´èåˆçš„ä¼˜ç‚¹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å°†è¯¥æ–¹æ³•ä¸ç°æœ‰çš„ç”¨äºç»„ç»‡å­¦äºšå‹åˆ†ç±»çš„PET&#x2F;CTå›¾åƒä¸­é—´èåˆæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ‰€æœ‰å…³é”®æŒ‡æ ‡ä¸Šå‡ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå‡†ç¡®ç‡å’ŒAUCåˆ†åˆ«ä¸º0.724å’Œ0.681ã€‚è¿™ç§éä¾µå…¥æ€§æ–¹æ³•æœ‰æœ›æ˜¾è‘—æé«˜è¯Šæ–­å‡†ç¡®æ€§ï¼Œä¿ƒè¿›æ›´æ˜æ™ºçš„æ²»ç–—å†³ç­–ï¼Œå¹¶æ¨åŠ¨è‚ºç™Œç®¡ç†çš„ä¸ªæ€§åŒ–æŠ¤ç†å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡†ç¡®åˆ†ç±»éå°ç»†èƒè‚ºç™Œï¼ˆNSCLCï¼‰äºšå‹åœ¨ç²¾å‡†åŒ»å­¦æ—¶ä»£éå¸¸é‡è¦ã€‚</li>
<li>å½“å‰ä¾µå…¥æ€§æŠ€æœ¯å¹¶ä¸æ€»æ˜¯å¯è¡Œï¼Œå¯èƒ½å¯¼è‡´ä¸´åºŠå¹¶å‘ç—‡ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µä¸­é—´èåˆæ–¹æ³•ï¼Œé€šè¿‡CTå’ŒPETå›¾åƒåˆ†ç±»NSCLCäºšå‹ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸åŒé˜¶æ®µçš„ç‰¹å¾æå–ä¸­èåˆä¸¤ç§æ¨¡æ€ï¼Œé‡‡ç”¨ä½“ç´ çº§èåˆæ–¹å¼ã€‚</li>
<li>ä¸å•ä¸€æ¨¡æ€æ–¹æ³•å’Œæ—©æœŸ&#x2F;æ™šæœŸèåˆæŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>ç ”ç©¶ç»“æœè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„é«˜å‡†ç¡®æ€§å’Œæ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12425v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12425v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12425v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12425v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Tackling-Small-Sample-Survival-Analysis-via-Transfer-Learning-A-Study-of-Colorectal-Cancer-Prognosis"><a href="#Tackling-Small-Sample-Survival-Analysis-via-Transfer-Learning-A-Study-of-Colorectal-Cancer-Prognosis" class="headerlink" title="Tackling Small Sample Survival Analysis via Transfer Learning: A Study   of Colorectal Cancer Prognosis"></a>Tackling Small Sample Survival Analysis via Transfer Learning: A Study   of Colorectal Cancer Prognosis</h2><p><strong>Authors:Yonghao Zhao, Changtao Li, Chi Shu, Qingbin Wu, Hong Li, Chuan Xu, Tianrui Li, Ziqiang Wang, Zhipeng Luo, Yazhou He</strong></p>
<p>Survival prognosis is crucial for medical informatics. Practitioners often confront small-sized clinical data, especially cancer patient cases, which can be insufficient to induce useful patterns for survival predictions. This study deals with small sample survival analysis by leveraging transfer learning, a useful machine learning technique that can enhance the target analysis with related knowledge pre-learned from other data. We propose and develop various transfer learning methods designed for common survival models. For parametric models such as DeepSurv, Cox-CC (Cox-based neural networks), and DeepHit (end-to-end deep learning model), we apply standard transfer learning techniques like pretraining and fine-tuning. For non-parametric models such as Random Survival Forest, we propose a new transfer survival forest (TSF) model that transfers tree structures from source tasks and fine-tunes them with target data. We evaluated the transfer learning methods on colorectal cancer (CRC) prognosis. The source data are 27,379 SEER CRC stage I patients, and the target data are 728 CRC stage I patients from the West China Hospital. When enhanced by transfer learning, Cox-CCâ€™s $C^{td}$ value was boosted from 0.7868 to 0.8111, DeepHitâ€™s from 0.8085 to 0.8135, DeepSurvâ€™s from 0.7722 to 0.8043, and RSFâ€™s from 0.7940 to 0.8297 (the highest performance). All models trained with data as small as 50 demonstrated even more significant improvement. Conclusions: Therefore, the current survival models used for cancer prognosis can be enhanced and improved by properly designed transfer learning techniques. The source code used in this study is available at <a target="_blank" rel="noopener" href="https://github.com/YonghaoZhao722/TSF">https://github.com/YonghaoZhao722/TSF</a>. </p>
<blockquote>
<p>ç”Ÿå­˜é¢„æµ‹åœ¨åŒ»å­¦ä¿¡æ¯åŒ–ä¸­è‡³å…³é‡è¦ã€‚ç ”ç©¶äººå‘˜ç»å¸¸é¢ä¸´å°å‹ä¸´åºŠæ•°æ®ï¼Œå°¤å…¶æ˜¯ç™Œç—‡æ‚£è€…ç—…ä¾‹ï¼Œè¿™äº›æ•°æ®ä¸è¶³ä»¥äº§ç”Ÿæœ‰ç”¨çš„ç”Ÿå­˜æ¨¡å¼é¢„æµ‹ã€‚æœ¬ç ”ç©¶é€šè¿‡åˆ©ç”¨è¿ç§»å­¦ä¹ è¿™ä¸€æœ‰ç”¨çš„æœºå™¨å­¦ä¹ æŠ€æœ¯æ¥è§£å†³å°æ ·æœ¬ç”Ÿå­˜åˆ†æçš„é—®é¢˜ï¼Œè¯¥æŠ€æœ¯å¯ä»¥åˆ©ç”¨ä»å…¶ä»–æ•°æ®ä¸­é¢„å…ˆå­¦ä¹ çš„ç›¸å…³çŸ¥è¯†æ¥å¢å¼ºç›®æ ‡åˆ†æã€‚æˆ‘ä»¬é’ˆå¯¹å¸¸è§çš„ç”Ÿå­˜æ¨¡å‹æå‡ºäº†å¤šç§è¿ç§»å­¦ä¹ æ–¹æ³•ã€‚å¯¹äºå‚æ•°æ¨¡å‹ï¼Œå¦‚DeepSurvã€Cox-CCï¼ˆåŸºäºCoxçš„ç¥ç»ç½‘ç»œï¼‰å’ŒDeepHitï¼ˆç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼‰ï¼Œæˆ‘ä»¬åº”ç”¨äº†é¢„è®­ç»ƒå’Œå¾®è°ƒç­‰æ ‡å‡†è¿ç§»å­¦ä¹ æŠ€æœ¯ã€‚å¯¹äºéå‚æ•°æ¨¡å‹ï¼Œå¦‚éšæœºç”Ÿå­˜æ£®æ—ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¿ç§»ç”Ÿå­˜æ£®æ—ï¼ˆTSFï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»æºä»»åŠ¡è½¬ç§»æ ‘ç»“æ„ï¼Œå¹¶ç”¨ç›®æ ‡æ•°æ®è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬å¯¹ç»“ç›´è‚ ç™Œï¼ˆCRCï¼‰çš„é¢„åè¿›è¡Œäº†è¿ç§»å­¦ä¹ æ–¹æ³•çš„è¯„ä¼°ã€‚æºæ•°æ®ä¸ºSEER CRC IæœŸæ‚£è€…27,379ä¾‹ï¼Œç›®æ ‡æ•°æ®ä¸ºæ¥è‡ªåè¥¿åŒ»é™¢çš„CRC IæœŸæ‚£è€…728ä¾‹ã€‚åœ¨è¿ç§»å­¦ä¹ çš„å¢å¼ºä¸‹ï¼ŒCox-CCçš„Ctdå€¼ä»0.7868æé«˜åˆ°0.8111ï¼ŒDeepHitä»0.8085æé«˜åˆ°0.8135ï¼ŒDeepSurvä»0.7722æé«˜åˆ°0.8043ï¼Œè€ŒRSFåˆ™ä»0.7940æé«˜åˆ°æœ€é«˜çš„0.8297ã€‚å³ä½¿ä½¿ç”¨å°åˆ°åªæœ‰50ä¸ªæ•°æ®çš„æ¨¡å‹ä¹Ÿè¡¨ç°å‡ºäº†æ›´æ˜¾è‘—çš„æ”¹è¿›ã€‚ç»“è®ºï¼šå› æ­¤ï¼Œé€šè¿‡é€‚å½“è®¾è®¡çš„è¿ç§»å­¦ä¹ æŠ€æœ¯ï¼Œç›®å‰ç”¨äºç™Œç—‡é¢„åçš„ç”Ÿå­˜æ¨¡å‹å¯ä»¥å¾—åˆ°å¢å¼ºå’Œæ”¹è¿›ã€‚æœ¬ç ”ç©¶ä½¿ç”¨çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/YonghaoZhao722/TSF%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/YonghaoZhao722/TSFä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12421v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬ç ”ç©¶åˆ©ç”¨è¿ç§»å­¦ä¹ æŠ€æœ¯è§£å†³å°æ ·æœ¬ç”Ÿå­˜åˆ†æé—®é¢˜ï¼Œæå‡ºå¹¶å¼€å‘é€‚ç”¨äºå¸¸è§ç”Ÿå­˜æ¨¡å‹çš„å¤šç§è¿ç§»å­¦ä¹ æ–¹æ³•ã€‚é€šè¿‡è¿ç§»å­¦ä¹ ï¼ŒCox-CCã€DeepHitã€DeepSurvå’ŒRSFç­‰æ¨¡å‹çš„æ€§èƒ½æœ‰æ‰€æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›®æ ‡æ•°æ®è¾ƒå°çš„æƒ…å†µä¸‹è¡¨ç°æ›´ä¸ºæ˜¾è‘—ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿ç§»å­¦ä¹ æŠ€æœ¯å¯å¢å¼ºå’Œæ”¹è¿›ç”¨äºç™Œç—‡é¢„åçš„ç°æœ‰ç”Ÿå­˜æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿ç§»å­¦ä¹ è¢«ç”¨æ¥è§£å†³å°æ ·æœ¬ç”Ÿå­˜åˆ†æé—®é¢˜ã€‚</li>
<li>é’ˆå¯¹ä¸åŒç”Ÿå­˜æ¨¡å‹ï¼ˆå¦‚DeepSurvã€Cox-CCå’ŒRandom Survival Forestï¼‰ï¼Œæå‡ºäº†ç›¸åº”çš„è¿ç§»å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>è¿ç§»å­¦ä¹ èƒ½å¤Ÿæå‡æ¨¡å‹çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›®æ ‡æ•°æ®è¾ƒå°çš„æƒ…å†µä¸‹ã€‚</li>
<li>åœ¨ç»“ç›´è‚ ç™Œé¢„ååˆ†æä¸­ï¼Œè¿ç§»å­¦ä¹ å¯¹æ¨¡å‹æ€§èƒ½çš„æå‡å¾—åˆ°äº†éªŒè¯ã€‚</li>
<li>æœ€ä½³æ€§èƒ½çš„æ¨¡å‹æ˜¯é€šè¿‡å¯¹ä»…æœ‰50ä¸ªæ•°æ®çš„è®­ç»ƒå¾—åˆ°çš„ï¼Œè¿™æ˜¾ç¤ºäº†è¿ç§»å­¦ä¹ åœ¨å°æ ·æœ¬æ•°æ®ä¸­çš„ä¼˜åŠ¿ã€‚</li>
<li>è¿ç§»å­¦ä¹ æŠ€æœ¯å¯å¢å¼ºå’Œæ”¹è¿›ç”¨äºç™Œç—‡é¢„åçš„ç°æœ‰ç”Ÿå­˜æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12421">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.12421v1/page_0_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="UNetVL-Enhancing-3D-Medical-Image-Segmentation-with-Chebyshev-KAN-Powered-Vision-LSTM"><a href="#UNetVL-Enhancing-3D-Medical-Image-Segmentation-with-Chebyshev-KAN-Powered-Vision-LSTM" class="headerlink" title="UNetVL: Enhancing 3D Medical Image Segmentation with Chebyshev KAN   Powered Vision-LSTM"></a>UNetVL: Enhancing 3D Medical Image Segmentation with Chebyshev KAN   Powered Vision-LSTM</h2><p><strong>Authors:Xuhui Guo, Tanmoy Dam, Rohan Dhamdhere, Gourav Modanwal, Anant Madabhushi</strong></p>
<p>3D medical image segmentation has progressed considerably due to Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), yet these methods struggle to balance long-range dependency acquisition with computational efficiency. To address this challenge, we propose UNETVL (U-Net Vision-LSTM), a novel architecture that leverages recent advancements in temporal information processing. UNETVL incorporates Vision-LSTM (ViL) for improved scalability and memory functions, alongside an efficient Chebyshev Kolmogorov-Arnold Networks (KAN) to handle complex and long-range dependency patterns more effectively. We validated our method on the ACDC and AMOS2022 (post challenge Task 2) benchmark datasets, showing a significant improvement in mean Dice score compared to recent state-of-the-art approaches, especially over its predecessor, UNETR, with increases of 7.3% on ACDC and 15.6% on AMOS, respectively. Extensive ablation studies were conducted to demonstrate the impact of each component in UNETVL, providing a comprehensive understanding of its architecture. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/tgrex6/UNETVL">https://github.com/tgrex6/UNETVL</a>, facilitating further research and applications in this domain. </p>
<blockquote>
<p>ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²ç”±äºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„è¿›æ­¥è€Œå¾—åˆ°äº†å¾ˆå¤§å‘å±•ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨å¹³è¡¡è¿œç¨‹ä¾èµ–è·å–ä¸è®¡ç®—æ•ˆç‡æ–¹é¢é¢ä¸´å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†UNETVLï¼ˆU-Net Vision-LSTMï¼‰è¿™ä¸€æ–°å‹æ¶æ„ï¼Œå®ƒåˆ©ç”¨æœ€æ–°çš„æ—¶é—´ä¿¡æ¯å¤„ç†æ–¹é¢çš„è¿›å±•ã€‚UNETVLç»“åˆäº†Vision-LSTMï¼ˆViLï¼‰æ¥æé«˜å¯ä¼¸ç¼©æ€§å’Œå†…å­˜åŠŸèƒ½ï¼ŒåŒæ—¶ä½¿ç”¨é«˜æ•ˆçš„åˆ‡æ¯”é›ªå¤«æŸ¯å°”è«å“¥æ´›å¤«-é˜¿è¯ºå°”å¾·ç½‘ç»œï¼ˆKANï¼‰æ¥æ›´æœ‰æ•ˆåœ°å¤„ç†å¤æ‚å’Œè¿œç¨‹ä¾èµ–æ¨¡å¼ã€‚æˆ‘ä»¬åœ¨ACDCå’ŒAMOS2022ï¼ˆä»»åŠ¡æŒ‘æˆ˜åçš„ç¬¬2æ¬¡ä»»åŠ¡ï¼‰åŸºå‡†æ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç›¸è¾ƒäºæœ€æ–°çš„å°–ç«¯æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…¶å‰èº«UNETRä¸Šï¼Œæˆ‘ä»¬çš„å¹³å‡Diceå¾—åˆ†æœ‰äº†æ˜¾è‘—æé«˜ï¼ŒACDCä¸Šæé«˜äº†7.3%ï¼Œè€Œåœ¨AMOSä¸Šæé«˜äº†é«˜è¾¾15.6%ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤§é‡æ¶ˆèç ”ç©¶ï¼Œå±•ç¤ºäº†UNETVLä¸­æ¯ä¸ªç»„ä»¶çš„å½±å“ï¼Œå¹¶å¯¹å…¶æ¶æ„è¿›è¡Œäº†å…¨é¢çš„äº†è§£ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/tgrex6/UNETVL%E6%89%BE%E5%88%B0%EF%BC%8C%E4%B8%BA%E8%AF%9A%E5%AD%A6%E7%9C%8D%E7%9A%84%E8%BF%BD%E9%9B%BB%E5%AF%BC%E7%A0%B4%E4%B8%AD%E7%9A%84%E7%BB%BC%E5%BD%B9%E5%BA%94%E7%94%A8%E4%BA%86%E4%BF%9D%E4%">https://github.com/tgrex6/UNETVLæ‰¾åˆ°ï¼Œä¸ºè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ä¸åº”ç”¨æä¾›äº†ä¾¿åˆ©ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07017v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºUNETVLçš„æ–°æ¶æ„ï¼Œå®ƒç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ã€è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰å’Œæ—¶ç©ºä¿¡æ¯å¤„ç†çš„æœ€æ–°è¿›å±•ï¼Œè§£å†³äº†åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¹³è¡¡é•¿æœŸä¾èµ–è·å–ä¸è®¡ç®—æ•ˆç‡çš„æŒ‘æˆ˜ã€‚UNETVLä½¿ç”¨Vision-LSTMï¼ˆViLï¼‰æé«˜å¯æ‰©å±•æ€§å’Œå†…å­˜åŠŸèƒ½ï¼Œå¹¶é‡‡ç”¨é«˜æ•ˆçš„Chebyshev Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰æ›´æœ‰æ•ˆåœ°å¤„ç†å¤æ‚å’Œé•¿æœŸçš„ä¾èµ–æ¨¡å¼ã€‚åœ¨ACDCå’ŒAMOS2022åŸºå‡†æ•°æ®é›†ä¸Šçš„éªŒè¯ç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„UNETRç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå°¤å…¶æ˜¯åœ¨AMOSæ•°æ®é›†ä¸Šçš„æé«˜å¹…åº¦æ›´å¤§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UNETVLæ˜¯ä¸€ç§æ–°çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¶æ„ï¼Œç»“åˆäº†CNNã€ViTå’Œæ—¶ç©ºä¿¡æ¯å¤„ç†çš„æœ€æ–°æŠ€æœ¯æ¥è§£å†³é•¿æœŸä¾èµ–ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚</li>
<li>UNETVLä½¿ç”¨Vision-LSTMï¼ˆViLï¼‰ä»¥æé«˜å¯æ‰©å±•æ€§å’Œå†…å­˜åŠŸèƒ½ã€‚</li>
<li>Chebyshev Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰è¢«ç”¨äºæ›´æœ‰æ•ˆåœ°å¤„ç†å¤æ‚å’Œé•¿æœŸçš„ä¾èµ–æ¨¡å¼ã€‚</li>
<li>åœ¨ACDCå’ŒAMOS2022åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒUNETVLåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œç›¸æ¯”æœ€æ–°çš„å…ˆè¿›æ–¹æ³•æœ‰æ˜æ˜¾çš„æ”¹è¿›ã€‚</li>
<li>ä¸å‰ä»»UNETRç›¸æ¯”ï¼ŒUNETVLåœ¨ACDCå’ŒAMOSæ•°æ®é›†ä¸Šçš„è¡¨ç°åˆ†åˆ«æé«˜äº†7.3%å’Œ15.6%ã€‚</li>
<li>è¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèç ”ç©¶ï¼Œä»¥å±•ç¤ºUNETVLä¸­æ¯ä¸ªç»„ä»¶çš„å½±å“ï¼Œæä¾›äº†å¯¹å…¶æ¶æ„çš„å…¨é¢ç†è§£ã€‚</li>
<li>ç ”ç©¶çš„ä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œä¾¿äºè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07017">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.07017v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.07017v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.07017v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.07017v2/page_3_1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Improved-joint-modelling-of-breast-cancer-radiomics-features-and-hazard-by-image-registration-aided-longitudinal-CT-data"><a href="#Improved-joint-modelling-of-breast-cancer-radiomics-features-and-hazard-by-image-registration-aided-longitudinal-CT-data" class="headerlink" title="Improved joint modelling of breast cancer radiomics features and hazard   by image registration aided longitudinal CT data"></a>Improved joint modelling of breast cancer radiomics features and hazard   by image registration aided longitudinal CT data</h2><p><strong>Authors:Subrata Mukherjee</strong></p>
<p>Patients with metastatic breast cancer (mBC) undergo continuous medical imaging during treatment, making accurate lesion detection and monitoring over time critical for clinical decisions. Predicting drug response from post-treatment data is essential for personalized care and pharmacological research. In collaboration with the U.S. Food and Drug Administration and Novartis Pharmaceuticals, we analyzed serial chest CT scans from two large-scale Phase III trials, MONALEESA 3 and MONALEESA 7. This paper has two objectives (a) Data Structuring developing a Registration Aided Automated Correspondence (RAMAC) algorithm for precise lesion tracking in longitudinal CT data, and (b) Survival Analysis creating imaging features and models from RAMAC structured data to predict patient outcomes. The RAMAC algorithm uses a two phase pipeline: three dimensional rigid registration aligns CT images, and a distance metric-based Hungarian algorithm tracks lesion correspondence. Using structured data, we developed interpretable models to assess progression-free survival (PFS) in mBC patients by combining baseline radiomics, post-treatment changes (Weeks 8, 16, 24), and demographic features. Radiomics effects were studied across time points separately and through a non-correlated additive framework. Radiomics features were reduced using (a) a regularized (L1-penalized) additive Cox proportional hazards model, and (b) variable selection via best subset selection. Performance, measured using the concordance index (C-index), improved with additional time points. Joint modeling, considering correlations among radiomics effects over time, provided insights into relationships between longitudinal radiomics and survival outcomes. </p>
<blockquote>
<p>è½¬ç§»æ€§ä¹³è…ºç™Œï¼ˆmBCï¼‰æ‚£è€…åœ¨æ²»ç–—è¿‡ç¨‹ä¸­ä¼šç»å†æŒç»­çš„åŒ»å­¦æˆåƒï¼Œå› æ­¤å‡†ç¡®çš„ç—…ç¶æ£€æµ‹å’Œé•¿æœŸç›‘æµ‹å¯¹ä¸´åºŠå†³ç­–è‡³å…³é‡è¦ã€‚ä»æ²»ç–—åæ•°æ®ä¸­é¢„æµ‹è¯ç‰©ååº”å¯¹äºä¸ªæ€§åŒ–æŠ¤ç†å’Œè¯ç‰©ç ”ç©¶è‡³å…³é‡è¦ã€‚æˆ‘ä»¬ä¸ç¾å›½é£Ÿå“è¯å“ç›‘ç£ç®¡ç†å±€ï¼ˆFDAï¼‰å’Œè¯ºååˆ¶è¯åˆä½œï¼Œåˆ†æäº†æ¥è‡ªä¸¤é¡¹å¤§è§„æ¨¡IIIæœŸè¯•éªŒï¼ˆMONALEESA 3å’ŒMONALEESA 7ï¼‰çš„è¿ç»­èƒ¸éƒ¨CTæ‰«æã€‚æœ¬æ–‡æœ‰ä¸¤ä¸ªç›®æ ‡ï¼šï¼ˆaï¼‰å¼€å‘ä¸€ç§åŸºäºæ³¨å†Œè¾…åŠ©è‡ªåŠ¨å¯¹åº”ï¼ˆRAMACï¼‰ç®—æ³•çš„ç”¨äºç²¾ç¡®è¿½è¸ªçºµå‘CTæ•°æ®ä¸­ç—…ç¶çš„æ•°æ®ç»“æ„åŒ–æ–¹æ³•ï¼›ï¼ˆbï¼‰é€šè¿‡RAMACç»“æ„åŒ–æ•°æ®åˆ›å»ºæˆåƒç‰¹å¾å’Œæ¨¡å‹æ¥é¢„æµ‹æ‚£è€…ç»“æœï¼Œè¿›è¡Œç”Ÿå­˜åˆ†æã€‚RAMACç®—æ³•é‡‡ç”¨ä¸¤ä¸ªé˜¶æ®µç®¡é“ï¼šé¦–å…ˆä½¿ç”¨ä¸‰ç»´åˆšæ€§æ³¨å†Œå¯¹é½CTå›¾åƒï¼Œç„¶ååŸºäºè·ç¦»åº¦é‡çš„åŒˆç‰™åˆ©ç®—æ³•è·Ÿè¸ªç—…ç¶å¯¹åº”å…³ç³»ã€‚æˆ‘ä»¬ä½¿ç”¨ç»“æ„åŒ–æ•°æ®ï¼Œç»“åˆåŸºçº¿æ”¾å°„ç»„å­¦ã€æ²»ç–—åå˜åŒ–ï¼ˆç¬¬8å‘¨ã€ç¬¬16å‘¨ã€ç¬¬24å‘¨ï¼‰å’Œäººå£ç»Ÿè®¡å­¦ç‰¹å¾ï¼Œå¼€å‘äº†å¯è§£é‡Šæ¨¡å‹æ¥è¯„ä¼°mBCæ‚£è€…çš„æ— è¿›å±•ç”Ÿå­˜ï¼ˆPFSï¼‰ã€‚æ”¾å°„å­¦æ•ˆåº”åˆ†åˆ«åœ¨å„ä¸ªæ—¶é—´ç‚¹ä»¥åŠé€šè¿‡éç›¸å…³é™„åŠ æ¡†æ¶è¿›è¡Œäº†ç ”ç©¶ã€‚ä½¿ç”¨ï¼ˆaï¼‰æ­£åˆ™åŒ–ï¼ˆL1æƒ©ç½šï¼‰é™„åŠ Coxæ¯”ä¾‹é£é™©æ¨¡å‹å’Œï¼ˆbï¼‰é€šè¿‡æœ€ä½³å­é›†é€‰æ‹©è¿›è¡Œå˜é‡é€‰æ‹©æ¥å‡å°‘æ”¾å°„å­¦ç‰¹å¾ã€‚ä½¿ç”¨ä¸€è‡´æ€§æŒ‡æ•°ï¼ˆCæŒ‡æ•°ï¼‰è¡¡é‡çš„æ€§èƒ½ä¼šéšç€æ—¶é—´ç‚¹çš„å¢åŠ è€Œæé«˜ã€‚è”åˆå»ºæ¨¡ï¼Œè€ƒè™‘åˆ°æ”¾å°„å­¦æ•ˆåº”éšæ—¶é—´çš„ç›¸å…³æ€§ï¼Œæä¾›äº†çºµå‘æ”¾å°„å­¦ä¸ç”Ÿå­˜ç»“æœä¹‹é—´å…³ç³»çš„æ–°è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06814v2">PDF</a> The organization with whom I have worked on the paper needs to info   clear first before it being released. Sorry for the inconvenience caused</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹è½¬ç§»æ€§ä¹³è…ºç™Œï¼ˆmBCï¼‰æ‚£è€…çš„è¿ç»­åŒ»å­¦å½±åƒåˆ†æï¼Œæœ¬ç ”ç©¶è‡´åŠ›äºå®ç°ä¸¤ä¸ªç›®æ ‡ï¼šï¼ˆaï¼‰å¼€å‘ä¸€ç§åä¸ºRAMACçš„æ³¨å†Œè¾…åŠ©è‡ªåŠ¨å¯¹åº”å…³ç³»ç®—æ³•ï¼Œç”¨äºç²¾ç¡®è¿½è¸ªçºµå‘CTå½±åƒä¸­çš„ç—…å˜ï¼›ï¼ˆbï¼‰ä½¿ç”¨RAMACç»“æ„åŒ–æ•°æ®è¿›è¡Œç”Ÿå­˜åˆ†æï¼Œé¢„æµ‹æ‚£è€…é¢„åã€‚è¯¥ç®—æ³•é‡‡ç”¨ä¸¤é˜¶æ®µæµç¨‹ï¼šä¸‰ç»´åˆšæ€§æ³¨å†Œå¯¹é½CTå›¾åƒï¼ŒåŸºäºè·ç¦»åº¦é‡çš„åŒˆç‰™åˆ©ç®—æ³•è¿½è¸ªç—…å˜å¯¹åº”å…³ç³»ã€‚é€šè¿‡ç»“æ„åŒ–æ•°æ®è¯„ä¼°ä¹³è…ºç™Œæ‚£è€…çš„æ— è¿›å±•ç”Ÿå­˜æœŸï¼ˆPFSï¼‰ï¼Œå¹¶ç»“åˆåŸºçº¿æ”¾å°„å­¦ç‰¹å¾ã€æ²»ç–—åçš„å˜åŒ–ï¼ˆç¬¬8å‘¨ã€ç¬¬16å‘¨å’Œç¬¬24å‘¨ï¼‰ä»¥åŠäººå£ç»Ÿè®¡å­¦ç‰¹å¾è¿›è¡Œæ¨¡å‹æ„å»ºã€‚æ”¾å°„å­¦ç‰¹å¾çš„å½±å“éšæ—¶é—´ç‚¹åˆ†åˆ«è¿›è¡Œäº†ç ”ç©¶ï¼Œå¹¶é€šè¿‡éç›¸å…³ç´¯åŠ æ¡†æ¶è¿›è¡Œç ”ç©¶ã€‚ä½¿ç”¨æ­£åˆ™åŒ–ï¼ˆL1æƒ©ç½šï¼‰ç´¯åŠ Coxæ¯”ä¾‹é£é™©æ¨¡å‹å’Œæœ€ä½³å­é›†é€‰æ‹©è¿›è¡Œå˜é‡é€‰æ‹©ï¼Œä»¥é™ä½æ”¾å°„å­¦ç‰¹å¾çš„å½±å“ã€‚éšç€é¢å¤–æ—¶é—´ç‚¹çš„åŠ å…¥ï¼Œé€šè¿‡ä¸€è‡´æ€§æŒ‡æ•°ï¼ˆC-indexï¼‰è¡¡é‡çš„æ€§èƒ½æœ‰æ‰€æé«˜ã€‚è”åˆå»ºæ¨¡è€ƒè™‘äº†æ”¾å°„å­¦æ•ˆåº”éšæ—¶é—´çš„ç›¸å…³æ€§ï¼Œæ­ç¤ºäº†çºµå‘æ”¾å°„å­¦ä¸ç”Ÿå­˜ç»“æœä¹‹é—´çš„å…³ç³»ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶é›†ä¸­äºè½¬ç§»æ€§ä¹³è…ºç™Œæ‚£è€…æ²»ç–—ä¸­ç—…å˜æ£€æµ‹çš„ç²¾å‡†æ€§å’Œé•¿æœŸç›‘æµ‹çš„é‡è¦æ€§ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§åä¸ºRAMACçš„ç®—æ³•ï¼Œç”¨äºç²¾ç¡®è¿½è¸ªçºµå‘CTå½±åƒä¸­çš„ç—…å˜ã€‚</li>
<li>RAMACç®—æ³•åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šä¸‰ç»´å›¾åƒå¯¹é½å’Œç—…å˜å¯¹åº”å…³ç³»è¿½è¸ªã€‚</li>
<li>é€šè¿‡ç»“åˆåŸºçº¿æ”¾å°„å­¦ç‰¹å¾ã€æ²»ç–—åçš„å˜åŒ–ä»¥åŠäººå£ç»Ÿè®¡å­¦ç‰¹å¾ï¼Œè¯„ä¼°äº†æ‚£è€…çš„æ— è¿›å±•ç”Ÿå­˜æœŸã€‚</li>
<li>ç ”ç©¶äº†æ”¾å°„å­¦ç‰¹å¾éšæ—¶é—´çš„å½±å“ï¼Œå¹¶é‡‡ç”¨éç›¸å…³ç´¯åŠ æ¡†æ¶è¿›è¡Œåˆ†æã€‚</li>
<li>é€šè¿‡æ­£åˆ™åŒ–Coxæ¯”ä¾‹é£é™©æ¨¡å‹å’Œæœ€ä½³å­é›†é€‰æ‹©ï¼Œä¼˜åŒ–äº†æ”¾å°„å­¦ç‰¹å¾çš„åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06814">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.06814v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.06814v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.06814v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.06814v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.06814v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2501.06814v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Towards-Interpretable-Radiology-Report-Generation-via-Concept-Bottlenecks-using-a-Multi-Agentic-RAG"><a href="#Towards-Interpretable-Radiology-Report-Generation-via-Concept-Bottlenecks-using-a-Multi-Agentic-RAG" class="headerlink" title="Towards Interpretable Radiology Report Generation via Concept   Bottlenecks using a Multi-Agentic RAG"></a>Towards Interpretable Radiology Report Generation via Concept   Bottlenecks using a Multi-Agentic RAG</h2><p><strong>Authors:Hasan Md Tusfiqur Alam, Devansh Srivastav, Md Abdul Kadir, Daniel Sonntag</strong></p>
<p>Deep learning has advanced medical image classification, but interpretability challenges hinder its clinical adoption. This study enhances interpretability in Chest X-ray (CXR) classification by using concept bottleneck models (CBMs) and a multi-agent Retrieval-Augmented Generation (RAG) system for report generation. By modeling relationships between visual features and clinical concepts, we create interpretable concept vectors that guide a multi-agent RAG system to generate radiology reports, enhancing clinical relevance, explainability, and transparency. Evaluation of the generated reports using an LLM-as-a-judge confirmed the interpretability and clinical utility of our modelâ€™s outputs. On the COVID-QU dataset, our model achieved 81% classification accuracy and demonstrated robust report generation performance, with five key metrics ranging between 84% and 90%. This interpretable multi-agent framework bridges the gap between high-performance AI and the explainability required for reliable AI-driven CXR analysis in clinical settings. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/tifat58/IRR-with-CBM-RAG.git">https://github.com/tifat58/IRR-with-CBM-RAG.git</a>. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†è§£é‡Šæ€§æŒ‘æˆ˜é˜»ç¢äº†å…¶åœ¨ä¸´åºŠçš„é‡‡çº³ã€‚æœ¬ç ”ç©¶é€šè¿‡é‡‡ç”¨æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰å’Œå¤šä»£ç†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿï¼Œæé«˜äº†èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰åˆ†ç±»ä¸­çš„è§£é‡Šæ€§ã€‚é€šè¿‡æ¨¡æ‹Ÿè§†è§‰ç‰¹å¾å’Œä¸´åºŠæ¦‚å¿µä¹‹é—´çš„å…³ç³»ï¼Œæˆ‘ä»¬åˆ›å»ºäº†å¯è§£é‡Šçš„æ¦‚å¿µå‘é‡ï¼Œå¼•å¯¼å¤šä»£ç†RAGç³»ç»Ÿç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šï¼Œæé«˜äº†ä¸´åºŠç›¸å…³æ€§ã€è§£é‡Šæ€§å’Œé€æ˜åº¦ã€‚ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯„ä¼°æ ‡å‡†å¯¹ç”Ÿæˆçš„æŠ¥å‘Šè¿›è¡Œè¯„ä¼°ï¼Œè¯å®äº†æˆ‘ä»¬æ¨¡å‹è¾“å‡ºçš„è§£é‡Šæ€§å’Œä¸´åºŠå®ç”¨æ€§ã€‚åœ¨COVID-QUæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¾¾åˆ°äº†81%çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œå¹¶å±•ç¤ºäº†ç¨³å¥çš„æŠ¥å‘Šç”Ÿæˆæ€§èƒ½ï¼Œäº”é¡¹å…³é”®æŒ‡æ ‡åœ¨84%è‡³90%ä¹‹é—´ã€‚è¿™ç§å¯è§£é‡Šçš„å¤šä»£ç†æ¡†æ¶ç¼©å°äº†é«˜æ€§èƒ½äººå·¥æ™ºèƒ½å’Œä¸´åºŠç¯å¢ƒä¸­å¯é AIé©±åŠ¨CXRåˆ†ææ‰€éœ€è§£é‡Šæ€§ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š[é“¾æ¥åœ°å€]ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/tifat58/IRR-with-CBM-RAG.git%EF%BC%89%E3%80%82">https://github.com/tifat58/IRR-with-CBM-RAG.gitï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16086v2">PDF</a> Accepted in the 47th European Conference for Information Retrieval   (ECIR) 2025</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰å’Œå¤šæ™ºèƒ½ä½“æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿï¼Œæé«˜åŒ»å­¦å›¾åƒä¸­èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰åˆ†ç±»çš„è§£è¯»æ€§ã€‚é€šè¿‡æ¨¡æ‹Ÿè§†è§‰ç‰¹å¾ä¸ä¸´åºŠæ¦‚å¿µä¹‹é—´çš„å…³ç³»ï¼Œåˆ›å»ºäº†è§£è¯»æ€§å¼ºçš„æ¦‚å¿µå‘é‡ï¼Œå¼•å¯¼å¤šæ™ºèƒ½ä½“RAGç³»ç»Ÿç”Ÿæˆæ›´å…·ä¸´åºŠç›¸å…³æ€§ã€è§£é‡Šæ€§å’Œé€æ˜åº¦çš„æ”¾å°„å­¦æŠ¥å‘Šã€‚è¯„ä¼°ç»“æœè¯å®æ¨¡å‹çš„è§£è¯»æ€§å’Œä¸´åºŠå®ç”¨æ€§ã€‚åœ¨COVID-QUæ•°æ®é›†ä¸Šï¼Œæ¨¡å‹åˆ†ç±»å‡†ç¡®ç‡è¾¾åˆ°äº†81%ï¼ŒæŠ¥å‘Šç”Ÿæˆæ€§èƒ½ç¨³å¥ï¼Œäº”é¡¹å…³é”®æŒ‡æ ‡åœ¨84%è‡³90%ä¹‹é—´ã€‚æ­¤è§£è¯»æ€§å¤šæ™ºèƒ½ä½“æ¡†æ¶ç¼©çŸ­äº†é«˜æ€§èƒ½äººå·¥æ™ºèƒ½å’Œä¸´åºŠç¯å¢ƒä¸­å¯é AIé©±åŠ¨CXRåˆ†ææ‰€éœ€è§£é‡Šæ€§ä¹‹é—´çš„å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰æé«˜åŒ»å­¦å›¾åƒåˆ†ç±»çš„è§£è¯»æ€§ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹Ÿè§†è§‰ç‰¹å¾ä¸ä¸´åºŠæ¦‚å¿µä¹‹é—´çš„å…³ç³»ï¼Œåˆ›å»ºæ¦‚å¿µå‘é‡ã€‚</li>
<li>ä½¿ç”¨å¤šæ™ºèƒ½ä½“æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šã€‚</li>
<li>æ¨¡å‹åœ¨COVID-QUæ•°æ®é›†ä¸Šå®ç°81%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚</li>
<li>æ¨¡å‹æŠ¥å‘Šç”Ÿæˆæ€§èƒ½ç¨³å¥ï¼Œäº”é¡¹å…³é”®æŒ‡æ ‡è¡¨ç°è‰¯å¥½ã€‚</li>
<li>æ¨¡å‹å¢å¼ºäº†ä¸´åºŠç›¸å…³æ€§ã€è§£é‡Šæ€§å’Œé€æ˜åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16086">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2412.16086v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2412.16086v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2412.16086v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2412.16086v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2412.16086v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2412.16086v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2412.16086v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Latent-Diffusion-for-Medical-Image-Segmentation-End-to-end-learning-for-fast-sampling-and-accuracy"><a href="#Latent-Diffusion-for-Medical-Image-Segmentation-End-to-end-learning-for-fast-sampling-and-accuracy" class="headerlink" title="Latent Diffusion for Medical Image Segmentation: End to end learning for   fast sampling and accuracy"></a>Latent Diffusion for Medical Image Segmentation: End to end learning for   fast sampling and accuracy</h2><p><strong>Authors:Fahim Ahmed Zaman, Mathews Jacob, Amanda Chang, Kan Liu, Milan Sonka, Xiaodong Wu</strong></p>
<p>Diffusion Probabilistic Models (DPMs) suffer from inefficient inference due to their slow sampling and high memory consumption, which limits their applicability to various medical imaging applications. In this work, we propose a novel conditional diffusion modeling framework (LDSeg) for medical image segmentation, utilizing the learned inherent low-dimensional latent shape manifolds of the target objects and the embeddings of the source image with an end-to-end framework. Conditional diffusion in latent space not only ensures accurate image segmentation for multiple interacting objects, but also tackles the fundamental issues of traditional DPM-based segmentation methods: (1) high memory consumption, (2) time-consuming sampling process, and (3) unnatural noise injection in the forward and reverse processes. The end-to-end training strategy enables robust representation learning in the latent space related to segmentation features, ensuring significantly faster sampling from the posterior distribution for segmentation generation in the inference phase. Our experiments demonstrate that LDSeg achieved state-of-the-art segmentation accuracy on three medical image datasets with different imaging modalities. In addition, we showed that our proposed model was significantly more robust to noise compared to traditional deterministic segmentation models. The code is available at <a target="_blank" rel="noopener" href="https://github.com/FahimZaman/LDSeg.git">https://github.com/FahimZaman/LDSeg.git</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMsï¼‰ç”±äºå…¶ç¼“æ…¢çš„é‡‡æ ·å’Œè¾ƒé«˜çš„å†…å­˜æ¶ˆè€—ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡ä½ä¸‹ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å„ç§åŒ»å­¦æˆåƒåº”ç”¨ä¸­çš„é€‚ç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæ¡ä»¶æ‰©æ•£å»ºæ¨¡çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼ˆLDSegï¼‰ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç›®æ ‡å¯¹è±¡çš„å›ºæœ‰ä½ç»´æ½œåœ¨å½¢çŠ¶æµå½¢çš„å­¦ä¹ å’Œæºå›¾åƒçš„åµŒå…¥ï¼Œä»¥åŠç«¯åˆ°ç«¯çš„æ¡†æ¶ã€‚æ½œåœ¨ç©ºé—´ä¸­çš„æ¡ä»¶æ‰©æ•£ä¸ä»…ç¡®ä¿äº†å¤šä¸ªäº¤äº’å¯¹è±¡çš„å‡†ç¡®å›¾åƒåˆ†å‰²ï¼Œè¿˜è§£å†³äº†åŸºäºä¼ ç»ŸDPMçš„åˆ†å‰²æ–¹æ³•çš„åŸºæœ¬é—®é¢˜ï¼šï¼ˆ1ï¼‰é«˜å†…å­˜æ¶ˆè€—ï¼Œï¼ˆ2ï¼‰è€—æ—¶çš„é‡‡æ ·è¿‡ç¨‹ï¼Œä»¥åŠï¼ˆ3ï¼‰å‰å‘å’Œåå‘è¿‡ç¨‹ä¸­çš„ä¸è‡ªç„¶å™ªå£°æ³¨å…¥ã€‚ç«¯åˆ°ç«¯çš„è®­ç»ƒç­–ç•¥èƒ½å¤Ÿåœ¨ä¸åˆ†å‰²ç‰¹å¾ç›¸å…³çš„æ½œåœ¨ç©ºé—´ä¸­å®ç°ç¨³å¥çš„è¡¨ç¤ºå­¦ä¹ ï¼Œç¡®ä¿åœ¨æ¨ç†é˜¶æ®µä»åéªŒåˆ†å¸ƒä¸­è¿›è¡Œåˆ†å‰²ç”Ÿæˆæ—¶æ˜¾è‘—åŠ å¿«é‡‡æ ·é€Ÿåº¦ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒLDSegåœ¨ä¸‰ç§ä¸åŒæˆåƒæ¨¡å¼çš„åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„åˆ†å‰²ç²¾åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œä¸ä¼ ç»Ÿç¡®å®šæ€§åˆ†å‰²æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬æå‡ºçš„æ¨¡å‹å¯¹å™ªå£°å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/FahimZaman/LDSeg.git%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/FahimZaman/LDSeg.gitè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.12952v2">PDF</a> 10 pages, 10 figures, journal article</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMsï¼‰åœ¨åŒ»ç–—å›¾åƒåº”ç”¨ä¸­çš„é‡‡æ ·ç¼“æ…¢å’Œå†…å­˜æ¶ˆè€—é«˜çš„ç¼ºç‚¹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæ¡ä»¶æ‰©æ•£å»ºæ¨¡çš„æ¡†æ¶ï¼ˆLDSegï¼‰ç”¨äºåŒ»ç–—å›¾åƒåˆ†å‰²ã€‚LDSegåˆ©ç”¨ç›®æ ‡å¯¹è±¡çš„å†…åœ¨ä½ç»´æ½œåœ¨å½¢çŠ¶æµå½¢å’Œæºå›¾åƒçš„åµŒå…¥ï¼Œé€šè¿‡ç«¯åˆ°ç«¯çš„æ¡†æ¶è¿›è¡Œå›¾åƒåˆ†å‰²ã€‚æ¡ä»¶æ‰©æ•£åœ¨æ½œåœ¨ç©ºé—´ä¸ä»…ç¡®ä¿äº†å¤šä¸ªäº¤äº’å¯¹è±¡çš„ç²¾ç¡®å›¾åƒåˆ†å‰²ï¼Œè¿˜è§£å†³äº†ä¼ ç»ŸDPMsæ–¹æ³•çš„æ ¹æœ¬é—®é¢˜ï¼Œå¦‚å†…å­˜æ¶ˆè€—é«˜ã€é‡‡æ ·è¿‡ç¨‹è€—æ—¶ä»¥åŠæ­£åå‘è¿‡ç¨‹ä¸­çš„ä¸è‡ªç„¶å™ªå£°æ³¨å…¥ã€‚å®éªŒè¯æ˜ï¼ŒLDSegåœ¨ä¸‰ç§ä¸åŒæˆåƒæ¨¡æ€çš„åŒ»ç–—å›¾åƒæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„åˆ†å‰²ç²¾åº¦ï¼Œä¸”ç›¸è¾ƒäºä¼ ç»Ÿçš„ç¡®å®šæ€§åˆ†å‰²æ¨¡å‹ï¼Œå¯¹å™ªå£°æ›´å…·é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LDSegåˆ©ç”¨æ¡ä»¶æ‰©æ•£å»ºæ¨¡è¿›è¡ŒåŒ»ç–—å›¾åƒåˆ†å‰²ï¼Œé’ˆå¯¹DPMsçš„ç¼ºç‚¹è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>LDSegé€šè¿‡åˆ©ç”¨æ½œåœ¨å½¢çŠ¶æµå½¢å’Œæºå›¾åƒåµŒå…¥å®ç°ç²¾ç¡®å›¾åƒåˆ†å‰²ã€‚</li>
<li>æ¡ä»¶æ‰©æ•£åœ¨æ½œåœ¨ç©ºé—´è§£å†³äº†ä¼ ç»ŸDPMsæ–¹æ³•çš„é«˜å†…å­˜æ¶ˆè€—å’Œè€—æ—¶é‡‡æ ·é—®é¢˜ã€‚</li>
<li>LDSegèƒ½æœ‰æ•ˆå¤„ç†å¤šä¸ªäº¤äº’å¯¹è±¡çš„å›¾åƒåˆ†å‰²ã€‚</li>
<li>LDSegåœ¨åŒ»ç–—å›¾åƒåˆ†å‰²ä¸Šå®ç°äº†å…ˆè¿›æ€§èƒ½ï¼Œä¸”å¯¹å™ªå£°å…·æœ‰é²æ£’æ€§ã€‚</li>
<li>LDSegé‡‡ç”¨ç«¯åˆ°ç«¯çš„è®­ç»ƒç­–ç•¥ï¼Œåœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œç¨³å¥çš„ç‰¹å¾è¡¨ç¤ºå­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.12952">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2407.12952v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2407.12952v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2407.12952v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2407.12952v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2407.12952v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2407.12952v2/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2407.12952v2/page_5_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2407.12952v2/page_5_3.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Radiative-decay-of-muonic-molecules-in-resonance-states"><a href="#Radiative-decay-of-muonic-molecules-in-resonance-states" class="headerlink" title="Radiative decay of muonic molecules in resonance states"></a>Radiative decay of muonic molecules in resonance states</h2><p><strong>Authors:Takuma Yamashita, Kazuhiro Yasuda, Yasushi Kino</strong></p>
<p>In this study, we theoretically investigated x-ray spectra from the radiative decay of muonic deuterium molecules in resonance states dd$\mu^\ast$, which plays an important role in a new kinetic model of muon catalyzed fusion ($\mu$CF). The resonance states are Feshbach resonances located below the d$\mu$($n&#x3D;2$) + d threshold energy and radiatively decay into the continuum or bound states. The x-ray spectra having characteristic shapes according to the radial distribution of the two nuclei are obtained using precise three-body wave functions. We carefully examined the convergence of the x-ray spectra and achieved agreements between the length- and velocity-gauge calculations. We revealed a non-adiabatic kinetic energy distribution of the decay fragments, indicating that the radiative decay becomes a heating source of muonic atoms. We also investigated the decay branch that directly results in bound-state muonic molecules. Some resonance states dd$\mu^\ast$ and dt$\mu^\ast$ are found to have high branching ratios to the bound state where intramolecular nuclear fusion occurs. The formation of the muonic molecules in the bound states from metastable muonic atoms can be a fast track in the $\mu$CF cycle which skips a slow path to form the bound state from the ground-state muonic atoms and increases the $\mu$CF cycle rate. Although the spectra from the radiative decays are located in the energy range of $1.5$â€“$1.997$ keV, which is close to the K$\alpha$ x-ray of 1.997 keV from muonic deuterium atoms, state-of-the-art microcalorimeters can distinguish them. </p>
<blockquote>
<p>åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šæ¢è®¨äº†å¤„äºå…±æŒ¯æ€çš„Î¼å­åŒ–æ°˜åˆ†å­æ”¾å°„æ€§è¡°å˜äº§ç”Ÿçš„Xå°„çº¿å…‰è°±ï¼Œè¿™åœ¨Î¼å­å‚¬åŒ–èšå˜ï¼ˆÎ¼CFï¼‰çš„æ–°åŠ¨åŠ›å­¦æ¨¡å‹ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚è¿™äº›å…±æŒ¯æ€ä½äºdÎ¼ï¼ˆn&#x3D;2ï¼‰+dçš„é˜ˆå€¼èƒ½é‡ä»¥ä¸‹ï¼Œå¹¶ä¼šé€šè¿‡è¾å°„è¡°å˜è¿›å…¥è¿ç»­æ€æˆ–æŸç¼šæ€ã€‚åˆ©ç”¨ç²¾ç¡®çš„ä¸‰ä½“æ³¢å‡½æ•°ï¼Œæˆ‘ä»¬è·å¾—äº†å…·æœ‰ç‰¹å¾å½¢çŠ¶çš„Xå°„çº¿å…‰è°±ï¼Œè¿™äº›ç‰¹å¾å½¢çŠ¶æ ¹æ®ä¸¤ä¸ªåŸå­æ ¸çš„å¾„å‘åˆ†å¸ƒè€Œå‘ˆç°ã€‚æˆ‘ä»¬ä»”ç»†ç ”ç©¶äº†Xå°„çº¿å…‰è°±çš„æ”¶æ•›æ€§ï¼Œå¹¶åœ¨é•¿åº¦è®¡é‡å’Œé€Ÿåº¦è®¡é‡è®¡ç®—ä¹‹é—´å–å¾—äº†å…±è¯†ã€‚æˆ‘ä»¬æ­ç¤ºäº†è¡°å˜ç‰‡æ®µçš„éç»çƒ­åŠ¨èƒ½åˆ†å¸ƒï¼Œè¡¨æ˜æ”¾å°„æ€§è¡°å˜æˆä¸ºÎ¼å­åŸå­çš„çƒ­æºã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†ç›´æ¥å¯¼è‡´æŸç¼šæ€Î¼å­åˆ†å­çš„è¡°å˜åˆ†æ”¯ã€‚å‘ç°ä¸€äº›å…±æŒ¯æ€ddÎ¼âˆ—å’ŒdtÎ¼âˆ—åˆ°æŸç¼šæ€çš„åˆ†æ”¯æ¯”éå¸¸é«˜ï¼Œåœ¨æŸç¼šæ€ä¸­å‘ç”Ÿåˆ†å­å†…çš„æ ¸èšå˜ã€‚ä»äºšç¨³çš„Î¼å­åŸå­å½¢æˆæŸç¼šæ€çš„Î¼å­åˆ†å­å¯èƒ½æ˜¯ä¸€ä¸ªå¿«é€Ÿé€šé“ï¼Œå¯ä»¥è·³è¿‡ä»åŸºæ€çš„Î¼å­åŸå­å½¢æˆæŸç¼šæ€çš„ç¼“æ…¢è·¯å¾„ï¼Œä»è€Œæé«˜Î¼CFå¾ªç¯é€Ÿç‡ã€‚å°½ç®¡æ¥è‡ªæ”¾å°„æ€§è¡°å˜çš„è°±ä½äºèƒ½é‡èŒƒå›´åœ¨$ 1.5 $è‡³$ 1.997 $åƒç”µå­ä¼ç‰¹ä¹‹é—´ï¼Œè¿™äº›è°±ä¸Î¼å­åŒ–æ°˜åŸå­çš„KÎ± Xå°„çº¿éå¸¸æ¥è¿‘ï¼Œä½†æœ€å…ˆè¿›çš„å¾®çƒ­é‡è®¡ä»ç„¶å¯ä»¥åŒºåˆ†å®ƒä»¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.01756v4">PDF</a> 19 pages, 16 figures, 4 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶ä»ç†è®ºä¸Šæ¢è®¨äº†å…±æŒ¯æ€ä¸‹çš„Î¼onicæ°˜åˆ†å­çš„æ”¾å°„æ€§è¡°å˜äº§ç”Ÿçš„xå°„çº¿å…‰è°±ï¼Œè¿™å¯¹äºæ–°çš„åŠ¨åŠ›å­¦æ¨¡å‹ä¸‹çš„Î¼å­å‚¬åŒ–èšå˜ï¼ˆÎ¼CFï¼‰å…·æœ‰å…³é”®ä½œç”¨ã€‚é€šè¿‡ç²¾ç¡®çš„ä¸‰ä½“æ³¢å‡½æ•°ï¼Œå¾—åˆ°äº†æ ¹æ®ä¸¤ä¸ªåŸå­æ ¸çš„å¾„å‘åˆ†å¸ƒç‰¹å¾å½¢çŠ¶ç‹¬ç‰¹çš„xå°„çº¿å…‰è°±ã€‚æœ¬ç ”ç©¶å¯¹xå°„çº¿å…‰è°±çš„æ”¶æ•›æ€§è¿›è¡Œäº†ä»”ç»†ç ”ç©¶ï¼Œå®ç°äº†é•¿åº¦å’Œé€Ÿåº¦è®¡é‡è®¡ç®—ä¹‹é—´çš„åè®®ã€‚æ­ç¤ºäº†éç»çƒ­åŠ¨èƒ½åˆ†å¸ƒçš„è¡°å˜ç‰‡æ®µï¼Œè¡¨æ˜æ”¾å°„æ€§è¡°å˜æˆä¸ºÎ¼onicåŸå­çš„çƒ­æºã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æ¢è®¨äº†ç›´æ¥äº§ç”ŸæŸç¼šæ€Î¼onicåˆ†å­çš„è¡°å˜åˆ†æ”¯ã€‚ä¸€äº›å…±æŒ¯æ€çš„ddÎ¼âˆ—å’ŒdtÎ¼âˆ—è¢«å‘ç°å…·æœ‰é«˜çš„åˆ†æ”¯æ¯”åˆ°æŸç¼šæ€ï¼Œå…¶ä¸­å‘ç”Ÿåˆ†å­å†…çš„æ ¸èšå˜ã€‚ä»äºšç¨³æ€Î¼onicåŸå­å½¢æˆæŸç¼šæ€muonicåˆ†å­å¯èƒ½æ˜¯Î¼CFå¾ªç¯çš„å¿«é€šé“ï¼Œé¿å…äº†ä»åŸºæ€muonicåŸå­å½¢æˆæŸç¼šæ€çš„ç¼“æ…¢è·¯å¾„ï¼Œä»è€Œæé«˜äº†Î¼CFå¾ªç¯é€Ÿç‡ã€‚è™½ç„¶æ¥è‡ªæ”¾å°„æ€§è¡°å˜çš„å…‰è°±ä½äº$ 1.5 $è‡³$ 1.997 $åƒç”µå­ä¼çš„èƒ½é‡èŒƒå›´å†…ï¼Œæ¥è¿‘muonicæ°˜åŸå­çš„KÎ± xå°„çº¿ï¼ˆ$ 1.997 $åƒç”µå­ä¼ï¼‰ï¼Œä½†æœ€å…ˆè¿›çš„å¾®çƒ­é‡è®¡å¯ä»¥åŒºåˆ†å®ƒä»¬ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™ä¸€å‘ç°æœ‰åŠ©äºæé«˜å¯¹muonicåŸå­è¡Œä¸ºçš„äº†è§£å’Œæ ¸èšå˜ååº”çš„é€Ÿç‡æ§åˆ¶ç²¾åº¦ã€‚é€šè¿‡å¯¹å…¶æ”¾å°„æ€§å’Œèƒ½é‡åˆ†å¸ƒçš„ç ”ç©¶ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦çš„ç†è®ºåŸºç¡€å’Œå®éªŒæŒ‡å¯¼ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶ä»ç†è®ºä¸Šæ¢è®¨äº†Î¼onicæ°˜åˆ†å­åœ¨å…±æŒ¯çŠ¶æ€ä¸‹çš„æ”¾å°„æ€§è¡°å˜äº§ç”Ÿçš„xå°„çº¿å…‰è°±ç‰¹å¾ï¼Œåœ¨åŠ¨åŠ›å­¦æ¨¡å‹ä¸­å…·æœ‰å…³é”®ä½œç”¨ã€‚</li>
<li>é€šè¿‡ç²¾ç¡®çš„ä¸‰ä½“æ³¢å‡½æ•°è·å¾—äº†å…·æœ‰ç‰¹å¾å½¢çŠ¶çš„xå°„çº¿å…‰è°±ï¼Œåæ˜ äº†ä¸¤ä¸ªåŸå­æ ¸çš„å¾„å‘åˆ†å¸ƒã€‚</li>
<li>ç ”ç©¶è¾¾æˆäº†é•¿åº¦å’Œé€Ÿåº¦æµ‹é‡è®¡ç®—çš„å…±è¯†ï¼ŒéªŒè¯äº†è®¡ç®—ç»“æœçš„å‡†ç¡®æ€§ã€‚</li>
<li>æ­ç¤ºäº†æ”¾å°„æ€§è¡°å˜ä¸­çš„éç»çƒ­åŠ¨èƒ½åˆ†å¸ƒç‰¹å¾ï¼Œè¯æ˜äº†å…¶ä¸ºÎ¼onicåŸå­çš„çƒ­æºä¹‹ä¸€ã€‚</li>
<li>ç ”ç©¶å‘ç°æŸäº›å…±æŒ¯æ€å¦‚ddÎ¼âˆ—å’ŒdtÎ¼âˆ—ç›´æ¥äº§ç”ŸæŸç¼šæ€muonicåˆ†å­å…·æœ‰è¾ƒé«˜çš„åˆ†æ”¯æ¯”ã€‚è¿™å¯¹æœªæ¥çš„åˆ†å­æ ¸èšå˜ç ”ç©¶æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.01756">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2407.01756v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2407.01756v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2407.01756v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2407.01756v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2407.01756v4/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2407.01756v4/page_5_1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Generalizable-Disaster-Damage-Assessment-via-Change-Detection-with-Vision-Foundation-Model"><a href="#Generalizable-Disaster-Damage-Assessment-via-Change-Detection-with-Vision-Foundation-Model" class="headerlink" title="Generalizable Disaster Damage Assessment via Change Detection with   Vision Foundation Model"></a>Generalizable Disaster Damage Assessment via Change Detection with   Vision Foundation Model</h2><p><strong>Authors:Kyeongjin Ahn, Sungwon Han, Sungwon Park, Jihee Kim, Sangyoon Park, Meeyoung Cha</strong></p>
<p>The increasing frequency and intensity of natural disasters call for rapid and accurate damage assessment. In response, disaster benchmark datasets from high-resolution satellite imagery have been constructed to develop methods for detecting damaged areas. However, these methods face significant challenges when applied to previously unseen regions due to the limited geographical and disaster-type diversity in the existing datasets. We introduce DAVI (Disaster Assessment with VIsion foundation model), a novel approach that addresses domain disparities and detects structural damage at the building level without requiring ground-truth labels for target regions. DAVI combines task-specific knowledge from a model trained on source regions with task-agnostic knowledge from an image segmentation model to generate pseudo labels indicating potential damage in target regions. It then utilizes a two-stage refinement process, which operate at both pixel and image levels, to accurately identify changes in disaster-affected areas. Our evaluation, including a case study on the 2023 T&quot;urkiye earthquake, demonstrates that our model achieves exceptional performance across diverse terrains (e.g., North America, Asia, and the Middle East) and disaster types (e.g., wildfires, hurricanes, and tsunamis). This confirms its robustness in disaster assessment without dependence on ground-truth labels and highlights its practical applicability. </p>
<blockquote>
<p>è‡ªç„¶ç¾å®³çš„é¢‘ç‡å’Œå¼ºåº¦ä¸æ–­ä¸Šå‡ï¼Œè¦æ±‚è¿›è¡Œå¿«é€Ÿå’Œå‡†ç¡®çš„æŸå¤±è¯„ä¼°ã€‚ä¸ºæ­¤ï¼Œå·²ç»åˆ©ç”¨é«˜åˆ†è¾¨ç‡å«æ˜Ÿå›¾åƒæ„å»ºäº†ç¾å®³åŸºå‡†æ•°æ®é›†ï¼Œä»¥å¼€å‘æ£€æµ‹å—æŸåŒºåŸŸçš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œå½“è¿™äº›æ–¹æ³•åº”ç”¨äºæœªè§è¿‡çš„åŒºåŸŸæ—¶ï¼Œç”±äºç°æœ‰æ•°æ®é›†çš„åœ°ç†å’Œç¾å®³ç±»å‹å¤šæ ·æ€§æœ‰é™ï¼Œè¿™äº›æ–¹æ³•é¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬ä»‹ç»äº†DAVIï¼ˆåŸºäºè§†è§‰åŸºç¡€æ¨¡å‹çš„ç¾å®³è¯„ä¼°ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œè§£å†³äº†é¢†åŸŸå·®å¼‚é—®é¢˜ï¼Œå¹¶åœ¨æ— éœ€é’ˆå¯¹ç›®æ ‡åŒºåŸŸè·å–çœŸå®æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œä»¥å»ºç­‘å±‚é¢æ£€æµ‹ç»“æ„æ€§æŸä¼¤ã€‚DAVIç»“åˆäº†æºåŒºåŸŸè®­ç»ƒæ¨¡å‹äº§ç”Ÿçš„ç‰¹å®šä»»åŠ¡çŸ¥è¯†å’Œå›¾åƒåˆ†å‰²æ¨¡å‹çš„é€šç”¨ä»»åŠ¡çŸ¥è¯†ï¼Œä»¥ç”ŸæˆæŒ‡ç¤ºç›®æ ‡åŒºåŸŸæ½œåœ¨æŸä¼¤çš„ä¼ªæ ‡ç­¾ã€‚ç„¶åï¼Œå®ƒé‡‡ç”¨ä¸¤çº§ç»†åŒ–è¿‡ç¨‹ï¼Œåœ¨åƒç´ å’Œå›¾åƒå±‚é¢è¿›è¡Œæ“ä½œï¼Œä»¥å‡†ç¡®è¯†åˆ«å—ç¾åŒºåŸŸçš„å˜åŒ–ã€‚æˆ‘ä»¬çš„è¯„ä¼°åŒ…æ‹¬å¯¹2023å¹´åœŸè€³å…¶åœ°éœ‡çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯æ˜æˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸åŒåœ°å½¢ï¼ˆå¦‚åŒ—ç¾ã€äºšæ´²å’Œä¸­ä¸œï¼‰å’Œç¾å®³ç±»å‹ï¼ˆå¦‚é‡ç«ã€é£“é£å’Œæµ·å•¸ï¼‰ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚è¿™è¯å®äº†å…¶åœ¨æ— éœ€ä¾èµ–çœŸå®æ ‡ç­¾è¿›è¡Œç¾å®³è¯„ä¼°æ–¹é¢çš„ç¨³å¥æ€§ï¼Œå¹¶å‡¸æ˜¾äº†å…¶å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.08020v2">PDF</a> Accepted to AAAI 2025 (oral)</p>
<p><strong>Summary</strong></p>
<p>é«˜é¢‘ç‡å’Œå¼ºåº¦çš„è‡ªç„¶ç¾å®³éœ€è¦è¿›è¡Œå¿«é€Ÿå‡†ç¡®çš„ç¾å®³æŸå¤±è¯„ä¼°ã€‚ä¸ºåº”å¯¹è¿™ä¸€éœ€æ±‚ï¼Œæ„å»ºäº†åŸºäºé«˜åˆ†è¾¨ç‡å«æ˜Ÿå½±åƒçš„ç¾å®³åŸºå‡†æ•°æ®é›†ä»¥å¼€å‘æ£€æµ‹å—æŸåŒºåŸŸçš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨åº”ç”¨äºæœªè§åŒºåŸŸæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºç°æœ‰æ•°æ®é›†çš„åœ°ç†å’Œç¾å®³ç±»å‹å¤šæ ·æ€§æœ‰é™ã€‚æˆ‘ä»¬å¼•å…¥äº†DAVIï¼ˆåŸºäºVision foundationæ¨¡å‹çš„ç¾å®³è¯„ä¼°ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œè§£å†³äº†é¢†åŸŸå·®å¼‚é—®é¢˜ï¼Œå¹¶åœ¨æ— éœ€ç›®æ ‡åŒºåŸŸåœ°é¢çœŸå®æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œä»¥å»ºç­‘çº§åˆ«æ£€æµ‹ç»“æ„æŸä¼¤ã€‚DAVIç»“åˆäº†æ¥è‡ªæºåŒºåŸŸè®­ç»ƒçš„æ¨¡å‹çš„ç‰¹å®šä»»åŠ¡çŸ¥è¯†å’Œæ¥è‡ªå›¾åƒåˆ†å‰²æ¨¡å‹çš„éç‰¹å®šä»»åŠ¡çŸ¥è¯†ï¼Œç”ŸæˆæŒ‡ç¤ºç›®æ ‡åŒºåŸŸæ½œåœ¨æŸä¼¤çš„ä¼ªæ ‡ç­¾ã€‚ç„¶åï¼Œå®ƒåˆ©ç”¨åƒç´ å’Œå›¾åƒçº§åˆ«çš„ä¸¤é˜¶æ®µç»†åŒ–è¿‡ç¨‹ï¼Œå‡†ç¡®è¯†åˆ«å—ç¾åŒºåŸŸçš„å˜åŒ–ã€‚æˆ‘ä»¬çš„è¯„ä¼°åŒ…æ‹¬å¯¹2023å¹´åœŸè€³å…¶åœ°éœ‡çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸åŒåœ°å½¢å’Œç¾å®³ç±»å‹ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œè¿™è¯å®äº†å…¶åœ¨ç¾å®³è¯„ä¼°ä¸­çš„ç¨³å¥æ€§ï¼Œå¹¶å¼ºè°ƒäº†å…¶ä¸ä¾èµ–åœ°é¢çœŸå®æ ‡ç­¾çš„å®é™…é€‚ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç„¶ç¾å®³çš„é¢‘ç‡å’Œå¼ºåº¦å¢åŠ ï¼Œéœ€è¦å¿«é€Ÿå‡†ç¡®çš„ç¾å®³æŸå¤±è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>ç¾å®³åŸºå‡†æ•°æ®é›†ç”¨äºå¼€å‘æ£€æµ‹å—æŸåŒºåŸŸçš„æ–¹æ³•ï¼Œä½†ç°æœ‰æ•°æ®é›†åœ°ç†å’Œç¾å®³ç±»å‹å¤šæ ·æ€§æœ‰é™ã€‚</li>
<li>DAVIæ–¹æ³•ç»“åˆç‰¹å®šä»»åŠ¡å’Œéç‰¹å®šä»»åŠ¡çŸ¥è¯†ï¼Œç”ŸæˆæŒ‡ç¤ºç›®æ ‡åŒºåŸŸæ½œåœ¨æŸä¼¤çš„ä¼ªæ ‡ç­¾ã€‚</li>
<li>DAVIé€šè¿‡ä¸¤é˜¶æ®µç»†åŒ–è¿‡ç¨‹å‡†ç¡®è¯†åˆ«å—ç¾åŒºåŸŸçš„å˜åŒ–ã€‚</li>
<li>DAVIæ¨¡å‹åœ¨ä¸åŒåœ°å½¢å’Œç¾å®³ç±»å‹ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ã€‚<br>6.DAVIæ¨¡å‹ä¸ä¾èµ–åœ°é¢çœŸå®æ ‡ç­¾ï¼Œå…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.08020">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2406.08020v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2406.08020v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2406.08020v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2406.08020v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Stitching-Fine-tuning-Re-training-A-SAM-enabled-Framework-for-Semi-supervised-3D-Medical-Image-Segmentation"><a href="#Stitching-Fine-tuning-Re-training-A-SAM-enabled-Framework-for-Semi-supervised-3D-Medical-Image-Segmentation" class="headerlink" title="Stitching, Fine-tuning, Re-training: A SAM-enabled Framework for   Semi-supervised 3D Medical Image Segmentation"></a>Stitching, Fine-tuning, Re-training: A SAM-enabled Framework for   Semi-supervised 3D Medical Image Segmentation</h2><p><strong>Authors:Shumeng Li, Lei Qi, Qian Yu, Jing Huo, Yinghuan Shi, Yang Gao</strong></p>
<p>Segment Anything Model (SAM) fine-tuning has shown remarkable performance in medical image segmentation in a fully supervised manner, but requires precise annotations. To reduce the annotation cost and maintain satisfactory performance, in this work, we leverage the capabilities of SAM for establishing semi-supervised medical image segmentation models. Rethinking the requirements of effectiveness, efficiency, and compatibility, we propose a three-stage framework, i.e., Stitching, Fine-tuning, and Re-training (SFR). The current fine-tuning approaches mostly involve 2D slice-wise fine-tuning that disregards the contextual information between adjacent slices. Our stitching strategy mitigates the mismatch between natural and 3D medical images. The stitched images are then used for fine-tuning SAM, providing robust initialization of pseudo-labels. Afterwards, we train a 3D semi-supervised segmentation model while maintaining the same parameter size as the conventional segmenter such as V-Net. Our SFR framework is plug-and-play, and easily compatible with various popular semi-supervised methods. We also develop an extended framework SFR$^+$ with selective fine-tuning and re-training through confidence estimation. Extensive experiments validate that our SFR and SFR$^+$ achieve significant improvements in both moderate annotation and scarce annotation across five datasets. In particular, SFR framework improves the Dice score of Mean Teacher from 29.68% to 74.40% with only one labeled data of LA dataset. </p>
<blockquote>
<p>SAMï¼ˆSegment Anything Modelï¼‰å¾®è°ƒåœ¨å…¨ç›‘ç£æ–¹å¼ä¸‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œä½†è¿™éœ€è¦ç²¾ç¡®çš„æ ‡æ³¨ã€‚ä¸ºäº†å‡å°‘æ ‡æ³¨æˆæœ¬å¹¶ä¿æŒä»¤äººæ»¡æ„çš„æ€§èƒ½ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨SAMçš„èƒ½åŠ›æ¥å»ºç«‹åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚è€ƒè™‘åˆ°æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œå…¼å®¹æ€§çš„è¦æ±‚ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„æ¡†æ¶ï¼Œå³æ‹¼æ¥ã€å¾®è°ƒã€å†è®­ç»ƒï¼ˆSFRï¼‰ã€‚ç›®å‰çš„å¾®è°ƒæ–¹æ³•å¤§å¤šæ¶‰åŠ2Dåˆ‡ç‰‡çº§çš„å¾®è°ƒï¼Œè¿™å¿½ç•¥äº†ç›¸é‚»åˆ‡ç‰‡ä¹‹é—´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ‹¼æ¥ç­–ç•¥ç¼“è§£äº†è‡ªç„¶å›¾åƒå’Œ3DåŒ»å­¦å›¾åƒä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æ‹¼æ¥åçš„å›¾åƒå¯¹SAMè¿›è¡Œå¾®è°ƒï¼Œä¸ºä¼ªæ ‡ç­¾æä¾›ç¨³å¥çš„åˆå§‹åŒ–ã€‚ä¹‹åï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ª3DåŠç›‘ç£åˆ†å‰²æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒä¸å¸¸è§„åˆ†å‰²å™¨ï¼ˆå¦‚V-Netï¼‰ç›¸åŒçš„å‚æ•°å¤§å°ã€‚æˆ‘ä»¬çš„SFRæ¡†æ¶å³æ’å³ç”¨ï¼Œå¾ˆå®¹æ˜“ä¸å„ç§æµè¡Œçš„åŠç›‘ç£æ–¹æ³•å…¼å®¹ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªæ‰©å±•æ¡†æ¶SFR+ï¼Œé€šè¿‡ç½®ä¿¡åº¦ä¼°è®¡è¿›è¡Œé€‰æ‹©æ€§å¾®è°ƒã€‚å¤§é‡å®éªŒéªŒè¯ï¼Œæˆ‘ä»¬çš„SFRå’ŒSFR+åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šï¼Œæ— è®ºæ˜¯åœ¨ä¸­ç­‰æ ‡æ³¨è¿˜æ˜¯ç¨€ç¼ºæ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œéƒ½å®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ç‰¹åˆ«æ˜¯SFRæ¡†æ¶ï¼Œåœ¨LAæ•°æ®é›†ä¸Šä»…ä½¿ç”¨ä¸€ä¸ªæ ‡ç­¾æ•°æ®ï¼Œå°±å°†Mean Teacherçš„Diceå¾—åˆ†ä»29.68%æé«˜åˆ°äº†74.40%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.11229v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹åˆ©ç”¨Segment Anything Modelï¼ˆSAMï¼‰è¿›è¡Œfine-tuningï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä¸ºé™ä½æ ‡æ³¨æˆæœ¬å¹¶ä¿æŒæ€§èƒ½ï¼Œæå‡ºä¸€ä¸ªä¸‰é˜¶æ®µçš„æ¡†æ¶ï¼šæ‹¼æ¥ï¼ˆStitchingï¼‰ã€fine-tuningå’Œå†è®­ç»ƒï¼ˆRe-trainingï¼Œç®€ç§°SFRï¼‰ã€‚ç°æœ‰fine-tuningæ–¹æ³•å¤šåŸºäº2Dåˆ‡ç‰‡ï¼Œå¿½ç•¥äº†ç›¸é‚»åˆ‡ç‰‡é—´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æœ¬ç ”ç©¶é‡‡ç”¨æ‹¼æ¥ç­–ç•¥ï¼Œç¼“è§£è‡ªç„¶å›¾åƒä¸åŒ»å­¦å›¾åƒçš„3Dç»“æ„ä¸åŒ¹é…é—®é¢˜ã€‚åˆ©ç”¨æ‹¼æ¥åçš„å›¾åƒè¿›è¡ŒSAMçš„fine-tuningï¼Œä¸ºä¼ªæ ‡ç­¾æä¾›ç¨³å¥çš„åˆå§‹åŒ–ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè®­ç»ƒä¸€ä¸ªä¸å¸¸è§„åˆ†å‰²å™¨å¦‚V-Netç›¸åŒå‚æ•°è§„æ¨¡çš„3DåŠç›‘ç£åˆ†å‰²æ¨¡å‹ã€‚SFRæ¡†æ¶æ˜“äºé›†æˆå„ç§æµè¡Œçš„åŠç›‘ç£æ–¹æ³•ã€‚è¿˜å¼€å‘äº†æ‰©å±•æ¡†æ¶SFR+ï¼Œé€šè¿‡ç½®ä¿¡åº¦ä¼°è®¡è¿›è¡Œé€‰æ‹©æ€§fine-tuningå’Œå†è®­ç»ƒã€‚å®éªŒè¯æ˜ï¼Œåœ¨äº”ä¸ªæ•°æ®é›†ä¸Šï¼Œæ— è®ºæ˜¯ä¸­ç­‰æ ‡æ³¨è¿˜æ˜¯ç¨€ç¼ºæ ‡æ³¨ï¼ŒSFRå’ŒSFR+å‡æœ‰æ˜¾è‘—æé«˜ã€‚ç‰¹åˆ«æ˜¯åœ¨LAæ•°æ®é›†ä¸Šï¼Œä»…ä½¿ç”¨ä¸€ä¸ªæ ‡æ³¨æ•°æ®ï¼ŒSFRæ¡†æ¶å°†Mean Teacherçš„Diceå¾—åˆ†ä»29.68%æé«˜åˆ°74.40%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨SAMè¿›è¡ŒåŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹çš„fine-tuningï¼Œä»¥é™ä½æ ‡æ³¨æˆæœ¬ã€‚</li>
<li>æå‡ºä¸€ä¸ªä¸‰é˜¶æ®µçš„æ¡†æ¶SFRï¼ŒåŒ…æ‹¬æ‹¼æ¥ã€fine-tuningå’Œå†è®­ç»ƒã€‚</li>
<li>ç°æœ‰fine-tuningæ–¹æ³•ä¸»è¦åŸºäº2Dåˆ‡ç‰‡ï¼Œå¿½ç•¥äº†ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼›ç ”ç©¶é‡‡ç”¨æ‹¼æ¥ç­–ç•¥è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨æ‹¼æ¥åçš„å›¾åƒè¿›è¡ŒSAMçš„fine-tuningï¼Œä¸ºä¼ªæ ‡ç­¾æä¾›ç¨³å¥åˆå§‹åŒ–ã€‚</li>
<li>è®­ç»ƒä¸€ä¸ªä¸å¸¸è§„åˆ†å‰²å™¨ç›¸åŒå‚æ•°è§„æ¨¡çš„3DåŠç›‘ç£åˆ†å‰²æ¨¡å‹ã€‚</li>
<li>SFRæ¡†æ¶æ˜“äºé›†æˆå„ç§æµè¡Œçš„åŠç›‘ç£æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.11229">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2403.11229v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2403.11229v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2403.11229v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2403.11229v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2403.11229v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2403.11229v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="VIS-MAE-An-Efficient-Self-supervised-Learning-Approach-on-Medical-Image-Segmentation-and-Classification"><a href="#VIS-MAE-An-Efficient-Self-supervised-Learning-Approach-on-Medical-Image-Segmentation-and-Classification" class="headerlink" title="VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image   Segmentation and Classification"></a>VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image   Segmentation and Classification</h2><p><strong>Authors:Zelong Liu, Andrew Tieu, Nikhil Patel, Georgios Soultanidis, Louisa Deyer, Ying Wang, Sean Huver, Alexander Zhou, Yunhao Mei, Zahi A. Fayad, Timothy Deyer, Xueyan Mei</strong></p>
<p>Artificial Intelligence (AI) has the potential to revolutionize diagnosis and segmentation in medical imaging. However, development and clinical implementation face multiple challenges including limited data availability, lack of generalizability, and the necessity to incorporate multi-modal data effectively. A foundation model, which is a large-scale pre-trained AI model, offers a versatile base that can be adapted to a variety of specific tasks and contexts. Here, we present VIsualization and Segmentation Masked AutoEncoder (VIS-MAE), novel model weights specifically designed for medical imaging. Specifically, VIS-MAE is trained on a dataset of 2.5 million unlabeled images from various modalities (CT, MR, PET,X-rays, and ultrasound), using self-supervised learning techniques. It is then adapted to classification and segmentation tasks using explicit labels. VIS-MAE has high label efficiency, outperforming several benchmark models in both in-domain and out-of-domain applications. In addition, VIS-MAE has improved label efficiency as it can achieve similar performance to other models with a reduced amount of labeled training data (50% or 80%) compared to other pre-trained weights. VIS-MAE represents a significant advancement in medical imaging AI, offering a generalizable and robust solution for improving segmentation and classification tasks while reducing the data annotation workload. The source code of this work is available at <a target="_blank" rel="noopener" href="https://github.com/lzl199704/VIS-MAE">https://github.com/lzl199704/VIS-MAE</a>. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å…·æœ‰æ½œåŠ›æ”¹å˜åŒ»å­¦å½±åƒçš„è¯Šæ–­å’Œåˆ†å‰²æ–¹å¼ã€‚ç„¶è€Œï¼Œå…¶å‘å±•å’Œä¸´åºŠå®æ–½é¢ä¸´ç€å¤šé‡æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®æœ‰é™ã€é€šç”¨æ€§ä¸è¶³ä»¥åŠéœ€è¦æœ‰æ•ˆåœ°æ•´åˆå¤šæ¨¡æ€æ•°æ®ã€‚åŸºç¡€æ¨¡å‹æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡é¢„è®­ç»ƒçš„AIæ¨¡å‹ï¼Œæä¾›äº†ä¸€ä¸ªé€šç”¨çš„åŸºç¡€ï¼Œå¯ä»¥é€‚åº”å„ç§ç‰¹å®šä»»åŠ¡å’Œä¸Šä¸‹æ–‡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºå¯è§†åŒ–åŠåˆ†å‰²æ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVIS-MAEï¼‰ï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºåŒ»å­¦å½±åƒè®¾è®¡çš„å…¨æ–°æ¨¡å‹æƒé‡ã€‚å…·ä½“æ¥è¯´ï¼ŒVIS-MAEæ˜¯åœ¨å„ç§æ¨¡æ€ï¼ˆCTã€MRã€PETã€Xå°„çº¿å’Œè¶…å£°ï¼‰çš„250ä¸‡å¼ æ— æ ‡ç­¾å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œä½¿ç”¨äº†è‡ªæˆ‘ç›‘ç£çš„å­¦ä¹ æŠ€æœ¯ã€‚ç„¶åï¼Œå®ƒä½¿ç”¨æ˜ç¡®çš„æ ‡ç­¾é€‚åº”åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ã€‚VIS-MAEå…·æœ‰é«˜çš„æ ‡ç­¾æ•ˆç‡ï¼Œåœ¨é¢†åŸŸå†…å’Œé¢†åŸŸå¤–çš„åº”ç”¨ä¸­å‡ä¼˜äºå¤šä¸ªåŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒVIS-MAEæé«˜äº†æ ‡ç­¾æ•ˆç‡ï¼Œä¸å…¶ä»–é¢„è®­ç»ƒæƒé‡ç›¸æ¯”ï¼Œå®ƒåœ¨å‡å°‘50%æˆ–80%çš„æœ‰æ ‡ç­¾è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥è¾¾åˆ°ä¸å…¶ä»–æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚VIS-MAEåœ¨åŒ»å­¦å½±åƒAIä¸­ä»£è¡¨äº†é‡å¤§è¿›æ­¥ï¼Œä¸ºæ”¹å–„åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡åŒæ—¶å‡å°‘æ•°æ®æ ‡æ³¨å·¥ä½œé‡æä¾›äº†é€šç”¨å’Œç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥å·¥ä½œçš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lzl199704/VIS-MAE">https://github.com/lzl199704/VIS-MAE</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.01034v3">PDF</a> Accepted at MLMI@MICCAI (Workshop on Machine Learning in Medical   Imaging at MICCAI 2024))</p>
<p><strong>Summary</strong></p>
<p>AIæŠ€æœ¯å…·æœ‰åœ¨åŒ»å­¦æˆåƒä¸­å˜é©è¯Šæ–­å’Œåˆ†å‰²çš„æ½œåŠ›ã€‚é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬æ•°æ®æœ‰é™ã€ç¼ºä¹æ™®éæ€§å’Œéœ€è¦æœ‰æ•ˆæ•´åˆå¤šæ¨¡æ€æ•°æ®ã€‚ä¸€é¡¹æ–°çš„è§†è§‰åŒ–åŠåˆ†å‰²æ©ç è‡ªç¼–ç å™¨ï¼ˆVIS-MAEï¼‰æ¨¡å‹ä¸“ä¸ºåŒ»å­¦æˆåƒè®¾è®¡ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§æ¨¡æ€ï¼ˆCTã€MRã€PETã€Xå…‰åŠè¶…å£°æ³¢ï¼‰çš„250ä¸‡å¼ æ— æ ‡ç­¾å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶é‡‡ç”¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ æŠ€æœ¯ã€‚å…¶é€‚åº”åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡æ—¶é‡‡ç”¨æ˜ç¡®çš„æ ‡ç­¾ã€‚VIS-MAEæ‹¥æœ‰é«˜æ ‡ç­¾æ•ˆç‡ï¼Œåœ¨å†…éƒ¨å’Œå¤–éƒ¨åº”ç”¨ä¸­è¡¨ç°å‡ä¼˜äºå¤šä¸ªåŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œä¸å…¶ä»–é¢„è®­ç»ƒæƒé‡ç›¸æ¯”ï¼ŒVIS-MAEå¯æœ›ä½¿ç”¨è¾ƒå°‘çš„æœ‰æ ‡ç­¾è®­ç»ƒæ•°æ®ï¼ˆä»…éœ€ä¸€åŠæˆ–å…«æˆï¼‰å³å¯å®ç°ç±»ä¼¼æ€§èƒ½ã€‚å®ƒæ˜¾è‘—æ¨åŠ¨äº†åŒ»å­¦æˆåƒAIçš„å‘å±•ï¼Œæä¾›äº†ä¸€ä¸ªé€šç”¨å’Œç¨³å¥çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥æ”¹å–„åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡å¹¶å‡å°‘æ•°æ®æ³¨é‡Šå·¥ä½œé‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨åŒ»å­¦æˆåƒä¸­å…·æœ‰æ”¹å˜è¯Šæ–­å’Œåˆ†å‰²çš„å·¨å¤§æ½œåŠ›ã€‚</li>
<li>å¼€å‘ä¸å®æ–½é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚æ•°æ®æœ‰é™æ€§ã€ç¼ºä¹æ™®éæ€§ä»¥åŠå¤šæ¨¡æ€æ•°æ®æ•´åˆçš„å¤æ‚æ€§ã€‚</li>
<li>VIS-MAEæ¨¡å‹æ˜¯ä¸€ç§ä¸“ä¸ºåŒ»å­¦æˆåƒè®¾è®¡çš„è§†è§‰åŒ–åŠåˆ†å‰²æ©ç è‡ªç¼–ç å™¨ã€‚</li>
<li>VIS-MAEæ¨¡å‹é‡‡ç”¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ æŠ€æœ¯åœ¨å¤šæ¨¡æ€æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
<li>VIS-MAEé€‚åº”äºåˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡å¹¶å…·æœ‰é«˜æ ‡ç­¾æ•ˆç‡ã€‚</li>
<li>VIS-MAEåœ¨å†…éƒ¨å’Œå¤–éƒ¨åº”ç”¨ä¸­è¡¨ç°å‡ä¼˜äºå¤šä¸ªåŸºå‡†æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.01034">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2402.01034v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-24\./crop_åŒ»å­¦å›¾åƒ/2402.01034v3/page_3_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-24/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-24/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-24/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-515a037586c879ba0134a2917a732a6e.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-24  A Domain Adaptation Framework for Speech Recognition Systems with Only   Synthetic data
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-24/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a42362520175822e3979e92166715baa.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-24  Accelerate High-Quality Diffusion Models with Inner Loop Feedback
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">16905.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
