<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-26  NOVER Incentive Training for Language Models via Verifier-Free   Reinforcement Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c61c772c5fd765f8c1f3570e98718fc8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-26-æ›´æ–°"><a href="#2025-05-26-æ›´æ–°" class="headerlink" title="2025-05-26 æ›´æ–°"></a>2025-05-26 æ›´æ–°</h1><h2 id="NOVER-Incentive-Training-for-Language-Models-via-Verifier-Free-Reinforcement-Learning"><a href="#NOVER-Incentive-Training-for-Language-Models-via-Verifier-Free-Reinforcement-Learning" class="headerlink" title="NOVER: Incentive Training for Language Models via Verifier-Free   Reinforcement Learning"></a>NOVER: Incentive Training for Language Models via Verifier-Free   Reinforcement Learning</h2><p><strong>Authors:Wei Liu, Siya Qi, Xinyu Wang, Chen Qian, Yali Du, Yulan He</strong></p>
<p>Recent advances such as DeepSeek R1-Zero highlight the effectiveness of incentive training, a reinforcement learning paradigm that computes rewards solely based on the final answer part of a language modelâ€™s output, thereby encouraging the generation of intermediate reasoning steps. However, these methods fundamentally rely on external verifiers, which limits their applicability to domains like mathematics and coding where such verifiers are readily available. Although reward models can serve as verifiers, they require high-quality annotated data and are costly to train. In this work, we propose NOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning framework that requires only standard supervised fine-tuning data with no need for an external verifier. NOVER enables incentive training across a wide range of text-to-text tasks and outperforms the model of the same size distilled from large reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the flexibility of NOVER enables new possibilities for optimizing large language models, such as inverse incentive training. </p>
<blockquote>
<p>æœ€è¿‘çš„è¿›å±•ï¼Œå¦‚DeepSeek R1-Zeroï¼Œå‡¸æ˜¾äº†æ¿€åŠ±è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚æ¿€åŠ±è®­ç»ƒæ˜¯ä¸€ç§åŸºäºæœ€ç»ˆç­”æ¡ˆçš„è¯­è¨€æ¨¡å‹è¾“å‡ºéƒ¨åˆ†æ¥è®¡ç®—å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œä»è€Œé¼“åŠ±ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»æ ¹æœ¬ä¸Šä¾èµ–äºå¤–éƒ¨éªŒè¯å™¨ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨è¯¸å¦‚æ•°å­¦å’Œç¼–ç ç­‰é¢†åŸŸçš„åº”ç”¨ï¼Œåœ¨è¿™äº›é¢†åŸŸå¯ä»¥å¾ˆå®¹æ˜“åœ°è·å¾—è¿™æ ·çš„éªŒè¯å™¨ã€‚è™½ç„¶å¥–åŠ±æ¨¡å‹å¯ä»¥ä½œä¸ºéªŒè¯å™¨ä½¿ç”¨ï¼Œä½†å®ƒä»¬éœ€è¦å¤§é‡çš„é«˜è´¨é‡æ³¨é‡Šæ•°æ®ï¼Œå¹¶ä¸”è®­ç»ƒæˆæœ¬é«˜æ˜‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†NOVERï¼ˆæ— éœ€éªŒè¯å™¨çš„å¼ºåŒ–å­¦ä¹ ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸€èˆ¬çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒåªéœ€è¦æ ‡å‡†çš„æœ‰ç›‘ç£å¾®è°ƒæ•°æ®ï¼Œæ— éœ€å¤–éƒ¨éªŒè¯å™¨ã€‚NOVERå¯ä»¥åœ¨å¹¿æ³›çš„æ–‡æœ¬åˆ°æ–‡æœ¬ä»»åŠ¡ä¸­è¿›è¡Œæ¿€åŠ±è®­ç»ƒï¼Œå¹¶ä¼˜äºä½¿ç”¨å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆå¦‚DeepSeek R1 671Bï¼‰è’¸é¦å¾—åˆ°çš„ç›¸åŒå¤§å°çš„æ¨¡å‹ï¼Œæ€§èƒ½æé«˜äº†7.7%ã€‚æ­¤å¤–ï¼ŒNOVERçš„çµæ´»æ€§ä¸ºä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†æ–°çš„å¯èƒ½æ€§ï¼Œå¦‚é€†å‘æ¿€åŠ±è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16022v1">PDF</a> 20 pages, 5 tables, 12 figures</p>
<p><strong>Summary</strong><br>åœ¨å¼ºåŒ–å­¦ä¹ é¢†åŸŸï¼Œæ¿€åŠ±è®­ç»ƒçš„æ–¹æ³•è¿‘æ¥å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä¾‹å¦‚DeepSeek R1-Zeroï¼Œå…¶ä»…åŸºäºæœ€ç»ˆç­”æ¡ˆæ¥è¯„ä¼°è¯­è¨€æ¨¡å‹çš„è¾“å‡ºã€‚ä½†è¿™ç§æ–¹æ³•ä¾èµ–äºå¤–éƒ¨éªŒè¯å™¨ï¼Œé™åˆ¶äº†å…¶åœ¨å¦‚æ•°å­¦å’Œç¼–ç¨‹ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚æœ¬æ–‡æå‡ºNOVERï¼ˆæ— éœ€éªŒè¯å™¨çš„å¼ºåŒ–å­¦ä¹ ï¼‰æ¡†æ¶ï¼Œåªéœ€æ ‡å‡†ç›‘ç£å¾®è°ƒæ•°æ®ï¼Œæ— éœ€å¤–éƒ¨éªŒè¯å™¨å³å¯è¿›è¡Œæ¿€åŠ±è®­ç»ƒã€‚NOVERåœ¨å¹¿æ³›çš„æ–‡æœ¬åˆ°æ–‡æœ¬ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶ä¸”ç›¸æ¯”DeepSeek R1 671Bè’¸é¦å‡ºçš„æ¨¡å‹ï¼Œæ€§èƒ½æå‡äº†7.7%ã€‚æ­¤å¤–ï¼ŒNOVERçš„çµæ´»æ€§è¿˜ä¸ºä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹å¸¦æ¥äº†æ–°çš„å¯èƒ½æ€§ï¼Œå¦‚åå‘æ¿€åŠ±è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¿€åŠ±è®­ç»ƒåœ¨å¼ºåŒ–å­¦ä¹ é¢†åŸŸå¤‡å—å…³æ³¨ï¼Œå¦‚DeepSeek R1-Zeroæ–¹æ³•åŸºäºæœ€ç»ˆç­”æ¡ˆè¿›è¡Œå¥–åŠ±è®¡ç®—ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–äºå¤–éƒ¨éªŒè¯å™¨ï¼Œé™åˆ¶äº†åº”ç”¨èŒƒå›´ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰é¢†åŸŸã€‚</li>
<li>æå‡ºNOVERæ¡†æ¶ï¼Œæ— éœ€å¤–éƒ¨éªŒè¯å™¨å³å¯è¿›è¡Œæ¿€åŠ±è®­ç»ƒã€‚</li>
<li>NOVERæ¡†æ¶é€‚ç”¨äºå¹¿æ³›çš„æ–‡æœ¬åˆ°æ–‡æœ¬ä»»åŠ¡ï¼Œæ€§èƒ½ä¼˜å¼‚ã€‚</li>
<li>ä¸DeepSeek R1 671Bæ¨¡å‹ç›¸æ¯”ï¼ŒNOVERæ€§èƒ½æå‡7.7%ã€‚</li>
<li>NOVERæ¡†æ¶çš„çµæ´»æ€§ä¸ºä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹å¸¦æ¥æ–°æœºä¼šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16022">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a48f545f4a5c67481e644b3e4d4ce28c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-824fb0e8750ec685e025e3bfc057e32c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43de329f1d9757e8d7e393d3de9a633e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16e7c09322d6a31dba2ff6b3e4dce628.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9daaa820d60a910b2c2ef7f2d50c9c5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c066077f1191210ed09d718466e3c93.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Pixel-Reasoner-Incentivizing-Pixel-Space-Reasoning-with-Curiosity-Driven-Reinforcement-Learning"><a href="#Pixel-Reasoner-Incentivizing-Pixel-Space-Reasoning-with-Curiosity-Driven-Reinforcement-Learning" class="headerlink" title="Pixel Reasoner: Incentivizing Pixel-Space Reasoning with   Curiosity-Driven Reinforcement Learning"></a>Pixel Reasoner: Incentivizing Pixel-Space Reasoning with   Curiosity-Driven Reinforcement Learning</h2><p><strong>Authors:Alex Su, Haozhe Wang, Weimin Ren, Fangzhen Lin, Wenhu Chen</strong></p>
<p>Chain-of-thought reasoning has significantly improved the performance of Large Language Models (LLMs) across various domains. However, this reasoning process has been confined exclusively to textual space, limiting its effectiveness in visually intensive tasks. To address this limitation, we introduce the concept of reasoning in the pixel-space. Within this novel framework, Vision-Language Models (VLMs) are equipped with a suite of visual reasoning operations, such as zoom-in and select-frame. These operations enable VLMs to directly inspect, interrogate, and infer from visual evidences, thereby enhancing reasoning fidelity for visual tasks. Cultivating such pixel-space reasoning capabilities in VLMs presents notable challenges, including the modelâ€™s initially imbalanced competence and its reluctance to adopt the newly introduced pixel-space operations. We address these challenges through a two-phase training approach. The first phase employs instruction tuning on synthesized reasoning traces to familiarize the model with the novel visual operations. Following this, a reinforcement learning (RL) phase leverages a curiosity-driven reward scheme to balance exploration between pixel-space reasoning and textual reasoning. With these visual operations, VLMs can interact with complex visual inputs, such as information-rich images or videos to proactively gather necessary information. We demonstrate that this approach significantly improves VLM performance across diverse visual reasoning benchmarks. Our 7B model, \model, achieves 84% on V* bench, 74% on TallyQA-Complex, and 84% on InfographicsVQA, marking the highest accuracy achieved by any open-source model to date. These results highlight the importance of pixel-space reasoning and the effectiveness of our framework. </p>
<blockquote>
<p>é“¾å¼æ€ç»´æ¨ç†å·²ç»æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸åŒé¢†åŸŸçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ç§æ¨ç†è¿‡ç¨‹ä¸€ç›´è¢«é™åˆ¶åœ¨æ–‡æœ¬ç©ºé—´å†…ï¼Œä½¿å…¶åœ¨è§†è§‰å¯†é›†å‹ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å—åˆ°é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åƒç´ ç©ºé—´æ¨ç†çš„æ¦‚å¿µã€‚åœ¨è¿™ä¸€æ–°æ¡†æ¶ä¸‹ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é…å¤‡äº†ä¸€ç³»åˆ—è§†è§‰æ¨ç†æ“ä½œï¼Œå¦‚æ”¾å¤§å’Œé€‰æ‹©å¸§ã€‚è¿™äº›æ“ä½œä½¿VLMèƒ½å¤Ÿç›´æ¥ä»è§†è§‰è¯æ®ä¸­è¿›è¡Œæ£€æŸ¥ã€è¯¢é—®å’Œæ¨æ–­ï¼Œä»è€Œæé«˜è§†è§‰ä»»åŠ¡çš„æ¨ç†ä¿çœŸåº¦ã€‚åœ¨VLMä¸­åŸ¹å…»è¿™ç§åƒç´ ç©ºé—´æ¨ç†èƒ½åŠ›é¢ä¸´ç€æ˜¾è‘—æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¨¡å‹åˆå§‹èƒ½åŠ›çš„ä¸å¹³è¡¡åŠå…¶å¯¹æ–°å¼•å…¥çš„åƒç´ ç©ºé—´æ“ä½œçš„æŠµè§¦ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚ç¬¬ä¸€é˜¶æ®µé‡‡ç”¨åˆæˆæ¨ç†è½¨è¿¹çš„æŒ‡ä»¤è°ƒæ•´ï¼Œä½¿æ¨¡å‹ç†Ÿæ‚‰æ–°çš„è§†è§‰æ“ä½œã€‚æ¥ä¸‹æ¥ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µåˆ©ç”¨åŸºäºå¥½å¥‡å¿ƒçš„å¥–åŠ±æ–¹æ¡ˆæ¥å¹³è¡¡åƒç´ ç©ºé—´æ¨ç†å’Œæ–‡æœ¬æ¨ç†ä¹‹é—´çš„æ¢ç´¢ã€‚æœ‰äº†è¿™äº›è§†è§‰æ“ä½œï¼ŒVLMå¯ä»¥ä¸å¤æ‚çš„è§†è§‰è¾“å…¥è¿›è¡Œäº¤äº’ï¼Œå¦‚ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒæˆ–è§†é¢‘ï¼Œä»¥ä¸»åŠ¨æ”¶é›†å¿…è¦çš„ä¿¡æ¯ã€‚æˆ‘ä»¬è¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†VLMåœ¨å¤šç§è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„7Bæ¨¡å‹å®ç°äº†V*åŸºå‡†æµ‹è¯•84%ã€TallyQA-ComplexåŸºå‡†æµ‹è¯•74%ã€InfographicsVQAåŸºå‡†æµ‹è¯•84%çš„å‡†ç¡®ç‡ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢ä»»ä½•å¼€æºæ¨¡å‹æ‰€å–å¾—çš„æœ€é«˜å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åƒç´ ç©ºé—´æ¨ç†çš„é‡è¦æ€§ä»¥åŠæˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15966v1">PDF</a> Haozhe Wang and Alex Su contributed equally and listed alphabetically</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†é“¾å¼æ€ç»´ï¼ˆChain-of-thought reasoningï¼‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½çš„æ”¹è¿›ã€‚ç„¶è€Œï¼Œè¯¥æ¨ç†è¿‡ç¨‹å±€é™äºæ–‡æœ¬ç©ºé—´ï¼Œåœ¨è§†è§‰å¯†é›†å‹ä»»åŠ¡ä¸­çš„æ•ˆæœæœ‰é™ã€‚ä¸ºåº”å¯¹è¿™ä¸€å±€é™ï¼Œæ–‡ä¸­æå‡ºäº†åƒç´ ç©ºé—´æ¨ç†çš„æ¦‚å¿µï¼Œå¹¶ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é…å¤‡äº†ä¸€ç³»åˆ—è§†è§‰æ¨ç†æ“ä½œï¼Œå¦‚æ”¾å¤§ã€é€‰æ‹©å¸§ç­‰ã€‚è¿™äº›æ“ä½œä½¿VLMèƒ½å¤Ÿç›´æ¥ä»è§†è§‰è¯æ®ä¸­æ£€æŸ¥ã€è¯¢é—®å’Œæ¨æ–­ï¼Œä»è€Œæé«˜è§†è§‰ä»»åŠ¡çš„æ¨ç†ä¿çœŸåº¦ã€‚åŸ¹å…»VLMçš„åƒç´ ç©ºé—´æ¨ç†èƒ½åŠ›é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¨¡å‹çš„åˆå§‹èƒ½åŠ›ä¸å‡è¡¡å’Œå¯¹æ–°å¼•å…¥çš„åƒç´ ç©ºé—´æ“ä½œçš„æ¥å—åº¦ä½ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæ–‡ä¸­é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡åˆæˆæ¨ç†è½¨è¿¹è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä½¿æ¨¡å‹ç†Ÿæ‚‰æ–°çš„è§†è§‰æ“ä½œã€‚æ¥ä¸‹æ¥æ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µï¼Œé‡‡ç”¨åŸºäºå¥½å¥‡å¿ƒçš„å¥–åŠ±æ–¹æ¡ˆï¼Œåœ¨åƒç´ ç©ºé—´æ¨ç†å’Œæ–‡æœ¬æ¨ç†ä¹‹é—´å®ç°å¹³è¡¡æ¢ç´¢ã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—æé«˜äº†VLMåœ¨å¤šç§è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ã€‚ä¾‹å¦‚ï¼Œå…¶7Bæ¨¡å‹åœ¨V*åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°84%ã€åœ¨å¤æ‚è®¡æ•°é—®ç­”ä»»åŠ¡ä¸Šè¾¾åˆ°74%ã€åœ¨ä¿¡æ¯å›¾è¡¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šè¾¾åˆ°84%ï¼Œæˆä¸ºè¿„ä»Šä¸ºæ­¢å…¬å¼€æ¨¡å‹ä¸­è¡¨ç°æœ€é«˜çš„ã€‚ç»“æœçªæ˜¾äº†åƒç´ ç©ºé—´æ¨ç†çš„é‡è¦æ€§åŠè¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é“¾å¼æ€ç»´å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æå‡æœ‰é‡è¦ä½œç”¨ï¼Œä½†åœ¨è§†è§‰å¯†é›†å‹ä»»åŠ¡ä¸­å—é™ã€‚</li>
<li>ä¸ºåº”å¯¹è¿™ä¸€å±€é™ï¼Œå¼•å…¥äº†åƒç´ ç©ºé—´æ¨ç†çš„æ¦‚å¿µï¼Œå¹¶ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹é…å¤‡è§†è§‰æ¨ç†æ“ä½œã€‚</li>
<li>è§†è§‰æ¨ç†æ“ä½œåŒ…æ‹¬æ”¾å¤§ã€é€‰æ‹©å¸§ç­‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç›´æ¥ä»è§†è§‰è¯æ®ä¸­æ£€æŸ¥ã€è¯¢é—®å’Œæ¨æ–­ã€‚</li>
<li>åŸ¹å…»è§†è§‰è¯­è¨€æ¨¡å‹çš„åƒç´ ç©ºé—´æ¨ç†èƒ½åŠ›é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚åˆå§‹èƒ½åŠ›ä¸å‡è¡¡å’Œå¯¹æ–°æ“ä½œçš„æ¥å—åº¦ä½ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é€šè¿‡åˆæˆæ¨ç†è½¨è¿¹è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´å’Œå¼ºåŒ–å­¦ä¹ é˜¶æ®µã€‚</li>
<li>è¿™ç§æ–¹æ³•æ˜¾è‘—æé«˜äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šç§è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15966">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8b8d8d06241b54139e6626fd289e3358.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21b36704bed0c7db46bd8427702f400f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d57b310f88595197e546e7e4c5563795.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-007f33c81c0e4837970c1723b22f4966.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Aligning-Dialogue-Agents-with-Global-Feedback-via-Large-Language-Model-Reward-Decomposition"><a href="#Aligning-Dialogue-Agents-with-Global-Feedback-via-Large-Language-Model-Reward-Decomposition" class="headerlink" title="Aligning Dialogue Agents with Global Feedback via Large Language Model   Reward Decomposition"></a>Aligning Dialogue Agents with Global Feedback via Large Language Model   Reward Decomposition</h2><p><strong>Authors:Dong Won Lee, Hae Won Park, Cynthia Breazeal, Louis-Philippe Morency</strong></p>
<p>We propose a large language model based reward decomposition framework for aligning dialogue agents using only a single session-level feedback signal. We leverage the reasoning capabilities of a frozen, pretrained large language model (LLM) to infer fine-grained local implicit rewards by decomposing global, session-level feedback. Our first text-only variant prompts the LLM to perform reward decomposition using only the dialogue transcript. The second multimodal variant incorporates additional behavioral cues, such as pitch, gaze, and facial affect, expressed as natural language descriptions. These inferred turn-level rewards are distilled into a lightweight reward model, which we utilize for RL-based fine-tuning for dialogue generation. We evaluate both text-only and multimodal variants against state-of-the-art reward decomposition methods and demonstrate notable improvements in human evaluations of conversation quality, suggesting that LLMs are strong reward decomposers that obviate the need for manual reward shaping and granular human feedback. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¥–åŠ±åˆ†è§£æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»…ä½¿ç”¨å•ä¸ªä¼šè¯çº§åˆ«çš„åé¦ˆä¿¡å·æ¥å¯¹å¯¹è¯ä»£ç†è¿›è¡Œå¯¹é½ã€‚æˆ‘ä»¬åˆ©ç”¨å†»ç»“çš„é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡åˆ†è§£å…¨å±€ä¼šè¯çº§åˆ«çš„åé¦ˆæ¥æ¨æ–­ç²¾ç»†çš„å±€éƒ¨éšå«å¥–åŠ±ã€‚æˆ‘ä»¬çš„ç¬¬ä¸€ç§çº¯æ–‡æœ¬å˜ä½“æç¤ºLLMä»…ä½¿ç”¨å¯¹è¯è®°å½•è¿›è¡Œå¥–åŠ±åˆ†è§£ã€‚ç¬¬äºŒç§å¤šæ¨¡å¼å˜ä½“åˆ™ç»“åˆäº†é¢å¤–çš„è¡Œä¸ºçº¿ç´¢ï¼Œå¦‚éŸ³è°ƒã€å‡è§†å’Œé¢éƒ¨è¡¨æƒ…ï¼Œä»¥è‡ªç„¶è¯­è¨€æè¿°çš„å½¢å¼è¡¨è¾¾ã€‚è¿™äº›æ¨æ–­å‡ºçš„å›åˆçº§å¥–åŠ±è¢«æç‚¼æˆä¸€ä¸ªè½»é‡çº§çš„å¥–åŠ±æ¨¡å‹ï¼Œæˆ‘ä»¬å°†å…¶ç”¨äºåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¯¹è¯ç”Ÿæˆå¾®è°ƒã€‚æˆ‘ä»¬å°†çº¯æ–‡æœ¬å’Œå¤šæ¨¡æ€å˜ä½“ä¸æœ€æ–°çš„å¥–åŠ±åˆ†è§£æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°å¯¹æ¯”ï¼Œåœ¨äººç±»å¯¹è¯è´¨é‡è¯„ä»·ä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œè¿™è¡¨æ˜LLMæ˜¯å¼ºå¤§çš„å¥–åŠ±åˆ†è§£å™¨ï¼Œæ— éœ€æ‰‹åŠ¨å¥–åŠ±å¡‘å½¢å’Œç²¾ç»†çš„äººç±»åé¦ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15922v1">PDF</a> 9 pages, 3 figures, 3 tables</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æå‡ºä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¥–åŠ±åˆ†è§£æ¡†æ¶ï¼Œä»…ä½¿ç”¨å•ä¸ªä¼šè¯çº§åé¦ˆä¿¡å·æ¥å¯¹å¯¹è¯ä»£ç†è¿›è¡Œå¯¹é½ã€‚åˆ©ç”¨å†»ç»“çš„é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡åˆ†è§£å…¨å±€ä¼šè¯çº§åé¦ˆæ¥æ¨æ–­ç²¾ç»†çš„å±€éƒ¨éšå¼å¥–åŠ±ã€‚æå‡ºæ–‡æœ¬æ¨¡æ€å’Œå¤šåª’ä½“æ¨¡æ€ä¸¤ç§å˜ä½“ï¼Œå‰è€…ä»…ä½¿ç”¨å¯¹è¯æ–‡æœ¬æ¥æç¤ºLLMè¿›è¡Œå¥–åŠ±åˆ†è§£ï¼Œåè€…åˆ™ç»“åˆäº†é¢å¤–çš„è¡Œä¸ºçº¿ç´¢ï¼Œå¦‚éŸ³è°ƒã€çœ¼ç¥å’Œé¢éƒ¨è¡¨æƒ…ç­‰è‡ªç„¶è¯­è¨€æè¿°ã€‚å°†è¿™äº›æ¨æ–­å‡ºçš„è½®çº§å¥–åŠ±è’¸é¦åˆ°ä¸€ä¸ªè½»é‡çº§çš„å¥–åŠ±æ¨¡å‹ä¸­ï¼Œç”¨äºåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¯¹è¯ç”Ÿæˆå¾®è°ƒã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºæœ€æ–°çš„å¥–åŠ±åˆ†è§£æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨äººç±»å¯¹è¯è´¨é‡è¯„ä»·ä¸Šæœ‰æ˜¾è‘—æ”¹è¿›ï¼Œè¡¨æ˜LLMæ˜¯å¼ºå¤§çš„å¥–åŠ±åˆ†è§£å™¨ï¼Œæ— éœ€æ‰‹åŠ¨å¥–åŠ±å¡‘å½¢å’Œè¯¦ç»†çš„äººç±»åé¦ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¥–åŠ±åˆ†è§£ï¼Œå®ç°å¯¹å¯¹è¯ä»£ç†çš„å¯¹é½ã€‚</li>
<li>é€šè¿‡åˆ†è§£å…¨å±€ä¼šè¯çº§åé¦ˆæ¥æ¨æ–­å±€éƒ¨éšå¼å¥–åŠ±ã€‚</li>
<li>æå‡ºæ–‡æœ¬æ¨¡æ€å’Œå¤šåª’ä½“æ¨¡æ€ä¸¤ç§å¥–åŠ±åˆ†è§£æ–¹æ³•ã€‚</li>
<li>æ–‡æœ¬æ¨¡æ€ä»…ä½¿ç”¨å¯¹è¯æ–‡æœ¬ï¼Œè€Œå¤šåª’ä½“æ¨¡æ€åˆ™ç»“åˆè¡Œä¸ºçº¿ç´¢ã€‚</li>
<li>å°†æ¨æ–­çš„è½®çº§å¥–åŠ±è’¸é¦åˆ°è½»é‡çº§å¥–åŠ±æ¨¡å‹ä¸­ã€‚</li>
<li>ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¯¹å¯¹è¯ç”Ÿæˆè¿›è¡Œå¾®è°ƒã€‚</li>
<li>åœ¨äººç±»å¯¹è¯è´¨é‡è¯„ä»·ä¸Šï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºå…¶ä»–æœ€æ–°å¥–åŠ±åˆ†è§£æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15922">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-360de7432bf926a2622f507ab9bd8656.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dabdbf681bf2dad8f5b62553219060e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3300bd1ac02e29d091bf0f008ddf882b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52c4215b4e37a4d08ef585882fb82445.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Learning-to-Reason-via-Mixture-of-Thought-for-Logical-Reasoning"><a href="#Learning-to-Reason-via-Mixture-of-Thought-for-Logical-Reasoning" class="headerlink" title="Learning to Reason via Mixture-of-Thought for Logical Reasoning"></a>Learning to Reason via Mixture-of-Thought for Logical Reasoning</h2><p><strong>Authors:Tong Zheng, Lichang Chen, Simeng Han, R. Thomas McCoy, Heng Huang</strong></p>
<p>Human beings naturally utilize multiple reasoning modalities to learn and solve logical problems, i.e., different representational formats such as natural language, code, and symbolic logic. In contrast, most existing LLM-based approaches operate with a single reasoning modality during training, typically natural language. Although some methods explored modality selection or augmentation at inference time, the training process remains modality-blind, limiting synergy among modalities. To fill in this gap, we propose Mixture-of-Thought (MoT), a framework that enables LLMs to reason across three complementary modalities: natural language, code, and a newly introduced symbolic modality, truth-table, which systematically enumerates logical cases and partially mitigates key failure modes in natural language reasoning. MoT adopts a two-phase design: (1) self-evolving MoT training, which jointly learns from filtered, self-generated rationales across modalities; and (2) MoT inference, which fully leverages the synergy of three modalities to produce better predictions. Experiments on logical reasoning benchmarks including FOLIO and ProofWriter demonstrate that our MoT framework consistently and significantly outperforms strong LLM baselines with single-modality chain-of-thought approaches, achieving up to +11.7pp average accuracy gain. Further analyses show that our MoT framework benefits both training and inference stages; that it is particularly effective on harder logical reasoning problems; and that different modalities contribute complementary strengths, with truth-table reasoning helping to overcome key bottlenecks in natural language inference. </p>
<blockquote>
<p>äººç±»è‡ªç„¶è¿ç”¨å¤šç§æ¨ç†æ¨¡å¼æ¥å­¦ä¹ å’Œè§£å†³é€»è¾‘é—®é¢˜ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€ã€ä»£ç å’Œç¬¦å·é€»è¾‘ç­‰ä¸åŒçš„è¡¨ç¤ºæ ¼å¼ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤§å¤šæ•°ç°æœ‰çš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åªä½¿ç”¨å•ä¸€çš„æ¨ç†æ¨¡å¼ï¼Œé€šå¸¸æ˜¯è‡ªç„¶è¯­è¨€ã€‚å°½ç®¡ä¸€äº›æ–¹æ³•åœ¨æ¨ç†æ—¶é—´æ¢ç´¢äº†æ¨¡å¼é€‰æ‹©æˆ–å¢å¼ºï¼Œä½†è®­ç»ƒè¿‡ç¨‹ä»ç„¶å¯¹æ¨¡å¼æ˜¯ç›²ç›®çš„ï¼Œé™åˆ¶äº†æ¨¡å¼ä¹‹é—´çš„ååŒä½œç”¨ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†â€œæ€ç»´æ··åˆâ€ï¼ˆMoTï¼‰æ¡†æ¶ï¼Œå®ƒä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸‰ç§äº’è¡¥æ¨¡å¼ä¸­è¿›è¡Œæ¨ç†ï¼šè‡ªç„¶è¯­è¨€ã€ä»£ç ï¼Œä»¥åŠæ–°å¼•å…¥çš„ç¬¦å·æ¨¡å¼â€”â€”çœŸå€¼è¡¨ã€‚çœŸå€¼è¡¨ç³»ç»Ÿåœ°æšä¸¾é€»è¾‘æƒ…å†µï¼Œéƒ¨åˆ†ç¼“è§£äº†è‡ªç„¶è¯­è¨€æ¨ç†ä¸­çš„å…³é”®å¤±è´¥æ¨¡å¼ã€‚MoTé‡‡ç”¨ä¸¤é˜¶æ®µè®¾è®¡ï¼šï¼ˆ1ï¼‰è‡ªæˆ‘è¿›åŒ–çš„MoTè®­ç»ƒï¼Œä»è·¨æ¨¡å¼çš„è¿‡æ»¤ã€è‡ªæˆ‘ç”Ÿæˆçš„ç†æ€§ä¸­è”åˆå­¦ä¹ ï¼›ï¼ˆ2ï¼‰MoTæ¨ç†ï¼Œå……åˆ†åˆ©ç”¨ä¸‰ç§æ¨¡å¼çš„ååŒä½œç”¨æ¥äº§ç”Ÿæ›´å¥½çš„é¢„æµ‹ã€‚åœ¨é€»è¾‘æ¨ç†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬FOLIOå’ŒProofWriterä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MoTæ¡†æ¶åœ¨å•æ¨¡æ€æ€ç»´é“¾æ–¹æ³•ä¸­å§‹ç»ˆä¸”æ˜¾è‘—åœ°ä¼˜äºå¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ï¼Œå¹³å‡å‡†ç¡®ç‡æé«˜äº†+11.7ppã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MoTæ¡†æ¶å¯¹è®­ç»ƒå’Œæ¨ç†é˜¶æ®µéƒ½æœ‰ç›Šï¼›å®ƒç‰¹åˆ«é€‚ç”¨äºæ›´å¤æ‚çš„é€»è¾‘æ¨ç†é—®é¢˜ï¼›ä¸åŒçš„æ¨¡å¼è´¡çŒ®å‡ºäº’è¡¥çš„ä¼˜åŠ¿ï¼ŒçœŸå€¼è¡¨æ¨ç†æœ‰åŠ©äºå…‹æœè‡ªç„¶è¯­è¨€æ¨ç†ä¸­çš„å…³é”®ç“¶é¢ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15817v1">PDF</a> 38 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMixture-of-Thoughtï¼ˆMoTï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿LLMsèƒ½å¤Ÿåœ¨è‡ªç„¶è¯­è¨€ã€ä»£ç å’Œç¬¦å·ä¸‰ç§äº’è¡¥æ¨¡æ€ä¸­è¿›è¡Œæ¨ç†ã€‚MoTé‡‡ç”¨è‡ªæˆ‘è¿›åŒ–çš„è®­ç»ƒæ–¹æ³•å’ŒMoTæ¨ç†è¿‡ç¨‹ï¼Œå®ç°äº†è·¨æ¨¡æ€çš„ååŒå·¥ä½œã€‚åœ¨é€»è¾‘åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMoTæ¡†æ¶ç›¸è¾ƒäºå•ä¸€æ¨¡æ€çš„æ¨ç†æ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼Œå°¤å…¶æ˜¯åœ¨è§£å†³å¤æ‚é€»è¾‘é—®é¢˜æ—¶æ•ˆæœæ›´æ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»åˆ©ç”¨å¤šç§æ¨ç†æ¨¡æ€å­¦ä¹ å’Œè§£å†³é€»è¾‘é—®é¢˜ï¼Œè€Œå¤§å¤šæ•°LLMæ–¹æ³•ä»…ä½¿ç”¨å•ä¸€æ¨¡æ€è¿›è¡Œè®­ç»ƒã€‚</li>
<li>MoTæ¡†æ¶å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œå…è®¸LLMsåœ¨è‡ªç„¶è¯­è¨€ã€ä»£ç å’Œç¬¦å·ä¸‰ç§æ¨¡æ€ä¸­è¿›è¡Œæ¨ç†ã€‚</li>
<li>MoTé‡‡ç”¨è‡ªæˆ‘è¿›åŒ–çš„è®­ç»ƒæ–¹æ³•å’ŒMoTæ¨ç†è¿‡ç¨‹ï¼Œæé«˜äº†è·¨æ¨¡æ€ååŒå·¥ä½œçš„èƒ½åŠ›ã€‚</li>
<li>åœ¨é€»è¾‘åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMoTæ¡†æ¶ç›¸è¾ƒäºå•ä¸€æ¨¡æ€çš„æ¨ç†æ–¹æ³•è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹³å‡å‡†ç¡®åº¦æé«˜äº†11.7%ã€‚</li>
<li>MoTæ¡†æ¶å¯¹è®­ç»ƒå’Œæ¨ç†é˜¶æ®µéƒ½æœ‰ç›Šã€‚</li>
<li>ç¬¦å·æ¨¡æ€åœ¨è§£å†³è‡ªç„¶è¯­è¨€æ¨ç†ä¸­çš„å…³é”®ç“¶é¢ˆæ–¹é¢å‘æŒ¥äº†ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-277871519126042cace101b930f05f09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a6e141f63c2d0334780af7ab1266413.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8f2d0056453fa4dc29d4d0d0a672206.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07836e01d775af572593ab167bcfdab7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-896ffb7787816f3a1f3f92e9dd468aa5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GUI-G1-Understanding-R1-Zero-Like-Training-for-Visual-Grounding-in-GUI-Agents"><a href="#GUI-G1-Understanding-R1-Zero-Like-Training-for-Visual-Grounding-in-GUI-Agents" class="headerlink" title="GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI   Agents"></a>GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI   Agents</h2><p><strong>Authors:Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinglin Jia, Jun Xu</strong></p>
<p>Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm, coupling online Reinforcement Learning (RL) with explicit chain-of-thought reasoning prior to object grounding and thereby achieving substantial performance gains. In this paper, we first conduct extensive analysis experiments of three key components of that training pipeline: input design, output evaluation, and policy update-each revealing distinct challenges arising from blindly applying general-purpose RL without adapting to GUI grounding tasks. Input design: Current templates encourage the model to generate chain-of-thought reasoning, but longer chains unexpectedly lead to worse grounding performance. Output evaluation: Reward functions based on hit signals or box area allow models to exploit box size, leading to reward hacking and poor localization quality. Policy update: Online RL tends to overfit easy examples due to biases in length and sample difficulty, leading to under-optimization on harder cases. To address these issues, we propose three targeted solutions. First, we adopt a Fast Thinking Template that encourages direct answer generation, reducing excessive reasoning during training. Second, we incorporate a box size constraint into the reward function to mitigate reward hacking. Third, we revise the RL objective by adjusting length normalization and adding a difficulty-aware scaling factor, enabling better optimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with Qwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on ScreenSpot-Pro. This surpasses all prior models of similar size and even outperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI agent grounding. The project repository is available at <a target="_blank" rel="noopener" href="https://github.com/Yuqi-Zhou/GUI-G1">https://github.com/Yuqi-Zhou/GUI-G1</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†å¤åˆ¶äº†R1-ZeroèŒƒå¼ï¼Œå°†åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸å¯¹è±¡å®šä½ä¹‹å‰çš„æ˜¾å¼é“¾å¼æ¨ç†ç›¸ç»“åˆï¼Œä»è€Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹è®­ç»ƒç®¡é“çš„ä¸‰ä¸ªå…³é”®ç»„ä»¶è¿›è¡Œå¹¿æ³›çš„åˆ†æå®éªŒï¼šè¾“å…¥è®¾è®¡ã€è¾“å‡ºè¯„ä¼°å’Œç­–ç•¥æ›´æ–°â€”â€”æ¯ä¸€ä¸ªéƒ½æ­ç¤ºäº†ç”±äºç›²ç›®åº”ç”¨é€šç”¨RLè€Œæ²¡æœ‰é€‚åº”GUIå®šä½ä»»åŠ¡è€Œäº§ç”Ÿçš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚è¾“å…¥è®¾è®¡ï¼šå½“å‰æ¨¡æ¿é¼“åŠ±æ¨¡å‹ç”Ÿæˆé“¾å¼æ¨ç†ï¼Œä½†æ›´é•¿çš„é“¾æ¡å‡ºä¹æ„æ–™åœ°å¯¼è‡´å®šä½æ€§èƒ½ä¸‹é™ã€‚è¾“å‡ºè¯„ä¼°ï¼šåŸºäºå‘½ä¸­ä¿¡å·æˆ–æ¡†åŒºåŸŸçš„å¥–åŠ±å‡½æ•°å…è®¸æ¨¡å‹åˆ©ç”¨æ¡†å¤§å°ï¼Œå¯¼è‡´å¥–åŠ±ä½œå¼Šå’Œå®šä½è´¨é‡å·®ã€‚ç­–ç•¥æ›´æ–°ï¼šåœ¨çº¿RLç”±äºé•¿åº¦å’Œæ ·æœ¬éš¾åº¦çš„åè§ï¼Œå¾€å¾€ä¼šå¯¹ç®€å•ç¤ºä¾‹è¿›è¡Œè¿‡åº¦æ‹Ÿåˆï¼Œå¯¼è‡´åœ¨æ›´å¤æ‚æƒ…å†µä¸‹çš„ä¼˜åŒ–ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ç§æœ‰é’ˆå¯¹æ€§çš„è§£å†³æ–¹æ¡ˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é‡‡ç”¨å¿«é€Ÿæ€è€ƒæ¨¡æ¿ï¼Œé¼“åŠ±ç›´æ¥ç­”æ¡ˆç”Ÿæˆï¼Œå‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¿‡åº¦æ¨ç†ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†æ¡†å¤§å°çº¦æŸçº³å…¥å¥–åŠ±å‡½æ•°ï¼Œä»¥å‡è½»å¥–åŠ±ä½œå¼Šã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬é€šè¿‡è°ƒæ•´é•¿åº¦å½’ä¸€åŒ–å¹¶æ·»åŠ ä¸€ä¸ªéš¾åº¦æ„ŸçŸ¥ç¼©æ”¾å› å­æ¥ä¿®è®¢RLç›®æ ‡ï¼Œä»è€Œåœ¨å›°éš¾æ ·æœ¬ä¸Šå®ç°æ›´å¥½çš„ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„GUI-G1-3Båœ¨17Kå…¬å…±æ ·æœ¬ä¸Šä½¿ç”¨Qwen2.5-VL-3B-Instructè¿›è¡Œè®­ç»ƒï¼Œåœ¨ScreenSpotä¸Šè¾¾åˆ°äº†90.3%çš„å‡†ç¡®ç‡ï¼Œåœ¨ScreenSpot-Proä¸Šè¾¾åˆ°äº†37.1%çš„å‡†ç¡®ç‡ã€‚è¿™è¶…è¶Šäº†ç±»ä¼¼å¤§å°çš„å…ˆå‰æ‰€æœ‰æ¨¡å‹ï¼Œç”šè‡³è¶…è¶Šäº†æ›´å¤§çš„UI-TARS-7Bï¼Œåœ¨GUIä»£ç†å®šä½æ–¹é¢å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„æ°´å¹³ã€‚é¡¹ç›®ä»“åº“å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Yuqi-Zhou/GUI-G1">https://github.com/Yuqi-Zhou/GUI-G1</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15810v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰çš„ä»£ç†æ¨¡å‹åœ¨å¯¹è±¡å®šä½æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†é’ˆå¯¹è®­ç»ƒç®¡é“ä¸­ä¸‰ä¸ªå…³é”®ç»„ä»¶çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡æ”¹è¿›è¾“å…¥è®¾è®¡ã€è¾“å‡ºè¯„ä¼°å’Œç­–ç•¥æ›´æ–°ï¼ŒæˆåŠŸæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚æ–°çš„è§£å†³æ–¹æ¡ˆåŒ…æ‹¬é‡‡ç”¨å¿«é€Ÿæ€è€ƒæ¨¡æ¿å‡å°‘è¿‡åº¦æ¨ç†ã€å°†ç›’å¤§å°çº¦æŸçº³å…¥å¥–åŠ±å‡½æ•°ä»¥åŠè°ƒæ•´RLç›®æ ‡ä»¥é€‚åº”å›°éš¾æ ·æœ¬çš„ä¼˜åŒ–ã€‚è¯¥ç ”ç©¶å®ç°äº†GUI-G1-3Bæ¨¡å‹çš„æ–°æ€§èƒ½ï¼Œå¹¶åœ¨ScreenSpotå’ŒScreenSpot-Proä¸Šå®ç°äº†è¶…è¿‡å…ˆå‰æ‰€æœ‰ç±»ä¼¼å¤§å°æ¨¡å‹çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GUIä»£ç†æ¨¡å‹é€šè¿‡ç»“åˆåœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸æ˜ç¡®çš„é“¾å¼æ€ç»´æ¨ç†ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>è¾“å…¥è®¾è®¡ï¼šè¿‡é•¿çš„æ¨ç†é“¾å¯èƒ½å¯¼è‡´å®šä½æ€§èƒ½ä¸‹é™ï¼Œéœ€è¦å¹³è¡¡æ¨ç†é•¿åº¦ä¸æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è¾“å‡ºè¯„ä¼°ï¼šåŸºäºå‘½ä¸­ä¿¡å·æˆ–æ¡†åŒºåŸŸçš„å¥–åŠ±å‡½æ•°å¯èƒ½å¯¼è‡´æ¨¡å‹åˆ©ç”¨æ¡†å¤§å°è¿›è¡Œâ€œå¥–åŠ±é»‘å®¢â€è¡Œä¸ºï¼Œéœ€è¦æ”¹è¿›å¥–åŠ±å‡½æ•°ä»¥æ”¹å–„å®šä½è´¨é‡ã€‚</li>
<li>ç­–ç•¥æ›´æ–°ï¼šåœ¨çº¿RLæ˜“äºå¯¹ç®€å•æ ·æœ¬è¿‡æ‹Ÿåˆï¼Œéœ€è¦åœ¨ç›®æ ‡å‡½æ•°ä¸­è€ƒè™‘æ ·æœ¬é•¿åº¦å’Œéš¾åº¦çš„è°ƒæ•´ã€‚</li>
<li>æå‡ºé’ˆå¯¹æ€§è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬å¿«é€Ÿæ€è€ƒæ¨¡æ¿ã€ç›’å¤§å°çº¦æŸçš„å¥–åŠ±å‡½æ•°ä»¥åŠä¼˜åŒ–å›°éš¾æ ·æœ¬çš„RLç›®æ ‡è°ƒæ•´ã€‚</li>
<li>GUI-G1-3Bæ¨¡å‹åœ¨ScreenSpotå’ŒScreenSpot-Proä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼Œè¾¾åˆ°æˆ–è¶…è¶Šå…ˆå‰æ‰€æœ‰ç±»ä¼¼å¤§å°æ¨¡å‹çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-93e4aa858e84fbb59a5d1f3ef62ecec9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fd4ff352e6c6e52c3811808acbab2c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4634f1c6008752c48af1f39095efa3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-057aeeb66cc0433f578069a2a100f587.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48bc256426467f534ecbb8e61d18a007.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3aa0d59216827c0371e9e8af5d495cc1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e0b807a67082931ee47779fc1beb7ec4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="HCRMP-A-LLM-Hinted-Contextual-Reinforcement-Learning-Framework-for-Autonomous-Driving"><a href="#HCRMP-A-LLM-Hinted-Contextual-Reinforcement-Learning-Framework-for-Autonomous-Driving" class="headerlink" title="HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for   Autonomous Driving"></a>HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for   Autonomous Driving</h2><p><strong>Authors:Zhiwen Chen, Bo Leng, Zhuoren Li, Hanming Deng, Guizhe Jin, Ran Yu, Huanxi Wen</strong></p>
<p>Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can enhance autonomous driving (AD) performance in complex scenarios. However, current LLM-Dominated RL methods over-rely on LLM outputs, which are prone to hallucinations. Evaluations show that state-of-the-art LLM indicates a non-hallucination rate of only approximately 57.95% when assessed on essential driving-related tasks. Thus, in these methods, hallucinations from the LLM can directly jeopardize the performance of driving policies. This paper argues that maintaining relative independence between the LLM and the RL is vital for solving the hallucinations problem. Consequently, this paper is devoted to propose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic hints for state augmentation and policy optimization to assist RL agent in motion planning, while the RL agent counteracts potential erroneous semantic indications through policy learning to achieve excellent driving performance. Based on this paradigm, we propose the HCRMP (LLM-Hinted Contextual Reinforcement Learning Motion Planner) architecture, which is designed that includes Augmented Semantic Representation Module to extend state space. Contextual Stability Anchor Module enhances the reliability of multi-critic weight hints by utilizing information from the knowledge base. Semantic Cache Module is employed to seamlessly integrate LLM low-frequency guidance with RL high-frequency control. Extensive experiments in CARLA validate HCRMPâ€™s strong overall driving performance. HCRMP achieves a task success rate of up to 80.3% under diverse driving conditions with different traffic densities. Under safety-critical driving conditions, HCRMP significantly reduces the collision rate by 11.4%, which effectively improves the driving performance in complex scenarios. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç›¸ç»“åˆï¼Œå¯ä»¥åœ¨å¤æ‚åœºæ™¯ä¸­æé«˜è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç›®å‰çš„ä»¥LLMä¸ºä¸»çš„RLæ–¹æ³•è¿‡äºä¾èµ–LLMçš„è¾“å‡ºï¼Œè¿™äº›è¾“å‡ºå®¹æ˜“äº§ç”Ÿå¹»è§‰ã€‚è¯„ä¼°è¡¨æ˜ï¼Œåœ¨å…³é”®çš„é©¾é©¶ç›¸å…³ä»»åŠ¡ä¸­ï¼Œæœ€å…ˆè¿›LLMçš„éå¹»è§‰ç‡åªæœ‰å¤§çº¦57.95%ã€‚å› æ­¤ï¼Œåœ¨è¿™äº›æ–¹æ³•ä¸­ï¼ŒLLMçš„å¹»è§‰ä¼šç›´æ¥å±åŠé©¾é©¶ç­–ç•¥çš„æ€§èƒ½ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œä¿æŒLLMå’ŒRLä¹‹é—´çš„ç›¸å¯¹ç‹¬ç«‹æ€§å¯¹äºè§£å†³å¹»è§‰é—®é¢˜è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œæœ¬æ–‡è‡´åŠ›äºæå‡ºä¸€ç§æ–°å‹çš„LLMæç¤ºRLèŒƒå¼ã€‚LLMç”¨äºç”Ÿæˆè¯­ä¹‰æç¤ºï¼Œç”¨äºçŠ¶æ€æ‰©å……å’Œæ”¿ç­–ä¼˜åŒ–ï¼Œä»¥ååŠ©RLä»£ç†è¿›è¡Œè¿åŠ¨è§„åˆ’ï¼Œè€ŒRLä»£ç†åˆ™é€šè¿‡æ”¿ç­–å­¦ä¹ æ¥å¯¹æŠ—æ½œåœ¨çš„é”™è¯¯è¯­ä¹‰æŒ‡ç¤ºï¼Œä»¥å®ç°å“è¶Šçš„é©¾é©¶æ€§èƒ½ã€‚åŸºäºæ­¤èŒƒå¼ï¼Œæˆ‘ä»¬æå‡ºäº†HCRMPï¼ˆLLMæç¤ºçš„ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ è¿åŠ¨è§„åˆ’å™¨ï¼‰æ¶æ„ï¼Œè¯¥æ¶æ„åŒ…æ‹¬æ‰©å±•çŠ¶æ€ç©ºé—´çš„å¢å¼ºè¯­ä¹‰è¡¨ç¤ºæ¨¡å—ã€‚ä¸Šä¸‹æ–‡ç¨³å®šæ€§é”šæ¨¡å—åˆ©ç”¨çŸ¥è¯†åº“ä¸­çš„ä¿¡æ¯æ¥æé«˜å¤šæ‰¹è¯„å®¶æƒé‡æç¤ºçš„å¯é æ€§ã€‚è¯­ä¹‰ç¼“å­˜æ¨¡å—æ— ç¼é›†æˆäº†LLMçš„ä½é¢‘æŒ‡å—å’ŒRLçš„é«˜é¢‘æ§åˆ¶ã€‚åœ¨CARLAè¿›è¡Œçš„å¹¿æ³›å®éªŒéªŒè¯äº†HCRMPå¼ºå¤§çš„æ•´ä½“é©¾é©¶æ€§èƒ½ã€‚åœ¨å„ç§é©¾é©¶æ¡ä»¶ä¸‹ï¼ŒHCRMPçš„ä»»åŠ¡æˆåŠŸç‡é«˜è¾¾80.3%ã€‚åœ¨å®‰å…¨å…³é”®çš„é©¾é©¶æ¡ä»¶ä¸‹ï¼ŒHCRMPå°†ç¢°æ’ç‡é™ä½äº†11.4%ï¼Œè¿™æœ‰æ•ˆåœ°æé«˜äº†å¤æ‚åœºæ™¯ä¸­çš„é©¾é©¶æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15793v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤æ‚åœºæ™¯ä¸­ï¼Œæ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥æé«˜è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰æ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“å‰LLMä¸»å¯¼çš„RLæ–¹æ³•è¿‡äºä¾èµ–LLMè¾“å‡ºï¼Œå®¹æ˜“äº§ç”Ÿå¹»è§‰ã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œæœ€å…ˆè¿›çš„LLMåœ¨å…³é”®é©¾é©¶ä»»åŠ¡ä¸Šçš„éå¹»è§‰ç‡ä»…ä¸ºçº¦57.95%ã€‚å› æ­¤ï¼ŒLLMçš„å¹»è§‰ä¼šç›´æ¥å±åŠé©¾é©¶ç­–ç•¥çš„æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºä¿æŒLLMå’ŒRLä¹‹é—´çš„ç›¸å¯¹ç‹¬ç«‹æ€§æ˜¯è§£å†³å¹»è§‰é—®é¢˜çš„å…³é”®ã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„LLMæç¤ºRLèŒƒå¼ã€‚LLMç”¨äºç”ŸæˆçŠ¶æ€å¢å¼ºå’Œæ”¿ç­–ä¼˜åŒ–çš„è¯­ä¹‰æç¤ºï¼ŒååŠ©RLä»£ç†è¿›è¡Œè¿åŠ¨è§„åˆ’ï¼Œè€ŒRLä»£ç†åˆ™é€šè¿‡æ”¿ç­–å­¦ä¹ æ¥å¯¹æŠ—æ½œåœ¨çš„é”™è¯¯è¯­ä¹‰æç¤ºï¼Œä»¥å®ç°å“è¶Šçš„é©¾é©¶æ€§èƒ½ã€‚åŸºäºè¿™ä¸€èŒƒå¼ï¼Œæå‡ºäº†HCRMPï¼ˆLLMæç¤ºçš„ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ è¿åŠ¨è§„åˆ’å™¨ï¼‰æ¶æ„ã€‚è¯¥æ¶æ„åŒ…æ‹¬æ‰©å±•çŠ¶æ€ç©ºé—´çš„å¢å¼ºè¯­ä¹‰è¡¨ç¤ºæ¨¡å—ã€å¢å¼ºå¤šæ‰¹åˆ¤æƒé‡æç¤ºçš„å¯é æ€§ä¸Šä¸‹æ–‡ç¨³å®šæ€§é”šæ¨¡å—ï¼Œä»¥åŠæ— ç¼æ•´åˆLLMä½é¢‘æŒ‡å¯¼å’ŒRLé«˜é¢‘æ§åˆ¶çš„è¯­ä¹‰ç¼“å­˜æ¨¡å—ã€‚åœ¨CARLAè¿›è¡Œçš„å¹¿æ³›å®éªŒéªŒè¯äº†HCRMPå¼ºå¤§çš„æ•´ä½“é©¾é©¶æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMä¸RLçš„ç»“åˆå¯ä»¥æé«˜è‡ªåŠ¨é©¾é©¶åœ¨å¤æ‚åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚</li>
<li>å½“å‰LLMä¸»å¯¼çš„RLæ–¹æ³•å­˜åœ¨è¿‡åº¦ä¾èµ–LLMè¾“å‡ºçš„é—®é¢˜ï¼Œå¯¼è‡´å¹»è§‰ã€‚</li>
<li>LLMçš„å¹»è§‰ä¼šç›´æ¥å½±å“é©¾é©¶ç­–ç•¥çš„æ€§èƒ½ã€‚</li>
<li>ä¿æŒLLMå’ŒRLä¹‹é—´çš„ç›¸å¯¹ç‹¬ç«‹æ€§æ˜¯è§£å†³å¹»è§‰é—®é¢˜çš„å…³é”®ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„LLMæç¤ºRLèŒƒå¼ï¼Œå…¶ä¸­LLMç”¨äºç”Ÿæˆè¯­ä¹‰æç¤ºï¼ŒååŠ©RLä»£ç†è¿›è¡Œè¿åŠ¨è§„åˆ’ã€‚</li>
<li>åŸºäºè¯¥èŒƒå¼ï¼Œæå‡ºäº†HCRMPæ¶æ„ï¼ŒåŒ…æ‹¬å¤šä¸ªæ¨¡å—ä»¥å¢å¼ºé©¾é©¶æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15793">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8a530509c76ba7efd74773a00a40190c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfb8bb69f0c4ba32ca87f35b13e5f607.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2993b464cbff5532ce569625c799efce.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Thought-Augmented-Policy-Optimization-Bridging-External-Guidance-and-Internal-Capabilities"><a href="#Thought-Augmented-Policy-Optimization-Bridging-External-Guidance-and-Internal-Capabilities" class="headerlink" title="Thought-Augmented Policy Optimization: Bridging External Guidance and   Internal Capabilities"></a>Thought-Augmented Policy Optimization: Bridging External Guidance and   Internal Capabilities</h2><p><strong>Authors:Jinyang Wu, Chonghua Liao, Mingkuan Feng, Shuai Zhang, Zhengqi Wen, Pengpeng Shao, Huazhe Xu, Jianhua Tao</strong></p>
<p>Reinforcement learning (RL) has emerged as an effective method for training reasoning models. However, existing RL approaches typically bias the modelâ€™s output distribution toward reward-maximizing paths without introducing external knowledge. This limits their exploration capacity and results in a narrower reasoning capability boundary compared to base models. To address this limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel framework that augments RL by incorporating external high-level guidance (â€œthought patternsâ€). By adaptively integrating structured thoughts during training, TAPO effectively balances model-internal exploration and external guidance exploitation. Extensive experiments show that our approach significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva Math. Notably, these high-level thought patterns, abstracted from only 500 prior samples, generalize effectively across various tasks and models. This highlights TAPOâ€™s potential for broader applications across multiple tasks and domains. Our further analysis reveals that introducing external guidance produces powerful reasoning models with superior explainability of inference behavior and enhanced output readability. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²ç»æˆä¸ºè®­ç»ƒæ¨ç†æ¨¡å‹çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸åå‘äºå¥–åŠ±æœ€å¤§åŒ–çš„è·¯å¾„ï¼Œè€Œæ²¡æœ‰å¼•å…¥å¤–éƒ¨çŸ¥è¯†ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›ï¼Œä¸åŸºç¡€æ¨¡å‹ç›¸æ¯”ï¼Œå…¶æ¨ç†èƒ½åŠ›è¾¹ç•Œè¾ƒçª„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†TAPOï¼ˆå¢å¼ºå‹ç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡èå…¥å¤–éƒ¨é«˜çº§æŒ‡å¯¼ï¼ˆå³â€œæ€ç»´æ¨¡å¼â€ï¼‰æ¥å¢å¼ºå¼ºåŒ–å­¦ä¹ çš„æ–°å‹æ¡†æ¶ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°æ•´åˆç»“æ„åŒ–æ€ç»´ï¼ŒTAPOæœ‰æ•ˆåœ°å¹³è¡¡äº†æ¨¡å‹å†…éƒ¨çš„æ¢ç´¢å’Œå¤–éƒ¨æŒ‡å¯¼çš„åˆ©ç”¨ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨AIMEä¸Šæ¯”GRPOé«˜å‡º99%ï¼Œåœ¨AMCä¸Šé«˜å‡º41%ï¼Œåœ¨Minerva Mathä¸Šé«˜å‡º17%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›é«˜çº§æ€ç»´æ¨¡å¼ä»…ä»500ä¸ªå…ˆéªŒæ ·æœ¬ä¸­æŠ½è±¡å‡ºæ¥ï¼Œèƒ½æœ‰æ•ˆåœ°åº”ç”¨äºå„ç§ä»»åŠ¡å’Œæ¨¡å‹ï¼Œè¿™çªæ˜¾äº†TAPOåœ¨å¤šä¸ªä»»åŠ¡å’Œé¢†åŸŸä¸­çš„æ›´å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚æˆ‘ä»¬çš„è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œå¼•å…¥å¤–éƒ¨æŒ‡å¯¼äº§ç”Ÿäº†å¼ºå¤§çš„æ¨ç†æ¨¡å‹ï¼Œå…·æœ‰å‡ºè‰²çš„æ¨ç†è¡Œä¸ºè§£é‡Šæ€§å’Œå¢å¼ºçš„è¾“å‡ºå¯è¯»æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15692v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºè®­ç»ƒæ¨ç†æ¨¡å‹çš„æœ‰æ•ˆæ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RLæ–¹æ³•é€šå¸¸åå‘å¥–åŠ±æœ€å¤§åŒ–çš„è·¯å¾„ï¼Œä¸å¼•å…¥å¤–éƒ¨çŸ¥è¯†ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›ï¼Œå¯¼è‡´å…¶æ¨ç†èƒ½åŠ›è¾¹ç•Œè¾ƒåŸºç¡€æ¨¡å‹æ›´çª„ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†TAPOï¼ˆæ€ç»´å¢å¼ºç­–ç•¥ä¼˜åŒ–ï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡å¼•å…¥å¤–éƒ¨é«˜çº§æŒ‡å¯¼ï¼ˆâ€œæ€ç»´æ¨¡å¼â€ï¼‰æ¥å¢å¼ºRLã€‚TAPOåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°é›†æˆç»“æ„åŒ–æ€ç»´ï¼Œæœ‰æ•ˆåœ°å¹³è¡¡äº†æ¨¡å‹å†…éƒ¨çš„æ¢ç´¢å’Œå¤–éƒ¨æŒ‡å¯¼çš„åˆ©ç”¨ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨AIMEä¸Šè¾ƒGRPOé«˜å‡º99%ï¼Œåœ¨AMCä¸Šé«˜å‡º41%ï¼Œåœ¨Minerva Mathä¸Šé«˜å‡º17%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›é«˜çº§æ€ç»´æ¨¡å¼ä»…ä»500ä¸ªå…ˆéªŒæ ·æœ¬ä¸­æŠ½è±¡å‡ºæ¥ï¼Œå¯æœ‰æ•ˆåœ°åº”ç”¨äºå„ç§ä»»åŠ¡å’Œæ¨¡å‹ã€‚è¿™å‡¸æ˜¾äº†TAPOåœ¨è·¨å¤šä¸ªä»»åŠ¡å’Œé¢†åŸŸçš„æ›´å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚æˆ‘ä»¬çš„è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œå¼•å…¥å¤–éƒ¨æŒ‡å¯¼äº§ç”Ÿäº†å¼ºå¤§çš„æ¨ç†æ¨¡å‹ï¼Œå…·æœ‰å‡ºè‰²çš„æ¨ç†è¡Œä¸ºè§£é‡Šæ€§å’Œå¢å¼ºçš„è¾“å‡ºå¯è¯»æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ æ˜¯æœ‰æ•ˆçš„è®­ç»ƒæ¨ç†æ¨¡å‹æ–¹æ³•ã€‚</li>
<li>ç°æœ‰RLæ–¹æ³•å­˜åœ¨åå‘å¥–åŠ±æœ€å¤§åŒ–è·¯å¾„çš„é—®é¢˜ï¼Œç¼ºä¹å¤–éƒ¨çŸ¥è¯†å¼•å…¥ã€‚</li>
<li>TAPOæ¡†æ¶é€šè¿‡å¼•å…¥å¤–éƒ¨é«˜çº§æŒ‡å¯¼ï¼ˆæ€ç»´æ¨¡å¼ï¼‰æ¥å¢å¼ºRLã€‚</li>
<li>TAPOè‡ªé€‚åº”åœ°é›†æˆç»“æ„åŒ–æ€ç»´ï¼Œå¹³è¡¡æ¨¡å‹å†…éƒ¨æ¢ç´¢å’Œå¤–éƒ¨æŒ‡å¯¼åˆ©ç”¨ã€‚</li>
<li>TAPOåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œä»…ä»å°‘é‡å…ˆéªŒæ ·æœ¬ä¸­æŠ½è±¡å‡ºçš„æ€ç»´æ¨¡å¼å¯æ³›åŒ–åˆ°ä¸åŒä»»åŠ¡å’Œæ¨¡å‹ã€‚</li>
<li>TAPOæœ‰åŠ©äºäº§ç”Ÿå¼ºå¤§çš„æ¨ç†æ¨¡å‹ï¼Œå…·æœ‰å‡ºè‰²çš„æ¨ç†è¡Œä¸ºè§£é‡Šæ€§å’Œè¾“å‡ºå¯è¯»æ€§ã€‚</li>
<li>TAPOå…·æœ‰è·¨å¤šä¸ªä»»åŠ¡å’Œé¢†åŸŸçš„æ›´å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15692">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1f88f94da02cb55851b12407d1f00546.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f0307c478331a1e7a85edc154c18372.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1d5a93dfde0fe226273e5c9f3caa470.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-74d459c7d89e8f601eb7cbc0b7c8152f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Learn-to-Reason-Efficiently-with-Adaptive-Length-based-Reward-Shaping"><a href="#Learn-to-Reason-Efficiently-with-Adaptive-Length-based-Reward-Shaping" class="headerlink" title="Learn to Reason Efficiently with Adaptive Length-based Reward Shaping"></a>Learn to Reason Efficiently with Adaptive Length-based Reward Shaping</h2><p><strong>Authors:Wei Liu, Ruochen Zhou, Yiyun Deng, Yuzhen Huang, Junteng Liu, Yuntian Deng, Yizhe Zhang, Junxian He</strong></p>
<p>Large Reasoning Models (LRMs) have shown remarkable capabilities in solving complex problems through reinforcement learning (RL), particularly by generating long reasoning traces. However, these extended outputs often exhibit substantial redundancy, which limits the efficiency of LRMs. In this paper, we investigate RL-based approaches to promote reasoning efficiency. Specifically, we first present a unified framework that formulates various efficient reasoning methods through the lens of length-based reward shaping. Building on this perspective, we propose a novel Length-bAsed StEp Reward shaping method (LASER), which employs a step function as the reward, controlled by a target length. LASER surpasses previous methods, achieving a superior Pareto-optimal balance between performance and efficiency. Next, we further extend LASER based on two key intuitions: (1) The reasoning behavior of the model evolves during training, necessitating reward specifications that are also adaptive and dynamic; (2) Rather than uniformly encouraging shorter or longer chains of thought (CoT), we posit that length-based reward shaping should be difficulty-aware i.e., it should penalize lengthy CoTs more for easy queries. This approach is expected to facilitate a combination of fast and slow thinking, leading to a better overall tradeoff. The resulting method is termed LASER-D (Dynamic and Difficulty-aware). Experiments on DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and DeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both reasoning performance and response length efficiency. For instance, LASER-D and its variant achieve a +6.1 improvement on AIME2024 while reducing token usage by 63%. Further analysis reveals our RL-based compression produces more concise reasoning patterns with less redundant â€œself-reflectionsâ€. Resources are at <a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/Laser">https://github.com/hkust-nlp/Laser</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¡¨ç°å‡ºäº†è§£å†³å¤æ‚é—®é¢˜çš„æ˜¾è‘—èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯é€šè¿‡ç”Ÿæˆé•¿çš„æ¨ç†è½¨è¿¹ã€‚ç„¶è€Œï¼Œè¿™äº›æ‰©å±•è¾“å‡ºå¸¸å¸¸è¡¨ç°å‡ºå¤§é‡çš„å†—ä½™ï¼Œè¿™é™åˆ¶äº†LRMsçš„æ•ˆç‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†åŸºäºRLçš„æ–¹æ³•æ¥æå‡æ¨ç†æ•ˆç‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ä¸ªé€šè¿‡é•¿åº¦åŸºç¡€å¥–åŠ±å¡‘é€ æ¥åˆ¶å®šå„ç§é«˜æ•ˆæ¨ç†æ–¹æ³•çš„ç»Ÿä¸€æ¡†æ¶ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºé•¿åº¦çš„æ­¥è¿›å¥–åŠ±å¡‘é€ æ–¹æ³•ï¼ˆLASERï¼‰ï¼Œå®ƒé‡‡ç”¨æ­¥è¿›å‡½æ•°ä½œä¸ºå¥–åŠ±ï¼Œç”±ç›®æ ‡é•¿åº¦æ§åˆ¶ã€‚LASERè¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ï¼Œåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†å¸•ç´¯æ‰˜æœ€ä¼˜å¹³è¡¡ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥åŸºäºä¸¤ä¸ªå…³é”®ç›´è§‰æ‰©å±•äº†LASERï¼šï¼ˆ1ï¼‰æ¨¡å‹çš„æ¨ç†è¡Œä¸ºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šå‘ç”Ÿå˜åŒ–ï¼Œéœ€è¦å¥–åŠ±è§„æ ¼ä¹Ÿæ˜¯è‡ªé€‚åº”å’ŒåŠ¨æ€çš„ï¼›ï¼ˆ2ï¼‰è€Œä¸æ˜¯ç»Ÿä¸€åœ°é¼“åŠ±æ›´çŸ­æˆ–æ›´é•¿çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œæˆ‘ä»¬è®¤ä¸ºåŸºäºé•¿åº¦çš„å¥–åŠ±å¡‘é€ åº”è¯¥æ˜¯éš¾åº¦æ„ŸçŸ¥çš„ï¼Œå³å¯¹äºç®€å•çš„æŸ¥è¯¢ï¼Œå®ƒåº”è¯¥æ›´ä¸¥å‰åœ°æƒ©ç½šå†—é•¿çš„CoTã€‚è¿™ç§æ–¹æ³•é¢„è®¡ä¼šä¿ƒè¿›å¿«é€Ÿå’Œæ…¢é€Ÿæ€ç»´çš„ç»“åˆï¼Œä»è€Œå®ç°æ›´å¥½çš„æ•´ä½“æƒè¡¡ã€‚å¾—åˆ°çš„æ–¹æ³•è¢«ç§°ä¸ºLASER-Dï¼ˆåŠ¨æ€å’Œéš¾åº¦æ„ŸçŸ¥ï¼‰ã€‚åœ¨DeepSeek-R1-Distill-Qwen-1.5Bã€DeepSeek-R1-Distill-Qwen-7Bå’ŒDeepSeek-R1-Distill-Qwen-32Bä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨ç†æ€§èƒ½å’Œå“åº”é•¿åº¦æ•ˆç‡ã€‚ä¾‹å¦‚ï¼ŒLASER-DåŠå…¶å˜ä½“åœ¨AIME2024ä¸Šå®ç°äº†+6.1çš„æ”¹è¿›ï¼ŒåŒæ—¶å‡å°‘äº†63%çš„ä»¤ç‰Œä½¿ç”¨ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„åŸºäºRLçš„å‹ç¼©äº§ç”Ÿäº†æ›´ç®€æ´çš„æ¨ç†æ¨¡å¼ï¼Œå‡å°‘äº†å†—ä½™çš„â€œè‡ªæˆ‘åæ€â€ã€‚ç›¸å…³èµ„æºè§ï¼š<a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/Laser%E3%80%82">https://github.com/hkust-nlp/Laserã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15612v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è§£å†³å¤æ‚é—®é¢˜çš„èƒ½åŠ›ä»¤äººç©ç›®ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿæˆé•¿æ¨ç†è½¨è¿¹æ–¹é¢ã€‚ç„¶è€Œï¼Œè¿™äº›æ‰©å±•è¾“å‡ºå¸¸å¸¸å­˜åœ¨å¤§é‡å†—ä½™ï¼Œé™åˆ¶äº†LRMsçš„æ•ˆç‡ã€‚æœ¬æ–‡ç ”ç©¶RLæ–¹æ³•æ¥æå‡æ¨ç†æ•ˆç‡ã€‚æˆ‘ä»¬æå‡ºä¸€ç§åŸºäºé•¿åº¦çš„æ­¥è¿›å¥–åŠ±å¡‘é€ æ–¹æ³•ï¼ˆLASERï¼‰ï¼Œåˆ©ç”¨æ­¥å‡½æ•°ä½œä¸ºå¥–åŠ±è¿›è¡Œæ§åˆ¶ï¼Œåœ¨æ€§èƒ½ä¸æ•ˆç‡ä¹‹é—´è¾¾åˆ°å¸•ç´¯æ‰˜æœ€ä¼˜å¹³è¡¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›ä¸€æ­¥æ‰©å±•äº†LASERï¼Œè€ƒè™‘åˆ°æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¨ç†è¡Œä¸ºå˜åŒ–ï¼Œæå‡ºäº†åŠ¨æ€å’Œéš¾åº¦æ„ŸçŸ¥çš„é•¿åº¦åŸºç¡€å¥–åŠ±å¡‘é€ æ–¹æ³•ï¼ˆLASER-Dï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨ç†æ€§èƒ½å’Œå“åº”é•¿åº¦æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨è§£å†³å¤æ‚é—®é¢˜ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å­˜åœ¨å†—ä½™è¾“å‡ºï¼Œå½±å“æ•ˆç‡ã€‚</li>
<li>é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼Œæå‡ºä¸€ç§åŸºäºé•¿åº¦çš„æ­¥è¿›å¥–åŠ±å¡‘é€ ï¼ˆLASERï¼‰ï¼Œä»¥æé«˜æ¨ç†æ•ˆç‡ã€‚</li>
<li>LASERæ–¹æ³•åœ¨æ€§èƒ½ä¸æ•ˆç‡ä¹‹é—´è¾¾åˆ°å¸•ç´¯æ‰˜æœ€ä¼˜å¹³è¡¡ã€‚</li>
<li>æ¨¡å‹çš„æ¨ç†è¡Œä¸ºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šå˜åŒ–ï¼Œéœ€è¦åŠ¨æ€å’Œéš¾åº¦æ„ŸçŸ¥çš„é•¿åº¦åŸºç¡€å¥–åŠ±å¡‘é€ æ–¹æ³•ï¼ˆLASER-Dï¼‰ã€‚</li>
<li>LASER-Dæ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨ç†æ€§èƒ½å’Œå“åº”é•¿åº¦æ•ˆç‡ã€‚</li>
<li>å®éªŒç»“æœåŒ…æ‹¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½æå‡å’Œä»¤ç‰Œä½¿ç”¨æ•ˆç‡çš„æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15612">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2bd0b1e42ee39b24b142ba9b6124de9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5b7a2af71aa470956a4b62420ebd06a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d1ab36829464996772568add5f73a1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86ff699f5f0a94729a16446d5d3f9f24.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Towards-Explainable-Temporal-Reasoning-in-Large-Language-Models-A-Structure-Aware-Generative-Framework"><a href="#Towards-Explainable-Temporal-Reasoning-in-Large-Language-Models-A-Structure-Aware-Generative-Framework" class="headerlink" title="Towards Explainable Temporal Reasoning in Large Language Models: A   Structure-Aware Generative Framework"></a>Towards Explainable Temporal Reasoning in Large Language Models: A   Structure-Aware Generative Framework</h2><p><strong>Authors:Zihao Jiang, Ben Liu, Miao Peng, Wenjie Xu, Yao Xiao, Zhenyan Shan, Min Peng</strong></p>
<p>While large language models (LLMs) show great potential in temporal reasoning, most existing work focuses heavily on enhancing performance, often neglecting the explainable reasoning processes underlying the results. To address this gap, we introduce a comprehensive benchmark covering a wide range of temporal granularities, designed to systematically evaluate LLMsâ€™ capabilities in explainable temporal reasoning. Furthermore, our findings reveal that LLMs struggle to deliver convincing explanations when relying solely on textual information. To address challenge, we propose GETER, a novel structure-aware generative framework that integrates Graph structures with text for Explainable TEmporal Reasoning. Specifically, we first leverage temporal knowledge graphs to develop a temporal encoder that captures structural information for the query. Subsequently, we introduce a structure-text prefix adapter to map graph structure features into the text embedding space. Finally, LLMs generate explanation text by seamlessly integrating the soft graph token with instruction-tuning prompt tokens. Experimental results indicate that GETER achieves state-of-the-art performance while also demonstrating its effectiveness as well as strong generalization capabilities. Our dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/carryTatum/GETER">https://github.com/carryTatum/GETER</a>. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¶åºæ¨ç†æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å¤§å¤šæ•°ç°æœ‰å·¥ä½œéƒ½é›†ä¸­åœ¨æé«˜æ€§èƒ½ä¸Šï¼Œå¾€å¾€å¿½è§†äº†ç»“æœèƒŒåå¯è§£é‡Šæ¨ç†è¿‡ç¨‹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†å„ç§æ—¶åºç²’åº¦ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°LLMåœ¨å¯è§£é‡Šæ—¶åºæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒLLMåœ¨ä»…ä¾èµ–æ–‡æœ¬ä¿¡æ¯æ—¶ï¼Œå¾ˆéš¾æä¾›ä»¤äººä¿¡æœçš„è§£é‡Šã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†GETERï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ç»“æ„æ„ŸçŸ¥ç”Ÿæˆæ¡†æ¶ï¼Œå®ƒå°†å›¾ç»“æ„ä¸æ–‡æœ¬é›†æˆåœ¨ä¸€èµ·ï¼Œç”¨äºå¯è§£é‡Šçš„æ—¶åºæ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ©ç”¨æ—¶åºçŸ¥è¯†å›¾è°±å¼€å‘äº†ä¸€ä¸ªæ—¶åºç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨å¯ä»¥æ•æ‰æŸ¥è¯¢çš„ç»“æ„ä¿¡æ¯ã€‚éšåï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç»“æ„æ–‡æœ¬å‰ç¼€é€‚é…å™¨ï¼Œå°†å›¾ç»“æ„ç‰¹å¾æ˜ å°„åˆ°æ–‡æœ¬åµŒå…¥ç©ºé—´ã€‚æœ€åï¼ŒLLMé€šè¿‡æ— ç¼èåˆè½¯å›¾ä»¤ç‰Œä¸æŒ‡ä»¤è°ƒæ•´æç¤ºä»¤ç‰Œæ¥ç”Ÿæˆè§£é‡Šæ–‡æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGETERè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶è¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/carryTatum/GETER%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/carryTatum/GETERæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15245v1">PDF</a> In Findings of the Association for Computational Linguistics: ACL   2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¶åºæ¨ç†é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨æ€§èƒ½æå‡ï¼Œå¿½è§†äº†ç»“æœèƒŒåçš„å¯è§£é‡Šæ¨ç†è¿‡ç¨‹ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€ä¸è¶³ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å„ç§æ—¶åºç²’åº¦ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°LLMåœ¨å¯è§£é‡Šæ—¶åºæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMåœ¨ä»…ä¾èµ–æ–‡æœ¬ä¿¡æ¯æ—¶ï¼Œéš¾ä»¥ç»™å‡ºä»¤äººä¿¡æœçš„è§£é‡Šã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†GETERï¼Œä¸€ä¸ªç»“åˆå›¾ç»“æ„ä¸æ–‡æœ¬çš„ç»“æ„æ„ŸçŸ¥ç”Ÿæˆæ¡†æ¶ã€‚GETERåˆ©ç”¨æ—¶åºçŸ¥è¯†å›¾è°±å¼€å‘æ—¶åºç¼–ç å™¨ï¼Œæ•æ‰æŸ¥è¯¢çš„ç»“æ„ä¿¡æ¯ï¼Œå¹¶é€šè¿‡ç»“æ„æ–‡æœ¬å‰ç¼€é€‚é…å™¨å°†å›¾ç»“æ„ç‰¹å¾æ˜ å°„åˆ°æ–‡æœ¬åµŒå…¥ç©ºé—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGETERåœ¨è¾¾åˆ°æœ€ä½³æ€§èƒ½çš„åŒæ—¶ï¼Œä¹Ÿå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¶åºæ¨ç†æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œä½†ç¼ºä¹å¯è§£é‡Šæ€§ã€‚</li>
<li>ç°æœ‰ç ”ç©¶è¿‡äºå…³æ³¨æ€§èƒ½æå‡ï¼Œå¿½è§†äº†ç»“æœèƒŒåçš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>æå‡ºä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°LLMåœ¨å¯è§£é‡Šæ—¶åºæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>LLMåœ¨ä»…ä¾èµ–æ–‡æœ¬ä¿¡æ¯æ—¶éš¾ä»¥ç»™å‡ºä»¤äººä¿¡æœçš„è§£é‡Šã€‚</li>
<li>å¼•å…¥GETERæ¡†æ¶ï¼Œç»“åˆå›¾ç»“æ„ä¸æ–‡æœ¬ï¼Œæé«˜LLMåœ¨æ—¶åºæ¨ç†æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>GETERåˆ©ç”¨æ—¶åºçŸ¥è¯†å›¾è°±å¼€å‘æ—¶åºç¼–ç å™¨ï¼Œå¹¶æ•æ‰æŸ¥è¯¢çš„ç»“æ„ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15245">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a0a5ac2a83a24bafee10563ac86628f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05f06549f2567246aba0e43a751b75f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c411766391cbf938f458d0c475146de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78bb4170457466c8c5677e316ca90456.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AvatarShield-Visual-Reinforcement-Learning-for-Human-Centric-Video-Forgery-Detection"><a href="#AvatarShield-Visual-Reinforcement-Learning-for-Human-Centric-Video-Forgery-Detection" class="headerlink" title="AvatarShield: Visual Reinforcement Learning for Human-Centric Video   Forgery Detection"></a>AvatarShield: Visual Reinforcement Learning for Human-Centric Video   Forgery Detection</h2><p><strong>Authors:Zhipei Xu, Xuanyu Zhang, Xing Zhou, Jian Zhang</strong></p>
<p>The rapid advancement of Artificial Intelligence Generated Content (AIGC) technologies, particularly in video generation, has led to unprecedented creative capabilities but also increased threats to information integrity, identity security, and public trust. Existing detection methods, while effective in general scenarios, lack robust solutions for human-centric videos, which pose greater risks due to their realism and potential for legal and ethical misuse. Moreover, current detection approaches often suffer from poor generalization, limited scalability, and reliance on labor-intensive supervised fine-tuning. To address these challenges, we propose AvatarShield, the first interpretable MLLM-based framework for detecting human-centric fake videos, enhanced via Group Relative Policy Optimization (GRPO). Through our carefully designed accuracy detection reward and temporal compensation reward, it effectively avoids the use of high-cost text annotation data, enabling precise temporal modeling and forgery detection. Meanwhile, we design a dual-encoder architecture, combining high-level semantic reasoning and low-level artifact amplification to guide MLLMs in effective forgery detection. We further collect FakeHumanVid, a large-scale human-centric video benchmark that includes synthesis methods guided by pose, audio, and text inputs, enabling rigorous evaluation of detection methods in real-world scenes. Extensive experiments show that AvatarShield significantly outperforms existing approaches in both in-domain and cross-domain detection, setting a new standard for human-centric video forensics. </p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œå°¤å…¶æ˜¯åœ¨è§†é¢‘ç”Ÿæˆé¢†åŸŸï¼Œè¿™å¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„åˆ›é€ åŠ›ï¼Œä½†åŒæ—¶ä¹Ÿå¢åŠ äº†ä¿¡æ¯å®Œæ•´æ€§ã€èº«ä»½å®‰å…¨å’Œå…¬ä¼—ä¿¡ä»»çš„å¨èƒã€‚å°½ç®¡ç°æœ‰æ£€æµ‹æ–¹æ³•åœ¨ä¸€èˆ¬åœºæ™¯ä¸­æœ‰æ•ˆï¼Œä½†å®ƒä»¬å¯¹äºä»¥äººç±»ä¸ºä¸­å¿ƒçš„è§†é¢‘ç¼ºä¹ç¨³å¥è§£å†³æ–¹æ¡ˆï¼Œè¿™äº›è§†é¢‘ç”±äºçœŸå®æ„Ÿå’Œæ½œåœ¨çš„æ»¥ç”¨é£é™©è€Œæ„æˆæ›´å¤§å¨èƒã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ£€æµ‹æ–¹æ³•å¾€å¾€å­˜åœ¨æ³›åŒ–èƒ½åŠ›å·®ã€å¯æ‰©å±•æ€§æœ‰é™ã€ä¾èµ–äºåŠ³åŠ¨å¯†é›†å‹çš„ç›‘ç£å¾®è°ƒç­‰é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†AvatarShieldï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºå¯è§£é‡Šå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ£€æµ‹äººç±»ä¸ºä¸­å¿ƒçš„è™šå‡è§†é¢‘çš„æ¡†æ¶ï¼Œé€šè¿‡ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å¾—åˆ°å¢å¼ºã€‚é€šè¿‡æˆ‘ä»¬ç²¾å¿ƒè®¾è®¡çš„ç²¾åº¦æ£€æµ‹å¥–åŠ±å’Œæ—¶é—´è¡¥å¿å¥–åŠ±ï¼Œå®ƒæœ‰æ•ˆåœ°é¿å…äº†ä½¿ç”¨é«˜æˆæœ¬æ–‡æœ¬æ³¨é‡Šæ•°æ®ï¼Œå®ç°äº†ç²¾ç¡®çš„æ—¶é—´å»ºæ¨¡å’Œä¼ªé€ æ£€æµ‹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è®¾è®¡äº†åŒç¼–ç å™¨æ¶æ„ï¼Œç»“åˆé«˜çº§è¯­ä¹‰æ¨ç†å’Œä½çº§ä¼ªå½±æ”¾å¤§ï¼Œä»¥æŒ‡å¯¼MLLMè¿›è¡Œæœ‰æ•ˆçš„ä¼ªé€ æ£€æµ‹ã€‚æˆ‘ä»¬è¿˜æ”¶é›†äº†FakeHumanVidï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„äººç±»ä¸ºä¸­å¿ƒçš„è§†é¢‘åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ç”±å§¿åŠ¿ã€éŸ³é¢‘å’Œæ–‡æœ¬è¾“å…¥å¼•å¯¼çš„åˆæˆæ–¹æ³•ï¼Œèƒ½å¯¹æ£€æµ‹æ–¹æ³•åœ¨çœŸå®åœºæ™¯ä¸­çš„è¡¨ç°è¿›è¡Œä¸¥æ ¼çš„è¯„ä¼°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAvatarShieldåœ¨åŸŸå†…å’Œè·¨åŸŸæ£€æµ‹æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºäººç±»ä¸ºä¸­å¿ƒçš„è§†é¢‘å–è¯è®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15173v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†é¢‘ç”Ÿæˆé¢†åŸŸï¼Œè™½ç„¶å¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„åˆ›é€ åŠ›ï¼Œä½†ä¹Ÿå¢åŠ äº†ä¿¡æ¯å®Œæ•´æ€§ã€èº«ä»½å®‰å…¨å’Œå…¬ä¼—ä¿¡ä»»çš„å¨èƒã€‚ç°æœ‰çš„æ£€æµ‹æ–¹æ³•å¯¹äºä»¥äººç±»ä¸ºä¸­å¿ƒçš„è§†é¢‘ç¼ºä¹ç¨³å¥è§£å†³æ–¹æ¡ˆï¼Œå› æ­¤å­˜åœ¨æ›´å¤§çš„é£é™©ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†AvatarShieldæ¡†æ¶ï¼Œé€šè¿‡ç»„åˆMLLMæŠ€æœ¯å’ŒGroup Relative Policy Optimizationï¼ˆGRPOï¼‰æ–¹æ³•ï¼Œå®ç°ç²¾ç¡®çš„æ—¶é—´å»ºæ¨¡å’Œä¼ªé€ æ£€æµ‹ã€‚è®¾è®¡äº†ä¸€ç§åŒç¼–ç å™¨æ¶æ„ï¼Œç»“åˆé«˜çº§è¯­ä¹‰æ¨ç†å’Œä½çº§ä¼ªå½±æ”¾å¤§ï¼Œä»¥æŒ‡å¯¼MLLMè¿›è¡Œæœ‰æ•ˆæ£€æµ‹ã€‚åŒæ—¶ï¼Œè¿˜æ¨å‡ºäº†å¤§å‹äººç±»ä¸ºä¸­å¿ƒçš„è§†é¢‘åŸºå‡†æµ‹è¯•FakeHumanVidã€‚å®éªŒè¯æ˜ï¼ŒAvatarShieldåœ¨åŸŸå†…å’Œè·¨åŸŸæ£€æµ‹æ–¹é¢éƒ½å¤§å¤§ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºä»¥äººä¸ºä¸­å¿ƒçš„è§†é¢‘å–è¯æ ‘ç«‹äº†æ–°æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIGCæŠ€æœ¯çš„å‘å±•åœ¨è§†é¢‘ç”Ÿæˆé¢†åŸŸå¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„åˆ›é€ åŠ›ï¼Œä½†ä¹Ÿå¸¦æ¥äº†ä¿¡æ¯å®Œæ•´æ€§ã€èº«ä»½å®‰å…¨å’Œå…¬ä¼—ä¿¡ä»»çš„å¨èƒã€‚</li>
<li>ç°æœ‰æ£€æµ‹æ–¹æ³•å¯¹äºä»¥äººç±»ä¸ºä¸­å¿ƒçš„è§†é¢‘ç¼ºä¹ç¨³å¥è§£å†³æ–¹æ¡ˆï¼Œå­˜åœ¨æ›´å¤§çš„é£é™©ã€‚</li>
<li>AvatarShieldæ¡†æ¶é€šè¿‡MLLMæŠ€æœ¯å’ŒGRPOæ–¹æ³•å®ç°äº†ç²¾ç¡®çš„æ—¶é—´å»ºæ¨¡å’Œä¼ªé€ æ£€æµ‹ã€‚</li>
<li>AvatarShieldè®¾è®¡äº†åŒç¼–ç å™¨æ¶æ„ï¼Œç»“åˆé«˜çº§è¯­ä¹‰æ¨ç†å’Œä½çº§ä¼ªå½±æ”¾å¤§ï¼Œæé«˜æ£€æµ‹æ•ˆç‡ã€‚</li>
<li>FakeHumanVidåŸºå‡†æµ‹è¯•ç”¨äºä¸¥æ ¼è¯„ä¼°ç°å®åœºæ™¯ä¸­çš„æ£€æµ‹æ–¹æ³•ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒAvatarShieldåœ¨åŸŸå†…å’Œè·¨åŸŸæ£€æµ‹æ–¹é¢éƒ½å¤§å¤§ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15173">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-174b0e958db5bc9e42c5494b00c6fe45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ada3ae3bf89ee650db0cdee567fd25c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54adfd89dd8b3e05bc37da32eacdc375.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4497aa555c53af6da12d6c77297f087b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="StepSearch-Igniting-LLMs-Search-Ability-via-Step-Wise-Proximal-Policy-Optimization"><a href="#StepSearch-Igniting-LLMs-Search-Ability-via-Step-Wise-Proximal-Policy-Optimization" class="headerlink" title="StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy   Optimization"></a>StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy   Optimization</h2><p><strong>Authors:Ziliang Wang, Xuhui Zheng, Kang An, Cijun Ouyang, Jialu Cai, Yuhang Wang, Yichao Wu</strong></p>
<p>Efficient multi-hop reasoning requires Large Language Models (LLMs) based agents to acquire high-value external knowledge iteratively. Previous work has explored reinforcement learning (RL) to train LLMs to perform search-based document retrieval, achieving notable improvements in QA performance, but underperform on complex, multi-hop QA resulting from the sparse rewards from global signal only. To address this gap in existing research, we introduce StepSearch, a framework for search LLMs that trained with step-wise proximal policy optimization method. It consists of richer and more detailed intermediate search rewards and token-level process supervision based on information gain and redundancy penalties to better guide each search step. We constructed a fine-grained question-answering dataset containing sub-question-level search trajectories based on open source datasets through a set of data pipeline method. On standard multi-hop QA benchmarks, it significantly outperforms global-reward baselines, achieving 11.2% and 4.2% absolute improvements for 3B and 7B models over various search with RL baselines using only 19k training data, demonstrating the effectiveness of fine-grained, stepwise supervision in optimizing deep search LLMs. Our implementation is publicly available at <a target="_blank" rel="noopener" href="https://github.com/zxh20001117/StepSearch">https://github.com/zxh20001117/StepSearch</a>. </p>
<blockquote>
<p>é«˜æ•ˆçš„å¤šè·³æ¨ç†éœ€è¦åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†ç¨‹åºæ¥è¿­ä»£åœ°è·å–é«˜ä»·å€¼å¤–éƒ¨çŸ¥è¯†ã€‚ä¹‹å‰çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥è®­ç»ƒLLMæ‰§è¡ŒåŸºäºæœç´¢çš„æ–‡æ¡£æ£€ç´¢ï¼Œä»è€Œåœ¨é—®ç­”æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œä½†åœ¨å¤æ‚çš„å¤šè·³é—®ç­”æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œè¿™ä»…æºäºå…¨å±€ä¿¡å·çš„ç¨€ç–å¥–åŠ±ã€‚ä¸ºäº†è§£å†³ç°æœ‰ç ”ç©¶ä¸­çš„è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†StepSearchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨é€æ­¥è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–æ–¹æ³•è®­ç»ƒçš„æœç´¢LLMæ¡†æ¶ã€‚å®ƒåŒ…å«æ›´ä¸°å¯Œå’Œæ›´è¯¦ç»†çš„ä¸­é—´æœç´¢å¥–åŠ±ï¼Œä»¥åŠåŸºäºä¿¡æ¯å¢ç›Šå’Œå†—ä½™æƒ©ç½šçš„åŸºäºä»¤ç‰Œçº§åˆ«çš„è¿‡ç¨‹ç›‘ç£ï¼Œä»¥æ›´å¥½åœ°å¼•å¯¼æ¯ä¸ªæœç´¢æ­¥éª¤ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŸºäºå¼€æ”¾æºä»£ç æ•°æ®é›†é€šè¿‡ä¸€ç³»åˆ—æ•°æ®ç®¡é“æ–¹æ³•çš„ç²¾ç»†ç²’åº¦é—®ç­”æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å­é—®é¢˜çº§åˆ«çš„æœç´¢è½¨è¿¹ã€‚åœ¨æ ‡å‡†çš„å¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸ä»…ä½¿ç”¨å…¨å±€å¥–åŠ±åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®­ç»ƒæ•°æ®ä»…æœ‰å°‘é‡ï¼ˆä»…ä½¿ç”¨è®­ç»ƒæ•°æ®çš„å‰19kï¼‰çš„æƒ…å†µä¸‹ï¼Œå¯¹åŸºçº¿æ¨¡å‹è¿›è¡Œäº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œåœ¨æ¨¡å‹è§„æ¨¡ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ä»¥åŠæ›´å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œåˆ†åˆ«å®ç°äº†ç»å¯¹æ”¹å–„å¹…åº¦ä¸ºç™¾åˆ†ä¹‹åä¸€ç‚¹äºŒå’Œç™¾åˆ†ä¹‹å››ç‚¹äºŒã€‚è¿™è¯æ˜äº†åœ¨ä¼˜åŒ–æ·±åº¦æœç´¢LLMæ—¶ï¼Œç²¾ç»†ç²’åº¦çš„é€æ­¥ç›‘ç£çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„å®ç°å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/zxh20001117/StepSearch">https://github.com/zxh20001117/StepSearch</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15107v1">PDF</a> 20 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é«˜æ•ˆçš„å¤šè·³æ¨ç†ä¸­éœ€è¦è¿­ä»£åœ°è·å–é«˜ä»·å€¼å¤–éƒ¨çŸ¥è¯†ã€‚é’ˆå¯¹ç°æœ‰ç ”ç©¶ä¸­å­˜åœ¨çš„ç¨€ç–å¥–åŠ±é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†StepSearchæ¡†æ¶ï¼Œé‡‡ç”¨åŸºäºæ­¥éª¤çš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–æ–¹æ³•è®­ç»ƒLLMè¿›è¡Œæœç´¢ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸°å¯Œçš„ä¸­é—´æœç´¢å¥–åŠ±å’ŒåŸºäºä¿¡æ¯å¢ç›Šå’Œå†—ä½™æƒ©ç½šçš„ä»¤ç‰Œçº§è¿‡ç¨‹ç›‘ç£ï¼Œä»¥æ›´å¥½åœ°æŒ‡å¯¼æ¯æ¬¡æœç´¢æ­¥éª¤ã€‚æˆ‘ä»¬åœ¨åŸºäºå¼€æºæ•°æ®é›†æ„å»ºçš„ç²¾ç»†ç²’åº¦çš„é—®ç­”æ•°æ®é›†ä¸Šæ„å»ºäº†åŒ…å«å­é—®é¢˜çº§åˆ«çš„æœç´¢è½¨è¿¹çš„æ•°æ®ç®¡é“æ–¹æ³•ã€‚åœ¨æ ‡å‡†çš„å¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸å…¨å±€å¥–åŠ±çš„åŸºçº¿ç›¸æ¯”ï¼ŒStepSearchå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œä½¿ç”¨ä»…19kè®­ç»ƒæ•°æ®ï¼Œå¯¹3Bå’Œ7Bæ¨¡å‹çš„ç»å¯¹æ”¹è¿›åˆ†åˆ«ä¸º11.2%å’Œ4.2%ï¼Œè¯æ˜äº†åœ¨ä¼˜åŒ–æ·±åº¦æœç´¢LLMæ–¹é¢ï¼Œç²¾ç»†ç²’åº¦çš„åˆ†æ­¥ç›‘ç£çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é«˜æ•ˆçš„å¤šè·³æ¨ç†ä¸­éœ€è¦è·å–é«˜ä»·å€¼å¤–éƒ¨çŸ¥è¯†ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è®­ç»ƒLLMè¿›è¡ŒåŸºäºæœç´¢çš„æ–‡æ¡£æ£€ç´¢æ–¹é¢å·²æœ‰æ˜¾è‘—æˆæ•ˆï¼Œä½†åœ¨å¤„ç†å¤æ‚çš„å¤šè·³é—®ç­”æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚</li>
<li>StepSearchæ¡†æ¶é€šè¿‡é‡‡ç”¨åŸºäºæ­¥éª¤çš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–æ–¹æ³•æ¥è§£å†³è¿™ä¸€å·®è·ã€‚</li>
<li>StepSearchåŒ…å«ä¸°å¯Œçš„ä¸­é—´æœç´¢å¥–åŠ±å’ŒåŸºäºä¿¡æ¯å¢ç›ŠåŠå†—ä½™æƒ©ç½šçš„ä»¤ç‰Œçº§è¿‡ç¨‹ç›‘ç£ï¼Œä»¥æŒ‡å¯¼æ¯æ¬¡æœç´¢æ­¥éª¤ã€‚</li>
<li>æ„å»ºäº†åŸºäºå¼€æºæ•°æ®é›†çš„ç²¾ç»†ç²’åº¦çš„é—®ç­”æ•°æ®é›†ï¼ŒåŒ…å«å­é—®é¢˜çº§åˆ«çš„æœç´¢è½¨è¿¹ã€‚</li>
<li>åœ¨æ ‡å‡†å¤šè·³é—®ç­”æµ‹è¯•ä¸­ï¼ŒStepSearchæ˜¾è‘—ä¼˜äºå…¨å±€å¥–åŠ±åŸºçº¿ï¼Œè¯æ˜äº†ç²¾ç»†ç²’åº¦çš„åˆ†æ­¥ç›‘ç£åœ¨ä¼˜åŒ–æ·±åº¦æœç´¢LLMæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15107">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b2b4c8e35d503889d6e9d47374e9a751.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-701400565a40060ed35b7eebcc355cdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-340157c97d9bdf5ae0e67cf3bf714449.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ddb7ead0c40c39eb428a74ea4e60b35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8f3937361dadd1c2d23f463515989b6.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DISCO-Balances-the-Scales-Adaptive-Domain-and-Difficulty-Aware-Reinforcement-Learning-on-Imbalanced-Data"><a href="#DISCO-Balances-the-Scales-Adaptive-Domain-and-Difficulty-Aware-Reinforcement-Learning-on-Imbalanced-Data" class="headerlink" title="DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware   Reinforcement Learning on Imbalanced Data"></a>DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware   Reinforcement Learning on Imbalanced Data</h2><p><strong>Authors:Yuhang Zhou, Jing Zhu, Shengyi Qian, Zhuokai Zhao, Xiyao Wang, Xiaoyu Liu, Ming Li, Paiheng Xu, Wei Ai, Furong Huang</strong></p>
<p>Large Language Models (LLMs) are increasingly aligned with human preferences through Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods, Group Relative Policy Optimization (GRPO) has gained attention for its simplicity and strong performance, notably eliminating the need for a learned value function. However, GRPO implicitly assumes a balanced domain distribution and uniform semantic alignment across groups - assumptions that rarely hold in real-world datasets. When applied to multi-domain, imbalanced data, GRPO disproportionately optimizes for dominant domains, neglecting underrepresented ones and resulting in poor generalization and fairness. We propose Domain-Informed Self-Consistency Policy Optimization (DISCO), a principled extension to GRPO that addresses inter-group imbalance with two key innovations. Domain-aware reward scaling counteracts frequency bias by reweighting optimization based on domain prevalence. Difficulty-aware reward scaling leverages prompt-level self-consistency to identify and prioritize uncertain prompts that offer greater learning value. Together, these strategies promote more equitable and effective policy learning across domains. Extensive experiments across multiple LLMs and skewed training distributions show that DISCO improves generalization, outperforms existing GRPO variants by 5% on Qwen3 models, and sets new state-of-the-art results on multi-domain alignment benchmarks. </p>
<blockquote>
<p>éšç€é€šè¿‡äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰çš„ä¸æ–­åº”ç”¨ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šç¬¦åˆäººç±»çš„åå¥½ã€‚åœ¨RLHFæ–¹æ³•ä¸­ï¼Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å› å…¶ç®€å•æ€§å’Œå‡ºè‰²çš„æ€§èƒ½è€Œå—åˆ°å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯å®ƒä¸éœ€è¦å­¦ä¹ ä»·å€¼å‡½æ•°ã€‚ç„¶è€Œï¼ŒGRPOéšå«åœ°å‡è®¾äº†åŸŸåˆ†å¸ƒçš„å¹³è¡¡å’Œè·¨ç¾¤ä½“çš„è¯­ä¹‰å¯¹é½çš„ç»Ÿä¸€æ€§ï¼Œè¿™äº›å‡è®¾åœ¨çœŸå®ä¸–ç•Œçš„æ•°æ®é›†ä¸­å¾ˆå°‘æˆç«‹ã€‚å½“åº”ç”¨äºå¤šåŸŸã€ä¸å¹³è¡¡æ•°æ®æ—¶ï¼ŒGRPOä¼šè¿‡åº¦ä¼˜åŒ–ä¸»å¯¼åŸŸï¼Œå¿½ç•¥ä»£è¡¨æ€§ä¸è¶³çš„åŸŸï¼Œå¯¼è‡´æ³›åŒ–å’Œå…¬å¹³æ€§è¾ƒå·®ã€‚æˆ‘ä»¬æå‡ºäº†é¢†åŸŸä¿¡æ¯è‡ªæˆ‘ä¸€è‡´æ€§ç­–ç•¥ä¼˜åŒ–ï¼ˆDISCOï¼‰ï¼Œè¿™æ˜¯å¯¹GRPOçš„ä¸€ç§åŸåˆ™æ€§æ‰©å±•ï¼Œé€šè¿‡ä¸¤ä¸ªå…³é”®åˆ›æ–°æ¥è§£å†³ç»„é—´ä¸å¹³è¡¡é—®é¢˜ã€‚é¢†åŸŸæ„ŸçŸ¥å¥–åŠ±ç¼©æ”¾é€šè¿‡æ ¹æ®é¢†åŸŸçš„æ™®åŠç¨‹åº¦é‡æ–°è°ƒæ•´ä¼˜åŒ–æ¥æŠµæ¶ˆé¢‘ç‡åè§ã€‚éš¾åº¦æ„ŸçŸ¥å¥–åŠ±ç¼©æ”¾åˆ©ç”¨æç¤ºçº§åˆ«çš„è‡ªæˆ‘ä¸€è‡´æ€§æ¥è¯†åˆ«å’Œä¼˜å…ˆå¤„ç†å…·æœ‰æ›´å¤§å­¦ä¹ ä»·å€¼çš„ä¸ç¡®å®šæç¤ºã€‚è¿™ä¸¤ç§ç­–ç•¥å…±åŒä¿ƒè¿›äº†è·¨é¢†åŸŸçš„æ›´å…¬å¹³å’Œæœ‰æ•ˆçš„ç­–ç•¥å­¦ä¹ ã€‚åœ¨å¤šä¸ªLLMå’Œåæ€è®­ç»ƒåˆ†å¸ƒä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDISCOæé«˜äº†æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨Qwen3æ¨¡å‹ä¸Šæ¯”ç°æœ‰GRPOå˜ä½“é«˜å‡º5%ï¼Œå¹¶åœ¨å¤šåŸŸå¯¹é½åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°çš„æœ€ä½³ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15074v1">PDF</a> 13 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­è¿›è¡Œå¯¹é½äººç±»åå¥½ã€‚GRPOæ–¹æ³•å› å…¶ç®€å•æ€§å’Œå‡ºè‰²æ€§èƒ½è€Œå—åˆ°å…³æ³¨ï¼Œå°¤å…¶æ˜¯ä¸éœ€è¦å­¦ä¹ ä»·å€¼å‡½æ•°ã€‚ç„¶è€Œï¼ŒGRPOéšå«å‡è®¾é¢†åŸŸåˆ†å¸ƒå¹³è¡¡å’Œè·¨ç»„è¯­ä¹‰å¯¹é½ä¸€è‡´ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œä¸­å¾ˆå°‘æˆç«‹ã€‚åœ¨å¤šé¢†åŸŸã€ä¸å‡è¡¡æ•°æ®ä¸­åº”ç”¨GRPOä¼šè¿‡åº¦ä¼˜åŒ–ä¸»å¯¼é¢†åŸŸï¼Œå¿½è§†ä»£è¡¨æ€§ä¸è¶³çš„é¢†åŸŸï¼Œå¯¼è‡´æ³›åŒ–å’Œå…¬å¹³æ€§è¾ƒå·®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†DISCOï¼ˆé¢†åŸŸä¿¡æ¯è‡ªæˆ‘ä¸€è‡´æ€§ç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œè¿™æ˜¯GRPOçš„ä¸€ç§åŸåˆ™æ€§æ‰©å±•ï¼Œè§£å†³äº†ç»„é—´ä¸å¹³è¡¡é—®é¢˜ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šé¢†åŸŸæ„ŸçŸ¥å¥–åŠ±ç¼©æ”¾å’Œéš¾åº¦æ„ŸçŸ¥å¥–åŠ±ç¼©æ”¾ã€‚è¿™ä¸¤ä¸ªç­–ç•¥ä¸€èµ·ä¿ƒè¿›äº†æ›´å…¬å¹³å’Œæœ‰æ•ˆçš„è·¨é¢†åŸŸç­–ç•¥å­¦ä¹ ã€‚åœ¨å¤šä¸ªLLMå’Œåæ–œè®­ç»ƒåˆ†å¸ƒä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDISCOæé«˜äº†æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨Qwen3æ¨¡å‹ä¸Šä¼˜äºç°æœ‰GRPOå˜ä½“5%ï¼Œå¹¶åœ¨å¤šé¢†åŸŸå¯¹é½åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æ–°çš„æœ€ä½³æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­å¯¹é½äººç±»åå¥½ã€‚</li>
<li>Group Relative Policy Optimization (GRPO) æ–¹æ³•å…·æœ‰ç®€å•æ€§å’Œå‡ºè‰²æ€§èƒ½ï¼Œä½†ä¸è€ƒè™‘ç°å®ä¸–ç•Œä¸­é¢†åŸŸåˆ†å¸ƒçš„ä¸å¹³è¡¡æ€§ã€‚</li>
<li>GRPOåœ¨å¤šé¢†åŸŸã€ä¸å‡è¡¡æ•°æ®çš„åº”ç”¨ä¸­å­˜åœ¨é—®é¢˜ï¼Œä¼šè¿‡åº¦ä¼˜åŒ–ä¸»å¯¼é¢†åŸŸè€Œå¿½ç•¥ä»£è¡¨æ€§ä¸è¶³çš„é¢†åŸŸã€‚</li>
<li>DISCOæ˜¯GRPOçš„ä¸€ç§æ‰©å±•ï¼Œé€šè¿‡é¢†åŸŸæ„ŸçŸ¥å¥–åŠ±ç¼©æ”¾å’Œéš¾åº¦æ„ŸçŸ¥å¥–åŠ±ç¼©æ”¾è§£å†³GRPOçš„é—®é¢˜ã€‚</li>
<li>DISCOèƒ½æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šé¢†åŸŸå¯¹é½åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>DISCOåœ¨Qwen3æ¨¡å‹ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰GRPOå˜ä½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d157ee47bbbbb0678bd1a2d4e376b44c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c6f8a54d0c74db32d1b0eac4e3eabe4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c86a3c12388aef30f14620fee3246c26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b26954f350c15264b2df0fdb937aec93.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="General-Reasoner-Advancing-LLM-Reasoning-Across-All-Domains"><a href="#General-Reasoner-Advancing-LLM-Reasoning-Across-All-Domains" class="headerlink" title="General-Reasoner: Advancing LLM Reasoning Across All Domains"></a>General-Reasoner: Advancing LLM Reasoning Across All Domains</h2><p><strong>Authors:Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, Wenhu Chen</strong></p>
<p>Reinforcement learning (RL) has recently demonstrated strong potential in enhancing the reasoning capabilities of large language models (LLMs). Particularly, the â€œZeroâ€ reinforcement learning introduced by Deepseek-R1-Zero, enables direct RL training of base LLMs without relying on an intermediate supervised fine-tuning stage. Despite these advancements, current works for LLM reasoning mainly focus on mathematical and coding domains, largely due to data abundance and the ease of answer verification. This limits the applicability and generalization of such models to broader domains, where questions often have diverse answer representations, and data is more scarce. In this paper, we propose General-Reasoner, a novel training paradigm designed to enhance LLM reasoning capabilities across diverse domains. Our key contributions include: (1) constructing a large-scale, high-quality dataset of questions with verifiable answers curated by web crawling, covering a wide range of disciplines; and (2) developing a generative model-based answer verifier, which replaces traditional rule-based verification with the capability of chain-of-thought and context-awareness. We train a series of models and evaluate them on a wide range of datasets covering wide domains like physics, chemistry, finance, electronics etc. Our comprehensive evaluation across these 12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC) demonstrates that General-Reasoner outperforms existing baseline methods, achieving robust and generalizable reasoning performance while maintaining superior effectiveness in mathematical reasoning tasks. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æœ€è¿‘åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ½œåŠ›ã€‚ç‰¹åˆ«æ˜¯Deepseek-R1-Zeroå¼•å…¥çš„â€Zeroâ€å¼ºåŒ–å­¦ä¹ ï¼Œèƒ½å¤Ÿå®ç°åŸºäºLLMçš„åŸºç¡€RLè®­ç»ƒï¼Œè€Œæ— éœ€ä¾èµ–ä¸­é—´ç›‘ç£å¾®è°ƒé˜¶æ®µã€‚å°½ç®¡æœ‰äº†è¿™äº›è¿›å±•ï¼Œä½†ç›®å‰LLMæ¨ç†çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ•°å­¦å’Œç¼–ç é¢†åŸŸï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºæ•°æ®ä¸°å¯Œä¸”ç­”æ¡ˆæ˜“äºéªŒè¯ã€‚è¿™é™åˆ¶äº†æ­¤ç±»æ¨¡å‹åœ¨æ›´å¹¿æ³›é¢†åŸŸçš„åº”ç”¨å’Œæ³›åŒ–èƒ½åŠ›ï¼Œåœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œé—®é¢˜çš„ç­”æ¡ˆè¡¨ç¤ºå½¢å¼å¾€å¾€å¤šç§å¤šæ ·ï¼Œè€Œä¸”æ•°æ®æ›´åŠ ç¨€ç¼ºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†General-Reasonerï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æé«˜LLMåœ¨å¤šä¸ªé¢†åŸŸæ¨ç†èƒ½åŠ›çš„æ–°å‹è®­ç»ƒèŒƒå¼ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰é€šè¿‡ç½‘é¡µçˆ¬è™«æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„é—®é¢˜æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å¯éªŒè¯çš„ç­”æ¡ˆï¼Œæ¶µç›–å¹¿æ³›çš„å­¦ç§‘ï¼›ä»¥åŠï¼ˆ2ï¼‰å¼€å‘äº†ä¸€ç§åŸºäºç”Ÿæˆæ¨¡å‹çš„ç­”æ¡ˆéªŒè¯å™¨ï¼Œå®ƒç”¨åŸºäºæ€ç»´é“¾å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„èƒ½åŠ›å–ä»£äº†ä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„éªŒè¯ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒäº†ä¸€ç³»åˆ—æ¨¡å‹ï¼Œå¹¶åœ¨æ¶µç›–ç‰©ç†ã€åŒ–å­¦ã€é‡‘èã€ç”µå­ç­‰é¢†åŸŸçš„12ä¸ªåŸºå‡†æµ‹è¯•é›†ï¼ˆå¦‚MMLU-Proã€GPQAã€SuperGPQAã€TheoremQAã€BBEHå’ŒMATH AMCï¼‰ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç»¼åˆè¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒGeneral-Reasoneråœ¨å¤šç§åŸºå‡†æµ‹è¯•é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œå®ç°äº†ç¨³å¥ä¸”å¯æ³›åŒ–çš„æ¨ç†æ€§èƒ½ï¼ŒåŒæ—¶åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ä¿æŒäº†è¾ƒé«˜çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14652v3">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚Deepseek-R1-Zeroæå‡ºçš„â€œé›¶å¼ºåŒ–å­¦ä¹ â€æ–¹æ³•èƒ½å¤Ÿç›´æ¥å¯¹åŸºç¡€è¯­è¨€æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæ— éœ€ä¾èµ–ä¸­é—´ç›‘ç£å¾®è°ƒé˜¶æ®µã€‚å½“å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ•°å­¦å’Œç¼–ç é¢†åŸŸï¼Œè¿™é™åˆ¶äº†æ¨¡å‹åœ¨æ›´å¹¿æ³›é¢†åŸŸçš„é€‚ç”¨æ€§ã€‚æœ¬æ–‡æå‡ºGeneral-Reasonerï¼Œä¸€ç§æ—¨åœ¨æå‡è¯­è¨€æ¨¡å‹åœ¨å¤šæ ·é¢†åŸŸæ¨ç†èƒ½åŠ›çš„æ–°å‹è®­ç»ƒèŒƒå¼ã€‚å…¶ä¸»è¦è´¡çŒ®åŒ…æ‹¬æ„å»ºå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†å’ŒåŸºäºç”Ÿæˆæ¨¡å‹çš„ç­”æ¡ˆéªŒè¯å™¨ã€‚åœ¨æ¶µç›–ç‰©ç†ã€åŒ–å­¦ã€é‡‘èã€ç”µå­ç­‰å¤šä¸ªé¢†åŸŸçš„å¹¿æ³›æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒGeneral-Reasoneråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†ç¨³å¥ä¸”å¯æ¨å¹¿çš„æ¨ç†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Deepseek-R1-Zeroæå‡ºäº†â€œé›¶å¼ºåŒ–å­¦ä¹ â€æ–¹æ³•ï¼Œå¯ç›´æ¥è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œæ— éœ€ä¸­é—´ç›‘ç£å¾®è°ƒã€‚</li>
<li>å½“å‰LLMæ¨ç†ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ•°å­¦å’Œç¼–ç é¢†åŸŸï¼Œå­˜åœ¨é¢†åŸŸé€‚ç”¨æ€§çš„é™åˆ¶ã€‚</li>
<li>General-Reasoneræ˜¯ä¸€ç§æ–°å‹è®­ç»ƒèŒƒå¼ï¼Œæ—¨åœ¨æå‡è¯­è¨€æ¨¡å‹åœ¨å¤šæ ·é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>General-Reasonerçš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬æ„å»ºå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„é—®é¢˜æ•°æ®é›†å’ŒåŸºäºç”Ÿæˆæ¨¡å‹çš„ç­”æ¡ˆéªŒè¯å™¨ã€‚</li>
<li>General-Reasoneråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†ç¨³å¥ä¸”å¯æ¨å¹¿çš„æ¨ç†æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-084fd5839984418f5d556f8634b96ca9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21cc0a7179f7bac17567dd372e69232a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa32ca03b448f35ebee2bd358bab9b61.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49858842a88a5b99574dd1b67a920b32.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Think-Only-When-You-Need-with-Large-Hybrid-Reasoning-Models"><a href="#Think-Only-When-You-Need-with-Large-Hybrid-Reasoning-Models" class="headerlink" title="Think Only When You Need with Large Hybrid-Reasoning Models"></a>Think Only When You Need with Large Hybrid-Reasoning Models</h2><p><strong>Authors:Lingjie Jiang, Xun Wu, Shaohan Huang, Qingxiu Dong, Zewen Chi, Li Dong, Xingxing Zhang, Tengchao Lv, Lei Cui, Furu Wei</strong></p>
<p>Recent Large Reasoning Models (LRMs) have shown substantially improved reasoning capabilities over traditional Large Language Models (LLMs) by incorporating extended thinking processes prior to producing final responses. However, excessively lengthy thinking introduces substantial overhead in terms of token consumption and latency, which is particularly unnecessary for simple queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the first kind of model capable of adaptively determining whether to perform thinking based on the contextual information of user queries. To achieve this, we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as a cold start, followed by online reinforcement learning with the proposed Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the appropriate thinking mode. Furthermore, we introduce a metric called Hybrid Accuracy to quantitatively assess the modelâ€™s capability for hybrid thinking. Extensive experimental results show that LHRMs can adaptively perform hybrid thinking on queries of varying difficulty and type. It outperforms existing LRMs and LLMs in reasoning and general capabilities while significantly improving efficiency. Together, our work advocates for a reconsideration of the appropriate use of extended thinking processes and provides a solid starting point for building hybrid thinking systems. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰é€šè¿‡èå…¥æœ€ç»ˆå“åº”ä¹‹å‰çš„æ‰©å±•æ€è€ƒè¿‡ç¨‹ï¼Œåœ¨æ¨ç†èƒ½åŠ›ä¸Šç›¸è¾ƒäºä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ‰äº†æ˜¾è‘—çš„æå‡ã€‚ç„¶è€Œï¼Œè¿‡åº¦å†—é•¿çš„æ€è€ƒè¿‡ç¨‹ä¼šå¯¼è‡´å¤§é‡çš„ç¬¦å·æ¶ˆè€—å’Œå»¶è¿Ÿï¼Œè¿™åœ¨ç®€å•çš„æŸ¥è¯¢ä¸Šæ˜¾å¾—å°¤ä¸ºä¸å¿…è¦ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤§å‹æ··åˆæ¨ç†æ¨¡å‹ï¼ˆLHRMï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ç§èƒ½å¤Ÿè‡ªé€‚åº”åœ°æ ¹æ®ç”¨æˆ·æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å†³å®šæ˜¯å¦è¿›è¡Œæ€è€ƒçš„æ¨¡å‹ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼Œé¦–å…ˆæ˜¯æ··åˆå¾®è°ƒï¼ˆHFTï¼‰çš„å†·å¯åŠ¨ï¼Œéšåæ˜¯é‡‡ç”¨æ‰€æå‡ºçš„æ··åˆç»„ç­–ç•¥ä¼˜åŒ–ï¼ˆHGPOï¼‰çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œä»¥éšå¼å­¦ä¹ é€‰æ‹©é€‚å½“çš„æ€è€ƒæ¨¡å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªåä¸ºæ··åˆå‡†ç¡®ç‡çš„æŒ‡æ ‡æ¥å®šé‡è¯„ä¼°æ¨¡å‹çš„æ··åˆæ€è€ƒèƒ½åŠ›ã€‚å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLHRMå¯ä»¥åœ¨ä¸åŒç±»å‹çš„æŸ¥è¯¢ä¸Šè‡ªé€‚åº”åœ°è¿›è¡Œæ··åˆæ€è€ƒï¼Œå¹¶åœ¨æ¨ç†å’Œä¸€èˆ¬èƒ½åŠ›ä¸Šä¼˜äºç°æœ‰çš„LRMå’ŒLLMï¼ŒåŒæ—¶æ˜¾è‘—æé«˜æ•ˆç‡ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç ”ç©¶é‡æ–°è€ƒè™‘äº†æ‰©å±•æ€è€ƒè¿‡ç¨‹çš„é€‚å½“ä½¿ç”¨ï¼Œå¹¶ä¸ºæ„å»ºæ··åˆæ€è€ƒç³»ç»Ÿæä¾›äº†ä¸€ä¸ªåšå®çš„èµ·ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14631v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ··åˆæ¨ç†æ¨¡å‹ï¼ˆLHRMsï¼‰çš„å¼•å…¥ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡ä¿¡æ¯è‡ªé€‚åº”åœ°å†³å®šæ˜¯å¦è¿›è¡Œæ¨ç†ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“å®ç°ï¼ŒåŒ…æ‹¬æ··åˆå¾®è°ƒï¼ˆHFTï¼‰çš„å†·å¯åŠ¨ï¼Œä»¥åŠä½¿ç”¨æ··åˆç»„ç­–ç•¥ä¼˜åŒ–ï¼ˆHGPOï¼‰çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œä»¥éšå¼å­¦ä¹ é€‰æ‹©é€‚å½“çš„æ€è€ƒæ¨¡å¼ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªåä¸ºæ··åˆå‡†ç¡®ç‡çš„æŒ‡æ ‡æ¥å®šé‡è¯„ä¼°æ¨¡å‹çš„æ··åˆæ€ç»´èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLHRMsèƒ½å¤Ÿåœ¨ä¸åŒç±»å‹çš„æŸ¥è¯¢ä¸Šè‡ªé€‚åº”åœ°è¿›è¡Œæ··åˆæ€ç»´ï¼Œåœ¨æ¨ç†å’Œé€šç”¨èƒ½åŠ›ä¸Šä¼˜äºç°æœ‰çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œå¹¶ä¸”å¤§å¤§æé«˜äº†æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ··åˆæ¨ç†æ¨¡å‹ï¼ˆLHRMsï¼‰èƒ½å¤Ÿè‡ªé€‚åº”åœ°æ ¹æ®ç”¨æˆ·æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å†³å®šæ˜¯å¦éœ€è¦æ¨ç†ã€‚</li>
<li>LHRMsé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒï¼ŒåŒ…æ‹¬æ··åˆå¾®è°ƒï¼ˆHFTï¼‰å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆHGPOï¼‰æ¥å®ç°è¿™ç§è‡ªé€‚åº”æ¨ç†ã€‚</li>
<li>LHRMséšå¼å­¦ä¹ é€‰æ‹©é€‚å½“çš„æ€è€ƒæ¨¡å¼ï¼Œä»¥é€‚åº”ä¸åŒç±»å‹çš„æŸ¥è¯¢å’Œéš¾åº¦çº§åˆ«ã€‚</li>
<li>æ··åˆå‡†ç¡®ç‡æ˜¯ä¸€ä¸ªæ–°æŒ‡æ ‡ï¼Œç”¨äºå®šé‡è¯„ä¼°æ¨¡å‹çš„æ··åˆæ€ç»´èƒ½åŠ›ã€‚</li>
<li>LHRMsåœ¨æ¨ç†å’Œé€šç”¨èƒ½åŠ›ä¸Šä¼˜äºç°æœ‰çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚</li>
<li>LHRMså¤§å¤§æé«˜äº†æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ç®€å•æŸ¥è¯¢æ—¶é¿å…äº†ä¸å¿…è¦çš„è¿‡é•¿æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14631">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ea772a95693717648d1ada7829193e24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-862a957a47f2ee33a00508036807f555.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-174a003a6a0d4479a481f386b88b6024.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="TinyV-Reducing-False-Negatives-in-Verification-Improves-RL-for-LLM-Reasoning"><a href="#TinyV-Reducing-False-Negatives-in-Verification-Improves-RL-for-LLM-Reasoning" class="headerlink" title="TinyV: Reducing False Negatives in Verification Improves RL for LLM   Reasoning"></a>TinyV: Reducing False Negatives in Verification Improves RL for LLM   Reasoning</h2><p><strong>Authors:Zhangchen Xu, Yuetai Li, Fengqing Jiang, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Radha Poovendran</strong></p>
<p>Reinforcement Learning (RL) has become a powerful tool for enhancing the reasoning abilities of large language models (LLMs) by optimizing their policies with reward signals. Yet, RLâ€™s success relies on the reliability of rewards, which are provided by verifiers. In this paper, we expose and analyze a widespread problemâ€“false negativesâ€“where verifiers wrongly reject correct model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals that over 38% of model-generated responses suffer from false negatives, where the verifier fails to recognize correct answers. We show, both empirically and theoretically, that these false negatives severely impair RL training by depriving the model of informative gradient signals and slowing convergence. To mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments existing rule-based methods, which dynamically identifies potential false negatives and recovers valid responses to produce more accurate reward estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts pass rates by up to 10% and accelerates convergence relative to the baseline. Our findings highlight the critical importance of addressing verifier false negatives and offer a practical approach to improve RL-based fine-tuning of LLMs. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/uw-nsl/TinyV">https://github.com/uw-nsl/TinyV</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºé€šè¿‡å¥–åŠ±ä¿¡å·ä¼˜åŒ–ç­–ç•¥æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„ä¸€ç§å¼ºå¤§å·¥å…·ã€‚ç„¶è€Œï¼ŒRLçš„æˆåŠŸä¾èµ–äºå¥–åŠ±çš„å¯é æ€§ï¼Œè¿™äº›å¥–åŠ±ç”±éªŒè¯å™¨æä¾›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æš´éœ²å¹¶åˆ†æäº†ä¸€ä¸ªæ™®éå­˜åœ¨çš„é—®é¢˜â€”â€”å‡é˜´æ€§ï¼ˆFalse Negativeï¼‰â€”â€”å…¶ä¸­éªŒè¯å™¨é”™è¯¯åœ°æ‹’ç»äº†æ­£ç¡®çš„æ¨¡å‹è¾“å‡ºã€‚æˆ‘ä»¬å¯¹Big-Math-RL-Verifiedæ•°æ®é›†çš„æ·±å…¥ç ”ç©¶æ­ç¤ºï¼Œè¶…è¿‡3procentçš„æ¨¡å‹ç”Ÿæˆå“åº”å­˜åœ¨å‡é˜´æ€§é—®é¢˜ï¼Œå³éªŒè¯å™¨æœªèƒ½è¯†åˆ«å‡ºæ­£ç¡®ç­”æ¡ˆã€‚æˆ‘ä»¬é€šè¿‡å®è¯å’Œç†è®ºè¯æ˜ï¼Œè¿™äº›å‡é˜´æ€§ä¼šä¸¥é‡æŸå®³RLè®­ç»ƒï¼Œå› ä¸ºå‰¥å¤ºäº†æ¨¡å‹çš„ä¿¡æ¯åŒ–æ¢¯åº¦ä¿¡å·å¹¶å‡ç¼“äº†æ”¶æ•›é€Ÿåº¦ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TinyVï¼Œä¸€ä¸ªåŸºäºLLMçš„è½»å‹éªŒè¯å™¨ï¼Œå®ƒå¯ä»¥å¢å¼ºç°æœ‰çš„åŸºäºè§„åˆ™çš„æ–¹æ³•ï¼ŒåŠ¨æ€è¯†åˆ«æ½œåœ¨çš„å‡é˜´æ€§å¹¶æ¢å¤æœ‰æ•ˆçš„å“åº”ï¼Œä»¥äº§ç”Ÿæ›´å‡†ç¡®çš„å¥–åŠ±ä¼°è®¡ã€‚åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œé›†æˆTinyVå°†é€šè¿‡ç‡æé«˜äº†é«˜è¾¾10%ï¼Œå¹¶ä¸åŸºçº¿ç›¸æ¯”åŠ é€Ÿäº†æ”¶æ•›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜è§£å†³éªŒè¯å™¨å‡é˜´æ€§è‡³å…³é‡è¦ï¼Œå¹¶æä¾›äº†ä¸€ç§å®ç”¨çš„æ–¹æ³•æ¥æ”¹è¿›åŸºäºRLçš„LLMå¾®è°ƒã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/uw-nsl/TinyV%E3%80%82">https://github.com/uw-nsl/TinyVã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14625v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€šè¿‡ä¼˜åŒ–ç­–ç•¥å’Œä½¿ç”¨å¥–åŠ±ä¿¡å·æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒRLçš„æˆåŠŸå–å†³äºå¥–åŠ±çš„å¯é æ€§ï¼Œè¿™äº›å¥–åŠ±ç”±éªŒè¯å™¨æä¾›ã€‚æœ¬æ–‡æ­ç¤ºäº†ä¸€ä¸ªæ™®éå­˜åœ¨çš„é—®é¢˜â€”â€”å‡é˜´æ€§ï¼ˆFalse Negativeï¼‰ï¼Œå³éªŒè¯å™¨é”™è¯¯åœ°æ‹’ç»äº†æ­£ç¡®çš„æ¨¡å‹è¾“å‡ºã€‚é€šè¿‡å¯¹Big-Math-RL-Verifiedæ•°æ®é›†çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°è¶…è¿‡38%çš„æ¨¡å‹ç”Ÿæˆå“åº”å—åˆ°å‡é˜´æ€§çš„å½±å“ã€‚æœ¬æ–‡å®è¯å’Œç†è®ºåœ°è¯æ˜äº†å‡é˜´æ€§ä¼šä¸¥é‡æŸå®³RLè®­ç»ƒï¼Œå‰¥å¤ºæ¨¡å‹çš„æ¢¯åº¦ä¿¡å·å¹¶å‡æ…¢æ”¶æ•›é€Ÿåº¦ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TinyVï¼Œä¸€ä¸ªåŸºäºLLMçš„è½»é‡çº§éªŒè¯å™¨ï¼Œå®ƒé€šè¿‡å¢å¼ºç°æœ‰çš„åŸºäºè§„åˆ™çš„æ–¹æ³•ï¼Œèƒ½å¤ŸåŠ¨æ€è¯†åˆ«æ½œåœ¨çš„å‡é˜´æ€§å¹¶æ¢å¤æœ‰æ•ˆçš„å“åº”ï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®çš„å¥–åŠ±ä¼°è®¡ã€‚åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œé›†æˆTinyVå¯å°†é€šè¿‡ç‡æé«˜10%ï¼Œå¹¶ç›¸å¯¹äºåŸºçº¿åŠ é€Ÿæ”¶æ•›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç”¨äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>éªŒè¯å™¨åœ¨RLè®­ç»ƒä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œä½†å…¶å¯é æ€§æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>å‡é˜´æ€§ç°è±¡æ™®éå­˜åœ¨äºéªŒè¯å™¨çš„ä½¿ç”¨è¿‡ç¨‹ä¸­ï¼Œå…¶ä¸­éªŒè¯å™¨ä¼šé”™è¯¯åœ°æ‹’ç»æ­£ç¡®çš„æ¨¡å‹è¾“å‡ºã€‚</li>
<li>å‡é˜´æ€§é—®é¢˜ä¸¥é‡å½±å“äº†RLè®­ç»ƒçš„æ•ˆæœï¼ŒåŒ…æ‹¬å‰¥å¤ºæ¨¡å‹çš„æ¢¯åº¦ä¿¡å·å’Œå‡æ…¢æ”¶æ•›é€Ÿåº¦ã€‚</li>
<li>TinyVæ˜¯ä¸€ä¸ªåŸºäºLLMçš„è½»é‡çº§éªŒè¯å™¨ï¼Œæ—¨åœ¨è§£å†³å‡é˜´æ€§é—®é¢˜ã€‚</li>
<li>é›†æˆTinyVå¯ä»¥æé«˜æ¨¡å‹çš„é€šè¿‡ç‡å¹¶åŠ é€Ÿæ”¶æ•›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14625">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-30e78e92622af442a94c03aa27a1c2ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45315ad641603139bd5ff244f95ac9c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a08c30a5d241b44320c26435bfbb837.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b98fa5864234d9a3f6c406da5a995e97.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Not-All-Correct-Answers-Are-Equal-Why-Your-Distillation-Source-Matters"><a href="#Not-All-Correct-Answers-Are-Equal-Why-Your-Distillation-Source-Matters" class="headerlink" title="Not All Correct Answers Are Equal: Why Your Distillation Source Matters"></a>Not All Correct Answers Are Equal: Why Your Distillation Source Matters</h2><p><strong>Authors:Xiaoyu Tian, Yunjie Ji, Haotian Wang, Shuaiting Chen, Sitong Zhao, Yiping Peng, Han Zhao, Xiangang Li</strong></p>
<p>Distillation has emerged as a practical and effective approach to enhance the reasoning capabilities of open-source language models. In this work, we conduct a large-scale empirical study on reasoning data distillation by collecting verified outputs from three state-of-the-art teacher models-AM-Thinking-v1, Qwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We construct three parallel datasets and analyze their distributions, revealing that AM-Thinking-v1-distilled data exhibits greater token length diversity and lower perplexity. Student models trained on each dataset are evaluated on reasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench. The model distilled from AM-Thinking-v1 consistently achieves the best performance (e.g., 84.3 on AIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and demonstrates adaptive output behavior-producing longer responses for harder tasks and shorter ones for simpler tasks. These findings highlight the value of high-quality, verified reasoning traces. We release the AM-Thinking-v1 and Qwen3-235B-A22B distilled datasets to support future research on open and high-performing reasoning-oriented language models. The datasets are publicly available on Hugging Face\footnote{Datasets are available on Hugging Face: \href{<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled%7D%7BAM-Thinking-v1-Distilled%7D">https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled}{AM-Thinking-v1-Distilled}</a>, \href{<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled%7D%7BAM-Qwen3-Distilled%7D.%7D">https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled}{AM-Qwen3-Distilled}.}</a>. </p>
<blockquote>
<p>æ‘˜è¦æç‚¼ä½œä¸ºä¸€ç§å®ç”¨è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå·²ç»å±•ç°å‡ºæé«˜å¼€æºè¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ½œåŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹æ¨ç†æ•°æ®æç‚¼è¿›è¡Œäº†å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œé€šè¿‡æ”¶é›†æ¥è‡ªä¸‰ä¸ªé¡¶å°–æ•™å¸ˆæ¨¡å‹ï¼ˆAM-Thinking-v1ã€Qwen3-235B-A22Bå’ŒDeepSeek-R1ï¼‰çš„éªŒè¯è¾“å‡ºï¼Œåœ¨åŒ…å«189ä¸‡ä¸ªæŸ¥è¯¢çš„å…±äº«è¯­æ–™åº“ä¸Šè¿›è¡Œå¯¹æ¯”åˆ†æã€‚æˆ‘ä»¬æ„å»ºäº†ä¸‰ä¸ªå¹¶è¡Œæ•°æ®é›†å¹¶å¯¹å…¶åˆ†å¸ƒè¿›è¡Œäº†åˆ†æï¼Œå‘ç°ç”±AM-Thinking-v1æç‚¼çš„æ•°æ®å…·æœ‰æ›´å¤§çš„ä»¤ç‰Œé•¿åº¦å¤šæ ·æ€§å’Œæ›´ä½çš„å›°æƒ‘åº¦ã€‚åœ¨åŒ…æ‹¬AIME2024ã€AIME2025ã€MATH500å’ŒLiveCodeBenchç­‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼Œå¯¹åŸºäºå„æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚ç”±AM-Thinking-v1æç‚¼çš„æ¨¡å‹è¡¨ç°æœ€ä½³ï¼ˆä¾‹å¦‚ï¼ŒAIME2024ä¸Šçš„å¾—åˆ†ä¸º84.3ï¼ŒAIME2025ä¸Šçš„å¾—åˆ†ä¸º72.2ï¼ŒMATH500ä¸Šçš„å¾—åˆ†ä¸º98.4ï¼ŒLiveCodeBenchä¸Šçš„å¾—åˆ†ä¸º65.9ï¼‰ï¼Œå¹¶å±•ç°å‡ºè‡ªé€‚åº”è¾“å‡ºè¡Œä¸ºâ€”â€”ä¸ºæ›´å›°éš¾çš„ä»»åŠ¡ç”Ÿæˆæ›´é•¿çš„å“åº”ï¼Œä¸ºæ›´ç®€å•ä»»åŠ¡ç”Ÿæˆæ›´çŸ­çš„å“åº”ã€‚è¿™äº›å‘ç°çªæ˜¾äº†é«˜è´¨é‡éªŒè¯æ¨ç†è½¨è¿¹çš„ä»·å€¼ã€‚æˆ‘ä»¬å‘å¸ƒäº†AM-Thinking-v1å’ŒQwen3-235B-A22Bæç‚¼çš„æ•°æ®é›†ï¼Œä»¥æ”¯æŒæœªæ¥å¯¹é«˜æ€§èƒ½æ¨ç†å¯¼å‘çš„è¯­è¨€æ¨¡å‹çš„ç ”ç©¶ã€‚æ•°æ®é›†å·²åœ¨Hugging Faceä¸Šå…¬å¼€^[æ•°æ®é›†å·²åœ¨Hugging Faceä¸Šå…¬å¼€ï¼šé“¾æ¥ä¸º<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled">AM-Thinking-v1æç‚¼æ•°æ®é›†</a>ï¼Œ<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled">AM-Qwen3æç‚¼æ•°æ®é›†</a>ã€‚]^ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14464v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ç ”ç©¶äº†åˆ©ç”¨æ¨ç†æ•°æ®è’¸é¦æŠ€æœ¯æå‡å¼€æºè¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚é€šè¿‡å¯¹ä¸‰ç§å…ˆè¿›æ•™å¸ˆæ¨¡å‹ï¼ˆAM-Thinking-v1ã€Qwen3-235B-A22Bå’ŒDeepSeek-R1ï¼‰çš„éªŒè¯è¾“å‡ºè¿›è¡Œå¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œæ„å»ºäº†ä¸‰ä¸ªå¹¶è¡Œæ•°æ®é›†ï¼Œå¹¶åˆ†æäº†å®ƒä»¬çš„åˆ†å¸ƒç‰¹æ€§ã€‚ç»“æœæ˜¾ç¤ºï¼ŒåŸºäºAM-Thinking-v1è’¸é¦çš„æ•°æ®å±•ç°å‡ºæ›´é«˜çš„ä»¤ç‰Œé•¿åº¦å¤šæ ·æ€§å’Œæ›´ä½çš„å›°æƒ‘åº¦ã€‚åœ¨å­¦ç”Ÿæ¨¡å‹çš„è¯„ä»·ä¸­ï¼ŒåŸºäºAM-Thinking-v1è’¸é¦çš„æ¨¡å‹åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°æœ€ä½³ï¼Œå¦‚AIME2024ã€AIME2025ã€MATH500å’ŒLiveCodeBenchã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”è¾“å‡ºè¡Œä¸ºï¼Œé’ˆå¯¹å¤æ‚ä»»åŠ¡äº§ç”Ÿæ›´é•¿çš„å“åº”ï¼Œé’ˆå¯¹ç®€å•ä»»åŠ¡äº§ç”Ÿæ›´çŸ­çš„å“åº”ã€‚æœ€åï¼Œå…¬å¼€å‘å¸ƒäº†AM-Thinking-v1å’ŒQwen3-235B-A22Bè’¸é¦æ•°æ®é›†ï¼Œä»¥æ”¯æŒæœªæ¥å¯¹é«˜æ€§èƒ½æ¨ç†å¯¼å‘çš„è¯­è¨€æ¨¡å‹çš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è’¸é¦æŠ€æœ¯è¢«è¯å®ä¸ºä¸€ç§æå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>é€šè¿‡æ”¶é›†ä¸‰ä¸ªå…ˆè¿›æ•™å¸ˆæ¨¡å‹çš„éªŒè¯è¾“å‡ºï¼Œæ„å»ºäº†ä¸‰ä¸ªå¹¶è¡Œæ•°æ®é›†ã€‚</li>
<li>AM-Thinking-v1è’¸é¦æ•°æ®å±•ç°å‡ºæ›´é«˜çš„ä»¤ç‰Œé•¿åº¦å¤šæ ·æ€§å’Œæ›´ä½çš„å›°æƒ‘åº¦ã€‚</li>
<li>åŸºäºAM-Thinking-v1è’¸é¦çš„æ¨¡å‹åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”è¾“å‡ºè¡Œä¸ºï¼Œé’ˆå¯¹ä»»åŠ¡å¤æ‚åº¦è°ƒæ•´å“åº”é•¿åº¦ã€‚</li>
<li>å…¬å¼€äº†AM-Thinking-v1å’ŒQwen3-235B-A22Bè’¸é¦æ•°æ®é›†ï¼Œä»¥æ”¯æŒæœªæ¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ec3a255e332deff7ff944d61914b5862.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c61c772c5fd765f8c1f3570e98718fc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b842aff11a4adc181e4d3c7dd928285.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9c3d714eec21c41da5da3fc9abb441c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Towards-Omnidirectional-Reasoning-with-360-R1-A-Dataset-Benchmark-and-GRPO-based-Method"><a href="#Towards-Omnidirectional-Reasoning-with-360-R1-A-Dataset-Benchmark-and-GRPO-based-Method" class="headerlink" title="Towards Omnidirectional Reasoning with 360-R1: A Dataset, Benchmark, and   GRPO-based Method"></a>Towards Omnidirectional Reasoning with 360-R1: A Dataset, Benchmark, and   GRPO-based Method</h2><p><strong>Authors:Xinshen Zhang, Zhen Ye, Xu Zheng</strong></p>
<p>Omnidirectional images (ODIs), with their 360{\deg} field of view, provide unparalleled spatial awareness for immersive applications like augmented reality and embodied AI. However, the capability of existing multi-modal large language models (MLLMs) to comprehend and reason about such panoramic scenes remains underexplored. This paper addresses this gap by introducing OmniVQA, the first dataset and conducting the first benchmark for omnidirectional visual question answering. Our evaluation of state-of-the-art MLLMs reveals significant limitations in handling omnidirectional visual question answering, highlighting persistent challenges in object localization, feature extraction, and hallucination suppression within panoramic contexts. These results underscore the disconnect between current MLLM capabilities and the demands of omnidirectional visual understanding, which calls for dedicated architectural or training innovations tailored to 360{\deg} imagery. Building on the OmniVQA dataset and benchmark, we further introduce a rule-based reinforcement learning method, 360-R1, based on Qwen2.5-VL-Instruct. Concretely, we modify the group relative policy optimization (GRPO) by proposing three novel reward functions: (1) reasoning process similarity reward, (2) answer semantic accuracy reward, and (3) structured format compliance reward. Extensive experiments on our OmniVQA demonstrate the superiority of our proposed method in omnidirectional space (+6% improvement). </p>
<blockquote>
<p>å…¨æ™¯å›¾åƒï¼ˆODIsï¼‰å…·æœ‰360Â°çš„è§†é‡ï¼Œä¸ºå¢å¼ºç°å®å’ŒåµŒå…¥å¼äººå·¥æ™ºèƒ½ç­‰æ²‰æµ¸å¼åº”ç”¨æä¾›äº†æ— ä¸ä¼¦æ¯”çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¯¹å…¨æ™¯åœºæ™¯çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ä»ç„¶è¢«æ¢ç´¢ä¸è¶³ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥OmniVQAæ•°æ®é›†å’Œå¼€å±•å…¨æ™¯è§†è§‰é—®ç­”çš„é¦–ä¸ªåŸºå‡†æµ‹è¯•æ¥è§£å†³è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬å¯¹æœ€æ–°MLLMsçš„è¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨å…¨æ™¯è§†è§‰é—®ç­”æ–¹é¢å­˜åœ¨é‡å¤§å±€é™ï¼Œçªæ˜¾äº†åœ¨å…¨æ™¯ä¸Šä¸‹æ–‡ä¸­ç‰©ä½“å®šä½ã€ç‰¹å¾æå–å’Œå¹»è§‰æŠ‘åˆ¶æ–¹é¢çš„æŒç»­æŒ‘æˆ˜ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†å½“å‰MLLMèƒ½åŠ›ä¸å…¨æ™¯è§†è§‰ç†è§£éœ€æ±‚ä¹‹é—´çš„è„±èŠ‚ï¼Œè¿™è¦æ±‚é’ˆå¯¹360Â°å›¾åƒé‡èº«å®šåˆ¶ä¸“é—¨çš„æ¶æ„æˆ–åŸ¹è®­åˆ›æ–°ã€‚åŸºäºOmniVQAæ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†åŸºäºè§„åˆ™å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•360-R1ï¼Œè¯¥æ–¹æ³•åŸºäºQwen2.5-VL-Instructã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡æå‡ºä¸‰ä¸ªæ–°çš„å¥–åŠ±å‡½æ•°æ¥ä¿®æ”¹ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPO)ï¼šï¼ˆ1ï¼‰æ¨ç†è¿‡ç¨‹ç›¸ä¼¼æ€§å¥–åŠ±ï¼›ï¼ˆ2ï¼‰ç­”æ¡ˆè¯­ä¹‰å‡†ç¡®æ€§å¥–åŠ±ï¼›ï¼ˆ3ï¼‰ç»“æ„åŒ–æ ¼å¼åˆè§„æ€§å¥–åŠ±ã€‚åœ¨æˆ‘ä»¬çš„OmniVQAä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•åœ¨å…¨æ™¯ç©ºé—´ä¸­çš„ä¼˜è¶Šæ€§ï¼ˆ+6%çš„æå‡ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14197v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†OmniVQAæ•°æ®é›†å’Œå…¨æ™¯è§†è§‰é—®ç­”çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å…¨æ™¯åœºæ™¯ä¸­çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚ç°æœ‰æ¨¡å‹çš„å±€é™æ€§åœ¨äºå¤„ç†å…¨æ™¯è§†è§‰é—®ç­”çš„èƒ½åŠ›ï¼Œå°¤å…¶åœ¨ç‰©ä½“å®šä½ã€ç‰¹å¾æå–å’Œå¹»è§‰æŠ‘åˆ¶æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œå¼•å…¥äº†åŸºäºè§„åˆ™å¼ºåŒ–å­¦ä¹ çš„360åº¦å­¦ä¹ æ–¹æ³•â€”â€”360-R1ã€‚å®ƒé€šè¿‡ä¸‰ä¸ªå¥–åŠ±å‡½æ•°æå‡äº†ç°æœ‰çš„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œæé«˜äº†åœ¨å…¨æ™¯ç©ºé—´ä¸­çš„æ€§èƒ½è¡¨ç°ã€‚å®éªŒè¯æ˜ï¼Œæ­¤æ–¹æ³•ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œæå‡äº†çº¦6%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å…¨æ™¯è§†è§‰é—®ç­”ä¸Šçš„åº”ç”¨ä»æœ‰è¾ƒå¤§å·®è·ã€‚ç°æœ‰çš„æ¨¡å‹åœ¨å…¨æ™¯åœºæ™¯ä¸­å¤„ç†é—®ç­”æ—¶é¢ä¸´ç‰©ä½“å®šä½ã€ç‰¹å¾æå–å’Œå¹»è§‰æŠ‘åˆ¶æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>OmniVQAæ•°æ®é›†ä¸ºå…¨æ™¯è§†è§‰é—®ç­”æä¾›äº†é¦–ä¸ªåŸºå‡†æµ‹è¯•å¹³å°ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºè§„åˆ™å¼ºåŒ–å­¦ä¹ çš„å…¨æ™¯å­¦ä¹ æ–¹æ³•â€”â€”360-R1ï¼Œæ­¤æ–¹æ³•å¯¹ç°æœ‰çš„ç­–ç•¥ä¼˜åŒ–è¿›è¡Œäº†æ”¹è¿›ã€‚åŠ å…¥äº†ä¸‰ä¸ªæ–°çš„å¥–åŠ±å‡½æ•°ä»¥æ”¹è¿›æ¨¡å‹çš„è¡¨ç°ï¼šæ¨ç†è¿‡ç¨‹ç›¸ä¼¼æ€§å¥–åŠ±ã€ç­”æ¡ˆè¯­ä¹‰å‡†ç¡®æ€§å¥–åŠ±å’Œç»“æ„æ ¼å¼éµå®ˆå¥–åŠ±ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14197">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3927d32e682de777efa27ae977c10144.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0bc216f1d0749da63b16f603e68d54c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b11dc7c181f78f2567d181e9eb0a04da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f11ad2b6a3ea6e18630bbd445a589da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cea2301b2de1e5bcc335f2eefa8fec3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-607be82a09951e1b0ebb2969cb2e47b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d7b177973ff2655866a58dc42e84e46.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Safety-Subspaces-are-Not-Distinct-A-Fine-Tuning-Case-Study"><a href="#Safety-Subspaces-are-Not-Distinct-A-Fine-Tuning-Case-Study" class="headerlink" title="Safety Subspaces are Not Distinct: A Fine-Tuning Case Study"></a>Safety Subspaces are Not Distinct: A Fine-Tuning Case Study</h2><p><strong>Authors:Kaustubh Ponkshe, Shaan Shah, Raghav Singhal, Praneeth Vepakomma</strong></p>
<p>Large Language Models (LLMs) rely on safety alignment to produce socially acceptable responses. This is typically achieved through instruction tuning and reinforcement learning from human feedback. However, this alignment is known to be brittle: further fine-tuning, even on benign or lightly contaminated data, can degrade safety and reintroduce harmful behaviors. A growing body of work suggests that alignment may correspond to identifiable geometric directions in weight space, forming subspaces that could, in principle, be isolated or preserved to defend against misalignment. In this work, we conduct a comprehensive empirical study of this geometric perspective. We examine whether safety-relevant behavior is concentrated in specific subspaces, whether it can be separated from general-purpose learning, and whether harmfulness arises from distinguishable patterns in internal representations. Across both parameter and activation space, our findings are consistent: subspaces that amplify safe behaviors also amplify unsafe ones, and prompts with different safety implications activate overlapping representations. We find no evidence of a subspace that selectively governs safety. These results challenge the assumption that alignment is geometrically localized. Rather than residing in distinct directions, safety appears to emerge from entangled, high-impact components of the modelâ€™s broader learning dynamics. This suggests that subspace-based defenses may face fundamental limitations and underscores the need for alternative strategies to preserve alignment under continued training. We corroborate these findings through multiple experiments on five open-source LLMs. Our code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/CERT-Lab/safety-subspaces">https://github.com/CERT-Lab/safety-subspaces</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¾èµ–äºå®‰å…¨å¯¹é½æ¥äº§ç”Ÿç¤¾ä¼šå¯æ¥å—çš„å›åº”ã€‚è¿™é€šå¸¸æ˜¯é€šè¿‡æŒ‡ä»¤è°ƒæ•´å’Œäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ¥å®ç°çš„ã€‚ç„¶è€Œï¼Œè¿™ç§å¯¹é½æ˜¯è„†å¼±çš„ï¼šå³ä½¿åœ¨è‰¯æ€§æˆ–è½»åº¦æ±¡æŸ“çš„æ•°æ®ä¸Šè¿›è¡Œè¿›ä¸€æ­¥çš„å¾®è°ƒï¼Œä¹Ÿå¯èƒ½ä¼šé™ä½å®‰å…¨æ€§å¹¶é‡æ–°å¼•å…¥æœ‰å®³è¡Œä¸ºã€‚è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶è¡¨æ˜ï¼Œå¯¹é½å¯èƒ½ä¸æƒé‡ç©ºé—´ä¸­çš„å¯è¯†åˆ«å‡ ä½•æ–¹å‘ç›¸å¯¹åº”ï¼Œå½¢æˆå­ç©ºé—´ï¼Œåœ¨åŸåˆ™ä¸Šå¯ä»¥è¢«éš”ç¦»æˆ–ä¿æŠ¤ä»¥é˜²èŒƒé”™ä½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹è¿™ä¸€å‡ ä½•è§‚ç‚¹è¿›è¡Œäº†å…¨é¢çš„å®è¯ç ”ç©¶ã€‚æˆ‘ä»¬ç ”ç©¶äº†å®‰å…¨ç›¸å…³çš„è¡Œä¸ºæ˜¯å¦é›†ä¸­åœ¨ç‰¹å®šçš„å­ç©ºé—´ä¸­ï¼Œæ˜¯å¦å¯ä»¥ä¸é€šç”¨å­¦ä¹ åˆ†ç¦»ï¼Œä»¥åŠæœ‰å®³æ€§æ˜¯å¦æºäºå†…éƒ¨è¡¨ç¤ºä¸­çš„å¯è¯†åˆ«æ¨¡å¼ã€‚åœ¨å‚æ•°ç©ºé—´å’Œæ¿€æ´»ç©ºé—´ä¸Šï¼Œæˆ‘ä»¬çš„å‘ç°æ˜¯ä¸€è‡´çš„ï¼šæ”¾å¤§å®‰å…¨è¡Œä¸ºçš„å­ç©ºé—´ä¹Ÿä¼šæ”¾å¤§ä¸å®‰å…¨çš„è¡Œä¸ºï¼Œå…·æœ‰ä¸åŒå®‰å…¨å«ä¹‰çš„æç¤ºä¼šæ¿€æ´»é‡å çš„è¡¨ç¤ºã€‚æˆ‘ä»¬æ²¡æœ‰å‘ç°é€‰æ‹©æ€§åœ°æ§åˆ¶å®‰å…¨çš„å­ç©ºé—´è¯æ®ã€‚è¿™äº›ç»“æœæŒ‘æˆ˜äº†å¯¹é½åœ¨å‡ ä½•ä¸Šå®šä½çš„å‡è®¾ã€‚å®‰å…¨ä¼¼ä¹å¹¶éæ¥è‡ªä¸åŒçš„æ–¹å‘ï¼Œè€Œæ˜¯æ¥è‡ªäºæ¨¡å‹æ›´å¹¿æ³›å­¦ä¹ åŠ¨æ€ä¸­çš„çº ç¼ ã€é«˜å½±å“åŠ›çš„ç»„æˆéƒ¨åˆ†ã€‚è¿™æš—ç¤ºåŸºäºå­ç©ºé—´çš„é˜²å¾¡å¯èƒ½é¢ä¸´æ ¹æœ¬æ€§çš„é™åˆ¶ï¼Œå¹¶å¼ºè°ƒäº†ç»§ç»­è®­ç»ƒæ—¶ä¿ç•™å¯¹é½çš„æ›¿ä»£ç­–ç•¥çš„éœ€è¦ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šé€šè¿‡å¤šæ¬¡å®éªŒè¯å®äº†è¿™äº›å‘ç°ã€‚æˆ‘ä»¬çš„ä»£ç åœ¨å…¬å…±å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/CERT-Lab/safety-subspaces%E3%80%82">https://github.com/CERT-Lab/safety-subspacesã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14185v1">PDF</a> Kaustubh Ponkshe, Shaan Shah, and Raghav Singhal contributed equally   to this work</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¾èµ–å®‰å…¨å¯¹é½æ¥ç”Ÿæˆç¤¾ä¼šå¯æ¥å—çš„å›åº”ï¼Œè¿™é€šå¸¸é€šè¿‡æŒ‡ä»¤è°ƒæ•´å’Œäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ¥å®ç°ã€‚ç„¶è€Œï¼Œè¿™ç§å¯¹é½æ˜¯è„†å¼±çš„ï¼šå³ä½¿åœ¨è‰¯æ€§æˆ–è½»åº¦æ±¡æŸ“çš„æ•°æ®ä¸Šè¿›è¡Œè¿›ä¸€æ­¥çš„å¾®è°ƒï¼Œä¹Ÿå¯èƒ½ä¼šç ´åå®‰å…¨æ€§å¹¶é‡æ–°å¼•å…¥æœ‰å®³è¡Œä¸ºã€‚æœ¬ç ”ç©¶ä»å‡ ä½•è§’åº¦å¯¹è¿™ä¸€ç°è±¡è¿›è¡Œäº†å…¨é¢çš„å®è¯ç ”ç©¶ã€‚æˆ‘ä»¬æ²¡æœ‰å‘ç°åªæ§åˆ¶å®‰å…¨æ€§çš„å­ç©ºé—´ï¼Œè¿™æ„å‘³ç€å®‰å…¨æ€§å¹¶éå±€é™äºç‰¹å®šçš„å‡ ä½•æ–¹å‘ï¼Œè€Œæ˜¯æºäºæ¨¡å‹å­¦ä¹ åŠ¨åŠ›å­¦çš„å¤æ‚çº ç¼ å’Œé«˜å½±å“åŠ›æˆåˆ†ã€‚è¿™æŒ‘æˆ˜äº†ç°æœ‰çš„å‡ ä½•å®šä½å‡è®¾ï¼Œå¹¶ä¸ºç»§ç»­è®­ç»ƒæ—¶ä¿æŒå¯¹é½æå‡ºäº†æ–°æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CERT-Lab/safety-subspaces">CERT-Lab&#x2F;safety-subspaces</a>å…¬å¼€è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éœ€è¦å®‰å…¨å¯¹é½ä»¥äº§ç”Ÿç¤¾ä¼šå¯æ¥å—å“åº”ã€‚</li>
<li>é€šè¿‡æŒ‡ä»¤è°ƒæ•´å’Œäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ å®ç°å®‰å…¨å¯¹é½ã€‚</li>
<li>å®‰å…¨å¯¹é½æ˜¯è„†å¼±çš„ï¼Œè¿›ä¸€æ­¥å¾®è°ƒå¯èƒ½ç ´åå®‰å…¨æ€§ã€‚</li>
<li>ä»å‡ ä½•è§’åº¦ç ”ç©¶å®‰å…¨æ€§çš„å­ç©ºé—´ï¼Œå‘ç°å®‰å…¨æ€§å¹¶éå±€é™äºç‰¹å®šæ–¹å‘ã€‚</li>
<li>å®‰å…¨è¡Œä¸ºå¹¶éå­¤ç«‹å­˜åœ¨ï¼Œè€Œæ˜¯æ¨¡å‹å­¦ä¹ åŠ¨åŠ›å­¦çš„å¤æ‚çº ç¼ å’Œé«˜å½±å“åŠ›æˆåˆ†çš„ç»“æœã€‚</li>
<li>éœ€è¦æ–°çš„ç­–ç•¥æ¥åœ¨ç»§ç»­è®­ç»ƒæ—¶ä¿æŒå¯¹é½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14185">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c98efefeb66776bd7ae000a748e36c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-53fc449e73e1256a68d4eeda3f48d84a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d49e86f5811453d32b2e6e4f8a3fb716.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1c10c279bd43ae4b47673a4c80b0875d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6359feaef0f40a90473e1481b63c18d.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Prior-Prompt-Engineering-for-Reinforcement-Fine-Tuning"><a href="#Prior-Prompt-Engineering-for-Reinforcement-Fine-Tuning" class="headerlink" title="Prior Prompt Engineering for Reinforcement Fine-Tuning"></a>Prior Prompt Engineering for Reinforcement Fine-Tuning</h2><p><strong>Authors:Pittawat Taveekitworachai, Potsawee Manakul, Sarana Nutanong, Kunat Pipatanakul</strong></p>
<p>This paper investigates prior prompt engineering (pPE) in the context of reinforcement fine-tuning (RFT), where language models (LMs) are incentivized to exhibit behaviors that maximize performance through reward signals. While existing RFT research has primarily focused on algorithms, reward shaping, and data curation, the design of the prior promptâ€“the instructions prepended to queries during training to elicit behaviors such as step-by-step reasoningâ€“remains underexplored. We investigate whether different pPE approaches can guide LMs to internalize distinct behaviors after RFT. Inspired by inference-time prompt engineering (iPE), we translate five representative iPE strategiesâ€“reasoning, planning, code-based reasoning, knowledge recall, and null-example utilizationâ€“into corresponding pPE approaches. We experiment with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and GPQA-Diamond). Our results show that all pPE-trained models surpass their iPE-prompted counterparts, with the null-example pPE approach achieving the largest average performance gain and the highest improvement on AIME2024 and GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by adapting a behavior-classification framework, we demonstrate that different pPE strategies instill distinct behavioral styles in the resulting models. These findings position pPE as a powerful yet understudied axis for RFT. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰èƒŒæ™¯ä¸‹çš„å…ˆéªŒæç¤ºå·¥ç¨‹ï¼ˆpPEï¼‰ï¼Œåœ¨å¼ºåŒ–å¾®è°ƒä¸­ï¼Œé€šè¿‡å¥–åŠ±ä¿¡å·æ¿€åŠ±è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰è¡¨ç°å‡ºæœ€å¤§åŒ–æ€§èƒ½çš„è¡Œä¸ºã€‚è™½ç„¶ç°æœ‰çš„RFTç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç®—æ³•ã€å¥–åŠ±å¡‘é€ å’Œæ•°æ®æ•´ç†ä¸Šï¼Œä½†å…ˆéªŒæç¤ºçš„è®¾è®¡ï¼Œå³åœ¨è®­ç»ƒæœŸé—´æŸ¥è¯¢ä¹‹å‰æ·»åŠ çš„æŒ‡å¯¼æ­¥éª¤è¡Œä¸ºï¼ˆå¦‚é€æ­¥æ¨ç†ï¼‰ï¼Œä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬è°ƒæŸ¥ä¸åŒçš„pPEæ–¹æ³•æ˜¯å¦èƒ½åœ¨RFTåå¼•å¯¼è¯­è¨€æ¨¡å‹å†…åŒ–ä¸åŒçš„è¡Œä¸ºã€‚æˆ‘ä»¬å—åˆ°æ¨ç†æ—¶é—´æç¤ºå·¥ç¨‹ï¼ˆiPEï¼‰çš„å¯å‘ï¼Œå°†äº”ç§ä»£è¡¨æ€§çš„iPEç­–ç•¥ï¼ˆæ¨ç†ã€è§„åˆ’ã€åŸºäºä»£ç çš„æ¨ç†ã€çŸ¥è¯†å›å¿†å’Œç©ºä¾‹åˆ©ç”¨ï¼‰ç¿»è¯‘æˆç›¸åº”çš„pPEæ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨Qwen2.5-7Bè¿›è¡Œè¯•éªŒï¼Œåˆ†åˆ«é‡‡ç”¨å„ç§pPEæ–¹æ³•ï¼Œç„¶ååœ¨åŸŸå†…å’ŒåŸŸå¤–åŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚AIME2024ã€HumanEval+å’ŒGPQA-Diamondï¼‰ä¸Šè¯„ä¼°æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰pPEè®­ç»ƒçš„æ¨¡å‹éƒ½è¶…è¿‡äº†iPEæç¤ºçš„å¯¹åº”æ¨¡å‹ï¼Œå…¶ä¸­ç©ºä¾‹pPEæ–¹æ³•å–å¾—äº†æœ€å¤§çš„å¹³å‡æ€§èƒ½æå‡ï¼Œå¹¶åœ¨AIME2024å’ŒGPQA-Diamondä¸Šçš„æ”¹è¿›æœ€é«˜ï¼Œè¶…è¿‡äº†å¸¸ç”¨çš„æ¨ç†æ–¹æ³•ã€‚æ­¤å¤–ï¼Œé€šè¿‡é‡‡ç”¨è¡Œä¸ºåˆ†ç±»æ¡†æ¶ï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸åŒçš„pPEç­–ç•¥åœ¨ç»“æœæ¨¡å‹ä¸­å½¢æˆäº†ä¸åŒçš„è¡Œä¸ºé£æ ¼ã€‚è¿™äº›å‘ç°å°†pPEå®šä½ä¸ºRFTä¸­ä¸€ä¸ªå¼ºå¤§è€Œå°šæœªå……åˆ†ç ”ç©¶çš„è½´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14157v1">PDF</a> 25 pages, 42 figures</p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡æ¢è®¨äº†å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰èƒŒæ™¯ä¸‹çš„å‰æœŸæç¤ºå·¥ç¨‹ï¼ˆpPEï¼‰ï¼Œç ”ç©¶å¦‚ä½•é€šè¿‡å¥–åŠ±ä¿¡å·æ¿€åŠ±è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰å±•ç°å‡ºæœ€å¤§åŒ–æ€§èƒ½çš„è¡Œä¸ºã€‚æ–‡ç« å‘ç°ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç®—æ³•ã€å¥–åŠ±å¡‘é€ å’Œæ•°æ®æ”¶é›†æ–¹é¢ï¼Œè€Œå‰æœŸæç¤ºçš„è®¾è®¡ä»è¢«å¿½è§†ã€‚æœ¬ç ”ç©¶é€šè¿‡ç¿»è¯‘äº”ç§å³æ—¶æç¤ºå·¥ç¨‹ç­–ç•¥ä¸ºç›¸åº”çš„å‰æœŸæç¤ºå·¥ç¨‹æ–¹æ³•ï¼Œæ¢è®¨äº†ä¸åŒpPEæ–¹æ³•æ˜¯å¦èƒ½å¼•å¯¼è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å¾®è°ƒåå†…åŒ–ä¸åŒè¡Œä¸ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æœ‰ä½¿ç”¨pPEè®­ç»ƒçš„æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸå’Œè·¨é¢†åŸŸåŸºå‡†æµ‹è¯•ä¸­å‡è¶…è¶Šäº†ä½¿ç”¨iPEæç¤ºçš„æ¨¡å‹ï¼Œå…¶ä¸­åŸºäºnull-exampleçš„pPEæ–¹æ³•å–å¾—äº†æœ€å¤§çš„å¹³å‡æ€§èƒ½æå‡å’Œæœ€é«˜çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œé€šè¿‡è¡Œä¸ºåˆ†ç±»æ¡†æ¶ï¼Œç ”ç©¶è¯æ˜äº†ä¸åŒçš„pPEç­–ç•¥å¯¹æ¨¡å‹çš„æœ€ç»ˆè¡Œä¸ºé£æ ¼æœ‰å½±å“ã€‚è¿™è¡¨æ˜å‰æœŸæç¤ºå·¥ç¨‹æ˜¯å¼ºåŒ–å¾®è°ƒä¸­ä¸€ä¸ªå¼ºå¤§ä½†å°šæœªè¢«å……åˆ†ç ”ç©¶çš„é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯¥ç ”ç©¶æ¢ç´¢äº†å‰æœŸæç¤ºå·¥ç¨‹ï¼ˆpPEï¼‰åœ¨å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ä¸­çš„ä½œç”¨ï¼Œä¸»è¦å…³æ³¨å¦‚ä½•é€šè¿‡å¥–åŠ±ä¿¡å·æ¿€åŠ±è¯­è¨€æ¨¡å‹å±•ç°å‡ºæœ€å¤§åŒ–æ€§èƒ½çš„è¡Œä¸ºã€‚</li>
<li>æ–‡ç« å‘ç°ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç®—æ³•ã€å¥–åŠ±å¡‘é€ å’Œæ•°æ®æ”¶é›†æ–¹é¢ï¼Œå‰æœŸæç¤ºè®¾è®¡å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚</li>
<li>é€šè¿‡ç¿»è¯‘äº”ç§å³æ—¶æç¤ºå·¥ç¨‹ç­–ç•¥ä¸ºç›¸åº”çš„å‰æœŸæç¤ºå·¥ç¨‹æ–¹æ³•ï¼Œç ”ç©¶äº†ä¸åŒpPEæ–¹æ³•æ˜¯å¦èƒ½å¼•å¯¼è¯­è¨€æ¨¡å‹å†…åŒ–ä¸åŒè¡Œä¸ºã€‚</li>
<li>ä½¿ç”¨pPEè®­ç»ƒçš„æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸå’Œè·¨é¢†åŸŸåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå…¶ä¸­null-exampleçš„pPEæ–¹æ³•å–å¾—äº†æœ€å¤§çš„æ€§èƒ½æå‡ã€‚</li>
<li>é€šè¿‡è¡Œä¸ºåˆ†ç±»æ¡†æ¶çš„ç ”ç©¶æ˜¾ç¤ºï¼Œä¸åŒçš„pPEç­–ç•¥å½±å“æ¨¡å‹çš„æœ€ç»ˆè¡Œä¸ºé£æ ¼ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜å‰æœŸæç¤ºå·¥ç¨‹åœ¨å¼ºåŒ–å¾®è°ƒä¸­å…·æœ‰é‡è¦ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e9d04d454decc647793275819276dbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cec3056e34c7989651d2ac2826c49671.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efbe8cfe858a805ffd124c98fbcc0406.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2dfb5653ed133365ec9b63c15b18ae35.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7578a83133fff34a63a04086f692d92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60c3260b04712cc08e5589650c77a713.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-26/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-26/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-27/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-70c31cfd1d18826970e8dacbfbad1abe.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-27  One RL to See Them All Visual Triple Unified Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-25/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-10c20e5ebcfcbbe0109d82b703a8120c.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-25  Evaluation and optimization of deep learning models for enhanced   detection of brain cancer using transmission optical microscopy of thin brain   tissue samples
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24801.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
