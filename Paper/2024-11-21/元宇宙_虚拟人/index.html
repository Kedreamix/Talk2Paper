<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="元宇宙/虚拟人"><meta name="description" content="元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-11-21  ResLearn Transformer-based Residual Learning for Metaverse Network   Traffic Prediction"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="referrer" content="no-referrer-when-downgrade"><title>元宇宙/虚拟人 | Talk2Paper</title><link rel="icon" type="image/png" href="/Talk2Paper/favicon.png"><style>body{background-image:url(/Talk2Paper/background.jpg);background-repeat:no-repeat;background-size:100% 100%;background-attachment:fixed}</style><link rel="stylesheet" href="/Talk2Paper/libs/awesome/css/all.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/materialize/materialize.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/aos/aos.css"><link rel="stylesheet" href="/Talk2Paper/libs/animate/animate.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" href="/Talk2Paper/css/matery.css"><link rel="stylesheet" href="/Talk2Paper/css/my.css"><link rel="stylesheet" href="/Talk2Paper/css/dark.css" media="none" onload='"all"!=media&&(media="all")'><link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css"><link rel="stylesheet" href="/Talk2Paper/css/post.css"><script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script><meta name="generator" content="Hexo 7.3.0"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/Talk2Paper/" class="waves-effect waves-light"><img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO"> <span class="logo-span">Talk2Paper</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:0.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:0.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:0.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:0.6"></i> <span>归档</span></a></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:0.85"></i></a></li><li><a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式"><i id="sum-moon-icon" class="fas fa-sun" style="zoom:0.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img"><div class="logo-name">Talk2Paper</div><div class="logo-desc">Never really desperate, only the lost of the soul.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/Talk2Paper/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/Talk2Paper/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/Talk2Paper/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/Talk2Paper/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li><div class="divider"></div></li><li><a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank"><i class="fab fa-github-square fa-fw"></i> Fork Me</a></li></ul></div></div><style>.nav-transparent .github-corner{display:none!important}.github-corner{position:absolute;z-index:10;top:0;right:0;border:0;transform:scale(1.1)}.github-corner svg{color:#0f9d58;fill:#fff;height:64px;width:64px}.github-corner:hover .octo-arm{animation:a .56s ease-in-out}.github-corner .octo-arm{animation:none}@keyframes a{0%,to{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}</style><a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank" data-tooltip="Fork Me" data-position="left" data-delay="50"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a></nav></header><div class="bg-cover pd-header post-cover" style="background-image:url('https://picx.zhimg.com/v2-6492c26268ceafb48fc99a926ebc7b93.jpg')"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">元宇宙/虚拟人</h1></div></div></div></div></div><main class="post-container content"><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"><span class="chip bg-color">元宇宙/虚拟人</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">元宇宙/虚拟人</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i> 发布日期:&nbsp;&nbsp; 2024-11-21</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i> 更新日期:&nbsp;&nbsp; 2024-12-11</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i> 文章字数:&nbsp;&nbsp; 3.5k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i> 阅读时长:&nbsp;&nbsp; 13 分</div><div id="busuanzi_container_page_pv" class="info-break-policy"><i class="far fa-eye fa-fw"></i> 阅读次数:&nbsp;&nbsp;<span id="busuanzi_value_page_pv"></span></div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-11-21-更新"><a href="#2024-11-21-更新" class="headerlink" title="2024-11-21 更新"></a>2024-11-21 更新</h1><h2 id="ResLearn-Transformer-based-Residual-Learning-for-Metaverse-Network-Traffic-Prediction"><a href="#ResLearn-Transformer-based-Residual-Learning-for-Metaverse-Network-Traffic-Prediction" class="headerlink" title="ResLearn: Transformer-based Residual Learning for Metaverse Network   Traffic Prediction"></a>ResLearn: Transformer-based Residual Learning for Metaverse Network Traffic Prediction</h2><p><strong>Authors:Yoga Suhas Kuruba Manjunath, Mathew Szymanowski, Austin Wissborn, Mushu Li, Lian Zhao, Xiao-Ping Zhang</strong></p><p>Our work proposes a comprehensive solution for predicting Metaverse network traffic, addressing the growing demand for intelligent resource management in eXtended Reality (XR) services. We first introduce a state-of-the-art testbed capturing a real-world dataset of virtual reality (VR), augmented reality (AR), and mixed reality (MR) traffic, made openly available for further research. To enhance prediction accuracy, we then propose a novel view-frame (VF) algorithm that accurately identifies video frames from traffic while ensuring privacy compliance, and we develop a Transformer-based progressive error-learning algorithm, referred to as ResLearn for Metaverse traffic prediction. ResLearn significantly improves time-series predictions by using fully connected neural networks to reduce errors, particularly during peak traffic, outperforming prior work by 99%. Our contributions offer Internet service providers (ISPs) robust tools for real-time network management to satisfy Quality of Service (QoS) and enhance user experience in the Metaverse.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11894v1">PDF</a></p><p><strong>Summary</strong><br>提出预测元宇宙网络流量的综合解决方案，提升XR服务资源管理的智能化水平。</p><p><strong>Key Takeaways</strong></p><ul><li>构建涵盖VR、AR和MR的真实数据集测试平台。</li><li>提出隐私合规的VF算法识别视频帧。</li><li>开发基于Transformer的ResLearn算法提高预测准确性。</li><li>ResLearn通过全连接神经网络降低误差，尤其在高峰期。</li><li>突破前人工作，预测准确率提升99%。</li><li>为ISP提供实时网络管理工具，优化元宇宙用户体验。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 《ResLearn: 基于Transformer的残差学习用于元宇宙网络流量预测》</p></li><li><p>Authors: Yoga Suhas Kuruba Manjunath, Mathew Szymanowski, Austin Wissborn, Mushu Li, Lian Zhao, and Xiao-Ping Zhang</p></li><li><p>Affiliation:</p></li></ol><ul><li>Yoga Suhas Kuruba Manjunath, Mathew Szymanowski, and Austin Wissborn are from the Department of Electrical, Computer &amp; Biomedical Engineering at Toronto Metropolitan University in Canada.</li><li>Mushu Li is from the Department of Computer Science and Engineering at Lehigh University in the United States.</li><li>Lian Zhao is also affiliated with Toronto Metropolitan University.</li><li>Xiao-Ping Zhang is from the Shenzhen Key Laboratory of Ubiquitous Data Enabling at Tsinghua Shenzhen International Graduate School in China.</li></ul><ol start="4"><li><p>Keywords: Metaverse Network Traffic Prediction, Residual Learning, Extended Reality (XR), virtual reality (VR), augmented reality (AR), mixed reality (MR)</p></li><li><p>Urls: Paper Link (To be provided after publication), Github Code Link (Github: None)</p></li><li><p>Summary:</p><ul><li>(1)研究背景：随着元宇宙（Metaverse）的快速发展，尤其是扩展现实（XR）技术的普及，元宇宙网络流量预测变得越来越重要。本文旨在提出一种用于预测元宇宙网络流量的综合解决方案，以满足智能资源管理的高服务质量（QoS）需求，提高用户体验。</li></ul><p>-(2)过去的方法及问题：现有研究中，对于元宇宙网络流量的预测主要依赖于传统的机器学习模型或深度学习模型。然而，这些方法在预测精度和实时性方面存在不足，特别是在处理复杂的、非线性的时间序列数据时表现不佳。此外，现有研究缺乏真实世界的元宇宙数据集，使得预测模型的性能评估受到限制。</p><p>-(3)研究方法：针对上述问题，本文提出了一种基于Transformer的残差学习算法（ResLearn）进行元宇宙网络流量预测。首先，引入了一种先进的测试平台来捕获虚拟现实（VR）、增强现实（AR）和混合现实（MR）的真实世界数据集，并公开用于进一步研究。其次，提出了一种新颖的视图帧（VF）算法，能够准确地从流量中识别视频帧，同时确保隐私合规性。最后，开发了基于Transformer的渐进误差学习算法进行流量预测。该算法利用全连接神经网络来减少误差，特别是在高峰时段，性能优于先前的工作。</p><p>-(4)任务与性能：本文的方法在预测元宇宙网络流量方面取得了显著成果。在真实世界数据集上的实验结果表明，ResLearn算法在预测精度和实时性方面均优于现有方法，特别是在峰值流量期间的预测效果更加显著。本文的贡献为互联网服务提供商（ISPs）提供了实时网络管理的稳健工具，为维持高质量的服务和提高用户体验提供了支持。</p></li><li><p>结论：</p></li></ol><p>(1) 研究意义：随着元宇宙（Metaverse）的快速发展，该研究工作对于元宇宙网络流量预测具有重要意义。该研究旨在提高预测精度和实时性，满足智能资源管理的高服务质量（QoS）需求，从而提升用户体验。</p><p>(2) 综述创新点、性能、工作量：</p><p>创新点：文章提出了一种基于Transformer的残差学习算法（ResLearn）进行元宇宙网络流量预测，这是一种新的视角和方法。此外，文章还引入了先进的测试平台来捕获VR、AR和MR的真实世界数据集，并公开用于进一步研究，这也是一个重大的贡献。</p><p>性能：在真实世界数据集上的实验结果表明，ResLearn算法在预测精度和实时性方面均优于现有方法，特别是在峰值流量期间的预测效果更加显著。这为互联网服务提供商（ISPs）提供了实时网络管理的稳健工具。</p><p>工作量：文章对元宇宙网络流量的预测问题进行了深入研究，从研究背景、现有方法的问题、研究方法、实验任务与性能等方面进行了全面的阐述，工作量较大。</p><p>总的来说，这篇文章在元宇宙网络流量预测方面取得了显著的成果，具有一定的创新性和实用性。</p><details><summary>点此查看论文截图</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3f7533012cdadbd5780f3d04c93d597c.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-401d877e0ad5a63cc64e55acdcf04e4e.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-e2d9060198fa957a41c01fb5635de1ac.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-860cc6b3311ba0e4c399c5c48afc0ba0.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-946039145542a4f8e64668801f5ea212.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-11b4e5b59d7b1dcabb049f4ec23be03f.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b320c13c958940818b9f3071e1b0e1d0.jpg" align="middle"></details><h2 id="GGAvatar-Reconstructing-Garment-Separated-3D-Gaussian-Splatting-Avatars-from-Monocular-Video"><a href="#GGAvatar-Reconstructing-Garment-Separated-3D-Gaussian-Splatting-Avatars-from-Monocular-Video" class="headerlink" title="GGAvatar: Reconstructing Garment-Separated 3D Gaussian Splatting Avatars   from Monocular Video"></a>GGAvatar: Reconstructing Garment-Separated 3D Gaussian Splatting Avatars from Monocular Video</h2><p><strong>Authors:Jingxuan Chen</strong></p><p>Avatar modelling has broad applications in human animation and virtual try-ons. Recent advancements in this field have focused on high-quality and comprehensive human reconstruction but often overlook the separation of clothing from the body. To bridge this gap, this paper introduces GGAvatar (Garment-separated 3D Gaussian Splatting Avatar), which relies on monocular videos. Through advanced parameterized templates and unique phased training, this model effectively achieves decoupled, editable, and realistic reconstruction of clothed humans. Comparative evaluations with other costly models confirm GGAvatar’s superior quality and efficiency in modelling both clothed humans and separable garments. The paper also showcases applications in clothing editing, as illustrated in Figure 1, highlighting the model’s benefits and the advantages of effective disentanglement. The code is available at <a target="_blank" rel="noopener" href="https://github.com/J-X-Chen/GGAvatar/">https://github.com/J-X-Chen/GGAvatar/</a>.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.09952v1">PDF</a> MMAsia’24 Accepted</p><p><strong>Summary</strong><br>该论文提出GGAvatar模型，通过单目视频实现服装分离的人体建模，提高了建模质量与效率。</p><p><strong>Key Takeaways</strong></p><ol><li>GGAvatar模型利用单目视频进行服装分离的人体建模。</li><li>采用参数化模板和独特分阶段训练实现服装与人体分离。</li><li>模型实现服装的可编辑性和真实感。</li><li>与其他模型相比，GGAvatar在建模质量和效率上更优。</li><li>应用在服装编辑中，展示了模型的优点。</li><li>模型代码开源。</li><li>模型可分离服装，具有有效解耦特性。</li></ol><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：基于单目视频的衣物分离三维高斯重建虚拟角色模型研究</p></li><li><p>作者：陈静轩</p></li><li><p>所属机构：英国伯明翰大学与暨南大学联合研究所（中国广州）</p></li><li><p>关键词：三维高斯重建（3DGS），新颖视角合成，衣饰重建，衣物编辑</p></li><li><p>代码链接：根据提供的链接，Github代码链接为：<a target="_blank" rel="noopener" href="https://github.com/J-X-Chen/GGAvatar/">Github链接地址</a>。但请注意，如果链接不可用或无代码提供，则填写为“Github:None”。</p></li><li><p>摘要：</p><ul><li><p>(1)研究背景：本文研究了计算机图形学和计算机视觉中的一项重要任务，即重建真实感衣物的数字人类及其服饰。随着技术的发展，尽管已经出现了许多重建方法，但如何快速高效且准确地重建衣物的数字人类仍然是一个挑战。本文旨在解决这一问题。</p></li><li><p>(2)过去的方法及问题：过去的重建方法主要集中在高质感的整体人类重建上，但往往忽略了衣物与身体的分离。虽然最近的模型尝试使用神经网络渲染技术来捕捉表面精细纹理，但它们通常需要大量的训练时间和计算资源。此外，由于缺乏解耦功能，这些模型的实用性在现实世界场景中受到限制。因此，需要一种既能快速重建又能实现衣物与身体分离的方法。</p></li><li><p>(3)研究方法：针对上述问题，本文提出了GGAvatar模型，即基于单目视频的衣物分离三维高斯重建虚拟角色模型。该模型采用参数化模板和分阶段训练策略，实现了快速、可编辑和逼真的衣物分离重建。通过利用先进的参数化模板和独特的分阶段训练策略，该模型有效地实现了衣物的解耦和重建。此外，该模型还支持衣物编辑等应用。</p></li><li><p>(4)任务与性能：本文的方法在重建衣物的数字人类和衣物编辑任务上取得了显著成果。通过与其他成本较高的模型进行比较，证明了GGAvatar模型在建模质量和效率方面的优越性。此外，该模型在虚拟试穿等实际应用中的表现也证明了其解耦能力的重要性。总体而言，该方法的性能达到了预期目标，并为相关领域的研究提供了新的思路和方法。</p></li></ul></li><li><p>方法论概述：</p></li></ol><p>本文提出了一种基于单目视频的衣物分离三维高斯重建虚拟角色模型的方法，具体步骤如下：</p><ul><li><p>(1) 研究背景与问题定义：针对计算机图形学和计算机视觉中的真实感衣物数字人类及其服饰重建问题，指出虽然已有许多重建方法，但如何快速高效且准确地重建衣物的数字人类仍然是一个挑战。</p></li><li><p>(2) 模板估计与初始化：采用GGAvatar模型，利用参数化模板和分阶段训练策略，实现快速、可编辑和逼真的衣物分离重建。首先，通过FrankMocap估计人体姿态，确定正确的参数。然后，利用SCHP方法和ISP模型，从前视图合成衣物模板。最后，通过多层感知器（MLP）学习衣物和身体的形状，创建衣物网格模板。</p></li><li><p>(3) 高斯表示与变形处理：借鉴3D高斯混合模型，将衣物和人体重建结果表示为高斯。通过定义高斯顶点集，结合旋转、尺寸调整、透明度因子和颜色辐射函数，构建衣物的高斯表示。利用可学习的皮肤权重和目标骨转换，实现高斯集的变形处理。</p></li><li><p>(4) 渲染与图像生成：在观察空间中，通过映射操作实现高斯集的渲染。采用体积渲染技术，根据高斯属性的贡献计算最终颜色。同时，通过引入二维高斯计算透明度贡献，实现图像的生成。</p></li><li><p>(5) 训练损失与优化：在初始隔离阶段，采用密集和修剪策略，计算衣物和身体部分的重建损失。在联合训练阶段，优化高斯集而不添加或删除组件。主要重建损失通过比较真实图像和渲染图像来计算，同时引入随机结构相似性损失以优化结果。</p></li></ul><p>本文通过整合参数化模板、高斯表示、变形处理和渲染技术，提出了一种有效的衣物分离三维高斯重建虚拟角色模型方法。<br>8. 结论：</p><p>（1）这篇论文研究的课题具有重要的现实意义和学术价值。它提出了一种基于单目视频的衣物分离三维高斯重建虚拟角色模型方法，能够为计算机图形学和计算机视觉领域的研究提供新的思路和方法。该研究能够为虚拟人物创建、虚拟试衣等应用提供技术支持，具有广泛的应用前景。</p><p>（2）创新点：该文章的创新性体现在提出了基于单目视频的衣物分离三维高斯重建模型，实现了快速、可编辑和逼真的衣物分离重建。该方法通过参数化模板和分阶段训练策略，有效地解决了传统重建方法中存在的问题，如计算量大、建模质量不高等。此外，该模型还支持衣物编辑等应用，进一步拓展了其应用场景。<br>性能：该文章的方法在重建衣物的数字人类和衣物编辑任务上取得了显著成果，通过与成本较高的模型进行比较，证明了其在建模质量和效率方面的优越性。同时，该模型在虚拟试穿等实际应用中的表现也证明了其解耦能力的重要性。总体而言，该方法的性能达到了预期目标。<br>工作量：文章详细介绍了方法论的各个步骤，包括模板估计与初始化、高斯表示与变形处理、渲染与图像生成、训练损失与优化等。同时，文章还通过大量的实验验证了方法的有效性，证明了其在计算机图形学和计算机视觉领域的应用价值。然而，文章未详细阐述代码实现的具体细节和复杂性，对于理解其工作量方面存在一定不足。</p><details><summary>点此查看论文截图</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1287a8ac11961246e3e1d086d0194818.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6492c26268ceafb48fc99a926ebc7b93.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-76a5aa7dc70d1361642cc0ee76260449.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-49e4e05cda8e566ffa362089bde45f5f.jpg" align="middle"></details></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者:</i></span> <span class="reprint-info"><a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接:</i></span> <span class="reprint-info"><a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-11-21/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">https://kedreamix.github.io/Talk2Paper/Paper/2024-11-21/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明:</i></span> <span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/Talk2Paper/about" target="_blank">Kedreamix</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",(function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})}))</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"><span class="chip bg-color">元宇宙/虚拟人</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" href="/Talk2Paper/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="/Talk2Paper/libs/share/js/social-share.min.js"></script></div></div></div></div></div><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i> &nbsp;上一篇</div><div class="card"><a href="/Talk2Paper/Paper/2024-11-21/Talking%20Head%20Generation/"><div class="card-image"><img src="https://picx.zhimg.com/v2-7ce0c1e947e80cd31a95888c4b28a09d.jpg" class="responsive-img" alt="Talking Head Generation"> <span class="card-title">Talking Head Generation</span></div></a><div class="card-content article-content"><div class="summary block-with-text">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-11-21 JoyVASA Portrait and Animal Image Animation with Diffusion-Based Audio-Driven Facial Dynamics and Head Motion Generation</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i> 2024-11-21</span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">Talking Head Generation</a></span></div></div><div class="card-action article-tags"><a href="/Talk2Paper/tags/Talking-Head-Generation/"><span class="chip bg-color">Talking Head Generation</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/Talk2Paper/Paper/2024-11-17/Diffusion%20Models/"><div class="card-image"><img src="https://picx.zhimg.com/v2-fcc1c8e0deb8684e1e88076a7877a286.jpg" class="responsive-img" alt="Diffusion Models"> <span class="card-title">Diffusion Models</span></div></a><div class="card-content article-content"><div class="summary block-with-text">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-11-17 Golden Noise for Diffusion Models A Learning Framework</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i> 2024-11-17</span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">Diffusion Models</a></span></div></div><div class="card-action article-tags"><a href="/Talk2Paper/tags/Diffusion-Models/"><span class="chip bg-color">Diffusion Models</span></a></div></div></div></div></article></div><script src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script><script src="/Talk2Paper/libs/codeBlock/codeLang.js"></script><script src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script><script src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget card" style="background-color:#fff"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script><script>$((function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=parseInt(.4*$(window).height()-64),e=$(".toc-widget");$(window).scroll((function(){$(window).scrollTop()>t?e.addClass("toc-fixed"):e.removeClass("toc-fixed")}));const o="expanded";let n=$("#toc-aside"),i=$("#main-content");$("#floating-toc-btn .btn-floating").click((function(){n.hasClass(o)?(n.removeClass(o).hide(),i.removeClass("l9")):(n.addClass(o).show(),i.addClass("l9")),function(t,e){let o=$("#"+t);if(0===o.length)return;let n=o.width();n+=n>=450?21:n>=350&&n<450?18:n>=300&&n<350?16:14,$("#"+e).width(n)}("artDetail","prenext-posts")}))}))</script></main><footer class="page-footer bg-color"><link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css"><style>.aplayer .aplayer-lrc p{display:none;font-size:12px;font-weight:700;line-height:16px!important}.aplayer .aplayer-lrc p.aplayer-lrc-current{display:none;font-size:15px;color:#42b983}.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body{left:-66px!important}.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover{left:0!important}</style><div><div class="row"><meting-js class="col l8 offset-l2 m10 offset-m1 s12" server="netease" type="playlist" id="503838841" fixed="true" autoplay theme="#42b983" loop order="random" preload="auto" volume="0.7" list-folded="true"></meting-js></div></div><script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script><script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script><div class="container row center-align" style="margin-bottom:15px!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2024</span> <a href="/Talk2Paper/about" target="_blank">Kedreamix</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">5755.3k</span> <span id="busuanzi_container_site_pv">&nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span></span> <span id="busuanzi_container_site_uv">&nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span></span><br><span id="sitetime">Loading ...</span><script>var calcSiteTime=function(){var e=864e5,t=new Date,n="2024",i=t.getFullYear(),a=t.getMonth()+1,r=t.getDate(),s=t.getHours(),o=t.getMinutes(),g=t.getSeconds(),d=Date.UTC(n,"1","1","0","0","0"),m=Date.UTC(i,a,r,s,o,g)-d,l=Math.floor(m/31536e6),c=Math.floor(m/e-365*l);if(n===String(i)){document.getElementById("year").innerHTML=i;var u="This site has been running for "+c+" days";u="本站已运行 "+c+" 天",document.getElementById("sitetime").innerHTML=u}else{document.getElementById("year").innerHTML=n+" - "+i;var T="This site has been running for "+l+" years and "+c+" days";T="本站已运行 "+l+" 年 "+c+" 天",document.getElementById("sitetime").innerHTML=T}};calcSiteTime()</script><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i></a><a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i></a> <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i> &nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script>$((function(){!function(t,e,r){"use strict";$.ajax({url:t,dataType:"xml",success:function(t){var n=$("entry",t).map((function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}})).get(),a=document.getElementById(e),s=document.getElementById(r);a.addEventListener("input",(function(){var t='<ul class="search-result-list">',e=this.value.trim().toLowerCase().split(/[\s\-]+/);s.innerHTML="",this.value.trim().length<=0||(n.forEach((function(r){var n=!0,a=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),i=r.url;i=0===i.indexOf("/")?r.url:"/"+i;var l=-1,c=-1,u=-1;if(""!==a&&""!==s&&e.forEach((function(t,e){l=a.indexOf(t),c=s.indexOf(t),l<0&&c<0?n=!1:(c<0&&(c=0),0===e&&(u=c))})),n){t+="<li><a href='"+i+"' class='search-result-title'>"+a+"</a>";var o=r.content.trim().replace(/<[^>]+>/g,"");if(u>=0){var h=u-20,f=u+80;h<0&&(h=0),0===h&&(f=100),f>o.length&&(f=o.length);var m=o.substr(h,f);e.forEach((function(t){var e=new RegExp(t,"gi");m=m.replace(e,'<em class="search-keyword">'+t+"</em>")})),t+='<p class="search-result">'+m+"...</p>"}t+="</li>"}})),t+="</ul>",s.innerHTML=t)}))}})}("/Talk2Paper/search.xml","searchInput","searchResult")}))</script><div class="stars-con"><div id="stars"></div><div id="stars2"></div><div id="stars3"></div></div><script>function switchNightMode(){$('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($("body")),setTimeout((function(){$("body").hasClass("DarkMode")?($("body").removeClass("DarkMode"),localStorage.setItem("isDark","0"),$("#sum-moon-icon").removeClass("fa-sun").addClass("fa-moon")):($("body").addClass("DarkMode"),localStorage.setItem("isDark","1"),$("#sum-moon-icon").addClass("fa-sun").removeClass("fa-moon")),setTimeout((function(){$(".Cuteen_DarkSky").fadeOut(1e3,(function(){$(this).remove()}))}),2e3)}))}</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-arrow-up"></i></a></div><script src="/Talk2Paper/libs/materialize/materialize.min.js"></script><script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script><script src="/Talk2Paper/libs/aos/aos.js"></script><script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script><script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="/Talk2Paper/js/matery.js"></script><script>var windowWidth=$(window).width();windowWidth>768&&document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>')</script><script src="https://ssl.captcha.qq.com/TCaptcha.js"></script><script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script><button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/Talk2Paper/libs/others/clicklove.js" async></script><script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script><script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script><script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script><style>[bg-lazy]{background-image:none!important;background-color:#eee!important}</style><script>window.imageLazyLoadSetting={isSPA:!1,preloadRatio:3,processImages:null}</script><script>window.addEventListener("load",(function(){var a=/\.(gif|jpg|jpeg|tiff|png)$/i,e=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach((function(t){var r=t.parentNode;"A"===r.tagName&&(a.test(r.href)||e.test(r.href))&&(r.href=t.dataset.original)}))}))</script><script>(t=>{t.imageLazyLoadSetting.processImages=n;var e=t.imageLazyLoadSetting.isSPA,a=t.imageLazyLoadSetting.preloadRatio||1,o=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function n(n){(e||n)&&(o=i());for(var r,d=0;d<o.length;d++)0<=(r=(r=o[d]).getBoundingClientRect()).bottom&&0<=r.left&&r.top<=(t.innerHeight*a||document.documentElement.clientHeight*a)&&(()=>{var e,a,i,n,r=o[d];a=function(){o=o.filter((function(t){return r!==t})),t.imageLazyLoadSetting.onImageLoaded&&t.imageLazyLoadSetting.onImageLoaded(r)},(e=r).dataset.loaded||(e.hasAttribute("bg-lazy")?(e.removeAttribute("bg-lazy"),a&&a()):(i=new Image,n=e.getAttribute("data-original"),i.onload=function(){e.src=n,e.removeAttribute("data-original"),e.setAttribute("data-loaded",!0),a&&a()},i.onerror=function(){e.removeAttribute("data-original"),e.setAttribute("data-loaded",!1),e.src=n},e.src!==n&&(i.src=n)))})()}function r(){clearTimeout(n.tId),n.tId=setTimeout(n,500)}n(),document.addEventListener("scroll",r),t.addEventListener("resize",r),t.addEventListener("orientationchange",r)})(this)</script></body></html><script>var st,OriginTitile=document.title;document.addEventListener("visibilitychange",(function(){document.hidden?(document.title="Σ(っ °Д °;)っ诶，页面崩溃了嘛？",clearTimeout(st)):(document.title="φ(゜▽゜*)♪咦，又好了！",st=setTimeout((function(){document.title=OriginTitile}),3e3))}))</script>