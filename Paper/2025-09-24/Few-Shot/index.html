<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-24  TempSamp-R1 Effective Temporal Sampling with Reinforcement Fine-Tuning   for Video LLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b8ddc625a6cbc42293c27a849cac472d')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-15
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-24-æ›´æ–°"><a href="#2025-09-24-æ›´æ–°" class="headerlink" title="2025-09-24 æ›´æ–°"></a>2025-09-24 æ›´æ–°</h1><h2 id="TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs"><a href="#TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs" class="headerlink" title="TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning   for Video LLMs"></a>TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning   for Video LLMs</h2><p><strong>Authors:Yunheng Li, Jing Cheng, Shaoyong Jia, Hangyi Kuang, Shaohui Jiao, Qibin Hou, Ming-Ming Cheng</strong></p>
<p>This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework designed to improve the effectiveness of adapting multimodal large language models (MLLMs) to video temporal grounding tasks. We reveal that existing reinforcement learning methods, such as Group Relative Policy Optimization (GRPO), rely on on-policy sampling for policy updates. However, in tasks with large temporal search spaces, this strategy becomes both inefficient and limited in performance, as it often fails to identify temporally accurate solutions. To address this limitation, TempSamp-R1 leverages ground-truth annotations as off-policy supervision to provide temporally precise guidance, effectively compensating for the sparsity and misalignment in on-policy solutions. To further stabilize training and reduce variance in reward-based updates, TempSamp-R1 provides a non-linear soft advantage computation method that dynamically reshapes the reward feedback via an asymmetric transformation. By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1 optimizes a single unified model to support both CoT and non-CoT inference modes, enabling efficient handling of queries with varying reasoning complexity. Experimental results demonstrate that TempSamp-R1 outperforms GRPO-based baselines, establishing new state-of-the-art performance on benchmark datasets: Charades-STA (<a href="mailto:&#82;&#x31;&#64;&#48;&#x2e;&#x37;">&#82;&#x31;&#64;&#48;&#x2e;&#x37;</a>: 52.9%, +2.7%), ActivityNet Captions (<a href="mailto:&#x52;&#x31;&#x40;&#x30;&#x2e;&#x35;">&#x52;&#x31;&#x40;&#x30;&#x2e;&#x35;</a>: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover, TempSamp-R1 shows robust few-shot generalization capabilities under limited data. Code: <a target="_blank" rel="noopener" href="https://github.com/HVision-NKU/TempSamp-R1">https://github.com/HVision-NKU/TempSamp-R1</a> </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†TempSamp-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€‚åº”è§†é¢‘æ—¶åºå®šä½ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å‘ç°ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå¦‚ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä¾èµ–äºç­–ç•¥æ›´æ–°æ—¶çš„åœ¨ç­–ç•¥é‡‡æ ·ã€‚ç„¶è€Œï¼Œåœ¨å…·æœ‰å¤§æ—¶é—´æœç´¢ç©ºé—´çš„ä»»åŠ¡ä¸­ï¼Œç”±äºè¿™ç§æ–¹æ³•å¸¸å¸¸æ— æ³•æ‰¾åˆ°æ—¶åºç²¾ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œæ‰€ä»¥å®ƒçš„æ•ˆç‡å’Œæ€§èƒ½å—é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼ŒTempSamp-R1åˆ©ç”¨çœŸå®æ ‡ç­¾ä½œä¸ºç¦»çº¿ç›‘ç£ï¼Œæä¾›ç²¾ç¡®çš„æ—¶é—´æŒ‡å¯¼ï¼Œæœ‰æ•ˆåœ°å¼¥è¡¥äº†åœ¨çº¿è§£å†³æ–¹æ¡ˆä¸­çš„ç¨€ç–æ€§å’Œä¸åŒ¹é…é—®é¢˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–åŸºäºå¥–åŠ±çš„æ›´æ–°å¹¶å‡å°‘æ–¹å·®ï¼ŒTempSamp-R1æä¾›äº†ä¸€ç§éçº¿æ€§è½¯ä¼˜åŠ¿è®¡ç®—æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¸å¯¹ç§°è½¬æ¢åŠ¨æ€é‡å¡‘å¥–åŠ±åé¦ˆã€‚é€šè¿‡é‡‡ç”¨æ··åˆå¼çš„æ€ç»´é“¾ï¼ˆCoTï¼‰è®­ç»ƒèŒƒå¼ï¼ŒTempSamp-R1ä¼˜åŒ–äº†ä¸€ä¸ªå•ä¸€ç»Ÿä¸€çš„æ¨¡å‹æ¥æ”¯æŒCoTå’ŒéCoTæ¨ç†æ¨¡å¼ï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†ä¸åŒæ¨ç†å¤æ‚åº¦çš„æŸ¥è¯¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTempSamp-R1ä¼˜äºåŸºäºGRPOçš„åŸºçº¿æ¨¡å‹ï¼Œåœ¨åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½ï¼šCharades-STAï¼ˆ<a href="mailto:&#82;&#49;&#64;&#48;&#x2e;&#x37;">&#82;&#49;&#64;&#48;&#x2e;&#x37;</a>: 52.9%ï¼Œ+2.7%ï¼‰ï¼ŒActivityNet Captionsï¼ˆ<a href="mailto:&#82;&#x31;&#x40;&#x30;&#46;&#x35;">&#82;&#x31;&#x40;&#x30;&#46;&#x35;</a>: 56.0%ï¼Œ+5.3%ï¼‰å’ŒQVHighlightsï¼ˆmAP: 30.0%ï¼Œ+3.0%ï¼‰ã€‚æ­¤å¤–ï¼ŒTempSamp-R1åœ¨æœ‰é™æ•°æ®ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/HVision-NKU/TempSamp-R1">https://github.com/HVision-NKU/TempSamp-R1</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18056v1">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºTempSamp-R1çš„æ–°å‹å¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œç”¨äºæ”¹è¿›å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æ—¶åºå®šä½ä»»åŠ¡ä¸­çš„é€‚åº”æ€§ã€‚é’ˆå¯¹ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤§å‹æ—¶åºæœç´¢ç©ºé—´ä¸­çš„æ•ˆç‡å’Œæ€§èƒ½å±€é™ï¼ŒTempSamp-R1åˆ©ç”¨çœŸå®æ ‡æ³¨ä½œä¸ºç¦»çº¿ç­–ç•¥ç›‘ç£ï¼Œæä¾›ç²¾ç¡®çš„æ—¶é—´æŒ‡å¯¼ï¼Œå¹¶å¼•å…¥éçº¿æ€§è½¯ä¼˜åŠ¿è®¡ç®—æ–¹æ³•ä»¥ç¨³å®šè®­ç»ƒå’Œå‡å°‘åŸºäºå¥–åŠ±çš„æ›´æ–°çš„æ–¹å·®ã€‚ç»“åˆChain-of-Thoughtè®­ç»ƒèŒƒå¼ï¼ŒTempSamp-R1ä¼˜åŒ–äº†å•ä¸€è”åˆæ¨¡å‹ï¼Œæ”¯æŒCoTå’ŒéCoTæ¨ç†æ¨¡å¼ï¼Œå®ç°äº†é«˜æ•ˆæŸ¥è¯¢å¤„ç†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTempSamp-R1åœ¨Charades-STAã€ActivityNet Captionså’ŒQVHighlightsç­‰åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†å¼ºå¤§çš„å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TempSamp-R1æ˜¯ä¸€ä¸ªå¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œç”¨äºæ”¹è¿›å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æ—¶åºå®šä½ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤§å‹æ—¶åºæœç´¢ç©ºé—´ä¸­å­˜åœ¨æ•ˆç‡å’Œæ€§èƒ½å±€é™ã€‚</li>
<li>TempSamp-R1åˆ©ç”¨çœŸå®æ ‡æ³¨ä½œä¸ºç¦»çº¿ç­–ç•¥ç›‘ç£ï¼Œæä¾›ç²¾ç¡®çš„æ—¶é—´æŒ‡å¯¼ã€‚</li>
<li>é€šè¿‡éçº¿æ€§è½¯ä¼˜åŠ¿è®¡ç®—æ–¹æ³•ï¼ŒTempSamp-R1ç¨³å®šè®­ç»ƒå¹¶å‡å°‘å¥–åŠ±æ›´æ–°çš„æ–¹å·®ã€‚</li>
<li>TempSamp-R1é‡‡ç”¨Chain-of-Thoughtè®­ç»ƒèŒƒå¼ï¼Œæ”¯æŒå¤šç§æ¨ç†æ¨¡å¼ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒTempSamp-R1åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
<li>TempSamp-R1å±•ç°å‡ºå¼ºå¤§çš„å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18056">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-585eeb01e1b5036bc615aaedb3aea715" align="middle">
<img src="https://picx.zhimg.com/v2-a02c9f82f60723021b14d04507190943" align="middle">
<img src="https://picx.zhimg.com/v2-a763b12df7eed6a48c0395e058a35dec" align="middle">
<img src="https://picx.zhimg.com/v2-f63fbf7d12ced746bb82be62d064abd6" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Dual-View-Alignment-Learning-with-Hierarchical-Prompt-for-Class-Imbalance-Multi-Label-Classification"><a href="#Dual-View-Alignment-Learning-with-Hierarchical-Prompt-for-Class-Imbalance-Multi-Label-Classification" class="headerlink" title="Dual-View Alignment Learning with Hierarchical-Prompt for   Class-Imbalance Multi-Label Classification"></a>Dual-View Alignment Learning with Hierarchical-Prompt for   Class-Imbalance Multi-Label Classification</h2><p><strong>Authors:Sheng Huang, Jiexuan Yan, Beiyan Liu, Bo Liu, Richang Hong</strong></p>
<p>Real-world datasets often exhibit class imbalance across multiple categories, manifesting as long-tailed distributions and few-shot scenarios. This is especially challenging in Class-Imbalanced Multi-Label Image Classification (CI-MLIC) tasks, where data imbalance and multi-object recognition present significant obstacles. To address these challenges, we propose a novel method termed Dual-View Alignment Learning with Hierarchical Prompt (HP-DVAL), which leverages multi-modal knowledge from vision-language pretrained (VLP) models to mitigate the class-imbalance problem in multi-label settings. Specifically, HP-DVAL employs dual-view alignment learning to transfer the powerful feature representation capabilities from VLP models by extracting complementary features for accurate image-text alignment. To better adapt VLP models for CI-MLIC tasks, we introduce a hierarchical prompt-tuning strategy that utilizes global and local prompts to learn task-specific and context-related prior knowledge. Additionally, we design a semantic consistency loss during prompt tuning to prevent learned prompts from deviating from general knowledge embedded in VLP models. The effectiveness of our approach is validated on two CI-MLIC benchmarks: MS-COCO and VOC2007. Extensive experimental results demonstrate the superiority of our method over SOTA approaches, achieving mAP improvements of 10.0% and 5.2% on the long-tailed multi-label image classification task, and 6.8% and 2.9% on the multi-label few-shot image classification task. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œçš„æ•°æ®é›†é€šå¸¸åœ¨å¤šä¸ªç±»åˆ«ä¹‹é—´è¡¨ç°å‡ºç±»åˆ«ä¸å¹³è¡¡ï¼Œè¡¨ç°ä¸ºé•¿å°¾åˆ†å¸ƒå’Œå°‘é‡åœºæ™¯ã€‚è¿™åœ¨ç±»åˆ«ä¸å¹³è¡¡çš„å¤šæ ‡ç­¾å›¾åƒåˆ†ç±»ï¼ˆCI-MLICï¼‰ä»»åŠ¡ä¸­å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œæ•°æ®ä¸å¹³è¡¡å’Œå¤šç›®æ ‡è¯†åˆ«æ„æˆäº†é‡å¤§éšœç¢ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºå¸¦æœ‰å±‚æ¬¡æç¤ºçš„åŒè§†å›¾å¯¹é½å­¦ä¹ ï¼ˆHP-DVALï¼‰çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰æ¨¡å‹çš„è·¨æ¨¡æ€çŸ¥è¯†æ¥ç¼“è§£å¤šæ ‡ç­¾è®¾ç½®ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼ŒHP-DVALé‡‡ç”¨åŒè§†å›¾å¯¹é½å­¦ä¹ ï¼Œé€šè¿‡æå–å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„äº’è¡¥ç‰¹å¾æ¥è½¬ç§»VLPæ¨¡å‹çš„å¼ºå¤§ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ï¼Œä»¥å®ç°å‡†ç¡®çš„å›¾åƒæ–‡æœ¬å¯¹é½ã€‚ä¸ºäº†æ›´å¥½åœ°é€‚åº”CI-MLICä»»åŠ¡çš„VLPæ¨¡å‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å±‚æ¬¡åŒ–çš„æç¤ºè°ƒæ•´ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åˆ©ç”¨å…¨å±€å’Œå±€éƒ¨æç¤ºæ¥å­¦ä¹ ç‰¹å®šä»»åŠ¡å’Œä¸Šä¸‹æ–‡ç›¸å…³çš„å…ˆéªŒçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æç¤ºè°ƒæ•´è¿‡ç¨‹ä¸­è®¾è®¡äº†ä¸€ç§è¯­ä¹‰ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥é˜²æ­¢å­¦ä¹ åˆ°çš„æç¤ºåç¦»VLPæ¨¡å‹ä¸­åµŒå…¥çš„é€šç”¨çŸ¥è¯†ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨MS-COCOå’ŒVOC2007ä¸¤ä¸ªCI-MLICåŸºå‡†æµ‹è¯•ä¸Šçš„æœ‰æ•ˆæ€§å¾—åˆ°äº†éªŒè¯ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é•¿å°¾å¤šæ ‡ç­¾å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šè¾ƒSOTAæ–¹æ³•å–å¾—äº†mAPæé«˜10.0%å’Œ5.2%çš„ä¼˜å¼‚è¡¨ç°ï¼Œåœ¨å¤šæ ‡ç­¾å°‘é•œå¤´å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šåˆ†åˆ«æé«˜äº†6.8%å’Œ2.9%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17747v1">PDF</a> accepted by IEEE Transactions on Image Processing</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹ç°å®ä¸–ç•Œæ•°æ®é›†ä¸­å¸¸è§çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ ‡ç­¾å›¾åƒåˆ†ç±»ä¸­çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”åŸºäºå±‚æ¬¡æç¤ºçš„åŒè§†å¯¹é½å­¦ä¹ ï¼ˆHP-DVALï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰æ¨¡å‹çš„å¤šæ¨¡æ€çŸ¥è¯†ï¼Œé€šè¿‡åŒè§†å¯¹é½å­¦ä¹ æŠ€æœ¯æå–äº’è¡¥ç‰¹å¾ï¼Œå®ç°å›¾åƒä¸æ–‡æœ¬çš„å‡†ç¡®å¯¹é½ï¼Œä»¥ç¼“è§£ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚åŒæ—¶ï¼Œå¼•å…¥å±‚æ¬¡æç¤ºè°ƒæ•´ç­–ç•¥ï¼Œåˆ©ç”¨å…¨å±€å’Œå±€éƒ¨æç¤ºæ¥å­¦ä¹ å’Œé€‚åº”ç‰¹å®šä»»åŠ¡å’Œä¸Šä¸‹æ–‡ç›¸å…³çš„å…ˆéªŒçŸ¥è¯†ã€‚åœ¨MS-COCOå’ŒVOC2007ä¸¤ä¸ªå¤šæ ‡ç­¾å›¾åƒåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºç°æœ‰æ–¹æ³•è¡¨ç°ä¼˜è¶Šï¼Œåœ¨é•¿å°¾å¤šæ ‡ç­¾å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„mAPæå‡10.0%å’Œ5.2%ï¼Œåœ¨å¤šæ ‡ç­¾å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„mAPæå‡6.8%å’Œ2.9%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« ä¸»è¦è§£å†³ç°å®ä¸–ç•Œæ•°æ®é›†ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ ‡ç­¾å›¾åƒåˆ†ç±»ä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”HP-DVALï¼Œè¯¥æ–¹æ³•ç»“åˆè§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼Œé€šè¿‡åŒè§†å¯¹é½å­¦ä¹ å®ç°å›¾åƒä¸æ–‡æœ¬çš„å‡†ç¡®å¯¹é½ã€‚</li>
<li>HP-DVALåˆ©ç”¨å±‚æ¬¡æç¤ºè°ƒæ•´ç­–ç•¥æ¥é€‚åº”ç‰¹å®šä»»åŠ¡å’Œä¸Šä¸‹æ–‡ç›¸å…³çš„å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨MS-COCOå’ŒVOC2007åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li>
<li>HP-DVALåœ¨é•¿å°¾å¤šæ ‡ç­¾å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„mAPæœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šæ ‡ç­¾å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17747">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c268144593345a945c8035686eb0f32" align="middle">
<img src="https://picx.zhimg.com/v2-db4b1a44c8612ffca7cad4e4dd594d70" align="middle">
<img src="https://picx.zhimg.com/v2-b8ddc625a6cbc42293c27a849cac472d" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-2-M-2-Net-Adaptively-Aligned-Multi-Scale-Moment-for-Few-Shot-Action-Recognition"><a href="#A-2-M-2-Net-Adaptively-Aligned-Multi-Scale-Moment-for-Few-Shot-Action-Recognition" class="headerlink" title="A$^2$M$^2$-Net: Adaptively Aligned Multi-Scale Moment for Few-Shot   Action Recognition"></a>A$^2$M$^2$-Net: Adaptively Aligned Multi-Scale Moment for Few-Shot   Action Recognition</h2><p><strong>Authors:Zilin Gao, Qilong Wang, Bingbing Zhang, Qinghua Hu, Peihua Li</strong></p>
<p>Thanks to capability to alleviate the cost of large-scale annotation, few-shot action recognition (FSAR) has attracted increased attention of researchers in recent years. Existing FSAR approaches typically neglect the role of individual motion pattern in comparison, and under-explore the feature statistics for video dynamics. Thereby, they struggle to handle the challenging temporal misalignment in video dynamics, particularly by using 2D backbones. To overcome these limitations, this work proposes an adaptively aligned multi-scale second-order moment network, namely A$^2$M$^2$-Net, to describe the latent video dynamics with a collection of powerful representation candidates and adaptively align them in an instance-guided manner. To this end, our A$^2$M$^2$-Net involves two core components, namely, adaptive alignment (A$^2$ module) for matching, and multi-scale second-order moment (M$^2$ block) for strong representation. Specifically, M$^2$ block develops a collection of semantic second-order descriptors at multiple spatio-temporal scales. Furthermore, A$^2$ module aims to adaptively select informative candidate descriptors while considering the individual motion pattern. By such means, our A$^2$M$^2$-Net is able to handle the challenging temporal misalignment problem by establishing an adaptive alignment protocol for strong representation. Notably, our proposed method generalizes well to various few-shot settings and diverse metrics. The experiments are conducted on five widely used FSAR benchmarks, and the results show our A$^2$M$^2$-Net achieves very competitive performance compared to state-of-the-arts, demonstrating its effectiveness and generalization. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œç”±äºèƒ½å¤Ÿå‡è½»å¤§è§„æ¨¡æ ‡æ³¨çš„æˆæœ¬ï¼Œå°æ ·æœ¬åŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰å·²ç»å¼•èµ·äº†ç ”ç©¶è€…ä»¬çš„å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„FSARæ–¹æ³•é€šå¸¸å¿½ç•¥äº†ä¸ªä½“è¿åŠ¨æ¨¡å¼çš„ä½œç”¨ï¼Œå¹¶ä¸”æ²¡æœ‰å……åˆ†æ¢ç´¢è§†é¢‘åŠ¨æ€çš„ç‰¹å¾ç»Ÿè®¡ã€‚å› æ­¤ï¼Œå®ƒä»¬åœ¨å¤„ç†è§†é¢‘åŠ¨æ€ä¸­çš„æŒ‘æˆ˜æ€§æ—¶é—´é”™ä½é—®é¢˜æ—¶é‡åˆ°äº†å›°éš¾ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨2Déª¨å¹²ç½‘æ—¶ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†è‡ªé€‚åº”å¯¹é½å¤šå°ºåº¦äºŒé˜¶çŸ©ç½‘ç»œï¼Œå³A$^2$M$^2$-Netï¼Œç”¨äºæè¿°æ½œåœ¨çš„è§†é¢‘åŠ¨æ€ï¼Œå¹¶æ”¶é›†ä¸€ç³»åˆ—å¼ºå¤§çš„è¡¨ç¤ºå€™é€‰è€…ï¼Œä»¥å®ä¾‹å¼•å¯¼çš„æ–¹å¼è‡ªé€‚åº”åœ°å¯¹é½å®ƒä»¬ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬çš„A$^2$M$^2$-NetåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œå³ç”¨äºåŒ¹é…çš„è‡ªé€‚åº”å¯¹é½ï¼ˆA$^2$æ¨¡å—ï¼‰å’Œç”¨äºå¼ºå¤§è¡¨ç¤ºçš„å¤šå°ºåº¦äºŒé˜¶çŸ©ï¼ˆM$^2$å—ï¼‰ã€‚å…·ä½“è€Œè¨€ï¼ŒM$^2$å—åœ¨å¤šä¸ªæ—¶ç©ºå°ºåº¦ä¸Šå¼€å‘äº†ä¸€ç³»åˆ—è¯­ä¹‰äºŒé˜¶æè¿°ç¬¦ã€‚æ­¤å¤–ï¼ŒA$^2$æ¨¡å—æ—¨åœ¨è‡ªé€‚åº”åœ°é€‰æ‹©ä¿¡æ¯å€™é€‰æè¿°ç¬¦ï¼ŒåŒæ—¶è€ƒè™‘ä¸ªä½“è¿åŠ¨æ¨¡å¼ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬çš„A$^2$M$^2$-Netèƒ½å¤Ÿé€šè¿‡ä¸ºå¼ºå¤§è¡¨ç¤ºå»ºç«‹è‡ªé€‚åº”å¯¹é½åè®®ï¼Œè§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„æ—¶é—´é”™ä½é—®é¢˜ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å„ç§å°æ ·æœ¬è®¾ç½®å’Œå¤šæ ·åŒ–æŒ‡æ ‡ä¸Šå…·æœ‰å¾ˆå¥½çš„é€šç”¨æ€§ã€‚å®éªŒæ˜¯åœ¨äº”ä¸ªå¹¿æ³›ä½¿ç”¨çš„FSARåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„ï¼Œç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„A$^2$M$^2$-Netä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”å–å¾—äº†éå¸¸æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17638v1">PDF</a> 27 pages, 13 figures, 7 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºA$^2$M$^2$-Netçš„é€‚åº”å¯¹é½å¤šå°ºåº¦äºŒé˜¶çŸ©ç½‘ç»œï¼Œç”¨äºæè¿°æ½œåœ¨çš„è§†é¢‘åŠ¨æ€ï¼Œå¹¶é€šè¿‡å®ä¾‹å¼•å¯¼çš„æ–¹å¼è‡ªé€‚åº”å¯¹é½ã€‚è¯¥ç½‘ç»œåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šç”¨äºåŒ¹é…çš„é€‚åº”æ€§å¯¹é½ï¼ˆA$^2$æ¨¡å—ï¼‰å’Œç”¨äºå¼ºè¡¨å¾çš„å¤šå°ºåº¦äºŒé˜¶çŸ©ï¼ˆM$^2$å—ï¼‰ã€‚M$^2$å—åœ¨å¤šä¸ªæ—¶ç©ºå°ºåº¦ä¸Šç”Ÿæˆè¯­ä¹‰äºŒé˜¶æè¿°ç¬¦ï¼Œè€ŒA$^2$æ¨¡å—åˆ™æ—¨åœ¨è‡ªé€‚åº”é€‰æ‹©ä¿¡æ¯å€™é€‰æè¿°ç¬¦ï¼ŒåŒæ—¶è€ƒè™‘ä¸ªä½“è¿åŠ¨æ¨¡å¼ã€‚å› æ­¤ï¼ŒA$^2$M$^2$-Netèƒ½å¤Ÿå¤„ç†å…·æœ‰æŒ‘æˆ˜æ€§çš„æ—¶é—´é”™ä½é—®é¢˜ï¼Œä¸ºå¼ºè¡¨å¾å»ºç«‹è‡ªé€‚åº”å¯¹é½åè®®ã€‚åœ¨äº”ä¸ªå¹¿æ³›ä½¿ç”¨çš„FSARåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒA$^2$M$^2$-Netä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”å–å¾—äº†æå…·ç«äº‰åŠ›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FSARï¼ˆFew-Shot Action Recognitionï¼‰è¿‘å¹´æ¥å—åˆ°ç ”ç©¶è€…çš„å…³æ³¨ï¼Œå› ä¸ºå®ƒèƒ½å‡è½»å¤§è§„æ¨¡æ ‡æ³¨çš„æˆæœ¬å‹åŠ›ã€‚</li>
<li>ç°æœ‰FSARæ–¹æ³•é€šå¸¸å¿½ç•¥ä¸ªä½“è¿åŠ¨æ¨¡å¼çš„ä½œç”¨ï¼Œå¹¶ä¸”åœ¨è§†é¢‘åŠ¨æ€çš„ç‰¹å¾ç»Ÿè®¡æ–¹é¢çš„æ¢ç´¢ä¸è¶³ã€‚</li>
<li>A$^2$M$^2$-Netç½‘ç»œè¢«æå‡ºï¼Œä»¥è§£å†³ç°æœ‰FSARæ–¹æ³•çš„å±€é™æ€§ã€‚å®ƒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šé€‚åº”æ€§å¯¹é½ï¼ˆA$^2$æ¨¡å—ï¼‰å’Œå¤šå°ºåº¦äºŒé˜¶çŸ©ï¼ˆM$^2$å—ï¼‰ã€‚</li>
<li>M$^2$å—åœ¨å¤šä¸ªæ—¶ç©ºå°ºåº¦ä¸Šç”Ÿæˆè¯­ä¹‰äºŒé˜¶æè¿°ç¬¦ï¼Œä»¥æè¿°è§†é¢‘åŠ¨æ€ã€‚</li>
<li>A$^2$æ¨¡å—èƒ½å¤Ÿè‡ªé€‚åº”é€‰æ‹©ä¿¡æ¯å€™é€‰æè¿°ç¬¦ï¼ŒåŒæ—¶è€ƒè™‘ä¸ªä½“è¿åŠ¨æ¨¡å¼ã€‚</li>
<li>A$^2$M$^2$-Netèƒ½å¤Ÿå¤„ç†æ—¶é—´é”™ä½é—®é¢˜ï¼Œå¹¶é€šè¿‡è‡ªé€‚åº”å¯¹é½åè®®å®ç°å¼ºè¡¨å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17638">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7f067e39c9450c1432f734c9b586587e" align="middle">
<img src="https://picx.zhimg.com/v2-acba6bde79820442153fd248c81f7b6c" align="middle">
<img src="https://picx.zhimg.com/v2-00457388859f525c78499e2e18b74314" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="From-Benchmarks-to-Reality-Advancing-Visual-Anomaly-Detection-by-the-VAND-3-0-Challenge"><a href="#From-Benchmarks-to-Reality-Advancing-Visual-Anomaly-Detection-by-the-VAND-3-0-Challenge" class="headerlink" title="From Benchmarks to Reality: Advancing Visual Anomaly Detection by the   VAND 3.0 Challenge"></a>From Benchmarks to Reality: Advancing Visual Anomaly Detection by the   VAND 3.0 Challenge</h2><p><strong>Authors:Lars Heckler-Kram, Ashwin Vaidya, Jan-Hendrik Neudeck, Ulla Scheler, Dick Ameln, Samet Akcay, Paula Ramos</strong></p>
<p>Visual anomaly detection is a strongly application-driven field of research. Consequently, the connection between academia and industry is of paramount importance. In this regard, we present the VAND 3.0 Challenge to showcase current progress in anomaly detection across different practical settings whilst addressing critical issues in the field. The challenge hosted two tracks, fostering the development of anomaly detection methods robust against real-world distribution shifts (Category 1) and exploring the capabilities of Vision Language Models within the few-shot regime (Category 2), respectively. The participantsâ€™ solutions reached significant improvements over previous baselines by combining or adapting existing approaches and fusing them with novel pipelines. While for both tracks the progress in large pre-trained vision (language) backbones played a pivotal role for the performance increase, scaling up anomaly detection methods more efficiently needs to be addressed by future research to meet real-time and computational constraints on-site. </p>
<blockquote>
<p>è§†è§‰å¼‚å¸¸æ£€æµ‹æ˜¯ä¸€ä¸ªåº”ç”¨é©±åŠ¨æ€§å¾ˆå¼ºçš„ç ”ç©¶é¢†åŸŸã€‚å› æ­¤ï¼Œå­¦æœ¯ç•Œä¸å·¥ä¸šç•Œä¹‹é—´çš„è”ç³»è‡³å…³é‡è¦ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºVAND 3.0æŒ‘æˆ˜èµ›ï¼Œæ—¨åœ¨å±•ç¤ºä¸åŒå®é™…åº”ç”¨åœºæ™¯ä¸­å¼‚å¸¸æ£€æµ‹çš„å½“å‰è¿›å±•ï¼ŒåŒæ—¶è§£å†³è¯¥é¢†åŸŸçš„å…³é”®é—®é¢˜ã€‚è¯¥æŒ‘æˆ˜èµ›åŒ…å«ä¸¤ä¸ªèµ›é“ï¼Œä¸€ä¸ªèµ›é“æ˜¯åŸ¹è‚²é’ˆå¯¹ç°å®ä¸–ç•Œåˆ†å¸ƒå˜åŒ–å…·æœ‰é²æ£’æ€§çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼ˆç¬¬ä¸€ç±»ï¼‰ï¼Œå¦ä¸€ä¸ªèµ›é“æ˜¯æ¢ç´¢å°‘æ ·æœ¬ä½“åˆ¶ä¸‹è§†è§‰è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼ˆç¬¬äºŒç±»ï¼‰ã€‚å‚èµ›è€…çš„è§£å†³æ–¹æ¡ˆé€šè¿‡ç»“åˆæˆ–é€‚åº”ç°æœ‰æ–¹æ³•å¹¶å°†å…¶ä¸æ–°å‹ç®¡é“èåˆï¼Œåœ¨åŸºçº¿ä¹‹ä¸Šå–å¾—äº†é‡å¤§æ”¹è¿›ã€‚å¯¹äºè¿™ä¸¤ä¸ªèµ›é“æ¥è¯´ï¼Œå¤§å‹é¢„è®­ç»ƒè§†è§‰ï¼ˆè¯­è¨€ï¼‰ä¸»å¹²çš„è¿›å±•å¯¹æ€§èƒ½æå‡èµ·åˆ°äº†è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ä¸ºäº†åœ¨ç°åœºæ»¡è¶³å®æ—¶å’Œè®¡ç®—çº¦æŸæ¡ä»¶ï¼Œæœªæ¥ç ”ç©¶è¿˜éœ€è¦æ›´æœ‰æ•ˆåœ°æé«˜å¼‚å¸¸æ£€æµ‹æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17615v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è§†è§‰å¼‚å¸¸æ£€æµ‹æ˜¯ä¸€ä¸ªåº”ç”¨é©±åŠ¨çš„ç ”ç©¶é¢†åŸŸï¼Œå­¦æœ¯ç•Œä¸å·¥ä¸šç•Œçš„è”ç³»è‡³å…³é‡è¦ã€‚ä¸ºå±•ç¤ºå½“å‰å¼‚å¸¸æ£€æµ‹åœ¨ä¸åŒå®é™…åœºæ™¯ä¸­çš„è¿›å±•å¹¶è§£å†³é¢†åŸŸä¸­çš„å…³é”®é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VAND 3.0æŒ‘æˆ˜èµ›ã€‚è¯¥æŒ‘æˆ˜åŒ…å«ä¸¤ä¸ªèµ›é“ï¼Œåˆ†åˆ«å…³æ³¨é’ˆå¯¹ç°å®ä¸–ç•Œåˆ†å¸ƒå˜åŒ–çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•çš„ç¨³å¥æ€§ï¼ˆèµ›é“ä¸€ï¼‰ä»¥åŠåœ¨å°‘é•œå¤´ä½“åˆ¶ä¸‹æ¢ç´¢è§†è§‰è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼ˆèµ›é“äºŒï¼‰ã€‚å‚èµ›è€…çš„è§£å†³æ–¹æ¡ˆé€šè¿‡ç»“åˆæˆ–é€‚åº”ç°æœ‰æ–¹æ³•å¹¶èåˆæ–°ç®¡é“ï¼Œæ˜¾è‘—æ”¹è¿›äº†ä¹‹å‰çš„åŸºçº¿ã€‚è™½ç„¶å¯¹äºä¸¤ä¸ªèµ›é“æ¥è¯´ï¼Œå¤§å‹é¢„è®­ç»ƒè§†è§‰ï¼ˆè¯­è¨€ï¼‰ä¸»å¹²çš„è¿›å±•å¯¹äºæ€§èƒ½æå‡èµ·åˆ°äº†å…³é”®ä½œç”¨ï¼Œä½†æœªæ¥ç ”ç©¶éœ€è¦æ›´é«˜æ•ˆåœ°æ‰©å±•å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œä»¥æ»¡è¶³ç°åœºå®æ—¶å’Œè®¡ç®—çº¦æŸçš„è¦æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰å¼‚å¸¸æ£€æµ‹æ˜¯ä¸€ä¸ªåº”ç”¨é©±åŠ¨çš„ç ”ç©¶é¢†åŸŸï¼Œéœ€è¦å­¦æœ¯ç•Œä¸å·¥ä¸šç•Œçš„ç´§å¯†åˆä½œã€‚</li>
<li>VAND 3.0æŒ‘æˆ˜èµ›æ—¨åœ¨å±•ç¤ºå¼‚å¸¸æ£€æµ‹æŠ€æœ¯çš„æœ€æ–°è¿›å±•ï¼Œå¹¶è§£å†³é¢†åŸŸä¸­çš„å…³é”®é—®é¢˜ã€‚</li>
<li>æŒ‘æˆ˜åŒ…æ‹¬ä¸¤ä¸ªèµ›é“ï¼Œåˆ†åˆ«å…³æ³¨å¼‚å¸¸æ£€æµ‹æ–¹æ³•çš„ç¨³å¥æ€§å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å°‘é•œå¤´ä½“åˆ¶ä¸‹çš„èƒ½åŠ›ã€‚</li>
<li>å‚èµ›è€…é€šè¿‡ç»“åˆæˆ–é€‚åº”ç°æœ‰æ–¹æ³•ä¸æ–°å‹ç®¡é“ï¼Œæ˜¾è‘—æ”¹è¿›äº†åŸºçº¿è¡¨ç°ã€‚</li>
<li>å¤§å‹é¢„è®­ç»ƒè§†è§‰ï¼ˆè¯­è¨€ï¼‰ä¸»å¹²çš„è¿›å±•å¯¹æ€§èƒ½æå‡èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>æœªæ¥ç ”ç©¶éœ€è¦æ›´é«˜æ•ˆåœ°æ‰©å±•å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œä»¥æ»¡è¶³å®æ—¶å’Œè®¡ç®—çº¦æŸçš„è¦æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17615">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4f3469729a62c72343d8b274c70f6dd0" align="middle">
<img src="https://picx.zhimg.com/v2-81debfd22b656dac28017ef13059087a" align="middle">
<img src="https://picx.zhimg.com/v2-c943d23fa0dda7b7a847c6c9c33e7a89" align="middle">
<img src="https://picx.zhimg.com/v2-2fb0a085fc7736a757a1557e45f2a544" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Reason-Over-Non-Text-Modalities-in-a-Training-Free-Manner-A-Case-Study-with-In-Context-Representation-Learning"><a href="#Can-LLMs-Reason-Over-Non-Text-Modalities-in-a-Training-Free-Manner-A-Case-Study-with-In-Context-Representation-Learning" class="headerlink" title="Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A   Case Study with In-Context Representation Learning"></a>Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A   Case Study with In-Context Representation Learning</h2><p><strong>Authors:Tianle Zhang, Wanlong Fang, Jonathan Woo, Paridhi Latawa, Deepak A. Subramanian, Alvin Chan</strong></p>
<p>The remarkable performance of Large Language Models (LLMs) can be enhanced with test-time computation, which relies on external tools and even other deep learning models. However, existing approaches for integrating non-text modality representations into LLMs typically require additional costly supervised training, restricting on-the-fly adaptation to new domains and modalities. In this work, we explore the feasibility of integrating representations from non-text foundational models (FMs) into text-based LLMs in a training-free manner. We propose In-Context Representation Learning (ICRL) as a proof-of-concept to allow LLMs to adaptively utilize non-text modality representations with few-shot learning. Unlike traditional in-context learning, which incorporates text-label pairs, ICRL replaces text inputs with FM representations, enabling the LLM to perform multi-modal inference without fine-tuning. We evaluate ICRL on a suite of tasks in the molecular domain, investigating three core research questions: (i) how to map FM representations into LLMs in a training-free manner, (ii) what factors influence ICRL performance, and (iii) what mechanisms underlie the effectiveness of ICRL. To the best of our knowledge, ICRL is the first training-free framework for integrating non-text modality representations into text-based LLMs, presenting a promising direction for adaptable, multi-modal generalization. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºè‰²æ€§èƒ½å¯ä»¥é€šè¿‡æµ‹è¯•æ—¶çš„è®¡ç®—å¢å¼ºï¼Œè¯¥è®¡ç®—ä¾èµ–äºå¤–éƒ¨å·¥å…·å’Œå…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ç„¶è€Œï¼Œå°†éæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºé›†æˆåˆ°LLMä¸­çš„ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„æ˜‚è´µçš„æœ‰ç›‘ç£è®­ç»ƒï¼Œè¿™é™åˆ¶äº†åœ¨æ–°é¢†åŸŸå’Œæ¨¡æ€çš„å³æ—¶é€‚åº”ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä»¥æ— è®­ç»ƒçš„æ–¹å¼å°†éæ–‡æœ¬åŸºç¡€æ¨¡å‹ï¼ˆFMï¼‰çš„è¡¨ç¤ºé›†æˆåˆ°æ–‡æœ¬ä¸ºåŸºç¡€çš„LLMä¸­çš„å¯è¡Œæ€§ã€‚æˆ‘ä»¬æå‡ºåŸºäºæƒ…å¢ƒè¡¨ç¤ºå­¦ä¹ ï¼ˆICRLï¼‰çš„æ¦‚å¿µéªŒè¯ï¼Œå…è®¸LLMä»¥è‡ªé€‚åº”çš„æ–¹å¼åˆ©ç”¨éæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºè¿›è¡Œå°æ ·æœ¬å­¦ä¹ ã€‚ä¸ä¼ ç»Ÿçš„ä¸Šä¸‹æ–‡å­¦ä¹ ä¸åŒï¼ŒICRLç»“åˆäº†æ–‡æœ¬æ ‡ç­¾å¯¹ï¼Œå¹¶ç”¨FMè¡¨ç¤ºæ›¿æ¢æ–‡æœ¬è¾“å…¥ï¼Œä½¿LLMèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹æ‰§è¡Œå¤šæ¨¡æ€æ¨ç†ã€‚æˆ‘ä»¬åœ¨åˆ†å­é¢†åŸŸçš„å¤šä¸ªä»»åŠ¡ä¸Šè¯„ä¼°äº†ICRLï¼Œå¹¶æ¢è®¨äº†ä¸‰ä¸ªæ ¸å¿ƒç ”ç©¶é—®é¢˜ï¼šï¼ˆiï¼‰å¦‚ä½•ä»¥æ— è®­ç»ƒçš„æ–¹å¼å°†FMè¡¨ç¤ºæ˜ å°„åˆ°LLMä¸­ï¼Œï¼ˆiiï¼‰å“ªäº›å› ç´ å½±å“ICRLçš„æ€§èƒ½ï¼Œä»¥åŠï¼ˆiiiï¼‰ICRLæœ‰æ•ˆæ€§çš„åŸºç¡€æœºåˆ¶æ˜¯ä»€ä¹ˆã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒICRLæ˜¯ç¬¬ä¸€ä¸ªå°†éæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºé›†æˆåˆ°åŸºäºæ–‡æœ¬çš„LLMä¸­çš„æ— è®­ç»ƒæ¡†æ¶ï¼Œä¸ºå¯é€‚åº”çš„å¤šæ¨¡æ€æ³›åŒ–æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17552v1">PDF</a> NIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†ä»¥éæ–‡æœ¬ä¸ºåŸºç¡€çš„æ¨¡å¼ï¼ˆFMsï¼‰è¡¨ç¤ºåœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹æ•´åˆåˆ°åŸºäºæ–‡æœ¬çš„è¶…å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„å¯è¡Œæ€§ã€‚æå‡ºä¸€ç§åä¸ºä¸Šä¸‹æ–‡è¡¨ç¤ºå­¦ä¹ ï¼ˆICRLï¼‰çš„æ¦‚å¿µéªŒè¯ï¼Œä½¿LLMsèƒ½å¤Ÿè‡ªé€‚åº”åœ°åˆ©ç”¨éæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºè¿›è¡Œå°æ ·æœ¬å­¦ä¹ ã€‚ä¸åŒäºä¼ ç»Ÿçš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ŒICRLç”¨FMè¡¨ç¤ºæ›¿æ¢æ–‡æœ¬è¾“å…¥ï¼Œä½¿LLMæ— éœ€å¾®è°ƒå³å¯è¿›è¡Œå¤šæ¨¡æ€æ¨ç†ã€‚åœ¨åˆ†å­é¢†åŸŸçš„å¤šä¸ªä»»åŠ¡ä¸Šè¯„ä¼°ICRLï¼Œå¹¶ç ”ç©¶ä¸‰ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šå¦‚ä½•åœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹å°†FMè¡¨ç¤ºæ˜ å°„åˆ°LLMsä¸­ã€å“ªäº›å› ç´ å½±å“ICRLçš„æ€§èƒ½ä»¥åŠICRLæœ‰æ•ˆæ€§çš„æœºåˆ¶æ˜¯ä»€ä¹ˆã€‚ICRLæ˜¯é¦–ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œç”¨äºæ•´åˆéæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºåˆ°åŸºäºæ–‡æœ¬çš„LLMsä¸­ï¼Œä¸ºå¯é€‚åº”çš„å¤šæ¨¡æ€æ¨å¹¿æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså¯ä»¥é€šè¿‡æµ‹è¯•æ—¶é—´çš„è®¡ç®—å¢å¼ºæ€§èƒ½ï¼Œè¿™ä¾èµ–äºå¤–éƒ¨å·¥å…·å’Œå…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>ç›®å‰å°†éæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºé›†æˆåˆ°LLMsä¸­çš„æ–¹æ³•éœ€è¦é¢å¤–çš„æ˜‚è´µç›‘ç£è®­ç»ƒï¼Œé™åˆ¶äº†åœ¨æ–°é¢†åŸŸå’Œæ¨¡æ€çš„å³æ—¶é€‚åº”ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ICRLï¼Œå…è®¸LLMsä»¥è®­ç»ƒæ— å…³çš„æ–¹å¼è‡ªé€‚åº”åœ°åˆ©ç”¨éæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºè¿›è¡Œå°æ ·æœ¬å­¦ä¹ ã€‚</li>
<li>ICRLä½¿ç”¨FMè¡¨ç¤ºæ›¿æ¢æ–‡æœ¬è¾“å…¥ï¼Œä½¿LLMè¿›è¡Œå¤šæ¨¡æ€æ¨ç†è€Œæ— éœ€å¾®è°ƒã€‚</li>
<li>ICRLåœ¨åˆ†å­é¢†åŸŸçš„å¤šä¸ªä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ç ”ç©¶äº†å¦‚ä½•å°†FMè¡¨ç¤ºæ˜ å°„åˆ°LLMsä¸­ã€å½±å“ICRLæ€§èƒ½çš„å› ç´ ä»¥åŠICRLæœ‰æ•ˆæ€§çš„æœºåˆ¶ã€‚</li>
<li>ICRLæ˜¯é¦–ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œç”¨äºæ•´åˆéæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºåˆ°åŸºäºæ–‡æœ¬çš„LLMsä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17552">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-963f87cc36a933634ab3a3d542582512" align="middle">
<img src="https://picx.zhimg.com/v2-ce257a3fd61cc171a22bcecfffe0d56b" align="middle">
<img src="https://picx.zhimg.com/v2-aa58c7c2f0300eaf576916ea7921c629" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Leveraging-Audio-Visual-Data-to-Reduce-the-Multilingual-Gap-in-Self-Supervised-Speech-Models"><a href="#Leveraging-Audio-Visual-Data-to-Reduce-the-Multilingual-Gap-in-Self-Supervised-Speech-Models" class="headerlink" title="Leveraging Audio-Visual Data to Reduce the Multilingual Gap in   Self-Supervised Speech Models"></a>Leveraging Audio-Visual Data to Reduce the Multilingual Gap in   Self-Supervised Speech Models</h2><p><strong>Authors:MarÃ­a Andrea Cruz BlandÃ³n, Zakaria Aldeneh, Jie Chi, Maureen de Seyssel</strong></p>
<p>Self-supervised learning (SSL) has made significant advances in speech representation learning. Models like wav2vec 2.0 and HuBERT have achieved state-of-the-art results in tasks such as speech recognition, particularly in monolingual settings. However, multilingual SSL models tend to underperform their monolingual counterparts on each individual language, especially in multilingual scenarios with few languages such as the bilingual setting. In this work, we investigate a novel approach to reduce this performance gap by introducing limited visual grounding into bilingual speech SSL models. Our results show that visual grounding benefits both monolingual and bilingual models, with especially pronounced gains for the latter, reducing the multilingual performance gap on zero-shot phonetic discrimination from 31.5% for audio-only models to 8.04% with grounding. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨è¯­éŸ³è¡¨å¾å­¦ä¹ æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚åƒwav2vec 2.0å’ŒHuBERTè¿™æ ·çš„æ¨¡å‹åœ¨è¯­éŸ³è¯†åˆ«ç­‰ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨å•è¯­ç¯å¢ƒä¸­ã€‚ç„¶è€Œï¼Œå¤šè¯­ç§SSLæ¨¡å‹å¾€å¾€åœ¨æ¯ä¸ªå•ä¸€è¯­è¨€çš„æ€§èƒ½ä¸Šä¸åŠå•è¯­ç§æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒè¯­ç­‰è¯­è¨€æ•°é‡è¾ƒå°‘çš„å¤šè¯­ç§åœºæ™¯ä¸­ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸€ç§é€šè¿‡å¼•å…¥æœ‰é™çš„è§†è§‰å®šä½æ¥å‡å°‘è¿™ç§æ€§èƒ½å·®è·çš„æ–°æ–¹æ³•ï¼Œå¹¶å°†å…¶åº”ç”¨äºåŒè¯­è¯­éŸ³SSLæ¨¡å‹ä¸­ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè§†è§‰å®šä½å¯¹å•è¯­ç§å’ŒåŒè¯­ç§æ¨¡å‹éƒ½æœ‰å¥½å¤„ï¼Œå¯¹åè€…çš„å¥½å¤„å°¤ä¸ºçªå‡ºï¼Œå°†é›¶å°„è¯­éŸ³çš„å¤šå…ƒè¯­è¨€æ€§èƒ½å·®è·ä»ä»…éŸ³é¢‘æ¨¡å‹çš„31.5%é™ä½åˆ°å¸¦æœ‰å®šä½åŠŸèƒ½çš„8.04%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17523v1">PDF</a> 5 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è‡ªæˆ‘ç›‘ç£å­¦ä¹ åœ¨è¯­éŸ³è¡¨ç¤ºå­¦ä¹ ä¸­çš„æœ€æ–°è¿›å±•ï¼Œå¹¶å°è¯•é€šè¿‡åœ¨åŒè¯­è¯­éŸ³SSLæ¨¡å‹ä¸­å¼•å…¥æœ‰é™çš„è§†è§‰åŸºç¡€æ¥è§£å†³å¤šè¯­ç§ç¯å¢ƒä¸‹çš„æ€§èƒ½é—®é¢˜ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œè§†è§‰åŸºç¡€ä¸ä»…æœ‰ç›Šäºå•è¯­æ¨¡å‹ï¼Œæ›´æœ‰åŠ©äºæå‡åŒè¯­æ¨¡å‹çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨é›¶å°„è¯­éŸ³è¾¨è¯†ä»»åŠ¡ä¸Šï¼Œé€šè¿‡è§†è§‰åŸºç¡€å°†å¤šè¯­ç§çš„æ€§èƒ½å·®è·ä»éŸ³é¢‘æ¨¡å‹çš„31.5%ç¼©å°åˆ°ä»…ä½¿ç”¨è§†è§‰åŸºç¡€çš„8.04%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li><p>è‡ªæˆ‘ç›‘ç£å­¦ä¹ åœ¨è¯­éŸ³è¡¨ç¤ºå­¦ä¹ ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚é€šè¿‡wav2vec 2.0å’ŒHuBERTç­‰æ¨¡å‹ï¼Œå·²ç»åœ¨è¯­éŸ³è¾¨è¯†ç­‰ä»»åŠ¡ä¸Šå®ç°äº†é¡¶å°–è¡¨ç°ã€‚ç„¶è€Œï¼Œåœ¨å¤šè¯­ç§ç¯å¢ƒä¸­å­˜åœ¨æ€§èƒ½çŸ­æ¿ã€‚å¯¹æ­¤è®ºæ–‡å°è¯•é€šè¿‡å¼•å…¥è§†è§‰åŸºç¡€æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ç ”ç©¶æŒ‡å‡ºåœ¨å¤šè¯­ç§åœºæ™¯ä¸‹ä»…ä½¿ç”¨éŸ³é¢‘æ¨¡å‹çš„æ€§èƒ½å·®è·è¾ƒå¤§ã€‚é€šè¿‡è§†è§‰åŸºç¡€çš„å¼•å…¥ï¼Œè¿™ä¸€å·®è·å¾—åˆ°äº†æ˜¾è‘—ç¼©å°ã€‚</p>
</li>
<li><p>åœ¨åŒè¯­ç¯å¢ƒä¸­å¼•å…¥è§†è§‰åŸºç¡€èƒ½å¤Ÿæœ‰æ•ˆæå‡è¯­éŸ³è‡ªæˆ‘ç›‘ç£å­¦ä¹ çš„æ€§èƒ½è¡¨ç°ã€‚å°½ç®¡è¿™ä¸€æ–¹æ³•åœ¨å•è¯­ç¯å¢ƒä¸‹ä¹Ÿè¡¨ç°è‰¯å¥½ï¼Œä½†å¯¹åŒè¯­ç¯å¢ƒçš„ä¼˜åŒ–æ•ˆæœæ›´ä¸ºæ˜¾è‘—ã€‚</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17523">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dfa62a2ea8cf510193d985640de18658" align="middle">
<img src="https://picx.zhimg.com/v2-5556ab3624916ec8531b9544477727d9" align="middle">
<img src="https://picx.zhimg.com/v2-6a9bf70efa4da85a2e95bc446fc02ac7" align="middle">
<img src="https://picx.zhimg.com/v2-7969c4a42c05b2f0d1903d034b9752bc" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Efficient-Sliced-Wasserstein-Distance-Computation-via-Adaptive-Bayesian-Optimization"><a href="#Efficient-Sliced-Wasserstein-Distance-Computation-via-Adaptive-Bayesian-Optimization" class="headerlink" title="Efficient Sliced Wasserstein Distance Computation via Adaptive Bayesian   Optimization"></a>Efficient Sliced Wasserstein Distance Computation via Adaptive Bayesian   Optimization</h2><p><strong>Authors:Manish Acharya, David Hyde</strong></p>
<p>The sliced Wasserstein distance (SW) reduces optimal transport on $\mathbb{R}^d$ to a sum of one-dimensional projections, and thanks to this efficiency, it is widely used in geometry, generative modeling, and registration tasks. Recent work shows that quasi-Monte Carlo constructions for computing SW (QSW) yield direction sets with excellent approximation error. This paper presents an alternate, novel approach: learning directions with Bayesian optimization (BO), particularly in settings where SW appears inside an optimization loop (e.g., gradient flows). We introduce a family of drop-in selectors for projection directions: BOSW, a one-shot BO scheme on the unit sphere; RBOSW, a periodic-refresh variant; ABOSW, an adaptive hybrid that seeds from competitive QSW sets and performs a few lightweight BO refinements; and ARBOSW, a restarted hybrid that periodically relearns directions during optimization. Our BO approaches can be composed with QSW and its variants (demonstrated by ABOSW&#x2F;ARBOSW) and require no changes to downstream losses or gradients. We provide numerical experiments where our methods achieve state-of-the-art performance, and on the experimental suite of the original QSW paper, we find that ABOSW and ARBOSW can achieve convergence comparable to the best QSW variants with modest runtime overhead. </p>
<blockquote>
<p>åˆ‡ç‰‡Wassersteinè·ç¦»ï¼ˆSWï¼‰å°†$\mathbb{R}^d$ä¸Šçš„æœ€ä¼˜ä¼ è¾“ç®€åŒ–ä¸ºä¸€ç³»åˆ—ä¸€ç»´æŠ•å½±ä¹‹å’Œã€‚ç”±äºè¿™ç§é«˜æ•ˆæ€§ï¼Œå®ƒå¹¿æ³›åº”ç”¨äºå‡ ä½•ã€ç”Ÿæˆå»ºæ¨¡å’Œæ³¨å†Œä»»åŠ¡ã€‚æœ€æ–°å·¥ä½œæ˜¾ç¤ºï¼Œä½¿ç”¨å‡†è’™ç‰¹å¡æ´›æ„é€ ï¼ˆQSWï¼‰è®¡ç®—SWå¯ä»¥äº§ç”Ÿå…·æœ‰è‰¯å¥½è¿‘ä¼¼è¯¯å·®çš„æ–¹å‘é›†ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ›¿ä»£çš„ã€æ–°é¢–çš„æ–¹æ³•ï¼šä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–ï¼ˆBOï¼‰å­¦ä¹ æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨SWå‡ºç°åœ¨ä¼˜åŒ–å¾ªç¯ä¸­çš„ç¯å¢ƒï¼ˆä¾‹å¦‚ï¼Œæ¢¯åº¦æµï¼‰ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç³»åˆ—å³æ’å³ç”¨çš„æŠ•å½±æ–¹å‘é€‰æ‹©å™¨ï¼šBOSWï¼Œå•ä½çƒä¸Šçš„ä¸€æ¬¡æ€§BOæ–¹æ¡ˆï¼›RBOSWï¼Œä¸€ç§å‘¨æœŸæ€§åˆ·æ–°å˜ä½“ï¼›ABOSWï¼Œä¸€ç§è‡ªé€‚åº”æ··åˆæ–¹æ³•ï¼Œä»ç«äº‰æ€§çš„QSWé›†åˆä¸­æ’­ç§å¹¶è¿›è¡Œä¸€äº›è½»é‡çº§çš„BOä¼˜åŒ–ï¼›ä»¥åŠARBOSWï¼Œä¸€ç§åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å®šæœŸé‡æ–°å­¦ä¹ æ–¹å‘çš„é‡å¯æ··åˆæ–¹æ³•ã€‚æˆ‘ä»¬çš„BOæ–¹æ³•ä¸QSWåŠå…¶å˜ä½“ç›¸ç»“åˆï¼ˆç”±ABOSW&#x2F;ARBOSWå±•ç¤ºï¼‰ï¼Œä¸éœ€è¦æ”¹å˜ä¸‹æ¸¸æŸå¤±æˆ–æ¢¯åº¦ã€‚æˆ‘ä»¬æä¾›æ•°å€¼å®éªŒï¼Œè¯æ˜æˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨åŸQSWè®ºæ–‡çš„å®éªŒå¥—ä»¶ä¸Šï¼Œæˆ‘ä»¬å‘ç°ABOSWå’ŒARBOSWå¯ä»¥è¾¾åˆ°ä¸æœ€ä½³QSWå˜ä½“ç›¸å½“çš„æ”¶æ•›é€Ÿåº¦ï¼Œä¸”è¿è¡Œæ—¶å¼€é”€é€‚åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17405v1">PDF</a> 19 pages, 11 figures</p>
<p><strong>æ‘˜è¦</strong><br>    Wassersteinè·ç¦»åˆ‡ç‰‡æ³•ï¼ˆSWï¼‰é€šè¿‡åœ¨$\mathbb{R}^d$ä¸Šçš„ä¸€ç³»åˆ—ä¸€ç»´æŠ•å½±ä¼˜åŒ–ä¼ è¾“ï¼Œå› å…¶æ•ˆç‡å¹¿æ³›åº”ç”¨äºå‡ ä½•ã€ç”Ÿæˆå»ºæ¨¡å’Œæ³¨å†Œä»»åŠ¡ã€‚è¿‘æœŸç ”ç©¶ä½¿ç”¨å‡†è’™ç‰¹å¡æ´›æ„é€ è®¡ç®—SWï¼ˆQSWï¼‰ï¼Œäº§ç”Ÿå…·æœ‰è‰¯å¥½è¿‘ä¼¼è¯¯å·®çš„æ–¹å‘é›†ã€‚æœ¬æ–‡ä»‹ç»ä¸€ç§æ–°å‹æ–¹æ³•ï¼šç”¨è´å¶æ–¯ä¼˜åŒ–ï¼ˆBOï¼‰å­¦ä¹ æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨SWç”¨äºä¼˜åŒ–å¾ªç¯ï¼ˆå¦‚æ¢¯åº¦æµï¼‰çš„åœºæ™¯ä¸‹ã€‚å¼•å…¥ä¸€ç³»åˆ—å³æ’å³ç”¨çš„é€‰æ‹©å™¨ç”¨äºæŠ•å½±æ–¹å‘ï¼šBOSWï¼Œåœ¨å•ä½çƒä¸Šçš„ä¸€æ¬¡æ€§BOæ–¹æ¡ˆï¼›RBOSWï¼Œå‘¨æœŸæ€§åˆ·æ–°å˜ä½“ï¼›ABOSWï¼Œä¸€ç§è‡ªé€‚åº”æ··åˆæ–¹æ³•ï¼Œä»ç«äº‰æ€§çš„QSWé›†åˆä¸­æ’­ç§ï¼Œå¹¶è¿›è¡Œä¸€äº›è½»é‡çº§çš„BOä¼˜åŒ–ï¼›ä»¥åŠARBOSWï¼Œä¸€ç§ä¼˜åŒ–è¿‡ç¨‹ä¸­å®šæœŸé‡æ–°å­¦ä¹ æ–¹å‘çš„é‡å¯æ··åˆæ–¹æ³•ã€‚æˆ‘ä»¬çš„BOæ–¹æ³•ä¸QSWåŠå…¶å˜ç§ç»“åˆï¼ˆé€šè¿‡ABOSW&#x2F;ARBOSWå±•ç¤ºï¼‰ï¼Œæ— éœ€æ›´æ”¹ä¸‹æ¸¸æŸå¤±æˆ–æ¢¯åº¦ã€‚æ•°å€¼å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œåœ¨åŸQSWè®ºæ–‡çš„å®éªŒå¥—ä»¶ä¸Šï¼Œæˆ‘ä»¬å‘ç°ABOSWå’ŒARBOSWçš„æ”¶æ•›æ€§ä¸æœ€ä½³QSWå˜ç§ç›¸å½“ï¼Œä¸”è¿è¡Œæ—¶å¼€é”€é€‚åº¦ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Wassersteinè·ç¦»åˆ‡ç‰‡æ³•ï¼ˆSWï¼‰é€šè¿‡ä¸€ç»´æŠ•å½±ä¼˜åŒ–äº†ä¼ è¾“æ•ˆç‡ï¼Œå¹¿æ³›åº”ç”¨äºå‡ ä½•ã€ç”Ÿæˆå»ºæ¨¡å’Œæ³¨å†Œä»»åŠ¡ã€‚</li>
<li>æœ€æ–°ç ”ç©¶ä½¿ç”¨å‡†è’™ç‰¹å¡æ´›æ„é€ ï¼ˆQSWï¼‰æ¥è®¡ç®—SWï¼Œç”Ÿæˆå…·æœ‰è‰¯å¥½è¿‘ä¼¼è¯¯å·®çš„æ–¹å‘é›†ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹æ–¹æ³•ï¼šç»“åˆè´å¶æ–¯ä¼˜åŒ–ï¼ˆBOï¼‰å­¦ä¹ æ–¹å‘ï¼Œç”¨äºæ›´å¤æ‚åœºæ™¯å¦‚æ¢¯åº¦æµä¸­çš„SWä¼˜åŒ–å¾ªç¯ã€‚</li>
<li>å¼•å…¥å¤šç§BOæ–¹å‘é€‰æ‹©å™¨ï¼Œå¦‚BOSWã€RBOSWã€ABOSWå’ŒARBOSWï¼Œè¿™äº›é€‰æ‹©å™¨å¯ä»¥ä¸QSWåŠå…¶å˜ç§ç»“åˆä½¿ç”¨ã€‚</li>
<li>BOæ–¹æ³•æ— éœ€ä¿®æ”¹ä¸‹æ¸¸æŸå¤±æˆ–æ¢¯åº¦å³å¯åº”ç”¨ã€‚</li>
<li>æ•°å€¼å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨BOæ–¹æ³•çš„æ–¹å‘é€‰æ‹©å™¨åœ¨æ€§èƒ½ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šäº†æœ€æ–°æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17405">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33f21ecbf8178861924644fb91e8ae89" align="middle">
<img src="https://picx.zhimg.com/v2-962cd8c1979b6cdf5e158fae242dba8d" align="middle">
<img src="https://picx.zhimg.com/v2-a13f1947730fa0b5312fe56ee482f04b" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Automated-Knowledge-Graph-Construction-using-Large-Language-Models-and-Sentence-Complexity-Modelling"><a href="#Automated-Knowledge-Graph-Construction-using-Large-Language-Models-and-Sentence-Complexity-Modelling" class="headerlink" title="Automated Knowledge Graph Construction using Large Language Models and   Sentence Complexity Modelling"></a>Automated Knowledge Graph Construction using Large Language Models and   Sentence Complexity Modelling</h2><p><strong>Authors:Sydney Anuyah, Mehedi Mahmud Kaushik, Krishna Dwarampudi, Rakesh Shiradkar, Arjan Durresi, Sunandan Chakraborty</strong></p>
<p>We introduce CoDe-KG, an open-source, end-to-end pipeline for extracting sentence-level knowledge graphs by combining robust coreference resolution with syntactic sentence decomposition. Using our model, we contribute a dataset of over 150,000 knowledge triples, which is open source. We also contribute a training corpus of 7248 rows for sentence complexity, 190 rows of gold human annotations for co-reference resolution using open source lung-cancer abstracts from PubMed, 900 rows of gold human annotations for sentence conversion policies, and 398 triples of gold human annotations. We systematically select optimal prompt-model pairs across five complexity categories, showing that hybrid chain-of-thought and few-shot prompting yields up to 99.8% exact-match accuracy on sentence simplification. On relation extraction (RE), our pipeline achieves 65.8% macro-F1 on REBEL, an 8-point gain over the prior state of the art, and 75.7% micro-F1 on WebNLG2, while matching or exceeding performance on Wiki-NRE and CaRB. Ablation studies demonstrate that integrating coreference and decomposition increases recall on rare relations by over 20%. Code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/KaushikMahmud/CoDe-KG_EMNLP_2025">https://github.com/KaushikMahmud/CoDe-KG_EMNLP_2025</a> </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†CoDe-KGè¿™ä¸€å¼€æºçš„ç«¯åˆ°ç«¯çŸ¥è¯†å›¾è°±æå–ç®¡é“ï¼Œå®ƒç»“åˆäº†é²æ£’çš„æ ¸å¿ƒå¼•ç”¨è§£æå’Œå¥æ³•å¥å­åˆ†è§£æ¥æå–å¥å­çº§çŸ¥è¯†å›¾è°±ã€‚ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬è´¡çŒ®äº†ä¸€ä¸ªåŒ…å«è¶…è¿‡15ä¸‡æ¡çŸ¥è¯†ä¸‰å…ƒç»„çš„å¼€æºæ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜ä¸ºå¥å­å¤æ‚åº¦è´¡çŒ®äº†7248è¡Œçš„è®­ç»ƒè¯­æ–™åº“ï¼Œä½¿ç”¨PubMedçš„å¼€æºè‚ºç™Œæ‘˜è¦ä¸ºæ ¸å¿ƒå¼•ç”¨è§£æè´¡çŒ®äº†190è¡Œçš„é»„é‡‘äººç±»æ ‡æ³¨ï¼Œä¸ºå¥å­è½¬æ¢ç­–ç•¥è´¡çŒ®äº†900è¡Œçš„é»„é‡‘äººç±»æ ‡æ³¨ï¼Œä»¥åŠ398ä¸ªçŸ¥è¯†ä¸‰å…ƒç»„çš„é»„é‡‘äººç±»æ ‡æ³¨ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°é€‰æ‹©äº†äº”ä¸ªå¤æ‚åº¦ç±»åˆ«ä¸­çš„æœ€ä½³æç¤ºæ¨¡å‹å¯¹ï¼Œè¡¨æ˜æ··åˆæ€ç»´é“¾å’Œå°‘é‡æç¤ºåœ¨å¥å­ç®€åŒ–æ–¹é¢å¯ä»¥è¾¾åˆ°é«˜è¾¾99.8%çš„ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ã€‚åœ¨å…³ç³»æŠ½å–ï¼ˆREï¼‰æ–¹é¢ï¼Œæˆ‘ä»¬çš„ç®¡é“åœ¨REBELä¸Šå®ç°äº†65.8%çš„å®è§‚F1åˆ†æ•°ï¼Œæ¯”ç°æœ‰æŠ€æœ¯é«˜å‡º8ä¸ªç™¾åˆ†ç‚¹ï¼Œåœ¨WebNLG2ä¸Šå®ç°äº†75.7%çš„å¾®è§‚F1åˆ†æ•°ï¼ŒåŒæ—¶åœ¨Wiki-NREå’ŒCaRBä¸Šçš„æ€§èƒ½ç›¸åŒ¹é…æˆ–æ›´é«˜ã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œé›†æˆæ ¸å¿ƒå¼•ç”¨å’Œåˆ†è§£å¯ä»¥å°†ç¨€æœ‰å…³ç³»çš„å¬å›ç‡æé«˜è¶…è¿‡20%ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/KaushikMahmud/CoDe-KG_EMNLP_2025%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/KaushikMahmud/CoDe-KG_EMNLP_2025æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17289v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†åä¸ºCoDe-KGçš„å¼€æºã€ç«¯åˆ°ç«¯çŸ¥è¯†å›¾è°±æå–ç®¡é“ï¼Œé€šè¿‡ç»“åˆå¥å£®çš„æ ¸å¿ƒè§£æä¸å¥æ³•å¥å­åˆ†è§£æ¥æå–å¥å­çº§åˆ«çš„çŸ¥è¯†å›¾è°±ã€‚åŒæ—¶è´¡çŒ®äº†ä¸€ä¸ªè¶…è¿‡15ä¸‡çŸ¥è¯†ä¸‰å…ƒç»„çš„å¼€æºæ•°æ®é›†ï¼Œå¹¶æä¾›äº†è®­ç»ƒè¯­æ–™åº“å’Œé»„é‡‘æ ‡å‡†æ³¨é‡Šã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ··åˆæ€ç»´é“¾å’Œå°‘é‡æç¤ºçš„æ–¹æ³•åœ¨å¥å­ç®€åŒ–ä»»åŠ¡ä¸Šè¾¾åˆ°äº†é«˜è¾¾99.8%çš„ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ã€‚åœ¨å…³ç³»æŠ½å–ä»»åŠ¡ä¸Šï¼Œè¯¥ç®¡é“åœ¨REBELä¸Šå®ç°äº†65.8%çš„å®è§‚F1åˆ†æ•°ï¼Œå¹¶åœ¨Wiki-NREå’ŒCaRBä¸Šè¾¾åˆ°æˆ–è¶…è¿‡äº†ä¹‹å‰çš„æ€§èƒ½æ°´å¹³ã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œæ•´åˆæ ¸å¿ƒå¼•ç”¨å’Œåˆ†è§£èƒ½æé«˜ç¨€æœ‰å…³ç³»çš„å¬å›ç‡è¶…è¿‡20%ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/KaushikMahmud/CoDe-KG_EMNLP_2025">é“¾æ¥</a>è·å–ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>CoDe-KGæ˜¯ä¸€ä¸ªç»“åˆæ ¸å¿ƒè§£æä¸å¥æ³•å¥å­åˆ†è§£çš„å¼€æºã€ç«¯åˆ°ç«¯çŸ¥è¯†å›¾è°±æå–ç®¡é“ã€‚</li>
<li>è´¡çŒ®äº†ä¸€ä¸ªåŒ…å«è¶…è¿‡15ä¸‡çŸ¥è¯†ä¸‰å…ƒç»„çš„å¼€æºæ•°æ®é›†ã€‚</li>
<li>é€šè¿‡æ··åˆæ€ç»´é“¾å’Œå°‘é‡æç¤ºçš„æ–¹æ³•ï¼Œåœ¨å¥å­ç®€åŒ–ä»»åŠ¡ä¸Šå®ç°äº†é«˜ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ã€‚</li>
<li>åœ¨å…³ç³»æŠ½å–ä»»åŠ¡ä¸Šï¼ŒCoDe-KGç®¡é“åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>æ¶ˆèç ”ç©¶è¡¨æ˜æ•´åˆæ ¸å¿ƒå¼•ç”¨å’Œåˆ†è§£èƒ½æ˜¾è‘—æé«˜ç¨€æœ‰å…³ç³»çš„å¬å›ç‡ã€‚</li>
<li>æä¾›äº†è®­ç»ƒè¯­æ–™åº“å’Œé»„é‡‘æ ‡å‡†æ³¨é‡Šç”¨äºæ¨¡å‹è®­ç»ƒä¸è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17289">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ec71a97fb5d4be24555a919027cd51e" align="middle">
<img src="https://picx.zhimg.com/v2-6ac66b85b54cb2e69d05040f6f2682d7" align="middle">
<img src="https://picx.zhimg.com/v2-4475a71b771992997aa175911d9ac24e" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CardiacCLIP-Video-based-CLIP-Adaptation-for-LVEF-Prediction-in-a-Few-shot-Manner"><a href="#CardiacCLIP-Video-based-CLIP-Adaptation-for-LVEF-Prediction-in-a-Few-shot-Manner" class="headerlink" title="CardiacCLIP: Video-based CLIP Adaptation for LVEF Prediction in a   Few-shot Manner"></a>CardiacCLIP: Video-based CLIP Adaptation for LVEF Prediction in a   Few-shot Manner</h2><p><strong>Authors:Yao Du, Jiarong Guo, Xiaomeng Li</strong></p>
<p>Echocardiography is a vital non-invasive modality for cardiac assessment, with left ventricular ejection fraction (LVEF) serving as a key indicator of heart function. Existing LVEF estimation methods depend on large-scale annotated video datasets, which are costly and limit adaptability across various clinical settings. Recent vision-language models for echocardiography, such as EchoCLIP, apply image-to-text pretraining but fail to capture crucial temporal dynamics and localized cardiac structures essential for accurate diagnosis. To address these challenges, we propose CardiacCLIP, a video-based framework that enhances LVEF prediction through attention-based frame aggregation and multi-resolution input scaling. Specifically, we introduce MFL (Multi Frame Learning), a novel attention-based mechanism for selectively fusing informative frames, and EchoZoom, a multi-scale feature extraction strategy that refines spatial representations of cardiac structures. As a novel adaptation of CLIP models for few-shot echocardiogram video analysis, our approach significantly improves diagnostic accuracy, reducing MAE by 2.07 on the EchoNet-Dynamic dataset under 1-shot setting. The code is available at <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/CardiacCLIP">https://github.com/xmed-lab/CardiacCLIP</a>. </p>
<blockquote>
<p>è¶…å£°å¿ƒåŠ¨å›¾æ˜¯ä¸€ç§é‡è¦çš„æ— åˆ›å¿ƒè„è¯„ä¼°æ–¹æ³•ï¼Œå·¦å¿ƒå®¤å°„è¡€åˆ†æ•°ï¼ˆLVEFï¼‰æ˜¯è¯„ä¼°å¿ƒè„åŠŸèƒ½çš„å…³é”®æŒ‡æ ‡ã€‚ç°æœ‰çš„LVEFä¼°è®¡æ–¹æ³•ä¾èµ–äºå¤§è§„æ¨¡æ³¨é‡Šè§†é¢‘æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†æˆæœ¬é«˜æ˜‚ï¼Œä¸”åœ¨ä¸åŒä¸´åºŠç¯å¢ƒä¸­çš„é€‚åº”æ€§æœ‰é™ã€‚æœ€è¿‘çš„è¶…å£°å¿ƒåŠ¨å›¾è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¦‚EchoCLIPï¼Œåº”ç”¨å›¾åƒåˆ°æ–‡æœ¬çš„é¢„è®­ç»ƒï¼Œä½†æœªèƒ½æ•æ‰å…³é”®çš„æ—¶é—´åŠ¨æ€å’Œå±€éƒ¨å¿ƒè„ç»“æ„ï¼Œè¿™å¯¹äºå‡†ç¡®è¯Šæ–­è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CardiacCLIPï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè§†é¢‘çš„æ¡†æ¶ï¼Œé€šè¿‡åŸºäºæ³¨æ„åŠ›çš„å¸§èšåˆå’Œå¤šåˆ†è¾¨ç‡è¾“å…¥ç¼©æ”¾æ¥å¢å¼ºLVEFé¢„æµ‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†MFLï¼ˆå¤šå¸§å­¦ä¹ ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„æ–°æœºåˆ¶ï¼Œç”¨äºé€‰æ‹©æ€§åœ°èåˆä¿¡æ¯å¸§ï¼Œä»¥åŠEchoZoomï¼Œä¸€ç§å¤šå°ºåº¦ç‰¹å¾æå–ç­–ç•¥ï¼Œç”¨äºç»†åŒ–å¿ƒè„ç»“æ„çš„ç©ºé—´è¡¨ç¤ºã€‚ä½œä¸ºCLIPæ¨¡å‹åœ¨å°‘æ•°è¶…å£°å¿ƒåŠ¨å›¾è§†é¢‘åˆ†æä¸­çš„æ–°åº”ç”¨ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†è¯Šæ–­å‡†ç¡®æ€§ï¼Œåœ¨EchoNet-Dynamicæ•°æ®é›†ä¸Šçš„å•æ ·æœ¬è®¾ç½®ä¸­ï¼Œå¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰é™ä½äº†2.07ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xmed-lab/CardiacCLIP%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/xmed-lab/CardiacCLIPè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17065v1">PDF</a> Accepted by MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¿ƒè„è¯„ä¼°ä¸­éä¾µå…¥æ€§çš„é‡è¦æ¨¡æ€â€”â€”è¶…å£°å¿ƒåŠ¨å›¾ï¼Œä»¥åŠå·¦å¿ƒå®¤å°„è¡€åˆ†æ•°ï¼ˆLVEFï¼‰ä½œä¸ºå¿ƒè„åŠŸèƒ½çš„å…³é”®æŒ‡æ ‡ã€‚ç°æœ‰LVEFä¼°è®¡æ–¹æ³•ä¾èµ–äºå¤§è§„æ¨¡æ³¨é‡Šè§†é¢‘æ•°æ®é›†ï¼Œæˆæœ¬é«˜æ˜‚ä¸”åœ¨ä¸åŒä¸´åºŠç¯å¢ƒä¸­çš„é€‚åº”æ€§æœ‰é™ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†CardiacCLIPè¿™ä¸€è§†é¢‘åŸºç¡€æ¡†æ¶ï¼Œé€šè¿‡åŸºäºæ³¨æ„åŠ›çš„å¸§èšåˆå’Œå¤šåˆ†è¾¨ç‡è¾“å…¥ç¼©æ”¾æŠ€æœ¯æ¥æå‡LVEFé¢„æµ‹ã€‚å¼•å…¥MFLï¼ˆå¤šå¸§å­¦ä¹ ï¼‰è¿™ä¸€æ–°å‹æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°ä¿¡æ¯å¸§çš„é€‰æ‹©æ€§èåˆï¼›åŒæ—¶æå‡ºEchoZoomå¤šå°ºåº¦ç‰¹å¾æå–ç­–ç•¥ï¼Œä»¥ä¼˜åŒ–å¿ƒè„ç»“æ„çš„ç©ºé—´è¡¨ç¤ºã€‚ä½œä¸ºCLIPæ¨¡å‹åœ¨å°‘æ•°æ ·æœ¬è¶…å£°å¿ƒåŠ¨å›¾è§†é¢‘åˆ†æä¸­çš„æ–°åº”ç”¨ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜è¯Šæ–­å‡†ç¡®æ€§ï¼Œåœ¨EchoNet-Dynamicæ•°æ®é›†ä¸Šçš„å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰å‡å°‘2.07ä¸ªç™¾åˆ†ç‚¹ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¶…å£°å¿ƒåŠ¨å›¾æ˜¯è¯„ä¼°å¿ƒè„åŠŸèƒ½çš„é‡è¦éä¾µå…¥æ€§æŠ€æœ¯ï¼Œå…¶ä¸­å·¦å¿ƒå®¤å°„è¡€åˆ†æ•°ï¼ˆLVEFï¼‰æ˜¯å…³é”®æŒ‡æ ‡ã€‚</li>
<li>ç°æœ‰LVEFä¼°è®¡æ–¹æ³•ä¾èµ–å¤§è§„æ¨¡æ³¨é‡Šè§†é¢‘æ•°æ®é›†ï¼Œæˆæœ¬é«˜ä¸”é€‚åº”æ€§æœ‰é™ã€‚</li>
<li>CardiacCLIPæ¡†æ¶é€šè¿‡åŸºäºæ³¨æ„åŠ›çš„å¸§èšåˆå’Œå¤šåˆ†è¾¨ç‡è¾“å…¥æŠ€æœ¯æå‡LVEFé¢„æµ‹ã€‚</li>
<li>MFLï¼ˆå¤šå¸§å­¦ä¹ ï¼‰æœºåˆ¶å®ç°ä¿¡æ¯å¸§é€‰æ‹©æ€§èåˆã€‚</li>
<li>EchoZoomç­–ç•¥ä¼˜åŒ–å¿ƒè„ç»“æ„çš„ç©ºé—´è¡¨ç¤ºã€‚</li>
<li>ä½œä¸ºCLIPæ¨¡å‹åœ¨å°‘æ•°æ ·æœ¬è¶…å£°å¿ƒåŠ¨å›¾è§†é¢‘åˆ†æçš„æ–°åº”ç”¨ï¼Œæ˜¾è‘—æé«˜äº†è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17065">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3875b1b5541a29a91978b0cc82581a65" align="middle">
<img src="https://picx.zhimg.com/v2-998ff1975eb627ab8dc550f980e9c0e9" align="middle">
<img src="https://picx.zhimg.com/v2-2f1e1589ccb37d2d94dcaa0e4fd673d5" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CLaC-at-DISRPT-2025-Hierarchical-Adapters-for-Cross-Framework-Multi-lingual-Discourse-Relation-Classification"><a href="#CLaC-at-DISRPT-2025-Hierarchical-Adapters-for-Cross-Framework-Multi-lingual-Discourse-Relation-Classification" class="headerlink" title="CLaC at DISRPT 2025: Hierarchical Adapters for Cross-Framework   Multi-lingual Discourse Relation Classification"></a>CLaC at DISRPT 2025: Hierarchical Adapters for Cross-Framework   Multi-lingual Discourse Relation Classification</h2><p><strong>Authors:Nawar Turk, Daniele Comitogianni, Leila Kosseim</strong></p>
<p>We present our submission to Task 3 (Discourse Relation Classification) of the DISRPT 2025 shared task. Task 3 introduces a unified set of 17 discourse relation labels across 39 corpora in 16 languages and six discourse frameworks, posing significant multilingual and cross-formalism challenges. We first benchmark the task by fine-tuning multilingual BERT-based models (mBERT, XLM-RoBERTa-Base, and XLM-RoBERTa-Large) with two argument-ordering strategies and progressive unfreezing ratios to establish strong baselines. We then evaluate prompt-based large language models (namely Claude Opus 4.0) in zero-shot and few-shot settings to understand how LLMs respond to the newly proposed unified labels. Finally, we introduce HiDAC, a Hierarchical Dual-Adapter Contrastive learning model. Results show that while larger transformer models achieve higher accuracy, the improvements are modest, and that unfreezing the top 75% of encoder layers yields performance comparable to full fine-tuning while training far fewer parameters. Prompt-based models lag significantly behind fine-tuned transformers, and HiDAC achieves the highest overall accuracy (67.5%) while remaining more parameter-efficient than full fine-tuning. </p>
<blockquote>
<p>æˆ‘ä»¬å‘DISRPT 2025å…±äº«ä»»åŠ¡çš„ç¬¬3ä¸ªä»»åŠ¡ï¼ˆè¯è¯­å…³ç³»åˆ†ç±»ï¼‰æäº¤æˆ‘ä»¬çš„ä½œå“ã€‚ç¬¬3ä¸ªä»»åŠ¡åœ¨39ä¸ªè¯­æ–™åº“çš„16ç§è¯­è¨€å’Œå…­ä¸ªè¯è¯­æ¡†æ¶ä¸­å¼•å…¥äº†ç»Ÿä¸€çš„17ä¸ªè¯è¯­å…³ç³»æ ‡ç­¾ï¼Œå¸¦æ¥äº†æ˜¾è‘—çš„å¤šè¯­ç§å’Œè·¨å½¢å¼ä¸»ä¹‰çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬é¦–å…ˆé€šè¿‡å¾®è°ƒåŸºäºBERTçš„å¤šè¯­ç§æ¨¡å‹ï¼ˆmBERTã€XLM-RoBERTa-Baseå’ŒXLM-RoBERTa-Largeï¼‰ä»¥åŠä¸¤ç§å‚æ•°æ’åºç­–ç•¥å’Œæ¸è¿›çš„è§£å†»æ¯”ä¾‹æ¥ä¸ºè¯¥ä»»åŠ¡åˆ¶å®šåŸºå‡†æµ‹è¯•ï¼Œä»¥å»ºç«‹å¼ºå¤§çš„åŸºå‡†çº¿ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬çš„æƒ…å†µä¸‹è¯„ä¼°åŸºäºæç¤ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå³Claude Opus 4.0ï¼‰ï¼Œä»¥äº†è§£å¤§å‹è¯­è¨€æ¨¡å‹å¦‚ä½•å“åº”æ–°æå‡ºçš„ç»Ÿä¸€æ ‡ç­¾ã€‚æœ€åï¼Œæˆ‘ä»¬ä»‹ç»äº†HiDACï¼Œä¸€ç§åˆ†å±‚åŒé€‚é…å™¨å¯¹æ¯”å­¦ä¹ æ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶æ›´å¤§çš„transformeræ¨¡å‹ç²¾åº¦æ›´é«˜ï¼Œä½†æ”¹è¿›å¹…åº¦é€‚ä¸­ï¼Œè§£å†»ç¼–ç å™¨é¡¶å±‚75%çš„å±‚æ€§èƒ½ä¸å®Œå…¨å¾®è°ƒç›¸å½“ï¼ŒåŒæ—¶è®­ç»ƒå‚æ•°æ›´å°‘ã€‚åŸºäºæç¤ºçš„æ¨¡å‹æ˜¾è‘—è½åäºå¾®è°ƒè¿‡çš„transformeræ¨¡å‹ï¼Œè€ŒHiDACåœ¨ä¿æŒè¾ƒé«˜æ€»ä½“ç²¾åº¦ï¼ˆ67.5%ï¼‰çš„åŒæ—¶ï¼Œæ¯”å®Œå…¨å¾®è°ƒæ›´å…·å‚æ•°æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16903v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä»»åŠ¡3ï¼ˆè¯è¯­å…³ç³»åˆ†ç±»ï¼‰çš„DISRPT 2025å…±äº«ä»»åŠ¡æäº¤ã€‚è¯¥ä»»åŠ¡å¼•å…¥äº†ç»Ÿä¸€çš„17ä¸ªè¯è¯­å…³ç³»æ ‡ç­¾ï¼Œè·¨è¶Š39ä¸ªè¯­æ–™åº“ã€æ¶‰åŠå¤šç§è¯­è¨€å’Œè¯è¯­æ¡†æ¶ï¼Œå¸¦æ¥å¤šè¯­ç§å’Œè·¨å½¢å¼ä¸»ä¹‰çš„æŒ‘æˆ˜ã€‚ç ”ç©¶é€šè¿‡å¾®è°ƒå¤šè¯­è¨€BERTæ¨¡å‹å¹¶å¼•å…¥åˆ†å±‚åŒé€‚é…å™¨å¯¹æ¯”å­¦ä¹ æ¨¡å‹HiDACï¼Œå‘ç°å¤§å‹è½¬æ¢å™¨æ¨¡å‹ç²¾åº¦è¾ƒé«˜ä½†æ”¹è¿›æœ‰é™ï¼Œè§£å†»é¡¶å±‚75%çš„ç¼–ç å™¨å±‚å¯å®ç°ä¸å…¨å¾®è°ƒç›¸å½“çš„æ€§èƒ½ä½†è®­ç»ƒå‚æ•°æ›´å°‘ã€‚åŸºäºæç¤ºçš„æ¨¡å‹æ˜¾è‘—è½åäºå¾®è°ƒåçš„è½¬æ¢å™¨ï¼Œè€ŒHiDACåœ¨ä¿æŒè¾ƒé«˜å‚æ•°æ•ˆç‡çš„åŒæ—¶å®ç°äº†æœ€é«˜æ€»ä½“ç²¾åº¦ï¼ˆ67.5%ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»»åŠ¡3åœ¨DISRPT 2025å…±äº«ä»»åŠ¡ä¸­å¼•å…¥äº†è·¨å¤šç§è¯­è¨€å’Œè¯è¯­æ¡†æ¶çš„ç»Ÿä¸€çš„17ä¸ªè¯è¯­å…³ç³»æ ‡ç­¾ã€‚</li>
<li>ä»»åŠ¡é¢ä¸´æ˜¾è‘—çš„å¤šè¯­ç§å’Œè·¨å½¢å¼ä¸»ä¹‰æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡å¾®è°ƒå¤šè¯­è¨€BERTæ¨¡å‹ï¼ˆmBERTç­‰ï¼‰å»ºç«‹åŸºå‡†çº¿ï¼Œå¹¶ç ”ç©¶ä¸¤ç§è®ºå…ƒæ’åºç­–ç•¥å’Œé€å±‚è§£å†»æ¯”ä¾‹å¯¹æ€§èƒ½çš„å½±å“ã€‚</li>
<li>è¯„ä¼°äº†åŸºäºæç¤ºçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚</li>
<li>å¼•å…¥HiDACæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨è¯è¯­å…³ç³»åˆ†ç±»ä»»åŠ¡ä¸Šå®ç°äº†æœ€é«˜æ€»ä½“ç²¾åº¦ï¼ˆ67.5%ï¼‰ã€‚</li>
<li>å¤§å‹è½¬æ¢å™¨æ¨¡å‹ç²¾åº¦é«˜ï¼Œä½†æ”¹è¿›æœ‰é™ï¼›è§£å†»é¡¶å±‚ç¼–ç å™¨å±‚å¯æœ‰æ•ˆæé«˜å‚æ•°æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16903">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-25ed8809aeba6654fc0c1c9e169c536a" align="middle">
<img src="https://picx.zhimg.com/v2-64159dfc611a516418501cb106b6f1ab" align="middle">
<img src="https://picx.zhimg.com/v2-09575b2d65f13cb50683ac1c0129b830" align="middle">
<img src="https://picx.zhimg.com/v2-3b96eb20d58c0c8c8a82863e9975d3c0" align="middle">
<img src="https://picx.zhimg.com/v2-31684160c2be7d5f7d9a805e1f644f64" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DA-Font-Few-Shot-Font-Generation-via-Dual-Attention-Hybrid-Integration"><a href="#DA-Font-Few-Shot-Font-Generation-via-Dual-Attention-Hybrid-Integration" class="headerlink" title="DA-Font: Few-Shot Font Generation via Dual-Attention Hybrid Integration"></a>DA-Font: Few-Shot Font Generation via Dual-Attention Hybrid Integration</h2><p><strong>Authors:Weiran Chen, Guiqian Zhu, Ying Li, Yi Ji, Chunping Liu</strong></p>
<p>Few-shot font generation aims to create new fonts with a limited number of glyph references. It can be used to significantly reduce the labor cost of manual font design. However, due to the variety and complexity of font styles, the results generated by existing methods often suffer from visible defects, such as stroke errors, artifacts and blurriness. To address these issues, we propose DA-Font, a novel framework which integrates a Dual-Attention Hybrid Module (DAHM). Specifically, we introduce two synergistic attention blocks: the component attention block that leverages component information from content images to guide the style transfer process, and the relation attention block that further refines spatial relationships through interacting the content feature with both original and stylized component-wise representations. These two blocks collaborate to preserve accurate character shapes and stylistic textures. Moreover, we also design a corner consistency loss and an elastic mesh feature loss to better improve geometric alignment. Extensive experiments show that our DA-Font outperforms the state-of-the-art methods across diverse font styles and characters, demonstrating its effectiveness in enhancing structural integrity and local fidelity. The source code can be found at \href{<a target="_blank" rel="noopener" href="https://github.com/wrchen2001/DA-Font%7D%7B/textit%7Bhttps://github.com/wrchen2001/DA-Font%7D%7D">https://github.com/wrchen2001/DA-Font}{\textit{https://github.com/wrchen2001/DA-Font}}</a>. </p>
<blockquote>
<p>åŸºäºå°‘æ ·æœ¬çš„å­—ä½“ç”Ÿæˆæ—¨åœ¨ä½¿ç”¨æœ‰é™çš„å­—ç¬¦å¼•ç”¨ç”Ÿæˆæ–°çš„å­—ä½“ã€‚å®ƒå¯å¤§å¤§å‡å°‘æ‰‹åŠ¨å­—ä½“è®¾è®¡çš„æˆæœ¬ã€‚ç„¶è€Œï¼Œç”±äºå­—ä½“çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ï¼Œç°æœ‰æ–¹æ³•ç”Ÿæˆçš„ç»“æœå¾€å¾€å­˜åœ¨æ˜æ˜¾çš„ç¼ºé™·ï¼Œå¦‚ç¬”ç”»é”™è¯¯ã€ä¼ªå½±å’Œæ¨¡ç³Šç­‰ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DA-Fontï¼Œè¿™æ˜¯ä¸€ä¸ªé›†æˆäº†åŒæ³¨æ„åŠ›æ··åˆæ¨¡å—ï¼ˆDAHMï¼‰çš„æ–°å‹æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§ååŒæ³¨æ„åŠ›å—ï¼šç»„ä»¶æ³¨æ„åŠ›å—ï¼Œå®ƒåˆ©ç”¨å†…å®¹å›¾åƒä¸­çš„ç»„ä»¶ä¿¡æ¯æ¥å¼•å¯¼é£æ ¼è½¬æ¢è¿‡ç¨‹ï¼›å…³ç³»æ³¨æ„åŠ›å—åˆ™é€šè¿‡äº¤äº’å†…å®¹ç‰¹å¾ä¸åŸå§‹å’Œé£æ ¼åŒ–ç»„ä»¶è¡¨ç¤ºæ¥è¿›ä¸€æ­¥æ”¹è¿›ç©ºé—´å…³ç³»ã€‚è¿™ä¸¤ä¸ªå—ååŒå·¥ä½œï¼Œä¿ç•™äº†å‡†ç¡®çš„å­—ç¬¦å½¢çŠ¶å’Œé£æ ¼çº¹ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†è§’ä¸€è‡´æ€§æŸå¤±å’Œå¼¹æ€§ç½‘æ ¼ç‰¹å¾æŸå¤±æ¥æ›´å¥½åœ°æ”¹è¿›å‡ ä½•å¯¹é½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DA-Fontåœ¨å¤šç§å­—ä½“é£æ ¼å’Œå­—ç¬¦ä¸Šè¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨æé«˜ç»“æ„å®Œæ•´æ€§å’Œå±€éƒ¨ä¿çœŸåº¦æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æºä»£ç å¯ä»¥åœ¨<em>\href{<a target="_blank" rel="noopener" href="https://github.com/wrchen2001/DA-Font%7D%7B/textless%7B%7Dhttps://github.com/wrchen2001/DA-Font/textgreater%7B%7D">https://github.com/wrchen2001/DA-Font}{\textless{}https://github.com/wrchen2001/DA-Font\textgreater{}</a></em>}æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16632v1">PDF</a> Accepted by ACM MM 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå°‘æ ·æœ¬çš„å­—ä½“ç”ŸæˆæŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡æœ‰é™çš„å­—å½¢å‚è€ƒæ¥åˆ›å»ºæ–°å­—ä½“ï¼Œé™ä½æ‰‹åŠ¨å­—ä½“è®¾è®¡çš„æˆæœ¬ã€‚ä¸ºè§£å†³ç°æœ‰æ–¹æ³•ç”Ÿæˆç»“æœä¸­å­˜åœ¨çš„ç¬”ç”»é”™è¯¯ã€ä¼ªå½±å’Œæ¨¡ç³Šç­‰å¯è§ç¼ºé™·ï¼Œæå‡ºäº†æ–°å‹çš„DA-Fontæ¡†æ¶ï¼Œé›†æˆäº†åŒæ³¨æ„åŠ›æ··åˆæ¨¡å—ï¼ˆDAHMï¼‰ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªååŒå·¥ä½œçš„æ³¨æ„åŠ›å—ï¼šç»„ä»¶æ³¨æ„åŠ›å—åˆ©ç”¨å†…å®¹å›¾åƒä¸­çš„ç»„ä»¶ä¿¡æ¯æ¥æŒ‡å¯¼é£æ ¼è½¬æ¢è¿‡ç¨‹ï¼Œå…³ç³»æ³¨æ„åŠ›å—åˆ™é€šè¿‡äº¤äº’å†…å®¹ç‰¹å¾ä¸åŸå§‹å’Œé£æ ¼åŒ–ç»„ä»¶è¡¨ç¤ºæ¥è¿›ä¸€æ­¥è°ƒæ•´ç©ºé—´å…³ç³»ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†è§’ä¸€è‡´æ€§æŸå¤±å’Œå¼¹æ€§ç½‘æ ¼ç‰¹å¾æŸå¤±æ¥æ”¹è¿›å‡ ä½•å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼ŒDA-Fontåœ¨å¤šç§å­—ä½“é£æ ¼å’Œå­—ç¬¦ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæé«˜äº†ç»“æ„å®Œæ•´æ€§å’Œå±€éƒ¨ä¿çœŸåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°‘æ ·æœ¬å­—ä½“ç”Ÿæˆæ—¨åœ¨å‡å°‘æ‰‹åŠ¨å­—ä½“è®¾è®¡çš„åŠ³åŠ¨åŠ›æˆæœ¬ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç”Ÿæˆçš„å­—ä½“å¸¸å­˜åœ¨ç¬”ç”»é”™è¯¯ã€ä¼ªå½±å’Œæ¨¡ç³Šç­‰ç¼ºé™·ã€‚</li>
<li>DA-Fontæ¡†æ¶é›†æˆäº†åŒæ³¨æ„åŠ›æ··åˆæ¨¡å—ï¼ˆDAHMï¼‰æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>DA-FontåŒ…å«ä¸¤ä¸ªååŒçš„æ³¨æ„åŠ›å—ï¼šç»„ä»¶æ³¨æ„åŠ›å—å’Œå…³ç³»æ³¨æ„åŠ›å—ã€‚</li>
<li>ç»„ä»¶æ³¨æ„åŠ›å—åˆ©ç”¨å†…å®¹å›¾åƒç»„ä»¶ä¿¡æ¯æŒ‡å¯¼é£æ ¼è½¬æ¢ã€‚</li>
<li>å…³ç³»æ³¨æ„åŠ›å—é€šè¿‡äº¤äº’å†…å®¹ç‰¹å¾ä¸åŸå§‹å’Œé£æ ¼åŒ–ç»„ä»¶è¡¨ç¤ºæ¥è°ƒæ•´ç©ºé—´å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16632">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fb2aac555e18e6909ae56c9f9eb2c015" align="middle">
<img src="https://picx.zhimg.com/v2-dd2677b5c3afbba723b180cec23c8c6f" align="middle">
<img src="https://picx.zhimg.com/v2-066fe2605becc1cb2d99f033406295c3" align="middle">
<img src="https://picx.zhimg.com/v2-e6cb738a3abcbb1c19d8a82874397a48" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Toward-Medical-Deepfake-Detection-A-Comprehensive-Dataset-and-Novel-Method"><a href="#Toward-Medical-Deepfake-Detection-A-Comprehensive-Dataset-and-Novel-Method" class="headerlink" title="Toward Medical Deepfake Detection: A Comprehensive Dataset and Novel   Method"></a>Toward Medical Deepfake Detection: A Comprehensive Dataset and Novel   Method</h2><p><strong>Authors:Shuaibo Li, Zhaohu Xing, Hongqiu Wang, Pengfei Hao, Xingyu Li, Zekai Liu, Lei Zhu</strong></p>
<p>The rapid advancement of generative AI in medical imaging has introduced both significant opportunities and serious challenges, especially the risk that fake medical images could undermine healthcare systems. These synthetic images pose serious risks, such as diagnostic deception, financial fraud, and misinformation. However, research on medical forensics to counter these threats remains limited, and there is a critical lack of comprehensive datasets specifically tailored for this field. Additionally, existing media forensic methods, which are primarily designed for natural or facial images, are inadequate for capturing the distinct characteristics and subtle artifacts of AI-generated medical images. To tackle these challenges, we introduce \textbf{MedForensics}, a large-scale medical forensics dataset encompassing six medical modalities and twelve state-of-the-art medical generative models. We also propose \textbf{DSKI}, a novel \textbf{D}ual-\textbf{S}tage \textbf{K}nowledge \textbf{I}nfusing detector that constructs a vision-language feature space tailored for the detection of AI-generated medical images. DSKI comprises two core components: 1) a cross-domain fine-trace adapter (CDFA) for extracting subtle forgery clues from both spatial and noise domains during training, and 2) a medical forensic retrieval module (MFRM) that boosts detection accuracy through few-shot retrieval during testing. Experimental results demonstrate that DSKI significantly outperforms both existing methods and human experts, achieving superior accuracy across multiple medical modalities. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒç”Ÿæˆäººå·¥æ™ºèƒ½çš„å¿«é€Ÿè¿›æ­¥æ—¢å¸¦æ¥äº†é‡å¤§æœºé‡ï¼Œä¹Ÿå¸¦æ¥äº†ä¸¥å³»æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å‡å†’åŒ»å­¦å½±åƒå¯èƒ½ç ´ååŒ»ç–—ç³»ç»Ÿçš„é£é™©ã€‚è¿™äº›åˆæˆå›¾åƒå¸¦æ¥äº†ä¸¥é‡çš„é£é™©ï¼Œå¦‚è¯Šæ–­æ¬ºéª—ã€é‡‘èæ¬ºè¯ˆå’Œè¯¯å¯¼ä¿¡æ¯ã€‚ç„¶è€Œï¼Œé’ˆå¯¹è¿™äº›å¨èƒçš„åŒ»ç–—å–è¯ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œè€Œä¸”ä¸“é—¨é’ˆå¯¹è¯¥é¢†åŸŸçš„ç»¼åˆæ•°æ®é›†ä¸¥é‡ç¼ºä¹ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åª’ä½“å–è¯æ–¹æ³•ä¸»è¦è®¾è®¡ç”¨äºè‡ªç„¶æˆ–é¢éƒ¨å›¾åƒï¼Œå› æ­¤ä¸è¶³ä»¥æ•æ‰äººå·¥æ™ºèƒ½ç”Ÿæˆçš„åŒ»å­¦å½±åƒçš„ç‹¬ç‰¹ç‰¹å¾å’Œç»†å¾®ä¼ªå½±ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MedForensicsæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„åŒ»ç–—å–è¯æ•°æ®é›†ï¼Œæ¶µç›–å…­ç§åŒ»å­¦æ¨¡æ€å’ŒåäºŒç§æœ€å…ˆè¿›çš„åŒ»å­¦ç”Ÿæˆæ¨¡å‹ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†DSKIï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŒé˜¶æ®µçŸ¥è¯†æ³¨å…¥æ£€æµ‹å™¨ï¼ˆDual-Stage Knowledge Infusing detectorï¼‰ï¼Œå®ƒæ„å»ºäº†ä¸€ä¸ªå®šåˆ¶çš„è§†è§‰è¯­è¨€ç‰¹å¾ç©ºé—´ï¼Œç”¨äºæ£€æµ‹äººå·¥æ™ºèƒ½ç”Ÿæˆçš„åŒ»å­¦å½±åƒã€‚DSKIåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š1ï¼‰è·¨åŸŸç²¾ç»†è·Ÿè¸ªé€‚é…å™¨ï¼ˆCDFAï¼‰ï¼Œç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»ç©ºé—´å’Œå™ªå£°åŸŸä¸­æå–å¾®å¦™çš„ä¼ªé€ çº¿ç´¢ï¼›2ï¼‰åŒ»ç–—å–è¯æ£€ç´¢æ¨¡å—ï¼ˆMFRMï¼‰ï¼Œé€šè¿‡æµ‹è¯•è¿‡ç¨‹ä¸­çš„å°‘é‡æ£€ç´¢æ¥æé«˜æ£€æµ‹å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDSKIåœ¨å¤šä¸ªåŒ»å­¦æ¨¡æ€ä¸Šçš„æ£€æµ‹ç²¾åº¦æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•å’Œäººç±»ä¸“å®¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15711v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    åŒ»å­¦æˆåƒé¢†åŸŸç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å¿«é€Ÿè¿›æ­¥å¸¦æ¥äº†é‡è¦æœºé‡å’Œä¸¥å³»æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯è™šå‡åŒ»å­¦å›¾åƒå¯èƒ½ç ´ååŒ»ç–—ç³»ç»Ÿçš„é£é™©ã€‚è¿™äº›åˆæˆå›¾åƒå¯èƒ½é€ æˆè¯Šæ–­æ¬ºéª—ã€é‡‘èæ¬ºè¯ˆå’Œè¯¯å¯¼ä¿¡æ¯ç­‰ä¸¥é‡é£é™©ã€‚ç„¶è€Œï¼Œé’ˆå¯¹è¿™äº›å¨èƒçš„åŒ»å­¦ç ”ç©¶ä»å—é™äºä¸“é—¨çš„ç»¼åˆæ•°æ®é›†ã€‚ç°æœ‰åª’ä½“é‰´å®šæ–¹æ³•ä¸»è¦ç”¨äºè‡ªç„¶å›¾åƒæˆ–é¢éƒ¨å›¾åƒï¼Œæ— æ³•æ•æ‰åŒ»å­¦å›¾åƒAIç”Ÿæˆç‰©çš„ç‹¬ç‰¹ç‰¹å¾å’Œç»†å¾®ç—•è¿¹ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºMedForensicsæ•°æ®é›†ï¼Œæ¶µç›–å…­ç§åŒ»å­¦æ¨¡æ€å’ŒåäºŒç§æœ€å…ˆè¿›çš„åŒ»å­¦ç”Ÿæˆæ¨¡å‹çš„å¤§è§„æ¨¡åŒ»å­¦é‰´å®šæ•°æ®é›†ã€‚åŒæ—¶æå‡ºDSKIæ£€æµ‹å™¨ï¼Œæ„å»ºä¸“é—¨ç”¨äºæ£€æµ‹AIç”Ÿæˆçš„åŒ»å­¦å›¾åƒçš„è§†è§‰è¯­è¨€ç‰¹å¾ç©ºé—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDSKIåœ¨å¤šä¸ªåŒ»å­¦æ¨¡æ€ä¸Šçš„æ£€æµ‹ç²¾åº¦æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•å’Œäººç±»ä¸“å®¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨åŒ»å­¦æˆåƒä¸­çš„è¿›æ­¥å¸¦æ¥äº†é‡å¤§æœºé‡å’ŒæŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯è™šå‡åŒ»å­¦å›¾åƒå¯¹åŒ»ç–—ç³»ç»Ÿçš„æ½œåœ¨å¨èƒã€‚</li>
<li>åˆæˆåŒ»å­¦å›¾åƒå­˜åœ¨è¯Šæ–­æ¬ºéª—ã€é‡‘èæ¬ºè¯ˆå’Œè¯¯å¯¼ä¿¡æ¯ç­‰é£é™©ã€‚</li>
<li>ç›®å‰é’ˆå¯¹åŒ»å­¦é¢†åŸŸçš„é‰´å®šç ”ç©¶ä»é¢ä¸´ç¼ºä¹ä¸“é—¨çš„ç»¼åˆæ•°æ®é›†çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰åª’ä½“é‰´å®šæ–¹æ³•ä¸»è¦é¢å‘è‡ªç„¶æˆ–é¢éƒ¨å›¾åƒï¼Œä¸é€‚ç”¨äºåŒ»å­¦å›¾åƒAIç”Ÿæˆç‰©çš„æ£€æµ‹ã€‚</li>
<li>æ¨å‡ºMedForensicsæ•°æ®é›†ï¼Œæ¶µç›–å¤šç§åŒ»å­¦æ¨¡æ€å’Œå…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹ï¼Œä¸ºåŒ»å­¦é‰´å®šæä¾›å¤§è§„æ¨¡èµ„æºã€‚</li>
<li>æå‡ºDSKIæ£€æµ‹å™¨ï¼Œé€šè¿‡åŒé˜¶æ®µçŸ¥è¯†æ³¨å…¥ï¼ˆDSKIï¼‰æ„å»ºä¸“é—¨çš„è§†è§‰è¯­è¨€ç‰¹å¾ç©ºé—´ç”¨äºæ£€æµ‹AIç”Ÿæˆçš„åŒ»å­¦å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15711">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f9287ff5550f65f41b97b62dfb497c4" align="middle">
<img src="https://picx.zhimg.com/v2-349ae97a427d9e296a6819a9806f5cec" align="middle">
<img src="https://picx.zhimg.com/v2-f628618b03d06062ec451a896c977de6" align="middle">
<img src="https://picx.zhimg.com/v2-9dad41ddab225c17c7b17fee7c30f3ef" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SCENEFORGE-Enhancing-3D-text-alignment-with-Structured-Scene-Compositions"><a href="#SCENEFORGE-Enhancing-3D-text-alignment-with-Structured-Scene-Compositions" class="headerlink" title="SCENEFORGE: Enhancing 3D-text alignment with Structured Scene   Compositions"></a>SCENEFORGE: Enhancing 3D-text alignment with Structured Scene   Compositions</h2><p><strong>Authors:Cristian Sbrolli, Matteo Matteucci</strong></p>
<p>The whole is greater than the sum of its parts-even in 3D-text contrastive learning. We introduce SceneForge, a novel framework that enhances contrastive alignment between 3D point clouds and text through structured multi-object scene compositions. SceneForge leverages individual 3D shapes to construct multi-object scenes with explicit spatial relations, pairing them with coherent multi-object descriptions refined by a large language model. By augmenting contrastive training with these structured, compositional samples, SceneForge effectively addresses the scarcity of large-scale 3D-text datasets, significantly enriching data complexity and diversity. We systematically investigate critical design elements, such as the optimal number of objects per scene, the proportion of compositional samples in training batches, and scene construction strategies. Extensive experiments demonstrate that SceneForge delivers substantial performance gains across multiple tasks, including zero-shot classification on ModelNet, ScanObjNN, Objaverse-LVIS, and ScanNet, as well as few-shot part segmentation on ShapeNetPart. SceneForgeâ€™s compositional augmentations are model-agnostic, consistently improving performance across multiple encoder architectures. Moreover, SceneForge improves 3D visual question answering on ScanQA, generalizes robustly to retrieval scenarios with increasing scene complexity, and showcases spatial reasoning capabilities by adapting spatial configurations to align precisely with textual instructions. </p>
<blockquote>
<p>æ•´ä½“å¤§äºéƒ¨åˆ†ä¹‹å’Œï¼Œç”šè‡³åœ¨3Dæ–‡æœ¬å¯¹æ¯”å­¦ä¹ ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬æ¨å‡ºäº†SceneForgeï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–å¤šç›®æ ‡åœºæ™¯ç»„åˆï¼Œå¢å¼ºäº†3Dç‚¹äº‘å’Œæ–‡æœ¬ä¹‹é—´çš„å¯¹æ¯”å¯¹é½ã€‚SceneForgeåˆ©ç”¨å•ä¸ª3Då½¢çŠ¶æ„å»ºå…·æœ‰æ˜ç¡®ç©ºé—´å…³ç³»çš„å¤šç›®æ ‡åœºæ™¯ï¼Œå°†å®ƒä»¬ä¸ç”±å¤§å‹è¯­è¨€æ¨¡å‹å®Œå–„çš„ä¸€è‡´å¤šç›®æ ‡æè¿°ç›¸é…å¯¹ã€‚é€šè¿‡å°†è¿™äº›ç»“æ„åŒ–çš„ã€ç»„åˆçš„æ ·æœ¬å¢å¼ºå¯¹æ¯”è®­ç»ƒï¼ŒSceneForgeæœ‰æ•ˆåœ°è§£å†³äº†å¤§è§„æ¨¡3Dæ–‡æœ¬æ•°æ®é›†çš„ç¨€ç¼ºé—®é¢˜ï¼Œæå¤§åœ°ä¸°å¯Œäº†æ•°æ®çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†å…³é”®çš„è®¾è®¡å…ƒç´ ï¼Œå¦‚æ¯ä¸ªåœºæ™¯ä¸­çš„æœ€ä½³ç›®æ ‡æ•°ã€è®­ç»ƒæ‰¹æ¬¡ä¸­ç»„åˆæ ·æœ¬çš„æ¯”ä¾‹ä»¥åŠåœºæ™¯æ„å»ºç­–ç•¥ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSceneForgeåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒ…æ‹¬ModelNetã€ScanObjNNã€Objaverse-LVISå’ŒScanNetä¸Šçš„é›¶æ ·æœ¬åˆ†ç±»ï¼Œä»¥åŠShapeNetPartä¸Šçš„å°‘æ ·æœ¬éƒ¨åˆ†åˆ†å‰²ã€‚SceneForgeçš„ç»„åˆå¢å¼ºæ˜¯æ¨¡å‹æ— å…³çš„ï¼Œåœ¨å¤šç§ç¼–ç å™¨æ¶æ„ä¸Šéƒ½èƒ½æé«˜æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒSceneForgeæ”¹è¿›äº†ScanQAçš„3Dè§†è§‰é—®ç­”ï¼Œåœ¨åœºæ™¯å¤æ‚æ€§ä¸æ–­å¢åŠ çš„æ£€ç´¢åœºæ™¯ä¸­å®ç°äº†ç¨³å¥çš„æ³›åŒ–ï¼Œå¹¶é€šè¿‡é€‚åº”ç©ºé—´é…ç½®æ¥å±•ç¤ºç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œä»¥ç²¾ç¡®ç¬¦åˆæ–‡æœ¬æŒ‡ä»¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15693v1">PDF</a> to appear in NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>SceneForgeæ¡†æ¶é€šè¿‡ç»“æ„åŒ–çš„å¤šå¯¹è±¡åœºæ™¯ç»„åˆï¼Œå¢å¼ºäº†3Dç‚¹äº‘å’Œæ–‡æœ¬ä¹‹é—´çš„å¯¹æ¯”å¯¹é½ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä¸ªä½“3Då½¢çŠ¶æ„å»ºå…·æœ‰æ˜ç¡®ç©ºé—´å…³ç³»çš„å¤šå¯¹è±¡åœºæ™¯ï¼Œå¹¶ä¸ç”±å¤§å‹è¯­è¨€æ¨¡å‹ç²¾ç‚¼çš„å¤šå¯¹è±¡æè¿°ç›¸é…å¯¹ã€‚é€šè¿‡åˆ©ç”¨è¿™äº›ç»“æ„åŒ–ã€ç»„åˆæ ·æœ¬å¢å¼ºå¯¹æ¯”è®­ç»ƒï¼ŒSceneForgeæœ‰æ•ˆè§£å†³å¤§è§„æ¨¡3D-æ–‡æœ¬æ•°æ®é›†ç¨€ç¼ºé—®é¢˜ï¼Œæå¤§åœ°ä¸°å¯Œäº†æ•°æ®å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SceneForgeæ˜¯ä¸€ä¸ªç”¨äºå¢å¼º3Dç‚¹äº‘å’Œæ–‡æœ¬ä¹‹é—´å¯¹æ¯”å¯¹é½çš„æ¡†æ¶ã€‚</li>
<li>é€šè¿‡æ„å»ºå…·æœ‰æ˜ç¡®ç©ºé—´å…³ç³»çš„å¤šå¯¹è±¡åœºæ™¯ï¼ŒSceneForgeè§£å†³äº†å¤§è§„æ¨¡3D-æ–‡æœ¬æ•°æ®é›†çš„ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>SceneForgeåˆ©ç”¨ä¸ªä½“3Då½¢çŠ¶å’Œå¤šå¯¹è±¡æè¿°è¿›è¡Œé…å¯¹ï¼Œå…¶ä¸­æè¿°ç”±å¤§å‹è¯­è¨€æ¨¡å‹ç²¾ç‚¼ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡ç»“æ„åŒ–ã€ç»„åˆæ ·æœ¬ä¸°å¯Œå¯¹æ¯”è®­ç»ƒï¼Œæœ‰æ•ˆæé«˜äº†æ•°æ®å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚</li>
<li>SceneForgeåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒ…æ‹¬ModelNetã€ScanObjNNã€Objaverse-LVISå’ŒScanNetçš„é›¶æ ·æœ¬åˆ†ç±»ï¼Œä»¥åŠShapeNetPartçš„å°‘é‡éƒ¨åˆ†åˆ†å‰²ã€‚</li>
<li>SceneForgeçš„ç»„åˆå¢å¼ºæ–¹æ³•æ˜¯æ¨¡å‹æ— å…³çš„ï¼Œå¯ä»¥åœ¨å¤šä¸ªç¼–ç å™¨æ¶æ„ä¸Šå®ç°æ€§èƒ½æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15693">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ea4e776da24bc314e9eee7efe53f611" align="middle">
<img src="https://picx.zhimg.com/v2-88ba3f49cbebe88aa79e0a7efaf748a2" align="middle">
<img src="https://picx.zhimg.com/v2-e19f2b18482c1e6ed2772c67470eea4a" align="middle">
<img src="https://picx.zhimg.com/v2-06cf11e090f5bf37cc4276e9432d5ad3" align="middle">
<img src="https://picx.zhimg.com/v2-6928a08af1790f18ac5d19e49eca0ffb" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="State-of-the-Art-Dysarthric-Speech-Recognition-with-MetaICL-for-on-the-fly-Personalization"><a href="#State-of-the-Art-Dysarthric-Speech-Recognition-with-MetaICL-for-on-the-fly-Personalization" class="headerlink" title="State-of-the-Art Dysarthric Speech Recognition with MetaICL for   on-the-fly Personalization"></a>State-of-the-Art Dysarthric Speech Recognition with MetaICL for   on-the-fly Personalization</h2><p><strong>Authors:Dhruuv Agarwal, Harry Zhang, Yang Yu, Quan Wang</strong></p>
<p>Personalizing Automatic Speech Recognition (ASR) for dysarthric speech is crucial but challenging due to training and storing of individual user adapters. We propose a hybrid meta-training method for a single model, excelling in zero-shot and few-shot on-the-fly personalization via in-context learning (ICL). Measuring Word Error Rate (WER) on state-of-the-art subsets, the model achieves 13.9% WER on Euphonia which surpasses speaker-independent baselines (17.5% WER) and rivals user-specific personalized models. On SAP Test 1, its 5.3% WER significantly bests the 8% from even personalized adapters. We also demonstrate the importance of example curation, where an oracle text-similarity method shows 5 curated examples can achieve performance similar to 19 randomly selected ones, highlighting a key area for future efficiency gains. Finally, we conduct data ablations to measure the data efficiency of this approach. This work presents a practical, scalable, and personalized solution. </p>
<blockquote>
<p>ä¸ªæ€§åŒ–è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å¯¹äºå‘éŸ³éšœç¢è€…çš„è¯­éŸ³è‡³å…³é‡è¦ï¼Œä½†ç”±äºéœ€è¦è®­ç»ƒå¹¶å­˜å‚¨ä¸ªåˆ«ç”¨æˆ·é€‚é…å™¨ï¼Œè¿™å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆå…ƒè®­ç»ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€‚ç”¨äºå•ä¸€æ¨¡å‹ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å®ç°é›¶æ ·æœ¬å’Œå°‘é‡æ ·æœ¬å³æ—¶ä¸ªæ€§åŒ–ï¼Œè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚åœ¨æœ€æ–°å­é›†ä¸Šæµ‹é‡è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œè¯¥æ¨¡å‹åœ¨Euphoniaä¸Šå®ç°äº†13.9%çš„WERï¼Œè¶…è¿‡äº†éè¯´è¯äººä¾èµ–çš„åŸºçº¿ï¼ˆ17.5%çš„WERï¼‰ï¼Œå¹¶ä¸ç”¨æˆ·ç‰¹å®šçš„ä¸ªæ€§åŒ–æ¨¡å‹ç›¸åŒ¹æ•Œã€‚åœ¨SAPæµ‹è¯•1ä¸­ï¼Œå…¶5.3%çš„WERæ˜æ˜¾ä¼˜äºä¸ªæ€§åŒ–é€‚é…å™¨çš„8%ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†ç¤ºä¾‹é€‰æ‹©çš„é‡è¦æ€§ï¼Œå…¶ä¸­ç†æƒ³æ–‡æœ¬ç›¸ä¼¼æ€§æ–¹æ³•è¡¨æ˜ï¼Œé€šè¿‡ç²¾å¿ƒæŒ‘é€‰çš„5ä¸ªç¤ºä¾‹å¯ä»¥å®ç°ä¸éšæœºé€‰æ‹©çš„19ä¸ªç¤ºä¾‹ç›¸ä¼¼çš„æ€§èƒ½ï¼Œè¿™çªæ˜¾äº†æœªæ¥æé«˜æ•ˆç‡çš„å…³é”®é¢†åŸŸã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡æ•°æ®æ¶ˆèæ³•æµ‹é‡äº†è¿™ç§æ–¹æ³•çš„æ•°æ®æ•ˆç‡ã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä¸€ä¸ªå®ç”¨ã€å¯æ‰©å±•å’Œä¸ªæ€§åŒ–çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15516v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å£é½¿ä¸æ¸…ï¼ˆdysarthric speechï¼‰çš„è¯­éŸ³è¯†åˆ«é—®é¢˜ï¼Œæå‡ºä¸€ç§æ··åˆå…ƒè®­ç»ƒæ³•ã€‚æ­¤æ–¹æ³•åœ¨ä¸€å•æ¨¡å‹ä¸Šå³å¯å®ç°é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬å®æ—¶ä¸ªæ€§åŒ–ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æå‡æ€§èƒ½ã€‚åœ¨æœ€æ–°å­é›†ä¸Šæµ‹é‡è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œæ¨¡å‹åœ¨Euphoniaä¸Šçš„è¡¨ç°ä¼˜äºéä¸ªæ€§åŒ–åŸºçº¿å¹¶æ¥è¿‘ä¸ªæ€§åŒ–æ¨¡å‹ï¼ŒåŒæ—¶åœ¨SAP Test 1ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¸ªæ€§åŒ–é€‚é…å™¨ã€‚æ­¤å¤–ï¼Œè¿˜å¼ºè°ƒäº†æ ·æœ¬é€‰æ‹©çš„é‡è¦æ€§ï¼Œå¹¶å±•ç¤ºäº†é€šè¿‡ä¼˜åŒ–é€‰æ‹©æ–¹æ³•å¯æé«˜æ•ˆç‡ã€‚æœ€åï¼Œé€šè¿‡æ•°æ®æ¶ˆèå®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æ•°æ®æ•ˆç‡ã€‚æœ¬ç ”ç©¶æä¾›äº†ä¸€ç§å®ç”¨ã€å¯æ‰©å±•ä¸”ä¸ªæ€§åŒ–çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§æ··åˆå…ƒè®­ç»ƒæ³•ç”¨äºè§£å†³å£é½¿ä¸æ¸…çš„è¯­éŸ³è¯†åˆ«ä¸ªæ€§åŒ–é—®é¢˜ã€‚</li>
<li>æ–¹æ³•åœ¨ä¸€å•æ¨¡å‹ä¸Šå®ç°é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬å®æ—¶ä¸ªæ€§åŒ–ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ æå‡æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨Euphoniaä¸Šçš„è¯é”™è¯¯ç‡è¡¨ç°ä¼˜äºéä¸ªæ€§åŒ–åŸºçº¿å¹¶æ¥è¿‘ä¸ªæ€§åŒ–æ¨¡å‹ã€‚</li>
<li>åœ¨SAP Test 1ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¸ªæ€§åŒ–é€‚é…å™¨ã€‚</li>
<li>æ ·æœ¬é€‰æ‹©å¯¹æ€§èƒ½æœ‰é‡è¦å½±å“ï¼Œä¼˜åŒ–é€‰æ‹©æ–¹æ³•å¯æé«˜æ•ˆç‡ã€‚</li>
<li>é€šè¿‡æ•°æ®æ¶ˆèå®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„å®ç”¨æ€§ã€å¯æ‰©å±•æ€§åŠå…¶æ•°æ®æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15516">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca6256d8a021a70dc2dea3c64187936b" align="middle">
<img src="https://picx.zhimg.com/v2-3b7043637b34136b60da07a7c888af2e" align="middle">
<img src="https://picx.zhimg.com/v2-1bab65e760f22478295ee0ebc7b0efdd" align="middle">
<img src="https://picx.zhimg.com/v2-1062664dedbe8086f6230f25e5b25548" align="middle">
<img src="https://picx.zhimg.com/v2-54f0df6fa2fc4fc22baa5b65e130a04c" align="middle">
<img src="https://picx.zhimg.com/v2-400a70a4d1e620344f2d806214d00477" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Seeing-3D-Through-2D-Lenses-3D-Few-Shot-Class-Incremental-Learning-via-Cross-Modal-Geometric-Rectification"><a href="#Seeing-3D-Through-2D-Lenses-3D-Few-Shot-Class-Incremental-Learning-via-Cross-Modal-Geometric-Rectification" class="headerlink" title="Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via   Cross-Modal Geometric Rectification"></a>Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via   Cross-Modal Geometric Rectification</h2><p><strong>Authors:Tuo Xiang, Xuemiao Xu, Bangzhen Liu, Jinyi Li, Yong Li, Shengfeng He</strong></p>
<p>The rapid growth of 3D digital content necessitates expandable recognition systems for open-world scenarios. However, existing 3D class-incremental learning methods struggle under extreme data scarcity due to geometric misalignment and texture bias. While recent approaches integrate 3D data with 2D foundation models (e.g., CLIP), they suffer from semantic blurring caused by texture-biased projections and indiscriminate fusion of geometric-textural cues, leading to unstable decision prototypes and catastrophic forgetting. To address these issues, we propose Cross-Modal Geometric Rectification (CMGR), a framework that enhances 3D geometric fidelity by leveraging CLIPâ€™s hierarchical spatial semantics. Specifically, we introduce a Structure-Aware Geometric Rectification module that hierarchically aligns 3D part structures with CLIPâ€™s intermediate spatial priors through attention-driven geometric fusion. Additionally, a Texture Amplification Module synthesizes minimal yet discriminative textures to suppress noise and reinforce cross-modal consistency. To further stabilize incremental prototypes, we employ a Base-Novel Discriminator that isolates geometric variations. Extensive experiments demonstrate that our method significantly improves 3D few-shot class-incremental learning, achieving superior geometric coherence and robustness to texture bias across cross-domain and within-domain settings. </p>
<blockquote>
<p>éšç€ä¸‰ç»´æ•°å­—å†…å®¹çš„å¿«é€Ÿå‘å±•ï¼Œå¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸‹éœ€è¦å¯æ‰©å±•çš„è¯†åˆ«ç³»ç»Ÿã€‚ç„¶è€Œï¼Œç”±äºå‡ ä½•ä¸å¯¹é½å’Œçº¹ç†åå·®ï¼Œç°æœ‰çš„ä¸‰ç»´ç±»å¢é‡å­¦ä¹ æ–¹æ³•åœ¨æç«¯æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹è¡¨ç°æŒ£æ‰ã€‚è™½ç„¶æœ€è¿‘çš„æ–¹æ³•å°†ä¸‰ç»´æ•°æ®ä¸äºŒç»´åŸºç¡€æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰ç›¸ç»“åˆï¼Œä½†å®ƒä»¬å—åˆ°çº¹ç†åå‘æŠ•å½±å’Œå‡ ä½•çº¹ç†çº¿ç´¢çš„éšæœºèåˆå¯¼è‡´çš„è¯­ä¹‰æ¨¡ç³Šçš„å½±å“ï¼Œå¯¼è‡´å†³ç­–åŸå‹ä¸ç¨³å®šå’Œç¾éš¾æ€§é—å¿˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨æ¨¡æ€å‡ ä½•æ ¡æ­£ï¼ˆCMGRï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨CLIPçš„åˆ†å±‚ç©ºé—´è¯­ä¹‰æ¥æé«˜ä¸‰ç»´å‡ ä½•ä¿çœŸåº¦ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç»“æ„æ„ŸçŸ¥å‡ ä½•æ ¡æ­£æ¨¡å—ï¼Œè¯¥æ¨¡å—é€šè¿‡æ³¨æ„åŠ›é©±åŠ¨çš„å‡ ä½•èåˆåˆ†å±‚åœ°å°†ä¸‰ç»´éƒ¨åˆ†ç»“æ„ä¸CLIPçš„ä¸­é—´ç©ºé—´å…ˆéªŒå¯¹é½ã€‚æ­¤å¤–ï¼Œçº¹ç†æ”¾å¤§æ¨¡å—åˆæˆæœ€å°ä½†å…·æœ‰åŒºåˆ†åŠ›çš„çº¹ç†ï¼Œä»¥æŠ‘åˆ¶å™ªå£°å¹¶å¢å¼ºè·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥ç¨³å®šå¢é‡åŸå‹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†åŸºç¡€-æ–°å‹é‰´åˆ«å™¨ï¼Œè¯¥é‰´åˆ«å™¨éš”ç¦»äº†å‡ ä½•å˜åŒ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ç»´å°æ ·æœ¬ç±»å¢é‡å­¦ä¹ ä¸Šæ˜¾è‘—æ”¹è¿›ï¼Œå®ç°äº†è·¨åŸŸå’ŒåŸŸå†…è®¾ç½®ä¸­çš„å‡ºè‰²å‡ ä½•è¿è´¯æ€§å’Œå¯¹çº¹ç†åå·®çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14958v2">PDF</a> ICCV2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä¸‰ç»´æ•°å­—å†…å®¹çš„å¿«é€Ÿå¢é•¿å¯¹å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸‹çš„è¯†åˆ«ç³»ç»Ÿæå‡ºçš„æ–°éœ€æ±‚ã€‚ç°æœ‰ä¸‰ç»´ç±»å¢é‡å­¦ä¹ æ–¹æ³•åœ¨æç«¯æ•°æ®ç¨€ç¼ºæƒ…å†µä¸‹å­˜åœ¨å‡ ä½•å¤±é…å’Œçº¹ç†åå·®çš„é—®é¢˜ã€‚æ–‡ç« é€šè¿‡æ•´åˆä¸‰ç»´æ•°æ®ä¸äºŒç»´åŸºç¡€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†å­˜åœ¨è¯­ä¹‰æ¨¡ç³Šçš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†è·¨æ¨¡æ€å‡ ä½•æ ¡æ­£ï¼ˆCMGRï¼‰æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨CLIPçš„åˆ†å±‚ç©ºé—´è¯­ä¹‰æ¥æé«˜ä¸‰ç»´å‡ ä½•ä¿çœŸåº¦ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ç»“æ„æ„ŸçŸ¥å‡ ä½•æ ¡æ­£æ¨¡å—å’Œçº¹ç†æ”¾å¤§æ¨¡å—ï¼Œåˆ†åˆ«å®ç°ä¸‰ç»´éƒ¨åˆ†ç»“æ„ä¸CLIPä¸­é—´ç©ºé—´å…ˆéªŒçš„å±‚æ¬¡å¯¹é½ä»¥åŠæœ€å°åˆ¤åˆ«æ€§çº¹ç†çš„åˆæˆï¼Œä»¥æŠ‘åˆ¶å™ªå£°å¹¶å¢å¼ºè·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¨³å®šå¢é‡åŸå‹ï¼Œè¿˜é‡‡ç”¨äº†åŸºç¡€-æ–°é¢–é‰´åˆ«å™¨æ¥éš”ç¦»å‡ ä½•å˜åŒ–ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†ä¸‰ç»´å°æ ·æœ¬ç±»å¢é‡å­¦ä¹ çš„å‡ ä½•ä¸€è‡´æ€§å’Œå¯¹çº¹ç†åå·®çš„é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸‰ç»´æ•°å­—å†…å®¹çš„å¢é•¿æ¨åŠ¨äº†å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸‹è¯†åˆ«ç³»ç»Ÿçš„éœ€æ±‚ã€‚</li>
<li>ç°æœ‰ä¸‰ç»´ç±»å¢é‡å­¦ä¹ æ–¹æ³•é¢ä¸´æç«¯æ•°æ®ç¨€ç¼ºæ—¶çš„æŒ‘æˆ˜ï¼Œä¸»è¦è¡¨ç°ä¸ºå‡ ä½•å¤±é…å’Œçº¹ç†åå·®ã€‚</li>
<li>è·¨æ¨¡æ€å‡ ä½•æ ¡æ­£ï¼ˆCMGRï¼‰æ¡†æ¶è¢«æå‡ºï¼Œä»¥æé«˜ä¸‰ç»´å‡ ä½•æ•°æ®çš„ä¿çœŸåº¦ã€‚</li>
<li>CMGRåˆ©ç”¨CLIPçš„åˆ†å±‚ç©ºé—´è¯­ä¹‰è¿›è¡Œç»“æ„æ„ŸçŸ¥å‡ ä½•æ ¡æ­£ï¼Œå®ç°ä¸‰ç»´æ•°æ®ä¸äºŒç»´æ¨¡å‹çš„æ·±åº¦èåˆã€‚</li>
<li>çº¹ç†æ”¾å¤§æ¨¡å—ç”¨äºåˆæˆæœ€å°åˆ¤åˆ«æ€§çº¹ç†ï¼Œä»¥å¢å¼ºè·¨æ¨¡æ€ä¸€è‡´æ€§å¹¶æŠ‘åˆ¶å™ªå£°ã€‚</li>
<li>åŸºç¡€-æ–°é¢–é‰´åˆ«å™¨çš„é‡‡ç”¨æœ‰åŠ©äºéš”ç¦»å‡ ä½•å˜åŒ–ï¼Œè¿›ä¸€æ­¥ç¨³å®šå¢é‡åŸå‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8bd6f55dd7796935b4470f185b4e7938" align="middle">
<img src="https://picx.zhimg.com/v2-e01d2c5a3277616a91317d76b754895f" align="middle">
<img src="https://picx.zhimg.com/v2-1a703653ef39737083a1d7ae0bb07d6d" align="middle">
<img src="https://picx.zhimg.com/v2-251425028bcf6ed3241d02c5eb83e868" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Two-Facets-of-the-Same-Optimization-Coin-Model-Degradation-and-Representation-Collapse-in-Graph-Foundation-Models"><a href="#Two-Facets-of-the-Same-Optimization-Coin-Model-Degradation-and-Representation-Collapse-in-Graph-Foundation-Models" class="headerlink" title="Two Facets of the Same Optimization Coin: Model Degradation and   Representation Collapse in Graph Foundation Models"></a>Two Facets of the Same Optimization Coin: Model Degradation and   Representation Collapse in Graph Foundation Models</h2><p><strong>Authors:Xunkai Li, Daohan Su, Sicheng Liu, Ru Zhang, Zhenjun Li, Bing Zhou, Rong-Hua Li, Guoren Wang</strong></p>
<p>Inspired by the success of LLMs, GFMs are designed to learn the optimal embedding functions from multi-domain text-attributed graphs for the downstream cross-task generalization capability. Among the diverse architectures, graph VQ-MAE stands out among the increasingly diverse landscape of GFM. This is attributed to its ability to jointly encode topology and textual attributes from multiple domains into discrete embedding spaces with clear semantic boundaries. Despite its potential, domain generalization conflicts cause imperceptible pitfalls. In this paper, we instantiate two of them, and they are just like two sides of the same GFM optimization coin - Side 1 Model Degradation: The encoder and codebook fail to capture the diversity of inputs; Side 2 Representation Collapse: The hidden embedding and codebook vector fail to preserve semantic separability due to constraints from narrow representation subspaces. These two pitfalls (sides) collectively impair the decoder and generate the low-quality reconstructed supervision, causing the GFM optimization dilemma during pre-training (coin). Through empirical investigation, we attribute the above challenges to Information Bottleneck and Regularization Deficit. To address them, we propose MoT - (1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic fusion strategy and a mixture-of-codebooks with domain-aware routing to improve information capacity. (2) Regularization Tinker for Optimization Coin, which utilizes two additional regularizations to further improve gradient supervision in our proposed Information Tinker. Notably, as a flexible architecture, MoT adheres to the scaling laws of GFM, offering a controllable model scale. Compared to SOTA baselines, experiments on 22 datasets across 6 domains demonstrate that MoT achieves significant improvements in supervised, few-shot, and zero-shot scenarios. </p>
<blockquote>
<p>å—åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æˆåŠŸçš„å¯å‘ï¼Œå›¾ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆGFMsï¼‰è¢«è®¾è®¡ç”¨äºä»å¤šåŸŸæ–‡æœ¬å±æ€§å›¾ä¸­å­¦ä¹ æœ€ä¼˜åµŒå…¥å‡½æ•°ï¼Œä»¥ç”¨äºä¸‹æ¸¸è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¤šç§æ¶æ„ä¸­ï¼Œå›¾VQ-MAEåœ¨ä¼—å¤šæ—¥ç›Šå¤šæ ·åŒ–çš„GFMé¢†åŸŸä¸­è„±é¢–è€Œå‡ºã€‚è¿™å½’åŠŸäºå…¶èƒ½å¤Ÿå°†å¤šä¸ªé¢†åŸŸçš„æ‹“æ‰‘å’Œæ–‡æœ¬å±æ€§è”åˆç¼–ç åˆ°å…·æœ‰æ¸…æ™°è¯­ä¹‰è¾¹ç•Œçš„ç¦»æ•£åµŒå…¥ç©ºé—´ä¸­çš„èƒ½åŠ›ã€‚å°½ç®¡å¦‚æ­¤ï¼Œé¢†åŸŸæ³›åŒ–å†²çªè¿˜æ˜¯ä¼šå¯¼è‡´ä¸€äº›ä¸æ˜“å¯Ÿè§‰çš„é™·é˜±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å®ä¾‹åŒ–äº†ä¸¤ä¸ªè¿™æ ·çš„é™·é˜±ï¼Œå®ƒä»¬å°±åƒæ˜¯GFMä¼˜åŒ–åŒä¸€æšç¡¬å¸çš„ä¸¤é¢â€”â€”ç¬¬ä¸€é¢æ˜¯æ¨¡å‹é€€åŒ–ï¼šç¼–ç å™¨å’Œä»£ç æœ¬æ— æ³•æ•æ‰è¾“å…¥çš„å¤šæ ·æ€§ï¼›ç¬¬äºŒé¢æ˜¯è¡¨ç¤ºå´©æºƒï¼šç”±äºæ¥è‡ªç‹­çª„è¡¨ç¤ºå­ç©ºé—´çš„çº¦æŸï¼Œéšè—åµŒå…¥å’Œä»£ç æœ¬å‘é‡æ— æ³•ä¿æŒè¯­ä¹‰å¯åˆ†æ€§ã€‚è¿™ä¸¤ä¸ªé™·é˜±ï¼ˆä¸¤é¢ï¼‰å…±åŒå½±å“äº†è§£ç å™¨ï¼Œå¹¶äº§ç”Ÿäº†ä½è´¨é‡é‡å»ºçš„ç›‘ç£ä¿¡æ¯ï¼Œå¯¼è‡´äº†é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„GFMä¼˜åŒ–å›°å¢ƒï¼ˆç¡¬å¸ï¼‰ã€‚é€šè¿‡å®è¯ç ”ç©¶ï¼Œæˆ‘ä»¬å°†ä¸Šè¿°æŒ‘æˆ˜å½’å› äºä¿¡æ¯ç“¶é¢ˆå’Œæ­£åˆ™åŒ–ç¼ºé™·ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MoTæ–¹æ³•ï¼šï¼ˆ1ï¼‰é’ˆå¯¹ä¸¤ä¸ªé™·é˜±çš„ä¿¡æ¯å¾®è°ƒç­–ç•¥ï¼Œå®ƒåˆ©ç”¨è¾¹ç¼˜è¯­ä¹‰èåˆç­–ç•¥å’Œæ··åˆä»£ç æœ¬ä»¥åŠåŸŸæ„ŸçŸ¥è·¯ç”±æ¥æé«˜ä¿¡æ¯å®¹é‡ï¼›ï¼ˆ2ï¼‰é’ˆå¯¹ä¼˜åŒ–ç¡¬å¸çš„æ­£åˆ™åŒ–å¾®è°ƒç­–ç•¥ï¼Œå®ƒåˆ©ç”¨ä¸¤ç§é¢å¤–çš„æ­£åˆ™åŒ–æ–¹æ³•æ¥è¿›ä¸€æ­¥æ”¹è¿›æˆ‘ä»¬æå‡ºçš„ä¿¡æ¯å¾®è°ƒä¸­çš„æ¢¯åº¦ç›‘ç£ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½œä¸ºä¸€ä¸ªçµæ´»çš„æ¶æ„ï¼ŒMoTéµå¾ªGFMçš„æ‰©å±•å®šå¾‹ï¼Œæä¾›äº†ä¸€ä¸ªå¯æ§çš„æ¨¡å‹è§„æ¨¡ã€‚åœ¨6ä¸ªé¢†åŸŸçš„22ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æŠ€æœ¯åŸºå‡†ç›¸æ¯”ï¼ŒMoTåœ¨ç›‘ç£å­¦ä¹ ã€å°æ ·æœ¬å­¦ä¹ å’Œé›¶æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­å®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08401v4">PDF</a> </p>
<p><strong>Summary</strong><br>     å›¾VQ-MAEåœ¨GFMæ¶æ„ä¸­è¡¨ç°çªå‡ºï¼Œä½†å­˜åœ¨æ¨¡å‹é€€åŒ–å’Œè¡¨ç¤ºå´©æºƒçš„é—®é¢˜ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MoTæ–¹æ¡ˆï¼Œé€šè¿‡ä¿¡æ¯è°ƒæ•´å’Œæ­£åˆ™åŒ–æ”¹è¿›æ¥è§£å†³ï¼Œæé«˜æ¨¡å‹çš„ä¿¡æ¯å®¹é‡å’Œæ¢¯åº¦ç›‘ç£ã€‚MoTåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå®ƒåœ¨ç›‘ç£ã€å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬åœºæ™¯ä¸‹å®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GFMsæ—¨åœ¨ä»å¤šåŸŸæ–‡æœ¬å±æ€§å›¾ä¸­å­¦ä¹ æœ€ä¼˜åµŒå…¥å‡½æ•°ï¼Œä»¥å®ç°ä¸‹æ¸¸è·¨ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å›¾VQ-MAEæ˜¯GFMsä¸­çš„çªå‡ºæ¶æ„ï¼Œèƒ½å¤Ÿè”åˆç¼–ç å¤šä¸ªé¢†åŸŸçš„æ‹“æ‰‘å’Œæ–‡æœ¬å±æ€§åˆ°ç¦»æ•£åµŒå…¥ç©ºé—´ã€‚</li>
<li>GFMä¸­å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šæ¨¡å‹é€€åŒ–å’Œè¡¨ç¤ºå´©æºƒï¼Œè¿™ä¸¤ä¸ªé—®é¢˜å…±åŒå½±å“äº†è§£ç å™¨å¹¶å¯¼è‡´GFMä¼˜åŒ–å›°å¢ƒã€‚</li>
<li>æ¨¡å‹é€€åŒ–è¡¨ç°ä¸ºç¼–ç å™¨å’Œä»£ç æœ¬æ— æ³•æ•æ‰è¾“å…¥å¤šæ ·æ€§ï¼›è¡¨ç¤ºå´©æºƒåˆ™å¯¼è‡´éšè—åµŒå…¥å’Œä»£ç æœ¬å‘é‡æ— æ³•ä¿æŒè¯­ä¹‰å¯åˆ†æ€§ã€‚</li>
<li>ä¿¡æ¯ç“¶é¢ˆå’Œæ­£åˆ™åŒ–ç¼ºé™·è¢«è®¤ä¸ºæ˜¯ä¸Šè¿°æŒ‘æˆ˜çš„åŸå› ã€‚</li>
<li>é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†MoTæ–¹æ¡ˆï¼ŒåŒ…æ‹¬ä¿¡æ¯è°ƒæ•´å’Œæ­£åˆ™åŒ–æ”¹è¿›ï¼Œä»¥æé«˜ä¿¡æ¯å®¹é‡å’Œæ¢¯åº¦ç›‘ç£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08401">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f02f5b95d2f82676b3f75c2c5a70647" align="middle">
<img src="https://picx.zhimg.com/v2-e475289a7c84bc3fadb96821b883877f" align="middle">
<img src="https://picx.zhimg.com/v2-563419dc4d1f066005129941a55a373c" align="middle">
<img src="https://picx.zhimg.com/v2-28b413ac8d3d175d5652748cb1546d8b" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="HealthSLM-Bench-Benchmarking-Small-Language-Models-for-Mobile-and-Wearable-Healthcare-Monitoring"><a href="#HealthSLM-Bench-Benchmarking-Small-Language-Models-for-Mobile-and-Wearable-Healthcare-Monitoring" class="headerlink" title="HealthSLM-Bench: Benchmarking Small Language Models for Mobile and   Wearable Healthcare Monitoring"></a>HealthSLM-Bench: Benchmarking Small Language Models for Mobile and   Wearable Healthcare Monitoring</h2><p><strong>Authors:Xin Wang, Ting Dang, Xinyu Zhang, Vassilis Kostakos, Michael J. Witbrock, Hong Jia</strong></p>
<p>Mobile and wearable healthcare monitoring play a vital role in facilitating timely interventions, managing chronic health conditions, and ultimately improving individualsâ€™ quality of life. Previous studies on large language models (LLMs) have highlighted their impressive generalization abilities and effectiveness in healthcare prediction tasks. However, most LLM-based healthcare solutions are cloud-based, which raises significant privacy concerns and results in increased memory usage and latency. To address these challenges, there is growing interest in compact models, Small Language Models (SLMs), which are lightweight and designed to run locally and efficiently on mobile and wearable devices. Nevertheless, how well these models perform in healthcare prediction remains largely unexplored. We systematically evaluated SLMs on health prediction tasks using zero-shot, few-shot, and instruction fine-tuning approaches, and deployed the best performing fine-tuned SLMs on mobile devices to evaluate their real-world efficiency and predictive performance in practical healthcare scenarios. Our results show that SLMs can achieve performance comparable to LLMs while offering substantial gains in efficiency and privacy. However, challenges remain, particularly in handling class imbalance and few-shot scenarios. These findings highlight SLMs, though imperfect in their current form, as a promising solution for next-generation, privacy-preserving healthcare monitoring. </p>
<blockquote>
<p>ç§»åŠ¨å’Œå¯ç©¿æˆ´åŒ»ç–—è®¾å¤‡åœ¨ä¿ƒè¿›åŠæ—¶å¹²é¢„ã€ç®¡ç†æ…¢æ€§å¥åº·çŠ¶å†µä»¥åŠæœ€ç»ˆæé«˜ä¸ªäººç”Ÿæ´»è´¨é‡æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚å…ˆå‰å…³äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç ”ç©¶å·²ç»çªå‡ºäº†å…¶åœ¨åŒ»ç–—ä¿å¥é¢„æµ‹ä»»åŠ¡ä¸­çš„æƒŠäººæ³›åŒ–èƒ½åŠ›å’Œæœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°åŸºäºLLMçš„åŒ»ç–—æœåŠ¡è§£å†³æ–¹æ¡ˆéƒ½æ˜¯åŸºäºäº‘çš„ï¼Œè¿™å¼•å‘äº†äººä»¬å¯¹éšç§çš„é‡å¤§æ‹…å¿§ï¼Œå¹¶å¯¼è‡´äº†å†…å­˜ä½¿ç”¨å’Œå»¶è¿Ÿçš„å¢åŠ ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œäººä»¬å¯¹å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„å…´è¶£æ—¥ç›Šæµ“åšï¼Œè¿™äº›æ¨¡å‹è½»ä¾¿ä¸”è®¾è®¡ç”¨äºåœ¨ç§»åŠ¨è®¾å¤‡å’Œå¯ç©¿æˆ´è®¾å¤‡ä¸Šæœ¬åœ°é«˜æ•ˆè¿è¡Œã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨åŒ»ç–—ä¿å¥é¢„æµ‹æ–¹é¢çš„è¡¨ç°å¦‚ä½•ä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†SLMåœ¨å¥åº·é¢„æµ‹ä»»åŠ¡ä¸Šçš„é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’ŒæŒ‡ä»¤å¾®è°ƒæ–¹æ³•çš„è¡¨ç°ï¼Œå¹¶åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šéƒ¨ç½²äº†è¡¨ç°æœ€ä½³çš„å¾®è°ƒSLMï¼Œä»¥è¯„ä¼°å…¶åœ¨ç°å®ä¸–ç•Œçš„æ•ˆç‡å’Œé¢„æµ‹æ€§èƒ½åœ¨å®é™…æƒ…å†µä¸‹çš„åŒ»ç–—ä¿å¥åœºæ™¯ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒSLMçš„æ€§èƒ½å¯ä»¥ä¸LLMç›¸åª²ç¾ï¼ŒåŒæ—¶åœ¨æ•ˆç‡å’Œéšç§æ–¹é¢æä¾›äº†å®è´¨æ€§çš„æ”¹è¿›ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡å’Œå°‘æ ·æœ¬åœºæ™¯æ–¹é¢ã€‚è¿™äº›å‘ç°çªå‡ºäº†SLMå°½ç®¡åœ¨å½“å‰å½¢å¼ä¸‹å¹¶ä¸å®Œç¾ï¼Œä½†ä½œä¸ºä¸‹ä¸€ä»£éšç§ä¿æŠ¤åŒ»ç–—ä¿å¥ç›‘æµ‹çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07260v2">PDF</a> 9 pages, 6 tables, 6 figures</p>
<p><strong>Summary</strong></p>
<p>ç§»åŠ¨å’Œå¯ç©¿æˆ´å¼å¥åº·ç›‘æµ‹åœ¨ä¿ƒè¿›åŠæ—¶å¹²é¢„ã€ç®¡ç†æ…¢æ€§å¥åº·çŠ¶å†µä»¥åŠæé«˜ä¸ªäººç”Ÿæ´»è´¨é‡æ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—ä¿å¥é¢„æµ‹ä»»åŠ¡ä¸­çš„éšç§å’Œæ•ˆç‡é—®é¢˜ï¼Œç ”ç©¶ç•Œè½¬å‘ç ”ç©¶å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ã€‚æœ¬æ–‡é€šè¿‡ç³»ç»Ÿåœ°è¯„ä¼°SLMsåœ¨å¥åº·é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå‘ç°å…¶åœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’ŒæŒ‡ä»¤å¾®è°ƒç­‰ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç›¸å½“ï¼Œä¸”åœ¨å®é™…éƒ¨ç½²æ—¶è¡¨ç°å‡ºè¾ƒé«˜çš„æ•ˆç‡å’Œéšç§ä¿æŠ¤èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤„ç†ç±»åˆ«ä¸å¹³è¡¡å’Œå°‘æ ·æœ¬åœºæ™¯ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æ€»ä½“è€Œè¨€ï¼Œå°å‹è¯­è¨€æ¨¡å‹æ˜¯ä¸‹ä¸€ä»£éšç§ä¿æŠ¤å¥åº·ç›‘æµ‹çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç§»åŠ¨å’Œå¯ç©¿æˆ´å¼å¥åº·ç›‘æµ‹å¯¹åŠæ—¶å¹²é¢„ã€ç®¡ç†æ…¢æ€§å¥åº·çŠ¶å†µå’Œæé«˜ç”Ÿæ´»è´¨é‡è‡³å…³é‡è¦ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—ä¿å¥é¢„æµ‹ä¸­å­˜åœ¨éšç§å’Œæ•ˆç‡é—®é¢˜ã€‚</li>
<li>å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰é€‚ç”¨äºç§»åŠ¨å’Œå¯ç©¿æˆ´è®¾å¤‡ï¼Œä»¥åº”å¯¹è¿™äº›é—®é¢˜ã€‚</li>
<li>SLMsåœ¨å¥åº·é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç›¸å½“ã€‚</li>
<li>SLMsåœ¨å®é™…éƒ¨ç½²ä¸­è¡¨ç°å‡ºè¾ƒé«˜çš„æ•ˆç‡å’Œéšç§ä¿æŠ¤èƒ½åŠ›ã€‚</li>
<li>å¤„ç†ç±»åˆ«ä¸å¹³è¡¡å’Œå°‘æ ·æœ¬åœºæ™¯æ˜¯SLMsé¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07260">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e19aa7d60db00ec5888904adc632b777" align="middle">
<img src="https://picx.zhimg.com/v2-507a8b677549a961e88b8e19155d265e" align="middle">
<img src="https://picx.zhimg.com/v2-b4ad3614c9d6f13457a6c02ce29d7d01" align="middle">
<img src="https://picx.zhimg.com/v2-39b08c4576179e31bcacddff076d6110" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="VisText-Mosquito-A-Unified-Multimodal-Benchmark-Dataset-for-Visual-Detection-Segmentation-and-Textual-Reasoning-on-Mosquito-Breeding-Sites"><a href="#VisText-Mosquito-A-Unified-Multimodal-Benchmark-Dataset-for-Visual-Detection-Segmentation-and-Textual-Reasoning-on-Mosquito-Breeding-Sites" class="headerlink" title="VisText-Mosquito: A Unified Multimodal Benchmark Dataset for Visual   Detection, Segmentation, and Textual Reasoning on Mosquito Breeding Sites"></a>VisText-Mosquito: A Unified Multimodal Benchmark Dataset for Visual   Detection, Segmentation, and Textual Reasoning on Mosquito Breeding Sites</h2><p><strong>Authors:Md. Adnanul Islam, Md. Faiyaz Abdullah Sayeedi, Md. Asaduzzaman Shuvo, Shahanur Rahman Bappy, Md Asiful Islam, Swakkhar Shatabda</strong></p>
<p>Mosquito-borne diseases pose a major global health risk, requiring early detection and proactive control of breeding sites to prevent outbreaks. In this paper, we present VisText-Mosquito, a multimodal dataset that integrates visual and textual data to support automated detection, segmentation, and reasoning for mosquito breeding site analysis. The dataset includes 1,828 annotated images for object detection, 142 images for water surface segmentation, and natural language reasoning texts linked to each image. The YOLOv9s model achieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object detection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and mAP@50 of 0.79795. For reasoning generation, we tested a range of large vision-language models (LVLMs) in both zero-shot and few-shot settings. Our fine-tuned Mosquito-LLaMA3-8B model achieved the best results, with a final loss of 0.0028, a BLEU score of 54.7, BERTScore of 0.91, and ROUGE-L of 0.85. This dataset and model framework emphasize the theme â€œPrevention is Better than Cureâ€, showcasing how AI-based detection can proactively address mosquito-borne disease risks. The dataset and implementation code are publicly available at GitHub: <a target="_blank" rel="noopener" href="https://github.com/adnanul-islam-jisun/VisText-Mosquito">https://github.com/adnanul-islam-jisun/VisText-Mosquito</a> </p>
<blockquote>
<p>èšŠè™«ä¼ æ’­çš„ç–¾ç—…æ„æˆäº†å…¨çƒæ€§çš„é‡å¤§å¥åº·å¨èƒï¼Œéœ€è¦æå‰æ£€æµ‹å’Œç§¯ææ§åˆ¶ç¹æ®–åœ°ç‚¹ä»¥é˜²æ­¢çˆ†å‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†VisText-Mosquitoï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡å¼æ•°æ®é›†ï¼Œèåˆäº†è§†è§‰å’Œæ–‡æœ¬æ•°æ®ï¼Œæ”¯æŒèšŠå­ç¹æ®–åœ°ç‚¹åˆ†æçš„è‡ªåŠ¨åŒ–æ£€æµ‹ã€åˆ†å‰²å’Œæ¨ç†ã€‚è¯¥æ•°æ®é›†åŒ…å«1828å¼ ç”¨äºç›®æ ‡æ£€æµ‹çš„æ³¨é‡Šå›¾åƒã€142å¼ ç”¨äºæ°´é¢åˆ†å‰²çš„å›¾åƒä»¥åŠä¸æ¯å¼ å›¾åƒç›¸å…³çš„è‡ªç„¶è¯­è¨€æ¨ç†æ–‡æœ¬ã€‚YOLOv9sæ¨¡å‹åœ¨ç›®æ ‡æ£€æµ‹æ–¹é¢è¾¾åˆ°äº†0.92926çš„ç²¾åº¦å’Œ0.92891çš„mAP@50ï¼Œè€ŒYOLOv11n-Segåœ¨åˆ†å‰²ç²¾åº¦ä¸Šè¾¾åˆ°äº†0.91587ï¼ŒmAP@50ä¸º0.79795ã€‚å¯¹äºæ¨ç†ç”Ÿæˆï¼Œæˆ‘ä»¬åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç¯å¢ƒä¸­æµ‹è¯•äº†ä¸€ç³»åˆ—å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ã€‚æˆ‘ä»¬å¾®è°ƒçš„Mosquito-LLaMA3-8Bæ¨¡å‹å–å¾—äº†æœ€ä½³ç»“æœï¼Œæœ€ç»ˆæŸå¤±ä¸º0.0028ï¼ŒBLEUå¾—åˆ†ä¸º54.7ï¼ŒBERTScoreä¸º0.91ï¼ŒROUGE-Lä¸º0.85ã€‚è¯¥æ•°æ®é›†å’Œæ¨¡å‹æ¡†æ¶çªå‡ºäº†â€œé¢„é˜²èƒœäºæ²»ç–—â€çš„ä¸»é¢˜ï¼Œå±•ç¤ºäº†åŸºäºAIçš„æ£€æµ‹å¦‚ä½•ç§¯æåº”å¯¹èšŠè™«ä¼ æ’­ç–¾ç—…çš„é£é™©ã€‚æ•°æ®é›†å’Œå®ç°çš„ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šï¼š<a target="_blank" rel="noopener" href="https://github.com/adnanul-islam-jisun/VisText-Mosquito">https://github.com/adnanul-islam-jisun/VisText-Mosquito</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14629v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†VisText-Mosquitoæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†èåˆäº†è§†è§‰å’Œæ–‡æœ¬æ•°æ®ï¼Œæ”¯æŒèšŠå­ç¹æ®–åœ°åˆ†æçš„è‡ªåŠ¨åŒ–æ£€æµ‹ã€åˆ†å‰²å’Œæ¨ç†ã€‚é€šè¿‡ä½¿ç”¨YOLOv9sæ¨¡å‹å’ŒYOLOv11n-Segæ¨¡å‹è¿›è¡Œå¯¹è±¡æ£€æµ‹å’Œå›¾åƒåˆ†å‰²ï¼Œä»¥åŠä½¿ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œæ¨ç†ç”Ÿæˆï¼Œè¯¥æ•°æ®é›†ä¸ºèšŠå­ç¹æ®–åœ°çš„æ—©æœŸæ£€æµ‹å’Œé¢„é˜²æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚æ­¤å¤–ï¼Œè¿˜å¼ºè°ƒäº†é¢„é˜²ä¼˜äºæ²»ç–—çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VisText-Mosquitoæ•°æ®é›†èåˆäº†è§†è§‰å’Œæ–‡æœ¬æ•°æ®ï¼Œç”¨äºæ”¯æŒèšŠå­ç¹æ®–åœ°çš„è‡ªåŠ¨åŒ–æ£€æµ‹ã€åˆ†å‰²å’Œæ¨ç†ã€‚</li>
<li>æ•°æ®é›†åŒ…å«ç”¨äºå¯¹è±¡æ£€æµ‹çš„1,828å¼ æ³¨é‡Šå›¾åƒã€ç”¨äºæ°´é¢åˆ†å‰²çš„142å¼ å›¾åƒä»¥åŠä¸æ¯å¼ å›¾åƒç›¸å…³çš„è‡ªç„¶è¯­è¨€æ¨ç†æ–‡æœ¬ã€‚</li>
<li>YOLOv9sæ¨¡å‹åœ¨å¯¹è±¡æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºé«˜ç²¾ç¡®åº¦ï¼Œè€ŒYOLOv11n-Segæ¨¡å‹åœ¨å›¾åƒåˆ†å‰²æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œæ¨ç†ç”Ÿæˆï¼Œåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹è¿›è¡Œäº†æµ‹è¯•ã€‚</li>
<li>Fine-tuned Mosquito-LLaMA3-8Bæ¨¡å‹åœ¨æ¨ç†ç”Ÿæˆæ–¹é¢å–å¾—äº†æœ€ä½³ç»“æœã€‚</li>
<li>è¯¥æ•°æ®é›†å’Œæ¨¡å‹æ¡†æ¶å¼ºè°ƒâ€œé¢„é˜²ä¼˜äºæ²»ç–—â€çš„ä¸»é¢˜ï¼Œå±•ç¤ºäº†AIæ£€æµ‹å¦‚ä½•ä¸»åŠ¨åº”å¯¹èšŠå­ä¼ æ’­çš„ç–¾ç—…é£é™©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14629">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c99369d4083f67199d47aba593a1ab36" align="middle">
<img src="https://picx.zhimg.com/v2-cca2c74eaa867c7d1105d26c197e9170" align="middle">
<img src="https://picx.zhimg.com/v2-201165b62b179d2ba868f00e0be07b12" align="middle">
<img src="https://picx.zhimg.com/v2-c095bdafcff89e80300bc67db89a0504" align="middle">
<img src="https://picx.zhimg.com/v2-3699298a02c789832f1ee25102183ae5" align="middle">
<img src="https://picx.zhimg.com/v2-66c5d8fcd04b44efaffb4a26309651ae" align="middle">
<img src="https://picx.zhimg.com/v2-dff5972ce1f7f857d456f0c3c1dd2dc1" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Are-LLMs-Better-Formalizers-than-Solvers-on-Complex-Problems"><a href="#Are-LLMs-Better-Formalizers-than-Solvers-on-Complex-Problems" class="headerlink" title="Are LLMs Better Formalizers than Solvers on Complex Problems?"></a>Are LLMs Better Formalizers than Solvers on Complex Problems?</h2><p><strong>Authors:Rikhil Amonkar, May Lai, Ronan Le Bras, Li Zhang</strong></p>
<p>A trending line of recent work advocates for using large language models (LLMs) as formalizers instead of as end-to-end solvers for logical reasoning problems. Instead of generating the solution, the LLM generates a formal program that derives a solution via an external solver. While performance gain of the seemingly scalable LLM-as-formalizer over the seemingly unscalable LLM-as-solver has been widely reported, we show that this superiority does not hold on real-life constraint satisfaction problems. On 4 domains, we systematically evaluate 6 LLMs including 4 large reasoning models with inference-time scaling, paired with 5 pipelines including 2 types of formalism. We show that in few-shot settings, LLM-as-formalizer underperforms LLM-as-solver. While LLM-as-formalizer promises accuracy, robustness, faithfulness, and efficiency, we observe that the present LLMs do not yet deliver any of those, as their limited ability to generate formal programs leads to failure to scale with complexity, hard-coded solutions, and excessive reasoning tokens. We present our detailed analysis and actionable remedies to drive future research that improves LLM-as-formalizer. </p>
<blockquote>
<p>è¿‘æœŸçš„ç ”ç©¶è¶‹åŠ¿ä¸»å¼ å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºå½¢å¼åŒ–å·¥å…·ï¼Œè€Œéä½œä¸ºç«¯åˆ°ç«¯çš„æ±‚è§£å™¨æ¥è§£å†³é€»è¾‘æ¨ç†é—®é¢˜ã€‚LLMä¸æ˜¯ç”Ÿæˆè§£å†³æ–¹æ¡ˆï¼Œè€Œæ˜¯ç”Ÿæˆä¸€ä¸ªå½¢å¼åŒ–ç¨‹åºï¼Œé€šè¿‡å¤–éƒ¨æ±‚è§£å™¨æ¥æ¨å¯¼è§£å†³æ–¹æ¡ˆã€‚è™½ç„¶ä½œä¸ºå½¢å¼åŒ–å·¥å…·çš„LLMåœ¨çœ‹ä¼¼å¯æ‰©å±•æ–¹é¢çš„æ€§èƒ½ä¼˜åŠ¿å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„æŠ¥é“ï¼Œä½†æˆ‘ä»¬åœ¨å®é™…ç”Ÿæ´»ä¸­çš„çº¦æŸæ»¡è¶³é—®é¢˜ä¸Šå¹¶æ²¡æœ‰ä¿æŒè¿™ç§ä¼˜åŠ¿ã€‚æˆ‘ä»¬åœ¨4ä¸ªé¢†åŸŸç³»ç»Ÿåœ°è¯„ä¼°äº†6ä¸ªLLMï¼ŒåŒ…æ‹¬4ä¸ªå…·æœ‰æ¨ç†æ—¶é—´ç¼©æ”¾çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼Œä¸5ä¸ªç®¡é“ï¼ˆåŒ…æ‹¬2ç§å½¢å¼åŒ–ç±»å‹ï¼‰é…å¯¹ã€‚æˆ‘ä»¬å‘ç°åœ¨å°æ ·æœ¬è®¾ç½®ä¸‹ï¼Œä½œä¸ºå½¢å¼åŒ–å·¥å…·çš„LLMæ€§èƒ½ä½äºä½œä¸ºæ±‚è§£å™¨çš„LLMã€‚è™½ç„¶LLMä½œä¸ºå½¢å¼åŒ–å·¥å…·åœ¨å‡†ç¡®æ€§ã€ç¨³å¥æ€§ã€å¿ è¯šæ€§å’Œæ•ˆç‡æ–¹é¢åšå‡ºäº†æ‰¿è¯ºï¼Œä½†æˆ‘ä»¬è§‚å¯Ÿåˆ°ç›®å‰çš„LLMå¹¶æ²¡æœ‰å®ç°è¿™äº›æ‰¿è¯ºï¼Œå› ä¸ºå®ƒä»¬ç”Ÿæˆå½¢å¼åŒ–ç¨‹åºçš„èƒ½åŠ›æœ‰é™ï¼Œå¯¼è‡´æ— æ³•éšç€å¤æ‚æ€§è¿›è¡Œæ‰©å±•ã€è§£å†³æ–¹æ¡ˆè¿‡äºåƒµåŒ–ä»¥åŠè¿‡å¤šçš„æ¨ç†ç¬¦å·ã€‚æˆ‘ä»¬æä¾›äº†è¯¦ç»†çš„åˆ†æå’Œå¯è¡Œçš„è¡¥æ•‘æªæ–½ï¼Œä»¥æ¨åŠ¨æœªæ¥ç ”ç©¶æ”¹è¿›LLMä½œä¸ºå½¢å¼åŒ–å·¥å…·çš„ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13252v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºå½¢å¼åŒ–å·¥å…·è€Œéç«¯åˆ°ç«¯çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨é€»è¾‘æ¨ç†é—®é¢˜ä¸Šçš„è¶‹åŠ¿é€æ¸æ˜¾ç°ã€‚è™½ç„¶LLMä½œä¸ºå½¢å¼åŒ–å·¥å…·åœ¨æ€§èƒ½ä¸Šçš„ä¼˜åŠ¿è¢«å¹¿æ³›æŠ¥é“ï¼Œä½†åœ¨ç°å®ç”Ÿæ´»ä¸­çš„çº¦æŸæ»¡è¶³é—®é¢˜ä¸Šï¼Œè¿™ç§ä¼˜åŠ¿å¹¶ä¸æ˜æ˜¾ã€‚åœ¨å°‘æ•°åœºæ™¯ä¸‹ï¼ŒLLMä½œä¸ºå½¢å¼åŒ–å·¥å…·çš„è¡¨ç°ç”šè‡³ä¸å¦‚LLMä½œä¸ºæ±‚è§£å™¨ã€‚å…³äºLLMä½œä¸ºå½¢å¼åŒ–å·¥å…·å­˜åœ¨ç²¾åº¦ã€ç¨³å¥æ€§ã€å¿ è¯šåº¦å’Œæ•ˆç‡çš„é—®é¢˜ï¼Œå› ç”Ÿæˆçš„å½¢å¼ç¨‹åºæœ‰é™ï¼Œéš¾ä»¥éšç€å¤æ‚æ€§è€Œæ‰©å±•ï¼Œäº§ç”Ÿç¡¬ç¼–ç è§£å†³æ–¹æ¡ˆå’Œè¿‡å¤šçš„æ¨ç†æ ‡è®°ã€‚æˆ‘ä»¬æä¾›äº†è¯¦ç»†çš„åˆ†æå’Œå¯è¡Œçš„è¡¥æ•‘æªæ–½ï¼Œä»¥æ¨åŠ¨æœªæ¥æ”¹å–„LLMä½œä¸ºå½¢å¼åŒ–å·¥å…·çš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥ä½œä¸ºå½¢å¼åŒ–å·¥å…·ç”¨äºé€»è¾‘é—®é¢˜çš„æ±‚è§£ã€‚</li>
<li>åœ¨ç°å®ç”Ÿæ´»ä¸­çš„çº¦æŸæ»¡è¶³é—®é¢˜ä¸Šï¼ŒLLMä½œä¸ºå½¢å¼åŒ–å·¥å…·çš„ä¼˜åŠ¿å¹¶ä¸æ˜¾è‘—ã€‚</li>
<li>åœ¨å°‘æ•°åœºæ™¯ä¸‹ï¼ŒLLMä½œä¸ºå½¢å¼åŒ–å·¥å…·çš„è¡¨ç°ä¸å¦‚ä½œä¸ºæ±‚è§£å™¨ã€‚</li>
<li>LLMä½œä¸ºå½¢å¼åŒ–å·¥å…·å­˜åœ¨ç²¾åº¦ã€ç¨³å¥æ€§ã€å¿ è¯šåº¦å’Œæ•ˆç‡çš„é—®é¢˜ã€‚</li>
<li>LLMç”Ÿæˆçš„å½¢å¼ç¨‹åºæœ‰é™ï¼Œéš¾ä»¥éšç€é—®é¢˜çš„å¤æ‚æ€§è€Œæ‰©å±•ã€‚</li>
<li>LLMå®¹æ˜“äº§ç”Ÿç¡¬ç¼–ç è§£å†³æ–¹æ¡ˆå’Œè¿‡å¤šçš„æ¨ç†æ ‡è®°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13252">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2951b0f5a74098d392ae0ab3f96cf7f6" align="middle">
<img src="https://picx.zhimg.com/v2-55a23ebd5a00baba89a0565186d822dc" align="middle">
<img src="https://picx.zhimg.com/v2-3718d7af33163979a441ac84d2a4ee67" align="middle">
<img src="https://picx.zhimg.com/v2-e304def0483614fd05e9200f2e959a97" align="middle">
<img src="https://picx.zhimg.com/v2-60080bedade5453e6a0f4386e6a3011b" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="DescriptorMedSAM-Language-Image-Fusion-with-Multi-Aspect-Text-Guidance-for-Medical-Image-Segmentation"><a href="#DescriptorMedSAM-Language-Image-Fusion-with-Multi-Aspect-Text-Guidance-for-Medical-Image-Segmentation" class="headerlink" title="DescriptorMedSAM: Language-Image Fusion with Multi-Aspect Text Guidance   for Medical Image Segmentation"></a>DescriptorMedSAM: Language-Image Fusion with Multi-Aspect Text Guidance   for Medical Image Segmentation</h2><p><strong>Authors:Wenjie Zhang, Liming Luo, Mengnan He, Jiarui Hai, Jiancheng Ye</strong></p>
<p>Accurate organ segmentation is essential for clinical tasks such as radiotherapy planning and disease monitoring. Recent foundation models like MedSAM achieve strong results using point or bounding-box prompts but still require manual interaction. We propose DescriptorMedSAM, a lightweight extension of MedSAM that incorporates structured text prompts, ranging from simple organ names to combined shape and location descriptors to enable click-free segmentation. DescriptorMedSAM employs a CLIP text encoder to convert radiology-style descriptors into dense embeddings, which are fused with visual tokens via a cross-attention block and a multi-scale feature extractor. We designed four descriptor types: Name (N), Name + Shape (NS), Name + Location (NL), and Name + Shape + Location (NSL), and evaluated them on the FLARE 2022 dataset under zero-shot and few-shot settings, where organs unseen during training must be segmented with minimal additional data. NSL prompts achieved the highest performance, with a Dice score of 0.9405 under full supervision, a 76.31% zero-shot retention ratio, and a 97.02% retention ratio after fine-tuning with only 50 labeled slices per unseen organ. Adding shape and location cues consistently improved segmentation accuracy, especially for small or morphologically complex structures. We demonstrate that structured language prompts can effectively replace spatial interactions, delivering strong zero-shot performance and rapid few-shot adaptation. By quantifying the role of descriptor, this work lays the groundwork for scalable, prompt-aware segmentation models that generalize across diverse anatomical targets with minimal annotation effort. </p>
<blockquote>
<p>ç²¾ç¡®çš„ç»„ç»‡åˆ†å‰²å¯¹äºæ”¾ç–—è®¡åˆ’å’Œç–¾ç—…ç›‘æµ‹ç­‰ä¸´åºŠä»»åŠ¡è‡³å…³é‡è¦ã€‚æœ€è¿‘çš„MedSAMç­‰åŸºç¡€æ¨¡å‹é€šè¿‡ä½¿ç”¨ç‚¹æˆ–è¾¹ç•Œæ¡†æç¤ºå–å¾—äº†å¼ºå¤§çš„ç»“æœï¼Œä½†ä»éœ€è¦æ‰‹åŠ¨äº¤äº’ã€‚æˆ‘ä»¬æå‡ºäº†DescriptorMedSAMï¼Œå®ƒæ˜¯MedSAMçš„ä¸€ä¸ªè½»é‡çº§æ‰©å±•ï¼Œç»“åˆäº†ç»“æ„åŒ–çš„æ–‡æœ¬æç¤ºï¼Œä»ç®€å•çš„å™¨å®˜åç§°åˆ°ç»„åˆçš„å½¢çŠ¶å’Œä½ç½®æè¿°ç¬¦ï¼Œä»¥å®ç°æ— ç‚¹å‡»åˆ†å‰²ã€‚DescriptorMedSAMé‡‡ç”¨CLIPæ–‡æœ¬ç¼–ç å™¨å°†æ”¾å°„å­¦é£æ ¼çš„æè¿°ç¬¦è½¬æ¢ä¸ºå¯†é›†åµŒå…¥ï¼Œé€šè¿‡äº¤å‰æ³¨æ„å—å’Œå¤šå°ºåº¦ç‰¹å¾æå–å™¨ä¸è§†è§‰ä»¤ç‰Œèåˆã€‚æˆ‘ä»¬è®¾è®¡äº†å››ç§æè¿°ç¬¦ç±»å‹ï¼šåç§°ï¼ˆNï¼‰ã€åç§°+å½¢çŠ¶ï¼ˆNSï¼‰ã€åç§°+ä½ç½®ï¼ˆNLï¼‰å’Œåç§°+å½¢çŠ¶+ä½ç½®ï¼ˆNSLï¼‰ï¼Œå¹¶åœ¨FLARE 2022æ•°æ®é›†ä¸Šè¿›è¡Œäº†é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹çš„è¯„ä¼°ï¼Œå…¶ä¸­åœ¨è®­ç»ƒæœŸé—´æœªè§åˆ°çš„å™¨å®˜å¿…é¡»ä½¿ç”¨å°½å¯èƒ½å°‘çš„æ•°æ®è¿›è¡Œåˆ†å‰²ã€‚NSLæç¤ºå–å¾—äº†æœ€é«˜æ€§èƒ½ï¼Œåœ¨å®Œå…¨ç›‘ç£ä¸‹çš„Diceå¾—åˆ†ä¸º0.9405ï¼Œé›¶æ ·æœ¬ä¿ç•™ç‡ä¸º76.31%ï¼Œä»…ä½¿ç”¨æ¯æœªè§å™¨å®˜50ä¸ªæ ‡è®°åˆ‡ç‰‡è¿›è¡Œå¾®è°ƒåï¼Œä¿ç•™ç‡è¾¾åˆ°äº†97.02%ã€‚æ·»åŠ å½¢çŠ¶å’Œä½ç½®çº¿ç´¢å§‹ç»ˆæé«˜äº†åˆ†å‰²ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå°æˆ–å½¢æ€å¤æ‚çš„ç»“æ„ã€‚æˆ‘ä»¬è¯æ˜äº†ç»“æ„åŒ–è¯­è¨€æç¤ºå¯ä»¥æœ‰æ•ˆåœ°æ›¿ä»£ç©ºé—´äº¤äº’ï¼Œå®ç°å¼ºå¤§çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œå¹¶å¿«é€Ÿé€‚åº”å°‘æ ·æœ¬ç¯å¢ƒã€‚é€šè¿‡é‡åŒ–æè¿°ç¬¦çš„ä½œç”¨ï¼Œè¿™é¡¹å·¥ä½œä¸ºå¯æ‰©å±•çš„ã€æç¤ºæ„ŸçŸ¥çš„åˆ†å‰²æ¨¡å‹å¥ å®šäº†åŸºç¡€ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥è·¨å¤šç§è§£å‰–ç›®æ ‡è¿›è¡Œæ¨å¹¿ï¼Œå¹¶æœ€å°åŒ–æ³¨é‡Šå·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13806v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDescriptorMedSAMçš„è½»é‡çº§åŒ»å­¦å½±åƒåˆ†å‰²æ¨¡å‹ï¼Œå®ƒç»“åˆäº†ç»“æ„åŒ–çš„æ–‡æœ¬æç¤ºä¿¡æ¯ï¼Œå¦‚å™¨å®˜åç§°ã€å½¢çŠ¶å’Œä½ç½®æè¿°ç­‰ï¼Œå®ç°äº†æ— éœ€ç‚¹å‡»çš„åˆ†å‰²ã€‚è¯¥æ¨¡å‹é‡‡ç”¨CLIPæ–‡æœ¬ç¼–ç å™¨å°†æ”¾å°„å­¦é£æ ¼çš„æè¿°ä¿¡æ¯è½¬æ¢ä¸ºå¯†é›†åµŒå…¥å‘é‡ï¼Œå¹¶é€šè¿‡è·¨æ³¨æ„åŠ›å—å’Œå¤šå°ºåº¦ç‰¹å¾æå–å™¨ä¸è§†è§‰æ ‡è®°èåˆã€‚åœ¨FLARE 2022æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç»“åˆåç§°ã€å½¢çŠ¶å’Œä½ç½®çš„æè¿°æç¤ºï¼ˆNSL promptsï¼‰å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œå…¨ç›‘ç£ä¸‹çš„Diceå¾—åˆ†ä¸º0.9405ã€‚è¯¥æ¨¡å‹èƒ½æœ‰æ•ˆæ›¿ä»£ç©ºé—´äº¤äº’ï¼Œå®ç°å¼ºå¤§çš„é›¶æ ·æœ¬æ€§èƒ½å¹¶å¿«é€Ÿé€‚åº”å°‘æ ·æœ¬æƒ…å†µã€‚è¿™é¡¹ç ”ç©¶ä¸ºå¯æ‰©å±•çš„ã€åŸºäºæç¤ºçš„åˆ†å‰²æ¨¡å‹å¥ å®šäº†åŸºç¡€ï¼Œå¯ä»¥è·¨ä¸åŒçš„è§£å‰–ç›®æ ‡è¿›è¡Œæ¨å¹¿ï¼Œå¹¶æœ€å°åŒ–æ ‡æ³¨å·¥ä½œé‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DescriptorMedSAMæ˜¯MedSAMçš„è½»é‡çº§æ‰©å±•ï¼Œå¯ç»“åˆç»“æ„åŒ–æ–‡æœ¬æç¤ºå®ç°æ— éœ€ç‚¹å‡»çš„åˆ†å‰²ã€‚</li>
<li>ä½¿ç”¨CLIPæ–‡æœ¬ç¼–ç å™¨å°†æ”¾å°„å­¦æè¿°è½¬æ¢ä¸ºå¯†é›†åµŒå…¥å‘é‡ã€‚</li>
<li>é€šè¿‡è·¨æ³¨æ„åŠ›å—å’Œå¤šå°ºåº¦ç‰¹å¾æå–å™¨èåˆè§†è§‰æ ‡è®°ã€‚</li>
<li>æè¿°ç¬¦ç±»å‹åŒ…æ‹¬åç§°ï¼ˆNï¼‰ã€åç§°+å½¢çŠ¶ï¼ˆNSï¼‰ã€åç§°+ä½ç½®ï¼ˆNLï¼‰å’Œåç§°+å½¢çŠ¶+ä½ç½®ï¼ˆNSLï¼‰ã€‚</li>
<li>åœ¨FLARE 2022æ•°æ®é›†ä¸Šï¼ŒNSLæç¤ºåœ¨å…¨ç›‘ç£ä¸‹è¾¾åˆ°è¾ƒé«˜çš„Diceå¾—åˆ†ã€‚</li>
<li>æ¨¡å‹èƒ½æœ‰æ•ˆæ›¿ä»£ç©ºé—´äº¤äº’ï¼Œå®ç°é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æƒ…å†µä¸‹çš„è‰¯å¥½æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13806">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4253c5d9fa17f9132478dce4fa5834bd" align="middle">
<img src="https://picx.zhimg.com/v2-d35ec510ec06e223c0ce830aa1e7106c" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8d00b0aba9de99d96435bb9df67387a3" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-24  Seg4Diff Unveiling Open-Vocabulary Segmentation in Text-to-Image   Diffusion Transformers
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-96335630fadeba55c4326db30a6c49f9" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-24  The STAR-XAI Protocol An Interactive Framework for Inducing   Second-Order Agency in AI Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32306k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
