<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-24  UniPixel Unified Object Referring and Segmentation for Pixel-Level   Visual Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-585eeb01e1b5036bc615aaedb3aea715~resize:0:q75.jpg?source=1f5c5e47&expiration=1760138220&auth_key=1760138220-0-0-d288918137d258a6fddff352b51887e8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-24-æ›´æ–°"><a href="#2025-09-24-æ›´æ–°" class="headerlink" title="2025-09-24 æ›´æ–°"></a>2025-09-24 æ›´æ–°</h1><h2 id="UniPixel-Unified-Object-Referring-and-Segmentation-for-Pixel-Level-Visual-Reasoning"><a href="#UniPixel-Unified-Object-Referring-and-Segmentation-for-Pixel-Level-Visual-Reasoning" class="headerlink" title="UniPixel: Unified Object Referring and Segmentation for Pixel-Level   Visual Reasoning"></a>UniPixel: Unified Object Referring and Segmentation for Pixel-Level   Visual Reasoning</h2><p><strong>Authors:Ye Liu, Zongyang Ma, Junfu Pu, Zhongang Qi, Yang Wu, Ying Shan, Chang Wen Chen</strong></p>
<p>Recent advances in Large Multi-modal Models (LMMs) have demonstrated their remarkable success as general-purpose multi-modal assistants, with particular focuses on holistic image- and video-language understanding. Conversely, less attention has been given to scaling fine-grained pixel-level understanding capabilities, where the models are expected to realize pixel-level alignment between visual signals and language semantics. Some previous studies have applied LMMs to related tasks such as region-level captioning and referring expression segmentation. However, these models are limited to performing either referring or segmentation tasks independently and fail to integrate these fine-grained perception capabilities into visual reasoning. To bridge this gap, we propose UniPixel, a large multi-modal model capable of flexibly comprehending visual prompt inputs and generating mask-grounded responses. Our model distinguishes itself by seamlessly integrating pixel-level perception with general visual understanding capabilities. Specifically, UniPixel processes visual prompts and generates relevant masks on demand, and performs subsequent reasoning conditioning on these intermediate pointers during inference, thereby enabling fine-grained pixel-level reasoning. The effectiveness of our approach has been verified on 10 benchmarks across a diverse set of tasks, including pixel-level referring&#x2F;segmentation and object-centric understanding in images&#x2F;videos. A novel PixelQA task that jointly requires referring, segmentation, and question answering is also designed to verify the flexibility of our method. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„è¿›å±•è¯æ˜äº†å®ƒä»¬åœ¨ä½œä¸ºé€šç”¨å¤šæ¨¡æ€åŠ©ç†æ–¹é¢çš„æ˜¾è‘—æˆåŠŸï¼Œç‰¹åˆ«æ˜¯åœ¨æ•´ä½“å›¾åƒå’Œè§†é¢‘è¯­è¨€ç†è§£æ–¹é¢ã€‚ç„¶è€Œï¼Œå¯¹äºç²¾ç»†åŒ–åƒç´ çº§åˆ«çš„ç†è§£èƒ½åŠ›çš„è§„æ¨¡æ‰©å±•ï¼Œå…³æ³¨åº¦ç›¸å¯¹è¾ƒä½ã€‚åœ¨è¿™äº›æ¨¡å‹ä¸­ï¼ŒæœŸæœ›æ¨¡å‹èƒ½å¤Ÿå®ç°è§†è§‰ä¿¡å·å’Œè¯­ä¹‰è¯­è¨€ä¹‹é—´çš„åƒç´ çº§å¯¹é½ã€‚è™½ç„¶ä¸€äº›æ—©æœŸç ”ç©¶å·²å°†LMMsåº”ç”¨äºç›¸å…³ä»»åŠ¡ï¼Œå¦‚åŒºåŸŸçº§æ ‡é¢˜å’Œå¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²ï¼Œä½†è¿™äº›æ¨¡å‹ä»…é™äºç‹¬ç«‹æ‰§è¡Œå¼•ç”¨æˆ–åˆ†å‰²ä»»åŠ¡ï¼Œæ— æ³•å°†è¿™äº›ç²¾ç»†åŒ–çš„æ„ŸçŸ¥èƒ½åŠ›æ•´åˆåˆ°è§†è§‰æ¨ç†ä¸­ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†UniPixelï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿçµæ´»ç†è§£è§†è§‰æç¤ºè¾“å…¥å¹¶äº§ç”ŸåŸºäºé®ç½©çš„å“åº”ã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡æ— ç¼é›†æˆåƒç´ çº§æ„ŸçŸ¥å’Œä¸€èˆ¬è§†è§‰ç†è§£èƒ½åŠ›æ¥åŒºåˆ†è‡ªå·±ã€‚å…·ä½“æ¥è¯´ï¼ŒUniPixelå¤„ç†è§†è§‰æç¤ºå¹¶æ ¹æ®éœ€æ±‚ç”Ÿæˆç›¸å…³é®ç½©ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŸºäºè¿™äº›ä¸­é—´æŒ‡é’ˆè¿›è¡Œåç»­æ¨ç†ï¼Œä»è€Œå®ç°ç²¾ç»†çš„åƒç´ çº§æ¨ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§å·²åœ¨10ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå¾—åˆ°éªŒè¯ï¼Œæ¶µç›–äº†ä¸€ç³»åˆ—ä»»åŠ¡ï¼ŒåŒ…æ‹¬åƒç´ çº§çš„å¼•ç”¨&#x2F;åˆ†å‰²å’Œå›¾åƒ&#x2F;è§†é¢‘çš„ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„ç†è§£ã€‚è¿˜è®¾è®¡äº†ä¸€ä¸ªæ–°é¢–çš„PixelQAä»»åŠ¡ï¼Œè”åˆéœ€è¦å¼•ç”¨ã€åˆ†å‰²å’Œé—®ç­”ï¼Œä»¥éªŒè¯æˆ‘ä»¬æ–¹æ³•çš„çµæ´»æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18094v1">PDF</a> NeurIPS 2025 Camera Ready. Project Page:   <a target="_blank" rel="noopener" href="https://polyu-chenlab.github.io/unipixel/">https://polyu-chenlab.github.io/unipixel/</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨é€šç”¨å¤šæ¨¡æ€åŠ©ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œç‰¹åˆ«æ˜¯åœ¨æ•´ä½“å›¾åƒå’Œè§†é¢‘è¯­è¨€ç†è§£æ–¹é¢ã€‚ç„¶è€Œï¼Œå¯¹äºç²¾ç»†ç²’åº¦çš„åƒç´ çº§ç†è§£èƒ½åŠ›çš„æ‰©å±•å…³æ³¨è¾ƒå°‘ï¼Œå…¶ä¸­æ¨¡å‹éœ€è¦åœ¨è§†è§‰ä¿¡å·å’Œè¯­è¨€è¯­ä¹‰ä¹‹é—´å®ç°åƒç´ çº§å¯¹é½ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†UniPixelæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿçµæ´»ç†è§£è§†è§‰æç¤ºè¾“å…¥å¹¶ç”ŸæˆåŸºäºæ©ç çš„å“åº”ã€‚UniPixelé€šè¿‡æ— ç¼é›†æˆåƒç´ çº§æ„ŸçŸ¥ä¸é€šç”¨è§†è§‰ç†è§£èƒ½åŠ›æ¥åŒºåˆ†è‡ªå·±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨åŒ…æ‹¬åƒç´ çº§å¼•ç”¨&#x2F;åˆ†å‰²å’Œå›¾åƒ&#x2F;è§†é¢‘ä¸­çš„å¯¹è±¡ä¸­å¿ƒç†è§£åœ¨å†…çš„å¤šä¸ªä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚è¿˜è®¾è®¡äº†ä¸€ä¸ªæ–°å‹PixelQAä»»åŠ¡æ¥éªŒè¯æ–¹æ³•çš„çµæ´»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å·²å¹¿æ³›åº”ç”¨äºé€šç”¨å¤šæ¨¡æ€åŠ©ç†é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨æ•´ä½“å›¾åƒå’Œè§†é¢‘è¯­è¨€ç†è§£æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æˆåŠŸã€‚</li>
<li>å½“å‰ç¼ºä¹ç²¾ç»†ç²’åº¦çš„åƒç´ çº§ç†è§£èƒ½åŠ›ï¼Œè¦æ±‚æ¨¡å‹å®ç°è§†è§‰ä¿¡å·ä¸è¯­è¨€è¯­ä¹‰ä¹‹é—´çš„åƒç´ çº§å¯¹é½ã€‚</li>
<li>UniPixelæ¨¡å‹æ—¨åœ¨è§£å†³è¿™ä¸€å·®è·ï¼Œé€šè¿‡çµæ´»ç†è§£è§†è§‰æç¤ºè¾“å…¥å¹¶ç”ŸæˆåŸºäºæ©ç çš„å“åº”æ¥å®ç°åƒç´ çº§ç†è§£ã€‚</li>
<li>UniPixelé€šè¿‡æ— ç¼é›†æˆåƒç´ çº§æ„ŸçŸ¥å’Œé€šç”¨è§†è§‰ç†è§£èƒ½åŠ›æ¥åŒºåˆ†å…¶ä»–æ¨¡å‹ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†UniPixelæ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬åƒç´ çº§å¼•ç”¨ã€åˆ†å‰²å’Œå›¾åƒ&#x2F;è§†é¢‘ä¸­çš„å¯¹è±¡ä¸­å¿ƒç†è§£ã€‚</li>
<li>æ–°å‹PixelQAä»»åŠ¡æ—¨åœ¨éªŒè¯æ¨¡å‹çš„çµæ´»æ€§ï¼Œæ¶µç›–å¼•ç”¨ã€åˆ†å‰²å’Œé—®ç­”ç­‰å¤šä¸ªæ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18094">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f73495780e4543228666c760bb8461e6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760138228&auth_key=1760138228-0-0-3b7358de81542037da95012c4e06d4c0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2f011be785ab61f58301839b2116118~resize:0:q75.jpg?source=1f5c5e47&expiration=1760138234&auth_key=1760138234-0-0-d6549bb48ba8efdf9695b204ebb456d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d303fe5fdb4fbbdbf9c0980486babfa7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760138241&auth_key=1760138241-0-0-c6a6893e57dc82db14a66fba88083fcc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7787fc56cf1a7ecbcde194e80bbc0975~resize:0:q75.jpg?source=1f5c5e47&expiration=1760138248&auth_key=1760138248-0-0-428da5bbddf1f56be4f6f951d59c4e3f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1cbd30b4c31a7a0ff191e9bc7646edb5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760138255&auth_key=1760138255-0-0-8cbc1764ccfe9f81779b679cf8135bc6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs"><a href="#TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs" class="headerlink" title="TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning   for Video LLMs"></a>TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning   for Video LLMs</h2><p><strong>Authors:Yunheng Li, Jing Cheng, Shaoyong Jia, Hangyi Kuang, Shaohui Jiao, Qibin Hou, Ming-Ming Cheng</strong></p>
<p>This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework designed to improve the effectiveness of adapting multimodal large language models (MLLMs) to video temporal grounding tasks. We reveal that existing reinforcement learning methods, such as Group Relative Policy Optimization (GRPO), rely on on-policy sampling for policy updates. However, in tasks with large temporal search spaces, this strategy becomes both inefficient and limited in performance, as it often fails to identify temporally accurate solutions. To address this limitation, TempSamp-R1 leverages ground-truth annotations as off-policy supervision to provide temporally precise guidance, effectively compensating for the sparsity and misalignment in on-policy solutions. To further stabilize training and reduce variance in reward-based updates, TempSamp-R1 provides a non-linear soft advantage computation method that dynamically reshapes the reward feedback via an asymmetric transformation. By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1 optimizes a single unified model to support both CoT and non-CoT inference modes, enabling efficient handling of queries with varying reasoning complexity. Experimental results demonstrate that TempSamp-R1 outperforms GRPO-based baselines, establishing new state-of-the-art performance on benchmark datasets: Charades-STA (<a href="mailto:&#82;&#x31;&#64;&#x30;&#x2e;&#x37;">&#82;&#x31;&#64;&#x30;&#x2e;&#x37;</a>: 52.9%, +2.7%), ActivityNet Captions (<a href="mailto:&#x52;&#x31;&#64;&#48;&#x2e;&#x35;">&#x52;&#x31;&#64;&#48;&#x2e;&#x35;</a>: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover, TempSamp-R1 shows robust few-shot generalization capabilities under limited data. Code: <a target="_blank" rel="noopener" href="https://github.com/HVision-NKU/TempSamp-R1">https://github.com/HVision-NKU/TempSamp-R1</a> </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†TempSamp-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¼ºåŒ–ç²¾ç»†è°ƒæ•´æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€‚åº”è§†é¢‘æ—¶é—´å®šä½ä»»åŠ¡çš„æ•ˆç‡ã€‚æˆ‘ä»¬å‘ç°ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå¦‚ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä¾èµ–äºç­–ç•¥æ›´æ–°è¿‡ç¨‹ä¸­çš„åœ¨çº¿ç­–ç•¥é‡‡æ ·ã€‚ç„¶è€Œï¼Œåœ¨å…·æœ‰å¤§æ—¶é—´æœç´¢ç©ºé—´çš„ä»»åŠ¡ä¸­ï¼Œæ­¤ç­–ç•¥æ—¢æ•ˆç‡ä½ä¸‹ï¼Œæ€§èƒ½ä¹Ÿå—åˆ°å±€é™ï¼Œå› ä¸ºå®ƒå¾€å¾€æ— æ³•æ‰¾åˆ°æ—¶é—´å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼ŒTempSamp-R1åˆ©ç”¨çœŸå®æ ‡æ³¨ä½œä¸ºç¦»çº¿ç­–ç•¥ç›‘ç£ï¼Œæä¾›æ—¶é—´ç²¾ç¡®çš„æŒ‡å¯¼ï¼Œæœ‰æ•ˆåœ°å¼¥è¡¥äº†åœ¨çº¿ç­–ç•¥è§£å†³æ–¹æ¡ˆä¸­çš„ç¨€ç–æ€§å’Œä¸å¯¹é½é—®é¢˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥ç¨³å®šè®­ç»ƒå’Œå‡å°‘åŸºäºå¥–åŠ±çš„æ›´æ–°çš„æ–¹å·®ï¼ŒTempSamp-R1æä¾›äº†ä¸€ç§éçº¿æ€§è½¯ä¼˜åŠ¿è®¡ç®—æ–¹æ³•ï¼Œé€šè¿‡ä¸å¯¹ç§°è½¬æ¢åŠ¨æ€é‡å¡‘å¥–åŠ±åé¦ˆã€‚é€šè¿‡é‡‡ç”¨æ··åˆçš„â€œæ€ç»´é“¾â€ï¼ˆCoTï¼‰è®­ç»ƒèŒƒå¼ï¼ŒTempSamp-R1ä¼˜åŒ–äº†ä¸€ä¸ªç»Ÿä¸€çš„å•ä¸€æ¨¡å‹ï¼Œä»¥æ”¯æŒCoTå’ŒéCoTæ¨ç†æ¨¡å¼ï¼Œä»è€Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†ä¸åŒå¤æ‚åº¦çš„æŸ¥è¯¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTempSamp-R1ä¼˜äºåŸºäºGRPOçš„åŸºçº¿æ¨¡å‹ï¼Œåœ¨åŸºå‡†æ•°æ®é›†ä¸Šåˆ›ä¸‹äº†æœ€æ–°çºªå½•ï¼šCharades-STAï¼ˆ<a href="mailto:&#x52;&#49;&#64;&#x30;&#46;&#x37;">&#x52;&#49;&#64;&#x30;&#46;&#x37;</a>: 52.9%ï¼Œ+2.7%ï¼‰ã€ActivityNet Captionsï¼ˆ<a href="mailto:&#82;&#49;&#x40;&#x30;&#46;&#x35;">&#82;&#49;&#x40;&#x30;&#46;&#x35;</a>: 56.0%ï¼Œ+5.3%ï¼‰å’ŒQVHighlightsï¼ˆmAP: 30.0%ï¼Œ+3.0%ï¼‰ã€‚æ­¤å¤–ï¼ŒTempSamp-R1åœ¨æœ‰é™æ•°æ®ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/HVision-NKU/TempSamp-R1%E3%80%82">https://github.com/HVision-NKU/TempSamp-R1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18056v1">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTempSamp-R1çš„æ–°å‹å¼ºåŒ–ç²¾ç»†è°ƒæ•´æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘æ—¶åºå®šä½ä»»åŠ¡ä¸­çš„æ•ˆç‡ã€‚é’ˆå¯¹ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤§å‹æ—¶åºæœç´¢ç©ºé—´ä¸­çš„å±€é™æ€§å’Œä¸è¶³ï¼ŒTempSamp-R1åˆ©ç”¨çœŸå®æ ‡æ³¨ä½œä¸ºç¦»çº¿ç­–ç•¥ç›‘ç£ï¼Œæä¾›ç²¾ç¡®çš„æ—¶é—´æŒ‡å¯¼ï¼Œå¹¶å¼•å…¥éçº¿æ€§è½¯ä¼˜åŠ¿è®¡ç®—æ–¹æ³•ï¼ŒåŠ¨æ€è°ƒæ•´å¥–åŠ±åé¦ˆã€‚ç»“åˆChain-of-Thoughtè®­ç»ƒèŒƒå¼ï¼ŒTempSamp-R1æ”¯æŒå¤šç§æ¨ç†æ¨¡å¼ï¼Œæé«˜å¤„ç†ä¸åŒå¤æ‚åº¦æŸ¥è¯¢çš„æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTempSamp-R1åœ¨Charades-STAã€ActivityNet Captionså’ŒQVHighlightsç­‰åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TempSamp-R1æ˜¯ä¸€ä¸ªé’ˆå¯¹è§†é¢‘æ—¶åºå®šä½ä»»åŠ¡çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å¼ºåŒ–ç²¾ç»†è°ƒæ•´æ¡†æ¶ã€‚</li>
<li>ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤§å‹æ—¶åºæœç´¢ç©ºé—´ä¸­è¡¨ç°ä¸ä½³ï¼ŒTempSamp-R1é€šè¿‡å¼•å…¥ç¦»çº¿ç­–ç•¥ç›‘ç£å’Œç²¾ç¡®æ—¶é—´æŒ‡å¯¼æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>TempSamp-R1é‡‡ç”¨éçº¿æ€§è½¯ä¼˜åŠ¿è®¡ç®—æ–¹æ³•ï¼ŒåŠ¨æ€è°ƒæ•´å¥–åŠ±åé¦ˆï¼Œè¿›ä¸€æ­¥æé«˜è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ã€‚</li>
<li>ç»“åˆChain-of-Thoughtè®­ç»ƒèŒƒå¼ï¼ŒTempSamp-R1æ”¯æŒå¤šç§æ¨ç†æ¨¡å¼ï¼Œé€‚åº”ä¸åŒå¤æ‚åº¦çš„æŸ¥è¯¢ã€‚</li>
<li>TempSamp-R1åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½ï¼ŒåŒ…æ‹¬Charades-STAã€ActivityNet Captionså’ŒQVHighlightsã€‚</li>
<li>TempSamp-R1å…·æœ‰å¼ºå¤§çš„å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨æœ‰é™æ•°æ®ä¸‹è¡¨ç°ç¨³å¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18056">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-585eeb01e1b5036bc615aaedb3aea715~resize:0:q75.jpg?source=1f5c5e47&expiration=1760138262&auth_key=1760138262-0-0-0380128442cb555725c8f1fae36c4837&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a02c9f82f60723021b14d04507190943~resize:0:q75.jpg?source=1f5c5e47&expiration=1760138270&auth_key=1760138270-0-0-5bac29171650bc8aaf60b023db1144b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a763b12df7eed6a48c0395e058a35dec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760138276&auth_key=1760138276-0-0-eab807930c71a8512b00a104e40e7ac3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f63fbf7d12ced746bb82be62d064abd6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760138283&auth_key=1760138283-0-0-b2e5a54d969f4789009d93d2fe57c38b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Everyday-Physics-in-Korean-Contexts-A-Culturally-Grounded-Physical-Reasoning-Benchmark"><a href="#Everyday-Physics-in-Korean-Contexts-A-Culturally-Grounded-Physical-Reasoning-Benchmark" class="headerlink" title="Everyday Physics in Korean Contexts: A Culturally Grounded Physical   Reasoning Benchmark"></a>Everyday Physics in Korean Contexts: A Culturally Grounded Physical   Reasoning Benchmark</h2><p><strong>Authors:Jihae Jeong, DaeYeop Lee, DongGeon Lee, Hwanjo Yu</strong></p>
<p>Existing physical commonsense reasoning benchmarks predominantly focus on Western contexts, overlooking cultural variations in physical problem-solving. To address this gap, we introduce EPiK (Everyday Physics in Korean Contexts), a novel benchmark comprising 181 binary-choice problems that test physical reasoning within Korean cultural contexts, ranging from kimchi (Korean food) to traditional fermentation. EPiK is constructed using a two-stage generation and verification pipeline to create culturally-authentic problems across 9 reasoning subtasks and 84 scenarios. Unlike approaches based on simple translation, our method generates problems organically from Korean contexts while upholding rigorous physical reasoning standards. Our evaluations show that Korean-specialized models consistently outperform general-purpose models of comparable size. This performance gap highlights the limitations of culturally-agnostic models and demonstrates the critical need for culturally-aware benchmarks to truly measure language understanding. Our EPiK is publicly available at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/jjae/EPiK">https://huggingface.co/datasets/jjae/EPiK</a>. </p>
<blockquote>
<p>ç°æœ‰çš„ç‰©ç†å¸¸è¯†æ¨ç†åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨è¥¿æ–¹èƒŒæ™¯ï¼Œå¿½è§†äº†ç‰©ç†é—®é¢˜è§£å†³ä¸­çš„æ–‡åŒ–å·®å¼‚ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†EPiKï¼ˆéŸ©å›½èƒŒæ™¯ä¸‹çš„æ—¥å¸¸ç‰©ç†å­¦ï¼‰è¿™ä¸€æ–°å‹åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«181ä¸ªäºŒå…ƒé€‰æ‹©é¢˜ï¼Œæ—¨åœ¨æµ‹è¯•éŸ©å›½æ–‡åŒ–èƒŒæ™¯ä¸‹çš„ç‰©ç†æ¨ç†èƒ½åŠ›ï¼Œæ¶µç›–ä»æ³¡èœï¼ˆéŸ©å›½é£Ÿå“ï¼‰åˆ°ä¼ ç»Ÿå‘é…µç­‰å¤šä¸ªé¢†åŸŸã€‚EPiKçš„æ„å»ºé‡‡ç”¨äº†ä¸¤é˜¶æ®µç”Ÿæˆå’ŒéªŒè¯æµç¨‹ï¼Œä»¥åˆ›å»ºæ¶µç›–9ä¸ªæ¨ç†å­ä»»åŠ¡å’Œ84ä¸ªåœºæ™¯çš„æ–‡åŒ–è®¤è¯é—®é¢˜ã€‚ä¸åŒäºåŸºäºç®€å•ç¿»è¯‘çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»éŸ©å›½èƒŒæ™¯ä¸­æœ‰æœºç”Ÿæˆé—®é¢˜ï¼ŒåŒæ—¶åšæŒä¸¥æ ¼çš„ç‰©ç†æ¨ç†æ ‡å‡†ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œé’ˆå¯¹éŸ©å›½ç‰¹è‰²çš„æ¨¡å‹æŒç»­ä¼˜äºé€šç”¨æ¨¡å‹ã€‚è¿™ç§æ€§èƒ½å·®è·çªæ˜¾äº†æ–‡åŒ–æ— çŸ¥æ¨¡å‹ï¼ˆculturally-agnostic modelsï¼‰çš„å±€é™æ€§ï¼Œå¹¶å±•ç¤ºäº†çœŸæ­£è¡¡é‡è¯­è¨€ç†è§£èƒ½åŠ›è¿«åˆ‡éœ€è¦å¯¹æ–‡åŒ–æ„è¯†çš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„EPiKå¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/jjae/EPiK">https://huggingface.co/datasets/jjae/EPiK</a>ä¸Šå…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17807v1">PDF</a> Accepted to MRL@EMNLP 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>ä¸ºäº†å¼¥è¡¥ç‰©ç†å¸¸è¯†æ¨ç†åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨è¥¿æ–¹èƒŒæ™¯ä¸‹çš„å±€é™æ€§ï¼Œå¿½è§†åœ¨ä¸åŒæ–‡åŒ–èƒŒæ™¯ä¸‹çš„ç‰©ç†é—®é¢˜è§£å†³èƒ½åŠ›çš„å·®å¼‚ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€é¡¹æ–°çš„åŸºå‡†æµ‹è¯•â€”â€”EPiKï¼ˆæ—¥å¸¸ç”Ÿæ´»ä¸­çš„ç‰©ç†å­¦åœ¨éŸ©å›½èƒŒæ™¯ä¸‹çš„åº”ç”¨ï¼‰ã€‚EPiKåŒ…å«181ä¸ªäºŒå…ƒé€‰æ‹©é¢˜ï¼Œæµ‹è¯•åœ¨éŸ©å›½æ–‡åŒ–èƒŒæ™¯ä¸‹å¯¹ç‰©ç†æ¨ç†çš„åº”ç”¨ï¼Œæ¶‰åŠéŸ©å›½é£Ÿå“å¦‚æ³¡èœå’Œä¼ ç»Ÿå‘é…µç­‰æ–¹é¢ã€‚EPiKé‡‡ç”¨ä¸¤é˜¶æ®µç”Ÿæˆå’ŒéªŒè¯æµç¨‹æ¥æ„å»ºçœŸå®åæ˜ éŸ©å›½æ–‡åŒ–çš„é—®é¢˜ï¼Œæ¶‰åŠ9ä¸ªæ¨ç†å­ä»»åŠ¡å’Œ84ä¸ªåœºæ™¯ã€‚ä¸å…¶ä»–ç®€å•ç¿»è¯‘çš„æ–¹æ³•ä¸åŒï¼ŒEPiKä»éŸ©å›½æ–‡åŒ–èƒŒæ™¯å‡ºå‘ï¼ŒåŒæ—¶éµå¾ªä¸¥æ ¼çš„ç‰©ç†æ¨ç†æ ‡å‡†ç”Ÿæˆé—®é¢˜ã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œé’ˆå¯¹éŸ©å›½ç‰¹è‰²çš„æ¨¡å‹è¡¨ç°ä¼˜äºé€šç”¨æ¨¡å‹ã€‚è¿™çªæ˜¾äº†å¿½è§†æ–‡åŒ–å·®å¼‚çš„æ¨¡å‹å±€é™æ€§ï¼Œå¼ºè°ƒå¼€å‘èƒ½å¤ŸçœŸæ­£è¡¡é‡è¯­è¨€ç†è§£èƒ½åŠ›çš„æ–‡åŒ–æ„è¯†åŸºå‡†æµ‹è¯•çš„é‡è¦æ€§ã€‚æœ¬æ–‡æä¾›çš„EPiKå¯åœ¨huggingface.co&#x2F;datasets&#x2F;jjae&#x2F;EPiKä¸Šå…¬å¼€è·å–ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>EPiKæ˜¯ä¸€ä¸ªåŸºäºéŸ©å›½æ–‡åŒ–èƒŒæ™¯çš„æ—¥å¸¸ç”Ÿæ´»ç‰©ç†å¸¸è¯†æ¨ç†åŸºå‡†æµ‹è¯•ã€‚</li>
<li>EPiKåŒ…å«181ä¸ªäºŒå…ƒé€‰æ‹©é¢˜ï¼Œæ¶µç›–éŸ©å›½é£Ÿå“ä¸ä¼ ç»Ÿå‘é…µç­‰å†…å®¹ã€‚</li>
<li>EPiKé‡‡ç”¨ä¸¤é˜¶æ®µç”Ÿæˆå’ŒéªŒè¯æµç¨‹ç¡®ä¿é—®é¢˜çš„çœŸå®æ€§å’Œç‰©ç†æ¨ç†çš„ä¸¥è°¨æ€§ã€‚</li>
<li>ä¸ç®€å•ç¿»è¯‘çš„æ–¹æ³•ä¸åŒï¼ŒEPiKä»éŸ©å›½æ–‡åŒ–èƒŒæ™¯å‡ºå‘ç”Ÿæˆé—®é¢˜ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼Œé’ˆå¯¹éŸ©å›½ç‰¹è‰²çš„æ¨¡å‹è¡¨ç°ä¼˜äºé€šç”¨æ¨¡å‹ï¼Œçªæ˜¾æ–‡åŒ–æ„è¯†çš„é‡è¦æ€§ã€‚</li>
<li>ç°æœ‰ç‰©ç†å¸¸è¯†æ¨ç†åŸºå‡†æµ‹è¯•å­˜åœ¨å¿½è§†æ–‡åŒ–å·®å¼‚çš„å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17807">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-cac5cb1ce43ea415c21674b2a5345864~resize:0:q75.jpg?source=1f5c5e47&expiration=1760138290&auth_key=1760138290-0-0-a6e6612b31e0b7fd363df7bb8c8f6f41&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4b9cfe6d966c08d5a7801850cd23fd0d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760138297&auth_key=1760138297-0-0-4ad6ea9b133ef6a40c14979a1ff72fd0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5236cd15c6aa9d88dcffa6ecdd344241~resize:0:q75.jpg?source=1f5c5e47&expiration=1760138303&auth_key=1760138303-0-0-cda660a70ae7964dad1bfd27d465496f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-944a9c67a0e12e6106707c61d81fe2c8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760138310&auth_key=1760138310-0-0-3a13988d866c53d2217175a226d3d60b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bcb7fe3e5c1704a38495621806fa16fa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760138317&auth_key=1760138317-0-0-50f5ab88dd7db778f371dfeeb9f23e35&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-53406ed085fa42dee379adc0974fde75~resize:0:q75.jpg?source=1f5c5e47&expiration=1760138324&auth_key=1760138324-0-0-defdedfb86c9500ad42be6a0fc1deefb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c5b1645662d019746728dfaa4949d762~resize:0:q75.jpg?source=1f5c5e47&expiration=1760138330&auth_key=1760138330-0-0-83f021c0dfd37eb728f36fed7d5bfb56&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Adaptive-Fast-and-Slow-Visual-Program-Reasoning-for-Long-Form-VideoQA"><a href="#Adaptive-Fast-and-Slow-Visual-Program-Reasoning-for-Long-Form-VideoQA" class="headerlink" title="Adaptive Fast-and-Slow Visual Program Reasoning for Long-Form VideoQA"></a>Adaptive Fast-and-Slow Visual Program Reasoning for Long-Form VideoQA</h2><p><strong>Authors:Chenglin Li, Feng Han,  FengTao, Ruilin Li, Qianglong Chen, Jingqi Tong, Yin Zhang, Jiaqi Wang</strong></p>
<p>Large language models (LLMs) have shown promise in generating program workflows for visual tasks. However, previous approaches often rely on closed-source models, lack systematic reasoning, and struggle with long-form video question answering (videoQA). To address these challenges, we introduce the FS-VisPR framework, an adaptive visual program reasoning approach that balances fast reasoning for simple queries with slow reasoning for difficult ones. First, we design efficient visual modules (e.g., key clip retrieval and subtitle retrieval) to support long-form video tasks. Then, we construct a diverse and high-quality fast-slow reasoning dataset with a strong LLM to align open-source language modelsâ€™ ability to generate visual program workflows as FS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple queries are directly solved by VideoLLMs, while difficult ones invoke visual program reasoning, motivated by human-like reasoning processes. During this process, low-confidence fast-thinking answers will trigger a second-stage slow-reasoning process, and a fallback mechanism to fast reasoning is activated if the program execution fails. Moreover, we improve visual programs through parameter search during both training and inference. By adjusting the parameters of the visual modules within the program, multiple variants are generated: during training, programs that yield correct answers are selected, while during inference, the program with the highest confidence result is applied. Experiments show that FS-VisPR improves both efficiency and reliability in visual program workflows. It achieves 50.4% accuracy on LVBench, surpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆè§†è§‰ä»»åŠ¡çš„ç¨‹åºå·¥ä½œæµç¨‹æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„æ–¹æ³•å¸¸å¸¸ä¾èµ–äºé—­æºæ¨¡å‹ï¼Œç¼ºä¹ç³»ç»Ÿæ¨ç†èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨é•¿è§†é¢‘é—®ç­”ï¼ˆvideoQAï¼‰æ–¹é¢é‡åˆ°å›°éš¾ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†FS-VisPRæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§è‡ªé€‚åº”çš„è§†è§‰ç¨‹åºæ¨ç†æ–¹æ³•ï¼Œèƒ½å¤Ÿå¹³è¡¡ç®€å•æŸ¥è¯¢çš„å¿«é€Ÿæ¨ç†å’Œå¤æ‚æŸ¥è¯¢çš„æ…¢é€Ÿæ¨ç†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†é«˜æ•ˆçš„è§†è§‰æ¨¡å—ï¼ˆå¦‚å…³é”®ç‰‡æ®µæ£€ç´¢å’Œå­—å¹•æ£€ç´¢ï¼‰æ¥æ”¯æŒé•¿è§†é¢‘ä»»åŠ¡ã€‚æ¥ç€ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤šæ ·ä¸”é«˜è´¨é‡çš„å¿«æ…¢æ¨ç†æ•°æ®é›†ï¼Œå¹¶å€ŸåŠ©å¼ºå¤§çš„LLMï¼Œå°†å¼€æºè¯­è¨€æ¨¡å‹ç”Ÿæˆè§†è§‰ç¨‹åºå·¥ä½œæµç¨‹çš„èƒ½åŠ›ä¸FS-LLMå¯¹é½ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¿«æ…¢æ¨ç†æ¡†æ¶ç»“åˆFS-LLMï¼šç®€å•æŸ¥è¯¢ç›´æ¥ç”±VideoLLMè§£å†³ï¼Œè€Œå¤æ‚çš„æŸ¥è¯¢åˆ™é€šè¿‡è§†è§‰ç¨‹åºæ¨ç†è§£å†³ï¼Œè¿™æ¿€å‘äº†ç±»ä¼¼äººç±»çš„æ¨ç†è¿‡ç¨‹ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œä½ä¿¡å¿ƒçš„å¿«é€Ÿæ€è€ƒç­”æ¡ˆå°†è§¦å‘ç¬¬äºŒé˜¶æ®µçš„æ…¢é€Ÿæ¨ç†è¿‡ç¨‹ï¼Œå¦‚æœç¨‹åºæ‰§è¡Œå¤±è´¥ï¼Œå°†æ¿€æ´»è¿”å›åˆ°å¿«é€Ÿæ¨ç†çš„å¤‡ä»½æœºåˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ç¨‹åºå’Œæ¨ç†è¿‡ç¨‹ä¸­çš„å‚æ•°æœç´¢æ”¹è¿›äº†è§†è§‰ç¨‹åºã€‚é€šè¿‡è°ƒæ•´ç¨‹åºä¸­è§†è§‰æ¨¡å—çš„å‚æ•°ï¼Œå¯ä»¥ç”Ÿæˆå¤šä¸ªå˜ä½“ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé€‰æ‹©å¾—å‡ºæ­£ç¡®ç­”æ¡ˆçš„ç¨‹åºï¼›åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œåº”ç”¨å…·æœ‰æœ€é«˜ç½®ä¿¡åº¦ç»“æœçš„ç¨‹åºã€‚å®éªŒè¡¨æ˜ï¼ŒFS-VisPRæé«˜äº†è§†è§‰ç¨‹åºå·¥ä½œæµç¨‹çš„æ•ˆç‡å’Œå¯é æ€§ã€‚å®ƒåœ¨LVBenchä¸Šè¾¾åˆ°äº†50.4%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†GPT-4oï¼Œä¸Qwen2.5VL-72Båœ¨VideoMMEä¸Šçš„æ€§èƒ½ç›¸åŒ¹é…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17743v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆè§†è§‰ä»»åŠ¡ç¨‹åºå·¥ä½œæµç¨‹æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å­˜åœ¨ä¾èµ–å°é—­æºä»£ç æ¨¡å‹ã€ç¼ºä¹ç³»ç»Ÿæ¨ç†å’Œåº”å¯¹é•¿è§†é¢‘é—®ç­”å›°éš¾ç­‰æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºFS-VisPRæ¡†æ¶ï¼Œé‡‡ç”¨å¿«æ…¢æ¨ç†ç»“åˆçš„æ–¹å¼ï¼Œæ”¯æŒé•¿è§†é¢‘ä»»åŠ¡ï¼Œæ„å»ºé«˜è´¨é‡æ•°æ®é›†ï¼Œè®¾è®¡å¿«æ…¢æ¨ç†æ¡†æ¶ï¼Œè§¦å‘ä½ä¿¡å¿ƒç­”æ¡ˆçš„ç¬¬äºŒé˜¶æ®µæ…¢æ¨ç†ï¼Œå¹¶åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­è°ƒæ•´å‚æ•°æé«˜ç¨‹åºæ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒFS-VisPRæé«˜äº†è§†è§‰ç¨‹åºå·¥ä½œæµç¨‹çš„æ•ˆç‡å’Œå¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ä»»åŠ¡ç¨‹åºç”Ÿæˆä¸­å…·æ½œåŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–å°é—­æºä»£ç æ¨¡å‹ã€ç¼ºä¹ç³»ç»Ÿæ¨ç†å’Œé•¿è§†é¢‘é—®ç­”èƒ½åŠ›ã€‚</li>
<li>FS-VisPRæ¡†æ¶ç»“åˆå¿«æ…¢æ¨ç†ï¼Œæ”¯æŒé•¿è§†é¢‘ä»»åŠ¡ã€‚</li>
<li>æ„å»ºé«˜è´¨é‡æ•°æ®é›†ï¼Œè®¾è®¡å¿«æ…¢æ¨ç†æ¡†æ¶ï¼Œè§¦å‘ä½ä¿¡å¿ƒç­”æ¡ˆçš„ç¬¬äºŒé˜¶æ®µæ…¢æ¨ç†ã€‚</li>
<li>è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­è°ƒæ•´å‚æ•°æé«˜ç¨‹åºæ€§èƒ½ã€‚</li>
<li>FS-VisPRæé«˜è§†è§‰ç¨‹åºå·¥ä½œæµç¨‹æ•ˆç‡å’Œå¯é æ€§ï¼Œè¾¾åˆ°è¡Œä¸šé¢†å…ˆæ°´å¹³ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17743">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3863d3bfbcda5c6d54275e40c0240522~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423093&auth_key=1760423093-0-0-40f4700a57b33cc10d9663914cc4f529&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7ac5f100268ed505820158c7c702e0b5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423101&auth_key=1760423101-0-0-6adab751d41127077a8acbdf8f24d104&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-03c9d26314c116009eb8dac56383853f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423107&auth_key=1760423107-0-0-89ed8c4b6bbf061c3076eb380b6094d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-410061511c44fafc22142b3bb2a59e9f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423115&auth_key=1760423115-0-0-6f0084d20b6b178d37bba3b131d51420&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2c564d49951dd30c12b1976594c7aac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423122&auth_key=1760423122-0-0-b37d2462c80ded103e58d05a99d66238&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ConfClip-Confidence-Weighted-and-Clipped-Reward-for-Reinforcement-Learning-in-LLMs"><a href="#ConfClip-Confidence-Weighted-and-Clipped-Reward-for-Reinforcement-Learning-in-LLMs" class="headerlink" title="ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement   Learning in LLMs"></a>ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement   Learning in LLMs</h2><p><strong>Authors:Bonan Zhang, Zhongqi Chen, Bowen Song, Qinya Li, Fan Wu, Guihai Chen</strong></p>
<p>Reinforcement learning (RL) has become a standard paradigm for refining large language models (LLMs) beyond pre-training and instruction tuning. A prominent line of work is RL with verifiable rewards (RLVR), which leverages automatically verifiable outcomes (e.g., correctness or executability) to generate reward signals. While efficient, this framework faces two key limitations: First, its binary feedback is too sparse to capture the quality of the reasoning process. Second, its coarse-grained rewards potentially lead to vanishing gradients. Inspired by observations from human learning, we introduce a RL technique that integrates verifiable outcomes with the modelâ€™s own confidence estimates. This joint design enriches the reward signal, providing finer-grained feedback and implicitly supervising the reasoning process. Experimental results demonstrate that our proposed method enhances RL performance across multiple datasets and reduces token consumption during inference, while incurring negligible additional training cost. Moreover, it can be used as a plug-in module to enhance other state-of-the-art RL methods. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²ç»æˆä¸ºè¶…è¶Šé¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒæ¥ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ‡å‡†èŒƒå¼ã€‚ä¸€æ¡çªå‡ºçš„å·¥ä½œçº¿æ˜¯å¸¦å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ï¼Œå®ƒåˆ©ç”¨å¯è‡ªåŠ¨éªŒè¯çš„ç»“æœï¼ˆå¦‚æ­£ç¡®æ€§æˆ–å¯æ‰§è¡Œæ€§ï¼‰æ¥ç”Ÿæˆå¥–åŠ±ä¿¡å·ã€‚è™½ç„¶æ•ˆç‡å¾ˆé«˜ï¼Œä½†è¿™ä¸ªæ¡†æ¶é¢ä¸´ä¸¤ä¸ªä¸»è¦å±€é™ï¼šé¦–å…ˆï¼Œå…¶äºŒè¿›åˆ¶åé¦ˆè¿‡äºç¨€ç–ï¼Œæ— æ³•æ•æ‰æ¨ç†è¿‡ç¨‹çš„è´¨é‡ï¼›å…¶æ¬¡ï¼Œå…¶ç²—ç²’åº¦çš„å¥–åŠ±å¯èƒ½å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ã€‚å—äººç±»å­¦ä¹ çš„è§‚å¯Ÿå¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å°†å¯éªŒè¯çš„ç»“æœä¸æ¨¡å‹æœ¬èº«çš„ç½®ä¿¡åº¦ä¼°è®¡ç›¸ç»“åˆã€‚è¿™ç§è”åˆè®¾è®¡ä¸°å¯Œäº†å¥–åŠ±ä¿¡å·ï¼Œæä¾›äº†æ›´ç²¾ç»†çš„åé¦ˆï¼Œå¹¶éšå«åœ°ç›‘ç£äº†æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæé«˜äº†å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ï¼Œé™ä½äº†æ¨ç†è¿‡ç¨‹ä¸­çš„ä»¤ç‰Œæ¶ˆè€—ï¼ŒåŒæ—¶åªäº§ç”Ÿäº†å¾®ä¸è¶³é“çš„é¢å¤–è®­ç»ƒæˆæœ¬ã€‚æ­¤å¤–ï¼Œå®ƒå¯ä»¥ä½œä¸ºæ’ä»¶æ¨¡å—ç”¨äºå¢å¼ºå…¶ä»–æœ€å…ˆè¿›å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17730v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ‡å‡†èŒƒå¼ï¼Œå°¤å…¶åœ¨é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒä¹‹åã€‚RLVRæ˜¯å…¶ä¸­çš„ä¸€æ¡é‡è¦ç ”ç©¶çº¿è·¯ï¼Œå®ƒåˆ©ç”¨å¯è‡ªåŠ¨éªŒè¯çš„ç»“æœï¼ˆå¦‚æ­£ç¡®æ€§ï¼‰æ¥ç”Ÿæˆå¥–åŠ±ä¿¡å·ã€‚ç„¶è€Œï¼Œè¯¥æ¡†æ¶å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™ï¼šä¸€æ˜¯å…¶åé¦ˆç¨€ç–ï¼Œæ— æ³•æ•æ‰æ¨ç†è¿‡ç¨‹çš„è´¨é‡ï¼›äºŒæ˜¯å…¶ç²—ç²’åº¦çš„å¥–åŠ±å¯èƒ½å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ã€‚æœ¬ç ”ç©¶å—äººç±»å­¦ä¹ çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå¯éªŒè¯ç»“æœä¸æ¨¡å‹è‡ªèº«ç½®ä¿¡åº¦ä¼°è®¡çš„RLæŠ€æœ¯ã€‚è¿™ç§è”åˆè®¾è®¡ä¸°å¯Œäº†å¥–åŠ±ä¿¡å·ï¼Œæä¾›äº†æ›´ç²¾ç»†çš„åé¦ˆï¼Œå¹¶éšå«åœ°ç›‘ç£äº†æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æé«˜äº†å¤šä¸ªæ•°æ®é›†ä¸Šçš„RLæ€§èƒ½ï¼Œé™ä½äº†æ¨ç†æ—¶çš„ä»¤ç‰Œæ¶ˆè€—ï¼Œä¸”å‡ ä¹ä¸å¢åŠ é¢å¤–çš„è®­ç»ƒæˆæœ¬ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥ä½œä¸ºæ’ä»¶æ¨¡å—å¢å¼ºå…¶ä»–å…ˆè¿›çš„RLæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å·²æˆä¸ºä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ ‡å‡†æ–¹æ³•ã€‚</li>
<li>RLVRåˆ©ç”¨å¯è‡ªåŠ¨éªŒè¯çš„ç»“æœç”Ÿæˆå¥–åŠ±ä¿¡å·ï¼Œä½†å­˜åœ¨åé¦ˆç¨€ç–å’Œç²—ç²’åº¦å¥–åŠ±çš„é—®é¢˜ã€‚</li>
<li>ç»“åˆå¯éªŒè¯ç»“æœä¸æ¨¡å‹è‡ªèº«ç½®ä¿¡åº¦ä¼°è®¡çš„RLæŠ€æœ¯è¢«æå‡ºï¼Œä»¥ä¸°å¯Œå¥–åŠ±ä¿¡å·å¹¶æä¾›æ›´ç²¾ç»†çš„åé¦ˆã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†å¤šä¸ªæ•°æ®é›†ä¸Šçš„RLæ€§èƒ½ï¼Œé™ä½äº†æ¨ç†æ—¶çš„ä»¤ç‰Œæ¶ˆè€—ã€‚</li>
<li>è¯¥æ–¹æ³•å‡ ä¹ä¸å¢åŠ é¢å¤–çš„è®­ç»ƒæˆæœ¬ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä½œä¸ºæ’ä»¶æ¨¡å—å¢å¼ºå…¶ä»–å…ˆè¿›çš„RLæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17730">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-96dd758acc93017237f01cbf29c1dbaf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423130&auth_key=1760423130-0-0-16ffff10ca31d68d773975f507eb84e1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-420ab5b94445ef28080cf0e83df8cbd2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423137&auth_key=1760423137-0-0-5d1c76ac47c846fea49ccac8a06f95b1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-39284dc37134a99e27728d0e4f8b3385~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423143&auth_key=1760423143-0-0-6734ae2e45ac1bb1dd1b63c0ee7edea0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5e1c3a1df3b1cbef2daee129dd547529~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423150&auth_key=1760423150-0-0-c776cdfe5401eab22edcf5a692a66d68&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="EngiBench-A-Benchmark-for-Evaluating-Large-Language-Models-on-Engineering-Problem-Solving"><a href="#EngiBench-A-Benchmark-for-Evaluating-Large-Language-Models-on-Engineering-Problem-Solving" class="headerlink" title="EngiBench: A Benchmark for Evaluating Large Language Models on   Engineering Problem Solving"></a>EngiBench: A Benchmark for Evaluating Large Language Models on   Engineering Problem Solving</h2><p><strong>Authors:Xiyuan Zhou, Xinlei Wang, Yirui He, Yang Wu, Ruixi Zou, Yuheng Cheng, Yulu Xie, Wenxuan Liu, Huan Zhao, Yan Xu, Jinjin Gu, Junhua Zhao</strong></p>
<p>Large language models (LLMs) have shown strong performance on mathematical reasoning under well-posed conditions. However, real-world engineering problems require more than mathematical symbolic computation â€“ they need to deal with uncertainty, context, and open-ended scenarios. Existing benchmarks fail to capture these complexities. We introduce EngiBench, a hierarchical benchmark designed to evaluate LLMs on solving engineering problems. It spans three levels of increasing difficulty (foundational knowledge retrieval, multi-step contextual reasoning, and open-ended modeling) and covers diverse engineering subfields. To facilitate a deeper understanding of model performance, we systematically rewrite each problem into three controlled variants (perturbed, knowledge-enhanced, and math abstraction), enabling us to separately evaluate the modelâ€™s robustness, domain-specific knowledge, and mathematical reasoning abilities. Experiment results reveal a clear performance gap across levels: models struggle more as tasks get harder, perform worse when problems are slightly changed, and fall far behind human experts on the high-level engineering tasks. These findings reveal that current LLMs still lack the high-level reasoning needed for real-world engineering, highlighting the need for future models with deeper and more reliable problem-solving capabilities. Our source code and data are available at <a target="_blank" rel="noopener" href="https://github.com/EngiBench/EngiBench">https://github.com/EngiBench/EngiBench</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¡ä»¶è‰¯å¥½çš„æƒ…å†µä¸‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œä¸­çš„å·¥ç¨‹é—®é¢˜ä¸ä»…ä»…éœ€è¦æ•°å­¦ç¬¦å·è®¡ç®—ï¼Œå®ƒä»¬è¿˜éœ€è¦å¤„ç†ä¸ç¡®å®šæ€§ã€ä¸Šä¸‹æ–‡å’Œå¼€æ”¾åœºæ™¯ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•æ— æ³•æ•æ‰è¿™äº›å¤æ‚æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†EngiBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ†å±‚åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMè§£å†³å·¥ç¨‹é—®é¢˜çš„èƒ½åŠ›ã€‚å®ƒæ¶µç›–äº†ä¸‰ä¸ªéš¾åº¦é€’å¢çš„çº§åˆ«ï¼ˆåŸºç¡€çŸ¥è¯†æ£€ç´¢ã€å¤šæ­¥éª¤ä¸Šä¸‹æ–‡æ¨ç†å’Œå¼€æ”¾å¼å»ºæ¨¡ï¼‰ï¼Œå¹¶æ¶µç›–äº†å¤šæ ·åŒ–çš„å·¥ç¨‹å­é¢†åŸŸã€‚ä¸ºäº†åŠ æ·±å¯¹æ¨¡å‹æ€§èƒ½çš„ç†è§£ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ä¸ºæ¯ä¸ªé—®é¢˜é‡æ–°ç¼–å†™äº†ä¸‰ç§å—æ§å˜ä½“ï¼ˆæ‰°åŠ¨ã€çŸ¥è¯†å¢å¼ºå’Œæ•°å­¦æŠ½è±¡ï¼‰ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ†åˆ«è¯„ä¼°æ¨¡å‹çš„ç¨³å¥æ€§ã€ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†å’Œæ•°å­¦æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ­ç¤ºäº†å„çº§ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼šéšç€ä»»åŠ¡éš¾åº¦çš„å¢åŠ ï¼Œæ¨¡å‹çš„å›°éš¾ç¨‹åº¦æ›´å¤§ï¼Œå½“é—®é¢˜ç¨ä½œæ›´æ”¹æ—¶ï¼Œè¡¨ç°æ›´å·®ï¼Œå¹¶ä¸”åœ¨é«˜çº§å·¥ç¨‹ä»»åŠ¡ä¸Šè¿œè¿œè½åäºäººç±»ä¸“å®¶ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»ç„¶ç¼ºä¹çœŸå®ä¸–ç•Œå·¥ç¨‹æ‰€éœ€çš„é«˜çº§æ¨ç†èƒ½åŠ›ï¼Œè¿™å¼ºè°ƒäº†æœªæ¥æ¨¡å‹éœ€è¦æ›´æ·±ã€æ›´å¯é çš„è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æºä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/EngiBench/EngiBench%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/EngiBench/EngiBenchè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17677v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨é¢å¯¹ç°å®ä¸–ç•Œå·¥ç¨‹é—®é¢˜æ—¶ï¼Œéœ€è¦å¤„ç†ä¸ç¡®å®šæ€§ã€ä¸Šä¸‹æ–‡å’Œå¼€æ”¾åœºæ™¯ç­‰å¤æ‚å› ç´ ï¼Œç°æœ‰åŸºå‡†æµ‹è¯•æ— æ³•æ•æ‰è¿™äº›å¤æ‚æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†EngiBenchï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°LLMè§£å†³å·¥ç¨‹é—®é¢˜èƒ½åŠ›çš„åˆ†å±‚åŸºå‡†æµ‹è¯•ã€‚å®ƒæ¶µç›–äº†ä¸‰ä¸ªéš¾åº¦é€’å¢çš„çº§åˆ«ï¼ˆåŸºç¡€çŸ¥è¯†æ£€ç´¢ã€å¤šæ­¥éª¤ä¸Šä¸‹æ–‡æ¨ç†å’Œå¼€æ”¾å»ºæ¨¡ï¼‰ï¼Œå¹¶æ¶‰åŠå¤šä¸ªå·¥ç¨‹å­é¢†åŸŸã€‚é€šè¿‡ç³»ç»Ÿåœ°æ”¹å†™æ¯ä¸ªé—®é¢˜ä¸ºä¸‰ç§å—æ§å˜ä½“ï¼ˆæ‰°åŠ¨ã€çŸ¥è¯†å¢å¼ºå’Œæ•°å­¦æŠ½è±¡ï¼‰ï¼Œå¯ä»¥å•ç‹¬è¯„ä¼°æ¨¡å‹çš„ç¨³å¥æ€§ã€é¢†åŸŸç‰¹å®šçŸ¥è¯†å’Œæ•°å­¦æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨é¢ä¸´æ›´å¤æ‚çš„ä»»åŠ¡æ—¶è¡¨ç°æŒ£æ‰ï¼Œå½“é—®é¢˜ç¨å¾®æ”¹å˜æ—¶è¡¨ç°æ›´å·®ï¼Œè€Œåœ¨é«˜çº§å·¥ç¨‹ä»»åŠ¡ä¸Šè¿œè¿œè½åäºäººç±»ä¸“å®¶ã€‚è¿™æ­ç¤ºäº†å½“å‰LLMä»ç„¶ç¼ºä¹çœŸå®ä¸–ç•Œå·¥ç¨‹æ‰€éœ€çš„é«˜çº§æ¨ç†èƒ½åŠ›ï¼Œå¼ºè°ƒæœªæ¥æ¨¡å‹éœ€è¦æ›´æ·±åˆ»ã€æ›´å¯é çš„è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è§£å†³å·¥ç¨‹é—®é¢˜æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•æ— æ³•æ•æ‰å·¥ç¨‹é—®é¢˜ä¸­çš„å¤æ‚æ€§ï¼Œå¦‚ä¸ç¡®å®šæ€§ã€ä¸Šä¸‹æ–‡å’Œå¼€æ”¾åœºæ™¯ã€‚</li>
<li>EngiBenchæ˜¯ä¸€ä¸ªæ–°çš„åˆ†å±‚åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMè§£å†³å·¥ç¨‹é—®é¢˜çš„èƒ½åŠ›ã€‚</li>
<li>EngiBenchåŒ…å«ä¸‰ä¸ªéš¾åº¦é€’å¢çš„çº§åˆ«ï¼Œè¦†ç›–å¤šä¸ªå·¥ç¨‹å­é¢†åŸŸã€‚</li>
<li>é€šè¿‡ç³»ç»Ÿæ”¹å†™é—®é¢˜ï¼Œå¯ä»¥å•ç‹¬è¯„ä¼°æ¨¡å‹çš„ç¨³å¥æ€§ã€é¢†åŸŸçŸ¥è¯†å’Œæ•°å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMåœ¨é¢ä¸´æ›´å¤æ‚çš„ä»»åŠ¡æ—¶è¡¨ç°ä¸è¶³ï¼Œéœ€è¦å¢å¼ºé«˜çº§æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17677">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e996245631e63045f5ba5a146be1b9cb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423157&auth_key=1760423157-0-0-d13c9ff8a75085ff9f41e6bae69ee340&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a3ba94b15e0ccc98d79c19f1e1bc7efd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423165&auth_key=1760423165-0-0-559aec776f56587a6f107a84f9a03325&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aec3d48bce9e554b155068271105e2e1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423172&auth_key=1760423172-0-0-11abd6014a344d7b554edec93690baaa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MSCoRe-A-Benchmark-for-Multi-Stage-Collaborative-Reasoning-in-LLM-Agents"><a href="#MSCoRe-A-Benchmark-for-Multi-Stage-Collaborative-Reasoning-in-LLM-Agents" class="headerlink" title="MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM   Agents"></a>MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM   Agents</h2><p><strong>Authors:Yuzhen Lei, Hongbin Xie, Jiaxing Zhao, Shuangxue Liu, Xuan Song</strong></p>
<p>Large Language Models (LLMs) have excelled in question-answering (QA) tasks within single domains. However, their reasoning and coordination capabilities in complex, multi-stage scenarios remain underexplored. Existing benchmarks typically focus on isolated tasks or narrow domains, overlooking modelsâ€™ abilities for multi-stage collaboration and optimization without explicit external guidance. To bridge this gap, we propose \textbf{MSCoRe}, a novel benchmark comprising 126696 domain-specific QA instances spanning scenarios in automotive, pharmaceutical, electronics, and energy sectors. The dataset is created using a structured three-phase pipeline: dynamic sampling, iterative question-answer generation, and a multi-level quality assessment to ensure data quality. Tasks are further categorized into three difficulty levels according to stage coverage and complexity. With MSCoRe, we have conducted a comprehensive evaluation of various state-of-the-art LLM agents. The commercial models performed best across all tasks and scenarios, but a notable gap in ROUGE scores remains between simple and complex tasks. We also tested the modelsâ€™ robustness and found that their performance is negatively affected by noisy data. MSCoRe provides a valuable new resource for the community to evaluate and improve multi-stage reasoning in LLM agents. The code and data are available at <a target="_blank" rel="noopener" href="https://github.com/D3E0-source/MSCoRE">https://github.com/D3E0-source/MSCoRE</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å•ä¸€é¢†åŸŸå†…çš„é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤æ‚å¤šé˜¶æ®µåœºæ™¯ä¸­çš„æ¨ç†å’Œåä½œèƒ½åŠ›ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸ä¾§é‡äºå­¤ç«‹çš„ä»»åŠ¡æˆ–ç‹­çª„çš„é¢†åŸŸï¼Œå¿½è§†äº†æ¨¡å‹åœ¨å¤šä¸ªé˜¶æ®µåä½œå’Œä¼˜åŒ–è€Œæ— éœ€æ˜ç¡®å¤–éƒ¨æŒ‡å¯¼çš„èƒ½åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>MSCoRe</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«126696ä¸ªç‰¹å®šé¢†åŸŸçš„é—®ç­”å®ä¾‹ï¼Œæ¶µç›–æ±½è½¦ã€åˆ¶è¯ã€ç”µå­å’Œèƒ½æºé¢†åŸŸã€‚è¯¥æ•°æ®é›†æ˜¯ä½¿ç”¨ç»“æ„åŒ–ä¸‰é˜¶æ®µç®¡é“åˆ›å»ºçš„ï¼šåŠ¨æ€é‡‡æ ·ã€è¿­ä»£é—®ç­”ç”Ÿæˆå’Œå¤šçº§è´¨é‡è¯„ä¼°ï¼Œä»¥ç¡®ä¿æ•°æ®è´¨é‡ã€‚ä»»åŠ¡æ ¹æ®é˜¶æ®µè¦†ç›–èŒƒå›´å’Œå¤æ‚æ€§è¿›ä¸€æ­¥åˆ†ä¸ºä¸‰çº§éš¾åº¦ã€‚é€šè¿‡MSCoReï¼Œæˆ‘ä»¬å¯¹å„ç§æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚å•†ä¸šæ¨¡å‹åœ¨æ‰€æœ‰ä»»åŠ¡å’Œåœºæ™¯ä¸­è¡¨ç°æœ€ä½³ï¼Œä½†åœ¨ç®€å•ä»»åŠ¡å’Œå¤æ‚ä»»åŠ¡ä¹‹é—´ä»ç„¶å­˜åœ¨æ˜æ˜¾çš„ROUGEè¯„åˆ†å·®è·ã€‚æˆ‘ä»¬è¿˜æµ‹è¯•äº†æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œå‘ç°å™ªå£°æ•°æ®ä¼šå¯¹æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚MSCoReä¸ºç¤¾åŒºæä¾›äº†ä¸€ä¸ªå®è´µçš„æ–°èµ„æºï¼Œç”¨äºè¯„ä¼°å’Œæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†çš„å¤šé˜¶æ®µæ¨ç†èƒ½åŠ›ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/D3E0-source/MSCoRE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/D3E0-source/MSCoREæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>Translation</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17628v1">PDF</a> 10 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å•ä¸€é¢†åŸŸå†…çš„é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚å¤šé˜¶æ®µåœºæ™¯ä¸­çš„æ¨ç†å’Œåä½œèƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨å­¤ç«‹ä»»åŠ¡æˆ–ç‹­çª„é¢†åŸŸï¼Œå¿½è§†äº†æ¨¡å‹åœ¨å¤šé˜¶æ®µåä½œå’Œä¼˜åŒ–æ–¹é¢çš„èƒ½åŠ›ï¼Œä¸”ç¼ºä¹æ˜ç¡®çš„å¤–éƒ¨æŒ‡å¯¼ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†MSCoReåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«126696ä¸ªç‰¹å®šé¢†åŸŸçš„é—®ç­”å®ä¾‹ï¼Œæ¶µç›–æ±½è½¦ã€åˆ¶è¯ã€ç”µå­å’Œèƒ½æºç­‰é¢†åŸŸã€‚è¯¥æ•°æ®é›†é€šè¿‡ç»“æ„åŒ–ä¸‰é˜¶æ®µç®¡é“åˆ›å»ºï¼ŒåŒ…æ‹¬åŠ¨æ€é‡‡æ ·ã€è¿­ä»£é—®ç­”ç”Ÿæˆå’Œå¤šçº§è´¨é‡è¯„ä¼°ï¼Œä»¥ç¡®ä¿æ•°æ®è´¨é‡ã€‚ä»»åŠ¡æ ¹æ®é˜¶æ®µè¦†ç›–å’Œå¤æ‚æ€§åˆ†ä¸ºä¸‰çº§éš¾åº¦ã€‚æˆ‘ä»¬å¯¹å„ç§æœ€æ–°LLMä»£ç†è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå‘ç°å•†ä¸šæ¨¡å‹åœ¨æ‰€æœ‰ä»»åŠ¡å’Œåœºæ™¯ä¸­è¡¨ç°æœ€ä½³ï¼Œä½†ç®€å•ä»»åŠ¡å’Œå¤æ‚ä»»åŠ¡ä¹‹é—´çš„ROUGEåˆ†æ•°ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚æˆ‘ä»¬è¿˜æµ‹è¯•äº†æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œå‘ç°å™ªå£°æ•°æ®å¯¹æ€§èƒ½æœ‰è´Ÿé¢å½±å“ã€‚MSCoReä¸ºç¤¾åŒºæä¾›äº†ä¸€ä¸ªå®è´µçš„æ–°èµ„æºï¼Œç”¨äºè¯„ä¼°å’Œæé«˜LLMä»£ç†çš„å¤šé˜¶æ®µæ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å•ä¸€é¢†åŸŸé—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤šé˜¶æ®µå¤æ‚åœºæ™¯ä¸­çš„æ¨ç†å’Œåä½œèƒ½åŠ›æœ‰å¾…æ¢ç´¢ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨å­¤ç«‹ä»»åŠ¡æˆ–ç‹­çª„é¢†åŸŸï¼Œç¼ºä¹å¤šé˜¶æ®µåä½œå’Œä¼˜åŒ–çš„æµ‹è¯•ã€‚</li>
<li>MSCoReåŸºå‡†æµ‹è¯•æ˜¯ä¸€ä¸ªå…¨æ–°çš„æ•°æ®é›†ï¼ŒåŒ…å«å¤šä¸ªé¢†åŸŸï¼ˆæ±½è½¦ã€åˆ¶è¯ç­‰ï¼‰çš„126696ä¸ªé—®ç­”å®ä¾‹ï¼Œç”¨äºè¯„ä¼°LLMçš„å¤šé˜¶æ®µæ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ•°æ®é›†é‡‡ç”¨ç»“æ„åŒ–ä¸‰é˜¶æ®µç®¡é“åˆ›å»ºï¼Œç¡®ä¿æ•°æ®è´¨é‡ã€‚</li>
<li>ä»»åŠ¡éš¾åº¦åˆ†ä¸ºä¸‰çº§ï¼Œæ¶µç›–ä¸åŒçš„é˜¶æ®µè¦†ç›–å’Œå¤æ‚æ€§ã€‚</li>
<li>å•†ä¸šæ¨¡å‹åœ¨æ‰€æœ‰ä»»åŠ¡å’Œåœºæ™¯ä¸­è¡¨ç°æœ€ä½³ï¼Œä½†ç®€å•å’Œå¤æ‚ä»»åŠ¡ä¹‹é—´çš„æ€§èƒ½å·®è·ä»ç„¶å­˜åœ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17628">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f7ce5270b0b380ecb6b025422265e07f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423180&auth_key=1760423180-0-0-b5eb94e1d7b7b92e2f873893265cfa4d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-df1c8ae4f4726ab8b9c98306a1a94a63~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423187&auth_key=1760423187-0-0-fc2449221d89926a88a92f49aa248822&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-11013dbce4662a11acbed89de9d2152b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423195&auth_key=1760423195-0-0-b551f49fdbd6088168700d78267f710b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0617dd7f460179a64aa6711fd5bd20dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423202&auth_key=1760423202-0-0-3fae12225e8da2fff125ae2941ad5e57&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-700146639677cc88091c7a48401cd7c1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423209&auth_key=1760423209-0-0-4739762654ebf4a2c63d88c062a3aac1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Table2LaTeX-RL-High-Fidelity-LaTeX-Code-Generation-from-Table-Images-via-Reinforced-Multimodal-Language-Models"><a href="#Table2LaTeX-RL-High-Fidelity-LaTeX-Code-Generation-from-Table-Images-via-Reinforced-Multimodal-Language-Models" class="headerlink" title="Table2LaTeX-RL: High-Fidelity LaTeX Code Generation from Table Images   via Reinforced Multimodal Language Models"></a>Table2LaTeX-RL: High-Fidelity LaTeX Code Generation from Table Images   via Reinforced Multimodal Language Models</h2><p><strong>Authors:Jun Ling, Yao Qi, Tao Huang, Shibo Zhou, Yanqin Huang, Jiang Yang, Ziqi Song, Ying Zhou, Yang Yang, Heng Tao Shen, Peng Wang</strong></p>
<p>In this work, we address the task of table image to LaTeX code generation, with the goal of automating the reconstruction of high-quality, publication-ready tables from visual inputs. A central challenge of this task lies in accurately handling complex tables â€“ those with large sizes, deeply nested structures, and semantically rich or irregular cell content â€“ where existing methods often fail. We begin with a comprehensive analysis, identifying key challenges and highlighting the limitations of current evaluation protocols. To overcome these issues, we propose a reinforced multimodal large language model (MLLM) framework, where a pre-trained MLLM is fine-tuned on a large-scale table-to-LaTeX dataset. To further improve generation quality, we introduce a dual-reward reinforcement learning strategy based on Group Relative Policy Optimization (GRPO). Unlike standard approaches that optimize purely over text outputs, our method incorporates both a structure-level reward on LaTeX code and a visual fidelity reward computed from rendered outputs, enabling direct optimization of the visual output quality. We adopt a hybrid evaluation protocol combining TEDS-Structure and CW-SSIM, and show that our method achieves state-of-the-art performance, particularly on structurally complex tables, demonstrating the effectiveness and robustness of our approach. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è‡´åŠ›äºå°†è¡¨æ ¼å›¾åƒè½¬æ¢ä¸ºLaTeXä»£ç ç”Ÿæˆçš„ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯å®ç°ä»è§†è§‰è¾“å…¥è‡ªåŠ¨é‡å»ºé«˜è´¨é‡ã€é€‚åˆå‡ºç‰ˆçš„è¡¨æ ¼ã€‚æ­¤ä»»åŠ¡çš„æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºå‡†ç¡®å¤„ç†å¤æ‚çš„è¡¨æ ¼ï¼Œè¿™äº›è¡¨æ ¼å…·æœ‰å¤§å°ºã€æ·±åº¦åµŒå¥—çš„ç»“æ„å’Œè¯­ä¹‰ä¸°å¯Œæˆ–ä¸è§„åˆ™çš„å•å…ƒæ ¼å†…å®¹ï¼Œç°æœ‰çš„æ–¹æ³•é€šå¸¸åœ¨è¿™é‡Œå¤±æ•ˆã€‚æˆ‘ä»¬é¦–å…ˆè¿›è¡Œå…¨é¢åˆ†æï¼Œç¡®å®šå…³é”®æŒ‘æˆ˜å¹¶å¼ºè°ƒå½“å‰è¯„ä¼°åè®®çš„å±€é™æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºçš„å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¡†æ¶ï¼Œå…¶ä¸­å¯¹é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”å¤§è§„æ¨¡è¡¨æ ¼åˆ°LaTeXæ•°æ®é›†çš„è®­ç»ƒã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜ç”Ÿæˆè´¨é‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„åŒé‡å¥–åŠ±å¼ºåŒ–å­¦ä¹ ç­–ç•¥ã€‚ä¸å…¶ä»–ä»…é’ˆå¯¹æ–‡æœ¬è¾“å‡ºçš„ä¼˜åŒ–æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†LaTeXä»£ç çš„åŸºäºç»“æ„å±‚æ¬¡çš„å¥–åŠ±ä»¥åŠæ ¹æ®æ¸²æŸ“è¾“å‡ºè®¡ç®—çš„è§†è§‰ä¿çœŸåº¦å¥–åŠ±ï¼Œå®ç°äº†å¯¹è§†è§‰è¾“å‡ºè´¨é‡çš„ç›´æ¥ä¼˜åŒ–ã€‚æˆ‘ä»¬é‡‡ç”¨ç»“åˆäº†TEDSç»“æ„å’ŒCW-SSIMçš„æ··åˆè¯„ä¼°åè®®ï¼Œå¹¶å±•ç¤ºæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†å‰æ²¿æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»“æ„å¤æ‚çš„è¡¨æ ¼ä¸Šï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17589v1">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä¸»è¦ä»‹ç»äº†ä»è¡¨æ ¼å›¾åƒç”ŸæˆLaTeXä»£ç çš„ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯å®ç°é«˜è´¨é‡è‡ªåŠ¨åŒ–é‡å»ºå‡ºç‰ˆç‰©å¯ç”¨è¡¨æ ¼ã€‚å…¶è§£å†³äº†å¤„ç†å¤æ‚è¡¨æ ¼çš„ä¸­å¿ƒé—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå¢å¼ºçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¡†æ¶å’ŒåŸºäºé›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–çš„åŒå¥–åŠ±å¼ºåŒ–å­¦ä¹ ç­–ç•¥ã€‚æ­¤æ–¹æ³•é‡‡ç”¨ç»“åˆTEDSç»“æ„å’ŒCW-SSIMçš„è¯„ä»·åè®®ï¼Œå¹¶ä¸”åœ¨ç»“æ„ä¸Šå¤æ‚è¡¨æ ¼çš„æ€§èƒ½æ–¹é¢è¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚æ•´ä¸ªä½“ç³»ä½“ç°äº†æœ¬æ–¹æ³•çš„ä¼˜è¶Šæ€§å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å¯¹è±¡ï¼šæœ¬æ–‡ä¸“æ³¨äºè§£å†³ä»è¡¨æ ¼å›¾åƒç”ŸæˆLaTeXä»£ç çš„ä»»åŠ¡ï¼Œæ—¨åœ¨å®ç°é«˜è´¨é‡è¡¨æ ¼çš„è§†è§‰è¾“å…¥é‡å»ºã€‚è¿™ä¸€é¢†åŸŸçš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜åœ¨äºå¤„ç†å¤æ‚çš„è¡¨æ ¼æ•°æ®ã€‚</li>
<li>ä¸»è¦æŒ‘æˆ˜ï¼šå¤„ç†å¤§å‹ã€åµŒå¥—ç»“æ„å¤æ‚ã€è¯­ä¹‰ä¸°å¯Œæˆ–ä¸è§„åˆ™å•å…ƒæ ¼å†…å®¹çš„å¤æ‚è¡¨æ ¼æ˜¯è¿™ä¸€ä»»åŠ¡çš„ä¸»è¦æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€åœ¨è¿™ä¸€é—®é¢˜ä¸Šéš¾ä»¥å–å¾—æ»¡æ„çš„ç»“æœã€‚</li>
<li>åˆ›æ–°è§£å†³æ–¹æ¡ˆï¼šæ–‡ç« æå‡ºäº†ä¸€ä¸ªå¢å¼ºå‹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¡†æ¶æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚åŒæ—¶ç»“åˆé¢„è®­ç»ƒçš„å¤§å‹æ¨¡å‹å’Œåœ¨å¤§å‹è¡¨æ ¼åˆ°LaTeXæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒçš„æ–¹æ³•ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åº”ç”¨ï¼šå¼•å…¥äº†ä¸€ç§åŸºäºé›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–çš„åŒå¥–åŠ±å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œè¿›ä¸€æ­¥æé«˜ç”Ÿæˆè´¨é‡ã€‚è¿™ç§æ–¹æ³•ç»“åˆäº†LaTeXä»£ç çš„ç»“æ„çº§åˆ«å¥–åŠ±å’Œæ¸²æŸ“è¾“å‡ºçš„è§†è§‰ä¿çœŸåº¦å¥–åŠ±ï¼Œèƒ½å¤Ÿç›´æ¥ä¼˜åŒ–è§†è§‰è¾“å‡ºè´¨é‡ã€‚</li>
<li>è¯„ä»·åè®®ï¼šé‡‡ç”¨äº†ç»“åˆTEDSç»“æ„å’ŒCW-SSIMçš„æ··åˆè¯„ä»·åè®®ï¼Œå¯¹æ¨¡å‹æ€§èƒ½è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚è¿™ç§è¯„ä»·åè®®èƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ æ¨¡å‹åœ¨å¤„ç†å¤æ‚ç»“æ„è¡¨æ ¼æ—¶çš„æ€§èƒ½ã€‚</li>
<li>æ€§èƒ½è¡¨ç°ï¼šè¯¥ç ”ç©¶å®ç°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ç»“æ„å¤æ‚çš„è¡¨æ ¼ä¸Šå±•ç¤ºäº†ä¼˜è¶Šçš„æ€§èƒ½è¡¨ç°ï¼Œè¿™ä½“ç°äº†æ‰€æå‡ºæ–¹æ³•çš„ç¨³å¥æ€§å’Œå®ç”¨æ€§ã€‚è¯¥ç ”ç©¶è¯æ˜äº†æ‰€æå‡ºæ–¹æ³•åœ¨è¡¨æ ¼å›¾åƒåˆ°LaTeXä»£ç ç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ä¸Šå…·æœ‰è¾ƒé«˜çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17589">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-63fffaa3ffee964f56741e212280bf97~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423216&auth_key=1760423216-0-0-88058a5379c184fd442f89b9ae49b529&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-99bff64e3cc2b1384ef224d8fa8046fd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423223&auth_key=1760423223-0-0-21de08bf88a69f2b942b65fdc2af9b41&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Reason-Over-Non-Text-Modalities-in-a-Training-Free-Manner-A-Case-Study-with-In-Context-Representation-Learning"><a href="#Can-LLMs-Reason-Over-Non-Text-Modalities-in-a-Training-Free-Manner-A-Case-Study-with-In-Context-Representation-Learning" class="headerlink" title="Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A   Case Study with In-Context Representation Learning"></a>Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A   Case Study with In-Context Representation Learning</h2><p><strong>Authors:Tianle Zhang, Wanlong Fang, Jonathan Woo, Paridhi Latawa, Deepak A. Subramanian, Alvin Chan</strong></p>
<p>The remarkable performance of Large Language Models (LLMs) can be enhanced with test-time computation, which relies on external tools and even other deep learning models. However, existing approaches for integrating non-text modality representations into LLMs typically require additional costly supervised training, restricting on-the-fly adaptation to new domains and modalities. In this work, we explore the feasibility of integrating representations from non-text foundational models (FMs) into text-based LLMs in a training-free manner. We propose In-Context Representation Learning (ICRL) as a proof-of-concept to allow LLMs to adaptively utilize non-text modality representations with few-shot learning. Unlike traditional in-context learning, which incorporates text-label pairs, ICRL replaces text inputs with FM representations, enabling the LLM to perform multi-modal inference without fine-tuning. We evaluate ICRL on a suite of tasks in the molecular domain, investigating three core research questions: (i) how to map FM representations into LLMs in a training-free manner, (ii) what factors influence ICRL performance, and (iii) what mechanisms underlie the effectiveness of ICRL. To the best of our knowledge, ICRL is the first training-free framework for integrating non-text modality representations into text-based LLMs, presenting a promising direction for adaptable, multi-modal generalization. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºè‰²æ€§èƒ½å¯ä»¥é€šè¿‡æµ‹è¯•æ—¶çš„è®¡ç®—å¢å¼ºï¼Œè¿™ä¾èµ–äºå¤–éƒ¨å·¥å…·å’Œå…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ç„¶è€Œï¼Œå°†éæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºé›†æˆåˆ°LLMä¸­çš„ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„æ˜‚è´µçš„æœ‰ç›‘ç£è®­ç»ƒï¼Œè¿™é™åˆ¶äº†åœ¨æ–°é¢†åŸŸå’Œæ¨¡æ€çš„å³æ—¶é€‚åº”ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä»¥æ— è®­ç»ƒæ–¹å¼å°†éæ–‡æœ¬åŸºç¡€æ¨¡å‹ï¼ˆFMï¼‰çš„è¡¨ç¤ºé›†æˆåˆ°åŸºäºæ–‡æœ¬çš„LLMä¸­çš„å¯è¡Œæ€§ã€‚æˆ‘ä»¬æå‡ºåŸºäºä¸Šä¸‹æ–‡è¡¨ç¤ºå­¦ä¹ ï¼ˆICRLï¼‰çš„æ¦‚å¿µéªŒè¯ï¼Œä»¥å…è®¸LLMä»¥å°‘é‡æ ·æœ¬å­¦ä¹ æ–¹å¼è‡ªé€‚åº”åœ°åˆ©ç”¨éæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºã€‚ä¸ä¼ ç»Ÿçš„ä¸Šä¸‹æ–‡å­¦ä¹ ä¸åŒï¼Œåè€…ç»“åˆäº†æ–‡æœ¬æ ‡ç­¾å¯¹ï¼ŒICRLç”¨FMè¡¨ç¤ºæ›¿æ¢æ–‡æœ¬è¾“å…¥ï¼Œä½¿LLMèƒ½å¤Ÿåœ¨æ— éœ€å¾®è°ƒçš„æƒ…å†µä¸‹æ‰§è¡Œå¤šæ¨¡æ€æ¨æ–­ã€‚æˆ‘ä»¬åœ¨åˆ†å­é¢†åŸŸçš„å¤šä¸ªä»»åŠ¡ä¸Šè¯„ä¼°ICRLï¼Œå¹¶æ¢ç©¶ä¸‰ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šï¼ˆiï¼‰å¦‚ä½•åœ¨æ— è®­ç»ƒæƒ…å†µä¸‹å°†FMè¡¨ç¤ºæ˜ å°„åˆ°LLMä¸­ï¼Œï¼ˆiiï¼‰å“ªäº›å› ç´ å½±å“ICRLçš„æ€§èƒ½ï¼Œä»¥åŠï¼ˆiiiï¼‰ICRLæœ‰æ•ˆæ€§çš„åŸºç¡€æœºåˆ¶æ˜¯ä»€ä¹ˆã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒICRLæ˜¯ç¬¬ä¸€ä¸ªç”¨äºå°†éæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºé›†æˆåˆ°åŸºäºæ–‡æœ¬çš„LLMä¸­çš„æ— è®­ç»ƒæ¡†æ¶ï¼Œä¸ºå¯é€‚åº”çš„å¤šæ¨¡æ€æ³›åŒ–æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17552v1">PDF</a> NIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æ¢è®¨äº†åœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå°†éæ–‡æœ¬åŸºç¡€æ¨¡å‹çš„è¡¨ç¤ºé›†æˆåˆ°æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¯è¡Œæ€§ã€‚æå‡ºäº†ä¸€ç§åä¸ºä¸Šä¸‹æ–‡è¡¨ç¤ºå­¦ä¹ ï¼ˆICRLï¼‰çš„æ¦‚å¿µï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°åˆ©ç”¨éæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºè¿›è¡Œå°‘æ ·æœ¬å­¦ä¹ ï¼Œå¹¶å®ç°äº†å¤šæ¨¡æ€æ¨ç†ã€‚è¯„ä»·ICRLåœ¨åˆ†å­é¢†åŸŸçš„ä¸€ç³»åˆ—ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå¹¶æ¢è®¨äº†å¦‚ä½•å°†FMè¡¨ç¤ºæ˜ å°„åˆ°LLMsä¸­ã€å½±å“ICRLæ€§èƒ½çš„å› ç´ ä»¥åŠICRLæœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚æ­¤ä¸ºé¦–ä¸ªæ— éœ€è®­ç»ƒå³å¯æ•´åˆéæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºçš„æ¡†æ¶ï¼Œä¸ºå¯é€‚åº”çš„å¤šæ¨¡æ€æ¨å¹¿æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å¯é€šè¿‡æµ‹è¯•æ—¶çš„è®¡ç®—å¢å¼ºï¼Œä¾èµ–å¤–éƒ¨å·¥å…·å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>ç°æœ‰é›†æˆéæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºåˆ°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•éœ€è¦é¢å¤–çš„ç›‘ç£è®­ç»ƒï¼Œé™åˆ¶äº†åœ¨æ–°é¢†åŸŸå’Œæ¨¡æ€çš„å³æ—¶é€‚åº”ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºä¸Šä¸‹æ–‡è¡¨ç¤ºå­¦ä¹ ï¼ˆICRLï¼‰çš„æ¦‚å¿µï¼Œå…è®¸å¤§å‹è¯­è¨€æ¨¡å‹ä»¥è®­ç»ƒæ— å…³çš„æ–¹å¼åˆ©ç”¨éæ–‡æœ¬åŸºç¡€æ¨¡å‹çš„è¡¨ç¤ºã€‚</li>
<li>ICRLå®ç°äº†å°‘æ ·æœ¬å­¦ä¹ ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œå¤šæ¨¡æ€æ¨ç†ï¼Œæ— éœ€å¾®è°ƒã€‚</li>
<li>ICRLåœ¨åˆ†å­é¢†åŸŸçš„ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶è§£å†³äº†å¦‚ä½•å°†FMè¡¨ç¤ºæ˜ å°„åˆ°LLMsã€å½±å“ICRLæ€§èƒ½çš„å› ç´ ä»¥åŠICRLæœºåˆ¶çš„æœ‰æ•ˆæ€§ç­‰æ ¸å¿ƒé—®é¢˜ã€‚</li>
<li>ICRLæ˜¯é¦–ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿæ•´åˆéæ–‡æœ¬æ¨¡æ€è¡¨ç¤ºåˆ°æ–‡æœ¬ä¸ºåŸºç¡€çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17552">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-963f87cc36a933634ab3a3d542582512~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423231&auth_key=1760423231-0-0-ce5f1f781c3e238f363c8df909890d89&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ce257a3fd61cc171a22bcecfffe0d56b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423238&auth_key=1760423238-0-0-f503378d4490e93c9988da39f2244003&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aa58c7c2f0300eaf576916ea7921c629~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423245&auth_key=1760423245-0-0-4575e52d76447eafbfe49d799e7328ee&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Correlation-or-Causation-Analyzing-the-Causal-Structures-of-LLM-and-LRM-Reasoning-Process"><a href="#Correlation-or-Causation-Analyzing-the-Causal-Structures-of-LLM-and-LRM-Reasoning-Process" class="headerlink" title="Correlation or Causation: Analyzing the Causal Structures of LLM and LRM   Reasoning Process"></a>Correlation or Causation: Analyzing the Causal Structures of LLM and LRM   Reasoning Process</h2><p><strong>Authors:Zhizhang FU, Guangsheng Bao, Hongbo Zhang, Chenkai Hu, Yue Zhang</strong></p>
<p>LLMs suffer from critical reasoning issues such as unfaithfulness, bias, and inconsistency, since they lack robust causal underpinnings and may rely on superficial correlations rather than genuine understanding. Successive LRMs have emerged as a promising alternative, leveraging advanced training techniques such as reinforcement learning (RL) and distillation to improve task accuracy. However, the impact of these training methods on causality remains largely unexplored. In this study, we conduct a systematic causal analysis on LLMs and LRMs, examining structural causal models (SCMs) of four key variables: problem instruction (Z), thinking process (T), reasoning steps (X), and answer (Y). Our findings reveal that RLVR-trained LRMs exhibit enhanced causal reasoning capabilities, aligning more closely with ideal causal structures, while LLMs and distilled LRMs fail to address causality-related deficiencies. Our further investigation indicates that RLVR reduces spurious correlations and strengthens genuine causal patterns, thereby mitigating unfaithfulness and bias. In addition, our inspection on the dynamics of the RLVR training process observes a high correlation between reduced spurious features and improved causal structures, where the causal relationships consistently improve in the training process. This study contributes to the understanding of causality in reasoning models, highlights the critical role of RLVR in enhancing causal reasoning, and provides insights for designing future AI systems with stronger causal foundations. We release our code and data at <a target="_blank" rel="noopener" href="https://github.com/Harryking1999/CoT_Causal_Analysis">https://github.com/Harryking1999/CoT_Causal_Analysis</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å­˜åœ¨æ¨ç†é—®é¢˜ï¼Œå¦‚ç¼ºä¹å¿ å®æ€§ã€åè§å’Œä¸ä¸€è‡´æ€§ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹åšå®çš„å› æœåŸºç¡€ï¼Œå¯èƒ½ä¾èµ–äºè¡¨é¢å…³è”è€ŒéçœŸæ­£çš„ç†è§£ã€‚éšç€è¿ç»­å­¦ä¹ æ¨¡å‹ï¼ˆLRMsï¼‰çš„å‡ºç°ï¼Œä½œä¸ºä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå®ƒåˆ©ç”¨å…ˆè¿›çš„è®­ç»ƒæŠ€æœ¯ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œè’¸é¦æŠ€æœ¯æ¥æé«˜ä»»åŠ¡å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œè¿™äº›è®­ç»ƒæ–¹æ³•å¯¹å› æœå…³ç³»çš„å½±å“åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå°šæœªè¢«æ¢ç´¢ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹LLMså’ŒLRMsè¿›è¡Œäº†ç³»ç»Ÿçš„å› æœåˆ†æï¼Œç ”ç©¶äº†å››ä¸ªå…³é”®å˜é‡çš„ç»“æ„å› æœæ¨¡å‹ï¼ˆSCMsï¼‰ï¼šé—®é¢˜æŒ‡ä»¤ï¼ˆZï¼‰ã€æ€è€ƒè¿‡ç¨‹ï¼ˆTï¼‰ã€æ¨ç†æ­¥éª¤ï¼ˆXï¼‰å’Œç­”æ¡ˆï¼ˆYï¼‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œç»è¿‡RLVRè®­ç»ƒçš„LRMsè¡¨ç°å‡ºå¢å¼ºçš„å› æœæ¨ç†èƒ½åŠ›ï¼Œæ›´æ¥è¿‘äºç†æƒ³çš„å› æœç»“æ„ï¼Œè€ŒLLMså’Œè’¸é¦LRMsæ— æ³•è§£å†³ä¸å› æœå…³ç³»ç›¸å…³çš„ç¼ºé™·ã€‚æˆ‘ä»¬çš„è¿›ä¸€æ­¥è°ƒæŸ¥è¡¨æ˜ï¼ŒRLVRå‡å°‘äº†è™šå‡å…³è”ï¼ŒåŠ å¼ºäº†çœŸæ­£çš„å› æœæ¨¡å¼ï¼Œä»è€Œå‡è½»äº†ç¼ºä¹å¿ å®æ€§å’Œåè§çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹RLVRè®­ç»ƒè¿‡ç¨‹çš„åŠ¨æ€æ€§çš„æ£€æŸ¥å‘ç°ï¼Œå‡å°‘çš„è™šå‡ç‰¹å¾ä¸æ”¹å–„çš„å› æœç»“æ„ä¹‹é—´å­˜åœ¨é«˜åº¦ç›¸å…³æ€§ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å› æœå…³ç³»æŒç»­å¾—åˆ°æ”¹å–„ã€‚æœ¬ç ”ç©¶æœ‰åŠ©äºç†è§£æ¨ç†æ¨¡å‹ä¸­çš„å› æœå…³ç³»ï¼Œçªå‡ºäº†RLVRåœ¨å¢å¼ºå› æœæ¨ç†ä¸­çš„å…³é”®ä½œç”¨ï¼Œä¸ºè®¾è®¡å…·æœ‰æ›´å¼ºå› æœåŸºç¡€çš„æœªæ¥äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†è§è§£ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/Harryking1999/CoT_Causal_Analysis">https://github.com/Harryking1999/CoT_Causal_Analysis</a>ä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17380v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å­˜åœ¨æ¨ç†ç¼ºé™·ï¼Œå¦‚ç¼ºä¹çœŸå®ç†è§£è€Œå¯¼è‡´çš„å¤±å®ã€åè§å’Œä¸ä¸€è‡´æ€§ã€‚æ–°å‹çš„è¿ç»­å­¦ä¹ æ¨¡å‹ï¼ˆLRMsï¼‰é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œè’¸é¦ç­‰é«˜çº§è®­ç»ƒæŠ€æœ¯æ¥æå‡ä»»åŠ¡å‡†ç¡®æ€§ï¼Œä½†å¯¹å› æœæ€§çš„å½±å“å°šæœªå……åˆ†ç ”ç©¶ã€‚æœ¬ç ”ç©¶é€šè¿‡ç»“æ„å› æœæ¨¡å‹ï¼ˆSCMï¼‰å¯¹LLMså’ŒLRMsè¿›è¡Œå› æœåˆ†æï¼Œå‘ç°RLVRè®­ç»ƒçš„LRMå±•ç°å‡ºæ›´å‡ºè‰²çš„å› æœæ¨ç†èƒ½åŠ›ï¼Œæ›´æ¥è¿‘ç†æƒ³å› æœç»“æ„ã€‚RLVRèƒ½å‡å°‘è™šå‡å…³è”å¹¶å¼ºåŒ–çœŸå®å› æœæ¨¡å¼ï¼Œæ”¹å–„ä¸å¿ å®å’Œåè§é—®é¢˜ã€‚ç ”ç©¶è¿˜è§‚å¯Ÿåˆ°RLVRè®­ç»ƒè¿‡ç¨‹ä¸­å‡å°‘è™šå‡ç‰¹å¾ä¸æ”¹å–„å› æœå…³ç³»çš„å¼ºç›¸å…³æ€§ã€‚æœ¬ç ”ç©¶æœ‰åŠ©äºç†è§£æ¨¡å‹ä¸­çš„å› æœå…³ç³»ï¼Œå¼ºè°ƒäº†RLVRåœ¨å¼ºåŒ–å› æœæ¨ç†ä¸­çš„è§’è‰²ï¼Œä¸ºè®¾è®¡æœªæ¥å…·æœ‰æ›´å¼ºå› æœåŸºç¡€çš„AIç³»ç»Ÿæä¾›äº†å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å­˜åœ¨æ¨ç†é—®é¢˜ï¼Œå¦‚å¤±å®ã€åè§å’Œä¸ä¸€è‡´æ€§ã€‚</li>
<li>æ–°å‹çš„è¿ç»­å­¦ä¹ æ¨¡å‹ï¼ˆLRMsï¼‰é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç­‰è®­ç»ƒæŠ€æœ¯æ”¹å–„ä»»åŠ¡å‡†ç¡®æ€§ã€‚</li>
<li>ç»“æ„å› æœæ¨¡å‹ï¼ˆSCMï¼‰ç”¨äºåˆ†æLLMså’ŒLRMsçš„å› æœå…³ç³»ã€‚</li>
<li>RLVRè®­ç»ƒçš„LRMå±•ç°å‡ºæ›´å¼ºçš„å› æœæ¨ç†èƒ½åŠ›ï¼Œæ›´æ¥è¿‘ç†æƒ³å› æœç»“æ„ã€‚</li>
<li>RLVRèƒ½æœ‰æ•ˆå‡å°‘è™šå‡å…³è”ï¼Œå¼ºåŒ–çœŸå®å› æœå…³ç³»ã€‚</li>
<li>RLVRè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå‡å°‘è™šå‡ç‰¹å¾ä¸æ”¹å–„å› æœå…³ç³»ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17380">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f64c55a0de4235bc04d7e90c4a78e43e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423253&auth_key=1760423253-0-0-d8b91ecf90a75801b1450249ddc7bfd8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f45cad47f384ec9cb1687225193168a2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423260&auth_key=1760423260-0-0-475067618f932997861c1e5476319a84&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1dfa21d3d0b4354b8244258fa2c230cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423267&auth_key=1760423267-0-0-900fec58d8ce9ed84e9c9e9eda7a041c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-faec3616501dc6b9912c362213817b5b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423274&auth_key=1760423274-0-0-7def822ad5f855809a60a7b67410ce1d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Medical-AI-Consensus-A-Multi-Agent-Framework-for-Radiology-Report-Generation-and-Evaluation"><a href="#Medical-AI-Consensus-A-Multi-Agent-Framework-for-Radiology-Report-Generation-and-Evaluation" class="headerlink" title="Medical AI Consensus: A Multi-Agent Framework for Radiology Report   Generation and Evaluation"></a>Medical AI Consensus: A Multi-Agent Framework for Radiology Report   Generation and Evaluation</h2><p><strong>Authors:Ahmed T. Elboardy, Ghada Khoriba, Essam A. Rashed</strong></p>
<p>Automating radiology report generation poses a dual challenge: building clinically reliable systems and designing rigorous evaluation protocols. We introduce a multi-agent reinforcement learning framework that serves as both a benchmark and evaluation environment for multimodal clinical reasoning in the radiology ecosystem. The proposed framework integrates large language models (LLMs) and large vision models (LVMs) within a modular architecture composed of ten specialized agents responsible for image analysis, feature extraction, report generation, review, and evaluation. This design enables fine-grained assessment at both the agent level (e.g., detection and segmentation accuracy) and the consensus level (e.g., report quality and clinical relevance). We demonstrate an implementation using chatGPT-4o on public radiology datasets, where LLMs act as evaluators alongside medical radiologist feedback. By aligning evaluation protocols with the LLM development lifecycle, including pretraining, finetuning, alignment, and deployment, the proposed benchmark establishes a path toward trustworthy deviance-based radiology report generation. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šé¢ä¸´åŒé‡æŒ‘æˆ˜ï¼šå»ºç«‹ä¸´åºŠå¯é çš„ç³»ç»Ÿå’Œè®¾è®¡ä¸¥æ ¼çš„è¯„ä»·åè®®ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¢å¯ä½œä¸ºæ”¾å°„ç”Ÿæ€ç³»ç»Ÿä¸­å¤šæ¨¡å¼ä¸´åºŠæ¨ç†çš„åŸºå‡†æµ‹è¯•ç¯å¢ƒï¼Œåˆå¯ä½œä¸ºè¯„ä»·ç¯å¢ƒã€‚æ‰€æå‡ºçš„æ¡†æ¶åœ¨æ¨¡å—åŒ–æ¶æ„ä¸­é›†æˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMï¼‰ï¼Œç”±åä¸ªä¸“é—¨è´Ÿè´£å›¾åƒåˆ†æã€ç‰¹å¾æå–ã€æŠ¥å‘Šç”Ÿæˆã€å®¡æŸ¥å’Œè¯„ä¼°çš„æ™ºèƒ½ä½“ç»„æˆã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿåœ¨æ™ºèƒ½ä½“çº§åˆ«ï¼ˆä¾‹å¦‚ï¼Œæ£€æµ‹å’Œåˆ†å‰²ç²¾åº¦ï¼‰å’Œå…±è¯†çº§åˆ«ï¼ˆä¾‹å¦‚ï¼ŒæŠ¥å‘Šè´¨é‡å’Œä¸´åºŠç›¸å…³æ€§ï¼‰è¿›è¡Œç²¾ç»†è¯„ä¼°ã€‚æˆ‘ä»¬åœ¨å…¬å…±æ”¾å°„å­¦æ•°æ®é›†ä¸Šå±•ç¤ºäº†ä½¿ç”¨ChatGPT-4oçš„å®ç°ï¼Œå…¶ä¸­LLMä½œä¸ºè¯„ä¼°è€…ï¼Œä¸åŒ»å­¦æ”¾å°„ç§‘åŒ»å¸ˆçš„åé¦ˆç›¸ç»“åˆã€‚é€šè¿‡ä½¿è¯„ä»·åè®®ä¸LLMå¼€å‘å‘¨æœŸï¼ˆåŒ…æ‹¬é¢„è®­ç»ƒã€å¾®è°ƒã€å¯¹é½å’Œéƒ¨ç½²ï¼‰ä¿æŒä¸€è‡´ï¼Œæ‰€æå‡ºçš„åŸºå‡†æµ‹è¯•ä¸ºåŸºäºå¯ä¿¡åå·®çš„æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆå»ºç«‹äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17353v1">PDF</a> NeurIPS2025 Workshop: Evaluating the Evolving LLM Lifecycle:   Benchmarks, Emergent Abilities, and Scaling</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè§£å†³æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä¸­çš„åŒé‡æŒ‘æˆ˜ï¼šæ„å»ºå¯é çš„åŒ»å­¦ç³»ç»Ÿå¹¶è®¾è®¡ä¸¥æ ¼çš„è¯„ä»·åè®®ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤§å‹è§†è§‰æ¨¡å‹ï¼Œåœ¨æ¨¡å—åŒ–æ¶æ„ä¸­å®ç°åç§ä¸“ä¸šæ™ºèƒ½ä½“çš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†æã€ç‰¹å¾æå–ã€æŠ¥å‘Šç”Ÿæˆã€å®¡æŸ¥å’Œè¯„ä¼°ç­‰ã€‚è¯¥è®¾è®¡å¯å®ç°åœ¨æ™ºèƒ½ä½“å±‚é¢å’Œå…±è¯†å±‚é¢çš„ç²¾ç»†è¯„ä¼°ï¼Œé€šè¿‡ä¸åŒ»å­¦æ”¾å°„ç§‘åŒ»å¸ˆåé¦ˆç›¸ç»“åˆï¼Œä½¿ç”¨ChatGPT-4oåœ¨å…¬å…±æ”¾å°„å­¦æ•°æ®é›†ä¸Šè¿›è¡Œäº†æ¼”ç¤ºå®ç°ã€‚é€šè¿‡ä½¿è¯„ä»·åè®®ä¸å¤§å‹è¯­è¨€æ¨¡å‹å¼€å‘å‘¨æœŸä¿æŒä¸€è‡´ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€å¾®è°ƒã€å¯¹é½å’Œéƒ¨ç½²ç­‰é˜¶æ®µï¼Œæ‰€æå‡ºçš„åŸºå‡†æµ‹è¯•æœ‰åŠ©äºå»ºç«‹å¯é çš„åŸºäºåå·®çš„æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆè·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä¸­çš„ä¸´åºŠå¯é æ€§å’Œä¸¥æ ¼è¯„ä»·åè®®çš„é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤§å‹è§†è§‰æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨æ¨¡å—åŒ–æ¶æ„å®ç°å¤šç§åŠŸèƒ½ã€‚</li>
<li>æ¡†æ¶ä¸­åŒ…å«åç§ä¸“ä¸šæ™ºèƒ½ä½“ï¼Œè´Ÿè´£å›¾åƒåˆ†æã€ç‰¹å¾æå–ã€æŠ¥å‘Šç”Ÿæˆã€å®¡æŸ¥å’Œè¯„ä¼°ç­‰ä»»åŠ¡ã€‚</li>
<li>å®ç°äº†æ™ºèƒ½ä½“å±‚é¢å’Œå…±è¯†å±‚é¢çš„ç²¾ç»†è¯„ä¼°ï¼Œä¸ºè¯„ä»·æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„è´¨é‡æä¾›äº†å…¨é¢çš„è§†è§’ã€‚</li>
<li>ä½¿ç”¨ChatGPT-4oåœ¨å…¬å…±æ”¾å°„å­¦æ•°æ®é›†ä¸Šè¿›è¡Œäº†æ¼”ç¤ºå®ç°ï¼Œç»“åˆäº†åŒ»å­¦æ”¾å°„ç§‘åŒ»å¸ˆçš„åé¦ˆã€‚</li>
<li>æ‰€æå‡ºçš„è¯„ä»·åè®®ä¸å¤§å‹è¯­è¨€æ¨¡å‹å¼€å‘å‘¨æœŸä¿æŒä¸€è‡´ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€å¾®è°ƒã€å¯¹é½å’Œéƒ¨ç½²ç­‰é˜¶æ®µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17353">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8df5270eea60fef21e1d1d3fc5f84823~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423281&auth_key=1760423281-0-0-f135ae0c31234e5304b5863dfaf77ebe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fba97895f7e3ad709b29c8d3425653cb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423289&auth_key=1760423289-0-0-fbde477797fd406f5776e5f18adfafb8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c3923e37fdcd69307d24cb91cbd0e79b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423296&auth_key=1760423296-0-0-1ea11e57a817f759b40c8a350754fb86&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LLaVul-A-Multimodal-LLM-for-Interpretable-Vulnerability-Reasoning-about-Source-Code"><a href="#LLaVul-A-Multimodal-LLM-for-Interpretable-Vulnerability-Reasoning-about-Source-Code" class="headerlink" title="LLaVul: A Multimodal LLM for Interpretable Vulnerability Reasoning about   Source Code"></a>LLaVul: A Multimodal LLM for Interpretable Vulnerability Reasoning about   Source Code</h2><p><strong>Authors:Ala Jararweh, Michael Adams, Avinash Sahu, Abdullah Mueen, Afsah Anwar</strong></p>
<p>Increasing complexity in software systems places a growing demand on reasoning tools that unlock vulnerabilities manifest in source code. Many current approaches focus on vulnerability analysis as a classifying task, oversimplifying the nuanced and context-dependent real-world scenarios. Even though current code large language models (LLMs) excel in code understanding, they often pay little attention to security-specific reasoning. We propose LLaVul, a multimodal LLM tailored to provide fine-grained reasoning about code through question-answering (QA). Our model is trained to integrate paired code and natural queries into a unified space, enhancing reasoning and context-dependent insights about code vulnerability. To evaluate our model performance, we construct a curated dataset of real-world vulnerabilities paired with security-focused questions and answers. Our model outperforms state-of-the-art general-purpose and code LLMs in the QA and detection tasks. We further explain decision-making by conducting qualitative analysis to highlight capabilities and limitations. By integrating code and QA, LLaVul enables more interpretable and security-focused code understanding. </p>
<blockquote>
<p>éšç€è½¯ä»¶ç³»ç»Ÿå¤æ‚æ€§çš„å¢åŠ ï¼Œå¯¹è§£é”æºä»£ç ä¸­è¡¨ç°å‡ºçš„æ¼æ´çš„æ¨ç†å·¥å…·çš„éœ€æ±‚ä¹Ÿåœ¨å¢é•¿ã€‚è®¸å¤šå½“å‰çš„æ–¹æ³•å°†æ¼æ´åˆ†æä½œä¸ºåˆ†ç±»ä»»åŠ¡ï¼Œè¿™ç®€åŒ–äº†ç°å®ä¸–ç•Œä¸­å¾®å¦™ä¸”ä¾èµ–äºä¸Šä¸‹æ–‡æƒ…å¢ƒçš„æƒ…å¢ƒã€‚å°½ç®¡å½“å‰çš„å¤§å‹ä»£ç è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£ä»£ç æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½è§†äº†ç‰¹å®šäºå®‰å…¨æ€§çš„æ¨ç†ã€‚æˆ‘ä»¬æå‡ºäº†LLaVulï¼Œè¿™æ˜¯ä¸€ä¸ªé‡èº«å®šåˆ¶çš„å¤šæ¨¡å¼LLMï¼Œé€šè¿‡é—®ç­”ï¼ˆQAï¼‰æä¾›å¯¹ä»£ç çš„ç²¾ç»†æ¨ç†ã€‚æˆ‘ä»¬çš„æ¨¡å‹ç»è¿‡è®­ç»ƒï¼Œèƒ½å¤Ÿå°†é…å¯¹ä»£ç å’Œè‡ªç„¶æŸ¥è¯¢æ•´åˆåˆ°ç»Ÿä¸€çš„ç©ºé—´ä¸­ï¼Œå¢å¼ºå¯¹ä»£ç æ¼æ´çš„æ¨ç†å’Œä¸Šä¸‹æ–‡ä¾èµ–çš„æ´å¯ŸåŠ›ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹æ€§èƒ½ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«ä»¥å®‰å…¨ä¸ºä¸­å¿ƒçš„é—®é¢˜å’Œç­”æ¡ˆçš„ç°å®ä¸–ç•Œæ¼æ´æ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨é—®ç­”å’Œæ£€æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºæœ€æ–°çš„é€šç”¨å’Œä»£ç LLMã€‚æˆ‘ä»¬è¿˜é€šè¿‡è¿›è¡Œå®šæ€§åˆ†ææ¥è§£é‡Šå†³ç­–åˆ¶å®šï¼Œä»¥çªå‡ºæˆ‘ä»¬çš„èƒ½åŠ›å’Œå±€é™æ€§ã€‚é€šè¿‡æ•´åˆä»£ç å’Œé—®ç­”ï¼ŒLLaVulå®ç°äº†æ›´å¯è§£é‡Šæ€§å’Œä»¥å®‰å…¨ä¸ºä¸­å¿ƒçš„ä»£ç ç†è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17337v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è½¯ä»¶ç³»ç»Ÿçš„å¤æ‚æ€§å¢é•¿å¯¹ç†è§£æºä»£ç ä¸­çš„æ¼æ´æ‰€éœ€çš„æ¨ç†å·¥å…·çš„éœ€æ±‚ã€‚é’ˆå¯¹å½“å‰å°†æ¼æ´åˆ†æä½œä¸ºåˆ†ç±»ä»»åŠ¡çš„ç®€åŒ–å¤„ç†ï¼Œä»¥åŠå¤§å‹ä»£ç è¯­è¨€æ¨¡å‹å¯¹å®‰å…¨ç‰¹å®šæ¨ç†çš„å¿½è§†ï¼Œæå‡ºäº†LLaVulæ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡é—®ç­”æ–¹å¼æä¾›ç²¾ç»†ç²’åº¦çš„ä»£ç æ¨ç†ï¼Œé€šè¿‡æ•´åˆä»£ç å’Œè‡ªç„¶æŸ¥è¯¢è¿›è¡Œç»Ÿä¸€ç©ºé—´å¤„ç†ï¼Œå¢å¼ºäº†å…³äºä»£ç æ¼æ´çš„ä¸Šä¸‹æ–‡ç›¸å…³è§è§£ã€‚é€šè¿‡æ„å»ºç°å®æ¼æ´ä¸å®‰å…¨ç›¸å…³é—®é¢˜ç­”æ¡ˆçš„æ•°æ®é›†è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ŒLLaVulåœ¨é—®ç­”å’Œæ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè¶…è¶Šç°æœ‰é€šç”¨å’Œä»£ç å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¼˜åŠ¿ã€‚é€šè¿‡å®šæ€§åˆ†æè§£é‡Šå†³ç­–åˆ¶å®šï¼Œå±•ç¤ºäº†LLaVulçš„èƒ½åŠ›ä¸å±€é™æ€§ã€‚æ•´åˆä»£ç å’Œé—®ç­”åŠŸèƒ½ä½¿LLaVulåœ¨ä»£ç ç†è§£æ–¹é¢æ›´å…·å¯è§£é‡Šæ€§å’Œå®‰å…¨æ€§å…³æ³¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è½¯ä»¶ç³»ç»Ÿçš„å¤æ‚æ€§è¦æ±‚å¯¹æºä»£ç ä¸­çš„æ¼æ´è¿›è¡Œæ›´æ·±å…¥çš„ç†è§£å’Œåˆ†æã€‚</li>
<li>å½“å‰çš„æ–¹æ³•å¤§å¤šå°†æ¼æ´åˆ†æè§†ä¸ºåˆ†ç±»ä»»åŠ¡ï¼Œæœªèƒ½å……åˆ†è€ƒè™‘å®é™…åœºæ™¯ä¸­çš„ç»†å¾®å·®åˆ«å’Œä¸Šä¸‹æ–‡ä¾èµ–ã€‚</li>
<li>å¤§å‹ä»£ç è¯­è¨€æ¨¡å‹è™½ç„¶èƒ½å¾ˆå¥½åœ°ç†è§£ä»£ç ï¼Œä½†åœ¨å®‰å…¨ç‰¹å®šçš„æ¨ç†æ–¹é¢å…³æ³¨ä¸è¶³ã€‚</li>
<li>LLaVulæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡é—®ç­”æ–¹å¼æä¾›ç²¾ç»†ç²’åº¦çš„ä»£ç æ¨ç†ã€‚</li>
<li>LLaVulæ¨¡å‹èƒ½å¤Ÿæ•´åˆä»£ç å’Œè‡ªç„¶æŸ¥è¯¢ï¼Œåœ¨ç»Ÿä¸€ç©ºé—´å†…å¢å¼ºå¯¹ä»£ç æ¼æ´çš„æ¨ç†å’Œä¸Šä¸‹æ–‡ç†è§£ã€‚</li>
<li>LLaVulåœ¨æ„å»ºçš„ç°å®æ¼æ´æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„é€šç”¨å’Œä»£ç å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17337">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-279f5e6bbb859b22afd42a0d63bda600~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423303&auth_key=1760423303-0-0-e9a748ab9e842dba68536162c99fbadb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-57cde6da0e135cf7a893d9370e7d8d28~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423311&auth_key=1760423311-0-0-c45579bfdd6d6917ec5d030a7bf6d392&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-99cd738cd855bde4963f815484dcd444~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423317&auth_key=1760423317-0-0-e2d36fdc4498a180ae6ace5c6c85ec00&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e9bf99a9b4bb01e1190dacc6bac2289e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423324&auth_key=1760423324-0-0-34ceb9a012685b9c03f0ae234e2ede9d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-002931b97fa4a6903da2e6db4e09da62~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423331&auth_key=1760423331-0-0-3ee7f7644c07f9f1aa45c2dd1eafb1e4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3e945c6cc08f6e5634c81c65361e3e7a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423338&auth_key=1760423338-0-0-a01220548860aaefdd66778814023e4e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4fb399a6aba2bf1b8cf9ca69686181da~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423345&auth_key=1760423345-0-0-e28cd7cdf497d74c5bebc701dda69ee0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Mano-Report"><a href="#Mano-Report" class="headerlink" title="Mano Report"></a>Mano Report</h2><p><strong>Authors:Tianyu Fu, Anyang Su, Chenxu Zhao, Hanning Wang, Minghui Wu, Zhe Yu, Fei Hu, Mingjia Shi, Wei Dong, Jiayao Wang, Yuyang Chen, Ruiyang Yu, Siran Peng, Menglin Li, Nan Huang, Haitian Wei, Jiawei Yu, Yi Xin, Xilin Zhao, Kai Gu, Ping Jiang, Sifan Zhou, Shuo Wang</strong></p>
<p>Graphical user interfaces (GUIs) are the primary medium for human-computer interaction, yet automating GUI interactions remains challenging due to the complexity of visual elements, dynamic environments, and the need for multi-step reasoning. Existing methods based on vision-language models (VLMs) often suffer from limited resolution, domain mismatch, and insufficient sequential decisionmaking capability. To address these issues, we propose Mano, a robust GUI agent built upon a multi-modal foundation model pre-trained on extensive web and computer system data. Our approach integrates a novel simulated environment for high-fidelity data generation, a three-stage training pipeline (supervised fine-tuning, offline reinforcement learning, and online reinforcement learning), and a verification module for error recovery. Mano demonstrates state-of-the-art performance on multiple GUI benchmarks, including Mind2Web and OSWorld, achieving significant improvements in success rate and operational accuracy. Our work provides new insights into the effective integration of reinforcement learning with VLMs for practical GUI agent deployment, highlighting the importance of domain-specific data, iterative training, and holistic reward design. </p>
<blockquote>
<p>å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æ˜¯äººæœºäº¤äº’çš„ä¸»è¦åª’ä»‹ã€‚ç„¶è€Œï¼Œç”±äºè§†è§‰å…ƒç´ çš„å¤æ‚æ€§ã€åŠ¨æ€ç¯å¢ƒä»¥åŠå¤šæ­¥éª¤æ¨ç†çš„éœ€æ±‚ï¼Œè‡ªåŠ¨åŒ–GUIäº¤äº’ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ç°æœ‰æ–¹æ³•é€šå¸¸å—åˆ°åˆ†è¾¨ç‡æœ‰é™ã€é¢†åŸŸä¸åŒ¹é…å’Œåºåˆ—å†³ç­–èƒ½åŠ›ä¸è¶³çš„å›°æ‰°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºManoçš„ç¨³å¥GUIä»£ç†ï¼Œå®ƒå»ºç«‹åœ¨å¤§é‡ç½‘ç»œå’Œè®¡ç®—æœºç³»ç»Ÿæ•°æ®ä¸Šé¢„è®­ç»ƒçš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ä¹‹ä¸Šã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ç”¨äºé«˜ä¿çœŸæ•°æ®ç”Ÿæˆçš„æ–°å‹æ¨¡æ‹Ÿç¯å¢ƒã€ä¸‰é˜¶æ®µè®­ç»ƒç®¡é“ï¼ˆç›‘ç£å¾®è°ƒã€ç¦»çº¿å¼ºåŒ–å­¦ä¹ å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼‰ä»¥åŠç”¨äºé”™è¯¯æ¢å¤çš„éªŒè¯æ¨¡å—ã€‚Manoåœ¨å¤šä¸ªGUIåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬Mind2Webå’ŒOSWorldï¼Œåœ¨æˆåŠŸç‡å’Œæ“ä½œå‡†ç¡®æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†å°†å¼ºåŒ–å­¦ä¹ ä¸VLMæœ‰æ•ˆç»“åˆä»¥è¿›è¡Œå®é™…GUIä»£ç†éƒ¨ç½²çš„æ–°è§è§£ï¼Œå¼ºè°ƒäº†é¢†åŸŸç‰¹å®šæ•°æ®ã€è¿­ä»£è®­ç»ƒå’Œæ•´ä½“å¥–åŠ±è®¾è®¡çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17336v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>GUIäº¤äº’è‡ªåŠ¨åŒ–æ˜¯è®¡ç®—æœºè§†è§‰å’Œæœºå™¨å­¦ä¹ é¢†åŸŸçš„çƒ­é—¨æŒ‘æˆ˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€æ¬¾åŸºäºå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„GUIäº¤äº’ä»£ç†â€”â€”Manoã€‚Manoé€šè¿‡æ¨¡æ‹Ÿç¯å¢ƒè¿›è¡Œé«˜ä¿çœŸæ•°æ®ç”Ÿæˆï¼Œé‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒã€ç¦»çº¿å¼ºåŒ–å­¦ä¹ å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸‰ä¸ªé˜¶æ®µï¼Œå¹¶é…å¤‡éªŒè¯æ¨¡å—è¿›è¡Œé”™è¯¯æ¢å¤ã€‚åœ¨å¤šä¸ªGUIåŸºå‡†æµ‹è¯•ä¸­ï¼ŒManoè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒæˆåŠŸç‡å’Œæ“ä½œå‡†ç¡®æ€§å‡æ˜¾è‘—æé«˜ã€‚è¯¥ç ”ç©¶ä¸ºå¼ºåŒ–å­¦ä¹ ä¸VLMåœ¨GUIä»£ç†éƒ¨ç½²ä¸­çš„æœ‰æ•ˆç»“åˆæä¾›äº†æ–°çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GUIäº¤äº’è‡ªåŠ¨åŒ–é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚è§†è§‰å…ƒç´ å¤æ‚æ€§ã€åŠ¨æ€ç¯å¢ƒå’Œå¤šæ­¥æ¨ç†éœ€æ±‚ã€‚</li>
<li>ç°æœ‰åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ–¹æ³•å­˜åœ¨åˆ†è¾¨ç‡æœ‰é™ã€é¢†åŸŸä¸åŒ¹é…å’Œå†³ç­–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>Manoæ˜¯ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„GUIä»£ç†ï¼Œé€šè¿‡æ¨¡æ‹Ÿç¯å¢ƒè¿›è¡Œé«˜ä¿çœŸæ•°æ®ç”Ÿæˆã€‚</li>
<li>Manoé‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒã€ç¦»çº¿å¼ºåŒ–å­¦ä¹ å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸‰ä¸ªé˜¶æ®µã€‚</li>
<li>Manoé…å¤‡äº†éªŒè¯æ¨¡å—ä»¥è¿›è¡Œé”™è¯¯æ¢å¤ï¼Œæé«˜äº†äº¤äº’çš„ç¨³å¥æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªGUIåŸºå‡†æµ‹è¯•ä¸­ï¼ŒManoè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒæˆåŠŸç‡å’Œæ“ä½œå‡†ç¡®æ€§æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17336">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-080a37e5c8a17d6f09963989d20e52e8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423352&auth_key=1760423352-0-0-ec1a5ce58f7581c24b71007fd270ad43&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b9b572c18ee5f8ceb4ddc8c1481fd525~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423359&auth_key=1760423359-0-0-073680827a5504b0c323e115d81e835a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-52a2aff8a509ab4efe0ce6987caafbb6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423366&auth_key=1760423366-0-0-1eae20c11ffd0737b650dca2d35a8915&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-810ee99af68d796beb7483f1fdf259a2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423373&auth_key=1760423373-0-0-8571bf18a22b112fbeb52ba6b004022f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CogAtom-From-Cognitive-Atoms-to-Olympiad-level-Mathematical-Reasoning-in-Large-Language-Models"><a href="#CogAtom-From-Cognitive-Atoms-to-Olympiad-level-Mathematical-Reasoning-in-Large-Language-Models" class="headerlink" title="CogAtom: From Cognitive Atoms to Olympiad-level Mathematical Reasoning   in Large Language Models"></a>CogAtom: From Cognitive Atoms to Olympiad-level Mathematical Reasoning   in Large Language Models</h2><p><strong>Authors:Zhuofan Chen, Jiyuan He, Yichi Zhang, Xing Hu, Haoxing Wen, Jun Bai, Wenge Rong</strong></p>
<p>Mathematical reasoning poses significant challenges for Large Language Models (LLMs) due to its demand for multi-step reasoning and abstract conceptual integration. While recent test-time scaling techniques rely heavily on high-quality, challenging problems, the scarcity of Olympiad-level math problems remains a bottleneck. We introduce CogAtom, a novel cognitive atom-based framework for synthesizing mathematically rigorous and cognitively diverse problems. Unlike prior approaches, CogAtom models problem construction as a process of selecting and recombining fundamental reasoning units, cognitive atoms, extracted from human-authored solutions. A diversity-promoting random walk algorithm enables exploration of the cognitive atom space, while a constraint-based recombination mechanism ensures logical soundness and structural validity. The combinatorial nature of the graph structure provides a near-infinite space of reasoning paths, and the walk algorithm systematically explores this space to achieve large-scale synthesis of high-quality problems; meanwhile, by controlling the number of cognitive atoms, we can precisely adjust problem difficulty, ensuring diversity, scalability, and controllability of the generated problems. Experimental results demonstrate that CogAtom outperforms existing methods in accuracy, reasoning depth, and diversity, generating problems that closely match the difficulty of AIME while exceeding it in structural variation. Our work offers a cognitively grounded pathway toward scalable, high-quality math problem generation.Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Icarus-1111/CogAtom">https://github.com/Icarus-1111/CogAtom</a>. </p>
<blockquote>
<p>æ•°å­¦æ¨ç†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå…¶éœ€è¦å¤šæ­¥éª¤æ¨ç†å’ŒæŠ½è±¡æ¦‚å¿µæ•´åˆã€‚å°½ç®¡æœ€è¿‘çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æŠ€æœ¯ä¸¥é‡ä¾èµ–äºé«˜è´¨é‡ã€å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œä½†ç¼ºä¹å¥¥æ—åŒ¹å…‹çº§åˆ«çš„æ•°å­¦é—®é¢˜ä»ç„¶æ˜¯ç“¶é¢ˆã€‚æˆ‘ä»¬å¼•å…¥äº†CogAtomï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè®¤çŸ¥åŸå­çš„æ–°å‹é—®é¢˜åˆæˆæ¡†æ¶ï¼Œç”¨äºåˆæˆæ•°å­¦ä¸¥è°¨ã€è®¤çŸ¥å¤šæ ·çš„é—®é¢˜ã€‚ä¸åŒäºä»¥å¾€çš„æ–¹æ³•ï¼ŒCogAtomå°†é—®é¢˜æ„å»ºè¿‡ç¨‹è§†ä¸ºä¸€ä¸ªé€‰æ‹©å’Œé‡ç»„äººç±»è§£å†³æ–¹æ¡ˆä¸­æå–çš„åŸºæœ¬æ¨ç†å•å…ƒâ€”â€”è®¤çŸ¥åŸå­çš„è¿‡ç¨‹ã€‚ä¸€ç§ä¿ƒè¿›å¤šæ ·æ€§çš„éšæœºæ¸¸èµ°ç®—æ³•èƒ½å¤Ÿæ¢ç´¢è®¤çŸ¥åŸå­ç©ºé—´ï¼Œè€ŒåŸºäºçº¦æŸçš„é‡ç»„æœºåˆ¶åˆ™ç¡®ä¿äº†é€»è¾‘ä¸¥è°¨æ€§å’Œç»“æ„æœ‰æ•ˆæ€§ã€‚å›¾ç»“æ„çš„ç»„åˆæ€§è´¨æä¾›äº†ä¸€ä¸ªè¿‘ä¹æ— é™çš„æ¨ç†è·¯å¾„ç©ºé—´ï¼Œéšæœºæ¸¸èµ°ç®—æ³•ç³»ç»Ÿåœ°æ¢ç´¢è¿™ä¸ªç©ºé—´ï¼Œä»¥å®ç°é«˜è´¨é‡é—®é¢˜çš„å¤§è§„æ¨¡åˆæˆï¼›åŒæ—¶ï¼Œé€šè¿‡æ§åˆ¶è®¤çŸ¥åŸå­çš„æ•°é‡ï¼Œæˆ‘ä»¬å¯ä»¥ç²¾ç¡®åœ°è°ƒæ•´é—®é¢˜çš„éš¾åº¦ï¼Œç¡®ä¿ç”Ÿæˆé—®é¢˜çš„å¤šæ ·æ€§ã€å¯æ‰©å±•æ€§å’Œå¯æ§æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCogAtomåœ¨å‡†ç¡®æ€§ã€æ¨ç†æ·±åº¦å’Œå¤šæ ·æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”Ÿæˆçš„é—®é¢˜ä¸AIMEçš„éš¾åº¦ç›¸åŒ¹é…ï¼Œå¹¶åœ¨ç»“æ„å˜åŒ–ä¸Šè¶…è¿‡äº†AIMEã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºå¯ä¼¸ç¼©ã€é«˜è´¨é‡çš„æ•°å­¦é—®é¢˜ç”Ÿæˆæä¾›äº†ä¸€æ¡è®¤çŸ¥åŸºç¡€é€”å¾„ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/Icarus-1111/CogAtom%E3%80%82">https://github.com/Icarus-1111/CogAtomã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17318v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢ä¸´æ•°å­¦æ¨ç†çš„å¤šæ­¥éª¤å’ŒæŠ½è±¡æ¦‚å¿µæ•´åˆçš„æŒ‘æˆ˜æ—¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CogAtomï¼Œä¸€ç§æ–°å‹çš„åŸºäºè®¤çŸ¥åŸå­çš„æ¡†æ¶ï¼Œç”¨äºåˆæˆä¸¥è°¨çš„æ•°å­¦å’Œå¤šæ ·åŒ–çš„è®¤çŸ¥é—®é¢˜ã€‚å®ƒé€šè¿‡é€‰æ‹©å’Œé‡ç»„ä»äººç±»è§£å†³æ–¹æ¡ˆä¸­æå–çš„åŸºæœ¬æ¨ç†å•å…ƒâ€”â€”è®¤çŸ¥åŸå­ï¼Œæ¥æ„å»ºé—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCogAtomåœ¨å‡†ç¡®æ€§ã€æ¨ç†æ·±åº¦å’Œå¤šæ ·æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”Ÿæˆçš„é—®é¢˜éš¾åº¦ä¸AIMEç›¸å½“ï¼Œåœ¨ç»“æ„å˜åŒ–ä¸Šè¡¨ç°æ›´ä¼˜ã€‚CogAtomä¸ºå¤§è§„æ¨¡é«˜è´¨é‡æ•°å­¦é—®é¢˜ç”Ÿæˆæä¾›äº†è®¤çŸ¥ä¾æ®çš„è·¯å¾„ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦å¤šæ­¥éª¤æ¨ç†å’ŒæŠ½è±¡æ¦‚å¿µæ•´åˆã€‚</li>
<li>CogAtomæ˜¯ä¸€ä¸ªåŸºäºè®¤çŸ¥åŸå­çš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºåˆæˆæ•°å­¦å’Œè®¤çŸ¥é—®é¢˜ã€‚</li>
<li>CogAtomé€šè¿‡é€‰æ‹©å’Œé‡ç»„è®¤çŸ¥åŸå­æ¥æ„å»ºé—®é¢˜ï¼Œç¡®ä¿é€»è¾‘åˆç†æ€§å’Œç»“æ„æœ‰æ•ˆæ€§ã€‚</li>
<li>è®¤çŸ¥åŸå­æå–è‡ªäººç±»è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å¤šæ ·æ€§ä¿ƒè¿›çš„éšæœºæ¸¸èµ°ç®—æ³•ç”¨äºæ¢ç´¢è®¤çŸ¥åŸå­ç©ºé—´ã€‚</li>
<li>ç»„åˆæ€§è´¨å›¾ç»“æ„æä¾›äº†è¿‘ä¹æ— é™çš„æ¨ç†è·¯å¾„ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17318">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-809a922d35a5ed56331eeb9718630d99~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423380&auth_key=1760423380-0-0-3bd786ba2f34755c2d2b365b69bda75d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2e41e20506bd9f286f7ce16288e9ffc6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423388&auth_key=1760423388-0-0-820c633dff3f841220980c585f493bd1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9bc78142eb7144e88b9eb28ebdb5e02a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423395&auth_key=1760423395-0-0-7d15a4f750ab85f53b147d3940e63f29&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-74f86844cae9770738760b1020b7ed29~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423402&auth_key=1760423402-0-0-0a7c70250c7024e4bce42892771158ce&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="GRPOformer-Advancing-Hyperparameter-Optimization-via-Group-Relative-Policy-Optimization"><a href="#GRPOformer-Advancing-Hyperparameter-Optimization-via-Group-Relative-Policy-Optimization" class="headerlink" title="GRPOformer: Advancing Hyperparameter Optimization via Group Relative   Policy Optimization"></a>GRPOformer: Advancing Hyperparameter Optimization via Group Relative   Policy Optimization</h2><p><strong>Authors:Haoxin Guo, Jiawen Pan, Weixin Zhai</strong></p>
<p>Hyperparameter optimization (HPO) plays a critical role in improving model performance. Transformer-based HPO methods have shown great potential; however, existing approaches rely heavily on large-scale historical optimization trajectories and lack effective reinforcement learning (RL) techniques, thereby limiting their efficiency and performance improvements. Inspired by the success of Group Relative Policy Optimization (GRPO) in large language models (LLMs), we propose GRPOformer â€“ a novel hyperparameter optimization framework that integrates reinforcement learning (RL) with Transformers. In GRPOformer, Transformers are employed to generate new hyperparameter configurations from historical optimization trajectories, while GRPO enables rapid trajectory construction and optimization strategy learning from scratch. Moreover, we introduce Policy Churn Regularization (PCR) to enhance the stability of GRPO training. Experimental results on OpenML demonstrate that GRPOformer consistently outperforms baseline methods across diverse tasks, offering new insights into the application of RL for HPO. </p>
<blockquote>
<p>è¶…å‚æ•°ä¼˜åŒ–ï¼ˆHPOï¼‰åœ¨æé«˜æ¨¡å‹æ€§èƒ½ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚åŸºäºTransformerçš„HPOæ–¹æ³•å·²æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼›ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºå¤§è§„æ¨¡çš„å†å²ä¼˜åŒ–è½¨è¿¹ï¼Œä¸”ç¼ºä¹æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æŠ€æœ¯ï¼Œä»è€Œé™åˆ¶äº†å…¶æ•ˆç‡å’Œæ€§èƒ½çš„æå‡ã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†GRPOformerâ€”â€”ä¸€ä¸ªå°†å¼ºåŒ–å­¦ä¹ ä¸Transformerç›¸ç»“åˆçš„æ–°å‹è¶…å‚æ•°ä¼˜åŒ–æ¡†æ¶ã€‚åœ¨GRPOformerä¸­ï¼ŒTransformerè¢«ç”¨äºä»å†å²ä¼˜åŒ–è½¨è¿¹ç”Ÿæˆæ–°çš„è¶…å‚æ•°é…ç½®ï¼Œè€ŒGRPOåˆ™èƒ½å¤Ÿå®ç°ä»é›¶å¼€å§‹çš„å¿«é€Ÿè½¨è¿¹æ„å»ºå’Œä¼˜åŒ–ç­–ç•¥å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç­–ç•¥å˜åŠ¨æ­£åˆ™åŒ–ï¼ˆPCRï¼‰ä»¥å¢å¼ºGRPOè®­ç»ƒç¨³å®šæ€§ã€‚åœ¨OpenMLä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGRPOformeråœ¨å¤šç§ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œä¸ºRLåœ¨HPOä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17105v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Hyperparameter optimizationï¼ˆHPOï¼‰åœ¨æ”¹è¿›æ¨¡å‹æ€§èƒ½ä¸­çš„å…³é”®ä½œç”¨ã€‚é’ˆå¯¹ç°æœ‰Transformer-based HPOæ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è¶…å‚æ•°ä¼˜åŒ–æ¡†æ¶GRPOformerï¼Œå®ƒç»“åˆäº†å¼ºåŒ–å­¦ä¹ ä¸TransformeræŠ€æœ¯ã€‚GRPOformeråˆ©ç”¨Transformerä»å†å²ä¼˜åŒ–è½¨è¿¹ç”Ÿæˆæ–°çš„è¶…å‚æ•°é…ç½®ï¼Œè€ŒGRPOåˆ™å®ç°äº†å¿«é€Ÿè½¨è¿¹æ„å»ºå’Œä¼˜åŒ–ç­–ç•¥å­¦ä¹ ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†Policy Churn Regularizationï¼ˆPCRï¼‰ä»¥å¢å¼ºGRPOè®­ç»ƒçš„ç¨³å®šæ€§ã€‚åœ¨OpenMLä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGRPOformeråœ¨å¤šç§ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œä¸ºHPOåœ¨RLä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HPOåœ¨æé«˜æ¨¡å‹æ€§èƒ½æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ã€‚</li>
<li>ç°æœ‰Transformer-based HPOæ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œç¼ºä¹å¼ºåŒ–å­¦ä¹ æŠ€æœ¯çš„åº”ç”¨ã€‚</li>
<li>GRPOformeræ˜¯ä¸€ä¸ªæ–°çš„è¶…å‚æ•°ä¼˜åŒ–æ¡†æ¶ï¼Œç»“åˆäº†å¼ºåŒ–å­¦ä¹ ä¸TransformeræŠ€æœ¯ã€‚</li>
<li>Transformersç”¨äºä»å†å²ä¼˜åŒ–è½¨è¿¹ç”Ÿæˆæ–°çš„è¶…å‚æ•°é…ç½®ã€‚</li>
<li>GRPOå®ç°äº†å¿«é€Ÿè½¨è¿¹æ„å»ºå’Œä¼˜åŒ–ç­–ç•¥å­¦ä¹ ã€‚</li>
<li>å¼•å…¥Policy Churn Regularizationï¼ˆPCRï¼‰ä»¥å¢å¼ºGRPOè®­ç»ƒçš„ç¨³å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17105">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-16f2d9eed2682b7f3b32816f64ea040a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423410&auth_key=1760423410-0-0-9cdce2dbeb2f36348d5383ffcfce8985&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f658bb825f0e93e0d43535562b2b3002~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423417&auth_key=1760423417-0-0-58475a9e6d52c34fb917fd894fcc3f75&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ed933537ddf575bab8c0496d6e5883a6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423424&auth_key=1760423424-0-0-fea82a1aa82fe6f98a757eea2f681e55&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b5bf1f681713040605daeac12d655e77~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423431&auth_key=1760423431-0-0-baade426ce4e174f0caec7c05ee7c3eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0a99d1cad780f1251db0ffc21977ee4a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423437&auth_key=1760423437-0-0-f9f1d11540e34e6f0646b62acb673f05&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CoPlanner-An-Interactive-Motion-Planner-with-Contingency-Aware-Diffusion-for-Autonomous-Driving"><a href="#CoPlanner-An-Interactive-Motion-Planner-with-Contingency-Aware-Diffusion-for-Autonomous-Driving" class="headerlink" title="CoPlanner: An Interactive Motion Planner with Contingency-Aware   Diffusion for Autonomous Driving"></a>CoPlanner: An Interactive Motion Planner with Contingency-Aware   Diffusion for Autonomous Driving</h2><p><strong>Authors:Ruiguo Zhong, Ruoyu Yao, Pei Liu, Xiaolong Chen, Rui Yang, Jun Ma</strong></p>
<p>Accurate trajectory prediction and motion planning are crucial for autonomous driving systems to navigate safely in complex, interactive environments characterized by multimodal uncertainties. However, current generation-then-evaluation frameworks typically construct multiple plausible trajectory hypotheses but ultimately adopt a single most likely outcome, leading to overconfident decisions and a lack of fallback strategies that are vital for safety in rare but critical scenarios. Moreover, the usual decoupling of prediction and planning modules could result in socially inconsistent or unrealistic joint trajectories, especially in highly interactive traffic. To address these challenges, we propose a contingency-aware diffusion planner (CoPlanner), a unified framework that jointly models multi-agent interactive trajectory generation and contingency-aware motion planning. Specifically, the pivot-conditioned diffusion mechanism anchors trajectory sampling on a validated, shared short-term segment to preserve temporal consistency, while stochastically generating diverse long-horizon branches that capture multimodal motion evolutions. In parallel, we design a contingency-aware multi-scenario scoring strategy that evaluates candidate ego trajectories across multiple plausible long-horizon evolution scenarios, balancing safety, progress, and comfort. This integrated design preserves feasible fallback options and enhances robustness under uncertainty, leading to more realistic interaction-aware planning. Extensive closed-loop experiments on the nuPlan benchmark demonstrate that CoPlanner consistently surpasses state-of-the-art methods on both Val14 and Test14 datasets, achieving significant improvements in safety and comfort under both reactive and non-reactive settings. Code and model will be made publicly available upon acceptance. </p>
<blockquote>
<p>å‡†ç¡®çš„è½¨è¿¹é¢„æµ‹å’Œè¿åŠ¨è§„åˆ’å¯¹äºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚ã€äº¤äº’æ€§å¼ºçš„ç¯å¢ƒä¸­å®‰å…¨å¯¼èˆªè‡³å…³é‡è¦ï¼Œè¿™äº›ç¯å¢ƒå…·æœ‰å¤šæ¨¡å¼ä¸ç¡®å®šæ€§ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç”Ÿæˆ-è¯„ä¼°æ¡†æ¶é€šå¸¸ä¼šæ„å»ºå¤šä¸ªåˆç†çš„è½¨è¿¹å‡è®¾ï¼Œä½†æœ€ç»ˆåªé‡‡ç”¨ä¸€ä¸ªæœ€å¯èƒ½çš„ç»“æœï¼Œè¿™å¯¼è‡´è¿‡äºè‡ªä¿¡çš„å†³ç­–å’Œç¼ºä¹å…³é”®çš„å¤‡ä»½ç­–ç•¥ï¼Œè¿™åœ¨ç½•è§ä½†å…³é”®çš„åœºæ™¯ä¸­è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œé¢„æµ‹å’Œè§„åˆ’æ¨¡å—çš„å¸¸è§„è§£è€¦å¯èƒ½å¯¼è‡´ç¤¾ä¼šä¸ä¸€è‡´æˆ–ä¸ç¬¦åˆç°å®çš„è”åˆè½¨è¿¹ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜åº¦äº¤äº’çš„äº¤é€šç¯å¢ƒä¸­ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ„å¤–æƒ…å†µçš„æ‰©æ•£è§„åˆ’å™¨ï¼ˆCoPlannerï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè”åˆæ¨¡å‹å¤šæ™ºèƒ½ä½“äº¤äº’è½¨è¿¹ç”Ÿæˆå’ŒåŸºäºæ„å¤–æƒ…å†µçš„ååŒè§„åˆ’çš„ç»Ÿä¸€æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒåŸºäºæ¢ç‚¹çš„æ‰©æ•£æœºåˆ¶å°†è½¨è¿¹é‡‡æ ·é”šå®šåœ¨ä¸€ä¸ªéªŒè¯è¿‡çš„å…±äº«çŸ­æœŸç‰‡æ®µä¸Šï¼Œä»¥ä¿æŒæ—¶é—´ä¸€è‡´æ€§ï¼ŒåŒæ—¶éšæœºç”Ÿæˆå¤šæ ·ä¸”é•¿è¿œçš„åˆ†æ”¯ï¼Œä»¥æ•æ‰å¤šæ¨¡å¼è¿åŠ¨æ¼”å˜ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºæ„å¤–æƒ…å†µçš„åœºæ™¯è¯„åˆ†ç­–ç•¥ï¼Œè¯¥ç­–ç•¥è¯„ä¼°å¤šä¸ªå¯èƒ½çš„é•¿è¿œæœªæ¥æƒ…æ™¯ä¸‹çš„å€™é€‰è‡ªæˆ‘è½¨è¿¹ï¼Œåœ¨å®‰å…¨æ€§ã€è¿›å±•å’Œèˆ’é€‚æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚è¿™ç§é›†æˆè®¾è®¡ä¿ç•™äº†å¯è¡Œçš„å¤‡ä»½é€‰é¡¹ï¼Œå¢å¼ºäº†ä¸ç¡®å®šæƒ…å†µä¸‹çš„ç¨³å¥æ€§ï¼Œä»è€Œå®ç°äº†æ›´ç°å®ã€äº¤äº’æ„è¯†æ›´å¼ºçš„è§„åˆ’ã€‚åœ¨nuPlanåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡é—­ç¯å®éªŒè¡¨æ˜ï¼ŒCoPlanneråœ¨Val14å’ŒTest14æ•°æ®é›†ä¸Šå§‹ç»ˆè¶…è¿‡æœ€æ–°æ–¹æ³•ï¼Œå¹¶åœ¨ååº”æ€§å’Œéååº”æ€§è®¾ç½®ä¸‹åœ¨å®‰å…¨æ€§å’Œèˆ’é€‚æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚è®ºæ–‡æ¥å—åå°†å…¬å¼€æä¾›ä»£ç å’Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17080v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡è®¨è®ºäº†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­å‡†ç¡®è½¨è¿¹é¢„æµ‹å’Œè¿åŠ¨è§„åˆ’çš„é‡è¦æ€§ï¼ŒæŒ‡å‡ºåœ¨å¤æ‚ã€äº¤äº’æ€§ç¯å¢ƒä¸­ï¼Œå¤šæ¨¡æ€ä¸ç¡®å®šæ€§ä¸‹çš„å®‰å…¨å¯¼èˆªæ˜¯å…³é”®æŒ‘æˆ˜ã€‚å½“å‰æ¡†æ¶é€šå¸¸é‡‡ç”¨ç”Ÿæˆ-è¯„ä¼°æ¨¡å¼ï¼Œæ„å»ºå¤šä¸ªå¯èƒ½çš„è½¨è¿¹å‡è®¾ï¼Œä½†æœ€ç»ˆé€‰æ‹©æœ€å¯èƒ½çš„å•ä¸€ç»“æœï¼Œå¯¼è‡´ç¼ºä¹åº”å¯¹ç½•è§ä½†å…³é”®åœºæ™¯çš„å¤‡ç”¨ç­–ç•¥ã€‚æ­¤å¤–ï¼Œé¢„æµ‹å’Œè§„åˆ’æ¨¡å—çš„è§£è€¦å¯èƒ½å¯¼è‡´ç¤¾ä¼šä¸ä¸€è‡´æˆ–åœ¨ä¸å…·é«˜åº¦äº¤äº’æ€§çš„äº¤é€šç¯å¢ƒä¸­äº§ç”Ÿä¸ç°å®çš„è”åˆè½¨è¿¹ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§åº”æ€¥æ„ŸçŸ¥æ‰©æ•£è§„åˆ’å™¨ï¼ˆCoPlannerï¼‰çš„æ¡†æ¶ï¼Œè”åˆè¿›è¡Œå¤šæ™ºèƒ½ä½“äº¤äº’è½¨è¿¹ç”Ÿæˆå’Œåº”æ€¥æ„ŸçŸ¥è¿åŠ¨è§„åˆ’ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸­å¿ƒæ¡ä»¶æ‰©æ•£æœºåˆ¶åœ¨éªŒè¯è¿‡çš„çŸ­æœŸè½¨è¿¹ä¸Šè¿›è¡Œè½¨è¿¹é‡‡æ ·ï¼ŒåŒæ—¶éšæœºç”Ÿæˆå¤šæ ·çš„é•¿æœŸåˆ†æ”¯ä»¥æ•æ‰å¤šæ¨¡æ€è¿åŠ¨æ¼”åŒ–ã€‚åŒæ—¶è®¾è®¡äº†ä¸€ç§åº”æ€¥æ„ŸçŸ¥å¤šåœºæ™¯è¯„åˆ†ç­–ç•¥ï¼Œåœ¨å¤šç§å¯èƒ½çš„é•¿æœŸæ¼”åŒ–åœºæ™¯ä¸­è¯„ä¼°å€™é€‰è½¨è¿¹ï¼Œå¹³è¡¡å®‰å…¨ã€è¿›å±•å’Œèˆ’é€‚åº¦ã€‚åœ¨nuPlanåŸºå‡†æµ‹è¯•ä¸Šçš„é—­ç¯å®éªŒè¡¨æ˜ï¼ŒCoPlanneråœ¨Val14å’ŒTest14æ•°æ®é›†ä¸Šå‡è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œåœ¨å®‰å…¨æ€§å’Œèˆ’é€‚æ€§æ–¹é¢å–å¾—æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿéœ€è¦å‡†ç¡®çš„è½¨è¿¹é¢„æµ‹å’Œè¿åŠ¨è§„åˆ’ä»¥åº”å¯¹å¤æ‚ç¯å¢ƒä¸­çš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>å½“å‰æ¡†æ¶é‡‡ç”¨å•ä¸€æœ€å¯èƒ½è½¨è¿¹å†³ç­–å¯èƒ½å¯¼è‡´ç¼ºä¹å¤‡ç”¨ç­–ç•¥ï¼Œå½±å“å®‰å…¨æ€§ã€‚</li>
<li>CoPlanneræ¡†æ¶è”åˆè¿›è¡Œå¤šæ™ºèƒ½ä½“äº¤äº’è½¨è¿¹ç”Ÿæˆå’Œåº”æ€¥æ„ŸçŸ¥è¿åŠ¨è§„åˆ’ï¼Œæé«˜å®‰å…¨æ€§ä¸ç¨³å¥æ€§ã€‚</li>
<li>æ¡†æ¶ä¸­çš„ä¸­å¿ƒæ¡ä»¶æ‰©æ•£æœºåˆ¶ç¡®ä¿è½¨è¿¹é‡‡æ ·çš„çŸ­æœŸä¸€è‡´æ€§ï¼Œå¹¶éšæœºç”Ÿæˆé•¿æœŸåˆ†æ”¯ä»¥æ•æ‰å¤šæ¨¡æ€è¿åŠ¨æ¼”åŒ–ã€‚</li>
<li>åº”æ€¥æ„ŸçŸ¥å¤šåœºæ™¯è¯„åˆ†ç­–ç•¥å¹³è¡¡å®‰å…¨ã€è¿›å±•å’Œèˆ’é€‚åº¦ï¼Œå®ç°æ›´ç°å®çš„äº¤äº’æ„ŸçŸ¥è§„åˆ’ã€‚</li>
<li>åœ¨nuPlanåŸºå‡†æµ‹è¯•ä¸Šï¼ŒCoPlanneråœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨æ€§å’Œèˆ’é€‚æ€§æ–¹é¢è¡¨ç°çªå‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7683dd14bd5f8f83c8e59c482b8eea62~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423444&auth_key=1760423444-0-0-2fb7bdc041916db5edb59f359a36e4e3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c95f12c1dadc7cb18833a65e8a6f1abc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423452&auth_key=1760423452-0-0-efc9b133797153b547d9b3d17533ca89&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dfeb58a398c1025d69cb5fc173675868~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423459&auth_key=1760423459-0-0-c0863542f8058e2f8169c65303140afa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-80b50cd41a46923cde8804316ec42646~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423466&auth_key=1760423466-0-0-d3460b07163a958382e24c5ed5783507&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Interpretable-Audio-Editing-Evaluation-via-Chain-of-Thought-Difference-Commonality-Reasoning-with-Multimodal-LLMs"><a href="#Interpretable-Audio-Editing-Evaluation-via-Chain-of-Thought-Difference-Commonality-Reasoning-with-Multimodal-LLMs" class="headerlink" title="Interpretable Audio Editing Evaluation via Chain-of-Thought   Difference-Commonality Reasoning with Multimodal LLMs"></a>Interpretable Audio Editing Evaluation via Chain-of-Thought   Difference-Commonality Reasoning with Multimodal LLMs</h2><p><strong>Authors:Yuhang Jia, Xu Zhang, Yang Chen, Hui Wang, Enzhi Wang, Yong Qin</strong></p>
<p>Automatic mean opinion score (MOS) prediction provides a more perceptual alternative to objective metrics, offering deeper insights into the evaluated models. With the rapid progress of multimodal large language models (MLLMs), their enhanced perceptual and reasoning abilities enable more comprehensive and interpretable audio quality assessment. In this work, we tackle the challenging task of audio editing evaluation and propose the first natural language-based automated evaluation framework built on MLLMs. Our approach introduces two fine-tuning tasks to boost multi-audio understanding, combined with Chain-of-Thought prompting, and lightweight instruction tuning, to enhance step-by-step reasoning. Experiment demonstrate that our framework delivers accurate, interpretable, and text-based editing evaluation, closely aligning with human judgments and objective metrics while substantially improving over baselines. The code and demo are available at <a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/Eval_Reasoning">https://github.com/NKU-HLT/Eval_Reasoning</a>. </p>
<blockquote>
<p>è‡ªåŠ¨å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰é¢„æµ‹æä¾›äº†ä¸€ç§æ¯”å®¢è§‚æŒ‡æ ‡æ›´æ„ŸçŸ¥çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä¸ºæ‰€è¯„ä¼°çš„æ¨¡å‹æä¾›äº†æ›´æ·±å…¥çš„äº†è§£ã€‚éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå…¶å¢å¼ºçš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›èƒ½å¤Ÿè¿›è¡Œæ›´å…¨é¢å’Œå¯è§£é‡Šæ€§çš„éŸ³é¢‘è´¨é‡è¯„ä¼°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†éŸ³é¢‘ç¼–è¾‘è¯„ä¼°è¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå¹¶åŸºäºMLLMsæå‡ºäº†ç¬¬ä¸€ä¸ªè‡ªç„¶è¯­è¨€åŸºç¡€çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸¤ä¸ªå¾®è°ƒä»»åŠ¡æ¥ä¿ƒè¿›å¤šéŸ³é¢‘ç†è§£ï¼Œç»“åˆâ€œæ€ç»´é“¾â€æç¤ºå’Œè½»é‡çº§æŒ‡ä»¤å¾®è°ƒï¼Œä»¥å¢å¼ºé€æ­¥æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿæä¾›å‡†ç¡®ã€å¯è§£é‡Šæ€§å’ŒåŸºäºæ–‡æœ¬ç¼–è¾‘è¯„ä¼°ï¼Œä¸äººç±»åˆ¤æ–­å’Œå®¢è§‚æŒ‡æ ‡ç´§å¯†å¯¹é½ï¼ŒåŒæ—¶åœ¨åŸºå‡†æµ‹è¯•ä¸Šæœ‰æ˜¾è‘—æé«˜ã€‚ä»£ç å’Œæ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/Eval_Reasoning%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NKU-HLT/Eval_Reasoningæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16975v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è‡ªåŠ¨å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰é¢„æµ‹ä¸ºå®¢è§‚åº¦é‡æä¾›äº†ä¸€ä¸ªæ›´åŠ æ„ŸçŸ¥çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå®ƒèƒ½ä¸ºæ‰€è¯„ä¼°çš„æ¨¡å‹æä¾›æ›´æ·±å…¥çš„è§è§£ã€‚éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå…¶å¢å¼ºçš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ä½¿éŸ³é¢‘è´¨é‡è¯„ä¼°æ›´åŠ å…¨é¢å’Œå¯è§£é‡Šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†éŸ³é¢‘ç¼–è¾‘è¯„ä¼°è¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå¹¶é¦–æ¬¡æå‡ºäº†åŸºäºMLLMsçš„è‡ªç„¶è¯­è¨€è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸¤ä¸ªå¾®è°ƒä»»åŠ¡æ¥æé«˜å¤šéŸ³é¢‘ç†è§£ï¼Œå¹¶ç»“åˆæ€ç»´é“¾æç¤ºå’Œè½»é‡çº§æŒ‡ä»¤å¾®è°ƒï¼Œä»¥å¢å¼ºé€æ­¥æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿæä¾›å‡†ç¡®ã€å¯è§£é‡Šã€åŸºäºæ–‡æœ¬ç¼–è¾‘è¯„ä¼°ï¼Œä¸äººç±»åˆ¤æ–­å’Œå®¢è§‚åº¦é‡ç´§å¯†å¯¹é½ï¼Œå¹¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¼˜äºåŸºçº¿ã€‚ä»£ç å’Œæ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/Eval_Reasoning%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NKU-HLT/Eval_Reasoningæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰é¢„æµ‹ä¸ºæ¨¡å‹è¯„ä¼°æä¾›äº†æ›´æ·±å…¥çš„æ„ŸçŸ¥è§†è§’ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•å¢å¼ºäº†æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨éŸ³é¢‘ç¼–è¾‘è¯„ä¼°ä»»åŠ¡ä¸­å¼•å…¥äº†è‡ªç„¶è¯­è¨€è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>é€šè¿‡ä¸¤ä¸ªå¾®è°ƒä»»åŠ¡å’Œæ€ç»´é“¾æç¤ºæé«˜å¤šéŸ³é¢‘ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ¡†æ¶å‡†ç¡®ã€å¯è§£é‡Šï¼ŒåŸºäºæ–‡æœ¬ç¼–è¾‘è¯„ä¼°ä¸äººç±»åˆ¤æ–­å’Œå®¢è§‚åº¦é‡å¯¹é½ã€‚</li>
<li>æ¡†æ¶åœ¨éŸ³é¢‘ç¼–è¾‘è¯„ä»·ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16975">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e3c7262854f689941a79c559eb39f10b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423475&auth_key=1760423475-0-0-83067d415b9445c4a5cd58ae9e7a3258&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2a4c8bcad4ebdd5021da337c7c309957~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423482&auth_key=1760423482-0-0-8d217bb27780fc17d0b29c6b1de34275&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2f0e76807e2bee2c478ca8d83a19aa70~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423489&auth_key=1760423489-0-0-63d9357b92a3e57d2e99295a3f894ba3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-568fbf97cc572c22256f6f1d5ac83e6b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423496&auth_key=1760423496-0-0-d3148766b503a13ac163d928821aac88&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="LLMs-as-Layout-Designers-A-Spatial-Reasoning-Perspective"><a href="#LLMs-as-Layout-Designers-A-Spatial-Reasoning-Perspective" class="headerlink" title="LLMs as Layout Designers: A Spatial Reasoning Perspective"></a>LLMs as Layout Designers: A Spatial Reasoning Perspective</h2><p><strong>Authors:Sha Li</strong></p>
<p>While Large Language Models (LLMs) have demonstrated impressive reasoning and planning abilities in textual domains and can effectively follow instructions for complex tasks, their capacity for spatial understanding and reasoning remains limited. Such capabilities, however, are critical for applications like content-aware graphic layout design, which demands precise placement, alignment, and structural organization of multiple elements within constrained visual spaces. To address this gap, we propose LaySPA, a reinforcement learning-based framework that augments LLM agents with explicit spatial reasoning capabilities. LaySPA leverages hybrid reward signals that capture geometric validity, structural fidelity, and visual quality, enabling agents to model inter-element relationships, navigate the canvas, and optimize spatial arrangements. Through iterative self-exploration and adaptive policy optimization, LaySPA produces both interpretable reasoning traces and structured layouts. Experimental results demonstrate that LaySPA generates structurally sound and visually appealing layouts, outperforming larger general-purpose LLMs and achieving results on par with state-of-the-art specialized layout models. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬é¢†åŸŸè¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ï¼Œå¹¶èƒ½æœ‰æ•ˆæ‰§è¡Œå¤æ‚ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œä½†å®ƒä»¬åœ¨ç©ºé—´ç†è§£å’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚ç„¶è€Œï¼Œå¯¹äºå†…å®¹æ„ŸçŸ¥çš„å›¾å½¢å¸ƒå±€è®¾è®¡è¿™ç±»åº”ç”¨è€Œè¨€ï¼Œè¿™ç§èƒ½åŠ›è‡³å…³é‡è¦ã€‚å†…å®¹æ„ŸçŸ¥çš„å›¾å½¢å¸ƒå±€è®¾è®¡è¦æ±‚åœ¨æœ‰é™çš„è§†è§‰ç©ºé—´å†…å¯¹å¤šä¸ªå…ƒç´ è¿›è¡Œç²¾ç¡®æ”¾ç½®ã€å¯¹é½å’Œç»“æ„ç»„ç»‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†LaySPAï¼Œä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œä¸ºLLMä»£ç†å¢åŠ äº†æ˜ç¡®çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚LaySPAåˆ©ç”¨æ··åˆå¥–åŠ±ä¿¡å·ï¼Œæ•æ‰å‡ ä½•æœ‰æ•ˆæ€§ã€ç»“æ„ä¿çœŸåº¦å’Œè§†è§‰è´¨é‡ï¼Œä½¿ä»£ç†èƒ½å¤Ÿæ¨¡æ‹Ÿå…ƒç´ é—´çš„å…³ç³»ã€å¯¼èˆªç”»å¸ƒå¹¶ä¼˜åŒ–ç©ºé—´å¸ƒå±€ã€‚é€šè¿‡è¿­ä»£è‡ªæˆ‘æ¢ç´¢å’Œè‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–ï¼ŒLaySPAæ—¢äº§ç”Ÿå¯è§£é‡Šçš„æ¨ç†è½¨è¿¹ï¼Œåˆç”Ÿæˆç»“æ„åŒ–å¸ƒå±€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaySPAç”Ÿæˆçš„å¸ƒå±€ç»“æ„å¥å…¨ã€è§†è§‰å¸å¼•åŠ›å¼ºï¼Œä¼˜äºæ›´å¤§çš„é€šç”¨LLMï¼Œå¹¶ä¸æœ€æ–°çš„ä¸“ä¸šå¸ƒå±€æ¨¡å‹ç»“æœç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16891v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬é¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ï¼Œå¹¶èƒ½æœ‰æ•ˆæ‰§è¡Œå¤æ‚ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œä½†åœ¨ç©ºé—´ç†è§£å’Œæ¨ç†æ–¹é¢ä»å­˜åœ¨å±€é™ã€‚å¯¹äºå†…å®¹æ„ŸçŸ¥çš„å›¾å½¢å¸ƒå±€è®¾è®¡ç­‰åº”ç”¨ï¼Œç²¾ç¡®æ”¾ç½®ã€å¯¹é½å’Œç»“æ„åŒ–ç»„ç»‡è§†è§‰ç©ºé—´å†…çš„å¤šä¸ªå…ƒç´ è‡³å…³é‡è¦ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæå‡ºLaySPAï¼Œä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œå¢å¼ºLLMä»£ç†çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚LaySPAåˆ©ç”¨æ··åˆå¥–åŠ±ä¿¡å·ï¼Œæ•æ‰å‡ ä½•æœ‰æ•ˆæ€§ã€ç»“æ„ä¿çœŸåº¦å’Œè§†è§‰è´¨é‡ï¼Œä½¿ä»£ç†èƒ½å¤Ÿæ¨¡æ‹Ÿå…ƒç´ é—´çš„å…³ç³»ã€å¯¼èˆªç”»å¸ƒå¹¶ä¼˜åŒ–ç©ºé—´å¸ƒå±€ã€‚é€šè¿‡è¿­ä»£è‡ªæˆ‘æ¢ç´¢å’Œè‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–ï¼ŒLaySPAäº§ç”Ÿå¯è§£é‡Šçš„æ¨ç†è½¨è¿¹å’Œç»“æ„åŒ–çš„å¸ƒå±€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaySPAç”Ÿæˆçš„å¸ƒå±€ç»“æ„ç¨³å¥ã€è§†è§‰å¸å¼•åŠ›å¼ºï¼Œä¼˜äºå¤§å‹é€šç”¨LLMsï¼Œå¹¶ä¸ä¸“ä¸šå¸ƒå±€æ¨¡å‹çš„æœ€æ–°ç»“æœç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬å¤„ç†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ï¼Œä½†åœ¨ç©ºé—´ç†è§£å’Œæ¨ç†æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>ç©ºé—´ç†è§£å’Œæ¨ç†å¯¹äºå†…å®¹æ„ŸçŸ¥çš„å›¾å½¢å¸ƒå±€è®¾è®¡ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>LaySPAæ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºLLMä»£ç†çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>LaySPAåˆ©ç”¨æ··åˆå¥–åŠ±ä¿¡å·æ¥æ•æ‰å‡ ä½•æœ‰æ•ˆæ€§ã€ç»“æ„ä¿çœŸåº¦å’Œè§†è§‰è´¨é‡ã€‚</li>
<li>LaySPAä½¿ä»£ç†èƒ½å¤Ÿæ¨¡æ‹Ÿå…ƒç´ é—´çš„å…³ç³»ã€å¯¼èˆªç”»å¸ƒå¹¶ä¼˜åŒ–ç©ºé—´å¸ƒå±€ã€‚</li>
<li>é€šè¿‡è¿­ä»£è‡ªæˆ‘æ¢ç´¢å’Œè‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–ï¼ŒLaySPAèƒ½å¤Ÿäº§ç”Ÿå¯è§£é‡Šçš„æ¨ç†è½¨è¿¹å’Œç»“æ„åŒ–çš„å¸ƒå±€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16891">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-44ac4520fd0aa154593c6353cf01d4e3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423503&auth_key=1760423503-0-0-38118c88404040f6ab1686f8c74de42e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eb823355a79bbdaef7d5a5a36d6f7cc9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423511&auth_key=1760423511-0-0-2881b57eeeb3a953a2392cd889d7a114&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dd2e230c6280495bad8c395556ba31fc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423518&auth_key=1760423518-0-0-9bc5194a316df7832f76ef7e9b451367&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e0562327b62134727a2a24f93d38232d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423526&auth_key=1760423526-0-0-0d4ee135845c882fa72b481000b0d9a6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Evaluating-LLM-Generated-Detection-Rules-in-Cybersecurity"><a href="#Evaluating-LLM-Generated-Detection-Rules-in-Cybersecurity" class="headerlink" title="Evaluating LLM Generated Detection Rules in Cybersecurity"></a>Evaluating LLM Generated Detection Rules in Cybersecurity</h2><p><strong>Authors:Anna Bertiger, Bobby Filar, Aryan Luthra, Stefano Meschiari, Aiden Mitchell, Sam Scholten, Vivek Sharath</strong></p>
<p>LLMs are increasingly pervasive in the security environment, with limited measures of their effectiveness, which limits trust and usefulness to security practitioners. Here, we present an open-source evaluation framework and benchmark metrics for evaluating LLM-generated cybersecurity rules. The benchmark employs a holdout set-based methodology to measure the effectiveness of LLM-generated security rules in comparison to a human-generated corpus of rules. It provides three key metrics inspired by the way experts evaluate security rules, offering a realistic, multifaceted evaluation of the effectiveness of an LLM-based security rule generator. This methodology is illustrated using rules from Sublime Securityâ€™s detection team and those written by Sublime Securityâ€™s Automated Detection Engineer (ADE), with a thorough analysis of ADEâ€™s skills presented in the results section. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å®‰å…¨ç¯å¢ƒä¸­çš„åº”ç”¨è¶Šæ¥è¶Šæ™®éï¼Œä½†å…¶æœ‰æ•ˆæ€§çš„è¡¡é‡æªæ–½æœ‰é™ï¼Œè¿™é™åˆ¶äº†å®‰å…¨ä»ä¸šäººå‘˜å¯¹å…¶çš„ä¿¡ä»»å’Œå®ç”¨æ€§ã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªå¼€æºè¯„ä¼°æ¡†æ¶å’ŒåŸºå‡†æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°LLMç”Ÿæˆçš„ç½‘ç»œå®‰å…¨è§„åˆ™çš„æ•ˆèƒ½ã€‚è¯¥åŸºå‡†é‡‡ç”¨åŸºäºä¿ç•™é›†çš„æ–¹æ³•ï¼Œæ¥è¡¡é‡LLMç”Ÿæˆçš„ç½‘ç»œå®‰å…¨è§„åˆ™ä¸äººä¸ºç”Ÿæˆçš„è§„åˆ™é›†ç›¸æ¯”çš„æœ‰æ•ˆæ€§ã€‚å®ƒæä¾›äº†ä¸‰ä¸ªå—ä¸“å®¶è¯„ä¼°å®‰å…¨è§„åˆ™å¯å‘çš„å…³é”®æŒ‡æ ‡ï¼Œä¸ºåŸºäºLLMçš„å®‰å…¨è§„åˆ™ç”Ÿæˆå™¨æä¾›ç°å®ã€å¤šæ–¹é¢çš„æœ‰æ•ˆæ€§è¯„ä¼°ã€‚è¯¥æ–¹æ³•ä½¿ç”¨Sublime Securityæ£€æµ‹å›¢é˜Ÿåˆ¶å®šçš„è§„åˆ™ä»¥åŠSublime Securityè‡ªåŠ¨æ£€æµ‹å·¥ç¨‹å¸ˆï¼ˆADEï¼‰ç¼–å†™çš„è§„åˆ™è¿›è¡Œè¯´æ˜ï¼Œå¹¶åœ¨ç»“æœéƒ¨åˆ†å¯¹ADEçš„æŠ€èƒ½è¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16749v1">PDF</a> Preprint of a paper accepted at the Conference on Applied Machine   Learning in Information Security (CAMLIS 2025). 11 pages, 3 figures, 4 tables</p>
<p><strong>Summary</strong>ï¼šéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®‰å…¨é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œå¯¹å…¶ç”Ÿæˆçš„å®‰å…¨è§„åˆ™çš„æœ‰æ•ˆæ€§è¯„ä¼°æ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¼€æºè¯„ä¼°æ¡†æ¶å’ŒåŸºå‡†æµ‹è¯•æŒ‡æ ‡ï¼Œä»¥æ¯”è¾ƒLLMç”Ÿæˆçš„å®‰å…¨è§„åˆ™ä¸äººç±»ç”Ÿæˆçš„å®‰å…¨è§„åˆ™é›†çš„æœ‰æ•ˆæ€§ã€‚è¯¥åŸºå‡†æµ‹è¯•é‡‡ç”¨ä¿ç•™é›†æ–¹æ³•ï¼Œé€šè¿‡ä¸“å®¶è¯„ä»·å®‰å…¨è§„åˆ™çš„ä¸‰ä¸ªå…³é”®æŒ‡æ ‡æ¥å…¨é¢è¯„ä¼°LLMç”Ÿæˆçš„å®‰å…¨è§„åˆ™çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶ä½¿ç”¨äº†Sublime Securityçš„æ£€æµ‹å›¢é˜Ÿè§„åˆ™å’Œè‡ªåŠ¨åŒ–æ£€æµ‹å·¥ç¨‹å¸ˆï¼ˆADEï¼‰ç¼–å†™çš„è§„åˆ™ï¼Œå¹¶å¯¹ADEçš„æŠ€èƒ½è¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>LLMsåœ¨å®‰å…¨é¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œä½†å¯¹å…¶ç”Ÿæˆçš„å®‰å…¨è§„åˆ™çš„æœ‰æ•ˆæ€§è¯„ä¼°æªæ–½æœ‰é™ï¼Œå½±å“äº†å®‰å…¨å®è·µè€…å¯¹å®ƒä»¬çš„ä¿¡ä»»å’Œä½¿ç”¨ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§å¼€æºè¯„ä¼°æ¡†æ¶å’ŒåŸºå‡†æµ‹è¯•æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°LLMç”Ÿæˆçš„å®‰å…¨è§„åˆ™çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥è¯„ä¼°æ¡†æ¶é‡‡ç”¨ä¿ç•™é›†æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œè¡¡é‡LLMç”Ÿæˆçš„å®‰å…¨è§„åˆ™ä¸äººç±»ç”Ÿæˆçš„å®‰å…¨è§„åˆ™é›†çš„æ•ˆæœã€‚</li>
<li>æä¾›äº†ä¸‰ä¸ªå…³é”®æŒ‡æ ‡ï¼Œä»¥ä¸“å®¶è¯„ä»·å®‰å…¨è§„åˆ™çš„æ–¹å¼å…¨é¢è¯„ä¼°LLMç”Ÿæˆçš„å®‰å…¨è§„åˆ™çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†Sublime Securityçš„æ£€æµ‹å›¢é˜Ÿè§„åˆ™å’ŒADEçš„è§„åˆ™è¿›è¡Œå®ä¾‹åˆ†æã€‚</li>
<li>è¯¥ç ”ç©¶å¼ºè°ƒäº†è‡ªåŠ¨åŒ–æ£€æµ‹å·¥ç¨‹å¸ˆï¼ˆADEï¼‰æŠ€èƒ½çš„é‡è¦æ€§ï¼Œåœ¨ç»“æœéƒ¨åˆ†å¯¹å…¶è¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16749">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ed9428e06e90b09517f399100c63fce2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423533&auth_key=1760423533-0-0-f76864e0fdc34d86db17ba598ae71057&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5ec9e6d381ad44a614316ae9b97f8fd1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423540&auth_key=1760423540-0-0-4fafa0ace7326527d16ffa87883d15dd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3f0359127ba94065f30d92cae413602b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423547&auth_key=1760423547-0-0-bf53717a436dddf7eeea08cee91c1c94&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="EG-MLA-Embedding-Gated-Multi-head-Latent-Attention-for-Scalable-and-Efficient-LLMs"><a href="#EG-MLA-Embedding-Gated-Multi-head-Latent-Attention-for-Scalable-and-Efficient-LLMs" class="headerlink" title="EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and   Efficient LLMs"></a>EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and   Efficient LLMs</h2><p><strong>Authors:Zhengge Cai, Haowen Hou</strong></p>
<p>Reducing the key-value (KV) cache size is a crucial step toward enabling efficient inference in large language models (LLMs), especially under latency and memory constraints. While Multi-Head Attention (MHA) offers strong representational power, it incurs significant memory overhead. Recent work on Multi-head Latent Attention (MLA) mitigates this by compressing KV representations into a shared latent space, achieving a better trade-off between performance and cache efficiency. While MLA already achieves significant KV cache reduction, the scope for further compression remains limited without performance loss. In this paper, we propose \textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel extension of MLA that further reduces KV cache size while enhancing representational expressiveness. EG-MLA introduces a token-specific embedding gating mechanism applied in the latent space, enabling fine-grained modulation of compressed KV vectors with minimal additional computation. Compared to MHA, EG-MLA achieves over 91.6% reduction in KV cache size with negligible performance degradation. Relative to MLA, EG-MLA consistently improves task accuracy across diverse reasoning benchmarks while achieving up to 59.9% additional memory savings. Our theoretical analysis highlights how embedding gating induces implicit high-order interactions, and empirical evaluations demonstrate robust generalization across model scales and compression regimes. Notably, we successfully scale EG-MLA to over 1 billion parameters, demonstrating its practical viability for large-scale LLM deployment. These results establish EG-MLA as a memory- and compute-efficient attention mechanism that enables scalable, high-performance inference in modern LLMs. </p>
<blockquote>
<p>å‡å°‘é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜å¤§å°æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å®ç°é«˜æ•ˆæ¨ç†çš„å…³é”®æ­¥éª¤ï¼Œå°¤å…¶æ˜¯åœ¨å»¶è¿Ÿå’Œå†…å­˜å—é™çš„æƒ…å†µä¸‹ã€‚è™½ç„¶å¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰å…·æœ‰å¾ˆå¼ºçš„è¡¨å¾èƒ½åŠ›ï¼Œä½†å®ƒä¼šå¼•èµ·å·¨å¤§çš„å†…å­˜å¼€é”€ã€‚æœ€è¿‘çš„å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼ˆMLAï¼‰çš„ç ”ç©¶é€šè¿‡å‹ç¼©KVè¡¨ç¤ºåˆ°å…±äº«æ½œåœ¨ç©ºé—´æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œåœ¨æ€§èƒ½å’Œç¼“å­˜æ•ˆç‡ä¹‹é—´å®ç°äº†æ›´å¥½çš„æƒè¡¡ã€‚å°½ç®¡MLAå·²ç»å®ç°äº†æ˜¾è‘—çš„KVç¼“å­˜å‡å°‘ï¼Œä½†åœ¨ä¸æŸå¤±æ€§èƒ½çš„æƒ…å†µä¸‹è¿›ä¸€æ­¥å‹ç¼©çš„èŒƒå›´ä»ç„¶æœ‰é™ã€‚</p>
</blockquote>
<p>åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>åµŒå…¥é—¨æ§å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼ˆEG-MLAï¼‰</strong>ï¼Œè¿™æ˜¯MLAçš„ä¸€ç§æ–°å‹æ‰©å±•ï¼Œå¯ä»¥è¿›ä¸€æ­¥å‡å°‘KVç¼“å­˜å¤§å°ï¼ŒåŒæ—¶æé«˜è¡¨å¾è¡¨è¾¾èƒ½åŠ›ã€‚EG-MLAåœ¨æ½œåœ¨ç©ºé—´ä¸­åº”ç”¨äº†ä¸€ç§ç‰¹å®šäºç¬¦å·çš„åµŒå…¥é—¨æ§æœºåˆ¶ï¼Œèƒ½å¤Ÿå¯¹å‹ç¼©çš„KVå‘é‡è¿›è¡Œç²¾ç»†è°ƒèŠ‚ï¼ŒåŒæ—¶åªéœ€æå°‘çš„é¢å¤–è®¡ç®—ã€‚ä¸MHAç›¸æ¯”ï¼ŒEG-MLAå®ç°äº†è¶…è¿‡91.6%çš„KVç¼“å­˜å¤§å°å‡å°‘ï¼ŒåŒæ—¶æ€§èƒ½æŸå¤±å¾®ä¹å…¶å¾®ã€‚ç›¸å¯¹äºMLAï¼ŒEG-MLAåœ¨å¤šç§æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆæé«˜äº†ä»»åŠ¡å‡†ç¡®æ€§ï¼ŒåŒæ—¶å®ç°äº†é«˜è¾¾59.9%çš„é¢å¤–å†…å­˜èŠ‚çœã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†æå¼ºè°ƒäº†åµŒå…¥é—¨æ§å¦‚ä½•å¼•å‘éšå¼çš„é«˜é˜¶äº¤äº’ï¼Œè€Œç»éªŒè¯„ä¼°åˆ™è¯æ˜äº†å…¶åœ¨æ¨¡å‹è§„æ¨¡å’Œå‹ç¼©åˆ¶åº¦ä¸‹çš„ç¨³å¥æ³›åŒ–èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬æˆåŠŸåœ°å°†EG-MLAæ‰©å±•åˆ°è¶…è¿‡1äº¿å‚æ•°ï¼Œè¿™è¯æ˜äº†å…¶åœ¨å¤§è§„æ¨¡LLMéƒ¨ç½²ä¸­çš„å®é™…å¯è¡Œæ€§ã€‚è¿™äº›ç»“æœå°†EG-MLAç¡®ç«‹ä¸ºä¸€ç§å†…å­˜å’Œè®¡ç®—é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨ç°ä»£LLMä¸­å®ç°å¯æ‰©å±•ã€é«˜æ€§èƒ½çš„æ¨ç†ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16686v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¦‚ä½•é€šè¿‡å‡å°‘é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜å¤§å°ä»¥å®ç°é«˜æ•ˆæ¨ç†ã€‚æ–‡ç« æå‡ºäº†Embedding-Gated Multi-head Latent Attentionï¼ˆEG-MLAï¼‰è¿™ä¸€æ–°å‹æ‰©å±•æœºåˆ¶ï¼Œè¿›ä¸€æ­¥å‡å°KVç¼“å­˜å¤§å°å¹¶æé«˜è¡¨è¾¾åŠ›ã€‚EG-MLAåœ¨æ½œåœ¨ç©ºé—´å¼•å…¥ç‰¹å®šç¬¦å·åµŒå…¥é—¨æ§æœºåˆ¶ï¼Œå¯å¯¹å‹ç¼©çš„KVå‘é‡è¿›è¡Œç²¾ç»†è°ƒåˆ¶ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬å¢åŠ æå°ã€‚ä¸Multi-Head Attentionï¼ˆMHAï¼‰ç›¸æ¯”ï¼ŒEG-MLAåœ¨KVç¼“å­˜å¤§å°ä¸Šå‡å°‘äº†è¶…è¿‡91.6%ï¼Œæ€§èƒ½ä¸‹é™å¾®ä¹å…¶å¾®ã€‚ç›¸å¯¹äºMulti-head Latent Attentionï¼ˆMLAï¼‰ï¼ŒEG-MLAåœ¨å„ç§æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä»»åŠ¡å‡†ç¡®åº¦æŒç»­æé«˜ï¼ŒåŒæ—¶å®ç°äº†é«˜è¾¾59.9%çš„é¢å¤–å†…å­˜èŠ‚çœã€‚æœ¬æ–‡ç†è®ºåˆ†æå’Œå®è¯è¯„ä¼°è¯æ˜äº†embeddingé—¨æ§æœºåˆ¶èƒ½å¤Ÿå¼•å‘éšå¼é«˜é˜¶äº¤äº’ï¼Œå¹¶åœ¨æ¨¡å‹è§„æ¨¡å’Œå‹ç¼©æœºåˆ¶ä¸Šå±•ç°å‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ï¼ŒEG-MLAæˆåŠŸæ‰©å±•è‡³è¶…è¿‡1äº¿å‚æ•°ï¼Œè¯æ˜å…¶åœ¨å¤§è§„æ¨¡LLMéƒ¨ç½²ä¸­çš„å®é™…å¯è¡Œæ€§ã€‚ç»“æœè¯æ˜EG-MLAæ˜¯ä¸€ç§å†…å­˜å’Œè®¡ç®—é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯å®ç°ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯ä¼¸ç¼©ã€é«˜æ€§èƒ½æ¨ç†ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å‡å°‘é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜å¤§å°æ˜¯å®ç°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é«˜æ•ˆæ¨ç†çš„å…³é”®æ­¥éª¤ã€‚</li>
<li>Multi-head Latent Attentionï¼ˆMLAï¼‰é€šè¿‡å‹ç¼©KVè¡¨ç¤ºåˆ°å…±äº«æ½œåœ¨ç©ºé—´ï¼Œå®ç°äº†æ€§èƒ½å’Œç¼“å­˜æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>æå‡ºçš„Embedding-Gated Multi-head Latent Attentionï¼ˆEG-MLAï¼‰æœºåˆ¶è¿›ä¸€æ­¥å‡å°KVç¼“å­˜å¤§å°ï¼ŒåŒæ—¶æé«˜è¡¨è¾¾åŠ›ã€‚</li>
<li>EG-MLAå¼•å…¥ç‰¹å®šç¬¦å·åµŒå…¥é—¨æ§æœºåˆ¶ï¼Œå¯å¯¹å‹ç¼©çš„KVå‘é‡è¿›è¡Œç²¾ç»†è°ƒåˆ¶ï¼Œä¸”å‡ ä¹ä¸å½±å“æ€§èƒ½ã€‚</li>
<li>ä¸MHAç›¸æ¯”ï¼ŒEG-MLAå¤§å¹…å‡å°‘äº†KVç¼“å­˜å¤§å°ï¼ŒåŒæ—¶ä¿æŒé«˜æ€§èƒ½ã€‚</li>
<li>ç›¸å¯¹MLAï¼ŒEG-MLAæé«˜äº†ä»»åŠ¡å‡†ç¡®åº¦ï¼Œå¹¶å®ç°äº†æ˜¾è‘—çš„é¢å¤–å†…å­˜èŠ‚çœã€‚</li>
<li>EG-MLAæˆåŠŸæ‰©å±•è‡³å¤§è§„æ¨¡å‚æ•°ï¼Œè¯æ˜å…¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹éƒ¨ç½²ä¸­çš„å®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-01fa8c61fa7421c5ea9525e33a8a55aa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423555&auth_key=1760423555-0-0-154b6031ae682949ebf8d59cc1bf80db&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d080617ddf75e84d4281479a3f1c1ec8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423563&auth_key=1760423563-0-0-986a6f70704e8f5dd083a9ebdadc9bac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6bc53de0a6675041dda443b2bcdaca37~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423569&auth_key=1760423569-0-0-ecf687a38acc4bf6cde4a0c16855fb27&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dbd9f7ad3812d0f18a95a706733cb80c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423576&auth_key=1760423576-0-0-4b5c5ce2b982b0fb7666c820af4b2eee&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-01faaf9af4ffdfe08a4ea059ac362f5c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423583&auth_key=1760423583-0-0-543ae7eca6102c104b229bb9552a0028&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c69cc033e6b5f8a12598b268a598edad~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423590&auth_key=1760423590-0-0-506a78e625de0328a70bf0e957e10545&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-d457104edf5e48858abc5b8cf10a6a4a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760423596&auth_key=1760423596-0-0-9f4ac54c2991ce00fc009fa6ccc9c691&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-24  Seg4Diff Unveiling Open-Vocabulary Segmentation in Text-to-Image   Diffusion Transformers
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-2728ea1bfc3efad0d6c082b65169f3e3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684145&auth_key=1760684145-0-0-470bcc0fbc27a2bc8e4ee5fe453f0bca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-24  Beat on Gaze Learning Stylized Generation of Gaze and Head Dynamics
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
