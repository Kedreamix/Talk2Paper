<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-09-24  UniPixel Unified Object Referring and Segmentation for Pixel-Level   Visual Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.18056v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    83 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-24-更新"><a href="#2025-09-24-更新" class="headerlink" title="2025-09-24 更新"></a>2025-09-24 更新</h1><h2 id="UniPixel-Unified-Object-Referring-and-Segmentation-for-Pixel-Level-Visual-Reasoning"><a href="#UniPixel-Unified-Object-Referring-and-Segmentation-for-Pixel-Level-Visual-Reasoning" class="headerlink" title="UniPixel: Unified Object Referring and Segmentation for Pixel-Level   Visual Reasoning"></a>UniPixel: Unified Object Referring and Segmentation for Pixel-Level   Visual Reasoning</h2><p><strong>Authors:Ye Liu, Zongyang Ma, Junfu Pu, Zhongang Qi, Yang Wu, Ying Shan, Chang Wen Chen</strong></p>
<p>Recent advances in Large Multi-modal Models (LMMs) have demonstrated their remarkable success as general-purpose multi-modal assistants, with particular focuses on holistic image- and video-language understanding. Conversely, less attention has been given to scaling fine-grained pixel-level understanding capabilities, where the models are expected to realize pixel-level alignment between visual signals and language semantics. Some previous studies have applied LMMs to related tasks such as region-level captioning and referring expression segmentation. However, these models are limited to performing either referring or segmentation tasks independently and fail to integrate these fine-grained perception capabilities into visual reasoning. To bridge this gap, we propose UniPixel, a large multi-modal model capable of flexibly comprehending visual prompt inputs and generating mask-grounded responses. Our model distinguishes itself by seamlessly integrating pixel-level perception with general visual understanding capabilities. Specifically, UniPixel processes visual prompts and generates relevant masks on demand, and performs subsequent reasoning conditioning on these intermediate pointers during inference, thereby enabling fine-grained pixel-level reasoning. The effectiveness of our approach has been verified on 10 benchmarks across a diverse set of tasks, including pixel-level referring&#x2F;segmentation and object-centric understanding in images&#x2F;videos. A novel PixelQA task that jointly requires referring, segmentation, and question answering is also designed to verify the flexibility of our method. </p>
<blockquote>
<p>近期大型多模态模型（LMMs）的进展证明了它们在作为通用多模态助理方面的显著成功，特别是在整体图像和视频语言理解方面。然而，对于精细化像素级别的理解能力的规模扩展，关注度相对较低。在这些模型中，期望模型能够实现视觉信号和语义语言之间的像素级对齐。虽然一些早期研究已将LMMs应用于相关任务，如区域级标题和引用表达式分割，但这些模型仅限于独立执行引用或分割任务，无法将这些精细化的感知能力整合到视觉推理中。为了弥补这一差距，我们提出了UniPixel，这是一个大型多模态模型，能够灵活理解视觉提示输入并产生基于遮罩的响应。我们的模型通过无缝集成像素级感知和一般视觉理解能力来区分自己。具体来说，UniPixel处理视觉提示并根据需求生成相关遮罩，在推理过程中基于这些中间指针进行后续推理，从而实现精细的像素级推理。我们的方法的有效性已在10个基准测试上得到验证，涵盖了一系列任务，包括像素级的引用&#x2F;分割和图像&#x2F;视频的以对象为中心的理解。还设计了一个新颖的PixelQA任务，联合需要引用、分割和问答，以验证我们方法的灵活性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18094v1">PDF</a> NeurIPS 2025 Camera Ready. Project Page:   <a target="_blank" rel="noopener" href="https://polyu-chenlab.github.io/unipixel/">https://polyu-chenlab.github.io/unipixel/</a></p>
<p><strong>Summary</strong></p>
<p>大型多模态模型（LMMs）在通用多模态助理方面取得了显著的成功，特别是在整体图像和视频语言理解方面。然而，对于精细粒度的像素级理解能力的扩展关注较少，其中模型需要在视觉信号和语言语义之间实现像素级对齐。为弥补这一差距，本文提出了UniPixel模型，该模型能够灵活理解视觉提示输入并生成基于掩码的响应。UniPixel通过无缝集成像素级感知与通用视觉理解能力来区分自己。实验结果表明，该模型在包括像素级引用&#x2F;分割和图像&#x2F;视频中的对象中心理解在内的多个任务上的有效性。还设计了一个新型PixelQA任务来验证方法的灵活性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型多模态模型（LMMs）已广泛应用于通用多模态助理领域，特别是在整体图像和视频语言理解方面表现出显著成功。</li>
<li>当前缺乏精细粒度的像素级理解能力，要求模型实现视觉信号与语言语义之间的像素级对齐。</li>
<li>UniPixel模型旨在解决这一差距，通过灵活理解视觉提示输入并生成基于掩码的响应来实现像素级理解。</li>
<li>UniPixel通过无缝集成像素级感知和通用视觉理解能力来区分其他模型。</li>
<li>实验结果证明了UniPixel模型在多个任务上的有效性，包括像素级引用、分割和图像&#x2F;视频中的对象中心理解。</li>
<li>新型PixelQA任务旨在验证模型的灵活性，涵盖引用、分割和问答等多个方面。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18094">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.18094v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.18094v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.18094v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.18094v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.18094v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs"><a href="#TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs" class="headerlink" title="TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning   for Video LLMs"></a>TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning   for Video LLMs</h2><p><strong>Authors:Yunheng Li, Jing Cheng, Shaoyong Jia, Hangyi Kuang, Shaohui Jiao, Qibin Hou, Ming-Ming Cheng</strong></p>
<p>This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework designed to improve the effectiveness of adapting multimodal large language models (MLLMs) to video temporal grounding tasks. We reveal that existing reinforcement learning methods, such as Group Relative Policy Optimization (GRPO), rely on on-policy sampling for policy updates. However, in tasks with large temporal search spaces, this strategy becomes both inefficient and limited in performance, as it often fails to identify temporally accurate solutions. To address this limitation, TempSamp-R1 leverages ground-truth annotations as off-policy supervision to provide temporally precise guidance, effectively compensating for the sparsity and misalignment in on-policy solutions. To further stabilize training and reduce variance in reward-based updates, TempSamp-R1 provides a non-linear soft advantage computation method that dynamically reshapes the reward feedback via an asymmetric transformation. By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1 optimizes a single unified model to support both CoT and non-CoT inference modes, enabling efficient handling of queries with varying reasoning complexity. Experimental results demonstrate that TempSamp-R1 outperforms GRPO-based baselines, establishing new state-of-the-art performance on benchmark datasets: Charades-STA (<a href="mailto:&#82;&#x31;&#64;&#x30;&#x2e;&#x37;">&#82;&#x31;&#64;&#x30;&#x2e;&#x37;</a>: 52.9%, +2.7%), ActivityNet Captions (<a href="mailto:&#82;&#x31;&#64;&#48;&#46;&#53;">&#82;&#x31;&#64;&#48;&#46;&#53;</a>: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover, TempSamp-R1 shows robust few-shot generalization capabilities under limited data. Code: <a target="_blank" rel="noopener" href="https://github.com/HVision-NKU/TempSamp-R1">https://github.com/HVision-NKU/TempSamp-R1</a> </p>
<blockquote>
<p>本文介绍了TempSamp-R1，这是一个新的强化精细调整框架，旨在提高多模态大型语言模型（MLLMs）适应视频时间定位任务的效率。我们发现现有的强化学习方法，如群组相对策略优化（GRPO），依赖于策略更新过程中的在线策略采样。然而，在具有大时间搜索空间的任务中，此策略既效率低下，性能也受到局限，因为它往往无法找到时间准确的解决方案。为了解决这一局限性，TempSamp-R1利用真实标注作为离线策略监督，提供时间精确的指导，有效地弥补了在线策略解决方案中的稀疏性和不对齐问题。为了进一步稳定训练和减少基于奖励的更新的方差，TempSamp-R1提供了一种非线性软优势计算方法，通过不对称转换动态重塑奖励反馈。通过采用混合的“思维链”（CoT）训练范式，TempSamp-R1优化了一个统一的单一模型，以支持CoT和非CoT推理模式，从而能够高效处理不同复杂度的查询。实验结果表明，TempSamp-R1优于基于GRPO的基线模型，在基准数据集上创下了最新纪录：Charades-STA（<a href="mailto:&#82;&#x31;&#64;&#x30;&#46;&#55;">&#82;&#x31;&#64;&#x30;&#46;&#55;</a>: 52.9%，+2.7%）、ActivityNet Captions（<a href="mailto:&#x52;&#x31;&#64;&#x30;&#46;&#53;">&#x52;&#x31;&#64;&#x30;&#46;&#53;</a>: 56.0%，+5.3%）和QVHighlights（mAP: 30.0%，+3.0%）。此外，TempSamp-R1在有限数据下表现出强大的泛化能力。代码：<a target="_blank" rel="noopener" href="https://github.com/HVision-NKU/TempSamp-R1%E3%80%82">https://github.com/HVision-NKU/TempSamp-R1。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18056v1">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为TempSamp-R1的新型强化精细调整框架，旨在提高多模态大型语言模型（MLLMs）在视频时序定位任务中的效率。针对现有强化学习方法在大型时序搜索空间中的局限性和不足，TempSamp-R1利用真实标注作为离线策略监督，提供精确的时间指导，并引入非线性软优势计算方法，动态调整奖励反馈。结合Chain-of-Thought训练范式，TempSamp-R1支持多种推理模式，提高处理不同复杂度查询的效率。实验结果显示，TempSamp-R1在Charades-STA、ActivityNet Captions和QVHighlights等基准数据集上取得了最新性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TempSamp-R1是一个针对视频时序定位任务的多模态大型语言模型强化精细调整框架。</li>
<li>现有强化学习方法在大型时序搜索空间中表现不佳，TempSamp-R1通过引入离线策略监督和精确时间指导来解决这一问题。</li>
<li>TempSamp-R1采用非线性软优势计算方法，动态调整奖励反馈，进一步提高训练稳定性和性能。</li>
<li>结合Chain-of-Thought训练范式，TempSamp-R1支持多种推理模式，适应不同复杂度的查询。</li>
<li>TempSamp-R1在多个基准数据集上取得了最新性能，包括Charades-STA、ActivityNet Captions和QVHighlights。</li>
<li>TempSamp-R1具有强大的少样本泛化能力，在有限数据下表现稳健。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18056">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.18056v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.18056v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.18056v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.18056v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Everyday-Physics-in-Korean-Contexts-A-Culturally-Grounded-Physical-Reasoning-Benchmark"><a href="#Everyday-Physics-in-Korean-Contexts-A-Culturally-Grounded-Physical-Reasoning-Benchmark" class="headerlink" title="Everyday Physics in Korean Contexts: A Culturally Grounded Physical   Reasoning Benchmark"></a>Everyday Physics in Korean Contexts: A Culturally Grounded Physical   Reasoning Benchmark</h2><p><strong>Authors:Jihae Jeong, DaeYeop Lee, DongGeon Lee, Hwanjo Yu</strong></p>
<p>Existing physical commonsense reasoning benchmarks predominantly focus on Western contexts, overlooking cultural variations in physical problem-solving. To address this gap, we introduce EPiK (Everyday Physics in Korean Contexts), a novel benchmark comprising 181 binary-choice problems that test physical reasoning within Korean cultural contexts, ranging from kimchi (Korean food) to traditional fermentation. EPiK is constructed using a two-stage generation and verification pipeline to create culturally-authentic problems across 9 reasoning subtasks and 84 scenarios. Unlike approaches based on simple translation, our method generates problems organically from Korean contexts while upholding rigorous physical reasoning standards. Our evaluations show that Korean-specialized models consistently outperform general-purpose models of comparable size. This performance gap highlights the limitations of culturally-agnostic models and demonstrates the critical need for culturally-aware benchmarks to truly measure language understanding. Our EPiK is publicly available at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/jjae/EPiK">https://huggingface.co/datasets/jjae/EPiK</a>. </p>
<blockquote>
<p>现有的物理常识推理基准测试主要关注西方背景，忽视了物理问题解决中的文化差异。为了弥补这一空白，我们推出了EPiK（韩国背景下的日常物理学）这一新型基准测试，包含181个二元选择题，旨在测试韩国文化背景下的物理推理能力，涵盖从泡菜（韩国食品）到传统发酵等多个领域。EPiK的构建采用了两阶段生成和验证流程，以创建涵盖9个推理子任务和84个场景的文化认证问题。不同于基于简单翻译的方法，我们的方法从韩国背景中有机生成问题，同时坚持严格的物理推理标准。我们的评估显示，针对韩国特色的模型持续优于通用模型。这种性能差距突显了文化无知模型（culturally-agnostic models）的局限性，并展示了真正衡量语言理解能力迫切需要对文化意识的基准测试。我们的EPiK可在<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/jjae/EPiK">https://huggingface.co/datasets/jjae/EPiK</a>上公开访问。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17807v1">PDF</a> Accepted to MRL@EMNLP 2025</p>
<p><strong>Summary</strong>：</p>
<p>为了弥补物理常识推理基准测试主要集中在西方背景下的局限性，忽视在不同文化背景下的物理问题解决能力的差异，本文引入了一项新的基准测试——EPiK（日常生活中的物理学在韩国背景下的应用）。EPiK包含181个二元选择题，测试在韩国文化背景下对物理推理的应用，涉及韩国食品如泡菜和传统发酵等方面。EPiK采用两阶段生成和验证流程来构建真实反映韩国文化的问题，涉及9个推理子任务和84个场景。与其他简单翻译的方法不同，EPiK从韩国文化背景出发，同时遵循严格的物理推理标准生成问题。评估显示，针对韩国特色的模型表现优于通用模型。这突显了忽视文化差异的模型局限性，强调开发能够真正衡量语言理解能力的文化意识基准测试的重要性。本文提供的EPiK可在huggingface.co&#x2F;datasets&#x2F;jjae&#x2F;EPiK上公开获取。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>EPiK是一个基于韩国文化背景的日常生活物理常识推理基准测试。</li>
<li>EPiK包含181个二元选择题，涵盖韩国食品与传统发酵等内容。</li>
<li>EPiK采用两阶段生成和验证流程确保问题的真实性和物理推理的严谨性。</li>
<li>与简单翻译的方法不同，EPiK从韩国文化背景出发生成问题。</li>
<li>评估显示，针对韩国特色的模型表现优于通用模型，突显文化意识的重要性。</li>
<li>现有物理常识推理基准测试存在忽视文化差异的局限性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17807">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17807v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17807v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17807v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17807v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17807v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17807v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17807v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Adaptive-Fast-and-Slow-Visual-Program-Reasoning-for-Long-Form-VideoQA"><a href="#Adaptive-Fast-and-Slow-Visual-Program-Reasoning-for-Long-Form-VideoQA" class="headerlink" title="Adaptive Fast-and-Slow Visual Program Reasoning for Long-Form VideoQA"></a>Adaptive Fast-and-Slow Visual Program Reasoning for Long-Form VideoQA</h2><p><strong>Authors:Chenglin Li, Feng Han,  FengTao, Ruilin Li, Qianglong Chen, Jingqi Tong, Yin Zhang, Jiaqi Wang</strong></p>
<p>Large language models (LLMs) have shown promise in generating program workflows for visual tasks. However, previous approaches often rely on closed-source models, lack systematic reasoning, and struggle with long-form video question answering (videoQA). To address these challenges, we introduce the FS-VisPR framework, an adaptive visual program reasoning approach that balances fast reasoning for simple queries with slow reasoning for difficult ones. First, we design efficient visual modules (e.g., key clip retrieval and subtitle retrieval) to support long-form video tasks. Then, we construct a diverse and high-quality fast-slow reasoning dataset with a strong LLM to align open-source language models’ ability to generate visual program workflows as FS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple queries are directly solved by VideoLLMs, while difficult ones invoke visual program reasoning, motivated by human-like reasoning processes. During this process, low-confidence fast-thinking answers will trigger a second-stage slow-reasoning process, and a fallback mechanism to fast reasoning is activated if the program execution fails. Moreover, we improve visual programs through parameter search during both training and inference. By adjusting the parameters of the visual modules within the program, multiple variants are generated: during training, programs that yield correct answers are selected, while during inference, the program with the highest confidence result is applied. Experiments show that FS-VisPR improves both efficiency and reliability in visual program workflows. It achieves 50.4% accuracy on LVBench, surpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME. </p>
<blockquote>
<p>大型语言模型（LLM）在生成视觉任务的程序工作流程方面显示出巨大的潜力。然而，之前的方法常常依赖于闭源模型，缺乏系统推理能力，并且在长视频问答（videoQA）方面遇到困难。为了应对这些挑战，我们引入了FS-VisPR框架，这是一种自适应的视觉程序推理方法，能够平衡简单查询的快速推理和复杂查询的慢速推理。首先，我们设计了高效的视觉模块（如关键片段检索和字幕检索）来支持长视频任务。接着，我们构建了一个多样且高质量的快慢推理数据集，并借助强大的LLM，将开源语言模型生成视觉程序工作流程的能力与FS-LLM对齐。然后，我们设计了一个快慢推理框架结合FS-LLM：简单查询直接由VideoLLM解决，而复杂的查询则通过视觉程序推理解决，这激发了类似人类的推理过程。在此过程中，低信心的快速思考答案将触发第二阶段的慢速推理过程，如果程序执行失败，将激活返回到快速推理的备份机制。此外，我们通过程序和推理过程中的参数搜索改进了视觉程序。通过调整程序中视觉模块的参数，可以生成多个变体：在训练过程中，选择得出正确答案的程序；在推理过程中，应用具有最高置信度结果的程序。实验表明，FS-VisPR提高了视觉程序工作流程的效率和可靠性。它在LVBench上达到了50.4%的准确率，超越了GPT-4o，与Qwen2.5VL-72B在VideoMME上的性能相匹配。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17743v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型在生成视觉任务程序工作流程方面展现出潜力，但存在依赖封闭源代码模型、缺乏系统推理和应对长视频问答困难等挑战。为解决这些问题，提出FS-VisPR框架，采用快慢推理结合的方式，支持长视频任务，构建高质量数据集，设计快慢推理框架，触发低信心答案的第二阶段慢推理，并在训练和推理过程中调整参数提高程序性能。实验表明，FS-VisPR提高了视觉程序工作流程的效率和可靠性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型在视觉任务程序生成中具潜力。</li>
<li>现有方法依赖封闭源代码模型、缺乏系统推理和长视频问答能力。</li>
<li>FS-VisPR框架结合快慢推理，支持长视频任务。</li>
<li>构建高质量数据集，设计快慢推理框架，触发低信心答案的第二阶段慢推理。</li>
<li>训练和推理过程中调整参数提高程序性能。</li>
<li>FS-VisPR提高视觉程序工作流程效率和可靠性，达到行业领先水平。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17743">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17743v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17743v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17743v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17743v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17743v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ConfClip-Confidence-Weighted-and-Clipped-Reward-for-Reinforcement-Learning-in-LLMs"><a href="#ConfClip-Confidence-Weighted-and-Clipped-Reward-for-Reinforcement-Learning-in-LLMs" class="headerlink" title="ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement   Learning in LLMs"></a>ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement   Learning in LLMs</h2><p><strong>Authors:Bonan Zhang, Zhongqi Chen, Bowen Song, Qinya Li, Fan Wu, Guihai Chen</strong></p>
<p>Reinforcement learning (RL) has become a standard paradigm for refining large language models (LLMs) beyond pre-training and instruction tuning. A prominent line of work is RL with verifiable rewards (RLVR), which leverages automatically verifiable outcomes (e.g., correctness or executability) to generate reward signals. While efficient, this framework faces two key limitations: First, its binary feedback is too sparse to capture the quality of the reasoning process. Second, its coarse-grained rewards potentially lead to vanishing gradients. Inspired by observations from human learning, we introduce a RL technique that integrates verifiable outcomes with the model’s own confidence estimates. This joint design enriches the reward signal, providing finer-grained feedback and implicitly supervising the reasoning process. Experimental results demonstrate that our proposed method enhances RL performance across multiple datasets and reduces token consumption during inference, while incurring negligible additional training cost. Moreover, it can be used as a plug-in module to enhance other state-of-the-art RL methods. </p>
<blockquote>
<p>强化学习（RL）已经成为超越预训练和指令微调来优化大型语言模型（LLM）的标准范式。一条突出的工作线是带可验证奖励的强化学习（RLVR），它利用可自动验证的结果（如正确性或可执行性）来生成奖励信号。虽然效率很高，但这个框架面临两个主要局限：首先，其二进制反馈过于稀疏，无法捕捉推理过程的质量；其次，其粗粒度的奖励可能导致梯度消失。受人类学习的观察启发，我们引入了一种强化学习技术，该技术将可验证的结果与模型本身的置信度估计相结合。这种联合设计丰富了奖励信号，提供了更精细的反馈，并隐含地监督了推理过程。实验结果表明，我们提出的方法在多个数据集上提高了强化学习的性能，降低了推理过程中的令牌消耗，同时只产生了微不足道的额外训练成本。此外，它可以作为插件模块用于增强其他最先进强化学习方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17730v1">PDF</a> </p>
<p><strong>Summary</strong><br>强化学习（RL）已成为优化大型语言模型（LLM）的标准范式，尤其在预训练和指令微调之后。RLVR是其中的一条重要研究线路，它利用可自动验证的结果（如正确性）来生成奖励信号。然而，该框架存在两个主要局限：一是其反馈稀疏，无法捕捉推理过程的质量；二是其粗粒度的奖励可能导致梯度消失。本研究受人类学习的启发，提出了一种结合可验证结果与模型自身置信度估计的RL技术。这种联合设计丰富了奖励信号，提供了更精细的反馈，并隐含地监督了推理过程。实验结果表明，该方法提高了多个数据集上的RL性能，降低了推理时的令牌消耗，且几乎不增加额外的训练成本。此外，它还可以作为插件模块增强其他先进的RL方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习已成为优化大型语言模型的标准方法。</li>
<li>RLVR利用可自动验证的结果生成奖励信号，但存在反馈稀疏和粗粒度奖励的问题。</li>
<li>结合可验证结果与模型自身置信度估计的RL技术被提出，以丰富奖励信号并提供更精细的反馈。</li>
<li>该方法提高了多个数据集上的RL性能，降低了推理时的令牌消耗。</li>
<li>该方法几乎不增加额外的训练成本。</li>
<li>该方法可作为插件模块增强其他先进的RL方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17730">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17730v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17730v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17730v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17730v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="EngiBench-A-Benchmark-for-Evaluating-Large-Language-Models-on-Engineering-Problem-Solving"><a href="#EngiBench-A-Benchmark-for-Evaluating-Large-Language-Models-on-Engineering-Problem-Solving" class="headerlink" title="EngiBench: A Benchmark for Evaluating Large Language Models on   Engineering Problem Solving"></a>EngiBench: A Benchmark for Evaluating Large Language Models on   Engineering Problem Solving</h2><p><strong>Authors:Xiyuan Zhou, Xinlei Wang, Yirui He, Yang Wu, Ruixi Zou, Yuheng Cheng, Yulu Xie, Wenxuan Liu, Huan Zhao, Yan Xu, Jinjin Gu, Junhua Zhao</strong></p>
<p>Large language models (LLMs) have shown strong performance on mathematical reasoning under well-posed conditions. However, real-world engineering problems require more than mathematical symbolic computation – they need to deal with uncertainty, context, and open-ended scenarios. Existing benchmarks fail to capture these complexities. We introduce EngiBench, a hierarchical benchmark designed to evaluate LLMs on solving engineering problems. It spans three levels of increasing difficulty (foundational knowledge retrieval, multi-step contextual reasoning, and open-ended modeling) and covers diverse engineering subfields. To facilitate a deeper understanding of model performance, we systematically rewrite each problem into three controlled variants (perturbed, knowledge-enhanced, and math abstraction), enabling us to separately evaluate the model’s robustness, domain-specific knowledge, and mathematical reasoning abilities. Experiment results reveal a clear performance gap across levels: models struggle more as tasks get harder, perform worse when problems are slightly changed, and fall far behind human experts on the high-level engineering tasks. These findings reveal that current LLMs still lack the high-level reasoning needed for real-world engineering, highlighting the need for future models with deeper and more reliable problem-solving capabilities. Our source code and data are available at <a target="_blank" rel="noopener" href="https://github.com/EngiBench/EngiBench">https://github.com/EngiBench/EngiBench</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在条件良好的情况下在数学推理方面表现出强大的性能。然而，现实世界中的工程问题不仅仅需要数学符号计算，它们还需要处理不确定性、上下文和开放场景。现有的基准测试无法捕捉这些复杂性。我们引入了EngiBench，这是一个分层基准测试，旨在评估LLM解决工程问题的能力。它涵盖了三个难度递增的级别（基础知识检索、多步骤上下文推理和开放式建模），并涵盖了多样化的工程子领域。为了加深对模型性能的理解，我们系统地为每个问题重新编写了三种受控变体（扰动、知识增强和数学抽象），这使我们能够分别评估模型的稳健性、特定领域的知识和数学推理能力。实验结果揭示了各级之间的性能差距：随着任务难度的增加，模型的困难程度更大，当问题稍作更改时，表现更差，并且在高级工程任务上远远落后于人类专家。这些发现表明，当前的大型语言模型仍然缺乏真实世界工程所需的高级推理能力，这强调了未来模型需要更深、更可靠的解决问题的能力。我们的源代码和数据可在<a target="_blank" rel="noopener" href="https://github.com/EngiBench/EngiBench%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/EngiBench/EngiBench获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17677v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在数学推理方面表现出强大的性能，但在面对现实世界工程问题时，需要处理不确定性、上下文和开放场景等复杂因素，现有基准测试无法捕捉这些复杂性。本文介绍了EngiBench，一个旨在评估LLM解决工程问题能力的分层基准测试。它涵盖了三个难度递增的级别（基础知识检索、多步骤上下文推理和开放建模），并涉及多个工程子领域。通过系统地改写每个问题为三种受控变体（扰动、知识增强和数学抽象），可以单独评估模型的稳健性、领域特定知识和数学推理能力。实验结果表明，模型在面临更复杂的任务时表现挣扎，当问题稍微改变时表现更差，而在高级工程任务上远远落后于人类专家。这揭示了当前LLM仍然缺乏真实世界工程所需的高级推理能力，强调未来模型需要更深刻、更可靠的解决问题的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在数学推理上表现出色，但在解决工程问题时面临挑战。</li>
<li>现有基准测试无法捕捉工程问题中的复杂性，如不确定性、上下文和开放场景。</li>
<li>EngiBench是一个新的分层基准测试，旨在评估LLM解决工程问题的能力。</li>
<li>EngiBench包含三个难度递增的级别，覆盖多个工程子领域。</li>
<li>通过系统改写问题，可以单独评估模型的稳健性、领域知识和数学推理能力。</li>
<li>实验结果表明，LLM在面临更复杂的任务时表现不足，需要增强高级推理能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17677">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17677v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17677v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17677v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MSCoRe-A-Benchmark-for-Multi-Stage-Collaborative-Reasoning-in-LLM-Agents"><a href="#MSCoRe-A-Benchmark-for-Multi-Stage-Collaborative-Reasoning-in-LLM-Agents" class="headerlink" title="MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM   Agents"></a>MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM   Agents</h2><p><strong>Authors:Yuzhen Lei, Hongbin Xie, Jiaxing Zhao, Shuangxue Liu, Xuan Song</strong></p>
<p>Large Language Models (LLMs) have excelled in question-answering (QA) tasks within single domains. However, their reasoning and coordination capabilities in complex, multi-stage scenarios remain underexplored. Existing benchmarks typically focus on isolated tasks or narrow domains, overlooking models’ abilities for multi-stage collaboration and optimization without explicit external guidance. To bridge this gap, we propose \textbf{MSCoRe}, a novel benchmark comprising 126696 domain-specific QA instances spanning scenarios in automotive, pharmaceutical, electronics, and energy sectors. The dataset is created using a structured three-phase pipeline: dynamic sampling, iterative question-answer generation, and a multi-level quality assessment to ensure data quality. Tasks are further categorized into three difficulty levels according to stage coverage and complexity. With MSCoRe, we have conducted a comprehensive evaluation of various state-of-the-art LLM agents. The commercial models performed best across all tasks and scenarios, but a notable gap in ROUGE scores remains between simple and complex tasks. We also tested the models’ robustness and found that their performance is negatively affected by noisy data. MSCoRe provides a valuable new resource for the community to evaluate and improve multi-stage reasoning in LLM agents. The code and data are available at <a target="_blank" rel="noopener" href="https://github.com/D3E0-source/MSCoRE">https://github.com/D3E0-source/MSCoRE</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在单一领域内的问答任务中表现出色。然而，它们在复杂多阶段场景中的推理和协作能力仍然被探索得不够。现有的基准测试通常侧重于孤立的任务或狭窄的领域，忽视了模型在多个阶段协作和优化而无需明确外部指导的能力。为了弥补这一差距，我们提出了<strong>MSCoRe</strong>，这是一个新的基准测试，包含126696个特定领域的问答实例，涵盖汽车、制药、电子和能源领域。该数据集是使用结构化三阶段管道创建的：动态采样、迭代问答生成和多级质量评估，以确保数据质量。任务根据阶段覆盖范围和复杂性进一步分为三级难度。通过MSCoRe，我们对各种最新的大型语言模型代理进行了全面评估。商业模型在所有任务和场景中表现最佳，但在简单任务和复杂任务之间仍然存在明显的ROUGE评分差距。我们还测试了模型的稳健性，发现噪声数据会对性能产生负面影响。MSCoRe为社区提供了一个宝贵的新资源，用于评估和提高大型语言模型代理的多阶段推理能力。代码和数据可在<a target="_blank" rel="noopener" href="https://github.com/D3E0-source/MSCoRE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/D3E0-source/MSCoRE找到。</a></p>
</blockquote>
<p><strong>Translation</strong></p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17628v1">PDF</a> 10 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在单一领域内的问答任务中表现出色，但在复杂多阶段场景中的推理和协作能力尚未得到充分探索。现有基准测试主要关注孤立任务或狭窄领域，忽视了模型在多阶段协作和优化方面的能力，且缺乏明确的外部指导。为弥补这一空白，我们提出了MSCoRe基准测试，包含126696个特定领域的问答实例，涵盖汽车、制药、电子和能源等领域。该数据集通过结构化三阶段管道创建，包括动态采样、迭代问答生成和多级质量评估，以确保数据质量。任务根据阶段覆盖和复杂性分为三级难度。我们对各种最新LLM代理进行了全面评估，发现商业模型在所有任务和场景中表现最佳，但简单任务和复杂任务之间的ROUGE分数仍存在显著差距。我们还测试了模型的稳健性，发现噪声数据对性能有负面影响。MSCoRe为社区提供了一个宝贵的新资源，用于评估和提高LLM代理的多阶段推理能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在单一领域问答任务中表现出色，但在多阶段复杂场景中的推理和协作能力有待探索。</li>
<li>现有基准测试主要关注孤立任务或狭窄领域，缺乏多阶段协作和优化的测试。</li>
<li>MSCoRe基准测试是一个全新的数据集，包含多个领域（汽车、制药等）的126696个问答实例，用于评估LLM的多阶段推理能力。</li>
<li>数据集采用结构化三阶段管道创建，确保数据质量。</li>
<li>任务难度分为三级，涵盖不同的阶段覆盖和复杂性。</li>
<li>商业模型在所有任务和场景中表现最佳，但简单和复杂任务之间的性能差距仍然存在。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17628">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17628v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17628v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17628v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17628v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17628v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Table2LaTeX-RL-High-Fidelity-LaTeX-Code-Generation-from-Table-Images-via-Reinforced-Multimodal-Language-Models"><a href="#Table2LaTeX-RL-High-Fidelity-LaTeX-Code-Generation-from-Table-Images-via-Reinforced-Multimodal-Language-Models" class="headerlink" title="Table2LaTeX-RL: High-Fidelity LaTeX Code Generation from Table Images   via Reinforced Multimodal Language Models"></a>Table2LaTeX-RL: High-Fidelity LaTeX Code Generation from Table Images   via Reinforced Multimodal Language Models</h2><p><strong>Authors:Jun Ling, Yao Qi, Tao Huang, Shibo Zhou, Yanqin Huang, Jiang Yang, Ziqi Song, Ying Zhou, Yang Yang, Heng Tao Shen, Peng Wang</strong></p>
<p>In this work, we address the task of table image to LaTeX code generation, with the goal of automating the reconstruction of high-quality, publication-ready tables from visual inputs. A central challenge of this task lies in accurately handling complex tables – those with large sizes, deeply nested structures, and semantically rich or irregular cell content – where existing methods often fail. We begin with a comprehensive analysis, identifying key challenges and highlighting the limitations of current evaluation protocols. To overcome these issues, we propose a reinforced multimodal large language model (MLLM) framework, where a pre-trained MLLM is fine-tuned on a large-scale table-to-LaTeX dataset. To further improve generation quality, we introduce a dual-reward reinforcement learning strategy based on Group Relative Policy Optimization (GRPO). Unlike standard approaches that optimize purely over text outputs, our method incorporates both a structure-level reward on LaTeX code and a visual fidelity reward computed from rendered outputs, enabling direct optimization of the visual output quality. We adopt a hybrid evaluation protocol combining TEDS-Structure and CW-SSIM, and show that our method achieves state-of-the-art performance, particularly on structurally complex tables, demonstrating the effectiveness and robustness of our approach. </p>
<blockquote>
<p>在这项工作中，我们致力于将表格图像转换为LaTeX代码生成的任务，目标是实现从视觉输入自动重建高质量、适合出版的表格。此任务的核心挑战在于准确处理复杂的表格，这些表格具有大尺、深度嵌套的结构和语义丰富或不规则的单元格内容，现有的方法通常在这里失效。我们首先进行全面分析，确定关键挑战并强调当前评估协议的局限性。为了克服这些问题，我们提出了一种增强的多模式大型语言模型（MLLM）框架，其中对预训练的大型语言模型进行微调，以适应大规模表格到LaTeX数据集的训练。为了进一步提高生成质量，我们引入了一种基于组相对策略优化（GRPO）的双重奖励强化学习策略。与其他仅针对文本输出的优化方法不同，我们的方法结合了LaTeX代码的基于结构层次的奖励以及根据渲染输出计算的视觉保真度奖励，实现了对视觉输出质量的直接优化。我们采用结合了TEDS结构和CW-SSIM的混合评估协议，并展示我们的方法实现了前沿性能，特别是在结构复杂的表格上，证明了我们的方法的有效性和稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17589v1">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong><br>文本主要介绍了从表格图像生成LaTeX代码的任务，目标是实现高质量自动化重建出版物可用表格。其解决了处理复杂表格的中心问题，提出了一个增强的多模态大型语言模型框架和基于集团相对政策优化的双奖励强化学习策略。此方法采用结合TEDS结构和CW-SSIM的评价协议，并且在结构上复杂表格的性能方面达到业界领先水平。整个体系体现了本方法的优越性和稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究对象：本文专注于解决从表格图像生成LaTeX代码的任务，旨在实现高质量表格的视觉输入重建。这一领域的一个重要挑战在于处理复杂的表格数据。</li>
<li>主要挑战：处理大型、嵌套结构复杂、语义丰富或不规则单元格内容的复杂表格是这一任务的主要挑战。现有方法往往在这一问题上难以取得满意的结果。</li>
<li>创新解决方案：文章提出了一个增强型多模态大型语言模型框架来解决上述问题。同时结合预训练的大型模型和在大型表格到LaTeX数据集上进行微调的方法。</li>
<li>强化学习应用：引入了一种基于集团相对政策优化的双奖励强化学习策略，进一步提高生成质量。这种方法结合了LaTeX代码的结构级别奖励和渲染输出的视觉保真度奖励，能够直接优化视觉输出质量。</li>
<li>评价协议：采用了结合TEDS结构和CW-SSIM的混合评价协议，对模型性能进行了全面评估。这种评价协议能够更准确地反映模型在处理复杂结构表格时的性能。</li>
<li>性能表现：该研究实现了业界领先水平，特别是在处理结构复杂的表格上展示了优越的性能表现，这体现了所提出方法的稳健性和实用性。该研究证明了所提出方法在表格图像到LaTeX代码生成任务的性能上具有较高的优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17589">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17589v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17589v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Reason-Over-Non-Text-Modalities-in-a-Training-Free-Manner-A-Case-Study-with-In-Context-Representation-Learning"><a href="#Can-LLMs-Reason-Over-Non-Text-Modalities-in-a-Training-Free-Manner-A-Case-Study-with-In-Context-Representation-Learning" class="headerlink" title="Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A   Case Study with In-Context Representation Learning"></a>Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A   Case Study with In-Context Representation Learning</h2><p><strong>Authors:Tianle Zhang, Wanlong Fang, Jonathan Woo, Paridhi Latawa, Deepak A. Subramanian, Alvin Chan</strong></p>
<p>The remarkable performance of Large Language Models (LLMs) can be enhanced with test-time computation, which relies on external tools and even other deep learning models. However, existing approaches for integrating non-text modality representations into LLMs typically require additional costly supervised training, restricting on-the-fly adaptation to new domains and modalities. In this work, we explore the feasibility of integrating representations from non-text foundational models (FMs) into text-based LLMs in a training-free manner. We propose In-Context Representation Learning (ICRL) as a proof-of-concept to allow LLMs to adaptively utilize non-text modality representations with few-shot learning. Unlike traditional in-context learning, which incorporates text-label pairs, ICRL replaces text inputs with FM representations, enabling the LLM to perform multi-modal inference without fine-tuning. We evaluate ICRL on a suite of tasks in the molecular domain, investigating three core research questions: (i) how to map FM representations into LLMs in a training-free manner, (ii) what factors influence ICRL performance, and (iii) what mechanisms underlie the effectiveness of ICRL. To the best of our knowledge, ICRL is the first training-free framework for integrating non-text modality representations into text-based LLMs, presenting a promising direction for adaptable, multi-modal generalization. </p>
<blockquote>
<p>大型语言模型（LLM）的出色性能可以通过测试时的计算增强，这依赖于外部工具和其他深度学习模型。然而，将非文本模态表示集成到LLM中的现有方法通常需要额外的昂贵的有监督训练，这限制了在新领域和模态的即时适应。在这项工作中，我们探索了以无训练方式将非文本基础模型（FM）的表示集成到基于文本的LLM中的可行性。我们提出基于上下文表示学习（ICRL）的概念验证，以允许LLM以少量样本学习方式自适应地利用非文本模态表示。与传统的上下文学习不同，后者结合了文本标签对，ICRL用FM表示替换文本输入，使LLM能够在无需微调的情况下执行多模态推断。我们在分子领域的多个任务上评估ICRL，并探究三个核心问题：（i）如何在无训练情况下将FM表示映射到LLM中，（ii）哪些因素影响ICRL的性能，以及（iii）ICRL有效性的基础机制是什么。据我们所知，ICRL是第一个用于将非文本模态表示集成到基于文本的LLM中的无训练框架，为可适应的多模态泛化提供了有前景的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17552v1">PDF</a> NIPS 2025</p>
<p><strong>Summary</strong></p>
<p>文本探讨了在无需训练的情况下，将非文本基础模型的表示集成到文本大型语言模型中的可行性。提出了一种名为上下文表示学习（ICRL）的概念，使大型语言模型能够自适应地利用非文本模态表示进行少样本学习，并实现了多模态推理。评价ICRL在分子领域的一系列任务上的表现，并探讨了如何将FM表示映射到LLMs中、影响ICRL性能的因素以及ICRL机制的有效性。此为首个无需训练即可整合非文本模态表示的框架，为可适应的多模态推广提供了有前景的方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型的性能可通过测试时的计算增强，依赖外部工具和深度学习模型。</li>
<li>现有集成非文本模态表示到大型语言模型的方法需要额外的监督训练，限制了在新领域和模态的即时适应。</li>
<li>提出了一种名为上下文表示学习（ICRL）的概念，允许大型语言模型以训练无关的方式利用非文本基础模型的表示。</li>
<li>ICRL实现了少样本学习，使大型语言模型能够进行多模态推理，无需微调。</li>
<li>ICRL在分子领域的任务上进行了评估，并解决了如何将FM表示映射到LLMs、影响ICRL性能的因素以及ICRL机制的有效性等核心问题。</li>
<li>ICRL是首个无需训练的框架，能够整合非文本模态表示到文本为基础的大型语言模型中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17552">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17552v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17552v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17552v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Correlation-or-Causation-Analyzing-the-Causal-Structures-of-LLM-and-LRM-Reasoning-Process"><a href="#Correlation-or-Causation-Analyzing-the-Causal-Structures-of-LLM-and-LRM-Reasoning-Process" class="headerlink" title="Correlation or Causation: Analyzing the Causal Structures of LLM and LRM   Reasoning Process"></a>Correlation or Causation: Analyzing the Causal Structures of LLM and LRM   Reasoning Process</h2><p><strong>Authors:Zhizhang FU, Guangsheng Bao, Hongbo Zhang, Chenkai Hu, Yue Zhang</strong></p>
<p>LLMs suffer from critical reasoning issues such as unfaithfulness, bias, and inconsistency, since they lack robust causal underpinnings and may rely on superficial correlations rather than genuine understanding. Successive LRMs have emerged as a promising alternative, leveraging advanced training techniques such as reinforcement learning (RL) and distillation to improve task accuracy. However, the impact of these training methods on causality remains largely unexplored. In this study, we conduct a systematic causal analysis on LLMs and LRMs, examining structural causal models (SCMs) of four key variables: problem instruction (Z), thinking process (T), reasoning steps (X), and answer (Y). Our findings reveal that RLVR-trained LRMs exhibit enhanced causal reasoning capabilities, aligning more closely with ideal causal structures, while LLMs and distilled LRMs fail to address causality-related deficiencies. Our further investigation indicates that RLVR reduces spurious correlations and strengthens genuine causal patterns, thereby mitigating unfaithfulness and bias. In addition, our inspection on the dynamics of the RLVR training process observes a high correlation between reduced spurious features and improved causal structures, where the causal relationships consistently improve in the training process. This study contributes to the understanding of causality in reasoning models, highlights the critical role of RLVR in enhancing causal reasoning, and provides insights for designing future AI systems with stronger causal foundations. We release our code and data at <a target="_blank" rel="noopener" href="https://github.com/Harryking1999/CoT_Causal_Analysis">https://github.com/Harryking1999/CoT_Causal_Analysis</a>. </p>
<blockquote>
<p>大型语言模型（LLMs）存在推理问题，如缺乏忠实性、偏见和不一致性，因为它们缺乏坚实的因果基础，可能依赖于表面关联而非真正的理解。随着连续学习模型（LRMs）的出现，作为一种有前途的替代方案，它利用先进的训练技术，如强化学习（RL）和蒸馏技术来提高任务准确性。然而，这些训练方法对因果关系的影响在很大程度上尚未被探索。在本研究中，我们对LLMs和LRMs进行了系统的因果分析，研究了四个关键变量的结构因果模型（SCMs）：问题指令（Z）、思考过程（T）、推理步骤（X）和答案（Y）。我们的研究发现，经过RLVR训练的LRMs表现出增强的因果推理能力，更接近于理想的因果结构，而LLMs和蒸馏LRMs无法解决与因果关系相关的缺陷。我们的进一步调查表明，RLVR减少了虚假关联，加强了真正的因果模式，从而减轻了缺乏忠实性和偏见的问题。此外，我们对RLVR训练过程的动态性的检查发现，减少的虚假特征与改善的因果结构之间存在高度相关性，在训练过程中因果关系持续得到改善。本研究有助于理解推理模型中的因果关系，突出了RLVR在增强因果推理中的关键作用，为设计具有更强因果基础的未来人工智能系统提供了见解。我们在<a target="_blank" rel="noopener" href="https://github.com/Harryking1999/CoT_Causal_Analysis">https://github.com/Harryking1999/CoT_Causal_Analysis</a>上发布了我们的代码和数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17380v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）存在推理缺陷，如缺乏真实理解而导致的失实、偏见和不一致性。新型的连续学习模型（LRMs）采用强化学习（RL）和蒸馏等高级训练技术来提升任务准确性，但对因果性的影响尚未充分研究。本研究通过结构因果模型（SCM）对LLMs和LRMs进行因果分析，发现RLVR训练的LRM展现出更出色的因果推理能力，更接近理想因果结构。RLVR能减少虚假关联并强化真实因果模式，改善不忠实和偏见问题。研究还观察到RLVR训练过程中减少虚假特征与改善因果关系的强相关性。本研究有助于理解模型中的因果关系，强调了RLVR在强化因果推理中的角色，为设计未来具有更强因果基础的AI系统提供了启示。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）存在推理问题，如失实、偏见和不一致性。</li>
<li>新型的连续学习模型（LRMs）采用强化学习（RL）等训练技术改善任务准确性。</li>
<li>结构因果模型（SCM）用于分析LLMs和LRMs的因果关系。</li>
<li>RLVR训练的LRM展现出更强的因果推理能力，更接近理想因果结构。</li>
<li>RLVR能有效减少虚假关联，强化真实因果关系。</li>
<li>RLVR训练过程中，减少虚假特征与改善因果关系之间存在强相关性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17380">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17380v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17380v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17380v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17380v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Medical-AI-Consensus-A-Multi-Agent-Framework-for-Radiology-Report-Generation-and-Evaluation"><a href="#Medical-AI-Consensus-A-Multi-Agent-Framework-for-Radiology-Report-Generation-and-Evaluation" class="headerlink" title="Medical AI Consensus: A Multi-Agent Framework for Radiology Report   Generation and Evaluation"></a>Medical AI Consensus: A Multi-Agent Framework for Radiology Report   Generation and Evaluation</h2><p><strong>Authors:Ahmed T. Elboardy, Ghada Khoriba, Essam A. Rashed</strong></p>
<p>Automating radiology report generation poses a dual challenge: building clinically reliable systems and designing rigorous evaluation protocols. We introduce a multi-agent reinforcement learning framework that serves as both a benchmark and evaluation environment for multimodal clinical reasoning in the radiology ecosystem. The proposed framework integrates large language models (LLMs) and large vision models (LVMs) within a modular architecture composed of ten specialized agents responsible for image analysis, feature extraction, report generation, review, and evaluation. This design enables fine-grained assessment at both the agent level (e.g., detection and segmentation accuracy) and the consensus level (e.g., report quality and clinical relevance). We demonstrate an implementation using chatGPT-4o on public radiology datasets, where LLMs act as evaluators alongside medical radiologist feedback. By aligning evaluation protocols with the LLM development lifecycle, including pretraining, finetuning, alignment, and deployment, the proposed benchmark establishes a path toward trustworthy deviance-based radiology report generation. </p>
<blockquote>
<p>自动化生成放射学报告面临双重挑战：建立临床可靠的系统和设计严格的评价协议。我们引入了一种多智能体强化学习框架，该框架既可作为放射生态系统中多模式临床推理的基准测试环境，又可作为评价环境。所提出的框架在模块化架构中集成了大型语言模型（LLM）和大型视觉模型（LVM），由十个专门负责图像分析、特征提取、报告生成、审查和评估的智能体组成。这种设计能够在智能体级别（例如，检测和分割精度）和共识级别（例如，报告质量和临床相关性）进行精细评估。我们在公共放射学数据集上展示了使用ChatGPT-4o的实现，其中LLM作为评估者，与医学放射科医师的反馈相结合。通过使评价协议与LLM开发周期（包括预训练、微调、对齐和部署）保持一致，所提出的基准测试为基于可信偏差的放射学报告生成建立了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17353v1">PDF</a> NeurIPS2025 Workshop: Evaluating the Evolving LLM Lifecycle:   Benchmarks, Emergent Abilities, and Scaling</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个多智能体强化学习框架，用于解决放射学报告生成中的双重挑战：构建可靠的医学系统并设计严格的评价协议。该框架结合了大型语言模型和大型视觉模型，在模块化架构中实现十种专业智能体的功能，包括图像分析、特征提取、报告生成、审查和评估等。该设计可实现在智能体层面和共识层面的精细评估，通过与医学放射科医师反馈相结合，使用ChatGPT-4o在公共放射学数据集上进行了演示实现。通过使评价协议与大型语言模型开发周期保持一致，包括预训练、微调、对齐和部署等阶段，所提出的基准测试有助于建立可靠的基于偏差的放射学报告生成路径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文提出了一个多智能体强化学习框架，用于放射学报告生成中的临床可靠性和严格评价协议的问题。</li>
<li>该框架结合了大型语言模型和大型视觉模型，并采用模块化架构实现多种功能。</li>
<li>框架中包含十种专业智能体，负责图像分析、特征提取、报告生成、审查和评估等任务。</li>
<li>实现了智能体层面和共识层面的精细评估，为评价放射学报告生成的质量提供了全面的视角。</li>
<li>使用ChatGPT-4o在公共放射学数据集上进行了演示实现，结合了医学放射科医师的反馈。</li>
<li>所提出的评价协议与大型语言模型开发周期保持一致，包括预训练、微调、对齐和部署等阶段。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17353">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17353v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17353v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17353v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LLaVul-A-Multimodal-LLM-for-Interpretable-Vulnerability-Reasoning-about-Source-Code"><a href="#LLaVul-A-Multimodal-LLM-for-Interpretable-Vulnerability-Reasoning-about-Source-Code" class="headerlink" title="LLaVul: A Multimodal LLM for Interpretable Vulnerability Reasoning about   Source Code"></a>LLaVul: A Multimodal LLM for Interpretable Vulnerability Reasoning about   Source Code</h2><p><strong>Authors:Ala Jararweh, Michael Adams, Avinash Sahu, Abdullah Mueen, Afsah Anwar</strong></p>
<p>Increasing complexity in software systems places a growing demand on reasoning tools that unlock vulnerabilities manifest in source code. Many current approaches focus on vulnerability analysis as a classifying task, oversimplifying the nuanced and context-dependent real-world scenarios. Even though current code large language models (LLMs) excel in code understanding, they often pay little attention to security-specific reasoning. We propose LLaVul, a multimodal LLM tailored to provide fine-grained reasoning about code through question-answering (QA). Our model is trained to integrate paired code and natural queries into a unified space, enhancing reasoning and context-dependent insights about code vulnerability. To evaluate our model performance, we construct a curated dataset of real-world vulnerabilities paired with security-focused questions and answers. Our model outperforms state-of-the-art general-purpose and code LLMs in the QA and detection tasks. We further explain decision-making by conducting qualitative analysis to highlight capabilities and limitations. By integrating code and QA, LLaVul enables more interpretable and security-focused code understanding. </p>
<blockquote>
<p>随着软件系统复杂性的增加，对解锁源代码中表现出的漏洞的推理工具的需求也在增长。许多当前的方法将漏洞分析作为分类任务，这简化了现实世界中微妙且依赖于上下文情境的情境。尽管当前的大型代码语言模型（LLM）在理解代码方面表现出色，但它们往往忽视了特定于安全性的推理。我们提出了LLaVul，这是一个量身定制的多模式LLM，通过问答（QA）提供对代码的精细推理。我们的模型经过训练，能够将配对代码和自然查询整合到统一的空间中，增强对代码漏洞的推理和上下文依赖的洞察力。为了评估我们的模型性能，我们构建了一个包含以安全为中心的问题和答案的现实世界漏洞数据集。我们的模型在问答和检测任务中的表现优于最新的通用和代码LLM。我们还通过进行定性分析来解释决策制定，以突出我们的能力和局限性。通过整合代码和问答，LLaVul实现了更可解释性和以安全为中心的代码理解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17337v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了软件系统的复杂性增长对理解源代码中的漏洞所需的推理工具的需求。针对当前将漏洞分析作为分类任务的简化处理，以及大型代码语言模型对安全特定推理的忽视，提出了LLaVul模型。该模型通过问答方式提供精细粒度的代码推理，通过整合代码和自然查询进行统一空间处理，增强了关于代码漏洞的上下文相关见解。通过构建现实漏洞与安全相关问题答案的数据集评估模型性能，LLaVul在问答和检测任务中表现出超越现有通用和代码大型语言模型的优势。通过定性分析解释决策制定，展示了LLaVul的能力与局限性。整合代码和问答功能使LLaVul在代码理解方面更具可解释性和安全性关注。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>软件系统的复杂性要求对源代码中的漏洞进行更深入的理解和分析。</li>
<li>当前的方法大多将漏洞分析视为分类任务，未能充分考虑实际场景中的细微差别和上下文依赖。</li>
<li>大型代码语言模型虽然能很好地理解代码，但在安全特定的推理方面关注不足。</li>
<li>LLaVul是一个多模态的大型语言模型，旨在通过问答方式提供精细粒度的代码推理。</li>
<li>LLaVul模型能够整合代码和自然查询，在统一空间内增强对代码漏洞的推理和上下文理解。</li>
<li>LLaVul在构建的现实漏洞数据集上的表现优于现有的通用和代码大型语言模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17337">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17337v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17337v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17337v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17337v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17337v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17337v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17337v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Mano-Report"><a href="#Mano-Report" class="headerlink" title="Mano Report"></a>Mano Report</h2><p><strong>Authors:Tianyu Fu, Anyang Su, Chenxu Zhao, Hanning Wang, Minghui Wu, Zhe Yu, Fei Hu, Mingjia Shi, Wei Dong, Jiayao Wang, Yuyang Chen, Ruiyang Yu, Siran Peng, Menglin Li, Nan Huang, Haitian Wei, Jiawei Yu, Yi Xin, Xilin Zhao, Kai Gu, Ping Jiang, Sifan Zhou, Shuo Wang</strong></p>
<p>Graphical user interfaces (GUIs) are the primary medium for human-computer interaction, yet automating GUI interactions remains challenging due to the complexity of visual elements, dynamic environments, and the need for multi-step reasoning. Existing methods based on vision-language models (VLMs) often suffer from limited resolution, domain mismatch, and insufficient sequential decisionmaking capability. To address these issues, we propose Mano, a robust GUI agent built upon a multi-modal foundation model pre-trained on extensive web and computer system data. Our approach integrates a novel simulated environment for high-fidelity data generation, a three-stage training pipeline (supervised fine-tuning, offline reinforcement learning, and online reinforcement learning), and a verification module for error recovery. Mano demonstrates state-of-the-art performance on multiple GUI benchmarks, including Mind2Web and OSWorld, achieving significant improvements in success rate and operational accuracy. Our work provides new insights into the effective integration of reinforcement learning with VLMs for practical GUI agent deployment, highlighting the importance of domain-specific data, iterative training, and holistic reward design. </p>
<blockquote>
<p>图形用户界面（GUI）是人机交互的主要媒介。然而，由于视觉元素的复杂性、动态环境以及多步骤推理的需求，自动化GUI交互仍然具有挑战性。基于视觉语言模型（VLM）的现有方法通常受到分辨率有限、领域不匹配和序列决策能力不足的困扰。为了解决这个问题，我们提出了名为Mano的稳健GUI代理，它建立在大量网络和计算机系统数据上预训练的多模态基础模型之上。我们的方法结合了用于高保真数据生成的新型模拟环境、三阶段训练管道（监督微调、离线强化学习和在线强化学习）以及用于错误恢复的验证模块。Mano在多个GUI基准测试中表现出卓越的性能，包括Mind2Web和OSWorld，在成功率和操作准确性方面取得了显著改进。我们的工作提供了将强化学习与VLM有效结合以进行实际GUI代理部署的新见解，强调了领域特定数据、迭代训练和整体奖励设计的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17336v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>GUI交互自动化是计算机视觉和机器学习领域的热门挑战，文章提出了一款基于多模态基础模型的GUI交互代理——Mano。Mano通过模拟环境进行高保真数据生成，采用分阶段训练策略，包括监督微调、离线强化学习和在线强化学习三个阶段，并配备验证模块进行错误恢复。在多个GUI基准测试中，Mano表现出卓越的性能，成功率和操作准确性均显著提高。该研究为强化学习与VLM在GUI代理部署中的有效结合提供了新的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GUI交互自动化面临诸多挑战，如视觉元素复杂性、动态环境和多步推理需求。</li>
<li>现有基于视觉语言模型（VLM）的方法存在分辨率有限、领域不匹配和决策能力不足的问题。</li>
<li>Mano是一个基于多模态基础模型的GUI代理，通过模拟环境进行高保真数据生成。</li>
<li>Mano采用分阶段训练策略，包括监督微调、离线强化学习和在线强化学习三个阶段。</li>
<li>Mano配备了验证模块以进行错误恢复，提高了交互的稳健性。</li>
<li>在多个GUI基准测试中，Mano表现出卓越性能，成功率和操作准确性显著提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17336">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17336v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17336v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17336v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17336v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CogAtom-From-Cognitive-Atoms-to-Olympiad-level-Mathematical-Reasoning-in-Large-Language-Models"><a href="#CogAtom-From-Cognitive-Atoms-to-Olympiad-level-Mathematical-Reasoning-in-Large-Language-Models" class="headerlink" title="CogAtom: From Cognitive Atoms to Olympiad-level Mathematical Reasoning   in Large Language Models"></a>CogAtom: From Cognitive Atoms to Olympiad-level Mathematical Reasoning   in Large Language Models</h2><p><strong>Authors:Zhuofan Chen, Jiyuan He, Yichi Zhang, Xing Hu, Haoxing Wen, Jun Bai, Wenge Rong</strong></p>
<p>Mathematical reasoning poses significant challenges for Large Language Models (LLMs) due to its demand for multi-step reasoning and abstract conceptual integration. While recent test-time scaling techniques rely heavily on high-quality, challenging problems, the scarcity of Olympiad-level math problems remains a bottleneck. We introduce CogAtom, a novel cognitive atom-based framework for synthesizing mathematically rigorous and cognitively diverse problems. Unlike prior approaches, CogAtom models problem construction as a process of selecting and recombining fundamental reasoning units, cognitive atoms, extracted from human-authored solutions. A diversity-promoting random walk algorithm enables exploration of the cognitive atom space, while a constraint-based recombination mechanism ensures logical soundness and structural validity. The combinatorial nature of the graph structure provides a near-infinite space of reasoning paths, and the walk algorithm systematically explores this space to achieve large-scale synthesis of high-quality problems; meanwhile, by controlling the number of cognitive atoms, we can precisely adjust problem difficulty, ensuring diversity, scalability, and controllability of the generated problems. Experimental results demonstrate that CogAtom outperforms existing methods in accuracy, reasoning depth, and diversity, generating problems that closely match the difficulty of AIME while exceeding it in structural variation. Our work offers a cognitively grounded pathway toward scalable, high-quality math problem generation.Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Icarus-1111/CogAtom">https://github.com/Icarus-1111/CogAtom</a>. </p>
<blockquote>
<p>数学推理对大型语言模型（LLM）提出了重大挑战，这主要是因为其需要多步骤推理和抽象概念整合。尽管最近的测试时间缩放技术严重依赖于高质量、具有挑战性的问题，但缺乏奥林匹克级别的数学问题仍然是瓶颈。我们引入了CogAtom，这是一种基于认知原子的新型问题合成框架，用于合成数学严谨、认知多样的问题。不同于以往的方法，CogAtom将问题构建过程视为一个选择和重组人类解决方案中提取的基本推理单元——认知原子的过程。一种促进多样性的随机游走算法能够探索认知原子空间，而基于约束的重组机制则确保了逻辑严谨性和结构有效性。图结构的组合性质提供了一个近乎无限的推理路径空间，随机游走算法系统地探索这个空间，以实现高质量问题的大规模合成；同时，通过控制认知原子的数量，我们可以精确地调整问题的难度，确保生成问题的多样性、可扩展性和可控性。实验结果表明，CogAtom在准确性、推理深度和多样性方面优于现有方法，生成的问题与AIME的难度相匹配，并在结构变化上超过了AIME。我们的工作为可伸缩、高质量的数学问题生成提供了一条认知基础途径。我们的代码公开在<a target="_blank" rel="noopener" href="https://github.com/Icarus-1111/CogAtom%E3%80%82">https://github.com/Icarus-1111/CogAtom。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17318v1">PDF</a> </p>
<p><strong>Summary</strong><br>在大型语言模型（LLM）面临数学推理的多步骤和抽象概念整合的挑战时，我们推出了CogAtom，一种新型的基于认知原子的框架，用于合成严谨的数学和多样化的认知问题。它通过选择和重组从人类解决方案中提取的基本推理单元——认知原子，来构建问题。实验结果表明，CogAtom在准确性、推理深度和多样性方面优于现有方法，生成的问题难度与AIME相当，在结构变化上表现更优。CogAtom为大规模高质量数学问题生成提供了认知依据的路径。相关代码已公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在数学推理方面存在挑战，需要多步骤推理和抽象概念整合。</li>
<li>CogAtom是一个基于认知原子的新型框架，用于合成数学和认知问题。</li>
<li>CogAtom通过选择和重组认知原子来构建问题，确保逻辑合理性和结构有效性。</li>
<li>认知原子提取自人类解决方案。</li>
<li>多样性促进的随机游走算法用于探索认知原子空间。</li>
<li>组合性质图结构提供了近乎无限的推理路径空间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17318">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17318v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17318v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17318v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17318v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="GRPOformer-Advancing-Hyperparameter-Optimization-via-Group-Relative-Policy-Optimization"><a href="#GRPOformer-Advancing-Hyperparameter-Optimization-via-Group-Relative-Policy-Optimization" class="headerlink" title="GRPOformer: Advancing Hyperparameter Optimization via Group Relative   Policy Optimization"></a>GRPOformer: Advancing Hyperparameter Optimization via Group Relative   Policy Optimization</h2><p><strong>Authors:Haoxin Guo, Jiawen Pan, Weixin Zhai</strong></p>
<p>Hyperparameter optimization (HPO) plays a critical role in improving model performance. Transformer-based HPO methods have shown great potential; however, existing approaches rely heavily on large-scale historical optimization trajectories and lack effective reinforcement learning (RL) techniques, thereby limiting their efficiency and performance improvements. Inspired by the success of Group Relative Policy Optimization (GRPO) in large language models (LLMs), we propose GRPOformer – a novel hyperparameter optimization framework that integrates reinforcement learning (RL) with Transformers. In GRPOformer, Transformers are employed to generate new hyperparameter configurations from historical optimization trajectories, while GRPO enables rapid trajectory construction and optimization strategy learning from scratch. Moreover, we introduce Policy Churn Regularization (PCR) to enhance the stability of GRPO training. Experimental results on OpenML demonstrate that GRPOformer consistently outperforms baseline methods across diverse tasks, offering new insights into the application of RL for HPO. </p>
<blockquote>
<p>超参数优化（HPO）在提高模型性能中起着关键作用。基于Transformer的HPO方法已显示出巨大的潜力；然而，现有方法严重依赖于大规模的历史优化轨迹，且缺乏有效的强化学习（RL）技术，从而限制了其效率和性能的提升。受大型语言模型（LLM）中群体相对策略优化（GRPO）成功的启发，我们提出了GRPOformer——一个将强化学习与Transformer相结合的新型超参数优化框架。在GRPOformer中，Transformer被用于从历史优化轨迹生成新的超参数配置，而GRPO则能够实现从零开始的快速轨迹构建和优化策略学习。此外，我们引入了策略变动正则化（PCR）以增强GRPO训练稳定性。在OpenML上的实验结果表明，GRPOformer在多种任务上始终优于基准方法，为RL在HPO中的应用提供了新的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17105v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了Hyperparameter optimization（HPO）在改进模型性能中的关键作用。针对现有Transformer-based HPO方法的局限性，提出了一种新的超参数优化框架GRPOformer，它结合了强化学习与Transformer技术。GRPOformer利用Transformer从历史优化轨迹生成新的超参数配置，而GRPO则实现了快速轨迹构建和优化策略学习。此外，还引入了Policy Churn Regularization（PCR）以增强GRPO训练的稳定性。在OpenML上的实验结果表明，GRPOformer在多种任务上始终优于基准方法，为HPO在RL中的应用提供了新的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HPO在提高模型性能方面起着关键作用。</li>
<li>现有Transformer-based HPO方法存在局限性，缺乏强化学习技术的应用。</li>
<li>GRPOformer是一个新的超参数优化框架，结合了强化学习与Transformer技术。</li>
<li>Transformers用于从历史优化轨迹生成新的超参数配置。</li>
<li>GRPO实现了快速轨迹构建和优化策略学习。</li>
<li>引入Policy Churn Regularization（PCR）以增强GRPO训练的稳定性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17105">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17105v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17105v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17105v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17105v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17105v1/page_3_2.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CoPlanner-An-Interactive-Motion-Planner-with-Contingency-Aware-Diffusion-for-Autonomous-Driving"><a href="#CoPlanner-An-Interactive-Motion-Planner-with-Contingency-Aware-Diffusion-for-Autonomous-Driving" class="headerlink" title="CoPlanner: An Interactive Motion Planner with Contingency-Aware   Diffusion for Autonomous Driving"></a>CoPlanner: An Interactive Motion Planner with Contingency-Aware   Diffusion for Autonomous Driving</h2><p><strong>Authors:Ruiguo Zhong, Ruoyu Yao, Pei Liu, Xiaolong Chen, Rui Yang, Jun Ma</strong></p>
<p>Accurate trajectory prediction and motion planning are crucial for autonomous driving systems to navigate safely in complex, interactive environments characterized by multimodal uncertainties. However, current generation-then-evaluation frameworks typically construct multiple plausible trajectory hypotheses but ultimately adopt a single most likely outcome, leading to overconfident decisions and a lack of fallback strategies that are vital for safety in rare but critical scenarios. Moreover, the usual decoupling of prediction and planning modules could result in socially inconsistent or unrealistic joint trajectories, especially in highly interactive traffic. To address these challenges, we propose a contingency-aware diffusion planner (CoPlanner), a unified framework that jointly models multi-agent interactive trajectory generation and contingency-aware motion planning. Specifically, the pivot-conditioned diffusion mechanism anchors trajectory sampling on a validated, shared short-term segment to preserve temporal consistency, while stochastically generating diverse long-horizon branches that capture multimodal motion evolutions. In parallel, we design a contingency-aware multi-scenario scoring strategy that evaluates candidate ego trajectories across multiple plausible long-horizon evolution scenarios, balancing safety, progress, and comfort. This integrated design preserves feasible fallback options and enhances robustness under uncertainty, leading to more realistic interaction-aware planning. Extensive closed-loop experiments on the nuPlan benchmark demonstrate that CoPlanner consistently surpasses state-of-the-art methods on both Val14 and Test14 datasets, achieving significant improvements in safety and comfort under both reactive and non-reactive settings. Code and model will be made publicly available upon acceptance. </p>
<blockquote>
<p>准确的轨迹预测和运动规划对于自动驾驶系统在复杂、交互性强的环境中安全导航至关重要，这些环境具有多模式不确定性。然而，当前的生成-评估框架通常会构建多个合理的轨迹假设，但最终只采用一个最可能的结果，这导致过于自信的决策和缺乏关键的备份策略，这在罕见但关键的场景中至关重要。此外，预测和规划模块的常规解耦可能导致社会不一致或不符合现实的联合轨迹，特别是在高度交互的交通环境中。为了解决这些挑战，我们提出了一种基于意外情况的扩散规划器（CoPlanner），这是一个联合模型多智能体交互轨迹生成和基于意外情况的协同规划的统一框架。具体来说，基于枢点的扩散机制将轨迹采样锚定在一个验证过的共享短期片段上，以保持时间一致性，同时随机生成多样且长远的分支，以捕捉多模式运动演变。同时，我们设计了一种基于意外情况的场景评分策略，该策略评估多个可能的长远未来情景下的候选自我轨迹，在安全性、进展和舒适性之间取得平衡。这种集成设计保留了可行的备份选项，增强了不确定情况下的稳健性，从而实现了更现实、交互意识更强的规划。在nuPlan基准测试上的大量闭环实验表明，CoPlanner在Val14和Test14数据集上始终超过最新方法，并在反应性和非反应性设置下在安全性和舒适性方面取得了显著的改进。论文接受后将公开提供代码和模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17080v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文讨论了自动驾驶系统中准确轨迹预测和运动规划的重要性，指出在复杂、交互性环境中，多模态不确定性下的安全导航是关键挑战。当前框架通常采用生成-评估模式，构建多个可能的轨迹假设，但最终选择最可能的单一结果，导致缺乏应对罕见但关键场景的备用策略。此外，预测和规划模块的解耦可能导致社会不一致或在不具高度交互性的交通环境中产生不现实的联合轨迹。针对这些挑战，提出一种应急感知扩散规划器（CoPlanner）的框架，联合进行多智能体交互轨迹生成和应急感知运动规划。该框架通过中心条件扩散机制在验证过的短期轨迹上进行轨迹采样，同时随机生成多样的长期分支以捕捉多模态运动演化。同时设计了一种应急感知多场景评分策略，在多种可能的长期演化场景中评估候选轨迹，平衡安全、进展和舒适度。在nuPlan基准测试上的闭环实验表明，CoPlanner在Val14和Test14数据集上均超越现有方法，在安全性和舒适性方面取得显著改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动驾驶系统需要准确的轨迹预测和运动规划以应对复杂环境中的不确定性。</li>
<li>当前框架采用单一最可能轨迹决策可能导致缺乏备用策略，影响安全性。</li>
<li>CoPlanner框架联合进行多智能体交互轨迹生成和应急感知运动规划，提高安全性与稳健性。</li>
<li>框架中的中心条件扩散机制确保轨迹采样的短期一致性，并随机生成长期分支以捕捉多模态运动演化。</li>
<li>应急感知多场景评分策略平衡安全、进展和舒适度，实现更现实的交互感知规划。</li>
<li>在nuPlan基准测试上，CoPlanner在多个数据集上超越现有方法，特别是在安全性和舒适性方面表现突出。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17080">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17080v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17080v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17080v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.17080v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Interpretable-Audio-Editing-Evaluation-via-Chain-of-Thought-Difference-Commonality-Reasoning-with-Multimodal-LLMs"><a href="#Interpretable-Audio-Editing-Evaluation-via-Chain-of-Thought-Difference-Commonality-Reasoning-with-Multimodal-LLMs" class="headerlink" title="Interpretable Audio Editing Evaluation via Chain-of-Thought   Difference-Commonality Reasoning with Multimodal LLMs"></a>Interpretable Audio Editing Evaluation via Chain-of-Thought   Difference-Commonality Reasoning with Multimodal LLMs</h2><p><strong>Authors:Yuhang Jia, Xu Zhang, Yang Chen, Hui Wang, Enzhi Wang, Yong Qin</strong></p>
<p>Automatic mean opinion score (MOS) prediction provides a more perceptual alternative to objective metrics, offering deeper insights into the evaluated models. With the rapid progress of multimodal large language models (MLLMs), their enhanced perceptual and reasoning abilities enable more comprehensive and interpretable audio quality assessment. In this work, we tackle the challenging task of audio editing evaluation and propose the first natural language-based automated evaluation framework built on MLLMs. Our approach introduces two fine-tuning tasks to boost multi-audio understanding, combined with Chain-of-Thought prompting, and lightweight instruction tuning, to enhance step-by-step reasoning. Experiment demonstrate that our framework delivers accurate, interpretable, and text-based editing evaluation, closely aligning with human judgments and objective metrics while substantially improving over baselines. The code and demo are available at <a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/Eval_Reasoning">https://github.com/NKU-HLT/Eval_Reasoning</a>. </p>
<blockquote>
<p>自动平均意见得分（MOS）预测提供了一种比客观指标更感知的替代方案，为所评估的模型提供了更深入的了解。随着多模态大型语言模型（MLLMs）的快速发展，其增强的感知和推理能力能够进行更全面和可解释性的音频质量评估。在这项工作中，我们解决了音频编辑评估这一具有挑战性的任务，并基于MLLMs提出了第一个自然语言基础的自动化评估框架。我们的方法引入了两个微调任务来促进多音频理解，结合“思维链”提示和轻量级指令微调，以增强逐步推理。实验表明，我们的框架能够提供准确、可解释性和基于文本编辑评估，与人类判断和客观指标紧密对齐，同时在基准测试上有显著提高。代码和演示可在<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/Eval_Reasoning%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NKU-HLT/Eval_Reasoning找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16975v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>自动平均意见得分（MOS）预测为客观度量提供了一个更加感知的替代方案，它能为所评估的模型提供更深入的见解。随着多模态大型语言模型（MLLMs）的快速发展，其增强的感知和推理能力使音频质量评估更加全面和可解释。在这项工作中，我们解决了音频编辑评估这一具有挑战性的任务，并首次提出了基于MLLMs的自然语言自动化评估框架。我们的方法引入了两个微调任务来提高多音频理解，并结合思维链提示和轻量级指令微调，以增强逐步推理。实验表明，我们的框架能够提供准确、可解释、基于文本编辑评估，与人类判断和客观度量紧密对齐，并在很大程度上优于基线。代码和演示可在<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/Eval_Reasoning%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NKU-HLT/Eval_Reasoning找到。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动平均意见得分（MOS）预测为模型评估提供了更深入的感知视角。</li>
<li>多模态大型语言模型（MLLMs）的快速发展增强了感知和推理能力。</li>
<li>在音频编辑评估任务中引入了自然语言自动化评估框架。</li>
<li>通过两个微调任务和思维链提示提高多音频理解和推理能力。</li>
<li>框架准确、可解释，基于文本编辑评估与人类判断和客观度量对齐。</li>
<li>框架在音频编辑评价上显著优于基线方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16975">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.16975v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.16975v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.16975v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.16975v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="LLMs-as-Layout-Designers-A-Spatial-Reasoning-Perspective"><a href="#LLMs-as-Layout-Designers-A-Spatial-Reasoning-Perspective" class="headerlink" title="LLMs as Layout Designers: A Spatial Reasoning Perspective"></a>LLMs as Layout Designers: A Spatial Reasoning Perspective</h2><p><strong>Authors:Sha Li</strong></p>
<p>While Large Language Models (LLMs) have demonstrated impressive reasoning and planning abilities in textual domains and can effectively follow instructions for complex tasks, their capacity for spatial understanding and reasoning remains limited. Such capabilities, however, are critical for applications like content-aware graphic layout design, which demands precise placement, alignment, and structural organization of multiple elements within constrained visual spaces. To address this gap, we propose LaySPA, a reinforcement learning-based framework that augments LLM agents with explicit spatial reasoning capabilities. LaySPA leverages hybrid reward signals that capture geometric validity, structural fidelity, and visual quality, enabling agents to model inter-element relationships, navigate the canvas, and optimize spatial arrangements. Through iterative self-exploration and adaptive policy optimization, LaySPA produces both interpretable reasoning traces and structured layouts. Experimental results demonstrate that LaySPA generates structurally sound and visually appealing layouts, outperforming larger general-purpose LLMs and achieving results on par with state-of-the-art specialized layout models. </p>
<blockquote>
<p>虽然大型语言模型（LLM）在文本领域表现出了令人印象深刻的推理和规划能力，并能有效执行复杂任务的指令，但它们在空间理解和推理方面的能力仍然有限。然而，对于内容感知的图形布局设计这类应用而言，这种能力至关重要。内容感知的图形布局设计要求在有限的视觉空间内对多个元素进行精确放置、对齐和结构组织。为了解决这一差距，我们提出了LaySPA，一个基于强化学习的框架，为LLM代理增加了明确的空间推理能力。LaySPA利用混合奖励信号，捕捉几何有效性、结构保真度和视觉质量，使代理能够模拟元素间的关系、导航画布并优化空间布局。通过迭代自我探索和自适应策略优化，LaySPA既产生可解释的推理轨迹，又生成结构化布局。实验结果表明，LaySPA生成的布局结构健全、视觉吸引力强，优于更大的通用LLM，并与最新的专业布局模型结果相当。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16891v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在文本领域展现出强大的推理和规划能力，并能有效执行复杂任务的指令，但在空间理解和推理方面仍存在局限。对于内容感知的图形布局设计等应用，精确放置、对齐和结构化组织视觉空间内的多个元素至关重要。为解决这一差距，提出LaySPA，一个基于强化学习的框架，增强LLM代理的空间推理能力。LaySPA利用混合奖励信号，捕捉几何有效性、结构保真度和视觉质量，使代理能够模拟元素间的关系、导航画布并优化空间布局。通过迭代自我探索和自适应策略优化，LaySPA产生可解释的推理轨迹和结构化的布局。实验结果表明，LaySPA生成的布局结构稳健、视觉吸引力强，优于大型通用LLMs，并与专业布局模型的最新结果相当。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在文本处理方面表现出强大的推理和规划能力，但在空间理解和推理方面存在局限。</li>
<li>空间理解和推理对于内容感知的图形布局设计等应用至关重要。</li>
<li>LaySPA是一个基于强化学习的框架，旨在增强LLM代理的空间推理能力。</li>
<li>LaySPA利用混合奖励信号来捕捉几何有效性、结构保真度和视觉质量。</li>
<li>LaySPA使代理能够模拟元素间的关系、导航画布并优化空间布局。</li>
<li>通过迭代自我探索和自适应策略优化，LaySPA能够产生可解释的推理轨迹和结构化的布局。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16891">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.16891v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.16891v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.16891v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.16891v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Evaluating-LLM-Generated-Detection-Rules-in-Cybersecurity"><a href="#Evaluating-LLM-Generated-Detection-Rules-in-Cybersecurity" class="headerlink" title="Evaluating LLM Generated Detection Rules in Cybersecurity"></a>Evaluating LLM Generated Detection Rules in Cybersecurity</h2><p><strong>Authors:Anna Bertiger, Bobby Filar, Aryan Luthra, Stefano Meschiari, Aiden Mitchell, Sam Scholten, Vivek Sharath</strong></p>
<p>LLMs are increasingly pervasive in the security environment, with limited measures of their effectiveness, which limits trust and usefulness to security practitioners. Here, we present an open-source evaluation framework and benchmark metrics for evaluating LLM-generated cybersecurity rules. The benchmark employs a holdout set-based methodology to measure the effectiveness of LLM-generated security rules in comparison to a human-generated corpus of rules. It provides three key metrics inspired by the way experts evaluate security rules, offering a realistic, multifaceted evaluation of the effectiveness of an LLM-based security rule generator. This methodology is illustrated using rules from Sublime Security’s detection team and those written by Sublime Security’s Automated Detection Engineer (ADE), with a thorough analysis of ADE’s skills presented in the results section. </p>
<blockquote>
<p>大型语言模型（LLMs）在安全环境中的应用越来越普遍，但其有效性的衡量措施有限，这限制了安全从业人员对其的信任和实用性。在此，我们提出一个开源评估框架和基准指标，用于评估LLM生成的网络安全规则的效能。该基准采用基于保留集的方法，来衡量LLM生成的网络安全规则与人为生成的规则集相比的有效性。它提供了三个受专家评估安全规则启发的关键指标，为基于LLM的安全规则生成器提供现实、多方面的有效性评估。该方法使用Sublime Security检测团队制定的规则以及Sublime Security自动检测工程师（ADE）编写的规则进行说明，并在结果部分对ADE的技能进行了详细分析。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16749v1">PDF</a> Preprint of a paper accepted at the Conference on Applied Machine   Learning in Information Security (CAMLIS 2025). 11 pages, 3 figures, 4 tables</p>
<p><strong>Summary</strong>：随着大型语言模型（LLM）在安全领域的广泛应用，对其生成的安全规则的有效性评估显得尤为重要。本研究提出了一种开源评估框架和基准测试指标，以比较LLM生成的安全规则与人类生成的安全规则集的有效性。该基准测试采用保留集方法，通过专家评价安全规则的三个关键指标来全面评估LLM生成的安全规则的有效性。研究使用了Sublime Security的检测团队规则和自动化检测工程师（ADE）编写的规则，并对ADE的技能进行了详细分析。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>LLMs在安全领域的应用越来越广泛，但对其生成的安全规则的有效性评估措施有限，影响了安全实践者对它们的信任和使用。</li>
<li>研究提出了一种开源评估框架和基准测试指标，用于评估LLM生成的安全规则的有效性。</li>
<li>该评估框架采用保留集方法进行比较，衡量LLM生成的安全规则与人类生成的安全规则集的效果。</li>
<li>提供了三个关键指标，以专家评价安全规则的方式全面评估LLM生成的安全规则的有效性。</li>
<li>研究使用了Sublime Security的检测团队规则和ADE的规则进行实例分析。</li>
<li>该研究强调了自动化检测工程师（ADE）技能的重要性，在结果部分对其进行了详细分析。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16749">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.16749v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.16749v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.16749v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="EG-MLA-Embedding-Gated-Multi-head-Latent-Attention-for-Scalable-and-Efficient-LLMs"><a href="#EG-MLA-Embedding-Gated-Multi-head-Latent-Attention-for-Scalable-and-Efficient-LLMs" class="headerlink" title="EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and   Efficient LLMs"></a>EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and   Efficient LLMs</h2><p><strong>Authors:Zhengge Cai, Haowen Hou</strong></p>
<p>Reducing the key-value (KV) cache size is a crucial step toward enabling efficient inference in large language models (LLMs), especially under latency and memory constraints. While Multi-Head Attention (MHA) offers strong representational power, it incurs significant memory overhead. Recent work on Multi-head Latent Attention (MLA) mitigates this by compressing KV representations into a shared latent space, achieving a better trade-off between performance and cache efficiency. While MLA already achieves significant KV cache reduction, the scope for further compression remains limited without performance loss. In this paper, we propose \textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel extension of MLA that further reduces KV cache size while enhancing representational expressiveness. EG-MLA introduces a token-specific embedding gating mechanism applied in the latent space, enabling fine-grained modulation of compressed KV vectors with minimal additional computation. Compared to MHA, EG-MLA achieves over 91.6% reduction in KV cache size with negligible performance degradation. Relative to MLA, EG-MLA consistently improves task accuracy across diverse reasoning benchmarks while achieving up to 59.9% additional memory savings. Our theoretical analysis highlights how embedding gating induces implicit high-order interactions, and empirical evaluations demonstrate robust generalization across model scales and compression regimes. Notably, we successfully scale EG-MLA to over 1 billion parameters, demonstrating its practical viability for large-scale LLM deployment. These results establish EG-MLA as a memory- and compute-efficient attention mechanism that enables scalable, high-performance inference in modern LLMs. </p>
<blockquote>
<p>减少键值（KV）缓存大小是在大型语言模型（LLM）中实现高效推理的关键步骤，尤其是在延迟和内存受限的情况下。虽然多头注意力（MHA）具有很强的表征能力，但它会引起巨大的内存开销。最近的多头潜在注意力（MLA）的研究通过压缩KV表示到共享潜在空间来缓解这个问题，在性能和缓存效率之间实现了更好的权衡。尽管MLA已经实现了显著的KV缓存减少，但在不损失性能的情况下进一步压缩的范围仍然有限。</p>
</blockquote>
<p>在本文中，我们提出了<strong>嵌入门控多头潜在注意力（EG-MLA）</strong>，这是MLA的一种新型扩展，可以进一步减少KV缓存大小，同时提高表征表达能力。EG-MLA在潜在空间中应用了一种特定于符号的嵌入门控机制，能够对压缩的KV向量进行精细调节，同时只需极少的额外计算。与MHA相比，EG-MLA实现了超过91.6%的KV缓存大小减少，同时性能损失微乎其微。相对于MLA，EG-MLA在多种推理基准测试中始终提高了任务准确性，同时实现了高达59.9%的额外内存节省。我们的理论分析强调了嵌入门控如何引发隐式的高阶交互，而经验评估则证明了其在模型规模和压缩制度下的稳健泛化能力。值得注意的是，我们成功地将EG-MLA扩展到超过1亿参数，这证明了其在大规模LLM部署中的实际可行性。这些结果将EG-MLA确立为一种内存和计算高效的注意力机制，能够在现代LLM中实现可扩展、高性能的推理。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16686v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>该文探讨了在大型语言模型（LLM）中如何通过减少键值（KV）缓存大小以实现高效推理。文章提出了Embedding-Gated Multi-head Latent Attention（EG-MLA）这一新型扩展机制，进一步减小KV缓存大小并提高表达力。EG-MLA在潜在空间引入特定符号嵌入门控机制，可对压缩的KV向量进行精细调制，同时计算成本增加极小。与Multi-Head Attention（MHA）相比，EG-MLA在KV缓存大小上减少了超过91.6%，性能下降微乎其微。相对于Multi-head Latent Attention（MLA），EG-MLA在各种推理基准测试中任务准确度持续提高，同时实现了高达59.9%的额外内存节省。本文理论分析和实证评估证明了embedding门控机制能够引发隐式高阶交互，并在模型规模和压缩机制上展现出稳健的泛化能力。特别是，EG-MLA成功扩展至超过1亿参数，证明其在大规模LLM部署中的实际可行性。结果证明EG-MLA是一种内存和计算高效的注意力机制，可实现现代大型语言模型的可伸缩、高性能推理。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>减少键值（KV）缓存大小是实现大型语言模型（LLM）高效推理的关键步骤。</li>
<li>Multi-head Latent Attention（MLA）通过压缩KV表示到共享潜在空间，实现了性能和缓存效率之间的平衡。</li>
<li>提出的Embedding-Gated Multi-head Latent Attention（EG-MLA）机制进一步减小KV缓存大小，同时提高表达力。</li>
<li>EG-MLA引入特定符号嵌入门控机制，可对压缩的KV向量进行精细调制，且几乎不影响性能。</li>
<li>与MHA相比，EG-MLA大幅减少了KV缓存大小，同时保持高性能。</li>
<li>相对MLA，EG-MLA提高了任务准确度，并实现了显著的额外内存节省。</li>
<li>EG-MLA成功扩展至大规模参数，证明其在大型语言模型部署中的实用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16686">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.16686v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.16686v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.16686v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.16686v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.16686v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_R1_Reasoning/2509.16686v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/LLM/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_LLM/2509.17259v1/page_5_0.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-09-24  Seg4Diff Unveiling Open-Vocabulary Segmentation in Text-to-Image   Diffusion Transformers
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Talking Head Generation/2509.16922v1/page_4_0.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-09-24  Beat on Gaze Learning Stylized Generation of Gaze and Head Dynamics
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30055.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
